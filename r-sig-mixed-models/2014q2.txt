From M.Fairbrother at bristol.ac.uk  Tue Apr  1 00:44:18 2014
From: M.Fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Mon, 31 Mar 2014 23:44:18 +0100
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 87, Issue 34
In-Reply-To: <20140325085620.10033te0vc50ka68@www.staffmail.ed.ac.uk>
References: <mailman.5.1395486002.17262.r-sig-mixed-models@r-project.org>
	<CAAH-yP-xLC2ap2q3uHdZ_2iUmE_7m3gyN-YBRT5ToGzQVuTa3w@mail.gmail.com>
	<20140325085620.10033te0vc50ka68@www.staffmail.ed.ac.uk>
Message-ID: <CAAH-yP--QGB_YfSJ9hhaH0fS335KHkAEWhOGtv95PvbSzH5zgg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140331/2406ccb4/attachment.pl>

From raluca.gui at gmail.com  Tue Apr  1 12:17:13 2014
From: raluca.gui at gmail.com (Raluca Gui)
Date: Tue, 1 Apr 2014 12:17:13 +0200
Subject: [R-sig-ME] Simulating hierarchical data with correlated errors
Message-ID: <CAEhWoo6H3F9ypnuP6=JmdzCu0CKHRFVg08MdnJvD07CUcXdPdA@mail.gmail.com>

-- 


*Raluca Ioana Gui*

PhD Student



University of Zurich | Department of Business Administration

Chair for Marketing and Market Research | URPP Social Networks

Andreasstrasse 15 | 8050 Zurich | Switzerland

Office AND 4.44



Phone: +41 44 634 9200 | Fax: +41 44 634 2940

raluca.gui at business.uzh.ch <margot.loewenberg at business.uzh.ch>

www.market-research.uzh.ch
www.socialnetworks.uzh.ch

From raluca.gui at gmail.com  Tue Apr  1 13:05:58 2014
From: raluca.gui at gmail.com (Raluca Gui)
Date: Tue, 1 Apr 2014 13:05:58 +0200
Subject: [R-sig-ME] lme4 with simulated hierarchical data with correlated
	errors
Message-ID: <CAEhWoo5P-OL2od6hfnCTwunx_fgkuZGJSEppLgQ1twkaaz=N0g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140401/e116c9cb/attachment.pl>

From john.hodsoll at kcl.ac.uk  Tue Apr  1 13:43:15 2014
From: john.hodsoll at kcl.ac.uk (Hodsoll, John)
Date: Tue, 1 Apr 2014 11:43:15 +0000
Subject: [R-sig-ME] summary fn zero-inflated MCMC models.
In-Reply-To: <20140331173837.13842co7j7pjutxc@www.staffmail.ed.ac.uk>
References: <af732c6f3e7f402eab6d5732d8ae4c9e@AM3PR03MB563.eurprd03.prod.outlook.com>
	<20140331173837.13842co7j7pjutxc@www.staffmail.ed.ac.uk>
Message-ID: <7a4612744099411dbd00fa98229c844a@AM3PR03MB563.eurprd03.prod.outlook.com>

Great. Thanks for the work-around!! 
Bw
John
-----Original Message-----
From: Jarrod Hadfield [mailto:j.hadfield at ed.ac.uk] 
Sent: 31 March 2014 17:39
To: Hodsoll, John
Cc: 'r-sig-mixed-models at r-project.org'
Subject: Re: [R-sig-ME] summary fn zero-inflated MCMC models.

Hi,

That's annoying. The bug happens with print.summary, and will occur for all zero-inflated, hurdle and multinomial models with idh residual structure. I will upload a fix ASAP. For now,

cf.za.1$Residual$nrt<-2

should work when using summary(cf.za.1).

Cheers,

Jarrod



Quoting "Hodsoll, John" <john.hodsoll at kcl.ac.uk> on Mon, 31 Mar 2014
13:04:03 +0000:

> Dear all / Jarrod
>
> Have just updated to the latest version of MCMCglmm but am now unable 
> to produce model info using the summary function for zero inflated 
> models. It doesn't work on hurdle models fitted on the previous 
> version either (but these do on an older version I have on my laptop).
>
> Error is
>
> Error in rep(rep(1:length(object$Residual$nrt), object$Residual$nrt),  :
>   invalid 'times' argument
>
> Unless something was wrong with my model specification previously and 
> something in the update is now showing that?
>
> Best wishses
> John Hodsoll
>
> prior and model definitions..
>
> #### Define prior  #####
> zal2.prior <-  list(B= list (mu = matrix(c(rep(0,20)),20),V = diag(20)*(20)),
>                     R = list(V = diag(2), n = 0.002, fix = 2),
>                     G = list(G1 = list(V = diag(2), n = 0.002)))
>
>
> zal2.prior$B$mu[9] <- 1
> diag(zal2.prior$B$V)[9]<-1e-9
>
> zal2.prior$B$mu[10] <- 1
> diag(zal2.prior$B$V)[10]<-1e-9
>
> #Level 2 models with complementary log log link.
>
> system.time(
>   cf.za.1 <- MCMCglmm(totflct ~ trait-1 + trait:(expcon.r*period.x + 
> log.beds + nshift + cday),
>                       data = rswAll.df, family = "zapoisson",
>                       random = ~us(trait):wardn,
>                       rcov = ~idh(trait):units,
>                       prior = zal2.prior,
>                  #     nitt = 300000, burnin = 50000, thin = 250,
>                       verbose = TRUE, pr = TRUE, pl = FALSE)
> )
>
> summary(cf.za.1)
> save(cf.za.1, file="cf.za.1.rda")
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



--
The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.


From bbolker at gmail.com  Tue Apr  1 16:13:38 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 01 Apr 2014 10:13:38 -0400
Subject: [R-sig-ME] lme4 with simulated hierarchical data with
 correlated errors
In-Reply-To: <CAEhWoo5P-OL2od6hfnCTwunx_fgkuZGJSEppLgQ1twkaaz=N0g@mail.gmail.com>
References: <CAEhWoo5P-OL2od6hfnCTwunx_fgkuZGJSEppLgQ1twkaaz=N0g@mail.gmail.com>
Message-ID: <533AC992.3070107@gmail.com>

  This is not trivial.

  The authors use a completely different estimation approach (and
describe, but do not give code for, their procedure). In Appendix 2 the
authors describe their method for random-effects estimation, which uses
a "feasible GLS estimator" from Verbeek 2000 "A guide to modern
econometrics": "For more details on the computation of the weighting
matrix, see Verbeek (2000), Hsiao (1986) and Baltagi (2001). Several
other random-effects estimation procedures for model (1) are available
that include the iterative GLS (IGLS) approach, (restricted) maximum
likelihood (REML), or Bayesian procedures (see e.g. Goldstein, 1995;
Longford, 1993)."

  It would take considerable work (at least on my part! maybe it's
easy/already known for someone else on the list) to work through and
understand the characteristics of these different estimation methods.

  Maybe it's a good thing that REML as implemented by lme4 is less biased?

  Ben Bolker


On 14-04-01 07:05 AM, Raluca Gui wrote:
> Hello,
> 
> I simulate a multilevel model, with 2 levels,  of the form:
> 
> y_ij=beta_0+beta_1*x_ij+alfa_i+eta_ij
> 
> Individual-level units: i=1,...,150
> Group-level units: j=1,...,10
> The error therms are assumed to follow N(0,1)
> 
> 
> I want to compute the magnitude of the bias of the estimators when there is
> correlation between x_ij and alfa_i and between x_ij and eta_ij.
> 
> I assign the following true values for the beta parameters: beta_0=10,
> beta_1=2. I run 250 replications.
> 
> I study 2 cases, depending on the magnitude of the correlation:
> 
> i)  corr(x_ij, alfa_i) = 0 , corr(x_ij, eta_ij) = 0.3
> ii)  corr(x_ij, alfa_i) = 0.3 , corr(x_ij, eta_ij) = 0.3
> 
> 
> What I am doing is actually a replications of a paper by Ebbes et al.
> (2004). The problem encountered is that I do not get the same magnitude of
> the bias as the authors. I get considerably smaller bias.
> 
> Below are the results of the authors across 250 replications (means and
> std. deviations):
> 
> 
>                                                      Case
> 
> 
> i                       ii
> 
> Fixed Effects          beta_0             -                       -
>                                beta_1       2.43 (0.04)          2.42 (0.04)
> 
> 
> Random Effects     beta_0        7.88 (0.20)          5.79 (0.30)
>                                beta_1       2.42 (0.04)          2.84 (0.06)
>                               sigma_alfa  0.99 (0.13)          0.00 (0.00)
>                               sigma_eta   0.90 (0.03)         0.91 (0.04)
> 
> 
> My code:
> 
> 
> require(lme4)
> require(MASS)
> k <- 10            # total number of firms
> n <- 15            # number of employees within each firm
> j <- k*n        # total number of employees
> beta0=10
> beta1=2
> rho1=0.0001     # this is Case iii
> rho2=0.3
> beta_zero <- rep(NA,250)
> beta_one <- rep(NA, 250)
> for (i in 1:250) {
> c <- cbind(FID = sort(rep(1:k,n)), Fint =rnorm(j,0,1),Z = rnorm(j,0,1))
>             #generate level 2 data
> alfa_i <- rnorm(j, mean=0, 1)
> eta_ij <- rnorm(j, mean=0, 1)
> b <- cbind(EID = 1:j, Eint=rnorm(j,0,1), X = rnorm(j,mean =0, sd
> =1)+eta_ij+alfa_i)  # generate level 1 data
> m1dat <- data.frame(c,b)
> m1dat <- within(m1dat, {
>    EID <- factor(EID)
>    FID <- factor(FID)}
>    )
> outcome <- data.frame(y_ij =
> beta0+beta1*(m1dat[,'X'])+rho1*alfa_i+rho2*eta_ij )
> m1dat <- cbind(m1dat,outcome)
> rand_eff <- lmer(y_ij ~1+X+(1|FID), data=m1dat)
> beta_zero[i] <- fixed.effects(rand_eff)[[1]]
> beta_one[i] <- fixed.effects(rand_eff)[[2]]
> }
>  means <- c(mean(beta_zero),mean(beta_one))
> 
> 
> 
> Thanks in advance
>


From markus.jantti at iki.fi  Tue Apr  1 16:21:49 2014
From: markus.jantti at iki.fi (=?ISO-8859-1?Q?Markus_J=E4ntti?=)
Date: Tue, 01 Apr 2014 16:21:49 +0200
Subject: [R-sig-ME] lme4 with simulated hierarchical data with
 correlated errors
In-Reply-To: <533AC992.3070107@gmail.com>
References: <CAEhWoo5P-OL2od6hfnCTwunx_fgkuZGJSEppLgQ1twkaaz=N0g@mail.gmail.com>
	<533AC992.3070107@gmail.com>
Message-ID: <533ACB7D.8020701@iki.fi>

You probably want to rerun your simulation using the nlme::gls. Econometricians 
tend to use gls to estimate what they call random effects models rather than REML.

Markus Jantti

On 01/04/14 16:13, Ben Bolker wrote:
>    This is not trivial.
>
>    The authors use a completely different estimation approach (and
> describe, but do not give code for, their procedure). In Appendix 2 the
> authors describe their method for random-effects estimation, which uses
> a "feasible GLS estimator" from Verbeek 2000 "A guide to modern
> econometrics": "For more details on the computation of the weighting
> matrix, see Verbeek (2000), Hsiao (1986) and Baltagi (2001). Several
> other random-effects estimation procedures for model (1) are available
> that include the iterative GLS (IGLS) approach, (restricted) maximum
> likelihood (REML), or Bayesian procedures (see e.g. Goldstein, 1995;
> Longford, 1993)."
>
>    It would take considerable work (at least on my part! maybe it's
> easy/already known for someone else on the list) to work through and
> understand the characteristics of these different estimation methods.
>
>    Maybe it's a good thing that REML as implemented by lme4 is less biased?
>
>    Ben Bolker
>
>
> On 14-04-01 07:05 AM, Raluca Gui wrote:
>> Hello,
>>
>> I simulate a multilevel model, with 2 levels,  of the form:
>>
>> y_ij=beta_0+beta_1*x_ij+alfa_i+eta_ij
>>
>> Individual-level units: i=1,...,150
>> Group-level units: j=1,...,10
>> The error therms are assumed to follow N(0,1)
>>
>>
>> I want to compute the magnitude of the bias of the estimators when there is
>> correlation between x_ij and alfa_i and between x_ij and eta_ij.
>>
>> I assign the following true values for the beta parameters: beta_0=10,
>> beta_1=2. I run 250 replications.
>>
>> I study 2 cases, depending on the magnitude of the correlation:
>>
>> i)  corr(x_ij, alfa_i) = 0 , corr(x_ij, eta_ij) = 0.3
>> ii)  corr(x_ij, alfa_i) = 0.3 , corr(x_ij, eta_ij) = 0.3
>>
>>
>> What I am doing is actually a replications of a paper by Ebbes et al.
>> (2004). The problem encountered is that I do not get the same magnitude of
>> the bias as the authors. I get considerably smaller bias.
>>
>> Below are the results of the authors across 250 replications (means and
>> std. deviations):
>>
>>
>>                                                       Case
>>
>>
>> i                       ii
>>
>> Fixed Effects          beta_0             -                       -
>>                                 beta_1       2.43 (0.04)          2.42 (0.04)
>>
>>
>> Random Effects     beta_0        7.88 (0.20)          5.79 (0.30)
>>                                 beta_1       2.42 (0.04)          2.84 (0.06)
>>                                sigma_alfa  0.99 (0.13)          0.00 (0.00)
>>                                sigma_eta   0.90 (0.03)         0.91 (0.04)
>>
>>
>> My code:
>>
>>
>> require(lme4)
>> require(MASS)
>> k <- 10            # total number of firms
>> n <- 15            # number of employees within each firm
>> j <- k*n        # total number of employees
>> beta0=10
>> beta1=2
>> rho1=0.0001     # this is Case iii
>> rho2=0.3
>> beta_zero <- rep(NA,250)
>> beta_one <- rep(NA, 250)
>> for (i in 1:250) {
>> c <- cbind(FID = sort(rep(1:k,n)), Fint =rnorm(j,0,1),Z = rnorm(j,0,1))
>>              #generate level 2 data
>> alfa_i <- rnorm(j, mean=0, 1)
>> eta_ij <- rnorm(j, mean=0, 1)
>> b <- cbind(EID = 1:j, Eint=rnorm(j,0,1), X = rnorm(j,mean =0, sd
>> =1)+eta_ij+alfa_i)  # generate level 1 data
>> m1dat <- data.frame(c,b)
>> m1dat <- within(m1dat, {
>>     EID <- factor(EID)
>>     FID <- factor(FID)}
>>     )
>> outcome <- data.frame(y_ij =
>> beta0+beta1*(m1dat[,'X'])+rho1*alfa_i+rho2*eta_ij )
>> m1dat <- cbind(m1dat,outcome)
>> rand_eff <- lmer(y_ij ~1+X+(1|FID), data=m1dat)
>> beta_zero[i] <- fixed.effects(rand_eff)[[1]]
>> beta_one[i] <- fixed.effects(rand_eff)[[2]]
>> }
>>   means <- c(mean(beta_zero),mean(beta_one))
>>
>>
>>
>> Thanks in advance
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


-- 
Markus Jantti
Professor of Economics
Swedish Institute for Social Research, Stockholm University


From 538280 at gmail.com  Tue Apr  1 20:32:37 2014
From: 538280 at gmail.com (Greg Snow)
Date: Tue, 1 Apr 2014 12:32:37 -0600
Subject: [R-sig-ME] Simulating hierarchical data with correlated errors
In-Reply-To: <CAEhWoo6H3F9ypnuP6=JmdzCu0CKHRFVg08MdnJvD07CUcXdPdA@mail.gmail.com>
References: <CAEhWoo6H3F9ypnuP6=JmdzCu0CKHRFVg08MdnJvD07CUcXdPdA@mail.gmail.com>
Message-ID: <CAFEqCdwj6Nnp38dwRFn=mDmf1QddDvAH+=uPC6V-uUHEO+-oDQ@mail.gmail.com>

The help page for the simfun function in the TeachingDemos package has
some examples of simulating hierarchical data.  Just replace the calls
to rnorm with calls to mvrnorm from the MASS package with the
correlation structure that you want and you will have a tool to
simulate hierarchical data with correlated errors.

If you want better help than that then you will need to at minimum ask
a question.  But you should read through the posting guide (link at
the bottom of this message and all messages on the list) to learn how
to ask a good question that will have the best chance of leading to a
good answer.

On Tue, Apr 1, 2014 at 4:17 AM, Raluca Gui <raluca.gui at gmail.com> wrote:
> --
>
>
> *Raluca Ioana Gui*
>
> PhD Student
>
>
>
> University of Zurich | Department of Business Administration
>
> Chair for Marketing and Market Research | URPP Social Networks
>
> Andreasstrasse 15 | 8050 Zurich | Switzerland
>
> Office AND 4.44
>
>
>
> Phone: +41 44 634 9200 | Fax: +41 44 634 2940
>
> raluca.gui at business.uzh.ch <margot.loewenberg at business.uzh.ch>
>
> www.market-research.uzh.ch
> www.socialnetworks.uzh.ch
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From maxwe128 at umn.edu  Tue Apr  1 21:14:43 2014
From: maxwe128 at umn.edu (Maxwell Elliott)
Date: Tue, 1 Apr 2014 14:14:43 -0500
Subject: [R-sig-ME] Including Nested Factor in Repeated Measures Logistic
	Regression
Message-ID: <CAGmtytuMVBo9XfzzUsBGr-b9t6=x8tb1LriGBHoJe8q2+VnqJA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140401/1ed00515/attachment.pl>

From caroli at sun.ac.za  Wed Apr  2 11:49:57 2014
From: caroli at sun.ac.za (De Waal, C, Mej <caroli@sun.ac.za>)
Date: Wed, 2 Apr 2014 09:49:57 +0000
Subject: [R-sig-ME] Use of offset variable when analysing rates in lme4
Message-ID: <17EB07B22F1D644289C4749D6230666F39B63875@STBEXMB02.stb.sun.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140402/d7c9710b/attachment.pl>

From longrob604 at gmail.com  Wed Apr  2 12:28:24 2014
From: longrob604 at gmail.com (W Robert Long)
Date: Wed, 02 Apr 2014 11:28:24 +0100
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>
Message-ID: <533BE648.4040005@gmail.com>

Hi all

I am running a simple random intercepts model using lme4 on 
approximately 70,000 observations, with 250 clusters. The code looks like

glmer(Y~x1+x2+x3+x4+x5+x6+x7+x8+x9+(1|clusdID),
	data=dt1, family=binomial(link=logit))

and I receive the following warnings:

Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
   Model failed to converge with max|grad| = 4847.75 (tol = 0.001)
2: In if (resHess$code != 0) { :
   the condition has length > 1 and only the first element will be used
3: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
   Model is nearly unidentifiable: very large eigenvalue
  - Rescale variables?;Model is nearly unidentifiable: large eigenvalue 
ratio
  - Rescale variables?

There are some small clusters (<10 obs per cluster), but even removing 
those, the warnings remain.

Using Stata -xtmelogit- there are no warnings and the output is almost 
identical to glmer() so this gives me some comfort, yet I still worry 
about these warnings from glmer.

I have tried setting nAGQ as high as 10, to no avail.

Could anyone suggest what I can look for or change ? The data are 
confidential so I can't easily make a reprodicible example.

Thanks in advance
Robert Long


From longrob604 at gmail.com  Wed Apr  2 12:40:48 2014
From: longrob604 at gmail.com (W Robert Long)
Date: Wed, 02 Apr 2014 11:40:48 +0100
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <533BE648.4040005@gmail.com>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>
	<533BE648.4040005@gmail.com>
Message-ID: <533BE930.90706@gmail.com>

I should perhaps also mention that of the 9 covariates, 3 are continous 
and I have tried standardising them. Of the remaining 6, 5 are binary 
and the last one is ordinal.

On 02/04/2014 11:28, W Robert Long wrote:
> Hi all
>
> I am running a simple random intercepts model using lme4 on
> approximately 70,000 observations, with 250 clusters. The code looks like
>
> glmer(Y~x1+x2+x3+x4+x5+x6+x7+x8+x9+(1|clusdID),
>      data=dt1, family=binomial(link=logit))
>
> and I receive the following warnings:
>
> Warning messages:
> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>    Model failed to converge with max|grad| = 4847.75 (tol = 0.001)
> 2: In if (resHess$code != 0) { :
>    the condition has length > 1 and only the first element will be used
> 3: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>    Model is nearly unidentifiable: very large eigenvalue
>   - Rescale variables?;Model is nearly unidentifiable: large eigenvalue
> ratio
>   - Rescale variables?
>
> There are some small clusters (<10 obs per cluster), but even removing
> those, the warnings remain.
>
> Using Stata -xtmelogit- there are no warnings and the output is almost
> identical to glmer() so this gives me some comfort, yet I still worry
> about these warnings from glmer.
>
> I have tried setting nAGQ as high as 10, to no avail.
>
> Could anyone suggest what I can look for or change ? The data are
> confidential so I can't easily make a reprodicible example.
>
> Thanks in advance
> Robert Long
>
>
>


From bbolker at gmail.com  Wed Apr  2 15:05:16 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 02 Apr 2014 09:05:16 -0400
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <533BE930.90706@gmail.com>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>	<533BE648.4040005@gmail.com>
	<533BE930.90706@gmail.com>
Message-ID: <533C0B0C.4070604@gmail.com>


  I think this is a false positive, caused by our recent introduction of
new convergence tests. There's been lots of discussion of this on the
list recently.

   I have a new trouble-shooting idea:

if g0 is your fitted model, can you see what happens if you scale the
estimated gradients by the curvature/standard errors?

gg <- g0 at optinfo$derivs$grad
hh <- g0 at optinfo$derivs$Hessian
vv <- sqrt(diag(solve(hh/2)))
summary(abs(gg*vv))


On 14-04-02 06:40 AM, W Robert Long wrote:
> I should perhaps also mention that of the 9 covariates, 3 are continous
> and I have tried standardising them. Of the remaining 6, 5 are binary
> and the last one is ordinal.
> 
> On 02/04/2014 11:28, W Robert Long wrote:
>> Hi all
>>
>> I am running a simple random intercepts model using lme4 on
>> approximately 70,000 observations, with 250 clusters. The code looks like
>>
>> glmer(Y~x1+x2+x3+x4+x5+x6+x7+x8+x9+(1|clusdID),
>>      data=dt1, family=binomial(link=logit))
>>
>> and I receive the following warnings:
>>
>> Warning messages:
>> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
>> control$checkConv,  :
>>    Model failed to converge with max|grad| = 4847.75 (tol = 0.001)
>> 2: In if (resHess$code != 0) { :
>>    the condition has length > 1 and only the first element will be used
>> 3: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
>> control$checkConv,  :
>>    Model is nearly unidentifiable: very large eigenvalue
>>   - Rescale variables?;Model is nearly unidentifiable: large eigenvalue
>> ratio
>>   - Rescale variables?
>>
>> There are some small clusters (<10 obs per cluster), but even removing
>> those, the warnings remain.
>>
>> Using Stata -xtmelogit- there are no warnings and the output is almost
>> identical to glmer() so this gives me some comfort, yet I still worry
>> about these warnings from glmer.
>>
>> I have tried setting nAGQ as high as 10, to no avail.
>>
>> Could anyone suggest what I can look for or change ? The data are
>> confidential so I can't easily make a reprodicible example.
>>
>> Thanks in advance
>> Robert Long
>>
>>
>>
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From raluca.gui at gmail.com  Wed Apr  2 17:06:48 2014
From: raluca.gui at gmail.com (Raluca Gui)
Date: Wed, 2 Apr 2014 17:06:48 +0200
Subject: [R-sig-ME] Simulating hierarchical data with correlated errors
Message-ID: <CAEhWoo4612W7k=XoV_B2EwUOTPG_t1xQ6YW-w3WcKq=vvY5ugA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140402/168083bc/attachment.pl>

From longrob604 at gmail.com  Wed Apr  2 21:25:14 2014
From: longrob604 at gmail.com (W Robert Long)
Date: Wed, 02 Apr 2014 20:25:14 +0100
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <533C0B0C.4070604@gmail.com>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>	<533BE648.4040005@gmail.com>	<533BE930.90706@gmail.com>
	<533C0B0C.4070604@gmail.com>
Message-ID: <533C641A.8010406@gmail.com>

Hi Ben

Thanks for your reply. The code you posted generates the following:

  Min.   1st Qu.    Median     Mean   3rd Qu.     Max.
0.001474 0.023920 0.045420 0.255600 0.068600 2.114000

This model was fitted with the raw data (not standardised continuous 
data) and without removing small clusters.

Thanks again
Robert Long




On 02/04/2014 14:05, Ben Bolker wrote:
>
>    I think this is a false positive, caused by our recent introduction of
> new convergence tests. There's been lots of discussion of this on the
> list recently.
>
>     I have a new trouble-shooting idea:
>
> if g0 is your fitted model, can you see what happens if you scale the
> estimated gradients by the curvature/standard errors?
>
> gg <- g0 at optinfo$derivs$grad
> hh <- g0 at optinfo$derivs$Hessian
> vv <- sqrt(diag(solve(hh/2)))
> summary(abs(gg*vv))
>
>
> On 14-04-02 06:40 AM, W Robert Long wrote:
>> I should perhaps also mention that of the 9 covariates, 3 are continous
>> and I have tried standardising them. Of the remaining 6, 5 are binary
>> and the last one is ordinal.
>>
>> On 02/04/2014 11:28, W Robert Long wrote:
>>> Hi all
>>>
>>> I am running a simple random intercepts model using lme4 on
>>> approximately 70,000 observations, with 250 clusters. The code looks like
>>>
>>> glmer(Y~x1+x2+x3+x4+x5+x6+x7+x8+x9+(1|clusdID),
>>>       data=dt1, family=binomial(link=logit))
>>>
>>> and I receive the following warnings:
>>>
>>> Warning messages:
>>> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
>>> control$checkConv,  :
>>>     Model failed to converge with max|grad| = 4847.75 (tol = 0.001)
>>> 2: In if (resHess$code != 0) { :
>>>     the condition has length > 1 and only the first element will be used
>>> 3: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
>>> control$checkConv,  :
>>>     Model is nearly unidentifiable: very large eigenvalue
>>>    - Rescale variables?;Model is nearly unidentifiable: large eigenvalue
>>> ratio
>>>    - Rescale variables?
>>>
>>> There are some small clusters (<10 obs per cluster), but even removing
>>> those, the warnings remain.
>>>
>>> Using Stata -xtmelogit- there are no warnings and the output is almost
>>> identical to glmer() so this gives me some comfort, yet I still worry
>>> about these warnings from glmer.
>>>
>>> I have tried setting nAGQ as high as 10, to no avail.
>>>
>>> Could anyone suggest what I can look for or change ? The data are
>>> confidential so I can't easily make a reprodicible example.
>>>
>>> Thanks in advance
>>> Robert Long
>>>
>>>
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From Thierry.ONKELINX at inbo.be  Thu Apr  3 14:22:56 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Thu, 3 Apr 2014 12:22:56 +0000
Subject: [R-sig-ME] Use of offset variable when analysing rates in lme4
In-Reply-To: <17EB07B22F1D644289C4749D6230666F39B63875@STBEXMB02.stb.sun.ac.za>
References: <17EB07B22F1D644289C4749D6230666F39B63875@STBEXMB02.stb.sun.ac.za>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A21EB3@inbomail.inbo.be>

Dear Caroli,

Your model is
visits ~ Pois(lamba)
log(lambda) = mu = beta_i * treat + beta_j * timecat + log(infl)

which can be rewritten to

log(lambda) - log(infl) = mu - log(infl) = beta_i * treat + beta_j * timecat
log(lambda) - log(infl) = log(lambda / infl) = log(visitation rate)

hence beta_i and beta_j are effects in log(visitation rate). Exponentiating the results from glht should give you the relative effects on the visitation rate.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens De Waal, C, Mej <caroli at sun.ac.za>
Verzonden: woensdag 2 april 2014 11:50
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Use of offset variable when analysing rates in lme4

Dear all,

This is my first post to the list as I have been unsuccessful in getting an answer elsewhere. I am also quite new to R.

I am investigating variation in pollinator visitation rate (number of visits per inflorescence) with treatment and time category as fixed factors. Block is a random factor. Following Zuur et al (2009), I used the number of visits as response variable with the log(number of inflorescences) as offset variable. A poisson model was overdispersed, and therefore I opted for a negative binomial model in lme4, as follows:

model1 = glmer.nb(visits ~ treat + timecat + offset(log(infl)) + (1|block))


I am specifically interested in differences in visitation rates between treatments. I therefore performed a post hoc test:

OPexp1 =  glht(model1,mcp(treat = "Tukey"))
plot(cld(OPexp1))

When I plot these results, I get number of visits on the y axis. But what I want is visitation rate (visits per inflorescence). How do I specifiy that the post hoc test should be performed using visitation rate?

I assume what is happening is that fitted values are currently expressed as ? ? V, but how to I specify that they should be expressed as ? (visits per inflorescence) only? On p 240, Zuur et al (2009) mentions that this is possible, but I have not been able to find an example.

Any advice would be much appreciated.

Kind regards,

Caroli de Waal

PhD Student
University of Stellenbosch, South Africa The integrity and confidentiality of this email is governed by these terms / Hierdie terme bepaal die integriteit en vertroulikheid van hierdie epos. http://www.sun.ac.za/emaildisclaimer

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From mrmetz at ucdavis.edu  Thu Apr  3 20:26:06 2014
From: mrmetz at ucdavis.edu (Margaret Metz)
Date: Thu, 3 Apr 2014 11:26:06 -0700
Subject: [R-sig-ME] lme4 convergence warnings and confirmatory path analysis
	with hierarchical data
Message-ID: <A905209D-789B-4BA6-8A13-F56040D3B739@ucdavis.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140403/c4b292bc/attachment.pl>

From highstat at highstat.com  Thu Apr  3 22:25:03 2014
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Thu, 03 Apr 2014 21:25:03 +0100
Subject: [R-sig-ME] Stats courses in Australia
Message-ID: <533DC39F.8040508@highstat.com>

We are planning to run a series of at least four courses in Australia 
(and/or New Zealand) in August 2014. Potential courses:

1. Data Exploration, Regression, GLM & GAM with introduction to R
2. Introduction to Linear Mixed Effects Models and GLMM with R
3. Zero Inflated Models & GLMM with R
4. Beginner?s Guide to GAM & GAMM with R

For flyers, see:
www.highstat.com/Courses/Flyer2014_08Australia1.pdf
www.highstat.com/Courses/Flyer2014_08Australia2.pdf
www.highstat.com/Courses/Flyer2014_08Australia3.pdf
www.highstat.com/Courses/Flyer2014_08Australia4.pdf

If you would like to participate, or organise a course at your 
institute, please contact us.

Kind regards,

Alain



-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From longrob604 at gmail.com  Fri Apr  4 16:50:27 2014
From: longrob604 at gmail.com (W Robert Long)
Date: Fri, 04 Apr 2014 15:50:27 +0100
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <533C641A.8010406@gmail.com>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>	<533BE648.4040005@gmail.com>	<533BE930.90706@gmail.com>
	<533C0B0C.4070604@gmail.com> <533C641A.8010406@gmail.com>
Message-ID: <533EC6B3.5040706@gmail.com>

Hi Ben

Does the output I posted in my earlier email help ?

Thanks
Rob

On 02/04/2014 20:25, W Robert Long wrote:
> Hi Ben
>
> Thanks for your reply. The code you posted generates the following:
>
>   Min.   1st Qu.    Median     Mean   3rd Qu.     Max.
> 0.001474 0.023920 0.045420 0.255600 0.068600 2.114000
>
> This model was fitted with the raw data (not standardised continuous
> data) and without removing small clusters.
>
> Thanks again
> Robert Long
>
>
>
>
> On 02/04/2014 14:05, Ben Bolker wrote:
>>
>>    I think this is a false positive, caused by our recent introduction of
>> new convergence tests. There's been lots of discussion of this on the
>> list recently.
>>
>>     I have a new trouble-shooting idea:
>>
>> if g0 is your fitted model, can you see what happens if you scale the
>> estimated gradients by the curvature/standard errors?
>>
>> gg <- g0 at optinfo$derivs$grad
>> hh <- g0 at optinfo$derivs$Hessian
>> vv <- sqrt(diag(solve(hh/2)))
>> summary(abs(gg*vv))
>>
>>
>> On 14-04-02 06:40 AM, W Robert Long wrote:
>>> I should perhaps also mention that of the 9 covariates, 3 are continous
>>> and I have tried standardising them. Of the remaining 6, 5 are binary
>>> and the last one is ordinal.
>>>
>>> On 02/04/2014 11:28, W Robert Long wrote:
>>>> Hi all
>>>>
>>>> I am running a simple random intercepts model using lme4 on
>>>> approximately 70,000 observations, with 250 clusters. The code looks
>>>> like
>>>>
>>>> glmer(Y~x1+x2+x3+x4+x5+x6+x7+x8+x9+(1|clusdID),
>>>>       data=dt1, family=binomial(link=logit))
>>>>
>>>> and I receive the following warnings:
>>>>
>>>> Warning messages:
>>>> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
>>>> control$checkConv,  :
>>>>     Model failed to converge with max|grad| = 4847.75 (tol = 0.001)
>>>> 2: In if (resHess$code != 0) { :
>>>>     the condition has length > 1 and only the first element will be
>>>> used
>>>> 3: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
>>>> control$checkConv,  :
>>>>     Model is nearly unidentifiable: very large eigenvalue
>>>>    - Rescale variables?;Model is nearly unidentifiable: large
>>>> eigenvalue
>>>> ratio
>>>>    - Rescale variables?
>>>>
>>>> There are some small clusters (<10 obs per cluster), but even removing
>>>> those, the warnings remain.
>>>>
>>>> Using Stata -xtmelogit- there are no warnings and the output is almost
>>>> identical to glmer() so this gives me some comfort, yet I still worry
>>>> about these warnings from glmer.
>>>>
>>>> I have tried setting nAGQ as high as 10, to no avail.
>>>>
>>>> Could anyone suggest what I can look for or change ? The data are
>>>> confidential so I can't easily make a reprodicible example.
>>>>
>>>> Thanks in advance
>>>> Robert Long
>>>>
>>>>
>>>>
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>


From jkhartshorne at gmail.com  Sun Apr  6 05:44:25 2014
From: jkhartshorne at gmail.com (Joshua Hartshorne)
Date: Sat, 5 Apr 2014 23:44:25 -0400
Subject: [R-sig-ME] interpreting interactions
Message-ID: <CA+3amhdhCX443uG6ADfdgmdw+=DroWc_bGFGiAEn81b5VyCreQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140405/6783eefa/attachment.pl>

From tibor at linguistics.rub.de  Sun Apr  6 10:31:22 2014
From: tibor at linguistics.rub.de (Tibor Kiss)
Date: Sun, 6 Apr 2014 10:31:22 +0200
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <533EC6B3.5040706@gmail.com>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>	<533BE648.4040005@gmail.com>	<533BE930.90706@gmail.com>
	<533C0B0C.4070604@gmail.com> <533C641A.8010406@gmail.com>
	<533EC6B3.5040706@gmail.com>
Message-ID: <2F10D5AB-AE1A-457A-AC60-EA428D2ABB5C@linguistics.rub.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140406/c5fd764f/attachment.pl>

From bbolker at gmail.com  Sun Apr  6 16:30:32 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 06 Apr 2014 10:30:32 -0400
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <2F10D5AB-AE1A-457A-AC60-EA428D2ABB5C@linguistics.rub.de>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>	<533BE648.4040005@gmail.com>	<533BE930.90706@gmail.com>	<533C0B0C.4070604@gmail.com>
	<533C641A.8010406@gmail.com>	<533EC6B3.5040706@gmail.com>
	<2F10D5AB-AE1A-457A-AC60-EA428D2ABB5C@linguistics.rub.de>
Message-ID: <53416508.8090603@gmail.com>

On 14-04-06 04:31 AM, Tibor Kiss wrote:
> Hi,
> 
> being somewhat nonplussed by similar messages, I also applied Ben's recent suggestion to one of my models to get:
> 
>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
> 1.343e-05 3.530e-05 5.756e-05 7.631e-05 9.841e-05 1.932e-04 
> 
> So following up on Rob's message: What does it mean?
> 
> With kind regards
> 
> Tibor
> 

  It means that on the scale of the _standard deviations_ of the
parameters, the estimated gradients at the MLE (or restricted MLE) are
not large.  I was surprised in Rob's case that these scaled gradients
were not that small; much smaller than without the scaling, but not
small enough to make me think  really understand what's going on.

  To recapitulate: the appearance of all of these new messages in the
latest version of lme4 is **not** due to a degradation or change in the
optimization or fitting procedure -- it's due to a new set of
convergence tests that we implemented, that we think are giving a lot of
false positives.  You can easily shut them off yourself, or raise the
tolerance for the warnings (see ?lmerControl/?glmerControl).  As
developers, we're a bit stuck now because we don't want to turn the
warnings _off_ until we understand the circumstances that are triggering
them better, and that takes more time and effort than we've been able to
muster so far.

   cheers
     Ben Bolker


> 
> 
>  
> 
> Am 04.04.2014 um 16:50 schrieb W Robert Long <longrob604 at gmail.com>:
> 
>> Hi Ben
>>
>> Does the output I posted in my earlier email help ?
>>
>> Thanks
>> Rob
>>
>> On 02/04/2014 20:25, W Robert Long wrote:
>>> Hi Ben
>>>
>>> Thanks for your reply. The code you posted generates the following:
>>>
>>>  Min.   1st Qu.    Median     Mean   3rd Qu.     Max.
>>> 0.001474 0.023920 0.045420 0.255600 0.068600 2.114000
>>>
>>> This model was fitted with the raw data (not standardised continuous
>>> data) and without removing small clusters.
>>>
>>> Thanks again
>>> Robert Long
>>>
>>>
>>>
>>>
>>> On 02/04/2014 14:05, Ben Bolker wrote:
>>>>
>>>>   I think this is a false positive, caused by our recent introduction of
>>>> new convergence tests. There's been lots of discussion of this on the
>>>> list recently.
>>>>
>>>>    I have a new trouble-shooting idea:
>>>>
>>>> if g0 is your fitted model, can you see what happens if you scale the
>>>> estimated gradients by the curvature/standard errors?
>>>>
>>>> gg <- g0 at optinfo$derivs$grad
>>>> hh <- g0 at optinfo$derivs$Hessian
>>>> vv <- sqrt(diag(solve(hh/2)))
>>>> summary(abs(gg*vv))
>>>>
>>>>
>>>> On 14-04-02 06:40 AM, W Robert Long wrote:
>>>>> I should perhaps also mention that of the 9 covariates, 3 are continous
>>>>> and I have tried standardising them. Of the remaining 6, 5 are binary
>>>>> and the last one is ordinal.
>>>>>
>>>>> On 02/04/2014 11:28, W Robert Long wrote:
>>>>>> Hi all
>>>>>>
>>>>>> I am running a simple random intercepts model using lme4 on
>>>>>> approximately 70,000 observations, with 250 clusters. The code looks
>>>>>> like
>>>>>>
>>>>>> glmer(Y~x1+x2+x3+x4+x5+x6+x7+x8+x9+(1|clusdID),
>>>>>>      data=dt1, family=binomial(link=logit))
>>>>>>
>>>>>> and I receive the following warnings:
>>>>>>
>>>>>> Warning messages:
>>>>>> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
>>>>>> control$checkConv,  :
>>>>>>    Model failed to converge with max|grad| = 4847.75 (tol = 0.001)
>>>>>> 2: In if (resHess$code != 0) { :
>>>>>>    the condition has length > 1 and only the first element will be
>>>>>> used
>>>>>> 3: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
>>>>>> control$checkConv,  :
>>>>>>    Model is nearly unidentifiable: very large eigenvalue
>>>>>>   - Rescale variables?;Model is nearly unidentifiable: large
>>>>>> eigenvalue
>>>>>> ratio
>>>>>>   - Rescale variables?
>>>>>>
>>>>>> There are some small clusters (<10 obs per cluster), but even removing
>>>>>> those, the warnings remain.
>>>>>>
>>>>>> Using Stata -xtmelogit- there are no warnings and the output is almost
>>>>>> identical to glmer() so this gives me some comfort, yet I still worry
>>>>>> about these warnings from glmer.
>>>>>>
>>>>>> I have tried setting nAGQ as high as 10, to no avail.
>>>>>>
>>>>>> Could anyone suggest what I can look for or change ? The data are
>>>>>> confidential so I can't easily make a reprodicible example.
>>>>>>
>>>>>> Thanks in advance
>>>>>> Robert Long
>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From mrahmankufmrt at gmail.com  Mon Apr  7 07:54:30 2014
From: mrahmankufmrt at gmail.com (Moshiur Rahman)
Date: Mon, 7 Apr 2014 13:54:30 +0800
Subject: [R-sig-ME] glmer output
Message-ID: <CAGNSkSnzei79yuYT3vRVPnvwp6Enh0sg5EQA4XfZ8t_jkSZhvw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140407/c25817f6/attachment.pl>

From robert.krause at student.ru.nl  Mon Apr  7 10:34:17 2014
From: robert.krause at student.ru.nl (Krause, R.W. (Robert))
Date: Mon, 7 Apr 2014 10:34:17 +0200 (CEST)
Subject: [R-sig-ME] glmer output
In-Reply-To: <CAGNSkSnzei79yuYT3vRVPnvwp6Enh0sg5EQA4XfZ8t_jkSZhvw@mail.gmail.com>
Message-ID: <1317244873.6237753.1396859657958.JavaMail.root@sculptor.zimbra.ru.nl>

Hello Moshi,

I want to give you an advice for your analysis and tried answer your questions. But have in mind: I am not a mixed models expert and just a student.
I suggest you read the paper by Barr et al. (2014) and change your model.
http://idiom.ucsd.edu/~rlevy/papers/barr-etal-2013-jml.pdf

You did not include your fixed effect as a random effect, which I think is highly recommended after reading Barr. Probably this makes no sense because you only have one obersvation per Family or there is no "random slope" in the population, however, Barr et al. state: "In terms of power, maximal models perform surprisingly well even in a ??worst case?? scenario where they assume random slope variation that is actually not present in the population."

Somehing like:
y ~ treat + (1 + treat| Family) + (1 | obs)

I do not know your data and experiment well enough and I hope that some of the experts will help with that.

I did not really get your question. When you use dummi coding,as it seems, you see in your first table (treatLH      -1.1329     0.3004  -3.772 0.000162 ***) that "treat" is a significant predictor. As you said there was no other level "(treat which has two levels - High and Low)", if you had a null treatment (no diet?), you could have had contrasts showing the effect of high and low treat vs null-treat.
As you can see in your second table, the over all effect of treat is 0.566 in one or the other direction, depending on your group.
If you just want the significance of including or excluding treat, you should use ML testing. Run the model:
y ~ 1 + (1 + treat| Family) + (1 | obs)
and compare it with
y ~ treat + (1 + treat| Family) + (1 | obs)



For the reporting, I do not know the biology guidelines, but unfortunately psychology is so slow in adaptation that we have none for mixed models, however you can say something like this:

We used a Generalized Linear Mixed Models for binomially distributed outcomes with glmer() function of the lme4 (cite lme4) package in R (cite R). The model included a fixed intercept and a fixed slope for treat, (here again, check if you do not want to use sum-to-zero contrast, I think your intercept will disappear than... options(contrasts=c("contr.sum"))).(If you change your model like Barr recommends, you can go on as follows - I saw that in a published paper:) We followed Barr, Levy, Scheepers, and Tily's(2013) advice to use a maximal random-effects structure: - And now you list the random effects, do not forget the random correlation terms amog the random effects, by now you do not have any, because you only use random intercept models but after adding a slope, the correlation between slope and intercept is also part of the model. If you run into no-convergence now and changing optimizers and increasing iterations do not help, I suggest you split your random effect (1|group) + (0+slope|group) and the correlation is excluded, but you should report that! If you use optimzers, then state which you used and you could also add the number of iterations, everything someone needs to replicate your findings.
You should write the model into your paper so everybody can see directly what you did: y ~ treat + (1 + treat| Family) + (1 | obs)

Lastly, I recommend you use bootstrapping to determine p-values with the package pbkrtest and the bootmer function. It might take some time but is, as far as I know, the "best" (reliable and valid) way to get p-values. And it does not have the theoretical issue of how many parameters and dfs your model really has...


The rest could be written as usual.

Again note: I am just a student, so if one of the masters out there sees something wrong in my explanations, please commend.

Kind regards,

Robert

--

Robert Krause
Student, Master of Bahvioural Science
Radboud University Nijmegen
robert.krause at student.ru.nl

----- Oorspronkelijk bericht -----
> Van: "Moshiur Rahman" <mrahmankufmrt at gmail.com>
> Aan: r-sig-mixed-models at r-project.org, r-sig-mixed-models-request at r-project.org,
> r-sig-mixed-models-owner at r-project.org
> Verzonden: Maandag 7 april 2014 07:54:30
> Onderwerp: [R-sig-ME] glmer output
> Dear mixed model experts,
> 
> 
> 
> I used the generalized linear models (GLM) to explore the dietary
> effects
> entering the number of offspring sired by both groups as the response
> variable (y), dietary treatment as a fixed effect (treat which has two
> levels - High and Low), family as a random effect with a binomial
> error
> distribution and a logit link function. The model was weighted by
> total
> number of offspring, and observation-level was included in the full
> model
> as a random effect to approximate overdispersion which reduced the
> dispersion parameter from 3.39 to 1.02. The output is below:
> 
> 
> 
> y=cbind(Data$Success,Data$Failure)
> 
> 
> Generalized linear mixed model fit by the Laplace approximation
> 
> Formula: y ~ treat + (1 | Family) + (1 | obs)
> 
> Data: Data
> 
> AIC BIC logLik deviance
> 
> 198.6 206.4 -95.28 190.6
> 
> Random effects:
> 
> Groups Name Variance Std.Dev.
> 
> obs (Intercept) 1.0383 1.019
> 
> Family (Intercept) 0.0000 0.000
> 
> Number of obs: 52, groups: obs, 52; Family, 26
> 
> 
> 
> Fixed effects:
> 
> Estimate Std. Error z value Pr(>|z|)
> 
> (Intercept) 0.5664 0.2124 2.667 0.007658 **
> 
> treatLH -1.1329 0.3004 -3.772 0.000162 ***
> 
> ---
> 
> Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> 
> 
> Correlation of Fixed Effects:
> 
> (Intr)
> 
> treatLH -0.707
> 
> 
> 
> The result gives only the effect of Low dietary group (treatLH). I
> know
> also how to see the effect of High group excluding intercept with the
> following formula:
> 
> Fixed effects:
> 
> Estimate Std. Error z value Pr(>|z|)
> 
> treatHH 0.5664 0.2124 2.667 0.00766 **
> 
> treatLH -0.5665 0.2124 -2.667 0.00765 **
> 
> 
> 
> I also know some experts suggest for likelihood-ratio test. But first
> of
> all, I'd like to see the effects of treat on y (not individual effect
> of
> High or Low group) like ANOVA.
> 
> 
> 
> Can anyone suggest how to get it? If it's not possible, how can I
> report
> the existing results for my thesis?
> 
> 
> 
> Any advice will be highly appreciated.
> 
> 
> 
> Cheers,
> 
> 
> 
> Moshi
> 
> --
> MD. MOSHIUR RAHMAN
> PhD Candidate
> School of Animal Biology/Zoology (M092)
> University of Western Australia
> 35 Stirling Hwy, Crawley, WA, 6009
> Australia.
> Mob.: 061-425205507
> 
> [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Tue Apr  8 02:50:55 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 8 Apr 2014 00:50:55 +0000 (UTC)
Subject: [R-sig-ME] interpreting interactions
References: <CA+3amhdhCX443uG6ADfdgmdw+=DroWc_bGFGiAEn81b5VyCreQ@mail.gmail.com>
Message-ID: <loom.20140408T022941-267@post.gmane.org>

Joshua Hartshorne <jkhartshorne at ...> writes:

> 
> A colleague recently made the argument that interaction terms in logistic
> regression are uninterpretable, citing Ai & Norton (2003)
>  Interaction terms
> in logit and probit models. On reading the paper, it seems to make the
> weaker claim that interaction terms of continuous predictors may be
> calculated incorrectly in 2003-era STATA, and that one should take care to
> calculate them correctly.
> 
> But this did make me wonder whether there are any issues in interpreting
> interpreting interaction terms for 'binomial' models in lmer. Can anyone
> comment?
> 
> Josh

  This topic was new to me.  As far as I can tell from my reading of
the paper, it's extremely important to make the distinction between
interaction _terms_ and interaction _effects_.  Again as far as I can
tell, the interaction _terms_ correspond exactly to the estimated
coefficients, and are relevant on the scale of the linear predictor
(where everything is indeed linear).  The interaction _effects_,
in contrast, seem to be defined on the response scale. Because there
is a nonlinear transformation between these scales, there is
not necessarily an intuitive correspondence between expected 
differences-in-difference (cross derivatives) on the linear predictor
scale (terms) and the response scale (effects).

  Not being an applied econometrician, I don't really understand why
one would want to do a statistical test of an interaction _effect_
rather than an interaction _term_.  To me it makes most sense to
do statistical tests on the scale of the linear predictor where
everything is linear and (relatively) simple ...

  As far as how this applies to GLMMs; I don't know, but
there is an additional level of variation and/or averaging that may raise
issues depending on whether you're trying to understand 
population-level, conditional, or marginal effects ...


From alice.jones01 at adelaide.edu.au  Tue Apr  8 10:23:12 2014
From: alice.jones01 at adelaide.edu.au (Alice Jones)
Date: Tue, 8 Apr 2014 08:23:12 +0000
Subject: [R-sig-ME] Query on predicting from GLMM to new data with a
 different range of values of the random term
Message-ID: <75B121F67C4F6B4789048793840E1067110FBBF0@MAILMB04.ad.adelaide.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140408/e6025db1/attachment.pl>

From Thierry.ONKELINX at inbo.be  Tue Apr  8 13:48:54 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 8 Apr 2014 11:48:54 +0000
Subject: [R-sig-ME] Query on predicting from GLMM to new data with a
 different range of values of the random term
In-Reply-To: <75B121F67C4F6B4789048793840E1067110FBBF0@MAILMB04.ad.adelaide.edu.au>
References: <75B121F67C4F6B4789048793840E1067110FBBF0@MAILMB04.ad.adelaide.edu.au>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A266DD@inbomail.inbo.be>

Dear Alice,

Have a look at the allow.new.levels argument of predict.merMod(). Read the helpfile carefully to know what this option does (and doesn't).

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Alice Jones
Verzonden: dinsdag 8 april 2014 10:23
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Query on predicting from GLMM to new data with a different range of values of the random term

Hi All,

I am fitting a Gaussian lmer model to a set of training data with a number of fixed effects and 'year' as a random effect.

I want to use the model that I have fit to the training data (which spans the period 1982 - 2001) to predict to another data set which covers the period 2001 - 2011.  I am doing this as a form of external model validation (how good is my model at predicted to a different dataset that does not span the same temporal extent as the training data?).  However I am getting an error message: "Error in (function (x, n)  : new levels detected in newdata".  This has caused me to question whether it is even possible to use an lmer model to predict to data with values outside the range of the random effect in the training data?  I assume this must be possible, because many studies have used mixed effects models to predict into the future........

I have checked that the levels of all the fixed effect factors are the same between the two datasets (training and validation).  Additionally I have tried specifying the random effect (year) as.numeric and as.integer as opposed to as.factor (in order to work out of this was a factor-specific problem), but this has made no difference.

Any advice on this would be much appreciated.
Thanks,
Alice


Dr Alice Jones
ARC Research Associate
Global Ecology Lab &
School of Earth and Environmental Sciences

G44, Mawson Laboratory,
North Terrace Campus,
The University of Adelaide,
Australia 5005

alice.jones01 at adelaide.edu.au<mailto:alice.jones01 at adelaide.edu.au>
Web profile<http://www.adelaide.edu.au/directory/alice.jones01>
Ph:  +61 8 8313 2243


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From baron at psych.upenn.edu  Tue Apr  8 16:00:23 2014
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 8 Apr 2014 10:00:23 -0400
Subject: [R-sig-ME] interpreting interactions
In-Reply-To: <loom.20140408T022941-267@post.gmane.org>
References: <CA+3amhdhCX443uG6ADfdgmdw+=DroWc_bGFGiAEn81b5VyCreQ@mail.gmail.com>
	<loom.20140408T022941-267@post.gmane.org>
Message-ID: <20140408140023.GA11637@psych.upenn.edu>

REMOVE ME

An additional problem with interactions is described in this excellent
paper, which is about "removable" interactions, i.e., those that can
be removed by a transformation of the dependent variable.

http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3267935/

I don't know about econometrics either, but in psychology this is a
huge problem because most of the dependent variables are not
necessarily linear functions of the underlying variable that they are
trying to measure.

I tried to read the recommended paper, but I did not get far enough to
write the kind of very helpful summary that is below. From that, it
sounds like it is about a special kind of removable interaction.



On 04/08/14 00:50, Ben Bolker wrote:
> Joshua Hartshorne <jkhartshorne at ...> writes:
> 
> > 
> > A colleague recently made the argument that interaction terms in logistic
> > regression are uninterpretable, citing Ai & Norton (2003)
> >  Interaction terms
> > in logit and probit models. On reading the paper, it seems to make the
> > weaker claim that interaction terms of continuous predictors may be
> > calculated incorrectly in 2003-era STATA, and that one should take care to
> > calculate them correctly.
> > 
> > But this did make me wonder whether there are any issues in interpreting
> > interpreting interaction terms for 'binomial' models in lmer. Can anyone
> > comment?
> > 
> > Josh
> 
>   This topic was new to me.  As far as I can tell from my reading of
> the paper, it's extremely important to make the distinction between
> interaction _terms_ and interaction _effects_.  Again as far as I can
> tell, the interaction _terms_ correspond exactly to the estimated
> coefficients, and are relevant on the scale of the linear predictor
> (where everything is indeed linear).  The interaction _effects_,
> in contrast, seem to be defined on the response scale. Because there
> is a nonlinear transformation between these scales, there is
> not necessarily an intuitive correspondence between expected 
> differences-in-difference (cross derivatives) on the linear predictor
> scale (terms) and the response scale (effects).
> 
>   Not being an applied econometrician, I don't really understand why
> one would want to do a statistical test of an interaction _effect_
> rather than an interaction _term_.  To me it makes most sense to
> do statistical tests on the scale of the linear predictor where
> everything is linear and (relatively) simple ...
> 
>   As far as how this applies to GLMMs; I don't know, but
> there is an additional level of variation and/or averaging that may raise
> issues depending on whether you're trying to understand 
> population-level, conditional, or marginal effects ...
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From baron at psych.upenn.edu  Tue Apr  8 16:09:27 2014
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 8 Apr 2014 10:09:27 -0400
Subject: [R-sig-ME] interpreting interactions
In-Reply-To: <20140408140023.GA11637@psych.upenn.edu>
References: <CA+3amhdhCX443uG6ADfdgmdw+=DroWc_bGFGiAEn81b5VyCreQ@mail.gmail.com>
	<loom.20140408T022941-267@post.gmane.org>
	<20140408140023.GA11637@psych.upenn.edu>
Message-ID: <20140408140927.GA26855@psych.upenn.edu>

THIS WAS A MISTAKE! SORRY!

(The message is not finished.)

Jon

On 04/08/14 10:00, Jonathan Baron wrote:
> REMOVE ME
> 
> An additional problem with interactions is described in this excellent
> paper, which is about "removable" interactions, i.e., those that can
> be removed by a transformation of the dependent variable.
> 
> http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3267935/
> 
> I don't know about econometrics either, but in psychology this is a
> huge problem because most of the dependent variables are not
> necessarily linear functions of the underlying variable that they are
> trying to measure.
> 
> I tried to read the recommended paper, but I did not get far enough to
> write the kind of very helpful summary that is below. From that, it
> sounds like it is about a special kind of removable interaction.
> 
> 
> 
> On 04/08/14 00:50, Ben Bolker wrote:
> > Joshua Hartshorne <jkhartshorne at ...> writes:
> > 
> > > 
> > > A colleague recently made the argument that interaction terms in logistic
> > > regression are uninterpretable, citing Ai & Norton (2003)
> > >  Interaction terms
> > > in logit and probit models. On reading the paper, it seems to make the
> > > weaker claim that interaction terms of continuous predictors may be
> > > calculated incorrectly in 2003-era STATA, and that one should take care to
> > > calculate them correctly.
> > > 
> > > But this did make me wonder whether there are any issues in interpreting
> > > interpreting interaction terms for 'binomial' models in lmer. Can anyone
> > > comment?
> > > 
> > > Josh
> > 
> >   This topic was new to me.  As far as I can tell from my reading of
> > the paper, it's extremely important to make the distinction between
> > interaction _terms_ and interaction _effects_.  Again as far as I can
> > tell, the interaction _terms_ correspond exactly to the estimated
> > coefficients, and are relevant on the scale of the linear predictor
> > (where everything is indeed linear).  The interaction _effects_,
> > in contrast, seem to be defined on the response scale. Because there
> > is a nonlinear transformation between these scales, there is
> > not necessarily an intuitive correspondence between expected 
> > differences-in-difference (cross derivatives) on the linear predictor
> > scale (terms) and the response scale (effects).
> > 
> >   Not being an applied econometrician, I don't really understand why
> > one would want to do a statistical test of an interaction _effect_
> > rather than an interaction _term_.  To me it makes most sense to
> > do statistical tests on the scale of the linear predictor where
> > everything is linear and (relatively) simple ...
> > 
> >   As far as how this applies to GLMMs; I don't know, but
> > there is an additional level of variation and/or averaging that may raise
> > issues depending on whether you're trying to understand 
> > population-level, conditional, or marginal effects ...
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> -- 
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron
> Editor: Judgment and Decision Making (http://journal.sjdm.org)
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From baron at psych.upenn.edu  Tue Apr  8 16:20:07 2014
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 8 Apr 2014 10:20:07 -0400
Subject: [R-sig-ME] interpreting interactions
In-Reply-To: <20140408140927.GA26855@psych.upenn.edu>
References: <CA+3amhdhCX443uG6ADfdgmdw+=DroWc_bGFGiAEn81b5VyCreQ@mail.gmail.com>
	<loom.20140408T022941-267@post.gmane.org>
	<20140408140023.GA11637@psych.upenn.edu>
	<20140408140927.GA26855@psych.upenn.edu>
Message-ID: <20140408142007.GA31085@psych.upenn.edu>

Again, sorry for this. I thought I was replying to the third spam from
the same person and was getting annoyed. In fact I was in my
"postponed mail". I postponed sending my comment because I wanted to
look at the Ai/Norton paper.

I have now looked at the Ai/Norton paper again, and it is NOT the same
issue as described in the Wagenmaker et al. paper that I cited. In the
Ai/Norton paper, probability of participation is what is truly of
interest. And the problem raised by Ai and Norton does not have to do
with interactions that are removable by transforming the dependent
variable.

However, I still think that the Wagenmaker et al. paper should be
required reading for psychologists.

Jon

On 04/08/14 10:09, Jonathan Baron wrote:
> THIS WAS A MISTAKE! SORRY!
> 
> (The message is not finished.)
> 
> Jon
> 
> On 04/08/14 10:00, Jonathan Baron wrote:
> > REMOVE ME
> > 
> > An additional problem with interactions is described in this excellent
> > paper, which is about "removable" interactions, i.e., those that can
> > be removed by a transformation of the dependent variable.
> > 
> > http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3267935/
> > 
> > I don't know about econometrics either, but in psychology this is a
> > huge problem because most of the dependent variables are not
> > necessarily linear functions of the underlying variable that they are
> > trying to measure.
> > 
> > I tried to read the recommended paper, but I did not get far enough to
> > write the kind of very helpful summary that is below. From that, it
> > sounds like it is about a special kind of removable interaction.
> > 
> > 
> > 
> > On 04/08/14 00:50, Ben Bolker wrote:
> > > Joshua Hartshorne <jkhartshorne at ...> writes:
> > > 
> > > > 
> > > > A colleague recently made the argument that interaction terms in logistic
> > > > regression are uninterpretable, citing Ai & Norton (2003)
> > > >  Interaction terms
> > > > in logit and probit models. On reading the paper, it seems to make the
> > > > weaker claim that interaction terms of continuous predictors may be
> > > > calculated incorrectly in 2003-era STATA, and that one should take care to
> > > > calculate them correctly.
> > > > 
> > > > But this did make me wonder whether there are any issues in interpreting
> > > > interpreting interaction terms for 'binomial' models in lmer. Can anyone
> > > > comment?
> > > > 
> > > > Josh
> > > 
> > >   This topic was new to me.  As far as I can tell from my reading of
> > > the paper, it's extremely important to make the distinction between
> > > interaction _terms_ and interaction _effects_.  Again as far as I can
> > > tell, the interaction _terms_ correspond exactly to the estimated
> > > coefficients, and are relevant on the scale of the linear predictor
> > > (where everything is indeed linear).  The interaction _effects_,
> > > in contrast, seem to be defined on the response scale. Because there
> > > is a nonlinear transformation between these scales, there is
> > > not necessarily an intuitive correspondence between expected 
> > > differences-in-difference (cross derivatives) on the linear predictor
> > > scale (terms) and the response scale (effects).
> > > 
> > >   Not being an applied econometrician, I don't really understand why
> > > one would want to do a statistical test of an interaction _effect_
> > > rather than an interaction _term_.  To me it makes most sense to
> > > do statistical tests on the scale of the linear predictor where
> > > everything is linear and (relatively) simple ...
> > > 
> > >   As far as how this applies to GLMMs; I don't know, but
> > > there is an additional level of variation and/or averaging that may raise
> > > issues depending on whether you're trying to understand 
> > > population-level, conditional, or marginal effects ...
> > > 
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > 
> > -- 
> > Jonathan Baron, Professor of Psychology, University of Pennsylvania
> > Home page: http://www.sas.upenn.edu/~baron
> > Editor: Judgment and Decision Making (http://journal.sjdm.org)
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> -- 
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron
> Editor: Judgment and Decision Making (http://journal.sjdm.org)
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From maxwe128 at umn.edu  Tue Apr  8 20:39:25 2014
From: maxwe128 at umn.edu (Maxwell Elliott)
Date: Tue, 8 Apr 2014 13:39:25 -0500
Subject: [R-sig-ME] Including Nested Factor in Repeated Measures
	Logistic Regression
In-Reply-To: <CAGmtytuMVBo9XfzzUsBGr-b9t6=x8tb1LriGBHoJe8q2+VnqJA@mail.gmail.com>
References: <CAGmtytuMVBo9XfzzUsBGr-b9t6=x8tb1LriGBHoJe8q2+VnqJA@mail.gmail.com>
Message-ID: <CAGmtyttK2uH2J3mUQYpF=+T2sCTQRuRLOjpBgsZVn3owzTkb_A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140408/93217780/attachment.pl>

From segerfan83 at gmail.com  Tue Apr  8 23:01:07 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Tue, 8 Apr 2014 17:01:07 -0400
Subject: [R-sig-ME] Logarithmic Mixed Models?
Message-ID: <CAHe08Sh71Yp9CQX=ko2O96Yh0=V9K7RdWf4jG4UTRcvZ06HEXQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140408/30709a1f/attachment.pl>

From i.m.s.white at ed.ac.uk  Tue Apr  8 23:26:33 2014
From: i.m.s.white at ed.ac.uk (ian m s white)
Date: Tue, 8 Apr 2014 22:26:33 +0100
Subject: [R-sig-ME] Including Nested Factor in Repeated Measures
	Logistic Regression
In-Reply-To: <CAGmtyttK2uH2J3mUQYpF=+T2sCTQRuRLOjpBgsZVn3owzTkb_A@mail.gmail.com>
References: <CAGmtytuMVBo9XfzzUsBGr-b9t6=x8tb1LriGBHoJe8q2+VnqJA@mail.gmail.com>
	<CAGmtyttK2uH2J3mUQYpF=+T2sCTQRuRLOjpBgsZVn3owzTkb_A@mail.gmail.com>
Message-ID: <85D3E337-3E39-4542-A819-12BC8E20ACE6@ed.ac.uk>

How many trial types are there? How many blocks? Does each subject do each combination of trial type
and block several times?

On 8 Apr 2014, at 19:39, Maxwell Elliott <maxwe128 at umn.edu> wrote:

> I have still been unable to solve this issue. Any help related to dealing
> with nested variables in lmer regression would be uch appreciated.
> 
> -Max Elliott
> 
> 
> On Tue, Apr 1, 2014 at 2:14 PM, Maxwell Elliott <maxwe128 at umn.edu> wrote:
> 
>> Hello,
>> 
>> I am attempting to model accuracy in a task that contains multiple trial
>> types, multiple blocks and many subjects. It is a repeated measure design
>> because each subject completes 160 trials. My current formula for the
>> regression is this:
>> 
>> Accuracy ~ TrialType * Block + (1 | subjectNumber)
>> 
>> I am now trying to include a symptom measure in the design. This measure
>> will be the one continuous value for each subject. So the symptom measure
>> is nested within subject. What is the syntax for including this symptom
>> measure in the design. I want to know if the symptom measure contributes to
>> model fit and if higher values increase or decrease the likelihood of
>> answering the the trial correctly. I know I cannot do this:
>> Accuracy ~ TrialType * Block * Symptom + (1 | subjectNumber)
>> , because symptom is nested within subject but I am not sure what else to
>> do.
>> 
>> Thanks for the help,
>> 
>> Max Elliott
>> 
> 
> 
> 
> -- 
> 
> -Max
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From maxwe128 at umn.edu  Wed Apr  9 02:53:42 2014
From: maxwe128 at umn.edu (Maxwell Elliott)
Date: Tue, 8 Apr 2014 19:53:42 -0500
Subject: [R-sig-ME] Including Nested Factor in Repeated Measures
	Logistic Regression
In-Reply-To: <85D3E337-3E39-4542-A819-12BC8E20ACE6@ed.ac.uk>
References: <CAGmtytuMVBo9XfzzUsBGr-b9t6=x8tb1LriGBHoJe8q2+VnqJA@mail.gmail.com>
	<CAGmtyttK2uH2J3mUQYpF=+T2sCTQRuRLOjpBgsZVn3owzTkb_A@mail.gmail.com>
	<85D3E337-3E39-4542-A819-12BC8E20ACE6@ed.ac.uk>
Message-ID: <CAGmtytvG1NPvXHU5kjqjU2SQVrRXVM-w=0wV4YTjhLfT-Z+d=g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140408/050dcad3/attachment.pl>

From bbolker at gmail.com  Wed Apr  9 03:00:42 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 9 Apr 2014 01:00:42 +0000 (UTC)
Subject: [R-sig-ME] Logarithmic Mixed Models?
References: <CAHe08Sh71Yp9CQX=ko2O96Yh0=V9K7RdWf4jG4UTRcvZ06HEXQ@mail.gmail.com>
Message-ID: <loom.20140409T025520-725@post.gmane.org>

AvianResearchDivision <segerfan83 at ...> writes:

> 
> Hi all,
> 
> I am working with ambient noise level as an environmental predictor
> variable and am wondering if using linear mixed models for this sort of
> data would be incorrect.  It seems since sound pressure level (dB) is on a
> log scale, that performing mixed models on a non-linear scale or
> transforming the response variable might be the correct way to proceed.
> Any thoughts on this?  Anyone have any experience?
> 
> Best,
> Jacob
> 

  I would say that there is no _a priori_ way to define the correct
scale on which to do a statistical analysis (this relates to a parallel
discussion on this list about the dependence of the presence, and
interpretation, of interactions on what scale you are thinking about).

You said that noise level is a predictor variable, but then you
talk about transforming the response variable, so I'm a bit confused.

* If your _response_ variable is on a scale that works well (responses
are linear functions of the predictors, 
distribution of residuals is Normal and heteroscedastic,
interactions might be minimized, variable is interpretable in some
sensible way) then you should just go for it.
* If you are worried about the scaling of a _predictor_ variable,
then you have even less to worry about since the distributional
issues are not important (the theory of linear models doesn't make
any assumptions about the distributions of predictor variables).

  Ben Bolker


From bbolker at gmail.com  Wed Apr  9 04:07:58 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 9 Apr 2014 02:07:58 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?lme4_convergence_warnings_and_confirmatory_p?=
	=?utf-8?q?ath_analysis=09with_hierarchical_data?=
References: <A905209D-789B-4BA6-8A13-F56040D3B739@ucdavis.edu>
Message-ID: <loom.20140409T030109-548@post.gmane.org>

Margaret Metz <mrmetz at ...> writes:

> 
> Hello all -- 
 
> I am attempting a confirmatory path analysis that uses lme4 for
> linear mixed effects models with hierarchically structured data
> (following methods in Shipley 2009 Ecology --
> http://www.esajournals.org/doi/pdf/10.1890/08-1034.1). The data are
> about tree survival with predictors measured at the level of the
> individual tree or at the level of the plot within which the trees
> are found.
 
> My models are triggering errors and warnings that have been the
> subject of quite a few recent posts on this list.  I have tried some
> of the checks that Ben Bolker has generously suggested, but I do not
> know how to interpret the outcomes.  The errors/warnings have also
> been changing as new versions of lme4 are available.  I would
> appreciate help to know whether it is possible for me to resolve
> these errors or whether I should seed a different analysis approach.
 

  Short answer -- I haven't gotten very far into this yet.
It did inspire me to post a utility function for trying out
all known optimizers:

https://raw.githubusercontent.com/lme4/lme4/master/misc/issues/allFit.R

When I try this on your data,

## setwd("~/R/pkgs/lme4git/lme4/misc/issues")
source("path.data.example.R")
library(lme4)
fm1 <- lmer(env1 ~ env2 + env3 + (1|plot) + (1|species), data=data)
source("allFit.R") ## URL above
rr <- allFit(fm1)
rr2 <- rr[!sapply(rr,inherits,"try-error")]
sapply(rr2,fixef)  ## bobyqa, nlminb, L-BFGS-B all give similar fixed effects
sapply(rr2,function(x) unlist(VarCorr(x)))   ## and RE variances
lapply(rr2,function(x) x at optinfo$conv$lme4$messages)  ## all complain
lapply(rr2,function(x) x at optinfo$warnings)

But my results are quite different from yours -- are we using the
same subset of the data?

> REML criterion at convergence: -9807.396
> Random effects:
>  Groups   Name        Std.Dev. 
>  plot     (Intercept) 2.555e-01
>  species  (Intercept) 4.179e-12
>  Residual             8.358e-08
> Number of obs: 356, groups: plot, 24; species, 9
> Fixed Effects:
> (Intercept)         env2         env3  
>     0.02395      0.56535     -0.16874  
> Warning messages:
> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 3795.17 (tol = 0.002)

  This message goes away in my local branch, where I scale the
gradient differently

> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
 
> Some models run with the full dataset give errors that
> "Downdated VtV is not positive definite."  Some model
> fits are singular, I think, but I do not fully understand
> the implications of that for proceeding.

  You know what it means, right? (Zero variances/perfect correlations
in the estimated random effects variance-covariance matrix).  I haven't
followed Shipley 2009, but if everything _else_ about your data
were reliable, and if your focus was on the fixed-effect estimates,
I might not worry about it too much.  Or you could remove those
terms from the model.  It really depends a bit more on the context
of your analysis.


From friso.muijsers at uni-oldenburg.de  Wed Apr  9 06:44:48 2014
From: friso.muijsers at uni-oldenburg.de (Friso Muijsers)
Date: Wed, 09 Apr 2014 06:44:48 +0200
Subject: [R-sig-ME] Logarithmic Mixed Models?
In-Reply-To: <CAHe08Sh71Yp9CQX=ko2O96Yh0=V9K7RdWf4jG4UTRcvZ06HEXQ@mail.gmail.com>
References: <CAHe08Sh71Yp9CQX=ko2O96Yh0=V9K7RdWf4jG4UTRcvZ06HEXQ@mail.gmail.com>
Message-ID: <5344D040.5020909@uni-oldenburg.de>

Hello Jabob,

I'm using LMM on log-transformed dependent variables, as well. Couldn't 
see a reason why this should be a problem.

Greetings

Friso Muijsers

Institute for Chemistry and Biology of the Marine Environment (ICBM)
Carl-von-Ossietzky University Oldenburg
Schleusenstrasse 1
26382 Wilhemshaven

Am 08.04.2014 23:01, schrieb AvianResearchDivision:
> Hi all,
>
> I am working with ambient noise level as an environmental predictor
> variable and am wondering if using linear mixed models for this sort of
> data would be incorrect.  It seems since sound pressure level (dB) is on a
> log scale, that performing mixed models on a non-linear scale or
> transforming the response variable might be the correct way to proceed.
> Any thoughts on this?  Anyone have any experience?
>
> Best,
> Jacob
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Jens.Astrom at nina.no  Wed Apr  9 09:20:59 2014
From: Jens.Astrom at nina.no (=?iso-8859-1?B?xXN0cvZtLCBKZW5z?=)
Date: Wed, 9 Apr 2014 07:20:59 +0000
Subject: [R-sig-ME] Use of offset variable when analysing rates in lme4
Message-ID: <B874035C64A0ED45828D21F4D13209244FC9860F@NINSRV05.nina.no>

Dear Caroli and Thierry,

I don't have any objections to what Thierry said regarding the offset in a Poisson model, but I am wondering if a discrete binomial model might be more appropriate than a Poisson. You said you counted the number of visits per inflorescence. Does that mean you recorded whether or not there was a pollinator present in a flower before moving on to the next inflorescence, and that there could be at most one visitor per flower? In other words if you counted say 200 inflorescences, you could record between 0-200 visits? If so, is that not a case of a binomial distribution? 

Stroup (2014) doi:10.2134/agronj2013.0342 , which was recently discussed on the list, has worked examples of a discrete binomial model if you are interested.

I'd welcome any thoughts on this since I'm about to do a similar analysis. Often in these types of data, the number of visits can be quite low (e.g. 0-20) even when you observe several hundred inflorescences. Does anyone have input on choosing between Poisson vs discrete binomial in similar cases?

Regards,

Jens Astrom 



-----------------------------------------------------
Message: 1
Date: Thu, 3 Apr 2014 12:22:56 +0000
From: "ONKELINX, Thierry" <Thierry.ONKELINX at inbo.be>
To: "De Waal, C, Mej <caroli at sun.ac.za>" <caroli at sun.ac.za>,
	"r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Use of offset variable when analysing rates in
	lme4
Message-ID:
	<AA818EAD2576BC488B4F623941DA7427F3A21EB3 at inbomail.inbo.be>
Content-Type: text/plain; charset="iso-2022-jp"

Dear Caroli,

Your model is
visits ~ Pois(lamba)
log(lambda) = mu = beta_i * treat + beta_j * timecat + log(infl)

which can be rewritten to

log(lambda) - log(infl) = mu - log(infl) = beta_i * treat + beta_j * timecat
log(lambda) - log(infl) = log(lambda / infl) = log(visitation rate)

hence beta_i and beta_j are effects in log(visitation rate). Exponentiating the results from glht should give you the relative effects on the visitation rate.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens De Waal, C, Mej <caroli at sun.ac.za>
Verzonden: woensdag 2 april 2014 11:50
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Use of offset variable when analysing rates in lme4

Dear all,

This is my first post to the list as I have been unsuccessful in getting an answer elsewhere. I am also quite new to R.

I am investigating variation in pollinator visitation rate (number of visits per inflorescence) with treatment and time category as fixed factors. Block is a random factor. Following Zuur et al (2009), I used the number of visits as response variable with the log(number of inflorescences) as offset variable. A poisson model was overdispersed, and therefore I opted for a negative binomial model in lme4, as follows:

model1 = glmer.nb(visits ~ treat + timecat + offset(log(infl)) + (1|block))


I am specifically interested in differences in visitation rates between treatments. I therefore performed a post hoc test:

OPexp1 =  glht(model1,mcp(treat = "Tukey"))
plot(cld(OPexp1))

When I plot these results, I get number of visits on the y axis. But what I want is visitation rate (visits per inflorescence). How do I specifiy that the post hoc test should be performed using visitation rate?

I assume what is happening is that fitted values are currently expressed as ? ? V, but how to I specify that they should be expressed as ? (visits per inflorescence) only? On p 240, Zuur et al (2009) mentions that this is possible, but I have not been able to find an example.

Any advice would be much appreciated.

Kind regards,

Caroli de Waal

PhD Student
University of Stellenbosch, South Africa The integrity and confidentiality of this email is governed by these terms / Hierdie terme bepaal die integriteit en vertroulikheid van hierdie epos. http://www.sun.ac.za/emaildisclaimer

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From bbolker at gmail.com  Thu Apr 10 04:33:49 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 10 Apr 2014 02:33:49 +0000 (UTC)
Subject: [R-sig-ME] lme4/glmer convergence warnings
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>	<533BE648.4040005@gmail.com>	<533BE930.90706@gmail.com>	<533C0B0C.4070604@gmail.com>
	<533C641A.8010406@gmail.com>	<533EC6B3.5040706@gmail.com>
	<2F10D5AB-AE1A-457A-AC60-EA428D2ABB5C@linguistics.rub.de>
	<53416508.8090603@gmail.com>
Message-ID: <loom.20140410T042736-48@post.gmane.org>

Ben Bolker <bbolker at ...> writes:

> 
> On 14-04-06 04:31 AM, Tibor Kiss wrote:
> > Hi,
> > 
> > being somewhat nonplussed by similar messages, I also applied
>  Ben's recent suggestion to one of my models
> to get:
> > 
> >      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
> > 1.343e-05 3.530e-05 5.756e-05 7.631e-05 9.841e-05 1.932e-04 
> > 
> > So following up on Rob's message: What does it mean?
> > 
> > With kind regards
> > 
> > Tibor
> > 
> 
>   It means that on the scale of the _standard deviations_ of the
> parameters, the estimated gradients at the MLE (or restricted MLE) are
> not large.  I was surprised in Rob's case that these scaled gradients
> were not that small; much smaller than without the scaling, but not
> small enough to make me think  really understand what's going on.
> 
>   To recapitulate: the appearance of all of these new messages in the
> latest version of lme4 is **not** due to a degradation or change in the
> optimization or fitting procedure -- it's due to a new set of
> convergence tests that we implemented, that we think are giving a lot of
> false positives.  You can easily shut them off yourself, or raise the
> tolerance for the warnings (see ?lmerControl/?glmerControl).  As
> developers, we're a bit stuck now because we don't want to turn the
> warnings _off_ until we understand the circumstances that are triggering
> them better, and that takes more time and effort than we've been able to
> muster so far.

  [much context snipped]

  Just to follow up on this: more technical discussion is going on at
https://github.com/lme4/lme4/issues/120 ... at present, it is looking
like scaling the gradient by the hessian is going to solve a lot of
problems.  If you are experiencing convergence warnings about
max|grad| that you suspect are false positives, it would be a great
help if you could try

  relgrad <- with(fitted_model at optinfo$derivs,solve(Hessian,gradient))
  max(abs(relgrad))

check if the result is a small number (e.g. <0.001) and report **one
way or the other** on this list, or at the Github url above, or
(least preferred) by e-mailing lme4-authors at lists.r-forge.r-project.org
We also hope that this test *will* pick up the cases where people have
reported problems with Nelder-Mead not working properly ...

  Ben Bolker


From longrob604 at gmail.com  Thu Apr 10 10:38:13 2014
From: longrob604 at gmail.com (W Robert Long)
Date: Thu, 10 Apr 2014 09:38:13 +0100
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <loom.20140410T042736-48@post.gmane.org>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>	<533BE648.4040005@gmail.com>	<533BE930.90706@gmail.com>	<533C0B0C.4070604@gmail.com>	<533C641A.8010406@gmail.com>	<533EC6B3.5040706@gmail.com>	<2F10D5AB-AE1A-457A-AC60-EA428D2ABB5C@linguistics.rub.de>	<53416508.8090603@gmail.com>
	<loom.20140410T042736-48@post.gmane.org>
Message-ID: <53465875.2090705@gmail.com>

Hi Ben

For my model, I get

 >   max(abs(relgrad))
[1] 1.081706

Does this help ?

Thanks
Rob

On 10/04/2014 03:33, Ben Bolker wrote:
> Ben Bolker <bbolker at ...> writes:
>
>>
>> On 14-04-06 04:31 AM, Tibor Kiss wrote:
>>> Hi,
>>>
>>> being somewhat nonplussed by similar messages, I also applied
>>   Ben's recent suggestion to one of my models
>> to get:
>>>
>>>       Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
>>> 1.343e-05 3.530e-05 5.756e-05 7.631e-05 9.841e-05 1.932e-04
>>>
>>> So following up on Rob's message: What does it mean?
>>>
>>> With kind regards
>>>
>>> Tibor
>>>
>>
>>    It means that on the scale of the _standard deviations_ of the
>> parameters, the estimated gradients at the MLE (or restricted MLE) are
>> not large.  I was surprised in Rob's case that these scaled gradients
>> were not that small; much smaller than without the scaling, but not
>> small enough to make me think  really understand what's going on.
>>
>>    To recapitulate: the appearance of all of these new messages in the
>> latest version of lme4 is **not** due to a degradation or change in the
>> optimization or fitting procedure -- it's due to a new set of
>> convergence tests that we implemented, that we think are giving a lot of
>> false positives.  You can easily shut them off yourself, or raise the
>> tolerance for the warnings (see ?lmerControl/?glmerControl).  As
>> developers, we're a bit stuck now because we don't want to turn the
>> warnings _off_ until we understand the circumstances that are triggering
>> them better, and that takes more time and effort than we've been able to
>> muster so far.
>
>    [much context snipped]
>
>    Just to follow up on this: more technical discussion is going on at
> https://github.com/lme4/lme4/issues/120 ... at present, it is looking
> like scaling the gradient by the hessian is going to solve a lot of
> problems.  If you are experiencing convergence warnings about
> max|grad| that you suspect are false positives, it would be a great
> help if you could try
>
>    relgrad <- with(fitted_model at optinfo$derivs,solve(Hessian,gradient))
>    max(abs(relgrad))
>
> check if the result is a small number (e.g. <0.001) and report **one
> way or the other** on this list, or at the Github url above, or
> (least preferred) by e-mailing lme4-authors at lists.r-forge.r-project.org
> We also hope that this test *will* pick up the cases where people have
> reported problems with Nelder-Mead not working properly ...
>
>    Ben Bolker
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From sh3083 at columbia.edu  Tue Apr  8 21:37:41 2014
From: sh3083 at columbia.edu (Sang Won Han)
Date: Tue, 8 Apr 2014 15:37:41 -0400
Subject: [R-sig-ME] questions about how to incorporate survey weights into
 multilevel model and etc.
Message-ID: <CAEE+LYmknthwGMMySAXzkzjm3PF6nV2o-++8HrGhSaag=5Tmog@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140408/8d9d9d80/attachment.pl>

From caroli at sun.ac.za  Thu Apr 10 08:58:35 2014
From: caroli at sun.ac.za (Caroli)
Date: Thu, 10 Apr 2014 06:58:35 +0000 (UTC)
Subject: [R-sig-ME] Use of offset variable when analysing rates in lme4
References: <B874035C64A0ED45828D21F4D13209244FC9860F@NINSRV05.nina.no>
Message-ID: <loom.20140410T084601-872@post.gmane.org>


> 
> Dear Caroli and Thierry,
> 
> I don't have any objections to what Thierry said regarding the offset in a 
Poisson model, but I am wondering
> if a discrete binomial model might be more appropriate than a Poisson. You 
said you counted the number of
> visits per inflorescence. Does that mean you recorded whether or not there 
was a pollinator present in a
> flower before moving on to the next inflorescence, and that there could be 
at most one visitor per flower?
> In other words if you counted say 200 inflorescences, you could record 
between 0-200 visits? If so, is that
> not a case of a binomial distribution? 
> 
> Stroup (2014) doi:10.2134/agronj2013.0342 , which was recently discussed 
on the list, has worked
> examples of a discrete binomial model if you are interested.
> 
> I'd welcome any thoughts on this since I'm about to do a similar analysis. 
Often in these types of data, the
> number of visits can be quite low (e.g. 0-20) even when you observe 
several hundred inflorescences. Does
> anyone have input on choosing between Poisson vs discrete binomial in 
similar cases?
> 
> Regards,
> 
> Jens Astrom 
> 
> 

Dear Jens,

Thanks for your response!

I see what you mean with using a binomial model which would be more 
appropriate in the case you described. That would almost be the same as 
asking what proportion of inflorescences were visited, right? 
However, in my case I observed say 200 inflorescences and observed 
pollinators visiting the patch over a 5 min period, so it can be regarded as 
a continuous count variable (each inflorescence can be visited more than 
once). I.e., I did not "follow" individual inflorescences.
Although, as you note, number of visits were quite low in my case and in 
only a few cases were visitation rates (visits/inflorescence) larger than 
one (and these were for treatments in my experiment with only four 
inflorescences). 

Best wishes
Caroli


From bbolker at gmail.com  Thu Apr 10 21:08:07 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 10 Apr 2014 15:08:07 -0400
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <53465875.2090705@gmail.com>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>	<533BE648.4040005@gmail.com>	<533BE930.90706@gmail.com>	<533C0B0C.4070604@gmail.com>	<533C641A.8010406@gmail.com>	<533EC6B3.5040706@gmail.com>	<2F10D5AB-AE1A-457A-AC60-EA428D2ABB5C@linguistics.rub.de>	<53416508.8090603@gmail.com>	<loom.20140410T042736-48@post.gmane.org>
	<53465875.2090705@gmail.com>
Message-ID: <5346EC17.9060009@gmail.com>

On 14-04-10 04:38 AM, W Robert Long wrote:
> Hi Ben
> 
> For my model, I get
> 
>>   max(abs(relgrad))
> [1] 1.081706
> 
> Does this help ?
> 
> Thanks
> Rob
> 

  Thanks.
   Unfortunately it means there's still something we don't understand.
To sum up:

  * we calculate the finite-difference gradients and Hessians with
respect to the model parameters at the estimated MLE (or restricted MLE).

  - these might be poor estimates; we are using our own hand-rolled,
naive central finite difference code rather than the better tested code
in the numDeriv package, which uses the more expensive but more accurate
Richardson method.  To see if this is the problem, compare

  fitted_model at optinfo$derivs$gradient

to

  dd <- update(fitted_model,devFunOnly=TRUE)
  params <- getME(fitted_model,"theta")
## or unlist(getME(fitted_model,c("theta","fixef")) for GLMMs
  grad(dd,params)

and see if they differ significantly.  I mostly have *not* seen a
discrepancy in these, although Rune Haubo has reported some
discrepancies (although I think they were in the Hessian rather than the
gradient ... ?)

  It might make sense to compare the Hessian in @optinfo with
hessian(dd,params) from numDeriv as well.

 * we then check those gradients, or the relative gradients from

  with(fitted_model at optinfo$derivs,solve(Hessian,gradient))

against some tolerance.  It makes sense that large gradients at the
estimated parameters (where they should be zero) are a problem, but it
continues to be unclear to me on exactly what scale they should be
small/how we should make this comparison ...

  It might make sense to see if John Nash's optfntools package (on
r-forge) has any useful tools or ideas, although as I recall (vaguely)
those implementations of (e.g.) the KKT criteria
<http://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions>
were pretty sensitive ...

 * as a crude test of whether the actual value you got is a false
convergence (I know in this case you don't think it is, but including
this for completeness) the things I know to do are (1) try re-fitting
with different optimizers, either starting from the putative best fit or
from scratch (see
<https://github.com/lme4/lme4/blob/master/misc/issues/allFit.R>); (2)
try re-starting the _same_ optimizer from the putative best fit; (3)
explore the likelihood surface (e.g. with profiling or bbmle::slice2D).

  Ben Bolker





> On 10/04/2014 03:33, Ben Bolker wrote:
>> Ben Bolker <bbolker at ...> writes:
>>
>>>
>>> On 14-04-06 04:31 AM, Tibor Kiss wrote:
>>>> Hi,
>>>>
>>>> being somewhat nonplussed by similar messages, I also applied
>>>   Ben's recent suggestion to one of my models
>>> to get:
>>>>
>>>>       Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
>>>> 1.343e-05 3.530e-05 5.756e-05 7.631e-05 9.841e-05 1.932e-04
>>>>
>>>> So following up on Rob's message: What does it mean?
>>>>
>>>> With kind regards
>>>>
>>>> Tibor
>>>>
>>>
>>>    It means that on the scale of the _standard deviations_ of the
>>> parameters, the estimated gradients at the MLE (or restricted MLE) are
>>> not large.  I was surprised in Rob's case that these scaled gradients
>>> were not that small; much smaller than without the scaling, but not
>>> small enough to make me think  really understand what's going on.
>>>
>>>    To recapitulate: the appearance of all of these new messages in the
>>> latest version of lme4 is **not** due to a degradation or change in the
>>> optimization or fitting procedure -- it's due to a new set of
>>> convergence tests that we implemented, that we think are giving a lot of
>>> false positives.  You can easily shut them off yourself, or raise the
>>> tolerance for the warnings (see ?lmerControl/?glmerControl).  As
>>> developers, we're a bit stuck now because we don't want to turn the
>>> warnings _off_ until we understand the circumstances that are triggering
>>> them better, and that takes more time and effort than we've been able to
>>> muster so far.
>>
>>    [much context snipped]
>>
>>    Just to follow up on this: more technical discussion is going on at
>> https://github.com/lme4/lme4/issues/120 ... at present, it is looking
>> like scaling the gradient by the hessian is going to solve a lot of
>> problems.  If you are experiencing convergence warnings about
>> max|grad| that you suspect are false positives, it would be a great
>> help if you could try
>>
>>    relgrad <- with(fitted_model at optinfo$derivs,solve(Hessian,gradient))
>>    max(abs(relgrad))
>>
>> check if the result is a small number (e.g. <0.001) and report **one
>> way or the other** on this list, or at the Github url above, or
>> (least preferred) by e-mailing lme4-authors at lists.r-forge.r-project.org
>> We also hope that this test *will* pick up the cases where people have
>> reported problems with Nelder-Mead not working properly ...
>>
>>    Ben Bolker
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From longrob604 at gmail.com  Thu Apr 10 22:28:08 2014
From: longrob604 at gmail.com (W Robert Long)
Date: Thu, 10 Apr 2014 21:28:08 +0100
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <5346EC17.9060009@gmail.com>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>	<533BE648.4040005@gmail.com>	<533BE930.90706@gmail.com>	<533C0B0C.4070604@gmail.com>	<533C641A.8010406@gmail.com>	<533EC6B3.5040706@gmail.com>	<2F10D5AB-AE1A-457A-AC60-EA428D2ABB5C@linguistics.rub.de>	<53416508.8090603@gmail.com>	<loom.20140410T042736-48@post.gmane.org>
	<53465875.2090705@gmail.com> <5346EC17.9060009@gmail.com>
Message-ID: <5346FED8.4060702@gmail.com>

Hi Ben

Could you clarify what you mean by

 >    params <- getME(fitted_model,"theta")
 > ## or unlist(getME(fitted_model,c("theta","fixef")) for GLMMs
 >    grad(dd,params)

This is a logistic model, so I assume I need to use unlist, however it 
seems that "fixef" is not a structure in fitted_model because:
Error in match.arg(name) :
   'arg' should be one of ?X?, ?Z?, ?Zt?, ?Ztlist?, ?y?, ?mu?, ?u?, ?b?, 
?Gp?, ?Tp?, ?L?, ?Lambda?, ?Lambdat?, ?Lind?, ?A?, ?RX?, ?RZX?, ?sigma?, 
?flist?, ?beta?, ?theta?, ?ST?, ?REML?, ?is_REML?, ?n_rtrms?, ?n_rfacs?, 
?cnms?, ?devcomp?, ?offset?, ?lower?

Also, grad() does not seem to be an available function.

Please advise

Thanks
Rob




On 10/04/2014 20:08, Ben Bolker wrote:
> On 14-04-10 04:38 AM, W Robert Long wrote:
>> Hi Ben
>>
>> For my model, I get
>>
>>>    max(abs(relgrad))
>> [1] 1.081706
>>
>> Does this help ?
>>
>> Thanks
>> Rob
>>
>
>    Thanks.
>     Unfortunately it means there's still something we don't understand.
> To sum up:
>
>    * we calculate the finite-difference gradients and Hessians with
> respect to the model parameters at the estimated MLE (or restricted MLE).
>
>    - these might be poor estimates; we are using our own hand-rolled,
> naive central finite difference code rather than the better tested code
> in the numDeriv package, which uses the more expensive but more accurate
> Richardson method.  To see if this is the problem, compare
>
>    fitted_model at optinfo$derivs$gradient
>
> to
>
>    dd <- update(fitted_model,devFunOnly=TRUE)
>    params <- getME(fitted_model,"theta")
> ## or unlist(getME(fitted_model,c("theta","fixef")) for GLMMs
>    grad(dd,params)
>
> and see if they differ significantly.  I mostly have *not* seen a
> discrepancy in these, although Rune Haubo has reported some
> discrepancies (although I think they were in the Hessian rather than the
> gradient ... ?)
>
>    It might make sense to compare the Hessian in @optinfo with
> hessian(dd,params) from numDeriv as well.
>
>   * we then check those gradients, or the relative gradients from
>
>    with(fitted_model at optinfo$derivs,solve(Hessian,gradient))
>
> against some tolerance.  It makes sense that large gradients at the
> estimated parameters (where they should be zero) are a problem, but it
> continues to be unclear to me on exactly what scale they should be
> small/how we should make this comparison ...
>
>    It might make sense to see if John Nash's optfntools package (on
> r-forge) has any useful tools or ideas, although as I recall (vaguely)
> those implementations of (e.g.) the KKT criteria
> <http://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions>
> were pretty sensitive ...
>
>   * as a crude test of whether the actual value you got is a false
> convergence (I know in this case you don't think it is, but including
> this for completeness) the things I know to do are (1) try re-fitting
> with different optimizers, either starting from the putative best fit or
> from scratch (see
> <https://github.com/lme4/lme4/blob/master/misc/issues/allFit.R>); (2)
> try re-starting the _same_ optimizer from the putative best fit; (3)
> explore the likelihood surface (e.g. with profiling or bbmle::slice2D).
>
>    Ben Bolker
>
>
>
>
>
>> On 10/04/2014 03:33, Ben Bolker wrote:
>>> Ben Bolker <bbolker at ...> writes:
>>>
>>>>
>>>> On 14-04-06 04:31 AM, Tibor Kiss wrote:
>>>>> Hi,
>>>>>
>>>>> being somewhat nonplussed by similar messages, I also applied
>>>>    Ben's recent suggestion to one of my models
>>>> to get:
>>>>>
>>>>>        Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
>>>>> 1.343e-05 3.530e-05 5.756e-05 7.631e-05 9.841e-05 1.932e-04
>>>>>
>>>>> So following up on Rob's message: What does it mean?
>>>>>
>>>>> With kind regards
>>>>>
>>>>> Tibor
>>>>>
>>>>
>>>>     It means that on the scale of the _standard deviations_ of the
>>>> parameters, the estimated gradients at the MLE (or restricted MLE) are
>>>> not large.  I was surprised in Rob's case that these scaled gradients
>>>> were not that small; much smaller than without the scaling, but not
>>>> small enough to make me think  really understand what's going on.
>>>>
>>>>     To recapitulate: the appearance of all of these new messages in the
>>>> latest version of lme4 is **not** due to a degradation or change in the
>>>> optimization or fitting procedure -- it's due to a new set of
>>>> convergence tests that we implemented, that we think are giving a lot of
>>>> false positives.  You can easily shut them off yourself, or raise the
>>>> tolerance for the warnings (see ?lmerControl/?glmerControl).  As
>>>> developers, we're a bit stuck now because we don't want to turn the
>>>> warnings _off_ until we understand the circumstances that are triggering
>>>> them better, and that takes more time and effort than we've been able to
>>>> muster so far.
>>>
>>>     [much context snipped]
>>>
>>>     Just to follow up on this: more technical discussion is going on at
>>> https://github.com/lme4/lme4/issues/120 ... at present, it is looking
>>> like scaling the gradient by the hessian is going to solve a lot of
>>> problems.  If you are experiencing convergence warnings about
>>> max|grad| that you suspect are false positives, it would be a great
>>> help if you could try
>>>
>>>     relgrad <- with(fitted_model at optinfo$derivs,solve(Hessian,gradient))
>>>     max(abs(relgrad))
>>>
>>> check if the result is a small number (e.g. <0.001) and report **one
>>> way or the other** on this list, or at the Github url above, or
>>> (least preferred) by e-mailing lme4-authors at lists.r-forge.r-project.org
>>> We also hope that this test *will* pick up the cases where people have
>>> reported problems with Nelder-Mead not working properly ...
>>>
>>>     Ben Bolker
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Thu Apr 10 22:32:38 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 10 Apr 2014 16:32:38 -0400
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <5346FED8.4060702@gmail.com>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>	<533BE648.4040005@gmail.com>	<533BE930.90706@gmail.com>	<533C0B0C.4070604@gmail.com>	<533C641A.8010406@gmail.com>	<533EC6B3.5040706@gmail.com>	<2F10D5AB-AE1A-457A-AC60-EA428D2ABB5C@linguistics.rub.de>	<53416508.8090603@gmail.com>	<loom.20140410T042736-48@post.gmane.org>	<53465875.2090705@gmail.com>
	<5346EC17.9060009@gmail.com> <5346FED8.4060702@gmail.com>
Message-ID: <5346FFE6.9080701@gmail.com>

On 14-04-10 04:28 PM, W Robert Long wrote:
> Hi Ben
> 
> Could you clarify what you mean by
> 
>>    params <- getME(fitted_model,"theta")
>> ## or unlist(getME(fitted_model,c("theta","fixef")) for GLMMs
>>    grad(dd,params)
> 
> This is a logistic model, so I assume I need to use unlist, however it
> seems that "fixef" is not a structure in fitted_model because:
> Error in match.arg(name) :
>   'arg' should be one of ?X?, ?Z?, ?Zt?, ?Ztlist?, ?y?, ?mu?, ?u?, ?b?,
> ?Gp?, ?Tp?, ?L?, ?Lambda?, ?Lambdat?, ?Lind?, ?A?, ?RX?, ?RZX?, ?sigma?,
> ?flist?, ?beta?, ?theta?, ?ST?, ?REML?, ?is_REML?, ?n_rtrms?, ?n_rfacs?,
> ?cnms?, ?devcomp?, ?offset?, ?lower?
> 
> Also, grad() does not seem to be an available function.
> 
> Please advise
> 
> Thanks
> Rob

  Oops, too much of a hurry.

 * We just added 'fixef' as a synonym for 'beta' in getME, but it's only
on github.  unlist(getME(fitted_model,c("theta","beta"))) should work ...
 * grad() is in the numDeriv package -- install/load it first, or use
numDeriv::grad() if it's installed and you don't feel like loading.

  cheers
    Ben Bolker

> 
> 
> 
> 
> On 10/04/2014 20:08, Ben Bolker wrote:
>> On 14-04-10 04:38 AM, W Robert Long wrote:
>>> Hi Ben
>>>
>>> For my model, I get
>>>
>>>>    max(abs(relgrad))
>>> [1] 1.081706
>>>
>>> Does this help ?
>>>
>>> Thanks
>>> Rob
>>>
>>
>>    Thanks.
>>     Unfortunately it means there's still something we don't understand.
>> To sum up:
>>
>>    * we calculate the finite-difference gradients and Hessians with
>> respect to the model parameters at the estimated MLE (or restricted MLE).
>>
>>    - these might be poor estimates; we are using our own hand-rolled,
>> naive central finite difference code rather than the better tested code
>> in the numDeriv package, which uses the more expensive but more accurate
>> Richardson method.  To see if this is the problem, compare
>>
>>    fitted_model at optinfo$derivs$gradient
>>
>> to
>>
>>    dd <- update(fitted_model,devFunOnly=TRUE)
>>    params <- getME(fitted_model,"theta")
>> ## or unlist(getME(fitted_model,c("theta","fixef")) for GLMMs
>>    grad(dd,params)
>>
>> and see if they differ significantly.  I mostly have *not* seen a
>> discrepancy in these, although Rune Haubo has reported some
>> discrepancies (although I think they were in the Hessian rather than the
>> gradient ... ?)
>>
>>    It might make sense to compare the Hessian in @optinfo with
>> hessian(dd,params) from numDeriv as well.
>>
>>   * we then check those gradients, or the relative gradients from
>>
>>    with(fitted_model at optinfo$derivs,solve(Hessian,gradient))
>>
>> against some tolerance.  It makes sense that large gradients at the
>> estimated parameters (where they should be zero) are a problem, but it
>> continues to be unclear to me on exactly what scale they should be
>> small/how we should make this comparison ...
>>
>>    It might make sense to see if John Nash's optfntools package (on
>> r-forge) has any useful tools or ideas, although as I recall (vaguely)
>> those implementations of (e.g.) the KKT criteria
>> <http://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions>
>>
>> were pretty sensitive ...
>>
>>   * as a crude test of whether the actual value you got is a false
>> convergence (I know in this case you don't think it is, but including
>> this for completeness) the things I know to do are (1) try re-fitting
>> with different optimizers, either starting from the putative best fit or
>> from scratch (see
>> <https://github.com/lme4/lme4/blob/master/misc/issues/allFit.R>); (2)
>> try re-starting the _same_ optimizer from the putative best fit; (3)
>> explore the likelihood surface (e.g. with profiling or bbmle::slice2D).
>>
>>    Ben Bolker
>>
>>
>>
>>
>>
>>> On 10/04/2014 03:33, Ben Bolker wrote:
>>>> Ben Bolker <bbolker at ...> writes:
>>>>
>>>>>
>>>>> On 14-04-06 04:31 AM, Tibor Kiss wrote:
>>>>>> Hi,
>>>>>>
>>>>>> being somewhat nonplussed by similar messages, I also applied
>>>>>    Ben's recent suggestion to one of my models
>>>>> to get:
>>>>>>
>>>>>>        Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
>>>>>> 1.343e-05 3.530e-05 5.756e-05 7.631e-05 9.841e-05 1.932e-04
>>>>>>
>>>>>> So following up on Rob's message: What does it mean?
>>>>>>
>>>>>> With kind regards
>>>>>>
>>>>>> Tibor
>>>>>>
>>>>>
>>>>>     It means that on the scale of the _standard deviations_ of the
>>>>> parameters, the estimated gradients at the MLE (or restricted MLE) are
>>>>> not large.  I was surprised in Rob's case that these scaled gradients
>>>>> were not that small; much smaller than without the scaling, but not
>>>>> small enough to make me think  really understand what's going on.
>>>>>
>>>>>     To recapitulate: the appearance of all of these new messages in
>>>>> the
>>>>> latest version of lme4 is **not** due to a degradation or change in
>>>>> the
>>>>> optimization or fitting procedure -- it's due to a new set of
>>>>> convergence tests that we implemented, that we think are giving a
>>>>> lot of
>>>>> false positives.  You can easily shut them off yourself, or raise the
>>>>> tolerance for the warnings (see ?lmerControl/?glmerControl).  As
>>>>> developers, we're a bit stuck now because we don't want to turn the
>>>>> warnings _off_ until we understand the circumstances that are
>>>>> triggering
>>>>> them better, and that takes more time and effort than we've been
>>>>> able to
>>>>> muster so far.
>>>>
>>>>     [much context snipped]
>>>>
>>>>     Just to follow up on this: more technical discussion is going on at
>>>> https://github.com/lme4/lme4/issues/120 ... at present, it is looking
>>>> like scaling the gradient by the hessian is going to solve a lot of
>>>> problems.  If you are experiencing convergence warnings about
>>>> max|grad| that you suspect are false positives, it would be a great
>>>> help if you could try
>>>>
>>>>     relgrad <-
>>>> with(fitted_model at optinfo$derivs,solve(Hessian,gradient))
>>>>     max(abs(relgrad))
>>>>
>>>> check if the result is a small number (e.g. <0.001) and report **one
>>>> way or the other** on this list, or at the Github url above, or
>>>> (least preferred) by e-mailing lme4-authors at lists.r-forge.r-project.org
>>>> We also hope that this test *will* pick up the cases where people have
>>>> reported problems with Nelder-Mead not working properly ...
>>>>
>>>>     Ben Bolker
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From longrob604 at gmail.com  Thu Apr 10 23:35:18 2014
From: longrob604 at gmail.com (W Robert Long)
Date: Thu, 10 Apr 2014 22:35:18 +0100
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <5346FFE6.9080701@gmail.com>
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>	<533BE648.4040005@gmail.com>	<533BE930.90706@gmail.com>	<533C0B0C.4070604@gmail.com>	<533C641A.8010406@gmail.com>	<533EC6B3.5040706@gmail.com>	<2F10D5AB-AE1A-457A-AC60-EA428D2ABB5C@linguistics.rub.de>	<53416508.8090603@gmail.com>	<loom.20140410T042736-48@post.gmane.org>	<53465875.2090705@gmail.com>
	<5346EC17.9060009@gmail.com> <5346FED8.4060702@gmail.com>
	<5346FFE6.9080701@gmail.com>
Message-ID: <53470E96.2090001@gmail.com>

Hi Ben

Thanks for clarifying. I get

 > fitted_model at optinfo$derivs$gradient
  [1]   -0.04070906   -0.41672993   -0.72225653  -32.57950953 
-61.30328647  -56.38420303   -0.64343470   -0.34225110   -0.03854723 
4847.74510014    0.27245700

 > dd <- update(fitted_model,devFunOnly=TRUE)
 > params <- unlist(getME(fitted_model,c("theta","beta")))
 > grad(dd,params)
  [1]   -0.04076175   -0.41673067   -0.72230322  -32.88266857 
-63.62821306  -56.99385033   -0.64343614   -0.34216063   -0.03854783 
-835.66161982    0.27250186

So one large discrepancy, one smaller one, and the rest pretty close.

Please advise.

Thanks again
Rob











On 10/04/2014 21:32, Ben Bolker wrote:
> On 14-04-10 04:28 PM, W Robert Long wrote:
>> Hi Ben
>>
>> Could you clarify what you mean by
>>
>>>     params <- getME(fitted_model,"theta")
>>> ## or unlist(getME(fitted_model,c("theta","fixef")) for GLMMs
>>>     grad(dd,params)
>>
>> This is a logistic model, so I assume I need to use unlist, however it
>> seems that "fixef" is not a structure in fitted_model because:
>> Error in match.arg(name) :
>>    'arg' should be one of ?X?, ?Z?, ?Zt?, ?Ztlist?, ?y?, ?mu?, ?u?, ?b?,
>> ?Gp?, ?Tp?, ?L?, ?Lambda?, ?Lambdat?, ?Lind?, ?A?, ?RX?, ?RZX?, ?sigma?,
>> ?flist?, ?beta?, ?theta?, ?ST?, ?REML?, ?is_REML?, ?n_rtrms?, ?n_rfacs?,
>> ?cnms?, ?devcomp?, ?offset?, ?lower?
>>
>> Also, grad() does not seem to be an available function.
>>
>> Please advise
>>
>> Thanks
>> Rob
>
>    Oops, too much of a hurry.
>
>   * We just added 'fixef' as a synonym for 'beta' in getME, but it's only
> on github.  unlist(getME(fitted_model,c("theta","beta"))) should work ...
>   * grad() is in the numDeriv package -- install/load it first, or use
> numDeriv::grad() if it's installed and you don't feel like loading.
>
>    cheers
>      Ben Bolker
>
>>
>>
>>
>>
>> On 10/04/2014 20:08, Ben Bolker wrote:
>>> On 14-04-10 04:38 AM, W Robert Long wrote:
>>>> Hi Ben
>>>>
>>>> For my model, I get
>>>>
>>>>>     max(abs(relgrad))
>>>> [1] 1.081706
>>>>
>>>> Does this help ?
>>>>
>>>> Thanks
>>>> Rob
>>>>
>>>
>>>     Thanks.
>>>      Unfortunately it means there's still something we don't understand.
>>> To sum up:
>>>
>>>     * we calculate the finite-difference gradients and Hessians with
>>> respect to the model parameters at the estimated MLE (or restricted MLE).
>>>
>>>     - these might be poor estimates; we are using our own hand-rolled,
>>> naive central finite difference code rather than the better tested code
>>> in the numDeriv package, which uses the more expensive but more accurate
>>> Richardson method.  To see if this is the problem, compare
>>>
>>>     fitted_model at optinfo$derivs$gradient
>>>
>>> to
>>>
>>>     dd <- update(fitted_model,devFunOnly=TRUE)
>>>     params <- getME(fitted_model,"theta")
>>> ## or unlist(getME(fitted_model,c("theta","fixef")) for GLMMs
>>>     grad(dd,params)
>>>
>>> and see if they differ significantly.  I mostly have *not* seen a
>>> discrepancy in these, although Rune Haubo has reported some
>>> discrepancies (although I think they were in the Hessian rather than the
>>> gradient ... ?)
>>>
>>>     It might make sense to compare the Hessian in @optinfo with
>>> hessian(dd,params) from numDeriv as well.
>>>
>>>    * we then check those gradients, or the relative gradients from
>>>
>>>     with(fitted_model at optinfo$derivs,solve(Hessian,gradient))
>>>
>>> against some tolerance.  It makes sense that large gradients at the
>>> estimated parameters (where they should be zero) are a problem, but it
>>> continues to be unclear to me on exactly what scale they should be
>>> small/how we should make this comparison ...
>>>
>>>     It might make sense to see if John Nash's optfntools package (on
>>> r-forge) has any useful tools or ideas, although as I recall (vaguely)
>>> those implementations of (e.g.) the KKT criteria
>>> <http://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions>
>>>
>>> were pretty sensitive ...
>>>
>>>    * as a crude test of whether the actual value you got is a false
>>> convergence (I know in this case you don't think it is, but including
>>> this for completeness) the things I know to do are (1) try re-fitting
>>> with different optimizers, either starting from the putative best fit or
>>> from scratch (see
>>> <https://github.com/lme4/lme4/blob/master/misc/issues/allFit.R>); (2)
>>> try re-starting the _same_ optimizer from the putative best fit; (3)
>>> explore the likelihood surface (e.g. with profiling or bbmle::slice2D).
>>>
>>>     Ben Bolker
>>>
>>>
>>>
>>>
>>>
>>>> On 10/04/2014 03:33, Ben Bolker wrote:
>>>>> Ben Bolker <bbolker at ...> writes:
>>>>>
>>>>>>
>>>>>> On 14-04-06 04:31 AM, Tibor Kiss wrote:
>>>>>>> Hi,
>>>>>>>
>>>>>>> being somewhat nonplussed by similar messages, I also applied
>>>>>>     Ben's recent suggestion to one of my models
>>>>>> to get:
>>>>>>>
>>>>>>>         Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
>>>>>>> 1.343e-05 3.530e-05 5.756e-05 7.631e-05 9.841e-05 1.932e-04
>>>>>>>
>>>>>>> So following up on Rob's message: What does it mean?
>>>>>>>
>>>>>>> With kind regards
>>>>>>>
>>>>>>> Tibor
>>>>>>>
>>>>>>
>>>>>>      It means that on the scale of the _standard deviations_ of the
>>>>>> parameters, the estimated gradients at the MLE (or restricted MLE) are
>>>>>> not large.  I was surprised in Rob's case that these scaled gradients
>>>>>> were not that small; much smaller than without the scaling, but not
>>>>>> small enough to make me think  really understand what's going on.
>>>>>>
>>>>>>      To recapitulate: the appearance of all of these new messages in
>>>>>> the
>>>>>> latest version of lme4 is **not** due to a degradation or change in
>>>>>> the
>>>>>> optimization or fitting procedure -- it's due to a new set of
>>>>>> convergence tests that we implemented, that we think are giving a
>>>>>> lot of
>>>>>> false positives.  You can easily shut them off yourself, or raise the
>>>>>> tolerance for the warnings (see ?lmerControl/?glmerControl).  As
>>>>>> developers, we're a bit stuck now because we don't want to turn the
>>>>>> warnings _off_ until we understand the circumstances that are
>>>>>> triggering
>>>>>> them better, and that takes more time and effort than we've been
>>>>>> able to
>>>>>> muster so far.
>>>>>
>>>>>      [much context snipped]
>>>>>
>>>>>      Just to follow up on this: more technical discussion is going on at
>>>>> https://github.com/lme4/lme4/issues/120 ... at present, it is looking
>>>>> like scaling the gradient by the hessian is going to solve a lot of
>>>>> problems.  If you are experiencing convergence warnings about
>>>>> max|grad| that you suspect are false positives, it would be a great
>>>>> help if you could try
>>>>>
>>>>>      relgrad <-
>>>>> with(fitted_model at optinfo$derivs,solve(Hessian,gradient))
>>>>>      max(abs(relgrad))
>>>>>
>>>>> check if the result is a small number (e.g. <0.001) and report **one
>>>>> way or the other** on this list, or at the Github url above, or
>>>>> (least preferred) by e-mailing lme4-authors at lists.r-forge.r-project.org
>>>>> We also hope that this test *will* pick up the cases where people have
>>>>> reported problems with Nelder-Mead not working properly ...
>>>>>
>>>>>      Ben Bolker
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From corey.sparks at utsa.edu  Fri Apr 11 03:01:58 2014
From: corey.sparks at utsa.edu (Corey Sparks)
Date: Fri, 11 Apr 2014 01:01:58 +0000
Subject: [R-sig-ME] lme4/glmer convergence warnings
Message-ID: <AB4F6BFB-45D8-4026-AFD5-A4D0CC819A0D@utsa.edu>

Hello all,
I?ve been seeing the aforementioned convergence errors for weeks in a course i?m teaching using lme4, so I decided to follow Ben?s advice on reporting the :

I?m fitting a binomial GLMM for small area estimation model building here:
glmer(I(bmi>30)~povz+vachousz+baccz+blackz+hispanicz+factor(region)+(1|state)+(1|cofips), family="binomial", data=merged, weights=cntywt/mean(cntywt))

n=~240000, n_cofips=217, n_state=46
I get the warning:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.235915 (tol = 0.001)

and the gradients:

relgrad <- with(fit.1 at optinfo$derivs,solve(Hessian,gradient))
> max(abs(relgrad))
[1] 0.0001631008

and when I use refit(), I get:
fit.1<-refit(fit.1)
> relgrad <- with(fit.1 at optinfo$derivs,solve(Hessian,gradient))
> max(abs(relgrad))
[1] 2.369877e-06

For another model on a LMM, I get:

fit.mix<-lmer(bmiz~agez+lths+coll+black+hispanic+other+(1|cofips), brfss_11)

In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00550412 (tol = 0.002)

relgrad <- with(fit.mix at optinfo$derivs,solve(Hessian,gradient))
> max(abs(relgrad))
[1] 8.099289e-08

Hope this helps


Corey Sparks
Assistant Professor
Department of Demography
The University of Texas at San Antonio
501 West Cesar E Chavez Blvd
corey.sparks 'at' utsa.edu
coreysparks.weebly.com
210 458 3166


From bbolker at gmail.com  Fri Apr 11 03:35:39 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 10 Apr 2014 21:35:39 -0400
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <AB4F6BFB-45D8-4026-AFD5-A4D0CC819A0D@utsa.edu>
References: <AB4F6BFB-45D8-4026-AFD5-A4D0CC819A0D@utsa.edu>
Message-ID: <534746EB.6090300@gmail.com>

On 14-04-10 09:01 PM, Corey Sparks wrote:

> Hello all, I?ve been seeing the aforementioned convergence errors for
> weeks in a course i?m teaching using lme4, so I decided to follow
> Ben?s advice on reporting the :

> I?m fitting a binomial GLMM for small area estimation model building
> here: 
> glmer(I(bmi>30)~povz+vachousz+baccz+blackz+hispanicz+factor(region)+(1|state)+(1|cofips),
> family="binomial", data=merged, weights=cntywt/mean(cntywt))
> 
> n=~240000, n_cofips=217, n_state=46 I get the warning: In
> checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  : 
> Model failed to converge with max|grad| = 0.235915 (tol = 0.001)
> 
> and the gradients:
> 
> relgrad <- with(fit.1 at optinfo$derivs,solve(Hessian,gradient))
>> max(abs(relgrad))
> [1] 0.0001631008

  This is good (relative gradient is less than the 0.001 or 0.002
tolerance we would think to set as a default)

> 
> and when I use refit(), I get: fit.1<-refit(fit.1)
>> relgrad <- with(fit.1 at optinfo$derivs,solve(Hessian,gradient)) 
>> max(abs(relgrad))
> [1] 2.369877e-06

  This doesn't really matter so much (as long as we're getting below
tolerance on the relative gradient, I don't care so much if we can
decrease it still further by refitting).

> 
> For another model on a LMM, I get:
> 
> fit.mix<-lmer(bmiz~agez+lths+coll+black+hispanic+other+(1|cofips),
> brfss_11)
> 
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
> : Model failed to converge with max|grad| = 0.00550412 (tol = 0.002)
> 
> relgrad <- with(fit.mix at optinfo$derivs,solve(Hessian,gradient))
>> max(abs(relgrad))
> [1] 8.099289e-08

  ditto.

> 
> Hope this helps
> 

  Yes, this is encouraging (switching to relative gradients would clear
everything up here)


From corey.sparks at utsa.edu  Fri Apr 11 16:09:35 2014
From: corey.sparks at utsa.edu (Corey Sparks)
Date: Fri, 11 Apr 2014 14:09:35 +0000
Subject: [R-sig-ME] questions about how to incorporate survey weights into
 multilevel model and etc.
Message-ID: <8683AA65-5A8F-4F17-A856-CC95555B18A8@utsa.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140411/21c7cfd6/attachment.pl>

From tomd792 at gmail.com  Sat Apr 12 00:01:09 2014
From: tomd792 at gmail.com (Tom Davis)
Date: Sat, 12 Apr 2014 00:01:09 +0200
Subject: [R-sig-ME] lme4 and solve(Hessian,
	gradient) check: Discrepancies between AMD/Intel/32bit/64bit?
Message-ID: <CA+doL7wP57KsUaUB4rh9xrHD2AFc8q8p_ovvN8_jz8QkYWT6Tg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140412/a65dae15/attachment.pl>

From segerfan83 at gmail.com  Sun Apr 13 20:52:20 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Sun, 13 Apr 2014 14:52:20 -0400
Subject: [R-sig-ME] Reconciling Near Identical AIC Values and Highly
	Significant P Value
Message-ID: <CAHe08Sg67+yf7G5UKx=_Gref-o3FLXkQi_rJ3riSMw8eJJfwYA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140413/cbfcb358/attachment.pl>

From emmanuel.curis at parisdescartes.fr  Sun Apr 13 21:06:10 2014
From: emmanuel.curis at parisdescartes.fr (Emmanuel Curis)
Date: Sun, 13 Apr 2014 21:06:10 +0200
Subject: [R-sig-ME] Reconciling Near Identical AIC Values and Highly
 Significant P Value
In-Reply-To: <CAHe08Sg67+yf7G5UKx=_Gref-o3FLXkQi_rJ3riSMw8eJJfwYA@mail.gmail.com>
References: <CAHe08Sg67+yf7G5UKx=_Gref-o3FLXkQi_rJ3riSMw8eJJfwYA@mail.gmail.com>
Message-ID: <20140413190610.GB4290@info124.pharmacie.univ-paris5.fr>

Hi,

I may completely misunderstand your problem, but if you replace a
predictor variable by another one with the same number of parameters
(let's say, for instance, ? length ? by ? surface ? in a linear
regression), then

 1) comparing AIC and likelihoods value is the same since the number
    of parameters does not change ;
    hence same AIC <=> predictors give equally good fits

 2) chi-square tests is meaningless, since models are not nested.
    Here, I guess you have the very low p because the function uses a
    0-degrees of freedom [same number of parameters...]  khi-square,
    that is the constant 0, and any value other than 0 as a null
    probability, hence p = 0 < whatever you want...

>From 2), it results than comparing AIC is, between your two options,
the only one valid when models are not nested.

For the second point, I don't know.

Hope this helps,

On Sun, Apr 13, 2014 at 02:52:20PM -0400, AvianResearchDivision wrote:
? Hi all,
? 
? When comparing identical models (only difference in predictor variable;
? same d.f.) in lme4' using anova(model2,model), sometime I see nearly
? identical AIC values like model2=1479.6 and model=1479.5 and a very low chi
? sq. value like 0.1062, yet an extremely low p-value of <0.0001.  How would
? you reconcile this?  Should we be more concerned with looking for
? differences in AIC values of >3 when determing a better fit model, rather
? than looking at a p-value?
? 
? Secondly, I read on the glmm.wikidot.com/faq page that when testing for the
? significance of random effects, p values are conservative and are roughly
? half what is returned when performing LRTs.  Do you find that what Pinheiro
? and Bates (2000) states is sufficient to justify reporting the significance
? of random effects when reported p values are between 0.05 and 0.10?  And is
? it enough to convince you that is the case, especially when examining the
? raw data with this in mind?
? 
? Thank you,
? Jacob
? 
? 	[[alternative HTML version deleted]]
? 
? _______________________________________________
? R-sig-mixed-models at r-project.org mailing list
? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From segerfan83 at gmail.com  Mon Apr 14 16:08:48 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Mon, 14 Apr 2014 10:08:48 -0400
Subject: [R-sig-ME] Reconciling Near Identical AIC Values and Highly
 Significant P Value
In-Reply-To: <20140413190610.GB4290@info124.pharmacie.univ-paris5.fr>
References: <CAHe08Sg67+yf7G5UKx=_Gref-o3FLXkQi_rJ3riSMw8eJJfwYA@mail.gmail.com>
	<20140413190610.GB4290@info124.pharmacie.univ-paris5.fr>
Message-ID: <CAHe08ShqT+BH1EQWUCB2AKYkwNXh3L9YqJA79RmGhEbUL7Agfg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140414/a3982b32/attachment.pl>

From bbolker at gmail.com  Mon Apr 14 18:25:07 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 14 Apr 2014 12:25:07 -0400
Subject: [R-sig-ME] Reconciling Near Identical AIC Values and Highly
 Significant P Value
In-Reply-To: <CAHe08ShqT+BH1EQWUCB2AKYkwNXh3L9YqJA79RmGhEbUL7Agfg@mail.gmail.com>
References: <CAHe08Sg67+yf7G5UKx=_Gref-o3FLXkQi_rJ3riSMw8eJJfwYA@mail.gmail.com>	<20140413190610.GB4290@info124.pharmacie.univ-paris5.fr>
	<CAHe08ShqT+BH1EQWUCB2AKYkwNXh3L9YqJA79RmGhEbUL7Agfg@mail.gmail.com>
Message-ID: <534C0BE3.5020107@gmail.com>

On 14-04-14 10:08 AM, AvianResearchDivision wrote:
> Hi,
> 
> Thank you for your response.  How then would you proceed with testing two
> models that are not nested within each other?  I suppose I could compare
> AIC values, but is it correct to use the AIC values obtained using my
> initial method of anova(model2,model)?
> 
> Thank you,
> Jacob

   I think you might have to back up and think about what hypothesis
you're testing when you're comparing two non-nested models.  You could
consider Vuong's test
http://en.wikipedia.org/wiki/Vuong%27s_closeness_test ;
http://fisher.osu.edu/~schroeder.9/AMIS900/Vuong1989.pdf ...
alternatively, I do think comparing AICs makes sense.  AIC(model,model2)
will just give you a list of AIC values.  bbmle::AICtab(model,model2)
will give you a slightly prettier output.
   Keep the various limitations of AIC (asymptotic; assumes internal
points -- see http://glmm.wikidot.com/faq) in mind too.

  Ben Bolker


> 
> 
> On Sun, Apr 13, 2014 at 3:06 PM, Emmanuel Curis <
> emmanuel.curis at parisdescartes.fr> wrote:
> 
>> Hi,
>>
>> I may completely misunderstand your problem, but if you replace a
>> predictor variable by another one with the same number of parameters
>> (let's say, for instance, ?? length ?? by ?? surface ?? in a linear
>> regression), then
>>
>>  1) comparing AIC and likelihoods value is the same since the number
>>     of parameters does not change ;
>>     hence same AIC <=> predictors give equally good fits
>>
>>  2) chi-square tests is meaningless, since models are not nested.
>>     Here, I guess you have the very low p because the function uses a
>>     0-degrees of freedom [same number of parameters...]  khi-square,
>>     that is the constant 0, and any value other than 0 as a null
>>     probability, hence p = 0 < whatever you want...
>>
>> From 2), it results than comparing AIC is, between your two options,
>> the only one valid when models are not nested.
>>
>> For the second point, I don't know.
>>
>> Hope this helps,
>>
>> On Sun, Apr 13, 2014 at 02:52:20PM -0400, AvianResearchDivision wrote:
>> ?? Hi all,
>> ??
>> ?? When comparing identical models (only difference in predictor variable;
>> ?? same d.f.) in lme4' using anova(model2,model), sometime I see nearly
>> ?? identical AIC values like model2=1479.6 and model=1479.5 and a very low
>> chi
>> ?? sq. value like 0.1062, yet an extremely low p-value of <0.0001.  How
>> would
>> ?? you reconcile this?  Should we be more concerned with looking for
>> ?? differences in AIC values of >3 when determing a better fit model, rather
>> ?? than looking at a p-value?
>> ??
>> ?? Secondly, I read on the glmm.wikidot.com/faq page that when testing for
>> the
>> ?? significance of random effects, p values are conservative and are roughly
>> ?? half what is returned when performing LRTs.  Do you find that what
>> Pinheiro
>> ?? and Bates (2000) states is sufficient to justify reporting the
>> significance
>> ?? of random effects when reported p values are between 0.05 and 0.10?  And
>> is
>> ?? it enough to convince you that is the case, especially when examining the
>> ?? raw data with this in mind?
>> ??
>> ?? Thank you,
>> ?? Jacob
>> ??
>> ??       [[alternative HTML version deleted]]
>> ??
>> ?? _______________________________________________
>> ?? R-sig-mixed-models at r-project.org mailing list
>> ?? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> --
>>                                 Emmanuel CURIS
>>                                 emmanuel.curis at parisdescartes.fr
>>
>> Page WWW: http://emmanuel.curis.online.fr/index.html
>>
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From segerfan83 at gmail.com  Tue Apr 15 20:00:58 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Tue, 15 Apr 2014 14:00:58 -0400
Subject: [R-sig-ME] Individual coef() not congruent with panel plot of data
Message-ID: <CAHe08Sh8xE0O4MqpLQLrS19QSG93GwD+yRbDga0wYuVQabAVtg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140415/7e81e158/attachment.pl>

From anniehoen at gmail.com  Tue Apr 15 20:14:03 2014
From: anniehoen at gmail.com (Anne Hoen)
Date: Tue, 15 Apr 2014 14:14:03 -0400
Subject: [R-sig-ME] difficulty fitting a model with nlme()
Message-ID: <38659203-3C8C-4C7D-9B74-97292BE10AAD@gmail.com>

Hi, 

I'm working on fitting a model with nlme. I have done this before (with some very helpful advice from this forum and from the excellent Pinheiro & Bates book) on a similar dataset but I'm having troubles with a new set of data. 

I have outcomes measured on test and reference materials made over a series of dilutions and performed over a number of replicates. I want to fit a 3-parameter logistic function to the data (outcome vs dilution) and estimate the effect of test vs reference on the parameters with the replicate number included as a random effect.   

Here is my reproducible example:

library(nlme)

#Data:

x<-c(1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8)
y<-c(1.9302400,1.4943900,1.1843600,1.0083900,0.7697200,0.4876400,0.3005200,0.1877900,1.8066200,1.6685600,1.2516900,1.0409400,0.9356900,0.5950900,0.3552200,0.2214900,1.8101575,1.5273775,1.3062875,1.0022375,0.7663775,0.5120175,0.2972375,0.1930575,1.8740375,1.6142875,1.4112575,1.1021575,0.9438775,0.6029375,0.3582775,0.2234575,1.8090425,1.5115425,1.3347625,0.8812225,0.7459425,0.5252425,0.3252925,0.1809025,1.8420925,1.6119025,1.4094025,1.0518425,0.7759025,0.5792625,0.3502225,0.2061425,1.7288700,1.3824700,1.1691500,0.9555800,0.7044500,0.4966700,0.2938700,0.1374800,1.7447500,1.5038300,1.2124100,0.8477100,0.7546500,0.5371800,0.3137300,0.1925500,2.0704300,1.9151700,1.5235800,1.2380100,0.9800500,0.6039100,0.3916100,0.2459700,2.1507500,1.9126500,1.6317700,1.3445500,0.8570100,0.7079300,0.4359800,0.2923700,2.0717450,1.9443050,1.5423450,1.2771950,0.8258650,0.5431250,0.2090950,0.0916050,2.1291650,1.8801650,1.6445650,1.3246050,0.8991050,0.6972950,0.2931050,0.1032650,1.8966625,1.7421425,1.4608625,1.1705925,0.9664625,0.5481225,0.1529625,0.0761425,2.0282025,1.7920225,1.5809425,1.2019925,0.8361225,0.6382225,0.2437925,0.0803025,2.0854675,1.8130475,1.5290675,1.1757475,0.9437975,0.5506275,0.1783475,0.0795975,2.1994675,1.8947675,1.6205975,1.2414475,0.9887675,0.6347475,0.2842675,0.0842675)
testref<-c("ref","ref","ref","ref","ref","ref","ref","ref","test","test","test","test","test","test","test","test","ref","ref","ref","ref","ref","ref","ref","ref","test","test","test","test","test","test","test","test","ref","ref","ref","ref","ref","ref","ref","ref","test","test","test","test","test","test","test","test","ref","ref","ref","ref","ref","ref","ref","ref","test","test","test","test","test","test","test","test","ref","ref","ref","ref","ref","ref","ref","ref","test","test","test","test","test","test","test","test","ref","ref","ref","ref","ref","ref","ref","ref","test","test","test","test","test","test","test","test","ref","ref","ref","ref","ref","ref","ref","ref","test","test","test","test","test","test","test","test","ref","ref","ref","ref","ref","ref","ref","ref","test","test","test","test","test","test","test","test")
rep<-c("R1","R1","R1","R1","R1","R1","R1","R1","R1","R1","R1","R1","R1","R1","R1","R1","R2","R2","R2","R2","R2","R2","R2","R2","R2","R2","R2","R2","R2","R2","R2","R2","R3","R3","R3","R3","R3","R3","R3","R3","R3","R3","R3","R3","R3","R3","R3","R3","R4","R4","R4","R4","R4","R4","R4","R4","R4","R4","R4","R4","R4","R4","R4","R4","R5","R5","R5","R5","R5","R5","R5","R5","R5","R5","R5","R5","R5","R5","R5","R5","R6","R6","R6","R6","R6","R6","R6","R6","R6","R6","R6","R6","R6","R6","R6","R6","R7","R7","R7","R7","R7","R7","R7","R7","R7","R7","R7","R7","R7","R7","R7","R7","R8","R8","R8","R8","R8","R8","R8","R8","R8","R8","R8","R8","R8","R8","R8","R8")

data<-data.frame(x, y, testref, rep)
data<-groupedData(y~x | rep, data=data) #create a groupedData object

#And the model:

init<-getInitial(y~SSlogis(x, Asym, xmid, scal), data) #find some starting values
fit<-nlme(y~SSlogis(x, Asym, xmid, scal), 
          fixed=Asym + xmid + scal ~ testref-1, #reference vs test as a fixed effect
          random=list(rep=pdDiag(Asym+xmid+scal~1)), #random effect of replicate 
          data=data,
          start=c(init[1], 1, init[2], 1, init[3],1)
)


Interestingly, I don't get an error when I specify the fixed effect as: fixed=list(Asym ~ testref - 1, xmid ~ testref, scal ~ testref); however, this doesn't allow me to estimate the xmid and scal parameters for both the test and reference. i.e.:  
  
fit<-nlme(y~SSlogis(x, Asym, xmid, scal), 
            fixed=list(Asym~testref-1, xmid~testref, scal~testref), #reference vs test as a fixed effect
            random=list(rep=pdDiag(Asym+xmid+scal~1)), #random effect of replicate 
            data=data,
            start=c(init[1], 1, init[2], 1, init[3],1)
  )
summary(fit)
coef(fit)

Nor do I get an error when I specify the original model but remove all observations where x=1, i.e.:
  
data<-data[data$x!=1,]
init<-getInitial(y~SSlogis(x, Asym, xmid, scal), data) #find some starting values
fit<-nlme(y~SSlogis(x, Asym, xmid, scal), 
          fixed=Asym + xmid + scal ~ testref-1, #reference vs test as a fixed effect
          random=list(rep=pdDiag(Asym+xmid+scal~1)), #random effect of replicate 
          data=data,
          start=c(init[1], 1, init[2], 1, init[3],1)
)

summary(fit)
coef(fit)


I am confused. Is this a convergence problem? Is my data not suited to this modeling approach?

I am so stuck! Any help would be very appreciated. 

From rhubyjo at gmail.com  Mon Apr 14 14:32:47 2014
From: rhubyjo at gmail.com (Elizabeth Crone)
Date: Mon, 14 Apr 2014 08:32:47 -0400
Subject: [R-sig-ME] Reconciling Near Identical AIC Values and Highly
In-Reply-To: <mailman.3.1397469602.31070.r-sig-mixed-models@r-project.org>
References: <mailman.3.1397469602.31070.r-sig-mixed-models@r-project.org>
Message-ID: <027A00C5-046D-4F66-B574-92C26EFE8D93@gmail.com>

Be careful of the p-values generated by summary() or single-model anova() for a lm/GLM/lmer model.  They are rarely for the hypotheses I want to test.  The most idiot-proof way to get p-values is with Anova() for one model, or anova() for two.

I hope this resolves your discrepancies.

Elizabeth Crone

Sent from my iPhone

On Apr 14, 2014, at 6:00, <r-sig-mixed-models-request at r-project.org> wrote:

> Reconciling Near Identical AIC Values and Highly


From segerfan83 at gmail.com  Wed Apr 16 01:02:49 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Tue, 15 Apr 2014 19:02:49 -0400
Subject: [R-sig-ME] Individual coef() not congruent with panel plot of
	data
In-Reply-To: <CAE2eRByro0eN2Z-Ow5BnoSobzhqJj1unLWiB+k_8EJ+ny-ytvg@mail.gmail.com>
References: <CAHe08Sh8xE0O4MqpLQLrS19QSG93GwD+yRbDga0wYuVQabAVtg@mail.gmail.com>
	<CAE2eRByro0eN2Z-Ow5BnoSobzhqJj1unLWiB+k_8EJ+ny-ytvg@mail.gmail.com>
Message-ID: <CAHe08SjPDJ4Byt8PmZJpUD-VeMziO6tweaxCZ-jqqJRnnZLVkA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140415/330d9838/attachment.pl>

From segerfan83 at gmail.com  Wed Apr 16 01:55:36 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Tue, 15 Apr 2014 19:55:36 -0400
Subject: [R-sig-ME] Individual coef() not congruent with panel plot of
	data
In-Reply-To: <CAHe08Sh8xE0O4MqpLQLrS19QSG93GwD+yRbDga0wYuVQabAVtg@mail.gmail.com>
References: <CAHe08Sh8xE0O4MqpLQLrS19QSG93GwD+yRbDga0wYuVQabAVtg@mail.gmail.com>
Message-ID: <CAHe08SjK5sw3yjHDU33PZqKLEUN+B1VmDnfcBHYAsOr9YHQknA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140415/70e8b64d/attachment.pl>

From merklee at missouri.edu  Wed Apr 16 04:02:44 2014
From: merklee at missouri.edu (Ed Merkle)
Date: Tue, 15 Apr 2014 21:02:44 -0500
Subject: [R-sig-ME] Reconciling Near Identical AIC Values and Highly
 Significant P Value (Ben Bolker)
In-Reply-To: <mailman.3.1397556001.24959.r-sig-mixed-models@r-project.org>
References: <mailman.3.1397556001.24959.r-sig-mixed-models@r-project.org>
Message-ID: <534DE4C4.7020107@missouri.edu>

Ben et al,
>     I think you might have to back up and think about what hypothesis
> you're testing when you're comparing two non-nested models.  You could
> consider Vuong's test
> http://en.wikipedia.org/wiki/Vuong%27s_closeness_test  ;
> http://fisher.osu.edu/~schroeder.9/AMIS900/Vuong1989.pdf  ...
> alternatively, I do think comparing AICs makes sense.  AIC(model,model2)
> will just give you a list of AIC values.  bbmle::AICtab(model,model2)
> will give you a slightly prettier output.
>     Keep the various limitations of AIC (asymptotic; assumes internal
> points -- seehttp://glmm.wikidot.com/faq) in mind too.
>
>    Ben Bolker
>
>
Slightly off topic, but speaking of Vuong's tests, I wonder about the 
ease with which we could obtain the necessary output from lme4. To carry 
out the tests, I believe we need individual observations' contributions 
to both the likelihood and gradient (evaluated at the ML estimates).  I 
believe these are not simple to obtain from models fit in lme4, but I 
wonder whether you have any hints here.

Thanks,
Ed

-- 
Ed Merkle, PhD
Assistant Professor
Department of Psychological Sciences
University of Missouri
Columbia, MO, USA 65211


From david.sears01 at gmail.com  Fri Apr 18 00:37:26 2014
From: david.sears01 at gmail.com (David Sears)
Date: Thu, 17 Apr 2014 18:37:26 -0400
Subject: [R-sig-ME] sum coding, omnibus tests for GLMMs, and pvalues for LMMs
Message-ID: <001801cf5a8d$9c127e60$d4377b20$@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140417/607fa826/attachment.pl>

From gf3 at pdx.edu  Fri Apr 18 05:40:37 2014
From: gf3 at pdx.edu (Gerasimos Fergadiotis)
Date: Thu, 17 Apr 2014 20:40:37 -0700
Subject: [R-sig-ME] Power Analysis for Linear Mixed Model with Covariates
Message-ID: <6644A3EA-E59B-46F6-B347-01F57F1CD03C@pdx.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140417/fc8c8d0a/attachment.pl>

From bbolker at gmail.com  Fri Apr 18 06:27:03 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 18 Apr 2014 00:27:03 -0400
Subject: [R-sig-ME] Power Analysis for Linear Mixed Model with Covariates
In-Reply-To: <6644A3EA-E59B-46F6-B347-01F57F1CD03C@pdx.edu>
References: <6644A3EA-E59B-46F6-B347-01F57F1CD03C@pdx.edu>
Message-ID: <5350A997.3080702@gmail.com>

On 14-04-17 11:40 PM, Gerasimos Fergadiotis wrote:
> Hello,
> 
> I was wondering if someone could point me to the right direction to
> conduct a power analysis for a design that includes (Levels): One
> Within Subjects Factor (Time(4)), Two Between Subject Factors
> (Treatment(2), Dosage(2)), one covariate (Language Skill at Baseline
> (4)). Random Effects for Intercept and Slope are typically seen in
> similar studies so I would like to include them as well. I spent some
> time with PASS 13 but it will not allow me to incorporate the
> covariate so I turned to R. Any ideas, thoughts, suggestions would be
> greatly appreciated.


Is http://rpubs.com/bbolker/11703  at all helpful?


From rdiaz02 at gmail.com  Fri Apr 18 13:25:24 2014
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Fri, 18 Apr 2014 13:25:24 +0200
Subject: [R-sig-ME] sum coding, omnibus tests for GLMMs,
	and pvalues for LMMs
In-Reply-To: <001801cf5a8d$9c127e60$d4377b20$@gmail.com>
References: <001801cf5a8d$9c127e60$d4377b20$@gmail.com>
Message-ID: <87bnvy3mmz.fsf@gmail.com>

Dear David,

A long and rambling answer to your first and second questions:

On Thu, 01-01-1970, at 01:00, David Sears <david.sears01 at gmail.com> wrote:
> Hello list,
>
>  
>
> I have a GLMM with a within-subjects fixed factor (cadcats = 5 levels) and a
> between-subjects fixed factor (trainings = 2 levels) and a maximal random
> effects structure. I would like to run an .ANOVA-style. GLMM, and Barr
> (2013) has said that one should use deviation coding to do this. I have 4
> questions (my R code and model summary is below the questions).
>
>  
>
> 1.      I have used sum coding (contr.sum), but deviation coding appears to
> be slightly different. Moreover, sum coding results in the loss of the last
> level of my factor (because it is a contrast, so n-1 groups means I can only
> have 4 contrasts). How do I deviation code my factors, and will it also
> result in the loss of the last level of my factor? I would prefer a method
> that allows me to retain all levels in the summary.


contr.sum and deviation coding are essentially the same thing, though
deviation coding as used by Barr et al. (and other authors) uses -.5 and .5
instead of 1 and -1.

These links (the first by Barr) might help here (note that the first does
differentiate between sum and deviation, whereas the second does not)

http://talklab.psy.gla.ac.uk/tvw/catpred/
http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm


Note that the first link sets A1 as -1 (or -5), whereas R, by default, will
set A1 to 1 and A2 to -1 with contr.sum.



You might want to understand how you can get, say, the cell mean from the
coefficients (whether you are using as reference a particular group or the
mean) as well as the consequences of coding for interpreting effects, for
instance the meaning of the footnote of p. 259 in Barr et al: "For
higher-order designs, treatment and deviation coding schemes will lead to
different interpretations for lower- order effects (simple effects for
contrast coding and main effects for deviation coding)".

I find the explanation of McCullagh and Nelder's "Generalized Linear
Models" (2nd ed), in their section 3.5.2 (pp. 65 to 68), extremely
clear. The book by Fox and Weisberg, "An R companion to applied regression"
(2nd ed), section 4.6, pp. 213 and ff, (note that they refer to this as
"deviation coding", by the way, even if using 1, -1) also explains the use
of contr.sum. 


Whether you use deviation or sum, in both cases you "loose" the last level
(since it is redundant). You can of course get the estimate by just adding
all the rest (e.g., with contr.sum do
-sum(model$coefficients[for.this.factor])) or you can just refit the model
after changing the ordering of the factor levels :-).


By the way, and having mentioned Fox's book, I always use his "contr.Sum"
(from package car), when using deviation/sum coding, instead of contr.sum,
because of the labeling of the contrasts (and the flexibility of getting
things labeled with parenthesis, brackets, or whatever ---see
?decorate.contrasts and ?decorate.contr.Sum)



>
> 2.      How do I interpret the summary for variables that have been sum
> coded? Florian Jaeger has said that .for sum coding. the coefficients
> correspond to main effects.. In my output does this mean that a significant
> effect of cadcat1 means that the estimated mean for cadcat1 is significantly
> different from the grand mean, which is represented by the intercept? 
>

I think there are two issues here:

a) whether cadcat (cadcat as "a whole", not just cadcat1) should or should
not be in the model, together with its interaction with trainings.

b) interpreting cadcat1.


a) Is not obvious, and you'll probably want to do this carefully (as
explained in Barr or the FAQ --- http://glmm.wikidot.com/faq--- or Ben
Bolker's notes from http://ms.mcmaster.ca/~bolker/classes/s4c03/, or
...). Note that there are four comparisons of marginal means (cadcat1 to 4)
and two of those have very large p-values. Likewise, the interaction does
not seem very large.

So I wonder if a model without cadcat (AND without the interaction, of
course) would be good enough.



b) Provided cadcat is kept in the model (and the interaction is kept, too),
then the results tell you that the marginal mean of cadcat1 is different
from the overall mean.

However, even if contr.sum allows you to estimate the marginal effect, it
is up to you to decide if that is something you want to interpret if there
are large interactions , specially if there are reversals of effects (what
is "very", "large", etc, can be highly context dependent).

The coefficient cadcats1:trainings1 is -0.71485, which means that a subject
with trainings1 and cadcats1 deviates from the value we would predict based
on the sum of the  intercept (2.64) the cadcats1 effect (1.015) and the
trainings1 effect (0.79), by -0.715.


Alternatively, you can look at it as a cell means model:

- the mean for a subject with cadcats1 and trainings1 is

2.6 + 1.015 + 0.79 - 0.71485 ~ 3.6

- the mean for a subject with cadcats1 and trainings2

2.6 + 1.015 - 0.79 + 0.71485 ~ 3.5

- cadcats2 and trainings1:

2.6 + 0.59 + 0.79 - 0.48 ~ 3.5

- cadcat2 and trainings2:

2.6 + 0.59 - 0.79 + 0.48 ~ 2.88


etc


Best,


R.

P.D. Just before hitting send I noticed you are using a binomial model, so
where I say "mean" you should understand the logit, etc.




> 3.      Is there a way to do omnibus tests for GLMMs (like pamer.fnc does
> for LMMs)?
>
> 4.      My last question is perhaps the most frustrating for the experts on
> this list to answer, but I would greatly appreciate your help. I would also
> like to report significance levels in the tables that summarize my LMMs
> (it.s a priming study, with GLMMs for accuracy and LMMs for logRT), but of
> course lmer does not provide pvalues in the model summary. In spite of the
> authors discontinuing mcmcsamp, most of the priming studies from the last 2
> years appearing in the Journal of Memory and Language still report pvalues
> for the model estimates (see, for example, Table 5 from Nooteboom & Quen.,
> 2013), but I don.t know how they did this. I would omit the pvalues, but I
> am publishing in music psychology, and these models are only just beginning
> to appear in our field, so any help or code people would be willing to
> provide would be much appreciated.
>
>  
>
> David Sears
>
> PhD Candidate, Music Theory
>
> Music Perception and Cognition Lab (MPCL)
>
> Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT)
> Schulich School of Music, McGill University
>
>  
>
>  
>
> CODE 
>
>  
>
> # Create factor variables.
>
> acc$subject <- factor(acc$subject)
>
> acc$training <- factor(acc$training)
>
> acc$correct <- factor(acc$correct)
>
> acc$cadcat <- factor(acc$cadcat)
>
>  
>
> # Create IVs with sum coding.
>
> acc$trainings <- acc$training
>
> contrasts(acc$trainings)=contr.sum(2)
>
> acc$cadcats <- acc$cadcat
>
> contrasts(acc$cadcats)=contr.sum(5)
>
>  
>
> # Maximal model.
>
> cadsum.acc <-
> glmer(correct~cadcats*trainings+(cadcats|subject)+(trainings|stimulus),data=
> acc,family="binomial",control=glmerControl(optCtrl=list(maxfun=100000)))
>
>  
>
>  
>
>> summary(cadsum.acc)
>
> Generalized linear mixed model fit by maximum likelihood ['glmerMod']
>
> Family: binomial ( logit )
>
> Formula: correct ~ cadcats * trainings + (cadcats | subject) + (trainings |
> stimulus) 
>
>    Data: acc 
>
>  
>
>       AIC       BIC    logLik  deviance 
>
>  789.5965  931.7901 -366.7983  733.5965 
>
>  
>
> Random effects:
>
> Groups   Name        Variance Std.Dev. Corr                   
>
>  stimulus (Intercept) 0.84853  0.9212                          
>
>           trainings1  0.01080  0.1039   -0.85                  
>
>  subject  (Intercept) 0.43439  0.6591                          
>
>           cadcats1    0.55165  0.7427   -0.34                  
>
>           cadcats2    0.76990  0.8774    0.31  0.52            
>
>           cadcats3    1.32294  1.1502   -0.09 -0.23 -0.85      
>
>           cadcats4    0.06946  0.2635   -0.42 -0.69 -0.64  0.14
>
> Number of obs: 1186, groups: stimulus, 40; subject, 30
>
>  
>
> Fixed effects:
>
>                     Estimate Std. Error z value Pr(>|z|)    
>
> (Intercept)          2.63867    0.22993  11.476  < 2e-16 ***
>
> cadcats1             1.01518    0.44687   2.272   0.0231 *  
>
> cadcats2             0.59321    0.42280   1.403   0.1606    
>
> cadcats3             0.54040    0.44940   1.202   0.2292    
>
> cadcats4            -0.69632    0.36178  -1.925   0.0543 .  
>
> trainings1           0.79023    0.17559   4.500 6.78e-06 ***
>
> cadcats1:trainings1 -0.71485    0.32224  -2.218   0.0265 *  
>
> cadcats2:trainings1 -0.48321    0.30379  -1.591   0.1117    
>
> cadcats3:trainings1 -0.22673    0.33094  -0.685   0.4933    
>
> cadcats4:trainings1  0.08375    0.21199   0.395   0.6928    
>
> ---
>
> Signif. codes:  0 .***. 0.001 .**. 0.01 .*. 0.05 ... 0.1 . . 1
>
>  
>
> Correlation of Fixed Effects:
>
>             (Intr) cdcts1 cdcts2 cdcts3 cdcts4 trnng1 cdc1:1 cdc2:1 cdc3:1
>
> cadcats1     0.047                                                        
>
> cadcats2     0.078 -0.187                                                 
>
> cadcats3    -0.006 -0.284 -0.368                                          
>
> cadcats4    -0.123 -0.282 -0.239 -0.187                                   
>
> trainings1   0.063 -0.035 -0.004  0.021 -0.004                            
>
> cdcts1:trn1 -0.037  0.006  0.011 -0.010  0.004  0.040                     
>
> cdcts2:trn1 -0.005  0.012  0.054 -0.029 -0.015  0.151 -0.116              
>
> cdcts3:trn1  0.022 -0.009 -0.028  0.100 -0.031 -0.035 -0.291 -0.467       
>
> cdcts4:trn1 -0.006  0.006 -0.018 -0.039  0.103 -0.256 -0.317 -0.244 -0.110
>
>  
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Ramon Diaz-Uriarte
Department of Biochemistry, Lab B-25
Facultad de Medicina
Universidad Aut?noma de Madrid 
Arzobispo Morcillo, 4
28029 Madrid
Spain

Phone: +34-91-497-2412

Email: rdiaz02 at gmail.com
       ramon.diaz at iib.uam.es

http://ligarto.org/rdiaz


From gf3 at pdx.edu  Sat Apr 19 00:21:20 2014
From: gf3 at pdx.edu (Gerasimos Fergadiotis)
Date: Fri, 18 Apr 2014 15:21:20 -0700
Subject: [R-sig-ME] Power Analysis for Linear Mixed Model with Covariates
In-Reply-To: <mailman.5.1397815202.6759.r-sig-mixed-models@r-project.org>
References: <mailman.5.1397815202.6759.r-sig-mixed-models@r-project.org>
Message-ID: <CF39B397-848A-44CF-969D-D231A70B26D6@pdx.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140418/3b8529e5/attachment.pl>

From steve.walker at utoronto.ca  Sat Apr 19 00:44:09 2014
From: steve.walker at utoronto.ca (Steve Walker)
Date: Fri, 18 Apr 2014 18:44:09 -0400
Subject: [R-sig-ME] Power Analysis for Linear Mixed Model with Covariates
In-Reply-To: <CF39B397-848A-44CF-969D-D231A70B26D6@pdx.edu>
References: <mailman.5.1397815202.6759.r-sig-mixed-models@r-project.org>
	<CF39B397-848A-44CF-969D-D231A70B26D6@pdx.edu>
Message-ID: <5351AAB9.9080200@utoronto.ca>

On 2014-04-18, 6:21 PM, Gerasimos Fergadiotis wrote:
> Ben,
>
> Thank you for the link. It is very helpful as it can serve as a map for achieving my goal.
> Based on my (limited) understanding of Linear Mixed Models, the core of your simulation is a model of that looks at fixed effects of two treatments while accounting for random effects at the individual level (intercept only) and at the observation level (intercept only). Random effects are not dependent and a slope is not included in the model.
>
> I started modifying the code to tailor it to my design. The first modification I wanted to do was build a model for a continuous dependent variable which would be otherwise similar to the one you have. I changed and ran the following code:
>
> expdat <- expand.grid(kid = factor(1:30), Time = factor(1:4), Treat = c("XTx", "BAU"))
> expdat$obs <- factor(seq(nrow(expdat)))
>
> set.seed(101)
> nsim <- 20
> beta <- c(100, -7)
> theta <- c(15, 15)
>
> ss <- simulate(~Treat + (1 | kid) + (1| Time), nsim = nsim, family = gaussian, weights = rep(25, nrow(expdat)), newdata = expdat, newparams = list(theta = theta, beta = beta))
>
> I received the following error message:
>
> In setParams(object, newparams) :
>    some parameters not specified in setParams()
>
> Assuming beta includes the intercept and fixed effect for treatment, it is not clear to me how theta should be specified and further what the implications are after choosing Family = gaussian (weights?).
>
> Any thoughts?

You need to specify the residual scale parameter, sigma.  So that would 
be something like:

newparams = list(theta = theta, beta = beta, sigma = 1))

Also, I _think_ you need to also include a response variable in your 
newdata.

Cheers,
Steve

>
> Thank you,
> Gerasimos
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Gerasimos Fergadiotis, PhD., CCC-SLP
> Assistant Professor
> Speech & Hearing Sciences
> 724 SW Harrison St., RM 84B
> Portland State University
> Portland, Oregon 97201
> Phone: 503.725.2217
> Fax: 503.725.9171
> Web page: http://aaldresearch.wix.com/aald
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>
>>
>> Message: 3
>> Date: Fri, 18 Apr 2014 00:27:03 -0400
>> From: Ben Bolker <bbolker at gmail.com>
>> To: r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Power Analysis for Linear Mixed Model with
>> 	Covariates
>> Message-ID: <5350A997.3080702 at gmail.com>
>> Content-Type: text/plain; charset=ISO-8859-1
>>
>> On 14-04-17 11:40 PM, Gerasimos Fergadiotis wrote:
>>> Hello,
>>>
>>> I was wondering if someone could point me to the right direction to
>>> conduct a power analysis for a design that includes (Levels): One
>>> Within Subjects Factor (Time(4)), Two Between Subject Factors
>>> (Treatment(2), Dosage(2)), one covariate (Language Skill at Baseline
>>> (4)). Random Effects for Intercept and Slope are typically seen in
>>> similar studies so I would like to include them as well. I spent some
>>> time with PASS 13 but it will not allow me to incorporate the
>>> covariate so I turned to R. Any ideas, thoughts, suggestions would be
>>> greatly appreciated.
>>
>>
>> Is http://rpubs.com/bbolker/11703  at all helpful?
>>
>>
>>
>> ------------------------------
>>
>> _______________________________________________
>> R-sig-mixed-models mailing list
>> R-sig-mixed-models at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>> End of R-sig-mixed-models Digest, Vol 88, Issue 19
>> **************************************************
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


From nelida.villasenor at anu.edu.au  Sat Apr 19 10:46:54 2014
From: nelida.villasenor at anu.edu.au (Nelida Villasenor)
Date: Sat, 19 Apr 2014 08:46:54 +0000
Subject: [R-sig-ME] Model selection GLM vs. GLMMs
Message-ID: <5f5ce6a2958d4d409516e689386a7333@SINPR06MB188.apcprd06.prod.outlook.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140419/f4a19374/attachment.pl>

From gf3 at pdx.edu  Sun Apr 20 04:29:09 2014
From: gf3 at pdx.edu (Gerasimos Fergadiotis)
Date: Sat, 19 Apr 2014 19:29:09 -0700
Subject: [R-sig-ME] Power Analysis for Linear Mixed Model with Covariates
In-Reply-To: <mailman.8924.1397897234.4563.r-sig-mixed-models@r-project.org>
References: <mailman.8924.1397897234.4563.r-sig-mixed-models@r-project.org>
Message-ID: <D700DF3D-0005-400D-868F-D79F398830B3@pdx.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140419/d2533a6a/attachment.pl>

From bbolker at gmail.com  Mon Apr 21 02:15:04 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 21 Apr 2014 00:15:04 +0000 (UTC)
Subject: [R-sig-ME] Model selection GLM vs. GLMMs
References: <5f5ce6a2958d4d409516e689386a7333@SINPR06MB188.apcprd06.prod.outlook.com>
Message-ID: <loom.20140421T020951-335@post.gmane.org>

Nelida Villasenor <nelida.villasenor at ...> writes:

> 
> Dear all,
 
> I'm performing model selection based on AICc on a set of GLMMs that
> only vary in their fixed effects. The data comes from 12 transects
> (5 measures along each transect), then each transect is modelled as
> a random effect "+(1|transect)". As the response variable was
> proportions (presences/n), I fitted the models using binomial family
> and the total number of points (n) as weights.
 
> Given that some models had boundary problems

  meaning singular fits (estimated zero variances and/or +/- 1 correlations
and/or values of estimated theta=0) ?

> I ran the model
> selection on a set of GLMs instead of GLMMs. The results were almost
> identical in terms of the list of models with the highest support
> (for 7 response variables where model selection was performed
> independently).
 
> I'm wondering which approach is correct? Or, as my results show, it
> does not really matter, because the random effect does not change in
> my GLMMs?

  I think you would find a bit of disagreement among experts about the
best procedure -- whether it would be to drop random effects until you
got a sensible non-singular fit, or to keep them in even though
they're singular.  Keep in mind that you should get the same estimates
with a GLM or a GLMM if the variance estimates are all zero ... Since
it doesn't sound as though it affects your einference/model selection
on the fixed effects, I would say you could choose either approach
(and explain clearly what you did).

  Ben Bolker


From bbolker at gmail.com  Mon Apr 21 02:41:50 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 21 Apr 2014 00:41:50 +0000 (UTC)
Subject: [R-sig-ME] Power Analysis for Linear Mixed Model with Covariates
References: <mailman.8924.1397897234.4563.r-sig-mixed-models@r-project.org>
	<D700DF3D-0005-400D-868F-D79F398830B3@pdx.edu>
Message-ID: <loom.20140421T022821-689@post.gmane.org>

Gerasimos Fergadiotis <gf3 at ...> writes:

>  Thank you all for your help so far.  Having as a starting point Ben
> Bolker's Code (http://rpubs.com/bbolker/11703), I have put together
> the following code that will allow me to generate data for a LMM
> with two fixed effects (Treatment and Time), their interaction, and
> and random intercepts and slopes for each participant.
 
expdat <- expand.grid(kid = factor(1:500), Time = factor(1:4), 
   Treat = c("XTx", "BAU"))
expdat$obs <- factor(seq(nrow(expdat)))
set.seed(101)
nsim <- 20
beta <- c(100, -7, 8, 15, 20)
theta <- c(15, 15, 15, 15, 1, 0, 0, 0, 0, 0)
ss <- simulate(~Treat + Time + (Time | kid), nsim = nsim, 
    family = gaussian, weights = rep(25,
nrow(expdat)), newdata = expdat, 
   newparams = list(theta = theta, beta = beta, sigma = 1))
expdat$Outcome <- ss[, 1]
fit1 <- lmer(Outcome ~Treat + Time + Treat*Time + (Time | kid), 
    data = expdat)

> I am puzzled now with who to specify "theta" to give specify
> correlational matrices for the time points. How is theta transformed
> to the correlational table that I see in the model output? I am
> assuming that theta corresponds to ?? to define the variance
> covariance matrix for the random effects.

theta is the "lower cholesky factor" of the (SCALED) variance-covariance
matrix (i.e. the random-effects vcov matrix divided by the (scalar) residual
variance): that is, if you have (t1,t2,t3) for the theta-vector of
a 2x2 var-cov matrix, the var-cov matrix will be

  ( t1  0  )  (  t1  t2  )   =    (  t1^2  t1*t2 )
  ( t2 t3  )  (   0  t3  )        (  t1*t2 t3^2  )

the results are similar (but much more complex ...) for the 4-by-4
var-cov matrix you're going to use here.   If you're feeling lazy
you can feed this to Wolfram Alpha:

{{t1, 0, 0, 0 }, {t2, t3, 0, 0}, {t4, t5, t6, 0},{t7,t8,t9,t10}}  
 {{t1, t2, t4, t7 }, {0, t3, t5, t8}, {0, 0, t6, t9},{0,0,0,t10}} 

You can also specify the variance-covariance matrix you want
and use cholesky decomp, e.g.

m <- matrix(1,nrow=4,ncol=4)
diag(m) <- 3
lc <- t(chol(m))
lc[lower.tri(lc,diag=TRUE)]  ## desired theta vector
lc %*% t(lc)

Note that 

> A second question is how I should establish
> the magnitude of sigma as it appears to play
> a significant role in > the simulation.

  You just have to figure out what's a sensible value,
or a range of sensible values, for your system.


From bradleyd at uoguelph.ca  Wed Apr 16 20:28:37 2014
From: bradleyd at uoguelph.ca (David Bradley)
Date: Wed, 16 Apr 2014 14:28:37 -0400
Subject: [R-sig-ME] MCMC prior issues
Message-ID: <CAE6kkfPna8636b8pBp9Ab5MbXsgXX=jFY-aCPZT+spLb+8yCEA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140416/cc625d51/attachment.pl>

From wzwei1636 at 163.com  Sat Apr 19 09:46:10 2014
From: wzwei1636 at 163.com (=?UTF-8?B?546L5ZSv?=)
Date: Sat, 19 Apr 2014 15:46:10 +0800 (CST)
Subject: [R-sig-ME] hard in lmer(),how to set parameter in lmer()
Message-ID: <394223be.5cb3.14578f31ddd.Coremail.wzwei1636@163.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140419/c6723a1c/attachment.pl>

From javanvonherp at gmail.com  Mon Apr 21 19:52:30 2014
From: javanvonherp at gmail.com (Javan Bauder)
Date: Mon, 21 Apr 2014 13:52:30 -0400
Subject: [R-sig-ME] Installing an old version of lme4 (v 0.999999-2)
Message-ID: <CAJKR=GPqYzdZ-bwpSKAEiYndwU97E=kHLCovqoDWOGXvkWvyCA@mail.gmail.com>

I am trying to install an old version of lme4 (v. 0.999999-2) in R
2.15.2 on my Windows 7 computer. The newer versions of lme4 do not
support the analyses I am trying to conduct. I saved the packaged
archive file (lme4_0.999999-2(1).tar) to my hard drive and then used
RStudio's "Install from Package Archive File" to attempt to install
the package. However, I received the following error message:



> install.packages("~/R/lme4_0.999999-2(1).tar.gz", repos = NULL, type = "source")

Installing package(s) into ?C:/Users/JBauder/Documents/R/win-library/2.15?

(as ?lib? is unspecified)

* installing *source* package 'lme4' ...

** package 'lme4' successfully unpacked and MD5 sums checked

** libs



*** arch - i386

ERROR: compilation failed for package 'lme4'

* removing 'C:/Users/JBauder/Documents/R/win-library/2.15/lme4'

Warning in install.packages :

  running command 'C:/PROGRA~1/R/R-215~1.2/bin/x64/R CMD INSTALL -l
"C:/Users/JBauder    /Documents/R/win-library/2.15"
"C:/Users/JBauder/Documents/R/lme4_0.999999-2(1).tar.gz"' had status 1

Warning in install.packages :

  installation of package
?C:/Users/JBauder/Documents/R/lme4_0.999999-2(1).tar.gz? had non-zero
exit status



Does anyone have any ideas of how to get around this error?



Thanks,

J.Bauder


From bbolker at gmail.com  Tue Apr 22 03:53:13 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 22 Apr 2014 01:53:13 +0000 (UTC)
Subject: [R-sig-ME] Installing an old version of lme4 (v 0.999999-2)
References: <CAJKR=GPqYzdZ-bwpSKAEiYndwU97E=kHLCovqoDWOGXvkWvyCA@mail.gmail.com>
Message-ID: <loom.20140422T035012-920@post.gmane.org>

Javan Bauder <javanvonherp at ...> writes:

> 
> I am trying to install an old version of lme4 (v. 0.999999-2) in R
> 2.15.2 on my Windows 7 computer. The newer versions of lme4 do not
> support the analyses I am trying to conduct. 

  Why not? (Just curious.)  Missing mcmcsamp() ?

> I saved the packaged
> archive file (lme4_0.999999-2(1).tar) to my hard drive and then used
> RStudio's "Install from Package Archive File" to attempt to install
> the package. However, I received the following error message:
> 
> > install.packages("~/R/lme4_0.999999-2(1).tar.gz", 
>  repos = NULL, type = "source")

  [snip]

> *** arch - i386
> 
> ERROR: compilation failed for package 'lme4'
> 
> * removing 'C:/Users/JBauder/Documents/R/win-library/2.15/lme4'
> 
> Warning in install.packages :
> 
>   running command 'C:/PROGRA~1/R/R-215~1.2/bin/x64/R CMD INSTALL -l
> "C:/Users/JBauder    /Documents/R/win-library/2.15"
> "C:/Users/JBauder/Documents/R/lme4_0.999999-2(1).tar.gz"' had status 1
> 
> Warning in install.packages :
> 
>   installation of package
> ?C:/Users/JBauder/Documents/R/lme4_0.999999-2(1).tar.gz? had non-zero
> exit status
> 

 [snip]

  Do you have all of the tools installed that are required to
build binary packages in R?

  Windows binary versions of lme4.0 (a backward compatibility
version of lme4) are available from

http://lme4.r-forge.r-project.org/repos/bin/windows/contrib/2.15/


From bbolker at gmail.com  Tue Apr 22 03:57:44 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 22 Apr 2014 01:57:44 +0000 (UTC)
Subject: [R-sig-ME] hard in lmer(),how to set parameter in lmer()
References: <394223be.5cb3.14578f31ddd.Coremail.wzwei1636@163.com>
Message-ID: <loom.20140422T035324-646@post.gmane.org>

?? <wzwei1636 at ...> writes:

>  Dear professor: I apologize for disturbing you! I'm appreciating
> your package 'lme4()' in the R ,because it's very useful when we
> deal with the linear mixed model. However,I get struck in doing my
> Master's degree paper , I'm hoping that you can help me.

  Can you possibly repost your question using only standard ASCII
characters that don't get mangled in translation to the mailing list/
my computer?

>        Y = X*?? + B*a + ??   is a linear mixed model , 
  X : dim=n*k .  
  ??: dim=k*1 represent fixed effects .
  B :dim=n*(n-2) .  
  a: dim=p*1 represent random effects with a ??? N(0, ??I). 

For example, n=100,k=15 , X and B is known,how can I use lmer() to get
 ?? and ?? .How to set parameters in lmer()?

  You should take a look at the ?modular help page for lme4: you can
set up a bare-bones model and then modify X and Z (corresponding to
X and B in your notation) as you like.

  You might have trouble fitting a reasonable model
with k=15 fixed-effect parameters with only n=100 observations ...


From bbolker at gmail.com  Tue Apr 22 04:00:55 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 22 Apr 2014 02:00:55 +0000 (UTC)
Subject: [R-sig-ME] MCMC prior issues
References: <CAE6kkfPna8636b8pBp9Ab5MbXsgXX=jFY-aCPZT+spLb+8yCEA@mail.gmail.com>
Message-ID: <loom.20140422T035842-846@post.gmane.org>

David Bradley <bradleyd at ...> writes:

> I am trying to use the MCMCglmm package to run a bivariate plasticity model
> to assess plasticity in the timing of egg laying and the relationship with
> reproductive success. The issue I am having is with the prior
> specification, which keep returning errors.
> 
> #The prior I am using is a flat prior:
> 
> prior.bi<-list(R = list(V = diag(2), nu = 0.002), G = list(G1 = list(V =
> diag(4), nu = 0.002)))

  [snip]
 
> *Error in MCMCglmm(cbind(savg.nf, sld) ~ trait - 1 +
> trait:st1 + trait:factor(min.age) +  :ill-conditioned G/R structure: use
> proper priors if you haven't or rescale data if you have*


> I thought that I was using a proper prior, and I have already scaled the
> data. What is the meaning of "ill-conditioned G/R structure"?
> 

  I'm not sure (a reproducible example would be nice), but have
you tried increasing nu to something larger and seeing if that
fixes the problem?  That may not necessarily make you happy --
you might prefer a flatter prior -- but that would indicate that
your structure is OK, but your priors are too flat ...


From yulya258 at yahoo.com  Tue Apr 22 09:06:33 2014
From: yulya258 at yahoo.com (Yla Savh)
Date: Tue, 22 Apr 2014 00:06:33 -0700 (PDT)
Subject: [R-sig-ME] Model specification (fixed and random effects)?
Message-ID: <1398150393.79692.YahooMailNeo@web162703.mail.bf1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140422/b0cbafc1/attachment.pl>

From Thierry.ONKELINX at inbo.be  Tue Apr 22 09:56:03 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 22 Apr 2014 07:56:03 +0000
Subject: [R-sig-ME] Model specification (fixed and random effects)?
In-Reply-To: <1398150393.79692.YahooMailNeo@web162703.mail.bf1.yahoo.com>
References: <1398150393.79692.YahooMailNeo@web162703.mail.bf1.yahoo.com>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A29C4B@inbomail.inbo.be>

Dear Julia,

Your model specification is OK if you assume that there is a linair trend along number of surgeries and tests.

The correlation should be  corAR1(form=~test_time|id/surgery_number)

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Yla Savh
Verzonden: dinsdag 22 april 2014 9:07
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Model specification (fixed and random effects)?

Hello!

I would appreciate any help with my model design. Each subject had a few surgeries (from 1 to 5) ("surgery number"). These subjects had a test taken a few times after each surgery (from 1 to 6 times) ("test time"). There was supposed to be a decrease in the dependent variable (test_results) immediately after each surgery.  I coded both variables as continuous (surgery_number and test_time).

I wonder if I specified fixed and random effects correctly. I also think there might be some correlation between the surgery numbers but I am not sure about how to specify it in the model.

x<-lme(test_results~ surgery_number*test_time, random=~1|id/surgery_number) s12<-update(x, correlation=corAR1(form=~1|id/surgery_number))

Thanks!
Julia

        [[alternative HTML version deleted]]

* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From javanvonherp at gmail.com  Tue Apr 22 15:56:18 2014
From: javanvonherp at gmail.com (Javan Bauder)
Date: Tue, 22 Apr 2014 09:56:18 -0400
Subject: [R-sig-ME] Mixed-effects conditional logistic regression in lme4 v.
	1.0-4 and above
Message-ID: <CAJKR=GOP1stucApHSg_T8ECezN9mzKdezHGSyc5aWTK3YQXTAg@mail.gmail.com>

Hi Dr. Bolker

Thank you for responding to my post about installing an older version
of lme4. I wanted to respond to your question about why I need an
older version for my analyses.

The analysis I am running is a mixed-effects conditional logistic
regression. My colleagues and I are using these models to conduct
habitat selection analyses with wildlife telemetry data where each
telemetry point is paired with a measure of availability unique to
that point. We then difference the value of our independent variable
at the telemetry point (used) with the measure of availability for
that variable and fit a no-intercept logistic regression model to
those differences and use individual animal as a random slope effect.
We have used the following model structure:

lmer(1 ~ ?1+diff100+?(-1+diff100+?|Individual), data=data, family=?binomial?)

Whenever we have used this specification in the newer versions of lme4
(v 1.0-4 and above) we receive an error. For example, when using lmer
and lme4 v 1.0-4, I receive the following:

> mod1<-lmer(status~-1+Density+(-1+Density|Name),data=input,family='binomial',weights=Weight)
Error in function (fr, X, reTrms, family, nAGQ = 1L, verbose = 0L,
control = glmerControl(),  :
  Response is constant - cannot fit the model
In addition: Warning message:
In lmer(status ~ -1 + Density + (-1 + Density | Name), data = input,  :
  calling lmer with 'family' is deprecated; please use glmer() instead

When we try using glmer we receive the following error:

> mod1<-glmer(status~-1+Density+(-1+Density|Name),data=input,family='binomial',weights=Weight)
Error in function (fr, X, reTrms, family, nAGQ = 1L, verbose = 0L,
control = glmerControl(),  :
  Response is constant - cannot fit the model



Do you know of any ways to run this type of model structure in the new
versions of lme4 (or any other R package)? We would really appreciate
any insights you could provide.

Thanks!


>
> I am trying to install an old version of lme4 (v. 0.999999-2) in R
> 2.15.2 on my Windows 7 computer. The newer versions of lme4 do not
> support the analyses I am trying to conduct.

  Why not? (Just curious.)  Missing mcmcsamp() ?

> I saved the packaged
> archive file (lme4_0.999999-2(1).tar) to my hard drive and then used
> RStudio's "Install from Package Archive File" to attempt to install
> the package. However, I received the following error message:
>
> > install.packages("~/R/lme4_0.
999999-2(1).tar.gz",
>  repos = NULL, type = "source")

  [snip]

> *** arch - i386
>
> ERROR: compilation failed for package 'lme4'
>
> * removing 'C:/Users/JBauder/Documents/R/win-library/2.15/lme4'
>
> Warning in install.packages :
>
>   running command 'C:/PROGRA~1/R/R-215~1.2/bin/x64/R CMD INSTALL -l
> "C:/Users/JBauder    /Documents/R/win-library/2.15"
> "C:/Users/JBauder/Documents/R/lme4_0.999999-2(1).tar.gz"' had status 1
>
> Warning in install.packages :
>
>   installation of package
> ?C:/Users/JBauder/Documents/R/lme4_0.999999-2(1).tar.gz? had non-zero
> exit status
>

 [snip]

  Do you have all of the tools installed that are required to
build binary packages in R?

  Windows binary versions of lme4.0 (a backward compatibility
version of lme4) are available from

http://lme4.r-forge.r-project.org/repos/bin/windows/contrib/2.15/


From miner at cornell.edu  Tue Apr 22 17:37:54 2014
From: miner at cornell.edu (Brooks Miner)
Date: Tue, 22 Apr 2014 15:37:54 +0000
Subject: [R-sig-ME] error distribution or transformation for
 Acid-neutralizing-capacity (ANC)
Message-ID: <3C004920-698E-421D-AB06-EB79C0DF8B0E@cornell.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140422/4d01f78d/attachment.pl>

From pauljohn32 at gmail.com  Tue Apr 22 17:42:08 2014
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Tue, 22 Apr 2014 10:42:08 -0500
Subject: [R-sig-ME] lme4/glmer convergence warnings
In-Reply-To: <534746EB.6090300@gmail.com>
References: <AB4F6BFB-45D8-4026-AFD5-A4D0CC819A0D@utsa.edu>
	<534746EB.6090300@gmail.com>
Message-ID: <CAErODj-5Ewy0C4p98Xa1syYmKmUgvK=h5FcvhQwCnKwaXrQyTQ@mail.gmail.com>

Hi

I have that same convergence warning, but I've fiddled around quite a
bit and get the same estimates whether I change to Nelder_Mead or use
the routines in optimx.  I did the relgrad check Ben asked for.

This means all good, right? Itty bitty number being good...

> max(abs(relgrad))
[1] 0.0000001698392
>

I can upload the data and code if you need, its got a lot of rows :)

> lmer3 <- lmer(as.formula(paste( "liquidAssetswASINH ~ ", fmlanew3, part2, "+(1|sippidf)")), data = dat, REML = FALSE, control = lmerControl(optimizer = "bobyqa"))
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00954838 (tol = 0.002)
> summary(lmer3)
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: liquidAssetswASINH ~ NoNo + (e2bonly + e2bPlus) * (tage) + (e2bonly +
    e2bPlus) * (tpearnTMmeanw1000) + esex + emsr + eracer + renroll +
    eeducater + rmesrrTMpcttrunc + errprRefNew + etenurerTM.ch6 +
    tfipsstr + (1 | sippidf)
   Data: dat
Control: lmerControl(optimizer = "bobyqa")

      AIC       BIC    logLik  deviance  df.resid
 484468.0  484763.2 -242203.0  484406.0    100967

Scaled residuals:
    Min      1Q  Median      3Q     Max
-4.4895 -0.2807  0.0128  0.4315  5.0182

Random effects:
 Groups   Name        Variance Std.Dev.
 sippidf  (Intercept) 3.186    1.785
 Residual             4.992    2.234
Number of obs: 100998, groups: sippidf, 36415

Fixed effects:
                              Estimate Std. Error t value
(Intercept)                   3.532580   0.114950   30.73
NoNoNoNo                     -5.827712   0.034154 -170.63
e2bonlyYes                    1.445270   0.131311   11.01
e2bPlus1                     -0.096062   0.120796   -0.80
tage                          0.044094   0.002449   18.01
tpearnTMmeanw1000             0.279506   0.012600   22.18
esexFemale                   -0.069325   0.025256   -2.74
emsrNot                       1.454538   0.025194   57.73
eracerNotWhite               -0.299493   0.036034   -8.31
eracerAsian                  -0.208051   0.064168   -3.24
renrollEnrolled full-time     0.320056   0.032989    9.70
renrollEnrolled part-time     0.181865   0.039189    4.64
eeducaterpartHigh            -0.064377   0.074716   -0.86
eeducaterHighSch             -0.090467   0.069822   -1.30
eeducaterpartColl             0.074347   0.070763    1.05
eeducaterCollege              0.807621   0.074188   10.89
rmesrrTMpcttruncPartJob      -0.017902   0.032287   -0.55
rmesrrTMpcttruncJob           0.082878   0.034674    2.39
errprRefNewYes                0.089412   0.046943    1.90
etenurerTM.ch6NotOwner_Owner  0.049244   0.041681    1.18
etenurerTM.ch6Owner           0.245273   0.023970   10.23
etenurerTM.ch6Owner_NotOwner  0.014995   0.049510    0.30
tfipsstrWest                 -0.139529   0.037873   -3.68
tfipsstrNorth Central        -0.020640   0.036890   -0.56
tfipsstrSouth                -0.163906   0.034739   -4.72
e2bonlyYes:tage              -0.131191   0.004494  -29.19
e2bPlus1:tage                 0.008010   0.003859    2.08
e2bonlyYes:tpearnTMmeanw1000 -0.160948   0.023931   -6.73
e2bPlus1:tpearnTMmeanw1000    0.263943   0.015025   17.57

Correlation matrix not shown by default, as p = 29 > 20.
Use print(...., correlation=TRUE)  or
     vcov(....)     if you need it



> relgrad <- with(lmer3 at optinfo$derivs,solve(Hessian,gradient))
> max(abs(relgrad))
[1] 0.0000001698392

On Thu, Apr 10, 2014 at 8:35 PM, Ben Bolker <bbolker at gmail.com> wrote:
> On 14-04-10 09:01 PM, Corey Sparks wrote:
>
>> Hello all, I?ve been seeing the aforementioned convergence errors for
>> weeks in a course i?m teaching using lme4, so I decided to follow
>> Ben?s advice on reporting the :
>
>> I?m fitting a binomial GLMM for small area estimation model building
>> here:
>> glmer(I(bmi>30)~povz+vachousz+baccz+blackz+hispanicz+factor(region)+(1|state)+(1|cofips),
>> family="binomial", data=merged, weights=cntywt/mean(cntywt))
>>
>> n=~240000, n_cofips=217, n_state=46 I get the warning: In
>> checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>> Model failed to converge with max|grad| = 0.235915 (tol = 0.001)
>>
>> and the gradients:
>>
>> relgrad <- with(fit.1 at optinfo$derivs,solve(Hessian,gradient))
>>> max(abs(relgrad))
>> [1] 0.0001631008
>
>   This is good (relative gradient is less than the 0.001 or 0.002
> tolerance we would think to set as a default)
>
>>
>> and when I use refit(), I get: fit.1<-refit(fit.1)
>>> relgrad <- with(fit.1 at optinfo$derivs,solve(Hessian,gradient))
>>> max(abs(relgrad))
>> [1] 2.369877e-06
>
>   This doesn't really matter so much (as long as we're getting below
> tolerance on the relative gradient, I don't care so much if we can
> decrease it still further by refitting).
>
>>
>> For another model on a LMM, I get:
>>
>> fit.mix<-lmer(bmiz~agez+lths+coll+black+hispanic+other+(1|cofips),
>> brfss_11)
>>
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
>> : Model failed to converge with max|grad| = 0.00550412 (tol = 0.002)
>>
>> relgrad <- with(fit.mix at optinfo$derivs,solve(Hessian,gradient))
>>> max(abs(relgrad))
>> [1] 8.099289e-08
>
>   ditto.
>
>>
>> Hope this helps
>>
>
>   Yes, this is encouraging (switching to relative gradients would clear
> everything up here)
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Paul E. Johnson
Professor, Political Science      Assoc. Director
1541 Lilac Lane, Room 504      Center for Research Methods
University of Kansas                 University of Kansas
http://pj.freefaculty.org               http://quant.ku.edu


From pauljohn32 at gmail.com  Tue Apr 22 17:50:21 2014
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Tue, 22 Apr 2014 10:50:21 -0500
Subject: [R-sig-ME] example of confidence intervals for predicted values?
Message-ID: <CAErODj8ZinOM-kYRZBLN668EK63B_T+P0MzdXQb6A4jK2yW7ug@mail.gmail.com>

Got an example?

I'm aware of controversy about problem of calculating CIs in glm. I'm
also aware other programs throw out CIs that may not be completely
correct.  I also understand profiled std errors for parameters and
about 60% of bootMer.

And I understand almost 0% of this comment in ?predict.merMod

Note:

     There is no option for computing standard errors of predictions
  because it is difficult to define an efficient method that
incorporates uncertainty in the variance parameters; we recommend
?bootMer? for this task.

Can I have a working example of how this leads to a 95% CI on
predictions from an lmer. or glmer?


-- 
Paul E. Johnson
Professor, Political Science      Assoc. Director
1541 Lilac Lane, Room 504      Center for Research Methods
University of Kansas                 University of Kansas
http://pj.freefaculty.org               http://quant.ku.edu


From jzl106 at gmail.com  Tue Apr 22 04:08:23 2014
From: jzl106 at gmail.com (Junyan Luo)
Date: Mon, 21 Apr 2014 22:08:23 -0400
Subject: [R-sig-ME] Is this a legitimate approach
Message-ID: <CAFT_Lc1Epcvv6-0TjnYwjGOBRPzKHP=+TCCqOb2x75HEAqyDEA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140421/5aef76c4/attachment.pl>

From ethomas at stanford.edu  Tue Apr 22 19:21:46 2014
From: ethomas at stanford.edu (Ewart Thomas)
Date: Tue, 22 Apr 2014 10:21:46 -0700
Subject: [R-sig-ME] error distribution or transformation for
	Acid-neutralizing-capacity (ANC)
In-Reply-To: <3C004920-698E-421D-AB06-EB79C0DF8B0E@cornell.edu>
References: <3C004920-698E-421D-AB06-EB79C0DF8B0E@cornell.edu>
Message-ID: <82985B45-B44A-4D77-823B-3BB230886EB8@stanford.edu>

brooks, try y = log(ANC + 17).  is the distrn approx normal.  i wd justify the offset of 17 as necessitated by the data, etc.  and it shdn't change your interpretation of originally negative values (now y < 17).
ewart

On Apr 22, 2014, at 8:37 AM, Brooks Miner wrote:

> I am a longtime user of nlme and then lme4 packages, and for the most part I know what I?m doing.
> 
> At the moment I?m grappling with a particularly difficult response variable that I would like to analyze in a mixed-effects model: Acid neutralizing-capacity (ANC)<https://en.wikipedia.org/wiki/Acid_neutralizing_capacity> ( http://en.wikipedia.org/wiki/Acid_neutralizing_capacity ). Although a very important measurement for lakes and streams, especially those recovering from acidification, ANC has difficult properties as a response variable: for example, in my current dataset the values range from -16 to ca. 400, and they are not normally distributed by any stretch of the imagination (see this figure: http://www.eeb.cornell.edu/miner/images/ANC_histogram.png )
> 
> There are negative values that are really important because they indicate water bodies in especially bad shape (called ?Acute Concern? by the National Acid Precipitation Assessment Program).
> 
> I have time-series data for ANC from 1988 to the present for 60 sampling sites, and I?d *really* like to use a mixed-effects model, with a random effect of ?Site,? to model how ANC values have been changing over time, overall across the 60 sites. A random effect for ?Site? is an ideal way to deal with the temporal pseudoreplication inherent in the time-series data.
> 
> My challenge: how to deal with my non-normal ANC response variable using lmer() or glmer()? Of course when I run it with a Gaussian error distribution, the Q-Q plot of residuals looks terrible. Because of the negative values, I can?t log- or sqrt-transform, use Box-Cox, or use family=?Gamma?. All of the existing literature analyzing ANC time series uses non-parametric methods (such as a Mann-Kendall test), but I?d really like to move beyond that in order to take advantage of a (G)LMM in order to draw general conclusions across all 60 sampling sites.
> 
> Any suggestions for how to deal with this frustratingly unique ANC response variable?
> 
> Many thanks ~
> 
> - Brooks
> 
> 
> ***************
> Brooks Miner
> Postdoctoral Fellow
> Department of Ecology & Evolutionary Biology
> Cornell University
> www.eeb.cornell.edu/miner<http://www.eeb.cornell.edu/miner>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From segerfan83 at gmail.com  Tue Apr 22 22:34:41 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Tue, 22 Apr 2014 16:34:41 -0400
Subject: [R-sig-ME] Calculate % Variance Attributed to Random Slopes
	(Phenotypic Plasticity)
Message-ID: <CAHe08Sh2PhKMMcowuEdRNnTHw9C27SJjPDZLGBdj8W7=Z7fagw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140422/807056fa/attachment.pl>

From jake987722 at hotmail.com  Wed Apr 23 01:48:28 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Tue, 22 Apr 2014 17:48:28 -0600
Subject: [R-sig-ME] Calculate % Variance Attributed to Random Slopes
 (Phenotypic Plasticity)
In-Reply-To: <CAHe08Sh2PhKMMcowuEdRNnTHw9C27SJjPDZLGBdj8W7=Z7fagw@mail.gmail.com>
References: <CAHe08Sh2PhKMMcowuEdRNnTHw9C27SJjPDZLGBdj8W7=Z7fagw@mail.gmail.com>
Message-ID: <BAY172-W41FED8DF8E6B6B722CD2BBCB590@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140422/e6e19504/attachment.pl>

From Thierry.ONKELINX at inbo.be  Wed Apr 23 09:44:36 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 23 Apr 2014 07:44:36 +0000
Subject: [R-sig-ME] error distribution or transformation
	for	Acid-neutralizing-capacity (ANC)
In-Reply-To: <82985B45-B44A-4D77-823B-3BB230886EB8@stanford.edu>
References: <3C004920-698E-421D-AB06-EB79C0DF8B0E@cornell.edu>
	<82985B45-B44A-4D77-823B-3BB230886EB8@stanford.edu>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A36B76@inbomail.inbo.be>

A more radical approach is to alter the definition of ANC: take the ratio of the sum of cations and the sum of the anions rather than their difference. Then you can apply the log transformation without problems (assuming neither the cations or anions sum to zero).

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Ewart Thomas
Verzonden: dinsdag 22 april 2014 19:22
Aan: Brooks Miner
CC: r-sig-mixed-models at r-project.org
Onderwerp: Re: [R-sig-ME] error distribution or transformation for Acid-neutralizing-capacity (ANC)

brooks, try y = log(ANC + 17).  is the distrn approx normal.  i wd justify the offset of 17 as necessitated by the data, etc.  and it shdn't change your interpretation of originally negative values (now y < 17).
ewart

On Apr 22, 2014, at 8:37 AM, Brooks Miner wrote:

> I am a longtime user of nlme and then lme4 packages, and for the most part I know what I?m doing.
>
> At the moment I?m grappling with a particularly difficult response
> variable that I would like to analyze in a mixed-effects model: Acid
> neutralizing-capacity
> (ANC)<https://en.wikipedia.org/wiki/Acid_neutralizing_capacity> (
> http://en.wikipedia.org/wiki/Acid_neutralizing_capacity ). Although a
> very important measurement for lakes and streams, especially those
> recovering from acidification, ANC has difficult properties as a
> response variable: for example, in my current dataset the values range
> from -16 to ca. 400, and they are not normally distributed by any
> stretch of the imagination (see this figure:
> http://www.eeb.cornell.edu/miner/images/ANC_histogram.png )
>
> There are negative values that are really important because they indicate water bodies in especially bad shape (called ?Acute Concern? by the National Acid Precipitation Assessment Program).
>
> I have time-series data for ANC from 1988 to the present for 60 sampling sites, and I?d *really* like to use a mixed-effects model, with a random effect of ?Site,? to model how ANC values have been changing over time, overall across the 60 sites. A random effect for ?Site? is an ideal way to deal with the temporal pseudoreplication inherent in the time-series data.
>
> My challenge: how to deal with my non-normal ANC response variable using lmer() or glmer()? Of course when I run it with a Gaussian error distribution, the Q-Q plot of residuals looks terrible. Because of the negative values, I can?t log- or sqrt-transform, use Box-Cox, or use family=?Gamma?. All of the existing literature analyzing ANC time series uses non-parametric methods (such as a Mann-Kendall test), but I?d really like to move beyond that in order to take advantage of a (G)LMM in order to draw general conclusions across all 60 sampling sites.
>
> Any suggestions for how to deal with this frustratingly unique ANC response variable?
>
> Many thanks ~
>
> - Brooks
>
>
> ***************
> Brooks Miner
> Postdoctoral Fellow
> Department of Ecology & Evolutionary Biology Cornell University
> www.eeb.cornell.edu/miner<http://www.eeb.cornell.edu/miner>
>
>       [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From paul.johnson at glasgow.ac.uk  Wed Apr 23 12:21:03 2014
From: paul.johnson at glasgow.ac.uk (Paul Johnson)
Date: Wed, 23 Apr 2014 11:21:03 +0100
Subject: [R-sig-ME] Calculate % Variance Attributed to Random Slopes
 (Phenotypic Plasticity)
In-Reply-To: <BAY172-W41FED8DF8E6B6B722CD2BBCB590@phx.gbl>
References: <CAHe08Sh2PhKMMcowuEdRNnTHw9C27SJjPDZLGBdj8W7=Z7fagw@mail.gmail.com>
	<BAY172-W41FED8DF8E6B6B722CD2BBCB590@phx.gbl>
Message-ID: <0AFA3018-055A-4C2C-B03B-1FCF0A782EEB@glasgow.ac.uk>

Hi Jacob

I?m not sure I?ve understood your question, particularly when you ask how to get the variance in the predictor variable.

>> model<-lmer(X~Y*Year+(Y|Individual)
>> 
>> but I do not know how to extract the proportion of variance explained by
>> the individual slope term.  

As I understand variance partitioning, we are partitioning the variance of the responses, here labelled X (I think Jake understandably mixed up X and Y in his reponse - to avoid confusion I?m going to use ?response? and ?predictor?, and ignore the other predictor, Year). The total variance in the responses explained by the random effects (in both the slope and intercept) is given by the numerator of the VPC equation on Jake?s slide 28 (Calculating the Total Variance). As Jake says this is dependent on the predictor, so some rows will have a higher random effects variance than others. I wouldn?t agree that it?s meaningless though - if summed across all rows it can be interpreted as the component of variance that is due to inter-individual variation, as separate from the variance due to the fixed effects and the residual variance within individuals. This total random effects variance can be calculated easily with lme4 output. It doesn?t depend on the predictor in the sense that it is summed across all values of the predictor:

   sum(diag(Z %*% D %*% t(Z)))

Z is the design matrix of the random effects and D is the random effects covariance matrix from VarCorr(model)$Individual (Laird & Ware, 1982, Biometrics, http://www.jstor.org/stable/2529876). The columns of Z and the rows and columns of Sigma have to match. This is equivalent to summing across the ij in the numerator of the VPC equation mentioned above. The mean variance is mean(diag(Z %*% Sigma %*% t(Z))). This random effects variance could be used in variance components stats e.g. R^2, ICC, etc, although I accept that it would have to be interpreted with caution and in some scenarios might not be meaningful.

This formula doesn?t give you what you asked for though, the proportion of variance explained by the individual slope term, it gives the variance component attributable to *all* random effects at the individual level, which here are the random intercept and random slope. I don?t know how you would isolate the variance due to the random slope alone, and I?m not sure that it's a meaningful quantity for the reasons that Jake gives, i.e. the dependence of the total random effects variance on the value of the predictor. E.g. in a simple one-predictor random slopes model Z will have a column of 1s for the intercept and column containing the values of the predictor. You could fix the predictor in the above formula to any value and calculate the mean random effects variance conditioned on that value, i.e. removing variation in the predictor, but the resulting variance will depend on the choice of predictor value, and there?s no guarantee that it will be less than the marginal variance (i.e. averaged across all values of the predictor).

>> If I run the printout:
>> 
>> summary(model)
>> 
>> I can get the variance in slope with respect to Y, but this is not the
>> variance in Y itself, but rather how X is influenced by Y.
>> 
>> Any ideas on how to get the variance in Y itself?


I don?t understand this part of your question. How does the variance of the predictor come into this? The total (or mean) random effects variance above depends on the predictor so will also depend on the variance of the predictor, but it?s the variance in the responses that we?re trying to explain (or have I misunderstood the question?).

There might be better ways to quantify the random slopes contribution? E.g. take a phenotypic trait that shows high inter-individual variation in different environments (predictor values), and where the same individuals tend to have high trait values in both environments. In this scenario you would expect a high intercept variance and a low slope variance (and presumably also a low covariance). These variances and covariances can be transformed into useful descriptive stats like environment-specific repeatabilities (ICCs) and a cross environment correlation coefficient (which in this example would be high). A cross-environment correlation coefficient close to 1 would indicate that the random slopes contribution is very small - this sound like it gets close to what you were asking for. How to do this, with R code, is described here:
Brommer, J.E. (2013). Variation in plasticity of personality traits implies that the ranking of personality measures changes between environmental contexts: calculating the cross-environmental correlation. Behavioral Ecology and Sociobiology, 67, 1709?1718.
Incidentally the methodology in this paper is quite similar to the random effects variance component formula above.

Good luck,
Paul



On 23 Apr 2014, at 00:48, Jake Westfall <jake987722 at hotmail.com> wrote:

> Jacob,
> 
> Unfortunately the meaning of percent variance attributed to random terms (or to different "levels" if you're into that) is conceptually pretty unclear when there are random slopes. In general the variance of Y differs at different values of X. Consequently, estimates of percent variance explained are different for different values of X, and so they are not particularly useful or informative statistics in themselves. You may want to check out the following resources:
> 
> Sections 1 and 2 of Goldstein, Browne, & Rasbash, 2002:
> http://www.bristol.ac.uk/cmm/research/pvmm.pdf
> 
> Slides 28, 29, and 30 from the following powerpoint presentation:
> http://www.bristol.ac.uk/cmm/software/support/workshops/materials/random-slopes.pdf
> 
> Jake
> 
>> Date: Tue, 22 Apr 2014 16:34:41 -0400
>> From: segerfan83 at gmail.com
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] Calculate % Variance Attributed to Random Slopes	(Phenotypic Plasticity)
>> 
>> Hi all,
>> 
>> I am trying to figure out how to calculate how strong individual plasticity
>> is in my model:
>> 
>> model<-lmer(X~Y*Year+(Y|Individual)
>> 
>> but I do not know how to extract the proportion of variance explained by
>> the individual slope term.  If I run the printout:
>> 
>> summary(model)
>> 
>> I can get the variance in slope with respect to Y, but this is not the
>> variance in Y itself, but rather how X is influenced by Y.
>> 
>> Any ideas on how to get the variance in Y itself?
>> 
>> Thank you,
>> Jacob
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  		 	   		  
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From msaussac at chu-clermontferrand.fr  Wed Apr 23 14:46:59 2014
From: msaussac at chu-clermontferrand.fr (Saussac Mathilde)
Date: Wed, 23 Apr 2014 14:46:59 +0200
Subject: [R-sig-ME] multinomial model with MCMCglmm by using gelman.prior
Message-ID: <0523A5CE5D1EED4D9A9846E530EE382D0431658AFD80@E2K7-CCR.chu-clermontferrand.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140423/295e1ac7/attachment.pl>

From oliver.stirrup.13 at ucl.ac.uk  Thu Apr 24 12:01:09 2014
From: oliver.stirrup.13 at ucl.ac.uk (Stirrup, Oliver)
Date: Thu, 24 Apr 2014 10:01:09 +0000
Subject: [R-sig-ME] Profile likelihood using R2admb
Message-ID: <aac88930c4f8473cb6d9c55b1c132835@DB3PR01MB282.eurprd01.prod.exchangelabs.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140424/c6c32435/attachment.pl>

From bbolker at gmail.com  Thu Apr 24 13:39:24 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 24 Apr 2014 07:39:24 -0400
Subject: [R-sig-ME] Profile likelihood using R2admb
In-Reply-To: <aac88930c4f8473cb6d9c55b1c132835@DB3PR01MB282.eurprd01.prod.exchangelabs.com>
References: <aac88930c4f8473cb6d9c55b1c132835@DB3PR01MB282.eurprd01.prod.exchangelabs.com>
Message-ID: <5358F7EC.3090203@gmail.com>

On 14-04-24 06:01 AM, Stirrup, Oliver wrote:
> Hello all,
> 
> I am currently experimenting with various mixed model structures
> using the R2admb package. Everything is working smoothly, except that
> I have been unable to make use of the functionality relating to
> likelihood profiles.
> 
> If I attempt to run 'do_admb' with the argument 'profile=TRUE', I
> receive the error message 'must specify profpars when
> checkparam=='write' and profile is TRUE'. However, if I then try to
> also specify the parameters to include, for example adding the
> argument 'profpars=c("rho")', I receive the error message 'unused
> argument (profpars=c("rho"))'.
> 
> I am using R version 3.1.0 and R2admb version 0.7.10, and I have
> noticed that the R2admb manual states that the profile option for
> do_admb is 'untested!'. Has anyone else encountered this problem? If
> so, is there any way around it?
> 

  I (the package maintainer) haven't tried this myself in a while, and
when I did I found it didn't work!  I imagine this is a fairly minor
glitch and will try to fix it soon:
https://github.com/bbolker/R2admb/issues/13 -- you may end up having to
install the github version (via devtools::install_github()) once I have
it fixed

  Ben Bolker


From ernest.mauya at nmbu.no  Thu Apr 24 21:49:14 2014
From: ernest.mauya at nmbu.no (Ernest William Mauya)
Date: Thu, 24 Apr 2014 19:49:14 +0000
Subject: [R-sig-ME] Fitting Non linear mixed effects models
Message-ID: <e8d87c22dea447acb9db0ca918152e9d@EXCH-MBX01.NMBU.NO>

I am learning how to fit non linear mixed effect models in R . I have data which are in clusters i.e 65 clusters each with 8 plots.
I am fitting the model with four predictors like this :


start <- coef(lm(log(y)~I(log(Maxf))+I(log(PF70))+I(log(TF3))+I(log(ps60)),data=data[data$y>0,]))
start[1] <- exp(start[1])
names(start) <- c("a","b1","b2","b3","b4")
model <- nlme(y~a*Maxf^b1*PF70^b2*TF3^b3*ps60^b4,data=data,start=start)
SSasymp<-start

win.m.1 <- nlme(y~a*Maxf^b1*PF70^b2*TF3^b3*ps60^b4, fixed = a + b1 + b2+b3+b4 ~ 1,
                random = a + b1 + b2+b3+b4~ 1 | clustno, data = data, start = start).


But I get errors any assistance from your side?


From oliver.stirrup.13 at ucl.ac.uk  Fri Apr 25 12:24:00 2014
From: oliver.stirrup.13 at ucl.ac.uk (Stirrup, Oliver)
Date: Fri, 25 Apr 2014 10:24:00 +0000
Subject: [R-sig-ME] Profile likelihood using R2admb
Message-ID: <5688717c291a4f8dbc25fbe42ff3873a@AM3PR01MB273.eurprd01.prod.exchangelabs.com>

Dear Prof Bolker

Thank you for the quick response, and for developing this package in the first place!

Best wishes

Oliver

Oliver Stirrup
PhD Student
MRC Clinical Trials Unit at UCL
------------------------------

Message: 2
Date: Thu, 24 Apr 2014 07:39:24 -0400
From: Ben Bolker <bbolker at gmail.com>
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Profile likelihood using R2admb
Message-ID: <5358F7EC.3090203 at gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

On 14-04-24 06:01 AM, Stirrup, Oliver wrote:
> Hello all,
> 
> I am currently experimenting with various mixed model structures using 
> the R2admb package. Everything is working smoothly, except that I have 
> been unable to make use of the functionality relating to likelihood 
> profiles.
> 
> If I attempt to run 'do_admb' with the argument 'profile=TRUE', I 
> receive the error message 'must specify profpars when 
> checkparam=='write' and profile is TRUE'. However, if I then try to 
> also specify the parameters to include, for example adding the 
> argument 'profpars=c("rho")', I receive the error message 'unused 
> argument (profpars=c("rho"))'.
> 
> I am using R version 3.1.0 and R2admb version 0.7.10, and I have 
> noticed that the R2admb manual states that the profile option for 
> do_admb is 'untested!'. Has anyone else encountered this problem? If 
> so, is there any way around it?
> 

  I (the package maintainer) haven't tried this myself in a while, and when I did I found it didn't work!  I imagine this is a fairly minor glitch and will try to fix it soon:
https://github.com/bbolker/R2admb/issues/13 -- you may end up having to install the github version (via devtools::install_github()) once I have it fixed

  Ben Bolker


From rhtardin at gmail.com  Fri Apr 25 13:53:02 2014
From: rhtardin at gmail.com (Rodrigo Tardin)
Date: Fri, 25 Apr 2014 08:53:02 -0300
Subject: [R-sig-ME] Should I use GLMM?
Message-ID: <CAE5HZZ+dAdwhaSkNW6qiSYC4GyQHLcjFqWD33BJRS1xHB9OStA@mail.gmail.com>

Um texto embutido e sem conjunto de caracteres especificado foi limpo...
Nome: n?o dispon?vel
Url: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140425/15470c56/attachment.pl>

From david.sears01 at gmail.com  Fri Apr 25 20:08:12 2014
From: david.sears01 at gmail.com (David Sears)
Date: Fri, 25 Apr 2014 14:08:12 -0400
Subject: [R-sig-ME] reporting parameter estimates for factors using
	sum/deviation coding
Message-ID: <023501cf60b1$531de620$f959b260$@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140425/1c5312a8/attachment.pl>

From xmjiang1983 at gmail.com  Sun Apr 27 06:41:50 2014
From: xmjiang1983 at gmail.com (xiaoming jiang)
Date: Sun, 27 Apr 2014 12:41:50 +0800
Subject: [R-sig-ME] error message in pamer.fnc
Message-ID: <CAAPVWgA7iLGXV9nOzqiDNF5Z05cZs2p7=+_6oydkjF+guUvaaw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140427/02d26825/attachment.pl>

From elizabeth.crone at tufts.edu  Sun Apr 27 18:38:41 2014
From: elizabeth.crone at tufts.edu (Elizabeth Crone)
Date: Sun, 27 Apr 2014 12:38:41 -0400
Subject: [R-sig-ME] glmerMod with covariance matrix set to 0?
Message-ID: <CA+YYKzAxZgxKW4kuAdEOd6Trti0xaV3=8rn4mQnQCWLtVtVjfQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140427/8d594f3a/attachment.pl>

From bbolker at gmail.com  Sun Apr 27 20:00:48 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 27 Apr 2014 14:00:48 -0400
Subject: [R-sig-ME] Fwd: Multi Processor / lme4
In-Reply-To: <535D1BFB.3060409@gmail.com>
References: <535D1BFB.3060409@gmail.com>
Message-ID: <535D45D0.10709@gmail.com>


  [This is a perfectly reasonable question for r-sig-mixed-models, so
I'm forwarding it there]

===================
This might be very naive.

I assume a very costly part of estimating parameters is evaluating the
likelihood -- particularly when the data are large. It seems like it'd
be fairly easy to distribute that evaluation across several processes
(i.e., multithread it) to speed up the procedure (e.g., mclapply or pvec
in R "parallel" package)

That said, I'd guess likelihood evaluation in (g)lmer actually happens
somewhere else where I am not so comfortable.

Any obvious reasons this map-reduce (I think it's called) sort of
technique is not in use?

Thanks for your time. -Nate

--
Nathan Doogan, Ph.D.
Post Doctoral Researcher
The Colleges of Social Work and Public Health
The Ohio State University


  It is indeed a little naive, but not silly at all.  The problem is
that it is *not* "fairly easy" to distribute the evaluation across
processors via map-reduce/mclapply etc..  Doug Bates has already done
all kind of wizardry to reduce the likelihood evaluation to linear
algebra operations that can be done very efficiently. While speeding the
process up enormously been doing some careful profiling with his Julia
code, and the largest single cost (if I am recalling things correctly)
is computing a sparse Cholesky decomposition.  He has been looking
*very* recently at the PaStiX library
<http://http://pastix.gforge.inria.fr/> as a possible way to parallelize
this operation, but it is not completely trivial.
  cheers
    Ben Bolker


From doogan.1 at osu.edu  Sun Apr 27 20:34:51 2014
From: doogan.1 at osu.edu (Doogan, Nathan)
Date: Sun, 27 Apr 2014 14:34:51 -0400
Subject: [R-sig-ME] Multi Processor / lme4
In-Reply-To: <535D45D0.10709@gmail.com>
References: <535D1BFB.3060409@gmail.com> <535D45D0.10709@gmail.com>
Message-ID: <CA+okLw9QNaEv+gZUgHGmmBqpK5odkrFb_udU3u8-7GzSgXYhAg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140427/951f0b08/attachment.pl>

From bbolker at gmail.com  Sun Apr 27 20:53:21 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 27 Apr 2014 14:53:21 -0400
Subject: [R-sig-ME] Multi Processor / lme4
In-Reply-To: <CA+okLw9QNaEv+gZUgHGmmBqpK5odkrFb_udU3u8-7GzSgXYhAg@mail.gmail.com>
References: <535D1BFB.3060409@gmail.com> <535D45D0.10709@gmail.com>
	<CA+okLw9QNaEv+gZUgHGmmBqpK5odkrFb_udU3u8-7GzSgXYhAg@mail.gmail.com>
Message-ID: <535D5221.2080700@gmail.com>

On 14-04-27 02:34 PM, Doogan, Nathan wrote:
> Thanks for the info, Ben.
> 
> It sounds like the best way for me to speed things up at the moment, then,
> is to build R with a threaded linear algebra library.
> 
> -Nate

  Actually, I don't know if that will help; most of the difficult linear
algebra in lme4 is sparse linear algebra, handled through the Matrix
package (wrapping Tim Davis's SuiteSparse library) and the RcppEigen
package.  I'm not sure how much of it really uses the standard BLAS
back-end.

  Someone with time on their hands could do some benchmarking and see
what happens.
http://cran.r-project.org/web/packages/gcbd/vignettes/gcbd.pdf? is a
good resource.

  If you're really interested in performance, and feeling adventurous, I
would definitely recommend Doug Bates's MixedModels package for Julia.

  It is also the case at the moment that lme4 is slower than lme4.0 for
some problems.

  If you have specific performance questions (rather than just "it would
be nice for lme4 to be faster", which I don't disagree with), it would
be good to give the parameters of your problem -- how many observations,
grouping variables, # of levels of grouping variables, LMM vs GLMM,
structure of grouping variables (nested, crossed, partially crossed) ... ?

  Ben Bolker






> p.s. a duplicate of this message (sent from a non-member email address) is
> waiting for moderation. feel free to delete.
> 
> 
> On Sun, Apr 27, 2014 at 2:00 PM, Ben Bolker <bbolker at gmail.com> wrote:
> 
>>
>>   [This is a perfectly reasonable question for r-sig-mixed-models, so
>> I'm forwarding it there]
>>
>> ===================
>> This might be very naive.
>>
>> I assume a very costly part of estimating parameters is evaluating the
>> likelihood -- particularly when the data are large. It seems like it'd
>> be fairly easy to distribute that evaluation across several processes
>> (i.e., multithread it) to speed up the procedure (e.g., mclapply or pvec
>> in R "parallel" package)
>>
>> That said, I'd guess likelihood evaluation in (g)lmer actually happens
>> somewhere else where I am not so comfortable.
>>
>> Any obvious reasons this map-reduce (I think it's called) sort of
>> technique is not in use?
>>
>> Thanks for your time. -Nate
>>
>> --
>> Nathan Doogan, Ph.D.
>> Post Doctoral Researcher
>> The Colleges of Social Work and Public Health
>> The Ohio State University
>>
>>
>>   It is indeed a little naive, but not silly at all.  The problem is
>> that it is *not* "fairly easy" to distribute the evaluation across
>> processors via map-reduce/mclapply etc..  Doug Bates has already done
>> all kind of wizardry to reduce the likelihood evaluation to linear
>> algebra operations that can be done very efficiently. While speeding the
>> process up enormously been doing some careful profiling with his Julia
>> code, and the largest single cost (if I am recalling things correctly)
>> is computing a sparse Cholesky decomposition.  He has been looking
>> *very* recently at the PaStiX library
>> <http://http://pastix.gforge.inria.fr/> as a possible way to parallelize
>> this operation, but it is not completely trivial.
>>   cheers
>>     Ben Bolker
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From doogan.1 at osu.edu  Sun Apr 27 21:05:50 2014
From: doogan.1 at osu.edu (Doogan, Nathan)
Date: Sun, 27 Apr 2014 15:05:50 -0400
Subject: [R-sig-ME] Multi Processor / lme4
In-Reply-To: <535D5221.2080700@gmail.com>
References: <535D1BFB.3060409@gmail.com> <535D45D0.10709@gmail.com>
	<CA+okLw9QNaEv+gZUgHGmmBqpK5odkrFb_udU3u8-7GzSgXYhAg@mail.gmail.com>
	<535D5221.2080700@gmail.com>
Message-ID: <CA+okLw9DPmPw=qia88gZxjQfJ3omdm6f1pHwdhSfa5A-QXjKPA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140427/ad4131f0/attachment.pl>

From bbolker at gmail.com  Sun Apr 27 22:54:25 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 27 Apr 2014 16:54:25 -0400
Subject: [R-sig-ME] glmerMod with covariance matrix set to 0?
In-Reply-To: <CA+YYKzAxZgxKW4kuAdEOd6Trti0xaV3=8rn4mQnQCWLtVtVjfQ@mail.gmail.com>
References: <CA+YYKzAxZgxKW4kuAdEOd6Trti0xaV3=8rn4mQnQCWLtVtVjfQ@mail.gmail.com>
Message-ID: <535D6E81.7080205@gmail.com>

On 14-04-27 12:38 PM, Elizabeth Crone wrote:
> Dear mixed modelers,
> 
> When lme4 fits a glmer model, the default parameterization is to model the
> full covariance matrix of random effects.  For some data sets, this takes a
> very long time, and the covariance is also difficult to estimate with
> missing data.  Is there a way to set the correlation matrix to 0?  [I can
> think of ways to hack it, but I wonder if there is a command I haven't
> noticed.]
> 
> To make my question more concrete, consider an example where I am modeling
> counts of animals at several sites through time:
> glmer(count ~ -1 + site + (-1 + site|year), family = poisson)
> 
> This model estimates the average log-count of animals at each site, the
> standard deviation through time at each site, and the covariance of
> log-counts at all pairs of sites through time.  [The -1's set the
> parameterization to means and standard deviations for each site, as opposed
> to a reference group, and deviations from that group.] Missing data are
> sites with no counts in some years, which is one thing that makes the
> covariance difficult to estimate.
> 
> Is there a command to set the random effects correlations to 0, without
> manually creating a dummy variable for each site, running a separate lmer
> model for each site?
> 
> Thanks!
> Elizabeth Crone
> 
> 


  This is possible, but it's not *very* easy at present. The basic idea
is to build the full deviance function, and then wrap it in a function
that sets the diagonal elements of the v-cov matrix to the parameters
specified (and the fixed effects vector, if it's a GLMM with nAGQ>0) but
sets the off-diagonal elements of the variance-covariance matrix to
zero.  I haven't had time to put together such an example, but a similar
trick is done at http://rpubs.com/bbolker/6298 ; the diagLmer() function
defined there might work with suitable modifications.

  A slightly better way would actually be to redefine the internal
Lind/Lambda structures; the best way would be for us to implement a
notation that would allow end-users to do this easily ...

  Ben Bolker


From baud-bovy.gabriel at hsr.it  Sun Apr 27 23:34:32 2014
From: baud-bovy.gabriel at hsr.it (Gabriel Baud-Bovy)
Date: Sun, 27 Apr 2014 23:34:32 +0200
Subject: [R-sig-ME] new syntax for lme ?
In-Reply-To: <535D6E81.7080205@gmail.com>
References: <CA+YYKzAxZgxKW4kuAdEOd6Trti0xaV3=8rn4mQnQCWLtVtVjfQ@mail.gmail.com>
	<535D6E81.7080205@gmail.com>
Message-ID: <535D77E8.9000906@hsr.it>

On 27/04/2014 10:54 PM, Ben Bolker wrote:
>    A slightly better way would actually be to redefine the internal
> Lind/Lambda structures; the best way would be for us to implement a
> notation that would allow end-users to do this easily ...
>
>    Ben Bolker
Dear Ben,

I am looking forward a new syntax :-). In this respect, a few months a 
ago I sent
you a reference to an old paper by Nelder that mentioned a way to improve
formulae expressiveness (though he himself did not think that it was 
necessary).
In short, the idea was to indicate explicitly the terms that were marginal
and should be eliminated using a notation like A:B[A].

This notation would not allow a completely flexible specification of the 
covariance
matrix but it would, for example, avoid the inclusion of two intercepts 
when trying
to set correlation between crossed factors to zero as it happens when doing

(1|group) + (A|group)+(B|group)+(A:B|group)

by doing something like that

(1|group) + (A[1]|group)+(B[1]|group)+(A:B[A,B]|group)

I wonder if you got any feedback about this idea. Perhaps, it did not
seem to be worth in light of the fact that a more flexible notation - ? 
la lme -
is needed to specify any kind of covariance matrix.

Best,

Gabriel


On 29/01/2014 2:29 PM, Ben Bolker wrote:
>    Extremely interesting indeed, will take some time to digest.  Cc'ing
> to other lme4 authors; hope that's alright.
>
>    sincerely
>      Ben Bolker
No problem about the cc.
Best,
Gabriel

PS.  Unfortunate typos and pasting when mentioning Nelder's paper (correct
reference is at the end of the quote). He obviously did not write on
brushless
motors (:-) ).

> On 14-01-29 07:38 AM, Gabriel Baud-Bovy wrote:
>> Dear Ben,
>>
>> Many people are puzzled by the  structure of the random effects.  As you
>> mentioned in the reply to this post, the problem is that the syntax for
>> random effects in lme4 leads to over-parameterization. The  fundamental
>> problem is each term between parentheses is evaluated independently.
>> For example,  I posted once a question about simplifying the correlation
>> structure (A*B|su) by using  (A|su)+(B|su)+(A:B|su) but that does not work.
>> (more details in my post
>> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q4/021063.html)
>>
>> The reason that I am sending you this email is that  I think
>> that it is difficult to come up with satisfying rules for
>> having automatic merging of random effects (for fixed effect the
>> marginality principle is enforced but I don't think that that would
>> always make sense  for random effects).  I think that it would be better
>> to extend lme4 syntax in a way that allow one to specify  them with more
>> flexibility.
>>
>> In particular, I wonder if it is not possible to use a syntax that was
>> proposed by Nelder (1977)  as a response to some discussants
>> who critizied the fact that terms like A:B meant different
>> things when included in a formulae like ~A+A:B and simply ~A:B
>> (see the paper, which also provides a great background
>> on the marginality principle). Let me quote Nelder:
>>
>> "Now it is arguable that the interpretation of ?_2 x^2 is different in the
>> two expressions ?0 + ?1x + ?2 x^2 and ?0 + ?2 x^2 yet we do not
>> distinguish notationally between them. It would of course be perfectly
>> possible to write, for example, A:B A:B[A] A:B[B] A:B[A,B] when the
>> marginal terms eliminated are appended in square brackets. Experience
>> with the notation of the paper, which is compact and unambiguous,
>> suggests that users quickly assimilate it and are not confused by the
>> convention adopted."Nelder J. A. (1977) A Reformulation of Linear Models
>> Journal of the Royal Statistical Society. Series A (General), Vol. 140,
>> pp.48-77
>>
[--sic--]
>>
>> With the same syntax, I could simplify the correlation structure between
>> crossed effects (A*B|group) by doing something like
>>
>> (1|group) + (A[1]|group)+(B[1]|group)+(A:B[A+B]|group)
>>
[--sic--]
>>
>> Best
>>
>> Gabriel
>>
>>
>>
>>
>>
-- 
#----------------------------------------------------------------
Gabriel Baud-Bovy                 email: gabriel.baud-bovy at iit.it
Italian Institute of Technology       tel.: (+39) 010 71781 (822)
via Morego 30, 16163 Genova            mobile: (+39) 348 172 4045
http://www.iit.it/en/rbcs-labs/dti.html
#----------------------------------------------------------------



-- 
---------------------------------------------------------------------
Gabriel Baud-Bovy               tel.: (+39) 02 2643 4839 (office)
UHSR University                       (+39) 02 2643 3429 (laboratory)
via Olgettina, 58                     (+39) 02 2643 4891 (secretary)
20132 Milan, Italy               fax: (+39) 02 2643 4892


From Thierry.ONKELINX at inbo.be  Mon Apr 28 08:32:55 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 28 Apr 2014 06:32:55 +0000
Subject: [R-sig-ME]
 =?windows-1252?q?=5BR=5D_lme4_Error_Help=3A_=93maxstep?=
 =?windows-1252?q?halfit=85pwrssUpdate=94?=
In-Reply-To: <BLU169-W100522438783894807A10CBC7470@phx.gbl>
References: <BLU169-W100522438783894807A10CBC7470@phx.gbl>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A40DE2@inbomail.inbo.be>

Dear Craig,

It is better to ask questions about lme4 at r-sig-mixed-models (in cc).

Are you using a recent version of lme4? Try upgrading lme4 and see if you still get the error.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Namens Craig O'Connell
Verzonden: maandag 28 april 2014 3:20
Aan: r-help at r-project.org
Onderwerp: [R] lme4 Error Help: ?maxstephalfit?pwrssUpdate?

I am using a mixed model to assess the effects of various variables (i.e. treatment, density, visibility) on bee behavior (e.g., avoidance frequency - total avoidances per total visits; feeding frequency, and mating frequency).  Bee individuals is my random factor (n=63 different bees), whereas treatment type, animal density, and air visibility are my fixed factors.
However, when I run my models, I immediately get an error that I cannot fix.  Here is a sample of my data:
Bee   Treatment    Visits    Avoid   Feeding    Mating    Density   Visibility

1   C   5   0   5   0      5        4
2   C   4   0   3   0      5        4
3   C   3   0   3   0      5        4
...
63

1   PC  2   0   1   1      5        4
2   PC  3   0   0   3      5        4
3   PC  1   0   0   0      5        4
...
63

1   M   5   0   1   3      5        4
2   M   3   2   0   0      5        4
3   M   2   0   0   2      5        4
...
63One I create my .txt file, I being my coding in R by first loading lme4.  After that, my coding starts off as follows:
barrierdat = read.table("GLMMROW.txt", header=TRUE) barrierdat barrierdat$Visibility = as.factor(barrierdat$Visibility);
barrierdat$Density    = as.factor(barrierdat$Density);

p01.glmer = glmer(Avoidance~offset(log(Visits))+(1|Bee),            family=poisson,
                  data=egghead);  # null model; p02.glmer = glmer(Avoidance~offset(log(Visits))+(1|Bee)+Treatment,  family=poisson,
                  data=egghead);
p03.glmer = glmer(Avoidance~offset(log(Visits))+(1|Bee)+Visibility, family=poisson,
                  data=egghead);
p04.glmer = glmer(Avoidance~offset(log(Visits))+(1|Bee)+Density,    family=poisson,
                  data=egghead);However, upon immediately running my models (e.g. p01.glmer), I receive the error:
Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate

Does anybody know what the issue is?  I ran similar data several weeks ago and had no issues.  Any Suggestions on how to proceed?

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From caroline.lustenberger at hotmail.com  Mon Apr 28 13:31:56 2014
From: caroline.lustenberger at hotmail.com (Caroline Lustenberger)
Date: Mon, 28 Apr 2014 13:31:56 +0200
Subject: [R-sig-ME] FW: linear mixed model for non-normal negative and
	continous data
In-Reply-To: <DUB110-W77FDB4E4F048BD295154DBE0470@phx.gbl>
References: <DUB110-W77FDB4E4F048BD295154DBE0470@phx.gbl>
Message-ID: <DUB110-W11644F28691E38DDB5C8EF0E0470@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140428/3c480fd1/attachment.pl>

From malsburg at posteo.de  Mon Apr 28 14:07:28 2014
From: malsburg at posteo.de (Titus von der Malsburg)
Date: Mon, 28 Apr 2014 13:07:28 +0100
Subject: [R-sig-ME] FW: linear mixed model for non-normal negative
	andcontinous dataa
In-Reply-To: <DUB110-W11644F28691E38DDB5C8EF0E0470@phx.gbl>
References: <DUB110-W77FDB4E4F048BD295154DBE0470@phx.gbl>
	<DUB110-W11644F28691E38DDB5C8EF0E0470@phx.gbl>
Message-ID: <87ppk14pz3.fsf@posteo.de>


On 2014-04-28 Mon 12:31, Caroline Lustenberger <caroline.lustenberger at hotmail.com> wrote:
> mod_Knmn<-lmer(Knmn~Group*Timepoint+(1|VPnr),data=data)
>  
> When performing a qq-plot my residuals are clearly deviant from the
> norm (long-tailed). Due to negative values I cannot perform classical
> transformation methods (e.g. log transformation). How could I proccede
> with this data. Is there a possibility to use a generalized linear
> model?

Just multiply with -1 (turning the measurements into strictly positive
values) and then do a log transform or box-cox transform.

  Titus


From Thierry.ONKELINX at inbo.be  Mon Apr 28 14:21:51 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 28 Apr 2014 12:21:51 +0000
Subject: [R-sig-ME] [R] linear mixed model for non-normal negative and
 continous data
In-Reply-To: <DUB110-W934FCD2B2ABE45E48A64DCE0470@phx.gbl>
References: <DUB110-W77FDB4E4F048BD295154DBE0470@phx.gbl>,
	<AA818EAD2576BC488B4F623941DA7427F3A4116C@inbomail.inbo.be>
	<DUB110-W934FCD2B2ABE45E48A64DCE0470@phx.gbl>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A411F5@inbomail.inbo.be>

Een ingesloten tekst met niet-gespecificeerde tekenset is gescrubt ...
Naam: niet beschikbaar
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140428/cba290f2/attachment.pl>

From craigpoconnell at hotmail.com  Mon Apr 28 13:57:53 2014
From: craigpoconnell at hotmail.com (Craig O'Connell)
Date: Mon, 28 Apr 2014 07:57:53 -0400
Subject: [R-sig-ME]
 =?windows-1252?q?=5BR=5D_lme4_Error_Help=3A_=93maxstep?=
 =?windows-1252?q?halfit=85pwrssUpdate=94?=
In-Reply-To: <AA818EAD2576BC488B4F623941DA7427F3A40DE2@inbomail.inbo.be>
References: <BLU169-W100522438783894807A10CBC7470@phx.gbl>,
	<AA818EAD2576BC488B4F623941DA7427F3A40DE2@inbomail.inbo.be>
Message-ID: <BLU169-W5721C82192B395DA6BFB33C7470@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140428/da5b1f03/attachment.pl>

From ndoogan at gmail.com  Sun Apr 27 20:29:31 2014
From: ndoogan at gmail.com (Nathan Doogan)
Date: Sun, 27 Apr 2014 14:29:31 -0400
Subject: [R-sig-ME] Fwd: Multi Processor / lme4
In-Reply-To: <535D45D0.10709@gmail.com>
References: <535D1BFB.3060409@gmail.com> <535D45D0.10709@gmail.com>
Message-ID: <535D4C8B.6000608@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140427/67a034ad/attachment.pl>

From bbolker at gmail.com  Mon Apr 28 17:11:22 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 28 Apr 2014 11:11:22 -0400
Subject: [R-sig-ME]
 =?windows-1252?q?=5BR=5D_lme4_Error_Help=3A_=93maxstep?=
 =?windows-1252?q?halfit=85pwrssUpdate=94?=
In-Reply-To: <BLU169-W5721C82192B395DA6BFB33C7470@phx.gbl>
References: <BLU169-W100522438783894807A10CBC7470@phx.gbl>,
	<AA818EAD2576BC488B4F623941DA7427F3A40DE2@inbomail.inbo.be>
	<BLU169-W5721C82192B395DA6BFB33C7470@phx.gbl>
Message-ID: <535E6F9A.3090801@gmail.com>

On 14-04-28 07:57 AM, Craig O'Connell wrote:

> Thanks.  I used the most current version of lme4 that is why I was a
  bit concerned.  My data seems appropriate and with lme4 working last
  week on a very similar data set, I was left a bit confused.  Since I
  only starting implementing this technique, does anybody have some
  pointers on what I should look for that may potentially cause some
  issues?


Does the new as well as the old version of lme4 work on the very
similar data set you analyzed last week?

If you think the problem is with a broken version of lme4 you could
try installing lme4.0 (from http://lme4.r-forge.r-project.org/repos)
and see if it makes a difference.

I would look for extreme observations/groups in the data sets
(e.g. cases with all-zero avoidances; if the overall mean is positive
you may need an infinite parameter value for this case).  If you
can come up with a reproducible example, there's a chance we can
set some bounds within the internal code to allow this to fit ...

If you set some of the groups to all-zero avoidance in the data
set that previously worked can you get it to stop working?

  PS -- I don't know how your data are structured, but if you have
a small number of visits and the number of avoidances is _not_
necessarily a small fraction of the number of visits, you may
want to consider a binomial rather than a Poisson model ...


>> -----Oorspronkelijk bericht-----
>> Van: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Namens Craig O'Connell
>> Verzonden: maandag 28 april 2014 3:20
>> Aan: r-help at r-project.org
>> Onderwerp: [R] lme4 Error Help: ?maxstephalfit?pwrssUpdate?

>> I am using a mixed model to assess the effects of various variables
   (i.e. treatment, density, visibility) on bee behavior (e.g.,
   avoidance frequency - total avoidances per total visits; feeding
   frequency, and mating frequency).  Bee individuals is my random
   factor (n=63 different bees), whereas treatment type, animal
   density, and air visibility are my fixed factors.

>> However, when I run my models, I immediately get an error that I
   cannot fix.  Here is a sample of my data:

>> Bee   Treatment    Visits    Avoid   Feeding    Mating    Density   Visibility
>>
>> 1   C   5   0   5   0      5        4
>> 2   C   4   0   3   0      5        4
>> 3   C   3   0   3   0      5        4
>> ...
>> 63
>>
>> 1   PC  2   0   1   1      5        4
>> 2   PC  3   0   0   3      5        4
>> 3   PC  1   0   0   0      5        4
>> ...
>> 63
>>
>> 1   M   5   0   1   3      5        4
>> 2   M   3   2   0   0      5        4
>> 3   M   2   0   0   2      5        4

>> 63One I create my .txt file, I being my coding in R by first loading lme4.  After that, my coding starts off as follows:
>> barrierdat = read.table("GLMMROW.txt", header=TRUE) barrierdat barrierdat$Visibility = as.factor(barrierdat$Visibility);
barrierdat$Density    = as.factor(barrierdat$Density)
p01.glmer = glmer(Avoidance~offset(log(Visits))+(1|Bee), family=poisson,
                  data=egghead);  # null model


   Note that you can often use update() to simplify your code, e.g.

p02.glmer = update(p01.glmer,.~.+Treatment)
p03.glmer = update(p01.glmer,.~.+Visibility)
p04.glmer = update(p01.glmer,.~.+Density)


From bates at stat.wisc.edu  Mon Apr 28 17:45:27 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 28 Apr 2014 10:45:27 -0500
Subject: [R-sig-ME] Fwd: Multi Processor / lme4
In-Reply-To: <535D4C8B.6000608@gmail.com>
References: <535D1BFB.3060409@gmail.com> <535D45D0.10709@gmail.com>
	<535D4C8B.6000608@gmail.com>
Message-ID: <CAO7JsnSYqEt5nz=9B=M42qsqtXpyHo8zbYs+O_dAjm6qAaKrRA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140428/c6d40f95/attachment.pl>

From bbolker at gmail.com  Tue Apr 29 00:14:24 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 28 Apr 2014 22:14:24 +0000 (UTC)
Subject: [R-sig-ME] Profile likelihood using R2admb
References: <5688717c291a4f8dbc25fbe42ff3873a@AM3PR01MB273.eurprd01.prod.exchangelabs.com>
Message-ID: <loom.20140429T001257-877@post.gmane.org>

Stirrup, Oliver <oliver.stirrup.13 at ...> writes:

> 
> Dear Prof Bolker
> 
> Thank you for the quick response, and for developing 
> this package in the first place!

  I have reported on Github (and am in the process of fixing)
that this is a documentation bug: you should use 
profile.opts=list(pars=c(...)) rather than profpars=c(...) as
in the vignette (this _is_ documented in ?do_admb, but it's
wrong in the vignette)

  Ben Bolker


From s.blomberg1 at uq.edu.au  Tue Apr 29 03:56:37 2014
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Tue, 29 Apr 2014 11:56:37 +1000
Subject: [R-sig-ME] FW: linear mixed model for non-normal negative and
 continous data
In-Reply-To: <DUB110-W11644F28691E38DDB5C8EF0E0470@phx.gbl>
References: <DUB110-W77FDB4E4F048BD295154DBE0470@phx.gbl>
	<DUB110-W11644F28691E38DDB5C8EF0E0470@phx.gbl>
Message-ID: <535F06D5.90400@uq.edu.au>

You could try using the Yeo-Johnson transformation in the car package.

Cheers,

Simon.

On 28/04/14 21:31, Caroline Lustenberger wrote:
> Dear all
>   
> I try to fit a linear mixed model to my data. In short, my dependent variable reflects changes of the bone level (Knmn, in mm), thus this variable is continous and provides negative values. I have two different groups (factor Group) that were measured 3 times each (thus repeated measures, factor Timepoint). I used the following model:
>   
> mod_Knmn<-lmer(Knmn~Group*Timepoint+(1|VPnr),data=data)
>   
> When performing a qq-plot my residuals are clearly deviant from the norm (long-tailed). Due to negative values I cannot perform classical transformation methods (e.g. log transformation). How could I proccede with this data. Is there a possibility to use a generalized linear model?
>   
> Thanks and all the best
> Caroline
>
>   		 	   		
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat, AStat.
Senior Lecturer and Consultant Statistician
School of Biological Sciences
The University of Queensland
St. Lucia Queensland 4072
Australia
T: +61 7 3365 2506
email: S.Blomberg1_at_uq.edu.au
http://www.evolutionarystatistics.org

Policies:
1.  I will NOT analyse your data for you.
2.  Your deadline is your problem.

Statistics is the grammar of science - Karl Pearson.


From zaslavsk at hcp.med.harvard.edu  Tue Apr 29 00:09:37 2014
From: zaslavsk at hcp.med.harvard.edu (Zaslavsky, Alan M.)
Date: Mon, 28 Apr 2014 18:09:37 -0400
Subject: [R-sig-ME] confint.merMod()  problem.
Message-ID: <A1E2A16402841149A85612B5EC3D6D76028545BB2765@ITCCRMAIL01.MED.HARVARD.EDU>

I am running into what might be a bug in confint.merMod()  with boot.type="norm".  The error occurs on the line near the end of confint.merMod() that labels the value:          dimnames(citab) <- list(names(bb[["t0"]]), pct)
but appears to arise a few lines up, where the expression that creates citab is hardwired to extract from the structure created when boot.type="perc":
        citab <- t(sapply(bci, function(x) x[["percent"]][4:5]))
More details appear below.  I don't understand the structures enough to propose  a fix other than suggesting that the line above should extract in a way dependent on the value of boot.type.

Package:            lme4
Version:            1.1-6

platform       x86_64-w64-mingw32          
version.string R version 3.1.0 (2014-04-10)
=================================================================================================

Here are the last few lines of confint.merMod():   
     bb <- bootMer(object, bootFun, nsim = nsim, ...)
        bci <- lapply(seq_along(bb$t0), boot.out = bb, boot::boot.ci, 
            type = boot.type, conf = level)
        citab <- t(sapply(bci, function(x) x[["percent"]][4:5]))
        a <- (1 - level)/2
        a <- c(a, 1 - a)
        pct <- format.perc(a, 3)
        dimnames(citab) <- list(names(bb[["t0"]]), pct)
        pnames <- rownames(citab)
        if (missing(parm)) parm <- pnames else if (is.numeric(parm)) parm <- pnames[parm]
        citab[parm, ]
    }, stop("unknown confidence interval method"))

=================================================================================================
This is the model and the call to confint() followed by verbose output and error message:
> xmodsp
Linear mixed model fit by REML ['lmerMod']
Formula: im.pneum ~ 0 + group + (0 + group | aco.id)
   Data: mydata
REML criterion at convergence: 10837.66
Random effects:
 Groups   Name    Std.Dev. Corr
 aco.id   groupFH 0.03173      
          groupFL 0.05405  1.00
 Residual         0.41305      
Number of obs: 10022, groups: aco.id, 152
Fixed Effects:
groupFH  groupFL  
  1.135    1.239  
> confint(xmodsp,parm=1:5,method="boot",boot.type="norm",nsim=5,verbose=T,.progress="win")
Computing bootstrap confidence intervals ...
    1 : Named num [1:6] 0.0208 1 0.0603 0.4118 1.1406 ...
 - attr(*, "names")= chr [1:6] "sd_groupFH|aco.id" "cor_groupFL.groupFH|aco.id" "sd_groupFL|aco.id" "sigma" ...
    2 : Named num [1:6] 0.0477 0.9133 0.0657 0.4121 1.138 ...
 - attr(*, "names")= chr [1:6] "sd_groupFH|aco.id" "cor_groupFL.groupFH|aco.id" "sd_groupFL|aco.id" "sigma" ...
    3 : Named num [1:6] 0.0302 1 0.0598 0.4136 1.1177 ...
 - attr(*, "names")= chr [1:6] "sd_groupFH|aco.id" "cor_groupFL.groupFH|aco.id" "sd_groupFL|aco.id" "sigma" ...
    4 : Named num [1:6] 0.0745 1 0.0512 0.4159 1.1364 ...
 - attr(*, "names")= chr [1:6] "sd_groupFH|aco.id" "cor_groupFL.groupFH|aco.id" "sd_groupFL|aco.id" "sigma" ...
    5 : Named num [1:6] 0.055 1 0.0533 0.4148 1.1247 ...
 - attr(*, "names")= chr [1:6] "sd_groupFH|aco.id" "cor_groupFL.groupFH|aco.id" "sd_groupFL|aco.id" "sigma" ...
Error in dimnames(citab) <- list(names(bb[["t0"]]), pct) : 
  length of 'dimnames' [1] not equal to array extent
=================================================================================================
Here are values of the variables involved just before the offending line:
Browse[1]> bb
Call:
bootMer(x = object, FUN = bootFun, nsim = nsim, verbose = ..1, 
    .progress = "win")
Bootstrap Statistics :
      original       bias    std. error
t1* 0.03172667 -0.003491017 0.020826059
t2* 1.00000000 -0.135380451 0.302719857
t3* 0.05404599 -0.003365170 0.008439177
t4* 0.41304973 -0.002945966 0.003552465
t5* 1.13522212  0.008405088 0.014985916
t6* 1.23883999  0.002633327 0.004492324
Browse[1]> str(bci)
List of 6
 $ :List of 4
  ..$ R     : int 5
  ..$ t0    : Named num 0.0317
  .. ..- attr(*, "names")= chr "sd_groupFH|aco.id"
  ..$ call  : language FUN(boot.out = ..1, conf = ..3, type = ..2, index = 1:6[[1L]])
  ..$ normal: num [1, 1:3] 0.95 -0.0056 0.076
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr "sd_groupFH|aco.id"
  .. .. ..$ : chr [1:3] "conf" "" ""
  ..- attr(*, "class")= chr "bootci"
		[5 more components, omitted for legibility]
Browse[1]> citab
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,] NULL NULL NULL NULL NULL NULL
=================================================================================================


From bbolker at gmail.com  Wed Apr 30 02:57:20 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 30 Apr 2014 00:57:20 +0000 (UTC)
Subject: [R-sig-ME] confint.merMod()  problem.
References: <A1E2A16402841149A85612B5EC3D6D76028545BB2765@ITCCRMAIL01.MED.HARVARD.EDU>
Message-ID: <loom.20140430T025538-133@post.gmane.org>

Zaslavsky, Alan M. <zaslavsk at ...> writes:

>  I am running into what might be a bug in confint.merMod() with
> boot.type="norm".  The error occurs on the line near the end of
> confint.merMod() that labels the value: dimnames(citab) <-
> list(names(bb[["t0"]]), pct) but appears to arise a few lines up,
> where the expression that creates citab is hardwired to extract from
> the structure created when boot.type="perc": citab <- t(sapply(bci,
> function(x) x[["percent"]][4:5])) More details appear below.  I
> don't understand the structures enough to propose a fix other than
> suggesting that the line above should extract in a way dependent on
> the value of boot.type.

> Package:            lme4
> Version:            1.1-6
> 
> platform       x86_64-w64-mingw32          
> version.string R version 3.1.0 (2014-04-10)
> 

  Thanks for the report.  This is fixed in the Github version (1.1-7)
now (if you or someone else needs this soon and can't do
library(devtools); install_github("lme4","lme4") , let me know
and I'll post new binaries at http://lme4.r-forge.r-project.org/repos .


From bbolker at gmail.com  Wed Apr 30 03:05:14 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 30 Apr 2014 01:05:14 +0000 (UTC)
Subject: [R-sig-ME] Should I use GLMM?
References: <CAE5HZZ+dAdwhaSkNW6qiSYC4GyQHLcjFqWD33BJRS1xHB9OStA@mail.gmail.com>
Message-ID: <loom.20140430T025743-612@post.gmane.org>

Rodrigo Tardin <rhtardin at ...> writes:

> 
> Dear all,
> 
> I am new to spatial analysis and I've been struck with what model to use
> for my PhD analysis. If somebody could help me, I would be very appreciated.
 
> I am investigating the distribution of Bryde's whales in Rio de Janeiro
> waters and trying to identify the influence of three explanatory variables
> (Depth, Distance to coast and Sea slope). Each day I went to the sea, in
> which I observed a whale, I marked a first GPS location for that whale and
> started to follow it, marking new GPS locations at every 500m of whale's
> movement. I have 22 days of observations, in which each day I have 7-8 GPS
> locations for each whale. I was told that if I use all that GPS locations I
> am using a dataset that is not independent from each other. Can GLMM work
> for me, in which I can insert in my model the ID of each whale and, thus,
> somehow correct or make the model work? If not, can anybody point me a
> direction?

   It's not entirely clear to me what question you're specifically
trying to address with "influence" -- are you trying essentially to
look for habitat preference/selection by whales?  This is itself a
bit of a difficult problem, because your data set contains only
the explanatory variables from the places where you found the whales,
not from places where you didn't find them.  A standard approach to
this (although not without its problems) is to make up a series
of "pseudo-absences" by picking points at random from the plausible
region where you might have found whales, and doing a logistic
regression with the presences and pseudo-absences.  In that case
you should probably pick a set of pseudo-absences for each individual,
then do

  glmer(pres ~ depth+dist+slope+ (depth+dist+slope|ID), 
     family=binomial, data= ...)

* How many whales?
* You may need to worry about temporal/spatial autocorrelation as
well -- also not trivial, aggregating data (at the cost of losing
data) is the simplest solution.  Over what temporal/spatial scales
are these variables similar to each other?

  You should really get some more focused statistical help if at
all possible -- this is not a trivial problem ...

  Ben Bolker


From medo_botany at hotmail.com  Wed Apr 30 11:44:39 2014
From: medo_botany at hotmail.com (Hamada Elsayed Ali)
Date: Wed, 30 Apr 2014 09:44:39 +0000
Subject: [R-sig-ME] Comparison of 2 treatments
Message-ID: <DUB119-W3179EBE4954C20EBF58BB39C410@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140430/a49bd955/attachment.pl>

From bbolker at gmail.com  Wed Apr 30 15:14:36 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 30 Apr 2014 09:14:36 -0400
Subject: [R-sig-ME] Comparison of 2 treatments
In-Reply-To: <DUB119-W3179EBE4954C20EBF58BB39C410@phx.gbl>
References: <DUB119-W3179EBE4954C20EBF58BB39C410@phx.gbl>
Message-ID: <5360F73C.80400@gmail.com>

On 14-04-30 05:44 AM, Hamada Elsayed Ali wrote:
> Dear All,

> I'd like for your help as I have a sediment data collected before,
> within and after the field margin and my data looks like that:

> Site sediment   position   management      slope
> 
> 1    15.20      before     managed         steep
> 1    10.10      within     managed         steep
> 1    11.20      after      managed         steep
> 2    14.20      before     natural         steep
> 2    12.10      within     natural         steep
> 2    13.20      after      natural         steep
> 3    8.20       before     managed         flat
> 3    11.10      within     managed         flat
> 3    14.20      after      managed         flat
> 4    16.20      before     natural         flat
> 4    12.10      within     natural         flat
> 4    10.20      after      natural         flat

> I'd like to compare the sediment before and after in the management
> and slope and to know how much the sediment reduction or the
> increasing in the different treatments.  Can anybody help me in such
> a question.  Thanks,

  Is this all of your data or a subset?

  What have you tried?  Have you read any literature on mixed models
(e.g. Pinheiro and Bates's 2000 book, or Gelman and Hill 2008,
or one of Zuur et al's books)?  Do you know/can you express
what model you would fit if there were no random effects involved?

  Ben Bolker


From dhocking at umass.edu  Wed Apr 30 18:03:44 2014
From: dhocking at umass.edu (Daniel Hocking)
Date: Wed, 30 Apr 2014 12:03:44 -0400
Subject: [R-sig-ME] gamm4 error with large dataset
Message-ID: <EA0E2A6F-8DF7-4A97-96A4-6B64A89572FE@umass.edu>

I am trying to predict daily water temperature from air temperature primarily but ideally would include other factors such as precipitation and landscape characteristics. I have paired air and water temperatures from 600+ sites over a ~10 year period. Some sites have daily temperatures for just a couple months and others for years, and some for a couple months sporadically in different years. I am trying to use a mixed effects gamm so I can include random effects of site and year and smooth over day of the year (dOY). My dataframe is 

and I get the following error when I run this code

system.time(gamm4Full <- gamm4(temp ~ airTemp + airTempLagged1 + airTempLagged2 + prcp + prcpLagged1 + prcpLagged2 + Latitude + Longitude + Forest + Agriculture + BasinElevationM + ReachSlopePCNT + CONUSWetland + SurficialCoarseC + s(dOY) + prcp*airTemp, random = ~(1| site) + (1 | year), data = etS)) 

# Error in crossprod(root.phi %*% Zt) : 
# Cholmod error 'problem too large' at file ../Core/cholmod_aat.c, line 173
# In addition: Warning message:
#   In optwrap(optimizer, devfun, getStart(start, rho$lower, rho$pp),  :
#               convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded


My plan was to try gamm4 and if there was autocorrelation issues to switch to gamm within mgcv. I know bam is designed for large data but I?m not sure how I would code the random effects using bam. I know in general it?s s(dOY, bs = ?re?) but I?m not sure how to relate this to site and year. Ideally I would have random slopes for airTemp effects for each site because of things like ground water inputs that we don?t measure.

Any advice would be appreciated,
Dan
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Daniel Hocking
Department of Environmental Conservation
Northeast Climate Science Center
University of Massachusetts

http://www.danieljhocking.wordpress.com
dhocking at umass.edu
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


From bbolker at gmail.com  Wed Apr 30 18:23:30 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 30 Apr 2014 12:23:30 -0400
Subject: [R-sig-ME] gamm4 error with large dataset
In-Reply-To: <EA0E2A6F-8DF7-4A97-96A4-6B64A89572FE@umass.edu>
References: <EA0E2A6F-8DF7-4A97-96A4-6B64A89572FE@umass.edu>
Message-ID: <53612382.6000703@gmail.com>

On 14-04-30 12:03 PM, Daniel Hocking wrote:
> I am trying to predict daily water temperature from air temperature
> primarily but ideally would include other factors such as
> precipitation and landscape characteristics. I have paired air and
> water temperatures from 600+ sites over a ~10 year period. Some sites
> have daily temperatures for just a couple months and others for
> years, and some for a couple months sporadically in different years.
> I am trying to use a mixed effects gamm so I can include random
> effects of site and year and smooth over day of the year (dOY). My
> dataframe is
> 
> and I get the following error when I run this code
> 
> system.time(gamm4Full <- gamm4(temp ~ airTemp + airTempLagged1 +
> airTempLagged2 + prcp + prcpLagged1 + prcpLagged2 + Latitude +
> Longitude + Forest + Agriculture + BasinElevationM + ReachSlopePCNT +
> CONUSWetland + SurficialCoarseC + s(dOY) + prcp*airTemp, random =
> ~(1| site) + (1 | year), data = etS))
> 
> # Error in crossprod(root.phi %*% Zt) : # Cholmod error 'problem too
> large' at file ../Core/cholmod_aat.c, line 173 # In addition: Warning
> message: #   In optwrap(optimizer, devfun, getStart(start, rho$lower,
> rho$pp),  : #               convergence code 1 from bobyqa: bobyqa --
> maximum number of function evaluations exceeded
> 
> 
> My plan was to try gamm4 and if there was autocorrelation issues to
> switch to gamm within mgcv. I know bam is designed for large data but
> I?m not sure how I would code the random effects using bam. I know in
> general it?s s(dOY, bs = ?re?) but I?m not sure how to relate this to
> site and year. Ideally I would have random slopes for airTemp effects
> for each site because of things like ground water inputs that we
> don?t measure.
> 

  I can imagine that this problem is caused by the size of the
fixed-effect matrix.  A couple of thoughts (none of them practical, I'm
afraid):

  * I was going to say that it's too bad that we haven't yet managed to
implement a sparse model matrix structure;
  * then I was going to say that a potential trick/workaround for this
(for many-level _categorical_ variables) is to treat the factor as a
random effect, then use devFunOnly/modular structure to fix the theta
parameter for that variable at a large value, making it a pseudo-fixed
effect and getting the benefits of (1) a little bit of regularization
and (2) model matrix sparsity -- but doing this within gamm4 would be
harder/require more hacking
  * then I realized that your fixed-effect model matrix probably isn't
sparse, because it looks like it's made up entirely of continuous covariates
  * that got me thinking about the fact that some of your continuous
covariates only vary at higher levels (i.e. Lat/Long and presumably
Forest, Agriculture, elevation, etc.), and wondering whether there would
be any way to save space by going back to the underlying model
formulation and writing this out in terms of another multiplication of
higher-level covariates times an indicator matrix ...

  ... all of which is fascinating (to me at least) but none of which
actually gets you any farther with your specific problem.  Sorry.

  Ben Bolker


From mwiederm at mtu.edu  Wed Apr 30 19:04:12 2014
From: mwiederm at mtu.edu (Lena)
Date: Wed, 30 Apr 2014 17:04:12 +0000 (UTC)
Subject: [R-sig-ME] previous posts about error message in glmmADMB
References: <CAOCHjhTc38==KKBeWVAfqJoNytSd+Q_P84dHwpq5wfDFkhdKkA@mail.gmail.com>
	<519BA9D2.4000101@gmail.com>
Message-ID: <loom.20140430T181519-196@post.gmane.org>

Parameters were estimated, but not standard errors were not: the most likely
problem is that the curvature at MLE was zero or negative Error in
glmmadmb(resp ~ N * treecov + (1 | plot/subplot), data = xyz,  : 
 The function maximizer failed (couldn't find STD file) Troubleshooting
steps include (1) run with 'save.dir' set and inspect output files; (2)
change run parameters: see '?admbControl'

In addition: Warning message:
running command 'C:\WINDOWS\system32\cmd.exe /c "C:/Program Files/R/R
3.0.2/library/glmmADMB/bin/windows32/glmmadmb.exe" -maxfn 500
-maxph 5 -noinit -shess' had status 1 

Dear list,

I get the exact same error messages (see above) regardless of using
family="binomial", family="nbinom1", family="nbinom2"

N, plot and subplot = factor
tree cover can only be continuous
resp<-cbind(dead,alive)

model<-glmmadmb(resp~N*treecov+(1|plot/subplot),
data=xyz,zeroInflation=TRUE,family="binomial")

the data consists of 306 zero values in the response (out of 750).

Any suggestions what else I could try to get the glmmADMB running.
Thank you,
Lena


From m.c.zwart at newcastle.ac.uk  Wed Apr 30 20:06:38 2014
From: m.c.zwart at newcastle.ac.uk (Mieke Zwart)
Date: Wed, 30 Apr 2014 18:06:38 +0000
Subject: [R-sig-ME] MCMCglmm: starting values and variance explained by
	random effects
Message-ID: <D311812C2E01714397A1925A334CD81F566EF125@EXMBCT03.campus.ncl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140430/0610d1d9/attachment.pl>

From j.hadfield at ed.ac.uk  Thu May  1 07:25:01 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 01 May 2014 06:25:01 +0100
Subject: [R-sig-ME] MCMCglmm: starting values and variance explained by
 random effects
In-Reply-To: <D311812C2E01714397A1925A334CD81F566EF125@EXMBCT03.campus.ncl.ac.uk>
References: <D311812C2E01714397A1925A334CD81F566EF125@EXMBCT03.campus.ncl.ac.uk>
Message-ID: <20140501062501.68545qewson390so@www.staffmail.ed.ac.uk>

Hi Mieke,

Your model sounds reasonable. The warning is because you had  
start=list(QUASI=FALSE) in the call to MCMCglmm, so `good' starting  
values weren't used and the starting latent variables were drawn from  
a unit normal. Nevertheless it looks like they converged, although I  
always plot the traces too diagnose bad mixing (for the types of model  
MCMCglmm fits you nearly always get convergence unless there is  
something wrong with the model).

The proportion of variance explained that you give is for the latent  
scale. This is OK, but you should bear in mind that on the data scale  
there is additional variance in the denominator coming from the  
Poisson distribution itself.

The default priors for the fixed effects are as you say. For the  
variance components the default is nu=0 (i.e. a flat improper prior).  
It sounds from your verbal description as if there is strong support  
for between-site heterogeneity in abundance.

Cheers,

Jarrod


uoting Mieke Zwart <m.c.zwart at newcastle.ac.uk> on Wed, 30 Apr 2014  
18:06:38 +0000:

> Dear list members,
>
> First of all, I would like to say that I think it is great that this  
> list exists. I have learned so much by reading posts regularly and  
> searching for answers when I encounter a problem.
>
> I have searched extensively before making this post, but have not  
> been able to find an answer to some specific issues I encountered:
>
> I need some help regarding results that I get from a model run with  
> the package MCMCglmm. I thought I interpreted things correctly after  
> reading a lot of posts on here and reading through the Course Notes  
> of the package, however a recent paper of mine got rejected and one  
> reviewer had quite a few problems with the model. Before I send the  
> paper anywhere else I would like to make sure that I am interpreting  
> and explaining things correctly.
>
> Some brief explanation about the study:
> The data contains counts of birds at 9 different locations before  
> and after a development (several years before, and several years  
> after (up to 15 years post-construction)). We are interested in  
> whether the counts changed after development. Since the initial  
> numbers at each site are variable and differ quite a lot between  
> sites, I used a random effect for site.
> I used MCMCglmm due to overdispersion using frequentist methods.
>
> The poisson model looks like this:
> MCMCglmm(counts ~ bef_af, random=~Site, data=dataframe, pr=TRUE,  
> pl=TRUE, family="poisson", nitt=65000, thin=50, burnin=15000,  
> start=list(QUASI=FALSE))
> where 'counts' is the number of birds per survey, 'bef_af' is a  
> factor with either 0 or 1 (where 0 is before and 1 is after), 'site'  
> is a character vector with the 9 different site names.
>
> The model is run 3 times to give 3 different chains. The chains are  
> then checked for convergence via plotting:
> plot(mcmc.list(chain1$Sol, chain2$Sol, chain3$Sol))
> In addition, I checked the Gelman and Rubin's convergence diagnostic:
> gelman.diag(mcmc.list(chain1$Sol, chain2$Sol, chain3$Sol))
>
> The model gives the following error for the starting values:
> Warning message:
> In MCMCglmm(counts ~ bef_af, random = ~Site, data = dataframe,  :
>   good starting values not obtained: using Norm(0,1)
>
> The plots show adequate mixing of the chains but I am wondering  
> whether the chains started at different appropriate values due to  
> the warning message. Should I be concerned about the warning  
> message? Did it use starting values drawn from a normal distribution?
>
> The Gelman and Rubin's diagnostic gave the following:
> Potential scale reduction factors:
>
>               Point est. Upper C.I.
> (Intercept)            1       1.00
> bef_af1                1       1.00
> Site.Site 1b          1       1.01
> Site.Site 2           1       1.00
> Site.Site 3           1       1.00
> Site.Site 4           1       1.00
> Site.Site 5           1       1.00
> Site.Site 6           1       1.00
> Site.Site 7           1       1.00
> Site.Site 8           1       1.00
> Site.Site 9           1       1.00
>
> Multivariate psrf
>
> 1.01
>
> Furthermore, I checked how much variance was explained by the random effect:
>
> HPDinterval(chain1$VCV[, "Site"]/(chain1$VCV[, "Site"] +  
> chain1$VCV[, "units"]))
>
>         lower     upper
> var1 0.388643 0.8729441
> attr(,"Probability")
> [1] 0.95
>
> I interpreted this as follows: the majority of the variation in the  
> data was explained by the difference between the locations. Both the  
> 'Site' random effect and the residuals 'Units' posterior  
> distribution plots show that both are located well away from zero  
> (Plotted via plot(chain1$VCV)). Is my interpretation correct? To me  
> it makes sense as the numbers of birds between the locations varied  
> a lot (from mean of 2 birds at one location to a mean of 20 birds at  
> another location).
> When a prior is not given in MCMCglmm() what defaults does it use?  
> From the documentation I can see prior=NULL, but I assume that some  
> prior must be given for bayesian models. Are the defaults: B$mu=0  
> and B$V=I*1e+10, where where I is an identity matrix of appropriate  
> dimension? I therefore assume that the default priors in MCMCglmm  
> are centered on zero and since the posterior distribution is well  
> away from zero, that therefore the random effects explain some  
> variation in the data (especially 'site' which explains 38.8-87.3%).  
> Is this correct?
>
> Thanks!
>
> Mieke
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From m.c.zwart at newcastle.ac.uk  Thu May  1 12:20:53 2014
From: m.c.zwart at newcastle.ac.uk (Mieke Zwart)
Date: Thu, 1 May 2014 10:20:53 +0000
Subject: [R-sig-ME] MCMCglmm: starting values and variance explained by
 random effects
In-Reply-To: <20140501062501.68545qewson390so@www.staffmail.ed.ac.uk>
References: <D311812C2E01714397A1925A334CD81F566EF125@EXMBCT03.campus.ncl.ac.uk>,
	<20140501062501.68545qewson390so@www.staffmail.ed.ac.uk>
Message-ID: <D311812C2E01714397A1925A334CD81F566EF15E@EXMBCT03.campus.ncl.ac.uk>

Hi Jarrod,

Thanks for your quick reply.

The reason why I used start=list(QUASI=FALSE) is because I read that you need overdispersed starting values to use the Gelman and Rubin's diagnostic. And in a different post on here you said (post from 23/10/2009 https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q4/002972.html):
"If you want over-dispersed starting values to make sure the chain is converging to the same distribution you can  
specify starting values in the start argument of MCMCglmm. start=list(QUASI=FALSE) is a quick way of getting sei-overdispersed starting values."

Due to the warning message, I assume the model did not use overdispersed starting values. Is it therefore still OK to use Gelman and Rubin's diagnostic? I plotted the traces and they show adequate mixing. If I can't use Gelman and Rubin's diagnostic (due to not using overdispersed starting values), is it sufficient to just check the trace plots for convergence?

Regarding the proportion of variance explained by the random effect: Can I measure the additional variance in the denominator coming from the Poisson distribution itself? Or is it something that I should not be concerned about? For example, I can confidently say that the majority of the variation in the data is explained by the random effect "site".

Thanks,

Mieke

Mieke Zwart
PhD student
School of Biology
Ridley 2
Newcastle University
Newcastle upon Tyne
NE1 7RU
United Kingdom
________________________________________
From: Jarrod Hadfield [j.hadfield at ed.ac.uk]
Sent: 01 May 2014 06:25
To: Mieke Zwart
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] MCMCglmm: starting values and variance explained by random effects

Hi Mieke,

Your model sounds reasonable. The warning is because you had
start=list(QUASI=FALSE) in the call to MCMCglmm, so `good' starting
values weren't used and the starting latent variables were drawn from
a unit normal. Nevertheless it looks like they converged, although I
always plot the traces too diagnose bad mixing (for the types of model
MCMCglmm fits you nearly always get convergence unless there is
something wrong with the model).

The proportion of variance explained that you give is for the latent
scale. This is OK, but you should bear in mind that on the data scale
there is additional variance in the denominator coming from the
Poisson distribution itself.

The default priors for the fixed effects are as you say. For the
variance components the default is nu=0 (i.e. a flat improper prior).
It sounds from your verbal description as if there is strong support
for between-site heterogeneity in abundance.

Cheers,

Jarrod


uoting Mieke Zwart <m.c.zwart at newcastle.ac.uk> on Wed, 30 Apr 2014
18:06:38 +0000:

> Dear list members,
>
> First of all, I would like to say that I think it is great that this
> list exists. I have learned so much by reading posts regularly and
> searching for answers when I encounter a problem.
>
> I have searched extensively before making this post, but have not
> been able to find an answer to some specific issues I encountered:
>
> I need some help regarding results that I get from a model run with
> the package MCMCglmm. I thought I interpreted things correctly after
> reading a lot of posts on here and reading through the Course Notes
> of the package, however a recent paper of mine got rejected and one
> reviewer had quite a few problems with the model. Before I send the
> paper anywhere else I would like to make sure that I am interpreting
> and explaining things correctly.
>
> Some brief explanation about the study:
> The data contains counts of birds at 9 different locations before
> and after a development (several years before, and several years
> after (up to 15 years post-construction)). We are interested in
> whether the counts changed after development. Since the initial
> numbers at each site are variable and differ quite a lot between
> sites, I used a random effect for site.
> I used MCMCglmm due to overdispersion using frequentist methods.
>
> The poisson model looks like this:
> MCMCglmm(counts ~ bef_af, random=~Site, data=dataframe, pr=TRUE,
> pl=TRUE, family="poisson", nitt=65000, thin=50, burnin=15000,
> start=list(QUASI=FALSE))
> where 'counts' is the number of birds per survey, 'bef_af' is a
> factor with either 0 or 1 (where 0 is before and 1 is after), 'site'
> is a character vector with the 9 different site names.
>
> The model is run 3 times to give 3 different chains. The chains are
> then checked for convergence via plotting:
> plot(mcmc.list(chain1$Sol, chain2$Sol, chain3$Sol))
> In addition, I checked the Gelman and Rubin's convergence diagnostic:
> gelman.diag(mcmc.list(chain1$Sol, chain2$Sol, chain3$Sol))
>
> The model gives the following error for the starting values:
> Warning message:
> In MCMCglmm(counts ~ bef_af, random = ~Site, data = dataframe,  :
>   good starting values not obtained: using Norm(0,1)
>
> The plots show adequate mixing of the chains but I am wondering
> whether the chains started at different appropriate values due to
> the warning message. Should I be concerned about the warning
> message? Did it use starting values drawn from a normal distribution?
>
> The Gelman and Rubin's diagnostic gave the following:
> Potential scale reduction factors:
>
>               Point est. Upper C.I.
> (Intercept)            1       1.00
> bef_af1                1       1.00
> Site.Site 1b          1       1.01
> Site.Site 2           1       1.00
> Site.Site 3           1       1.00
> Site.Site 4           1       1.00
> Site.Site 5           1       1.00
> Site.Site 6           1       1.00
> Site.Site 7           1       1.00
> Site.Site 8           1       1.00
> Site.Site 9           1       1.00
>
> Multivariate psrf
>
> 1.01
>
> Furthermore, I checked how much variance was explained by the random effect:
>
> HPDinterval(chain1$VCV[, "Site"]/(chain1$VCV[, "Site"] +
> chain1$VCV[, "units"]))
>
>         lower     upper
> var1 0.388643 0.8729441
> attr(,"Probability")
> [1] 0.95
>
> I interpreted this as follows: the majority of the variation in the
> data was explained by the difference between the locations. Both the
> 'Site' random effect and the residuals 'Units' posterior
> distribution plots show that both are located well away from zero
> (Plotted via plot(chain1$VCV)). Is my interpretation correct? To me
> it makes sense as the numbers of birds between the locations varied
> a lot (from mean of 2 birds at one location to a mean of 20 birds at
> another location).
> When a prior is not given in MCMCglmm() what defaults does it use?
> From the documentation I can see prior=NULL, but I assume that some
> prior must be given for bayesian models. Are the defaults: B$mu=0
> and B$V=I*1e+10, where where I is an identity matrix of appropriate
> dimension? I therefore assume that the default priors in MCMCglmm
> are centered on zero and since the posterior distribution is well
> away from zero, that therefore the random effects explain some
> variation in the data (especially 'site' which explains 38.8-87.3%).
> Is this correct?
>
> Thanks!
>
> Mieke
>
>       [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

--
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.




From bbolker at gmail.com  Thu May  1 20:45:38 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 1 May 2014 18:45:38 +0000 (UTC)
Subject: [R-sig-ME] gamm4 error with large dataset
References: <EA0E2A6F-8DF7-4A97-96A4-6B64A89572FE@umass.edu>
	<53612382.6000703@gmail.com>
Message-ID: <loom.20140501T203953-47@post.gmane.org>

Ben Bolker <bbolker at ...> writes:

> 
> On 14-04-30 12:03 PM, Daniel Hocking wrote:
> > I am trying to predict daily water temperature from air temperature
> > primarily but ideally would include other factors such as
> > precipitation and landscape characteristics. I have paired air and
> > water temperatures from 600+ sites over a ~10 year period. Some sites
> > have daily temperatures for just a couple months and others for
> > years, and some for a couple months sporadically in different years.
> > I am trying to use a mixed effects gamm so I can include random
> > effects of site and year and smooth over day of the year (dOY).

  [snip to make Gmane happy]

> > 
> > 
> > My plan was to try gamm4 and if there was autocorrelation issues to
> > switch to gamm within mgcv. I know bam is designed for large data but
> > I?m not sure how I would code the random effects using bam. I know in
> > general it?s s(dOY, bs = ?re?) but I?m not sure how to relate this to
> > site and year. Ideally I would have random slopes for airTemp effects
> > for each site because of things like ground water inputs that we
> > don?t measure.
> > 
> 
>   I can imagine that this problem is caused by the size of the
> fixed-effect matrix.  A couple of thoughts (none of them practical, I'm
> afraid):
> 
>   * I was going to say that it's too bad that we haven't yet managed to
> implement a sparse model matrix structure;
>   * then I was going to say that a potential trick/workaround for this
> (for many-level _categorical_ variables) is to treat the factor as a
> random effect, then use devFunOnly/modular structure to fix the theta
> parameter for that variable at a large value, making it a pseudo-fixed
> effect and getting the benefits of (1) a little bit of regularization
> and (2) model matrix sparsity -- but doing this within gamm4 would be
> harder/require more hacking
>   * then I realized that your fixed-effect model matrix probably isn't
> sparse, because it looks like it's made up entirely of continuous covariates
>   * that got me thinking about the fact that some of your continuous
> covariates only vary at higher levels (i.e. Lat/Long and presumably
> Forest, Agriculture, elevation, etc.), and wondering whether there would
> be any way to save space by going back to the underlying model
> formulation and writing this out in terms of another multiplication of
> higher-level covariates times an indicator matrix ...
> 
>   ... all of which is fascinating (to me at least) but none of which
> actually gets you any farther with your specific problem.  Sorry.
> 
>   Ben Bolker
> 
  
  A couple of further thoughts:

 * if it's true that the size of the fixed-effects model matrix
is your problem, then bam may not help you either (I would guess,
without bothering to check, that it's intended to reduce the size/
increase the sparsity of the spline bases, rather than messing
with the fixed-effect covariates).  
 * you can check whether the fixed-effects matrix is the problem
by running the problem as a straight linear mixed model, i.e. with
lmer either without dOY or with something simple like ns(dOY,10);
you could also try Doug Bates's MixedModels package from Julia.
 * If you were willing to make year a fixed effect you would only
have a single RE, then you might be able to take advantage of
some model structures for increased efficiency (at least until
you incorporate interactions of site with various covariates)


From bbolker at gmail.com  Fri May  2 03:39:16 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 2 May 2014 01:39:16 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?Mixed-effects_conditional_logistic_regressio?=
	=?utf-8?q?n_in_lme4_v=2E=091=2E0-4_and_above?=
References: <CAJKR=GOP1stucApHSg_T8ECezN9mzKdezHGSyc5aWTK3YQXTAg@mail.gmail.com>
Message-ID: <loom.20140502T031853-879@post.gmane.org>

Javan Bauder <javanvonherp at ...> writes:

> 

 [snip]
 
> The analysis I am running is a mixed-effects conditional logistic
> regression. My colleagues and I are using these models to conduct
> habitat selection analyses with wildlife telemetry data where each
> telemetry point is paired with a measure of availability unique to
> that point. We then difference the value of our independent variable
> at the telemetry point (used) with the measure of availability for
> that variable and fit a no-intercept logistic regression model to
> those differences and use individual animal as a random slope effect.
> We have used the following model structure:

lmer(1 ~ ?1+diff100+?(-1+diff100+?|Individual), 
   data=data, family=?binomial?)

Do you *really* have a 1 on the left-hand side of the formula?
It's hard for me to even understand what this mean/what kind of
sensible results this would produce ...

> Whenever we have used this specification in the newer versions of lme4
> (v 1.0-4 and above) we receive an error. For example, when using lmer
> and lme4 v 1.0-4, I receive the following:
> 

[snip lmer example -- no real reason to even try that at this point]

> When we try using glmer we receive the following error:
>

mod1 <- glmer(status~-1+Density+(-1+Density|Name),
   data=input,family='binomial',weights=Weight)

> Error in function (fr, X, reTrms, family, nAGQ = 1L, verbose = 0L,
> control = glmerControl(),  :
>   Response is constant - cannot fit the model


I guess I don't understand your example sufficiently well.
This error should only be triggered when there is only a single
unique value of the response variable, and I can't figure out
why that should happen in your case.  Can you provide a reproducible
example?  The following trivial example works, but I guess it doesn't
look like your data ...

set.seed(101)
input <- expand.grid(Density=1:20,Name=factor(1:20))
input$status <- rbinom(nrow(input),size=1,prob=0.5)

library(lme4)
mod1 <- glmer(status~-1+Density+(-1+Density|Name),
   data=input,family='binomial')

                     
> Do you know of any ways to run this type of model structure in the new
> versions of lme4 (or any other R package)? We would really appreciate
> any insights you could provide.

  Can you point me to a reference for this type of model?
If I can convince myself that it ever makes sense to allow a model
with a constant response value, we could allow an option to glmerControl
to override/ignore the test for unique values

  Also note that conditional logits are _not_ quite the same thing
as a regular logit model -- see

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2011q2/016030.html
http://data.princeton.edu/wws509/notes/c6s3.html

  There is probably a way to set this up with the machinery of
lme4, but I haven't thought it through -- add it to the ridiculously
long list of things I think are possible but haven't had the time
to work out.

  Ben Bolker


From bbolker at gmail.com  Fri May  2 03:42:29 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 2 May 2014 01:42:29 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?Mixed-effects_conditional_logistic_regressio?=
	=?utf-8?q?n_in_lme4_v=2E=091=2E0-4_and_above?=
References: <CAJKR=GOP1stucApHSg_T8ECezN9mzKdezHGSyc5aWTK3YQXTAg@mail.gmail.com>
Message-ID: <loom.20140502T033933-589@post.gmane.org>

Javan Bauder <javanvonherp at ...> writes:

> 
> Hi Dr. Bolker
> 
> Thank you for responding to my post about installing an older version
> of lme4. I wanted to respond to your question about why I need an
> older version for my analyses.
> 
> The analysis I am running is a mixed-effects conditional logistic
> regression. My colleagues and I are using these models to conduct
> habitat selection analyses with wildlife telemetry data where each
> telemetry point is paired with a measure of availability unique to
> that point. We then difference the value of our independent variable
> at the telemetry point (used) with the measure of availability for
> that variable and fit a no-intercept logistic regression model to
> those differences and use individual animal as a random slope effect.
> We have used the following model structure:
>
 
glmer(1 ~ ?1+diff100+?(-1+diff100+?|Individual), 
  data=data, family=?binomial?)
> 

  PS -- sorry, now that I'm thinking about this more clearly,
I can see that you *are* thinking about conditional regression,
and I can see how you get your constant '1' on the left hand side.
I still don't think it makes sense though ... there must be some
more appropriate way to do this ...

  Ben Bolker


From bbolker at gmail.com  Fri May  2 23:18:31 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 02 May 2014 17:18:31 -0400
Subject: [R-sig-ME] extract specific values from lmer summary
In-Reply-To: <5363F7CC.8020505@alaska.edu>
References: <5363F7CC.8020505@alaska.edu>
Message-ID: <53640BA7.1060307@gmail.com>

On 14-05-02 03:53 PM, Laura Prugh wrote:
> Hi Ben,
> I am trying to save specific outputs from the lmer summary, and I was
> hoping you might be able to tell me the code to do this. I'm running a
> random effects model with only the intercept as a fixed effect. I want
> to extract the fixed effect intercept estimate and the variance (or
> standard deviation) of the random effect. I seem to have figured out how
> to get the fixed effect (model at fixedef) but have had no luck getting the
> random effect. I guessed a variety of things and model at ranef returned
> something, but it didn't match anything in the summary output. Here is
> the output of my model, and I would like to extract and save the st dev
> of the random effect (0.21287). Any help would be greatly appreciated.
> If you generally know of a resource for figuring out the syntax to
> extract specific values from summary outputs, that would be great
> because this is a recurring problem for me! By the way, I plan to have
> as many of my students at possible attend the workshop you're offering
> at Chena HS near Fairbanks in August.
> cheers,
> Laura

   This is really more of an r-sig-mixed-models at r-project.org question:
I'm forwarding the answer there.

  methods(class="merMod") is a good way to check the available accessor
methods for (g)lmer fits, as is ?getME (an lme4-specific function for
getting at lower-level components of a (g)lmer fit in a safe/stable way).


>> lmer.sheep <- lmer(y ~ (1|year),family=binomial, data = coy,
> + control=list(msVerbose=F,maxIter=1000,maxFN=3000))
>> lmer.sheep
> Generalized linear mixed model fit by the Laplace approximation
> Formula: y ~ (1 | year)
> Data: coy

> Fixed effects:
> Estimate Std. Error z value Pr(>|z|)
> (Intercept) -0.8083 0.2168 -3.729 0.000192 ***
> ---
> Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>> mn <- lmer.sheep at fixef
>> mn
> (Intercept)
> -0.8083453
>> lmer.sheep at ranef
> [1] 0.09490052 0.09490052 0.03168227 -0.01783168 -0.14832171 -0.04832885

   A reproducible example would be slightly better, but here I believe
you're looking for

  fixef(lmer.sheep)[1]  ## intercept
  sqrt(unlist(VarCorr(lmer.sheep))  ## std dev of year effect

  It looks like you're still using an older version of lme4: I believe
the incantations I gave above are backward compatible, but there are a
fair number of convenience methods in the newer (>1.0) version that
might be helpful (e.g. as.data.frame(VarCorr(lmer.sheep)))

  Ben Bolker


From bbolker at gmail.com  Fri May  2 23:23:06 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 02 May 2014 17:23:06 -0400
Subject: [R-sig-ME] [Lme4-authors] Fixing the level 1 residual variance
In-Reply-To: <CAO7JsnRzwWxOw5HQgswdRx3cJF4At7z17GdWdtQ1RBRsqYtPzQ@mail.gmail.com>
References: <5363AA68.8050700@uni-landau.de>
	<CAO7JsnRzwWxOw5HQgswdRx3cJF4At7z17GdWdtQ1RBRsqYtPzQ@mail.gmail.com>
Message-ID: <53640CBA.4060501@gmail.com>

On 14-05-02 03:43 PM, Douglas Bates wrote:
> On Fri, May 2, 2014 at 9:23 AM, Charlotte Arndt <arndtch at uni-landau.de
> <mailto:arndtch at uni-landau.de>> wrote:
> 
>     Dear Sir / Madam,
>     I am analyzing some data by means of multivariate multilevel models
>     and want to fix the residual variance on level 1 to zero. In HLM,
>     you can do this by fixing the level 1 residual variance (sigma
>     square) to a very small value, e.g. 0.00001. Is it possible to
>     constrain the level 1 residual variance with lme4? It would be
>     great, if I could use lme4 for my analyses. Thanking you in advance.
> 
> 
> I forget how the "levels" are numbered in the multilevel modeling
> literature but as you say "residual variance" I imagine you are
> referring to the variance of the conditional distribution of the
> response given the random effects.  The way the model is defined and fit
> in the lme4 package the covariance matrix of the random effects is
> defined relative to this variance.  In other words it would not be
> possible to fit the model in the way you describe.
> 
> Actually I can't imagine how such a model could make sense.  Where does
> the variability in the conditional distribution get absorbed?


  Actually, this can be done with blme.   See

http://thread.gmane.org/gmane.comp.lang.r.lme4.devel/11500/
http://thread.gmane.org/gmane.comp.lang.r.lme4.devel/11093/


From bates at stat.wisc.edu  Fri May  2 23:45:59 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 2 May 2014 16:45:59 -0500
Subject: [R-sig-ME] [Lme4-authors] Fixing the level 1 residual variance
In-Reply-To: <53640CBA.4060501@gmail.com>
References: <5363AA68.8050700@uni-landau.de>
	<CAO7JsnRzwWxOw5HQgswdRx3cJF4At7z17GdWdtQ1RBRsqYtPzQ@mail.gmail.com>
	<53640CBA.4060501@gmail.com>
Message-ID: <CAO7JsnSYqnXBARy+YpT2C3yP4AsVTwKh+dceue9yxEDhT=MzAw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140502/eafaa266/attachment.pl>

From bbolker at gmail.com  Sat May  3 00:35:20 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 02 May 2014 18:35:20 -0400
Subject: [R-sig-ME] [Lme4-authors] Fixing the level 1 residual variance
In-Reply-To: <CAO7JsnSYqnXBARy+YpT2C3yP4AsVTwKh+dceue9yxEDhT=MzAw@mail.gmail.com>
References: <5363AA68.8050700@uni-landau.de>	<CAO7JsnRzwWxOw5HQgswdRx3cJF4At7z17GdWdtQ1RBRsqYtPzQ@mail.gmail.com>	<53640CBA.4060501@gmail.com>
	<CAO7JsnSYqnXBARy+YpT2C3yP4AsVTwKh+dceue9yxEDhT=MzAw@mail.gmail.com>
Message-ID: <53641DA8.2020009@gmail.com>

On 14-05-02 05:45 PM, Douglas Bates wrote:
> On Fri, May 2, 2014 at 4:23 PM, Ben Bolker <bbolker at gmail.com
> <mailto:bbolker at gmail.com>> wrote:
> 
>     On 14-05-02 03:43 PM, Douglas Bates wrote:
>     > On Fri, May 2, 2014 at 9:23 AM, Charlotte Arndt
>     <arndtch at uni-landau.de <mailto:arndtch at uni-landau.de>
>     > <mailto:arndtch at uni-landau.de <mailto:arndtch at uni-landau.de>>> wrote:
>     >
>     >     Dear Sir / Madam,
>     >     I am analyzing some data by means of multivariate multilevel
>     models
>     >     and want to fix the residual variance on level 1 to zero. In HLM,
>     >     you can do this by fixing the level 1 residual variance (sigma
>     >     square) to a very small value, e.g. 0.00001. Is it possible to
>     >     constrain the level 1 residual variance with lme4? It would be
>     >     great, if I could use lme4 for my analyses. Thanking you in
>     advance.
>     >
>     >
>     > I forget how the "levels" are numbered in the multilevel modeling
>     > literature but as you say "residual variance" I imagine you are
>     > referring to the variance of the conditional distribution of the
>     > response given the random effects.  The way the model is defined
>     and fit
>     > in the lme4 package the covariance matrix of the random effects is
>     > defined relative to this variance.  In other words it would not be
>     > possible to fit the model in the way you describe.
>     >
>     > Actually I can't imagine how such a model could make sense.  Where
>     does
>     > the variability in the conditional distribution get absorbed?
> 
> 
>       Actually, this can be done with blme.   See
> 
> 
> But what does it mean?  What model is being fit? 

   This is essentially the same model that is being fitted when one
computes the likelihood profile for the residual variance; that is, the
scaled deviance is computed on the basis of the theta parameters, then
the deviance is computed on the basis of the specified residual variance.

 code from devfun2:

 thpars <- Sv_to_Cv(pars,n=vlist,s=sigma)
 .Call(lmer_Deviance, pp$ptr(), resp$ptr(), thpars)
 sigsq <- sigma^2
 pp$ldL2() - ldW + (resp$wrss() + pp$sqrL(1))/sigsq +
                    n * log(2 * pi * sigsq)

where ldW  is the sum of the log weights (if any)

  Makes sense to me ...


From christianvanbrauner at gmail.com  Sat May  3 13:59:48 2014
From: christianvanbrauner at gmail.com (Christian Brauner)
Date: Sat, 3 May 2014 13:59:48 +0200
Subject: [R-sig-ME] Why do extremely similar parametric bootstraps on lmer()
 objects yield (not only marginally) different results?
Message-ID: <20140503115947.GA27363@gmail.com>

Hello all,


warning ahead: This will be quite a long post. But have faith, it is necessary.

I run parametric bootstraps in order to compare two models. I bootstrap the
likelihood ratio between the models. For anybody acquainted with this, the code
should be self explanatory. Model selection is more or less done and these are
exercises I like to perform with real data sets to get a feel for how different
softwares treat the same models. Unfortunately, I am not in a position to
provide the real data. A few words about the models though:

Both models are fit on a balanced data set which includes around 483 missing
values. There are slightly above 11196 observations (within: 70 subjects).
Every subject saw every item only one time. The dependent variable are reading
times; sentence_type and matching are crossed two-level factors. Context and
subject are treated as random effects. Context has 40 levels and (well you
guessed) subjects has 70 levels. (See this related post on:
http://stats.stackexchange.com/questions/94302/lmer-parametric-bootstrap-testing-for-fixed-effects):

Model formulae:
===============
        
# I only show the model formulae here because this piece stays fixed with
# every call to lmer(). The options change however, depending on the parametric  
# bootstrap procedure used. Any change can be seen in the code:
mod_min := value ~ passung + (1 + satz_typ | vp) + (1 + satz_typ | kontext)
mod8    := value ~ passung + (1 + satz_typ | vp) + (1 + satz_typ | kontext)

I set up different ways to perform parametric bootstraps. I will describe the
different ways and explain what I changed every time and show the results. The
ultimate question will be: Why the different results? Especially in one of the
ways to perform a parametric bootstrap.

Method 1 (pbkrtest).
====================

1.1. lmer() model calls:
------------------------

mod_min <- lmer(formula = log(value)
                ~ 1
                + (1 + satz_typ | vp)
                + (1 + satz_typ | kontext),
                data = wort12_lmm2_melt,
                REML = FALSE)

mod8 <- lmer(formula = log(value)
             ~ passung
             + (1 + satz_typ | vp)
             + (1 + satz_typ | kontext),
             data = wort12_lmm2_melt,
             REML = FALSE)

1.2. Parametric Bootstrap:
--------------------------

# remove NAs otherwise PBmodcomp() will complain as it calls the simulate()
# function directly without the argument na.action = na.exclude
# Relevant: See Ben Bolkers comments on this:
# https://github.com/lme4/lme4/issues/189 dirty fix:
mod8_nona    <- mod8
mod_min_nona <- mod_min

attr(x     = mod8_nona at frame, 
     which = "na.action") <- NULL # removing na.action attribute from the model

attr(x     = mod_min_nona at frame,
     which = "na.action") <- NULL # removing na.action attribute from the model

library(pbkrtest)

pbmod_mod8_mod_min <- PBmodcomp(largeModel = mod8_nona,
                                smallModel = mod_min_nona,
                                nsim       = 10000,
                                cl         = cl)


Take home message PBmodcomp(): p.value = 0.003

1.3 Notable:
------------
na.action attribute has been removed from both models before simulate() call.


Method 2.
=========

2.1 lmer() model calls:
-----------------------

identical to Method 1 (pbkrtest).

2.2 Parametric Bootstrap:
-------------------------

mod <- mod8
modnull <- mod_min
# save the observed likelihood ratio test statistic
lrt_obs <- anova(modnull,
                 mod)$Chisq[2]

n_sim <- 10000

dattemp <- mod at frame

ptime <- system.time({
    lrt_sim <- foreach (i = 1:n_sim,
                        .combine  = "c",
                        .packages = "lme4") %dopar% {

        ysim       <- unlist(x = simulate(object = modnull))

        modnullsim <- lmer(formula = ysim
                           ~ 1
                           + (1 + satz_typ | vp)
                           + (1 + satz_typ | kontext),
                           data = dattemp,
                           REML = FALSE) # Fit the null model.

        modaltsim  <- lmer(formula = ysim
                           ~ passung
                           + (1 + satz_typ | vp)
                           + (1 + satz_typ | kontext),
                           data = dattemp,
                           REML = FALSE) # Fit the alternative model.

        anova(modnullsim,
              modaltsim)$Chisq[2] # Save the likelihood ratio test statistic.

    }
})[3]

sum(lrt_sim >= lrt_obs) + 1)/(n_sim + 1) = 0.00639936 # p.value

Take home message Method 2: p.value = 0.00639936

2.3 Notable:
------------
na.action has not been altered!


Method 3.
=========
3.1 lmer() model calls:
-----------------------

# differences explained in 3.3
mod_min <- lmer(formula = log(value)
                ~ 1
                + (1 + satz_typ | vp)
                + (1 + satz_typ | kontext),
                data = wort12_lmm2_melt_noa,
                REML = FALSE)

mod8 <- lmer(formula = log(value)
             ~ passung
             + (1 + satz_typ | vp)
             + (1 + satz_typ | kontext),
             data = wort12_lmm2_melt_nona,
             REML = FALSE)

3.2 Parametric Bootstrap:
-------------------------

indentical to Method 2

Take home message Method 3: p.value = 0.00659934


3.3 Notable:
------------

The models were fit on the original dataset which only included complete cases.
That is, on a data set that was altered by the following code:

wort12_lmm2_melt_nona <-
    wort12_lmm2_melt[complete.cases(wort12_lmm2_melt), ]


Method 4.
=========
4.1 lmer() model calls:
-----------------------

# differences explained in 4.3
mod_min_naexclude <- lmer(formula = log(value)
                          ~ 1
                          + (1 + satz_typ | vp)
                          + (1 + satz_typ | kontext),
                          data      = wort12_lmm2_melt,
                          REML      = FALSE,
                          na.action = na.exclude)

mod8_naexclude <- lmer(formula = log(value)
                       ~ passung
                       + (1 + satz_typ | vp)
                       + (1 + satz_typ | kontext),
                       data      = wort12_lmm2_melt,
                       REML      = FALSE,
                       na.action = na.exclude)


4.2 Parametric Bootstrap:
-------------------------
mod <- mod8_naexclude
modnull <- mod_min_naexclude

lrt_obs <- anova(modnull,
                 mod)$Chisq[2] # Save the observed likelihood ratio test statistic.

n_sim <- 10000

ptime <- system.time({
    lrt_sim <- foreach (i = 1:n_sim,
                        .combine   = "c",
                        .packages = "lme4") %dopar% {

        ysim       <- unlist(simulate(object = modnull))

        modnullsim <- refit(object  = modnull,
                            newresp = ysim) # Fit the null-model.

        modaltsim  <- refit(object  = mod,
                            newresp = ysim) # Fit the alternative model.

        anova(modnullsim,
              modaltsim)$Chisq[2] # Save the likelihood ratio test statistic.

    }
})[3]

Take home message Method 4: p.value = 0.00429957


4.3 Notable:
------------
The models were fit with the option na.action = na.exclude.


Method 5.
=========

5.1 lmer() model calls:
-----------------------

# differences explained in 5.3
mod_min_naexclude <- lmer(formula = log(value)
                          ~ 1
                          + (1 + satz_typ | vp)
                          + (1 + satz_typ | kontext),
                          data      = wort12_lmm2_melt_nona,
                          REML      = FALSE)

mod8_naexclude <- lmer(formula = log(value)
                       ~ passung
                       + (1 + satz_typ | vp)
                       + (1 + satz_typ | kontext),
                       data      = wort12_lmm2_melt_nona,
                       REML      = FALSE)


5.2 Parametric Bootstrap:
-------------------------

indentical to Method 4

Take home message Method 5: p.value = 0.0039996

5.3 Notable:
------------
The models were fit on the original dataset which only included complete cases.
That is, on a data set that was altered by the following code:

wort12_lmm2_melt_nona <-
    wort12_lmm2_melt[complete.cases(wort12_lmm2_melt), ]


Question:
=========

Why do Method 2 and Method 3 yield such (in my opinion) p.values that deviate
by about 0.002 from all other methods?

(I think at least the explanation of why I do not see it is really simple: I
just have been looking to long at this code and I am missing the obvious.)

Two final remarks:

(1) I ran everything on a cluster. That?s why I repeated Methods 2 and Methods
3. The results they yield seem to be numerically stable.

(2) I plotted the likelihood ratio test statistics for all methods and compared
them to a chi-squared distribution with df=1, and their likelihood ratio test
density. Everything is normal. The chi-square assumption is correct for these
models

Thank you so much for your patience!

Best,
Christian Brauner


From bbolker at gmail.com  Sat May  3 20:14:30 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 03 May 2014 14:14:30 -0400
Subject: [R-sig-ME] Why do extremely similar parametric bootstraps on
 lmer() objects yield (not only marginally) different results?
In-Reply-To: <20140503115947.GA27363@gmail.com>
References: <20140503115947.GA27363@gmail.com>
Message-ID: <53653206.7070800@gmail.com>

On 14-05-03 07:59 AM, Christian Brauner wrote:
> Hello all,
> 

> warning ahead: This will be quite a long post. But have faith, it is
>  necessary.

  I have some questions about the larger context here.  I agree that
it's important to understand the behaviour of the code, and to ferret
out possible bugs in the code (I will say that handling all of the
permutations of NA handling in the predict and simulate code is a big
headache ...)  However, I'm wondering about a few things:

 * is the difference between p=0.004 and p=0.006 _practically_
important?
 * for this example, it seems that (as you comment) almost any approach
will give reasonable answers -- simple LRT (because the number of levels
of the minimum-size grouping variable is >=40), Kenward-Roger (because
this is a LMM).  Parametric bootstrap feels like a waste of computation
time.
* I can't do it off the top of my head, but for a balanced/fully crossed
random effect, one might even be able to compute the denominator df
for the F distribution exactly.
* is there a reason you're not using refit() in method 2?  It should
be faster.
* does PBmodcomp() work with the latest version of lme4, with the
simulate/NA fix?  Or do we or the pbkrtest maintainers still have work
to do?
* I'm a little bit worried by method 2 -- I can imagine (although haven't
tested) that ysim may be a different length/may not match mod at frame,
which will be NA-processed. However, I think it should work OK
if the default na.action is 'na.omit' (I'd prefer that people _never_ used @
in their code ...)


> I run parametric bootstraps in order to compare two models. I
> bootstrap the likelihood ratio between the models. For anybody
> acquainted with this, the code should be self explanatory. Model
> selection is more or less done and these are exercises I like to
> perform with real data sets to get a feel for how different
> softwares treat the same models. Unfortunately, I am not in a
> position to provide the real data. A few words about the models
> though:

> Both models are fit on a balanced data set which includes around 483 missing
> values. There are slightly above 11196 observations (within: 70 subjects).
> Every subject saw every item only one time. The dependent variable are reading
> times; sentence_type and matching are crossed two-level factors. Context and
> subject are treated as random effects. Context has 40 levels and (well you
> guessed) subjects has 70 levels. (See this related post on:
> http://stats.stackexchange.com/questions/94302/lmer-parametric-bootstrap-testing-for-fixed-effects):
> 
> Model formulae:
> ===============
>         
> # I only show the model formulae here because this piece stays fixed with
> # every call to lmer(). The options change however, depending on the parametric  
> # bootstrap procedure used. Any change can be seen in the code:
> mod_min := value ~ passung + (1 + satz_typ | vp) + (1 + satz_typ | kontext)
> mod8    := value ~ passung + (1 + satz_typ | vp) + (1 + satz_typ | kontext)
> 
> I set up different ways to perform parametric bootstraps. I will describe the
> different ways and explain what I changed every time and show the results. The
> ultimate question will be: Why the different results? Especially in one of the
> ways to perform a parametric bootstrap.

  Making my own summary:

1. pbkrtest, na.action attribute removed. p=0.003
2. lmer refitting: p=0.0064
3. fitted on complete cases: p=0.0066
4. using na.exclude and refit: p = 0.0043
5. using complete cases and refit: p = 0.004

So I guess to boil it down, the difference is in the way that refit
handles missing values / accesses the model frame.  If I were going
to pursue this farther, I would probably try to strip it down and look
at the results of a _single_ refit with the random number seed set to
be identical in every case ...


> 
> Method 1 (pbkrtest).
> ====================
> 
> 1.1. lmer() model calls:
> ------------------------
> 
> mod_min <- lmer(formula = log(value)
>                 ~ 1
>                 + (1 + satz_typ | vp)
>                 + (1 + satz_typ | kontext),
>                 data = wort12_lmm2_melt,
>                 REML = FALSE)
> 
> mod8 <- lmer(formula = log(value)
>              ~ passung
>              + (1 + satz_typ | vp)
>              + (1 + satz_typ | kontext),
>              data = wort12_lmm2_melt,
>              REML = FALSE)
> 
> 1.2. Parametric Bootstrap:
> --------------------------
> 
> # remove NAs otherwise PBmodcomp() will complain as it calls the simulate()
> # function directly without the argument na.action = na.exclude
> # Relevant: See Ben Bolkers comments on this:
> # https://github.com/lme4/lme4/issues/189 dirty fix:
> mod8_nona    <- mod8
> mod_min_nona <- mod_min
> 
> attr(x     = mod8_nona at frame, 
>      which = "na.action") <- NULL # removing na.action attribute from the model
> 
> attr(x     = mod_min_nona at frame,
>      which = "na.action") <- NULL # removing na.action attribute from the model
> 
> library(pbkrtest)
> 
> pbmod_mod8_mod_min <- PBmodcomp(largeModel = mod8_nona,
>                                 smallModel = mod_min_nona,
>                                 nsim       = 10000,
>                                 cl         = cl)
> 
> 
> Take home message PBmodcomp(): p.value = 0.003
> 
> 1.3 Notable:
> ------------
> na.action attribute has been removed from both models before simulate() call.
> 
> 
> Method 2.
> =========
> 
> 2.1 lmer() model calls:
> -----------------------
> 
> identical to Method 1 (pbkrtest).
> 
> 2.2 Parametric Bootstrap:
> -------------------------
> 
> mod <- mod8
> modnull <- mod_min
> # save the observed likelihood ratio test statistic
> lrt_obs <- anova(modnull,
>                  mod)$Chisq[2]
> 
> n_sim <- 10000
> 
> dattemp <- mod at frame
> 
> ptime <- system.time({
>     lrt_sim <- foreach (i = 1:n_sim,
>                         .combine  = "c",
>                         .packages = "lme4") %dopar% {
> 
>         ysim       <- unlist(x = simulate(object = modnull))
> 
>         modnullsim <- lmer(formula = ysim
>                            ~ 1
>                            + (1 + satz_typ | vp)
>                            + (1 + satz_typ | kontext),
>                            data = dattemp,
>                            REML = FALSE) # Fit the null model.
> 
>         modaltsim  <- lmer(formula = ysim
>                            ~ passung
>                            + (1 + satz_typ | vp)
>                            + (1 + satz_typ | kontext),
>                            data = dattemp,
>                            REML = FALSE) # Fit the alternative model.
> 
>         anova(modnullsim,
>               modaltsim)$Chisq[2] # Save the likelihood ratio test statistic.
> 
>     }
> })[3]
> 
> sum(lrt_sim >= lrt_obs) + 1)/(n_sim + 1) = 0.00639936 # p.value
> 
> Take home message Method 2: p.value = 0.00639936
> 
> 2.3 Notable:
> ------------
> na.action has not been altered!
> 
> 
> Method 3.
> =========
> 3.1 lmer() model calls:
> -----------------------
> 
> # differences explained in 3.3
> mod_min <- lmer(formula = log(value)
>                 ~ 1
>                 + (1 + satz_typ | vp)
>                 + (1 + satz_typ | kontext),
>                 data = wort12_lmm2_melt_noa,
>                 REML = FALSE)
> 
> mod8 <- lmer(formula = log(value)
>              ~ passung
>              + (1 + satz_typ | vp)
>              + (1 + satz_typ | kontext),
>              data = wort12_lmm2_melt_nona,
>              REML = FALSE)
> 
> 3.2 Parametric Bootstrap:
> -------------------------
> 
> indentical to Method 2
> 
> Take home message Method 3: p.value = 0.00659934
> 
> 
> 3.3 Notable:
> ------------
> 
> The models were fit on the original dataset which only included complete cases.
> That is, on a data set that was altered by the following code:
> 
> wort12_lmm2_melt_nona <-
>     wort12_lmm2_melt[complete.cases(wort12_lmm2_melt), ]
> 
> 
> Method 4.
> =========
> 4.1 lmer() model calls:
> -----------------------
> 
> # differences explained in 4.3
> mod_min_naexclude <- lmer(formula = log(value)
>                           ~ 1
>                           + (1 + satz_typ | vp)
>                           + (1 + satz_typ | kontext),
>                           data      = wort12_lmm2_melt,
>                           REML      = FALSE,
>                           na.action = na.exclude)
> 
> mod8_naexclude <- lmer(formula = log(value)
>                        ~ passung
>                        + (1 + satz_typ | vp)
>                        + (1 + satz_typ | kontext),
>                        data      = wort12_lmm2_melt,
>                        REML      = FALSE,
>                        na.action = na.exclude)
> 
> 
> 4.2 Parametric Bootstrap:
> -------------------------
> mod <- mod8_naexclude
> modnull <- mod_min_naexclude
> 
> lrt_obs <- anova(modnull,
>                  mod)$Chisq[2] # Save the observed likelihood ratio test statistic.
> 
> n_sim <- 10000
> 
> ptime <- system.time({
>     lrt_sim <- foreach (i = 1:n_sim,
>                         .combine   = "c",
>                         .packages = "lme4") %dopar% {
> 
>         ysim       <- unlist(simulate(object = modnull))
> 
>         modnullsim <- refit(object  = modnull,
>                             newresp = ysim) # Fit the null-model.
> 
>         modaltsim  <- refit(object  = mod,
>                             newresp = ysim) # Fit the alternative model.
> 
>         anova(modnullsim,
>               modaltsim)$Chisq[2] # Save the likelihood ratio test statistic.
> 
>     }
> })[3]
> 
> Take home message Method 4: p.value = 0.00429957
> 
> 
> 4.3 Notable:
> ------------
> The models were fit with the option na.action = na.exclude.
> 
> 
> Method 5.
> =========
> 
> 5.1 lmer() model calls:
> -----------------------
> 
> # differences explained in 5.3
> mod_min_naexclude <- lmer(formula = log(value)
>                           ~ 1
>                           + (1 + satz_typ | vp)
>                           + (1 + satz_typ | kontext),
>                           data      = wort12_lmm2_melt_nona,
>                           REML      = FALSE)
> 
> mod8_naexclude <- lmer(formula = log(value)
>                        ~ passung
>                        + (1 + satz_typ | vp)
>                        + (1 + satz_typ | kontext),
>                        data      = wort12_lmm2_melt_nona,
>                        REML      = FALSE)
> 
> 
> 5.2 Parametric Bootstrap:
> -------------------------
> 
> indentical to Method 4
> 
> Take home message Method 5: p.value = 0.0039996
> 
> 5.3 Notable:
> ------------
> The models were fit on the original dataset which only included complete cases.
> That is, on a data set that was altered by the following code:
> 
> wort12_lmm2_melt_nona <-
>     wort12_lmm2_melt[complete.cases(wort12_lmm2_melt), ]
> 
> 
> Question:
> =========
> 
> Why do Method 2 and Method 3 yield such (in my opinion) p.values that deviate
> by about 0.002 from all other methods?
> 
> (I think at least the explanation of why I do not see it is really simple: I
> just have been looking to long at this code and I am missing the obvious.)
> 
> Two final remarks:
> 
> (1) I ran everything on a cluster. That?s why I repeated Methods 2 and Methods
> 3. The results they yield seem to be numerically stable.
> 
> (2) I plotted the likelihood ratio test statistics for all methods and compared
> them to a chi-squared distribution with df=1, and their likelihood ratio test
> density. Everything is normal. The chi-square assumption is correct for these
> models
> 
> Thank you so much for your patience!
> 
> Best,
> Christian Brauner
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From christianvanbrauner at gmail.com  Sat May  3 21:38:30 2014
From: christianvanbrauner at gmail.com (Christian Brauner)
Date: Sat, 3 May 2014 21:38:30 +0200
Subject: [R-sig-ME] Why do extremely similar parametric bootstraps on
 lmer() objects yield (not only marginally) different results?
In-Reply-To: <53653206.7070800@gmail.com>
References: <20140503115947.GA27363@gmail.com>
 <53653206.7070800@gmail.com>
Message-ID: <20140503193830.GA2677@gmail.com>



>  * is the difference between p=0.004 and p=0.006 _practically_
> important?
No, I don?t think that it is important at all. Mostly because I don?t rely on p
values. (But a lot of people still like to see them.) In this special case it
is not important because we are looking at a p that is significantly smaller
than the specified alpha level; in this case the alpha level is the abundant
0.05. Still, the results differ at least by 0.002. This phenomenon seems stable
across different ways to treat NAs (as I?ve tried to show). I think this
suggests that one of the methods has a systematical bias. But I don?t know
which method. Hence, I don?t know if it is an anticonservative or a
conservative bias. It was pure curiosity which of the two methods it is. Any
hints on how to find out are welcome. (I saw you suggested looking at a single
refit while setting seed.)

>  * for this example, it seems that (as you comment) almost any approach
> will give reasonable answers -- simple LRT (because the number of levels
> of the minimum-size grouping variable is >=40), Kenward-Roger (because
> this is a LMM).  Parametric bootstrap feels like a waste of computation
> time.
Agreed. But the computation time didn?t matter much to me since all these
things are run on a grid at 5 in the morning and finish in 30 min. Usually
nobody is doing anyhting on it at that time.

> * I can't do it off the top of my head, but for a balanced/fully crossed
> random effect, one might even be able to compute the denominator df
> for the F distribution exactly.
I actually did not know that. If you have any papers, code or explanatio that
you could share with me I would greatly appreciate it!

> * is there a reason you're not using refit() in method 2?  It should
> be faster.
I am usually only comfortable with calling in functions which I had the time to
understand (reasonably) properly (e.g. by looking at its code.) and of which I
know how they're doing what they're doing. In this manner I can make sure that
I can stand for the things I calculated. I am not that acquainted with refit()
so I was up until now relying on doing stuff by hand. Furthermore, I like to do
stuff by hand first. This is just to check if I know what I am doing.

> * does PBmodcomp() work with the latest version of lme4, with the
> simulate/NA fix?  Or do we or the pbkrtest maintainers still have work
> to do?
I just did a test with pbkrtest_0.3-8 and the github version of lme4_1.1-7. If
I specify my models:

mod8 <- lmer(formula = log(value)
             ~ passung
             + (1 + satz_typ | vp)
             + (1 + satz_typ | kontext),
             data = wort12_lmm2_melt,
             REML = FALSE)

mod_min <- lmer(formula = log(value)
                ~ 1
                + (1 + satz_typ | vp)
                + (1 + satz_typ | kontext),
                data = wort12_lmm2_melt,
                REML = FALSE)

and call:

pbmod_mod8_mod_min <- PBmodcomp(largeModel = mod8_nona,
                                smallModel = mod_min_nona,
                                nsim       = 100,
                                cl         = cl)

without removing the na.action attribute, I get:

> pbmod_mod8_mod_min <- PBmodcomp(largeModel = mod8_nona,
+                                 smallModel = mod_min_nona,
+                                 nsim       = 100,
+                                 cl         = cl)
Error in checkForRemoteErrors(lapply(cl, recvResult)) : 
  one node produced an error: replacement has 10263 rows, data has 10713

> * I'm a little bit worried by method 2 -- I can imagine (although haven't
> tested) that ysim may be a different length/may not match mod at frame,
> which will be NA-processed. However, I think it should work OK
> if the default na.action is 'na.omit' (I'd prefer that people _never_ used @
> in their code ...)
That actually was a problem. If I fit the models with lmer(..., na.action =
na.exclude) but leave the rest the way it is. I will get a model.frame.default
error message which has to do with differing lengths.  That?s why I redid
Method 2 with Method 3 were I constructed a data frame with complete cases.

Thank you so much for taking a look at this!
Christian

Eberhard Karls Universit?t T?bingen
Mathematisch-Naturwissenschaftliche Fakult?t - Faculty of Science
Evolutionary Cognition - Cognitive Science
Schleichstra?e 4 ? 72076 T?bingen ? Germany
Telefon +49 7071 29-75643 ? Telefax +49 7071 29-4721
christian.brauner at uni-tuebingen.de


On Sat, May 03, 2014 at 02:14:30PM -0400, Ben Bolker wrote:
> On 14-05-03 07:59 AM, Christian Brauner wrote:
> > Hello all,
> > 
> 
> > warning ahead: This will be quite a long post. But have faith, it is
> >  necessary.
> 
>   I have some questions about the larger context here.  I agree that
> it's important to understand the behaviour of the code, and to ferret
> out possible bugs in the code (I will say that handling all of the
> permutations of NA handling in the predict and simulate code is a big
> headache ...)  However, I'm wondering about a few things:
> 
>  * is the difference between p=0.004 and p=0.006 _practically_
> important?
>  * for this example, it seems that (as you comment) almost any approach
> will give reasonable answers -- simple LRT (because the number of levels
> of the minimum-size grouping variable is >=40), Kenward-Roger (because
> this is a LMM).  Parametric bootstrap feels like a waste of computation
> time.
> * I can't do it off the top of my head, but for a balanced/fully crossed
> random effect, one might even be able to compute the denominator df
> for the F distribution exactly.
> * is there a reason you're not using refit() in method 2?  It should
> be faster.
> * does PBmodcomp() work with the latest version of lme4, with the
> simulate/NA fix?  Or do we or the pbkrtest maintainers still have work
> to do?
> * I'm a little bit worried by method 2 -- I can imagine (although haven't
> tested) that ysim may be a different length/may not match mod at frame,
> which will be NA-processed. However, I think it should work OK
> if the default na.action is 'na.omit' (I'd prefer that people _never_ used @
> in their code ...)
> 
> 
> > I run parametric bootstraps in order to compare two models. I
> > bootstrap the likelihood ratio between the models. For anybody
> > acquainted with this, the code should be self explanatory. Model
> > selection is more or less done and these are exercises I like to
> > perform with real data sets to get a feel for how different
> > softwares treat the same models. Unfortunately, I am not in a
> > position to provide the real data. A few words about the models
> > though:
> 
> > Both models are fit on a balanced data set which includes around 483 missing
> > values. There are slightly above 11196 observations (within: 70 subjects).
> > Every subject saw every item only one time. The dependent variable are reading
> > times; sentence_type and matching are crossed two-level factors. Context and
> > subject are treated as random effects. Context has 40 levels and (well you
> > guessed) subjects has 70 levels. (See this related post on:
> > http://stats.stackexchange.com/questions/94302/lmer-parametric-bootstrap-testing-for-fixed-effects):
> > 
> > Model formulae:
> > ===============
> >         
> > # I only show the model formulae here because this piece stays fixed with
> > # every call to lmer(). The options change however, depending on the parametric  
> > # bootstrap procedure used. Any change can be seen in the code:
> > mod_min := value ~ passung + (1 + satz_typ | vp) + (1 + satz_typ | kontext)
> > mod8    := value ~ passung + (1 + satz_typ | vp) + (1 + satz_typ | kontext)
> > 
> > I set up different ways to perform parametric bootstraps. I will describe the
> > different ways and explain what I changed every time and show the results. The
> > ultimate question will be: Why the different results? Especially in one of the
> > ways to perform a parametric bootstrap.
> 
>   Making my own summary:
> 
> 1. pbkrtest, na.action attribute removed. p=0.003
> 2. lmer refitting: p=0.0064
> 3. fitted on complete cases: p=0.0066
> 4. using na.exclude and refit: p = 0.0043
> 5. using complete cases and refit: p = 0.004
> 
> So I guess to boil it down, the difference is in the way that refit
> handles missing values / accesses the model frame.  If I were going
> to pursue this farther, I would probably try to strip it down and look
> at the results of a _single_ refit with the random number seed set to
> be identical in every case ...
> 
> 
> > 
> > Method 1 (pbkrtest).
> > ====================
> > 
> > 1.1. lmer() model calls:
> > ------------------------
> > 
> > mod_min <- lmer(formula = log(value)
> >                 ~ 1
> >                 + (1 + satz_typ | vp)
> >                 + (1 + satz_typ | kontext),
> >                 data = wort12_lmm2_melt,
> >                 REML = FALSE)
> > 
> > mod8 <- lmer(formula = log(value)
> >              ~ passung
> >              + (1 + satz_typ | vp)
> >              + (1 + satz_typ | kontext),
> >              data = wort12_lmm2_melt,
> >              REML = FALSE)
> > 
> > 1.2. Parametric Bootstrap:
> > --------------------------
> > 
> > # remove NAs otherwise PBmodcomp() will complain as it calls the simulate()
> > # function directly without the argument na.action = na.exclude
> > # Relevant: See Ben Bolkers comments on this:
> > # https://github.com/lme4/lme4/issues/189 dirty fix:
> > mod8_nona    <- mod8
> > mod_min_nona <- mod_min
> > 
> > attr(x     = mod8_nona at frame, 
> >      which = "na.action") <- NULL # removing na.action attribute from the model
> > 
> > attr(x     = mod_min_nona at frame,
> >      which = "na.action") <- NULL # removing na.action attribute from the model
> > 
> > library(pbkrtest)
> > 
> > pbmod_mod8_mod_min <- PBmodcomp(largeModel = mod8_nona,
> >                                 smallModel = mod_min_nona,
> >                                 nsim       = 10000,
> >                                 cl         = cl)
> > 
> > 
> > Take home message PBmodcomp(): p.value = 0.003
> > 
> > 1.3 Notable:
> > ------------
> > na.action attribute has been removed from both models before simulate() call.
> > 
> > 
> > Method 2.
> > =========
> > 
> > 2.1 lmer() model calls:
> > -----------------------
> > 
> > identical to Method 1 (pbkrtest).
> > 
> > 2.2 Parametric Bootstrap:
> > -------------------------
> > 
> > mod <- mod8
> > modnull <- mod_min
> > # save the observed likelihood ratio test statistic
> > lrt_obs <- anova(modnull,
> >                  mod)$Chisq[2]
> > 
> > n_sim <- 10000
> > 
> > dattemp <- mod at frame
> > 
> > ptime <- system.time({
> >     lrt_sim <- foreach (i = 1:n_sim,
> >                         .combine  = "c",
> >                         .packages = "lme4") %dopar% {
> > 
> >         ysim       <- unlist(x = simulate(object = modnull))
> > 
> >         modnullsim <- lmer(formula = ysim
> >                            ~ 1
> >                            + (1 + satz_typ | vp)
> >                            + (1 + satz_typ | kontext),
> >                            data = dattemp,
> >                            REML = FALSE) # Fit the null model.
> > 
> >         modaltsim  <- lmer(formula = ysim
> >                            ~ passung
> >                            + (1 + satz_typ | vp)
> >                            + (1 + satz_typ | kontext),
> >                            data = dattemp,
> >                            REML = FALSE) # Fit the alternative model.
> > 
> >         anova(modnullsim,
> >               modaltsim)$Chisq[2] # Save the likelihood ratio test statistic.
> > 
> >     }
> > })[3]
> > 
> > sum(lrt_sim >= lrt_obs) + 1)/(n_sim + 1) = 0.00639936 # p.value
> > 
> > Take home message Method 2: p.value = 0.00639936
> > 
> > 2.3 Notable:
> > ------------
> > na.action has not been altered!
> > 
> > 
> > Method 3.
> > =========
> > 3.1 lmer() model calls:
> > -----------------------
> > 
> > # differences explained in 3.3
> > mod_min <- lmer(formula = log(value)
> >                 ~ 1
> >                 + (1 + satz_typ | vp)
> >                 + (1 + satz_typ | kontext),
> >                 data = wort12_lmm2_melt_noa,
> >                 REML = FALSE)
> > 
> > mod8 <- lmer(formula = log(value)
> >              ~ passung
> >              + (1 + satz_typ | vp)
> >              + (1 + satz_typ | kontext),
> >              data = wort12_lmm2_melt_nona,
> >              REML = FALSE)
> > 
> > 3.2 Parametric Bootstrap:
> > -------------------------
> > 
> > indentical to Method 2
> > 
> > Take home message Method 3: p.value = 0.00659934
> > 
> > 
> > 3.3 Notable:
> > ------------
> > 
> > The models were fit on the original dataset which only included complete cases.
> > That is, on a data set that was altered by the following code:
> > 
> > wort12_lmm2_melt_nona <-
> >     wort12_lmm2_melt[complete.cases(wort12_lmm2_melt), ]
> > 
> > 
> > Method 4.
> > =========
> > 4.1 lmer() model calls:
> > -----------------------
> > 
> > # differences explained in 4.3
> > mod_min_naexclude <- lmer(formula = log(value)
> >                           ~ 1
> >                           + (1 + satz_typ | vp)
> >                           + (1 + satz_typ | kontext),
> >                           data      = wort12_lmm2_melt,
> >                           REML      = FALSE,
> >                           na.action = na.exclude)
> > 
> > mod8_naexclude <- lmer(formula = log(value)
> >                        ~ passung
> >                        + (1 + satz_typ | vp)
> >                        + (1 + satz_typ | kontext),
> >                        data      = wort12_lmm2_melt,
> >                        REML      = FALSE,
> >                        na.action = na.exclude)
> > 
> > 
> > 4.2 Parametric Bootstrap:
> > -------------------------
> > mod <- mod8_naexclude
> > modnull <- mod_min_naexclude
> > 
> > lrt_obs <- anova(modnull,
> >                  mod)$Chisq[2] # Save the observed likelihood ratio test statistic.
> > 
> > n_sim <- 10000
> > 
> > ptime <- system.time({
> >     lrt_sim <- foreach (i = 1:n_sim,
> >                         .combine   = "c",
> >                         .packages = "lme4") %dopar% {
> > 
> >         ysim       <- unlist(simulate(object = modnull))
> > 
> >         modnullsim <- refit(object  = modnull,
> >                             newresp = ysim) # Fit the null-model.
> > 
> >         modaltsim  <- refit(object  = mod,
> >                             newresp = ysim) # Fit the alternative model.
> > 
> >         anova(modnullsim,
> >               modaltsim)$Chisq[2] # Save the likelihood ratio test statistic.
> > 
> >     }
> > })[3]
> > 
> > Take home message Method 4: p.value = 0.00429957
> > 
> > 
> > 4.3 Notable:
> > ------------
> > The models were fit with the option na.action = na.exclude.
> > 
> > 
> > Method 5.
> > =========
> > 
> > 5.1 lmer() model calls:
> > -----------------------
> > 
> > # differences explained in 5.3
> > mod_min_naexclude <- lmer(formula = log(value)
> >                           ~ 1
> >                           + (1 + satz_typ | vp)
> >                           + (1 + satz_typ | kontext),
> >                           data      = wort12_lmm2_melt_nona,
> >                           REML      = FALSE)
> > 
> > mod8_naexclude <- lmer(formula = log(value)
> >                        ~ passung
> >                        + (1 + satz_typ | vp)
> >                        + (1 + satz_typ | kontext),
> >                        data      = wort12_lmm2_melt_nona,
> >                        REML      = FALSE)
> > 
> > 
> > 5.2 Parametric Bootstrap:
> > -------------------------
> > 
> > indentical to Method 4
> > 
> > Take home message Method 5: p.value = 0.0039996
> > 
> > 5.3 Notable:
> > ------------
> > The models were fit on the original dataset which only included complete cases.
> > That is, on a data set that was altered by the following code:
> > 
> > wort12_lmm2_melt_nona <-
> >     wort12_lmm2_melt[complete.cases(wort12_lmm2_melt), ]
> > 
> > 
> > Question:
> > =========
> > 
> > Why do Method 2 and Method 3 yield such (in my opinion) p.values that deviate
> > by about 0.002 from all other methods?
> > 
> > (I think at least the explanation of why I do not see it is really simple: I
> > just have been looking to long at this code and I am missing the obvious.)
> > 
> > Two final remarks:
> > 
> > (1) I ran everything on a cluster. That?s why I repeated Methods 2 and Methods
> > 3. The results they yield seem to be numerically stable.
> > 
> > (2) I plotted the likelihood ratio test statistics for all methods and compared
> > them to a chi-squared distribution with df=1, and their likelihood ratio test
> > density. Everything is normal. The chi-square assumption is correct for these
> > models
> > 
> > Thank you so much for your patience!
> > 
> > Best,
> > Christian Brauner
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > 
>


From devin.kearns at gmail.com  Sat May  3 22:18:53 2014
From: devin.kearns at gmail.com (Devin Kearns)
Date: Sat, 3 May 2014 16:18:53 -0400
Subject: [R-sig-ME] BOBYQA warning message
Message-ID: <000001cf670c$e8271e60$b8755b20$@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140503/52af7122/attachment.pl>

From stcmwilliams at gmail.com  Sun May  4 00:20:16 2014
From: stcmwilliams at gmail.com (Stacey Williams)
Date: Sat, 3 May 2014 16:20:16 -0600
Subject: [R-sig-ME] p values for random effects in an unbalanced nested
	mixed model
Message-ID: <CA+HozpvvA9U7hdo2V0Rhj42W6sOrakq4sxpz+RJ4ctiSF96Vxw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140503/71a2be37/attachment.pl>

From hughes.dupond at gmx.de  Sun May  4 10:44:14 2014
From: hughes.dupond at gmx.de (Lionel)
Date: Sun, 04 May 2014 10:44:14 +0200
Subject: [R-sig-ME] p values for random effects in an unbalanced nested
 mixed model
In-Reply-To: <CA+HozpvvA9U7hdo2V0Rhj42W6sOrakq4sxpz+RJ4ctiSF96Vxw@mail.gmail.com>
References: <CA+HozpvvA9U7hdo2V0Rhj42W6sOrakq4sxpz+RJ4ctiSF96Vxw@mail.gmail.com>
Message-ID: <5365FDDE.1090503@gmx.de>

Dear Stacey,

The classical null hypothesis of a question like: "is the variation in 
richness significant at each spatial scale. " is that the variation (or 
standard deviation) equal 0. You can have a look at the Box3 in Bolker 
et al (2009) Trends in Ecology and Evolution, there it is explained that 
such a null hypothesis make little sense since you cannot have 0 
standard deviation, then this test will be too conservative (high Type 
II error).

You could look at ?pvalues after having loaded the lme4 package, there 
are various options you could use, the easiest being a likelihood ratio 
test using anova(model1,model2), model1 having a random term like: 
random=~1|Ecoregion/Area/Site  and model2=~1|Ecoregion/Area, the p-value 
you will get correspond to the null hypothesis: The likelihood ratio is 
equal to one, in other words there is no difference in likelihood 
between the two models. In this context significant difference in 
likelihoods can be interpreted as one model fitting better the data due 
to its particular random structure.

Yours,
Lionel


On 04/05/2014 00:20, Stacey Williams wrote:
> Hi List-serv community,
> I would like to calculate the p values for the random effects in an
> unbalanced nested mixed model (lme or lmer). There are three factor levels
> (all random), ecoregions, area nested in ecoregion, and sites nested in
> area and ecoregion.
>
> My model looks like something like this
> coral_richness<-lme(Richness~1, random=~1|Ecoregion/Area/Site)
>
> My goals are to examine 1) how much richness varies at different spatial
> scales (ecoregions, areas, sites) and 2) is the variation in richness
> significant at each spatial scale.
>
> Any help would be greatly appreciated!!!!!
>
> Thanks,
> Stacey
>


From christianvanbrauner at gmail.com  Sun May  4 14:46:05 2014
From: christianvanbrauner at gmail.com (Christian Brauner)
Date: Sun, 4 May 2014 14:46:05 +0200
Subject: [R-sig-ME] Why do extremely similar parametric bootstraps on
 lmer() objects yield (not only marginally) different results?
In-Reply-To: <20140503193830.GA2677@gmail.com>
References: <20140503115947.GA27363@gmail.com> <53653206.7070800@gmail.com>
	<20140503193830.GA2677@gmail.com>
Message-ID: <20140504124604.GA5443@gmail.com>

I ran some more tests and I can safely say that the culprit in Method 2
and Method 3 is the call to:

dattemp <- mod at frame

and subsequently using it as the data argument in the foreach() loop calls
to lmer().

If I use the original data frame in Method 2 and Method 3 and set
na.action = na.exclude when fitting the model the (significant) difference
to using refit() disappears. It seems that copying only the data frame of
an S4 object and using this data frame when fitting/refitting a new model
with lmer() causes some sort of "behaviour" (I refrain from calling it
trouble. I do not fully grasp what is happing there.).

Best,
Christian
Eberhard Karls Universit?t T?bingen
Mathematisch-Naturwissenschaftliche Fakult?t - Faculty of Science
Evolutionary Cognition - Cognitive Science
Schleichstra?e 4 ? 72076 T?bingen ? Germany
Telefon +49 7071 29-75643 ? Telefax +49 7071 29-4721
christian.brauner at uni-tuebingen.de


On Sat, May 03, 2014 at 09:38:30PM +0200, Christian Brauner wrote:
> 
> 
> >  * is the difference between p=0.004 and p=0.006 _practically_
> > important?
> No, I don?t think that it is important at all. Mostly because I don?t rely on p
> values. (But a lot of people still like to see them.) In this special case it
> is not important because we are looking at a p that is significantly smaller
> than the specified alpha level; in this case the alpha level is the abundant
> 0.05. Still, the results differ at least by 0.002. This phenomenon seems stable
> across different ways to treat NAs (as I?ve tried to show). I think this
> suggests that one of the methods has a systematical bias. But I don?t know
> which method. Hence, I don?t know if it is an anticonservative or a
> conservative bias. It was pure curiosity which of the two methods it is. Any
> hints on how to find out are welcome. (I saw you suggested looking at a single
> refit while setting seed.)
> 
> >  * for this example, it seems that (as you comment) almost any approach
> > will give reasonable answers -- simple LRT (because the number of levels
> > of the minimum-size grouping variable is >=40), Kenward-Roger (because
> > this is a LMM).  Parametric bootstrap feels like a waste of computation
> > time.
> Agreed. But the computation time didn?t matter much to me since all these
> things are run on a grid at 5 in the morning and finish in 30 min. Usually
> nobody is doing anyhting on it at that time.
> 
> > * I can't do it off the top of my head, but for a balanced/fully crossed
> > random effect, one might even be able to compute the denominator df
> > for the F distribution exactly.
> I actually did not know that. If you have any papers, code or explanatio that
> you could share with me I would greatly appreciate it!
> 
> > * is there a reason you're not using refit() in method 2?  It should
> > be faster.
> I am usually only comfortable with calling in functions which I had the time to
> understand (reasonably) properly (e.g. by looking at its code.) and of which I
> know how they're doing what they're doing. In this manner I can make sure that
> I can stand for the things I calculated. I am not that acquainted with refit()
> so I was up until now relying on doing stuff by hand. Furthermore, I like to do
> stuff by hand first. This is just to check if I know what I am doing.
> 
> > * does PBmodcomp() work with the latest version of lme4, with the
> > simulate/NA fix?  Or do we or the pbkrtest maintainers still have work
> > to do?
> I just did a test with pbkrtest_0.3-8 and the github version of lme4_1.1-7. If
> I specify my models:
> 
> mod8 <- lmer(formula = log(value)
>              ~ passung
>              + (1 + satz_typ | vp)
>              + (1 + satz_typ | kontext),
>              data = wort12_lmm2_melt,
>              REML = FALSE)
> 
> mod_min <- lmer(formula = log(value)
>                 ~ 1
>                 + (1 + satz_typ | vp)
>                 + (1 + satz_typ | kontext),
>                 data = wort12_lmm2_melt,
>                 REML = FALSE)
> 
> and call:
> 
> pbmod_mod8_mod_min <- PBmodcomp(largeModel = mod8_nona,
>                                 smallModel = mod_min_nona,
>                                 nsim       = 100,
>                                 cl         = cl)
> 
> without removing the na.action attribute, I get:
> 
> > pbmod_mod8_mod_min <- PBmodcomp(largeModel = mod8_nona,
> +                                 smallModel = mod_min_nona,
> +                                 nsim       = 100,
> +                                 cl         = cl)
> Error in checkForRemoteErrors(lapply(cl, recvResult)) : 
>   one node produced an error: replacement has 10263 rows, data has 10713
> 
> > * I'm a little bit worried by method 2 -- I can imagine (although haven't
> > tested) that ysim may be a different length/may not match mod at frame,
> > which will be NA-processed. However, I think it should work OK
> > if the default na.action is 'na.omit' (I'd prefer that people _never_ used @
> > in their code ...)
> That actually was a problem. If I fit the models with lmer(..., na.action =
> na.exclude) but leave the rest the way it is. I will get a model.frame.default
> error message which has to do with differing lengths.  That?s why I redid
> Method 2 with Method 3 were I constructed a data frame with complete cases.
> 
> Thank you so much for taking a look at this!
> Christian
> 
> Eberhard Karls Universit?t T?bingen
> Mathematisch-Naturwissenschaftliche Fakult?t - Faculty of Science
> Evolutionary Cognition - Cognitive Science
> Schleichstra?e 4 ? 72076 T?bingen ? Germany
> Telefon +49 7071 29-75643 ? Telefax +49 7071 29-4721
> christian.brauner at uni-tuebingen.de
> 
> 
> On Sat, May 03, 2014 at 02:14:30PM -0400, Ben Bolker wrote:
> > On 14-05-03 07:59 AM, Christian Brauner wrote:
> > > Hello all,
> > > 
> > 
> > > warning ahead: This will be quite a long post. But have faith, it is
> > >  necessary.
> > 
> >   I have some questions about the larger context here.  I agree that
> > it's important to understand the behaviour of the code, and to ferret
> > out possible bugs in the code (I will say that handling all of the
> > permutations of NA handling in the predict and simulate code is a big
> > headache ...)  However, I'm wondering about a few things:
> > 
> >  * is the difference between p=0.004 and p=0.006 _practically_
> > important?
> >  * for this example, it seems that (as you comment) almost any approach
> > will give reasonable answers -- simple LRT (because the number of levels
> > of the minimum-size grouping variable is >=40), Kenward-Roger (because
> > this is a LMM).  Parametric bootstrap feels like a waste of computation
> > time.
> > * I can't do it off the top of my head, but for a balanced/fully crossed
> > random effect, one might even be able to compute the denominator df
> > for the F distribution exactly.
> > * is there a reason you're not using refit() in method 2?  It should
> > be faster.
> > * does PBmodcomp() work with the latest version of lme4, with the
> > simulate/NA fix?  Or do we or the pbkrtest maintainers still have work
> > to do?
> > * I'm a little bit worried by method 2 -- I can imagine (although haven't
> > tested) that ysim may be a different length/may not match mod at frame,
> > which will be NA-processed. However, I think it should work OK
> > if the default na.action is 'na.omit' (I'd prefer that people _never_ used @
> > in their code ...)
> > 
> > 
> > > I run parametric bootstraps in order to compare two models. I
> > > bootstrap the likelihood ratio between the models. For anybody
> > > acquainted with this, the code should be self explanatory. Model
> > > selection is more or less done and these are exercises I like to
> > > perform with real data sets to get a feel for how different
> > > softwares treat the same models. Unfortunately, I am not in a
> > > position to provide the real data. A few words about the models
> > > though:
> > 
> > > Both models are fit on a balanced data set which includes around 483 missing
> > > values. There are slightly above 11196 observations (within: 70 subjects).
> > > Every subject saw every item only one time. The dependent variable are reading
> > > times; sentence_type and matching are crossed two-level factors. Context and
> > > subject are treated as random effects. Context has 40 levels and (well you
> > > guessed) subjects has 70 levels. (See this related post on:
> > > http://stats.stackexchange.com/questions/94302/lmer-parametric-bootstrap-testing-for-fixed-effects):
> > > 
> > > Model formulae:
> > > ===============
> > >         
> > > # I only show the model formulae here because this piece stays fixed with
> > > # every call to lmer(). The options change however, depending on the parametric  
> > > # bootstrap procedure used. Any change can be seen in the code:
> > > mod_min := value ~ passung + (1 + satz_typ | vp) + (1 + satz_typ | kontext)
> > > mod8    := value ~ passung + (1 + satz_typ | vp) + (1 + satz_typ | kontext)
> > > 
> > > I set up different ways to perform parametric bootstraps. I will describe the
> > > different ways and explain what I changed every time and show the results. The
> > > ultimate question will be: Why the different results? Especially in one of the
> > > ways to perform a parametric bootstrap.
> > 
> >   Making my own summary:
> > 
> > 1. pbkrtest, na.action attribute removed. p=0.003
> > 2. lmer refitting: p=0.0064
> > 3. fitted on complete cases: p=0.0066
> > 4. using na.exclude and refit: p = 0.0043
> > 5. using complete cases and refit: p = 0.004
> > 
> > So I guess to boil it down, the difference is in the way that refit
> > handles missing values / accesses the model frame.  If I were going
> > to pursue this farther, I would probably try to strip it down and look
> > at the results of a _single_ refit with the random number seed set to
> > be identical in every case ...
> > 
> > 
> > > 
> > > Method 1 (pbkrtest).
> > > ====================
> > > 
> > > 1.1. lmer() model calls:
> > > ------------------------
> > > 
> > > mod_min <- lmer(formula = log(value)
> > >                 ~ 1
> > >                 + (1 + satz_typ | vp)
> > >                 + (1 + satz_typ | kontext),
> > >                 data = wort12_lmm2_melt,
> > >                 REML = FALSE)
> > > 
> > > mod8 <- lmer(formula = log(value)
> > >              ~ passung
> > >              + (1 + satz_typ | vp)
> > >              + (1 + satz_typ | kontext),
> > >              data = wort12_lmm2_melt,
> > >              REML = FALSE)
> > > 
> > > 1.2. Parametric Bootstrap:
> > > --------------------------
> > > 
> > > # remove NAs otherwise PBmodcomp() will complain as it calls the simulate()
> > > # function directly without the argument na.action = na.exclude
> > > # Relevant: See Ben Bolkers comments on this:
> > > # https://github.com/lme4/lme4/issues/189 dirty fix:
> > > mod8_nona    <- mod8
> > > mod_min_nona <- mod_min
> > > 
> > > attr(x     = mod8_nona at frame, 
> > >      which = "na.action") <- NULL # removing na.action attribute from the model
> > > 
> > > attr(x     = mod_min_nona at frame,
> > >      which = "na.action") <- NULL # removing na.action attribute from the model
> > > 
> > > library(pbkrtest)
> > > 
> > > pbmod_mod8_mod_min <- PBmodcomp(largeModel = mod8_nona,
> > >                                 smallModel = mod_min_nona,
> > >                                 nsim       = 10000,
> > >                                 cl         = cl)
> > > 
> > > 
> > > Take home message PBmodcomp(): p.value = 0.003
> > > 
> > > 1.3 Notable:
> > > ------------
> > > na.action attribute has been removed from both models before simulate() call.
> > > 
> > > 
> > > Method 2.
> > > =========
> > > 
> > > 2.1 lmer() model calls:
> > > -----------------------
> > > 
> > > identical to Method 1 (pbkrtest).
> > > 
> > > 2.2 Parametric Bootstrap:
> > > -------------------------
> > > 
> > > mod <- mod8
> > > modnull <- mod_min
> > > # save the observed likelihood ratio test statistic
> > > lrt_obs <- anova(modnull,
> > >                  mod)$Chisq[2]
> > > 
> > > n_sim <- 10000
> > > 
> > > dattemp <- mod at frame
> > > 
> > > ptime <- system.time({
> > >     lrt_sim <- foreach (i = 1:n_sim,
> > >                         .combine  = "c",
> > >                         .packages = "lme4") %dopar% {
> > > 
> > >         ysim       <- unlist(x = simulate(object = modnull))
> > > 
> > >         modnullsim <- lmer(formula = ysim
> > >                            ~ 1
> > >                            + (1 + satz_typ | vp)
> > >                            + (1 + satz_typ | kontext),
> > >                            data = dattemp,
> > >                            REML = FALSE) # Fit the null model.
> > > 
> > >         modaltsim  <- lmer(formula = ysim
> > >                            ~ passung
> > >                            + (1 + satz_typ | vp)
> > >                            + (1 + satz_typ | kontext),
> > >                            data = dattemp,
> > >                            REML = FALSE) # Fit the alternative model.
> > > 
> > >         anova(modnullsim,
> > >               modaltsim)$Chisq[2] # Save the likelihood ratio test statistic.
> > > 
> > >     }
> > > })[3]
> > > 
> > > sum(lrt_sim >= lrt_obs) + 1)/(n_sim + 1) = 0.00639936 # p.value
> > > 
> > > Take home message Method 2: p.value = 0.00639936
> > > 
> > > 2.3 Notable:
> > > ------------
> > > na.action has not been altered!
> > > 
> > > 
> > > Method 3.
> > > =========
> > > 3.1 lmer() model calls:
> > > -----------------------
> > > 
> > > # differences explained in 3.3
> > > mod_min <- lmer(formula = log(value)
> > >                 ~ 1
> > >                 + (1 + satz_typ | vp)
> > >                 + (1 + satz_typ | kontext),
> > >                 data = wort12_lmm2_melt_noa,
> > >                 REML = FALSE)
> > > 
> > > mod8 <- lmer(formula = log(value)
> > >              ~ passung
> > >              + (1 + satz_typ | vp)
> > >              + (1 + satz_typ | kontext),
> > >              data = wort12_lmm2_melt_nona,
> > >              REML = FALSE)
> > > 
> > > 3.2 Parametric Bootstrap:
> > > -------------------------
> > > 
> > > indentical to Method 2
> > > 
> > > Take home message Method 3: p.value = 0.00659934
> > > 
> > > 
> > > 3.3 Notable:
> > > ------------
> > > 
> > > The models were fit on the original dataset which only included complete cases.
> > > That is, on a data set that was altered by the following code:
> > > 
> > > wort12_lmm2_melt_nona <-
> > >     wort12_lmm2_melt[complete.cases(wort12_lmm2_melt), ]
> > > 
> > > 
> > > Method 4.
> > > =========
> > > 4.1 lmer() model calls:
> > > -----------------------
> > > 
> > > # differences explained in 4.3
> > > mod_min_naexclude <- lmer(formula = log(value)
> > >                           ~ 1
> > >                           + (1 + satz_typ | vp)
> > >                           + (1 + satz_typ | kontext),
> > >                           data      = wort12_lmm2_melt,
> > >                           REML      = FALSE,
> > >                           na.action = na.exclude)
> > > 
> > > mod8_naexclude <- lmer(formula = log(value)
> > >                        ~ passung
> > >                        + (1 + satz_typ | vp)
> > >                        + (1 + satz_typ | kontext),
> > >                        data      = wort12_lmm2_melt,
> > >                        REML      = FALSE,
> > >                        na.action = na.exclude)
> > > 
> > > 
> > > 4.2 Parametric Bootstrap:
> > > -------------------------
> > > mod <- mod8_naexclude
> > > modnull <- mod_min_naexclude
> > > 
> > > lrt_obs <- anova(modnull,
> > >                  mod)$Chisq[2] # Save the observed likelihood ratio test statistic.
> > > 
> > > n_sim <- 10000
> > > 
> > > ptime <- system.time({
> > >     lrt_sim <- foreach (i = 1:n_sim,
> > >                         .combine   = "c",
> > >                         .packages = "lme4") %dopar% {
> > > 
> > >         ysim       <- unlist(simulate(object = modnull))
> > > 
> > >         modnullsim <- refit(object  = modnull,
> > >                             newresp = ysim) # Fit the null-model.
> > > 
> > >         modaltsim  <- refit(object  = mod,
> > >                             newresp = ysim) # Fit the alternative model.
> > > 
> > >         anova(modnullsim,
> > >               modaltsim)$Chisq[2] # Save the likelihood ratio test statistic.
> > > 
> > >     }
> > > })[3]
> > > 
> > > Take home message Method 4: p.value = 0.00429957
> > > 
> > > 
> > > 4.3 Notable:
> > > ------------
> > > The models were fit with the option na.action = na.exclude.
> > > 
> > > 
> > > Method 5.
> > > =========
> > > 
> > > 5.1 lmer() model calls:
> > > -----------------------
> > > 
> > > # differences explained in 5.3
> > > mod_min_naexclude <- lmer(formula = log(value)
> > >                           ~ 1
> > >                           + (1 + satz_typ | vp)
> > >                           + (1 + satz_typ | kontext),
> > >                           data      = wort12_lmm2_melt_nona,
> > >                           REML      = FALSE)
> > > 
> > > mod8_naexclude <- lmer(formula = log(value)
> > >                        ~ passung
> > >                        + (1 + satz_typ | vp)
> > >                        + (1 + satz_typ | kontext),
> > >                        data      = wort12_lmm2_melt_nona,
> > >                        REML      = FALSE)
> > > 
> > > 
> > > 5.2 Parametric Bootstrap:
> > > -------------------------
> > > 
> > > indentical to Method 4
> > > 
> > > Take home message Method 5: p.value = 0.0039996
> > > 
> > > 5.3 Notable:
> > > ------------
> > > The models were fit on the original dataset which only included complete cases.
> > > That is, on a data set that was altered by the following code:
> > > 
> > > wort12_lmm2_melt_nona <-
> > >     wort12_lmm2_melt[complete.cases(wort12_lmm2_melt), ]
> > > 
> > > 
> > > Question:
> > > =========
> > > 
> > > Why do Method 2 and Method 3 yield such (in my opinion) p.values that deviate
> > > by about 0.002 from all other methods?
> > > 
> > > (I think at least the explanation of why I do not see it is really simple: I
> > > just have been looking to long at this code and I am missing the obvious.)
> > > 
> > > Two final remarks:
> > > 
> > > (1) I ran everything on a cluster. That?s why I repeated Methods 2 and Methods
> > > 3. The results they yield seem to be numerically stable.
> > > 
> > > (2) I plotted the likelihood ratio test statistics for all methods and compared
> > > them to a chi-squared distribution with df=1, and their likelihood ratio test
> > > density. Everything is normal. The chi-square assumption is correct for these
> > > models
> > > 
> > > Thank you so much for your patience!
> > > 
> > > Best,
> > > Christian Brauner
> > > 
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > > 
> >


From bbolker at gmail.com  Sun May  4 17:04:03 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 04 May 2014 11:04:03 -0400
Subject: [R-sig-ME] BOBYQA warning message
In-Reply-To: <000001cf670c$e8271e60$b8755b20$@gmail.com>
References: <000001cf670c$e8271e60$b8755b20$@gmail.com>
Message-ID: <536656E3.3020108@gmail.com>

On 14-05-03 04:18 PM, Devin Kearns wrote:
> Hello List:
> 
>  
> 
> I am getting the following warning message when running a model with BOBYQA
> optimization. I am not sure how to interpret it:
> 
>  
> 
> Warning message:
> 
> In commonArgs(par, fn, control, environment()) :
> 
>   maxfun < 10 * length(par)^2 is not recommended.

  You have 32 parameters (20 fixed and 12 random-effects
variance-covariance parameters), unless I miscounted.  That means that
BOBYQA recommends that the maximum number of function evaluations should
be at least 10*32^2 = 10240.  It's a bit hard to dig out the default
value of maxfun, but it's 10000 (so you only missed by a little bit).
Arguably lme4 should make this adjustment automatically; someone could
add an issue at https://github.com/lme4/lme4/issues if they wanted.
Bottom line:

  * you can use control=glmerControl(optCtrl=list(maxfun=1e5)) to adjust
the maximum number of function evaluations.
  * if @optinfo$feval will tell you how many function evaluations were
actually used.


> 
>> summary(model)
> 
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
> 
> Family: binomial ( logit )
> 
> Formula: wr ~ 1 + a + b + c + d*e + f*e + g*e + h*e + i +  
> 
>     i*k + k*e + l*e + m + (1+f+i|w) + (-1+d|w) + (1+m|x) + (1|y) + (1| z)
> 
>    Data: m
> 
> Control: glmerControl(optimizer = "bobyqa")
> 
>  
> 
>      AIC      BIC   logLik deviance df.resid 
> 
>   6198.5   6426.2  -3067.3   6134.5     9058 
> 
>  
> 
> Scaled residuals: 
> 
>      Min       1Q   Median       3Q      Max 
> 
> -10.7164  -0.2638   0.1284   0.3356  10.3950 
> 
>  
> 
> Random effects:
> 
> Groups Name        Variance Std.Dev. Corr       
> 
>      (Intercept)    0.189556 0.43538             
> 
>  x      m           0.087545 0.29588  0.24       
> 
>  w      d           0.385553 0.62093             
> 
>  w.1    (Intercept) 1.673762 1.29374             
> 
>         f           0.045596 0.21353  -0.39      
> 
>         i           0.036892 0.19207  -0.55 -0.36
> 
> y      (Intercept) 0.005768 0.07595             
> 
>  z      (Intercept) 0.000000 0.00000             
> 
> Number of obs: 9090, groups: x, 202; w, 45; y, 23; z, 6
> 
>  
> 
> Fixed effects:
> 
>                       Estimate Std. Error z value Pr(>|z|)    
> 
> (Intercept)            1.72456    0.21281   8.104 5.32e-16 ***
> 
> a                      0.29682    0.06380   4.652 3.28e-06 ***
> 
> b                     -0.06976    0.17471  -0.399 0.689698    
> 
> c                      0.12498    0.09981   1.252 0.210496    
> 
> d                      1.00641    0.13625   7.387 1.51e-13 ***
> 
> e                      1.28465    0.20023   6.416 1.40e-10 ***
> 
> f                      0.17962    0.08665   2.073 0.038175 *  
> 
> g                      1.30007    0.08096  16.057  < 2e-16 ***
> 
> h                      0.16457    0.07051   2.334 0.019601 *  
> 
> i                      0.01631    0.07326   0.223 0.823805    
> 
> k                      0.76396    0.21435   3.564 0.000365 ***
> 
> l                     -0.35635    0.19711  -1.808 0.070633 .  
> 
> m                      0.30247    0.22093   1.369 0.170979    
> 
> d:e                    0.08819    0.13846   0.637 0.524198    
> 
> e:f                   -0.01060    0.06633  -0.160 0.873059    
> 
> e:g                    0.04057    0.05732   0.708 0.479095    
> 
> e:h                    0.02144    0.05099   0.421 0.674085    
> 
> i:k                    0.14288    0.05095   2.804 0.005045 ** 
> 
> e:k                   -0.31092    0.19069  -1.630 0.103000    
> 
> e:l                   -0.28359    0.22508  -1.260 0.207682    
> 
>  
> 
> If it is not clear above, "w" and "x" are perfectly crossed effects (there
> are no missing values). In multilevel language, "x" is nested within "y"
> which is nested within "z".
> 
>  
> 
> "a" is continuous and varies across "x".
> 
> "b" is binary and varies across "x". 
> 
> "c" and "d" are binary and vary across "w" and "x".
> 
> "e", "k", "l", and "m" are continuous and vary across "w".
> 
> "f", "g", "h", "i", and "I" are continuous and vary across "x". 
> 
>  
> 
> To explain the random intercepts and slopes, these were derived from a model
> building procedure that involved the addition of a random slope for each
> fixed effect and testing whether the model without the random slope fit
> worse than that with it (the idea was to follow the "forward selection"
> approach Bates described here:
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q2/003921.html).
> Correlations were added in the same way. The "maximal" strategy is ideal,
> but the model with all possible random slopes and correlations did not
> converge, nor did a "near maximal" model. Barr, Levy, Scheepers, and Tily
> (2013) suggested that this approach controls Type I error almost as well as
> the maximal strategy, which is why it was used here. 
> 
>  
> 
> It is not clear to me what this warning means or why it occurred. Here were
> my initial thoughts: 
> 
>  
> 
> "z" has almost no variability (and it is a random effect with only six
> levels). I was included because the unconditional model without fit worse
> than that with it (the p value for the likelihood ratio test was .07, which
> is significant for these). I am therefore reluctant to remove this term,
> although I suspected the error might relate to the lack of information
> related to z. 
> 
>  
> 
> I also wondered whether the number of "w" covariates was too large given
> that I have only 45 "w" cases. I have seen papers where authors have
> included more covariates with fewer numbers (I've seen as many as 8
> covariates for a random effect with only 39 levels). In logistic regression,
> there is the 10:1 rule of thumb for observations to covariates, but there do
> not seem to be any such rules of thumb for these models. It is my
> understanding that lme4 includes some tests for overfitting, but this seems
> to be under development: https://github.com/lme4/lme4/issues/7). So, I
> considered the possibility of overfitting (although the standard errors do
> not raise concerns). 
> 
>  
> 
> Of course, "warnings" are not "errors" and it is possible that nothing is
> wrong. However, I am reluctant to believe that!
> 
>  
> 
> Would anyone be able to explain this warning? Is there something I could do
> differently? 
> 
>  
> 
> Thank you for your time. I am very appreciative of the list's help.
> 
>  
> 
> Devin.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Mon May  5 02:11:17 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 04 May 2014 20:11:17 -0400
Subject: [R-sig-ME] Fwd: Re: extract specific values from lmer summary
In-Reply-To: <5366CC76.6060300@alaska.edu>
References: <5366CC76.6060300@alaska.edu>
Message-ID: <5366D725.2050308@gmail.com>


 * yes, in general these sorts of question should go directly to
r-sig-mixed-models at r-project.org

 * it's mildly surprising but not shocking that this format doesn't work
any more.

 * without testing, here are a few idioms that should work better:


for(i in 1:nlevels(vital)) {
   lmer.sheep <- glmer(y ~ (1|year),
      family = binomial,
      data = subset(sheep.dat,vital==levels(vital)[i])
}

or

for(i in 1:nlevels(vital)) {
   lmer.sheep <- glmer(y ~ (1|year),
      family = binomial,
      data = sheep.dat,
      subset = vital==levels(vital)[i])
}

 or

model.list <- vector(nlevels(vital),mode="list")
model.list[[1]] <- glmer(y ~ (1|year),
      family = binomial,
      data = sheep.dat,
      subset = vital==levels(vital)[1])
for (i in 2:nlevels(vital)) {
  model.list[[i]] <- update(model.list[[1]],
       subset = vital==levels(vital)[i]
}

  etc.





-------- Original Message --------
Subject: Re: extract specific values from lmer summary
Date: Sun, 04 May 2014 15:25:42 -0800
From: Laura Prugh <lprugh at alaska.edu>
To: Ben Bolker <bbolker at gmail.com>

Hi Ben,
Sorry to bother you again (please let me know if you prefer I post these
questions to a list?)--I updated my version of R to the newest and reran
the code, which had run fine on the older version. It's now giving me an
error referring to how I'm trying to get it to loop through my
dataset--my code is:
for(i in 1:nlevels(vital)) {
lmer.sheep <- glmer(y[vital==levels(vital)[i]] ~
(1|year[vital==levels(vital)[i]]), family = binomial, data = sheep.dat)
}
and it gives the error:
Error in FUN(X[[1L]], ...) :
  Invalid grouping factor specification, year[vital == levels(vital)[i]]

When I put "year[vital == levels(vital)[i]]" directly into R, it appears
to work properly, and this syntax worked yesterday on the older R
version. Is a new syntax required or could this perhaps be a bug?

thanks,
Laura

On 5/2/2014 1:18 PM, Ben Bolker wrote:
> On 14-05-02 03:53 PM, Laura Prugh wrote:
>> Hi Ben,
>> I am trying to save specific outputs from the lmer summary, and I was
>> hoping you might be able to tell me the code to do this. I'm running a
>> random effects model with only the intercept as a fixed effect. I want
>> to extract the fixed effect intercept estimate and the variance (or
>> standard deviation) of the random effect. I seem to have figured out how
>> to get the fixed effect (model at fixedef) but have had no luck getting the
>> random effect. I guessed a variety of things and model at ranef returned
>> something, but it didn't match anything in the summary output. Here is
>> the output of my model, and I would like to extract and save the st dev
>> of the random effect (0.21287). Any help would be greatly appreciated.
>> If you generally know of a resource for figuring out the syntax to
>> extract specific values from summary outputs, that would be great
>> because this is a recurring problem for me! By the way, I plan to have
>> as many of my students at possible attend the workshop you're offering
>> at Chena HS near Fairbanks in August.
>> cheers,
>> Laura
>     This is really more of an r-sig-mixed-models at r-project.org question:
> I'm forwarding the answer there.
>
>    methods(class="merMod") is a good way to check the available accessor
> methods for (g)lmer fits, as is ?getME (an lme4-specific function for
> getting at lower-level components of a (g)lmer fit in a safe/stable way).
>
>
>>> lmer.sheep <- lmer(y ~ (1|year),family=binomial, data = coy,
>> + control=list(msVerbose=F,maxIter=1000,maxFN=3000))
>>> lmer.sheep
>> Generalized linear mixed model fit by the Laplace approximation
>> Formula: y ~ (1 | year)
>> Data: coy
>> Fixed effects:
>> Estimate Std. Error z value Pr(>|z|)
>> (Intercept) -0.8083 0.2168 -3.729 0.000192 ***
>> ---
>> Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>> mn <- lmer.sheep at fixef
>>> mn
>> (Intercept)
>> -0.8083453
>>> lmer.sheep at ranef
>> [1] 0.09490052 0.09490052 0.03168227 -0.01783168 -0.14832171 -0.04832885
>     A reproducible example would be slightly better, but here I believe
> you're looking for
>
>    fixef(lmer.sheep)[1]  ## intercept
>    sqrt(unlist(VarCorr(lmer.sheep))  ## std dev of year effect
>
>    It looks like you're still using an older version of lme4: I believe
> the incantations I gave above are backward compatible, but there are a
> fair number of convenience methods in the newer (>1.0) version that
> might be helpful (e.g. as.data.frame(VarCorr(lmer.sheep)))
>
>    Ben Bolker
>
>


From simons6831 at gmail.com  Sat May  3 11:29:40 2014
From: simons6831 at gmail.com (Simon Sun)
Date: Sat, 3 May 2014 09:29:40 +0000 (UTC)
Subject: [R-sig-ME] standardized coefficients in lme4 package
References: <AD30DD503BC9E9479E223CA90866D28601DF0AB4@clmail1.nioo.int>
	<Pine.LNX.4.64.1012120623530.7782@orpheus.qimr.edu.au>
	<AD30DD503BC9E9479E223CA90866D28601DF0AB8@clmail1.nioo.int>
	<4D0439EE.7060208@gmail.com>
Message-ID: <loom.20140503T112235-225@post.gmane.org>

Dear Ben,

I am a user of your excellent work lme4. While during my work I realized the 
problem that the package doesn't provide function to return standardized 
coefficients.

I found a section of code on your website:
##
lm.beta.lmer <- function(mod) {
  b <- fixef(mod)[-1]              ## fixed-effect coefs, sans intercept
  sd.x <- apply(mod @ X[,-1],2,sd) ## pull out model (design) matrix,
                                   ## drop intercept column, calculate
                                   ## sd of remaining columns
  sd.y <- sd(mod @ y)              ## sd of response
  b*sd.x/sd.y
}
##

But I couldn't make it work, every time it showed:

 Error in apply(Mod at x[, -1], 2, sd) : no slot of name "x" for this object of 
class "lmerMod"

Could you please help me figure it out? Thank you very much!


From andrea.cantieni at phsz.ch  Mon May  5 16:54:33 2014
From: andrea.cantieni at phsz.ch (Andrea Cantieni)
Date: Mon, 5 May 2014 14:54:33 +0000
Subject: [R-sig-ME] standardized coefficients in lme4 package
In-Reply-To: <loom.20140503T112235-225@post.gmane.org>
References: <AD30DD503BC9E9479E223CA90866D28601DF0AB4@clmail1.nioo.int>
	<Pine.LNX.4.64.1012120623530.7782@orpheus.qimr.edu.au>
	<AD30DD503BC9E9479E223CA90866D28601DF0AB8@clmail1.nioo.int>
	<4D0439EE.7060208@gmail.com> <loom.20140503T112235-225@post.gmane.org>
Message-ID: <CCCEFF97-8528-4C0C-B214-0CF751753B10@phsz.ch>

Ein eingebundener Text mit undefiniertem Zeichensatz wurde abgetrennt.
Name: nicht verf?gbar
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140505/3bd790d4/attachment.pl>

From cantalarrana at yahoo.com  Mon May  5 18:20:24 2014
From: cantalarrana at yahoo.com (laura laura)
Date: Mon, 5 May 2014 09:20:24 -0700 (PDT)
Subject: [R-sig-ME] glmmADMB zero-inflated poisson model. How to solve
Message-ID: <1399306824.21369.YahooMailNeo@web162405.mail.bf1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140505/c378647e/attachment.pl>

From Thierry.ONKELINX at inbo.be  Mon May  5 20:58:08 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 5 May 2014 18:58:08 +0000
Subject: [R-sig-ME] glmmADMB zero-inflated poisson model. How to solve
In-Reply-To: <1399306824.21369.YahooMailNeo@web162405.mail.bf1.yahoo.com>
References: <1399306824.21369.YahooMailNeo@web162405.mail.bf1.yahoo.com>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A44AD0@inbomail.inbo.be>

Laura,

You have 29 non-zero observations. Assuming that all variables are continuous (or discrete with only two levels), your full model estimates 9 parameters for the poisson part (and 1 for the zero inflated part). 29 observations for 9 parameters is just not enough. As a rule of thumb, you need about 10 observations per parameter.

This leaves you with 3 options: 1) gather more non-zero data (aim for at least 300 more non-zero observations). 2) rethink your full model so that is only uses 3 parameters. Note that you need 2 parameters for your random effects. Hence one (1!) left for the fixed effects. 3) Put this dataset in the garbage bin and start a new experiment from scratch. Consult a local statistician while designing your experiment.

I'm afraid the quotes in my signature apply to your problem.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens laura laura
Verzonden: maandag 5 mei 2014 18:20
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] glmmADMB zero-inflated poisson model. How to solve

Dear all,

I'm trying to run a ZIGLMM  with the package glmmADMB. My data set contains 37 zero-valued of 66 observations in the dependent variable (number of attaks), so I consider that a zero-inflated model would be appropriated. I checked the dataset and, apparently, there are enough variation for all combinations.

I centered and rescaled continuos input variables before performing the model. I run several models and they looked OK, but an error message appeared when trying some combinations, usually with only one explanatory  variable (but not in all cases!!) and with the null model (only random effects).
This is the full model:

model_full<-glmmadmb (Nat~ coldecoy+ colmale*colfem+sfecha+ snnd+ colvecino+ (1|Year) +(1|Nido),data=data, zeroInflation=TRUE,family="poisson")
summary(model_full)

and this is the error message:

model_7<-glmmadmb (Nat~ colfem+ (1|Year) +(1|Nido),data=data, zeroInflation=TRUE,family="poisson")
Parameters were estimated, but not standard errors were not: the most likely problem is that the curvature at MLE was zero or negative Error en glmmadmb(Nat ~ colfem + (1 | Year) + (1 | Nido), data = data,  :
  The function maximizer failed (couldn't find STD file) Troubleshooting steps include (1) run with 'save.dir' set and inspect output files; (2) change run parameters: see '?admbControl'
Adem?s: Mensajes de aviso perdidos
comando ejecutado './glmmadmb -maxfn 500 -maxph 5 -noinit -shess' tiene estatus 1

I found similar an almost identical question in the list (sorry, I was unable to find it again and I cannot give more details). I checked the ?admbControl and further explanations about this provided in a previous post, but it was unclear to me what should I do. Sorry if I'm repeating an already "solved" topic, but I was unable to find an answer.  I would be very grateful if someone can give me some help.

Best,

Laura
        [[alternative HTML version deleted]]

* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From asafw.at.wharton at gmail.com  Tue May  6 06:11:59 2014
From: asafw.at.wharton at gmail.com (Asaf Weinstein)
Date: Tue, 6 May 2014 00:11:59 -0400
Subject: [R-sig-ME] Accessing and updating lmer objects
Message-ID: <CAGG0PdDj=uWjfaVWCHoXjHJDoKtkQogt3Nm5h-3i=Deb4ohoSw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140506/c921212c/attachment.pl>

From dhocking at umass.edu  Tue May  6 04:38:10 2014
From: dhocking at umass.edu (Daniel Hocking)
Date: Mon, 5 May 2014 22:38:10 -0400
Subject: [R-sig-ME] gamm4 error with large dataset
Message-ID: <2BFA9B62-4C7E-440D-8361-017868CE2300@umass.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140505/87548b18/attachment.pl>

From bbolker at gmail.com  Tue May  6 21:27:09 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 06 May 2014 15:27:09 -0400
Subject: [R-sig-ME] Accessing and updating lmer objects
In-Reply-To: <CAGG0PdDj=uWjfaVWCHoXjHJDoKtkQogt3Nm5h-3i=Deb4ohoSw@mail.gmail.com>
References: <CAGG0PdDj=uWjfaVWCHoXjHJDoKtkQogt3Nm5h-3i=Deb4ohoSw@mail.gmail.com>
Message-ID: <5369378D.3030502@gmail.com>

On 14-05-06 12:11 AM, Asaf Weinstein wrote:
> Hi,
> 
> I am trying to follow the lme4 manual (Bates) where objects returned by
> lmer() are accessed in the following way (for example):
> 
> fm08 at re@Lambda at x[] <- c(1,0,1)[fm08 at re@Lind]
> 
> If I understand correctly, accessing with "@" no longer works, and to
> obtain, eg, Lambda, I'll need to use
> 
> getME(fm08, 'Lambda')
> 
> instead.
> 
> But how do I update an object (not just view it)? In other words, what
> command replaces
> 
> fm08 at re@Lambda at x[] <- c(1,0,1)[fm08 at re@Lind] ?
> 
> 

   I *believe* (without much testing) that if m is a merMod object this
would be something like

m at pp$Lambdat at x[] <- c(1,0,1)[m at pp$Lind]

but I'm afraid that you might run into a _lot_ of roadblocks if you try
to work your way through the old manual with the new lme4.  We are
working on updated documentation ... it might make more sense to work
either with lme4.0, or with the lme4pureR package from Github ...

  Ben Bolker


From John.Morrongiello at csiro.au  Wed May  7 03:38:45 2014
From: John.Morrongiello at csiro.au (John.Morrongiello at csiro.au)
Date: Wed, 7 May 2014 01:38:45 +0000
Subject: [R-sig-ME] conditional AIC (cAIC) for lmer
Message-ID: <2547E22D246F3945BB491BDD8257C2E77F03ACCF@exmbx06-cdc.nexus.csiro.au>

Hi 
I would like to calculate the conditional AIC (cAIC) to compare a series of mixed models with different random effects structures for the purpose of ecological inference (method as per Vaida and Blanchard 2005 & Greven and Kneib 2010). I am using lme4 1.1-6 in R 3.1.0. Greven and Kneib provide a package for nlme; I got in touch with them directly and an lme4 version will likely be available later this year. I do, however, have a little bit more of a finite deadline and I was wondering if anyone else has developed a function to do this that works in the latest version of lme4? 

Kyle Edwards posted a function in 2008 (http://r.789695.n4.nabble.com/Using-Conditional-AIC-with-lmer-td847899.html) but this no longer works as the function 'hatTrace' is no longer available (http://r.789695.n4.nabble.com/Function-hatTrace-in-package-lme4-td4646071.html). In the package 'phmm' there is a 'cAIC' function but this doesn't work for lmer models.

Doing sequential likelihood ratio tests is another option, and I have seen some papers using DIC (which I thought was a Bayesian technique so not really applicable to a lmer model). I guess I could report AIC values and note that they are biased in this case due to issues around calculating degrees of freedom, but if there is a better option (e.g. cAIC) I'd prefer to go with that. 

Thanks for your time

John


From corey.sparks at utsa.edu  Wed May  7 16:14:47 2014
From: corey.sparks at utsa.edu (Corey Sparks)
Date: Wed, 7 May 2014 14:14:47 +0000
Subject: [R-sig-ME] conditional AIC (cAIC) for lmer
Message-ID: <C099AA44-6CDB-42F7-ADE5-2678DAD7EBEA@utsa.edu>

Hi John,
in the arm library, the display() function will give you a DIC for lmer models. The DIC is usually for fully bayesian models, but the models in lmer are just using approximations to the posterior distributions of the random effects, instead of MCMC sampling, so it?s still a valid method for model comparison and won?t make the assumptions about model degrees of freedom like the AIC reported in lme4:::summary.merMod.

Cheers,
Corey


Subject: [R-sig-ME] conditional AIC (cAIC) for lmer
Message-ID:
	<2547E22D246F3945BB491BDD8257C2E77F03ACCF at exmbx06-cdc.nexus.csiro.au>
Content-Type: text/plain; charset="us-ascii"

Hi 
I would like to calculate the conditional AIC (cAIC) to compare a series of mixed models with different random effects structures for the purpose of ecological inference (method as per Vaida and Blanchard 2005 & Greven and Kneib 2010). I am using lme4 1.1-6 in R 3.1.0. Greven and Kneib provide a package for nlme; I got in touch with them directly and an lme4 version will likely be available later this year. I do, however, have a little bit more of a finite deadline and I was wondering if anyone else has developed a function to do this that works in the latest version of lme4? 

Kyle Edwards posted a function in 2008 (http://r.789695.n4.nabble.com/Using-Conditional-AIC-with-lmer-td847899.html) but this no longer works as the function 'hatTrace' is no longer available (http://r.789695.n4.nabble.com/Function-hatTrace-in-package-lme4-td4646071.html). In the package 'phmm' there is a 'cAIC' function but this doesn't work for lmer models.

Doing sequential likelihood ratio tests is another option, and I have seen some papers using DIC (which I thought was a Bayesian technique so not really applicable to a lmer model). I guess I could report AIC values and note that they are biased in this case due to issues around calculating degrees of freedom, but if there is a better option (e.g. cAIC) I'd prefer to go with that. 

Thanks for your time

John
 


Corey Sparks
Assistant Professor
Department of Demography
The University of Texas at San Antonio
501 West Cesar E Chavez Blvd
corey.sparks 'at' utsa.edu
coreysparks.weebly.com
210 458 3166


From henrik.singmann at psychologie.uni-freiburg.de  Thu May  8 16:33:27 2014
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Thu, 08 May 2014 16:33:27 +0200
Subject: [R-sig-ME] difference in lmer results with REML = FALSE and refitML
Message-ID: <536B95B7.7080901@psychologie.uni-freiburg.de>

Dear list,

based on a question on the sister list for linguistics (http://thread.gmane.org/gmane.comp.lang.r.linguistics/844/focus=846) I came across some weird inconsistcneis when comparing a lmer fit with REML = FALSE and the same model first fitted with REML and then refitted with ML. The strange behavior is that both ML fits do not agree. This seems to be somewhat unexpected.

See case below.

Cheers,
Henrik

PS: I will only irregularly check my mails in the next days but wanted to let you know nevertheless.


require(lme4)
ddat = read.delim("http://www.ccunix.ccu.edu.tw/~lngproc/doramiR.txt")
ddat = na.omit(ddat)

contrasts(ddat$Education) = contr.sum(levels(ddat$Education)) # 1 = College
contrasts(ddat$SynCat) = contr.sum(levels(ddat$SynCat)) # 1 = Noun
contrasts(ddat$Freq) = contr.sum(levels(ddat$Freq)) # 1 = High

ddat$Education = 2*(ddat$Education=="College")-1


reml <- lmer(RT~ (Education*SynCat)+(Education*SynCat):Freq+((SynCat*Freq)|Participant)+(Education|Item),data = ddat)
ml2 <- refitML(reml)
summary(ml2)
## Linear mixed model fit by maximum likelihood  ['lmerMod']
## Formula: RT ~ (Education * SynCat) + (Education * SynCat):Freq + ((SynCat *      Freq) | Participant) + (Education | Item)
##    Data: ddat
##
##      AIC      BIC   logLik deviance df.resid
##   5119.3   5202.4  -2538.7   5077.3      365
## ...

ml <- lmer(RT~ (Education*SynCat)+(Education*SynCat):Freq+((SynCat*Freq)|Participant)+(Education|Item),data = ddat, REML = FALSE)
summary(ml)
## Linear mixed model fit by maximum likelihood  ['lmerMod']
## Formula: RT ~ (Education * SynCat) + (Education * SynCat):Freq + ((SynCat *      Freq) | Participant) + (Education | Item)
##    Data: ddat
##
##      AIC      BIC   logLik deviance df.resid
##   5122.1   5205.1  -2540.0   5080.1      365
## ...


-- 
Dr. Henrik Singmann
Albert-Ludwigs-Universit?t Freiburg, Germany
http://www.psychologie.uni-freiburg.de/Members/singmann


From witold.proskura at zut.edu.pl  Wed May  7 00:41:50 2014
From: witold.proskura at zut.edu.pl (Witold Proskura)
Date: Wed, 07 May 2014 00:41:50 +0200
Subject: [R-sig-ME] pedigreemm with one observation per individual
Message-ID: <60408dd8bc7b391d49fc55fd1b5ed759@webmail.zut.edu.pl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140507/f6dd43db/attachment.pl>

From vjd4 at nyu.edu  Thu May  8 19:30:06 2014
From: vjd4 at nyu.edu (Vincent Dorie)
Date: Thu, 8 May 2014 13:30:06 -0400
Subject: [R-sig-ME] Accessing and updating lmer objects
In-Reply-To: <5369378D.3030502@gmail.com>
References: <CAGG0PdDj=uWjfaVWCHoXjHJDoKtkQogt3Nm5h-3i=Deb4ohoSw@mail.gmail.com>
	<5369378D.3030502@gmail.com>
Message-ID: <C4D51035-4CE3-409B-87E2-C0E766C0ACEF@nyu.edu>

After you install new random effect covariance parameters into a merMod object, you need to propagate the changes to the various dependent matrix decompositions. For a lmm, it looks something like:

newTheta <- c(1, 0, 1)
fm at pp$setTheta(newTheta)
fm at pp$updateDecomp()
fm at resp$updateMu(fm at pp$linPred(0.0))
fm at pp$updateRes(fm at resp$wtres);
fm at pp$solve()
fm at resp$updateMu(fm at pp$linPred(1.0))

And then to get the deviance for the object,

fm at resp$objective(fm at pp$ldL2(), fm at pp$ldRX2(), fm at pp$sqrL(1.0))


That's the R equivalent of what the C++ code does for each optimization step. If you're working with a glmm, the process is similar but you can see how to do it much more easily by calling glmer with devFunOnly = TRUE and examining the resulting function.

In fact, if all you want to do is install the parameters, you can grab the deviance function for a lmm/glmm and simply call that with your desired parameters to get an updated object. However, if you want to interject modifications you will need to modify the above steps.

Vince

On May 6, 2014, at 3:27 PM, Ben Bolker wrote:

> On 14-05-06 12:11 AM, Asaf Weinstein wrote:
>> Hi,
>> 
>> I am trying to follow the lme4 manual (Bates) where objects returned by
>> lmer() are accessed in the following way (for example):
>> 
>> fm08 at re@Lambda at x[] <- c(1,0,1)[fm08 at re@Lind]
>> 
>> If I understand correctly, accessing with "@" no longer works, and to
>> obtain, eg, Lambda, I'll need to use
>> 
>> getME(fm08, 'Lambda')
>> 
>> instead.
>> 
>> But how do I update an object (not just view it)? In other words, what
>> command replaces
>> 
>> fm08 at re@Lambda at x[] <- c(1,0,1)[fm08 at re@Lind] ?
>> 
>> 
> 
>   I *believe* (without much testing) that if m is a merMod object this
> would be something like
> 
> m at pp$Lambdat at x[] <- c(1,0,1)[m at pp$Lind]
> 
> but I'm afraid that you might run into a _lot_ of roadblocks if you try
> to work your way through the old manual with the new lme4.  We are
> working on updated documentation ... it might make more sense to work
> either with lme4.0, or with the lme4pureR package from Github ...
> 
>  Ben Bolker
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Thu May  8 20:01:44 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 08 May 2014 14:01:44 -0400
Subject: [R-sig-ME] Accessing and updating lmer objects
In-Reply-To: <C4D51035-4CE3-409B-87E2-C0E766C0ACEF@nyu.edu>
References: <CAGG0PdDj=uWjfaVWCHoXjHJDoKtkQogt3Nm5h-3i=Deb4ohoSw@mail.gmail.com>
	<5369378D.3030502@gmail.com>
	<C4D51035-4CE3-409B-87E2-C0E766C0ACEF@nyu.edu>
Message-ID: <536BC688.8090506@gmail.com>

On 14-05-08 01:30 PM, Vincent Dorie wrote:

> After you install new random effect covariance parameters into a
  merMod object, you need to propagate the changes to the various
  dependent matrix decompositions. For a lmm, it looks something like:

> newTheta <- c(1, 0, 1)
> fm at pp$setTheta(newTheta)
> fm at pp$updateDecomp()
> fm at resp$updateMu(fm at pp$linPred(0.0))
> fm at pp$updateRes(fm at resp$wtres);
> fm at pp$solve()
> fm at resp$updateMu(fm at pp$linPred(1.0))

> And then to get the deviance for the object,
> 
> fm at resp$objective(fm at pp$ldL2(), fm at pp$ldRX2(), fm at pp$sqrL(1.0))

>  That's the R equivalent of what the C++ code does for each
> optimization step. If you're working with a glmm, the process is
> similar but you can see how to do it much more easily by calling
> glmer with devFunOnly = TRUE and examining the resulting function.

> In fact, if all you want to do is install the parameters, you can
  grab the deviance function for a lmm/glmm and simply call that with
  your desired parameters to get an updated object. However, if you
  want to interject modifications you will need to modify the above
  steps.

  Thanks, Vince.  I think Asaf was trying to follow the step-by-step
procedure (not just calculate the deviance for a new set of procedures),
so this is exactly what he needs -- although it *might* be somewhat
higher-level than the description in the Bates manual referred to
previously. (If you look at the code in src/predModule.cpp::setTheta,
the payload is

	int    *lipt = d_Lind.data();
	double *LamX = d_Lambdat.valuePtr(), *thpt = d_theta.data();
	for (int i = 0; i < d_Lind.size(); ++i) {
	    LamX[i] = thpt[lipt[i] - 1];
	}

which is more or less equivalent to the R code referenced earlier.)

  I really would recommend that people interested in following the
details check out https://github.com/lme4/lme4pureR ... especially
https://github.com/lme4/lme4pureR/blob/master/R/JSS.R ...

  cheers
    Ben

> Vince
> 
> On May 6, 2014, at 3:27 PM, Ben Bolker wrote:
> 
>> On 14-05-06 12:11 AM, Asaf Weinstein wrote:
>>> Hi,
>>>
>>> I am trying to follow the lme4 manual (Bates) where objects returned by
>>> lmer() are accessed in the following way (for example):
>>>
>>> fm08 at re@Lambda at x[] <- c(1,0,1)[fm08 at re@Lind]
>>>
>>> If I understand correctly, accessing with "@" no longer works, and to
>>> obtain, eg, Lambda, I'll need to use
>>>
>>> getME(fm08, 'Lambda')
>>>
>>> instead.
>>>
>>> But how do I update an object (not just view it)? In other words, what
>>> command replaces
>>>
>>> fm08 at re@Lambda at x[] <- c(1,0,1)[fm08 at re@Lind] ?
>>>
>>>
>>
>>   I *believe* (without much testing) that if m is a merMod object this
>> would be something like
>>
>> m at pp$Lambdat at x[] <- c(1,0,1)[m at pp$Lind]
>>
>> but I'm afraid that you might run into a _lot_ of roadblocks if you try
>> to work your way through the old manual with the new lme4.  We are
>> working on updated documentation ... it might make more sense to work
>> either with lme4.0, or with the lme4pureR package from Github ...
>>
>>  Ben Bolker
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From kegurney at alaska.edu  Thu May  8 22:25:18 2014
From: kegurney at alaska.edu (Kirsty Gurney)
Date: Thu, 8 May 2014 12:25:18 -0800
Subject: [R-sig-ME] Zero alteration and mixed models?
Message-ID: <24fdcc8985e013280f6108b28b6137d1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140508/e551c52d/attachment.pl>

From bbolker at gmail.com  Fri May  9 22:07:05 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 9 May 2014 20:07:05 +0000 (UTC)
Subject: [R-sig-ME] difference in lmer results with REML = FALSE and
	refitML
References: <536B95B7.7080901@psychologie.uni-freiburg.de>
Message-ID: <loom.20140509T220519-84@post.gmane.org>

 I've logged this at https://github.com/lme4/lme4/issues/202 .

Henrik Singmann <henrik.singmann at ...> writes:

> 
> Dear list,
> 
> based on a question on the sister list for linguistics
> (http://thread.gmane.org/gmane.comp.lang.r.linguistics/844/focus=846) 
> I came across some weird
> inconsistencies when comparing a lmer fit with REML = FALSE 
> and the same model first fitted with REML and
> then refitted with ML. The strange behavior is that both ML
>  fits do not agree. This seems to be somewhat unexpected.
> 
> See case below.
> 
> Cheers,
> Henrik

   I do at least get convergence warnings (with my current
development version of lme4) for the second, suboptimal fit.
I should look at slices to see what the surface looks like
(i.e. where the suboptimal stopping point is), and how this
behaves with other optimizers.

  thanks
    Ben Bolker


From bbolker at gmail.com  Fri May  9 22:12:04 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 9 May 2014 20:12:04 +0000 (UTC)
Subject: [R-sig-ME] pedigreemm with one observation per individual
References: <60408dd8bc7b391d49fc55fd1b5ed759@webmail.zut.edu.pl>
Message-ID: <loom.20140509T220715-485@post.gmane.org>

Witold Proskura <witold.proskura at ...> writes:

> 
> 
> I would like to ask how to exactly apply that fix to lme4 and pedigreemm
> packages.


  This is a response to a very old thread
<https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q1/003341.html>,
that refers to old versions of lme4 and pedigreemm.

   I started trying to reproduce the old example from the thread, but
there have been enough changes (kinship has given way to kinship2,
the pedigreemm package has changed, etc etc) that I was having trouble.
My guess is that with a modern version of pedigreemm+lme4 you can
specify something like

control=lmerControl(check.nobs.vs.nlev="ignore",
  check.nobs.vs.nRE="ignore")

in your pedigreemm call to override this test.

  You can either check this yourself, or provide a reproducible
example ...

  Ben Bolker


> As I understand i need to call edit(lmer) and edit(pedigreem) to be able
> to fix the problem. 
> Please indicate exactly where in the lmer and pedigreemm function codes
> I need to place the solution given below. 
> 
> Here is a somewhat crude fix; it adds an additional parameter
> (checklevels) to the lmer_finalize and glmer_finalize functions in
> lme4 that turns off the unwanted sanity check. The default is
> checklevels=TRUE, which maintains the previous behavior. The
> pedigreemm function is then changed to set checklevels=FALSE before
> calling the appropriate finalizing function. I'm running these
> changes locally and they seem to work fine.
> 
> Comments or suggestions are welcome, as is consideration of this
> change or some variant on it for inclusion in the pedigreemm and lme4
> packages.
> 
> ----- in lmer.R in the lme4 package -----
> lmer_finalize <- function(fr, FL, start, REML, verbose,
> checklevels=TRUE)
> {
> ...
> ### This checks that the number of levels in a grouping factor < n
> ### Only need to check the first factor because it is the one with
> ### the most levels.
>  IF (CHECKLEVELS & !(LENGTH(LEVELS(DM$FLIST[[1]])) < LENGTH(Y)))
> ...
> 
> glmer_finalize <- function(fr, FL, glmFit, start, nAGQ, verbose,
> checklevels=TRUE)
> {
> ...
> ### This checks that the number of levels in a grouping factor < n
> ### Only need to check the first factor because it is the one with
> ### the most levels.
>  if (checklevels & !(length(levels(dm$flist[[1]])) < ncol(dm$Zt)))
> ...
> 
> ----- in pedigree.R in the pedigreemm package -----
> pedigreemm <-
>  function(formula, data, family = NULL, REML = TRUE, pedigree = list(),
> ...
>  lmf$checklevels <- FALSE;
>  ans <- do.call(if (!is.null(lmf$glmFit)) lme4:::glmer_finalize
> else lme4:::lmer_finalize, lmf)
> ...
>


From francis.77 at osu.edu  Fri May  9 02:34:48 2014
From: francis.77 at osu.edu (Francis, David)
Date: Fri, 9 May 2014 00:34:48 +0000
Subject: [R-sig-ME] lme4 question
Message-ID: <353D44B0CD8ABE478390D0C37A7E9639A469371B@CIO-KRC-D1MBX04.osuad.osu.edu>

I have an R/lme4 question that is vexing me, and I am hoping that you can provide some direction (even a hint at where I should look would be fine).

We have been using R and lme4 to extract variance components and BLUPs for quality traits from field trials of tomato.
There is a data set that I have been using for teaching purposes in my plant breeding class.  The data set is unbalanced and has some missing data such that 
there is only 1 level for one term in the model. 

When we first started using lme4, we were using R version 2.15.0

the model y  = lmer(BRIX~ (1|LINE) + (1|LOC) + (1|YEAR) + (1|REP%in%LOC:YEAR) + (1|LINE:LOC) + (1|LINE:YEAR))

was used to generate the variance components and to extract BLUPs for lines (different inbred lines, or varieties).

However with newer core packages, the model leads to an error (with the same data set as used above...)

Error in checkNlevels(reTrms$flist, n = n, control):
grouping factors must have > 1 sampled level


This error suggests that the model is over-parametrized; and if I drop the interaction REP%in%LOC:YEAR, I can again generate 
variance components and extract BLUPs.  The variance components for the remaining terms are identical to those obtained using 
R version 2.15.0.

If I reload R version 2.15.0 Plus adding the following 3 work packages to the library; lme4_0.999902344-0.zip; minqa_1.2.1.zip; Rcpp_0.9.10.zip the model works and estimates are the same as when 
we first started using the data set and R in class back in 2011.

The simple question (which probably isn't): what has changed with the core-package and or lme4 that the same data set is leading to an error in newer versions?

Thanks for any direction that you can provide.

Yours,

David

From bbolker at gmail.com  Sat May 10 20:05:40 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 10 May 2014 14:05:40 -0400
Subject: [R-sig-ME] lme4 question
In-Reply-To: <353D44B0CD8ABE478390D0C37A7E9639A469371B@CIO-KRC-D1MBX04.osuad.osu.edu>
References: <353D44B0CD8ABE478390D0C37A7E9639A469371B@CIO-KRC-D1MBX04.osuad.osu.edu>
Message-ID: <536E6A74.4080406@gmail.com>

On 14-05-08 08:34 PM, Francis, David wrote:
> I have an R/lme4 question that is vexing me, and I am hoping that you
> can provide some direction (even a hint at where I should look would
> be fine).
> 

  I thought I had answered this, but it must have slipped through the
cracks.
  If you look at the ?lmerControl manual page, you'll see the
check.nobs.vs.nlev (you may need check.nobs.vs.nRE as well, I don't
remember) which will allow you to override/bypass this check.

  The reason for this default is that, although there _are_ use cases
(such as yours) where the unidentifiability of the variance-covariance
matrix doesn't matter, people who specify these models most often (in my
experience) don't know they are doing it, and usually don't want to.

  hope that helps,
    Ben Bolker

> We have been using R and lme4 to extract variance components and
> BLUPs for quality traits from field trials of tomato. There is a data
> set that I have been using for teaching purposes in my plant breeding
> class.  The data set is unbalanced and has some missing data such
> that there is only 1 level for one term in the model.
> 
> When we first started using lme4, we were using R version 2.15.0
> 
> the model y  = lmer(BRIX~ (1|LINE) + (1|LOC) + (1|YEAR) +
> (1|REP%in%LOC:YEAR) + (1|LINE:LOC) + (1|LINE:YEAR))
> 
> was used to generate the variance components and to extract BLUPs for
> lines (different inbred lines, or varieties).
> 
> However with newer core packages, the model leads to an error (with
> the same data set as used above...)
> 
> Error in checkNlevels(reTrms$flist, n = n, control): grouping factors
> must have > 1 sampled level
> 
> 
> This error suggests that the model is over-parametrized; and if I
> drop the interaction REP%in%LOC:YEAR, I can again generate variance
> components and extract BLUPs.  The variance components for the
> remaining terms are identical to those obtained using R version
> 2.15.0.
> 
> If I reload R version 2.15.0 Plus adding the following 3 work
> packages to the library; lme4_0.999902344-0.zip; minqa_1.2.1.zip;
> Rcpp_0.9.10.zip the model works and estimates are the same as when we
> first started using the data set and R in class back in 2011.
> 
> The simple question (which probably isn't): what has changed with the
> core-package and or lme4 that the same data set is leading to an
> error in newer versions?
> 
> Thanks for any direction that you can provide.
> 
> Yours,
> 
> David _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Sat May 10 21:26:30 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 10 May 2014 19:26:30 +0000 (UTC)
Subject: [R-sig-ME] Zero alteration and mixed models?
References: <24fdcc8985e013280f6108b28b6137d1@mail.gmail.com>
Message-ID: <loom.20140510T202755-615@post.gmane.org>

Kirsty Gurney <kegurney at ...> writes:

> 
> Good afternoon;
> 
> A non-R specific question about mixed models, but I am sending this email
> with the hope that others in this group may be familiar with zero-altered
> (i.e. zero-inflated or hurdle) models in a mixed model context.

  I'll take at a stab at this with the usual caveat that the advice
is worth whatever you paid for it :-)   

> My current project is focused on understanding changes in wetland biota
> related to environmental change, and I'm using zero-altered negative
> binomial models (implemented in SAS) to evaluate changes in abundance of
> invertebrates in these habitats.  I have had good success implementing
> models that accurately reflect my data structure and that include predictor
> variables of interest, but I do have a question or two outstanding.
> 
> Specifically, I am curious about the logit (zero) part of the ZINB mixed
> models.  If parameters for this part of the model are estimated imprecisely
> and thereby uninformative, should they be removed from the model?
> 
> Prior to model construction, I plotted the proportion of zeroes in the
> dataset as a function of several variables, and these plots suggested that
> wetland class had an important influence on whether or not excess zeroes
> were observed.  However, none of the parameter estimates for the wetland
> class variable are predicted with any precision (nor is the intercept for
> this logit model) in the model that includes them.
> 
> If anyone on this list is willing / able to provide any insights or
> suggestions as to the mathematical interpretation for the inflation
> probability portion of these models, I would be most grateful.
> 
> Thank you in advance.
> 
> kbg

  I think the answer to this question depends on your general purpose
in modeling, and your general philosophy of model selection.  In other
words, the answer is similar to the question of whether you should
drop or simplify any terms that don't appear to be doing anything useful.

  If you are doing confirmatory hypothesis testing, then you definitely
shouldn't.

  If you are primarily interested in prediction, it might be
reasonable to try to do some form of model selection, which will
generally increase the bias and decrease the variance (with due
attention to the effect of model uncertainty and model selection on
the confidence intervals/uncertainty of the estimates and predictions).
What you're describing above is essentially a crude form of backward
stepwise model selection (or at least the first step ...)

  Ben Bolker


From witoldproskura at gmail.com  Sat May 10 16:25:03 2014
From: witoldproskura at gmail.com (Witold Proskura)
Date: Sat, 10 May 2014 16:25:03 +0200
Subject: [R-sig-ME] Significance of fixed effects. Kinship package (Marc
	Moragues)
Message-ID: <CALTfETEMJoahMAGCwinFci7yMoSjwT9+YoGZfsw5nOPO8U25jA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140510/6ffe0c71/attachment.pl>

From iainmstott at gmail.com  Mon May 12 12:54:50 2014
From: iainmstott at gmail.com (Iain Stott)
Date: Mon, 12 May 2014 11:54:50 +0100
Subject: [R-sig-ME] Can an uninformative prior be too diffuse?
Message-ID: <CABq_EW8M1WB_gM_Cx5VJXYW8PsGa=QEffA6m3QejqouVPzRu6A@mail.gmail.com>

Hi R-users

I'm having an interesting problem in using MCMCglmm for a meta analysis.

I run models as I would normally run them, with diffuse priors on
fixed and random effects (fixed: mu=0, V=10e8; random: V=1, nu=0.001;
gaussian model), and the posteriors I'm getting out of the models are
not like anything I've seen before. The range is very high but the
variance is very low, so that fixed effect posteriors are a spike
around the mean and random effects posteriors are highly truncated at
0. A handful of coefficient sets are taking extreme values that seem
to make no sense at all.

I wonder why this is: running for longer does not fix the problem and
the chains aren't autocorrelated. Parameter expansion does not help
the random coefficients. I'm always seeing a handful of samples that
are orders of magnitude larger than the data themselves (which are
real weighted mean differences over about -2 to 2) whilst the vast
majority are being taken from a much more sensible range.

It seems the only solution to getting better posteriors is to make
them less diffuse (decrease V for fixed effects, increase nu for
random effects). This makes sense, but I'm not comfortable doing it
when the posteriors are so sensitive to variances on the priors, and I
don't know what it would mean for interpretation of the model. I'm not
convinced that a prior can be "too" diffuse, and I'm not sure why
these extreme samples are being accepted. But then, perhaps allowing
the model to sample from a range that is so much larger than the data
just doesn't make sense... although like I say, I would have expected
these extreme values to be ditched based on likelihood.

If anyone can shed some light, it would be greatly appreciated. For
now I'll have to forego some of the random effects and forge ahead
with gls...


Iain

- - - - - - - - - - - -
Dr. Iain Stott
Environment and Sustainability Institute
University of Exeter, Cornwall Campus
Tremough, Treliever Road
Penryn, Cornwall, TR10 9FE, UK.
- - - - - - - - - - - -
http://www.exeter.ac.uk/esi/
http://biosciences.exeter.ac.uk/cec/


From orzack at freshpond.org  Mon May 12 16:11:24 2014
From: orzack at freshpond.org (orzack at freshpond.org)
Date: Mon, 12 May 2014 07:11:24 -0700
Subject: [R-sig-ME] glmer syntax for model with "common" random effects
Message-ID: <dfa661b$6949d5dc$27c53036$@freshpond.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140512/216ae8b8/attachment.pl>

From j.hadfield at ed.ac.uk  Mon May 12 17:09:31 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 12 May 2014 16:09:31 +0100
Subject: [R-sig-ME] Can an uninformative prior be too diffuse?
In-Reply-To: <CABq_EW8M1WB_gM_Cx5VJXYW8PsGa=QEffA6m3QejqouVPzRu6A@mail.gmail.com>
References: <CABq_EW8M1WB_gM_Cx5VJXYW8PsGa=QEffA6m3QejqouVPzRu6A@mail.gmail.com>
Message-ID: <20140512160931.20711qj0wedewo4g@www.staffmail.ed.ac.uk>

Hi Iain,

It's a bit hard to diagnose without seeing the data. Would it be  
possible to post them? Is it possible you have some random terms with  
very few levels?

Cheers,

Jarrod

Quoting Iain Stott <iainmstott at gmail.com> on Mon, 12 May 2014 11:54:50 +0100:

> Hi R-users
>
> I'm having an interesting problem in using MCMCglmm for a meta analysis.
>
> I run models as I would normally run them, with diffuse priors on
> fixed and random effects (fixed: mu=0, V=10e8; random: V=1, nu=0.001;
> gaussian model), and the posteriors I'm getting out of the models are
> not like anything I've seen before. The range is very high but the
> variance is very low, so that fixed effect posteriors are a spike
> around the mean and random effects posteriors are highly truncated at
> 0. A handful of coefficient sets are taking extreme values that seem
> to make no sense at all.
>
> I wonder why this is: running for longer does not fix the problem and
> the chains aren't autocorrelated. Parameter expansion does not help
> the random coefficients. I'm always seeing a handful of samples that
> are orders of magnitude larger than the data themselves (which are
> real weighted mean differences over about -2 to 2) whilst the vast
> majority are being taken from a much more sensible range.
>
> It seems the only solution to getting better posteriors is to make
> them less diffuse (decrease V for fixed effects, increase nu for
> random effects). This makes sense, but I'm not comfortable doing it
> when the posteriors are so sensitive to variances on the priors, and I
> don't know what it would mean for interpretation of the model. I'm not
> convinced that a prior can be "too" diffuse, and I'm not sure why
> these extreme samples are being accepted. But then, perhaps allowing
> the model to sample from a range that is so much larger than the data
> just doesn't make sense... although like I say, I would have expected
> these extreme values to be ditched based on likelihood.
>
> If anyone can shed some light, it would be greatly appreciated. For
> now I'll have to forego some of the random effects and forge ahead
> with gls...
>
>
> Iain
>
> - - - - - - - - - - - -
> Dr. Iain Stott
> Environment and Sustainability Institute
> University of Exeter, Cornwall Campus
> Tremough, Treliever Road
> Penryn, Cornwall, TR10 9FE, UK.
> - - - - - - - - - - - -
> http://www.exeter.ac.uk/esi/
> http://biosciences.exeter.ac.uk/cec/
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From Thierry.ONKELINX at inbo.be  Mon May 12 17:41:28 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 12 May 2014 15:41:28 +0000
Subject: [R-sig-ME] glmer syntax for model with "common" random effects
In-Reply-To: <dfa661b$6949d5dc$27c53036$@freshpond.org>
References: <dfa661b$6949d5dc$27c53036$@freshpond.org>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A4D46E@inbomail.inbo.be>

Dear John Doe,

I think you want something like this (if I understand you question correctly).

PGD_21.df$Norm_Inter <- interaction(PGD_21.df$NormXY, PGD_21.df$Norm_21)
model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 + (1|Pat_ID) + (1|PatID: Proc_ID) + (1|Pat_ID:Norm_Inter) + (1|PatID: Proc_ID:Norm_Inter), data =  PGD_21.df,family = binomial)

However that would yield uncorrelated 'common slopes'

Having correlated random slopes would require something like the code below + a mechanism to set a variance structure. That last part is not available in lme4. It is in nlme, but then you are restricted to the Gaussian distribution.

model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 + (1 + Norm_Inter |Pat_ID) + (1 + Norm_Inter |PatID: Proc_ID), data =  PGD_21.df,family = binomial)

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens orzack at freshpond.org
Verzonden: maandag 12 mei 2014 16:11
Aan: r-sig-mixed-models at r-project.org
CC: orzack at freshpond.org
Onderwerp: [R-sig-ME] glmer syntax for model with "common" random effects

 I have a quick question about syntax for a glmer call.

I am fitting models with categorial predictors (Norm_XY and Norm_21)

e.g.

model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 + (Norm_XY|Pat_ID/Proc_ID)  + (Norm_21|Pat_ID/Proc_ID), data =  PGD_21.df,family = binomial)

this will fit random effects for the intercept and "slope" associated with each predictor

Random effects:
 Groups           Name        Variance  Std.Dev. Corr
 Proc_ID.Pat_ID   (Intercept) 0.0371635 0.19278
                  Norm_21NN   0.0073152 0.08553  -1.00
 Proc_ID.Pat_ID.1 (Intercept) 0.0007932 0.02816
                  Norm_XYNN   0.0390357 0.19757  -0.89
 Pat_ID           (Intercept) 0.0055269 0.07434
                  Norm_21NN   0.0129897 0.11397  0.99
 Pat_ID.1         (Intercept) 0.0781525 0.27956
                  Norm_XYNN   0.0579198 0.24067  -1.00

What I can't figure (and have failed to find an example of) is the syntax for "common" estimates of the random effects (i.e., for both categorical predictors).

The following syntax will generate a common estimate of the random effects for the intercept

model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 + ((Norm_XY + Norm_21)|Pat_ID/Proc_ID), data =  PGD_21.df,family = binomial))

the estimates are

Random effects:
 Groups         Name        Variance Std.Dev. Corr
 Proc_ID:Pat_ID (Intercept) 0.010129 0.10064
                Norm_XYNN   0.035686 0.18891   0.99
                Norm_21NN   0.009532 0.09763  -0.99 -1.00
 Pat_ID         (Intercept) 0.101927 0.31926
                Norm_XYNN   0.070295 0.26513  -0.98
                Norm_21NN   0.019164 0.13843   0.11  0.08

So how does one specify a model that estimates the random effects for the "common" or joint categorical predictor of the "slope"  (i.e. for Norm_XY and Norm_21 combined)?

This model would produce estimates like this

Random effects:
 Groups         Name        Variance Std.Dev. Corr
 Proc_ID:Pat_ID (Intercept) XXX
                           Common "slope" ZZZ     QQQ
 Pat_ID               (Intercept) YYY
                           Common "slope" WWW  RRR

In these applications, the random effects are nuisance parameters, so I don't necessarily need predictor-specific estimates. That said, I do want to account for the random effects and their influence on the estimates of the fixed effects (otherwise I would just use GLM)

The first model above penalizes AIC by 24 (just considering random effects) and the last model would penalize it by 12. this is a big difference.

MANY THANKS!



        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From xav.harrison at gmail.com  Mon May 12 20:18:11 2014
From: xav.harrison at gmail.com (Xavier Harrison)
Date: Mon, 12 May 2014 19:18:11 +0100
Subject: [R-sig-ME] Diagnosing Overdispersion in Mixed Models with
	Parametric Bootstrapping
Message-ID: <97B23DEE-F825-4597-BC4B-32611ABBB822@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140512/1e640b67/attachment.pl>

From bbolker at gmail.com  Tue May 13 03:37:15 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 13 May 2014 01:37:15 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?Diagnosing_Overdispersion_in_Mixed_Models_wi?=
	=?utf-8?q?th=09Parametric_Bootstrapping?=
References: <97B23DEE-F825-4597-BC4B-32611ABBB822@gmail.com>
Message-ID: <loom.20140513T032323-815@post.gmane.org>

Xavier Harrison <xav.harrison at ...> writes:

> 
> Hi All
 
> I have a question regarding the appropriateness of diagnosing
> overdispersion in mixed models using parametric bootstrap as
> provided in the lme4 package.
 
> Following Zuur et al (2009), Bolker et al (2009), and nicely
> summarised on the glmm wiki, one can approximate overdispersion in a
> model as the ratio of sum of squared residuals to residual degrees
> of freedom. However this suffers the problem of not knowing exactly
> how many residual degrees of freedom there are in a mixed model.

  Yes, although keeping in mind that the whole thing is an approximation
anyway, getting the residual df wrong by a few percent is hopefully
not that important
 
> My question is, given the flexibility of the parametric
> bootstrapping function in lme4, could one not calculate
> overdispersion by looking at the ratio of the model SS residuals to
> the mean SS residuals from n parametric bootstrap samples? Does it
> follow that if the data are well approximated by a Poisson process,
> this ratio should be roughly 1, whereas if there is extra
> variability in the data than a Poisson can predict', the ratio
> should be a fair bit higher?

  This seems quite reasonable.
 
> Below I have provided code to reproduce an example along the lines
> of what I have just tried to describe. I have simulated two sets of
> data where a Poisson outcome (number of offspring) is affected by
> body size (with positive slope). In both cases the data are
> generated from Poisson process, but in the latter case I add some
> random noise to the linear predictor drawn from a normal
> distribution with mean 0 and known SD. The code below models the
> effect of body size on offspring, performs parametric bootstrapping,
> and then calculates the ratio of observed model SS residuals to mean
> parametrically bootstrapped SS residuals.  As expected, the "over
> dispersed" data yield a SS ratio of much greater than 1.

  This particular model seems to be more or less exactly equivalent
to using an observation-level random effect -- see below ...
 
> If this approach seems valid, it seems it would be simple to use sim
> to sim variability to put some CIs on this point estimate.
 
> Any help greatly appreciated

  The parametric bootstrap is worthwhile for finding reliable
confidence intervals, but otherwise a parametric approach (e.g.
likelihood

  I would do this as follows (slightly more compactly):

n.pops <- 10
n.indiv <- 50

set.seed(101)
popid <- gl(n.pops,n.indiv)
bodysize <- rnorm(length(popid),30,4)
d <- data.frame(popid,bodysize,obs=factor(1:(n.pops*n.indiv)))
library(lme4)
d$y <- simulate(~bodysize+(1|popid)+(1|obs),family="poisson",
         newdata=d,newparams=list(theta=c(0.5,0.8),
                                  beta=c(-0.5,0.05)))[[1]]

f <- glmer(y~bodysize+(1|popid)+(1|obs),family="poisson",
           data=d)
p <- profile(f,which="theta_")
confint(p)

Random effects:
 Groups Name        Std.Dev.
 obs    (Intercept) 0.4795  
 popid  (Intercept) 0.7990  
Number of obs: 500, groups:  obs, 500 popid, 10
Fixed Effects:
(Intercept)     bodysize  
   -1.05462      0.06281  

The intercept looks a bit low, but the std. error is also large
(0.37)

confidence intervals:
    
           2.5 %    97.5 %
.sig01 0.4076925 0.5568827
.sig02 0.5328584 1.3516996


From bbolker at gmail.com  Tue May 13 03:52:03 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 13 May 2014 01:52:03 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?Significance_of_fixed_effects=2E_Kinship_pac?=
	=?utf-8?q?kage_=28Marc=09Moragues=29?=
References: <CALTfETEMJoahMAGCwinFci7yMoSjwT9+YoGZfsw5nOPO8U25jA@mail.gmail.com>
Message-ID: <loom.20140513T033802-524@post.gmane.org>

Witold Proskura <witoldproskura at ...> writes:

> 
> Dear R Support Team,

  (Sadly there is no "R Support Team" -- just the people
who read the mailing list.)

> I have tried to compare the log-likelihoods of the two models, with and
> without the fixed effect which I want to test significance for. 
 
 [snip]

> Moreover please indicate some method for multiple comparisons of groups
> within particular fixed effect. When I tried to obtain df value for the
> model, the result was always NULL.
> The last question is how to extract variation components and to calculate
> heritability?

  We really need a reproducible example before we can help.
  What package is this from?  kinship (which is now obsolete),
or coxme?

  My only thought is that you have 'loglik' component that you
can probably extract via m1$loglik, and you can do something like

pchisq(-2*(diff. in log-likelihoods),df=(diff. in number of coefficients),
       lower.tail=FALSE)

to implement your own likelihood ratio test.


> My model:
> m1=lmekin(kgm~s1+rs+w+d+(1|id), data=milk,varlist=list(kmat))
> ,where:
> s1 and rs are categorical factors
> w and d are continuous variables
> 
> > summary(m1)
>              Length Class  Mode
> coefficients   2    -none- list
> var          576    -none- numeric
> vcoef          1    -none- list
> residuals    424    -none- numeric
> method         1    -none- character
> loglik         1    -none- numeric


 [snip]

> Linear mixed-effects kinship model fit by maximum likelihood
>   Data: milk
>   Log-likelihood = -3604.361
>   n= 424
> 
> Model:  kgm ~ s1 + rs + w + d + (1 | id)
> Fixed coefficients
>                    Value    Std Error     z       p
> (Intercept) -7920.701534 3023.8600619 -2.62 8.8e-03
> s1CT           76.794638  170.4547258  0.45 6.5e-01
> d              41.339062    9.9779674  4.14 3.4e-05
>

  [SNIP]

> Random effects
>  Group Variable Std Dev     Variance
>  id    Vmat.1      1210.785 1465999.783
> Residual error= 864.8264


From xav.harrison at gmail.com  Tue May 13 10:04:07 2014
From: xav.harrison at gmail.com (Xavier Harrison)
Date: Tue, 13 May 2014 09:04:07 +0100
Subject: [R-sig-ME] Diagnosing Overdispersion in Mixed Models with
	Parametric Bootstrapping
Message-ID: <7C6D133B-7760-42F8-9674-248FF2BE9202@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140513/1c9d397e/attachment.pl>

From ABaggett at umhb.edu  Tue May 13 18:04:00 2014
From: ABaggett at umhb.edu (Baggett, Aaron)
Date: Tue, 13 May 2014 16:04:00 +0000
Subject: [R-sig-ME] Assumptions of GLMMs
Message-ID: <304CAFFA-3BBF-47C7-8F13-3D429F7D92C4@umhb.edu>

Hi all:

Can anyone direct me to a few resources which outline the fundamental assumptions of GLMMs with a dichotomous outcome?

Thanks,

Aaron Baggett
Dept. of Psychology
University of Mary Hardin-Baylor
(254) 295-4553
--------------------------------------
Sent via mobile device

From carlosambarboza at gmail.com  Tue May 13 20:11:33 2014
From: carlosambarboza at gmail.com (Carlos Barboza)
Date: Tue, 13 May 2014 15:11:33 -0300
Subject: [R-sig-ME] Doubts about models in glmmADMB
Message-ID: <CAGAvxRkS9yJoubfV3JaWj-O3pwGhUq7dNWzhSupS7xoqsRR3ZQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140513/826eb27c/attachment.pl>

From bbolker at gmail.com  Tue May 13 20:50:38 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 13 May 2014 14:50:38 -0400
Subject: [R-sig-ME] Assumptions of GLMMs
In-Reply-To: <304CAFFA-3BBF-47C7-8F13-3D429F7D92C4@umhb.edu>
References: <304CAFFA-3BBF-47C7-8F13-3D429F7D92C4@umhb.edu>
Message-ID: <5372697E.2050108@gmail.com>

On 14-05-13 12:04 PM, Baggett, Aaron wrote:
> Hi all:
> 
> Can anyone direct me to a few resources which outline the fundamental
> assumptions of GLMMs with a dichotomous outcome?
> 
> Thanks,
> 
> Aaron Baggett Dept. of Psychology University of Mary Hardin-Baylor 
> (254) 295-4553 -------------------------------------- Sent via mobile
> device _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf (p 119)
http://ms.mcmaster.ca/~bolker/bbpapers/Bolker+2009-glmm.pdf (username
'bbpapers', password 'research')


From gf3 at pdx.edu  Wed May 14 00:12:51 2014
From: gf3 at pdx.edu (Gerasimos Fergadiotis)
Date: Tue, 13 May 2014 15:12:51 -0700
Subject: [R-sig-ME]  Power Analysis for Linear Mixed Model with Covariates
Message-ID: <B5F0E9F7-3279-4DE1-871F-DF1E2FBA3394@pdx.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140513/1a544659/attachment.pl>

From ukoether at uke.de  Wed May 14 00:41:24 2014
From: ukoether at uke.de (=?ISO-8859-1?Q?Ulf_K=F6ther?=)
Date: Wed, 14 May 2014 00:41:24 +0200
Subject: [R-sig-ME] Power Analysis for Linear Mixed Model with Covariates
In-Reply-To: <B5F0E9F7-3279-4DE1-871F-DF1E2FBA3394@pdx.edu>
References: <B5F0E9F7-3279-4DE1-871F-DF1E2FBA3394@pdx.edu>
Message-ID: <53729F94.4040005@uke.de>

Dear Gerasimos,

I am really no expert, but I would guess, you get no slope because you
implemented time as a factor variable, therefore lmer is giving you
correlated random intercepts, one for each factor-level...
Just try to add another variable for time:

expdat <- expand.grid(kid = factor(1:500), Time = factor(1:4), Treat = c("XTx", "BAU"))
expdat$obs <- factor(seq(nrow(expdat)))
expdat$time.month <- rep(c(0,3,6,9), each = 500)

...

fit1 <- lmer(Outcome ~ Treat*Time + (1 + time.month | kid), data = expdat)
 

This should give you what you want, I think...

Ulf


Am 14.05.2014 00:12, schrieb Gerasimos Fergadiotis:
> I would like to conduct a power analysis for a linear mixed model with fixed effects for Treatment(two levels) and Time (four time points: pre, mid, post treatment, 3 months post treatment; each three months apart) and correlated random effects for children (intercepts and slopes). I am using the following code in R using the lmer function:
>
> expdat <- expand.grid(kid = factor(1:500), Time = factor(1:4), Treat = c("XTx", "BAU"))
> expdat$obs <- factor(seq(nrow(expdat)))
> set.seed(101)
> nsim <- 20
> beta <- c(100, -7, 8, 15, 20, 0, 0, 0)
> theta <- c(15.000000, 7.500000, 7.500000, 7.500000, 12.990381, 4.330127, 4.330127, 
>            12.247449, 3.061862, 11.858541)
> ss <- simulate(~Treat*Time + (1+Time | kid), nsim = nsim, family = gaussian, 
>       weights = rep(25, nrow(expdat)), newdata = expdat, newparams = 
>       list(theta = theta, beta = beta, sigma = 1))
> expdat$Outcome <- ss[, 1]
> fit1 <- lmer(Outcome ~ Treat*Time + (1+Time | kid), data = expdat)
> I have the following questions:
>
> In the output I see the variance associated with the random intercepts at each time point and their correlation. However, I cannot find in the output the variance associated with the slopes, and I cannot find information about the correction between the intercepts and the slopes. Where is that information?
> How does the lmer know that Time has levels and that it should estimate a unique effect for each level of the Time factor instead of a general slope parameter?
> I am attaching the summary statement of the model:
> Linear mixed model fit by REML ['lmerMod']
> Formula: Outcome ~ Treat * Time + (1 + Time | kid)
> Data: expdat
>
> REML criterion at convergence: 22951.8
>
> Scaled residuals: 
> Min       1Q   Median       3Q      Max 
> -2.55473 -0.47186 -0.00007  0.47268  2.57786 
>
> Random effects:
> Groups   Name        Variance Std.Dev. Corr          
>  kid      (Intercept) 235.4531 15.3445                
>           Time2       230.9249 15.1962  0.43          
>           Time3       220.6076 14.8529  0.52 0.53     
>           Time4       213.2725 14.6039  0.48 0.51 0.52
>  Residual               0.9749  0.9874                
> Number of obs: 4000, groups: kid, 500
>
> Fixed effects:
>                 Estimate Std. Error t value
> (Intercept)    100.92006    0.68765  146.76
> TreatBAU        -6.94796    0.06245 -111.26
> Time2            7.39371    0.68246   10.83
> Time3           15.17849    0.66717   22.75
> Time4           19.27623    0.65608   29.38
> TreatBAU:Time2  -0.16052    0.08831   -1.82
> TreatBAU:Time3  -0.05291    0.08831   -0.60
> TreatBAU:Time4  -0.14215    0.08831   -1.61
>
> Correlation of Fixed Effects:
>             (Intr) TrtBAU Time2  Time3  Time4  TBAU:T2 TBAU:T3
> TreatBAU    -0.045                                            
> Time2        0.422  0.046                                     
> Time3        0.510  0.047  0.528                              
> Time4        0.468  0.048  0.506  0.520                       
> TretBAU:Tm2  0.032 -0.707 -0.065 -0.033 -0.034                
> TretBAU:Tm3  0.032 -0.707 -0.032 -0.066 -0.034  0.500         
> TretBAU:Tm4  0.032 -0.707 -0.032 -0.033 -0.067  0.500   0.500
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Gerasimos Fergadiotis, PhD., CCC-SLP
> Assistant Professor
> Speech & Hearing Sciences
> 724 SW Harrison St., RM 84B
> Portland State University
> Portland, Oregon 97201
> Phone: 503.725.2217
> Fax: 503.725.9171
> Web page: http://aaldresearch.wix.com/aald
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

-- 
________________________________________

Dipl.-Psych. Ulf K?ther

PEPP-Team 
Klinik f?r Psychiatrie und Psychotherapie
Universit?tsklinikum Hamburg-Eppendorf
Martinistr. 52
20246 Hamburg

PEPP-Team:
Tel.: +49 (0) 40 7410 53243
pepp at uke.de

Pers?nlich:
Tel.: +49 (0) 40 7410 55851
Mobil: (9) 55851
ukoether at uke.de
________________________________________

--

DANKE F?R 125 JAHRE ENGAGEMENT UND VERTRAUEN.
www.uke.de/125
_____________________________________________________________________

Besuchen Sie uns auf: www.uke.de
_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg
Vorstandsmitglieder: Prof. Dr. Christian Gerloff (Vertreter des Vorsitzenden), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Rainer Schoppik
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From thomas.merkling at univ-tlse3.fr  Wed May 14 02:54:17 2014
From: thomas.merkling at univ-tlse3.fr (Thomas Merkling)
Date: Wed, 14 May 2014 10:54:17 +1000
Subject: [R-sig-ME] accounting for overdispersion in binary GLMM in lme4
Message-ID: <5372BEB9.2010803@univ-tlse3.fr>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
URL : <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140514/0be53bc5/attachment.pl>

From bbolker at gmail.com  Wed May 14 03:32:31 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 13 May 2014 21:32:31 -0400
Subject: [R-sig-ME] Doubts about models in glmmADMB
In-Reply-To: <CAGAvxRkS9yJoubfV3JaWj-O3pwGhUq7dNWzhSupS7xoqsRR3ZQ@mail.gmail.com>
References: <CAGAvxRkS9yJoubfV3JaWj-O3pwGhUq7dNWzhSupS7xoqsRR3ZQ@mail.gmail.com>
Message-ID: <5372C7AF.4080909@gmail.com>

On 14-05-13 02:11 PM, Carlos Barboza wrote:
>> I'm a phd student from The Federal University of Rio de Janeiro
>> Brazil in Marine Biology. First it's a pleasure to to send you a message.
> The reason
>> for it is that I'm using glmmADMB package for run my analyses and I
>> have some doubts. In a simple way I have the following model:
>> sectors a fixed factor; sites (nested in sectors, random), points
>> (nested in sites, random). So:
>>
>> teste1<- glmmadmb(total ~ sector + (1 | sector/site/point),...

This doesn't make sense to me.  In general, categorical predictors
should appear in fixed effects or as grouping variables, but not both ...

>>
>> # why  did the result gave me the effect of the factor sector alone,
>> also in the random effects...? since the random effect included
>> only the nested factors and must include only the effects of
>> sector/site and sector/site/point
>>
>> # but if I try:
>>
>> teste1<- glmmadmb(total ~ set + (1 | site/point),...
>>
>> # I get that result, but I think that with this path, site/point
>> were not nested within sector

  It depends whether site is implicitly or explicitly nested; if sites
have unique labels (implicit nesting), then this should be fine (see
http://glmm.wikidot.com/faq for more discussion).  I have done this on
occasion when the top level of the hierarchy has insufficient
replication to treat as a random effect (see e.g. the 'herbivory'
example at http://glmm.wikidot.com/examples).

>>
>> # in another ocasion it is correct to code only random effects like:
>> model<- glmmadmb(log(total+1) ~ 1, random= ~ 1 | set/loc/pon,
> family=......

  This would be OK

  I'm not entirely sure what your specific question is, though ...

>


From bbolker at gmail.com  Wed May 14 04:30:33 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 13 May 2014 22:30:33 -0400
Subject: [R-sig-ME] accounting for overdispersion in binary GLMM in lme4
In-Reply-To: <5372BEB9.2010803@univ-tlse3.fr>
References: <5372BEB9.2010803@univ-tlse3.fr>
Message-ID: <5372D549.9060702@gmail.com>

On 14-05-13 08:54 PM, Thomas Merkling wrote:
> Hi all,
> 
> I have a large dataset (n= 893) of chick sex (0 or 1) and I am looking 
> at which variables can influence the probability of producing a male 
> versus a female.
> I had to put random effects for year (as I have data for different 
> years), fatherID and motherID (as I have many chicks for a same 
> individual and mother and father were not always with the same partner). 
> Surprisingly,  variance was zero (or almost depending on the models) for 
> all 3 random effects, but as I have seen that it can happen, so I kept 
> going.
> 
> A model looked like that:
> mod = glmer (CodeSex ~ X *Y + Z + (1|Year) + (1|FatherID) + 
> (1|MotherID), family="binomial", data = sexratio)
> 
> Using the dispersion_glmer function from the GLMM wiki website I 
> estimated overdispersion. The ratio was around 1.4, which is not very 
> high (but p-value highly significant, due to the large dataset?).
> I saw a lot of model alternatives when using count data, but the only 
> one I found for binary data seem to be adding an observation-level 
> random effect.

  Overdispersion is unidentifiable for binary data (unless there are
multiple observations with identical sets of predictors, in which case
the data can be aggregated into binomial data with N>1), so all of this
is more or less irrelevant/unnecessary ...

  Ben Bolker


From henrik.singmann at psychologie.uni-freiburg.de  Wed May 14 12:16:48 2014
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Wed, 14 May 2014 12:16:48 +0200
Subject: [R-sig-ME] Assumptions of GLMMs
In-Reply-To: <5372697E.2050108@gmail.com>
References: <304CAFFA-3BBF-47C7-8F13-3D429F7D92C4@umhb.edu>
	<5372697E.2050108@gmail.com>
Message-ID: <53734290.7030806@psychologie.uni-freiburg.de>

As you are from psychology, the following two migth also be of interest (and contain further references):

Jaeger, T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language, 59(4), 434?446. doi:10.1016/j.jml.2007.11.007

Dixon, P. (2008). Models of accuracy in repeated-measures designs. Journal of Memory and Language, 59(4), 447?456. doi:10.1016/j.jml.2007.11.004

Cheers,
Henrik

Am 13.05.2014 20:50, schrieb Ben Bolker:
> On 14-05-13 12:04 PM, Baggett, Aaron wrote:
>> Hi all:
>>
>> Can anyone direct me to a few resources which outline the fundamental
>> assumptions of GLMMs with a dichotomous outcome?
>>
>> Thanks,
>>
>> Aaron Baggett Dept. of Psychology University of Mary Hardin-Baylor
>> (254) 295-4553 -------------------------------------- Sent via mobile
>> device _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf (p 119)
> http://ms.mcmaster.ca/~bolker/bbpapers/Bolker+2009-glmm.pdf (username
> 'bbpapers', password 'research')
>

-- 
Dipl. Psych. Henrik Singmann
PhD Student
Albert-Ludwigs-Universit?t Freiburg, Germany
http://www.psychologie.uni-freiburg.de/Members/singmann


From hmcampbe at ncsu.edu  Wed May 14 15:45:02 2014
From: hmcampbe at ncsu.edu (Heather Moylett)
Date: Wed, 14 May 2014 09:45:02 -0400
Subject: [R-sig-ME] ANOVA type lll ss table for GLMER?
Message-ID: <CAPEhjRBa3NWUm9qzgcBSw0Xee1TwhsNzKsmCkhS_UiZfWoOLqg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140514/fafa421f/attachment.pl>

From henrik.singmann at psychologie.uni-freiburg.de  Wed May 14 16:02:11 2014
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Wed, 14 May 2014 16:02:11 +0200
Subject: [R-sig-ME] ANOVA type lll ss table for GLMER?
In-Reply-To: <CAPEhjRBa3NWUm9qzgcBSw0Xee1TwhsNzKsmCkhS_UiZfWoOLqg@mail.gmail.com>
References: <CAPEhjRBa3NWUm9qzgcBSw0Xee1TwhsNzKsmCkhS_UiZfWoOLqg@mail.gmail.com>
Message-ID: <53737763.40808@psychologie.uni-freiburg.de>

Dear Heather,

you could try to use mixed from the afex package which will give you Type III p-values for the effects via Chi^2 tests (or alternatively via parametric bootstrap):

require(afex)
(spden2 <- mixed(SpDens~(treat*samp)-1+(1|TRANSECT),family=poisson, data=rm, nAGQ = 9, method = "LRT")

Note however, that loading afex changes your overall contrasts, to reset the default contrasts use:
options(contrasts=c('contr.treatment', 'contr.poly'))

Furthermore, (g)lmer doesn't break the factors done by *all* levels. It removes the first levels (usually). Hence the parameters cannot directly be interpreted if this level is "significant".

Hope this helps,
Henrik

Am 14.05.2014 15:45, schrieb Heather Moylett:
>   Hello group,
>
> This is my first time posting, so I hope I have explained my needs clearly
> below.
>
> I am running a repeated measure analysis with a raw species count data set
> (SpDens). I have run different model types (zeroinfl, glm, glmer) and have
> identified glmer to have the best fit. The output generated by GLMER breaks
> my between groups (treat) and within groups (samp) factors down by levels.
> In addition to this, I would like to look at the effect of treat and samp
> overall, something similar to an ANOVA table (Type lll SS). When I use
> Anova(object) I receive an ANOVA table with an F val and no P-vals. I would
> prefer to stick with the z-stat and p-vals. I have seen this reported in
> other papers, so I know it can be done...just can't figure out how to do it!
>
> Components of the model:
> samp: 23 sampling dates is the repeated measure (within groups)
> treat: 4 levels (between groups)
> TRANSECT: experimental unit (subject), 4/treat and data collected from all
> 16 every sampling date
>
> When I run this code:
>
> RM <- read.csv("C:/Users/heatbell/Desktop/Walthour-Moss/STATS/CH
> 1/Final/R/RM.csv")
>
> View(RM)
>
> rm <- subset(RM, SAMPLE >= 2)
>
>
>
> rm<- within(rm, {
>
>    samp<-factor(SAMPLE)
>
>    yr<-factor(YEAR)
>
>    treat<-factor(TREAT)
>
> })
>
> summary(rm)
>
>
> summary(spden<-glmer(SpDens~(treat*samp)-1+(1|TRANSECT),family=poisson,
> data=rm, nAGQ = 9))
>
> Thank you for the help!
> Heather
>

-- 
Dr. Henrik Singmann
Albert-Ludwigs-Universit?t Freiburg, Germany
http://www.psychologie.uni-freiburg.de/Members/singmann


From carlosambarboza at gmail.com  Wed May 14 14:56:47 2014
From: carlosambarboza at gmail.com (Carlos Barboza)
Date: Wed, 14 May 2014 09:56:47 -0300
Subject: [R-sig-ME] Doubts about models in glmmADMB
In-Reply-To: <CAGAvxRkS9yJoubfV3JaWj-O3pwGhUq7dNWzhSupS7xoqsRR3ZQ@mail.gmail.com>
References: <CAGAvxRkS9yJoubfV3JaWj-O3pwGhUq7dNWzhSupS7xoqsRR3ZQ@mail.gmail.com>
Message-ID: <CAGAvxRmwv8_y5994s4DFTkz3iqCOso-_FR1jP3Rve1wigbpJiQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140514/3c60dbc7/attachment.pl>

From iainmstott at gmail.com  Wed May 14 13:37:11 2014
From: iainmstott at gmail.com (Iain Stott)
Date: Wed, 14 May 2014 12:37:11 +0100
Subject: [R-sig-ME] Can an uninformative prior be too diffuse?
In-Reply-To: <20140512160931.20711qj0wedewo4g@www.staffmail.ed.ac.uk>
References: <CABq_EW8M1WB_gM_Cx5VJXYW8PsGa=QEffA6m3QejqouVPzRu6A@mail.gmail.com>
	<20140512160931.20711qj0wedewo4g@www.staffmail.ed.ac.uk>
Message-ID: <CABq_EW_g2XPfyUjgomRYFxjpif3jbHOFUP1GW4apKchDMR8qvA@mail.gmail.com>

Jarrod, you hit the nail on the head. I had a random term with only a
couple of levels (taxonomic group) which I was planning to replace
with a phylogeny anyway. As soon as I took out that variable from the
models, they're running brilliantly.

Thanks for your help, and to others that emailed me with similar advice!

Iain

- - - - - - - - - - - -
Dr. Iain Stott
Environment and Sustainability Institute
University of Exeter, Cornwall Campus
Tremough, Treliever Road
Penryn, Cornwall, TR10 9EZ, UK.
- - - - - - - - - - - -
http://www.exeter.ac.uk/esi/
http://biosciences.exeter.ac.uk/cec/



On 12 May 2014 16:09, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:
> Hi Iain,
>
> It's a bit hard to diagnose without seeing the data. Would it be possible to
> post them? Is it possible you have some random terms with very few levels?
>
> Cheers,
>
> Jarrod
>
>
> Quoting Iain Stott <iainmstott at gmail.com> on Mon, 12 May 2014 11:54:50
> +0100:
>
>> Hi R-users
>>
>> I'm having an interesting problem in using MCMCglmm for a meta analysis.
>>
>> I run models as I would normally run them, with diffuse priors on
>> fixed and random effects (fixed: mu=0, V=10e8; random: V=1, nu=0.001;
>> gaussian model), and the posteriors I'm getting out of the models are
>> not like anything I've seen before. The range is very high but the
>> variance is very low, so that fixed effect posteriors are a spike
>> around the mean and random effects posteriors are highly truncated at
>> 0. A handful of coefficient sets are taking extreme values that seem
>> to make no sense at all.
>>
>> I wonder why this is: running for longer does not fix the problem and
>> the chains aren't autocorrelated. Parameter expansion does not help
>> the random coefficients. I'm always seeing a handful of samples that
>> are orders of magnitude larger than the data themselves (which are
>> real weighted mean differences over about -2 to 2) whilst the vast
>> majority are being taken from a much more sensible range.
>>
>> It seems the only solution to getting better posteriors is to make
>> them less diffuse (decrease V for fixed effects, increase nu for
>> random effects). This makes sense, but I'm not comfortable doing it
>> when the posteriors are so sensitive to variances on the priors, and I
>> don't know what it would mean for interpretation of the model. I'm not
>> convinced that a prior can be "too" diffuse, and I'm not sure why
>> these extreme samples are being accepted. But then, perhaps allowing
>> the model to sample from a range that is so much larger than the data
>> just doesn't make sense... although like I say, I would have expected
>> these extreme values to be ditched based on likelihood.
>>
>> If anyone can shed some light, it would be greatly appreciated. For
>> now I'll have to forego some of the random effects and forge ahead
>> with gls...
>>
>>
>> Iain
>>
>> - - - - - - - - - - - -
>> Dr. Iain Stott
>> Environment and Sustainability Institute
>> University of Exeter, Cornwall Campus
>> Tremough, Treliever Road
>> Penryn, Cornwall, TR10 9FE, UK.
>> - - - - - - - - - - - -
>> http://www.exeter.ac.uk/esi/
>> http://biosciences.exeter.ac.uk/cec/
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>


From uriel.gelin at usherbrooke.ca  Wed May 14 16:13:43 2014
From: uriel.gelin at usherbrooke.ca (UG)
Date: Wed, 14 May 2014 14:13:43 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?Wrong_convergence_warnings_with_glmer_in_lme?=
	=?utf-8?b?NCB2ZXJzaW9uCTEuMS02Pz8=?=
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>
Message-ID: <loom.20140514T161323-465@post.gmane.org>

Tom Davis <tomd792 at ...> writes:

> 
> Dear lme4 experts,
> 
> Yesterday, I ran the code for two published papers (de Boeck et al.,2011;
> de Boeck and Partchev, 2012) on psychometric modeling with glmer in lme4
> version 1.1-6 and the vast majority of the models I ran produce convergence
> warnings (even the simple ones).
> 
> For instance, the basic Rasch model in de Boeck et al. (2011) yields:
> 
> > ## our example data set is bundled with lme4
> > data("VerbAgg")
> >
> > ## A first example: A Rasch model with fixed item effects and random
> person effects
> > m1 = glmer(r2 ~ 0 + item + (1|id), data=VerbAgg, family="binomial",
> + control=glmerControl(optCtrl=list(maxfun=100000)))
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.308607 (tol = 0.001)
> 
> I am a bit puzzeled because, to my knowledge, especially the models for the
> VerAgg data (included in lme4) have been checked in many other programs
> (also ltm in R) and I heard that glmer produces results that are valid and
> consistent with SAS, HLM, ltm, and so on. However, this dataset produces
> convergence warnings even though the models are comparatively simple for
> psychometric research (basic Rasch and LLTM) and the estimates all seem
> reasonable.
> 
> I also tried some datasets in the ltm package. Again, convergence warnings
> (the mobility data). The estimates are close to what ltm gives me.
> 
> Even when I simulate data from a basic Rasch model using the rmvlogis
> function in ltm with a couple of extreme item parameters (which occur in
> psychometric tests), I also get these warnings. This happens despite glmer
> seems to recover the true values very well. The "gradient" errors and the
> hessian errors tend to increase with sample size and when I use
> binomial("probit") instead of binomial("logit"). It also seems like vector
> random effects produce many warnings. optimizer="bobqya" tends to reduce
> the warnings but not consistently. To me, it seems like the warnings occur
> randomly all over the place. Sometimes glmer "converges" (no warnings) with
> one optimizer setting and the values that are very close to the true
> values. However, with another optimizer setting, one gets practically
> exactly the same estimates and virtually the same likelihood value but a
> warning. I really do not understand what is going on.
> 
> I had no warnings using version 1.0-5 and version 1.0-6 so this seems to be
> a recent problem of lme4?
> 
> Is it best to ignore all these convergence warnings for now? Should I
> switch back to an older version of lme4 to avoid this problem? Should I
> generally avoid using large datasets with lme4?
> 
> Many thanks in advance,
> Tom
> 
> Papers (scripts are online):
> De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A., Tuerlinckx,
> F., & Partchev, I. (2011). The estimation of item response models with the
> lmer function from the lme4 package in R. Journal of Statistical Software,
> 39, 1-28.  http://www.jstatsoft.org/v39/i12
> 
> De Boeck, P., & Partchev, I. (2012). IRTrees: Tree-based item response
> models of the GLMM family. Journal of Statistical Software, 48, 1-28.
> http://www.jstatsoft.org/v48/c01/
> 
> Simulated data:
> 
> > library("reshape")
> > library("ltm")
> > library("lme4")
> > set.seed("12345")
> >
> > simrasch<-data.frame(rmvlogis(200, cbind(seq(-5, 5, 0.5), 1)))
> > rasch(simrasch,IRT.param=F)
> 
> Call:
> rasch(data = simrasch, IRT.param = F)
> 
> Coefficients:
>     X1      X2      X3      X4      X5      X6      X7      X8      X9
> X10     X11     X12
> 17.342   5.157   5.157   3.441   3.141   2.100   2.193   1.969   1.484
> 0.717   0.054  -0.347
>    X13     X14     X15     X16     X17     X18     X19     X20
> X21       z
> -0.874  -1.415  -2.103  -2.696  -3.065  -3.152  -4.453  -4.216  -5.175
> 1.091
> 
> Log.Lik: -1329.186
> 
> >
> > simrasch$person = rownames(simrasch)
> > simraschlong = melt(simrasch, id = "person")
> > glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
>  2702.484  2842.026 -1329.242  2658.484      4178
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 1.091
> Number of obs: 4200, groups:  person, 200
> Fixed Effects:
>  variableX1   variableX2   variableX3   variableX4   variableX5
> variableX6   variableX7
>    19.04000      5.14239      5.13514      3.43274      3.13736
> 2.10085      2.19335
>  variableX8   variableX9  variableX10  variableX11  variableX12
> variableX13  variableX14
>     1.97086      1.48698      0.71828      0.05206     -0.35255
> -0.88099     -1.42318
> variableX15  variableX16  variableX17  variableX18  variableX19
> variableX20  variableX21
>    -2.11070     -2.70069     -3.06758     -3.15545     -4.44743
> -4.21083     -5.16615
> Warning messages:
> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.135068 (tol = 0.001)
> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
> > glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial,
> + control=glmerControl(optimizer="bobyqa"))
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
>  2702.482  2842.025 -1329.241  2658.482      4178
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 1.09
> Number of obs: 4200, groups:  person, 200
> Fixed Effects:
>  variableX1   variableX2   variableX3   variableX4   variableX5
> variableX6   variableX7
>    17.75458      5.14416      5.14417      3.43797      3.13991
> 2.10444      2.19676
>  variableX8   variableX9  variableX10  variableX11  variableX12
> variableX13  variableX14
>     1.97412      1.48907      0.72049      0.05389     -0.34882
> -0.87841     -1.42030
> variableX15  variableX16  variableX17  variableX18  variableX19
> variableX20  variableX21
>    -2.10751     -2.69767     -3.06471     -3.15126     -4.44429
> -4.20819     -5.16356
> 
> > simrasch<-data.frame(rmvlogis(1000, cbind(seq(-5, 5, 0.5), 1)))
> > rasch(simrasch,IRT.param=F)
> 
> Call:
> rasch(data = simrasch, IRT.param = F)
> 
> Coefficients:
>     X1      X2      X3      X4      X5      X6      X7      X8      X9
> X10     X11     X12
>  5.195   4.422   3.868   3.599   3.192   2.558   2.211   1.605   0.984
> 0.559   0.003  -0.398
>    X13     X14     X15     X16     X17     X18     X19     X20
> X21       z
> -0.985  -1.590  -1.994  -2.401  -2.876  -3.411  -3.815  -4.683  -4.832
> 1.014
> 
> Log.Lik: -6874.631
> 
> >
> > simrasch$person = rownames(simrasch)
> > simraschlong = melt(simrasch, id = "person")
> > glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
> 13793.707 13968.657 -6874.853 13749.707     20978
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 1.014
> Number of obs: 21000, groups:  person, 1000
> Fixed Effects:
>  variableX1   variableX2   variableX3   variableX4   variableX5
> variableX6   variableX7
>    5.182206     4.415991     3.871713     3.599747     3.191841
> 2.561370     2.219830
>  variableX8   variableX9  variableX10  variableX11  variableX12
> variableX13  variableX14
>    1.611983     0.992610     0.564159     0.003716    -0.398537
> -0.987771    -1.591732
> variableX15  variableX16  variableX17  variableX18  variableX19
> variableX20  variableX21
>   -1.998437    -2.400455    -2.870716    -3.408691    -3.806151
> -4.672642    -4.813643
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.907091 (tol = 0.001)
> > glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial,
> + control=glmerControl(optimizer="bobyqa"))
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
> 13793.696 13968.646 -6874.848 13749.696     20978
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 1.014
> Number of obs: 21000, groups:  person, 1000
> Fixed Effects:
>  variableX1   variableX2   variableX3   variableX4   variableX5
> variableX6   variableX7
>    5.184774     4.414100     3.862784     3.594478     3.190096
> 2.559954     2.214893
>  variableX8   variableX9  variableX10  variableX11  variableX12
> variableX13  variableX14
>    1.610161     0.988363     0.562450     0.002823    -0.400700
> -0.990108    -1.595035
> variableX15  variableX16  variableX17  variableX18  variableX19
> variableX20  variableX21
>   -1.997929    -2.403493    -2.875794    -3.408200    -3.809483
> -4.674454    -4.822606
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.00124419 (tol = 0.001)
> >
> >
> >
> 
> > set.seed("12345")
> > simrasch<-data.frame(rmvlogis(200, cbind(c(seq(-3, 3, 1),10), 1)))
> > rasch(simrasch,IRT.param=F)
> 
> Call:
> rasch(data = simrasch, IRT.param = F)
> 
> Coefficients:
>      X1       X2       X3       X4       X5       X6       X7
> X8        z
>   3.226    1.820    1.461    0.044   -0.706   -1.494   -2.771  -10.454
> 0.801
> 
> Log.Lik: -647.099
> 
> >
> > simrasch$person = rownames(simrasch)
> > simraschlong = melt(simrasch, id = "person")
> > glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
> 1312.7956 1361.1955 -647.3978 1294.7956      1591
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 0.7881
> Number of obs: 1600, groups:  person, 200
> Fixed Effects:
> variableX1  variableX2  variableX3  variableX4  variableX5  variableX6
> variableX7  variableX8
>    3.21214     1.82080     1.46473     0.04478    -0.71031    -1.49811
> -2.76154   -27.10659
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
> > glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial,
> + control=glmerControl(optimizer="bobyqa"))
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
> 1312.7956 1361.1955 -647.3978 1294.7956      1591
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 0.7881
> Number of obs: 1600, groups:  person, 200
> Fixed Effects:
> variableX1  variableX2  variableX3  variableX4  variableX5  variableX6
> variableX7  variableX8
>    3.21213     1.82080     1.46473     0.04478    -0.71032    -1.49811
> -2.76154   -19.56371
> 
> > set.seed("12345")
> > simrasch<-data.frame(rmvlogis(10000, cbind(c(seq(-3, 3, 1),10), 1)))
> > rasch(simrasch,IRT.param=F)
> 
> Call:
> rasch(data = simrasch, IRT.param = F)
> 
> Coefficients:
>     X1      X2      X3      X4      X5      X6      X7      X8       z
>  3.018   2.026   1.011   0.012  -1.013  -1.941  -2.977  -9.696   0.986
> 
> Log.Lik: -31914.71
> 
> >
> > simrasch$person = rownames(simrasch)
> > simraschlong = melt(simrasch, id = "person")
> > glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
>  63888.47  63972.08 -31935.24  63870.47     79991
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 0.9739
> Number of obs: 80000, groups:  person, 10000
> Fixed Effects:
> variableX1  variableX2  variableX3  variableX4  variableX5  variableX6
> variableX7  variableX8
>    3.01884     2.03562     1.02945     0.02299    -1.01066    -1.93548
> -2.95234    -9.44503
> Warning messages:
> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 25.6865 (tol = 0.001)
> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
> > glmer(value ~ 0 + variable + (1|person), data=simraschlong,
> family=binomial,
> + control=glmerControl(optimizer="bobyqa"))
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: value ~ 0 + variable + (1 | person)
>    Data: simraschlong
>       AIC       BIC    logLik  deviance  df.resid
>  63887.88  63971.49 -31934.94  63869.88     79991
> Random effects:
>  Groups Name        Std.Dev.
>  person (Intercept) 0.9737
> Number of obs: 80000, groups:  person, 10000
> Fixed Effects:
> variableX1  variableX2  variableX3  variableX4  variableX5  variableX6
> variableX7  variableX8
>    3.00598     2.02857     1.01952     0.01191    -1.02146    -1.94484
> -2.96534    -9.65211
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.0128742 (tol = 0.001)
> 
> 	[[alternative HTML version deleted]]
> 
> 

Hi,

Sorry for the length of the message below but I also would need some help
and advice.

I am having similar issues (warnings about convergence, may be more) than
other people on this subject but I have been totally lost among all
suggestions and their meanings.

I test the effect of various covriant including inbreeding (O or 1) on
number of occurence of a given behaviour. As it is counting data, I use a
poisson family

The output of my model is :

Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.26172 (tol = 0.001)
2: In if (resHess$code != 0) { :
  the condition has length > 1 and only the first element will be used
3: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?

> summary(test)
Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['glmerMod']
 Family: poisson ( log )
Formula: behaviour~ inbreeding+ opponent inbreeding + sexratio + opponent
aggressiveness+ opponent mass + duration of the test +  
    inbreeding:sexratio + opponent inbreeding:opponent mass+ (1 |
family/IDfocal)
   Data: datamal

     AIC      BIC   logLik deviance df.resid 
  1469.3   1507.5   -723.6   1447.3      228 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-3.211 -1.223 -0.279  0.847  4.491 

Random effects:
 Groups       Name        Variance Std.Dev.
 ID:family (Intercept) 0.25751  0.5075  
 family      (Intercept) 0.04598  0.2144  
Number of obs: 239, groups: ID:family, 63; family, 19

Fixed effects:
                  Estimate Std. Error z value Pr(>|z|)    
(Intercept)      0.4300096  0.3798771   1.132  0.25765    
inb             -0.6387831  0.4348942  -1.469  0.14188    
inbopp           1.2304585  0.4218720   2.917  0.00354 ** 
sexratio        -0.7260132  0.4096143  -1.772  0.07632 .  
oppaggre         0.0017798  0.0004272   4.166  3.1e-05 ***
oppmass          0.0010126  0.0078721   0.129  0.89764    
duration         0.0065125  0.0003442  18.922  < 2e-16 ***
inb:sexratio     1.7354628  0.6444371   2.693  0.00708 ** 
inbopp:oppmass  -0.0499209  0.0175064  -2.852  0.00435 ** 
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
               (Intr) inb    inbopp sexrat oppaggr oppmass dura  inb:sx
inb            -0.598                                                 
inbopp         -0.241 -0.004                                          
sexratio       -0.761  0.686 -0.007                                   
oppaggre       -0.121  0.021 -0.116  0.013                            
oppmass        -0.545  0.048  0.482  0.052 -0.101                     
duration       -0.079 -0.053 -0.013 -0.019 -0.331 -0.095              
inb:sexrati     0.499 -0.900  0.004 -0.665 -0.039 -0.048  0.100       
inbopp:oppmass  0.198  0.012 -0.987  0.012  0.154 -0.436 -0.001 -0.012

> Anova(test, type="III")
Analysis of Deviance Table (Type III Wald chisquare tests)

Response: behaviour
                   Chisq Df Pr(>Chisq)    
(Intercept)       1.2814  1   0.257647    
inb               2.1574  1   0.141880    
inbopp            8.5069  1   0.003538 ** 
sexratio          3.1415  1   0.076323 .  
oppaggre         17.3554  1    3.1e-05 ***
oppmass           0.0165  1   0.897644    
duration        358.0516  1  < 2.2e-16 ***
inb:sexratio      7.2522  1   0.007081 ** 
inbopp:oppmass    8.1315  1   0.004350 ** 

As usual, I looked at the distribution of residuals: nicely normal.

After that, according to the suggestions, I tested that:
relgrad <- with(test at optinfo$derivs,solve(Hessian,gradient))
max(abs(relgrad))
[1] 0.0002681561
-> if this is <0.001 or 0.002, is it enough to assume that the model is ok? 

In addition, I tested other optimizer, even if I do not really understand
what it means.
I started with:
testbobyqa <- update(test,control=glmerControl(optimizer="bobyqa"))
> max(abs(relgrad))
[1] 0.01506629
testnelder <- update(test,control=glmerControl(optimizer="Nelder_Mead"))
> max(abs(relgrad))
[1] 0.001706096
both showed the same warnings and similar results than the first model.

I continued with:
library(optimx)
testnlminb <- update(test,control=glmerControl(optimizer="optimx",
                                                  
optCtrl=list(method="nlminb")))
testBFGS <- update(test,control=glmerControl(optimizer="optimx",
                                                  
optCtrl=list(method="L-BFGS-B")))

-> in both case: error
testnlminb: 
Error in ans.ret[meth, ] <- c(ans$par, ans$value, ans$fevals, ans$gevals,  : 
  number of items to replace is not a multiple of replacement length
In addition: Warning messages:
1: In optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower,  :
  Parameters or bounds appear to have different scalings.
  This can cause poor performance in optimization. 
  It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA.
2: In pwrssUpdate(pp, resp, tolPwrss, GQmat, compDev, fac, verbose) :
  Cholmod warning 'not positive definite' at
file:../Cholesky/t_cholmod_rowfac.c, line 431
3: In pwrssUpdate(pp, resp, tolPwrss, GQmat, compDev, fac, verbose) :
  Cholmod warning 'not positive definite' at
file:../Cholesky/t_cholmod_rowfac.c, line 431

testBFGS:
Error in eval(expr, envir, enclos) : 
  (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate
In addition: Warning messages:
1: In optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower,  :
  Parameters or bounds appear to have different scalings.
  This can cause poor performance in optimization. 
  It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA.
2: In pwrssUpdate(pp, resp, tolPwrss, GQmat, compDev, fac, verbose) :
  Cholmod warning 'not positive definite' at
file:../Cholesky/t_cholmod_rowfac.c, line 431
3: In pwrssUpdate(pp, resp, tolPwrss, GQmat, compDev, fac, verbose) :
  Cholmod warning 'not positive definite' at
file:../Cholesky/t_cholmod_rowfac.c, line 431
4: In optwrap(optimizer, devfun, start, rho$lower, control = control,  :
  convergence code 9999 from optimx

I then used:
library(nloptr)
## from https://github.com/lme4/lme4/issues/98:
defaultControl <- list(algorithm="NLOPT_LN_BOBYQA",xtol_rel=1e-6,maxeval=1e5)
nloptwrap2 <- function(fn,par,lower,upper,control=list(),...) {
  for (n in names(defaultControl)) 
    if (is.null(control[[n]])) control[[n]] <- defaultControl[[n]]
  res <- nloptr(x0=par,eval_f=fn,lb=lower,ub=upper,opts=control,...)
  with(res,list(par=solution,
                fval=objective,
                feval=iterations,
                conv=if (status>0) 0 else status,
                message=message))
}
test.bobyqa2 <- update(g0.bobyqa,control=glmerControl(optimizer=nloptwrap2))
test.NM2 <- update(g0.bobyqa,control=glmerControl(optimizer=nloptwrap2,
                                               
optCtrl=list(algorithm="NLOPT_LN_NELDERMEAD")))
-> similar results than test, testbobyqa, testnelder and warnings for both:
Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.88422 (tol = 0.001)
2: In if (resHess$code != 0) { :
  the condition has length > 1 and only the first element will be used
3: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
> relgrad <- with(test.bobyqa2 at optinfo$derivs,solve(Hessian,gradient))
> max(abs(relgrad))
[1] 1.845328e-05
> relgrad <- with(test.NM2 at optinfo$derivs,solve(Hessian,gradient))
> max(abs(relgrad))
[1] 1.925339e-05

I also tried that:
getpar <- function(x) c(getME(x,c("theta")),fixef(x))
modList <- list(bobyqa=testbobyqa,NM=testnelder ,
+ bobyqa2=test.bobyqa2,NM2=test.NM2)
ctab <- sapply(modList,getpar)
library(reshape2)
mtab <- melt(ctab)
library(ggplot2)
theme_set(theme_bw())
ggplot(mtab,aes(x=Var2,y=value,colour=Var2))+
+ geom_point()+facet_wrap(~Var1,scale="free")
->I don't really understand wht does this graph mean, if you could explain
to me?

I also looked at this:
likList <- sapply(modList,logLik)
round(log10(max(likList)-likList),1)
 bobyqa      NM bobyqa2     NM2 
   -2.3    -2.3    -9.2    -Inf 
->What do those values mean?

lbound <- getME(test,"lower")
theta <- getME(test,"theta")
any(lbound==0 & theta<1e-8)
[1] FALSE
-> it means that it is good?

Finally, I tried rescalling my covariables to see if it could be the source
of warnings by using:
datamal$variable.scaled <- datamal$variable/sd(datamal$variable)
and when I introduced the sclaed variables in the model, I received this
message:
Error in FUN(X[[1L]], ...) : 
  Invalid grouping factor specification, noms:famille

I have tried many things and I am totally lost so if you can help me
clarifying all that, it would be very appreciated.

I hope I wrote down all you needed to understand.

Thanks in advance for your help and sorry again for thlength of the message.


From mwiederm at mtu.edu  Wed May 14 18:35:25 2014
From: mwiederm at mtu.edu (Lena)
Date: Wed, 14 May 2014 16:35:25 +0000 (UTC)
Subject: [R-sig-ME] previous posts about error message in glmmADMB
References: <CAOCHjhTc38==KKBeWVAfqJoNytSd+Q_P84dHwpq5wfDFkhdKkA@mail.gmail.com>
	<519BA9D2.4000101@gmail.com>
	<loom.20140430T181519-196@post.gmane.org>
Message-ID: <loom.20140514T174257-255@post.gmane.org>

Dear list,
I am still stuck on this. I am trying to run a GLMM on my data. 
Here are my models:
Test$N<-factor(Test$N)
Test$plot<-factor(Test$plot)
Test$subplot<-factor(Test$subplot)
resp<-cbind(Test$D,Test$A)
m1<glmmadmb(resp~N*T+(1|plot/subplot),data=Test,zeroInflation=FALSE,
family="binomial")
m2<glmmadmb(resp~N+T+
(1|pl/subplot),data=Test,zeroInflation=FALSE,family="binomial")

First (see my last post) I ran it using ?glmmadmb? with zeroInflation, with
the known Error message. However, the calculations went through using the
same model but without zeroInflation. Now there were no significant
interactions, so I attempted to reduce the model by throwing them out. But
N+T again yielded in the previous error term: 

Parameters were estimated, but not standard errors were not: the most likely
problem is that the curvature at MLE was zero or negative Error in
glmmadmb(resp ~ N + T + (1 | plot/subplot), data = Test, zeroInflation =
FALSE,  : The function maximizer failed (couldn't find STD file)
Troubleshooting steps include (1) run with 'save.dir' set and inspect output
files; (2) change run parameters: see '?admbControl'
In addition: Warning message:
running command 'C:\WINDOWS\system32\cmd.exe /c "C:/Program
Files/R/R-3.0.2/library/glmmADMB/bin/windows32/glmmadmb.exe" -maxfn 500
-maxph 5 -noinit -shess' had status 1 

For the heck of it I also tried the glmmPQL and encountered the same problem
more or less instantly though (N*T worked N+T did not)

m3<-glmmPQL(resp~N*T,random=~1|plot/subplot,family=binomial,data=Test)
m4<-glmmPQL(resp~N+T,random=~1|plot/subplot,family=binomial,data=Test)

The glmer had, with more than 4 hours the longest calculation time and ended
with the longest error message. 

m5<-glmer(resp~N*T+(plot|subplot),data=Test,family=binomial)

Warning messages:
1: In commonArgs(par, fn, control, environment()) :
  maxfun < 10 * length(par)^2 is not recommended.
2: In optwrap(optimizer, devfun, start, rho$lower, control = control,  :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function
evaluations exceeded
3: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
4: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 269.495 (tol = 0.001)
5: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 26 negative eigenvalues

I have a hard time interpreting these error messages. Did anybody bump into
similar issues or is that just me? Any ideas on how to go about it are much
appreciated!
Thanks, Lena


From bbolker at gmail.com  Wed May 14 21:47:35 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 14 May 2014 15:47:35 -0400
Subject: [R-sig-ME] ANOVA type lll ss table for GLMER?
In-Reply-To: <53737763.40808@psychologie.uni-freiburg.de>
References: <CAPEhjRBa3NWUm9qzgcBSw0Xee1TwhsNzKsmCkhS_UiZfWoOLqg@mail.gmail.com>
	<53737763.40808@psychologie.uni-freiburg.de>
Message-ID: <5373C857.90104@gmail.com>

On 14-05-14 10:02 AM, Henrik Singmann wrote:
> Dear Heather,
> 
> you could try to use mixed from the afex package which will give you
> Type III p-values for the effects via Chi^2 tests (or alternatively via
> parametric bootstrap):
> 
> require(afex)
> (spden2 <- mixed(SpDens~(treat*samp)-1+(1|TRANSECT),family=poisson,
> data=rm, nAGQ = 9, method = "LRT")
> 
> Note however, that loading afex changes your overall contrasts, to reset
> the default contrasts use:
> options(contrasts=c('contr.treatment', 'contr.poly'))

   But (despite the fact that I **really** don't like afex's default
behaviour of changing the overall contrasts) -- you should definitely
use contr.sum when computing a marginal ANOVA table (i.e. do NOT reset
the contrasts until after you're done constructing your table), if you
insist on doing that.

> 
> Furthermore, (g)lmer doesn't break the factors done by *all* levels. It
> removes the first levels (usually). Hence the parameters cannot directly
> be interpreted if this level is "significant".
> 
> Hope this helps,
> Henrik
> 
> Am 14.05.2014 15:45, schrieb Heather Moylett:
>>   Hello group,
>>
>> This is my first time posting, so I hope I have explained my needs
>> clearly
>> below.
>>
>> I am running a repeated measure analysis with a raw species count data
>> set
>> (SpDens). I have run different model types (zeroinfl, glm, glmer) and
>> have
>> identified glmer to have the best fit. The output generated by GLMER
>> breaks
>> my between groups (treat) and within groups (samp) factors down by
>> levels.
>> In addition to this, I would like to look at the effect of treat and samp
>> overall, something similar to an ANOVA table (Type lll SS). When I use
>> Anova(object) I receive an ANOVA table with an F val and no P-vals. I
>> would
>> prefer to stick with the z-stat and p-vals. I have seen this reported in
>> other papers, so I know it can be done...just can't figure out how to
>> do it!
>>
>> Components of the model:
>> samp: 23 sampling dates is the repeated measure (within groups)
>> treat: 4 levels (between groups)
>> TRANSECT: experimental unit (subject), 4/treat and data collected from
>> all
>> 16 every sampling date
>>
>> When I run this code:
>>
>> RM <- read.csv("C:/Users/heatbell/Desktop/Walthour-Moss/STATS/CH
>> 1/Final/R/RM.csv")
>>
>> View(RM)
>>
>> rm <- subset(RM, SAMPLE >= 2)
>>
>>
>>
>> rm<- within(rm, {
>>
>>    samp<-factor(SAMPLE)
>>
>>    yr<-factor(YEAR)
>>
>>    treat<-factor(TREAT)
>>
>> })
>>
>> summary(rm)
>>
>>
>> summary(spden<-glmer(SpDens~(treat*samp)-1+(1|TRANSECT),family=poisson,
>> data=rm, nAGQ = 9))
>>
>> Thank you for the help!
>> Heather
>>
>


From bbolker at gmail.com  Wed May 14 21:59:20 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 14 May 2014 19:59:20 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?Wrong_convergence_warnings_with_glmer_in_lme?=
	=?utf-8?b?NCB2ZXJzaW9uCTEuMS02Pz8=?=
References: <CA+doL7zex9Fy6RG3cCshNXzUndOc39MefQRBE=t4sf70d-HHaw@mail.gmail.com>
	<loom.20140514T161323-465@post.gmane.org>
Message-ID: <loom.20140514T215156-188@post.gmane.org>

UG <uriel.gelin at ...> writes:

> 
> Tom Davis <tomd792 <at> ...> writes:
> 
> > 
> > Dear lme4 experts,
> > 
> > Yesterday, I ran the code for two published papers 
> (de Boeck et al.,2011;
> > de Boeck and Partchev, 2012) on psychometric modeling with glmer in lme4
> > version 1.1-6 and the vast majority of the models I 
> ran produce convergence
> > warnings (even the simple ones).

 [major snippage]

> > I had no warnings using version 1.0-5 and 
> version 1.0-6 so this seems to be
> > a recent problem of lme4?
> > 
> > Is it best to ignore all these convergence warnings for now? Should I
> > switch back to an older version of lme4 to avoid this problem? Should I
> > generally avoid using large datasets with lme4?
> > 
> > Many thanks in advance,
> > Tom

  Re-iterating what I may have said earlier:

 * these warnings are a result of changes in the WARNING behaviour, not a
change in the fitting algorithm.  The only major recent change in the
optimizer has been to move in 1.1-6 from Nelder-Mead to bobyqa for
lmer fits <http://cran.r-project.org/web/packages/lme4/news.html>; this
should isn't relevant to glmer fits and we believe should _improve_
results in almost all cases.
 * You should probably ignore the convergence warnings; we now believe
that they are (almost entirely) the result of checking the gradients,
rather than the scaled gradients (i.e. the results of solve(Hessian,grad).
 * Version 1.1-7, now on Github (available via devtools::install_github)
and with a macos binary on http://lme4.r-forge.r-project.org/repos ,
tests the scaled gradients and should make the problem go away.
 * If you want to double-check, either 
   * test with lme4.0 (although we
have had reports  <http://stackoverflow.com/questions/23662589/
r-reverting-to-lme4-0-and-still-getting-inconsistent-results>
(broken URL!) of 'Downdated XtX' problems -- still chasing that down ...
or * use the code at
https://github.com/lme4/lme4/blob/master/misc/issues/allFit.R
to test with a wide range of optimizers.

  Ben Bolker


From bbolker at gmail.com  Wed May 14 22:13:19 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 14 May 2014 16:13:19 -0400
Subject: [R-sig-ME] Doubts about models in glmmADMB
In-Reply-To: <CAGAvxRmwv8_y5994s4DFTkz3iqCOso-_FR1jP3Rve1wigbpJiQ@mail.gmail.com>
References: <CAGAvxRkS9yJoubfV3JaWj-O3pwGhUq7dNWzhSupS7xoqsRR3ZQ@mail.gmail.com>
	<CAGAvxRmwv8_y5994s4DFTkz3iqCOso-_FR1jP3Rve1wigbpJiQ@mail.gmail.com>
Message-ID: <5373CE5F.9090605@gmail.com>

On 14-05-14 08:56 AM, Carlos Barboza wrote:
> Dear Dr. Bolker,

  (note this is sent to the mixed models list, not just to me ...)

> I'm working with spatial factors: sectors are spaced by kilometers, sites
> (randomly sampled within sectors) are spaced by hundred of meters, and
> points (randomly sampled within sites) are spaced by dozen of meters. I
> have, 3 sectors, 3 sites within each sector and 3 points within each site,
> replicated 5 times (n=135). Sectors specify a pollution gradient. So I want
> to investigate the sector's effect (fixed) and the spatial variability
> within each sector by two spatial scales, sites and points. Labels were
> coded like your example in http://glmm.wikidot.com/faq:  (e.g. A1, A2, ???,
> B1, B2, ???), the only difference is to include points (A11,A12,A13.....A33).
> So this was my doubt:
> 
>> model.1<- glmmadmb(total ~ sector + (1 | sector/site/point),...
> 
> or
> 
>> model.2<-glmmadmb(total ~ sector + (1 | site/point),...
> 
> when I want to include all spatial variability????

  Given what you have said, the second specification is correct.  The
first includes sector, redundantly, both as a fixed effect and as a
grouping variable for variation in intercepts.

> 
> # since I want to test if a model including only sector or sector and
> within spatial variability of each sectors, are better then a null model, I
> specified this null model with the intercept only:
> 
>> null.model<-glmmadmb(total ~ 1 ,..

  Sounds reasonable, although keep in mind the issues with using
likelihood ratio tests to test null hypotheses of zero variance ...

  Ben Bolker


From carlosambarboza at gmail.com  Wed May 14 23:09:06 2014
From: carlosambarboza at gmail.com (Carlos Barboza)
Date: Wed, 14 May 2014 18:09:06 -0300
Subject: [R-sig-ME] Doubts about models in glmmADMB
In-Reply-To: <5373CE5F.9090605@gmail.com>
References: <CAGAvxRkS9yJoubfV3JaWj-O3pwGhUq7dNWzhSupS7xoqsRR3ZQ@mail.gmail.com>
	<CAGAvxRmwv8_y5994s4DFTkz3iqCOso-_FR1jP3Rve1wigbpJiQ@mail.gmail.com>
	<5373CE5F.9090605@gmail.com>
Message-ID: <CAGAvxRmjfFjOFWE_sPYgWeYDmCDr4w1tyLW2RP38ay78k_tHeg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140514/769d9713/attachment.pl>

From henrik.singmann at psychologie.uni-freiburg.de  Thu May 15 12:49:24 2014
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Thu, 15 May 2014 12:49:24 +0200
Subject: [R-sig-ME] ANOVA type lll ss table for GLMER?
In-Reply-To: <5373C857.90104@gmail.com>
References: <CAPEhjRBa3NWUm9qzgcBSw0Xee1TwhsNzKsmCkhS_UiZfWoOLqg@mail.gmail.com>	<53737763.40808@psychologie.uni-freiburg.de>
	<5373C857.90104@gmail.com>
Message-ID: <53749BB4.6020609@psychologie.uni-freiburg.de>



Am 14.05.2014 21:47, schrieb Ben Bolker:
> On 14-05-14 10:02 AM, Henrik Singmann wrote:
>> Dear Heather,
>>
>> you could try to use mixed from the afex package which will give you
>> Type III p-values for the effects via Chi^2 tests (or alternatively via
>> parametric bootstrap):
>>
>> require(afex)
>> (spden2 <- mixed(SpDens~(treat*samp)-1+(1|TRANSECT),family=poisson,
>> data=rm, nAGQ = 9, method = "LRT")
>>
>> Note however, that loading afex changes your overall contrasts, to reset
>> the default contrasts use:
>> options(contrasts=c('contr.treatment', 'contr.poly'))
>
>     But (despite the fact that I **really** don't like afex's default
> behaviour of changing the overall contrasts) -- you should definitely
> use contr.sum when computing a marginal ANOVA table (i.e. do NOT reset
> the contrasts until after you're done constructing your table), if you
> insist on doing that.


First, all afex functions (including mixed) are unaffected by global contrasts as long as the argument check.contrasts = TRUE (which is the default). In other words, mixed per default uses contr.sum independently of the global contrasts (more specifically, it sets it for all factors if not already contr.sum or if the global contrasts are not contr.sum).

Second, I give in. From the current development version on (version 0.10-110) afex *does not* change the global contrasts anymore. This should not affect any of the functions within afex (my tests confirm that). To make setting contrasts globally easy, I added the following convenience functions: set_sum_contrasts(), set_default_contrasts(), set_treatment_contrasts(), ...

Are you happy now, Ben? :)

You can install the the development version of afex from R-forge (may take a few hours):  install.packages("afex", repos="http://R-Forge.R-project.org")


>
>>
>> Furthermore, (g)lmer doesn't break the factors done by *all* levels. It
>> removes the first levels (usually). Hence the parameters cannot directly
>> be interpreted if this level is "significant".
>>
>> Hope this helps,
>> Henrik
>>
>> Am 14.05.2014 15:45, schrieb Heather Moylett:
>>>    Hello group,
>>>
>>> This is my first time posting, so I hope I have explained my needs
>>> clearly
>>> below.
>>>
>>> I am running a repeated measure analysis with a raw species count data
>>> set
>>> (SpDens). I have run different model types (zeroinfl, glm, glmer) and
>>> have
>>> identified glmer to have the best fit. The output generated by GLMER
>>> breaks
>>> my between groups (treat) and within groups (samp) factors down by
>>> levels.
>>> In addition to this, I would like to look at the effect of treat and samp
>>> overall, something similar to an ANOVA table (Type lll SS). When I use
>>> Anova(object) I receive an ANOVA table with an F val and no P-vals. I
>>> would
>>> prefer to stick with the z-stat and p-vals. I have seen this reported in
>>> other papers, so I know it can be done...just can't figure out how to
>>> do it!
>>>
>>> Components of the model:
>>> samp: 23 sampling dates is the repeated measure (within groups)
>>> treat: 4 levels (between groups)
>>> TRANSECT: experimental unit (subject), 4/treat and data collected from
>>> all
>>> 16 every sampling date
>>>
>>> When I run this code:
>>>
>>> RM <- read.csv("C:/Users/heatbell/Desktop/Walthour-Moss/STATS/CH
>>> 1/Final/R/RM.csv")
>>>
>>> View(RM)
>>>
>>> rm <- subset(RM, SAMPLE >= 2)
>>>
>>>
>>>
>>> rm<- within(rm, {
>>>
>>>     samp<-factor(SAMPLE)
>>>
>>>     yr<-factor(YEAR)
>>>
>>>     treat<-factor(TREAT)
>>>
>>> })
>>>
>>> summary(rm)
>>>
>>>
>>> summary(spden<-glmer(SpDens~(treat*samp)-1+(1|TRANSECT),family=poisson,
>>> data=rm, nAGQ = 9))
>>>
>>> Thank you for the help!
>>> Heather
>>>
>>
>

-- 
Dr. Henrik Singmann
Albert-Ludwigs-Universit?t Freiburg, Germany
http://www.psychologie.uni-freiburg.de/Members/singmann


From coccia at ebd.csic.es  Thu May 15 15:26:01 2014
From: coccia at ebd.csic.es (CRISTINA.COCCIA)
Date: Thu, 15 May 2014 15:26:01 +0200
Subject: [R-sig-ME] error en glmmadmb
Message-ID: <20140515152601.Horde.v34qTegqnSdcO52CYiIDdQ5@webmail.csic.es>


Dear list,

after checking for collinearity, overdispersion and zeroinflation of  
my data I found the glmmadmb the best approach for them.


So, I'm now starting from the following code:


Model<-glmmadmb(Cor.affi~Depth+Temp+pH+Sal+TN+TP+Tur+Chla+Fish+fType+Sub_Veg+(1|fPonds)+(1|fYear:fDate),data=na.omit(dataLG1),
family="nbinom",zeroInflation=TRUE)

Number of observations:  total=239, fPonds=42, fYear:fDate=8


to find the best fixed structure through the backward  
elimination,however when I ran this model:


M9<-glmmadmb(Cor.affi~  
Depth+TP+Tur+fType+Sub_Veg+(1|fPonds)+(1|fYear:fDate),data=na.omit(dataLG1),
family="nbinom",zeroInflation=TRUE)

I get the following error:

Parameters were estimated, but not standard errors were not: the most  
likely problem is that the curvature at MLE was zero or negative
Error in glmmadmb(Cor.affi ~ Depth + TP + Tur + fType + Sub_Veg + (1 |  :
  The function maximizer failed (couldn't find STD file)  
Troubleshooting steps include (1) run with 'save.dir' set and inspect  
output files; (2) change run parameters: see '?admbControl'
In addition: Warning message:
running command './glmmadmb -maxfn 500 -maxph 5 -noinit -shess' had status 1

that only appears at this stage by removing the variable Fish (P/A).

Could anyone give me any suggestion?

Thank in advance


From Thierry.ONKELINX at inbo.be  Fri May 16 10:24:24 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 16 May 2014 08:24:24 +0000
Subject: [R-sig-ME] glmer syntax for model with "common" random effects
In-Reply-To: <p06230901cf96a452fa86@[192.168.1.115]>
References: <dfa661b$6949d5dc$27c53036$@freshpond.org>
	<AA818EAD2576BC488B4F623941DA7427F3A4D46E@inbomail.inbo.be>
	<p06230901cf96a452fa86@[192.168.1.115]>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A4FCC2@inbomail.inbo.be>

Dear Steven,

Please keep the mailing list in cc. Others might chime in and you might get a faster reply.

You need to create a variable the combines both norms. That is what Norm_Combined <- interaction(Norm_XY, Norm_21) does. Then you could do glmer(Sex ~ 1 + Norm_XY + Norm_21 + (Norm_Combined|Pat_ID/Proc_ID), data =  PGD_21.df,family = binomial). Note that this would be equivalent to glmer(Sex ~ 1 + Norm_XY + Norm_21 + (Norm_XY * Norm_21|Pat_ID/Proc_ID), data =  PGD_21.df,family = binomial). So even more complex than your original model.

Note that (1|A/B) is equivalent to (1|A + 1|A:B) Therefor my suggestion to use (1|Pat_ID) + (1|PatID:Proc_ID) + (1|Pat_ID:Norm_Combined) + (1|PatID: Proc_ID:Norm_Combined) which is the verbatim of (1|PatID/Proc_ID/Norm_Combined) + (1|PatID/ /Norm_Combined). This model allows the same fit as (Norm_Combined|Pat_ID/Proc_ID) but assumes that all levels of Norm_Combined come from a univariate iid normal distribution (same variance and uncorrelated). (Norm_Combined|Pat_ID/Proc_ID)
assumes that the levels of Norm_Combined come from a multivariate normal distribution. And hence requires a lot more parameters.

If this is not what you want, then give us the mathematical equation of the model that you need.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: orzack [mailto:orzack at freshpond.org]
Verzonden: woensdag 14 mei 2014 15:20
Aan: ONKELINX, Thierry
Onderwerp: RE: [R-sig-ME] glmer syntax for model with "common" random effects

Dear Thierry,
   many thanks for your suggestion. I think we are talking past each other, which is likely mostly due to my not being clear and/or my not understanding your suggestion. Perhaps this would help clarify:

in my first email, I presented this model

>model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 + ((Norm_XY +
>Norm_21)|Pat_ID/Proc_ID), data =  PGD_21.df,family = binomial))

this generates a "common" or "overall" estimate of the intercept, i.e., one that estimates the random effects for both predictors combined. What I would like is a model that provides a similar "combined" estimate for the slope. it would be something like

>model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 +
>((COMBINEDNORM)|Pat_ID/Proc_ID), data =  PGD_21.df,family =
>binomial))

so that the fixed effects are calculated separately but norm-specific estimates of the random effects for the slope were not provided.

When I use the interaction approach you mention, glmer returns estimates for each combination of the levels

>
>I think you want something like this (if I understand you question correctly).
>
>PGD_21.df$Norm_Inter <- interaction(PGD_21.df$NormXY,
>PGD_21.df$Norm_21) model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 + (1|Pat_ID) + (1|PatID:
>Proc_ID) + (1|Pat_ID:Norm_Inter) + (1|PatID: Proc_ID:Norm_Inter), data
>=  PGD_21.df,family = binomial)
>

I want to reduce the AIC penalty. This increases it unless I misunderstand your syntax. Speaking of which, what does this syntax mean

1|PatID: Proc_ID:Norm_Inter?

In my problem, only  PatID and Proc_ID provide estimates of random effects.

I look forward to hearing from you.

S.
--
Steven Orzack

The Fresh Pond Research Institute
173 Harvey Street
Cambridge, MA. 02140
617 864-4307

www.freshpond.org
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From Thierry.ONKELINX at inbo.be  Fri May 16 11:33:11 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 16 May 2014 09:33:11 +0000
Subject: [R-sig-ME] glmer syntax for model with "common" random effects
In-Reply-To: <3D16D37DD079C34BBE452E7D43D39781A94F9B65@UWMBX01.uw.lu.se>
References: <dfa661b$6949d5dc$27c53036$@freshpond.org>
	<AA818EAD2576BC488B4F623941DA7427F3A4D46E@inbomail.inbo.be>
	<p06230901cf96a452fa86@[192.168.1.115]>
	<AA818EAD2576BC488B4F623941DA7427F3A4FCC2@inbomail.inbo.be>,
	<3D16D37DD079C34BBE452E7D43D39781A94F9B65@UWMBX01.uw.lu.se>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A4FCFF@inbomail.inbo.be>

Dear Martin,

When A and B are factors (A:B|X) and (A * B|X) are equivalent, not equal. The will give the same model fit. The difference is the parametrisation.


Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey

________________________________________
Van: Martin Stjernman [martin.stjernman at biol.lu.se]
Verzonden: vrijdag 16 mei 2014 11:22
Aan: ONKELINX, Thierry; orzack
CC: r-sig-mixed-models at r-project.org
Onderwerp: RE: [R-sig-ME] glmer syntax for model with "common" random effects

Hi all,

Am I completely off track claiming that (Norm_Combined|Pat_ID/Proc_ID) is equivalent to (Norm_XY :Norm_21|Pat_ID/Proc_ID) rather than to  (Norm_XY * Norm_21|Pat_ID/Proc_ID) in the post by Thierry?

Norm_XY*Norm_21 would expand to Norm_XY + Norm_21 + Norm_XY:Norm_21 wouldn't it?

(Just to understand the formula specifications in R)

Sincerely

Martin
__________________________________________________________________________________
Martin Stjernman
Department of Biology
Lund University
Ecology building
S-223 62 Lund
Sweden
phone: +46 (0)46-222 38 20
mobile: +46 (0)708-810 887
fax: +46 (0)46-222 47 16
e-mail: martin.stjernman at biol.lu.se
__________________________________________________________________________________

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of ONKELINX, Thierry
Sent: den 16 maj 2014 10:24
To: orzack
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] glmer syntax for model with "common" random effects

Dear Steven,

Please keep the mailing list in cc. Others might chime in and you might get a faster reply.

You need to create a variable the combines both norms. That is what Norm_Combined <- interaction(Norm_XY, Norm_21) does. Then you could do glmer(Sex ~ 1 + Norm_XY + Norm_21 + (Norm_Combined|Pat_ID/Proc_ID), data =  PGD_21.df,family = binomial). Note that this would be equivalent to glmer(Sex ~ 1 + Norm_XY + Norm_21 + (Norm_XY * Norm_21|Pat_ID/Proc_ID), data =  PGD_21.df,family = binomial). So even more complex than your original model.

Note that (1|A/B) is equivalent to (1|A + 1|A:B) Therefor my suggestion to use (1|Pat_ID) + (1|PatID:Proc_ID) + (1|Pat_ID:Norm_Combined) + (1|PatID: Proc_ID:Norm_Combined) which is the verbatim of (1|PatID/Proc_ID/Norm_Combined) + (1|PatID/ /Norm_Combined). This model allows the same fit as (Norm_Combined|Pat_ID/Proc_ID) but assumes that all levels of Norm_Combined come from a univariate iid normal distribution (same variance and uncorrelated). (Norm_Combined|Pat_ID/Proc_ID) assumes that the levels of Norm_Combined come from a multivariate normal distribution. And hence requires a lot more parameters.

If this is not what you want, then give us the mathematical equation of the model that you need.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: orzack [mailto:orzack at freshpond.org]
Verzonden: woensdag 14 mei 2014 15:20
Aan: ONKELINX, Thierry
Onderwerp: RE: [R-sig-ME] glmer syntax for model with "common" random effects

Dear Thierry,
   many thanks for your suggestion. I think we are talking past each other, which is likely mostly due to my not being clear and/or my not understanding your suggestion. Perhaps this would help clarify:

in my first email, I presented this model

>model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 + ((Norm_XY +
>Norm_21)|Pat_ID/Proc_ID), data =  PGD_21.df,family = binomial))

this generates a "common" or "overall" estimate of the intercept, i.e., one that estimates the random effects for both predictors combined. What I would like is a model that provides a similar "combined" estimate for the slope. it would be something like

>model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 +
>((COMBINEDNORM)|Pat_ID/Proc_ID), data =  PGD_21.df,family =
>binomial))

so that the fixed effects are calculated separately but norm-specific estimates of the random effects for the slope were not provided.

When I use the interaction approach you mention, glmer returns estimates for each combination of the levels

>
>I think you want something like this (if I understand you question correctly).
>
>PGD_21.df$Norm_Inter <- interaction(PGD_21.df$NormXY,
>PGD_21.df$Norm_21) model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 + (1|Pat_ID) + (1|PatID:
>Proc_ID) + (1|Pat_ID:Norm_Inter) + (1|PatID: Proc_ID:Norm_Inter), data
>=  PGD_21.df,family = binomial)
>

I want to reduce the AIC penalty. This increases it unless I misunderstand your syntax. Speaking of which, what does this syntax mean

1|PatID: Proc_ID:Norm_Inter?

In my problem, only  PatID and Proc_ID provide estimates of random effects.

I look forward to hearing from you.

S.
--
Steven Orzack

The Fresh Pond Research Institute
173 Harvey Street
Cambridge, MA. 02140
617 864-4307

www.freshpond.org
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From martin.stjernman at biol.lu.se  Fri May 16 11:22:46 2014
From: martin.stjernman at biol.lu.se (Martin Stjernman)
Date: Fri, 16 May 2014 09:22:46 +0000
Subject: [R-sig-ME] glmer syntax for model with "common" random effects
In-Reply-To: <AA818EAD2576BC488B4F623941DA7427F3A4FCC2@inbomail.inbo.be>
References: <dfa661b$6949d5dc$27c53036$@freshpond.org>
	<AA818EAD2576BC488B4F623941DA7427F3A4D46E@inbomail.inbo.be>
	<p06230901cf96a452fa86@[192.168.1.115]>
	<AA818EAD2576BC488B4F623941DA7427F3A4FCC2@inbomail.inbo.be>
Message-ID: <3D16D37DD079C34BBE452E7D43D39781A94F9B65@UWMBX01.uw.lu.se>

Hi all,

Am I completely off track claiming that (Norm_Combined|Pat_ID/Proc_ID) is equivalent to (Norm_XY :Norm_21|Pat_ID/Proc_ID) rather than to  (Norm_XY * Norm_21|Pat_ID/Proc_ID) in the post by Thierry?

Norm_XY*Norm_21 would expand to Norm_XY + Norm_21 + Norm_XY:Norm_21 wouldn't it?

(Just to understand the formula specifications in R)

Sincerely

Martin
__________________________________________________________________________________
Martin Stjernman
Department of Biology
Lund University
Ecology building
S-223 62 Lund
Sweden
phone: +46 (0)46-222 38 20
mobile: +46 (0)708-810 887
fax: +46 (0)46-222 47 16
e-mail: martin.stjernman at biol.lu.se
__________________________________________________________________________________

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of ONKELINX, Thierry
Sent: den 16 maj 2014 10:24
To: orzack
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] glmer syntax for model with "common" random effects

Dear Steven,

Please keep the mailing list in cc. Others might chime in and you might get a faster reply.

You need to create a variable the combines both norms. That is what Norm_Combined <- interaction(Norm_XY, Norm_21) does. Then you could do glmer(Sex ~ 1 + Norm_XY + Norm_21 + (Norm_Combined|Pat_ID/Proc_ID), data =  PGD_21.df,family = binomial). Note that this would be equivalent to glmer(Sex ~ 1 + Norm_XY + Norm_21 + (Norm_XY * Norm_21|Pat_ID/Proc_ID), data =  PGD_21.df,family = binomial). So even more complex than your original model.

Note that (1|A/B) is equivalent to (1|A + 1|A:B) Therefor my suggestion to use (1|Pat_ID) + (1|PatID:Proc_ID) + (1|Pat_ID:Norm_Combined) + (1|PatID: Proc_ID:Norm_Combined) which is the verbatim of (1|PatID/Proc_ID/Norm_Combined) + (1|PatID/ /Norm_Combined). This model allows the same fit as (Norm_Combined|Pat_ID/Proc_ID) but assumes that all levels of Norm_Combined come from a univariate iid normal distribution (same variance and uncorrelated). (Norm_Combined|Pat_ID/Proc_ID) assumes that the levels of Norm_Combined come from a multivariate normal distribution. And hence requires a lot more parameters.

If this is not what you want, then give us the mathematical equation of the model that you need.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: orzack [mailto:orzack at freshpond.org]
Verzonden: woensdag 14 mei 2014 15:20
Aan: ONKELINX, Thierry
Onderwerp: RE: [R-sig-ME] glmer syntax for model with "common" random effects

Dear Thierry,
   many thanks for your suggestion. I think we are talking past each other, which is likely mostly due to my not being clear and/or my not understanding your suggestion. Perhaps this would help clarify:

in my first email, I presented this model

>model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 + ((Norm_XY + 
>Norm_21)|Pat_ID/Proc_ID), data =  PGD_21.df,family = binomial))

this generates a "common" or "overall" estimate of the intercept, i.e., one that estimates the random effects for both predictors combined. What I would like is a model that provides a similar "combined" estimate for the slope. it would be something like

>model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 + 
>((COMBINEDNORM)|Pat_ID/Proc_ID), data =  PGD_21.df,family =
>binomial))

so that the fixed effects are calculated separately but norm-specific estimates of the random effects for the slope were not provided.

When I use the interaction approach you mention, glmer returns estimates for each combination of the levels

>
>I think you want something like this (if I understand you question correctly).
>
>PGD_21.df$Norm_Inter <- interaction(PGD_21.df$NormXY,
>PGD_21.df$Norm_21) model <- glmer(Sex ~ 1 + Norm_XY + Norm_21 + (1|Pat_ID) + (1|PatID:
>Proc_ID) + (1|Pat_ID:Norm_Inter) + (1|PatID: Proc_ID:Norm_Inter), data 
>=  PGD_21.df,family = binomial)
>

I want to reduce the AIC penalty. This increases it unless I misunderstand your syntax. Speaking of which, what does this syntax mean

1|PatID: Proc_ID:Norm_Inter?

In my problem, only  PatID and Proc_ID provide estimates of random effects.

I look forward to hearing from you.

S.
--
Steven Orzack

The Fresh Pond Research Institute
173 Harvey Street
Cambridge, MA. 02140
617 864-4307

www.freshpond.org
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * * Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From henrik.singmann at psychologie.uni-freiburg.de  Sun May 18 00:07:13 2014
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Sun, 18 May 2014 00:07:13 +0200
Subject: [R-sig-ME] fixed effects estimates too small in binomial GLMM with
	low "success"-rate
Message-ID: <5377DD91.9030705@psychologie.uni-freiburg.de>

Dear list,

I would really appreciate your input on an issue in which the fixed effects estimates of a binomial GLMM with relatively low rate of "successes" are too low (compared to the observed effect) by around .5%.

The data comes from six comparable experiments in which we measured participants' performance errors on a series of trials in a design with one factor with two levels ("a" and "b"). Overall the difference in error rate between the two levels was very small (~0.5%), but when analyzing all data (with participant and experiment as random effects) the effect of factor reached significance (p = .03), which is also consistent with other published data.

However, the estimated effects are around .5% lower than the observed mean error rates indicating that the model may perhaps not adequately describe the data. Furthermore, I also get a convergence warning ("Model failed to converge with max|grad| = 0.0013716 (tol = 0.001)").

####### code 1 ##########
require(lme4)
dat <- read.table("http://pastebin.com/raw.php?i=KWjD5EG3")
dat$experiment <- factor(dat$experiment)

# observed effects
aggregate(errors ~ factor, dat, mean) #unweighted
##   factor     errors
## 1      a 0.02266914
## 2      b 0.02772540
by(dat, dat$factor, function(x) with(x, sum(errors*weights)/sum(weights))) #weighted
## dat$factor: a
## [1] 0.0222343
## ------------------------------------------------------------
## dat$factor: b
## [1] 0.02777651

m1 <- glmer(errors ~ factor + (factor|id) + (1|experiment), dat, weights = dat$weights, family = binomial)
## Warnmeldung:
## In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
##   Model failed to converge with max|grad| = 0.0013716 (tol = 0.001)
coef(summary(m1))[2,, drop = FALSE]
##          Estimate Std. Error  z value   Pr(>|z|)
## factorb 0.1788881 0.08005533 2.234556 0.02544653

# estimated effect:
logistic <- function(x) 1/(1+exp(-x))
logistic(cumsum(fixef(m1)))
## (Intercept)     factorb
##  0.01825229  0.02174991

####### end code 1 ##########

Interestingly, I can replicate this behavior when simulating binomial data similar to the one I am having. When the overall mean is relatively small, the estimates from the GLMM are consistently too low. When increasing the overall mean, this consistent bias in the estimated effect seems to go away (and the observed effect gets larger due to the non-linear nature of the logistic model). The code of the simulation for one data set follows below.

My question is: Does anyone have an idea what I could do to reach a better agreement between observed and predicted values? I tried different link functions, but that didn't help. And I am unsure whether I can trust my model given this discrepancy.

Thanks a lot,
Henrik


##### code 2 #####

mu <- -3.7  # when larger, problem disappears
n_experiments <- 6
n_participants <- 20
n_trials <- 400
sigma_effect <- 0.2
effect_size <- 0.4
sigma_p <- 0.7
sigma_e <- 0.1
prob_level <- 0.5

# create data:
df <- expand.grid(experiment = factor(seq(1, n_experiments)), id = factor(seq(1, n_participants)), factor = c(-1, 1))
df$id <- with(df, experiment:id)
df <- merge(df, data.frame(experiment = seq(1, n_experiments), re_e = rnorm(n_experiments, sd = sigma_e)))
df <- merge(df, data.frame(id = levels(df$id), re_p = rnorm(length(levels(df$id)), sd = sigma_p)))
df <- merge(df, data.frame(id = levels(df$id), re_a = rnorm(length(levels(df$id)), sd = sigma_effect)))
df <- with(df, df[order(id, factor),])
tmp <- rbinom(n_experiments*n_participants, n_trials, prob_level)
tmp <- rep(tmp, each = 2)
tmp[c(FALSE, TRUE)] <- n_trials-tmp[c(FALSE, TRUE)]
df$errors <- mapply(function(x, y) rbinom(1, y, x), with(df, logistic(mu+re_p+(factor*(effect_size/2)+factor*re_a)+re_e)), tmp)/tmp
df$weights <- tmp
df$factor <- factor(df$factor)

aggregate(errors ~ factor, df, mean) #unweighted
##   factor     errors
## 1     -1 0.02352719
## 2      1 0.03718523

s1 <- glmer(errors ~ factor + (factor|id) + (1|experiment), df, weights = df$weights, family = binomial)
## Warnmeldung:
## In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
##   Model failed to converge with max|grad| = 0.0130648 (tol = 0.001)
coef(summary(s1))[2,, drop = FALSE]
#          Estimate Std. Error  z value     Pr(>|z|)
# factor1 0.4765694 0.07578773 6.288213 3.211416e-10

logistic(cumsum(fixef(s1)))
# (Intercept)     factor1
#  0.01849934  0.02946117

#### end code 2 ####


-- 
Dr. Henrik Singmann
Albert-Ludwigs-Universit?t Freiburg, Germany
http://www.psychologie.uni-freiburg.de/Members/singmann


From bbolker at gmail.com  Sun May 18 00:19:05 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 17 May 2014 18:19:05 -0400
Subject: [R-sig-ME] fixed effects estimates too small in binomial GLMM
 with low "success"-rate
In-Reply-To: <5377DD91.9030705@psychologie.uni-freiburg.de>
References: <5377DD91.9030705@psychologie.uni-freiburg.de>
Message-ID: <5377E059.2050800@gmail.com>

On 14-05-17 06:07 PM, Henrik Singmann wrote:
> Dear list,
> 
> I would really appreciate your input on an issue in which the fixed
> effects estimates of a binomial GLMM with relatively low rate of
> "successes" are too low (compared to the observed effect) by around .5%.

  Don't know, but one comment and one thought:

  (1) the convergence warning goes away with the most recent version
of lme4 (1.1-7)

  (2) I'm _guessing_ that you will find that this phenomenon is
due to Jensen's effect (i.e., a nonlinear averaging effect) --
that would mean its magnitude should be proportional to the
random-effects variances and to the strength of the nonlinear
(inverse link) relationship.
  (I think there was a similar, possibly neglected, question along
these lines on the mailing list earlier ...)

  Ben Bolker


From henrik.singmann at psychologie.uni-freiburg.de  Sun May 18 00:38:20 2014
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Sun, 18 May 2014 00:38:20 +0200
Subject: [R-sig-ME] fixed effects estimates too small in binomial GLMM
	with low "success"-rate
In-Reply-To: <5377E059.2050800@gmail.com>
References: <5377DD91.9030705@psychologie.uni-freiburg.de>
	<5377E059.2050800@gmail.com>
Message-ID: <5377E4DC.6080809@psychologie.uni-freiburg.de>



Am 18.05.2014 00:19, schrieb Ben Bolker:
> On 14-05-17 06:07 PM, Henrik Singmann wrote:
>> Dear list,
>>
>> I would really appreciate your input on an issue in which the fixed
>> effects estimates of a binomial GLMM with relatively low rate of
>> "successes" are too low (compared to the observed effect) by around .5%.
>
>    Don't know, but one comment and one thought:
>
>    (1) the convergence warning goes away with the most recent version
> of lme4 (1.1-7)

This is true, thanks.
>
>    (2) I'm _guessing_ that you will find that this phenomenon is
> due to Jensen's effect (i.e., a nonlinear averaging effect) --
> that would mean its magnitude should be proportional to the
> random-effects variances and to the strength of the nonlinear
> (inverse link) relationship.
>    (I think there was a similar, possibly neglected, question along
> these lines on the mailing list earlier ...)

Thanks for the pointer. In fact, when entering experiment as fixed effect (which removes the random effect with the smallest variance), the estimates are way more precise. They are almost perfect. Perhaps I should then use this approach (although I would prefer treating experiment as random). Or what would you do?


>
>    Ben Bolker
>

-- 
Dr. Henrik Singmann
Albert-Ludwigs-Universit?t Freiburg, Germany
http://www.psychologie.uni-freiburg.de/Members/singmann


From jake987722 at hotmail.com  Sun May 18 08:03:52 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Sun, 18 May 2014 00:03:52 -0600
Subject: [R-sig-ME] "parm" parameter in lme4::confint.merMod
Message-ID: <BAY172-W2375062D4CFCE0B6E5F9CBCB330@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140518/0d6ee97f/attachment.pl>

From henrik.singmann at psychologie.uni-freiburg.de  Sun May 18 13:34:36 2014
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Sun, 18 May 2014 13:34:36 +0200
Subject: [R-sig-ME] "parm" parameter in lme4::confint.merMod
In-Reply-To: <BAY172-W2375062D4CFCE0B6E5F9CBCB330@phx.gbl>
References: <BAY172-W2375062D4CFCE0B6E5F9CBCB330@phx.gbl>
Message-ID: <53789ACC.90208@psychologie.uni-freiburg.de>

Hi Jake,

I think fixef() or names(fixef()) gives you what you want:

require(lme4)
data(md_16.4, package = "afex")
m1 <-  lmer(induct ~ cond*cog + (cog|room:cond), md_16.4)

fixef(m1)
## (Intercept)       cond1         cog   cond1:cog
## -6.14153043 10.32001485  0.66722061 -0.02520666

confint(m1, parm = c(3,2), method="Wald")
##             2.5 %    97.5 %
## cog     0.2498764  1.084565
## cond1 -18.4845630 39.124593

I hope I didn't misunderstand your question,
Henrik


Am 18.05.2014 08:03, schrieb Jake Westfall:
> Hi all,
>
> According to the documentation, in confint.merMod() you can use the "parm" parameter to get confidence intervals for only a specified subset of parameters by indicating the integer positions of the desired parameters. But how is one supposed to know which integer positions correspond with which parameters? I poked around a little but found nothing in the methods or slots of the merMod object that indicates this, and nothing in the documentation detailing this. Have I missed something? I guess you can call the function leaving that argument at its default, and the order of the parameters in that output will correspond with the integer positions, but this approach would seem to defeat the point of being able to specify particular subsets of parameters...
>
> Jake 		 	   		
> 	[[alternative HTML version deleted]]
>

-- 
Dr. Henrik Singmann
Albert-Ludwigs-Universit?t Freiburg, Germany
http://www.psychologie.uni-freiburg.de/Members/singmann


From bbolker at gmail.com  Sun May 18 16:21:00 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 18 May 2014 10:21:00 -0400
Subject: [R-sig-ME] "parm" parameter in lme4::confint.merMod
In-Reply-To: <53789ACC.90208@psychologie.uni-freiburg.de>
References: <BAY172-W2375062D4CFCE0B6E5F9CBCB330@phx.gbl>
	<53789ACC.90208@psychologie.uni-freiburg.de>
Message-ID: <5378C1CC.1090408@gmail.com>

On 14-05-18 07:34 AM, Henrik Singmann wrote:
> Hi Jake,
> 
> I think fixef() or names(fixef()) gives you what you want:

  I don't think it gets you everything.  Your solution works
in the method="Wald" case because method="Wald" only returns
the fixed effects, so one only has to keep track of those
positions.  method="profile" and "boot" include both
variance-covariance and fixed effect parameters, in that order.
Some possibilities are

names(unlist(getME(m1,c("theta","fixef"))))
## this gives names of 'theta' parameters (i.e. Cholesky factors)
## rather than standard deviations/correlations, but I believe
## the order is the same

## theta parameters only (hidden function)
lme4:::tnames(m1)

require(lme4)
data(md_16.4, package = "afex")
m1 <-  lmer(induct ~ cond*cog + (cog|room:cond), md_16.4)

## I actually had some problems with profiling this example --
## not quite sure why.  Will have to look into it.
pp <- profile(m1)


confint(m1,parm=3)
> fixef(m1)
> ## (Intercept)       cond1         cog   cond1:cog
> ## -6.14153043 10.32001485  0.66722061 -0.02520666
> 
> confint(m1, parm = c(3,2), method="Wald")
> ##             2.5 %    97.5 %
> ## cog     0.2498764  1.084565
> ## cond1 -18.4845630 39.124593
> 
> I hope I didn't misunderstand your question,
> Henrik
> 
> 
> Am 18.05.2014 08:03, schrieb Jake Westfall:
>> Hi all,
>>
>> According to the documentation, in confint.merMod() you can use the
>> "parm" parameter to get confidence intervals for only a specified
>> subset of parameters by indicating the integer positions of the
>> desired parameters. But how is one supposed to know which integer
>> positions correspond with which parameters? I poked around a little
>> but found nothing in the methods or slots of the merMod object that
>> indicates this, and nothing in the documentation detailing this. Have
>> I missed something? I guess you can call the function leaving that
>> argument at its default, and the order of the parameters in that
>> output will correspond with the integer positions, but this approach
>> would seem to defeat the point of being able to specify particular
>> subsets of parameters...
>>
>> Jake                        
>>     [[alternative HTML version deleted]]
>>
>


From highstat at highstat.com  Mon May 19 12:02:36 2014
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Mon, 19 May 2014 11:02:36 +0100
Subject: [R-sig-ME] Course: Introduction to GAM and GAMM with R
Message-ID: <5379D6BC.9050407@highstat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140519/3eb0abf5/attachment.pl>

From t.o.lentz at uu.nl  Mon May 19 12:22:01 2014
From: t.o.lentz at uu.nl (Solis)
Date: Mon, 19 May 2014 12:22:01 +0200
Subject: [R-sig-ME] fixed effects estimates too small in binomial GLMM with
	low "success"-rate (
In-Reply-To: <mailman.3.1400407203.2793.r-sig-mixed-models@r-project.org>
References: <mailman.3.1400407203.2793.r-sig-mixed-models@r-project.org>
Message-ID: <17351565-9F9B-4A64-9918-437DA011F0DE@uu.nl>

Dear Ben and others on the list, 

The 'possibly neglected' mail exchange is probably this one; 
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021919.html

I hope the link helps ( I'm about to board a plane) but I'm excited to see my question is picked up and might have an answer. 

Kind regards,

Tom

> Op 18 mei 2014 om 12:00 heeft <r-sig-mixed-models-request at r-project.org> het volgende geschreven:
> 
> Send R-sig-mixed-models mailing list submissions to
>    r-sig-mixed-models at r-project.org
> 
> To subscribe or unsubscribe via the World Wide Web, visit
>    https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> or, via email, send a message with subject or body 'help' to
>    r-sig-mixed-models-request at r-project.org
> 
> You can reach the person managing the list at
>    r-sig-mixed-models-owner at r-project.org
> 
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-sig-mixed-models digest..."
> 
> 
> Today's Topics:
> 
>   1. fixed effects estimates too small in binomial GLMM with    low
>      "success"-rate (Henrik
>   2. Re: fixed effects estimates too small in binomial GLMM with
>      low "success"-rate (Ben Bolker)
>   3. Re: fixed effects estimates too small in binomial GLMM    with
>      low "success"-rate (Henrik Singmann)
>   4. "parm" parameter in lme4::confint.merMod (Jake Westfall)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Sun, 18 May 2014 00:07:13 +0200
> From: Henrik Singmann <henrik.singmann at psychologie.uni-freiburg.de>
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] fixed effects estimates too small in binomial GLMM
>    with    low "success"-rate
> Message-ID: <5377DD91.9030705 at psychologie.uni-freiburg.de>
> Content-Type: text/plain; charset=ISO-8859-15; format=flowed
> 
> Dear list,
> 
> I would really appreciate your input on an issue in which the fixed effects estimates of a binomial GLMM with relatively low rate of "successes" are too low (compared to the observed effect) by around .5%.
> 
> The data comes from six comparable experiments in which we measured participants' performance errors on a series of trials in a design with one factor with two levels ("a" and "b"). Overall the difference in error rate between the two levels was very small (~0.5%), but when analyzing all data (with participant and experiment as random effects) the effect of factor reached significance (p = .03), which is also consistent with other published data.
> 
> However, the estimated effects are around .5% lower than the observed mean error rates indicating that the model may perhaps not adequately describe the data. Furthermore, I also get a convergence warning ("Model failed to converge with max|grad| = 0.0013716 (tol = 0.001)").
> 
> ####### code 1 ##########
> require(lme4)
> dat <- read.table("http://pastebin.com/raw.php?i=KWjD5EG3")
> dat$experiment <- factor(dat$experiment)
> 
> # observed effects
> aggregate(errors ~ factor, dat, mean) #unweighted
> ##   factor     errors
> ## 1      a 0.02266914
> ## 2      b 0.02772540
> by(dat, dat$factor, function(x) with(x, sum(errors*weights)/sum(weights))) #weighted
> ## dat$factor: a
> ## [1] 0.0222343
> ## ------------------------------------------------------------
> ## dat$factor: b
> ## [1] 0.02777651
> 
> m1 <- glmer(errors ~ factor + (factor|id) + (1|experiment), dat, weights = dat$weights, family = binomial)
> ## Warnmeldung:
> ## In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
> ##   Model failed to converge with max|grad| = 0.0013716 (tol = 0.001)
> coef(summary(m1))[2,, drop = FALSE]
> ##          Estimate Std. Error  z value   Pr(>|z|)
> ## factorb 0.1788881 0.08005533 2.234556 0.02544653
> 
> # estimated effect:
> logistic <- function(x) 1/(1+exp(-x))
> logistic(cumsum(fixef(m1)))
> ## (Intercept)     factorb
> ##  0.01825229  0.02174991
> 
> ####### end code 1 ##########
> 
> Interestingly, I can replicate this behavior when simulating binomial data similar to the one I am having. When the overall mean is relatively small, the estimates from the GLMM are consistently too low. When increasing the overall mean, this consistent bias in the estimated effect seems to go away (and the observed effect gets larger due to the non-linear nature of the logistic model). The code of the simulation for one data set follows below.
> 
> My question is: Does anyone have an idea what I could do to reach a better agreement between observed and predicted values? I tried different link functions, but that didn't help. And I am unsure whether I can trust my model given this discrepancy.
> 
> Thanks a lot,
> Henrik
> 
> 
> ##### code 2 #####
> 
> mu <- -3.7  # when larger, problem disappears
> n_experiments <- 6
> n_participants <- 20
> n_trials <- 400
> sigma_effect <- 0.2
> effect_size <- 0.4
> sigma_p <- 0.7
> sigma_e <- 0.1
> prob_level <- 0.5
> 
> # create data:
> df <- expand.grid(experiment = factor(seq(1, n_experiments)), id = factor(seq(1, n_participants)), factor = c(-1, 1))
> df$id <- with(df, experiment:id)
> df <- merge(df, data.frame(experiment = seq(1, n_experiments), re_e = rnorm(n_experiments, sd = sigma_e)))
> df <- merge(df, data.frame(id = levels(df$id), re_p = rnorm(length(levels(df$id)), sd = sigma_p)))
> df <- merge(df, data.frame(id = levels(df$id), re_a = rnorm(length(levels(df$id)), sd = sigma_effect)))
> df <- with(df, df[order(id, factor),])
> tmp <- rbinom(n_experiments*n_participants, n_trials, prob_level)
> tmp <- rep(tmp, each = 2)
> tmp[c(FALSE, TRUE)] <- n_trials-tmp[c(FALSE, TRUE)]
> df$errors <- mapply(function(x, y) rbinom(1, y, x), with(df, logistic(mu+re_p+(factor*(effect_size/2)+factor*re_a)+re_e)), tmp)/tmp
> df$weights <- tmp
> df$factor <- factor(df$factor)
> 
> aggregate(errors ~ factor, df, mean) #unweighted
> ##   factor     errors
> ## 1     -1 0.02352719
> ## 2      1 0.03718523
> 
> s1 <- glmer(errors ~ factor + (factor|id) + (1|experiment), df, weights = df$weights, family = binomial)
> ## Warnmeldung:
> ## In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
> ##   Model failed to converge with max|grad| = 0.0130648 (tol = 0.001)
> coef(summary(s1))[2,, drop = FALSE]
> #          Estimate Std. Error  z value     Pr(>|z|)
> # factor1 0.4765694 0.07578773 6.288213 3.211416e-10
> 
> logistic(cumsum(fixef(s1)))
> # (Intercept)     factor1
> #  0.01849934  0.02946117
> 
> #### end code 2 ####
> 
> 
> -- 
> Dr. Henrik Singmann
> Albert-Ludwigs-Universit?t Freiburg, Germany
> http://www.psychologie.uni-freiburg.de/Members/singmann
> 
> 
> 
> ------------------------------
> 
> Message: 2
> Date: Sat, 17 May 2014 18:19:05 -0400
> From: Ben Bolker <bbolker at gmail.com>
> To: Henrik Singmann <henrik.singmann at psychologie.uni-freiburg.de>,
>    r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] fixed effects estimates too small in binomial
>    GLMM with low "success"-rate
> Message-ID: <5377E059.2050800 at gmail.com>
> Content-Type: text/plain; charset=ISO-8859-15
> 
>> On 14-05-17 06:07 PM, Henrik Singmann wrote:
>> Dear list,
>> 
>> I would really appreciate your input on an issue in which the fixed
>> effects estimates of a binomial GLMM with relatively low rate of
>> "successes" are too low (compared to the observed effect) by around .5%.
> 
>  Don't know, but one comment and one thought:
> 
>  (1) the convergence warning goes away with the most recent version
> of lme4 (1.1-7)
> 
>  (2) I'm _guessing_ that you will find that this phenomenon is
> due to Jensen's effect (i.e., a nonlinear averaging effect) --
> that would mean its magnitude should be proportional to the
> random-effects variances and to the strength of the nonlinear
> (inverse link) relationship.
>  (I think there was a similar, possibly neglected, question along
> these lines on the mailing list earlier ...)
> 
>  Ben Bolker
> 
> 
> 
> ------------------------------
> 
> Message: 3
> Date: Sun, 18 May 2014 00:38:20 +0200
> From: Henrik Singmann <henrik.singmann at psychologie.uni-freiburg.de>
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] fixed effects estimates too small in binomial
>    GLMM    with low "success"-rate
> Message-ID: <5377E4DC.6080809 at psychologie.uni-freiburg.de>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> 
> 
> 
> Am 18.05.2014 00:19, schrieb Ben Bolker:
>> On 14-05-17 06:07 PM, Henrik Singmann wrote:
>>> Dear list,
>>> 
>>> I would really appreciate your input on an issue in which the fixed
>>> effects estimates of a binomial GLMM with relatively low rate of
>>> "successes" are too low (compared to the observed effect) by around .5%.
>> 
>>   Don't know, but one comment and one thought:
>> 
>>   (1) the convergence warning goes away with the most recent version
>> of lme4 (1.1-7)
> 
> This is true, thanks.
>> 
>>   (2) I'm _guessing_ that you will find that this phenomenon is
>> due to Jensen's effect (i.e., a nonlinear averaging effect) --
>> that would mean its magnitude should be proportional to the
>> random-effects variances and to the strength of the nonlinear
>> (inverse link) relationship.
>>   (I think there was a similar, possibly neglected, question along
>> these lines on the mailing list earlier ...)
> 
> Thanks for the pointer. In fact, when entering experiment as fixed effect (which removes the random effect with the smallest variance), the estimates are way more precise. They are almost perfect. Perhaps I should then use this approach (although I would prefer treating experiment as random). Or what would you do?
> 
> 
>> 
>>   Ben Bolker
> 
> -- 
> Dr. Henrik Singmann
> Albert-Ludwigs-Universit?t Freiburg, Germany
> http://www.psychologie.uni-freiburg.de/Members/singmann
> 
> 
> 
> ------------------------------
> 
> Message: 4
> Date: Sun, 18 May 2014 00:03:52 -0600
> From: Jake Westfall <jake987722 at hotmail.com>
> To: "r-sig-mixed-models at r-project.org"
>    <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] "parm" parameter in lme4::confint.merMod
> Message-ID: <BAY172-W2375062D4CFCE0B6E5F9CBCB330 at phx.gbl>
> Content-Type: text/plain
> 
> Hi all,
> 
> According to the documentation, in confint.merMod() you can use the "parm" parameter to get confidence intervals for only a specified subset of parameters by indicating the integer positions of the desired parameters. But how is one supposed to know which integer positions correspond with which parameters? I poked around a little but found nothing in the methods or slots of the merMod object that indicates this, and nothing in the documentation detailing this. Have I missed something? I guess you can call the function leaving that argument at its default, and the order of the parameters in that output will correspond with the integer positions, but this approach would seem to defeat the point of being able to specify particular subsets of parameters...
> 
> Jake                         
>    [[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------
> 
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> End of R-sig-mixed-models Digest, Vol 89, Issue 22
> **************************************************


From bbolker at gmail.com  Mon May 19 15:37:42 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 19 May 2014 09:37:42 -0400
Subject: [R-sig-ME] fixed effects estimates too small in binomial GLMM
 with low "success"-rate (
In-Reply-To: <17351565-9F9B-4A64-9918-437DA011F0DE@uu.nl>
References: <mailman.3.1400407203.2793.r-sig-mixed-models@r-project.org>
	<17351565-9F9B-4A64-9918-437DA011F0DE@uu.nl>
Message-ID: <537A0926.8000905@gmail.com>


  Just to give one more comment (in lieu of actually doing the work): if
my conjecture is correct (that it's a Jensen's inequality issue), then
the comparison of means on the linear predictor scale would be correct.
 The problem is that with a lot of GLMM problems you can't sensibly look
at the raw data on the linear predictor scale -- e.g. you can't
logit-transform binary data and get anything sensible.  You could try to
work out the expected effect of nonlinear averaging/Jensen's inequality
analytically or numerically and see if it matched up with the observed
discrepancy ...



On 14-05-19 06:22 AM, Solis wrote:
> Dear Ben and others on the list,
> 
> The 'possibly neglected' mail exchange is probably this one; 
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021919.html
> 
> I hope the link helps ( I'm about to board a plane) but I'm excited
> to see my question is picked up and might have an answer.
> 
> Kind regards,
> 
> Tom
> 
>> Op 18 mei 2014 om 12:00 heeft
>> <r-sig-mixed-models-request at r-project.org> het volgende
>> geschreven:
>> 
>> Send R-sig-mixed-models mailing list submissions to 
>> r-sig-mixed-models at r-project.org
>> 
>> To subscribe or unsubscribe via the World Wide Web, visit 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models or, via
>> email, send a message with subject or body 'help' to 
>> r-sig-mixed-models-request at r-project.org
>> 
>> You can reach the person managing the list at 
>> r-sig-mixed-models-owner at r-project.org
>> 
>> When replying, please edit your Subject line so it is more
>> specific than "Re: Contents of R-sig-mixed-models digest..."
>> 
>> 
>> Today's Topics:
>> 
>> 1. fixed effects estimates too small in binomial GLMM with    low 
>> "success"-rate (Henrik 2. Re: fixed effects estimates too small in
>> binomial GLMM with low "success"-rate (Ben Bolker) 3. Re: fixed
>> effects estimates too small in binomial GLMM    with low
>> "success"-rate (Henrik Singmann) 4. "parm" parameter in
>> lme4::confint.merMod (Jake Westfall)
>> 
>> 
>> ----------------------------------------------------------------------
>>
>>
>> 
Message: 1
>> Date: Sun, 18 May 2014 00:07:13 +0200 From: Henrik Singmann
>> <henrik.singmann at psychologie.uni-freiburg.de> To:
>> r-sig-mixed-models at r-project.org Subject: [R-sig-ME] fixed effects
>> estimates too small in binomial GLMM with    low "success"-rate 
>> Message-ID: <5377DD91.9030705 at psychologie.uni-freiburg.de> 
>> Content-Type: text/plain; charset=ISO-8859-15; format=flowed
>> 
>> Dear list,
>> 
>> I would really appreciate your input on an issue in which the fixed
>> effects estimates of a binomial GLMM with relatively low rate of
>> "successes" are too low (compared to the observed effect) by around
>> .5%.
>> 
>> The data comes from six comparable experiments in which we measured
>> participants' performance errors on a series of trials in a design
>> with one factor with two levels ("a" and "b"). Overall the
>> difference in error rate between the two levels was very small
>> (~0.5%), but when analyzing all data (with participant and
>> experiment as random effects) the effect of factor reached
>> significance (p = .03), which is also consistent with other
>> published data.
>> 
>> However, the estimated effects are around .5% lower than the
>> observed mean error rates indicating that the model may perhaps not
>> adequately describe the data. Furthermore, I also get a convergence
>> warning ("Model failed to converge with max|grad| = 0.0013716 (tol
>> = 0.001)").
>> 
>> ####### code 1 ########## require(lme4) dat <-
>> read.table("http://pastebin.com/raw.php?i=KWjD5EG3") dat$experiment
>> <- factor(dat$experiment)
>> 
>> # observed effects aggregate(errors ~ factor, dat, mean)
>> #unweighted ##   factor     errors ## 1      a 0.02266914 ## 2
>> b 0.02772540 by(dat, dat$factor, function(x) with(x,
>> sum(errors*weights)/sum(weights))) #weighted ## dat$factor: a ##
>> [1] 0.0222343 ##
>> ------------------------------------------------------------ ##
>> dat$factor: b ## [1] 0.02777651
>> 
>> m1 <- glmer(errors ~ factor + (factor|id) + (1|experiment), dat,
>> weights = dat$weights, family = binomial) ## Warnmeldung: ## In
>> checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
>> : ##   Model failed to converge with max|grad| = 0.0013716 (tol =
>> 0.001) coef(summary(m1))[2,, drop = FALSE] ##          Estimate
>> Std. Error  z value   Pr(>|z|) ## factorb 0.1788881 0.08005533
>> 2.234556 0.02544653
>> 
>> # estimated effect: logistic <- function(x) 1/(1+exp(-x)) 
>> logistic(cumsum(fixef(m1))) ## (Intercept)     factorb ##
>> 0.01825229  0.02174991
>> 
>> ####### end code 1 ##########
>> 
>> Interestingly, I can replicate this behavior when simulating
>> binomial data similar to the one I am having. When the overall mean
>> is relatively small, the estimates from the GLMM are consistently
>> too low. When increasing the overall mean, this consistent bias in
>> the estimated effect seems to go away (and the observed effect gets
>> larger due to the non-linear nature of the logistic model). The
>> code of the simulation for one data set follows below.
>> 
>> My question is: Does anyone have an idea what I could do to reach a
>> better agreement between observed and predicted values? I tried
>> different link functions, but that didn't help. And I am unsure
>> whether I can trust my model given this discrepancy.
>> 
>> Thanks a lot, Henrik
>> 
>> 
>> ##### code 2 #####
>> 
>> mu <- -3.7  # when larger, problem disappears n_experiments <- 6 
>> n_participants <- 20 n_trials <- 400 sigma_effect <- 0.2 
>> effect_size <- 0.4 sigma_p <- 0.7 sigma_e <- 0.1 prob_level <- 0.5
>> 
>> # create data: df <- expand.grid(experiment = factor(seq(1,
>> n_experiments)), id = factor(seq(1, n_participants)), factor =
>> c(-1, 1)) df$id <- with(df, experiment:id) df <- merge(df,
>> data.frame(experiment = seq(1, n_experiments), re_e =
>> rnorm(n_experiments, sd = sigma_e))) df <- merge(df, data.frame(id
>> = levels(df$id), re_p = rnorm(length(levels(df$id)), sd =
>> sigma_p))) df <- merge(df, data.frame(id = levels(df$id), re_a =
>> rnorm(length(levels(df$id)), sd = sigma_effect))) df <- with(df,
>> df[order(id, factor),]) tmp <- rbinom(n_experiments*n_participants,
>> n_trials, prob_level) tmp <- rep(tmp, each = 2) tmp[c(FALSE, TRUE)]
>> <- n_trials-tmp[c(FALSE, TRUE)] df$errors <- mapply(function(x, y)
>> rbinom(1, y, x), with(df,
>> logistic(mu+re_p+(factor*(effect_size/2)+factor*re_a)+re_e)),
>> tmp)/tmp df$weights <- tmp df$factor <- factor(df$factor)
>> 
>> aggregate(errors ~ factor, df, mean) #unweighted ##   factor
>> errors ## 1     -1 0.02352719 ## 2      1 0.03718523
>> 
>> s1 <- glmer(errors ~ factor + (factor|id) + (1|experiment), df,
>> weights = df$weights, family = binomial) ## Warnmeldung: ## In
>> checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
>> : ##   Model failed to converge with max|grad| = 0.0130648 (tol =
>> 0.001) coef(summary(s1))[2,, drop = FALSE] #          Estimate Std.
>> Error  z value     Pr(>|z|) # factor1 0.4765694 0.07578773 6.288213
>> 3.211416e-10
>> 
>> logistic(cumsum(fixef(s1))) # (Intercept)     factor1 #  0.01849934
>> 0.02946117
>> 
>> #### end code 2 ####
>> 
>> 
>> -- Dr. Henrik Singmann Albert-Ludwigs-Universit?t Freiburg,
>> Germany http://www.psychologie.uni-freiburg.de/Members/singmann
>> 
>> 
>> 
>> ------------------------------
>> 
>> Message: 2 Date: Sat, 17 May 2014 18:19:05 -0400 From: Ben Bolker
>> <bbolker at gmail.com> To: Henrik Singmann
>> <henrik.singmann at psychologie.uni-freiburg.de>, 
>> r-sig-mixed-models at r-project.org Subject: Re: [R-sig-ME] fixed
>> effects estimates too small in binomial GLMM with low
>> "success"-rate Message-ID: <5377E059.2050800 at gmail.com> 
>> Content-Type: text/plain; charset=ISO-8859-15
>> 
>>> On 14-05-17 06:07 PM, Henrik Singmann wrote: Dear list,
>>> 
>>> I would really appreciate your input on an issue in which the
>>> fixed effects estimates of a binomial GLMM with relatively low
>>> rate of "successes" are too low (compared to the observed effect)
>>> by around .5%.
>> 
>> Don't know, but one comment and one thought:
>> 
>> (1) the convergence warning goes away with the most recent version 
>> of lme4 (1.1-7)
>> 
>> (2) I'm _guessing_ that you will find that this phenomenon is due
>> to Jensen's effect (i.e., a nonlinear averaging effect) -- that
>> would mean its magnitude should be proportional to the 
>> random-effects variances and to the strength of the nonlinear 
>> (inverse link) relationship. (I think there was a similar, possibly
>> neglected, question along these lines on the mailing list earlier
>> ...)
>> 
>> Ben Bolker
>> 
>> 
>> 
>> ------------------------------
>> 
>> Message: 3 Date: Sun, 18 May 2014 00:38:20 +0200 From: Henrik
>> Singmann <henrik.singmann at psychologie.uni-freiburg.de> To:
>> r-sig-mixed-models at r-project.org Subject: Re: [R-sig-ME] fixed
>> effects estimates too small in binomial GLMM    with low
>> "success"-rate Message-ID:
>> <5377E4DC.6080809 at psychologie.uni-freiburg.de> Content-Type:
>> text/plain; charset=ISO-8859-1; format=flowed
>> 
>> 
>> 
>> Am 18.05.2014 00:19, schrieb Ben Bolker:
>>> On 14-05-17 06:07 PM, Henrik Singmann wrote:
>>>> Dear list,
>>>> 
>>>> I would really appreciate your input on an issue in which the
>>>> fixed effects estimates of a binomial GLMM with relatively low
>>>> rate of "successes" are too low (compared to the observed
>>>> effect) by around .5%.
>>> 
>>> Don't know, but one comment and one thought:
>>> 
>>> (1) the convergence warning goes away with the most recent
>>> version of lme4 (1.1-7)
>> 
>> This is true, thanks.
>>> 
>>> (2) I'm _guessing_ that you will find that this phenomenon is due
>>> to Jensen's effect (i.e., a nonlinear averaging effect) -- that
>>> would mean its magnitude should be proportional to the 
>>> random-effects variances and to the strength of the nonlinear 
>>> (inverse link) relationship. (I think there was a similar,
>>> possibly neglected, question along these lines on the mailing
>>> list earlier ...)
>> 
>> Thanks for the pointer. In fact, when entering experiment as fixed
>> effect (which removes the random effect with the smallest
>> variance), the estimates are way more precise. They are almost
>> perfect. Perhaps I should then use this approach (although I would
>> prefer treating experiment as random). Or what would you do?
>> 
>> 
>>> 
>>> Ben Bolker
>> 
>> -- Dr. Henrik Singmann Albert-Ludwigs-Universit?t Freiburg,
>> Germany http://www.psychologie.uni-freiburg.de/Members/singmann
>> 
>> 
>> 
>> ------------------------------
>> 
>> Message: 4 Date: Sun, 18 May 2014 00:03:52 -0600 From: Jake
>> Westfall <jake987722 at hotmail.com> To:
>> "r-sig-mixed-models at r-project.org" 
>> <r-sig-mixed-models at r-project.org> Subject: [R-sig-ME] "parm"
>> parameter in lme4::confint.merMod Message-ID:
>> <BAY172-W2375062D4CFCE0B6E5F9CBCB330 at phx.gbl> Content-Type:
>> text/plain
>> 
>> Hi all,
>> 
>> According to the documentation, in confint.merMod() you can use the
>> "parm" parameter to get confidence intervals for only a specified
>> subset of parameters by indicating the integer positions of the
>> desired parameters. But how is one supposed to know which integer
>> positions correspond with which parameters? I poked around a little
>> but found nothing in the methods or slots of the merMod object that
>> indicates this, and nothing in the documentation detailing this.
>> Have I missed something? I guess you can call the function leaving
>> that argument at its default, and the order of the parameters in
>> that output will correspond with the integer positions, but this
>> approach would seem to defeat the point of being able to specify
>> particular subsets of parameters...
>> 
>> Jake [[alternative HTML version deleted]]
>> 
>> 
>> 
>> ------------------------------
>> 
>> _______________________________________________ R-sig-mixed-models
>> mailing list R-sig-mixed-models at r-project.org 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>> 
>> End of R-sig-mixed-models Digest, Vol 89, Issue 22 
>> **************************************************
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Mon May 19 19:32:39 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 19 May 2014 13:32:39 -0400
Subject: [R-sig-ME] starting value setting in glmer
In-Reply-To: <CAMev3zx=RZ1WNWVjpTspJ3VkFmo2CAQqP3gZhK_LtJLHSNn_3w@mail.gmail.com>
References: <CAMev3zx=RZ1WNWVjpTspJ3VkFmo2CAQqP3gZhK_LtJLHSNn_3w@mail.gmail.com>
Message-ID: <537A4037.9030509@gmail.com>


  [It's admittedly a judgement call, but it is in general preferable,
  and especially if you think your problem *might* be a general one,
  to send first to the r-sig-mixed-models at r-project.org mailing list,
  where (a) the questions and answers will be visible to everyone, and
  archived publicly for future reference, and (b) someone other than
  the lme4 maintainers might be able to answer.  Forwarding ...]

On 14-05-19 11:39 AM, Yuanbo Song wrote:
> Hi Ben,
> 
> I am currently using lme4.0 for my research and I really appreciated
> your and other author's contribution for this package. Unfortunately, I
> encoutered a problem about starting value setting when I used glmer
> function. I have five fixed effects and seven random effects, so I set
> my starting value as "start=list(fixef=start.fixef, theta=start.theta)",
> where start.fixef is a vector of 6 elements (including intercept and
> five fixed effects) and start.theta is a vector of 7 elements indicating
> the 7 random effects. However, I got a warning message saying:
> 
>  In sort(names(start)) == sort(names(FL)):
>  longer object length is not a multiple of shorter object length.
> 
> I do not quite understand why I got this warning message. Did I
> misunderstand the syntax of "start = "? If so, what should be done in my
> situation.
> 
> Thanks,
> Bob

This looks like a bug in lme4.0; at first glance it is just a
false-positive test, but on second glance (looking through the
internals of lme4.0) I'm not at all sure that I would trust
the "start" argument of lme4.0 at all for GLMMs without significant
testing on your side to ensure that it's giving sensible answers.
On a cursory glance, it looks like a lot of the code was copied
from the LMM functionality and may never have been tested properly
for GLMMs.  (It's possible that the code is cleverer than I think
and is actually working just fine ...)

  However, it looks in this case as though you're using the
syntax from lme4 >= 1.0.0 (not lme4.0 syntax): if you read the
help page for ?glmer in lme4.0 you'll see it's quite a bit different.

  More broadly, I would wonder why you're using the old version
of the package -- there *are* reasons to use it (that's why we
made it available in the first place), but I would strongly encourage
anyone doing a new analysis to use the most recent version(s) -- and
if they don't work for you for some reason, please let the maintainers
know so that we can try to resolve those issues!

  cheers
    Ben Bolker


From bbolker at gmail.com  Mon May 19 20:08:02 2014
From: bbolker at gmail.com (lme4 maintainer)
Date: Mon, 19 May 2014 14:08:02 -0400
Subject: [R-sig-ME] [Lme4-authors] A Question Regarding a Warning
 Message Generated from Fitting a 'glmer' Model
In-Reply-To: <CADMdyXNpLBJnvcTOD7mpbh_rR595w8E9b0YGr+tdJQmjWCUrfg@mail.gmail.com>
References: <CADMdyXNpLBJnvcTOD7mpbh_rR595w8E9b0YGr+tdJQmjWCUrfg@mail.gmail.com>
Message-ID: <537A4882.5050609@lists.r-forge.r-project.org>

  Please see http://thread.gmane.org/gmane.comp.lang.r.lme4.devel/11893

  cheers
    Ben Bolker

On 14-05-19 02:04 AM, Chia-Chieh Lin wrote:
> Dear LME4 Authors,
> 
> I have a dataset with 200 subjects and total number of observations of
> 1113. I use the 'glmer' function to fit a mixed-effects linear model to
> a binary response. I then get the following warning message:
> 
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  ... :
>   Model failed to converge with max|grad| = 0.00311153 (tol = 0.001)
> 
> 
> May I know how I could remedy the problem?
> 
> Thank you very much for your time. Your response is highly appreciated.
> 
> Best regards,
> Chia-Chieh Lin
> 
> 
> 
> _______________________________________________
> Lme4-authors mailing list
> Lme4-authors at lists.r-forge.r-project.org
> https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/lme4-authors
>


From jake987722 at hotmail.com  Mon May 19 20:24:10 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Mon, 19 May 2014 12:24:10 -0600
Subject: [R-sig-ME] "parm" parameter in lme4::confint.merMod
In-Reply-To: <5378C1CC.1090408@gmail.com>
References: <BAY172-W2375062D4CFCE0B6E5F9CBCB330@phx.gbl>,
	<53789ACC.90208@psychologie.uni-freiburg.de>,
	<5378C1CC.1090408@gmail.com>
Message-ID: <BAY172-W21564705A9B3C21EC68FE9CB320@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140519/651b1348/attachment.pl>

From bbolker at gmail.com  Mon May 19 20:28:47 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 19 May 2014 14:28:47 -0400
Subject: [R-sig-ME] "parm" parameter in lme4::confint.merMod
In-Reply-To: <BAY172-W21564705A9B3C21EC68FE9CB320@phx.gbl>
References: <BAY172-W2375062D4CFCE0B6E5F9CBCB330@phx.gbl>,
	<53789ACC.90208@psychologie.uni-freiburg.de>,
	<5378C1CC.1090408@gmail.com>
	<BAY172-W21564705A9B3C21EC68FE9CB320@phx.gbl>
Message-ID: <537A4D5F.70802@gmail.com>

On 14-05-19 02:24 PM, Jake Westfall wrote:
> Thanks Ben, it looks like a slight tweaking of your command should
> work:
> 
> c(names(getME(sig0,c("theta"))), names(fixef(sig0)))
> 
> Although (not sure if this is the right thread for this but here
> goes) I'm having trouble verifying that the order matches because,
> from what I can tell, confint.merMod() does not appear to work for
> profile confidence intervals for models that include covariance
> parameters between random terms. I always get an error about
> "Starting values violate bounds" and additionally some warnings about
> non-monotonic profiles. If I fix the covariance at 0 then
> confint.merMod() works fine for profile confidence intervals. I could
> send more info if you'd like, but it sounds like maybe you are aware
> of this already. Jake


  I'm vaguely aware of it, but it would be great if you were able to add
an issue (with simple reproducible example) at
https://github.com/lme4/lme4/issues ...

  cheers
    Ben


>> Date: Sun, 18 May 2014 10:21:00 -0400 From: bbolker at gmail.com To:
>> henrik.singmann at psychologie.uni-freiburg.de;
>> r-sig-mixed-models at r-project.org Subject: Re: [R-sig-ME] "parm"
>> parameter in lme4::confint.merMod
>> 
>> On 14-05-18 07:34 AM, Henrik Singmann wrote:
>>> Hi Jake,
>>> 
>>> I think fixef() or names(fixef()) gives you what you want:
>> 
>> I don't think it gets you everything.  Your solution works in the
>> method="Wald" case because method="Wald" only returns the fixed
>> effects, so one only has to keep track of those positions.
>> method="profile" and "boot" include both variance-covariance and
>> fixed effect parameters, in that order. Some possibilities are
>> 
>> names(unlist(getME(m1,c("theta","fixef")))) ## this gives names of
>> 'theta' parameters (i.e. Cholesky factors) ## rather than standard
>> deviations/correlations, but I believe ## the order is the same
>> 
>> ## theta parameters only (hidden function) lme4:::tnames(m1)
>> 
>> require(lme4) data(md_16.4, package = "afex") m1 <-  lmer(induct ~
>> cond*cog + (cog|room:cond), md_16.4)
>> 
>> ## I actually had some problems with profiling this example -- ##
>> not quite sure why.  Will have to look into it. pp <- profile(m1)
>> 
>> 
>> confint(m1,parm=3)
>>> fixef(m1) ## (Intercept)       cond1         cog   cond1:cog ##
>>> -6.14153043 10.32001485  0.66722061 -0.02520666
>>> 
>>> confint(m1, parm = c(3,2), method="Wald") ##             2.5 %
>>> 97.5 % ## cog     0.2498764  1.084565 ## cond1 -18.4845630
>>> 39.124593
>>> 
>>> I hope I didn't misunderstand your question, Henrik
>>> 
>>> 
>>> Am 18.05.2014 08:03, schrieb Jake Westfall:
>>>> Hi all,
>>>> 
>>>> According to the documentation, in confint.merMod() you can use
>>>> the "parm" parameter to get confidence intervals for only a
>>>> specified subset of parameters by indicating the integer
>>>> positions of the desired parameters. But how is one supposed to
>>>> know which integer positions correspond with which parameters?
>>>> I poked around a little but found nothing in the methods or
>>>> slots of the merMod object that indicates this, and nothing in
>>>> the documentation detailing this. Have I missed something? I
>>>> guess you can call the function leaving that argument at its
>>>> default, and the order of the parameters in that output will
>>>> correspond with the integer positions, but this approach would
>>>> seem to defeat the point of being able to specify particular 
>>>> subsets of parameters...
>>>> 
>>>> Jake [[alternative HTML version deleted]]
>>>> 
>>> 
>> 
>> _______________________________________________ 
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>  [[alternative HTML version deleted]]
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From jake987722 at hotmail.com  Mon May 19 20:31:21 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Mon, 19 May 2014 12:31:21 -0600
Subject: [R-sig-ME] "parm" parameter in lme4::confint.merMod
In-Reply-To: <537A4D5F.70802@gmail.com>
References: <BAY172-W2375062D4CFCE0B6E5F9CBCB330@phx.gbl>, ,
	<53789ACC.90208@psychologie.uni-freiburg.de>, ,
	<5378C1CC.1090408@gmail.com>,
	<BAY172-W21564705A9B3C21EC68FE9CB320@phx.gbl>,
	<537A4D5F.70802@gmail.com>
Message-ID: <BAY172-W23D6DF549455829E24D25CCB320@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140519/8a8eeab8/attachment.pl>

From c.b.stride at sheffield.ac.uk  Mon May 19 22:37:55 2014
From: c.b.stride at sheffield.ac.uk (Chris Stride)
Date: Mon, 19 May 2014 21:37:55 +0100
Subject: [R-sig-ME] Regions of significance plot for multilevel logistic
	regression model
In-Reply-To: <BAY172-W23D6DF549455829E24D25CCB320@phx.gbl>
References: <BAY172-W2375062D4CFCE0B6E5F9CBCB330@phx.gbl>, ,
	<53789ACC.90208@psychologie.uni-freiburg.de>, ,
	<5378C1CC.1090408@gmail.com>,
	<BAY172-W21564705A9B3C21EC68FE9CB320@phx.gbl>,
	<537A4D5F.70802@gmail.com>
	<BAY172-W23D6DF549455829E24D25CCB320@phx.gbl>
Message-ID: <537A6BA3.5070202@sheffield.ac.uk>

Hi all
To avoid re-inventing the wheel I was wondering if anyone could point me towards some R code to plot simple slopes and the region(s) of significance for an interaction between two continuous higher-level predictors in a 2-level multilevel logistic regression model?
cheers
Chris


From bbolker at gmail.com  Tue May 20 00:00:19 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 19 May 2014 18:00:19 -0400
Subject: [R-sig-ME] Regions of significance plot for multilevel logistic
 regression model
In-Reply-To: <537A6BA3.5070202@sheffield.ac.uk>
References: <BAY172-W2375062D4CFCE0B6E5F9CBCB330@phx.gbl>, ,
	<53789ACC.90208@psychologie.uni-freiburg.de>, ,
	<5378C1CC.1090408@gmail.com>,
	<BAY172-W21564705A9B3C21EC68FE9CB320@phx.gbl>,
	<537A4D5F.70802@gmail.com>	<BAY172-W23D6DF549455829E24D25CCB320@phx.gbl>
	<537A6BA3.5070202@sheffield.ac.uk>
Message-ID: <537A7EF3.8080000@gmail.com>

On 14-05-19 04:37 PM, Chris Stride wrote:
> Hi all
> To avoid re-inventing the wheel I was wondering if anyone could point me
> towards some R code to plot simple slopes and the region(s) of
> significance for an interaction between two continuous higher-level
> predictors in a 2-level multilevel logistic regression model?
> cheers
> Chris
> 

  Could you be a little more specific and possibly provide a
reproducible example?  Can you point to/have you found solutions of your
problem for either (1) a plain linear model (2) a LMM (i.e. non-logistic
but multilevel) (3) a plain logistic regression (i.e. binary response
but single-level)?  (These would all help because they would clarify
precisely what you want and because the general strategies might be
extendable to your case ...)

  Ben Bolker


From John.Morrongiello at csiro.au  Tue May 20 15:59:34 2014
From: John.Morrongiello at csiro.au (John.Morrongiello at csiro.au)
Date: Tue, 20 May 2014 13:59:34 +0000
Subject: [R-sig-ME] transformation of BLUPs to response scale
Message-ID: <2547E22D246F3945BB491BDD8257C2E77F03C399@exmbx06-cdc.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140520/4bed8e56/attachment.pl>

From julien.riou.k at gmail.com  Tue May 20 12:03:35 2014
From: julien.riou.k at gmail.com (Julien Riou)
Date: Tue, 20 May 2014 12:03:35 +0200
Subject: [R-sig-ME] Variance of predictions from lme4 models at existing
	levels?
Message-ID: <CAL49zWOkaS=ioXsxniGcLCfDvo_53TyVV_wcLzBz2ON_UyoDdA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140520/6ad810a8/attachment.pl>

From bbolker at gmail.com  Tue May 20 20:32:38 2014
From: bbolker at gmail.com (lme4 maintainer)
Date: Tue, 20 May 2014 14:32:38 -0400
Subject: [R-sig-ME] about the glmer function
In-Reply-To: <CAJ_w8UMr=pp2i+SEb-QiH_kz4GpcdAGKwg-vTTq-kCA8aD9jvQ@mail.gmail.com>
References: <CAJ_w8UMr=pp2i+SEb-QiH_kz4GpcdAGKwg-vTTq-kCA8aD9jvQ@mail.gmail.com>
Message-ID: <537B9FC6.3000305@lists.r-forge.r-project.org>


  [forwarding to r-sig-mixed-models]

  This is an unfortunate limitation of the current version of lme4.  You
could try lme4.0, which did allow AGHQ on vector-valued random effects,
or wait until we can find enough resources to (re)implement this
feature. (Or fit your model in MCMCglmm, or live with the Laplace
approximation, or ...)

https://github.com/lme4/lme4/issues/123

  sincerely
    Ben Bolker

On 14-05-20 12:29 AM, An?l AKTAS SAMUR wrote:
> Dear LME4 Authors,
> 
> My outcome variable is binomial, and I have 11 independent variables and
> a time variable. The time variable has different slopes, so I fixed it
> to |time-before| and |time-after|. I used the |lme4| package (the
> |glmer| function). I have a random intercept and two random slopes. I
> created my model like this:
> 
> |m3.glmm <- glmer(y ~ timebefore + timeafter + x1 + x2 +...+ x11 + (1+timebefore+timeafter|id),
>              data = data, family = binomial (link="logit"), nAGQ=3)|
> 
> When I used this model, I had this error:
> 
> |Error in updateGlmerDevfun(devfun, glmod$reTrms, nAGQ = nAGQ) :
> 
> nAGQ > 1 is only available for models with a single, scalar random-effects term|
> 
> 
> Could you please explain how to fit (or code) this model?
> 
> Bests,
> 
> 
> Anil


From segerfan83 at gmail.com  Tue May 20 20:59:28 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Tue, 20 May 2014 14:59:28 -0400
Subject: [R-sig-ME] Assessing Normality for Mixed Models
Message-ID: <CAHe08SgUgXykJUzV800gxbBtOTu+hiNzK-vOCF=HpyMQ+pA5cQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140520/3945d220/attachment.pl>

From hughes.dupond at gmx.de  Tue May 20 21:25:31 2014
From: hughes.dupond at gmx.de (Lionel)
Date: Tue, 20 May 2014 21:25:31 +0200
Subject: [R-sig-ME] Assessing Normality for Mixed Models
In-Reply-To: <CAHe08SgUgXykJUzV800gxbBtOTu+hiNzK-vOCF=HpyMQ+pA5cQ@mail.gmail.com>
References: <CAHe08SgUgXykJUzV800gxbBtOTu+hiNzK-vOCF=HpyMQ+pA5cQ@mail.gmail.com>
Message-ID: <537BAC2B.4090504@gmx.de>

Hi Jacob,

You should do similar normality check as you would do for linear models, 
I usually use the qqplot, you can use qqnorm(resid(model)) and 
qqline(resid(model)). Then another assumptions from linear mixed models 
is that the random effect are normally distributed with a mean of 0, you 
can use qqnorm(unlist(model)) and qqline(unlist(model)) if you have only 
one random term.

So two things should be normally distributed in linear mixed models: the 
residuals and the random effects.

When you have a low number of level in the random effects normality will 
in some case not be reached just due to the small number of levels, I am 
not aware of ways to account for this, I would either include the random 
effect as fixed effect or use simulation.

Sincerely yours,
Lionel


On 20/05/2014 20:59, AvianResearchDivision wrote:
> Hi All,
>
> After doing some extensive googling, searching for ways to assess normality
> for linear mixed models, I can honestly say my head is swimming in
> different proposed techniques that may or may not be valid.  Also, when
> reading the literature, I find that few studies that use linear mixed
> models and random regression actually explicitly address how they assess
> normality.  What are the rules with normality with mixed models (if there
> are any) and what are your techniques to assess normality?  Any input that
> you can provide would be great and hopefully we help to settle my mind on
> this issue.
>
> Thank you,
> Jacob
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From hughes.dupond at gmx.de  Tue May 20 21:34:16 2014
From: hughes.dupond at gmx.de (Lionel)
Date: Tue, 20 May 2014 21:34:16 +0200
Subject: [R-sig-ME] Assessing Normality for Mixed Models
In-Reply-To: <537BAC2B.4090504@gmx.de>
References: <CAHe08SgUgXykJUzV800gxbBtOTu+hiNzK-vOCF=HpyMQ+pA5cQ@mail.gmail.com>
	<537BAC2B.4090504@gmx.de>
Message-ID: <537BAE38.9060404@gmx.de>

Sorry forgot one function in the code for the check of random effect 
normality:
qqnorm(unlist(ranef(model)))
qqline(unlist(ranef(model)))

Lionel

On 20/05/2014 21:25, Lionel wrote:
> Hi Jacob,
>
> You should do similar normality check as you would do for linear 
> models, I usually use the qqplot, you can use qqnorm(resid(model)) and 
> qqline(resid(model)). Then another assumptions from linear mixed 
> models is that the random effect are normally distributed with a mean 
> of 0, you can use qqnorm(unlist(ranef(model))) and 
> qqline(unlist(ranef(model))) if you have only one random term.
>
> So two things should be normally distributed in linear mixed models: 
> the residuals and the random effects.
>
> When you have a low number of level in the random effects normality 
> will in some case not be reached just due to the small number of 
> levels, I am not aware of ways to account for this, I would either 
> include the random effect as fixed effect or use simulation.
>
> Sincerely yours,
> Lionel
>
>
> On 20/05/2014 20:59, AvianResearchDivision wrote:
>> Hi All,
>>
>> After doing some extensive googling, searching for ways to assess 
>> normality
>> for linear mixed models, I can honestly say my head is swimming in
>> different proposed techniques that may or may not be valid. Also, when
>> reading the literature, I find that few studies that use linear mixed
>> models and random regression actually explicitly address how they assess
>> normality.  What are the rules with normality with mixed models (if 
>> there
>> are any) and what are your techniques to assess normality?  Any input 
>> that
>> you can provide would be great and hopefully we help to settle my 
>> mind on
>> this issue.
>>
>> Thank you,
>> Jacob
>>
>>     [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From emmanuel.curis at parisdescartes.fr  Tue May 20 22:19:00 2014
From: emmanuel.curis at parisdescartes.fr (Emmanuel Curis)
Date: Tue, 20 May 2014 22:19:00 +0200
Subject: [R-sig-ME] Assessing Normality for Mixed Models
In-Reply-To: <CAHe08SgUgXykJUzV800gxbBtOTu+hiNzK-vOCF=HpyMQ+pA5cQ@mail.gmail.com>
References: <CAHe08SgUgXykJUzV800gxbBtOTu+hiNzK-vOCF=HpyMQ+pA5cQ@mail.gmail.com>
Message-ID: <20140520201900.GB12614@info124.pharmacie.univ-paris5.fr>

Hi All,

I give here a tentative response, and would be happy to be corrected
by more experimented statisticians.

When using the Y = X %*% theta + X' %*% Z + epsilon model, where Z is
the random effects matrix and epsilon the residuals, the aim is, with
the random part, to reproduce the covariance matrix of Y starting with
Z and epsilon.

A first assumption is that Z and epsilon are (multivariate) normally
distributed. A second assumption is that Z and epsilon are independant.

However, the decomposition of Y covariance matrix between X' %*% Z and
epsilon can be done in several ways, ranging from X' %*% Z = 0 and
everything is in the epsilon part --- this is the gls approach in
nlme, if I understood it correctly ---, to epsilon is iid (as in usual
linear model) and X' %*% Z explains everything else. And everything
inbetween can be wrote, with some classical kinds of matrices
(? compound symmetry ? and so on). This corresponds to the G and R
parts of the (random effects) model using SAS vocabulary [not quite
sure of the two letters right now, sorry]. Hence, other assumptions
and what exactly mean ? normally distributed ? will depend on the
exact model you write.

In lme4, the model is that epsilon is iid, and everything is in the
X'%*%Z part, which is a one-dimension Gaussian in the simplest case of
a single random effect (and no interaction with fixed effects either),
and multidimensionnal Gaussian otherwise, with a covariance matrix
depending on the exact model formula for the random part.

In (n)lme, you can have more flexibility on both random effects and
residual parts, so more combinations are possible.

So, with lme4, checks would be
 - residuals are normal
     ==> qqplot of residuals
 - residuals are independant
     ==> autocorrelation or (ei, ei+1) plots

 - residuals are homoscedastic
     ==> residuals = f( fixed effects) plots

        (keeping in mind that observed residuals are not independant,
        nor normally distributed)

   here, the situation is very similar to the linear model case,
   except that I'm not sure that (externally) studentized residuals
   follow a known distribution to improve check quality...

 - random effects are _multi-dimensionnal_ normals
     which is more difficult to assess ; marginal normality of each
     column of the matrix of random effects is necessary but not
     sufficient.

   here also, observed randoms effects are not normally distributed,
   and not independant, which means one cannot be too strict when
   graphically assessing normality

 - random effects and residuals are independant
     not quite sure, but I guess a ranef() ~ residuals() plot should
     give a hint?

For nlme, since both epsilon and random effects can be
multi-dimensionnal, for both of them we are in the second case.

The frontier between model assumption and model building is not so
clear either, at least for me: if you assume homoscedasticity of
random effects between groups, it is a model assumption to be checked,
but it can be checked by comparing it to an heteroscedastic model...

Hope this help,

PS : I've found a lot of things about ? sphericity ? assumptions and
things like that, but I'm not sure how general it is, or only to be
considered when trying to build F-tests, my impression is that it is
only for F-tests.

PS-2 : I am currently working on scripts that try to help checking
such assumptions, both for LM and LMM, leading to an HTML report for
the user; if anyone wants to help me in that development or testing,
please feel free to contact me.

On Tue, May 20, 2014 at 02:59:28PM -0400, AvianResearchDivision wrote:
? Hi All,
? 
? After doing some extensive googling, searching for ways to assess normality
? for linear mixed models, I can honestly say my head is swimming in
? different proposed techniques that may or may not be valid.  Also, when
? reading the literature, I find that few studies that use linear mixed
? models and random regression actually explicitly address how they assess
? normality.  What are the rules with normality with mixed models (if there
? are any) and what are your techniques to assess normality?  Any input that
? you can provide would be great and hopefully we help to settle my mind on
? this issue.
? 
? Thank you,
? Jacob
? 
? 	[[alternative HTML version deleted]]
? 
? _______________________________________________
? R-sig-mixed-models at r-project.org mailing list
? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From christianvanbrauner at gmail.com  Tue May 20 22:53:03 2014
From: christianvanbrauner at gmail.com (Christian Brauner)
Date: Tue, 20 May 2014 22:53:03 +0200
Subject: [R-sig-ME] Calculations and interpretation of ordered logit model
 with clmm() and related functions
Message-ID: <20140520205301.GA13787@gmail.com>

Hello all,

I fitted a proportional ordered logit model and I have some questions regarding
my interpretation of the R output and calculating certain values. I will
illustrate this with the wine dataset which can be found in the ordinal
package. The models won?t necessarily make sense but this will not have any
bearing on the case. I tried to formulate most questions as simple yes-no
questions to make it easier. I gathered that the r-sig-mixed-models help page
might be the right place to ask.

(A) Simple model:

library(ordinal)
mod1 <- clmm(rating
             ~ bottle
             + (1 | judge),
             data = wine,
             link = "logit")

summary(mod1)

> summary(mod1)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: rating ~ bottle + (1 | judge)
data:    wine

 link  threshold nobs logLik AIC    niter     max.grad cond.H 
 logit flexible  72   -80.26 184.52 607(1824) 2.50e-05 1.0e+02

Random effects:
 Groups Name        Variance Std.Dev.
 judge  (Intercept) 1.321    1.149   
Number of groups:  judge 9 

Coefficients:
        Estimate Std. Error z value Pr(>|z|)    
bottle2   1.1992     0.9652   1.242 0.214079    
bottle3   2.6115     1.0440   2.501 0.012367 *  
bottle4   2.2340     1.0174   2.196 0.028114 *  
bottle5   3.3366     1.0621   3.142 0.001680 ** 
bottle6   4.0071     1.0959   3.657 0.000256 ***
bottle7   5.9393     1.2201   4.868 1.13e-06 ***
bottle8   5.4545     1.1551   4.722 2.34e-06 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Threshold coefficients:
    Estimate Std. Error z value
1|2  -1.0696     0.8472  -1.263
2|3   2.1406     0.8945   2.393
3|4   4.9217     1.0580   4.652
4|5   6.8276     1.1852   5.761


 exp(mod1$beta)

> exp(mod$beta)
   bottle2    bottle3    bottle4    bottle5    bottle6    bottle7 
  3.317391  13.620129   9.337043  28.123588  54.987120 379.658280 
   bottle8 
233.819536

Questions for the simple model:

(1) Am I correct in assuming that the odds ratios which I get from
"exp(mod$beta)" for all the bottles are calculated with respect to
"bottle1"?

(2) If (1) is true is the reference level "bottle1" just the sum of all
the intercepts for every single bottle 1 to 8?

(3) If (1) and (2) are true are the following statements correct?:

(3.1) The odds ratio to be rated higher increases by 3.31739 if I choose
"bottle2" in comparison to "bottle1".

(3.2) The odds ratio to be rated higher increases by 13.620219 if I choose
"bottle7" in comparison to "bottle1".


(B) Additive model:

mod2 <- clmm(rating
             ~ contact
             + temp
             + (1 | judge),
             data = wine,
             link = "logit")

summary(mod2)

> summary(mod2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: rating ~ contact + temp + (1 | judge)
data:    wine

 link  threshold nobs logLik AIC    niter    max.grad cond.H 
 logit flexible  72   -81.57 177.13 331(996) 1.04e-05 2.8e+01

 Random effects:
Groups Name        Variance Std.Dev.
  judge  (Intercept) 1.279    1.131   
Number of groups:  judge 9 

Coefficients:
           Estimate Std. Error z value Pr(>|z|)    
contactyes   1.8349     0.5125   3.580 0.000344 ***
tempwarm     3.0630     0.5954   5.145 2.68e-07 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Threshold coefficients:
    Estimate Std. Error z value
1|2  -1.6237     0.6824  -2.379
2|3   1.5134     0.6038   2.507
3|4   4.2285     0.8090   5.227
4|5   6.0888     0.9725   6.261


exp(mod2$beta)

> exp(mod2$beta)
contactyes   tempwarm 
  6.264414  21.391566 

Questions for the additive model:

(1) Analogous to the non-additive model case.

(2) Is the reference level "tempcold-contactno" just the sum of all the
intercepts?

(3) Assuming I fit a simple linear model with dummy/treatment coding to the
wine dataset I would get a model matrix of the form:

        1 0 0 := tempcold + contactno
        1 1 0 := tempwarm + contactno
        1 0 1 := tempcold + contactyes
        1 1 1 := tempwarm + contactyes

How does this compare to the the clmm() model?

(3.1) How do I calculate the "x" in the following statement?: "The odds ratio
to be rated higher increases by "x" for "tempwarm-contactyes" in comparison to
"tempcold-contactno"?

(One last point: There is a related post here: 
http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression
While this post does address one point my inference algorithm is not sufficient to generalise this solution after having looked at this code for so long.)

Best,
Christian


From chris at trickysolutions.com.au  Wed May 21 04:13:17 2014
From: chris at trickysolutions.com.au (Chris Howden)
Date: Wed, 21 May 2014 12:13:17 +1000
Subject: [R-sig-ME] Assessing Normality for Mixed Models
In-Reply-To: <CAHe08SgUgXykJUzV800gxbBtOTu+hiNzK-vOCF=HpyMQ+pA5cQ@mail.gmail.com>
References: <CAHe08SgUgXykJUzV800gxbBtOTu+hiNzK-vOCF=HpyMQ+pA5cQ@mail.gmail.com>
Message-ID: <03e619c4c74e0de8920d8cea40a5177a@mail.gmail.com>

They may not be addressing normality since we don?t always expect the
residuals to be normal e.g. if we are doing a GLMM with a Poisson or
binomial error distribution.

Chris Howden B.Sc. (Hons) GStat.
Founding Partner
Evidence Based Strategic Development, IP Commercialisation and Innovation,
Data Analysis, Modelling and Training
(mobile) 0410 689 945
(skype) chris.howden
chris at trickysolutions.com.au




Disclaimer: The information in this email and any attachments to it are
confidential and may contain legally privileged information.?If you are
not the named or intended recipient, please delete this communication and
contact us immediately.?Please note you are not authorised to copy, use or
disclose this communication or any attachments without our consent.
Although this email has been checked by anti-virus software, there is a
risk that email messages may be corrupted or infected by viruses or other
interferences. No responsibility is accepted for such interference. Unless
expressly stated, the views of the writer are not those of the company.
Tricky Solutions always does our best to provide accurate forecasts and
analyses based on the data supplied, however it is possible that some
important predictors were not included in the data sent to us. Information
provided by us should not be solely relied upon when making decisions and
clients should use their own judgement.


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of
AvianResearchDivision
Sent: Wednesday, 21 May 2014 4:59 AM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Assessing Normality for Mixed Models

Hi All,

After doing some extensive googling, searching for ways to assess
normality for linear mixed models, I can honestly say my head is swimming
in different proposed techniques that may or may not be valid.  Also, when
reading the literature, I find that few studies that use linear mixed
models and random regression actually explicitly address how they assess
normality.  What are the rules with normality with mixed models (if there
are any) and what are your techniques to assess normality?  Any input that
you can provide would be great and hopefully we help to settle my mind on
this issue.

Thank you,
Jacob

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Farrar.David at epa.gov  Wed May 21 16:00:33 2014
From: Farrar.David at epa.gov (Farrar, David)
Date: Wed, 21 May 2014 14:00:33 +0000
Subject: [R-sig-ME] Assessing Normality for Mixed Models
In-Reply-To: <03e619c4c74e0de8920d8cea40a5177a@mail.gmail.com>
References: <CAHe08SgUgXykJUzV800gxbBtOTu+hiNzK-vOCF=HpyMQ+pA5cQ@mail.gmail.com>
	<03e619c4c74e0de8920d8cea40a5177a@mail.gmail.com>
Message-ID: <e6a70a5767464a70a2497460c3b6eb5e@BY2PR09MB030.namprd09.prod.outlook.com>


There could be, of course, normality or otherwise at multiple levels of variation.  
Also I would think the importance of normality (versus perhaps just not too heavy-tailed?) could depend on the inferences one wants to make.
David


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Chris Howden
Sent: Tuesday, May 20, 2014 10:13 PM
To: AvianResearchDivision; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Assessing Normality for Mixed Models

They may not be addressing normality since we don?t always expect the residuals to be normal e.g. if we are doing a GLMM with a Poisson or binomial error distribution.

Chris Howden B.Sc. (Hons) GStat.
Founding Partner
Evidence Based Strategic Development, IP Commercialisation and Innovation, Data Analysis, Modelling and Training
(mobile) 0410 689 945
(skype) chris.howden
chris at trickysolutions.com.au




Disclaimer: The information in this email and any attachments to it are confidential and may contain legally privileged information.?If you are not the named or intended recipient, please delete this communication and contact us immediately.?Please note you are not authorised to copy, use or disclose this communication or any attachments without our consent.
Although this email has been checked by anti-virus software, there is a risk that email messages may be corrupted or infected by viruses or other interferences. No responsibility is accepted for such interference. Unless expressly stated, the views of the writer are not those of the company.
Tricky Solutions always does our best to provide accurate forecasts and analyses based on the data supplied, however it is possible that some important predictors were not included in the data sent to us. Information provided by us should not be solely relied upon when making decisions and clients should use their own judgement.


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of AvianResearchDivision
Sent: Wednesday, 21 May 2014 4:59 AM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Assessing Normality for Mixed Models

Hi All,

After doing some extensive googling, searching for ways to assess normality for linear mixed models, I can honestly say my head is swimming in different proposed techniques that may or may not be valid.  Also, when reading the literature, I find that few studies that use linear mixed models and random regression actually explicitly address how they assess normality.  What are the rules with normality with mixed models (if there are any) and what are your techniques to assess normality?  Any input that you can provide would be great and hopefully we help to settle my mind on this issue.

Thank you,
Jacob

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From segerfan83 at gmail.com  Wed May 21 16:16:11 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Wed, 21 May 2014 10:16:11 -0400
Subject: [R-sig-ME] Assessing Normality for Mixed Models
In-Reply-To: <e6a70a5767464a70a2497460c3b6eb5e@BY2PR09MB030.namprd09.prod.outlook.com>
References: <CAHe08SgUgXykJUzV800gxbBtOTu+hiNzK-vOCF=HpyMQ+pA5cQ@mail.gmail.com>
	<03e619c4c74e0de8920d8cea40a5177a@mail.gmail.com>
	<e6a70a5767464a70a2497460c3b6eb5e@BY2PR09MB030.namprd09.prod.outlook.com>
Message-ID: <CAHe08Sj4GcXSHg9_cSJC31VSqCRasexQWebKD0zVPJCvCUbheQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140521/5af342a7/attachment.pl>

From guillaumechaumet at gmail.com  Wed May 21 16:57:35 2014
From: guillaumechaumet at gmail.com (guillaume chaumet)
Date: Wed, 21 May 2014 16:57:35 +0200
Subject: [R-sig-ME] ] Zero inflated mixed model with constraint (lasso or
	other)
Message-ID: <CAGg8SkKV1w0Nix3+QW4486NKRzdiuedWjhzMfxgG77DTBKaqKA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140521/4cc8ad94/attachment.pl>

From jkingston at linguist.umass.edu  Wed May 21 17:27:29 2014
From: jkingston at linguist.umass.edu (John Kingston)
Date: Wed, 21 May 2014 11:27:29 -0400
Subject: [R-sig-ME] recent message not posted
In-Reply-To: <mailman.5.1400666402.17616.r-sig-mixed-models@r-project.org>
References: <mailman.5.1400666402.17616.r-sig-mixed-models@r-project.org>
Message-ID: <20140521112729.30232wzy5aoyvsv5@umail.oit.umass.edu>

I recently sent the message repeated below to this address, but it  
hasn't appeared. I haven't attached the data set referred to in that  
message to this one. Can you let me know what's up? Thank you.

John Kingston

I have recently been running a hierarchy pf mixed effect logistic  
regression models using glmer on data collected from 28 participants  
who had to choose between one of two possible responses on each trial.  
The code for two models in the model hierarchy is pasted below, along  
with the fixed effects estimates. Yhe data file is attached to this  
message (as .csv file). As you can see, the the first model includes  
only the random effect of participant on the intercept, while the  
second model adds the random effects of participant on the slopes of  
all the fixed effects. Unsurprisingly, that model with those random  
effects fits the data significantly better:

anova(sfInd, sfIndR)

#        Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(>Chisq)
# sfInd   6 15438 15485 -7712.9    15426
# sfIndR 20 14146 14303 -7053.2    14106 1319.4     14  < 2.2e-16 ***


What does surprise/trouble me is that the second model takes an  
extremely long time to complete, and moreover, once I moved to models  
including interactions between the fixed effects, the maximum number  
of function calls had to be increased, first to 100,000, and  
eventually to 1,000,000. Does anyone have any suggestions about why  
the models take so long to calculate, and why the maximum number of  
function calls must be increased so much. Neither has been typical of  
my experience calculating such models, which I do routinely. Thank you.

Intercept only:

attach(sfCompiled)

# independence

sfInd <- glmer(fRspRN ~ cTrialNumber +
         cStep +
         cContext +
         cSpeed +
         (1 | participant),
         family = "binomial",
         control = glmerControl(optCtrl = list(maxfun=1000000)),
         data = sfCompiled)

#                Estimate Std. Error z value Pr(>|z|)
# (Intercept)  -6.024e-01  1.769e-01   -3.40 0.000662 ***
# cTrialNumber  9.739e-04  9.454e-05   10.30  < 2e-16 ***
# cStep         2.252e-01  3.357e-03   67.09  < 2e-16 ***
# cContext      5.356e-01  2.528e-02   21.19  < 2e-16 ***
# cSpeed        2.783e-01  2.492e-02   11.16  < 2e-16 ***

#
sfIndR <- glmer(fRspRN ~ cTrialNumber +
         cStep +
         cContext +
         cSpeed +
         (1 + cTrialNumber +
         cStep +
         cContext +
         cSpeed | participant),
         family = "binomial",
         control = glmerControl(optCtrl = list(maxfun=1000000)),
         data = sfCompiled)

summary(sfIndR)

#                Estimate Std. Error z value Pr(>|z|)
# (Intercept)  -0.8191734  0.3079764  -2.660 0.007817 **
# cTrialNumber  0.0012851  0.0003641   3.530 0.000416 ***
# cStep         0.2665563  0.0197676  13.485  < 2e-16 ***
# cContext      0.6654796  0.1140261   5.836 5.34e-09 ***
# cSpeed        0.3245607  0.0812122   3.996 6.43e-05 ***

John Kingston
--------------------------------
Professor and Head
Linguistics Department
University of Massachusetts
150 Hicks Way, 226 South College
Amherst, MA 01003-9274
1-413-545-6837, fax -2792
jkingston at linguist.umass.edu
https://blogs.umass.edu/jkingstn/


From wolfgang.viechtbauer at maastrichtuniversity.nl  Wed May 21 18:19:25 2014
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Wed, 21 May 2014 18:19:25 +0200
Subject: [R-sig-ME] Assessing Normality for Mixed Models
Message-ID: <077E31A57DA26E46AB0D493C9966AC730DC2C63375@UM-MAIL4112.unimaas.nl>

Just some general comments:

For some discussion and results on the robustness of the linear mixed-effects model (LMM) to non-normal random effects, see:

Verbeke, G., & Lesaffre, E. (1997). The effect of misspecifying the random-effects distribution in linear mixed models for longitudinal data. Computational Statistics & Data Analysis, 23, 541-556.

In essence, yes, the LMM is robust (in the sense of: the estimates are consistent), but a sandwich-type estimator of the variance-covariance matrix of the fixed-effects may be needed under non-normality.

As for your model selection strategy, it may be better to start with (Predictor|Male) and leave it as such. See:

Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255-278.

And why not just fit:

model <- lmer(Response ~ Predictor(within) + Predictor(between) + (Predictor(within)|Male))

and just report that? Any model selection strategy is going to alter the statistical properties of any tests you run on the fixed effects on the 'final' model. The above seems like an a priori sensible model, so my suggestion would be to just fit that and report the results (of course, you are free to do anything 'exploratory' beyond that if it is clearly marked as such).

Best,
Wolfgang

--   
Wolfgang Viechtbauer, Ph.D., Statistician   
Department of Psychiatry and Psychology   
School for Mental Health and Neuroscience   
Faculty of Health, Medicine, and Life Sciences   
Maastricht University, P.O. Box 616 (VIJV1)   
6200 MD Maastricht, The Netherlands   
+31 (43) 388-4170 | http://www.wvbauer.com   

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-bounces at r-project.org] On Behalf Of AvianResearchDivision
> Sent: Wednesday, May 21, 2014 16:16
> To: Farrar, David
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Assessing Normality for Mixed Models
> 
> Hi all,
> 
> Thank you for the answers so far.  I have been looking at
> qqnorm(residuals(model)) and notice that there isn't a linear line, but
> rather some skewing at both ends.  When I try various transformations
> (log,
> sqrt, boxcox, etc...), I don't see much of an improvement in this plot.
> I'm just wondering if these models are robust to such deviations or not.
> I
> was also looking for some methods in various R packages that
> quantitatively
> assess these assumptions.
> 
> If it helps with answers, I am running the following models over 59 males
> with roughly 15 observations per male for a total of 872 observations:
> 
> model1<-lmer(Response~Predictor+(1|Male)) or
> model2<-lmer(Response~Predictor+(Predictor|Male)) or
> model3<-lmer(Response~Predictor+(1|Male)+(Predictor+0|Male))
> 
> where my response and predictor values are continuous.  I have run models
> where I have within individual mean centered my predictor, along with
> using
> the mean value of the predictor for each male for each observation of
> each
> male to separate the within and between subject effects as per van de Pol
> and Wright (2009), so my model looks like
> 
> model4<-lmer(Response~Predictor(within)+Predictor(between)+(1|Male))
> 
> The Predictor(between) is never significant, so I end up running
> 
> model5<-lmer(Response~Predictor+Predictor(between)+(1|Male)
> 
> to verify that there are no between subject effects.
> 
> Since I never see any between subject effects, I end up running 'model1'
> from above, where the predictor variable is the actual unadjusted
> observed
> values, which now represents only the within subject effect as tested in
> 'model4' and 'model5'.
> 
> Once I have my fixed effects structure set, I have set the random effects
> structure using LRT fitted by ML to test the significant of random slopes
> and correlation between slopes and intercepts.
> 
> I now am at the point where I just need to make sure I am meeting model
> assumptions, which is the subject of my initial email.  I hope this extra
> detail sheds some light on my particular issue and helps with providing
> advice.
> 
> Jacob


From drmccloy at uw.edu  Wed May 21 20:04:53 2014
From: drmccloy at uw.edu (Dan McCloy)
Date: Wed, 21 May 2014 11:04:53 -0700
Subject: [R-sig-ME] glmer warning "fixed-effect model matrix is rank
	deficient"
Message-ID: <CAOE0pYnKx_bP5ex60ivBCh=DzZfLPxWmZ9w-PT5TwAkKmHKi6A@mail.gmail.com>

sessionInfo:
R version 3.1.0 (2014-04-10)
Platform: x86_64-pc-linux-gnu (64-bit)

locale:
LC_CTYPE=en_US.UTF-8
LC_NUMERIC=C
LC_TIME=en_US.UTF-8
LC_COLLATE=en_US.UTF-8
LC_MONETARY=en_US.UTF-8
LC_MESSAGES=en_US.UTF-8
LC_PAPER=en_US.UTF-8
LC_NAME=C
LC_ADDRESS=C
LC_TELEPHONE=C
LC_MEASUREMENT=en_US.UTF-8
LC_IDENTIFICATION=C

attached base packages:
stats
graphics
grDevices
utils
datasets
methods
base

other attached packages:
caret_6.0-24
ggplot2_0.9.3.1
lattice_0.20-29
zoo_1.7-11
lme4_1.1-6
Rcpp_0.11.1
Matrix_1.1-3

loaded via a namespace (and not attached):
car_2.0-20
codetools_0.2-8
colorspace_1.2-4
digest_0.6.4
foreach_1.4.2
grid_3.1.0
gtable_0.1.2
iterators_1.0.7
MASS_7.3-31
minqa_1.2.3
munsell_0.4.2
nlme_3.1-117
nnet_7.3-8
plyr_1.8.1
proto_0.3-10
RcppEigen_0.3.2.1.1 reshape2_1.4        scales_0.2.4
[19] splines_3.1.0       stringr_0.6.2       tools_3.1.0


From drmccloy at uw.edu  Wed May 21 20:50:32 2014
From: drmccloy at uw.edu (Dan McCloy)
Date: Wed, 21 May 2014 11:50:32 -0700
Subject: [R-sig-ME] glmer warning "fixed-effect model matrix is rank
	deficient"
Message-ID: <CAOE0pYk-1r_JrV1QXcozkFT2A+GLv7NsbUo8WtfZjSTnH8ZUeg@mail.gmail.com>

Apologies for the prematurely-sent message containing only session
info (thanks, gmail keyboard shortcuts).  I was going to post about
getting the warning "fixed-effect model matrix is rank deficient, so
dropping 4 columns/coefficients" when running a glmer model, but after
additional testing while creating a MWE, it turned out to be user
error.
-- dan

Daniel McCloy
http://dan.mccloy.info/
Postdoctoral Research Fellow
Institute for Learning and Brain Sciences
University of Washington


From slu at ccsr.uchicago.edu  Thu May 22 00:15:02 2014
From: slu at ccsr.uchicago.edu (Stuart Luppescu)
Date: Wed, 21 May 2014 17:15:02 -0500
Subject: [R-sig-ME] How to interpret verbose output
Message-ID: <1400710502.9391.17.camel@musuko.uchicago.edu>


Hello, I'm trying to do a variance decomposition of teacher performance
ratings. Each teacher is observed four times and rated on 9 components.
The model looks like this:

 lme8 <- lmer(rating ~
              (1|tid.f) + (1|obsorder.f) + (1|comp.f) +
               (1|tid.f:comp.f) +  (1|obsorder.f:comp.f) + (1|
tid.f:obsorder.f) ,
               data=ratings, REML=FALSE, verbose=2)

where tid.f is the teacher identifier, comp.f the component identifier,
and obsorder.f the observation number. 

This works fine for the whole dataset, but I want to do it separately by
decile based on the average rating for each teacher, so I added this: ,
subset=ratings$bins==quantile
where the bins are {1, ..., 10} indicating the decile, and quantile is a
loop index used like this: for(quantile in 1:10) {}

It works fine for the lowest decile, but fails for every decile after
that. I get 0.00 for the tid.f variance component, which is the one I'm
really interested in. I have no idea why. I checked the distribution of
average ratings by decile and it all looks unremarkable. I think there
may be a clue in the iteration details as shown by the verbose output
but I don't know how to interpret it. Here it is for decile 1:

npt = 8 , n =  6 
rhobeg =  0.2 , rhoend =  2e-07 
   0.020:  16:      18359.8;0.454554 0.735854 0.376302 0.705437 0.787591
0.802885 
  0.0020:  32:      18346.2;0.454004 0.699893 0.407403 0.574987 0.702683
0.880645 
 0.00020:  77:      18263.7;0.432231 0.697895 0.398860 0.0461275
0.326475  1.55039 
 2.0e-05: 292:      18252.6;0.432051 0.700841 0.394275 0.0472994
0.315551 0.130071 
 2.0e-06: 339:      18252.6;0.432029 0.700839 0.394515 0.0472285
0.314849 0.129962 
 2.0e-07: 364:      18252.6;0.432028 0.700822 0.394548 0.0472181
0.314777 0.129904 
At return
395:     18252.578: 0.432030 0.700822 0.394554 0.0472204 0.314755
0.129913
>   print(summary(lme8))
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: rating ~ (1 | tid.f) + (1 | obsorder.f) + (1 | comp.f) + (1 |  
    tid.f:comp.f) + (1 | obsorder.f:comp.f) + (1 | tid.f:obsorder.f)
   Data: ratings
 Subset: ratings$bins == quantile

     AIC      BIC   logLik deviance df.resid 
 18268.6  18328.1  -9126.3  18252.6    12622 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2200 -0.5592 -0.0306  0.5711  4.4942 

Random effects:
 Groups            Name        Variance Std.Dev.
 tid.f:comp.f      (Intercept) 0.033067 0.18184 
 tid.f:obsorder.f  (Intercept) 0.087013 0.29498 
 tid.f             (Intercept) 0.027579 0.16607 
 obsorder.f:comp.f (Intercept) 0.000395 0.01988 
 comp.f            (Intercept) 0.017551 0.13248 
 obsorder.f        (Intercept) 0.002990 0.05468 
 Residual                      0.177161 0.42090 
Number of obs: 12630, groups: tid.f:comp.f, 3159; tid.f:obsorder.f,
1404; tid.f, 351; obsorder.f:comp.f, 45; comp.f, 9; obsorder.f, 5

Fixed effects:
            Estimate Std. Error t value
(Intercept)  2.01657    0.05355   37.65

And here for decile 5:

npt = 8 , n =  6 
rhobeg =  0.2 , rhoend =  2e-07 
   0.020:  18:      15847.4;0.261469 0.514852 0.178718 0.0962381
0.152470 0.160530 
  0.0020:  35:      15749.5;0.356245 0.532056  0.00000 0.0961818
0.219586 0.166497 
 0.00020:  90:      15742.1;0.349036 0.516678  0.00000 0.0755764
0.330633 0.248861 
 2.0e-05: 107:      15742.1;0.348612 0.515454  0.00000 0.0766747
0.332641 0.251693 
 2.0e-06: 250:      15742.1;0.348335 0.515211  0.00000 0.0765898
0.330456 0.260112 
 2.0e-07: 273:      15742.1;0.348339 0.515207  0.00000 0.0765856
0.330510 0.260162 
At return
281:     15742.076: 0.348339 0.515207 1.97296e-07 0.0765858 0.330510
0.260162
>   print(summary(lme8))
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: rating ~ (1 | tid.f) + (1 | obsorder.f) + (1 | comp.f) + (1 |  
    tid.f:comp.f) + (1 | obsorder.f:comp.f) + (1 | tid.f:obsorder.f)
   Data: ratings
 Subset: ratings$bins == quantile

     AIC      BIC   logLik deviance df.resid 
 15758.1  15817.7  -7871.0  15742.1    12772 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.6535 -0.3833  0.1362  0.5225  3.4610 

Random effects:
 Groups            Name        Variance  Std.Dev.
 tid.f:comp.f      (Intercept) 0.0192757 0.13884 
 tid.f:obsorder.f  (Intercept) 0.0421666 0.20535 
 tid.f             (Intercept) 0.0000000 0.00000 
 obsorder.f:comp.f (Intercept) 0.0009318 0.03052 
 comp.f            (Intercept) 0.0173531 0.13173 
 obsorder.f        (Intercept) 0.0107521 0.10369 
 Residual                      0.1588568 0.39857 
Number of obs: 12780, groups: tid.f:comp.f, 3195; tid.f:obsorder.f,
1420; tid.f, 355; obsorder.f:comp.f, 45; comp.f, 9; obsorder.f, 5

Fixed effects:
            Estimate Std. Error t value
(Intercept)  2.85126    0.06774   42.09

For decile 5, I notice that the column fourth from the right goes to 0
after the first line. Does that mean something? Any ideas about why this
is failing?

Thanks in advance.


-- 
Stuart Luppescu -=-=- slu <AT> ccsr <DOT> uchicago <DOT> edu
CCSR at U of C ,.;-*^*-;.,  ccsr.uchicago.edu
     (^_^)/    ????????
[Crash programs] fail because they are based on the theory that, 
with nine women pregnant, you can get a baby a month.
                -- Wernher von Braun


From bbolker at gmail.com  Thu May 22 02:05:26 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 21 May 2014 20:05:26 -0400
Subject: [R-sig-ME] How to interpret verbose output
In-Reply-To: <1400710502.9391.17.camel@musuko.uchicago.edu>
References: <1400710502.9391.17.camel@musuko.uchicago.edu>
Message-ID: <537D3F46.4000105@gmail.com>

On 14-05-21 06:15 PM, Stuart Luppescu wrote:
> 
> Hello, I'm trying to do a variance decomposition of teacher performance
> ratings. Each teacher is observed four times and rated on 9 components.
> The model looks like this:
> 
>  lme8 <- lmer(rating ~
>               (1|tid.f) + (1|obsorder.f) + (1|comp.f) +
>                (1|tid.f:comp.f) +  (1|obsorder.f:comp.f) + (1|
> tid.f:obsorder.f) ,
>                data=ratings, REML=FALSE, verbose=2)
> 
> where tid.f is the teacher identifier, comp.f the component identifier,
> and obsorder.f the observation number. 
> 
> This works fine for the whole dataset, but I want to do it separately by
> decile based on the average rating for each teacher, so I added this: ,
> subset=ratings$bins==quantile
> where the bins are {1, ..., 10} indicating the decile, and quantile is a
> loop index used like this: for(quantile in 1:10) {}
> 
> It works fine for the lowest decile, but fails for every decile after
> that. I get 0.00 for the tid.f variance component, which is the one I'm
> really interested in. I have no idea why. I checked the distribution of
> average ratings by decile and it all looks unremarkable. I think there
> may be a clue in the iteration details as shown by the verbose output
> but I don't know how to interpret it. Here it is for decile 1:
> 

  This printing comes from minqa::bobyqa.  ?bobyqa says:

iprint The value of ?iprint? should be set to ?0, 1, 2 or 3?,
          which controls the amount of printing. Specifically, there is
          no output if ?iprint=0? and there is output only at the
          return if ?iprint=1?. Otherwise, each new value of ?rho? is
          printed, with the best vector of variables so far and the
          corresponding value of the objective function. Further, each
          new value of the objective function with its variables are
          output if ?iprint=3?.  Default value is ?0?.


> npt = 8 , n =  6

Number of points used for quadratic approximation, number of points
(parameters)

> rhobeg =  0.2 , rhoend =  2e-07 
>    0.020:  16:      18359.8;0.454554 0.735854 0.376302 0.705437 0.787591
> 0.802885 

   rho (current trust region radius): number of function evaluations
(?): current value of objective function (PWRSS/deviance); values of
parameters ("theta") -- lower triangle of the Cholesky factor of the RE
variance-covariance matrices, scaled by the residual standard deviation.

>   0.0020:  32:      18346.2;0.454004 0.699893 0.407403 0.574987 0.702683
> 0.880645 
>  0.00020:  77:      18263.7;0.432231 0.697895 0.398860 0.0461275
> 0.326475  1.55039 
>  2.0e-05: 292:      18252.6;0.432051 0.700841 0.394275 0.0472994
> 0.315551 0.130071 
>  2.0e-06: 339:      18252.6;0.432029 0.700839 0.394515 0.0472285
> 0.314849 0.129962 
>  2.0e-07: 364:      18252.6;0.432028 0.700822 0.394548 0.0472181
> 0.314777 0.129904 
> At return
> 395:     18252.578: 0.432030 0.700822 0.394554 0.0472204 0.314755
> 0.129913
>>   print(summary(lme8))
> Linear mixed model fit by maximum likelihood  ['lmerMod']
> Formula: rating ~ (1 | tid.f) + (1 | obsorder.f) + (1 | comp.f) + (1 |  
>     tid.f:comp.f) + (1 | obsorder.f:comp.f) + (1 | tid.f:obsorder.f)
>    Data: ratings
>  Subset: ratings$bins == quantile
> 
>      AIC      BIC   logLik deviance df.resid 
>  18268.6  18328.1  -9126.3  18252.6    12622 
> 
> Scaled residuals: 
>     Min      1Q  Median      3Q     Max 
> -3.2200 -0.5592 -0.0306  0.5711  4.4942 
> 
> Random effects:
>  Groups            Name        Variance Std.Dev.
>  tid.f:comp.f      (Intercept) 0.033067 0.18184 
>  tid.f:obsorder.f  (Intercept) 0.087013 0.29498 
>  tid.f             (Intercept) 0.027579 0.16607 
>  obsorder.f:comp.f (Intercept) 0.000395 0.01988 
>  comp.f            (Intercept) 0.017551 0.13248 
>  obsorder.f        (Intercept) 0.002990 0.05468 
>  Residual                      0.177161 0.42090 
> Number of obs: 12630, groups: tid.f:comp.f, 3159; tid.f:obsorder.f,
> 1404; tid.f, 351; obsorder.f:comp.f, 45; comp.f, 9; obsorder.f, 5
> 
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)  2.01657    0.05355   37.65
> 
> And here for decile 5:
> 
> npt = 8 , n =  6 
> rhobeg =  0.2 , rhoend =  2e-07 
>    0.020:  18:      15847.4;0.261469 0.514852 0.178718 0.0962381
> 0.152470 0.160530 
>   0.0020:  35:      15749.5;0.356245 0.532056  0.00000 0.0961818
> 0.219586 0.166497 
>  0.00020:  90:      15742.1;0.349036 0.516678  0.00000 0.0755764
> 0.330633 0.248861 
>  2.0e-05: 107:      15742.1;0.348612 0.515454  0.00000 0.0766747
> 0.332641 0.251693 
>  2.0e-06: 250:      15742.1;0.348335 0.515211  0.00000 0.0765898
> 0.330456 0.260112 
>  2.0e-07: 273:      15742.1;0.348339 0.515207  0.00000 0.0765856
> 0.330510 0.260162 
> At return
> 281:     15742.076: 0.348339 0.515207 1.97296e-07 0.0765858 0.330510
> 0.260162
>>   print(summary(lme8))
> Linear mixed model fit by maximum likelihood  ['lmerMod']
> Formula: rating ~ (1 | tid.f) + (1 | obsorder.f) + (1 | comp.f) + (1 |  
>     tid.f:comp.f) + (1 | obsorder.f:comp.f) + (1 | tid.f:obsorder.f)
>    Data: ratings
>  Subset: ratings$bins == quantile
> 
>      AIC      BIC   logLik deviance df.resid 
>  15758.1  15817.7  -7871.0  15742.1    12772 
> 
> Scaled residuals: 
>     Min      1Q  Median      3Q     Max 
> -3.6535 -0.3833  0.1362  0.5225  3.4610 
> 
> Random effects:
>  Groups            Name        Variance  Std.Dev.
>  tid.f:comp.f      (Intercept) 0.0192757 0.13884 
>  tid.f:obsorder.f  (Intercept) 0.0421666 0.20535 
>  tid.f             (Intercept) 0.0000000 0.00000 
>  obsorder.f:comp.f (Intercept) 0.0009318 0.03052 
>  comp.f            (Intercept) 0.0173531 0.13173 
>  obsorder.f        (Intercept) 0.0107521 0.10369 
>  Residual                      0.1588568 0.39857 
> Number of obs: 12780, groups: tid.f:comp.f, 3195; tid.f:obsorder.f,
> 1420; tid.f, 355; obsorder.f:comp.f, 45; comp.f, 9; obsorder.f, 5
> 
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)  2.85126    0.06774   42.09
> 
> For decile 5, I notice that the column fourth from the right goes to 0
> after the first line. Does that mean something? Any ideas about why this
> is failing?
> 
> Thanks in advance.
> 
> 

  It means the second element of the theta vector (which controls the
among-'tid.f' variation in the intercept) is going to zero.  This is
probably the correct numeric value ...

  I would consider further


(1) scaling and centering continuous predictors to see if that helps
(2) simulating pseudo-data and fitting it to see how it performs
(?simulate.merMod will help with this)
(3) using bbmle::slice2D to view 2D slices of the deviance surface.


From guillaumechaumet at gmail.com  Thu May 22 11:09:49 2014
From: guillaumechaumet at gmail.com (guillaume chaumet)
Date: Thu, 22 May 2014 11:09:49 +0200
Subject: [R-sig-ME] Zero inflated mixed model with constraint (lasso or
	other)
Message-ID: <CAGg8SkK4U_jyV83oe_fx0n-8okxDUTKdhCfnZ2htZOUKP6ZejQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140522/f22222ca/attachment.pl>

From dhocking at wildcats.unh.edu  Thu May 22 17:26:52 2014
From: dhocking at wildcats.unh.edu (Daniel J Hocking)
Date: Thu, 22 May 2014 15:26:52 +0000
Subject: [R-sig-ME] Getting each bootstrapped data frame from bootMer
Message-ID: <B21F9ED4-84C5-446A-A9F8-A54A735437A9@wildcats.unh.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140522/413e70cb/attachment.pl>

From c.wordley at live.com  Thu May 22 21:04:47 2014
From: c.wordley at live.com (Claire)
Date: Thu, 22 May 2014 20:04:47 +0100
Subject: [R-sig-ME] =?windows-1256?q?Previous_messageNext_messageBack_to_m?=
 =?windows-1256?q?essages_Post-hoc_tests_on_linear_mixed_model_give_mixed_?=
 =?windows-1256?b?cmVzdWx0cy7+?=
Message-ID: <DUB122-W22A55102B168D98D424A79903F0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140522/a47d373f/attachment.pl>

From tobias.heed.uhh at gmail.com  Fri May 23 17:34:07 2014
From: tobias.heed.uhh at gmail.com (Tobias Heed)
Date: Fri, 23 May 2014 17:34:07 +0200
Subject: [R-sig-ME] negative Hessian eigenvalues
Message-ID: <DAC51A9C-8824-4ED8-BEC6-C133CC41B151@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140523/2d3d43a0/attachment.pl>

From bbolker at gmail.com  Fri May 23 18:53:29 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 23 May 2014 12:53:29 -0400
Subject: [R-sig-ME] negative Hessian eigenvalues
In-Reply-To: <DAC51A9C-8824-4ED8-BEC6-C133CC41B151@gmail.com>
References: <DAC51A9C-8824-4ED8-BEC6-C133CC41B151@gmail.com>
Message-ID: <537F7D09.9000509@gmail.com>

On 14-05-23 11:34 AM, Tobias Heed wrote:
> Hello,
> 
> I have read the long thread about the convergence warnings in lmer v.
> 1.1.6. However, I am still unclear as to how to work with them. If I
> understand correctly, I should probably ignore the max | grad |
> warnings, at least if they are close to the tol value. However, what
> about the negative Hessian eigenvalues? Can I ignore those as well?
> Is the number of negative Hessian eigenvalues relevant at all? In
> older versions, models also did not always converge. Does any
> combination of the new warnings map to the cases in which earlier
> versions would have told me that they did not achieve convergence?

  max |grad| warnings are (mostly) ignorable: the convergence warning
thread gives a recipe for computing the _scaled_ gradients if you want
to double-check to see if they would still give a convergence warning
with 1.1-7.

  Negative Hessian eigenvalues are most likely to indicate problems with
uncentered/unscaled variables.

  I'm not sure precisely which version/warnings you have in mind, but I
suspect there's not a simple mapping.

  cheers
    Ben Bolker


> Thanks Tobias
> 
> 
> -- 
> -------------------------------------------------------------------------------------------
>
> 
Dr. Tobias Heed
> Biological Psychology and Neuropsychology Faculty of Psychology and
> Human Movement Science | University of Hamburg Von-Melle-Park 11,
> Room 206 | D-20146 Hamburg, Germany Phone: (49) 40 - 42838 5831 |
> Fax: (49) 40 - 42838 6591 tobias.heed at uni-hamburg.de 
> -------------------------------------------------------------------------------------------
>
> 
[[alternative HTML version deleted]]
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From zia207 at gmail.com  Sat May 24 20:43:00 2014
From: zia207 at gmail.com (Zia Ahmed)
Date: Sun, 25 May 2014 00:43:00 +0600
Subject: [R-sig-ME] Help: mixed model with nested structure
Message-ID: <CAKqUiGkLG8ojLmSZ6=XbmG3xKLx5_y-UB+wqi4n96-eMyTV5-A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140525/1fcbe229/attachment.pl>

From bbolker at gmail.com  Sat May 24 20:44:11 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 24 May 2014 14:44:11 -0400
Subject: [R-sig-ME] Fwd: Re: Regions of significance plot for multilevel
 logistic regression model
In-Reply-To: <5380E662.6000801@sheffield.ac.uk>
References: <5380E662.6000801@sheffield.ac.uk>
Message-ID: <5380E87B.4020300@gmail.com>



 [taking the liberty of forwarding this back to r-sig-mixed-models]

I'm looking for the multilevel logistic regression equivalent of this:

http://www.quantpsy.org/interact/hlm2.htm

with my scenario being as per case 2 in the spreadsheet

cheers
Chris

On 19/05/2014 23:00, Ben Bolker wrote:
> On 14-05-19 04:37 PM, Chris Stride wrote:
>> Hi all
>> To avoid re-inventing the wheel I was wondering if anyone could point me
>> towards some R code to plot simple slopes and the region(s) of
>> significance for an interaction between two continuous higher-level
>> predictors in a 2-level multilevel logistic regression model?
>> cheers
>> Chris
>>
>    Could you be a little more specific and possibly provide a
> reproducible example?  Can you point to/have you found solutions of your
> problem for either (1) a plain linear model (2) a LMM (i.e. non-logistic
> but multilevel) (3) a plain logistic regression (i.e. binary response
> but single-level)?  (These would all help because they would clarify
> precisely what you want and because the general strategies might be
> extendable to your case ...)
>
>    Ben Bolker
>
>


From bbolker at gmail.com  Sun May 25 03:01:37 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 24 May 2014 21:01:37 -0400
Subject: [R-sig-ME] Getting each bootstrapped data frame from bootMer
In-Reply-To: <B21F9ED4-84C5-446A-A9F8-A54A735437A9@wildcats.unh.edu>
References: <B21F9ED4-84C5-446A-A9F8-A54A735437A9@wildcats.unh.edu>
Message-ID: <538140F1.7040407@gmail.com>

On 14-05-22 11:26 AM, Daniel J Hocking wrote:
> I am trying to get bootstrapped predictions from a logistic glmer
> (lme4) object and compare them to the ?observed? independent variable
> from each bootstrap iteration. I can use bootMer to get the
> bootstrapped predictions:
> 
> bb <- bootMer(glmm.M35,
> FUN=function(x)predict(x,re.form=NA,newdata=df.fit,type="response"),
> nsim=10) # test with just 10 draws
> 
> but it doesn?t appear that bootMer saves the replicate datasets. Can
> anyone help me modify the bootMer function to output the bootstrap
> data frames or at least the independent variable (y) from each? Also
> any advice on handling the occasions when an iteration fails to
> converge in bootMer would be appreciated.

  I would be tempted just to re-invent the guts of bootMer, which are
not actually very complicated -- they're just a simulate/refit loop.  So
I might do something like

nsim <- 10
bootData <- simulate(glmm.M35,nsim)  ## data frame of sim data sets
predvals <- matrix(NA,nrow=length(df.fit),ncol=nsim)
set.seed(101)
for (i  in 1:10) {
   predvals[,i] <- predict(refit(glmm.M35,bootData[[i]]),
                     ...)
}

I also suspect (although haven't tried to check) that bootMer doesn't
use any random numbers other than the ones it uses to generate the
simulations, so you would *probably* be able to retrieve the response
values used if you just did this:

bootMer(...,seed=101)
bootData <- simulate(...,seed=101)

(slightly inefficient, but simulating the data is the smallest part of
the problem)

 hope that helps
    Ben Bolker




}

> 
> Thanks, Dan ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Daniel
> Hocking Department of Environmental Conservation Northeast Climate
> Science Center University of Massachusetts
> 
> http://www.danieljhocking.wordpress.com 
> dhocking at umass.edu<mailto:dhocking at umass.edu> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> [[alternative HTML version deleted]]
> 
> 
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Sun May 25 04:26:43 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 24 May 2014 22:26:43 -0400
Subject: [R-sig-ME] LMER
In-Reply-To: <CALz0ddsFVXm+QoBJmO-7-PNQpn56=4+sgm4ckmmRMONnekWP9Q@mail.gmail.com>
References: <CALz0ddsFVXm+QoBJmO-7-PNQpn56=4+sgm4ckmmRMONnekWP9Q@mail.gmail.com>
Message-ID: <538154E3.1090508@gmail.com>

  [taking the liberty of cc'ing to r-sig-mixed-models]

  I've just put up a document on rpubs
<http://rpubs.com/bbolker/varwald> that shows how to get the asymptotic
(Wald) variance-covariance matrix for the *ML* estimates, using the
internal 'devfun2' function.  It will take a bit more work to do the
same thing for the REML estimates, but maybe this document will provide
enough hints that someone else can do it ...

  cheers
     Ben Bolker

On 14-05-23 12:26 AM, Douglas Hawkins wrote:
> Hi, Ben,
> 
> I saw your thread from a couple years back on the the standard errors of
> variance component estimates.
> 
> I see lots of nested random effects data sets where we estimate the
> variance components by REML and then want to get asymptotic confidence
> intervals for partial sums of them.  
> 
> Traditionally, I have done this in SAS using the asymptotic covariance
> matrix of the estimates.  But I would much rather do it in R.
> 
> It sounds like you were writing code that would do this -- have there
> been any developments along these lines?
> 
> Best,
> 
> Doug Hawkins
> 
> 
> -- 
> Douglas M. Hawkins
> Professor, School of Statistics
> University of Minnesota
> 313 Ford Hall
> Minneapolis, MN 55455-0493
> +1 612 624 4166Hi,


From rfaustinol at gmail.com  Sun May 25 17:06:54 2014
From: rfaustinol at gmail.com (Ricardo Lima)
Date: Sun, 25 May 2014 16:06:54 +0100
Subject: [R-sig-ME] {dsm} warning message "step size truncated due to
	divergence"
Message-ID: <CADDBpO2gDcB8WESzM3MFyggjSx901JPGvf7J6k2gVLMJnOeReA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140525/6e835476/attachment.pl>

From bbolker at gmail.com  Sun May 25 17:59:43 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 25 May 2014 11:59:43 -0400
Subject: [R-sig-ME] {dsm} warning message "step size truncated due to
 divergence"
In-Reply-To: <CADDBpO2gDcB8WESzM3MFyggjSx901JPGvf7J6k2gVLMJnOeReA@mail.gmail.com>
References: <CADDBpO2gDcB8WESzM3MFyggjSx901JPGvf7J6k2gVLMJnOeReA@mail.gmail.com>
Message-ID: <5382136F.3060105@gmail.com>

On 14-05-25 11:06 AM, Ricardo Lima wrote:
> Hello,
> 
> I'm trying to fit a density surface model, following "Spatial models for
> distance sampling data: recent developments and future directions" (Miller
> et al. 2013 - Methods in Ecology and Evolution 4 (11): 1001-1010).
> 
> When I run the model, I get the following:
> "warning messages:
> 1: Step size truncated due to divergence
> 2: Step size truncated due to divergence
> 3: Step size truncated due to divergence
> 4: Step size truncated due to divergence
> 5: Step size truncated due to divergence "
> 
> Then, when I try to use "predict", 17 grid cells (belonging to 5 groups)
> appear as "Inf", instead of having a value. And that keeps from mapping the
> predicted abundance or calculating the abundance for the entire survey area.
> 
> Can you please help me understanding what this means and how I can get
> around this?
> 

  Pretty much impossible to say without more detail.  I could go read
the paper, but it would be more efficient if you could give a minimal
reproducible example ...  It sounds like your model is overspecified
somehow ...

  Ben Bolker


From jkingston at linguist.umass.edu  Mon May 26 06:29:13 2014
From: jkingston at linguist.umass.edu (John Kingston)
Date: Mon, 26 May 2014 00:29:13 -0400
Subject: [R-sig-ME] update to previous message about false positives
Message-ID: <20140526002913.175955paivw23fh5@umail.oit.umass.edu>

This message follows my earlier one regarding possible false positives.

The test:

relgrad <- with(sfHSxRtCxRtR at optinfo$derivs, solve(Hessian, gradient))
max(abs(relgrad))

yielded a small value, too: 0.0003628809

And a further test:

xx <- update(sfHSxRtCxRtR,devFunOnly=TRUE)
params <- unlist(getME(sfHSxRtCxRtR, c("theta", "beta")))
grad(xx,params)

yielded:

[1] -0.0033705847  0.0114374278 -0.0088389651 -0.0013375443  
-0.0112697898  0.0011420500  0.0024532777  0.0023931444  0.0109466237  
-0.0007213523 -0.0006562741 -0.0004396474  0.0039678684
[14] -0.0001146848  0.0044656719  0.0150139408 -0.0049835953   
0.0106851591  0.0022508263 -0.0031896368  0.0085055924 -0.0020030117   
0.0117194911

which differs relatively little from gg, calculated earlier:

  [1] -3.368978e-03  1.132823e-02 -8.620318e-03 -1.396875e-03  
-1.111574e-02 -5.324182e-05  2.831584e-03  3.310606e-03  1.246932e-02  
-1.287135e-03 -7.301514e-04 -2.738943e-04  3.954865e-03
[14] -2.018623e-04  4.035155e-04  1.503184e-02 -4.957783e-03   
1.072177e-02  2.304259e-03 -3.187470e-03  2.128822e-01 -9.298437e-03   
1.459412e-02

grad(xx, params) - gg =

  [1] -1.607096e-06  1.091984e-04 -2.186470e-04  5.933041e-05  
-1.540546e-04  1.195292e-03 -3.783067e-04 -9.174618e-04 -1.522694e-03   
5.657828e-04  7.387734e-05 -1.657531e-04  1.300342e-05
[14]  8.717753e-05  4.062156e-03 -1.789655e-05 -2.581237e-05  
-3.661019e-05 -5.343309e-05 -2.167105e-06 -2.043766e-01  7.295426e-03  
-2.874625e-03

Doing the same comparison for the Hessian yields:

> hessian(xx, params)

                [,1]         [,2]          [,3]         [,4]           
[,5]         [,6]         [,7]        [,8]          [,9]         [,10]  
        [,11]        [,12]       [,13]        [,14]
  [1,]   120.1740592   105.442861  -107.5745280    52.670499     
822.582502    -7.504852    -1.506024    1.358660    -6.3002315  
-8.502787e+00    1.0206522     5.367529   -2.984625     4.145573
  [2,]   105.4428611  8002.527882 -2457.2221203  -102.419212   
-2312.601923   369.489001    35.510026   17.399144   118.7589983  
-1.505657e+01    5.7026516    32.342155  -18.621484     5.162396
  [3,]  -107.5745280 -2457.222120  4577.1322564  -276.573290     
346.906019    16.382111   -38.982669   -2.432062    -0.7133908  
-7.764190e+01   -5.3027926     5.844216    3.691355    21.479957
  [4,]    52.6704989  -102.419212  -276.5732902   737.922041    
1300.431563    10.459564    -1.764205   10.622713    16.2841119  
-2.731945e+01    1.0064528    -4.795930   -1.518830    39.934229
  [5,]   822.5825016 -2312.601923   346.9060192  1300.431563   
19098.452236    33.916906     7.832260   19.364769    35.5692713  
-1.012937e+02    3.5862966    58.764282  -41.774123    28.094613
  [6,]    -7.5048522   369.489001    16.3821110    10.459564      
33.916906 13986.671883 -2236.417643 -106.713739 -2273.5207699  
-1.669204e+01   41.5750859     5.155602   -2.023642    11.201909
  [7,]    -1.5060244    35.510026   -38.9826685    -1.764205       
7.832260 -2236.417643  4413.504625 -266.587541   332.9324243  
-8.312557e+01  -39.2883785   -37.440896  -41.408529   -31.265860
  [8,]     1.3586601    17.399144    -2.4320616    10.622713      
19.364769  -106.713739  -266.587541  749.607763  1364.0263240  
-4.925123e+01   -8.7128203    21.746147   25.574510    44.477344
  [9,]    -6.3002315   118.758998    -0.7133908    16.284112      
35.569271 -2273.520770   332.932424 1364.026324 19104.3149051  
-1.046625e+02   16.1393701   308.306118   32.406834   117.265865
[10,]    -8.5027867   -15.056572   -77.6419036   -27.319448    
-101.293680   -16.692040   -83.125575  -49.251226  -104.6624698   
8.228576e+03 -339.0694670   282.233599  -43.532856     9.178118
[11,]     1.0206522     5.702652    -5.3027926     1.006453       
3.586297    41.575086   -39.288379   -8.712820    16.1393701  
-3.390695e+02  710.6842938  1240.305931   -5.786735   -60.986719
[12,]     5.3675294    32.342155     5.8442155    -4.795930      
58.764282     5.155602   -37.440896   21.746147   308.3061177   
2.822336e+02 1240.3059308 18444.354308  -93.130339   -87.987859
[13,]    -2.9846245   -18.621484     3.6913547    -1.518830     
-41.774123    -2.023642   -41.408529   25.574510    32.4068336  
-4.353286e+01   -5.7867350   -93.130339 1090.460202   778.296730
[14,]     4.1455734     5.162396    21.4799567    39.934229      
28.094613    11.201909   -31.265860   44.477344   117.2658650   
9.178118e+00  -60.9867194   -87.987859  778.296730 15541.250246
[15,]    -3.3266306  -202.993511    19.9748010    96.184231     
482.812590  -206.702930    54.502400   67.653255   451.8587709  
-6.693112e+01  -38.1608720   -29.873503 -292.912538 -3764.642188
[16,]     0.2912339    -9.349129    -2.4009252     3.349133      
12.877780    -4.025141     4.372661   -3.911761   -32.6386647  
-2.411221e+00   -0.4705178    -3.584774   -2.357494    -4.436009
[17,]   -14.0198211    89.356392   -13.3961674    -4.616096     
-67.199087  -454.832736    61.714665    7.017614    48.7781235  
-1.033230e+02   13.6842720    72.066398  -31.391468    44.439705
[18,]    10.9251160   -84.749391   -25.1606425    89.013582     
434.877479    64.788609    27.690336  -97.933405  -750.7025742   
9.253063e-02  -30.1165345  -176.577184  -47.932266  -132.656737
[19,]    -2.5757481    -5.377063    12.6585955   -16.815331     
-41.860866    86.888308  -112.384214   20.561753    23.2548131  
-6.523163e+01  -11.3970316   -48.690053   -2.417331    29.048922
[20,]     2.9369754    -7.413088   -12.5884897    38.095891      
88.441659     4.104393    20.001128  -47.375002  -102.5932378  
-8.267404e+00  -12.1042232   -27.950744   -7.499306   -66.464994
[21,] -1140.1100054  8677.326772 -2094.5772787 -3292.407408  
-47965.509870 -1906.108077  1508.941003 -164.753978 -6475.7960985  
-2.840359e+03   15.8131613  4192.604414 -694.771182  -469.902347
[22,]  -302.8082321   990.009194 -1388.5931994  -272.781461   
-5756.551118 -1110.695682 -1293.057165  161.812331   748.1841020   
1.588311e+03 -382.3264838 -3456.696864  252.860935  1383.021534
[23,]   -17.0974103    31.132358     6.7955401  -156.860690    
-285.905857   -36.209011    -2.910781 -170.972637   -66.8478459  
-1.881380e+02  127.4555154   468.018755  -31.070752   493.226459
              [,15]        [,16]        [,17]         [,18]         
[,19]        [,20]         [,21]        [,22]         [,23]
  [1,]    -3.326631    0.2912339   -14.019821  1.092512e+01     
-2.575748     2.936975   -1140.11001    -302.8082    -17.097410
  [2,]  -202.993511   -9.3491287    89.356392 -8.474939e+01     
-5.377063    -7.413088    8677.32677     990.0092     31.132358
  [3,]    19.974801   -2.4009252   -13.396167 -2.516064e+01     
12.658596   -12.588490   -2094.57728   -1388.5932      6.795540
  [4,]    96.184231    3.3491332    -4.616096  8.901358e+01    
-16.815331    38.095891   -3292.40741    -272.7815   -156.860690
  [5,]   482.812590   12.8777796   -67.199087  4.348775e+02    
-41.860866    88.441659  -47965.50987   -5756.5511   -285.905857
  [6,]  -206.702930   -4.0251414  -454.832736  6.478861e+01     
86.888308     4.104393   -1906.10808   -1110.6957    -36.209011
  [7,]    54.502400    4.3726609    61.714665  2.769034e+01   
-112.384214    20.001128    1508.94100   -1293.0572     -2.910781
  [8,]    67.653255   -3.9117609     7.017614 -9.793341e+01     
20.561753   -47.375002    -164.75398     161.8123   -170.972637
  [9,]   451.858771  -32.6386647    48.778124 -7.507026e+02     
23.254813  -102.593238   -6475.79610     748.1841    -66.847846
[10,]   -66.931125   -2.4112213  -103.323010  9.253063e-02    
-65.231628    -8.267404   -2840.35931    1588.3113   -188.138017
[11,]   -38.160872   -0.4705178    13.684272 -3.011653e+01    
-11.397032   -12.104223      15.81316    -382.3265    127.455515
[12,]   -29.873503   -3.5847738    72.066398 -1.765772e+02    
-48.690053   -27.950744    4192.60441   -3456.6969    468.018755
[13,]  -292.912538   -2.3574936   -31.391468 -4.793227e+01     
-2.417331    -7.499306    -694.77118     252.8609    -31.070752
[14,] -3764.642188   -4.4360095    44.439705 -1.326567e+02     
29.048922   -66.464994    -469.90235    1383.0215    493.226459
[15,] 32619.378440  -19.1405337  -191.118547 -3.797848e+02     
12.457206   -67.412229   -3534.26399   -4364.0650   -213.676684
[16,]   -19.140534   83.3564695    99.644738  8.638272e+02   
-113.919302    57.787450     568.33498    -256.6745     95.290072
[17,]  -191.118547   99.6447377  8321.518528 -2.336166e+03  
-2504.879964  -116.069782   23410.78444   -7737.2818   -413.061830
[18,]  -379.784795  863.8272298 -2336.166492  2.031025e+04    
327.923878  1449.493218    5210.53272    -331.5682   2365.347580
[19,]    12.457206 -113.9193021 -2504.879964  3.279239e+02   
4729.120058  -302.478304   -8249.67578    9321.4641   -513.296202
[20,]   -67.412229   57.7874499  -116.069782  1.449493e+03   
-302.478304   798.715340    -120.24179   -1027.9713   1229.253152
[21,] -3534.263989  568.3349771 23410.784438  5.210533e+03  
-8249.675776  -120.241792 5103138.59345 -532580.1167 -21433.406503
[22,] -4364.064956 -256.6744716 -7737.281789 -3.315682e+02   
9321.464107 -1027.971291 -532580.11673 2448665.3906  -4467.482967
[23,]  -213.676684   95.2900719  -413.061830  2.365348e+03   
-513.296202  1229.253152  -21433.40650   -4467.4830  95608.545859

> hessian(xx, params) - hh

                [,1]          [,2]          [,3]          [,4]          
  [,5]          [,6]          [,7]          [,8]          [,9]          
[,10]         [,11]         [,12]         [,13]
  [1,] -2.174154e-02  2.675581e-03  0.0045735202 -5.465446e-03  
-1.116520e-03  0.0031433803  0.0044736188  0.0012992506 -1.952712e-03  
-0.0028477402  5.027177e-03  0.0037599058  0.0052436214
  [2,]  2.675581e-03  5.381220e-04 -0.0030651423 -6.248245e-03   
4.339480e-03 -0.0073984082  0.0005657110 -0.0017347483 -4.856674e-03   
0.0053176302  5.080799e-03 -0.0034874581 -0.0074703357
  [3,]  4.573520e-03 -3.065142e-03  0.0126275109 -2.306296e-03   
3.553370e-03  0.0002750179 -0.0003687140  0.0017763164  1.201199e-02   
0.0031159288  1.528691e-03 -0.0187605446  0.0019015601
  [4,] -5.465446e-03 -6.248245e-03 -0.0023062958  1.725601e-02  
-7.753215e-05  0.0003357576 -0.0040729665 -0.0152260468 -8.856897e-03  
-0.0067041255  4.409771e-05 -0.0242620770 -0.0010689367
  [5,] -1.116520e-03  4.339480e-03  0.0035533701 -7.753215e-05  
-1.529336e-02 -0.0064341321  0.0030975296  0.0094225420 -2.017720e-03   
0.0008753729 -6.537866e-03 -0.0041504293  0.0076763089
  [6,]  3.143380e-03 -7.398408e-03  0.0002750179  3.357576e-04  
-6.434132e-03 -0.0412513715  0.0050258520  0.0099552340  8.588000e-03  
-0.0046618529  5.627845e-03  0.0007554417  0.0008332370
  [7,]  4.473619e-03  5.657110e-04 -0.0003687140 -4.072967e-03   
3.097530e-03  0.0050258520 -0.0310195782  0.0072467573  3.347200e-03  
-0.0022839282 -5.785742e-03 -0.0013209058 -0.0014243946
  [8,]  1.299251e-03 -1.734748e-03  0.0017763164 -1.522605e-02   
9.422542e-03  0.0099552340  0.0072467573 -0.0213878149  1.971016e-03  
-0.0025075191 -2.432119e-03 -0.0101399815 -0.0033095579
  [9,] -1.952712e-03 -4.856674e-03  0.0120119931 -8.856897e-03  
-2.017720e-03  0.0085880000  0.0033472002  0.0019710163 -4.430788e-03  
-0.0054263026 -8.276628e-04 -0.0089457676 -0.0060692192
[10,] -2.847740e-03  5.317630e-03  0.0031159288 -6.704125e-03   
8.753729e-04 -0.0046618529 -0.0022839282 -0.0025075191 -5.426303e-03  
-0.0300369404 -4.362219e-04 -0.0041943211  0.0041312709
[11,]  5.027177e-03  5.080799e-03  0.0015286913  4.409771e-05  
-6.537866e-03  0.0056278445 -0.0057857419 -0.0024321188 -8.276628e-04  
-0.0004362219 -2.078431e-02 -0.0077289030 -0.0001138733
[12,]  3.759906e-03 -3.487458e-03 -0.0187605446 -2.426208e-02  
-4.150429e-03  0.0007554417 -0.0013209058 -0.0101399815 -8.945768e-03  
-0.0041943211 -7.728903e-03  0.0667107794 -0.0011886298
[13,]  5.243621e-03 -7.470336e-03  0.0019015601 -1.068937e-03   
7.676309e-03  0.0008332370 -0.0014243946 -0.0033095579 -6.069219e-03   
0.0041312709 -1.138733e-04 -0.0011886298 -0.0102566466
[14,] -4.512078e-03  6.329052e-03  0.0043097096 -6.262132e-03   
7.271405e-03  0.0032390754 -0.0017607563  0.0091437088  1.208078e-02   
0.0055714542  7.360190e-03 -0.0105033416 -0.0000836716
[15,]  3.813757e-03 -2.971857e-03  0.0139855283  5.886356e-03  
-1.626739e-02  0.0019036593  0.0049630113 -0.0010414008 -1.031374e-03  
-0.0023530046  1.014852e-02  0.0016194862  0.0033557662
[16,] -5.519030e-03  5.346496e-05 -0.0009618441 -4.931706e-03  
-3.079761e-03  0.0026905827 -0.0031935818 -0.0015436450  8.369026e-03   
0.0008270508  1.242832e-04 -0.0002401364  0.0061538743
[17,]  4.776049e-03 -1.151523e-03  0.0066035479 -1.135960e-02   
6.968132e-03  0.0021026348 -0.0006667256  0.0016224097  1.695299e-03  
-0.0024531098 -2.556583e-03  0.0004799392 -0.0061534164
[18,]  6.129375e-06  1.891128e-03 -0.0112895132 -9.184442e-03   
1.181774e-02  0.0067484503 -0.0035722955  0.0026908638  6.118690e-04  
-0.0021959342 -3.924641e-03 -0.0034539168  0.0029756813
[19,] -6.748987e-04 -2.306801e-03 -0.0031476283  5.164174e-03  
-4.481530e-03 -0.0026219509 -0.0023168610  0.0014504421 -9.241644e-04   
0.0012215803  5.373212e-03 -0.0041398000 -0.0054653719
[20,]  1.855820e-03  2.683495e-03 -0.0062143107  1.239519e-02  
-3.653245e-03  0.0107655543 -0.0091255232  0.0009132156 -9.818685e-05  
-0.0029384774  7.410119e-03 -0.0029292879 -0.0066907956
[21,]  6.937942e-03  6.023135e-02 -0.0059286185 -5.185891e-03   
8.235450e-02 -0.5279987913  0.1448606613  0.0017593525 -4.176498e-02  
-0.0161059468  6.337541e-03 -0.0459771204  0.0046360941
[22,]  7.807939e-03 -1.137521e-02  0.0180066118 -4.385140e-02   
1.412810e-02  0.0402804010 -0.0241446273 -0.0131691950 -6.235801e-02  
-0.0082738265  1.342096e-02 -0.1189830478  0.0044892077
[23,] -1.585136e-03 -3.327251e-04  0.0068438419 -2.395877e-02  
-5.226395e-04 -0.0033223756  0.0043186677 -0.0182913958 -2.185717e-02   
0.0018760011  4.373088e-04 -0.0308054882 -0.0041019134
               [,14]         [,15]         [,16]         [,17]          
[,18]         [,19]         [,20]         [,21]        [,22]          
[,23]
  [1,] -0.0045120783  0.0038137571 -5.519030e-03  0.0047760491   
6.129375e-06 -0.0006748987  1.855820e-03  0.0069379417  0.007807939  
-0.0015851357
  [2,]  0.0063290515 -0.0029718566  5.346496e-05 -0.0011515230   
1.891128e-03 -0.0023068005  2.683495e-03  0.0602313476 -0.011375205  
-0.0003327251
  [3,]  0.0043097096  0.0139855283 -9.618441e-04  0.0066035479  
-1.128951e-02 -0.0031476283 -6.214311e-03 -0.0059286185  0.018006612   
0.0068438419
  [4,] -0.0062621325  0.0058863563 -4.931706e-03 -0.0113595996  
-9.184442e-03  0.0051641741  1.239519e-02 -0.0051858913 -0.043851395  
-0.0239587711
  [5,]  0.0072714047 -0.0162673916 -3.079761e-03  0.0069681318   
1.181774e-02 -0.0044815295 -3.653245e-03  0.0823544982  0.014128099  
-0.0005226395
  [6,]  0.0032390754  0.0019036593  2.690583e-03  0.0021026348   
6.748450e-03 -0.0026219509  1.076555e-02 -0.5279987913  0.040280401  
-0.0033223756
  [7,] -0.0017607563  0.0049630113 -3.193582e-03 -0.0006667256  
-3.572295e-03 -0.0023168610 -9.125523e-03  0.1448606613 -0.024144627   
0.0043186677
  [8,]  0.0091437088 -0.0010414008 -1.543645e-03  0.0016224097   
2.690864e-03  0.0014504421  9.132156e-04  0.0017593525 -0.013169195  
-0.0182913958
  [9,]  0.0120807757 -0.0010313741  8.369026e-03  0.0016952993   
6.118690e-04 -0.0009241644 -9.818685e-05 -0.0417649796 -0.062358010  
-0.0218571699
[10,]  0.0055714542 -0.0023530046  8.270508e-04 -0.0024531098  
-2.195934e-03  0.0012215803 -2.938477e-03 -0.0161059468 -0.008273826   
0.0018760011
[11,]  0.0073601903  0.0101485229  1.242832e-04 -0.0025565827  
-3.924641e-03  0.0053732115  7.410119e-03  0.0063375408  0.013420956   
0.0004373088
[12,] -0.0105033416  0.0016194862 -2.401364e-04  0.0004799392  
-3.453917e-03 -0.0041398000 -2.929288e-03 -0.0459771204 -0.118983048  
-0.0308054882
[13,] -0.0000836716  0.0033557662  6.153874e-03 -0.0061534164   
2.975681e-03 -0.0054653719 -6.690796e-03  0.0046360941  0.004489208  
-0.0041019134
[14,]  0.0175798113  0.0902339108 -2.659874e-03  0.0020223468   
6.166098e-03  0.0079064466  1.558615e-03 -0.0389437950 -0.001049385   
0.0140569129
[15,]  0.0902339108 -0.1323017313 -5.801023e-04 -0.0026411842   
1.452162e-02  0.0078649448 -5.490748e-03  0.1736938980  0.022812160  
-0.0005974097
[16,] -0.0026598737 -0.0005801023 -1.340357e-02 -0.0022349675   
4.598005e-03  0.0028781256  7.908915e-03  0.0009316806 -0.006869086  
-0.0039344365
[17,]  0.0020223468 -0.0026411842 -2.234967e-03  0.0136455855   
2.514638e-03  0.0013840837  2.861133e-04  0.0897969890 -0.010487408   
0.0070791040
[18,]  0.0061660984  0.0145216196  4.598005e-03  0.0025146382  
-5.823754e-03 -0.0022082184 -2.570122e-03 -0.0021295272  0.009604257  
-0.0039216392
[19,]  0.0079064466  0.0078649448  2.878126e-03  0.0013840837  
-2.208218e-03 -0.0027442362 -3.267145e-03 -0.0297189825 -0.008732238   
0.0011612826
[20,]  0.0015586155 -0.0054907482  7.908915e-03  0.0002861133  
-2.570122e-03 -0.0032671450 -3.148659e-02 -0.0018014206 -0.002723892  
-0.0032563813
[21,] -0.0389437950  0.1736938980  9.316806e-04  0.0897969890  
-2.129527e-03 -0.0297189825 -1.801421e-03  3.0560996104 -0.281465531  
-0.0451748433
[22,] -0.0010493852  0.0228121604 -6.869086e-03 -0.0104874083   
9.604257e-03 -0.0087322380 -2.723892e-03 -0.2814655309  1.216295037  
-0.1057700633
[23,]  0.0140569129 -0.0005974097 -3.934436e-03  0.0070791040  
-3.921639e-03  0.0011612826 -3.256381e-03 -0.0451748433 -0.105770063   
0.0580663878

The maximum absolute value of this difference is 3.0561, which appears  
in [21,21]; no other difference is within an order of magnitude of  
that size. A histogram of the log transforms of the absolute values of  
these differences is attached.

John Kingston
--------------------------------
Professor and Head
Linguistics Department
University of Massachusetts
150 Hicks Way, 226 South College
Amherst, MA 01003-9274
1-413-545-6837, fax -2792
jkingston at linguist.umass.edu
https://blogs.umass.edu/jkingstn/
-------------- next part --------------
A non-text attachment was scrubbed...
Name: histHessianDifs_26May14.pdf
Type: application/pdf
Size: 108217 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140526/9baf5826/attachment-0001.pdf>

From highstat at highstat.com  Mon May 26 22:14:47 2014
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Mon, 26 May 2014 21:14:47 +0100
Subject: [R-sig-ME] Mixed modelling and GLMM course in Banff, Canada
Message-ID: <5383A0B7.8090706@highstat.com>

We would like to announce the following statistics course:

Topic:     Introduction to MCMC, Linear mixed effects models and GLMM 
with R
Where:    Parks Canada, Banff, Canada
When:     22-26 September, 2014
Flyer:      http://www.highstat.com/Courses/Flyer2014_09Banff.pdf
Website: http://www.highstat.com/statscourse.htm


Kind regards,

Alain Zuur



-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From jkingston at linguist.umass.edu  Mon May 26 05:34:52 2014
From: jkingston at linguist.umass.edu (John Kingston)
Date: Sun, 25 May 2014 23:34:52 -0400
Subject: [R-sig-ME] false positives?
Message-ID: <20140525233452.71905nrya87d8evw@umail.oit.umass.edu>

I received the warning messages below in response to running the  
second model displayed below. The data file is attached to this message.

Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
   Model failed to converge with max|grad| = 0.212882 (tol = 1e-06)
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
   Model is nearly unidentifiable: very large eigenvalue
  - Rescale variables?

sfHIndR <- glmer(fRspRN ~ cStep +
	h1Context +
	h2Context +
	cQRT +
	(1 + cStep +
	h1Context +
	h2Context +
	cQRT | participant),
	family = "binomial",
	control = glmerControl(optCtrl = list(maxfun=1000000)),
	data = sfCompiled)

summary(sfHIndR)

# interactions with RT

sfHSxRtCxRtR <- glmer(fRspRN ~ cStep * cQRT +
	h1Context * cQRT +
	h2Context * cQRT +
	(1 + cStep +
	h1Context +
	h2Context +
	cQRT | participant),
	family = "binomial",
	control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun=1000000)),
	data = sfCompiled)

The test suggested in a earlier post produced the output below. These  
values look small.

gg <- sfHSxRtCxRtR at optinfo$derivs$grad
hh <- sfHSxRtCxRtR at optinfo$derivs$Hessian
vv <- sqrt(diag(solve(hh/2)))
summary(abs(gg*vv))

#       Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
# 6.720e-07 3.108e-05 8.866e-05 2.542e-04 1.745e-04 3.345e-03

Any advice about how to proceed? Thanks.
Best,
John



John Kingston
--------------------------------
Professor and Head
Linguistics Department
University of Massachusetts
150 Hicks Way, 226 South College
Amherst, MA 01003-9274
1-413-545-6837, fax -2792
jkingston at linguist.umass.edu
https://blogs.umass.edu/jkingstn/

From lucinda.kirkpatrick at stir.ac.uk  Mon May 26 19:28:37 2014
From: lucinda.kirkpatrick at stir.ac.uk (Lucinda Kirkpatrick)
Date: Mon, 26 May 2014 17:28:37 +0000 (UTC)
Subject: [R-sig-ME] Coefplot2 availability? or How to interpret?
References: <CAATUFed8ZWm=dYNnUepZsPy80RwG6H=2uwJ9OGKyo0VKLr9T-g@mail.gmail.com>
	<loom.20130129T230812-393@post.gmane.org>
Message-ID: <loom.20140526T191451-369@post.gmane.org>



Ben Bolker <bbolker at ...> writes:


> 
> The error message is giving you a hint that you should try installing
> from the source package:
> 
> install.packages("coefplot2",repos="http://www.math.mcmaster.ca/bolker/R",
>     type="source")
> 
>   That should work even if you don't have the tools installed for 
compiling
> packages from source, because the coefplot2 package doesn't require any
> binary compilation (all R has to do is unpack an archive file and put the
> bits in the right places, which it shouldn't need any external tools to 
do).
> 

I want to do something very similar - use coefplot2 to 
present a zero inflated negatively binomial model generated with glmmADMB.

I am looking at how forest stand type and ground vegetation height inlfuence 
relative activity of bats.  SiteID and Forest are random factors, the number 
of stand types varies between forest.  Count is constrained at zero and 
residuals are overdispersed.  I have tried link provided and 
still have an error:

ERROR: dependency 'reshape' is not available for package 'coefplot2'
* removing 'H:/My Documents/R/win-library/3.1/coefplot2'
Warning in install.packages :
  running command '"C:/PROGRA~1/R/R-31~1.0/bin/i386/R" CMD INSTALL -l "H:\My 
Documents\R\win-library\3.1" 
C:\Users\lk17\AppData\Local\Temp\RtmpEnKhgA/downloaded_packages/coefplot2_0.
1.3.2.tar.gz' had status 1
Warning in install.packages :
  installation of package ?coefplot2? had non-zero exit status


 I am using the latest version of R (spring dance) - 
does coefplot2 work with this?

Thank you, Luci


From bbolker at gmail.com  Tue May 27 19:56:18 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 27 May 2014 13:56:18 -0400
Subject: [R-sig-ME] false positives?
In-Reply-To: <20140525233452.71905nrya87d8evw@umail.oit.umass.edu>
References: <20140525233452.71905nrya87d8evw@umail.oit.umass.edu>
Message-ID: <5384D1C2.7030000@gmail.com>


   The test you show below (although solve(Hessian,grad) is slightly
better) suggests that the first warning is indeed a false positive.  I
would expect the second message to go away if you scale (centering is
probably a good idea too) your continuous predictors.  (The t-statistics
shouldn't change.)

  Ben Bolker

On 14-05-25 11:34 PM, John Kingston wrote:
> I received the warning messages below in response to running the second
> model displayed below. The data file is attached to this message.
> 
> Warning messages:
> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.212882 (tol = 1e-06)
> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model is nearly unidentifiable: very large eigenvalue
>  - Rescale variables?
> 
> sfHIndR <- glmer(fRspRN ~ cStep +
>     h1Context +
>     h2Context +
>     cQRT +
>     (1 + cStep +
>     h1Context +
>     h2Context +
>     cQRT | participant),
>     family = "binomial",
>     control = glmerControl(optCtrl = list(maxfun=1000000)),
>     data = sfCompiled)
> 
> summary(sfHIndR)
> 
> # interactions with RT
> 
> sfHSxRtCxRtR <- glmer(fRspRN ~ cStep * cQRT +
>     h1Context * cQRT +
>     h2Context * cQRT +
>     (1 + cStep +
>     h1Context +
>     h2Context +
>     cQRT | participant),
>     family = "binomial",
>     control = glmerControl(optimizer = "bobyqa", optCtrl =
> list(maxfun=1000000)),
>     data = sfCompiled)
> 
> The test suggested in a earlier post produced the output below. These
> values look small.
> 
> gg <- sfHSxRtCxRtR at optinfo$derivs$grad
> hh <- sfHSxRtCxRtR at optinfo$derivs$Hessian
> vv <- sqrt(diag(solve(hh/2)))
> summary(abs(gg*vv))
> 
> #       Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
> # 6.720e-07 3.108e-05 8.866e-05 2.542e-04 1.745e-04 3.345e-03
> 
> Any advice about how to proceed? Thanks.
> Best,
> John
> 
> 
> 
> John Kingston
> --------------------------------
> Professor and Head
> Linguistics Department
> University of Massachusetts
> 150 Hicks Way, 226 South College
> Amherst, MA 01003-9274
> 1-413-545-6837, fax -2792
> jkingston at linguist.umass.edu
> https://blogs.umass.edu/jkingstn/
> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From r.turner at auckland.ac.nz  Wed May 28 03:59:33 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 28 May 2014 13:59:33 +1200
Subject: [R-sig-ME] Anomalous results with glmer().
Message-ID: <53854305.8060809@auckland.ac.nz>



Some months back I sent an inquiry to this list concerning the analysis 
of some linguistics data with which I am involved.  I am *still* 
(psigh!!!) struggling with these data and am getting results which are
making no sense to me.

Basically if I fit a (reasonably sensible) model using an old version of 
lme4 (0.999999-0) I get sensible looking estimates for the fixed effect 
coefficients, but the estimates of the variances of the random effects 
are essentially zero.  Which is silly.

If I fit the same model using lme4 version 1.1-7 (and ignore the warning 
about failure to converge) I get sensible looking estimates of the 
variances of the random effects, but an impossibly wrong estimate
of at least one of the fixed effect coefficients.  (The estimate says 
that the success probability is larger for phoneme type "Mclus" than it 
is for the baseline type "Fclus".  However a raw tabulation show that 
the success probability for Mclus is much, much smaller than for Fclus.

I have included more detail in the attached file notesME.txt for those 
who are interested.  This file induces explicit specification of the 
model that I used. The results of the fit using version 0.999999-0 are 
in the file oldLme4Rslts.txt; the results from version 1.1-7 are in 
newLme4Rslts.txt.

The data set is a bit too big to attach; it has 62601 records.  I have 
therefore made it available (as a *.csv file) on my web page:

     https://www.stat.auckland.ac.nz/~rolf

Click on "Linguistics data for R-SIG-ME".

I am really being driven nuts by this weirdness and would appreciate 
some avuncular advice from the knowledgeable.  (Ben???)

cheers,

Rolf

--
Rolf Turner
Technical Editor ANZJS
-------------- next part --------------

The model that I used in R with the lme4 package was:

    fit <- glmer(y ~ sex + type + age + (1|student) + (1|word),
                 family=binomial,data=X)

The type variable is the type of phoneme.  Of course sex has two
levels, M and F. The age variable consists of the midpoint of the
age intervals (5.0 - 5.5, 5.6 - 5.9, 6.0 - 6.5, etc.) into which the
students were classified.  This variable is treated as a numerical
covariate in the model that I use.  The variables student and word
are treated as random effects.  The response y is binary; 1 for
correct pronunciation of the phoneme in question, 0 for incorrect.

The model given above is simplistic and I was previously advised
to include interactions between the fixed effects and the random
effects.  However (a) this brings my computer to its knees, and
(b) sufficient unto the day is the evil thereof.

What is puzzling me is the estimates of the type coefficients.
When I fit the model using an elderly version of lme4 (0.999999-0)
I get what appear to me to be sensible results for these (and
for the other fixed effects) but the variance estimates for the
random effects are essentially both zero.  This *can't* be so!

When I fit the model using the latest version of lme4 (1.1-7), installed
from the r-forge repository, I get sensible estimates for the variances
of the random effects, but something is very strange about the
estimates of the type coefficients.  Explicitly the Mclus (medial
consonant cluster) coefficient is positive --- "significantly" so
at the 0.10 level (p-value = about 0.08).

This is saying that Mclus yields higher success rate than the
baseline type Fclus (which is the first, and hence reference,
level of the type factor).  Note that I stick with the default
"treatment contrasts".

However when I tabulated successes by phoneme type and I got the
following table:

   type
y   Fclus  Fcon Iclus  Icon Mclus  Mcon Vowel
  0   245  1216  2451   793  1077  1113   768
  1   733  9216  2439  8661   227  9645 24017


Note that the (raw) success rate for "Mclus" is ***MUCH*** smaller
than for any other phoneme type, in particular than for Fclus.
So how can the model produce an indication that it is larger than
the success rate for "Fclus"?

Admittedly the raw tabulation does not (of course) allow for the random
student effect and word effect which are included in the "formal" model.
Even so, I find it hard to believe that there could be such a huge
discrepancy between the "raw result" an the "model result".

Also:  Why is there this complete discordance between the results from
lme4 0.999999-0 and those from lme4 1.1-7?

I am toadally mystified.
-------------- next part --------------

Results from lme4 version 0.999999-0:
=====================================

Generalized linear mixed model fit by the Laplace approximation 
Formula: y ~ sex + type + age + (1 | student) + (1 | word) 
   Data: lingDat 
   AIC   BIC logLik deviance
 35707 35807 -17843    35685
Random effects:
 Groups  Name        Variance   Std.Dev.  
 student (Intercept) 3.8939e-13 6.2401e-07
 word    (Intercept) 0.0000e+00 0.0000e+00
Number of obs: 62601, groups: student, 326; word, 50

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -0.80082    0.13114  -6.107 1.02e-09 ***
sexM        -0.10162    0.02838  -3.581 0.000343 ***
typeFcon     0.93906    0.08036  11.686  < 2e-16 ***
typeIclus   -1.11890    0.07972 -14.035  < 2e-16 ***
typeIcon     1.30674    0.08309  15.726  < 2e-16 ***
typeMclus   -2.69497    0.10455 -25.778  < 2e-16 ***
typeMcon     1.07398    0.08080  13.292  < 2e-16 ***
typeVowel    2.36251    0.08287  28.507  < 2e-16 ***
age          0.30475    0.01667  18.280  < 2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
          (Intr) sexM   typFcn typIcl typIcn typMcl typMcn typVwl
sexM      -0.170                                                 
typeFcon  -0.533 -0.002                                          
typeIclus -0.510  0.005  0.861                                   
typeIcon  -0.518 -0.003  0.827  0.833                            
typeMclus -0.370  0.009  0.656  0.663  0.635                     
typeMcon  -0.531 -0.002  0.850  0.856  0.822  0.653              
typeVowel -0.523 -0.003  0.829  0.835  0.802  0.636  0.824       
age       -0.815  0.057  0.012 -0.023  0.014 -0.041  0.013  0.019
-------------- next part --------------

Results from lme4 version 1.1-7:
================================

Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: y ~ sex + type + age + (1 | student) + (1 | word)
   Data: lingDat

     AIC      BIC   logLik deviance df.resid 
 27456.3  27555.8 -13717.2  27434.3    62590 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-37.166   0.049   0.122   0.267  10.350 

Random effects:
 Groups  Name        Variance Std.Dev.
 student (Intercept) 0.166    0.4074  
 word    (Intercept) 3.301    1.8170  
Number of obs: 62601, groups:  student, 326 word, 50

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -3.55421    0.37306   -9.53   <2e-16 ***
sexM        -0.13841    0.05597   -2.47   0.0134 *  
typeFcon     4.45879    0.17727   25.15   <2e-16 ***
typeIclus    1.47383    0.17061    8.64   <2e-16 ***
typeIcon     4.33003    0.18051   23.99   <2e-16 ***
typeMclus    0.33574    0.19876    1.69   0.0912 .  
typeMcon     3.41373    0.17724   19.26   <2e-16 ***
typeVowel    5.67714    0.17745   31.99   <2e-16 ***
age          0.39553    0.03253   12.16   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
          (Intr) sexM   typFcn typIcl typIcn typMcl typMcn typVwl
sexM      -0.120                                                 
typeFcon  -0.435 -0.005                                          
typeIclus -0.410 -0.001  0.912                                   
typeIcon  -0.431 -0.005  0.947  0.883                            
typeMclus -0.375  0.001  0.838  0.804  0.841                     
typeMcon  -0.429 -0.004  0.936  0.881  0.938  0.850              
typeVowel -0.436 -0.006  0.953  0.906  0.943  0.846  0.939       
age       -0.568  0.068  0.020  0.002  0.019 -0.006  0.015  0.025

From bbolker at gmail.com  Wed May 28 04:14:56 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 27 May 2014 22:14:56 -0400
Subject: [R-sig-ME] Anomalous results with glmer().
In-Reply-To: <53854305.8060809@auckland.ac.nz>
References: <53854305.8060809@auckland.ac.nz>
Message-ID: <538546A0.8030007@gmail.com>

  I will try to have a look.
  I agree that the positive variance estimates seem (much) more sensible
in a data set of this size ...

  Obviously it will take me a little while to get all the fits done on a
data set of this non-trivial size, but here are some preliminary thoughts:

 * I will try out the 'allFit.R' code mentioned on the mailing list
previously just to see how the results from 5 or 6 different optimizers
compare.
 * I will try lme4.0 and hope to get results the same as lme4 0.999999-0
 * I will evaluate the deviances of both the old and new fits to see
which fit is actually better, and use bbmle::slicetrans to look at the
shape of the likelihood surface between the two points

  As for explaining the 'crazy' result, if it actually turns out to be
(close to) the MLE for this data set: I would look at pictures of the
data, predictions, etc., and try to see if there's some sort of
confounder/Simpson's paradox thing going on here where the marginal
effect (= raw tabulation) is in fact very different from the conditional
effect ...

  Ben Bolker



On 14-05-27 09:59 PM, Rolf Turner wrote:
> 
> 
> Some months back I sent an inquiry to this list concerning the analysis
> of some linguistics data with which I am involved.  I am *still*
> (psigh!!!) struggling with these data and am getting results which are
> making no sense to me.
> 
> Basically if I fit a (reasonably sensible) model using an old version of
> lme4 (0.999999-0) I get sensible looking estimates for the fixed effect
> coefficients, but the estimates of the variances of the random effects
> are essentially zero.  Which is silly.
> 
> If I fit the same model using lme4 version 1.1-7 (and ignore the warning
> about failure to converge) I get sensible looking estimates of the
> variances of the random effects, but an impossibly wrong estimate
> of at least one of the fixed effect coefficients.  (The estimate says
> that the success probability is larger for phoneme type "Mclus" than it
> is for the baseline type "Fclus".  However a raw tabulation show that
> the success probability for Mclus is much, much smaller than for Fclus.
> 
> I have included more detail in the attached file notesME.txt for those
> who are interested.  This file induces explicit specification of the
> model that I used. The results of the fit using version 0.999999-0 are
> in the file oldLme4Rslts.txt; the results from version 1.1-7 are in
> newLme4Rslts.txt.
> 
> The data set is a bit too big to attach; it has 62601 records.  I have
> therefore made it available (as a *.csv file) on my web page:
> 
>     https://www.stat.auckland.ac.nz/~rolf
> 
> Click on "Linguistics data for R-SIG-ME".
> 
> I am really being driven nuts by this weirdness and would appreciate
> some avuncular advice from the knowledgeable.  (Ben???)
> 
> cheers,
> 
> Rolf
> 
> -- 
> Rolf Turner
> Technical Editor ANZJS
> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From r.turner at auckland.ac.nz  Wed May 28 04:26:31 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 28 May 2014 14:26:31 +1200
Subject: [R-sig-ME] Anomalous results with glmer().
In-Reply-To: <538546A0.8030007@gmail.com>
References: <53854305.8060809@auckland.ac.nz> <538546A0.8030007@gmail.com>
Message-ID: <53854957.2000601@auckland.ac.nz>


Thanks Ben.  I now feel that there is hope! :-)

On 28/05/14 14:14, Ben Bolker wrote:

<SNIP>

>    As for explaining the 'crazy' result, if it actually turns out to be
> (close to) the MLE for this data set: I would look at pictures of the
> data, predictions, etc., and try to see if there's some sort of
> confounder/Simpson's paradox thing going on here where the marginal
> effect (= raw tabulation) is in fact very different from the conditional
> effect ...

I have tried to think of ways, like Simpson's paradox, to explain the 
anomaly, but nothing sensible comes to me.  If anyone can point me in 
the right direction .....

And is it not weird that the old version gives an intuitively plausible
estimate for the Mclus coefficient whereas the new version doesn't?

cheers,

Rolf


From David.Duffy at qimr.edu.au  Wed May 28 04:49:55 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 28 May 2014 12:49:55 +1000
Subject: [R-sig-ME] Anomalous results with glmer().
In-Reply-To: <53854305.8060809@auckland.ac.nz>
References: <53854305.8060809@auckland.ac.nz>
Message-ID: <alpine.LMD.2.00.1405281240430.16613@orpheus.qimr.edu.au>

On Wed, 28 May 2014, Rolf Turner wrote:

> Some months back I sent an inquiry to this list concerning the analysis of 
> some linguistics data with which I am involved.
> essentially zero.  Which is silly.
>
> If I fit the same model using lme4 version 1.1-7 (and ignore the warning 
> about failure to converge) I get sensible looking estimates of the variances 
> of the random effects, but an impossibly wrong estimate
> of at least one of the fixed effect coefficients.  (The estimate says that 
> the success probability is larger for phoneme type "Mclus" than it is for the 
> baseline type "Fclus".  However a raw tabulation show that the success 
> probability for Mclus is much, much smaller than for Fclus.

FWIW,
                   MClus
glm               -2.6950  (0.10455)

glmer (+Stud)     -2.74464 (0.10536)
glmer (+Words)     0.36826 (0.19783)
glmer (+S+W)       0.33574 (0.19981)

glmmML (+Stud)    -2.7444 (0.10546)
glmmML (+Words)    0.3683 (0.19816)

It's words that will always get you into trouble...

| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From bbolker at gmail.com  Wed May 28 04:55:24 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 27 May 2014 22:55:24 -0400
Subject: [R-sig-ME] Anomalous results with glmer().
In-Reply-To: <alpine.LMD.2.00.1405281240430.16613@orpheus.qimr.edu.au>
References: <53854305.8060809@auckland.ac.nz>
	<alpine.LMD.2.00.1405281240430.16613@orpheus.qimr.edu.au>
Message-ID: <5385501C.5030001@gmail.com>

  Very useful observation.  Thanks.

On 14-05-27 10:49 PM, David Duffy wrote:
> On Wed, 28 May 2014, Rolf Turner wrote:
> 
>> Some months back I sent an inquiry to this list concerning the
>> analysis of some linguistics data with which I am involved.
>> essentially zero.  Which is silly.
>>
>> If I fit the same model using lme4 version 1.1-7 (and ignore the
>> warning about failure to converge) I get sensible looking estimates of
>> the variances of the random effects, but an impossibly wrong estimate
>> of at least one of the fixed effect coefficients.  (The estimate says
>> that the success probability is larger for phoneme type "Mclus" than
>> it is for the baseline type "Fclus".  However a raw tabulation show
>> that the success probability for Mclus is much, much smaller than for
>> Fclus.
> 
> FWIW,
>                   MClus
> glm               -2.6950  (0.10455)
> 
> glmer (+Stud)     -2.74464 (0.10536)
> glmer (+Words)     0.36826 (0.19783)
> glmer (+S+W)       0.33574 (0.19981)
> 
> glmmML (+Stud)    -2.7444 (0.10546)
> glmmML (+Words)    0.3683 (0.19816)
> 
> It's words that will always get you into trouble...
> 
> | David Duffy (MBBS PhD)
> | email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax:
> -0101
> | Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
> | 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From r.turner at auckland.ac.nz  Wed May 28 05:02:40 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 28 May 2014 15:02:40 +1200
Subject: [R-sig-ME] Anomalous results with glmer().
In-Reply-To: <alpine.LMD.2.00.1405281240430.16613@orpheus.qimr.edu.au>
References: <53854305.8060809@auckland.ac.nz>
	<alpine.LMD.2.00.1405281240430.16613@orpheus.qimr.edu.au>
Message-ID: <538551D0.3080100@auckland.ac.nz>


Thanks.

Yes.  That makes quite a bit of sense.  Phoneme types are going to be 
partially confounded with words.  A glimmer of light is starting to seep 
through the cracks.

Still leaves a bit of a puzzle as to why the 0.999999-0 and 1.1-7 
results are so different.

cheers,

Rolf

On 28/05/14 14:49, David Duffy wrote:
> On Wed, 28 May 2014, Rolf Turner wrote:
>
>> Some months back I sent an inquiry to this list concerning the
>> analysis of some linguistics data with which I am involved.
>> essentially zero.  Which is silly.
>>
>> If I fit the same model using lme4 version 1.1-7 (and ignore the
>> warning about failure to converge) I get sensible looking estimates of
>> the variances of the random effects, but an impossibly wrong estimate
>> of at least one of the fixed effect coefficients.  (The estimate says
>> that the success probability is larger for phoneme type "Mclus" than
>> it is for the baseline type "Fclus".  However a raw tabulation show
>> that the success probability for Mclus is much, much smaller than for
>> Fclus.
>
> FWIW,
>                    MClus
> glm               -2.6950  (0.10455)
>
> glmer (+Stud)     -2.74464 (0.10536)
> glmer (+Words)     0.36826 (0.19783)
> glmer (+S+W)       0.33574 (0.19981)
>
> glmmML (+Stud)    -2.7444 (0.10546)
> glmmML (+Words)    0.3683 (0.19816)
>
> It's words that will always get you into trouble...


From jkingston at linguist.umass.edu  Wed May 28 06:02:18 2014
From: jkingston at linguist.umass.edu (John Kingston)
Date: Wed, 28 May 2014 00:02:18 -0400
Subject: [R-sig-ME] following up reply regarding false positives
Message-ID: <20140528000218.10007070rhwlgqmi@umail.oit.umass.edu>

Dear Ben,

Thanks for your reply. I've responded to your suggestions below  
between *** ... ***


    The test you show below (although solve(Hessian,grad) is slightly
better) suggests that the first warning is indeed a false positive.

*** I ran the second test you suggest and reported the results in  
another post to this list with the subject heading "update to previous  
message about false positives". Here's the result again:

relgrad <- with(sfHSxRtCxRtR at optinfo$derivs, solve(Hessian, gradient))
max(abs(relgrad))

yielded a small value, too: 0.0003628809

Further tests following your suggestions to other who have posted to  
this list on this warning are also included in that message.

I've also run the allFit.R function on this model that you posted in  
yet another message. The outcome was OK for all optimizers. And this  
has been true for models of other data that have produced the same  
warning. ***

  I
would expect the second message to go away if you scale (centering is
probably a good idea too) your continuous predictors.  (The t-statistics
shouldn't change.)

*** I routinely center my predictors, and that was done in the model  
this message is about. Scaling is not an adjustment I'm familiar with.  
Should the continuous predictors be scaled down or up?

Current values of continuous predictors:

cStep:  -19 -15 -11  -7  -3  -1   1   3   7  11  15  19
cQRT:   -9 -5 -1  3  7  9

h1Context and h2Context are Helmert contrasts:

h1Context: 6 -3 -3
h2Context: 0 1 -1

***

Thanks again.
Best,
John

John Kingston
--------------------------------
Professor and Head
Linguistics Department
University of Massachusetts
150 Hicks Way, 226 South College
Amherst, MA 01003-9274
1-413-545-6837, fax -2792
jkingston at linguist.umass.edu
https://blogs.umass.edu/jkingstn/

John Kingston
--------------------------------
Professor and Head
Linguistics Department
University of Massachusetts
150 Hicks Way, 226 South College
Amherst, MA 01003-9274
1-413-545-6837, fax -2792
jkingston at linguist.umass.edu
https://blogs.umass.edu/jkingstn/


From bbolker at gmail.com  Wed May 28 20:04:31 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 28 May 2014 14:04:31 -0400
Subject: [R-sig-ME] Coefplot2 availability? or How to interpret?
In-Reply-To: <loom.20140526T191451-369@post.gmane.org>
References: <CAATUFed8ZWm=dYNnUepZsPy80RwG6H=2uwJ9OGKyo0VKLr9T-g@mail.gmail.com>	<loom.20130129T230812-393@post.gmane.org>
	<loom.20140526T191451-369@post.gmane.org>
Message-ID: <5386252F.5030504@gmail.com>

On 14-05-26 01:28 PM, Lucinda Kirkpatrick wrote:
> 
> 
> Ben Bolker <bbolker at ...> writes:
> 
> 
>>
>> The error message is giving you a hint that you should try installing
>> from the source package:
>>
>> install.packages("coefplot2",repos="http://www.math.mcmaster.ca/bolker/R",
>>     type="source")
>>
>>   That should work even if you don't have the tools installed for 
> compiling
>> packages from source, because the coefplot2 package doesn't require any
>> binary compilation (all R has to do is unpack an archive file and put the
>> bits in the right places, which it shouldn't need any external tools to 
> do).
>>
> 
> I want to do something very similar - use coefplot2 to 
> present a zero inflated negatively binomial model generated with glmmADMB.
> 
> I am looking at how forest stand type and ground vegetation height inlfuence 
> relative activity of bats.  SiteID and Forest are random factors, the number 
> of stand types varies between forest.  Count is constrained at zero and 
> residuals are overdispersed.  I have tried link provided and 
> still have an error:
> 
> ERROR: dependency 'reshape' is not available for package 'coefplot2'
> * removing 'H:/My Documents/R/win-library/3.1/coefplot2'
> Warning in install.packages :
>   running command '"C:/PROGRA~1/R/R-31~1.0/bin/i386/R" CMD INSTALL -l "H:\My 
> Documents\R\win-library\3.1" 
> C:\Users\lk17\AppData\Local\Temp\RtmpEnKhgA/downloaded_packages/coefplot2_0.
> 1.3.2.tar.gz' had status 1
> Warning in install.packages :
>   installation of package ?coefplot2? had non-zero exit status
> 
> 
>  I am using the latest version of R (spring dance) - 
> does coefplot2 work with this?
> 
> Thank you, Luci

  Hmmm.  I don't quite understand why it's looking for 'reshape', but
the short answer to this one is:  do install.packages("reshape") (to get
the reshape package from your regular CRAN mirror), then try again.  If
you get another error, install _that_ package, then try one more time.
Keep going until it works.  (Sorry this is a bit tedious, but it should
only take a few minutes -- in the meantime I will try to clean things up.)

  Ben Bolker


From jake987722 at hotmail.com  Thu May 29 07:32:02 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Wed, 28 May 2014 23:32:02 -0600
Subject: [R-sig-ME] citable references for profile likelihood CIs
Message-ID: <BAY172-W47F99677C7056D33C2DFB6CB240@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140528/9f97c5c5/attachment.pl>

From jwiley.psych at gmail.com  Thu May 29 09:17:51 2014
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Thu, 29 May 2014 00:17:51 -0700
Subject: [R-sig-ME] citable references for profile likelihood CIs
In-Reply-To: <BAY172-W47F99677C7056D33C2DFB6CB240@phx.gbl>
References: <BAY172-W47F99677C7056D33C2DFB6CB240@phx.gbl>
Message-ID: <CANz9Z_JUeHiO3yCoP0DtG_HYsjv9Si=cFkuBR+3PWqwFfdb-Og@mail.gmail.com>

Hi Jake,

As a teaching article, this is nice:
http://www.tandfonline.com/doi/abs/10.1080/00031305.1995.10476112#.U4beUx1dWtY

Cheers,

Josh



On Wed, May 28, 2014 at 10:32 PM, Jake Westfall <jake987722 at hotmail.com> wrote:
> Hello list,
>
> I am writing up the results from an analysis involving mixed models, and I am considering including the profile likelihood confidence intervals (as computed by confint.merMod() in lme4 package) for all parameter estimates in the manuscript. Does anyone have suggestions for good references I can cite that explain basically how it works and why profile CIs are a Good Thing? All justificationary purposes aside, I would actually be interested in reading some of these myself.
>
> Jake
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Joshua F. Wiley
Ph.D. Student, UCLA Department of Psychology
http://joshuawiley.com/
Senior Analyst, Elkhart Group Ltd.
http://elkhartgroup.com
Office: 260.673.5518


From s06mw3 at abdn.ac.uk  Thu May 29 10:24:15 2014
From: s06mw3 at abdn.ac.uk (Matthew Wolak)
Date: Thu, 29 May 2014 09:24:15 +0100
Subject: [R-sig-ME] citable references for profile likelihood CIs
In-Reply-To: <BAY172-W47F99677C7056D33C2DFB6CB240@phx.gbl>
References: <BAY172-W47F99677C7056D33C2DFB6CB240@phx.gbl>
Message-ID: <5386EEAF.9010608@abdn.ac.uk>

Hi Jake,

Meyer (2008. Likelihood calculations to evaluate experimental designs to
estimate genetic variances. Heredity. 101:212-221) makes a rather
compelling case for using profile likelihood CIs and does a good job
explaining them.

Sincerely,
Matthew

....................................................
Dr. Matthew E. Wolak
School of Biological Sciences
Zoology Building
University of Aberdeen
Tillydrone Avenue
Aberdeen AB24 2TZ
office phone: +44 (0)1224 273255

On 29/05/14 06:32, Jake Westfall wrote:
> Hello list,
>
> I am writing up the results from an analysis involving mixed models, and I am considering including the profile likelihood confidence intervals (as computed by confint.merMod() in lme4 package) for all parameter estimates in the manuscript. Does anyone have suggestions for good references I can cite that explain basically how it works and why profile CIs are a Good Thing? All justificationary purposes aside, I would actually be interested in reading some of these myself.
>
> Jake
>       [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



The University of Aberdeen is a charity registered in Scotland, No SC013683.
Tha Oilthigh Obar Dheathain na charthannas cl?raichte ann an Alba, ?ir. SC013683.


From bbolker at gmail.com  Thu May 29 19:17:59 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 29 May 2014 17:17:59 +0000 (UTC)
Subject: [R-sig-ME] citable references for profile likelihood CIs
References: <BAY172-W47F99677C7056D33C2DFB6CB240@phx.gbl>
Message-ID: <loom.20140529T191552-873@post.gmane.org>

Jake Westfall <jake987722 at ...> writes:

> 
> Hello list,
 
> I am writing up the results from an analysis involving mixed models,
> and I am considering including the profile likelihood confidence
> intervals (as computed by confint.merMod() in lme4 package) for all
> parameter estimates in the manuscript. Does anyone have suggestions
> for good references I can cite that explain basically how it works
> and why profile CIs are a Good Thing? All justificationary purposes
> aside, I would actually be interested in reading some of these
> myself.

If you're writing for a sufficiently naive audience (e.g. typical
ecologists), I will be bold enough to suggest Chapter 6 of Bolker 2008
(_Ecological Models and Data in R_) ... not very rigorous though
(and not available online via university libraries).


From jake987722 at hotmail.com  Thu May 29 22:55:49 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Thu, 29 May 2014 14:55:49 -0600
Subject: [R-sig-ME] citable references for profile likelihood CIs
In-Reply-To: <loom.20140529T191552-873@post.gmane.org>
References: <BAY172-W47F99677C7056D33C2DFB6CB240@phx.gbl>,
	<loom.20140529T191552-873@post.gmane.org>
Message-ID: <BAY172-W48F04AD5A4A7D67C5ACDF5CB240@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140529/6fd0abed/attachment.pl>

From bbolker at gmail.com  Fri May 30 00:50:52 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 29 May 2014 18:50:52 -0400
Subject: [R-sig-ME] [Lme4-authors] lmer/glmer false positive in
	convergence problems
In-Reply-To: <DUB123-W113C352800B7D23B8EFA7FC0240@phx.gbl>
References: <DUB123-W210832C427C1609B6D7DDFC0240@phx.gbl>
	<DUB123-W21EDA8BCCF2D5592F17F8CC0240@phx.gbl>,
	<538799F1.9080900@lists.r-forge.r-project.org>
	<DUB123-W113C352800B7D23B8EFA7FC0240@phx.gbl>
Message-ID: <5387B9CC.7000005@gmail.com>

 [cc'ing to r-sig-mixed-models]

  To be frank, we don't really know.  The tolerance for this warning
will be set to 0.002 in the next release, but that just seems to us (OK,
me) to be for a fit that's supposed to be a decent numerical fit to the
minimum of a smooth, deterministic objective function.

  I would normally try the fit with
control=glmerControl(optimizer="bobyqa") and see if that helps ...

On 14-05-29 05:38 PM, Miren Arantzeta wrote:
> Sorry for disturbing you again. I keep getting the warning message in
> each model I fit, I would like to know under what value should give the
> max(abs(relgrad)) to consider the warning a false positive. The highest
> value I got was 0.08.
> 
> Thank you again.
> 
> /Miren
> /
> 
> 
>> From: bbolker at gmail.com
>> Date: Thu, 29 May 2014 16:34:57 -0400
>> To: arantzetam at hotmail.com
>> Subject: Re: [Lme4-authors] lmer/glmer false positive in convergence
> problems
>>
>>
>> Yes, that indicates a false positive (i.e. convergence is actually OK).
>>
>> On 14-05-29 01:36 PM, Miren Arantzeta wrote:
>> >
>> >
>> > Dear all,
>> >
>> > I write you in relation to the post of Ben Bolker about the lmer/glmer
>> > convergence warnings. I am modeling binomial data with glmer and I get
>> > this warning message (I did not get it previously while using the
>> > previous version of r). As mentioned by Ben, I applied the next to test
>> > if it is a false positive.
>> >
>> >> relgrad <- with(mixedModelACC4 @
> optinfo$derivs,solve(Hessian,gradient))
>> >> max(abs(relgrad))
>> > [1] 0.0001807051
>> >
>> >
>> > It is not clear to me if this value indicates that the convergence
>> > warning is a false positive, and therefore I can continue using glmer
>> > with this dataset. I would appreciate your feedback.
>> >
>> > Thank you in advance.
>> >
>> > /Miren/
>> >
>> >
>> > _______________________________________________
>> > Lme4-authors mailing list
>> > Lme4-authors at lists.r-forge.r-project.org
>> >
> https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/lme4-authors
>> >
>>


From juliet.hannah at gmail.com  Fri May 30 14:53:43 2014
From: juliet.hannah at gmail.com (Juliet Hannah)
Date: Fri, 30 May 2014 08:53:43 -0400
Subject: [R-sig-ME] reference for expression given on glmm wiki
Message-ID: <CALzuZRRd1g8kaCNW2UaLubHBS8ZwHoiojwjPC_OKcaieJfPTaw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140530/da7a012c/attachment.pl>

From bbolker at gmail.com  Sun Jun  1 02:27:38 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 1 Jun 2014 00:27:38 +0000 (UTC)
Subject: [R-sig-ME] reference for expression given on glmm wiki
References: <CALzuZRRd1g8kaCNW2UaLubHBS8ZwHoiojwjPC_OKcaieJfPTaw@mail.gmail.com>
Message-ID: <loom.20140601T022511-952@post.gmane.org>

Juliet Hannah <juliet.hannah at ...> writes:

> 
> All,
> 
> The glmm wiki
> 
> http://glmm.wikidot.com/faq
> 
> gives examples for computing confidence intervals and prediction intervals
> for normal mixed models. Can anyone give (ideally page numbers) references
> for a text describing these expressions.

  I'd love to know too!

  I haven't looked in the standard places (Pinheiro and Bates, 
Searle et al, Stroup, Demidenko, Galecki and Burzykowski ... ) but
don't remember seeing a reference -- these are just taken from
linear model theory or are common-sense extensions ...


From ligia_oceanica at hotmail.com  Mon Jun  2 03:44:13 2014
From: ligia_oceanica at hotmail.com (Ligia Pizzatto do Prado)
Date: Mon, 2 Jun 2014 01:44:13 +0000
Subject: [R-sig-ME] repeated-measures mixed models
Message-ID: <COL126-W3029C9C3EBDA46850F53D1F7200@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140602/9a7847e1/attachment.pl>

From ggrothendieck at gmail.com  Mon Jun  2 05:04:58 2014
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 1 Jun 2014 23:04:58 -0400
Subject: [R-sig-ME] lme4 performance
Message-ID: <CAP01uRnjfUZwAj-5ydGwsBuzFSOJ5TP=QehSOZLhNt0C8pZknw@mail.gmail.com>

Ben Bolker and I have been running some lmer timings on the lme4 package and
the older lme4.0 package and I am posting some timings here for comment.

This model was used:

   lmer(y ~ service * dept + (1|s) + (1|d), InstEval)

with lme4 from https://github.com/lme4/lme4/ and lme4.0 from
http://download.r-forge.r-project.org/src/contrib/lme4.0_0.999999-4.tar.gz

The newer lme4 is slower than the older lme4.0:

- lme4 takes 51 sec for the above command vs. 33 sec for lme4.0
- lme4 spends 37 sec of that in C/C++ optimizer whereas lme4.0 spends 26 sec
- lme4 spends 33 sec of that performing cholesky factorization whereas
  lme4.0 spends 14 sec
- lme4 involves 57 iterations of bobyqa wheresd lme4.0 involves 14 iterations
  of nlminb.  The iterations are the counts displayed if the actual argument
  verbose = TRUE is added to the lmer call.

We can reduce the 51 sec run time of lme4 to 45 sec (about 10% savings) if we
add this actual argument to the lmer call above:
   control = lmerControl(calc.derivs = FALSE)

The fixed and random coefficients from the two packages were very close.

system.time(lmer(...)) was used to get the lmer run time.
UNIX time was used to get the time used when launched from littler and
the percentages given by the google perftools profiler, also launched from
littler, were applied to the UNIX time to get the other timings.

R 3.0.3 running under Linux was used.


-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From Thierry.ONKELINX at inbo.be  Mon Jun  2 08:45:47 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 2 Jun 2014 06:45:47 +0000
Subject: [R-sig-ME] repeated-measures mixed models
In-Reply-To: <COL126-W3029C9C3EBDA46850F53D1F7200@phx.gbl>
References: <COL126-W3029C9C3EBDA46850F53D1F7200@phx.gbl>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A731AA@inbomail.inbo.be>

Dear Ligia,

You have a one on one relation between treatment and enclosure. Therefore it is not possible estimate the separate effect. So you must drop either (1|enclosure) or treat from your model. (1|enclosure) is the obvious choice.

Unless you have multiple measurements per animal at each time, (Time|ID) can lead to a perfect fit. Reduce that to (1|ID).

lmer(response ~ treat + Time + (1|ID), data = data1) is a more sensible model given your data.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Ligia Pizzatto do Prado
Verzonden: maandag 2 juni 2014 3:44
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] repeated-measures mixed models

I've been struggling with this analyses for a while now. Did quite a bit of reading but I'm getting more confused than before.
I wanna look at the the response of animals to a treatment (4 levels). Each group of 5 animals were subjected to one treatment level, and kept in the same enclosure. Response was measured for each subjected at time0 (previous to treat.), 5days, 10 and 50days post treatment. Covariate is log(BodyMass at time0). So subjects are repeated and nested within bin. After setting ID, enclosure, and treatment as factors I am using lme4 to fit the model:
m1<-lmer(response~treat + Time +(Time|ID)+(1|enclosure), data=data1) and get the Warning messages:1: In optwrap(optimizer, devfun, getStart(start, rho$lower, rho$pp),  :  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :  Model failed to converge with max|grad| = 1.55933 (tol = 0.002)3: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :  Model failed to converge: degenerate  Hessian with 2 negative eigenvalues What am I doing wrong?
Cheers
Ligia
        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From asafw.at.wharton at gmail.com  Tue Jun  3 04:23:28 2014
From: asafw.at.wharton at gmail.com (Asaf Weinstein)
Date: Mon, 2 Jun 2014 22:23:28 -0400
Subject: [R-sig-ME] Solving sparse linear systems
Message-ID: <CAGG0PdDp68fz=ffQaeao4B0mf=qwz0+kRmmrqCnkYw3eWzEsXQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140602/6d8d1b8d/attachment.pl>

From bates at stat.wisc.edu  Tue Jun  3 16:32:06 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 3 Jun 2014 09:32:06 -0500
Subject: [R-sig-ME] Solving sparse linear systems
In-Reply-To: <CAGG0PdDp68fz=ffQaeao4B0mf=qwz0+kRmmrqCnkYw3eWzEsXQ@mail.gmail.com>
References: <CAGG0PdDp68fz=ffQaeao4B0mf=qwz0+kRmmrqCnkYw3eWzEsXQ@mail.gmail.com>
Message-ID: <CAO7JsnS4Joz2qtdyQKC2xkss-YYZOCHDLBD-acrO5VjUEyFAjA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140603/e6cb90fd/attachment.pl>

From thomas.debray at gmail.com  Wed Jun  4 13:34:20 2014
From: thomas.debray at gmail.com (Thomas Debray)
Date: Wed, 4 Jun 2014 13:34:20 +0200
Subject: [R-sig-ME] glmer model with independent random effects
Message-ID: <CACKhmtMk-nGKnQwkz9yiKJ5APE=Aj7ex09jX=2MGzx1mA9iPog@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140604/3579d2d9/attachment.pl>

From steve.walker at utoronto.ca  Wed Jun  4 14:09:19 2014
From: steve.walker at utoronto.ca (Steve Walker)
Date: Wed, 04 Jun 2014 08:09:19 -0400
Subject: [R-sig-ME] glmer model with independent random effects
In-Reply-To: <CACKhmtMk-nGKnQwkz9yiKJ5APE=Aj7ex09jX=2MGzx1mA9iPog@mail.gmail.com>
References: <CACKhmtMk-nGKnQwkz9yiKJ5APE=Aj7ex09jX=2MGzx1mA9iPog@mail.gmail.com>
Message-ID: <538F0C6F.1030505@utoronto.ca>

Odd.  I get no problems on the current version of lme4 on CRAN.  Can you 
reproduce your problem with the example below?  If so, is it possible to 
update?

Steve


library(lme4)
set.seed(1)
m <- 50
n <- m^2
form <- y~ x1 + x2+(1|studyid) + ( x1 -1|studyid)+( x2 -1|studyid)

dat <- data.frame(studyid = gl(m, m),
                   x1 = rbinom(n, 1, 0.5), x2 = rbinom(n, 1, 0.5),
                   y = rep(0:1, length = n))

pars <- list(beta = setNames(1:3, c("(Intercept)", "x1", "x2")),
              theta = setNames(rep(1, 3), c("studyid.(Intercept)", 
"studyid.x1", "studyid.x2")))

dat$y <- unlist(simulate(form, newdata = dat, newparams = pars, family = 
binomial))

glmer(form, dat, binomial)

Generalized linear mixed model fit by maximum likelihood (Laplace
   Approximation) [glmerMod]
  Family: binomial ( logit )
Formula: y ~ x1 + x2 + (1 | studyid) + (x1 - 1 | studyid) + (x2 - 1 |
     studyid)
    Data: dat
       AIC       BIC    logLik  deviance  df.resid
1499.5705 1534.5148 -743.7852 1487.5705      2494
Random effects:
  Groups    Name        Std.Dev.
  studyid   (Intercept) 0.8685
  studyid.1 x1          0.7900
  studyid.2 x2          0.6559
Number of obs: 2500, groups: studyid, 50
Fixed Effects:
(Intercept)           x1           x2
      0.6693       1.8733       2.8996

sessionInfo()

R Under development (unstable) (2014-03-08 r65144)
Platform: x86_64-apple-darwin10.8.0 (64-bit)

locale:
[1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_1.1-6   Rcpp_0.11.1  Matrix_1.1-3 mysys_1.0

loaded via a namespace (and not attached):
[1] compiler_3.1.0      grid_3.1.0          lattice_0.20-27
[4] MASS_7.3-29         minqa_1.2.2         nlme_3.1-113
[7] RcppEigen_0.3.2.1.2 splines_3.1.0       tools_3.1.0


On 2014-06-04, 7:34 AM, Thomas Debray wrote:
> Dear userlist,
>
> I recently updated the lme4 package to version 1.0-6 and have the
> impression formulas are not interpreted exactly the same way as before.
>
> I am trying to fit a mixed effect model with independent random effects for
> the covariates, that is:
>
> logit(y_ij) = b_0i + b_1i x1_ij + b_2i x2_ij
> where
> b_0i ~ N(beta0, tau0)
> b_1i ~ N(beta1, tau1)
> b_2i ~N(beta2, tau2)
>
> i indicates the subject, j indicates an index of different study IDs.
>
> In want to specify random slopes for b1 and b2, and a random intercept for
> b0. The variables x0 and x1 are binary, and take values 0 or 1. I have
> constructed the following formula:
>
> "y~ x1 + x2+(1|studyid) + ( x1 -1|studyid)+( x2 -1|studyid)"
>
>
> In one of the earlier versions of lme4, I would have gotten something like this:
>
>
> Random effects:
>   Groups    Name        Std.Dev.
>   studyid   (Intercept) 2.217e-04
>   studyid.1 x1        2.518e-05
>   studyid.2 x2     8.891e-05
>
>
> where tau0, tau1 and tau2 are indicated by the Std.Dev. Now, in the latest
> version of lme4 I get:
>
>
> Random effects:
>   Groups    Name        Std.Dev.  Corr
>   studyid   (Intercept) 2.217e-04
>   studyid.1 x10        2.518e-05
>             x11        4.480e-04 0.08
>   studyid.2 x20     8.891e-05
>
>             x21     1.108e-04 0.79
>
>
>
> Can someone tell me whether I am doing something wrong, or how I
> should adjust my formula to estimate the specified model?
>
>
> Thanks in advance!
>
>
> Thomas
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


From thomas.debray at gmail.com  Wed Jun  4 15:00:46 2014
From: thomas.debray at gmail.com (Thomas Debray)
Date: Wed, 4 Jun 2014 15:00:46 +0200
Subject: [R-sig-ME] glmer model with independent random effects
In-Reply-To: <538F0C6F.1030505@utoronto.ca>
References: <CACKhmtMk-nGKnQwkz9yiKJ5APE=Aj7ex09jX=2MGzx1mA9iPog@mail.gmail.com>
	<538F0C6F.1030505@utoronto.ca>
Message-ID: <CACKhmtN4i6N9cbSy0qH1KzO7VqHqFNZzDQxTiFXsWjuyZBGN9A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140604/9b9342b7/attachment.pl>

From jackiewood7 at gmail.com  Wed Jun  4 18:06:41 2014
From: jackiewood7 at gmail.com (Jackie Wood)
Date: Wed, 4 Jun 2014 12:06:41 -0400
Subject: [R-sig-ME] Power analysis for a LMM with unbalanced data and
	continuous covariates
Message-ID: <CAOxxGRnXOaVtR7D-_zCZvf-3ibpbUsVROLZqqth-BZGet_yr8A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140604/f69b88f7/attachment.pl>

From jake987722 at hotmail.com  Wed Jun  4 18:52:51 2014
From: jake987722 at hotmail.com (Jake Westfall)
Date: Wed, 4 Jun 2014 10:52:51 -0600
Subject: [R-sig-ME] Power analysis for a LMM with unbalanced data and
 continuous covariates
In-Reply-To: <CAOxxGRnXOaVtR7D-_zCZvf-3ibpbUsVROLZqqth-BZGet_yr8A@mail.gmail.com>
References: <CAOxxGRnXOaVtR7D-_zCZvf-3ibpbUsVROLZqqth-BZGet_yr8A@mail.gmail.com>
Message-ID: <BAY172-W15611C8FCB39EB8E8D01C3CB220@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140604/587a5fd4/attachment.pl>

From smckinney at bccrc.ca  Wed Jun  4 19:18:01 2014
From: smckinney at bccrc.ca (Steven McKinney)
Date: Wed, 4 Jun 2014 10:18:01 -0700
Subject: [R-sig-ME] Power analysis for a LMM with unbalanced data and
 continuous covariates
In-Reply-To: <BAY172-W15611C8FCB39EB8E8D01C3CB220@phx.gbl>
References: <CAOxxGRnXOaVtR7D-_zCZvf-3ibpbUsVROLZqqth-BZGet_yr8A@mail.gmail.com>,
	<BAY172-W15611C8FCB39EB8E8D01C3CB220@phx.gbl>
Message-ID: <DCE81E14EB74504B971DAD4D2DB0356B0CD6D6C5C4@crcmail4.BCCRC.CA>

Well said Jake,

The only proper answer to a reviewer's question about the power associated with these findings is
"We did not perform an a-priori power calculation so the power is unknown".

In this case, if you fail to reject the null hypothesis, you can make no conclusion that will have
a known error rate.  Failure to reject does not mean you can conclude that there is no
difference in plasticity across the range of population sizes.  The answer remains unknown.
A confidence interval provides useful information, and power calculations for future studies
could be undertaken with these data, after some discussion as to what size of plasticity
differences is scientifically or biologically meaningful.

Post-hoc power calculations are inappropriate, and if the reviewer insists on them, you
can educate the reviewer about the inappropriate nature of post-hoc power calculations.

Russel Lenth has very good articles discussing this issue.

 

Steven McKinney, Ph.D.

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney +at+ bccrc +dot+ ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C.
V5Z 1L3
Canada

________________________________________
From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Jake Westfall [jake987722 at hotmail.com]
Sent: June 4, 2014 9:52 AM
To: Jackie Wood; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Power analysis for a LMM with unbalanced data and continuous covariates

Hi Jackie,

Can I suggest a reframing of the problem? The reviewer asked for a post hoc power analysis, but it seems to me that what they are really interested in is knowing something about the range of plausible values (in colloquial terms) for the effect size in question. Your non-significant result suggests that 0 is a plausible value for the effect size, but it may be the case that various "large" values would also be plausible / consistent with your results. In other words, it sounds like they really just want to see some confidence intervals for the effect in question, and they suspect that these confidence intervals will be wide. confint() provides various methods of computing confidence intervals for the parameter estimates of an lmer model. Maybe this will satisfy the reviewer?

Jake

> From: jackiewood7 at gmail.com
> Date: Wed, 4 Jun 2014 12:06:41 -0400
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Power analysis for a LMM with unbalanced data and continuous covariates
>
> Dear R users,
>
> I was wondering if anyone can offer some advice regarding how to conduct a
> power analysis (if it is possible) for a LMM with an unbalanced data set.
>
> Basically I'm conducting a common garden experiment investigating the
> strength of phenotypic plasticity (measured as the magnitude of the
> family-level slope between two temperatures) in relation to population size
> across a number of stream fish populations. We found no evidence that
> plasticity differs across the range of population sizes used in our
> experiment, however one of the reviewers wanted to know the power of our
> analysis to detect Type II error (i.e. failing to reject a false
> null-hypothesis). I realize that post-hoc power analyses are generally
> frowned upon, but here we are.
>
> For the experiment, I generated between 6 and 23 full-sib families for each
> of 8 differentially abundant fish populations. We were constrained in the
> number of families we could generate in certain populations since they were
> so small we could only ethically collect gametes from a small number of
> individuals which explains our unbalanced data.
>
> I have looked at the simulation example at http://rpubs.com/bbolker/11703
> and while what's done here does make sense to me, the example is for a
> balanced design (2 treatments, 30 individuals per treatment, 5 observations
> per individual). I do understand that the point of simulation (at least I
> think so) is to compare power across a range of different values of the
> experimental parameters of interest but I was wondering if it is possible
> to specify different numbers of families per population in a simulation or
> if I'm restricted to running simulations with only equal numbers of
> families per pop.
>
> Additionally, my model is currently set up as:
>
> glmm<- lmer(formula=fam.slope~N*egg.size+N*density+(1|stream)
>
> where N (population size), egg size, and density are continuous. In the
> example given by Ben Bolker he states that if you want to include
> continuous covariates you have to decide what the distribution is going to
> be but I was wondering how the code would differ from the discrete fixed
> effects used in the example? I've tried to look for this, but keep getting
> referred back to the original example.
>
> Any insight would be much appreciated!
>
> Cheers,
> Jackie
>
> --
> Jacquelyn L.A. Wood, PhD.
> Biology Department
> Concordia University
> 7141 Sherbrooke St. West
> Montreal, QC
> H4B 1R6
> Phone: (514) 293-7255
>
>       [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From dmcastil at umail.iu.edu  Wed Jun  4 20:33:26 2014
From: dmcastil at umail.iu.edu (Dean Castillo)
Date: Wed, 4 Jun 2014 14:33:26 -0400
Subject: [R-sig-ME] genetic distance as covariance matrix through ginverse?
Message-ID: <CAMmHrnxCKEOgujUE1jzAeMatPKqgonL_pZst+TDmQ24u88UBhw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140604/45531402/attachment.pl>

From r.turner at auckland.ac.nz  Wed Jun  4 23:09:09 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 05 Jun 2014 09:09:09 +1200
Subject: [R-sig-ME] Power analysis for a LMM with unbalanced data and
 continuous covariates
In-Reply-To: <DCE81E14EB74504B971DAD4D2DB0356B0CD6D6C5C4@crcmail4.BCCRC.CA>
References: <CAOxxGRnXOaVtR7D-_zCZvf-3ibpbUsVROLZqqth-BZGet_yr8A@mail.gmail.com>,
	<BAY172-W15611C8FCB39EB8E8D01C3CB220@phx.gbl>
	<DCE81E14EB74504B971DAD4D2DB0356B0CD6D6C5C4@crcmail4.BCCRC.CA>
Message-ID: <538F8AF5.7070200@auckland.ac.nz>

On 05/06/14 05:18, Steven McKinney wrote:

<SNIP>

> Post-hoc power calculations are inappropriate, and if the reviewer insists on them, you
> can educate the reviewer about the inappropriate nature of post-hoc power calculations.

<SNIP>

You can *try* to educate the reviewer!!! :-)

Good luck! :-)

cheers,

Rolf Turner

P. S. Your (Prof. McKinney's) post was very well expressed.

R. T.


From s06mw3 at abdn.ac.uk  Thu Jun  5 11:18:01 2014
From: s06mw3 at abdn.ac.uk (Matthew Wolak)
Date: Thu, 5 Jun 2014 10:18:01 +0100
Subject: [R-sig-ME] genetic distance as covariance matrix through
	ginverse?
In-Reply-To: <CAMmHrnxCKEOgujUE1jzAeMatPKqgonL_pZst+TDmQ24u88UBhw@mail.gmail.com>
References: <CAMmHrnxCKEOgujUE1jzAeMatPKqgonL_pZst+TDmQ24u88UBhw@mail.gmail.com>
Message-ID: <539035C9.5030502@abdn.ac.uk>

Dear Dean,

MCMCglmm needs the generalized inverse (your M1) formatted in a specific
way - for example as the "dgCMatrix" class defined in the Matrix package.

Substituting the lines below gets the model to work.  NOTE, I had to
substitute "0.00" wherever you had NA in the M matrix in order to invert
it.  This is probably not the best practice and maybe some simulations
to determine the sensitivity of your parameter estimates when there is
error (i.e., saying NA=0.00) in your genetic distance matrix.

INSTEAD OF:

M1<-ginv(M) #I have also been using M in the model with the same error

dimnames(M1) <- list(c("sp1","sp2","sp3","sp4"),c("sp1","sp2","sp3","sp4"))


DO

M1 <- as(ginv(M), "dgCMatrix")

M1 at Dimnames <- list(c("sp1", "sp2", "sp3", "sp4"), c("sp1", "sp2", "sp3", "sp4"))



Sincerely,
Matthew

....................................................
Dr. Matthew E. Wolak
School of Biological Sciences
Zoology Building
University of Aberdeen
Tillydrone Avenue
Aberdeen AB24 2TZ
office phone: +44 (0)1224 273255

On 04/06/14 19:33, Dean Castillo wrote:
> I am currently using MCMCglmm to model species ability to cross with one
> another. For most of the datasets species relationships can be defined by a
> phylogeny and I can simply incorporate this information into the model
> using the pedigree or ginverse options.
>
> I would like to run similar models now but for subspecies/races that do not
> have well defined phylogenies (the questions is identical). I have genetic
> distances for some of the species in these data sets (some I have complete
> pairwise genetic distances, others I only have distances for specific
> species pairs). I have considered making a  simple NJ tree based on
> distance matrices and incorporating this "phylogeny" via pedigree or
> ginverse, but this strategy will not work without complete pairwise
> distances.
>
> Is there a way to utilize the distance matrix (complete or more sparse)
> directly? Since the genetic distance matrix looks very similar to the
> relationship matrix "A" in the MCMCglmm tutorial and coursenotes I tried
> incorporating it into the model but have run into a few errors.
>
> Here is the fake data I have been playing around with. Any help would be
> greatly appreciated
>
> #Genetic Distance Matrix. I have also been replacing NA with 0 but still
> encounter same error
>
> x <- c( 0.07 ,0.08, 0.11,NA,
>
>   0.16, NA)
>
> M <- matrix(0, 4, 4)
>
> M[lower.tri(M)] <- x
>
> M <- t(M)
>
> M[lower.tri(M)] <- x
>
> dimnames(M) <- list(c("sp1","sp2","sp3","sp4"),c("sp1","sp2","sp3","sp4"))
>
> diag(M)<-1
>
> #Fake response variable, species ID, and categorical predictor
>
>
> y<-c(5,5,6,6,10,10,12,11,4,3,4,5,11,12,13,14)
>
> species<-c("sp1","sp1","sp1","sp1","sp2","sp2","sp2","sp2","sp3","sp3","sp3"
> ,"sp3","sp4","sp4","sp4","sp4")
>
> species<-as.factor(species)
>
> ms<-c(0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1)
>
> data<-data.frame(y,ms,species)
>
>
> # want to run a model for y~ma taking into account species
>
> library(MCMCglmm)
>
> #define prior
>
> p.var<-var(y)
>
> prior1=list(R=list(V=p.var/2, n=1), G=list(G1=list(V=p.var/2, n=1)))
>
> library(MASS)
>
> M1<-ginv(M) #I have also been using M in the model with the same error
>
> dimnames(M1) <- list(c("sp1","sp2","sp3","sp4"),c("sp1","sp2","sp3","sp4"))
>
> model1<-MCMCglmm(y~ms,random=~species,ginverse=list(species=M1),prior=prior1
> ,data=data)
>
> #The error I receive.
>
> Error in FUN(X[[1L]], ...) :
>
>    trying to get slot "Dim" from an object of a basic class ("matrix") with
> no slots
>
>       [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



The University of Aberdeen is a charity registered in Scotland, No SC013683.
Tha Oilthigh Obar Dheathain na charthannas cl?raichte ann an Alba, ?ir. SC013683.


From dmcastil at umail.iu.edu  Thu Jun  5 15:18:16 2014
From: dmcastil at umail.iu.edu (Dean Castillo)
Date: Thu, 5 Jun 2014 09:18:16 -0400
Subject: [R-sig-ME] genetic distance as covariance matrix through
	ginverse?
In-Reply-To: <539035C9.5030502@abdn.ac.uk>
References: <CAMmHrnxCKEOgujUE1jzAeMatPKqgonL_pZst+TDmQ24u88UBhw@mail.gmail.com>
	<539035C9.5030502@abdn.ac.uk>
Message-ID: <CAMmHrnzY91j12G_J_-Zs0ckk-_dNV2h6BRNNuJABT1_L5eCMjQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140605/a664903d/attachment.pl>

From raptorbio at hotmail.com  Thu Jun  5 19:58:36 2014
From: raptorbio at hotmail.com (Adam Smith)
Date: Thu, 5 Jun 2014 13:58:36 -0400
Subject: [R-sig-ME] Assistance with specification of crossover design model
Message-ID: <SNT151-W4957F2A47E8BDD44107821A12D0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140605/7f753f04/attachment.pl>

From segerfan83 at gmail.com  Fri Jun  6 16:31:25 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Fri, 6 Jun 2014 10:31:25 -0400
Subject: [R-sig-ME] rpt.remlLMM(y, groups) causes R to crash
Message-ID: <CAHe08Sh4LmeCc5aOqV_tVixKdBk7ci0p_S93m1brgw--bB=B-A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140606/aa6229eb/attachment.pl>

From bbolker at gmail.com  Fri Jun  6 16:54:35 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 06 Jun 2014 10:54:35 -0400
Subject: [R-sig-ME] rpt.remlLMM(y, groups) causes R to crash
In-Reply-To: <CAHe08Sh4LmeCc5aOqV_tVixKdBk7ci0p_S93m1brgw--bB=B-A@mail.gmail.com>
References: <CAHe08Sh4LmeCc5aOqV_tVixKdBk7ci0p_S93m1brgw--bB=B-A@mail.gmail.com>
Message-ID: <5391D62B.2060701@gmail.com>

On 14-06-06 10:31 AM, AvianResearchDivision wrote:
> Hi all,
> 
> After running mixed models using 'lme4', I have moved on to calculating
> repeatabilities using the package 'rptR' on my data set.  I have 879
> observations over 59 individuals.  I am using the calll
> rpt.remlLMM(y,groups) to obtain repeatabilities, but after about 15 seconds
> I get a error stating:
> 
>  R for Windows GUI front-end has stopped working
> 
> A problem caused the program to stop working correctly.  Windows will close
> the program and notify you if a solution is available.
> 
> 
> I am running Windows 7 with a i3 processor and 4 gb of memory so I wouldn't
> expect this error to be computer performance related.
> 
> I should note that I can run the rpt.aov(y,groups) call just fine.  When
> running the following mixed model, I don't have any convergence issues:
> 
> lmer(Response~Predictor+(Predictor|Individual))
> 
> 
> Has anyone come across this issue or have any suggestions?
> 
> 
> Best,
> Jacob
> 

  (It would help to specify that rptR is available from r-forge: it took
me a few extra minutes to dig around and find it.)

  For what it's worth, rpt.remlLMM appears to use nlme::lme (not
lme4::lmer) internally.   There doesn't seem to be anything particularly
scary in the guts of the function (e.g. no calls to compiled code), so I
really haven't much of a clue.  A reproducible example would probably be
helpful.  (Probably worth checking with the package maintainer as well:
 maintainer("rptR")).

  Can you run the examples in ?rpt.remlLMM successfully?  What if you
take those examples and bump up the number of permutation/bootstrap
replicates?

  Ben Bolker


From segerfan83 at gmail.com  Fri Jun  6 18:28:41 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Fri, 6 Jun 2014 12:28:41 -0400
Subject: [R-sig-ME] rpt.remlLMM(y, groups) causes R to crash
In-Reply-To: <5391D62B.2060701@gmail.com>
References: <CAHe08Sh4LmeCc5aOqV_tVixKdBk7ci0p_S93m1brgw--bB=B-A@mail.gmail.com>
	<5391D62B.2060701@gmail.com>
Message-ID: <CAHe08SgDu1gyoQQ_pfs9K3GWEipeL4QvXRiMNdd-+LNaOzB7QQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140606/097a8c72/attachment.pl>

From raptorbio at hotmail.com  Fri Jun  6 19:50:59 2014
From: raptorbio at hotmail.com (Adam Smith)
Date: Fri, 6 Jun 2014 13:50:59 -0400
Subject: [R-sig-ME] Reformat: Assistance with specification of crossover
	design model
Message-ID: <SNT151-W34316E3FE3ABF9E55EEDBFA12C0@phx.gbl>

Apologies for the formatting in the initial post.

Hello list,

I'm analyzing data (variable names in brackets) generated by a 4-period [period], 3-treatment [trt] crossover design.
The design was strongly balanced (all treatments preceded all others, including itself) and uniform within period (all treatments occurred the same # of times in each of the 4 periods).

This produced 18 distinct sequences of treatments (e.g., ABCC, CAAB, etc.), to which a single individual [id] was randomly assigned. Two covariates [cov1, cov2] were also measured on each individual.

The response [y] was continuous, and each individual was associated with a pre-study baseline measure [y0].

Because a single individual was assigned to each sequence, I believe that a random effect for each individual will capture individual, baseline, and sequence effects (they're perfectly confounded).

The question is simple: does the response differ among treatments?

I'm hoping the correct specification is as simple as I think it is, illustrated below with mock data.
My specific questions:

1 - Does this model correctly capture treatment, period, and potential carryover effects? As I understand it, in a strongly balanced design such as this, carryover and treatment effects are not confounded, so my assumption is that I don't have to specify any addition variable to capture this effect (e.g., a variable indicating the treatment in the preceding period). I'm happy to be corrected.

2 - Is an interaction between treatment and period prescribed? Because the design is uniform within periods, I believe that period and treatment effects are likewise not confounded.

3 - Does the random effect for each individual captures variation due to individual, sequence, and baseline measurement?

Thanks very much for your assistance,

Adam Smith
Department of Natural Resources Science
University of Rhode Island

# Download data
datURL <- "https://dl.dropboxusercontent.com/u/23278690/xover_test.csv"
dat <- repmis::source_data(datURL, sep = ",", header = TRUE)
dat <- within(dat, {
? ? ?id = factor(id)
? ? ?trt = factor(trt)
? ? ?period = factor(period)
? ? ?cov1 = factor(cov1)
? ? ?cov2 = factor(cov2)
})

require(lme4)
# Proposed linear mixed model analysis
mod1 <- lmer(y ~ trt + period + cov1 + cov2 + (1|id), data = dat)

# Test global treatment effect, for example...
mod2 <- update(mod1, . ~ . - trt)
anova(mod1, mod2)

sessionInfo()

R version 3.0.3 (2014-03-06)

Platform: x86_64-w64-mingw32/x64 (64-bit)

attached base packages:[1] stats graphics grDevices utils datasets methods base 

other attached packages:[1] car_2.0-19 AICcmodavg_1.35 lme4_1.1-5 Rcpp_0.11.1 Matrix_1.1-3 plyr_1.8.1 

 		 	   		  

From bbolker at gmail.com  Fri Jun  6 20:00:35 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 06 Jun 2014 14:00:35 -0400
Subject: [R-sig-ME] rpt.remlLMM(y, groups) causes R to crash
In-Reply-To: <CAHe08SgDu1gyoQQ_pfs9K3GWEipeL4QvXRiMNdd-+LNaOzB7QQ@mail.gmail.com>
References: <CAHe08Sh4LmeCc5aOqV_tVixKdBk7ci0p_S93m1brgw--bB=B-A@mail.gmail.com>	<5391D62B.2060701@gmail.com>
	<CAHe08SgDu1gyoQQ_pfs9K3GWEipeL4QvXRiMNdd-+LNaOzB7QQ@mail.gmail.com>
Message-ID: <539201C3.6040902@gmail.com>

On 14-06-06 12:28 PM, AvianResearchDivision wrote:
> Hi Ben,
> 
> Thanks for the response.  I'm sorry I didn't give you the heads up about
> r-forge.  I messed around with 'nboot' and 'npermut' by decreasing from
> their defaults of 1000 to 10 and that allowed me to run it just fine.  In
> general, what is the harm in straying away from these default parameters?
> 
> Jacob

     I don't think you've actually solved your problem this way, but you
have demonstrated that it's something having to do with a
computationally intensive workload, and not something intrinsic about
the code. That is, there's not something about running a single
bootstrap or permutation that will make your computer crash.  (The other
thing to try is using small values of nboot/npermut, but re-running the
command many times to see if you can trigger a crash.) On the other
hand, computer-crashing bugs are usually *not* deterministic in this way
-- they often depend on some haphazard or not-easily-repeatable sequence
of interactions with the operating system ...)

   My more basic question is whether you can make R crash by using the
examples with large values of nboot/npermut (in which case this is a
general issue) or not (in which case it seems like an interaction
between some quirk of your data and the software).  I haven't looked
into what npermut/nboot are doing, but they're presuming computing some
sort of simulation-based p-values/confidence intervals; if you only run
a small number of replicates, then your estimates will be very coarse.
I'm guessing that the small values of nboot/npermut in the examples are
there so that people aren't accidentally running long/slow jobs when
they try out the examples, not that these values are really recommended
for production use. It *might* be possible to get the same answers by
running a large number of commands that each run a small number of
permutation/bootstrap samples and then assembling them, but that's
likely to be tricky.

  Do you have the same kinds of problems if you run from a batch file
rather than from the Windows GUI?

  I *was* going to say that we do know of a few memory-access issues
with lme4, but now that I remember that rpt.remlLMM uses lme and not
lmer, I can't see why that would matter ...

  cheers
    Ben Bolker

> 
> 
> On Fri, Jun 6, 2014 at 10:54 AM, Ben Bolker <bbolker at gmail.com> wrote:
> 
>> On 14-06-06 10:31 AM, AvianResearchDivision wrote:
>>> Hi all,
>>>
>>> After running mixed models using 'lme4', I have moved on to calculating
>>> repeatabilities using the package 'rptR' on my data set.  I have 879
>>> observations over 59 individuals.  I am using the calll
>>> rpt.remlLMM(y,groups) to obtain repeatabilities, but after about 15
>> seconds
>>> I get a error stating:
>>>
>>>  R for Windows GUI front-end has stopped working
>>>
>>> A problem caused the program to stop working correctly.  Windows will
>> close
>>> the program and notify you if a solution is available.
>>>
>>>
>>> I am running Windows 7 with a i3 processor and 4 gb of memory so I
>> wouldn't
>>> expect this error to be computer performance related.
>>>
>>> I should note that I can run the rpt.aov(y,groups) call just fine.  When
>>> running the following mixed model, I don't have any convergence issues:
>>>
>>> lmer(Response~Predictor+(Predictor|Individual))
>>>
>>>
>>> Has anyone come across this issue or have any suggestions?
>>>
>>>
>>> Best,
>>> Jacob
>>>
>>
>>   (It would help to specify that rptR is available from r-forge: it took
>> me a few extra minutes to dig around and find it.)
>>
>>   For what it's worth, rpt.remlLMM appears to use nlme::lme (not
>> lme4::lmer) internally.   There doesn't seem to be anything particularly
>> scary in the guts of the function (e.g. no calls to compiled code), so I
>> really haven't much of a clue.  A reproducible example would probably be
>> helpful.  (Probably worth checking with the package maintainer as well:
>>  maintainer("rptR")).
>>
>>   Can you run the examples in ?rpt.remlLMM successfully?  What if you
>> take those examples and bump up the number of permutation/bootstrap
>> replicates?
>>
>>   Ben Bolker
>>
>>
>>
>>
>>
>


From bbolker at gmail.com  Sat Jun  7 02:32:49 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 7 Jun 2014 00:32:49 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?Reformat=3A_Assistance_with_specification_of?=
	=?utf-8?q?_crossover=09design_model?=
References: <SNT151-W34316E3FE3ABF9E55EEDBFA12C0@phx.gbl>
Message-ID: <loom.20140607T022519-341@post.gmane.org>

Adam Smith <raptorbio at ...> writes:

> 
> Apologies for the formatting in the initial post.
> 
> Hello list,
 
> I'm analyzing data (variable names in brackets) generated by a
> 4-period [period], 3-treatment [trt] crossover design.  The design
> was strongly balanced (all treatments preceded all others, including
> itself) and uniform within period (all treatments occurred the same
> # of times in each of the 4 periods).

> This produced 18 distinct sequences of treatments (e.g., ABCC, CAAB,
> etc.), to which a single individual [id] was randomly assigned. Two
> covariates [cov1, cov2] were also measured on each individual.
 
> The response [y] was continuous, and each individual was associated
  with a pre-study baseline measure [y0].
 
> Because a single individual was assigned to each sequence, I believe
> that a random effect for each individual will capture individual,
> baseline, and sequence effects (they're perfectly confounded).
 
> The question is simple: does the response differ among treatments?
> I'm hoping the correct specification is as simple as I think it is,
> illustrated below with mock data.  My specific questions:
 

  Thanks for a very clear question.  I'm going to take a stab at
these, but haven't thought *very* deeply about them; hopefully
this will inspire corrections/alternative answers.

> 1 - Does this model correctly capture treatment, period, and
> potential carryover effects? As I understand it, in a strongly
> balanced design such as this, carryover and treatment effects are
> not confounded, so my assumption is that I don't have to specify any
> addition variable to capture this effect (e.g., a variable
> indicating the treatment in the preceding period). I'm happy to be
> corrected.

  I think you're right that the individual-level random effect
controls for pre-treatment and sequence/carryover effects.  However,
I'm not sure it will capture period or treatment-by-period effects
(i.e. residuals within the same period might be correlated, and
residuals from individuals who received the treatment in the same
period might be correlated).

  One way to see whether you might have missed something would
be to draw (e.g.) boxplots of residuals by whatever groupings
you might be concerned about.

 
> 2 - Is an interaction between treatment and period prescribed?
> Because the design is uniform within periods, I believe that period
> and treatment effects are likewise not confounded.

  I think so.
 
> 3 - Does the random effect for each individual captures variation
> due to individual, sequence, and baseline measurement?

  I think so.
 
> Thanks very much for your assistance,
> 
> Adam Smith
> Department of Natural Resources Science
> University of Rhode Island
> 
> # Download data
> datURL <- "https://dl.dropboxusercontent.com/u/23278690/xover_test.csv"
> dat <- repmis::source_data(datURL, sep = ",", header = TRUE)
> dat <- within(dat, {
> ? ? ?id = factor(id)
> ? ? ?trt = factor(trt)
> ? ? ?period = factor(period)
> ? ? ?cov1 = factor(cov1)
> ? ? ?cov2 = factor(cov2)
> })
> 
> require(lme4)
> # Proposed linear mixed model analysis
> mod1 <- lmer(y ~ trt + period + cov1 + cov2 + (1|id), data = dat)

  I think you do want trt*period (possibly as a random effect?)
  In a setting where I had access to some regularization (e.g.
blme) I would be tempted to treat period as random -- but I wouldn't
do it in this vanilla model, because there aren't enough periods to
estimate it reliably.

  While you don't _need_ to incorporate pre-treatment covariate,
carryover effects, etc., it might be interesting -- I think these
would be partitioned out of the among-individual variation ...

> # Test global treatment effect, for example...
> mod2 <- update(mod1, . ~ . - trt)
> anova(mod1, mod2)
> 
> sessionInfo()
> 
> R version 3.0.3 (2014-03-06)
> 
> Platform: x86_64-w64-mingw32/x64 (64-bit)
 
> attached base packages:[1] stats graphics grDevices utils datasets
  methods base
 
> other attached packages:[1] car_2.0-19 AICcmodavg_1.35 lme4_1.1-5
> Rcpp_0.11.1 Matrix_1.1-3 plyr_1.8.1


From sunilmundra at hotmail.com  Sat Jun  7 22:29:07 2014
From: sunilmundra at hotmail.com (Sunil Mundra)
Date: Sun, 8 Jun 2014 01:59:07 +0530
Subject: [R-sig-ME] Error in LME4
In-Reply-To: <loom.20140607T022519-341@post.gmane.org>
References: <SNT151-W34316E3FE3ABF9E55EEDBFA12C0@phx.gbl>,
	<loom.20140607T022519-341@post.gmane.org>
Message-ID: <BLU182-W68E66A8846D9D6DE6BA420A32F0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140608/85b7a5ee/attachment.pl>

From bbolker at gmail.com  Sun Jun  8 01:54:23 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 07 Jun 2014 19:54:23 -0400
Subject: [R-sig-ME] Error in LME4
In-Reply-To: <BLU182-W68E66A8846D9D6DE6BA420A32F0@phx.gbl>
References: <SNT151-W34316E3FE3ABF9E55EEDBFA12C0@phx.gbl>,
	<loom.20140607T022519-341@post.gmane.org>
	<BLU182-W68E66A8846D9D6DE6BA420A32F0@phx.gbl>
Message-ID: <5393A62F.6070904@gmail.com>

On 14-06-07 04:29 PM, Sunil Mundra wrote:
> Hello all,
> I am new to use LME4 for analysing mixed effect model.
> I am trying to test impact of Deep-snow treatment, sampling round, and
> sampling depth on fungal richness.
> In my experiment:
> I have two block (C and D), each Block has 3 fence group.
> I have two treatment one is where snow is deep (causing soil warming)
> and other in control sample, at each fence.
> Samples are collected 9 times in summer from two different depth (2cm
> and 5cm).
> I hope i am able to explain my experiment well...
> 
> My first question the the model I am using (see below) here, is OK for
> this question?
>  Z1 <- lmer(Richness ~ Treatment*Round*Depth +
> (1|Block/Fence/Treatment/Round), REML= FALSE, data = data)
> 
> Second when i am running this this argument i am getting following error
> as mention below.
> Error in lme4::lFormula(formula = Richness ~ Treatment * Round * Depth +  : 
>   rank of X = 28 < ncol(X) = 36
> I tried to google it, but did not manage to find the solution.
> I also get similar error (col pivoting) when i was trying CCA analysis
> for community study with similar predictor and random variables.
> 
> Help in this regards would be highly appreciated....!!!
> Regards
> Sunil

  This means that you don't have all 36 combinations of Treatment,
Round, and Depth in your experimental design (based on the arithmetic it
seems that "Round" is your 9 samples).  (You probably only have 28
combinations -- were some treatment/depth combinations missed in some
rounds?)  If you use a recent version of lme4 you should be able to ask
lme4 to drop the aliased/unidentifiable columns -- in fact this should
happen automatically.

  However, you have more problems than this (sorry)

 * you're probably not going to be able to fit block as a random effect
(because there are only two levels)
 * you shouldn't have the same factor included as both a random and a
fixed effect

 I would suggest (I think)

Richness ~ Treatment*Depth + (1|(Block:Fence)/Round)

 possibly

 Richness ~ Treatment*Depth + (Treatment*Depth|(Block:Fence)) +
(1|Block:Fence:Round)

 to allow for variation in treatment/depth effects across blocks and
block/fence combinations


From hans.ekbrand at gmail.com  Sun Jun  8 22:08:58 2014
From: hans.ekbrand at gmail.com (Hans Ekbrand)
Date: Sun, 8 Jun 2014 22:08:58 +0200
Subject: [R-sig-ME] Error in LME4
In-Reply-To: <BLU182-W68E66A8846D9D6DE6BA420A32F0@phx.gbl>
References: <SNT151-W34316E3FE3ABF9E55EEDBFA12C0@phx.gbl>
	<loom.20140607T022519-341@post.gmane.org>
	<BLU182-W68E66A8846D9D6DE6BA420A32F0@phx.gbl>
Message-ID: <20140608200858.GA28396@pc245.socio.gu.se>

On Sun, Jun 08, 2014 at 01:59:07AM +0530, Sunil Mundra wrote:
> Hello all,I am new to use LME4 for analysing mixed effect model.I am trying to test impact of Deep-snow treatment, sampling round, and sampling depth on fungal richness.In my experiment:I have two block (C and D), each Block has 3 fence group.I have two treatment one is where snow is deep (causing soil warming) and other in control sample, at each fence.Samples are collected 9 times in summer from two different depth (2cm and 5cm).I hope i am able to explain my experiment well...
> My first question the the model I am using (see below) here, is OK for this question? Z1 <- lmer(Richness ~ Treatment*Round*Depth + (1|Block/Fence/Treatment/Round), REML= FALSE, data = data)

You don't need to specify wether your data is nested or crossed, lme4
will figure out that anyway. If you want random intercepts try:

Z1 <- lmer(Richness ~ Treatment*Round*Depth + (1|Block) + (1|Fence) + (1|Treatment) + (1|Round), REML= FALSE, data = data)

If you want random slopes, I think you need to explain what you want
better.


From hans.ekbrand at gmail.com  Sun Jun  8 22:14:43 2014
From: hans.ekbrand at gmail.com (Hans Ekbrand)
Date: Sun, 8 Jun 2014 22:14:43 +0200
Subject: [R-sig-ME] Error in LME4
In-Reply-To: <20140608200858.GA28396@pc245.socio.gu.se>
References: <SNT151-W34316E3FE3ABF9E55EEDBFA12C0@phx.gbl>
	<loom.20140607T022519-341@post.gmane.org>
	<BLU182-W68E66A8846D9D6DE6BA420A32F0@phx.gbl>
	<20140608200858.GA28396@pc245.socio.gu.se>
Message-ID: <20140608201442.GB28396@pc245.socio.gu.se>

On Sun, Jun 08, 2014 at 10:08:58PM +0200, Hans Ekbrand wrote:
> On Sun, Jun 08, 2014 at 01:59:07AM +0530, Sunil Mundra wrote:
> > Hello all,I am new to use LME4 for analysing mixed effect model.I am trying to test impact of Deep-snow treatment, sampling round, and sampling depth on fungal richness.In my experiment:I have two block (C and D), each Block has 3 fence group.I have two treatment one is where snow is deep (causing soil warming) and other in control sample, at each fence.Samples are collected 9 times in summer from two different depth (2cm and 5cm).I hope i am able to explain my experiment well...
> > My first question the the model I am using (see below) here, is OK for this question? Z1 <- lmer(Richness ~ Treatment*Round*Depth + (1|Block/Fence/Treatment/Round), REML= FALSE, data = data)

[...]

> If you want random slopes, I think you need to explain what you want
> better.

When reading Ben Bolkers suggestion, I realised that my suggestion
was not useful, please forget about it.


From segerfan83 at gmail.com  Sun Jun  8 22:30:17 2014
From: segerfan83 at gmail.com (AvianResearchDivision)
Date: Sun, 8 Jun 2014 16:30:17 -0400
Subject: [R-sig-ME] rpt.remlLMM(y, groups) causes R to crash
In-Reply-To: <539201C3.6040902@gmail.com>
References: <CAHe08Sh4LmeCc5aOqV_tVixKdBk7ci0p_S93m1brgw--bB=B-A@mail.gmail.com>
	<5391D62B.2060701@gmail.com>
	<CAHe08SgDu1gyoQQ_pfs9K3GWEipeL4QvXRiMNdd-+LNaOzB7QQ@mail.gmail.com>
	<539201C3.6040902@gmail.com>
Message-ID: <CAHe08Sj-uODp8D5_U82mR4ytC1Avy9zk6nA1_YYT2hp4zK9G-Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140608/72fe114b/attachment.pl>

From bbolker at gmail.com  Sun Jun  8 23:05:02 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 08 Jun 2014 17:05:02 -0400
Subject: [R-sig-ME] Error in LME4
In-Reply-To: <20140608200858.GA28396@pc245.socio.gu.se>
References: <SNT151-W34316E3FE3ABF9E55EEDBFA12C0@phx.gbl>	<loom.20140607T022519-341@post.gmane.org>	<BLU182-W68E66A8846D9D6DE6BA420A32F0@phx.gbl>
	<20140608200858.GA28396@pc245.socio.gu.se>
Message-ID: <5394CFFE.5000902@gmail.com>

On 14-06-08 04:08 PM, Hans Ekbrand wrote:
> On Sun, Jun 08, 2014 at 01:59:07AM +0530, Sunil Mundra wrote:
>> Hello all,I am new to use LME4 for analysing mixed effect model.I
>> am trying to test impact of Deep-snow treatment, sampling round,
>> and sampling depth on fungal richness.In my experiment:I have two
>> block (C and D), each Block has 3 fence group.I have two treatment
>> one is where snow is deep (causing soil warming) and other in
>> control sample, at each fence.Samples are collected 9 times in
>> summer from two different depth (2cm and 5cm).I hope i am able to
>> explain my experiment well... My first question the the model I am
>> using (see below) here, is OK for this question? Z1 <-
>> lmer(Richness ~ Treatment*Round*Depth +
>> (1|Block/Fence/Treatment/Round), REML= FALSE, data = data)
> 
> You don't need to specify wether your data is nested or crossed,
> lme4 will figure out that anyway. If you want random intercepts try:
> 
> Z1 <- lmer(Richness ~ Treatment*Round*Depth + (1|Block) + (1|Fence) +
> (1|Treatment) + (1|Round), REML= FALSE, data = data)
> 

  Minor correction here: lme4 can *only* figure out whether the grouping
variables are nested if they are uniquely labeled (e.g. fences within
block are labeled b1f1, b1f2, b1f3, b2f1, b2f2, b2f3, ... rather than
f1, f2, f3, f1, f2, f3 ...

> If you want random slopes, I think you need to explain what you want 
> better.
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Sun Jun  8 23:08:07 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 08 Jun 2014 17:08:07 -0400
Subject: [R-sig-ME] rpt.remlLMM(y, groups) causes R to crash
In-Reply-To: <CAHe08Sj-uODp8D5_U82mR4ytC1Avy9zk6nA1_YYT2hp4zK9G-Q@mail.gmail.com>
References: <CAHe08Sh4LmeCc5aOqV_tVixKdBk7ci0p_S93m1brgw--bB=B-A@mail.gmail.com>	<5391D62B.2060701@gmail.com>	<CAHe08SgDu1gyoQQ_pfs9K3GWEipeL4QvXRiMNdd-+LNaOzB7QQ@mail.gmail.com>	<539201C3.6040902@gmail.com>
	<CAHe08Sj-uODp8D5_U82mR4ytC1Avy9zk6nA1_YYT2hp4zK9G-Q@mail.gmail.com>
Message-ID: <5394D0B7.2060400@gmail.com>

On 14-06-08 04:30 PM, AvianResearchDivision wrote:
> Hi Ben,
> 
> Thank you again for the response and I apologize for my delay getting
> back to you.  I tried running the example data from 'rptR' and got my
> computer to crash by increasing npermut and nboot a bit, so it doesn't
> seem it's necessarily an issue with my data, other than the fact that my
> data might make it happen quicker.  I ran my data using rpt.anova and
> mcmc instead of remlLMM and those run fine, so who knows what is going on.
> 
> I suppose I don't know what you mean by "Do you have the same kinds of
> problems if you run from a batch file rather than from the Windows
> GUI?"  I'm not overly competent in R, so I apologize for my lack of
> understanding.
> 
> 
> Jacob

  You should be able to run a batch file by saving all of your commands
to a self-contained .R file; opening a terminal/command window and
making sure that Rscript.exe is in your path for executable files; and
then running (something like) Rscript.exe -e filename.R

http://stackoverflow.com/questions/3412911/r-exe-rcmd-exe-rscript-exe-and-rterm-exe-whats-the-difference
> 
> 
> 
> On Fri, Jun 6, 2014 at 2:00 PM, Ben Bolker <bbolker at gmail.com
> <mailto:bbolker at gmail.com>> wrote:
> 
>     On 14-06-06 12:28 PM, AvianResearchDivision wrote:
>     > Hi Ben,
>     >
>     > Thanks for the response.  I'm sorry I didn't give you the heads up
>     about
>     > r-forge.  I messed around with 'nboot' and 'npermut' by decreasing
>     from
>     > their defaults of 1000 to 10 and that allowed me to run it just
>     fine.  In
>     > general, what is the harm in straying away from these default
>     parameters?
>     >
>     > Jacob
> 
>          I don't think you've actually solved your problem this way, but you
>     have demonstrated that it's something having to do with a
>     computationally intensive workload, and not something intrinsic about
>     the code. That is, there's not something about running a single
>     bootstrap or permutation that will make your computer crash.  (The other
>     thing to try is using small values of nboot/npermut, but re-running the
>     command many times to see if you can trigger a crash.) On the other
>     hand, computer-crashing bugs are usually *not* deterministic in this way
>     -- they often depend on some haphazard or not-easily-repeatable sequence
>     of interactions with the operating system ...)
> 
>        My more basic question is whether you can make R crash by using the
>     examples with large values of nboot/npermut (in which case this is a
>     general issue) or not (in which case it seems like an interaction
>     between some quirk of your data and the software).  I haven't looked
>     into what npermut/nboot are doing, but they're presuming computing some
>     sort of simulation-based p-values/confidence intervals; if you only run
>     a small number of replicates, then your estimates will be very coarse.
>     I'm guessing that the small values of nboot/npermut in the examples are
>     there so that people aren't accidentally running long/slow jobs when
>     they try out the examples, not that these values are really recommended
>     for production use. It *might* be possible to get the same answers by
>     running a large number of commands that each run a small number of
>     permutation/bootstrap samples and then assembling them, but that's
>     likely to be tricky.
> 
>       Do you have the same kinds of problems if you run from a batch file
>     rather than from the Windows GUI?
> 
>       I *was* going to say that we do know of a few memory-access issues
>     with lme4, but now that I remember that rpt.remlLMM uses lme and not
>     lmer, I can't see why that would matter ...
> 
>       cheers
>         Ben Bolker
> 
>     >
>     >
>     > On Fri, Jun 6, 2014 at 10:54 AM, Ben Bolker <bbolker at gmail.com
>     <mailto:bbolker at gmail.com>> wrote:
>     >
>     >> On 14-06-06 10:31 AM, AvianResearchDivision wrote:
>     >>> Hi all,
>     >>>
>     >>> After running mixed models using 'lme4', I have moved on to
>     calculating
>     >>> repeatabilities using the package 'rptR' on my data set.  I have 879
>     >>> observations over 59 individuals.  I am using the calll
>     >>> rpt.remlLMM(y,groups) to obtain repeatabilities, but after about 15
>     >> seconds
>     >>> I get a error stating:
>     >>>
>     >>>  R for Windows GUI front-end has stopped working
>     >>>
>     >>> A problem caused the program to stop working correctly.  Windows
>     will
>     >> close
>     >>> the program and notify you if a solution is available.
>     >>>
>     >>>
>     >>> I am running Windows 7 with a i3 processor and 4 gb of memory so I
>     >> wouldn't
>     >>> expect this error to be computer performance related.
>     >>>
>     >>> I should note that I can run the rpt.aov(y,groups) call just
>     fine.  When
>     >>> running the following mixed model, I don't have any convergence
>     issues:
>     >>>
>     >>> lmer(Response~Predictor+(Predictor|Individual))
>     >>>
>     >>>
>     >>> Has anyone come across this issue or have any suggestions?
>     >>>
>     >>>
>     >>> Best,
>     >>> Jacob
>     >>>
>     >>
>     >>   (It would help to specify that rptR is available from r-forge:
>     it took
>     >> me a few extra minutes to dig around and find it.)
>     >>
>     >>   For what it's worth, rpt.remlLMM appears to use nlme::lme (not
>     >> lme4::lmer) internally.   There doesn't seem to be anything
>     particularly
>     >> scary in the guts of the function (e.g. no calls to compiled
>     code), so I
>     >> really haven't much of a clue.  A reproducible example would
>     probably be
>     >> helpful.  (Probably worth checking with the package maintainer as
>     well:
>     >>  maintainer("rptR")).
>     >>
>     >>   Can you run the examples in ?rpt.remlLMM successfully?  What if you
>     >> take those examples and bump up the number of permutation/bootstrap
>     >> replicates?
>     >>
>     >>   Ben Bolker
>     >>
>     >>
>     >>
>     >>
>     >>
>     >
> 
>


From shinichi.nakagawa at otago.ac.nz  Sun Jun  8 23:59:05 2014
From: shinichi.nakagawa at otago.ac.nz (Shinichi Nakagawa)
Date: Sun, 8 Jun 2014 21:59:05 +0000
Subject: [R-sig-ME] rpt.remlLMM(y, groups) causes R to crash
In-Reply-To: <5394D0B7.2060400@gmail.com>
References: <CAHe08Sh4LmeCc5aOqV_tVixKdBk7ci0p_S93m1brgw--bB=B-A@mail.gmail.com>
	<5391D62B.2060701@gmail.com>
	<CAHe08SgDu1gyoQQ_pfs9K3GWEipeL4QvXRiMNdd-+LNaOzB7QQ@mail.gmail.com>
	<539201C3.6040902@gmail.com>
	<CAHe08Sj-uODp8D5_U82mR4ytC1Avy9zk6nA1_YYT2hp4zK9G-Q@mail.gmail.com>
	<5394D0B7.2060400@gmail.com>
Message-ID: <54C801E3-6501-4E7B-B4A1-D750C06065E4@otago.ac.nz>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140608/b1e25483/attachment.pl>

From M.Fairbrother at bristol.ac.uk  Mon Jun  9 10:59:38 2014
From: M.Fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Mon, 9 Jun 2014 09:59:38 +0100
Subject: [R-sig-ME] uninformative priors for a threshold model estimated
	with MCMCglmm?
Message-ID: <CAAH-yP9SLJuz_Hxm5WohPg=Egb=twUeMjGahHfhEoHj8wuj=FQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140609/7b6455d9/attachment.pl>

From hans.ekbrand at gmail.com  Wed Jun 11 15:58:10 2014
From: hans.ekbrand at gmail.com (Hans Ekbrand)
Date: Wed, 11 Jun 2014 15:58:10 +0200
Subject: [R-sig-ME] anova() and the difference between (x | y) and (1 | y:x)
	in lme4
Message-ID: <20140611135810.GE6540@hans>

Dear list,

I have a question about the difference between

y ~ (1 | var2:var1) vs y ~ (var1 | var2).

In reality my model is more complex:

y ~ 1 + var1 + (1 | var2:var1) + var3+ .... + var9

vs

y ~ 1 + var1 + (var1 | var2) + var3+ .... + var9

Following the advice kindly given by Reinhold Kliegl way back ago
(https://stat.ethz.ch/pipermail/r-sig-mixed-models/2011q2/016545.html)
I have used the following specification with glmer() in lme4 (version 1.1-7):

fit.flat <- glmer(below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) + gender + age + age.2 + n.adults.minus.n.children + n.children + education + household.type, family = binomial("logit"), data = my.df)

and 

fit.hierarchical <- glmer(below.poverty.line ~ 1 + employment.type + (employment.type | country) + gender + age + age.2 + n.adults.minus.n.children + n.children + education + household.type, family = binomial("logit"), data = my.df)

Info on the data:

str(my.df)
'data.frame':	93178 obs. of  10 variables:
 $ below.poverty.line       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ employment.type          : Factor w/ 6 levels "Core labour force",..: 1 5 1 1 1 5 5 5 1 1 ...
 $ country                  : Factor w/ 22 levels "austria","belgium",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ gender                   : Factor w/ 2 levels "female","male": 2 1 2 2 1 1 1 1 2 2 ...
 $ age                      : num  22 22 32 56 40 54 42 18 49 20 ...
 $ age.2                    : num  3.39e-02 3.39e-02 7.08e-03 2.43e-02 1.71e-05 ...
 $ n.adults.minus.n.children: num  3 3 1 5 2 2 3 5 5 5 ...
 $ n.children               : num  1 1 2 0 1 0 1 0 0 0 ...
 $ education                : Factor w/ 4 levels "primary","lower secondary",..: 2 2 4 2 3 4 2 2 2 3 ...
 $ household.type           : Factor w/ 4 levels "couple without children",..: 2 2 3 1 3 4 3 4 1 4 ...

If you want to replicate the analysis - or inspect the data - try this:

load(url("http://hansekbrand.se/code/my.df.RData"))

The total computation time for both models is about one hour on my computer.

My primary question is whether or not anova() is usable to choose between the two models?

Data: my.df
Models:
fit.flat: below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) + 
fit.flat:     gender + age + age.2 + n.adults.minus.n.children + n.children + 
fit.flat:     education + household.type
fit.hierarchical: below.poverty.line ~ 1 + employment.type + (employment.type | 
fit.hierarchical:     country) + gender + age + age.2 + n.adults.minus.n.children + 
fit.hierarchical:     n.children + education + household.type
                 Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(>Chisq)    
fit.flat         18 38852 39022 -19408    38816                             
fit.hierarchical 38 38804 39163 -19364    38728 88.082     20  1.602e-10 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

My second question is whether or not I should care about the warnings
I get (not entirely sure which one belongs to which model, but the
first one should be against fit.hierarchcial).

Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00636715 (tol = 0.001, component 30)
Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?

summary(fit.flat)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) +  
    gender + age + age.2 + n.adults.minus.n.children + n.children +      education + household.type
   Data: my.df

     AIC      BIC   logLik deviance df.resid 
 38852.1  39022.1 -19408.1  38816.1    93160 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.5785 -0.2741 -0.1841 -0.1240 14.1696 

Random effects:
 Groups                  Name        Variance Std.Dev.
 country:employment.type (Intercept) 0.301    0.5486  
Number of obs: 93178, groups:  country:employment.type, 132

Fixed effects:
                                                     Estimate Std. Error z value Pr(>|z|)    
(Intercept)                                         -2.821530   0.164883 -17.112  < 2e-16 ***
employment.typeCore self-employed                    1.779764   0.177173  10.045  < 2e-16 ***
employment.typeInto core labour force                0.873362   0.183095   4.770 1.84e-06 ***
employment.typeMarginalized peripheral labour force  1.791760   0.185840   9.641  < 2e-16 ***
employment.typePeripheral labour force               1.036154   0.175026   5.920 3.22e-09 ***
employment.typePeripheral self-employed              1.699013   0.180444   9.416  < 2e-16 ***
gendermale                                           0.152666   0.029487   5.177 2.25e-07 ***
age                                                 -0.008906   0.001537  -5.794 6.86e-09 ***
age.2                                               -3.647558   1.044310  -3.493 0.000478 ***
n.adults.minus.n.children                            0.034069   0.010769   3.164 0.001559 ** 
n.children                                           0.258188   0.028628   9.019  < 2e-16 ***
educationlower secondary                            -0.399377   0.051611  -7.738 1.01e-14 ***
educationupper secondary                            -0.902910   0.049323 -18.306  < 2e-16 ***
educationpost secondary                             -1.582793   0.056489 -28.019  < 2e-16 ***
household.typecouple with children                  -0.120115   0.058652  -2.048 0.040568 *  
household.typesingle adult with children             0.514623   0.069323   7.424 1.14e-13 ***
household.typesingle adult without children          0.195389   0.041295   4.732 2.23e-06 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

summary(fit.hierarchical)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: below.poverty.line ~ 1 + employment.type + (employment.type |  
    country) + gender + age + age.2 + n.adults.minus.n.children +      n.children + education + household.type
   Data: my.df

     AIC      BIC   logLik deviance df.resid 
 38804.0  39162.8 -19364.0  38728.0    93140 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.5710 -0.2728 -0.1835 -0.1243 14.1030 

Random effects:
 Groups  Name                                                Variance Std.Dev. Corr                         
 country (Intercept)                                         0.2204   0.4695                                
         employment.typeCore self-employed                   0.4457   0.6676   -0.20                        
         employment.typeInto core labour force               0.3922   0.6263   -0.35  0.44                  
         employment.typeMarginalized peripheral labour force 0.1228   0.3504   -0.63  0.57  0.39            
         employment.typePeripheral labour force              0.1090   0.3301   -0.38  0.24  0.70  0.66      
         employment.typePeripheral self-employed             0.3823   0.6183   -0.32  0.85  0.82  0.66  0.65
Number of obs: 93178, groups:  country, 22

Fixed effects:
                                                     Estimate Std. Error z value Pr(>|z|)    
(Intercept)                                         -2.817822   0.153843 -18.316  < 2e-16 ***
employment.typeCore self-employed                    1.741686   0.156367  11.138  < 2e-16 ***
employment.typeInto core labour force                0.847817   0.156475   5.418 6.02e-08 ***
employment.typeMarginalized peripheral labour force  1.771534   0.110705  16.002  < 2e-16 ***
employment.typePeripheral labour force               1.021857   0.090266  11.321  < 2e-16 ***
employment.typePeripheral self-employed              1.636287   0.151647  10.790  < 2e-16 ***
gendermale                                           0.153356   0.029485   5.201 1.98e-07 ***
age                                                 -0.008938   0.001540  -5.805 6.44e-09 ***
age.2                                               -3.629799   1.103239  -3.290  0.00100 ** 
n.adults.minus.n.children                            0.035107   0.010791   3.253  0.00114 ** 
n.children                                           0.257672   0.028595   9.011  < 2e-16 ***
educationlower secondary                            -0.403009   0.051870  -7.770 7.87e-15 ***
educationupper secondary                            -0.899745   0.049781 -18.074  < 2e-16 ***
educationpost secondary                             -1.584911   0.056888 -27.860  < 2e-16 ***
household.typecouple with children                  -0.117651   0.058688  -2.005  0.04500 *  
household.typesingle adult with children             0.512608   0.069332   7.393 1.43e-13 ***
household.typesingle adult without children          0.197551   0.041314   4.782 1.74e-06 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


From Thierry.ONKELINX at inbo.be  Wed Jun 11 16:21:40 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 11 Jun 2014 14:21:40 +0000
Subject: [R-sig-ME] anova() and the difference between (x | y) and (1 |
 y:x)	in lme4
In-Reply-To: <20140611135810.GE6540@hans>
References: <20140611135810.GE6540@hans>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A78474@inbomail.inbo.be>

Dear Hans,

I assume that var1 is a factor variable.

The difference is in the distribution of the random effects.

(1|var1:var2) : all random intercept come from the same univariate normal distribution rnorm(mean = 0, sd = sigma)
(0 + var1|var2): the random intercepts come from a multivariate normal distribution: rmvnorm(mean = 0, sigma = Sigma). Sigma is a positive definite matrix

(0 + var1|var2) is a bit easier to understand because the BLUP's have the same interpretation of those of (1|var1:var2)

The bottom-line is that (var1|var2) and (1|var1:var2) allow the same model fit but (var1|var2) makes less assumptions at the cost of estimation more parameters. (var1|var2) requires n * (n + 1) / 2 parameters, with n = number of levels of var1. (1|var1:var2) requires just 1 parameter.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Hans Ekbrand
Verzonden: woensdag 11 juni 2014 15:58
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] anova() and the difference between (x | y) and (1 | y:x) in lme4

Dear list,

I have a question about the difference between

y ~ (1 | var2:var1) vs y ~ (var1 | var2).

In reality my model is more complex:

y ~ 1 + var1 + (1 | var2:var1) + var3+ .... + var9

vs

y ~ 1 + var1 + (var1 | var2) + var3+ .... + var9

Following the advice kindly given by Reinhold Kliegl way back ago
(https://stat.ethz.ch/pipermail/r-sig-mixed-models/2011q2/016545.html)
I have used the following specification with glmer() in lme4 (version 1.1-7):

fit.flat <- glmer(below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) + gender + age + age.2 + n.adults.minus.n.children + n.children + education + household.type, family = binomial("logit"), data = my.df)

and

fit.hierarchical <- glmer(below.poverty.line ~ 1 + employment.type + (employment.type | country) + gender + age + age.2 + n.adults.minus.n.children + n.children + education + household.type, family = binomial("logit"), data = my.df)

Info on the data:

str(my.df)
'data.frame':   93178 obs. of  10 variables:
 $ below.poverty.line       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ employment.type          : Factor w/ 6 levels "Core labour force",..: 1 5 1 1 1 5 5 5 1 1 ...
 $ country                  : Factor w/ 22 levels "austria","belgium",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ gender                   : Factor w/ 2 levels "female","male": 2 1 2 2 1 1 1 1 2 2 ...
 $ age                      : num  22 22 32 56 40 54 42 18 49 20 ...
 $ age.2                    : num  3.39e-02 3.39e-02 7.08e-03 2.43e-02 1.71e-05 ...
 $ n.adults.minus.n.children: num  3 3 1 5 2 2 3 5 5 5 ...
 $ n.children               : num  1 1 2 0 1 0 1 0 0 0 ...
 $ education                : Factor w/ 4 levels "primary","lower secondary",..: 2 2 4 2 3 4 2 2 2 3 ...
 $ household.type           : Factor w/ 4 levels "couple without children",..: 2 2 3 1 3 4 3 4 1 4 ...

If you want to replicate the analysis - or inspect the data - try this:

load(url("http://hansekbrand.se/code/my.df.RData"))

The total computation time for both models is about one hour on my computer.

My primary question is whether or not anova() is usable to choose between the two models?

Data: my.df
Models:
fit.flat: below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) +
fit.flat:     gender + age + age.2 + n.adults.minus.n.children + n.children +
fit.flat:     education + household.type
fit.hierarchical: below.poverty.line ~ 1 + employment.type + (employment.type |
fit.hierarchical:     country) + gender + age + age.2 + n.adults.minus.n.children +
fit.hierarchical:     n.children + education + household.type
                 Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(>Chisq)
fit.flat         18 38852 39022 -19408    38816
fit.hierarchical 38 38804 39163 -19364    38728 88.082     20  1.602e-10 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

My second question is whether or not I should care about the warnings I get (not entirely sure which one belongs to which model, but the first one should be against fit.hierarchcial).

Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00636715 (tol = 0.001, component 30) Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?

summary(fit.flat)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) +
    gender + age + age.2 + n.adults.minus.n.children + n.children +      education + household.type
   Data: my.df

     AIC      BIC   logLik deviance df.resid
 38852.1  39022.1 -19408.1  38816.1    93160

Scaled residuals:
    Min      1Q  Median      3Q     Max
-1.5785 -0.2741 -0.1841 -0.1240 14.1696

Random effects:
 Groups                  Name        Variance Std.Dev.
 country:employment.type (Intercept) 0.301    0.5486
Number of obs: 93178, groups:  country:employment.type, 132

Fixed effects:
                                                     Estimate Std. Error z value Pr(>|z|)
(Intercept)                                         -2.821530   0.164883 -17.112  < 2e-16 ***
employment.typeCore self-employed                    1.779764   0.177173  10.045  < 2e-16 ***
employment.typeInto core labour force                0.873362   0.183095   4.770 1.84e-06 ***
employment.typeMarginalized peripheral labour force  1.791760   0.185840   9.641  < 2e-16 ***
employment.typePeripheral labour force               1.036154   0.175026   5.920 3.22e-09 ***
employment.typePeripheral self-employed              1.699013   0.180444   9.416  < 2e-16 ***
gendermale                                           0.152666   0.029487   5.177 2.25e-07 ***
age                                                 -0.008906   0.001537  -5.794 6.86e-09 ***
age.2                                               -3.647558   1.044310  -3.493 0.000478 ***
n.adults.minus.n.children                            0.034069   0.010769   3.164 0.001559 **
n.children                                           0.258188   0.028628   9.019  < 2e-16 ***
educationlower secondary                            -0.399377   0.051611  -7.738 1.01e-14 ***
educationupper secondary                            -0.902910   0.049323 -18.306  < 2e-16 ***
educationpost secondary                             -1.582793   0.056489 -28.019  < 2e-16 ***
household.typecouple with children                  -0.120115   0.058652  -2.048 0.040568 *
household.typesingle adult with children             0.514623   0.069323   7.424 1.14e-13 ***
household.typesingle adult without children          0.195389   0.041295   4.732 2.23e-06 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

summary(fit.hierarchical)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: below.poverty.line ~ 1 + employment.type + (employment.type |
    country) + gender + age + age.2 + n.adults.minus.n.children +      n.children + education + household.type
   Data: my.df

     AIC      BIC   logLik deviance df.resid
 38804.0  39162.8 -19364.0  38728.0    93140

Scaled residuals:
    Min      1Q  Median      3Q     Max
-1.5710 -0.2728 -0.1835 -0.1243 14.1030

Random effects:
 Groups  Name                                                Variance Std.Dev. Corr
 country (Intercept)                                         0.2204   0.4695
         employment.typeCore self-employed                   0.4457   0.6676   -0.20
         employment.typeInto core labour force               0.3922   0.6263   -0.35  0.44
         employment.typeMarginalized peripheral labour force 0.1228   0.3504   -0.63  0.57  0.39
         employment.typePeripheral labour force              0.1090   0.3301   -0.38  0.24  0.70  0.66
         employment.typePeripheral self-employed             0.3823   0.6183   -0.32  0.85  0.82  0.66  0.65
Number of obs: 93178, groups:  country, 22

Fixed effects:
                                                     Estimate Std. Error z value Pr(>|z|)
(Intercept)                                         -2.817822   0.153843 -18.316  < 2e-16 ***
employment.typeCore self-employed                    1.741686   0.156367  11.138  < 2e-16 ***
employment.typeInto core labour force                0.847817   0.156475   5.418 6.02e-08 ***
employment.typeMarginalized peripheral labour force  1.771534   0.110705  16.002  < 2e-16 ***
employment.typePeripheral labour force               1.021857   0.090266  11.321  < 2e-16 ***
employment.typePeripheral self-employed              1.636287   0.151647  10.790  < 2e-16 ***
gendermale                                           0.153356   0.029485   5.201 1.98e-07 ***
age                                                 -0.008938   0.001540  -5.805 6.44e-09 ***
age.2                                               -3.629799   1.103239  -3.290  0.00100 **
n.adults.minus.n.children                            0.035107   0.010791   3.253  0.00114 **
n.children                                           0.257672   0.028595   9.011  < 2e-16 ***
educationlower secondary                            -0.403009   0.051870  -7.770 7.87e-15 ***
educationupper secondary                            -0.899745   0.049781 -18.074  < 2e-16 ***
educationpost secondary                             -1.584911   0.056888 -27.860  < 2e-16 ***
household.typecouple with children                  -0.117651   0.058688  -2.005  0.04500 *
household.typesingle adult with children             0.512608   0.069332   7.393 1.43e-13 ***
household.typesingle adult without children          0.197551   0.041314   4.782 1.74e-06 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.

From j.hadfield at ed.ac.uk  Wed Jun 11 16:27:33 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 11 Jun 2014 15:27:33 +0100
Subject: [R-sig-ME] Laplace approximation
Message-ID: <20140611152733.10851g3lwgblqzio@www.staffmail.ed.ac.uk>

Hi All,

I am confused about the differences between PQL and Laplace  
approximations to GLMM (after all, Breslow and Calyton's `PQL paper'  
seems to describe a Laplace approximation). If anyone could point me  
to a review paper that summarises the literature on these  
approximations, and ideally their history, I would be very grateful.

Cheers,

Jarrod

-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bbolker at gmail.com  Wed Jun 11 16:29:43 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 11 Jun 2014 10:29:43 -0400
Subject: [R-sig-ME] Laplace approximation
In-Reply-To: <20140611152733.10851g3lwgblqzio@www.staffmail.ed.ac.uk>
References: <20140611152733.10851g3lwgblqzio@www.staffmail.ed.ac.uk>
Message-ID: <539867D7.5020103@gmail.com>

On 14-06-11 10:27 AM, Jarrod Hadfield wrote:
> Hi All,
> 
> I am confused about the differences between PQL and Laplace
> approximations to GLMM (after all, Breslow and Calyton's `PQL paper'
> seems to describe a Laplace approximation). If anyone could point me to
> a review paper that summarises the literature on these approximations,
> and ideally their history, I would be very grateful.
> 
> Cheers,
> 
> Jarrod
> 


  Breslow 2003 "Whither PQL?" is pretty good, I think.


From bbolker at gmail.com  Wed Jun 11 16:38:38 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 11 Jun 2014 10:38:38 -0400
Subject: [R-sig-ME] anova() and the difference between (x | y) and (1 |
 y:x)	in lme4
In-Reply-To: <AA818EAD2576BC488B4F623941DA7427F3A78474@inbomail.inbo.be>
References: <20140611135810.GE6540@hans>
	<AA818EAD2576BC488B4F623941DA7427F3A78474@inbomail.inbo.be>
Message-ID: <539869EE.4020307@gmail.com>

On 14-06-11 10:21 AM, ONKELINX, Thierry wrote:
> Dear Hans,
> 
> I assume that var1 is a factor variable.
> 
> The difference is in the distribution of the random effects.
> 
> (1|var1:var2) : all random intercept come from the same univariate normal distribution rnorm(mean = 0, sd = sigma)
> (0 + var1|var2): the random intercepts come from a multivariate normal distribution: rmvnorm(mean = 0, sigma = Sigma). Sigma is a positive definite matrix
> 
> (0 + var1|var2) is a bit easier to understand because the BLUP's have the same interpretation of those of (1|var1:var2)
> 
> The bottom-line is that (var1|var2) and (1|var1:var2) allow the same model fit but (var1|var2) makes less assumptions at the cost of estimation more parameters. (var1|var2) requires n * (n + 1) / 2 parameters, with n = number of levels of var1. (1|var1:var2) requires just 1 parameter.
> 
> Best regards,
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> + 32 2 525 02 51
> + 32 54 43 61 85
> Thierry.Onkelinx at inbo.be
> www.inbo.be

  Thanks Tierry.

  I would add:

  (1) for what it's worth, lme offers an intermediate model (compound
symmetry), which allows for homogeneous but _negative_ within-group
correlation ((1|var1:var2) only allows for non-negative within-group
correlation)
  (2) the 'unstructured' (var1|var2) and 'grouped/positive compound
symmetry' models (1|var1:var2) are in principle nested (all
off-diagonals equal to zero, all diagonals identical -> simpler model),
so you should be able to use a likelihood ratio test/ANOVA to test.
  (3) your max|grad| convergence warnings are probably false positives;
I would try scaling&centring your continuous predictors to see if that
makes the eigenvalue warnings go away.

  Ben Bolker


> 
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
> 
> The plural of anecdote is not data.
> ~ Roger Brinner
> 
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
> 
> -----Oorspronkelijk bericht-----
> Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Hans Ekbrand
> Verzonden: woensdag 11 juni 2014 15:58
> Aan: r-sig-mixed-models at r-project.org
> Onderwerp: [R-sig-ME] anova() and the difference between (x | y) and (1 | y:x) in lme4
> 
> Dear list,
> 
> I have a question about the difference between
> 
> y ~ (1 | var2:var1) vs y ~ (var1 | var2).
> 
> In reality my model is more complex:
> 
> y ~ 1 + var1 + (1 | var2:var1) + var3+ .... + var9
> 
> vs
> 
> y ~ 1 + var1 + (var1 | var2) + var3+ .... + var9
> 
> Following the advice kindly given by Reinhold Kliegl way back ago
> (https://stat.ethz.ch/pipermail/r-sig-mixed-models/2011q2/016545.html)
> I have used the following specification with glmer() in lme4 (version 1.1-7):
> 
> fit.flat <- glmer(below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) + gender + age + age.2 + n.adults.minus.n.children + n.children + education + household.type, family = binomial("logit"), data = my.df)
> 
> and
> 
> fit.hierarchical <- glmer(below.poverty.line ~ 1 + employment.type + (employment.type | country) + gender + age + age.2 + n.adults.minus.n.children + n.children + education + household.type, family = binomial("logit"), data = my.df)
> 
> Info on the data:
> 
> str(my.df)
> 'data.frame':   93178 obs. of  10 variables:
>  $ below.poverty.line       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
>  $ employment.type          : Factor w/ 6 levels "Core labour force",..: 1 5 1 1 1 5 5 5 1 1 ...
>  $ country                  : Factor w/ 22 levels "austria","belgium",..: 1 1 1 1 1 1 1 1 1 1 ...
>  $ gender                   : Factor w/ 2 levels "female","male": 2 1 2 2 1 1 1 1 2 2 ...
>  $ age                      : num  22 22 32 56 40 54 42 18 49 20 ...
>  $ age.2                    : num  3.39e-02 3.39e-02 7.08e-03 2.43e-02 1.71e-05 ...
>  $ n.adults.minus.n.children: num  3 3 1 5 2 2 3 5 5 5 ...
>  $ n.children               : num  1 1 2 0 1 0 1 0 0 0 ...
>  $ education                : Factor w/ 4 levels "primary","lower secondary",..: 2 2 4 2 3 4 2 2 2 3 ...
>  $ household.type           : Factor w/ 4 levels "couple without children",..: 2 2 3 1 3 4 3 4 1 4 ...
> 
> If you want to replicate the analysis - or inspect the data - try this:
> 
> load(url("http://hansekbrand.se/code/my.df.RData"))
> 
> The total computation time for both models is about one hour on my computer.
> 
> My primary question is whether or not anova() is usable to choose between the two models?
> 
> Data: my.df
> Models:
> fit.flat: below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) +
> fit.flat:     gender + age + age.2 + n.adults.minus.n.children + n.children +
> fit.flat:     education + household.type
> fit.hierarchical: below.poverty.line ~ 1 + employment.type + (employment.type |
> fit.hierarchical:     country) + gender + age + age.2 + n.adults.minus.n.children +
> fit.hierarchical:     n.children + education + household.type
>                  Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(>Chisq)
> fit.flat         18 38852 39022 -19408    38816
> fit.hierarchical 38 38804 39163 -19364    38728 88.082     20  1.602e-10 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> My second question is whether or not I should care about the warnings I get (not entirely sure which one belongs to which model, but the first one should be against fit.hierarchcial).
> 
> Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.00636715 (tol = 0.001, component 30) Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model is nearly unidentifiable: very large eigenvalue
>  - Rescale variables?
> Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model is nearly unidentifiable: very large eigenvalue
>  - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
>  - Rescale variables?
> 
> summary(fit.flat)
> Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) +
>     gender + age + age.2 + n.adults.minus.n.children + n.children +      education + household.type
>    Data: my.df
> 
>      AIC      BIC   logLik deviance df.resid
>  38852.1  39022.1 -19408.1  38816.1    93160
> 
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -1.5785 -0.2741 -0.1841 -0.1240 14.1696
> 
> Random effects:
>  Groups                  Name        Variance Std.Dev.
>  country:employment.type (Intercept) 0.301    0.5486
> Number of obs: 93178, groups:  country:employment.type, 132
> 
> Fixed effects:
>                                                      Estimate Std. Error z value Pr(>|z|)
> (Intercept)                                         -2.821530   0.164883 -17.112  < 2e-16 ***
> employment.typeCore self-employed                    1.779764   0.177173  10.045  < 2e-16 ***
> employment.typeInto core labour force                0.873362   0.183095   4.770 1.84e-06 ***
> employment.typeMarginalized peripheral labour force  1.791760   0.185840   9.641  < 2e-16 ***
> employment.typePeripheral labour force               1.036154   0.175026   5.920 3.22e-09 ***
> employment.typePeripheral self-employed              1.699013   0.180444   9.416  < 2e-16 ***
> gendermale                                           0.152666   0.029487   5.177 2.25e-07 ***
> age                                                 -0.008906   0.001537  -5.794 6.86e-09 ***
> age.2                                               -3.647558   1.044310  -3.493 0.000478 ***
> n.adults.minus.n.children                            0.034069   0.010769   3.164 0.001559 **
> n.children                                           0.258188   0.028628   9.019  < 2e-16 ***
> educationlower secondary                            -0.399377   0.051611  -7.738 1.01e-14 ***
> educationupper secondary                            -0.902910   0.049323 -18.306  < 2e-16 ***
> educationpost secondary                             -1.582793   0.056489 -28.019  < 2e-16 ***
> household.typecouple with children                  -0.120115   0.058652  -2.048 0.040568 *
> household.typesingle adult with children             0.514623   0.069323   7.424 1.14e-13 ***
> household.typesingle adult without children          0.195389   0.041295   4.732 2.23e-06 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> summary(fit.hierarchical)
> Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
>  Family: binomial  ( logit )
> Formula: below.poverty.line ~ 1 + employment.type + (employment.type |
>     country) + gender + age + age.2 + n.adults.minus.n.children +      n.children + education + household.type
>    Data: my.df
> 
>      AIC      BIC   logLik deviance df.resid
>  38804.0  39162.8 -19364.0  38728.0    93140
> 
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -1.5710 -0.2728 -0.1835 -0.1243 14.1030
> 
> Random effects:
>  Groups  Name                                                Variance Std.Dev. Corr
>  country (Intercept)                                         0.2204   0.4695
>          employment.typeCore self-employed                   0.4457   0.6676   -0.20
>          employment.typeInto core labour force               0.3922   0.6263   -0.35  0.44
>          employment.typeMarginalized peripheral labour force 0.1228   0.3504   -0.63  0.57  0.39
>          employment.typePeripheral labour force              0.1090   0.3301   -0.38  0.24  0.70  0.66
>          employment.typePeripheral self-employed             0.3823   0.6183   -0.32  0.85  0.82  0.66  0.65
> Number of obs: 93178, groups:  country, 22
> 
> Fixed effects:
>                                                      Estimate Std. Error z value Pr(>|z|)
> (Intercept)                                         -2.817822   0.153843 -18.316  < 2e-16 ***
> employment.typeCore self-employed                    1.741686   0.156367  11.138  < 2e-16 ***
> employment.typeInto core labour force                0.847817   0.156475   5.418 6.02e-08 ***
> employment.typeMarginalized peripheral labour force  1.771534   0.110705  16.002  < 2e-16 ***
> employment.typePeripheral labour force               1.021857   0.090266  11.321  < 2e-16 ***
> employment.typePeripheral self-employed              1.636287   0.151647  10.790  < 2e-16 ***
> gendermale                                           0.153356   0.029485   5.201 1.98e-07 ***
> age                                                 -0.008938   0.001540  -5.805 6.44e-09 ***
> age.2                                               -3.629799   1.103239  -3.290  0.00100 **
> n.adults.minus.n.children                            0.035107   0.010791   3.253  0.00114 **
> n.children                                           0.257672   0.028595   9.011  < 2e-16 ***
> educationlower secondary                            -0.403009   0.051870  -7.770 7.87e-15 ***
> educationupper secondary                            -0.899745   0.049781 -18.074  < 2e-16 ***
> educationpost secondary                             -1.584911   0.056888 -27.860  < 2e-16 ***
> household.typecouple with children                  -0.117651   0.058688  -2.005  0.04500 *
> household.typesingle adult with children             0.512608   0.069332   7.393 1.43e-13 ***
> household.typesingle adult without children          0.197551   0.041314   4.782 1.74e-06 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> * * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
> The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From hans.ekbrand at gmail.com  Wed Jun 11 16:37:50 2014
From: hans.ekbrand at gmail.com (Hans Ekbrand)
Date: Wed, 11 Jun 2014 16:37:50 +0200
Subject: [R-sig-ME] anova() and the difference between (x | y) and (1 |
 y:x)	in lme4
In-Reply-To: <AA818EAD2576BC488B4F623941DA7427F3A78474@inbomail.inbo.be>
References: <20140611135810.GE6540@hans>
	<AA818EAD2576BC488B4F623941DA7427F3A78474@inbomail.inbo.be>
Message-ID: <20140611143750.GA17657@hans>

On Wed, Jun 11, 2014 at 02:21:40PM +0000, ONKELINX, Thierry wrote:
> Dear Hans,
> 
> I assume that var1 is a factor variable.

Yes.

> The difference is in the distribution of the random effects.
> 
> (1|var1:var2) : all random intercept come from the same univariate normal distribution rnorm(mean = 0, sd = sigma)
> (0 + var1|var2): the random intercepts come from a multivariate normal distribution: rmvnorm(mean = 0, sigma = Sigma). Sigma is a positive definite matrix
> 
> (0 + var1|var2) is a bit easier to understand because the BLUP's have the same interpretation of those of (1|var1:var2)
> 
> The bottom-line is that (var1|var2) and (1|var1:var2) allow the same model fit but (var1|var2) makes less assumptions at the cost of estimation more parameters. (var1|var2) requires n * (n + 1) / 2 parameters, with n = number of levels of var1. (1|var1:var2) requires just 1 parameter.

Thank you for the explanation, Thierry.

Do you know if it is sensible to compare the models with anova()?


From hans.ekbrand at gmail.com  Wed Jun 11 16:43:21 2014
From: hans.ekbrand at gmail.com (Hans Ekbrand)
Date: Wed, 11 Jun 2014 16:43:21 +0200
Subject: [R-sig-ME] anova() and the difference between (x | y) and (1 |
 y:x)	in lme4
In-Reply-To: <539869EE.4020307@gmail.com>
References: <20140611135810.GE6540@hans>
	<AA818EAD2576BC488B4F623941DA7427F3A78474@inbomail.inbo.be>
	<539869EE.4020307@gmail.com>
Message-ID: <20140611144321.GB17657@hans>

On Wed, Jun 11, 2014 at 10:38:38AM -0400, Ben Bolker wrote:
>   Thanks Tierry.
> 
>   I would add:
> 
>   (1) for what it's worth, lme offers an intermediate model (compound
> symmetry), which allows for homogeneous but _negative_ within-group
> correlation ((1|var1:var2) only allows for non-negative within-group
> correlation)

OK, good to know for the future.

>   (2) the 'unstructured' (var1|var2) and 'grouped/positive compound
> symmetry' models (1|var1:var2) are in principle nested (all
> off-diagonals equal to zero, all diagonals identical -> simpler model),
> so you should be able to use a likelihood ratio test/ANOVA to test.

Great, thanks!

>   (3) your max|grad| convergence warnings are probably false positives;
> I would try scaling&centring your continuous predictors to see if that
> makes the eigenvalue warnings go away.

OK, will do. Thanks again!


From hans.ekbrand at gmail.com  Wed Jun 11 23:16:51 2014
From: hans.ekbrand at gmail.com (Hans Ekbrand)
Date: Wed, 11 Jun 2014 23:16:51 +0200
Subject: [R-sig-ME] anova() and the difference between (x | y) and (1 |
 y:x) in lme4
In-Reply-To: <539869EE.4020307@gmail.com>
References: <20140611135810.GE6540@hans>
	<AA818EAD2576BC488B4F623941DA7427F3A78474@inbomail.inbo.be>
	<539869EE.4020307@gmail.com>
Message-ID: <20140611211651.GF6540@hans>

On Wed, Jun 11, 2014 at 10:38:38AM -0400, Ben Bolker wrote:
> On 14-06-11 10:21 AM, ONKELINX, Thierry wrote:
> > Dear Hans,
> > 
> > I assume that var1 is a factor variable.
> > 
> > The difference is in the distribution of the random effects.
> > 
> > (1|var1:var2) : all random intercept come from the same univariate normal distribution rnorm(mean = 0, sd = sigma)
> > (0 + var1|var2): the random intercepts come from a multivariate normal distribution: rmvnorm(mean = 0, sigma = Sigma). Sigma is a positive definite matrix
> > 
> > (0 + var1|var2) is a bit easier to understand because the BLUP's have the same interpretation of those of (1|var1:var2)
> > 
> > The bottom-line is that (var1|var2) and (1|var1:var2) allow the same model fit but (var1|var2) makes less assumptions at the cost of estimation more parameters. (var1|var2) requires n * (n + 1) / 2 parameters, with n = number of levels of var1. (1|var1:var2) requires just 1 parameter.

[...]

>   (2) the 'unstructured' (var1|var2) and 'grouped/positive compound
> symmetry' models (1|var1:var2) are in principle nested (all
> off-diagonals equal to zero, all diagonals identical -> simpler model),
> so you should be able to use a likelihood ratio test/ANOVA to test.

Would that hold even if I include a random intercept term for var2 (=country)
in the 'grouped/positive compound symmetry' model?

below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) + (1 | country) + gender + age + age.2 + n.adults.minus.n.children + n.children + education + household.type

> anova(fit.hierarchical, fit.flat.plus.random.intercept)
Data: my.df
Models:
fit.flat.plus.random.intercept: below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) + 
fit.flat.plus.random.intercept:     (1 | country) + gender + age + age.2 + n.adults.minus.n.children + 
fit.flat.plus.random.intercept:     n.children + education + household.type
fit.hierarchical: below.poverty.line ~ 1 + employment.type + (employment.type | 
fit.hierarchical:     country) + gender + age + age.2 + n.adults.minus.n.children + 
fit.hierarchical:     n.children + education + household.type
                               Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(>Chisq)   
fit.flat.plus.random.intercept 19 38808 38988 -19385    38770                            
fit.hierarchical               38 38804 39163 -19364    38728 42.161     19   0.001686 **


From ggrothendieck at gmail.com  Thu Jun 12 01:37:14 2014
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 11 Jun 2014 19:37:14 -0400
Subject: [R-sig-ME] lme4 performance
In-Reply-To: <CAP01uRnjfUZwAj-5ydGwsBuzFSOJ5TP=QehSOZLhNt0C8pZknw@mail.gmail.com>
References: <CAP01uRnjfUZwAj-5ydGwsBuzFSOJ5TP=QehSOZLhNt0C8pZknw@mail.gmail.com>
Message-ID: <CAP01uRn7qPOgwRLUUGP=szJ27-VrkCYdadDgTC2-V9beL4f6dg@mail.gmail.com>

Regarding the slowness issue of lme4 vs. lme4.0 I have had some good
initial results getting lme4 to run as fast or faster than lme4.0 by
replacing the optimizer (which can be done without touching the code
of lme4 itself thanks to lme4's modular nature).  See my comment in
lme4 issue 150 on github

https://github.com/lme4/lme4/issues/150

where I have posted the results and the code need to try this.

If anyone has a mixed model dataset they would like to try out using
this approach please let me know.



On Sun, Jun 1, 2014 at 11:04 PM, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> Ben Bolker and I have been running some lmer timings on the lme4 package and
> the older lme4.0 package and I am posting some timings here for comment.
>
> This model was used:
>
>    lmer(y ~ service * dept + (1|s) + (1|d), InstEval)
>
> with lme4 from https://github.com/lme4/lme4/ and lme4.0 from
> http://download.r-forge.r-project.org/src/contrib/lme4.0_0.999999-4.tar.gz
>
> The newer lme4 is slower than the older lme4.0:
>
> - lme4 takes 51 sec for the above command vs. 33 sec for lme4.0
> - lme4 spends 37 sec of that in C/C++ optimizer whereas lme4.0 spends 26 sec
> - lme4 spends 33 sec of that performing cholesky factorization whereas
>   lme4.0 spends 14 sec
> - lme4 involves 57 iterations of bobyqa wheresd lme4.0 involves 14 iterations
>   of nlminb.  The iterations are the counts displayed if the actual argument
>   verbose = TRUE is added to the lmer call.
>
> We can reduce the 51 sec run time of lme4 to 45 sec (about 10% savings) if we
> add this actual argument to the lmer call above:
>    control = lmerControl(calc.derivs = FALSE)
>
> The fixed and random coefficients from the two packages were very close.
>
> system.time(lmer(...)) was used to get the lmer run time.
> UNIX time was used to get the time used when launched from littler and
> the percentages given by the google perftools profiler, also launched from
> littler, were applied to the UNIX time to get the other timings.
>
> R 3.0.3 running under Linux was used.
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From j.hadfield at ed.ac.uk  Thu Jun 12 10:42:54 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 12 Jun 2014 09:42:54 +0100
Subject: [R-sig-ME] uninformative priors for a threshold model estimated
 with MCMCglmm?
In-Reply-To: <CAAH-yP9SLJuz_Hxm5WohPg=Egb=twUeMjGahHfhEoHj8wuj=FQ@mail.gmail.com>
References: <CAAH-yP9SLJuz_Hxm5WohPg=Egb=twUeMjGahHfhEoHj8wuj=FQ@mail.gmail.com>
Message-ID: <20140612094254.17309o9k9el6cmq8@www.staffmail.ed.ac.uk>

Hi Malcolm,

It is of course hard to say how data and prior will interact to  
generate a posterior (otherwise we wouldn't need MCMC).

However, if you look at the marginal properties of your four priors  
(see plot) then you can see that with nu=2.02 in the inverse-Wishart  
prior (particularly prior 1), small values of the variance have very  
low prior density.

v<-seq(0,1,length=1000)
par(mfrow=c(2,2))

plot(MCMCpack::dinvgamma(v, shape = 1.02/2, scale =(2.02*1)/2)~v,  
type="l", ylab="Density", xlab="Variance", main="Prior 1")
plot(MCMCpack::dinvgamma(v, shape = 1.02/2, scale =(2.02*0.1)/2)~v,  
type="l", ylab="Density", xlab="Variance", main="Prior 2")
plot(df(v, df1 = 1, df2 = 1.02)~v, type="l", ylab="Density",  
xlab="Variance", main="Prior 3")
plot(df(v/25, df1 = 1, df2 = 1.02)~v, type="l", ylab="Density",  
xlab="Variance", main="Prior 4")

I tend to use parameter expanded priors. I haven't come across any  
papers exploring their properties in the multivariate case (i.e. a  
covariance matrix) but some noddy simulations I've done in the past  
suggest they have better inferential properties than the  
inverse-Wishart. If there are papers out there I would love to hear  
about them.

Cheers,

Jarrod

Quoting Malcolm Fairbrother <M.Fairbrother at bristol.ac.uk> on Mon, 9  
Jun 2014 09:59:38 +0100:

> Dear all,
>
> I'm using MCMCglmm to estimate a two-level threshold model (for an ordinal
> outcome) with random intercepts plus a random slope for one covariate. I've
> tried using a variety of priors, and am finding they can make quite a
> difference to the variance of the posterior distribution for the
> coefficient on the variable whose slope I am allowing to vary.
>
> I would like to use a completely uninformative prior, and I'm wondering if
> I can get some advice on how to do so.
>
> The FOUR priors I've tried are:
>
> (1) list(R = list(V = 1, fix = 1), G = list(G1 = list(V = diag(2), nu =
> 2.02)))
>
> (2) list(R = list(V = 1, fix = 1), G = list(G1 = list(V = diag(2)*0.1, nu =
> 2.02))))
>
> (3) list(R = list(V = 1, fix = 1), G = list(G1 = list(V = diag(2), nu =
> 2.02, alpha.mu = rep(0, 2), alpha.V = diag(2))))
>
> (4) list(R = list(V = 1, fix = 1), G = list(G1 = list(V = diag(2), nu =
> 2.02, alpha.mu = rep(0, 2), alpha.V = diag(25^2, 2, 2))))
>
> With (1), the posterior mean for the random intercept variance is 0.145,
> random slope variance 0.074, and the CI on the relevant fixed effect
> coefficient is -0.059 to 0.142 ( ).
>
> With (2), the same things are 0.074, 0.011, and 0.006 to 0.086 (*).
>
> With (3), 0.070, 0.003, and 0.024 to 0.066 (***).
>
> With (4), 0.072, 0.003, and 0.025 to 0.067 (***).
>
> So, depending on the prior, the posterior distribution for the fixed effect
> coefficient may or may not overlap zero, though my impression is that the
> first prior doesn't make much sense because it suggests the two variances
> should each be around 1 when they're clearly both much smaller... And
> somehow this is affecting the distribution of the estimates of the fixed
> effect coefficient.
>
> The parameter-expanded priors are seemingly not much different depending on
> what I assign as the alpha.V. Am I safe/justified reporting results using
> these priors, and ignoring prior (1)?
>
> Any comments would be appreciated. Maybe this is just a simple/obvious (to
> some) illustration of the superiority of parameter-expanded priors.
>
> - Malcolm
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From M.Fairbrother at bristol.ac.uk  Fri Jun 13 00:46:38 2014
From: M.Fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Thu, 12 Jun 2014 23:46:38 +0100
Subject: [R-sig-ME] uninformative priors for a threshold model estimated
 with MCMCglmm?
In-Reply-To: <20140612094254.17309o9k9el6cmq8@www.staffmail.ed.ac.uk>
References: <CAAH-yP9SLJuz_Hxm5WohPg=Egb=twUeMjGahHfhEoHj8wuj=FQ@mail.gmail.com>
	<20140612094254.17309o9k9el6cmq8@www.staffmail.ed.ac.uk>
Message-ID: <CAAH-yP-Q577TRyTrAqBp6XPPHwmHk2UoGJOh=anJd5+j7U3JBQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140612/08f11074/attachment.pl>

From Thierry.ONKELINX at inbo.be  Fri Jun 13 13:35:02 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 13 Jun 2014 11:35:02 +0000
Subject: [R-sig-ME] anova() and the difference between (x | y) and (1 |
 y:x) in lme4
In-Reply-To: <20140611211651.GF6540@hans>
References: <20140611135810.GE6540@hans>
	<AA818EAD2576BC488B4F623941DA7427F3A78474@inbomail.inbo.be>
	<539869EE.4020307@gmail.com> <20140611211651.GF6540@hans>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A79CDA@inbomail.inbo.be>

Dear Hans,

I'm not sure if one can consider (A|B) and (1|B)  + (1|A:B) to be nested. (1|B)  + (A|B) and (1|B)  + (1|A:B) are nested. (1|A:B) is the same as (A|B) with constrains on the variance-covariance matrix.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Hans Ekbrand
Verzonden: woensdag 11 juni 2014 23:17
Aan: r-sig-mixed-models at r-project.org
Onderwerp: Re: [R-sig-ME] anova() and the difference between (x | y) and (1 | y:x) in lme4

On Wed, Jun 11, 2014 at 10:38:38AM -0400, Ben Bolker wrote:
> On 14-06-11 10:21 AM, ONKELINX, Thierry wrote:
> > Dear Hans,
> >
> > I assume that var1 is a factor variable.
> >
> > The difference is in the distribution of the random effects.
> >
> > (1|var1:var2) : all random intercept come from the same univariate
> > normal distribution rnorm(mean = 0, sd = sigma)
> > (0 + var1|var2): the random intercepts come from a multivariate
> > normal distribution: rmvnorm(mean = 0, sigma = Sigma). Sigma is a
> > positive definite matrix
> >
> > (0 + var1|var2) is a bit easier to understand because the BLUP's
> > have the same interpretation of those of (1|var1:var2)
> >
> > The bottom-line is that (var1|var2) and (1|var1:var2) allow the same model fit but (var1|var2) makes less assumptions at the cost of estimation more parameters. (var1|var2) requires n * (n + 1) / 2 parameters, with n = number of levels of var1. (1|var1:var2) requires just 1 parameter.

[...]

>   (2) the 'unstructured' (var1|var2) and 'grouped/positive compound
> symmetry' models (1|var1:var2) are in principle nested (all
> off-diagonals equal to zero, all diagonals identical -> simpler
> model), so you should be able to use a likelihood ratio test/ANOVA to test.

Would that hold even if I include a random intercept term for var2 (=country) in the 'grouped/positive compound symmetry' model?

below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) + (1 | country) + gender + age + age.2 + n.adults.minus.n.children + n.children + education + household.type

> anova(fit.hierarchical, fit.flat.plus.random.intercept)
Data: my.df
Models:
fit.flat.plus.random.intercept: below.poverty.line ~ 1 + employment.type + (1 | country:employment.type) +
fit.flat.plus.random.intercept:     (1 | country) + gender + age + age.2 + n.adults.minus.n.children +
fit.flat.plus.random.intercept:     n.children + education + household.type
fit.hierarchical: below.poverty.line ~ 1 + employment.type + (employment.type |
fit.hierarchical:     country) + gender + age + age.2 + n.adults.minus.n.children +
fit.hierarchical:     n.children + education + household.type
                               Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(>Chisq)
fit.flat.plus.random.intercept 19 38808 38988 -19385    38770
fit.hierarchical               38 38804 39163 -19364    38728 42.161     19   0.001686 **

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From nzm503 at gmail.com  Sun Jun  8 18:55:46 2014
From: nzm503 at gmail.com (Noelia Zarza Moratalla)
Date: Sun, 8 Jun 2014 18:55:46 +0200
Subject: [R-sig-ME] glmmADMB dude
Message-ID: <CAA=E8Rsx-N2YQwhU8BvffYmdjVy+jd1prJdnphkePJqyoKDCsA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140608/a2dad879/attachment.pl>

From dkarthik at iimahd.ernet.in  Fri Jun 13 10:54:58 2014
From: dkarthik at iimahd.ernet.in (Prof. Karthik D.)
Date: Fri, 13 Jun 2014 14:24:58 +0530
Subject: [R-sig-ME] =?utf-8?q?Is_there_a_way_of_getting_=E2=80=9Cmarginal_?=
	=?utf-8?q?effects=E2=80=9D_from_a_=60glmer=60_object?=
Message-ID: <CAASZSQqxEvn-ZCKDjf4TxUTu2r=wC2sV4yzdMETP21KQ1sv74Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140613/2dd783ac/attachment.pl>

From raptorbio at hotmail.com  Fri Jun 13 19:33:01 2014
From: raptorbio at hotmail.com (Adam Smith)
Date: Fri, 13 Jun 2014 13:33:01 -0400
Subject: [R-sig-ME] Reformat: Assistance with specification of crossover
 design model
In-Reply-To: <SNT151-W58573F42300BB2F68F9CB8A1290@phx.gbl>
References: <SNT151-W58573F42300BB2F68F9CB8A1290@phx.gbl>
Message-ID: <SNT151-W161450BD835131DB4DE6BBA1150@phx.gbl>

Thanks very much for your thoughts, Ben. I've added some additional thoughts below
that I hope you'll correct me on. ?

Best,
Adam

----------------------------------------

>
>> I'm analyzing data (variable names in brackets) generated by a
>> 4-period [period], 3-treatment [trt] crossover design. The design
>> was strongly balanced (all treatments preceded all others, including
>> itself) and uniform within period (all treatments occurred the same
>> # of times in each of the 4 periods).
>
>> This produced 18 distinct sequences of treatments (e.g., ABCC, CAAB,
>> etc.), to which a single individual [id] was randomly assigned. Two
>> covariates [cov1, cov2] were also measured on each individual.
>
>> The response [y] was continuous, and each individual was associated
> with a pre-study baseline measure [y0].
>
>> Because a single individual was assigned to each sequence, I believe
>> that a random effect for each individual will capture individual,
>> baseline, and sequence effects (they're perfectly confounded).
>
>> The question is simple: does the response differ among treatments?
>> I'm hoping the correct specification is as simple as I think it is,
>> illustrated below with mock data. My specific questions:
>
>
> Thanks for a very clear question. I'm going to take a stab at
> these, but haven't thought *very* deeply about them; hopefully
> this will inspire corrections/alternative answers.
>
>> 1 - Does this model correctly capture treatment, period, and
>> potential carryover effects? As I understand it, in a strongly
>> balanced design such as this, carryover and treatment effects are
>> not confounded, so my assumption is that I don't have to specify any
>> addition variable to capture this effect (e.g., a variable
>> indicating the treatment in the preceding period). I'm happy to be
>> corrected.
>
> I think you're right that the individual-level random effect
> controls for pre-treatment and sequence/carryover effects. However,
> I'm not sure it will capture period or treatment-by-period effects
> (i.e. residuals within the same period might be correlated, and
> residuals from individuals who received the treatment in the same
> period might be correlated).

On further thought, I think the carryover effects should be modeled
explicitly. In fact, we can distinguish between two types of carryover
effects - so-called 'self' (treatment follows itself) and 'mixed'?
(treatment follows a different treatment) carryover effects. ?A common?
model to analyze in this?case is:

Ydks = mu + Ak + Td(k,s) + I*Sd(k-1, s) + (1-I)*Md(k-1, s) + Us + eks

where:

Ydks = response to treatment d in period k for subject s
Ak = effect of period
Td = effect of treatment
I = indicator if treatment in period k-1 was the same as in period k
Sd = Self carryover effect of treatment d
Md = Mixed carryover effect of treatment d
? ?(NOTE: no carryover effect in period 1; i.e., Sd and Md = 0 in period 1)
? ?(I'm not sure how to distinguish this from treatment "0"; currently in?
? ? period 1, I've set Sd = Md = NA, but this is incorrect b/c it drops from
? ? consideration the observations from period 1)
Us = subject effect (random)
eks = error term

The corresponding LMM is specified, I think, as follows:

datURL <- "https://www.dropbox.com/s/qyer7qrl1ay2h22/xover_test.csv"

# Download data
dat <- repmis::source_data(datURL, sep = ",", header = TRUE)
dat <- within(dat, {
? ? ? ? ? ? ? id = factor(id)
? ? ? ? ? ? ? period = factor(period)
? ? ? ? ? ? ? cov1 = factor(cov1)
? ? ? ? ? ? ? cov2 = factor(cov2)
? ? ? ? ? ? ? selfco = factor(self * prevtrt)
? ? ? ? ? ? ? mixedco = factor((1-self) * prevtrt)
? ? ? ? ? ? ? trt = factor(trt)
? ? ? ? ? ? ? })
str(dat)


# Proposed linear mixed model analysis
require(lme4)
mod1 <- lmer(y ~ trt + period + selfco + mixedco + cov1 + cov2 +?
? ? ? ? ? ? ? ? (1|id), data = dat)
summary(mod1)

I tried including the baseline measurement (y0) as you suggested, but it?
reduced the variance estimate of (1|id) to zero. ?I'm inclined, then, to?
drop the baseline measurement and leave the random intercept for individuals. ?
In fact, the variance is very near zero even when excluding y0.?

>
> One way to see whether you might have missed something would
> be to draw (e.g.) boxplots of residuals by whatever groupings
> you might be concerned about.
>
>
>> 2 - Is an interaction between treatment and period prescribed?
>> Because the design is uniform within periods, I believe that period
>> and treatment effects are likewise not confounded.
>
> I think so.
>
>> 3 - Does the random effect for each individual captures variation
>> due to individual, sequence, and baseline measurement?
>
> I think so.
>
>> Thanks very much for your assistance,
>>
>> Adam Smith
>> Department of Natural Resources Science
>> University of Rhode Island
>>
>> # Download data
>> datURL <- "https://dl.dropboxusercontent.com/u/23278690/xover_test.csv"
>> dat <- repmis::source_data(datURL, sep = ",", header = TRUE)
>> dat <- within(dat, {
>> ? ? ?id = factor(id)
>> ? ? ?trt = factor(trt)
>> ? ? ?period = factor(period)
>> ? ? ?cov1 = factor(cov1)
>> ? ? ?cov2 = factor(cov2)
>> })
>>
>> require(lme4)
>> # Proposed linear mixed model analysis
>> mod1 <- lmer(y ~ trt + period + cov1 + cov2 + (1|id), data = dat)
>
> I think you do want trt*period (possibly as a random effect?)
> In a setting where I had access to some regularization (e.g.
> blme) I would be tempted to treat period as random -- but I wouldn't
> do it in this vanilla model, because there aren't enough periods to
> estimate it reliably.

I'm still unsure about the trt*period interaction. ?It will eat several degrees of
freedom as a fixed effect. ?Including it as a random effect (1|trt:period) produces
an estimate of zero variance. ?What is my interpretation if treating it as a random
effect?

>
> While you don't _need_ to incorporate pre-treatment covariate,
> carryover effects, etc., it might be interesting -- I think these
> would be partitioned out of the among-individual variation ...
>
>> # Test global treatment effect, for example...
>> mod2 <- update(mod1, . ~ . - trt)
>> anova(mod1, mod2)
>>
>> sessionInfo()
>>
>> R version 3.0.3 (2014-03-06)
>>
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>
>> attached base packages:[1] stats graphics grDevices utils datasets
> methods base
>
>> other attached packages:[1] car_2.0-19 AICcmodavg_1.35 lme4_1.1-5
>> Rcpp_0.11.1 Matrix_1.1-3 plyr_1.8.1
>
 		 	   		  

From hans.ekbrand at gmail.com  Sat Jun 14 00:12:37 2014
From: hans.ekbrand at gmail.com (Hans Ekbrand)
Date: Sat, 14 Jun 2014 00:12:37 +0200
Subject: [R-sig-ME]
 =?utf-8?q?Is_there_a_way_of_getting_=E2=80=9Cmarginal_?=
 =?utf-8?q?effects=E2=80=9D_from_a_=60glmer=60_object?=
In-Reply-To: <CAASZSQqxEvn-ZCKDjf4TxUTu2r=wC2sV4yzdMETP21KQ1sv74Q@mail.gmail.com>
References: <CAASZSQqxEvn-ZCKDjf4TxUTu2r=wC2sV4yzdMETP21KQ1sv74Q@mail.gmail.com>
Message-ID: <20140613221237.GA24110@hans>

On Fri, Jun 13, 2014 at 02:24:58PM +0530, Prof. Karthik D. wrote:
> I am estimating random effects logit model using glmer and I would like to
> report Marginal Effects for the independent variables. For glm models,
> package mfx helps compute marginal effects. Is there any package or
> function for glmer objects?

I recommend the "effects" package.

Package: effects
Version: 3.0-0
Date:    2014/03/20
Title:   Effect Displays for Linear, Generalized Linear,
         Multinomial-Logit, Proportional-Odds Logit Models and
         Mixed-Effects Models

Try:

install.packages("effects")
library(effects)
plot(Effect(c("gre"), cfelr, grid = TRUE))

Or, for interactions, eg:

plot(Effect(c("gre", "rank"), cfelr, grid = TRUE))


From mertens.ulf at gmail.com  Sat Jun 14 10:56:28 2014
From: mertens.ulf at gmail.com (Ulf Mertens)
Date: Sat, 14 Jun 2014 10:56:28 +0200
Subject: [R-sig-ME] Random Intercept + random slope model yields exactly the
 same results as random slope
Message-ID: <CA+7RmvbT8-Kdzyqtjix9EBOsmU-P3ZqjFD05+QxkW8k59c_gqQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140614/b0eebd1a/attachment.pl>

From Thierry.ONKELINX at inbo.be  Sat Jun 14 12:59:19 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Sat, 14 Jun 2014 10:59:19 +0000
Subject: [R-sig-ME] Random Intercept + random slope model yields exactly
 the same results as random slope
In-Reply-To: <CA+7RmvbT8-Kdzyqtjix9EBOsmU-P3ZqjFD05+QxkW8k59c_gqQ@mail.gmail.com>
References: <CA+7RmvbT8-Kdzyqtjix9EBOsmU-P3ZqjFD05+QxkW8k59c_gqQ@mail.gmail.com>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3A79E61@inbomail.inbo.be>

Dear Ulf,

Treatment is a factor. Hence the random 'slope' is a random intercept for each level of treatment. The difference between 1 + treatment and 0 + treatment (in case of a factor) is that 1 + treament uses the first level as a reference (random 'intercept'). The other levels are coded as the difference from the reference levels.

0 + treatment uses no reference level. All levels are coded as the direct effect of the level. Hence you get the same fit but with different parametrisation. Note that the variance-covariance matrix of the random effects are different.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey

________________________________________
Van: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] namens Ulf Mertens [mertens.ulf at gmail.com]
Verzonden: zaterdag 14 juni 2014 10:56
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Random Intercept + random slope model yields exactly the same results as random slope

Hi there,

When I run a random intercept + random slope model, I get exactly the same
result as when running a model where there is no random intercept. I can't
figure out why this is.

Model 1:

*m1 <- lmer(rt ~ treatment + (1+treatment|subject),data=df)*
*m1*
*Linear mixed model fit by REML ['lmerMod']*
*Formula: rt ~ treatment + (1 + treatment | subject)*
*   Data: df*
*REML criterion at convergence: 32558.52*
*Random effects:*
* Groups   Name        Std.Dev. Corr       *
* subject  (Intercept) 71.64               *
*          treatment2  54.55    -0.31      *
*          treatment3  54.79    -0.34  0.89*
* Residual             97.38               *
*Number of obs: 2700, groups: subject, 30*
*Fixed Effects:*
*(Intercept)   treatment2   treatment3  *
*   598.6675      -0.9037      50.9770  *

Model 2:

*m2 <- lmer(rt ~ treatment + (0+treatment|subject) ,data=df)*
*m2*
*Linear mixed model fit by REML ['lmerMod']*
*Formula: rt ~ treatment + (0 + treatment | subject)*
*   Data: df*
*REML criterion at convergence: 32558.52*
*Random effects:*
* Groups   Name       Std.Dev. Corr     *
* subject  treatment1 71.64             *
*          treatment2 75.18    0.72     *
*          treatment3 74.03    0.72 0.94*
* Residual            97.38             *
*Number of obs: 2700, groups: subject, 30*
*Fixed Effects:*
*(Intercept)   treatment2   treatment3  *
*   598.6675      -0.9037      50.9770  *

Compare both models:

*anova(m1,m2)*
*refitting model(s) with ML (instead of REML)*
*Data: df*
*Models:*
*m1: rt ~ treatment + (1 + treatment | subject)*
*m2: rt ~ treatment + (0 + treatment | subject)*
*    Df   AIC   BIC logLik deviance Chisq Chi Df Pr(>Chisq)*
*m1 10 32598 32657 -16289    32578                        *
*m2 10 32598 32657 -16289    32578     0      0          1*

Thanks in advance

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From mertens.ulf at gmail.com  Sat Jun 14 15:51:14 2014
From: mertens.ulf at gmail.com (Ulf Mertens)
Date: Sat, 14 Jun 2014 15:51:14 +0200
Subject: [R-sig-ME] Random Intercept + random slope model yields exactly
 the same results as random slope
In-Reply-To: <AA818EAD2576BC488B4F623941DA7427F3A79E61@inbomail.inbo.be>
References: <CA+7RmvbT8-Kdzyqtjix9EBOsmU-P3ZqjFD05+QxkW8k59c_gqQ@mail.gmail.com>
	<AA818EAD2576BC488B4F623941DA7427F3A79E61@inbomail.inbo.be>
Message-ID: <CA+7RmvZ3rR62LaXTyDSN1gktX+SJyyqQx5qYr=_gw5r0xMe2cQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140614/86407acf/attachment.pl>

From bbolker at gmail.com  Sat Jun 14 18:17:19 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 14 Jun 2014 12:17:19 -0400
Subject: [R-sig-ME] Random Intercept + random slope model yields exactly
 the same results as random slope
In-Reply-To: <CA+7RmvZ3rR62LaXTyDSN1gktX+SJyyqQx5qYr=_gw5r0xMe2cQ@mail.gmail.com>
References: <CA+7RmvbT8-Kdzyqtjix9EBOsmU-P3ZqjFD05+QxkW8k59c_gqQ@mail.gmail.com>	<AA818EAD2576BC488B4F623941DA7427F3A79E61@inbomail.inbo.be>
	<CA+7RmvZ3rR62LaXTyDSN1gktX+SJyyqQx5qYr=_gw5r0xMe2cQ@mail.gmail.com>
Message-ID: <539C758F.9070904@gmail.com>


On 14-06-14 09:51 AM, Ulf Mertens wrote:
> Dear Thierry,
> 
> thanks for your answer. The same also applies for those two models:
> 
>   m1 <- lmer(rt ~ treatment + (1+treatment|subject),data=df,REML=F)
>   m2 <- lmer(rt ~ treatment + (1|subject)+ (0+treatment|subject)
> ,data=df,REML=F)

   There are two possibilities, the first seems to be the case:

 (1) (1|subject) is confounded with (0+treatment|subject) -- that is,
the variation of treatments by subject includes the intercept variation
among subjects.  If this is the case, you should be getting warnings
about eigenvalues, and eigen(m2 at optinfo$derivs$Hessian)$values should
include a zero.  This seems to be the case (see example code below).

(2) the other would be that the model is well-behaved/well-defined, but
if the among-subject variance was zero, then you'd get the same fit to
both models.


===========
## set up data
dd <- expand.grid(treatment=LETTERS[1:4],subject=factor(1:30),rep=1:10)

## v-cov matrix
mm <- matrix(0.5,nrow=4,ncol=4)
diag(mm) <- 1:4
## transform to Cholesky-factor
theta <- t(chol(mm))[lower.tri(mm,diag=TRUE)]

## simulate response
library(lme4)
dd$s1 <- simulate(~treatment+(1+treatment|subject),
         newdata=dd,
         newparams=list(beta=rep(0,4),
                        theta=theta,
                        sigma=0.5),
               family=gaussian, seed=101)[[1]]

## utility
library(nloptr)
nloptWrap <- function(fn, par, lower, upper, control=list(), ...) {
    defaultControl <- list(xtol_rel = 1e-6, maxeval = 1e5)
    for (n in names(defaultControl))
      if (is.null(control[[n]])) control[[n]] <- defaultControl[[n]]
    res <- nloptr(x0=par, eval_f=fn, lb=lower, ub=upper, opts=control, ...)
    ##     ------
    with(res,list(par=solution,
                  fval=objective,
                  feval=iterations,
                  conv=if (status>0) 0 else status,
                  message=message))
}

## basic fit
m1 <- lmer(s1~treatment+(1+treatment|subject),data=dd,REML=FALSE)
m1B <- lmer(s1~treatment+(0+treatment|subject),data=dd,REML=FALSE)
nlc <- lmerControl(optimizer=nloptWrap,
                   optCtrl=list(algorithm="NLOPT_LN_BOBYQA"))
##  none of these models converge/fit well, even playing around
## with optimizers etc.
m2 <-
lmer(s1~treatment+(1|subject)+(1+treatment|subject),data=dd,REML=FALSE,
           control=nlc)
m3 <-
lmer(s1~treatment+(1|subject)+(0+treatment|subject),data=dd,REML=FALSE)
library(Matrix)
tmpf <- function(x) {
    h <- Matrix(x at optinfo$derivs$Hessian)
    ee <- eigen(h)$values
    ## force pos def
    if (min(ee)<0) h <- Matrix::nearPD(Matrix(h))$mat
    cc <- cov2cor(solve(h))
    cc[row(cc)==col(cc)] <- NA  ## diag() <- NA doesn't work for Matrix?
    print(image(cc))
    ee
}
tmpf(m1)
tmpf(m1B) ## rearranged, but not fundamentally different
tmpf(m2)
tmpf(m3)



> 
> deviance(m1)
> [1] 32769.35
> 
> deviance(m2)
> [1] 32769.35
> 
> anova(m1,m2)
> Data: df
> Models:
> ml3: rt ~ treatment + (1 + treatment | subject)
> ml2: rt ~ treatment + (1 | subject) + (0 + treatment | subject)
>     Df   AIC   BIC logLik deviance Chisq Chi Df Pr(>Chisq)
> ml3 10 32789 32848 -16385    32769
> ml2 11 32791 32856 -16385    32769     0      1          1
> 
> Can you tell me why this is?
> 
> Thanks for your help!
> 
> 
> On Sat, Jun 14, 2014 at 12:59 PM, ONKELINX, Thierry <
> Thierry.ONKELINX at inbo.be> wrote:
> 
>> Dear Ulf,
>>
>> Treatment is a factor. Hence the random 'slope' is a random intercept for
>> each level of treatment. The difference between 1 + treatment and 0 +
>> treatment (in case of a factor) is that 1 + treament uses the first level
>> as a reference (random 'intercept'). The other levels are coded as the
>> difference from the reference levels.
>>
>> 0 + treatment uses no reference level. All levels are coded as the direct
>> effect of the level. Hence you get the same fit but with different
>> parametrisation. Note that the variance-covariance matrix of the random
>> effects are different.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
>> Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>> + 32 2 525 02 51
>> + 32 54 43 61 85
>> Thierry.Onkelinx at inbo.be
>> www.inbo.be
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ________________________________________
>> Van: r-sig-mixed-models-bounces at r-project.org [
>> r-sig-mixed-models-bounces at r-project.org] namens Ulf Mertens [
>> mertens.ulf at gmail.com]
>> Verzonden: zaterdag 14 juni 2014 10:56
>> Aan: r-sig-mixed-models at r-project.org
>> Onderwerp: [R-sig-ME] Random Intercept + random slope model yields exactly
>> the same results as random slope
>>
>> Hi there,
>>
>> When I run a random intercept + random slope model, I get exactly the same
>> result as when running a model where there is no random intercept. I can't
>> figure out why this is.
>>
>> Model 1:
>>
>> *m1 <- lmer(rt ~ treatment + (1+treatment|subject),data=df)*
>> *m1*
>> *Linear mixed model fit by REML ['lmerMod']*
>> *Formula: rt ~ treatment + (1 + treatment | subject)*
>> *   Data: df*
>> *REML criterion at convergence: 32558.52*
>> *Random effects:*
>> * Groups   Name        Std.Dev. Corr       *
>> * subject  (Intercept) 71.64               *
>> *          treatment2  54.55    -0.31      *
>> *          treatment3  54.79    -0.34  0.89*
>> * Residual             97.38               *
>> *Number of obs: 2700, groups: subject, 30*
>> *Fixed Effects:*
>> *(Intercept)   treatment2   treatment3  *
>> *   598.6675      -0.9037      50.9770  *
>>
>> Model 2:
>>
>> *m2 <- lmer(rt ~ treatment + (0+treatment|subject) ,data=df)*
>> *m2*
>> *Linear mixed model fit by REML ['lmerMod']*
>> *Formula: rt ~ treatment + (0 + treatment | subject)*
>> *   Data: df*
>> *REML criterion at convergence: 32558.52*
>> *Random effects:*
>> * Groups   Name       Std.Dev. Corr     *
>> * subject  treatment1 71.64             *
>> *          treatment2 75.18    0.72     *
>> *          treatment3 74.03    0.72 0.94*
>> * Residual            97.38             *
>> *Number of obs: 2700, groups: subject, 30*
>> *Fixed Effects:*
>> *(Intercept)   treatment2   treatment3  *
>> *   598.6675      -0.9037      50.9770  *
>>
>> Compare both models:
>>
>> *anova(m1,m2)*
>> *refitting model(s) with ML (instead of REML)*
>> *Data: df*
>> *Models:*
>> *m1: rt ~ treatment + (1 + treatment | subject)*
>> *m2: rt ~ treatment + (0 + treatment | subject)*
>> *    Df   AIC   BIC logLik deviance Chisq Chi Df Pr(>Chisq)*
>> *m1 10 32598 32657 -16289    32578                        *
>> *m2 10 32598 32657 -16289    32578     0      0          1*
>>
>> Thanks in advance
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> * * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
>> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver
>> weer en binden het INBO onder geen enkel beding, zolang dit bericht niet
>> bevestigd is door een geldig ondertekend document.
>> The views expressed in this message and any annex are purely those of the
>> writer and may not be regarded as stating an official position of INBO, as
>> long as the message is not confirmed by a duly signed document.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Sat Jun 14 20:01:15 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 14 Jun 2014 14:01:15 -0400
Subject: [R-sig-ME] Random Intercept + random slope model yields exactly
 the same results as random slope
In-Reply-To: <CA+7RmvaV_Cdnb_upr203sF6VecV3j+Z-+rWOmNM14-+fnLwU=g@mail.gmail.com>
References: <CA+7RmvbT8-Kdzyqtjix9EBOsmU-P3ZqjFD05+QxkW8k59c_gqQ@mail.gmail.com>
	<AA818EAD2576BC488B4F623941DA7427F3A79E61@inbomail.inbo.be>
	<CA+7RmvZ3rR62LaXTyDSN1gktX+SJyyqQx5qYr=_gw5r0xMe2cQ@mail.gmail.com>
	<539C758F.9070904@gmail.com>
	<CA+7RmvaV_Cdnb_upr203sF6VecV3j+Z-+rWOmNM14-+fnLwU=g@mail.gmail.com>
Message-ID: <539C8DEB.4070508@gmail.com>

  [cc'ing back to r-sig-mixed-models]

 My point (which I probably should have said more directly) is that
you've constructed an unidentifiable model.  It's not really clear what
you're trying to do, but as currently formulated it's just not possible
to tell the difference between "among-subject variation" and
"among-subject variation in response to treatment".  I think if you want
to do this, you might have to work on constructing dummy variables
yourself: I think it might work better to use sum-to-zero contrasts ...

On 14-06-14 01:16 PM, Ulf Mertens wrote:
> Thanks for your detailed answer. Actually, I am simulating my data, too,
> so it shouldn't be very difficult to figure out what I am doing wrong.
> First, here's the output from eigen(m2 at optinfo$derivs$Hessian)$values:
> 
> eigen(m2 at optinfo$derivs$Hessian)$values
> [1] 482.082704275 441.411371639 219.759912576 193.156411039
> 155.499675763  69.193189621  -0.000237569
> 
> See my script to simulate the data attached. I am sampling the random
> intercept and the three random slopes from a multivariate normal
> distribution. The data, in the end, looks like this (here with 6 trials
> per subject):
> 
>   subject trial beta0 beta1 beta2    beta3 treatEff1 treatEff2 treatEff3
> subjectEffect       error treatment       rt
> 1       1     1   600     0     0 64.55883  40.01480 -14.71201 -27.38827
>      51.57127 -103.181056         1 588.4050
> 2       1     2   600     0     0 64.55883  40.01480 -14.71201 -27.38827
>      51.57127   34.425169         1 726.0112
> 3       1     3   600     0     0 64.55883  40.01480 -14.71201 -27.38827
>      51.57127  -24.821108         2 612.0382
> 4       1     4   600     0     0 64.55883  40.01480 -14.71201 -27.38827
>      51.57127  -63.495611         2 573.3637
> 5       1     5   600     0     0 64.55883  40.01480 -14.71201 -27.38827
>      51.57127  -67.078324         3 621.6635
> 6       1     6   600     0     0 64.55883  40.01480 -14.71201 -27.38827
>      51.57127   -5.059233         3 683.6826
> 7       2     1   600     0     0 64.55883  15.39385  23.04594 -78.88895
>     -62.95682   -5.987356         1 546.4497
> 
> and the model which created the response times (rt) is this:
> 
> df <- within(df, rt <- beta0 + 
>                  subjectEffect + 
>                  (beta1+treatmentEffect1)*x1 + 
>                  (beta2 + treatmentEffect2)*x2 + 
>                  (beta3 + treatmentEffect3)*x3 +
>                  error)
>   
> Thanks in advance
> 
> Regards,
> Ulf
> 
> 
> 
> 
> 
> On Sat, Jun 14, 2014 at 6:17 PM, Ben Bolker <bbolker at gmail.com
> <mailto:bbolker at gmail.com>> wrote:
> 
> 
>     On 14-06-14 09:51 AM, Ulf Mertens wrote:
>     > Dear Thierry,
>     >
>     > thanks for your answer. The same also applies for those two models:
>     >
>     >   m1 <- lmer(rt ~ treatment + (1+treatment|subject),data=df,REML=F)
>     >   m2 <- lmer(rt ~ treatment + (1|subject)+ (0+treatment|subject)
>     > ,data=df,REML=F)
> 
>        There are two possibilities, the first seems to be the case:
> 
>      (1) (1|subject) is confounded with (0+treatment|subject) -- that is,
>     the variation of treatments by subject includes the intercept variation
>     among subjects.  If this is the case, you should be getting warnings
>     about eigenvalues, and eigen(m2 at optinfo$derivs$Hessian)$values should
>     include a zero.  This seems to be the case (see example code below).
> 
>     (2) the other would be that the model is well-behaved/well-defined, but
>     if the among-subject variance was zero, then you'd get the same fit to
>     both models.
> 
> 
>     ===========
>     ## set up data
>     dd <- expand.grid(treatment=LETTERS[1:4],subject=factor(1:30),rep=1:10)
> 
>     ## v-cov matrix
>     mm <- matrix(0.5,nrow=4,ncol=4)
>     diag(mm) <- 1:4
>     ## transform to Cholesky-factor
>     theta <- t(chol(mm))[lower.tri(mm,diag=TRUE)]
> 
>     ## simulate response
>     library(lme4)
>     dd$s1 <- simulate(~treatment+(1+treatment|subject),
>              newdata=dd,
>              newparams=list(beta=rep(0,4),
>                             theta=theta,
>                             sigma=0.5),
>                    family=gaussian, seed=101)[[1]]
> 
>     ## utility
>     library(nloptr)
>     nloptWrap <- function(fn, par, lower, upper, control=list(), ...) {
>         defaultControl <- list(xtol_rel = 1e-6, maxeval = 1e5)
>         for (n in names(defaultControl))
>           if (is.null(control[[n]])) control[[n]] <- defaultControl[[n]]
>         res <- nloptr(x0=par, eval_f=fn, lb=lower, ub=upper,
>     opts=control, ...)
>         ##     ------
>         with(res,list(par=solution,
>                       fval=objective,
>                       feval=iterations,
>                       conv=if (status>0) 0 else status,
>                       message=message))
>     }
> 
>     ## basic fit
>     m1 <- lmer(s1~treatment+(1+treatment|subject),data=dd,REML=FALSE)
>     m1B <- lmer(s1~treatment+(0+treatment|subject),data=dd,REML=FALSE)
>     nlc <- lmerControl(optimizer=nloptWrap,
>                        optCtrl=list(algorithm="NLOPT_LN_BOBYQA"))
>     ##  none of these models converge/fit well, even playing around
>     ## with optimizers etc.
>     m2 <-
>     lmer(s1~treatment+(1|subject)+(1+treatment|subject),data=dd,REML=FALSE,
>                control=nlc)
>     m3 <-
>     lmer(s1~treatment+(1|subject)+(0+treatment|subject),data=dd,REML=FALSE)
>     library(Matrix)
>     tmpf <- function(x) {
>         h <- Matrix(x at optinfo$derivs$Hessian)
>         ee <- eigen(h)$values
>         ## force pos def
>         if (min(ee)<0) h <- Matrix::nearPD(Matrix(h))$mat
>         cc <- cov2cor(solve(h))
>         cc[row(cc)==col(cc)] <- NA  ## diag() <- NA doesn't work for Matrix?
>         print(image(cc))
>         ee
>     }
>     tmpf(m1)
>     tmpf(m1B) ## rearranged, but not fundamentally different
>     tmpf(m2)
>     tmpf(m3)
> 
> 
> 
>     >
>     > deviance(m1)
>     > [1] 32769.35
>     >
>     > deviance(m2)
>     > [1] 32769.35
>     >
>     > anova(m1,m2)
>     > Data: df
>     > Models:
>     > ml3: rt ~ treatment + (1 + treatment | subject)
>     > ml2: rt ~ treatment + (1 | subject) + (0 + treatment | subject)
>     >     Df   AIC   BIC logLik deviance Chisq Chi Df Pr(>Chisq)
>     > ml3 10 32789 32848 -16385    32769
>     > ml2 11 32791 32856 -16385    32769     0      1          1
>     >
>     > Can you tell me why this is?
>     >
>     > Thanks for your help!
>     >
>     >
>     > On Sat, Jun 14, 2014 at 12:59 PM, ONKELINX, Thierry <
>     > Thierry.ONKELINX at inbo.be <mailto:Thierry.ONKELINX at inbo.be>> wrote:
>     >
>     >> Dear Ulf,
>     >>
>     >> Treatment is a factor. Hence the random 'slope' is a random
>     intercept for
>     >> each level of treatment. The difference between 1 + treatment and 0 +
>     >> treatment (in case of a factor) is that 1 + treament uses the
>     first level
>     >> as a reference (random 'intercept'). The other levels are coded
>     as the
>     >> difference from the reference levels.
>     >>
>     >> 0 + treatment uses no reference level. All levels are coded as
>     the direct
>     >> effect of the level. Hence you get the same fit but with different
>     >> parametrisation. Note that the variance-covariance matrix of the
>     random
>     >> effects are different.
>     >>
>     >> Best regards,
>     >>
>     >> ir. Thierry Onkelinx
>     >> Instituut voor natuur- en bosonderzoek / Research Institute for
>     Nature and
>     >> Forest
>     >> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>     >> Kliniekstraat 25
>     >> 1070 Anderlecht
>     >> Belgium
>     >> + 32 2 525 02 51 <tel:%2B%2032%202%20525%2002%2051>
>     >> + 32 54 43 61 85 <tel:%2B%2032%2054%2043%2061%2085>
>     >> Thierry.Onkelinx at inbo.be <mailto:Thierry.Onkelinx at inbo.be>
>     >> www.inbo.be <http://www.inbo.be>
>     >> To call in the statistician after the experiment is done may be
>     no more
>     >> than asking him to perform a post-mortem examination: he may be
>     able to say
>     >> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>     >> The plural of anecdote is not data. ~ Roger Brinner
>     >> The combination of some data and an aching desire for an answer
>     does not
>     >> ensure that a reasonable answer can be extracted from a given
>     body of data.
>     >> ~ John Tukey
>     >>
>     >> ________________________________________
>     >> Van: r-sig-mixed-models-bounces at r-project.org
>     <mailto:r-sig-mixed-models-bounces at r-project.org> [
>     >> r-sig-mixed-models-bounces at r-project.org
>     <mailto:r-sig-mixed-models-bounces at r-project.org>] namens Ulf Mertens [
>     >> mertens.ulf at gmail.com <mailto:mertens.ulf at gmail.com>]
>     >> Verzonden: zaterdag 14 juni 2014 10:56
>     >> Aan: r-sig-mixed-models at r-project.org
>     <mailto:r-sig-mixed-models at r-project.org>
>     >> Onderwerp: [R-sig-ME] Random Intercept + random slope model
>     yields exactly
>     >> the same results as random slope
>     >>
>     >> Hi there,
>     >>
>     >> When I run a random intercept + random slope model, I get exactly
>     the same
>     >> result as when running a model where there is no random
>     intercept. I can't
>     >> figure out why this is.
>     >>
>     >> Model 1:
>     >>
>     >> *m1 <- lmer(rt ~ treatment + (1+treatment|subject),data=df)*
>     >> *m1*
>     >> *Linear mixed model fit by REML ['lmerMod']*
>     >> *Formula: rt ~ treatment + (1 + treatment | subject)*
>     >> *   Data: df*
>     >> *REML criterion at convergence: 32558.52*
>     >> *Random effects:*
>     >> * Groups   Name        Std.Dev. Corr       *
>     >> * subject  (Intercept) 71.64               *
>     >> *          treatment2  54.55    -0.31      *
>     >> *          treatment3  54.79    -0.34  0.89*
>     >> * Residual             97.38               *
>     >> *Number of obs: 2700, groups: subject, 30*
>     >> *Fixed Effects:*
>     >> *(Intercept)   treatment2   treatment3  *
>     >> *   598.6675      -0.9037      50.9770  *
>     >>
>     >> Model 2:
>     >>
>     >> *m2 <- lmer(rt ~ treatment + (0+treatment|subject) ,data=df)*
>     >> *m2*
>     >> *Linear mixed model fit by REML ['lmerMod']*
>     >> *Formula: rt ~ treatment + (0 + treatment | subject)*
>     >> *   Data: df*
>     >> *REML criterion at convergence: 32558.52*
>     >> *Random effects:*
>     >> * Groups   Name       Std.Dev. Corr     *
>     >> * subject  treatment1 71.64             *
>     >> *          treatment2 75.18    0.72     *
>     >> *          treatment3 74.03    0.72 0.94*
>     >> * Residual            97.38             *
>     >> *Number of obs: 2700, groups: subject, 30*
>     >> *Fixed Effects:*
>     >> *(Intercept)   treatment2   treatment3  *
>     >> *   598.6675      -0.9037      50.9770  *
>     >>
>     >> Compare both models:
>     >>
>     >> *anova(m1,m2)*
>     >> *refitting model(s) with ML (instead of REML)*
>     >> *Data: df*
>     >> *Models:*
>     >> *m1: rt ~ treatment + (1 + treatment | subject)*
>     >> *m2: rt ~ treatment + (0 + treatment | subject)*
>     >> *    Df   AIC   BIC logLik deviance Chisq Chi Df Pr(>Chisq)*
>     >> *m1 10 32598 32657 -16289    32578                        *
>     >> *m2 10 32598 32657 -16289    32578     0      0          1*
>     >>
>     >> Thanks in advance
>     >>
>     >>         [[alternative HTML version deleted]]
>     >>
>     >> _______________________________________________
>     >> R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>     >> * * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * *
>     * * *
>     >> Dit bericht en eventuele bijlagen geven enkel de visie van de
>     schrijver
>     >> weer en binden het INBO onder geen enkel beding, zolang dit
>     bericht niet
>     >> bevestigd is door een geldig ondertekend document.
>     >> The views expressed in this message and any annex are purely
>     those of the
>     >> writer and may not be regarded as stating an official position of
>     INBO, as
>     >> long as the message is not confirmed by a duly signed document.
>     >>
>     >
>     >       [[alternative HTML version deleted]]
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>     >
> 
>     _______________________________________________
>     R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
>


From mertens.ulf at gmail.com  Sun Jun 15 21:27:56 2014
From: mertens.ulf at gmail.com (Ulf Mertens)
Date: Sun, 15 Jun 2014 21:27:56 +0200
Subject: [R-sig-ME] Random Intercept + random slope model yields exactly
 the same results as random slope
In-Reply-To: <539C8DEB.4070508@gmail.com>
References: <CA+7RmvbT8-Kdzyqtjix9EBOsmU-P3ZqjFD05+QxkW8k59c_gqQ@mail.gmail.com>
	<AA818EAD2576BC488B4F623941DA7427F3A79E61@inbomail.inbo.be>
	<CA+7RmvZ3rR62LaXTyDSN1gktX+SJyyqQx5qYr=_gw5r0xMe2cQ@mail.gmail.com>
	<539C758F.9070904@gmail.com>
	<CA+7RmvaV_Cdnb_upr203sF6VecV3j+Z-+rWOmNM14-+fnLwU=g@mail.gmail.com>
	<539C8DEB.4070508@gmail.com>
Message-ID: <CA+7RmvbNa6T6d+EMUQepHJF055wQ8+PHC-17OBNqxH47N27RHQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140615/fc7c3090/attachment.pl>

From mbrooks at ufl.edu  Mon Jun 16 12:24:59 2014
From: mbrooks at ufl.edu (Mollie Brooks)
Date: Mon, 16 Jun 2014 12:24:59 +0200
Subject: [R-sig-ME] gamm4: predict to reflect random effects?
Message-ID: <5CF76FF0-7702-4007-A619-EF815125B3E1@ufl.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140616/2d0da052/attachment.pl>

From henrik.singmann at psychologie.uni-freiburg.de  Mon Jun 16 22:14:43 2014
From: henrik.singmann at psychologie.uni-freiburg.de (Henrik Singmann)
Date: Mon, 16 Jun 2014 22:14:43 +0200
Subject: [R-sig-ME]
 =?windows-1252?q?Is_there_a_way_of_getting_=93marginal?=
 =?windows-1252?q?_effects=94_from_a_=60glmer=60_object?=
In-Reply-To: <CAASZSQqxEvn-ZCKDjf4TxUTu2r=wC2sV4yzdMETP21KQ1sv74Q@mail.gmail.com>
References: <CAASZSQqxEvn-ZCKDjf4TxUTu2r=wC2sV4yzdMETP21KQ1sv74Q@mail.gmail.com>
Message-ID: <539F5033.10506@psychologie.uni-freiburg.de>

An alternatvie to the effects package could be either the lsmeans package or even lsmeans in combination with multcomp (see the great multcomp book for more examples).

Note however that this approach to my knowledge completely ignores the random effects. You most likely will need to use precit.merMod and marginalize on your own given the random effects structure you desire if you want to incorporate the random effects.

Hope that helps,
Henrik

require(lsmeans)
(l1 <- lsmeans(cfelr, ~ rank | ran, at = list(ran = quantile(mydata$ran, c(0.25, 0.5, 0.75)))))
## ran = 0.3027645:
##  rank      lsmean        SE df  asymp.LCL   asymp.UCL
##  1    -0.04255557 0.2966504 NA -0.6240500  0.53893886
##  2    -0.72272008 0.2089092 NA -1.1322242 -0.31321594
##  3    -1.46011460 0.2828895 NA -2.0146349 -0.90559430
##  4    -1.62713014 0.3568840 NA -2.3266946 -0.92756565
##
## ran = 0.5592498:
##  rank      lsmean        SE df  asymp.LCL   asymp.UCL
##  1     0.06329396 0.2809776 NA -0.4874786  0.61406651
##  2    -0.61687056 0.1872073 NA -0.9838346 -0.24990654
##  3    -1.35426508 0.2619736 NA -1.8677861 -0.84074405
##  4    -1.52128062 0.3490954 NA -2.2055778 -0.83698340
##
## ran = 0.7915128:
##  rank      lsmean        SE df  asymp.LCL   asymp.UCL
##  1     0.15914709 0.3000271 NA -0.4289664  0.74726057
##  2    -0.52101743 0.2157033 NA -0.9438392 -0.09819563
##  3    -1.25841195 0.2785249 NA -1.8043768 -0.71244708
##  4    -1.42542749 0.3689908 NA -2.1487238 -0.70213122
##
## Confidence level used: 0.95

# on probability scale
binomial()$linkinv(summary(l1)$lsmean)
## [1] 0.4893627 0.3267943 0.1884498 0.1642239 0.5158182 0.3504935
## 0.2051740 0.1792730 0.5397030 0.3726144 0.2212474 0.1938121

require(multcomp)
lapply(as.glht(l1, df = 0), summary)
## $`ran = 0.302764488209505`
##
## 	 Simultaneous Tests for General Linear Hypotheses
##
## Linear Hypotheses:
##        Estimate Std. Error z value Pr(>|z|)
## 1 == 0 -0.04256    0.29665  -0.143  0.99982
## 2 == 0 -0.72272    0.20891  -3.459  0.00217 **
## 3 == 0 -1.46011    0.28289  -5.161  < 1e-04 ***
## 4 == 0 -1.62713    0.35688  -4.559  < 1e-04 ***
## ---
## Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
## (Adjusted p values reported -- single-step method)
##
##
## $`ran = 0.559249840560369`
##
## 	 Simultaneous Tests for General Linear Hypotheses
##
## Linear Hypotheses:
##        Estimate Std. Error z value Pr(>|z|)
## 1 == 0  0.06329    0.28098   0.225  0.99898
## 2 == 0 -0.61687    0.18721  -3.295  0.00393 **
## 3 == 0 -1.35427    0.26197  -5.169  < 1e-05 ***
## 4 == 0 -1.52128    0.34910  -4.358 5.27e-05 ***
## ---
## Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
## (Adjusted p values reported -- single-step method)
##
##
## $`ran = 0.791512808995321`
##
## 	 Simultaneous Tests for General Linear Hypotheses
##
## Linear Hypotheses:
##        Estimate Std. Error z value Pr(>|z|)
## 1 == 0   0.1591     0.3000   0.530 0.970969
## 2 == 0  -0.5210     0.2157  -2.415 0.060143 .
## 3 == 0  -1.2584     0.2785  -4.518  < 1e-04 ***
## 4 == 0  -1.4254     0.3690  -3.863 0.000449 ***
## ---
## Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
## (Adjusted p values reported -- single-step method)


Am 13.06.2014 10:54, schrieb Prof. Karthik D.:
> Hi
> I have asked this question on StackOverflow and was asked by Ben Bolker to
> post it here.
>
> thanks for your help.
>
> Karthik
>
>
>
> *reproducing the SO
> question http://stackoverflow.com/questions/24177197/is-there-a-way-of-getting-marginal-effects-from-a-glmer-object
> <http://stackoverflow.com/questions/24177197/is-there-a-way-of-getting-marginal-effects-from-a-glmer-object>*
>
> I am estimating random effects logit model using glmer and I would like to
> report Marginal Effects for the independent variables. For glm models,
> package mfx helps compute marginal effects. Is there any package or
> function for glmer objects?
>
> Thanks for your help.
>
> A reproducible example is given below
>
> mydata <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
> mydata$rank <- factor(mydata$rank) #creating ranks
> id <- rep(1:ceiling(nrow(mydata)/2), times=c(2)) #creating ID variable
> mydata <- cbind(mydata,data.frame(id,stringsAsFactors=FALSE))
> set.seed(12345)
> mydata$ran <- runif(nrow(mydata),0,1) #creating a random variable
>
> library(lme4)
> cfelr <- glmer(admit ~ (1 | id) + rank + gpa + ran + gre, data=mydata
> ,family = binomial)
> summary(cfelr)
>
> 	[[alternative HTML version deleted]]
>

-- 
Dr. Henrik Singmann
Albert-Ludwigs-Universit?t Freiburg, Germany
http://www.psychologie.uni-freiburg.de/Members/singmann


From brandarizsofia at gmail.com  Sun Jun 15 20:57:26 2014
From: brandarizsofia at gmail.com (=?UTF-8?Q?Sof=C3=ADa_Brandariz?=)
Date: Sun, 15 Jun 2014 15:57:26 -0300
Subject: [R-sig-ME] Fwd: lme4 convergence warnings
In-Reply-To: <CAOn1tG0YctfFpMuoYwsYLn2xdVLDZF=ZkL8kiVL294xNEhuruw@mail.gmail.com>
References: <CAOn1tG0YctfFpMuoYwsYLn2xdVLDZF=ZkL8kiVL294xNEhuruw@mail.gmail.com>
Message-ID: <CAOn1tG2u8u052J9CxMvaZx17q=kz_2S47G4TKSmPUiTX=NGLSw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140615/3768b6dc/attachment.pl>

From pcp35 at cornell.edu  Tue Jun 17 15:10:46 2014
From: pcp35 at cornell.edu (Phillips Perry)
Date: Tue, 17 Jun 2014 09:10:46 -0400
Subject: [R-sig-ME] Fwd: lme4 convergence warnings
In-Reply-To: <CAOn1tG2u8u052J9CxMvaZx17q=kz_2S47G4TKSmPUiTX=NGLSw@mail.gmail.com>
References: <CAOn1tG0YctfFpMuoYwsYLn2xdVLDZF=ZkL8kiVL294xNEhuruw@mail.gmail.com>
	<CAOn1tG2u8u052J9CxMvaZx17q=kz_2S47G4TKSmPUiTX=NGLSw@mail.gmail.com>
Message-ID: <CAMrwhV9AsutV5NyuZWKdH9qVsemw8v2rd5DD2UEs3mzT80E9=A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140617/04273fae/attachment.pl>

From fcaryl at unimelb.edu.au  Wed Jun 18 08:22:08 2014
From: fcaryl at unimelb.edu.au (Fiona Mae Caryl)
Date: Wed, 18 Jun 2014 06:22:08 +0000
Subject: [R-sig-ME] Error using MuMIn dredge with glmer.nb
Message-ID: <E12DEB6AB909BC48ABBF65C54584DD0272A50B9F@000S-EX-MBX-QS4.unimelb.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140618/207c698d/attachment.pl>

From bbolker at gmail.com  Thu Jun 19 00:01:33 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 18 Jun 2014 22:01:33 +0000 (UTC)
Subject: [R-sig-ME] Error using MuMIn dredge with glmer.nb
References: <E12DEB6AB909BC48ABBF65C54584DD0272A50B9F@000S-EX-MBX-QS4.unimelb.edu.au>
Message-ID: <loom.20140619T000124-606@post.gmane.org>

Fiona Mae Caryl <fcaryl at ...> writes:

> 
> Dear R Users,
> 
> I receive an error message when using the dredge function in MuMIn
> on a glmer.nb glmerMod object.
> 
> "Error in dredge(cm.nb) :
>   call to 'global.model' contains '...' 
> arguments and cannot be updated: glmer(formula = CM ~ BUSH + FRAG +
> I(TREE^2) + (DWELL * TREE) + (1 | XSCAPE), data = ..2, 
> family = negative.binomial(theta = th))"
 
> Before I provide an example I thought it best to ask: is it possible
>  to dredge a glmer.nb glmerMod object?

  Well, at the moment (as you can see) no.  In principle we should
be able to make this possible, but glmer.nb has not been very thoroughly
tested, and involves another level of nesting of operations within
functions, so it doesn't particularly surprise me that this fails.
https://github.com/lme4/lme4/issues/176 highlights a closely related
problem.

> I have successfully dredged other glmerMod objects as well as
>  negative binomial mixed model objects from glmmadmb.
 
> Any pointers would be much appreciated as the fitted vs residual
> plots from glmer.nb are much less skewed than those from glmmadmb.

  Well, that probably has something to do with the way the residuals
are calculated (which also hasn't been carefully tested, neither in
glmmADMB nor in glmer.nb)!  It probably makes more sense to
double-check the way the residuals are calculated in both cases and
see why they don't agree.  Do your fits themselves agree (i.e., the
fixed-effect and variance-covariance estimates match closely) if you
try the same model in glmer.nb and glmmADMB?


> Best wishes,
> Fiona
> ---------------------------------------------
> Dr Fiona Mae Caryl
> Research Fellow
> Australian Research Centre for Urban Ecology
> School of Botany

 [snip]


From richabharti74 at gmail.com  Thu Jun 19 17:37:20 2014
From: richabharti74 at gmail.com (Richa Bharti)
Date: Thu, 19 Jun 2014 17:37:20 +0200
Subject: [R-sig-ME] lmer/pamer.fnc error
Message-ID: <CAHAAfB-dGjv-PvZJ7XvUhH25VQfsbxcU3EfpiGcq=jkUJx4cyQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140619/132ada09/attachment.pl>

From bbolker at gmail.com  Thu Jun 19 21:55:20 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 19 Jun 2014 19:55:20 +0000 (UTC)
Subject: [R-sig-ME] lmer/pamer.fnc error
References: <CAHAAfB-dGjv-PvZJ7XvUhH25VQfsbxcU3EfpiGcq=jkUJx4cyQ@mail.gmail.com>
Message-ID: <loom.20140619T200323-495@post.gmane.org>

Richa Bharti <richabharti74 at ...> writes:

> 
> *Dear R group,*
> 
> *I am doing lmer with 2 factors and cage and as random effect.*
> *
> library(lmerTest)
> library(LMERConvenienceFunctions)
> 
> lmer_null<-lmer(D1 ~ 1 + (1|Cage_Nr), data=data_m)
> lmer_full<-lmer(D1 ~ Day*Mutant+Gender+(1|Cage_Nr),
> REML=FALSE, data=data_m)
> (anova_full<-anova(lmer_null,lmer_full))
> summary(lmer_full)
> (expl.dev<-pamer.fnc(lmer_full))
> but with last command I am getting an error
> Error in pf(anova.table[term, "F value"], anova.table[term, "Df"],
> nrow(model <at> frame) -  :  
  Non-numeric argument to mathematical function

> I don't what's the reason of this error? Is my model not
> correct or could it any other reason.

  It looks like at least this part of LMERConvenienceFunctions must
not have been updated to work with new lme4.
Here's an updated version (I haven't tested carefully,
use at your own risk!)  (The package must not contain any
examples or testing of this function, or this would have failed
a while ago ...)

library(lme4)
library(LMERConvenienceFunctions)
library(lmerTest)
fm1 <- lmer(Reaction~Days+(1|Subject),sleepstudy)
fm2 <- update(fm1,.~.-Days)
print(pamer.fnc(fm1),digits=3)
##      Sum Sq Mean Sq NumDF DenDF F.value    Pr(>F) DenDF(UPPER) Pr(>F,UPPER)
## Days 162703  162703     1   161     169 7.88e-244          178     1.21e-27
##      DenDF(LOWER) Pr(>F,LOWER) expl.dev(%)
## Days          160     7.12e-27       0.286

Transform <- function (`_data`, ..., check.names=TRUE) 
{
    e <- eval(substitute(list(...)), `_data`, parent.frame())
    tags <- names(e)
    inx <- match(tags, names(`_data`))
    matched <- !is.na(inx)
    if (any(matched)) {
        `_data`[inx[matched]] <- e[matched]
        `_data` <- data.frame(`_data`,check.names=check.names)
    }
    if (!all(matched)) 
        do.call("data.frame", c(list(`_data`), e[!matched],
                list(check.names=check.names)))
    else `_data`
}


pamer.fnc <- function (model, ndigits = 4) 
{
    if (length(rownames(anova.table <- anova(model))) == 0) {
        cat("nothing to evaluate: model has only an intercept.\n\n")
        cat("printing model fixed effects:\n")
        fixef(model)
    } else {
        dims <- NULL
        rank.X = qr(getME(model,"X"))$rank
        lower.bound <- sum(sapply(ranef(model),function(x) prod(dim(x))))
        updf <- nobs(model) -  rank.X
        lodf <- updf - lower.bound
        dv <- as.character(formula(model)[[2]])
        ss.tot <- sum(scale(model.frame(model)[[dv]],scale=FALSE)^2)
        aov.table <- as.data.frame(anova(model))
        new.anova.table <-
            Transform(anova.table,
                      `Pr(>F)`=pf(`Mean Sq`,NumDF,DenDF,lower.tail=FALSE),
                      `DenDF(UPPER)` = updf,
                      `Pr(>F,UPPER)`=pf(F.value,NumDF,updf,lower.tail=FALSE),
                      `DenDF(LOWER)` = lodf,
                      `Pr(>F,LOWER)`=pf(F.value,NumDF,lodf,lower.tail=FALSE),
                      `expl.dev(%)`=aov.table[,2]/ss.tot,
                      check.names=FALSE)
        class(new.anova.table) <- class(anova.table)
        return(new.anova.table)
    }
}


From mervat_moh2006 at yahoo.com  Fri Jun 20 07:59:49 2014
From: mervat_moh2006 at yahoo.com (mervat mohamed)
Date: Thu, 19 Jun 2014 22:59:49 -0700
Subject: [R-sig-ME] simulation of grouped data
Message-ID: <1403243989.44079.YahooMailNeo@web161601.mail.bf1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140619/952b67d1/attachment.pl>

From mervat_moh2006 at yahoo.com  Fri Jun 20 08:05:26 2014
From: mervat_moh2006 at yahoo.com (mervat mohamed)
Date: Thu, 19 Jun 2014 23:05:26 -0700
Subject: [R-sig-ME] (no subject)
Message-ID: <1403244326.66301.YahooMailNeo@web161601.mail.bf1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140619/8924de21/attachment.pl>

From teplitsky at mnhn.fr  Fri Jun 20 14:39:33 2014
From: teplitsky at mnhn.fr (=?ISO-8859-1?Q?C=E9line_Teplitsky?=)
Date: Fri, 20 Jun 2014 14:39:33 +0200
Subject: [R-sig-ME] No residual variance using MCMCglmm
Message-ID: <53A42B85.4040203@mnhn.fr>

Dear all,

I have recently bumped twice in the same issue running glmm in MCMCglmm: 
the posterior distribution of residual collapses on 0. While I have 
often seen it for other effects (e.g ID) and interpreted it as evidence 
of non existence / non significance of these effects, I can not get why 
residual variance would not be well defined.

More specifically, with priors V=1, nu=0.02, I was trying to estimate 
additive genetic variance in age at first breeding. I first tried a 
Poisson distribution and the posterior distribution of the residual 
looked more or less ok, although not perfectly bell shaped. Then I 
thought as age at first breeding could not be zero, that a zero 
truncated Poisson might be better but then the posterior distribution of 
residual variance totally collapses on zero. As I thought it could be 
due to over parametrisation, I rerun the model with only intercept  but 
results were the same.

Is it a problem with the variables distributions not really fitting the 
distribution I'm specifying? Any help would be greatly appreciated!

Many thanks in advance

Celine

-- 

Celine Teplitsky
UMR 7204 - CESCO
D?partement Ecologie et Gestion de la Biodiversit?
CP 51
55 rue Buffon 75005 Paris

Webpage : http://www2.mnhn.fr/cersp/spip.php?rubrique96
Fax : (33-1)-4079-3835
Phone: (33-1)-4079-3443


From bbolker at gmail.com  Fri Jun 20 21:26:25 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 20 Jun 2014 19:26:25 +0000 (UTC)
Subject: [R-sig-ME] simulation of grouped data
References: <1403243989.44079.YahooMailNeo@web161601.mail.bf1.yahoo.com>
Message-ID: <loom.20140620T211436-334@post.gmane.org>

mervat mohamed <mervat_moh2006 at ...> writes:

> 
> Dear R group
 
> I simulate 100 data sets, each data set consists of 50000
> observations (X, Y) and 100 groups, each group contains 500
> observations (data file=indiv).  Then I use the simulated population
> to draw a sample of 25 observations from 5 groups (s.grp) (5
> observations from each group (p)) (data file=indiv.str). And ,I am
> doing lmer on the drawing sample with these two variables Y & X and
> group as random effect. I estimate the slope of the model of y on
> x. But, I have a problem which is the difference between the
> estimated slope (beta) and the real value (r.beta=0.2) is sometimes
> large and I do not know the reason.

(It would help substantially if you could format your code more nicely ...)

in any case (see end of this message), if you draw a histogram of
your results you'll see that they're approximately normally distributed
with a mean near the true value (0.2) and a s.d. of about 0.25.
If you saw occasional outliers that might suggest that the fit was
sometimes going wrong, but this result strongly suggests that everything
is exactly as expected (if you look at the estimated standard errors
of your slope parameter they should be around 0.25 as well).  The
difference is sometimes large just through the effects of stochastic
sampling.

You can probably run your simulations considerably more efficiently/
with more compact code (although since most of the computation is
taken by lmer(), it might not matter much).
                        
 The program code is as follows:


hh=100              ##  the number of simulated data sets
beta<-rep(NA,hh)    ## the estimated values of the slope of the model
for (h in 1:hh){
     n=50000  ##  the total size of the simulated population
     grp=100  ##  the number of groups of the simulated population
     s.grp=5  ##  the number of sampled groups from the simulated popualtion
     p=5      ##  the number of sampled observations 
              ##      from each group of the sampled groups
     r.beta=0.2  ## the real value of beta 
     x=c(0)
     y=c(0)
     group=c(0)
     e=c(0)   ##  the error term within each group
     u=c(0)  ## the error term between the groups
     for (j in 1:100){
          u[j] <- rnorm (1,0,1)
          for (i in 1:500){
              group[i+j*500-500]=j
              e[i+j*500-500]<-rnorm(1,0,1)
              x[i+j*500-500]=rnorm(1,3,1)
              y[i+j*500-500]=2+b*x[i+j*500-500]+e[i+j*500-500]+u[j]
           }
     }
     indiv<- data.frame(group,x,y)
     s.id<-sample(1:100,s.grp)
     str.rows<-c(0)
     r1<-c(0)
     r2<-c(0)
     for (j in 1:s.grp){
         r2[j]=500*s.id[j]
         r1[j]=r2[j]-500+1
         for (i in 1:p){
             str.rows[i+j*p-p] <-c(sample(r1[j]:r2[j],1))
       }
     }
     str <-indiv[str.rows,]
     indiv.str=data.frame(str$group,str$x,str$y)
     names(indiv.str)<-c("group","x","y")
     fcn=lmer(y~x+(1|group),data=indiv.str)
     beta[h]<-fixef(fcn)["x"]
   }
 
## results reformatted
res <- c(0.231995519,-0.188624285,0.018167679,0.053498977,0.534498744,
-0.066925736,0.080323888,0.386649648,0.516902831,0.515848003,
0.203306100,-0.068883270,-0.267411862,0.390552183,0.276073648,
0.007330362,0.104371287,-0.160514069,-0.195189352,0.387713063,
0.184829681,0.144895545,0.631665413,0.646678912,0.015387116,
-0.192252053,0.181388009,0.171305460,-0.015506910,-0.058342395,
0.507789283,0.169497841,-0.307642411,0.523006541,0.404130042,
0.195879055,0.229919059,-0.069286463,0.447006382,0.244852676,
-0.186131780,0.308434630,0.178955713,0.043044122,-0.095523847,
0.165376798,0.638022903,0.264380374,-0.054814189,0.022157603,
0.219759274,-0.034772547,0.758598418,0.052701825,0.439542452,
0.130938211,0.025220602,0.183105063,0.125483056,0.365605925,
-0.105069737,-0.007282673,-0.140160646,0.415598284,0.073348887,
-0.055605028,0.238275730,-0.052132121,0.063960296,0.085801721,
0.093794361,-0.204151040,-0.159957827,0.351359718,0.390467192,
0.683148716,0.382928456,0.174347587,0.430329912,0.202229216,
0.397045769,0.305609822,0.125820851,0.164247981,0.253652445,
0.237743071,0.219102084,0.474830159,-0.181714623,0.384835726,
0.361376723,0.032047569,0.097940523,0.287583642,0.572813309,
0.338947802,-0.030983428,0.334985479,0.533231095,0.283882400)

par(las=1)
hist(sort(res),main="",col="gray")
abline(v=0.2,col=2,lwd=2)


From jonlopez.research at gmail.com  Sat Jun 21 05:38:13 2014
From: jonlopez.research at gmail.com (Jon Lopez)
Date: Fri, 20 Jun 2014 20:38:13 -0700
Subject: [R-sig-ME] How can I estimate deviance explained of a mixed gamm?
Message-ID: <CAPypr8qVZ39+h=+kfD3CipkNkGQNC=tqjdw2YiErTGzfMx_6cg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140620/f216c04b/attachment.pl>

From jdb at uw.edu  Fri Jun 20 23:26:56 2014
From: jdb at uw.edu (Jonathon D. Brown)
Date: Fri, 20 Jun 2014 14:26:56 -0700
Subject: [R-sig-ME] Henderson iterations for random slopes
Message-ID: <001d01cf8cce$5d64c330$182e4990$@uw.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140620/76126969/attachment.pl>

From bates at stat.wisc.edu  Sat Jun 21 18:24:36 2014
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 21 Jun 2014 11:24:36 -0500
Subject: [R-sig-ME] Henderson iterations for random slopes
In-Reply-To: <001d01cf8cce$5d64c330$182e4990$@uw.edu>
References: <001d01cf8cce$5d64c330$182e4990$@uw.edu>
Message-ID: <CAO7JsnR7S8+bbobJBJFwDTMu4sO5e95L4YZNPZNoXwq8bhi8sQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140621/9307e1f0/attachment.pl>

From rf371 at cam.ac.uk  Sun Jun 22 21:15:14 2014
From: rf371 at cam.ac.uk (Roxane Foulser-Piggott)
Date: Sun, 22 Jun 2014 20:15:14 +0100
Subject: [R-sig-ME] Convert nlme formula to nlmer
References: <loom.20140620T210550-815@post.gmane.org>
Message-ID: <6317E161-EEED-4E12-9241-96D93527FC8C@cam.ac.uk>

I posted this query to the R-help list and was advised to try this mailing list:
R version 3.1.0 (2014-04-10) -- "Spring Dance?. Platform: x86_64-apple-darwin13.1.0 (64-bit).  

I would like to convert the following function from nlme to nlmer. I am finding it difficult to apply the documentation I can find on this procedure to this problem.  The function predicts earthquake ground-shaking (lnIa) as a function of M, Rrup, Ztor, Fevent, Vs30 and e1.  I am converting to nlmer, as once I have it running, I will explore crossed and nested mixed effects.

eq1 <- nlme( lnIa ~ c0 + m1*(M-5) + (r1a + r1b*M)*log(sqrt(Rrup^2 + Ztor^2)) + f1*Fevent + v1*log(Vs30/760)  + e1,fixed=c0+m1+r1a+r1b+f1+v1~1,random=e1~1|Event,data = data, start = c(c0=11,m1=-0.9,r1a=0.5,r1b=-0.52,f1=0.9,v1=-1.32))

Many thanks,
Roxane.


From bbolker at gmail.com  Sun Jun 22 21:20:00 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 22 Jun 2014 15:20:00 -0400
Subject: [R-sig-ME] Convert nlme formula to nlmer
In-Reply-To: <6317E161-EEED-4E12-9241-96D93527FC8C@cam.ac.uk>
References: <loom.20140620T210550-815@post.gmane.org>
	<6317E161-EEED-4E12-9241-96D93527FC8C@cam.ac.uk>
Message-ID: <53A72C60.7030706@gmail.com>

On 14-06-22 03:15 PM, Roxane Foulser-Piggott wrote:
> I posted this query to the R-help list and was advised to try this
> mailing list: R version 3.1.0 (2014-04-10) -- "Spring Dance?.
> Platform: x86_64-apple-darwin13.1.0 (64-bit).
> 
> I would like to convert the following function from nlme to nlmer. I
> am finding it difficult to apply the documentation I can find on this
> procedure to this problem.  The function predicts earthquake
> ground-shaking (lnIa) as a function of M, Rrup, Ztor, Fevent, Vs30
> and e1.  I am converting to nlmer, as once I have it running, I will
> explore crossed and nested mixed effects.
> 
> eq1 <- nlme( lnIa ~ c0 + m1*(M-5) + (r1a + r1b*M)*log(sqrt(Rrup^2 +
> Ztor^2)) + f1*Fevent + v1*log(Vs30/760)  +
> e1,fixed=c0+m1+r1a+r1b+f1+v1~1,random=e1~1|Event,data = data, start =
> c(c0=11,m1=-0.9,r1a=0.5,r1b=-0.52,f1=0.9,v1=-1.32))
> 
> Many thanks, Roxane.

 Did you look at http://rpubs.com/bbolker/3423 yet ?

 Ben Bolker


From bbolker at gmail.com  Mon Jun 23 15:27:17 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 23 Jun 2014 09:27:17 -0400
Subject: [R-sig-ME] upcoming release of lme4 1.1-7
Message-ID: <53A82B35.40609@mcmaster.ca>

   We are on the verge of submitting a new release of lme4 to CRAN.
Probably the most interesting aspect of this release from the end-user
perspective is that we have improved the convergence tests (following
much discussion on this list) so that we no longer expect
false-positive max|grad| warnings even on large problems.  Other than
that, this is largely a bug-fix release.  (We have added some
trouble-shooting information to the ?lmerControl help page, and hope
to expand this section in future releases.)

  We have not changed any of the optimization behaviour, but we are
strongly considering moving to using the nloptr implementation of
BOBYQA as the default optimizer in future versions (lme4 now Imports
the nloptr package).

  We don't expect any compatibility problems with downstream packages,
but as always it would be a good idea for downstream package
maintainers to test with the new version (from Github).

  sincerely
    Ben Bolker, on behalf of the lme4 maintainers

===========
CHANGES IN VERSION 1.1-7:

  NEW FEATURES:

         ? the ?nloptr? package is now imported; a wrapper function
           (?nloptwrap?) is provided so that
           ?lmerControl(optimizer="nloptwrap")? is all that's necessary
           to use ?nloptr? optimizers in the nonlinear optimization
           stage (the default algorithm is NLopt's implementation of
           BOBYQA: see ??nloptwrap? for examples)

         ? preliminary implementation of checks for scaling of model
           matrix columns (see ?check.scaleX? in ??lmerControl?)

         ? ?beta? is now allowed as a synonym for ?fixef? when
           specifying starting parameters (Github #194)

  USER-VISIBLE CHANGES:

         ? the use of ?deviance? to return the REML criterion is now
           deprecated; users should use ?REMLcrit()? instead (Github
           #211)

         ? changed the default value of ?check.nobs.vs.rankZ? to
           ?"ignore"? (Github #214)

  BUG FIXES:

         ? change gradient testing from absolute to relative

         ? fix ?confint(.,method="boot")? to allow/work properly with
           ?boot.type? values other than ?"perc"? (reported by Alan
           Zaslavsky)

         ? allow ?plot()? to work when data are specified in a
           different environment (reported by Dieter Menne)

         ? ?predict? and ?simulate? work for matrix-valued predictors
           (Github #201)

         ? other ?simulate? bugs (Github #212)

         ? ?predict? no longer warns spuriously when original response
           was a factor (Github #205)

         ? fix memory access issues (Github #200)


From j.hadfield at ed.ac.uk  Mon Jun 23 21:22:13 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 23 Jun 2014 20:22:13 +0100
Subject: [R-sig-ME] No residual variance using MCMCglmm
In-Reply-To: <53A42B85.4040203@mnhn.fr>
References: <53A42B85.4040203@mnhn.fr>
Message-ID: <20140623202213.236654ouzfcuqigw@www.staffmail.ed.ac.uk>

Hi C?line,

Zero residual variance with (truncated) Poisson response would imply  
that the data are under-dispersed with respect to the (truncated)  
Poisson model. You could check this by comparing the variance of the  
data with the expected variance given the intercept.


Cheers,

Jarrod



Quoting C?line Teplitsky <teplitsky at mnhn.fr> on Fri, 20 Jun 2014  
14:39:33 +0200:

> Dear all,
>
> I have recently bumped twice in the same issue running glmm in  
> MCMCglmm: the posterior distribution of residual collapses on 0.  
> While I have often seen it for other effects (e.g ID) and  
> interpreted it as evidence of non existence / non significance of  
> these effects, I can not get why residual variance would not be well  
> defined.
>
> More specifically, with priors V=1, nu=0.02, I was trying to  
> estimate additive genetic variance in age at first breeding. I first  
> tried a Poisson distribution and the posterior distribution of the  
> residual looked more or less ok, although not perfectly bell shaped.  
> Then I thought as age at first breeding could not be zero, that a  
> zero truncated Poisson might be better but then the posterior  
> distribution of residual variance totally collapses on zero. As I  
> thought it could be due to over parametrisation, I rerun the model  
> with only intercept  but results were the same.
>
> Is it a problem with the variables distributions not really fitting  
> the distribution I'm specifying? Any help would be greatly  
> appreciated!
>
> Many thanks in advance
>
> Celine
>
> -- 
>
> Celine Teplitsky
> UMR 7204 - CESCO
> D?partement Ecologie et Gestion de la Biodiversit?
> CP 51
> 55 rue Buffon 75005 Paris
>
> Webpage : http://www2.mnhn.fr/cersp/spip.php?rubrique96
> Fax : (33-1)-4079-3835
> Phone: (33-1)-4079-3443
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From spatrick at glos.ac.uk  Tue Jun 24 13:51:53 2014
From: spatrick at glos.ac.uk (PATRICK, Samantha)
Date: Tue, 24 Jun 2014 11:51:53 +0000
Subject: [R-sig-ME] MCMCglmm error message:Error in `[.data.frame undefined
 columns selected
Message-ID: <89A06A313CF21543AC6BE0F5AACCE6872E03CCB2@C18760.chelt.local>

Hi

I am having problems with an error message in MCMCglmm.  I am trying to partition the variance in the model so that each group has its own estimated variance.  Originally I had a more complicated model but I have simplified it below to show the source of the problem (so the prior is not optimised for this model and only part of the data set is given).  If I fit random = ~GroupA, it runs ok, but if I add in ~us (GroupA):units it gives the error:

Error in `[.data.frame`(data, , components$rterms[select.terms]) :
undefined columns selected

Previous posts have suggested it may be because the variables do not exist but it is not this.  I have tried other variants but I keep getting the same error.  I have changed all the variable names in case there was a double with a function but still no luck.

Does anyone have any ideas? The same error occurs if you just try to fit the rcov statement (with units).  I thought there could be a problem estimating the units term perhaps and so it can not find the residual variance.  But if I try other terms in its place I get the same error

Thanks

Sam

Script
library(MCMCglmm)

prior2.1 <- list(R = list(V= 1, nu =0.002), G = list(G1 = list(V =diag(4), nu = 1.002)))

mod2.1<- MCMCglmm(DistanceA ~FixedA, random = ~us(GroupA):units,
# rcov = ~idh(GroupA):units,
prior = prior2.1,
data=h,
pr=TRUE,  burnin = 200, nitt = 5000000, thin = 200)




Data

IDA     DistanceA       FixedA  GroupA
1       35.76203413     12      A
1       353.8044206     26      A
1       28.6894637      14      A
1       302.9190056     16      A
1       308.7219701     23      A
2       173.9284996     111     B
2       231.2895313     37      B
2       223.0156577     11      B
2       257.7869899     27      B
3       308.3258686     110     A
3       178.3784977     37      A
3       218.3712172     15      A
4       288.5865862     65      B
5       54.62104181     20      A
5       310.5786671     118     A
5       185.3627266     18      A
5       300.8304258     51      A
6       461.4315924     75      B
6       496.403949      52      B
7       708.2367263     181     B
7       214.8704587     35      B
8       183.6161529     44      A
8       183.326292      21      A
8       319.0479119     39      A
9       269.6319359     79      B
9       325.1318986     98      B
9       399.208417      99      B
10      135.2138385     50      B
10      162.9959776     67      B
11      49.05385099     17      A
11      80.92750054     26      A
11      184.7205464     16      A
12      327.6768626     47      A
29      240.6830677     85      C
29      126.9140906     23      C
29      216.4021125     29      C
29      168.8125168     52      C
30      230.8613452     25      D
30      134.8893388     38      D
30      164.2181205     48      D
30      140.8627105     55      D
31      15.51295239     6       D
31      42.45851533     3       D
31      16.30773693     4       D
31      89.57289202     20      D
31      108.0256345     115     D
32      118.2199077     12      D
32      247.9492249     26      D
32      18.20298953     15      D
32      70.97933919     20      D
32      109.7264855     36      D
32      101.8653273     19      D
32      68.54583069     7       D
33      24.1465819      7       C
33      67.33233486     75      C
33      64.92951246     31      C
33      94.90538004     53      C
34      474.3625341     171     C
54      150.6926733     43      D
54      191.6157478     58      D
54      127.1248342     14      D
55      385.2042737     42      D
55      202.8058375     23      D
55      144.4546654     44      D
56      194.0171206     81      D
56      163.6960289     40      D
57      122.5337279     89      D
57      172.2405156     44      D
57      206.4998878     79      D
57      122.677563      19      D
58      147.3699167     31      D
59      140.7191172     84      D
59      241.2723386     50      D
59      230.1874477     83      D
60      130.6675375     101     D
60      104.4518299     9       D
60      134.473374      56      D
61      135.7597774     17      D
61      232.3798885     40      D
62      164.1352534     65      D
62      259.1312989     28      D
63      177.8687393     145     C
63      121.3886107     43      C
64      202.009693      43      D
64      218.7044853     41      D

Dr Samantha Patrick
Research Fellow
Biosciences QU116
Francis Close Hall Campus
University of Gloucestershire
Cheltenham
GL50 4AZ
UK

Research Associate: ABRG, University of Oxford

Tel : +44 1242 714 668
Skype: sammy_patrick

________________________________________
-
?In the top 5 in the Green League Table; committed to sustainability?
This email is confidential to the intended recipient. If you have received it in error please notify the sender and delete it from your computer.
The University of Gloucestershire is a company limited by guarantee registered in England and Wales.  Registered number: 06023243.  Registered office: The Park, Cheltenham, GL50 2RH
Please consider the environment before printing this email.
-

From anainesvs at gmail.com  Tue Jun 24 07:40:00 2014
From: anainesvs at gmail.com (Ana Ines Vazquez)
Date: Tue, 24 Jun 2014 05:40:00 +0000 (UTC)
Subject: [R-sig-ME] pedigreemm number of levels per grouping factor
References: <DBBDA54CB159CC41A0A01219B109A4D001BBD248@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk><52F28469.9010203@gmail.com>
	<DBBDA54CB159CC41A0A01219B109A4D001BBF06C@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>
	<alpine.LMD.2.00.1402071324230.9819@orpheus.qimr.edu.au>
	<DBBDA54CB159CC41A0A01219B109A4D001BBF2C1@VMEXCHANGEMBS5A.isad.isadroot.ex.ac.uk>
	<alpine.LMD.2.00.1402081824430.19877@orpheus.qimr.edu.au>
	<loom.20140210T033440-585@post.gmane.org>
Message-ID: <loom.20140624T073356-763@post.gmane.org>

HELLO, 
I fixed the problem in relfactor(), pedigreemm package. Thank you for
letting me know of the bug. The new code is in the R-Forge (version 153):
r-forge.r-project.org/R/?group_id=159
I will submit to CRAN in the coming days. 
Ana Ines Vazquez.


From mb334 at st-andrews.ac.uk  Mon Jun 23 09:17:49 2014
From: mb334 at st-andrews.ac.uk (Miguel Barbosa)
Date: Mon, 23 Jun 2014 07:17:49 +0000 (UTC)
Subject: [R-sig-ME] glmmADMB does not run
Message-ID: <loom.20140623T091002-379@post.gmane.org>

Hi there, 
I am trying to install the glmmADMB package but I get the message 
#is not available (for R version 3.0.2). 
I've tried installing it on a older R version 
but get the same message (albeit not with 3.0.2). 

Is there anyone out there that can tell me the R version
 in which glmmADMB package will work? I am 
using a Mac 10.9.3.

thanks in advance

Miguel


From romunov at gmail.com  Tue Jun 24 15:03:47 2014
From: romunov at gmail.com (romunov)
Date: Tue, 24 Jun 2014 15:03:47 +0200
Subject: [R-sig-ME] glmmADMB does not run
In-Reply-To: <loom.20140623T091002-379@post.gmane.org>
References: <loom.20140623T091002-379@post.gmane.org>
Message-ID: <CAHT1vpg3SnVysPXEA-5F554_9FdnAnh5PSkVH=DVKe=ovwu42A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140624/6574b760/attachment.pl>

From neilandertal at gmail.com  Tue Jun 24 15:04:58 2014
From: neilandertal at gmail.com (Neil Collier)
Date: Tue, 24 Jun 2014 15:04:58 +0200
Subject: [R-sig-ME] glmmADMB does not run
In-Reply-To: <loom.20140623T091002-379@post.gmane.org>
References: <loom.20140623T091002-379@post.gmane.org>
Message-ID: <CAOVghqpi3kYtF4dNpNONa8Twq9N7e5TTGb2u3Cv5wXXtMwrt0g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140624/4db18b97/attachment.pl>

From asafw.at.wharton at gmail.com  Tue Jun 24 15:12:33 2014
From: asafw.at.wharton at gmail.com (Asaf Weinstein)
Date: Tue, 24 Jun 2014 09:12:33 -0400
Subject: [R-sig-ME] Obtaining X beta + Zb
Message-ID: <CAGG0PdCvJG24oOmtCyFVy8kS+jEjwrMSS1NC0hTcW-hz89yNhg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140624/d516e35f/attachment.pl>

From j.hadfield at ed.ac.uk  Tue Jun 24 15:25:36 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 24 Jun 2014 14:25:36 +0100
Subject: [R-sig-ME] MCMCglmm error message:Error in `[.data.frame
 undefined columns selected
In-Reply-To: <89A06A313CF21543AC6BE0F5AACCE6872E03CCB2@C18760.chelt.local>
References: <89A06A313CF21543AC6BE0F5AACCE6872E03CCB2@C18760.chelt.local>
Message-ID: <20140624142536.14788j3776tnmx5c@www.staffmail.ed.ac.uk>

Hi Sam,

Which version of MCMCglmm are you using, and do you get the same error  
with the newest version?

Also, the covariances in us(GroupA):units cannot be estimated - so  
idh(GroupA):units is more appropriate (as an rcov term).

Cheers,

Jarrod


Quoting "PATRICK, Samantha" <spatrick at glos.ac.uk> on Tue, 24 Jun 2014  
11:51:53 +0000:

> Hi
>
> I am having problems with an error message in MCMCglmm.  I am trying  
> to partition the variance in the model so that each group has its  
> own estimated variance.  Originally I had a more complicated model  
> but I have simplified it below to show the source of the problem (so  
> the prior is not optimised for this model and only part of the data  
> set is given).  If I fit random = ~GroupA, it runs ok, but if I add  
> in ~us (GroupA):units it gives the error:
>
> Error in `[.data.frame`(data, , components$rterms[select.terms]) :
> undefined columns selected
>
> Previous posts have suggested it may be because the variables do not  
> exist but it is not this.  I have tried other variants but I keep  
> getting the same error.  I have changed all the variable names in  
> case there was a double with a function but still no luck.
>
> Does anyone have any ideas? The same error occurs if you just try to  
> fit the rcov statement (with units).  I thought there could be a  
> problem estimating the units term perhaps and so it can not find the  
> residual variance.  But if I try other terms in its place I get the  
> same error
>
> Thanks
>
> Sam
>
> Script
> library(MCMCglmm)
>
> prior2.1 <- list(R = list(V= 1, nu =0.002), G = list(G1 = list(V  
> =diag(4), nu = 1.002)))
>
> mod2.1<- MCMCglmm(DistanceA ~FixedA, random = ~us(GroupA):units,
> # rcov = ~idh(GroupA):units,
> prior = prior2.1,
> data=h,
> pr=TRUE,  burnin = 200, nitt = 5000000, thin = 200)
>
>
>
>
> Data
>
> IDA     DistanceA       FixedA  GroupA
> 1       35.76203413     12      A
> 1       353.8044206     26      A
> 1       28.6894637      14      A
> 1       302.9190056     16      A
> 1       308.7219701     23      A
> 2       173.9284996     111     B
> 2       231.2895313     37      B
> 2       223.0156577     11      B
> 2       257.7869899     27      B
> 3       308.3258686     110     A
> 3       178.3784977     37      A
> 3       218.3712172     15      A
> 4       288.5865862     65      B
> 5       54.62104181     20      A
> 5       310.5786671     118     A
> 5       185.3627266     18      A
> 5       300.8304258     51      A
> 6       461.4315924     75      B
> 6       496.403949      52      B
> 7       708.2367263     181     B
> 7       214.8704587     35      B
> 8       183.6161529     44      A
> 8       183.326292      21      A
> 8       319.0479119     39      A
> 9       269.6319359     79      B
> 9       325.1318986     98      B
> 9       399.208417      99      B
> 10      135.2138385     50      B
> 10      162.9959776     67      B
> 11      49.05385099     17      A
> 11      80.92750054     26      A
> 11      184.7205464     16      A
> 12      327.6768626     47      A
> 29      240.6830677     85      C
> 29      126.9140906     23      C
> 29      216.4021125     29      C
> 29      168.8125168     52      C
> 30      230.8613452     25      D
> 30      134.8893388     38      D
> 30      164.2181205     48      D
> 30      140.8627105     55      D
> 31      15.51295239     6       D
> 31      42.45851533     3       D
> 31      16.30773693     4       D
> 31      89.57289202     20      D
> 31      108.0256345     115     D
> 32      118.2199077     12      D
> 32      247.9492249     26      D
> 32      18.20298953     15      D
> 32      70.97933919     20      D
> 32      109.7264855     36      D
> 32      101.8653273     19      D
> 32      68.54583069     7       D
> 33      24.1465819      7       C
> 33      67.33233486     75      C
> 33      64.92951246     31      C
> 33      94.90538004     53      C
> 34      474.3625341     171     C
> 54      150.6926733     43      D
> 54      191.6157478     58      D
> 54      127.1248342     14      D
> 55      385.2042737     42      D
> 55      202.8058375     23      D
> 55      144.4546654     44      D
> 56      194.0171206     81      D
> 56      163.6960289     40      D
> 57      122.5337279     89      D
> 57      172.2405156     44      D
> 57      206.4998878     79      D
> 57      122.677563      19      D
> 58      147.3699167     31      D
> 59      140.7191172     84      D
> 59      241.2723386     50      D
> 59      230.1874477     83      D
> 60      130.6675375     101     D
> 60      104.4518299     9       D
> 60      134.473374      56      D
> 61      135.7597774     17      D
> 61      232.3798885     40      D
> 62      164.1352534     65      D
> 62      259.1312989     28      D
> 63      177.8687393     145     C
> 63      121.3886107     43      C
> 64      202.009693      43      D
> 64      218.7044853     41      D
>
> Dr Samantha Patrick
> Research Fellow
> Biosciences QU116
> Francis Close Hall Campus
> University of Gloucestershire
> Cheltenham
> GL50 4AZ
> UK
>
> Research Associate: ABRG, University of Oxford
>
> Tel : +44 1242 714 668
> Skype: sammy_patrick
>
> ________________________________________
> -
> ?In the top 5 in the Green League Table; committed to sustainability?
> This email is confidential to the intended recipient. If you have  
> received it in error please notify the sender and delete it from  
> your computer.
> The University of Gloucestershire is a company limited by guarantee  
> registered in England and Wales.  Registered number: 06023243.   
> Registered office: The Park, Cheltenham, GL50 2RH
> Please consider the environment before printing this email.
> -
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From Thierry.ONKELINX at inbo.be  Tue Jun 24 15:25:34 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 24 Jun 2014 13:25:34 +0000
Subject: [R-sig-ME] Obtaining X beta + Zb
In-Reply-To: <CAGG0PdCvJG24oOmtCyFVy8kS+jEjwrMSS1NC0hTcW-hz89yNhg@mail.gmail.com>
References: <CAGG0PdCvJG24oOmtCyFVy8kS+jEjwrMSS1NC0hTcW-hz89yNhg@mail.gmail.com>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3AADCA9@inbomail.inbo.be>

Dear Asaf,

Does fitted() gives you what you want?

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Asaf Weinstein
Verzonden: dinsdag 24 juni 2014 15:13
Aan: r-sig-mixed-models
Onderwerp: [R-sig-ME] Obtaining X beta + Zb

Hello,

I wonder if there is a direct way to obtain the (BLUP of the) linear predictor, X beta + Zb, without having to first extract the (BLUP of the) random effects (with function ranef).

Thanks so much,
Asaf

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From bbolker at gmail.com  Tue Jun 24 15:27:22 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 24 Jun 2014 13:27:22 +0000 (UTC)
Subject: [R-sig-ME] Obtaining X beta + Zb
References: <CAGG0PdCvJG24oOmtCyFVy8kS+jEjwrMSS1NC0hTcW-hz89yNhg@mail.gmail.com>
Message-ID: <loom.20140624T152650-262@post.gmane.org>

Asaf Weinstein <asafw.at.wharton at ...> writes:

> 
> Hello,
> 
> I wonder if there is a direct way to obtain the (BLUP of the) linear
> predictor, X beta + Zb, without having to first extract the (BLUP of the)
> random effects (with function ranef).

  Isn't this what predict(model) or fitted(model) does?


From steve.walker at utoronto.ca  Tue Jun 24 15:31:01 2014
From: steve.walker at utoronto.ca (Steve Walker)
Date: Tue, 24 Jun 2014 09:31:01 -0400
Subject: [R-sig-ME] Obtaining X beta + Zb
In-Reply-To: <CAGG0PdCvJG24oOmtCyFVy8kS+jEjwrMSS1NC0hTcW-hz89yNhg@mail.gmail.com>
References: <CAGG0PdCvJG24oOmtCyFVy8kS+jEjwrMSS1NC0hTcW-hz89yNhg@mail.gmail.com>
Message-ID: <53A97D95.9030600@utoronto.ca>

I'm assuming you are referring to linear mixed models (i.e. lmer):

fm <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
fitted.values(fm)

Cheers,
Steve


On 2014-06-24, 9:12 AM, Asaf Weinstein wrote:
> Hello,
>
> I wonder if there is a direct way to obtain the (BLUP of the) linear
> predictor, X beta + Zb, without having to first extract the (BLUP of the)
> random effects (with function ranef).
>
> Thanks so much,
> Asaf
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


From spatrick at glos.ac.uk  Tue Jun 24 16:23:02 2014
From: spatrick at glos.ac.uk (PATRICK, Samantha)
Date: Tue, 24 Jun 2014 14:23:02 +0000
Subject: [R-sig-ME] MCMCglmm error message:Error in `[.data.frame
 undefined columns selected
In-Reply-To: <20140624142536.14788j3776tnmx5c@www.staffmail.ed.ac.uk>
References: <89A06A313CF21543AC6BE0F5AACCE6872E03CCB2@C18760.chelt.local>,
	<20140624142536.14788j3776tnmx5c@www.staffmail.ed.ac.uk>
Message-ID: <89A06A313CF21543AC6BE0F5AACCE6872E03CD08@C18760.chelt.local>

Hi Jarrod

Argh, sorry it works ok with the new version.

Apologies!!

Sam

Dr Samantha Patrick
Research Fellow
Biosciences QU116
Francis Close Hall Campus
University of Gloucestershire
Cheltenham
GL50 4AZ
UK

Research Associate: ABRG, University of Oxford

Tel : +44 1242 714 668
Skype: sammy_patrick

________________________________________
From: Jarrod Hadfield [j.hadfield at ed.ac.uk]
Sent: 24 June 2014 14:25
To: PATRICK, Samantha
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] MCMCglmm error message:Error in `[.data.frame undefined columns selected

Hi Sam,

Which version of MCMCglmm are you using, and do you get the same error
with the newest version?

Also, the covariances in us(GroupA):units cannot be estimated - so
idh(GroupA):units is more appropriate (as an rcov term).

Cheers,

Jarrod


Quoting "PATRICK, Samantha" <spatrick at glos.ac.uk> on Tue, 24 Jun 2014
11:51:53 +0000:

> Hi
>
> I am having problems with an error message in MCMCglmm.  I am trying
> to partition the variance in the model so that each group has its
> own estimated variance.  Originally I had a more complicated model
> but I have simplified it below to show the source of the problem (so
> the prior is not optimised for this model and only part of the data
> set is given).  If I fit random = ~GroupA, it runs ok, but if I add
> in ~us (GroupA):units it gives the error:
>
> Error in `[.data.frame`(data, , components$rterms[select.terms]) :
> undefined columns selected
>
> Previous posts have suggested it may be because the variables do not
> exist but it is not this.  I have tried other variants but I keep
> getting the same error.  I have changed all the variable names in
> case there was a double with a function but still no luck.
>
> Does anyone have any ideas? The same error occurs if you just try to
> fit the rcov statement (with units).  I thought there could be a
> problem estimating the units term perhaps and so it can not find the
> residual variance.  But if I try other terms in its place I get the
> same error
>
> Thanks
>
> Sam
>
> Script
> library(MCMCglmm)
>
> prior2.1 <- list(R = list(V= 1, nu =0.002), G = list(G1 = list(V
> =diag(4), nu = 1.002)))
>
> mod2.1<- MCMCglmm(DistanceA ~FixedA, random = ~us(GroupA):units,
> # rcov = ~idh(GroupA):units,
> prior = prior2.1,
> data=h,
> pr=TRUE,  burnin = 200, nitt = 5000000, thin = 200)
>
>
>
>
> Data
>
> IDA     DistanceA       FixedA  GroupA
> 1       35.76203413     12      A
> 1       353.8044206     26      A
> 1       28.6894637      14      A
> 1       302.9190056     16      A
> 1       308.7219701     23      A
> 2       173.9284996     111     B
> 2       231.2895313     37      B
> 2       223.0156577     11      B
> 2       257.7869899     27      B
> 3       308.3258686     110     A
> 3       178.3784977     37      A
> 3       218.3712172     15      A
> 4       288.5865862     65      B
> 5       54.62104181     20      A
> 5       310.5786671     118     A
> 5       185.3627266     18      A
> 5       300.8304258     51      A
> 6       461.4315924     75      B
> 6       496.403949      52      B
> 7       708.2367263     181     B
> 7       214.8704587     35      B
> 8       183.6161529     44      A
> 8       183.326292      21      A
> 8       319.0479119     39      A
> 9       269.6319359     79      B
> 9       325.1318986     98      B
> 9       399.208417      99      B
> 10      135.2138385     50      B
> 10      162.9959776     67      B
> 11      49.05385099     17      A
> 11      80.92750054     26      A
> 11      184.7205464     16      A
> 12      327.6768626     47      A
> 29      240.6830677     85      C
> 29      126.9140906     23      C
> 29      216.4021125     29      C
> 29      168.8125168     52      C
> 30      230.8613452     25      D
> 30      134.8893388     38      D
> 30      164.2181205     48      D
> 30      140.8627105     55      D
> 31      15.51295239     6       D
> 31      42.45851533     3       D
> 31      16.30773693     4       D
> 31      89.57289202     20      D
> 31      108.0256345     115     D
> 32      118.2199077     12      D
> 32      247.9492249     26      D
> 32      18.20298953     15      D
> 32      70.97933919     20      D
> 32      109.7264855     36      D
> 32      101.8653273     19      D
> 32      68.54583069     7       D
> 33      24.1465819      7       C
> 33      67.33233486     75      C
> 33      64.92951246     31      C
> 33      94.90538004     53      C
> 34      474.3625341     171     C
> 54      150.6926733     43      D
> 54      191.6157478     58      D
> 54      127.1248342     14      D
> 55      385.2042737     42      D
> 55      202.8058375     23      D
> 55      144.4546654     44      D
> 56      194.0171206     81      D
> 56      163.6960289     40      D
> 57      122.5337279     89      D
> 57      172.2405156     44      D
> 57      206.4998878     79      D
> 57      122.677563      19      D
> 58      147.3699167     31      D
> 59      140.7191172     84      D
> 59      241.2723386     50      D
> 59      230.1874477     83      D
> 60      130.6675375     101     D
> 60      104.4518299     9       D
> 60      134.473374      56      D
> 61      135.7597774     17      D
> 61      232.3798885     40      D
> 62      164.1352534     65      D
> 62      259.1312989     28      D
> 63      177.8687393     145     C
> 63      121.3886107     43      C
> 64      202.009693      43      D
> 64      218.7044853     41      D
>
> Dr Samantha Patrick
> Research Fellow
> Biosciences QU116
> Francis Close Hall Campus
> University of Gloucestershire
> Cheltenham
> GL50 4AZ
> UK
>
> Research Associate: ABRG, University of Oxford
>
> Tel : +44 1242 714 668
> Skype: sammy_patrick
>
> ________________________________________
> -
> ?In the top 5 in the Green League Table; committed to sustainability?
> This email is confidential to the intended recipient. If you have
> received it in error please notify the sender and delete it from
> your computer.
> The University of Gloucestershire is a company limited by guarantee
> registered in England and Wales.  Registered number: 06023243.
> Registered office: The Park, Cheltenham, GL50 2RH
> Please consider the environment before printing this email.
> -
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



--
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


-
?In the top 5 in the Green League Table; committed to sustainability?
This email is confidential to the intended recipient. If you have received it in error please notify the sender and delete it from your computer.
The University of Gloucestershire is a company limited by guarantee registered in England and Wales.  Registered number: 06023243.  Registered office: The Park, Cheltenham, GL50 2RH
Please consider the environment before printing this email.
-


From oneil.shawnt at gmail.com  Tue Jun 24 22:37:12 2014
From: oneil.shawnt at gmail.com (Shawn O'Neil)
Date: Tue, 24 Jun 2014 16:37:12 -0400
Subject: [R-sig-ME] Problems fitting a hurdle model using glmmADMB
Message-ID: <CAHJOu+r7-svR-p3vYB3eD0s=G43k+V_WqzcxsNjCtkeVuR=1rA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140624/eb807ddf/attachment.pl>

From joseph.bulbulia at gmail.com  Tue Jun 24 23:42:24 2014
From: joseph.bulbulia at gmail.com (josephbulbulia)
Date: Tue, 24 Jun 2014 23:42:24 +0200
Subject: [R-sig-ME] glmmADMB does not run
In-Reply-To: <CAOVghqpi3kYtF4dNpNONa8Twq9N7e5TTGb2u3Cv5wXXtMwrt0g@mail.gmail.com>
References: <loom.20140623T091002-379@post.gmane.org>
	<CAOVghqpi3kYtF4dNpNONa8Twq9N7e5TTGb2u3Cv5wXXtMwrt0g@mail.gmail.com>
Message-ID: <600C8BF0-0D31-4EF3-8F48-8C42F8567561@gmail.com>

Hi Neil, try:


install.packages("glmmADMB",repos=c("http://glmmadmb.r-forge.r-project.org/repos", getOption("repos")),type="source")




++++++++++++++++++++

Joseph Bulbulia
joseph.bulbulia at vuw.ac.nz
(+64) 21 95 94 23

Reader
Faculty of Humanities and Social Sciences
Victoria University
PO Box 600
Wellington, NZ, 6140





On 24/06/2014, at 3:04 pm, Neil Collier <neilandertal at gmail.com> wrote:

> Miguel,
> 
> I am running it fine on a Mac 10.9.2
> 
> Please post your code so that we can check for errors. That message
> sometimes returns because the incorrect name is entered. I've seen this
> happen many times.
> 
> Neil.
> 
> 
> On Mon, Jun 23, 2014 at 9:17 AM, Miguel Barbosa <mb334 at st-andrews.ac.uk>
> wrote:
> 
>> Hi there,
>> I am trying to install the glmmADMB package but I get the message
>> #is not available (for R version 3.0.2).
>> I've tried installing it on a older R version
>> but get the same message (albeit not with 3.0.2).
>> 
>> Is there anyone out there that can tell me the R version
>> in which glmmADMB package will work? I am
>> using a Mac 10.9.3.
>> 
>> thanks in advance
>> 
>> Miguel
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From holtermann at hwwi.org  Wed Jun 25 17:00:52 2014
From: holtermann at hwwi.org (Linus Holtermann)
Date: Wed, 25 Jun 2014 17:00:52 +0200
Subject: [R-sig-ME] Wieght MAtrix in Error term in MCMCglmm
Message-ID: <AD0050057515F54084E7D5B93478C8481FC6620645@winxbede39.exchange.xchg>

Hello,

is it possible to fit the following weight Matrix W (spatial error model) in MCMCglmm? 
Dimension of W = N x N (row standardised). rho is the coefficient for W.

Yij = b0 + b1Xij + U0j + Eij 
Eij = rho W Eij + eij

Except of W, it is a standard multilevel model with gaussian error distribution. Please see attachment for a better formal description of the model.

Mit freundlichen Gr??en
 
 
Linus Holtermann
Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
Heimhuder Stra?e 71
20148 Hamburg
Tel +49-(0)40-340576-336
Fax+49-(0)40-340576-776
Internet: www.hwwi.org
Email: holtermann at hwwi.org
 
AmtsgerichtHamburg HRB 94303
Gesch?ftsf?hrer: Prof. Dr. Thomas Straubhaar, Gunnar Geyer
Umsatzsteuer-ID: DE 241849425

From russell-lenth at uiowa.edu  Thu Jun 26 16:45:47 2014
From: russell-lenth at uiowa.edu (Lenth, Russell V)
Date: Thu, 26 Jun 2014 14:45:47 +0000
Subject: [R-sig-ME] lsmeans in glmmADMB
Message-ID: <51F0C7C54B032A42A23B74A088E7141C2E56383B@itsnt443.iowa.uiowa.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140626/bb3da97c/attachment.pl>

From daniela at uoregon.edu  Thu Jun 26 17:11:40 2014
From: daniela at uoregon.edu (Daniel Anderson)
Date: Thu, 26 Jun 2014 08:11:40 -0700
Subject: [R-sig-ME] Variance component specification
Message-ID: <3BBD6107-AB79-4D41-ABB4-A0FC5D5E7F80@uoregon.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140626/04ede211/attachment.pl>

From oneil.shawnt at gmail.com  Thu Jun 26 18:15:03 2014
From: oneil.shawnt at gmail.com (Shawn O'Neil)
Date: Thu, 26 Jun 2014 12:15:03 -0400
Subject: [R-sig-ME] Problems fitting a hurdle model using glmmADMB
In-Reply-To: <CAHJOu+r7-svR-p3vYB3eD0s=G43k+V_WqzcxsNjCtkeVuR=1rA@mail.gmail.com>
References: <CAHJOu+r7-svR-p3vYB3eD0s=G43k+V_WqzcxsNjCtkeVuR=1rA@mail.gmail.com>
Message-ID: <CAHJOu+o3FDgAcuSBsFqP4wB54BDkjGoHH9AHjb6cK_r+B7rEOQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20140626/977d2815/attachment.pl>

From thomas.worthington at okstate.edu  Thu Jun 26 23:00:41 2014
From: thomas.worthington at okstate.edu (Worthington, Thomas A)
Date: Thu, 26 Jun 2014 21:00:41 +0000
Subject: [R-sig-ME] Mixed Model Specification
Message-ID: <C1A5238848713043B7C18ED38FFEF1F812B8065D@STWMB01.ad.okstate.edu>

Dear All 

I have a question about the use of a mixed effects model. I have presence/absence data for a mussel species collected at 25 sites. I wish to relate the presence/absence to a number of environmental variables and also want to take into account site. Is it feasible to use site as a random effect as I have only one replicate per site   e.g.

M1<-glmer(Presence ~ Substrate, (1 | Site), family = binomial, data = data)

Best wishes

Tom 


From bbolker at gmail.com  Thu Jun 26 23:14:37 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 26 Jun 2014 17:14:37 -0400
Subject: [R-sig-ME] Mixed Model Specification
In-Reply-To: <C1A5238848713043B7C18ED38FFEF1F812B8065D@STWMB01.ad.okstate.edu>
References: <C1A5238848713043B7C18ED38FFEF1F812B8065D@STWMB01.ad.okstate.edu>
Message-ID: <53AC8D3D.5080308@gmail.com>

On 14-06-26 05:00 PM, Worthington, Thomas A wrote:
> Dear All
> 
> I have a question about the use of a mixed effects model. I have
> presence/absence data for a mussel species collected at 25 sites. I
> wish to relate the presence/absence to a number of environmental
> variables and also want to take into account site. Is it feasible to
> use site as a random effect as I have only one replicate per site
> e.g.
> 
> M1<-glmer(Presence ~ Substrate, (1 | Site), family = binomial, data =
> data)
> 

  No  -- the among-site variance is not identifiable with binary
(Bernoulli/presence-absence) responses (unless the responses can be
grouped into sets of more than one observation with the same
covariates), see e.g. http://www.gllamm.org/JEBSredundant_07.pdf section
2.2.  You can probably find further discussion elsewhere on this list.

  In theory if you have a single categorical predictor (Substrate) you
could group your data into the form

  Substrate, num.present, num.absent

which gives a binomial (non-binary) response, but if you want to
estimate a fixed effect of substrate you still can't use an
observation-level random effect to model overdispersion because

  cbind(num.present,num.absent) ~ Substrate + (1|obs)

will still confound observation and substrate.

  Even if you could do this, 25 presence-absence samples is a very small
data set (sorry). The 'effective sample size' for binary variables is
max(# presence, # absence) (see e.g. Harrell _Regression Modeling
Strategies_), and the rule of thumb is that you should have 10-20
observations per parameter, so even in the best case (50%
presence/absence) you probably can't do very much more than estimate the
difference in probability of presence between 2 or 3 different kinds of
substrates.

 Ben Bolker


From hughes.dupond at gmx.de  Thu Jun 26 23:17:46 2014
From: hughes.dupond at gmx.de (Lionel)
Date: Thu, 26 Jun 2014 23:17:46 +0200
Subject: [R-sig-ME] Mixed Model Specification
In-Reply-To: <C1A5238848713043B7C18ED38FFEF1F812B8065D@STWMB01.ad.okstate.edu>
References: <C1A5238848713043B7C18ED38FFEF1F812B8065D@STWMB01.ad.okstate.edu>
Message-ID: <53AC8DFA.6030805@gmx.de>

Dear Tom,

Here are my thoughts on your question:
In mixed effect models the aim is to account for the variation present 
in the random terms when the standard error around the coefficients of 
the fixed effect part is computed. As you have only one replicate per 
site you cannot assess this variation. For this dataset I would drop the 
site variable since any statistical method I know of need some kind of 
assessment of the variation due to a variable to derive inference.

Hope this help,
Lionel


On 26/06/2014 23:00, Worthington, Thomas A wrote:
> Dear All
>
> I have a question about the use of a mixed effects model. I have presence/absence data for a mussel species collected at 25 sites. I wish to relate the presence/absence to a number of environmental variables and also want to take into account site. Is it feasible to use site as a random effect as I have only one replicate per site   e.g.
>
> M1<-glmer(Presence ~ Substrate, (1 | Site), family = binomial, data = data)
>
> Best wishes
>
> Tom
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Thierry.ONKELINX at inbo.be  Thu Jun 26 23:26:09 2014
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Thu, 26 Jun 2014 21:26:09 +0000
Subject: [R-sig-ME] Mixed Model Specification
In-Reply-To: <C1A5238848713043B7C18ED38FFEF1F812B8065D@STWMB01.ad.okstate.edu>
References: <C1A5238848713043B7C18ED38FFEF1F812B8065D@STWMB01.ad.okstate.edu>
Message-ID: <AA818EAD2576BC488B4F623941DA7427F3AB39ED@inbomail.inbo.be>

Dear Tom,

Your dataset is very small. Consider yourself lucky when a simple glm gives reasonable estimates. A rule of thumb is that your need 10 effective observations per parameter. The number of effective observations in the binomial case is equal to the number of presences or absences (the smallest of the two). If you are very lucky: 25/2 = 12. So at best you can fit a model with 1 (one) parameter. e.g. glm(Presence ~ Substrate) when Substrate has only 2 (two) levels.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey

________________________________________
Van: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] namens Worthington, Thomas A [thomas.worthington at okstate.edu]
Verzonden: donderdag 26 juni 2014 23:00
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Mixed Model Specification

Dear All

I have a question about the use of a mixed effects model. I have presence/absence data for a mussel species collected at 25 sites. I wish to relate the presence/absence to a number of environmental variables and also want to take into account site. Is it feasible to use site as a random effect as I have only one replicate per site   e.g.

M1<-glmer(Presence ~ Substrate, (1 | Site), family = binomial, data = data)

Best wishes

Tom

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From bbolker at gmail.com  Fri Jun 27 01:24:10 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 26 Jun 2014 19:24:10 -0400
Subject: [R-sig-ME] Mixed Model Specification
In-Reply-To: <AA818EAD2576BC488B4F623941DA7427F3AB39ED@inbomail.inbo.be>
References: <C1A5238848713043B7C18ED38FFEF1F812B8065D@STWMB01.ad.okstate.edu>
	<AA818EAD2576BC488B4F623941DA7427F3AB39ED@inbomail.inbo.be>
Message-ID: <53ACAB9A.3070703@gmail.com>

On 14-06-26 05:26 PM, ONKELINX, Thierry wrote:
> Dear Tom,
> 
> Your dataset is very small. Consider yourself lucky when a simple glm
> gives reasonable estimates. A rule of thumb is that your need 10
> effective observations per parameter. The number of effective
> observations in the binomial case is equal to the number of presences
> or absences (the smallest of the two). If you are very lucky: 25/2 =
> 12. So at best you can fit a model with 1 (one) parameter. e.g.
> glm(Presence ~ Substrate) when Substrate has only 2 (two) levels.
> 
> Best regards,
> 
> Thierry

  I should probably re-read this advice on the R-help posting guide:

===================
When responding to a very simple question, use the following algorithm:

    compose your response
    type 4*runif(1) at the R prompt, and wait this many hours
    check for new posts to R-help; if no similar suggestion, post your
response

(This is partly in jest, but if you know immediately why it is
suggested, you probably should use it! Also, it's a nice idea to replace
4 by the number of years you have been using R or S-plus.)
=====================

  except that I think I shouldn't compose my response first!

> 
> [r-sig-mixed-models-bounces at r-project.org] namens Worthington, Thomas
> A [thomas.worthington at okstate.edu] Verzonden: donderdag 26 juni 2014
> 23:00 Aan: r-sig-mixed-models at r-project.org Onderwerp: [R-sig-ME]
> Mixed Model Specification
> 
> Dear All
> 
> I have a question about the use of a mixed effects model. I have
> presence/absence data for a mussel species collected at 25 sites. I
> wish to relate the presence/absence to a number of environmental
> variables and also want to take into account site. Is it feasible to
> use site as a random effect as I have only one replicate per site
> e.g.
> 
> M1<-glmer(Presence ~ Substrate, (1 | Site), family = binomial, data =
> data)
> 
> Best wishes
> 
> Tom
>


From David.Duffy at qimr.edu.au  Fri Jun 27 03:56:55 2014
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Fri, 27 Jun 2014 11:56:55 +1000
Subject: [R-sig-ME] Problems fitting a hurdle model using glmmADMB
In-Reply-To: <CAHJOu+o3FDgAcuSBsFqP4wB54BDkjGoHH9AHjb6cK_r+B7rEOQ@mail.gmail.com>
References: <CAHJOu+r7-svR-p3vYB3eD0s=G43k+V_WqzcxsNjCtkeVuR=1rA@mail.gmail.com>
	<CAHJOu+o3FDgAcuSBsFqP4wB54BDkjGoHH9AHjb6cK_r+B7rEOQ@mail.gmail.com>
Message-ID: <alpine.LMD.2.00.1406271147360.18015@orpheus.qimr.edu.au>

On Fri, 27 Jun 2014, Shawn O'Neil wrote:

> I've explored my data some more and have an update to the above message. I
> think my model for the non-zero counts is underfitting because the
> zero-truncated negative binomial distribution is not a great representation
> of the data. My count data for y>0 are as following:
>
>> summary(as.factor(dat.nz$Count))
>  1   2   3   4   5   6   7   8  10  11  12
> 172  39  18  14   4   8   2   1   1   1   1
>
> The zero-truncated negative binomial family does not seem to fit for this
> many ones and the corresponding drop-off to lower values. Thus, I suppose I
> shouldn't use this family of models for the 2nd part of the hurdle model
> that I'm trying to fit. But what are the alternatives? I have thought about
> quantile regression and am searching for non-parametric methods, but I am
> not sure if it is acceptable to split up the model in this way.

Ordinal or threshold models eg ordinal or lavaan packages.  The gaussian 
threshold model allows you to check model assumptions to some extent eg 
the chisq test for bivariate normality from polycor in the polychor 
package (which you can do on all two-way margins).

Cheers, David Duffy.

| David Duffy (MBBS PhD)
| email: David.Duffy at qimrberghofer.edu.au  ph: INT+61+7+3362-0217 fax: -0101
| Genetic Epidemiology, QIMR Berghofer Institute of Medical Research
| 300 Herston Rd, Brisbane, Queensland 4006, Australia  GPG 4D0B994A


From lisa.rummel at web.de  Fri Jun 27 13:25:06 2014
From: lisa.rummel at web.de (Lisa Rummel)
Date: Fri, 27 Jun 2014 13:25:06 +0200
Subject: [R-sig-ME] glmmadmb instead of glmer to avoid convergence warnings
	in lme4
Message-ID: <trinity-a369c149-e84a-4135-acd8-320f38f01139-1403868306748@3capp-webde-bs13>

Dear mixed models experts,

I'm working with a dataset to analize the success of translocation programs of animal species. For this purpose, the success of various programs has been evaluated at a scale from 0 to 10. Now we want to investigate, in which way different factors like the duration of the program or the number of released animals, influence the success rate. 

We?re using a linear mixed-effects model (package lme4 1.1-6) with "success" as response variable, "duration" in years as predictor variable and species as random effect. But when I run the model, I obtain the following convergence warning:


> model10 <- glmer(succes~duration+(1|species), family=poisson)
 Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0136827 (tol = 0.001)


In order to avoid the convergence failure, we changed the distribution into a negative binomial distribution, using a glmmadmb model.


> model11 <- glmmadmb(succes~duration+(1|species), data, family="nbinom")
> summary(model11)

Call:
glmmadmb(formula = succes ~ duration + (1 | species), data = data, 
    family = "nbinom")

AIC: 183.8 

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  1.944321   0.123230   15.78   <2e-16 ***
duration    -0.000988   0.013070   -0.08     0.94    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Number of observations: total=33, species=21 
Random effect variance(s):
Group=species
            Variance StdDev
(Intercept)  0.01666 0.1291

Negative binomial dispersion parameter: 15.451 (std. err.: 24.659)

Log-likelihood: -87.8784 


Our question is, if using the glmmadmb model instead of the glmer is an applicable solution for the convergence failure in this case? Also, we're not quite sure if we can use the poisson/negative binomial distribution for the variable "success", because it`s limited to discrete values from 0 to 10. Perhaps there is kind of a truncated distribution we could use?

Thank you in advanced,
Lisa


From bbolker at gmail.com  Fri Jun 27 13:51:39 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 27 Jun 2014 07:51:39 -0400
Subject: [R-sig-ME] glmmadmb instead of glmer to avoid convergence
 warnings in lme4
In-Reply-To: <trinity-a369c149-e84a-4135-acd8-320f38f01139-1403868306748@3capp-webde-bs13>
References: <trinity-a369c149-e84a-4135-acd8-320f38f01139-1403868306748@3capp-webde-bs13>
Message-ID: <53AD5ACB.7030806@gmail.com>

  The convergence warnings are false positives, as much discussed on the
list: see https://github.com/lme4/lme4/blob/master/README.md

  Poisson or NB models are not completely silly for modeling a success
scale, but they may not be appropriate either -- depends on what your
data actually look like.  If you have enough data, an ordinal model
could be appropriate.

On 14-06-27 07:25 AM, Lisa Rummel wrote:
> Dear mixed models experts,
> 
> I'm working with a dataset to analize the success of translocation
> programs of animal species. For this purpose, the success of various
> programs has been evaluated at a scale from 0 to 10. Now we want to
> investigate, in which way different factors like the duration of the
> program or the number of released animals, influence the success
> rate.
> 
> We?re using a linear mixed-effects model (package lme4 1.1-6) with
> "success" as response variable, "duration" in years as predictor
> variable and species as random effect. But when I run the model, I
> obtain the following convergence warning:
> 
> 
>> model10 <- glmer(succes~duration+(1|species), family=poisson)
> Warning message: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
> control$checkConv,  : Model failed to converge with max|grad| =
> 0.0136827 (tol = 0.001)
> 
> 
> In order to avoid the convergence failure, we changed the
> distribution into a negative binomial distribution, using a glmmadmb
> model.
> 
> 
>> model11 <- glmmadmb(succes~duration+(1|species), data,
>> family="nbinom") summary(model11)
> 
> Call: glmmadmb(formula = succes ~ duration + (1 | species), data =
> data, family = "nbinom")
> 
> AIC: 183.8
> 
> Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept)
> 1.944321   0.123230   15.78   <2e-16 *** duration    -0.000988
> 0.013070   -0.08     0.94 --- Signif. codes:  0 ?***? 0.001 ?**? 0.01
> ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Number of observations: total=33, species=21 Random effect
> variance(s): Group=species Variance StdDev (Intercept)  0.01666
> 0.1291
> 
> Negative binomial dispersion parameter: 15.451 (std. err.: 24.659)
> 
> Log-likelihood: -87.8784
> 
> 
> Our question is, if using the glmmadmb model instead of the glmer is
> an applicable solution for the convergence failure in this case?
> Also, we're not quite sure if we can use the poisson/negative
> binomial distribution for the variable "success", because it`s
> limited to discrete values from 0 to 10. Perhaps there is kind of a
> truncated distribution we could use?
> 
> Thank you in advanced, Lisa
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


