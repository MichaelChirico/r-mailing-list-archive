From bates at stat.wisc.edu  Thu Jan  1 20:44:48 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 1 Jan 2009 13:44:48 -0600
Subject: [R-sig-ME] Is it right to specify a random slope for the dummy
	variables
In-Reply-To: <aefe4d0a0812310335p1f454836p7f68ea108b2062ad@mail.gmail.com>
References: <2fc17e30812310045w1c65af69n61878f5468b4f644@mail.gmail.com>
	<aefe4d0a0812310335p1f454836p7f68ea108b2062ad@mail.gmail.com>
Message-ID: <40e66e0b0901011144s2d957b41tdb96801c1c768ed3@mail.gmail.com>

This is an interesting question in that it provides an opportunity for
philosophical musings on fixed- and random-effects and on the way that
categorical covariates are incorporated into model matrices.  Allow me
to expand a bit (well, maybe more than a bit) on Reinhold's remarks
below.

On Wed, Dec 31, 2008 at 5:35 AM, Reinhold Kliegl
<reinhold.kliegl at gmail.com> wrote:
> By default, a categorical variable (factor) with n levels comes with
> n-1 treatment contrasts. Therefore, in the fixed-effect part of the
> model the intercept represents the reference level and the n-1
> contrasts represent the mean differences between the other levels and
> the reference level, assuming a balanced design. You can check your
> specification with contrasts(factor). Of course, you should change
> from treatment to other contrasts as required by your hypotheses. See
> ?contrasts.

> Now suppose you have the variable group as random factor in the model
> and you include the variable factor also in the random effects part:

> lmer(y ~  factor + (factor|group))

> Then, you can estimate the variance of the intercept (i.e., variance
> of reference level for groups), variances of the n-1  difference
> scores for group,  and correlations between intercept and difference
> scores as random effects (i.e., you estimate varying intercepts and
> varying differences and the correlations between them).

> Thus, with categorical variables you are mostly looking at the
> variance and correlation of difference scores between levels of a
> factor rather than variance and correlation of slopes (which are also
> a kind of difference score, of course).

Reinhold is exactly correct in stating that the coefficients
associated with a categorical factor are usually expressed as an
intercept and a set of k-1 "contrasts", where k is the number of
levels in the factor.  There are alternatives, however, and it can be
interesting to explore them.

For brevity let me write f for a factor whose levels are considered
fixed and repeatable, g for a factor whose levels represent a sample
from a, possibly infinite, set of potential levels, x for a continuous
covariate and y for the response.  The number of levels for factor f
is k.

As most R users are (or should be) aware, a model formula, which is
used to generate a model matrix from a data set, includes an implicit
intercept term.  Thus the formula

y ~ x

is equivalent to the formula

y ~ 1 + x

In fact, many people prefer to use the second form because it more
clearly illustrates that there will be two coefficients estimated, the
first associated with a constant term and the second associated with
the numeric values of x.

If we wish to suppress the intercept we can replace the '1' by a '0'
to obtain the formula

y ~ 0 + x

When we have only continuous covariates in the model formula, the
underlying models represented by 1 + x and 0 + x are different but the
parameterization is the same.  That statement may seem opaque but bear
with me and I will try to explain what I mean.  The "underlying model"
is the set of all possible fitted values from the model.  One of the
characteristics of linear models is that the model is defined by the
set of all possible predictions, which must be a linear subspace of
the response space.  For those who know the linear algebra
terminology, this subspace is the column span of the model matrix.
The coefficients are associated with a particular parameterization but
the model itself exists independently of the parameterization.  (It is
this concept that Don Watts and I ported over to nonlinear regression
models to explain some of the effects of nonlinearity.)

So what I was saying about 1 + x versus 0 + x is that the column spans
of the model matrices are different; the first is two dimensional and
the second is one dimensional, but the parameterization for the column
spans is the same.

This is not the case for a categorical factor.  The model formula

y ~ 1 + f

and the model formula

y ~ 0 + f

generate the same model.  Each generates a model matrix whose column
span is the k-dimensional span of the indicator columns for the levels
of f.  For the second formula the parameterization is derived from the
indicator columns.  We can think of the parameterization for the first
model formula as resulting from the constant column followed by the
complete set of indicator columns generating a model matrix with k + 1
columns and rank k.  To obtain a full-rank matrix we must drop some
linear combination of the columns.  The default method is to drop the
indicator of the first level of the factor but any one will do.  That
is, we have a well-defined column span but with an arbitrary
parameterization.

There is an interesting point here regarding the difference between
fixed-effects and random-effects terms.  The coefficients in
fixed-effects terms are estimated by least squares, which I think of
as a rigid criterion.  The model matrix for the fixed-effects terms
must have full column rank.  If it doesn't then the coefficient
estimates are undetermined.  Thus, to obtain a well-defined and unique
set of coefficient estimates we must check the rank of the model
matrix and adjust the form of the matrix (and hence the
parameterization of the model) if we detect a rank-deficient matrix.
Much of the checking and adjustment can be and is done at the symbolic
level, which is why the code for model.matrix is much more subtle than
most would suspect (and also why just about any formula about analysis
of variance given in an introductory book is not really the way that
analysis of variance results are calculated).  The model formula ->
model frame -> model matrix path in the S language is almost never
recognized for the great accomplishment that it is.  However, even the
symbolic analysis doesn't account for all cases, which is why there is
a secondary check on the numerical rank of the model matrix -
something that is not nearly as simple as it sounds.  As anyone who
has taken a linear algebra class knows, the rank of a matrix is a
well-defined property that is easily evaluated.  As anyone who has
taken a numerical linear algebra class knows, the rank of a matrix is
essentially undefined and impossible to evaluate reliably.

One side note; it is exactly the need to assign a rank to a model
matrix and to take corrective action for rank-deficient cases that
keeps us using Linpack code for the QR decomposition instead of the
numerically superior and more efficient Lapack code.

Anyway, back at the fixed-effects versus random-effects computational
issues.  The coefficients for a random-effects term are "estimated" by
penalized least squares, which I think of as a flexible criterion.  (I
would embark on analogies about least squares being the oak and
penalized least squares being the willow in the wind storm but that is
probably a little too much.)  The penalty term takes care of any
problems with rank deficiencies.  We say that it "regularizes" the
calculation in the sense that that the conditional means of the random
effects are "shrunken" toward zero relative to the least squares
estimates.  It is exactly this regularization that allows the models
to be expressed in the natural parameterization.  That is, although
the model matrix for the formula

y ~ 1 + f

cannot be expressed as a constant and the complete set of indicator
columns because it will be rank deficient, the model matrix for the
formula

y ~ 1 + (1 | g)

is expressed as a constant and the complete set of indicator columns
for the levels of g.  Furthermore all of these coefficients are
estimable.

Finally I get to the point of Zhijie's question about incorporating
coefficients for a factor f in the random effects associated with the
levels of g.  Reinhold explained what the formula that could be
written as

1)   y ~ 1 + f + (1 + f | g)

would generate.  This model is equivalent to

2)   y ~ 1 + f + (0 + f | g)

or

3)  y ~ 0 + f + (0 + f | g)

and I would recommend one of these forms for interpretability.  The
parameterization of the fixed-effects is unimportant so either 2) or
3) will do.  The parameterization of the random effects is similarly
unimportant in that formulas 1), 2) and 3) all generate the same model
fits but I feel there is an advantage in using the indicator column
representation from 2) and 3).  All three of these formulas will
result in k variances and k(k-1)/2 covariances (or correlations) being
estimated and up to k random effects for each level of g.  In 2) and
3) the variances reported are the variance of the effect of level i of
factor f on level j on factor g (or vice versa).

The disadvantage of all three formulations is that when k is moderate
the number of variance/covariance parameters being estimated,
k(k+1)/2, can be large.   An alternative model, which I prefer as a
starting point, is

4) y ~ 0 + f + (1 | g) + (1 | f:g)

Model 4) allows for an additive shift for each level of of g and an
additive shift for each (observed) combination of levels of f and g.
There are actually more random effects in 4) than in 1), 2) or 3) but
many fewer variance/covariance parameters.  Model 4) has only two
variance parameters to be estimated and no covariances.   Model 4)
corresponds to model 3) subject to the constraint that the
variance-covariance matrix for the random effects have a compound
symmetry pattern.

Some of these issues are illustrated in the section "Interactions of
grouping factors and other covariates" in the slides at
http://www.stat.wisc.edu/~bates/UseR2008/WorkshopD.pdf (slides 84 to
95).

> On Wed, Dec 31, 2008 at 9:45 AM, zhijie zhang <epistat at gmail.com> wrote:
>> Dear all,
>>  Today, i was thinking the following question.
>> We know the variables may be classified into continuous, ordinal, and
>> categorical variables. I was confused about how to handle with
>> the categorical variables in the multi-level models.
>> For fixed effects,  the categorical variables were always treated as dummy
>> variables, my questions are:
>> 1. Could the random slope be specified for categorical variables that was
>> always changed into the form of  dummy variables?
>> 2. If  the random slope could be specified for categorical variables, how to
>> explain it? It seems a little different from the continuous variables.
>>  I tried the GLIMMIX Procedure in SAS. It seems that SAS treats categorical
>> variables as  continuous variables. While in MLWin, it seems that random
>> slope could be specified for the dummy variables .
>>  Any ideas on it are greatly appreciated.



From epistat at gmail.com  Fri Jan  2 03:09:28 2009
From: epistat at gmail.com (zhijie zhang)
Date: Fri, 2 Jan 2009 10:09:28 +0800
Subject: [R-sig-ME] Is it right to specify a random slope for the dummy
	variables
In-Reply-To: <40e66e0b0901011144s2d957b41tdb96801c1c768ed3@mail.gmail.com>
References: <2fc17e30812310045w1c65af69n61878f5468b4f644@mail.gmail.com>
	<aefe4d0a0812310335p1f454836p7f68ea108b2062ad@mail.gmail.com>
	<40e66e0b0901011144s2d957b41tdb96801c1c768ed3@mail.gmail.com>
Message-ID: <2fc17e30901011809kb0511dft6cf6d6e5ac9d538f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090102/af424e98/attachment.pl>

From bates at stat.wisc.edu  Fri Jan  2 17:07:06 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 2 Jan 2009 10:07:06 -0600
Subject: [R-sig-ME] Fwd: updating functions mangling original copies of
	glmer fits?
In-Reply-To: <40e66e0b0901011426p643279a1ud78fd41dfaa08c82@mail.gmail.com>
References: <4957B296.8000005@ufl.edu>
	<40e66e0b0901011426p643279a1ud78fd41dfaa08c82@mail.gmail.com>
Message-ID: <40e66e0b0901020807v60131ccfg624bb4838fb93f4d@mail.gmail.com>

I thought others might be interested in this exchange that Ben and I
had over New Year's.


---------- Forwarded message ----------
From: Douglas Bates <bates at stat.wisc.edu>
Date: Thu, Jan 1, 2009 at 4:26 PM
Subject: Re: updating functions mangling original copies of glmer fits?
To: Ben Bolker <bolker at ufl.edu>


Thanks for the note, Ben.

On Sun, Dec 28, 2008 at 11:08 AM, Ben Bolker <bolker at ufl.edu> wrote:
>
>  Dear Prof Bates,
>
>  in trying to replicate the deviance-profiling code in the
> Implementation vignette, I've run across a slightly scary side
> effect.  The code below generates some data from the model
>
>  eps_b ~ Normal(0,1)
>  x ~  Uniform(0,1)
>  Linpred = 1+2*x+eps_b
>  Y ~ Poisson(exp(Linpred))
>
> that is, a fairly standard GLMM with a single continuous
> covariate, a single random effect on the intercept,
> log link, Poisson, etc. (I don't think any of this matters
> that much but I'm trying to be careful).
>
>  Then I bundled the code used to generate Figure 2 in
> Implementation.Rnw, with the addition of a .Call("mer_update_dev")
> to make it work for a GLMM, into a function called varprof
> that evaluates the model for a series of different random-effects
> standard deviations.
>
>  The answers make sense so far, but ... the **original model
> fit object** (which has been passed into the function, so ought to have
> had an independent copy made???) has now ended up being modified
> by the profiling code, so that its standard deviation is
> now equal to the maximum sd tried in the profile ...
>
>  I would try to sort this out but it feels like very deep R magic ...

What you have observed is indeed a violation of the functional
semantics of the S language, briefly summarized as "Thou shalt not
modify the value of an argument without copying".  The magic to do
this is not deep - inside a C function called through .Call you
can do whatever you want, including modifying the value of an
argument.

My excuse for doing things this way is the usual: efficiency.  The mer
object encapsulates the model and its current state during the
optimization process.  It would be possible to copy and update that
object every time the deviance is evaluated during the optimization
but
 a) that object can be huge
 b) even with all the tricks that are used for optimization, it still
requires many, many evaluations

Even one copy of such an object can be expensive.  There was a recent
message to the R-SIG-Mixed-Models mailing list regarding being able to
fit the model but running out of memory when trying to summarize it.
The problem was that there is a copy made in the summary method.

In the currently released version of lme4 I finessed the copying issue
by modifying the object in place during the optimization and keeping
the whole thing behind closed doors, as it were.  I suppose that when
I wrote code such as that in the Implementation vignette I should have
added a "Don't try this at home, kids" caveat but instead I modified
(yet again!) the overall approach.  If you check the code in the
"allcoef" branch of the SVN repository you will see that the model is
represented by an environment through the optimization phase and only
then converted to an S4 object.  Environments are special composite
objects in that they are never copied.  If an environment is passed to
a function and modified during the evaluation of the function then the
modifications are visible outside the function.  As such they are a
natural way of representing "state" during an optimization.

There are a couple of other (well, many other) enhancements in the
allcoef branch.  It uses an explicit call to nlminb for the
optimization with all the information encapsulated in the environment
and there is provision for incorporating additional parameters in the
optimization.  The name "allcoef" comes from a switch in the role of
the fixed-effects parameters in a generalized linear mixed model or a
nonlinear mixed model.  Now, all the coefficients in the linear
predictor are optimized during the IRLS iterations (the "inner"
optimization, which is well-controlled) and not during the general,
constrained optimization (the "outer" optimization performed by
nlminb).

Would it be okay for me to copy this reply to the R-SIG-Mixed-Models email list?

Happy New Year,
Doug Bates



>  I can obviously work around this by re-fitting the object after
> computing the profile to restore it, but it does feel like a bug ...
>
>  Haven't tested yet whether this problem is specific to mer_update_dev
> or whether it happens with LMMs as well ...
>
>  cheers
>    Ben Bolker
>
>
> set.seed(1001)
> x <- runif(1000)
> f <- factor(rep(1:10,each=100))
> blockeff <- rnorm(10,sd=1)
> linpred <- exp(1+2*x+blockeff[f])
> y <- rpois(1000,linpred)
> dat <- data.frame(x,f,y)
>
> plot(x,1+y,type="n",log="y")
> text(x,1+y,as.character(f))
> library(lme4)
> mod <- glmer(y~x+(1|f),data=dat,family=poisson)
>
> varprof <- function(mm,lower=0,upper=20,n=101) {
>  sg <- seq(lower, upper, len = n)
>  dev <- mm at deviance
>  nc <- length(dev)
>  nms <- names(dev)
>  vals <- matrix(0, nrow = length(sg), ncol = nc, dimnames = list(NULL,
> nms))
>  for (i in seq(along = sg)) {
>    .Call("mer_ST_setPars", mm, sg[i], PACKAGE = "lme4")
>    .Call("mer_update_L", mm, PACKAGE = "lme4")
>    res <- try(.Call("mer_update_RX", mm, PACKAGE = "lme4"), silent = TRUE)
>    if (inherits(res, "try-error")) {
>        vals[i,] <- NA
>      } else {
>        .Call("mer_update_ranef", mm, PACKAGE = "lme4")
>        .Call("mer_update_dev", mm, PACKAGE = "lme4") ## added for glmmML
>        vals[i,] <- mm at deviance
>      }
>  }
>  vals
> }
>
> attr(VarCorr(mod)$f,"stddev") ## 1.633
> logLik(mod) ## -535
> vv <- varprof(mod)
> attr(VarCorr(mod)$f,"stddev") ## 20
> logLik(mod)  ## -556!
>
>> sessionInfo()
> R version 2.8.1 (2008-12-22)
> i486-pc-linux-gnu
>
> locale:
> LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=C;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats4    stats     graphics  grDevices utils     datasets  methods
> [8] base
>
> other attached packages:
> [1] emdbook_1.1.1.1    MASS_7.2-45        glmmADMB_0.3
> glmmML_0.81-3
> [5] lme4_0.999375-28   Matrix_0.999375-17 lattice_0.17-17
>
> loaded via a namespace (and not attached):
> [1] coda_0.13-3   grid_2.8.1    rjags_1.0.3-4
>
> --
> Ben Bolker
> Associate professor, Biology Dep't, Univ. of Florida
> bolker at ufl.edu / www.zoology.ufl.edu/bolker
> GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>
>



From henrik.parn at bio.ntnu.no  Sat Jan  3 15:28:02 2009
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Sat, 03 Jan 2009 15:28:02 +0100
Subject: [R-sig-ME] 'adjusting for bias' in a Poisson model
Message-ID: <495F75F2.3000102@bio.ntnu.no>

Dear all,

Basically, I have fitted a mixed model with poisson error to analyse how
number of offspring (y) depend on a fixed factor (x). The data is
grouped by two random factors (gr1, gr2).

# Some test data with at least a similar structure:

set.seed(100)
test.data <- data.frame(
y = c(rpois(40, 5.5), rpois(40, 3.5)),
x = factor(rep(0:1, each = 40)),
gr1 = factor(rep(1:4, each = 10)),
gr2 = factor(rep(1:2, each = 5)))

# The model
model <- lmer(y ~ x + (1|gr1) + (1|gr2),
data = test.data, family = poisson)

summary(model)
Generalized linear mixed model fit by the Laplace approximation
Formula: y ~ x + (1 | gr1) + (1 | gr2)
    Data: test.data
    AIC   BIC logLik deviance
  61.01 70.54 -26.51    53.01
Random effects:
  Groups Name        Variance   Std.Dev.
  gr1    (Intercept) 0.00045437 0.021316
  gr2    (Intercept) 0.00000000 0.000000
Number of obs: 80, groups: gr1, 4; gr2, 2

Fixed effects:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)  1.75331    0.06666  26.302  < 2e-16 ***
x1          -0.48660    0.10665  -4.563 5.05e-06 ***


Thus, y is significantly higher for x = 0 than for x = 1. However, it 
has been suggested that the estimate of x may be biased (for biological 
reasons) and actually be less negative than what is found above. 
Specifically, even in absence of an effect of x, the bias could cause y 
for x = 1 to be 20% lower than y for x = 0.
(y for x=0 - y for x=1)/ y for x=0 = 20%; y for x=1 could be 0.8*y for 
x=0 due to bias.

Thus, the estimate of x would be (1.75331 - 0.2 * 1.75331) - 1.75331 = 
-1.75331 * 0.2 = -0.350662 just due to the bias.

I wish to test if there is an effect of x on y over and above the 
potential 20% bias.

My naive starting point: Instead of testing if the estimate of x 
(-0.48660) differs from zero, I would need to test if -0.48660 - 
(-0.350662) = -0.135938 differs fro zero.

Or could I somehow make the adjustment already in the data set, e.g. 
adjusting the y's for x = 1? But I assume that I cannot 'just add ?? % 
to y in group x', because the response has to be integers?

Does anyone have a suggestion of a convenient way of performe test for 
significance of x on y while taking the bias into account?


Thanks a lot in advance!


-- 
Henrik P?rn
Centre for Conservation Biology
Department of Biology
Norwegian University of Science and Technology
NO-7491 Trondheim
Norway

Office: +47 73596285
Fax: +47 73596100
Mobile: +47 90989255

E-mail: henrik.parn at bio.ntnu.no



From bolker at ufl.edu  Sat Jan  3 16:23:26 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Sat, 03 Jan 2009 10:23:26 -0500
Subject: [R-sig-ME] 'adjusting for bias' in a Poisson model
In-Reply-To: <495F75F2.3000102@bio.ntnu.no>
References: <495F75F2.3000102@bio.ntnu.no>
Message-ID: <495F82EE.2040402@ufl.edu>


  "offset" is you want to look for.
  offsets apply on the scale of the linear predictor
(log scale in this case), so I think adding something
like offset=((x==1)*log(0.8)) to the model will do
the trick.

Henrik Parn wrote:
> Dear all,
> 
> Basically, I have fitted a mixed model with poisson error to analyse how
> number of offspring (y) depend on a fixed factor (x). The data is
> grouped by two random factors (gr1, gr2).
> 
> # Some test data with at least a similar structure:
> 
> set.seed(100)
> test.data <- data.frame(
> y = c(rpois(40, 5.5), rpois(40, 3.5)),
> x = factor(rep(0:1, each = 40)),
> gr1 = factor(rep(1:4, each = 10)),
> gr2 = factor(rep(1:2, each = 5)))
> 
> # The model
> model <- lmer(y ~ x + (1|gr1) + (1|gr2),
> data = test.data, family = poisson)
> 
> summary(model)
> Generalized linear mixed model fit by the Laplace approximation
> Formula: y ~ x + (1 | gr1) + (1 | gr2)
>    Data: test.data
>    AIC   BIC logLik deviance
>  61.01 70.54 -26.51    53.01
> Random effects:
>  Groups Name        Variance   Std.Dev.
>  gr1    (Intercept) 0.00045437 0.021316
>  gr2    (Intercept) 0.00000000 0.000000
> Number of obs: 80, groups: gr1, 4; gr2, 2
> 
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  1.75331    0.06666  26.302  < 2e-16 ***
> x1          -0.48660    0.10665  -4.563 5.05e-06 ***
> 
> 
> Thus, y is significantly higher for x = 0 than for x = 1. However, it
> has been suggested that the estimate of x may be biased (for biological
> reasons) and actually be less negative than what is found above.
> Specifically, even in absence of an effect of x, the bias could cause y
> for x = 1 to be 20% lower than y for x = 0.
> (y for x=0 - y for x=1)/ y for x=0 = 20%; y for x=1 could be 0.8*y for
> x=0 due to bias.
> 
> Thus, the estimate of x would be (1.75331 - 0.2 * 1.75331) - 1.75331 =
> -1.75331 * 0.2 = -0.350662 just due to the bias.
> 
> I wish to test if there is an effect of x on y over and above the
> potential 20% bias.
> 
> My naive starting point: Instead of testing if the estimate of x
> (-0.48660) differs from zero, I would need to test if -0.48660 -
> (-0.350662) = -0.135938 differs fro zero.
> 
> Or could I somehow make the adjustment already in the data set, e.g.
> adjusting the y's for x = 1? But I assume that I cannot 'just add ?? %
> to y in group x', because the response has to be integers?
> 
> Does anyone have a suggestion of a convenient way of performe test for
> significance of x on y while taking the bias into account?
> 
> 
> Thanks a lot in advance!
> 
> 


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From bates at stat.wisc.edu  Sat Jan  3 16:45:54 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 3 Jan 2009 09:45:54 -0600
Subject: [R-sig-ME] 'adjusting for bias' in a Poisson model
In-Reply-To: <495F82EE.2040402@ufl.edu>
References: <495F75F2.3000102@bio.ntnu.no> <495F82EE.2040402@ufl.edu>
Message-ID: <40e66e0b0901030745s1cf5dd1fla5e8b7debd89c075@mail.gmail.com>

Thanks for pointing this out, Ben.

At the risk of muddying the waters, I will point out that there are
two ways of specifying an offset and the way that Ben suggests is the
preferred one (the other is with an offset() term in the model
formula).  Using the offset argument is simpler and easier to modify.

Also, you may find it handy to assign, say

myoff <- (x==1)*log(0.8)

and use offset = myoff.  Creation of a model frame and the
corresponding model matrices is more subtle than it may seem at first,
and sometimes evaluation of the offset as an expression is tricky.

On Sat, Jan 3, 2009 at 9:23 AM, Ben Bolker <bolker at ufl.edu> wrote:
>
>  "offset" is you want to look for.
>  offsets apply on the scale of the linear predictor
> (log scale in this case), so I think adding something
> like offset=((x==1)*log(0.8)) to the model will do
> the trick.
>
> Henrik Parn wrote:
>> Dear all,
>>
>> Basically, I have fitted a mixed model with poisson error to analyse how
>> number of offspring (y) depend on a fixed factor (x). The data is
>> grouped by two random factors (gr1, gr2).
>>
>> # Some test data with at least a similar structure:
>>
>> set.seed(100)
>> test.data <- data.frame(
>> y = c(rpois(40, 5.5), rpois(40, 3.5)),
>> x = factor(rep(0:1, each = 40)),
>> gr1 = factor(rep(1:4, each = 10)),
>> gr2 = factor(rep(1:2, each = 5)))
>>
>> # The model
>> model <- lmer(y ~ x + (1|gr1) + (1|gr2),
>> data = test.data, family = poisson)
>>
>> summary(model)
>> Generalized linear mixed model fit by the Laplace approximation
>> Formula: y ~ x + (1 | gr1) + (1 | gr2)
>>    Data: test.data
>>    AIC   BIC logLik deviance
>>  61.01 70.54 -26.51    53.01
>> Random effects:
>>  Groups Name        Variance   Std.Dev.
>>  gr1    (Intercept) 0.00045437 0.021316
>>  gr2    (Intercept) 0.00000000 0.000000
>> Number of obs: 80, groups: gr1, 4; gr2, 2
>>
>> Fixed effects:
>>             Estimate Std. Error z value Pr(>|z|)
>> (Intercept)  1.75331    0.06666  26.302  < 2e-16 ***
>> x1          -0.48660    0.10665  -4.563 5.05e-06 ***
>>
>>
>> Thus, y is significantly higher for x = 0 than for x = 1. However, it
>> has been suggested that the estimate of x may be biased (for biological
>> reasons) and actually be less negative than what is found above.
>> Specifically, even in absence of an effect of x, the bias could cause y
>> for x = 1 to be 20% lower than y for x = 0.
>> (y for x=0 - y for x=1)/ y for x=0 = 20%; y for x=1 could be 0.8*y for
>> x=0 due to bias.
>>
>> Thus, the estimate of x would be (1.75331 - 0.2 * 1.75331) - 1.75331 =
>> -1.75331 * 0.2 = -0.350662 just due to the bias.
>>
>> I wish to test if there is an effect of x on y over and above the
>> potential 20% bias.
>>
>> My naive starting point: Instead of testing if the estimate of x
>> (-0.48660) differs from zero, I would need to test if -0.48660 -
>> (-0.350662) = -0.135938 differs fro zero.
>>
>> Or could I somehow make the adjustment already in the data set, e.g.
>> adjusting the y's for x = 1? But I assume that I cannot 'just add ?? %
>> to y in group x', because the response has to be integers?
>>
>> Does anyone have a suggestion of a convenient way of performe test for
>> significance of x on y while taking the bias into account?
>>
>>
>> Thanks a lot in advance!
>>
>>
>
>
> --
> Ben Bolker
> Associate professor, Biology Dep't, Univ. of Florida
> bolker at ufl.edu / www.zoology.ufl.edu/bolker
> GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bolker at ufl.edu  Sat Jan  3 17:58:30 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Sat, 03 Jan 2009 11:58:30 -0500
Subject: [R-sig-ME] 'adjusting for bias' in a Poisson model
Message-ID: <495F9936.6070902@ufl.edu>


  PS I would be somewhat concerned with your number of
random-effects levels (4 for group 1, 2 for group 2).  It's
not at all surprising that you got an estimated value
of zero variance for group 2 ... there are various schools
of thought on this, but I would suggest that you might
alternatively try (1) fitting the grouping variables as
fixed effects and (??) (2) Bayesian analysis with appropriate
non-informative priors on the variances [this is a big
can of worms, though].

  [Forwarding back to the list]

  Ben Bolker


Henrik Parn wrote:
> Thanks a lot Ben for your very rapid answer! I will try it out!
> 
> Cheers,
> 
> Henrik
> 
> Ben Bolker wrote:
>>   "offset" is you want to look for.
>>   offsets apply on the scale of the linear predictor
>> (log scale in this case), so I think adding something
>> like offset=((x==1)*log(0.8)) to the model will do
>> the trick.
>>
>> Henrik Parn wrote:
>>> Dear all,
>>>
>>> Basically, I have fitted a mixed model with poisson error to analyse how
>>> number of offspring (y) depend on a fixed factor (x). The data is
>>> grouped by two random factors (gr1, gr2).
>>>
>>> # Some test data with at least a similar structure:
>>>
>>> set.seed(100)
>>> test.data <- data.frame(
>>> y = c(rpois(40, 5.5), rpois(40, 3.5)),
>>> x = factor(rep(0:1, each = 40)),
>>> gr1 = factor(rep(1:4, each = 10)),
>>> gr2 = factor(rep(1:2, each = 5)))
>>>
>>> # The model
>>> model <- lmer(y ~ x + (1|gr1) + (1|gr2),
>>> data = test.data, family = poisson)
>>>
>>> summary(model)
>>> Generalized linear mixed model fit by the Laplace approximation
>>> Formula: y ~ x + (1 | gr1) + (1 | gr2)
>>>    Data: test.data
>>>    AIC   BIC logLik deviance
>>>  61.01 70.54 -26.51    53.01
>>> Random effects:
>>>  Groups Name        Variance   Std.Dev.
>>>  gr1    (Intercept) 0.00045437 0.021316
>>>  gr2    (Intercept) 0.00000000 0.000000
>>> Number of obs: 80, groups: gr1, 4; gr2, 2
>>>
>>> Fixed effects:
>>>             Estimate Std. Error z value Pr(>|z|)
>>> (Intercept)  1.75331    0.06666  26.302  < 2e-16 ***
>>> x1          -0.48660    0.10665  -4.563 5.05e-06 ***
>>>
>>>
>>> Thus, y is significantly higher for x = 0 than for x = 1. However, it
>>> has been suggested that the estimate of x may be biased (for biological
>>> reasons) and actually be less negative than what is found above.
>>> Specifically, even in absence of an effect of x, the bias could cause y
>>> for x = 1 to be 20% lower than y for x = 0.
>>> (y for x=0 - y for x=1)/ y for x=0 = 20%; y for x=1 could be 0.8*y for
>>> x=0 due to bias.
>>>
>>> Thus, the estimate of x would be (1.75331 - 0.2 * 1.75331) - 1.75331 =
>>> -1.75331 * 0.2 = -0.350662 just due to the bias.
>>>
>>> I wish to test if there is an effect of x on y over and above the
>>> potential 20% bias.
>>>
>>> My naive starting point: Instead of testing if the estimate of x
>>> (-0.48660) differs from zero, I would need to test if -0.48660 -
>>> (-0.350662) = -0.135938 differs fro zero.
>>>
>>> Or could I somehow make the adjustment already in the data set, e.g.
>>> adjusting the y's for x = 1? But I assume that I cannot 'just add ?? %
>>> to y in group x', because the response has to be integers?
>>>
>>> Does anyone have a suggestion of a convenient way of performe test for
>>> significance of x on y while taking the bias into account?
>>>
>>>
>>> Thanks a lot in advance!
>>>
>>>
>>
>>
> 


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From henrik.parn at bio.ntnu.no  Sat Jan  3 19:09:16 2009
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Sat, 03 Jan 2009 19:09:16 +0100
Subject: [R-sig-ME] 'adjusting for bias' in a Poisson model
In-Reply-To: <495F98EC.2050401@ufl.edu>
References: <495F75F2.3000102@bio.ntnu.no> <495F82EE.2040402@ufl.edu>
	<495F8472.2020305@bio.ntnu.no> <495F98EC.2050401@ufl.edu>
Message-ID: <495FA9CC.9010303@bio.ntnu.no>

Thanks for the comment!

I would also be concerned. However, although the test.data and the real 
data are similar in the sense that both have two random grouping 
variables, the real data has, among other things, more levels in both 
groups. I just wanted to keep the test.data small, but apparently I lost 
some (too much?) realism on the way (I wish I had taken the short course 
"How to efficiently produce frequently used minimal, self contained, 
reproducible code and example data in R").

Henrik

Ben Bolker wrote:
>   PS I would be somewhat concerned with your number of
> random-effects levels (4 for group 1, 2 for group 2).  It's
> not at all surprising that you got an estimated value
> of zero variance for group 2 ... there are various schools
> of thought on this, but I would suggest that you might
> alternatively try (1) fitting the grouping variables as
> fixed effects and (??) (2) Bayesian analysis with appropriate
> non-informative priors on the variances [this is a big
> can of worms, though].
> 
>   [Forwarding back to the list]
> 
>   Ben Bolker
> 
> 
> Henrik Parn wrote:
>> Thanks a lot Ben for your very rapid answer! I will try it out!
>>
>> Cheers,
>>
>> Henrik
>>
>> Ben Bolker wrote:
>>>   "offset" is you want to look for.
>>>   offsets apply on the scale of the linear predictor
>>> (log scale in this case), so I think adding something
>>> like offset=((x==1)*log(0.8)) to the model will do
>>> the trick.
>>>
>>> Henrik Parn wrote:
>>>> Dear all,
>>>>
>>>> Basically, I have fitted a mixed model with poisson error to analyse how
>>>> number of offspring (y) depend on a fixed factor (x). The data is
>>>> grouped by two random factors (gr1, gr2).
>>>>
>>>> # Some test data with at least a similar structure:
>>>>
>>>> set.seed(100)
>>>> test.data <- data.frame(
>>>> y = c(rpois(40, 5.5), rpois(40, 3.5)),
>>>> x = factor(rep(0:1, each = 40)),
>>>> gr1 = factor(rep(1:4, each = 10)),
>>>> gr2 = factor(rep(1:2, each = 5)))
>>>>
>>>> # The model
>>>> model <- lmer(y ~ x + (1|gr1) + (1|gr2),
>>>> data = test.data, family = poisson)
>>>>
>>>> summary(model)
>>>> Generalized linear mixed model fit by the Laplace approximation
>>>> Formula: y ~ x + (1 | gr1) + (1 | gr2)
>>>>    Data: test.data
>>>>    AIC   BIC logLik deviance
>>>>  61.01 70.54 -26.51    53.01
>>>> Random effects:
>>>>  Groups Name        Variance   Std.Dev.
>>>>  gr1    (Intercept) 0.00045437 0.021316
>>>>  gr2    (Intercept) 0.00000000 0.000000
>>>> Number of obs: 80, groups: gr1, 4; gr2, 2
>>>>
>>>> Fixed effects:
>>>>             Estimate Std. Error z value Pr(>|z|)
>>>> (Intercept)  1.75331    0.06666  26.302  < 2e-16 ***
>>>> x1          -0.48660    0.10665  -4.563 5.05e-06 ***
>>>>
>>>>
>>>> Thus, y is significantly higher for x = 0 than for x = 1. However, it
>>>> has been suggested that the estimate of x may be biased (for biological
>>>> reasons) and actually be less negative than what is found above.
>>>> Specifically, even in absence of an effect of x, the bias could cause y
>>>> for x = 1 to be 20% lower than y for x = 0.
>>>> (y for x=0 - y for x=1)/ y for x=0 = 20%; y for x=1 could be 0.8*y for
>>>> x=0 due to bias.
>>>>
>>>> Thus, the estimate of x would be (1.75331 - 0.2 * 1.75331) - 1.75331 =
>>>> -1.75331 * 0.2 = -0.350662 just due to the bias.
>>>>
>>>> I wish to test if there is an effect of x on y over and above the
>>>> potential 20% bias.
>>>>
>>>> My naive starting point: Instead of testing if the estimate of x
>>>> (-0.48660) differs from zero, I would need to test if -0.48660 -
>>>> (-0.350662) = -0.135938 differs fro zero.
>>>>
>>>> Or could I somehow make the adjustment already in the data set, e.g.
>>>> adjusting the y's for x = 1? But I assume that I cannot 'just add ?? %
>>>> to y in group x', because the response has to be integers?
>>>>
>>>> Does anyone have a suggestion of a convenient way of performe test for
>>>> significance of x on y while taking the bias into account?
>>>>
>>>>
>>>> Thanks a lot in advance!
>>>>
>>>>
>>>
> 
> 

-- 
Henrik P?rn
Centre for Conservation Biology
Department of Biology
Norwegian University of Science and Technology
NO-7491 Trondheim
Norway

Office: +47 73596285
Fax: +47 73596100
Mobile: +47 90989255

E-mail: henrik.parn at bio.ntnu.no



From Thierry.ONKELINX at inbo.be  Mon Jan  5 00:19:16 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 5 Jan 2009 00:19:16 +0100
Subject: [R-sig-ME] [R] how specify lme() with multiple within-subject
	factors?
In-Reply-To: <07AB467B-6790-4F82-AB15-A3E9ABA47669@student.rug.nl>
References: <07AB467B-6790-4F82-AB15-A3E9ABA47669@student.rug.nl>
Message-ID: <2E9C414912813E4EB981326983E0A10405DF5128@inboexch.inbo.be>

Dear Ben,

I'm cc'ing R-sig-mixed-models because that's a more appropriate list for
questions on lme().

Lme() is only able to work with nested random effects, not with crossed
random effects. Therefore you would need lmer() from the lme4 package.
But I don't think you need crossed random effects. Random slopes should
do the trick since wtype and present have only two levels. Try something
like lme(.., .., random = ~wtype * present | subj) 

HTH,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of
data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
Namens Ben Meijering
Verzonden: zaterdag 3 januari 2009 19:59
Aan: r-help at r-project.org
Onderwerp: [R] how specify lme() with multiple within-subject factors?

I have some questions about the use of lme().
Below, I constructed a minimal dataset to explain what difficulties I  
experience:

# two participants
subj <- factor(c(1, 1, 1, 1, 2, 2, 2, 2))
# within-subjects factor Word Type
wtype <- factor(c("nw", "w", "nw", "w", "nw", "w", "nw", "w"))
# within-subjects factor Target Present/Absent
present <- factor(c(0, 0, 1, 1, 0, 0, 1, 1))
# dependend variable Accuracy
acc <- c(.74, .81, .84, .88, .75, .95, .88, .94)

# repeated-measures analysis of variance
acc.aov <- aov(acc ~ wtype * present + Error(subj/wtype*present))
summary(acc.aov)

# to use lme
library(nlme)
# mixed-effects model
acc.lme <- lme(acc ~ wtype * present, random = ~ 1 | subj)
anova(acc.lme)

How do I have to specify the model to have 1 degree of freedom for the  
denominator or error-term, as in aov()?
I know how to do this for the first factor:

lme(.., .., random = ~1 | subj/wtype),

or

lme(.., .., random = list( ~ 1 | subj, ~1 | wtype))

, but not how to get the same degrees of freedom as in the specified  
aov(), i.e., 1 degree of freedom of the denominator for both factors  
and the interaction term.

How do I specify such a model?

~ Ben

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From B.Meijering at student.rug.nl  Mon Jan  5 15:31:44 2009
From: B.Meijering at student.rug.nl (Ben Meijering)
Date: Mon, 5 Jan 2009 15:31:44 +0100
Subject: [R-sig-ME] [R] how specify lme() with multiple within-subject
	factors?
In-Reply-To: <2E9C414912813E4EB981326983E0A10405DF5128@inboexch.inbo.be>
References: <07AB467B-6790-4F82-AB15-A3E9ABA47669@student.rug.nl>
	<2E9C414912813E4EB981326983E0A10405DF5128@inboexch.inbo.be>
Message-ID: <E63BDE42-2F08-4E03-B4CC-3701EF9D1D37@student.rug.nl>

Thierry,

Thanks for your reply!
I've specified the random part as you suggested, but with lmer()  
instead of lme(). For a thorough walk-through of lmer(), I've read  
Baayen, Davidson and Bates' (2007) article.
However, I still have a question about the degrees of freedom.

My model:

lmer(acc ~ wtype * present + (wtype + present | subj), data=...)

Then, with aovlmer.fnc() I obtained the p-values of the factors. I  
don't understand why Df2 differs from the residual degrees of freedom  
of oav()[, which is aov(acc ~ wtype * present + Error(subj/ 
wtype*present), data=...)]

In lmer() all terms (wtype, present and wtype:present) have the same  
df2, i.e., levels wtype * levels present * (levels subject - 1).
In repeated-measures aov each within-subject term has it's own  
denominator degrees of freedom, i.e., for wtype it's (levels wtype  
-1), for present it's (levels present -1) and for wtype:present it's  
(levels wtype - 1)*(levels present - 1).

Why are the residual degrees of freedom different between aov and  
lmer()?

Best,

Ben Meijering



Op 5 jan 2009, om 00:19 heeft ONKELINX, Thierry het volgende geschreven:

> Dear Ben,
>
> I'm cc'ing R-sig-mixed-models because that's a more appropriate list  
> for
> questions on lme().
>
> Lme() is only able to work with nested random effects, not with  
> crossed
> random effects. Therefore you would need lmer() from the lme4 package.
> But I don't think you need crossed random effects. Random slopes  
> should
> do the trick since wtype and present have only two levels. Try  
> something
> like lme(.., .., random = ~wtype * present | subj)
>
> HTH,
>
> Thierry
>
> ------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be
> www.inbo.be
>
> To call in the statistician after the experiment is done may be no  
> more
> than asking him to perform a post-mortem examination: he may be able  
> to
> say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
>
> The plural of anecdote is not data.
> ~ Roger Brinner
>
> The combination of some data and an aching desire for an answer does  
> not
> ensure that a reasonable answer can be extracted from a given body of
> data.
> ~ John Tukey
>
> -----Oorspronkelijk bericht-----
> Van: r-help-bounces at r-project.org [mailto:r-help-bounces at r- 
> project.org]
> Namens Ben Meijering
> Verzonden: zaterdag 3 januari 2009 19:59
> Aan: r-help at r-project.org
> Onderwerp: [R] how specify lme() with multiple within-subject factors?
>
> I have some questions about the use of lme().
> Below, I constructed a minimal dataset to explain what difficulties I
> experience:
>
> # two participants
> subj <- factor(c(1, 1, 1, 1, 2, 2, 2, 2))
> # within-subjects factor Word Type
> wtype <- factor(c("nw", "w", "nw", "w", "nw", "w", "nw", "w"))
> # within-subjects factor Target Present/Absent
> present <- factor(c(0, 0, 1, 1, 0, 0, 1, 1))
> # dependend variable Accuracy
> acc <- c(.74, .81, .84, .88, .75, .95, .88, .94)
>
> # repeated-measures analysis of variance
> acc.aov <- aov(acc ~ wtype * present + Error(subj/wtype*present))
> summary(acc.aov)
>
> # to use lme
> library(nlme)
> # mixed-effects model
> acc.lme <- lme(acc ~ wtype * present, random = ~ 1 | subj)
> anova(acc.lme)
>
> How do I have to specify the model to have 1 degree of freedom for the
> denominator or error-term, as in aov()?
> I know how to do this for the first factor:
>
> lme(.., .., random = ~1 | subj/wtype),
>
> or
>
> lme(.., .., random = list( ~ 1 | subj, ~1 | wtype))
>
> , but not how to get the same degrees of freedom as in the specified
> aov(), i.e., 1 degree of freedom of the denominator for both factors
> and the interaction term.
>
> How do I specify such a model?
>
> ~ Ben
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> Dit bericht en eventuele bijlagen geven enkel de visie van de  
> schrijver weer
> en binden het INBO onder geen enkel beding, zolang dit bericht niet  
> bevestigd is
> door een geldig ondertekend document. The views expressed in  this  
> message
> and any annex are purely those of the writer and may not be regarded  
> as stating
> an official position of INBO, as long as the message is not  
> confirmed by a duly
> signed document.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



From bates at stat.wisc.edu  Mon Jan  5 18:57:38 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 5 Jan 2009 11:57:38 -0600
Subject: [R-sig-ME] [R] how specify lme() with multiple within-subject
	factors?
In-Reply-To: <000001c96ec4$1d9d1910$6701a8c0@gne.windows.gene.com>
References: <07AB467B-6790-4F82-AB15-A3E9ABA47669@student.rug.nl>
	<2E9C414912813E4EB981326983E0A10405DF5128@inboexch.inbo.be>
	<000001c96ec4$1d9d1910$6701a8c0@gne.windows.gene.com>
Message-ID: <40e66e0b0901050957vd1949bewa70dc96fda0523f8@mail.gmail.com>

On Sun, Jan 4, 2009 at 5:28 PM, Bert Gunter <gunter.berton at gene.com> wrote:
> Folks:
>
>> Lme() is only able to work with nested random effects, not with crossed
>> random effects.
>
> Not quite true. Crossed models **can** be done, albeit clumsily, via
> pdMatrix objects: the Bates/Pinheiro book even contains an example or two
> (one on assay plates, I recall, but I don't have my book with me for the
> reference).

Clumsily indeed and very inefficiently.  I would not recommend using
lme or nlme to fit models with crossed random effects other than for
small toy examples.

> Also, lme, not lmer, is currently the only way to implement
> penalized splines as random effects -- see the lmeSplines package.

And this is relevant because ???

The original question was about the number of denominator degrees of
freedom and I will freely admit that lmer, in particular, does not
produce the "correct" denominator degrees of freedom. (As many people
discover to their chagrin, it doesn't provide denominator degrees of
freedom at all.)  In my opinion there isn't such a thing as the
"correct" denominator degrees of freedom except in small, perfectly
balanced, toy examples, which is why I am not that terribly concerned.



From atyre2 at unlnotes.unl.edu  Tue Jan  6 05:10:16 2009
From: atyre2 at unlnotes.unl.edu (Andrew J Tyre)
Date: Mon, 5 Jan 2009 22:10:16 -0600
Subject: [R-sig-ME] What is the maximum number of groups?
Message-ID: <OF1096A343.DA330B9C-ON86257536.001395C5-86257536.0016CEA9@unl.edu>

Hi All,

I've been thinking alot about the earlier productive and helpful (for me!) 
discussion on overdispersion and glmms etc. I haven't gotten back to my 
grasshopper example yet, but I will as soon as the students complete 
another component of the data collection. 

I have just come across an example in which I was able to account for most 
of the overdispersion - enough to make me happy anyway - but I'm worried 
about what I did to do it. The data are the number of traps at a site on 
one day that caught or did not catch a deer, so binomial data. There are 
several sites sampled in several different years, and a couple of weather 
covariates that are the same for all traps at a given site on a particular 
day; there are no trap level covariates, but many days of trapping within 
each site/year combination. We expect there to be variation among 
site/year combinations related to the number of deer at a site, variation 
among days due to weather (some covariate data) or other factors, and 
variation among traps due to the location of the trap relative to the deer 
population. 

Some simulated data:

library(lme4)

# simulate some data
x = runif(300,1,10)
siteyear = factor(rep(1:10,each=30))
logit.p = x*0.2 - 0.025*x^2 + rep(rnorm(10,0,0.4),each=30) + 
rnorm(300,0,0.2)
p = 1/(1+exp(-logit.p))
size = sample(5:15,300,replace=TRUE)
y = rbinom(300,prob=p,size=size)
siteday=factor(1:300)
mm.1 = lmer(cbind(y,size-y)~x+I(x^2)+(1|siteday),family=binomial)
mm.2 = lmer(cbind(y,size-y)~x+I(x^2)+(1|siteyear),family=binomial)
mm.3 = lmer(cbind(y,size-y)~x+I(x^2)+siteyear+(1|siteday),family=binomial)

mm.list = list(mm.1,mm.2,mm.3)
k = c(4,4,13)
ll = sapply(mm.list,logLik)
# use chat estimate to get quasiAIC
chat = deviance(mm.3)/300

qaic = -2*ll/chat+2*k
delta = qaic-min(qaic)
AIC.table=cbind(qaic,k,delta,w = exp(-(delta/2))/sum(exp(-delta/2)))
AIC.table
       qaic  k    delta            w
ML 404.6663  4 78.66631 8.269964e-18
ML 340.3928  4 14.39277 7.487291e-04
ML 326.0000 13  0.00000 9.992513e-01
> chat
      ML 
1.106134 

Now this is pretty close to the structure that I have with the real data. 
All models had the (1|siteday) random effect, various combinations of 
covariates of interest (x in the example), and then either did or did not 
include siteyear as a fixed effect. In the real example, as with my 
simulated example, the model with both a fixed effect of siteyear and a 
random effect of siteday is the best model. The best model makes 
biological sense. The question: is having as many groups as observations 
an issue? It seems to work but ... I didn't start worrying about it until 
afterwards. Does it work because there are multiple traps per siteday? Any 
insights appreciated.

> sessionInfo()
R version 2.7.2 (2008-08-25) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base 

other attached packages:
[1] lme4_0.999375-27   Matrix_0.999375-15 lattice_0.17-13 

loaded via a namespace (and not attached):
[1] grid_2.7.2  nlme_3.1-89

Drew Tyre

School of Natural Resources
University of Nebraska-Lincoln
416 Hardin Hall, East Campus
3310 Holdrege Street
Lincoln, NE 68583-0974

phone: +1 402 472 4054 
fax: +1 402 472 2946
email: atyre2 at unl.edu
http://snr.unl.edu/tyre



From bolker at ufl.edu  Tue Jan  6 16:11:04 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 06 Jan 2009 10:11:04 -0500
Subject: [R-sig-ME] What is the maximum number of groups?
In-Reply-To: <OF1096A343.DA330B9C-ON86257536.001395C5-86257536.0016CEA9@unl.edu>
References: <OF1096A343.DA330B9C-ON86257536.001395C5-86257536.0016CEA9@unl.edu>
Message-ID: <49637488.7030209@ufl.edu>


  If I understand what you're doing correctly, you're setting
up an "individual-level" covariate, essentially fitting
a binomial-normal model (or binomial-logitnormal).  That makes sense,
although I'm a bit surprised it worked -- in some versions of lme4 it
has triggered an error message about too many random effect parameters.
For example see

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q1/000603.html

and following discussion.

  Ben Bolker

Andrew J Tyre wrote:
> Hi All,
> 
> I've been thinking alot about the earlier productive and helpful (for me!) 
> discussion on overdispersion and glmms etc. I haven't gotten back to my 
> grasshopper example yet, but I will as soon as the students complete 
> another component of the data collection. 
> 
> I have just come across an example in which I was able to account for most 
> of the overdispersion - enough to make me happy anyway - but I'm worried 
> about what I did to do it. The data are the number of traps at a site on 
> one day that caught or did not catch a deer, so binomial data. There are 
> several sites sampled in several different years, and a couple of weather 
> covariates that are the same for all traps at a given site on a particular 
> day; there are no trap level covariates, but many days of trapping within 
> each site/year combination. We expect there to be variation among 
> site/year combinations related to the number of deer at a site, variation 
> among days due to weather (some covariate data) or other factors, and 
> variation among traps due to the location of the trap relative to the deer 
> population. 
> 
> Some simulated data:
> 
> library(lme4)
> 
> # simulate some data
> x = runif(300,1,10)
> siteyear = factor(rep(1:10,each=30))
> logit.p = x*0.2 - 0.025*x^2 + rep(rnorm(10,0,0.4),each=30) + 
> rnorm(300,0,0.2)
> p = 1/(1+exp(-logit.p))
> size = sample(5:15,300,replace=TRUE)
> y = rbinom(300,prob=p,size=size)
> siteday=factor(1:300)
> mm.1 = lmer(cbind(y,size-y)~x+I(x^2)+(1|siteday),family=binomial)
> mm.2 = lmer(cbind(y,size-y)~x+I(x^2)+(1|siteyear),family=binomial)
> mm.3 = lmer(cbind(y,size-y)~x+I(x^2)+siteyear+(1|siteday),family=binomial)
> 
> mm.list = list(mm.1,mm.2,mm.3)
> k = c(4,4,13)
> ll = sapply(mm.list,logLik)
> # use chat estimate to get quasiAIC
> chat = deviance(mm.3)/300
> 
> qaic = -2*ll/chat+2*k
> delta = qaic-min(qaic)
> AIC.table=cbind(qaic,k,delta,w = exp(-(delta/2))/sum(exp(-delta/2)))
> AIC.table
>        qaic  k    delta            w
> ML 404.6663  4 78.66631 8.269964e-18
> ML 340.3928  4 14.39277 7.487291e-04
> ML 326.0000 13  0.00000 9.992513e-01
>> chat
>       ML 
> 1.106134 
> 
> Now this is pretty close to the structure that I have with the real data. 
> All models had the (1|siteday) random effect, various combinations of 
> covariates of interest (x in the example), and then either did or did not 
> include siteyear as a fixed effect. In the real example, as with my 
> simulated example, the model with both a fixed effect of siteyear and a 
> random effect of siteday is the best model. The best model makes 
> biological sense. The question: is having as many groups as observations 
> an issue? It seems to work but ... I didn't start worrying about it until 
> afterwards. Does it work because there are multiple traps per siteday? Any 
> insights appreciated.
> 
>> sessionInfo()
> R version 2.7.2 (2008-08-25) 
> i386-pc-mingw32 
> 
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
> States.1252;LC_MONETARY=English_United 
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices datasets  utils     methods   base 
> 
> other attached packages:
> [1] lme4_0.999375-27   Matrix_0.999375-15 lattice_0.17-13 
> 
> loaded via a namespace (and not attached):
> [1] grid_2.7.2  nlme_3.1-89
> 
> Drew Tyre
> 
> School of Natural Resources
> University of Nebraska-Lincoln
> 416 Hardin Hall, East Campus
> 3310 Holdrege Street
> Lincoln, NE 68583-0974
> 
> phone: +1 402 472 4054 
> fax: +1 402 472 2946
> email: atyre2 at unl.edu
> http://snr.unl.edu/tyre
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From atyre2 at unlnotes.unl.edu  Tue Jan  6 19:55:57 2009
From: atyre2 at unlnotes.unl.edu (Andrew J Tyre)
Date: Tue, 6 Jan 2009 12:55:57 -0600
Subject: [R-sig-ME] What is the maximum number of groups?
In-Reply-To: <49637488.7030209@ufl.edu>
Message-ID: <OF19E94413.3EDB8CD4-ON86257536.00551093-86257536.0067E49E@unl.edu>

Right - I hadn't thought about it that way, but I think a 
"binomial-lognormal" is correct:

log(p/(1-p) = X*Beta + epsilon

episilon~N(0,sigma)

The summary says I have 300 groups and 300 observations, so the random 
effects vector is the same size as the dataset. If I attempt to include 
the "siteyear" level variation as a random effect I get the message 

Error in mer_finalize(ans) : q = 310 > n = 300

which is the error described in the previous posts. So its working here 
because I only have the individual level random effect. 



cheers,

Drew Tyre

School of Natural Resources
University of Nebraska-Lincoln
416 Hardin Hall, East Campus
3310 Holdrege Street
Lincoln, NE 68583-0974

phone: +1 402 472 4054 
fax: +1 402 472 2946
email: atyre2 at unl.edu
http://snr.unl.edu/tyre



Ben Bolker <bolker at ufl.edu> 
Sent by: r-sig-mixed-models-bounces at r-project.org
01/06/2009 09:13 AM

To
Andrew J Tyre <atyre2 at unlnotes.unl.edu>, R Mixed Models 
<r-sig-mixed-models at r-project.org>
cc

Subject
Re: [R-sig-ME] What is the maximum number of groups?







  If I understand what you're doing correctly, you're setting
up an "individual-level" covariate, essentially fitting
a binomial-normal model (or binomial-logitnormal).  That makes sense,
although I'm a bit surprised it worked -- in some versions of lme4 it
has triggered an error message about too many random effect parameters.
For example see

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q1/000603.html

and following discussion.

  Ben Bolker

Andrew J Tyre wrote:
> Hi All,
> 
> I've been thinking alot about the earlier productive and helpful (for 
me!) 
> discussion on overdispersion and glmms etc. I haven't gotten back to my 
> grasshopper example yet, but I will as soon as the students complete 
> another component of the data collection. 
> 
> I have just come across an example in which I was able to account for 
most 
> of the overdispersion - enough to make me happy anyway - but I'm worried 

> about what I did to do it. The data are the number of traps at a site on 

> one day that caught or did not catch a deer, so binomial data. There are 

> several sites sampled in several different years, and a couple of 
weather 
> covariates that are the same for all traps at a given site on a 
particular 
> day; there are no trap level covariates, but many days of trapping 
within 
> each site/year combination. We expect there to be variation among 
> site/year combinations related to the number of deer at a site, 
variation 
> among days due to weather (some covariate data) or other factors, and 
> variation among traps due to the location of the trap relative to the 
deer 
> population. 
> 
> Some simulated data:
> 
> library(lme4)
> 
> # simulate some data
> x = runif(300,1,10)
> siteyear = factor(rep(1:10,each=30))
> logit.p = x*0.2 - 0.025*x^2 + rep(rnorm(10,0,0.4),each=30) + 
> rnorm(300,0,0.2)
> p = 1/(1+exp(-logit.p))
> size = sample(5:15,300,replace=TRUE)
> y = rbinom(300,prob=p,size=size)
> siteday=factor(1:300)
> mm.1 = lmer(cbind(y,size-y)~x+I(x^2)+(1|siteday),family=binomial)
> mm.2 = lmer(cbind(y,size-y)~x+I(x^2)+(1|siteyear),family=binomial)
> mm.3 = 
lmer(cbind(y,size-y)~x+I(x^2)+siteyear+(1|siteday),family=binomial)
> 
> mm.list = list(mm.1,mm.2,mm.3)
> k = c(4,4,13)
> ll = sapply(mm.list,logLik)
> # use chat estimate to get quasiAIC
> chat = deviance(mm.3)/300
> 
> qaic = -2*ll/chat+2*k
> delta = qaic-min(qaic)
> AIC.table=cbind(qaic,k,delta,w = exp(-(delta/2))/sum(exp(-delta/2)))
> AIC.table
>        qaic  k    delta            w
> ML 404.6663  4 78.66631 8.269964e-18
> ML 340.3928  4 14.39277 7.487291e-04
> ML 326.0000 13  0.00000 9.992513e-01
>> chat
>       ML 
> 1.106134 
> 
> Now this is pretty close to the structure that I have with the real 
data. 
> All models had the (1|siteday) random effect, various combinations of 
> covariates of interest (x in the example), and then either did or did 
not 
> include siteyear as a fixed effect. In the real example, as with my 
> simulated example, the model with both a fixed effect of siteyear and a 
> random effect of siteday is the best model. The best model makes 
> biological sense. The question: is having as many groups as observations 

> an issue? It seems to work but ... I didn't start worrying about it 
until 
> afterwards. Does it work because there are multiple traps per siteday? 
Any 
> insights appreciated.
> 
>> sessionInfo()
> R version 2.7.2 (2008-08-25) 
> i386-pc-mingw32 
> 
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
> States.1252;LC_MONETARY=English_United 
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices datasets  utils     methods   base 
> 
> other attached packages:
> [1] lme4_0.999375-27   Matrix_0.999375-15 lattice_0.17-13 
> 
> loaded via a namespace (and not attached):
> [1] grid_2.7.2  nlme_3.1-89
> 
> Drew Tyre
> 
> School of Natural Resources
> University of Nebraska-Lincoln
> 416 Hardin Hall, East Campus
> 3310 Holdrege Street
> Lincoln, NE 68583-0974
> 
> phone: +1 402 472 4054 
> fax: +1 402 472 2946
> email: atyre2 at unl.edu
> http://snr.unl.edu/tyre
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From kyler at mail.smu.edu  Wed Jan  7 16:16:53 2009
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Wed, 7 Jan 2009 09:16:53 -0600
Subject: [R-sig-ME] R in the New York Times
Message-ID: <551E1CBE65B7EB44B9DF69AF8ED0BE7F03ACE0F3@SXMBXA.systems.smu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090107/2c652be3/attachment.pl>

From Sebastiaan.DeSmedt at ua.ac.be  Thu Jan  8 18:39:31 2009
From: Sebastiaan.DeSmedt at ua.ac.be (De Smedt Sebastiaan)
Date: Thu, 8 Jan 2009 18:39:31 +0100
Subject: [R-sig-ME] Strange logLik's
Message-ID: <930B1A45F446404FA4D99A46F09209C401414AF6@xmail05.ad.ua.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090108/901a8f1e/attachment.pl>

From bates at stat.wisc.edu  Thu Jan  8 19:07:26 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 8 Jan 2009 12:07:26 -0600
Subject: [R-sig-ME] Strange logLik's
In-Reply-To: <930B1A45F446404FA4D99A46F09209C401414AF6@xmail05.ad.ua.ac.be>
References: <930B1A45F446404FA4D99A46F09209C401414AF6@xmail05.ad.ua.ac.be>
Message-ID: <40e66e0b0901081007y389783a6t5fd631782824b76a@mail.gmail.com>

On Thu, Jan 8, 2009 at 11:39 AM, De Smedt Sebastiaan
<Sebastiaan.DeSmedt at ua.ac.be> wrote:
> Dear list members,

> I sampled a total of about 2000 leaves, nested in trees (10 per tree).
> In the field, those trees are nested in 9 different provenances. On the
> leaves, I measured some morphological parameters, for example 'length'.

> Every provenance is characterised by some environmental factors, as for
> example 'rainfall' and 'number of dry months'. These factors are the
> same within avery provenance. Further, each tree is characterised by a
> (arbitrary) degree of human pressure ('MG').

> The objective of the study is to see in which way the environmental and
> human factors are influencing leaf variables.

> I constructed a linear mixed-effects model:

>> lme1<-lme(lenght~MG+dry.months+rainfall, random=~1|provenance/tree)

> I've chosen this random structure because of the field reality (leaves
> are nested in trees, are nested in provenances).

> Further, I constructed two other models with different random
> structures:

>> lme2<-update(lme1,random=~1|provenance)

>> lme3<-update(lme1,random=~1|nr.boom)

> When I compare these models with an anova, I have this output:

>     Model df       AIC       BIC    logLik   Test  L.Ratio p-value
> lme1     1  6 -3123.050 -3089.028 1567.5252
> lme2     2  5 -1653.025 -1624.673  831.5125 1 vs 2 1472.025  <.0001
> lme3     3  5 -2947.977 -2919.625 1478.9884

> I have some questions about this output:

> 1)      Why are the logLik-values positive?

Are you asking this because you believe that log-likelihoods must be
negative?  If so, you are mistaken.  When the response is modeled by a
continuous random variable the likelihood is the value of the
probability density evaluated at the observed data.  A probability
density can exceed unity - all that is required is that it be
non-negative and that it integrate to unity.  Thus the log-likelihood
can be positive, as in this case.

> 2)      I always heard: 'The smaller the logLik, the better the model'.
> Is this also the case with positive logLik's? The residual errors of
> model lme1 and lme3 are much smaller than the residual error of lme2, so
> the 'tree'-effect is really important!

Again, I think you are laboring under the misconception that a
log-likelihood must be negative and, furthermore, when you say
"smaller is better" you mean smaller magnitude, not smaller
algebraically.

The method of estimation is "maximum likelihood" so larger values of
the likelihood or log-likelihood are preferred.  The likelihood ratio
test statistic for model 2 versus model 1 is positive, as it should
be, because model 2 is a specialization of model 1.
> 3)      The variables 'dry.months' and 'rainfall' are the same within
> every provenance. What means this for the model?
>
>
>
> Can anyone help me?
>
>
>
> Thanks!!!
>
>
>
>
>
> Sebastiaan De Smedt
>
> Dept. Bioscience Engineering
>
> University of Antwerp
>
> Groenenborgerlaan 171-V616
>
> B-2020 Antwerpen
>
> T: +32 (0)3/265.35.17
>
> F: +32 (0)3/265.32.25
>
>
>
>
>
> P  Help to save paper - do you really need to print this e-mail???  P
>
>
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From atyre2 at unlnotes.unl.edu  Thu Jan  8 20:50:54 2009
From: atyre2 at unlnotes.unl.edu (Andrew J Tyre)
Date: Thu, 8 Jan 2009 13:50:54 -0600
Subject: [R-sig-ME] Strange logLik's
In-Reply-To: <930B1A45F446404FA4D99A46F09209C401414AF6@xmail05.ad.ua.ac.be>
Message-ID: <OFF838D63A.9B3C1E35-ON86257538.006A3506-86257538.006D0639@unl.edu>

[snip]
> I constructed a linear mixed-effects model:

>> lme1<-lme(lenght~MG+dry.months+rainfall, random=~1|provenance/tree)
[snip]
> 3)      The variables 'dry.months' and 'rainfall' are the same within
> every provenance. What means this for the model?

Without seeing the coefficient estimates it is impossible to tell, really, 
but I've been thinking about this for another model, so here's what I 
think (and hopefully, if it is really wrong someone will correct me)

forgetting about the random effect for a moment, length~dry.months is an 
estimate of the linear relationship between dry.months and leaf length. 
when you add a random effect of the form ~1|provenance you are allowing 
for there to be a different line for every provenance - parallel lines, 
because only the intercept is different. This model partitions the 
variation among provenances into two parts - one explained by the number 
of dry months, and everything else that differs between provenances. Why 
are these things different? Well, imagine you want to predict the leaf 
length in a new provenance not included in your current dataset. You can 
use the coefficient of dry.months times the number of dry months in the 
new provenance to tell you the average leaf length. The estimated variance 
of the random effect of provenance tells you how much additional variation 
you expect in that *average* leaf length. If you want the variance of the 
distribution of leaf lengths then you'd also need to add in the residual 
variance. And of course in your model you have also estimated the 
variation associated with trees within provenances - essentially you now 
have as many parallel lines as there are trees. 

The part that seems somewhat unnatural to me is that you can get an 
estimate of the effect of something that varies among the groups used in 
the random effect - seems like you are accounting for that variation among 
groups in two ways, which feels illegal somehow. I'm guessing that it 
works better when you have more groups.  In addition, if the fixed effect 
was a factor with as many levels as there are groups I think you run into 
problems as well. And by problems I mean a failure to converge. 

hth,

Drew Tyre

School of Natural Resources
University of Nebraska-Lincoln
416 Hardin Hall, East Campus
3310 Holdrege Street
Lincoln, NE 68583-0974

phone: +1 402 472 4054 
fax: +1 402 472 2946
email: atyre2 at unl.edu
http://snr.unl.edu/tyre



From barbara.cuesta at uah.es  Mon Jan 12 17:33:49 2009
From: barbara.cuesta at uah.es (=?iso-8859-1?Q?Cuesta_Poveda_B=E1rbara?=)
Date: Mon, 12 Jan 2009 17:33:49 +0100
Subject: [R-sig-ME] Repeated measures on a split-plot experiment
Message-ID: <9E5993BE3016A84E8FE7367A7C82F8714B7936@X-EVS-01.uah.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090112/c036db8d/attachment.pl>

From lucianolasala at yahoo.com.ar  Tue Jan 13 15:15:55 2009
From: lucianolasala at yahoo.com.ar (Luciano La Sala)
Date: Tue, 13 Jan 2009 12:15:55 -0200
Subject: [R-sig-ME] Zero variance and Std. Dev. using lmer?
Message-ID: <ED69EBED14BD41DEA488093238F5B064@Negro1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090113/2851606d/attachment.pl>

From a.renwick at abdn.ac.uk  Tue Jan 13 15:24:31 2009
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Tue, 13 Jan 2009 14:24:31 +0000
Subject: [R-sig-ME] Zero variance and Std. Dev. using lmer?
In-Reply-To: <ED69EBED14BD41DEA488093238F5B064@Negro1>
References: <ED69EBED14BD41DEA488093238F5B064@Negro1>
Message-ID: <B9D1301370916C44B5874AF340C18B9B439F271AED@VMAILB.uoa.abdn.ac.uk>

I asked a similar question and got a good reponse last year.
Follow the link below:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q3/001245.html

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Luciano La Sala
Sent: 13 January 2009 14:16
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Zero variance and Std. Dev. using lmer?

Dear R-people:



I have run a GLMM (using lmer) with the fixed and random effects detailed below. Oddly I think, I get zero variance and Std. Dev. values.

How is that possible? Does it mean that the RE "NestID" is not helping to account for autocorrelation among sibling chicks at the nest level?

Or is this a small sample size problem?



As well, I ran an ordinary logistic regression using he exact same fixed variables, and I got the exact same AIC and BIC values and estimates for fixed effects, error, z value, and Pr(>|z|).



Does this support the idea that the GLMM with RE for NestID is not necessary at all?



Look forward to hearing from you.

Cheers for now.



Luciano





GENERALIZED LINEAR MIXED MODEL WITH RANDOM INTERCEPT



model <- lmer(Death10~HO+ClutchSize+SibComp+Yr+(1|NestID),family=binomial,1)



Generalized linear mixed model fit by the Laplace approximation

Formula: Death10 ~ HO + ClutchSize + Sibcomp + yr + (1 | NestID)



Data: 1

AIC      BIC       logLik     deviance

242.2    268.5     -113.1     226.2



Random effects:

Groups Name           Variance      Std. Dev.

NestID (Intercept)    0                  0



Number of obs: 198, groups: NestID, 104



Fixed effects:

                                   Estimate                     Std. Error
z value             Pr(>|z|)

(Intercept)                   -1.2239                      0.5114
-2.3934           0.0167 *

HOSecond                  -0.6910                      0.8928
-0.7739           0.4390

HOThird                       0.6768                       1.0327
0.6554             0.5122

ClutchSizeTwo-eggs     1.3961                     0.5864             2.3809
0.0173 *

ClutchSizeThree-eggs   0.3958                      0.5843             0.6773
0.4982

SibcompAbsent             1.7804                     0.9140
1.9479             0.0514 .

yr2007                         -0.8299                      0.3423
-2.4245           0.0153 *

---



Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1





Correlation of Fixed Effects:

                          (Intr)      HOScnd     HOThrd     CltchSzTw-
CltchSzTh-   SbcmpA

HOSecond        -0.034

HOThird           -0.031    0.837

CltchSzTw-g   -0.830   -0.069          -0.022

CltchSzThr-      -0.816   -0.088          -0.107         0.785


SibcmpAbsnt     0.052   -0.904          -0.836       -0.018
-0.050

yr2007              -0.233    0.145           0.133         0.025
-0.050       - 0.224






        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From Thierry.ONKELINX at inbo.be  Tue Jan 13 15:31:18 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 13 Jan 2009 15:31:18 +0100
Subject: [R-sig-ME] Zero variance and Std. Dev. using lmer?
In-Reply-To: <ED69EBED14BD41DEA488093238F5B064@Negro1>
References: <ED69EBED14BD41DEA488093238F5B064@Negro1>
Message-ID: <2E9C414912813E4EB981326983E0A10405F1AE0E@inboexch.inbo.be>

Dear Luciano,

Your variables are strongly correlated. Sibling competition is only
absent when the clutch size is one. Likewise the hatching order can only
be three if the clutch size is three. This could cause numberical
instability of your model. So I suggest that you simplify your model.
What results do you get with this model: lmer(Death10 ~ ClutchSize + Yr
+ (1|NestID), family = binomial)

HTH,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of
data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] Namens Luciano La Sala
Verzonden: dinsdag 13 januari 2009 15:16
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Zero variance and Std. Dev. using lmer?

Dear R-people: 



I have run a GLMM (using lmer) with the fixed and random effects
detailed
below. Oddly I think, I get zero variance and Std. Dev. values. 

How is that possible? Does it mean that the RE "NestID" is not helping
to
account for autocorrelation among sibling chicks at the nest level? 

Or is this a small sample size problem? 



As well, I ran an ordinary logistic regression using he exact same fixed
variables, and I got the exact same AIC and BIC values and estimates for
fixed effects, error, z value, and Pr(>|z|). 



Does this support the idea that the GLMM with RE for NestID is not
necessary
at all? 



Look forward to hearing from you. 

Cheers for now.



Luciano   





GENERALIZED LINEAR MIXED MODEL WITH RANDOM INTERCEPT



model <-
lmer(Death10~HO+ClutchSize+SibComp+Yr+(1|NestID),family=binomial,1)



Generalized linear mixed model fit by the Laplace approximation 

Formula: Death10 ~ HO + ClutchSize + Sibcomp + yr + (1 | NestID) 



Data: 1 

AIC      BIC       logLik     deviance

242.2    268.5     -113.1     226.2



Random effects:

Groups Name           Variance      Std. Dev.

NestID (Intercept)    0                  0      



Number of obs: 198, groups: NestID, 104



Fixed effects:

                                   Estimate                     Std.
Error
z value             Pr(>|z|)  

(Intercept)                   -1.2239                      0.5114
-2.3934           0.0167 *

HOSecond                  -0.6910                      0.8928
-0.7739           0.4390  

HOThird                       0.6768                       1.0327
0.6554             0.5122  

ClutchSizeTwo-eggs     1.3961                     0.5864
2.3809
0.0173 *

ClutchSizeThree-eggs   0.3958                      0.5843
0.6773
0.4982  

SibcompAbsent             1.7804                     0.9140
1.9479             0.0514 .

yr2007                         -0.8299                      0.3423
-2.4245           0.0153 *

---



Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 





Correlation of Fixed Effects:

                          (Intr)      HOScnd     HOThrd     CltchSzTw-
CltchSzTh-   SbcmpA

HOSecond        -0.034                                            

HOThird           -0.031    0.837                                    

CltchSzTw-g   -0.830   -0.069          -0.022


CltchSzThr-      -0.816   -0.088          -0.107         0.785


SibcmpAbsnt     0.052   -0.904          -0.836       -0.018
-0.050           

yr2007              -0.233    0.145           0.133         0.025
-0.050       - 0.224






	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From mjuanjorda at gmail.com  Tue Jan 13 16:15:28 2009
From: mjuanjorda at gmail.com (Maria Jose Juan Jorda)
Date: Tue, 13 Jan 2009 16:15:28 +0100
Subject: [R-sig-ME] how to compute Confident intervals for BLUPS for lme
	function in nlme library
Message-ID: <9bcbdf360901130715y6859e2d5gc201ad755568daf1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090113/9512142f/attachment.pl>

From David.Duffy at qimr.edu.au  Tue Jan 13 22:53:21 2009
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 14 Jan 2009 07:53:21 +1000 (EST)
Subject: [R-sig-ME] Zero variance and Std. Dev. using lmer?
In-Reply-To: <ED69EBED14BD41DEA488093238F5B064@Negro1>
References: <ED69EBED14BD41DEA488093238F5B064@Negro1>
Message-ID: <Pine.LNX.4.64.0901140746400.29455@orpheus.qimr.edu.au>

On Tue, 13 Jan 2009, Luciano La Sala wrote:

> Dear R-people:
>
> I have run a GLMM (using lmer) with the fixed and random effects detailed
> below. Oddly I think, I get zero variance and Std. Dev. values.
>
> How is that possible? Does it mean that the RE "NestID" is not helping to
> account for autocorrelation among sibling chicks at the nest level?
>
> Or is this a small sample size problem?
>
> model <- lmer(Death10~HO+ClutchSize+SibComp+Yr+(1|NestID),family=binomial,1)
>

Either, I would think.  Have you performed a simple test for extrabinomial
variation? eg Tarone test or Chi-square [latter is just
contingency Chi-square test for NestID x Death10]

David Duffy.
-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From sp8ial at gmail.com  Wed Jan 14 00:15:09 2009
From: sp8ial at gmail.com (Greg Lee)
Date: Wed, 14 Jan 2009 10:15:09 +1100
Subject: [R-sig-ME] how to compute Confident intervals for BLUPS for lme
	function in nlme library
In-Reply-To: <9bcbdf360901130715y6859e2d5gc201ad755568daf1@mail.gmail.com>
References: <9bcbdf360901130715y6859e2d5gc201ad755568daf1@mail.gmail.com>
Message-ID: <54c8adaa0901131515k59de5fdamc686906cd2c89644@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090114/20c8d2b2/attachment.pl>

From sp8ial at gmail.com  Wed Jan 14 02:03:13 2009
From: sp8ial at gmail.com (Greg Lee)
Date: Wed, 14 Jan 2009 12:03:13 +1100
Subject: [R-sig-ME] how to compute Confident intervals for BLUPS for lme
	function in nlme library
In-Reply-To: <54c8adaa0901131515k59de5fdamc686906cd2c89644@mail.gmail.com>
References: <9bcbdf360901130715y6859e2d5gc201ad755568daf1@mail.gmail.com>
	<54c8adaa0901131515k59de5fdamc686906cd2c89644@mail.gmail.com>
Message-ID: <54c8adaa0901131703u19805ec7y50f8bff0d1ba34b0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090114/0d3ed56a/attachment.pl>

From reinhold.kliegl at gmail.com  Wed Jan 14 07:50:22 2009
From: reinhold.kliegl at gmail.com (Reinhold Kliegl)
Date: Wed, 14 Jan 2009 07:50:22 +0100
Subject: [R-sig-ME] how to compute Confident intervals for BLUPS for lme
	function in nlme library
In-Reply-To: <54c8adaa0901131703u19805ec7y50f8bff0d1ba34b0@mail.gmail.com>
References: <9bcbdf360901130715y6859e2d5gc201ad755568daf1@mail.gmail.com>
	<54c8adaa0901131515k59de5fdamc686906cd2c89644@mail.gmail.com>
	<54c8adaa0901131703u19805ec7y50f8bff0d1ba34b0@mail.gmail.com>
Message-ID: <aefe4d0a0901132250r6423e7c1q17fd2b664683a4c@mail.gmail.com>

Well, intervals() is a pretty good nlme response, I think, even if it
was not exactly answering the question. The lme4 equivalent is
doplot() which produces a so-called "caterpillar plot" of conditional
means.

For example:
?ranef

fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
rr1 <- ranef(fm1, postVar = TRUE)
dotplot(rr1,scales = list(x = list(relation = 'free')))[["Subject"]]

It is important to be clear about the distinction between random
effect estimates of variances and correlations, provided as part of
the model output and the conditional means for groups/units based on
("predicted with") them. At least, correlations computed from
conditional means can differ strongly from the correlation estimates.
The short story: You should only trust the model estimates. As Greg
Lee said, conditional means are to be handled with care; you can not
apply any of the usual inferential statistics to them. Nevertheless,
the caterpillar plots are highly informative and diagnostic about, for
example, whether you need a random effect to account for unit-by-unit
variability. They may also lead you to  discover distinct subgroups
suggestive of a fixed effect (for a future model). There is a
manuscript at the top of my publications page for download (and
constructive feedback) walking through an example.

Reinhold Kliegl

On Wed, Jan 14, 2009 at 2:03 AM, Greg Lee <sp8ial at gmail.com> wrote:
> Hello again Maria,
>
> I was on autopilot when I answered the first time. The intervals()
> function in nlme provides confidence intervals for the estimated model
> components (fixed effects, random effect variances and correlation
> coefficients, if present), which is not what you were asking.
>
> Actually, the more I think about your question the less certain I am. The
> coefficients in a mixed effects model are (potentially) comprised of both
> fixed- and random-effects. But the random effects are not "estimated" so
> much as "predicted". The only (assumed) requirement on them is that they be
> normally distributed, with standard deviation estimates as reported by
> intervals().
>
> Does it even make sense to seek confidence intervals in this context? I am
> not sure, and leave it for wiser heads to respond.
>
> Regards,
> Greg
>
>
>
> 2009/1/14 Greg Lee <sp8ial at gmail.com>
>
>> Hi Maria,
>>
>> Try:
>>
>> intervals(ssb_model2.1)
>>
>> Cheers,
>> Greg
>>
>>
>>
>> 2009/1/14 Maria Jose Juan Jorda <mjuanjorda at gmail.com>
>>
>>>  Dear all,
>>>
>>>
>>> How do you compute the confident intervals for the estimations of the
>>> random
>>> coefficients? I have not seem a lot of discussion about this in the forum.
>>> I
>>> wonder why. Is there any theory background about this?
>>>
>>> Description of my data:
>>>
>>> I have 29 time series of data, biomass over time for 29 populations. I
>>> want
>>> a hierarchical linear model with varying intercepts and slopes. So it
>>> computes 29 intercepts, 29 slopes and the overall mean of the intercept
>>> and
>>> slope and the variation among intercepts and slopes.
>>>
>>> *Here you have my linear mixed model:
>>> *
>>> ssb_model2.1<-lme(log(SSB_tonnes)~year2,random=~year2|id, data=ssb)
>>>
>>> *Data:*
>>> SSB_tonnes: spawning stock biomass (ssb), continuous variable
>>> year2: time in years, continuous variable
>>> id: there are 29 id, in other words 29 populations of fish
>>>
>>> *Here, in the summary of the ouput* I find the oveall mean of the
>>> interceps
>>> and slopes and their variation.
>>> *> summary(ssb_model2.1)*
>>> Linear mixed-effects model fit by maximum likelihood
>>>  Data: ssb
>>>       AIC      BIC    logLik
>>>  1575.647 1605.880 -781.8235
>>>
>>> Random effects:
>>>  Formula: ~year2 | id
>>>  Structure: General positive-definite, Log-Cholesky parametrization
>>>            StdDev     Corr
>>> (Intercept) 3.30025584 (Intr)
>>> year2       0.02960239 -0.674
>>> Residual    0.42259598
>>>
>>> Fixed effects: log(SSB_tonnes) ~ year2
>>>                Value Std.Error   DF   t-value p-value
>>> (Intercept) 13.190739 0.6271420 1110 21.033098  0.0000
>>> year2       -0.019053 0.0058418 1110 -3.261529  0.0011
>>>  Correlation:
>>>      (Intr)
>>> year2 -0.69
>>>
>>> Standardized Within-Group Residuals:
>>>        Min          Q1         Med          Q3         Max
>>> -5.34384192 -0.39116381  0.04824214  0.41871509  4.60300635
>>>
>>> Number of Observations: 1140
>>> Number of Groups: 29
>>>
>>> *Here I have the estimations for the random coefficient. 29 interceps and
>>> 29
>>> slopes. How do you compute the confidence intervals for these estimations?
>>> please any advise.
>>>  *
>>> *> coef(ssb_model2.1)*
>>>   (Intercept)        year2
>>> 1    11.490207 -0.017008685
>>> 2    11.169138  0.008690365
>>> 3    13.373769 -0.030298582
>>> 4    13.680485 -0.024891089
>>> 5    15.498426 -0.033080606
>>> 6    14.869951 -0.012568273
>>> 7    13.016336 -0.025133918
>>> 8    13.968938 -0.030511346
>>> 9    13.079165 -0.017713727
>>> 10   12.935920 -0.054212415
>>> 11    9.124597 -0.026398183
>>> 12   11.493282 -0.005128911
>>> 13   10.371121  0.012636602
>>> 14   18.273836 -0.081414152
>>> 15   15.679493 -0.015946371
>>> 16    5.397496  0.027787751
>>> 17   14.479558 -0.024691465
>>> 18   16.388451 -0.030843356
>>> 19   18.326202 -0.098614391
>>> 20   12.031710 -0.010605874
>>> 21   10.352634 -0.008145436
>>> 22   14.501495 -0.038909443
>>> 23   14.415786  0.009461663
>>> 24    6.915476  0.025257749
>>> 25    7.247051  0.016988203
>>> 26   14.519202 -0.026469286
>>> 27   16.403880 -0.035931279
>>> 28   18.120565  0.014417736
>>> 29   15.407256 -0.019264309
>>>
>>> Thanks
>>>
>>> --
>>> >))):)   >))):)   >))):)   >))):)   >))):)   >))):)   >))):)   >))):)
>>>
>>> Maria Jose Juan Jorda
>>>
>>> AZTI - Tecnalia / Unidad de Investigaci?n Marina
>>> Herrera Kaia Portualde z/g
>>> 20110 Pasaia, Gipuzkoa, Spain
>>>
>>> Recursos Marinos y Pesquerias
>>> Depart. Biologia Animal, Vegetal y Ecologia
>>> Universidade A Coru?a
>>> Campus A Zapateira s/n
>>> 15071, A Coru?a, Spain
>>>
>>> Tel. Oficina +34-981167000 ext. 2204
>>> Tel. Mobil + 34-671072900
>>> Fax. +34981167065
>>> mjuan at pas.azti.es
>>> mjuanjorda at gmail.com
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>
>>
>>
>
>
> --
> --------------
> Greg Lee
> Biometrician
> Tasmanian Institute of Agricultural Research
> New Town Research Laboratories
> University of Tasmania
> 13 St Johns Avenue,
> New Town, 7008
> Australia
> Ph:  +613 6233 6858
> Fax: +613 6233 6145
>
>        [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



From bates at stat.wisc.edu  Wed Jan 14 15:46:52 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 14 Jan 2009 08:46:52 -0600
Subject: [R-sig-ME] how to compute Confident intervals for BLUPS for lme
	function in nlme library
In-Reply-To: <aefe4d0a0901132250r6423e7c1q17fd2b664683a4c@mail.gmail.com>
References: <9bcbdf360901130715y6859e2d5gc201ad755568daf1@mail.gmail.com>
	<54c8adaa0901131515k59de5fdamc686906cd2c89644@mail.gmail.com>
	<54c8adaa0901131703u19805ec7y50f8bff0d1ba34b0@mail.gmail.com>
	<aefe4d0a0901132250r6423e7c1q17fd2b664683a4c@mail.gmail.com>
Message-ID: <40e66e0b0901140646t77fdd3c2gda9472cf2a56525@mail.gmail.com>

On Wed, Jan 14, 2009 at 12:50 AM, Reinhold Kliegl
<reinhold.kliegl at gmail.com> wrote:
> Well, intervals() is a pretty good nlme response, I think, even if it
> was not exactly answering the question. The lme4 equivalent is
> doplot() which produces a so-called "caterpillar plot" of conditional
> means.

There's a typo there.  The function is dotplot(), not doplot().

> For example:
> ?ranef
>
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> rr1 <- ranef(fm1, postVar = TRUE)
> dotplot(rr1,scales = list(x = list(relation = 'free')))[["Subject"]]

> It is important to be clear about the distinction between random
> effect estimates of variances and correlations, provided as part of
> the model output and the conditional means for groups/units based on
> ("predicted with") them. At least, correlations computed from
> conditional means can differ strongly from the correlation estimates.
> The short story: You should only trust the model estimates. As Greg
> Lee said, conditional means are to be handled with care; you can not
> apply any of the usual inferential statistics to them. Nevertheless,
> the caterpillar plots are highly informative and diagnostic about, for
> example, whether you need a random effect to account for unit-by-unit
> variability. They may also lead you to  discover distinct subgroups
> suggestive of a fixed effect (for a future model). There is a
> manuscript at the top of my publications page for download (and
> constructive feedback) walking through an example.

I agree with all of the above.  As Reinhold says, the BLUPs are the
conditional mean vector of the distribution of the random effects
given Y = y, the observed data and evaluated at the parameter
estimates.  Sometimes I use the term "conditional modes".  For a
linear mixed model the conditional distribution is multivariate
Gaussian so the mean and the mode coincide.  In generalized linear
mixed models or nonlinear mixed models they do not necessarily
coincide and what is calculated is the conditional mode.

The intervals shown in the dotplot come from the evaluation of the
conditional means and the conditional variances of the random effects
given the observed data.  You can obtain the conditional variances by
including the argument postVar = TRUE in a call to ranef. (The
argument is "postVar" and not "condVar" because the person who
requested it used the term "posterior variance", which is what this is
if you take a Bayesian or empirical Bayes approach.)
> Reinhold Kliegl
>
> On Wed, Jan 14, 2009 at 2:03 AM, Greg Lee <sp8ial at gmail.com> wrote:
>> Hello again Maria,
>>
>> I was on autopilot when I answered the first time. The intervals()
>> function in nlme provides confidence intervals for the estimated model
>> components (fixed effects, random effect variances and correlation
>> coefficients, if present), which is not what you were asking.
>>
>> Actually, the more I think about your question the less certain I am. The
>> coefficients in a mixed effects model are (potentially) comprised of both
>> fixed- and random-effects. But the random effects are not "estimated" so
>> much as "predicted". The only (assumed) requirement on them is that they be
>> normally distributed, with standard deviation estimates as reported by
>> intervals().
>>
>> Does it even make sense to seek confidence intervals in this context? I am
>> not sure, and leave it for wiser heads to respond.
>>
>> Regards,
>> Greg
>>
>>
>>
>> 2009/1/14 Greg Lee <sp8ial at gmail.com>
>>
>>> Hi Maria,
>>>
>>> Try:
>>>
>>> intervals(ssb_model2.1)
>>>
>>> Cheers,
>>> Greg
>>>
>>>
>>>
>>> 2009/1/14 Maria Jose Juan Jorda <mjuanjorda at gmail.com>
>>>
>>>>  Dear all,
>>>>
>>>>
>>>> How do you compute the confident intervals for the estimations of the
>>>> random
>>>> coefficients? I have not seem a lot of discussion about this in the forum.
>>>> I
>>>> wonder why. Is there any theory background about this?
>>>>
>>>> Description of my data:
>>>>
>>>> I have 29 time series of data, biomass over time for 29 populations. I
>>>> want
>>>> a hierarchical linear model with varying intercepts and slopes. So it
>>>> computes 29 intercepts, 29 slopes and the overall mean of the intercept
>>>> and
>>>> slope and the variation among intercepts and slopes.
>>>>
>>>> *Here you have my linear mixed model:
>>>> *
>>>> ssb_model2.1<-lme(log(SSB_tonnes)~year2,random=~year2|id, data=ssb)
>>>>
>>>> *Data:*
>>>> SSB_tonnes: spawning stock biomass (ssb), continuous variable
>>>> year2: time in years, continuous variable
>>>> id: there are 29 id, in other words 29 populations of fish
>>>>
>>>> *Here, in the summary of the ouput* I find the oveall mean of the
>>>> interceps
>>>> and slopes and their variation.
>>>> *> summary(ssb_model2.1)*
>>>> Linear mixed-effects model fit by maximum likelihood
>>>>  Data: ssb
>>>>       AIC      BIC    logLik
>>>>  1575.647 1605.880 -781.8235
>>>>
>>>> Random effects:
>>>>  Formula: ~year2 | id
>>>>  Structure: General positive-definite, Log-Cholesky parametrization
>>>>            StdDev     Corr
>>>> (Intercept) 3.30025584 (Intr)
>>>> year2       0.02960239 -0.674
>>>> Residual    0.42259598
>>>>
>>>> Fixed effects: log(SSB_tonnes) ~ year2
>>>>                Value Std.Error   DF   t-value p-value
>>>> (Intercept) 13.190739 0.6271420 1110 21.033098  0.0000
>>>> year2       -0.019053 0.0058418 1110 -3.261529  0.0011
>>>>  Correlation:
>>>>      (Intr)
>>>> year2 -0.69
>>>>
>>>> Standardized Within-Group Residuals:
>>>>        Min          Q1         Med          Q3         Max
>>>> -5.34384192 -0.39116381  0.04824214  0.41871509  4.60300635
>>>>
>>>> Number of Observations: 1140
>>>> Number of Groups: 29
>>>>
>>>> *Here I have the estimations for the random coefficient. 29 interceps and
>>>> 29
>>>> slopes. How do you compute the confidence intervals for these estimations?
>>>> please any advise.
>>>>  *
>>>> *> coef(ssb_model2.1)*
>>>>   (Intercept)        year2
>>>> 1    11.490207 -0.017008685
>>>> 2    11.169138  0.008690365
>>>> 3    13.373769 -0.030298582
>>>> 4    13.680485 -0.024891089
>>>> 5    15.498426 -0.033080606
>>>> 6    14.869951 -0.012568273
>>>> 7    13.016336 -0.025133918
>>>> 8    13.968938 -0.030511346
>>>> 9    13.079165 -0.017713727
>>>> 10   12.935920 -0.054212415
>>>> 11    9.124597 -0.026398183
>>>> 12   11.493282 -0.005128911
>>>> 13   10.371121  0.012636602
>>>> 14   18.273836 -0.081414152
>>>> 15   15.679493 -0.015946371
>>>> 16    5.397496  0.027787751
>>>> 17   14.479558 -0.024691465
>>>> 18   16.388451 -0.030843356
>>>> 19   18.326202 -0.098614391
>>>> 20   12.031710 -0.010605874
>>>> 21   10.352634 -0.008145436
>>>> 22   14.501495 -0.038909443
>>>> 23   14.415786  0.009461663
>>>> 24    6.915476  0.025257749
>>>> 25    7.247051  0.016988203
>>>> 26   14.519202 -0.026469286
>>>> 27   16.403880 -0.035931279
>>>> 28   18.120565  0.014417736
>>>> 29   15.407256 -0.019264309
>>>>
>>>> Thanks
>>>>
>>>> --
>>>> >))):)   >))):)   >))):)   >))):)   >))):)   >))):)   >))):)   >))):)
>>>>
>>>> Maria Jose Juan Jorda
>>>>
>>>> AZTI - Tecnalia / Unidad de Investigaci?n Marina
>>>> Herrera Kaia Portualde z/g
>>>> 20110 Pasaia, Gipuzkoa, Spain
>>>>
>>>> Recursos Marinos y Pesquerias
>>>> Depart. Biologia Animal, Vegetal y Ecologia
>>>> Universidade A Coru?a
>>>> Campus A Zapateira s/n
>>>> 15071, A Coru?a, Spain
>>>>
>>>> Tel. Oficina +34-981167000 ext. 2204
>>>> Tel. Mobil + 34-671072900
>>>> Fax. +34981167065
>>>> mjuan at pas.azti.es
>>>> mjuanjorda at gmail.com
>>>>
>>>>        [[alternative HTML version deleted]]
>>>>
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>>
>>>
>>>
>>>
>>
>>
>> --
>> --------------
>> Greg Lee
>> Biometrician
>> Tasmanian Institute of Agricultural Research
>> New Town Research Laboratories
>> University of Tasmania
>> 13 St Johns Avenue,
>> New Town, 7008
>> Australia
>> Ph:  +613 6233 6858
>> Fax: +613 6233 6145
>>
>>        [[alternative HTML version deleted]]
>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From mjuanjorda at gmail.com  Wed Jan 14 16:55:52 2009
From: mjuanjorda at gmail.com (Maria Jose Juan Jorda)
Date: Wed, 14 Jan 2009 16:55:52 +0100
Subject: [R-sig-ME] how to compute Confident intervals for BLUPS for lme
	function in nlme library
In-Reply-To: <40e66e0b0901140646t77fdd3c2gda9472cf2a56525@mail.gmail.com>
References: <9bcbdf360901130715y6859e2d5gc201ad755568daf1@mail.gmail.com>
	<54c8adaa0901131515k59de5fdamc686906cd2c89644@mail.gmail.com>
	<54c8adaa0901131703u19805ec7y50f8bff0d1ba34b0@mail.gmail.com>
	<aefe4d0a0901132250r6423e7c1q17fd2b664683a4c@mail.gmail.com>
	<40e66e0b0901140646t77fdd3c2gda9472cf2a56525@mail.gmail.com>
Message-ID: <9bcbdf360901140755k199e7f39g44705a8da81e2b8d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090114/67948f19/attachment.pl>

From n.l.pace at utah.edu  Wed Jan 14 17:50:08 2009
From: n.l.pace at utah.edu (Nathan Leon Pace, MD, MStat)
Date: Wed, 14 Jan 2009 09:50:08 -0700
Subject: [R-sig-ME] Mixed Model Overview
In-Reply-To: <40e66e0b0901140646t77fdd3c2gda9472cf2a56525@mail.gmail.com>
Message-ID: <C59365D0.178A3%n.l.pace@utah.edu>

I ask the indulgence of the group for some assistance in a short
lecture/seminar.

I'm suppose to give a 30 minute overview of mixed models (GL, NL, and GNL)
to a small group of mostly physician researchers - some of whom have
experience using the NONMEM package to do pharmacokinetic/pharmacodynamic
population modeling.

I looked at some my favorite archives/blogs/academic sites, but can't find a
existing outline, syllabus, .ppt file, etc, for such a purpose.

If someone can point me toward an example, I'd appreciate seeing how someone
else did a similar presentation.

Nathan
-- 
Nathan Leon Pace, MD, MStat
University of Utah
n.l.pace at utah.edu
W: 801.581.6393
F: 801.581.4367
M: 801.205.1019



From kyler at mail.smu.edu  Wed Jan 14 18:52:00 2009
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Wed, 14 Jan 2009 11:52:00 -0600
Subject: [R-sig-ME] School Sampling
Message-ID: <551E1CBE65B7EB44B9DF69AF8ED0BE7F03ACE119@SXMBXA.systems.smu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090114/1c1c9af2/attachment.pl>

From danielezrajohnson at gmail.com  Wed Jan 14 19:00:07 2009
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Wed, 14 Jan 2009 18:00:07 +0000
Subject: [R-sig-ME] School Sampling
In-Reply-To: <551E1CBE65B7EB44B9DF69AF8ED0BE7F03ACE119@SXMBXA.systems.smu.edu>
References: <551E1CBE65B7EB44B9DF69AF8ED0BE7F03ACE119@SXMBXA.systems.smu.edu>
Message-ID: <a46630750901141000gafee238tf75daf5e7b32493d@mail.gmail.com>

>> z<-data.frame(school=factor(rep(1:5, each=2)), score=21:30)
>> school.sample<-sample(levels(z$school), 3, replace=T)
>> school.sample
> [1] "5" "1" "2"
>> new<-subset(z, school==school.sample)
> Warning messages:
> 1: In is.na(e1) | is.na(e2) :
>  longer object length is not a multiple of shorter object length
> 2: In `==.default`(school, school.sample) :
>  longer object length is not a multiple of shorter object length

is it

> new <- subset(z,school%in%school.sample)

that gives you the result you want?



From kyler at mail.smu.edu  Wed Jan 14 19:02:27 2009
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Wed, 14 Jan 2009 12:02:27 -0600
Subject: [R-sig-ME] School Sampling
In-Reply-To: <a46630750901141000gafee238tf75daf5e7b32493d@mail.gmail.com>
References: <551E1CBE65B7EB44B9DF69AF8ED0BE7F03ACE119@SXMBXA.systems.smu.edu>
	<a46630750901141000gafee238tf75daf5e7b32493d@mail.gmail.com>
Message-ID: <551E1CBE65B7EB44B9DF69AF8ED0BE7F03ACE11C@SXMBXA.systems.smu.edu>

Yes, but it doesn't work when schools are sampled more than once. For example:
> z<-data.frame(school=factor(rep(1:5, each=2)), score=21:30)
> school.sample<-sample(levels(z$school), 3, replace=T)
> school.sample
[1] "5" "5" "4"
> new<-subset(z, school%in%school.sample)
> new
   school score
7       4    27
8       4    28
9       5    29
10      5    30

*********************************************************
Dr. J. Kyle Roberts
Department of Teaching and Learning
Annette Caldwell Simmons School of Education 
   and Human Development
Southern Methodist University
P.O. Box 750381
Dallas, TX? 75275
214-768-4494
http://www.hlm-online.com/
*********************************************************


-----Original Message-----
From: Daniel Ezra Johnson [mailto:danielezrajohnson at gmail.com] 
Sent: Wednesday, January 14, 2009 12:00 PM
To: Roberts, Kyle
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] School Sampling

>> z<-data.frame(school=factor(rep(1:5, each=2)), score=21:30)
>> school.sample<-sample(levels(z$school), 3, replace=T)
>> school.sample
> [1] "5" "1" "2"
>> new<-subset(z, school==school.sample)
> Warning messages:
> 1: In is.na(e1) | is.na(e2) :
>  longer object length is not a multiple of shorter object length
> 2: In `==.default`(school, school.sample) :
>  longer object length is not a multiple of shorter object length

is it

> new <- subset(z,school%in%school.sample)

that gives you the result you want?



From kyler at mail.smu.edu  Wed Jan 14 19:12:09 2009
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Wed, 14 Jan 2009 12:12:09 -0600
Subject: [R-sig-ME] School Sampling
In-Reply-To: <551E1CBE65B7EB44B9DF69AF8ED0BE7F03ACE11C@SXMBXA.systems.smu.edu>
References: <551E1CBE65B7EB44B9DF69AF8ED0BE7F03ACE119@SXMBXA.systems.smu.edu>
	<a46630750901141000gafee238tf75daf5e7b32493d@mail.gmail.com>
	<551E1CBE65B7EB44B9DF69AF8ED0BE7F03ACE11C@SXMBXA.systems.smu.edu>
Message-ID: <551E1CBE65B7EB44B9DF69AF8ED0BE7F03ACE11E@SXMBXA.systems.smu.edu>

Thanks to Daniel for this great solution!

> new<-data.frame()
> for (i in school.sample)
+ new <- rbind(new,z[z$school==i,])
> new
    school score
9        5    29
10       5    30
91       5    29
101      5    30
7        4    27
8        4    28

*********************************************************
Dr. J. Kyle Roberts
Department of Teaching and Learning
Annette Caldwell Simmons School of Education 
   and Human Development
Southern Methodist University
P.O. Box 750381
Dallas, TX? 75275
214-768-4494
http://www.hlm-online.com/
*********************************************************


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Roberts, Kyle
Sent: Wednesday, January 14, 2009 12:02 PM
To: 'Daniel Ezra Johnson'
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] School Sampling

Yes, but it doesn't work when schools are sampled more than once. For example:
> z<-data.frame(school=factor(rep(1:5, each=2)), score=21:30)
> school.sample<-sample(levels(z$school), 3, replace=T)
> school.sample
[1] "5" "5" "4"
> new<-subset(z, school%in%school.sample)
> new
   school score
7       4    27
8       4    28
9       5    29
10      5    30

*********************************************************
Dr. J. Kyle Roberts
Department of Teaching and Learning
Annette Caldwell Simmons School of Education 
   and Human Development
Southern Methodist University
P.O. Box 750381
Dallas, TX? 75275
214-768-4494
http://www.hlm-online.com/
*********************************************************


-----Original Message-----
From: Daniel Ezra Johnson [mailto:danielezrajohnson at gmail.com] 
Sent: Wednesday, January 14, 2009 12:00 PM
To: Roberts, Kyle
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] School Sampling

>> z<-data.frame(school=factor(rep(1:5, each=2)), score=21:30)
>> school.sample<-sample(levels(z$school), 3, replace=T)
>> school.sample
> [1] "5" "1" "2"
>> new<-subset(z, school==school.sample)
> Warning messages:
> 1: In is.na(e1) | is.na(e2) :
>  longer object length is not a multiple of shorter object length
> 2: In `==.default`(school, school.sample) :
>  longer object length is not a multiple of shorter object length

is it

> new <- subset(z,school%in%school.sample)

that gives you the result you want?

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From Thierry.ONKELINX at inbo.be  Thu Jan 15 11:13:12 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Thu, 15 Jan 2009 11:13:12 +0100
Subject: [R-sig-ME] heteroscedastic model in lme4
In-Reply-To: <ED7B522EE00C9A4FA515AA71724D61EE01C649D9@DC1EXCL01.air.org>
References: <mailman.5.1229684401.6913.r-sig-mixed-models@r-project.org><fc.004c4d193ab9516e3b9aca00882f2171.3ab97563@umit.maine.edu>
	<ED7B522EE00C9A4FA515AA71724D61EE01C649D9@DC1EXCL01.air.org>
Message-ID: <2E9C414912813E4EB981326983E0A10405FA09B0@inboexch.inbo.be>

Dear all,

I would like to analyse some spatial data with mixed model. As I'm
dealing with presence/absence data or counts I should use the bionomial
or poisson family. These families are implemented in lme4 but
correlation structures are not. I'm wondering if the steps from section
5 in Pinheiro and Bates can be applied in case of a GLMM. If one can do
that, should one apply the transformation on the response in the
original scale or the transformed (logit / log) scale?

Another, more approximate, solution might be to code the GLMM as a NLMM.
E.g. glmer(Count ~ A + B + (1|Group), family = poisson) versus
nlme(model = Count ~ exp(mu), fixed = mu ~ A + B, random = mu ~ Group)
Any ideas on that?

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of
data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] Namens Doran, Harold
Verzonden: vrijdag 19 december 2008 20:52
Aan: Alan Cobo-Lewis; r-sig-mixed-models at r-project.org
Onderwerp: Re: [R-sig-ME] heteroscedastic model in lme4

This isn't an entirely accurate statement. nlme has built-in functions
that implement the methods for correlational and variance structures as
described in section 5 of Pinhiero and Bates. lme4 doesn't have these
functions built in as does nlme, but those same methods can be
implemented by the user and then the data can be analyzed using
functions in lme4. So, functions in lme4 can "handle" the same issues as
nlme, it just requires the user to perform the steps described in PB
section 5 et seq on their own. 




-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org on behalf of Alan
Cobo-Lewis
Sent: Fri 12/19/2008 11:19 AM
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] heteroscedastic model in lme4


Anna,

lme4 cannot handle certain kinds of heteroscedasticity, but I believe it
can handle the kind you have in mind. Search the r-sig-mixed-models
archive for a discussion involving me and David Afshartous, especially
the summary message titled
"[R-sig-ME] random effect variance per treatment group in lmer" that
David posted 07/13/2007 04:18:08 PM

I can't be certain that the suggestion below would work without knowing
more about your design, but if width were a factor with three levels
then you might try setting up indicator variables Wind1, Wind2, and
Wind3 (that each take on the value 1
when a site is at the indicator's target width and 0 otherwise) and then
fit the model with something like
mrem <- lmer( log(Nhat+1)~Group + GreenPerc + sess + crop + VegDensity +
Group:sess + Group:VegDensity + (0+Wind1|site) + (0+Wind2|site) +
(0+Wind3|site), data=all, method="REML" )

alan


r-sig-mixed-models at r-project.org on Friday, December 19, 2008 at 6:00 AM
-0500 wrote:
>Message: 1
>Date: Thu, 18 Dec 2008 11:23:46 +0000
>From: "Renwick, A. R." <a.renwick at abdn.ac.uk>
>Subject: [R-sig-ME] heteroscedastic model in lme4
>To: "'r-sig-mixed-models at r-project.org'"
>	<r-sig-mixed-models at r-project.org>
>Message-ID:
>
<B9D1301370916C44B5874AF340C18B9B28AE890D50 at VMAILB.uoa.abdn.ac.uk>
>Content-Type: text/plain; charset="us-ascii"
>
>I have been using the nlme package to run some LMM's, however I would
like to try rerunning them using the lme4 package so that I can use mcmc
sampling.  The data I am using shows some heteroscesdasticity of the
within error group and so I have
>been using the 'weights' argument and the varIdent variance function
structure to allow different variances for each level of my factor
(patch width).
>
>My problem is how to code for a heteroscedastic model in lme4 and any
suggestion wouuld be much apprecaited.
>
>The code I used in the nlme package:
>
># model fit using "REML"
>mrem<-lme(log(Nhat+1)~Group + GreenPerc + sess + crop + VegDensity +
Group:sess + Group:VegDensity ,random=~1|Site, data=all,
>       method="REML",correlation=NULL,weights=varIdent(form=~1|width))
>
>
>Many thanks,
>Anna
>
>Anna Renwick
>Institute of Biological & Environment Sciences
>University of Aberdeen
>Zoology Building
>Tillydrone Avenue
>Aberdeen
>AB24 2TZ
>
>
>The University of Aberdeen is a charity registered in Scotland, No
SC013683.


--
Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
Department of Psychology		(207) 581-6128 fax
University of Maine
Orono, ME 04469-5742     		alanc at maine.edu

http://www.umaine.edu/visualperception

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From vmuggeo at dssm.unipa.it  Thu Jan 15 13:21:29 2009
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Thu, 15 Jan 2009 13:21:29 +0100
Subject: [R-sig-ME] heteroscedastic model in lme4
In-Reply-To: <2E9C414912813E4EB981326983E0A10405FA09B0@inboexch.inbo.be>
References: <mailman.5.1229684401.6913.r-sig-mixed-models@r-project.org><fc.004c4d193ab9516e3b9aca00882f2171.3ab97563@umit.maine.edu>	<ED7B522EE00C9A4FA515AA71724D61EE01C649D9@DC1EXCL01.air.org>
	<2E9C414912813E4EB981326983E0A10405FA09B0@inboexch.inbo.be>
Message-ID: <496F2A49.7070701@dssm.unipa.it>

dear Thierry,
I am adding a simple comment only on your second point.

If I am not wrong, I think that the two alternatives underlie different 
models

1)glmer(.., family = poisson) assumes a real Poisson distribution for 
your response y (conditioned to random effects), i.e. y=rpois(n,exp(mu)).

2) nlme(..) assumes a gaussian distribution for your response with a 
nonlinear mean model, i.e. y=rnorm(n,exp(mu))

Another (different) approach would be lmer() with log-transformed data, 
i.e. y=exp(rnorm(n,mu))

Probably, in a pure likelihood framework the first approach should be 
preferred if you have real count data..

Hope this helps,

vito




ONKELINX, Thierry ha scritto:
> Dear all,
> 
> I would like to analyse some spatial data with mixed model. As I'm
> dealing with presence/absence data or counts I should use the bionomial
> or poisson family. These families are implemented in lme4 but
> correlation structures are not. I'm wondering if the steps from section
> 5 in Pinheiro and Bates can be applied in case of a GLMM. If one can do
> that, should one apply the transformation on the response in the
> original scale or the transformed (logit / log) scale?
> 
> Another, more approximate, solution might be to code the GLMM as a NLMM.
> E.g. glmer(Count ~ A + B + (1|Group), family = poisson) versus
> nlme(model = Count ~ exp(mu), fixed = mu ~ A + B, random = mu ~ Group)
> Any ideas on that?
> 
> Thierry
> 
> ------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium 
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be 
> www.inbo.be 
> 
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to
> say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
> 
> The plural of anecdote is not data.
> ~ Roger Brinner
> 
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of
> data.
> ~ John Tukey
> 
> -----Oorspronkelijk bericht-----
> Van: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Doran, Harold
> Verzonden: vrijdag 19 december 2008 20:52
> Aan: Alan Cobo-Lewis; r-sig-mixed-models at r-project.org
> Onderwerp: Re: [R-sig-ME] heteroscedastic model in lme4
> 
> This isn't an entirely accurate statement. nlme has built-in functions
> that implement the methods for correlational and variance structures as
> described in section 5 of Pinhiero and Bates. lme4 doesn't have these
> functions built in as does nlme, but those same methods can be
> implemented by the user and then the data can be analyzed using
> functions in lme4. So, functions in lme4 can "handle" the same issues as
> nlme, it just requires the user to perform the steps described in PB
> section 5 et seq on their own. 
> 
> 
> 
> 
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org on behalf of Alan
> Cobo-Lewis
> Sent: Fri 12/19/2008 11:19 AM
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] heteroscedastic model in lme4
> 
> 
> Anna,
> 
> lme4 cannot handle certain kinds of heteroscedasticity, but I believe it
> can handle the kind you have in mind. Search the r-sig-mixed-models
> archive for a discussion involving me and David Afshartous, especially
> the summary message titled
> "[R-sig-ME] random effect variance per treatment group in lmer" that
> David posted 07/13/2007 04:18:08 PM
> 
> I can't be certain that the suggestion below would work without knowing
> more about your design, but if width were a factor with three levels
> then you might try setting up indicator variables Wind1, Wind2, and
> Wind3 (that each take on the value 1
> when a site is at the indicator's target width and 0 otherwise) and then
> fit the model with something like
> mrem <- lmer( log(Nhat+1)~Group + GreenPerc + sess + crop + VegDensity +
> Group:sess + Group:VegDensity + (0+Wind1|site) + (0+Wind2|site) +
> (0+Wind3|site), data=all, method="REML" )
> 
> alan
> 
> 
> r-sig-mixed-models at r-project.org on Friday, December 19, 2008 at 6:00 AM
> -0500 wrote:
>> Message: 1
>> Date: Thu, 18 Dec 2008 11:23:46 +0000
>> From: "Renwick, A. R." <a.renwick at abdn.ac.uk>
>> Subject: [R-sig-ME] heteroscedastic model in lme4
>> To: "'r-sig-mixed-models at r-project.org'"
>> 	<r-sig-mixed-models at r-project.org>
>> Message-ID:
>>
> <B9D1301370916C44B5874AF340C18B9B28AE890D50 at VMAILB.uoa.abdn.ac.uk>
>> Content-Type: text/plain; charset="us-ascii"
>>
>> I have been using the nlme package to run some LMM's, however I would
> like to try rerunning them using the lme4 package so that I can use mcmc
> sampling.  The data I am using shows some heteroscesdasticity of the
> within error group and so I have
>> been using the 'weights' argument and the varIdent variance function
> structure to allow different variances for each level of my factor
> (patch width).
>> My problem is how to code for a heteroscedastic model in lme4 and any
> suggestion wouuld be much apprecaited.
>> The code I used in the nlme package:
>>
>> # model fit using "REML"
>> mrem<-lme(log(Nhat+1)~Group + GreenPerc + sess + crop + VegDensity +
> Group:sess + Group:VegDensity ,random=~1|Site, data=all,
>>       method="REML",correlation=NULL,weights=varIdent(form=~1|width))
>>
>>
>> Many thanks,
>> Anna
>>
>> Anna Renwick
>> Institute of Biological & Environment Sciences
>> University of Aberdeen
>> Zoology Building
>> Tillydrone Avenue
>> Aberdeen
>> AB24 2TZ
>>
>>
>> The University of Aberdeen is a charity registered in Scotland, No
> SC013683.
> 
> 
> --
> Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
> Department of Psychology		(207) 581-6128 fax
> University of Maine
> Orono, ME 04469-5742     		alanc at maine.edu
> 
> http://www.umaine.edu/visualperception
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
> en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
> door een geldig ondertekend document. The views expressed in  this message 
> and any annex are purely those of the writer and may not be regarded as stating 
> an official position of INBO, as long as the message is not confirmed by a duly 
> signed document.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612
http://dssm.unipa.it/vmuggeo



From Thierry.ONKELINX at inbo.be  Thu Jan 15 14:47:22 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Thu, 15 Jan 2009 14:47:22 +0100
Subject: [R-sig-ME] heteroscedastic model in lme4
In-Reply-To: <496F2A49.7070701@dssm.unipa.it>
References: <mailman.5.1229684401.6913.r-sig-mixed-models@r-project.org><fc.004c4d193ab9516e3b9aca00882f2171.3ab97563@umit.maine.edu>	<ED7B522EE00C9A4FA515AA71724D61EE01C649D9@DC1EXCL01.air.org>
	<2E9C414912813E4EB981326983E0A10405FA09B0@inboexch.inbo.be>
	<496F2A49.7070701@dssm.unipa.it>
Message-ID: <2E9C414912813E4EB981326983E0A10405FA0A87@inboexch.inbo.be>

Dear Vito,

I aggree that the glmer() and nlme() examples assume different distribution. That's why I called the nlme() version an apprioximation. If the steps described in P&B are only valid for linear models but not in the generalised models, then I have a dilemma. With glmer() I can use the appropriate distribution but a wrong correlation structure. A structure of which I'm certain that it is there (spatially clustered points). nlme() allows me to model the correlation structure but only unther the gaussion distribution. The latter is in my opinion a better alternative given that with enough data the residuals will behave approximately gaussian. Please do correct me if that is an incorrect statement.

Why nlme() and not lme() with a log-transformation? Well: the zero's in counts. Using a log(x + 1) transformation complicates the interpretation of the model. And what transformation would you suggest with binomial data? nlme() handles zero's with a log-link as well as true-false data with a logit-link.

Thierry

----------------------------------------------------------------------------
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics, methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: vito muggeo [mailto:vmuggeo at dssm.unipa.it] 
Verzonden: donderdag 15 januari 2009 13:21
Aan: ONKELINX, Thierry
CC: Doran, Harold; Alan Cobo-Lewis; r-sig-mixed-models at r-project.org
Onderwerp: Re: [R-sig-ME] heteroscedastic model in lme4

dear Thierry,
I am adding a simple comment only on your second point.

If I am not wrong, I think that the two alternatives underlie different 
models

1)glmer(.., family = poisson) assumes a real Poisson distribution for 
your response y (conditioned to random effects), i.e. y=rpois(n,exp(mu)).

2) nlme(..) assumes a gaussian distribution for your response with a 
nonlinear mean model, i.e. y=rnorm(n,exp(mu))

Another (different) approach would be lmer() with log-transformed data, 
i.e. y=exp(rnorm(n,mu))

Probably, in a pure likelihood framework the first approach should be 
preferred if you have real count data..

Hope this helps,

vito




ONKELINX, Thierry ha scritto:
> Dear all,
> 
> I would like to analyse some spatial data with mixed model. As I'm
> dealing with presence/absence data or counts I should use the bionomial
> or poisson family. These families are implemented in lme4 but
> correlation structures are not. I'm wondering if the steps from section
> 5 in Pinheiro and Bates can be applied in case of a GLMM. If one can do
> that, should one apply the transformation on the response in the
> original scale or the transformed (logit / log) scale?
> 
> Another, more approximate, solution might be to code the GLMM as a NLMM.
> E.g. glmer(Count ~ A + B + (1|Group), family = poisson) versus
> nlme(model = Count ~ exp(mu), fixed = mu ~ A + B, random = mu ~ Group)
> Any ideas on that?
> 
> Thierry
> 
> ------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium 
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be 
> www.inbo.be 
> 
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to
> say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
> 
> The plural of anecdote is not data.
> ~ Roger Brinner
> 
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of
> data.
> ~ John Tukey
> 
> -----Oorspronkelijk bericht-----
> Van: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Doran, Harold
> Verzonden: vrijdag 19 december 2008 20:52
> Aan: Alan Cobo-Lewis; r-sig-mixed-models at r-project.org
> Onderwerp: Re: [R-sig-ME] heteroscedastic model in lme4
> 
> This isn't an entirely accurate statement. nlme has built-in functions
> that implement the methods for correlational and variance structures as
> described in section 5 of Pinhiero and Bates. lme4 doesn't have these
> functions built in as does nlme, but those same methods can be
> implemented by the user and then the data can be analyzed using
> functions in lme4. So, functions in lme4 can "handle" the same issues as
> nlme, it just requires the user to perform the steps described in PB
> section 5 et seq on their own. 
> 
> 
> 
> 
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org on behalf of Alan
> Cobo-Lewis
> Sent: Fri 12/19/2008 11:19 AM
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] heteroscedastic model in lme4
> 
> 
> Anna,
> 
> lme4 cannot handle certain kinds of heteroscedasticity, but I believe it
> can handle the kind you have in mind. Search the r-sig-mixed-models
> archive for a discussion involving me and David Afshartous, especially
> the summary message titled
> "[R-sig-ME] random effect variance per treatment group in lmer" that
> David posted 07/13/2007 04:18:08 PM
> 
> I can't be certain that the suggestion below would work without knowing
> more about your design, but if width were a factor with three levels
> then you might try setting up indicator variables Wind1, Wind2, and
> Wind3 (that each take on the value 1
> when a site is at the indicator's target width and 0 otherwise) and then
> fit the model with something like
> mrem <- lmer( log(Nhat+1)~Group + GreenPerc + sess + crop + VegDensity +
> Group:sess + Group:VegDensity + (0+Wind1|site) + (0+Wind2|site) +
> (0+Wind3|site), data=all, method="REML" )
> 
> alan
> 
> 
> r-sig-mixed-models at r-project.org on Friday, December 19, 2008 at 6:00 AM
> -0500 wrote:
>> Message: 1
>> Date: Thu, 18 Dec 2008 11:23:46 +0000
>> From: "Renwick, A. R." <a.renwick at abdn.ac.uk>
>> Subject: [R-sig-ME] heteroscedastic model in lme4
>> To: "'r-sig-mixed-models at r-project.org'"
>> 	<r-sig-mixed-models at r-project.org>
>> Message-ID:
>>
> <B9D1301370916C44B5874AF340C18B9B28AE890D50 at VMAILB.uoa.abdn.ac.uk>
>> Content-Type: text/plain; charset="us-ascii"
>>
>> I have been using the nlme package to run some LMM's, however I would
> like to try rerunning them using the lme4 package so that I can use mcmc
> sampling.  The data I am using shows some heteroscesdasticity of the
> within error group and so I have
>> been using the 'weights' argument and the varIdent variance function
> structure to allow different variances for each level of my factor
> (patch width).
>> My problem is how to code for a heteroscedastic model in lme4 and any
> suggestion wouuld be much apprecaited.
>> The code I used in the nlme package:
>>
>> # model fit using "REML"
>> mrem<-lme(log(Nhat+1)~Group + GreenPerc + sess + crop + VegDensity +
> Group:sess + Group:VegDensity ,random=~1|Site, data=all,
>>       method="REML",correlation=NULL,weights=varIdent(form=~1|width))
>>
>>
>> Many thanks,
>> Anna
>>
>> Anna Renwick
>> Institute of Biological & Environment Sciences
>> University of Aberdeen
>> Zoology Building
>> Tillydrone Avenue
>> Aberdeen
>> AB24 2TZ
>>
>>
>> The University of Aberdeen is a charity registered in Scotland, No
> SC013683.
> 
> 
> --
> Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
> Department of Psychology		(207) 581-6128 fax
> University of Maine
> Orono, ME 04469-5742     		alanc at maine.edu
> 
> http://www.umaine.edu/visualperception
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
> en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
> door een geldig ondertekend document. The views expressed in  this message 
> and any annex are purely those of the writer and may not be regarded as stating 
> an official position of INBO, as long as the message is not confirmed by a duly 
> signed document.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612
http://dssm.unipa.it/vmuggeo
====================================

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From HDoran at air.org  Thu Jan 15 14:50:51 2009
From: HDoran at air.org (Doran, Harold)
Date: Thu, 15 Jan 2009 08:50:51 -0500
Subject: [R-sig-ME] heteroscedastic model in lme4
In-Reply-To: <8D9383E1-4501-4996-B81E-C498A612CAAE@me.com>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE01D47858@DC1EXCL01.air.org>

Arrrg, that whole level 1 level 2 thing eats me up. There are random
effects and everything else is just a covariate. Not quite sure what
your data are, but a random slope can account for the non-constant
variance. Just a random intercept is compound symmetry where random
intercept and slope is a more general covariance structure that allows
for the variance to be different over time.



> -----Original Message-----
> From: Rense Nieuwenhuis [mailto:rense.nieuwenhuis at gmail.com] 
> On Behalf Of Rense Nieuwenhuis
> Sent: Thursday, January 15, 2009 8:01 AM
> To: ONKELINX, Thierry
> Cc: Doran, Harold; Alan Cobo-Lewis; r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] heteroscedastic model in lme4
> 
> Dear all,
> 
> I'm currently working on a similar problem in which I expect 
> the within-group variance to change over years. Of course, in 
> an ideal world correlation structures would be readily 
> available in lme4, but as Thierry already states, this is not 
> the case.
> 
> I have been considering a different approach to solve the 
> problem, based on statements on heteroscedasticity made by 
> Snijders & Bosker (1999, page 119). This only works for 
> heteroscedasticity at the second (or higher) level, and if 
> this heteroscedasticity can be related to an observed variable.
> 
> He argues that the random part of the second level can be defined as:
> U0j + U1j*zj
> "Thus, strange as it may sound, the level-two variable Z 
> formally gets a random slope at level two".
> 
> In my understanding, this would lead to something like:
> 
> glmer(Count ~ A + B + (B|Group), family=poisson)
> 
> assuming that B measures a  'group'-level characteristic.
> 
> I'm still trying to get a firm grasp on the matter, but 
> possibly this solution can help out in certain situations,
> 
> kind regards,
> 
> Rense Nieuwenhuis
> 
> 
> 
> 
> 
> On 15 jan 2009, at 11:13, ONKELINX, Thierry wrote:
> 
> > glmer(Count ~ A + B + (1|Group), family = poisson)
> 
> 
> 
> 
> 
> @book{Snijders:1999,
> 	Author = {Snijders, Tom A.B. and Bosker, Roel J.},
> 	Date-Added = {2009-01-15 13:48:53 +0100},
> 	Date-Modified = {2009-01-15 13:49:35 +0100},
> 	Publisher = {Sage},
> 	Title = {Multilevel Analysis, an introduction to basic 
> and advanced multilevel modelling},
> 	Year = {1999}}
> 
> 



From HDoran at air.org  Thu Jan 15 15:10:05 2009
From: HDoran at air.org (Doran, Harold)
Date: Thu, 15 Jan 2009 09:10:05 -0500
Subject: [R-sig-ME] heteroscedastic model in lme4
In-Reply-To: <2E9C414912813E4EB981326983E0A10405FA09B0@inboexch.inbo.be>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE01D4785A@DC1EXCL01.air.org>

I would think on the transformed data. In a GLMM an offset is applied on
the transformed data and not on the original data, which is what makes
me think the same would be used here.

> -----Original Message-----
> From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be] 
> Sent: Thursday, January 15, 2009 5:13 AM
> To: Doran, Harold; Alan Cobo-Lewis; r-sig-mixed-models at r-project.org
> Subject: RE: [R-sig-ME] heteroscedastic model in lme4
> 
> Dear all,
> 
> I would like to analyse some spatial data with mixed model. 
> As I'm dealing with presence/absence data or counts I should 
> use the bionomial or poisson family. These families are 
> implemented in lme4 but correlation structures are not. I'm 
> wondering if the steps from section
> 5 in Pinheiro and Bates can be applied in case of a GLMM. If 
> one can do that, should one apply the transformation on the 
> response in the original scale or the transformed (logit / log) scale?
> 
> Another, more approximate, solution might be to code the GLMM 
> as a NLMM.
> E.g. glmer(Count ~ A + B + (1|Group), family = poisson) 
> versus nlme(model = Count ~ exp(mu), fixed = mu ~ A + B, 
> random = mu ~ Group) Any ideas on that?
> 
> Thierry
> 
> --------------------------------------------------------------
> ----------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute 
> for Nature and Forest Cel biometrie, methodologie en 
> kwaliteitszorg / Section biometrics, methodology and quality 
> assurance Gaverstraat 4 9500 Geraardsbergen Belgium tel. + 32 
> 54/436 185 Thierry.Onkelinx at inbo.be www.inbo.be 
> 
> To call in the statistician after the experiment is done may 
> be no more than asking him to perform a post-mortem 
> examination: he may be able to say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
> 
> The plural of anecdote is not data.
> ~ Roger Brinner
> 
> The combination of some data and an aching desire for an 
> answer does not ensure that a reasonable answer can be 
> extracted from a given body of data.
> ~ John Tukey
> 
> -----Oorspronkelijk bericht-----
> Van: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Doran, Harold
> Verzonden: vrijdag 19 december 2008 20:52
> Aan: Alan Cobo-Lewis; r-sig-mixed-models at r-project.org
> Onderwerp: Re: [R-sig-ME] heteroscedastic model in lme4
> 
> This isn't an entirely accurate statement. nlme has built-in 
> functions that implement the methods for correlational and 
> variance structures as described in section 5 of Pinhiero and 
> Bates. lme4 doesn't have these functions built in as does 
> nlme, but those same methods can be implemented by the user 
> and then the data can be analyzed using functions in lme4. 
> So, functions in lme4 can "handle" the same issues as nlme, 
> it just requires the user to perform the steps described in 
> PB section 5 et seq on their own. 
> 
> 
> 
> 
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org on behalf of 
> Alan Cobo-Lewis
> Sent: Fri 12/19/2008 11:19 AM
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] heteroscedastic model in lme4
>  
> 
> Anna,
> 
> lme4 cannot handle certain kinds of heteroscedasticity, but I 
> believe it can handle the kind you have in mind. Search the 
> r-sig-mixed-models archive for a discussion involving me and 
> David Afshartous, especially the summary message titled 
> "[R-sig-ME] random effect variance per treatment group in 
> lmer" that David posted 07/13/2007 04:18:08 PM
> 
> I can't be certain that the suggestion below would work 
> without knowing more about your design, but if width were a 
> factor with three levels then you might try setting up 
> indicator variables Wind1, Wind2, and
> Wind3 (that each take on the value 1
> when a site is at the indicator's target width and 0 
> otherwise) and then fit the model with something like mrem <- 
> lmer( log(Nhat+1)~Group + GreenPerc + sess + crop + 
> VegDensity + Group:sess + Group:VegDensity + (0+Wind1|site) + 
> (0+Wind2|site) + (0+Wind3|site), data=all, method="REML" )
> 
> alan
> 
> 
> r-sig-mixed-models at r-project.org on Friday, December 19, 2008 
> at 6:00 AM -0500 wrote:
> >Message: 1
> >Date: Thu, 18 Dec 2008 11:23:46 +0000
> >From: "Renwick, A. R." <a.renwick at abdn.ac.uk>
> >Subject: [R-sig-ME] heteroscedastic model in lme4
> >To: "'r-sig-mixed-models at r-project.org'"
> >	<r-sig-mixed-models at r-project.org>
> >Message-ID:
> >
> <B9D1301370916C44B5874AF340C18B9B28AE890D50 at VMAILB.uoa.abdn.ac.uk>
> >Content-Type: text/plain; charset="us-ascii"
> >
> >I have been using the nlme package to run some LMM's, however I would
> like to try rerunning them using the lme4 package so that I 
> can use mcmc sampling.  The data I am using shows some 
> heteroscesdasticity of the within error group and so I have
> >been using the 'weights' argument and the varIdent variance function
> structure to allow different variances for each level of my 
> factor (patch width).
> >
> >My problem is how to code for a heteroscedastic model in lme4 and any
> suggestion wouuld be much apprecaited.
> >
> >The code I used in the nlme package:
> >
> ># model fit using "REML"
> >mrem<-lme(log(Nhat+1)~Group + GreenPerc + sess + crop + VegDensity +
> Group:sess + Group:VegDensity ,random=~1|Site, data=all,
> >       
> method="REML",correlation=NULL,weights=varIdent(form=~1|width))
> >
> >
> >Many thanks,
> >Anna
> >
> >Anna Renwick
> >Institute of Biological & Environment Sciences University of 
> Aberdeen 
> >Zoology Building Tillydrone Avenue Aberdeen
> >AB24 2TZ
> >
> >
> >The University of Aberdeen is a charity registered in Scotland, No
> SC013683.
> 
> 
> --
> Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
> Department of Psychology		(207) 581-6128 fax
> University of Maine
> Orono, ME 04469-5742     		alanc at maine.edu
> 
> http://www.umaine.edu/visualperception
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> Dit bericht en eventuele bijlagen geven enkel de visie van de 
> schrijver weer 
> en binden het INBO onder geen enkel beding, zolang dit 
> bericht niet bevestigd is
> door een geldig ondertekend document. The views expressed in  
> this message 
> and any annex are purely those of the writer and may not be 
> regarded as stating 
> an official position of INBO, as long as the message is not 
> confirmed by a duly 
> signed document.
> 



From pierces1 at msu.edu  Thu Jan 15 15:13:02 2009
From: pierces1 at msu.edu (Steven J. Pierce)
Date: Thu, 15 Jan 2009 09:13:02 -0500
Subject: [R-sig-ME] heteroscedastic model in lme4
In-Reply-To: <2E9C414912813E4EB981326983E0A10405FA0A87@inboexch.inbo.be>
References: <mailman.5.1229684401.6913.r-sig-mixed-models@r-project.org><fc.004c4d193ab9516e3b9aca00882f2171.3ab97563@umit.maine.edu>	<ED7B522EE00C9A4FA515AA71724D61EE01C649D9@DC1EXCL01.air.org><2E9C414912813E4EB981326983E0A10405FA09B0@inboexch.inbo.be><496F2A49.7070701@dssm.unipa.it>
	<2E9C414912813E4EB981326983E0A10405FA0A87@inboexch.inbo.be>
Message-ID: <8BAE717C7321451B961640F1D213C425@TheVoid>

Have you looked at the spBayes package? That might offer an alternative
that's designed for doing spatial analyses.  


Steven J. Pierce
E-mail: pierces1 at msu.edu

-----Original Message-----
From: ONKELINX, Thierry [mailto:Thierry.ONKELINX at inbo.be] 
Sent: Thursday, January 15, 2009 8:47 AM
To: vito muggeo
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] heteroscedastic model in lme4

Dear Vito,

I aggree that the glmer() and nlme() examples assume different distribution.
That's why I called the nlme() version an apprioximation. If the steps
described in P&B are only valid for linear models but not in the generalised
models, then I have a dilemma. With glmer() I can use the appropriate
distribution but a wrong correlation structure. A structure of which I'm
certain that it is there (spatially clustered points). nlme() allows me to
model the correlation structure but only unther the gaussion distribution.
The latter is in my opinion a better alternative given that with enough data
the residuals will behave approximately gaussian. Please do correct me if
that is an incorrect statement.

Why nlme() and not lme() with a log-transformation? Well: the zero's in
counts. Using a log(x + 1) transformation complicates the interpretation of
the model. And what transformation would you suggest with binomial data?
nlme() handles zero's with a log-link as well as true-false data with a
logit-link.

Thierry

----------------------------------------------------------------------------
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance Gaverstraat 4 9500 Geraardsbergen Belgium
tel. + 32 54/436 185 Thierry.Onkelinx at inbo.be www.inbo.be 

To call in the statistician after the experiment is done may be no more than
asking him to perform a post-mortem examination: he may be able to say what
the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: vito muggeo [mailto:vmuggeo at dssm.unipa.it]
Verzonden: donderdag 15 januari 2009 13:21
Aan: ONKELINX, Thierry
CC: Doran, Harold; Alan Cobo-Lewis; r-sig-mixed-models at r-project.org
Onderwerp: Re: [R-sig-ME] heteroscedastic model in lme4

dear Thierry,
I am adding a simple comment only on your second point.

If I am not wrong, I think that the two alternatives underlie different
models

1)glmer(.., family = poisson) assumes a real Poisson distribution for your
response y (conditioned to random effects), i.e. y=rpois(n,exp(mu)).

2) nlme(..) assumes a gaussian distribution for your response with a
nonlinear mean model, i.e. y=rnorm(n,exp(mu))

Another (different) approach would be lmer() with log-transformed data, i.e.
y=exp(rnorm(n,mu))

Probably, in a pure likelihood framework the first approach should be
preferred if you have real count data..

Hope this helps,

vito




ONKELINX, Thierry ha scritto:
> Dear all,
> 
> I would like to analyse some spatial data with mixed model. As I'm 
> dealing with presence/absence data or counts I should use the 
> bionomial or poisson family. These families are implemented in lme4 
> but correlation structures are not. I'm wondering if the steps from 
> section
> 5 in Pinheiro and Bates can be applied in case of a GLMM. If one can 
> do that, should one apply the transformation on the response in the 
> original scale or the transformed (logit / log) scale?
> 
> Another, more approximate, solution might be to code the GLMM as a NLMM.
> E.g. glmer(Count ~ A + B + (1|Group), family = poisson) versus 
> nlme(model = Count ~ exp(mu), fixed = mu ~ A + B, random = mu ~ Group) 
> Any ideas on that?
> 
> Thierry
> 
> ----------------------------------------------------------------------
> --
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature 
> and Forest Cel biometrie, methodologie en kwaliteitszorg / Section 
> biometrics, methodology and quality assurance Gaverstraat 4 9500 
> Geraardsbergen Belgium tel. + 32 54/436 185 Thierry.Onkelinx at inbo.be 
> www.inbo.be
> 
> To call in the statistician after the experiment is done may be no 
> more than asking him to perform a post-mortem examination: he may be 
> able to say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
> 
> The plural of anecdote is not data.
> ~ Roger Brinner
> 
> The combination of some data and an aching desire for an answer does 
> not ensure that a reasonable answer can be extracted from a given body 
> of data.
> ~ John Tukey
> 
> -----Oorspronkelijk bericht-----
> Van: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Doran, Harold
> Verzonden: vrijdag 19 december 2008 20:52
> Aan: Alan Cobo-Lewis; r-sig-mixed-models at r-project.org
> Onderwerp: Re: [R-sig-ME] heteroscedastic model in lme4
> 
> This isn't an entirely accurate statement. nlme has built-in functions 
> that implement the methods for correlational and variance structures 
> as described in section 5 of Pinhiero and Bates. lme4 doesn't have 
> these functions built in as does nlme, but those same methods can be 
> implemented by the user and then the data can be analyzed using 
> functions in lme4. So, functions in lme4 can "handle" the same issues 
> as nlme, it just requires the user to perform the steps described in 
> PB section 5 et seq on their own.
> 
> 
> 
> 
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org on behalf of Alan 
> Cobo-Lewis
> Sent: Fri 12/19/2008 11:19 AM
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] heteroscedastic model in lme4
> 
> 
> Anna,
> 
> lme4 cannot handle certain kinds of heteroscedasticity, but I believe 
> it can handle the kind you have in mind. Search the r-sig-mixed-models 
> archive for a discussion involving me and David Afshartous, especially 
> the summary message titled "[R-sig-ME] random effect variance per 
> treatment group in lmer" that David posted 07/13/2007 04:18:08 PM
> 
> I can't be certain that the suggestion below would work without 
> knowing more about your design, but if width were a factor with three 
> levels then you might try setting up indicator variables Wind1, Wind2, 
> and
> Wind3 (that each take on the value 1
> when a site is at the indicator's target width and 0 otherwise) and 
> then fit the model with something like mrem <- lmer( log(Nhat+1)~Group 
> + GreenPerc + sess + crop + VegDensity + Group:sess + Group:VegDensity 
> + (0+Wind1|site) + (0+Wind2|site) + (0+Wind3|site), data=all, 
> method="REML" )
> 
> alan
> 
> 
> r-sig-mixed-models at r-project.org on Friday, December 19, 2008 at 6:00 
> AM -0500 wrote:
>> Message: 1
>> Date: Thu, 18 Dec 2008 11:23:46 +0000
>> From: "Renwick, A. R." <a.renwick at abdn.ac.uk>
>> Subject: [R-sig-ME] heteroscedastic model in lme4
>> To: "'r-sig-mixed-models at r-project.org'"
>> 	<r-sig-mixed-models at r-project.org>
>> Message-ID:
>>
> <B9D1301370916C44B5874AF340C18B9B28AE890D50 at VMAILB.uoa.abdn.ac.uk>
>> Content-Type: text/plain; charset="us-ascii"
>>
>> I have been using the nlme package to run some LMM's, however I would
> like to try rerunning them using the lme4 package so that I can use 
> mcmc sampling.  The data I am using shows some heteroscesdasticity of 
> the within error group and so I have
>> been using the 'weights' argument and the varIdent variance function
> structure to allow different variances for each level of my factor 
> (patch width).
>> My problem is how to code for a heteroscedastic model in lme4 and any
> suggestion wouuld be much apprecaited.
>> The code I used in the nlme package:
>>
>> # model fit using "REML"
>> mrem<-lme(log(Nhat+1)~Group + GreenPerc + sess + crop + VegDensity +
> Group:sess + Group:VegDensity ,random=~1|Site, data=all,
>>       method="REML",correlation=NULL,weights=varIdent(form=~1|width))
>>
>>
>> Many thanks,
>> Anna
>>
>> Anna Renwick
>> Institute of Biological & Environment Sciences University of Aberdeen 
>> Zoology Building Tillydrone Avenue Aberdeen
>> AB24 2TZ
>>
>>
>> The University of Aberdeen is a charity registered in Scotland, No
> SC013683.
> 
> 
> --
> Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
> Department of Psychology		(207) 581-6128 fax
> University of Maine
> Orono, ME 04469-5742     		alanc at maine.edu
> 
> http://www.umaine.edu/visualperception
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> Dit bericht en eventuele bijlagen geven enkel de visie van de 
> schrijver weer en binden het INBO onder geen enkel beding, zolang dit 
> bericht niet bevestigd is door een geldig ondertekend document. The 
> views expressed in  this message and any annex are purely those of the 
> writer and may not be regarded as stating an official position of 
> INBO, as long as the message is not confirmed by a duly signed document.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 

--
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612
http://dssm.unipa.it/vmuggeo
====================================

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer
en binden het INBO onder geen enkel beding, zolang dit bericht niet
bevestigd is door een geldig ondertekend document. The views expressed in
this message and any annex are purely those of the writer and may not be
regarded as stating an official position of INBO, as long as the message is
not confirmed by a duly signed document.



From contact at rensenieuwenhuis.nl  Thu Jan 15 14:00:39 2009
From: contact at rensenieuwenhuis.nl (Rense Nieuwenhuis)
Date: Thu, 15 Jan 2009 14:00:39 +0100
Subject: [R-sig-ME] heteroscedastic model in lme4
In-Reply-To: <2E9C414912813E4EB981326983E0A10405FA09B0@inboexch.inbo.be>
References: <mailman.5.1229684401.6913.r-sig-mixed-models@r-project.org>
	<fc.004c4d193ab9516e3b9aca00882f2171.3ab97563@umit.maine.edu>
	<ED7B522EE00C9A4FA515AA71724D61EE01C649D9@DC1EXCL01.air.org>
	<2E9C414912813E4EB981326983E0A10405FA09B0@inboexch.inbo.be>
Message-ID: <8D9383E1-4501-4996-B81E-C498A612CAAE@me.com>

Dear all,

I'm currently working on a similar problem in which I expect the  
within-group variance to change over years. Of course, in an ideal  
world correlation structures would be readily available in lme4, but  
as Thierry already states, this is not the case.

I have been considering a different approach to solve the problem,  
based on statements on heteroscedasticity made by Snijders & Bosker  
(1999, page 119). This only works for heteroscedasticity at the second  
(or higher) level, and if this heteroscedasticity can be related to an  
observed variable.

He argues that the random part of the second level can be defined as:
U0j + U1j*zj
"Thus, strange as it may sound, the level-two variable Z formally gets  
a random slope at level two".

In my understanding, this would lead to something like:

glmer(Count ~ A + B + (B|Group), family=poisson)

assuming that B measures a  'group'-level characteristic.

I'm still trying to get a firm grasp on the matter, but possibly this  
solution can help out in certain situations,

kind regards,

Rense Nieuwenhuis





On 15 jan 2009, at 11:13, ONKELINX, Thierry wrote:

> glmer(Count ~ A + B + (1|Group), family = poisson)





@book{Snijders:1999,
	Author = {Snijders, Tom A.B. and Bosker, Roel J.},
	Date-Added = {2009-01-15 13:48:53 +0100},
	Date-Modified = {2009-01-15 13:49:35 +0100},
	Publisher = {Sage},
	Title = {Multilevel Analysis, an introduction to basic and advanced  
multilevel modelling},
	Year = {1999}}



From njbisaac at googlemail.com  Fri Jan 16 12:09:59 2009
From: njbisaac at googlemail.com (Nick Isaac)
Date: Fri, 16 Jan 2009 11:09:59 +0000
Subject: [R-sig-ME] how to compute Confident intervals for BLUPS for lme
	function in nlme library
In-Reply-To: <aefe4d0a0901132250r6423e7c1q17fd2b664683a4c@mail.gmail.com>
References: <9bcbdf360901130715y6859e2d5gc201ad755568daf1@mail.gmail.com>
	<54c8adaa0901131515k59de5fdamc686906cd2c89644@mail.gmail.com>
	<54c8adaa0901131703u19805ec7y50f8bff0d1ba34b0@mail.gmail.com>
	<aefe4d0a0901132250r6423e7c1q17fd2b664683a4c@mail.gmail.com>
Message-ID: <a072ed700901160309g9e68edcod2411590933d0bba@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090116/116c2e72/attachment.pl>

From jude.phillips at gmail.com  Fri Jan 16 21:07:45 2009
From: jude.phillips at gmail.com (Jude Phillips)
Date: Fri, 16 Jan 2009 15:07:45 -0500
Subject: [R-sig-ME] Zero variance and Std. Dev. using lmer?
Message-ID: <c012ca2e0901161207l29c7cb8bg55bb261e54b6bda5@mail.gmail.com>

Hi, I have a similar problem to Luciano.  I am running a mixed effects
model with a continuous dependent variable, a categorical fixed effect
and a categorical random effect.  The dependent variable has a
distribution that is skewed to the left, which can be normalized by a
log transformation.  There is a huge difference in the results I get,
depending on whether I use the transformation, and I am not sure why.
When I run

spime1<-lmer(X13c ~ crop + (1|field.single), spi)

I get

Linear mixed model fit by REML
Formula: X13c ~ crop + (1 | field.single)
   Data: spi
   AIC   BIC    logLik    deviance    REMLdev
 422.5  440.1  -204.2    414.3        408.5
Random effects:
 Groups          Name        Variance Std.Dev.
 field.single    (Intercept)    0.0000   0.0000
 Residual                          5.8811   2.4251
Number of obs: 91, groups: field.single, 27

Fixed effects:
                  Estimate Std. Error   t value
(Intercept)   -21.3635     0.5882    -36.32
cropHay      -2.6131     0.9997      -2.61
cropHedge   -2.0445     0.6715     -3.04
cropSoy      3.7215     1.2338        3.02
cropWheat   0.6910     1.3477        0.51

Correlation of Fixed Effects:
                  (Intr)     cropHy crpHdg  cropSy
cropHay     -0.588
cropHedge  -0.876  0.515
cropSoy     -0.477   0.280    0.418
cropWheat  -0.436  0.257    0.382    0.208

then

> spime1 at deviance
          ML         REML               ldL2                ldRX2
          sigmaML            sigmaREML        pwrss               disc
                  usqr                  wrss
4.143326e+02   4.084787e+02   3.301496e-08   1.205152e+01
2.357530e+00    2.425094e+00      5.057730e+02    5.057730e+02
1.514514e-07    5.057730e+02
         dev         llik      NULLdev
          NA           NA           NA

However,

spimelog<-lmer(log(X13c +28) ~ crop + (1|field.single), spi)

gives

Linear mixed model fit by REML
Formula: log(X13c + 28) ~ crop + (1 | field.single)
   Data: spi
   AIC     BIC       logLik      deviance   REMLdev
  143.9   161.5     -64.95    120.7        129.9
Random effects:
 Groups       Name        Variance   Std.Dev.
 field.single (Intercept)   0.024557   0.15671
 Residual                     0.212558   0.46104
Number of obs: 91, groups: field.single, 27

Fixed effects:
                  Estimate    Std. Error    t value
(Intercept)    1.8692      0.1341       13.937
cropHay      -0.7846      0.2242      -3.500
cropHedge    -0.4403     0.1545      -2.850
cropSoy       0.4236     0.2705        1.566
cropWheat     0.1113     0.2888      0.385

Correlation of Fixed Effects:
          (Intr) cropHy crpHdg cropSy
cropHay   -0.598
cropHedge -0.868  0.519
cropSoy   -0.496  0.297  0.431
cropWheat -0.464  0.278  0.403  0.230

> spimelog at deviance
         ML        REML              ldL2           ldRX2
sigmaML     sigmaREML       pwrss           disc              usqr
       wrss                dev
120.7413657   129.9082247   8.5530877  10.4710000   0.4481952
0.4610400         18.2799815   17.0420840   1.2381553  17.0418262
    NA
       llik        NULLdev
         NA          NA


>From the previous posts, I think I understand that the first model has
0 std.dev and var for the random effects because the log-likelihood is
not being evaluated correctly, but why is the result so different when
the dependent variable is transformed.  (note that the dependent
variable happens to take negative values, the lowest of which is
-27.5, which is why I add 28 before the log transformation).

Thanks for your attention

Jude Phillips

PhD Candidate
GLEL, Biology Dept.  Carleton University.



From David.Duffy at qimr.edu.au  Sat Jan 17 22:56:43 2009
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Sun, 18 Jan 2009 07:56:43 +1000 (EST)
Subject: [R-sig-ME] Zero variance and Std. Dev. using lmer?
In-Reply-To: <c012ca2e0901161207l29c7cb8bg55bb261e54b6bda5@mail.gmail.com>
References: <c012ca2e0901161207l29c7cb8bg55bb261e54b6bda5@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0901180744550.10819@orpheus.qimr.edu.au>

On Fri, 16 Jan 2009, Jude Phillips wrote:

> Hi, I have a similar problem to Luciano.  I am running a mixed effects
> model with a continuous dependent variable, a categorical fixed effect
> and a categorical random effect.  The dependent variable has a
> distribution that is skewed to the left, which can be normalized by a
> log transformation.  There is a huge difference in the results I get,
> depending on whether I use the transformation, and I am not sure why.
>
> spimelog<-lmer(log(X13c +28) ~ crop + (1|field.single), spi)
>
>> From the previous posts, I think I understand that the first model has
> 0 std.dev and var for the random effects because the log-likelihood is
> not being evaluated correctly, but why is the result so different when
> the dependent variable is transformed.  (note that the dependent
> variable happens to take negative values, the lowest of which is
> -27.5, which is why I add 28 before the log transformation).
>

Because these types of analysis are sensitive to the distribution of y. 
Have you looked at the distribution of the field means under the two 
transformations?  Not knowing (and not wanting to know ;)) about your 
data, is log(x + c) with c=28 a bit extreme?  Have you looked at Box-Cox 
type approaches?

David Duffy

| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From rusers.sh at gmail.com  Sat Jan 17 16:10:54 2009
From: rusers.sh at gmail.com (zhijie zhang)
Date: Sat, 17 Jan 2009 23:10:54 +0800
Subject: [R-sig-ME] [R] glmer documentation
In-Reply-To: <21506036.post@talk.nabble.com>
References: <21506036.post@talk.nabble.com>
Message-ID: <a835c81e0901170710g35db3bacia563a8054e94cd2f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090117/377ef89c/attachment.pl>

From argchris at hotmail.com  Sun Jan 18 16:16:17 2009
From: argchris at hotmail.com (Christos Argyropoulos)
Date: Sun, 18 Jan 2009 17:16:17 +0200
Subject: [R-sig-ME] Interpretation of heteroscedastic structures for the
 within group error
Message-ID: <BLU149-W4C344E53472C02E0CD3F6D8D20@phx.gbl>


Hi, 
I apologise if the question is stupid, but I was wondering about the proper interpretation of the ratio of variances reported by heteroscedastic structures (varIdent).
Could one (within reason) interpret them as the intrinsic variability of the processes generating the data ? Stated otherwise, can one view Mixed Models as a generative 
model?

The background of my problem is the following:
I have data regarding mood, daytime performance and quality of sleep assessed sequentially over 14 days in three (unbalanced) groups of patients under different non-psychiatric treatments.
When the data are examined with a "population averaged" method such as a GEE or GLS there is evidence that the standard deviation of the repeated measures differ in these groups.
Since this relationship between standard deviations in the three groups could be due to the unbalanced nature of the data (different number of patients in each of the three groups) , I would like to explore
it further with a LMM. The call to lme looks like this:

lme(fixed=response~TREATMENT+ContCovars+OtherFactors,random=pdDiag(~1+ContCovar+OtherFactors|ID,weights=varIdent(form=~1|TREATMENT)) 

(I tried to put TREATMENT in the pdDiag wrapper but lme complained about positive-definitess of the covariate matrix, so I used the varIdent construct instead)

Even though I do get a reasonable output from LME  i.e. the relation of the variances reported by the intervals function is in line with the understanding of the biology/clinical problem I'm still worried about the interpretation of these variances. Can one interpet these variances as estimates of the intrinsic variability in the three outcomes under the treatment assignment?

Christos Argyropoulos
University of Pittsburgh
 
PS There is very little evidence of a consistent cycling behaviour, linear or general trend in these longitudinal data and thus I do not have to use the time series correlation structures for the analyses.

_________________________________________________________________
Show them the way! Add maps and directions to your party invites. 



From reinhold.kliegl at gmail.com  Sun Jan 18 18:20:23 2009
From: reinhold.kliegl at gmail.com (Reinhold Kliegl)
Date: Sun, 18 Jan 2009 18:20:23 +0100
Subject: [R-sig-ME] how to compute Confident intervals for BLUPS for lme
	function in nlme library
In-Reply-To: <a072ed700901160309g9e68edcod2411590933d0bba@mail.gmail.com>
References: <9bcbdf360901130715y6859e2d5gc201ad755568daf1@mail.gmail.com>
	<54c8adaa0901131515k59de5fdamc686906cd2c89644@mail.gmail.com>
	<54c8adaa0901131703u19805ec7y50f8bff0d1ba34b0@mail.gmail.com>
	<aefe4d0a0901132250r6423e7c1q17fd2b664683a4c@mail.gmail.com>
	<a072ed700901160309g9e68edcod2411590933d0bba@mail.gmail.com>
Message-ID: <aefe4d0a0901180920g561b4bc3m80ae70fd9b081ae8@mail.gmail.com>

On Fri, Jan 16, 2009 at 12:09 PM, Nick Isaac <njbisaac at googlemail.com> wrote:

> 1) Figure 4 in your manuscript is very similar to the plot produced by the
> code in your example. You desribe the bars around the conditional modes as
> '95% prediction intervals'. Are these synonymous with the posterior
> variances of the postVar attribute, or did you apply some kind of
> transformation?
They are the default output of the dotplot() function; no
transformations necessary.

>
> 2) Is it appropriate to cite your manuscript? If so, should the citation
> include information not on the website (e.g. journal title)?
Actually, I am require to include "This is a preprint of an article
submitted for consideration in Visual Cognition (c) 2008 Taylor &
Francis" on the title page.

>
> 3) You recently posted a question about sorting the output from ranef() for
> a dotplot (13/10/08). Did you ever figure out how to do this? I guess one
> could always extract the components, sort them directly and plot them in
> another function...
Actually, I was able to help myself. I copied the original dotplot()
function to a private dotplot.RK() function.  There,  I added the
argument "refvar" (reference variable) to the dotplot function and
replaced:

ss$.nn <- rep.int(reorder(factor(rownames(x)), x[[1]]), ncol(x))

with

ss$.nn <- rep.int(reorder(factor(rownames(x)), x[[refvar]]), ncol(x))

This worked for my models, but the solution will not hold for the
general form of the model, as detailed in the following comment by
Douglas Bates in an off-line exchange:

"The tricky part, as always, is whether that argument can be applied
to general forms of the model.  In general the ranef.mer class is a
list of arrays in which the numbers of rows and columns do not need to
be, and usually aren't, consistent.  The names of the columns don't
need to be consistent either.

One could specify the refvar as a number or as a name and get it to
work there but there should be a failsafe line that specifies what to
do if the number is greater than the number of columns or if the name
is not in the column names."

So some caution is in order.

Reinhold Kliegl
>
>
> 2009/1/14 Reinhold Kliegl <reinhold.kliegl at gmail.com>
>
>> Well, intervals() is a pretty good nlme response, I think, even if it
>> was not exactly answering the question. The lme4 equivalent is
>> dotplot() which produces a so-called "caterpillar plot" of conditional
>> means.
>>
>> For example:
>> ?ranef
>>
>> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
>> rr1 <- ranef(fm1, postVar = TRUE)
>> dotplot(rr1,scales = list(x = list(relation = 'free')))[["Subject"]]
>>
>> It is important to be clear about the distinction between random
>> effect estimates of variances and correlations, provided as part of
>> the model output, and the conditional means for groups/units based on
>> ("predicted with") them. At least, correlations computed from
>> conditional means can differ strongly from the correlation estimates.
>> The short story: You should only trust the model estimates. As Greg
>> Lee said, conditional means are to be handled with care; you can not
>> apply any of the usual inferential statistics to them. Nevertheless,
>> the caterpillar plots are highly informative and diagnostic about, for
>> example, whether you need a random effect to account for unit-by-unit
>> variability. They may also lead you to  discover distinct subgroups
>> suggestive of a fixed effect (for a future model). There is a
>> manuscript at the top of my publications page for download (and
>> constructive feedback) walking through an example.
>>
>> Reinhold Kliegl
>>
>>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From njbisaac at googlemail.com  Mon Jan 19 13:36:36 2009
From: njbisaac at googlemail.com (Nick Isaac)
Date: Mon, 19 Jan 2009 12:36:36 +0000
Subject: [R-sig-ME] Testing for non-linearity and heterogeneity
Message-ID: <a072ed700901190436j32b136f0ke7f3b717260d4fe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090119/0f5eede5/attachment.pl>

From reinhold.kliegl at gmail.com  Mon Jan 19 19:55:43 2009
From: reinhold.kliegl at gmail.com (Reinhold Kliegl)
Date: Mon, 19 Jan 2009 19:55:43 +0100
Subject: [R-sig-ME] Testing for non-linearity and heterogeneity
In-Reply-To: <a072ed700901190436j32b136f0ke7f3b717260d4fe@mail.gmail.com>
References: <a072ed700901190436j32b136f0ke7f3b717260d4fe@mail.gmail.com>
Message-ID: <aefe4d0a0901191055y32d280c9t4f604f42c43317c3@mail.gmail.com>

It appears that you want to model nonlinearity with a polynomial
function, that is test whether  the regression of Q on M is better
described by a quadratic than a linear relation. In addition, you test
whether there are reliable differences between units in the linear and
possibly also the quadratic trends.

Such an analysis can be nicely illustrated with the  "sleepstudy" data.

> fm1 <- lmer(Reaction ~ poly(Days,2) + (1 | Subject), sleepstudy)
> fm2 <- lmer(Reaction ~ poly(Days,2) + (poly(Days,1) | Subject), sleepstudy)
> fm3 <- lmer(Reaction ~ poly(Days,2) + (poly(Days,2) | Subject), sleepstudy)

Use poly() to specify the degree of the polynomial. fm1 allows only
varying intercepts, fm2 allows varying intercepts and varying linear
slopes, and fm3 allows varying intercepts, and varying linear and
quadratic trends.

You can use anova() to check whether addition of varying linear trends
and varying quadratic trends leads to significant improvement in
goodness of fit:
> anova(fm1,fm2,fm3)
Data: sleepstudy
Models:
fm1: Reaction ~ poly(Days, 2) + (1 | Subject)
fm2: Reaction ~ poly(Days, 2) + (poly(Days, 1) | Subject)
fm3: Reaction ~ poly(Days, 2) + (poly(Days, 2) | Subject)
    Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq)
fm1  5 1802.96 1818.92 -896.48
fm2  7 1764.32 1786.67 -875.16 42.642       2    5.5e-10 ***
fm3 10 1757.68 1789.61 -868.84 12.639      3   0.005487 **

Obviously, it does: AIC and BIC decline, logLik grows significantly.
Then, you inspect the CMs (conditional means for LMM; conditional
modes for GLMM and NLMM) with

> dotplot(ranef(fm3, postVar=TRUE)

This will show you that the subject 332 is a bit of an outlier for the
quadratic CMs. Once you remove this subject, there is no significant
reliable between-subject variance for the quadratic trend.

> fm1.2 <- lmer(Reaction ~ poly(Days,2) + (1 | Subject), sleepstudy, subset=Subject != 332)
> fm2.2 <- lmer(Reaction ~ poly(Days,2) + (poly(Days,1) | Subject), sleepstudy, subset=Subject != 332)
> fm3.2 <- lmer(Reaction ~ poly(Days,2) + (poly(Days,2) | Subject), sleepstudy, subset=Subject != 332)

> anova(fm1.2, fm2.2, fm3.2)
      Df    AIC    BIC logLik   Chisq Chi Df Pr(>Chisq)
fm1.2  5 1676.6 1692.3 -833.3
fm2.2  7 1618.4 1640.3 -802.2 62.2002      2  3.115e-14 ***
fm3.2 10 1622.2 1653.6 -801.1  2.1971      3     0.5325

Interestingly, the fixed-effect quadratic trend is not significant
for the original data set, but after removal of Subject 332, it is
consistently so. Thus, if there were an independent reason that
justifies exclusion of Subject 332, Reaction appears to follow a
quadratic trend over days but only the mean (intercept) and the linear
trend across days varies reliably between subjects. (This analysis is
only meant as an illustration. I did not check carefully whether this
conclusion holds up under close scrutiny.)

Please note that with the above model specification the number of
random effects grows very quickly. You should not hold lmer
responsible if it fails to converge for your data. Douglas Bates
provided some alternative specifications in an earlier post to keep
the number of random effects limited. Generally, I think that any
question that appears to require a follow-up analysis of CMs can be
rephrased in such a way that it is appropriately represented in the
model from the outset.

Reinhold Kliegl


On Mon, Jan 19, 2009 at 1:36 PM, Nick Isaac <njbisaac at googlemail.com> wrote:
> Dear list members,
>
> I would appreciate some advice on how to model nonlinearity in my dataset.
>
> The dataset contains >1000 observations on variables Q and M. The
> observations (actually species) are partitioned into around 50 groups. Each
> group spans only a small part of the total range of M. The principle
> research focus is on the relationship between Q and M, and whether this
> relationship displays heterogenity among groups. This is relatively easy to
> explore using:
>
> m1 <- lmer( Q ~ M +  (1|g) )
> m2 <- lmer( Q ~ M +  (M|g) )
>
> So far so good. However, I also want to test some recent theories that Q~M
> should vary with M, particularly that the relationship between slope and M
> might be negative curvilinear.
>
> Until recently, I thought that it would be appropriate to extract the
> conditional modes from the model and seek correlations separately. However,
> recent posts by Reinhold Kliegl on this list have made it clear that this
> would not be appropriate.
>
> So I've been wondered how to model this in lmer. I was thinking of fitting
> 2nd order polynomials to model the curvilinearity:
>
> m3 <- lmer( Q ~ M + I((M+20)^2) +  (M|g) ) #M is centered on it's mean, so I
> need to add 20 to make all values positive
> m4 <- lmer( Q ~ M + I((M+20)^2) +  (M + I((M+20)^2)|g) ) )
> m5 <- lmer( Q ~ M + I((M+20)^2) + (M|g) + (I((M+20)^2)|g) ) )
>
> I'm not sure how I would interpret differences in the fit of these models,
> and I'm sure that other (probably better) solutions exist.
>
> It has also been proposed that variance in the Q~M relationship should
> decline with increasing M. Is this a question about heteroscedasticity that
> I need to model explicitly (the residuals of m2 are quite well-behaved)?
>
> I would be extremely grateful for any advice on this topic.
>
> Best wishes, Nick
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From ddaniel at nmsu.edu  Tue Jan 20 06:47:19 2009
From: ddaniel at nmsu.edu (David Daniel)
Date: Mon, 19 Jan 2009 22:47:19 -0700
Subject: [R-sig-ME] Odd plot.lme() behavior
Message-ID: <B7658C54-6957-4DF9-A0E9-1B0A75DC34B5@nmsu.edu>

Hi all,

I've come across what appears to me to be strange behavior in the  
plot.lme() method.  In the example code below, the first plot() call  
gives a plot with no points plotted.  But the second plot() call gives  
a plot as expected (with points).  The only difference is that in the  
second case, lme() uses a data set not containing the variable, YES,  
but this variable is not referenced in any of the functions used in  
the code.

Even more odd is that if a seed of 1 is used in the call to  
set.seed(), the second call to plot() gives an error message not given  
when the seed is 2 (the seed only affects which values of variables  
YES and NO are NA's, and these variables are not used):

|> plot(lmeB,  Group ~ resid(.,  type="p"))
|  Error in `[<-.data.frame`(`*tmp*`, , ".y", value = c(1, 1, 1, 1, 1,  
1,  :
|  replacement has 36 rows, data has 80

But if *both* variables YES and NO are removed from the data set used,  
the plot works fine with a seed of 1.

Have others seen this?  Is this expected behavior?

I'm on a MacBook Pro 2.16 GHz Core 2 Duo, with OS 10.5.5, R version  
2.7.0, package nlme version 3.1-89, and package lattice version  
0.17-8.  The sessionInfo() output is below.

Thanks,
-David

#---------------------------------------
library(lattice);
library(nlme);

# Save the user's random seed
save.seed <- .Random.seed


#---------------------------------------
set.seed(2, kind = NULL)

n <- 10	# Number of subjects in each Group
k <- 4	# Number of phases

Group <- factor(c(rep("A",n*k), rep("B",n*k)))
m <- length(Group)/k		# total subjects

Gender <- factor( rep(c(rep("M", k), rep("F", k)), n) )
ID <- rep(1:m, rep(k, m))
phase <- rep(1:k, m)
YES <- rep(ceiling(12*runif(m)), rep(k, m))
NO <- YES
YES[YES > 6] <- NA
NO[NO <= 6] <- NA
PB <- rnorm(m*k) + phase + as.numeric(Group)
plotData <- data.frame(cbind(ID, PB, Group, Gender, phase, YES, NO))

#---------------------------------------
# Removing either YES or NO from the data.frame gives the correct plot
#---------------------------------------
plotData2 <- subset(plotData, select=c(ID, PB, Group, phase, YES, NO))
lmeA <- lme(PB ~ Group + phase,
					random= list(ID= pdDiag(form= ~phase)),
					data=plotData2,
					na.action= na.omit)
plot(lmeA,  Group ~ resid(.,  type="p"))

#---------------------------------------
# Removing either YES or NO from the data.frame gives the correct plot
#---------------------------------------
plotData3 <- subset(plotData, select=c(ID, PB, Group, Gender, phase,  
YES))
lmeB <- lme(PB ~ Group + phase,
					random= list(ID= pdDiag(form= ~phase)),
					data=plotData3,
					na.action= na.omit)
plot(lmeB,  Group ~ resid(.,  type="p"))

#---------------------------------------
# Restore the user's random seed
.Random.seed <- save.seed
#---------------------------------------



 > sessionInfo()
R version 2.7.0 (2008-04-22)
i386-apple-darwin8.10.1

locale:
en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] Matrix_0.999375-9 grid_2.7.0        lattice_0.17-8     
lme4_0.999375-20
[5] nlme_3.1-89

----------------------------------
David Daniel
Associate Professor
University Statistics Center
New Mexico State University

ddaniel at nmsu.edu



From Thierry.ONKELINX at inbo.be  Tue Jan 20 09:33:41 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 20 Jan 2009 09:33:41 +0100
Subject: [R-sig-ME] Odd plot.lme() behavior
In-Reply-To: <B7658C54-6957-4DF9-A0E9-1B0A75DC34B5@nmsu.edu>
References: <B7658C54-6957-4DF9-A0E9-1B0A75DC34B5@nmsu.edu>
Message-ID: <2E9C414912813E4EB981326983E0A10405FA103D@inboexch.inbo.be>

Dear David,

This is probably due to the fact that either YES or NO is NA in your
dataset and you included the na.action = na.omit argument in your model.
Without that argument in the model I get the plot with points.
My guess is that lme() omits rows only if they have at least one NA
value in a variable that is present in the model (which neither YES and
NO are). But that the plot function looks for missing values in the
entire row. Hence YES and NO are included and thus all rows omited.

HTH,

Thierry


------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of
data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] Namens David Daniel
Verzonden: dinsdag 20 januari 2009 6:47
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Odd plot.lme() behavior

Hi all,

I've come across what appears to me to be strange behavior in the  
plot.lme() method.  In the example code below, the first plot() call  
gives a plot with no points plotted.  But the second plot() call gives  
a plot as expected (with points).  The only difference is that in the  
second case, lme() uses a data set not containing the variable, YES,  
but this variable is not referenced in any of the functions used in  
the code.

Even more odd is that if a seed of 1 is used in the call to  
set.seed(), the second call to plot() gives an error message not given  
when the seed is 2 (the seed only affects which values of variables  
YES and NO are NA's, and these variables are not used):

|> plot(lmeB,  Group ~ resid(.,  type="p"))
|  Error in `[<-.data.frame`(`*tmp*`, , ".y", value = c(1, 1, 1, 1, 1,  
1,  :
|  replacement has 36 rows, data has 80

But if *both* variables YES and NO are removed from the data set used,  
the plot works fine with a seed of 1.

Have others seen this?  Is this expected behavior?

I'm on a MacBook Pro 2.16 GHz Core 2 Duo, with OS 10.5.5, R version  
2.7.0, package nlme version 3.1-89, and package lattice version  
0.17-8.  The sessionInfo() output is below.

Thanks,
-David

#---------------------------------------
library(lattice);
library(nlme);

# Save the user's random seed
save.seed <- .Random.seed


#---------------------------------------
set.seed(2, kind = NULL)

n <- 10	# Number of subjects in each Group
k <- 4	# Number of phases

Group <- factor(c(rep("A",n*k), rep("B",n*k)))
m <- length(Group)/k		# total subjects

Gender <- factor( rep(c(rep("M", k), rep("F", k)), n) )
ID <- rep(1:m, rep(k, m))
phase <- rep(1:k, m)
YES <- rep(ceiling(12*runif(m)), rep(k, m))
NO <- YES
YES[YES > 6] <- NA
NO[NO <= 6] <- NA
PB <- rnorm(m*k) + phase + as.numeric(Group)
plotData <- data.frame(cbind(ID, PB, Group, Gender, phase, YES, NO))

#---------------------------------------
# Removing either YES or NO from the data.frame gives the correct plot
#---------------------------------------
plotData2 <- subset(plotData, select=c(ID, PB, Group, phase, YES, NO))
lmeA <- lme(PB ~ Group + phase,
					random= list(ID= pdDiag(form=
~phase)),
					data=plotData2,
					na.action= na.omit)
plot(lmeA,  Group ~ resid(.,  type="p"))

#---------------------------------------
# Removing either YES or NO from the data.frame gives the correct plot
#---------------------------------------
plotData3 <- subset(plotData, select=c(ID, PB, Group, Gender, phase,  
YES))
lmeB <- lme(PB ~ Group + phase,
					random= list(ID= pdDiag(form=
~phase)),
					data=plotData3,
					na.action= na.omit)
plot(lmeB,  Group ~ resid(.,  type="p"))

#---------------------------------------
# Restore the user's random seed
.Random.seed <- save.seed
#---------------------------------------



 > sessionInfo()
R version 2.7.0 (2008-04-22)
i386-apple-darwin8.10.1

locale:
en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] Matrix_0.999375-9 grid_2.7.0        lattice_0.17-8     
lme4_0.999375-20
[5] nlme_3.1-89

----------------------------------
David Daniel
Associate Professor
University Statistics Center
New Mexico State University

ddaniel at nmsu.edu

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From a.beckerman at sheffield.ac.uk  Tue Jan 20 12:47:22 2009
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Tue, 20 Jan 2009 11:47:22 +0000
Subject: [R-sig-ME] Teaching Mixed Effects
Message-ID: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk>

Dear R-Mixed people -

I am about to embark on a day of attempting to teach some aspects of  
mixed models using R to PhD students.  I was wondering if anyone would  
be willing to indulge in this summary below, developed through reading  
threads on R-Mixed and R-Help over the past few months, and vet my  
list of issues/questions/topics (4)  associated with mixed models?

Let me reduce any rising blood pressure by saying that I understand  
(possibly) and accept why there are no p-values in lmer, and NONE of  
the comments/questions below are about why lmer does not produce  
sensible df's and p-values to calculate significance (Phew).

#######################

First, a technical question:

Based on these two threads:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001456.html

IS mcmcsamp() broken for "complicated" random effects? Is it in good  
enough shape to teach for "simple" gaussian mixed models, to  
demonstrate the principle?

#######################

Now, here is what I am possibly going to talk about.....

0) Rule number 1 is to design experiments well, and aim for  
orthogonal, well replicated and  balanced designs.  If you get data  
that conforms to all of that, old school F-ratio's CAN be used.  If  
not, see 1-4 below (we will assume that Rule number 1 will be broken).

1) It is agreed that the Laplacian methods for estimating terms and  
"likelihoods" in mixed effects models is considered most reliable  
(accurate and precise). R (lme4) and ADMB model builder use these  
methods. SAS nlmixed does, but SAS proc mixed does not appear to.   
STATA can.  Genstat does/can (see final note below**).

2) It is agreed that the appropriate test for fixed effects in mixed  
models should be between nested models.  However, there is no  
agreement as how to characterise the distributions that would be used  
to generate p-values.  This is the crux of the Bates et al argument:  
Likelihood Ratio Tests, Wald tests etc all need to assume a  
distribtion and some degrees of freedom.  But, in many mixed models,  
the distribution need not conform to any of our standard ones (t,F,  
Chi-square etc), especially when the number of subjects in the random  
effects is small.  Moreover, the relationship between fixed and random  
effects means that it is nearly impossible, and perhaps not worthwhile  
to calcuate what might be appropriate "degrees of freedom".

2.1) However, Bates et al have mentioned the restricted likelihood  
ratio test.  There is a package in R implementing some of these tools  
(RLRsim), but these appear to be limited to and or focused on tests of  
random effects.

2.2) What some "other" packages do: SAS can produce wald tests and  
LRT's if you want, and can implement the kenward-rogers adjustement.   
There is some theory behind the K-R, but it is still not dealing with  
the crux of the problem (see 2).  Genstat uses wald tests and warns  
you that with small numbers of subjects, these are not reliable.  
Genstat is also experimenting with HGLM by Nelder and Lee (see **)

2.3) "Testing" random effects is considered inappropriate (but see 2.1  
for methods?).

3) Because of 2, there is the resounding argument that bayesian and or  
simulation/bootstrapping tools be used to evalaute fixed effects.   
Current methods proposed and coded, but in various states of  
usefulness are:

mcmcsamp() and HPDinterval() from lme4 + baayen *.fnc's from languageR,
BUGS and winBugs,
RH Baayen's simulation tools (e.g. page 307 method)
Andrew Gelman and Jennifer Hill's tools (e.g. sim() method from  
package arm)
Ben Bolker's suggestions in this list for glmm's (thread: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html)

3.1) These all evalaute "simple" tests of whether beta's and intercept  
are different than 0, and are linked to the contrasts.  There is no  
emerging method equivalent to a LRT (but see 2.1 and **Final Note  
Below).

4) Andrew Gelman et al also suggest AIC based methods and model  
averaging for model inference, given constant random effects.  I think  
their argument about AIC is that if the "likelihood" is estimated  
well, relative differences in AIC will be constant, irrespective of  
any adjustement made to numbers of paramters used in calculating AIC:  
i.e. as long as the random effects structure stays the same, the  
relative differences between nested models will not change if the  
number of paramters is estimated consistently. These methods still do  
not produce p-values.

**Final Note Below - I have noticed a relative lack of discussion of  
Nelder et al's  H-likelihood and their methods to generate a general  
method for all heirarchical modelling (HGLM?!).  Would anybody be able  
to comment?  A recent paper (http://www.springerlink.com/content/17p17r046lx4053r/fulltext.pdf 
) that is somewhat beyond my skills, indicates the use of Laplace  
methods to estimate likelihoods in heirarchical models and various  
capacity for model inference.

Thanks again, in advance, to anyone who took this on..... apologies  
for any glaring errors or assignment of ideas to people incorrectly.

Andrew

---------------------------------------------------------------------------------
Dr. Andrew Beckerman
Department of Animal and Plant Sciences, University of Sheffield,
Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
http://www.beckslab.staff.shef.ac.uk/

http://www.flickr.com/photos/apbeckerman/
http://www.warblefly.co.uk



From bolker at UFL.EDU  Tue Jan 20 15:59:01 2009
From: bolker at UFL.EDU (Bolker,Benjamin Michael)
Date: Tue, 20 Jan 2009 09:59:01 -0500
Subject: [R-sig-ME] Teaching Mixed Effects
In-Reply-To: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk>
References: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk>
Message-ID: <2180B808CDA3404B8FF30C1EA0AC09EEB4D648@UFEXCH-MBXCL03.ad.ufl.edu>



   Some comments (others will certainly have more to add)

________________________________________
From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Andrew Beckerman [a.beckerman at sheffield.ac.uk]
Sent: Tuesday, January 20, 2009 6:47 AM
To: R Models Mixed
Subject: [R-sig-ME] Teaching Mixed Effects

Dear R-Mixed people -

I am about to embark on a day of attempting to teach some aspects of
mixed models using R to PhD students.  I was wondering if anyone would
be willing to indulge in this summary below, developed through reading
threads on R-Mixed and R-Help over the past few months, and vet my
list of issues/questions/topics (4)  associated with mixed models?

Let me reduce any rising blood pressure by saying that I understand
(possibly) and accept why there are no p-values in lmer, and NONE of
the comments/questions below are about why lmer does not produce
sensible df's and p-values to calculate significance (Phew).

#######################

First, a technical question:

Based on these two threads:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001456.html

IS mcmcsamp() broken for "complicated" random effects? Is it in good
enough shape to teach for "simple" gaussian mixed models, to
demonstrate the principle?

  BMB: good question. I'd like to know, although I would
guess that it would be OK for demonstrating the principle.
You could try it out and see if it's sensible ...

#######################

Now, here is what I am possibly going to talk about.....

0) Rule number 1 is to design experiments well, and aim for
orthogonal, well replicated and  balanced designs.  If you get data
that conforms to all of that, old school F-ratio's CAN be used.  If
not, see 1-4 below (we will assume that Rule number 1 will be broken).

  BMB: good idea. 

1) It is agreed that the Laplacian methods for estimating terms and
"likelihoods" in mixed effects models is considered most reliable
(accurate and precise). 

  BMB:  I believe (but stand ready to be corrected) that
PQL vs Laplace vs adaptive Gauss-Hermite quadrature
(AGHQ) is an issue for GLMMs, not so much for LMMs.
AGHQ is generally even better (but slower) than Laplace,
which is a special case.

R (lme4) and ADMB model builder use these
methods. SAS nlmixed does, but SAS proc mixed does not appear to.
STATA can.  Genstat does/can (see final note below**).

 BMB: SAS PROC MIXED does, as of version 9.2
see www2.sas.com/proceedings/forum2007/177-2007.pdf

2) It is agreed that the appropriate test for fixed effects in mixed
models should be between nested models.  However, there is no
agreement as how to characterise the distributions that would be used
to generate p-values.  This is the crux of the Bates et al argument:
Likelihood Ratio Tests, Wald tests etc all need to assume a
distribtion and some degrees of freedom.  But, in many mixed models,
the distribution need not conform to any of our standard ones (t,F,
Chi-square etc), especially when the number of subjects in the random
effects is small.  Moreover, the relationship between fixed and random
effects means that it is nearly impossible, and perhaps not worthwhile
to calcuate what might be appropriate "degrees of freedom".

  BMB: and specifically, if you happily use the anova() method
to calculate LRT it will do so -- but Pinheiro and Bates 2000 expressly
warn against the results/show that they can be "anticonservative"
for small sample sizes.  If you have huge sample sizes (i.e.
you're not a field ecologist) then LRT may be OK (I seem to remember
Bates using it without comment in an analysis of a (large)
Bangladeshi arsenic data set).

2.1) However, Bates et al have mentioned the restricted likelihood
ratio test.  There is a package in R implementing some of these tools
(RLRsim), but these appear to be limited to and or focused on tests of
random effects.

   BMB: You can test fixed effects, apparently, but only *in combination with*
a test of the random effect (the null hypothesis is always a model
without random effects). Also limited to LMMs, and a single
random effect.

2.2) What some "other" packages do: SAS can produce wald tests and
LRT's if you want, and can implement the kenward-rogers adjustement.

  BMB: Kenward-Roger ! (not Rogers)

There is some theory behind the K-R, but it is still not dealing with
the crux of the problem (see 2).  Genstat uses wald tests and warns
you that with small numbers of subjects, these are not reliable.
Genstat is also experimenting with HGLM by Nelder and Lee (see **)

2.3) "Testing" random effects is considered inappropriate (but see 2.1
for methods?).

  BMB: I don't think this is necessarily true. Admittedly it is a point
null hypothesis (variance will never be _exactly_ zero), but I
can certainly see cases ("does variation among species contribute
significantly to the overall variance observed"?) where one would
want to test this question.  This is a bit murky but I think the
distinction is often between random effects as part of an experimental
design (no point in testing, not interesting) and random effects
as observational data.

3) Because of 2, there is the resounding argument that bayesian and or
simulation/bootstrapping tools be used to evalaute fixed effects.

   BMB: I don't know about "resounding", but OK.  Probably
the best option.

Current methods proposed and coded, but in various states of
usefulness are:

mcmcsamp() and HPDinterval() from lme4 + baayen *.fnc's from languageR,
BUGS and winBugs,
RH Baayen's simulation tools (e.g. page 307 method)
Andrew Gelman and Jennifer Hill's tools (e.g. sim() method from
package arm)
Ben Bolker's suggestions in this list for glmm's (thread: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html)

3.1) These all evalaute "simple" tests of whether beta's and intercept
are different than 0, and are linked to the contrasts.  There is no
emerging method equivalent to a LRT (but see 2.1 and **Final Note
Below).

  BMB: I think if you want to calculate the parametric bootstrap/
null-hypothesis simulation of the change in deviances between
nested models, it's actually reasonably straightforward. See
examples on glmm.wikidot.com , especially
http://glmm.wikidot.com/basic-glmm-simulation
[geared toward GLMMs but even easier for LMMs]

4) Andrew Gelman et al also suggest AIC based methods and model
averaging for model inference, given constant random effects.  I think
their argument about AIC is that if the "likelihood" is estimated
well, relative differences in AIC will be constant, irrespective of
any adjustement made to numbers of paramters used in calculating AIC:
i.e. as long as the random effects structure stays the same, the
relative differences between nested models will not change if the
number of paramters is estimated consistently. These methods still do
not produce p-values.

  BMB: and AIC is an asymptotic method anyway, like
LRTs ... which means it is likely to have the same problems, but I don't
think anyone has evaluated them.  If you want to use the finite-size
corrections (AICc) then you are back in the situation of guessing
at residual degrees of freedom ...

**Final Note Below - I have noticed a relative lack of discussion of
Nelder et al's  H-likelihood and their methods to generate a general
method for all heirarchical modelling (HGLM?!).  Would anybody be able
to comment?  A recent paper (http://www.springerlink.com/content/17p17r046lx4053r/fulltext.pdf
) that is somewhat beyond my skills, indicates the use of Laplace
methods to estimate likelihoods in heirarchical models and various
capacity for model inference.

   BMB: I think HGLMs are a very promising way of *estimating*
the parameters of GLMMs (I too wonder why they don't seem to
be discussed much), but I don't think they get us any farther forward
with inference.

Thanks again, in advance, to anyone who took this on..... apologies
for any glaring errors or assignment of ideas to people incorrectly.

Andrew

---------------------------------------------------------------------------------
Dr. Andrew Beckerman
Department of Animal and Plant Sciences, University of Sheffield,
Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
http://www.beckslab.staff.shef.ac.uk/

http://www.flickr.com/photos/apbeckerman/
http://www.warblefly.co.uk

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From lee.wurm at wayne.edu  Tue Jan 20 17:17:47 2009
From: lee.wurm at wayne.edu (Lee Wurm)
Date: Tue, 20 Jan 2009 11:17:47 -0500
Subject: [R-sig-ME] power analysis for multi-level models
Message-ID: <7036bc6c0901200817yee67d99g6d606df65b9a3e2f@mail.gmail.com>

I'm really happy to see the explosion of mixed models in more and more
areas of research, but am now faced with the problem that grant
reviewers (and some journals) insist on seeing effect sizes and power
calculations for these models. I sent my query to the r-help forum and
got a valuable idea, along with the suggestion that I try you folks.

Suppose I want to see if men or women show a stronger word frequency
effect. I have 50 words of varying frequency that I show to 30 men and
30 women, who are supposed to decide as quickly as possible whether
it's a real word. My data object would end up being 3000 lines long,
and look like this:

Subject  Word  Sex  Frequency  ReactionTime
s1 w1 M 23 2543
s1 w2 M 67 1438
s1 w3 M 1 8033
...
s60 w50 F 4 1099

I analyze with:

lmer(ReactionTime ~ (Sex*Frequency) + (1|Subject) + (1|Word)

Can anyone help me get started with calculating effect sizes (or even
better, with attempting power analyses) for such a model? Or with
giving an explanation about why it's an ill-formed question, in terms
that non-experts could understand? Old ways die hard, sometimes, and
grant reviewers control the purse-strings.

Thanks.

--Lee



From Greg.Snow at imail.org  Tue Jan 20 19:56:54 2009
From: Greg.Snow at imail.org (Greg Snow)
Date: Tue, 20 Jan 2009 11:56:54 -0700
Subject: [R-sig-ME] power analysis for multi-level models
In-Reply-To: <7036bc6c0901200817yee67d99g6d606df65b9a3e2f@mail.gmail.com>
References: <7036bc6c0901200817yee67d99g6d606df65b9a3e2f@mail.gmail.com>
Message-ID: <B37C0A15B8FB3C468B5BC7EBC7DA14CC61C8360BBB@LP-EXMBVS10.CO.IHC.COM>

Here is some code to get you started (based on some assumptions that may be way off):

library(lme4)

sim1 <- function(bSex=0, bFreq=0, bSF=0, b0=1000, Vsubj=1, Vword=1, Verror=1) {
	Subject <- rep( 1:60, each=50 )
	Word <- rep( 1:50, 60 )
	Sex <- rep(c('M','F'), each=50*30)
	
	#  assume frequency is constant accross word, random from 1-100
	tmp <- sample( 1:100, 50, replace=TRUE )
	Frequency <- tmp[Word]

	# random effects per subject
	S.re <- rnorm(60, 0, sqrt(Vsubj))
	
	# random effects per word
	W.re <- rnorm(50, 0, sqrt(Vword))

	# epsilons
	eps <- rnorm(50*60, 0, sqrt(Verror))

	# put it all together
	ReactionTime <- b0 + bSex*(Sex=='M') + bFreq*Frequency + bSF*(Sex=='M')*Frequency +
		S.re[Subject] + W.re[Word] + eps

	# put into a data frame
	mydata <- data.frame( Subject = paste('s',Subject, sep=''), 
					Word = paste('w', Word, sep=''), Sex=Sex, Frequency=Frequency,
					ReactionTime = ReactionTime)

	# analyze looking at interaction term with LR test
	fit1 <- lmer( ReactionTime ~ (Sex*Frequency) + (1|Subject) + (1|Word), data=mydata)
	fit2 <- lmer( ReactionTime ~ Sex + Frequency + (1|Subject) + (1|Word), data=mydata)
	anova(fit2,fit1)[2,7]
}
	


pb <- winProgressBar(max=100) # or tkProgressBar or txtProgressbar

setWinProgressBar(pb, 0)
out1 <- replicate( 100, {setWinProgressBar(pb, getWinProgressBar(pb)+1);
				sim1( bSex=10, bFreq=2, bSF=0.25, Vsub=4000, Vword=2500, Verror=10000)})
hist(out1)
mean( out1 < 0.05 )

####

Now edit the sim1 function to match your real situation (in any cases that I guessed wrong) and analysis.  Run the simulation for reasonable values (guesses) and see what the power is.  I usually start with about 100 runs just to get a feel in a reasonable amount of time, change the values and rerun the last 4 lines several times.  Once you have the values that you want to use, up the number of simulations (change the progress bar as well) to 1,000 or maybe even 10,000 (start it running at the end of the day, then go home and let it run over night) to get your final values.

You may want to include a table/graph that shows the power for different effects of the interaction term, etc.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
801.408.8111


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-bounces at r-project.org] On Behalf Of Lee Wurm
> Sent: Tuesday, January 20, 2009 9:18 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] power analysis for multi-level models
> 
> I'm really happy to see the explosion of mixed models in more and more
> areas of research, but am now faced with the problem that grant
> reviewers (and some journals) insist on seeing effect sizes and power
> calculations for these models. I sent my query to the r-help forum and
> got a valuable idea, along with the suggestion that I try you folks.
> 
> Suppose I want to see if men or women show a stronger word frequency
> effect. I have 50 words of varying frequency that I show to 30 men and
> 30 women, who are supposed to decide as quickly as possible whether
> it's a real word. My data object would end up being 3000 lines long,
> and look like this:
> 
> Subject  Word  Sex  Frequency  ReactionTime
> s1 w1 M 23 2543
> s1 w2 M 67 1438
> s1 w3 M 1 8033
> ...
> s60 w50 F 4 1099
> 
> I analyze with:
> 
> lmer(ReactionTime ~ (Sex*Frequency) + (1|Subject) + (1|Word)
> 
> Can anyone help me get started with calculating effect sizes (or even
> better, with attempting power analyses) for such a model? Or with
> giving an explanation about why it's an ill-formed question, in terms
> that non-experts could understand? Old ways die hard, sometimes, and
> grant reviewers control the purse-strings.
> 
> Thanks.
> 
> --Lee
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From ddaniel at nmsu.edu  Tue Jan 20 21:46:21 2009
From: ddaniel at nmsu.edu (David Daniel)
Date: Tue, 20 Jan 2009 13:46:21 -0700
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 25, Issue 20
In-Reply-To: <mailman.3.1232449201.7920.r-sig-mixed-models@r-project.org>
References: <mailman.3.1232449201.7920.r-sig-mixed-models@r-project.org>
Message-ID: <F7F709A3-A73D-47DF-B594-CFF18CB11744@nmsu.edu>

Thanks for the reply, Thierry.

I thought this initially, but when only one of the variables, YES or  
NO, is excluded it works for some data sets.  For example for the data  
generated with set.seed(2) and dropping variable NO but retaining  
variable YES, resulting in the "plotData2" data.frame, half of the YES  
values in the data.frame are NA, but the plot works fine.  Examining  
the y-values for the plot object's panel.args in this case gives all  
80 values as expected:

|> plot(lmeB,  Group ~ resid(.,  type="p"))$panel.args[[1]]$y
| [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2  
2 2 2 2 2 2 2 2 1 1
|[43] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2  
2 2 2 2 2 2

As opposed to when both YES and NO are both in the data set, all 80  
values are NA:

|> plot(lmeA,  Group ~ resid(.,  type="p"))$panel.args[[1]]$y
| [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA  
NA NA NA NA NA NA NA
|[29] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA  
NA NA NA NA NA NA NA
|[57] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA  
NA NA NA

Also, I don't believe the lme() method omits observations having NA's  
for variables that are not included in the model.

Thanks,
-David


On Jan 20, 2009, at 4:00 AM, r-sig-mixed-models-request at r-project.org  
wrote:

> This is probably due to the fact that either YES or NO is NA in your
> dataset and you included the na.action = na.omit argument in your  
> model.
> Without that argument in the model I get the plot with points.
> My guess is that lme() omits rows only if they have at least one NA
> value in a variable that is present in the model (which neither YES  
> and
> NO are). But that the plot function looks for missing values in the
> entire row. Hence YES and NO are included and thus all rows omited.
>
> HTH,
>
> Thierry

----------------------------------
David Daniel
Associate Professor
University Statistics Center
New Mexico State University

ddaniel at nmsu.edu



From Fabian.Scheipl at stat.uni-muenchen.de  Tue Jan 20 22:01:19 2009
From: Fabian.Scheipl at stat.uni-muenchen.de (Fabian Scheipl)
Date: Tue, 20 Jan 2009 22:01:19 +0100
Subject: [R-sig-ME] Teaching Mixed Effects
Message-ID: <4836bc6a0901201301rf806388ke924fdec64573f3c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090120/55c921cb/attachment.pl>

From sylvestre at lunenfeld.ca  Wed Jan 21 16:21:53 2009
From: sylvestre at lunenfeld.ca (Marie-Pierre Sylvestre)
Date: Wed, 21 Jan 2009 10:21:53 -0500
Subject: [R-sig-ME] Model specification for partially nested random effects
Message-ID: <A249C197854D3442BDAE4C6BA4D5553C01D19A93@ex1.ad.mshri.on.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090121/7e81c38d/attachment.pl>

From kyler at mail.smu.edu  Wed Jan 21 16:55:35 2009
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Wed, 21 Jan 2009 09:55:35 -0600
Subject: [R-sig-ME] EM and Missing Data in R
Message-ID: <551E1CBE65B7EB44B9DF69AF8ED0BE7F010ED226BC@SXMBXA.systems.smu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090121/a051fccd/attachment.pl>

From kyler at mail.smu.edu  Wed Jan 21 17:11:06 2009
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Wed, 21 Jan 2009 10:11:06 -0600
Subject: [R-sig-ME] Teaching Mixed Effects
In-Reply-To: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk>
References: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk>
Message-ID: <551E1CBE65B7EB44B9DF69AF8ED0BE7F010ED226BE@SXMBXA.systems.smu.edu>

Not indulging in the questions below, but I just started teaching a course at SMU on MLM. I'll be putting things up on the course website as the semester goes on. You are welcome to anything that seems helpful.

http://www.jkyleroberts.com/rfiles/mlm/

One note . . . I am starting with nlme because School of Ed students "freak out" without a p-value. I'll be teaching lmer in a couple of weeks after I attempt to replicate Doug's argument about p-values and degrees of freedom.

Blessings,
Kyle

*********************************************************
Dr. J. Kyle Roberts
Department of Teaching and Learning
Annette Caldwell Simmons School of Education 
   and Human Development
Southern Methodist University
P.O. Box 750381
Dallas, TX? 75275
214-768-4494
http://www.hlm-online.com/
*********************************************************


-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Andrew Beckerman
Sent: Tuesday, January 20, 2009 5:47 AM
To: R Models Mixed
Subject: [R-sig-ME] Teaching Mixed Effects

Dear R-Mixed people -

I am about to embark on a day of attempting to teach some aspects of  
mixed models using R to PhD students.  I was wondering if anyone would  
be willing to indulge in this summary below, developed through reading  
threads on R-Mixed and R-Help over the past few months, and vet my  
list of issues/questions/topics (4)  associated with mixed models?

Let me reduce any rising blood pressure by saying that I understand  
(possibly) and accept why there are no p-values in lmer, and NONE of  
the comments/questions below are about why lmer does not produce  
sensible df's and p-values to calculate significance (Phew).

#######################

First, a technical question:

Based on these two threads:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001456.html

IS mcmcsamp() broken for "complicated" random effects? Is it in good  
enough shape to teach for "simple" gaussian mixed models, to  
demonstrate the principle?

#######################

Now, here is what I am possibly going to talk about.....

0) Rule number 1 is to design experiments well, and aim for  
orthogonal, well replicated and  balanced designs.  If you get data  
that conforms to all of that, old school F-ratio's CAN be used.  If  
not, see 1-4 below (we will assume that Rule number 1 will be broken).

1) It is agreed that the Laplacian methods for estimating terms and  
"likelihoods" in mixed effects models is considered most reliable  
(accurate and precise). R (lme4) and ADMB model builder use these  
methods. SAS nlmixed does, but SAS proc mixed does not appear to.   
STATA can.  Genstat does/can (see final note below**).

2) It is agreed that the appropriate test for fixed effects in mixed  
models should be between nested models.  However, there is no  
agreement as how to characterise the distributions that would be used  
to generate p-values.  This is the crux of the Bates et al argument:  
Likelihood Ratio Tests, Wald tests etc all need to assume a  
distribtion and some degrees of freedom.  But, in many mixed models,  
the distribution need not conform to any of our standard ones (t,F,  
Chi-square etc), especially when the number of subjects in the random  
effects is small.  Moreover, the relationship between fixed and random  
effects means that it is nearly impossible, and perhaps not worthwhile  
to calcuate what might be appropriate "degrees of freedom".

2.1) However, Bates et al have mentioned the restricted likelihood  
ratio test.  There is a package in R implementing some of these tools  
(RLRsim), but these appear to be limited to and or focused on tests of  
random effects.

2.2) What some "other" packages do: SAS can produce wald tests and  
LRT's if you want, and can implement the kenward-rogers adjustement.   
There is some theory behind the K-R, but it is still not dealing with  
the crux of the problem (see 2).  Genstat uses wald tests and warns  
you that with small numbers of subjects, these are not reliable.  
Genstat is also experimenting with HGLM by Nelder and Lee (see **)

2.3) "Testing" random effects is considered inappropriate (but see 2.1  
for methods?).

3) Because of 2, there is the resounding argument that bayesian and or  
simulation/bootstrapping tools be used to evalaute fixed effects.   
Current methods proposed and coded, but in various states of  
usefulness are:

mcmcsamp() and HPDinterval() from lme4 + baayen *.fnc's from languageR,
BUGS and winBugs,
RH Baayen's simulation tools (e.g. page 307 method)
Andrew Gelman and Jennifer Hill's tools (e.g. sim() method from  
package arm)
Ben Bolker's suggestions in this list for glmm's (thread: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html)

3.1) These all evalaute "simple" tests of whether beta's and intercept  
are different than 0, and are linked to the contrasts.  There is no  
emerging method equivalent to a LRT (but see 2.1 and **Final Note  
Below).

4) Andrew Gelman et al also suggest AIC based methods and model  
averaging for model inference, given constant random effects.  I think  
their argument about AIC is that if the "likelihood" is estimated  
well, relative differences in AIC will be constant, irrespective of  
any adjustement made to numbers of paramters used in calculating AIC:  
i.e. as long as the random effects structure stays the same, the  
relative differences between nested models will not change if the  
number of paramters is estimated consistently. These methods still do  
not produce p-values.

**Final Note Below - I have noticed a relative lack of discussion of  
Nelder et al's  H-likelihood and their methods to generate a general  
method for all heirarchical modelling (HGLM?!).  Would anybody be able  
to comment?  A recent paper (http://www.springerlink.com/content/17p17r046lx4053r/fulltext.pdf 
) that is somewhat beyond my skills, indicates the use of Laplace  
methods to estimate likelihoods in heirarchical models and various  
capacity for model inference.

Thanks again, in advance, to anyone who took this on..... apologies  
for any glaring errors or assignment of ideas to people incorrectly.

Andrew

---------------------------------------------------------------------------------
Dr. Andrew Beckerman
Department of Animal and Plant Sciences, University of Sheffield,
Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
http://www.beckslab.staff.shef.ac.uk/

http://www.flickr.com/photos/apbeckerman/
http://www.warblefly.co.uk

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From njbisaac at googlemail.com  Thu Jan 22 13:12:46 2009
From: njbisaac at googlemail.com (Nick Isaac)
Date: Thu, 22 Jan 2009 12:12:46 +0000
Subject: [R-sig-ME] Model specification for partially nested random
	effects
In-Reply-To: <A249C197854D3442BDAE4C6BA4D5553C01D19A93@ex1.ad.mshri.on.ca>
References: <A249C197854D3442BDAE4C6BA4D5553C01D19A93@ex1.ad.mshri.on.ca>
Message-ID: <a072ed700901220412m5eba3458jd3be566f129bc3bd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090122/3e9ba11c/attachment.pl>

From Gregor.Gorjanc at bfro.uni-lj.si  Thu Jan 22 13:54:24 2009
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Thu, 22 Jan 2009 13:54:24 +0100
Subject: [R-sig-ME] Priors in mcmcsamp for mixed linear model
Message-ID: <F189E18BBAA8B6479F618B9044CF4E9C68D7BCC681@REGULUS.bfro.uni-lj.si>

Hi!

Can anyone point me to the documentation, where I could read about
which priors are used in the mcmcsamp() for mixed linear model. I only found
the mail from Douglas, where he says that an improper uniform prior is used
on log of variance or standard deviation, but he does not mention if this is for
the residual or also for other random effects in the model.

http://thread.gmane.org/gmane.comp.lang.r.lme4.devel/372

Say we have a model

y|b,u,\sigma^2_e ~ Normal(Xb + Zu, I\sigma^2_e)
u|\sigma^2_u ~ Normal(0, I\sigma^2_u)

I would like to know the priors for

b ~ ???
\sigma^2_e ~ ???
\sigma^2_u ~ ???

Thank you!

Lep pozdrav / With regards,
    Gregor Gorjanc
----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty        www: http://gregor.gorjanc.googlepages.com
Animal Science Department   blog: http://ggorjan.blogspot.com
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe            tel: +386 (0)1 72 17 861



From HDoran at air.org  Thu Jan 22 15:21:11 2009
From: HDoran at air.org (Doran, Harold)
Date: Thu, 22 Jan 2009 09:21:11 -0500
Subject: [R-sig-ME] EM and Missing Data in R
In-Reply-To: <551E1CBE65B7EB44B9DF69AF8ED0BE7F010ED226BC@SXMBXA.systems.smu.edu>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE01D47ABB@DC1EXCL01.air.org>

Kyle:

I realize in the HLM circles it is common to use the term "levels", but
this is really quite confusing and, in fact, misleading. In a multilevel
model, there are multiple levels of random variation (many variance
components) but there are not multiple levels of fixed effects. These
are linear models with additive fixed and random effects. That is, there
are random effects and everything else is just a covariate---there are
no levels associated with covariates. So, now let's consider your
question. 

The matrix notation of the model is Y = XB + Zu + e where X is a known
model matrix, B are the coefficients of the fixed effects, Z is also a
model matrix and u are the random effects.

Now, u is completely missing (as is B). If they weren't missing, the
problem of solving for B would be easy. That is, if we had the complete
data, the maximization problem is simple. But, this is a missing data
problem and so some process is necessary to help us along. That is what
EM does. It can be used to augment the missing data in the vector u to
form a complete data problem and subsequently then perform the
maximization w.r.t B.

So yes, EM is a useful tool for missing data problems. EM, I think, is
easily programable in R. But, because EM is a general algorithm, I think
the best path for you is to go to the Dempster et al paper to understand
how it works and how it can be applied to missing data problems. Then,
you need to consider how it will work with your specific problem and
work out the conditional expectations and then maximize (which is often
the easiest part).

HTH,
Harold

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Roberts, Kyle
> Sent: Wednesday, January 21, 2009 10:56 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] EM and Missing Data in R
> 
> Friends,
> 
> Do you know of any way to use the EM algorithm to do 
> imputation for missing data at the second level in R? I have 
> heard some things about this at conferences, but can't put my 
> fingers on the actual references. I have a student who is 
> looking at missing data treatments for level-2 variables. I 
> haven't done any research in this area, but I want to point 
> her in the right direction.
> 
> If this is an "nonsensical"-type question, please forgive my naivety!
> 
> Thanks for your instruction.
> 
> Blessings,
> Kyle
> 
> *********************************************************
> Dr. J. Kyle Roberts
> Department of Teaching and Learning
> Annette Caldwell Simmons School of Education
>    and Human Development
> Southern Methodist University
> P.O. Box 750381
> Dallas, TX  75275
> 214-768-4494
> http://www.hlm-online.com/
> *********************************************************
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From danielezrajohnson at gmail.com  Thu Jan 22 15:40:56 2009
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Thu, 22 Jan 2009 14:40:56 +0000
Subject: [R-sig-ME] ignoring rather than omitting NA covariates
Message-ID: <a46630750901220640p72892071q43a18db13b1b9a96@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090122/ac37dcc3/attachment.pl>

From a.renwick at abdn.ac.uk  Thu Jan 22 18:10:01 2009
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Thu, 22 Jan 2009 17:10:01 +0000
Subject: [R-sig-ME] Nested random factor
Message-ID: <B9D1301370916C44B5874AF340C18B9B7F911B5302@VMAILB.uoa.abdn.ac.uk>


I am running a LME with a nested random factor using the 'nlme' package and ahave a query interpreting the varince of the netsed random factor.
My data contains samples from 7 farms, and within these farms I sampled at 2 sites.  The total number of sites was therefore 14, of which I sampled 4 times.

I therefore have used the following random effect:
1|Farm/Site

My final model is below:

mrem<-lme(log(Nhat+1)~ Width + crop+sess +Width:sess ,random=~1|Farm/Site,data=all, method="REML",correlation=NULL,weights=varIdent(form=~1|Group3))


summary(mrem)

#part of the summary
#Linear mixed-effects model fit by REML
# Data: all
#       AIC      BIC    logLik
#  106.2403 136.8477 -34.12013
#
#Random effects:
# Formula: ~1 | Farm
#        (Intercept)
#StdDev:   0.1597866
#
# Formula: ~1 | Site %in% Farm
#        (Intercept) Residual
#StdDev:   0.2458009 0.928392
#Number of Observations: 51
#Number of Groups:
#          Farm Site %in% Farm
#             7             14

I would like to check that I am interpreting the random effect correctly.
1) The std dev of the between-farm variance is 0.16
2) The std dev of the between-site variance is 0.25
3) The std dev of the within-site variance is 0.93.

However, I think I may have missed out the 'nesting' part,i.e the variance within sites within farms, and the variance between sites within farms.

Any help to clarify this point would be much appreciated.

Many thanks,
Anna

Anna Renwick
Institute of Biological & Environment Sciences
University of Aberdeen
Zoology Building
Tillydrone Avenue
Aberdeen
AB24 2TZ


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From ken at kjbeath.com.au  Thu Jan 22 21:37:15 2009
From: ken at kjbeath.com.au (Ken Beath)
Date: Fri, 23 Jan 2009 07:37:15 +1100
Subject: [R-sig-ME] ignoring rather than omitting NA covariates
In-Reply-To: <a46630750901220640p72892071q43a18db13b1b9a96@mail.gmail.com>
References: <a46630750901220640p72892071q43a18db13b1b9a96@mail.gmail.com>
Message-ID: <2D206C9B-B11B-48A2-A6CF-537413159989@kjbeath.com.au>

On 23/01/2009, at 1:40 AM, Daniel Ezra Johnson wrote:

> Dear all,
> This is not primarily a mixed models question, so I'll ask it in the
> framework of glm(). But I have the same question w/r/t glmer().
>
> In my field, sociolinguistics, researchers have used a software tool  
> for
> some thirty years that performs logistic regression assuming  
> categorical
> predictors. This software is usually called VARBRUL (the current  
> version of
> it is called GoldVarb).
>
> Assume a data file like this:
>
> response pred1 pred2
> 0 a x
> 1 a y
> 1 a x
> 0 a y
> 0 b x
> 1 b y
> 0 b x
> 0 b y
> 1 a /
> 0 b /
>
> My question is about the behavior of the slash (/) used in the last  
> two
> lines. Assume sum contrasts. The software estimates the values of a  
> and b
> (which sum to zero) and of x and y (which sum to zero). The  
> interesting part
> is that for the last two data points, the predicted values are  
> calculated on
> the basis of pred1 only, and pred2 is ignored.
>
> Looking at the various options of na.action, I do not see anything  
> that
> would correspond to this. Basically we have NA in a certain  
> predictor column
> and we want this predictor ignored for the row in question - we  
> don't want
> the whole row omitted.
>
> Any way to accomplish this in R?

There are ways to deal with missing data through multiple imputation,  
mi package is one.

There are other ways the GoldVarb package may use, possibly it builds  
a model for the missing data. This could be done in R it just has to  
be programmed. :-)  Possibly they aren't actually covariates but part  
of multivariate responses in which case missing data is much easier to  
deal with.

Ken



From bates at stat.wisc.edu  Thu Jan 22 23:40:11 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 22 Jan 2009 16:40:11 -0600
Subject: [R-sig-ME] Teaching Mixed Effects
In-Reply-To: <2180B808CDA3404B8FF30C1EA0AC09EEB4D648@UFEXCH-MBXCL03.ad.ufl.edu>
References: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk>
	<2180B808CDA3404B8FF30C1EA0AC09EEB4D648@UFEXCH-MBXCL03.ad.ufl.edu>
Message-ID: <40e66e0b0901221440lcc6cc9cmedc91fa7714081ed@mail.gmail.com>

My thanks to Andrew for starting this thread and to Ben for his
responses.  I will add more responses below.

I'm sorry that my responses have been delayed - it happens that this
is the first week of our spring semester and I have been tied up
getting courses underway.

On Tue, Jan 20, 2009 at 8:59 AM, Bolker,Benjamin Michael <bolker at ufl.edu> wrote:
>   Some comments (others will certainly have more to add)
> ________________________________________
> From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Andrew Beckerman [a.beckerman at sheffield.ac.uk]
> Sent: Tuesday, January 20, 2009 6:47 AM
> To: R Models Mixed
> Subject: [R-sig-ME] Teaching Mixed Effects
>
> Dear R-Mixed people -
>
> I am about to embark on a day of attempting to teach some aspects of
> mixed models using R to PhD students.  I was wondering if anyone would
> be willing to indulge in this summary below, developed through reading
> threads on R-Mixed and R-Help over the past few months, and vet my
> list of issues/questions/topics (4)  associated with mixed models?
>
> Let me reduce any rising blood pressure by saying that I understand
> (possibly) and accept why there are no p-values in lmer, and NONE of
> the comments/questions below are about why lmer does not produce
> sensible df's and p-values to calculate significance (Phew).
>
> #######################
>
> First, a technical question:
>
> Based on these two threads:
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001456.html
>
> IS mcmcsamp() broken for "complicated" random effects? Is it in good
> enough shape to teach for "simple" gaussian mixed models, to
> demonstrate the principle?
>
>  BMB: good question. I'd like to know, although I would
> guess that it would be OK for demonstrating the principle.
> You could try it out and see if it's sensible ...

I have not verified the results from the current mcmcsamp, even for
simple Gaussian models.  They seem reasonable for these models but I
need to look at them much more closely before I could advise trusting
those results.

The problem with designing an mcmcsamp method is that the variances of
the random effects can legitimately be zero and often have a
non-negligible probability of assuming the value of zero during the
MCMC iteraions.  However, most methods of sampling from the
distribution of a variance are based on sampling from the distribution
of a multiplier of the current value.  If the current value is zero,
you end up stuck there.

> #######################
>
> Now, here is what I am possibly going to talk about.....
>
> 0) Rule number 1 is to design experiments well, and aim for
> orthogonal, well replicated and  balanced designs.  If you get data
> that conforms to all of that, old school F-ratio's CAN be used.  If
> not, see 1-4 below (we will assume that Rule number 1 will be broken).

>  BMB: good idea.

In a designed experiment you can do this.  In an observational study
you can't expect to end up with balanced data.  I also tell my
students that assuming a balanced design will always produce balanced
data is contrary to Murphy's Law.  Balance is fragile.  I think it is
unwise to depend on balance being achieved.

> 1) It is agreed that the Laplacian methods for estimating terms and
> "likelihoods" in mixed effects models is considered most reliable
> (accurate and precise).
>
>  BMB:  I believe (but stand ready to be corrected) that
> PQL vs Laplace vs adaptive Gauss-Hermite quadrature
> (AGHQ) is an issue for GLMMs, not so much for LMMs.
> AGHQ is generally even better (but slower) than Laplace,
> which is a special case.

Exactly.  Adaptive Gauss-Hermite quadrature uses a quadrature formula
centered at the conditional modes of the random effects and scaled by
an approximation to the conditional standard deviations.  (That's
where the term "adaptive" comes from.)  The quadrature formula depends
on the number of quadrature points.  Generally we use an odd number of
points so one of the evaluations is at the conditional mode.  Thus you
could use a 3-point evaluation or a 5-point evaluation.  The simplest
formula, the 1-point Gauss-Hermite evaluation, is exactly the Laplace
approximation.

It turns out that the Laplace approximation is the only feasible such
method (at least the only one that I can imagine) when you have more
than one grouping factor for the random effects.  Even with just one
grouping factor, if you have vector-valued random effects (meaning
that you have more than one random effect associated with with each
level of the grouping factor) then the complexity of AGHQ is the
number of quadrature points raised to the dimension of the
vector-valued random effect.  Thus if you have a random intercept and
a random slope for each subject, say, and you choose a 7 point formula
then you must do 49 evaluations of the deviance residuals for each
AGHQ evaluation.

Oliver Schaubenberger's paper on SAS PROC GLIMMIX, which Ben mentions
below, discusses the problem of proliferation of the number of
evaluations required by AGHQ.

> R (lme4) and ADMB model builder use these
> methods. SAS nlmixed does, but SAS proc mixed does not appear to.
> STATA can.  Genstat does/can (see final note below**).
>
>  BMB: SAS PROC MIXED does, as of version 9.2
> see www2.sas.com/proceedings/forum2007/177-2007.pdf

I think you mean SAS PROC GLIMMIX, not SAS PROC MIXED.

Regarding the comparison of methods provided by different software,
remember that details of the implementation can be important.  The
term "adaptive Gauss-Hermite quadrature" describes a technical
approach but there can be many variations on the implementation, with
important consequences for precision, speed and accuracy.  It is a
gross oversimplification to imagine that such a technique is
implemented by handing a paper with some formulas to a "programmer"
and declaring the job done.  Comparing implementations, including
looking at the intermediate steps, is important but only possible for
open source implementations.

> 2) It is agreed that the appropriate test for fixed effects in mixed
> models should be between nested models.  However, there is no
> agreement as how to characterise the distributions that would be used
> to generate p-values.  This is the crux of the Bates et al argument:
> Likelihood Ratio Tests, Wald tests etc all need to assume a
> distribtion and some degrees of freedom.  But, in many mixed models,
> the distribution need not conform to any of our standard ones (t,F,
> Chi-square etc), especially when the number of subjects in the random
> effects is small.  Moreover, the relationship between fixed and random
> effects means that it is nearly impossible, and perhaps not worthwhile
> to calcuate what might be appropriate "degrees of freedom".

I'm afraid my response will be rather long-winded, for which I
apologize.  I feel that as statisticians we have done a disservice to
those who use statistics by taking complex problems (tests of
hypotheses for terms in complicated model structures) for which there
is an enormous simplification in the central special case (linear
dependence of the mean on the model parameters, "spherical" Gaussian
distribution) and describing certain computational shortcuts that can
be used only in this special case.  For the most part we don't even
hint at the general problem or even discuss the approach used in the
special case.  We jump right in to a particular form of a summary of a
computational shortcut, the analysis of variance table.

I am very pleased to see you describe the situation as a comparison of
two nested models.  That is indeed how we should approach the problem.
 We have a general model and a specific model that is a special case
of the general model.  Obviously the general model will fit the data
at least as well as the more specialized model.  We wish to determine
if the fit is sufficiently better to justify using the more general
model.   To do so we must decide how to judge the extent to which the
general model fits better and how much more complex it is, so we can
form some kind of cost/benefit criterion.  Then we must assess the
value of this test statistic by comparing it to a reference
distribution.  In the special case of a Gaussian linear model it all
works out beautifully and we can summarize the results very compactly
with t statistics or F statistics and their degrees of freedom.  But
this case is special.  It is misleading to believe that things will
simplify like this in more general cases.

Consider the question of how we measure the comparative complexity of
two models. Typically we can measure the difference in the complexity
of the models in terms of the number of additional parameters in the
general model. In the central special case (Gaussian linear models)
there is a geometric representation of the model where the general
model corresponds to a linear subspace of the sample space and the
specific model corresponds to a subspace contained in the general
model.  The spherical Gaussian assumption (i.e. Gaussian distribution
with variance-covariance of sigma^2 I, for which the contours of
constant probability density are spheres) links the probability model
to the Euclidean geometry of the space.  From those two assumptions
the methods of testing a general linear hypothesis can be derived
geometrically.  Gosset derived a statistic that is equivalent to the t
statistic using analytic means but the current form of the t statistic
and its relation to degrees of freedom came from Fisher and were based
on his geometric insight (see the Wikipedia article on Gosset).  And,
of course, Fisher was able to generalize the t distribution to the F
distribution, again based on geometric principles.

In the first chapter of our 1980 book "Nonlinear Regression Analysis
and Its Applications" Don Watts and I illustrate the geometric
approach to the t and F statistics for linear models.  Fisher's genius
was to see that questions about comparative model fits, which are
related to distances in the geometric representation, can be
transformed into questions about angles, related to the ratios of
distances or, equivalently, squared distances of orthogonal components
of a response vector.  The use of the ratio allows one scale factor
(variance component in statistical terminology) to be canceled out.
That is, the null distribution of an F ratio can be expressed without
needing to specify the unknown error variance.

Regrettably, the "use a ratio to eliminate a variance component" trick
only works once.  The first variance component is free but not
subsequent ones.  If you have a perfectly balanced, orthogonal design
then you can apply the trick multiple times by isolating certain
orthogonal submodels and applying the trick within the submodel.  That
is, you can use estimates from different "error strata" in ratios for
different terms.  However, that approach based on certain mean squares
and expected mean squares doesn't generalize well.  The
computationally tractable approach to estimation of parameters in
mixed models is maximum likelihood or REML estimation.

The reason for my long-winded explanation is your saying " This is the
crux of the Bates et al argument: Likelihood Ratio Tests, Wald tests
etc all need to assume a distribution and some degrees of freedom.",
which is a natural statement given the way that we teach analysis of
variance.  We teach the "what" (i.e. create a table of sums of
squares, degrees of freedom, mean squares, F ratios, p-values) and not
the "why".  If you only see the "what" then it is natural to assume
that there are some magical properties associated with sums of squares
and degrees of freedom and all we need to do is to figure out which
sums of squares and which degrees of freedom to use.  The magical
properties are actually the simplified geometric representation
(orthogonal linear subspaces, Euclidean geometry) that is unique to
the Gaussian linear model.  The beauty of that model is that, no
matter how complicated the representation of a test as a formula may
be, the geometric representation is always the same, as the ratio of
the normalized squared lengths of two orthogonal components of the
response vector.

When we step away from that Gaussian linear model the simplifications
all break down.  I spent the early part of career thinking of what
parts of the Gaussian linear model can be transferred over to the
Gaussian nonlinear model and what that would mean for inference.  This
is why I concentrated so much on the geometry of models based on the
spherical Gaussian distribution.  Generalized linear models retain the
linear predictor, transformed through an inverse link function to the
appropriate scale for the mean, but allow for distributions other than
the Gaussian.  They require another way of thinking.  I think of mixed
models as being based on two random vectors, the response vector and
the unobserved random effects.  In the case of a Gaussian linear mixed
model the conditional distribution of the response, given the random
effects, is a spherical Gaussian and the unconditional distribution of
the random effects is multivariate Gaussian but our inferences require
the marginal distribution of the response.  For a linear mixed model
this is Gaussian but with more than one variance component.  Getting
rid of just one variance component won't do, yet the t and F
derivations depend strongly on just having one variance component that
can be removed by considering a ratio.

If we want to perform a hypothesis test related to a fixed-effects
term in a mixed model (and, for the moment, I will not go into the
question of whether statistical inferences should always be phrased as
the result of hypothesis tests) I would claim we should start at the
beginning, which is considering two models for the data at hand, one
model being a special case of the other.  We need to decide how we
measure the quality of the fit of the general model relative to the
more specific model and how we measure the additional cost of the
general model.  Then we need to formulate a test statistic.  If we are
incredibly lucky, the null distribution of this test statistic will be
well-defined (that is, it will not depend on the values of other,
unknown parameters) and we can evaluate probabilities associated with
it.  That does happen in the case of the Gaussian linear model.  I
personally don't think it will be possible to possible to provide a
general approach that isolates the effect of a fixed-effect term in a
linear mixed model using a statistic that does not depend on the
values of other parameters.  I would be delighted if someone can do it
but I think there is too much that goes right in the case of the
Gaussian linear model to expect that the same incredible
simplifications will apply to other models.

I don't feel that holy grail of inference in mixed effects models
should be a magical formula for degrees of freedom to be associated
with some ratio that looks like a t or an F statistic (despite the
strongly held beliefs of those in the First Church of the
Kenward-Roger Approximation). Certainly there has been a lot of
statistical research related to approximating a difficult distribution
by a more common distribution but I view this approach as belonging to
an earlier era.  It is the approach embodied in software like SAS
whose purpose often seems to be to evaluate difficult formulas and
provide reams of output including every number that could possibly be
of interest.  I think we should use the power of current and future
computers to interact with and explore data and models for the data.
MCMC is one way to do this.  In nonlinear regression Don and I
advocated profiling the sum of squares function with respect to the
values of individual parameters as another way of assessing the actual
behavior of the model versus trying to formulate an approximation.
I'm sure that creative people will come up with many other ways to use
the power of computers to this end.  The point is to explore the
actual behavior of the model/data combination, not to do just one fit,
calculate a bunch of summary statistics, apply approximate
distributions to get p-values and go home.

If we want to generalize methods of inference we should consider the
whole chain of reasoning that leads us to the result rather than
concentrating only on the last step, which is "now that I have the
value of a statistic how do I convert it to a p-value?" or, even more
specifically, "I have calculated something that looks like a t-ratio
so I am going to assume that its distribution is indeed a
t-distribution which leaves me with only one question and that is on
how many degrees of freedom".

I appreciate that this is inconvenient to those applying such models
to their data.  Philosophical discussions of the fundamentals of
statistical inference are all well and good but when the referees on
your paper say you have to provide a p-value for a particular term or
tests, it is a practical matter, not an academic, theoretical debate.
Those with sufficient energy and skill plus a stellar reputation as a
researcher may be able to convince editors that p-values are not the
"be all and end all" of data analysis - Reinhold Kleigl has been able
to do this in some cases - but that is definitely not the path of
least resistance.  The sad reality is that p-values have taken on a
role as the coin of the realm in science that is far beyond what any
statistician would have imagined.  (Apparently the default
"significance level" of 5%, which is considered in some disciplines to
be carved in stone, resulted from a casual comment by Fisher to the
effect that he might regard an outcome that would be expected to occur
less than, say, 1 time in 20 as "significant".)

It is unhelpful of me not to provide p-values in the lmer summaries
but I develop the software out of interest in doing it as well as I
possibly can and not because someone assigns me a task to compute
something.  I really don't know of a good way of assigning p-values to
t-ratios or F-ratios so I don't.  I still report the ratio of the
estimate divided by it standard error, and even call it a t-ratio,
because I think it is informative.

>  BMB: and specifically, if you happily use the anova() method
> to calculate LRT it will do so -- but Pinheiro and Bates 2000 expressly
> warn against the results/show that they can be "anticonservative"
> for small sample sizes.  If you have huge sample sizes (i.e.
> you're not a field ecologist) then LRT may be OK (I seem to remember
> Bates using it without comment in an analysis of a (large)
> Bangladeshi arsenic data set).

I think that was data on artificial contraception use obtained as part
of a fertility survey in Bangladesh.

I feel that the likelihood ratio is a perfectly reasonable way of
comparing two model fits where one is a special case of the other.  In
fact, if the models have been fit by maximum likelihood, the
likelihood ratio would, I think, be the first candidate for a test
statistic.  The problem with likelihood ratio tests is not the
likelihood ratio, per se -- it is converting the likelihood ratio to a
p-value.  You need to be able to evaluate the distribution of the
likelihood ratio under the null hypothesis.  The chi-square
approximation to the distribution is exactly that - an approximation -
and its validity depends on not testing at the boundary and on having
a large sample, in some sense of the sample size.  If I were really
interested in evaluating a p-value for the likelihood ratio I would
probably try a parametric bootstrap to get a reference distribution.

> 2.1) However, Bates et al have mentioned the restricted likelihood
> ratio test.  There is a package in R implementing some of these tools
> (RLRsim), but these appear to be limited to and or focused on tests of
> random effects.
>
>   BMB: You can test fixed effects, apparently, but only *in combination with*
> a test of the random effect (the null hypothesis is always a model
> without random effects). Also limited to LMMs, and a single
> random effect.
>
> 2.2) What some "other" packages do: SAS can produce wald tests and
> LRT's if you want, and can implement the kenward-rogers adjustement.
>
>  BMB: Kenward-Roger ! (not Rogers)
>
> There is some theory behind the K-R, but it is still not dealing with
> the crux of the problem (see 2).  Genstat uses wald tests and warns
> you that with small numbers of subjects, these are not reliable.
> Genstat is also experimenting with HGLM by Nelder and Lee (see **)
>
> 2.3) "Testing" random effects is considered inappropriate (but see 2.1
> for methods?).
>
>  BMB: I don't think this is necessarily true. Admittedly it is a point
> null hypothesis (variance will never be _exactly_ zero), but I
> can certainly see cases ("does variation among species contribute
> significantly to the overall variance observed"?) where one would
> want to test this question.  This is a bit murky but I think the
> distinction is often between random effects as part of an experimental
> design (no point in testing, not interesting) and random effects
> as observational data.

Actually the MLE or the REML estimate of a variance component can
indeed be zero.  The residual variance (i.e. the variance of the "per
observation" noise term) is only zero for artificial data but the
estimates of other variance components can be exactly zero.

I think of such situations as an indication that I should simplify the
model by eliminating such a term.  I have spent most of my career at
the University of Wisconsin-Madison in a statistics department founded
by George Box who famously said "All models are wrong; some models are
useful."  I don't expect a model to be correct, I am only interested
in whether the terms in the model are useful in explaining the
observed data.

> 3) Because of 2, there is the resounding argument that bayesian and or
> simulation/bootstrapping tools be used to evalaute fixed effects.
>
>   BMB: I don't know about "resounding", but OK.  Probably
> the best option.
>
> Current methods proposed and coded, but in various states of
> usefulness are:
>
> mcmcsamp() and HPDinterval() from lme4 + baayen *.fnc's from languageR,
> BUGS and winBugs,
> RH Baayen's simulation tools (e.g. page 307 method)
> Andrew Gelman and Jennifer Hill's tools (e.g. sim() method from
> package arm)
> Ben Bolker's suggestions in this list for glmm's (thread: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html)
>
> 3.1) These all evalaute "simple" tests of whether beta's and intercept
> are different than 0, and are linked to the contrasts.  There is no
> emerging method equivalent to a LRT (but see 2.1 and **Final Note
> Below).
>
>  BMB: I think if you want to calculate the parametric bootstrap/
> null-hypothesis simulation of the change in deviances between
> nested models, it's actually reasonably straightforward. See
> examples on glmm.wikidot.com , especially
> http://glmm.wikidot.com/basic-glmm-simulation
> [geared toward GLMMs but even easier for LMMs]
>
> 4) Andrew Gelman et al also suggest AIC based methods and model
> averaging for model inference, given constant random effects.  I think
> their argument about AIC is that if the "likelihood" is estimated
> well, relative differences in AIC will be constant, irrespective of
> any adjustement made to numbers of paramters used in calculating AIC:
> i.e. as long as the random effects structure stays the same, the
> relative differences between nested models will not change if the
> number of paramters is estimated consistently. These methods still do
> not produce p-values.
>
>  BMB: and AIC is an asymptotic method anyway, like
> LRTs ... which means it is likely to have the same problems, but I don't
> think anyone has evaluated them.  If you want to use the finite-size
> corrections (AICc) then you are back in the situation of guessing
> at residual degrees of freedom ...
>
> **Final Note Below - I have noticed a relative lack of discussion of
> Nelder et al's  H-likelihood and their methods to generate a general
> method for all heirarchical modelling (HGLM?!).  Would anybody be able
> to comment?  A recent paper (http://www.springerlink.com/content/17p17r046lx4053r/fulltext.pdf
> ) that is somewhat beyond my skills, indicates the use of Laplace
> methods to estimate likelihoods in heirarchical models and various
> capacity for model inference.
>
>   BMB: I think HGLMs are a very promising way of *estimating*
> the parameters of GLMMs (I too wonder why they don't seem to
> be discussed much), but I don't think they get us any farther forward
> with inference.
>
> Thanks again, in advance, to anyone who took this on..... apologies
> for any glaring errors or assignment of ideas to people incorrectly.
>
> Andrew
>
> ---------------------------------------------------------------------------------
> Dr. Andrew Beckerman
> Department of Animal and Plant Sciences, University of Sheffield,
> Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
> ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
> http://www.beckslab.staff.shef.ac.uk/
>
> http://www.flickr.com/photos/apbeckerman/
> http://www.warblefly.co.uk
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Fri Jan 23 00:14:25 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 22 Jan 2009 17:14:25 -0600
Subject: [R-sig-ME] Nested random factor
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B7F911B5302@VMAILB.uoa.abdn.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B7F911B5302@VMAILB.uoa.abdn.ac.uk>
Message-ID: <40e66e0b0901221514h1bbe4ba6u91fbfdb93632dd8c@mail.gmail.com>

On Thu, Jan 22, 2009 at 11:10 AM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:

> I am running a LME with a nested random factor using the 'nlme' package and ahave a query interpreting the varince of the netsed random factor.
> My data contains samples from 7 farms, and within these farms I sampled at 2 sites.  The total number of sites was therefore 14, of which I sampled 4 times.

> I therefore have used the following random effect:
> 1|Farm/Site

> My final model is below:

> mrem<-lme(log(Nhat+1)~ Width + crop+sess +Width:sess ,random=~1|Farm/Site,data=all, method="REML",correlation=NULL,weights=varIdent(form=~1|Group3))

> summary(mrem)

> #part of the summary
> #Linear mixed-effects model fit by REML
> # Data: all
> #       AIC      BIC    logLik
> #  106.2403 136.8477 -34.12013
> #
> #Random effects:
> # Formula: ~1 | Farm
> #        (Intercept)
> #StdDev:   0.1597866
> #
> # Formula: ~1 | Site %in% Farm
> #        (Intercept) Residual
> #StdDev:   0.2458009 0.928392
> #Number of Observations: 51
> #Number of Groups:
> #          Farm Site %in% Farm
> #             7             14

> I would like to check that I am interpreting the random effect correctly.
> 1) The std dev of the between-farm variance is 0.16

That phrase is a bit peculiar.  I think I would say either,

The standard deviation of the random effect for farm is 0.16

or

The between-farm variance is (0.16)^2

> 2) The std dev of the between-site variance is 0.25

In your terminology I would say, "The variance between sites within
farms is (0.25)^2."

> 3) The std dev of the within-site variance is 0.93.

Again, I would phrase this as "The variance within sites is (0.93)^2."

> However, I think I may have missed out the 'nesting' part,i.e the variance within sites within farms, and the variance between sites within farms.

Because sites are nested within farms, "within site" is the same as
"within site, within farms" and "between sites within farms" is your
line 2) above.  If you were to eliminate the random effect for farm
(and ensure that each distinct site has a distinct label - that is,
avoid "implicit nesting") then you would estimate a "between site"
variance.

I'm not surprised that this seems confusing.  I certainly found all
the material on variance components with mean squares and expected
mean squares to be confusing when I was first exposed to it. I vowed I
would stay as far away from that type of statistics as I could because
it was just so tedious and messy.  Obviously I didn't succeed in
staying away from it.

> Any help to clarify this point would be much appreciated.
>
> Many thanks,
> Anna
>
> Anna Renwick
> Institute of Biological & Environment Sciences
> University of Aberdeen
> Zoology Building
> Tillydrone Avenue
> Aberdeen
> AB24 2TZ
>
>
> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From a.renwick at abdn.ac.uk  Fri Jan 23 00:28:59 2009
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Thu, 22 Jan 2009 23:28:59 +0000
Subject: [R-sig-ME] Nested random factor
In-Reply-To: <40e66e0b0901221514h1bbe4ba6u91fbfdb93632dd8c@mail.gmail.com>
References: <B9D1301370916C44B5874AF340C18B9B7F911B5302@VMAILB.uoa.abdn.ac.uk>,
	<40e66e0b0901221514h1bbe4ba6u91fbfdb93632dd8c@mail.gmail.com>
Message-ID: <B9D1301370916C44B5874AF340C18B9B7F913660DC@VMAILB.uoa.abdn.ac.uk>

Thank you so much for your reply.
Can I just check the labelling of my data regards you comment "ensure that each distinct site has a distinct label - that is,
avoid "implicit nesting"".

for example:
7 farms, 14 margins (2 margins in each farm).  All margins sampled 4 times
DUMMY DATA:

Random effect specified as (1|Farm/Site)

Farm          Site               Sample period      Abundance
1                 1                     1                             10
2                 2                      1                              3
3                 3                      1                              2
4                 4                       1                             6
5                 5                        1                          13
6                 6                          1                        11
7                 7                     1                              12
1                 8                        1                            2
2                 9                          1                         3
3                 10                         1                          22
4                 11                       1                           1
5                 12                        1                           33
6                 13                        1                           2
7                 14                         1                          13
1                 1                          2                             13
2                 2                            2                             12
3                 3                            2                            11
4                 4                              2                              3
5                 5                             2                        6
6                 6                           2                             5
7                 7                               2                      4
1                 8                             2                         12
2                 9                              2                          24
3                 10                           2                      25
4                 11                          2                       3
5                 12                         2                        22
6                 13                          2                         23
7                 14                          2                          8

Many many thanks
Anna

________________________________________
From: dmbates at gmail.com [dmbates at gmail.com] On Behalf Of Douglas Bates [bates at stat.wisc.edu]
Sent: 22 January 2009 23:14
To: Renwick, A. R.
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Nested random factor

On Thu, Jan 22, 2009 at 11:10 AM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:

> I am running a LME with a nested random factor using the 'nlme' package and ahave a query interpreting the varince of the netsed random factor.
> My data contains samples from 7 farms, and within these farms I sampled at 2 sites.  The total number of sites was therefore 14, of which I sampled 4 times.

> I therefore have used the following random effect:
> 1|Farm/Site

> My final model is below:

> mrem<-lme(log(Nhat+1)~ Width + crop+sess +Width:sess ,random=~1|Farm/Site,data=all, method="REML",correlation=NULL,weights=varIdent(form=~1|Group3))

> summary(mrem)

> #part of the summary
> #Linear mixed-effects model fit by REML
> # Data: all
> #       AIC      BIC    logLik
> #  106.2403 136.8477 -34.12013
> #
> #Random effects:
> # Formula: ~1 | Farm
> #        (Intercept)
> #StdDev:   0.1597866
> #
> # Formula: ~1 | Site %in% Farm
> #        (Intercept) Residual
> #StdDev:   0.2458009 0.928392
> #Number of Observations: 51
> #Number of Groups:
> #          Farm Site %in% Farm
> #             7             14

> I would like to check that I am interpreting the random effect correctly.
> 1) The std dev of the between-farm variance is 0.16

That phrase is a bit peculiar.  I think I would say either,

The standard deviation of the random effect for farm is 0.16

or

The between-farm variance is (0.16)^2

> 2) The std dev of the between-site variance is 0.25

In your terminology I would say, "The variance between sites within
farms is (0.25)^2."

> 3) The std dev of the within-site variance is 0.93.

Again, I would phrase this as "The variance within sites is (0.93)^2."

> However, I think I may have missed out the 'nesting' part,i.e the variance within sites within farms, and the variance between sites within farms.

Because sites are nested within farms, "within site" is the same as
"within site, within farms" and "between sites within farms" is your
line 2) above.  If you were to eliminate the random effect for farm
(and ensure that each distinct site has a distinct label - that is,
avoid "implicit nesting") then you would estimate a "between site"
variance.

I'm not surprised that this seems confusing.  I certainly found all
the material on variance components with mean squares and expected
mean squares to be confusing when I was first exposed to it. I vowed I
would stay as far away from that type of statistics as I could because
it was just so tedious and messy.  Obviously I didn't succeed in
staying away from it.

> Any help to clarify this point would be much appreciated.
>
> Many thanks,
> Anna
>
> Anna Renwick
> Institute of Biological & Environment Sciences
> University of Aberdeen
> Zoology Building
> Tillydrone Avenue
> Aberdeen
> AB24 2TZ
>
>
> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From bolker at ufl.edu  Fri Jan 23 00:42:28 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 22 Jan 2009 18:42:28 -0500
Subject: [R-sig-ME] Teaching Mixed Effects
Message-ID: <49790464.4080700@ufl.edu>


  Wow.
  Two very small points:

> 
> I feel that the likelihood ratio is a perfectly reasonable way of
> comparing two model fits where one is a special case of the other.  In
> fact, if the models have been fit by maximum likelihood, the
> likelihood ratio would, I think, be the first candidate for a test
> statistic.  The problem with likelihood ratio tests is not the
> likelihood ratio, per se -- it is converting the likelihood ratio to a
> p-value.  You need to be able to evaluate the distribution of the
> likelihood ratio under the null hypothesis.  The chi-square
> approximation to the distribution is exactly that - an approximation -
> and its validity depends on not testing at the boundary and on having
> a large sample, in some sense of the sample size.  If I were really
> interested in evaluating a p-value for the likelihood ratio I would
> probably try a parametric bootstrap to get a reference distribution.
> 

  Even if we are not p-value obsessed, we would still presumably
like to be able make some kind of (even informal) inference from
the difference in fits, perhaps at the level of "model 1 fits
(much better|a little better|about the same|a little worse|much worse)
than model 2", or "the range of plausible estimates for this
parameter is (tiny|small|moderate|large|absurdly large)". To
do that we need some kind of metric (if we have not yet fled
to Bayesian or quasi-Bayesian methods) for the range of
the deviance under some kind of null case -- for example,
where should we set cutoff levels on the likelihood profile
to determine confidence regions for parameters? Parametric
bootstrap makes sense, although it is a little scary to think
e.g. of doing a power analysis for such a procedure ...

>> 2.3) "Testing" random effects is considered inappropriate (but see 2.1
>> for methods?).
>>
>>  BMB: I don't think this is necessarily true. Admittedly it is a point
>> null hypothesis (variance will never be _exactly_ zero), but I
>> can certainly see cases ("does variation among species contribute
>> significantly to the overall variance observed"?) where one would
>> want to test this question.  This is a bit murky but I think the
>> distinction is often between random effects as part of an experimental
>> design (no point in testing, not interesting) and random effects
>> as observational data.
> 
> Actually the MLE or the REML estimate of a variance component can
> indeed be zero.  The residual variance (i.e. the variance of the "per
> observation" noise term) is only zero for artificial data but the
> estimates of other variance components can be exactly zero.
> 

  I agree that there is a non-zero probability that the _estimate_
will be exactly zero, but my point is that there is really no chance
in reality that species, blocks, or other random effects will
not vary at all ... (sorry for the convolution of that last sentence)

  Ben Bolker



From bates at stat.wisc.edu  Fri Jan 23 00:42:45 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 22 Jan 2009 17:42:45 -0600
Subject: [R-sig-ME] Nested random factor
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B7F913660DC@VMAILB.uoa.abdn.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B7F911B5302@VMAILB.uoa.abdn.ac.uk>
	<40e66e0b0901221514h1bbe4ba6u91fbfdb93632dd8c@mail.gmail.com>
	<B9D1301370916C44B5874AF340C18B9B7F913660DC@VMAILB.uoa.abdn.ac.uk>
Message-ID: <40e66e0b0901221542y55e89a14wa68a7a1bbc603b7c@mail.gmail.com>

On Thu, Jan 22, 2009 at 5:28 PM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:
> Thank you so much for your reply.
> Can I just check the labelling of my data regards you comment "ensure that each distinct site has a distinct label - that is,
> avoid "implicit nesting"".

You do have a distinct label for each site.

I hesitate to describe what I call "implicit nesting" because my point
is that it is not a good way to organize the data.  However many
people use it, I think as a holdover from earlier techniques and
software.  It would be a layout like

> Farm          Site               Sample period      Abundance
> 1                 1                     1                             10
> 2                 1                      1                              3
> 3                 1                      1                              2
...
> 1                 2                        1                            2
> 2                 2                          1                         3
> 3                 2                         1                          22

That is, the two sites within a farm are always labeled '1' and '2'
and we are supposed to somehow know that Site '1' in Farm '1' is not
in any way related to Site '1' in Farm '2'.

If you didn't plan to organize your data that way then ignore the
whole issue.  You have 14 sites with 14 distinct labels so everything
will work out.

> for example:
> 7 farms, 14 margins (2 margins in each farm).  All margins sampled 4 times
> DUMMY DATA:
>
> Random effect specified as (1|Farm/Site)
>
> Farm          Site               Sample period      Abundance
> 1                 1                     1                             10
> 2                 2                      1                              3
> 3                 3                      1                              2
> 4                 4                       1                             6
> 5                 5                        1                          13
> 6                 6                          1                        11
> 7                 7                     1                              12
> 1                 8                        1                            2
> 2                 9                          1                         3
> 3                 10                         1                          22
> 4                 11                       1                           1
> 5                 12                        1                           33
> 6                 13                        1                           2
> 7                 14                         1                          13
> 1                 1                          2                             13
> 2                 2                            2                             12
> 3                 3                            2                            11
> 4                 4                              2                              3
> 5                 5                             2                        6
> 6                 6                           2                             5
> 7                 7                               2                      4
> 1                 8                             2                         12
> 2                 9                              2                          24
> 3                 10                           2                      25
> 4                 11                          2                       3
> 5                 12                         2                        22
> 6                 13                          2                         23
> 7                 14                          2                          8
>
> Many many thanks
> Anna
>
> ________________________________________
> From: dmbates at gmail.com [dmbates at gmail.com] On Behalf Of Douglas Bates [bates at stat.wisc.edu]
> Sent: 22 January 2009 23:14
> To: Renwick, A. R.
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Nested random factor
>
> On Thu, Jan 22, 2009 at 11:10 AM, Renwick, A. R. <a.renwick at abdn.ac.uk> wrote:
>
>> I am running a LME with a nested random factor using the 'nlme' package and ahave a query interpreting the varince of the netsed random factor.
>> My data contains samples from 7 farms, and within these farms I sampled at 2 sites.  The total number of sites was therefore 14, of which I sampled 4 times.
>
>> I therefore have used the following random effect:
>> 1|Farm/Site
>
>> My final model is below:
>
>> mrem<-lme(log(Nhat+1)~ Width + crop+sess +Width:sess ,random=~1|Farm/Site,data=all, method="REML",correlation=NULL,weights=varIdent(form=~1|Group3))
>
>> summary(mrem)
>
>> #part of the summary
>> #Linear mixed-effects model fit by REML
>> # Data: all
>> #       AIC      BIC    logLik
>> #  106.2403 136.8477 -34.12013
>> #
>> #Random effects:
>> # Formula: ~1 | Farm
>> #        (Intercept)
>> #StdDev:   0.1597866
>> #
>> # Formula: ~1 | Site %in% Farm
>> #        (Intercept) Residual
>> #StdDev:   0.2458009 0.928392
>> #Number of Observations: 51
>> #Number of Groups:
>> #          Farm Site %in% Farm
>> #             7             14
>
>> I would like to check that I am interpreting the random effect correctly.
>> 1) The std dev of the between-farm variance is 0.16
>
> That phrase is a bit peculiar.  I think I would say either,
>
> The standard deviation of the random effect for farm is 0.16
>
> or
>
> The between-farm variance is (0.16)^2
>
>> 2) The std dev of the between-site variance is 0.25
>
> In your terminology I would say, "The variance between sites within
> farms is (0.25)^2."
>
>> 3) The std dev of the within-site variance is 0.93.
>
> Again, I would phrase this as "The variance within sites is (0.93)^2."
>
>> However, I think I may have missed out the 'nesting' part,i.e the variance within sites within farms, and the variance between sites within farms.
>
> Because sites are nested within farms, "within site" is the same as
> "within site, within farms" and "between sites within farms" is your
> line 2) above.  If you were to eliminate the random effect for farm
> (and ensure that each distinct site has a distinct label - that is,
> avoid "implicit nesting") then you would estimate a "between site"
> variance.
>
> I'm not surprised that this seems confusing.  I certainly found all
> the material on variance components with mean squares and expected
> mean squares to be confusing when I was first exposed to it. I vowed I
> would stay as far away from that type of statistics as I could because
> it was just so tedious and messy.  Obviously I didn't succeed in
> staying away from it.
>
>> Any help to clarify this point would be much appreciated.
>>
>> Many thanks,
>> Anna
>>
>> Anna Renwick
>> Institute of Biological & Environment Sciences
>> University of Aberdeen
>> Zoology Building
>> Tillydrone Avenue
>> Aberdeen
>> AB24 2TZ
>>
>>
>> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
> The University of Aberdeen is a charity registered in Scotland, No SC013683.
>



From rmetras at rvc.ac.uk  Fri Jan 23 11:33:22 2009
From: rmetras at rvc.ac.uk (Metras, Raphaelle)
Date: Fri, 23 Jan 2009 10:33:22 -0000
Subject: [R-sig-ME] intra-class correlation coeff
In-Reply-To: <40e66e0b0901221542y55e89a14wa68a7a1bbc603b7c@mail.gmail.com>
References: <B9D1301370916C44B5874AF340C18B9B7F911B5302@VMAILB.uoa.abdn.ac.uk><40e66e0b0901221514h1bbe4ba6u91fbfdb93632dd8c@mail.gmail.com><B9D1301370916C44B5874AF340C18B9B7F913660DC@VMAILB.uoa.abdn.ac.uk>
	<40e66e0b0901221542y55e89a14wa68a7a1bbc603b7c@mail.gmail.com>
Message-ID: <C24CC6A41A748448B8B39036068C09CC0177ACC4@cmw2kex01.rvc.ac.uk>

Hello,

I am a very beginner with R and mixed-models, so please apologize if you
think my questions are naive.

I am fitting a glmer Poisson, with one variable as random effect
(market) and 2 variables as fixed effects.
My observations are clustered markets, there are 3 markets.

When looking at the variance of the random effect, and it is close to
zero (0.07484).

I would like to know if it is possible to extract the intra-class
correlation coefficient somehow, or if knowing the between market
variance (0.07484) is enough to say that there is almost no clustering.

Thank you very much, I copy the ouput below:

Generalized linear mixed model fit by the Laplace approximation 
Formula: clear_bsk ~ dist_mkt + same_trader + offset(log(no_bsk)) + (1 |
market) 
   Data: essai 
  AIC   BIC logLik deviance
 55.9 63.39 -23.95    47.91
Random effects:
 Groups Name        Variance Std.Dev.
 market (Intercept) 0.07484  0.27357 
Number of obs: 48, groups: market, 3

Fixed effects:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -1.34246    0.32716  -4.103 4.07e-05 ***
dist_mkt     -0.02948    0.01380  -2.137 0.032639 *  
same_traderY  0.99278    0.27366   3.628 0.000286 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Correlation of Fixed Effects:
            (Intr) dst_mk
dist_mkt    -0.546       
same_tradrY -0.656  0.282



From atyre2 at unlnotes.unl.edu  Fri Jan 23 15:27:50 2009
From: atyre2 at unlnotes.unl.edu (Andrew J Tyre)
Date: Fri, 23 Jan 2009 08:27:50 -0600
Subject: [R-sig-ME] intra-class correlation coeff
In-Reply-To: <C24CC6A41A748448B8B39036068C09CC0177ACC4@cmw2kex01.rvc.ac.uk>
Message-ID: <OFC12C1207.A450BE15-ON86257547.004EB8BE-86257547.004F6311@unl.edu>

Raphaelle,

The model you are estimating assumes that the differences among markets 
(on a log scale) are multivariate normal with mean 0 and variance sigma^2 
= 0.07 - and zero correlation among markets. Another way of interpreting 
the magnitude of that value is to compare the standard deviation of the 
market random effect with the size of the intercept - so you have a CV 
there of 0.27 / |-1.37| = 20% not an insignificant amount of variation 
among markets. This model allows the mean responses to be similar WITHIN 
markets - accounting for the correlations between traders at the same 
market. 

hth,

Drew Tyre

School of Natural Resources
University of Nebraska-Lincoln
416 Hardin Hall, East Campus
3310 Holdrege Street
Lincoln, NE 68583-0974

phone: +1 402 472 4054 
fax: +1 402 472 2946
email: atyre2 at unl.edu
http://snr.unl.edu/tyre



From nikko at hailmail.net  Fri Jan 23 18:01:26 2009
From: nikko at hailmail.net (Nicholas Lewin-Koh)
Date: Fri, 23 Jan 2009 09:01:26 -0800
Subject: [R-sig-ME] (no subject)
In-Reply-To: <mailman.2022.1232664028.7150.r-sig-mixed-models@r-project.org>
References: <mailman.2022.1232664028.7150.r-sig-mixed-models@r-project.org>
Message-ID: <1232730086.32255.1296393301@webmail.messagingengine.com>

Wow!
Now I have to go back and reread Bates and Watts. Thank you  Doug, for
that very insightful
commentary. Would Bruce Lindsay's work on the geometry of mixtures be
applicable in
the mixed model setting? Maybe my understanding is a bit shaky (not the
first time nor the last)
but aren't the mixed effects, in the case of fixed effects comparisons,
nuisance parameters? 
So at least in the case of the likelihood ratio, provided that the
assumed family (link included) 
the likelihood ratio is in essence a sort of odds ratio between the two
models. Whether or not
a p-value is valid or even necessary is a different question, and comes
down to how well
the distribution can be approximated.

Nicholas
> Date: Thu, 22 Jan 2009 16:40:11 -0600
> From: Douglas Bates <bates at stat.wisc.edu>
> Subject: Re: [R-sig-ME] Teaching Mixed Effects
> To: "Bolker,Benjamin Michael" <bolker at ufl.edu>
> Cc: R Models Mixed <r-sig-mixed-models at r-project.org>
> Message-ID:
> 	<40e66e0b0901221440lcc6cc9cmedc91fa7714081ed at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> My thanks to Andrew for starting this thread and to Ben for his
> responses.  I will add more responses below.
> 
> I'm sorry that my responses have been delayed - it happens that this
> is the first week of our spring semester and I have been tied up
> getting courses underway.
> 
> On Tue, Jan 20, 2009 at 8:59 AM, Bolker,Benjamin Michael <bolker at ufl.edu>
> wrote:
> >   Some comments (others will certainly have more to add)
> > ________________________________________
> > From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Andrew Beckerman [a.beckerman at sheffield.ac.uk]
> > Sent: Tuesday, January 20, 2009 6:47 AM
> > To: R Models Mixed
> > Subject: [R-sig-ME] Teaching Mixed Effects
> >
> > Dear R-Mixed people -
> >
> > I am about to embark on a day of attempting to teach some aspects of
> > mixed models using R to PhD students.  I was wondering if anyone would
> > be willing to indulge in this summary below, developed through reading
> > threads on R-Mixed and R-Help over the past few months, and vet my
> > list of issues/questions/topics (4)  associated with mixed models?
> >
> > Let me reduce any rising blood pressure by saying that I understand
> > (possibly) and accept why there are no p-values in lmer, and NONE of
> > the comments/questions below are about why lmer does not produce
> > sensible df's and p-values to calculate significance (Phew).
> >
> > #######################
> >
> > First, a technical question:
> >
> > Based on these two threads:
> > https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html
> > https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001456.html
> >
> > IS mcmcsamp() broken for "complicated" random effects? Is it in good
> > enough shape to teach for "simple" gaussian mixed models, to
> > demonstrate the principle?
> >
> >  BMB: good question. I'd like to know, although I would
> > guess that it would be OK for demonstrating the principle.
> > You could try it out and see if it's sensible ...
> 
> I have not verified the results from the current mcmcsamp, even for
> simple Gaussian models.  They seem reasonable for these models but I
> need to look at them much more closely before I could advise trusting
> those results.
> 
> The problem with designing an mcmcsamp method is that the variances of
> the random effects can legitimately be zero and often have a
> non-negligible probability of assuming the value of zero during the
> MCMC iteraions.  However, most methods of sampling from the
> distribution of a variance are based on sampling from the distribution
> of a multiplier of the current value.  If the current value is zero,
> you end up stuck there.
> 
> > #######################
> >
> > Now, here is what I am possibly going to talk about.....
> >
> > 0) Rule number 1 is to design experiments well, and aim for
> > orthogonal, well replicated and  balanced designs.  If you get data
> > that conforms to all of that, old school F-ratio's CAN be used.  If
> > not, see 1-4 below (we will assume that Rule number 1 will be broken).
> 
> >  BMB: good idea.
> 
> In a designed experiment you can do this.  In an observational study
> you can't expect to end up with balanced data.  I also tell my
> students that assuming a balanced design will always produce balanced
> data is contrary to Murphy's Law.  Balance is fragile.  I think it is
> unwise to depend on balance being achieved.
> 
> > 1) It is agreed that the Laplacian methods for estimating terms and
> > "likelihoods" in mixed effects models is considered most reliable
> > (accurate and precise).
> >
> >  BMB:  I believe (but stand ready to be corrected) that
> > PQL vs Laplace vs adaptive Gauss-Hermite quadrature
> > (AGHQ) is an issue for GLMMs, not so much for LMMs.
> > AGHQ is generally even better (but slower) than Laplace,
> > which is a special case.
> 
> Exactly.  Adaptive Gauss-Hermite quadrature uses a quadrature formula
> centered at the conditional modes of the random effects and scaled by
> an approximation to the conditional standard deviations.  (That's
> where the term "adaptive" comes from.)  The quadrature formula depends
> on the number of quadrature points.  Generally we use an odd number of
> points so one of the evaluations is at the conditional mode.  Thus you
> could use a 3-point evaluation or a 5-point evaluation.  The simplest
> formula, the 1-point Gauss-Hermite evaluation, is exactly the Laplace
> approximation.
> 
> It turns out that the Laplace approximation is the only feasible such
> method (at least the only one that I can imagine) when you have more
> than one grouping factor for the random effects.  Even with just one
> grouping factor, if you have vector-valued random effects (meaning
> that you have more than one random effect associated with with each
> level of the grouping factor) then the complexity of AGHQ is the
> number of quadrature points raised to the dimension of the
> vector-valued random effect.  Thus if you have a random intercept and
> a random slope for each subject, say, and you choose a 7 point formula
> then you must do 49 evaluations of the deviance residuals for each
> AGHQ evaluation.
> 
> Oliver Schaubenberger's paper on SAS PROC GLIMMIX, which Ben mentions
> below, discusses the problem of proliferation of the number of
> evaluations required by AGHQ.
> 
> > R (lme4) and ADMB model builder use these
> > methods. SAS nlmixed does, but SAS proc mixed does not appear to.
> > STATA can.  Genstat does/can (see final note below**).
> >
> >  BMB: SAS PROC MIXED does, as of version 9.2
> > see www2.sas.com/proceedings/forum2007/177-2007.pdf
> 
> I think you mean SAS PROC GLIMMIX, not SAS PROC MIXED.
> 
> Regarding the comparison of methods provided by different software,
> remember that details of the implementation can be important.  The
> term "adaptive Gauss-Hermite quadrature" describes a technical
> approach but there can be many variations on the implementation, with
> important consequences for precision, speed and accuracy.  It is a
> gross oversimplification to imagine that such a technique is
> implemented by handing a paper with some formulas to a "programmer"
> and declaring the job done.  Comparing implementations, including
> looking at the intermediate steps, is important but only possible for
> open source implementations.
> 
> > 2) It is agreed that the appropriate test for fixed effects in mixed
> > models should be between nested models.  However, there is no
> > agreement as how to characterise the distributions that would be used
> > to generate p-values.  This is the crux of the Bates et al argument:
> > Likelihood Ratio Tests, Wald tests etc all need to assume a
> > distribtion and some degrees of freedom.  But, in many mixed models,
> > the distribution need not conform to any of our standard ones (t,F,
> > Chi-square etc), especially when the number of subjects in the random
> > effects is small.  Moreover, the relationship between fixed and random
> > effects means that it is nearly impossible, and perhaps not worthwhile
> > to calcuate what might be appropriate "degrees of freedom".
> 
> I'm afraid my response will be rather long-winded, for which I
> apologize.  I feel that as statisticians we have done a disservice to
> those who use statistics by taking complex problems (tests of
> hypotheses for terms in complicated model structures) for which there
> is an enormous simplification in the central special case (linear
> dependence of the mean on the model parameters, "spherical" Gaussian
> distribution) and describing certain computational shortcuts that can
> be used only in this special case.  For the most part we don't even
> hint at the general problem or even discuss the approach used in the
> special case.  We jump right in to a particular form of a summary of a
> computational shortcut, the analysis of variance table.
> 
> I am very pleased to see you describe the situation as a comparison of
> two nested models.  That is indeed how we should approach the problem.
>  We have a general model and a specific model that is a special case
> of the general model.  Obviously the general model will fit the data
> at least as well as the more specialized model.  We wish to determine
> if the fit is sufficiently better to justify using the more general
> model.   To do so we must decide how to judge the extent to which the
> general model fits better and how much more complex it is, so we can
> form some kind of cost/benefit criterion.  Then we must assess the
> value of this test statistic by comparing it to a reference
> distribution.  In the special case of a Gaussian linear model it all
> works out beautifully and we can summarize the results very compactly
> with t statistics or F statistics and their degrees of freedom.  But
> this case is special.  It is misleading to believe that things will
> simplify like this in more general cases.
> 
> Consider the question of how we measure the comparative complexity of
> two models. Typically we can measure the difference in the complexity
> of the models in terms of the number of additional parameters in the
> general model. In the central special case (Gaussian linear models)
> there is a geometric representation of the model where the general
> model corresponds to a linear subspace of the sample space and the
> specific model corresponds to a subspace contained in the general
> model.  The spherical Gaussian assumption (i.e. Gaussian distribution
> with variance-covariance of sigma^2 I, for which the contours of
> constant probability density are spheres) links the probability model
> to the Euclidean geometry of the space.  From those two assumptions
> the methods of testing a general linear hypothesis can be derived
> geometrically.  Gosset derived a statistic that is equivalent to the t
> statistic using analytic means but the current form of the t statistic
> and its relation to degrees of freedom came from Fisher and were based
> on his geometric insight (see the Wikipedia article on Gosset).  And,
> of course, Fisher was able to generalize the t distribution to the F
> distribution, again based on geometric principles.
> 
> In the first chapter of our 1980 book "Nonlinear Regression Analysis
> and Its Applications" Don Watts and I illustrate the geometric
> approach to the t and F statistics for linear models.  Fisher's genius
> was to see that questions about comparative model fits, which are
> related to distances in the geometric representation, can be
> transformed into questions about angles, related to the ratios of
> distances or, equivalently, squared distances of orthogonal components
> of a response vector.  The use of the ratio allows one scale factor
> (variance component in statistical terminology) to be canceled out.
> That is, the null distribution of an F ratio can be expressed without
> needing to specify the unknown error variance.
> 
> Regrettably, the "use a ratio to eliminate a variance component" trick
> only works once.  The first variance component is free but not
> subsequent ones.  If you have a perfectly balanced, orthogonal design
> then you can apply the trick multiple times by isolating certain
> orthogonal submodels and applying the trick within the submodel.  That
> is, you can use estimates from different "error strata" in ratios for
> different terms.  However, that approach based on certain mean squares
> and expected mean squares doesn't generalize well.  The
> computationally tractable approach to estimation of parameters in
> mixed models is maximum likelihood or REML estimation.
> 
> The reason for my long-winded explanation is your saying " This is the
> crux of the Bates et al argument: Likelihood Ratio Tests, Wald tests
> etc all need to assume a distribution and some degrees of freedom.",
> which is a natural statement given the way that we teach analysis of
> variance.  We teach the "what" (i.e. create a table of sums of
> squares, degrees of freedom, mean squares, F ratios, p-values) and not
> the "why".  If you only see the "what" then it is natural to assume
> that there are some magical properties associated with sums of squares
> and degrees of freedom and all we need to do is to figure out which
> sums of squares and which degrees of freedom to use.  The magical
> properties are actually the simplified geometric representation
> (orthogonal linear subspaces, Euclidean geometry) that is unique to
> the Gaussian linear model.  The beauty of that model is that, no
> matter how complicated the representation of a test as a formula may
> be, the geometric representation is always the same, as the ratio of
> the normalized squared lengths of two orthogonal components of the
> response vector.
> 
> When we step away from that Gaussian linear model the simplifications
> all break down.  I spent the early part of career thinking of what
> parts of the Gaussian linear model can be transferred over to the
> Gaussian nonlinear model and what that would mean for inference.  This
> is why I concentrated so much on the geometry of models based on the
> spherical Gaussian distribution.  Generalized linear models retain the
> linear predictor, transformed through an inverse link function to the
> appropriate scale for the mean, but allow for distributions other than
> the Gaussian.  They require another way of thinking.  I think of mixed
> models as being based on two random vectors, the response vector and
> the unobserved random effects.  In the case of a Gaussian linear mixed
> model the conditional distribution of the response, given the random
> effects, is a spherical Gaussian and the unconditional distribution of
> the random effects is multivariate Gaussian but our inferences require
> the marginal distribution of the response.  For a linear mixed model
> this is Gaussian but with more than one variance component.  Getting
> rid of just one variance component won't do, yet the t and F
> derivations depend strongly on just having one variance component that
> can be removed by considering a ratio.
> 
> If we want to perform a hypothesis test related to a fixed-effects
> term in a mixed model (and, for the moment, I will not go into the
> question of whether statistical inferences should always be phrased as
> the result of hypothesis tests) I would claim we should start at the
> beginning, which is considering two models for the data at hand, one
> model being a special case of the other.  We need to decide how we
> measure the quality of the fit of the general model relative to the
> more specific model and how we measure the additional cost of the
> general model.  Then we need to formulate a test statistic.  If we are
> incredibly lucky, the null distribution of this test statistic will be
> well-defined (that is, it will not depend on the values of other,
> unknown parameters) and we can evaluate probabilities associated with
> it.  That does happen in the case of the Gaussian linear model.  I
> personally don't think it will be possible to possible to provide a
> general approach that isolates the effect of a fixed-effect term in a
> linear mixed model using a statistic that does not depend on the
> values of other parameters.  I would be delighted if someone can do it
> but I think there is too much that goes right in the case of the
> Gaussian linear model to expect that the same incredible
> simplifications will apply to other models.
> 
> I don't feel that holy grail of inference in mixed effects models
> should be a magical formula for degrees of freedom to be associated
> with some ratio that looks like a t or an F statistic (despite the
> strongly held beliefs of those in the First Church of the
> Kenward-Roger Approximation). Certainly there has been a lot of
> statistical research related to approximating a difficult distribution
> by a more common distribution but I view this approach as belonging to
> an earlier era.  It is the approach embodied in software like SAS
> whose purpose often seems to be to evaluate difficult formulas and
> provide reams of output including every number that could possibly be
> of interest.  I think we should use the power of current and future
> computers to interact with and explore data and models for the data.
> MCMC is one way to do this.  In nonlinear regression Don and I
> advocated profiling the sum of squares function with respect to the
> values of individual parameters as another way of assessing the actual
> behavior of the model versus trying to formulate an approximation.
> I'm sure that creative people will come up with many other ways to use
> the power of computers to this end.  The point is to explore the
> actual behavior of the model/data combination, not to do just one fit,
> calculate a bunch of summary statistics, apply approximate
> distributions to get p-values and go home.
> 
> If we want to generalize methods of inference we should consider the
> whole chain of reasoning that leads us to the result rather than
> concentrating only on the last step, which is "now that I have the
> value of a statistic how do I convert it to a p-value?" or, even more
> specifically, "I have calculated something that looks like a t-ratio
> so I am going to assume that its distribution is indeed a
> t-distribution which leaves me with only one question and that is on
> how many degrees of freedom".
> 
> I appreciate that this is inconvenient to those applying such models
> to their data.  Philosophical discussions of the fundamentals of
> statistical inference are all well and good but when the referees on
> your paper say you have to provide a p-value for a particular term or
> tests, it is a practical matter, not an academic, theoretical debate.
> Those with sufficient energy and skill plus a stellar reputation as a
> researcher may be able to convince editors that p-values are not the
> "be all and end all" of data analysis - Reinhold Kleigl has been able
> to do this in some cases - but that is definitely not the path of
> least resistance.  The sad reality is that p-values have taken on a
> role as the coin of the realm in science that is far beyond what any
> statistician would have imagined.  (Apparently the default
> "significance level" of 5%, which is considered in some disciplines to
> be carved in stone, resulted from a casual comment by Fisher to the
> effect that he might regard an outcome that would be expected to occur
> less than, say, 1 time in 20 as "significant".)
> 
> It is unhelpful of me not to provide p-values in the lmer summaries
> but I develop the software out of interest in doing it as well as I
> possibly can and not because someone assigns me a task to compute
> something.  I really don't know of a good way of assigning p-values to
> t-ratios or F-ratios so I don't.  I still report the ratio of the
> estimate divided by it standard error, and even call it a t-ratio,
> because I think it is informative.
> 
> >  BMB: and specifically, if you happily use the anova() method
> > to calculate LRT it will do so -- but Pinheiro and Bates 2000 expressly
> > warn against the results/show that they can be "anticonservative"
> > for small sample sizes.  If you have huge sample sizes (i.e.
> > you're not a field ecologist) then LRT may be OK (I seem to remember
> > Bates using it without comment in an analysis of a (large)
> > Bangladeshi arsenic data set).
> 
> I think that was data on artificial contraception use obtained as part
> of a fertility survey in Bangladesh.
> 
> I feel that the likelihood ratio is a perfectly reasonable way of
> comparing two model fits where one is a special case of the other.  In
> fact, if the models have been fit by maximum likelihood, the
> likelihood ratio would, I think, be the first candidate for a test
> statistic.  The problem with likelihood ratio tests is not the
> likelihood ratio, per se -- it is converting the likelihood ratio to a
> p-value.  You need to be able to evaluate the distribution of the
> likelihood ratio under the null hypothesis.  The chi-square
> approximation to the distribution is exactly that - an approximation -
> and its validity depends on not testing at the boundary and on having
> a large sample, in some sense of the sample size.  If I were really
> interested in evaluating a p-value for the likelihood ratio I would
> probably try a parametric bootstrap to get a reference distribution.
> 
> > 2.1) However, Bates et al have mentioned the restricted likelihood
> > ratio test.  There is a package in R implementing some of these tools
> > (RLRsim), but these appear to be limited to and or focused on tests of
> > random effects.
> >
> >   BMB: You can test fixed effects, apparently, but only *in combination with*
> > a test of the random effect (the null hypothesis is always a model
> > without random effects). Also limited to LMMs, and a single
> > random effect.
> >
> > 2.2) What some "other" packages do: SAS can produce wald tests and
> > LRT's if you want, and can implement the kenward-rogers adjustement.
> >
> >  BMB: Kenward-Roger ! (not Rogers)
> >
> > There is some theory behind the K-R, but it is still not dealing with
> > the crux of the problem (see 2).  Genstat uses wald tests and warns
> > you that with small numbers of subjects, these are not reliable.
> > Genstat is also experimenting with HGLM by Nelder and Lee (see **)
> >
> > 2.3) "Testing" random effects is considered inappropriate (but see 2.1
> > for methods?).
> >
> >  BMB: I don't think this is necessarily true. Admittedly it is a point
> > null hypothesis (variance will never be _exactly_ zero), but I
> > can certainly see cases ("does variation among species contribute
> > significantly to the overall variance observed"?) where one would
> > want to test this question.  This is a bit murky but I think the
> > distinction is often between random effects as part of an experimental
> > design (no point in testing, not interesting) and random effects
> > as observational data.
> 
> Actually the MLE or the REML estimate of a variance component can
> indeed be zero.  The residual variance (i.e. the variance of the "per
> observation" noise term) is only zero for artificial data but the
> estimates of other variance components can be exactly zero.
> 
> I think of such situations as an indication that I should simplify the
> model by eliminating such a term.  I have spent most of my career at
> the University of Wisconsin-Madison in a statistics department founded
> by George Box who famously said "All models are wrong; some models are
> useful."  I don't expect a model to be correct, I am only interested
> in whether the terms in the model are useful in explaining the
> observed data.
> 
> > 3) Because of 2, there is the resounding argument that bayesian and or
> > simulation/bootstrapping tools be used to evalaute fixed effects.
> >
> >   BMB: I don't know about "resounding", but OK.  Probably
> > the best option.
> >
> > Current methods proposed and coded, but in various states of
> > usefulness are:
> >
> > mcmcsamp() and HPDinterval() from lme4 + baayen *.fnc's from languageR,
> > BUGS and winBugs,
> > RH Baayen's simulation tools (e.g. page 307 method)
> > Andrew Gelman and Jennifer Hill's tools (e.g. sim() method from
> > package arm)
> > Ben Bolker's suggestions in this list for glmm's (thread: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html)
> >
> > 3.1) These all evalaute "simple" tests of whether beta's and intercept
> > are different than 0, and are linked to the contrasts.  There is no
> > emerging method equivalent to a LRT (but see 2.1 and **Final Note
> > Below).
> >
> >  BMB: I think if you want to calculate the parametric bootstrap/
> > null-hypothesis simulation of the change in deviances between
> > nested models, it's actually reasonably straightforward. See
> > examples on glmm.wikidot.com , especially
> > http://glmm.wikidot.com/basic-glmm-simulation
> > [geared toward GLMMs but even easier for LMMs]
> >
> > 4) Andrew Gelman et al also suggest AIC based methods and model
> > averaging for model inference, given constant random effects.  I think
> > their argument about AIC is that if the "likelihood" is estimated
> > well, relative differences in AIC will be constant, irrespective of
> > any adjustement made to numbers of paramters used in calculating AIC:
> > i.e. as long as the random effects structure stays the same, the
> > relative differences between nested models will not change if the
> > number of paramters is estimated consistently. These methods still do
> > not produce p-values.
> >
> >  BMB: and AIC is an asymptotic method anyway, like
> > LRTs ... which means it is likely to have the same problems, but I don't
> > think anyone has evaluated them.  If you want to use the finite-size
> > corrections (AICc) then you are back in the situation of guessing
> > at residual degrees of freedom ...
> >
> > **Final Note Below - I have noticed a relative lack of discussion of
> > Nelder et al's  H-likelihood and their methods to generate a general
> > method for all heirarchical modelling (HGLM?!).  Would anybody be able
> > to comment?  A recent paper (http://www.springerlink.com/content/17p17r046lx4053r/fulltext.pdf
> > ) that is somewhat beyond my skills, indicates the use of Laplace
> > methods to estimate likelihoods in heirarchical models and various
> > capacity for model inference.
> >
> >   BMB: I think HGLMs are a very promising way of *estimating*
> > the parameters of GLMMs (I too wonder why they don't seem to
> > be discussed much), but I don't think they get us any farther forward
> > with inference.
> >
> > Thanks again, in advance, to anyone who took this on..... apologies
> > for any glaring errors or assignment of ideas to people incorrectly.
> >
> > Andrew
> >
> > ---------------------------------------------------------------------------------
> > Dr. Andrew Beckerman
> > Department of Animal and Plant Sciences, University of Sheffield,
> > Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
> > ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
> > http://www.beckslab.staff.shef.ac.uk/
> >
> > http://www.flickr.com/photos/apbeckerman/
> > http://www.warblefly.co.uk
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> 
> 
> 
> ------------------------------
> 
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> End of R-sig-mixed-models Digest, Vol 25, Issue 24
> **************************************************



From steibelj at msu.edu  Fri Jan 23 18:09:36 2009
From: steibelj at msu.edu (Juan Pedro Steibel)
Date: Fri, 23 Jan 2009 12:09:36 -0500
Subject: [R-sig-ME] Teaching Mixed Effects
In-Reply-To: <40e66e0b0901221440lcc6cc9cmedc91fa7714081ed@mail.gmail.com>
References: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk>	<2180B808CDA3404B8FF30C1EA0AC09EEB4D648@UFEXCH-MBXCL03.ad.ufl.edu>
	<40e66e0b0901221440lcc6cc9cmedc91fa7714081ed@mail.gmail.com>
Message-ID: <4979F9D0.6080107@msu.edu>

Hello all,
This is my first post in the list as I just started to use lmer for my 
routine analyses (side by side with P.Mixed... just for now 8^D ).

The following comment caught my attention:
> I have not verified the results from the current mcmcsamp, even for
> simple Gaussian models.  They seem reasonable for these models but I
> need to look at them much more closely before I could advise trusting
> those results.
>
> The problem with designing an mcmcsamp method is that the variances of
> the random effects can legitimately be zero and often have a
> non-negligible probability of assuming the value of zero during the
> MCMC iteraions.  However, most methods of sampling from the
> distribution of a variance are based on sampling from the distribution
> of a multiplier of the current value.  If the current value is zero,
> you end up stuck there.
>   
If I understand correctly, it is claimed that once the Markov Chain hits 
a value 0 for a given VC, it stays there. Is this Correct? Should I 
interpret the statement above differently?

This is not the behavior I am observing in mcmcsamp, fitting models with 
a non-negligible posterior probability of certain VC=0, I see the chain 
hitting zero for a while, then leaving (VC>0) and back... the actual 
mixing is very good, even for a VC that is estimated as zero by REML 
(posterior mode of mcmc is ~practically~ zero). 
Thanks in advance
JP



-- 
=============================
Juan Pedro Steibel

Assistant Professor
Statistical Genetics and Genomics

Department of Animal Science & 
Department of Fisheries and Wildlife

Michigan State University
1205-I Anthony Hall
East Lansing, MI
48824 USA 

Phone: 1-517-353-5102
E-mail: steibelj at msu.edu



From bates at stat.wisc.edu  Fri Jan 23 18:48:59 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 23 Jan 2009 11:48:59 -0600
Subject: [R-sig-ME] Teaching Mixed Effects
In-Reply-To: <4979F9D0.6080107@msu.edu>
References: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk>
	<2180B808CDA3404B8FF30C1EA0AC09EEB4D648@UFEXCH-MBXCL03.ad.ufl.edu>
	<40e66e0b0901221440lcc6cc9cmedc91fa7714081ed@mail.gmail.com>
	<4979F9D0.6080107@msu.edu>
Message-ID: <40e66e0b0901230948s3dbd004dn2ab76c144e56e46@mail.gmail.com>

On Fri, Jan 23, 2009 at 11:09 AM, Juan Pedro Steibel <steibelj at msu.edu> wrote:
> Hello all,
> This is my first post in the list as I just started to use lmer for my
> routine analyses (side by side with P.Mixed... just for now 8^D ).
>
> The following comment caught my attention:
>>
>> I have not verified the results from the current mcmcsamp, even for
>> simple Gaussian models.  They seem reasonable for these models but I
>> need to look at them much more closely before I could advise trusting
>> those results.
>>
>> The problem with designing an mcmcsamp method is that the variances of
>> the random effects can legitimately be zero and often have a
>> non-negligible probability of assuming the value of zero during the
>> MCMC iteraions.  However, most methods of sampling from the
>> distribution of a variance are based on sampling from the distribution
>> of a multiplier of the current value.  If the current value is zero,
>> you end up stuck there.
>>
>
> If I understand correctly, it is claimed that once the Markov Chain hits a
> value 0 for a given VC, it stays there. Is this Correct? Should I interpret
> the statement above differently?
>
> This is not the behavior I am observing in mcmcsamp, fitting models with a
> non-negligible posterior probability of certain VC=0, I see the chain
> hitting zero for a while, then leaving (VC>0) and back... the actual mixing
> is very good, even for a VC that is estimated as zero by REML (posterior
> mode of mcmc is ~practically~ zero). Thanks in advance

I should have been more careful in what I wrote.  I meant to say that
dealing with the possibility of having a zero variance component is
the reason that it is difficult to formulate MCMC methods for these
models.  The straightforward method doesn't work.

The mcmcsamp function doesn't use the straightforward sampling method
for the variance components.  It uses an indirect method that allows
it to visit zero without getting stuck.



From bates at stat.wisc.edu  Fri Jan 23 20:27:37 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 23 Jan 2009 13:27:37 -0600
Subject: [R-sig-ME] UseR!2009
Message-ID: <40e66e0b0901231127l405f2a33vefbf5048fc18bc6c@mail.gmail.com>

Please note that the UseR!2009 conference, to be held in Rennes,
France July 8-10 is now open for submission of abstracts.  See
http://www.R-project.org/useR-2009/

I am on the program committee and would be pleased to organize a
session on a topic related to mixed models.  It could be oriented
toward a specific applications area (social sciences, life sciences
and agriculture, education, pharmacology) or towards specific types of
model (linear mixed models, generalized linear mixed models, nonlinear
mixed models) or towards certain types of data layouts (models with
nested random effects or crossed random effects or partially crossed
random effects) or towards specific technical issues
(assessing precision of fixed-effects estimates, hypothesis testing,
model comparisons)

Regrettably, funds for invited speakers are already committed so I am
seeking speakers who would be willing both to pay for their attendance
and to prepare a talk.  If you have ideas for a suitable session
please email the list or send me mail off-list.



From bates at stat.wisc.edu  Fri Jan 23 20:52:26 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 23 Jan 2009 13:52:26 -0600
Subject: [R-sig-ME] (no subject)
In-Reply-To: <1232730086.32255.1296393301@webmail.messagingengine.com>
References: <mailman.2022.1232664028.7150.r-sig-mixed-models@r-project.org>
	<1232730086.32255.1296393301@webmail.messagingengine.com>
Message-ID: <40e66e0b0901231152t701a2abeh36ced8f0cd6ce766@mail.gmail.com>

On Fri, Jan 23, 2009 at 11:01 AM, Nicholas Lewin-Koh <nikko at hailmail.net> wrote:
> Wow!
> Now I have to go back and reread Bates and Watts. Thank you  Doug, for
> that very insightful commentary.

> Would Bruce Lindsay's work on the geometry of mixtures be applicable in
> the mixed model setting?

I can't say because I am not familiar with that work.

> Maybe my understanding is a bit shaky (not the
> first time nor the last)
> but aren't the mixed effects, in the case of fixed effects comparisons,
> nuisance parameters?

It depends.  From the analytic point of view, yes they are.  From the
geometric point of view they are another set of coefficients in a
linear predictor so they use up dimensions.  However, their estimates
are not ordinary least squares estimates they are penalized least
squares estimates so they don't really correspond to full dimensions.

> So at least in the case of the likelihood ratio, provided that the
> assumed family (link included)
> the likelihood ratio is in essence a sort of odds ratio between the two
> models.

I haven't really thought of things in that way so I'm not sure what to
say about it.

> Whether or not
> a p-value is valid or even necessary is a different question, and comes
> down to how well
> the distribution can be approximated.

A p-value is a useful metric, when we can calculate it reliably.
However, I don't think we should regard it as the sole purpose of
statistical inference.



From bates at stat.wisc.edu  Fri Jan 23 21:57:27 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 23 Jan 2009 14:57:27 -0600
Subject: [R-sig-ME] intra-class correlation coeff
In-Reply-To: <C24CC6A41A748448B8B39036068C09CC0177ACC4@cmw2kex01.rvc.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B7F911B5302@VMAILB.uoa.abdn.ac.uk>
	<40e66e0b0901221514h1bbe4ba6u91fbfdb93632dd8c@mail.gmail.com>
	<B9D1301370916C44B5874AF340C18B9B7F913660DC@VMAILB.uoa.abdn.ac.uk>
	<40e66e0b0901221542y55e89a14wa68a7a1bbc603b7c@mail.gmail.com>
	<C24CC6A41A748448B8B39036068C09CC0177ACC4@cmw2kex01.rvc.ac.uk>
Message-ID: <40e66e0b0901231257t2f7e11e1we86544b5d8f0bd81@mail.gmail.com>

I saw Andrew Tyne's reply and agree with what he said.

Another way of approaching this is to realize that you only have 3
distinct levels of market.  That's in the area where you probably are
better off modeling it as a fixed-effects term, rather than a
random-effects term.  It may make sense logically to regard it as a
random effect but practically it is difficult to estimate a variance
from only a few observations.  We can get an estimate but often it has
very poor precision.

I would suggest modeling it as a fixed effect and seeing if the term
is significant there.

On Fri, Jan 23, 2009 at 4:33 AM, Metras, Raphaelle <rmetras at rvc.ac.uk> wrote:
> Hello,
>
> I am a very beginner with R and mixed-models, so please apologize if you
> think my questions are naive.
>
> I am fitting a glmer Poisson, with one variable as random effect
> (market) and 2 variables as fixed effects.
> My observations are clustered markets, there are 3 markets.
>
> When looking at the variance of the random effect, and it is close to
> zero (0.07484).
>
> I would like to know if it is possible to extract the intra-class
> correlation coefficient somehow, or if knowing the between market
> variance (0.07484) is enough to say that there is almost no clustering.
>
> Thank you very much, I copy the ouput below:
>
> Generalized linear mixed model fit by the Laplace approximation
> Formula: clear_bsk ~ dist_mkt + same_trader + offset(log(no_bsk)) + (1 |
> market)
>   Data: essai
>  AIC   BIC logLik deviance
>  55.9 63.39 -23.95    47.91
> Random effects:
>  Groups Name        Variance Std.Dev.
>  market (Intercept) 0.07484  0.27357
> Number of obs: 48, groups: market, 3
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -1.34246    0.32716  -4.103 4.07e-05 ***
> dist_mkt     -0.02948    0.01380  -2.137 0.032639 *
> same_traderY  0.99278    0.27366   3.628 0.000286 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Correlation of Fixed Effects:
>            (Intr) dst_mk
> dist_mkt    -0.546
> same_tradrY -0.656  0.282
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From jones at reed.edu  Sat Jan 24 00:58:56 2009
From: jones at reed.edu (Albyn Jones)
Date: Fri, 23 Jan 2009 15:58:56 -0800
Subject: [R-sig-ME] (no subject)
In-Reply-To: <40e66e0b0901231152t701a2abeh36ced8f0cd6ce766@mail.gmail.com>
References: <mailman.2022.1232664028.7150.r-sig-mixed-models@r-project.org>
	<1232730086.32255.1296393301@webmail.messagingengine.com>
	<40e66e0b0901231152t701a2abeh36ced8f0cd6ce766@mail.gmail.com>
Message-ID: <20090123235856.GC17419@laplace.reed.edu>



On Fri, Jan 23, 2009 at 01:52:26PM -0600, Douglas Bates wrote:

> 
> > Would Bruce Lindsay's work on the geometry of mixtures be applicable in
> > the mixed model setting?
> 
> I can't say because I am not familiar with that work.
> 

I don't have it handy, but as I recall, it deals with mixture
distributions, not the mixed model.

albyn



From john.maindonald at anu.edu.au  Sat Jan 24 02:50:53 2009
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sat, 24 Jan 2009 12:50:53 +1100
Subject: [R-sig-ME] Teaching Mixed Effects
In-Reply-To: <40e66e0b0901221440lcc6cc9cmedc91fa7714081ed@mail.gmail.com>
References: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk>
	<2180B808CDA3404B8FF30C1EA0AC09EEB4D648@UFEXCH-MBXCL03.ad.ufl.edu>
	<40e66e0b0901221440lcc6cc9cmedc91fa7714081ed@mail.gmail.com>
Message-ID: <13B55D02-544A-491F-856C-7006D1873673@anu.edu.au>

Douglas, your extensive illuminating commentary, added
to questions and comment from list members, makes this
a marvelous list to read and learn from!  It would be good
to get such lists started for other parts of R.  (or are there
other comparable lists, with running highly expert
commentary laid on?)

Whether testing for random effects is legitimate depends
on the purpose.  What I consider wrong-headed is to test as
a preliminary to calculating SEs and/or tests for fixed effects.
I leave aside the question of whether one should be testing
at all.

I've just now posted a revised version of overheads from a
talk I gave a few months ago on multilevel models in R.
As often, I was over-ambitious in what I tried to cover.
Later slides may however be interesting for poring over at
one's leisure.  For inclusion on my web page, they would
benefit from a bit more commentary, which I will try to add
in due course.

http://www.maths.anu.edu.au/~johnm/r-book/2edn/xtras/mlm-ohp.pdf

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 23/01/2009, at 9:40 AM, Douglas Bates wrote:

> My thanks to Andrew for starting this thread and to Ben for his
> responses.  I will add more responses below.
>
> I'm sorry that my responses have been delayed - it happens that this
> is the first week of our spring semester and I have been tied up
> getting courses underway.
>
> On Tue, Jan 20, 2009 at 8:59 AM, Bolker,Benjamin Michael <bolker at ufl.edu 
> > wrote:
>>  Some comments (others will certainly have more to add)
>> ________________________________________
>> From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org 
>> ] On Behalf Of Andrew Beckerman [a.beckerman at sheffield.ac.uk]
>> Sent: Tuesday, January 20, 2009 6:47 AM
>> To: R Models Mixed
>> Subject: [R-sig-ME] Teaching Mixed Effects
>>
>> Dear R-Mixed people -
>>
>> I am about to embark on a day of attempting to teach some aspects of
>> mixed models using R to PhD students.  I was wondering if anyone  
>> would
>> be willing to indulge in this summary below, developed through  
>> reading
>> threads on R-Mixed and R-Help over the past few months, and vet my
>> list of issues/questions/topics (4)  associated with mixed models?
>>
>> Let me reduce any rising blood pressure by saying that I understand
>> (possibly) and accept why there are no p-values in lmer, and NONE of
>> the comments/questions below are about why lmer does not produce
>> sensible df's and p-values to calculate significance (Phew).
>>
>> #######################
>>
>> First, a technical question:
>>
>> Based on these two threads:
>> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html
>> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001456.html
>>
>> IS mcmcsamp() broken for "complicated" random effects? Is it in good
>> enough shape to teach for "simple" gaussian mixed models, to
>> demonstrate the principle?
>>
>> BMB: good question. I'd like to know, although I would
>> guess that it would be OK for demonstrating the principle.
>> You could try it out and see if it's sensible ...
>
> I have not verified the results from the current mcmcsamp, even for
> simple Gaussian models.  They seem reasonable for these models but I
> need to look at them much more closely before I could advise trusting
> those results.
>
> The problem with designing an mcmcsamp method is that the variances of
> the random effects can legitimately be zero and often have a
> non-negligible probability of assuming the value of zero during the
> MCMC iteraions.  However, most methods of sampling from the
> distribution of a variance are based on sampling from the distribution
> of a multiplier of the current value.  If the current value is zero,
> you end up stuck there.
>
>> #######################
>>
>> Now, here is what I am possibly going to talk about.....
>>
>> 0) Rule number 1 is to design experiments well, and aim for
>> orthogonal, well replicated and  balanced designs.  If you get data
>> that conforms to all of that, old school F-ratio's CAN be used.  If
>> not, see 1-4 below (we will assume that Rule number 1 will be  
>> broken).
>
>> BMB: good idea.
>
> In a designed experiment you can do this.  In an observational study
> you can't expect to end up with balanced data.  I also tell my
> students that assuming a balanced design will always produce balanced
> data is contrary to Murphy's Law.  Balance is fragile.  I think it is
> unwise to depend on balance being achieved.
>
>> 1) It is agreed that the Laplacian methods for estimating terms and
>> "likelihoods" in mixed effects models is considered most reliable
>> (accurate and precise).
>>
>> BMB:  I believe (but stand ready to be corrected) that
>> PQL vs Laplace vs adaptive Gauss-Hermite quadrature
>> (AGHQ) is an issue for GLMMs, not so much for LMMs.
>> AGHQ is generally even better (but slower) than Laplace,
>> which is a special case.
>
> Exactly.  Adaptive Gauss-Hermite quadrature uses a quadrature formula
> centered at the conditional modes of the random effects and scaled by
> an approximation to the conditional standard deviations.  (That's
> where the term "adaptive" comes from.)  The quadrature formula depends
> on the number of quadrature points.  Generally we use an odd number of
> points so one of the evaluations is at the conditional mode.  Thus you
> could use a 3-point evaluation or a 5-point evaluation.  The simplest
> formula, the 1-point Gauss-Hermite evaluation, is exactly the Laplace
> approximation.
>
> It turns out that the Laplace approximation is the only feasible such
> method (at least the only one that I can imagine) when you have more
> than one grouping factor for the random effects.  Even with just one
> grouping factor, if you have vector-valued random effects (meaning
> that you have more than one random effect associated with with each
> level of the grouping factor) then the complexity of AGHQ is the
> number of quadrature points raised to the dimension of the
> vector-valued random effect.  Thus if you have a random intercept and
> a random slope for each subject, say, and you choose a 7 point formula
> then you must do 49 evaluations of the deviance residuals for each
> AGHQ evaluation.
>
> Oliver Schaubenberger's paper on SAS PROC GLIMMIX, which Ben mentions
> below, discusses the problem of proliferation of the number of
> evaluations required by AGHQ.
>
>> R (lme4) and ADMB model builder use these
>> methods. SAS nlmixed does, but SAS proc mixed does not appear to.
>> STATA can.  Genstat does/can (see final note below**).
>>
>> BMB: SAS PROC MIXED does, as of version 9.2
>> see www2.sas.com/proceedings/forum2007/177-2007.pdf
>
> I think you mean SAS PROC GLIMMIX, not SAS PROC MIXED.
>
> Regarding the comparison of methods provided by different software,
> remember that details of the implementation can be important.  The
> term "adaptive Gauss-Hermite quadrature" describes a technical
> approach but there can be many variations on the implementation, with
> important consequences for precision, speed and accuracy.  It is a
> gross oversimplification to imagine that such a technique is
> implemented by handing a paper with some formulas to a "programmer"
> and declaring the job done.  Comparing implementations, including
> looking at the intermediate steps, is important but only possible for
> open source implementations.
>
>> 2) It is agreed that the appropriate test for fixed effects in mixed
>> models should be between nested models.  However, there is no
>> agreement as how to characterise the distributions that would be used
>> to generate p-values.  This is the crux of the Bates et al argument:
>> Likelihood Ratio Tests, Wald tests etc all need to assume a
>> distribtion and some degrees of freedom.  But, in many mixed models,
>> the distribution need not conform to any of our standard ones (t,F,
>> Chi-square etc), especially when the number of subjects in the random
>> effects is small.  Moreover, the relationship between fixed and  
>> random
>> effects means that it is nearly impossible, and perhaps not  
>> worthwhile
>> to calcuate what might be appropriate "degrees of freedom".
>
> I'm afraid my response will be rather long-winded, for which I
> apologize.  I feel that as statisticians we have done a disservice to
> those who use statistics by taking complex problems (tests of
> hypotheses for terms in complicated model structures) for which there
> is an enormous simplification in the central special case (linear
> dependence of the mean on the model parameters, "spherical" Gaussian
> distribution) and describing certain computational shortcuts that can
> be used only in this special case.  For the most part we don't even
> hint at the general problem or even discuss the approach used in the
> special case.  We jump right in to a particular form of a summary of a
> computational shortcut, the analysis of variance table.
>
> I am very pleased to see you describe the situation as a comparison of
> two nested models.  That is indeed how we should approach the problem.
> We have a general model and a specific model that is a special case
> of the general model.  Obviously the general model will fit the data
> at least as well as the more specialized model.  We wish to determine
> if the fit is sufficiently better to justify using the more general
> model.   To do so we must decide how to judge the extent to which the
> general model fits better and how much more complex it is, so we can
> form some kind of cost/benefit criterion.  Then we must assess the
> value of this test statistic by comparing it to a reference
> distribution.  In the special case of a Gaussian linear model it all
> works out beautifully and we can summarize the results very compactly
> with t statistics or F statistics and their degrees of freedom.  But
> this case is special.  It is misleading to believe that things will
> simplify like this in more general cases.
>
> Consider the question of how we measure the comparative complexity of
> two models. Typically we can measure the difference in the complexity
> of the models in terms of the number of additional parameters in the
> general model. In the central special case (Gaussian linear models)
> there is a geometric representation of the model where the general
> model corresponds to a linear subspace of the sample space and the
> specific model corresponds to a subspace contained in the general
> model.  The spherical Gaussian assumption (i.e. Gaussian distribution
> with variance-covariance of sigma^2 I, for which the contours of
> constant probability density are spheres) links the probability model
> to the Euclidean geometry of the space.  From those two assumptions
> the methods of testing a general linear hypothesis can be derived
> geometrically.  Gosset derived a statistic that is equivalent to the t
> statistic using analytic means but the current form of the t statistic
> and its relation to degrees of freedom came from Fisher and were based
> on his geometric insight (see the Wikipedia article on Gosset).  And,
> of course, Fisher was able to generalize the t distribution to the F
> distribution, again based on geometric principles.
>
> In the first chapter of our 1980 book "Nonlinear Regression Analysis
> and Its Applications" Don Watts and I illustrate the geometric
> approach to the t and F statistics for linear models.  Fisher's genius
> was to see that questions about comparative model fits, which are
> related to distances in the geometric representation, can be
> transformed into questions about angles, related to the ratios of
> distances or, equivalently, squared distances of orthogonal components
> of a response vector.  The use of the ratio allows one scale factor
> (variance component in statistical terminology) to be canceled out.
> That is, the null distribution of an F ratio can be expressed without
> needing to specify the unknown error variance.
>
> Regrettably, the "use a ratio to eliminate a variance component" trick
> only works once.  The first variance component is free but not
> subsequent ones.  If you have a perfectly balanced, orthogonal design
> then you can apply the trick multiple times by isolating certain
> orthogonal submodels and applying the trick within the submodel.  That
> is, you can use estimates from different "error strata" in ratios for
> different terms.  However, that approach based on certain mean squares
> and expected mean squares doesn't generalize well.  The
> computationally tractable approach to estimation of parameters in
> mixed models is maximum likelihood or REML estimation.
>
> The reason for my long-winded explanation is your saying " This is the
> crux of the Bates et al argument: Likelihood Ratio Tests, Wald tests
> etc all need to assume a distribution and some degrees of freedom.",
> which is a natural statement given the way that we teach analysis of
> variance.  We teach the "what" (i.e. create a table of sums of
> squares, degrees of freedom, mean squares, F ratios, p-values) and not
> the "why".  If you only see the "what" then it is natural to assume
> that there are some magical properties associated with sums of squares
> and degrees of freedom and all we need to do is to figure out which
> sums of squares and which degrees of freedom to use.  The magical
> properties are actually the simplified geometric representation
> (orthogonal linear subspaces, Euclidean geometry) that is unique to
> the Gaussian linear model.  The beauty of that model is that, no
> matter how complicated the representation of a test as a formula may
> be, the geometric representation is always the same, as the ratio of
> the normalized squared lengths of two orthogonal components of the
> response vector.
>
> When we step away from that Gaussian linear model the simplifications
> all break down.  I spent the early part of career thinking of what
> parts of the Gaussian linear model can be transferred over to the
> Gaussian nonlinear model and what that would mean for inference.  This
> is why I concentrated so much on the geometry of models based on the
> spherical Gaussian distribution.  Generalized linear models retain the
> linear predictor, transformed through an inverse link function to the
> appropriate scale for the mean, but allow for distributions other than
> the Gaussian.  They require another way of thinking.  I think of mixed
> models as being based on two random vectors, the response vector and
> the unobserved random effects.  In the case of a Gaussian linear mixed
> model the conditional distribution of the response, given the random
> effects, is a spherical Gaussian and the unconditional distribution of
> the random effects is multivariate Gaussian but our inferences require
> the marginal distribution of the response.  For a linear mixed model
> this is Gaussian but with more than one variance component.  Getting
> rid of just one variance component won't do, yet the t and F
> derivations depend strongly on just having one variance component that
> can be removed by considering a ratio.
>
> If we want to perform a hypothesis test related to a fixed-effects
> term in a mixed model (and, for the moment, I will not go into the
> question of whether statistical inferences should always be phrased as
> the result of hypothesis tests) I would claim we should start at the
> beginning, which is considering two models for the data at hand, one
> model being a special case of the other.  We need to decide how we
> measure the quality of the fit of the general model relative to the
> more specific model and how we measure the additional cost of the
> general model.  Then we need to formulate a test statistic.  If we are
> incredibly lucky, the null distribution of this test statistic will be
> well-defined (that is, it will not depend on the values of other,
> unknown parameters) and we can evaluate probabilities associated with
> it.  That does happen in the case of the Gaussian linear model.  I
> personally don't think it will be possible to possible to provide a
> general approach that isolates the effect of a fixed-effect term in a
> linear mixed model using a statistic that does not depend on the
> values of other parameters.  I would be delighted if someone can do it
> but I think there is too much that goes right in the case of the
> Gaussian linear model to expect that the same incredible
> simplifications will apply to other models.
>
> I don't feel that holy grail of inference in mixed effects models
> should be a magical formula for degrees of freedom to be associated
> with some ratio that looks like a t or an F statistic (despite the
> strongly held beliefs of those in the First Church of the
> Kenward-Roger Approximation). Certainly there has been a lot of
> statistical research related to approximating a difficult distribution
> by a more common distribution but I view this approach as belonging to
> an earlier era.  It is the approach embodied in software like SAS
> whose purpose often seems to be to evaluate difficult formulas and
> provide reams of output including every number that could possibly be
> of interest.  I think we should use the power of current and future
> computers to interact with and explore data and models for the data.
> MCMC is one way to do this.  In nonlinear regression Don and I
> advocated profiling the sum of squares function with respect to the
> values of individual parameters as another way of assessing the actual
> behavior of the model versus trying to formulate an approximation.
> I'm sure that creative people will come up with many other ways to use
> the power of computers to this end.  The point is to explore the
> actual behavior of the model/data combination, not to do just one fit,
> calculate a bunch of summary statistics, apply approximate
> distributions to get p-values and go home.
>
> If we want to generalize methods of inference we should consider the
> whole chain of reasoning that leads us to the result rather than
> concentrating only on the last step, which is "now that I have the
> value of a statistic how do I convert it to a p-value?" or, even more
> specifically, "I have calculated something that looks like a t-ratio
> so I am going to assume that its distribution is indeed a
> t-distribution which leaves me with only one question and that is on
> how many degrees of freedom".
>
> I appreciate that this is inconvenient to those applying such models
> to their data.  Philosophical discussions of the fundamentals of
> statistical inference are all well and good but when the referees on
> your paper say you have to provide a p-value for a particular term or
> tests, it is a practical matter, not an academic, theoretical debate.
> Those with sufficient energy and skill plus a stellar reputation as a
> researcher may be able to convince editors that p-values are not the
> "be all and end all" of data analysis - Reinhold Kleigl has been able
> to do this in some cases - but that is definitely not the path of
> least resistance.  The sad reality is that p-values have taken on a
> role as the coin of the realm in science that is far beyond what any
> statistician would have imagined.  (Apparently the default
> "significance level" of 5%, which is considered in some disciplines to
> be carved in stone, resulted from a casual comment by Fisher to the
> effect that he might regard an outcome that would be expected to occur
> less than, say, 1 time in 20 as "significant".)
>
> It is unhelpful of me not to provide p-values in the lmer summaries
> but I develop the software out of interest in doing it as well as I
> possibly can and not because someone assigns me a task to compute
> something.  I really don't know of a good way of assigning p-values to
> t-ratios or F-ratios so I don't.  I still report the ratio of the
> estimate divided by it standard error, and even call it a t-ratio,
> because I think it is informative.
>
>> BMB: and specifically, if you happily use the anova() method
>> to calculate LRT it will do so -- but Pinheiro and Bates 2000  
>> expressly
>> warn against the results/show that they can be "anticonservative"
>> for small sample sizes.  If you have huge sample sizes (i.e.
>> you're not a field ecologist) then LRT may be OK (I seem to remember
>> Bates using it without comment in an analysis of a (large)
>> Bangladeshi arsenic data set).
>
> I think that was data on artificial contraception use obtained as part
> of a fertility survey in Bangladesh.
>
> I feel that the likelihood ratio is a perfectly reasonable way of
> comparing two model fits where one is a special case of the other.  In
> fact, if the models have been fit by maximum likelihood, the
> likelihood ratio would, I think, be the first candidate for a test
> statistic.  The problem with likelihood ratio tests is not the
> likelihood ratio, per se -- it is converting the likelihood ratio to a
> p-value.  You need to be able to evaluate the distribution of the
> likelihood ratio under the null hypothesis.  The chi-square
> approximation to the distribution is exactly that - an approximation -
> and its validity depends on not testing at the boundary and on having
> a large sample, in some sense of the sample size.  If I were really
> interested in evaluating a p-value for the likelihood ratio I would
> probably try a parametric bootstrap to get a reference distribution.
>
>> 2.1) However, Bates et al have mentioned the restricted likelihood
>> ratio test.  There is a package in R implementing some of these tools
>> (RLRsim), but these appear to be limited to and or focused on tests  
>> of
>> random effects.
>>
>>  BMB: You can test fixed effects, apparently, but only *in  
>> combination with*
>> a test of the random effect (the null hypothesis is always a model
>> without random effects). Also limited to LMMs, and a single
>> random effect.
>>
>> 2.2) What some "other" packages do: SAS can produce wald tests and
>> LRT's if you want, and can implement the kenward-rogers adjustement.
>>
>> BMB: Kenward-Roger ! (not Rogers)
>>
>> There is some theory behind the K-R, but it is still not dealing with
>> the crux of the problem (see 2).  Genstat uses wald tests and warns
>> you that with small numbers of subjects, these are not reliable.
>> Genstat is also experimenting with HGLM by Nelder and Lee (see **)
>>
>> 2.3) "Testing" random effects is considered inappropriate (but see  
>> 2.1
>> for methods?).
>>
>> BMB: I don't think this is necessarily true. Admittedly it is a point
>> null hypothesis (variance will never be _exactly_ zero), but I
>> can certainly see cases ("does variation among species contribute
>> significantly to the overall variance observed"?) where one would
>> want to test this question.  This is a bit murky but I think the
>> distinction is often between random effects as part of an  
>> experimental
>> design (no point in testing, not interesting) and random effects
>> as observational data.
>
> Actually the MLE or the REML estimate of a variance component can
> indeed be zero.  The residual variance (i.e. the variance of the "per
> observation" noise term) is only zero for artificial data but the
> estimates of other variance components can be exactly zero.
>
> I think of such situations as an indication that I should simplify the
> model by eliminating such a term.  I have spent most of my career at
> the University of Wisconsin-Madison in a statistics department founded
> by George Box who famously said "All models are wrong; some models are
> useful."  I don't expect a model to be correct, I am only interested
> in whether the terms in the model are useful in explaining the
> observed data.
>
>> 3) Because of 2, there is the resounding argument that bayesian and  
>> or
>> simulation/bootstrapping tools be used to evalaute fixed effects.
>>
>>  BMB: I don't know about "resounding", but OK.  Probably
>> the best option.
>>
>> Current methods proposed and coded, but in various states of
>> usefulness are:
>>
>> mcmcsamp() and HPDinterval() from lme4 + baayen *.fnc's from  
>> languageR,
>> BUGS and winBugs,
>> RH Baayen's simulation tools (e.g. page 307 method)
>> Andrew Gelman and Jennifer Hill's tools (e.g. sim() method from
>> package arm)
>> Ben Bolker's suggestions in this list for glmm's (thread: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html)
>>
>> 3.1) These all evalaute "simple" tests of whether beta's and  
>> intercept
>> are different than 0, and are linked to the contrasts.  There is no
>> emerging method equivalent to a LRT (but see 2.1 and **Final Note
>> Below).
>>
>> BMB: I think if you want to calculate the parametric bootstrap/
>> null-hypothesis simulation of the change in deviances between
>> nested models, it's actually reasonably straightforward. See
>> examples on glmm.wikidot.com , especially
>> http://glmm.wikidot.com/basic-glmm-simulation
>> [geared toward GLMMs but even easier for LMMs]
>>
>> 4) Andrew Gelman et al also suggest AIC based methods and model
>> averaging for model inference, given constant random effects.  I  
>> think
>> their argument about AIC is that if the "likelihood" is estimated
>> well, relative differences in AIC will be constant, irrespective of
>> any adjustement made to numbers of paramters used in calculating AIC:
>> i.e. as long as the random effects structure stays the same, the
>> relative differences between nested models will not change if the
>> number of paramters is estimated consistently. These methods still do
>> not produce p-values.
>>
>> BMB: and AIC is an asymptotic method anyway, like
>> LRTs ... which means it is likely to have the same problems, but I  
>> don't
>> think anyone has evaluated them.  If you want to use the finite-size
>> corrections (AICc) then you are back in the situation of guessing
>> at residual degrees of freedom ...
>>
>> **Final Note Below - I have noticed a relative lack of discussion of
>> Nelder et al's  H-likelihood and their methods to generate a general
>> method for all heirarchical modelling (HGLM?!).  Would anybody be  
>> able
>> to comment?  A recent paper (http://www.springerlink.com/content/17p17r046lx4053r/fulltext.pdf
>> ) that is somewhat beyond my skills, indicates the use of Laplace
>> methods to estimate likelihoods in heirarchical models and various
>> capacity for model inference.
>>
>>  BMB: I think HGLMs are a very promising way of *estimating*
>> the parameters of GLMMs (I too wonder why they don't seem to
>> be discussed much), but I don't think they get us any farther forward
>> with inference.
>>
>> Thanks again, in advance, to anyone who took this on..... apologies
>> for any glaring errors or assignment of ideas to people incorrectly.
>>
>> Andrew
>>
>> ---------------------------------------------------------------------------------
>> Dr. Andrew Beckerman
>> Department of Animal and Plant Sciences, University of Sheffield,
>> Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
>> ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
>> http://www.beckslab.staff.shef.ac.uk/
>>
>> http://www.flickr.com/photos/apbeckerman/
>> http://www.warblefly.co.uk
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From nikko at hailmail.net  Sat Jan 24 16:24:46 2009
From: nikko at hailmail.net (Nicholas Lewin-Koh)
Date: Sat, 24 Jan 2009 07:24:46 -0800
Subject: [R-sig-ME] (no subject)
In-Reply-To: <20090123235856.GC17419@laplace.reed.edu>
References: <mailman.2022.1232664028.7150.r-sig-mixed-models@r-project.org>
	<1232730086.32255.1296393301@webmail.messagingengine.com>
	<40e66e0b0901231152t701a2abeh36ced8f0cd6ce766@mail.gmail.com>
	<20090123235856.GC17419@laplace.reed.edu>
Message-ID: <1232810686.15324.1296523091@webmail.messagingengine.com>

Hi,
Yes, it does deal with mixture distributions, But in the sense I was
talking
about, random effects being nuisance parameters, one is in in effect
integrating them out
of the likelihood and hence it is a mixture, no? 
As I said, I am learning more than I am contributing to this discussion.

Nicholas
On Fri, 23 Jan 2009 15:58:56 -0800, "Albyn Jones" <jones at reed.edu> said:
> 
> 
> On Fri, Jan 23, 2009 at 01:52:26PM -0600, Douglas Bates wrote:
> 
> > 
> > > Would Bruce Lindsay's work on the geometry of mixtures be applicable in
> > > the mixed model setting?
> > 
> > I can't say because I am not familiar with that work.
> > 
> 
> I don't have it handy, but as I recall, it deals with mixture
> distributions, not the mixed model.
> 
> albyn



From dieter.menne at menne-biomed.de  Mon Jan 26 14:44:03 2009
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 26 Jan 2009 13:44:03 +0000 (UTC)
Subject: [R-sig-ME] mixed model with non-continuous numeric response
References: <a46630750812220354x593cc3f4of35ff5846fe6c90e@mail.gmail.com>
Message-ID: <loom.20090126T134215-270@post.gmane.org>

Daniel Ezra Johnson <danielezrajohnson at ...> writes:

> I have survey results where the response is 1, 2, 3, or 4. These can
> be thought of as equally-spaced points on a scale, I don't have a
> problem with that. (They're actually more like "not at all", "some",
> "mostly", "totally"; the subject is judging a stimulus.)


As a bayesian alternative, you could try DPolmm in DPackage which can be 
use with ordinal data and syntax similar to lme(r)

fit1<-DPolmm(fixed=imps79o~sweek+tx+sweek*tx,random=~1|id,prior=prior,
mcmc=mcmc,state=state,status=TRUE)
fit1

Dieter



From Greg.Snow at imail.org  Mon Jan 26 18:15:15 2009
From: Greg.Snow at imail.org (Greg Snow)
Date: Mon, 26 Jan 2009 10:15:15 -0700
Subject: [R-sig-ME] Teaching Mixed Effects
In-Reply-To: <49790464.4080700@ufl.edu>
References: <49790464.4080700@ufl.edu>
Message-ID: <B37C0A15B8FB3C468B5BC7EBC7DA14CC61C939D22C@LP-EXMBVS10.CO.IHC.COM>

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-bounces at r-project.org] On Behalf Of Ben Bolker
> Sent: Thursday, January 22, 2009 4:42 PM
> To: R Mixed Models
> Subject: Re: [R-sig-ME] Teaching Mixed Effects
> 
> 
>   Wow.
>   Two very small points:
> 
> >
> > I feel that the likelihood ratio is a perfectly reasonable way of
> > comparing two model fits where one is a special case of the other.
> In
> > fact, if the models have been fit by maximum likelihood, the
> > likelihood ratio would, I think, be the first candidate for a test
> > statistic.  The problem with likelihood ratio tests is not the
> > likelihood ratio, per se -- it is converting the likelihood ratio to
> a
> > p-value.  You need to be able to evaluate the distribution of the
> > likelihood ratio under the null hypothesis.  The chi-square
> > approximation to the distribution is exactly that - an approximation
> -
> > and its validity depends on not testing at the boundary and on having
> > a large sample, in some sense of the sample size.  If I were really
> > interested in evaluating a p-value for the likelihood ratio I would
> > probably try a parametric bootstrap to get a reference distribution.
> >
> 
>   Even if we are not p-value obsessed, we would still presumably
> like to be able make some kind of (even informal) inference from
> the difference in fits, perhaps at the level of "model 1 fits
> (much better|a little better|about the same|a little worse|much worse)
> than model 2", or "the range of plausible estimates for this
> parameter is (tiny|small|moderate|large|absurdly large)". To
> do that we need some kind of metric (if we have not yet fled
> to Bayesian or quasi-Bayesian methods) for the range of
> the deviance under some kind of null case -- for example,
> where should we set cutoff levels on the likelihood profile
> to determine confidence regions for parameters? Parametric
> bootstrap makes sense, although it is a little scary to think
> e.g. of doing a power analysis for such a procedure ...
> 

I took this as a bit of a challenge, to look at the idea of finding the preferred model to use for a mixed effects case (I just stuck with the simple either or choice between 2 models, but this could be expanded to your 5 levels above, or other information).  For this I am using the sample dataset, sleepstudy, from the lme4 package.

Let's start with the example from the help page that basically tests if there is a correlation between the random effects for the intercept and the random effects for the slope:

The following function will simulate data of the same structure with given values for the betas and variances:

sleepsimfun1 <- function(b0, b1, Vb0, Vb1, V01, Serror) {
	mydata <- expand.grid( Days=0:9, Subject=1:18 )
	RE <- MASS::mvrnorm(18, c(0,0), matrix( c(Vb0,V01,V01,Vb1), 2) )
	mydata$Reaction <- with(mydata, 
		(b0+RE[Subject,1]) + (b1+RE[Subject,2])*Days + rnorm(180, 0, Serror)
			)

	fit1 <- lmer(Reaction ~ Days + (Days|Subject), mydata)
	fit2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), mydata)
	anova(fit2,fit1)
}

We can use this to simulate data and analysis from the null condition (no correlation):

pb <- winProgressBar(max=1000)
pbinc <- function() setWinProgressBar(pb, getWinProgressBar(pb)+1)

setWinProgressBar(pb,0)
out1 <- replicate(1000, {pbinc(); 
		sleepsimfun1(251, 10.5, 627, 36, 0, sqrt(653))}, FALSE )

p1 <- sapply(out1, function(x) x[2,7])

This gives the results:

> hist( p1 )
> mean( p1 <= 0.05 )
[1] 0.06
> prop.test( sum(p1 <= 0.05), 1000 )

        1-sample proportions test with continuity correction

data:  sum(p1 <= 0.05) out of 1000, null probability 0.5 
X-squared = 772.641, df = 1, p-value < 2.2e-16
alternative hypothesis: true p is not equal to 0.5 
95 percent confidence interval:
 0.04645524 0.07702574 
sample estimates:
   p 
0.06 

The histogram is fairly uniform and the size of the test is pretty close to the designated alpha of 0.05, so the regular use of anova is probably appropriate here.

Lets look at a different model comparison, we want to know if the number of days has any effect.  A reduced model without days at all will be simultaneously testing the slope, the random effects on slope, and the correlation between the random effects (3 fewer parameters).

This function simulates the data and compares the 2 models:

sleepsimfun2 <- function(b0, b1, Vb0, Vb1, V01, Serror) {
	mydata <- expand.grid( Days=0:9, Subject=1:18 )
	RE <- MASS::mvrnorm(18, c(0,0), matrix( c(Vb0,V01,V01,Vb1), 2) )
	mydata$Reaction <- with(mydata, 
		(b0+RE[Subject,1]) + (b1+RE[Subject,2])*Days + rnorm(180, 0, Serror)
			)

	fit1 <- lmer(Reaction ~ Days + (Days|Subject), mydata)
	fit2 <- lmer(Reaction ~ 1 + (1|Subject), mydata)
	anova(fit2,fit1)
}

We can test it under the null hypothesis with the following code:

setWinProgressBar(pb,0)
out2 <- replicate(1000, {pbinc(); 
		sleepsimfun2(298.5, 0, 1278.3, 0, 0, sqrt(1959))}, FALSE )

p2 <- sapply(out2, function(x) x[2,7])

The results are:

> hist( p2 )
> mean( p2 <= 0.05 )
[1] 0.033
> prop.test( sum(p2 <= 0.05), 1000 )

        1-sample proportions test with continuity correction

data:  sum(p2 <= 0.05) out of 1000, null probability 0.5 
X-squared = 870.489, df = 1, p-value < 2.2e-16
alternative hypothesis: true p is not equal to 0.5 
95 percent confidence interval:
 0.02317469 0.04655855 
sample estimates:
    p 
0.033

The histogram shows p-values biased towards 1 rather than uniform and the true size of the test is smaller than the proposed alpha = 0.05.  This could be a good candidate for doing a likelihood comparison based on simulation (parametric bootstrap).

> ll2 <- sapply(out2, function(x) x[2,5])
> hist(ll2)
> quantile( ll2, 0.95 )
    95% 
6.95113 
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> fm3 <- lmer(Reaction ~ 1 + (1|Subject), sleepstudy)
> (ts <- anova(fm1,fm3)[2,5])
[1] 158.5811
> mean( ll2 >= ts )
[1] 0

So 2* log lik difference for the real data is greater than all the 2* log lik differences from the simulation (should probably redo it with exact values instead of my quick rounding off, but the results should be similar), this is roughly equivalent to a p-value of close to 0.

But we may be interested in if this approach really behaves properly, and what would our power be for this type of study but with different values.

This function simulates the above procedure by generating 1000 values simulated under the null hypothesis to get a reference distribution, then simulates the data and tests it by comparing to the refrence distribution.  (this version creates the reference distribution from the hypothesized parameters, an alternative would be to generate the data first, fit the reduced model, then simulate the refrence distribution based on that fit, which is better is another discussion):

sleepsimfun3 <- function(b0, b1, Vb0, Vb1, V01, Serror) {
	setWinProgressBar(pb,0)
	out.null <- replicate(1000, {pbinc(); 
		sleepsimfun2(b0, 0, Vb0, 0, 0, Serror)[2,5]} )

	mydata <- expand.grid( Days=0:9, Subject=1:18 )
	RE <- MASS::mvrnorm(18, c(0,0), matrix( c(Vb0,V01,V01,Vb1), 2) )
	mydata$Reaction <- with(mydata, 
		(b0+RE[Subject,1]) + (b1+RE[Subject,2])*Days + rnorm(180, 0, Serror)
			)

	fit1 <- lmer(Reaction ~ Days + (Days|Subject), mydata)
	fit2 <- lmer(Reaction ~ 1 + (1|Subject), mydata)
	ts <- anova(fit2,fit1)[2,5]
	mean( out.null >= ts, na.rm=TRUE )
}

We can run it under the null hypothesis (arbitrarily chosen values for the non-zero parameters) to test the behavior (note for anyone running the code for themselves, I started the next 2 batches of code running before leaving work Friday afternoon, the first batch ended late Saturday morning and the 2 batch ended sometime after I went to bed on Saturday):

pb2 <- winProgressBar('Progress Bar 2',max=1000)
pbinc2 <- function() setWinProgressBar(pb2, getWinProgressBar(pb2)+1)

# check under the null

setWinProgressBar(pb2, 0)
out3 <- replicate(1000, {pbinc2(); 
		sleepsimfun3(100, 0, 1000, 0, 0, 45)} )

The output:

> hist(out3)
> mean(out3 <= 0.05)
[1] 0.056
> prop.test( sum(out3<=0.05), 1000)

        1-sample proportions test with continuity correction

data:  sum(out3 <= 0.05) out of 1000, null probability 0.5 
X-squared = 786.769, df = 1, p-value < 2.2e-16
alternative hypothesis: true p is not equal to 0.5 
95 percent confidence interval:
 0.04293596 0.07258036 
sample estimates:
    p 
0.056 

The histogram looks close enough to uniform (I know I could do specific tests, additional plots, but I am just going for the eyeball close enough test here) and the size of the test is close enough to the target alpha=0.05

Now let's try a power analysis for a given slope (arbitrarily chosen in this case):

# find power for b1=5, Vb1=0, V01=0

setWinProgressBar(pb2, 0)
out4 <- replicate(1000, {pbinc2(); 
		sleepsimfun3(100, 5, 1000, 0, 0, 45)} )

with results:

> hist(out4)
> mean(out4 <= 0.05)
[1] 0.972
> prop.test( sum(out4<=0.05), 1000)

        1-sample proportions test with continuity correction

data:  sum(out4 <= 0.05) out of 1000, null probability 0.5 
X-squared = 889.249, df = 1, p-value < 2.2e-16
alternative hypothesis: true p is not equal to 0.5 
95 percent confidence interval:
 0.9592453 0.9809686 
sample estimates:
    p 
0.972

So we have over 95% power to detect a true slope of 5 (conditional on the other parameters set).

So while a power study may take a bit of time (parallelization will help), it seems feasible (scary or not depends on the person still).

And to think that a few years ago when I was trying to get the flawed Pentium processor in my computer replaced I got the runaround with each person trying to explain that I did not need it replaced because only large corporations and research scientists who do tens of thousands floating point operations would be affected by the flaw.


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
801.408.8111



From a.beckerman at sheffield.ac.uk  Mon Jan 26 18:32:24 2009
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Mon, 26 Jan 2009 17:32:24 +0000
Subject: [R-sig-ME] Teaching Mixed Effects
In-Reply-To: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk>
References: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk>
Message-ID: <7DCC522F-C56B-40A1-A7F5-0B2A9770D052@sheffield.ac.uk>

Dear all -

I just wanted to say thank you "thus far" to the insightful comments  
and references to various methods and software that are pouring in.  I  
do hope to make some attempt soon to organise the responses, possibly  
via an email to the list containing links to archived messages that  
are tied to specific questions I asked.

So, keep em coming;  I am sure there is more to say!

Best wishes,
Andrew

---------------------------------------------------------------------------------
Dr. Andrew Beckerman
Department of Animal and Plant Sciences, University of Sheffield,
Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
http://www.beckslab.staff.shef.ac.uk/

http://www.flickr.com/photos/apbeckerman/
http://www.warblefly.co.uk
----------------------------------------------------------------------------------




On 20 Jan 2009, at 11:47, Andrew Beckerman wrote:

> Dear R-Mixed people -
>
> I am about to embark on a day of attempting to teach some aspects of  
> mixed models using R to PhD students.  I was wondering if anyone  
> would be willing to indulge in this summary below, developed through  
> reading threads on R-Mixed and R-Help over the past few months, and  
> vet my list of issues/questions/topics (4)  associated with mixed  
> models?
>
> Let me reduce any rising blood pressure by saying that I understand  
> (possibly) and accept why there are no p-values in lmer, and NONE of  
> the comments/questions below are about why lmer does not produce  
> sensible df's and p-values to calculate significance (Phew).
>
> #######################
>
> First, a technical question:
>
> Based on these two threads:
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001456.html
>
> IS mcmcsamp() broken for "complicated" random effects? Is it in good  
> enough shape to teach for "simple" gaussian mixed models, to  
> demonstrate the principle?
>
> #######################
>
> Now, here is what I am possibly going to talk about.....
>
> 0) Rule number 1 is to design experiments well, and aim for  
> orthogonal, well replicated and  balanced designs.  If you get data  
> that conforms to all of that, old school F-ratio's CAN be used.  If  
> not, see 1-4 below (we will assume that Rule number 1 will be broken).
>
> 1) It is agreed that the Laplacian methods for estimating terms and  
> "likelihoods" in mixed effects models is considered most reliable  
> (accurate and precise). R (lme4) and ADMB model builder use these  
> methods. SAS nlmixed does, but SAS proc mixed does not appear to.   
> STATA can.  Genstat does/can (see final note below**).
>
> 2) It is agreed that the appropriate test for fixed effects in mixed  
> models should be between nested models.  However, there is no  
> agreement as how to characterise the distributions that would be  
> used to generate p-values.  This is the crux of the Bates et al  
> argument: Likelihood Ratio Tests, Wald tests etc all need to assume  
> a distribtion and some degrees of freedom.  But, in many mixed  
> models, the distribution need not conform to any of our standard  
> ones (t,F, Chi-square etc), especially when the number of subjects  
> in the random effects is small.  Moreover, the relationship between  
> fixed and random effects means that it is nearly impossible, and  
> perhaps not worthwhile to calcuate what might be appropriate  
> "degrees of freedom".
>
> 2.1) However, Bates et al have mentioned the restricted likelihood  
> ratio test.  There is a package in R implementing some of these  
> tools (RLRsim), but these appear to be limited to and or focused on  
> tests of random effects.
>
> 2.2) What some "other" packages do: SAS can produce wald tests and  
> LRT's if you want, and can implement the kenward-rogers  
> adjustement.  There is some theory behind the K-R, but it is still  
> not dealing with the crux of the problem (see 2).  Genstat uses wald  
> tests and warns you that with small numbers of subjects, these are  
> not reliable. Genstat is also experimenting with HGLM by Nelder and  
> Lee (see **)
>
> 2.3) "Testing" random effects is considered inappropriate (but see  
> 2.1 for methods?).
>
> 3) Because of 2, there is the resounding argument that bayesian and  
> or simulation/bootstrapping tools be used to evalaute fixed  
> effects.  Current methods proposed and coded, but in various states  
> of usefulness are:
>
> mcmcsamp() and HPDinterval() from lme4 + baayen *.fnc's from  
> languageR,
> BUGS and winBugs,
> RH Baayen's simulation tools (e.g. page 307 method)
> Andrew Gelman and Jennifer Hill's tools (e.g. sim() method from  
> package arm)
> Ben Bolker's suggestions in this list for glmm's (thread: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001459.html)
>
> 3.1) These all evalaute "simple" tests of whether beta's and  
> intercept are different than 0, and are linked to the contrasts.   
> There is no emerging method equivalent to a LRT (but see 2.1 and  
> **Final Note Below).
>
> 4) Andrew Gelman et al also suggest AIC based methods and model  
> averaging for model inference, given constant random effects.  I  
> think their argument about AIC is that if the "likelihood" is  
> estimated well, relative differences in AIC will be constant,  
> irrespective of any adjustement made to numbers of paramters used in  
> calculating AIC: i.e. as long as the random effects structure stays  
> the same, the relative differences between nested models will not  
> change if the number of paramters is estimated consistently. These  
> methods still do not produce p-values.
>
> **Final Note Below - I have noticed a relative lack of discussion of  
> Nelder et al's  H-likelihood and their methods to generate a general  
> method for all heirarchical modelling (HGLM?!).  Would anybody be  
> able to comment?  A recent paper (http://www.springerlink.com/content/17p17r046lx4053r/fulltext.pdf 
> ) that is somewhat beyond my skills, indicates the use of Laplace  
> methods to estimate likelihoods in heirarchical models and various  
> capacity for model inference.
>
> Thanks again, in advance, to anyone who took this on..... apologies  
> for any glaring errors or assignment of ideas to people incorrectly.
>
> Andrew
>
> ---------------------------------------------------------------------------------
> Dr. Andrew Beckerman
> Department of Animal and Plant Sciences, University of Sheffield,
> Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
> ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
> http://www.beckslab.staff.shef.ac.uk/
>
> http://www.flickr.com/photos/apbeckerman/
> http://www.warblefly.co.uk
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From steibelj at msu.edu  Mon Jan 26 18:50:55 2009
From: steibelj at msu.edu (Juan Pedro Steibel)
Date: Mon, 26 Jan 2009 12:50:55 -0500
Subject: [R-sig-ME] grouped data or multiple responses
Message-ID: <497DF7FF.3080007@msu.edu>

Hello everyone,
Suppose I want to fit the same mixed model to a set of response 
variables and store the results in a single object.  Then retrieve the 
results one by one using an index or some other referencing method.

The response may be stored either in multiple columns of a response 
matrix (no missing data, same design for all responses) or in a single 
vector, with an index variable indicating the response (allowing for 
different sampling schemes).
Can anyone please help with some pointers on this?
Thanks in advance,
JP

-- 
=============================
Juan Pedro Steibel

Assistant Professor
Statistical Genetics and Genomics

Department of Animal Science & 
Department of Fisheries and Wildlife

Michigan State University
1205-I Anthony Hall
East Lansing, MI
48824 USA 

Phone: 1-517-353-5102
E-mail: steibelj at msu.edu



From bates at stat.wisc.edu  Mon Jan 26 20:02:38 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 26 Jan 2009 13:02:38 -0600
Subject: [R-sig-ME] grouped data or multiple responses
In-Reply-To: <497DF7FF.3080007@msu.edu>
References: <497DF7FF.3080007@msu.edu>
Message-ID: <40e66e0b0901261102p433dac36va47e2aa2257afa34@mail.gmail.com>

On Mon, Jan 26, 2009 at 11:50 AM, Juan Pedro Steibel <steibelj at msu.edu> wrote:
> Hello everyone,
> Suppose I want to fit the same mixed model to a set of response variables
> and store the results in a single object.  Then retrieve the results one by
> one using an index or some other referencing method.

> The response may be stored either in multiple columns of a response matrix
> (no missing data, same design for all responses) or in a single vector, with
> an index variable indicating the response (allowing for different sampling
> schemes).

> Can anyone please help with some pointers on this?
> Thanks in advance,

Fit the first model then use refit with each subsequent data vector.  See

?refit



From Paul.Prew at ecolab.com  Mon Jan 26 21:42:57 2009
From: Paul.Prew at ecolab.com (Prew, Paul)
Date: Mon, 26 Jan 2009 14:42:57 -0600
Subject: [R-sig-ME] Should blocking factors be modeled as random effects?
Message-ID: <6B810AFB14C606439FD57E5985E037910324E3AC@useagan1500p.GLOBAL.ECOLAB.CORP>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090126/03ae768f/attachment.pl>

From steibelj at msu.edu  Mon Jan 26 22:51:17 2009
From: steibelj at msu.edu (Juan Pedro Steibel)
Date: Mon, 26 Jan 2009 16:51:17 -0500
Subject: [R-sig-ME] Should blocking factors be modeled as random effects?
In-Reply-To: <6B810AFB14C606439FD57E5985E037910324E3AC@useagan1500p.GLOBAL.ECOLAB.CORP>
References: <6B810AFB14C606439FD57E5985E037910324E3AC@useagan1500p.GLOBAL.ECOLAB.CORP>
Message-ID: <497E3055.2050406@msu.edu>

Hello,
Treating a block effect as random allows recovering inter-block 
information. In a complete randomized block design (CRBD), treating 
blocks as fixed or random should yield identical results. In an 
incomplete block design (incomplete by design or by missing at random 
some observations), the results will differ.

If the Gaussian assumption regarding the block effects are sound, I 
would expect that treating the block as random will be more efficient 
that fitting block as fixed. Moreover, one could compute the relative 
efficiency of both analyses by comparing the variances of a particular 
treatment difference when block is treated as fixed versus when it is 
treated as a random effect.

The catch is that the relative efficiency depends on the actual variance 
ratios (unknown) and on the assumptions regarding the random effects 
(commonly, Gaussian distribution).

In practice, when analyzing field or lab experiments, I tend to specify 
the block as a random effect. Always.
In some cases there are very few levels, though. In those cases, if 
someone asks "how can you reliably estimate a variance component for a 
(blocking) factor with only (say) 4 or 5 levels?", I just shrug. 8^D

JP



Prew, Paul wrote:
> I have been following your R discussion list on mixed modeling for a few
> weeks, in hopes of understanding mixed modeling better.  And it has
> helped.  I was not aware of the controversy surrounding degrees of
> freedom and the distribution of test statistics.  I have just been
> trusting the ANOVA output from software (Minitab, JMP) that reported F
> tests.  JMP uses Kenward-Roger, Minitab's ANOVA reports an F-statistic,
> followed by "F-test not exact for this term". 
>
> A recent mention by Douglas Bates of George Box, though, hit upon an
> aspect of mixed models that has confused me.  I'm an industrial
> statistician, and studied statistics at Iowa State and the University of
> Minnesota.  I have had 3 courses in DOE, 2 at the graduate level, and
> none of them mentioned blocking factors could (should?) be modeled as
> random effects.  **Exception: the whole plots in a split plot design
> were taught as random effects.**
>
> The 2005 update to Box Hunter Hunter discusses blocking as does Wu &
> Hamada (2000).  Both texts model blocking factors such as Days and
> Batches as fixed effects.  Montgomery's DOE text, 2009 rev., pretty
> consistently states that blocks can be either random or fixed.  Don't
> have a consensus from that small sample. 
>
> I'm trying to understand the implications if I consistently used random
> effects for DOE analysis.
>
> I'm quite willing to use R for mixed models, seeing as Minitab, JMP etc.
> appear to use degrees of freedom calculations that are questionable.
> But as Douglas points out --- Box said, "all models are wrong, some are
> useful" => Box's latest text doesn't bother with random effects for DOE
> =>  does it follow that for practical purposes it's OK to consider
> blocks as fixed?  There are certainly several advantages to keeping it
> simple (i.e. fixed only):
> * The analyses we (my statistics group) provide to our chemists and
> engineers are more easily understood
> * The 2-day short courses we teach in DOE to these same coworkers
> couldn't realistically get across the idea of mixed model analysis ---
> they would become less self-sufficient, where we're trying to make them
> more self-sufficient
> * We have a handful of softwares (Minitab, JMP, Design Expert) that can
> perform DOE and augment the results in a number of ways:
>    *** fold-over the design to resolve aliasing in fractional designs
>    *** add axial runs to enable Response Surface methods
>    *** add distributions to the input factors, enabling
> Robustness/Sensitivity analyses
>    *** running optimization algorithms to suggest the factor settings
> that simultaneously consider multiple objectives
>  *****  Not to mention the loss of Sample Size Calculations, far and
> away my most frequent request
> None of these softwares recognize random factors to perform these
> augmentations
>
> Replacing this functionality with R is going to be a high learning
> curve, and probably not entirely possible.  My coding skills in R
> consist of cutting and pasting what others have done.
>
> I don't really expect that there's a "right" answer to the question of
> random effects in DOE.  But I do believe that beyond the loss of
> p-values, there are other ramifications for advising experimenters,
> '"You can't trust results from your blocking on Days (or Shifts or RM
> Lots or Batches, etc) unless they are modeled as random effects."
>
> There's statistical significance, and practical significance.  My hope
> is that blocks while random effects are statistically "truer", their
> marginal worth over fixed effects in DOE is ignorable. Again, I don't
> want this to come across as shooting the messenger, you are only laying
> out the current state of art and the work that remains to be done.  But
> any insight you can provide into what's practical right now would be
> highly interesting.
>
> Thank you for your time and consideration,
> Paul Prew
>
> 651-795-5942     fax 651-204-7504
> Ecolab Research Center
> Mail Stop ESC-F4412
> Lone Oak Drive
> Eagan, MN 55121-1560
>
>
> CONFIDENTIALITY NOTICE: \ This e-mail communication an...{{dropped:11}}
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>   


-- 
=============================
Juan Pedro Steibel

Assistant Professor
Statistical Genetics and Genomics

Department of Animal Science & 
Department of Fisheries and Wildlife

Michigan State University
1205-I Anthony Hall
East Lansing, MI
48824 USA 

Phone: 1-517-353-5102
E-mail: steibelj at msu.edu



From lawandqueen at hotmail.com  Tue Jan 27 10:23:54 2009
From: lawandqueen at hotmail.com (Lawrence Lee)
Date: Tue, 27 Jan 2009 01:23:54 -0800
Subject: [R-sig-ME] (no subject)
Message-ID: <BAY112-W15838E1FA5A5AA543D987B4CB0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090127/3e1396ae/attachment.pl>

From a.renwick at abdn.ac.uk  Tue Jan 27 10:51:54 2009
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Tue, 27 Jan 2009 09:51:54 +0000
Subject: [R-sig-ME] (no subject)
In-Reply-To: <BAY112-W15838E1FA5A5AA543D987B4CB0@phx.gbl>
References: <BAY112-W15838E1FA5A5AA543D987B4CB0@phx.gbl>
Message-ID: <B9D1301370916C44B5874AF340C18B9B7F911B531F@VMAILB.uoa.abdn.ac.uk>

If I understand your question correctly you need to relevel your variable:
age<-relevel(age,ref="3")
Then rerun the model and age3 should be used as the reference group

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Lawrence Lee
Sent: 27 January 2009 09:24
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] (no subject)


Dear all,

        Does anyone know how to change the reference group within lme().
For example,
lme(fixed = obs ~ line + age - 1,random = ~1 |sire, data = lambweight)

        Suppose age has 3 levels, so the code above will give me estimate of age2 and age3:

Fixed effects: obs ~ line + age - 1           Value Std.Error DF   t-value p-valueline1 10.491153 0.7261881 18 14.446882  0.0000line2 12.290287 0.8242309 18 14.911219  0.0000line3 11.032864 0.7647870 18 14.426061  0.0000line4 10.276735 0.7389137 18 13.907897  0.0000line5 10.952840 0.6084109 18 18.002373  0.0000age2  -0.155435 0.7157030 38 -0.217178  0.8292age3   0.009646 0.5481034 38  0.017599  0.9861

         How I can change within lme() in order to give me the estimate of age1 and age2.


Thanks,
Lawrence
_________________________________________________________________


ore_012009
        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From lawandqueen at hotmail.com  Tue Jan 27 20:21:22 2009
From: lawandqueen at hotmail.com (Lawrence Lee)
Date: Tue, 27 Jan 2009 11:21:22 -0800
Subject: [R-sig-ME] (no subject)
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B7F911B531F@VMAILB.uoa.abdn.ac.uk>
References: <BAY112-W15838E1FA5A5AA543D987B4CB0@phx.gbl>
	<B9D1301370916C44B5874AF340C18B9B7F911B531F@VMAILB.uoa.abdn.ac.uk> 
Message-ID: <BAY112-W3265174D177D88F7EA53BAB4CB0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090127/014ab03c/attachment.pl>

From adik at ilovebacon.org  Tue Jan 27 20:28:58 2009
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Tue, 27 Jan 2009 11:28:58 -0800 (PST)
Subject: [R-sig-ME] (no subject)
Message-ID: <Pine.LNX.4.64.0901271128450.30937@parser.ilovebacon.org>

Hi Lawrence,

 	contrasts(lambweight$age) returns the contrast matrix that is tested
by lm, lme, etc....you can assign to this matrix. You probably want
contrasts(lambweight$age) <- matrix(c(
1,0,0,
0,1,0),nrow=2)

...to do it in lme proper, add
contrasts=list(age=matrix(c(1,0,0,0,1,0),nrow=2))
to your command line.

Note also that you're not getting estimates for age2 and 3, you're getting
estimates for the difference between age2 and age1, and between age3 and
age1.

--Adam

On Tue, 27 Jan 2009, Lawrence Lee wrote:

> 
> Dear all,
>
>        Does anyone know how to change the reference group within lme().
> For example,
> lme(fixed = obs ~ line + age - 1,random = ~1 |sire, data = lambweight)
>
>        Suppose age has 3 levels, so the code above will give me estimate
> of age2 and age3:
> 
> Fixed effects: obs ~ line + age - 1           Value Std.Error DF   t-value 
> p-valueline1 10.491153 0.7261881 18 14.446882  0.0000line2 12.290287 
> 0.8242309 18 14.911219  0.0000line3 11.032864 0.7647870 18 14.426061 
> 0.0000line4 10.276735 0.7389137 18 13.907897  0.0000line5 10.952840 0.6084109 
> 18 18.002373  0.0000age2  -0.155435 0.7157030 38 -0.217178  0.8292age3 
> 0.009646 0.5481034 38  0.017599  0.9861
>
>         How I can change within lme() in order to give me the estimate of
> age1 and age2.
> 
> 
> Thanks,
> Lawrence
> _________________________________________________________________
> 
> 
> ore_012009
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From queirozrafaelmv at yahoo.com.br  Wed Jan 28 12:04:37 2009
From: queirozrafaelmv at yahoo.com.br (Rafael Maia)
Date: Wed, 28 Jan 2009 09:04:37 -0200
Subject: [R-sig-ME] Comparing random slopes?
In-Reply-To: <B37C0A15B8FB3C468B5BC7EBC7DA14CC61C939D22C@LP-EXMBVS10.CO.IHC.COM>
References: <49790464.4080700@ufl.edu>
	<B37C0A15B8FB3C468B5BC7EBC7DA14CC61C939D22C@LP-EXMBVS10.CO.IHC.COM>
Message-ID: <3EEEC825-BD91-42E9-AAC3-96A51EBCAE97@yahoo.com.br>

Dear list members,

I am analyzing a data set comprised of two variables measured over  
time (once a month, over a year, for a total of 12 observations per  
individual). I am interested in modeling the relationships between  
these two variables, but I'd like specifically to compare if  
individual variation over time is associated between them. There is  
great individual variation in slopes for both variables, and I'd like  
to measure the degree of intra-individual association between the  
slopes for the two.

My first intuition was to conduct random slopes models for the  
variables and compare random effects (conditional modes for the  
slope?). However, I recall reading in some slides from Douglas Bates  
that these are not actually "estimates", actually something more  
similar to predictors. I also think I remember someone mentioning on  
this list specifically that these values should not be extracted and  
used as parameters for other analyses.

If this was not a mixed-model "situation", I guess it would be  
reasonable to compare regression slopes through an analysis of  
covariance, for example. Anyone has any suggestion as to how to treat  
this case? Any help would be greatly appreciated.

Many thanks in advance!

Abra?os,
Rafael Maia
---
"A little learning is a dangerous thing; drink deep, or taste not the  
Pierian spring." (A. Pope)
M.Sc., Animal Behavior Lab - Dept. of Zoology
Universidade de Brasilia, Brazil
http://www.unb.br/ib/zoo/rhmacedo/



From D.Gillespie at sheffield.ac.uk  Wed Jan 28 14:31:13 2009
From: D.Gillespie at sheffield.ac.uk (D O S Gillespie)
Date: Wed, 28 Jan 2009 13:31:13 +0000
Subject: [R-sig-ME] glmm AIC/LogLik reliability
Message-ID: <1233149473.49805e212bedd@webmail.shef.ac.uk>

Dear R-Sig-ME -

Lets assume that I am going to use a model averaging AIC based  
approach to evaluate nested glmm's.

I would like to assume that the estimation of AIC and LogLik in the  
glmm's of lmer are consistent enough (precise, if not accurate) to use  
in this framework. I realize that we don't trust anova(m1, m2), mainly  
due to df and tests statistics issues.

I realise that some of you may suggest that this is not the correct  
framework.  If so, can you distinguish arguments about the philosophy  
of AIC model averaging from the practical implementation - i.e. is the  
output consistent enough to use if, even if you don't believe the  
answer.  Perhaps they are too intertwined.

Thanks,

Duncan Gillespie



From frank.dziock at tu-berlin.de  Wed Jan 28 17:33:02 2009
From: frank.dziock at tu-berlin.de (Frank Dziock)
Date: Wed, 28 Jan 2009 17:33:02 +0100
Subject: [R-sig-ME] grasshoppers and mixed models
Message-ID: <498088BE.6060306@tu-berlin.de>

Hi there!

our response variable is the number of grasshoppers (abundance) on ski
slopes/non ski study plots in alpine ecosystems.

We are interested in the fixed effects of skiing, fertilization,
management intensity and other properties of the study plots on
grasshopper abundance.

We have 82 study plots arranged as pairs (one plot on a ski slope, the
other away from the slope), we call this a BLOCK. We studied these plots
in three different study areas. This lead us to believe we have a nested
design with random effect= 1~AREA/BLOCK

data table arranged as follows:

AREA	BLOCK	grasshopper	fertilizer	management	skiing
1	0	8		1		2		0
1	1	4		1		1		1
1	0	16		0		3		0
1	1	3		1		0		1
2	0	4		0		2		0
2	1	5		1		2		1

etc.


The number of grasshoppers recorded is a count variable, so we used
Poisson-distribution.

model1 <- glmmPQL(abundance~fertilizer+management,random=~1| AREA/BLOCK,
family="poisson")

I was wondering, whether a GLMM like this would be appropriate to decide
which predictor variables have distinct effects on grasshopper numbers.
We are especially unsure, whether the random effect has been properly
defined, because one of the predictor variables (skiing) is also our
BLOCK factor.

We have 15 predictor variables in total and in order not to overfit our
model we did the following:

1. Calculate separate models for single predictor variables (linear,
quadratic and logarithmic terms)
2. We included only the most significant terms of each variable in the
initial model
3. Model was simplified by stepwise removing variables according to
their p-values
4. This was done until all remaining varibles had p-values below 0.05

As I followed the former discussions in your group on p-values, you will
probably want to slaughter me for that approach. But I am an absolute
beginner in mixed models and it would be very helpful for me to receive
a hint, how I could decide which variables play a major role in
determining my grasshoppers abundance while taking into account the
nested study design structure.

Comments would be very welcome!!!


Best wishes from cold Berlin,

Frank



-- 

Prof. Dr. Frank Dziock
Animal Ecologist and Head of Department (Juniorprofessor)

Department of Biodiversity Dynamics
Technische Universitaet Berlin
Sekr. AB 1
Rothenburgstr. 12
D - 12165 Berlin
Germany

Tel: 030 ? 314 71368

Secretary Gisela Falk
gisela.falk at tu-berlin.de
Tel: 030 - 314 71350
Fax: 030 ? 314 71355

http://www.biodiv.tu-berlin.de/



From Greg.Snow at imail.org  Wed Jan 28 18:21:10 2009
From: Greg.Snow at imail.org (Greg Snow)
Date: Wed, 28 Jan 2009 10:21:10 -0700
Subject: [R-sig-ME] (no subject)
In-Reply-To: <BAY112-W3265174D177D88F7EA53BAB4CB0@phx.gbl>
References: <BAY112-W15838E1FA5A5AA543D987B4CB0@phx.gbl>
	<B9D1301370916C44B5874AF340C18B9B7F911B531F@VMAILB.uoa.abdn.ac.uk>
	<BAY112-W3265174D177D88F7EA53BAB4CB0@phx.gbl>
Message-ID: <B37C0A15B8FB3C468B5BC7EBC7DA14CC61C939DB8E@LP-EXMBVS10.CO.IHC.COM>

The formatting is a bit garbled, but hopefully this addresses the correct question.

You ask if there is a way to change which level in a factor is the baseline with treatment contrasts (the default) in one line rather than using relevel on the factor before calling mle (is this correct?).

First one issue to be aware of is whether the baseline level is considered to be a property of the data or a property of the analysis (some packages (maybe most or all other than S/R) don't allow for this distinction so people don't consider it, even though it can be quite important).

If the baseline level (or even the full ordering) is a property of the data (or even if the correct set of contrasts to be used is a property of the data), then this information should be stored as part of the meta-data for the factor.  R/S allows for you to set the ordering of the levels in the factor as part of creating/editing the factor itself.  This information will then be used for analysis and plotting of that factor (unless overridden).  We can even set up a set of contrasts to be used for the factor (see ?contrasts) that will override the default (but can also be overridden in a specific analysis).

In my opinion, in most cases the order of the levels is either irrelevant or a property of the data itself and therefore should be set as part of the data, not the analysis (and this saves work by not having to specify the order in every analysis/plot/etc.)

If you want to keep the current order and just override some specifics for one particular analysis, then one option is to use the C function (note uppercase) along with the appropriate contr function.  For example:

> lm( Sepal.Width~Species, data=iris)

Call:
lm(formula = Sepal.Width ~ Species, data = iris)

Coefficients:
      (Intercept)  Speciesversicolor   Speciesvirginica  
            3.428             -0.658             -0.454  

> lm( Sepal.Width~C(Species,contr.treatment,base=2), data=iris)

Call:
lm(formula = Sepal.Width ~ C(Species, contr.treatment, base = 2),     data = iris)

Coefficients:
                           (Intercept)  C(Species, contr.treatment, base = 2)1  
                                 2.770                                   0.658  
C(Species, contr.treatment, base = 2)3  
                                 0.204  
> 3.428             -0.658
[1] 2.77

Note that the intercept in the 2nd case is the mean of Species 2 (as can be seen by combining the values from the 1st case) and that the first slope changed sign but not absolute value since it is still the difference between the first 2 species, just which one is baseline changed.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
801.408.8111


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-bounces at r-project.org] On Behalf Of Lawrence Lee
> Sent: Tuesday, January 27, 2009 12:21 PM
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] (no subject)
> 
> 
> 
> 
> Hi all,       Thanks a lot. Besides, is there a way to do this task
> just in 1 line of code? Thats is, combine  age<-relevel (age,ref =
> "3")lme( ...) into: lme(...) and have the same effect? Lawrence> From:
> a.renwick at abdn.ac.uk> To: lawandqueen at hotmail.com; r-sig-mixed-
> models at r-project.org> Date: Tue, 27 Jan 2009 09:51:54 +0000> Subject:
> RE: [R-sig-ME] (no subject)> > If I understand your question correctly
> you need to relevel your variable:> age<-relevel(age,ref="3")> Then
> rerun the model and age3 should be used as the reference group> > -----
> Original Message-----> From: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Lawrence
> Lee> Sent: 27 January 2009 09:24> To: r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] (no subject)> > > Dear all,> > Does anyone know how
> to change the reference group within lme().> For example,> lme(fixed =
> obs ~ line + age - 1,random = ~1 |sire, data = lambweight)> > Suppose
> age has 3 levels, so the code above will give me estimate of age2 and
> age3:> > Fixed effects: obs ~ line + age - 1 Value Std.Error DF t-value
> p-valueline1 10.491153 0.7261881 18 14.446882 0.0000line2 12.290287
> 0.8242309 18 14.911219 0.0000line3 11.032864 0.7647870 18 14.426061
> 0.0000line4 10.276735 0.7389137 18 13.907897 0.0000line5 10.952840
> 0.6084109 18 18.002373 0.0000age2 -0.155435 0.7157030 38 -0.217178
> 0.8292age3 0.009646 0.5481034 38 0.017599 0.9861> > How I can change
> within lme() in order to give me the estimate of age1 and age2.> > >
> Thanks,> Lawrence>
> _________________________________________________________________> > >
> ore_012009> [[alternative HTML version deleted]]> >
> _______________________________________________> R-sig-mixed-models at r-
> project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-
> mixed-models> > > The University of Aberdeen is a charity registered in
> Scotland, No SC013683.
> 
> Windows Live(tm) Hotmail(r)...more than just e-mail. See how it works.
> _________________________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]



From bolker at ufl.edu  Wed Jan 28 20:11:36 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 28 Jan 2009 14:11:36 -0500
Subject: [R-sig-ME] glmm AIC/LogLik reliability
In-Reply-To: <1233149473.49805e212bedd@webmail.shef.ac.uk>
References: <1233149473.49805e212bedd@webmail.shef.ac.uk>
Message-ID: <4980ADE8.7090009@ufl.edu>

  I would argue that there's very little we *can* trust
in the realm of GLMM inference, with the exception
of randomization/parametric bootstrapping (and possibly
Bayesian) approaches.

   I think AIC is no worse than anything else in this regard,
except that it hasn't been explored as carefully
as some of the alternatives: thus we suspect by analogy
that there are problems similar to those of the LRT,
but we don't know for sure.
Vaida and Blanchard (2005), Greven (2008), and Burnham
and White (2002) are good references.  There are
two basic issues:
  (1) if you choose to include models that differ
in their random effects components, how do you count
"effective" degrees of freedom?
  (2) how big a sample does it take to reach the
"asymptopia" of AIC?  If you're not there, what is
the best strategy for finite-size correction?  If
you use AICc, what should you put in for effective
residual degrees of freedom?

   Ben Bolker


D O S Gillespie wrote:
> Dear R-Sig-ME -
> 
> Lets assume that I am going to use a model averaging AIC based  
> approach to evaluate nested glmm's.
> 
> I would like to assume that the estimation of AIC and LogLik in the  
> glmm's of lmer are consistent enough (precise, if not accurate) to use  
> in this framework. I realize that we don't trust anova(m1, m2), mainly  
> due to df and tests statistics issues.
> 
> I realise that some of you may suggest that this is not the correct  
> framework.  If so, can you distinguish arguments about the philosophy  
> of AIC model averaging from the practical implementation - i.e. is the  
> output consistent enough to use if, even if you don't believe the  
> answer.  Perhaps they are too intertwined.
> 
> Thanks,
> 
> Duncan Gillespie
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From bolker at ufl.edu  Wed Jan 28 20:26:26 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 28 Jan 2009 14:26:26 -0500
Subject: [R-sig-ME] grasshoppers and mixed models
Message-ID: <4980B162.8090501@ufl.edu>


Frank Dziock wrote:
> Hi there!
> 
> our response variable is the number of grasshoppers (abundance) on ski
> slopes/non ski study plots in alpine ecosystems.
> 
> We are interested in the fixed effects of skiing, fertilization,
> management intensity and other properties of the study plots on
> grasshopper abundance.
> 
> We have 82 study plots arranged as pairs (one plot on a ski slope, the
> other away from the slope), we call this a BLOCK. We studied these plots
> in three different study areas. This lead us to believe we have a nested
> design with random effect= 1~AREA/BLOCK
> 
> data table arranged as follows:
> 
> AREA	BLOCK	grasshopper	fertilizer	management	skiing
> 1	0	8		1		2		0
> 1	1	4		1		1		1
> 1	0	16		0		3		0
> 1	1	3		1		0		1
> 2	0	4		0		2		0
> 2	1	5		1		2		1
> 
> etc.
> 
> 
> The number of grasshoppers recorded is a count variable, so we used
> Poisson-distribution.
> 
> model1 <- glmmPQL(abundance~fertilizer+management,random=~1| AREA/BLOCK,
> family="poisson")

  If your means are fairly low (values <5 per sample much of the time)
then PQL is biased -- you should probably bite the bullet and use
(g)lmer in the lme4 package (or glmmML, or glmmADMB ...)

> I was wondering, whether a GLMM like this would be appropriate to decide
> which predictor variables have distinct effects on grasshopper numbers.
> We are especially unsure, whether the random effect has been properly
> defined, because one of the predictor variables (skiing) is also our
> BLOCK factor.

  That should be OK.  "skiing" (which is missing in your model above)
describes the overall effect of skiing on expected grasshoppers, while
BLOCK describes the variation within AREAs.  However, you may have
a technical problem in that you seem to have a single observation per
block -- that means that you can't separate "residual" variation from
"within-AREA" variation.  In principle this might still be OK since
you are using a GLMM where the expected variation is fixed (i.e., it
should be equal to the expected mean, anything extra must be
within-AREA/between-BLOCK variation), but you may run into fitting
troubles.
> 
> We have 15 predictor variables in total and in order not to overfit our
> model we did the following:
> 
> 1. Calculate separate models for single predictor variables (linear,
> quadratic and logarithmic terms)
> 2. We included only the most significant terms of each variable in the
> initial model
> 3. Model was simplified by stepwise removing variables according to
> their p-values
> 4. This was done until all remaining varibles had p-values below 0.05
> 
> As I followed the former discussions in your group on p-values, you will
> probably want to slaughter me for that approach. But I am an absolute
> beginner in mixed models and it would be very helpful for me to receive
> a hint, how I could decide which variables play a major role in
> determining my grasshoppers abundance while taking into account the
> nested study design structure.

   The nested design and the overfitting/model selection problems are
separate issues. I'd recommend Frank Harrell's book on model reduction
strategies.  Your model selection is definitely problematic -- expecting
to sort out 3 response shapes (lin/quad/logarithmic) * 15 variables
from a data set with 82 total samples seems highly problematic.

  Ecological analysis problems that combine multivariate structure
with non-normality and block structure are quite challenging.
Non-normal+block = GLMM, moderately challenging;
Multivariate = ordination techniques (see vegan)
Multivariate+non-normal = ordination techniques with randomization to
   establish confidence bounds/significance
Multivariate+non-normal+blocks = ? (constrained randomization)?

  I haven't yet looked at Zuur et al's books ( http://www.highstat.com/
) but probably should.  Although I don't know if I will agree with them
or not.

   cheers
    Ben Bolker




-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From Virgilio.Gomez at uclm.es  Wed Jan 28 20:52:11 2009
From: Virgilio.Gomez at uclm.es (Virgilio Gomez Rubio)
Date: Wed, 28 Jan 2009 20:52:11 +0100
Subject: [R-sig-ME] glmm AIC/LogLik reliability
In-Reply-To: <4980ADE8.7090009@ufl.edu>
References: <1233149473.49805e212bedd@webmail.shef.ac.uk>
	<4980ADE8.7090009@ufl.edu>
Message-ID: <1233172331.7262.50.camel@Virgilio-Gomez>

Hi,

>    I think AIC is no worse than anything else in this regard,
> except that it hasn't been explored as carefully
> as some of the alternatives: thus we suspect by analogy
> that there are problems similar to those of the LRT,
> but we don't know for sure.
> Vaida and Blanchard (2005), Greven (2008), and Burnham
> and White (2002) are good references.  There are

I would also point to the paper by Spiegelhalter et al. (2002) on the
DIC. It is a 'Bayesian version' of the DIC but the examples and
discussions therein are quite interesting.

> two basic issues:
>   (1) if you choose to include models that differ
> in their random effects components, how do you count
> "effective" degrees of freedom?
>   (2) how big a sample does it take to reach the
> "asymptopia" of AIC?  If you're not there, what is
> the best strategy for finite-size correction?  If
> you use AICc, what should you put in for effective
> residual degrees of freedom?

We are trying to make a comparison of AIC, cAIC (Vaida and Blanchard,
2005) and DIC in this working paper:

http://www.bias-project.org.uk/papers/ComparisonSAE.pdf

I believe it is a bit of an unfinished work but we have computed several
linear (mixed) models in the context of Small Area Estimation and we
display the values of AIC/cAIC/DIC in a table for comparison purposes
together with the penalty terms. The aim is to study up to what point
the AIC, cAIC and DIC are comparable using different structures for the
random effects. Any comments are welcome.

Hope this helps.

Virgilio

P.S: Is there any way of obtaining the design matrix of the random
effects and the matrix of the variance from an lme object. That would
help to compute the cAIC more easily.



> 
>    Ben Bolker
> 
> 
> D O S Gillespie wrote:
> > Dear R-Sig-ME -
> > 
> > Lets assume that I am going to use a model averaging AIC based  
> > approach to evaluate nested glmm's.
> > 
> > I would like to assume that the estimation of AIC and LogLik in the  
> > glmm's of lmer are consistent enough (precise, if not accurate) to use  
> > in this framework. I realize that we don't trust anova(m1, m2), mainly  
> > due to df and tests statistics issues.
> > 
> > I realise that some of you may suggest that this is not the correct  
> > framework.  If so, can you distinguish arguments about the philosophy  
> > of AIC model averaging from the practical implementation - i.e. is the  
> > output consistent enough to use if, even if you don't believe the  
> > answer.  Perhaps they are too intertwined.
> > 
> > Thanks,
> > 
> > Duncan Gillespie
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
>



From a.beckerman at sheffield.ac.uk  Wed Jan 28 21:26:29 2009
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Wed, 28 Jan 2009 20:26:29 +0000
Subject: [R-sig-ME] glmm AIC/LogLik reliability
In-Reply-To: <4980ADE8.7090009@ufl.edu>
References: <1233149473.49805e212bedd@webmail.shef.ac.uk>
	<4980ADE8.7090009@ufl.edu>
Message-ID: <56819F81-C0E5-414A-92F3-098BE5BDF959@sheffield.ac.uk>

Perhaps the question was not clear enough (I helped Duncan try and  
articulate this....)

Lets assume that we maintain random effects structure in all models,  
but we have a large multiple regression problem in the fixed effects  
(say 8 variables potentially affecting reproduction in a  population).

Can we assume that the LogLik calculations work in this instance?

If we can say yes to this, then we can assume that some calculation of  
AIC is possible. The adjustement of the LogLik by # of paramters can  
be manipulated by the researcher, deciding on what df means to him or  
her, etc.  The crux of the questions is not whether inference is  
correct, but whether the bits/mechanics about getting an AIC value for  
a set of nested models with the same random effects are internally  
consistent.

Andrew

On 28 Jan 2009, at 19:11, Ben Bolker wrote:

>  I would argue that there's very little we *can* trust
> in the realm of GLMM inference, with the exception
> of randomization/parametric bootstrapping (and possibly
> Bayesian) approaches.
>
>   I think AIC is no worse than anything else in this regard,
> except that it hasn't been explored as carefully
> as some of the alternatives: thus we suspect by analogy
> that there are problems similar to those of the LRT,
> but we don't know for sure.
> Vaida and Blanchard (2005), Greven (2008), and Burnham
> and White (2002) are good references.  There are
> two basic issues:
>  (1) if you choose to include models that differ
> in their random effects components, how do you count
> "effective" degrees of freedom?
>  (2) how big a sample does it take to reach the
> "asymptopia" of AIC?  If you're not there, what is
> the best strategy for finite-size correction?  If
> you use AICc, what should you put in for effective
> residual degrees of freedom?
>
>   Ben Bolker
>
>
> D O S Gillespie wrote:
>> Dear R-Sig-ME -
>>
>> Lets assume that I am going to use a model averaging AIC based
>> approach to evaluate nested glmm's.
>>
>> I would like to assume that the estimation of AIC and LogLik in the
>> glmm's of lmer are consistent enough (precise, if not accurate) to  
>> use
>> in this framework. I realize that we don't trust anova(m1, m2),  
>> mainly
>> due to df and tests statistics issues.
>>
>> I realise that some of you may suggest that this is not the correct
>> framework.  If so, can you distinguish arguments about the philosophy
>> of AIC model averaging from the practical implementation - i.e. is  
>> the
>> output consistent enough to use if, even if you don't believe the
>> answer.  Perhaps they are too intertwined.
>>
>> Thanks,
>>
>> Duncan Gillespie
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> -- 
> Ben Bolker
> Associate professor, Biology Dep't, Univ. of Florida
> bolker at ufl.edu / www.zoology.ufl.edu/bolker
> GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From ltiana_m at yahoo.com  Thu Jan 29 15:39:04 2009
From: ltiana_m at yahoo.com (Liliana Martinez)
Date: Thu, 29 Jan 2009 06:39:04 -0800 (PST)
Subject: [R-sig-ME] lmer for a binary dependent variable
Message-ID: <625760.99273.qm@web53001.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090129/6b4861f9/attachment.pl>

From ltiana_m at yahoo.com  Thu Jan 29 15:56:58 2009
From: ltiana_m at yahoo.com (Liliana Martinez)
Date: Thu, 29 Jan 2009 06:56:58 -0800 (PST)
Subject: [R-sig-ME] lmer,
	problem with dependent variables with a binomial distribution
Message-ID: <216333.9677.qm@web53001.mail.re2.yahoo.com>

Hi,
?
I am trying to use the lmer function from the lme4 package in R 2.8.0. to fit a generalized mixed-effects model for a dependent variable with a binomial distribution (for more info on my experiment, look below). However, I encounter?some problems due to the nature of my data. 
?
First, some background info without which it is not possible to explain my problem:
I am a linguist trying to analyze the geometry of path encoded by some verbs of motion. In my experiment the subjects drew sketches of the trajectory of a moving car, prompted by stimulus sentences containing certain verbs. The sketches then were categorized according to the extension of the curve drawn by the subject. Thus, my dependent variable was originally intended to be a factor ? curve extension ? with 6 levels (45, 90, 135, 180, 270 and 360 degrees). I preferred a factor instead of a numeric variable, because it was not always possible to assign a precise angular value, and the only solution was to assign the sketch to one of several cognitively motivated categories. When I sought advice for appropriate means of analysis, it became clear that no known (to my circle of contacts) means exist to analyze this type of data. Therefore I had to restructure my original dependent variable into 6 dependent variables with a binomial distribution (occur
 ? no-occur) ? one for each of the curve-extension categories. 
?
The problem:
For some of these binary dependent variables some values of the predictor do not elicit any occurrences, e.g., none of the subjects drew a 360-degree curve when the stimulus contained verb A, and many subjects drew loads of 360-degree curves when the stimulus contained verb B, yet the model does not find any significant difference between verb A and verb B (and there is a tremendously big value for Standard Error. 
?
[13:43:20] Liliana Martinez sier : Fixed effects:
???????????????????? Estimate Std. Error z value Pr(>|z|)??? 
(Intercept)??????????? 1.5476???? 0.4260?? 3.633 0.000280 ***
v_prepobikaliam_ze??? -0.1370???? 0.3691? -0.371 0.710548??? 
v_prepzaobikaliam_ze? -4.9991???? 0.5346? -9.352? < 2e-16 ***
v_prepzavivam_kum??? -19.6208?? 525.3207? -0.037 0.970206??? 
v_prepzavivam_pokrai? -6.6705???? 0.9123? -7.312 2.64e-13 ***
?
Yet, the difference is striking, and is obvious both in the figures (see attachment) and in crosstab statistics. How can I document and report the difference between the predictor values in such cases? If this method of analysis cannot provide a good solution, can you suggest alternative ways of analysis? 
?
?
best regards
?
Liliana


      _________________________________________________________
Alt i ett. F
.mail.yahoo.com

From qizhang at wfubmc.edu  Thu Jan 29 21:09:05 2009
From: qizhang at wfubmc.edu (peter zhang)
Date: Thu, 29 Jan 2009 20:09:05 +0000 (UTC)
Subject: [R-sig-ME] Could lme4 fit a nonlinear mixed effect model using AGQ?
Message-ID: <loom.20090129T200426-711@post.gmane.org>

I wonder whether lmer or nlmer could be similar to NLMIXED in fitting a general 
nonlinear mixed effect model by the ML approach with AGQ. It seem Bin Bai only 
implemented it for some specific models in the summer of 2008. So I wonder 
what's the development at this stage.

Thanks,

Peter Zhang



From bolker at ufl.edu  Thu Jan 29 23:45:31 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 29 Jan 2009 17:45:31 -0500
Subject: [R-sig-ME] glmm AIC/LogLik reliability
In-Reply-To: <56819F81-C0E5-414A-92F3-098BE5BDF959@sheffield.ac.uk>
References: <1233149473.49805e212bedd@webmail.shef.ac.uk>
	<4980ADE8.7090009@ufl.edu>
	<56819F81-C0E5-414A-92F3-098BE5BDF959@sheffield.ac.uk>
Message-ID: <4982318B.5070304@ufl.edu>

Andrew Beckerman wrote:
> Perhaps the question was not clear enough (I helped Duncan try and  
> articulate this....)
> 
> Lets assume that we maintain random effects structure in all models,  
> but we have a large multiple regression problem in the fixed effects  
> (say 8 variables potentially affecting reproduction in a  population).

  Maintaining the random effects structure takes care of the issue
of counting degrees of freedom for random effects, EXCEPT in the
finite-data (AICc or equivalent) case.
> 
> Can we assume that the LogLik calculations work in this instance?

  I would guess that you would get correct log-likelihoods/deviances
in this case, if you use ML rather than REML.  (These will essentially
be marginal deviances, integrated over the random effects.)

> If we can say yes to this, then we can assume that some calculation of  
> AIC is possible. The adjustement of the LogLik by # of paramters can  
> be manipulated by the researcher, deciding on what df means to him or  
> her, etc.  The crux of the questions is not whether inference is  
> correct, but whether the bits/mechanics about getting an AIC value for  
> a set of nested models with the same random effects are internally  
> consistent.

   If you're not worried about inference, then I'd say you're OK.
Likelihood/deviance should correctly rank models with the same degree
of complexity.  But I don't see how you're going to be able to
confidently rank models unless (a) your Ns are so large
that you can assert that you are in "asymptopia" (and N here means
both (?) number of random-effects units and total sample size)
or (b) you can figure out how to inflate penalties based on
"residual df" ...

  As always, I'm happy to be corrected.

  [blatant plug: I have a GLMM paper available online now
<http://dx.doi.org/10.1016/j.tree.2008.10.008> although much of what
it says will be well known to everyone here ...]

  Ben Bolker



From a.beckerman at sheffield.ac.uk  Fri Jan 30 10:29:02 2009
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Fri, 30 Jan 2009 09:29:02 +0000
Subject: [R-sig-ME] MCMCglmm
Message-ID: <2B1E21CC-8AE1-4606-B40E-C4942375DF04@sheffield.ac.uk>

Given Ben's recent plug for his paper on glmm's .... we might also  
want to look at a new package by Jarrod Hadfield called MCMCglmm (link  
via bristol mirror below).  It has a nice vignette as well, covering  
many of the interesting issues associated with specific variance -  
covariance matrices and hypothesis testing embedded in the R-Sig-Mixed  
archive.

http://www.stats.bris.ac.uk/R/web/packages/MCMCglmm/index.html

Andrew
---------------------------------------------------------------------------------
Dr. Andrew Beckerman
Department of Animal and Plant Sciences, University of Sheffield,
Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
http://www.beckslab.staff.shef.ac.uk/

http://www.flickr.com/photos/apbeckerman/
http://www.warblefly.co.uk



From a.beckerman at sheffield.ac.uk  Fri Jan 30 10:29:52 2009
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Fri, 30 Jan 2009 09:29:52 +0000
Subject: [R-sig-ME] lmer for a binary dependent variable
In-Reply-To: <625760.99273.qm@web53001.mail.re2.yahoo.com>
References: <625760.99273.qm@web53001.mail.re2.yahoo.com>
Message-ID: <2DAE8CED-2FDC-4BEA-B638-82BEE9FFC658@sheffield.ac.uk>

Lillian -

There is a lengthy, satisfying, but ultimately frustration discussion  
of this issues starting with this link and the thread therein.....

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q1/001787.html

Look in the archives under the teaching mixed models OR anything to do  
with mcmc sampling and glmm's

You might also look at Ben Bokers recent paper, and a new package  
called MCMCglmm by Jarrod Hadfield.

<http://dx.doi.org/10.1016/j.tree.2008.10.008>

Andrew
---------------------------------------------------------------------------------
Dr. Andrew Beckerman
Department of Animal and Plant Sciences, University of Sheffield,
Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
http://www.beckslab.staff.shef.ac.uk/

http://www.flickr.com/photos/apbeckerman/
http://www.warblefly.co.uk
----------------------------------------------------------------------------------




On 29 Jan 2009, at 14:39, Liliana Martinez wrote:

> Hi,
>
> I am trying to use the lmer function from the lme4 package in R  
> 2.8.0. to fit a generalized mixed-effects model for a dependent  
> variable with a binomial distribution (for more info on my  
> experiment, look below). However, I encounter a major problem: How  
> is it possible to find the general test statistic and see the  
> relative importance of the predictors? The methods which I found  
> described in Baayen (2008). Analyzing Linguistic Data: A Practical  
> Introduction to Statistics Using Ror on the net did not work out.  
> Here is what I got:
>
>> prec0_va2.lmer
> Generalized linear mixed model fit by the Laplace approximation
> Formula: prec_0 ~ (verb + agent)^2 + (1 | subject)
>    Data: risuvane1_binom_tolmer
>    AIC   BIC logLik deviance
>  559.7 590.5 -272.8    545.7
> Random effects:
>  Groups  Name        Variance Std.Dev.
>  subject (Intercept) 1.9975   1.4133
> Number of obs: 600, groups: subject, 30
>
> Fixed effects:
>                          Estimate Std. Error z value Pr(>|z|)
> (Intercept)                3.3120     0.5757   5.753 8.75e-09 ***
> verbzaobikaliam           -4.2031     0.5530  -7.601 2.94e-14 ***
> verbzavivam               -4.2508     0.5113  -8.313  < 2e-16 ***
> agentveh                  -2.7286     0.7219  -3.780 0.000157 ***
> verbzaobikaliam:agentveh   1.0255     0.7440   1.378 0.168058
> verbzavivam:agentveh       1.9629     0.6217   3.158 0.001591 **
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>             (Intr) vrbzbk vrbzvv agntvh vrbzb:
> verbzaobklm -0.644
> verbzavivam -0.697  0.760
> agentveh    -0.797  0.513  0.556
> vrbzbklm:gn  0.479 -0.743 -0.565 -0.491
> vrbzvvm:gnt  0.573 -0.625 -0.823 -0.584  0.620
>
>> pvals.fnc(prec0_va2.lmer)
> Error in pvals.fnc(prec0_va2.lmer) :
> mcmc sampling is not yet implemented for generalized mixed models
>
>> mcmcsamp(prec0_va2.lmer, n=500)
> Error in .local(object, n, verbose, ...) : Update not yet written
>
> Can anyone suggest a solution to this problem?
>
>
> best regards
>
> Liliana
>
>
>      _________________________________________________________
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From njbisaac at googlemail.com  Fri Jan 30 10:56:50 2009
From: njbisaac at googlemail.com (Nick Isaac)
Date: Fri, 30 Jan 2009 09:56:50 +0000
Subject: [R-sig-ME] Comparing random slopes?
In-Reply-To: <3EEEC825-BD91-42E9-AAC3-96A51EBCAE97@yahoo.com.br>
References: <49790464.4080700@ufl.edu>
	<B37C0A15B8FB3C468B5BC7EBC7DA14CC61C939D22C@LP-EXMBVS10.CO.IHC.COM>
	<3EEEC825-BD91-42E9-AAC3-96A51EBCAE97@yahoo.com.br>
Message-ID: <a072ed700901300156g12a2f29k64c3f3471e426315@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090130/038c3541/attachment.pl>

From ken at kjbeath.com.au  Fri Jan 30 11:00:06 2009
From: ken at kjbeath.com.au (Ken Beath)
Date: Fri, 30 Jan 2009 21:00:06 +1100
Subject: [R-sig-ME] lmer,
	problem with dependent variables with a binomial distribution
In-Reply-To: <216333.9677.qm@web53001.mail.re2.yahoo.com>
References: <216333.9677.qm@web53001.mail.re2.yahoo.com>
Message-ID: <A6CE452F-DD3E-4BED-A012-C6C00CB2D54A@kjbeath.com.au>

On 30/01/2009, at 1:56 AM, Liliana Martinez wrote:

> Hi,
>
> I am trying to use the lmer function from the lme4 package in R  
> 2.8.0. to fit a generalized mixed-effects model for a dependent  
> variable with a binomial distribution (for more info on my  
> experiment, look below). However, I encounter some problems due to  
> the nature of my data.
>
> First, some background info without which it is not possible to  
> explain my problem:
> I am a linguist trying to analyze the geometry of path encoded by  
> some verbs of motion. In my experiment the subjects drew sketches of  
> the trajectory of a moving car, prompted by stimulus sentences  
> containing certain verbs. The sketches then were categorized  
> according to the extension of the curve drawn by the subject. Thus,  
> my dependent variable was originally intended to be a factor ? curve  
> extension ? with 6 levels (45, 90, 135, 180, 270 and 360 degrees). I  
> preferred a factor instead of a numeric variable, because it was not  
> always possible to assign a precise angular value, and the only  
> solution was to assign the sketch to one of several cognitively  
> motivated categories. When I sought advice for appropriate means of  
> analysis, it became clear that no known (to my circle of contacts)  
> means exist to analyze this type of data. Therefore I had to  
> restructure my original dependent variable into 6 dependent  
> variables with a binomial distribution (occur
> ? no-occur) ? one for each of the curve-extension categories.
>

Commercial programs like Latent GOLD (probably requiring the Syntax  
Module) and MPlus will probably analyse your model, as they allow  
ordinal outcomes and multilevel modelling. If you have access to stata  
gllamm may also work.

The method you are using looks like it will be difficult to interpret.

>
> The problem:
> For some of these binary dependent variables some values of the  
> predictor do not elicit any occurrences, e.g., none of the subjects  
> drew a 360-degree curve when the stimulus contained verb A, and many  
> subjects drew loads of 360-degree curves when the stimulus contained  
> verb B, yet the model does not find any significant difference  
> between verb A and verb B (and there is a tremendously big value for  
> Standard Error.
>

The large value of the standard error is due to the estimate tending  
towards negative infinity, resulting from perfect relationship between  
the dependent and that level of the covariate.


>
> [13:43:20] Liliana Martinez sier : Fixed effects:
>                      Estimate Std. Error z value Pr(>|z|)
> (Intercept)            1.5476     0.4260   3.633 0.000280 ***
> v_prepobikaliam_ze    -0.1370     0.3691  -0.371 0.710548
> v_prepzaobikaliam_ze  -4.9991     0.5346  -9.352  < 2e-16 ***
> v_prepzavivam_kum    -19.6208   525.3207  -0.037 0.970206
> v_prepzavivam_pokrai  -6.6705     0.9123  -7.312 2.64e-13 ***
>
> Yet, the difference is striking, and is obvious both in the figures  
> (see attachment) and in crosstab statistics. How can I document and  
> report the difference between the predictor values in such cases? If  
> this method of analysis cannot provide a good solution, can you  
> suggest alternative ways of analysis?
>
>
> best regards
>
> Liliana
>
>
>      _________________________________________________________
> Alt i ett. F
> .mail.yahoo.com_______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From ian.dworkin at gmail.com  Fri Jan 30 14:58:05 2009
From: ian.dworkin at gmail.com (Ian Dworkin)
Date: Fri, 30 Jan 2009 08:58:05 -0500
Subject: [R-sig-ME] MCMCglmm
In-Reply-To: <2B1E21CC-8AE1-4606-B40E-C4942375DF04@sheffield.ac.uk>
References: <2B1E21CC-8AE1-4606-B40E-C4942375DF04@sheffield.ac.uk>
Message-ID: <14a74d330901300558j3251d3a0p930936968132af1f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090130/b54160f2/attachment.pl>

From fauna at pngp.it  Fri Jan 30 15:20:23 2009
From: fauna at pngp.it (Achaz von Hardenberg)
Date: Fri, 30 Jan 2009 15:20:23 +0100
Subject: [R-sig-ME] Problems installing matrix and lme4 on R 2.8.1
Message-ID: <F044FBC4-EECC-4E00-89CE-64831D0E78BD@pngp.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090130/981c258a/attachment.pl>

From njbisaac at googlemail.com  Fri Jan 30 17:26:23 2009
From: njbisaac at googlemail.com (Nick Isaac)
Date: Fri, 30 Jan 2009 16:26:23 +0000
Subject: [R-sig-ME] Problems installing matrix and lme4 on R 2.8.1
In-Reply-To: <F044FBC4-EECC-4E00-89CE-64831D0E78BD@pngp.it>
References: <F044FBC4-EECC-4E00-89CE-64831D0E78BD@pngp.it>
Message-ID: <a072ed700901300826w7d5cdc40ia0c676a08f3c5979@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090130/079e0b49/attachment.pl>

From fauna at pngp.it  Fri Jan 30 17:42:54 2009
From: fauna at pngp.it (Achaz von Hardenberg)
Date: Fri, 30 Jan 2009 17:42:54 +0100
Subject: [R-sig-ME] Problems installing matrix and lme4 on R 2.8.1
In-Reply-To: <a072ed700901300826w7d5cdc40ia0c676a08f3c5979@mail.gmail.com>
References: <F044FBC4-EECC-4E00-89CE-64831D0E78BD@pngp.it>
	<a072ed700901300826w7d5cdc40ia0c676a08f3c5979@mail.gmail.com>
Message-ID: <71876117-0D8C-4812-90B2-8D94223D28E9@pngp.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090130/51891d8d/attachment.pl>

From s.ruiter at maw.ru.nl  Fri Jan 30 18:50:39 2009
From: s.ruiter at maw.ru.nl (Stijn Ruiter)
Date: Fri, 30 Jan 2009 18:50:39 +0100
Subject: [R-sig-ME] Jointly testing several parameters of factor variable
Message-ID: <49833DEF.90609@maw.ru.nl>

Hi all,
I am estimating a lmer model and now I would like to test whether some 
coefficients for specific levels of factor variable are equal.
I am thinking of either:
(1) applying equality constraints on several parameters for specific 
levels of a factor variable (and use that to do LRT against model 
without equality constraints), but I have no clue on how to that; or
(2) do a Wald test in which several parameter estimates of a factor 
variable are jointly tested. Although I noticed "wald.test" from the 
"aod" package, but I don't quite follow how to apply that to my lmer model.

Any suggestions on how to jointly test several parameters of a lmer 
model (preferably applied to factor variables)?

Stijn

-- 
Best regards,

Stijn Ruiter
Department of Sociology / ICS
Radboud University Nijmegen
P.O. Box 9104
6500 HE Nijmegen
Netherlands

Phone: + 31 24 361 2272
Fax:   + 31 24 361 2399

Visiting address:
Thomas van Aquinostraat 4.01.71
Nijmegen

website: http://oase.uci.ru.nl/~sruiter



From richard_raubertas at merck.com  Fri Jan 30 19:22:44 2009
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Fri, 30 Jan 2009 13:22:44 -0500
Subject: [R-sig-ME] Teaching Mixed Effects
In-Reply-To: <40e66e0b0901221440lcc6cc9cmedc91fa7714081ed@mail.gmail.com>
References: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk><2180B808CDA3404B8FF30C1EA0AC09EEB4D648@UFEXCH-MBXCL03.ad.ufl.edu>
	<40e66e0b0901221440lcc6cc9cmedc91fa7714081ed@mail.gmail.com>
Message-ID: <D9786C12E3E5534884EA7CBFB251576A0393396F@usctmx1114.merck.com>

The accumulation of quoted posts in this thread is quite long, so I 
have trimmed out all but some paragraphs from Doug Bates that prompt my 
comments.

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Douglas Bates
> Sent: Thursday, January 22, 2009 5:40 PM
> To: Bolker,Benjamin Michael
> Cc: R Models Mixed
> Subject: Re: [R-sig-ME] Teaching Mixed Effects
> 
   [ ... ]
> 
> If we want to perform a hypothesis test related to a fixed-effects
> term in a mixed model (and, for the moment, I will not go into the
> question of whether statistical inferences should always be phrased as
> the result of hypothesis tests) I would claim we should start at the
> beginning, which is considering two models for the data at hand, one
> model being a special case of the other.  We need to decide how we
> measure the quality of the fit of the general model relative to the
> more specific model and how we measure the additional cost of the
> general model.  Then we need to formulate a test statistic.  If we are
> incredibly lucky, the null distribution of this test statistic will be
> well-defined (that is, it will not depend on the values of other,
> unknown parameters) and we can evaluate probabilities associated with
> it.  That does happen in the case of the Gaussian linear model.  I
> personally don't think it will be possible to possible to provide a
> general approach that isolates the effect of a fixed-effect term in a
> linear mixed model using a statistic that does not depend on the
> values of other parameters.  I would be delighted if someone can do it
> but I think there is too much that goes right in the case of the
> Gaussian linear model to expect that the same incredible
> simplifications will apply to other models.
> 
> I don't feel that holy grail of inference in mixed effects models
> should be a magical formula for degrees of freedom to be associated
> with some ratio that looks like a t or an F statistic (despite the
> strongly held beliefs of those in the First Church of the
> Kenward-Roger Approximation). Certainly there has been a lot of
> statistical research related to approximating a difficult distribution
> by a more common distribution but I view this approach as belonging to
> an earlier era.  It is the approach embodied in software like SAS
> whose purpose often seems to be to evaluate difficult formulas and
> provide reams of output including every number that could possibly be
> of interest.  I think we should use the power of current and future
> computers to interact with and explore data and models for the data.
> MCMC is one way to do this.  In nonlinear regression Don and I
> advocated profiling the sum of squares function with respect to the
> values of individual parameters as another way of assessing the actual
> behavior of the model versus trying to formulate an approximation.
> I'm sure that creative people will come up with many other ways to use
> the power of computers to this end.  The point is to explore the
> actual behavior of the model/data combination, not to do just one fit,
> calculate a bunch of summary statistics, apply approximate
> distributions to get p-values and go home.

But a goal of this exploration of the model/data is still to come up 
with an approximation to a sampling distribution (of a test statistic 
or parameter estimate), right?  Ultimately, once all the exploration is 
done, the scientific researcher still wants to be able to tell his/her 
colleagues "Based on these data my conclusion about the effect of 
treatment X is ______ and my confidence in this conclusion is _____."  
That is, the researcher wants to make *inferences* from the data.

> 
> If we want to generalize methods of inference we should consider the
> whole chain of reasoning that leads us to the result rather than
> concentrating only on the last step, which is "now that I have the
> value of a statistic how do I convert it to a p-value?" or, even more
> specifically, "I have calculated something that looks like a t-ratio
> so I am going to assume that its distribution is indeed a
> t-distribution which leaves me with only one question and that is on
> how many degrees of freedom".
> 
> I appreciate that this is inconvenient to those applying such models
> to their data.  Philosophical discussions of the fundamentals of
> statistical inference are all well and good but when the referees on
> your paper say you have to provide a p-value for a particular term or
> tests, it is a practical matter, not an academic, theoretical debate.
> Those with sufficient energy and skill plus a stellar reputation as a
> researcher may be able to convince editors that p-values are not the
> "be all and end all" of data analysis - Reinhold Kleigl has been able
> to do this in some cases - but that is definitely not the path of
> least resistance.  The sad reality is that p-values have taken on a
> role as the coin of the realm in science that is far beyond what any
> statistician would have imagined.  (Apparently the default
> "significance level" of 5%, which is considered in some disciplines to
> be carved in stone, resulted from a casual comment by Fisher to the
> effect that he might regard an outcome that would be expected to occur
> less than, say, 1 time in 20 as "significant".)

This paragraph brings to mind some comments about p-values and
hypothesis 
tests that seem popular on this list.  Among users of lme4 a theme seems
to 
be that "this ignorant editor/referee is insisting that I demonstrate
that 
my discovery of the effect of treatment X is not a false positive.  How
can 
I get around this?"

Every field of science needs to protect its literature from being
overwhelmed 
by false "discoveries".  Since all the professional and economic rewards
go to 
those who make discoveries, there is enormous incentive to claim that
one 
has found an effect or association, and so there needs to be a reality
check.  
The conventional way of judging these claims is via p-values and
hypothesis 
tests.  Granting all of their faults and limitations, what do people
propose 
as a better way?  

Some will probably say we should focus on (interval) estimation of
parameters 
rather than testing.  But this doesn't solve the problem at hand.
Remember 
how this discussion started:  the difficulty of calculating reliable
p-values 
for fixed effects.  Because of the duality between hypothesis tests and
confidence 
intervals, if you can't get reliable p-values, then essentially by
definition 
you can't get reliable confidence intervals either.  So the issue of
testing-
versus-estimation is a red herring with respect to the deeper problem of

inference for fixed effects in mixed models.

Others may try to dodge the problem by claiming to be Bayesians,
calculating 
credible intervals from posterior distributions.  But this won't hold
water 
if they are using lme4 and mcmcsamp.  No honest Bayesian can use an 
"uninformative" prior (if such a thing even exists), or at least not
more 
than once in any area of research:  after analysis of one data set, the 
posterior from that analysis should inform the prior for the next, but
lme4 
has its priors hard-coded.  I think the real rationale for mcmcsamp is
the 
hope that it will produce results with good frequentist properties.  I
am 
not aware that this has been demonstrated for mixed models in the
peer-reviewed 
statistics literature.

> 
> It is unhelpful of me not to provide p-values in the lmer summaries
> but I develop the software out of interest in doing it as well as I
> possibly can and not because someone assigns me a task to compute
> something.  I really don't know of a good way of assigning p-values to
> t-ratios or F-ratios so I don't.  I still report the ratio of the
> estimate divided by it standard error, and even call it a t-ratio,
> because I think it is informative.
> 

Doug, can you elaborate on that last clause?  In what way is the
(absolute) 
ratio |T| informative that the monotonic transformation 2*(1 -
pnorm(abs(T)))
is not?  In other words, if a p-value (based in this case on a standard 
normal) is not reliable for inference, what inferential value does T
have?
Less formally, if T = 2, for example, what exactly do you conclude about

the parameter, and what is your confidence in that conclusion?
 
   [ ... ]
> 
> I feel that the likelihood ratio is a perfectly reasonable way of
> comparing two model fits where one is a special case of the other.  In
> fact, if the models have been fit by maximum likelihood, the
> likelihood ratio would, I think, be the first candidate for a test
> statistic.  The problem with likelihood ratio tests is not the
> likelihood ratio, per se -- it is converting the likelihood ratio to a
> p-value.  You need to be able to evaluate the distribution of the
> likelihood ratio under the null hypothesis.  The chi-square
> approximation to the distribution is exactly that - an approximation -
> and its validity depends on not testing at the boundary and on having
> a large sample, in some sense of the sample size.  If I were really
> interested in evaluating a p-value for the likelihood ratio I would
> probably try a parametric bootstrap to get a reference distribution.
> 

Yes, but the problem is that (as you noted earlier in your post) the
null 
distribution depends on the values of the nuisance parameters, except in

very special cases.  A simple parametric bootstrap conditions on the 
estimated values of those parameters, as if they were known.
Nevertheless, 
a function to do this might be a useful addition to lme4.

Rich Raubertas
Merck & Co.
Notice:  This e-mail message, together with any attachme...{{dropped:12}}



From ken at kjbeath.com.au  Fri Jan 30 19:29:22 2009
From: ken at kjbeath.com.au (Ken Beath)
Date: Sat, 31 Jan 2009 05:29:22 +1100
Subject: [R-sig-ME] MCMCglmm
In-Reply-To: <14a74d330901300558j3251d3a0p930936968132af1f@mail.gmail.com>
References: <2B1E21CC-8AE1-4606-B40E-C4942375DF04@sheffield.ac.uk>
	<14a74d330901300558j3251d3a0p930936968132af1f@mail.gmail.com>
Message-ID: <C6971DDD-BC0F-48BD-97AC-120A4B79C0F3@kjbeath.com.au>

On 31/01/2009, at 12:58 AM, Ian Dworkin wrote:

> It is worth noting that MCMCglmm is not yet available for OS X.
>

It compiles from source, so it is just a delay in the build.

Ken

>



From bolker at ufl.edu  Fri Jan 30 19:55:14 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 30 Jan 2009 13:55:14 -0500
Subject: [R-sig-ME] Teaching Mixed Effects
In-Reply-To: <D9786C12E3E5534884EA7CBFB251576A0393396F@usctmx1114.merck.com>
References: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk><2180B808CDA3404B8FF30C1EA0AC09EEB4D648@UFEXCH-MBXCL03.ad.ufl.edu>	<40e66e0b0901221440lcc6cc9cmedc91fa7714081ed@mail.gmail.com>
	<D9786C12E3E5534884EA7CBFB251576A0393396F@usctmx1114.merck.com>
Message-ID: <49834D12.5080404@ufl.edu>

Raubertas, Richard wrote:

  [much more snippage]
> 
> Some will probably say we should focus on (interval) estimation of
> parameters
> rather than testing.  But this doesn't solve the problem at hand.
> Remember
> how this discussion started:  the difficulty of calculating reliable
> p-values
> for fixed effects.  Because of the duality between hypothesis tests and
> confidence
> intervals, if you can't get reliable p-values, then essentially by
> definition
> you can't get reliable confidence intervals either.  So the issue of
> testing-
> versus-estimation is a red herring with respect to the deeper problem of
> inference for fixed effects in mixed models.

  Hear, hear.
> 
> Others may try to dodge the problem by claiming to be Bayesians,
> calculating
> credible intervals from posterior distributions.  But this won't hold
> water
> if they are using lme4 and mcmcsamp.  No honest Bayesian can use an
> "uninformative" prior (if such a thing even exists), or at least not
> more
> than once in any area of research:  after analysis of one data set, the
> posterior from that analysis should inform the prior for the next, but
> lme4
> has its priors hard-coded.  I think the real rationale for mcmcsamp is
> the
> hope that it will produce results with good frequentist properties.  I
> am
> not aware that this has been demonstrated for mixed models in the
> peer-reviewed
> statistics literature.

   If mcmcsamp worked at the moment I would strongly suggest that we
should be trying to figure out how its conclusions differ from those
based on other priors (e.g. Gelman's half-Cauchy priors for variances).

>> It is unhelpful of me not to provide p-values in the lmer summaries
>> but I develop the software out of interest in doing it as well as I
>> possibly can and not because someone assigns me a task to compute
>> something.  I really don't know of a good way of assigning p-values to
>> t-ratios or F-ratios so I don't.  I still report the ratio of the
>> estimate divided by it standard error, and even call it a t-ratio,
>> because I think it is informative.
> 
> Doug, can you elaborate on that last clause?  In what way is the
> (absolute)
> ratio |T| informative that the monotonic transformation 2*(1 -
> pnorm(abs(T)))
> is not?  In other words, if a p-value (based in this case on a standard
> normal) is not reliable for inference, what inferential value does T
> have?
> Less formally, if T = 2, for example, what exactly do you conclude about
> 
> the parameter, and what is your confidence in that conclusion?

   Perhaps it's just that people are a little less likely to
take it too seriously?

>    [ ... ]
>> I feel that the likelihood ratio is a perfectly reasonable way of
>> comparing two model fits where one is a special case of the other.  In
>> fact, if the models have been fit by maximum likelihood, the
>> likelihood ratio would, I think, be the first candidate for a test
>> statistic.  The problem with likelihood ratio tests is not the
>> likelihood ratio, per se -- it is converting the likelihood ratio to a
>> p-value.  You need to be able to evaluate the distribution of the
>> likelihood ratio under the null hypothesis.  The chi-square
>> approximation to the distribution is exactly that - an approximation -
>> and its validity depends on not testing at the boundary and on having
>> a large sample, in some sense of the sample size.  If I were really
>> interested in evaluating a p-value for the likelihood ratio I would
>> probably try a parametric bootstrap to get a reference distribution.
>>
> 
> Yes, but the problem is that (as you noted earlier in your post) the
> null
> distribution depends on the values of the nuisance parameters, except in
> 
> very special cases.  A simple parametric bootstrap conditions on the
> estimated values of those parameters, as if they were known.
> Nevertheless,
> a function to do this might be a useful addition to lme4.
> 

  Well, this does already exist, more or less, in the form of
the simulate() method for mer objects: see

http://glmm.wikidot.com/basic-glmm-simulation

  cheers
    Ben Bolker



From adik at ilovebacon.org  Fri Jan 30 20:00:01 2009
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Fri, 30 Jan 2009 11:00:01 -0800 (PST)
Subject: [R-sig-ME] Problems installing matrix and lme4 on R 2.8.1
In-Reply-To: <F044FBC4-EECC-4E00-89CE-64831D0E78BD@pngp.it>
References: <F044FBC4-EECC-4E00-89CE-64831D0E78BD@pngp.it>
Message-ID: <Pine.LNX.4.64.0901301058140.7667@parser.ilovebacon.org>

Hi Achaz,

 	Have you compiled software on your Mac before? Inability to find the
"make" command may indicate that your computer is just not set up to compile
software. You can fix this by installing Xtools off of your installation
disks for the computer or downloading it from http://developer.apple.com
...otherwise, sticking to "pre-compiled" binaries is a pretty safe
bet...lots of care goes into ensuring that they work correctly.

--Adam

On Fri, 30 Jan 2009, Achaz von Hardenberg wrote:

> Hi all,
> after updating my copy of R to 2.8.1, I tried to install Matrix and
> lme4  but I get the following error:
>
>  WARNING: ignoring environment value of R_HOME
> * Installing *source* package 'Matrix' ...
> ** libs
> ** arch - i386
> /Library/Frameworks/R.framework/Resources/bin/INSTALL: line 1: make:
> command not found
> ERROR: compilation failed for package 'Matrix'
> ** Removing '/Library/Frameworks/R.framework/Versions/2.8/Resources/
> library/Matrix'
>
> The downloaded packages are in
> 	/private/tmp/RtmpiifIVE/downloaded_packages
>
> Can anybody please help me out?
>
> thanks a lot,
>
> Dr. Achaz von Hardenberg
> ------------------------------------------------------------------------
> --------------------------------
> Centro Studi Fauna Alpina - Alpine Wildlife Research Centre
> Servizio Sanitario e della Ricerca Scientifica
> Parco Nazionale Gran Paradiso, Degioz, 11, 11010-Valsavarenche (Ao),
> Italy
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From benjamin.risk at gmail.com  Fri Jan 30 23:02:11 2009
From: benjamin.risk at gmail.com (Benjamin Risk)
Date: Fri, 30 Jan 2009 17:02:11 -0500
Subject: [R-sig-ME] AICc and lmer
Message-ID: <660a7b9e0901301402t6ce1b6c6jb5935b85df5505ab@mail.gmail.com>

This question is related to the discussion started by Duncan Gillespie
on glmm AIC/LogLik reliability and Ben Bolker's comments. I am trying
to use lmer to rank 22 log-linear models with unbalanced repeated
sampling of bird density at 11 sites, where all models contain
(~1|site) and models differ in their fixed effects (and for the most
part are not nested). I have been using REML and AICc.  From previous
discussions, it appears I should be using ML instead of REML, and that
AICc may be inappropriate.

I have between 6 and 8 parameters and 60 observations in my candidate
set. I am doing two analyses, one examining the effect of habitat
covariates on bird density, the second examining the effect of habitat
covariates on bird richness density (species/hectare). In both
analyses, the AICc's are somewhat different from the AICs, roughly 1-2
units, and ranking with AIC would change the ordering somewhat. At the
end of the analysis, I make inferences about the role of different
water depths and depth diversity on bird density (first analysis) and
on bird richness density (second analysis). Water depth diversity
comes out as more supported than various measures of water depth in
both analyses. For the shorebird density analysis, the delta AICs are
not that large, and I have been playing around using model averaging
and bootstrapping model-averaged estimates. For the richness density
analysis, a model containing depth diversity and a second model
containing depth diversity and a quadratic of depth diversity are
similarly supported, but models containing water depth variables have
delta AICc's>5. I've left out many of the details in the interest of
trying to present the crux of the problem--hopefully this makes sense!
Is it inappropriate to use AICc in these models with fairly small
sample sizes?


Thank you,
Ben Risk
Master's Student
Beissinger Lab, Environmental Science, Policy, and Management at UC Berkeley



From john.maindonald at anu.edu.au  Fri Jan 30 23:14:13 2009
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sat, 31 Jan 2009 09:14:13 +1100
Subject: [R-sig-ME] Should blocking factors be modeled as random effects?
In-Reply-To: <497E3055.2050406@msu.edu>
References: <6B810AFB14C606439FD57E5985E037910324E3AC@useagan1500p.GLOBAL.ECOLAB.CORP>
	<497E3055.2050406@msu.edu>
Message-ID: <88CEC1E0-5387-4271-B73C-F163562E55B6@anu.edu.au>

"In a complete randomized block design (CRBD), treating blocks as  
fixed or random should yield identical results."

It depends what you mean by "results".  SEs of effects, for treatments  
that are estimated "within blocks", will be the same.  The between  
block variance does not contribute to this SE.

Estimates of SEs of treatment means may be very different.  The  
between block variance does contribute to this SE.  This is where it  
does matter if there are very few blocks.  The SE will be estimated  
with very poor accuracy (low df).

Of course, the SEs of effects assume that there is no systematic  
change in treatment effect from one block to another.  Unless there  
are super-blocks (sites?), there is no way to estimate the SE of any  
block-treatment interaction.  Look at the kiwishade data in the DAAG  
package for an example where there might well be differences between  
blocks that are affected by the direction in which the blocks face.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 27/01/2009, at 8:51 AM, Juan Pedro Steibel wrote:

> Hello,
> Treating a block effect as random allows recovering inter-block  
> information. In a complete randomized block design (CRBD), treating  
> blocks as fixed or random should yield identical results. In an  
> incomplete block design (incomplete by design or by missing at  
> random some observations), the results will differ.
>
> If the Gaussian assumption regarding the block effects are sound, I  
> would expect that treating the block as random will be more  
> efficient that fitting block as fixed. Moreover, one could compute  
> the relative efficiency of both analyses by comparing the variances  
> of a particular treatment difference when block is treated as fixed  
> versus when it is treated as a random effect.
>
> The catch is that the relative efficiency depends on the actual  
> variance ratios (unknown) and on the assumptions regarding the  
> random effects (commonly, Gaussian distribution).
>
> In practice, when analyzing field or lab experiments, I tend to  
> specify the block as a random effect. Always.
> In some cases there are very few levels, though. In those cases, if  
> someone asks "how can you reliably estimate a variance component for  
> a (blocking) factor with only (say) 4 or 5 levels?", I just shrug. 8^D
>
> JP
>
>
>
> Prew, Paul wrote:
>> I have been following your R discussion list on mixed modeling for  
>> a few
>> weeks, in hopes of understanding mixed modeling better.  And it has
>> helped.  I was not aware of the controversy surrounding degrees of
>> freedom and the distribution of test statistics.  I have just been
>> trusting the ANOVA output from software (Minitab, JMP) that  
>> reported F
>> tests.  JMP uses Kenward-Roger, Minitab's ANOVA reports an F- 
>> statistic,
>> followed by "F-test not exact for this term".
>> A recent mention by Douglas Bates of George Box, though, hit upon an
>> aspect of mixed models that has confused me.  I'm an industrial
>> statistician, and studied statistics at Iowa State and the  
>> University of
>> Minnesota.  I have had 3 courses in DOE, 2 at the graduate level, and
>> none of them mentioned blocking factors could (should?) be modeled as
>> random effects.  **Exception: the whole plots in a split plot design
>> were taught as random effects.**
>>
>> The 2005 update to Box Hunter Hunter discusses blocking as does Wu &
>> Hamada (2000).  Both texts model blocking factors such as Days and
>> Batches as fixed effects.  Montgomery's DOE text, 2009 rev., pretty
>> consistently states that blocks can be either random or fixed.  Don't
>> have a consensus from that small sample.
>> I'm trying to understand the implications if I consistently used  
>> random
>> effects for DOE analysis.
>>
>> I'm quite willing to use R for mixed models, seeing as Minitab, JMP  
>> etc.
>> appear to use degrees of freedom calculations that are questionable.
>> But as Douglas points out --- Box said, "all models are wrong, some  
>> are
>> useful" => Box's latest text doesn't bother with random effects for  
>> DOE
>> =>  does it follow that for practical purposes it's OK to consider
>> blocks as fixed?  There are certainly several advantages to keeping  
>> it
>> simple (i.e. fixed only):
>> * The analyses we (my statistics group) provide to our chemists and
>> engineers are more easily understood
>> * The 2-day short courses we teach in DOE to these same coworkers
>> couldn't realistically get across the idea of mixed model analysis  
>> ---
>> they would become less self-sufficient, where we're trying to make  
>> them
>> more self-sufficient
>> * We have a handful of softwares (Minitab, JMP, Design Expert) that  
>> can
>> perform DOE and augment the results in a number of ways:
>> *** fold-over the design to resolve aliasing in fractional designs
>> *** add axial runs to enable Response Surface methods
>> *** add distributions to the input factors, enabling
>> Robustness/Sensitivity analyses
>> *** running optimization algorithms to suggest the factor settings
>> that simultaneously consider multiple objectives
>> *****  Not to mention the loss of Sample Size Calculations, far and
>> away my most frequent request
>> None of these softwares recognize random factors to perform these
>> augmentations
>>
>> Replacing this functionality with R is going to be a high learning
>> curve, and probably not entirely possible.  My coding skills in R
>> consist of cutting and pasting what others have done.
>>
>> I don't really expect that there's a "right" answer to the question  
>> of
>> random effects in DOE.  But I do believe that beyond the loss of
>> p-values, there are other ramifications for advising experimenters,
>> '"You can't trust results from your blocking on Days (or Shifts or RM
>> Lots or Batches, etc) unless they are modeled as random effects."
>>
>> There's statistical significance, and practical significance.  My  
>> hope
>> is that blocks while random effects are statistically "truer", their
>> marginal worth over fixed effects in DOE is ignorable. Again, I don't
>> want this to come across as shooting the messenger, you are only  
>> laying
>> out the current state of art and the work that remains to be done.   
>> But
>> any insight you can provide into what's practical right now would be
>> highly interesting.
>>
>> Thank you for your time and consideration,
>> Paul Prew
>>
>> 651-795-5942     fax 651-204-7504
>> Ecolab Research Center
>> Mail Stop ESC-F4412
>> Lone Oak Drive
>> Eagan, MN 55121-1560
>>
>>
>> CONFIDENTIALITY NOTICE: \ This e-mail communication an...{{dropped: 
>> 11}}
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>
>
> -- 
> =============================
> Juan Pedro Steibel
>
> Assistant Professor
> Statistical Genetics and Genomics
>
> Department of Animal Science & Department of Fisheries and Wildlife
>
> Michigan State University
> 1205-I Anthony Hall
> East Lansing, MI
> 48824 USA
> Phone: 1-517-353-5102
> E-mail: steibelj at msu.edu
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From maj at stats.waikato.ac.nz  Sat Jan 31 00:44:40 2009
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Sat, 31 Jan 2009 12:44:40 +1300
Subject: [R-sig-ME] Plotting Grouped Data
Message-ID: <498390E8.2030208@stats.waikato.ac.nz>

plot() works nicely with the data sets that come with nlme, so 
presumably there is a plot.something() that is well set up to plot data 
with grouping factors.

Now how can I get my data sets into "something" form so that I may use 
plot.something() or, alternatively, what is "something" so that I may 
hack plot.something() to work with my data as it is?

Cheers,  Murray Jorgensen



From steibelj at msu.edu  Sat Jan 31 01:38:33 2009
From: steibelj at msu.edu (Juan Pedro Steibel)
Date: Fri, 30 Jan 2009 19:38:33 -0500
Subject: [R-sig-ME] Should blocking factors be modeled as random effects?
In-Reply-To: <88CEC1E0-5387-4271-B73C-F163562E55B6@anu.edu.au>
References: <6B810AFB14C606439FD57E5985E037910324E3AC@useagan1500p.GLOBAL.ECOLAB.CORP>	<497E3055.2050406@msu.edu>
	<88CEC1E0-5387-4271-B73C-F163562E55B6@anu.edu.au>
Message-ID: <49839D89.7060601@msu.edu>

Thanks for the comment John,
I should have written that better. I had in mind a very simple CRBD with 
one grouping factor (treatment) and complete blocks with only one plot 
per treatment. You are perfectly right, when there are between and 
within block (or plot) treatments (example: split-plot, strip-plot, 
split-block designs), the way to go is to consider the blocks and plots 
as random effects.

I meant to say that in treating the block as random produced the same 
inferences (SE and all) only in (very) simple design, while in more 
complex designs, the random block effect leads to better inferences. 
That is the reason I treat block as random by default.
Thanks again.
JP

John Maindonald wrote:
> "In a complete randomized block design (CRBD), treating blocks as 
> fixed or random should yield identical results."
>
> It depends what you mean by "results".  SEs of effects, for treatments 
> that are estimated "within blocks", will be the same.  The between 
> block variance does not contribute to this SE.
>
> Estimates of SEs of treatment means may be very different.  The 
> between block variance does contribute to this SE.  This is where it 
> does matter if there are very few blocks.  The SE will be estimated 
> with very poor accuracy (low df).
>
> Of course, the SEs of effects assume that there is no systematic 
> change in treatment effect from one block to another.  Unless there 
> are super-blocks (sites?), there is no way to estimate the SE of any 
> block-treatment interaction.  Look at the kiwishade data in the DAAG 
> package for an example where there might well be differences between 
> blocks that are affected by the direction in which the blocks face.
>
> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Mathematics & Its Applications, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
>
>
> On 27/01/2009, at 8:51 AM, Juan Pedro Steibel wrote:
>
>> Hello,
>> Treating a block effect as random allows recovering inter-block 
>> information. In a complete randomized block design (CRBD), treating 
>> blocks as fixed or random should yield identical results. In an 
>> incomplete block design (incomplete by design or by missing at random 
>> some observations), the results will differ.
>>
>> If the Gaussian assumption regarding the block effects are sound, I 
>> would expect that treating the block as random will be more efficient 
>> that fitting block as fixed. Moreover, one could compute the relative 
>> efficiency of both analyses by comparing the variances of a 
>> particular treatment difference when block is treated as fixed versus 
>> when it is treated as a random effect.
>>
>> The catch is that the relative efficiency depends on the actual 
>> variance ratios (unknown) and on the assumptions regarding the random 
>> effects (commonly, Gaussian distribution).
>>
>> In practice, when analyzing field or lab experiments, I tend to 
>> specify the block as a random effect. Always.
>> In some cases there are very few levels, though. In those cases, if 
>> someone asks "how can you reliably estimate a variance component for 
>> a (blocking) factor with only (say) 4 or 5 levels?", I just shrug. 8^D
>>
>> JP
>>
>>
>>
>> Prew, Paul wrote:
>>> I have been following your R discussion list on mixed modeling for a 
>>> few
>>> weeks, in hopes of understanding mixed modeling better.  And it has
>>> helped.  I was not aware of the controversy surrounding degrees of
>>> freedom and the distribution of test statistics.  I have just been
>>> trusting the ANOVA output from software (Minitab, JMP) that reported F
>>> tests.  JMP uses Kenward-Roger, Minitab's ANOVA reports an F-statistic,
>>> followed by "F-test not exact for this term".
>>> A recent mention by Douglas Bates of George Box, though, hit upon an
>>> aspect of mixed models that has confused me.  I'm an industrial
>>> statistician, and studied statistics at Iowa State and the 
>>> University of
>>> Minnesota.  I have had 3 courses in DOE, 2 at the graduate level, and
>>> none of them mentioned blocking factors could (should?) be modeled as
>>> random effects.  **Exception: the whole plots in a split plot design
>>> were taught as random effects.**
>>>
>>> The 2005 update to Box Hunter Hunter discusses blocking as does Wu &
>>> Hamada (2000).  Both texts model blocking factors such as Days and
>>> Batches as fixed effects.  Montgomery's DOE text, 2009 rev., pretty
>>> consistently states that blocks can be either random or fixed.  Don't
>>> have a consensus from that small sample.
>>> I'm trying to understand the implications if I consistently used random
>>> effects for DOE analysis.
>>>
>>> I'm quite willing to use R for mixed models, seeing as Minitab, JMP 
>>> etc.
>>> appear to use degrees of freedom calculations that are questionable.
>>> But as Douglas points out --- Box said, "all models are wrong, some are
>>> useful" => Box's latest text doesn't bother with random effects for DOE
>>> =>  does it follow that for practical purposes it's OK to consider
>>> blocks as fixed?  There are certainly several advantages to keeping it
>>> simple (i.e. fixed only):
>>> * The analyses we (my statistics group) provide to our chemists and
>>> engineers are more easily understood
>>> * The 2-day short courses we teach in DOE to these same coworkers
>>> couldn't realistically get across the idea of mixed model analysis ---
>>> they would become less self-sufficient, where we're trying to make them
>>> more self-sufficient
>>> * We have a handful of softwares (Minitab, JMP, Design Expert) that can
>>> perform DOE and augment the results in a number of ways:
>>> *** fold-over the design to resolve aliasing in fractional designs
>>> *** add axial runs to enable Response Surface methods
>>> *** add distributions to the input factors, enabling
>>> Robustness/Sensitivity analyses
>>> *** running optimization algorithms to suggest the factor settings
>>> that simultaneously consider multiple objectives
>>> *****  Not to mention the loss of Sample Size Calculations, far and
>>> away my most frequent request
>>> None of these softwares recognize random factors to perform these
>>> augmentations
>>>
>>> Replacing this functionality with R is going to be a high learning
>>> curve, and probably not entirely possible.  My coding skills in R
>>> consist of cutting and pasting what others have done.
>>>
>>> I don't really expect that there's a "right" answer to the question of
>>> random effects in DOE.  But I do believe that beyond the loss of
>>> p-values, there are other ramifications for advising experimenters,
>>> '"You can't trust results from your blocking on Days (or Shifts or RM
>>> Lots or Batches, etc) unless they are modeled as random effects."
>>>
>>> There's statistical significance, and practical significance.  My hope
>>> is that blocks while random effects are statistically "truer", their
>>> marginal worth over fixed effects in DOE is ignorable. Again, I don't
>>> want this to come across as shooting the messenger, you are only laying
>>> out the current state of art and the work that remains to be done.  But
>>> any insight you can provide into what's practical right now would be
>>> highly interesting.
>>>
>>> Thank you for your time and consideration,
>>> Paul Prew
>>>
>>> 651-795-5942     fax 651-204-7504
>>> Ecolab Research Center
>>> Mail Stop ESC-F4412
>>> Lone Oak Drive
>>> Eagan, MN 55121-1560
>>>
>>>
>>> CONFIDENTIALITY NOTICE: \ This e-mail communication an...{{dropped:11}}
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>>
>>
>>
>> -- 
>> =============================
>> Juan Pedro Steibel
>>
>> Assistant Professor
>> Statistical Genetics and Genomics
>>
>> Department of Animal Science & Department of Fisheries and Wildlife
>>
>> Michigan State University
>> 1205-I Anthony Hall
>> East Lansing, MI
>> 48824 USA
>> Phone: 1-517-353-5102
>> E-mail: steibelj at msu.edu
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


-- 
=============================
Juan Pedro Steibel

Assistant Professor
Statistical Genetics and Genomics

Department of Animal Science & 
Department of Fisheries and Wildlife

Michigan State University
1205-I Anthony Hall
East Lansing, MI
48824 USA 

Phone: 1-517-353-5102
E-mail: steibelj at msu.edu



From maj at stats.waikato.ac.nz  Sat Jan 31 05:02:09 2009
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Sat, 31 Jan 2009 17:02:09 +1300
Subject: [R-sig-ME] Plotting Grouped Data
In-Reply-To: <498390E8.2030208@stats.waikato.ac.nz>
References: <498390E8.2030208@stats.waikato.ac.nz>
Message-ID: <4983CD41.2090509@stats.waikato.ac.nz>

Oh, I think that I see some of the picture now!

"something" = "groupedData" !

Murray

Murray Jorgensen wrote:
> plot() works nicely with the data sets that come with nlme, so 
> presumably there is a plot.something() that is well set up to plot data 
> with grouping factors.
> 
> Now how can I get my data sets into "something" form so that I may use 
> plot.something() or, alternatively, what is "something" so that I may 
> hack plot.something() to work with my data as it is?
> 
> Cheers,  Murray Jorgensen
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From s.ruiter at maw.ru.nl  Sat Jan 31 12:30:46 2009
From: s.ruiter at maw.ru.nl (Stijn Ruiter)
Date: Sat, 31 Jan 2009 12:30:46 +0100
Subject: [R-sig-ME] Jointly testing several parameters of factor variable
Message-ID: <49843666.9070705@maw.ru.nl>

Hi all,
I am estimating a lmer model and now I would like to test whether some
coefficients for specific levels of factor variable are equal.
I am thinking of either:
(1) applying equality constraints on several parameters for specific
levels of a factor variable (and use that to do LRT against model
without equality constraints), but I have no clue on how to that; or
(2) do a Wald test in which several parameter estimates of a factor
variable are jointly tested. Although I noticed "wald.test" from the
"aod" package, but I don't quite follow how to apply that to my lmer model.

Any suggestions on how to jointly test several parameters of a lmer
model (preferably applied to factor variables)?

Stijn

-- 
Best regards,

Stijn Ruiter
Department of Sociology / ICS
Radboud University Nijmegen
P.O. Box 9104
6500 HE Nijmegen
Netherlands

Phone: + 31 24 361 2272
Fax:   + 31 24 361 2399

Visiting address:
Thomas van Aquinostraat 4.01.71
Nijmegen

website: http://oase.uci.ru.nl/~sruiter



From bates at stat.wisc.edu  Sat Jan 31 17:20:29 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 31 Jan 2009 10:20:29 -0600
Subject: [R-sig-ME] Should blocking factors be modeled as random effects?
In-Reply-To: <6B810AFB14C606439FD57E5985E037910324E3AC@useagan1500p.GLOBAL.ECOLAB.CORP>
References: <6B810AFB14C606439FD57E5985E037910324E3AC@useagan1500p.GLOBAL.ECOLAB.CORP>
Message-ID: <40e66e0b0901310820n737f27d0nafbbf3b7e299634e@mail.gmail.com>

On Mon, Jan 26, 2009 at 2:42 PM, Prew, Paul <Paul.Prew at ecolab.com> wrote:
> I have been following your R discussion list on mixed modeling for a few
> weeks, in hopes of understanding mixed modeling better.  And it has
> helped.  I was not aware of the controversy surrounding degrees of
> freedom and the distribution of test statistics.  I have just been
> trusting the ANOVA output from software (Minitab, JMP) that reported F
> tests.  JMP uses Kenward-Roger, Minitab's ANOVA reports an F-statistic,
> followed by "F-test not exact for this term".
>
> A recent mention by Douglas Bates of George Box, though, hit upon an
> aspect of mixed models that has confused me.  I'm an industrial
> statistician, and studied statistics at Iowa State and the University of
> Minnesota.  I have had 3 courses in DOE, 2 at the graduate level, and
> none of them mentioned blocking factors could (should?) be modeled as
> random effects.  **Exception: the whole plots in a split plot design
> were taught as random effects.**
>
> The 2005 update to Box Hunter Hunter discusses blocking as does Wu &
> Hamada (2000).  Both texts model blocking factors such as Days and
> Batches as fixed effects.  Montgomery's DOE text, 2009 rev., pretty
> consistently states that blocks can be either random or fixed.  Don't
> have a consensus from that small sample.
>
> I'm trying to understand the implications if I consistently used random
> effects for DOE analysis.
>
> I'm quite willing to use R for mixed models, seeing as Minitab, JMP etc.
> appear to use degrees of freedom calculations that are questionable.
> But as Douglas points out --- Box said, "all models are wrong, some are
> useful" => Box's latest text doesn't bother with random effects for DOE
> =>  does it follow that for practical purposes it's OK to consider
> blocks as fixed?  There are certainly several advantages to keeping it
> simple (i.e. fixed only):
> * The analyses we (my statistics group) provide to our chemists and
> engineers are more easily understood
> * The 2-day short courses we teach in DOE to these same coworkers
> couldn't realistically get across the idea of mixed model analysis ---
> they would become less self-sufficient, where we're trying to make them
> more self-sufficient
> * We have a handful of softwares (Minitab, JMP, Design Expert) that can
> perform DOE and augment the results in a number of ways:
>   *** fold-over the design to resolve aliasing in fractional designs
>   *** add axial runs to enable Response Surface methods
>   *** add distributions to the input factors, enabling
> Robustness/Sensitivity analyses
>   *** running optimization algorithms to suggest the factor settings
> that simultaneously consider multiple objectives
>  *****  Not to mention the loss of Sample Size Calculations, far and
> away my most frequent request
> None of these softwares recognize random factors to perform these
> augmentations
>
> Replacing this functionality with R is going to be a high learning
> curve, and probably not entirely possible.  My coding skills in R
> consist of cutting and pasting what others have done.
>
> I don't really expect that there's a "right" answer to the question of
> random effects in DOE.  But I do believe that beyond the loss of
> p-values, there are other ramifications for advising experimenters,
> '"You can't trust results from your blocking on Days (or Shifts or RM
> Lots or Batches, etc) unless they are modeled as random effects."
>
> There's statistical significance, and practical significance.  My hope
> is that blocks while random effects are statistically "truer", their
> marginal worth over fixed effects in DOE is ignorable. Again, I don't
> want this to come across as shooting the messenger, you are only laying
> out the current state of art and the work that remains to be done.  But
> any insight you can provide into what's practical right now would be
> highly interesting.

Thanks for bringing up the topic, Paul.  As you and I know, you
originally sent your question to me and I encouraged you to send it to
this list.

As I wrote in my initial response to you,  "My off-the-cuff reaction
is that in these situations the effects of blocking factors are
regarded as nuisance parameters whereas in many mixed-model situations
the variances and sometimes the values of the random effects are
themselves of interest.  When the effects are
nuisance parameters the simplest approach is to model them as fixed effects."

On thinking about it more, I can imagine several different approaches
to this question.  If you just ask, "Are the levels of this blocking
factor a fixed set of levels or a random selection from a population
of possible levels?" then in most cases I imagine you would say they
are a random selection and should be modeled using random effects.
This would especially be true of what Taguchi called "environmental
factors" which, by definition, are not under the control of the
experimenter.

If you say that blocking factors are not of interest per se and that
your purpose is simply to control for them, it is simpler to model
them as fixed effects.  There are two aspects to "simpler":
computationally simpler and conceptually simpler.  Of these I think
that conceptually is more important.  The computational burden for
fitting a mixed model versus a fixed-effects model is really a
software problem, not a hardware problem.  Commercial statistical
software like Minitab or JMP with a simple, convenient interface has
limited flexibility, in part because it is designed to have a simple,
convenient interface - the "what you see is all you get" problem.  (I
googled that phrase and got a laugh from the article at
www.computer-dictionary-online.org which referred to "point-and-drool
interfaces".)  The actual calculations involved in fitting mixed
models are not that formidable but designing the interface can be.
(One of the underappreciated aspects of the model-fitting software in
R, and in the S language in general, is the structure of the
model.frame, model.matrix sequence for transforming a formula into a
numerical representation.  This makes designing an interface much.
much easier as long as you count on the user to input a formula.)

Conceptually fixed-effects models are simpler than mixed models but
they may over-simplify the analysis.  If your purpose is estimation of
fixed-effects parameters, including assessing precision of the
estimates, then you need to ask if you want to estimate those
parameters conditional the particular levels of the blocking factor
that you observed or with respect to the possible values of the
blocking factors that are represented by the sample you observed.  If
you are willing to condition on the particular levels you observed
then use fixed-effects for the blocking factor.  For all possible
levels of the blocking factor you could use random effects.  For a
designed experiment the estimates of the fixed effects will probably
not be affected much by using random effects for the blocking factor
instead of fixed effects.  However the precision of the estimates may
be different.  Perhaps more importantly, the precision of predictions
of future responses would be different.  I'm not even sure how one
would even formulate such a prediction from a model with fixed effects
for the blocking factor if the factor was something like "batch" and
the batches from the experiment were already used up.

Having said that the precision of the estimates of the fixed-effects
parameters would be different if random effects are used for the
blocking factor I should admit that this is exactly the problem to
which I don't have a good general solution.

It appears that the question of fixed or random for a blocking factor
is like many others in statistics - the choice of the model depends on
what you want to do with it.



From orzack at freshpond.org  Sat Jan 31 21:15:49 2009
From: orzack at freshpond.org (orzack)
Date: Sat, 31 Jan 2009 15:15:49 -0500
Subject: [R-sig-ME] calculation of AIC
Message-ID: <p06230904c5aa4ebd6003@[192.168.1.104]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090131/ad1a4bf4/attachment.pl>

From bolker at ufl.edu  Sun Feb  1 05:30:16 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Sat, 31 Jan 2009 23:30:16 -0500
Subject: [R-sig-ME] calculation of AIC
Message-ID: <49852558.7090309@ufl.edu>



  I believe the issue is in the output of glm, not the
calculation.  Take a look at print.glm and consider the following:

> format(signif(83245.1,4))
[1] "83250"
> format(signif(83249.4,4))
[1] "83250"


orzack wrote:
> I am puzzled by the output of AIC values for glm (yes, this is not 
> strictly a mixed model question, except as a special case) but I ask 
> it here anyway. My apologies in advance if this has been raised 
> before and resolved.
> 
> I fit a binomial GLM with a constant:
> 
>>  conGLM
> 
> Call:  glm(formula = Sex ~ 1, family = binomial, data = CVS_GG.df, 
> subset = CVS_GG_GA.NE.NA_sel)
> 
> Coefficients:
> (Intercept) 
>      0.05324 
> 
> Degrees of Freedom: 60080 Total (i.e. Null);  60080 Residual
> Null Deviance:	    83250
> Residual Deviance: 83250	AIC: 83250
> 
> Note the AIC value.
> 
> I next fit a binomial GLM with a constant and a covariate:
> 
>>  conageGLM
> 
> Call:  glm(formula = Sex ~ 1 + Gest_Age, family = binomial, data = 
> CVS_GG.df,      subset = CVS_GG_GA.NE.NA_sel)
> 
> Coefficients:
> (Intercept)     Gest_Age 
>     -0.21787      0.02314 
> 
> Degrees of Freedom: 60080 Total (i.e. Null);  60079 Residual
> Null Deviance:	    83250
> Residual Deviance: 83240	AIC: 83250
> 
> Note the AIC value. The two models produce the same AIC value.
> 
> 
> When I calculate the AIC values "by hand" I get
> 
> con AIC = -2loglikelihood + 2n = -2*-41623.70 + 2 = 83249.4
> conage AIC = -2*-41620.55 + 4 = 83245.1
> 
> It appears that the AIC value produced by glm for conGLM differs from 
> the hand value due only to rounding. So far, so good. BUT, the glm 
> and hand values of AIC are different (83250 vs. 83245). more than 2 
> units. this cannot (should not!) be rounding error
> 
> If I ask for the AIC values directly, I get the hand values, save for 
> trivial differences:
> 
>>  AIC(conGLM)
> [1] 83249.39
> 
>>  AIC(conageGLM)
> [1] 83245.1
> 
> Finally, stepAIC produces the hand values (remembering the read 
> output correctly, i.e., <none> denotes the unchanged model, not the 
> simplest model)
> 
>>  stepAIC(conageGLM)
> Start:  AIC=83245.1
> Sex ~ 1 + Gest_Age
> 
>                        Df Deviance   AIC
> <none>               83241 83245
> - Gest_Age  1     83247 83249
> 
> Call:  glm(formula = Sex ~ 1 + Gest_Age, family = binomial, data = 
> CVS_GG.df,      subset = CVS_GG_GA.NE.NA_sel)
> 
> Coefficients:
> (Intercept)     Gest_Age 
>     -0.21787      0.02314 
> 
> Degrees of Freedom: 60080 Total (i.e. Null);  60079 Residual
> Null Deviance:	    83250
> Residual Deviance: 83240	AIC: 83250
> 
> Note the last value of AIC, which is equal to the value produced by 
> the direct glm call.
> 
> SO, given this, for conageGLM why are the AIC value produced by glm 
> and the AIC value produced by hand (or by AIC or by stepAIC) not 
> equal?
> 
> The simplest explanation is that glm always returns the AIC of the 
> constant model (e.g., conGLM) and not the AIC associated with the 
> model being fitted if it is different from the constant model (e.g., 
> conageGLM). If this explanation correct? If so, this just so happens 
> to be something I have never seen noted and in fact, it would 
> contradict my previous usage of glm, where it appears that a call to 
> glm returns the AIC for the model being fitted........
> 
> So, what is the explanation?
> 
> 
> many thanks,
> 
> S.


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From j.hadfield at ed.ac.uk  Sun Feb  1 13:58:03 2009
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sun, 1 Feb 2009 12:58:03 +0000
Subject: [R-sig-ME] MCMCglmm
Message-ID: <6C74E83D-F259-4496-859E-D5CA60F18A47@ed.ac.uk>

Hi,

Thanks to everyone who emailed on or off-list about MCMCglmm. I had a  
lot of suggestions about additions/improvements and I've uploaded an  
update on CRAN with most of the teething problems sorted out. This  
should be available over the next couple of days

For those experiencing  dramatic crashes this is caused by variances  
hitting zero or correlations going to -1/1.  This problem does not  
happen when a proper prior is specified, which is recommended.   
MCMCglmm should terminate with more class in the updated version, and  
should print a suitable error message.

Users also asked about the negative binomial distribution, Bayes  
Factors/DIC, and alternative link functions.

The Poisson log-normal seems to fit many data as well as the neg bin  
so it probably wont be included in the near future.

I'm about half way through the code for obtainging DIC

Alternative link functions are easy to implement and if you email me  
the link function you would like and for which distribution, I'm happy  
to include it.

Alternative single parameter dsitributions are also easy to implement  
and I'm happy to include them if you can send me/refer me to their pdf  
and cdf.  If you already have C/C++ code to calculate the density  
functions then even better.

Cheers,

Jarrod

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090201/1a12a841/attachment.pl>

From orzack at freshpond.org  Sun Feb  1 17:52:08 2009
From: orzack at freshpond.org (orzack)
Date: Sun, 1 Feb 2009 11:52:08 -0500
Subject: [R-sig-ME] calculation of AIC
In-Reply-To: <49852536.5050906@ufl.edu>
References: <p06230904c5aa4ebd6003@[192.168.1.104]> <49852536.5050906@ufl.edu>
Message-ID: <p06230901c5ab78ea4835@[192.168.1.104]>

>   I believe the issue is in the output of glm, not the
>calculation.  Take a look at print.glm and consider the following:
>
>>  format(signif(83245.1,4))
>[1] "83250"
>>  format(signif(83249.4,4))
>[1] "83250"



>I haven't ever encountered this, but looks like the output is rounding
>to four significant figures. All your degrees of freedom, null
>deviance, residual deviance, and AIC are rounded to four significant
>figures. I'd directly call the degrees of freedom, null deviance, and
>residual deviance (don't know the code, but like you did for AIC(glm
>object)), and see if they are indeed rounded...
>

Dear Ben and Ben,
   Many thanks for your help. (how probable is it that the two 
responses are from Bens.......?)

Indeed, I think you are both correct that the issue is the rounding 
in glm. For what it is worth, this does speak to how even a 
well-vetted (and wonderful!) function like glm can generate an 
"issue".

Of course, the "issue" here is that the AIC values are large enough 
that the default for significant digits generates rounded values that 
are "equal" even though the "real" values differ by more than 2 
support units. The issue is that most people might not go the extra 
length of wondering about the values and would not proceed to 
calculate the AIC values separately. Of course, one response is 
garbage in, garbage out, that is, one should always be wondering 
about what comes out after the button to start a black-box 
calculation is pushed and that the user is responsible. True enough, 
but I can't find any documentation for glm that mentions this 
rounding OR how one might change the default rounding in glm.

Speaking of this, does anybody know how to change the default 
rounding for glm (and lmer) OR for an R session in general (e.g., so 
that a regular call to glm would generate AIC values with more 
digits)?

Finally, perhaps you are wondering how meaningful it is to use the 2 
support unit change in AIC to decide between models when the AIC 
values themselves are so large. To this extent, one might think that 
the rounding convention in glm was implemented with this in mind, 
i.e., "we, the makers of glm, are making sure that you, the user, 
does not use a very small difference for model decision-making when 
the AIC values are so large." Perhaps. But probably not, especially 
given the explicit discussion in Burnham and Anderson (2002, page 71) 
of the meaning of relying on small AIC differences when the AIC 
values themselves are large. They write

People are often surprised that [differences of AIC values] of only 1 
- 10 are very important, when the associated AIC values that led to 
the difference are on the order of 97,000 or 243,000.

They go to write in bold

It is not the absolute size of the AIC values, it is the relative 
values, and particularly the AIC differences that are important.

any help is much appreciated.

S.
-- 
Steven Orzack

The Fresh Pond Research Institute
173 Harvey Street
Cambridge, MA. 02140
617 864-4307

www.freshpond.org



From adik at ilovebacon.org  Sun Feb  1 20:41:44 2009
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Sun, 1 Feb 2009 11:41:44 -0800 (PST)
Subject: [R-sig-ME] calculation of AIC
In-Reply-To: <p06230901c5ab78ea4835@[192.168.1.104]>
References: <p06230904c5aa4ebd6003@[192.168.1.104]> <49852536.5050906@ufl.edu>
	<p06230901c5ab78ea4835@[192.168.1.104]>
Message-ID: <Pine.LNX.4.64.0902011134060.7667@parser.ilovebacon.org>


On Sun, 1 Feb 2009, orzack wrote:

> Speaking of this, does anybody know how to change the default rounding for 
> glm (and lmer) OR for an R session in general (e.g., so that a regular call 
> to glm would generate AIC values with more digits)?

> 1/7
[1] 0.1428571
> options(digits=22)
> 1/7
[1] 0.1428571428571428
> options(digits=23)
Error in options(digits = 23) :
   invalid 'digits' parameter, allowed 1...22
> options(digits=22)

...but this of course won't help if there is explicit rounding programmed
into glm/lmer. I also do not understand what would motivate this code,
instead of a more straightforward round(aic,0).

--Adam



From r.turner at auckland.ac.nz  Sun Feb  1 20:55:59 2009
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 2 Feb 2009 08:55:59 +1300
Subject: [R-sig-ME] calculation of AIC
In-Reply-To: <Pine.LNX.4.64.0902011134060.7667@parser.ilovebacon.org>
References: <p06230904c5aa4ebd6003@[192.168.1.104]>
	<49852536.5050906@ufl.edu>	<p06230901c5ab78ea4835@[192.168.1.104]>
	<Pine.LNX.4.64.0902011134060.7667@parser.ilovebacon.org>
Message-ID: <E555B271-815A-464E-9269-2FC16695DC9F@auckland.ac.nz>


On 2/02/2009, at 8:41 AM, Adam D. I. Kramer wrote:

>
> On Sun, 1 Feb 2009, orzack wrote:
>
>> Speaking of this, does anybody know how to change the default  
>> rounding for
>> glm (and lmer) OR for an R session in general (e.g., so that a  
>> regular call
>> to glm would generate AIC values with more digits)?
>
>> 1/7
> [1] 0.1428571
>> options(digits=22)
>> 1/7
> [1] 0.1428571428571428
>> options(digits=23)
> Error in options(digits = 23) :
>    invalid 'digits' parameter, allowed 1...22
>> options(digits=22)
>
> ...but this of course won't help if there is explicit rounding  
> programmed
> into glm/lmer. I also do not understand what would motivate this code,
> instead of a more straightforward round(aic,0).

R is open source.  Look at the code (for print.glm()). Or better,  
look at

	args(print.glm)

and you will see that these are ``x'' and ``digits''.

This suggests that

	print(fit,digits=7)

or

	print(fit,digits=17)

(where ``fit'' is the object returned by glm()) may get you somewhere.

You could also simply type ``fit$aic'' to see the AIC printed to  
whatever
number of significant digits your (global) options are set for.

	cheers,

		Rolf Turner

######################################################################
Attention:\ This e-mail message is privileged and confid...{{dropped:9}}



From evansj18 at msu.edu  Sun Feb  1 23:58:05 2009
From: evansj18 at msu.edu (evansj18 at msu.edu)
Date: Sun, 01 Feb 2009 17:58:05 -0500
Subject: [R-sig-ME] lmer maxiter not working?
Message-ID: <20090201175804.19497k4a5suffzlo@mail.msu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090201/8e2ad675/attachment.pl>

From jeremiahrounds at hotmail.com  Mon Feb  2 01:06:25 2009
From: jeremiahrounds at hotmail.com (Jeremiah Rounds)
Date: Sun, 1 Feb 2009 17:06:25 -0700
Subject: [R-sig-ME] calculation of AIC
In-Reply-To: <Pine.LNX.4.64.0902011134060.7667@parser.ilovebacon.org>
References: <p06230904c5aa4ebd6003@[192.168.1.104]>
	<49852536.5050906@ufl.edu>	<p06230901c5ab78ea4835@[192.168.1.104]> 
	<Pine.LNX.4.64.0902011134060.7667@parser.ilovebacon.org>
Message-ID: <BLU127-W20C3D36A645D66FA6CB795CBC50@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090201/af0a1491/attachment.pl>

From john.maindonald at anu.edu.au  Mon Feb  2 01:13:42 2009
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Mon, 2 Feb 2009 11:13:42 +1100
Subject: [R-sig-ME] calculation of AIC
In-Reply-To: <BLU127-W20C3D36A645D66FA6CB795CBC50@phx.gbl>
References: <p06230904c5aa4ebd6003@[192.168.1.104]> <49852536.5050906@ufl.edu>
	<p06230901c5ab78ea4835@[192.168.1.104]>
	<Pine.LNX.4.64.0902011134060.7667@parser.ilovebacon.org>
	<BLU127-W20C3D36A645D66FA6CB795CBC50@phx.gbl>
Message-ID: <48DEF208-D913-40DB-AF2B-609C4D8B5C88@anu.edu.au>

On 02/02/2009, at 11:06 AM, Jeremiah Rounds wrote:

>
>> Date: Sun, 1 Feb 2009 11:41:44 -0800> From: adik at ilovebacon.org>  
>> To: orzack at freshpond.org> CC: r-sig-mixed-models at r-project.org>  
>> Subject: Re: [R-sig-ME] calculation of AIC> > > On Sun, 1 Feb 2009,  
>> orzack wrote:> > > Speaking of this, does anybody know how to  
>> change the default rounding for > > glm (and lmer) OR for an R  
>> session in general (e.g., so that a regular call > > to glm would  
>> generate AIC values with more digits)?> > > 1/7> [1] 0.1428571> >  
>> options(digits=22)> > 1/7> [1] 0.1428571428571428> >  
>> options(digits=23)> Error in options(digits = 23) :> invalid  
>> 'digits' parameter, allowed 1...22> > options(digits=22)> > ...but  
>> this of course won't help if there is explicit rounding programmed>  
>> into glm/lmer. I also do not understand what would motivate this  
>> code,> instead of a more straightforward round(aic,0).> > --Adam
>
> First, glm apparently is not using rounding from "round".  glm is  
> using signif or equivalent logic.  There is a difference. The  
> difference is significant digits is a fairly precisely defined  
> notion.  It is the number of digits you keep on the front of the  
> power of 10 in "a X 10^b".  Round is much more cosmetic from what I  
> can tell.
>
> Second,  round(aic,0) is not more straightforward in the presence of  
> very small AIC.  Here the distinct difference from round(a,0) and  
> signif(a,4) is that you never know prior to viewing the aic how many  
> digits round(a,0) will be keeping from the original unrounded AIC  
> value.  With signif(a,4) you always know there will be a 4 digit  
> number times 10 to some power.
>
> Third, in my limited statistical experience (I am a master's  
> student) AIC is not a method where those extra digits ever really  
> matter.  I did a project on AIC/BIC model selection.  IMO in order  
> to use AIC properly you have to consider the models in a nearby  
> neighborhood to the best AIC as just as good as the model with the  
> best AIC and consider sensitivity in your estimates.    There is no  
> real context where you can properly say "the fifth significant digit  
> of AIC has decided that model A is better than model B, and so I  
> discard model B."

Well said!

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.

> That is what I think,
> Jeremiah
>
>>> _______________________________________________> R-sig-mixed-models at r-project.org 
>>>  mailing list> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> _________________________________________________________________
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From maj at stats.waikato.ac.nz  Mon Feb  2 01:31:08 2009
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 02 Feb 2009 13:31:08 +1300
Subject: [R-sig-ME] calculation of AIC
In-Reply-To: <48DEF208-D913-40DB-AF2B-609C4D8B5C88@anu.edu.au>
References: <p06230904c5aa4ebd6003@[192.168.1.104]>
	<49852536.5050906@ufl.edu>	<p06230901c5ab78ea4835@[192.168.1.104]>	<Pine.LNX.4.64.0902011134060.7667@parser.ilovebacon.org>	<BLU127-W20C3D36A645D66FA6CB795CBC50@phx.gbl>
	<48DEF208-D913-40DB-AF2B-609C4D8B5C88@anu.edu.au>
Message-ID: <49863ECC.8070308@stats.waikato.ac.nz>

Although if log-likelihood is arbitrary up to an additive constant we 
may add a sufficiently large constant onto any two AICs and push the 
first non-equal digit of the two AICs as far down as we want!

Murray Jorgensen

John Maindonald wrote:
> On 02/02/2009, at 11:06 AM, Jeremiah Rounds wrote:
> 
>>
>>> Date: Sun, 1 Feb 2009 11:41:44 -0800> From: adik at ilovebacon.org> To: 
>>> orzack at freshpond.org> CC: r-sig-mixed-models at r-project.org> Subject: 
>>> Re: [R-sig-ME] calculation of AIC> > > On Sun, 1 Feb 2009, orzack 
>>> wrote:> > > Speaking of this, does anybody know how to change the 
>>> default rounding for > > glm (and lmer) OR for an R session in 
>>> general (e.g., so that a regular call > > to glm would generate AIC 
>>> values with more digits)?> > > 1/7> [1] 0.1428571> > 
>>> options(digits=22)> > 1/7> [1] 0.1428571428571428> > 
>>> options(digits=23)> Error in options(digits = 23) :> invalid 'digits' 
>>> parameter, allowed 1...22> > options(digits=22)> > ...but this of 
>>> course won't help if there is explicit rounding programmed> into 
>>> glm/lmer. I also do not understand what would motivate this code,> 
>>> instead of a more straightforward round(aic,0).> > --Adam
>>
>> First, glm apparently is not using rounding from "round".  glm is 
>> using signif or equivalent logic.  There is a difference. The 
>> difference is significant digits is a fairly precisely defined 
>> notion.  It is the number of digits you keep on the front of the power 
>> of 10 in "a X 10^b".  Round is much more cosmetic from what I can tell.
>>
>> Second,  round(aic,0) is not more straightforward in the presence of 
>> very small AIC.  Here the distinct difference from round(a,0) and 
>> signif(a,4) is that you never know prior to viewing the aic how many 
>> digits round(a,0) will be keeping from the original unrounded AIC 
>> value.  With signif(a,4) you always know there will be a 4 digit 
>> number times 10 to some power.
>>
>> Third, in my limited statistical experience (I am a master's student) 
>> AIC is not a method where those extra digits ever really matter.  I 
>> did a project on AIC/BIC model selection.  IMO in order to use AIC 
>> properly you have to consider the models in a nearby neighborhood to 
>> the best AIC as just as good as the model with the best AIC and 
>> consider sensitivity in your estimates.    There is no real context 
>> where you can properly say "the fifth significant digit of AIC has 
>> decided that model A is better than model B, and so I discard model B."
> 
> Well said!
> 
> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Mathematics & Its Applications, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
> 
>> That is what I think,
>> Jeremiah
>>
>>>> _______________________________________________> 
>>>> R-sig-mixed-models at r-project.org mailing list> 
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> _________________________________________________________________
>>
>>
>>     [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862



From bolker at ufl.edu  Mon Feb  2 01:34:44 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Sun, 01 Feb 2009 19:34:44 -0500
Subject: [R-sig-ME] calculation of AIC
In-Reply-To: <BLU127-W20C3D36A645D66FA6CB795CBC50@phx.gbl>
References: <p06230904c5aa4ebd6003@[192.168.1.104]>	<49852536.5050906@ufl.edu>	<p06230901c5ab78ea4835@[192.168.1.104]>
	<Pine.LNX.4.64.0902011134060.7667@parser.ilovebacon.org>
	<BLU127-W20C3D36A645D66FA6CB795CBC50@phx.gbl>
Message-ID: <49863FA4.7020309@ufl.edu>


  Well, my two cents is (are?) that the AICs should definitely be
presented via round(AIC,0) or round(AIC,1) and not with signif()
as they are.  Deviance/AIC differences are really only important
on an absolute scale (agreeing with Murray Jorgensen).  Thus
I would consider the use of signif() rather than round() in
print.glm() a "misfeature" ...  (Does anyone know of a context
where *relative* differences in AIC or deviance are important?)

  Ben Bolker

Jeremiah Rounds wrote:
>> Date: Sun, 1 Feb 2009 11:41:44 -0800> From: adik at ilovebacon.org>
>> To: orzack at freshpond.org> CC: r-sig-mixed-models at r-project.org>
>> Subject: Re: [R-sig-ME] calculation of AIC> > > On Sun, 1 Feb 2009,
>> orzack wrote:> > > Speaking of this, does anybody know how to
>> change the default rounding for > > glm (and lmer) OR for an R
>> session in general (e.g., so that a regular call > > to glm would
>> generate AIC values with more digits)?> > > 1/7> [1] 0.1428571> >
>> options(digits=22)> > 1/7> [1] 0.1428571428571428> >
>> options(digits=23)> Error in options(digits = 23) :> invalid
>> 'digits' parameter, allowed 1...22> > options(digits=22)> > ...but
>> this of course won't help if there is explicit rounding programmed>
>> into glm/lmer. I also do not understand what would motivate this
>> code,> instead of a more straightforward round(aic,0).> > --Adam
> 
> First, glm apparently is not using rounding from "round".  glm is
> using signif or equivalent logic.  There is a difference. The
> difference is significant digits is a fairly precisely defined
> notion.  It is the number of digits you keep on the front of the
> power of 10 in "a X 10^b".  Round is much more cosmetic from what I
> can tell.
> 
> Second,  round(aic,0) is not more straightforward in the presence of
> very small AIC.  Here the distinct difference from round(a,0) and
> signif(a,4) is that you never know prior to viewing the aic how many
> digits round(a,0) will be keeping from the original unrounded AIC
> value.  With signif(a,4) you always know there will be a 4 digit
> number times 10 to some power.
> 
> Third, in my limited statistical experience (I am a master's student)
> AIC is not a method where those extra digits ever really matter.  I
> did a project on AIC/BIC model selection.  IMO in order to use AIC
> properly you have to consider the models in a nearby neighborhood to
> the best AIC as just as good as the model with the best AIC and
> consider sensitivity in your estimates.    There is no real context
> where you can properly say "the fifth significant digit of AIC has
> decided that model A is better than model B, and so I discard model
> B."
> 
> That is what I think, Jeremiah
> 
>>> _______________________________________________>
>>> R-sig-mixed-models at r-project.org mailing list>
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> _________________________________________________________________
> 
> 
> [[alternative HTML version deleted]]
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From kingsfordjones at gmail.com  Mon Feb  2 05:14:54 2009
From: kingsfordjones at gmail.com (Kingsford Jones)
Date: Sun, 1 Feb 2009 21:14:54 -0700
Subject: [R-sig-ME] Jointly testing several parameters of factor variable
In-Reply-To: <49843666.9070705@maw.ru.nl>
References: <49843666.9070705@maw.ru.nl>
Message-ID: <2ad0cc110902012014u2d7f8a1dg69061a6bb2d4d93@mail.gmail.com>

Hi Stijn,

#install.packages('gmodels')
?gmodels::estimable


hth,

Kingsford Jones



On Sat, Jan 31, 2009 at 4:30 AM, Stijn Ruiter <s.ruiter at maw.ru.nl> wrote:
> Hi all,
> I am estimating a lmer model and now I would like to test whether some
> coefficients for specific levels of factor variable are equal.
> I am thinking of either:
> (1) applying equality constraints on several parameters for specific
> levels of a factor variable (and use that to do LRT against model
> without equality constraints), but I have no clue on how to that; or
> (2) do a Wald test in which several parameter estimates of a factor
> variable are jointly tested. Although I noticed "wald.test" from the
> "aod" package, but I don't quite follow how to apply that to my lmer model.
>
> Any suggestions on how to jointly test several parameters of a lmer
> model (preferably applied to factor variables)?
>
> Stijn
>
> --
> Best regards,
>
> Stijn Ruiter
> Department of Sociology / ICS
> Radboud University Nijmegen
> P.O. Box 9104
> 6500 HE Nijmegen
> Netherlands
>
> Phone: + 31 24 361 2272
> Fax:   + 31 24 361 2399
>
> Visiting address:
> Thomas van Aquinostraat 4.01.71
> Nijmegen
>
> website: http://oase.uci.ru.nl/~sruiter
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From orzack at freshpond.org  Mon Feb  2 02:34:19 2009
From: orzack at freshpond.org (orzack)
Date: Sun, 1 Feb 2009 20:34:19 -0500
Subject: [R-sig-ME] calculation of AIC
Message-ID: <p06230901c5abfaf19547@[192.168.1.104]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090201/c36917e1/attachment.pl>

From christina.bogner at uni-bayreuth.de  Mon Feb  2 18:00:18 2009
From: christina.bogner at uni-bayreuth.de (Christina Bogner)
Date: Mon, 02 Feb 2009 18:00:18 +0100
Subject: [R-sig-ME] Mixed-models and condition number
Message-ID: <498726A2.8040005@uni-bayreuth.de>

Dear list members,

I'm working with both nlme and lme4 packages trying to fit linear 
mixed-models to soil chemical and physical data. I know that for linear 
models one can calculate the condition number kappa of the model matrix 
to know whether the problem is well- or ill-conditioned. Does it make 
any sense to compute kappa on the design matrix of the fixed-effects in 
nlme or lme4? For comparison I fitted a simple linear model to my data 
and scaling some numerical predictors decreased kappa considerably. So I 
wonder if scaling them in the mixed-model has any advantages?

Thanks a lot for your help.

Christina Bogner

-- 
=======================================================
Christina Bogner
D8

Ecological Modelling
Soil Physics Group
University of Bayreuth
Dr.-Hans-Frisch-Stra?e 1-3
D-95440 Bayreuth
Germany

Tel:	++49 (0)921 555655
Fax:	++49 (0)921 555799
Mail:	christina.bogner at uni-bayreuth.de
Web:	http://www.bitoek.uni-bayreuth.de/mod/



From evansj18 at msu.edu  Mon Feb  2 18:06:23 2009
From: evansj18 at msu.edu (Jeff Evans)
Date: Mon, 2 Feb 2009 12:06:23 -0500
Subject: [R-sig-ME] lmer maxiter not working?
In-Reply-To: <20090201175804.19497k4a5suffzlo@mail.msu.edu>
References: <20090201175804.19497k4a5suffzlo@mail.msu.edu>
Message-ID: <5240B7E6AF5A48F09B1EC35098A2AFD4@myelin>

Addendum:
	I have tried this now on XP 32 bit (R 2.8.0) and Ubuntu 64 bit
(2.8.1) with the same errors. 

Thanks again,

Jeff Evans,
Michigan State University




I have been trying to change the number of iterations allowed by lmer in
lme4 but it always runs for exactly 300 iterations.


mp1 <- lmer(Siliques ~ State * Year +
AdultsJuneD + RosOctD + SumSrv + WinSrv +
soilsPC1 + soilsPC2 + 
WinClimPC1 + WinClimPC2 + WinClimPC2_2 +
(RosOctD*Year | Site) + (1 | ID),
data=dat.gm,
family="poisson",
verbose=TRUE,
control = list(maxIter = 100))

299:     3796.3537: 0.669478 0.895477 5.18862e-10 0.612108 
300:     3796.3537: 0.669478 0.895478  0.00000 0.612108 ......
Warning message:
In mer_finalize(ans) : iteration limit reached without convergence (9)

changing to:
...control = list(maxIter = 10000))

still only runs for 300 iterations with same warning

Any thoughts?

particulars:
?lme4? version 0.999375-28
R version 2.8.1 (2008-12-22)
Vista Ultimate 64 Bit sp1

Thanks,
Jeff


	[[alternative HTML version deleted]]



From Greg.Snow at imail.org  Mon Feb  2 19:27:30 2009
From: Greg.Snow at imail.org (Greg Snow)
Date: Mon, 2 Feb 2009 11:27:30 -0700
Subject: [R-sig-ME] Jointly testing several parameters of factor variable
In-Reply-To: <49843666.9070705@maw.ru.nl>
References: <49843666.9070705@maw.ru.nl>
Message-ID: <B37C0A15B8FB3C468B5BC7EBC7DA14CC61C9DCDC25@LP-EXMBVS10.CO.IHC.COM>

For number (1) below you can either set up a set of contrasts such that when some of the contrasts are 0, the equality constraints of interest hold, then look at those specific contrasts (see functions 'contrasts' and 'C' for ways to specify the contrasts).  Or you can create variables recoded such that using one set of the variables is the full model and the other set is your constrained model, then just do a full/reduced model comparison between the 2 groups of variables.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
801.408.8111


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-bounces at r-project.org] On Behalf Of Stijn Ruiter
> Sent: Saturday, January 31, 2009 4:31 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Jointly testing several parameters of factor
> variable
> 
> Hi all,
> I am estimating a lmer model and now I would like to test whether some
> coefficients for specific levels of factor variable are equal.
> I am thinking of either:
> (1) applying equality constraints on several parameters for specific
> levels of a factor variable (and use that to do LRT against model
> without equality constraints), but I have no clue on how to that; or
> (2) do a Wald test in which several parameter estimates of a factor
> variable are jointly tested. Although I noticed "wald.test" from the
> "aod" package, but I don't quite follow how to apply that to my lmer
> model.
> 
> Any suggestions on how to jointly test several parameters of a lmer
> model (preferably applied to factor variables)?
> 
> Stijn
> 
> --
> Best regards,
> 
> Stijn Ruiter
> Department of Sociology / ICS
> Radboud University Nijmegen
> P.O. Box 9104
> 6500 HE Nijmegen
> Netherlands
> 
> Phone: + 31 24 361 2272
> Fax:   + 31 24 361 2399
> 
> Visiting address:
> Thomas van Aquinostraat 4.01.71
> Nijmegen
> 
> website: http://oase.uci.ru.nl/~sruiter
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From p.taylor at niwa.co.nz  Mon Feb  2 21:20:15 2009
From: p.taylor at niwa.co.nz (Paul Taylor)
Date: Tue, 03 Feb 2009 09:20:15 +1300
Subject: [R-sig-ME] (no subject)
Message-ID: <49880C4F.92C5.00A2.0@niwa.co.nz>



From Stephan.Kolassa at gmx.de  Mon Feb  2 21:20:00 2009
From: Stephan.Kolassa at gmx.de (Stephan Kolassa)
Date: Mon, 02 Feb 2009 21:20:00 +0100
Subject: [R-sig-ME] Mixed-models and condition number
In-Reply-To: <498726A2.8040005@uni-bayreuth.de>
References: <498726A2.8040005@uni-bayreuth.de>
Message-ID: <49875570.6000902@gmx.de>

Hi Christina,

let me start by saying that I don't know of anyone looking at
conditioning of design matrices in a mixed model environment. Might be a
nice topic to have an M. Sc. student play around with empirically. The 
problem with ill-conditioning in fixed-effects models basically comes 
down to high variances in the parameter estimates, so one could actually 
build a mixed model with an ill-conditioned design matrix and play 
around with small changes to simulated observations, checking whether 
inferences or estimates exhibit "large" variance.

If you find out anything about this, would you let me know?

That said, my recent interest has been in collinearity between
predictors, which is not exactly conditioning, but reasonably close to
it. I'd recommend you look at Hill & Adkins (2001) and the collinearity
diagnostics they recommend. Belsley (1991a) wrote an entire monograph
about them, but there are also shorter introductions, e.g., Belsley (1991b).

Scaling the columns of X to equal euclidean length (usually to length 1)
before diagnosing collinearity appears to be accepted procedure, so I 
think scaling would be a good starting point in the mixed model, too. 
However, there is a discussion as to whether to first remove the
constant column from X and subtract the column mean from each of the 
remaining columns.

Marquardt (1980) claims that centering removes "nonessential ill
conditioning." Weisberg (1980) and Montgomery and Peck (1982) also 
advocate centering.

Other practitioners maintain that centering removes meaningful
information from X, such as collinearity with the constant column, and 
should not be used (Belsey et al., 1980; Belsley, 1984a, 1984b, 1986, 
1991a, 1991b; Echambadi & Hess, 2007; Hill & Adkins, 2001). For example, 
Simon and Lesage (1988) found that collinearity with the constant
column introduces numerical instability, which is mitigated but not 
prevented by employing collinearity diagnostics after centering X. In 
addition, these problems are not confined to the constant coefficient, 
but extend to all estimates.

For a very lively debate on this topic see Belsley (1984a); Cook (1984);
Gunst (1984); Snee and Marquardt (1984); Wood (1984); Belsley (1984b). 
The consensus seems to be that centering cannot be once and for all be 
advised or rejected; rather, whether or not to center data depends on 
the problem one is facing.

HTH,
Stephan


* Belsey, D. A., Kuh, E., & Welsch, R. E. (1980). Regression 
Diagnostics: Identifying Influential Data and Sources of Collinearity. 
New York, NY: John Wiley & Sons.

* Belsley, D. A. (1984a, May). Demeaning Conditioning Diagnostics 
through Centering. The American Statistician, 38(2), 73-77.

* Belsley, D. A. (1984b, May). Demeaning Conditioning Diagnostics 
through Centering: Reply. The American Statistician, 38(2), 90-93.

* Belsley, D. A. (1986). Centering, the constant, first-differencing, 
and assessing conditioning. In E. Kuh & D. A. Belsley (Eds.), Model 
Reliability (p. 117-153). Cambridge: MIT Press.

* Belsley, D. A. (1987). Collinearity and Least Squares Regression: 
Comment -- Well-Conditioned Collinearity Indices. Statistical Science, 
2(1), 86-91. Available from http://projecteuclid.org/euclid.ss/1177013441

* Belsley, D. A. (1991a). Conditioning Diagnostics: Collinearity and 
Weak Data in Regression. New York, NY: Wiley.

* Belsley, D. A. (1991b, February). A Guide to using the collinearity 
diagnostics. Computational Economics, 4(1), 33-50. Available from 
http://www.springerlink.com/content/v135h6631x412kk8/

* Cook, R. D. (1984, May). Demeaning Conditioning Diagnostics through 
Centering: Comment. The American Statistician, 38(2), 78-79.

* Echambadi, R., & Hess, J. D. (2007, May-June). Mean-Centering Does Not 
Alleviate Collinearity Problems in Moderated Multiple Regression Models. 
Marketing Science, 26(3), 438-445.

* Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (3rd ed.). 
Baltimore: Johns Hopkins University Press.

* Gunst, R. F. (1984, May). Comment: Toward a Balanced Assessment of 
Collinearity Diagnostics. The American Statistician, 38(2), 79-82.

* Hill, R. C., & Adkins, L. C. (2001). Collinearity. In B. H. Baltagi 
(Ed.), A Companion to Theoretical Econometrics (p. 256-278). Oxford: 
Blackwell.

* Marquardt, D. W. (1987). Collinearity and Least Squares Regression: 
Comment. Statistical Science, 2(1), 84-85. Available from 
http://projecteuclid.org/euclid.ss/1177013440

* Montgomery, D. C., & Peck, E. A. (1982). Introduction to Linear 
Regression Analysis. New York, NY: John Wiley.

* Simon, S. D., & Lesage, J. P. (1988, January). The impact of 
collinearity involving the intercept term on the numerical accuracy of 
regression. Computational Economics (formerly Computer Science in 
Economics and Management), 1(2), 137-152.

* Snee, R. D., & Marquardt, D. W. (1984, May). Comment: Collinearity 
Diagnostics Depend on the Domain of Prediction, the Model, and the Data. 
The American Statistician, 38(2), 83-87.

* Weisberg, S. (1980). Applied Linear Regression. New York, NY: John Wiley.

* Wood, F. S. (1984, May). Comment: Effect of Centering on Collinearity 
and Interpretation of the Constant. The American Statistician, 38(2), 88-90.



Christina Bogner schrieb:
> Dear list members,
> 
> I'm working with both nlme and lme4 packages trying to fit linear 
> mixed-models to soil chemical and physical data. I know that for
> linear models one can calculate the condition number kappa of the
> model matrix to know whether the problem is well- or ill-conditioned.
> Does it make any sense to compute kappa on the design matrix of the
> fixed-effects in nlme or lme4? For comparison I fitted a simple
> linear model to my data and scaling some numerical predictors
> decreased kappa considerably. So I wonder if scaling them in the
> mixed-model has any advantages?
> 
> Thanks a lot for your help.
> 
> Christina Bogner
>



From kevin.thorpe at utoronto.ca  Mon Feb  2 21:23:05 2009
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Mon, 02 Feb 2009 15:23:05 -0500
Subject: [R-sig-ME] Status of mcmcsamp in 0.999375-28
Message-ID: <49875629.5010000@utoronto.ca>

Hello,

I'm just wondering what the status of mcmcsamp is in version 0.999375-28 
of lme4.  No offense intended, but is it working with a lmer() fit?  I 
only ask, because I've read warnings in the past.

Thanks.

-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Dalla Lana School of Public Health
University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.6057



From bates at stat.wisc.edu  Mon Feb  2 21:49:48 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 2 Feb 2009 14:49:48 -0600
Subject: [R-sig-ME] Mixed-models and condition number
In-Reply-To: <498726A2.8040005@uni-bayreuth.de>
References: <498726A2.8040005@uni-bayreuth.de>
Message-ID: <40e66e0b0902021249x11fe6e47u20f5c1e8864a1955@mail.gmail.com>

On Mon, Feb 2, 2009 at 11:00 AM, Christina Bogner
<christina.bogner at uni-bayreuth.de> wrote:
> Dear list members,

> I'm working with both nlme and lme4 packages trying to fit linear
> mixed-models to soil chemical and physical data. I know that for linear
> models one can calculate the condition number kappa of the model matrix to
> know whether the problem is well- or ill-conditioned. Does it make any sense
> to compute kappa on the design matrix of the fixed-effects in nlme or lme4?
> For comparison I fitted a simple linear model to my data and scaling some
> numerical predictors decreased kappa considerably. So I wonder if scaling
> them in the mixed-model has any advantages?

I'm not sure that checking the conditioning of only the fixed-effects
model matrix is what you want to do.  In some ways it would be more
informative to check the conditioning of the triangular matrix derived
from the fixed-effects model matrix after removing the random effects.
 That upper triangular matrix is stored as the RX slot.  Generally I
don't recommend reaching in to a structure or an S4 object and pulling
out a piece of it (because you can't count on those slots continuing
to be there and to have the same names and contents) but in this case
it may be the best way of going about things.

If this is considered useful I could add a kappa method for the class.



From a.renwick at abdn.ac.uk  Tue Feb  3 15:22:22 2009
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Tue, 3 Feb 2009 14:22:22 +0000
Subject: [R-sig-ME] Unbalanced presence/absence data
Message-ID: <B9D1301370916C44B5874AF340C18B9B7F911B5363@VMAILB.uoa.abdn.ac.uk>

I am trying to analyse some data I have on the presence/absence of parasite infestation on small mammals using a GLMM, however I have a severely unbalanced data set in that I have a large number of 0's compared to 1's (i.e. 1333 0's and 86 1's).

The response variable (presence/absence) is at the individual level whereas all the explanatory variables (apart from sex) are at the site level.  This means that a lot of the individuals have exactly the same combination of all explanatory variables and when there is so many individuals with 0's it leaves very little power.

When I reduce the model I find that I can remove a number of interactions terms without really affecting the AIC which lead me to be slightly concerned.

One option would be to analyses the data at the site level, i.e parasite prevalence, rather than the probability of being infested.

Any advice as to how to deal with this unbalanced data set would be very much appreciated.

Anna Renwick
Institute of Biological & Environment Sciences
University of Aberdeen
Zoology Building
Tillydrone Avenue
Aberdeen
AB24 2TZ


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From cmk6 at umd.edu  Tue Feb  3 16:33:03 2009
From: cmk6 at umd.edu (cmk6 at umd.edu)
Date: Tue,  3 Feb 2009 10:33:03 -0500 (EST)
Subject: [R-sig-ME] help with getting pvalues in lme4 - problems with
 mcmcpvalue, aovlmer.fnc, and HPDinterval
Message-ID: <20090203103303.AMC01627@po5.mail.umd.edu>

I can?t seem to get sensible pvalues testing the effect of one factor ?landtype? in the following model using function mcmcpvalue; and it seems from previous emails that it is no longer possible to use aovlmer.fnc ( ) in lme4.  And as of last night, I get new error (?Error in UseMethod("HPDinterval") : no applicable method for "HPDinterval")when trying to perform HPDinterval ().  

I would appreciate any guidance on how to get pvalues to test one factor using lmer function!  In addition, I'd like to verify that the best way to determine significant pairwise differences among factor levels is to examine overlap of 95% CIs generated based on MCMC (i.e., using HPDinterval or pvals.fnc).

I?m running the following model:
>modlmer <- lmer(Chao1.Res ~ landtype  + (1|patch), data = S.final, REML=FALSE)

Where:  landtype has 4 levels:  Agriculture (as Intercept), Bauxite, Forest, Urban.

Model output:
Linear mixed model fit by maximum likelihood 
Formula: Chao1.Res ~ landtype + (1 | patch) 
   Data: S.final 
  AIC  BIC logLik deviance REMLdev
 1732 1753 -859.8     1720    1711
Random effects:
 Groups   Name        Variance Std.Dev.
 patch    (Intercept) 22.084   4.6993  
 Residual             36.615   6.0510  
Number of obs: 253, groups: patch, 99

Fixed effects:
                Estimate Std. Error t value
(Intercept)       27.300      1.322  20.652
landtypeBauxite   -6.357      1.772  -3.588
landtypeForest    -1.049      1.727  -0.607
landtypeUrban     -5.504      1.885  -2.920

Correlation of Fixed Effects:
            (Intr) lndtyB lndtyF
landtypeBxt -0.746              
landtypFrst -0.765  0.571       
landtypUrbn -0.701  0.523  0.537

> markov <- mcmcsamp(modlmer, 50000)

I get the below error (as of last night) when trying to run HPDinterval
> HPDinterval(markov)
Error in UseMethod("HPDinterval") : no applicable method for "HPDinterval"

So instead I ran:
> pvals.fnc(modlmer)$fixed
Estimate MCMCmean HPD95lower HPD95upper  pMCMC Pr(>|t|)
(Intercept)  27.300   27.403     25.279     29.610 0.0001   0.0000
landtypeBauxite   -6.357   -6.519     -9.384     -3.603 0.0001   0.0012
landtypeForest    -1.049   -1.124     -3.864      1.790 0.4296   0.5829
landtypeUrban     -5.504   -5.610     -8.705     -2.677 0.0002   0.0077

Question:  So should I look at HPD95lower and HPD95upper to determine whether factor levels (Ag, Bauxite, Forest, or Urban) differ pair-wise?

Trying to get pvalues using:
> mcmcpvalue <- function(samp) {
+ std <- backsolve(chol(var(samp)),
+                      cbind(0, t(samp)) - colMeans(samp),
+                      transpose = TRUE)
+     sqdist <- colSums(std * std)
+     sum(sqdist[-1] > sqdist[1])/nrow(samp)
+ }

I need pvalue to test landtype ? levels Agriculture (Intercept), Bauxite, Forest, and Urban:
> mcmcpvalue(as.matrix(markov)[, 1:4])
[1] 0

Pvalue of 0 does not make sense.

Verified that I wanted first 4 columns:
> head(as.matrix(markov)) 
     (Intercept) landtypeBauxite landtypeForest landtypeUrban       ST1    sigma
[1,]    27.30047       -6.357429      -1.048611     -5.503726 0.7766178 6.051015
[2,]    28.04026       -6.345041      -0.329192     -7.144239 0.5959559 5.379194
[3,]    28.83598       -8.727255      -4.117192     -5.349411 0.6346374 6.175085
[4,]    27.33180       -5.973496      -1.231745     -5.913463 0.3999811 6.015337
[5,]    27.68958       -5.833905      -2.323891     -5.954748 0.3312879 6.738105
[6,]    28.62220       -8.177245      -1.173439     -5.937315 0.3879350 7.291594

Just to check I tried difference columns and do not understand what these pvalues represent:
> mcmcpvalue(as.matrix(markov)[, 2:3]) #Bauxite & Forest?
[1] 0
> mcmcpvalue(as.matrix(markov)[, 1:3]) #Ag & Forest? 
[1] 0
> mcmcpvalue(as.matrix(markov)[, 3:4]) #Forest & Urban?
[1] 0.00062
> mcmcpvalue(as.matrix(markov)[, 1:2]) #Ag & Bauxite?
[1] 0

Determined anti-conservative pvalue based on LRT:
> mod0 <- lmer(Chao1.Res ~ 1  + (1|patch), data = S.final, REML=FALSE)
> mod1 <- lmer(Chao1.Res ~ landtype  + (1|patch), data = S.final, REML=FALSE)
> anova(mod0,mod1)
Data: S.final
Models:
mod0: Chao1.Res ~ 1 + (1 | patch)
mod1: Chao1.Res ~ landtype + (1 | patch)
     Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq)    
mod0  3 1743.43 1754.03 -868.71                             
mod1  6 1731.60 1752.80 -859.80 17.832      3  0.0004764 ***

Then tried to get pvalues based on aovlmer.fnc
> mod1.aov = aovlmer.fnc (modlmer, mcmc = x$mcmc, which = c("(Intercept)","landtypeBauxite", "landtypeForest", "landtypeUrban" )) 
Error in anova(object) : Calculated PWRSS for a LMM is negative

I have the following packages installed and loaded:
require(lme4)
require(Matrix)
require(lattice)
require(SASmixed)
require(coda)
require(languageR)


Christina Kennedy
Behavior, Ecology, Evolution & Systematics
University of Maryland
3221 Biology-Psychology Building
College Park, Maryland 20742
Cell Phone: (202) 288-7483
Lab Phone: (301) 405-4512
Email: cmk6 at umd.edu

From bolker at ufl.edu  Tue Feb  3 16:44:49 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 03 Feb 2009 10:44:49 -0500
Subject: [R-sig-ME] help with getting pvalues in lme4 - problems with
 mcmcpvalue, aovlmer.fnc, and HPDinterval
In-Reply-To: <20090203103303.AMC01627@po5.mail.umd.edu>
References: <20090203103303.AMC01627@po5.mail.umd.edu>
Message-ID: <49886671.9020107@ufl.edu>


  Since your model is relatively simple (not a GLMM, not crossed
random effects) you are likely to have much better luck getting
p values with the nlme package at this point.

  lme(fixed=Chao1Res ~ landtype, random = ~1|patch, data=S.final,
   method="ML")

 should be about right -- then I believe anova() on the result
will give you an ANOVA table.

cmk6 at umd.edu wrote:
> I can?t seem to get sensible pvalues testing the effect of one factor
> ?landtype? in the following model using function mcmcpvalue; and it
> seems from previous emails that it is no longer possible to use
> aovlmer.fnc ( ) in lme4.  And as of last night, I get new error
> (?Error in UseMethod("HPDinterval") : no applicable method for
> "HPDinterval")when trying to perform HPDinterval ().
> 
> I would appreciate any guidance on how to get pvalues to test one
> factor using lmer function!  In addition, I'd like to verify that the
> best way to determine significant pairwise differences among factor
> levels is to examine overlap of 95% CIs generated based on MCMC
> (i.e., using HPDinterval or pvals.fnc).
> 
> I?m running the following model:
>> modlmer <- lmer(Chao1.Res ~ landtype  + (1|patch), data = S.final,
>> REML=FALSE)
> 
> Where:  landtype has 4 levels:  Agriculture (as Intercept), Bauxite,
> Forest, Urban.
> 
> Model output: Linear mixed model fit by maximum likelihood Formula:
> Chao1.Res ~ landtype + (1 | patch) Data: S.final AIC  BIC logLik
> deviance REMLdev 1732 1753 -859.8     1720    1711 Random effects: 
> Groups   Name        Variance Std.Dev. patch    (Intercept) 22.084
> 4.6993 Residual             36.615   6.0510 Number of obs: 253,
> groups: patch, 99
> 
> Fixed effects: Estimate Std. Error t value (Intercept)       27.300
> 1.322  20.652 landtypeBauxite   -6.357      1.772  -3.588 
> landtypeForest    -1.049      1.727  -0.607 landtypeUrban     -5.504
> 1.885  -2.920
> 
> Correlation of Fixed Effects: (Intr) lndtyB lndtyF landtypeBxt -0.746
>  landtypFrst -0.765  0.571 landtypUrbn -0.701  0.523  0.537
> 
>> markov <- mcmcsamp(modlmer, 50000)
> 
> I get the below error (as of last night) when trying to run
> HPDinterval
>> HPDinterval(markov)
> Error in UseMethod("HPDinterval") : no applicable method for
> "HPDinterval"
> 
> So instead I ran:
>> pvals.fnc(modlmer)$fixed
> Estimate MCMCmean HPD95lower HPD95upper  pMCMC Pr(>|t|) (Intercept)
> 27.300   27.403     25.279     29.610 0.0001   0.0000 landtypeBauxite
> -6.357   -6.519     -9.384     -3.603 0.0001   0.0012 landtypeForest
> -1.049   -1.124     -3.864      1.790 0.4296   0.5829 landtypeUrban
> -5.504   -5.610     -8.705     -2.677 0.0002   0.0077
> 
> Question:  So should I look at HPD95lower and HPD95upper to determine
> whether factor levels (Ag, Bauxite, Forest, or Urban) differ
> pair-wise?
> 
> Trying to get pvalues using:
>> mcmcpvalue <- function(samp) {
> + std <- backsolve(chol(var(samp)), +                      cbind(0,
> t(samp)) - colMeans(samp), +                      transpose = TRUE) +
> sqdist <- colSums(std * std) +     sum(sqdist[-1] >
> sqdist[1])/nrow(samp) + }
> 
> I need pvalue to test landtype ? levels Agriculture (Intercept),
> Bauxite, Forest, and Urban:
>> mcmcpvalue(as.matrix(markov)[, 1:4])
> [1] 0
> 
> Pvalue of 0 does not make sense.
> 
> Verified that I wanted first 4 columns:
>> head(as.matrix(markov))
> (Intercept) landtypeBauxite landtypeForest landtypeUrban       ST1
> sigma [1,]    27.30047       -6.357429      -1.048611     -5.503726
> 0.7766178 6.051015 [2,]    28.04026       -6.345041      -0.329192
> -7.144239 0.5959559 5.379194 [3,]    28.83598       -8.727255
> -4.117192     -5.349411 0.6346374 6.175085 [4,]    27.33180
> -5.973496      -1.231745     -5.913463 0.3999811 6.015337 [5,]
> 27.68958       -5.833905      -2.323891     -5.954748 0.3312879
> 6.738105 [6,]    28.62220       -8.177245      -1.173439
> -5.937315 0.3879350 7.291594
> 
> Just to check I tried difference columns and do not understand what
> these pvalues represent:
>> mcmcpvalue(as.matrix(markov)[, 2:3]) #Bauxite & Forest?
> [1] 0
>> mcmcpvalue(as.matrix(markov)[, 1:3]) #Ag & Forest?
> [1] 0
>> mcmcpvalue(as.matrix(markov)[, 3:4]) #Forest & Urban?
> [1] 0.00062
>> mcmcpvalue(as.matrix(markov)[, 1:2]) #Ag & Bauxite?
> [1] 0
> 
> Determined anti-conservative pvalue based on LRT:
>> mod0 <- lmer(Chao1.Res ~ 1  + (1|patch), data = S.final,
>> REML=FALSE) mod1 <- lmer(Chao1.Res ~ landtype  + (1|patch), data =
>> S.final, REML=FALSE) anova(mod0,mod1)
> Data: S.final Models: mod0: Chao1.Res ~ 1 + (1 | patch) mod1:
> Chao1.Res ~ landtype + (1 | patch) Df     AIC     BIC  logLik  Chisq
> Chi Df Pr(>Chisq) mod0  3 1743.43 1754.03 -868.71
>  mod1  6 1731.60 1752.80 -859.80 17.832      3  0.0004764 ***
> 
> Then tried to get pvalues based on aovlmer.fnc
>> mod1.aov = aovlmer.fnc (modlmer, mcmc = x$mcmc, which =
>> c("(Intercept)","landtypeBauxite", "landtypeForest",
>> "landtypeUrban" ))
> Error in anova(object) : Calculated PWRSS for a LMM is negative
> 
> I have the following packages installed and loaded: require(lme4) 
> require(Matrix) require(lattice) require(SASmixed) require(coda) 
> require(languageR)
> 
> 
> Christina Kennedy Behavior, Ecology, Evolution & Systematics 
> University of Maryland 3221 Biology-Psychology Building College Park,
> Maryland 20742 Cell Phone: (202) 288-7483 Lab Phone: (301) 405-4512 
> Email: cmk6 at umd.edu _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From atyre2 at unlnotes.unl.edu  Tue Feb  3 15:41:32 2009
From: atyre2 at unlnotes.unl.edu (Andrew J Tyre)
Date: Tue, 3 Feb 2009 08:41:32 -0600
Subject: [R-sig-ME] Unbalanced presence/absence data
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B7F911B5363@VMAILB.uoa.abdn.ac.uk>
Message-ID: <OFF91D13D3.56B63F6D-ON86257552.00504757-86257552.005092CA@unl.edu>

Hi Anna,

if your covariates are at the site level, then I suggest reducing your 
sample to a pure binomial case - counts of individuals with and without 
parasites. This is exactly the case when you will run into large amounts 
of overdispersion, because between individual differences in 
susceptibility and exposure within sites lead to larger than binomial 
variation between sites. However, you can at least partially account for 
this by including a random effect of site in the model - this leads to the 
"normal-binomial" model discussed in earlier posts (how do you all find 
those earlier posts?). 

hth,


Drew Tyre

School of Natural Resources
University of Nebraska-Lincoln
416 Hardin Hall, East Campus
3310 Holdrege Street
Lincoln, NE 68583-0974

phone: +1 402 472 4054 
fax: +1 402 472 2946
email: atyre2 at unl.edu
http://snr.unl.edu/tyre



"Renwick, A. R." <a.renwick at abdn.ac.uk> 
Sent by: r-sig-mixed-models-bounces at r-project.org
02/03/2009 08:33 AM

To
"'r-sig-mixed-models at r-project.org'" <r-sig-mixed-models at r-project.org>
cc

Subject
[R-sig-ME] Unbalanced presence/absence data






I am trying to analyse some data I have on the presence/absence of 
parasite infestation on small mammals using a GLMM, however I have a 
severely unbalanced data set in that I have a large number of 0's compared 
to 1's (i.e. 1333 0's and 86 1's).

The response variable (presence/absence) is at the individual level 
whereas all the explanatory variables (apart from sex) are at the site 
level.  This means that a lot of the individuals have exactly the same 
combination of all explanatory variables and when there is so many 
individuals with 0's it leaves very little power.

When I reduce the model I find that I can remove a number of interactions 
terms without really affecting the AIC which lead me to be slightly 
concerned.

One option would be to analyses the data at the site level, i.e parasite 
prevalence, rather than the probability of being infested.

Any advice as to how to deal with this unbalanced data set would be very 
much appreciated.

Anna Renwick
Institute of Biological & Environment Sciences
University of Aberdeen
Zoology Building
Tillydrone Avenue
Aberdeen
AB24 2TZ


The University of Aberdeen is a charity registered in Scotland, No 
SC013683.

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From Greg.Distiller at uct.ac.za  Tue Feb  3 15:18:12 2009
From: Greg.Distiller at uct.ac.za (Greg Distiller)
Date: Tue, 03 Feb 2009 16:18:12 +0200
Subject: [R-sig-ME] augPred plots in nlme: configuring the legend
Message-ID: <49886E44.90E8.005C.0@uct.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090203/4697f196/attachment.pl>

From Thierry.ONKELINX at inbo.be  Wed Feb  4 10:51:14 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 4 Feb 2009 10:51:14 +0100
Subject: [R-sig-ME] [R] ANOVA in R
In-Reply-To: <830888.17245.qm@web45407.mail.sp1.yahoo.com>
References: <830888.17245.qm@web45407.mail.sp1.yahoo.com>
Message-ID: <2E9C414912813E4EB981326983E0A104060ABF9D@inboexch.inbo.be>

Dear Samor,

Note that the R-Sig-mixed-models is more suitable for that kind of questions.
 
You need to add "+", ":" or "*" between the variables in the formula

The correlation is lme() is not the same as the correlation in SAS. The correlation in lme() is the correlation among residuals, not among random effects. You need one of the pdClasses if you want to define the correlation among the random effects. Have a look at ?nlme::pdClasses

Hence your code should probably look like this

Model <- lme(response ~ seq + period + treat*time, random = pdCompSymm(form = ~1|SUB))
anova(Model)

HTH,

Thierry
----------------------------------------------------------------------------
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics, methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org] Namens Samor Gandhi
Verzonden: woensdag 4 februari 2009 7:43
Aan: r-help at r-project.org
Onderwerp: [R] ANOVA in R

Hi,I'm using a repeated measures ANOVA in R using lme(). The SAS code would be: ?
PROC MIXED DATA=[data set below];
???? CLASS pid treat period time seq;
???? MODEL Y = seq period treat time treat*time;
???? REPEATED time / SUBJECT=pid TYPE=cs;
RUN, ?I donot have SAS, instead I have R and I would like to try the following:
anova(lme(response ~ seq period treat time treat*time,random= ~1|SUB, ???correlation=corCompSymm()))

Is this correct? Can I also write the model as

Y_ijklt = m + a_l + b_k + c_j + d_t + (cd)_jt + u_ijkltY_ijklt is the response variable due to pid i, treat j, period k, seq l, and time t. Thank you very much in advance for your help :)
Samor 






      
	[[alternative HTML version deleted]]


Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From samorgandhi at yahoo.com  Wed Feb  4 10:58:47 2009
From: samorgandhi at yahoo.com (Samor Gandhi)
Date: Wed, 4 Feb 2009 01:58:47 -0800 (PST)
Subject: [R-sig-ME] [R] ANOVA in R
In-Reply-To: <2E9C414912813E4EB981326983E0A104060ABF9D@inboexch.inbo.be>
Message-ID: <388486.89172.qm@web45414.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090204/ed45860f/attachment.pl>

From Paul.Prew at ecolab.com  Thu Feb  5 02:20:37 2009
From: Paul.Prew at ecolab.com (Prew, Paul)
Date: Wed, 4 Feb 2009 19:20:37 -0600
Subject: [R-sig-ME] Should blocking factors be modeled as random effects?
In-Reply-To: <mailman.3.1233486001.27669.r-sig-mixed-models@r-project.org>
References: <mailman.3.1233486001.27669.r-sig-mixed-models@r-project.org>
Message-ID: <6B810AFB14C606439FD57E5985E03791032E92F2@useagan1500p.GLOBAL.ECOLAB.CORP>

Thank to everyone who replied to my initial query.  I've appreciated the
different angles presented for how random effects fit with designed
experiments.

------------------------------

Message: 2
Date: Sat, 31 Jan 2009 10:20:29 -0600
From: Douglas Bates <bates at stat.wisc.edu>
Subject: Re: [R-sig-ME] Should blocking factors be modeled as random
	effects?
To: "Prew, Paul" <Paul.Prew at ecolab.com>
Cc: r-sig-mixed-models at r-project.org
Message-ID:
	<40e66e0b0901310820n737f27d0nafbbf3b7e299634e at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

On Mon, Jan 26, 2009 at 2:42 PM, Prew, Paul <Paul.Prew at ecolab.com>
wrote:
> I have been following your R discussion list on mixed modeling for a 
> few weeks, in hopes of understanding mixed modeling better.  And it 
> has helped.  I was not aware of the controversy surrounding degrees of

> freedom and the distribution of test statistics.  I have just been 
> trusting the ANOVA output from software (Minitab, JMP) that reported F

> tests.  JMP uses Kenward-Roger, Minitab's ANOVA reports an 
> F-statistic, followed by "F-test not exact for this term".
>
> A recent mention by Douglas Bates of George Box, though, hit upon an 
> aspect of mixed models that has confused me.  I'm an industrial 
> statistician, and studied statistics at Iowa State and the University 
> of Minnesota.  I have had 3 courses in DOE, 2 at the graduate level, 
> and none of them mentioned blocking factors could (should?) be modeled

> as random effects.  **Exception: the whole plots in a split plot 
> design were taught as random effects.**
>
> The 2005 update to Box Hunter Hunter discusses blocking as does Wu & 
> Hamada (2000).  Both texts model blocking factors such as Days and 
> Batches as fixed effects.  Montgomery's DOE text, 2009 rev., pretty 
> consistently states that blocks can be either random or fixed.  Don't 
> have a consensus from that small sample.
>
> I'm trying to understand the implications if I consistently used 
> random effects for DOE analysis.
>
> I'm quite willing to use R for mixed models, seeing as Minitab, JMP
etc.
> appear to use degrees of freedom calculations that are questionable.
> But as Douglas points out --- Box said, "all models are wrong, some 
> are useful" => Box's latest text doesn't bother with random effects 
> for DOE =>  does it follow that for practical purposes it's OK to 
> consider blocks as fixed?  There are certainly several advantages to 
> keeping it simple (i.e. fixed only):
> * The analyses we (my statistics group) provide to our chemists and 
> engineers are more easily understood
> * The 2-day short courses we teach in DOE to these same coworkers 
> couldn't realistically get across the idea of mixed model analysis ---

> they would become less self-sufficient, where we're trying to make 
> them more self-sufficient
> * We have a handful of softwares (Minitab, JMP, Design Expert) that 
> can perform DOE and augment the results in a number of ways:
>   *** fold-over the design to resolve aliasing in fractional designs
>   *** add axial runs to enable Response Surface methods
>   *** add distributions to the input factors, enabling 
> Robustness/Sensitivity analyses
>   *** running optimization algorithms to suggest the factor settings 
> that simultaneously consider multiple objectives
>  *****  Not to mention the loss of Sample Size Calculations, far and 
> away my most frequent request None of these softwares recognize random

> factors to perform these augmentations
>
> Replacing this functionality with R is going to be a high learning 
> curve, and probably not entirely possible.  My coding skills in R 
> consist of cutting and pasting what others have done.
>
> I don't really expect that there's a "right" answer to the question of

> random effects in DOE.  But I do believe that beyond the loss of 
> p-values, there are other ramifications for advising experimenters, 
> '"You can't trust results from your blocking on Days (or Shifts or RM 
> Lots or Batches, etc) unless they are modeled as random effects."
>
> There's statistical significance, and practical significance.  My hope

> is that blocks while random effects are statistically "truer", their 
> marginal worth over fixed effects in DOE is ignorable. Again, I don't 
> want this to come across as shooting the messenger, you are only 
> laying out the current state of art and the work that remains to be 
> done.  But any insight you can provide into what's practical right now

> would be highly interesting.

Thanks for bringing up the topic, Paul.  As you and I know, you
originally sent your question to me and I encouraged you to send it to
this list.

As I wrote in my initial response to you,  "My off-the-cuff reaction is
that in these situations the effects of blocking factors are regarded as
nuisance parameters whereas in many mixed-model situations the variances
and sometimes the values of the random effects are themselves of
interest.  When the effects are nuisance parameters the simplest
approach is to model them as fixed effects."

On thinking about it more, I can imagine several different approaches to
this question.  If you just ask, "Are the levels of this blocking factor
a fixed set of levels or a random selection from a population of
possible levels?" then in most cases I imagine you would say they are a
random selection and should be modeled using random effects.
This would especially be true of what Taguchi called "environmental
factors" which, by definition, are not under the control of the
experimenter.

If you say that blocking factors are not of interest per se and that
your purpose is simply to control for them, it is simpler to model them
as fixed effects.  There are two aspects to "simpler":
computationally simpler and conceptually simpler.  Of these I think that
conceptually is more important.  The computational burden for fitting a
mixed model versus a fixed-effects model is really a software problem,
not a hardware problem.  Commercial statistical software like Minitab or
JMP with a simple, convenient interface has limited flexibility, in part
because it is designed to have a simple, convenient interface - the
"what you see is all you get" problem.  (I googled that phrase and got a
laugh from the article at www.computer-dictionary-online.org which
referred to "point-and-drool
interfaces".)  The actual calculations involved in fitting mixed models
are not that formidable but designing the interface can be.
(One of the underappreciated aspects of the model-fitting software in R,
and in the S language in general, is the structure of the model.frame,
model.matrix sequence for transforming a formula into a numerical
representation.  This makes designing an interface much.
much easier as long as you count on the user to input a formula.)

Conceptually fixed-effects models are simpler than mixed models but they
may over-simplify the analysis.  If your purpose is estimation of
fixed-effects parameters, including assessing precision of the
estimates, then you need to ask if you want to estimate those parameters
conditional the particular levels of the blocking factor that you
observed or with respect to the possible values of the blocking factors
that are represented by the sample you observed.  If you are willing to
condition on the particular levels you observed then use fixed-effects
for the blocking factor.  For all possible levels of the blocking factor
you could use random effects.  For a designed experiment the estimates
of the fixed effects will probably not be affected much by using random
effects for the blocking factor instead of fixed effects.  However the
precision of the estimates may be different.  Perhaps more importantly,
the precision of predictions of future responses would be different.
I'm not even sure how one would even formulate such a prediction from a
model with fixed effects for the blocking factor if the factor was
something like "batch" and the batches from the experiment were already
used up.

Having said that the precision of the estimates of the fixed-effects
parameters would be different if random effects are used for the
blocking factor I should admit that this is exactly the problem to which
I don't have a good general solution.

It appears that the question of fixed or random for a blocking factor is
like many others in statistics - the choice of the model depends on what
you want to do with it.

CONFIDENTIALITY NOTICE: =\ \ This e-mail communication a...{{dropped:12}}



From christina.bogner at uni-bayreuth.de  Thu Feb  5 09:12:49 2009
From: christina.bogner at uni-bayreuth.de (Christina Bogner)
Date: Thu, 05 Feb 2009 09:12:49 +0100
Subject: [R-sig-ME] Mixed-models and condition number
In-Reply-To: <49875570.6000902@gmx.de>
References: <498726A2.8040005@uni-bayreuth.de> <49875570.6000902@gmx.de>
Message-ID: <498A9F81.9030508@uni-bayreuth.de>

Dear Stephan,

thank your very much for your response and the detailed list of 
literature. I knew about Belsley (1991b) and used it on the design 
matrix of the fixed-effects. My (absolutely empirical) results were the 
following:

on  a small data set of 58 values and the mixed-effects model:
mymodel=lme(log(calcium) ~ soil.horizon+flow.region+content.of.silt, 
data=mydata, random=~1|plot)
with soil.horizon and flow.region: factors
content.of.silt: continuous covariate

1. mean-centering the continuous covariate decreased the collinearity 
between the intercept term and the continuous covariate 
(summary.lme$corFixed and Belsley 1991b on the design matrix of 
fixed-effects) and decreased kappa (of the design matrix of 
fixed-effects) by factor 12.
2. scaling the covariate to obtain fixed-effects estimates of comparable 
size decreased kappa by factor 4, but had no effect on correlation of 
the fixed-effects.
3. I compared kappas of the mixed-effects design matrix and as proposed 
by Douglas Bates of the "triangular matrix derived from the 
fixed-effects model matrix after removing the random effects" in lme4: 
the influence of mean-centering and scaling on kappa was comparable and 
values of kappas of the triangular matrix and the design matrix of the 
fixed-effects differed little for mean-centered and scaled model, but 
largely for the non-scaled and non-centered one.

I will try to find a mathematician at my university who would like to 
play around with mixed-models  ;-).

Thanks again

Christina


Stephan Kolassa schrieb:
> Hi Christina,
> let me start by saying that I don't know of anyone looking at
> conditioning of design matrices in a mixed model environment. Might be a
> nice topic to have an M. Sc. student play around with empirically. The 
> problem with ill-conditioning in fixed-effects models basically comes 
> down to high variances in the parameter estimates, so one could 
> actually build a mixed model with an ill-conditioned design matrix and 
> play around with small changes to simulated observations, checking 
> whether inferences or estimates exhibit "large" variance.
>
> If you find out anything about this, would you let me know?
>
> That said, my recent interest has been in collinearity between
> predictors, which is not exactly conditioning, but reasonably close to
> it. I'd recommend you look at Hill & Adkins (2001) and the collinearity
> diagnostics they recommend. Belsley (1991a) wrote an entire monograph
> about them, but there are also shorter introductions, e.g., Belsley 
> (1991b).
>
> Scaling the columns of X to equal euclidean length (usually to length 1)
> before diagnosing collinearity appears to be accepted procedure, so I 
> think scaling would be a good starting point in the mixed model, too. 
> However, there is a discussion as to whether to first remove the
> constant column from X and subtract the column mean from each of the 
> remaining columns.
>
> Marquardt (1980) claims that centering removes "nonessential ill
> conditioning." Weisberg (1980) and Montgomery and Peck (1982) also 
> advocate centering.
>
> Other practitioners maintain that centering removes meaningful
> information from X, such as collinearity with the constant column, and 
> should not be used (Belsey et al., 1980; Belsley, 1984a, 1984b, 1986, 
> 1991a, 1991b; Echambadi & Hess, 2007; Hill & Adkins, 2001). For 
> example, Simon and Lesage (1988) found that collinearity with the 
> constant
> column introduces numerical instability, which is mitigated but not 
> prevented by employing collinearity diagnostics after centering X. In 
> addition, these problems are not confined to the constant coefficient, 
> but extend to all estimates.
>
> For a very lively debate on this topic see Belsley (1984a); Cook (1984);
> Gunst (1984); Snee and Marquardt (1984); Wood (1984); Belsley (1984b). 
> The consensus seems to be that centering cannot be once and for all be 
> advised or rejected; rather, whether or not to center data depends on 
> the problem one is facing.
>
> HTH,
> Stephan
>
>
> * Belsey, D. A., Kuh, E., & Welsch, R. E. (1980). Regression 
> Diagnostics: Identifying Influential Data and Sources of Collinearity. 
> New York, NY: John Wiley & Sons.
>
> * Belsley, D. A. (1984a, May). Demeaning Conditioning Diagnostics 
> through Centering. The American Statistician, 38(2), 73-77.
>
> * Belsley, D. A. (1984b, May). Demeaning Conditioning Diagnostics 
> through Centering: Reply. The American Statistician, 38(2), 90-93.
>
> * Belsley, D. A. (1986). Centering, the constant, first-differencing, 
> and assessing conditioning. In E. Kuh & D. A. Belsley (Eds.), Model 
> Reliability (p. 117-153). Cambridge: MIT Press.
>
> * Belsley, D. A. (1987). Collinearity and Least Squares Regression: 
> Comment -- Well-Conditioned Collinearity Indices. Statistical Science, 
> 2(1), 86-91. Available from http://projecteuclid.org/euclid.ss/1177013441
>
> * Belsley, D. A. (1991a). Conditioning Diagnostics: Collinearity and 
> Weak Data in Regression. New York, NY: Wiley.
>
> * Belsley, D. A. (1991b, February). A Guide to using the collinearity 
> diagnostics. Computational Economics, 4(1), 33-50. Available from 
> http://www.springerlink.com/content/v135h6631x412kk8/
>
> * Cook, R. D. (1984, May). Demeaning Conditioning Diagnostics through 
> Centering: Comment. The American Statistician, 38(2), 78-79.
>
> * Echambadi, R., & Hess, J. D. (2007, May-June). Mean-Centering Does 
> Not Alleviate Collinearity Problems in Moderated Multiple Regression 
> Models. Marketing Science, 26(3), 438-445.
>
> * Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (3rd 
> ed.). Baltimore: Johns Hopkins University Press.
>
> * Gunst, R. F. (1984, May). Comment: Toward a Balanced Assessment of 
> Collinearity Diagnostics. The American Statistician, 38(2), 79-82.
>
> * Hill, R. C., & Adkins, L. C. (2001). Collinearity. In B. H. Baltagi 
> (Ed.), A Companion to Theoretical Econometrics (p. 256-278). Oxford: 
> Blackwell.
>
> * Marquardt, D. W. (1987). Collinearity and Least Squares Regression: 
> Comment. Statistical Science, 2(1), 84-85. Available from 
> http://projecteuclid.org/euclid.ss/1177013440
>
> * Montgomery, D. C., & Peck, E. A. (1982). Introduction to Linear 
> Regression Analysis. New York, NY: John Wiley.
>
> * Simon, S. D., & Lesage, J. P. (1988, January). The impact of 
> collinearity involving the intercept term on the numerical accuracy of 
> regression. Computational Economics (formerly Computer Science in 
> Economics and Management), 1(2), 137-152.
>
> * Snee, R. D., & Marquardt, D. W. (1984, May). Comment: Collinearity 
> Diagnostics Depend on the Domain of Prediction, the Model, and the 
> Data. The American Statistician, 38(2), 83-87.
>
> * Weisberg, S. (1980). Applied Linear Regression. New York, NY: John 
> Wiley.
>
> * Wood, F. S. (1984, May). Comment: Effect of Centering on 
> Collinearity and Interpretation of the Constant. The American 
> Statistician, 38(2), 88-90.
>
>
>
> Christina Bogner schrieb:
>> Dear list members,
>>
>> I'm working with both nlme and lme4 packages trying to fit linear 
>> mixed-models to soil chemical and physical data. I know that for
>> linear models one can calculate the condition number kappa of the
>> model matrix to know whether the problem is well- or ill-conditioned.
>> Does it make any sense to compute kappa on the design matrix of the
>> fixed-effects in nlme or lme4? For comparison I fitted a simple
>> linear model to my data and scaling some numerical predictors
>> decreased kappa considerably. So I wonder if scaling them in the
>> mixed-model has any advantages?
>>
>> Thanks a lot for your help.
>>
>> Christina Bogner
>>
>


From christina.bogner at uni-bayreuth.de  Thu Feb  5 09:21:51 2009
From: christina.bogner at uni-bayreuth.de (Christina Bogner)
Date: Thu, 05 Feb 2009 09:21:51 +0100
Subject: [R-sig-ME] Mixed-models and condition number
In-Reply-To: <40e66e0b0902021249x11fe6e47u20f5c1e8864a1955@mail.gmail.com>
References: <498726A2.8040005@uni-bayreuth.de>
	<40e66e0b0902021249x11fe6e47u20f5c1e8864a1955@mail.gmail.com>
Message-ID: <498AA19F.2060805@uni-bayreuth.de>

Dear Douglas Bates,

Thank you for your response. I would appreciate if you could add a 
kappa-method to the lme4 package, please. It was indeed rather revealing 
to play around with different design matrices (mean-centered, scaled and 
different reference levels for factors).

Christina

Douglas Bates schrieb:
> On Mon, Feb 2, 2009 at 11:00 AM, Christina Bogner
> <christina.bogner at uni-bayreuth.de> wrote:
>   
>> Dear list members,
>>     
>
>   
>> I'm working with both nlme and lme4 packages trying to fit linear
>> mixed-models to soil chemical and physical data. I know that for linear
>> models one can calculate the condition number kappa of the model matrix to
>> know whether the problem is well- or ill-conditioned. Does it make any sense
>> to compute kappa on the design matrix of the fixed-effects in nlme or lme4?
>> For comparison I fitted a simple linear model to my data and scaling some
>> numerical predictors decreased kappa considerably. So I wonder if scaling
>> them in the mixed-model has any advantages?
>>     
>
> I'm not sure that checking the conditioning of only the fixed-effects
> model matrix is what you want to do.  In some ways it would be more
> informative to check the conditioning of the triangular matrix derived
> from the fixed-effects model matrix after removing the random effects.
>  That upper triangular matrix is stored as the RX slot.  Generally I
> don't recommend reaching in to a structure or an S4 object and pulling
> out a piece of it (because you can't count on those slots continuing
> to be there and to have the same names and contents) but in this case
> it may be the best way of going about things.
>
> If this is considered useful I could add a kappa method for the class.
>
>   


From Sebastiaan.DeSmedt at ua.ac.be  Thu Feb  5 11:35:50 2009
From: Sebastiaan.DeSmedt at ua.ac.be (De Smedt Sebastiaan)
Date: Thu, 5 Feb 2009 11:35:50 +0100
Subject: [R-sig-ME] Heteroscedasticity in lme4
Message-ID: <930B1A45F446404FA4D99A46F09209C4014154DE@xmail05.ad.ua.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090205/b1467387/attachment.pl>

From Herve.Chapuis at tours.inra.fr  Thu Feb  5 14:12:09 2009
From: Herve.Chapuis at tours.inra.fr (=?ISO-8859-1?Q?Herv=E9_CHAPUIS?=)
Date: Thu, 05 Feb 2009 14:12:09 +0100
Subject: [R-sig-ME] dealing with the convergence status
Message-ID: <498AE5A9.1010204@tours.inra.fr>

Hi everyone,

I am using lmer (and glmer) in a simulation study in order to find the 
best experimental design.
Using gaussian deviates I simulate a population and  I analyse the data 
with a mixed model.
I repeat these operations many times (about 5000 replicates)  in order  
to derive trends and distributions of parameters.

My problem is that  I sometimes run into false convergences. I have a 
warning message but that's all.
"Il y a eu 50 avis ou plus (utilisez warnings() pour voir les 50 premiers)"

Is there a way to obtain the convergence status of the lmer/glmer models 
in a tractable way so that I  do not account for this replicate in case 
of false  convergence ? Given the large number of replicates, I do not 
run the program interactively.

I hope I made it clear.
Many thanks.

-- 
Cordialement, 


Herv? CHAPUIS



From StefanKraemer1 at gmx.net  Thu Feb  5 17:25:05 2009
From: StefanKraemer1 at gmx.net (Stefan Kraemer)
Date: Thu, 05 Feb 2009 17:25:05 +0100
Subject: [R-sig-ME] basic question: Linear model
Message-ID: <20090205162505.90860@gmx.net>

Hey,
I try to understand a basic statistic question. I am modelling a linear regression Y=X*B+E. To compute the effect of ?group? the B-values of the regressors/columns that code the interaction effects (col. 5-8 and col. 11-14, see below) have to be weighted with non-zero elements within the contrast "Group 1" minus "Group 2" (see below). My first understanding was that the interaction effects add up to zero in each group. Why do they get non-zero contrast weights? Do you recommend any literature discussing this topic? 

Many Thanks,
Stefan

The columns of matrix X dummy-codes
1.	Data of Group1 (G1)
2.	Data of Group2 (G2)
3.	Data condition A1 (factor A)
4.	Data condition A2 (factor A)
5.	Data in cell A1G1
6.	Data in cell A2G1 
7.	Data in cell A1G2
8.	Data in cell A2G2
9.	Data condition B1 (factor B)
10.	Data condition B2 (factor B)
11.	Data in cell B1G1
12.	Data in cell B2G1
13.	Data in cell B1G2
14.	Data in cell B2G2
15.	Mean

Example:
#create X
dimnames = list( paste("sub",c(1:8)),
c("Group1","Group2","A1","A2","A1G1","A2G1", "A1G2","A2G2","B1","B2", "B1G1","B2G1", "B1G2","B2G2","mean" ))
X=matrix(nrow=8,ncol=15,dimnames=dimnames)
X[,1]= c(rep(1,4), rep(0,4))
X[,2]=c(rep(0,4),rep(1,4))
X[,3]=c(1,1,0,0,1,1,0,0);
X[,4]=c(0,0,1,1,0,0,1,1);
X[,5]=c(1,1,0,0,0,0,0,0);
X[,6]=c(0,0,1,1,0,0,0,0);
X[,7]=c(0,0,0,0,1,1,0,0);
X[,8]=c(0,0,0,0,0,0,1,1);
X[,9]=c(1, 0, 1, 0, 1, 0,1, 0);
X[,10]=c(0, 1, 0, 1, 0, 1, 0, 1);
X[,11]=c(1, 0, 1, 0,0, 0, 0, 0);
X[,12]=c( 0, 1, 0, 1,0, 0, 0, 0);
X[,13]=c( 0, 0, 0, 0,  1, 0, 1, 0)
X[,14]=c(0, 0, 0, 0,    0, 1, 0, 1)
X[,15]=c(1,1,1,1,1,1,1,1)
#y data
Y=c(1.2,5.1,1.7,7,6.1,8,1.1,2.5)

library(MASS)
ginv()
B=(ginv(t(X)%*%X) %*% t(X)) %*% Y

#contrast
c1=rep(0,15)
c1[1]=1
c1[c(5,6,11,12)]=1/2
c2=rep(0,15)
c2[2]=-1
c2[c(7,8,13,14)]=-1/2

#Group effect
mean(Y[1:4])-mean(Y[5:8])
c1%*%B+c2%*%B

-- 


-- 
Jetzt 1 Monat kostenlos! GMX FreeDSL - Telefonanschluss + DSL 
f?r nur 17,95 Euro/mtl.!* http://dsl.gmx.de/?ac=OM.AD.PD003K11308T4569a



From bolker at ufl.edu  Thu Feb  5 18:19:57 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 05 Feb 2009 12:19:57 -0500
Subject: [R-sig-ME] dealing with the convergence status
Message-ID: <498B1FBD.6090506@ufl.edu>


  I'm interested in the answer as well.  I thought about this
briefly,  but the only thing I came up with (which is a terrible
hack) is to use options(warn=2) to turn warnings into errors and
then use try() to catch and evaluate the errors.  One might
also be able to inspect last.warning ... There must
be a better way though ...

  Ben Bolker

Herv? CHAPUIS wrote:
> Hi everyone,
> 
> I am using lmer (and glmer) in a simulation study in order to find the 
> best experimental design.
> Using gaussian deviates I simulate a population and  I analyse the data 
> with a mixed model.
> I repeat these operations many times (about 5000 replicates)  in order  
> to derive trends and distributions of parameters.
> 
> My problem is that  I sometimes run into false convergences. I have a 
> warning message but that's all.
> "Il y a eu 50 avis ou plus (utilisez warnings() pour voir les 50 premiers)"
> 
> Is there a way to obtain the convergence status of the lmer/glmer models 
> in a tractable way so that I  do not account for this replicate in case 
> of false  convergence ? Given the large number of replicates, I do not 
> run the program interactively.
> 
> I hope I made it clear.
> Many thanks.
> 


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From Stephan.Kolassa at gmx.de  Thu Feb  5 11:27:58 2009
From: Stephan.Kolassa at gmx.de (Stephan Kolassa)
Date: Thu, 05 Feb 2009 11:27:58 +0100
Subject: [R-sig-ME] Mixed-models and condition number
In-Reply-To: <498A9F81.9030508@uni-bayreuth.de>
References: <498726A2.8040005@uni-bayreuth.de> <49875570.6000902@gmx.de>
	<498A9F81.9030508@uni-bayreuth.de>
Message-ID: <498ABF2E.8020103@gmx.de>

Hi Christina,

if centering and/or scaling the columns of design matrices reduces 
kappa, that's nice and dandy - but of course the question remains what 
this means for the stability of your linear system, which is what you 
are really interested in.

An aside: kappa depends on the matrix norm you are using, and all 
sensitivity and other results will then be expressed in a compatible 
vector norm. The 2-norm is the most widely used one (and kappa_2 can be 
easily calculated as the ratio of the largest to the smallest 
eigenvalue), but both the 1-norm and the \infty-norm may be more useful, 
depending on your application and interest.

Getting a mathematician interested in your problem sounds like a great 
idea! Looking at uni-bayreuth.de, I guess the engineering mathematicians 
are your best bet:
http://www.ingenieurmathematik.uni-bayreuth.de/forschung.html
Mathe V seems to be more focused on dynamical systems, this may not be 
their core interest.

Good luck!
Stephan


Christina Bogner schrieb:
> Dear Stephan,
> 
> thank your very much for your response and the detailed list of 
> literature. I knew about Belsley (1991b) and used it on the design 
> matrix of the fixed-effects. My (absolutely empirical) results were the 
> following:
> 
> on  a small data set of 58 values and the mixed-effects model:
> mymodel=lme(log(calcium) ~ soil.horizon+flow.region+content.of.silt, 
> data=mydata, random=~1|plot)
> with soil.horizon and flow.region: factors
> content.of.silt: continuous covariate
> 
> 1. mean-centering the continuous covariate decreased the collinearity 
> between the intercept term and the continuous covariate 
> (summary.lme$corFixed and Belsley 1991b on the design matrix of 
> fixed-effects) and decreased kappa (of the design matrix of 
> fixed-effects) by factor 12.
> 2. scaling the covariate to obtain fixed-effects estimates of comparable 
> size decreased kappa by factor 4, but had no effect on correlation of 
> the fixed-effects.
> 3. I compared kappas of the mixed-effects design matrix and as proposed 
> by Douglas Bates of the "triangular matrix derived from the 
> fixed-effects model matrix after removing the random effects" in lme4: 
> the influence of mean-centering and scaling on kappa was comparable and 
> values of kappas of the triangular matrix and the design matrix of the 
> fixed-effects differed little for mean-centered and scaled model, but 
> largely for the non-scaled and non-centered one.
> 
> I will try to find a mathematician at my university who would like to 
> play around with mixed-models  ;-).
> 
> Thanks again
> 
> Christina
> 
> 
> Stephan Kolassa schrieb:
>> Hi Christina,
>> let me start by saying that I don't know of anyone looking at
>> conditioning of design matrices in a mixed model environment. Might be a
>> nice topic to have an M. Sc. student play around with empirically. The 
>> problem with ill-conditioning in fixed-effects models basically comes 
>> down to high variances in the parameter estimates, so one could 
>> actually build a mixed model with an ill-conditioned design matrix and 
>> play around with small changes to simulated observations, checking 
>> whether inferences or estimates exhibit "large" variance.
>>
>> If you find out anything about this, would you let me know?
>>
>> That said, my recent interest has been in collinearity between
>> predictors, which is not exactly conditioning, but reasonably close to
>> it. I'd recommend you look at Hill & Adkins (2001) and the collinearity
>> diagnostics they recommend. Belsley (1991a) wrote an entire monograph
>> about them, but there are also shorter introductions, e.g., Belsley 
>> (1991b).
>>
>> Scaling the columns of X to equal euclidean length (usually to length 1)
>> before diagnosing collinearity appears to be accepted procedure, so I 
>> think scaling would be a good starting point in the mixed model, too. 
>> However, there is a discussion as to whether to first remove the
>> constant column from X and subtract the column mean from each of the 
>> remaining columns.
>>
>> Marquardt (1980) claims that centering removes "nonessential ill
>> conditioning." Weisberg (1980) and Montgomery and Peck (1982) also 
>> advocate centering.
>>
>> Other practitioners maintain that centering removes meaningful
>> information from X, such as collinearity with the constant column, and 
>> should not be used (Belsey et al., 1980; Belsley, 1984a, 1984b, 1986, 
>> 1991a, 1991b; Echambadi & Hess, 2007; Hill & Adkins, 2001). For 
>> example, Simon and Lesage (1988) found that collinearity with the 
>> constant
>> column introduces numerical instability, which is mitigated but not 
>> prevented by employing collinearity diagnostics after centering X. In 
>> addition, these problems are not confined to the constant coefficient, 
>> but extend to all estimates.
>>
>> For a very lively debate on this topic see Belsley (1984a); Cook (1984);
>> Gunst (1984); Snee and Marquardt (1984); Wood (1984); Belsley (1984b). 
>> The consensus seems to be that centering cannot be once and for all be 
>> advised or rejected; rather, whether or not to center data depends on 
>> the problem one is facing.
>>
>> HTH,
>> Stephan
>>
>>
>> * Belsey, D. A., Kuh, E., & Welsch, R. E. (1980). Regression 
>> Diagnostics: Identifying Influential Data and Sources of Collinearity. 
>> New York, NY: John Wiley & Sons.
>>
>> * Belsley, D. A. (1984a, May). Demeaning Conditioning Diagnostics 
>> through Centering. The American Statistician, 38(2), 73-77.
>>
>> * Belsley, D. A. (1984b, May). Demeaning Conditioning Diagnostics 
>> through Centering: Reply. The American Statistician, 38(2), 90-93.
>>
>> * Belsley, D. A. (1986). Centering, the constant, first-differencing, 
>> and assessing conditioning. In E. Kuh & D. A. Belsley (Eds.), Model 
>> Reliability (p. 117-153). Cambridge: MIT Press.
>>
>> * Belsley, D. A. (1987). Collinearity and Least Squares Regression: 
>> Comment -- Well-Conditioned Collinearity Indices. Statistical Science, 
>> 2(1), 86-91. Available from http://projecteuclid.org/euclid.ss/1177013441
>>
>> * Belsley, D. A. (1991a). Conditioning Diagnostics: Collinearity and 
>> Weak Data in Regression. New York, NY: Wiley.
>>
>> * Belsley, D. A. (1991b, February). A Guide to using the collinearity 
>> diagnostics. Computational Economics, 4(1), 33-50. Available from 
>> http://www.springerlink.com/content/v135h6631x412kk8/
>>
>> * Cook, R. D. (1984, May). Demeaning Conditioning Diagnostics through 
>> Centering: Comment. The American Statistician, 38(2), 78-79.
>>
>> * Echambadi, R., & Hess, J. D. (2007, May-June). Mean-Centering Does 
>> Not Alleviate Collinearity Problems in Moderated Multiple Regression 
>> Models. Marketing Science, 26(3), 438-445.
>>
>> * Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (3rd 
>> ed.). Baltimore: Johns Hopkins University Press.
>>
>> * Gunst, R. F. (1984, May). Comment: Toward a Balanced Assessment of 
>> Collinearity Diagnostics. The American Statistician, 38(2), 79-82.
>>
>> * Hill, R. C., & Adkins, L. C. (2001). Collinearity. In B. H. Baltagi 
>> (Ed.), A Companion to Theoretical Econometrics (p. 256-278). Oxford: 
>> Blackwell.
>>
>> * Marquardt, D. W. (1987). Collinearity and Least Squares Regression: 
>> Comment. Statistical Science, 2(1), 84-85. Available from 
>> http://projecteuclid.org/euclid.ss/1177013440
>>
>> * Montgomery, D. C., & Peck, E. A. (1982). Introduction to Linear 
>> Regression Analysis. New York, NY: John Wiley.
>>
>> * Simon, S. D., & Lesage, J. P. (1988, January). The impact of 
>> collinearity involving the intercept term on the numerical accuracy of 
>> regression. Computational Economics (formerly Computer Science in 
>> Economics and Management), 1(2), 137-152.
>>
>> * Snee, R. D., & Marquardt, D. W. (1984, May). Comment: Collinearity 
>> Diagnostics Depend on the Domain of Prediction, the Model, and the 
>> Data. The American Statistician, 38(2), 83-87.
>>
>> * Weisberg, S. (1980). Applied Linear Regression. New York, NY: John 
>> Wiley.
>>
>> * Wood, F. S. (1984, May). Comment: Effect of Centering on 
>> Collinearity and Interpretation of the Constant. The American 
>> Statistician, 38(2), 88-90.
>>
>>
>>
>> Christina Bogner schrieb:
>>> Dear list members,
>>>
>>> I'm working with both nlme and lme4 packages trying to fit linear 
>>> mixed-models to soil chemical and physical data. I know that for
>>> linear models one can calculate the condition number kappa of the
>>> model matrix to know whether the problem is well- or ill-conditioned.
>>> Does it make any sense to compute kappa on the design matrix of the
>>> fixed-effects in nlme or lme4? For comparison I fitted a simple
>>> linear model to my data and scaling some numerical predictors
>>> decreased kappa considerably. So I wonder if scaling them in the
>>> mixed-model has any advantages?
>>>
>>> Thanks a lot for your help.
>>>
>>> Christina Bogner
>>>
>>
>



From evansj18 at msu.edu  Thu Feb  5 19:48:08 2009
From: evansj18 at msu.edu (Jeff Evans)
Date: Thu, 5 Feb 2009 13:48:08 -0500
Subject: [R-sig-ME] inference for random effects
Message-ID: <392F001D52E34A10868571F30AB625DD@myelin>

I'm sure this must have been discussed before, but in searching the archives
I haven't found an answer yet. 

Simple question:

In lme4 can I evaluate the significance of a random effect in a model by
substituting an uninformative dummy variable for it and comparing it to the
model with the "real" random effect using anova? 

M1 = lmer(cbind(successes, total-successes) ~ A * B + (1|C), data=dat,
family="binomial")

M2 = lmer(cbind(successes, total-successes) ~ A * B + (1|Cdummy) , data=dat,
family="binomial")

anova(M1,M2)

Where A, B, and C are factors, and Cdummy is a column with the word "dummy"
in every row.

Then compare the AIC, subtracting 2 from the M2 AIC score since it "falsely"
estimated a parameter for the random effect. When I do this, I get delta AIC
of about 600 favoring the more informative M1. Is this approach
fundamentally wrong? 


Thanks,

Jeff Evans
Michigan State University



From steibelj at msu.edu  Thu Feb  5 20:38:16 2009
From: steibelj at msu.edu (Juan Pedro Steibel)
Date: Thu, 05 Feb 2009 14:38:16 -0500
Subject: [R-sig-ME] inference for random effects
In-Reply-To: <392F001D52E34A10868571F30AB625DD@myelin>
References: <392F001D52E34A10868571F30AB625DD@myelin>
Message-ID: <498B4028.208@msu.edu>

Jeff,
Why not use the model without the random effect as the null model?
JP

Jeff Evans wrote:
> I'm sure this must have been discussed before, but in searching the archives
> I haven't found an answer yet. 
>
> Simple question:
>
> In lme4 can I evaluate the significance of a random effect in a model by
> substituting an uninformative dummy variable for it and comparing it to the
> model with the "real" random effect using anova? 
>
> M1 = lmer(cbind(successes, total-successes) ~ A * B + (1|C), data=dat,
> family="binomial")
>
> M2 = lmer(cbind(successes, total-successes) ~ A * B + (1|Cdummy) , data=dat,
> family="binomial")
>
> anova(M1,M2)
>
> Where A, B, and C are factors, and Cdummy is a column with the word "dummy"
> in every row.
>
> Then compare the AIC, subtracting 2 from the M2 AIC score since it "falsely"
> estimated a parameter for the random effect. When I do this, I get delta AIC
> of about 600 favoring the more informative M1. Is this approach
> fundamentally wrong? 
>
>
> Thanks,
>
> Jeff Evans
> Michigan State University
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>   


-- 
=============================
Juan Pedro Steibel

Assistant Professor
Statistical Genetics and Genomics

Department of Animal Science & 
Department of Fisheries and Wildlife

Michigan State University
1205-I Anthony Hall
East Lansing, MI
48824 USA 

Phone: 1-517-353-5102
E-mail: steibelj at msu.edu



From bolker at ufl.edu  Thu Feb  5 20:44:04 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 05 Feb 2009 14:44:04 -0500
Subject: [R-sig-ME] inference for random effects
In-Reply-To: <498B4028.208@msu.edu>
References: <392F001D52E34A10868571F30AB625DD@myelin> <498B4028.208@msu.edu>
Message-ID: <498B4184.4060700@ufl.edu>

Juan Pedro Steibel wrote:
> Jeff,
> Why not use the model without the random effect as the null model?
> JP

  Because the model without the random effect can't be fitted by lmer
(and there is no equivalent of nlme::gls in the lme4 package, which
handles the same syntax but without random effects), and as it turns
out glm calculates the likelihood differently so that they are not
comparable, so that

glm(cbind(successes, total-successes) ~ A * B  data=dat,
           family="binomial")

 would not give an appropriate comparison.

  Jeff's approach is clever, if I (or someone else) gets a
chance it would be nice to compare it against the approach
discussed in http://glmm.wikidot.com/reef-fish , which uses
a modification of some of the code in one of the lme4 vignettes
to compute a likelihood profile for the random effects variance
(including at V=0, which is the likelihood we're interested in
here).

  Ben Bolker

> 
> Jeff Evans wrote:
>> I'm sure this must have been discussed before, but in searching the archives
>> I haven't found an answer yet. 
>>
>> Simple question:
>>
>> In lme4 can I evaluate the significance of a random effect in a model by
>> substituting an uninformative dummy variable for it and comparing it to the
>> model with the "real" random effect using anova? 
>>
>> M1 = lmer(cbind(successes, total-successes) ~ A * B + (1|C), data=dat,
>> family="binomial")
>>
>> M2 = lmer(cbind(successes, total-successes) ~ A * B + (1|Cdummy) , data=dat,
>> family="binomial")
>>
>> anova(M1,M2)
>>
>> Where A, B, and C are factors, and Cdummy is a column with the word "dummy"
>> in every row.
>>
>> Then compare the AIC, subtracting 2 from the M2 AIC score since it "falsely"
>> estimated a parameter for the random effect. When I do this, I get delta AIC
>> of about 600 favoring the more informative M1. Is this approach
>> fundamentally wrong? 
>>
>>
>> Thanks,
>>
>> Jeff Evans
>> Michigan State University
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>   
> 
> 


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From evansj18 at msu.edu  Thu Feb  5 20:44:23 2009
From: evansj18 at msu.edu (Jeff Evans)
Date: Thu, 5 Feb 2009 14:44:23 -0500
Subject: [R-sig-ME] inference for random effects
In-Reply-To: <498B4028.208@msu.edu>
References: <392F001D52E34A10868571F30AB625DD@myelin> <498B4028.208@msu.edu>
Message-ID: <A3A2FF7594C849B299DF57EFCE226F7B@myelin>

Thanks Juan,

I would have done this, but lmer and glmer won't run without a random
effects term. So I thought that maybe I could trick it.

-----Original Message-----
From: Juan Pedro Steibel [mailto:steibelj at msu.edu] 
Sent: Thursday, February 05, 2009 2:38 PM
To: Jeff Evans
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] inference for random effects

Jeff,
Why not use the model without the random effect as the null model?
JP

Jeff Evans wrote:
> I'm sure this must have been discussed before, but in searching the
archives
> I haven't found an answer yet. 
>
> Simple question:
>
> In lme4 can I evaluate the significance of a random effect in a model by
> substituting an uninformative dummy variable for it and comparing it to
the
> model with the "real" random effect using anova? 
>
> M1 = lmer(cbind(successes, total-successes) ~ A * B + (1|C), data=dat,
> family="binomial")
>
> M2 = lmer(cbind(successes, total-successes) ~ A * B + (1|Cdummy) ,
data=dat,
> family="binomial")
>
> anova(M1,M2)
>
> Where A, B, and C are factors, and Cdummy is a column with the word
"dummy"
> in every row.
>
> Then compare the AIC, subtracting 2 from the M2 AIC score since it
"falsely"
> estimated a parameter for the random effect. When I do this, I get delta
AIC
> of about 600 favoring the more informative M1. Is this approach
> fundamentally wrong? 
>
>
> Thanks,
>
> Jeff Evans
> Michigan State University
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>   


-- 
=============================
Juan Pedro Steibel

Assistant Professor
Statistical Genetics and Genomics

Department of Animal Science & 
Department of Fisheries and Wildlife

Michigan State University
1205-I Anthony Hall
East Lansing, MI
48824 USA 

Phone: 1-517-353-5102
E-mail: steibelj at msu.edu



From evansj18 at msu.edu  Thu Feb  5 21:32:04 2009
From: evansj18 at msu.edu (Jeff Evans)
Date: Thu, 5 Feb 2009 15:32:04 -0500
Subject: [R-sig-ME] inference for random effects
In-Reply-To: <498B4184.4060700@ufl.edu>
References: <392F001D52E34A10868571F30AB625DD@myelin> <498B4028.208@msu.edu>
	<498B4184.4060700@ufl.edu>
Message-ID: <834F4206AC8B422DBBE25408A3A8399A@myelin>

Hi Ben,

I've just tryied to run the simulation approach you posted on glmm.wikidot,
but all the values returned in sim1 are NaN. I think my recoding parallels
your example, but maybe you'd best have a look. You ran your simulation from
the glm object, then refit the full glmer model from the simulation right? 

Also, the file where the function zerodev is said to be (glmmprof.r) on the
wiki page isn't there, though I found it in glmm_Stier.Rnw.

Here's the code I've run so far:

library(lme4)
library(emdbook)
source("C:/Users/Jeff/Documents/Jeffs Work/Analytic
Tools/R_Resources/Bolker/glmmfuns.r")

# Full Model
fm1 = glmer(cbind(SdlFinal, SdlMax-SdlFinal)~ State * Year  +
    (1|Site),
    data = dat.gm,
    family = "binomial")

# Reduced Model
rm1 = glm(cbind(SdlFinal, SdlMax-SdlFinal) ~ State * State,
    data = dat.gm,
    family = "binomial")

zerodev <- function(mm) {
  varprof(mm,0,0,1)[["ML"]]
}


svals <- simulate(rm1, nsim = 1000)
t0 <- system.time(sim1 <- apply(svals, 2, function(x) {
  r <- refit(fm1, x)
  zerodev(r) - deviance(r)
  }))
save("svals", "t0", "sim1", file = "glmersim1.RData")

> str(sim1)
 Named num [1:1000] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...
 - attr(*, "names")= chr [1:1000] "V1" "V2" "V3" "V4" ... 


Jeff

#############################################################


-----Original Message-----
From: Ben Bolker [mailto:bolker at ufl.edu] 
Sent: Thursday, February 05, 2009 2:44 PM
To: Juan Pedro Steibel
Cc: Jeff Evans; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] inference for random effects



Juan Pedro Steibel wrote:
> Jeff,
> Why not use the model without the random effect as the null model?
> JP

  Because the model without the random effect can't be fitted by lmer
(and there is no equivalent of nlme::gls in the lme4 package, which
handles the same syntax but without random effects), and as it turns
out glm calculates the likelihood differently so that they are not
comparable, so that

glm(cbind(successes, total-successes) ~ A * B  data=dat,
           family="binomial")

 would not give an appropriate comparison.

  Jeff's approach is clever, if I (or someone else) gets a
chance it would be nice to compare it against the approach
discussed in http://glmm.wikidot.com/reef-fish , which uses
a modification of some of the code in one of the lme4 vignettes
to compute a likelihood profile for the random effects variance
(including at V=0, which is the likelihood we're interested in
here).

  Ben Bolker

> 
> Jeff Evans wrote:
>> I'm sure this must have been discussed before, but in searching the
archives
>> I haven't found an answer yet. 
>>
>> Simple question:
>>
>> In lme4 can I evaluate the significance of a random effect in a model by
>> substituting an uninformative dummy variable for it and comparing it to
the
>> model with the "real" random effect using anova? 
>>
>> M1 = lmer(cbind(successes, total-successes) ~ A * B + (1|C), data=dat,
>> family="binomial")
>>
>> M2 = lmer(cbind(successes, total-successes) ~ A * B + (1|Cdummy) ,
data=dat,
>> family="binomial")
>>
>> anova(M1,M2)
>>
>> Where A, B, and C are factors, and Cdummy is a column with the word
"dummy"
>> in every row.
>>
>> Then compare the AIC, subtracting 2 from the M2 AIC score since it
"falsely"
>> estimated a parameter for the random effect. When I do this, I get delta
AIC
>> of about 600 favoring the more informative M1. Is this approach
>> fundamentally wrong? 
>>
>>
>> Thanks,
>>
>> Jeff Evans
>> Michigan State University
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>   
> 
> 


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From danielezrajohnson at gmail.com  Fri Feb  6 01:31:57 2009
From: danielezrajohnson at gmail.com (Daniel Ezra Johnson)
Date: Thu, 5 Feb 2009 19:31:57 -0500
Subject: [R-sig-ME] dealing with the convergence status
In-Reply-To: <498B1FBD.6090506@ufl.edu>
References: <498B1FBD.6090506@ufl.edu>
Message-ID: <a46630750902051631w619acb3fr5f0b1be74de9c71@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090205/5457e8b0/attachment.pl>

From christina.bogner at uni-bayreuth.de  Fri Feb  6 08:04:33 2009
From: christina.bogner at uni-bayreuth.de (Christina Bogner)
Date: Fri, 06 Feb 2009 08:04:33 +0100
Subject: [R-sig-ME] inference for random effects
Message-ID: <498BE101.1070408@uni-bayreuth.de>

> Message: 5
> Date: Thu, 5 Feb 2009 13:48:08 -0500
> From: "Jeff Evans" <evansj18 at msu.edu>
> Subject: [R-sig-ME] inference for random effects
> To: <r-sig-mixed-models at r-project.org>
> Message-ID: <392F001D52E34A10868571F30AB625DD at myelin>
> Content-Type: text/plain;	charset="us-ascii"
> 
> I'm sure this must have been discussed before, but in searching the archives
> I haven't found an answer yet. 
> 
> Simple question:
> 
> In lme4 can I evaluate the significance of a random effect in a model by
> substituting an uninformative dummy variable for it and comparing it to the
> model with the "real" random effect using anova? 
> 
> M1 = lmer(cbind(successes, total-successes) ~ A * B + (1|C), data=dat,
> family="binomial")
> 
> M2 = lmer(cbind(successes, total-successes) ~ A * B + (1|Cdummy) , data=dat,
> family="binomial")
> 
> anova(M1,M2)
> 
> Where A, B, and C are factors, and Cdummy is a column with the word "dummy"
> in every row.
> 
> Then compare the AIC, subtracting 2 from the M2 AIC score since it "falsely"
> estimated a parameter for the random effect. When I do this, I get delta AIC
> of about 600 favoring the more informative M1. Is this approach
> fundamentally wrong? 
> 
> 
> Thanks,
> 
> Jeff Evans
> Michigan State University

Hello Jeff,

for lme and lmer models there is a simulation package RLRsim by Fabian 
Scheipl "for testing the presence of variance components" that might be 
helpful.

Christina



From jenny at stat.ubc.ca  Fri Feb  6 22:54:47 2009
From: jenny at stat.ubc.ca (Jenny Bryan)
Date: Fri, 6 Feb 2009 13:54:47 -0800
Subject: [R-sig-ME] logistic growth,
	vexing choice of which timepoints to include
Message-ID: <4B7E17CD-C967-479F-85CD-97404D975969@stat.ubc.ca>

Hello.  I'm looking for advice on how to make a seemingly unavoidable  
subjective choice in an analysis I'm doing, using the logistic growth  
model.  I'm using nlme, but that has nothing to do with my question,  
so I hope it's not too inappropriate for me to post this here.   
Reading the list archive suggests that it's not too hard to tempt this  
group into philosophical discussions :-)

I have growth data that can be reasonably modelled with a four- 
parameter logistic curve.  The experimental unit is a well in a  
microtitre plate and I get light absorbance readings over time that  
reflect cell density.  There are many wells on a plate, e.g. 96 or  
384, and experiments often span many plates.  Systematic differences  
between the wells can be, for example, specific genetic mutations  
carried by the cells and/or different chemicals added to the growth  
medium.  I am mostly interested in performing inference on the fixed  
effects, i.e. how the genetic perturbations, the chemicals, or their  
interactions, modify key growth parameters, especially the one  
inversely related to the underlying exponential growth rate we'd see  
in the absence of resource constraints (phi_4 in Pinheiro & Bates p.  
517).

Problem:  The number of cells inoculated into the wells at the start  
is quite small -- well below the detection threshhold for the light  
absorbance readings.  Therefore, each timecourse begins with a loooong  
string of zeros, before the classic sigmoidal shape kicks in.  And, of  
course, the timing of this happy event is both ill-defined and very  
variable across the wells.

For figure-making purposes, I removed some early timepoints that were  
uniformly zero for all wells.  Which made me wonder: why couldn't  
(shouldn't?) I do the same prior to model fitting?  When I fit the  
logistic growth model with and without these early timepoints, I get  
essentially the same estimated fixed effects and, even, estimated  
variances for the random effects.  But there *is* a substantial  
difference in the estimate of residual variance, which then obviously  
has a noticeable effect on the inference for the fixed effects and,  
especially, the one I care about.  Including all the timepoints drives  
the residual variance down, as you might expect.  But that almost  
seems misleading or artificial ... other collaborators I work with  
don't even start taking OD readings until the first 12 hours have  
passed, which makes their initial strings of zeros quite short,  
which ... gives them less statistical significance for the same  
observed effect size?!?

Does anyone have a comment or advice?

Thanks in advance for reading this,
Jenny

Jennifer Bryan
Department of Statistics and
   the Michael Smith Laboratories
University of British Columbia



From smckinney at bccrc.ca  Sat Feb  7 00:18:53 2009
From: smckinney at bccrc.ca (Steven McKinney)
Date: Fri, 6 Feb 2009 15:18:53 -0800
Subject: [R-sig-ME] logistic growth,
	vexing choice of which timepoints to include
References: <8531_1233958312_1233958312_4B7E17CD-C967-479F-85CD-97404D975969@stat.ubc.ca>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0328A5AC@crcmail1.BCCRC.CA>

Hi Jenny,

[Caveat: Comments from an applied statistician, not
a world-heavyweight likelihood theorist]

In the logistic world a zero value maps to a 
minus infinity value.  It seems to me that only
the 'last' zero value contains any information
relevant to the likelihood (equivalently only
the 'first' one value (plus infinity in the
logistic realm) contains any information
relevant to the likelihood).  Perhaps the
coding for the likelihood has not been set
up to take this into account so you are getting
the artificial contribution of the rest of the
zero values folded into the likelihood, artificially
deflating the variance estimates.

I would exclude or set to NA all but the last
(or even all of) the zero values for any well
and all but the first (or even all of) the one 
values.

The zero values are really below the detection
limit of the sensor involved so should theoretically
be handled as truncated data but that's another
level of complexity for the analysis.

Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney +at+ bccrc +dot+ ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C. 
V5Z 1L3
Canada




-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org on behalf of Jenny Bryan
Sent: Fri 2/6/2009 1:54 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] logistic growth,vexing choice of which timepoints to include
 
Hello.  I'm looking for advice on how to make a seemingly unavoidable  
subjective choice in an analysis I'm doing, using the logistic growth  
model.  I'm using nlme, but that has nothing to do with my question,  
so I hope it's not too inappropriate for me to post this here.   
Reading the list archive suggests that it's not too hard to tempt this  
group into philosophical discussions :-)

I have growth data that can be reasonably modelled with a four- 
parameter logistic curve.  The experimental unit is a well in a  
microtitre plate and I get light absorbance readings over time that  
reflect cell density.  There are many wells on a plate, e.g. 96 or  
384, and experiments often span many plates.  Systematic differences  
between the wells can be, for example, specific genetic mutations  
carried by the cells and/or different chemicals added to the growth  
medium.  I am mostly interested in performing inference on the fixed  
effects, i.e. how the genetic perturbations, the chemicals, or their  
interactions, modify key growth parameters, especially the one  
inversely related to the underlying exponential growth rate we'd see  
in the absence of resource constraints (phi_4 in Pinheiro & Bates p.  
517).

Problem:  The number of cells inoculated into the wells at the start  
is quite small -- well below the detection threshhold for the light  
absorbance readings.  Therefore, each timecourse begins with a loooong  
string of zeros, before the classic sigmoidal shape kicks in.  And, of  
course, the timing of this happy event is both ill-defined and very  
variable across the wells.

For figure-making purposes, I removed some early timepoints that were  
uniformly zero for all wells.  Which made me wonder: why couldn't  
(shouldn't?) I do the same prior to model fitting?  When I fit the  
logistic growth model with and without these early timepoints, I get  
essentially the same estimated fixed effects and, even, estimated  
variances for the random effects.  But there *is* a substantial  
difference in the estimate of residual variance, which then obviously  
has a noticeable effect on the inference for the fixed effects and,  
especially, the one I care about.  Including all the timepoints drives  
the residual variance down, as you might expect.  But that almost  
seems misleading or artificial ... other collaborators I work with  
don't even start taking OD readings until the first 12 hours have  
passed, which makes their initial strings of zeros quite short,  
which ... gives them less statistical significance for the same  
observed effect size?!?

Does anyone have a comment or advice?

Thanks in advance for reading this,
Jenny

Jennifer Bryan
Department of Statistics and
   the Michael Smith Laboratories
University of British Columbia

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From gangchen6 at gmail.com  Sat Feb  7 00:21:24 2009
From: gangchen6 at gmail.com (Gang Chen)
Date: Fri, 6 Feb 2009 18:21:24 -0500
Subject: [R-sig-ME] Analysis with standard deviations
Message-ID: <b0f0ada60902061521l21f355edhe84abb5bdc7b0d7d@mail.gmail.com>

Let me use the Machines data that comes with nlme package as an
example to describe the situation I'm facing:

> library(nlme)
> str(Machines)
Classes 'nffGroupedData', 'nfGroupedData', 'groupedData' and
'data.frame':      54 obs. of  3 variables:
 $ Worker : Ord.factor w/ 6 levels "6"<"2"<"4"<"1"<..: 4 4 4 2 2 2 5 5 5 3 ...
 $ Machine: Factor w/ 3 levels "A","B","C": 1 1 1 1 1 1 1 1 1 1 ...
 $ score  : num  52 52.8 53.1 51.8 52.8 53.1 60 60.2 58.4 51.1 ...

Suppose I want to model the data with the following

Y_ijk = a_j + b_i + b_ij + e_ijk, i (worker) = 1,...,6, j (machine) =
1,...,3, k (sample repetitions) = 1,...,3

> lme(score~Machine, data=Machines, random=~1|Worker/Machine)

And suppose I don't have the whole data set Machines. Instead what I
have are Y_ij. (dot here means the score average across the k index:
those repeated samples for each (i,j)), and its standard deviation. So
my real data is myMachines as below:

> tmp1 <- tapply(Machines$score, Machines[c("Worker", "Machine")], mean)   # mean
> tmp2 <- tapply(Machines$score, Machines[c("Worker", "Machine")], sd)        # standard deviation
> myMachines <- data.frame(expand.grid(dimnames(tmp1)), matrix(c(unlist(tmp1), unlist(tmp2)), byrow=FALSE,ncol=2))
> names(myMachines) <- c("Worker", "Machine", "myScore", "sd")

My question is, how can I analyze myMachines with lme or lmer? In
other words, is there a way to take the standard deviations into
consideration when modeling?

Thanks in advance,
Gang



From bates at stat.wisc.edu  Sat Feb  7 15:33:59 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 7 Feb 2009 08:33:59 -0600
Subject: [R-sig-ME] Lack of replies from me
Message-ID: <40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be@mail.gmail.com>

I regret that I have been absent from the list for some time.  It
happens that in the last couple of weeks I have been involved in a
series of extremely unpleasant interactions on another, private email
list that have left me with little enthusiasm for developing and
supporting CRAN packages.  One conclusion I have reached from these
interactions is that I will never allow lme4 to be a recommended
package in R.  I am even having doubts about whether I want it to
continue to be a CRAN package at all, as opposed to, say, moving it to
Bioconductor or even switching development to another language.

I'm sure the last option would be "cutting off my nose to spite my
face" and I don't expect I would ever do that.  There are many
wonderful aspects to R and many reasons why I want to continue to use
it.  But right now I find myself forced to evaluate options other than
putting a package on CRAN.



From A.Robinson at ms.unimelb.edu.au  Sat Feb  7 21:13:06 2009
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 8 Feb 2009 07:13:06 +1100
Subject: [R-sig-ME] Lack of replies from me
In-Reply-To: <40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be@mail.gmail.com>
References: <40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be@mail.gmail.com>
Message-ID: <20090207201306.GP51827@ms.unimelb.edu.au>

On Sat, Feb 07, 2009 at 08:33:59AM -0600, Douglas Bates wrote:
> I regret that I have been absent from the list for some time.  It
> happens that in the last couple of weeks I have been involved in a
> series of extremely unpleasant interactions on another, private email
> list that have left me with little enthusiasm for developing and
> supporting CRAN packages.  One conclusion I have reached from these
> interactions is that I will never allow lme4 to be a recommended
> package in R.  I am even having doubts about whether I want it to
> continue to be a CRAN package at all, as opposed to, say, moving it to
> Bioconductor or even switching development to another language.
> 
> I'm sure the last option would be "cutting off my nose to spite my
> face" and I don't expect I would ever do that.  There are many
> wonderful aspects to R and many reasons why I want to continue to use
> it.  But right now I find myself forced to evaluate options other than
> putting a package on CRAN.

I'm very sorry to hear about your frustrations, Doug.  I'm sure that I
speak for all of us when I say that we'll continue to use and support
lme4 regardless of the delivery mechanism.

Warm wishes

Andrew
-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-6410
University of Melbourne, VIC 3010 Australia               (prefer email)
http://www.ms.unimelb.edu.au/~andrewpr              Fax: +61-3-8344-4599
http://blogs.mbs.edu/fishing-in-the-bay/



From nikko at hailmail.net  Sat Feb  7 23:16:56 2009
From: nikko at hailmail.net (Nicholas Lewin-Koh)
Date: Sat, 07 Feb 2009 14:16:56 -0800
Subject: [R-sig-ME] logistic growth, vexing choice of which timepoints
In-Reply-To: <mailman.5.1234004401.12695.r-sig-mixed-models@r-project.org>
References: <mailman.5.1234004401.12695.r-sig-mixed-models@r-project.org>
Message-ID: <1234045016.6848.1299107865@webmail.messagingengine.com>

Hi Jenny
What Steven said below is true, the zeros are below the detection limit.
However,
one might ask if the time until the populations cross the detection
threshold matters?
For instance if two wells treated differently both have similar logistic
curves, but one 
starts accelerating at t(i) and the other at t(j), j > i that does
provide some information about
what is going on below the detection limit. A sophisticated approach
might be to fit a joint model
modeling the time to the event, and the logistic growth simultaneously.
Given that that is 
hard, and there may not be any software to do it, you might want to fit
the survival model (time to event)
and then the logistic growth model n the non-zero data. This is very
add-hoc, but will at least give you
some idea of whether it is worth chasing a more complicated model. This
will be more effective if 
you have replicate wells. 

Nicholas

> Message: 1
> Date: Fri, 6 Feb 2009 13:54:47 -0800
> From: Jenny Bryan <jenny at stat.ubc.ca>
> Subject: [R-sig-ME] logistic growth,	vexing choice of which timepoints
> 	to include
> To: r-sig-mixed-models at r-project.org
> Message-ID: <4B7E17CD-C967-479F-85CD-97404D975969 at stat.ubc.ca>
> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
> 
> Hello.  I'm looking for advice on how to make a seemingly unavoidable  
> subjective choice in an analysis I'm doing, using the logistic growth  
> model.  I'm using nlme, but that has nothing to do with my question,  
> so I hope it's not too inappropriate for me to post this here.   
> Reading the list archive suggests that it's not too hard to tempt this  
> group into philosophical discussions :-)
> 
> I have growth data that can be reasonably modelled with a four- 
> parameter logistic curve.  The experimental unit is a well in a  
> microtitre plate and I get light absorbance readings over time that  
> reflect cell density.  There are many wells on a plate, e.g. 96 or  
> 384, and experiments often span many plates.  Systematic differences  
> between the wells can be, for example, specific genetic mutations  
> carried by the cells and/or different chemicals added to the growth  
> medium.  I am mostly interested in performing inference on the fixed  
> effects, i.e. how the genetic perturbations, the chemicals, or their  
> interactions, modify key growth parameters, especially the one  
> inversely related to the underlying exponential growth rate we'd see  
> in the absence of resource constraints (phi_4 in Pinheiro & Bates p.  
> 517).
> 
> Problem:  The number of cells inoculated into the wells at the start  
> is quite small -- well below the detection threshhold for the light  
> absorbance readings.  Therefore, each timecourse begins with a loooong  
> string of zeros, before the classic sigmoidal shape kicks in.  And, of  
> course, the timing of this happy event is both ill-defined and very  
> variable across the wells.
> 
> For figure-making purposes, I removed some early timepoints that were  
> uniformly zero for all wells.  Which made me wonder: why couldn't  
> (shouldn't?) I do the same prior to model fitting?  When I fit the  
> logistic growth model with and without these early timepoints, I get  
> essentially the same estimated fixed effects and, even, estimated  
> variances for the random effects.  But there *is* a substantial  
> difference in the estimate of residual variance, which then obviously  
> has a noticeable effect on the inference for the fixed effects and,  
> especially, the one I care about.  Including all the timepoints drives  
> the residual variance down, as you might expect.  But that almost  
> seems misleading or artificial ... other collaborators I work with  
> don't even start taking OD readings until the first 12 hours have  
> passed, which makes their initial strings of zeros quite short,  
> which ... gives them less statistical significance for the same  
> observed effect size?!?
> 
> Does anyone have a comment or advice?
> 
> Thanks in advance for reading this,
> Jenny
> 
> Jennifer Bryan
> Department of Statistics and
>    the Michael Smith Laboratories
> University of British Columbia
> 
> 
> 
> ------------------------------
> 
> Message: 2
> Date: Fri, 6 Feb 2009 15:18:53 -0800
> From: Steven McKinney <smckinney at bccrc.ca>
> Subject: Re: [R-sig-ME] logistic growth,	vexing choice of which
> 	timepoints to include
> To: "Jenny Bryan" <jenny at stat.ubc.ca>,
> 	<r-sig-mixed-models at r-project.org>
> Message-ID:
> 	<0BE438149FF2254DB4199E2682C8DFEB0328A5AC at crcmail1.BCCRC.CA>
> Content-Type: text/plain; charset="iso-8859-1"
> 
> Hi Jenny,
> 
> [Caveat: Comments from an applied statistician, not
> a world-heavyweight likelihood theorist]
> 
> In the logistic world a zero value maps to a 
> minus infinity value.  It seems to me that only
> the 'last' zero value contains any information
> relevant to the likelihood (equivalently only
> the 'first' one value (plus infinity in the
> logistic realm) contains any information
> relevant to the likelihood).  Perhaps the
> coding for the likelihood has not been set
> up to take this into account so you are getting
> the artificial contribution of the rest of the
> zero values folded into the likelihood, artificially
> deflating the variance estimates.
> 
> I would exclude or set to NA all but the last
> (or even all of) the zero values for any well
> and all but the first (or even all of) the one 
> values.
> 
> The zero values are really below the detection
> limit of the sensor involved so should theoretically
> be handled as truncated data but that's another
> level of complexity for the analysis.
> 
> Steven McKinney
> 
> Statistician
> Molecular Oncology and Breast Cancer Program
> British Columbia Cancer Research Centre
> 
> email: smckinney +at+ bccrc +dot+ ca
> 
> tel: 604-675-8000 x7561
> 
> BCCRC
> Molecular Oncology
> 675 West 10th Ave, Floor 4
> Vancouver B.C. 
> V5Z 1L3
> Canada
> 
> 
> 
>



From lamprianou at yahoo.com  Sun Feb  8 20:24:59 2009
From: lamprianou at yahoo.com (Iasonas Lamprianou)
Date: Sun, 8 Feb 2009 11:24:59 -0800 (PST)
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 26, Issue 12
In-Reply-To: <mailman.1.1234090803.9172.r-sig-mixed-models@r-project.org>
Message-ID: <322783.41002.qm@web54103.mail.re2.yahoo.com>

Dear Douglas Bates

we all support you and you should not allow anyone make you feel upset. You are contributing/offering a lot to all of us and we are greateful. Keep on good work and try to ignore other nuisances. lme4 is the reason why a number of people considered R at the first place

jason

Dr. Iasonas Lamprianou
Department of Education
The University of Manchester
Oxford Road, Manchester M13 9PL, UK
Tel. 0044  161 275 3485
iasonas.lamprianou at manchester.ac.uk


--- On Sun, 8/2/09, r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org> wrote:

> From: r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org>
> Subject: R-sig-mixed-models Digest, Vol 26, Issue 12
> To: r-sig-mixed-models at r-project.org
> Date: Sunday, 8 February, 2009, 11:00 AM
> Send R-sig-mixed-models mailing list submissions to
> 	r-sig-mixed-models at r-project.org
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> or, via email, send a message with subject or body
> 'help' to
> 	r-sig-mixed-models-request at r-project.org
> 
> You can reach the person managing the list at
> 	r-sig-mixed-models-owner at r-project.org
> 
> When replying, please edit your Subject line so it is more
> specific
> than "Re: Contents of R-sig-mixed-models
> digest..."
> 
> 
> Today's Topics:
> 
>    1. Lack of replies from me (Douglas Bates)
>    2. Re: Lack of replies from me (Andrew Robinson)
>    3. logistic growth, vexing choice of which timepoints
>       (Nicholas Lewin-Koh)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Sat, 7 Feb 2009 08:33:59 -0600
> From: Douglas Bates <bates at stat.wisc.edu>
> Subject: [R-sig-ME] Lack of replies from me
> To: R Models Mixed <r-sig-mixed-models at r-project.org>
> Message-ID:
> 	<40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> I regret that I have been absent from the list for some
> time.  It
> happens that in the last couple of weeks I have been
> involved in a
> series of extremely unpleasant interactions on another,
> private email
> list that have left me with little enthusiasm for
> developing and
> supporting CRAN packages.  One conclusion I have reached
> from these
> interactions is that I will never allow lme4 to be a
> recommended
> package in R.  I am even having doubts about whether I want
> it to
> continue to be a CRAN package at all, as opposed to, say,
> moving it to
> Bioconductor or even switching development to another
> language.
> 
> I'm sure the last option would be "cutting off my
> nose to spite my
> face" and I don't expect I would ever do that. 
> There are many
> wonderful aspects to R and many reasons why I want to
> continue to use
> it.  But right now I find myself forced to evaluate options
> other than
> putting a package on CRAN.
> 
> 
> 
> ------------------------------
> 
> Message: 2
> Date: Sun, 8 Feb 2009 07:13:06 +1100
> From: Andrew Robinson <A.Robinson at ms.unimelb.edu.au>
> Subject: Re: [R-sig-ME] Lack of replies from me
> To: Douglas Bates <bates at stat.wisc.edu>
> Cc: R Models Mixed <r-sig-mixed-models at r-project.org>
> Message-ID:
> <20090207201306.GP51827 at ms.unimelb.edu.au>
> Content-Type: text/plain; charset=us-ascii
> 
> On Sat, Feb 07, 2009 at 08:33:59AM -0600, Douglas Bates
> wrote:
> > I regret that I have been absent from the list for
> some time.  It
> > happens that in the last couple of weeks I have been
> involved in a
> > series of extremely unpleasant interactions on
> another, private email
> > list that have left me with little enthusiasm for
> developing and
> > supporting CRAN packages.  One conclusion I have
> reached from these
> > interactions is that I will never allow lme4 to be a
> recommended
> > package in R.  I am even having doubts about whether I
> want it to
> > continue to be a CRAN package at all, as opposed to,
> say, moving it to
> > Bioconductor or even switching development to another
> language.
> > 
> > I'm sure the last option would be "cutting
> off my nose to spite my
> > face" and I don't expect I would ever do
> that.  There are many
> > wonderful aspects to R and many reasons why I want to
> continue to use
> > it.  But right now I find myself forced to evaluate
> options other than
> > putting a package on CRAN.
> 
> I'm very sorry to hear about your frustrations, Doug. 
> I'm sure that I
> speak for all of us when I say that we'll continue to
> use and support
> lme4 regardless of the delivery mechanism.
> 
> Warm wishes
> 
> Andrew
> -- 
> Andrew Robinson  
> Department of Mathematics and Statistics            Tel:
> +61-3-8344-6410
> University of Melbourne, VIC 3010 Australia              
> (prefer email)
> http://www.ms.unimelb.edu.au/~andrewpr              Fax:
> +61-3-8344-4599
> http://blogs.mbs.edu/fishing-in-the-bay/
> 
> 
> 
> ------------------------------
> 
> Message: 3
> Date: Sat, 07 Feb 2009 14:16:56 -0800
> From: "Nicholas Lewin-Koh"
> <nikko at hailmail.net>
> Subject: [R-sig-ME] logistic growth, vexing choice of which
> timepoints
> To: r-sig-mixed-models at r-project.org
> Cc: Jenny Bryan <jenny at stat.ubc.ca>
> Message-ID:
> <1234045016.6848.1299107865 at webmail.messagingengine.com>
> Content-Type: text/plain; charset="ISO-8859-1"
> 
> Hi Jenny
> What Steven said below is true, the zeros are below the
> detection limit.
> However,
> one might ask if the time until the populations cross the
> detection
> threshold matters?
> For instance if two wells treated differently both have
> similar logistic
> curves, but one 
> starts accelerating at t(i) and the other at t(j), j > i
> that does
> provide some information about
> what is going on below the detection limit. A sophisticated
> approach
> might be to fit a joint model
> modeling the time to the event, and the logistic growth
> simultaneously.
> Given that that is 
> hard, and there may not be any software to do it, you might
> want to fit
> the survival model (time to event)
> and then the logistic growth model n the non-zero data.
> This is very
> add-hoc, but will at least give you
> some idea of whether it is worth chasing a more complicated
> model. This
> will be more effective if 
> you have replicate wells. 
> 
> Nicholas
> 
> > Message: 1
> > Date: Fri, 6 Feb 2009 13:54:47 -0800
> > From: Jenny Bryan <jenny at stat.ubc.ca>
> > Subject: [R-sig-ME] logistic growth,	vexing choice of
> which timepoints
> > 	to include
> > To: r-sig-mixed-models at r-project.org
> > Message-ID:
> <4B7E17CD-C967-479F-85CD-97404D975969 at stat.ubc.ca>
> > Content-Type: text/plain; charset=US-ASCII;
> format=flowed; delsp=yes
> > 
> > Hello.  I'm looking for advice on how to make a
> seemingly unavoidable  
> > subjective choice in an analysis I'm doing, using
> the logistic growth  
> > model.  I'm using nlme, but that has nothing to do
> with my question,  
> > so I hope it's not too inappropriate for me to
> post this here.   
> > Reading the list archive suggests that it's not
> too hard to tempt this  
> > group into philosophical discussions :-)
> > 
> > I have growth data that can be reasonably modelled
> with a four- 
> > parameter logistic curve.  The experimental unit is a
> well in a  
> > microtitre plate and I get light absorbance readings
> over time that  
> > reflect cell density.  There are many wells on a
> plate, e.g. 96 or  
> > 384, and experiments often span many plates. 
> Systematic differences  
> > between the wells can be, for example, specific
> genetic mutations  
> > carried by the cells and/or different chemicals added
> to the growth  
> > medium.  I am mostly interested in performing
> inference on the fixed  
> > effects, i.e. how the genetic perturbations, the
> chemicals, or their  
> > interactions, modify key growth parameters, especially
> the one  
> > inversely related to the underlying exponential growth
> rate we'd see  
> > in the absence of resource constraints (phi_4 in
> Pinheiro & Bates p.  
> > 517).
> > 
> > Problem:  The number of cells inoculated into the
> wells at the start  
> > is quite small -- well below the detection threshhold
> for the light  
> > absorbance readings.  Therefore, each timecourse
> begins with a loooong  
> > string of zeros, before the classic sigmoidal shape
> kicks in.  And, of  
> > course, the timing of this happy event is both
> ill-defined and very  
> > variable across the wells.
> > 
> > For figure-making purposes, I removed some early
> timepoints that were  
> > uniformly zero for all wells.  Which made me wonder:
> why couldn't  
> > (shouldn't?) I do the same prior to model fitting?
>  When I fit the  
> > logistic growth model with and without these early
> timepoints, I get  
> > essentially the same estimated fixed effects and,
> even, estimated  
> > variances for the random effects.  But there *is* a
> substantial  
> > difference in the estimate of residual variance, which
> then obviously  
> > has a noticeable effect on the inference for the fixed
> effects and,  
> > especially, the one I care about.  Including all the
> timepoints drives  
> > the residual variance down, as you might expect.  But
> that almost  
> > seems misleading or artificial ... other collaborators
> I work with  
> > don't even start taking OD readings until the
> first 12 hours have  
> > passed, which makes their initial strings of zeros
> quite short,  
> > which ... gives them less statistical significance for
> the same  
> > observed effect size?!?
> > 
> > Does anyone have a comment or advice?
> > 
> > Thanks in advance for reading this,
> > Jenny
> > 
> > Jennifer Bryan
> > Department of Statistics and
> >    the Michael Smith Laboratories
> > University of British Columbia
> > 
> > 
> > 
> > ------------------------------
> > 
> > Message: 2
> > Date: Fri, 6 Feb 2009 15:18:53 -0800
> > From: Steven McKinney <smckinney at bccrc.ca>
> > Subject: Re: [R-sig-ME] logistic growth,	vexing choice
> of which
> > 	timepoints to include
> > To: "Jenny Bryan" <jenny at stat.ubc.ca>,
> > 	<r-sig-mixed-models at r-project.org>
> > Message-ID:
> >
> 	<0BE438149FF2254DB4199E2682C8DFEB0328A5AC at crcmail1.BCCRC.CA>
> > Content-Type: text/plain;
> charset="iso-8859-1"
> > 
> > Hi Jenny,
> > 
> > [Caveat: Comments from an applied statistician, not
> > a world-heavyweight likelihood theorist]
> > 
> > In the logistic world a zero value maps to a 
> > minus infinity value.  It seems to me that only
> > the 'last' zero value contains any information
> > relevant to the likelihood (equivalently only
> > the 'first' one value (plus infinity in the
> > logistic realm) contains any information
> > relevant to the likelihood).  Perhaps the
> > coding for the likelihood has not been set
> > up to take this into account so you are getting
> > the artificial contribution of the rest of the
> > zero values folded into the likelihood, artificially
> > deflating the variance estimates.
> > 
> > I would exclude or set to NA all but the last
> > (or even all of) the zero values for any well
> > and all but the first (or even all of) the one 
> > values.
> > 
> > The zero values are really below the detection
> > limit of the sensor involved so should theoretically
> > be handled as truncated data but that's another
> > level of complexity for the analysis.
> > 
> > Steven McKinney
> > 
> > Statistician
> > Molecular Oncology and Breast Cancer Program
> > British Columbia Cancer Research Centre
> > 
> > email: smckinney +at+ bccrc +dot+ ca
> > 
> > tel: 604-675-8000 x7561
> > 
> > BCCRC
> > Molecular Oncology
> > 675 West 10th Ave, Floor 4
> > Vancouver B.C. 
> > V5Z 1L3
> > Canada
> > 
> > 
> > 
> >
> 
> 
> 
> ------------------------------
> 
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> End of R-sig-mixed-models Digest, Vol 26, Issue 12
> **************************************************






From Jesus.Frias at dit.ie  Sun Feb  8 21:20:02 2009
From: Jesus.Frias at dit.ie (Jesus Frias)
Date: Sun, 8 Feb 2009 20:20:02 +0000 (GMT)
Subject: [R-sig-ME] logistic growth,
	vexing choice of which timepoints to include
Message-ID: <1553995.1234124402296.JavaMail.SYSTEM@asal>

Hi Jenny,

(this one coming from a total non statistician, so excuse me my 
naivety).

If you are interested on how the growth rate parameter depends on 
factors such as antimicrobial concentration or genetic expression, 
eliminating or letting points in your model fitting to influence your 
parameters variance makes me think two things: 

1) If the variance of your growth rate parameter changes a lot 
depending on the number of zero points that you use for the fit, please 
consider that a microtiter reader is an automated  experiment and the 
experimental design may be accomodated for your purpose: The data 
acquisition rate can be modified so that you have as many time points 
as you want in  the "important" part of the curve (your exponential 
growth phase for the growth rate) and then the zero growth data should 
have a minimal influence. There would be an optimal design strategy for 
that data acquisition if you had some previous knowledge of the 
parameters of your growth curves.

2) By deleting no-growth data, aren't you giving more weight to the 
experiments where your antimicrobial wasn't effective? Wouldn't be 
informative to have information on appropriate antimicrobial 
concentration ranges to ensure that your secondary model is able to 
predict zero-growth-rate conditions? 


The pile of zeros before the important data arrives is below the limit 
of detection of growth as mentioned in the previous email, so you 
should be able to take them out, but you need to think about those two 
things as well...

regards,

Jesus

School of Food Science and Environmental Health
Dublin Institute of Technology
Dublin, Ireland


Jenny Bryan wrote:


>Hello.  I'm looking for advice on how to make a seemingly unavoidable  
>subjective choice in an analysis I'm doing, using the logistic growth  
>model.  I'm using nlme, but that has nothing to do with my question,  
>so I hope it's not too inappropriate for me to post this here.   
>Reading the list archive suggests that it's not too hard to tempt this  
>group into philosophical discussions :-)
>
>I have growth data that can be reasonably modelled with a four- 
>parameter logistic curve.  The experimental unit is a well in a  
>microtitre plate and I get light absorbance readings over time that  
>reflect cell density.  There are many wells on a plate, e.g. 96 or  
>384, and experiments often span many plates.  Systematic differences  
>between the wells can be, for example, specific genetic mutations  
>carried by the cells and/or different chemicals added to the growth  
>medium.  I am mostly interested in performing inference on the fixed  
>effects, i.e. how the genetic perturbations, the chemicals, or their  
>interactions, modify key growth parameters, especially the one  
>inversely related to the underlying exponential growth rate we'd see  
>in the absence of resource constraints (phi_4 in Pinheiro & Bates p.  
>517).
>
>Problem:  The number of cells inoculated into the wells at the start  
>is quite small -- well below the detection threshhold for the light  
>absorbance readings.  Therefore, each timecourse begins with a loooong  
>string of zeros, before the classic sigmoidal shape kicks in.  And, of  
>course, the timing of this happy event is both ill-defined and very  
>variable across the wells.
>
>For figure-making purposes, I removed some early timepoints that were  
>uniformly zero for all wells.  Which made me wonder: why couldn't  
>(shouldn't?) I do the same prior to model fitting?  When I fit the  
>logistic growth model with and without these early timepoints, I get  
>essentially the same estimated fixed effects and, even, estimated  
>variances for the random effects.  But there *is* a substantial  
>difference in the estimate of residual variance, which then obviously  
>has a noticeable effect on the inference for the fixed effects and,  
>especially, the one I care about.  Including all the timepoints drives  
>the residual variance down, as you might expect.  But that almost  
>seems misleading or artificial ... other collaborators I work with  
>don't even start taking OD readings until the first 12 hours have  
>passed, which makes their initial strings of zeros quite short,  
>which ... gives them less statistical significance for the same  
>observed effect size?!?
>
>Does anyone have a comment or advice?
>
>Thanks in advance for reading this,
>Jenny
>
>Jennifer Bryan
>Department of Statistics and
>   the Michael Smith Laboratories
>University of British Columbia
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
This message has been scanned for content and viruses by the DIT Information Services E-Mail Scanning Service, and is believed to be clean. http://www.dit.ie



From renaud.lancelot at cirad.fr  Mon Feb  9 11:26:42 2009
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Mon, 09 Feb 2009 11:26:42 +0100
Subject: [R-sig-ME] Lack of replies from me
In-Reply-To: <40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be@mail.gmail.com>
References: <40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be@mail.gmail.com>
Message-ID: <499004E2.7000207@cirad.fr>

Dear Douglas,

As a long-time user of nlme and lme4, and more generally your numerous 
and invaluable contributions and support to the R community, I feel very 
sad after your message. I hope a better solution will be found for you 
and all of us , and I wish to bring you all my sympathy and support.

Best wishes,

Renaud

Douglas Bates a ?crit :
> I regret that I have been absent from the list for some time.  It
> happens that in the last couple of weeks I have been involved in a
> series of extremely unpleasant interactions on another, private email
> list that have left me with little enthusiasm for developing and
> supporting CRAN packages.  One conclusion I have reached from these
> interactions is that I will never allow lme4 to be a recommended
> package in R.  I am even having doubts about whether I want it to
> continue to be a CRAN package at all, as opposed to, say, moving it to
> Bioconductor or even switching development to another language.
> 
> I'm sure the last option would be "cutting off my nose to spite my
> face" and I don't expect I would ever do that.  There are many
> wonderful aspects to R and many reasons why I want to continue to use
> it.  But right now I find myself forced to evaluate options other than
> putting a package on CRAN.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

-- 
Renaud Lancelot
EDEN Project, coordinator
http://www.eden-fp6project.net/

UMR CIRAD-INRA "Contr?le des maladies animales exotiques et ?mergentes"
Joint research unit "Control of emerging and exotic animal diseases"

CIRAD
Campus International de Baillarguet TA A-DIR / B
F34398 Montpellier
http://www.cirad.fr  http://bluetongue.cirad.fr/

Tel.  +33 4 67 59 37 17  -  Fax  +33 4 67 59 37 95
Secr. +33 4 67 59 37 37  - Cell. +33 6 77 52 08 69



From ken at kjbeath.com.au  Mon Feb  9 12:21:32 2009
From: ken at kjbeath.com.au (Ken Beath)
Date: Mon, 9 Feb 2009 22:21:32 +1100
Subject: [R-sig-ME] Unbalanced presence/absence data
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B7F911B5363@VMAILB.uoa.abdn.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B7F911B5363@VMAILB.uoa.abdn.ac.uk>
Message-ID: <C336913E-94E2-4047-B0B9-14FECFF80052@kjbeath.com.au>

On 04/02/2009, at 1:22 AM, Renwick, A. R. wrote:

> I am trying to analyse some data I have on the presence/absence of  
> parasite infestation on small mammals using a GLMM, however I have a  
> severely unbalanced data set in that I have a large number of 0's  
> compared to 1's (i.e. 1333 0's and 86 1's).
>
> The response variable (presence/absence) is at the individual level  
> whereas all the explanatory variables (apart from sex) are at the  
> site level.  This means that a lot of the individuals have exactly  
> the same combination of all explanatory variables and when there is  
> so many individuals with 0's it leaves very little power.
>

This shouldn't be a problem, what you may need is to use the nAGQ  
parameter to increase the number of quadrature points, and avoid any  
numerical problems. This is especially important if there is high  
correlation between individuals within a site. Also unbalanced means  
something different to what you have.

> When I reduce the model I find that I can remove a number of  
> interactions terms without really affecting the AIC which lead me to  
> be slightly concerned.
>

This most likely means the interactions are not significant.

> One option would be to analyses the data at the site level, i.e  
> parasite prevalence, rather than the probability of being infested.
>

While you can do this, it is throwing away information, possibly a lot  
of information.

Ken

> Any advice as to how to deal with this unbalanced data set would be  
> very much appreciated.
>
> Anna Renwick
> Institute of Biological & Environment Sciences
> University of Aberdeen
> Zoology Building
> Tillydrone Avenue
> Aberdeen
> AB24 2TZ
>
>
> The University of Aberdeen is a charity registered in Scotland, No  
> SC013683.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From rroa at udec.cl  Mon Feb  9 15:11:54 2009
From: rroa at udec.cl (Ruben Roa Ureta)
Date: Mon, 9 Feb 2009 11:11:54 -0300 (CLST)
Subject: [R-sig-ME] Lack of replies from me
In-Reply-To: <40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be@mail.gmail.com>
References: <40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be@mail.gmail.com>
Message-ID: <1684.190.20.194.225.1234188714.squirrel@webmail.udec.cl>

> I regret that I have been absent from the list for some time.  It
> happens that in the last couple of weeks I have been involved in a
> series of extremely unpleasant interactions on another, private email
> list that have left me with little enthusiasm for developing and
> supporting CRAN packages.  One conclusion I have reached from these
> interactions is that I will never allow lme4 to be a recommended
> package in R.  I am even having doubts about whether I want it to
> continue to be a CRAN package at all, as opposed to, say, moving it to
> Bioconductor or even switching development to another language.
>
> I'm sure the last option would be "cutting off my nose to spite my
> face" and I don't expect I would ever do that.  There are many
> wonderful aspects to R and many reasons why I want to continue to use
> it.  But right now I find myself forced to evaluate options other than
> putting a package on CRAN.

If you do that, please do let us know how to find information about and/or
new versions of lme4. I have been using your package for years now and
have been reading all your posts on mixed models. That and your book with
Pinheiro are my main source of knowledge and wisdom about mixed models.
Ruben



From burkea at nwrel.org  Mon Feb  9 19:06:39 2009
From: burkea at nwrel.org (Arthur Burke)
Date: Mon, 9 Feb 2009 10:06:39 -0800
Subject: [R-sig-ME] Lack of replies from me (Douglas Bates)
In-Reply-To: <mailman.1.1234090803.9172.r-sig-mixed-models@r-project.org>
References: <mailman.1.1234090803.9172.r-sig-mixed-models@r-project.org>
Message-ID: <CF736B65E2E03F42A197473E43E074A20529DB32@w23-7928.nwrel.org>

lme4 is an important contribution and I would regret losing access to
it.

Art
------------------------------------------------------------------------
Art Burke
Northwest Regional Educational Laboratory
101 SW Main St, Suite 500
Portland, OR 97204-3213


-----Original Message-----

Date: Sat, 7 Feb 2009 08:33:59 -0600
From: Douglas Bates <bates at stat.wisc.edu>
Subject: [R-sig-ME] Lack of replies from me
To: R Models Mixed <r-sig-mixed-models at r-project.org>
Message-ID:
	<40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

I regret that I have been absent from the list for some time.  It
happens that in the last couple of weeks I have been involved in a
series of extremely unpleasant interactions on another, private email
list that have left me with little enthusiasm for developing and
supporting CRAN packages.  One conclusion I have reached from these
interactions is that I will never allow lme4 to be a recommended package
in R.  I am even having doubts about whether I want it to continue to be
a CRAN package at all, as opposed to, say, moving it to Bioconductor or
even switching development to another language.

I'm sure the last option would be "cutting off my nose to spite my face"
and I don't expect I would ever do that.  There are many wonderful
aspects to R and many reasons why I want to continue to use it.  But
right now I find myself forced to evaluate options other than putting a
package on CRAN.



From nola at stanford.edu  Mon Feb  9 19:08:12 2009
From: nola at stanford.edu (Nola Stephens)
Date: Mon, 09 Feb 2009 10:08:12 -0800
Subject: [R-sig-ME] Estimated scale in R 2.8.1
Message-ID: <4990710C.9010109@stanford.edu>

Hi,
When using earlier version of R (2.6.2), I got a scale factor for an
lmer model (where family=binomial). But after downloading R version
2.8.1, I'm unable to find this value.

I'd really appreciate some advice on how to find this.
Thanks!
nola



From f.calboli at imperial.ac.uk  Mon Feb  9 19:24:14 2009
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 9 Feb 2009 18:24:14 +0000
Subject: [R-sig-ME] Lack of replies from me (Douglas Bates)
In-Reply-To: <CF736B65E2E03F42A197473E43E074A20529DB32@w23-7928.nwrel.org>
References: <mailman.1.1234090803.9172.r-sig-mixed-models@r-project.org>
	<CF736B65E2E03F42A197473E43E074A20529DB32@w23-7928.nwrel.org>
Message-ID: <CBDB0628-4BB8-48CD-88E6-EB6BE6805EAE@imperial.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090209/223e17ff/attachment.pl>

From maj at stats.waikato.ac.nz  Mon Feb  9 19:43:10 2009
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Tue, 10 Feb 2009 07:43:10 +1300
Subject: [R-sig-ME] Lack of replies from me
In-Reply-To: <40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be@mail.gmail.com>
References: <40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be@mail.gmail.com>
Message-ID: <4990793E.6070709@stats.waikato.ac.nz>

cc: List

Dear Douglas,

"Noli illegitemes Carborundum!"

lme4 is a shining example of how statistical software should be
developed with software, theory and practice all advancing together and
in communication.  I truly hope that this patch of nastiness, whatever
it is, fades away and you get back on track. This is such a central part
of statistics that the whole community is grateful for the time and
efforts that you and your students have put into it. If others want to
take a different approach they may surely develop their own software. I
would be very sad if you took lme4 out of R. R does not belong to
anyone, not even prominent members of the core group.

Best wishes,  Murray Jorgensen

Douglas Bates wrote:
> I regret that I have been absent from the list for some time.  It
> happens that in the last couple of weeks I have been involved in a
> series of extremely unpleasant interactions on another, private email
> list that have left me with little enthusiasm for developing and
> supporting CRAN packages.  One conclusion I have reached from these
> interactions is that I will never allow lme4 to be a recommended
> package in R.  I am even having doubts about whether I want it to
> continue to be a CRAN package at all, as opposed to, say, moving it to
> Bioconductor or even switching development to another language.
> 
> I'm sure the last option would be "cutting off my nose to spite my
> face" and I don't expect I would ever do that.  There are many
> wonderful aspects to R and many reasons why I want to continue to use
> it.  But right now I find myself forced to evaluate options other than
> putting a package on CRAN.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862



From HDoran at air.org  Mon Feb  9 19:56:18 2009
From: HDoran at air.org (Doran, Harold)
Date: Mon, 9 Feb 2009 13:56:18 -0500
Subject: [R-sig-ME] Lack of replies from me
In-Reply-To: <4990793E.6070709@stats.waikato.ac.nz>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE020C0E5F@DC1EXCL01.air.org>

Not only is lme4 perhaps the most sophisticated package available for
(generalized) linear mixed models, it is also comes with gads of support
from its developer. Computationally, it crushes the more common programs
(e.g., HLM, mlWin).

The solution for the critic is simple, build a better mousetrap and
support it equally as well. 



> Douglas Bates wrote:
> > I regret that I have been absent from the list for some time.  It 
> > happens that in the last couple of weeks I have been involved in a 
> > series of extremely unpleasant interactions on another, 
> private email 
> > list that have left me with little enthusiasm for developing and 
> > supporting CRAN packages.  One conclusion I have reached from these 
> > interactions is that I will never allow lme4 to be a recommended 
> > package in R.  I am even having doubts about whether I want it to 
> > continue to be a CRAN package at all, as opposed to, say, 
> moving it to 
> > Bioconductor or even switching development to another language.
> > 
> > I'm sure the last option would be "cutting off my nose to spite my 
> > face" and I don't expect I would ever do that.  There are many 
> > wonderful aspects to R and many reasons why I want to 
> continue to use 
> > it.  But right now I find myself forced to evaluate options 
> other than 
> > putting a package on CRAN.
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> -- 
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From bates at stat.wisc.edu  Mon Feb  9 20:20:40 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 9 Feb 2009 13:20:40 -0600
Subject: [R-sig-ME] Lack of replies from me
In-Reply-To: <4990793E.6070709@stats.waikato.ac.nz>
References: <40e66e0b0902070633s428cbf0ekd3e37f1e82ccf2be@mail.gmail.com>
	<4990793E.6070709@stats.waikato.ac.nz>
Message-ID: <40e66e0b0902091120m1dc32e99o2b80e86d13fcba42@mail.gmail.com>

On Mon, Feb 9, 2009 at 12:43 PM, Murray Jorgensen
<maj at stats.waikato.ac.nz> wrote:
> Dear Douglas,

> "Noli illegitemes Carborundum!"

> lme4 is a shining example of how statistical software should be
> developed with software, theory and practice all advancing together and
> in communication.  I truly hope that this patch of nastiness, whatever
> it is, fades away and you get back on track. This is such a central part
> of statistics that the whole community is grateful for the time and
> efforts that you and your students have put into it. If others want to
> take a different approach they may surely develop their own software. I
> would be very sad if you took lme4 out of R. R does not belong to
> anyone, not even prominent members of the core group.

> Best wishes,  Murray Jorgensen

My thanks to you, Murray, and to the many other people who offered
their support and encouragement privately.  It is very gratifying to
me to know that people appreciate access to the software and the
discussions that we have been able to provide.  In the last sentence I
wrote "we" referring to the entire mailing list because the exchanges
on the list and the suggestions from many people over the years have
helped in the process of what Murray describes as "software, theory
and practice all advancing together and in communication".

For example, I have said on several occasions (but may never have
written to this group) that the original suggestion to consider sparse
matrix methods for mixed-effects or multilevel models came Harold
Doran on the first day that I met him.  He was attending a short
course I was giving and, at a reception before the course, we were
talking and he made this suggestion.  Naturally my reaction was, "Hmm,
that's interesting but I really don't know much about sparse matrix
methods" and I planned to just go on with things as they had been.
Fortunately, Harold is persuasive and convinced me that I should learn
something about sparse matrices.

Right now I plan to take a break from acrimonious discussions (and let
me emphasize that it was not discussions on this list that made me so
despondent - Murray's last sentence is much more to the point) and
devote myself to developing code and writing a manuscript for a book.
When the code is developed I will consider in more detail exactly how
it should be released.  For the time being I will retain the R-forge
SVN archive.

Again, let me say how much I appreciate the many kind messages I have received.



> Douglas Bates wrote:
>>
>> I regret that I have been absent from the list for some time.  It
>> happens that in the last couple of weeks I have been involved in a
>> series of extremely unpleasant interactions on another, private email
>> list that have left me with little enthusiasm for developing and
>> supporting CRAN packages.  One conclusion I have reached from these
>> interactions is that I will never allow lme4 to be a recommended
>> package in R.  I am even having doubts about whether I want it to
>> continue to be a CRAN package at all, as opposed to, say, moving it to
>> Bioconductor or even switching development to another language.
>>
>> I'm sure the last option would be "cutting off my nose to spite my
>> face" and I don't expect I would ever do that.  There are many
>> wonderful aspects to R and many reasons why I want to continue to use
>> it.  But right now I find myself forced to evaluate options other than
>> putting a package on CRAN.
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> --
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From marc.moragues at gmail.com  Mon Feb  9 22:02:33 2009
From: marc.moragues at gmail.com (Marc Moragues)
Date: Mon, 9 Feb 2009 14:02:33 -0700
Subject: [R-sig-ME] Significance of fixed effects. Kinship package
Message-ID: <200902091402.33547.moragues@lamar.colostate.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090209/1311e131/attachment.pl>

From Gregor.Gorjanc at bfro.uni-lj.si  Mon Feb  9 22:52:20 2009
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Mon, 9 Feb 2009 22:52:20 +0100
Subject: [R-sig-ME] Correlation of Fixed Effects
Message-ID: <F189E18BBAA8B6479F618B9044CF4E9C68DE5CCBFD@REGULUS.bfro.uni-lj.si>

Hi!

The default print method outputs also Correlation of Fixed Effects.
How is this computed and what does it actually represent? I have two models
that essentially give me the same message, but in one model the correlations
between covariates are really high 0.9 and higher, while in other model use of
poly(), reduced correlations a lot! Should I care?

Thanks!

Lep pozdrav / With regards,
    Gregor Gorjanc
----------------------------------------------------------------------
University of Ljubljana       PhD student
Biotechnical Faculty          www: http://gregor.gorjanc.googlepages.com
Department of Animal Science  blog: http://ggorjan.blogspot.com
Groblje 3                     mail: gregor.gorjanc <at> bfro.uni-lj.si
SI-1230 Domzale               fax: +386 (0)1 72 17 888
Slovenia, Europe              tel: +386 (0)1 72 17 861



From bolker at ufl.edu  Mon Feb  9 23:46:32 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 09 Feb 2009 17:46:32 -0500
Subject: [R-sig-ME] Estimated scale in R 2.8.1
Message-ID: <4990B248.9060407@ufl.edu>


  Your mileage may vary, but I have found

lme4:::sigma(model)

to be a reasonably reliable way to get it.

  Ben Bolker

Nola Stephens wrote:
> Hi,
> When using earlier version of R (2.6.2), I got a scale factor for an
> lmer model (where family=binomial). But after downloading R version
> 2.8.1, I'm unable to find this value.
> 
> I'd really appreciate some advice on how to find this.
> Thanks!
> nola
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From bolker at ufl.edu  Tue Feb 10 00:02:28 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 09 Feb 2009 18:02:28 -0500
Subject: [R-sig-ME] Correlation of Fixed Effects
In-Reply-To: <F189E18BBAA8B6479F618B9044CF4E9C68DE5CCBFD@REGULUS.bfro.uni-lj.si>
References: <F189E18BBAA8B6479F618B9044CF4E9C68DE5CCBFD@REGULUS.bfro.uni-lj.si>
Message-ID: <4990B604.7020609@ufl.edu>


  Reducing correlations among fixed effects should
improve numerical stability and may help interpretability
(by allowing estimation of some parameters precisely
rather than spreading variance across several correlated
parameters).  If you're not interested in separating
the effects of the different parameters, and if your
model fits OK either way, I wouldn't say it was critical.
   At least that's my impression.  I'm happy to be
enlightened.

  Ben Bolker

Gorjanc Gregor wrote:
> Hi!
> 
> The default print method outputs also Correlation of Fixed Effects.
> How is this computed and what does it actually represent? I have two models
> that essentially give me the same message, but in one model the correlations
> between covariates are really high 0.9 and higher, while in other model use of
> poly(), reduced correlations a lot! Should I care?
> 
> Thanks!
> 
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> ----------------------------------------------------------------------
> University of Ljubljana       PhD student
> Biotechnical Faculty          www: http://gregor.gorjanc.googlepages.com
> Department of Animal Science  blog: http://ggorjan.blogspot.com
> Groblje 3                     mail: gregor.gorjanc <at> bfro.uni-lj.si
> SI-1230 Domzale               fax: +386 (0)1 72 17 888
> Slovenia, Europe              tel: +386 (0)1 72 17 861
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From kingsfordjones at gmail.com  Tue Feb 10 02:01:04 2009
From: kingsfordjones at gmail.com (Kingsford Jones)
Date: Mon, 9 Feb 2009 18:01:04 -0700
Subject: [R-sig-ME] Correlation of Fixed Effects
In-Reply-To: <F189E18BBAA8B6479F618B9044CF4E9C68DE5CCBFD@REGULUS.bfro.uni-lj.si>
References: <F189E18BBAA8B6479F618B9044CF4E9C68DE5CCBFD@REGULUS.bfro.uni-lj.si>
Message-ID: <2ad0cc110902091701q6dd1a6fwe5bd88f35dd0cb9c@mail.gmail.com>

On Mon, Feb 9, 2009 at 2:52 PM, Gorjanc Gregor
<Gregor.Gorjanc at bfro.uni-lj.si> wrote:
> Hi!
>
> The default print method outputs also Correlation of Fixed Effects.
> How is this computed and what does it actually represent?

Hi Gregor,

In a standard LM it's calculated Cov(\beta) = \sigma^{2}(X'IX)^{-1},
where X is the model design matrix.  In practice \sigma^2 is estimated
by the sum of squared residuals divided by the number of cols in X
minus its rank.

Although I'm guessing here, I assume the equation changes for an LMM
in that the estimate of \sigma^2 becomes a sum of estimated variance
components, and rather than an identity matrix, there may be any
positive definite matrix in between the X' and the X (e.g. if weights
or corr arguments are used in lme we get non-Identity error
covariance).  I tried to confirm this 'guess' by looking at the code
for the vcov method for mer objects, but my S4 skills are too limited
to know how to find it ---  anyone?  (and on a side note -- the fact
that this can (usually) be easily done is yet another reason why I
would be very happy to see Doug's future work to remain in R ;-))

As far as what it represents, as you'd guess the sqrt of the diagonals
are the SEs for the estimated coefficients and the off-diagonals are
the estimated covariances between those estimates.  I suppose another
answer is that the off-diagonals provide indication of the amount of
collinearity in X.

> I have two models
> that essentially give me the same message, but in one model the correlations
> between covariates are really high 0.9 and higher, while in other model use of
> poly(), reduced correlations a lot! Should I care?

Not surprisingly, when X has columns that are higher-order terms of
another column collinearity occurs and the correlations between
coefficients are high.  'poly' produces orthogonal polynomials, so
covariances of the resulting coefficients should be essentially zero.
The nice thing about that is that terms can be added/removed from the
model without affecting the remaining estimates.  On the other hand,
when estimated coefficients are highly correlated their
interpretations are confounded.

hth,

Kingsford Jones



>
> Thanks!
>
> Lep pozdrav / With regards,
>    Gregor Gorjanc
> ----------------------------------------------------------------------
> University of Ljubljana       PhD student
> Biotechnical Faculty          www: http://gregor.gorjanc.googlepages.com
> Department of Animal Science  blog: http://ggorjan.blogspot.com
> Groblje 3                     mail: gregor.gorjanc <at> bfro.uni-lj.si
> SI-1230 Domzale               fax: +386 (0)1 72 17 888
> Slovenia, Europe              tel: +386 (0)1 72 17 861
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From nola at stanford.edu  Tue Feb 10 05:23:38 2009
From: nola at stanford.edu (Nola M. Stephens)
Date: Mon, 09 Feb 2009 20:23:38 -0800
Subject: [R-sig-ME] Estimated scale in R 2.8.1
In-Reply-To: <4990B20D.8090000@ufl.edu>
References: <4990710C.9010109@stanford.edu> <4990B20D.8090000@ufl.edu>
Message-ID: <4991014A.6070203@stanford.edu>

Thanks for the suggestion. But when I try sigma(model), I get this error 
message: could not find function "sigma". Any other thoughts?

Thanks!
nola

Ben Bolker wrote:
>   Your mileage may vary, but I have found
> 
> lme4:::sigma(model)
> 
> to be a reasonably reliable way to get it.
> 
>   Ben Bolker
> 
> Nola Stephens wrote:
>> Hi,
>> When using earlier version of R (2.6.2), I got a scale factor for an
>> lmer model (where family=binomial). But after downloading R version
>> 2.8.1, I'm unable to find this value.
>>
>> I'd really appreciate some advice on how to find this.
>> Thanks!
>> nola
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
>



From kingsfordjones at gmail.com  Tue Feb 10 07:05:44 2009
From: kingsfordjones at gmail.com (Kingsford Jones)
Date: Mon, 9 Feb 2009 23:05:44 -0700
Subject: [R-sig-ME] Correlation of Fixed Effects
In-Reply-To: <2ad0cc110902091701q6dd1a6fwe5bd88f35dd0cb9c@mail.gmail.com>
References: <F189E18BBAA8B6479F618B9044CF4E9C68DE5CCBFD@REGULUS.bfro.uni-lj.si>
	<2ad0cc110902091701q6dd1a6fwe5bd88f35dd0cb9c@mail.gmail.com>
Message-ID: <2ad0cc110902092205y6e54906bj3d3a1fb8df298333@mail.gmail.com>

Just to be complete, here's an example of getting the correlation of
fixed effects from the covariance matrix:

> example(lmer, package='lme4', echo=FALSE)
> fm1
Linear mixed model fit by REML
Formula: Reaction ~ Days + (Days | Subject)
   Data: sleepstudy
  AIC  BIC logLik deviance REMLdev
 1756 1775 -871.8     1752    1744
Random effects:
 Groups   Name        Variance Std.Dev. Corr
 Subject  (Intercept) 612.092  24.7405
          Days         35.072   5.9221  0.066
 Residual             654.941  25.5918
Number of obs: 180, groups: Subject, 18

Fixed effects:
            Estimate Std. Error t value
(Intercept)  251.405      6.825   36.84
Days          10.467      1.546    6.77

Correlation of Fixed Effects:
     (Intr)
Days -0.138

> vcov(fm1)
2 x 2 Matrix of class "dpoMatrix"
          [,1]      [,2]
[1,] 46.574676 -1.452393
[2,] -1.452393  2.389416

> -1.4524/prod(sqrt(diag(vcov(fm1))))
[1] -0.1376783



On Mon, Feb 9, 2009 at 6:01 PM, Kingsford Jones
<kingsfordjones at gmail.com> wrote:
> On Mon, Feb 9, 2009 at 2:52 PM, Gorjanc Gregor
> <Gregor.Gorjanc at bfro.uni-lj.si> wrote:
>> Hi!
>>
>> The default print method outputs also Correlation of Fixed Effects.
>> How is this computed and what does it actually represent?
>
> Hi Gregor,
>
> In a standard LM it's calculated Cov(\beta) = \sigma^{2}(X'IX)^{-1},
> where X is the model design matrix.  In practice \sigma^2 is estimated
> by the sum of squared residuals divided by the number of cols in X
> minus its rank.
>
> Although I'm guessing here, I assume the equation changes for an LMM
> in that the estimate of \sigma^2 becomes a sum of estimated variance
> components, and rather than an identity matrix, there may be any
> positive definite matrix in between the X' and the X (e.g. if weights
> or corr arguments are used in lme we get non-Identity error
> covariance).  I tried to confirm this 'guess' by looking at the code
> for the vcov method for mer objects, but my S4 skills are too limited
> to know how to find it ---  anyone?  (and on a side note -- the fact
> that this can (usually) be easily done is yet another reason why I
> would be very happy to see Doug's future work to remain in R ;-))
>
> As far as what it represents, as you'd guess the sqrt of the diagonals
> are the SEs for the estimated coefficients and the off-diagonals are
> the estimated covariances between those estimates.  I suppose another
> answer is that the off-diagonals provide indication of the amount of
> collinearity in X.
>
>> I have two models
>> that essentially give me the same message, but in one model the correlations
>> between covariates are really high 0.9 and higher, while in other model use of
>> poly(), reduced correlations a lot! Should I care?
>
> Not surprisingly, when X has columns that are higher-order terms of
> another column collinearity occurs and the correlations between
> coefficients are high.  'poly' produces orthogonal polynomials, so
> covariances of the resulting coefficients should be essentially zero.
> The nice thing about that is that terms can be added/removed from the
> model without affecting the remaining estimates.  On the other hand,
> when estimated coefficients are highly correlated their
> interpretations are confounded.
>
> hth,
>
> Kingsford Jones
>
>
>
>>
>> Thanks!
>>
>> Lep pozdrav / With regards,
>>    Gregor Gorjanc
>> ----------------------------------------------------------------------
>> University of Ljubljana       PhD student
>> Biotechnical Faculty          www: http://gregor.gorjanc.googlepages.com
>> Department of Animal Science  blog: http://ggorjan.blogspot.com
>> Groblje 3                     mail: gregor.gorjanc <at> bfro.uni-lj.si
>> SI-1230 Domzale               fax: +386 (0)1 72 17 888
>> Slovenia, Europe              tel: +386 (0)1 72 17 861
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>



From pelle at wallace.emg.umu.se  Tue Feb 10 07:30:25 2009
From: pelle at wallace.emg.umu.se (Pelle Ingvarsson)
Date: Tue, 10 Feb 2009 07:30:25 +0100
Subject: [R-sig-ME] Significance of fixed effects. Kinship package (Marc
	Moragues)
In-Reply-To: <mailman.2962.1234245960.8445.r-sig-mixed-models@r-project.org>
References: <mailman.2962.1234245960.8445.r-sig-mixed-models@r-project.org>
Message-ID: <49911F01.7060300@wallace.emg.umu.se>

Hi,

When I use the kinship package I fit two models, with and without the 
fixed effect you want to test. The compare the log-likelihoods of the 
two models, twice the difference in log-likelihood between models should 
be apprixmately chi-square with df equal to the difference in number of 
parameters in the two models.

So using your example:

aa<-lmekin(dta1[,j] ~ dta1[,k] + g1:g2:g3:g4,data = dta1, random = 
~1|geno, varlist = list(K), subset = Year==i)

aa2<-lmekin(dta1[,j] ~ g1:g2:g3:g4,data = dta1, random = ~1|geno, 
varlist = list(K), subset = Year==i)

X2<-2*(logLik(aa)-logLik(aa2))
df<-aa$df-aa2$df
p<-1-pchisq(X2,df)

would give you a chi-square value and the associated degrees of freedom. 
The p-value of the effect can then be calculated using pchisq.

-Pelle



> Message: 1
> Date: Mon, 9 Feb 2009 14:02:33 -0700
> From: Marc Moragues <marc.moragues at gmail.com>
> Subject: [R-sig-ME] Significance of fixed effects. Kinship package
> To: R Mixed Models <R-sig-mixed-models at r-project.org>
> Message-ID: <200902091402.33547.moragues at lamar.colostate.edu>
> Content-Type: text/plain
> 
> Hi,
> 
> Some time ago, I was pointed to use the kinship package to include the 
> variance/co-variance in a mixed model. My code is as follows and works well 
> (it does not give any error).
> 
>>     aa <- lmekin(dta1[,j] ~ dta1[,k] + g1:g2:g3:g4,data = dta1, random = ~ 
> 1|geno, varlist = list(K), subset = Year==i)
>> aa
> Linear mixed-effects kinship model fit by maximum likelihood
>   Data: dta1 
>   Subset: Year == i 
>   Log-likelihood = -581.048 
>   n= 192 
> 
> Fixed effects: dta1[, j] ~ dta1[, k] + g1:g2:g3:g4 
>                 Estimate  Std. Error    t value      Pr(>|t|)
> (Intercept)   80.5622757    1.468117 54.8745492 1.185858e-117
> dta1[, k]1    -0.5091199    1.448478 -0.3514860  7.256174e-01
> dta1[, k]na   -7.8414997    2.202464 -3.5603297  4.691413e-04
> g1:g2:g3:g4 -657.2792180 1337.706920 -0.4913477  6.237537e-01
> 
> Wald test of fixed effects =  18.29943 df =  3 p =  0.0003815272
> 
> Random effects: ~1 | geno 
>  Variance list: list(K) 
>                    geno     resid
> Standard Dev: 3.2863076 3.8496073
> % Variance:   0.4215502 0.5784498
> 
> 
> Now I would like to calculate the significance of dta[,k]. The anova function 
> does not work on objects of class lmekin. Any help will be very much 
> appreciated.
> 
> Marc.
> 
> 	[[alternative HTML version deleted]]
> 
> 

-- 
P?r K. Ingvarsson
Senior Researcher, Swedish Research Council
Associate Professor
Ume? Plant Science Centre
Department of Ecology and Environmental Science
Linneaus v?g 6
Ume? University, SE-901 87 Ume?
tel. +46-(0)90-786-7414, fax. +46-(0)90-786-6705



From HStevens at muohio.edu  Tue Feb 10 09:45:14 2009
From: HStevens at muohio.edu (Martin Henry H. Stevens)
Date: Tue, 10 Feb 2009 03:45:14 -0500
Subject: [R-sig-ME] Estimated scale in R 2.8.1
In-Reply-To: <4991014A.6070203@stanford.edu>
References: <4990710C.9010109@stanford.edu> <4990B20D.8090000@ufl.edu>
	<4991014A.6070203@stanford.edu>
Message-ID: <5BCD59ED-493B-40C6-802B-0132D3583D34@muohio.edu>

have you tried
lme4:::sigma(model)
?
Hank
On Feb 9, 2009, at 11:23 PM, Nola M. Stephens wrote:

> Thanks for the suggestion. But when I try sigma(model), I get this  
> error
> message: could not find function "sigma". Any other thoughts?
>
> Thanks!
> nola
>
> Ben Bolker wrote:
>>   Your mileage may vary, but I have found
>>
>> lme4:::sigma(model)
>>
>> to be a reasonably reliable way to get it.
>>
>>   Ben Bolker
>>
>> Nola Stephens wrote:
>>> Hi,
>>> When using earlier version of R (2.6.2), I got a scale factor for an
>>> lmer model (where family=binomial). But after downloading R version
>>> 2.8.1, I'm unable to find this value.
>>>
>>> I'd really appreciate some advice on how to find this.
>>> Thanks!
>>> nola
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.users.muohio.edu/harkesae/
http://www.cas.muohio.edu/ecology
http://www.muohio.edu/botany/
"E Pluribus Unum"

"I love deadlines. I love the whooshing noise they make as they go by."
                                             (Douglas Adams)


If you send an attachment, please try to send it in a format anyone  
can read, such as PDF, text, Open Document Format, HTML, or RTF.
Why? See:  http://www.gnu.org/philosophy/no-word-attachments.html



From bates at stat.wisc.edu  Tue Feb 10 14:55:51 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 10 Feb 2009 07:55:51 -0600
Subject: [R-sig-ME] Estimated scale in R 2.8.1
In-Reply-To: <5BCD59ED-493B-40C6-802B-0132D3583D34@muohio.edu>
References: <4990710C.9010109@stanford.edu> <4990B20D.8090000@ufl.edu>
	<4991014A.6070203@stanford.edu>
	<5BCD59ED-493B-40C6-802B-0132D3583D34@muohio.edu>
Message-ID: <40e66e0b0902100555u9c18150hbff9a2098c88d1bc@mail.gmail.com>

A word of caution.  There is a reason that the function sigma() is not
exported from the lme4 package, meaning that you must use

lme4:::sigma

to access it rather than the more obvious

sigma

Functions that are exported from the namespace of a package are
intended for general use.  Functions that are not exported are
generally for internal use.  Users who are sufficiently determined can
access them but doing so is on a "caveat emptor" basis.

I do not export this function because I'm not sure that this
calculation is appropriate for all generalized linear mixed models
(GLMMs).  There are many areas where the definition of the likelihood
in a GLMM and its relationship to the iteratively reweighted least
squares (IRLS) algorithm, which for GLMMs becomes a penalized IRLS,
become murky, to me at least.

I wouldn't want to provide a measure of the appropriateness of a model
only to find out later that I had been misleading users.

On Tue, Feb 10, 2009 at 2:45 AM, Martin Henry H. Stevens
<HStevens at muohio.edu> wrote:
> have you tried
> lme4:::sigma(model)
> ?
> Hank
> On Feb 9, 2009, at 11:23 PM, Nola M. Stephens wrote:
>
>> Thanks for the suggestion. But when I try sigma(model), I get this error
>> message: could not find function "sigma". Any other thoughts?
>>
>> Thanks!
>> nola
>>
>> Ben Bolker wrote:
>>>
>>>  Your mileage may vary, but I have found
>>>
>>> lme4:::sigma(model)
>>>
>>> to be a reasonably reliable way to get it.
>>>
>>>  Ben Bolker
>>>
>>> Nola Stephens wrote:
>>>>
>>>> Hi,
>>>> When using earlier version of R (2.6.2), I got a scale factor for an
>>>> lmer model (where family=binomial). But after downloading R version
>>>> 2.8.1, I'm unable to find this value.
>>>>
>>>> I'd really appreciate some advice on how to find this.
>>>> Thanks!
>>>> nola
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> Dr. Hank Stevens, Associate Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.users.muohio.edu/harkesae/
> http://www.cas.muohio.edu/ecology
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
>
> "I love deadlines. I love the whooshing noise they make as they go by."
>                                            (Douglas Adams)
>
>
> If you send an attachment, please try to send it in a format anyone can
> read, such as PDF, text, Open Document Format, HTML, or RTF.
> Why? See:  http://www.gnu.org/philosophy/no-word-attachments.html
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bolker at ufl.edu  Tue Feb 10 15:17:52 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 10 Feb 2009 09:17:52 -0500
Subject: [R-sig-ME] Estimated scale in R 2.8.1
In-Reply-To: <4991014A.6070203@stanford.edu>
References: <4990710C.9010109@stanford.edu> <4990B20D.8090000@ufl.edu>
	<4991014A.6070203@stanford.edu>
Message-ID: <49918C90.8000008@ufl.edu>

  You have to explicitly write

lme4:::sigma(model)

because in this version of lme4 "sigma" is hidden within
the package namespace.  (The triple colon is S4 magic
for extracting an otherwise hidden function from
a namespace.)

  Ben

Nola M. Stephens wrote:
> Thanks for the suggestion. But when I try sigma(model), I get this error 
> message: could not find function "sigma". Any other thoughts?
> 
> Thanks!
> nola
> 
> Ben Bolker wrote:
>>   Your mileage may vary, but I have found
>>
>> lme4:::sigma(model)
>>
>> to be a reasonably reliable way to get it.
>>
>>   Ben Bolker
>>
>> Nola Stephens wrote:
>>> Hi,
>>> When using earlier version of R (2.6.2), I got a scale factor for an
>>> lmer model (where family=binomial). But after downloading R version
>>> 2.8.1, I'm unable to find this value.
>>>
>>> I'd really appreciate some advice on how to find this.
>>> Thanks!
>>> nola
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From a.renwick at abdn.ac.uk  Tue Feb 10 17:54:30 2009
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Tue, 10 Feb 2009 16:54:30 +0000
Subject: [R-sig-ME] CHOLMOD warning with GLMM
Message-ID: <B9D1301370916C44B5874AF340C18B9B7F911B5389@VMAILB.uoa.abdn.ac.uk>

Dear All
I am trying to model the burden of fleas on field voles using a GLMM with poisson error and a nested random effect.
The random effect is (1|Farm/Margin) I have 7 farms and 14 margins (2 margins per farm) and have sampled each margin 4 times.
However, I continually get an error message even when I simplify the model.  The model I am trying to run is:


bin<-lmer(fleaburden ~ sex + width + month + vole abundance + alternative host abunandance +(1|Farm/Margin), family=poisson, data=flea, REML=FALSE)


The error message:

CHOLMOD warning: ?e
Error in mer_finalize(ans) :
  Cholmod error `not positive definite' at file:../Cholesky/t_cholmod_rowfac.c, line 432

Any help in understanding this error would be much appreciated.

Thanks,
Anna

Anna Renwick
Institute of Biological & Environment Sciences
University of Aberdeen
Zoology Building
Tillydrone Avenue
Aberdeen
AB24 2TZ


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From bates at stat.wisc.edu  Tue Feb 10 18:38:14 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 10 Feb 2009 11:38:14 -0600
Subject: [R-sig-ME] CHOLMOD warning with GLMM
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B7F911B5389@VMAILB.uoa.abdn.ac.uk>
References: <B9D1301370916C44B5874AF340C18B9B7F911B5389@VMAILB.uoa.abdn.ac.uk>
Message-ID: <40e66e0b0902100938v15e3e478s3fa26accd368ce35@mail.gmail.com>

2009/2/10 Renwick, A. R. <a.renwick at abdn.ac.uk>:
> Dear All
> I am trying to model the burden of fleas on field voles using a GLMM with poisson error and a nested random effect.
> The random effect is (1|Farm/Margin) I have 7 farms and 14 margins (2 margins per farm) and have sampled each margin 4 times.
> However, I continually get an error message even when I simplify the model.  The model I am trying to run is:

> bin<-lmer(fleaburden ~ sex + width + month + vole abundance + alternative host abunandance +(1|Farm/Margin), family=poisson, data=flea, REML=FALSE)

Do you really have variables named 'vole abundance' and 'alternative
host abundance' or did you edit that part of the transcript?  I would
imagine you would get a syntax error reported from something like that
because of the embedded blanks without appropriate quoting.

> The error message:

> CHOLMOD warning:   ?e
> Error in mer_finalize(ans) :
>  Cholmod error `not positive definite' at file:../Cholesky/t_cholmod_rowfac.c, line 432

> Any help in understanding this error would be much appreciated.

Usually I suggest using verbose = TRUE to begin looking at the source
of the problem.  The error message is unexpected.  I would have
guessed that it couldn't happen in this calculation.  It is from the
Cholesky factorization of a matrix derived from the model matrix for
the random effects and it indicates that the matrix is singular.  As I
said, I expected that the way the calculation is done that matrix
never could be singular.

Would you be willing to share your data with me (you can make it
anonymous if you wish - I'm not sure if privacy considerations extend
to voles but I am willing to have their real names removed, I only
need the numbers) so that I can check in more detail what is
happening?



From jianfeng.mao at gmail.com  Wed Feb 11 09:36:51 2009
From: jianfeng.mao at gmail.com (Mao Jianfeng)
Date: Wed, 11 Feb 2009 16:36:51 +0800
Subject: [R-sig-ME] how to derive 5 level nested anova results table
Message-ID: <9111cf820902110036i41e56a85s5e1585d9bfcb1212@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090211/7973a0c3/attachment.pl>

From gasper.cankar at ric.si  Wed Feb 11 12:31:12 2009
From: gasper.cankar at ric.si (Gasper Cankar)
Date: Wed, 11 Feb 2009 12:31:12 +0100
Subject: [R-sig-ME] sample weights in lmer
Message-ID: <05FDD1814AFFAB408E15A0ABF3AAE6D88071E2@intra2003.ric.si>

Dear R users,

I found in this mailing list the post from Adam that's been unanswered.
Since I came across same problem, I'll join his kind request for any
help on using sample weights with lmer.

Thanks for any response,

___________________
Gasper Cankar. PhD 
researcher
National Examinations Centre (Ric)
gasper.cankar at ric.si
tel. +386 1 54 84 682
fax. +386 1 54 84 601

Adam's post:

>From aslez at ssc.wisc.edu  Sat Oct 18 00:09:31 2008
From: aslez at ssc.wisc.edu (aslez at ssc.wisc.edu)
Date: Fri, 17 Oct 2008 17:09:31 -0500 (CDT)
Subject: [R-sig-ME] sample weights in lmer
Message-ID:
<4739.128.104.27.172.1224281371.squirrel at webmail.ssc.wisc.edu>

Fellow R users,

I was wondering whether it is possible incorporate sample weights into
the
lmer command.  My understanding is that the existing weights option
follows the types of weights used with the lm command; that is, they are
the type of weights you would use when doing something like weighted
least
squares models.  To the best of my knowledge, sample or design weights
are
not equivalent to the types of model weights we use for WLS.  I have at
least quasi-confirmed this by comparing lmer results to results produced
using HLM.  Without weights, the results are identical. With weights,
they
differ.  I know for a fact HLM is using sample weights.

This maybe a relatively simple question, but I just made the switch to
R,
so the answer isn't completely obvious to me.  My thought is either that
there is a way of linking svydesign commands with the lmer routine, or
that theres a way of "tricking" lmer so that it produces sample-weighted
results using the WLS-type weights command.

Any help would be much appreciated.

Adam



From bates at stat.wisc.edu  Wed Feb 11 13:58:45 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 11 Feb 2009 06:58:45 -0600
Subject: [R-sig-ME] how to derive 5 level nested anova results table
In-Reply-To: <9111cf820902110036i41e56a85s5e1585d9bfcb1212@mail.gmail.com>
References: <9111cf820902110036i41e56a85s5e1585d9bfcb1212@mail.gmail.com>
Message-ID: <40e66e0b0902110458l1cde6cc7m57cfdeefee20a0c3@mail.gmail.com>

On Wed, Feb 11, 2009 at 2:36 AM, Mao Jianfeng <jianfeng.mao at gmail.com> wrote:
> Hello.

> I am new to R. And, I want to perform a multiple nested anova on a large
> datasets (with 9448 observations).  Under the helps from R-Sig-ecology
> mailing list, I have gained many progresses. But I still have some
> confusions. I want to ask for some helps here.

> my dataset("SeedL.txt") was not attached. Data are not sorted by factors.

> In this dataset, "SpecN" "PopN"  "TreeN" "ConeN" "SeedN" were 5 factors (as
> Explanatory), with "PopN" nested within "SpecN"; "TreeN" nested within
> "PopN"; "ConeN" nested within "TreeN" and "SeedN" nested within "ConeN".
> These are categories.

> "SeedL" is a dependent variate (as Response).

Thank you for your inquiry.  Your data sound fascinating.  I have, on
occasion, described several levels of nested categories with a
hypothetical example having a structure like this (my hypothetical
example used "seed pod" instead of cone).  It is charming to see a
hypothetical example suddenly spring to life.

> I have performed a successful mutinested anova using function lme()
> (library(nlme)).But I still do not know how to get a anova result table
> (sth. like SAS output).

Could you provide more detail on what you would like to see in such an
anova table, please?  For example, are you interested in the results
of hypothesis tests regarding whether certain variance components can
be zero?

You may find that

intervals(f1)

provides some of the information you want to see (although not the
type of hypothesis test I mentioned above).

>> f1 <- lme(SeedL~1, data=seedL, random=~1|SpecN/PopN/TreeN,
> na.action=na.omit)

>> f1
> Linear mixed-effects model fit by REML
>  Data: seedL
>  Log-restricted-likelihood: 14369.45
>  Fixed: SeedL ~ 1
> (Intercept)
>  0.6153105
>
> Random effects:
>  Formula: ~1 | SpecN
>        (Intercept)
> StdDev:  0.08008076
>
>  Formula: ~1 | PopN %in% SpecN
>        (Intercept)
> StdDev:  0.05413198
>
>  Formula: ~1 | TreeN %in% PopN %in% SpecN
>        (Intercept)   Residual
> StdDev:  0.04566776 0.04790476
>
> Number of Observations: 9447
> Number of Groups:
>                     SpecN            PopN %in% SpecN TreeN %in% PopN %in%
> SpecN
>                         3                         47
> 731
>> anova(f1)
>            numDF denDF  F-value p-value
> (Intercept)     1  8716 169.2052  <.0001
>
> I appreciate any advice.
>
> Mao J-F
> State Key Lab of Systematics and Evolutionary Botany
> Institute of Botany
> Chinese Academy of Sciences
> Beijing, China
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From David.Springate at postgrad.manchester.ac.uk  Wed Feb 11 23:19:42 2009
From: David.Springate at postgrad.manchester.ac.uk (David Springate)
Date: Wed, 11 Feb 2009 22:19:42 +0000
Subject: [R-sig-ME] interaction model and variance in lme
Message-ID: <20090211221942.30803im8rfmrffgg@webmail.manchester.ac.uk>

Hi,

I am new to R and I have been trying to build a model that I can  
extract ML variance components for the interaction from, but am  
struggling to make sense of the formula.

I am looking at the interaction of family (random) and two  
environmental treatments (fixed) on a trait.  So far I have tried:

model=lme(trait~treatment,data=dataset,random=~1|family/treatment)

But this seems to give me just the treatment nested in the family  
rather than family*treatment values.

I also tried:

Model = lme(trait~family*treatment,data=dataset,random=~1|family)

which gives me an interaction MS and F value in an anova, but seems to  
treat each interaction between each treatment and family as a separate  
fixed factor.  Also VarCorr() doesn't seem to give a variance for the  
interaction term.

What am I doing wrong?

I am sure this should be simple, but the docs seem pretty unclear to  
me on modeling interactions.

Help please!

David Springate



From bates at stat.wisc.edu  Thu Feb 12 15:03:04 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 12 Feb 2009 08:03:04 -0600
Subject: [R-sig-ME] interaction model and variance in lme
In-Reply-To: <20090211221942.30803im8rfmrffgg@webmail.manchester.ac.uk>
References: <20090211221942.30803im8rfmrffgg@webmail.manchester.ac.uk>
Message-ID: <40e66e0b0902120603v3966ef26t9e19cc13ff32cd94@mail.gmail.com>

On Wed, Feb 11, 2009 at 4:19 PM, David Springate
<David.Springate at postgrad.manchester.ac.uk> wrote:
> Hi,

> I am new to R and I have been trying to build a model that I can extract ML
> variance components for the interaction from, but am struggling to make
> sense of the formula.

> I am looking at the interaction of family (random) and two environmental
> treatments (fixed) on a trait.  So far I have tried:

> model=lme(trait~treatment,data=dataset,random=~1|family/treatment)

> But this seems to give me just the treatment nested in the family rather
> than family*treatment values.

Some of the difficulty here may be in nomenclature and notation.  The
phrase "family*treatment values" means different things in SAS and in
R.  It may help if you could describe verbally what you want to obtain
rather than symbolically.

For a model like this I would recommend using lmer from the lme4
package as it has several enhancements relative to lme from the nlme
package.

As you have seen, the model above, which would be written

lmer(trait ~ treatment + (1|family/treatment), dataset)

or, equivalently,

lmer(trait ~ treatment + (1|family) + (1|family:treatment), dataset)

provides fixed effects for the treatment (Intercept and an effect of
one level) plus the random effects for each family plus the random
effects for each family:treatment combination.  (In R an interaction
term is written family:treatment; the notation family*treatment
indicates crossing of fixed effects so that family*treatment expands
to family + treatment + family:treatment.)

Another model incorporating random effects for each level of treatment
within family is

lmer(trait ~ treatment + (treatment|family), dataset)

for which I prefer and alternative expression as

lmer(trait ~ treatment + (0+treatment|family), dataset)

These models are equivalent but have different parameterizations.
Suppose that the treatment levels are called A and B.  Then the
default model matrix for the treatment term provides the intercept and
the indicator of level B.  The model matrix for 0+treatment suppresses
the intercept and provides an indicator for A and an indicator for B.

The notation (0+treatment|family) produces a pair of random effects
for each level of family, one for treatment A and one for treatment B,
and the variance-covariance matrix for these random effects is a
general positive-definite 2x2 matrix.  (In SAS-speak this is an
"unconstrained" variance-covariance matrix but the mathematician in me
will not accept the concept of an unconstrained matrix that is subject
to the constraints of being symmetric and positive definite.)

I hope this helps but I encourage you to follow up on your question if
this did not answer it.


> I also tried:
>
> Model = lme(trait~family*treatment,data=dataset,random=~1|family)
>
> which gives me an interaction MS and F value in an anova, but seems to treat
> each interaction between each treatment and family as a separate fixed
> factor.  Also VarCorr() doesn't seem to give a variance for the interaction
> term.
>
> What am I doing wrong?
>
> I am sure this should be simple, but the docs seem pretty unclear to me on
> modeling interactions.
>
> Help please!
>
> David Springate
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From harlancampbell at gmail.com  Fri Feb 13 15:55:04 2009
From: harlancampbell at gmail.com (H c)
Date: Fri, 13 Feb 2009 09:55:04 -0500
Subject: [R-sig-ME] weighted-ML estimates
Message-ID: <222824550902130655s57dbc49cmd20d5530e8c21003@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090213/52bc0fc7/attachment.pl>

From harlancampbell at gmail.com  Fri Feb 13 15:50:16 2009
From: harlancampbell at gmail.com (H c)
Date: Fri, 13 Feb 2009 09:50:16 -0500
Subject: [R-sig-ME] weighted ML estimates
Message-ID: <222824550902130650n411f3e64h75043fb09a97c90@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090213/63de4a20/attachment.pl>

From bates at stat.wisc.edu  Sat Feb 14 00:23:10 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 13 Feb 2009 17:23:10 -0600
Subject: [R-sig-ME] weighted-ML estimates
In-Reply-To: <222824550902130655s57dbc49cmd20d5530e8c21003@mail.gmail.com>
References: <222824550902130655s57dbc49cmd20d5530e8c21003@mail.gmail.com>
Message-ID: <40e66e0b0902131523h4504c05bu46dbee8d23ea45e@mail.gmail.com>

On Fri, Feb 13, 2009 at 8:55 AM, H c <harlancampbell at gmail.com> wrote:
> Hi,
> We are trying to calculate ML-parameter estimates of a mixed effects models
> where the observations are weighted.  The "weights" option in lmer() with
> RELM=FALSE seems attractive.  Does anyone know the mechanism it uses to
> calculate weighted ML estimates?

The weights are with respect to the observations.  The log-likelihood
depends on a residual sum of squares in the unweighted case.  In the
weighted case a weighted residual sum of squares is used.

> (Is there a paper?).  Also, our model
> includes a serial correlation structure(e.g. AR(1)) among the residuals.  As
> far as I know, no such "cor" option is available in lmer() unlike in nlme.
>  Is this correct and if so, any suggestions on how to proceed?

You are correct.  The lmer function does not incorporate serial
correlation structures.



From bates at stat.wisc.edu  Sat Feb 14 00:40:12 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 13 Feb 2009 17:40:12 -0600
Subject: [R-sig-ME] Correlation of Fixed Effects
In-Reply-To: <F189E18BBAA8B6479F618B9044CF4E9C68DE5CCBFD@REGULUS.bfro.uni-lj.si>
References: <F189E18BBAA8B6479F618B9044CF4E9C68DE5CCBFD@REGULUS.bfro.uni-lj.si>
Message-ID: <40e66e0b0902131540s12d3fceane5c2a54b75fb3977@mail.gmail.com>

On Mon, Feb 9, 2009 at 3:52 PM, Gorjanc Gregor
<Gregor.Gorjanc at bfro.uni-lj.si> wrote:

> The default print method outputs also Correlation of Fixed Effects.
> How is this computed and what does it actually represent?

It is an approximate correlation of the estimator of the fixed
effects.  (I include the word "approximate" because I should but in
this case the approximation is very good.)  I'm not sure how to
explain it better than that.  Suppose that you took an MCMC sample
from the parameters in the model, then you would expect the sample of
the fixed-effects parameters to display a correlation structure like
this matrix.

As for how it is calculated, look in the vignettes for the definition
of a p by p upper triangular matrix called R_X.  It is returned as the
RX slot in the fitted model.  This matrix is part of the Cholesky
factor in the combined model matrices for the penalized least squares
problem that determines the conditional modes of the random effects
and the conditional estimates of the fixed effects.  If we didn't have
any random effects this would be the R matrix from the QR
decomposition of X.  The same calculation that creates the correlation
of the coefficients in a fixed-effects model from R creates the
correlation of the fixed-effects coefficients from RX here.  See

?chol2inv

> I have two models
> that essentially give me the same message, but in one model the correlations
> between covariates are really high 0.9 and higher, while in other model use of
> poly(), reduced correlations a lot! Should I care?
>
> Thanks!
>
> Lep pozdrav / With regards,
>    Gregor Gorjanc
> ----------------------------------------------------------------------
> University of Ljubljana       PhD student
> Biotechnical Faculty          www: http://gregor.gorjanc.googlepages.com
> Department of Animal Science  blog: http://ggorjan.blogspot.com
> Groblje 3                     mail: gregor.gorjanc <at> bfro.uni-lj.si
> SI-1230 Domzale               fax: +386 (0)1 72 17 888
> Slovenia, Europe              tel: +386 (0)1 72 17 861
> ----------------------------------------------------------------------
>



From tahirajamil at yahoo.com  Sun Feb 15 23:44:16 2009
From: tahirajamil at yahoo.com (Tahira Jamil)
Date: Sun, 15 Feb 2009 14:44:16 -0800 (PST)
Subject: [R-sig-ME] model selection in lme4
Message-ID: <505520.96158.qm@web50803.mail.re2.yahoo.com>


Hi
I have run  GLMM models in lme4 with different fixed effects and random effects . But now the problem is model selction Is AIC or BIC results are definitive specially for Gernalized linear mixed models or what critera should I use for model selction. So I can decide which explantory variable should be in the model because I have more than 10 explantory variables and some are entering in the model as random effect. In some cases If AIC has lower value but BIC is comparatively high. 
    some suggestion for model selection would be highly appricated.

    WIth best wishes
    T Jamil 
    Ph.D student
    Biometris 
    Wageningen University and Research centre Netherlands.



From desja004 at umn.edu  Sun Feb 15 23:53:25 2009
From: desja004 at umn.edu (Christopher David Desjardins)
Date: Sun, 15 Feb 2009 16:53:25 -0600
Subject: [R-sig-ME] model selection in lme4
In-Reply-To: <505520.96158.qm@web50803.mail.re2.yahoo.com>
References: <505520.96158.qm@web50803.mail.re2.yahoo.com>
Message-ID: <15AA2AE3-B576-4470-8B01-77C47716A601@umn.edu>

You could use either the BIC or the AIC. My understanding is that the  
AIC tends to favor overly complex models whereas the BIC tends to  
favor parsimonious models. I am generally inclined to always use the  
BIC. If you have a small sample size you might also consider using the  
AICC which is a correction of the AIC for small sample sizes. That  
said, in my experience the AICC still selects more complex models than  
the BIC. Also if you have nested models you could use the chi-square  
tests.
Cheers,
Chris

On Feb 15, 2009, at 4:44 PM, Tahira Jamil wrote:

>
> Hi
> I have run  GLMM models in lme4 with different fixed effects and  
> random effects . But now the problem is model selction Is AIC or BIC  
> results are definitive specially for Gernalized linear mixed models  
> or what critera should I use for model selction. So I can decide  
> which explantory variable should be in the model because I have more  
> than 10 explantory variables and some are entering in the model as  
> random effect. In some cases If AIC has lower value but BIC is  
> comparatively high.
>    some suggestion for model selection would be highly appricated.
>
>    WIth best wishes
>    T Jamil
>    Ph.D student
>    Biometris
>    Wageningen University and Research centre Netherlands.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-----------------
Christopher David Desjardins
Ph.D. Student
Quantitative Methods in Education
Department of Educational Psychology
University of  Minnesota
http://blog.lib.umn.edu/desja004/educationalpsychology/



From bolker at ufl.edu  Mon Feb 16 00:07:04 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Sun, 15 Feb 2009 18:07:04 -0500
Subject: [R-sig-ME] model selection in lme4
In-Reply-To: <15AA2AE3-B576-4470-8B01-77C47716A601@umn.edu>
References: <505520.96158.qm@web50803.mail.re2.yahoo.com>
	<15AA2AE3-B576-4470-8B01-77C47716A601@umn.edu>
Message-ID: <4998A018.2030306@ufl.edu>

  Some caution on this advice: you seem to be quoting
the general advice on AIC/BIC/AICc

  1. The AIC/BIC distinction is between "best prediction"
and "consistent estimation of true model" dimension, e.g.

Yang, Yuhong. 2005. Can the strengths of AIC and BIC be shared? A
conflict between model identification and regression estimation.
Biometrika 92, no. 4 (December 1): 937-950. doi:10.1093/biomet/92.4.937.

  I favor AIC on these grounds, but you can decide for yourself.

  2. For models with different random effects, AIC and BIC share
a "degrees of freedom counting" problem with all model selection
approaches -- there are two aspects here, (1) whether you are
focused on individual-level prediction or population-level
prediction (Vaida and Blanchard 2005, Spiegelhalter et al 2002)
and (2) whether AIC/BIC share the boundary problems that
also apply to likelihood ratio tests (Greven, Sonja. 2008. Non-Standard
Problems in Inference for Additive and Linear Mixed Models. G?ttingen,
Germany: Cuvillier Verlag.
http://www.cuvillier.de/flycms/en/html/30/-UickI3zKPS,3cEY=/Buchdetails.html?SID=wVZnpL8f0fbc.
)

  3. AIC and BIC are asymptotic tests (which can be especially
problematic with random effects problems, when there are not
large number of random blocks -- this makes likelihood ratio
tests NOT OK for fixed-effect comparisons with small numbers
of blocks (Pinheiro and Bates 2000)).  If you want to use
AICc then you are back to counting residual degrees of freedom ...
as far as I know there isn't much guidance available on this
issue.

  My bottom line:

  I would go ahead and use (Q)AIC with caution for data sets with large
(?) numbers of blocks.  With smaller numbers of blocks I would probably
try to find some kind of randomization/permutation approach to get a
sense of the relevant size of delta-AIC values ...
   ... or damn the torpedoes and see if you can get away with straight
AIC.

  Ben Bolker

Christopher David Desjardins wrote:
> You could use either the BIC or the AIC. My understanding is that the  
> AIC tends to favor overly complex models whereas the BIC tends to  
> favor parsimonious models. I am generally inclined to always use the  
> BIC. If you have a small sample size you might also consider using the  
> AICC which is a correction of the AIC for small sample sizes. That  
> said, in my experience the AICC still selects more complex models than  
> the BIC. Also if you have nested models you could use the chi-square  
> tests.
> Cheers,
> Chris
> 
> On Feb 15, 2009, at 4:44 PM, Tahira Jamil wrote:
> 
>> Hi
>> I have run  GLMM models in lme4 with different fixed effects and  
>> random effects . But now the problem is model selction Is AIC or BIC  
>> results are definitive specially for Gernalized linear mixed models  
>> or what critera should I use for model selction. So I can decide  
>> which explantory variable should be in the model because I have more  
>> than 10 explantory variables and some are entering in the model as  
>> random effect. In some cases If AIC has lower value but BIC is  
>> comparatively high.
>>    some suggestion for model selection would be highly appricated.
>>
>>    WIth best wishes
>>    T Jamil
>>    Ph.D student
>>    Biometris
>>    Wageningen University and Research centre Netherlands.
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> -----------------
> Christopher David Desjardins
> Ph.D. Student
> Quantitative Methods in Education
> Department of Educational Psychology
> University of  Minnesota
> http://blog.lib.umn.edu/desja004/educationalpsychology/
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From bolker at ufl.edu  Mon Feb 16 02:50:30 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Sun, 15 Feb 2009 20:50:30 -0500
Subject: [R-sig-ME] model selection in lme4
In-Reply-To: <B9D1301370916C44B5874AF340C18B9B7F913660EE@VMAILB.uoa.abdn.ac.uk>
References: <505520.96158.qm@web50803.mail.re2.yahoo.com>	<15AA2AE3-B576-4470-8B01-77C47716A601@umn.edu>,
	<4998A018.2030306@ufl.edu>
	<B9D1301370916C44B5874AF340C18B9B7F913660EE@VMAILB.uoa.abdn.ac.uk>
Message-ID: <4998C666.5060801@ufl.edu>

  It would be better to use AICc, but I'm not sure what I would
use for "number of parameters" for a random effect with n
levels: any number between 0.5 and n seems plausible!
Someone should send Shane Richards (who has done some
very nice work testing (Q)AIC(c) in ecological settings)
and see if he's willing to tackle this one, although I can
imagine he's getting sick of this kind of exercise ...

  Ben Bolker

Renwick, A. R. wrote:
> Just a quickie Ben,
> Are you saying that you would use AIC rather than AICc even with
> small sample size - due to difficulty in counting residual degrees of
freedom?
> Thanks
> Anna
> p.s. this forum really is fantastic
> 
> ________________________________________
> From: r-sig-mixed-models-bounces at r-project.org [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker [bolker at ufl.edu]
> Sent: 15 February 2009 23:07
> To: Christopher David Desjardins
> Cc: r-sig-mixed-models at r-project.org; tahirajamil at yahoo.com
> Subject: Re: [R-sig-ME] model selection in lme4
> 
>   Some caution on this advice: you seem to be quoting
> the general advice on AIC/BIC/AICc
> 
>   1. The AIC/BIC distinction is between "best prediction"
> and "consistent estimation of true model" dimension, e.g.
> 
> Yang, Yuhong. 2005. Can the strengths of AIC and BIC be shared? A
> conflict between model identification and regression estimation.
> Biometrika 92, no. 4 (December 1): 937-950. doi:10.1093/biomet/92.4.937.
> 
>   I favor AIC on these grounds, but you can decide for yourself.
> 
>   2. For models with different random effects, AIC and BIC share
> a "degrees of freedom counting" problem with all model selection
> approaches -- there are two aspects here, (1) whether you are
> focused on individual-level prediction or population-level
> prediction (Vaida and Blanchard 2005, Spiegelhalter et al 2002)
> and (2) whether AIC/BIC share the boundary problems that
> also apply to likelihood ratio tests (Greven, Sonja. 2008. Non-Standard
> Problems in Inference for Additive and Linear Mixed Models. G?ttingen,
> Germany: Cuvillier Verlag.
> http://www.cuvillier.de/flycms/en/html/30/-UickI3zKPS,3cEY=/Buchdetails.html?SID=wVZnpL8f0fbc.
> )
> 
>   3. AIC and BIC are asymptotic tests (which can be especially
> problematic with random effects problems, when there are not
> large number of random blocks -- this makes likelihood ratio
> tests NOT OK for fixed-effect comparisons with small numbers
> of blocks (Pinheiro and Bates 2000)).  If you want to use
> AICc then you are back to counting residual degrees of freedom ...
> as far as I know there isn't much guidance available on this
> issue.
> 
>   My bottom line:
> 
>   I would go ahead and use (Q)AIC with caution for data sets with large
> (?) numbers of blocks.  With smaller numbers of blocks I would probably
> try to find some kind of randomization/permutation approach to get a
> sense of the relevant size of delta-AIC values ...
>    ... or damn the torpedoes and see if you can get away with straight
> AIC.
> 
>   Ben Bolker
> 
> Christopher David Desjardins wrote:
>> You could use either the BIC or the AIC. My understanding is that the
>> AIC tends to favor overly complex models whereas the BIC tends to
>> favor parsimonious models. I am generally inclined to always use the
>> BIC. If you have a small sample size you might also consider using the
>> AICC which is a correction of the AIC for small sample sizes. That
>> said, in my experience the AICC still selects more complex models than
>> the BIC. Also if you have nested models you could use the chi-square
>> tests.
>> Cheers,
>> Chris
>>
>> On Feb 15, 2009, at 4:44 PM, Tahira Jamil wrote:
>>
>>> Hi
>>> I have run  GLMM models in lme4 with different fixed effects and
>>> random effects . But now the problem is model selction Is AIC or BIC
>>> results are definitive specially for Gernalized linear mixed models
>>> or what critera should I use for model selction. So I can decide
>>> which explantory variable should be in the model because I have more
>>> than 10 explantory variables and some are entering in the model as
>>> random effect. In some cases If AIC has lower value but BIC is
>>> comparatively high.
>>>    some suggestion for model selection would be highly appricated.
>>>
>>>    WIth best wishes
>>>    T Jamil
>>>    Ph.D student
>>>    Biometris
>>>    Wageningen University and Research centre Netherlands.
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> -----------------
>> Christopher David Desjardins
>> Ph.D. Student
>> Quantitative Methods in Education
>> Department of Educational Psychology
>> University of  Minnesota
>> http://blog.lib.umn.edu/desja004/educationalpsychology/
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> --
> Ben Bolker
> Associate professor, Biology Dep't, Univ. of Florida
> bolker at ufl.edu / www.zoology.ufl.edu/bolker
> GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> The University of Aberdeen is a charity registered in Scotland, No SC013683.


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From desja004 at umn.edu  Mon Feb 16 03:23:12 2009
From: desja004 at umn.edu (Christopher David Desjardins)
Date: Sun, 15 Feb 2009 20:23:12 -0600
Subject: [R-sig-ME] model selection in lme4
In-Reply-To: <4998C666.5060801@ufl.edu>
References: <505520.96158.qm@web50803.mail.re2.yahoo.com>
	<B9D1301370916C44B5874AF340C18B9B7F913660EE@VMAILB.uoa.abdn.ac.uk>
	<4998C666.5060801@ufl.edu>
Message-ID: <200902152023.12346.desja004@umn.edu>

For a discussion of BIC, please see Raftery (1995) in Sociological 
Methodology. Before you commit yourself on the AIC, I do encourage you to 
look at your BIC. In the models I've run when there is disagreement between 
the BIC and the AIC, it's usually that the AIC selects the overly complex 
model and includes unnecessary parameters.
Cheers,
Chris

On Sunday 15 February 2009 19:50:30 Ben Bolker wrote:
>   It would be better to use AICc, but I'm not sure what I would
> use for "number of parameters" for a random effect with n
> levels: any number between 0.5 and n seems plausible!
> Someone should send Shane Richards (who has done some
> very nice work testing (Q)AIC(c) in ecological settings)
> and see if he's willing to tackle this one, although I can
> imagine he's getting sick of this kind of exercise ...
>
>   Ben Bolker
>
> Renwick, A. R. wrote:
> > Just a quickie Ben,
> > Are you saying that you would use AIC rather than AICc even with
> > small sample size - due to difficulty in counting residual degrees of
>
> freedom?
>
> > Thanks
> > Anna
> > p.s. this forum really is fantastic
> >
> > ________________________________________
> > From: r-sig-mixed-models-bounces at r-project.org
> > [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker
> > [bolker at ufl.edu] Sent: 15 February 2009 23:07
> > To: Christopher David Desjardins
> > Cc: r-sig-mixed-models at r-project.org; tahirajamil at yahoo.com
> > Subject: Re: [R-sig-ME] model selection in lme4
> >
> >   Some caution on this advice: you seem to be quoting
> > the general advice on AIC/BIC/AICc
> >
> >   1. The AIC/BIC distinction is between "best prediction"
> > and "consistent estimation of true model" dimension, e.g.
> >
> > Yang, Yuhong. 2005. Can the strengths of AIC and BIC be shared? A
> > conflict between model identification and regression estimation.
> > Biometrika 92, no. 4 (December 1): 937-950. doi:10.1093/biomet/92.4.937.
> >
> >   I favor AIC on these grounds, but you can decide for yourself.
> >
> >   2. For models with different random effects, AIC and BIC share
> > a "degrees of freedom counting" problem with all model selection
> > approaches -- there are two aspects here, (1) whether you are
> > focused on individual-level prediction or population-level
> > prediction (Vaida and Blanchard 2005, Spiegelhalter et al 2002)
> > and (2) whether AIC/BIC share the boundary problems that
> > also apply to likelihood ratio tests (Greven, Sonja. 2008. Non-Standard
> > Problems in Inference for Additive and Linear Mixed Models. G?ttingen,
> > Germany: Cuvillier Verlag.
> > http://www.cuvillier.de/flycms/en/html/30/-UickI3zKPS,3cEY=/Buchdetails.h
> >tml?SID=wVZnpL8f0fbc. )
> >
> >   3. AIC and BIC are asymptotic tests (which can be especially
> > problematic with random effects problems, when there are not
> > large number of random blocks -- this makes likelihood ratio
> > tests NOT OK for fixed-effect comparisons with small numbers
> > of blocks (Pinheiro and Bates 2000)).  If you want to use
> > AICc then you are back to counting residual degrees of freedom ...
> > as far as I know there isn't much guidance available on this
> > issue.
> >
> >   My bottom line:
> >
> >   I would go ahead and use (Q)AIC with caution for data sets with large
> > (?) numbers of blocks.  With smaller numbers of blocks I would probably
> > try to find some kind of randomization/permutation approach to get a
> > sense of the relevant size of delta-AIC values ...
> >    ... or damn the torpedoes and see if you can get away with straight
> > AIC.
> >
> >   Ben Bolker
> >
> > Christopher David Desjardins wrote:
> >> You could use either the BIC or the AIC. My understanding is that the
> >> AIC tends to favor overly complex models whereas the BIC tends to
> >> favor parsimonious models. I am generally inclined to always use the
> >> BIC. If you have a small sample size you might also consider using the
> >> AICC which is a correction of the AIC for small sample sizes. That
> >> said, in my experience the AICC still selects more complex models than
> >> the BIC. Also if you have nested models you could use the chi-square
> >> tests.
> >> Cheers,
> >> Chris
> >>
> >> On Feb 15, 2009, at 4:44 PM, Tahira Jamil wrote:
> >>> Hi
> >>> I have run  GLMM models in lme4 with different fixed effects and
> >>> random effects . But now the problem is model selction Is AIC or BIC
> >>> results are definitive specially for Gernalized linear mixed models
> >>> or what critera should I use for model selction. So I can decide
> >>> which explantory variable should be in the model because I have more
> >>> than 10 explantory variables and some are entering in the model as
> >>> random effect. In some cases If AIC has lower value but BIC is
> >>> comparatively high.
> >>>    some suggestion for model selection would be highly appricated.
> >>>
> >>>    WIth best wishes
> >>>    T Jamil
> >>>    Ph.D student
> >>>    Biometris
> >>>    Wageningen University and Research centre Netherlands.
> >>>
> >>> _______________________________________________
> >>> R-sig-mixed-models at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >> -----------------
> >> Christopher David Desjardins
> >> Ph.D. Student
> >> Quantitative Methods in Education
> >> Department of Educational Psychology
> >> University of  Minnesota
> >> http://blog.lib.umn.edu/desja004/educationalpsychology/
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> > --
> > Ben Bolker
> > Associate professor, Biology Dep't, Univ. of Florida
> > bolker at ufl.edu / www.zoology.ufl.edu/bolker
> > GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> >
> > The University of Aberdeen is a charity registered in Scotland, No
> > SC013683.



From bolker at ufl.edu  Mon Feb 16 04:02:42 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Sun, 15 Feb 2009 22:02:42 -0500
Subject: [R-sig-ME] model selection in lme4
In-Reply-To: <200902152023.12346.desja004@umn.edu>
References: <505520.96158.qm@web50803.mail.re2.yahoo.com>
	<B9D1301370916C44B5874AF340C18B9B7F913660EE@VMAILB.uoa.abdn.ac.uk>
	<4998C666.5060801@ufl.edu> <200902152023.12346.desja004@umn.edu>
Message-ID: <4998D752.1000107@ufl.edu>

Took a (very) quick look at Raftery, which all seems sensible
and well-argued.  However ... the paper contrasts Bayes/BIC
with classical hypothesis testing.  Many of the points listed on p. 155
(better assessment of evidence, applicability to non-nested models, take
model uncertainty into account, allow model averaging, easy to
implement) apply to AIC as well as BIC.  BIC does have many good
qualities (approximation to Bayes factor, sensible "flat prior"
interpretation, statistical consistency, ...).  But the crux of the
argument between BIC and AIC is the difference in their objective. BIC
aims to identify the "true model", which essentially assumes that there
is a sharp cutoff between parameters/processes that are in the model and
those that are out. Burnham and Anderson have a lot to say about
tapering effect sizes; they are zealots about AIC, and I often discount
their enthusiasm, but after much percolation I've decided that AIC
really does make sense for the kinds of questions I (and many
ecologists) tend to ask.

   When you say that AIC selects an overly complex model, how
do you know what the correct model is and which parameters are
unnecessary?  Is this a case of fitting to simulation output?
In that case I might bring up B&A's "tapering effects" argument
again -- selecting the correct model with a fixed number of parameters
with non-tapering effects is what BIC is for, not what AIC is for.

  I have tried to say this more coherently at
http://emdbolker.wikidot.com/blog:aic-vs-bic

  As an aside, I don't have a vested interest in this, and I don't
claim that AIC is better for everything ... just that it seems
most ecologists are working with "true models" that are of
arbitrarily large dimension with tapering effects, which is where
AIC should select the model with the best predictive capability ...

  Ben Bolker

Christopher David Desjardins wrote:
> For a discussion of BIC, please see Raftery (1995) in Sociological 
> Methodology. Before you commit yourself on the AIC, I do encourage you to 
> look at your BIC. In the models I've run when there is disagreement between 
> the BIC and the AIC, it's usually that the AIC selects the overly complex 
> model and includes unnecessary parameters.
> Cheers,
> Chris
> 
> On Sunday 15 February 2009 19:50:30 Ben Bolker wrote:
>>   It would be better to use AICc, but I'm not sure what I would
>> use for "number of parameters" for a random effect with n
>> levels: any number between 0.5 and n seems plausible!
>> Someone should send Shane Richards (who has done some
>> very nice work testing (Q)AIC(c) in ecological settings)
>> and see if he's willing to tackle this one, although I can
>> imagine he's getting sick of this kind of exercise ...
>>
>>   Ben Bolker
>>
>> Renwick, A. R. wrote:
>>> Just a quickie Ben,
>>> Are you saying that you would use AIC rather than AICc even with
>>> small sample size - due to difficulty in counting residual degrees of
>> freedom?
>>
>>> Thanks
>>> Anna
>>> p.s. this forum really is fantastic
>>>
>>> ________________________________________
>>> From: r-sig-mixed-models-bounces at r-project.org
>>> [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker
>>> [bolker at ufl.edu] Sent: 15 February 2009 23:07
>>> To: Christopher David Desjardins
>>> Cc: r-sig-mixed-models at r-project.org; tahirajamil at yahoo.com
>>> Subject: Re: [R-sig-ME] model selection in lme4
>>>
>>>   Some caution on this advice: you seem to be quoting
>>> the general advice on AIC/BIC/AICc
>>>
>>>   1. The AIC/BIC distinction is between "best prediction"
>>> and "consistent estimation of true model" dimension, e.g.
>>>
>>> Yang, Yuhong. 2005. Can the strengths of AIC and BIC be shared? A
>>> conflict between model identification and regression estimation.
>>> Biometrika 92, no. 4 (December 1): 937-950. doi:10.1093/biomet/92.4.937.
>>>
>>>   I favor AIC on these grounds, but you can decide for yourself.
>>>
>>>   2. For models with different random effects, AIC and BIC share
>>> a "degrees of freedom counting" problem with all model selection
>>> approaches -- there are two aspects here, (1) whether you are
>>> focused on individual-level prediction or population-level
>>> prediction (Vaida and Blanchard 2005, Spiegelhalter et al 2002)
>>> and (2) whether AIC/BIC share the boundary problems that
>>> also apply to likelihood ratio tests (Greven, Sonja. 2008. Non-Standard
>>> Problems in Inference for Additive and Linear Mixed Models. G?ttingen,
>>> Germany: Cuvillier Verlag.
>>> http://www.cuvillier.de/flycms/en/html/30/-UickI3zKPS,3cEY=/Buchdetails.h
>>> tml?SID=wVZnpL8f0fbc. )
>>>
>>>   3. AIC and BIC are asymptotic tests (which can be especially
>>> problematic with random effects problems, when there are not
>>> large number of random blocks -- this makes likelihood ratio
>>> tests NOT OK for fixed-effect comparisons with small numbers
>>> of blocks (Pinheiro and Bates 2000)).  If you want to use
>>> AICc then you are back to counting residual degrees of freedom ...
>>> as far as I know there isn't much guidance available on this
>>> issue.
>>>
>>>   My bottom line:
>>>
>>>   I would go ahead and use (Q)AIC with caution for data sets with large
>>> (?) numbers of blocks.  With smaller numbers of blocks I would probably
>>> try to find some kind of randomization/permutation approach to get a
>>> sense of the relevant size of delta-AIC values ...
>>>    ... or damn the torpedoes and see if you can get away with straight
>>> AIC.
>>>
>>>   Ben Bolker
>>>
>>> Christopher David Desjardins wrote:
>>>> You could use either the BIC or the AIC. My understanding is that the
>>>> AIC tends to favor overly complex models whereas the BIC tends to
>>>> favor parsimonious models. I am generally inclined to always use the
>>>> BIC. If you have a small sample size you might also consider using the
>>>> AICC which is a correction of the AIC for small sample sizes. That
>>>> said, in my experience the AICC still selects more complex models than
>>>> the BIC. Also if you have nested models you could use the chi-square
>>>> tests.
>>>> Cheers,
>>>> Chris
>>>>
>>>> On Feb 15, 2009, at 4:44 PM, Tahira Jamil wrote:
>>>>> Hi
>>>>> I have run  GLMM models in lme4 with different fixed effects and
>>>>> random effects . But now the problem is model selction Is AIC or BIC
>>>>> results are definitive specially for Gernalized linear mixed models
>>>>> or what critera should I use for model selction. So I can decide
>>>>> which explantory variable should be in the model because I have more
>>>>> than 10 explantory variables and some are entering in the model as
>>>>> random effect. In some cases If AIC has lower value but BIC is
>>>>> comparatively high.
>>>>>    some suggestion for model selection would be highly appricated.
>>>>>
>>>>>    WIth best wishes
>>>>>    T Jamil
>>>>>    Ph.D student
>>>>>    Biometris
>>>>>    Wageningen University and Research centre Netherlands.
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> -----------------
>>>> Christopher David Desjardins
>>>> Ph.D. Student
>>>> Quantitative Methods in Education
>>>> Department of Educational Psychology
>>>> University of  Minnesota
>>>> http://blog.lib.umn.edu/desja004/educationalpsychology/
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> --
>>> Ben Bolker
>>> Associate professor, Biology Dep't, Univ. of Florida
>>> bolker at ufl.edu / www.zoology.ufl.edu/bolker
>>> GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>> The University of Aberdeen is a charity registered in Scotland, No
>>> SC013683.
> 
> 


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From s.blomberg1 at uq.edu.au  Mon Feb 16 05:15:45 2009
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Mon, 16 Feb 2009 14:15:45 +1000
Subject: [R-sig-ME] model selection in lme4
In-Reply-To: <200902152023.12346.desja004@umn.edu>
References: <505520.96158.qm@web50803.mail.re2.yahoo.com>
	<B9D1301370916C44B5874AF340C18B9B7F913660EE@VMAILB.uoa.abdn.ac.uk>
	<4998C666.5060801@ufl.edu>  <200902152023.12346.desja004@umn.edu>
Message-ID: <1234757745.4787.53.camel@sib-sblomber01d.sib.uq.edu.au>

Vaida and Blanchard Biometrika [(2005), 92, 2, pp. 351?370 Conditional
Akaike information for mixed-effects models] discuss using AIC for model
selection in mixed-effects models, and make recommendations. There is
also a follow-up not by Liang, Wu and Zhou. Biometrika (2008), 95, 3,
pp. 773?778 A note on conditional AIC for linear mixed-effects models.

The general message is that the "type" of AIC statistic will depend on
your motivation for model selection. Is it the fixed effects part of the
model that is of most interest? Or are the random effects of specific
interest too? This "focus" will determine the number of "effective
parameters" in the penalty term (using results from Hodges, J.S. and
Sargent, D. J. (2001). Counting degrees of freedom in hierarchical and
other richly parameterized models. Biometrika 88, 367?79). There is also
the issue of REML v ML estimation...

Cheers,

Simon.

On Sun, 2009-02-15 at 20:23 -0600, Christopher David Desjardins wrote:
> For a discussion of BIC, please see Raftery (1995) in Sociological 
> Methodology. Before you commit yourself on the AIC, I do encourage you to 
> look at your BIC. In the models I've run when there is disagreement between 
> the BIC and the AIC, it's usually that the AIC selects the overly complex 
> model and includes unnecessary parameters.
> Cheers,
> Chris
> 
> On Sunday 15 February 2009 19:50:30 Ben Bolker wrote:
> >   It would be better to use AICc, but I'm not sure what I would
> > use for "number of parameters" for a random effect with n
> > levels: any number between 0.5 and n seems plausible!
> > Someone should send Shane Richards (who has done some
> > very nice work testing (Q)AIC(c) in ecological settings)
> > and see if he's willing to tackle this one, although I can
> > imagine he's getting sick of this kind of exercise ...
> >
> >   Ben Bolker
> >
> > Renwick, A. R. wrote:
> > > Just a quickie Ben,
> > > Are you saying that you would use AIC rather than AICc even with
> > > small sample size - due to difficulty in counting residual degrees of
> >
> > freedom?
> >
> > > Thanks
> > > Anna
> > > p.s. this forum really is fantastic
> > >
> > > ________________________________________
> > > From: r-sig-mixed-models-bounces at r-project.org
> > > [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker
> > > [bolker at ufl.edu] Sent: 15 February 2009 23:07
> > > To: Christopher David Desjardins
> > > Cc: r-sig-mixed-models at r-project.org; tahirajamil at yahoo.com
> > > Subject: Re: [R-sig-ME] model selection in lme4
> > >
> > >   Some caution on this advice: you seem to be quoting
> > > the general advice on AIC/BIC/AICc
> > >
> > >   1. The AIC/BIC distinction is between "best prediction"
> > > and "consistent estimation of true model" dimension, e.g.
> > >
> > > Yang, Yuhong. 2005. Can the strengths of AIC and BIC be shared? A
> > > conflict between model identification and regression estimation.
> > > Biometrika 92, no. 4 (December 1): 937-950. doi:10.1093/biomet/92.4.937.
> > >
> > >   I favor AIC on these grounds, but you can decide for yourself.
> > >
> > >   2. For models with different random effects, AIC and BIC share
> > > a "degrees of freedom counting" problem with all model selection
> > > approaches -- there are two aspects here, (1) whether you are
> > > focused on individual-level prediction or population-level
> > > prediction (Vaida and Blanchard 2005, Spiegelhalter et al 2002)
> > > and (2) whether AIC/BIC share the boundary problems that
> > > also apply to likelihood ratio tests (Greven, Sonja. 2008. Non-Standard
> > > Problems in Inference for Additive and Linear Mixed Models. G?ttingen,
> > > Germany: Cuvillier Verlag.
> > > http://www.cuvillier.de/flycms/en/html/30/-UickI3zKPS,3cEY=/Buchdetails.h
> > >tml?SID=wVZnpL8f0fbc. )
> > >
> > >   3. AIC and BIC are asymptotic tests (which can be especially
> > > problematic with random effects problems, when there are not
> > > large number of random blocks -- this makes likelihood ratio
> > > tests NOT OK for fixed-effect comparisons with small numbers
> > > of blocks (Pinheiro and Bates 2000)).  If you want to use
> > > AICc then you are back to counting residual degrees of freedom ...
> > > as far as I know there isn't much guidance available on this
> > > issue.
> > >
> > >   My bottom line:
> > >
> > >   I would go ahead and use (Q)AIC with caution for data sets with large
> > > (?) numbers of blocks.  With smaller numbers of blocks I would probably
> > > try to find some kind of randomization/permutation approach to get a
> > > sense of the relevant size of delta-AIC values ...
> > >    ... or damn the torpedoes and see if you can get away with straight
> > > AIC.
> > >
> > >   Ben Bolker
> > >
> > > Christopher David Desjardins wrote:
> > >> You could use either the BIC or the AIC. My understanding is that the
> > >> AIC tends to favor overly complex models whereas the BIC tends to
> > >> favor parsimonious models. I am generally inclined to always use the
> > >> BIC. If you have a small sample size you might also consider using the
> > >> AICC which is a correction of the AIC for small sample sizes. That
> > >> said, in my experience the AICC still selects more complex models than
> > >> the BIC. Also if you have nested models you could use the chi-square
> > >> tests.
> > >> Cheers,
> > >> Chris
> > >>
> > >> On Feb 15, 2009, at 4:44 PM, Tahira Jamil wrote:
> > >>> Hi
> > >>> I have run  GLMM models in lme4 with different fixed effects and
> > >>> random effects . But now the problem is model selction Is AIC or BIC
> > >>> results are definitive specially for Gernalized linear mixed models
> > >>> or what critera should I use for model selction. So I can decide
> > >>> which explantory variable should be in the model because I have more
> > >>> than 10 explantory variables and some are entering in the model as
> > >>> random effect. In some cases If AIC has lower value but BIC is
> > >>> comparatively high.
> > >>>    some suggestion for model selection would be highly appricated.
> > >>>
> > >>>    WIth best wishes
> > >>>    T Jamil
> > >>>    Ph.D student
> > >>>    Biometris
> > >>>    Wageningen University and Research centre Netherlands.
> > >>>
> > >>> _______________________________________________
> > >>> R-sig-mixed-models at r-project.org mailing list
> > >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >>
> > >> -----------------
> > >> Christopher David Desjardins
> > >> Ph.D. Student
> > >> Quantitative Methods in Education
> > >> Department of Educational Psychology
> > >> University of  Minnesota
> > >> http://blog.lib.umn.edu/desja004/educationalpsychology/
> > >>
> > >> _______________________________________________
> > >> R-sig-mixed-models at r-project.org mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >
> > > --
> > > Ben Bolker
> > > Associate professor, Biology Dep't, Univ. of Florida
> > > bolker at ufl.edu / www.zoology.ufl.edu/bolker
> > > GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
> > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >
> > >
> > > The University of Aberdeen is a charity registered in Scotland, No
> > > SC013683.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat. 
Lecturer and Consultant Statistician 
School of Biological Sciences
The University of Queensland 
St. Lucia Queensland 4072 
Australia
Room 320 Goddard Building (8)
T: +61 7 3365 2506
http://www.uq.edu.au/~uqsblomb
email: S.Blomberg1_at_uq.edu.au

Policies:
1.  I will NOT analyse your data for you.
2.  Your deadline is your problem.

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer can 
be extracted from a given body of data. - John Tukey.



From melissayen at gmail.com  Mon Feb 16 10:46:17 2009
From: melissayen at gmail.com (Miao-Hsuan Yen)
Date: Mon, 16 Feb 2009 17:46:17 +0800
Subject: [R-sig-ME] factors with multiple levels
Message-ID: <86f8e5070902160146g4b096ff8s6a9f79cb23cd162f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090216/fd16a8bb/attachment.pl>

From maechler at stat.math.ethz.ch  Mon Feb 16 12:12:16 2009
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 16 Feb 2009 12:12:16 +0100
Subject: [R-sig-ME] factors with multiple levels
In-Reply-To: <86f8e5070902160146g4b096ff8s6a9f79cb23cd162f@mail.gmail.com>
References: <86f8e5070902160146g4b096ff8s6a9f79cb23cd162f@mail.gmail.com>
Message-ID: <18841.18960.837398.233460@stat.math.ethz.ch>

>>>>> "MY" == Miao-Hsuan Yen <melissayen at gmail.com>
>>>>>     on Mon, 16 Feb 2009 17:46:17 +0800 writes:

    MY> Dear lmer list I have one within-subject factor with 3
    MY> levels (A1 A2 A3). By default, I will get an intercept
    MY> for the reference group (A3) and two estimates for A1
    MY> and A2 compared to A3. How can I get an estimate for the
    MY> main effect of factor A as the traditional ANOVA does?
    MY> Is there any function or option that allows me to have
    MY> an overall test?

    MY> In addition, if I have two within-subject factors (A1 A2
    MY> A3 and B1 B2) and their interactions, I will get 2
    MY> estimates for factor A, 1 estimate for factor B and 2
    MY> estimates for the interaction. A similar question is,
    MY> how can I get an estimate for the interaction as the
    MY> ANOVA does?

To both questions:

You compare the model *with* the corresponding factor (or interaction)
term with a model *without* the corresponding term.
That is *the* (generalization of the) classical ANOVA,
and works for many kinds of models in  S and R  via

anova(<model1>,  <model2>, ..)

--> the beginning of 'Examples' on    help(lmer)  aka  ?lmer
does exactly do this ..

    MY> Thanks a lot.

you're welcome;
Martin Maechler, ETH Zurich

    MY> Best, Miao-Hsuan Yen

    MY> 	[[alternative HTML version deleted]]

    MY> _______________________________________________
    MY> R-sig-mixed-models at r-project.org mailing list
    MY> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From cddesjardins at gmail.com  Mon Feb 16 03:20:47 2009
From: cddesjardins at gmail.com (Christopher David Desjardins)
Date: Sun, 15 Feb 2009 20:20:47 -0600
Subject: [R-sig-ME] model selection in lme4
In-Reply-To: <4998C666.5060801@ufl.edu>
References: <505520.96158.qm@web50803.mail.re2.yahoo.com>
	<B9D1301370916C44B5874AF340C18B9B7F913660EE@VMAILB.uoa.abdn.ac.uk>
	<4998C666.5060801@ufl.edu>
Message-ID: <200902152020.47969.cddesjardins@gmail.com>

For a discussion of BIC, please see Raftery (1995) in Sociological 
Methodology. Before you commit yourself on the AIC, I do encourage you to 
look at your BIC. In the models I've run when there is disagreement between 
the BIC and the AIC, it's usually that the AIC selects the overly complex 
model and includes unnecessary parameters.
Cheers,
Chris

On Sunday 15 February 2009 19:50:30 Ben Bolker wrote:
>   It would be better to use AICc, but I'm not sure what I would
> use for "number of parameters" for a random effect with n
> levels: any number between 0.5 and n seems plausible!
> Someone should send Shane Richards (who has done some
> very nice work testing (Q)AIC(c) in ecological settings)
> and see if he's willing to tackle this one, although I can
> imagine he's getting sick of this kind of exercise ...
>
>   Ben Bolker
>
> Renwick, A. R. wrote:
> > Just a quickie Ben,
> > Are you saying that you would use AIC rather than AICc even with
> > small sample size - due to difficulty in counting residual degrees of
>
> freedom?
>
> > Thanks
> > Anna
> > p.s. this forum really is fantastic
> >
> > ________________________________________
> > From: r-sig-mixed-models-bounces at r-project.org
> > [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker
> > [bolker at ufl.edu] Sent: 15 February 2009 23:07
> > To: Christopher David Desjardins
> > Cc: r-sig-mixed-models at r-project.org; tahirajamil at yahoo.com
> > Subject: Re: [R-sig-ME] model selection in lme4
> >
> >   Some caution on this advice: you seem to be quoting
> > the general advice on AIC/BIC/AICc
> >
> >   1. The AIC/BIC distinction is between "best prediction"
> > and "consistent estimation of true model" dimension, e.g.
> >
> > Yang, Yuhong. 2005. Can the strengths of AIC and BIC be shared? A
> > conflict between model identification and regression estimation.
> > Biometrika 92, no. 4 (December 1): 937-950. doi:10.1093/biomet/92.4.937.
> >
> >   I favor AIC on these grounds, but you can decide for yourself.
> >
> >   2. For models with different random effects, AIC and BIC share
> > a "degrees of freedom counting" problem with all model selection
> > approaches -- there are two aspects here, (1) whether you are
> > focused on individual-level prediction or population-level
> > prediction (Vaida and Blanchard 2005, Spiegelhalter et al 2002)
> > and (2) whether AIC/BIC share the boundary problems that
> > also apply to likelihood ratio tests (Greven, Sonja. 2008. Non-Standard
> > Problems in Inference for Additive and Linear Mixed Models. G?ttingen,
> > Germany: Cuvillier Verlag.
> > http://www.cuvillier.de/flycms/en/html/30/-UickI3zKPS,3cEY=/Buchdetails.h
> >tml?SID=wVZnpL8f0fbc. )
> >
> >   3. AIC and BIC are asymptotic tests (which can be especially
> > problematic with random effects problems, when there are not
> > large number of random blocks -- this makes likelihood ratio
> > tests NOT OK for fixed-effect comparisons with small numbers
> > of blocks (Pinheiro and Bates 2000)).  If you want to use
> > AICc then you are back to counting residual degrees of freedom ...
> > as far as I know there isn't much guidance available on this
> > issue.
> >
> >   My bottom line:
> >
> >   I would go ahead and use (Q)AIC with caution for data sets with large
> > (?) numbers of blocks.  With smaller numbers of blocks I would probably
> > try to find some kind of randomization/permutation approach to get a
> > sense of the relevant size of delta-AIC values ...
> >    ... or damn the torpedoes and see if you can get away with straight
> > AIC.
> >
> >   Ben Bolker
> >
> > Christopher David Desjardins wrote:
> >> You could use either the BIC or the AIC. My understanding is that the
> >> AIC tends to favor overly complex models whereas the BIC tends to
> >> favor parsimonious models. I am generally inclined to always use the
> >> BIC. If you have a small sample size you might also consider using the
> >> AICC which is a correction of the AIC for small sample sizes. That
> >> said, in my experience the AICC still selects more complex models than
> >> the BIC. Also if you have nested models you could use the chi-square
> >> tests.
> >> Cheers,
> >> Chris
> >>
> >> On Feb 15, 2009, at 4:44 PM, Tahira Jamil wrote:
> >>> Hi
> >>> I have run  GLMM models in lme4 with different fixed effects and
> >>> random effects . But now the problem is model selction Is AIC or BIC
> >>> results are definitive specially for Gernalized linear mixed models
> >>> or what critera should I use for model selction. So I can decide
> >>> which explantory variable should be in the model because I have more
> >>> than 10 explantory variables and some are entering in the model as
> >>> random effect. In some cases If AIC has lower value but BIC is
> >>> comparatively high.
> >>>    some suggestion for model selection would be highly appricated.
> >>>
> >>>    WIth best wishes
> >>>    T Jamil
> >>>    Ph.D student
> >>>    Biometris
> >>>    Wageningen University and Research centre Netherlands.
> >>>
> >>> _______________________________________________
> >>> R-sig-mixed-models at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >> -----------------
> >> Christopher David Desjardins
> >> Ph.D. Student
> >> Quantitative Methods in Education
> >> Department of Educational Psychology
> >> University of  Minnesota
> >> http://blog.lib.umn.edu/desja004/educationalpsychology/
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> > --
> > Ben Bolker
> > Associate professor, Biology Dep't, Univ. of Florida
> > bolker at ufl.edu / www.zoology.ufl.edu/bolker
> > GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> >
> > The University of Aberdeen is a charity registered in Scotland, No
> > SC013683.



From cddesjardins at gmail.com  Mon Feb 16 06:53:20 2009
From: cddesjardins at gmail.com (Christopher David Desjardins)
Date: Sun, 15 Feb 2009 23:53:20 -0600
Subject: [R-sig-ME] model selection in lme4
In-Reply-To: <4998D752.1000107@ufl.edu>
References: <505520.96158.qm@web50803.mail.re2.yahoo.com>
	<200902152023.12346.desja004@umn.edu> <4998D752.1000107@ufl.edu>
Message-ID: <200902152353.21153.desja004@umn.edu>

> In that case I might bring up B&A's "tapering effects" argument
> again -- selecting the correct model with a fixed number of parameters
> with non-tapering effects is what BIC is for, not what AIC is for.


I think this may be the case. The data that I have used is real data not 
simulated. However, I can tell that the parameters were unnecessary as they 
didn't explain any variation above and beyond the more simpler models. I 
think it may also be a case of the type of research questions that I ask in 
psychology vs. ecology.

I'm always interested in knowing more about BIC/AIC and I'll check out your 
references.
Thanks!
Chris
 

On Sunday 15 February 2009 21:02:42 Ben Bolker wrote:
> Took a (very) quick look at Raftery, which all seems sensible
> and well-argued.  However ... the paper contrasts Bayes/BIC
> with classical hypothesis testing.  Many of the points listed on p. 155
> (better assessment of evidence, applicability to non-nested models, take
> model uncertainty into account, allow model averaging, easy to
> implement) apply to AIC as well as BIC.  BIC does have many good
> qualities (approximation to Bayes factor, sensible "flat prior"
> interpretation, statistical consistency, ...).  But the crux of the
> argument between BIC and AIC is the difference in their objective. BIC
> aims to identify the "true model", which essentially assumes that there
> is a sharp cutoff between parameters/processes that are in the model and
> those that are out. Burnham and Anderson have a lot to say about
> tapering effect sizes; they are zealots about AIC, and I often discount
> their enthusiasm, but after much percolation I've decided that AIC
> really does make sense for the kinds of questions I (and many
> ecologists) tend to ask.
>
>    When you say that AIC selects an overly complex model, how
> do you know what the correct model is and which parameters are
> unnecessary?  Is this a case of fitting to simulation output?

>   I have tried to say this more coherently at
> http://emdbolker.wikidot.com/blog:aic-vs-bic
>
>   As an aside, I don't have a vested interest in this, and I don't
> claim that AIC is better for everything ... just that it seems
> most ecologists are working with "true models" that are of
> arbitrarily large dimension with tapering effects, which is where
> AIC should select the model with the best predictive capability ...
>
>   Ben Bolker
>
> Christopher David Desjardins wrote:
> > For a discussion of BIC, please see Raftery (1995) in Sociological
> > Methodology. Before you commit yourself on the AIC, I do encourage you to
> > look at your BIC. In the models I've run when there is disagreement
> > between the BIC and the AIC, it's usually that the AIC selects the overly
> > complex model and includes unnecessary parameters.
> > Cheers,
> > Chris
> >
> > On Sunday 15 February 2009 19:50:30 Ben Bolker wrote:
> >>   It would be better to use AICc, but I'm not sure what I would
> >> use for "number of parameters" for a random effect with n
> >> levels: any number between 0.5 and n seems plausible!
> >> Someone should send Shane Richards (who has done some
> >> very nice work testing (Q)AIC(c) in ecological settings)
> >> and see if he's willing to tackle this one, although I can
> >> imagine he's getting sick of this kind of exercise ...
> >>
> >>   Ben Bolker
> >>
> >> Renwick, A. R. wrote:
> >>> Just a quickie Ben,
> >>> Are you saying that you would use AIC rather than AICc even with
> >>> small sample size - due to difficulty in counting residual degrees of
> >>
> >> freedom?
> >>
> >>> Thanks
> >>> Anna
> >>> p.s. this forum really is fantastic
> >>>
> >>> ________________________________________
> >>> From: r-sig-mixed-models-bounces at r-project.org
> >>> [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker
> >>> [bolker at ufl.edu] Sent: 15 February 2009 23:07
> >>> To: Christopher David Desjardins
> >>> Cc: r-sig-mixed-models at r-project.org; tahirajamil at yahoo.com
> >>> Subject: Re: [R-sig-ME] model selection in lme4
> >>>
> >>>   Some caution on this advice: you seem to be quoting
> >>> the general advice on AIC/BIC/AICc
> >>>
> >>>   1. The AIC/BIC distinction is between "best prediction"
> >>> and "consistent estimation of true model" dimension, e.g.
> >>>
> >>> Yang, Yuhong. 2005. Can the strengths of AIC and BIC be shared? A
> >>> conflict between model identification and regression estimation.
> >>> Biometrika 92, no. 4 (December 1): 937-950.
> >>> doi:10.1093/biomet/92.4.937.
> >>>
> >>>   I favor AIC on these grounds, but you can decide for yourself.
> >>>
> >>>   2. For models with different random effects, AIC and BIC share
> >>> a "degrees of freedom counting" problem with all model selection
> >>> approaches -- there are two aspects here, (1) whether you are
> >>> focused on individual-level prediction or population-level
> >>> prediction (Vaida and Blanchard 2005, Spiegelhalter et al 2002)
> >>> and (2) whether AIC/BIC share the boundary problems that
> >>> also apply to likelihood ratio tests (Greven, Sonja. 2008. Non-Standard
> >>> Problems in Inference for Additive and Linear Mixed Models. G?ttingen,
> >>> Germany: Cuvillier Verlag.
> >>> http://www.cuvillier.de/flycms/en/html/30/-UickI3zKPS,3cEY=/Buchdetails
> >>>.h tml?SID=wVZnpL8f0fbc. )
> >>>
> >>>   3. AIC and BIC are asymptotic tests (which can be especially
> >>> problematic with random effects problems, when there are not
> >>> large number of random blocks -- this makes likelihood ratio
> >>> tests NOT OK for fixed-effect comparisons with small numbers
> >>> of blocks (Pinheiro and Bates 2000)).  If you want to use
> >>> AICc then you are back to counting residual degrees of freedom ...
> >>> as far as I know there isn't much guidance available on this
> >>> issue.
> >>>
> >>>   My bottom line:
> >>>
> >>>   I would go ahead and use (Q)AIC with caution for data sets with large
> >>> (?) numbers of blocks.  With smaller numbers of blocks I would probably
> >>> try to find some kind of randomization/permutation approach to get a
> >>> sense of the relevant size of delta-AIC values ...
> >>>    ... or damn the torpedoes and see if you can get away with straight
> >>> AIC.
> >>>
> >>>   Ben Bolker
> >>>
> >>> Christopher David Desjardins wrote:
> >>>> You could use either the BIC or the AIC. My understanding is that the
> >>>> AIC tends to favor overly complex models whereas the BIC tends to
> >>>> favor parsimonious models. I am generally inclined to always use the
> >>>> BIC. If you have a small sample size you might also consider using the
> >>>> AICC which is a correction of the AIC for small sample sizes. That
> >>>> said, in my experience the AICC still selects more complex models than
> >>>> the BIC. Also if you have nested models you could use the chi-square
> >>>> tests.
> >>>> Cheers,
> >>>> Chris
> >>>>
> >>>> On Feb 15, 2009, at 4:44 PM, Tahira Jamil wrote:
> >>>>> Hi
> >>>>> I have run  GLMM models in lme4 with different fixed effects and
> >>>>> random effects . But now the problem is model selction Is AIC or BIC
> >>>>> results are definitive specially for Gernalized linear mixed models
> >>>>> or what critera should I use for model selction. So I can decide
> >>>>> which explantory variable should be in the model because I have more
> >>>>> than 10 explantory variables and some are entering in the model as
> >>>>> random effect. In some cases If AIC has lower value but BIC is
> >>>>> comparatively high.
> >>>>>    some suggestion for model selection would be highly appricated.
> >>>>>
> >>>>>    WIth best wishes
> >>>>>    T Jamil
> >>>>>    Ph.D student
> >>>>>    Biometris
> >>>>>    Wageningen University and Research centre Netherlands.
> >>>>>
> >>>>> _______________________________________________
> >>>>> R-sig-mixed-models at r-project.org mailing list
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>>
> >>>> -----------------
> >>>> Christopher David Desjardins
> >>>> Ph.D. Student
> >>>> Quantitative Methods in Education
> >>>> Department of Educational Psychology
> >>>> University of  Minnesota
> >>>> http://blog.lib.umn.edu/desja004/educationalpsychology/
> >>>>
> >>>> _______________________________________________
> >>>> R-sig-mixed-models at r-project.org mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>
> >>> --
> >>> Ben Bolker
> >>> Associate professor, Biology Dep't, Univ. of Florida
> >>> bolker at ufl.edu / www.zoology.ufl.edu/bolker
> >>> GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
> >>>
> >>> _______________________________________________
> >>> R-sig-mixed-models at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>
> >>>
> >>> The University of Aberdeen is a charity registered in Scotland, No
> >>> SC013683.



From a.s.wade at reading.ac.uk  Mon Feb 16 14:33:24 2009
From: a.s.wade at reading.ac.uk (Amy Wade)
Date: Mon, 16 Feb 2009 13:33:24 -0000
Subject: [R-sig-ME] No estimated scale value given
Message-ID: <000001c9903b$24936740$6dba35c0$@s.wade@reading.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090216/1968da95/attachment.pl>

From a.renwick at abdn.ac.uk  Mon Feb 16 15:24:05 2009
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Mon, 16 Feb 2009 14:24:05 +0000
Subject: [R-sig-ME] No estimated scale value given
In-Reply-To: <000001c9903b$24936740$6dba35c0$@s.wade@reading.ac.uk>
References: <000001c9903b$24936740$6dba35c0$@s.wade@reading.ac.uk>
Message-ID: <B9D1301370916C44B5874AF340C18B9B7F911B53B7@VMAILB.uoa.abdn.ac.uk>

This has just recently been discussed:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q1/001922.html
Hope that help,
Anna

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Amy Wade
Sent: 16 February 2009 13:33
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] No estimated scale value given

Hello,



I'm trying to run generalized linear mixed models using the lmer() function in the lme4 package. The problem is that the output does not give me a value for the Estimated scale. The rest of the output is as it should be. This makes it very difficult to assess whether the model is overdispersed. My colleague tried the same code on his computer and it did give an estimated scale value. I tried unistalling R and reinstalling the latest edition
(2.8.1) with the latest lme4, but this made no difference. I also tried 'summary(model)@sigma' which people suggested on the CRAN forums to extract the scale parameter. This always gave me an answer of '1' even when I used data where I knew this was not the case. When I load up the lme4 library it does warn me about several objects being masked from 'stats' and 'base'.



Any idea why the output is not giving me the estimate scale? Or any idea how to extract it somehow?



Many thanks,

Amy





Amy Wade

Centre for Agri-Environmental Research

University of Reading

Tel: +44 (0)118 987 5467

Mobile: +44 (0)7786 557 007

Email: a.s.wade at reading.ac.uk

http://www.reading.ac.uk/caer/student_amy_wade.html




        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From ltiana_m at yahoo.com  Mon Feb 16 15:42:04 2009
From: ltiana_m at yahoo.com (Liliana Martinez)
Date: Mon, 16 Feb 2009 06:42:04 -0800 (PST)
Subject: [R-sig-ME] how to report the results from lmer() in APA-style
Message-ID: <132013.33797.qm@web53008.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090216/0df1ecba/attachment.pl>

From Thierry.ONKELINX at inbo.be  Mon Feb 16 16:29:41 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 16 Feb 2009 16:29:41 +0100
Subject: [R-sig-ME] how to report the results from lmer() in APA-style
In-Reply-To: <132013.33797.qm@web53008.mail.re2.yahoo.com>
References: <132013.33797.qm@web53008.mail.re2.yahoo.com>
Message-ID: <2E9C414912813E4EB981326983E0A10406132FC9@inboexch.inbo.be>

Dear Liliana,

Have at look at https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html. Douglas Bates explaines in that post why you can't find p-values in the summary of lmer().

You could also have a look at RSiteSearch("lmer p-value").

HTH,

Thierry


----------------------------------------------------------------------------
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics, methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Liliana Martinez
Verzonden: maandag 16 februari 2009 15:42
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] how to report the results from lmer() in APA-style

Dear all,

I am trying to apply the lmer function in R 2.8.0. to some linguistic data, but I am at a loss when it comes to reporting the results (see below). The APA recommendations say that effects should be reported as follows:

F (df1, df2) = ... , p. = ... 

The question is, where do I find all these things? So far I have??learned through different sources that df1 and F can be found through using the anova() function (is this??correct?), but where do I find df2 and p ? 
I have even bigger problems when my dependent variable has a binomial distribution, because then the anova() and pvals.fnc() functions cannot be applied.

I wonder as well whether there is a commonly approved way of reporting the output of the 'print (xxx.lmer)' and 'xxx.pvals$fixed' commands? I can see that some of the levels of a factor are significantly different from the baseline, and this is of interests for me, but how??shal I report it? Or should other tests be applied in order to find the difference between the levels? (and, if yes, what tests?)

Any help/ advice/ references??will be greatly appreciated.


Best regards

Liliana

----------------------------------

print (all_v_a_va_vf_vp_vt.lmer , corr = F)
Linear mixed model fit by REML 
Formula: rating ~ verb + angle + verb:angle + verb:type + verb:prec +?????????? verb:fol + (1 | subject) 
???? Data: rating_allbegend_no270 
???? AIC???? BIC logLik deviance REMLdev
??11067 11225?? -5507?????? 10941???? 11015
Random effects:
??Groups???? Name?????????????? Variance Std.Dev.
??subject?? (Intercept) 0.092792 0.30462 
??Residual???????????????????????? 1.691374 1.30053 
Number of obs: 3240, groups: subject, 40
Fixed effects:
???????????????????????????????????????????????????????????? Estimate Std. Error t value
(Intercept)???????????????????????????????????????? 4.51944?????? 0.13102???? 34.49
verbzaobikalia???????????????????????????????? -1.42685?????? 0.15830???? -9.01
verbzaviva???????????????????????????????????????? -2.82315?????? 0.15830?? -17.83
angle180???????????????????????????????????????????? -0.88611?????? 0.09694???? -9.14
angle360???????????????????????????????????????????? -2.84722?????? 0.09694?? -29.37
verbzaobikalia:angle180?????????????? -0.37778?????? 0.13709???? -2.76
verbzaviva:angle180???????????????????????? 1.76944?????? 0.13709???? 12.91
verbzaobikalia:angle360???????????????? 2.24444?????? 0.13709???? 16.37
verbzaviva:angle360???????????????????????? 4.95833?????? 0.13709???? 36.17
verbobikalia:typeround?????????????????? 0.23333?????? 0.12466?????? 1.87
verbzaobikalia:typeround???????????? -0.07407?????? 0.12466???? -0.59
verbzaviva:typeround?????????????????????? 0.35370?????? 0.12466?????? 2.84
verbobikalia:precno_prec???????????? -0.01111?????? 0.09694???? -0.11
verbzaobikalia:precno_prec?????????? 0.20556?????? 0.09694?????? 2.12
verbzaviva:precno_prec???????????????? -0.24722?????? 0.09694???? -2.55
verbobikalia:precsmooth_prec???? -0.19722?????? 0.09694???? -2.03
verbzaobikalia:precsmooth_prec?? 0.12778?????? 0.09694?????? 1.32
verbzaviva:precsmooth_prec???????? -0.15833?????? 0.09694???? -1.63
verbobikalia:folno_fol???????????????? -0.33333?????? 0.09694???? -3.44
verbzaobikalia:folno_fol?????????????? 0.21389?????? 0.09694?????? 2.21
verbzaviva:folno_fol???????????????????? -0.14722?????? 0.09694???? -1.52
verbobikalia:folsmooth_fol???????? -0.26667?????? 0.09694???? -2.75
verbzaobikalia:folsmooth_fol?????? 0.31944?????? 0.09694?????? 3.30
verbzaviva:folsmooth_fol???????????? -0.04167?????? 0.09694???? -0.43


> anova (all_v_a_va_vf_vp_vt.lmer )
Analysis of Variance Table
???????????????????? Df?? Sum Sq Mean Sq?? F value
verb?????????????? 2?? 131.07???? 65.53?? 38.7458
angle???????????? 2?? 136.09???? 68.05?? 40.2310
verb:angle?? 4 2489.53?? 622.38 367.9741
verb:type???? 3???? 30.62???? 10.21???? 6.0350
verb:prec???? 6???? 27.89?????? 4.65???? 2.7478
verb:fol?????? 6???? 45.62?????? 7.60???? 4.4952


> all_v_a_va_vf_vp_vt.pvals 
$fixed
???????????????????????????????????????????????????????????? Estimate MCMCmean HPD95lower HPD95upper?? pMCMC Pr(>|t|)
(Intercept)?????????????????????????????????????????? 4.5194???? 4.5198???????? 4.2530???????? 4.7685 0.0001???? 0.0000
verbzaobikalia?????????????????????????????????? -1.4269?? -1.4253?????? -1.7338?????? -1.1122 0.0001???? 0.0000
verbzaviva?????????????????????????????????????????? -2.8231?? -2.8247?????? -3.1252?????? -2.5124 0.0001???? 0.0000
angle180?????????????????????????????????????????????? -0.8861?? -0.8862?????? -1.0768?????? -0.6987 0.0001???? 0.0000
angle360?????????????????????????????????????????????? -2.8472?? -2.8491?????? -3.0388?????? -2.6610 0.0001???? 0.0000
verbzaobikalia:angle180???????????????? -0.3778?? -0.3766?????? -0.6426?????? -0.1051 0.0056???? 0.0059
verbzaviva:angle180?????????????????????????? 1.7694???? 1.7696???????? 1.5117???????? 2.0402 0.0001???? 0.0000
verbzaobikalia:angle360?????????????????? 2.2444???? 2.2465???????? 1.9892???????? 2.5295 0.0001???? 0.0000
verbzaviva:angle360?????????????????????????? 4.9583???? 4.9611???????? 4.7059???????? 5.2427 0.0001???? 0.0000
verbobikalia:typeround???????????????????? 0.2333???? 0.2339?????? -0.0069???????? 0.4895 0.0666???? 0.0613
verbzaobikalia:typeround?????????????? -0.0741?? -0.0757?????? -0.3212???????? 0.1628 0.5384???? 0.5524
verbzaviva:typeround???????????????????????? 0.3537???? 0.3527???????? 0.0980???????? 0.5983 0.0060???? 0.0046
verbobikalia:precno_prec?????????????? -0.0111?? -0.0118?????? -0.2111???????? 0.1717 0.9012???? 0.9088
verbzaobikalia:precno_prec???????????? 0.2056???? 0.2050???????? 0.0145???????? 0.3916 0.0344???? 0.0340
verbzaviva:precno_prec?????????????????? -0.2472?? -0.2456?????? -0.4378?????? -0.0615 0.0136???? 0.0108
verbobikalia:precsmooth_prec?????? -0.1972?? -0.1969?????? -0.3790???????? 0.0009 0.0412???? 0.0420
verbzaobikalia:precsmooth_prec???? 0.1278???? 0.1278?????? -0.0743???????? 0.3078 0.1890???? 0.1875
verbzaviva:precsmooth_prec?????????? -0.1583?? -0.1569?????? -0.3441???????? 0.0336 0.0952???? 0.1025
verbobikalia:folno_fol?????????????????? -0.3333?? -0.3344?????? -0.5240?????? -0.1438 0.0002???? 0.0006
verbzaobikalia:folno_fol???????????????? 0.2139???? 0.2133???????? 0.0254???????? 0.4008 0.0282???? 0.0274
verbzaviva:folno_fol?????????????????????? -0.1472?? -0.1467?????? -0.3379???????? 0.0420 0.1314???? 0.1289
verbobikalia:folsmooth_fol?????????? -0.2667?? -0.2674?????? -0.4645?????? -0.0839 0.0058???? 0.0060
verbzaobikalia:folsmooth_fol???????? 0.3194???? 0.3186???????? 0.1252???????? 0.5074 0.0006???? 0.0010
verbzaviva:folsmooth_fol?????????????? -0.0417?? -0.0412?????? -0.2339???????? 0.1437 0.6828???? 0.6673
$random
?????? Groups?????????????? Name Std.Dev. MCMCmedian MCMCmean HPD95lower HPD95upper
1?? subject (Intercept)???? 0.3046???????? 0.2993???? 0.3027???????? 0.2235???????? 0.3868
2 Residual???????????????????????????? 1.3005???????? 1.3011???? 1.3012???????? 1.2694???????? 1.3329



      _________________________________________________________


	[[alternative HTML version deleted]]


Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From peter.dixon at ualberta.ca  Mon Feb 16 16:55:03 2009
From: peter.dixon at ualberta.ca (Peter Dixon)
Date: Mon, 16 Feb 2009 08:55:03 -0700
Subject: [R-sig-ME] how to report the results from lmer() in APA-style
In-Reply-To: <132013.33797.qm@web53008.mail.re2.yahoo.com>
References: <132013.33797.qm@web53008.mail.re2.yahoo.com>
Message-ID: <F2ACC98E-4126-437C-AE38-622D5E8EC98A@ualberta.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090216/35e7d748/attachment.pl>

From lescroel at cebc.cnrs.fr  Mon Feb 16 16:59:53 2009
From: lescroel at cebc.cnrs.fr (Amelie LESCROEL)
Date: Mon, 16 Feb 2009 16:59:53 +0100
Subject: [R-sig-ME] lmer, poisson family and mcmcsamp
Message-ID: <49998D79.6050806@cebc.cnrs.fr>

Dear all,

Could someone explain to me why summary(lmer.object) is giving z values 
and Pr(>|z|) when the lmer.object was fitted with family=poisson? I used 
to fit linear mixed models with family=gaussian and link="identity" or 
link="log", then used mcmcsamp() and HPDinterval() for computing 
"confidence" intervals and considered that a fixed effect was 
significant when the 95% HPD interval did not include zero. Now, with 
family=poisson, when I use mcmcsamp(), I get the following:

 > set.seed(101); mcmcfm1 <- mcmcsamp(fm1, n=10000)
Error in .local(object, n, verbose, ...) : Update not yet written

but I get z values and Pr(>|z|). Why is it so? Can we "trust" this 
probability for judging of the fixed effects significance? Would the 
mcmcsamp method be available soon for families other than gaussian?

Thanks in advance,

Am?lie

PS: When will lme4_1.0 be released? Many thanks to Douglas Bates for 
developing lme4 in interaction with users, and for sharing it. Please 
don't give up on making it available in R. lme4 is a wonderful tool 
which is helping me understanding penguin's foraging behaviour (and 
advanced stats!) as it is helping others in plant ecology or human 
psychology.

 > sessionInfo()
R version 2.7.2 (2008-08-25)
i386-pc-mingw32

locale:
LC_COLLATE=French_France.1252;LC_CTYPE=French_France.1252;LC_MONETARY=French_France.1252;LC_NUMERIC=C;LC_TIME=French_France.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base    

other attached packages:
[1] lme4_0.999375-28   Matrix_0.999375-17 lattice_0.17-17    
gdata_2.4.2      

loaded via a namespace (and not attached):
[1] grid_2.7.2   gtools_2.5.0 tools_2.7.2



__________ Information from ESET Mail Security, version of virus signature database 3857 (20090216) __________

The message was checked by ESET Mail Security.
http://www.eset.com



From desja004 at umn.edu  Mon Feb 16 17:06:06 2009
From: desja004 at umn.edu (Christopher David Desjardins)
Date: Mon, 16 Feb 2009 10:06:06 -0600
Subject: [R-sig-ME] lmer, poisson family and mcmcsamp
In-Reply-To: <49998D79.6050806@cebc.cnrs.fr>
References: <49998D79.6050806@cebc.cnrs.fr>
Message-ID: <07DA1F42-C390-4917-8CA7-FC109D3A68C0@umn.edu>

Regarding mcmcsamp() see https://mailman.stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001660.html 
  specifically

...  In particular mcmcsamp is not currently working in that branch.   
My goal is to get mcmcsamp stable and to get all the other parts  
working in this branch then release it for testing and finally release  
lme4-1.0.  The subtext in that message is that only the models I  
describe above will be fit by the lme4-1.0 package.


On Feb 16, 2009, at 9:59 AM, Amelie LESCROEL wrote:

> Dear all,
>
> Could someone explain to me why summary(lmer.object) is giving z  
> values and Pr(>|z|) when the lmer.object was fitted with  
> family=poisson? I used to fit linear mixed models with  
> family=gaussian and link="identity" or link="log", then used  
> mcmcsamp() and HPDinterval() for computing "confidence" intervals  
> and considered that a fixed effect was significant when the 95% HPD  
> interval did not include zero. Now, with family=poisson, when I use  
> mcmcsamp(), I get the following:
>
> > set.seed(101); mcmcfm1 <- mcmcsamp(fm1, n=10000)
> Error in .local(object, n, verbose, ...) : Update not yet written
>
> but I get z values and Pr(>|z|). Why is it so? Can we "trust" this  
> probability for judging of the fixed effects significance? Would the  
> mcmcsamp method be available soon for families other than gaussian?
>
> Thanks in advance,
>
> Am?lie
>
> PS: When will lme4_1.0 be released? Many thanks to Douglas Bates for  
> developing lme4 in interaction with users, and for sharing it.  
> Please don't give up on making it available in R. lme4 is a  
> wonderful tool which is helping me understanding penguin's foraging  
> behaviour (and advanced stats!) as it is helping others in plant  
> ecology or human psychology.
>
> > sessionInfo()
> R version 2.7.2 (2008-08-25)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=French_France.1252;LC_CTYPE=French_France. 
> 1252;LC_MONETARY=French_France. 
> 1252;LC_NUMERIC=C;LC_TIME=French_France.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> other attached packages:
> [1] lme4_0.999375-28   Matrix_0.999375-17 lattice_0.17-17     
> gdata_2.4.2
> loaded via a namespace (and not attached):
> [1] grid_2.7.2   gtools_2.5.0 tools_2.7.2
>
>
>
> __________ Information from ESET Mail Security, version of virus  
> signature database 3857 (20090216) __________
>
> The message was checked by ESET Mail Security.
> http://www.eset.com
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-----------------
Christopher David Desjardins
Ph.D. Student
Quantitative Methods in Education
Department of Educational Psychology
University of  Minnesota
http://blog.lib.umn.edu/desja004/educationalpsychology/



From harlancampbell at gmail.com  Mon Feb 16 17:08:54 2009
From: harlancampbell at gmail.com (H c)
Date: Mon, 16 Feb 2009 11:08:54 -0500
Subject: [R-sig-ME] Weighted-ML estimates
Message-ID: <222824550902160808o645a1f10la441dc41da3451e8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090216/6b183e9d/attachment.pl>

From maj at stats.waikato.ac.nz  Mon Feb 16 19:17:10 2009
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Tue, 17 Feb 2009 07:17:10 +1300
Subject: [R-sig-ME] Weighted-ML estimates
In-Reply-To: <222824550902160808o645a1f10la441dc41da3451e8@mail.gmail.com>
References: <222824550902160808o645a1f10la441dc41da3451e8@mail.gmail.com>
Message-ID: <4999ADA6.5040109@stats.waikato.ac.nz>

I wonder if in your situation you might be better off using a completely 
EM approach treating both cluster indicator variables and the random 
effects as the missing data?

Murray Jorgensen

H c wrote:
> Hi,
> I'll briefly describe the situation. An EM algorithm is being implemented in
> order to cluster observations into components of a mixture of mixed effects
> models.  It was our hope to use the lme() or lmer() function in the M-step
> to easily find weighted-ML parameter estimates.  Unfortunately, the mixed
> models are often quite complex including serial correlation structures etc.
> 
> 
> Weighted-ML estimates of certain parameters (such as the correlation
> parameters) does not seem likely in closed form or by using lme()/lmer().
>  Due to the weighted-likelihood function, it does not seem appropriate to
> simply use a weighted-least squares approach.  Are appropriate numeric
> approaches available? Has anyone had experience with mixtures of mixed
> models?
> 
> any help would be greatly appreciated,
> 
> Harlan
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz    majorgensen at ihug.co.nz      Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 139 5862



From john.maindonald at anu.edu.au  Tue Feb 17 01:05:54 2009
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Tue, 17 Feb 2009 11:05:54 +1100
Subject: [R-sig-ME] model selection in lme4
In-Reply-To: <1234757745.4787.53.camel@sib-sblomber01d.sib.uq.edu.au>
References: <505520.96158.qm@web50803.mail.re2.yahoo.com>
	<B9D1301370916C44B5874AF340C18B9B7F913660EE@VMAILB.uoa.abdn.ac.uk>
	<4998C666.5060801@ufl.edu> <200902152023.12346.desja004@umn.edu>
	<1234757745.4787.53.camel@sib-sblomber01d.sib.uq.edu.au>
Message-ID: <F0562EEB-9C8F-40E8-AF74-AA247F3415AF@anu.edu.au>

The issue seems to be what kind of generations one wishes to make.  
This determines what conditioning is appropriate, and it determines  
the distribution with respect to which one tries to find the  
expectation that is involved in calculating the AIC or other such  
statistic.  Should one condition wrt to, e.g., the actual numbers of  
plots at the different sites and the actual number of sites, as in the  
data?  Or should these be treated as random?   It all gets too  
horrible to contemplate.  Vaida and Blanchard, and the Liang & Wu &  
Zhou paper, do not do much more than scratch the surface of these  
complications.

The complications are of the same kind as those involved in  
calculating predicted values.  These differ depending on the  
population to which one wishes to generalize.  The SEs vary also, and  
depend on whether one wants the SE of the prediction, or the SE of the  
equivalent observation.  A focus on prediction may be the way to get a  
clear understanding of what should be optimized.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 16/02/2009, at 3:15 PM, Simon Blomberg wrote:

> Vaida and Blanchard Biometrika [(2005), 92, 2, pp. 351?370 Conditional
> Akaike information for mixed-effects models] discuss using AIC for  
> model
> selection in mixed-effects models, and make recommendations. There is
> also a follow-up not by Liang, Wu and Zhou. Biometrika (2008), 95, 3,
> pp. 773?778 A note on conditional AIC for linear mixed-effects models.
>
> The general message is that the "type" of AIC statistic will depend on
> your motivation for model selection. Is it the fixed effects part of  
> the
> model that is of most interest? Or are the random effects of specific
> interest too? This "focus" will determine the number of "effective
> parameters" in the penalty term (using results from Hodges, J.S. and
> Sargent, D. J. (2001). Counting degrees of freedom in hierarchical and
> other richly parameterized models. Biometrika 88, 367?79). There is  
> also
> the issue of REML v ML estimation...
>
> Cheers,
>
> Simon.
>
> On Sun, 2009-02-15 at 20:23 -0600, Christopher David Desjardins wrote:
>> For a discussion of BIC, please see Raftery (1995) in Sociological
>> Methodology. Before you commit yourself on the AIC, I do encourage  
>> you to
>> look at your BIC. In the models I've run when there is disagreement  
>> between
>> the BIC and the AIC, it's usually that the AIC selects the overly  
>> complex
>> model and includes unnecessary parameters.
>> Cheers,
>> Chris
>>
>> On Sunday 15 February 2009 19:50:30 Ben Bolker wrote:
>>> It would be better to use AICc, but I'm not sure what I would
>>> use for "number of parameters" for a random effect with n
>>> levels: any number between 0.5 and n seems plausible!
>>> Someone should send Shane Richards (who has done some
>>> very nice work testing (Q)AIC(c) in ecological settings)
>>> and see if he's willing to tackle this one, although I can
>>> imagine he's getting sick of this kind of exercise ...
>>>
>>> Ben Bolker
>>>
>>> Renwick, A. R. wrote:
>>>> Just a quickie Ben,
>>>> Are you saying that you would use AIC rather than AICc even with
>>>> small sample size - due to difficulty in counting residual  
>>>> degrees of
>>>
>>> freedom?
>>>
>>>> Thanks
>>>> Anna
>>>> p.s. this forum really is fantastic
>>>>
>>>> ________________________________________
>>>> From: r-sig-mixed-models-bounces at r-project.org
>>>> [r-sig-mixed-models-bounces at r-project.org] On Behalf Of Ben Bolker
>>>> [bolker at ufl.edu] Sent: 15 February 2009 23:07
>>>> To: Christopher David Desjardins
>>>> Cc: r-sig-mixed-models at r-project.org; tahirajamil at yahoo.com
>>>> Subject: Re: [R-sig-ME] model selection in lme4
>>>>
>>>> Some caution on this advice: you seem to be quoting
>>>> the general advice on AIC/BIC/AICc
>>>>
>>>> 1. The AIC/BIC distinction is between "best prediction"
>>>> and "consistent estimation of true model" dimension, e.g.
>>>>
>>>> Yang, Yuhong. 2005. Can the strengths of AIC and BIC be shared? A
>>>> conflict between model identification and regression estimation.
>>>> Biometrika 92, no. 4 (December 1): 937-950. doi:10.1093/biomet/ 
>>>> 92.4.937.
>>>>
>>>> I favor AIC on these grounds, but you can decide for yourself.
>>>>
>>>> 2. For models with different random effects, AIC and BIC share
>>>> a "degrees of freedom counting" problem with all model selection
>>>> approaches -- there are two aspects here, (1) whether you are
>>>> focused on individual-level prediction or population-level
>>>> prediction (Vaida and Blanchard 2005, Spiegelhalter et al 2002)
>>>> and (2) whether AIC/BIC share the boundary problems that
>>>> also apply to likelihood ratio tests (Greven, Sonja. 2008. Non- 
>>>> Standard
>>>> Problems in Inference for Additive and Linear Mixed Models.  
>>>> G?ttingen,
>>>> Germany: Cuvillier Verlag.
>>>> http://www.cuvillier.de/flycms/en/html/30/-UickI3zKPS,3cEY=/Buchdetails.h
>>>> tml?SID=wVZnpL8f0fbc. )
>>>>
>>>> 3. AIC and BIC are asymptotic tests (which can be especially
>>>> problematic with random effects problems, when there are not
>>>> large number of random blocks -- this makes likelihood ratio
>>>> tests NOT OK for fixed-effect comparisons with small numbers
>>>> of blocks (Pinheiro and Bates 2000)).  If you want to use
>>>> AICc then you are back to counting residual degrees of freedom ...
>>>> as far as I know there isn't much guidance available on this
>>>> issue.
>>>>
>>>> My bottom line:
>>>>
>>>> I would go ahead and use (Q)AIC with caution for data sets with  
>>>> large
>>>> (?) numbers of blocks.  With smaller numbers of blocks I would  
>>>> probably
>>>> try to find some kind of randomization/permutation approach to  
>>>> get a
>>>> sense of the relevant size of delta-AIC values ...
>>>>  ... or damn the torpedoes and see if you can get away with  
>>>> straight
>>>> AIC.
>>>>
>>>> Ben Bolker
>>>>
>>>> Christopher David Desjardins wrote:
>>>>> You could use either the BIC or the AIC. My understanding is  
>>>>> that the
>>>>> AIC tends to favor overly complex models whereas the BIC tends to
>>>>> favor parsimonious models. I am generally inclined to always use  
>>>>> the
>>>>> BIC. If you have a small sample size you might also consider  
>>>>> using the
>>>>> AICC which is a correction of the AIC for small sample sizes. That
>>>>> said, in my experience the AICC still selects more complex  
>>>>> models than
>>>>> the BIC. Also if you have nested models you could use the chi- 
>>>>> square
>>>>> tests.
>>>>> Cheers,
>>>>> Chris
>>>>>
>>>>> On Feb 15, 2009, at 4:44 PM, Tahira Jamil wrote:
>>>>>> Hi
>>>>>> I have run  GLMM models in lme4 with different fixed effects and
>>>>>> random effects . But now the problem is model selction Is AIC  
>>>>>> or BIC
>>>>>> results are definitive specially for Gernalized linear mixed  
>>>>>> models
>>>>>> or what critera should I use for model selction. So I can decide
>>>>>> which explantory variable should be in the model because I have  
>>>>>> more
>>>>>> than 10 explantory variables and some are entering in the model  
>>>>>> as
>>>>>> random effect. In some cases If AIC has lower value but BIC is
>>>>>> comparatively high.
>>>>>>  some suggestion for model selection would be highly appricated.
>>>>>>
>>>>>>  WIth best wishes
>>>>>>  T Jamil
>>>>>>  Ph.D student
>>>>>>  Biometris
>>>>>>  Wageningen University and Research centre Netherlands.
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>> -----------------
>>>>> Christopher David Desjardins
>>>>> Ph.D. Student
>>>>> Quantitative Methods in Education
>>>>> Department of Educational Psychology
>>>>> University of  Minnesota
>>>>> http://blog.lib.umn.edu/desja004/educationalpsychology/
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>> --
>>>> Ben Bolker
>>>> Associate professor, Biology Dep't, Univ. of Florida
>>>> bolker at ufl.edu / www.zoology.ufl.edu/bolker
>>>> GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>>
>>>> The University of Aberdeen is a charity registered in Scotland, No
>>>> SC013683.
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> -- 
> Simon Blomberg, BSc (Hons), PhD, MAppStat.
> Lecturer and Consultant Statistician
> School of Biological Sciences
> The University of Queensland
> St. Lucia Queensland 4072
> Australia
> Room 320 Goddard Building (8)
> T: +61 7 3365 2506
> http://www.uq.edu.au/~uqsblomb
> email: S.Blomberg1_at_uq.edu.au
>
> Policies:
> 1.  I will NOT analyse your data for you.
> 2.  Your deadline is your problem.
>
> The combination of some data and an aching desire for
> an answer does not ensure that a reasonable answer can
> be extracted from a given body of data. - John Tukey.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From lescroel at cebc.cnrs.fr  Tue Feb 17 10:55:51 2009
From: lescroel at cebc.cnrs.fr (Amelie LESCROEL)
Date: Tue, 17 Feb 2009 10:55:51 +0100
Subject: [R-sig-ME] lmer, poisson family and mcmcsamp
In-Reply-To: <07DA1F42-C390-4917-8CA7-FC109D3A68C0@umn.edu>
References: <49998D79.6050806@cebc.cnrs.fr>
	<07DA1F42-C390-4917-8CA7-FC109D3A68C0@umn.edu>
Message-ID: <499A89A7.7050907@cebc.cnrs.fr>

Thanks Chris, I saw this one but I was wondering whether the situation 
changed or was about to change. What about Pr(>|z|)? Are they reliable?
Amelie

Christopher David Desjardins a ?crit :
> Regarding mcmcsamp() see 
> https://mailman.stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001660.html specifically 
>
>
> ...  In particular mcmcsamp is not currently working in that branch.  
> My goal is to get mcmcsamp stable and to get all the other parts 
> working in this branch then release it for testing and finally release 
> lme4-1.0.  The subtext in that message is that only the models I 
> describe above will be fit by the lme4-1.0 package.
>
>
> On Feb 16, 2009, at 9:59 AM, Amelie LESCROEL wrote:
>
>> Dear all,
>>
>> Could someone explain to me why summary(lmer.object) is giving z 
>> values and Pr(>|z|) when the lmer.object was fitted with 
>> family=poisson? I used to fit linear mixed models with 
>> family=gaussian and link="identity" or link="log", then used 
>> mcmcsamp() and HPDinterval() for computing "confidence" intervals and 
>> considered that a fixed effect was significant when the 95% HPD 
>> interval did not include zero. Now, with family=poisson, when I use 
>> mcmcsamp(), I get the following:
>>
>> > set.seed(101); mcmcfm1 <- mcmcsamp(fm1, n=10000)
>> Error in .local(object, n, verbose, ...) : Update not yet written
>>
>> but I get z values and Pr(>|z|). Why is it so? Can we "trust" this 
>> probability for judging of the fixed effects significance? Would the 
>> mcmcsamp method be available soon for families other than gaussian?
>>
>> Thanks in advance,
>>
>> Am?lie
>>
>> PS: When will lme4_1.0 be released? Many thanks to Douglas Bates for 
>> developing lme4 in interaction with users, and for sharing it. Please 
>> don't give up on making it available in R. lme4 is a wonderful tool 
>> which is helping me understanding penguin's foraging behaviour (and 
>> advanced stats!) as it is helping others in plant ecology or human 
>> psychology.
>>
>> > sessionInfo()
>> R version 2.7.2 (2008-08-25)
>> i386-pc-mingw32
>>
>> locale:
>> LC_COLLATE=French_France.1252;LC_CTYPE=French_France.1252;LC_MONETARY=French_France.1252;LC_NUMERIC=C;LC_TIME=French_France.1252 
>>
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>> other attached packages:
>> [1] lme4_0.999375-28   Matrix_0.999375-17 lattice_0.17-17    gdata_2.4.2
>> loaded via a namespace (and not attached):
>> [1] grid_2.7.2   gtools_2.5.0 tools_2.7.2
>>
>>
>>
>> __________ Information from ESET Mail Security, version of virus 
>> signature database 3857 (20090216) __________
>>
>> The message was checked by ESET Mail Security.
>> http://www.eset.com
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> -----------------
> Christopher David Desjardins
> Ph.D. Student
> Quantitative Methods in Education
> Department of Educational Psychology
> University of  Minnesota
> http://blog.lib.umn.edu/desja004/educationalpsychology/
>
>
>
>
>
>
>
> __________ Information from ESET Mail Security, version of virus 
> signature database 3857 (20090216) __________
>
> The message was checked by ESET Mail Security.
> http://www.eset.com
>
>
>
>
>



__________ Information from ESET Mail Security, version of virus signature database 3860 (20090217) __________

The message was checked by ESET Mail Security.
http://www.eset.com



From ltiana_m at yahoo.com  Tue Feb 17 12:19:22 2009
From: ltiana_m at yahoo.com (Liliana Martinez)
Date: Tue, 17 Feb 2009 03:19:22 -0800 (PST)
Subject: [R-sig-ME] Vedr. how to report the results from lmer() in APA-style
References: <132013.33797.qm@web53008.mail.re2.yahoo.com>
	<F2ACC98E-4126-437C-AE38-622D5E8EC98A@ualberta.ca>
Message-ID: <150104.5156.qm@web53011.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090217/f11fa7f2/attachment.pl>

From desja004 at umn.edu  Tue Feb 17 15:21:31 2009
From: desja004 at umn.edu (Christopher Desjardins)
Date: Tue, 17 Feb 2009 08:21:31 -0600
Subject: [R-sig-ME] lmer, poisson family and mcmcsamp
Message-ID: <2ciidyp4ipuokxi2xhe1yunn.1234880491656@email.android.com>

I think if you have a large number of observations you can treat them as accurate. However, if your number of observations are small then your approximation of z is poor and your probably best off running MCMC or a bootstrap maybe? Is that you're wondering?


Amelie LESCROEL <lescroel at cebc.cnrs.fr> wrote:

>Thanks Chris, I saw this one but I was wondering whether the situation 
>changed or was about to change. What about Pr(>|z|)? Are they reliable?
>Amelie
>
>Christopher David Desjardins a ?crit :
>> Regarding mcmcsamp() see 
>> https://mailman.stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001660.html specifically 
>>
>>
>> ...  In particular mcmcsamp is not currently working in that branch.  
>> My goal is to get mcmcsamp stable and to get all the other parts 
>> working in this branch then release it for testing and finally release 
>> lme4-1.0.  The subtext in that message is that only the models I 
>> describe above will be fit by the lme4-1.0 package.
>>
>>
>> On Feb 16, 2009, at 9:59 AM, Amelie LESCROEL wrote:
>>
>>> Dear all,
>>>
>>> Could someone explain to me why summary(lmer.object) is giving z 
>>> values and Pr(>|z|) when the lmer.object was fitted with 
>>> family=poisson? I used to fit linear mixed models with 
>>> family=gaussian and link="identity" or link="log", then used 
>>> mcmcsamp() and HPDinterval() for computing "confidence" intervals and 
>>> considered that a fixed effect was significant when the 95% HPD 
>>> interval did not include zero. Now, with family=poisson, when I use 
>>> mcmcsamp(), I get the following:
>>>
>>> > set.seed(101); mcmcfm1 <- mcmcsamp(fm1, n=10000)
>>> Error in .local(object, n, verbose, ...) : Update not yet written
>>>
>>> but I get z values and Pr(>|z|). Why is it so? Can we "trust" this 
>>> probability for judging of the fixed effects significance? Would the 
>>> mcmcsamp method be available soon for families other than gaussian?
>>>
>>> Thanks in advance,
>>>
>>> Am?lie
>>>
>>> PS: When will lme4_1.0 be released? Many thanks to Douglas Bates for 
>>> developing lme4 in interaction with users, and for sharing it. Please 
>>> don't give up on making it available in R. lme4 is a wonderful tool 
>>> which is helping me understanding penguin's foraging behaviour (and 
>>> advanced stats!) as it is helping others in plant ecology or human 
>>> psychology.
>>>
>>> > sessionInfo()
>>> R version 2.7.2 (2008-08-25)
>>> i386-pc-mingw32
>>>
>>> locale:
>>> LC_COLLATE=French_France.1252;LC_CTYPE=French_France.1252;LC_MONETARY=French_France.1252;LC_NUMERIC=C;LC_TIME=French_France.1252 
>>>
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>> other attached packages:
>>> [1] lme4_0.999375-28   Matrix_0.999375-17 lattice_0.17-17    gdata_2.4.2
>>> loaded via a namespace (and not attached):
>>> [1] grid_2.7.2   gtools_2.5.0 tools_2.7.2
>>>
>>>
>>>
>>> __________ Information from ESET Mail Security, version of virus 
>>> signature database 3857 (20090216) __________
>>>
>>> The message was checked by ESET Mail Security.
>>> http://www.eset.com
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> -----------------
>> Christopher David Desjardins
>> Ph.D. Student
>> Quantitative Methods in Education
>> Department of Educational Psychology
>> University of  Minnesota
>> http://blog.lib.umn.edu/desja004/educationalpsychology/
>>
>>
>>
>>
>>
>>
>>
>> __________ Information from ESET Mail Security, version of virus 
>> signature database 3857 (20090216) __________
>>
>> The message was checked by ESET Mail Security.
>> http://www.eset.com
>>
>>
>>
>>
>>
>
>
>
>__________ Information from ESET Mail Security, version of virus signature database 3860 (20090217) __________
>
>The message was checked by ESET Mail Security.
>http://www.eset.com
>
>

From david.springate at postgrad.manchester.ac.uk  Tue Feb 17 18:04:50 2009
From: david.springate at postgrad.manchester.ac.uk (David Springate)
Date: Tue, 17 Feb 2009 17:04:50 -0000
Subject: [R-sig-ME] interaction model and variance in lme
In-Reply-To: <40e66e0b0902120603v3966ef26t9e19cc13ff32cd94@mail.gmail.com>
References: <20090211221942.30803im8rfmrffgg@webmail.manchester.ac.uk>
	<40e66e0b0902120603v3966ef26t9e19cc13ff32cd94@mail.gmail.com>
Message-ID: <8869607CD1DF4005B08E977407E5B1C2@ds.man.ac.uk>

Thanks for the help!

The ML model that seems to be appropriate for my work is

model = lmer(trait~treatment + (1|family) +
(1|family:treatment),data=d,REML=F)

as I am most concerned with determining the interaction term of family and
treatment.

However, I still have a problem.  I need to get to the variance components
of the random effects to do some further downstream analysis.
The part of the summary output I need is:

Random effects:
 Groups           Name        Variance Std.Dev.
 family:treatment (Intercept)  5.42852 2.32992 
 family           (Intercept)  0.79754 0.89305 
 Residual                     17.16665 4.14327 
Number of obs: 172, groups: family:treatment, 49; family, 25

I have used VarCorr to get the variances for family and family:treatment:
a = VarCorr(model)
family.var = a$family[1,1]

but VarCorr doesn't have the residual variance term.  

How can I get to this variable?  Do I need to put an additional error term
in the model?

Thanks in advance!


-----Original Message-----
From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas
Bates
Sent: 12 February 2009 14:03
To: David Springate
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] interaction model and variance in lme

On Wed, Feb 11, 2009 at 4:19 PM, David Springate
<David.Springate at postgrad.manchester.ac.uk> wrote:
> Hi,

> I am new to R and I have been trying to build a model that I can extract
ML
> variance components for the interaction from, but am struggling to make
> sense of the formula.

> I am looking at the interaction of family (random) and two environmental
> treatments (fixed) on a trait.  So far I have tried:

> model=lme(trait~treatment,data=dataset,random=~1|family/treatment)

> But this seems to give me just the treatment nested in the family rather
> than family*treatment values.

Some of the difficulty here may be in nomenclature and notation.  The
phrase "family*treatment values" means different things in SAS and in
R.  It may help if you could describe verbally what you want to obtain
rather than symbolically.

For a model like this I would recommend using lmer from the lme4
package as it has several enhancements relative to lme from the nlme
package.

As you have seen, the model above, which would be written

lmer(trait ~ treatment + (1|family/treatment), dataset)

or, equivalently,

lmer(trait ~ treatment + (1|family) + (1|family:treatment), dataset)

provides fixed effects for the treatment (Intercept and an effect of
one level) plus the random effects for each family plus the random
effects for each family:treatment combination.  (In R an interaction
term is written family:treatment; the notation family*treatment
indicates crossing of fixed effects so that family*treatment expands
to family + treatment + family:treatment.)

Another model incorporating random effects for each level of treatment
within family is

lmer(trait ~ treatment + (treatment|family), dataset)

for which I prefer and alternative expression as

lmer(trait ~ treatment + (0+treatment|family), dataset)

These models are equivalent but have different parameterizations.
Suppose that the treatment levels are called A and B.  Then the
default model matrix for the treatment term provides the intercept and
the indicator of level B.  The model matrix for 0+treatment suppresses
the intercept and provides an indicator for A and an indicator for B.

The notation (0+treatment|family) produces a pair of random effects
for each level of family, one for treatment A and one for treatment B,
and the variance-covariance matrix for these random effects is a
general positive-definite 2x2 matrix.  (In SAS-speak this is an
"unconstrained" variance-covariance matrix but the mathematician in me
will not accept the concept of an unconstrained matrix that is subject
to the constraints of being symmetric and positive definite.)

I hope this helps but I encourage you to follow up on your question if
this did not answer it.


> I also tried:
>
> Model = lme(trait~family*treatment,data=dataset,random=~1|family)
>
> which gives me an interaction MS and F value in an anova, but seems to
treat
> each interaction between each treatment and family as a separate fixed
> factor.  Also VarCorr() doesn't seem to give a variance for the
interaction
> term.
>
> What am I doing wrong?
>
> I am sure this should be simple, but the docs seem pretty unclear to me on
> modeling interactions.
>
> Help please!
>
> David Springate
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From HDoran at air.org  Tue Feb 17 18:12:04 2009
From: HDoran at air.org (Doran, Harold)
Date: Tue, 17 Feb 2009 12:12:04 -0500
Subject: [R-sig-ME] interaction model and variance in lme
In-Reply-To: <8869607CD1DF4005B08E977407E5B1C2@ds.man.ac.uk>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE020C1278@DC1EXCL01.air.org>

It is an attribute. You can grab it like this:

> example(lmer)

> qq <- VarCorr(fm1)
> str(qq)
List of 1
 $ Subject: num [1:2, 1:2] 612.1 9.6 9.6 35.1
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:2] "(Intercept)" "Days"
  .. ..$ : chr [1:2] "(Intercept)" "Days"
  ..- attr(*, "stddev")= Named num [1:2] 24.74 5.92
  .. ..- attr(*, "names")= chr [1:2] "(Intercept)" "Days"
  ..- attr(*, "correlation")= num [1:2, 1:2] 1 0.0655 0.0655 1
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:2] "(Intercept)" "Days"
  .. .. ..$ : chr [1:2] "(Intercept)" "Days"
 - attr(*, "sc")= Named num 25.6
  ..- attr(*, "names")= chr "sigmaREML"
> attr(qq, 'sc')
sigmaREML 
 25.59182  

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of David Springate
> Sent: Tuesday, February 17, 2009 12:05 PM
> To: 'Douglas Bates'
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] interaction model and variance in lme
> 
> Thanks for the help!
> 
> The ML model that seems to be appropriate for my work is
> 
> model = lmer(trait~treatment + (1|family) +
> (1|family:treatment),data=d,REML=F)
> 
> as I am most concerned with determining the interaction term 
> of family and treatment.
> 
> However, I still have a problem.  I need to get to the 
> variance components of the random effects to do some further 
> downstream analysis.
> The part of the summary output I need is:
> 
> Random effects:
>  Groups           Name        Variance Std.Dev.
>  family:treatment (Intercept)  5.42852 2.32992 
>  family           (Intercept)  0.79754 0.89305 
>  Residual                     17.16665 4.14327 
> Number of obs: 172, groups: family:treatment, 49; family, 25
> 
> I have used VarCorr to get the variances for family and 
> family:treatment:
> a = VarCorr(model)
> family.var = a$family[1,1]
> 
> but VarCorr doesn't have the residual variance term.  
> 
> How can I get to this variable?  Do I need to put an 
> additional error term in the model?
> 
> Thanks in advance!
> 
> 
> -----Original Message-----
> From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf 
> Of Douglas Bates
> Sent: 12 February 2009 14:03
> To: David Springate
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] interaction model and variance in lme
> 
> On Wed, Feb 11, 2009 at 4:19 PM, David Springate 
> <David.Springate at postgrad.manchester.ac.uk> wrote:
> > Hi,
> 
> > I am new to R and I have been trying to build a model that I can 
> > extract
> ML
> > variance components for the interaction from, but am struggling to 
> > make sense of the formula.
> 
> > I am looking at the interaction of family (random) and two 
> > environmental treatments (fixed) on a trait.  So far I have tried:
> 
> > model=lme(trait~treatment,data=dataset,random=~1|family/treatment)
> 
> > But this seems to give me just the treatment nested in the family 
> > rather than family*treatment values.
> 
> Some of the difficulty here may be in nomenclature and 
> notation.  The phrase "family*treatment values" means 
> different things in SAS and in R.  It may help if you could 
> describe verbally what you want to obtain rather than symbolically.
> 
> For a model like this I would recommend using lmer from the 
> lme4 package as it has several enhancements relative to lme 
> from the nlme package.
> 
> As you have seen, the model above, which would be written
> 
> lmer(trait ~ treatment + (1|family/treatment), dataset)
> 
> or, equivalently,
> 
> lmer(trait ~ treatment + (1|family) + (1|family:treatment), dataset)
> 
> provides fixed effects for the treatment (Intercept and an 
> effect of one level) plus the random effects for each family 
> plus the random effects for each family:treatment 
> combination.  (In R an interaction term is written 
> family:treatment; the notation family*treatment indicates 
> crossing of fixed effects so that family*treatment expands to 
> family + treatment + family:treatment.)
> 
> Another model incorporating random effects for each level of 
> treatment within family is
> 
> lmer(trait ~ treatment + (treatment|family), dataset)
> 
> for which I prefer and alternative expression as
> 
> lmer(trait ~ treatment + (0+treatment|family), dataset)
> 
> These models are equivalent but have different parameterizations.
> Suppose that the treatment levels are called A and B.  Then 
> the default model matrix for the treatment term provides the 
> intercept and the indicator of level B.  The model matrix for 
> 0+treatment suppresses the intercept and provides an 
> indicator for A and an indicator for B.
> 
> The notation (0+treatment|family) produces a pair of random 
> effects for each level of family, one for treatment A and one 
> for treatment B, and the variance-covariance matrix for these 
> random effects is a general positive-definite 2x2 matrix.  
> (In SAS-speak this is an "unconstrained" variance-covariance 
> matrix but the mathematician in me will not accept the 
> concept of an unconstrained matrix that is subject to the 
> constraints of being symmetric and positive definite.)
> 
> I hope this helps but I encourage you to follow up on your 
> question if this did not answer it.
> 
> 
> > I also tried:
> >
> > Model = lme(trait~family*treatment,data=dataset,random=~1|family)
> >
> > which gives me an interaction MS and F value in an anova, 
> but seems to
> treat
> > each interaction between each treatment and family as a 
> separate fixed 
> > factor.  Also VarCorr() doesn't seem to give a variance for the
> interaction
> > term.
> >
> > What am I doing wrong?
> >
> > I am sure this should be simple, but the docs seem pretty 
> unclear to 
> > me on modeling interactions.
> >
> > Help please!
> >
> > David Springate
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From bolker at ufl.edu  Tue Feb 17 21:23:28 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 17 Feb 2009 15:23:28 -0500
Subject: [R-sig-ME] No estimated scale value given
In-Reply-To: <001501c9904e$841d6ea0$8c584be0$@s.wade@reading.ac.uk>
References: <000001c9903b$24936740$6dba35c0$@s.wade@reading.ac.uk>
	<4999740D.8090907@ufl.edu>
	<001501c9904e$841d6ea0$8c584be0$@s.wade@reading.ac.uk>
Message-ID: <499B1CC0.9030302@ufl.edu>

  The not-necessarily-obvious thing is that none of the
parameter estimates change when you go from poisson,
binomial, etc. to their quasi- equivalents -- the variance
on all points is inflated by the same amount, so the
point estimates don't change.  So you can go ahead and
use the quasi- variant to get the estimated scale.

  I'm still struggling with exactly what "sigma" is in
the quasi- case (as is Doug Bates) -- it seems
NOT equal to the Pearson residuals^2/(resid df) ?

  It would be worth exploring this some more ...

  cheers
   Ben Bolker

set.seed(1001)
ntot <- 1000
x <- runif(ntot)
f <- rep(1:20,each=50)
reff <- rnorm(20,mean=0,sd=0.5)
y <- rnbinom(ntot,mu=exp(2*x-1+reff[f]),size=1)
library(lme4)

m1 <- glmer(y~x+(1|f),family=poisson)
m2 <- update(m1,family=quasipoisson)

lme4:::sigma(m1) ## 1
lme4:::sigma(m2) ## 1.04 (??)

## residuals returned are Pearson residuals
##  (uncorrected for overdispersion)

myres <- (y-fitted(m1))/sqrt(fitted(m1))
all.equal(myres,residuals(m1))
## not quite identical, but nearly
all(abs(myres-residuals(m1))<1e-4)

all.equal(residuals(m1),residuals(m2)) ## TRUE

sum(residuals(m1)^2)/ntot ## NOT sigmaML
m1 at deviance["sigmaML"]
m2 at deviance["sigmaML"]


Amy Wade wrote:
> Ben,
> 
> When I fit a quasi- variant it gives what seems to be a sensible value for
> the sigma. The trouble is I would like to know if my models are
> overdispersed without the quasi-variant. I'm using data with poisson
> distribution from which I know what the estimated scale is and it still
> gives '1' when it should be 0.97. There was a discussion about this
> previously and it was suggested to use 'lme4:::sigma(model)', that just
> gives me the same result as using 'summary(model)@sigma'.
> 
> Thanks very much for your help.
> 
> Amy
> 
> Should I have put this on the public thread? Not really sure how to do that!
> 
> 
> 
> -----Original Message-----
> From: Ben Bolker [mailto:bolker at ufl.edu] 
> Sent: 16 February 2009 14:11
> To: Amy Wade
> Subject: Re: [R-sig-ME] No estimated scale value given
> 
>   What happens if you fit the same model with a quasi- variant and then try
> to access sigma?
> 
> 
> 
>   Ben Bolker
> 
> Amy Wade wrote:
>> Hello,
>>
>>  
>>
>> I'm trying to run generalized linear mixed models using the lmer() 
>> function in the lme4 package. The problem is that the output does not 
>> give me a value for the Estimated scale. The rest of the output is as 
>> it should be. This makes it very difficult to assess whether the model 
>> is overdispersed. My colleague tried the same code on his computer and 
>> it did give an estimated scale value. I tried unistalling R and 
>> reinstalling the latest edition
>> (2.8.1) with the latest lme4, but this made no difference. I also 
>> tried 'summary(model)@sigma' which people suggested on the CRAN forums 
>> to extract the scale parameter. This always gave me an answer of '1' 
>> even when I used data where I knew this was not the case. When I load 
>> up the lme4 library it does warn me about several objects being masked
> from 'stats' and 'base'.
>>  
>>
>> Any idea why the output is not giving me the estimate scale? Or any 
>> idea how to extract it somehow?
>>
>>  
>>
>> Many thanks,
>>
>> Amy
>>
>>  
>>
>>  
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> --
> Ben Bolker
> Associate professor, Biology Dep't, Univ. of Florida bolker at ufl.edu /
> www.zoology.ufl.edu/bolker GPG key:
> www.zoology.ufl.edu/bolker/benbolker-publickey.asc
> 
> 


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From bates at stat.wisc.edu  Wed Feb 18 20:56:56 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 18 Feb 2009 13:56:56 -0600
Subject: [R-sig-ME] Desperately seeking help with simple lmer fit (lmer
	vs Proc Mixed)
In-Reply-To: <OF349028FC.ED6163B9-ON86257561.0062FFF6-86257561.0066B1BE@abbott.com>
References: <40e66e0b0810071531q425c5520o1ff8c4fe15699297@mail.gmail.com>
	<OF349028FC.ED6163B9-ON86257561.0062FFF6-86257561.0066B1BE@abbott.com>
Message-ID: <40e66e0b0902181156m62b891a7m17017717f216a6eb@mail.gmail.com>

Thanks for the example.

The estimates of the variance components are the result of an
optimization and it appears that there are two local optima for this
problem.  I'm sorry to say that the optimum determined by SAS PROC
MIXED represents a better fit (a lower deviance) than the one
determined by lmer.

If you plot the response versus run, say

dotplot(reorder(Run, Y) ~ Y)

you will see that there is one run (number 13) with unusually large Y,
which may have abnormal influence.  I was able to reproduce the SAS
results by modifying the starting estimates.  This is not optimal
because it means you need to know the correct answer before you can
calculate it.

> (fm1 <- lmer(Y ~ (1|Run) + (1|Prep), pr, verbose = TRUE))
  0:     1536.6629: 0.666667 0.384900
  1:     1441.4487:  1.44044  1.01837
  2:     1421.4662:  2.35838 0.621655
  3:     1408.1762:  3.27848 0.229969
  4:     1401.9874:  4.04888  0.00000
  5:     1399.8678:  4.59188  0.00000
  6:     1398.9477:  5.12145  0.00000
  7:     1398.7722:  5.41633  0.00000
  8:     1398.7543:  5.53332  0.00000
  9:     1398.7539:  5.55421  0.00000
 10:     1398.7539:  5.55527 1.68436e-07
 11:     1398.7539:  5.55527 1.68436e-07
Linear mixed model fit by REML
Formula: Y ~ (1 | Run) + (1 | Prep)
   Data: pr
  AIC  BIC logLik deviance REMLdev
 1407 1417 -699.4     1410    1399
Random effects:
 Groups   Name        Variance   Std.Dev.
 Run      (Intercept) 3.5859e+05 5.9882e+02
 Prep     (Intercept) 3.2965e-10 1.8156e-05
 Residual             1.1619e+04 1.0779e+02
Number of obs: 108, groups: Run, 18; Prep, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)  28216.5      141.5   199.4
> (fm1a <- lmer(Y ~ (1|Run) + (1|Prep), pr, verbose = TRUE, start = c(4,4)))
  0:     1393.6144:  4.00000  4.00000
  1:     1393.5078:  3.74366  4.09884
  2:     1393.4819:  3.72800  4.32664
  3:     1393.4810:  3.73535  4.37362
  4:     1393.4810:  3.73373  4.37885
  5:     1393.4810:  3.73404  4.37877
  6:     1393.4810:  3.73404  4.37875
  7:     1393.4810:  3.73404  4.37875
Linear mixed model fit by REML
Formula: Y ~ (1 | Run) + (1 | Prep)
   Data: pr
  AIC  BIC logLik deviance REMLdev
 1401 1412 -696.7     1406    1393
Random effects:
 Groups   Name        Variance Std.Dev.
 Run      (Intercept) 162010   402.50
 Prep     (Intercept) 222784   472.00
 Residual              11619   107.79
Number of obs: 108, groups: Run, 18; Prep, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)    28216        215   131.2

If you remove Run 13 you do get results like those from SAS using lmer

> (fm2 <- lmer(Y ~ (1|Run) + (1|Prep), pr, verbose = TRUE, subset = Run != 13))
  0:     1426.8110: 0.666667 0.396059
  1:     1342.0591:  1.47666 0.982502
  2:     1325.5670:  2.35364 0.501989
  3:     1311.1286:  3.60322  0.00000
  4:     1309.3047:  3.96698  0.00000
  5:     1307.8040:  4.56388 1.65053e-05
  6:     1307.5200:  4.88366 6.39529e-05
  7:     1307.4763:  5.04660 0.000170199
  8:     1307.4746:  5.08457 0.000341660
  9:     1307.4745:  5.08786 0.000627531
 10:     1307.4745:  5.08806 0.00113168
 11:     1307.4743:  5.09250 0.0198356
 12:     1306.7915:  5.55085  2.00076
 13:     1306.7202:  5.57778  2.29410
 14:     1304.5962:  3.26016  2.72067
 15:     1304.1219:  3.90630  4.98691
 16:     1303.9334:  3.70822  4.85924
 17:     1303.7839:  3.39994  4.44476
 18:     1303.7039:  3.78966  4.10571
 19:     1303.6539:  3.50693  3.67340
 20:     1303.6333:  3.63992  3.74762
 21:     1303.6285:  3.55042  3.87085
 22:     1303.6259:  3.59760  3.87143
 23:     1303.6258:  3.59381  3.86414
 24:     1303.6258:  3.59257  3.86456
 25:     1303.6258:  3.59308  3.86577
 26:     1303.6258:  3.59318  3.86504
 27:     1303.6258:  3.59312  3.86514
Linear mixed model fit by REML
Formula: Y ~ (1 | Run) + (1 | Prep)
   Data: pr
 Subset: Run != 13
  AIC  BIC logLik deviance REMLdev
 1312 1322 -651.8     1316    1304
Random effects:
 Groups   Name        Variance Std.Dev.
 Run      (Intercept) 135857   368.59
 Prep     (Intercept) 157206   396.49
 Residual              10523   102.58
Number of obs: 102, groups: Run, 17; Prep, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)  28161.8      185.5   151.8


This example may be motivation for me to try out a modification in the
optimization method that I have been contemplating.


On Wed, Feb 18, 2009 at 12:41 PM, David LeBlond
<David.LeBlond at abbott.com> wrote:
>
> Dear Dr. Bates,
>
> First, apologies for this request ....  however I am desperate. I have spent
> 3 days on this. I must have a very fundamental misunderstanding of lmer or
> factors or grouped data or syntax or something else.
>
> To show due diligence, I have read all I can find on lmer syntax, gone
> through the SASmixed Package including the "lmer for SAS PROC MIXED Users",
> other presentations from yourself and others on lmer. I have visited the
> blogs.  In the last 24h on this problem I have tried every permutation I can
> think of.
>
> The experiment is a simple balanced 3 level nested variance component study
> virtually identical to Example 4.4, page 156 of Little et al, SAS System for
> Mixed Models. There are 3 variables: There are 6 Y results nested within
> each of 18 runs. There are 3 runs nested within each of 6 preps. Site is
> ignored.
>
> I am using the lmer code analagous to what you used to analyze Example 4.4
> in lmer:
>
>         lmer syntax:         lmer(Y~1+(1|Prep/Run))
>         MIXED syntax:         proc mixed;class run prep site; model
> y=/s;random prep run(prep);run;
>
> The error variance components and fixed effect estimates agree, but the
> variance components for Prep and Run are completely different.
>
> Source                lmer                SAS
> Run                   358580                162010
> Prep                 0.012776        222784
>
> For a nearly identical problem, (Semi2 in SASmixed) lmer and MIXED give
> identical results. As I can repeat the SAS result in JMP, I trust it. I
> believe I am making some error in my use of lmer. I need to complete the
> analysis in lmer, not SAS or JMP.
>
> Can you or someone please explain to me why the variance component estimates
> are so radically different between MIXED and lmer? I know I am making some
> dumb mistake and I am prepared to be humbled.
>
> Thank you so much!!
>
> Dave
> ***************************************************************
> PS - Below I am sending my lmer data (a dif file), code and output along
> with the SAS data (in code), code and output.
>
>
> Here is the Excel data needed for R:
>
>
> Below is the R code which also contains the outputs and SAS data and code
> (sorry for length - 108 lines of data).
>
> library(lattice)
> library(foreign)
> library(lme4)
> Suitability<-read.DIF("C:/Documents and Settings/leblodj/Desktop/Synthroid
> Dissolution Method Change Lynn Davis Feb09/Suitability.dif",
>                header=TRUE,transpose=TRUE,nrows=369,colClasses="character")
>
> str(Suitability)
> Y<-as.numeric(Suitability$Y)
> Site<-ordered(Suitability$Site)
> Prep<-ordered(Suitability$Prep)
> Run<-ordered(Suitability$Run)
> levels(Run)
> levels(Prep)
> lmer(Y~1+(1|Prep/Run))
>
> ####### lmer output #################
> #Linear mixed model fit by REML
> #Formula: Y ~ 1 + (1 | Prep/Run)
> #  AIC  BIC logLik deviance REMLdev
> # 1407 1417 -699.4     1410    1399
> #Random effects:
> # Groups   Name        Variance   Std.Dev.
> # Run:Prep (Intercept) 3.5858e+05 598.81878
> # Prep     (Intercept) 1.2776e-02   0.11303
> # Residual             1.1619e+04 107.79332
> #Number of obs: 108, groups: Run:Prep, 18; Prep, 6
> #
> #Fixed effects:
> #            Estimate Std. Error t value
> #(Intercept)  28216.5      141.5   199.4
> #######################################
>
>
>
> #############################################
>
>
> ##### SAS Proc Mixed Output (matches JMP)  ##
> ##### See full SAS code Below  ##############
> # Covariance Parameter
> #      Estimates
> #
> #Cov Parm      Estimate
> #
> #Prep            222784
> #Run(Prep)       162010
> #Residual         11619
>                    Solution for Fixed Effects
>                          Standard
> Effect       Estimate       Error      DF    t Value    Pr > |t|
> Intercept       28216      215.03       5     131.22      <.0001
> ############################################
> ###### SAS Proc Mixed Code  ##########
> data Suitability;
> input Site$        Prep$        Run$        Y;
> cards;
> 2        1        1        28325
> 2        1        1        28314
> 2        1        1        28277
> 2        1        1        28183
> 2        1        1        28423
> 2        1        1        28299
> 2        1        2        27578
> 2        1        2        27724
> 2        1        2        27541
> 2        1        2        27686
> 2        1        2        27679
> 2        1        2        27567
> 2        1        3        27207
> 2        1        3        27351
> 2        1        3        27416
> 2        1        3        27292
> 2        1        3        27168
> 2        1        3        27117
> 2        2        4        28036
> 2        2        4        28042
> 2        2        4        27902
> 2        2        4        28110
> 2        2        4        27794
> 2        2        4        28181
> 2        2        5        28018
> 2        2        5        28318
> 2        2        5        28170
> 2        2        5        28410
> 2        2        5        28444
> 2        2        5        28620
> 2        2        6        27764
> 2        2        6        28008
> 2        2        6        27849
> 2        2        6        28096
> 2        2        6        27788
> 2        2        6        27884
> 2        3        7        28201
> 2        3        7        28224
> 2        3        7        28206
> 2        3        7        28462
> 2        3        7        28290
> 2        3        7        28303
> 2        3        8        28244
> 2        3        8        28125
> 2        3        8        28078
> 2        3        8        28023
> 2        3        8        28037
> 2        3        8        28014
> 2        3        9        28738
> 2        3        9        28726
> 2        3        9        28765
> 2        3        9        28534
> 2        3        9        28518
> 2        3        9        28489
> 1        4        10        28717
> 1        4        10        28622
> 1        4        10        28790
> 1        4        10        28697
> 1        4        10        28531
> 1        4        10        28502
> 1        4        11        28478
> 1        4        11        28425
> 1        4        11        28609
> 1        4        11        28541
> 1        4        11        28518
> 1        4        11        28492
> 1        4        12        28808
> 1        4        12        28764
> 1        4        12        28745
> 1        4        12        28786
> 1        4        12        28773
> 1        4        12        28622
> 1        5        13        29405
> 1        5        13        29372
> 1        5        13        29435
> 1        5        13        29831
> 1        5        13        29550
> 1        5        13        29402
> 1        5        14        28708
> 1        5        14        28684
> 1        5        14        28772
> 1        5        14        28705
> 1        5        14        28749
> 1        5        14        28754
> 1        5        15        28559
> 1        5        15        28584
> 1        5        15        28636
> 1        5        15        28713
> 1        5        15        28652
> 1        5        15        28530
> 1        6        16        26991
> 1        6        16        27009
> 1        6        16        26949
> 1        6        16        26957
> 1        6        16        26965
> 1        6        16        26934
> 1        6        17        28174
> 1        6        17        28074
> 1        6        17        28113
> 1        6        17        28154
> 1        6        17        28200
> 1        6        17        27952
> 1        6        18        27628
> 1        6        18        27569
> 1        6        18        27624
> 1        6        18        27737
> 1        6        18        27675
> 1        6        18        27654
> ;
>
> proc mixed;
>   class run prep site;
>   model y=/s;
>   random prep run(prep);run;
>
>
> ________________________________
> David LeBlond
> Senior Research Statistician
> MS Statistics, MS/PhD Biochemistry
> Exploratory Statistics Global Pharmaceutical R&D
> 100 Abbott Park Road
> Abbott Park, IL 60064-3500
> USA R13-1 847-935-6031
> AP9A-1 847-935-1899
> David.LeBlond at abbott.com
>
> ________________________________
> This communication may contain information that is proprietary,
> confidential, or exempt from disclosure. If you are not the intended
> recipient, please note that any other dissemination, distribution, use or
> copying of this communication is strictly prohibited. Anyone who receives
> this message in error should notify the sender immediately by telephone or
> by return e-mail and delete it from his or her computer.
>



From bates at stat.wisc.edu  Wed Feb 18 21:20:58 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 18 Feb 2009 14:20:58 -0600
Subject: [R-sig-ME] Desperately seeking help with simple lmer fit (lmer
	vs Proc Mixed)
In-Reply-To: <40e66e0b0902181156m62b891a7m17017717f216a6eb@mail.gmail.com>
References: <40e66e0b0810071531q425c5520o1ff8c4fe15699297@mail.gmail.com>
	<OF349028FC.ED6163B9-ON86257561.0062FFF6-86257561.0066B1BE@abbott.com>
	<40e66e0b0902181156m62b891a7m17017717f216a6eb@mail.gmail.com>
Message-ID: <40e66e0b0902181220l61df2c59v8088b7dc82beac17@mail.gmail.com>

On Wed, Feb 18, 2009 at 1:56 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
> Thanks for the example.
>
> The estimates of the variance components are the result of an
> optimization and it appears that there are two local optima for this
> problem.  I'm sorry to say that the optimum determined by SAS PROC
> MIXED represents a better fit (a lower deviance) than the one
> determined by lmer.
>
> If you plot the response versus run, say
>
> dotplot(reorder(Run, Y) ~ Y)
>
> you will see that there is one run (number 13) with unusually large Y,
> which may have abnormal influence.  I was able to reproduce the SAS
> results by modifying the starting estimates.  This is not optimal
> because it means you need to know the correct answer before you can
> calculate it.
>
>> (fm1 <- lmer(Y ~ (1|Run) + (1|Prep), pr, verbose = TRUE))
>  0:     1536.6629: 0.666667 0.384900
>  1:     1441.4487:  1.44044  1.01837
>  2:     1421.4662:  2.35838 0.621655
>  3:     1408.1762:  3.27848 0.229969
>  4:     1401.9874:  4.04888  0.00000
>  5:     1399.8678:  4.59188  0.00000
>  6:     1398.9477:  5.12145  0.00000
>  7:     1398.7722:  5.41633  0.00000
>  8:     1398.7543:  5.53332  0.00000
>  9:     1398.7539:  5.55421  0.00000
>  10:     1398.7539:  5.55527 1.68436e-07
>  11:     1398.7539:  5.55527 1.68436e-07
> Linear mixed model fit by REML
> Formula: Y ~ (1 | Run) + (1 | Prep)
>   Data: pr
>  AIC  BIC logLik deviance REMLdev
>  1407 1417 -699.4     1410    1399
> Random effects:
>  Groups   Name        Variance   Std.Dev.
>  Run      (Intercept) 3.5859e+05 5.9882e+02
>  Prep     (Intercept) 3.2965e-10 1.8156e-05
>  Residual             1.1619e+04 1.0779e+02
> Number of obs: 108, groups: Run, 18; Prep, 6
>
> Fixed effects:
>            Estimate Std. Error t value
> (Intercept)  28216.5      141.5   199.4
>> (fm1a <- lmer(Y ~ (1|Run) + (1|Prep), pr, verbose = TRUE, start = c(4,4)))
>  0:     1393.6144:  4.00000  4.00000
>  1:     1393.5078:  3.74366  4.09884
>  2:     1393.4819:  3.72800  4.32664
>  3:     1393.4810:  3.73535  4.37362
>  4:     1393.4810:  3.73373  4.37885
>  5:     1393.4810:  3.73404  4.37877
>  6:     1393.4810:  3.73404  4.37875
>  7:     1393.4810:  3.73404  4.37875
> Linear mixed model fit by REML
> Formula: Y ~ (1 | Run) + (1 | Prep)
>   Data: pr
>  AIC  BIC logLik deviance REMLdev
>  1401 1412 -696.7     1406    1393
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  Run      (Intercept) 162010   402.50
>  Prep     (Intercept) 222784   472.00
>  Residual              11619   107.79
> Number of obs: 108, groups: Run, 18; Prep, 6
>
> Fixed effects:
>            Estimate Std. Error t value
> (Intercept)    28216        215   131.2
>
> If you remove Run 13 you do get results like those from SAS using lmer
>
>> (fm2 <- lmer(Y ~ (1|Run) + (1|Prep), pr, verbose = TRUE, subset = Run != 13))
>  0:     1426.8110: 0.666667 0.396059
>  1:     1342.0591:  1.47666 0.982502
>  2:     1325.5670:  2.35364 0.501989
>  3:     1311.1286:  3.60322  0.00000
>  4:     1309.3047:  3.96698  0.00000
>  5:     1307.8040:  4.56388 1.65053e-05
>  6:     1307.5200:  4.88366 6.39529e-05
>  7:     1307.4763:  5.04660 0.000170199
>  8:     1307.4746:  5.08457 0.000341660
>  9:     1307.4745:  5.08786 0.000627531
>  10:     1307.4745:  5.08806 0.00113168
>  11:     1307.4743:  5.09250 0.0198356
>  12:     1306.7915:  5.55085  2.00076
>  13:     1306.7202:  5.57778  2.29410
>  14:     1304.5962:  3.26016  2.72067
>  15:     1304.1219:  3.90630  4.98691
>  16:     1303.9334:  3.70822  4.85924
>  17:     1303.7839:  3.39994  4.44476
>  18:     1303.7039:  3.78966  4.10571
>  19:     1303.6539:  3.50693  3.67340
>  20:     1303.6333:  3.63992  3.74762
>  21:     1303.6285:  3.55042  3.87085
>  22:     1303.6259:  3.59760  3.87143
>  23:     1303.6258:  3.59381  3.86414
>  24:     1303.6258:  3.59257  3.86456
>  25:     1303.6258:  3.59308  3.86577
>  26:     1303.6258:  3.59318  3.86504
>  27:     1303.6258:  3.59312  3.86514
> Linear mixed model fit by REML
> Formula: Y ~ (1 | Run) + (1 | Prep)
>   Data: pr
>  Subset: Run != 13
>  AIC  BIC logLik deviance REMLdev
>  1312 1322 -651.8     1316    1304
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  Run      (Intercept) 135857   368.59
>  Prep     (Intercept) 157206   396.49
>  Residual              10523   102.58
> Number of obs: 102, groups: Run, 17; Prep, 6
>
> Fixed effects:
>            Estimate Std. Error t value
> (Intercept)  28161.8      185.5   151.8
>
>
> This example may be motivation for me to try out a modification in the
> optimization method that I have been contemplating.

Well, the good news is that the modification was successful on this example

> (fm1 <- lmer(Y ~ 1 + (1|Run) + (1|Prep), pr, verbose = TRUE))
  0:     1536.6629: 0.816497 0.620403
  1:     1405.8868:  1.66562  1.14860
  2:     1399.0326:  1.71590  1.44355
  3:     1394.7782:  1.84561  1.71319
  4:     1393.9976:  1.79780  2.00855
  5:     1393.9690:  2.09290  2.05797
  6:     1393.5053:  1.96491  2.07162
  7:     1393.4829:  1.92326  2.08972
  8:     1393.4810:  1.93265  2.09030
  9:     1393.4810:  1.93240  2.09189
 10:     1393.4810:  1.93236  2.09254
 11:     1393.4810:  1.93237  2.09255
Linear mixed model fit by REML
Formula: Y ~ 1 + (1 | Run) + (1 | Prep)
   Data: pr
  AIC  BIC logLik deviance REMLdev
 1401 1412 -696.7     1406    1393
Random effects:
 Groups   Name        Variance Std.Dev.
 Run      (Intercept) 162010   402.50
 Prep     (Intercept) 222784   472.00
 Residual              11619   107.79
Number of obs: 108, groups: Run, 18; Prep, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)    28216        215   131.2

The bad news is that I made the modification in a development version
of the lme4 package and that version won't be ready for prime time for
a while.  I still have a lot of work to do on the generalized linear
mixed models.


> On Wed, Feb 18, 2009 at 12:41 PM, David LeBlond
> <David.LeBlond at abbott.com> wrote:
>>
>> Dear Dr. Bates,
>>
>> First, apologies for this request ....  however I am desperate. I have spent
>> 3 days on this. I must have a very fundamental misunderstanding of lmer or
>> factors or grouped data or syntax or something else.
>>
>> To show due diligence, I have read all I can find on lmer syntax, gone
>> through the SASmixed Package including the "lmer for SAS PROC MIXED Users",
>> other presentations from yourself and others on lmer. I have visited the
>> blogs.  In the last 24h on this problem I have tried every permutation I can
>> think of.
>>
>> The experiment is a simple balanced 3 level nested variance component study
>> virtually identical to Example 4.4, page 156 of Little et al, SAS System for
>> Mixed Models. There are 3 variables: There are 6 Y results nested within
>> each of 18 runs. There are 3 runs nested within each of 6 preps. Site is
>> ignored.
>>
>> I am using the lmer code analagous to what you used to analyze Example 4.4
>> in lmer:
>>
>>         lmer syntax:         lmer(Y~1+(1|Prep/Run))
>>         MIXED syntax:         proc mixed;class run prep site; model
>> y=/s;random prep run(prep);run;
>>
>> The error variance components and fixed effect estimates agree, but the
>> variance components for Prep and Run are completely different.
>>
>> Source                lmer                SAS
>> Run                   358580                162010
>> Prep                 0.012776        222784
>>
>> For a nearly identical problem, (Semi2 in SASmixed) lmer and MIXED give
>> identical results. As I can repeat the SAS result in JMP, I trust it. I
>> believe I am making some error in my use of lmer. I need to complete the
>> analysis in lmer, not SAS or JMP.
>>
>> Can you or someone please explain to me why the variance component estimates
>> are so radically different between MIXED and lmer? I know I am making some
>> dumb mistake and I am prepared to be humbled.
>>
>> Thank you so much!!
>>
>> Dave
>> ***************************************************************
>> PS - Below I am sending my lmer data (a dif file), code and output along
>> with the SAS data (in code), code and output.
>>
>>
>> Here is the Excel data needed for R:
>>
>>
>> Below is the R code which also contains the outputs and SAS data and code
>> (sorry for length - 108 lines of data).
>>
>> library(lattice)
>> library(foreign)
>> library(lme4)
>> Suitability<-read.DIF("C:/Documents and Settings/leblodj/Desktop/Synthroid
>> Dissolution Method Change Lynn Davis Feb09/Suitability.dif",
>>                header=TRUE,transpose=TRUE,nrows=369,colClasses="character")
>>
>> str(Suitability)
>> Y<-as.numeric(Suitability$Y)
>> Site<-ordered(Suitability$Site)
>> Prep<-ordered(Suitability$Prep)
>> Run<-ordered(Suitability$Run)
>> levels(Run)
>> levels(Prep)
>> lmer(Y~1+(1|Prep/Run))
>>
>> ####### lmer output #################
>> #Linear mixed model fit by REML
>> #Formula: Y ~ 1 + (1 | Prep/Run)
>> #  AIC  BIC logLik deviance REMLdev
>> # 1407 1417 -699.4     1410    1399
>> #Random effects:
>> # Groups   Name        Variance   Std.Dev.
>> # Run:Prep (Intercept) 3.5858e+05 598.81878
>> # Prep     (Intercept) 1.2776e-02   0.11303
>> # Residual             1.1619e+04 107.79332
>> #Number of obs: 108, groups: Run:Prep, 18; Prep, 6
>> #
>> #Fixed effects:
>> #            Estimate Std. Error t value
>> #(Intercept)  28216.5      141.5   199.4
>> #######################################
>>
>>
>>
>> #############################################
>>
>>
>> ##### SAS Proc Mixed Output (matches JMP)  ##
>> ##### See full SAS code Below  ##############
>> # Covariance Parameter
>> #      Estimates
>> #
>> #Cov Parm      Estimate
>> #
>> #Prep            222784
>> #Run(Prep)       162010
>> #Residual         11619
>>                    Solution for Fixed Effects
>>                          Standard
>> Effect       Estimate       Error      DF    t Value    Pr > |t|
>> Intercept       28216      215.03       5     131.22      <.0001
>> ############################################
>> ###### SAS Proc Mixed Code  ##########
>> data Suitability;
>> input Site$        Prep$        Run$        Y;
>> cards;
>> 2        1        1        28325
>> 2        1        1        28314
>> 2        1        1        28277
>> 2        1        1        28183
>> 2        1        1        28423
>> 2        1        1        28299
>> 2        1        2        27578
>> 2        1        2        27724
>> 2        1        2        27541
>> 2        1        2        27686
>> 2        1        2        27679
>> 2        1        2        27567
>> 2        1        3        27207
>> 2        1        3        27351
>> 2        1        3        27416
>> 2        1        3        27292
>> 2        1        3        27168
>> 2        1        3        27117
>> 2        2        4        28036
>> 2        2        4        28042
>> 2        2        4        27902
>> 2        2        4        28110
>> 2        2        4        27794
>> 2        2        4        28181
>> 2        2        5        28018
>> 2        2        5        28318
>> 2        2        5        28170
>> 2        2        5        28410
>> 2        2        5        28444
>> 2        2        5        28620
>> 2        2        6        27764
>> 2        2        6        28008
>> 2        2        6        27849
>> 2        2        6        28096
>> 2        2        6        27788
>> 2        2        6        27884
>> 2        3        7        28201
>> 2        3        7        28224
>> 2        3        7        28206
>> 2        3        7        28462
>> 2        3        7        28290
>> 2        3        7        28303
>> 2        3        8        28244
>> 2        3        8        28125
>> 2        3        8        28078
>> 2        3        8        28023
>> 2        3        8        28037
>> 2        3        8        28014
>> 2        3        9        28738
>> 2        3        9        28726
>> 2        3        9        28765
>> 2        3        9        28534
>> 2        3        9        28518
>> 2        3        9        28489
>> 1        4        10        28717
>> 1        4        10        28622
>> 1        4        10        28790
>> 1        4        10        28697
>> 1        4        10        28531
>> 1        4        10        28502
>> 1        4        11        28478
>> 1        4        11        28425
>> 1        4        11        28609
>> 1        4        11        28541
>> 1        4        11        28518
>> 1        4        11        28492
>> 1        4        12        28808
>> 1        4        12        28764
>> 1        4        12        28745
>> 1        4        12        28786
>> 1        4        12        28773
>> 1        4        12        28622
>> 1        5        13        29405
>> 1        5        13        29372
>> 1        5        13        29435
>> 1        5        13        29831
>> 1        5        13        29550
>> 1        5        13        29402
>> 1        5        14        28708
>> 1        5        14        28684
>> 1        5        14        28772
>> 1        5        14        28705
>> 1        5        14        28749
>> 1        5        14        28754
>> 1        5        15        28559
>> 1        5        15        28584
>> 1        5        15        28636
>> 1        5        15        28713
>> 1        5        15        28652
>> 1        5        15        28530
>> 1        6        16        26991
>> 1        6        16        27009
>> 1        6        16        26949
>> 1        6        16        26957
>> 1        6        16        26965
>> 1        6        16        26934
>> 1        6        17        28174
>> 1        6        17        28074
>> 1        6        17        28113
>> 1        6        17        28154
>> 1        6        17        28200
>> 1        6        17        27952
>> 1        6        18        27628
>> 1        6        18        27569
>> 1        6        18        27624
>> 1        6        18        27737
>> 1        6        18        27675
>> 1        6        18        27654
>> ;
>>
>> proc mixed;
>>   class run prep site;
>>   model y=/s;
>>   random prep run(prep);run;
>>
>>
>> ________________________________
>> David LeBlond
>> Senior Research Statistician
>> MS Statistics, MS/PhD Biochemistry
>> Exploratory Statistics Global Pharmaceutical R&D
>> 100 Abbott Park Road
>> Abbott Park, IL 60064-3500
>> USA R13-1 847-935-6031
>> AP9A-1 847-935-1899
>> David.LeBlond at abbott.com
>>
>> ________________________________
>> This communication may contain information that is proprietary,
>> confidential, or exempt from disclosure. If you are not the intended
>> recipient, please note that any other dissemination, distribution, use or
>> copying of this communication is strictly prohibited. Anyone who receives
>> this message in error should notify the sender immediately by telephone or
>> by return e-mail and delete it from his or her computer.
>>
>



From bates at stat.wisc.edu  Wed Feb 18 22:36:52 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 18 Feb 2009 15:36:52 -0600
Subject: [R-sig-ME] Testing a new version of lmer
Message-ID: <40e66e0b0902181336k622a70acqaed743dea7faa7d2@mail.gmail.com>

For some time I have been considering changing the parameterization
used by lmer for the covariance matrix of the random effects.  Today a
person contacted me regarding an lmer fit that gave very different
answers from SAS PROC MIXED.  It turns out that there were at least
two local optima for the REML criterion and PROC MIXED was picking up
the global maximum while lmer converged to a different local maximum.
The root cause was an group for which the response was very different
from the others and if you removed that group then lmer did fine.
Nevertheless I was chagrined to see lmer converge to a local optimum
and not a global optimum

This motivated me to make the change.  I have been thinking about it
for months and it ended up taking about half an hour.  I wish I would
have just done it when I thought of it.

The good news is that the modified code seems more reliable.  The bad
news is that I made the change in an experimental version of the code
which is in the "allcoef" branch of the SVN archive at R-forge.  (If
that sounds like gibberish to you, you can skip the rest of this
message.)

I think I will thread the change back into the released version of
lme4 but first I want to check that it is not harmful.  The testing
that I have done indicates that it does at least as well as the
current version and usually better.  I haven't uncovered any cases
where the current version converges but the modified version doesn't.
However, distributed testing is much more effective than my trying to
test different variations.

If you have a favorite linear mixed-effects example, preferably a
difficult fit, then I would appreciate your comparing the old method
and the new method.  Here is a trivial example using the Dyestuff data

Old:
> (fm1 <- lmer(Yield ~ 1|Batch, Dyestuff, verb = 1))
  0:     319.76562: 0.730297
  1:     319.73549: 0.962389
  2:     319.65735: 0.869461
  3:     319.65441: 0.844025
  4:     319.65428: 0.848469
  5:     319.65428: 0.848327
  6:     319.65428: 0.848324
Linear mixed model fit by REML
Formula: Yield ~ 1 | Batch
   Data: Dyestuff
   AIC   BIC logLik deviance REMLdev
 325.7 329.9 -159.8    327.4   319.7
Random effects:
 Groups   Name        Variance Std.Dev.
 Batch    (Intercept) 1764.0   42.001
 Residual             2451.3   49.510
Number of obs: 30, groups: Batch, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)  1527.50      19.38   78.81

New:
> (fm1 <- lmer(Yield ~ 1|Batch, Dyestuff, verb = 1))
  0:     319.79239:  1.00000
  1:     319.65467: 0.925112
  2:     319.65428: 0.920527
  3:     319.65428: 0.921046
  4:     319.65428: 0.921045
Linear mixed model fit by REML
Formula: Yield ~ 1 | Batch
   Data: Dyestuff
   AIC   BIC logLik deviance REMLdev
 325.7 329.9 -159.8    327.4   319.7
Random effects:
 Groups   Name        Variance Std.Dev.
 Batch    (Intercept) 1764.0   42.001
 Residual             2451.3   49.510
Number of obs: 30, groups: Batch, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)  1527.50      19.38    78.8

(To save you wondering what the change is, the new method uses the
square root of the relative standard deviation whereas the old method
used the relative standard deviation.  Also the starting values for
the new method are trivial.  It always starts at 1.)

You can help with examples in one of two ways.  If you are comfortable
installing a package from source then you can install the version of
lme4 from the allcoef branch and try both versions on the example as
above.  If you are not up to checking out code from an SVN archive and
installing a source package then please perform the fit in the old
version with verb = 1 and send me the output and the data so I can
check out the fit using the new lme4 and compare it to the old fit.

Thanks in advance for your help.



From bates at stat.wisc.edu  Wed Feb 18 23:43:42 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 18 Feb 2009 16:43:42 -0600
Subject: [R-sig-ME] No estimated scale value given
In-Reply-To: <499B1CC0.9030302@ufl.edu>
References: <4999740D.8090907@ufl.edu> <499B1CC0.9030302@ufl.edu>
Message-ID: <40e66e0b0902181443g5be7f520k94ec548558e8a15b@mail.gmail.com>

On Tue, Feb 17, 2009 at 2:23 PM, Ben Bolker <bolker at ufl.edu> wrote:
>  The not-necessarily-obvious thing is that none of the
> parameter estimates change when you go from poisson,
> binomial, etc. to their quasi- equivalents -- the variance
> on all points is inflated by the same amount, so the
> point estimates don't change.  So you can go ahead and
> use the quasi- variant to get the estimated scale.
>
>  I'm still struggling with exactly what "sigma" is in
> the quasi- case (as is Doug Bates) -- it seems
> NOT equal to the Pearson residuals^2/(resid df) ?
>
>  It would be worth exploring this some more ...
>
>  cheers
>   Ben Bolker
>
> set.seed(1001)
> ntot <- 1000
> x <- runif(ntot)
> f <- rep(1:20,each=50)
> reff <- rnorm(20,mean=0,sd=0.5)
> y <- rnbinom(ntot,mu=exp(2*x-1+reff[f]),size=1)
> library(lme4)
>
> m1 <- glmer(y~x+(1|f),family=poisson)
> m2 <- update(m1,family=quasipoisson)
>
> lme4:::sigma(m1) ## 1
> lme4:::sigma(m2) ## 1.04 (??)

The first one is 1 by definition.  The second is the penalized
weighted residual sum of squares at the parameter estimates divided by
the number of observations.  You did not include the penalty term in
your calculations.

As we have discussed, it is not clear if this is a meaningful quantity
in the quasi-Poisson case.

> ## residuals returned are Pearson residuals
> ##  (uncorrected for overdispersion)
>
> myres <- (y-fitted(m1))/sqrt(fitted(m1))
> all.equal(myres,residuals(m1))
> ## not quite identical, but nearly
> all(abs(myres-residuals(m1))<1e-4)
>
> all.equal(residuals(m1),residuals(m2)) ## TRUE
>
> sum(residuals(m1)^2)/ntot ## NOT sigmaML
> m1 at deviance["sigmaML"]
> m2 at deviance["sigmaML"]
>
>
> Amy Wade wrote:
>> Ben,
>>
>> When I fit a quasi- variant it gives what seems to be a sensible value for
>> the sigma. The trouble is I would like to know if my models are
>> overdispersed without the quasi-variant. I'm using data with poisson
>> distribution from which I know what the estimated scale is and it still
>> gives '1' when it should be 0.97. There was a discussion about this
>> previously and it was suggested to use 'lme4:::sigma(model)', that just
>> gives me the same result as using 'summary(model)@sigma'.
>>
>> Thanks very much for your help.
>>
>> Amy
>>
>> Should I have put this on the public thread? Not really sure how to do that!
>>
>>
>>
>> -----Original Message-----
>> From: Ben Bolker [mailto:bolker at ufl.edu]
>> Sent: 16 February 2009 14:11
>> To: Amy Wade
>> Subject: Re: [R-sig-ME] No estimated scale value given
>>
>>   What happens if you fit the same model with a quasi- variant and then try
>> to access sigma?
>>
>>
>>
>>   Ben Bolker
>>
>> Amy Wade wrote:
>>> Hello,
>>>
>>>
>>>
>>> I'm trying to run generalized linear mixed models using the lmer()
>>> function in the lme4 package. The problem is that the output does not
>>> give me a value for the Estimated scale. The rest of the output is as
>>> it should be. This makes it very difficult to assess whether the model
>>> is overdispersed. My colleague tried the same code on his computer and
>>> it did give an estimated scale value. I tried unistalling R and
>>> reinstalling the latest edition
>>> (2.8.1) with the latest lme4, but this made no difference. I also
>>> tried 'summary(model)@sigma' which people suggested on the CRAN forums
>>> to extract the scale parameter. This always gave me an answer of '1'
>>> even when I used data where I knew this was not the case. When I load
>>> up the lme4 library it does warn me about several objects being masked
>> from 'stats' and 'base'.
>>>
>>>
>>> Any idea why the output is not giving me the estimate scale? Or any
>>> idea how to extract it somehow?
>>>
>>>
>>>
>>> Many thanks,
>>>
>>> Amy
>>>
>>>
>>>
>>>
>>>
>>>
>>>      [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>> --
>> Ben Bolker
>> Associate professor, Biology Dep't, Univ. of Florida bolker at ufl.edu /
>> www.zoology.ufl.edu/bolker GPG key:
>> www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>>
>>
>
>
> --
> Ben Bolker
> Associate professor, Biology Dep't, Univ. of Florida
> bolker at ufl.edu / www.zoology.ufl.edu/bolker
> GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From David.Duffy at qimr.edu.au  Thu Feb 19 05:32:15 2009
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Thu, 19 Feb 2009 14:32:15 +1000 (EST)
Subject: [R-sig-ME] Desperately seeking help with simple lmer fit (lmer
 vs Proc Mixed)
In-Reply-To: <40e66e0b0901221440lcc6cc9cmedc91fa7714081ed@mail.gmail.com>
References: <3F094D0B-03BE-4DD1-A27A-192E563A8549@sheffield.ac.uk><2180B808CDA3404B8FF30C1EA0AC09EEB4D648@UFEXCH-MBXCL03.ad.ufl.edu>
	<40e66e0b0901221440lcc6cc9cmedc91fa7714081ed@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0902191423030.8907@orpheus.qimr.edu.au>

Of course the really annoying thing is that this particular problem runs 
fine in lme() ;)

David Duffy.

(PS. though not using other packages eg Karin Meyer's WOMBAT using PX-EM 
or AIREML)
-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From yfong at u.washington.edu  Thu Feb 19 05:01:23 2009
From: yfong at u.washington.edu (Youyi Fong)
Date: Wed, 18 Feb 2009 20:01:23 -0800
Subject: [R-sig-ME] observation level random effects;
	estimated variance of variance 	component estimates
Message-ID: <2019f8e20902182001w4a0780b3wdca40b6eae066be2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090218/f71a47a3/attachment.pl>

From steibelj at msu.edu  Thu Feb 19 14:38:41 2009
From: steibelj at msu.edu (Juan Pedro Steibel)
Date: Thu, 19 Feb 2009 08:38:41 -0500
Subject: [R-sig-ME] Random covariates with common variance
Message-ID: <499D60E1.3050907@msu.edu>

Hello everyone,
I can not find in the documentation how to fit the following model with 
lmer, any help would be much appreciated:
Suppose I have a set of three covariates z1, z2, z3 that I want to fit 
as random effects. Their coefficients U=(u1,u2, u3) have distribution: 
U~N(0,sig.sq_u*I)
Where sig.sq_u is a scalar, I is the Identity matrix of order 3.
I can fit a model with an unstructured covariance matrix on U, and with 
independent heteroskedastic distributions on the components of U, but 
cannot find a way of fitting what I need.

Any pointers/suggestion/comments?
Thanks!
JP


-- 
=============================
Juan Pedro Steibel

Assistant Professor
Statistical Genetics and Genomics

Department of Animal Science & 
Department of Fisheries and Wildlife

Michigan State University
1205-I Anthony Hall
East Lansing, MI
48824 USA 

Phone: 1-517-353-5102
E-mail: steibelj at msu.edu



From Thierry.ONKELINX at inbo.be  Thu Feb 19 15:04:58 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Thu, 19 Feb 2009 15:04:58 +0100
Subject: [R-sig-ME] Random covariates with common variance
In-Reply-To: <499D60E1.3050907@msu.edu>
References: <499D60E1.3050907@msu.edu>
Message-ID: <2E9C414912813E4EB981326983E0A104061B3C3D@inboexch.inbo.be>

Dear Juan Pedro,

I don't know how to do that in lme4. It is possible with lme() from the
nlme-package. You would need something like random = pdIdent(form = ~ z1
+ z2 + z3|groupingvariable) 

HTH,

Thierry


------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of
data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] Namens Juan Pedro
Steibel
Verzonden: donderdag 19 februari 2009 14:39
Aan: R Models Mixed
Onderwerp: [R-sig-ME] Random covariates with common variance

Hello everyone,
I can not find in the documentation how to fit the following model with 
lmer, any help would be much appreciated:
Suppose I have a set of three covariates z1, z2, z3 that I want to fit 
as random effects. Their coefficients U=(u1,u2, u3) have distribution: 
U~N(0,sig.sq_u*I)
Where sig.sq_u is a scalar, I is the Identity matrix of order 3.
I can fit a model with an unstructured covariance matrix on U, and with 
independent heteroskedastic distributions on the components of U, but 
cannot find a way of fitting what I need.

Any pointers/suggestion/comments?
Thanks!
JP


-- 
=============================
Juan Pedro Steibel

Assistant Professor
Statistical Genetics and Genomics

Department of Animal Science & 
Department of Fisheries and Wildlife

Michigan State University
1205-I Anthony Hall
East Lansing, MI
48824 USA 

Phone: 1-517-353-5102
E-mail: steibelj at msu.edu

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From bolker at ufl.edu  Thu Feb 19 23:24:00 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 19 Feb 2009 17:24:00 -0500
Subject: [R-sig-ME] observation level random effects;
 estimated variance of variance 	component estimates
In-Reply-To: <2019f8e20902182001w4a0780b3wdca40b6eae066be2@mail.gmail.com>
References: <2019f8e20902182001w4a0780b3wdca40b6eae066be2@mail.gmail.com>
Message-ID: <499DDC00.5070300@ufl.edu>


  I have a hacked version of lme4 that comments out the
error you are hitting (in the C code), and gets a plausible fit (at
least the fixed effects look pretty similar to Breslow and
Clayton 1993) -- see below.

  Don't know about your second question --

============================
> fit
Generalized linear mixed model fit by the Laplace approximation
Formula: update(formula1, . ~ . + (1 | id) + (1 | rand))
   Data: dat
   AIC   BIC logLik deviance
 499.7 527.4 -241.9    483.7
Random effects:
 Groups Name        Variance Std.Dev.
 rand   (Intercept) 0.12747  0.35702
 id     (Intercept) 0.21097  0.45932
Number of obs: 236, groups: rand, 236; id, 59

Fixed effects:
              Estimate Std. Error z value Pr(>|z|)
(Intercept)   -1.41194    1.16349  -1.214   0.2249
Base           0.88034    0.12910   6.819 9.17e-12 ***
Trt           -0.94857    0.39521  -2.400   0.0164 *
I(Trt * Base)  0.34922    0.20027   1.744   0.0812 .
Age            0.49015    0.34162   1.435   0.1514
V4TRUE        -0.10312    0.08583  -1.201   0.2296
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
            (Intr) Base   Trt    I(T*B) Age
Base        -0.163
Trt          0.047  0.595
I(Trt*Base) -0.119 -0.653 -0.930
Age         -0.976 -0.038 -0.192  0.254
V4TRUE      -0.018 -0.003  0.002  0.000  0.001

> sessionInfo()
R version 2.8.1 (2008-12-22)
i486-pc-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=C;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] splines   stats     graphics  grDevices utils     datasets  methods
[8] base

other attached packages:
[1] glmmAK_1.2         mvtnorm_0.9-4      coda_0.13-4
smoothSurv_0.3-12
[5] survival_2.34-1    lme4_0.999375-28   Matrix_0.999375-20
lattice_0.17-20

loaded via a namespace (and not attached):
[1] grid_2.8.1    rjags_1.0.3-4 tools_2.8.1


Youyi Fong wrote:
> Dear lmers,
> 
> I have two questions regarding fitting GLMM using maximum likelihood method.
> The first one arises from trying repeat an analysis in the Breslow and
> Clayton 1993 JASA paper. Model 3 of the epileptic dataset has two random
> effects, one subject specific, and one observation specific. Thus if we
> count random effects, there are more parameters than observations. When I
> try to run the following code, I get an error saying: "Error in
> mer_finalize(ans) : q = 295 > n = 236".
> 
> require (lme4)
> require (glmmAK)
> data(epilepticBC)
> dat = epilepticBC
> dat$rand=1:nrow(dat)
> dat$V4=dat$visit==4
> formula1 = Seizure ~ Base + Trt + I(Trt*Base) + Age + V4
> fit=lmer (update (formula1, .~. + (1|id) + (1|rand)), family=poisson,
> data=dat, nAGQ=1)
> 
> Is it true that there is no way to fit such a model in an ML analysis? In
> other words, is there a way to approximate the likelihood of fixed effects
> and variance components without relying on estimates of random effects?
> 
> The second question is that when it is possible to obtain MLE of a GLMM
> model, how can I obtain an estimated variance of the variance component
> estimates using lmer or other functions?
> 
> Thank you very much for your help!
> 
> Youyi Fong
> 
> -------------------------------------------------------------------------------------
> Youyi Fong, Graduate Student, Department of Biostatistics
> University of Washington, Box 357232, Seattle, WA 98195
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From j.hadfield at ed.ac.uk  Fri Feb 20 13:10:23 2009
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 20 Feb 2009 12:10:23 +0000
Subject: [R-sig-ME] observation level random effects;
	estimated 	variance of variance 	component estimates
Message-ID: <3BAB8A74-42EB-4560-B1FB-2D23C1A06DE1@ed.ac.uk>

Hi,

I'm not sure how to fit the model you want in lmer, but hopefully  
someone will have a better idea.  However, if its of interest or use  
I've released the package MCMCglmm that fits the model you're after.

  By default MCMCglmm fits rand as a residual component:

m1<-MCMCglmm(Seizure ~ Base + Trt*Base + Age + V4, random=~id,  
family="poisson", data=dat)

which I believe is the model you want.

HPDinterval(m1$VCV)

gives CI's on the variance components (the second of which is you rand  
term)

Hope this helps,

Jarrod


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.



From christos at nuverabio.com  Fri Feb 20 19:01:21 2009
From: christos at nuverabio.com (Christos Hatzis)
Date: Fri, 20 Feb 2009 13:01:21 -0500
Subject: [R-sig-ME] Power calculations for random effect models
Message-ID: <D4C82ADB2FA24A808781C8C4ACD50F73@headquarters.silicoinsights>

Hello,

I have a nested random effects model of the form

Yijk = M + Ai + Bj(i) + Ek(ij)

where biopsies (B) are nested within persons (A) and arrays (E) are nested
within biopsies.
I am interested in estimating the power of a study of a given size to
determine whether the variance associated with B is trivial, i.e. H0: var_b
= 0 vs Ha: var_b > 0 at a fixed Type-I error rate.

I have written a function to simulate data from this model and used lmer to
estimate the random effects as shown below.  What is the recommended way of
going about testing the null hypothesis?  One approach would be to use the
estimated standard deviation of the random effect and assuming normality to
test whether the appropriate CI contains zero. Another would be to use the
theoretical chi-square distribution for var_b, but that would require the
appropriate degrees of freedom.  Or to use mcmc to estimate the distribution
of var_b and use this distribution for inference.  

I would think that the mcmc approach is the recommended one, but I would
appreciate any advise on this. In this case, I have tried to run the MCMC
simulation (which runs fine), but have not been able yet to figure out how
to use the results of MCMC to test the above hypothesis.  Any hints or
pointing out materials that explain how to use MCMC for inference on random
effects will be very much appreciated.

Thank you.
-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com <http://www.nuverabio.com/>    

> fake.dt <- tumor.heter.dt(10, 3, 2, k=0, sa=4, sb=.6, se=.2)
> fake.dt
   person biopsy array      resp bioWper
1       1      1     1  4.308434     1:1
2       1      1     2  4.293186     1:1
3       1      2     1  5.503841     1:2
4       1      2     2  5.640362     1:2
5       1      3     1  5.579461     1:3
6       1      3     2  5.416201     1:3
7       2      1     1 12.479513     2:1
8       2      1     2 12.311426     2:1
9       2      2     1 13.283566     2:2
10      2      2     2 13.138276     2:2
11      2      3     1 12.954277     2:3
12      2      3     2 13.059925     2:3
13      3      1     1 11.649726     3:1
14      3      1     2 11.694472     3:1
15      3      2     1 12.342050     3:2
16      3      2     2 12.316214     3:2
17      3      3     1 12.762695     3:3
18      3      3     2 12.451338     3:3
19      4      1     1 17.168248     4:1
20      4      1     2 17.573315     4:1
21      4      2     1 16.885176     4:2
22      4      2     2 16.819536     4:2
23      4      3     1 15.924120     4:3
24      4      3     2 16.038491     4:3
25      5      1     1  7.981279     5:1
26      5      1     2  7.777929     5:1
27      5      2     1  6.701801     5:2
28      5      2     2  6.978284     5:2
29      5      3     1  7.136417     5:3
30      5      3     2  7.378498     5:3
31      6      1     1 21.499796     6:1
32      6      1     2 21.397173     6:1
33      6      2     1 22.551116     6:2
34      6      2     2 22.521006     6:2
35      6      3     1 21.027998     6:3
36      6      3     2 21.416872     6:3
37      7      1     1 13.312857     7:1
38      7      1     2 13.559062     7:1
39      7      2     1 13.753016     7:2
40      7      2     2 13.608642     7:2
41      7      3     1 13.556446     7:3
42      7      3     2 13.400672     7:3
43      8      1     1 12.758548     8:1
44      8      1     2 12.486574     8:1
45      8      2     1 13.388409     8:2
46      8      2     2 13.263029     8:2
47      8      3     1 12.991308     8:3
48      8      3     2 12.962116     8:3
49      9      1     1 11.420214     9:1
50      9      1     2 11.308010     9:1
51      9      2     1 13.186774     9:2
52      9      2     2 12.778966     9:2
53      9      3     1 11.625079     9:3
54      9      3     2 11.637015     9:3
55     10      1     1  9.108635    10:1
56     10      1     2  8.895658    10:1
57     10      2     1  9.718046    10:2
58     10      2     2  9.552510    10:2
59     10      3     1  8.794893    10:3
60     10      3     2  9.047379    10:3
> heter.lmer <- lmer(resp ~ (1 | person) + (1 | bioWper), fake.dt)
> heter.lmer
Linear mixed model fit by REML 
Formula: resp ~ (1 | person) + (1 | bioWper) 
   Data: fake.dt 
   AIC   BIC logLik deviance REMLdev
 98.24 106.6 -45.12    92.85   90.24
Random effects:
 Groups   Name        Variance  Std.Dev.
 bioWper  (Intercept)  0.312327 0.55886 
 person   (Intercept) 21.780993 4.66701 
 Residual              0.020633 0.14364 
Number of obs: 60, groups: bioWper, 30; person, 10
 
Fixed effects:
            Estimate Std. Error t value
(Intercept)   12.368      1.479    8.36


> VarCorr(heter.lmer)[["bioWper"]]
            (Intercept)
(Intercept)   0.3123273
attr(,"stddev")
(Intercept) 
  0.5588625 
attr(,"correlation")
            (Intercept)
(Intercept)           1

> heter.mcmc <- mcmcsamp(heter.lmer, n=1000)
> str(heter.mcmc)
Formal class 'merMCMC' [package "lme4"] with 9 slots
  ..@ Gp      : int [1:3] 0 30 40
  ..@ ST      : num [1:2, 1:1000] 3.89 32.49 1.44 27.06 1.24 ...
  ..@ call    : language lmer(formula = resp ~ (1 | person) + (1 | bioWper),
data = fake.dt)
  ..@ deviance: num [1:1000] 92.9 92.9 114.5 119.1 134 ...
  ..@ dims    : Named int [1:18] 2 60 1 40 1 2 0 1 2 5 ...
  .. ..- attr(*, "names")= chr [1:18] "nt" "n" "p" "q" ...
  ..@ fixef   : num [1, 1:1000] 12.4 14.1 11.9 12.6 12.6 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr "(Intercept)"
  .. .. ..$ : NULL
  ..@ nc      : int [1:2] 1 1
  ..@ ranef   : num[1:40, 0 ] 
  ..@ sigma   : num [1, 1:1000] 0.144 0.128 0.126 0.212 0.228 ...



From d.rizopoulos at erasmusmc.nl  Fri Feb 20 21:00:52 2009
From: d.rizopoulos at erasmusmc.nl (Dimitris Rizopoulos)
Date: Fri, 20 Feb 2009 21:00:52 +0100
Subject: [R-sig-ME] Power calculations for random effect models
In-Reply-To: <D4C82ADB2FA24A808781C8C4ACD50F73@headquarters.silicoinsights>
References: <D4C82ADB2FA24A808781C8C4ACD50F73@headquarters.silicoinsights>
Message-ID: <499F0BF4.8000109@erasmusmc.nl>

you could also have a look at the following recent articles:

Greven, S., Crainiceanu. C., Kuchenhoff, H. and Peters, A. (2008). 
Restricted Likelihood Ratio Testing for Zero Variance Components in 
Linear Mixed Models. Journal of Computational and Graphical Statistics 
17, 870-891.

Scheipl, F., Greven, S. and Kuchenhoff, H. (2008). Size and power of 
tests for a zero random effect variance or polynomial regression in 
additive and linear mixed models. Computational Statistics & Data 
Analysis 52, 3283-3299.

and the associated package: 
http://cran.r-project.org/web/packages/RLRsim/index.html


I hope it helps.

Best,
Dimitris


Christos Hatzis wrote:
> Hello,
> 
> I have a nested random effects model of the form
> 
> Yijk = M + Ai + Bj(i) + Ek(ij)
> 
> where biopsies (B) are nested within persons (A) and arrays (E) are nested
> within biopsies.
> I am interested in estimating the power of a study of a given size to
> determine whether the variance associated with B is trivial, i.e. H0: var_b
> = 0 vs Ha: var_b > 0 at a fixed Type-I error rate.
> 
> I have written a function to simulate data from this model and used lmer to
> estimate the random effects as shown below.  What is the recommended way of
> going about testing the null hypothesis?  One approach would be to use the
> estimated standard deviation of the random effect and assuming normality to
> test whether the appropriate CI contains zero. Another would be to use the
> theoretical chi-square distribution for var_b, but that would require the
> appropriate degrees of freedom.  Or to use mcmc to estimate the distribution
> of var_b and use this distribution for inference.  
> 
> I would think that the mcmc approach is the recommended one, but I would
> appreciate any advise on this. In this case, I have tried to run the MCMC
> simulation (which runs fine), but have not been able yet to figure out how
> to use the results of MCMC to test the above hypothesis.  Any hints or
> pointing out materials that explain how to use MCMC for inference on random
> effects will be very much appreciated.
> 
> Thank you.
> -Christos
> 
> Christos Hatzis, Ph.D.
> Nuvera Biosciences, Inc.
> 400 West Cummings Park
> Suite 5350
> Woburn, MA 01801
> Tel: 781-938-3830
> www.nuverabio.com <http://www.nuverabio.com/>    
> 
>> fake.dt <- tumor.heter.dt(10, 3, 2, k=0, sa=4, sb=.6, se=.2)
>> fake.dt
>    person biopsy array      resp bioWper
> 1       1      1     1  4.308434     1:1
> 2       1      1     2  4.293186     1:1
> 3       1      2     1  5.503841     1:2
> 4       1      2     2  5.640362     1:2
> 5       1      3     1  5.579461     1:3
> 6       1      3     2  5.416201     1:3
> 7       2      1     1 12.479513     2:1
> 8       2      1     2 12.311426     2:1
> 9       2      2     1 13.283566     2:2
> 10      2      2     2 13.138276     2:2
> 11      2      3     1 12.954277     2:3
> 12      2      3     2 13.059925     2:3
> 13      3      1     1 11.649726     3:1
> 14      3      1     2 11.694472     3:1
> 15      3      2     1 12.342050     3:2
> 16      3      2     2 12.316214     3:2
> 17      3      3     1 12.762695     3:3
> 18      3      3     2 12.451338     3:3
> 19      4      1     1 17.168248     4:1
> 20      4      1     2 17.573315     4:1
> 21      4      2     1 16.885176     4:2
> 22      4      2     2 16.819536     4:2
> 23      4      3     1 15.924120     4:3
> 24      4      3     2 16.038491     4:3
> 25      5      1     1  7.981279     5:1
> 26      5      1     2  7.777929     5:1
> 27      5      2     1  6.701801     5:2
> 28      5      2     2  6.978284     5:2
> 29      5      3     1  7.136417     5:3
> 30      5      3     2  7.378498     5:3
> 31      6      1     1 21.499796     6:1
> 32      6      1     2 21.397173     6:1
> 33      6      2     1 22.551116     6:2
> 34      6      2     2 22.521006     6:2
> 35      6      3     1 21.027998     6:3
> 36      6      3     2 21.416872     6:3
> 37      7      1     1 13.312857     7:1
> 38      7      1     2 13.559062     7:1
> 39      7      2     1 13.753016     7:2
> 40      7      2     2 13.608642     7:2
> 41      7      3     1 13.556446     7:3
> 42      7      3     2 13.400672     7:3
> 43      8      1     1 12.758548     8:1
> 44      8      1     2 12.486574     8:1
> 45      8      2     1 13.388409     8:2
> 46      8      2     2 13.263029     8:2
> 47      8      3     1 12.991308     8:3
> 48      8      3     2 12.962116     8:3
> 49      9      1     1 11.420214     9:1
> 50      9      1     2 11.308010     9:1
> 51      9      2     1 13.186774     9:2
> 52      9      2     2 12.778966     9:2
> 53      9      3     1 11.625079     9:3
> 54      9      3     2 11.637015     9:3
> 55     10      1     1  9.108635    10:1
> 56     10      1     2  8.895658    10:1
> 57     10      2     1  9.718046    10:2
> 58     10      2     2  9.552510    10:2
> 59     10      3     1  8.794893    10:3
> 60     10      3     2  9.047379    10:3
>> heter.lmer <- lmer(resp ~ (1 | person) + (1 | bioWper), fake.dt)
>> heter.lmer
> Linear mixed model fit by REML 
> Formula: resp ~ (1 | person) + (1 | bioWper) 
>    Data: fake.dt 
>    AIC   BIC logLik deviance REMLdev
>  98.24 106.6 -45.12    92.85   90.24
> Random effects:
>  Groups   Name        Variance  Std.Dev.
>  bioWper  (Intercept)  0.312327 0.55886 
>  person   (Intercept) 21.780993 4.66701 
>  Residual              0.020633 0.14364 
> Number of obs: 60, groups: bioWper, 30; person, 10
>  
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)   12.368      1.479    8.36
> 
> 
>> VarCorr(heter.lmer)[["bioWper"]]
>             (Intercept)
> (Intercept)   0.3123273
> attr(,"stddev")
> (Intercept) 
>   0.5588625 
> attr(,"correlation")
>             (Intercept)
> (Intercept)           1
> 
>> heter.mcmc <- mcmcsamp(heter.lmer, n=1000)
>> str(heter.mcmc)
> Formal class 'merMCMC' [package "lme4"] with 9 slots
>   ..@ Gp      : int [1:3] 0 30 40
>   ..@ ST      : num [1:2, 1:1000] 3.89 32.49 1.44 27.06 1.24 ...
>   ..@ call    : language lmer(formula = resp ~ (1 | person) + (1 | bioWper),
> data = fake.dt)
>   ..@ deviance: num [1:1000] 92.9 92.9 114.5 119.1 134 ...
>   ..@ dims    : Named int [1:18] 2 60 1 40 1 2 0 1 2 5 ...
>   .. ..- attr(*, "names")= chr [1:18] "nt" "n" "p" "q" ...
>   ..@ fixef   : num [1, 1:1000] 12.4 14.1 11.9 12.6 12.6 ...
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : chr "(Intercept)"
>   .. .. ..$ : NULL
>   ..@ nc      : int [1:2] 1 1
>   ..@ ranef   : num[1:40, 0 ] 
>   ..@ sigma   : num [1, 1:1000] 0.144 0.128 0.126 0.212 0.228 ...
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

-- 
Dimitris Rizopoulos
Assistant Professor
Department of Biostatistics
Erasmus University Medical Center

Address: PO Box 2040, 3000 CA Rotterdam, the Netherlands
Tel: +31/(0)10/7043478
Fax: +31/(0)10/7043014



From christos.hatzis at nuverabio.com  Fri Feb 20 21:20:40 2009
From: christos.hatzis at nuverabio.com (Christos Hatzis)
Date: Fri, 20 Feb 2009 15:20:40 -0500
Subject: [R-sig-ME] Power calculations for random effect models
In-Reply-To: <499F0BF4.8000109@erasmusmc.nl>
References: <D4C82ADB2FA24A808781C8C4ACD50F73@headquarters.silicoinsights>
	<499F0BF4.8000109@erasmusmc.nl>
Message-ID: <6A3B2739C262479B9B740560F5B7FB7B@headquarters.silicoinsights>

Thanks, Dimitri.  I'll take a look a these references.

I am still hoping to figure out how to use the results of MCMC since it
seems to be a relevant tool and is built in lme4.

Thanks again.
-Christos 

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Dimitris Rizopoulos
> Sent: Friday, February 20, 2009 3:01 PM
> To: christos at nuverabio.com
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Power calculations for random effect models
> 
> you could also have a look at the following recent articles:
> 
> Greven, S., Crainiceanu. C., Kuchenhoff, H. and Peters, A. (2008). 
> Restricted Likelihood Ratio Testing for Zero Variance 
> Components in Linear Mixed Models. Journal of Computational 
> and Graphical Statistics 17, 870-891.
> 
> Scheipl, F., Greven, S. and Kuchenhoff, H. (2008). Size and 
> power of tests for a zero random effect variance or 
> polynomial regression in additive and linear mixed models. 
> Computational Statistics & Data Analysis 52, 3283-3299.
> 
> and the associated package: 
> http://cran.r-project.org/web/packages/RLRsim/index.html
> 
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> 
> Christos Hatzis wrote:
> > Hello,
> > 
> > I have a nested random effects model of the form
> > 
> > Yijk = M + Ai + Bj(i) + Ek(ij)
> > 
> > where biopsies (B) are nested within persons (A) and arrays (E) are 
> > nested within biopsies.
> > I am interested in estimating the power of a study of a 
> given size to 
> > determine whether the variance associated with B is 
> trivial, i.e. H0: 
> > var_b = 0 vs Ha: var_b > 0 at a fixed Type-I error rate.
> > 
> > I have written a function to simulate data from this model and used 
> > lmer to estimate the random effects as shown below.  What is the 
> > recommended way of going about testing the null hypothesis?  One 
> > approach would be to use the estimated standard deviation of the 
> > random effect and assuming normality to test whether the 
> appropriate 
> > CI contains zero. Another would be to use the theoretical 
> chi-square 
> > distribution for var_b, but that would require the 
> appropriate degrees 
> > of freedom.  Or to use mcmc to estimate the distribution of 
> var_b and use this distribution for inference.
> > 
> > I would think that the mcmc approach is the recommended one, but I 
> > would appreciate any advise on this. In this case, I have 
> tried to run 
> > the MCMC simulation (which runs fine), but have not been 
> able yet to 
> > figure out how to use the results of MCMC to test the above 
> > hypothesis.  Any hints or pointing out materials that 
> explain how to 
> > use MCMC for inference on random effects will be very much 
> appreciated.
> > 
> > Thank you.
> > -Christos
> > 
> > Christos Hatzis, Ph.D.
> > Nuvera Biosciences, Inc.
> > 400 West Cummings Park
> > Suite 5350
> > Woburn, MA 01801
> > Tel: 781-938-3830
> > www.nuverabio.com <http://www.nuverabio.com/>    
> > 
> >> fake.dt <- tumor.heter.dt(10, 3, 2, k=0, sa=4, sb=.6, 
> se=.2) fake.dt
> >    person biopsy array      resp bioWper
> > 1       1      1     1  4.308434     1:1
> > 2       1      1     2  4.293186     1:1
> > 3       1      2     1  5.503841     1:2
> > 4       1      2     2  5.640362     1:2
> > 5       1      3     1  5.579461     1:3
> > 6       1      3     2  5.416201     1:3
> > 7       2      1     1 12.479513     2:1
> > 8       2      1     2 12.311426     2:1
> > 9       2      2     1 13.283566     2:2
> > 10      2      2     2 13.138276     2:2
> > 11      2      3     1 12.954277     2:3
> > 12      2      3     2 13.059925     2:3
> > 13      3      1     1 11.649726     3:1
> > 14      3      1     2 11.694472     3:1
> > 15      3      2     1 12.342050     3:2
> > 16      3      2     2 12.316214     3:2
> > 17      3      3     1 12.762695     3:3
> > 18      3      3     2 12.451338     3:3
> > 19      4      1     1 17.168248     4:1
> > 20      4      1     2 17.573315     4:1
> > 21      4      2     1 16.885176     4:2
> > 22      4      2     2 16.819536     4:2
> > 23      4      3     1 15.924120     4:3
> > 24      4      3     2 16.038491     4:3
> > 25      5      1     1  7.981279     5:1
> > 26      5      1     2  7.777929     5:1
> > 27      5      2     1  6.701801     5:2
> > 28      5      2     2  6.978284     5:2
> > 29      5      3     1  7.136417     5:3
> > 30      5      3     2  7.378498     5:3
> > 31      6      1     1 21.499796     6:1
> > 32      6      1     2 21.397173     6:1
> > 33      6      2     1 22.551116     6:2
> > 34      6      2     2 22.521006     6:2
> > 35      6      3     1 21.027998     6:3
> > 36      6      3     2 21.416872     6:3
> > 37      7      1     1 13.312857     7:1
> > 38      7      1     2 13.559062     7:1
> > 39      7      2     1 13.753016     7:2
> > 40      7      2     2 13.608642     7:2
> > 41      7      3     1 13.556446     7:3
> > 42      7      3     2 13.400672     7:3
> > 43      8      1     1 12.758548     8:1
> > 44      8      1     2 12.486574     8:1
> > 45      8      2     1 13.388409     8:2
> > 46      8      2     2 13.263029     8:2
> > 47      8      3     1 12.991308     8:3
> > 48      8      3     2 12.962116     8:3
> > 49      9      1     1 11.420214     9:1
> > 50      9      1     2 11.308010     9:1
> > 51      9      2     1 13.186774     9:2
> > 52      9      2     2 12.778966     9:2
> > 53      9      3     1 11.625079     9:3
> > 54      9      3     2 11.637015     9:3
> > 55     10      1     1  9.108635    10:1
> > 56     10      1     2  8.895658    10:1
> > 57     10      2     1  9.718046    10:2
> > 58     10      2     2  9.552510    10:2
> > 59     10      3     1  8.794893    10:3
> > 60     10      3     2  9.047379    10:3
> >> heter.lmer <- lmer(resp ~ (1 | person) + (1 | bioWper), fake.dt) 
> >> heter.lmer
> > Linear mixed model fit by REML
> > Formula: resp ~ (1 | person) + (1 | bioWper) 
> >    Data: fake.dt 
> >    AIC   BIC logLik deviance REMLdev
> >  98.24 106.6 -45.12    92.85   90.24
> > Random effects:
> >  Groups   Name        Variance  Std.Dev.
> >  bioWper  (Intercept)  0.312327 0.55886 
> >  person   (Intercept) 21.780993 4.66701 
> >  Residual              0.020633 0.14364 
> > Number of obs: 60, groups: bioWper, 30; person, 10
> >  
> > Fixed effects:
> >             Estimate Std. Error t value
> > (Intercept)   12.368      1.479    8.36
> > 
> > 
> >> VarCorr(heter.lmer)[["bioWper"]]
> >             (Intercept)
> > (Intercept)   0.3123273
> > attr(,"stddev")
> > (Intercept) 
> >   0.5588625
> > attr(,"correlation")
> >             (Intercept)
> > (Intercept)           1
> > 
> >> heter.mcmc <- mcmcsamp(heter.lmer, n=1000)
> >> str(heter.mcmc)
> > Formal class 'merMCMC' [package "lme4"] with 9 slots
> >   ..@ Gp      : int [1:3] 0 30 40
> >   ..@ ST      : num [1:2, 1:1000] 3.89 32.49 1.44 27.06 1.24 ...
> >   ..@ call    : language lmer(formula = resp ~ (1 | person) 
> + (1 | bioWper),
> > data = fake.dt)
> >   ..@ deviance: num [1:1000] 92.9 92.9 114.5 119.1 134 ...
> >   ..@ dims    : Named int [1:18] 2 60 1 40 1 2 0 1 2 5 ...
> >   .. ..- attr(*, "names")= chr [1:18] "nt" "n" "p" "q" ...
> >   ..@ fixef   : num [1, 1:1000] 12.4 14.1 11.9 12.6 12.6 ...
> >   .. ..- attr(*, "dimnames")=List of 2
> >   .. .. ..$ : chr "(Intercept)"
> >   .. .. ..$ : NULL
> >   ..@ nc      : int [1:2] 1 1
> >   ..@ ranef   : num[1:40, 0 ] 
> >   ..@ sigma   : num [1, 1:1000] 0.144 0.128 0.126 0.212 0.228 ...
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > 
> 
> --
> Dimitris Rizopoulos
> Assistant Professor
> Department of Biostatistics
> Erasmus University Medical Center
> 
> Address: PO Box 2040, 3000 CA Rotterdam, the Netherlands
> Tel: +31/(0)10/7043478
> Fax: +31/(0)10/7043014
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
>



From bates at stat.wisc.edu  Sat Feb 21 00:01:28 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 20 Feb 2009 17:01:28 -0600
Subject: [R-sig-ME] Desperately seeking help with simple lmer fit (lmer
	vs Proc Mixed)
In-Reply-To: <40e66e0b0902181220l61df2c59v8088b7dc82beac17@mail.gmail.com>
References: <40e66e0b0810071531q425c5520o1ff8c4fe15699297@mail.gmail.com>
	<OF349028FC.ED6163B9-ON86257561.0062FFF6-86257561.0066B1BE@abbott.com>
	<40e66e0b0902181156m62b891a7m17017717f216a6eb@mail.gmail.com>
	<40e66e0b0902181220l61df2c59v8088b7dc82beac17@mail.gmail.com>
Message-ID: <40e66e0b0902201501m45e5c344ud98c9653fe8c3bde@mail.gmail.com>

On Wed, Feb 18, 2009 at 2:20 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Wed, Feb 18, 2009 at 1:56 PM, Douglas Bates <bates at stat.wisc.edu> wrote:
>> Thanks for the example.
>>
>> The estimates of the variance components are the result of an
>> optimization and it appears that there are two local optima for this
>> problem.  I'm sorry to say that the optimum determined by SAS PROC
>> MIXED represents a better fit (a lower deviance) than the one
>> determined by lmer.
>>
>> If you plot the response versus run, say
>>
>> dotplot(reorder(Run, Y) ~ Y)
>>
>> you will see that there is one run (number 13) with unusually large Y,
>> which may have abnormal influence.  I was able to reproduce the SAS
>> results by modifying the starting estimates.  This is not optimal
>> because it means you need to know the correct answer before you can
>> calculate it.
>>
>>> (fm1 <- lmer(Y ~ (1|Run) + (1|Prep), pr, verbose = TRUE))
>>  0:     1536.6629: 0.666667 0.384900
>>  1:     1441.4487:  1.44044  1.01837
>>  2:     1421.4662:  2.35838 0.621655
>>  3:     1408.1762:  3.27848 0.229969
>>  4:     1401.9874:  4.04888  0.00000
>>  5:     1399.8678:  4.59188  0.00000
>>  6:     1398.9477:  5.12145  0.00000
>>  7:     1398.7722:  5.41633  0.00000
>>  8:     1398.7543:  5.53332  0.00000
>>  9:     1398.7539:  5.55421  0.00000
>>  10:     1398.7539:  5.55527 1.68436e-07
>>  11:     1398.7539:  5.55527 1.68436e-07
>> Linear mixed model fit by REML
>> Formula: Y ~ (1 | Run) + (1 | Prep)
>>   Data: pr
>>  AIC  BIC logLik deviance REMLdev
>>  1407 1417 -699.4     1410    1399
>> Random effects:
>>  Groups   Name        Variance   Std.Dev.
>>  Run      (Intercept) 3.5859e+05 5.9882e+02
>>  Prep     (Intercept) 3.2965e-10 1.8156e-05
>>  Residual             1.1619e+04 1.0779e+02
>> Number of obs: 108, groups: Run, 18; Prep, 6
>>
>> Fixed effects:
>>            Estimate Std. Error t value
>> (Intercept)  28216.5      141.5   199.4
>>> (fm1a <- lmer(Y ~ (1|Run) + (1|Prep), pr, verbose = TRUE, start = c(4,4)))
>>  0:     1393.6144:  4.00000  4.00000
>>  1:     1393.5078:  3.74366  4.09884
>>  2:     1393.4819:  3.72800  4.32664
>>  3:     1393.4810:  3.73535  4.37362
>>  4:     1393.4810:  3.73373  4.37885
>>  5:     1393.4810:  3.73404  4.37877
>>  6:     1393.4810:  3.73404  4.37875
>>  7:     1393.4810:  3.73404  4.37875
>> Linear mixed model fit by REML
>> Formula: Y ~ (1 | Run) + (1 | Prep)
>>   Data: pr
>>  AIC  BIC logLik deviance REMLdev
>>  1401 1412 -696.7     1406    1393
>> Random effects:
>>  Groups   Name        Variance Std.Dev.
>>  Run      (Intercept) 162010   402.50
>>  Prep     (Intercept) 222784   472.00
>>  Residual              11619   107.79
>> Number of obs: 108, groups: Run, 18; Prep, 6
>>
>> Fixed effects:
>>            Estimate Std. Error t value
>> (Intercept)    28216        215   131.2
>>
>> If you remove Run 13 you do get results like those from SAS using lmer
>>
>>> (fm2 <- lmer(Y ~ (1|Run) + (1|Prep), pr, verbose = TRUE, subset = Run != 13))
>>  0:     1426.8110: 0.666667 0.396059
>>  1:     1342.0591:  1.47666 0.982502
>>  2:     1325.5670:  2.35364 0.501989
>>  3:     1311.1286:  3.60322  0.00000
>>  4:     1309.3047:  3.96698  0.00000
>>  5:     1307.8040:  4.56388 1.65053e-05
>>  6:     1307.5200:  4.88366 6.39529e-05
>>  7:     1307.4763:  5.04660 0.000170199
>>  8:     1307.4746:  5.08457 0.000341660
>>  9:     1307.4745:  5.08786 0.000627531
>>  10:     1307.4745:  5.08806 0.00113168
>>  11:     1307.4743:  5.09250 0.0198356
>>  12:     1306.7915:  5.55085  2.00076
>>  13:     1306.7202:  5.57778  2.29410
>>  14:     1304.5962:  3.26016  2.72067
>>  15:     1304.1219:  3.90630  4.98691
>>  16:     1303.9334:  3.70822  4.85924
>>  17:     1303.7839:  3.39994  4.44476
>>  18:     1303.7039:  3.78966  4.10571
>>  19:     1303.6539:  3.50693  3.67340
>>  20:     1303.6333:  3.63992  3.74762
>>  21:     1303.6285:  3.55042  3.87085
>>  22:     1303.6259:  3.59760  3.87143
>>  23:     1303.6258:  3.59381  3.86414
>>  24:     1303.6258:  3.59257  3.86456
>>  25:     1303.6258:  3.59308  3.86577
>>  26:     1303.6258:  3.59318  3.86504
>>  27:     1303.6258:  3.59312  3.86514
>> Linear mixed model fit by REML
>> Formula: Y ~ (1 | Run) + (1 | Prep)
>>   Data: pr
>>  Subset: Run != 13
>>  AIC  BIC logLik deviance REMLdev
>>  1312 1322 -651.8     1316    1304
>> Random effects:
>>  Groups   Name        Variance Std.Dev.
>>  Run      (Intercept) 135857   368.59
>>  Prep     (Intercept) 157206   396.49
>>  Residual              10523   102.58
>> Number of obs: 102, groups: Run, 17; Prep, 6
>>
>> Fixed effects:
>>            Estimate Std. Error t value
>> (Intercept)  28161.8      185.5   151.8
>>
>>
>> This example may be motivation for me to try out a modification in the
>> optimization method that I have been contemplating.
>
> Well, the good news is that the modification was successful on this example
>
>> (fm1 <- lmer(Y ~ 1 + (1|Run) + (1|Prep), pr, verbose = TRUE))
>  0:     1536.6629: 0.816497 0.620403
>  1:     1405.8868:  1.66562  1.14860
>  2:     1399.0326:  1.71590  1.44355
>  3:     1394.7782:  1.84561  1.71319
>  4:     1393.9976:  1.79780  2.00855
>  5:     1393.9690:  2.09290  2.05797
>  6:     1393.5053:  1.96491  2.07162
>  7:     1393.4829:  1.92326  2.08972
>  8:     1393.4810:  1.93265  2.09030
>  9:     1393.4810:  1.93240  2.09189
>  10:     1393.4810:  1.93236  2.09254
>  11:     1393.4810:  1.93237  2.09255
> Linear mixed model fit by REML
> Formula: Y ~ 1 + (1 | Run) + (1 | Prep)
>   Data: pr
>  AIC  BIC logLik deviance REMLdev
>  1401 1412 -696.7     1406    1393
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  Run      (Intercept) 162010   402.50
>  Prep     (Intercept) 222784   472.00
>  Residual              11619   107.79
> Number of obs: 108, groups: Run, 18; Prep, 6
>
> Fixed effects:
>            Estimate Std. Error t value
> (Intercept)    28216        215   131.2
>
> The bad news is that I made the modification in a development version
> of the lme4 package and that version won't be ready for prime time for
> a while.  I still have a lot of work to do on the generalized linear
> mixed models.

I usually want to see a picture to help me understand what is going on
with a model so I created a contour plot of the profiled deviance as a
function of the two parameters in the new version of the optimization.
 These parameters represent the square roots of the standard
deviations of the two sets of random effects relative to the residual
standard deviation.  The conditional estimates of all the other
parameters, given these two, can be evaluated directly.  The contours
are the joint confidence regions for this pair of parameters.  You can
see that even on this square root scale the contours are somewhat
elongated to the right, and the value of 0 for the second variance
component (the prep component) is within confidence regions at about
85%.  This is an indication that the variance component for Prep is
ill-determined.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: contours.pdf
Type: application/pdf
Size: 18787 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090220/68619abe/attachment.pdf>

From Greg.Snow at imail.org  Sat Feb 21 02:12:52 2009
From: Greg.Snow at imail.org (Greg Snow)
Date: Fri, 20 Feb 2009 18:12:52 -0700
Subject: [R-sig-ME] Power calculations for random effect models
In-Reply-To: <D4C82ADB2FA24A808781C8C4ACD50F73@headquarters.silicoinsights>
References: <D4C82ADB2FA24A808781C8C4ACD50F73@headquarters.silicoinsights>
Message-ID: <B37C0A15B8FB3C468B5BC7EBC7DA14CC61CA3E2EBE@LP-EXMBVS10.CO.IHC.COM>

Since you already have a function to simulate the data, it should be fairly simple to use simulations to compare the methods you suggest below, or to do a simple test based on the likelihood ratio, but simulating the cutoff rather than depending on the chi-squared approximation.

See:
http://finzi.psych.upenn.edu/R/Rhelp08/archive/156522.html

for some examples of assessing these types of questions using simulation (I have an updated version of the last example if you want it as well).

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
801.408.8111


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-bounces at r-project.org] On Behalf Of Christos Hatzis
> Sent: Friday, February 20, 2009 11:01 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Power calculations for random effect models
> 
> Hello,
> 
> I have a nested random effects model of the form
> 
> Yijk = M + Ai + Bj(i) + Ek(ij)
> 
> where biopsies (B) are nested within persons (A) and arrays (E) are
> nested
> within biopsies.
> I am interested in estimating the power of a study of a given size to
> determine whether the variance associated with B is trivial, i.e. H0:
> var_b
> = 0 vs Ha: var_b > 0 at a fixed Type-I error rate.
> 
> I have written a function to simulate data from this model and used
> lmer to
> estimate the random effects as shown below.  What is the recommended
> way of
> going about testing the null hypothesis?  One approach would be to use
> the
> estimated standard deviation of the random effect and assuming
> normality to
> test whether the appropriate CI contains zero. Another would be to use
> the
> theoretical chi-square distribution for var_b, but that would require
> the
> appropriate degrees of freedom.  Or to use mcmc to estimate the
> distribution
> of var_b and use this distribution for inference.
> 
> I would think that the mcmc approach is the recommended one, but I
> would
> appreciate any advise on this. In this case, I have tried to run the
> MCMC
> simulation (which runs fine), but have not been able yet to figure out
> how
> to use the results of MCMC to test the above hypothesis.  Any hints or
> pointing out materials that explain how to use MCMC for inference on
> random
> effects will be very much appreciated.
> 
> Thank you.
> -Christos
> 
> Christos Hatzis, Ph.D.
> Nuvera Biosciences, Inc.
> 400 West Cummings Park
> Suite 5350
> Woburn, MA 01801
> Tel: 781-938-3830
> www.nuverabio.com <http://www.nuverabio.com/>
> 
> > fake.dt <- tumor.heter.dt(10, 3, 2, k=0, sa=4, sb=.6, se=.2)
> > fake.dt
>    person biopsy array      resp bioWper
> 1       1      1     1  4.308434     1:1
> 2       1      1     2  4.293186     1:1
> 3       1      2     1  5.503841     1:2
> 4       1      2     2  5.640362     1:2
> 5       1      3     1  5.579461     1:3
> 6       1      3     2  5.416201     1:3
> 7       2      1     1 12.479513     2:1
> 8       2      1     2 12.311426     2:1
> 9       2      2     1 13.283566     2:2
> 10      2      2     2 13.138276     2:2
> 11      2      3     1 12.954277     2:3
> 12      2      3     2 13.059925     2:3
> 13      3      1     1 11.649726     3:1
> 14      3      1     2 11.694472     3:1
> 15      3      2     1 12.342050     3:2
> 16      3      2     2 12.316214     3:2
> 17      3      3     1 12.762695     3:3
> 18      3      3     2 12.451338     3:3
> 19      4      1     1 17.168248     4:1
> 20      4      1     2 17.573315     4:1
> 21      4      2     1 16.885176     4:2
> 22      4      2     2 16.819536     4:2
> 23      4      3     1 15.924120     4:3
> 24      4      3     2 16.038491     4:3
> 25      5      1     1  7.981279     5:1
> 26      5      1     2  7.777929     5:1
> 27      5      2     1  6.701801     5:2
> 28      5      2     2  6.978284     5:2
> 29      5      3     1  7.136417     5:3
> 30      5      3     2  7.378498     5:3
> 31      6      1     1 21.499796     6:1
> 32      6      1     2 21.397173     6:1
> 33      6      2     1 22.551116     6:2
> 34      6      2     2 22.521006     6:2
> 35      6      3     1 21.027998     6:3
> 36      6      3     2 21.416872     6:3
> 37      7      1     1 13.312857     7:1
> 38      7      1     2 13.559062     7:1
> 39      7      2     1 13.753016     7:2
> 40      7      2     2 13.608642     7:2
> 41      7      3     1 13.556446     7:3
> 42      7      3     2 13.400672     7:3
> 43      8      1     1 12.758548     8:1
> 44      8      1     2 12.486574     8:1
> 45      8      2     1 13.388409     8:2
> 46      8      2     2 13.263029     8:2
> 47      8      3     1 12.991308     8:3
> 48      8      3     2 12.962116     8:3
> 49      9      1     1 11.420214     9:1
> 50      9      1     2 11.308010     9:1
> 51      9      2     1 13.186774     9:2
> 52      9      2     2 12.778966     9:2
> 53      9      3     1 11.625079     9:3
> 54      9      3     2 11.637015     9:3
> 55     10      1     1  9.108635    10:1
> 56     10      1     2  8.895658    10:1
> 57     10      2     1  9.718046    10:2
> 58     10      2     2  9.552510    10:2
> 59     10      3     1  8.794893    10:3
> 60     10      3     2  9.047379    10:3
> > heter.lmer <- lmer(resp ~ (1 | person) + (1 | bioWper), fake.dt)
> > heter.lmer
> Linear mixed model fit by REML
> Formula: resp ~ (1 | person) + (1 | bioWper)
>    Data: fake.dt
>    AIC   BIC logLik deviance REMLdev
>  98.24 106.6 -45.12    92.85   90.24
> Random effects:
>  Groups   Name        Variance  Std.Dev.
>  bioWper  (Intercept)  0.312327 0.55886
>  person   (Intercept) 21.780993 4.66701
>  Residual              0.020633 0.14364
> Number of obs: 60, groups: bioWper, 30; person, 10
> 
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)   12.368      1.479    8.36
> 
> 
> > VarCorr(heter.lmer)[["bioWper"]]
>             (Intercept)
> (Intercept)   0.3123273
> attr(,"stddev")
> (Intercept)
>   0.5588625
> attr(,"correlation")
>             (Intercept)
> (Intercept)           1
> 
> > heter.mcmc <- mcmcsamp(heter.lmer, n=1000)
> > str(heter.mcmc)
> Formal class 'merMCMC' [package "lme4"] with 9 slots
>   ..@ Gp      : int [1:3] 0 30 40
>   ..@ ST      : num [1:2, 1:1000] 3.89 32.49 1.44 27.06 1.24 ...
>   ..@ call    : language lmer(formula = resp ~ (1 | person) + (1 |
> bioWper),
> data = fake.dt)
>   ..@ deviance: num [1:1000] 92.9 92.9 114.5 119.1 134 ...
>   ..@ dims    : Named int [1:18] 2 60 1 40 1 2 0 1 2 5 ...
>   .. ..- attr(*, "names")= chr [1:18] "nt" "n" "p" "q" ...
>   ..@ fixef   : num [1, 1:1000] 12.4 14.1 11.9 12.6 12.6 ...
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : chr "(Intercept)"
>   .. .. ..$ : NULL
>   ..@ nc      : int [1:2] 1 1
>   ..@ ranef   : num[1:40, 0 ]
>   ..@ sigma   : num [1, 1:1000] 0.144 0.128 0.126 0.212 0.228 ...
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From lamprianou at yahoo.com  Sat Feb 21 12:31:44 2009
From: lamprianou at yahoo.com (Iasonas Lamprianou)
Date: Sat, 21 Feb 2009 03:31:44 -0800 (PST)
Subject: [R-sig-ME] Rasch model
In-Reply-To: <mailman.3.1235214002.26243.r-sig-mixed-models@r-project.org>
Message-ID: <155350.42290.qm@web54106.mail.re2.yahoo.com>

Dear friends, 
I am running a simple model where 424 pupils took 13 test items. I model the item easiness as fixed effects and the pupil abilities as random effects (this is basically the 'so-called' marginal maximum likelihood estimation used in some Rasch software packages. I also run the analysis using two 'traditional' Rasch packages. I have found that there is a near-perfect correlation between item estimates from the 'traditional' packages and lme4. See the results below. However, when I use ranef(model$id) to  get the 'ability' estimates of the pupils, the correlation is just 0.9! Shouldnt the correlation be much bigger? I mean, how would you estimate the ability estimates of the pupils in this context?

Thanks for any help

Generalized linear mixed model fit by the Laplace approximation 
Formula: score ~ 0 + item + (1 | id) 
   Data: rasch_data 
  AIC  BIC logLik deviance
 6502 6595  -3237     6474
Random effects:
 Groups Name        Variance Std.Dev.
 id     (Intercept) 1.9821   1.4079  
Number of obs: 5512, groups: id, 424

Fixed effects:
         Estimate Std. Error z value Pr(>|z|)    
item   1  0.05819    0.13004   0.447 0.654524    
item   2  1.26791    0.14126   8.976  < 2e-16 ***
item   3  0.93972    0.13615   6.902 5.12e-12 ***
item   4  0.70219    0.13345   5.262 1.43e-07 ***
item   5  0.36877    0.13099   2.815 0.004874 ** 
item   6  0.52699    0.13197   3.993 6.52e-05 ***
item   7 -0.79568    0.13404  -5.936 2.92e-09 ***
item   8  0.38186    0.13106   2.914 0.003572 ** 
item   9 -0.61867    0.13239  -4.673 2.97e-06 ***
item  10  0.30358    0.13069   2.323 0.020184 *  
item  11  0.23870    0.13044   1.830 0.067262 .  
item  12 -0.79568    0.13404  -5.936 2.92e-09 ***
item  13 -0.47278    0.13137  -3.599 0.000320 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Correlation of Fixed Effects:
         item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 item11 item12
item   2 0.258                                                                     
item   3 0.269 0.252                                                               
item   4 0.275 0.256 0.265                                                         
item   5 0.281 0.258 0.268 0.274                                                   
item   6 0.278 0.257 0.267 0.273 0.277                                             
item   7 0.273 0.243 0.255 0.262 0.269 0.266                                       
item   8 0.280 0.258 0.268 0.274 0.279 0.277 0.269                                 
item   9 0.277 0.248 0.259 0.266 0.273 0.270 0.271 0.273                           
item  10 0.281 0.258 0.269 0.275 0.280 0.278 0.270 0.280 0.274                     
item  11 0.282 0.258 0.269 0.275 0.280 0.278 0.271 0.280 0.275 0.281               
item  12 0.273 0.243 0.255 0.262 0.269 0.266 0.268 0.269 0.271 0.270  0.271        
item  13 0.279 0.251 0.263 0.269 0.276 0.273 0.272 0.276 0.275 0.277  0.278  0.272 
> MML_Rasch <- lmer(score ~ 0+item+(1|id), rasch_data, family=binomial(link="logit"))
> summary(MML_Rasch)
Generalized linear mixed model fit by the Laplace approximation 
Formula: score ~ 0 + item + (1 | id) 
   Data: rasch_data 
  AIC  BIC logLik deviance
 6502 6595  -3237     6474
Random effects:
 Groups Name        Variance Std.Dev.
 id     (Intercept) 1.9821   1.4079  
Number of obs: 5512, groups: id, 424

Fixed effects:
         Estimate Std. Error z value Pr(>|z|)    
item   1  0.05819    0.13004   0.447 0.654524    
item   2  1.26791    0.14126   8.976  < 2e-16 ***
item   3  0.93972    0.13615   6.902 5.12e-12 ***
item   4  0.70219    0.13345   5.262 1.43e-07 ***
item   5  0.36877    0.13099   2.815 0.004874 ** 
item   6  0.52699    0.13197   3.993 6.52e-05 ***
item   7 -0.79568    0.13404  -5.936 2.92e-09 ***
item   8  0.38186    0.13106   2.914 0.003572 ** 
item   9 -0.61867    0.13239  -4.673 2.97e-06 ***
item  10  0.30358    0.13069   2.323 0.020184 *  
item  11  0.23870    0.13044   1.830 0.067262 .  
item  12 -0.79568    0.13404  -5.936 2.92e-09 ***
item  13 -0.47278    0.13137  -3.599 0.000320 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Correlation of Fixed Effects:
         item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 item11 item12
item   2 0.258                                                                     
item   3 0.269 0.252                                                               
item   4 0.275 0.256 0.265                                                         
item   5 0.281 0.258 0.268 0.274                                                   
item   6 0.278 0.257 0.267 0.273 0.277                                             
item   7 0.273 0.243 0.255 0.262 0.269 0.266                                       
item   8 0.280 0.258 0.268 0.274 0.279 0.277 0.269                                 
item   9 0.277 0.248 0.259 0.266 0.273 0.270 0.271 0.273                           
item  10 0.281 0.258 0.269 0.275 0.280 0.278 0.270 0.280 0.274                     
item  11 0.282 0.258 0.269 0.275 0.280 0.278 0.271 0.280 0.275 0.281               
item  12 0.273 0.243 0.255 0.262 0.269 0.266 0.268 0.269 0.271 0.270  0.271        
item  13 0.279 0.251 0.263 0.269 0.276 0.273 0.272 0.276 0.275 0.277  0.278  0.272 


Dr. Iasonas Lamprianou
Department of Education
The University of Manchester
Oxford Road, Manchester M13 9PL, UK
Tel. 0044  161 275 3485
iasonas.lamprianou at manchester.ac.uk


--- On Sat, 21/2/09, r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org> wrote:

> From: r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org>
> Subject: R-sig-mixed-models Digest, Vol 26, Issue 33
> To: r-sig-mixed-models at r-project.org
> Date: Saturday, 21 February, 2009, 11:00 AM
> Send R-sig-mixed-models mailing list submissions to
> 	r-sig-mixed-models at r-project.org
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> or, via email, send a message with subject or body
> 'help' to
> 	r-sig-mixed-models-request at r-project.org
> 
> You can reach the person managing the list at
> 	r-sig-mixed-models-owner at r-project.org
> 
> When replying, please edit your Subject line so it is more
> specific
> than "Re: Contents of R-sig-mixed-models
> digest..."
> 
> 
> Today's Topics:
> 
>    1. Re: Power calculations for random effect models (Greg
> Snow)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Fri, 20 Feb 2009 18:12:52 -0700
> From: Greg Snow <Greg.Snow at imail.org>
> Subject: Re: [R-sig-ME] Power calculations for random
> effect models
> To: "christos at nuverabio.com"
> <christos at nuverabio.com>,
> 	"r-sig-mixed-models at r-project.org"
> <r-sig-mixed-models at r-project.org>
> Message-ID:
> 	<B37C0A15B8FB3C468B5BC7EBC7DA14CC61CA3E2EBE at LP-EXMBVS10.CO.IHC.COM>
> Content-Type: text/plain; charset="us-ascii"
> 
> Since you already have a function to simulate the data, it
> should be fairly simple to use simulations to compare the
> methods you suggest below, or to do a simple test based on
> the likelihood ratio, but simulating the cutoff rather than
> depending on the chi-squared approximation.
> 
> See:
> http://finzi.psych.upenn.edu/R/Rhelp08/archive/156522.html
> 
> for some examples of assessing these types of questions
> using simulation (I have an updated version of the last
> example if you want it as well).
> 
> Hope this helps,
> 
> -- 
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at imail.org
> 801.408.8111
> 
> 
> > -----Original Message-----
> > From: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-
> > models-bounces at r-project.org] On Behalf Of Christos
> Hatzis
> > Sent: Friday, February 20, 2009 11:01 AM
> > To: r-sig-mixed-models at r-project.org
> > Subject: [R-sig-ME] Power calculations for random
> effect models
> > 
> > Hello,
> > 
> > I have a nested random effects model of the form
> > 
> > Yijk = M + Ai + Bj(i) + Ek(ij)
> > 
> > where biopsies (B) are nested within persons (A) and
> arrays (E) are
> > nested
> > within biopsies.
> > I am interested in estimating the power of a study of
> a given size to
> > determine whether the variance associated with B is
> trivial, i.e. H0:
> > var_b
> > = 0 vs Ha: var_b > 0 at a fixed Type-I error rate.
> > 
> > I have written a function to simulate data from this
> model and used
> > lmer to
> > estimate the random effects as shown below.  What is
> the recommended
> > way of
> > going about testing the null hypothesis?  One approach
> would be to use
> > the
> > estimated standard deviation of the random effect and
> assuming
> > normality to
> > test whether the appropriate CI contains zero. Another
> would be to use
> > the
> > theoretical chi-square distribution for var_b, but
> that would require
> > the
> > appropriate degrees of freedom.  Or to use mcmc to
> estimate the
> > distribution
> > of var_b and use this distribution for inference.
> > 
> > I would think that the mcmc approach is the
> recommended one, but I
> > would
> > appreciate any advise on this. In this case, I have
> tried to run the
> > MCMC
> > simulation (which runs fine), but have not been able
> yet to figure out
> > how
> > to use the results of MCMC to test the above
> hypothesis.  Any hints or
> > pointing out materials that explain how to use MCMC
> for inference on
> > random
> > effects will be very much appreciated.
> > 
> > Thank you.
> > -Christos
> > 
> > Christos Hatzis, Ph.D.
> > Nuvera Biosciences, Inc.
> > 400 West Cummings Park
> > Suite 5350
> > Woburn, MA 01801
> > Tel: 781-938-3830
> > www.nuverabio.com <http://www.nuverabio.com/>
> > 
> > > fake.dt <- tumor.heter.dt(10, 3, 2, k=0, sa=4,
> sb=.6, se=.2)
> > > fake.dt
> >    person biopsy array      resp bioWper
> > 1       1      1     1  4.308434     1:1
> > 2       1      1     2  4.293186     1:1
> > 3       1      2     1  5.503841     1:2
> > 4       1      2     2  5.640362     1:2
> > 5       1      3     1  5.579461     1:3
> > 6       1      3     2  5.416201     1:3
> > 7       2      1     1 12.479513     2:1
> > 8       2      1     2 12.311426     2:1
> > 9       2      2     1 13.283566     2:2
> > 10      2      2     2 13.138276     2:2
> > 11      2      3     1 12.954277     2:3
> > 12      2      3     2 13.059925     2:3
> > 13      3      1     1 11.649726     3:1
> > 14      3      1     2 11.694472     3:1
> > 15      3      2     1 12.342050     3:2
> > 16      3      2     2 12.316214     3:2
> > 17      3      3     1 12.762695     3:3
> > 18      3      3     2 12.451338     3:3
> > 19      4      1     1 17.168248     4:1
> > 20      4      1     2 17.573315     4:1
> > 21      4      2     1 16.885176     4:2
> > 22      4      2     2 16.819536     4:2
> > 23      4      3     1 15.924120     4:3
> > 24      4      3     2 16.038491     4:3
> > 25      5      1     1  7.981279     5:1
> > 26      5      1     2  7.777929     5:1
> > 27      5      2     1  6.701801     5:2
> > 28      5      2     2  6.978284     5:2
> > 29      5      3     1  7.136417     5:3
> > 30      5      3     2  7.378498     5:3
> > 31      6      1     1 21.499796     6:1
> > 32      6      1     2 21.397173     6:1
> > 33      6      2     1 22.551116     6:2
> > 34      6      2     2 22.521006     6:2
> > 35      6      3     1 21.027998     6:3
> > 36      6      3     2 21.416872     6:3
> > 37      7      1     1 13.312857     7:1
> > 38      7      1     2 13.559062     7:1
> > 39      7      2     1 13.753016     7:2
> > 40      7      2     2 13.608642     7:2
> > 41      7      3     1 13.556446     7:3
> > 42      7      3     2 13.400672     7:3
> > 43      8      1     1 12.758548     8:1
> > 44      8      1     2 12.486574     8:1
> > 45      8      2     1 13.388409     8:2
> > 46      8      2     2 13.263029     8:2
> > 47      8      3     1 12.991308     8:3
> > 48      8      3     2 12.962116     8:3
> > 49      9      1     1 11.420214     9:1
> > 50      9      1     2 11.308010     9:1
> > 51      9      2     1 13.186774     9:2
> > 52      9      2     2 12.778966     9:2
> > 53      9      3     1 11.625079     9:3
> > 54      9      3     2 11.637015     9:3
> > 55     10      1     1  9.108635    10:1
> > 56     10      1     2  8.895658    10:1
> > 57     10      2     1  9.718046    10:2
> > 58     10      2     2  9.552510    10:2
> > 59     10      3     1  8.794893    10:3
> > 60     10      3     2  9.047379    10:3
> > > heter.lmer <- lmer(resp ~ (1 | person) + (1 |
> bioWper), fake.dt)
> > > heter.lmer
> > Linear mixed model fit by REML
> > Formula: resp ~ (1 | person) + (1 | bioWper)
> >    Data: fake.dt
> >    AIC   BIC logLik deviance REMLdev
> >  98.24 106.6 -45.12    92.85   90.24
> > Random effects:
> >  Groups   Name        Variance  Std.Dev.
> >  bioWper  (Intercept)  0.312327 0.55886
> >  person   (Intercept) 21.780993 4.66701
> >  Residual              0.020633 0.14364
> > Number of obs: 60, groups: bioWper, 30; person, 10
> > 
> > Fixed effects:
> >             Estimate Std. Error t value
> > (Intercept)   12.368      1.479    8.36
> > 
> > 
> > > VarCorr(heter.lmer)[["bioWper"]]
> >             (Intercept)
> > (Intercept)   0.3123273
> > attr(,"stddev")
> > (Intercept)
> >   0.5588625
> > attr(,"correlation")
> >             (Intercept)
> > (Intercept)           1
> > 
> > > heter.mcmc <- mcmcsamp(heter.lmer, n=1000)
> > > str(heter.mcmc)
> > Formal class 'merMCMC' [package
> "lme4"] with 9 slots
> >   ..@ Gp      : int [1:3] 0 30 40
> >   ..@ ST      : num [1:2, 1:1000] 3.89 32.49 1.44
> 27.06 1.24 ...
> >   ..@ call    : language lmer(formula = resp ~ (1 |
> person) + (1 |
> > bioWper),
> > data = fake.dt)
> >   ..@ deviance: num [1:1000] 92.9 92.9 114.5 119.1 134
> ...
> >   ..@ dims    : Named int [1:18] 2 60 1 40 1 2 0 1 2 5
> ...
> >   .. ..- attr(*, "names")= chr [1:18]
> "nt" "n" "p" "q" ...
> >   ..@ fixef   : num [1, 1:1000] 12.4 14.1 11.9 12.6
> 12.6 ...
> >   .. ..- attr(*, "dimnames")=List of 2
> >   .. .. ..$ : chr "(Intercept)"
> >   .. .. ..$ : NULL
> >   ..@ nc      : int [1:2] 1 1
> >   ..@ ranef   : num[1:40, 0 ]
> >   ..@ sigma   : num [1, 1:1000] 0.144 0.128 0.126
> 0.212 0.228 ...
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> >
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 
> ------------------------------
> 
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> End of R-sig-mixed-models Digest, Vol 26, Issue 33
> **************************************************






From bates at stat.wisc.edu  Sat Feb 21 15:19:05 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 21 Feb 2009 08:19:05 -0600
Subject: [R-sig-ME] Rasch model
In-Reply-To: <155350.42290.qm@web54106.mail.re2.yahoo.com>
References: <mailman.3.1235214002.26243.r-sig-mixed-models@r-project.org>
	<155350.42290.qm@web54106.mail.re2.yahoo.com>
Message-ID: <40e66e0b0902210619x50269a86sef3b79660d236c3e@mail.gmail.com>

On Sat, Feb 21, 2009 at 5:31 AM, Iasonas Lamprianou
<lamprianou at yahoo.com> wrote:
> Dear friends,
> I am running a simple model where 424 pupils took 13 test items. I model the item easiness as fixed effects and the pupil abilities as random effects (this is basically the 'so-called' marginal maximum likelihood estimation used in some Rasch software packages. I also run the analysis using two 'traditional' Rasch packages. I have found that there is a near-perfect correlation between item estimates from the 'traditional' packages and lme4. See the results below. However, when I use ranef(model$id) to  get the 'ability' estimates of the pupils, the correlation is just 0.9! Shouldnt the correlation be much bigger? I mean, how would you estimate the ability estimates of the pupils in this context?

> Thanks for any help

As far as I know the conditional modes of the random effects for
students should correspond to the student ability estimates from
marginal maximum likelihood.  However, I don't know much about
commercial Rasch packages calculate these parameters.

With more items is becomes reasonable to model both the item easiness
and the pupil abilities as random effects which is not easily done in
commercial packages.  There is a chapter in the book "Exploratory Item
Response Models" describing the model and why it is desirable but
without indication of how it could be fit.  In the paper

@article{Dowling:Bliese:Bates:Doran:2007:JSSOBK:v20i02,
  author =	"Harold Doran and Douglas Bates and Paul Bliese and Maritza
 Dowling",
  title =	"Estimating the Multilevel Rasch Model: With the lme4 Package",
  journal =	"Journal of Statistical Software",
  volume =	"20",
  number =	"2",
  pages =	"1--18",
  day =  	"22",
  month =	"2",
  year = 	"2007",
  CODEN =	"JSSOBK",
  ISSN = 	"1548-7660",
  bibdate =	"2007-02-22",
  URL =  	"http://www.jstatsoft.org/v20/i02",
  accepted =	"2007-02-22",
  acknowledgement = "",
  keywords =	"",
  submitted =	"2006-10-01",
}

we show how to fit that model and generalizations of it where items
and/or subjects fall into groups.  Also, there are several slides in
the section "Item Response Models as GLMMs" of the workshop
presentation at http://www.stat.wisc.edu/~bates/UseR2008/WorkshopD.pdf
related to fitting Rasch models with crossed random effects.



> Generalized linear mixed model fit by the Laplace approximation
> Formula: score ~ 0 + item + (1 | id)
>   Data: rasch_data
>  AIC  BIC logLik deviance
>  6502 6595  -3237     6474
> Random effects:
>  Groups Name        Variance Std.Dev.
>  id     (Intercept) 1.9821   1.4079
> Number of obs: 5512, groups: id, 424
>
> Fixed effects:
>         Estimate Std. Error z value Pr(>|z|)
> item   1  0.05819    0.13004   0.447 0.654524
> item   2  1.26791    0.14126   8.976  < 2e-16 ***
> item   3  0.93972    0.13615   6.902 5.12e-12 ***
> item   4  0.70219    0.13345   5.262 1.43e-07 ***
> item   5  0.36877    0.13099   2.815 0.004874 **
> item   6  0.52699    0.13197   3.993 6.52e-05 ***
> item   7 -0.79568    0.13404  -5.936 2.92e-09 ***
> item   8  0.38186    0.13106   2.914 0.003572 **
> item   9 -0.61867    0.13239  -4.673 2.97e-06 ***
> item  10  0.30358    0.13069   2.323 0.020184 *
> item  11  0.23870    0.13044   1.830 0.067262 .
> item  12 -0.79568    0.13404  -5.936 2.92e-09 ***
> item  13 -0.47278    0.13137  -3.599 0.000320 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Correlation of Fixed Effects:
>         item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 item11 item12
> item   2 0.258
> item   3 0.269 0.252
> item   4 0.275 0.256 0.265
> item   5 0.281 0.258 0.268 0.274
> item   6 0.278 0.257 0.267 0.273 0.277
> item   7 0.273 0.243 0.255 0.262 0.269 0.266
> item   8 0.280 0.258 0.268 0.274 0.279 0.277 0.269
> item   9 0.277 0.248 0.259 0.266 0.273 0.270 0.271 0.273
> item  10 0.281 0.258 0.269 0.275 0.280 0.278 0.270 0.280 0.274
> item  11 0.282 0.258 0.269 0.275 0.280 0.278 0.271 0.280 0.275 0.281
> item  12 0.273 0.243 0.255 0.262 0.269 0.266 0.268 0.269 0.271 0.270  0.271
> item  13 0.279 0.251 0.263 0.269 0.276 0.273 0.272 0.276 0.275 0.277  0.278  0.272
>> MML_Rasch <- lmer(score ~ 0+item+(1|id), rasch_data, family=binomial(link="logit"))
>> summary(MML_Rasch)
> Generalized linear mixed model fit by the Laplace approximation
> Formula: score ~ 0 + item + (1 | id)
>   Data: rasch_data
>  AIC  BIC logLik deviance
>  6502 6595  -3237     6474
> Random effects:
>  Groups Name        Variance Std.Dev.
>  id     (Intercept) 1.9821   1.4079
> Number of obs: 5512, groups: id, 424
>
> Fixed effects:
>         Estimate Std. Error z value Pr(>|z|)
> item   1  0.05819    0.13004   0.447 0.654524
> item   2  1.26791    0.14126   8.976  < 2e-16 ***
> item   3  0.93972    0.13615   6.902 5.12e-12 ***
> item   4  0.70219    0.13345   5.262 1.43e-07 ***
> item   5  0.36877    0.13099   2.815 0.004874 **
> item   6  0.52699    0.13197   3.993 6.52e-05 ***
> item   7 -0.79568    0.13404  -5.936 2.92e-09 ***
> item   8  0.38186    0.13106   2.914 0.003572 **
> item   9 -0.61867    0.13239  -4.673 2.97e-06 ***
> item  10  0.30358    0.13069   2.323 0.020184 *
> item  11  0.23870    0.13044   1.830 0.067262 .
> item  12 -0.79568    0.13404  -5.936 2.92e-09 ***
> item  13 -0.47278    0.13137  -3.599 0.000320 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Correlation of Fixed Effects:
>         item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 item11 item12
> item   2 0.258
> item   3 0.269 0.252
> item   4 0.275 0.256 0.265
> item   5 0.281 0.258 0.268 0.274
> item   6 0.278 0.257 0.267 0.273 0.277
> item   7 0.273 0.243 0.255 0.262 0.269 0.266
> item   8 0.280 0.258 0.268 0.274 0.279 0.277 0.269
> item   9 0.277 0.248 0.259 0.266 0.273 0.270 0.271 0.273
> item  10 0.281 0.258 0.269 0.275 0.280 0.278 0.270 0.280 0.274
> item  11 0.282 0.258 0.269 0.275 0.280 0.278 0.271 0.280 0.275 0.281
> item  12 0.273 0.243 0.255 0.262 0.269 0.266 0.268 0.269 0.271 0.270  0.271
> item  13 0.279 0.251 0.263 0.269 0.276 0.273 0.272 0.276 0.275 0.277  0.278  0.272
>
>
> Dr. Iasonas Lamprianou
> Department of Education
> The University of Manchester
> Oxford Road, Manchester M13 9PL, UK
> Tel. 0044  161 275 3485
> iasonas.lamprianou at manchester.ac.uk
>
>
> --- On Sat, 21/2/09, r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org> wrote:
>
>> From: r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org>
>> Subject: R-sig-mixed-models Digest, Vol 26, Issue 33
>> To: r-sig-mixed-models at r-project.org
>> Date: Saturday, 21 February, 2009, 11:00 AM
>> Send R-sig-mixed-models mailing list submissions to
>>       r-sig-mixed-models at r-project.org
>>
>> To subscribe or unsubscribe via the World Wide Web, visit
>>       https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> or, via email, send a message with subject or body
>> 'help' to
>>       r-sig-mixed-models-request at r-project.org
>>
>> You can reach the person managing the list at
>>       r-sig-mixed-models-owner at r-project.org
>>
>> When replying, please edit your Subject line so it is more
>> specific
>> than "Re: Contents of R-sig-mixed-models
>> digest..."
>>
>>
>> Today's Topics:
>>
>>    1. Re: Power calculations for random effect models (Greg
>> Snow)
>>
>>
>> ----------------------------------------------------------------------
>>
>> Message: 1
>> Date: Fri, 20 Feb 2009 18:12:52 -0700
>> From: Greg Snow <Greg.Snow at imail.org>
>> Subject: Re: [R-sig-ME] Power calculations for random
>> effect models
>> To: "christos at nuverabio.com"
>> <christos at nuverabio.com>,
>>       "r-sig-mixed-models at r-project.org"
>> <r-sig-mixed-models at r-project.org>
>> Message-ID:
>>       <B37C0A15B8FB3C468B5BC7EBC7DA14CC61CA3E2EBE at LP-EXMBVS10.CO.IHC.COM>
>> Content-Type: text/plain; charset="us-ascii"
>>
>> Since you already have a function to simulate the data, it
>> should be fairly simple to use simulations to compare the
>> methods you suggest below, or to do a simple test based on
>> the likelihood ratio, but simulating the cutoff rather than
>> depending on the chi-squared approximation.
>>
>> See:
>> http://finzi.psych.upenn.edu/R/Rhelp08/archive/156522.html
>>
>> for some examples of assessing these types of questions
>> using simulation (I have an updated version of the last
>> example if you want it as well).
>>
>> Hope this helps,
>>
>> --
>> Gregory (Greg) L. Snow Ph.D.
>> Statistical Data Center
>> Intermountain Healthcare
>> greg.snow at imail.org
>> 801.408.8111
>>
>>
>> > -----Original Message-----
>> > From: r-sig-mixed-models-bounces at r-project.org
>> [mailto:r-sig-mixed-
>> > models-bounces at r-project.org] On Behalf Of Christos
>> Hatzis
>> > Sent: Friday, February 20, 2009 11:01 AM
>> > To: r-sig-mixed-models at r-project.org
>> > Subject: [R-sig-ME] Power calculations for random
>> effect models
>> >
>> > Hello,
>> >
>> > I have a nested random effects model of the form
>> >
>> > Yijk = M + Ai + Bj(i) + Ek(ij)
>> >
>> > where biopsies (B) are nested within persons (A) and
>> arrays (E) are
>> > nested
>> > within biopsies.
>> > I am interested in estimating the power of a study of
>> a given size to
>> > determine whether the variance associated with B is
>> trivial, i.e. H0:
>> > var_b
>> > = 0 vs Ha: var_b > 0 at a fixed Type-I error rate.
>> >
>> > I have written a function to simulate data from this
>> model and used
>> > lmer to
>> > estimate the random effects as shown below.  What is
>> the recommended
>> > way of
>> > going about testing the null hypothesis?  One approach
>> would be to use
>> > the
>> > estimated standard deviation of the random effect and
>> assuming
>> > normality to
>> > test whether the appropriate CI contains zero. Another
>> would be to use
>> > the
>> > theoretical chi-square distribution for var_b, but
>> that would require
>> > the
>> > appropriate degrees of freedom.  Or to use mcmc to
>> estimate the
>> > distribution
>> > of var_b and use this distribution for inference.
>> >
>> > I would think that the mcmc approach is the
>> recommended one, but I
>> > would
>> > appreciate any advise on this. In this case, I have
>> tried to run the
>> > MCMC
>> > simulation (which runs fine), but have not been able
>> yet to figure out
>> > how
>> > to use the results of MCMC to test the above
>> hypothesis.  Any hints or
>> > pointing out materials that explain how to use MCMC
>> for inference on
>> > random
>> > effects will be very much appreciated.
>> >
>> > Thank you.
>> > -Christos
>> >
>> > Christos Hatzis, Ph.D.
>> > Nuvera Biosciences, Inc.
>> > 400 West Cummings Park
>> > Suite 5350
>> > Woburn, MA 01801
>> > Tel: 781-938-3830
>> > www.nuverabio.com <http://www.nuverabio.com/>
>> >
>> > > fake.dt <- tumor.heter.dt(10, 3, 2, k=0, sa=4,
>> sb=.6, se=.2)
>> > > fake.dt
>> >    person biopsy array      resp bioWper
>> > 1       1      1     1  4.308434     1:1
>> > 2       1      1     2  4.293186     1:1
>> > 3       1      2     1  5.503841     1:2
>> > 4       1      2     2  5.640362     1:2
>> > 5       1      3     1  5.579461     1:3
>> > 6       1      3     2  5.416201     1:3
>> > 7       2      1     1 12.479513     2:1
>> > 8       2      1     2 12.311426     2:1
>> > 9       2      2     1 13.283566     2:2
>> > 10      2      2     2 13.138276     2:2
>> > 11      2      3     1 12.954277     2:3
>> > 12      2      3     2 13.059925     2:3
>> > 13      3      1     1 11.649726     3:1
>> > 14      3      1     2 11.694472     3:1
>> > 15      3      2     1 12.342050     3:2
>> > 16      3      2     2 12.316214     3:2
>> > 17      3      3     1 12.762695     3:3
>> > 18      3      3     2 12.451338     3:3
>> > 19      4      1     1 17.168248     4:1
>> > 20      4      1     2 17.573315     4:1
>> > 21      4      2     1 16.885176     4:2
>> > 22      4      2     2 16.819536     4:2
>> > 23      4      3     1 15.924120     4:3
>> > 24      4      3     2 16.038491     4:3
>> > 25      5      1     1  7.981279     5:1
>> > 26      5      1     2  7.777929     5:1
>> > 27      5      2     1  6.701801     5:2
>> > 28      5      2     2  6.978284     5:2
>> > 29      5      3     1  7.136417     5:3
>> > 30      5      3     2  7.378498     5:3
>> > 31      6      1     1 21.499796     6:1
>> > 32      6      1     2 21.397173     6:1
>> > 33      6      2     1 22.551116     6:2
>> > 34      6      2     2 22.521006     6:2
>> > 35      6      3     1 21.027998     6:3
>> > 36      6      3     2 21.416872     6:3
>> > 37      7      1     1 13.312857     7:1
>> > 38      7      1     2 13.559062     7:1
>> > 39      7      2     1 13.753016     7:2
>> > 40      7      2     2 13.608642     7:2
>> > 41      7      3     1 13.556446     7:3
>> > 42      7      3     2 13.400672     7:3
>> > 43      8      1     1 12.758548     8:1
>> > 44      8      1     2 12.486574     8:1
>> > 45      8      2     1 13.388409     8:2
>> > 46      8      2     2 13.263029     8:2
>> > 47      8      3     1 12.991308     8:3
>> > 48      8      3     2 12.962116     8:3
>> > 49      9      1     1 11.420214     9:1
>> > 50      9      1     2 11.308010     9:1
>> > 51      9      2     1 13.186774     9:2
>> > 52      9      2     2 12.778966     9:2
>> > 53      9      3     1 11.625079     9:3
>> > 54      9      3     2 11.637015     9:3
>> > 55     10      1     1  9.108635    10:1
>> > 56     10      1     2  8.895658    10:1
>> > 57     10      2     1  9.718046    10:2
>> > 58     10      2     2  9.552510    10:2
>> > 59     10      3     1  8.794893    10:3
>> > 60     10      3     2  9.047379    10:3
>> > > heter.lmer <- lmer(resp ~ (1 | person) + (1 |
>> bioWper), fake.dt)
>> > > heter.lmer
>> > Linear mixed model fit by REML
>> > Formula: resp ~ (1 | person) + (1 | bioWper)
>> >    Data: fake.dt
>> >    AIC   BIC logLik deviance REMLdev
>> >  98.24 106.6 -45.12    92.85   90.24
>> > Random effects:
>> >  Groups   Name        Variance  Std.Dev.
>> >  bioWper  (Intercept)  0.312327 0.55886
>> >  person   (Intercept) 21.780993 4.66701
>> >  Residual              0.020633 0.14364
>> > Number of obs: 60, groups: bioWper, 30; person, 10
>> >
>> > Fixed effects:
>> >             Estimate Std. Error t value
>> > (Intercept)   12.368      1.479    8.36
>> >
>> >
>> > > VarCorr(heter.lmer)[["bioWper"]]
>> >             (Intercept)
>> > (Intercept)   0.3123273
>> > attr(,"stddev")
>> > (Intercept)
>> >   0.5588625
>> > attr(,"correlation")
>> >             (Intercept)
>> > (Intercept)           1
>> >
>> > > heter.mcmc <- mcmcsamp(heter.lmer, n=1000)
>> > > str(heter.mcmc)
>> > Formal class 'merMCMC' [package
>> "lme4"] with 9 slots
>> >   ..@ Gp      : int [1:3] 0 30 40
>> >   ..@ ST      : num [1:2, 1:1000] 3.89 32.49 1.44
>> 27.06 1.24 ...
>> >   ..@ call    : language lmer(formula = resp ~ (1 |
>> person) + (1 |
>> > bioWper),
>> > data = fake.dt)
>> >   ..@ deviance: num [1:1000] 92.9 92.9 114.5 119.1 134
>> ...
>> >   ..@ dims    : Named int [1:18] 2 60 1 40 1 2 0 1 2 5
>> ...
>> >   .. ..- attr(*, "names")= chr [1:18]
>> "nt" "n" "p" "q" ...
>> >   ..@ fixef   : num [1, 1:1000] 12.4 14.1 11.9 12.6
>> 12.6 ...
>> >   .. ..- attr(*, "dimnames")=List of 2
>> >   .. .. ..$ : chr "(Intercept)"
>> >   .. .. ..$ : NULL
>> >   ..@ nc      : int [1:2] 1 1
>> >   ..@ ranef   : num[1:40, 0 ]
>> >   ..@ sigma   : num [1, 1:1000] 0.144 0.128 0.126
>> 0.212 0.228 ...
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> >
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>> ------------------------------
>>
>> _______________________________________________
>> R-sig-mixed-models mailing list
>> R-sig-mixed-models at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>> End of R-sig-mixed-models Digest, Vol 26, Issue 33
>> **************************************************
>
>
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From HDoran at air.org  Sun Feb 22 15:33:07 2009
From: HDoran at air.org (Doran, Harold)
Date: Sun, 22 Feb 2009 09:33:07 -0500
Subject: [R-sig-ME] Rasch model
References: <mailman.3.1235214002.26243.r-sig-mixed-models@r-project.org><155350.42290.qm@web54106.mail.re2.yahoo.com>
	<40e66e0b0902210619x50269a86sef3b79660d236c3e@mail.gmail.com>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE01C64A59@DC1EXCL01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090222/fce1b358/attachment.pl>

From lamprianou at yahoo.com  Sun Feb 22 17:49:53 2009
From: lamprianou at yahoo.com (Iasonas Lamprianou)
Date: Sun, 22 Feb 2009 08:49:53 -0800 (PST)
Subject: [R-sig-ME] Rasch model
In-Reply-To: <ED7B522EE00C9A4FA515AA71724D61EE01C64A59@DC1EXCL01.air.org>
Message-ID: <974267.81451.qm@web54109.mail.re2.yahoo.com>

Thank you DH,
I used bigsteps and Analysis which both give exactly the same results. The ltm package also gave similar results to the BigSteps and Analysis. However, why does lme4 give different ability estimates, while it gives practically the same difficulty estimates?
In case you are interested, I attach the raw data as well as a comparison with the results of various packages in an excel file

thanks

Dr. Iasonas Lamprianou
Department of Education
The University of Manchester
Oxford Road, Manchester M13 9PL, UK
Tel. 0044  161 275 3485
iasonas.lamprianou at manchester.ac.uk


--- On Sun, 22/2/09, Doran, Harold <HDoran at air.org> wrote:

> From: Doran, Harold <HDoran at air.org>
> Subject: RE: [R-sig-ME] Rasch model
> To: "Douglas Bates" <bates at stat.wisc.edu>, lamprianou at yahoo.com
> Cc: r-sig-mixed-models at r-project.org
> Date: Sunday, 22 February, 2009, 2:33 PM
> What "traditional" method (and what packages) did
> you use for ability estimation in the other packages? When
> you treat students as random as you did and then get the
> ability estimates, this is equivalent to MAP estimation.
> Most traditional packages use ML estimation, which limits
> one's ability to get estimates for those individuals
> with perfect or 0 scores since the likelihood function is
> unbounded. Arbitrary scoring rules are typically applied in
> these cases.
> 
> You might check out the irt.ability() function in the
> MiscPsycho package for generating abilities as it offer you
> MAP, EAP and ML estimates in R.
> 
> 
> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org on behalf of
> Douglas Bates
> Sent: Sat 2/21/2009 9:19 AM
> To: lamprianou at yahoo.com
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Rasch model
>  
> On Sat, Feb 21, 2009 at 5:31 AM, Iasonas Lamprianou
> <lamprianou at yahoo.com> wrote:
> > Dear friends,
> > I am running a simple model where 424 pupils took 13
> test items. I model the item easiness as fixed effects and
> the pupil abilities as random effects (this is basically the
> 'so-called' marginal maximum likelihood estimation
> used in some Rasch software packages. I also run the
> analysis using two 'traditional' Rasch packages. I
> have found that there is a near-perfect correlation between
> item estimates from the 'traditional' packages and
> lme4. See the results below. However, when I use
> ranef(model$id) to  get the 'ability' estimates of
> the pupils, the correlation is just 0.9! Shouldnt the
> correlation be much bigger? I mean, how would you estimate
> the ability estimates of the pupils in this context?
> 
> > Thanks for any help
> 
> As far as I know the conditional modes of the random
> effects for
> students should correspond to the student ability estimates
> from
> marginal maximum likelihood.  However, I don't know
> much about
> commercial Rasch packages calculate these parameters.
> 
> With more items is becomes reasonable to model both the
> item easiness
> and the pupil abilities as random effects which is not
> easily done in
> commercial packages.  There is a chapter in the book
> "Exploratory Item
> Response Models" describing the model and why it is
> desirable but
> without indication of how it could be fit.  In the paper
> 
> @article{Dowling:Bliese:Bates:Doran:2007:JSSOBK:v20i02,
>   author =	"Harold Doran and Douglas Bates and Paul
> Bliese and Maritza
>  Dowling",
>   title =	"Estimating the Multilevel Rasch Model: With
> the lme4 Package",
>   journal =	"Journal of Statistical Software",
>   volume =	"20",
>   number =	"2",
>   pages =	"1--18",
>   day =  	"22",
>   month =	"2",
>   year = 	"2007",
>   CODEN =	"JSSOBK",
>   ISSN = 	"1548-7660",
>   bibdate =	"2007-02-22",
>   URL =  	"http://www.jstatsoft.org/v20/i02",
>   accepted =	"2007-02-22",
>   acknowledgement = "",
>   keywords =	"",
>   submitted =	"2006-10-01",
> }
> 
> we show how to fit that model and generalizations of it
> where items
> and/or subjects fall into groups.  Also, there are several
> slides in
> the section "Item Response Models as GLMMs" of
> the workshop
> presentation at
> http://www.stat.wisc.edu/~bates/UseR2008/WorkshopD.pdf
> related to fitting Rasch models with crossed random
> effects.
> 
> 
> 
> > Generalized linear mixed model fit by the Laplace
> approximation
> > Formula: score ~ 0 + item + (1 | id)
> >   Data: rasch_data
> >  AIC  BIC logLik deviance
> >  6502 6595  -3237     6474
> > Random effects:
> >  Groups Name        Variance Std.Dev.
> >  id     (Intercept) 1.9821   1.4079
> > Number of obs: 5512, groups: id, 424
> >
> > Fixed effects:
> >         Estimate Std. Error z value Pr(>|z|)
> > item   1  0.05819    0.13004   0.447 0.654524
> > item   2  1.26791    0.14126   8.976  < 2e-16 ***
> > item   3  0.93972    0.13615   6.902 5.12e-12 ***
> > item   4  0.70219    0.13345   5.262 1.43e-07 ***
> > item   5  0.36877    0.13099   2.815 0.004874 **
> > item   6  0.52699    0.13197   3.993 6.52e-05 ***
> > item   7 -0.79568    0.13404  -5.936 2.92e-09 ***
> > item   8  0.38186    0.13106   2.914 0.003572 **
> > item   9 -0.61867    0.13239  -4.673 2.97e-06 ***
> > item  10  0.30358    0.13069   2.323 0.020184 *
> > item  11  0.23870    0.13044   1.830 0.067262 .
> > item  12 -0.79568    0.13404  -5.936 2.92e-09 ***
> > item  13 -0.47278    0.13137  -3.599 0.000320 ***
> > ---
> > Signif. codes:  0 '***' 0.001 '**'
> 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> > Correlation of Fixed Effects:
> >         item1 item2 item3 item4 item5 item6 item7
> item8 item9 item10 item11 item12
> > item   2 0.258
> > item   3 0.269 0.252
> > item   4 0.275 0.256 0.265
> > item   5 0.281 0.258 0.268 0.274
> > item   6 0.278 0.257 0.267 0.273 0.277
> > item   7 0.273 0.243 0.255 0.262 0.269 0.266
> > item   8 0.280 0.258 0.268 0.274 0.279 0.277 0.269
> > item   9 0.277 0.248 0.259 0.266 0.273 0.270 0.271
> 0.273
> > item  10 0.281 0.258 0.269 0.275 0.280 0.278 0.270
> 0.280 0.274
> > item  11 0.282 0.258 0.269 0.275 0.280 0.278 0.271
> 0.280 0.275 0.281
> > item  12 0.273 0.243 0.255 0.262 0.269 0.266 0.268
> 0.269 0.271 0.270  0.271
> > item  13 0.279 0.251 0.263 0.269 0.276 0.273 0.272
> 0.276 0.275 0.277  0.278  0.272
> >> MML_Rasch <- lmer(score ~ 0+item+(1|id),
> rasch_data, family=binomial(link="logit"))
> >> summary(MML_Rasch)
> > Generalized linear mixed model fit by the Laplace
> approximation
> > Formula: score ~ 0 + item + (1 | id)
> >   Data: rasch_data
> >  AIC  BIC logLik deviance
> >  6502 6595  -3237     6474
> > Random effects:
> >  Groups Name        Variance Std.Dev.
> >  id     (Intercept) 1.9821   1.4079
> > Number of obs: 5512, groups: id, 424
> >
> > Fixed effects:
> >         Estimate Std. Error z value Pr(>|z|)
> > item   1  0.05819    0.13004   0.447 0.654524
> > item   2  1.26791    0.14126   8.976  < 2e-16 ***
> > item   3  0.93972    0.13615   6.902 5.12e-12 ***
> > item   4  0.70219    0.13345   5.262 1.43e-07 ***
> > item   5  0.36877    0.13099   2.815 0.004874 **
> > item   6  0.52699    0.13197   3.993 6.52e-05 ***
> > item   7 -0.79568    0.13404  -5.936 2.92e-09 ***
> > item   8  0.38186    0.13106   2.914 0.003572 **
> > item   9 -0.61867    0.13239  -4.673 2.97e-06 ***
> > item  10  0.30358    0.13069   2.323 0.020184 *
> > item  11  0.23870    0.13044   1.830 0.067262 .
> > item  12 -0.79568    0.13404  -5.936 2.92e-09 ***
> > item  13 -0.47278    0.13137  -3.599 0.000320 ***
> > ---
> > Signif. codes:  0 '***' 0.001 '**'
> 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> > Correlation of Fixed Effects:
> >         item1 item2 item3 item4 item5 item6 item7
> item8 item9 item10 item11 item12
> > item   2 0.258
> > item   3 0.269 0.252
> > item   4 0.275 0.256 0.265
> > item   5 0.281 0.258 0.268 0.274
> > item   6 0.278 0.257 0.267 0.273 0.277
> > item   7 0.273 0.243 0.255 0.262 0.269 0.266
> > item   8 0.280 0.258 0.268 0.274 0.279 0.277 0.269
> > item   9 0.277 0.248 0.259 0.266 0.273 0.270 0.271
> 0.273
> > item  10 0.281 0.258 0.269 0.275 0.280 0.278 0.270
> 0.280 0.274
> > item  11 0.282 0.258 0.269 0.275 0.280 0.278 0.271
> 0.280 0.275 0.281
> > item  12 0.273 0.243 0.255 0.262 0.269 0.266 0.268
> 0.269 0.271 0.270  0.271
> > item  13 0.279 0.251 0.263 0.269 0.276 0.273 0.272
> 0.276 0.275 0.277  0.278  0.272
> >
> >
> > Dr. Iasonas Lamprianou
> > Department of Education
> > The University of Manchester
> > Oxford Road, Manchester M13 9PL, UK
> > Tel. 0044  161 275 3485
> > iasonas.lamprianou at manchester.ac.uk
> >
> >
> > --- On Sat, 21/2/09,
> r-sig-mixed-models-request at r-project.org
> <r-sig-mixed-models-request at r-project.org> wrote:
> >
> >> From: r-sig-mixed-models-request at r-project.org
> <r-sig-mixed-models-request at r-project.org>
> >> Subject: R-sig-mixed-models Digest, Vol 26, Issue
> 33
> >> To: r-sig-mixed-models at r-project.org
> >> Date: Saturday, 21 February, 2009, 11:00 AM
> >> Send R-sig-mixed-models mailing list submissions
> to
> >>       r-sig-mixed-models at r-project.org
> >>
> >> To subscribe or unsubscribe via the World Wide
> Web, visit
> >>      
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> or, via email, send a message with subject or body
> >> 'help' to
> >>       r-sig-mixed-models-request at r-project.org
> >>
> >> You can reach the person managing the list at
> >>       r-sig-mixed-models-owner at r-project.org
> >>
> >> When replying, please edit your Subject line so it
> is more
> >> specific
> >> than "Re: Contents of R-sig-mixed-models
> >> digest..."
> >>
> >>
> >> Today's Topics:
> >>
> >>    1. Re: Power calculations for random effect
> models (Greg
> >> Snow)
> >>
> >>
> >>
> ----------------------------------------------------------------------
> >>
> >> Message: 1
> >> Date: Fri, 20 Feb 2009 18:12:52 -0700
> >> From: Greg Snow <Greg.Snow at imail.org>
> >> Subject: Re: [R-sig-ME] Power calculations for
> random
> >> effect models
> >> To: "christos at nuverabio.com"
> >> <christos at nuverabio.com>,
> >>       "r-sig-mixed-models at r-project.org"
> >> <r-sig-mixed-models at r-project.org>
> >> Message-ID:
> >>      
> <B37C0A15B8FB3C468B5BC7EBC7DA14CC61CA3E2EBE at LP-EXMBVS10.CO.IHC.COM>
> >> Content-Type: text/plain;
> charset="us-ascii"
> >>
> >> Since you already have a function to simulate the
> data, it
> >> should be fairly simple to use simulations to
> compare the
> >> methods you suggest below, or to do a simple test
> based on
> >> the likelihood ratio, but simulating the cutoff
> rather than
> >> depending on the chi-squared approximation.
> >>
> >> See:
> >>
> http://finzi.psych.upenn.edu/R/Rhelp08/archive/156522.html
> >>
> >> for some examples of assessing these types of
> questions
> >> using simulation (I have an updated version of the
> last
> >> example if you want it as well).
> >>
> >> Hope this helps,
> >>
> >> --
> >> Gregory (Greg) L. Snow Ph.D.
> >> Statistical Data Center
> >> Intermountain Healthcare
> >> greg.snow at imail.org
> >> 801.408.8111
> >>
> >>
> >> > -----Original Message-----
> >> > From:
> r-sig-mixed-models-bounces at r-project.org
> >> [mailto:r-sig-mixed-
> >> > models-bounces at r-project.org] On Behalf Of
> Christos
> >> Hatzis
> >> > Sent: Friday, February 20, 2009 11:01 AM
> >> > To: r-sig-mixed-models at r-project.org
> >> > Subject: [R-sig-ME] Power calculations for
> random
> >> effect models
> >> >
> >> > Hello,
> >> >
> >> > I have a nested random effects model of the
> form
> >> >
> >> > Yijk = M + Ai + Bj(i) + Ek(ij)
> >> >
> >> > where biopsies (B) are nested within persons
> (A) and
> >> arrays (E) are
> >> > nested
> >> > within biopsies.
> >> > I am interested in estimating the power of a
> study of
> >> a given size to
> >> > determine whether the variance associated
> with B is
> >> trivial, i.e. H0:
> >> > var_b
> >> > = 0 vs Ha: var_b > 0 at a fixed Type-I
> error rate.
> >> >
> >> > I have written a function to simulate data
> from this
> >> model and used
> >> > lmer to
> >> > estimate the random effects as shown below. 
> What is
> >> the recommended
> >> > way of
> >> > going about testing the null hypothesis?  One
> approach
> >> would be to use
> >> > the
> >> > estimated standard deviation of the random
> effect and
> >> assuming
> >> > normality to
> >> > test whether the appropriate CI contains
> zero. Another
> >> would be to use
> >> > the
> >> > theoretical chi-square distribution for
> var_b, but
> >> that would require
> >> > the
> >> > appropriate degrees of freedom.  Or to use
> mcmc to
> >> estimate the
> >> > distribution
> >> > of var_b and use this distribution for
> inference.
> >> >
> >> > I would think that the mcmc approach is the
> >> recommended one, but I
> >> > would
> >> > appreciate any advise on this. In this case,
> I have
> >> tried to run the
> >> > MCMC
> >> > simulation (which runs fine), but have not
> been able
> >> yet to figure out
> >> > how
> >> > to use the results of MCMC to test the above
> >> hypothesis.  Any hints or
> >> > pointing out materials that explain how to
> use MCMC
> >> for inference on
> >> > random
> >> > effects will be very much appreciated.
> >> >
> >> > Thank you.
> >> > -Christos
> >> >
> >> > Christos Hatzis, Ph.D.
> >> > Nuvera Biosciences, Inc.
> >> > 400 West Cummings Park
> >> > Suite 5350
> >> > Woburn, MA 01801
> >> > Tel: 781-938-3830
> >> > www.nuverabio.com
> <http://www.nuverabio.com/>
> >> >
> >> > > fake.dt <- tumor.heter.dt(10, 3, 2,
> k=0, sa=4,
> >> sb=.6, se=.2)
> >> > > fake.dt
> >> >    person biopsy array      resp bioWper
> >> > 1       1      1     1  4.308434     1:1
> >> > 2       1      1     2  4.293186     1:1
> >> > 3       1      2     1  5.503841     1:2
> >> > 4       1      2     2  5.640362     1:2
> >> > 5       1      3     1  5.579461     1:3
> >> > 6       1      3     2  5.416201     1:3
> >> > 7       2      1     1 12.479513     2:1
> >> > 8       2      1     2 12.311426     2:1
> >> > 9       2      2     1 13.283566     2:2
> >> > 10      2      2     2 13.138276     2:2
> >> > 11      2      3     1 12.954277     2:3
> >> > 12      2      3     2 13.059925     2:3
> >> > 13      3      1     1 11.649726     3:1
> >> > 14      3      1     2 11.694472     3:1
> >> > 15      3      2     1 12.342050     3:2
> >> > 16      3      2     2 12.316214     3:2
> >> > 17      3      3     1 12.762695     3:3
> >> > 18      3      3     2 12.451338     3:3
> >> > 19      4      1     1 17.168248     4:1
> >> > 20      4      1     2 17.573315     4:1
> >> > 21      4      2     1 16.885176     4:2
> >> > 22      4      2     2 16.819536     4:2
> >> > 23      4      3     1 15.924120     4:3
> >> > 24      4      3     2 16.038491     4:3
> >> > 25      5      1     1  7.981279     5:1
> >> > 26      5      1     2  7.777929     5:1
> >> > 27      5      2     1  6.701801     5:2
> >> > 28      5      2     2  6.978284     5:2
> >> > 29      5      3     1  7.136417     5:3
> >> > 30      5      3     2  7.378498     5:3
> >> > 31      6      1     1 21.499796     6:1
> >> > 32      6      1     2 21.397173     6:1
> >> > 33      6      2     1 22.551116     6:2
> >> > 34      6      2     2 22.521006     6:2
> >> > 35      6      3     1 21.027998     6:3
> >> > 36      6      3     2 21.416872     6:3
> >> > 37      7      1     1 13.312857     7:1
> >> > 38      7      1     2 13.559062     7:1
> >> > 39      7      2     1 13.753016     7:2
> >> > 40      7      2     2 13.608642     7:2
> >> > 41      7      3     1 13.556446     7:3
> >> > 42      7      3     2 13.400672     7:3
> >> > 43      8      1     1 12.758548     8:1
> >> > 44      8      1     2 12.486574     8:1
> >> > 45      8      2     1 13.388409     8:2
> >> > 46      8      2     2 13.263029     8:2
> >> > 47      8      3     1 12.991308     8:3
> >> > 48      8      3     2 12.962116     8:3
> >> > 49      9      1     1 11.420214     9:1
> >> > 50      9      1     2 11.308010     9:1
> >> > 51      9      2     1 13.186774     9:2
> >> > 52      9      2     2 12.778966     9:2
> >> > 53      9      3     1 11.625079     9:3
> >> > 54      9      3     2 11.637015     9:3
> >> > 55     10      1     1  9.108635    10:1
> >> > 56     10      1     2  8.895658    10:1
> >> > 57     10      2     1  9.718046    10:2
> >> > 58     10      2     2  9.552510    10:2
> >> > 59     10      3     1  8.794893    10:3
> >> > 60     10      3     2  9.047379    10:3
> >> > > heter.lmer <- lmer(resp ~ (1 |
> person) + (1 |
> >> bioWper), fake.dt)
> >> > > heter.lmer
> >> > Linear mixed model fit by REML
> >> > Formula: resp ~ (1 | person) + (1 | bioWper)
> >> >    Data: fake.dt
> >> >    AIC   BIC logLik deviance REMLdev
> >> >  98.24 106.6 -45.12    92.85   90.24
> >> > Random effects:
> >> >  Groups   Name        Variance  Std.Dev.
> >> >  bioWper  (Intercept)  0.312327 0.55886
> >> >  person   (Intercept) 21.780993 4.66701
> >> >  Residual              0.020633 0.14364
> >> > Number of obs: 60, groups: bioWper, 30;
> person, 10
> >> >
> >> > Fixed effects:
> >> >             Estimate Std. Error t value
> >> > (Intercept)   12.368      1.479    8.36
> >> >
> >> >
> >> > >
> VarCorr(heter.lmer)[["bioWper"]]
> >> >             (Intercept)
> >> > (Intercept)   0.3123273
> >> > attr(,"stddev")
> >> > (Intercept)
> >> >   0.5588625
> >> > attr(,"correlation")
> >> >             (Intercept)
> >> > (Intercept)           1
> >> >
> >> > > heter.mcmc <- mcmcsamp(heter.lmer,
> n=1000)
> >> > > str(heter.mcmc)
> >> > Formal class 'merMCMC' [package
> >> "lme4"] with 9 slots
> >> >   ..@ Gp      : int [1:3] 0 30 40
> >> >   ..@ ST      : num [1:2, 1:1000] 3.89 32.49
> 1.44
> >> 27.06 1.24 ...
> >> >   ..@ call    : language lmer(formula = resp
> ~ (1 |
> >> person) + (1 |
> >> > bioWper),
> >> > data = fake.dt)
> >> >   ..@ deviance: num [1:1000] 92.9 92.9 114.5
> 119.1 134
> >> ...
> >> >   ..@ dims    : Named int [1:18] 2 60 1 40 1
> 2 0 1 2 5
> >> ...
> >> >   .. ..- attr(*, "names")= chr
> [1:18]
> >> "nt" "n" "p"
> "q" ...
> >> >   ..@ fixef   : num [1, 1:1000] 12.4 14.1
> 11.9 12.6
> >> 12.6 ...
> >> >   .. ..- attr(*, "dimnames")=List
> of 2
> >> >   .. .. ..$ : chr "(Intercept)"
> >> >   .. .. ..$ : NULL
> >> >   ..@ nc      : int [1:2] 1 1
> >> >   ..@ ranef   : num[1:40, 0 ]
> >> >   ..@ sigma   : num [1, 1:1000] 0.144 0.128
> 0.126
> >> 0.212 0.228 ...
> >> >
> >> >
> _______________________________________________
> >> > R-sig-mixed-models at r-project.org mailing list
> >> >
> >>
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >>
> >>
> >> ------------------------------
> >>
> >> _______________________________________________
> >> R-sig-mixed-models mailing list
> >> R-sig-mixed-models at r-project.org
> >>
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >>
> >> End of R-sig-mixed-models Digest, Vol 26, Issue 33
> >> **************************************************
> >
> >
> >
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> >
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


      

From HDoran at air.org  Sun Feb 22 18:17:04 2009
From: HDoran at air.org (Doran, Harold)
Date: Sun, 22 Feb 2009 12:17:04 -0500
Subject: [R-sig-ME] Rasch model
References: <974267.81451.qm@web54109.mail.re2.yahoo.com>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE01C64A5A@DC1EXCL01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090222/6625583b/attachment.pl>

From kyler at mail.smu.edu  Tue Feb 24 15:28:56 2009
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Tue, 24 Feb 2009 08:28:56 -0600
Subject: [R-sig-ME] AIC in Cross-Classified Model
In-Reply-To: <40e66e0b0902210619x50269a86sef3b79660d236c3e@mail.gmail.com>
References: <mailman.3.1235214002.26243.r-sig-mixed-models@r-project.org>
	<155350.42290.qm@web54106.mail.re2.yahoo.com>
	<40e66e0b0902210619x50269a86sef3b79660d236c3e@mail.gmail.com>
Message-ID: <551E1CBE65B7EB44B9DF69AF8ED0BE7F09212E2F2A@SXMBXA.systems.smu.edu>

Dear All,

I was working an example for my class and came upon a problem. This example is the cross-classified model presented by Hox on p 127 of "Multilevel Analysis." The problem is that the AIC and BIC in the summary of the model are not the same AIC and BIC when the anova function is used. Any idea as to why? I think that the anova function gives the correct AIC, but I don't know why. I threw the dataset up on my website if anyone wants to replicate.

> sessionInfo()
R version 2.7.2 (2008-08-25) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] foreign_0.8-29     mlmRev_0.99875-1   lme4_0.999375-27   Matrix_0.999375-16
[5] lattice_0.17-15   

loaded via a namespace (and not attached):
[1] grid_2.7.2  tools_2.7.2

> pupils<-read.table("http://www.jkyleroberts.com/rfiles/mlm/HoxData/pupils.txt", header=T)
> 
> m0 <- lmer(ACHIEV ~ 1 + (1 | PSCHOOL) + (1 | SSCHOOL), pupils)
> summary(m0)
Linear mixed model fit by REML 
Formula: ACHIEV ~ 1 + (1 | PSCHOOL) + (1 | SSCHOOL) 
   Data: pupils 
  AIC  BIC logLik deviance REMLdev
 2329 2349  -1161     2318    2321
Random effects:
 Groups   Name        Variance Std.Dev.
 PSCHOOL  (Intercept) 0.171900 0.41461 
 SSCHOOL  (Intercept) 0.066652 0.25817 
 Residual             0.513128 0.71633 
Number of obs: 1000, groups: PSCHOOL, 50; SSCHOOL, 30

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.34861    0.07891   80.45
> 
> m1 <- lmer(ACHIEV ~ PUPSEX + PUPSES + (1 | PSCHOOL) + (1 | SSCHOOL), pupils)
> summary(m1)
Linear mixed model fit by REML 
Formula: ACHIEV ~ PUPSEX + PUPSES + (1 | PSCHOOL) + (1 | SSCHOOL) 
   Data: pupils 
  AIC  BIC logLik deviance REMLdev
 2270 2299  -1129     2243    2258
Random effects:
 Groups   Name        Variance Std.Dev.
 PSCHOOL  (Intercept) 0.171530 0.41416 
 SSCHOOL  (Intercept) 0.064809 0.25458 
 Residual             0.475236 0.68937 
Number of obs: 1000, groups: PSCHOOL, 50; SSCHOOL, 30

Fixed effects:
            Estimate Std. Error t value
(Intercept)  5.75548    0.10576   54.42
PUPSEXgirl   0.26132    0.04569    5.72
PUPSES       0.11407    0.01612    7.08

Correlation of Fixed Effects:
           (Intr) PUPSEX
PUPSEXgirl -0.254       
PUPSES     -0.641  0.075
> 
> m2 <- lmer(ACHIEV~PUPSEX+PUPSES+PDENOM+SDENOM+(1 | PSCHOOL) + (1 | SSCHOOL), pupils)
> summary(m2)
Linear mixed model fit by REML 
Formula: ACHIEV ~ PUPSEX + PUPSES + PDENOM + SDENOM + (1 | PSCHOOL) +      (1 | SSCHOOL) 
   Data: pupils 
  AIC  BIC logLik deviance REMLdev
 2273 2312  -1128     2238    2257
Random effects:
 Groups   Name        Variance Std.Dev.
 PSCHOOL  (Intercept) 0.165721 0.40709 
 SSCHOOL  (Intercept) 0.058446 0.24176 
 Residual             0.475207 0.68935 
Number of obs: 1000, groups: PSCHOOL, 50; SSCHOOL, 30

Fixed effects:
            Estimate Std. Error t value
(Intercept)  5.51888    0.14275   38.66
PUPSEXgirl   0.26309    0.04568    5.76
PUPSES       0.11352    0.01612    7.04
PDENOMyes    0.20400    0.12623    1.62
SDENOMyes    0.17572    0.09625    1.83

Correlation of Fixed Effects:
           (Intr) PUPSEX PUPSES PDENOM
PUPSEXgirl -0.200                     
PUPSES     -0.466  0.075              
PDENOMyes  -0.526  0.021  0.004       
SDENOMyes  -0.429  0.004 -0.025 -0.014
> 
> anova(m0, m1, m2)
Data: pupils
Models:
m0: ACHIEV ~ 1 + (1 | PSCHOOL) + (1 | SSCHOOL)
m1: ACHIEV ~ PUPSEX + PUPSES + (1 | PSCHOOL) + (1 | SSCHOOL)
m2: ACHIEV ~ PUPSEX + PUPSES + PDENOM + SDENOM + (1 | PSCHOOL) + 
m2:     (1 | SSCHOOL)
   Df     AIC     BIC  logLik   Chisq Chi Df Pr(>Chisq)    
m0  4  2325.9  2345.5 -1158.9                              
m1  6  2255.5  2284.9 -1121.7 74.3678      2    < 2e-16 ***
m2  8  2253.5  2292.8 -1118.8  5.9684      2    0.05058 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

*********************************************************
Dr. J. Kyle Roberts
Department of Teaching and Learning
Annette Caldwell Simmons School of Education 
   and Human Development
Southern Methodist University
P.O. Box 750381
Dallas, TX? 75275
214-768-4494
http://www.hlm-online.com/



From HDoran at air.org  Tue Feb 24 16:01:36 2009
From: HDoran at air.org (Doran, Harold)
Date: Tue, 24 Feb 2009 10:01:36 -0500
Subject: [R-sig-ME] AIC in Cross-Classified Model
In-Reply-To: <551E1CBE65B7EB44B9DF69AF8ED0BE7F09212E2F2A@SXMBXA.systems.smu.edu>
Message-ID: <ED7B522EE00C9A4FA515AA71724D61EE020C1534@DC1EXCL01.air.org>

Kyle:

You need to use REML=FALSE in your code to compare models with different fixed effects. The ANOVA is giving the loglik for the ML. But, your using REML. 

The AIC is -2*LogLik + 2*number of parameters. In your m0 model you have 1 fixed effect, the residual variance, and I think the relative standard deviations are both scalars here, giving a total of 4 parameters. So, the AIC in the model summary is computed as -2*-1161 + 2*4= 2330 (although, the AIC is 2329 in the model summary) 

Now, in the ANOVA output, the loglik for the same model is -1158.9, so -2*-1158.9+2*4 = 2325.8. The ANOVA gives an AIC of 2325.9. Both seem to be using the same number of parameters, but clearly not using the same loglik. Look at the following example to see why.

(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
(fm2 <- lmer(Reaction ~ 1 + (Days|Subject), sleepstudy))
anova(fm1,fm2)

### Compare loglik in ANOVA to loglik in model summary for fm2
### notice they are different

(fm2 <- lmer(Reaction ~ 1 + (Days|Subject), sleepstudy, REML=FALSE))
anova(fm1,fm2)

### Compare loglik in ANOVA to loglik in model summary for fm2
### Now they are the same

HTH,
Harold

> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org 
> [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf 
> Of Roberts, Kyle
> Sent: Tuesday, February 24, 2009 9:29 AM
> To: 'Douglas Bates'; r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] AIC in Cross-Classified Model
> 
> Dear All,
> 
> I was working an example for my class and came upon a 
> problem. This example is the cross-classified model presented 
> by Hox on p 127 of "Multilevel Analysis." The problem is that 
> the AIC and BIC in the summary of the model are not the same 
> AIC and BIC when the anova function is used. Any idea as to 
> why? I think that the anova function gives the correct AIC, 
> but I don't know why. I threw the dataset up on my website if 
> anyone wants to replicate.
> 
> > sessionInfo()
> R version 2.7.2 (2008-08-25)
> i386-pc-mingw32 
> 
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
> States.1252;LC_MONETARY=English_United 
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods 
>   base     
> 
> other attached packages:
> [1] foreign_0.8-29     mlmRev_0.99875-1   lme4_0.999375-27   
> Matrix_0.999375-16
> [5] lattice_0.17-15   
> 
> loaded via a namespace (and not attached):
> [1] grid_2.7.2  tools_2.7.2
> 
> > 
> pupils<-read.table("http://www.jkyleroberts.com/rfiles/mlm/HoxData/pup
> > ils.txt", header=T)
> > 
> > m0 <- lmer(ACHIEV ~ 1 + (1 | PSCHOOL) + (1 | SSCHOOL), pupils)
> > summary(m0)
> Linear mixed model fit by REML
> Formula: ACHIEV ~ 1 + (1 | PSCHOOL) + (1 | SSCHOOL) 
>    Data: pupils
>   AIC  BIC logLik deviance REMLdev
>  2329 2349  -1161     2318    2321
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  PSCHOOL  (Intercept) 0.171900 0.41461
>  SSCHOOL  (Intercept) 0.066652 0.25817 
>  Residual             0.513128 0.71633 
> Number of obs: 1000, groups: PSCHOOL, 50; SSCHOOL, 30
> 
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)  6.34861    0.07891   80.45
> > 
> > m1 <- lmer(ACHIEV ~ PUPSEX + PUPSES + (1 | PSCHOOL) + (1 | 
> SSCHOOL), 
> > pupils)
> > summary(m1)
> Linear mixed model fit by REML
> Formula: ACHIEV ~ PUPSEX + PUPSES + (1 | PSCHOOL) + (1 | SSCHOOL) 
>    Data: pupils
>   AIC  BIC logLik deviance REMLdev
>  2270 2299  -1129     2243    2258
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  PSCHOOL  (Intercept) 0.171530 0.41416
>  SSCHOOL  (Intercept) 0.064809 0.25458 
>  Residual             0.475236 0.68937 
> Number of obs: 1000, groups: PSCHOOL, 50; SSCHOOL, 30
> 
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)  5.75548    0.10576   54.42
> PUPSEXgirl   0.26132    0.04569    5.72
> PUPSES       0.11407    0.01612    7.08
> 
> Correlation of Fixed Effects:
>            (Intr) PUPSEX
> PUPSEXgirl -0.254       
> PUPSES     -0.641  0.075
> > 
> > m2 <- lmer(ACHIEV~PUPSEX+PUPSES+PDENOM+SDENOM+(1 | PSCHOOL) + (1 | 
> > SSCHOOL), pupils)
> > summary(m2)
> Linear mixed model fit by REML 
> Formula: ACHIEV ~ PUPSEX + PUPSES + PDENOM + SDENOM + (1 | 
> PSCHOOL) +      (1 | SSCHOOL) 
>    Data: pupils
>   AIC  BIC logLik deviance REMLdev
>  2273 2312  -1128     2238    2257
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  PSCHOOL  (Intercept) 0.165721 0.40709
>  SSCHOOL  (Intercept) 0.058446 0.24176 
>  Residual             0.475207 0.68935 
> Number of obs: 1000, groups: PSCHOOL, 50; SSCHOOL, 30
> 
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)  5.51888    0.14275   38.66
> PUPSEXgirl   0.26309    0.04568    5.76
> PUPSES       0.11352    0.01612    7.04
> PDENOMyes    0.20400    0.12623    1.62
> SDENOMyes    0.17572    0.09625    1.83
> 
> Correlation of Fixed Effects:
>            (Intr) PUPSEX PUPSES PDENOM
> PUPSEXgirl -0.200                     
> PUPSES     -0.466  0.075              
> PDENOMyes  -0.526  0.021  0.004       
> SDENOMyes  -0.429  0.004 -0.025 -0.014
> > 
> > anova(m0, m1, m2)
> Data: pupils
> Models:
> m0: ACHIEV ~ 1 + (1 | PSCHOOL) + (1 | SSCHOOL)
> m1: ACHIEV ~ PUPSEX + PUPSES + (1 | PSCHOOL) + (1 | SSCHOOL)
> m2: ACHIEV ~ PUPSEX + PUPSES + PDENOM + SDENOM + (1 | PSCHOOL) + 
> m2:     (1 | SSCHOOL)
>    Df     AIC     BIC  logLik   Chisq Chi Df Pr(>Chisq)    
> m0  4  2325.9  2345.5 -1158.9                              
> m1  6  2255.5  2284.9 -1121.7 74.3678      2    < 2e-16 ***
> m2  8  2253.5  2292.8 -1118.8  5.9684      2    0.05058 .  
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> *********************************************************
> Dr. J. Kyle Roberts
> Department of Teaching and Learning
> Annette Caldwell Simmons School of Education 
>    and Human Development
> Southern Methodist University
> P.O. Box 750381
> Dallas, TX? 75275
> 214-768-4494
> http://www.hlm-online.com/
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 



From ltiana_m at yahoo.com  Thu Feb 26 03:54:48 2009
From: ltiana_m at yahoo.com (Liliana Martinez)
Date: Wed, 25 Feb 2009 18:54:48 -0800 (PST)
Subject: [R-sig-ME] error messages generated by fit.contrast()
Message-ID: <714171.26984.qm@web53010.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090225/891a63f5/attachment.pl>

From DAfshartous at med.miami.edu  Thu Feb 26 17:31:08 2009
From: DAfshartous at med.miami.edu (Afshartous, David)
Date: Thu, 26 Feb 2009 11:31:08 -0500
Subject: [R-sig-ME] Reproducing results from an old lmer fit
Message-ID: <C5CC2DFC.9185%dafshartous@med.miami.edu>


All,

For a paper revision I'm trying to reproduce some results from an old lmer
fit with Rv2.7.1 prior to 5/28/08.  However, when I currently load Rv2.7.1
and lmer, the variance component estimates are slightly different than the
original fit; the sessionInfo() is as follows:

> sessionInfo()
R version 2.7.1 (2008-06-23)
i386-apple-darwin8.10.1

locale:
en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-8

loaded via a namespace (and not attached):
[1] grid_2.7.1  nlme_3.1-89

Thus, I assume that I need to use the same older version of lme4 and/or
Matrix which might be responsible for the difference in the results?  If
this is possible, how is this done?

Cheers,
David

PS - for whatever it's worth, if I do the fit with lme (nlme_3.1-89) under
Rv2.7.1 the results are closer to the original lmer results.



___________________________________________________

Original lmer fit from 5/08:
Model 2:
AIC  BIC logLik MLdeviance REMLdeviance
 2813 2843  -1397       2829         2795
Random effects:
 Groups     Name            Variance Std.Dev. Corr
 subject   (Intercept)      2226.3   47.183
            Drug            2132.9   46.184  -0.865
 Residual                   13673.6  116.934

Current lmer fit:
 AIC  BIC logLik deviance REMLdev
 2815 2849  -1397     2830    2795
Random effects:
 Groups     Name               Variance Std.Dev. Corr
 Patient_no (Intercept)         2165.1   46.531
            Drug.full.reverseC  1386.3   37.233  -1.000
 Residual                      13947.5  118.100


Current lme fit:
   AIC      BIC    logLik
  2814.611 2848.638 -1397.305

                   StdDev    Corr
(Intercept)         47.21031 (Intr)
Drug.full.reverseC  46.14014 -0.866
Residual           116.93541



From bates at stat.wisc.edu  Thu Feb 26 19:17:49 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 26 Feb 2009 12:17:49 -0600
Subject: [R-sig-ME] Reproducing results from an old lmer fit
In-Reply-To: <C5CC2DFC.9185%dafshartous@med.miami.edu>
References: <C5CC2DFC.9185%dafshartous@med.miami.edu>
Message-ID: <40e66e0b0902261017h62f8e13el7dc5dcbdb28a8e06@mail.gmail.com>

On Thu, Feb 26, 2009 at 10:31 AM, Afshartous, David
<DAfshartous at med.miami.edu> wrote:
>
> All,
>
> For a paper revision I'm trying to reproduce some results from an old lmer
> fit with Rv2.7.1 prior to 5/28/08. ?However, when I currently load Rv2.7.1
> and lmer, the variance component estimates are slightly different than the
> original fit; the sessionInfo() is as follows:
>
>> sessionInfo()
> R version 2.7.1 (2008-06-23)
> i386-apple-darwin8.10.1
>
> locale:
> en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] stats ? ? graphics ?grDevices utils ? ? datasets ?methods ? base
>
> other attached packages:
> [1] lme4_0.999375-24 ? Matrix_0.999375-11 lattice_0.17-8
>
> loaded via a namespace (and not attached):
> [1] grid_2.7.1 ?nlme_3.1-89
>
> Thus, I assume that I need to use the same older version of lme4 and/or
> Matrix which might be responsible for the difference in the results? ?If
> this is possible, how is this done?
>
> Cheers,
> David
>
> PS - for whatever it's worth, if I do the fit with lme (nlme_3.1-89) under
> Rv2.7.1 the results are closer to the original lmer results.
>
>
>
> ___________________________________________________
>
> Original lmer fit from 5/08:
> Model 2:
> AIC ?BIC logLik MLdeviance REMLdeviance
> ?2813 2843 ?-1397 ? ? ? 2829 ? ? ? ? 2795
> Random effects:
> ?Groups ? ? Name ? ? ? ? ? ?Variance Std.Dev. Corr
> ?subject ? (Intercept) ? ? ?2226.3 ? 47.183
> ? ? ? ? ? ?Drug ? ? ? ? ? ?2132.9 ? 46.184 ?-0.865
> ?Residual ? ? ? ? ? ? ? ? ? 13673.6 ?116.934
>
> Current lmer fit:
> ?AIC ?BIC logLik deviance REMLdev
> ?2815 2849 ?-1397 ? ? 2830 ? ?2795
> Random effects:
> ?Groups ? ? Name ? ? ? ? ? ? ? Variance Std.Dev. Corr
> ?Patient_no (Intercept) ? ? ? ? 2165.1 ? 46.531
> ? ? ? ? ? ?Drug.full.reverseC ?1386.3 ? 37.233 ?-1.000
> ?Residual ? ? ? ? ? ? ? ? ? ? ?13947.5 ?118.100

Notice the large change in the estimated correlation with very little
change in the log-likelihood or deviance.  This is an indication that
the model is over-specified.

Are you able to install R packages from the sources?  If so, you could
try the branches/allcoef version from the SVN archive.  On an
optimization problem like this it may be more successful in converging
to the global optimum instead of the local optimum.

This, by the way, is why I am always looking for better optimization
code to incorporate in R.  The code in the nlme and lme4 packages just
evaluates the log-likelihood or the REML criterion for the model at
the observed data and a proposed value of the parameters.  The actual
optimization is done by the nlminb optimizer which is based on very
old Fortran code written by David Gay.  Even though the code is old
this optimizer is, in my experience, more reliable than the optimizers
used by optim and by nlm.

It is surprisingly difficult to find good optimization code that is
covered by an open source license.  There is not a strong tradition of
open source code in the numerical analysis world.  Many users are
enthused about the ipopt library (projects.coin-or.org/Ipopt) but even
though that code is open source it depends on other software, some of
which is commercial.

The optimization in lme4 is minimization of a real-valued function of
real parameters, some of which are subject to non-negativity
constraints.  It is not an unconstrained optimization problem but the
constraints are very simple.  The objective function can be evaluated
and, in theory, the gradient can also be evaluated.  However, for
models with non-nested random effects evaluation of the gradient is
much, much more difficult and time consuming than is evaluation of the
objective function.  Thus the ideal optimizer would allow for simple
"box constraints" on the parameters and would be derivative-free or at
least allow for numeric evaluation of the gradient.  If anyone knows
of such code covered by a valid open-source license I would be
delighted to hear of it.

> Current lme fit:
> ? AIC ? ? ?BIC ? ?logLik
> ?2814.611 2848.638 -1397.305
>
> ? ? ? ? ? ? ? ? ? StdDev ? ?Corr
> (Intercept) ? ? ? ? 47.21031 (Intr)
> Drug.full.reverseC ?46.14014 -0.866
> Residual ? ? ? ? ? 116.93541
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From Richard.Silverwood at lshtm.ac.uk  Thu Feb 26 19:34:52 2009
From: Richard.Silverwood at lshtm.ac.uk (Richard.Silverwood at lshtm.ac.uk)
Date: Thu, 26 Feb 2009 18:34:52 +0000
Subject: [R-sig-ME] lmer/lmer2 & zero weights
Message-ID: <49A6E0CB.719B.00BB.0@lshtm.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090226/372f68c8/attachment.pl>

From bates at stat.wisc.edu  Thu Feb 26 20:59:43 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 26 Feb 2009 13:59:43 -0600
Subject: [R-sig-ME] lmer/lmer2 & zero weights
In-Reply-To: <49A6E0CB.719B.00BB.0@lshtm.ac.uk>
References: <49A6E0CB.719B.00BB.0@lshtm.ac.uk>
Message-ID: <40e66e0b0902261159m6673a62vf4e9f80575044b74@mail.gmail.com>

On Thu, Feb 26, 2009 at 12:34 PM,  <Richard.Silverwood at lshtm.ac.uk> wrote:
> Dear all,
>
> I simple query from a simple mind. Please humour me.
>
> I recently (possibly foolishly) updated my lme4 package. I have lots of code which calls lmer2. I understand that lmer2 is now deprecated so these calls produce an equivalent call to lmer. However, lmer2 accepted zero weights whilst lmer appears not to ("Error in lmerFrames(mc, formula, contrasts) : negative weights or weights of zero are not allowed"). Is this correct? Is there a reason for this difference?

To tell you the truth I'm not sure whether weights of zero in lmer
would cause problems.  It is fairly easy to find the piece of code
that checks for negative or zero weights and change it to just check
for negative weights then see what the effect is.  As the message
says, the check is in the (hidden) function named lmerFrames near the
end

    ## check weights and offset
    if (any(wts <= 0))
        stop(gettextf("negative weights or weights of zero are not allowed"))
    if(length(off) && length(off) != NROW(Y))
        stop(gettextf("number of offsets is %d should equal %d (number
of observations)",
                      length(off), NROW(Y)))



From DAfshartous at med.miami.edu  Thu Feb 26 21:11:00 2009
From: DAfshartous at med.miami.edu (Afshartous, David)
Date: Thu, 26 Feb 2009 15:11:00 -0500
Subject: [R-sig-ME] Reproducing results from an old lmer fit
In-Reply-To: <40e66e0b0902261017h62f8e13el7dc5dcbdb28a8e06@mail.gmail.com>
Message-ID: <C5CC6184.91A2%dafshartous@med.miami.edu>


I haven't installed R packages from the sources before.  Looking in the e-mail help archives, it appears that this is potentially problematic; do you have a suggested link for the entire procedure? For Windows I found a very brief description at http://win-builder.r-project.org (I think I might need more detail than this).  I can do it on
either Mac or Windows OS, is one easier than the other?

RE the branches/allcoef version you mention, I looked at https://svn.r-project.org/R/branches/ and didn't see this.

My main goal is to produce the exact results I produced before since I need to extract some of the information from this model that I hadn't saved.  I'm not sure which lmer version I had installed on 5/08.  Thus, I need to attempt to install the version that was available on 5/08 and perhaps the one preceding that, and check which one matches my results.



On 2/26/09 1:17 PM, "Douglas Bates" <bates at stat.wisc.edu> wrote:

On Thu, Feb 26, 2009 at 10:31 AM, Afshartous, David
<DAfshartous at med.miami.edu> wrote:
>
> All,
>
> For a paper revision I'm trying to reproduce some results from an old lmer
> fit with Rv2.7.1 prior to 5/28/08.  However, when I currently load Rv2.7.1
> and lmer, the variance component estimates are slightly different than the
> original fit; the sessionInfo() is as follows:
>
>> sessionInfo()
> R version 2.7.1 (2008-06-23)
> i386-apple-darwin8.10.1
>
> locale:
> en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-8
>
> loaded via a namespace (and not attached):
> [1] grid_2.7.1  nlme_3.1-89
>
> Thus, I assume that I need to use the same older version of lme4 and/or
> Matrix which might be responsible for the difference in the results?  If
> this is possible, how is this done?
>
> Cheers,
> David
>
> PS - for whatever it's worth, if I do the fit with lme (nlme_3.1-89) under
> Rv2.7.1 the results are closer to the original lmer results.
>
>
>
> ___________________________________________________
>
> Original lmer fit from 5/08:
> Model 2:
> AIC  BIC logLik MLdeviance REMLdeviance
>  2813 2843  -1397       2829         2795
> Random effects:
>  Groups     Name            Variance Std.Dev. Corr
>  subject   (Intercept)      2226.3   47.183
>            Drug            2132.9   46.184  -0.865
>  Residual                   13673.6  116.934
>
> Current lmer fit:
>  AIC  BIC logLik deviance REMLdev
>  2815 2849  -1397     2830    2795
> Random effects:
>  Groups     Name               Variance Std.Dev. Corr
>  Patient_no (Intercept)         2165.1   46.531
>            Drug.full.reverseC  1386.3   37.233  -1.000
>  Residual                      13947.5  118.100

Notice the large change in the estimated correlation with very little
change in the log-likelihood or deviance.  This is an indication that
the model is over-specified.

Are you able to install R packages from the sources?  If so, you could
try the branches/allcoef version from the SVN archive.  On an
optimization problem like this it may be more successful in converging
to the global optimum instead of the local optimum.


This, by the way, is why I am always looking for better optimization
code to incorporate in R.  The code in the nlme and lme4 packages just
evaluates the log-likelihood or the REML criterion for the model at
the observed data and a proposed value of the parameters.  The actual
optimization is done by the nlminb optimizer which is based on very
old Fortran code written by David Gay.  Even though the code is old
this optimizer is, in my experience, more reliable than the optimizers
used by optim and by nlm.

It is surprisingly difficult to find good optimization code that is
covered by an open source license.  There is not a strong tradition of
open source code in the numerical analysis world.  Many users are
enthused about the ipopt library (projects.coin-or.org/Ipopt) but even
though that code is open source it depends on other software, some of
which is commercial.

The optimization in lme4 is minimization of a real-valued function of
real parameters, some of which are subject to non-negativity
constraints.  It is not an unconstrained optimization problem but the
constraints are very simple.  The objective function can be evaluated
and, in theory, the gradient can also be evaluated.  However, for
models with non-nested random effects evaluation of the gradient is
much, much more difficult and time consuming than is evaluation of the
objective function.  Thus the ideal optimizer would allow for simple
"box constraints" on the parameters and would be derivative-free or at
least allow for numeric evaluation of the gradient.  If anyone knows
of such code covered by a valid open-source license I would be
delighted to hear of it.

> Current lme fit:
>   AIC      BIC    logLik
>  2814.611 2848.638 -1397.305
>
>                   StdDev    Corr
> (Intercept)         47.21031 (Intr)
> Drug.full.reverseC  46.14014 -0.866
> Residual           116.93541
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Thu Feb 26 21:43:29 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 26 Feb 2009 14:43:29 -0600
Subject: [R-sig-ME] [R] error message and convergence issues in fitting
	glmer in package lme4
In-Reply-To: <36FA2D4F89496341B64E8AABEB4CD14309E497CD37@sdc-mbx-01.exchange.washington.edu>
References: <36FA2D4F89496341B64E8AABEB4CD14309E497CD37@sdc-mbx-01.exchange.washington.edu>
Message-ID: <40e66e0b0902261243t1e24af5ep76595bb9523dbe83@mail.gmail.com>

On Thu, Feb 26, 2009 at 10:58 AM, Tanja Srebotnjak
<tanjas at u.washington.edu> wrote:
> I'm resending this message because I did not include a subject line in my first posting.

Also, it is generally more effective to send questions about
lmer/glmer to the R-SIG-Mixed-Models list, which I am cc:ing on this
reply.

>> Hello,

>> I'm trying to fit a generalized linear mixed model to estimate diabetes prevalence at US county level. To do this I'm using the glmer() function in package lme4. I can fit relatively simple models (i.e. few covariates) but when expanding the number of covariates I usually encounter the following error message.

>> gm8 <-
>> glmer(DIAB05F~AGE+as.factor(SEX)+poolt+poolx+poverty+fastfood+(1|as.factor(diab$fips)), family = binomial(link="logit"), data = diab, doFit=TRUE)

Error in validObject(.Object) : ? invalid class "mer" object: Slot Zt
must by dims['q'] ?by dims['n']*dims['s']

Getting that error message from this model is peculiar.  I couldn't
actually say what might be happening without trying the fit myself.  I
would suggest setting doFit = FALSE but I think that this error would
be encountered even with doFit = FALSE.  Again, it would be hard to
say exactly what is happening here.

>> In the above, the response is person-level diabetes status as a function of AGE=age, SEX=sex, poolt=average county diabetes prevalence for previous years, poolx=pooled county diabetes prevalence for counties with similar age, sex, race, and income structure, poverty=county poverty rate, fastfood=number of fastfood places per 100,000 people in the county, and a county random effect.

>> If I leave out fastfood, the model gets at least fitted - although it doesn't converge (yet):

The version of lmer currently under development tries to address that
problem.  The optimization of the parameter estimates is performed in
a slightly different way that will, I hope, provide smoother
convergence.  If your data are not restricted and you would be willing
to send me a copy of the diab data frame I could check what happens on
that version (or you could install the development version yourself
but that is a non-trivial undertaking).  If you can send the data the
best way to send it is to create an R data file as

save(diab, file = "diab.rda")

and send the file diab.rda

>> Warning message:
>> In mer_finalize(ans) : false convergence (8)

Frequently that is a sign of an overspecified model.

>>
>
>> I would be grateful for any advice on what the problem could be and how to resolve it.
>
>>
>
>> Thanks,
>
>> Tanja
>
>
> Tanja Srebotnjak, PhD, MSc, Dipl. Stat.
> Postgraduate Fellow
> Institute for Health Metrics and Evaluation
> University of Washington
> 2301 5th Ave, Suite 600
> Seattle, WA 98121
> Email: tanjas at u.washington.edu<mailto:tanjas at u.washington.edu>
> Tel: +1-206-897-2866
> www.healthmetricsandevaluation.org<http://www.healthmetricsandevaluation.org>
>
> From: Tanja Srebotnjak
> Sent: Thursday, February 26, 2009 12:17 AM
> To: 'r-help at r-project.org'
> Subject:
>
> Hello,
>
> I'm trying to fit a generalized linear mixed model to estimate diabetes prevalence at US county level. To do this I'm using the glmer() function in package lme4. I can fit relatively simple models (i.e. few covariates) but when expanding the number of covariates I usually encounter the following error message.
>
> gm8 <- glmer(DIAB05F~AGE+as.factor(SEX)+poolt+poolx+poverty+fastfood+(1|as.factor(diab$fips)), family = binomial(link="logit"), data = diab, doFit=TRUE)
> Error in validObject(.Object) :
> ?invalid class "mer" object: Slot Zt must by dims['q'] ?by dims['n']*dims['s']
>
> In the above, the response is person-level diabetes status as a function of AGE=age, SEX=sex, poolt=average county diabetes prevalence for previous years, poolx=pooled county diabetes prevalence for counties with similar age, sex, race, and income structure, poverty=county poverty rate, fastfood=number of fastfood places per 100,000 people in the county, and a county random effect.
>
> If I leave out fastfood, the model gets at least fitted - although it doesn't converge (yet):
>
> Warning message:
> In mer_finalize(ans) : false convergence (8)
>
> I would be grateful for any advice on what the problem could be and how to resolve it.
>
> Thanks,
> Tanja
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



From austin.frank at gmail.com  Thu Feb 26 22:05:50 2009
From: austin.frank at gmail.com (Austin Frank)
Date: Thu, 26 Feb 2009 16:05:50 -0500
Subject: [R-sig-ME] removing random slope effect from fitted values
Message-ID: <m0k57crhq7.fsf@urwireless-dhcp-128-151-21-108.wireless.rochester.edu>

Hello all!

I would like to look at the difference between predicted values using
the original models specification (using fitted()) and predicted values
based on all predictors except one.

Say I have a model like

  model <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)

And I want to look at the predicted values of the model if all
information about Days is excluded (not very interesting in this model,
I know).

If the normal predictions are

  ## taken from lmer.R
  Xbeta = model at X %*% model at fixef
  Zb = crossprod(model at Zt, model at ranef)@x
  Y = Xbeta + Zb

Then I think the fixed effects excluding the predictor of interest
should be

  predictorIndex = which(names(model at fixef) == "Days")
  Xbeta.hat = model at X[, -predictorIndex] %*% model at fixef[-predictorIndex]

But I can't figure out how to exclude a particular random slope term.
Part of the difficulty is that there might be more than one random slope
term for the same predictor if there are non-nested random effects in
the model.  I want to exclude the random slope term for the predictor of
interest from any random effect it appeared in.

I think that the best way to approach this might be to zero out the
relevant cells in the @Zt slot of the model, but I can't come up with a
good way to programmatically determine which cells need to be changed.
I've looked at lme4:::whichterms(), lme4:::reinds(), and
lme4:::whichreinds(), but those tell you where different grouping
factors start in @Zt, not where different random effects terms start
within a grouping factor.

Does anyone have any suggestions about how to remove a specific random
slope term from the Zb calculation above?


Thanks for any help,
/au

-- 
Austin Frank
http://aufrank.net
GPG Public Key (D7398C2F): http://aufrank.net/personal.asc



From austin.frank at gmail.com  Thu Feb 26 23:23:43 2009
From: austin.frank at gmail.com (Austin Frank)
Date: Thu, 26 Feb 2009 17:23:43 -0500
Subject: [R-sig-ME] lmer objects and mcmcsamp
References: <CCB6A086-2C52-42B3-AA14-555C687FF335@gmail.com>
Message-ID: <m0prh4et1s.fsf@urwireless-dhcp-128-151-21-108.wireless.rochester.edu>

Hugh--

Interesting question.  I just tried it out and replicated the report on
R version 2.8.1 (2008-12-22) powerpc-apple-darwin8.11.1 with
lme4_0.999375-28 and Matrix_0.999375-21.  I don't have the first idea
what's going on here, but I'm quite certain this question should also go
to R-sig-mixed-models, so I've copied that group in this response.

Thanks for the report,
/au

On Thu, Feb 26 2009, Hugh Rabagliati wrote:

> This is a bit of a general question, but it arose out of using the
> pvals.fnc function so I figured this forum might have a few ideas
> about it.
>
> The issue is whether there's a very odd bug in the mcmcsamp package of
> lme4? (or whether I still don't understand mcmc methods well enough).
>
> If I create a mer object using lmer, use it as an argument for
> mcmcsamp (sampling > 1 times), assign the output to a new mermcmc
> object and then examine my mer object again, I notice a rather
> peculiar thing. In particular, all of the variance/standard error
> terms change, as do the associated t values for the fixed effects.
> The estimated coefficients are unaffected. I figure this is a bug,
> because I can't see any reason why mcmcsamp would want to do this. I
> took a look through the code for mcmcsamp, but I don't speak C and
> nothing jumped out at me. Certainly, the code is only supposed to
> return a mermcmc object, so I have no idea why its messing with my
> mer.
>
> I've got this same result on a couple of different computers (all
> macs). It does, however, seem to be specific to either the version of
> lmer ( 0.999375-28) or of R (2.8.1) that I'm using. I tried it on an
> old PC version of R (2.5.1) using lme4 version 0.99875-9, and the
> same problems don't happen then. I've included the output from both
> the PC and mac versions below.
>
> Sage advice, comments, or confirmation that this is a known bug (I
> couldn't find mention of it elsewhere) would be welcome.
>
> Thanks very much,
>
> Hugh Rabagliati
>
>
> ########
> #
> #PC Code & output - R v. 2.5.1 & lme4 v. 0.99875-9.
> #
>   (fm1 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),
> sleepstudy))
> #Linear mixed-effects model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik MLdeviance REMLdeviance
> # 1752 1764 -871.8       1752         1744
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 627.508  25.0501
> # Subject  Days         35.858   5.9881
> # Residual             653.590  25.5654
> #number of obs: 180, groups: Subject, 18; Subject, 18
>
> #Fixed effects:
>  #           Estimate Std. Error t value
> #(Intercept)  251.405      6.885   36.51
> #Days          10.467      1.560    6.71
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.184
>
>  fm1  -> fm1.old
>   samp0 <- mcmcsamp(fm1, n = 1000)
>  fm1
> #Linear mixed-effects model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik MLdeviance REMLdeviance
> # 1752 1764 -871.8       1752         1744
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 627.508  25.0501
> # Subject  Days         35.858   5.9881
> # Residual             653.590  25.5654
> #number of obs: 180, groups: Subject, 18; Subject, 18
>
> #Fixed effects:
> #            Estimate Std. Error t value
> #(Intercept)  251.405      6.885   36.51
> #Days          10.467      1.560    6.71
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.184
>
> #
> # As you can see, the estimates don't change here
> #
> ###########
>
>
> #########
> #
> # Mac R v. 2.8.1, ?lme4? version 0.999375-28
> #
> # I make two mers, which should be identical (I make two because there
> are some assignment weirdnesses going on here too, #which I  haven't
> yet understood)
>
> (fm1.to_mcmc <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|
> Subject), sleepstudy))
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik deviance REMLdev
> # 1754 1770 -871.8     1752    1744
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 627.568  25.0513
> # Subject  Days         35.858   5.9882
> # Residual             653.584  25.5653
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> #            Estimate Std. Error t value
> #(Intercept)  251.405      6.885   36.51
> #Days          10.467      1.559    6.71
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.184
>
>
> (fm1.not_to_mcmc <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|
> Subject), sleepstudy))
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik deviance REMLdev
> # 1754 1770 -871.8     1752    1744
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 627.568  25.0513
> # Subject  Days         35.858   5.9882
> # Residual             653.584  25.5653
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> #            Estimate Std. Error t value
> #(Intercept)  251.405      6.885   36.51
> #Days          10.467      1.559    6.71
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.184
>
>
>  samp0 <- mcmcsamp(fm1.to_mcmc, n = 1000)
>  fm1.to_mcmc
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik deviance REMLdev
> # 1763 1779 -876.7     1761    1753
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 868.398  29.469
> # Subject  Days         49.619   7.044
>  #Residual             904.398  30.073
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> #            Estimate Std. Error t value
> #(Intercept)  251.405      5.260   47.79
> #Days          10.467      1.518    6.90
>
> # All the variances etc are different from before
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.343
>
> fm1.not_to_mcmc
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik deviance REMLdev
> # 1754 1770 -871.8     1752    1744
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 627.568  25.0513
> # Subject  Days         35.858   5.9882
> # Residual             653.584  25.5653
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> #            Estimate Std. Error t value
> #(Intercept)  251.405      6.885   36.51
> #Days          10.467      1.559    6.71
>
> # Variances are unaffected here
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.184

-- 
Austin Frank
http://aufrank.net
GPG Public Key (D7398C2F): http://aufrank.net/personal.asc



From hugh.rabagliati at gmail.com  Thu Feb 26 23:21:21 2009
From: hugh.rabagliati at gmail.com (Hugh Rabagliati)
Date: Thu, 26 Feb 2009 17:21:21 -0500
Subject: [R-sig-ME] lmer & mcmcsamp bug?
Message-ID: <52FD71CE-0D62-4B6E-8F86-397040958AC5@gmail.com>

Hi all,

Yesterday I noticed what I take to be a bug in the current version of  
lme4, and the folks over at the R-lang forum suggested checking in  
about it here.

If I create a mer object using lmer, use it as an argument for  
mcmcsamp (sampling > 1 times) and then examine my mer object again, I  
notice a rather peculiar thing. In particular, all of the variance/ 
standard error terms change, as do the associated t values for the  
fixed effects. The estimated coefficients are unaffected. I figure  
this is a bug, because I can't see any reason why mcmcsamp would want  
to do this. I took a look through the code for mcmcsamp, but I don't  
speak C and nothing jumped out at me. Certainly the function looks  
like its only meant to return a mermcmc object, so I have no idea why  
its messing with my mer.

I've got this same result on a couple of different computers (all  
macs). It does, however, seem to be specific to either the version of  
lmer ( 0.999375-28) or of R (2.8.1) that I'm using. I tried it on an  
old PC version of R (2.5.1) using lme4 version 0.99875-9, and the  
same problems don't happen then. I've included the output from both  
the PC and mac versions below.

Sage advice, comments, or confirmation that this is a known bug (I  
couldn't find mention of it elsewhere) would be welcome.

Thanks very much,

Hugh Rabagliati


########
#
#PC Code & output - R v. 2.5.1 & lme4 v. 0.99875-9.
#
   (fm1 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),  
sleepstudy))
#Linear mixed-effects model fit by REML
#Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
#   Data: sleepstudy
#  AIC  BIC logLik MLdeviance REMLdeviance
# 1752 1764 -871.8       1752         1744
#Random effects:
# Groups   Name        Variance Std.Dev.
# Subject  (Intercept) 627.508  25.0501
# Subject  Days         35.858   5.9881
# Residual             653.590  25.5654
#number of obs: 180, groups: Subject, 18; Subject, 18

#Fixed effects:
  #           Estimate Std. Error t value
#(Intercept)  251.405      6.885   36.51
#Days          10.467      1.560    6.71

#Correlation of Fixed Effects:
#     (Intr)
#Days -0.184

  fm1  -> fm1.old
   samp0 <- mcmcsamp(fm1, n = 1000)
  fm1
#Linear mixed-effects model fit by REML
#Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
#   Data: sleepstudy
#  AIC  BIC logLik MLdeviance REMLdeviance
# 1752 1764 -871.8       1752         1744
#Random effects:
# Groups   Name        Variance Std.Dev.
# Subject  (Intercept) 627.508  25.0501
# Subject  Days         35.858   5.9881
# Residual             653.590  25.5654
#number of obs: 180, groups: Subject, 18; Subject, 18

#Fixed effects:
#            Estimate Std. Error t value
#(Intercept)  251.405      6.885   36.51
#Days          10.467      1.560    6.71

#Correlation of Fixed Effects:
#     (Intr)
#Days -0.184

#
# As you can see, the estimates don't change here
#
###########


#########
#
# Mac R v. 2.8.1, ?lme4? version 0.999375-28
#
# I make two mers, which should be identical (I make two because  
there are some assignment weirdnesses going on here too, #which I  
haven't yet understood)

(fm1.to_mcmc <- lmer(Reaction ~ Days + (1|Subject) + (0+Days| 
Subject), sleepstudy))

#Linear mixed model fit by REML
#Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
#   Data: sleepstudy
#  AIC  BIC logLik deviance REMLdev
# 1754 1770 -871.8     1752    1744
#Random effects:
# Groups   Name        Variance Std.Dev.
# Subject  (Intercept) 627.568  25.0513
# Subject  Days         35.858   5.9882
# Residual             653.584  25.5653
#Number of obs: 180, groups: Subject, 18

#Fixed effects:
#            Estimate Std. Error t value
#(Intercept)  251.405      6.885   36.51
#Days          10.467      1.559    6.71

#Correlation of Fixed Effects:
#     (Intr)
#Days -0.184


(fm1.not_to_mcmc <- lmer(Reaction ~ Days + (1|Subject) + (0+Days| 
Subject), sleepstudy))

#Linear mixed model fit by REML
#Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
#   Data: sleepstudy
#  AIC  BIC logLik deviance REMLdev
# 1754 1770 -871.8     1752    1744
#Random effects:
# Groups   Name        Variance Std.Dev.
# Subject  (Intercept) 627.568  25.0513
# Subject  Days         35.858   5.9882
# Residual             653.584  25.5653
#Number of obs: 180, groups: Subject, 18

#Fixed effects:
#            Estimate Std. Error t value
#(Intercept)  251.405      6.885   36.51
#Days          10.467      1.559    6.71

#Correlation of Fixed Effects:
#     (Intr)
#Days -0.184


  samp0 <- mcmcsamp(fm1.to_mcmc, n = 1000)
  fm1.to_mcmc

#Linear mixed model fit by REML
#Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
#   Data: sleepstudy
#  AIC  BIC logLik deviance REMLdev
# 1763 1779 -876.7     1761    1753
#Random effects:
# Groups   Name        Variance Std.Dev.
# Subject  (Intercept) 868.398  29.469
# Subject  Days         49.619   7.044
  #Residual             904.398  30.073
#Number of obs: 180, groups: Subject, 18

#Fixed effects:
#            Estimate Std. Error t value
#(Intercept)  251.405      5.260   47.79
#Days          10.467      1.518    6.90

# All the variances etc are different from before

#Correlation of Fixed Effects:
#     (Intr)
#Days -0.343

fm1.not_to_mcmc

#Linear mixed model fit by REML
#Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
#   Data: sleepstudy
#  AIC  BIC logLik deviance REMLdev
# 1754 1770 -871.8     1752    1744
#Random effects:
# Groups   Name        Variance Std.Dev.
# Subject  (Intercept) 627.568  25.0513
# Subject  Days         35.858   5.9882
# Residual             653.584  25.5653
#Number of obs: 180, groups: Subject, 18

#Fixed effects:
#            Estimate Std. Error t value
#(Intercept)  251.405      6.885   36.51
#Days          10.467      1.559    6.71

# Variances are unaffected here

#Correlation of Fixed Effects:
#     (Intr)
#Days -0.184



From bates at stat.wisc.edu  Thu Feb 26 23:51:58 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 26 Feb 2009 16:51:58 -0600
Subject: [R-sig-ME] lmer & mcmcsamp bug?
In-Reply-To: <52FD71CE-0D62-4B6E-8F86-397040958AC5@gmail.com>
References: <52FD71CE-0D62-4B6E-8F86-397040958AC5@gmail.com>
Message-ID: <40e66e0b0902261451w42eeff2dwe6f2c1588cf01844@mail.gmail.com>

On Thu, Feb 26, 2009 at 4:21 PM, Hugh Rabagliati
<hugh.rabagliati at gmail.com> wrote:
> Hi all,
>
> Yesterday I noticed what I take to be a bug in the current version of lme4,
> and the folks over at the R-lang forum suggested checking in about it here.
>
> If I create a mer object using lmer, use it as an argument for mcmcsamp
> (sampling > 1 times) and then examine my mer object again, I notice a rather
> peculiar thing. In particular, all of the variance/standard error terms
> change, as do the associated t values for the fixed effects. The estimated
> coefficients are unaffected. I figure this is a bug, because I can't see any
> reason why mcmcsamp would want to do this. I took a look through the code
> for mcmcsamp, but I don't speak C and nothing jumped out at me. Certainly
> the function looks like its only meant to return a mermcmc object, so I have
> no idea why its messing with my mer.
>
> I've got this same result on a couple of different computers (all macs). It
> does, however, seem to be specific to either the version of lmer (
> 0.999375-28) or of R (2.8.1) that I'm using. I tried it on an old PC version
> of R (2.5.1) using lme4 version 0.99875-9, and the same problems don't
> happen then. I've included the output from both the PC and mac versions
> below.
>
> Sage advice, comments, or confirmation that this is a known bug (I couldn't
> find mention of it elsewhere) would be welcome.

Well, it's a bug.

The C code called by mcmcsamp does something naughty - it changes the
value of slots of the fitted model object in place.  I plead the usual
excuse for such inexcusable behavior: efficiency.  If one copies the
whole fitted model object at each step in the MCMC iterations the
function would only be applicable to small models and would take
forever, even on those.  (Actually I guess such a statement further
makes me guilty of Knuth's "root of all evil" - premature optimization
- because I haven't actually checked that.)

The code at the end of the C function mer_MCMCsamp is supposed to
restore the original values.  I guess some "infelicities", as Bill
Venables calls them, must have crept in.  I'll take a look.

> ########
> #
> #PC Code & output - R v. 2.5.1 & lme4 v. 0.99875-9.
> #
> ?(fm1 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy))
> #Linear mixed-effects model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> # ? Data: sleepstudy
> # ?AIC ?BIC logLik MLdeviance REMLdeviance
> # 1752 1764 -871.8 ? ? ? 1752 ? ? ? ? 1744
> #Random effects:
> # Groups ? Name ? ? ? ?Variance Std.Dev.
> # Subject ?(Intercept) 627.508 ?25.0501
> # Subject ?Days ? ? ? ? 35.858 ? 5.9881
> # Residual ? ? ? ? ? ? 653.590 ?25.5654
> #number of obs: 180, groups: Subject, 18; Subject, 18
>
> #Fixed effects:
> ?# ? ? ? ? ? Estimate Std. Error t value
> #(Intercept) ?251.405 ? ? ?6.885 ? 36.51
> #Days ? ? ? ? ?10.467 ? ? ?1.560 ? ?6.71
>
> #Correlation of Fixed Effects:
> # ? ? (Intr)
> #Days -0.184
>
> ?fm1 ?-> fm1.old
> ?samp0 <- mcmcsamp(fm1, n = 1000)
> ?fm1
> #Linear mixed-effects model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> # ? Data: sleepstudy
> # ?AIC ?BIC logLik MLdeviance REMLdeviance
> # 1752 1764 -871.8 ? ? ? 1752 ? ? ? ? 1744
> #Random effects:
> # Groups ? Name ? ? ? ?Variance Std.Dev.
> # Subject ?(Intercept) 627.508 ?25.0501
> # Subject ?Days ? ? ? ? 35.858 ? 5.9881
> # Residual ? ? ? ? ? ? 653.590 ?25.5654
> #number of obs: 180, groups: Subject, 18; Subject, 18
>
> #Fixed effects:
> # ? ? ? ? ? ?Estimate Std. Error t value
> #(Intercept) ?251.405 ? ? ?6.885 ? 36.51
> #Days ? ? ? ? ?10.467 ? ? ?1.560 ? ?6.71
>
> #Correlation of Fixed Effects:
> # ? ? (Intr)
> #Days -0.184
>
> #
> # As you can see, the estimates don't change here
> #
> ###########
>
>
> #########
> #
> # Mac R v. 2.8.1, ?lme4? version 0.999375-28
> #
> # I make two mers, which should be identical (I make two because there are
> some assignment weirdnesses going on here too, #which I haven't yet
> understood)
>
> (fm1.to_mcmc <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),
> sleepstudy))
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> # ? Data: sleepstudy
> # ?AIC ?BIC logLik deviance REMLdev
> # 1754 1770 -871.8 ? ? 1752 ? ?1744
> #Random effects:
> # Groups ? Name ? ? ? ?Variance Std.Dev.
> # Subject ?(Intercept) 627.568 ?25.0513
> # Subject ?Days ? ? ? ? 35.858 ? 5.9882
> # Residual ? ? ? ? ? ? 653.584 ?25.5653
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> # ? ? ? ? ? ?Estimate Std. Error t value
> #(Intercept) ?251.405 ? ? ?6.885 ? 36.51
> #Days ? ? ? ? ?10.467 ? ? ?1.559 ? ?6.71
>
> #Correlation of Fixed Effects:
> # ? ? (Intr)
> #Days -0.184
>
>
> (fm1.not_to_mcmc <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),
> sleepstudy))
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> # ? Data: sleepstudy
> # ?AIC ?BIC logLik deviance REMLdev
> # 1754 1770 -871.8 ? ? 1752 ? ?1744
> #Random effects:
> # Groups ? Name ? ? ? ?Variance Std.Dev.
> # Subject ?(Intercept) 627.568 ?25.0513
> # Subject ?Days ? ? ? ? 35.858 ? 5.9882
> # Residual ? ? ? ? ? ? 653.584 ?25.5653
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> # ? ? ? ? ? ?Estimate Std. Error t value
> #(Intercept) ?251.405 ? ? ?6.885 ? 36.51
> #Days ? ? ? ? ?10.467 ? ? ?1.559 ? ?6.71
>
> #Correlation of Fixed Effects:
> # ? ? (Intr)
> #Days -0.184
>
>
> ?samp0 <- mcmcsamp(fm1.to_mcmc, n = 1000)
> ?fm1.to_mcmc
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> # ? Data: sleepstudy
> # ?AIC ?BIC logLik deviance REMLdev
> # 1763 1779 -876.7 ? ? 1761 ? ?1753
> #Random effects:
> # Groups ? Name ? ? ? ?Variance Std.Dev.
> # Subject ?(Intercept) 868.398 ?29.469
> # Subject ?Days ? ? ? ? 49.619 ? 7.044
> ?#Residual ? ? ? ? ? ? 904.398 ?30.073
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> # ? ? ? ? ? ?Estimate Std. Error t value
> #(Intercept) ?251.405 ? ? ?5.260 ? 47.79
> #Days ? ? ? ? ?10.467 ? ? ?1.518 ? ?6.90
>
> # All the variances etc are different from before
>
> #Correlation of Fixed Effects:
> # ? ? (Intr)
> #Days -0.343
>
> fm1.not_to_mcmc
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> # ? Data: sleepstudy
> # ?AIC ?BIC logLik deviance REMLdev
> # 1754 1770 -871.8 ? ? 1752 ? ?1744
> #Random effects:
> # Groups ? Name ? ? ? ?Variance Std.Dev.
> # Subject ?(Intercept) 627.568 ?25.0513
> # Subject ?Days ? ? ? ? 35.858 ? 5.9882
> # Residual ? ? ? ? ? ? 653.584 ?25.5653
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> # ? ? ? ? ? ?Estimate Std. Error t value
> #(Intercept) ?251.405 ? ? ?6.885 ? 36.51
> #Days ? ? ? ? ?10.467 ? ? ?1.559 ? ?6.71
>
> # Variances are unaffected here
>
> #Correlation of Fixed Effects:
> # ? ? (Intr)
> #Days -0.184
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From S.Ellison at lgc.co.uk  Fri Feb 27 00:47:17 2009
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Thu, 26 Feb 2009 23:47:17 +0000
Subject: [R-sig-ME] lmer & mcmcsamp bug?
Message-ID: <s9a72a10.032@tedmail.lgc.co.uk>

This behaviour has also been implicated - correctly or otherwise I can't say - in the 'negative PWRSS' question reported previously (http://www.nabble.com/Problem-with-aovlmer.fnc-in-languageR-td20706128.html, http://www.nabble.com/Re%3A-Problem-with-aovlmer.fnc-in-languageR-p21365322.html), so a fix may kill two birds with one stone. That would be most welcome!

Steve Ellison


>>> Douglas Bates <bates at stat.wisc.edu> 02/26/09 10:51 PM >>>
On Thu, Feb 26, 2009 at 4:21 PM, Hugh Rabagliati
<hugh.rabagliati at gmail.com> wrote:
> Hi all,
>
> Yesterday I noticed what I take to be a bug in the current version of lme4,
> and the folks over at the R-lang forum suggested checking in about it here.
>
> If I create a mer object using lmer, use it as an argument for mcmcsamp
> (sampling > 1 times) and then examine my mer object again, I notice a rather
> peculiar thing. In particular, all of the variance/standard error terms
> change, as do the associated t values for the fixed effects. The estimated
> coefficients are unaffected. I figure this is a bug, because I can't see any
> reason why mcmcsamp would want to do this. I took a look through the code
> for mcmcsamp, but I don't speak C and nothing jumped out at me. Certainly
> the function looks like its only meant to return a mermcmc object, so I have
> no idea why its messing with my mer.
>
> I've got this same result on a couple of different computers (all macs). It
> does, however, seem to be specific to either the version of lmer (
> 0.999375-28) or of R (2.8.1) that I'm using. I tried it on an old PC version
> of R (2.5.1) using lme4 version 0.99875-9, and the same problems don't
> happen then. I've included the output from both the PC and mac versions
> below.
>
> Sage advice, comments, or confirmation that this is a known bug (I couldn't
> find mention of it elsewhere) would be welcome.

Well, it's a bug.

The C code called by mcmcsamp does something naughty - it changes the
value of slots of the fitted model object in place.  I plead the usual
excuse for such inexcusable behavior: efficiency.  If one copies the
whole fitted model object at each step in the MCMC iterations the
function would only be applicable to small models and would take
forever, even on those.  (Actually I guess such a statement further
makes me guilty of Knuth's "root of all evil" - premature optimization
- because I haven't actually checked that.)

The code at the end of the C function mer_MCMCsamp is supposed to
restore the original values.  I guess some "infelicities", as Bill
Venables calls them, must have crept in.  I'll take a look.

> ########
> #
> #PC Code & output - R v. 2.5.1 & lme4 v. 0.99875-9.
> #
>  (fm1 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy))
> #Linear mixed-effects model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik MLdeviance REMLdeviance
> # 1752 1764 -871.8       1752         1744
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 627.508  25.0501
> # Subject  Days         35.858   5.9881
> # Residual             653.590  25.5654
> #number of obs: 180, groups: Subject, 18; Subject, 18
>
> #Fixed effects:
>  #           Estimate Std. Error t value
> #(Intercept)  251.405      6.885   36.51
> #Days          10.467      1.560    6.71
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.184
>
>  fm1  -> fm1.old
>  samp0 <- mcmcsamp(fm1, n = 1000)
>  fm1
> #Linear mixed-effects model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik MLdeviance REMLdeviance
> # 1752 1764 -871.8       1752         1744
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 627.508  25.0501
> # Subject  Days         35.858   5.9881
> # Residual             653.590  25.5654
> #number of obs: 180, groups: Subject, 18; Subject, 18
>
> #Fixed effects:
> #            Estimate Std. Error t value
> #(Intercept)  251.405      6.885   36.51
> #Days          10.467      1.560    6.71
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.184
>
> #
> # As you can see, the estimates don't change here
> #
> ###########
>
>
> #########
> #
> # Mac R v. 2.8.1, ?lme4? version 0.999375-28
> #
> # I make two mers, which should be identical (I make two because there are
> some assignment weirdnesses going on here too, #which I haven't yet
> understood)
>
> (fm1.to_mcmc <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),
> sleepstudy))
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik deviance REMLdev
> # 1754 1770 -871.8     1752    1744
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 627.568  25.0513
> # Subject  Days         35.858   5.9882
> # Residual             653.584  25.5653
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> #            Estimate Std. Error t value
> #(Intercept)  251.405      6.885   36.51
> #Days          10.467      1.559    6.71
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.184
>
>
> (fm1.not_to_mcmc <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),
> sleepstudy))
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik deviance REMLdev
> # 1754 1770 -871.8     1752    1744
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 627.568  25.0513
> # Subject  Days         35.858   5.9882
> # Residual             653.584  25.5653
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> #            Estimate Std. Error t value
> #(Intercept)  251.405      6.885   36.51
> #Days          10.467      1.559    6.71
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.184
>
>
>  samp0 <- mcmcsamp(fm1.to_mcmc, n = 1000)
>  fm1.to_mcmc
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik deviance REMLdev
> # 1763 1779 -876.7     1761    1753
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 868.398  29.469
> # Subject  Days         49.619   7.044
>  #Residual             904.398  30.073
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> #            Estimate Std. Error t value
> #(Intercept)  251.405      5.260   47.79
> #Days          10.467      1.518    6.90
>
> # All the variances etc are different from before
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.343
>
> fm1.not_to_mcmc
>
> #Linear mixed model fit by REML
> #Formula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)
> #   Data: sleepstudy
> #  AIC  BIC logLik deviance REMLdev
> # 1754 1770 -871.8     1752    1744
> #Random effects:
> # Groups   Name        Variance Std.Dev.
> # Subject  (Intercept) 627.568  25.0513
> # Subject  Days         35.858   5.9882
> # Residual             653.584  25.5653
> #Number of obs: 180, groups: Subject, 18
>
> #Fixed effects:
> #            Estimate Std. Error t value
> #(Intercept)  251.405      6.885   36.51
> #Days          10.467      1.559    6.71
>
> # Variances are unaffected here
>
> #Correlation of Fixed Effects:
> #     (Intr)
> #Days -0.184
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}



From DAfshartous at med.miami.edu  Fri Feb 27 20:28:50 2009
From: DAfshartous at med.miami.edu (Afshartous, David)
Date: Fri, 27 Feb 2009 14:28:50 -0500
Subject: [R-sig-ME] Reproducing results from an old lmer fit
In-Reply-To: <40e66e0b0902261017h62f8e13el7dc5dcbdb28a8e06@mail.gmail.com>
Message-ID: <C5CDA922.91E4%dafshartous@med.miami.edu>


I was actually able to install the old lme4 version and did indeed produce my previous results.  For those interested, below is a summary of the steps I followed.   There is more detailed information at http://cran.r-project.org/doc/manuals/R-admin.html#The-Windows-toolset for those that want to build R as well as packages, but for those only interested in packages, perhaps the instructions below well be more useful.  (thanks to Henrik Parn for several useful pointers).

1) The Toolset
Install the R Toolset from http://www.murdoch-sutherland.com/Rtools/
I installed it directly under C:\

Although the instructions during installation mention that there are several remaining tasks to complete installation, it appears that these are only necessary if one wants to build R in addition to just add-on packages.

2) The old version of the add-on package
I downloaded the old version of lme4 that I needed from http://cran.r-project.org/src/contrib/Archive/lme4/

I put it under C:\Documents and Settings\parn\Desktop\lme4_0.99875-9.tar.gz" and did NOT unpack it.

3) Setting the PATH:
I added the following to the beginning of my PATH variable:
C:\Rtools\bin;C:\Rtools\perl\bin;C:\Rtools\MinGW\bin;C:\R\R-2.7.1\bin;

I didn't add the other parts mentioned at http://cran.r-project.org/doc/manuals/R-admin.html#The-Windows-toolset.
My entire path was now thus:
C:\Rtools\bin;C:\Rtools\perl\bin;C:\Rtools\MinGW\bin;C:\R\R-2.7.1\bin;C:\Program Files\MiKTeX 2.6\miktex\bin;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;C:\Program Files\WinLD;

I noticed also that the change in the path was not dynamic, i.e., for it to take effect I had to exit the DOS prompt and re-enter the DOS prompt, then type "path" at the DOS prompt to see make sure that it had taken effect.

4) Deleting old version of lme4:
I deleted the entire folder of the unwanted version of lme4, on my machine located at C:\R\R-2.7.1\library

5) R CMD
At the DOS prompt, I typed:
R CMD "C:\Documents and Settings\parn\Desktop\lme4_0.99385-9.tar.gz"

A 'new' lme4-folder with all its contents was created 'automatically' in
the appropriate place, i.e. here: C:\R\R-2.8.1\library. I did not specify this target myself.  Thus, I could load the package from the menu interface after starting R.



On 2/26/09 1:17 PM, "Douglas Bates" <bates at stat.wisc.edu> wrote:

On Thu, Feb 26, 2009 at 10:31 AM, Afshartous, David
<DAfshartous at med.miami.edu> wrote:
>
> All,
>
> For a paper revision I'm trying to reproduce some results from an old lmer
> fit with Rv2.7.1 prior to 5/28/08.  However, when I currently load Rv2.7.1
> and lmer, the variance component estimates are slightly different than the
> original fit; the sessionInfo() is as follows:
>
>> sessionInfo()
> R version 2.7.1 (2008-06-23)
> i386-apple-darwin8.10.1
>
> locale:
> en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lme4_0.999375-24   Matrix_0.999375-11 lattice_0.17-8
>
> loaded via a namespace (and not attached):
> [1] grid_2.7.1  nlme_3.1-89
>
> Thus, I assume that I need to use the same older version of lme4 and/or
> Matrix which might be responsible for the difference in the results?  If
> this is possible, how is this done?
>
> Cheers,
> David
>
> PS - for whatever it's worth, if I do the fit with lme (nlme_3.1-89) under
> Rv2.7.1 the results are closer to the original lmer results.
>
>
>
> ___________________________________________________
>
> Original lmer fit from 5/08:
> Model 2:
> AIC  BIC logLik MLdeviance REMLdeviance
>  2813 2843  -1397       2829         2795
> Random effects:
>  Groups     Name            Variance Std.Dev. Corr
>  subject   (Intercept)      2226.3   47.183
>            Drug            2132.9   46.184  -0.865
>  Residual                   13673.6  116.934
>
> Current lmer fit:
>  AIC  BIC logLik deviance REMLdev
>  2815 2849  -1397     2830    2795
> Random effects:
>  Groups     Name               Variance Std.Dev. Corr
>  Patient_no (Intercept)         2165.1   46.531
>            Drug.full.reverseC  1386.3   37.233  -1.000
>  Residual                      13947.5  118.100

Notice the large change in the estimated correlation with very little
change in the log-likelihood or deviance.  This is an indication that
the model is over-specified.

Are you able to install R packages from the sources?  If so, you could
try the branches/allcoef version from the SVN archive.  On an
optimization problem like this it may be more successful in converging
to the global optimum instead of the local optimum.

This, by the way, is why I am always looking for better optimization
code to incorporate in R.  The code in the nlme and lme4 packages just
evaluates the log-likelihood or the REML criterion for the model at
the observed data and a proposed value of the parameters.  The actual
optimization is done by the nlminb optimizer which is based on very
old Fortran code written by David Gay.  Even though the code is old
this optimizer is, in my experience, more reliable than the optimizers
used by optim and by nlm.

It is surprisingly difficult to find good optimization code that is
covered by an open source license.  There is not a strong tradition of
open source code in the numerical analysis world.  Many users are
enthused about the ipopt library (projects.coin-or.org/Ipopt) but even
though that code is open source it depends on other software, some of
which is commercial.

The optimization in lme4 is minimization of a real-valued function of
real parameters, some of which are subject to non-negativity
constraints.  It is not an unconstrained optimization problem but the
constraints are very simple.  The objective function can be evaluated
and, in theory, the gradient can also be evaluated.  However, for
models with non-nested random effects evaluation of the gradient is
much, much more difficult and time consuming than is evaluation of the
objective function.  Thus the ideal optimizer would allow for simple
"box constraints" on the parameters and would be derivative-free or at
least allow for numeric evaluation of the gradient.  If anyone knows
of such code covered by a valid open-source license I would be
delighted to hear of it.

> Current lme fit:
>   AIC      BIC    logLik
>  2814.611 2848.638 -1397.305
>
>                   StdDev    Corr
> (Intercept)         47.21031 (Intr)
> Drug.full.reverseC  46.14014 -0.866
> Residual           116.93541
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Fri Feb 27 23:17:02 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 27 Feb 2009 16:17:02 -0600
Subject: [R-sig-ME] [R] error message and convergence issues in fitting
	glmer in package lme4
In-Reply-To: <36FA2D4F89496341B64E8AABEB4CD14309E4B16288@sdc-mbx-01.exchange.washington.edu>
References: <36FA2D4F89496341B64E8AABEB4CD14309E497CD37@sdc-mbx-01.exchange.washington.edu>
	<40e66e0b0902261243t1e24af5ep76595bb9523dbe83@mail.gmail.com>
	<36FA2D4F89496341B64E8AABEB4CD14309E4B16288@sdc-mbx-01.exchange.washington.edu>
Message-ID: <40e66e0b0902271417q7e7ac0b8q7ac5c9d7fcdcbc6d@mail.gmail.com>

On Fri, Feb 27, 2009 at 1:28 PM, Tanja Srebotnjak
<tanjas at u.washington.edu> wrote:
> Dear Prof. Bates,

> Thank you for your prompt response. And yes, it would be great if you wanted to look at the data.

> Yesterday, I continued working on the data and the model. When I changed the specification of the random effect to (1|fips) instead of (1|as.factor(diab$fips)), ?I seem to have got rid of the error message (at least I could fit models that I couldn't previously...):

> Error in validObject(.Object) : ? invalid class "mer" object: Slot Zt
> must by dims['q'] ?by dims['n']*dims['s']

Of course. I should have noticed that.  The problem is that diab$fibs
can be different from fibs if you have missing data in one of the
covariates.  I expect that there are some missing values in the
fastfood variable.  If you create the model matrix for the random
effects (this matrix is sometimes written as Z and the Zt slot is the
transpose of that matrix) based on the original fips variable then it
will not have the same size as the model matrix created from the
variable in the model frame.



From bates at stat.wisc.edu  Sat Feb 28 19:25:16 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 28 Feb 2009 12:25:16 -0600
Subject: [R-sig-ME] [R] lme4 and Variable level detection
In-Reply-To: <22262944.post@talk.nabble.com>
References: <22262944.post@talk.nabble.com>
Message-ID: <40e66e0b0902281025p5318a0c4o565e6b5038c183a1@mail.gmail.com>

On Sat, Feb 28, 2009 at 9:00 AM, Jeroen Ooms <j.c.l.ooms at uu.nl> wrote:

> I am making a little GUI for lme4, and I was wondering if there is a function
> that automatically detects on which level every variable exists.
> Furtheremore I got kind of confused about what a random effects model
> actually calculates.

Questions such as this may be answered more quickly if you send them
to the R-SIG-Mixed-Models mailing list, which I am cc:ing on this
reply.

> I have some experience with commercial software packages for multilevel
> analysis, like HLM6, and I was surprised that lme4 does not require the user
> to specify the level for every predictor variable. Is this because the
> function automatically detects the level by testing on which levels the
> predictor has variance, or is this information simply not needed?

In some ways, exposure to software like HLM or MLWin can be more of a
hindrance than a help when learning about mixed models.  In
presentation of the model and in the software itself these packages
emphasize "levels" of random effects leading to the impression that we
can only associate random effects with factors that are nested.  This
is a misconception.  There are many cases where is it eminently
sensible to associate random effects with factors that are completely
crossed ('subject' and 'item' are a prime example) or partially
crossed.  The archetypal example used in multilevel modeling,
achievement scores on students nested in classes nested in schools
nested in ..., becomes partially crossed when we track students over
time and they move from class to class or school to school.

I imagine that the reason for defining the model in terms of nested
factors for random effects is computational.  If you insist that the
random effects must always be defined with respect to nested factors
then you can employ methods that take advantage of this, with
considerable simplification in the storage and computational burden.
The lme4 package adopts a different approach based on sparse matrix
storage and decomposition methods.  It turns out that these methods
are competitive with the best methods for models based on nested
factors, in the cases to which they apply, and these methods allow for
fitting much more general models.

An unfortunate side-effect of the emphasis on levels in MLWin and HLM
is the perception that other covariates must be characterized by the
level at which they vary, even if these covariates only determine
fixed-effects parameters.  This is quite untrue and misleading.  The
only constraints on the covariates and the model matrix for the
fixed-effects parameters is that the model matrix must be of full
column rank.  In models that define random effects for slopes, or in
general for the coefficients associated with a covariate, the
constraint is that the covariate cannot be constant within each level
of the grouping factor of the random effect.  For example, we cannot
estimate a random effect for the coefficients for sex (M/F) within
subject (assuming we do not have transgender people in the study).

My advice would be to avoid phrasing the model in terms of levels of
random effects.  Although I realize that those with a background of
using MLWin or HLM may find this more comfortable, I think it would be
propagating bad practices and misconceptions.

> I was taught that a crosslevel interaction predicts the regression
> coefficient of the lower level variable, which is also what is implied by
> the HLM gui. However, in an lme4 formula, a crosslevel interaction has the
> same syntax as a regular interaction term. Furthermore, lme4 also allows
> adding crosslevel interactions without a random slope for the lower level
> variable. Now I'm confused. Is there a fundamental difference between a
> crosslevel interaction, or is the same thing as a regular interaction when
> the model also holds an error term for the lower level variable?
>
>
>
>
> -----
> Jeroen Ooms * Dept. of Methodology and Statistics * Utrecht University
>
> Visit ?http://www.jeroenooms.com www.jeroenooms.com ?to explore some of my
> current projects.
>
>
>
>
>
>
> --
> View this message in context: http://www.nabble.com/lme4-and-Variable-level-detection-tp22262944p22262944.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



From burg4401 at uni-trier.de  Sun Mar  1 13:16:11 2009
From: burg4401 at uni-trier.de (burg4401 at uni-trier.de)
Date: Sun, 1 Mar 2009 13:16:11 +0100
Subject: [R-sig-ME] prediction for glmer objects
Message-ID: <200903011316.11921.burg4401@uni-trier.de>

Dear list-users,

in my Diploma-Theses I have the need for predicting new Y from a fitted object 
on a different dataset. Essentially what i want to do is:

	Y ~ X\beta +Zb
	and using the \beta and the variance Components of the random effects to get
	Ynew ~Xnew\beta + Znew rnorm(b,0,sd(b))

I hacked myself a bit through the (genious) code of lme4 and wanted to ask 
whether my code really does what I need. The outcoming vectors have quite 
reasonable probablitys. Binomial and poisson are the familys I use. 
I'm not interested in EBLUBS nor random slopes models. Just the fixed effects 
and the variances of the random (intercept) effects.

my code is:
prediction<-function(model,data,npred){
  require(lme4)
#  The function gl is an internal function of lme4
  gl <-function(formula, data, family = gaussian, start = NULL,
		  verbose = FALSE, nAGQ = 1, doFit = FALSE, subset, weights,
		  na.action, offset, contrasts = NULL, model = TRUE,
		  control = list(), ...){
		  mc <- match.call()
		  fr <- lme4:::lmerFrames(mc, formula, contrasts) # model frame, X, etc.
		  lme4:::lmerFactorList(formula, fr, 0L, 0L) # flist, Zt, dims
  }
  beta<-fixef(model)
  X <- model.matrix(terms(model), data=data)
  fix<-X%*%beta
  rm(beta,X)
  gc()
  gc()
 calllist<-
list(family=model at call$family,formula=model at call$formula,data=data,doFit=FALSE,verbose=T)
  tmp<-do.call(gl,calllist)
  Zt<-lme4:::mkZt(tmp,NULL)
  rm(tmp,calllist)
  gc()
  if(missing(npred)){
	raneff<-numeric()
	for(i in 1:(length(Zt$Gp)-1)){
		  raneff<-c(raneff,rnorm(Zt$Gp[i+1]-Zt$Gp[i],mean=0,sd=model at ST[[i]][1]))
	}
	ran<-crossprod(Zt$Zt,raneff)
	return(as.numeric(ran)+as.numeric(fix))
  }else{
	ranff<-matrix(NA,nrow=mult,ncol=length(data[,1]))
	for(j in 1:npred){
	  raneff<-numeric()
	  for(i in 1:(length(Zt$Gp)-1)){
		  raneff<-c(raneff,rnorm(Zt$Gp[i+1]-Zt$Gp[i],mean=0,sd=model at ST[[i]][1]))
	  }
	  ranff[j,]<-as.numeric(crossprod(Zt$Zt,raneff))+as.numeric(fix)
	}
	return(ranff)
  }
}

Any idea or advice would be appreciated. As I have a large dataset on which I 
want to make a prediction ( about 6 million with 9 covariates 3 Random effects 
with 16 , 550 an 38000 levels) any memory saving idea is also most important 
to me.

Many thanks in advance,
Jan Pablo Burgard



From Virgilio.Gomez at uclm.es  Sun Mar  1 14:29:39 2009
From: Virgilio.Gomez at uclm.es (Virgilio Gomez Rubio)
Date: Sun, 01 Mar 2009 14:29:39 +0100
Subject: [R-sig-ME] prediction for glmer objects
In-Reply-To: <200903011316.11921.burg4401@uni-trier.de>
References: <200903011316.11921.burg4401@uni-trier.de>
Message-ID: <1235914179.7168.9.camel@Virgilio-Gomez>

Hi Pablo,

Hope all is fine in Trier. :)

El dom, 01-03-2009 a las 13:16 +0100, burg4401 at uni-trier.de escribi?:
> Dear list-users,
> 
> in my Diploma-Theses I have the need for predicting new Y from a fitted object 
> on a different dataset. Essentially what i want to do is:
> 
> 	Y ~ X\beta +Zb
> 	and using the \beta and the variance Components of the random effects to get
> 	Ynew ~Xnew\beta + Znew rnorm(b,0,sd(b))


Is that a sort of predictive check? If you have some many registers, you
have several options:

- You can put all your data in a database and then extract it in small
groups, perform the prediction on that and the add the predictions to
the database.

- If you have a big data frame with your data, you could use package
snow (or snowfall) on a cluster. However, I believe that this uses
shared memory so you will need a big machine.

Hope this helps.

Virgilio



From j.c.l.ooms at uu.nl  Sun Mar  1 00:00:41 2009
From: j.c.l.ooms at uu.nl (Jeroen Ooms)
Date: Sun, 1 Mar 2009 00:00:41 +0100
Subject: [R-sig-ME] crossed random effects example
Message-ID: <673e1b980902281500t2e99cd0dya8fe7aa581ebb8bc@mail.gmail.com>

I am trying to learn about crossed random effects modeling in lme4. I
found this presentation that provides a small crossed dataset.
http://www.biostat.jhsph.edu/~fdominic/teaching/bio656/lectures/5addsin.crosslevels.ppt
I would like to reproduce the variance components as reported on slide
9 of the powerpoint. Here is my code:

y <- c(0.38,0,0.38,0,.33,1,.12,1,.25,0,.5,.12,.5,1,.12,.86,.5,.67,.33,0,.14,1,0,1,.14,0,.71,0,.29,1,.14,1,.43,0,.29,.86,.86,.86,.14,.75)
x2 <- rep(paste("airport",1:8,sep=""),5)
x1 <- rep(paste("treatment",1:5,sep=""),rep(8,5))
mydata <- data.frame(y=y,airport=x2,treatment=x1)
lmer(y~1+(1|airport)+(1|treatment),data=mydata)

However, the variance components as reported by lmer are different
from the ones in the slides. What formula should I use? thank you!



From ggrothendieck at gmail.com  Sun Mar  1 16:27:56 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 1 Mar 2009 10:27:56 -0500
Subject: [R-sig-ME] crossed random effects example
In-Reply-To: <673e1b980902281500t2e99cd0dya8fe7aa581ebb8bc@mail.gmail.com>
References: <673e1b980902281500t2e99cd0dya8fe7aa581ebb8bc@mail.gmail.com>
Message-ID: <971536df0903010727n1f5e67d9p3e13c9e0fc11a973@mail.gmail.com>

I suspect that the slide you are referencing mislabeled the
standard deviations as variances since there is reasonable
correspondence between your output and the slides if that
were the case.

Also check out:
http://www.stat.columbia.edu/~gelman/arm/examples/pilots/

On Sat, Feb 28, 2009 at 6:00 PM, Jeroen Ooms <j.c.l.ooms at uu.nl> wrote:
> I am trying to learn about crossed random effects modeling in lme4. I
> found this presentation that provides a small crossed dataset.
> http://www.biostat.jhsph.edu/~fdominic/teaching/bio656/lectures/5addsin.crosslevels.ppt
> I would like to reproduce the variance components as reported on slide
> 9 of the powerpoint. Here is my code:
>
> y <- c(0.38,0,0.38,0,.33,1,.12,1,.25,0,.5,.12,.5,1,.12,.86,.5,.67,.33,0,.14,1,0,1,.14,0,.71,0,.29,1,.14,1,.43,0,.29,.86,.86,.86,.14,.75)
> x2 <- rep(paste("airport",1:8,sep=""),5)
> x1 <- rep(paste("treatment",1:5,sep=""),rep(8,5))
> mydata <- data.frame(y=y,airport=x2,treatment=x1)
> lmer(y~1+(1|airport)+(1|treatment),data=mydata)
>
> However, the variance components as reported by lmer are different
> from the ones in the slides. What formula should I use? thank you!
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From ggrothendieck at gmail.com  Sun Mar  1 17:24:51 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 1 Mar 2009 11:24:51 -0500
Subject: [R-sig-ME] crossed random effects example
In-Reply-To: <673e1b980903010812g6dbce1e6r20b21538662b9eb5@mail.gmail.com>
References: <673e1b980902281500t2e99cd0dya8fe7aa581ebb8bc@mail.gmail.com>
	<971536df0903010727n1f5e67d9p3e13c9e0fc11a973@mail.gmail.com>
	<673e1b980903010812g6dbce1e6r20b21538662b9eb5@mail.gmail.com>
Message-ID: <971536df0903010824l5b3e3caeie4c56def430143ff@mail.gmail.com>

The values are the same to one decimal place and nearly the
same to two decimal places.

On Sun, Mar 1, 2009 at 11:12 AM, Jeroen Ooms <j.c.l.ooms at uu.nl> wrote:
> Hello Gabor,
>
> I don't understand what you mean. lmer reports a variance of treatment
> that is practically 0:
>
> Random effects:
> Groups ? ?Name ? ? ? ?Variance ? Std.Dev.
> airport ? (Intercept) 1.0369e-01 3.2202e-01
> treatment (Intercept) 1.0140e-23 3.1844e-12
> Residual ? ? ? ? ? ? ?4.6991e-02 2.1677e-01
>
> It seems as if lmer shows there is no variance at all for the
> treatment effect. I think this is quite different than the 0.04
> reported in the slide. Are you sure I did not did use the correct
> syntax?
>
>
>
> On Sun, Mar 1, 2009 at 4:27 PM, Gabor Grothendieck
> <ggrothendieck at gmail.com> wrote:
>> I suspect that the slide you are referencing mislabeled the
>> standard deviations as variances since there is reasonable
>> correspondence between your output and the slides if that
>> were the case.
>>
>> Also check out:
>> http://www.stat.columbia.edu/~gelman/arm/examples/pilots/
>>
>> On Sat, Feb 28, 2009 at 6:00 PM, Jeroen Ooms <j.c.l.ooms at uu.nl> wrote:
>>> I am trying to learn about crossed random effects modeling in lme4. I
>>> found this presentation that provides a small crossed dataset.
>>> http://www.biostat.jhsph.edu/~fdominic/teaching/bio656/lectures/5addsin.crosslevels.ppt
>>> I would like to reproduce the variance components as reported on slide
>>> 9 of the powerpoint. Here is my code:
>>>
>>> y <- c(0.38,0,0.38,0,.33,1,.12,1,.25,0,.5,.12,.5,1,.12,.86,.5,.67,.33,0,.14,1,0,1,.14,0,.71,0,.29,1,.14,1,.43,0,.29,.86,.86,.86,.14,.75)
>>> x2 <- rep(paste("airport",1:8,sep=""),5)
>>> x1 <- rep(paste("treatment",1:5,sep=""),rep(8,5))
>>> mydata <- data.frame(y=y,airport=x2,treatment=x1)
>>> lmer(y~1+(1|airport)+(1|treatment),data=mydata)
>>>
>>> However, the variance components as reported by lmer are different
>>> from the ones in the slides. What formula should I use? thank you!
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>



From j.c.l.ooms at uu.nl  Sun Mar  1 17:12:33 2009
From: j.c.l.ooms at uu.nl (Jeroen Ooms)
Date: Sun, 1 Mar 2009 17:12:33 +0100
Subject: [R-sig-ME] crossed random effects example
In-Reply-To: <971536df0903010727n1f5e67d9p3e13c9e0fc11a973@mail.gmail.com>
References: <673e1b980902281500t2e99cd0dya8fe7aa581ebb8bc@mail.gmail.com>
	<971536df0903010727n1f5e67d9p3e13c9e0fc11a973@mail.gmail.com>
Message-ID: <673e1b980903010812g6dbce1e6r20b21538662b9eb5@mail.gmail.com>

Hello Gabor,

I don't understand what you mean. lmer reports a variance of treatment
that is practically 0:

Random effects:
Groups ? ?Name ? ? ? ?Variance ? Std.Dev.
airport ? (Intercept) 1.0369e-01 3.2202e-01
treatment (Intercept) 1.0140e-23 3.1844e-12
Residual ? ? ? ? ? ? ?4.6991e-02 2.1677e-01

It seems as if lmer shows there is no variance at all for the
treatment effect. I think this is quite different than the 0.04
reported in the slide. Are you sure I did not did use the correct
syntax?



On Sun, Mar 1, 2009 at 4:27 PM, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> I suspect that the slide you are referencing mislabeled the
> standard deviations as variances since there is reasonable
> correspondence between your output and the slides if that
> were the case.
>
> Also check out:
> http://www.stat.columbia.edu/~gelman/arm/examples/pilots/
>
> On Sat, Feb 28, 2009 at 6:00 PM, Jeroen Ooms <j.c.l.ooms at uu.nl> wrote:
>> I am trying to learn about crossed random effects modeling in lme4. I
>> found this presentation that provides a small crossed dataset.
>> http://www.biostat.jhsph.edu/~fdominic/teaching/bio656/lectures/5addsin.crosslevels.ppt
>> I would like to reproduce the variance components as reported on slide
>> 9 of the powerpoint. Here is my code:
>>
>> y <- c(0.38,0,0.38,0,.33,1,.12,1,.25,0,.5,.12,.5,1,.12,.86,.5,.67,.33,0,.14,1,0,1,.14,0,.71,0,.29,1,.14,1,.43,0,.29,.86,.86,.86,.14,.75)
>> x2 <- rep(paste("airport",1:8,sep=""),5)
>> x1 <- rep(paste("treatment",1:5,sep=""),rep(8,5))
>> mydata <- data.frame(y=y,airport=x2,treatment=x1)
>> lmer(y~1+(1|airport)+(1|treatment),data=mydata)
>>
>> However, the variance components as reported by lmer are different
>> from the ones in the slides. What formula should I use? thank you!
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>



From jeroenooms at gmail.com  Sun Mar  1 17:44:09 2009
From: jeroenooms at gmail.com (Jeroen Ooms)
Date: Sun, 1 Mar 2009 17:44:09 +0100
Subject: [R-sig-ME] crossed random effects example
In-Reply-To: <971536df0903010824l5b3e3caeie4c56def430143ff@mail.gmail.com>
References: <673e1b980902281500t2e99cd0dya8fe7aa581ebb8bc@mail.gmail.com>
	<971536df0903010727n1f5e67d9p3e13c9e0fc11a973@mail.gmail.com>
	<673e1b980903010812g6dbce1e6r20b21538662b9eb5@mail.gmail.com>
	<971536df0903010824l5b3e3caeie4c56def430143ff@mail.gmail.com>
Message-ID: <673e1b980903010844k7a02df87v6dc9cc34106ea3b3@mail.gmail.com>

OK, maybe you are right. I tried the link you sent for another
example. However, also in this example, the variance of the second
random effect is estimated at zero:

pilots1 <- lmer (y ~ 1 + (1 | group.id) + (1 | scenario.id))
pilots1

Random effects:
 Groups      Name        Variance   Std.Dev.
 scenario.id (Intercept) 1.0333e-01 3.2145e-01
 group.id    (Intercept) 9.3719e-24 3.0614e-12
 Residual                4.6738e-02 2.1619e-01

I find this very confusing. Is it also possible to have crossed random
effects that both have variance? An example would be great. Thanks
very much!


On Sun, Mar 1, 2009 at 5:24 PM, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> The values are the same to one decimal place and nearly the
> same to two decimal places.
>
> On Sun, Mar 1, 2009 at 11:12 AM, Jeroen Ooms <j.c.l.ooms at uu.nl> wrote:
>> Hello Gabor,
>>
>> I don't understand what you mean. lmer reports a variance of treatment
>> that is practically 0:
>>
>> Random effects:
>> Groups ? ?Name ? ? ? ?Variance ? Std.Dev.
>> airport ? (Intercept) 1.0369e-01 3.2202e-01
>> treatment (Intercept) 1.0140e-23 3.1844e-12
>> Residual ? ? ? ? ? ? ?4.6991e-02 2.1677e-01
>>
>> It seems as if lmer shows there is no variance at all for the
>> treatment effect. I think this is quite different than the 0.04
>> reported in the slide. Are you sure I did not did use the correct
>> syntax?
>>
>>
>>
>> On Sun, Mar 1, 2009 at 4:27 PM, Gabor Grothendieck
>> <ggrothendieck at gmail.com> wrote:
>>> I suspect that the slide you are referencing mislabeled the
>>> standard deviations as variances since there is reasonable
>>> correspondence between your output and the slides if that
>>> were the case.
>>>
>>> Also check out:
>>> http://www.stat.columbia.edu/~gelman/arm/examples/pilots/
>>>
>>> On Sat, Feb 28, 2009 at 6:00 PM, Jeroen Ooms <j.c.l.ooms at uu.nl> wrote:
>>>> I am trying to learn about crossed random effects modeling in lme4. I
>>>> found this presentation that provides a small crossed dataset.
>>>> http://www.biostat.jhsph.edu/~fdominic/teaching/bio656/lectures/5addsin.crosslevels.ppt
>>>> I would like to reproduce the variance components as reported on slide
>>>> 9 of the powerpoint. Here is my code:
>>>>
>>>> y <- c(0.38,0,0.38,0,.33,1,.12,1,.25,0,.5,.12,.5,1,.12,.86,.5,.67,.33,0,.14,1,0,1,.14,0,.71,0,.29,1,.14,1,.43,0,.29,.86,.86,.86,.14,.75)
>>>> x2 <- rep(paste("airport",1:8,sep=""),5)
>>>> x1 <- rep(paste("treatment",1:5,sep=""),rep(8,5))
>>>> mydata <- data.frame(y=y,airport=x2,treatment=x1)
>>>> lmer(y~1+(1|airport)+(1|treatment),data=mydata)
>>>>
>>>> However, the variance components as reported by lmer are different
>>>> from the ones in the slides. What formula should I use? thank you!
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>
>>
>



From reinhold.kliegl at gmail.com  Sun Mar  1 20:57:47 2009
From: reinhold.kliegl at gmail.com (Reinhold Kliegl)
Date: Sun, 1 Mar 2009 20:57:47 +0100
Subject: [R-sig-ME] crossed random effects example
In-Reply-To: <673e1b980903010844k7a02df87v6dc9cc34106ea3b3@mail.gmail.com>
References: <673e1b980902281500t2e99cd0dya8fe7aa581ebb8bc@mail.gmail.com>
	<971536df0903010727n1f5e67d9p3e13c9e0fc11a973@mail.gmail.com>
	<673e1b980903010812g6dbce1e6r20b21538662b9eb5@mail.gmail.com>
	<971536df0903010824l5b3e3caeie4c56def430143ff@mail.gmail.com>
	<673e1b980903010844k7a02df87v6dc9cc34106ea3b3@mail.gmail.com>
Message-ID: <aefe4d0a0903011157g70971940i84224040c2ba9d3c@mail.gmail.com>

Try:
> data(ScotsSec, package = "mlmRev")
> (fm <- lmer(attain ~ 0 + sex + verbal + (1|primary) + (1|second), ScotsSec) )

Reinhold Kliegl

On Sun, Mar 1, 2009 at 5:44 PM, Jeroen Ooms <jeroenooms at gmail.com> wrote:
> OK, maybe you are right. I tried the link you sent for another
> example. However, also in this example, the variance of the second
> random effect is estimated at zero:
>
> pilots1 <- lmer (y ~ 1 + (1 | group.id) + (1 | scenario.id))
> pilots1
>
> Random effects:
> ?Groups ? ? ?Name ? ? ? ?Variance ? Std.Dev.
> ?scenario.id (Intercept) 1.0333e-01 3.2145e-01
> ?group.id ? ?(Intercept) 9.3719e-24 3.0614e-12
> ?Residual ? ? ? ? ? ? ? ?4.6738e-02 2.1619e-01
>
> I find this very confusing. Is it also possible to have crossed random
> effects that both have variance? An example would be great. Thanks
> very much!
>
>
> On Sun, Mar 1, 2009 at 5:24 PM, Gabor Grothendieck
> <ggrothendieck at gmail.com> wrote:
>> The values are the same to one decimal place and nearly the
>> same to two decimal places.
>>
>> On Sun, Mar 1, 2009 at 11:12 AM, Jeroen Ooms <j.c.l.ooms at uu.nl> wrote:
>>> Hello Gabor,
>>>
>>> I don't understand what you mean. lmer reports a variance of treatment
>>> that is practically 0:
>>>
>>> Random effects:
>>> Groups ? ?Name ? ? ? ?Variance ? Std.Dev.
>>> airport ? (Intercept) 1.0369e-01 3.2202e-01
>>> treatment (Intercept) 1.0140e-23 3.1844e-12
>>> Residual ? ? ? ? ? ? ?4.6991e-02 2.1677e-01
>>>
>>> It seems as if lmer shows there is no variance at all for the
>>> treatment effect. I think this is quite different than the 0.04
>>> reported in the slide. Are you sure I did not did use the correct
>>> syntax?
>>>
>>>
>>>
>>> On Sun, Mar 1, 2009 at 4:27 PM, Gabor Grothendieck
>>> <ggrothendieck at gmail.com> wrote:
>>>> I suspect that the slide you are referencing mislabeled the
>>>> standard deviations as variances since there is reasonable
>>>> correspondence between your output and the slides if that
>>>> were the case.
>>>>
>>>> Also check out:
>>>> http://www.stat.columbia.edu/~gelman/arm/examples/pilots/
>>>>
>>>> On Sat, Feb 28, 2009 at 6:00 PM, Jeroen Ooms <j.c.l.ooms at uu.nl> wrote:
>>>>> I am trying to learn about crossed random effects modeling in lme4. I
>>>>> found this presentation that provides a small crossed dataset.
>>>>> http://www.biostat.jhsph.edu/~fdominic/teaching/bio656/lectures/5addsin.crosslevels.ppt
>>>>> I would like to reproduce the variance components as reported on slide
>>>>> 9 of the powerpoint. Here is my code:
>>>>>
>>>>> y <- c(0.38,0,0.38,0,.33,1,.12,1,.25,0,.5,.12,.5,1,.12,.86,.5,.67,.33,0,.14,1,0,1,.14,0,.71,0,.29,1,.14,1,.43,0,.29,.86,.86,.86,.14,.75)
>>>>> x2 <- rep(paste("airport",1:8,sep=""),5)
>>>>> x1 <- rep(paste("treatment",1:5,sep=""),rep(8,5))
>>>>> mydata <- data.frame(y=y,airport=x2,treatment=x1)
>>>>> lmer(y~1+(1|airport)+(1|treatment),data=mydata)
>>>>>
>>>>> However, the variance components as reported by lmer are different
>>>>> from the ones in the slides. What formula should I use? thank you!
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>
>>>
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From H.Quene at uu.nl  Sun Mar  1 19:55:41 2009
From: H.Quene at uu.nl (=?UTF-8?B?SHVnbyBRdWVuwo7DqQ==?=)
Date: Sun, 01 Mar 2009 19:55:41 +0100
Subject: [R-sig-ME] crossed random effects example [R-sig-mixed-models
 Digest, Vol 27, Issue 2]
In-Reply-To: <mailman.3069.1235926961.4476.r-sig-mixed-models@r-project.org>
References: <mailman.3069.1235926961.4476.r-sig-mixed-models@r-project.org>
Message-ID: <49AADA2D.1080305@uu.nl>

Dear Jeroen,

Perhaps you'll also appreciate these examples of crossed random 
effects:

http://dx.doi.org/doi:10.1016/j.jml.2008.02.002

with appendix materials at
http://www.let.uu.nl/~Hugo.Quene/personal/mixedeffects/

Best wishes, Hugo Quen?

> Date: Sun, 1 Mar 2009 00:00:41 +0100
> From: Jeroen Ooms <j.c.l.ooms at uu.nl>
> Subject: [R-sig-ME] crossed random effects example
> To: r-sig-mixed-models at r-project.org
> Message-ID:
>         <673e1b980902281500t2e99cd0dya8fe7aa581ebb8bc at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> I am trying to learn about crossed random effects modeling in lme4. I
> found this presentation that provides a small crossed dataset.
> http://www.biostat.jhsph.edu/~fdominic/teaching/bio656/lectures/5addsin.crosslevels.ppt 
> 

-- 
Dr Hugo Quen? | senior associate professor in Phonetics | Dept 
Modern Languages | Utrecht inst of Linguistics OTS | Utrecht 
University | Trans 10 | room 1.17 | 3512 JK Utrecht | The 
Netherlands | T +31 30 253 6070 | F +31 30 253 6000 | H.Quene at uu.nl 
| www.hugoquene.nl | www.hum.uu.nl



From hugo.quene at let.uu.nl  Sun Mar  1 23:26:03 2009
From: hugo.quene at let.uu.nl (=?UTF-8?B?SHVnbyBRdWVuw6k=?=)
Date: Sun, 01 Mar 2009 23:26:03 +0100
Subject: [R-sig-ME] crossed random effects example [R-sig-mixed-models
 Digest, Vol 27, Issue 2]
In-Reply-To: <mailman.3069.1235926961.4476.r-sig-mixed-models@r-project.org>
References: <mailman.3069.1235926961.4476.r-sig-mixed-models@r-project.org>
Message-ID: <49AB0B7B.80109@let.uu.nl>

Dear Jeroen and others,

Perhaps you'll also appreciate these examples of crossed random
effects:

http://dx.doi.org/doi:10.1016/j.jml.2008.02.002

with appendix materials at
http://www.let.uu.nl/~Hugo.Quene/personal/mixedeffects/

Best wishes, Hugo Quen?

> Date: Sun, 1 Mar 2009 00:00:41 +0100
> From: Jeroen Ooms <j.c.l.ooms at uu.nl>
> Subject: [R-sig-ME] crossed random effects example
> To: r-sig-mixed-models at r-project.org
> Message-ID:
>         <673e1b980902281500t2e99cd0dya8fe7aa581ebb8bc at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> I am trying to learn about crossed random effects modeling in lme4. I
> found this presentation that provides a small crossed dataset.
> http://www.biostat.jhsph.edu/~fdominic/teaching/bio656/lectures/5addsin.crosslevels.ppt 
> 

-- 
Dr Hugo Quen? | senior docent-onderzoeker Fonetiek | Departement 
Moderne Talen | Utrecht inst of Linguistics OTS | Universiteit 
Utrecht | Trans 10 | kamer 1.17 | 3512 JK Utrecht | The Netherlands 
| T +31 30 253 6070 | F +31 30 253 6000 | hugo.quene at let.uu.nl | 
www.hugoquene.nl | www.hum.uu.nl



From tahirajamil at yahoo.com  Mon Mar  2 10:38:30 2009
From: tahirajamil at yahoo.com (Tahira Jamil)
Date: Mon, 2 Mar 2009 01:38:30 -0800 (PST)
Subject: [R-sig-ME] Nonlinear mixed model in lme4
In-Reply-To: <mailman.3.1235905202.32070.r-sig-mixed-models@r-project.org>
Message-ID: <378752.24115.qm@web50808.mail.re2.yahoo.com>


Hi
I have found this forum very useful. So thank to all who contribute to this forum.

I have a problem with non-linear mixed model. When I run the model 

 (nm1.a <-fixef( nlmer(logmv ~ SSfpl(logC, A, B, xmid, scal)~xmid | status,
   data=mix.data,start = c(A = 0.3, B = 0.7, xmid=-1.4,scal = 0.3)))

it runs nicely and gives estimates. But when I try to extract the fixed and randoms effect it gives error as

fixef( nlmer(logmv ~ SSfpl(logC, A, B, xmid, scal)~xmid | status,
  + data=mix.data,start = c(A = 0.3, B = 0.7, xmid=-1.4,scal = 0.3)))
  Error in UseMethod("fixef") : no applicable method for "fixef"

Is there some applicable method that we can extract the effects for nonlinear mixed model in lme4. In nlme I am able to extract the random effect and fixed effect. But I am interested in lme4.
Can someone guide me how lme4 differ from nlme.

Tahira Jamil
Biometris
Wageningen University Wageningen
Netherlands

--- On Sun, 3/1/09, r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org> wrote:

> From: r-sig-mixed-models-request at r-project.org <r-sig-mixed-models-request at r-project.org>
> Subject: R-sig-mixed-models Digest, Vol 27, Issue 1
> To: r-sig-mixed-models at r-project.org
> Date: Sunday, March 1, 2009, 4:00 PM
> Send R-sig-mixed-models mailing list submissions to
> r-sig-mixed-models at r-project.org
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> or, via email, send a message with subject or body
> 'help' to
> r-sig-mixed-models-request at r-project.org
> 
> You can reach the person managing the list at
> r-sig-mixed-models-owner at r-project.org
> 
> When replying, please edit your Subject line so it is more
> specific
> than "Re: Contents of R-sig-mixed-models
> digest..."
> 
> 
> Today's Topics:
> 
> 1. Re: [R] lme4 and Variable level detection (Douglas
> Bates)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Sat, 28 Feb 2009 12:25:16 -0600
> From: Douglas Bates <bates at stat.wisc.edu>
> Subject: Re: [R-sig-ME] [R] lme4 and Variable level
> detection
> To: Jeroen Ooms <j.c.l.ooms at uu.nl>
> Cc: r-help at r-project.org, R Mixed Models
> <r-sig-mixed-models at r-project.org>
> Message-ID:
> <40e66e0b0902281025p5318a0c4o565e6b5038c183a1 at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> On Sat, Feb 28, 2009 at 9:00 AM, Jeroen Ooms
> <j.c.l.ooms at uu.nl> wrote:
> 
> > I am making a little GUI for lme4, and I was wondering
> if there is a function
> > that automatically detects on which level every
> variable exists.
> > Furtheremore I got kind of confused about what a
> random effects model
> > actually calculates.
> 
> Questions such as this may be answered more quickly if you
> send them
> to the R-SIG-Mixed-Models mailing list, which I am cc:ing
> on this
> reply.
> 
> > I have some experience with commercial software
> packages for multilevel
> > analysis, like HLM6, and I was surprised that lme4
> does not require the user
> > to specify the level for every predictor variable. Is
> this because the
> > function automatically detects the level by testing on
> which levels the
> > predictor has variance, or is this information simply
> not needed?
> 
> In some ways, exposure to software like HLM or MLWin can be
> more of a
> hindrance than a help when learning about mixed models. In
> presentation of the model and in the software itself these
> packages
> emphasize "levels" of random effects leading to
> the impression that we
> can only associate random effects with factors that are
> nested. This
> is a misconception. There are many cases where is it
> eminently
> sensible to associate random effects with factors that are
> completely
> crossed ('subject' and 'item' are a prime
> example) or partially
> crossed. The archetypal example used in multilevel
> modeling,
> achievement scores on students nested in classes nested in
> schools
> nested in ..., becomes partially crossed when we track
> students over
> time and they move from class to class or school to school.
> 
> I imagine that the reason for defining the model in terms
> of nested
> factors for random effects is computational. If you insist
> that the
> random effects must always be defined with respect to
> nested factors
> then you can employ methods that take advantage of this,
> with
> considerable simplification in the storage and
> computational burden.
> The lme4 package adopts a different approach based on
> sparse matrix
> storage and decomposition methods. It turns out that these
> methods
> are competitive with the best methods for models based on
> nested
> factors, in the cases to which they apply, and these
> methods allow for
> fitting much more general models.
> 
> An unfortunate side-effect of the emphasis on levels in
> MLWin and HLM
> is the perception that other covariates must be
> characterized by the
> level at which they vary, even if these covariates only
> determine
> fixed-effects parameters. This is quite untrue and
> misleading. The
> only constraints on the covariates and the model matrix for
> the
> fixed-effects parameters is that the model matrix must be
> of full
> column rank. In models that define random effects for
> slopes, or in
> general for the coefficients associated with a covariate,
> the
> constraint is that the covariate cannot be constant within
> each level
> of the grouping factor of the random effect. For example,
> we cannot
> estimate a random effect for the coefficients for sex (M/F)
> within
> subject (assuming we do not have transgender people in the
> study).
> 
> My advice would be to avoid phrasing the model in terms of
> levels of
> random effects. Although I realize that those with a
> background of
> using MLWin or HLM may find this more comfortable, I think
> it would be
> propagating bad practices and misconceptions.
> 
> > I was taught that a crosslevel interaction predicts
> the regression
> > coefficient of the lower level variable, which is also
> what is implied by
> > the HLM gui. However, in an lme4 formula, a crosslevel
> interaction has the
> > same syntax as a regular interaction term.
> Furthermore, lme4 also allows
> > adding crosslevel interactions without a random slope
> for the lower level
> > variable. Now I'm confused. Is there a fundamental
> difference between a
> > crosslevel interaction, or is the same thing as a
> regular interaction when
> > the model also holds an error term for the lower level
> variable?
> >
> >
> >
> >
> > -----
> > Jeroen Ooms * Dept. of Methodology and Statistics *
> Utrecht University
> >
> > Visit ?http://www.jeroenooms.com www.jeroenooms.com
> ?to explore some of my
> > current projects.
> >
> >
> >
> >
> >
> >
> > --
> > View this message in context:
> http://www.nabble.com/lme4-and-Variable-level-detection-tp22262944p22262944.html
> > Sent from the R help mailing list archive at
> Nabble.com.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> 
> 
> 
> ------------------------------
> 
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> End of R-sig-mixed-models Digest, Vol 27, Issue 1
> *************************************************



From burg4401 at uni-trier.de  Mon Mar  2 15:01:59 2009
From: burg4401 at uni-trier.de (burg4401 at uni-trier.de)
Date: Mon, 2 Mar 2009 15:01:59 +0100
Subject: [R-sig-ME] prediction for glmer objects
In-Reply-To: <mailman.3069.1235926961.4476.r-sig-mixed-models@r-project.org>
References: <mailman.3069.1235926961.4476.r-sig-mixed-models@r-project.org>
Message-ID: <200903021501.59795.burg4401@uni-trier.de>

Hola Virgilio,

thanks for your reply. This indeed helps. Partitioning the data.frame along 
the top-level random effect decreases the magnitude of the model matrices by 
far, such that RAM comes quite handy.

What I'm trying to do is just to replicate the structure of Y from one dataset 
to an other. Though the predictive check was meant in the beginning of his 
work to test whether the models holds on the same data-set. But this is 
postponed.

Are you coming to the Rhine River Cruise?

Thanks again,
Pablo

Am Sonntag 01 M?rz 2009 18:02:41 schrieb r-sig-mixed-models-request at r-
project.org:
> Message: 2
> Date: Sun, 01 Mar 2009 14:29:39 +0100
> From: Virgilio Gomez Rubio <Virgilio.Gomez at uclm.es>
> Subject: Re: [R-sig-ME] prediction for glmer objects
> To: burg4401 at uni-trier.de
> Cc: r-sig-mixed-models at r-project.org
> Message-ID: <1235914179.7168.9.camel at Virgilio-Gomez>
> Content-Type: text/plain; charset="UTF-8"
>
> Hi Pablo,
>
> Hope all is fine in Trier. :)
>
> El dom, 01-03-2009 a las 13:16 +0100, burg4401 at uni-trier.de escribi?:
> > Dear list-users,
> >
> > in my Diploma-Theses I have the need for predicting new Y from a fitted
> > object on a different dataset. Essentially what i want to do is:
> >
> > 	Y ~ X\beta +Zb
> > 	and using the \beta and the variance Components of the random effects to
> > get Ynew ~Xnew\beta + Znew rnorm(b,0,sd(b))
>
> Is that a sort of predictive check? If you have some many registers, you
> have several options:
>
> - You can put all your data in a database and then extract it in small
> groups, perform the prediction on that and the add the predictions to
> the database.
>
> - If you have a big data frame with your data, you could use package
> snow (or snowfall) on a cluster. However, I believe that this uses
> shared memory so you will need a big machine.
>
> Hope this helps.
>
> Virgilio



From julien.martin2 at usherbrooke.ca  Mon Mar  2 20:59:07 2009
From: julien.martin2 at usherbrooke.ca (Julien Martin)
Date: Mon, 02 Mar 2009 14:59:07 -0500
Subject: [R-sig-ME] equivalent of intervals() for lmer
In-Reply-To: <mailman.3.1235991601.28022.r-sig-mixed-models@r-project.org>
References: <mailman.3.1235991601.28022.r-sig-mixed-models@r-project.org>
Message-ID: <1236023947.6023.81.camel@julien-uni>

Hi
when using lme or nlme, I could use intervals() to obtain 95%CI for sd
estimates of random effects.
However intervals does not work with lmer. is there any equivalent
function? or is there any way to obtain those 95%CI with lmer?
Thanks
Julien



From r.turner at auckland.ac.nz  Tue Mar  3 00:23:13 2009
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 3 Mar 2009 12:23:13 +1300
Subject: [R-sig-ME] [R] Balanced design,
	differences in results using anova and lmer/anova
In-Reply-To: <49AC5C8D.1080803@mpi-sb.mpg.de>
References: <49A7AD09.5050007@mpi-sb.mpg.de>
	<9857B098-8C3F-4242-8A30-6E7D0A9809EC@auckland.ac.nz>
	<49AC5C8D.1080803@mpi-sb.mpg.de>
Message-ID: <B80C5418-2D2E-41D5-809A-616CF7970FD2@auckland.ac.nz>


On 3/03/2009, at 11:24 AM, Lars Kunert wrote:

> Hi Rolf,
>
> thanks for your nice reply, however I already spotted that problem  
> - for
> some reason
> Bates included that bit of information in his Rnews artikel (see  
> below),
> and not in the help file...
> The read.data function which I included in my original post derived a
> combinded blocking factor (d$loc:d$block.tmp) as an replacement for
> block. Do you have some other idea?

I'm a frayed knot. :-)  I have exhausted my (very limited) expertise
in this area.  It would be nice if one the mixed models gurus would
help you out, but much of the time, inexplicably it seems to me, it  
is impossible
to elicit a response from any of them.

Sorry I can't be of any help.

	cheers,

		Rolf

######################################################################
Attention:\ This e-mail message is privileged and confid...{{dropped:9}}



From orzack at freshpond.org  Tue Mar  3 05:54:15 2009
From: orzack at freshpond.org (orzack)
Date: Mon, 2 Mar 2009 23:54:15 -0500
Subject: [R-sig-ME] Data aggregation
Message-ID: <p0623090ac5d2665e77bc@[192.168.1.104]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090302/8fd138c0/attachment.pl>

From kyler at mail.smu.edu  Tue Mar  3 18:40:43 2009
From: kyler at mail.smu.edu (Roberts, Kyle)
Date: Tue, 3 Mar 2009 11:40:43 -0600
Subject: [R-sig-ME] fitted for binomial model
In-Reply-To: <C5CDA922.91E4%dafshartous@med.miami.edu>
References: <40e66e0b0902261017h62f8e13el7dc5dcbdb28a8e06@mail.gmail.com>
	<C5CDA922.91E4%dafshartous@med.miami.edu>
Message-ID: <551E1CBE65B7EB44B9DF69AF8ED0BE7F09212E2F5F@SXMBXA.systems.smu.edu>

Dear All,

If I am running a model like:

M1<-lmer(model, data, family=binomial)

What does the "fitted" function produce?

e.g. fitted(M1)

I had thought that this was the fitted probabilities, but after checking the math, I think it is something else. Also, if this is not the fitted probabilities [exp(model)/(1 + exp(model))], is there a way to have them easily produced?

Thanks,
Kyle

*********************************************************
Dr. J. Kyle Roberts
Department of Teaching and Learning
Annette Caldwell Simmons School of Education 
   and Human Development
Southern Methodist University
P.O. Box 750381
Dallas, TX? 75275
214-768-4494
http://www.hlm-online.com/



From bates at stat.wisc.edu  Tue Mar  3 19:18:10 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 3 Mar 2009 12:18:10 -0600
Subject: [R-sig-ME] [R] Balanced design,
	differences in results using anova and 	lmer/anova
In-Reply-To: <49A7AD09.5050007@mpi-sb.mpg.de>
References: <49A7AD09.5050007@mpi-sb.mpg.de>
Message-ID: <40e66e0b0903031018h409af50fhe4e3abc12cd82779@mail.gmail.com>

On Fri, Feb 27, 2009 at 3:06 AM, Lars Kunert <lkunert at mpi-sb.mpg.de> wrote:
> Hi, I am trying to do an analysis of variance for an unbalanced design.
> As a toy example, I use a dataset presented by K. Hinkelmann and O.
> Kempthorne in "Design and Anaylysis of Experiments" (p353-356).
> This example is very similar to my own dataset, with one difference: it
> is balanced.
> Thus it is possible to do an anaylsis using both: (1) anova, and (2) lmer.
> Furthermore, I can compare my results with the results presented in the
> book (the book uses SAS).

> In short:
>> using anova, I can reproduce the results presented in the book.
>> using lmer, I fail to reproduce the results
> However, for my "real" analysis, I need lmer - what do I do wrong?

> The example uses as randomized complete block desigh (RCBD) with a
> nested blocking structure
> and subsampling.

> response:
> ?height (of some trees)
> covariates:
> ?HSF (type of the trees)
> nested covariates:
> ?loc (location)
> ?block ?(block is nested in location)
>
> # the data (file: pine.txt) looks like this:
>
> loc ? ?block ? ?HSF ? ?height
> 1 ? ?1 ? ?1 ? ?210
> 1 ? ?1 ? ?1 ? ?221
> 1 ? ?1 ? ?2 ? ?252
> 1 ? ?1 ? ?2 ? ?260
> 1 ? ?1 ? ?3 ? ?197
> 1 ? ?1 ? ?3 ? ?190
> 1 ? ?2 ? ?1 ? ?222
> 1 ? ?2 ? ?1 ? ?214
> 1 ? ?2 ? ?2 ? ?265
> 1 ? ?2 ? ?2 ? ?271
> 1 ? ?2 ? ?3 ? ?201
> 1 ? ?2 ? ?3 ? ?210
> 1 ? ?3 ? ?1 ? ?220
> 1 ? ?3 ? ?1 ? ?225
> 1 ? ?3 ? ?2 ? ?271
> 1 ? ?3 ? ?2 ? ?277
> 1 ? ?3 ? ?3 ? ?205
> 1 ? ?3 ? ?3 ? ?204
> 1 ? ?4 ? ?1 ? ?224
> 1 ? ?4 ? ?1 ? ?231
> 1 ? ?4 ? ?2 ? ?270
> 1 ? ?4 ? ?2 ? ?283
> 1 ? ?4 ? ?3 ? ?211
> 1 ? ?4 ? ?3 ? ?216
> 2 ? ?1 ? ?1 ? ?178
> 2 ? ?1 ? ?1 ? ?175
> 2 ? ?1 ? ?2 ? ?191
> 2 ? ?1 ? ?2 ? ?193
> 2 ? ?1 ? ?3 ? ?182
> 2 ? ?1 ? ?3 ? ?179
> 2 ? ?2 ? ?1 ? ?180
> 2 ? ?2 ? ?1 ? ?184
> 2 ? ?2 ? ?2 ? ?198
> 2 ? ?2 ? ?2 ? ?201
> 2 ? ?2 ? ?3 ? ?183
> 2 ? ?2 ? ?3 ? ?190
> 2 ? ?3 ? ?1 ? ?189
> 2 ? ?3 ? ?1 ? ?183
> 2 ? ?3 ? ?2 ? ?200
> 2 ? ?3 ? ?2 ? ?195
> 2 ? ?3 ? ?3 ? ?197
> 2 ? ?3 ? ?3 ? ?205
> 2 ? ?4 ? ?1 ? ?184
> 2 ? ?4 ? ?1 ? ?192
> 2 ? ?4 ? ?2 ? ?197
> 2 ? ?4 ? ?2 ? ?204
> 2 ? ?4 ? ?3 ? ?192
> 2 ? ?4 ? ?3 ? ?190
>
> #
> # then I load the data
> #
> read.data = function()
> {
> ? ? ? ?d = read.table( "pines.txt", header=TRUE )
>
> ? ? ? ?d$loc ? ? ? = as.factor( d$loc ? )
> ? ? ? ?d$block.tmp = as.factor( d$block )
> ? ? ? ?d$block ? ? = ( d$loc:d$block.tmp )[drop=TRUE] ?# lme4 does not support
> implicit nesting
>
> ? ? ? ?d$HSF ? = as.factor( d$HSF )
>
> ? ? ? ?return( d )
> }
>
> d = read.data()
>
>
> #
> # using anova.....
> #
> m.aov = aov( height ~ HSF*loc + Error(loc/block + HSF:loc/block), data=d )
> summary( m.aov )
>
> #
> # I get:
> #
> Error: loc
> ? ?Df Sum Sq Mean Sq
> loc ?1 ?20336 ? 20336
>
> Error: loc:block
> ? ? ? ? ?Df ?Sum Sq Mean Sq F value Pr(>F)
> Residuals ?6 1462.33 ?243.72
>
> Error: loc:HSF
> ? ? ? ?Df ?Sum Sq Mean Sq
> HSF ? ? ?2 12170.7 ?6085.3
> HSF:loc ?2 ?6511.2 ?3255.6
>
> Error: loc:block:HSF
> ? ? ? ? ?Df ?Sum Sq Mean Sq F value Pr(>F)
> Residuals 12 301.167 ?25.097
>
> Error: Within
> ? ? ? ? ?Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 24 529.00 ? 22.04
>
> #
> # which is, what I expected, however, using lmer....
> #
> m.lmer = lmer( height ~ HSF*loc + HSF*(loc|block), data=d )
> anova( m.lmer )
>
> #
> # I get:
> #
> Analysis of Variance Table
> ? ? ? ?Df ?Sum Sq Mean Sq
> HSF ? ? ?2 12170.7 ?6085.3
> loc ? ? ?1 ?1924.6 ?1924.6
> HSF:loc ?2 ?6511.2 ?3255.6
>
> #
> # what is, at least not what I expected...
> #
> Thanks for your help, Lars

Before I begin fitting models I try to plot the data, as shown in the
enclosed files.  What I see there is a strong effect for location, a
strong effect for HSF and an HSF:loc interaction.  The effect for
block within location is marginal compared to the other effects.  You
didn't say, but I suspect that these are constructed data because the
levels of block seem to be systematically related to the height and I
would not expect that if the block was randomly chosen within
location.

As you and Rolf have both discovered, the block factor is implicitly
nested within loc so any random effects should be defined with respect
to loc:block instead of block.  An alternative is to label the blocks
so that different blocks get different labels, which seems eminently
sensible to me but I appreciate that the implicitly nested labels are
frequently used in texts and you may want to preserve the original
form of the data.

As for fitting models, I would probably not use random effects for loc
in these data because there are only two levels of loc.  When you use
random effects you are estimating variances not means and estimating a
variance from only two groups is precarious at best.

We may be able to be more helpful if you described the structure of your data.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pine.pdf
Type: application/pdf
Size: 8801 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090303/294d4aec/attachment.pdf>

From biolger at yahoo.com.ar  Tue Mar  3 21:36:30 2009
From: biolger at yahoo.com.ar (German Garcia)
Date: Tue, 3 Mar 2009 12:36:30 -0800 (PST)
Subject: [R-sig-ME] estimated P values
Message-ID: <716914.33335.qm@web56406.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090303/aaf71d37/attachment.pl>

From julien.vezilier at gmail.com  Wed Mar  4 11:22:58 2009
From: julien.vezilier at gmail.com (Julien Vezilier)
Date: Wed, 4 Mar 2009 11:22:58 +0100
Subject: [R-sig-ME] F test in lmer quasipoisson
Message-ID: <cd354c560903040222x17b8f57fscce1170f94062e57@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090304/28ef774d/attachment.pl>

From Greg.Snow at imail.org  Wed Mar  4 19:19:43 2009
From: Greg.Snow at imail.org (Greg Snow)
Date: Wed, 4 Mar 2009 11:19:43 -0700
Subject: [R-sig-ME] estimated P values
In-Reply-To: <716914.33335.qm@web56406.mail.re3.yahoo.com>
References: <716914.33335.qm@web56406.mail.re3.yahoo.com>
Message-ID: <B37C0A15B8FB3C468B5BC7EBC7DA14CC61CC0343C3@LP-EXMBVS10.CO.IHC.COM>

This post:

http://finzi.psych.upenn.edu/R/Rhelp08/archive/156522.html

has some examples that may help.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
801.408.8111


> -----Original Message-----
> From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-
> models-bounces at r-project.org] On Behalf Of German Garcia
> Sent: Tuesday, March 03, 2009 1:37 PM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] estimated P values
> 
> Hello all list?!!!
> I` am currently comparing generalized linear mixed models in lmer, with
> quasibinomial families. I read some of the discussions on the R-forum
> and it seems
> that the Laplace approximation used in the current version of lmer is
> good. My problem is that in lmer, with quasibinomial families, only
> get t-values but no associated p-values.
> 
> I understood that LRT for testing fixed effects in no good (Bolker et
> al 2008). So, Is there a way to get a reliable p-value for the fixed
> effects?
> 
> 
> In this forum spoke about Wald F test and wald t test, but I don?t know
> how I estimated this test in R software for GLMM analysis. Can anybody
> talk me about this?
>  Thank you so much
> 
> Lic. Germ?n Garc?a Ecolog?a y Conservaci?n de Aves Marinas Departamento
> de Biolog?a, Facultad de Ciencias Exactas y Naturales, Universidad
> Nacional de Mar del Plata - CONICET Funes 3250 (B7602AYJ) Mar del
> Plata, Argentina Tel Oficina: +54 223 4757008
> 
> 
> 
>       Yahoo! Cocina
> Recetas pr?cticas y comida saludable
> http://ar.mujer.yahoo.com/cocina/
> 	[[alternative HTML version deleted]]


From harlancampbell at gmail.com  Wed Mar  4 22:11:28 2009
From: harlancampbell at gmail.com (H c)
Date: Wed, 4 Mar 2009 16:11:28 -0500
Subject: [R-sig-ME] Numerical methods for the estimation of correlation
	parameters in lme()?
Message-ID: <222824550903041311j7dd2f23ey6ceaef46ec481948@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090304/611d00e0/attachment.pl>

From bolker at ufl.edu  Thu Mar  5 00:26:08 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 04 Mar 2009 18:26:08 -0500
Subject: [R-sig-ME] Numerical methods for the estimation of correlation
 parameters in lme()?
In-Reply-To: <222824550903041311j7dd2f23ey6ceaef46ec481948@mail.gmail.com>
References: <222824550903041311j7dd2f23ey6ceaef46ec481948@mail.gmail.com>
Message-ID: <49AF0E10.1060006@ufl.edu>

H c wrote:
> Hello
> 
> the standard lme() from the nlme library is equipped to estimate the
> correlation coefficient, phi, of a model with say AR(1)  correlation
> structure  for e.g..The estimation is done using numerical methods.  It is
> relatively fast.  Does anyone know what numerical methods are used?
> 
> Thanks,
> 
> Harlan
> 

 Pinheiro and Bates 2000 (Springer) discusses the methods at length.

  cheers
   Ben Bolker



From bolker at ufl.edu  Thu Mar  5 01:36:26 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 04 Mar 2009 19:36:26 -0500
Subject: [R-sig-ME] F test in lmer quasipoisson
In-Reply-To: <cd354c560903040222x17b8f57fscce1170f94062e57@mail.gmail.com>
References: <cd354c560903040222x17b8f57fscce1170f94062e57@mail.gmail.com>
Message-ID: <49AF1E8A.1030000@ufl.edu>


  Please don't cross-post to r-help and r-sig-mixed-models : it's
considered bad form (most of the experts on this subject are reading
r-sig-mixed-models anyway ...)

  Because this capability is not built in.
  If you can send or post your data, I can try to show an example.
More to the point, it's not clear that this will completely solve
your problem -- as the running discussion on this list indicates,
extrapolating from GLMs or LMMs to procedures for GLMMs is a little
dangerous (although it's often the best we can do).  In this case,
the hard part is probably picking the degrees of freedom for
the denominator of the F test ...

  cheers
   Ben Bolker


Julien Vezilier wrote:
> Hello !!
> 
> II'm trying to test for my fixed effects using an lmer with quasipoisson
> errors.
> 
> Since my lmer model is corrected for overdispersion using this kind of
> errors, I should use during model simplification in my Anovas *F test *and
> not *Chi square test* to compare two models. So I write:
> 
>> anova(model,model2,test="F")
> 
> but R keeps performing a Chi square instead of the F test I want...
> 
> Any ideas why ?
> 
> Thank you very much !!!
> Julien.
> 
> 
> 
> 
> 
> 
> 
> 


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From pravani at ucalgary.ca  Thu Mar  5 06:15:34 2009
From: pravani at ucalgary.ca (Pietro Ravani)
Date: Wed, 4 Mar 2009 22:15:34 -0700
Subject: [R-sig-ME] LMM covariance structure
Message-ID: <7B5B1699-E106-42C4-8B4C-AF99DD664E27@ucalgary.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090304/67e15cf7/attachment.pl>

From tanjas at u.washington.edu  Thu Mar  5 07:42:43 2009
From: tanjas at u.washington.edu (Tanja Srebotnjak)
Date: Wed, 4 Mar 2009 22:42:43 -0800
Subject: [R-sig-ME] Question on using 'weights' option in glmer
Message-ID: <36FA2D4F89496341B64E8AABEB4CD14309E4C8EF5A@sdc-mbx-01.exchange.washington.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090304/71139dfb/attachment.pl>

From fauna at pngp.it  Thu Mar  5 17:07:22 2009
From: fauna at pngp.it (Achaz von Hardenberg)
Date: Thu, 5 Mar 2009 17:07:22 +0100
Subject: [R-sig-ME] Advice on model selection procedure in glmer
Message-ID: <E2B199BC-E6F6-4F1A-BD71-E02C2051F4B7@pngp.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090305/3ad15302/attachment.pl>

From pravani at ucalgary.ca  Thu Mar  5 21:45:46 2009
From: pravani at ucalgary.ca (Pietro Ravani)
Date: Thu, 5 Mar 2009 13:45:46 -0700
Subject: [R-sig-ME] Erratum
Message-ID: <A50B1A36-BC93-49A0-B31A-945B2F5AD028@ucalgary.ca>

Sorry I meant "non zero" not "non negative", of course
Thank you
Pietro



From kingsfordjones at gmail.com  Fri Mar  6 01:49:40 2009
From: kingsfordjones at gmail.com (Kingsford Jones)
Date: Thu, 5 Mar 2009 17:49:40 -0700
Subject: [R-sig-ME] LMM covariance structure
In-Reply-To: <7B5B1699-E106-42C4-8B4C-AF99DD664E27@ucalgary.ca>
References: <7B5B1699-E106-42C4-8B4C-AF99DD664E27@ucalgary.ca>
Message-ID: <2ad0cc110903051649v58ac8f4bwbc83090e6409352d@mail.gmail.com>

On Wed, Mar 4, 2009 at 10:15 PM, Pietro Ravani <pravani at ucalgary.ca> wrote:
> Dear list
> I need confirmation or correction on this: how can the structure of
> the R vcv matrix of a LMM have non negative covariances if the
> residuals are independent given the random effects (and the co-
> variates)?

Hi Pietro,

Are you suggesting that i) error covariance within a given level of a
random effect should be negative, and ii) residuals are independent?
Neither is true.

> Covariance structures such as compound symmetry, AR 1,
> Toeplitz are specified by the user or are the marginal type of vcv
> implied/induced by the random effects.

In nlme such structures are added via the correlation argument.  See
?corClasses.

hope it helps,
Kingsford Jones


> Thank you
> Pietro
>
> ---------------------------------------------------------------------------------
> Dr. Pietro Ravani, MSc (Biostatistics), MD, FNCPI
> Associate Professor - Division of Nephrology
> Departments of Medicine & Community Health Sciences
> Faculty of Medicine - University of Calgary
> Foothills Medical Centre - AB Health Services - CHR
> 1403 - 29th Street N.W. (room C210N)
> Calgary, AB, Canada, T2N 2T9
> T 403-944-8168; F 403-944-2876; E pravani at ucalgary.ca
> ---------------------------------------------------------------------------------
> P Please consider before printing
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From David.Duffy at qimr.edu.au  Fri Mar  6 01:54:28 2009
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Fri, 6 Mar 2009 10:54:28 +1000 (EST)
Subject: [R-sig-ME] Advice on model selection procedure in glmer
In-Reply-To: <E2B199BC-E6F6-4F1A-BD71-E02C2051F4B7@pngp.it>
References: <E2B199BC-E6F6-4F1A-BD71-E02C2051F4B7@pngp.it>
Message-ID: <Pine.LNX.4.64.0903061035050.4868@orpheus.qimr.edu.au>

On Thu, 5 Mar 2009, Achaz von Hardenberg wrote:

> Dear list members,
>
> I am analyzing data on Golden eagle productivity (PROD = number of
> chicks fledging for each eagle couple each year) to see how it is
> influenced by various meteorological factors, Nearest neighbor
> distance (NND) and reproductive success the year before (back1) and
> two years before (back2). for each nest/couple (Nterr) we have one
> PROD measure for each year of the study (about 20 years). PROD varies
> between 0 and 3.

>    AIC   BIC logLik deviance
>  246.7 271.8 -116.4    232.7
> Random effects:
>  Groups Name        Variance  Std.Dev.
>  anno   (Intercept) 0.0056248 0.074999
>  Nterr  (Intercept) 0.0324398 0.180111
> Number of obs: 266, groups: anno, 24; Nterr, 22
>
> Fixed effects:
>              Estimate Std. Error
> (Intercept) -1.016941   0.279004
> back1       -0.067498   0.146796
> back2        0.181103   0.136243
> NND          0.029392   0.053093
> Pcrescita    0.003070   0.003008
>
> removed 	df AIC	   BIC	  Loglik
> FULL		7  246.73  271.82 -116.37
> Back1		6  257.79  279.68 -122.89
> Back2		6  259.03  280.97 -123.52
> NND		6  281.81  304.34 -134.91
> Pcrescita	6  276.86  299.16 -132.43
>
> I would really like to know if you think this model selection
> procedure is correct.

I don't pretend to have any deep understanding, but it seems plausible to 
me.  I am curious about your including Back1 and Back2 as fixed effects, 
given they are partly measures of the "couple fitness" latent variable 
NTerr.  Do you actually need Nterr if these are included?  And since PROD 
has so few levels, you can check whether Poisson is a good representation, 
and how your predictions are (crossvalidation), maybe compare 
results from gamm (in the mgcv package). Are you interested in 
just predicting PROD for next year?

David Duffy.
-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From jukka.corander at abo.fi  Fri Mar  6 10:44:22 2009
From: jukka.corander at abo.fi (Jukka Corander)
Date: Fri, 06 Mar 2009 11:44:22 +0200
Subject: [R-sig-ME] Error in mer_finalize(ans) : Downdated X'X is not
 positive definite, 9.
Message-ID: <49B0F076.8030208@abo.fi>

Dear all,

I tried to fit several models with lmer, some of which yield the 
following error message: Error in mer_finalize(ans) : Downdated X'X is 
not positive definite, 9.
The experimental data are as follows:
3 factors: Sexual_orientat (2 values), X_type (4 values), Y_type (4 values)
response lgRT is assumed Gaussian,
each participant has been measured in multiple trials (~190) and there 
are 23 participants in the study.
Design is not balanced, because the cases where X_type and Y_type have 
the same value (Y_type=1&X_type=1 etc) are excluded.
Any model without an interaction term Y_type*X_type can be fitted 
without problems using lmer, but all models with that term (including 
3-way interaction) yield the error message.
For instance,
FittedModel4 <- lmer(lgRT ~ Y_type*Sexual_orientat + 
X_type*Sexual_orientat + (1|Subject),data=arraydata)
works fine, but
FittedModel5 <- lmer(lgRT ~ Y_type*X_type*Sexual_orientat + 
(1|Subject),data=arraydata)
yields the error. If the repeated nature of the measurements is ignored 
and a 3-way interaction model is fitted using ordinary anova, it goes 
through, but, of course, estimates cannot be obtained for all 
interaction terms because they are lacking in the design. Is the error 
due to the inbalanced design, or is there something else I've 
ignored/done wrong? Any insights are highly appreciated!

Cheers, Jukka

-- 
Jukka Corander, Professor, PhD
Department of Mathematics
?bo Akademi University



From bates at stat.wisc.edu  Fri Mar  6 12:41:15 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 6 Mar 2009 05:41:15 -0600
Subject: [R-sig-ME] Error in mer_finalize(ans) : Downdated X'X is not
	positive definite, 9.
In-Reply-To: <49B0F076.8030208@abo.fi>
References: <49B0F076.8030208@abo.fi>
Message-ID: <40e66e0b0903060341o513646bci1ea60afcb13f6131@mail.gmail.com>

On Fri, Mar 6, 2009 at 3:44 AM, Jukka Corander <jukka.corander at abo.fi> wrote:
> Dear all,
>
> I tried to fit several models with lmer, some of which yield the following
> error message: Error in mer_finalize(ans) : Downdated X'X is not positive
> definite, 9.
> The experimental data are as follows:
> 3 factors: Sexual_orientat (2 values), X_type (4 values), Y_type (4 values)
> response lgRT is assumed Gaussian,
> each participant has been measured in multiple trials (~190) and there are
> 23 participants in the study.
> Design is not balanced, because the cases where X_type and Y_type have the
> same value (Y_type=1&X_type=1 etc) are excluded.
> Any model without an interaction term Y_type*X_type can be fitted without
> problems using lmer, but all models with that term (including 3-way
> interaction) yield the error message.
> For instance,
> FittedModel4 <- lmer(lgRT ~ Y_type*Sexual_orientat + X_type*Sexual_orientat
> + (1|Subject),data=arraydata)
> works fine, but
> FittedModel5 <- lmer(lgRT ~ Y_type*X_type*Sexual_orientat +
> (1|Subject),data=arraydata)
> yields the error. If the repeated nature of the measurements is ignored and
> a 3-way interaction model is fitted using ordinary anova, it goes through,
> but, of course, estimates cannot be obtained for all interaction terms
> because they are lacking in the design.

There is special code in the lm.fit function (that actual "workhorse"
that is called by lm) to handle a rank-deficient model matrix. A
specially modified version of the Linpack (not Lapack) dqrdc
subroutine is used to detect and deal with rank deficiency.  There is
no similar code available for the Cholesky decomposition that is used
in lmer so it just declares that the X'X matrix is rank deficient
after downdating.  (In this case it was rank deficient even before
downdating.)

>  Is the error due to the inbalanced
> design, or is there something else I've ignored/done wrong? Any insights are
> highly appreciated!



From bates at stat.wisc.edu  Fri Mar  6 12:52:11 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 6 Mar 2009 05:52:11 -0600
Subject: [R-sig-ME] LMM covariance structure
In-Reply-To: <7B5B1699-E106-42C4-8B4C-AF99DD664E27@ucalgary.ca>
References: <7B5B1699-E106-42C4-8B4C-AF99DD664E27@ucalgary.ca>
Message-ID: <40e66e0b0903060352w52823467n6f0eb51b9600a50e@mail.gmail.com>

On Wed, Mar 4, 2009 at 11:15 PM, Pietro Ravani <pravani at ucalgary.ca> wrote:
> I need confirmation or correction on this: how can the structure of
> the R vcv matrix of a LMM have non negative covariances if the
> residuals are independent given the random effects (and the co-
> variates)?

Could you be more explicit about what you mean by "the R vcv matrix of
a LMM" and why you are specifically interested in non-negative?  (Or
did you intend "non-zero"?)  In particular, does "R" in your question
refer to a model fit by R packages such as lme4 or nlme or are you
referring to a conditional variance-covariance structure of the
response given the random effects, which is sometimes written as R?

> Covariance structures such as compound symmetry, AR 1,
> Toeplitz are specified by the user or are the marginal type of vcv
> implied/induced by the random effects?



From e.sofianopoulou at newcastle.ac.uk  Fri Mar  6 15:37:07 2009
From: e.sofianopoulou at newcastle.ac.uk (Eleni Sofianopoulou)
Date: Fri, 6 Mar 2009 14:37:07 +0000
Subject: [R-sig-ME] offset in lme4
Message-ID: <6673AD38BA66C7458C16E445430A765941D1683D25@EXSAN03.campus.ncl.ac.uk>

Hello all

I have problem to use the offset in lme4 successfully. When I add the offset in my model it seems that it has no effect on the output. The models with and without the offset seem to produce the same results.

I have tried 2 different ways to add the offset on the model but I think the code I use is correct. I wonder whether anyone has an idea why this is and if can save me at this point.


Model A with offset:
>slb21offset<-lmer(log(slb+1)~pmavg0+trafsum0+conmonth+INC+EMP+EDU+(1|GP
>), offset=(logoffset_slb), slb21R)

Model B with offset:
slb21offsetb<-lmer(log(slb+1)~pmavg0+trafsum0+conmonth+INC+EMP+EDU+yrreg+offset(logoffset_slb)+(1|GP), slb21R)

Model without offset:
slb21<-lmer(log(slb+1)~pmavg0+trafsum0+conmonth+INC+EMP+EDU+(1|GP), slb21R)

I attach a file with my data.

Many thanks
Eleni

Eleni Sofianopoulou
Institute of Health and Society,
Newcastle University
William Leech Building
NE2 4HH
Tel:+44(0)191 246 4840



From robert.espesser at lpl-aix.fr  Fri Mar  6 18:17:57 2009
From: robert.espesser at lpl-aix.fr (espesser)
Date: Fri, 06 Mar 2009 18:17:57 +0100
Subject: [R-sig-ME] discrepancy between contrast and pooled levels
Message-ID: <49B15AC5.8070105@lpl-aix.fr>

Dear list members,

I am confused with the following results,
so I greatly appreciate any suggestions or remarks.

The structure of the data:
 > str(eglob_new)
'data.frame': 1200 obs. of 8 variables:
$ subject : Factor w/ 40 levels "letl01","letl03",..: 1 1 1 1 1 1 1 1 1 
1 ...

$ dlmscentre : num -2 -2 -2 -2 -1 -1 -1 -1 0 0 ...
$ nfcompo :    num 2 1 3 2 2 1 3 2 2 1 ...

$ conditionbis : Factor w/ 3 levels "e2b","e2a","e2c": 2 2 2 2 2 2 2 2 2 
2 ...
$ rlet : num 9 7 7 5 8 10 5 6 5 8 ...
$ rnlet : num 8 10 10 12 9 7 12 11 12 9 ...

$ conditionBLOCKVOY : Factor w/ 2 levels "e2ab","e2c": 1 1 1 1 1 1 1 1 1 
1 ...
$ conditionFEEDBACK : Factor w/ 2 levels "e2a","e2bc": 1 1 1 1 1 1 1 1 1 
1 ...


- a first group of 20 subjects experiments the both condition e2b and e2c
- an other group of 20 subjects only experiments the condition e2a

I think the subject labels are correct, ie there is no implicit nesting.

I fit probability of the "let" response with a logit mixed model .
the "best" model I obtained was :

glmer(cbind(rlet, rnlet) ~ dlmscentre*conditionbis + nfcompo + 
(dlmscentre | subject),family=binomial, data=eglob_new)
# model A

 > summary(eglob_new_conditionbis_leger.glmer)
Generalized linear mixed model fit by the Laplace approximation
Formula: cbind(rlet, rnlet) ~ (dlmscentre + conditionbis)^2 + nfcompo 
+      (dlmscentre | subject)
   Data: eglob_new
  AIC  BIC logLik deviance
 2083 2134  -1032     2063
Random effects:
 Groups  Name        Variance Std.Dev. Corr 
 subject (Intercept) 0.22676  0.47619       
         dlmscentre  0.32139  0.56691  0.338
Number of obs: 1200, groups: subject, 40

Fixed effects:
                           Estimate Std. Error z value Pr(>|z|)   
(Intercept)                -0.01225    0.11792  -0.104    0.917   
dlmscentre                 -0.85908    0.12867  -6.677 2.44e-11 ***
conditionbise2a             0.02483    0.15577   0.159    0.873   
conditionbise2c            -0.03907    0.03666  -1.066    0.286   
nfcompo                     0.13241    0.02155   6.144 8.03e-10 ***
dlmscentre:conditionbise2a  0.26804    0.18213   1.472    0.141   
dlmscentre:conditionbise2c  0.17383    0.02828   6.146 7.94e-10 ***


the results are plausible;
there is a decreasing slope (dlmscentre); slope for e2c differs from the 
slope of e2b.

More specifically, I'm interested to test the hypothesis:
Is there a slope difference between e2c vs (e2a pooled with e2b) ?


I first recoded conditionbis into a new factor: conditionBLOCKVOY

#model BLOCKVOY
glmer( cbind(rlet, rnlet) ~ dlmscentre * conditionBLOCKVOY +
nfcompo+(dlmscentre | subject),family=binomial, data=eglob_new)

summary(eglob_new_conditionBLOCKVOY_leger.glmer)
Generalized linear mixed model fit by the Laplace approximation
Formula: cbind(rlet, rnlet) ~ dlmscentre * conditionBLOCKVOY + nfcompo 
+      (dlmscentre | subject)
   Data: eglob_new
  AIC  BIC logLik deviance
 2082 2122  -1033     2066
Random effects:
 Groups  Name        Variance Std.Dev. Corr 
 subject (Intercept) 0.22843  0.47795       
         dlmscentre  0.34013  0.58321  0.336
Number of obs: 1200, groups: subject, 40

Fixed effects:
                                  Estimate Std. Error z value Pr(>|z|)   
(Intercept)                      0.0001614  0.0891725   0.002    0.999   
dlmscentre                      -0.7257465  0.0936049  -7.753 8.95e-15 ***
conditionBLOCKVOYe2c            -0.0371947  0.0363233  -1.024    0.306   
nfcompo                          0.1324147  0.0215497   6.145 8.02e-10 ***
dlmscentre:conditionBLOCKVOYe2c  0.1702476  0.0281450   6.049 1.46e-09 ***


The interaction is significant: slope for e2c is different from the 
slope for pooled (e2a ,e2b)

b)
I test the hypothesis by setting a specific contrast for conditionbis .

eglob_new$conditionbis -> eglob_new$conditionBLOCKVOYcontr
ginv( cbind(1,1,-2)) -> contrasts(eglob_new$conditionBLOCKVOYcontr)
 > contrasts(eglob_new$conditionBLOCKVOYcontr)
[,1] [,2]
e2b 0.1666667 -7.071068e-01
e2a 0.1666667 7.071068e-01
e2c -0.3333333 -9.877082e-17

# model BLOCKVOYcontr

 > summary(eglob_new_conditionBLOCKVOYcontr_leger.glmer)
Generalized linear mixed model fit by the Laplace approximation
Formula: cbind(rlet, rnlet) ~ dlmscentre * conditionBLOCKVOYcontr + 
nfcompo + (dlmscentre | subject)
   Data: eglob_new
  AIC  BIC logLik deviance
 2083 2134  -1032     2063
Random effects:
 Groups  Name        Variance Std.Dev. Corr 
 subject (Intercept) 0.22676  0.47619       
         dlmscentre  0.32138  0.56691  0.338
Number of obs: 1200, groups: subject, 40

Fixed effects:
                                   Estimate Std. Error z value Pr(>|z|)   
(Intercept)                        -0.01699    0.09165  -0.185    0.853   
dlmscentre                         -0.71179    0.09540  -7.461 8.60e-14 ***
conditionBLOCKVOYcontr1             0.10297    0.16385   0.628    0.530   
conditionBLOCKVOYcontr2             0.01756    0.11014   0.159    0.873   
nfcompo                             0.13241    0.02155   6.144 8.03e-10 ***
dlmscentre:conditionBLOCKVOYcontr1 -0.07966    0.18615  -0.428    0.669   
dlmscentre:conditionBLOCKVOYcontr2  0.18949    0.12878   1.471    0.141   
---

Correlation of Fixed Effects:
            (Intr) dlmscn cBLOCKVOY1 cBLOCKVOY2 nfcomp d:BLOCKVOY1
dlmscentre   0.291                                               
cnBLOCKVOY1 -0.247 -0.097                                        
cnBLOCKVOY2 -0.262 -0.101  0.896                                 
nfcompo     -0.469 -0.004  0.000      0.000                      
d:BLOCKVOY1 -0.089 -0.299  0.294      0.316      0.000           
d:BLOCKVOY2 -0.091 -0.308  0.307      0.320      0.001  0.953

dlmscentre:conditionBLOCKVOYcontr1, the interest interaction is not 
significant.
the slope for (e2a+e2b) cannot be distinguished from the slope for e2c

I did'nt expect  the same results for the model BLOCKVOY and the model 
BLOCKVOYcontr,
but not such an opposite result .

There is a high correlation (0.953) between the two interaction 
coefficients,
which means this last model is badly specified ?
Do I misunderstand something about contrasts ?

 > sessionInfo()
R version 2.8.1 (2008-12-22)
i386-pc-mingw32
......
other attached packages:
[1] lme4_0.999375-28 Matrix_0.999375-21 lattice_0.17-17

Sorry for this long email, and thank you in advance
Regards

R. Espesser
Laboratoire Parole et Langage,CNRS et Universit? de Provence
13100 Aix-en-provence, France



From pravani at ucalgary.ca  Sat Mar  7 16:14:19 2009
From: pravani at ucalgary.ca (Pietro Ravani)
Date: Sat, 7 Mar 2009 08:14:19 -0700
Subject: [R-sig-ME] LMM covariance structure
Message-ID: <08E6D636-545B-4AB8-87AD-49C81400B763@ucalgary.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090307/4866c8b9/attachment.pl>

From bates at stat.wisc.edu  Sun Mar  8 17:32:07 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 8 Mar 2009 11:32:07 -0500
Subject: [R-sig-ME] LMM covariance structure
In-Reply-To: <08E6D636-545B-4AB8-87AD-49C81400B763@ucalgary.ca>
References: <08E6D636-545B-4AB8-87AD-49C81400B763@ucalgary.ca>
Message-ID: <40e66e0b0903080932y51d39fd7ya5c8696cfd29014c@mail.gmail.com>

On Sat, Mar 7, 2009 at 10:14 AM, Pietro Ravani <pravani at ucalgary.ca> wrote:
> Dear Doug
> I sent my reply twice (* copied below) but I cannot see it
> Can you pls check?

The first copy was sent to me only and the second copy was held for
list moderator's approval because you are not enrolled on the mailing
list or not at the same email address from which you posted.  To cut
down on spam we restrict postings to those email addresses that are
enrolled.

Many people send questions or replies to me personally and I try to
forward them to the mailing list, asking permission before I do so, so
that others can provide answers or follow-up questions and so that
there is an archival record.  I enjoy corresponding with people about
these questions but I don't always have the time to do so and, quite
frankly, personalized tutoring is not the best use of my time.  My
long-suffering editor would prefer that I spend my time writing a book
manuscript so that many people can learn about such models and how to
use the lme4 package simultaneously and I do agree that it would be a
more efficient approach.

> (*)
> Yes, I was referring to the conditional variance-covariance structure
> of the response given the random effects, which is referred to as R in
> the book of West, Welch, Galecki that I found mathematically
> affordable (I am studying the R language now, and trying to learn more
> about correlated data for study design purposes).

It would have helped to state that rather than assuming that everyone
uses the notation and terminology from that particular book.  There
are many different ways or writing mixed-effects models.

> And yes, I meant "non-zero", as stated in the erratum.

> Looking at the output of the getVarCov() function in R, I see that
> choosing "conditional" as "type" I obtain a matrix with the estimate
> of the error "variance" on the diagonal (which can be heterogeneous,
> i.e. vary within cluster/group by values of cluster level co-variates)
> and all "zeros" off the diagonal. ?I thought this is what LMM do:
> explaining the group heterogeneity in the data (and the resulting
> correlation in the responses) through splitting the random portion of
> the statistical model into two layers, the random effects and the
> random errors. ?These random errors - conditioning on the random
> effects - I thought were normally distributed with zero mean and some
> variance sigma2 (on the diagonal of the R matrix) and independent
> (thus with zero co-variances off the diagonal of the R matrix).
> Typing "marginal" in the above cmd tells R to give me what I thought
> it was the combination (marginal model implied by the LMM) of the 2
> VCV matrices, D (matrix of the random effects parameters) and R
> (matrix of the random error parameter). ?The fact that different
> structures (of the R matrix?) - mentioned in the previous emails
> (compound symmetry, AR 1, Toeplitz, etc) - can be specified in the
> lme() function via the correlation argument confuses me, unless they
> refer to the resulting marginal model matrix (not the R matrix
> conditional on the random effects). ?I have the impression I am lost
> (although I know I have much more to learn).

As explained in the posting guide for the R-help mailing list,
providing a small self-contained example will often lead to more
productive responses.  A clearly stated question, including such items
as which package you have attached, is easier to answer.

Reading between the lines and given the information that you are
adopting the terminology of the West, Welch and Galeki book I imagine
that you are viewing the nlme package the lme function for fitting
linear mixed effects models as "R".  R is a language and environment.
Base R includes several functions and a set of required packages.  As
far as I know the only function there that can fit mixed-effects
models is anova and that can only be applied to certain balanced
designs.  There are now several thousand contributed packages in R,
many of which provide functions to fit mixed-effects models.  The nlme
package is one, the lme4 package is another.  Most discussions on this
mailing list are related to the lme4 package and the lmer function but
we do also discuss nlme and lme (and you did mention that you were
using lme() so we can pick that up).

I really don't remember what getVarCov returns in complex cases and I
don't want to drop everything to go back and figure it out.

> Directions re math friendly sources / learning tools (especially using
> R) would be very appreciated of course

Well, Jose Pinheiro and I did write a book "Mixed-effects Models in S
and S-PLUS" (Springer, 2000) that does cover the mathematical theory
as well as the computational details.  I'm not sure what "math
friendly" means so I can't judge whether it fits that description.
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From kingsfordjones at gmail.com  Sun Mar  8 21:55:42 2009
From: kingsfordjones at gmail.com (Kingsford Jones)
Date: Sun, 8 Mar 2009 13:55:42 -0700
Subject: [R-sig-ME] LMM covariance structure
In-Reply-To: <08E6D636-545B-4AB8-87AD-49C81400B763@ucalgary.ca>
References: <08E6D636-545B-4AB8-87AD-49C81400B763@ucalgary.ca>
Message-ID: <2ad0cc110903081355n23df4c8r3e711e7e6bb2655a@mail.gmail.com>

Hi Pietro

First I'll second the nomination for Pinheiro and Bates as the best
resource for fitting LMMs in R/S.  Although there are theoretical
chapters that require a good understanding of matrix algebra, most of
the book consists of clear examples with code and graphics.  Of
course, as you seem to be aware, fitting mixed models without
understanding some theory can be hazardous.

Some more comments below...

On Sat, Mar 7, 2009 at 8:14 AM, Pietro Ravani <pravani at ucalgary.ca> wrote:
> Dear Doug
> I sent my reply twice (* copied below) but I cannot see it
> Can you pls check?
> Thank you
> Pietro
>
> (*)
> Yes, I was referring to the conditional variance-covariance structure
> of the response given the random effects, which is referred to as R in
> the book of West, Welch, Galecki that I found mathematically
> affordable (I am studying the R language now, and trying to learn more
> about correlated data for study design purposes). ?And yes, I meant
> "non-zero", as stated in the erratum.

I didn't notice the erratum before my last response -- the 'non-zero'
question is more clear.  The "conditional variance-covariance
structure of the response given the random effects" is complicated by
the structure of the random effects (i.e. Var(y_i) = Z_i \Psi Z'_i  +
\Sigma, where y_i is the vector of responses for the i^th subject,
Z_i is the random effects design matrix for the i^th subject, \Psi is
the random effects covariance matrix, and \Sigma is the within-subject
error covariance matrix).  So, to keep things "simple" I'll focus on
the within-subject error covariance, which can be decomposed into
\sigma^2 VCV, where V is diagonal with possibly non-constant error
standard deviations on the diagonal (this is structured by the
'weights' argument to modeling functions in the nlme package), and C
contains the within-subject correlation structure (1's on the diagonal
and off-diagonal structured by the parameters associated with one of
the stuctures seen in ?corClasses, or one supplied by the user).  So,
the 'correlation' argument to nlme modeling functions provides a tool
for structuring the non-zero off-diagonals that you were asking about.

A couple things about non-zero off-diagonals to note:

i) IIRC, the off-diagonal structure does not have to be described
within subjects (e.g. if you had observations in space you might have
correlation = corExp(form=~lat + lon), OR you could fit the
exponential spatial structure within, e.g., states, with correlation =
corExp(form=~lat + lon|state).

ii) even without error covariance structure, the response
off-diagonals are non-zero when there are random effects.  For
example, if there is a subject random intercept \sigma_b^2 and within
subject errors are assumed independent (i.e. \Sigma = \sigma^2 I, then
Var(y_i) contains \sigma_b^2 + \sigma^2 on the diagonal and \sigma_b^2
on the off-diagonal.  Thus a compound symmetric correlation structure
has been induced, where within subjects observations are assumed to
have a constant correlation \rho = \sigma_b^2 / (\sigma_b^2 +
\sigma^2).

>
> Looking at the output of the getVarCov() function in R, I see that
> choosing "conditional" as "type" I obtain a matrix with the estimate
> of the error "variance" on the diagonal (which can be heterogeneous,
> i.e. vary within cluster/group by values of cluster level co-variates)
> and all "zeros" off the diagonal. ?I thought this is what LMM do:
> explaining the group heterogeneity in the data (and the resulting
> correlation in the responses) through splitting the random portion of
> the statistical model into two layers, the random effects and the
> random errors. ?These random errors - conditioning on the random
> effects - I thought were normally distributed with zero mean and some
> variance sigma2 (on the diagonal of the R matrix) and independent
> (thus with zero co-variances off the diagonal of the R matrix).

As explained above, the "weights" argument frees you from the
restriction of \sigma^2 on the diagonal, and the "correlation"
argument frees you from independence conditional on the level of the
random effect.

> Typing "marginal" in the above cmd tells R to give me what I thought
> it was the combination (marginal model implied by the LMM) of the 2
> VCV matrices, D (matrix of the random effects parameters) and R
> (matrix of the random error parameter). ?The fact that different
> structures (of the R matrix?) - mentioned in the previous emails
> (compound symmetry, AR 1, Toeplitz, etc) - can be specified in the
> lme() function via the correlation argument confuses me, unless they
> refer to the resulting marginal model matrix (not the R matrix
> conditional on the random effects). ?I have the impression I am lost
> (although I know I have much more to learn).
>
> Directions re math friendly sources / learning tools (especially using
> R) would be very appreciated of course

A few more R/S resources: Julian Faraway's Extending the Linear Model
with R, Venables and Ripley's MASS, the mixed-models appendix to John
Fox's Companion to Applied Regression, and many documents that show up
if you google:

mixed OR multilevel lme OR nlme OR lmer filetype:pdf


hope that helps,

Kingsford Jones

>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From steibelj at msu.edu  Mon Mar  9 14:04:57 2009
From: steibelj at msu.edu (Juan Pedro Steibel)
Date: Mon, 09 Mar 2009 09:04:57 -0400
Subject: [R-sig-ME] fixing variance components ratios
Message-ID: <49B513F9.9060704@msu.edu>

Hello all,

I have a question on using lmer with know variance component ratios:

Background:
I want to fit a model of the type: y=X b+Z_1 u_1+...+Z_Q u_Q+e,
with u_q~N(0,V_q I), q=1...Q and e~N(0,V_e).
The values of these variance components are unknown, but we know the 
values of the variance ratios: R_q=V_q/V_e for q=1...Q.

Question:
Is there a way to specify this in lmer, such that lmer uses the variance 
ratios (R_q) and produces estimates of V_e, blue(b) and blup(u)?

Comment:
I know that given those VC ratios the problem could be solved with GLS 
in a single iteration, but I was hoping to use the lmer computational 
machinery if this is feasible.

Thanks in advance!
Cheers
JP



-- 
=============================
Juan Pedro Steibel

Assistant Professor
Statistical Genetics and Genomics

Department of Animal Science & 
Department of Fisheries and Wildlife

Michigan State University
1205-I Anthony Hall
East Lansing, MI
48824 USA 

Phone: 1-517-353-5102
E-mail: steibelj at msu.edu



From m.k.oliver at abdn.ac.uk  Mon Mar  9 16:48:36 2009
From: m.k.oliver at abdn.ac.uk (Oliver, M. K.)
Date: Mon, 9 Mar 2009 15:48:36 +0000
Subject: [R-sig-ME] Help: making predictions for applied problems using glmm
Message-ID: <B9D1301370916C44B5874AF340C18B9B7F9129E6C3@VMAILB.uoa.abdn.ac.uk>


Dear R users,

My query concerns how best to make inferences for applied problems from the outcomes of a mixed modelling approach. I am an ecologist and have been modelling the presence or absence of a species of vole in patches of suitable habitat in upland areas in relation to a number of biologically meaningful covariates. My sampling design includes 310 vole habitat patches within 9 river subcatchments and I have included subcatchment as a random effect in order to incorporate this structure in the data. I have used the information-theoretic approach with model weighting and averaging to make inferences about model selection and the consistency of parameter estimates. However, I am faced with one outstanding issue: as my question is an applied one, it is highly desirable to predict the probability of a vole habitat patch being occupied given a particular set of covariate values. This is easily done in glm using the predict() function, but I am aware that no such function exist for glmm.

However, would it be sound to take a similar approach to the coefficients and se for glmm i.e by back-transforming and applying to a range of x?

Or, would such an approach fail to take into account the model structure based on the random effect?

If so, would a reasonable approach be to estimate coefficients from re-samples of the data whilst maintaining the model structure? Predicted values could then be presented with confidence intervals that reflected variation in the structure of the data.

If not, what (if there is such a thing) would be the appropriate way to make predictions from glmm for applied problems such as this?

I hope I have been reasonably clear and am not being too ignorant. Any advice or comments would be much appreciated.

Many thanks

Matt

Dr Matthew Oliver
Research Fellow
School of Biological Sciences
University of Aberdeen
Zoology building
Tillydrone Avenue
Aberdeen AB24 2TZ
UK
tel + 44(0)1224 272789


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From datkins at u.washington.edu  Mon Mar  9 18:07:18 2009
From: datkins at u.washington.edu (David Atkins)
Date: Mon, 09 Mar 2009 10:07:18 -0700
Subject: [R-sig-ME] predict.fixef? -- Help: making predictions for applied
 problems using glmm
Message-ID: <49B54CC6.1050002@u.washington.edu>


Perhaps I can tag on to Matt's question.  Back when I used lme() for my 
analyses, I would often use:

predict(lmeobj, newdata, level=0)

to get predictions from the model, only using the fixed-effects.  This 
is quite useful with interactions, nonlinear terms, and just generally 
for understanding the results of the model (as I'm sure many listmembers 
know...).

With lmer():

1. There have been a handful of emails over the years noting that it is 
particularly tricky to know what to do with *random-effects* in 
predictions.  Frankly, I'm not concerned about this for my own work, as 
I invariably was looking at predictions based on fixed-effects only.

2. There have also been emails showing how to "roll you own" 
predictions, using newdata, model.matrix, and multiplying through by the 
vector of fixed-effects coefficients.

Thus, #2 can do what I'd like (and I've used it), though I've also 
gotten bit once or twice by not having the columns line-up correctly 
between coefficients and newdata (okay, I concede operator error...). 
But, it sure would be handy to have a predict(..., level=0), or perhaps 
predict.fixef() command for use with lmer() objects.

I did actually look at the predict.lme code at one point, but I fear 
it's a bit beyond my limited coding talents...

[And, let me end by saying I'm reticent to ever suggest that Doug (or 
someone else) "ought" to code this up, because I'm acutely aware of how 
much work Doug has done to benefit the lmer() users.  I thought I'd 
throw out this request b/c Matt had suggested something similar, and 
perhaps someone has even cooked up such code already...]

cheers, Dave


-- 
Dave Atkins, PhD
Research Associate Professor
Center for the Study of Health and Risk Behaviors
Department of  Psychiatry and Behavioral Science
1100 NE 45th Street, Suite 300
Seattle, WA  98105
206-616-3879
datkins at u.washington.edu



Dear R users,

My query concerns how best to make inferences for applied problems from 
the outcomes of a mixed modelling approach. I am an ecologist and have 
been modelling the presence or absence of a species of vole in patches 
of suitable habitat in upland areas in relation to a number of 
biologically meaningful covariates. My sampling design includes 310 vole 
habitat patches within 9 river subcatchments and I have included 
subcatchment as a random effect in order to incorporate this structure 
in the data. I have used the information-theoretic approach with model 
weighting and averaging to make inferences about model selection and the 
consistency of parameter estimates. However, I am faced with one 
outstanding issue: as my question is an applied one, it is highly 
desirable to predict the probability of a vole habitat patch being 
occupied given a particular set of covariate values. This is easily done 
in glm using the predict() function, but I am aware that no such 
function exist for glmm.

However, would it be sound to take a similar approach to the 
coefficients and se for glmm i.e by back-transforming and applying to a 
range of x?

Or, would such an approach fail to take into account the model structure 
based on the random effect?

If so, would a reasonable approach be to estimate coefficients from 
re-samples of the data whilst maintaining the model structure? Predicted 
values could then be presented with confidence intervals that reflected 
variation in the structure of the data.

If not, what (if there is such a thing) would be the appropriate way to 
make predictions from glmm for applied problems such as this?

I hope I have been reasonably clear and am not being too ignorant. Any 
advice or comments would be much appreciated.

Many thanks

Matt

Dr Matthew Oliver
Research Fellow
School of Biological Sciences
University of Aberdeen
Zoology building
Tillydrone Avenue
Aberdeen AB24 2TZ
UK
tel + 44(0)1224 272789



From gentz at ensat.fr  Tue Mar 10 16:56:56 2009
From: gentz at ensat.fr (Laurent Gentzbittel)
Date: Tue, 10 Mar 2009 16:56:56 +0100
Subject: [R-sig-ME] syntax for indicating fixed covariates in nlmer ?
Message-ID: <49B68DC8.30601@ensat.fr>

Dear all,

I would like to use the nlmer for fitting logistic curves to disease
index curves. I'm typically comparing different plant lines (several
plants from each line are subjected to a pathogen) and would like to
test if the 'population' (fixed effects) parameters of the curves are
the same between the lines.
I need to specify a random effect for each parameter to account for
plant-to-plant variability and a fixed effect for some parameter to
account for 'general' differences among lines.
I previously used the nlme function of the nlme package and was able to
specify both fixed and random effects for each parameter, including a
fixed covariate 'line' for example.

I would like to switch to the lme4 package, but I was unable to find how
to specify fixed covariates (fixed effects) in the formula.
A search in the different mail lists and Googl'ing  was also unsuccessful.

Does anybody knows how to write it ?

Regards

-- 
Prof. L. GENTZBITTEL            Phone: +33 (0)5 62 19 35 96
INP - ENSAT                     Fax  : +33 (0)5 62 19 35 89
Symb. & Path. Plantes   - IFR40
18, Chemin de Borde Rouge - Auzeville Tolosane
31326 CASTANET TOLOSAN - FRANCE
E-mail:                         gentz at ensat.fr
Plant genomes synteny  :       
http://bioinfo.genopole-toulouse.prd.fr/iccare/
Linux user #301231



From wouter.vahl at helsinki.fi  Tue Mar 10 16:50:19 2009
From: wouter.vahl at helsinki.fi (vahl)
Date: Tue, 10 Mar 2009 17:50:19 +0200
Subject: [R-sig-ME] response residuals from a logistic mixed regression
	model using lmer
Message-ID: <49B68C3B.7010909@helsinki.fi>

Dear,

Using the lmer function, I have fitted the following logistic mixed 
regression model on an experimental data set containing three fixed 
factors (A, B & C) and two random variables (day & cage):

model1 <- lmer(cbind(k1, k2) ~A*B*C + (1|day) + 
(1|cage),family=binomial(logit),data=data),

where k1 is the number of successes and k2 the number of failures in a 
trial.

Presenting all data in the same figure turns out to result in a graph 
that is hard to read. Therefore, I am now planning to present my data in 
two graphs, one containing information on A and B, the other containing 
information on B & C.

In presenting my data regarding factor A and B visually, I would like to 
take out the (experimentally imposed) variation explained by the factor 
C and the two random block factors. To do so, I need the raw, response 
residuals, that is the residuals on the logit scale. I have studied the 
help-file of the lmer function as well as that of the mer class, and I 
have read the question posted by Andy Fugard on this list as well as the 
reply of Douglas Bates (both posted at 14 march 2008), but am not able 
to find the response residuals. Any advice would be kindly appreciated.

Wouter Vahl



From dieter.menne at menne-biomed.de  Tue Mar 10 17:51:14 2009
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 10 Mar 2009 16:51:14 +0000 (UTC)
Subject: [R-sig-ME] syntax for indicating fixed covariates in nlmer ?
References: <49B68DC8.30601@ensat.fr>
Message-ID: <loom.20090310T164924-261@post.gmane.org>

Laurent Gentzbittel <gentz at ...> writes:

> I would like to switch to the lme4 package, but I was unable to find how
> to specify fixed covariates (fixed effects) in the formula.

I am facing the same problem. Assuming I want to run fm2Quin.nlme in PB, page
381 in lme4: How would the syntax look like

Dieter



From bolker at ufl.edu  Wed Mar 11 12:20:32 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 11 Mar 2009 07:20:32 -0400
Subject: [R-sig-ME] syntax for indicating fixed covariates in nlmer ?
In-Reply-To: <49B68DC8.30601@ensat.fr>
References: <49B68DC8.30601@ensat.fr>
Message-ID: <49B79E80.2090205@ufl.edu>

Laurent Gentzbittel wrote:
> Dear all,
> 
> I would like to use the nlmer for fitting logistic curves to disease
> index curves. I'm typically comparing different plant lines (several
> plants from each line are subjected to a pathogen) and would like to
> test if the 'population' (fixed effects) parameters of the curves are
> the same between the lines.
> I need to specify a random effect for each parameter to account for
> plant-to-plant variability and a fixed effect for some parameter to
> account for 'general' differences among lines.
> I previously used the nlme function of the nlme package and was able to
> specify both fixed and random effects for each parameter, including a
> fixed covariate 'line' for example.
> 
> I would like to switch to the lme4 package, but I was unable to find how
> to specify fixed covariates (fixed effects) in the formula.
> A search in the different mail lists and Googl'ing  was also unsuccessful.
> 

   The fixed effects formula looks just like that in nlme.  The random
effects are specified differently -- where nlme would use (for example)

fixed=response~(something),random=~1|plant+a|plant+b|plant

nlmer would use

formula = response~(something)+(1|plant)+(a|plant)+(b|plant)

  Ben Bolker


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From bates at stat.wisc.edu  Wed Mar 11 13:30:38 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 11 Mar 2009 07:30:38 -0500
Subject: [R-sig-ME] syntax for indicating fixed covariates in nlmer ?
In-Reply-To: <49B79E80.2090205@ufl.edu>
References: <49B68DC8.30601@ensat.fr> <49B79E80.2090205@ufl.edu>
Message-ID: <40e66e0b0903110530k53718b86id54afcd5c3228598@mail.gmail.com>

On Wed, Mar 11, 2009 at 6:20 AM, Ben Bolker <bolker at ufl.edu> wrote:
> Laurent Gentzbittel wrote:
>> Dear all,
>>
>> I would like to use the nlmer for fitting logistic curves to disease
>> index curves. I'm typically comparing different plant lines (several
>> plants from each line are subjected to a pathogen) and would like to
>> test if the 'population' (fixed effects) parameters of the curves are
>> the same between the lines.
>> I need to specify a random effect for each parameter to account for
>> plant-to-plant variability and a fixed effect for some parameter to
>> account for 'general' differences among lines.
>> I previously used the nlme function of the nlme package and was able to
>> specify both fixed and random effects for each parameter, including a
>> fixed covariate 'line' for example.
>>
>> I would like to switch to the lme4 package, but I was unable to find how
>> to specify fixed covariates (fixed effects) in the formula.
>> A search in the different mail lists and Googl'ing ?was also unsuccessful.
>>
>
> ? The fixed effects formula looks just like that in nlme. ?The random
> effects are specified differently -- where nlme would use (for example)
>
> fixed=response~(something),random=~1|plant+a|plant+b|plant
>
> nlmer would use
>
> formula = response~(something)+(1|plant)+(a|plant)+(b|plant)

I think Laurent and Dieter might have been asking a different
question.  In a nonlinear mixed-effects model the nonlinear model
function incorporates covariates and "nonlinear model parameters".
For example, the four-parameter logistic model is a function of a
covariate, often something like log-dose, and four parameters that
could be the asymptote on the left, the asymptote on the right, the
midpoint and a scale parameter.

example(SSfpl)

produces a plot showing these.

I think the question was how to express an NLMM in which one of these
parameters, say the midpoint, xmid, incorporates the effect of a
covariate like treatment group.

If that is the question then the answer is "not easily at present".
The development version of nlmer, in the branches/allcoef section of
the SVN repository, has the capability of doing that.  That's the good
news.  The bad news is that the syntax of the formula has changed a
bit and I don't want to release the development branch until I can
resolve problems with GLMMs in that branch.

I am having some bizarre problems with GLMMs there - the sort of
problem that will seem trivial once I know the answer but right now is
very frustrating.  For some reason the current code fits Poisson GLMMs
like a charm and diverges on Bernoulli GLMMs and binomial GLMMs.

What I will do is to polish up the documentation of the nlmer function
over the next few days so the brave (or foolhardy, depending on your
point of view) user can fit those models.  Then I will try to get the
binomial GLMMs happy again.



From christina.bogner at uni-bayreuth.de  Thu Mar 12 08:49:27 2009
From: christina.bogner at uni-bayreuth.de (Christina Bogner)
Date: Thu, 12 Mar 2009 08:49:27 +0100
Subject: [R-sig-ME] Variable transformation and back transformation
Message-ID: <49B8BE87.3080809@uni-bayreuth.de>

Dear all,
 
I have fitted a couple of mixed-effects models to environmental data 
(chemical and physical soil parameters) with log-transformed dependent 
variables. I tried generalized mixed-models, but the results were not 
satisfactory (probably because I am a soil scientist and not a 
statistician ;-)) Now, as log of concentrations are ecologically not 
very informative, I would like to back-transform my model parameters. 
Taking a Gaussian linear mixed-model:
 
log(Mg2)=intercept+beta1*Silt+beta2*Soil.depth+beta3*Flow.region+b1*Plot+b2*/Soil.Depth%in%Plot+var 

where Mg2 is the concentration of magnesium, betas are fixed-effects and 
bs random ones. All independent variables except Silt are factors; Silt 
is continuous.
 
I would write:
Mg2=exp(intercept+beta1*mean(Silt in respective 
Soil.Depth)+beta3*Flow.region+estimate of b1*Plot + estimate of 
b2*/Soil.Depth%in%Plot+0.5*var)
to back-transform to the original scale on the Soil.Depth-level.
 
To back-transform the fixed-effects only, I would drop the estimates of 
the random-effects:
Mg2=exp(intercept+beta1*mean(Silt in respective 
Soil.Depth)+beta3*Flow.region+ 0.5*var)
 
 
This approach treats the estimated random effects as dummies, not as an 
additional variance. Is this right?

Thanks a lot for your help

Christina Bogner

From dieter.menne at menne-biomed.de  Thu Mar 12 11:39:21 2009
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 12 Mar 2009 10:39:21 +0000 (UTC)
Subject: [R-sig-ME] Covariables in nlmer
Message-ID: <loom.20090312T103252-793@post.gmane.org>

I am struggling with covariables in nlmer; I tried to get something 
that worked well in nlme in nlmer, and was stuck.

The last I noted was 

<http://www.nabble.com/Re:-covariates-in-nlmer-function-p12754515.html>

so I waited a few months after for the dust to settle.

Below my examples: one includes the variable <treat>, the other
leaves it out. Both functions converge, but the results are 
EXACTLY the same, as if the variable treat was simply not parsed at all.
Even if this is a misunderstanding of my side how to specify covariables, 
an error message would be more welcome than a black hole.

Dieter

----------------------------
R version 2.8.1 (2008-12-22) 
i386-pc-mingw32 

locale:
LC_COLLATE=German_Germany.1252;LC_CTYPE=German_Germany.1252;

LC_MONETARY=German_Germany.1252;LC_NUMERIC=C;LC_TIME=German_Germany.1252

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base     

other attached packages:
[1] lme4_0.999375-28   Matrix_0.999375-21 lattice_0.17-20   

loaded via a namespace (and not attached):
[1] grid_2.8.1  tools_2.8.1
--------------------------


# gastemptnlmer.r
library(lme4)

ge = read.table("http://www.menne-biomed.de/gastempt/gastempt5.csv", 
  header=TRUE)
ge$subj = as.factor(ge$subj)

EmptInit= function(mCall,LHS,data){ # dummy, not explicitely used
  stop("Should not be called")
}

# Standard LinExp model for gastric emptying
SSEmptLinExp=selfStart(~v0*(1+kappa*t/tempt)*exp(-t/tempt),
  initial=EmptInit, parameters= c("v0","kappa","tempt"))


start = list(fixef=c(v0=534,kappa=1,tempt=60))
ge0.nlmer = nlmer(
    v~SSEmptLinExp(t,v0,kappa,tempt)~ treat+(v0+kappa+tempt|subj),
    data=ge,  start=start)
summary(ge0.nlmer)

ge1.nlmer = nlmer(
    v~SSEmptLinExp(t,v0,kappa,tempt)~ (v0+kappa+tempt|subj),
    data=ge,  start=start)
summary(ge1.nlmer)



From dieter.menne at menne-biomed.de  Thu Mar 12 11:42:12 2009
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 12 Mar 2009 10:42:12 +0000 (UTC)
Subject: [R-sig-ME] syntax for indicating fixed covariates in nlmer ?
References: <49B68DC8.30601@ensat.fr> <49B79E80.2090205@ufl.edu>
	<40e66e0b0903110530k53718b86id54afcd5c3228598@mail.gmail.com>
Message-ID: <loom.20090312T104012-371@post.gmane.org>

Douglas Bates <bates at ...> writes:

> What I will do is to polish up the documentation of the nlmer function
> over the next few days so the brave (or foolhardy, depending on your
> point of view) user can fit those models.  Then I will try to get the
> binomial GLMMs happy again.
> 

Sorry, Doug, I read this message of your's only after I had posted my
example. But maybe you can use it to test your svn version. Take
a brave heart.

Dieter



From harlancampbell at gmail.com  Fri Mar 13 21:14:44 2009
From: harlancampbell at gmail.com (H c)
Date: Fri, 13 Mar 2009 16:14:44 -0400
Subject: [R-sig-ME] odd occurance of "Error in model.frame.default(formula =
	... variable lengths differ "!
Message-ID: <222824550903131314q12f5c7b8kb6aea13f29dd365c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090313/3d0bcec7/attachment.pl>

From bates at stat.wisc.edu  Sat Mar 14 14:04:30 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 14 Mar 2009 08:04:30 -0500
Subject: [R-sig-ME] [R] Mixed model help!
In-Reply-To: <000601c9a413$78cf8690$6a6e93b0$@com.ar>
References: <000601c9a413$78cf8690$6a6e93b0$@com.ar>
Message-ID: <40e66e0b0903140604q6a16aaqf11a685b72c5713a@mail.gmail.com>

On Fri, Mar 13, 2009 at 2:39 PM, Mart?n Quiroga <mquiroga at ssdfe.com.ar> wrote:
> Hi everyone! I am a biologist from Argentina and have to solve this problem.
> I have an insect population obtained from 10 different nests and need to
> know its sex ratio. But as I cannot ensure insects independence I need to
> run a model where I can include the variable ?nest? as with a random effect.
> The response variable has a binomial distribution (males or females).
> I?ve been reading for a while and found the MASS and lmer packages that will
> allow me to do such a thing with my data. I found the script I should write
> ?lmer(y~fixed+(time | random), family=binomial)? but, despite of using the R
> commander cannot make it work. Can you help me with this? I?ll appreciate
> that!

The package is called lme4 and the function is called lmer.   To be
able to use the lmer function you must first enter

library(lme4)

in a script or console session (I'm not sure how this would be done in
R Commander).

There is a mailing list, R-SIG-Mixed-Models at R-project.org specifically
for discussion of mixed models.  I am cc:ing that list on this reply.



From Emma.Stone at bristol.ac.uk  Mon Mar 16 16:21:00 2009
From: Emma.Stone at bristol.ac.uk (Emma Stone)
Date: Mon, 16 Mar 2009 15:21:00 +0000
Subject: [R-sig-ME] glmer and poisson distributions and P values
Message-ID: <70468607E1755A2B468C801F@bio-mammal012.bio.bris.ac.uk>


Dear all,

I am trying to run a glmer model with count data,with one random factor. 
When I run it with more than one predictor all is well, but when there is 
only one predictor in the model I get this warning message, can anyone 
explain this at all?

Warning message: In mer_finalize(ans) : false convergence (8)

Also, for other models in the canidate set I am looking at interactions 
between predictors and I want to get the P values for these interactions, 
any ideas how I can do this from a glmer model?

Thanks

Emma


--



From a.renwick at abdn.ac.uk  Mon Mar 16 16:27:31 2009
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Mon, 16 Mar 2009 15:27:31 +0000
Subject: [R-sig-ME] glmer and poisson distributions and P values
In-Reply-To: <70468607E1755A2B468C801F@bio-mammal012.bio.bris.ac.uk>
References: <70468607E1755A2B468C801F@bio-mammal012.bio.bris.ac.uk>
Message-ID: <B9D1301370916C44B5874AF340C18B9B8F0D8B1BC6@VMAILB.uoa.abdn.ac.uk>

Dear Emma
There has been many queries on this foum regarding false convergence but before diagnosing your problem if may be necessary for you to send some more information on your model, i.e what the model is, how may data points you have for each variable etc.

Anna

-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Emma Stone
Sent: 16 March 2009 15:21
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] glmer and poisson distributions and P values


Dear all,

I am trying to run a glmer model with count data,with one random factor.
When I run it with more than one predictor all is well, but when there is only one predictor in the model I get this warning message, can anyone explain this at all?

Warning message: In mer_finalize(ans) : false convergence (8)

Also, for other models in the canidate set I am looking at interactions between predictors and I want to get the P values for these interactions, any ideas how I can do this from a glmer model?

Thanks

Emma


--

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From datkins at u.washington.edu  Mon Mar 16 19:31:56 2009
From: datkins at u.washington.edu (David Atkins)
Date: Mon, 16 Mar 2009 11:31:56 -0700
Subject: [R-sig-ME] mtable (package: memsic) with lmer objects
Message-ID: <49BE9B1C.4050906@u.washington.edu>


lmer useRs--

Not too long ago, I discovered the mtable() function in the memisc 
package, which formats tables of regression results for one or more 
modeles in "friendly" ways.  At least, it formats them in ways that I 
often see in psychology, psychiatry, and sociology journals (where I 
usually read/publish).  There is an example below.

With some assistance from Martin Elff, the package author, I now have a 
summary method that works with lmer() and glmer(), though admittedly, it 
is pretty bare bones and could be extended in useful ways.

That is part of the reason why I'm posting this.  It could be 
interesting to have options for including random-effects variances, or 
group sizes, but these will vary depending on the model and gets a bit 
beyond my primitive coding skills.

So, here's a plain-jane version for those who might find it useful, and 
for those who might want to take a crack at extending (though could you 
let me know if you do?  I think Martin may role this into the next 
memisc update).  [BTW, this is based off the getSummary.glm code, which 
explains most of the commented out code.]

cheers, Dave

library(lme4)
library(memisc)

### create three models
fm1 <- lmer(Reaction ~ 1 + (Days|Subject), sleepstudy)
fm1.1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
fm1.2 <- lmer(Reaction ~ as.factor(Days) + (Days|Subject), sleepstudy)

### note: need to run the code below fro setCoefTemplate and
### getSummary.lmer first

mtable("Model 1"=fm1, "Model 2"=fm1.1, "Model 3"=fm1.2,
     			coef.style = "est.ci", # using "homegrown" est.ci, specified above
     			summary.stats=c("AIC","BIC"),
     			getSummary = "getSummary.lmer")#,

setCoefTemplate(
  est.ci=c(
	est = "($est:#)($p:*)",
	ci = "[($lwr:#),($upr:#)]"))

getSummary.lmer <- function (obj, alpha = 0.05, ...)
{
     require(lme4)
     smry <- summary(obj)
     #N <- if (length(weights(obj))) ### NOTE: how to deal with 
groups/samp size?
     #    sum(weights(obj))
     #else sum(smry$df[1:2])
     coef <- smry at coefs
     lower <- qnorm(p = alpha/2, mean = coef[, 1], sd = coef[,
         2])
     upper <- qnorm(p = 1 - alpha/2, mean = coef[, 1], sd = coef[,
         2])
     if (ncol(smry at coefs) == 3) {
     	p <- (1 - pnorm(smry at coefs[,3]))*2 # NOTE: no p-values for lmer() 
due to
     											# unclear dfs; calculate p-values based on z
     	coef <- cbind(coef, p, lower, upper)
    	} else {
    			coef <- cbind(coef, lower, upper) # glmer will have 4 columns 
with p-values
    			}
     colnames(coef) <- c("est", "se", "stat", "p", "lwr", "upr")
     #phi <- smry$dispersion
     #LR <- smry$null.deviance - smry$deviance
     #df <- smry$df.null - smry$df.residual
     ll <- smry at AICtab[3][,1]
     deviance <- smry at AICtab[4][,1]
     #if (df > 0) {
     #    p <- pchisq(LR, df, lower.tail = FALSE)
     #    L0.pwr <- exp(-smry$null.deviance/N)
     #    McFadden <- 1 - smry$deviance/smry$null.deviance
     #    Cox.Snell <- 1 - exp(-LR/N)
     #    Nagelkerke <- Cox.Snell/(1 - L0.pwr)
     #}
     #else {
     #    LR <- NA
     #    df <- NA
     #    p <- NA
     #    McFadden <- NA
     #    Cox.Snell <- NA
     #    Nagelkerke <- NA
     #}
     AIC <- smry at AICtab[1][,1] # NOTE: these are both data.frames? not 
sure why...
     BIC <- smry at AICtab[2][,1]
     ### NOTE: don't see a similar slot for "xlevels" to get levels of
     ###		factor variables used as predictors; for time being, force
     ###		user to specify explicitly; nope that didn't work...
     #if (fac != NULL) {
     # 	n <- length(fac)
     #	xlevels <- vector(n, mode = "list")
     #	for (i in 1:n) {
     #		xlevels[i] <- levels(obj at frame[,fac[i]])
     #		}
     #	}
     #sumstat <- c(phi = phi, LR = LR, df = df, p = p, logLik = ll,
     #    deviance = deviance, McFadden = McFadden, Cox.Snell = Cox.Snell,
     #    Nagelkerke = Nagelkerke, AIC = AIC, BIC = BIC, N = N)
     sumstat <- c(logLik = ll, deviance = deviance, AIC = AIC, BIC = BIC)
     list(coef = coef, sumstat = sumstat,
     	 contrasts = attr(model.matrix(obj), "contrasts"),
         xlevels = NULL, call = obj at call)
}

-- 
Dave Atkins, PhD
Research Associate Professor
Center for the Study of Health and Risk Behaviors
Department of  Psychiatry and Behavioral Science
1100 NE 45th Street, Suite 300
Seattle, WA  98105
206-616-3879
datkins at u.washington.edu



From lborger at uoguelph.ca  Mon Mar 16 22:52:05 2009
From: lborger at uoguelph.ca (Luca Borger)
Date: Mon, 16 Mar 2009 17:52:05 -0400 (EDT)
Subject: [R-sig-ME] glmer and poisson distributions and P values
In-Reply-To: <70468607E1755A2B468C801F@bio-mammal012.bio.bris.ac.uk>
Message-ID: <792978174.2114581237240325517.JavaMail.root@huron.cs.uoguelph.ca>

Hi Emma,

(my two cents) It is usually very helpful to do first a search on the mailing list archives. For example, at:


############################################################################
http://markmail.org/message/oogvmvpxldr5i57q

you can find a reply by Prof. Bates:

"In mer_finalize(ans) : false convergence (8)

That's a warning that the parameter estimates are suspect but it's not
an error. I believe that a fitted model would have been returned.

I suggest you turn on the verbose option in the optimizer and examine
the progress of the iterations.  In the original call to glmer you add
verbose = TRUE.  If re-calling "mer_optimize" as above then change the
FALSE to TRUE."
#############################################################################


In addition, more information would be needed about your data etc. For example, in some cases similar convergence problems can be solved by centreing/standardising the predictor variable (but I may get corrected on this by the experts on this list).

HTH



Cheers,

Luca



----- Messaggio originale -----
Da: "Emma Stone" <Emma.Stone at bristol.ac.uk>
A: r-sig-mixed-models at r-project.org
Inviato: Luned?, 16 marzo 2009 11:21:00 GMT -05:00 U.S.A./Canada, stati orientali
Oggetto: [R-sig-ME] glmer and poisson distributions and P values


Dear all,

I am trying to run a glmer model with count data,with one random factor. 
When I run it with more than one predictor all is well, but when there is
only one predictor in the model I get this warning message, can anyone 
explain this at all?

Warning message: In mer_finalize(ans) : false convergence (8)

Also, for other models in the canidate set I am looking at interactions 
between predictors and I want to get the P values for these interactions,
any ideas how I can do this from a glmer model?

Thanks

Emma


--

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From Emma.Stone at bristol.ac.uk  Tue Mar 17 09:09:52 2009
From: Emma.Stone at bristol.ac.uk (Emma Stone)
Date: Tue, 17 Mar 2009 08:09:52 +0000
Subject: [R-sig-ME] glmer and poisson distributions and P values
In-Reply-To: <792978174.2114581237240325517.JavaMail.root@huron.cs.uoguelph.ca>
References: <792978174.2114581237240325517.JavaMail.root@huron.cs.uoguelph.c a>
Message-ID: <9EF1808F4BF2303620FEA08A@bio-mammal012.bio.bris.ac.uk>

Hi Luca,

Yep had a look on the help archive but this doesn't really help me I'm 
afraid,

There was no fitted model returned and I don't know how to turn on the 
verbose option in the optimizer.

Any ideas?



--On 16 March 2009 17:52 -0400 Luca Borger <lborger at uoguelph.ca> wrote:

> Hi Emma,
>
> (my two cents) It is usually very helpful to do first a search on the
> mailing list archives. For example, at:
>
>
>############################################################################
> http://markmail.org/message/oogvmvpxldr5i57q
>
> you can find a reply by Prof. Bates:
>
> "In mer_finalize(ans) : false convergence (8)
>
> That's a warning that the parameter estimates are suspect but it's not
> an error. I believe that a fitted model would have been returned.
>
> I suggest you turn on the verbose option in the optimizer and examine
> the progress of the iterations.  In the original call to glmer you add
> verbose = TRUE.  If re-calling "mer_optimize" as above then change the
> FALSE to TRUE."
>#############################################################################
>
>
> In addition, more information would be needed about your data etc. For
> example, in some cases similar convergence problems can be solved by
> centreing/standardising the predictor variable (but I may get corrected
> on this by the experts on this list).
>
> HTH
>
>
>
> Cheers,
>
> Luca
>
>
>
> ----- Messaggio originale -----
> Da: "Emma Stone" <Emma.Stone at bristol.ac.uk>
> A: r-sig-mixed-models at r-project.org
> Inviato: Luned?, 16 marzo 2009 11:21:00 GMT -05:00 U.S.A./Canada, stati
> orientali Oggetto: [R-sig-ME] glmer and poisson distributions and P values
>
>
> Dear all,
>
> I am trying to run a glmer model with count data,with one random factor.
> When I run it with more than one predictor all is well, but when there is
> only one predictor in the model I get this warning message, can anyone
> explain this at all?
>
> Warning message: In mer_finalize(ans) : false convergence (8)
>
> Also, for other models in the canidate set I am looking at interactions
> between predictors and I want to get the P values for these interactions,
> any ideas how I can do this from a glmer model?
>
> Thanks
>
> Emma
>
>
> --
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



----------------------
Emma Stone
Postgraduate Researcher
Bat Ecology and Bioacoustics Lab
& Mammal Research Unit
School of Biological Sciences,
University of Bristol, Woodland Road,
Bristol, BS8 1UG
Email: emma.stone at bristol.ac.uk



From Thierry.ONKELINX at inbo.be  Tue Mar 17 09:23:18 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 17 Mar 2009 09:23:18 +0100
Subject: [R-sig-ME] glmer and poisson distributions and P values
In-Reply-To: <9EF1808F4BF2303620FEA08A@bio-mammal012.bio.bris.ac.uk>
References: <792978174.2114581237240325517.JavaMail.root@huron.cs.uoguelph.c
	a> <9EF1808F4BF2303620FEA08A@bio-mammal012.bio.bris.ac.uk>
Message-ID: <2E9C414912813E4EB981326983E0A1040639F24B@inboexch.inbo.be>

Dear Emma,

As Douglas stated in his post: add verbose = TRUE to your call to glmer. Eg glmer(Response ~ Predictor, verbose = TRUE)

Cheers,

Thierry


----------------------------------------------------------------------------
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics, methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Emma Stone
Verzonden: dinsdag 17 maart 2009 9:10
Aan: Luca Borger
CC: r-sig-mixed-models at r-project.org
Onderwerp: Re: [R-sig-ME] glmer and poisson distributions and P values

Hi Luca,

Yep had a look on the help archive but this doesn't really help me I'm 
afraid,

There was no fitted model returned and I don't know how to turn on the 
verbose option in the optimizer.

Any ideas?



--On 16 March 2009 17:52 -0400 Luca Borger <lborger at uoguelph.ca> wrote:

> Hi Emma,
>
> (my two cents) It is usually very helpful to do first a search on the
> mailing list archives. For example, at:
>
>
>############################################################################
> http://markmail.org/message/oogvmvpxldr5i57q
>
> you can find a reply by Prof. Bates:
>
> "In mer_finalize(ans) : false convergence (8)
>
> That's a warning that the parameter estimates are suspect but it's not
> an error. I believe that a fitted model would have been returned.
>
> I suggest you turn on the verbose option in the optimizer and examine
> the progress of the iterations.  In the original call to glmer you add
> verbose = TRUE.  If re-calling "mer_optimize" as above then change the
> FALSE to TRUE."
>#############################################################################
>
>
> In addition, more information would be needed about your data etc. For
> example, in some cases similar convergence problems can be solved by
> centreing/standardising the predictor variable (but I may get corrected
> on this by the experts on this list).
>
> HTH
>
>
>
> Cheers,
>
> Luca
>
>
>
> ----- Messaggio originale -----
> Da: "Emma Stone" <Emma.Stone at bristol.ac.uk>
> A: r-sig-mixed-models at r-project.org
> Inviato: Luned?, 16 marzo 2009 11:21:00 GMT -05:00 U.S.A./Canada, stati
> orientali Oggetto: [R-sig-ME] glmer and poisson distributions and P values
>
>
> Dear all,
>
> I am trying to run a glmer model with count data,with one random factor.
> When I run it with more than one predictor all is well, but when there is
> only one predictor in the model I get this warning message, can anyone
> explain this at all?
>
> Warning message: In mer_finalize(ans) : false convergence (8)
>
> Also, for other models in the canidate set I am looking at interactions
> between predictors and I want to get the P values for these interactions,
> any ideas how I can do this from a glmer model?
>
> Thanks
>
> Emma
>
>
> --
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



----------------------
Emma Stone
Postgraduate Researcher
Bat Ecology and Bioacoustics Lab
& Mammal Research Unit
School of Biological Sciences,
University of Bristol, Woodland Road,
Bristol, BS8 1UG
Email: emma.stone at bristol.ac.uk

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From Emma.Stone at bristol.ac.uk  Tue Mar 17 11:52:51 2009
From: Emma.Stone at bristol.ac.uk (Emma Stone)
Date: Tue, 17 Mar 2009 10:52:51 +0000
Subject: [R-sig-ME] glmer and poisson distributions and P values
In-Reply-To: <2E9C414912813E4EB981326983E0A1040639F24B@inboexch.inbo.be>
References: <792978174.2114581237240325517.JavaMail.root@huron.cs.uoguelph.c
	a> <9EF1808F4BF2303620FEA08A@bio-mammal012.bio.bris.ac.uk>
	<2E9C414912813E4EB981326983E0A1040639F24B@inboexch.inbo.be>
Message-ID: <4A4932AB1AD8C7B7ACC0A88F@bio-mammal012.bio.bris.ac.uk>

Hi Thierry,

Thanks for this, does that mean I add this to the model instead of the 
family = poisson? Is this still applicable for poisson data?

Sorry if I am being stupid, quite new to all this

Emma

--On 17 March 2009 09:23 +0100 "ONKELINX, Thierry" 
<Thierry.ONKELINX at inbo.be> wrote:

> Dear Emma,
>
> As Douglas stated in his post: add verbose = TRUE to your call to glmer.
> Eg glmer(Response ~ Predictor, verbose = TRUE)
>
> Cheers,
>
> Thierry
>
>
> -------------------------------------------------------------------------
> --- ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest Cel biometrie, methodologie en kwaliteitszorg / Section
> biometrics, methodology and quality assurance Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be
> www.inbo.be
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to
> say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>
> The plural of anecdote is not data.
> ~ Roger Brinner
>
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of
> data. ~ John Tukey
>
> -----Oorspronkelijk bericht-----
> Van: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Emma Stone
> Verzonden: dinsdag 17 maart 2009 9:10
> Aan: Luca Borger
> CC: r-sig-mixed-models at r-project.org
> Onderwerp: Re: [R-sig-ME] glmer and poisson distributions and P values
>
> Hi Luca,
>
> Yep had a look on the help archive but this doesn't really help me I'm
> afraid,
>
> There was no fitted model returned and I don't know how to turn on the
> verbose option in the optimizer.
>
> Any ideas?
>
>
>
> --On 16 March 2009 17:52 -0400 Luca Borger <lborger at uoguelph.ca> wrote:
>
>> Hi Emma,
>>
>> (my two cents) It is usually very helpful to do first a search on the
>> mailing list archives. For example, at:
>>
>>
>>############################################################################
>> http://markmail.org/message/oogvmvpxldr5i57q
>>
>> you can find a reply by Prof. Bates:
>>
>> "In mer_finalize(ans) : false convergence (8)
>>
>> That's a warning that the parameter estimates are suspect but it's not
>> an error. I believe that a fitted model would have been returned.
>>
>> I suggest you turn on the verbose option in the optimizer and examine
>> the progress of the iterations.  In the original call to glmer you add
>> verbose = TRUE.  If re-calling "mer_optimize" as above then change the
>> FALSE to TRUE."
>>#############################################################################
>>
>>
>> In addition, more information would be needed about your data etc. For
>> example, in some cases similar convergence problems can be solved by
>> centreing/standardising the predictor variable (but I may get corrected
>> on this by the experts on this list).
>>
>> HTH
>>
>>
>>
>> Cheers,
>>
>> Luca
>>
>>
>>
>> ----- Messaggio originale -----
>> Da: "Emma Stone" <Emma.Stone at bristol.ac.uk>
>> A: r-sig-mixed-models at r-project.org
>> Inviato: Luned?, 16 marzo 2009 11:21:00 GMT -05:00 U.S.A./Canada, stati
>> orientali Oggetto: [R-sig-ME] glmer and poisson distributions and P
>> values
>>
>>
>> Dear all,
>>
>> I am trying to run a glmer model with count data,with one random factor.
>> When I run it with more than one predictor all is well, but when there is
>> only one predictor in the model I get this warning message, can anyone
>> explain this at all?
>>
>> Warning message: In mer_finalize(ans) : false convergence (8)
>>
>> Also, for other models in the canidate set I am looking at interactions
>> between predictors and I want to get the P values for these interactions,
>> any ideas how I can do this from a glmer model?
>>
>> Thanks
>>
>> Emma
>>
>>
>> --
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
> ----------------------
> Emma Stone
> Postgraduate Researcher
> Bat Ecology and Bioacoustics Lab
> & Mammal Research Unit
> School of Biological Sciences,
> University of Bristol, Woodland Road,
> Bristol, BS8 1UG
> Email: emma.stone at bristol.ac.uk
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver
> weer  en binden het INBO onder geen enkel beding, zolang dit bericht niet
> bevestigd is door een geldig ondertekend document. The views expressed in
> this message  and any annex are purely those of the writer and may not be
> regarded as stating  an official position of INBO, as long as the message
> is not confirmed by a duly  signed document.



----------------------
Emma Stone
Postgraduate Researcher
Bat Ecology and Bioacoustics Lab
& Mammal Research Unit
School of Biological Sciences,
University of Bristol, Woodland Road,
Bristol, BS8 1UG
Email: emma.stone at bristol.ac.uk



From Thierry.ONKELINX at inbo.be  Tue Mar 17 12:51:42 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 17 Mar 2009 12:51:42 +0100
Subject: [R-sig-ME] glmer and poisson distributions and P values
In-Reply-To: <4A4932AB1AD8C7B7ACC0A88F@bio-mammal012.bio.bris.ac.uk>
References: <792978174.2114581237240325517.JavaMail.root@huron.cs.uoguelph.c
	a> <9EF1808F4BF2303620FEA08A@bio-mammal012.bio.bris.ac.uk>
	<2E9C414912813E4EB981326983E0A1040639F24B@inboexch.inbo.be>
	<4A4932AB1AD8C7B7ACC0A88F@bio-mammal012.bio.bris.ac.uk>
Message-ID: <2E9C414912813E4EB981326983E0A1040639F365@inboexch.inbo.be>

Dear Emma,

Verbose is just another argument of the glmer function. Just like family is. Hence it does not replace the family argument. Have a look at ?glmer for all its arguments.

Verbose will give you more details on the convergence of the model. It does not make your model converge. I'm still willing to look at your data if you send it in an easy to read format (.Rdata, .csv, .txt).

HTH,

Thierry


----------------------------------------------------------------------------
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics, methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: Emma Stone [mailto:Emma.Stone at bristol.ac.uk] 
Verzonden: dinsdag 17 maart 2009 11:53
Aan: ONKELINX, Thierry; Emma Stone; Luca Borger
CC: r-sig-mixed-models at r-project.org
Onderwerp: RE: [R-sig-ME] glmer and poisson distributions and P values

Hi Thierry,

Thanks for this, does that mean I add this to the model instead of the 
family = poisson? Is this still applicable for poisson data?

Sorry if I am being stupid, quite new to all this

Emma

--On 17 March 2009 09:23 +0100 "ONKELINX, Thierry" 
<Thierry.ONKELINX at inbo.be> wrote:

> Dear Emma,
>
> As Douglas stated in his post: add verbose = TRUE to your call to glmer.
> Eg glmer(Response ~ Predictor, verbose = TRUE)
>
> Cheers,
>
> Thierry
>
>
> -------------------------------------------------------------------------
> --- ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest Cel biometrie, methodologie en kwaliteitszorg / Section
> biometrics, methodology and quality assurance Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be
> www.inbo.be
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to
> say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>
> The plural of anecdote is not data.
> ~ Roger Brinner
>
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of
> data. ~ John Tukey
>
> -----Oorspronkelijk bericht-----
> Van: r-sig-mixed-models-bounces at r-project.org
> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Emma Stone
> Verzonden: dinsdag 17 maart 2009 9:10
> Aan: Luca Borger
> CC: r-sig-mixed-models at r-project.org
> Onderwerp: Re: [R-sig-ME] glmer and poisson distributions and P values
>
> Hi Luca,
>
> Yep had a look on the help archive but this doesn't really help me I'm
> afraid,
>
> There was no fitted model returned and I don't know how to turn on the
> verbose option in the optimizer.
>
> Any ideas?
>
>
>
> --On 16 March 2009 17:52 -0400 Luca Borger <lborger at uoguelph.ca> wrote:
>
>> Hi Emma,
>>
>> (my two cents) It is usually very helpful to do first a search on the
>> mailing list archives. For example, at:
>>
>>
>>############################################################################
>> http://markmail.org/message/oogvmvpxldr5i57q
>>
>> you can find a reply by Prof. Bates:
>>
>> "In mer_finalize(ans) : false convergence (8)
>>
>> That's a warning that the parameter estimates are suspect but it's not
>> an error. I believe that a fitted model would have been returned.
>>
>> I suggest you turn on the verbose option in the optimizer and examine
>> the progress of the iterations.  In the original call to glmer you add
>> verbose = TRUE.  If re-calling "mer_optimize" as above then change the
>> FALSE to TRUE."
>>#############################################################################
>>
>>
>> In addition, more information would be needed about your data etc. For
>> example, in some cases similar convergence problems can be solved by
>> centreing/standardising the predictor variable (but I may get corrected
>> on this by the experts on this list).
>>
>> HTH
>>
>>
>>
>> Cheers,
>>
>> Luca
>>
>>
>>
>> ----- Messaggio originale -----
>> Da: "Emma Stone" <Emma.Stone at bristol.ac.uk>
>> A: r-sig-mixed-models at r-project.org
>> Inviato: Luned?, 16 marzo 2009 11:21:00 GMT -05:00 U.S.A./Canada, stati
>> orientali Oggetto: [R-sig-ME] glmer and poisson distributions and P
>> values
>>
>>
>> Dear all,
>>
>> I am trying to run a glmer model with count data,with one random factor.
>> When I run it with more than one predictor all is well, but when there is
>> only one predictor in the model I get this warning message, can anyone
>> explain this at all?
>>
>> Warning message: In mer_finalize(ans) : false convergence (8)
>>
>> Also, for other models in the canidate set I am looking at interactions
>> between predictors and I want to get the P values for these interactions,
>> any ideas how I can do this from a glmer model?
>>
>> Thanks
>>
>> Emma
>>
>>
>> --
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
> ----------------------
> Emma Stone
> Postgraduate Researcher
> Bat Ecology and Bioacoustics Lab
> & Mammal Research Unit
> School of Biological Sciences,
> University of Bristol, Woodland Road,
> Bristol, BS8 1UG
> Email: emma.stone at bristol.ac.uk
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver
> weer  en binden het INBO onder geen enkel beding, zolang dit bericht niet
> bevestigd is door een geldig ondertekend document. The views expressed in
> this message  and any annex are purely those of the writer and may not be
> regarded as stating  an official position of INBO, as long as the message
> is not confirmed by a duly  signed document.



----------------------
Emma Stone
Postgraduate Researcher
Bat Ecology and Bioacoustics Lab
& Mammal Research Unit
School of Biological Sciences,
University of Bristol, Woodland Road,
Bristol, BS8 1UG
Email: emma.stone at bristol.ac.uk



Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From tanjas at u.washington.edu  Tue Mar 17 18:30:02 2009
From: tanjas at u.washington.edu (Tanja Srebotnjak)
Date: Tue, 17 Mar 2009 10:30:02 -0700
Subject: [R-sig-ME] update on mcmcsamp for glmer
Message-ID: <36FA2D4F89496341B64E8AABEB4CD14309E73B0ED6@sdc-mbx-01.exchange.washington.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090317/f01e0045/attachment.pl>

From si.fu at ntlworld.com  Wed Mar 18 12:30:29 2009
From: si.fu at ntlworld.com (si.fu at ntlworld.com)
Date: Wed, 18 Mar 2009 11:30:29 +0000
Subject: [R-sig-ME] Generalized Mixed Model (Poisson) for recurrent events
 study design (Sample Size estimation by simulation)
Message-ID: <20090318113029.35ZXA.216453.root@web08-winn.ispmail.private.ntl.com>

Hello, 
 
I have a general help question that some of you might be able able to help. I have posed this to R-help list and I have been told that people reading this list might be best placed to help me.
 
I would like calculate sample size for a study (a two group comparison) based on the outcome of reduction on (recurrent) events (say hospital admissions). 
 
In a previous study, hospital admission rate of 140 admissions per 72 patients 
(over a 4 month period) has been observed. That is rate is about 1.9. In order 
to see the admission rate reduction of 50% over a 4 month period, i.e. 0.95, in 
a two group comparison (alpha 0.05) with appropriate power (say 80%) what sort 
of group sizes are needed. 

The number of hospital admissions per patient differ within  the four month period between 4 to none (i.e. poisson). So in order to account for between subject and within subject variation, lmer with poission model could be used for the simulation. I am grateful for your help in getting me started.
 
 
Many Thanks, 
 
Si.



From bates at stat.wisc.edu  Wed Mar 18 14:29:49 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 18 Mar 2009 08:29:49 -0500
Subject: [R-sig-ME] Variable transformation and back transformation
In-Reply-To: <49B8BE87.3080809@uni-bayreuth.de>
References: <49B8BE87.3080809@uni-bayreuth.de>
Message-ID: <40e66e0b0903180629o3fbf9de5m7bb6505b3cca6cc6@mail.gmail.com>

On Thu, Mar 12, 2009 at 2:49 AM, Christina Bogner
<christina.bogner at uni-bayreuth.de> wrote:
> Dear all,
>
> I have fitted a couple of mixed-effects models to environmental data
> (chemical and physical soil parameters) with log-transformed dependent
> variables. I tried generalized mixed-models, but the results were not
> satisfactory (probably because I am a soil scientist and not a statistician
> ;-)) Now, as log of concentrations are ecologically not very informative, I
> would like to back-transform my model parameters. Taking a Gaussian linear
> mixed-model:

> log(Mg2)=intercept+beta1*Silt+beta2*Soil.depth+beta3*Flow.region+b1*Plot+b2*/Soil.Depth%in%Plot+var
> where Mg2 is the concentration of magnesium, betas are fixed-effects and bs
> random ones. All independent variables except Silt are factors; Silt is
> continuous.

> I would write:
> Mg2=exp(intercept+beta1*mean(Silt in respective
> Soil.Depth)+beta3*Flow.region+estimate of b1*Plot + estimate of
> b2*/Soil.Depth%in%Plot+0.5*var)
> to back-transform to the original scale on the Soil.Depth-level.

> To back-transform the fixed-effects only, I would drop the estimates of the
> random-effects:
> Mg2=exp(intercept+beta1*mean(Silt in respective
> Soil.Depth)+beta3*Flow.region+ 0.5*var)

> This approach treats the estimated random effects as dummies, not as an
> additional variance. Is this right?

I'm not sure exactly what you mean by treating the estimated random
effects as dummies.

In a linear mixed model the random effects are incorporated
additively.  It is common with data like concentrations that the
effect of different levels of variability is more appropriately
modelled as a multiplicative change than as an additive change, which
corresponds to the additive change on the scale of the logarithm of
the concentration.

I would try to communicate this graphically by plotting the magnesium
concentration under various conditions and then plotting the logarithm
of this concentration.  I would hope to use this to overcome
resistance to the idea of using a transformation, such as when you say
that logarithms of concentrations are not meaningful ecologically.  I
have been fortunate to be present at many informal consulting sessions
led by the great statistician George Box who started his career as a
chemist at ICI, a British chemical company.  George always wants to
examine the data graphically and consider appropriate transformation
(the Box-Cox transformation family are the result of his work with Sir
David Cox).  He is aware of the resistance to transformation and has,
somewhat but not entirely facetiously, suggested that one way around
it is simply to create a new unit.  Recall that pH is the logarithm of
the hydrogen ion concentration.  Other examples are decibels
(logarithm of sound pressure) and octaves (doubling or halving the
frequency).

A summary of a random variable using location and scale parameters is
meaningful when the distribution is reasonably symmetric.  It is not
as easy to summarize asymmetric distributions.  (A log-normal
distribution is more complicated than a normal distribution.)  In more
general models, such as a linear mixed model, we can simplify the
description of the model under the appropriate scale but if we try to
back-transform then the description becomes much more complicated.

I'm not sure that this is addressing your question.  I think you are
trying to determine a simple way of communicating the meaning of the
parameter estimates on the concentration scale and, if so, my answer
is that they don't have a nice simple meaning on that scale.

One thing I noticed in your model is that Soil.Depth is being treated
as a categorical covariate, as opposed to a continuous covariate.  Was
the experimental design such that a fixed set of soil depths were
used?  In other words I am wondering if a model like

log(Mg2) ~ Silt + Flow + (Depth | Plot)

might be more appropriate than

log(Mg2) ~ Silt + Flow + (1 | Depth:Plot) + (1 | Plot)

Of course, the first model does assume that the effect of soil depth
on magnesium concentration is linear and that may not be appropriate,
although I would be tempted to store soil depth as at least an ordered
factor so I could check on the relative importance of linear and
higher order terms.



From lbaril at montana.edu  Wed Mar 18 16:27:15 2009
From: lbaril at montana.edu (lbaril at montana.edu)
Date: Wed, 18 Mar 2009 09:27:15 -0600 (MDT)
Subject: [R-sig-ME] [R] dispcrepancy between aov F test and tukey
 contrasts results with mixed effects model
In-Reply-To: <49BD99F7.90400@biostat.ku.dk>
References: In-Reply-To:
	<1935.69.145.66.89.1237073650.squirrel@gemini.msu.montana.edu>
	<49BCC1DA.2060304@biostat.ku.dk>
	<1194.69.145.66.89.1237131079.squirrel@gemini.msu.montana.edu>
	<49BD99F7.90400@biostat.ku.dk>
Message-ID: <1583.153.90.138.58.1237390035.squirrel@gemini.msu.montana.edu>

Unforunately, the data are highly unbalanced.  I have 2 to 3 sites per
stand, but observations within a stand range from 4 to 16 so my variances
in each site will not be similar and so using site averages would not work
in this case, right? Note that at the stand scale, I have roughly equal
observations.  Often in ecology balanced designs are not possible
(although if I'd known this would be an issue I would have made more of an
effort towards balance), so it seems as though there ought to be a
solution or others who have experienced this.  I have spent a fair bit of
time researching this issue, but it seems like all the advice points to
what I tried originally - which obviously did not work correctly given my
data.  Is there potentially another solution?


> lbaril at montana.edu wrote:
>> Thanks Peter for the advice and quick response.  I just want to clarify
>> what you suggest.  I should average values within a site then do a
>> one-way
>> anova to test for differnces between sites based on the 2 to 3 new
>> samples
>> per stand type -- and not use random effects for site?  Or, because I've
>> reduced the data I've 'corrected' the problem with the glht multiple
>> comparisons and can use the p-values from that summary if I include site
>> as a random effect?   Thanks again for your advice.
>
> This is tricky to say in a few lines, but the basic idea of a random
> effects model is that the site averages vary more than they should
> according to within-site variability. In the balanced case (equal
> number of observations per site), it turns out that the mixed-effects
> analysis is _equivalent_ to modeling the site averages. This is not
> ignoring the random effects of site; rather, it is coalescing it with
> the residual since the variance of a site average is v_site + 1/k v_res
> where k is the number of within-site observations.
>
> In the unbalanced case it is not strictly correct to analyze averages,
> because thy have different variances. However, the differences can be
> slight (when the k's are similar or v_site dominates in the above
> formula).
>
> A side effect of looking at averages is that you are fitting a plain lm
> model rather than lme and that glht in that case knows how to handle the
> degrees of freedom adjustment. (Assuming that the averages are normally
> distributed, which is as always dubious; but presumably, it is better
> than not correcting at all.)
>
>
>>
>>
>>> lbaril at montana.edu wrote:
>>>> Hello,
>>>> I have some conflicting output from an aov summary and tukey contrasts
>> with a mixed effects model I was hoping someone could clarify.  I am
>> comparing the abundance of a species across three willow stand types.
>> Since I have 2 or 3 sites within a habitat I have included site as a
>> random effect in the lme model.  My confusion is that the F test given
>> by
>>>> aov(model) indicates there is no difference between habitats, but the
>> tukey contrasts using the multcomp package shows that one pair of
>> habits
>>>> is significantly different from each other.  Why is there a
>> discrepancy?
>>>> Have I specified my model correctly?  I included the code and output
>> below.  Thank you.
>>> Looks like glht() is ignoring degrees of freedom. So what it does is
>> wrong but it is not easy to do it right (whatever "right" is in these
>> cases). If I understand correctly, what you have is that "stand" is
>> strictly coarser than "site", presumably the stands representing each 2,
>> 2, and 3 sites, with a varying number of replications within each site.
>> Since the between-site variation is considered random, you end up with a
>> comparison of stands based on essentially only 7 pieces of information.
>> (The latter statement requires some qualification, but let's not go
>> there
>> to day.)
>>> If you have roughly equal replications within each site, I'd be
>>> strongly
>> tempted to reduce the analysis to a simple 1-way ANOVA of the site
>> averages.
>>>>> co.lme=lme(coye~stand,data=t,random=~1|site)
>>>>> summary (co.lme)
>>>> Linear mixed-effects model fit by REML
>>>>  Data: R
>>>>        AIC      BIC    logLik
>>>>   53.76606 64.56047 -21.88303
>>>> Random effects:
>>>>  Formula: ~1 | site
>>>>         (Intercept)  Residual
>>>> StdDev:   0.3122146 0.2944667
>>>> Fixed effects: coye ~ stand
>>>>                  Value Std.Error DF    t-value p-value
>>>> (Intercept)  0.4936837 0.2305072 60  2.1417277  0.0363
>>>> stand2       0.4853222 0.3003745  4  1.6157240  0.1815
>>>> stand3      -0.3159230 0.3251201  4 -0.9717117  0.3862
>>>>  Correlation:
>>>>        (Intr) stand2
>>>> stand2 -0.767
>>>> stand3 -0.709  0.544
>>>> Standardized Within-Group Residuals:
>>>>        Min         Q1        Med         Q3        Max
>>>> -2.4545673 -0.5495609 -0.3148274  0.7527378  2.5151476
>>>> Number of Observations: 67
>>>> Number of Groups: 7
>>>>> anova(co.lme)
>>>>             numDF denDF   F-value p-value
>>>> (Intercept)     1    60 23.552098  <.0001
>>>> stand           2     4  3.738199  0.1215
>>>>> summary(glht(co.lme,linfct=mcp(stand="Tukey")))
>>>>          Simultaneous Tests for General Linear Hypotheses
>>>> Multiple Comparisons of Means: Tukey Contrasts
>>>> Fit: lme.formula(fixed = coye ~ stand, data = R, random = ~1 | site)
>> Linear Hypotheses:
>>>>            Estimate Std. Error z value Pr(>|z|)
>>>> 2 - 1 == 0   0.4853     0.3004   1.616   0.2385
>>>> 3 - 1 == 0  -0.3159     0.3251  -0.972   0.5943
>>>> 3 - 2 == 0  -0.8012     0.2994  -2.676   0.0202 *
>>>> ---
>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>> (Adjusted p values reported -- single-step method)
>>>> Lisa Baril
>>>> Masters Candidate
>>>> Department of Ecology
>>>> Montana State University - Bozeman
>>>> 406.994.2670
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> --
>>>     O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>>    c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>>   (*) \(*) -- University of Copenhagen   Denmark      Ph:  (+45)
>> 35327918
>>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)              FAX: (+45)
>>> 35327907
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>
>>
>> Lisa Baril
>> Masters Candidate
>> Department of Ecology
>> Montana State University - Bozeman
>> 406.994.2670
>>
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
> --
>     O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>    c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>   (*) \(*) -- University of Copenhagen   Denmark      Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)              FAX: (+45) 35327907
>
>
>


Lisa Baril
Masters Candidate
Department of Ecology
Montana State University - Bozeman
406.994.2670



From lbaril at montana.edu  Wed Mar 18 17:48:26 2009
From: lbaril at montana.edu (lbaril at montana.edu)
Date: Wed, 18 Mar 2009 10:48:26 -0600 (MDT)
Subject: [R-sig-ME] [R] dispcrepancy between aov F test and tukey
 contrasts results with mixed effects model
In-Reply-To: <49BD99F7.90400@biostat.ku.dk>
References: In-Reply-To:
	<1935.69.145.66.89.1237073650.squirrel@gemini.msu.montana.edu>
	<49BCC1DA.2060304@biostat.ku.dk>
	<1194.69.145.66.89.1237131079.squirrel@gemini.msu.montana.edu>
	<49BD99F7.90400@biostat.ku.dk>
Message-ID: <2324.153.90.138.58.1237394906.squirrel@gemini.msu.montana.edu>

Unforunately, the data are highly unbalanced.  I have 2 to 3 sites per
stand, but observations within a stand range from 4 to 16 so my variances
in each site will not be similar and so using site averages would not work
in this case, right? Note that at the stand scale, I have roughly equal
observations.  Often in ecology balanced designs are not possible
(although if I'd known this would be an issue I would have made more of an
effort towards balance), so it seems as though there ought to be a
solution or others who have experienced this.  I have spent a fair bit of
time researching this issue, but it seems like all the advice points to
what I tried originally - which obviously did not work correctly given my
data.  Is there potentially another solution?


> lbaril at montana.edu wrote:
>> Thanks Peter for the advice and quick response.  I just want to clarify
what you suggest.  I should average values within a site then do a
one-way
>> anova to test for differnces between sites based on the 2 to 3 new samples
>> per stand type -- and not use random effects for site?  Or, because
I've
>> reduced the data I've 'corrected' the problem with the glht multiple
comparisons and can use the p-values from that summary if I include
site
>> as a random effect?   Thanks again for your advice.
>
> This is tricky to say in a few lines, but the basic idea of a random
effects model is that the site averages vary more than they should
according to within-site variability. In the balanced case (equal number
of observations per site), it turns out that the mixed-effects analysis
is _equivalent_ to modeling the site averages. This is not ignoring the
random effects of site; rather, it is coalescing it with the residual
since the variance of a site average is v_site + 1/k v_res where k is
the number of within-site observations.
>
> In the unbalanced case it is not strictly correct to analyze averages,
because thy have different variances. However, the differences can be
slight (when the k's are similar or v_site dominates in the above
formula).
>
> A side effect of looking at averages is that you are fitting a plain lm
model rather than lme and that glht in that case knows how to handle the
degrees of freedom adjustment. (Assuming that the averages are normally
distributed, which is as always dubious; but presumably, it is better
than not correcting at all.)
>
>
>>> lbaril at montana.edu wrote:
>>>> Hello,
>>>> I have some conflicting output from an aov summary and tukey
contrasts
>> with a mixed effects model I was hoping someone could clarify.  I am
comparing the abundance of a species across three willow stand types.
Since I have 2 or 3 sites within a habitat I have included site as a
random effect in the lme model.  My confusion is that the F test given
by
>>>> aov(model) indicates there is no difference between habitats, but the
>> tukey contrasts using the multcomp package shows that one pair of habits
>>>> is significantly different from each other.  Why is there a
>> discrepancy?
>>>> Have I specified my model correctly?  I included the code and output
>> below.  Thank you.
>>> Looks like glht() is ignoring degrees of freedom. So what it does is
>> wrong but it is not easy to do it right (whatever "right" is in these
cases). If I understand correctly, what you have is that "stand" is
strictly coarser than "site", presumably the stands representing each
2,
>> 2, and 3 sites, with a varying number of replications within each site.
Since the between-site variation is considered random, you end up with
a
>> comparison of stands based on essentially only 7 pieces of information.
(The latter statement requires some qualification, but let's not go
there
>> to day.)
>>> If you have roughly equal replications within each site, I'd be strongly
>> tempted to reduce the analysis to a simple 1-way ANOVA of the site
averages.
>>>>> co.lme=lme(coye~stand,data=t,random=~1|site)
>>>>> summary (co.lme)
>>>> Linear mixed-effects model fit by REML
>>>>  Data: R
>>>>        AIC      BIC    logLik
>>>>   53.76606 64.56047 -21.88303
>>>> Random effects:
>>>>  Formula: ~1 | site
>>>>         (Intercept)  Residual
>>>> StdDev:   0.3122146 0.2944667
>>>> Fixed effects: coye ~ stand
>>>>                  Value Std.Error DF    t-value p-value
>>>> (Intercept)  0.4936837 0.2305072 60  2.1417277  0.0363
>>>> stand2       0.4853222 0.3003745  4  1.6157240  0.1815
>>>> stand3      -0.3159230 0.3251201  4 -0.9717117  0.3862
>>>>  Correlation:
>>>>        (Intr) stand2
>>>> stand2 -0.767
>>>> stand3 -0.709  0.544
>>>> Standardized Within-Group Residuals:
>>>>        Min         Q1        Med         Q3        Max
>>>> -2.4545673 -0.5495609 -0.3148274  0.7527378  2.5151476
>>>> Number of Observations: 67
>>>> Number of Groups: 7
>>>>> anova(co.lme)
>>>>             numDF denDF   F-value p-value
>>>> (Intercept)     1    60 23.552098  <.0001
>>>> stand           2     4  3.738199  0.1215
>>>>> summary(glht(co.lme,linfct=mcp(stand="Tukey")))
>>>>          Simultaneous Tests for General Linear Hypotheses
>>>> Multiple Comparisons of Means: Tukey Contrasts
>>>> Fit: lme.formula(fixed = coye ~ stand, data = R, random = ~1 | site)
>> Linear Hypotheses:
>>>>            Estimate Std. Error z value Pr(>|z|)
>>>> 2 - 1 == 0   0.4853     0.3004   1.616   0.2385
>>>> 3 - 1 == 0  -0.3159     0.3251  -0.972   0.5943
>>>> 3 - 2 == 0  -0.8012     0.2994  -2.676   0.0202 *
>>>> ---
>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
(Adjusted p values reported -- single-step method)
>>>> Lisa Baril
>>>> Masters Candidate
>>>> Department of Ecology
>>>> Montana State University - Bozeman
>>>> 406.994.2670
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> --
>>>     O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>>    c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>>   (*) \(*) -- University of Copenhagen   Denmark      Ph:  (+45)
>> 35327918
>>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)              FAX: (+45) 35327907
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> Lisa Baril
>> Masters Candidate
>> Department of Ecology
>> Montana State University - Bozeman
>> 406.994.2670
>> ______________________________________________
>> R-help at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
> --
>     O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>    c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>   (*) \(*) -- University of Copenhagen   Denmark      Ph:  (+45)
35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)              FAX: (+45) 35327907
>
>
>


Lisa Baril
Masters Candidate
Department of Ecology
Montana State University - Bozeman
406.994.2670



From p.dalgaard at biostat.ku.dk  Wed Mar 18 20:15:03 2009
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 18 Mar 2009 20:15:03 +0100
Subject: [R-sig-ME] [R] dispcrepancy between aov F test and tukey
 contrasts results with mixed effects model
In-Reply-To: <2324.153.90.138.58.1237394906.squirrel@gemini.msu.montana.edu>
References: In-Reply-To:	<1935.69.145.66.89.1237073650.squirrel@gemini.msu.montana.edu>	<49BCC1DA.2060304@biostat.ku.dk>	<1194.69.145.66.89.1237131079.squirrel@gemini.msu.montana.edu>	<49BD99F7.90400@biostat.ku.dk>
	<2324.153.90.138.58.1237394906.squirrel@gemini.msu.montana.edu>
Message-ID: <49C14837.9030306@biostat.ku.dk>

lbaril at montana.edu wrote:
> Unforunately, the data are highly unbalanced.  I have 2 to 3 sites per
> stand, but observations within a stand range from 4 to 16 so my variances
> in each site will not be similar and so using site averages would not work
> in this case, right? Note that at the stand scale, I have roughly equal
> observations.  Often in ecology balanced designs are not possible
> (although if I'd known this would be an issue I would have made more of an
> effort towards balance), so it seems as though there ought to be a
> solution or others who have experienced this.  I have spent a fair bit of
> time researching this issue, but it seems like all the advice points to
> what I tried originally - which obviously did not work correctly given my
> data.  Is there potentially another solution?


It's not necessarily as bad as you think. The relative sizes of the 
variance components also factor in. If the residual variance is small 
enough, it doesn't really matter how many observations you are 
averaging. In the present case, the two variance components are roughly 
equal, so the relative variances of averages is about (1+1/4):(1+1/16) = 
1.176 between the extreme cases. Ideally, you would weight the averages 
according to their variance, but the efficiency lost by not doing so is 
going to be small. And don't forget that whatever you do, you rely on 
assuming a normal distribution of the site levels, which is  really no 
verifiable at that sample size.

Also, there is some merit to just considering the difference in 
replication as part of the random variation of the averages (are there 
*systematically* more replicates on some stands?).




> 
> 
>> lbaril at montana.edu wrote:
>>> Thanks Peter for the advice and quick response.  I just want to clarify
> what you suggest.  I should average values within a site then do a
> one-way
>>> anova to test for differnces between sites based on the 2 to 3 new samples
>>> per stand type -- and not use random effects for site?  Or, because
> I've
>>> reduced the data I've 'corrected' the problem with the glht multiple
> comparisons and can use the p-values from that summary if I include
> site
>>> as a random effect?   Thanks again for your advice.
>> This is tricky to say in a few lines, but the basic idea of a random
> effects model is that the site averages vary more than they should
> according to within-site variability. In the balanced case (equal number
> of observations per site), it turns out that the mixed-effects analysis
> is _equivalent_ to modeling the site averages. This is not ignoring the
> random effects of site; rather, it is coalescing it with the residual
> since the variance of a site average is v_site + 1/k v_res where k is
> the number of within-site observations.
>> In the unbalanced case it is not strictly correct to analyze averages,
> because thy have different variances. However, the differences can be
> slight (when the k's are similar or v_site dominates in the above
> formula).
>> A side effect of looking at averages is that you are fitting a plain lm
> model rather than lme and that glht in that case knows how to handle the
> degrees of freedom adjustment. (Assuming that the averages are normally
> distributed, which is as always dubious; but presumably, it is better
> than not correcting at all.)
>>
>>>> lbaril at montana.edu wrote:
>>>>> Hello,
>>>>> I have some conflicting output from an aov summary and tukey
> contrasts
>>> with a mixed effects model I was hoping someone could clarify.  I am
> comparing the abundance of a species across three willow stand types.
> Since I have 2 or 3 sites within a habitat I have included site as a
> random effect in the lme model.  My confusion is that the F test given
> by
>>>>> aov(model) indicates there is no difference between habitats, but the
>>> tukey contrasts using the multcomp package shows that one pair of habits
>>>>> is significantly different from each other.  Why is there a
>>> discrepancy?
>>>>> Have I specified my model correctly?  I included the code and output
>>> below.  Thank you.
>>>> Looks like glht() is ignoring degrees of freedom. So what it does is
>>> wrong but it is not easy to do it right (whatever "right" is in these
> cases). If I understand correctly, what you have is that "stand" is
> strictly coarser than "site", presumably the stands representing each
> 2,
>>> 2, and 3 sites, with a varying number of replications within each site.
> Since the between-site variation is considered random, you end up with
> a
>>> comparison of stands based on essentially only 7 pieces of information.
> (The latter statement requires some qualification, but let's not go
> there
>>> to day.)
>>>> If you have roughly equal replications within each site, I'd be strongly
>>> tempted to reduce the analysis to a simple 1-way ANOVA of the site
> averages.
>>>>>> co.lme=lme(coye~stand,data=t,random=~1|site)
>>>>>> summary (co.lme)
>>>>> Linear mixed-effects model fit by REML
>>>>>  Data: R
>>>>>        AIC      BIC    logLik
>>>>>   53.76606 64.56047 -21.88303
>>>>> Random effects:
>>>>>  Formula: ~1 | site
>>>>>         (Intercept)  Residual
>>>>> StdDev:   0.3122146 0.2944667
>>>>> Fixed effects: coye ~ stand
>>>>>                  Value Std.Error DF    t-value p-value
>>>>> (Intercept)  0.4936837 0.2305072 60  2.1417277  0.0363
>>>>> stand2       0.4853222 0.3003745  4  1.6157240  0.1815
>>>>> stand3      -0.3159230 0.3251201  4 -0.9717117  0.3862
>>>>>  Correlation:
>>>>>        (Intr) stand2
>>>>> stand2 -0.767
>>>>> stand3 -0.709  0.544
>>>>> Standardized Within-Group Residuals:
>>>>>        Min         Q1        Med         Q3        Max
>>>>> -2.4545673 -0.5495609 -0.3148274  0.7527378  2.5151476
>>>>> Number of Observations: 67
>>>>> Number of Groups: 7
>>>>>> anova(co.lme)
>>>>>             numDF denDF   F-value p-value
>>>>> (Intercept)     1    60 23.552098  <.0001
>>>>> stand           2     4  3.738199  0.1215
>>>>>> summary(glht(co.lme,linfct=mcp(stand="Tukey")))
>>>>>          Simultaneous Tests for General Linear Hypotheses
>>>>> Multiple Comparisons of Means: Tukey Contrasts
>>>>> Fit: lme.formula(fixed = coye ~ stand, data = R, random = ~1 | site)
>>> Linear Hypotheses:
>>>>>            Estimate Std. Error z value Pr(>|z|)
>>>>> 2 - 1 == 0   0.4853     0.3004   1.616   0.2385
>>>>> 3 - 1 == 0  -0.3159     0.3251  -0.972   0.5943
>>>>> 3 - 2 == 0  -0.8012     0.2994  -2.676   0.0202 *
>>>>> ---
>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> (Adjusted p values reported -- single-step method)
>>>>> Lisa Baril
>>>>> Masters Candidate
>>>>> Department of Ecology
>>>>> Montana State University - Bozeman
>>>>> 406.994.2670
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> --
>>>>     O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>>>    c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>>>   (*) \(*) -- University of Copenhagen   Denmark      Ph:  (+45)
>>> 35327918
>>>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)              FAX: (+45) 35327907
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> Lisa Baril
>>> Masters Candidate
>>> Department of Ecology
>>> Montana State University - Bozeman
>>> 406.994.2670
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>>     O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>    c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>   (*) \(*) -- University of Copenhagen   Denmark      Ph:  (+45)
> 35327918
>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)              FAX: (+45) 35327907
>>
>>
>>
> 
> 
> Lisa Baril
> Masters Candidate
> Department of Ecology
> Montana State University - Bozeman
> 406.994.2670
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
  (*) \(*) -- University of Copenhagen   Denmark      Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)              FAX: (+45) 35327907



From brant.inman at me.com  Thu Mar 19 03:11:02 2009
From: brant.inman at me.com (Brant Inman)
Date: Wed, 18 Mar 2009 22:11:02 -0400
Subject: [R-sig-ME] Multilevel logistic regression
Message-ID: <4AF69CE1-6F48-42BB-A6E5-9248EB47FBDE@me.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090318/2d640c8e/attachment.pl>

From steibelj at msu.edu  Thu Mar 19 13:54:01 2009
From: steibelj at msu.edu (Juan Pedro Steibel)
Date: Thu, 19 Mar 2009 08:54:01 -0400
Subject: [R-sig-ME] Variable transformation and back transformation
In-Reply-To: <49B8BE87.3080809@uni-bayreuth.de>
References: <49B8BE87.3080809@uni-bayreuth.de>
Message-ID: <49C24069.200@msu.edu>

Hello Christina,
Dr. Bates already gave an excelent explanation with some examples were 
transformed variables are widely accepted.

My experience comes from working with gene expression data, where 
colleagues like to express comparisons as "Fold-change" or ratios. In 
that case, working with logs makes much more sense too. As stated before 
concentrations tend to follow multiplicative models (that is implicit in 
the use of Ratios for comparisons) so the log brings everything to the 
additive scale. Plus the Gausian mixed effects model usually fits 
log-concentration reasonably well.

So I commonly analyze log-concentrations and provide all results in the 
log-scale.

Sometimes I am asked to report values in the Fold Change scale (ratios). 
What I do then is to plot everything in the log-scale and attach as a 
second scale the ratio or "back-transformed" scale (commonly to the 
right axis of the plot). That seems to appease even the most 
"anti-logarithmic people".

The good thing about this approach is that you do not have to deal with 
back-transforming the mean differences (and uncertainty measures), but 
only the scale. for example in the left axis your (log) scale may be: 
-2,-1,0,1,2 and the corresponding back-transformed scale (right axis) in 
log2 (the one used in qPCR data for example) would be: 1/4, 1/2, 1,2,4.  
Moreover, using 1/4 and 1/2 instead of 0.25 and 0.5 seems more appealing 
to biologists as they easily read it as "4-fold down or 2-fold down".

These type of plots are really straight forward to generate using R, 
because you create the plot and add all the annotation needed (second 
scale, etc).

Now if you want to create a table, and report the actual values, AND you 
are forced to go back to the original scale... Then you need to 
back-trasform the results. Back-transforming mean differences or means 
is not a big deal, but providing an uncertainty measure  in the 
backtransformed scale may be. I would recommend in this case to use 
mcmcsamp to tackle the problem. I've never done it, but it should be 
easy to generate a sample from the posterior distribution of 
parameters,  compute the contrast of interest (mean differences) and 
back-transform and summarize them (mean, CI, etc, etc).

In any case you do not have to change your model, if you do it and 
ignore random effects, your SE and other uncertainty measures will be in 
the desired scale, but most likely the inferences will be incorrect.

Hope this helps,
Cheers,
JP



Christina Bogner wrote:
> Dear all,
>
> I have fitted a couple of mixed-effects models to environmental data 
> (chemical and physical soil parameters) with log-transformed dependent 
> variables. I tried generalized mixed-models, but the results were not 
> satisfactory (probably because I am a soil scientist and not a 
> statistician ;-)) Now, as log of concentrations are ecologically not 
> very informative, I would like to back-transform my model parameters. 
> Taking a Gaussian linear mixed-model:
>
> log(Mg2)=intercept+beta1*Silt+beta2*Soil.depth+beta3*Flow.region+b1*Plot+b2*/Soil.Depth%in%Plot+var 
>
> where Mg2 is the concentration of magnesium, betas are fixed-effects 
> and bs random ones. All independent variables except Silt are factors; 
> Silt is continuous.
>
> I would write:
> Mg2=exp(intercept+beta1*mean(Silt in respective 
> Soil.Depth)+beta3*Flow.region+estimate of b1*Plot + estimate of 
> b2*/Soil.Depth%in%Plot+0.5*var)
> to back-transform to the original scale on the Soil.Depth-level.
>
> To back-transform the fixed-effects only, I would drop the estimates 
> of the random-effects:
> Mg2=exp(intercept+beta1*mean(Silt in respective 
> Soil.Depth)+beta3*Flow.region+ 0.5*var)
>
>
> This approach treats the estimated random effects as dummies, not as 
> an additional variance. Is this right?
>
> Thanks a lot for your help
>
> Christina Bogner
> ------------------------------------------------------------------------
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>   


-- 
=============================
Juan Pedro Steibel

Assistant Professor
Statistical Genetics and Genomics

Department of Animal Science & 
Department of Fisheries and Wildlife

Michigan State University
1205-I Anthony Hall
East Lansing, MI
48824 USA 

Phone: 1-517-353-5102
E-mail: steibelj at msu.edu



From Thierry.ONKELINX at inbo.be  Fri Mar 20 10:22:41 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 20 Mar 2009 10:22:41 +0100
Subject: [R-sig-ME] Variable transformation and back transformation
In-Reply-To: <49C24069.200@msu.edu>
References: <49B8BE87.3080809@uni-bayreuth.de> <49C24069.200@msu.edu>
Message-ID: <2E9C414912813E4EB981326983E0A1040639FA1B@inboexch.inbo.be>

Dear all,

Christina's question will be a bit more clear with some extra background
information on the topic. When interpolating concentrations in the soil
with kriging, a log-transformation is often used. The predicted value
for a location in the log-scale is a distribution with mean mu(s) and
standard deviation sigma(s). Mu(s) is a function which yields the mean
value depending on location s.
A simple form of backtransformation would be exp(mu(s)). That will no
longer be the mean of the distribution (in the original scale) at
location s, but rather its median. The mean of the distribution in the
original scale is exp(mu(s) + 0.5 * sigma(s) ^ 2). That is the reason
why Christina includes the variance in the backtransformation.

This is the backtransformation one would use with a linear model. So I
suppose that Christina's question is how the deal with the variance from
the random effect in such a backtransformation.

Personally I would settle with confidence intervals. The nice thing
about them is that they are based on the order of values. Since a
monotone transformations (like exp() and log()) don't change the order
of values, the backtransformation is straightforward: exp([LCL, UCL]).
That requires two maps to depict the information.
If you have some important treshold (e.g. some legal maximum
concentration) you could create a map with the probability of exeeding
that treshold.

HTH,

Thierry


------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of
data.
~ John Tukey

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From Thierry.ONKELINX at inbo.be  Fri Mar 20 10:32:16 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 20 Mar 2009 10:32:16 +0100
Subject: [R-sig-ME] Multilevel logistic regression
In-Reply-To: <4AF69CE1-6F48-42BB-A6E5-9248EB47FBDE@me.com>
References: <4AF69CE1-6F48-42BB-A6E5-9248EB47FBDE@me.com>
Message-ID: <2E9C414912813E4EB981326983E0A1040639FA31@inboexch.inbo.be>

Dear Brant,

The model is too complex. You have maximum three observations for each
level of the random effect. Allowing for a random intercept and two
random slopes does not make much sense then. Does it?

HTH,

Thierry


------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of
data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] Namens Brant Inman
Verzonden: donderdag 19 maart 2009 3:11
Aan: r-sig-mixed-models at r-project.org
Onderwerp: [R-sig-ME] Multilevel logistic regression


lmer Experts:

I am trying to use lmer to duplicate the results found in Joop Hox's  
book "Multilevel Analysis: technique and applications" 2002.  In  
chapter 6 of his book he shows an example of multilevel logistic  
regression for a meta-analysis of survey response rates.  The data are  
available in the file "metaresp.xls" at his website:

<http://www.geocities.com/joophox/mlbook/excelxls.zip>

The dataset includes the following variables of interest:

Individual level (Level 1) variables:
TELDUM	 = telephone questioning
MAILDUM  = mail questioning
RESPONSE = the outcome of interest, the study response rate
DENOM    = the number of people questioned

Study/group level (Level 2) variables:
SOURCE 	 = the study identifier
YEAR	 = year of study
SALIENCY = how salient the questionnaire was (0 to 2)
RESPISRR = the way the response rate was calculated


The null model (Table 6.2) proposed by Joop is easy to fit:

SUCCESS <- as.integer(RESPISRR*DENOM)
y  	<- cbind(SUCCESS, DENOM-SUCCESS)

f1 <- lmer(y ~ RESPISRR + (1 | SOURCE), family=binomial(link=logit))


Joop then adds a couple Level 1 variables (Table 6.3):

f2 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (1 | SOURCE),  
family=binomial(link=logit))


He then says that these two Level 1 variables should be allowed to  
vary across studies (varying slopes).  When I try to fit what I  
believe to be the correct model, I get an error


f3 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (TELNUM | SOURCE) +  
(MAILDUM | SOURCE)
	+ (1 | SOURCE), family=binomial(link=logit))

Error in mer_finalize(ans) : q = 240 > n = 105


Can anyone tell me what I am doing wrong here?  Thanks so much in  
advance.

Brant Inman
Duke University Medical Center

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From francoisrousseu at hotmail.com  Fri Mar 20 16:53:26 2009
From: francoisrousseu at hotmail.com (Francois Rousseu)
Date: Fri, 20 Mar 2009 11:53:26 -0400
Subject: [R-sig-ME] lmer and multiple membership
Message-ID: <BLU133-W5106B11B3D8C61E0217282BA970@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090320/682c2d4f/attachment.pl>

From datkins at u.washington.edu  Fri Mar 20 18:08:33 2009
From: datkins at u.washington.edu (David Atkins)
Date: Fri, 20 Mar 2009 10:08:33 -0700
Subject: [R-sig-ME] [Fwd: Announce: New (free) beta version of MLPowSim
	software]
Message-ID: <49C3CD91.6000003@u.washington.edu>


For those of you not on the general multilevel listserv, this might be 
of interest.  Note that their software can output R code to do the 
simulations.

[Caveat: I just saw this today and haven't played with the software at 
all...]

cheers, Dave

-- 
Dave Atkins, PhD
Research Associate Professor
Center for the Study of Health and Risk Behaviors
Department of  Psychiatry and Behavioral Science
1100 NE 45th Street, Suite 300
Seattle, WA  98105
206-616-3879
datkins at u.washington.edu

-------- Original Message --------
Subject: Announce: New (free) beta version of MLPowSim software
Date: Fri, 20 Mar 2009 13:06:47 +0000
From: WJ Browne <William.Browne at BRISTOL.AC.UK>
Reply-To: Multilevel modelling discussion list <MULTILEVEL at JISCMAIL.AC.UK>
To: MULTILEVEL at JISCMAIL.AC.UK

Dear All Multilevellers,
We are pleased to make available a new free piece of software MLPowSim
that is designed for performing sample size/power calculations in
multilevel models via simulation.
The software package has been developed as part of a UK ESRC
funded-project and is an 'old-fashioned' text input program that creates
files that can be used in conjunction with MLwiN or R to perform the
necessary computations to perform complex power calculations.
The program is available along with an extensive 150 page manual from the
webpage <http://seis.bris.ac.uk/~frwjb/esrc.html>
We will describe the software currently as a Beta version as we have only
had time to do preliminary testing and we haven't included much error
trapping. The software is FREE and as such comes with no guarantees in
terms of producing correct answers (although we hope it does!) and no
guarantee of fast response to fixing of any bugs reported (although we hope
there aren't many). We will however be genuinely pleased if people use it
and let us know of any bugs they find or if they have a 'wish list' of
additional features they might like.
  If you do use the software and would like to give feedback, bug reports
or a wish list please E-mail either Richard Parker
(richard.parker at bristol.ac.uk) or me (frwjb at bristol.ac.uk).
  Good luck with the software,
    Best wishes,
      Bill Browne.

P.S. Note for MLwiN users there are further goodies (which are very much
work in progress and will be changing over time) with regard new MCMC
features in MLwiN, many of which you can try out in the current version.
Again these are currently hidden to the user by default and have not been
thoroughly tested so any bugs you spot please let me know.

----------------------
WJ Browne
frwjb at bristol.ac.uk

-------------------------- Multilevel list --------------------------
To leave, send    leave multilevel    to jiscmail at jiscmail.ac.uk
For further info about the Multilevel list, please see
http://www.jiscmail.ac.uk/lists/multilevel.html     and
http://www.nursing.teaching.man.ac.uk/staff/mcampbell/multilevel.html



From a.renwick at abdn.ac.uk  Fri Mar 20 20:40:07 2009
From: a.renwick at abdn.ac.uk (Renwick, A. R.)
Date: Fri, 20 Mar 2009 19:40:07 +0000
Subject: [R-sig-ME] Predicted values and confidence intervals
Message-ID: <B9D1301370916C44B5874AF340C18B9B8F0D8B1BF0@VMAILB.uoa.abdn.ac.uk>

Dear All

I have tried to calculate the predicted values of my model estimates based only on the fixed effects from a GLMM model with Binomial error using the lme4 package. I graphed these along with te observed data.
I would be grateful if anybody has any comments on this.

An example of the code is:
#model of tick presence with width, Nhat (vole abundance) and alt(alternative host abundance) as variables
ball<-lmer(TrianPresence~width+Nhat+alt+(1|Farm/LocTran), family=binomial, data=tick, REML=FALSE)

#I then created a function to backtransform the logit estimates
invlogit<-function(x){1/(1+exp(-x))}

#I then created function to jitter the binary observed data while keeping pts between 0 and 1
jitter.binary<-function(a, jitt=0.05){
ifelse(a==0,runif(length(a),0,jitt),runif(length(a),1-jitt,1))}

#I then graphed the observed data against the variable I was interested in (Nhat)
tick.jitter<-jitter.binary(tick$TrianPresence)

plot(tick$Nhat,tick.jitter,xlim=c(0,max(tick$Nhat)),ylab="predicted probability",xlab="vole abundance",cex.axis=1.3,cex.lab=1.3, ylim=c(0,1))

#I then added the predicted probabilities of the fixed effects holding the other variables constant
curve(invlogit(cbind(1,0,x,0)%*%fixef(ball)),add=TRUE,lty=1,lwd=3)


Now this does not produce any standard errors but I was wondering if it was possible to add a 95% confidence interval to this based on 1.95*St Dev of the random effect.  If I had a simple structure such as (1|Group) then this would be simplier but I have a nested structure (1|Farm/LocTran).  I therefore took the St Dev of the between LocTran within Farm to be the most conservative:

#so to add the 95% CI when the St Dev of  between LocTran within Farm is 0.2913
curve(invlogit(cbind(1,0,x,0)%*%fixef(ball)+(1.95*0.2913)),add=TRUE,lty=1,lwd=3)
curve(invlogit(cbind(1,0,x,0)%*%fixef(ball)-(1.95*0.2913)),add=TRUE,lty=1,lwd=3)

Many thanks,
Anna

Anna Renwick
Institute of Biological & Environment Sciences
University of Aberdeen
Zoology Building
Tillydrone Avenue
Aberdeen
AB24 2TZ


The University of Aberdeen is a charity registered in Scotland, No SC013683.



From brant.inman at me.com  Sun Mar 22 05:20:59 2009
From: brant.inman at me.com (Brant Inman)
Date: Sat, 21 Mar 2009 23:20:59 -0500
Subject: [R-sig-ME] Multilevel logistic regression
Message-ID: <44235240692821956752143982874329716044-Webmail@me.com>


Thierry et al:

I think I solved my problem and it provides some insight into the way lmer handles binomial data, so I will share the findings here. First, I have made two datasets available on the web, a long format and a wide format version of the "metaresp" data of Joop Hox..  They can be found here

http://www.duke.edu/~bi6/

Hox has a draft of the chapter of interest that discusses the metaresp dataset and the modeling process/problem that I am trying to solve.  Note that the results that I am trying to reproduce are in Tables 6.3 and 6.4.  The chapter can be found at:

http://www.geocities.com/joophox/papers/chap6.pdf

Now here are the models that I fit with lmer.  Assume that the wide version of the data is called "wide" and the long version "long".
-----------------------------

y <- cbind(wide$SUCCESS, wide$FAIL)

f1 <- lmer(y ~ RESPISRR + (1 | SOURCE), family=binomial, data=wide)
summary(f1)

f2 <- lmer(y ~ RESPISRR + TELDUM + MAILDUM + (1 | SOURCE), family=binomial, data=wide)
summary(f2)

f3 <- lmer(y ~  ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM + MAILDUM | SOURCE), 
                family=binomial, data=wide)
summary(f3)

f4 <- lmer(SUCCESS ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM + MAILDUM | SOURCE), 
		family=binomial, data=long)
summary(f4)

-------------------------------

Models f1, f2, and f4 work and reproduce the results of Hox.  Model f4 takes a hell of a long time to compute, but it seems to give the expected results.  Model f3, which I assumed (wrongly) would be the same as f4, does not work.  Instead, when it is run, you get the error message:

> Error in mer_finalize(ans) : q = 240 > n = 105

I guess the question that I have now is: did I do something wrong with model f3 or is lmer doing something unusual?  My assumption that models f3 and f4 were the same comes from MASS4 p190 where Ripley describes the glm function for logistic regression.

I very much appreciate any insight.

Brant

#####################################################################################
 
On Friday, March 20, 2009, at 04:32AM, "ONKELINX, Thierry" <Thierry.ONKELINX at inbo.be> wrote:
>Dear Brant,
>
>The model is too complex. You have maximum three observations for each
>level of the random effect. Allowing for a random intercept and two
>random slopes does not make much sense then. Does it?
>
>HTH,
>
>Thierry
>
>
>------------------------------------------------------------------------
>----
>ir. Thierry Onkelinx
>Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>and Forest
>Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
>methodology and quality assurance
>Gaverstraat 4
>9500 Geraardsbergen
>Belgium 
>tel. + 32 54/436 185
>Thierry.Onkelinx at inbo.be 
>www.inbo.be 
>
>To call in the statistician after the experiment is done may be no more
>than asking him to perform a post-mortem examination: he may be able to
>say what the experiment died of.
>~ Sir Ronald Aylmer Fisher
>
>The plural of anecdote is not data.
>~ Roger Brinner
>
>The combination of some data and an aching desire for an answer does not
>ensure that a reasonable answer can be extracted from a given body of
>data.
>~ John Tukey
>
>-----Oorspronkelijk bericht-----
>Van: r-sig-mixed-models-bounces at r-project.org
>[mailto:r-sig-mixed-models-bounces at r-project.org] Namens Brant Inman
>Verzonden: donderdag 19 maart 2009 3:11
>Aan: r-sig-mixed-models at r-project.org
>Onderwerp: [R-sig-ME] Multilevel logistic regression
>
>
>lmer Experts:
>
>I am trying to use lmer to duplicate the results found in Joop Hox's  
>book "Multilevel Analysis: technique and applications" 2002.  In  
>chapter 6 of his book he shows an example of multilevel logistic  
>regression for a meta-analysis of survey response rates.  The data are  
>available in the file "metaresp.xls" at his website:
>
><http://www.geocities.com/joophox/mlbook/excelxls.zip>
>
>The dataset includes the following variables of interest:
>
>Individual level (Level 1) variables:
>TELDUM	 = telephone questioning
>MAILDUM  = mail questioning
>RESPONSE = the outcome of interest, the study response rate
>DENOM    = the number of people questioned
>
>Study/group level (Level 2) variables:
>SOURCE 	 = the study identifier
>YEAR	 = year of study
>SALIENCY = how salient the questionnaire was (0 to 2)
>RESPISRR = the way the response rate was calculated
>
>
>The null model (Table 6.2) proposed by Joop is easy to fit:
>
>SUCCESS <- as.integer(RESPISRR*DENOM)
>y  	<- cbind(SUCCESS, DENOM-SUCCESS)
>
>f1 <- lmer(y ~ RESPISRR + (1 | SOURCE), family=binomial(link=logit))
>
>
>Joop then adds a couple Level 1 variables (Table 6.3):
>
>f2 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (1 | SOURCE),  
>family=binomial(link=logit))
>
>
>He then says that these two Level 1 variables should be allowed to  
>vary across studies (varying slopes).  When I try to fit what I  
>believe to be the correct model, I get an error
>
>
>f3 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (TELNUM | SOURCE) +  
>(MAILDUM | SOURCE)
>	+ (1 | SOURCE), family=binomial(link=logit))
>
>Error in mer_finalize(ans) : q = 240 > n = 105
>
>
>Can anyone tell me what I am doing wrong here?  Thanks so much in  
>advance.
>
>Brant Inman
>Duke University Medical Center
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
>en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
>door een geldig ondertekend document. The views expressed in  this message 
>and any annex are purely those of the writer and may not be regarded as stating 
>an official position of INBO, as long as the message is not confirmed by a duly 
>signed document.
>
>



From bolker at ufl.edu  Sun Mar 22 05:52:51 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Sun, 22 Mar 2009 00:52:51 -0400
Subject: [R-sig-ME] Multilevel logistic regression
In-Reply-To: <44235240692821956752143982874329716044-Webmail@me.com>
References: <44235240692821956752143982874329716044-Webmail@me.com>
Message-ID: <49C5C423.9070203@ufl.edu>

   I tried this out with a hacked version of lme4 that removes
the test on the number of observations, and get reasonable answers
for f3, to wit:

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SOURCE (Intercept) 0.78845  0.88795
        TELDUM      0.24216  0.49210  -0.281
        MAILDUM     0.52215  0.72260  -0.365  0.317
Number of obs: 105, groups: SOURCE, 48

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  1.12762    0.19519   5.777  7.6e-09 ***
RESPISRR     0.20714    0.21497   0.964 0.335258
TELDUM      -0.20251    0.09141  -2.215 0.026737 *
MAILDUM     -0.56094    0.14708  -3.814 0.000137 ***
---

  The random effects (esp for telephone/mail) are a little different.
It doesn't look like the software used in the chapter estimates
correlations among random effects?

## Table 6.4 Models for response rates in different conditions
## Fixed part           conditions fixed       conditions random
##                      coeff. (s.e.)          coeff. (s.e.)
## Predictor
## intercept            0.90 (.14)             1.17 (.21)
## resptype             0.53 (.06)             0.20 (.23)
## telephone            -0.16 (.02)            -0.20 (.10)
## mail                 -0.49 (.03)            -0.58 (.16)
## Random part
## intercept1           1.00                   1.00
## intercept2           0.86 (.18)             0.87 (.20)
## telephone                                   0.26 (.08)
## mail                                        0.59 (.20)

Eliminating the correlation among random effects:

f3B <- lmer(y  ~ RESPISRR + TELDUM + MAILDUM + (1|SOURCE) +
(0+TELDUM|SOURCE) + (0+MAILDUM|SOURCE),
                family=binomial, data=wide)
summary(f3B)

Random effects:
 Groups Name        Variance Std.Dev.
 SOURCE (Intercept) 0.72203  0.84972
 SOURCE TELDUM      0.23582  0.48562
 SOURCE MAILDUM     0.43923  0.66274
Number of obs: 105, groups: SOURCE, 48


  telephone/mail random effects still estimated higher.
Tried nAGQ=6 (crank up Gauss-Hermite quadrature):

Random effects:
 Groups Name        Variance Std.Dev.
 SOURCE (Intercept) 0.74275  0.86183
 SOURCE TELDUM      0.24565  0.49563
 SOURCE MAILDUM     0.28567  0.53448
Number of obs: 105, groups: SOURCE, 48

Brings mail RE down (*below* PQL est.) but telephone RE is still high.




Brant Inman wrote:
> Thierry et al:
> 
> I think I solved my problem and it provides some insight into the way
> lmer handles binomial data, so I will share the findings here. First,
> I have made two datasets available on the web, a long format and a
> wide format version of the "metaresp" data of Joop Hox..  They can be
> found here
> 
> http://www.duke.edu/~bi6/
> 
> Hox has a draft of the chapter of interest that discusses the
> metaresp dataset and the modeling process/problem that I am trying to
> solve.  Note that the results that I am trying to reproduce are in
> Tables 6.3 and 6.4.  The chapter can be found at:
> 
> http://www.geocities.com/joophox/papers/chap6.pdf
> 
> Now here are the models that I fit with lmer.  Assume that the wide
> version of the data is called "wide" and the long version "long". 
> -----------------------------
> 
> y <- cbind(wide$SUCCESS, wide$FAIL)
> 
> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE), family=binomial, data=wide) 
> summary(f1)
> 
> f2 <- lmer(y ~ RESPISRR + TELDUM + MAILDUM + (1 | SOURCE),
> family=binomial, data=wide) summary(f2)
> 
> f3 <- lmer(y ~  ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM + MAILDUM
> | SOURCE), family=binomial, data=wide) summary(f3)
> 
> f4 <- lmer(SUCCESS ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM +
> MAILDUM | SOURCE), family=binomial, data=long) summary(f4)
> 
> -------------------------------
> 
> Models f1, f2, and f4 work and reproduce the results of Hox.  Model
> f4 takes a hell of a long time to compute, but it seems to give the
> expected results.  Model f3, which I assumed (wrongly) would be the
> same as f4, does not work.  Instead, when it is run, you get the
> error message:
> 
>> Error in mer_finalize(ans) : q = 240 > n = 105
> 
> I guess the question that I have now is: did I do something wrong
> with model f3 or is lmer doing something unusual?  My assumption that
> models f3 and f4 were the same comes from MASS4 p190 where Ripley
> describes the glm function for logistic regression.
> 
> I very much appreciate any insight.
> 
> Brant
> 
> #####################################################################################
> 
> 
> On Friday, March 20, 2009, at 04:32AM, "ONKELINX, Thierry"
> <Thierry.ONKELINX at inbo.be> wrote:
>> Dear Brant,
>> 
>> The model is too complex. You have maximum three observations for
>> each level of the random effect. Allowing for a random intercept
>> and two random slopes does not make much sense then. Does it?
>> 
>> HTH,
>> 
>> Thierry
>> 
>> 
>> ------------------------------------------------------------------------
>>  ---- ir. Thierry Onkelinx Instituut voor natuur- en bosonderzoek /
>> Research Institute for Nature and Forest Cel biometrie,
>> methodologie en kwaliteitszorg / Section biometrics, methodology
>> and quality assurance Gaverstraat 4 9500 Geraardsbergen Belgium 
>> tel. + 32 54/436 185 Thierry.Onkelinx at inbo.be www.inbo.be
>> 
>> To call in the statistician after the experiment is done may be no
>> more than asking him to perform a post-mortem examination: he may
>> be able to say what the experiment died of. ~ Sir Ronald Aylmer
>> Fisher
>> 
>> The plural of anecdote is not data. ~ Roger Brinner
>> 
>> The combination of some data and an aching desire for an answer
>> does not ensure that a reasonable answer can be extracted from a
>> given body of data. ~ John Tukey
>> 
>> -----Oorspronkelijk bericht----- Van:
>> r-sig-mixed-models-bounces at r-project.org 
>> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Brant
>> Inman Verzonden: donderdag 19 maart 2009 3:11 Aan:
>> r-sig-mixed-models at r-project.org Onderwerp: [R-sig-ME] Multilevel
>> logistic regression
>> 
>> 
>> lmer Experts:
>> 
>> I am trying to use lmer to duplicate the results found in Joop
>> Hox's book "Multilevel Analysis: technique and applications" 2002.
>> In chapter 6 of his book he shows an example of multilevel logistic
>>  regression for a meta-analysis of survey response rates.  The data
>> are available in the file "metaresp.xls" at his website:
>> 
>> <http://www.geocities.com/joophox/mlbook/excelxls.zip>
>> 
>> The dataset includes the following variables of interest:
>> 
>> Individual level (Level 1) variables: TELDUM	 = telephone
>> questioning MAILDUM  = mail questioning RESPONSE = the outcome of
>> interest, the study response rate DENOM    = the number of people
>> questioned
>> 
>> Study/group level (Level 2) variables: SOURCE 	 = the study
>> identifier YEAR	 = year of study SALIENCY = how salient the
>> questionnaire was (0 to 2) RESPISRR = the way the response rate was
>> calculated
>> 
>> 
>> The null model (Table 6.2) proposed by Joop is easy to fit:
>> 
>> SUCCESS <- as.integer(RESPISRR*DENOM) y  	<- cbind(SUCCESS,
>> DENOM-SUCCESS)
>> 
>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE),
>> family=binomial(link=logit))
>> 
>> 
>> Joop then adds a couple Level 1 variables (Table 6.3):
>> 
>> f2 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (1 | SOURCE), 
>> family=binomial(link=logit))
>> 
>> 
>> He then says that these two Level 1 variables should be allowed to
>>  vary across studies (varying slopes).  When I try to fit what I 
>> believe to be the correct model, I get an error
>> 
>> 
>> f3 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (TELNUM | SOURCE) + 
>> (MAILDUM | SOURCE) + (1 | SOURCE), family=binomial(link=logit))
>> 
>> Error in mer_finalize(ans) : q = 240 > n = 105
>> 
>> 
>> Can anyone tell me what I am doing wrong here?  Thanks so much in
>>  advance.
>> 
>> Brant Inman Duke University Medical Center
>> 
>> [[alternative HTML version deleted]]
>> 
>> _______________________________________________ 
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> 
>> Dit bericht en eventuele bijlagen geven enkel de visie van de
>> schrijver weer en binden het INBO onder geen enkel beding, zolang
>> dit bericht niet bevestigd is door een geldig ondertekend document.
>> The views expressed in  this message and any annex are purely those
>> of the writer and may not be regarded as stating an official
>> position of INBO, as long as the message is not confirmed by a duly
>>  signed document.
>> 
>> 
> 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From brant.inman at me.com  Sun Mar 22 15:22:18 2009
From: brant.inman at me.com (Brant Inman)
Date: Sun, 22 Mar 2009 09:22:18 -0500
Subject: [R-sig-ME] Multilevel logistic regression
In-Reply-To: <49C5C423.9070203@ufl.edu>
References: <44235240692821956752143982874329716044-Webmail@me.com>
	<49C5C423.9070203@ufl.edu>
Message-ID: <75060186703337822374928343348585665411-Webmail@me.com>

Ben,

Thanks for confirming my findings. Two questions for you:

1) Is it normal that lmer does NOT find the following 3 formulae to be equivalent?

f1 <- lmer( cbind(success, fail) ~ covariates + (1 | group), data=wide ...
f2 <- lmer( (success/(success+fail)) ~ covariates + (1 | group), weights=(success+fail), data=wide ...
f3 <- lmer( success ~ covariates + (1 | group), data=long ...

2) How did you hack lmer to avoid this problem and did it compute the result faster with the wide dataset than the long one?


Brant


----------------------------------------------------------------------------------------------------------------------------
 
On Saturday, March 21, 2009, at 11:52PM, "Ben Bolker" <bolker at ufl.edu> wrote:
>   I tried this out with a hacked version of lme4 that removes
>the test on the number of observations, and get reasonable answers
>for f3, to wit:
>
>Random effects:
> Groups Name        Variance Std.Dev. Corr
> SOURCE (Intercept) 0.78845  0.88795
>        TELDUM      0.24216  0.49210  -0.281
>        MAILDUM     0.52215  0.72260  -0.365  0.317
>Number of obs: 105, groups: SOURCE, 48
>
>Fixed effects:
>            Estimate Std. Error z value Pr(>|z|)
>(Intercept)  1.12762    0.19519   5.777  7.6e-09 ***
>RESPISRR     0.20714    0.21497   0.964 0.335258
>TELDUM      -0.20251    0.09141  -2.215 0.026737 *
>MAILDUM     -0.56094    0.14708  -3.814 0.000137 ***
>---
>
>  The random effects (esp for telephone/mail) are a little different.
>It doesn't look like the software used in the chapter estimates
>correlations among random effects?
>
>## Table 6.4 Models for response rates in different conditions
>## Fixed part           conditions fixed       conditions random
>##                      coeff. (s.e.)          coeff. (s.e.)
>## Predictor
>## intercept            0.90 (.14)             1.17 (.21)
>## resptype             0.53 (.06)             0.20 (.23)
>## telephone            -0.16 (.02)            -0.20 (.10)
>## mail                 -0.49 (.03)            -0.58 (.16)
>## Random part
>## intercept1           1.00                   1.00
>## intercept2           0.86 (.18)             0.87 (.20)
>## telephone                                   0.26 (.08)
>## mail                                        0.59 (.20)
>
>Eliminating the correlation among random effects:
>
>f3B <- lmer(y  ~ RESPISRR + TELDUM + MAILDUM + (1|SOURCE) +
>(0+TELDUM|SOURCE) + (0+MAILDUM|SOURCE),
>                family=binomial, data=wide)
>summary(f3B)
>
>Random effects:
> Groups Name        Variance Std.Dev.
> SOURCE (Intercept) 0.72203  0.84972
> SOURCE TELDUM      0.23582  0.48562
> SOURCE MAILDUM     0.43923  0.66274
>Number of obs: 105, groups: SOURCE, 48
>
>
>  telephone/mail random effects still estimated higher.
>Tried nAGQ=6 (crank up Gauss-Hermite quadrature):
>
>Random effects:
> Groups Name        Variance Std.Dev.
> SOURCE (Intercept) 0.74275  0.86183
> SOURCE TELDUM      0.24565  0.49563
> SOURCE MAILDUM     0.28567  0.53448
>Number of obs: 105, groups: SOURCE, 48
>
>Brings mail RE down (*below* PQL est.) but telephone RE is still high.
>
>
>
>
>Brant Inman wrote:
>> Thierry et al:
>> 
>> I think I solved my problem and it provides some insight into the way
>> lmer handles binomial data, so I will share the findings here. First,
>> I have made two datasets available on the web, a long format and a
>> wide format version of the "metaresp" data of Joop Hox..  They can be
>> found here
>> 
>> http://www.duke.edu/~bi6/
>> 
>> Hox has a draft of the chapter of interest that discusses the
>> metaresp dataset and the modeling process/problem that I am trying to
>> solve.  Note that the results that I am trying to reproduce are in
>> Tables 6.3 and 6.4.  The chapter can be found at:
>> 
>> http://www.geocities.com/joophox/papers/chap6.pdf
>> 
>> Now here are the models that I fit with lmer.  Assume that the wide
>> version of the data is called "wide" and the long version "long". 
>> -----------------------------
>> 
>> y <- cbind(wide$SUCCESS, wide$FAIL)
>> 
>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE), family=binomial, data=wide) 
>> summary(f1)
>> 
>> f2 <- lmer(y ~ RESPISRR + TELDUM + MAILDUM + (1 | SOURCE),
>> family=binomial, data=wide) summary(f2)
>> 
>> f3 <- lmer(y ~  ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM + MAILDUM
>> | SOURCE), family=binomial, data=wide) summary(f3)
>> 
>> f4 <- lmer(SUCCESS ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM +
>> MAILDUM | SOURCE), family=binomial, data=long) summary(f4)
>> 
>> -------------------------------
>> 
>> Models f1, f2, and f4 work and reproduce the results of Hox.  Model
>> f4 takes a hell of a long time to compute, but it seems to give the
>> expected results.  Model f3, which I assumed (wrongly) would be the
>> same as f4, does not work.  Instead, when it is run, you get the
>> error message:
>> 
>>> Error in mer_finalize(ans) : q = 240 > n = 105
>> 
>> I guess the question that I have now is: did I do something wrong
>> with model f3 or is lmer doing something unusual?  My assumption that
>> models f3 and f4 were the same comes from MASS4 p190 where Ripley
>> describes the glm function for logistic regression.
>> 
>> I very much appreciate any insight.
>> 
>> Brant
>> 
>> #####################################################################################
>> 
>> 
>> On Friday, March 20, 2009, at 04:32AM, "ONKELINX, Thierry"
>> <Thierry.ONKELINX at inbo.be> wrote:
>>> Dear Brant,
>>> 
>>> The model is too complex. You have maximum three observations for
>>> each level of the random effect. Allowing for a random intercept
>>> and two random slopes does not make much sense then. Does it?
>>> 
>>> HTH,
>>> 
>>> Thierry
>>> 
>>> 
>>> ------------------------------------------------------------------------
>>>  ---- ir. Thierry Onkelinx Instituut voor natuur- en bosonderzoek /
>>> Research Institute for Nature and Forest Cel biometrie,
>>> methodologie en kwaliteitszorg / Section biometrics, methodology
>>> and quality assurance Gaverstraat 4 9500 Geraardsbergen Belgium 
>>> tel. + 32 54/436 185 Thierry.Onkelinx at inbo.be www.inbo.be
>>> 
>>> To call in the statistician after the experiment is done may be no
>>> more than asking him to perform a post-mortem examination: he may
>>> be able to say what the experiment died of. ~ Sir Ronald Aylmer
>>> Fisher
>>> 
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> 
>>> The combination of some data and an aching desire for an answer
>>> does not ensure that a reasonable answer can be extracted from a
>>> given body of data. ~ John Tukey
>>> 
>>> -----Oorspronkelijk bericht----- Van:
>>> r-sig-mixed-models-bounces at r-project.org 
>>> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Brant
>>> Inman Verzonden: donderdag 19 maart 2009 3:11 Aan:
>>> r-sig-mixed-models at r-project.org Onderwerp: [R-sig-ME] Multilevel
>>> logistic regression
>>> 
>>> 
>>> lmer Experts:
>>> 
>>> I am trying to use lmer to duplicate the results found in Joop
>>> Hox's book "Multilevel Analysis: technique and applications" 2002.
>>> In chapter 6 of his book he shows an example of multilevel logistic
>>>  regression for a meta-analysis of survey response rates.  The data
>>> are available in the file "metaresp.xls" at his website:
>>> 
>>> <http://www.geocities.com/joophox/mlbook/excelxls.zip>
>>> 
>>> The dataset includes the following variables of interest:
>>> 
>>> Individual level (Level 1) variables: TELDUM	 = telephone
>>> questioning MAILDUM  = mail questioning RESPONSE = the outcome of
>>> interest, the study response rate DENOM    = the number of people
>>> questioned
>>> 
>>> Study/group level (Level 2) variables: SOURCE 	 = the study
>>> identifier YEAR	 = year of study SALIENCY = how salient the
>>> questionnaire was (0 to 2) RESPISRR = the way the response rate was
>>> calculated
>>> 
>>> 
>>> The null model (Table 6.2) proposed by Joop is easy to fit:
>>> 
>>> SUCCESS <- as.integer(RESPISRR*DENOM) y  	<- cbind(SUCCESS,
>>> DENOM-SUCCESS)
>>> 
>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE),
>>> family=binomial(link=logit))
>>> 
>>> 
>>> Joop then adds a couple Level 1 variables (Table 6.3):
>>> 
>>> f2 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (1 | SOURCE), 
>>> family=binomial(link=logit))
>>> 
>>> 
>>> He then says that these two Level 1 variables should be allowed to
>>>  vary across studies (varying slopes).  When I try to fit what I 
>>> believe to be the correct model, I get an error
>>> 
>>> 
>>> f3 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (TELNUM | SOURCE) + 
>>> (MAILDUM | SOURCE) + (1 | SOURCE), family=binomial(link=logit))
>>> 
>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>> 
>>> 
>>> Can anyone tell me what I am doing wrong here?  Thanks so much in
>>>  advance.
>>> 
>>> Brant Inman Duke University Medical Center
>>> 
>>> [[alternative HTML version deleted]]
>>> 
>>> _______________________________________________ 
>>> R-sig-mixed-models at r-project.org mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> 
>>> Dit bericht en eventuele bijlagen geven enkel de visie van de
>>> schrijver weer en binden het INBO onder geen enkel beding, zolang
>>> dit bericht niet bevestigd is door een geldig ondertekend document.
>>> The views expressed in  this message and any annex are purely those
>>> of the writer and may not be regarded as stating an official
>>> position of INBO, as long as the message is not confirmed by a duly
>>>  signed document.
>>> 
>>> 
>> 
>> _______________________________________________ 
>> R-sig-mixed-models at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>-- 
>Ben Bolker
>Associate professor, Biology Dep't, Univ. of Florida
>bolker at ufl.edu / www.zoology.ufl.edu/bolker
>GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>
>



From bates at stat.wisc.edu  Sun Mar 22 17:42:52 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 22 Mar 2009 11:42:52 -0500
Subject: [R-sig-ME] lmer and multiple membership
In-Reply-To: <BLU133-W5106B11B3D8C61E0217282BA970@phx.gbl>
References: <BLU133-W5106B11B3D8C61E0217282BA970@phx.gbl>
Message-ID: <40e66e0b0903220942k7d583337qd4e64b3f0b772579@mail.gmail.com>

On Fri, Mar 20, 2009 at 10:53 AM, Francois Rousseu
<francoisrousseu at hotmail.com> wrote:

> Hi everyone

> I know that lmer can handle cross-classified random effects, but can it handle random effects
> involving multiple membership as well? My problem is this: I am modelling the number of visits
> made by different individuals at every feeder they visit daily. I have three random effects:
> individual, feeder and date. There is multiple membership in terms of feeders and dates
> and an individual can potentially be seen at many different feeders which can change everyday.
> To make it simpler, here is an example of the data:

> Individual ? ? ? ? ? Date ? ? ? ? ?Feeder ? ? ? ? ?Number of visits
> ? ? ? 1 ? ? ? ? ? ? ? ? ? 155 ? ? ? ? ? ? ?A ? ? ? ? ? ? ? ? ? ? ? ? ? 13
> ? ? ? 1 ? ? ? ? ? ? ? ? ? 155 ? ? ? ? ? ? ?B ? ? ? ? ? ? ? ? ? ? ? ? ? 34
> ? ? ? 1 ? ? ? ? ? ? ? ? ? 157 ? ? ? ? ? ? ?A ? ? ? ? ? ? ? ? ? ? ? ? ? 76
> ? ? ? 2 ? ? ? ? ? ? ? ? ? 155 ? ? ? ? ? ? ?A ? ? ? ? ? ? ? ? ? ? ? ? ? 87
> ? ? ? 2 ? ? ? ? ? ? ? ? ? 156 ? ? ? ? ? ? ?B ? ? ? ? ? ? ? ? ? ? ? ? ? 34
> ? ? ? 2 ? ? ? ? ? ? ? ? ? 157 ? ? ? ? ? ? ?C ? ? ? ? ? ? ? ? ? ? ? ? ? 23
> ? ? ? 3 ? ? ? ? ? ? ? ? ? 155 ? ? ? ? ? ? ?A ? ? ? ? ? ? ? ? ? ? ? ? ? ? 3
> ? ? ? 3 ? ? ? ? ? ? ? ? ? 155 ? ? ? ? ? ? ?D ? ? ? ? ? ? ? ? ? ? ? ? 123
> ? ? ? 3 ? ? ? ? ? ? ? ? ? 156 ? ? ? ? ? ? ?D ? ? ? ? ? ? ? ? ? ? ? ? ? 12
> ? ? ? 3 ? ? ? ? ? ? ? ? ? 157 ? ? ? ? ? ? ?A ? ? ? ? ? ? ? ? ? ? ? ? ? 56
> ? ? ? 3 ? ? ? ? ? ? ? ? ? 157 ? ? ? ? ? ? ?E ? ? ? ? ? ? ? ? ? ? ? ? ? 24
> ? ? ? 3 ? ? ? ? ? ? ? ? ? 168 ? ? ? ? ? ? ?A ? ? ? ? ? ? ? ? ? ? ? ? ?45
> ? ? ? 3 ? ? ? ? ? ? ? ? ? 168 ? ? ? ? ? ? ?B ? ? ? ? ? ? ? ? ? ? ? ? ? ?6
> ? ? ? 3 ? ? ? ? ? ? ? ? ? 168 ? ? ? ? ? ? ?C ? ? ? ? ? ? ? ? ? ? ? ? ?78

Thank you for sending a section of the data.  Based on what you have
described I would say that individual, date and feeder are partially
crossed factors and it would be legitimate to fit the model you show
below.

> If I am not interested in date as a fixed effect, I have to put it in the model as a random effect
> and I?m wondering if specifying the model like this in lmer is the thing to do (not interested in
> varying slopes yet):

> fixed effects + (1|individual) + (1|feeder) + (1|date)

> However, if I am interested by the date as a fixed effect, do I have to specify it in the random
> effects as well to indicate that the repeated measures on individuals are related to feeders and
> dates? If so, does it have to be specified in any special way?

I'm not quite sure what the question is but I think the answer is
"no".  The structure of the data is determined by the factors
themselves.  If Date is a categorical covariate it will generate
equivalent structures in the model matrices whether it is a
fixef-effects term or a random-effects term, although the coefficient
estimates will be somewhat different.  I don't see a need to include
that covariate in both the fixed effects and the random effects.



From bates at stat.wisc.edu  Sun Mar 22 18:41:27 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 22 Mar 2009 12:41:27 -0500
Subject: [R-sig-ME] Multilevel logistic regression
In-Reply-To: <75060186703337822374928343348585665411-Webmail@me.com>
References: <44235240692821956752143982874329716044-Webmail@me.com>
	<49C5C423.9070203@ufl.edu>
	<75060186703337822374928343348585665411-Webmail@me.com>
Message-ID: <40e66e0b0903221041h678fd928ld2308df947acf72e@mail.gmail.com>

On Sun, Mar 22, 2009 at 9:22 AM, Brant Inman <brant.inman at me.com> wrote:
> Ben,

> Thanks for confirming my findings. Two questions for you:

> 1) Is it normal that lmer does NOT find the following 3 formulae to be equivalent?

> f1 <- lmer( cbind(success, fail) ~ covariates + (1 | group), data=wide ...
> f2 <- lmer( (success/(success+fail)) ~ covariates + (1 | group), weights=(success+fail), data=wide ...
> f3 <- lmer( success ~ covariates + (1 | group), data=long ...

The code in lmer to count the "number of observations" is too
simplistic.  It uses the length of the vector of responses which is
appropriate except for the case of models like f1.  What should be
done in that case is the evaluate the sum of the number of cases
represented by each row of the data.

Tracking such things down is not terribly easy.  The "initialize"
expression from a glm family is "not unlike" a gross hack. It is an
expression (not a function) that is evaluated for the sole purpose of
scattering a bunch of objects around the function evaluation
environment, which often ends up looking like a teenager's bedroom
afterwards.

Yes, special purpose code could be written to detect a situation like
model f1 and account for it appropriately.  Doing so will mean keeping
track of two, possibly different, definitions of the "number of
observations".  If someone is willing to produce a patch for the code
I will consider it.

I don't think it will be as easy to detect that model f2 represents
more observations than are recorded in the data.  All you know is that
you are given observation weights, which could be the result of
effects other than multiple "observations" per observation.

> 2) How did you hack lmer to avoid this problem and did it compute the result faster with the wide dataset than the long one?

Well, you start by grepping the source code to determine where the
error message is produced then you disable that part of the code.

The error message is there for a reason - primarily related to linear
mixed models.  Sometimes people get carried away and define random
effects with respect to a factor that has a distinct level for each
observation.  You can estimate the parameters in such a model but not
uniquely.  The variance due to such a "one level per observation"
factor is completely confounded with the variance of the
"per-observation" noise term.  People have reported results from such
models fit by lme so I wanted to protect against that by flagging such
models in lme4.  One could adopt a "caveat emptor" approach and say
that the user is responsible for ensuring that they model that they
fit is mathematically reasonable so it they get nonsense results for a
nonsense model it's their fault.  Unfortunately, that ends up
reflecting badly on lme4 or the R project.  People fail to distinguish
between a package in R and R in general so this type of problem is
reported as a flaw in R, often by people who want to convince
companies to pony up large amounts of money for commercial software
licenses.

Hence, I have opted for the more conservative approach of throwing an
error in such cases.   As Ben has pointed out, lme4 is open source so
if you want to modify it to fit a particular type of model that
currently produces an error, you have the opportunity of doing so.


> ----------------------------------------------------------------------------------------------------------------------------
>
> On Saturday, March 21, 2009, at 11:52PM, "Ben Bolker" <bolker at ufl.edu> wrote:
>> ? I tried this out with a hacked version of lme4 that removes
>>the test on the number of observations, and get reasonable answers
>>for f3, to wit:
>>
>>Random effects:
>> Groups Name ? ? ? ?Variance Std.Dev. Corr
>> SOURCE (Intercept) 0.78845 ?0.88795
>> ? ? ? ?TELDUM ? ? ?0.24216 ?0.49210 ?-0.281
>> ? ? ? ?MAILDUM ? ? 0.52215 ?0.72260 ?-0.365 ?0.317
>>Number of obs: 105, groups: SOURCE, 48
>>
>>Fixed effects:
>> ? ? ? ? ? ?Estimate Std. Error z value Pr(>|z|)
>>(Intercept) ?1.12762 ? ?0.19519 ? 5.777 ?7.6e-09 ***
>>RESPISRR ? ? 0.20714 ? ?0.21497 ? 0.964 0.335258
>>TELDUM ? ? ?-0.20251 ? ?0.09141 ?-2.215 0.026737 *
>>MAILDUM ? ? -0.56094 ? ?0.14708 ?-3.814 0.000137 ***
>>---
>>
>> ?The random effects (esp for telephone/mail) are a little different.
>>It doesn't look like the software used in the chapter estimates
>>correlations among random effects?
>>
>>## Table 6.4 Models for response rates in different conditions
>>## Fixed part ? ? ? ? ? conditions fixed ? ? ? conditions random
>>## ? ? ? ? ? ? ? ? ? ? ?coeff. (s.e.) ? ? ? ? ?coeff. (s.e.)
>>## Predictor
>>## intercept ? ? ? ? ? ?0.90 (.14) ? ? ? ? ? ? 1.17 (.21)
>>## resptype ? ? ? ? ? ? 0.53 (.06) ? ? ? ? ? ? 0.20 (.23)
>>## telephone ? ? ? ? ? ?-0.16 (.02) ? ? ? ? ? ?-0.20 (.10)
>>## mail ? ? ? ? ? ? ? ? -0.49 (.03) ? ? ? ? ? ?-0.58 (.16)
>>## Random part
>>## intercept1 ? ? ? ? ? 1.00 ? ? ? ? ? ? ? ? ? 1.00
>>## intercept2 ? ? ? ? ? 0.86 (.18) ? ? ? ? ? ? 0.87 (.20)
>>## telephone ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0.26 (.08)
>>## mail ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0.59 (.20)
>>
>>Eliminating the correlation among random effects:
>>
>>f3B <- lmer(y ?~ RESPISRR + TELDUM + MAILDUM + (1|SOURCE) +
>>(0+TELDUM|SOURCE) + (0+MAILDUM|SOURCE),
>> ? ? ? ? ? ? ? ?family=binomial, data=wide)
>>summary(f3B)
>>
>>Random effects:
>> Groups Name ? ? ? ?Variance Std.Dev.
>> SOURCE (Intercept) 0.72203 ?0.84972
>> SOURCE TELDUM ? ? ?0.23582 ?0.48562
>> SOURCE MAILDUM ? ? 0.43923 ?0.66274
>>Number of obs: 105, groups: SOURCE, 48
>>
>>
>> ?telephone/mail random effects still estimated higher.
>>Tried nAGQ=6 (crank up Gauss-Hermite quadrature):
>>
>>Random effects:
>> Groups Name ? ? ? ?Variance Std.Dev.
>> SOURCE (Intercept) 0.74275 ?0.86183
>> SOURCE TELDUM ? ? ?0.24565 ?0.49563
>> SOURCE MAILDUM ? ? 0.28567 ?0.53448
>>Number of obs: 105, groups: SOURCE, 48
>>
>>Brings mail RE down (*below* PQL est.) but telephone RE is still high.
>>
>>
>>
>>
>>Brant Inman wrote:
>>> Thierry et al:
>>>
>>> I think I solved my problem and it provides some insight into the way
>>> lmer handles binomial data, so I will share the findings here. First,
>>> I have made two datasets available on the web, a long format and a
>>> wide format version of the "metaresp" data of Joop Hox.. ?They can be
>>> found here
>>>
>>> http://www.duke.edu/~bi6/
>>>
>>> Hox has a draft of the chapter of interest that discusses the
>>> metaresp dataset and the modeling process/problem that I am trying to
>>> solve. ?Note that the results that I am trying to reproduce are in
>>> Tables 6.3 and 6.4. ?The chapter can be found at:
>>>
>>> http://www.geocities.com/joophox/papers/chap6.pdf
>>>
>>> Now here are the models that I fit with lmer. ?Assume that the wide
>>> version of the data is called "wide" and the long version "long".
>>> -----------------------------
>>>
>>> y <- cbind(wide$SUCCESS, wide$FAIL)
>>>
>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE), family=binomial, data=wide)
>>> summary(f1)
>>>
>>> f2 <- lmer(y ~ RESPISRR + TELDUM + MAILDUM + (1 | SOURCE),
>>> family=binomial, data=wide) summary(f2)
>>>
>>> f3 <- lmer(y ~ ?~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM + MAILDUM
>>> | SOURCE), family=binomial, data=wide) summary(f3)
>>>
>>> f4 <- lmer(SUCCESS ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM +
>>> MAILDUM | SOURCE), family=binomial, data=long) summary(f4)
>>>
>>> -------------------------------
>>>
>>> Models f1, f2, and f4 work and reproduce the results of Hox. ?Model
>>> f4 takes a hell of a long time to compute, but it seems to give the
>>> expected results. ?Model f3, which I assumed (wrongly) would be the
>>> same as f4, does not work. ?Instead, when it is run, you get the
>>> error message:
>>>
>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>
>>> I guess the question that I have now is: did I do something wrong
>>> with model f3 or is lmer doing something unusual? ?My assumption that
>>> models f3 and f4 were the same comes from MASS4 p190 where Ripley
>>> describes the glm function for logistic regression.
>>>
>>> I very much appreciate any insight.
>>>
>>> Brant
>>>
>>> #####################################################################################
>>>
>>>
>>> On Friday, March 20, 2009, at 04:32AM, "ONKELINX, Thierry"
>>> <Thierry.ONKELINX at inbo.be> wrote:
>>>> Dear Brant,
>>>>
>>>> The model is too complex. You have maximum three observations for
>>>> each level of the random effect. Allowing for a random intercept
>>>> and two random slopes does not make much sense then. Does it?
>>>>
>>>> HTH,
>>>>
>>>> Thierry
>>>>
>>>>
>>>> ------------------------------------------------------------------------
>>>> ?---- ir. Thierry Onkelinx Instituut voor natuur- en bosonderzoek /
>>>> Research Institute for Nature and Forest Cel biometrie,
>>>> methodologie en kwaliteitszorg / Section biometrics, methodology
>>>> and quality assurance Gaverstraat 4 9500 Geraardsbergen Belgium
>>>> tel. + 32 54/436 185 Thierry.Onkelinx at inbo.be www.inbo.be
>>>>
>>>> To call in the statistician after the experiment is done may be no
>>>> more than asking him to perform a post-mortem examination: he may
>>>> be able to say what the experiment died of. ~ Sir Ronald Aylmer
>>>> Fisher
>>>>
>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>
>>>> The combination of some data and an aching desire for an answer
>>>> does not ensure that a reasonable answer can be extracted from a
>>>> given body of data. ~ John Tukey
>>>>
>>>> -----Oorspronkelijk bericht----- Van:
>>>> r-sig-mixed-models-bounces at r-project.org
>>>> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Brant
>>>> Inman Verzonden: donderdag 19 maart 2009 3:11 Aan:
>>>> r-sig-mixed-models at r-project.org Onderwerp: [R-sig-ME] Multilevel
>>>> logistic regression
>>>>
>>>>
>>>> lmer Experts:
>>>>
>>>> I am trying to use lmer to duplicate the results found in Joop
>>>> Hox's book "Multilevel Analysis: technique and applications" 2002.
>>>> In chapter 6 of his book he shows an example of multilevel logistic
>>>> ?regression for a meta-analysis of survey response rates. ?The data
>>>> are available in the file "metaresp.xls" at his website:
>>>>
>>>> <http://www.geocities.com/joophox/mlbook/excelxls.zip>
>>>>
>>>> The dataset includes the following variables of interest:
>>>>
>>>> Individual level (Level 1) variables: TELDUM ? ? ? ? = telephone
>>>> questioning MAILDUM ?= mail questioning RESPONSE = the outcome of
>>>> interest, the study response rate DENOM ? ?= the number of people
>>>> questioned
>>>>
>>>> Study/group level (Level 2) variables: SOURCE ? ? ? ?= the study
>>>> identifier YEAR ? ? ?= year of study SALIENCY = how salient the
>>>> questionnaire was (0 to 2) RESPISRR = the way the response rate was
>>>> calculated
>>>>
>>>>
>>>> The null model (Table 6.2) proposed by Joop is easy to fit:
>>>>
>>>> SUCCESS <- as.integer(RESPISRR*DENOM) y ? ? <- cbind(SUCCESS,
>>>> DENOM-SUCCESS)
>>>>
>>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE),
>>>> family=binomial(link=logit))
>>>>
>>>>
>>>> Joop then adds a couple Level 1 variables (Table 6.3):
>>>>
>>>> f2 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (1 | SOURCE),
>>>> family=binomial(link=logit))
>>>>
>>>>
>>>> He then says that these two Level 1 variables should be allowed to
>>>> ?vary across studies (varying slopes). ?When I try to fit what I
>>>> believe to be the correct model, I get an error
>>>>
>>>>
>>>> f3 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (TELNUM | SOURCE) +
>>>> (MAILDUM | SOURCE) + (1 | SOURCE), family=binomial(link=logit))
>>>>
>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>>
>>>>
>>>> Can anyone tell me what I am doing wrong here? ?Thanks so much in
>>>> ?advance.
>>>>
>>>> Brant Inman Duke University Medical Center
>>>>
>>>> [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>> Dit bericht en eventuele bijlagen geven enkel de visie van de
>>>> schrijver weer en binden het INBO onder geen enkel beding, zolang
>>>> dit bericht niet bevestigd is door een geldig ondertekend document.
>>>> The views expressed in ?this message and any annex are purely those
>>>> of the writer and may not be regarded as stating an official
>>>> position of INBO, as long as the message is not confirmed by a duly
>>>> ?signed document.
>>>>
>>>>
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>--
>>Ben Bolker
>>Associate professor, Biology Dep't, Univ. of Florida
>>bolker at ufl.edu / www.zoology.ufl.edu/bolker
>>GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>>
>>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From christina.bogner at uni-bayreuth.de  Mon Mar 23 08:22:58 2009
From: christina.bogner at uni-bayreuth.de (Christina Bogner)
Date: Mon, 23 Mar 2009 08:22:58 +0100
Subject: [R-sig-ME] Variable transformation and back transformation
In-Reply-To: <40e66e0b0903180629o3fbf9de5m7bb6505b3cca6cc6@mail.gmail.com>
References: <49B8BE87.3080809@uni-bayreuth.de>
	<40e66e0b0903180629o3fbf9de5m7bb6505b3cca6cc6@mail.gmail.com>
Message-ID: <49C738D2.5000704@uni-bayreuth.de>

Douglas Bates schrieb:
> On Thu, Mar 12, 2009 at 2:49 AM, Christina Bogner
> <christina.bogner at uni-bayreuth.de> wrote:
>   
>> Dear all,
>>
>> I have fitted a couple of mixed-effects models to environmental data
>> (chemical and physical soil parameters) with log-transformed dependent
>> variables. I tried generalized mixed-models, but the results were not
>> satisfactory (probably because I am a soil scientist and not a statistician
>> ;-)) Now, as log of concentrations are ecologically not very informative, I
>> would like to back-transform my model parameters. Taking a Gaussian linear
>> mixed-model:
>>     
>
>   
>> log(Mg2)=intercept+beta1*Silt+beta2*Soil.depth+beta3*Flow.region+b1*Plot+b2*/Soil.Depth%in%Plot+var
>> where Mg2 is the concentration of magnesium, betas are fixed-effects and bs
>> random ones. All independent variables except Silt are factors; Silt is
>> continuous.
>>     
>
>   
>> I would write:
>> Mg2=exp(intercept+beta1*mean(Silt in respective
>> Soil.Depth)+beta3*Flow.region+estimate of b1*Plot + estimate of
>> b2*/Soil.Depth%in%Plot+0.5*var)
>> to back-transform to the original scale on the Soil.Depth-level.
>>     
>
>   
>> To back-transform the fixed-effects only, I would drop the estimates of the
>> random-effects:
>> Mg2=exp(intercept+beta1*mean(Silt in respective
>> Soil.Depth)+beta3*Flow.region+ 0.5*var)
>>     
>
>   
>> This approach treats the estimated random effects as dummies, not as an
>> additional variance. Is this right?
>>     
>
>   
Dear Dr. Bates,

thank you very much for your answer.
> I'm not sure exactly what you mean by treating the estimated random
> effects as dummies.
>   
By dummy, I mean just treating the random effects as if they were an 
additional effect. So when calculating the backtransformation, I just 
add the estimated random effect for the respective level to the 
estimates of fixed-effects.
But honestly, I have a problem with this approach. For me, 
random-effects are something like additional variance. In a simple 
linear model, when transforming from log-scale to the original scale, 
variance is multiplied by 0.5. And in my transformation equation I just 
add the estimated random effect.
> In a linear mixed model the random effects are incorporated
> additively.  It is common with data like concentrations that the
> effect of different levels of variability is more appropriately
> modelled as a multiplicative change than as an additive change, which
> corresponds to the additive change on the scale of the logarithm of
> the concentration.
>
> I would try to communicate this graphically by plotting the magnesium
> concentration under various conditions and then plotting the logarithm
> of this concentration.  I would hope to use this to overcome
> resistance to the idea of using a transformation, such as when you say
> that logarithms of concentrations are not meaningful ecologically.  I
> have been fortunate to be present at many informal consulting sessions
> led by the great statistician George Box who started his career as a
> chemist at ICI, a British chemical company.  George always wants to
> examine the data graphically and consider appropriate transformation
> (the Box-Cox transformation family are the result of his work with Sir
> David Cox).  He is aware of the resistance to transformation and has,
> somewhat but not entirely facetiously, suggested that one way around
> it is simply to create a new unit.  Recall that pH is the logarithm of
> the hydrogen ion concentration.  Other examples are decibels
> (logarithm of sound pressure) and octaves (doubling or halving the
> frequency).
>   
You are absolutely right about the pH. But it is already more than 
difficult to communicate the need for a mixed-effects model, even if 
samples are extracted hierarchically. So backtransformation is like 
"forgive me the complicated statistical approach, but I can tell you how 
much magnesium is in the subsoil".
> A summary of a random variable using location and scale parameters is
> meaningful when the distribution is reasonably symmetric.  It is not
> as easy to summarize asymmetric distributions.  (A log-normal
> distribution is more complicated than a normal distribution.)  In more
> general models, such as a linear mixed model, we can simplify the
> description of the model under the appropriate scale but if we try to
> back-transform then the description becomes much more complicated.
>
> I'm not sure that this is addressing your question.  I think you are
> trying to determine a simple way of communicating the meaning of the
> parameter estimates on the concentration scale and, if so, my answer
> is that they don't have a nice simple meaning on that scale.
>
> One thing I noticed in your model is that Soil.Depth is being treated
> as a categorical covariate, as opposed to a continuous covariate.  Was
> the experimental design such that a fixed set of soil depths were
> used?  In other words I am wondering if a model like
>
> log(Mg2) ~ Silt + Flow + (Depth | Plot)
>
> might be more appropriate than
>
> log(Mg2) ~ Silt + Flow + (1 | Depth:Plot) + (1 | Plot)
>
> Of course, the first model does assume that the effect of soil depth
> on magnesium concentration is linear and that may not be appropriate,
> although I would be tempted to store soil depth as at least an ordered
> factor so I could check on the relative importance of linear and
> higher order terms.
>   
Indeed, we had to use fixed soil depths (horizons) because a certain 
amount of soil material is needed for the analysis. So depth designates 
different (chemically distinct) soil subunits. In further studies we 
will be able to analyse small portions of the soil in situ, so that 
depth will be a numerical variable. (I am really excited about producing 
more data and going beyond my actual simplistic approach ;-)).

Thank you again

Christina Bogner



From christina.bogner at uni-bayreuth.de  Mon Mar 23 11:58:36 2009
From: christina.bogner at uni-bayreuth.de (Christina Bogner)
Date: Mon, 23 Mar 2009 11:58:36 +0100
Subject: [R-sig-ME] Variable transformation and back transformation
In-Reply-To: <2E9C414912813E4EB981326983E0A1040639FA1B@inboexch.inbo.be>
References: <49B8BE87.3080809@uni-bayreuth.de> <49C24069.200@msu.edu>
	<2E9C414912813E4EB981326983E0A1040639FA1B@inboexch.inbo.be>
Message-ID: <49C76B5C.3060708@uni-bayreuth.de>

ONKELINX, Thierry schrieb:
> Dear all,
>   
Dear Dr. Onkelinx, dear Dr. Steibel, dear list,

thanks a lot for your help!
> Christina's question will be a bit more clear with some extra background
> information on the topic. When interpolating concentrations in the soil
> with kriging, a log-transformation is often used. The predicted value
> for a location in the log-scale is a distribution with mean mu(s) and
> standard deviation sigma(s). Mu(s) is a function which yields the mean
> value depending on location s.
> A simple form of backtransformation would be exp(mu(s)). That will no
> longer be the mean of the distribution (in the original scale) at
> location s, but rather its median. The mean of the distribution in the
> original scale is exp(mu(s) + 0.5 * sigma(s) ^ 2). That is the reason
> why Christina includes the variance in the backtransformation.
>
> This is the backtransformation one would use with a linear model. So I
> suppose that Christina's question is how the deal with the variance from
> the random effect in such a backtransformation.
>   
Indeed, I am unsure how to treat the random-effects. On one side, a 
random-effect is a random variable like the within-group error. On the 
other side, a mixed-effects model provides an estimate of random-effects 
for different experimental units. So what should one to do when 
backtransforming: taking 0.5*random-variance plus 0.5*within-group error 
or the estimate of the random-effect for the respective experimental 
unit and 0.5*within-group variance?
The latter approach follows the logic of fitted values in nlme: we have 
estimates on population level and to get estimates on the level of 
experimental units we add the estimates of random-effects (realisations 
of random variables). So, for backtransformation, I used the fitted 
values on the level of experimental units and added 0.5*within 
group-variance. So, somehow, I treated the fixed and the random-effects 
equally and it confuses me!.
> Personally I would settle with confidence intervals. The nice thing
> about them is that they are based on the order of values. Since a
> monotone transformations (like exp() and log()) don't change the order
> of values, the backtransformation is straightforward: exp([LCL, UCL]).
> That requires two maps to depict the information.
> If you have some important treshold (e.g. some legal maximum
> concentration) you could create a map with the probability of exeeding
> that treshold.
>
> HTH,
>
> Thierry
>   
Thanks again!

Christina
>
> ------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium 
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be 
> www.inbo.be 
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to
> say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
>
> The plural of anecdote is not data.
> ~ Roger Brinner
>
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of
> data.
> ~ John Tukey
>
> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
> en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
> door een geldig ondertekend document. The views expressed in  this message 
> and any annex are purely those of the writer and may not be regarded as stating 
> an official position of INBO, as long as the message is not confirmed by a duly 
> signed document.
>
>



From Thierry.ONKELINX at inbo.be  Mon Mar 23 14:54:59 2009
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 23 Mar 2009 14:54:59 +0100
Subject: [R-sig-ME] Variable transformation and back transformation
In-Reply-To: <49C76B5C.3060708@uni-bayreuth.de>
References: <49B8BE87.3080809@uni-bayreuth.de> <49C24069.200@msu.edu>
	<2E9C414912813E4EB981326983E0A1040639FA1B@inboexch.inbo.be>
	<49C76B5C.3060708@uni-bayreuth.de>
Message-ID: <2E9C414912813E4EB981326983E0A1040642F049@inboexch.inbo.be>

Dear Christina,

Beform backtransforming the data you should thing about the level you
want to use. Do you need specific data (for the plots and soil depth in
your dataset)? Or rather data at population level: for an average plot
and an average soil depth? In the first case you can use the formula
that you mentioned in your first mail. There is no need to add the
variance of the random effects as you allready take them into account.
In the case you want predictions at the population level, you should
omit the effects from the random effects but add their variances
instead. 

HTH,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium 
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be 
www.inbo.be 

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of
data.
~ John Tukey

-----Oorspronkelijk bericht-----
Van: Christina Bogner [mailto:christina.bogner at uni-bayreuth.de] 
Verzonden: maandag 23 maart 2009 11:59
Aan: ONKELINX, Thierry
CC: Juan Pedro Steibel; r-sig-mixed-models at r-project.org
Onderwerp: Re: [R-sig-ME] Variable transformation and back
transformation

ONKELINX, Thierry schrieb:
> Dear all,
>   
Dear Dr. Onkelinx, dear Dr. Steibel, dear list,

thanks a lot for your help!
> Christina's question will be a bit more clear with some extra
background
> information on the topic. When interpolating concentrations in the
soil
> with kriging, a log-transformation is often used. The predicted value
> for a location in the log-scale is a distribution with mean mu(s) and
> standard deviation sigma(s). Mu(s) is a function which yields the mean
> value depending on location s.
> A simple form of backtransformation would be exp(mu(s)). That will no
> longer be the mean of the distribution (in the original scale) at
> location s, but rather its median. The mean of the distribution in the
> original scale is exp(mu(s) + 0.5 * sigma(s) ^ 2). That is the reason
> why Christina includes the variance in the backtransformation.
>
> This is the backtransformation one would use with a linear model. So I
> suppose that Christina's question is how the deal with the variance
from
> the random effect in such a backtransformation.
>   
Indeed, I am unsure how to treat the random-effects. On one side, a 
random-effect is a random variable like the within-group error. On the 
other side, a mixed-effects model provides an estimate of random-effects

for different experimental units. So what should one to do when 
backtransforming: taking 0.5*random-variance plus 0.5*within-group error

or the estimate of the random-effect for the respective experimental 
unit and 0.5*within-group variance?
The latter approach follows the logic of fitted values in nlme: we have 
estimates on population level and to get estimates on the level of 
experimental units we add the estimates of random-effects (realisations 
of random variables). So, for backtransformation, I used the fitted 
values on the level of experimental units and added 0.5*within 
group-variance. So, somehow, I treated the fixed and the random-effects 
equally and it confuses me!.
> Personally I would settle with confidence intervals. The nice thing
> about them is that they are based on the order of values. Since a
> monotone transformations (like exp() and log()) don't change the order
> of values, the backtransformation is straightforward: exp([LCL, UCL]).
> That requires two maps to depict the information.
> If you have some important treshold (e.g. some legal maximum
> concentration) you could create a map with the probability of exeeding
> that treshold.
>
> HTH,
>
> Thierry
>   
Thanks again!

Christina
>
>
------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium 
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be 
> www.inbo.be 
>
> To call in the statistician after the experiment is done may be no
more
> than asking him to perform a post-mortem examination: he may be able
to
> say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
>
> The plural of anecdote is not data.
> ~ Roger Brinner
>
> The combination of some data and an aching desire for an answer does
not
> ensure that a reasonable answer can be extracted from a given body of
> data.
> ~ John Tukey
>
> Dit bericht en eventuele bijlagen geven enkel de visie van de
schrijver weer 
> en binden het INBO onder geen enkel beding, zolang dit bericht niet
bevestigd is
> door een geldig ondertekend document. The views expressed in  this
message 
> and any annex are purely those of the writer and may not be regarded
as stating 
> an official position of INBO, as long as the message is not confirmed
by a duly 
> signed document.
>
>   


Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer 
en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is
door een geldig ondertekend document. The views expressed in  this message 
and any annex are purely those of the writer and may not be regarded as stating 
an official position of INBO, as long as the message is not confirmed by a duly 
signed document.



From brant.inman at me.com  Mon Mar 23 16:05:18 2009
From: brant.inman at me.com (Brant Inman)
Date: Mon, 23 Mar 2009 10:05:18 -0500
Subject: [R-sig-ME] Multilevel logistic regression
Message-ID: <108579524797779583901888226610559395865-Webmail@me.com>


Thanks for everyone's explanations.  I truly appreciate it.  Since I have never really delved into S4 objects like lmer, I now have a reason now to crack open Chambers' green book and figure out how to track lmer down to its depths to modify the error message line.

This thread is why I think R is so great: as long as your question is not too terribly dumb you get advice from the experts!

Cheers,

Brant

-----------------------------------------

On Sunday, March 22, 2009, at 12:41PM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
>On Sun, Mar 22, 2009 at 9:22 AM, Brant Inman <brant.inman at me.com> wrote:
>> Ben,
>
>> Thanks for confirming my findings. Two questions for you:
>
>> 1) Is it normal that lmer does NOT find the following 3 formulae to be equivalent?
>
>> f1 <- lmer( cbind(success, fail) ~ covariates + (1 | group), data=wide ...
>> f2 <- lmer( (success/(success+fail)) ~ covariates + (1 | group), weights=(success+fail), data=wide ...
>> f3 <- lmer( success ~ covariates + (1 | group), data=long ...
>
>The code in lmer to count the "number of observations" is too
>simplistic.  It uses the length of the vector of responses which is
>appropriate except for the case of models like f1.  What should be
>done in that case is the evaluate the sum of the number of cases
>represented by each row of the data.
>
>Tracking such things down is not terribly easy.  The "initialize"
>expression from a glm family is "not unlike" a gross hack. It is an
>expression (not a function) that is evaluated for the sole purpose of
>scattering a bunch of objects around the function evaluation
>environment, which often ends up looking like a teenager's bedroom
>afterwards.
>
>Yes, special purpose code could be written to detect a situation like
>model f1 and account for it appropriately.  Doing so will mean keeping
>track of two, possibly different, definitions of the "number of
>observations".  If someone is willing to produce a patch for the code
>I will consider it.
>
>I don't think it will be as easy to detect that model f2 represents
>more observations than are recorded in the data.  All you know is that
>you are given observation weights, which could be the result of
>effects other than multiple "observations" per observation.
>
>> 2) How did you hack lmer to avoid this problem and did it compute the result faster with the wide dataset than the long one?
>
>Well, you start by grepping the source code to determine where the
>error message is produced then you disable that part of the code.
>
>The error message is there for a reason - primarily related to linear
>mixed models.  Sometimes people get carried away and define random
>effects with respect to a factor that has a distinct level for each
>observation.  You can estimate the parameters in such a model but not
>uniquely.  The variance due to such a "one level per observation"
>factor is completely confounded with the variance of the
>"per-observation" noise term.  People have reported results from such
>models fit by lme so I wanted to protect against that by flagging such
>models in lme4.  One could adopt a "caveat emptor" approach and say
>that the user is responsible for ensuring that they model that they
>fit is mathematically reasonable so it they get nonsense results for a
>nonsense model it's their fault.  Unfortunately, that ends up
>reflecting badly on lme4 or the R project.  People fail to distinguish
>between a package in R and R in general so this type of problem is
>reported as a flaw in R, often by people who want to convince
>companies to pony up large amounts of money for commercial software
>licenses.
>
>Hence, I have opted for the more conservative approach of throwing an
>error in such cases.   As Ben has pointed out, lme4 is open source so
>if you want to modify it to fit a particular type of model that
>currently produces an error, you have the opportunity of doing so.
>
>
>> ----------------------------------------------------------------------------------------------------------------------------
>>
>> On Saturday, March 21, 2009, at 11:52PM, "Ben Bolker" <bolker at ufl.edu> wrote:
>>> ? I tried this out with a hacked version of lme4 that removes
>>>the test on the number of observations, and get reasonable answers
>>>for f3, to wit:
>>>
>>>Random effects:
>>> Groups Name ? ? ? ?Variance Std.Dev. Corr
>>> SOURCE (Intercept) 0.78845 ?0.88795
>>> ? ? ? ?TELDUM ? ? ?0.24216 ?0.49210 ?-0.281
>>> ? ? ? ?MAILDUM ? ? 0.52215 ?0.72260 ?-0.365 ?0.317
>>>Number of obs: 105, groups: SOURCE, 48
>>>
>>>Fixed effects:
>>> ? ? ? ? ? ?Estimate Std. Error z value Pr(>|z|)
>>>(Intercept) ?1.12762 ? ?0.19519 ? 5.777 ?7.6e-09 ***
>>>RESPISRR ? ? 0.20714 ? ?0.21497 ? 0.964 0.335258
>>>TELDUM ? ? ?-0.20251 ? ?0.09141 ?-2.215 0.026737 *
>>>MAILDUM ? ? -0.56094 ? ?0.14708 ?-3.814 0.000137 ***
>>>---
>>>
>>> ?The random effects (esp for telephone/mail) are a little different.
>>>It doesn't look like the software used in the chapter estimates
>>>correlations among random effects?
>>>
>>>## Table 6.4 Models for response rates in different conditions
>>>## Fixed part ? ? ? ? ? conditions fixed ? ? ? conditions random
>>>## ? ? ? ? ? ? ? ? ? ? ?coeff. (s.e.) ? ? ? ? ?coeff. (s.e.)
>>>## Predictor
>>>## intercept ? ? ? ? ? ?0.90 (.14) ? ? ? ? ? ? 1.17 (.21)
>>>## resptype ? ? ? ? ? ? 0.53 (.06) ? ? ? ? ? ? 0.20 (.23)
>>>## telephone ? ? ? ? ? ?-0.16 (.02) ? ? ? ? ? ?-0.20 (.10)
>>>## mail ? ? ? ? ? ? ? ? -0.49 (.03) ? ? ? ? ? ?-0.58 (.16)
>>>## Random part
>>>## intercept1 ? ? ? ? ? 1.00 ? ? ? ? ? ? ? ? ? 1.00
>>>## intercept2 ? ? ? ? ? 0.86 (.18) ? ? ? ? ? ? 0.87 (.20)
>>>## telephone ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0.26 (.08)
>>>## mail ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0.59 (.20)
>>>
>>>Eliminating the correlation among random effects:
>>>
>>>f3B <- lmer(y ?~ RESPISRR + TELDUM + MAILDUM + (1|SOURCE) +
>>>(0+TELDUM|SOURCE) + (0+MAILDUM|SOURCE),
>>> ? ? ? ? ? ? ? ?family=binomial, data=wide)
>>>summary(f3B)
>>>
>>>Random effects:
>>> Groups Name ? ? ? ?Variance Std.Dev.
>>> SOURCE (Intercept) 0.72203 ?0.84972
>>> SOURCE TELDUM ? ? ?0.23582 ?0.48562
>>> SOURCE MAILDUM ? ? 0.43923 ?0.66274
>>>Number of obs: 105, groups: SOURCE, 48
>>>
>>>
>>> ?telephone/mail random effects still estimated higher.
>>>Tried nAGQ=6 (crank up Gauss-Hermite quadrature):
>>>
>>>Random effects:
>>> Groups Name ? ? ? ?Variance Std.Dev.
>>> SOURCE (Intercept) 0.74275 ?0.86183
>>> SOURCE TELDUM ? ? ?0.24565 ?0.49563
>>> SOURCE MAILDUM ? ? 0.28567 ?0.53448
>>>Number of obs: 105, groups: SOURCE, 48
>>>
>>>Brings mail RE down (*below* PQL est.) but telephone RE is still high.
>>>
>>>
>>>
>>>
>>>Brant Inman wrote:
>>>> Thierry et al:
>>>>
>>>> I think I solved my problem and it provides some insight into the way
>>>> lmer handles binomial data, so I will share the findings here. First,
>>>> I have made two datasets available on the web, a long format and a
>>>> wide format version of the "metaresp" data of Joop Hox.. ?They can be
>>>> found here
>>>>
>>>> http://www.duke.edu/~bi6/
>>>>
>>>> Hox has a draft of the chapter of interest that discusses the
>>>> metaresp dataset and the modeling process/problem that I am trying to
>>>> solve. ?Note that the results that I am trying to reproduce are in
>>>> Tables 6.3 and 6.4. ?The chapter can be found at:
>>>>
>>>> http://www.geocities.com/joophox/papers/chap6.pdf
>>>>
>>>> Now here are the models that I fit with lmer. ?Assume that the wide
>>>> version of the data is called "wide" and the long version "long".
>>>> -----------------------------
>>>>
>>>> y <- cbind(wide$SUCCESS, wide$FAIL)
>>>>
>>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE), family=binomial, data=wide)
>>>> summary(f1)
>>>>
>>>> f2 <- lmer(y ~ RESPISRR + TELDUM + MAILDUM + (1 | SOURCE),
>>>> family=binomial, data=wide) summary(f2)
>>>>
>>>> f3 <- lmer(y ~ ?~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM + MAILDUM
>>>> | SOURCE), family=binomial, data=wide) summary(f3)
>>>>
>>>> f4 <- lmer(SUCCESS ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM +
>>>> MAILDUM | SOURCE), family=binomial, data=long) summary(f4)
>>>>
>>>> -------------------------------
>>>>
>>>> Models f1, f2, and f4 work and reproduce the results of Hox. ?Model
>>>> f4 takes a hell of a long time to compute, but it seems to give the
>>>> expected results. ?Model f3, which I assumed (wrongly) would be the
>>>> same as f4, does not work. ?Instead, when it is run, you get the
>>>> error message:
>>>>
>>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>>
>>>> I guess the question that I have now is: did I do something wrong
>>>> with model f3 or is lmer doing something unusual? ?My assumption that
>>>> models f3 and f4 were the same comes from MASS4 p190 where Ripley
>>>> describes the glm function for logistic regression.
>>>>
>>>> I very much appreciate any insight.
>>>>
>>>> Brant
>>>>
>>>> #####################################################################################
>>>>
>>>>
>>>> On Friday, March 20, 2009, at 04:32AM, "ONKELINX, Thierry"
>>>> <Thierry.ONKELINX at inbo.be> wrote:
>>>>> Dear Brant,
>>>>>
>>>>> The model is too complex. You have maximum three observations for
>>>>> each level of the random effect. Allowing for a random intercept
>>>>> and two random slopes does not make much sense then. Does it?
>>>>>
>>>>> HTH,
>>>>>
>>>>> Thierry
>>>>>
>>>>>
>>>>> ------------------------------------------------------------------------
>>>>> ?---- ir. Thierry Onkelinx Instituut voor natuur- en bosonderzoek /
>>>>> Research Institute for Nature and Forest Cel biometrie,
>>>>> methodologie en kwaliteitszorg / Section biometrics, methodology
>>>>> and quality assurance Gaverstraat 4 9500 Geraardsbergen Belgium
>>>>> tel. + 32 54/436 185 Thierry.Onkelinx at inbo.be www.inbo.be
>>>>>
>>>>> To call in the statistician after the experiment is done may be no
>>>>> more than asking him to perform a post-mortem examination: he may
>>>>> be able to say what the experiment died of. ~ Sir Ronald Aylmer
>>>>> Fisher
>>>>>
>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>
>>>>> The combination of some data and an aching desire for an answer
>>>>> does not ensure that a reasonable answer can be extracted from a
>>>>> given body of data. ~ John Tukey
>>>>>
>>>>> -----Oorspronkelijk bericht----- Van:
>>>>> r-sig-mixed-models-bounces at r-project.org
>>>>> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Brant
>>>>> Inman Verzonden: donderdag 19 maart 2009 3:11 Aan:
>>>>> r-sig-mixed-models at r-project.org Onderwerp: [R-sig-ME] Multilevel
>>>>> logistic regression
>>>>>
>>>>>
>>>>> lmer Experts:
>>>>>
>>>>> I am trying to use lmer to duplicate the results found in Joop
>>>>> Hox's book "Multilevel Analysis: technique and applications" 2002.
>>>>> In chapter 6 of his book he shows an example of multilevel logistic
>>>>> ?regression for a meta-analysis of survey response rates. ?The data
>>>>> are available in the file "metaresp.xls" at his website:
>>>>>
>>>>> <http://www.geocities.com/joophox/mlbook/excelxls.zip>
>>>>>
>>>>> The dataset includes the following variables of interest:
>>>>>
>>>>> Individual level (Level 1) variables: TELDUM ? ? ? ? = telephone
>>>>> questioning MAILDUM ?= mail questioning RESPONSE = the outcome of
>>>>> interest, the study response rate DENOM ? ?= the number of people
>>>>> questioned
>>>>>
>>>>> Study/group level (Level 2) variables: SOURCE ? ? ? ?= the study
>>>>> identifier YEAR ? ? ?= year of study SALIENCY = how salient the
>>>>> questionnaire was (0 to 2) RESPISRR = the way the response rate was
>>>>> calculated
>>>>>
>>>>>
>>>>> The null model (Table 6.2) proposed by Joop is easy to fit:
>>>>>
>>>>> SUCCESS <- as.integer(RESPISRR*DENOM) y ? ? <- cbind(SUCCESS,
>>>>> DENOM-SUCCESS)
>>>>>
>>>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE),
>>>>> family=binomial(link=logit))
>>>>>
>>>>>
>>>>> Joop then adds a couple Level 1 variables (Table 6.3):
>>>>>
>>>>> f2 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (1 | SOURCE),
>>>>> family=binomial(link=logit))
>>>>>
>>>>>
>>>>> He then says that these two Level 1 variables should be allowed to
>>>>> ?vary across studies (varying slopes). ?When I try to fit what I
>>>>> believe to be the correct model, I get an error
>>>>>
>>>>>
>>>>> f3 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (TELNUM | SOURCE) +
>>>>> (MAILDUM | SOURCE) + (1 | SOURCE), family=binomial(link=logit))
>>>>>
>>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>>>
>>>>>
>>>>> Can anyone tell me what I am doing wrong here? ?Thanks so much in
>>>>> ?advance.
>>>>>
>>>>> Brant Inman Duke University Medical Center
>>>>>
>>>>> [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>> Dit bericht en eventuele bijlagen geven enkel de visie van de
>>>>> schrijver weer en binden het INBO onder geen enkel beding, zolang
>>>>> dit bericht niet bevestigd is door een geldig ondertekend document.
>>>>> The views expressed in ?this message and any annex are purely those
>>>>> of the writer and may not be regarded as stating an official
>>>>> position of INBO, as long as the message is not confirmed by a duly
>>>>> ?signed document.
>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>>--
>>>Ben Bolker
>>>Associate professor, Biology Dep't, Univ. of Florida
>>>bolker at ufl.edu / www.zoology.ufl.edu/bolker
>>>GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>>>
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>



From bates at stat.wisc.edu  Mon Mar 23 17:45:38 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 23 Mar 2009 11:45:38 -0500
Subject: [R-sig-ME] Multilevel logistic regression
In-Reply-To: <108579524797779583901888226610559395865-Webmail@me.com>
References: <108579524797779583901888226610559395865-Webmail@me.com>
Message-ID: <40e66e0b0903230945p51c52b94l91e6a8f6345deefa@mail.gmail.com>

On Mon, Mar 23, 2009 at 10:05 AM, Brant Inman <brant.inman at me.com> wrote:

> Thanks for everyone's explanations. ?I truly appreciate it. ?Since I have never really delved into S4 objects like lmer, I now have a reason now to crack open Chambers' green book and figure out how to track lmer down to its depths to modify the error message line.

Actually you don't need to learn about S4 classes and methods to be
able to bypass this error message.  If you search in the source code
for a string that begins with "q = " you will find that the error
message is generated in the C function called update_u in the
lme4/src/lmer.c file.

> This thread is why I think R is so great: as long as your question is not too terribly dumb you get advice from the experts!
>
> Cheers,
>
> Brant
>
> -----------------------------------------
>
> On Sunday, March 22, 2009, at 12:41PM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
>>On Sun, Mar 22, 2009 at 9:22 AM, Brant Inman <brant.inman at me.com> wrote:
>>> Ben,
>>
>>> Thanks for confirming my findings. Two questions for you:
>>
>>> 1) Is it normal that lmer does NOT find the following 3 formulae to be equivalent?
>>
>>> f1 <- lmer( cbind(success, fail) ~ covariates + (1 | group), data=wide ...
>>> f2 <- lmer( (success/(success+fail)) ~ covariates + (1 | group), weights=(success+fail), data=wide ...
>>> f3 <- lmer( success ~ covariates + (1 | group), data=long ...
>>
>>The code in lmer to count the "number of observations" is too
>>simplistic. ?It uses the length of the vector of responses which is
>>appropriate except for the case of models like f1. ?What should be
>>done in that case is the evaluate the sum of the number of cases
>>represented by each row of the data.
>>
>>Tracking such things down is not terribly easy. ?The "initialize"
>>expression from a glm family is "not unlike" a gross hack. It is an
>>expression (not a function) that is evaluated for the sole purpose of
>>scattering a bunch of objects around the function evaluation
>>environment, which often ends up looking like a teenager's bedroom
>>afterwards.
>>
>>Yes, special purpose code could be written to detect a situation like
>>model f1 and account for it appropriately. ?Doing so will mean keeping
>>track of two, possibly different, definitions of the "number of
>>observations". ?If someone is willing to produce a patch for the code
>>I will consider it.
>>
>>I don't think it will be as easy to detect that model f2 represents
>>more observations than are recorded in the data. ?All you know is that
>>you are given observation weights, which could be the result of
>>effects other than multiple "observations" per observation.
>>
>>> 2) How did you hack lmer to avoid this problem and did it compute the result faster with the wide dataset than the long one?
>>
>>Well, you start by grepping the source code to determine where the
>>error message is produced then you disable that part of the code.
>>
>>The error message is there for a reason - primarily related to linear
>>mixed models. ?Sometimes people get carried away and define random
>>effects with respect to a factor that has a distinct level for each
>>observation. ?You can estimate the parameters in such a model but not
>>uniquely. ?The variance due to such a "one level per observation"
>>factor is completely confounded with the variance of the
>>"per-observation" noise term. ?People have reported results from such
>>models fit by lme so I wanted to protect against that by flagging such
>>models in lme4. ?One could adopt a "caveat emptor" approach and say
>>that the user is responsible for ensuring that they model that they
>>fit is mathematically reasonable so it they get nonsense results for a
>>nonsense model it's their fault. ?Unfortunately, that ends up
>>reflecting badly on lme4 or the R project. ?People fail to distinguish
>>between a package in R and R in general so this type of problem is
>>reported as a flaw in R, often by people who want to convince
>>companies to pony up large amounts of money for commercial software
>>licenses.
>>
>>Hence, I have opted for the more conservative approach of throwing an
>>error in such cases. ? As Ben has pointed out, lme4 is open source so
>>if you want to modify it to fit a particular type of model that
>>currently produces an error, you have the opportunity of doing so.
>>
>>
>>> ----------------------------------------------------------------------------------------------------------------------------
>>>
>>> On Saturday, March 21, 2009, at 11:52PM, "Ben Bolker" <bolker at ufl.edu> wrote:
>>>> ? I tried this out with a hacked version of lme4 that removes
>>>>the test on the number of observations, and get reasonable answers
>>>>for f3, to wit:
>>>>
>>>>Random effects:
>>>> Groups Name ? ? ? ?Variance Std.Dev. Corr
>>>> SOURCE (Intercept) 0.78845 ?0.88795
>>>> ? ? ? ?TELDUM ? ? ?0.24216 ?0.49210 ?-0.281
>>>> ? ? ? ?MAILDUM ? ? 0.52215 ?0.72260 ?-0.365 ?0.317
>>>>Number of obs: 105, groups: SOURCE, 48
>>>>
>>>>Fixed effects:
>>>> ? ? ? ? ? ?Estimate Std. Error z value Pr(>|z|)
>>>>(Intercept) ?1.12762 ? ?0.19519 ? 5.777 ?7.6e-09 ***
>>>>RESPISRR ? ? 0.20714 ? ?0.21497 ? 0.964 0.335258
>>>>TELDUM ? ? ?-0.20251 ? ?0.09141 ?-2.215 0.026737 *
>>>>MAILDUM ? ? -0.56094 ? ?0.14708 ?-3.814 0.000137 ***
>>>>---
>>>>
>>>> ?The random effects (esp for telephone/mail) are a little different.
>>>>It doesn't look like the software used in the chapter estimates
>>>>correlations among random effects?
>>>>
>>>>## Table 6.4 Models for response rates in different conditions
>>>>## Fixed part ? ? ? ? ? conditions fixed ? ? ? conditions random
>>>>## ? ? ? ? ? ? ? ? ? ? ?coeff. (s.e.) ? ? ? ? ?coeff. (s.e.)
>>>>## Predictor
>>>>## intercept ? ? ? ? ? ?0.90 (.14) ? ? ? ? ? ? 1.17 (.21)
>>>>## resptype ? ? ? ? ? ? 0.53 (.06) ? ? ? ? ? ? 0.20 (.23)
>>>>## telephone ? ? ? ? ? ?-0.16 (.02) ? ? ? ? ? ?-0.20 (.10)
>>>>## mail ? ? ? ? ? ? ? ? -0.49 (.03) ? ? ? ? ? ?-0.58 (.16)
>>>>## Random part
>>>>## intercept1 ? ? ? ? ? 1.00 ? ? ? ? ? ? ? ? ? 1.00
>>>>## intercept2 ? ? ? ? ? 0.86 (.18) ? ? ? ? ? ? 0.87 (.20)
>>>>## telephone ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0.26 (.08)
>>>>## mail ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0.59 (.20)
>>>>
>>>>Eliminating the correlation among random effects:
>>>>
>>>>f3B <- lmer(y ?~ RESPISRR + TELDUM + MAILDUM + (1|SOURCE) +
>>>>(0+TELDUM|SOURCE) + (0+MAILDUM|SOURCE),
>>>> ? ? ? ? ? ? ? ?family=binomial, data=wide)
>>>>summary(f3B)
>>>>
>>>>Random effects:
>>>> Groups Name ? ? ? ?Variance Std.Dev.
>>>> SOURCE (Intercept) 0.72203 ?0.84972
>>>> SOURCE TELDUM ? ? ?0.23582 ?0.48562
>>>> SOURCE MAILDUM ? ? 0.43923 ?0.66274
>>>>Number of obs: 105, groups: SOURCE, 48
>>>>
>>>>
>>>> ?telephone/mail random effects still estimated higher.
>>>>Tried nAGQ=6 (crank up Gauss-Hermite quadrature):
>>>>
>>>>Random effects:
>>>> Groups Name ? ? ? ?Variance Std.Dev.
>>>> SOURCE (Intercept) 0.74275 ?0.86183
>>>> SOURCE TELDUM ? ? ?0.24565 ?0.49563
>>>> SOURCE MAILDUM ? ? 0.28567 ?0.53448
>>>>Number of obs: 105, groups: SOURCE, 48
>>>>
>>>>Brings mail RE down (*below* PQL est.) but telephone RE is still high.
>>>>
>>>>
>>>>
>>>>
>>>>Brant Inman wrote:
>>>>> Thierry et al:
>>>>>
>>>>> I think I solved my problem and it provides some insight into the way
>>>>> lmer handles binomial data, so I will share the findings here. First,
>>>>> I have made two datasets available on the web, a long format and a
>>>>> wide format version of the "metaresp" data of Joop Hox.. ?They can be
>>>>> found here
>>>>>
>>>>> http://www.duke.edu/~bi6/
>>>>>
>>>>> Hox has a draft of the chapter of interest that discusses the
>>>>> metaresp dataset and the modeling process/problem that I am trying to
>>>>> solve. ?Note that the results that I am trying to reproduce are in
>>>>> Tables 6.3 and 6.4. ?The chapter can be found at:
>>>>>
>>>>> http://www.geocities.com/joophox/papers/chap6.pdf
>>>>>
>>>>> Now here are the models that I fit with lmer. ?Assume that the wide
>>>>> version of the data is called "wide" and the long version "long".
>>>>> -----------------------------
>>>>>
>>>>> y <- cbind(wide$SUCCESS, wide$FAIL)
>>>>>
>>>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE), family=binomial, data=wide)
>>>>> summary(f1)
>>>>>
>>>>> f2 <- lmer(y ~ RESPISRR + TELDUM + MAILDUM + (1 | SOURCE),
>>>>> family=binomial, data=wide) summary(f2)
>>>>>
>>>>> f3 <- lmer(y ~ ?~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM + MAILDUM
>>>>> | SOURCE), family=binomial, data=wide) summary(f3)
>>>>>
>>>>> f4 <- lmer(SUCCESS ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM +
>>>>> MAILDUM | SOURCE), family=binomial, data=long) summary(f4)
>>>>>
>>>>> -------------------------------
>>>>>
>>>>> Models f1, f2, and f4 work and reproduce the results of Hox. ?Model
>>>>> f4 takes a hell of a long time to compute, but it seems to give the
>>>>> expected results. ?Model f3, which I assumed (wrongly) would be the
>>>>> same as f4, does not work. ?Instead, when it is run, you get the
>>>>> error message:
>>>>>
>>>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>>>
>>>>> I guess the question that I have now is: did I do something wrong
>>>>> with model f3 or is lmer doing something unusual? ?My assumption that
>>>>> models f3 and f4 were the same comes from MASS4 p190 where Ripley
>>>>> describes the glm function for logistic regression.
>>>>>
>>>>> I very much appreciate any insight.
>>>>>
>>>>> Brant
>>>>>
>>>>> #####################################################################################
>>>>>
>>>>>
>>>>> On Friday, March 20, 2009, at 04:32AM, "ONKELINX, Thierry"
>>>>> <Thierry.ONKELINX at inbo.be> wrote:
>>>>>> Dear Brant,
>>>>>>
>>>>>> The model is too complex. You have maximum three observations for
>>>>>> each level of the random effect. Allowing for a random intercept
>>>>>> and two random slopes does not make much sense then. Does it?
>>>>>>
>>>>>> HTH,
>>>>>>
>>>>>> Thierry
>>>>>>
>>>>>>
>>>>>> ------------------------------------------------------------------------
>>>>>> ?---- ir. Thierry Onkelinx Instituut voor natuur- en bosonderzoek /
>>>>>> Research Institute for Nature and Forest Cel biometrie,
>>>>>> methodologie en kwaliteitszorg / Section biometrics, methodology
>>>>>> and quality assurance Gaverstraat 4 9500 Geraardsbergen Belgium
>>>>>> tel. + 32 54/436 185 Thierry.Onkelinx at inbo.be www.inbo.be
>>>>>>
>>>>>> To call in the statistician after the experiment is done may be no
>>>>>> more than asking him to perform a post-mortem examination: he may
>>>>>> be able to say what the experiment died of. ~ Sir Ronald Aylmer
>>>>>> Fisher
>>>>>>
>>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>>
>>>>>> The combination of some data and an aching desire for an answer
>>>>>> does not ensure that a reasonable answer can be extracted from a
>>>>>> given body of data. ~ John Tukey
>>>>>>
>>>>>> -----Oorspronkelijk bericht----- Van:
>>>>>> r-sig-mixed-models-bounces at r-project.org
>>>>>> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Brant
>>>>>> Inman Verzonden: donderdag 19 maart 2009 3:11 Aan:
>>>>>> r-sig-mixed-models at r-project.org Onderwerp: [R-sig-ME] Multilevel
>>>>>> logistic regression
>>>>>>
>>>>>>
>>>>>> lmer Experts:
>>>>>>
>>>>>> I am trying to use lmer to duplicate the results found in Joop
>>>>>> Hox's book "Multilevel Analysis: technique and applications" 2002.
>>>>>> In chapter 6 of his book he shows an example of multilevel logistic
>>>>>> ?regression for a meta-analysis of survey response rates. ?The data
>>>>>> are available in the file "metaresp.xls" at his website:
>>>>>>
>>>>>> <http://www.geocities.com/joophox/mlbook/excelxls.zip>
>>>>>>
>>>>>> The dataset includes the following variables of interest:
>>>>>>
>>>>>> Individual level (Level 1) variables: TELDUM ? ? ? ? = telephone
>>>>>> questioning MAILDUM ?= mail questioning RESPONSE = the outcome of
>>>>>> interest, the study response rate DENOM ? ?= the number of people
>>>>>> questioned
>>>>>>
>>>>>> Study/group level (Level 2) variables: SOURCE ? ? ? ?= the study
>>>>>> identifier YEAR ? ? ?= year of study SALIENCY = how salient the
>>>>>> questionnaire was (0 to 2) RESPISRR = the way the response rate was
>>>>>> calculated
>>>>>>
>>>>>>
>>>>>> The null model (Table 6.2) proposed by Joop is easy to fit:
>>>>>>
>>>>>> SUCCESS <- as.integer(RESPISRR*DENOM) y ? ? <- cbind(SUCCESS,
>>>>>> DENOM-SUCCESS)
>>>>>>
>>>>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE),
>>>>>> family=binomial(link=logit))
>>>>>>
>>>>>>
>>>>>> Joop then adds a couple Level 1 variables (Table 6.3):
>>>>>>
>>>>>> f2 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (1 | SOURCE),
>>>>>> family=binomial(link=logit))
>>>>>>
>>>>>>
>>>>>> He then says that these two Level 1 variables should be allowed to
>>>>>> ?vary across studies (varying slopes). ?When I try to fit what I
>>>>>> believe to be the correct model, I get an error
>>>>>>
>>>>>>
>>>>>> f3 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (TELNUM | SOURCE) +
>>>>>> (MAILDUM | SOURCE) + (1 | SOURCE), family=binomial(link=logit))
>>>>>>
>>>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>>>>
>>>>>>
>>>>>> Can anyone tell me what I am doing wrong here? ?Thanks so much in
>>>>>> ?advance.
>>>>>>
>>>>>> Brant Inman Duke University Medical Center
>>>>>>
>>>>>> [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>> Dit bericht en eventuele bijlagen geven enkel de visie van de
>>>>>> schrijver weer en binden het INBO onder geen enkel beding, zolang
>>>>>> dit bericht niet bevestigd is door een geldig ondertekend document.
>>>>>> The views expressed in ?this message and any annex are purely those
>>>>>> of the writer and may not be regarded as stating an official
>>>>>> position of INBO, as long as the message is not confirmed by a duly
>>>>>> ?signed document.
>>>>>>
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>>
>>>>--
>>>>Ben Bolker
>>>>Associate professor, Biology Dep't, Univ. of Florida
>>>>bolker at ufl.edu / www.zoology.ufl.edu/bolker
>>>>GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>>>>
>>>>
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>
>



From brant.inman at me.com  Mon Mar 23 18:40:32 2009
From: brant.inman at me.com (Brant Inman)
Date: Mon, 23 Mar 2009 12:40:32 -0500
Subject: [R-sig-ME] Multilevel logistic regression
Message-ID: <12840009485277185702852235836928546564-Webmail@me.com>


If I am working with a windows version of R 2.8.1, where would the src folder be?

In C:\Program Files\R\R-2.8.1\library\lme4, which is my lme4 installation site, I have no folder called src.


Brant

------------------------------------------------

On Monday, March 23, 2009, at 11:45AM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
>On Mon, Mar 23, 2009 at 10:05 AM, Brant Inman <brant.inman at me.com> wrote:
>
>> Thanks for everyone's explanations. ?I truly appreciate it. ?Since I have never really delved into S4 objects like lmer, I now have a reason now to crack open Chambers' green book and figure out how to track lmer down to its depths to modify the error message line.
>
>Actually you don't need to learn about S4 classes and methods to be
>able to bypass this error message.  If you search in the source code
>for a string that begins with "q = " you will find that the error
>message is generated in the C function called update_u in the
>lme4/src/lmer.c file.
>
>> This thread is why I think R is so great: as long as your question is not too terribly dumb you get advice from the experts!
>>
>> Cheers,
>>
>> Brant
>>
>> -----------------------------------------
>>
>> On Sunday, March 22, 2009, at 12:41PM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
>>>On Sun, Mar 22, 2009 at 9:22 AM, Brant Inman <brant.inman at me.com> wrote:
>>>> Ben,
>>>
>>>> Thanks for confirming my findings. Two questions for you:
>>>
>>>> 1) Is it normal that lmer does NOT find the following 3 formulae to be equivalent?
>>>
>>>> f1 <- lmer( cbind(success, fail) ~ covariates + (1 | group), data=wide ...
>>>> f2 <- lmer( (success/(success+fail)) ~ covariates + (1 | group), weights=(success+fail), data=wide ...
>>>> f3 <- lmer( success ~ covariates + (1 | group), data=long ...
>>>
>>>The code in lmer to count the "number of observations" is too
>>>simplistic. ?It uses the length of the vector of responses which is
>>>appropriate except for the case of models like f1. ?What should be
>>>done in that case is the evaluate the sum of the number of cases
>>>represented by each row of the data.
>>>
>>>Tracking such things down is not terribly easy. ?The "initialize"
>>>expression from a glm family is "not unlike" a gross hack. It is an
>>>expression (not a function) that is evaluated for the sole purpose of
>>>scattering a bunch of objects around the function evaluation
>>>environment, which often ends up looking like a teenager's bedroom
>>>afterwards.
>>>
>>>Yes, special purpose code could be written to detect a situation like
>>>model f1 and account for it appropriately. ?Doing so will mean keeping
>>>track of two, possibly different, definitions of the "number of
>>>observations". ?If someone is willing to produce a patch for the code
>>>I will consider it.
>>>
>>>I don't think it will be as easy to detect that model f2 represents
>>>more observations than are recorded in the data. ?All you know is that
>>>you are given observation weights, which could be the result of
>>>effects other than multiple "observations" per observation.
>>>
>>>> 2) How did you hack lmer to avoid this problem and did it compute the result faster with the wide dataset than the long one?
>>>
>>>Well, you start by grepping the source code to determine where the
>>>error message is produced then you disable that part of the code.
>>>
>>>The error message is there for a reason - primarily related to linear
>>>mixed models. ?Sometimes people get carried away and define random
>>>effects with respect to a factor that has a distinct level for each
>>>observation. ?You can estimate the parameters in such a model but not
>>>uniquely. ?The variance due to such a "one level per observation"
>>>factor is completely confounded with the variance of the
>>>"per-observation" noise term. ?People have reported results from such
>>>models fit by lme so I wanted to protect against that by flagging such
>>>models in lme4. ?One could adopt a "caveat emptor" approach and say
>>>that the user is responsible for ensuring that they model that they
>>>fit is mathematically reasonable so it they get nonsense results for a
>>>nonsense model it's their fault. ?Unfortunately, that ends up
>>>reflecting badly on lme4 or the R project. ?People fail to distinguish
>>>between a package in R and R in general so this type of problem is
>>>reported as a flaw in R, often by people who want to convince
>>>companies to pony up large amounts of money for commercial software
>>>licenses.
>>>
>>>Hence, I have opted for the more conservative approach of throwing an
>>>error in such cases. ? As Ben has pointed out, lme4 is open source so
>>>if you want to modify it to fit a particular type of model that
>>>currently produces an error, you have the opportunity of doing so.
>>>
>>>
>>>> ----------------------------------------------------------------------------------------------------------------------------
>>>>
>>>> On Saturday, March 21, 2009, at 11:52PM, "Ben Bolker" <bolker at ufl.edu> wrote:
>>>>> ? I tried this out with a hacked version of lme4 that removes
>>>>>the test on the number of observations, and get reasonable answers
>>>>>for f3, to wit:
>>>>>
>>>>>Random effects:
>>>>> Groups Name ? ? ? ?Variance Std.Dev. Corr
>>>>> SOURCE (Intercept) 0.78845 ?0.88795
>>>>> ? ? ? ?TELDUM ? ? ?0.24216 ?0.49210 ?-0.281
>>>>> ? ? ? ?MAILDUM ? ? 0.52215 ?0.72260 ?-0.365 ?0.317
>>>>>Number of obs: 105, groups: SOURCE, 48
>>>>>
>>>>>Fixed effects:
>>>>> ? ? ? ? ? ?Estimate Std. Error z value Pr(>|z|)
>>>>>(Intercept) ?1.12762 ? ?0.19519 ? 5.777 ?7.6e-09 ***
>>>>>RESPISRR ? ? 0.20714 ? ?0.21497 ? 0.964 0.335258
>>>>>TELDUM ? ? ?-0.20251 ? ?0.09141 ?-2.215 0.026737 *
>>>>>MAILDUM ? ? -0.56094 ? ?0.14708 ?-3.814 0.000137 ***
>>>>>---
>>>>>
>>>>> ?The random effects (esp for telephone/mail) are a little different.
>>>>>It doesn't look like the software used in the chapter estimates
>>>>>correlations among random effects?
>>>>>
>>>>>## Table 6.4 Models for response rates in different conditions
>>>>>## Fixed part ? ? ? ? ? conditions fixed ? ? ? conditions random
>>>>>## ? ? ? ? ? ? ? ? ? ? ?coeff. (s.e.) ? ? ? ? ?coeff. (s.e.)
>>>>>## Predictor
>>>>>## intercept ? ? ? ? ? ?0.90 (.14) ? ? ? ? ? ? 1.17 (.21)
>>>>>## resptype ? ? ? ? ? ? 0.53 (.06) ? ? ? ? ? ? 0.20 (.23)
>>>>>## telephone ? ? ? ? ? ?-0.16 (.02) ? ? ? ? ? ?-0.20 (.10)
>>>>>## mail ? ? ? ? ? ? ? ? -0.49 (.03) ? ? ? ? ? ?-0.58 (.16)
>>>>>## Random part
>>>>>## intercept1 ? ? ? ? ? 1.00 ? ? ? ? ? ? ? ? ? 1.00
>>>>>## intercept2 ? ? ? ? ? 0.86 (.18) ? ? ? ? ? ? 0.87 (.20)
>>>>>## telephone ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0.26 (.08)
>>>>>## mail ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0.59 (.20)
>>>>>
>>>>>Eliminating the correlation among random effects:
>>>>>
>>>>>f3B <- lmer(y ?~ RESPISRR + TELDUM + MAILDUM + (1|SOURCE) +
>>>>>(0+TELDUM|SOURCE) + (0+MAILDUM|SOURCE),
>>>>> ? ? ? ? ? ? ? ?family=binomial, data=wide)
>>>>>summary(f3B)
>>>>>
>>>>>Random effects:
>>>>> Groups Name ? ? ? ?Variance Std.Dev.
>>>>> SOURCE (Intercept) 0.72203 ?0.84972
>>>>> SOURCE TELDUM ? ? ?0.23582 ?0.48562
>>>>> SOURCE MAILDUM ? ? 0.43923 ?0.66274
>>>>>Number of obs: 105, groups: SOURCE, 48
>>>>>
>>>>>
>>>>> ?telephone/mail random effects still estimated higher.
>>>>>Tried nAGQ=6 (crank up Gauss-Hermite quadrature):
>>>>>
>>>>>Random effects:
>>>>> Groups Name ? ? ? ?Variance Std.Dev.
>>>>> SOURCE (Intercept) 0.74275 ?0.86183
>>>>> SOURCE TELDUM ? ? ?0.24565 ?0.49563
>>>>> SOURCE MAILDUM ? ? 0.28567 ?0.53448
>>>>>Number of obs: 105, groups: SOURCE, 48
>>>>>
>>>>>Brings mail RE down (*below* PQL est.) but telephone RE is still high.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>Brant Inman wrote:
>>>>>> Thierry et al:
>>>>>>
>>>>>> I think I solved my problem and it provides some insight into the way
>>>>>> lmer handles binomial data, so I will share the findings here. First,
>>>>>> I have made two datasets available on the web, a long format and a
>>>>>> wide format version of the "metaresp" data of Joop Hox.. ?They can be
>>>>>> found here
>>>>>>
>>>>>> http://www.duke.edu/~bi6/
>>>>>>
>>>>>> Hox has a draft of the chapter of interest that discusses the
>>>>>> metaresp dataset and the modeling process/problem that I am trying to
>>>>>> solve. ?Note that the results that I am trying to reproduce are in
>>>>>> Tables 6.3 and 6.4. ?The chapter can be found at:
>>>>>>
>>>>>> http://www.geocities.com/joophox/papers/chap6.pdf
>>>>>>
>>>>>> Now here are the models that I fit with lmer. ?Assume that the wide
>>>>>> version of the data is called "wide" and the long version "long".
>>>>>> -----------------------------
>>>>>>
>>>>>> y <- cbind(wide$SUCCESS, wide$FAIL)
>>>>>>
>>>>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE), family=binomial, data=wide)
>>>>>> summary(f1)
>>>>>>
>>>>>> f2 <- lmer(y ~ RESPISRR + TELDUM + MAILDUM + (1 | SOURCE),
>>>>>> family=binomial, data=wide) summary(f2)
>>>>>>
>>>>>> f3 <- lmer(y ~ ?~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM + MAILDUM
>>>>>> | SOURCE), family=binomial, data=wide) summary(f3)
>>>>>>
>>>>>> f4 <- lmer(SUCCESS ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM +
>>>>>> MAILDUM | SOURCE), family=binomial, data=long) summary(f4)
>>>>>>
>>>>>> -------------------------------
>>>>>>
>>>>>> Models f1, f2, and f4 work and reproduce the results of Hox. ?Model
>>>>>> f4 takes a hell of a long time to compute, but it seems to give the
>>>>>> expected results. ?Model f3, which I assumed (wrongly) would be the
>>>>>> same as f4, does not work. ?Instead, when it is run, you get the
>>>>>> error message:
>>>>>>
>>>>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>>>>
>>>>>> I guess the question that I have now is: did I do something wrong
>>>>>> with model f3 or is lmer doing something unusual? ?My assumption that
>>>>>> models f3 and f4 were the same comes from MASS4 p190 where Ripley
>>>>>> describes the glm function for logistic regression.
>>>>>>
>>>>>> I very much appreciate any insight.
>>>>>>
>>>>>> Brant
>>>>>>
>>>>>> #####################################################################################
>>>>>>
>>>>>>
>>>>>> On Friday, March 20, 2009, at 04:32AM, "ONKELINX, Thierry"
>>>>>> <Thierry.ONKELINX at inbo.be> wrote:
>>>>>>> Dear Brant,
>>>>>>>
>>>>>>> The model is too complex. You have maximum three observations for
>>>>>>> each level of the random effect. Allowing for a random intercept
>>>>>>> and two random slopes does not make much sense then. Does it?
>>>>>>>
>>>>>>> HTH,
>>>>>>>
>>>>>>> Thierry
>>>>>>>
>>>>>>>
>>>>>>> ------------------------------------------------------------------------
>>>>>>> ?---- ir. Thierry Onkelinx Instituut voor natuur- en bosonderzoek /
>>>>>>> Research Institute for Nature and Forest Cel biometrie,
>>>>>>> methodologie en kwaliteitszorg / Section biometrics, methodology
>>>>>>> and quality assurance Gaverstraat 4 9500 Geraardsbergen Belgium
>>>>>>> tel. + 32 54/436 185 Thierry.Onkelinx at inbo.be www.inbo.be
>>>>>>>
>>>>>>> To call in the statistician after the experiment is done may be no
>>>>>>> more than asking him to perform a post-mortem examination: he may
>>>>>>> be able to say what the experiment died of. ~ Sir Ronald Aylmer
>>>>>>> Fisher
>>>>>>>
>>>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>>>
>>>>>>> The combination of some data and an aching desire for an answer
>>>>>>> does not ensure that a reasonable answer can be extracted from a
>>>>>>> given body of data. ~ John Tukey
>>>>>>>
>>>>>>> -----Oorspronkelijk bericht----- Van:
>>>>>>> r-sig-mixed-models-bounces at r-project.org
>>>>>>> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Brant
>>>>>>> Inman Verzonden: donderdag 19 maart 2009 3:11 Aan:
>>>>>>> r-sig-mixed-models at r-project.org Onderwerp: [R-sig-ME] Multilevel
>>>>>>> logistic regression
>>>>>>>
>>>>>>>
>>>>>>> lmer Experts:
>>>>>>>
>>>>>>> I am trying to use lmer to duplicate the results found in Joop
>>>>>>> Hox's book "Multilevel Analysis: technique and applications" 2002.
>>>>>>> In chapter 6 of his book he shows an example of multilevel logistic
>>>>>>> ?regression for a meta-analysis of survey response rates. ?The data
>>>>>>> are available in the file "metaresp.xls" at his website:
>>>>>>>
>>>>>>> <http://www.geocities.com/joophox/mlbook/excelxls.zip>
>>>>>>>
>>>>>>> The dataset includes the following variables of interest:
>>>>>>>
>>>>>>> Individual level (Level 1) variables: TELDUM ? ? ? ? = telephone
>>>>>>> questioning MAILDUM ?= mail questioning RESPONSE = the outcome of
>>>>>>> interest, the study response rate DENOM ? ?= the number of people
>>>>>>> questioned
>>>>>>>
>>>>>>> Study/group level (Level 2) variables: SOURCE ? ? ? ?= the study
>>>>>>> identifier YEAR ? ? ?= year of study SALIENCY = how salient the
>>>>>>> questionnaire was (0 to 2) RESPISRR = the way the response rate was
>>>>>>> calculated
>>>>>>>
>>>>>>>
>>>>>>> The null model (Table 6.2) proposed by Joop is easy to fit:
>>>>>>>
>>>>>>> SUCCESS <- as.integer(RESPISRR*DENOM) y ? ? <- cbind(SUCCESS,
>>>>>>> DENOM-SUCCESS)
>>>>>>>
>>>>>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE),
>>>>>>> family=binomial(link=logit))
>>>>>>>
>>>>>>>
>>>>>>> Joop then adds a couple Level 1 variables (Table 6.3):
>>>>>>>
>>>>>>> f2 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (1 | SOURCE),
>>>>>>> family=binomial(link=logit))
>>>>>>>
>>>>>>>
>>>>>>> He then says that these two Level 1 variables should be allowed to
>>>>>>> ?vary across studies (varying slopes). ?When I try to fit what I
>>>>>>> believe to be the correct model, I get an error
>>>>>>>
>>>>>>>
>>>>>>> f3 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (TELNUM | SOURCE) +
>>>>>>> (MAILDUM | SOURCE) + (1 | SOURCE), family=binomial(link=logit))
>>>>>>>
>>>>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>>>>>
>>>>>>>
>>>>>>> Can anyone tell me what I am doing wrong here? ?Thanks so much in
>>>>>>> ?advance.
>>>>>>>
>>>>>>> Brant Inman Duke University Medical Center
>>>>>>>
>>>>>>> [[alternative HTML version deleted]]
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>
>>>>>>> Dit bericht en eventuele bijlagen geven enkel de visie van de
>>>>>>> schrijver weer en binden het INBO onder geen enkel beding, zolang
>>>>>>> dit bericht niet bevestigd is door een geldig ondertekend document.
>>>>>>> The views expressed in ?this message and any annex are purely those
>>>>>>> of the writer and may not be regarded as stating an official
>>>>>>> position of INBO, as long as the message is not confirmed by a duly
>>>>>>> ?signed document.
>>>>>>>
>>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>>
>>>>>--
>>>>>Ben Bolker
>>>>>Associate professor, Biology Dep't, Univ. of Florida
>>>>>bolker at ufl.edu / www.zoology.ufl.edu/bolker
>>>>>GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>
>>>
>>
>
>



From brant.inman at me.com  Mon Mar 23 18:56:22 2009
From: brant.inman at me.com (Brant Inman)
Date: Mon, 23 Mar 2009 12:56:22 -0500
Subject: [R-sig-ME] Multilevel logistic regression
In-Reply-To: <12840009485277185702852235836928546564-Webmail@me.com>
References: <12840009485277185702852235836928546564-Webmail@me.com>
Message-ID: <115529357402377517400595820227124702082-Webmail@me.com>


Sorry for the dumb last question. I still find some of the more technical computer/programming aspects of R a bit confusing :)   

I just realized after reading Ewe Ligges article on source code in Rnews 6/4 that my copy of R is the binary copy and therefore does not allow me to access the stuff you said. I did download the package in source format and did find the appropriate error message that I could perhaps comment out.  I guess that I will need to reinstall R from source to get myself on the right track.

Brant

-------------------------------------------------

On Monday, March 23, 2009, at 12:40PM, "Brant Inman" <brant.inman at me.com> wrote:
>
>If I am working with a windows version of R 2.8.1, where would the src folder be?
>
>In C:\Program Files\R\R-2.8.1\library\lme4, which is my lme4 installation site, I have no folder called src.
>
>
>Brant
>
>------------------------------------------------
> 
>On Monday, March 23, 2009, at 11:45AM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
>>On Mon, Mar 23, 2009 at 10:05 AM, Brant Inman <brant.inman at me.com> wrote:
>>
>>> Thanks for everyone's explanations. ?I truly appreciate it. ?Since I have never really delved into S4 objects like lmer, I now have a reason now to crack open Chambers' green book and figure out how to track lmer down to its depths to modify the error message line.
>>
>>Actually you don't need to learn about S4 classes and methods to be
>>able to bypass this error message.  If you search in the source code
>>for a string that begins with "q = " you will find that the error
>>message is generated in the C function called update_u in the
>>lme4/src/lmer.c file.
>>
>>> This thread is why I think R is so great: as long as your question is not too terribly dumb you get advice from the experts!
>>>
>>> Cheers,
>>>
>>> Brant
>>>
>>> -----------------------------------------
>>>
>>> On Sunday, March 22, 2009, at 12:41PM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
>>>>On Sun, Mar 22, 2009 at 9:22 AM, Brant Inman <brant.inman at me.com> wrote:
>>>>> Ben,
>>>>
>>>>> Thanks for confirming my findings. Two questions for you:
>>>>
>>>>> 1) Is it normal that lmer does NOT find the following 3 formulae to be equivalent?
>>>>
>>>>> f1 <- lmer( cbind(success, fail) ~ covariates + (1 | group), data=wide ...
>>>>> f2 <- lmer( (success/(success+fail)) ~ covariates + (1 | group), weights=(success+fail), data=wide ...
>>>>> f3 <- lmer( success ~ covariates + (1 | group), data=long ...
>>>>
>>>>The code in lmer to count the "number of observations" is too
>>>>simplistic. ?It uses the length of the vector of responses which is
>>>>appropriate except for the case of models like f1. ?What should be
>>>>done in that case is the evaluate the sum of the number of cases
>>>>represented by each row of the data.
>>>>
>>>>Tracking such things down is not terribly easy. ?The "initialize"
>>>>expression from a glm family is "not unlike" a gross hack. It is an
>>>>expression (not a function) that is evaluated for the sole purpose of
>>>>scattering a bunch of objects around the function evaluation
>>>>environment, which often ends up looking like a teenager's bedroom
>>>>afterwards.
>>>>
>>>>Yes, special purpose code could be written to detect a situation like
>>>>model f1 and account for it appropriately. ?Doing so will mean keeping
>>>>track of two, possibly different, definitions of the "number of
>>>>observations". ?If someone is willing to produce a patch for the code
>>>>I will consider it.
>>>>
>>>>I don't think it will be as easy to detect that model f2 represents
>>>>more observations than are recorded in the data. ?All you know is that
>>>>you are given observation weights, which could be the result of
>>>>effects other than multiple "observations" per observation.
>>>>
>>>>> 2) How did you hack lmer to avoid this problem and did it compute the result faster with the wide dataset than the long one?
>>>>
>>>>Well, you start by grepping the source code to determine where the
>>>>error message is produced then you disable that part of the code.
>>>>
>>>>The error message is there for a reason - primarily related to linear
>>>>mixed models. ?Sometimes people get carried away and define random
>>>>effects with respect to a factor that has a distinct level for each
>>>>observation. ?You can estimate the parameters in such a model but not
>>>>uniquely. ?The variance due to such a "one level per observation"
>>>>factor is completely confounded with the variance of the
>>>>"per-observation" noise term. ?People have reported results from such
>>>>models fit by lme so I wanted to protect against that by flagging such
>>>>models in lme4. ?One could adopt a "caveat emptor" approach and say
>>>>that the user is responsible for ensuring that they model that they
>>>>fit is mathematically reasonable so it they get nonsense results for a
>>>>nonsense model it's their fault. ?Unfortunately, that ends up
>>>>reflecting badly on lme4 or the R project. ?People fail to distinguish
>>>>between a package in R and R in general so this type of problem is
>>>>reported as a flaw in R, often by people who want to convince
>>>>companies to pony up large amounts of money for commercial software
>>>>licenses.
>>>>
>>>>Hence, I have opted for the more conservative approach of throwing an
>>>>error in such cases. ? As Ben has pointed out, lme4 is open source so
>>>>if you want to modify it to fit a particular type of model that
>>>>currently produces an error, you have the opportunity of doing so.
>>>>
>>>>
>>>>> ----------------------------------------------------------------------------------------------------------------------------
>>>>>
>>>>> On Saturday, March 21, 2009, at 11:52PM, "Ben Bolker" <bolker at ufl.edu> wrote:
>>>>>> ? I tried this out with a hacked version of lme4 that removes
>>>>>>the test on the number of observations, and get reasonable answers
>>>>>>for f3, to wit:
>>>>>>
>>>>>>Random effects:
>>>>>> Groups Name ? ? ? ?Variance Std.Dev. Corr
>>>>>> SOURCE (Intercept) 0.78845 ?0.88795
>>>>>> ? ? ? ?TELDUM ? ? ?0.24216 ?0.49210 ?-0.281
>>>>>> ? ? ? ?MAILDUM ? ? 0.52215 ?0.72260 ?-0.365 ?0.317
>>>>>>Number of obs: 105, groups: SOURCE, 48
>>>>>>
>>>>>>Fixed effects:
>>>>>> ? ? ? ? ? ?Estimate Std. Error z value Pr(>|z|)
>>>>>>(Intercept) ?1.12762 ? ?0.19519 ? 5.777 ?7.6e-09 ***
>>>>>>RESPISRR ? ? 0.20714 ? ?0.21497 ? 0.964 0.335258
>>>>>>TELDUM ? ? ?-0.20251 ? ?0.09141 ?-2.215 0.026737 *
>>>>>>MAILDUM ? ? -0.56094 ? ?0.14708 ?-3.814 0.000137 ***
>>>>>>---
>>>>>>
>>>>>> ?The random effects (esp for telephone/mail) are a little different.
>>>>>>It doesn't look like the software used in the chapter estimates
>>>>>>correlations among random effects?
>>>>>>
>>>>>>## Table 6.4 Models for response rates in different conditions
>>>>>>## Fixed part ? ? ? ? ? conditions fixed ? ? ? conditions random
>>>>>>## ? ? ? ? ? ? ? ? ? ? ?coeff. (s.e.) ? ? ? ? ?coeff. (s.e.)
>>>>>>## Predictor
>>>>>>## intercept ? ? ? ? ? ?0.90 (.14) ? ? ? ? ? ? 1.17 (.21)
>>>>>>## resptype ? ? ? ? ? ? 0.53 (.06) ? ? ? ? ? ? 0.20 (.23)
>>>>>>## telephone ? ? ? ? ? ?-0.16 (.02) ? ? ? ? ? ?-0.20 (.10)
>>>>>>## mail ? ? ? ? ? ? ? ? -0.49 (.03) ? ? ? ? ? ?-0.58 (.16)
>>>>>>## Random part
>>>>>>## intercept1 ? ? ? ? ? 1.00 ? ? ? ? ? ? ? ? ? 1.00
>>>>>>## intercept2 ? ? ? ? ? 0.86 (.18) ? ? ? ? ? ? 0.87 (.20)
>>>>>>## telephone ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0.26 (.08)
>>>>>>## mail ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0.59 (.20)
>>>>>>
>>>>>>Eliminating the correlation among random effects:
>>>>>>
>>>>>>f3B <- lmer(y ?~ RESPISRR + TELDUM + MAILDUM + (1|SOURCE) +
>>>>>>(0+TELDUM|SOURCE) + (0+MAILDUM|SOURCE),
>>>>>> ? ? ? ? ? ? ? ?family=binomial, data=wide)
>>>>>>summary(f3B)
>>>>>>
>>>>>>Random effects:
>>>>>> Groups Name ? ? ? ?Variance Std.Dev.
>>>>>> SOURCE (Intercept) 0.72203 ?0.84972
>>>>>> SOURCE TELDUM ? ? ?0.23582 ?0.48562
>>>>>> SOURCE MAILDUM ? ? 0.43923 ?0.66274
>>>>>>Number of obs: 105, groups: SOURCE, 48
>>>>>>
>>>>>>
>>>>>> ?telephone/mail random effects still estimated higher.
>>>>>>Tried nAGQ=6 (crank up Gauss-Hermite quadrature):
>>>>>>
>>>>>>Random effects:
>>>>>> Groups Name ? ? ? ?Variance Std.Dev.
>>>>>> SOURCE (Intercept) 0.74275 ?0.86183
>>>>>> SOURCE TELDUM ? ? ?0.24565 ?0.49563
>>>>>> SOURCE MAILDUM ? ? 0.28567 ?0.53448
>>>>>>Number of obs: 105, groups: SOURCE, 48
>>>>>>
>>>>>>Brings mail RE down (*below* PQL est.) but telephone RE is still high.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>Brant Inman wrote:
>>>>>>> Thierry et al:
>>>>>>>
>>>>>>> I think I solved my problem and it provides some insight into the way
>>>>>>> lmer handles binomial data, so I will share the findings here. First,
>>>>>>> I have made two datasets available on the web, a long format and a
>>>>>>> wide format version of the "metaresp" data of Joop Hox.. ?They can be
>>>>>>> found here
>>>>>>>
>>>>>>> http://www.duke.edu/~bi6/
>>>>>>>
>>>>>>> Hox has a draft of the chapter of interest that discusses the
>>>>>>> metaresp dataset and the modeling process/problem that I am trying to
>>>>>>> solve. ?Note that the results that I am trying to reproduce are in
>>>>>>> Tables 6.3 and 6.4. ?The chapter can be found at:
>>>>>>>
>>>>>>> http://www.geocities.com/joophox/papers/chap6.pdf
>>>>>>>
>>>>>>> Now here are the models that I fit with lmer. ?Assume that the wide
>>>>>>> version of the data is called "wide" and the long version "long".
>>>>>>> -----------------------------
>>>>>>>
>>>>>>> y <- cbind(wide$SUCCESS, wide$FAIL)
>>>>>>>
>>>>>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE), family=binomial, data=wide)
>>>>>>> summary(f1)
>>>>>>>
>>>>>>> f2 <- lmer(y ~ RESPISRR + TELDUM + MAILDUM + (1 | SOURCE),
>>>>>>> family=binomial, data=wide) summary(f2)
>>>>>>>
>>>>>>> f3 <- lmer(y ~ ?~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM + MAILDUM
>>>>>>> | SOURCE), family=binomial, data=wide) summary(f3)
>>>>>>>
>>>>>>> f4 <- lmer(SUCCESS ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM +
>>>>>>> MAILDUM | SOURCE), family=binomial, data=long) summary(f4)
>>>>>>>
>>>>>>> -------------------------------
>>>>>>>
>>>>>>> Models f1, f2, and f4 work and reproduce the results of Hox. ?Model
>>>>>>> f4 takes a hell of a long time to compute, but it seems to give the
>>>>>>> expected results. ?Model f3, which I assumed (wrongly) would be the
>>>>>>> same as f4, does not work. ?Instead, when it is run, you get the
>>>>>>> error message:
>>>>>>>
>>>>>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>>>>>
>>>>>>> I guess the question that I have now is: did I do something wrong
>>>>>>> with model f3 or is lmer doing something unusual? ?My assumption that
>>>>>>> models f3 and f4 were the same comes from MASS4 p190 where Ripley
>>>>>>> describes the glm function for logistic regression.
>>>>>>>
>>>>>>> I very much appreciate any insight.
>>>>>>>
>>>>>>> Brant
>>>>>>>
>>>>>>> #####################################################################################
>>>>>>>
>>>>>>>
>>>>>>> On Friday, March 20, 2009, at 04:32AM, "ONKELINX, Thierry"
>>>>>>> <Thierry.ONKELINX at inbo.be> wrote:
>>>>>>>> Dear Brant,
>>>>>>>>
>>>>>>>> The model is too complex. You have maximum three observations for
>>>>>>>> each level of the random effect. Allowing for a random intercept
>>>>>>>> and two random slopes does not make much sense then. Does it?
>>>>>>>>
>>>>>>>> HTH,
>>>>>>>>
>>>>>>>> Thierry
>>>>>>>>
>>>>>>>>
>>>>>>>> ------------------------------------------------------------------------
>>>>>>>> ?---- ir. Thierry Onkelinx Instituut voor natuur- en bosonderzoek /
>>>>>>>> Research Institute for Nature and Forest Cel biometrie,
>>>>>>>> methodologie en kwaliteitszorg / Section biometrics, methodology
>>>>>>>> and quality assurance Gaverstraat 4 9500 Geraardsbergen Belgium
>>>>>>>> tel. + 32 54/436 185 Thierry.Onkelinx at inbo.be www.inbo.be
>>>>>>>>
>>>>>>>> To call in the statistician after the experiment is done may be no
>>>>>>>> more than asking him to perform a post-mortem examination: he may
>>>>>>>> be able to say what the experiment died of. ~ Sir Ronald Aylmer
>>>>>>>> Fisher
>>>>>>>>
>>>>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>>>>
>>>>>>>> The combination of some data and an aching desire for an answer
>>>>>>>> does not ensure that a reasonable answer can be extracted from a
>>>>>>>> given body of data. ~ John Tukey
>>>>>>>>
>>>>>>>> -----Oorspronkelijk bericht----- Van:
>>>>>>>> r-sig-mixed-models-bounces at r-project.org
>>>>>>>> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Brant
>>>>>>>> Inman Verzonden: donderdag 19 maart 2009 3:11 Aan:
>>>>>>>> r-sig-mixed-models at r-project.org Onderwerp: [R-sig-ME] Multilevel
>>>>>>>> logistic regression
>>>>>>>>
>>>>>>>>
>>>>>>>> lmer Experts:
>>>>>>>>
>>>>>>>> I am trying to use lmer to duplicate the results found in Joop
>>>>>>>> Hox's book "Multilevel Analysis: technique and applications" 2002.
>>>>>>>> In chapter 6 of his book he shows an example of multilevel logistic
>>>>>>>> ?regression for a meta-analysis of survey response rates. ?The data
>>>>>>>> are available in the file "metaresp.xls" at his website:
>>>>>>>>
>>>>>>>> <http://www.geocities.com/joophox/mlbook/excelxls.zip>
>>>>>>>>
>>>>>>>> The dataset includes the following variables of interest:
>>>>>>>>
>>>>>>>> Individual level (Level 1) variables: TELDUM ? ? ? ? = telephone
>>>>>>>> questioning MAILDUM ?= mail questioning RESPONSE = the outcome of
>>>>>>>> interest, the study response rate DENOM ? ?= the number of people
>>>>>>>> questioned
>>>>>>>>
>>>>>>>> Study/group level (Level 2) variables: SOURCE ? ? ? ?= the study
>>>>>>>> identifier YEAR ? ? ?= year of study SALIENCY = how salient the
>>>>>>>> questionnaire was (0 to 2) RESPISRR = the way the response rate was
>>>>>>>> calculated
>>>>>>>>
>>>>>>>>
>>>>>>>> The null model (Table 6.2) proposed by Joop is easy to fit:
>>>>>>>>
>>>>>>>> SUCCESS <- as.integer(RESPISRR*DENOM) y ? ? <- cbind(SUCCESS,
>>>>>>>> DENOM-SUCCESS)
>>>>>>>>
>>>>>>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE),
>>>>>>>> family=binomial(link=logit))
>>>>>>>>
>>>>>>>>
>>>>>>>> Joop then adds a couple Level 1 variables (Table 6.3):
>>>>>>>>
>>>>>>>> f2 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (1 | SOURCE),
>>>>>>>> family=binomial(link=logit))
>>>>>>>>
>>>>>>>>
>>>>>>>> He then says that these two Level 1 variables should be allowed to
>>>>>>>> ?vary across studies (varying slopes). ?When I try to fit what I
>>>>>>>> believe to be the correct model, I get an error
>>>>>>>>
>>>>>>>>
>>>>>>>> f3 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (TELNUM | SOURCE) +
>>>>>>>> (MAILDUM | SOURCE) + (1 | SOURCE), family=binomial(link=logit))
>>>>>>>>
>>>>>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>>>>>>
>>>>>>>>
>>>>>>>> Can anyone tell me what I am doing wrong here? ?Thanks so much in
>>>>>>>> ?advance.
>>>>>>>>
>>>>>>>> Brant Inman Duke University Medical Center
>>>>>>>>
>>>>>>>> [[alternative HTML version deleted]]
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>>
>>>>>>>> Dit bericht en eventuele bijlagen geven enkel de visie van de
>>>>>>>> schrijver weer en binden het INBO onder geen enkel beding, zolang
>>>>>>>> dit bericht niet bevestigd is door een geldig ondertekend document.
>>>>>>>> The views expressed in ?this message and any annex are purely those
>>>>>>>> of the writer and may not be regarded as stating an official
>>>>>>>> position of INBO, as long as the message is not confirmed by a duly
>>>>>>>> ?signed document.
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>>
>>>>>>--
>>>>>>Ben Bolker
>>>>>>Associate professor, Biology Dep't, Univ. of Florida
>>>>>>bolker at ufl.edu / www.zoology.ufl.edu/bolker
>>>>>>GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>>>>>>
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>
>>>>
>>>
>>
>>
>
>



From bates at stat.wisc.edu  Mon Mar 23 19:05:10 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 23 Mar 2009 13:05:10 -0500
Subject: [R-sig-ME] Multilevel logistic regression
In-Reply-To: <115529357402377517400595820227124702082-Webmail@me.com>
References: <12840009485277185702852235836928546564-Webmail@me.com>
	<115529357402377517400595820227124702082-Webmail@me.com>
Message-ID: <40e66e0b0903231105m35223732j4eb5793e83b971a7@mail.gmail.com>

On Mon, Mar 23, 2009 at 12:56 PM, Brant Inman <brant.inman at me.com> wrote:

> Sorry for the dumb last question. I still find some of the more technical computer/programming aspects of R a bit confusing :)

> I just realized after reading Ewe Ligges article on source code in Rnews 6/4 that my copy of R is the binary copy and therefore does not allow me to access the stuff you said. I did download the package in source format and did find the appropriate error message that I could perhaps comment out. ?I guess that I will need to reinstall R from source to get myself on the right track.

Not quite.  As you have seen, installing a source package under
Windows is, shall we say, involved, but there are instructions in the
"R Installation and Administration Manual" (section 6.3.1) on how to
set up an installation environment without needing to go to the
lengths of compiling R from sources.  Personally, I avoid the problem
by only using Linux but others may not find this a suitable solution.

Of course, after next week we may not need to be concerned about
computers running Windows. :-)  Apparently the Conficker worm, which
is widespread on networked Windows computers, has an alarm set for
April 1, 2009 and it is not yet known how severe the consequences will
be when this alarm goes off.  Windows users are advised to back up
their data before the end of the month.
http://bits.blogs.nytimes.com/2009/03/19/the-conficker-worm-april-fools-joke-or-unthinkable-disaster/



>
> Brant
>
> -------------------------------------------------
>
> On Monday, March 23, 2009, at 12:40PM, "Brant Inman" <brant.inman at me.com> wrote:
>>
>>If I am working with a windows version of R 2.8.1, where would the src folder be?
>>
>>In C:\Program Files\R\R-2.8.1\library\lme4, which is my lme4 installation site, I have no folder called src.
>>
>>
>>Brant
>>
>>------------------------------------------------
>>
>>On Monday, March 23, 2009, at 11:45AM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
>>>On Mon, Mar 23, 2009 at 10:05 AM, Brant Inman <brant.inman at me.com> wrote:
>>>
>>>> Thanks for everyone's explanations. ?I truly appreciate it. ?Since I have never really delved into S4 objects like lmer, I now have a reason now to crack open Chambers' green book and figure out how to track lmer down to its depths to modify the error message line.
>>>
>>>Actually you don't need to learn about S4 classes and methods to be
>>>able to bypass this error message. ?If you search in the source code
>>>for a string that begins with "q = " you will find that the error
>>>message is generated in the C function called update_u in the
>>>lme4/src/lmer.c file.
>>>
>>>> This thread is why I think R is so great: as long as your question is not too terribly dumb you get advice from the experts!
>>>>
>>>> Cheers,
>>>>
>>>> Brant
>>>>
>>>> -----------------------------------------
>>>>
>>>> On Sunday, March 22, 2009, at 12:41PM, "Douglas Bates" <bates at stat.wisc.edu> wrote:
>>>>>On Sun, Mar 22, 2009 at 9:22 AM, Brant Inman <brant.inman at me.com> wrote:
>>>>>> Ben,
>>>>>
>>>>>> Thanks for confirming my findings. Two questions for you:
>>>>>
>>>>>> 1) Is it normal that lmer does NOT find the following 3 formulae to be equivalent?
>>>>>
>>>>>> f1 <- lmer( cbind(success, fail) ~ covariates + (1 | group), data=wide ...
>>>>>> f2 <- lmer( (success/(success+fail)) ~ covariates + (1 | group), weights=(success+fail), data=wide ...
>>>>>> f3 <- lmer( success ~ covariates + (1 | group), data=long ...
>>>>>
>>>>>The code in lmer to count the "number of observations" is too
>>>>>simplistic. ?It uses the length of the vector of responses which is
>>>>>appropriate except for the case of models like f1. ?What should be
>>>>>done in that case is the evaluate the sum of the number of cases
>>>>>represented by each row of the data.
>>>>>
>>>>>Tracking such things down is not terribly easy. ?The "initialize"
>>>>>expression from a glm family is "not unlike" a gross hack. It is an
>>>>>expression (not a function) that is evaluated for the sole purpose of
>>>>>scattering a bunch of objects around the function evaluation
>>>>>environment, which often ends up looking like a teenager's bedroom
>>>>>afterwards.
>>>>>
>>>>>Yes, special purpose code could be written to detect a situation like
>>>>>model f1 and account for it appropriately. ?Doing so will mean keeping
>>>>>track of two, possibly different, definitions of the "number of
>>>>>observations". ?If someone is willing to produce a patch for the code
>>>>>I will consider it.
>>>>>
>>>>>I don't think it will be as easy to detect that model f2 represents
>>>>>more observations than are recorded in the data. ?All you know is that
>>>>>you are given observation weights, which could be the result of
>>>>>effects other than multiple "observations" per observation.
>>>>>
>>>>>> 2) How did you hack lmer to avoid this problem and did it compute the result faster with the wide dataset than the long one?
>>>>>
>>>>>Well, you start by grepping the source code to determine where the
>>>>>error message is produced then you disable that part of the code.
>>>>>
>>>>>The error message is there for a reason - primarily related to linear
>>>>>mixed models. ?Sometimes people get carried away and define random
>>>>>effects with respect to a factor that has a distinct level for each
>>>>>observation. ?You can estimate the parameters in such a model but not
>>>>>uniquely. ?The variance due to such a "one level per observation"
>>>>>factor is completely confounded with the variance of the
>>>>>"per-observation" noise term. ?People have reported results from such
>>>>>models fit by lme so I wanted to protect against that by flagging such
>>>>>models in lme4. ?One could adopt a "caveat emptor" approach and say
>>>>>that the user is responsible for ensuring that they model that they
>>>>>fit is mathematically reasonable so it they get nonsense results for a
>>>>>nonsense model it's their fault. ?Unfortunately, that ends up
>>>>>reflecting badly on lme4 or the R project. ?People fail to distinguish
>>>>>between a package in R and R in general so this type of problem is
>>>>>reported as a flaw in R, often by people who want to convince
>>>>>companies to pony up large amounts of money for commercial software
>>>>>licenses.
>>>>>
>>>>>Hence, I have opted for the more conservative approach of throwing an
>>>>>error in such cases. ? As Ben has pointed out, lme4 is open source so
>>>>>if you want to modify it to fit a particular type of model that
>>>>>currently produces an error, you have the opportunity of doing so.
>>>>>
>>>>>
>>>>>> ----------------------------------------------------------------------------------------------------------------------------
>>>>>>
>>>>>> On Saturday, March 21, 2009, at 11:52PM, "Ben Bolker" <bolker at ufl.edu> wrote:
>>>>>>> ? I tried this out with a hacked version of lme4 that removes
>>>>>>>the test on the number of observations, and get reasonable answers
>>>>>>>for f3, to wit:
>>>>>>>
>>>>>>>Random effects:
>>>>>>> Groups Name ? ? ? ?Variance Std.Dev. Corr
>>>>>>> SOURCE (Intercept) 0.78845 ?0.88795
>>>>>>> ? ? ? ?TELDUM ? ? ?0.24216 ?0.49210 ?-0.281
>>>>>>> ? ? ? ?MAILDUM ? ? 0.52215 ?0.72260 ?-0.365 ?0.317
>>>>>>>Number of obs: 105, groups: SOURCE, 48
>>>>>>>
>>>>>>>Fixed effects:
>>>>>>> ? ? ? ? ? ?Estimate Std. Error z value Pr(>|z|)
>>>>>>>(Intercept) ?1.12762 ? ?0.19519 ? 5.777 ?7.6e-09 ***
>>>>>>>RESPISRR ? ? 0.20714 ? ?0.21497 ? 0.964 0.335258
>>>>>>>TELDUM ? ? ?-0.20251 ? ?0.09141 ?-2.215 0.026737 *
>>>>>>>MAILDUM ? ? -0.56094 ? ?0.14708 ?-3.814 0.000137 ***
>>>>>>>---
>>>>>>>
>>>>>>> ?The random effects (esp for telephone/mail) are a little different.
>>>>>>>It doesn't look like the software used in the chapter estimates
>>>>>>>correlations among random effects?
>>>>>>>
>>>>>>>## Table 6.4 Models for response rates in different conditions
>>>>>>>## Fixed part ? ? ? ? ? conditions fixed ? ? ? conditions random
>>>>>>>## ? ? ? ? ? ? ? ? ? ? ?coeff. (s.e.) ? ? ? ? ?coeff. (s.e.)
>>>>>>>## Predictor
>>>>>>>## intercept ? ? ? ? ? ?0.90 (.14) ? ? ? ? ? ? 1.17 (.21)
>>>>>>>## resptype ? ? ? ? ? ? 0.53 (.06) ? ? ? ? ? ? 0.20 (.23)
>>>>>>>## telephone ? ? ? ? ? ?-0.16 (.02) ? ? ? ? ? ?-0.20 (.10)
>>>>>>>## mail ? ? ? ? ? ? ? ? -0.49 (.03) ? ? ? ? ? ?-0.58 (.16)
>>>>>>>## Random part
>>>>>>>## intercept1 ? ? ? ? ? 1.00 ? ? ? ? ? ? ? ? ? 1.00
>>>>>>>## intercept2 ? ? ? ? ? 0.86 (.18) ? ? ? ? ? ? 0.87 (.20)
>>>>>>>## telephone ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0.26 (.08)
>>>>>>>## mail ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0.59 (.20)
>>>>>>>
>>>>>>>Eliminating the correlation among random effects:
>>>>>>>
>>>>>>>f3B <- lmer(y ?~ RESPISRR + TELDUM + MAILDUM + (1|SOURCE) +
>>>>>>>(0+TELDUM|SOURCE) + (0+MAILDUM|SOURCE),
>>>>>>> ? ? ? ? ? ? ? ?family=binomial, data=wide)
>>>>>>>summary(f3B)
>>>>>>>
>>>>>>>Random effects:
>>>>>>> Groups Name ? ? ? ?Variance Std.Dev.
>>>>>>> SOURCE (Intercept) 0.72203 ?0.84972
>>>>>>> SOURCE TELDUM ? ? ?0.23582 ?0.48562
>>>>>>> SOURCE MAILDUM ? ? 0.43923 ?0.66274
>>>>>>>Number of obs: 105, groups: SOURCE, 48
>>>>>>>
>>>>>>>
>>>>>>> ?telephone/mail random effects still estimated higher.
>>>>>>>Tried nAGQ=6 (crank up Gauss-Hermite quadrature):
>>>>>>>
>>>>>>>Random effects:
>>>>>>> Groups Name ? ? ? ?Variance Std.Dev.
>>>>>>> SOURCE (Intercept) 0.74275 ?0.86183
>>>>>>> SOURCE TELDUM ? ? ?0.24565 ?0.49563
>>>>>>> SOURCE MAILDUM ? ? 0.28567 ?0.53448
>>>>>>>Number of obs: 105, groups: SOURCE, 48
>>>>>>>
>>>>>>>Brings mail RE down (*below* PQL est.) but telephone RE is still high.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>Brant Inman wrote:
>>>>>>>> Thierry et al:
>>>>>>>>
>>>>>>>> I think I solved my problem and it provides some insight into the way
>>>>>>>> lmer handles binomial data, so I will share the findings here. First,
>>>>>>>> I have made two datasets available on the web, a long format and a
>>>>>>>> wide format version of the "metaresp" data of Joop Hox.. ?They can be
>>>>>>>> found here
>>>>>>>>
>>>>>>>> http://www.duke.edu/~bi6/
>>>>>>>>
>>>>>>>> Hox has a draft of the chapter of interest that discusses the
>>>>>>>> metaresp dataset and the modeling process/problem that I am trying to
>>>>>>>> solve. ?Note that the results that I am trying to reproduce are in
>>>>>>>> Tables 6.3 and 6.4. ?The chapter can be found at:
>>>>>>>>
>>>>>>>> http://www.geocities.com/joophox/papers/chap6.pdf
>>>>>>>>
>>>>>>>> Now here are the models that I fit with lmer. ?Assume that the wide
>>>>>>>> version of the data is called "wide" and the long version "long".
>>>>>>>> -----------------------------
>>>>>>>>
>>>>>>>> y <- cbind(wide$SUCCESS, wide$FAIL)
>>>>>>>>
>>>>>>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE), family=binomial, data=wide)
>>>>>>>> summary(f1)
>>>>>>>>
>>>>>>>> f2 <- lmer(y ~ RESPISRR + TELDUM + MAILDUM + (1 | SOURCE),
>>>>>>>> family=binomial, data=wide) summary(f2)
>>>>>>>>
>>>>>>>> f3 <- lmer(y ~ ?~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM + MAILDUM
>>>>>>>> | SOURCE), family=binomial, data=wide) summary(f3)
>>>>>>>>
>>>>>>>> f4 <- lmer(SUCCESS ~ RESPISRR + TELDUM + MAILDUM + (1 + TELDUM +
>>>>>>>> MAILDUM | SOURCE), family=binomial, data=long) summary(f4)
>>>>>>>>
>>>>>>>> -------------------------------
>>>>>>>>
>>>>>>>> Models f1, f2, and f4 work and reproduce the results of Hox. ?Model
>>>>>>>> f4 takes a hell of a long time to compute, but it seems to give the
>>>>>>>> expected results. ?Model f3, which I assumed (wrongly) would be the
>>>>>>>> same as f4, does not work. ?Instead, when it is run, you get the
>>>>>>>> error message:
>>>>>>>>
>>>>>>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>>>>>>
>>>>>>>> I guess the question that I have now is: did I do something wrong
>>>>>>>> with model f3 or is lmer doing something unusual? ?My assumption that
>>>>>>>> models f3 and f4 were the same comes from MASS4 p190 where Ripley
>>>>>>>> describes the glm function for logistic regression.
>>>>>>>>
>>>>>>>> I very much appreciate any insight.
>>>>>>>>
>>>>>>>> Brant
>>>>>>>>
>>>>>>>> #####################################################################################
>>>>>>>>
>>>>>>>>
>>>>>>>> On Friday, March 20, 2009, at 04:32AM, "ONKELINX, Thierry"
>>>>>>>> <Thierry.ONKELINX at inbo.be> wrote:
>>>>>>>>> Dear Brant,
>>>>>>>>>
>>>>>>>>> The model is too complex. You have maximum three observations for
>>>>>>>>> each level of the random effect. Allowing for a random intercept
>>>>>>>>> and two random slopes does not make much sense then. Does it?
>>>>>>>>>
>>>>>>>>> HTH,
>>>>>>>>>
>>>>>>>>> Thierry
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> ------------------------------------------------------------------------
>>>>>>>>> ?---- ir. Thierry Onkelinx Instituut voor natuur- en bosonderzoek /
>>>>>>>>> Research Institute for Nature and Forest Cel biometrie,
>>>>>>>>> methodologie en kwaliteitszorg / Section biometrics, methodology
>>>>>>>>> and quality assurance Gaverstraat 4 9500 Geraardsbergen Belgium
>>>>>>>>> tel. + 32 54/436 185 Thierry.Onkelinx at inbo.be www.inbo.be
>>>>>>>>>
>>>>>>>>> To call in the statistician after the experiment is done may be no
>>>>>>>>> more than asking him to perform a post-mortem examination: he may
>>>>>>>>> be able to say what the experiment died of. ~ Sir Ronald Aylmer
>>>>>>>>> Fisher
>>>>>>>>>
>>>>>>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>>>>>>>
>>>>>>>>> The combination of some data and an aching desire for an answer
>>>>>>>>> does not ensure that a reasonable answer can be extracted from a
>>>>>>>>> given body of data. ~ John Tukey
>>>>>>>>>
>>>>>>>>> -----Oorspronkelijk bericht----- Van:
>>>>>>>>> r-sig-mixed-models-bounces at r-project.org
>>>>>>>>> [mailto:r-sig-mixed-models-bounces at r-project.org] Namens Brant
>>>>>>>>> Inman Verzonden: donderdag 19 maart 2009 3:11 Aan:
>>>>>>>>> r-sig-mixed-models at r-project.org Onderwerp: [R-sig-ME] Multilevel
>>>>>>>>> logistic regression
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> lmer Experts:
>>>>>>>>>
>>>>>>>>> I am trying to use lmer to duplicate the results found in Joop
>>>>>>>>> Hox's book "Multilevel Analysis: technique and applications" 2002.
>>>>>>>>> In chapter 6 of his book he shows an example of multilevel logistic
>>>>>>>>> ?regression for a meta-analysis of survey response rates. ?The data
>>>>>>>>> are available in the file "metaresp.xls" at his website:
>>>>>>>>>
>>>>>>>>> <http://www.geocities.com/joophox/mlbook/excelxls.zip>
>>>>>>>>>
>>>>>>>>> The dataset includes the following variables of interest:
>>>>>>>>>
>>>>>>>>> Individual level (Level 1) variables: TELDUM ? ? ? ? = telephone
>>>>>>>>> questioning MAILDUM ?= mail questioning RESPONSE = the outcome of
>>>>>>>>> interest, the study response rate DENOM ? ?= the number of people
>>>>>>>>> questioned
>>>>>>>>>
>>>>>>>>> Study/group level (Level 2) variables: SOURCE ? ? ? ?= the study
>>>>>>>>> identifier YEAR ? ? ?= year of study SALIENCY = how salient the
>>>>>>>>> questionnaire was (0 to 2) RESPISRR = the way the response rate was
>>>>>>>>> calculated
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> The null model (Table 6.2) proposed by Joop is easy to fit:
>>>>>>>>>
>>>>>>>>> SUCCESS <- as.integer(RESPISRR*DENOM) y ? ? <- cbind(SUCCESS,
>>>>>>>>> DENOM-SUCCESS)
>>>>>>>>>
>>>>>>>>> f1 <- lmer(y ~ RESPISRR + (1 | SOURCE),
>>>>>>>>> family=binomial(link=logit))
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Joop then adds a couple Level 1 variables (Table 6.3):
>>>>>>>>>
>>>>>>>>> f2 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (1 | SOURCE),
>>>>>>>>> family=binomial(link=logit))
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> He then says that these two Level 1 variables should be allowed to
>>>>>>>>> ?vary across studies (varying slopes). ?When I try to fit what I
>>>>>>>>> believe to be the correct model, I get an error
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> f3 <- lmer(y ~ RESPISRR + TELNUM + MAILDUM + (TELNUM | SOURCE) +
>>>>>>>>> (MAILDUM | SOURCE) + (1 | SOURCE), family=binomial(link=logit))
>>>>>>>>>
>>>>>>>>> Error in mer_finalize(ans) : q = 240 > n = 105
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Can anyone tell me what I am doing wrong here? ?Thanks so much in
>>>>>>>>> ?advance.
>>>>>>>>>
>>>>>>>>> Brant Inman Duke University Medical Center
>>>>>>>>>
>>>>>>>>> [[alternative HTML version deleted]]
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>>>
>>>>>>>>> Dit bericht en eventuele bijlagen geven enkel de visie van de
>>>>>>>>> schrijver weer en binden het INBO onder geen enkel beding, zolang
>>>>>>>>> dit bericht niet bevestigd is door een geldig ondertekend document.
>>>>>>>>> The views expressed in ?this message and any annex are purely those
>>>>>>>>> of the writer and may not be regarded as stating an official
>>>>>>>>> position of INBO, as long as the message is not confirmed by a duly
>>>>>>>>> ?signed document.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>
>>>>>>>
>>>>>>>--
>>>>>>>Ben Bolker
>>>>>>>Associate professor, Biology Dep't, Univ. of Florida
>>>>>>>bolker at ufl.edu / www.zoology.ufl.edu/bolker
>>>>>>>GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc
>>>>>>>
>>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>
>>>>>
>>>>
>>>
>>>
>>
>>
>



From bates at stat.wisc.edu  Tue Mar 24 14:08:53 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 24 Mar 2009 08:08:53 -0500
Subject: [R-sig-ME] [R] CONFIDENCE INTERVAL FOR GLMER MODEL
In-Reply-To: <22677543.post@talk.nabble.com>
References: <22677543.post@talk.nabble.com>
Message-ID: <40e66e0b0903240608s773f69bhdaa2dcbb256e1dab@mail.gmail.com>

On Tue, Mar 24, 2009 at 5:25 AM, LiatL <liat.lampel at gmail.com> wrote:

> I've built a poisson regression model for multiple subjects by using the
> GLMER function. I've also developed some curves for defining its limits but
> I did not succeed in developing confidence interval for the model's curve
> (confint or predict does not work - only for glm).
> Does anyone know how can I produce confidence interva for a glmer model?
> I'll appriciate any help...

You would need to be more explicit about what you mean by "the model's
curve" and what the confidence interval would represent, in the sense
of what components of the variability would be incorporated in the
confidence interval, before we could answer such a question.

May I suggest that the discussion be moved to the
R-SIG-Mixed-Models at R-project.org mailing list, which I have cc:d on
this reply?



From Wiebke.Neumann at vfm.slu.se  Tue Mar 24 16:13:14 2009
From: Wiebke.Neumann at vfm.slu.se (Wiebke Neumann)
Date: Tue, 24 Mar 2009 16:13:14 +0100
Subject: [R-sig-ME] lmer problems;
 1) bs-splines with interaction term, 2) model plotting
Message-ID: <9BADC0E68A9CD44FA320511FCCBA96CAA6664E68E4@exmbx3.ad.slu.se>


Dear R users,

I am analyzing moose crossing roads in northern Sweden, where a GPS moose location gets a 1 when it is connected to road crossing event (1). I compare this dataset with a same-sized random sample of moose fixes that are not connected to any road crossing event (0). The explanatory variables include road density and julian day. I am trying to find out what could predict road crossings in moose to reduce collisions with cars. The lmer appeared most appropriate to handle large amount of data as well as take random effects into account and assign smoothers. 


I have two questions regarding the lmer model frame.

1) How can I apply an interaction term in the lmer using continuous fixed factors with bs-splines and random effects? And how have the degrees of freedom to be assigned? Have they to be the sum of the main effects?
glmer(cross ~ bs(Rden)+ bs(j,df=4)+ bs(Rden,j,df=7)+(1|OBJECT_ID),family=binomial) 
 

The following warning message is given by R.

1: In if (nIknots < 0) { :
  the condition has length > 1 and only the first element will be used
2: In if (nIknots > 0) { :
  the condition has length > 1 and only the first element will be used
3: In if (nIknots < 0) { :
  the condition has length > 1 and only the first element will be used
4: In if (nIknots > 0) { :
  the condition has length > 1 and only the first element will be used
5: In mer_finalize(ans) : gr cannot be computed at initial par (65)



2) What is the command to plot the model for model checking and to show the pattern of the terms contributing to the model in lmer?



I work on a Windows machine in R 2.8.1, Package lme4 version 0.999375-28 Index


Thanks for your help in advance. 


Best regards, Wiebke Neumann 



_________________________________________________________________
Wiebke Neumann
Dept. of Wildlife, Fish, and Environmental Studies                                                                                                                                                                                                                           
Faculty of Forest Science                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Swedish University of Agricultural Sciences, SLU
SE-90183 Ume?, Sweden
Office: +46 90 786 83 02
Mobile:+46 70 520 92 81
Fax: +46 90 786 81 62
Email: Wiebke.Neumann at vfm.slu.se                                                                                                                                                                                                                                                                                                                       http://www.moose-research.se/



From Kate.Pressland at bristol.ac.uk  Tue Mar 24 20:13:20 2009
From: Kate.Pressland at bristol.ac.uk (CL Pressland)
Date: Tue, 24 Mar 2009 19:13:20 +0000
Subject: [R-sig-ME] repeat measures: time series or mixed model?
Message-ID: <1A87C2869A905D801A0FC022@bio-mammal03.bio.bris.ac.uk>

Dear all,

I've scrolled through the archives and CRAN help pages and can't find an 
answer for my query: my apologies if it is rather basic.

I have a data set that is unbalanced and consists of:

67 SITEs measured over several YEARs every WEEK (April-Sept) for 
butterflies (LEPS per m - continuous data). I'm interested in the 
MANagement code (categorical) assigned to each site, but I have also data 
on TEMPerature, average SUN and WIND (some missing data with weather 
variables though). My guess is that a linear mixed model would be most 
appropriate and have constructed this code first of all:

model<-lme(LEPS~MAN,random=~YEAR/WEEK|SITE)

The output gives me:
--------------------------------------------------------------------
Linear mixed-effects model fit by REML
 Data: NULL
        AIC       BIC   logLik
  -37631.24 -37566.48 18824.62

Random effects:
 Formula: ~YEAR/WEEK| SITE
 Structure: General positive-definite, Log-Cholesky parametrization
             	 StdDev       Corr
(Intercept)   	5.875102e-03 (Intr) YEAR
YEAR          	1.392439e-06 -0.164
YEAR:WEEK 		5.068196e-07  0.531  0.301
Residual      	3.532589e-02

Fixed effects: LEPS ~ MAN
                  Value   Std.Error   DF  t-value p-value
(Intercept) 0.009866718 0.001428957 9793 6.904841    0.00
MAN     	0.000028304 0.001127429   65 0.025105    0.98
 Correlation:
        (Intr)
MAN -0.685

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max
-2.70566579 -0.40089121 -0.18073723  0.05900735 19.16411466

Number of Observations: 9860
Number of Groups: 67
--------------------------------------------------------------------

I am slightly confused by the output. I figure that this clearly means that 
management has no effect on butterflies but how can I figure out what 
effect SITE, YEAR and WEEK have on the data? Would I have to also include 
them in the fixed effects side of the formula (I'm unsure if this is 
allowed)? Also, how could I include my weather variables? Would they just 
be placed on the fixed effect side of the formula as they are covariates?

e.g. model<-lme(LEPS~MAN+TEMP+SUN+WIND,random=~YEAR/WEEK|SITE)

They are bound to be correlated so does that cause problems when putting 
into the same model? Could I simply use na.exclude in this instance to 
remove records missing but still include the data for the other effects? I 
have seen so many different ways in which this can be done - I want to make 
sure I do it correctly.

Furthermore, I am unsure if this qualifies as a time-series analysis or if 
linear mixed modeling is ok. The data is unbalanced as not all sites have 
records for each week. The data are clearly nested so from all I've read 
seems to be pointing to lmm. I understand that there will be correlations 
between each repeat measure (week) as a butterfly recorded in week 1 
*might* be the same butterfly in week 2, but surely this occurs with all 
repeat measures designs?

I'm certain this query must be simple - can anyone clarify what to do?

Any help is truly appreciated.

Kate



From smouksassi at Pharsight.com  Tue Mar 24 20:43:27 2009
From: smouksassi at Pharsight.com (Samer Mouksassi)
Date: Tue, 24 Mar 2009 12:43:27 -0700
Subject: [R-sig-ME] repeat measures: time series or mixed model?
In-Reply-To: <1A87C2869A905D801A0FC022@bio-mammal03.bio.bris.ac.uk>
References: <1A87C2869A905D801A0FC022@bio-mammal03.bio.bris.ac.uk>
Message-ID: <5B833E900330354F9FDA984D06F924280599EB33@ca-exchange.corp.pharsight.com>


Hello,

One way to account for within butterfly correlation is to have this
effect entered as a random effect. This will automatically accounts for
the correlation.

A good structural model ( i.e linear, quadratic or nonlinear in Week
(time)  ) will explain the time course of your endpoint and then no
residual serial correlation should remain. You may also model the
correlation itself but again this will depend on the questions you are
trying to answer.

Samer




-----Original Message-----
From: r-sig-mixed-models-bounces at r-project.org
[mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of CL
Pressland
Sent: 2009-03-24 15:13
To: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] repeat measures: time series or mixed model?

Dear all,

I've scrolled through the archives and CRAN help pages and can't find an

answer for my query: my apologies if it is rather basic.

I have a data set that is unbalanced and consists of:

67 SITEs measured over several YEARs every WEEK (April-Sept) for 
butterflies (LEPS per m - continuous data). I'm interested in the 
MANagement code (categorical) assigned to each site, but I have also
data 
on TEMPerature, average SUN and WIND (some missing data with weather 
variables though). My guess is that a linear mixed model would be most 
appropriate and have constructed this code first of all:

model<-lme(LEPS~MAN,random=~YEAR/WEEK|SITE)

The output gives me:
--------------------------------------------------------------------
Linear mixed-effects model fit by REML
 Data: NULL
        AIC       BIC   logLik
  -37631.24 -37566.48 18824.62

Random effects:
 Formula: ~YEAR/WEEK| SITE
 Structure: General positive-definite, Log-Cholesky parametrization
             	 StdDev       Corr
(Intercept)   	5.875102e-03 (Intr) YEAR
YEAR          	1.392439e-06 -0.164
YEAR:WEEK 		5.068196e-07  0.531  0.301
Residual      	3.532589e-02

Fixed effects: LEPS ~ MAN
                  Value   Std.Error   DF  t-value p-value
(Intercept) 0.009866718 0.001428957 9793 6.904841    0.00
MAN     	0.000028304 0.001127429   65 0.025105    0.98
 Correlation:
        (Intr)
MAN -0.685

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max
-2.70566579 -0.40089121 -0.18073723  0.05900735 19.16411466

Number of Observations: 9860
Number of Groups: 67
--------------------------------------------------------------------

I am slightly confused by the output. I figure that this clearly means
that 
management has no effect on butterflies but how can I figure out what 
effect SITE, YEAR and WEEK have on the data? Would I have to also
include 
them in the fixed effects side of the formula (I'm unsure if this is 
allowed)? Also, how could I include my weather variables? Would they
just 
be placed on the fixed effect side of the formula as they are
covariates?

e.g. model<-lme(LEPS~MAN+TEMP+SUN+WIND,random=~YEAR/WEEK|SITE)

They are bound to be correlated so does that cause problems when putting

into the same model? Could I simply use na.exclude in this instance to 
remove records missing but still include the data for the other effects?
I 
have seen so many different ways in which this can be done - I want to
make 
sure I do it correctly.

Furthermore, I am unsure if this qualifies as a time-series analysis or
if 
linear mixed modeling is ok. The data is unbalanced as not all sites
have 
records for each week. The data are clearly nested so from all I've read

seems to be pointing to lmm. I understand that there will be
correlations 
between each repeat measure (week) as a butterfly recorded in week 1 
*might* be the same butterfly in week 2, but surely this occurs with all

repeat measures designs?

I'm certain this query must be simple - can anyone clarify what to do?

Any help is truly appreciated.

Kate

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From mick.wu at mail.mcgill.ca  Tue Mar 24 21:07:58 2009
From: mick.wu at mail.mcgill.ca (Gi-Mick Wu)
Date: Tue, 24 Mar 2009 16:07:58 -0400
Subject: [R-sig-ME] Correct dispersion parameter for glmmML?
Message-ID: <B89AE5DF4F2A964FB322E3F53F02EE0D2301D09490@EXMBXVS4A.campus.mcgill.ca>

Dear mixed model users, (my first post, after reading the archives a lot)

This post is about mixed models, but not lmer or lme4; please let me know if it is inappropriate. (I have searched the archives for hours without finding an answer to my question. I have also searched and posted in "R-sig-eco" without results).
Thank you in advance for reading!

I?m fitting a generalized linear mixed model to binary data using glmmML (version 0.81-4), but am unsure of how to obtain the correct dispersion parameter. (I have also just updated R to version 2.8.1)

For a glm model, I read that it should be the SS Pearson's residuals / df, rather than the default residuals (deviance). In my case:

# GLM
# using Pearson's residuals:
> sum(residuals(modl.glm,type="pearson")^2)/modl.glm$df.residual
[1] 1.062947
# using deviance
> modl.glm$deviance/modl.glm$df.residual
[1] 1.409863

For glmmML however, I cannot obtain the pearson residuals, but only the deviance and null deviance (same as deviance for glm):

# GLMM
# using Pearson's residuals
> residuals(modl.glmmML)
NULL
# using deviance
> modl.glmmML$deviance/modl.glmmML$df.residual
[1] 1.413364
# using null deviance
> modl.glmmML$cluster.null.deviance/modl.glmmML$cluster.null.df
[1] 1.409863

I am confused as to which (if any) of the dispersion parameter is valid for the glmm model. Any one have an idea?

Thanks in advance for reading and hopefully for some much needed answers or pointers.
Mick
PS In case it is relevant or for curiosity's sake, here's the experimental design:

The experiment consists in testing the effect of experience on odour preferences of parasitoid wasps. Parasitoids lay eggs in host insects, which will be devoured from the inside until they die; like the movie Alien :-)

Wasps were exposed to two different odours, with or without the presence of hosts (call them rewarded / unrewarded odour). Their odour preference was then tested in a Y-tube olfactometer 5 times after exposure (2h, 6h, 26h, 50h, 98h). 36 individuals were exposed to each odour either 1,2,3, or 4 times (9 individuals/treatment level). Odours were presented in alternation with half the wasps starting with the rewarded odour.
(Total number of binary choices = 180). I set Time and NbExposure as fixed effects because I'm really interested in their effect and wasp ID as random. I also added body size, which seem to play a role on the effect of experience in honeybees, but it is secondary, so it could be removed (to avoid overfitting).

# R code for fitting the glmm
form <- cbind(RewardOdour, UnrewardOdour) ~ (Time + NbExposure + Ordr + BodySize)^2  
# interaction terms limited to second order for parsimony
modl.glmmML <- glmmML(form, family=binomial, data=clean.dat,cluster=clean.dat[,"ID"],
			prior="gaussian", method="ghq", n.points=8)           # same results with n.points = 20

> modl.glmmML

Call:  glmmML(formula = form, family = binomial, data = clean.dat, cluster = clean.dat[, "ID"], prior = "gaussian", method = "ghq", n.points = 8) 

                                     coef            se(coef)      z           Pr(>|z|)
(Intercept)             -7.539e+00  7.5906651  -0.99324   0.3210
Time                        -6.303e-03   0.0648177  -0.09724   0.9230
Exposure                 2.843e+00  2.3465958   1.21139   0.2260
Ordr                         4.366e+00  5.1119166   0.85401   0.3930
BodySize                  1.324e-02   0.0126869   1.04333   0.2970
Time:Exposure         1.295e-03   0.0042982   0.30123   0.7630
Time:Ordr                 2.042e-02   0.0093118   2.19259   0.0283
Time:BodySize         -2.554e-06  0.0001093  -0.02337   0.9810
Exposure:Ordr        -2.333e-01  0.3489015  -0.66859   0.5040
Exposure:BodySize  -4.833e-03   0.0037688  -1.28235   0.2000
Ordr:BodySize          -7.487e-03   0.0086466  -0.86585   0.3870

Scale parameter in mixing distribution:  0.484 gaussian 
Std. Error:                                       0.3204 

Residual deviance: 237.4 on 168 degrees of freedom      AIC: 261.4



From brant.inman at me.com  Wed Mar 25 03:10:21 2009
From: brant.inman at me.com (Brant Inman)
Date: Tue, 24 Mar 2009 22:10:21 -0400
Subject: [R-sig-ME] lmer: ML and REML estimation
Message-ID: <1183493B-6581-4676-B10F-55708C70EF45@me.com>

Experts:

I am writing a paper using the results that I obtained this week with  
lmer (thanks Doug Bates).  I wanted to clarify one technical point  
about lmer to make sure that I understand its mechanics correctly when  
reporting my statistical methods in the paper.

I used lmer to fit several multilevel logistic regression models.  The  
help page for lmer states that these binomial models are estimated  
with maximum likelihood (ML) methods.  My high-level reading on linear  
mixed effects models has suggested that REML estimates are better than  
ML estimates, I wondered whether non-linear likelihoods, like those of  
the binomial models that I have used, can be estimated with REML  
methods or not.  If this question is obviously stupid, keep in mind  
that I am just a dumb surgeon, not a statistician.

Brant Inman



From bates at stat.wisc.edu  Wed Mar 25 14:47:04 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 25 Mar 2009 08:47:04 -0500
Subject: [R-sig-ME] lmer: ML and REML estimation
In-Reply-To: <1183493B-6581-4676-B10F-55708C70EF45@me.com>
References: <1183493B-6581-4676-B10F-55708C70EF45@me.com>
Message-ID: <40e66e0b0903250647n42ba3838l2d722032283ceb1@mail.gmail.com>

On Tue, Mar 24, 2009 at 9:10 PM, Brant Inman <brant.inman at me.com> wrote:
> Experts:

> I am writing a paper using the results that I obtained this week with lmer
> (thanks Doug Bates). ?I wanted to clarify one technical point about lmer to
> make sure that I understand its mechanics correctly when reporting my
> statistical methods in the paper.

> I used lmer to fit several multilevel logistic regression models. ?The help
> page for lmer states that these binomial models are estimated with maximum
> likelihood (ML) methods. ?My high-level reading on linear mixed effects
> models has suggested that REML estimates are better than ML estimates, I
> wondered whether non-linear likelihoods, like those of the binomial models
> that I have used, can be estimated with REML methods or not.

I think of the REML criterion as an adjustment to the likelihood for
linear mixed-effects models for the purpose of producing the point
estimates of variance components that people think should be returned.
 It is patterned after the adjustments for degrees of freedom in
estimating variances from a single sample and for a linear regression
model.  For example, we define the sample variance as the sum of
squared deviations from the sample mean divided by n - 1.  If we were
to create the maximum likelihood estimate of the variance (assuming
the sample is a realization of independent and identically distributed
Gaussian random variables) we would divide by n, not n-1.  Similarly,
in a linear regression model we estimate the residual variance as the
residual sum of squares divided by n - p whereas the maximum
likelihood estimator has n in the denominator.

One way of thinking of this process is to divide the sample space into
two orthogonal linear subspaces where the sample mean or, more
generally, the coefficients of the linear regression model are
determined by the component in the p-dimensional predictor space and
the variance component is defined by the component in the
(n-p)-dimensional space that is orthogonal to the predictor space.
This argument can be carried over to linear mixed models but begins to
break down seriously if you try to carry it over to generalized linear
models or generalized linear mixed models.

I would claim that maximum likelihood estimates are well-defined for
generalized linear mixed models but REML estimates are not. (It is
true that Mary Lindstrom and I did offer a definition of REML
estimates for nonlinear mixed-effects models but I consider that a
youthful indiscretion and I didn't inhale. :-)

The bottom line is that REML only makes sense for linear mixed-effects models.

> If this
> question is obviously stupid, keep in mind that I am just a dumb surgeon,
> not a statistician.



From bolker at ufl.edu  Wed Mar 25 23:21:57 2009
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 25 Mar 2009 18:21:57 -0400
Subject: [R-sig-ME] lmer: ML and REML estimation
In-Reply-To: <40e66e0b0903250647n42ba3838l2d722032283ceb1@mail.gmail.com>
References: <1183493B-6581-4676-B10F-55708C70EF45@me.com>
	<40e66e0b0903250647n42ba3838l2d722032283ceb1@mail.gmail.com>
Message-ID: <49CAAE85.3070608@ufl.edu>

Douglas Bates wrote:

> I would claim that maximum likelihood estimates are well-defined for
> generalized linear mixed models but REML estimates are not. (It is
> true that Mary Lindstrom and I did offer a definition of REML
> estimates for nonlinear mixed-effects models but I consider that a
> youthful indiscretion and I didn't inhale. :-)
> 
> The bottom line is that REML only makes sense for linear mixed-effects models.

  Thank you!  Good to see that clarified.  Looks like we got it
wrong, or at least misleading, in our recent TREE paper -- oh well,
science marches on.

  Might anyone here be able to point to a citation (other than "D.
Bates, r-sig-mixed-models mailing list, 25 March 2009) that would
support this statement ... ?

  For what it's worth (risking the wrath of the GODS), PROC NLMIXED
doesn't try to do REML, but claims it's a computational issue rather
than one of definition:

"With PROC MIXED you can perform both maximum likelihood and restricted
maximum likelihood (REML) estimation, whereas PROC NLMIXED only
implements maximum likelihood. This is because the analog to the REML
method in PROC NLMIXED would involve a high dimensional integral over
all of the fixed-effects parameters, and this integral is typically not
available in closed form."
<http://www.sfu.ca/sasdoc/sashtml/stat/chap46/sect4.htm>

On the other hand, GLIMMIX lets you go ahead and hang yourself:
                                                    "Additionally,
GLIMMIX allows the use of restricted maximum likelihood (REML) methods,
which have been shown to produce better estimates than full maximum
likelihood (ML) when the number of higher-level units is small. REML is
not available in NLMIXED."
<www.nesug.org/proceedings/nesug06/an/da08.pdf>

This is shortly after stating that
                                                       "GLIMMIX, in
contrast, can produce potentially biased estimates for both fixed
effects and covariance parameters, especially for binary data
(Schabenberger 2005)."  (!! see also Breslow 2003)

  Does anyone out there have a suggestion/defense for when it *is*
acceptable to use PQL/MQL to fit binary GLMMs?

  A further question: do you think it will generally be true that ML
estimates of random effects variances will be slightly biased downwards
because we don't have an analogue of REML?  (A wild guess, but I would
think that the mean of the posterior distribution of the variance
estimate (with uninformative priors) would be unbiased since it averages
across the variation in the estimate of the fixed effects????)

  cheers
    Ben Bolker

-- 
Ben Bolker
Associate professor, Biology Dep't, Univ. of Florida
bolker at ufl.edu / www.zoology.ufl.edu/bolker
GPG key: www.zoology.ufl.edu/bolker/benbolker-publickey.asc



From maj at stats.waikato.ac.nz  Thu Mar 26 01:47:11 2009
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 26 Mar 2009 13:47:11 +1300
Subject: [R-sig-ME] Arctic Nematode Growth Data
Message-ID: <49CAD08F.1090502@stats.waikato.ac.nz>

I thought some list members might appreciate the following data set as 
an example suitable for nonlinear mixed effects modelling. It is data 
from Dennis Proctor's PhD thesis which I came by 30 years ago but have 
only just fully entered onto the computer. Unfortunately I have lost 
touch with Dennis but I have no doubt that he would not object to fair 
usage provided attribution was made.

At each age there are 3 replicate groups of nematodes measured. 
Apparently the length measurement was destructive and no nematodes are 
multiply measured.

The R code below reads the data into a data frame as well as creating 
some other structures of interest. I am well aware that constructing a 
vector by concatenation inside a loop is regarded as poor R style, but I 
could not remember what the approved alternative was!

# Arctic Nematode Growth Data from
# Proctor, D. L. C. (1979)  "Energy flow through free-
# living soil nematodes in High Arctic terrestrial
# communities. PhD thesis, Dept of Entomology, U. Alberta.
reps = scan()
1 4 21 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
5 20 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
2 15 10 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 1 8 10 13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 4 9 10 4 6 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 1 7 8 13 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 1 2 3 14 5 5 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 1 1 15 13 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 1 7 12 17 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 1 0 0 7 13 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 2 1 12 9 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 3 1 2 6 13 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 1 0 2 2 1 1 4 5 0 2 5 5 4 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 1 12 5 5 5 2 3 4 0 0 0 0 0 0 0 0 0
0 0 0 0 1 0 1 0 0 1 0 4 7 6 4 7 2 1 0 0 0 0 0 0 0 0 0
0 0 0 0 2 1 1 1 1 0 1 2 5 1 1 4 3 2 4 3 0 0 0 0 0 0 0
0 1 0 0 1 0 0 0 1 2 4 2 6 0 1 4 4 4 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 6 9 9 4 2 0 0 0 0
0 0 0 0 0 1 0 0 1 0 0 1 2 2 1 1 2 0 3 6 11 3 2 0 0 0 0
1 0 0 0 0 1 0 0 0 0 1 1 3 3 1 4 1 3 6 5 1 3 1 0 0 0 0
0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 4 8 8 7 4 1 0 0
0 0 0 0 0 0 0 1 0 0 0 2 0 0 1 1 2 6 7 6 5 1 0 0 0 0 0
0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 1 1 2 7 12 7 2 0 1 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 8 9 8 5 1
0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 2 11 12 6 2 1 0
0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 3 3 0 1 5 3 7 2 5 2 0
0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 6 7 12 6 0 0 1 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 6 12 6 9 1 0 0
0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 3 1 3 6 8 6 3 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3 7 16 5 5 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 14 11 2 2 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 5 7 11 9 1 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 3 3 5 8 6 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 6 3 3 7 6 2 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 7 9 10 4 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 2 1 10 14 4 1 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 3 12 6 4 1 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 4 6 8 6 2 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 1 6 5 8 5 4 1 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3 2 6 6 6 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 3 5 11 6 5 1 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 4 7 10 4 2 1 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 5 4 8 11 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3 8 10 2 6 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 1 5 9 5 3 0 0

len = seq(0.22, by=0.04,length=27) # class mid-points
repage = rep(seq(4,60,4),rep(3,15))
fr = matrix(reps,nrow=45,ncol=27,byrow = TRUE)
repsize = apply(fr,1,sum)
age = rep(repage,repsize)    # age of nemetode in days
repl = rep(1:45,repsize)     # replicate number
leng = NULL
for ( row in 1:45 ) {
    leng = c(leng, len[rep(1:27,fr[row,])])
    }
# leng is rounded length in mm
nematodes = data.frame(age,repl,leng)



-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441   Mobile 021 0200 8350



From otter at otter-rsch.com  Thu Mar 26 04:22:58 2009
From: otter at otter-rsch.com (dave fournier)
Date: Wed, 25 Mar 2009 19:22:58 -0800
Subject: [R-sig-ME] lmer: ML and REML estimation
In-Reply-To: <49CAAE85.3070608@ufl.edu>
References: <49CAAE85.3070608@ufl.edu>
Message-ID: <49CAF512.609@otter-rsch.com>

Since you asked. With AD Model Builders Random Effects module
it is possible to declare any of the parameters which might be
considered fixed effects to be random effects so that they are
"integrated over". I did some simulations work with this
with nonlinear fisheries management models far more difficult that what
lmer can deal with and found that the procedure worked quite well.
I can discuss this with you off off list if you are interested.

   Dave

-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



From p.dalgaard at biostat.ku.dk  Thu Mar 26 09:57:20 2009
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 26 Mar 2009 09:57:20 +0100
Subject: [R-sig-ME] lmer: ML and REML estimation
In-Reply-To: <40e66e0b0903250647n42ba3838l2d722032283ceb1@mail.gmail.com>
References: <1183493B-6581-4676-B10F-55708C70EF45@me.com>
	<40e66e0b0903250647n42ba3838l2d722032283ceb1@mail.gmail.com>
Message-ID: <49CB4370.2030109@biostat.ku.dk>

Douglas Bates wrote:

> 
> I would claim that maximum likelihood estimates are well-defined for
> generalized linear mixed models but REML estimates are not. (It is
> true that Mary Lindstrom and I did offer a definition of REML
> estimates for nonlinear mixed-effects models but I consider that a
> youthful indiscretion and I didn't inhale. :-)

:-)

> The bottom line is that REML only makes sense for linear mixed-effects models.

Presumably this requires some qualification.

It's not like people haven't tried. I have seen at least one paper 
attempting to make REML work with some GLM cases (it's been a while, but 
I think I can still locate the pile in which I put it...).

What is certainly true is that it is not usually possible to achieve the 
clean separation of the sample space into the linear mean value subspace 
and its orthogonal (or quotient space if you like), that REML relies on.

In the other hand, it's not like the biases that REML tries to overcome 
suddenly disappears when things become nonlinear, so _some_ form of 
adjusted likelihood may be appropriate, it's just not necessarily REML.

-- 
    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
  (*) \(*) -- University of Copenhagen   Denmark      Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)              FAX: (+45) 35327907



From ken at kjbeath.com.au  Thu Mar 26 11:10:55 2009
From: ken at kjbeath.com.au (Ken Beath)
Date: Thu, 26 Mar 2009 21:10:55 +1100
Subject: [R-sig-ME] Correct dispersion parameter for glmmML?
In-Reply-To: <B89AE5DF4F2A964FB322E3F53F02EE0D2301D09490@EXMBXVS4A.campus.mcgill.ca>
References: <B89AE5DF4F2A964FB322E3F53F02EE0D2301D09490@EXMBXVS4A.campus.mcgill.ca>
Message-ID: <461AF912-9BFB-456A-9DDF-7C1B275F0695@kjbeath.com.au>

In your model fitting you have the intercept estimate large and with  
large standard error and similarly for Ordr and maybe Exposure. This  
is usually an indication of a problem with the fitting of binomial  
data, probably separation due to overfitting. I would start with a  
simple model and try and work out which terms are causing problems.

I prefer not to think about overdispersed GLMM, they seem a bit odd  
statistically, maybe a GEE based approach is better.

Ken

On 25/03/2009, at 7:07 AM, Gi-Mick Wu wrote:

> Dear mixed model users, (my first post, after reading the archives a  
> lot)
>
> This post is about mixed models, but not lmer or lme4; please let me  
> know if it is inappropriate. (I have searched the archives for hours  
> without finding an answer to my question. I have also searched and  
> posted in "R-sig-eco" without results).
> Thank you in advance for reading!
>
> I?m fitting a generalized linear mixed model to binary data using  
> glmmML (version 0.81-4), but am unsure of how to obtain the correct  
> dispersion parameter. (I have also just updated R to version 2.8.1)
>
> For a glm model, I read that it should be the SS Pearson's  
> residuals / df, rather than the default residuals (deviance). In my  
> case:
>
> # GLM
> # using Pearson's residuals:
>> sum(residuals(modl.glm,type="pearson")^2)/modl.glm$df.residual
> [1] 1.062947
> # using deviance
>> modl.glm$deviance/modl.glm$df.residual
> [1] 1.409863
>
> For glmmML however, I cannot obtain the pearson residuals, but only  
> the deviance and null deviance (same as deviance for glm):
>
> # GLMM
> # using Pearson's residuals
>> residuals(modl.glmmML)
> NULL
> # using deviance
>> modl.glmmML$deviance/modl.glmmML$df.residual
> [1] 1.413364
> # using null deviance
>> modl.glmmML$cluster.null.deviance/modl.glmmML$cluster.null.df
> [1] 1.409863
>
> I am confused as to which (if any) of the dispersion parameter is  
> valid for the glmm model. Any one have an idea?
>
> Thanks in advance for reading and hopefully for some much needed  
> answers or pointers.
> Mick
> PS In case it is relevant or for curiosity's sake, here's the  
> experimental design:
>
> The experiment consists in testing the effect of experience on odour  
> preferences of parasitoid wasps. Parasitoids lay eggs in host  
> insects, which will be devoured from the inside until they die; like  
> the movie Alien :-)
>
> Wasps were exposed to two different odours, with or without the  
> presence of hosts (call them rewarded / unrewarded odour). Their  
> odour preference was then tested in a Y-tube olfactometer 5 times  
> after exposure (2h, 6h, 26h, 50h, 98h). 36 individuals were exposed  
> to each odour either 1,2,3, or 4 times (9 individuals/treatment  
> level). Odours were presented in alternation with half the wasps  
> starting with the rewarded odour.
> (Total number of binary choices = 180). I set Time and NbExposure as  
> fixed effects because I'm really interested in their effect and wasp  
> ID as random. I also added body size, which seem to play a role on  
> the effect of experience in honeybees, but it is secondary, so it  
> could be removed (to avoid overfitting).
>
> # R code for fitting the glmm
> form <- cbind(RewardOdour, UnrewardOdour) ~ (Time + NbExposure +  
> Ordr + BodySize)^2
> # interaction terms limited to second order for parsimony
> modl.glmmML <- glmmML(form, family=binomial,  
> data=clean.dat,cluster=clean.dat[,"ID"],
> 			prior="gaussian", method="ghq", n.points=8)           # same  
> results with n.points = 20
>
>> modl.glmmML
>
> Call:  glmmML(formula = form, family = binomial, data = clean.dat,  
> cluster = clean.dat[, "ID"], prior = "gaussian", method = "ghq",  
> n.points = 8)
>
>                                     coef            se(coef)       
> z           Pr(>|z|)
> (Intercept)             -7.539e+00  7.5906651  -0.99324   0.3210
> Time                        -6.303e-03   0.0648177  -0.09724   0.9230
> Exposure                 2.843e+00  2.3465958   1.21139   0.2260
> Ordr                         4.366e+00  5.1119166   0.85401   0.3930
> BodySize                  1.324e-02   0.0126869   1.04333   0.2970
> Time:Exposure         1.295e-03   0.0042982   0.30123   0.7630
> Time:Ordr                 2.042e-02   0.0093118   2.19259   0.0283
> Time:BodySize         -2.554e-06  0.0001093  -0.02337   0.9810
> Exposure:Ordr        -2.333e-01  0.3489015  -0.66859   0.5040
> Exposure:BodySize  -4.833e-03   0.0037688  -1.28235   0.2000
> Ordr:BodySize          -7.487e-03   0.0086466  -0.86585   0.3870
>
> Scale parameter in mixing distribution:  0.484 gaussian
> Std. Error:                                       0.3204
>
> Residual deviance: 237.4 on 168 degrees of freedom      AIC: 261.4
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Thu Mar 26 18:15:16 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 26 Mar 2009 12:15:16 -0500
Subject: [R-sig-ME] Arctic Nematode Growth Data
In-Reply-To: <49CAD08F.1090502@stats.waikato.ac.nz>
References: <49CAD08F.1090502@stats.waikato.ac.nz>
Message-ID: <40e66e0b0903261015h4ba10d4eyd69473d3239a5fc4@mail.gmail.com>

On Wed, Mar 25, 2009 at 7:47 PM, Murray Jorgensen
<maj at stats.waikato.ac.nz> wrote:
> I thought some list members might appreciate the following data set as an
> example suitable for nonlinear mixed effects modelling. It is data from
> Dennis Proctor's PhD thesis which I came by 30 years ago but have only just
> fully entered onto the computer. Unfortunately I have lost touch with Dennis
> but I have no doubt that he would not object to fair usage provided
> attribution was made.

Thanks for sending the data, Murray.

> At each age there are 3 replicate groups of nematodes measured. Apparently
> the length measurement was destructive and no nematodes are multiply
> measured.

> The R code below reads the data into a data frame as well as creating some
> other structures of interest. I am well aware that constructing a vector by
> concatenation inside a loop is regarded as poor R style, but I could not
> remember what the approved alternative was!

unlist the result of apply

> str(unlist(apply(fr, 1, function(x) len[rep(1:27, x)])))
 num [1:1503] 0.22 0.26 0.26 0.26 0.26 0.3 0.3 0.3 0.3 0.3 ...



From julien.martin2 at usherbrooke.ca  Thu Mar 26 18:29:59 2009
From: julien.martin2 at usherbrooke.ca (Julien Martin)
Date: Thu, 26 Mar 2009 13:29:59 -0400
Subject: [R-sig-ME] Confidence intervals of variance components
In-Reply-To: <mailman.3069.1235926961.4476.r-sig-mixed-models@r-project.org>
References: <mailman.3069.1235926961.4476.r-sig-mixed-models@r-project.org>
Message-ID: <49CBBB97.7090202@usherbrooke.ca>

Hi
I want to estimate standard error or confidence intervals for the 
estimate of varaince associated with a random effect.
With "lme", I could use "intervals" however, this does not work with "lmer"
I tried "pvals.fnc" from "languageR" package that should provide what I 
want.
However, I find stange that the MCMCmean is quite different from the 
model estimate and that the model estimate is not even included in the 
95CI provided.
Is it normal? Do I misinterpret the output?
Is there another way to obtain CI or SE of varaince (or Std.Dev) of 
random effects with lmer?

Here an example
Estimate of Standard Deviance for Subject (Intercept) is not even close 
from the MCMCmedian
 >fm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy)
 >pvals.fnc(fm2,10000,addPlot=F)$random
    Groups        Name Std.Dev. MCMCmedian MCMCmean HPD95lower HPD95upper
1  Subject (Intercept)  29.8219    15.4880  15.5692     8.6060    22.9435
2  Subject        Days   7.1285     6.4052   6.5464     4.1641     9.1933
3 Residual              30.4337    26.8669  26.9354    23.8227    30.1357

Thanks
Julien



From bates at stat.wisc.edu  Thu Mar 26 22:36:21 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 26 Mar 2009 16:36:21 -0500
Subject: [R-sig-ME] lmer: ML and REML estimation
In-Reply-To: <49CAAE85.3070608@ufl.edu>
References: <1183493B-6581-4676-B10F-55708C70EF45@me.com>
	<40e66e0b0903250647n42ba3838l2d722032283ceb1@mail.gmail.com>
	<49CAAE85.3070608@ufl.edu>
Message-ID: <40e66e0b0903261436i639e7ebevd43c14ab52d7aff6@mail.gmail.com>

On Wed, Mar 25, 2009 at 5:21 PM, Ben Bolker <bolker at ufl.edu> wrote:
> Douglas Bates wrote:
>
>> I would claim that maximum likelihood estimates are well-defined for
>> generalized linear mixed models but REML estimates are not. (It is
>> true that Mary Lindstrom and I did offer a definition of REML
>> estimates for nonlinear mixed-effects models but I consider that a
>> youthful indiscretion and I didn't inhale. :-)
>>
>> The bottom line is that REML only makes sense for linear mixed-effects models.
>
> ?Thank you! ?Good to see that clarified. ?Looks like we got it
> wrong, or at least misleading, in our recent TREE paper -- oh well,
> science marches on.
>
> ?Might anyone here be able to point to a citation (other than "D.
> Bates, r-sig-mixed-models mailing list, 25 March 2009) that would
> support this statement ... ?
>
> ?For what it's worth (risking the wrath of the GODS), PROC NLMIXED
> doesn't try to do REML, but claims it's a computational issue rather
> than one of definition:
>
> "With PROC MIXED you can perform both maximum likelihood and restricted
> maximum likelihood (REML) estimation, whereas PROC NLMIXED only
> implements maximum likelihood. This is because the analog to the REML
> method in PROC NLMIXED would involve a high dimensional integral over
> all of the fixed-effects parameters, and this integral is typically not
> available in closed form."
> <http://www.sfu.ca/sasdoc/sashtml/stat/chap46/sect4.htm>

Interesting.  (Also interesting that the URL is to Simon Fraser
University in British Columbia and not SAS Institute but whatever
works.)

My concept of REML is more like what Peter Dalgaard described in a
later response on this thread.  I consider the REML criterion to be
related to the decomposition of the response space into the linear
subspace spanned by the columns of the fixed-effects model matrix and
the orthogonal to this subspace.   It happens this is equivalent to
using the integral of the likelihood over the fixed effects as the
estimation criterion but that relationship depends on the mean
response being a linear function of the fixed-effects parameters, and
on the probability model generating a Euclidean metric.  In
generalized linear models the metric is non-Euclidean and the mean is
related to the linear predictor through a nonlinear inverse link
function.

It is not obvious to me that integrating the likelihood over the
fixed-effects would produce a meaningful estimation criterion for
GLMMs.  I'm not saying that it wouldn't but I haven't thought through
the ramifications.  It wouldn't correspond to what I would recognize
as "residual" maximum likelihood because there is no cleanly defined
residual space.

> On the other hand, GLIMMIX lets you go ahead and hang yourself:
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?"Additionally,
> GLIMMIX allows the use of restricted maximum likelihood (REML) methods,
> which have been shown to produce better estimates than full maximum
> likelihood (ML) when the number of higher-level units is small. REML is
> not available in NLMIXED."
> <www.nesug.org/proceedings/nesug06/an/da08.pdf>

> This is shortly after stating that
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? "GLIMMIX, in
> contrast, can produce potentially biased estimates for both fixed
> effects and covariance parameters, especially for binary data
> (Schabenberger 2005)." ?(!! see also Breslow 2003)

Bias is a funny beast and I wish we were not so inclined to divide
estimators into unbiased and biased.  First, bias is a binary
property.  An estimator is either biased or unbiased, which provides
you with exactly 1 bit of information (in the information-theoretic
sense) about the estimator.  Second, it is based on an expected value,
which is an integral.  Thus it depends upon the scale on which you are
evaluating the parameter.  In the "i.i.d. sample from a Gaussian
distribution" case S^2 is an unbiased estimate of \sigma^2 but S is
not an unbiased estimate of \sigma.

I have the feeling that the REML versus ML estimator (I try to avoid
the phrase "full maximum likelihood" - to me there is only one maximum
likelihood estimator for a given probability model and I regret that
phrases of the form "<adjective> maximum likelihood" have crept into
the nomenclature) may be important for the point estimator but may not
be as important for the distribution of the estimator but I don't have
any evidence to back that up.

> ?Does anyone out there have a suggestion/defense for when it *is*
> acceptable to use PQL/MQL to fit binary GLMMs?

How did PQL get into this discussion?  I think of PQL as an
optimization algorithm that is intended to produce the maximum
likelihood estimates by an indirect optimization.  If you want to
define a criterion based on integrating the likelihood over the
fixed-effects that is one thing, but I don't think that has to lead
you to the PQL method.

> ?A further question: do you think it will generally be true that ML
> estimates of random effects variances will be slightly biased downwards
> because we don't have an analogue of REML? ?(A wild guess, but I would
> think that the mean of the posterior distribution of the variance
> estimate (with uninformative priors) would be unbiased since it averages
> across the variation in the estimate of the fixed effects????)

Is it true even for linear mixed models that maximum likelihood
estimates of variance components are biased downward? (And what would
happen if instead of estimating a variance component you estimated the
logarithm of a variance or the square root of a variance?)  I would
try a few examples and see what happens.  I have seen cases where the
ML estimates of a particular variance are actually greater than the
REML variance estimate for that component.  Most of the time there is
at least one variance component whose ML estimate is smaller than the
REML estimate but it doesn't have to be so for all the components.
Strangely, the REML and ML estimates of the residual variance are
often similar and that is one case where I know there is a smaller
denominator for REML than for ML.  It appears that the numerator,
which is the penalized residual sum of squares at the parameter
estimates, changes more-or-less proportionally to the denominator.



From rense.nieuwenhuis at me.com  Fri Mar 27 07:47:05 2009
From: rense.nieuwenhuis at me.com (Rense Nieuwenhuis)
Date: Fri, 27 Mar 2009 07:47:05 +0100
Subject: [R-sig-ME] Confidence intervals of variance components
In-Reply-To: <49CBBB97.7090202@usherbrooke.ca>
References: <mailman.3069.1235926961.4476.r-sig-mixed-models@r-project.org>
	<49CBBB97.7090202@usherbrooke.ca>
Message-ID: <1E6C6B62-E1E6-4E94-B9D6-5FA7B8CBF2C5@me.com>


How about the se.ranef() function found in the arm-package?

Kind regards, Rense
- - -- --- ----- --------
Rense Nieuwenhuis
+ 31 6 481 05 683

http://www.rensenieuwenhuis.nl

On 26 mrt 2009, at 18:29, Julien Martin  
<julien.martin2 at usherbrooke.ca> wrote:

> Hi
> I want to estimate standard error or confidence intervals for the  
> estimate of varaince associated with a random effect.
> With "lme", I could use "intervals" however, this does not work with  
> "lmer"
> I tried "pvals.fnc" from "languageR" package that should provide  
> what I want.
> However, I find stange that the MCMCmean is quite different from the  
> model estimate and that the model estimate is not even included in  
> the 95CI provided.
> Is it normal? Do I misinterpret the output?
> Is there another way to obtain CI or SE of varaince (or Std.Dev) of  
> random effects with lmer?
>
> Here an example
> Estimate of Standard Deviance for Subject (Intercept) is not even  
> close from the MCMCmedian
> >fm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),  
> sleepstudy)
> >pvals.fnc(fm2,10000,addPlot=F)$random
>   Groups        Name Std.Dev. MCMCmedian MCMCmean HPD95lower  
> HPD95upper
> 1  Subject (Intercept)  29.8219    15.4880  15.5692     8.6060     
> 22.9435
> 2  Subject        Days   7.1285     6.4052   6.5464     4.1641      
> 9.1933
> 3 Residual              30.4337    26.8669  26.9354    23.8227     
> 30.1357
>
> Thanks
> Julien
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From otter at otter-rsch.com  Fri Mar 27 19:01:12 2009
From: otter at otter-rsch.com (dave fournier)
Date: Fri, 27 Mar 2009 10:01:12 -0800
Subject: [R-sig-ME] lmer: ML and REML estimation
In-Reply-To: <49CB4370.2030109@biostat.ku.dk>
References: <49CB4370.2030109@biostat.ku.dk>
Message-ID: <49CD1468.5020506@otter-rsch.com>

  Maybe I'm missing something here. The way I see it,
in its most general form a mixed model is simply one
where some of the parameters are random variables with a prior.
So say the likelihood looks like

   f(x,u)p(u)

where the vector of parameters u have a prior p(u).
If the u are normally distributed with a vector of
std devs sigma this becomes

    f(x,u)p(u,sigma)

Integrating over u gives

  F(x,sigma) = \Int F(x,u)p(u,sigma) du

so the MLE's xhat,sigamhat are found by maximizing
F wrt x and sigma.

Now if we are willing to take a Bayesian point of view
we could also integrate over x and get the marginal
likelihood for the sigma alone. We may need a prior on x,
or assume a locally uniform prior. In any event we obtain

 G(sigma) = \Int F(x,u)p(u,sigma) du dx

The integral can be approximated using the Laplace approximation
at the mode of  F(x,u)p(u,sigma).  Now I'm not claiming that this
approach always works well, but it is a simple recipe for producing
an estimation procedure which may give better estimates for sigma.
Using AD Model Builders Random effects module which is freely available
at http://admb-project.org it is simple to
change the model specification to carry out this procedure. As I said
before it seemed to work well in a nonlinear fisheries model I looked
at.

-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



From bates at stat.wisc.edu  Sat Mar 28 15:45:31 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 28 Mar 2009 09:45:31 -0500
Subject: [R-sig-ME] lmer: ML and REML estimation
In-Reply-To: <49CD1468.5020506@otter-rsch.com>
References: <49CB4370.2030109@biostat.ku.dk> <49CD1468.5020506@otter-rsch.com>
Message-ID: <40e66e0b0903280745v6f54ac3dp5891f7426090e5d4@mail.gmail.com>

On Fri, Mar 27, 2009 at 1:01 PM, dave fournier <otter at otter-rsch.com> wrote:
> ?Maybe I'm missing something here. The way I see it,
> in its most general form a mixed model is simply one
> where some of the parameters are random variables with a prior.

That's a rather general class of models.  Some might use the name
"Empirical Bayes".

I read the phrase "some of the parameters are random variables" to be
referring to the random effects.  I phrase things slightly
differently.  In particular I don't regard the random effects as
parameters. I regard a mixed-effects model as being based on two
random variables: the response Y whose value, y, has been observed and
an unobserved random effects vector B.  The probability model
describes the conditional distribution of Y, given B = b, and the
unconditional distribution of B.  For the models that can be fit by
the lme4 package the unconditional distribution of B is a multivariate
Gaussian with mean zero and a parameterized variance-covariance
matrix.  The conditional mean of Y, given B = b, depends on b through
a linear predictor expression, X\beta + Zb, that also incorporates the
fixed-effects parameters \beta.

The random effects B are similar to the fixed-effects \beta in that
they both occur as coefficients in the linear predictor but I find
that, for me at least, it is important to retain a distinction between
the random effects and the model parameters.

> So say the likelihood looks like
>
> ? f(x,u)p(u)
>
> where the vector of parameters u have a prior p(u).
> If the u are normally distributed with a vector of
> std devs sigma this becomes
>
> ? ?f(x,u)p(u,sigma)
>
> Integrating over u gives
>
> ?F(x,sigma) = \Int F(x,u)p(u,sigma) du
>
> so the MLE's xhat,sigamhat are found by maximizing
> F wrt x and sigma.

Well, that was kind of the point of the discussion.  For a linear
mixed model there are many equivalent ways to arrive at the REML
criterion.  One way that provides a convenient computational approach
is to maximize, with respect to the random-effects dispersion
parameters, the integral of the likelihood over the fixed-effects
parameters.  The original motivation for the criterion is as the
maximum likelihood estimator of the variance parameters based on a set
of contrasts that are orthogonal to the fixed-effects model space.
That is, these are the maximum likelihood estimators from a set of
linear combinations of data values that are to a certain subspace, the
subspace of all possible residuals.  This is why they are called the
restricted or residual maximum likelihood estimators.

I first saw the relationship between maximizing the integrated
likelihood and the REML estimators in the 1982 Biometrics paper
"Random Effects Models for Longitudinal Data" by Laird and Ware but it
may have been known before that.

It is certainly possible to define estimators that maximize the
integrated likelihood in more general mixed-effects model
specifications but should we call these REML estimators and do we know
that they will share the properties that make REML a popular
estimation criterion for linear mixed models?

I oversimplified when I said that REML is not defined for GLMMs.  I
was using REML to refer to the estimators based on orthogonal
subspaces in the sample space.

> Now if we are willing to take a Bayesian point of view

I thought you were already there when you wrote about "parameters are
random variables with a prior" :-)

> we could also integrate over x and get the marginal
> likelihood for the sigma alone. We may need a prior on x,
> or assume a locally uniform prior. In any event we obtain
>
> ?G(sigma) = \Int F(x,u)p(u,sigma) du dx
>
> The integral can be approximated using the Laplace approximation
> at the mode of ?F(x,u)p(u,sigma). ?Now I'm not claiming that this
> approach always works well, but it is a simple recipe for producing
> an estimation procedure which may give better estimates for sigma.
> Using AD Model Builders Random effects module which is freely available
> at http://admb-project.org it is simple to
> change the model specification to carry out this procedure. As I said
> before it seemed to work well in a nonlinear fisheries model I looked
> at.
>
> --
> David A. Fournier
> P.O. Box 2040,
> Sidney, B.C. V8l 3S3
> Canada
> Phone/FAX 250-655-3364
> http://otter-rsch.com
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Sat Mar 28 15:49:36 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 28 Mar 2009 09:49:36 -0500
Subject: [R-sig-ME] lmer: ML and REML estimation
In-Reply-To: <40e66e0b0903280745v6f54ac3dp5891f7426090e5d4@mail.gmail.com>
References: <49CB4370.2030109@biostat.ku.dk> <49CD1468.5020506@otter-rsch.com>
	<40e66e0b0903280745v6f54ac3dp5891f7426090e5d4@mail.gmail.com>
Message-ID: <40e66e0b0903280749i3d3cc6fay5631c3a55f016a86@mail.gmail.com>

I managed to edit out an important word in one sentence below.

On Sat, Mar 28, 2009 at 9:45 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Fri, Mar 27, 2009 at 1:01 PM, dave fournier <otter at otter-rsch.com> wrote:
>> ?Maybe I'm missing something here. The way I see it,
>> in its most general form a mixed model is simply one
>> where some of the parameters are random variables with a prior.
>
> That's a rather general class of models. ?Some might use the name
> "Empirical Bayes".
>
> I read the phrase "some of the parameters are random variables" to be
> referring to the random effects. ?I phrase things slightly
> differently. ?In particular I don't regard the random effects as
> parameters. I regard a mixed-effects model as being based on two
> random variables: the response Y whose value, y, has been observed and
> an unobserved random effects vector B. ?The probability model
> describes the conditional distribution of Y, given B = b, and the
> unconditional distribution of B. ?For the models that can be fit by
> the lme4 package the unconditional distribution of B is a multivariate
> Gaussian with mean zero and a parameterized variance-covariance
> matrix. ?The conditional mean of Y, given B = b, depends on b through
> a linear predictor expression, X\beta + Zb, that also incorporates the
> fixed-effects parameters \beta.
>
> The random effects B are similar to the fixed-effects \beta in that
> they both occur as coefficients in the linear predictor but I find
> that, for me at least, it is important to retain a distinction between
> the random effects and the model parameters.
>
>> So say the likelihood looks like
>>
>> ? f(x,u)p(u)
>>
>> where the vector of parameters u have a prior p(u).
>> If the u are normally distributed with a vector of
>> std devs sigma this becomes
>>
>> ? ?f(x,u)p(u,sigma)
>>
>> Integrating over u gives
>>
>> ?F(x,sigma) = \Int F(x,u)p(u,sigma) du
>>
>> so the MLE's xhat,sigamhat are found by maximizing
>> F wrt x and sigma.
>
> Well, that was kind of the point of the discussion. ?For a linear
> mixed model there are many equivalent ways to arrive at the REML
> criterion. ?One way that provides a convenient computational approach
> is to maximize, with respect to the random-effects dispersion
> parameters, the integral of the likelihood over the fixed-effects
> parameters. ?The original motivation for the criterion is as the
> maximum likelihood estimator of the variance parameters based on a set
> of contrasts that are orthogonal to the fixed-effects model space.
> That is, these are the maximum likelihood estimators from a set of
> linear combinations of data values that are to a certain subspace, the

that should read "... that are restricted to a certain ..."

> subspace of all possible residuals. ?This is why they are called the
> restricted or residual maximum likelihood estimators.
>
> I first saw the relationship between maximizing the integrated
> likelihood and the REML estimators in the 1982 Biometrics paper
> "Random Effects Models for Longitudinal Data" by Laird and Ware but it
> may have been known before that.
>
> It is certainly possible to define estimators that maximize the
> integrated likelihood in more general mixed-effects model
> specifications but should we call these REML estimators and do we know
> that they will share the properties that make REML a popular
> estimation criterion for linear mixed models?
>
> I oversimplified when I said that REML is not defined for GLMMs. ?I
> was using REML to refer to the estimators based on orthogonal
> subspaces in the sample space.
>
>> Now if we are willing to take a Bayesian point of view
>
> I thought you were already there when you wrote about "parameters are
> random variables with a prior" :-)
>
>> we could also integrate over x and get the marginal
>> likelihood for the sigma alone. We may need a prior on x,
>> or assume a locally uniform prior. In any event we obtain
>>
>> ?G(sigma) = \Int F(x,u)p(u,sigma) du dx
>>
>> The integral can be approximated using the Laplace approximation
>> at the mode of ?F(x,u)p(u,sigma). ?Now I'm not claiming that this
>> approach always works well, but it is a simple recipe for producing
>> an estimation procedure which may give better estimates for sigma.
>> Using AD Model Builders Random effects module which is freely available
>> at http://admb-project.org it is simple to
>> change the model specification to carry out this procedure. As I said
>> before it seemed to work well in a nonlinear fisheries model I looked
>> at.
>>
>> --
>> David A. Fournier
>> P.O. Box 2040,
>> Sidney, B.C. V8l 3S3
>> Canada
>> Phone/FAX 250-655-3364
>> http://otter-rsch.com
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>



From otter at otter-rsch.com  Sun Mar 29 00:03:17 2009
From: otter at otter-rsch.com (dave fournier)
Date: Sat, 28 Mar 2009 15:03:17 -0800
Subject: [R-sig-ME] lmer: ML and REML estimation
In-Reply-To: <40e66e0b0903280749i3d3cc6fay5631c3a55f016a86@mail.gmail.com>
References: <40e66e0b0903280749i3d3cc6fay5631c3a55f016a86@mail.gmail.com>
Message-ID: <49CEACB5.8000205@otter-rsch.com>

The original question was:

 "I wondered whether non-linear likelihoods, like those of
the binomial models that I have used, can be estimated with REML
methods or not." which I take to mean "Is there anything analogous to
REML methods for linear mixed models which can be used in  nonlinear
mixed models or for glmm's at the very least)?"

The response the OP got was more or less, no there are not, which I
think is incorrect. The correct answer is yes there are, but with the
caveat that perhaps nobody has investigated whether they have good
properties. (I wonder why not?)

-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com



From lucianolasala at yahoo.com.ar  Sun Mar 29 20:47:04 2009
From: lucianolasala at yahoo.com.ar (Luciano La Sala)
Date: Sun, 29 Mar 2009 11:47:04 -0700 (PDT)
Subject: [R-sig-ME] Can interaction term cause Estimates and Std. Errors to
	be too large?
Message-ID: <569608.4594.qm@web59905.mail.ac4.yahoo.com>


Dear R-experts,

I am running version 2.7.1 on Windows Vista. I have small dataset which consists of:

# NestID: nest indicator for each chicken. Siblings sharing the same nest have the same nest indicator.

# Chick: chick indicator consisting of a unique ID for each single chick.

# Year: 2006, 2007. 

# ClutchSize: 1-, 2- , 3-eggs.

# HO: hatching order within each clutch (1, 2, 3 [first, second and third-hatched chick]).

In order to account for lack of independence at the nest level (many 
chicks are nested in nest...), I'd like to run a GLMM with random slopes and intercepts for nests.

My approach to model building was as follows: Variables that had P ? 0.20 on their own in an initial bivariate analysis were forced into the multivariable analysis. The general procedure for model selection involved starting from a maximum model based on the bivariate analyses and eliminating terms to achieve a simpler model that only retained the significant main effects and two-way interactions. The model was restricted by stepwise manual elimination of variables using the Akaike Information Criterion (AIC) as a measure of goodness-of-fit. 
Interactions were tested only between main effects which remained in the final model. 

My final model for hatching failure (without testing of interaction between main effects) is:

model <- lmer(Hatching ~ HatchOrder + Year + (1|NestID), family=binomial, 1)

I get the following output: 

best.model <- lmer(Hatching~HatchOrder+Year+(1|NestID), family=binomial, 1)

Generalized linear mixed model fit by the Laplace approximation 
Formula: Hatching2 ~ HatchingOrder + Year1 + (1 | NestID) 
 Data: 1 
 AIC      BIC         logLik      deviance
 167.8    185.3       -78.9        157.8

Random effects:
Groups Name              Variance     Std. Dev.
NestID (Intercept)       1.9682        1.4029  

Number of obs: 247, groups: NestID, 120

Fixed effects:
                                                                                        Estimate   Std. Error   z value   Pr(>|z|)    
(Intercept)  -5.4800    0.8329        -6.579   4.73e-11     ***
HO_Second    1.6344     0.6841         2.389   0.01689      *  
HO_Third     3.3007     0.7162         4.609   4.05e-06     ***
Year2006     2.1169     0.6741         3.140   0.00169      **

So far, so good? but then I fit the same model incorporating interaction between the main effects as follows: 

interaction <-lmer(Hatching~HatchOrder+Year+HatchingOrder*Year+(1|NestID), family=binomial,1)

And I get the following output:

Data: 1 
AIC       BIC       logLik      deviance
157.8     182.3     -71.89      143.8

Random effects:
Groups Name              Variance         Std. Dev.
NestID (Intercept)       155.22            12.459  

Number of obs: 247, groups: NestID, 120

Fixed effects:
                                Estimate Std. Error z value Pr(>|z|)   
(Intercept)                     -13.6158     4.8287 -2.8198  0.00481 **
HO_Second                       -23.1961 36249.1930 -0.0006  0.99949   
HO_Third                          5.6624     2.6823  2.1110  0.03477 * 
Year2006                         -0.9602     6.1245 -0.1568  0.87541   
HO_Second:Year2006               30.2249 36249.1931  0.0008  0.99933   
HO_Third:Year2006                10.5549     5.2232  2.0208  0.04331 * 
 

Correlation of Fixed Effects:
            (Intr) HtchOS HtchOT Y12006 HOS:Y1
HtchngOrdrS  0.000                            
HtchngOrdrT -0.384  0.000                     
Year12006   -0.788  0.000  0.303              
HtOS:Y12006  0.000 -1.000  0.000  0.000       
HtOT:Y12006  0.197  0.000 -0.514 -0.556  0.000

Question 1: I am worried about the overly large values of the Estimate and Std. Error for "HO_Second" and "HO_Second*Year2006" from the second model (with interaction term included).  
So what may me causing such large values? Should I be concerned? If so, how can I solve the problem? Is this an over-fitting problem? 

Question 2: The Estimate for "Year2006" becomes negative in the second model. Any clue as to why this happens? 

Question 3: Should I stick with the simpler model 1 which does not asses interaction? 
 
Thank you in advance for the help! 

Lucho 


      Yahoo! Cocina
Recetas pr?cticas y comida saludable
http://ar.mujer.yahoo.com/cocina/



From maj at stats.waikato.ac.nz  Sun Mar 29 22:44:57 2009
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 30 Mar 2009 09:44:57 +1300
Subject: [R-sig-ME] Parameters and unobserved random variables - was Re:
 lmer: ML and REML estimation
In-Reply-To: <40e66e0b0903280745v6f54ac3dp5891f7426090e5d4@mail.gmail.com>
References: <49CB4370.2030109@biostat.ku.dk> <49CD1468.5020506@otter-rsch.com>
	<40e66e0b0903280745v6f54ac3dp5891f7426090e5d4@mail.gmail.com>
Message-ID: <49CFDDC9.2020202@stats.waikato.ac.nz>

Perhaps a bit of a tangent so I have adjusted the subject line. About 10 
years ago I was visiting the late Professor Chris Wallace at Monash and 
getting into discussions about the relationship between the EM algorithm 
and his "minimum message length" approach to inference. Chris was 
adamant it treating what I thought of as "unobserved random variables" 
as "parameters". Now Chris was a Bayesian and so for him all parameters 
were random variables. It would seem that if you are a Bayesian that no 
consistent distinction can be made between parameters and unobserved 
random variables. Are their any Bayesians who attempt to make such a 
distinction and if so, how and why?

Murray

Douglas Bates wrote:
[...]
> I read the phrase "some of the parameters are random variables" to be
> referring to the random effects.  I phrase things slightly
> differently.  In particular I don't regard the random effects as
> parameters. I regard a mixed-effects model as being based on two
> random variables: the response Y whose value, y, has been observed and
> an unobserved random effects vector B.  
[...]
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441   Mobile 021 0200 8350



From r.turner at auckland.ac.nz  Sun Mar 29 23:10:25 2009
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 30 Mar 2009 10:10:25 +1300
Subject: [R-sig-ME] Parameters and unobserved random variables - was Re:
	lmer: ML and REML estimation
In-Reply-To: <49CFDDC9.2020202@stats.waikato.ac.nz>
References: <49CB4370.2030109@biostat.ku.dk> <49CD1468.5020506@otter-rsch.com>
	<40e66e0b0903280745v6f54ac3dp5891f7426090e5d4@mail.gmail.com>
	<49CFDDC9.2020202@stats.waikato.ac.nz>
Message-ID: <9EA48FFB-A73E-4AFD-B4D0-978F0D2FF8D9@auckland.ac.nz>


On 30/03/2009, at 9:44 AM, Murray Jorgensen wrote:

> Perhaps a bit of a tangent so I have adjusted the subject line.  
> About 10
> years ago I was visiting the late Professor Chris Wallace at Monash  
> and
> getting into discussions about the relationship between the EM  
> algorithm
> and his "minimum message length" approach to inference. Chris was
> adamant it treating what I thought of as "unobserved random variables"
> as "parameters". Now Chris was a Bayesian and so for him all  
> parameters
> were random variables. It would seem that if you are a Bayesian  
> that no
> consistent distinction can be made between parameters and unobserved
> random variables. Are their any Bayesians who attempt to make such a
> distinction and if so, how and why?

Point of order, mister chairman.  The EM algorithm is just that:  an
algorithm.  (Or rather, it is a technique for *constructing* algorithms,
but that's another story.)  It is a technique for maximizing the  
likelihood
of a model and set of data where a part of the data is missing.  So the
issue is the relationship between ``minimum message length'' and  
``maximum
likelihood in the presence of missing data''.  The EM algorithm is  
just one
approach to maximizing such a likelihood; there are (in some contexts  
at least)
others and it doesn't really matter which technique you use.  The  
model is
the same; the way you get a numerical fit to the model doesn't matter  
in any
fundamental way.

I know I'm being picky/pedantic/dogmatic and so on, but in  
discussions of topics
like this if any fuzziness is left lying around confusion can easily  
set in and
people can very easily wind up talking at cross-purposes.

Sorry for wasting band-width if this is all toadally obvious to  
everyone else.

	cheers,

		Rolf Turner



######################################################################
Attention:\ This e-mail message is privileged and confid...{{dropped:9}}



From maj at stats.waikato.ac.nz  Sun Mar 29 23:25:37 2009
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 30 Mar 2009 10:25:37 +1300
Subject: [R-sig-ME] Parameters and unobserved random variables - was Re:
 lmer: ML and REML estimation
In-Reply-To: <9EA48FFB-A73E-4AFD-B4D0-978F0D2FF8D9@auckland.ac.nz>
References: <49CB4370.2030109@biostat.ku.dk> <49CD1468.5020506@otter-rsch.com>
	<40e66e0b0903280745v6f54ac3dp5891f7426090e5d4@mail.gmail.com>
	<49CFDDC9.2020202@stats.waikato.ac.nz>
	<9EA48FFB-A73E-4AFD-B4D0-978F0D2FF8D9@auckland.ac.nz>
Message-ID: <49CFE751.1030607@stats.waikato.ac.nz>

Rolf, I didn't ask any question about the EM algorithm, or about MML for 
that matter, I was just indicating the context in which my query first 
arose. To avoid any tangents to my tangent and to suppress any fuzziness 
which Rolf and I both deplore I will repeat the question:

_Are their any Bayesians who attempt to make a distinction between 
parameters and unobserved random variables and if so, how and why?_

(Possibly the wrong group to address such a question to, though I could 
attempt to defend my choice of group if required!)

Murray

Rolf Turner wrote:
> 
> On 30/03/2009, at 9:44 AM, Murray Jorgensen wrote:
> 
>> Perhaps a bit of a tangent so I have adjusted the subject line. About 10
>> years ago I was visiting the late Professor Chris Wallace at Monash and
>> getting into discussions about the relationship between the EM algorithm
>> and his "minimum message length" approach to inference. Chris was
>> adamant it treating what I thought of as "unobserved random variables"
>> as "parameters". Now Chris was a Bayesian and so for him all parameters
>> were random variables. It would seem that if you are a Bayesian that no
>> consistent distinction can be made between parameters and unobserved
>> random variables. Are their any Bayesians who attempt to make such a
>> distinction and if so, how and why?
> 
> Point of order, mister chairman.  The EM algorithm is just that:  an
> algorithm.  (Or rather, it is a technique for *constructing* algorithms,
> but that's another story.)  It is a technique for maximizing the likelihood
> of a model and set of data where a part of the data is missing.  So the
> issue is the relationship between ``minimum message length'' and ``maximum
> likelihood in the presence of missing data''.  The EM algorithm is just one
> approach to maximizing such a likelihood; there are (in some contexts at 
> least)
> others and it doesn't really matter which technique you use.  The model is
> the same; the way you get a numerical fit to the model doesn't matter in 
> any
> fundamental way.
> 
> I know I'm being picky/pedantic/dogmatic and so on, but in discussions 
> of topics
> like this if any fuzziness is left lying around confusion can easily set 
> in and
> people can very easily wind up talking at cross-purposes.
> 
> Sorry for wasting band-width if this is all toadally obvious to everyone 
> else.
> 
>     cheers,
> 
>         Rolf Turner
> 
> 
> 
> ######################################################################
> Attention:This e-mail message is privileged and confidential. If you are 
> not theintended recipient please delete the message and notify the 
> sender.Any views or opinions presented are solely those of the author.
> 
> This e-mail has been scanned and cleared by 
> MailMarshalwww.marshalsoftware.com
> ######################################################################

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441   Mobile 021 0200 8350



From ken at kjbeath.com.au  Mon Mar 30 00:51:59 2009
From: ken at kjbeath.com.au (ken at kjbeath.com.au)
Date: Sun, 29 Mar 2009 22:51:59 -0000 (GMT)
Subject: [R-sig-ME] Can interaction term cause Estimates and Std. Errors
 to be too large?
In-Reply-To: <569608.4594.qm@web59905.mail.ac4.yahoo.com>
References: <569608.4594.qm@web59905.mail.ac4.yahoo.com>
Message-ID: <2062.137.111.57.71.1238367119.squirrel@65.99.229.10>

This is the result of what is known as complete separation, where the
model perfectly predicts the outcome. An aspect of this is the probably
incorrect lack of significance, known as the Hauck-Donner effect. Usually
it is a result of overfitting.

Ken

>
> Dear R-experts,
>
> I am running version 2.7.1 on Windows Vista. I have small dataset which
> consists of:
>
> # NestID: nest indicator for each chicken. Siblings sharing the same nest
> have the same nest indicator.
>
> # Chick: chick indicator consisting of a unique ID for each single chick.
>
> # Year: 2006, 2007.
>
> # ClutchSize: 1-, 2- , 3-eggs.
>
> # HO: hatching order within each clutch (1, 2, 3 [first, second and
> third-hatched chick]).
>
> In order to account for lack of independence at the nest level (many
> chicks are nested in nest...), I'd like to run a GLMM with random slopes
> and intercepts for nests.
>
> My approach to model building was as follows: Variables that had P ???
> 0.20 on their own in an initial bivariate analysis were forced into the
> multivariable analysis. The general procedure for model selection involved
> starting from a maximum model based on the bivariate analyses and
> eliminating terms to achieve a simpler model that only retained the
> significant main effects and two-way interactions. The model was
> restricted by stepwise manual elimination of variables using the Akaike
> Information Criterion (AIC) as a measure of goodness-of-fit.
> Interactions were tested only between main effects which remained in the
> final model.
>
> My final model for hatching failure (without testing of interaction
> between main effects) is:
>
> model <- lmer(Hatching ~ HatchOrder + Year + (1|NestID), family=binomial,
> 1)
>
> I get the following output:
>
> best.model <- lmer(Hatching~HatchOrder+Year+(1|NestID), family=binomial,
> 1)
>
> Generalized linear mixed model fit by the Laplace approximation
> Formula: Hatching2 ~ HatchingOrder + Year1 + (1 | NestID)
>  Data: 1
>  AIC      BIC         logLik      deviance
>  167.8    185.3       -78.9        157.8
>
> Random effects:
> Groups Name              Variance     Std. Dev.
> NestID (Intercept)       1.9682        1.4029
>
> Number of obs: 247, groups: NestID, 120
>
> Fixed effects:
>                                                                                         Estimate
>
>
> Std.
> Error
>
>
> z
> value
>
>
> Pr(>|z|)
> (Intercept)  -5.4800    0.8329        -6.579   4.73e-11     ***
> HO_Second    1.6344     0.6841         2.389   0.01689      *
> HO_Third     3.3007     0.7162         4.609   4.05e-06     ***
> Year2006     2.1169     0.6741         3.140   0.00169      **
>
> So far, so good??? but then I fit the same model incorporating interaction
> between the main effects as follows:
>
> interaction <-lmer(Hatching~HatchOrder+Year+HatchingOrder*Year+(1|NestID),
> family=binomial,1)
>
> And I get the following output:
>
> Data: 1
> AIC       BIC       logLik      deviance
> 157.8     182.3     -71.89      143.8
>
> Random effects:
> Groups Name              Variance         Std. Dev.
> NestID (Intercept)       155.22            12.459
>
> Number of obs: 247, groups: NestID, 120
>
> Fixed effects:
>                                 Estimate Std. Error z value Pr(>|z|)
> (Intercept)                     -13.6158     4.8287 -2.8198  0.00481 **
> HO_Second                       -23.1961 36249.1930 -0.0006  0.99949
> HO_Third                          5.6624     2.6823  2.1110  0.03477 *
> Year2006                         -0.9602     6.1245 -0.1568  0.87541
> HO_Second:Year2006               30.2249 36249.1931  0.0008  0.99933
> HO_Third:Year2006                10.5549     5.2232  2.0208  0.04331 *
>
>
> Correlation of Fixed Effects:
>             (Intr) HtchOS HtchOT Y12006 HOS:Y1
> HtchngOrdrS  0.000
> HtchngOrdrT -0.384  0.000
> Year12006   -0.788  0.000  0.303
> HtOS:Y12006  0.000 -1.000  0.000  0.000
> HtOT:Y12006  0.197  0.000 -0.514 -0.556  0.000
>
> Question 1: I am worried about the overly large values of the Estimate and
> Std. Error for "HO_Second" and "HO_Second*Year2006" from the second model
> (with interaction term included).
> So what may me causing such large values? Should I be concerned? If so,
> how can I solve the problem? Is this an over-fitting problem?
>
> Question 2: The Estimate for "Year2006" becomes negative in the second
> model. Any clue as to why this happens?
>
> Question 3: Should I stick with the simpler model 1 which does not asses
> interaction?
>
> Thank you in advance for the help!
>
> Lucho
>
>
>       Yahoo! Cocina
> Recetas pr??cticas y comida saludable
> http://ar.mujer.yahoo.com/cocina/
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From ral at lcfltd.com  Mon Mar 30 03:26:06 2009
From: ral at lcfltd.com (Robert A LaBudde)
Date: Sun, 29 Mar 2009 20:26:06 -0500
Subject: [R-sig-ME] Parameters and unobserved random variables - was Re:
 lmer: ML and REML estimation
In-Reply-To: <49CFE751.1030607@stats.waikato.ac.nz>
References: <49CB4370.2030109@biostat.ku.dk> <49CD1468.5020506@otter-rsch.com>
	<40e66e0b0903280745v6f54ac3dp5891f7426090e5d4@mail.gmail.com>
	<49CFDDC9.2020202@stats.waikato.ac.nz>
	<9EA48FFB-A73E-4AFD-B4D0-978F0D2FF8D9@auckland.ac.nz>
	<49CFE751.1030607@stats.waikato.ac.nz>
Message-ID: <0KHA0021EMJJ0ZH5@vms173003.mailsrvcs.net>

At 04:25 PM 3/29/2009, Murray Jorgensen wrote:
>Rolf, I didn't ask any question about the EM algorithm, or about MML 
>for that matter, I was just indicating the context in which my query 
>first arose. To avoid any tangents to my tangent and to suppress any 
>fuzziness which Rolf and I both deplore I will repeat the question:
>
>_Are their any Bayesians who attempt to make a distinction between 
>parameters and unobserved random variables and if so, how and why?_
>
>(Possibly the wrong group to address such a question to, though I 
>could attempt to defend my choice of group if required!)

I believe the correct question to ask should be "Is there any 
distinction between parameters and latent variables in the Bayesian 
viewpoint?".

Offhand, I don't see any substantiative distinction from a 
methodology point of view. But, under some forseeable circumstances 
in some experiment, latent variables might become manifest, but 
parameters never will. Variables might be latent in one experiment, 
where they are not measured, and manifest in others, where they are.

(Reminds me of Erwin Schroedinger's cat.)

================================================================
Robert A. LaBudde, PhD, PAS, Dpl. ACAFS  e-mail: ral at lcfltd.com
Least Cost Formulations, Ltd.            URL: http://lcfltd.com/
824 Timberlake Drive                     Tel: 757-467-0954
Virginia Beach, VA 23464-3239            Fax: 757-467-2947

"Vere scire est per causas scire"



From mick.wu at mail.mcgill.ca  Mon Mar 30 03:38:32 2009
From: mick.wu at mail.mcgill.ca (Gi-Mick Wu)
Date: Sun, 29 Mar 2009 21:38:32 -0400
Subject: [R-sig-ME] Correct dispersion parameter for glmmML?
In-Reply-To: <461AF912-9BFB-456A-9DDF-7C1B275F0695@kjbeath.com.au>
References: <B89AE5DF4F2A964FB322E3F53F02EE0D2301D09490@EXMBXVS4A.campus.mcgill.ca>,
	<461AF912-9BFB-456A-9DDF-7C1B275F0695@kjbeath.com.au>
Message-ID: <B89AE5DF4F2A964FB322E3F53F02EE0D2301D0949F@EXMBXVS4A.campus.mcgill.ca>

Thanks for the reply,

That was very useful. I removed one covariate, body size (and related interaction terms) to reduce/avoid overfitting. I kept the others because they are factors that I am really interested in. Here's the new model, which is much better I believe (no more really large coefficients):

################
> modl.glmmML.miss

Call:  glmmML(formula = form, family = binomial, data = clean.dat.miss,      cluster = clean.dat.miss[, "ID"], prior = "gaussian", method = "ghq",      n.points = 8) 

                         coef se(coef)       z Pr(>|z|)
(Intercept)          0.535786 0.791211  0.6772   0.4980
Time                -0.007409 0.012067 -0.6140   0.5390
Exposure            -0.145812 0.283806 -0.5138   0.6070
Ordr                -0.292222 0.956798 -0.3054   0.7600
Time:Exposure        0.001098 0.004114  0.2668   0.7900
Time:Ordr            0.020255 0.009289  2.1805   0.0292
Exposure:Ordr       -0.135699 0.330928 -0.4101   0.6820

Scale parameter in mixing distribution:  0.568 gaussian 
Std. Error:                              0.2985 

Residual deviance: 240.3 on 172 degrees of freedom      AIC: 256.3
##############

As for the scale parameter, I am interested in it, because I would like to choose the appropriate test for null hypothesis testing (required in my field) according to Bolker et al.'s (2009) review in "Trends in Ecology & Evolution" (doi:10.1016/j.tree.2008.10.008). If I am reading it correctly (else please correct me), I should be using Wald Z or chi-square for models without overdispersion and Wald t or F for overdispersion when testing for fixed effects.

I was trying to calculate the correct dispersion parameter; was it already given to me as the "Scale parameter in mixing distribution"? (that would be too easy)

All the best,
Mick

________________________________________
From: Ken Beath [ken at kjbeath.com.au]
Sent: March 26, 2009 6:10 AM
To: Gi-Mick Wu
Cc: r-sig-mixed-models at r-project.org Models
Subject: Re: [R-sig-ME] Correct dispersion parameter for glmmML?

In your model fitting you have the intercept estimate large and with
large standard error and similarly for Ordr and maybe Exposure. This
is usually an indication of a problem with the fitting of binomial
data, probably separation due to overfitting. I would start with a
simple model and try and work out which terms are causing problems.

I prefer not to think about overdispersed GLMM, they seem a bit odd
statistically, maybe a GEE based approach is better.

Ken

On 25/03/2009, at 7:07 AM, Gi-Mick Wu wrote:

> Dear mixed model users, (my first post, after reading the archives a
> lot)
>
> This post is about mixed models, but not lmer or lme4; please let me
> know if it is inappropriate. (I have searched the archives for hours
> without finding an answer to my question. I have also searched and
> posted in "R-sig-eco" without results).
> Thank you in advance for reading!
>
> I?m fitting a generalized linear mixed model to binary data using
> glmmML (version 0.81-4), but am unsure of how to obtain the correct
> dispersion parameter. (I have also just updated R to version 2.8.1)
>
> For a glm model, I read that it should be the SS Pearson's
> residuals / df, rather than the default residuals (deviance). In my
> case:
>
> # GLM
> # using Pearson's residuals:
>> sum(residuals(modl.glm,type="pearson")^2)/modl.glm$df.residual
> [1] 1.062947
> # using deviance
>> modl.glm$deviance/modl.glm$df.residual
> [1] 1.409863
>
> For glmmML however, I cannot obtain the pearson residuals, but only
> the deviance and null deviance (same as deviance for glm):
>
> # GLMM
> # using Pearson's residuals
>> residuals(modl.glmmML)
> NULL
> # using deviance
>> modl.glmmML$deviance/modl.glmmML$df.residual
> [1] 1.413364
> # using null deviance
>> modl.glmmML$cluster.null.deviance/modl.glmmML$cluster.null.df
> [1] 1.409863
>
> I am confused as to which (if any) of the dispersion parameter is
> valid for the glmm model. Any one have an idea?
>
> Thanks in advance for reading and hopefully for some much needed
> answers or pointers.
> Mick
> PS In case it is relevant or for curiosity's sake, here's the
> experimental design:
>
> The experiment consists in testing the effect of experience on odour
> preferences of parasitoid wasps. Parasitoids lay eggs in host
> insects, which will be devoured from the inside until they die; like
> the movie Alien :-)
>
> Wasps were exposed to two different odours, with or without the
> presence of hosts (call them rewarded / unrewarded odour). Their
> odour preference was then tested in a Y-tube olfactometer 5 times
> after exposure (2h, 6h, 26h, 50h, 98h). 36 individuals were exposed
> to each odour either 1,2,3, or 4 times (9 individuals/treatment
> level). Odours were presented in alternation with half the wasps
> starting with the rewarded odour.
> (Total number of binary choices = 180). I set Time and NbExposure as
> fixed effects because I'm really interested in their effect and wasp
> ID as random. I also added body size, which seem to play a role on
> the effect of experience in honeybees, but it is secondary, so it
> could be removed (to avoid overfitting).
>
> # R code for fitting the glmm
> form <- cbind(RewardOdour, UnrewardOdour) ~ (Time + NbExposure +
> Ordr + BodySize)^2
> # interaction terms limited to second order for parsimony
> modl.glmmML <- glmmML(form, family=binomial,
> data=clean.dat,cluster=clean.dat[,"ID"],
>                       prior="gaussian", method="ghq", n.points=8)           # same
> results with n.points = 20
>
>> modl.glmmML
>
> Call:  glmmML(formula = form, family = binomial, data = clean.dat,
> cluster = clean.dat[, "ID"], prior = "gaussian", method = "ghq",
> n.points = 8)
>
>                                     coef            se(coef)
> z           Pr(>|z|)
> (Intercept)             -7.539e+00  7.5906651  -0.99324   0.3210
> Time                        -6.303e-03   0.0648177  -0.09724   0.9230
> Exposure                 2.843e+00  2.3465958   1.21139   0.2260
> Ordr                         4.366e+00  5.1119166   0.85401   0.3930
> BodySize                  1.324e-02   0.0126869   1.04333   0.2970
> Time:Exposure         1.295e-03   0.0042982   0.30123   0.7630
> Time:Ordr                 2.042e-02   0.0093118   2.19259   0.0283
> Time:BodySize         -2.554e-06  0.0001093  -0.02337   0.9810
> Exposure:Ordr        -2.333e-01  0.3489015  -0.66859   0.5040
> Exposure:BodySize  -4.833e-03   0.0037688  -1.28235   0.2000
> Ordr:BodySize          -7.487e-03   0.0086466  -0.86585   0.3870
>
> Scale parameter in mixing distribution:  0.484 gaussian
> Std. Error:                                       0.3204
>
> Residual deviance: 237.4 on 168 degrees of freedom      AIC: 261.4
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From j.hadfield at ed.ac.uk  Mon Mar 30 10:48:53 2009
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 30 Mar 2009 09:48:53 +0100
Subject: [R-sig-ME] Can interaction term cause Estimates and Std. Errors
	to be too large?
Message-ID: <7BB0058A-ABDB-4572-93E4-A8702341FD79@ed.ac.uk>

Hi,

I think it unlikely that the problem arises through overfitting in the  
sense that there are too many parameters for the amount of  data.   
It's more likely that the underlying probabilities really are extreme  
for some categories causing what are also known as "extreme category  
problems" (eg Miztal 1998 J. Dairy Science 72 1557-1568): the binary  
variable in one or more groups is always 0 or 1, even though there are  
probably many eggs  in most categories.  A solution to this type of  
problem is to place an informative prior on the fixed effects to stop  
them wandering into extreme values on the logit scale. For the purist  
this may be anathema, but as a practical solution it seems to work  
quite well.  Having a normal prior on the logit scale with mean zero  
and variance pi, is the closest (I think?) to a uniform prior on the  
probability scale. If there are more elegant solutions to the problem  
I'd be interested to hear about them.

Cheers,

Jarrod



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.



From ken at kjbeath.com.au  Mon Mar 30 12:08:58 2009
From: ken at kjbeath.com.au (Ken Beath)
Date: Mon, 30 Mar 2009 21:08:58 +1100
Subject: [R-sig-ME] Can interaction term cause Estimates and Std. Errors
	to be too large?
In-Reply-To: <7BB0058A-ABDB-4572-93E4-A8702341FD79@ed.ac.uk>
References: <7BB0058A-ABDB-4572-93E4-A8702341FD79@ed.ac.uk>
Message-ID: <68347D8C-AE1C-48BF-B194-D7E1DDD50B50@kjbeath.com.au>

I meant overfitting in the sense of trying to fit too complex a model,  
which is the same as what you are describing. Gelman has some papers  
on the use of priors, one is http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aoas/1231424214 
  In the case of complete separation the results seem to be very  
dependent on the prior which doesn't look to be a good thing. It would  
appear much better to admit that there is insufficient data to perform  
the analysis.

Ken


On 30/03/2009, at 7:48 PM, Jarrod Hadfield wrote:

> Hi,
>
> I think it unlikely that the problem arises through overfitting in  
> the sense that there are too many parameters for the amount of   
> data.  It's more likely that the underlying probabilities really are  
> extreme for some categories causing what are also known as "extreme  
> category problems" (eg Miztal 1998 J. Dairy Science 72 1557-1568):  
> the binary variable in one or more groups is always 0 or 1, even  
> though there are probably many eggs  in most categories.  A solution  
> to this type of problem is to place an informative prior on the  
> fixed effects to stop them wandering into extreme values on the  
> logit scale. For the purist this may be anathema, but as a practical  
> solution it seems to work quite well.  Having a normal prior on the  
> logit scale with mean zero and variance pi, is the closest (I  
> think?) to a uniform prior on the probability scale. If there are  
> more elegant solutions to the problem I'd be interested to hear  
> about them.
>
> Cheers,
>
> Jarrod
>
>
>
> -- 
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>



From j.hadfield at ed.ac.uk  Mon Mar 30 12:21:39 2009
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 30 Mar 2009 11:21:39 +0100
Subject: [R-sig-ME] Can interaction term cause Estimates and Std. Errors
	to be too large?
In-Reply-To: <68347D8C-AE1C-48BF-B194-D7E1DDD50B50@kjbeath.com.au>
References: <7BB0058A-ABDB-4572-93E4-A8702341FD79@ed.ac.uk>
	<68347D8C-AE1C-48BF-B194-D7E1DDD50B50@kjbeath.com.au>
Message-ID: <AC5DBE6D-DEEF-47A2-90BE-071FE9CBCC79@ed.ac.uk>

Hi Ken,

Thanks for the reference, it looks interesting. I disagree that  
Luciano's second model should be classified as over fitting. Imagine  
this....

y<-rbinom(100, 1, c(0.001, 0.999))
x<-gl(2,1,100)

summary(glm(y~1, family="binomial"))
summary(glm(y~x, family="binomial"))

There is a very high probability of complete separation, the second  
model gives non-significant p-values for the effect of x, but I think  
it would be a mistake to say the 2nd model is over-fitted and should  
be avoided.

Cheers,

Jarrod


On 30 Mar 2009, at 11:08, Ken Beath wrote:

> I meant overfitting in the sense of trying to fit too complex a  
> model, which is the same as what you are describing. Gelman has some  
> papers on the use of priors, one is http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aoas/1231424214 
>  In the case of complete separation the results seem to be very  
> dependent on the prior which doesn't look to be a good thing. It  
> would appear much better to admit that there is insufficient data to  
> perform the analysis.
>
> Ken
>
>
> On 30/03/2009, at 7:48 PM, Jarrod Hadfield wrote:
>
>> Hi,
>>
>> I think it unlikely that the problem arises through overfitting in  
>> the sense that there are too many parameters for the amount of   
>> data.  It's more likely that the underlying probabilities really  
>> are extreme for some categories causing what are also known as  
>> "extreme category problems" (eg Miztal 1998 J. Dairy Science 72  
>> 1557-1568): the binary variable in one or more groups is always 0  
>> or 1, even though there are probably many eggs  in most  
>> categories.  A solution to this type of problem is to place an  
>> informative prior on the fixed effects to stop them wandering into  
>> extreme values on the logit scale. For the purist this may be  
>> anathema, but as a practical solution it seems to work quite well.   
>> Having a normal prior on the logit scale with mean zero and  
>> variance pi, is the closest (I think?) to a uniform prior on the  
>> probability scale. If there are more elegant solutions to the  
>> problem I'd be interested to hear about them.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>> -- 
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>
>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20090330/df46ef59/attachment.pl>

From ken at kjbeath.com.au  Mon Mar 30 12:50:53 2009
From: ken at kjbeath.com.au (Ken Beath)
Date: Mon, 30 Mar 2009 21:50:53 +1100
Subject: [R-sig-ME] Can interaction term cause Estimates and Std. Errors
	to be too large?
In-Reply-To: <AC5DBE6D-DEEF-47A2-90BE-071FE9CBCC79@ed.ac.uk>
References: <7BB0058A-ABDB-4572-93E4-A8702341FD79@ed.ac.uk>
	<68347D8C-AE1C-48BF-B194-D7E1DDD50B50@kjbeath.com.au>
	<AC5DBE6D-DEEF-47A2-90BE-071FE9CBCC79@ed.ac.uk>
Message-ID: <AF2C8F6D-8857-4855-BAED-8DD9B525212D@kjbeath.com.au>

On 30/03/2009, at 9:21 PM, Jarrod Hadfield wrote:

> Hi Ken,
>
> Thanks for the reference, it looks interesting. I disagree that  
> Luciano's second model should be classified as over fitting. Imagine  
> this....
>
> y<-rbinom(100, 1, c(0.001, 0.999))
> x<-gl(2,1,100)
>
> summary(glm(y~1, family="binomial"))
> summary(glm(y~x, family="binomial"))
>
> There is a very high probability of complete separation, the second  
> model gives non-significant p-values for the effect of x, but I  
> think it would be a mistake to say the 2nd model is over-fitted and  
> should be avoided.
>
> Cheers,
>
> Jarrod
>

My original posting said "usually" and obviously you can create data  
with perfect or almost perfect correlation over a large table, but in  
practice it commonly happens because there is a small table. One good  
reason for producing some descriptive tables before fitting.

Ken


>
> On 30 Mar 2009, at 11:08, Ken Beath wrote:
>
>> I meant overfitting in the sense of trying to fit too complex a  
>> model, which is the same as what you are describing. Gelman has  
>> some papers on the use of priors, one is http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aoas/1231424214 
>>  In the case of complete separation the results seem to be very  
>> dependent on the prior which doesn't look to be a good thing. It  
>> would appear much better to admit that there is insufficient data  
>> to perform the analysis.
>>
>> Ken
>>
>>
>> On 30/03/2009, at 7:48 PM, Jarrod Hadfield wrote:
>>
>>> Hi,
>>>
>>> I think it unlikely that the problem arises through overfitting in  
>>> the sense that there are too many parameters for the amount of   
>>> data.  It's more likely that the underlying probabilities really  
>>> are extreme for some categories causing what are also known as  
>>> "extreme category problems" (eg Miztal 1998 J. Dairy Science 72  
>>> 1557-1568): the binary variable in one or more groups is always 0  
>>> or 1, even though there are probably many eggs  in most  
>>> categories.  A solution to this type of problem is to place an  
>>> informative prior on the fixed effects to stop them wandering into  
>>> extreme values on the logit scale. For the purist this may be  
>>> anathema, but as a practical solution it seems to work quite  
>>> well.  Having a normal prior on the logit scale with mean zero and  
>>> variance pi, is the closest (I think?) to a uniform prior on the  
>>> probability scale. If there are more elegant solutions to the  
>>> problem I'd be interested to hear about them.
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>>
>>> -- 
>>> The University of Edinburgh is a charitable body, registered in
>>> Scotland, with registration number SC005336.
>>>
>>>
>>
>>
>
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.



From Y.K.Tu at leeds.ac.uk  Mon Mar 30 19:16:46 2009
From: Y.K.Tu at leeds.ac.uk (Yu-Kang Tu)
Date: Mon, 30 Mar 2009 18:16:46 +0100
Subject: [R-sig-ME] Meta-analysis using lmer
Message-ID: <7131EF1EF27893479833FCF59E7AAC440B3E818BE1@HERMES9.ds.leeds.ac.uk>


Hi,

I am trying to use lme and lmer to do random effects meta-analysis as described in Hox (2002) and UCLA website: http://www.ats.ucla.edu/stat/mlwin/examples/ma_hox/chapter8.htm

Basically, what I want to do is to constraint one residual error variance to be unity and use the inverse of standard errors as the covariate (weight) for this variance. And an additional random effects terms is used to estimate the between-study variation. I did take a look at the Pinheiro & Bates book on varFunc, but unfortunately, I cannot figure out how this can be done. Any suggestions/advices will be greatly appreciated. Many thanks.

Yu-Kang
--------------------------------------------
Dr Yu-Kang Tu
Senior Clinical Research Fellow
Division of Biostatistics, Centre for Epidemiology and Biostatistics
Leeds Institute of Genetics, Health and Therapeutics, and 
Department of Periodontology, Leeds Dental Institute
Room 8.01, Level 8, Worsley Building,
Clarendon Way
University of Leeds, LS2 9JT
Email: y.k.tu at leeds.ac.uk
Tel: +44 (0) 113 3431877
Fax: +44 (0) 113 3434877



From ccleland at optonline.net  Mon Mar 30 19:29:15 2009
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 30 Mar 2009 13:29:15 -0400
Subject: [R-sig-ME] Mixed Model for Travel Distance
Message-ID: <49D1016B.8050808@optonline.net>

Hello:
  I am attempting to model the distance that clients travel to a
treatment program.  There are 14385 clients nested in 83 treatment
programs (the grouping factor).  The raw data are in miles driven from
the client's residence to the treatment program.  A natural logarithm
transformation of miles driven works well to reduce the positive skew in
miles driven.  I fit a model with lme() that looks like this:

Linear mixed-effects model fit by REML
 Data: dist.df
       AIC      BIC    logLik
  38145.37 38319.54 -19049.68

Random effects:
 Formula: ~1 | PROGRAM
        (Intercept)  Residual
StdDev:   0.4268969 0.8988483

Fixed effects: log(DIST5DZ + 1) ~ QUAD + BEAL_TRI + log(RZIPAREA + 1) +
log(PZIPAREA + 1) + AGE.TRI + P3GEND + P5RACEX + EMPLD + P13REASN +
METHFST + URGE.DI + WDRAW.DI + RX_30 + P7HR30

                               Value  Std.Error    DF   t-value p-value
(Intercept)                1.2235603 0.13153231 14288   9.30236  0.0000
QUADSouthEast              0.2100666 0.13200891    76   1.59131  0.1157
QUADMidWest                0.2760390 0.15709516    76   1.75715  0.0829
QUADWest                  -0.1655914 0.15536003    76  -1.06586  0.2899
BEAL_TRI250K-1M           -0.0264939 0.11724713    76  -0.22597  0.8218
BEAL_TRI<250K             -0.0965256 0.16399464    76  -0.58859  0.5579
log(RZIPAREA + 1)          0.2965304 0.00757138 14288  39.16463  0.0000
log(PZIPAREA + 1)         -0.0042061 0.04413826    76  -0.09529  0.9243
AGE.TRI30-43              -0.0309444 0.01789442 14288  -1.72927  0.0838
AGE.TRI43-83              -0.1281177 0.02168648 14288  -5.90772  0.0000
P3GENDFemale              -0.0195289 0.01632703 14288  -1.19611  0.2317
P5RACEXLatino             -0.3527584 0.02904416 14288 -12.14559  0.0000
P5RACEXBlack              -0.5485861 0.03306146 14288 -16.59292  0.0000
P5RACEXOther              -0.1580669 0.04811350 14288  -3.28529  0.0010
EMPLDYes                   0.0098856 0.01650635 14288   0.59890  0.5493
P13REASNYes               -0.0095057 0.01650128 14288  -0.57606  0.5646
METHFSTYes                 0.0073478 0.01672948 14288   0.43921  0.6605
URGE.DI Strong-VeryStrong -0.0272769 0.02400068 14288  -1.13651  0.2558
WDRAW.DISevere-VerySevere  0.0012054 0.01810067 14288   0.06659  0.9469
RX_30Yes                   0.0934411 0.02165389 14288   4.31521  0.0000
P7HR30Yes                 -0.0408189 0.02256842 14288  -1.80867  0.0705

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max
-4.17921640 -0.49875991  0.08672984  0.59020542  4.54644432

Number of Observations: 14385
Number of Groups: 83

  I would like to summarize the fixed effects in terms of miles rather
than log(miles + 1).  How can that be done?  Are there common
generalized linear mixed models for miles driven that would avoid the
transformation and allow effects to be presented in miles?

thanks,

Chuck

-- 
Chuck Cleland, Ph.D.
NDRI, Inc. (www.ndri.org)
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894



From d.rizopoulos at erasmusmc.nl  Mon Mar 30 20:26:38 2009
From: d.rizopoulos at erasmusmc.nl (Dimitris Rizopoulos)
Date: Mon, 30 Mar 2009 20:26:38 +0200
Subject: [R-sig-ME] Mixed Model for Travel Distance
In-Reply-To: <49D1016B.8050808@optonline.net>
References: <49D1016B.8050808@optonline.net>
Message-ID: <49D10EDE.8050501@erasmusmc.nl>

well, if you're only interested in the fixed effects, then you can also 
use a Generalized Estimating Equations approach that does not make a 
parametric assumption for the distribution of your error terms, e.g., 
have a look at the 'geepack' package. Furthermore and in case it is 
relevant for your application, in GEE the estimated parameters will have 
a population interpretation, contrary to the GLMMs approach in which 
they will have a conditional on the random effects interpretation.


I hope it helps.

Best,
Dimitris


Chuck Cleland wrote:
> Hello:
>   I am attempting to model the distance that clients travel to a
> treatment program.  There are 14385 clients nested in 83 treatment
> programs (the grouping factor).  The raw data are in miles driven from
> the client's residence to the treatment program.  A natural logarithm
> transformation of miles driven works well to reduce the positive skew in
> miles driven.  I fit a model with lme() that looks like this:
> 
> Linear mixed-effects model fit by REML
>  Data: dist.df
>        AIC      BIC    logLik
>   38145.37 38319.54 -19049.68
> 
> Random effects:
>  Formula: ~1 | PROGRAM
>         (Intercept)  Residual
> StdDev:   0.4268969 0.8988483
> 
> Fixed effects: log(DIST5DZ + 1) ~ QUAD + BEAL_TRI + log(RZIPAREA + 1) +
> log(PZIPAREA + 1) + AGE.TRI + P3GEND + P5RACEX + EMPLD + P13REASN +
> METHFST + URGE.DI + WDRAW.DI + RX_30 + P7HR30
> 
>                                Value  Std.Error    DF   t-value p-value
> (Intercept)                1.2235603 0.13153231 14288   9.30236  0.0000
> QUADSouthEast              0.2100666 0.13200891    76   1.59131  0.1157
> QUADMidWest                0.2760390 0.15709516    76   1.75715  0.0829
> QUADWest                  -0.1655914 0.15536003    76  -1.06586  0.2899
> BEAL_TRI250K-1M           -0.0264939 0.11724713    76  -0.22597  0.8218
> BEAL_TRI<250K             -0.0965256 0.16399464    76  -0.58859  0.5579
> log(RZIPAREA + 1)          0.2965304 0.00757138 14288  39.16463  0.0000
> log(PZIPAREA + 1)         -0.0042061 0.04413826    76  -0.09529  0.9243
> AGE.TRI30-43              -0.0309444 0.01789442 14288  -1.72927  0.0838
> AGE.TRI43-83              -0.1281177 0.02168648 14288  -5.90772  0.0000
> P3GENDFemale              -0.0195289 0.01632703 14288  -1.19611  0.2317
> P5RACEXLatino             -0.3527584 0.02904416 14288 -12.14559  0.0000
> P5RACEXBlack              -0.5485861 0.03306146 14288 -16.59292  0.0000
> P5RACEXOther              -0.1580669 0.04811350 14288  -3.28529  0.0010
> EMPLDYes                   0.0098856 0.01650635 14288   0.59890  0.5493
> P13REASNYes               -0.0095057 0.01650128 14288  -0.57606  0.5646
> METHFSTYes                 0.0073478 0.01672948 14288   0.43921  0.6605
> URGE.DI Strong-VeryStrong -0.0272769 0.02400068 14288  -1.13651  0.2558
> WDRAW.DISevere-VerySevere  0.0012054 0.01810067 14288   0.06659  0.9469
> RX_30Yes                   0.0934411 0.02165389 14288   4.31521  0.0000
> P7HR30Yes                 -0.0408189 0.02256842 14288  -1.80867  0.0705
> 
> Standardized Within-Group Residuals:
>         Min          Q1         Med          Q3         Max
> -4.17921640 -0.49875991  0.08672984  0.59020542  4.54644432
> 
> Number of Observations: 14385
> Number of Groups: 83
> 
>   I would like to summarize the fixed effects in terms of miles rather
> than log(miles + 1).  How can that be done?  Are there common
> generalized linear mixed models for miles driven that would avoid the
> transformation and allow effects to be presented in miles?
> 
> thanks,
> 
> Chuck
> 

-- 
Dimitris Rizopoulos
Assistant Professor
Department of Biostatistics
Erasmus University Medical Center

Address: PO Box 2040, 3000 CA Rotterdam, the Netherlands
Tel: +31/(0)10/7043478
Fax: +31/(0)10/7043014



From Gregor.Gorjanc at bfro.uni-lj.si  Mon Mar 30 23:35:33 2009
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Mon, 30 Mar 2009 23:35:33 +0200
Subject: [R-sig-ME] AIC and BIC with ML and REML
Message-ID: <F189E18BBAA8B6479F618B9044CF4E9C87D65AC299@REGULUS.bfro.uni-lj.si>

Hi!

A time ago I forgot the details about the calculation about how AIC and BIC are calculated and I did
a search. All went fine that day, i.e., AIC = - 2 * l + 2 * p, where l is log-likelihood value and p is a number
of parameters in the model. Similar for BIC. However, I have read a paper today where they state that when
we fit a model using ML we should use

AIC = - 2 * l + 2 * (p + q),

while the following should be used in the case of REML

AIC = - 2 * l + 2 * q,

where l is as above, p is a number of free parameters in the fixed part of the model and q is the number of
variance components for the random part of the model. Is this correct? I checked the calculations
with lmer() and it seems that only the first approach is taken. Can anyone comment on this?

Thank you!

mu <- 10
a <- c(0, 3)
b <- rnorm(n=20, sd=3)

facA <- gl(n=2, k=100)
facB <- gl(n=20, k=10)

y <- rnorm(n=length(facA), mean=(mu + a[facA] + b[facB]), sd=3)

library(package="lme4")

lmer(y ~ facA + (1 | facB))
##  AIC  BIC logLik deviance REMLdev
## 1054 1067 -522.9     1050    1046
##
## (1054 - 2 * 522.9) / 2 = 4.1 --> 4 parameters, mu, a2, sigma^2_b, and sigma^2_e --> p = 2, q = 2

lmer(y ~ facA + (1 | facB), REML=FALSE)
##   AIC  BIC logLik deviance REMLdev
##  1058 1071 -524.9     1050    1046
##
## (1058 - 2 * 524.9) / 2 = 4.1  --> 4 parameters, mu, a2, sigma^2_b, and sigma^2_e --> p = 2, q = 2

Lep pozdrav / With regards,
    Gregor Gorjanc
----------------------------------------------------------------------
University of Ljubljana       PhD student
Biotechnical Faculty          www: http://gregor.gorjanc.googlepages.com
Department of Animal Science  blog: http://ggorjan.blogspot.com
Groblje 3                     mail: gregor.gorjanc <at> bfro.uni-lj.si
SI-1230 Domzale               fax: +386 (0)1 72 17 888
Slovenia, Europe              tel: +386 (0)1 72 17 861



From bates at stat.wisc.edu  Tue Mar 31 00:13:15 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 30 Mar 2009 17:13:15 -0500
Subject: [R-sig-ME] AIC and BIC with ML and REML
In-Reply-To: <F189E18BBAA8B6479F618B9044CF4E9C87D65AC299@REGULUS.bfro.uni-lj.si>
References: <F189E18BBAA8B6479F618B9044CF4E9C87D65AC299@REGULUS.bfro.uni-lj.si>
Message-ID: <40e66e0b0903301513g540485aepc19fb4adaaeac553@mail.gmail.com>

On Mon, Mar 30, 2009 at 4:35 PM, Gorjanc Gregor
<Gregor.Gorjanc at bfro.uni-lj.si> wrote:
> Hi!

> A time ago I forgot the details about the calculation about how AIC and BIC are calculated and I did
> a search. All went fine that day, i.e., AIC = - 2 * l + 2 * p, where l is log-likelihood value and p is a number
> of parameters in the model. Similar for BIC. However, I have read a paper today where they state that when
> we fit a model using ML we should use

> AIC = - 2 * l + 2 * (p + q),

> while the following should be used in the case of REML

> AIC = - 2 * l + 2 * q,

> where l is as above, p is a number of free parameters in the fixed part of the model and q is the number of
> variance components for the random part of the model. Is this correct? I checked the calculations
> with lmer() and it seems that only the first approach is taken. Can anyone comment on this?

In a way this question relates to the earlier discussion on whether to
regard the random effects as parameters or as unobserved random
variables.  The difference between -2 * l + 2 * p and -2 * l + 2 * (p
+ q) can be considered to be a question of how many parameters there
are in the model.  In particular, do the random effects count as
parameters?  I had an interesting discussion with Georges Monette
about this a few days ago and both of us feel the saying the random
effects don't affect the parameter count is underestimating the
complexity of the model but saying they should add q to the number of
parameters is overestimating the complexity.

I don't know a good answer to the question of how to "count" the
number of parameters in a mixed model (but I also don't feel that AIC
or BIC should be taken too seriously - these quantities are, at best,
a guide for model comparisons).


> mu <- 10
> a <- c(0, 3)
> b <- rnorm(n=20, sd=3)
>
> facA <- gl(n=2, k=100)
> facB <- gl(n=20, k=10)
>
> y <- rnorm(n=length(facA), mean=(mu + a[facA] + b[facB]), sd=3)
>
> library(package="lme4")
>
> lmer(y ~ facA + (1 | facB))
> ## ?AIC ?BIC logLik deviance REMLdev
> ## 1054 1067 -522.9 ? ? 1050 ? ?1046
> ##
> ## (1054 - 2 * 522.9) / 2 = 4.1 --> 4 parameters, mu, a2, sigma^2_b, and sigma^2_e --> p = 2, q = 2
>
> lmer(y ~ facA + (1 | facB), REML=FALSE)
> ## ? AIC ?BIC logLik deviance REMLdev
> ## ?1058 1071 -524.9 ? ? 1050 ? ?1046
> ##
> ## (1058 - 2 * 524.9) / 2 = 4.1 ?--> 4 parameters, mu, a2, sigma^2_b, and sigma^2_e --> p = 2, q = 2
>
> Lep pozdrav / With regards,
> ? ?Gregor Gorjanc
> ----------------------------------------------------------------------
> University of Ljubljana ? ? ? PhD student
> Biotechnical Faculty ? ? ? ? ?www: http://gregor.gorjanc.googlepages.com
> Department of Animal Science ?blog: http://ggorjan.blogspot.com
> Groblje 3 ? ? ? ? ? ? ? ? ? ? mail: gregor.gorjanc <at> bfro.uni-lj.si
> SI-1230 Domzale ? ? ? ? ? ? ? fax: +386 (0)1 72 17 888
> Slovenia, Europe ? ? ? ? ? ? ?tel: +386 (0)1 72 17 861
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From Gregor.Gorjanc at bfro.uni-lj.si  Tue Mar 31 00:30:24 2009
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Tue, 31 Mar 2009 00:30:24 +0200
Subject: [R-sig-ME] AIC and BIC with ML and REML
In-Reply-To: <40e66e0b0903301513g540485aepc19fb4adaaeac553@mail.gmail.com>
References: <F189E18BBAA8B6479F618B9044CF4E9C87D65AC299@REGULUS.bfro.uni-lj.si>,
	<40e66e0b0903301513g540485aepc19fb4adaaeac553@mail.gmail.com>
Message-ID: <F189E18BBAA8B6479F618B9044CF4E9C87D65AC29A@REGULUS.bfro.uni-lj.si>

> In a way this question relates to the earlier discussion on whether to
> regard the random effects as parameters or as unobserved random
> variables.  The difference between -2 * l + 2 * p and -2 * l + 2 * (p
> + q) can be considered to be a question of how many parameters there
> are in the model.  In particular, do the random effects count as
> parameters?  I had an interesting discussion with Georges Monette
> about this a few days ago and both of us feel the saying the random
> effects don't affect the parameter count is underestimating the
> complexity of the model but saying they should add q to the number of
> parameters is overestimating the complexity.

Thanks for this comment. I will take this as a yes to: "Are there are several
definitions of AIC and BIC lurking around?".

DIC "solves" the issue of effective number of parameters to some extent
though it also has its own problems

http://www.mrc-bsu.cam.ac.uk/bugs/winbugs/dicpage.shtml

> I don't know a good answer to the question of how to "count" the
> number of parameters in a mixed model (but I also don't feel that AIC
> or BIC should be taken too seriously - these quantities are, at best,
> a guide for model comparisons).

Sure.

gg


From d.rizopoulos at erasmusmc.nl  Tue Mar 31 09:21:04 2009
From: d.rizopoulos at erasmusmc.nl (Dimitris Rizopoulos)
Date: Tue, 31 Mar 2009 09:21:04 +0200
Subject: [R-sig-ME] AIC and BIC with ML and REML
In-Reply-To: <F189E18BBAA8B6479F618B9044CF4E9C87D65AC29A@REGULUS.bfro.uni-lj.si>
References: <F189E18BBAA8B6479F618B9044CF4E9C87D65AC299@REGULUS.bfro.uni-lj.si>,
	<40e66e0b0903301513g540485aepc19fb4adaaeac553@mail.gmail.com>
	<F189E18BBAA8B6479F618B9044CF4E9C87D65AC29A@REGULUS.bfro.uni-lj.si>
Message-ID: <49D1C460.8060605@erasmusmc.nl>



Gorjanc Gregor wrote:
>> In a way this question relates to the earlier discussion on whether to
>> regard the random effects as parameters or as unobserved random
>> variables.  The difference between -2 * l + 2 * p and -2 * l + 2 * (p
>> + q) can be considered to be a question of how many parameters there
>> are in the model.  In particular, do the random effects count as
>> parameters?  I had an interesting discussion with Georges Monette
>> about this a few days ago and both of us feel the saying the random
>> effects don't affect the parameter count is underestimating the
>> complexity of the model but saying they should add q to the number of
>> parameters is overestimating the complexity.
> 
> Thanks for this comment. I will take this as a yes to: "Are there are several
> definitions of AIC and BIC lurking around?".
> 
> DIC "solves" the issue of effective number of parameters to some extent
> though it also has its own problems
> 
> http://www.mrc-bsu.cam.ac.uk/bugs/winbugs/dicpage.shtml

there is also the conditional AIC of Vaida and Blanchard (Biometrika, 
2005, 351--370) that calculates the effective degrees of freedom for the 
random effects. Both these authors and the DIC authors distinguish 
between the focus of inference (or better the focus of prediction) in 
mixed models, i.e., either the marginal log-likelihood or the 
conditional on the random effects log-likelihood.

Best,
Dimitris


>> I don't know a good answer to the question of how to "count" the
>> number of parameters in a mixed model (but I also don't feel that AIC
>> or BIC should be taken too seriously - these quantities are, at best,
>> a guide for model comparisons).
> 
> Sure.
> 
> gg
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

-- 
Dimitris Rizopoulos
Assistant Professor
Department of Biostatistics
Erasmus University Medical Center

Address: PO Box 2040, 3000 CA Rotterdam, the Netherlands
Tel: +31/(0)10/7043478
Fax: +31/(0)10/7043014



From r.millar at auckland.ac.nz  Tue Mar 31 10:11:30 2009
From: r.millar at auckland.ac.nz (r.millar at auckland.ac.nz)
Date: Tue, 31 Mar 2009 21:11:30 +1300 (NZDT)
Subject: [R-sig-ME] Incorrect std errors from nlmer?
Message-ID: <1389.125.237.187.150.1238487090.squirrel@www.stat.auckland.ac.nz>

Hi All,

I just copied and paste the example nlmer code from the lmer help file
(see below), which fits the logistic to the orange tree data.
The point estimates of the fixed effects look correct, but the s.e.'s
are not (I've fitted the model using three other software). The s.e. of
Asym is way out (it should be about 15), as is the tree effect variance
(should be about 1000)??????

Regards,

Russell Millar
U. Auckland

> (nm1 <- nlmer(circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym|Tree,
+               Orange, start = c(Asym = 200, xmid = 725, scal = 350)))
Nonlinear mixed model fit by the Laplace approximation
Formula: circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym | Tree
   Data: Orange
  AIC  BIC logLik deviance
 1901 1908 -945.3     1891
Random effects:
 Groups   Name Variance  Std.Dev.
 Tree     Asym 53985.920 232.349
 Residual         52.868   7.271
Number of obs: 35, groups: Tree, 5

Fixed effects:
     Estimate Std. Error t value
Asym   192.04     104.09   1.845
xmid   727.89      31.97  22.771
scal   347.97      24.42  14.252

Correlation of Fixed Effects:
     Asym  xmid
xmid 0.053
scal 0.050 0.763


> sessionInfo()
R version 2.8.1 (2008-12-22)
i386-pc-mingw32

locale:
LC_COLLATE=English_New Zealand.1252;LC_CTYPE=English_New
Zealand.1252;LC_MONETARY=English_New
Zealand.1252;LC_NUMERIC=C;LC_TIME=English_New Zealand.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lme4_0.999375-28   Matrix_0.999375-22 lattice_0.17-20

loaded via a namespace (and not attached):
[1] grid_2.8.1  tools_2.8.1



From spluque at gmail.com  Tue Mar 31 15:10:04 2009
From: spluque at gmail.com (Sebastian P. Luque)
Date: Tue, 31 Mar 2009 08:10:04 -0500
Subject: [R-sig-ME] syntax for indicating fixed covariates in nlmer ?
References: <49B68DC8.30601@ensat.fr> <49B79E80.2090205@ufl.edu>
	<40e66e0b0903110530k53718b86id54afcd5c3228598@mail.gmail.com>
Message-ID: <87hc19rg7n.fsf@patagonia.sebmags.homelinux.org>

On Wed, 11 Mar 2009 07:30:38 -0500,
Douglas Bates <bates at stat.wisc.edu> wrote:

[...]

> I think Laurent and Dieter might have been asking a different
> question.  In a nonlinear mixed-effects model the nonlinear model
> function incorporates covariates and "nonlinear model parameters".
> For example, the four-parameter logistic model is a function of a
> covariate, often something like log-dose, and four parameters that
> could be the asymptote on the left, the asymptote on the right, the
> midpoint and a scale parameter.

> example(SSfpl)

> produces a plot showing these.

> I think the question was how to express an NLMM in which one of these
> parameters, say the midpoint, xmid, incorporates the effect of a
> covariate like treatment group.

> If that is the question then the answer is "not easily at present".
> The development version of nlmer, in the branches/allcoef section of
> the SVN repository, has the capability of doing that.  That's the good
> news.  The bad news is that the syntax of the formula has changed a
> bit and I don't want to release the development branch until I can
> resolve problems with GLMMs in that branch.

> I am having some bizarre problems with GLMMs there - the sort of
> problem that will seem trivial once I know the answer but right now is
> very frustrating.  For some reason the current code fits Poisson GLMMs
> like a charm and diverges on Bernoulli GLMMs and binomial GLMMs.

> What I will do is to polish up the documentation of the nlmer function
> over the next few days so the brave (or foolhardy, depending on your
> point of view) user can fit those models.  Then I will try to get the
> binomial GLMMs happy again.

To make sure I understand the situation, is this equivalent to the case
where an nlme Gompertz growth model (3 parameters: Linf, b, and k) was
specified as:

nlme(y ~ Linf * exp(-b * exp(-k * x)), data=growthdata,
     fixed=Linf + b + k ~ 1, random=Linf ~ 1 | population,
     start=c(Linf=400, b=0.9, k=0.1))

so currently we can't do this in nlmer?


Cheers,

-- 
Seb



From brant.inman at mac.com  Tue Mar 31 01:43:27 2009
From: brant.inman at mac.com (Brant Inman)
Date: Mon, 30 Mar 2009 19:43:27 -0400
Subject: [R-sig-ME] Meta-analysis using lmer
In-Reply-To: <7131EF1EF27893479833FCF59E7AAC440B3E818BE1@HERMES9.ds.leeds.ac.uk>
References: <7131EF1EF27893479833FCF59E7AAC440B3E818BE1@HERMES9.ds.leeds.ac.uk>
Message-ID: <3BA9957C-1B7B-43F7-B2DB-C8262399263F@mac.com>

I asked a similar question a while back on R-help.  As far as I know,  
the only way to do this type of meta-analysis using the S language was  
to use S-Plus. My understanding is that SAS, HLM and MLWin can do this  
too.

All the examples from meta-analysis in Chapter 6 of the Hox book are  
do-able in R though.  I have the code if you want it.

Brant Inman


On Mar 30, 2009, at 1:16 PM, Yu-Kang Tu wrote:

>
> Hi,
>
> I am trying to use lme and lmer to do random effects meta-analysis  
> as described in Hox (2002) and UCLA website: http://www.ats.ucla.edu/stat/mlwin/examples/ma_hox/chapter8.htm
>
> Basically, what I want to do is to constraint one residual error  
> variance to be unity and use the inverse of standard errors as the  
> covariate (weight) for this variance. And an additional random  
> effects terms is used to estimate the between-study variation. I did  
> take a look at the Pinheiro & Bates book on varFunc, but  
> unfortunately, I cannot figure out how this can be done. Any  
> suggestions/advices will be greatly appreciated. Many thanks.
>
> Yu-Kang
> --------------------------------------------
> Dr Yu-Kang Tu
> Senior Clinical Research Fellow
> Division of Biostatistics, Centre for Epidemiology and Biostatistics
> Leeds Institute of Genetics, Health and Therapeutics, and
> Department of Periodontology, Leeds Dental Institute
> Room 8.01, Level 8, Worsley Building,
> Clarendon Way
> University of Leeds, LS2 9JT
> Email: y.k.tu at leeds.ac.uk
> Tel: +44 (0) 113 3431877
> Fax: +44 (0) 113 3434877
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



From bates at stat.wisc.edu  Tue Mar 31 21:54:44 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 31 Mar 2009 14:54:44 -0500
Subject: [R-sig-ME] Incorrect std errors from nlmer?
In-Reply-To: <1389.125.237.187.150.1238487090.squirrel@www.stat.auckland.ac.nz>
References: <1389.125.237.187.150.1238487090.squirrel@www.stat.auckland.ac.nz>
Message-ID: <40e66e0b0903311254x39e25ca2l1bb3030385487dca@mail.gmail.com>

On Tue, Mar 31, 2009 at 3:11 AM,  <r.millar at auckland.ac.nz> wrote:
> Hi All,

> I just copied and paste the example nlmer code from the lmer help file
> (see below), which fits the logistic to the orange tree data.
> The point estimates of the fixed effects look correct, but the s.e.'s
> are not (I've fitted the model using three other software). The s.e. of
> Asym is way out (it should be about 15), as is the tree effect variance
> (should be about 1000)??????

You're right.  I was kind of hoping that people wouldn't notice that :-)

This is corrected in the development version, which uses a slightly
different syntax in the model formula (all fixed-effects terms must be
explicitly included).  I won't have the development version ready for
release soon so I should go back and fix that in the released version.
 Unfortunately, it will probably be a week before I can look at it.
If someone can produce a patch I would greatly appreciate it.

Here is what the development version produces.

> (nm1 <- nlmer(circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym + xmid + scal + (Asym|Tree), Orange, start = c(Asym = 200, xmid = 725, scal = 350)))
Nonlinear mixed model fit by the Laplace approximation
Formula: circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym + xmid
+      scal + (Asym | Tree)
   Data: Orange
   AIC   BIC logLik deviance
 273.2 280.9 -131.6    263.2
Random effects:
 Groups   Name Variance Std.Dev.
 Tree     Asym 1000.911 31.637
 Residual        61.466  7.840
Number of obs: 35, groups: Tree, 5

Fixed effects:
     Estimate Std. Error t value
Asym   191.06      15.51   12.32
xmid   722.61      33.59   21.51
scal   344.20      25.94   13.27

Correlation of Fixed Effects:
     Asym  xmid
xmid 0.373
scal 0.353 0.755


> Regards,
>
> Russell Millar
> U. Auckland
>
>> (nm1 <- nlmer(circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym|Tree,
> + ? ? ? ? ? ? ? Orange, start = c(Asym = 200, xmid = 725, scal = 350)))
> Nonlinear mixed model fit by the Laplace approximation
> Formula: circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym | Tree
> ? Data: Orange
> ?AIC ?BIC logLik deviance
> ?1901 1908 -945.3 ? ? 1891
> Random effects:
> ?Groups ? Name Variance ?Std.Dev.
> ?Tree ? ? Asym 53985.920 232.349
> ?Residual ? ? ? ? 52.868 ? 7.271
> Number of obs: 35, groups: Tree, 5
>
> Fixed effects:
> ? ? Estimate Std. Error t value
> Asym ? 192.04 ? ? 104.09 ? 1.845
> xmid ? 727.89 ? ? ?31.97 ?22.771
> scal ? 347.97 ? ? ?24.42 ?14.252
>
> Correlation of Fixed Effects:
> ? ? Asym ?xmid
> xmid 0.053
> scal 0.050 0.763
>
>
>> sessionInfo()
> R version 2.8.1 (2008-12-22)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_New Zealand.1252;LC_CTYPE=English_New
> Zealand.1252;LC_MONETARY=English_New
> Zealand.1252;LC_NUMERIC=C;LC_TIME=English_New Zealand.1252
>
> attached base packages:
> [1] stats ? ? graphics ?grDevices utils ? ? datasets ?methods ? base
>
> other attached packages:
> [1] lme4_0.999375-28 ? Matrix_0.999375-22 lattice_0.17-20
>
> loaded via a namespace (and not attached):
> [1] grid_2.8.1 ?tools_2.8.1
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



From bates at stat.wisc.edu  Tue Mar 31 23:29:23 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 31 Mar 2009 16:29:23 -0500
Subject: [R-sig-ME] modelling a nested student-school-district model
In-Reply-To: <49D1B0F5.00007F.10234@app-03>
References: <49D1B0F5.00007F.10234@app-03>
Message-ID: <40e66e0b0903311429l4f10ef50qdf7bcf6a031e4249@mail.gmail.com>

2009/3/31 chenlei <chenlei at ibcas.ac.cn>:
> Dear all and dr.Bates,

> ? I have a dataset with?students nested in schools?and also schools?belong
> to each district.?The data was explicitly nested as previous?examples.
> ? In my case, I don't care the variance between schools or district,and I
> just want to assess the effect of gender on stuedents'
> scores,traditionally,the model can be specified in lmer like :
> ? lmer(score~gender+(1|district/school),data)
> ???notes: the gender was a factor(male,female)

> in my study ,I?want?to know the variance between genders,and I also use some
> covariates at gender level to explain the variance between genders.

It doesn't make sense to me to model the effect of gender as a random
effect.  I think of random effects as being associated with particular
experimental or observational units.  On the other hand, the levels of
gender, male and female, are fixed.  This type of factor is the
archetypal example of a factor for which you would use fixed effects.
In particular, you will only have two levels of gender, even if you
consider more schools or districts.  Trying to estimate a variance
from a single contrast is difficult.

> ?I construct the unconditional model and conditional models like these:
> ? unconditional model :lmer(score~1+(1|gender)+(1/district/school),data)
> ? conditional model :lmer(score~1+IQ+(1|gender)+(1/district/school),data)
> ? the?IQ indicates the mean IQ scores for different genders.

> What I want to confirm is whether the specification of all the models was
> reasonable and correct in lmer.Thanks.
> yours,
> Lei?Chen



From r.millar at auckland.ac.nz  Tue Mar 31 23:47:42 2009
From: r.millar at auckland.ac.nz (Russell Millar)
Date: Wed, 01 Apr 2009 10:47:42 +1300
Subject: [R-sig-ME] Incorrect std errors from nlmer?
In-Reply-To: <40e66e0b0903311254x39e25ca2l1bb3030385487dca@mail.gmail.com>
References: <1389.125.237.187.150.1238487090.squirrel@www.stat.auckland.ac.nz>
	<40e66e0b0903311254x39e25ca2l1bb3030385487dca@mail.gmail.com>
Message-ID: <49D28F7E.9040007@stat.auckland.ac.nz>


Thanks for that, but I do notice that the point estimates have now changed.
The laplace method (ADMB), quadrature (SAS), and simulated likelihood 
(Millar, 2004, Aust&NZ J. Stat)
all produce the same ML point estimates (to at least 4 sig places) that 
the current nlmer is finding.
Is the development version doing something different?

Regards,

Russell Millar
> On Tue, Mar 31, 2009 at 3:11 AM,  <r.millar at auckland.ac.nz> wrote:
>   
>> Hi All,
>>     
>
>   
>> I just copied and paste the example nlmer code from the lmer help file
>> (see below), which fits the logistic to the orange tree data.
>> The point estimates of the fixed effects look correct, but the s.e.'s
>> are not (I've fitted the model using three other software). The s.e. of
>> Asym is way out (it should be about 15), as is the tree effect variance
>> (should be about 1000)??????
>>     
>
> You're right.  I was kind of hoping that people wouldn't notice that :-)
>
> This is corrected in the development version, which uses a slightly
> different syntax in the model formula (all fixed-effects terms must be
> explicitly included).  I won't have the development version ready for
> release soon so I should go back and fix that in the released version.
>  Unfortunately, it will probably be a week before I can look at it.
> If someone can produce a patch I would greatly appreciate it.
>
> Here is what the development version produces.
>
>   
>> (nm1 <- nlmer(circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym + xmid + scal + (Asym|Tree), Orange, start = c(Asym = 200, xmid = 725, scal = 350)))
>>     
> Nonlinear mixed model fit by the Laplace approximation
> Formula: circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym + xmid
> +      scal + (Asym | Tree)
>    Data: Orange
>    AIC   BIC logLik deviance
>  273.2 280.9 -131.6    263.2
> Random effects:
>  Groups   Name Variance Std.Dev.
>  Tree     Asym 1000.911 31.637
>  Residual        61.466  7.840
> Number of obs: 35, groups: Tree, 5
>
> Fixed effects:
>      Estimate Std. Error t value
> Asym   191.06      15.51   12.32
> xmid   722.61      33.59   21.51
> scal   344.20      25.94   13.27
>
> Correlation of Fixed Effects:
>      Asym  xmid
> xmid 0.373
> scal 0.353 0.755
>
>
>   
>> Regards,
>>
>> Russell Millar
>> U. Auckland
>>
>>     
>>> (nm1 <- nlmer(circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym|Tree,
>>>       
>> +               Orange, start = c(Asym = 200, xmid = 725, scal = 350)))
>> Nonlinear mixed model fit by the Laplace approximation
>> Formula: circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym | Tree
>>   Data: Orange
>>  AIC  BIC logLik deviance
>>  1901 1908 -945.3     1891
>> Random effects:
>>  Groups   Name Variance  Std.Dev.
>>  Tree     Asym 53985.920 232.349
>>  Residual         52.868   7.271
>> Number of obs: 35, groups: Tree, 5
>>
>> Fixed effects:
>>     Estimate Std. Error t value
>> Asym   192.04     104.09   1.845
>> xmid   727.89      31.97  22.771
>> scal   347.97      24.42  14.252
>>
>> Correlation of Fixed Effects:
>>     Asym  xmid
>> xmid 0.053
>> scal 0.050 0.763
>>
>>
>>     
>>> sessionInfo()
>>>       
>> R version 2.8.1 (2008-12-22)
>> i386-pc-mingw32
>>
>> locale:
>> LC_COLLATE=English_New Zealand.1252;LC_CTYPE=English_New
>> Zealand.1252;LC_MONETARY=English_New
>> Zealand.1252;LC_NUMERIC=C;LC_TIME=English_New Zealand.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> other attached packages:
>> [1] lme4_0.999375-28   Matrix_0.999375-22 lattice_0.17-20
>>
>> loaded via a namespace (and not attached):
>> [1] grid_2.8.1  tools_2.8.1
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>



From bates at stat.wisc.edu  Tue Mar 31 23:59:45 2009
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 31 Mar 2009 16:59:45 -0500
Subject: [R-sig-ME] Incorrect std errors from nlmer?
In-Reply-To: <49D28F7E.9040007@stat.auckland.ac.nz>
References: <1389.125.237.187.150.1238487090.squirrel@www.stat.auckland.ac.nz>
	<40e66e0b0903311254x39e25ca2l1bb3030385487dca@mail.gmail.com>
	<49D28F7E.9040007@stat.auckland.ac.nz>
Message-ID: <40e66e0b0903311459k1bbcbe9eie807d7831b24e6a2@mail.gmail.com>

On Tue, Mar 31, 2009 at 4:47 PM, Russell Millar <r.millar at auckland.ac.nz> wrote:

> Thanks for that, but I do notice that the point estimates have now changed.
> The laplace method (ADMB), quadrature (SAS), and simulated likelihood
> (Millar, 2004, Aust&NZ J. Stat)
> all produce the same ML point estimates (to at least 4 sig places) that the
> current nlmer is finding.
> Is the development version doing something different?

Yes.  It is doing the optimization of the fixed-effects parameters in
a different part of the algorithm.  Looks like I need to think that
through more carefully.  Rats - it sure seemed like a good idea.

>
> Regards,
>
> Russell Millar
>>
>> On Tue, Mar 31, 2009 at 3:11 AM, ?<r.millar at auckland.ac.nz> wrote:
>>
>>>
>>> Hi All,
>>>
>>
>>
>>>
>>> I just copied and paste the example nlmer code from the lmer help file
>>> (see below), which fits the logistic to the orange tree data.
>>> The point estimates of the fixed effects look correct, but the s.e.'s
>>> are not (I've fitted the model using three other software). The s.e. of
>>> Asym is way out (it should be about 15), as is the tree effect variance
>>> (should be about 1000)??????
>>>
>>
>> You're right. ?I was kind of hoping that people wouldn't notice that :-)
>>
>> This is corrected in the development version, which uses a slightly
>> different syntax in the model formula (all fixed-effects terms must be
>> explicitly included). ?I won't have the development version ready for
>> release soon so I should go back and fix that in the released version.
>> ?Unfortunately, it will probably be a week before I can look at it.
>> If someone can produce a patch I would greatly appreciate it.
>>
>> Here is what the development version produces.
>>
>>
>>>
>>> (nm1 <- nlmer(circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym +
>>> xmid + scal + (Asym|Tree), Orange, start = c(Asym = 200, xmid = 725, scal =
>>> 350)))
>>>
>>
>> Nonlinear mixed model fit by the Laplace approximation
>> Formula: circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym + xmid
>> + ? ? ?scal + (Asym | Tree)
>> ? Data: Orange
>> ? AIC ? BIC logLik deviance
>> ?273.2 280.9 -131.6 ? ?263.2
>> Random effects:
>> ?Groups ? Name Variance Std.Dev.
>> ?Tree ? ? Asym 1000.911 31.637
>> ?Residual ? ? ? ?61.466 ?7.840
>> Number of obs: 35, groups: Tree, 5
>>
>> Fixed effects:
>> ? ? Estimate Std. Error t value
>> Asym ? 191.06 ? ? ?15.51 ? 12.32
>> xmid ? 722.61 ? ? ?33.59 ? 21.51
>> scal ? 344.20 ? ? ?25.94 ? 13.27
>>
>> Correlation of Fixed Effects:
>> ? ? Asym ?xmid
>> xmid 0.373
>> scal 0.353 0.755
>>
>>
>>
>>>
>>> Regards,
>>>
>>> Russell Millar
>>> U. Auckland
>>>
>>>
>>>>
>>>> (nm1 <- nlmer(circumference ~ SSlogis(age, Asym, xmid, scal) ~
>>>> Asym|Tree,
>>>>
>>>
>>> + ? ? ? ? ? ? ? Orange, start = c(Asym = 200, xmid = 725, scal = 350)))
>>> Nonlinear mixed model fit by the Laplace approximation
>>> Formula: circumference ~ SSlogis(age, Asym, xmid, scal) ~ Asym | Tree
>>> ?Data: Orange
>>> ?AIC ?BIC logLik deviance
>>> ?1901 1908 -945.3 ? ? 1891
>>> Random effects:
>>> ?Groups ? Name Variance ?Std.Dev.
>>> ?Tree ? ? Asym 53985.920 232.349
>>> ?Residual ? ? ? ? 52.868 ? 7.271
>>> Number of obs: 35, groups: Tree, 5
>>>
>>> Fixed effects:
>>> ? ?Estimate Std. Error t value
>>> Asym ? 192.04 ? ? 104.09 ? 1.845
>>> xmid ? 727.89 ? ? ?31.97 ?22.771
>>> scal ? 347.97 ? ? ?24.42 ?14.252
>>>
>>> Correlation of Fixed Effects:
>>> ? ?Asym ?xmid
>>> xmid 0.053
>>> scal 0.050 0.763
>>>
>>>
>>>
>>>>
>>>> sessionInfo()
>>>>
>>>
>>> R version 2.8.1 (2008-12-22)
>>> i386-pc-mingw32
>>>
>>> locale:
>>> LC_COLLATE=English_New Zealand.1252;LC_CTYPE=English_New
>>> Zealand.1252;LC_MONETARY=English_New
>>> Zealand.1252;LC_NUMERIC=C;LC_TIME=English_New Zealand.1252
>>>
>>> attached base packages:
>>> [1] stats ? ? graphics ?grDevices utils ? ? datasets ?methods ? base
>>>
>>> other attached packages:
>>> [1] lme4_0.999375-28 ? Matrix_0.999375-22 lattice_0.17-20
>>>
>>> loaded via a namespace (and not attached):
>>> [1] grid_2.8.1 ?tools_2.8.1
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>
>



