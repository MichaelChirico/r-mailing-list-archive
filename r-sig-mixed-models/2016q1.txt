From M.Fairbrother at bristol.ac.uk  Sat Jan  2 23:47:08 2016
From: M.Fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Sat, 2 Jan 2016 14:47:08 -0800
Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
	model?
Message-ID: <CAAH-yP-jCvZ903MNUHFASVi_BDkNMkiUCvnFPnhNk4Y=H=0Pmw@mail.gmail.com>

Dear Alberto (I believe),
To my knowledge, this is not possible in MCMCglmm (though Jarrod Hadfield,
the package author, may weigh in with another response).
A collaborator and I have been working on a paper that shows how to fit
such models in JAGS (and perhaps Stan), though thus far we've only been
able to fit such models correcting for measurement error in the predictors
at the lowest level. Multiple such predictors (including with different
measurement error variances) are no problem.
That paper, however, is probably still some months away from being finished
and presentable. In the meantime, I don't know of any good options for you.
If other subscribers to this list have any ideas, I'll be quite interested
too!
- Malcolm





Date: Tue, 29 Dec 2015 16:09:53 -0500
> From: Alberto Gallano <alberto.gc8 at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
>         model?
>
> I posted this question on Stack Overflow a week ago but received no
> answers:
>
>
> http://stackoverflow.com/questions/34446618/bayesian-error-in-variables-total-least-squares-model-in-r-using-mcmcglmm
>
> This may be a more appropriate venue.
>
>
> I am fitting some Bayesian linear mixed models using the MCMCglmm package.
> My data includes predictors that are measured with error. I'd therefore
> like to build a model that takes this into account. My understanding is
> that a basic mixed effects model in MCMCglmm will minimize error only for
> the response variable (as in frequentist OLS regression). In other words,
> vertical errors will be minimized. Instead, I'd like to minimize errors
> orthogonal to the regression line/plane/hyperplane.
>
>    1. Is it possible to fit an error-in-variables (aka total least squares)
>    model using MCMCglmm or would I have to use JAGS / STAN to do this?
>    2. Is it possible to do this with multiple predictors in the same model
>    (I have some models with 3 or 4 predictors, each measured with error)?
>

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Sun Jan  3 11:24:27 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sun, 03 Jan 2016 10:24:27 +0000
Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
 model?
In-Reply-To: <CAAH-yP-jCvZ903MNUHFASVi_BDkNMkiUCvnFPnhNk4Y=H=0Pmw@mail.gmail.com>
References: <CAAH-yP-jCvZ903MNUHFASVi_BDkNMkiUCvnFPnhNk4Y=H=0Pmw@mail.gmail.com>
Message-ID: <20160103102427.12126bmxnimtdyvs@www.staffmail.ed.ac.uk>

Hi Alberto,

Do you know the measurement error in the predictors in advance or do  
you have multiple observations for each predictor variable and wish to  
estimate the error simultaneously?

Cheers,

Jarrod




Quoting Malcolm Fairbrother <M.Fairbrother at bristol.ac.uk> on Sat, 2  
Jan 2016 14:47:08 -0800:

> Dear Alberto (I believe),
> To my knowledge, this is not possible in MCMCglmm (though Jarrod Hadfield,
> the package author, may weigh in with another response).
> A collaborator and I have been working on a paper that shows how to fit
> such models in JAGS (and perhaps Stan), though thus far we've only been
> able to fit such models correcting for measurement error in the predictors
> at the lowest level. Multiple such predictors (including with different
> measurement error variances) are no problem.
> That paper, however, is probably still some months away from being finished
> and presentable. In the meantime, I don't know of any good options for you.
> If other subscribers to this list have any ideas, I'll be quite interested
> too!
> - Malcolm
>
>
>
>
>
> Date: Tue, 29 Dec 2015 16:09:53 -0500
>> From: Alberto Gallano <alberto.gc8 at gmail.com>
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
>>         model?
>>
>> I posted this question on Stack Overflow a week ago but received no
>> answers:
>>
>>
>> http://stackoverflow.com/questions/34446618/bayesian-error-in-variables-total-least-squares-model-in-r-using-mcmcglmm
>>
>> This may be a more appropriate venue.
>>
>>
>> I am fitting some Bayesian linear mixed models using the MCMCglmm package.
>> My data includes predictors that are measured with error. I'd therefore
>> like to build a model that takes this into account. My understanding is
>> that a basic mixed effects model in MCMCglmm will minimize error only for
>> the response variable (as in frequentist OLS regression). In other words,
>> vertical errors will be minimized. Instead, I'd like to minimize errors
>> orthogonal to the regression line/plane/hyperplane.
>>
>>    1. Is it possible to fit an error-in-variables (aka total least squares)
>>    model using MCMCglmm or would I have to use JAGS / STAN to do this?
>>    2. Is it possible to do this with multiple predictors in the same model
>>    (I have some models with 3 or 4 predictors, each measured with error)?
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Sun Jan  3 17:16:12 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sun, 03 Jan 2016 16:16:12 +0000
Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
 model?
In-Reply-To: <CAO+b4j-VghYh6pjMNSJK+0YyTyShAMqLOf=GOBn4+FX4_f45kw@mail.gmail.com>
References: <CAAH-yP-jCvZ903MNUHFASVi_BDkNMkiUCvnFPnhNk4Y=H=0Pmw@mail.gmail.com>
	<20160103102427.12126bmxnimtdyvs@www.staffmail.ed.ac.uk>
	<CAO+b4j-VghYh6pjMNSJK+0YyTyShAMqLOf=GOBn4+FX4_f45kw@mail.gmail.com>
Message-ID: <20160103161612.21125f2zso7a0mos@www.staffmail.ed.ac.uk>

Hi Alberto,

When you say you have multiple observations for each species, do you  
mean that you have multiple observations for the response and the  
predictors? Do you expect the response and/or the predictors to be  
correlated at the observation level (for example are they measured on  
the same individuals)? I presume the answer to both these questions is  
yes if you wish to use the van de Pol method?

Cheers,

Jarrod


Quoting Alberto Gallano <alberto.gc8 at gmail.com> on Sun, 3 Jan 2016  
10:35:02 -0500:

> Hi Jarrod,
>
> I don't know the measurement error in the predictors in advance, so I guess
> it would need to be estimated simultaneously. I'm not 100% sure what you
> mean by 'multiple observations for each predictor variable'. I have data on
> 132 species and have multiple observations (7 to 80) for each species. I'm
> using a species level random effect and a phylogenetic covariance matrix
> (using ginverse) to account for phylogenetic autocorrelation, and I'm also
> using van de Pol and Wright's (2009) method for partitioning slopes into
> between- and within-species (i'm interested in the between species slope).
> My understanding is that neither of these things fits a model in which
> orthogonal residuals are minimized.
>
> best,
> Alberto
>
> On Sun, Jan 3, 2016 at 5:24 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:
>
>> Hi Alberto,
>>
>> Do you know the measurement error in the predictors in advance or do you
>> have multiple observations for each predictor variable and wish to estimate
>> the error simultaneously?
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>>
>> Quoting Malcolm Fairbrother <M.Fairbrother at bristol.ac.uk> on Sat, 2 Jan
>> 2016 14:47:08 -0800:
>>
>> Dear Alberto (I believe),
>>> To my knowledge, this is not possible in MCMCglmm (though Jarrod Hadfield,
>>> the package author, may weigh in with another response).
>>> A collaborator and I have been working on a paper that shows how to fit
>>> such models in JAGS (and perhaps Stan), though thus far we've only been
>>> able to fit such models correcting for measurement error in the predictors
>>> at the lowest level. Multiple such predictors (including with different
>>> measurement error variances) are no problem.
>>> That paper, however, is probably still some months away from being
>>> finished
>>> and presentable. In the meantime, I don't know of any good options for
>>> you.
>>> If other subscribers to this list have any ideas, I'll be quite interested
>>> too!
>>> - Malcolm
>>>
>>>
>>>
>>>
>>>
>>> Date: Tue, 29 Dec 2015 16:09:53 -0500
>>>
>>>> From: Alberto Gallano <alberto.gc8 at gmail.com>
>>>> To: r-sig-mixed-models at r-project.org
>>>> Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
>>>>         model?
>>>>
>>>> I posted this question on Stack Overflow a week ago but received no
>>>> answers:
>>>>
>>>>
>>>>
>>>> http://stackoverflow.com/questions/34446618/bayesian-error-in-variables-total-least-squares-model-in-r-using-mcmcglmm
>>>>
>>>> This may be a more appropriate venue.
>>>>
>>>>
>>>> I am fitting some Bayesian linear mixed models using the MCMCglmm
>>>> package.
>>>> My data includes predictors that are measured with error. I'd therefore
>>>> like to build a model that takes this into account. My understanding is
>>>> that a basic mixed effects model in MCMCglmm will minimize error only for
>>>> the response variable (as in frequentist OLS regression). In other words,
>>>> vertical errors will be minimized. Instead, I'd like to minimize errors
>>>> orthogonal to the regression line/plane/hyperplane.
>>>>
>>>>    1. Is it possible to fit an error-in-variables (aka total least
>>>> squares)
>>>>    model using MCMCglmm or would I have to use JAGS / STAN to do this?
>>>>    2. Is it possible to do this with multiple predictors in the same
>>>> model
>>>>    (I have some models with 3 or 4 predictors, each measured with error)?
>>>>
>>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From da.skandalis at gmail.com  Sun Jan  3 18:05:57 2016
From: da.skandalis at gmail.com (Dimitri Skandalis)
Date: Sun, 3 Jan 2016 09:05:57 -0800
Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
	model?
In-Reply-To: <20160103161612.21125f2zso7a0mos@www.staffmail.ed.ac.uk>
References: <CAAH-yP-jCvZ903MNUHFASVi_BDkNMkiUCvnFPnhNk4Y=H=0Pmw@mail.gmail.com>
	<20160103102427.12126bmxnimtdyvs@www.staffmail.ed.ac.uk>
	<CAO+b4j-VghYh6pjMNSJK+0YyTyShAMqLOf=GOBn4+FX4_f45kw@mail.gmail.com>
	<20160103161612.21125f2zso7a0mos@www.staffmail.ed.ac.uk>
Message-ID: <CALV5z9POYX80wSqHR5qNBd6JHvdfAUczc85cWxF5TAc+Y8N_WA@mail.gmail.com>

Hi Alberto,

Have you looked at the book Modern Phylogenetic Comparative Methods? R code
provided with Chapter 11 (2) deals with correlated measurements, and could
be a good place to start.

http://www.mpcm-evolution.org/practice/online-practical-material-chapter-11

Also, de Villemereuil et al. have developed an approach to related models
in BUGS/JAGS.

http://bmcevolbiol.biomedcentral.com/articles/10.1186/1471-2148-12-102

Dimitri

On Sun, Jan 3, 2016 at 8:16 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:

> Hi Alberto,
>
> When you say you have multiple observations for each species, do you mean
> that you have multiple observations for the response and the predictors? Do
> you expect the response and/or the predictors to be correlated at the
> observation level (for example are they measured on the same individuals)?
> I presume the answer to both these questions is yes if you wish to use the
> van de Pol method?
>
> Cheers,
>
> Jarrod
>
>
> Quoting Alberto Gallano <alberto.gc8 at gmail.com> on Sun, 3 Jan 2016
> 10:35:02 -0500:
>
> Hi Jarrod,
>>
>> I don't know the measurement error in the predictors in advance, so I
>> guess
>> it would need to be estimated simultaneously. I'm not 100% sure what you
>> mean by 'multiple observations for each predictor variable'. I have data
>> on
>> 132 species and have multiple observations (7 to 80) for each species. I'm
>> using a species level random effect and a phylogenetic covariance matrix
>> (using ginverse) to account for phylogenetic autocorrelation, and I'm also
>> using van de Pol and Wright's (2009) method for partitioning slopes into
>> between- and within-species (i'm interested in the between species slope).
>> My understanding is that neither of these things fits a model in which
>> orthogonal residuals are minimized.
>>
>> best,
>> Alberto
>>
>>
>> On Sun, Jan 3, 2016 at 5:24 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
>> wrote:
>>
>> Hi Alberto,
>>>
>>> Do you know the measurement error in the predictors in advance or do you
>>> have multiple observations for each predictor variable and wish to
>>> estimate
>>> the error simultaneously?
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>>
>>>
>>>
>>> Quoting Malcolm Fairbrother <M.Fairbrother at bristol.ac.uk> on Sat, 2 Jan
>>> 2016 14:47:08 -0800:
>>>
>>> Dear Alberto (I believe),
>>>
>>>> To my knowledge, this is not possible in MCMCglmm (though Jarrod
>>>> Hadfield,
>>>> the package author, may weigh in with another response).
>>>> A collaborator and I have been working on a paper that shows how to fit
>>>> such models in JAGS (and perhaps Stan), though thus far we've only been
>>>> able to fit such models correcting for measurement error in the
>>>> predictors
>>>> at the lowest level. Multiple such predictors (including with different
>>>> measurement error variances) are no problem.
>>>> That paper, however, is probably still some months away from being
>>>> finished
>>>> and presentable. In the meantime, I don't know of any good options for
>>>> you.
>>>> If other subscribers to this list have any ideas, I'll be quite
>>>> interested
>>>> too!
>>>> - Malcolm
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> Date: Tue, 29 Dec 2015 16:09:53 -0500
>>>>
>>>> From: Alberto Gallano <alberto.gc8 at gmail.com>
>>>>> To: r-sig-mixed-models at r-project.org
>>>>> Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
>>>>>         model?
>>>>>
>>>>> I posted this question on Stack Overflow a week ago but received no
>>>>> answers:
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> http://stackoverflow.com/questions/34446618/bayesian-error-in-variables-total-least-squares-model-in-r-using-mcmcglmm
>>>>>
>>>>> This may be a more appropriate venue.
>>>>>
>>>>>
>>>>> I am fitting some Bayesian linear mixed models using the MCMCglmm
>>>>> package.
>>>>> My data includes predictors that are measured with error. I'd therefore
>>>>> like to build a model that takes this into account. My understanding is
>>>>> that a basic mixed effects model in MCMCglmm will minimize error only
>>>>> for
>>>>> the response variable (as in frequentist OLS regression). In other
>>>>> words,
>>>>> vertical errors will be minimized. Instead, I'd like to minimize errors
>>>>> orthogonal to the regression line/plane/hyperplane.
>>>>>
>>>>>    1. Is it possible to fit an error-in-variables (aka total least
>>>>> squares)
>>>>>    model using MCMCglmm or would I have to use JAGS / STAN to do this?
>>>>>    2. Is it possible to do this with multiple predictors in the same
>>>>> model
>>>>>    (I have some models with 3 or 4 predictors, each measured with
>>>>> error)?
>>>>>
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>>
>>>>
>>>>
>>>
>>> --
>>> The University of Edinburgh is a charitable body, registered in
>>> Scotland, with registration number SC005336.
>>>
>>>
>>>
>>>
>>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Mon Jan  4 00:10:26 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 3 Jan 2016 23:10:26 +0000 (UTC)
Subject: [R-sig-ME] Question about lme4
References: <CAPD3YzZA+20x4_HofEO1KWty9+JKcHdY5t7urbVdiCimk11pGg@mail.gmail.com>
Message-ID: <loom.20160104T000731-999@post.gmane.org>

jesse cloud <cloud.jesse7 at ...> writes:

> 
> Hi, This is Jesse. I am very interested in your lme4 package. I hope to fit
> the linear mixed model using your package. But I can only get the variance
> estimates for the random effect from the output. Is that possible to get
> the asymptotic variance estimates of these variance estimates?
> 
> Thank you very much.
> 
> Best,
> Jesse
> 


  With some difficulty, yes.  I seem to have posted partial answers
to this on Rpubs twice:

http://rpubs.com/bbolker/varwald
http://rpubs.com/bbolker/waldvar

The latter is later, but it might be worth looking at both of them
(at some point I should reconcile them ...)

Keep in mind that the asymptotic variances of the variance estimates
are often very inaccurate summaries of the uncertainty ...


From j.hadfield at ed.ac.uk  Mon Jan  4 09:16:05 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 04 Jan 2016 08:16:05 +0000
Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
 model?
In-Reply-To: <CAO+b4j8QvfifmuUt+kHZXai=SPhay60=S9zA2ez5yv1Q3=37ug@mail.gmail.com>
References: <CAAH-yP-jCvZ903MNUHFASVi_BDkNMkiUCvnFPnhNk4Y=H=0Pmw@mail.gmail.com>
	<20160103102427.12126bmxnimtdyvs@www.staffmail.ed.ac.uk>
	<CAO+b4j-VghYh6pjMNSJK+0YyTyShAMqLOf=GOBn4+FX4_f45kw@mail.gmail.com>
	<20160103161612.21125f2zso7a0mos@www.staffmail.ed.ac.uk>
	<CALV5z9POYX80wSqHR5qNBd6JHvdfAUczc85cWxF5TAc+Y8N_WA@mail.gmail.com>
	<CAO+b4j8QvfifmuUt+kHZXai=SPhay60=S9zA2ez5yv1Q3=37ug@mail.gmail.com>
Message-ID: <20160104081605.30452h7fxzspouo0@www.staffmail.ed.ac.uk>

Hi,

The least parsimonious model (and not the one I would necessarily  
recommend fitting) is:


m1<-MCMCglmm(cbind(X1,X2,X3,Y)~trait,
              random=~us(trait):species+us(trait):species.ide,
              rcov=~us(trait):units,
              ginverse=list(species=tree))

where species and species.ide are columns of species names.

This deals with the measurement error on the species means, and also  
allows you to address the fact that the regressions of the X's on Y  
may be different at different levels. The method advocated by van de  
Pol has the problem that the mean in the mean centering is just the  
observed mean rather than the true unobserved mean. For example,  
imagine that you only had one observation for some of the species.   
You can obtain the regression coefficients at each level, by using the  
relationship beta = VAR(X)^{-1}COV(X,Y). For example, the posterior  
distribution of the regression coefficients at the phylogenetic level  
would be:


reg.coef<-function(x, X=1:3, Y=4){
V<-matrix(x,c(X,Y),c(X,Y))
solve(V[X,X], V[X,Y])
}

apply(m1$VCV[,1:16], 1, reg.coef)

The model doesn't deal with measurement error on the individual  
measurements, but if you had repeat measurements per individual you  
could also fit these (as a diagonal matrix, rather than unstructured).

After taking into account measurement error, some people suggest that  
species.ide should be dropped from the model. I am not completely  
convinced by this argument.

Priors are going to be a pain in this model.

You could replace the us structures by ante3 structures. The model is  
then fitted directly in terms of the regression coefficients.  
Antedependence regression coefficients 3,5,6 are the regressions of  
X3, X2 and X1 on Y. If you are interested in this we have a  
mini-tutorial associated with a recently submitted paper I can send you.

Cheers,

Jarrod






Quoting Alberto Gallano <alberto.gc8 at gmail.com> on Sun, 3 Jan 2016  
15:45:26 -0500:

> Hi Jarrod,
>
> yes, that's right, I have multiple measurements for both response and
> predictors and these are measured on the same individuals. The model i'm
> fitting is very similar to the model called "model_repeat2" from Modern
> Phylogenetic Comparative Methods:
>
> http://www.mpcm-evolution.org/practice/online-practical-material-chapter-11/chapter-11-2-multiple-measurements-model-mcmcglmm
>
> same random effects structure, same between/within structure for the fixed
> effects, and i'm also using the inverse of the matrix of phylogenetic
> correlation.
>
> @Dimitri: I'm aware of the de Villemereuil et al. approach, which, If I
> understand correctly, does a version of orthogonal regression (in JAGS).
> I'm trying find out if this is possible in MCMCglmm.
>
> best,
> Alberto
>
> On Sun, Jan 3, 2016 at 12:05 PM, Dimitri Skandalis <da.skandalis at gmail.com>
> wrote:
>
>> Hi Alberto,
>>
>> Have you looked at the book Modern Phylogenetic Comparative Methods? R
>> code provided with Chapter 11 (2) deals with correlated measurements, and
>> could be a good place to start.
>>
>> http://www.mpcm-evolution.org/practice/online-practical-material-chapter-11
>>
>> Also, de Villemereuil et al. have developed an approach to related models
>> in BUGS/JAGS.
>>
>> http://bmcevolbiol.biomedcentral.com/articles/10.1186/1471-2148-12-102
>>
>> Dimitri
>>
>> On Sun, Jan 3, 2016 at 8:16 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
>> wrote:
>>
>>> Hi Alberto,
>>>
>>> When you say you have multiple observations for each species, do you mean
>>> that you have multiple observations for the response and the predictors? Do
>>> you expect the response and/or the predictors to be correlated at the
>>> observation level (for example are they measured on the same individuals)?
>>> I presume the answer to both these questions is yes if you wish to use the
>>> van de Pol method?
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>> Quoting Alberto Gallano <alberto.gc8 at gmail.com> on Sun, 3 Jan 2016
>>> 10:35:02 -0500:
>>>
>>> Hi Jarrod,
>>>>
>>>> I don't know the measurement error in the predictors in advance, so I
>>>> guess
>>>> it would need to be estimated simultaneously. I'm not 100% sure what you
>>>> mean by 'multiple observations for each predictor variable'. I have data
>>>> on
>>>> 132 species and have multiple observations (7 to 80) for each species.
>>>> I'm
>>>> using a species level random effect and a phylogenetic covariance matrix
>>>> (using ginverse) to account for phylogenetic autocorrelation, and I'm
>>>> also
>>>> using van de Pol and Wright's (2009) method for partitioning slopes into
>>>> between- and within-species (i'm interested in the between species
>>>> slope).
>>>> My understanding is that neither of these things fits a model in which
>>>> orthogonal residuals are minimized.
>>>>
>>>> best,
>>>> Alberto
>>>>
>>>>
>>>> On Sun, Jan 3, 2016 at 5:24 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
>>>> wrote:
>>>>
>>>> Hi Alberto,
>>>>>
>>>>> Do you know the measurement error in the predictors in advance or do you
>>>>> have multiple observations for each predictor variable and wish to
>>>>> estimate
>>>>> the error simultaneously?
>>>>>
>>>>> Cheers,
>>>>>
>>>>> Jarrod
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> Quoting Malcolm Fairbrother <M.Fairbrother at bristol.ac.uk> on Sat, 2 Jan
>>>>> 2016 14:47:08 -0800:
>>>>>
>>>>> Dear Alberto (I believe),
>>>>>
>>>>>> To my knowledge, this is not possible in MCMCglmm (though Jarrod
>>>>>> Hadfield,
>>>>>> the package author, may weigh in with another response).
>>>>>> A collaborator and I have been working on a paper that shows how to fit
>>>>>> such models in JAGS (and perhaps Stan), though thus far we've only been
>>>>>> able to fit such models correcting for measurement error in the
>>>>>> predictors
>>>>>> at the lowest level. Multiple such predictors (including with different
>>>>>> measurement error variances) are no problem.
>>>>>> That paper, however, is probably still some months away from being
>>>>>> finished
>>>>>> and presentable. In the meantime, I don't know of any good options for
>>>>>> you.
>>>>>> If other subscribers to this list have any ideas, I'll be quite
>>>>>> interested
>>>>>> too!
>>>>>> - Malcolm
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> Date: Tue, 29 Dec 2015 16:09:53 -0500
>>>>>>
>>>>>> From: Alberto Gallano <alberto.gc8 at gmail.com>
>>>>>>> To: r-sig-mixed-models at r-project.org
>>>>>>> Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
>>>>>>>         model?
>>>>>>>
>>>>>>> I posted this question on Stack Overflow a week ago but received no
>>>>>>> answers:
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> http://stackoverflow.com/questions/34446618/bayesian-error-in-variables-total-least-squares-model-in-r-using-mcmcglmm
>>>>>>>
>>>>>>> This may be a more appropriate venue.
>>>>>>>
>>>>>>>
>>>>>>> I am fitting some Bayesian linear mixed models using the MCMCglmm
>>>>>>> package.
>>>>>>> My data includes predictors that are measured with error. I'd
>>>>>>> therefore
>>>>>>> like to build a model that takes this into account. My understanding
>>>>>>> is
>>>>>>> that a basic mixed effects model in MCMCglmm will minimize error only
>>>>>>> for
>>>>>>> the response variable (as in frequentist OLS regression). In other
>>>>>>> words,
>>>>>>> vertical errors will be minimized. Instead, I'd like to minimize
>>>>>>> errors
>>>>>>> orthogonal to the regression line/plane/hyperplane.
>>>>>>>
>>>>>>>    1. Is it possible to fit an error-in-variables (aka total least
>>>>>>> squares)
>>>>>>>    model using MCMCglmm or would I have to use JAGS / STAN to do this?
>>>>>>>    2. Is it possible to do this with multiple predictors in the same
>>>>>>> model
>>>>>>>    (I have some models with 3 or 4 predictors, each measured with
>>>>>>> error)?
>>>>>>>
>>>>>>>
>>>>>>>         [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>> --
>>>>> The University of Edinburgh is a charitable body, registered in
>>>>> Scotland, with registration number SC005336.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>
>>>
>>> --
>>> The University of Edinburgh is a charitable body, registered in
>>> Scotland, with registration number SC005336.
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From alberto.gc8 at gmail.com  Sun Jan  3 16:35:02 2016
From: alberto.gc8 at gmail.com (Alberto Gallano)
Date: Sun, 3 Jan 2016 10:35:02 -0500
Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
	model?
In-Reply-To: <20160103102427.12126bmxnimtdyvs@www.staffmail.ed.ac.uk>
References: <CAAH-yP-jCvZ903MNUHFASVi_BDkNMkiUCvnFPnhNk4Y=H=0Pmw@mail.gmail.com>
	<20160103102427.12126bmxnimtdyvs@www.staffmail.ed.ac.uk>
Message-ID: <CAO+b4j-VghYh6pjMNSJK+0YyTyShAMqLOf=GOBn4+FX4_f45kw@mail.gmail.com>

Hi Jarrod,

I don't know the measurement error in the predictors in advance, so I guess
it would need to be estimated simultaneously. I'm not 100% sure what you
mean by 'multiple observations for each predictor variable'. I have data on
132 species and have multiple observations (7 to 80) for each species. I'm
using a species level random effect and a phylogenetic covariance matrix
(using ginverse) to account for phylogenetic autocorrelation, and I'm also
using van de Pol and Wright's (2009) method for partitioning slopes into
between- and within-species (i'm interested in the between species slope).
My understanding is that neither of these things fits a model in which
orthogonal residuals are minimized.

best,
Alberto

On Sun, Jan 3, 2016 at 5:24 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:

> Hi Alberto,
>
> Do you know the measurement error in the predictors in advance or do you
> have multiple observations for each predictor variable and wish to estimate
> the error simultaneously?
>
> Cheers,
>
> Jarrod
>
>
>
>
>
> Quoting Malcolm Fairbrother <M.Fairbrother at bristol.ac.uk> on Sat, 2 Jan
> 2016 14:47:08 -0800:
>
> Dear Alberto (I believe),
>> To my knowledge, this is not possible in MCMCglmm (though Jarrod Hadfield,
>> the package author, may weigh in with another response).
>> A collaborator and I have been working on a paper that shows how to fit
>> such models in JAGS (and perhaps Stan), though thus far we've only been
>> able to fit such models correcting for measurement error in the predictors
>> at the lowest level. Multiple such predictors (including with different
>> measurement error variances) are no problem.
>> That paper, however, is probably still some months away from being
>> finished
>> and presentable. In the meantime, I don't know of any good options for
>> you.
>> If other subscribers to this list have any ideas, I'll be quite interested
>> too!
>> - Malcolm
>>
>>
>>
>>
>>
>> Date: Tue, 29 Dec 2015 16:09:53 -0500
>>
>>> From: Alberto Gallano <alberto.gc8 at gmail.com>
>>> To: r-sig-mixed-models at r-project.org
>>> Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
>>>         model?
>>>
>>> I posted this question on Stack Overflow a week ago but received no
>>> answers:
>>>
>>>
>>>
>>> http://stackoverflow.com/questions/34446618/bayesian-error-in-variables-total-least-squares-model-in-r-using-mcmcglmm
>>>
>>> This may be a more appropriate venue.
>>>
>>>
>>> I am fitting some Bayesian linear mixed models using the MCMCglmm
>>> package.
>>> My data includes predictors that are measured with error. I'd therefore
>>> like to build a model that takes this into account. My understanding is
>>> that a basic mixed effects model in MCMCglmm will minimize error only for
>>> the response variable (as in frequentist OLS regression). In other words,
>>> vertical errors will be minimized. Instead, I'd like to minimize errors
>>> orthogonal to the regression line/plane/hyperplane.
>>>
>>>    1. Is it possible to fit an error-in-variables (aka total least
>>> squares)
>>>    model using MCMCglmm or would I have to use JAGS / STAN to do this?
>>>    2. Is it possible to do this with multiple predictors in the same
>>> model
>>>    (I have some models with 3 or 4 predictors, each measured with error)?
>>>
>>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>
>

	[[alternative HTML version deleted]]


From alberto.gc8 at gmail.com  Sun Jan  3 21:45:26 2016
From: alberto.gc8 at gmail.com (Alberto Gallano)
Date: Sun, 3 Jan 2016 15:45:26 -0500
Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
	model?
In-Reply-To: <CALV5z9POYX80wSqHR5qNBd6JHvdfAUczc85cWxF5TAc+Y8N_WA@mail.gmail.com>
References: <CAAH-yP-jCvZ903MNUHFASVi_BDkNMkiUCvnFPnhNk4Y=H=0Pmw@mail.gmail.com>
	<20160103102427.12126bmxnimtdyvs@www.staffmail.ed.ac.uk>
	<CAO+b4j-VghYh6pjMNSJK+0YyTyShAMqLOf=GOBn4+FX4_f45kw@mail.gmail.com>
	<20160103161612.21125f2zso7a0mos@www.staffmail.ed.ac.uk>
	<CALV5z9POYX80wSqHR5qNBd6JHvdfAUczc85cWxF5TAc+Y8N_WA@mail.gmail.com>
Message-ID: <CAO+b4j8QvfifmuUt+kHZXai=SPhay60=S9zA2ez5yv1Q3=37ug@mail.gmail.com>

Hi Jarrod,

yes, that's right, I have multiple measurements for both response and
predictors and these are measured on the same individuals. The model i'm
fitting is very similar to the model called "model_repeat2" from Modern
Phylogenetic Comparative Methods:

http://www.mpcm-evolution.org/practice/online-practical-material-chapter-11/chapter-11-2-multiple-measurements-model-mcmcglmm

same random effects structure, same between/within structure for the fixed
effects, and i'm also using the inverse of the matrix of phylogenetic
correlation.

@Dimitri: I'm aware of the de Villemereuil et al. approach, which, If I
understand correctly, does a version of orthogonal regression (in JAGS).
I'm trying find out if this is possible in MCMCglmm.

best,
Alberto

On Sun, Jan 3, 2016 at 12:05 PM, Dimitri Skandalis <da.skandalis at gmail.com>
wrote:

> Hi Alberto,
>
> Have you looked at the book Modern Phylogenetic Comparative Methods? R
> code provided with Chapter 11 (2) deals with correlated measurements, and
> could be a good place to start.
>
> http://www.mpcm-evolution.org/practice/online-practical-material-chapter-11
>
> Also, de Villemereuil et al. have developed an approach to related models
> in BUGS/JAGS.
>
> http://bmcevolbiol.biomedcentral.com/articles/10.1186/1471-2148-12-102
>
> Dimitri
>
> On Sun, Jan 3, 2016 at 8:16 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
> wrote:
>
>> Hi Alberto,
>>
>> When you say you have multiple observations for each species, do you mean
>> that you have multiple observations for the response and the predictors? Do
>> you expect the response and/or the predictors to be correlated at the
>> observation level (for example are they measured on the same individuals)?
>> I presume the answer to both these questions is yes if you wish to use the
>> van de Pol method?
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>> Quoting Alberto Gallano <alberto.gc8 at gmail.com> on Sun, 3 Jan 2016
>> 10:35:02 -0500:
>>
>> Hi Jarrod,
>>>
>>> I don't know the measurement error in the predictors in advance, so I
>>> guess
>>> it would need to be estimated simultaneously. I'm not 100% sure what you
>>> mean by 'multiple observations for each predictor variable'. I have data
>>> on
>>> 132 species and have multiple observations (7 to 80) for each species.
>>> I'm
>>> using a species level random effect and a phylogenetic covariance matrix
>>> (using ginverse) to account for phylogenetic autocorrelation, and I'm
>>> also
>>> using van de Pol and Wright's (2009) method for partitioning slopes into
>>> between- and within-species (i'm interested in the between species
>>> slope).
>>> My understanding is that neither of these things fits a model in which
>>> orthogonal residuals are minimized.
>>>
>>> best,
>>> Alberto
>>>
>>>
>>> On Sun, Jan 3, 2016 at 5:24 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
>>> wrote:
>>>
>>> Hi Alberto,
>>>>
>>>> Do you know the measurement error in the predictors in advance or do you
>>>> have multiple observations for each predictor variable and wish to
>>>> estimate
>>>> the error simultaneously?
>>>>
>>>> Cheers,
>>>>
>>>> Jarrod
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> Quoting Malcolm Fairbrother <M.Fairbrother at bristol.ac.uk> on Sat, 2 Jan
>>>> 2016 14:47:08 -0800:
>>>>
>>>> Dear Alberto (I believe),
>>>>
>>>>> To my knowledge, this is not possible in MCMCglmm (though Jarrod
>>>>> Hadfield,
>>>>> the package author, may weigh in with another response).
>>>>> A collaborator and I have been working on a paper that shows how to fit
>>>>> such models in JAGS (and perhaps Stan), though thus far we've only been
>>>>> able to fit such models correcting for measurement error in the
>>>>> predictors
>>>>> at the lowest level. Multiple such predictors (including with different
>>>>> measurement error variances) are no problem.
>>>>> That paper, however, is probably still some months away from being
>>>>> finished
>>>>> and presentable. In the meantime, I don't know of any good options for
>>>>> you.
>>>>> If other subscribers to this list have any ideas, I'll be quite
>>>>> interested
>>>>> too!
>>>>> - Malcolm
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> Date: Tue, 29 Dec 2015 16:09:53 -0500
>>>>>
>>>>> From: Alberto Gallano <alberto.gc8 at gmail.com>
>>>>>> To: r-sig-mixed-models at r-project.org
>>>>>> Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
>>>>>>         model?
>>>>>>
>>>>>> I posted this question on Stack Overflow a week ago but received no
>>>>>> answers:
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> http://stackoverflow.com/questions/34446618/bayesian-error-in-variables-total-least-squares-model-in-r-using-mcmcglmm
>>>>>>
>>>>>> This may be a more appropriate venue.
>>>>>>
>>>>>>
>>>>>> I am fitting some Bayesian linear mixed models using the MCMCglmm
>>>>>> package.
>>>>>> My data includes predictors that are measured with error. I'd
>>>>>> therefore
>>>>>> like to build a model that takes this into account. My understanding
>>>>>> is
>>>>>> that a basic mixed effects model in MCMCglmm will minimize error only
>>>>>> for
>>>>>> the response variable (as in frequentist OLS regression). In other
>>>>>> words,
>>>>>> vertical errors will be minimized. Instead, I'd like to minimize
>>>>>> errors
>>>>>> orthogonal to the regression line/plane/hyperplane.
>>>>>>
>>>>>>    1. Is it possible to fit an error-in-variables (aka total least
>>>>>> squares)
>>>>>>    model using MCMCglmm or would I have to use JAGS / STAN to do this?
>>>>>>    2. Is it possible to do this with multiple predictors in the same
>>>>>> model
>>>>>>    (I have some models with 3 or 4 predictors, each measured with
>>>>>> error)?
>>>>>>
>>>>>>
>>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>>
>>>>>
>>>>>
>>>>
>>>> --
>>>> The University of Edinburgh is a charitable body, registered in
>>>> Scotland, with registration number SC005336.
>>>>
>>>>
>>>>
>>>>
>>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>

	[[alternative HTML version deleted]]


From alberto.gc8 at gmail.com  Tue Jan  5 05:49:51 2016
From: alberto.gc8 at gmail.com (Alberto Gallano)
Date: Mon, 4 Jan 2016 23:49:51 -0500
Subject: [R-sig-ME] MCMCglmm error-in-variables (total least squares)
	model?
In-Reply-To: <20160104081605.30452h7fxzspouo0@www.staffmail.ed.ac.uk>
References: <CAAH-yP-jCvZ903MNUHFASVi_BDkNMkiUCvnFPnhNk4Y=H=0Pmw@mail.gmail.com>
	<20160103102427.12126bmxnimtdyvs@www.staffmail.ed.ac.uk>
	<CAO+b4j-VghYh6pjMNSJK+0YyTyShAMqLOf=GOBn4+FX4_f45kw@mail.gmail.com>
	<20160103161612.21125f2zso7a0mos@www.staffmail.ed.ac.uk>
	<CALV5z9POYX80wSqHR5qNBd6JHvdfAUczc85cWxF5TAc+Y8N_WA@mail.gmail.com>
	<CAO+b4j8QvfifmuUt+kHZXai=SPhay60=S9zA2ez5yv1Q3=37ug@mail.gmail.com>
	<20160104081605.30452h7fxzspouo0@www.staffmail.ed.ac.uk>
Message-ID: <CAO+b4j83BcnTUx2gqt5JX-MddczghYpsQR4U2xCK-=dLgtxazA@mail.gmail.com>

Hi Jarrod,

that's great - I think I understand what that model is doing. If you could
forward the information about the tutorial and paper (if it's published
yet) that would be very helpful. Thanks a lot.

best,
Alberto

On Mon, Jan 4, 2016 at 3:16 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:

> Hi,
>
> The least parsimonious model (and not the one I would necessarily
> recommend fitting) is:
>
>
> m1<-MCMCglmm(cbind(X1,X2,X3,Y)~trait,
>              random=~us(trait):species+us(trait):species.ide,
>              rcov=~us(trait):units,
>              ginverse=list(species=tree))
>
> where species and species.ide are columns of species names.
>
> This deals with the measurement error on the species means, and also
> allows you to address the fact that the regressions of the X's on Y may be
> different at different levels. The method advocated by van de Pol has the
> problem that the mean in the mean centering is just the observed mean
> rather than the true unobserved mean. For example, imagine that you only
> had one observation for some of the species.  You can obtain the regression
> coefficients at each level, by using the relationship beta =
> VAR(X)^{-1}COV(X,Y). For example, the posterior distribution of the
> regression coefficients at the phylogenetic level would be:
>
>
> reg.coef<-function(x, X=1:3, Y=4){
> V<-matrix(x,c(X,Y),c(X,Y))
> solve(V[X,X], V[X,Y])
> }
>
> apply(m1$VCV[,1:16], 1, reg.coef)
>
> The model doesn't deal with measurement error on the individual
> measurements, but if you had repeat measurements per individual you could
> also fit these (as a diagonal matrix, rather than unstructured).
>
> After taking into account measurement error, some people suggest that
> species.ide should be dropped from the model. I am not completely convinced
> by this argument.
>
> Priors are going to be a pain in this model.
>
> You could replace the us structures by ante3 structures. The model is then
> fitted directly in terms of the regression coefficients. Antedependence
> regression coefficients 3,5,6 are the regressions of X3, X2 and X1 on Y. If
> you are interested in this we have a mini-tutorial associated with a
> recently submitted paper I can send you.
>
> Cheers,
>
> Jarrod
>
>
>
>
>
>
>
> Quoting Alberto Gallano <alberto.gc8 at gmail.com> on Sun, 3 Jan 2016
> 15:45:26 -0500:
>
> Hi Jarrod,
>>
>> yes, that's right, I have multiple measurements for both response and
>> predictors and these are measured on the same individuals. The model i'm
>> fitting is very similar to the model called "model_repeat2" from Modern
>> Phylogenetic Comparative Methods:
>>
>>
>> http://www.mpcm-evolution.org/practice/online-practical-material-chapter-11/chapter-11-2-multiple-measurements-model-mcmcglmm
>>
>> same random effects structure, same between/within structure for the fixed
>> effects, and i'm also using the inverse of the matrix of phylogenetic
>> correlation.
>>
>> @Dimitri: I'm aware of the de Villemereuil et al. approach, which, If I
>> understand correctly, does a version of orthogonal regression (in JAGS).
>> I'm trying find out if this is possible in MCMCglmm.
>>
>> best,
>> Alberto
>>
>> On Sun, Jan 3, 2016 at 12:05 PM, Dimitri Skandalis <
>> da.skandalis at gmail.com>
>> wrote:
>>
>> Hi Alberto,
>>>
>>> Have you looked at the book Modern Phylogenetic Comparative Methods? R
>>> code provided with Chapter 11 (2) deals with correlated measurements, and
>>> could be a good place to start.
>>>
>>>
>>> http://www.mpcm-evolution.org/practice/online-practical-material-chapter-11
>>>
>>> Also, de Villemereuil et al. have developed an approach to related models
>>> in BUGS/JAGS.
>>>
>>> http://bmcevolbiol.biomedcentral.com/articles/10.1186/1471-2148-12-102
>>>
>>> Dimitri
>>>
>>> On Sun, Jan 3, 2016 at 8:16 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
>>> wrote:
>>>
>>> Hi Alberto,
>>>>
>>>> When you say you have multiple observations for each species, do you
>>>> mean
>>>> that you have multiple observations for the response and the
>>>> predictors? Do
>>>> you expect the response and/or the predictors to be correlated at the
>>>> observation level (for example are they measured on the same
>>>> individuals)?
>>>> I presume the answer to both these questions is yes if you wish to use
>>>> the
>>>> van de Pol method?
>>>>
>>>> Cheers,
>>>>
>>>> Jarrod
>>>>
>>>>
>>>> Quoting Alberto Gallano <alberto.gc8 at gmail.com> on Sun, 3 Jan 2016
>>>> 10:35:02 -0500:
>>>>
>>>> Hi Jarrod,
>>>>
>>>>>
>>>>> I don't know the measurement error in the predictors in advance, so I
>>>>> guess
>>>>> it would need to be estimated simultaneously. I'm not 100% sure what
>>>>> you
>>>>> mean by 'multiple observations for each predictor variable'. I have
>>>>> data
>>>>> on
>>>>> 132 species and have multiple observations (7 to 80) for each species.
>>>>> I'm
>>>>> using a species level random effect and a phylogenetic covariance
>>>>> matrix
>>>>> (using ginverse) to account for phylogenetic autocorrelation, and I'm
>>>>> also
>>>>> using van de Pol and Wright's (2009) method for partitioning slopes
>>>>> into
>>>>> between- and within-species (i'm interested in the between species
>>>>> slope).
>>>>> My understanding is that neither of these things fits a model in which
>>>>> orthogonal residuals are minimized.
>>>>>
>>>>> best,
>>>>> Alberto
>>>>>
>>>>>
>>>>> On Sun, Jan 3, 2016 at 5:24 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
>>>>> wrote:
>>>>>
>>>>> Hi Alberto,
>>>>>
>>>>>>
>>>>>> Do you know the measurement error in the predictors in advance or do
>>>>>> you
>>>>>> have multiple observations for each predictor variable and wish to
>>>>>> estimate
>>>>>> the error simultaneously?
>>>>>>
>>>>>> Cheers,
>>>>>>
>>>>>> Jarrod
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> Quoting Malcolm Fairbrother <M.Fairbrother at bristol.ac.uk> on Sat, 2
>>>>>> Jan
>>>>>> 2016 14:47:08 -0800:
>>>>>>
>>>>>> Dear Alberto (I believe),
>>>>>>
>>>>>> To my knowledge, this is not possible in MCMCglmm (though Jarrod
>>>>>>> Hadfield,
>>>>>>> the package author, may weigh in with another response).
>>>>>>> A collaborator and I have been working on a paper that shows how to
>>>>>>> fit
>>>>>>> such models in JAGS (and perhaps Stan), though thus far we've only
>>>>>>> been
>>>>>>> able to fit such models correcting for measurement error in the
>>>>>>> predictors
>>>>>>> at the lowest level. Multiple such predictors (including with
>>>>>>> different
>>>>>>> measurement error variances) are no problem.
>>>>>>> That paper, however, is probably still some months away from being
>>>>>>> finished
>>>>>>> and presentable. In the meantime, I don't know of any good options
>>>>>>> for
>>>>>>> you.
>>>>>>> If other subscribers to this list have any ideas, I'll be quite
>>>>>>> interested
>>>>>>> too!
>>>>>>> - Malcolm
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> Date: Tue, 29 Dec 2015 16:09:53 -0500
>>>>>>>
>>>>>>> From: Alberto Gallano <alberto.gc8 at gmail.com>
>>>>>>>
>>>>>>>> To: r-sig-mixed-models at r-project.org
>>>>>>>> Subject: [R-sig-ME] MCMCglmm error-in-variables (total least
>>>>>>>> squares)
>>>>>>>>         model?
>>>>>>>>
>>>>>>>> I posted this question on Stack Overflow a week ago but received no
>>>>>>>> answers:
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> http://stackoverflow.com/questions/34446618/bayesian-error-in-variables-total-least-squares-model-in-r-using-mcmcglmm
>>>>>>>>
>>>>>>>> This may be a more appropriate venue.
>>>>>>>>
>>>>>>>>
>>>>>>>> I am fitting some Bayesian linear mixed models using the MCMCglmm
>>>>>>>> package.
>>>>>>>> My data includes predictors that are measured with error. I'd
>>>>>>>> therefore
>>>>>>>> like to build a model that takes this into account. My understanding
>>>>>>>> is
>>>>>>>> that a basic mixed effects model in MCMCglmm will minimize error
>>>>>>>> only
>>>>>>>> for
>>>>>>>> the response variable (as in frequentist OLS regression). In other
>>>>>>>> words,
>>>>>>>> vertical errors will be minimized. Instead, I'd like to minimize
>>>>>>>> errors
>>>>>>>> orthogonal to the regression line/plane/hyperplane.
>>>>>>>>
>>>>>>>>    1. Is it possible to fit an error-in-variables (aka total least
>>>>>>>> squares)
>>>>>>>>    model using MCMCglmm or would I have to use JAGS / STAN to do
>>>>>>>> this?
>>>>>>>>    2. Is it possible to do this with multiple predictors in the same
>>>>>>>> model
>>>>>>>>    (I have some models with 3 or 4 predictors, each measured with
>>>>>>>> error)?
>>>>>>>>
>>>>>>>>
>>>>>>>>         [[alternative HTML version deleted]]
>>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>> --
>>>>>> The University of Edinburgh is a charitable body, registered in
>>>>>> Scotland, with registration number SC005336.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>> --
>>>> The University of Edinburgh is a charitable body, registered in
>>>> Scotland, with registration number SC005336.
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>>
>>>
>>>
>>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>
>

	[[alternative HTML version deleted]]


From alberto.gc8 at gmail.com  Wed Jan  6 23:39:33 2016
From: alberto.gc8 at gmail.com (Alberto Gallano)
Date: Wed, 6 Jan 2016 17:39:33 -0500
Subject: [R-sig-ME] MCMCglmm predictions using new data error
Message-ID: <CAO+b4j8wCNw6UEHocir2W+jeGXucMz9216dC3e-CgQ3y1SpcXQ@mail.gmail.com>

I'm trying to make predictions from an MCMCglmm model using new data. This
is a feature that was recently added to the predict.MCMCglmm function
(version 2.22). However, when I set things up as I would for other predict
methods, I get the following error:

> Error in eval(expr, envir, enclos) : object 'y' not found

where 'y' is my response vector. I'm including a simplified replicable
example below. Is the set up of the prediction grid different for the
MCMCglmm predict method compared with other methods?

best,
Alberto

#
----------------------------------------------------------------------------------
library(MCMCglmm)

set.seed(123)

dat <- data.frame(x = rnorm(100), y = rnorm(100))


fit <- MCMCglmm(

    fixed = y ~ x,

    rcov = ~ units,

    data = dat,

    family = "gaussian",

    pr = TRUE, pl = TRUE,

    saveX = TRUE,  saveZ = TRUE,

    nitt = 1.3e+4, thin = 10, burnin = 3e+3

)


pred_grid <- data.frame(x = seq(-1, 1, length.out = 30))


predict(fit, newdata = pred_grid)

#
----------------------------------------------------------------------------------

	[[alternative HTML version deleted]]


From sven.templer at gmail.com  Thu Jan  7 21:06:16 2016
From: sven.templer at gmail.com (Sven E. Templer)
Date: Thu, 7 Jan 2016 21:06:16 +0100
Subject: [R-sig-ME] MCMCglmm predictions using new data error
In-Reply-To: <CAO+b4j8wCNw6UEHocir2W+jeGXucMz9216dC3e-CgQ3y1SpcXQ@mail.gmail.com>
References: <CAO+b4j8wCNw6UEHocir2W+jeGXucMz9216dC3e-CgQ3y1SpcXQ@mail.gmail.com>
Message-ID: <052754C5-E195-4867-9A0E-43F9E5E225B1@icloud.com>

Hi Alberto,

avoid the error from predict by running 

pred_grid$y <- 0

before

predict(fit, newdata = pred_grid)

Best,
Sven

> On 06 Jan 2016, at 23:39, Alberto Gallano <alberto.gc8 at gmail.com> wrote:
> 
> I'm trying to make predictions from an MCMCglmm model using new data. This
> is a feature that was recently added to the predict.MCMCglmm function
> (version 2.22). However, when I set things up as I would for other predict
> methods, I get the following error:
> 
>> Error in eval(expr, envir, enclos) : object 'y' not found
> 
> where 'y' is my response vector. I'm including a simplified replicable
> example below. Is the set up of the prediction grid different for the
> MCMCglmm predict method compared with other methods?
> 
> best,
> Alberto
> 
> #
> ----------------------------------------------------------------------------------
> library(MCMCglmm)
> 
> set.seed(123)
> 
> dat <- data.frame(x = rnorm(100), y = rnorm(100))
> 
> 
> fit <- MCMCglmm(
> 
>    fixed = y ~ x,
> 
>    rcov = ~ units,
> 
>    data = dat,
> 
>    family = "gaussian",
> 
>    pr = TRUE, pl = TRUE,
> 
>    saveX = TRUE,  saveZ = TRUE,
> 
>    nitt = 1.3e+4, thin = 10, burnin = 3e+3
> 
> )
> 
> 
> pred_grid <- data.frame(x = seq(-1, 1, length.out = 30))
> 
> 
> predict(fit, newdata = pred_grid)
> 
> #
> ----------------------------------------------------------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From sven.templer at gmail.com  Sat Jan  9 08:21:09 2016
From: sven.templer at gmail.com (Sven Templer)
Date: Sat, 9 Jan 2016 08:21:09 +0100
Subject: [R-sig-ME] MCMCglmm predictions using new data error
In-Reply-To: <CAO+b4j-uRjS3_hGR+hTUxLAchs8-N3Kne9=fxtZDa0q5Jmvtsw@mail.gmail.com>
References: <CAO+b4j8wCNw6UEHocir2W+jeGXucMz9216dC3e-CgQ3y1SpcXQ@mail.gmail.com>
	<052754C5-E195-4867-9A0E-43F9E5E225B1@icloud.com>
	<CAO+b4j-uRjS3_hGR+hTUxLAchs8-N3Kne9=fxtZDa0q5Jmvtsw@mail.gmail.com>
Message-ID: <97A9D965-7B51-470E-8D63-DCCD95625CEE@icloud.com>

Hi Alberto,

see comments inline.

Best,
Sven

> On 08 Jan 2016, at 17:51, Alberto Gallano <alberto.gc8 at gmail.com> wrote:
> 
> Hi Sven,
> 
> thanks so much. That works. Do you know if there is any documentation as to how to set up new data for the MCMCglmm predict method? Adding a constant response vector to the prediction grid is different to every other R predict method (that I know of).

I just interpreted the error message and looked at the predict.MCMCglmm method. In the documentation the description is not very detailed and I am not aware of other sources. Maybe have a look at the vignettes? 

> 
> One further problem i'm having is that I actually have random effects, and now the predict function throws an error when these are not entered into the prediction grid. It's unclear to me how they should be entered - using a constant doesn't work. The number of random effect grouping levels may not equal the dimensions of the rest of the prediction grid, so just adding a vector with each unique grouping level won't work here either. Any thoughts would be much appreciated. See "pred_grid2" at bottom of code below:
> 
> 
> # ----------------------------------------------------------------------------------
> library(MCMCglmm)
> 
> set.seed(123)
> 
> dat <- data.frame(
>     y = rnorm(100),
>     x = rnorm(100), 
>     id = factor(rep(paste("ID", 1:20), 5))
>     )
> 
> fit <- MCMCglmm(
>     fixed = y ~ x,
>     random = ~ id,
>     rcov = ~ units,
>     data = dat,
>     family = "gaussian",
>     pr = TRUE, pl = TRUE,
>     saveX = TRUE,  saveZ = TRUE,
>     nitt = 1.3e+4, thin = 10, burnin = 3e+3
> )
> 
> 
> pred_grid1 <- with(dat, 
>     data.frame( 
>     y = 0,
>     x = seq(-1, 1, length.out = 30)
>     ))
> 
> predict(fit, newdata = pred_grid1)
> 
> # > Error in buildZ(rmodel.terms[r], data = data, nginverse = names(ginverse)) : object id not found
>   
>   
> pred_grid2 <- with(dat, 
>     data.frame( 
>     y = 0,
>     x = seq(-1, 1, length.out = 30),
>     id = 0
>     ))
> 
> predict(fit, newdata = pred_grid2)
> 
> # > Error in MCMCglmm(fixed = object$Fixed$formula, random = object$Random$formula,  : trying to get slot "Dim" from an object of a basic class ("numeric") with no slots

Again I would investigate the error message 'trying to get slot "Dim"'. I looks like it expects a class of the 'Matrix' package. But right now I have no idea how to solve it and do not know of any examples.

> # ----------------------------------------------------------------------------------
> 
> On Thu, Jan 7, 2016 at 3:06 PM, Sven E. Templer <sven.templer at gmail.com> wrote:
> Hi Alberto,
> 
> avoid the error from predict by running
> 
> pred_grid$y <- 0
> 
> before
> 
> predict(fit, newdata = pred_grid)
> 
> Best,
> Sven
> 
> > On 06 Jan 2016, at 23:39, Alberto Gallano <alberto.gc8 at gmail.com> wrote:
> >
> > I'm trying to make predictions from an MCMCglmm model using new data. This
> > is a feature that was recently added to the predict.MCMCglmm function
> > (version 2.22). However, when I set things up as I would for other predict
> > methods, I get the following error:
> >
> >> Error in eval(expr, envir, enclos) : object 'y' not found
> >
> > where 'y' is my response vector. I'm including a simplified replicable
> > example below. Is the set up of the prediction grid different for the
> > MCMCglmm predict method compared with other methods?
> >
> > best,
> > Alberto
> >
> > #
> > ----------------------------------------------------------------------------------
> > library(MCMCglmm)
> >
> > set.seed(123)
> >
> > dat <- data.frame(x = rnorm(100), y = rnorm(100))
> >
> >
> > fit <- MCMCglmm(
> >
> >    fixed = y ~ x,
> >
> >    rcov = ~ units,
> >
> >    data = dat,
> >
> >    family = "gaussian",
> >
> >    pr = TRUE, pl = TRUE,
> >
> >    saveX = TRUE,  saveZ = TRUE,
> >
> >    nitt = 1.3e+4, thin = 10, burnin = 3e+3
> >
> > )
> >
> >
> > pred_grid <- data.frame(x = seq(-1, 1, length.out = 30))
> >
> >
> > predict(fit, newdata = pred_grid)
> >
> > #
> > ----------------------------------------------------------------------------------
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 


From chirleu at gmail.com  Mon Jan 11 11:38:47 2016
From: chirleu at gmail.com (=?UTF-8?Q?David_Villegas_R=C3=ADos?=)
Date: Mon, 11 Jan 2016 11:38:47 +0100
Subject: [R-sig-ME] random effects plot
Message-ID: <CALC46t_jOAWpLfvdw3kAhHC1fyByg6fqq+gEw=r-N6kRNVmArA@mail.gmail.com>

Hi all

I?m running a MCMCglmm mixed model in which the random effect is the
interaction between the individual identity and the season of the year
(categorical variable, four levels). The response variable is a behavioral
trait (dvm).

Basically I?m trying to figure out if the individuals adjust their behavior
over the seasons in a different way (individual plasticity).



model=MCMCglmm(dvm~season+year,random=~us(season):ID,rcov=~idh(season):units,data=df,prior=prior1,nitt=110000,thin=100,burnin=10000,
pr=TRUE)



I?m trying now to plot the random effects (the mode I guess) for each
season. So the plot I need is ?season? in the x-axis and then the random
effects on the y-axis. The different values of each individual for each
season will be linked by a line, so I can have a way to visualize the
reaction norm for each individual.



Can you advise me on how to  proceed? I guess what I want is stored in
model$Sol ??



Many thanks and happy new year.



David

	[[alternative HTML version deleted]]


From alberto.gc8 at gmail.com  Fri Jan  8 17:51:33 2016
From: alberto.gc8 at gmail.com (Alberto Gallano)
Date: Fri, 8 Jan 2016 11:51:33 -0500
Subject: [R-sig-ME] MCMCglmm predictions using new data error
In-Reply-To: <052754C5-E195-4867-9A0E-43F9E5E225B1@icloud.com>
References: <CAO+b4j8wCNw6UEHocir2W+jeGXucMz9216dC3e-CgQ3y1SpcXQ@mail.gmail.com>
	<052754C5-E195-4867-9A0E-43F9E5E225B1@icloud.com>
Message-ID: <CAO+b4j-uRjS3_hGR+hTUxLAchs8-N3Kne9=fxtZDa0q5Jmvtsw@mail.gmail.com>

Hi Sven,

thanks so much. That works. Do you know if there is any documentation as to
how to set up new data for the MCMCglmm predict method? Adding a constant
response vector to the prediction grid is different to every other R
predict method (that I know of).

One further problem i'm having is that I actually have random effects, and
now the predict function throws an error when these are not entered into
the prediction grid. It's unclear to me how they should be entered - using
a constant doesn't work. The number of random effect grouping levels may
not equal the dimensions of the rest of the prediction grid, so just adding
a vector with each unique grouping level won't work here either. Any
thoughts would be much appreciated. See "pred_grid2" at bottom of code
below:


#
----------------------------------------------------------------------------------
library(MCMCglmm)

set.seed(123)

dat <- data.frame(
    y = rnorm(100),
    x = rnorm(100),
    id = factor(rep(paste("ID", 1:20), 5))
    )

fit <- MCMCglmm(
    fixed = y ~ x,
    random = ~ id,
    rcov = ~ units,
    data = dat,
    family = "gaussian",
    pr = TRUE, pl = TRUE,
    saveX = TRUE,  saveZ = TRUE,
    nitt = 1.3e+4, thin = 10, burnin = 3e+3
)


pred_grid1 <- with(dat,
    data.frame(
    y = 0,
    x = seq(-1, 1, length.out = 30)
    ))

predict(fit, newdata = pred_grid1)

# > Error in buildZ(rmodel.terms[r], data = data, nginverse =
names(ginverse)) : object id not found


pred_grid2 <- with(dat,
    data.frame(
    y = 0,
    x = seq(-1, 1, length.out = 30),
    id = 0
    ))

predict(fit, newdata = pred_grid2)

# > Error in MCMCglmm(fixed = object$Fixed$formula, random =
object$Random$formula,  : trying to get slot "Dim" from an object of a
basic class ("numeric") with no slots
#
----------------------------------------------------------------------------------

On Thu, Jan 7, 2016 at 3:06 PM, Sven E. Templer <sven.templer at gmail.com>
wrote:

> Hi Alberto,
>
> avoid the error from predict by running
>
> pred_grid$y <- 0
>
> before
>
> predict(fit, newdata = pred_grid)
>
> Best,
> Sven
>
> > On 06 Jan 2016, at 23:39, Alberto Gallano <alberto.gc8 at gmail.com> wrote:
> >
> > I'm trying to make predictions from an MCMCglmm model using new data.
> This
> > is a feature that was recently added to the predict.MCMCglmm function
> > (version 2.22). However, when I set things up as I would for other
> predict
> > methods, I get the following error:
> >
> >> Error in eval(expr, envir, enclos) : object 'y' not found
> >
> > where 'y' is my response vector. I'm including a simplified replicable
> > example below. Is the set up of the prediction grid different for the
> > MCMCglmm predict method compared with other methods?
> >
> > best,
> > Alberto
> >
> > #
> >
> ----------------------------------------------------------------------------------
> > library(MCMCglmm)
> >
> > set.seed(123)
> >
> > dat <- data.frame(x = rnorm(100), y = rnorm(100))
> >
> >
> > fit <- MCMCglmm(
> >
> >    fixed = y ~ x,
> >
> >    rcov = ~ units,
> >
> >    data = dat,
> >
> >    family = "gaussian",
> >
> >    pr = TRUE, pl = TRUE,
> >
> >    saveX = TRUE,  saveZ = TRUE,
> >
> >    nitt = 1.3e+4, thin = 10, burnin = 3e+3
> >
> > )
> >
> >
> > pred_grid <- data.frame(x = seq(-1, 1, length.out = 30))
> >
> >
> > predict(fit, newdata = pred_grid)
> >
> > #
> >
> ----------------------------------------------------------------------------------
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

	[[alternative HTML version deleted]]


From diego.pavonjordan at gmail.com  Tue Jan 12 22:31:31 2016
From: diego.pavonjordan at gmail.com (Diego Pavon)
Date: Tue, 12 Jan 2016 23:31:31 +0200
Subject: [R-sig-ME] glmmadmb - problems understanding the output when run in
	cmd console
Message-ID: <CAD93_FrRR-LStJ9ykBcQ2k8L4JMdZ_H=PV2xogCYotE5BmvG6g@mail.gmail.com>

Hello all

I am trying to run GLMM, ZIP and ZINB using glmmadmb package. I have count
data and I want to see whether there is an effect of temperature on the
counts of 12 species over a period of 29 years (counts in ca. 2000 sites).
I am having troubles getting the models converged when running them in
RStudio. For some reason, they do run nicely when I use the windows command
line (cmd).
Then I get all these files with the results (.eval, .par, .std, etc...).

My question is, how do I read those? I can open them in R or a text editor,
but I can't understand what is what. In the glmmadmb website there is a
small description but I can't really understand. for instance, the betas
that I get in the .par file are not the actual estimates. In the website,
It says:

"BETA: fixed-effect parameter estimates: note that these are the versions
of the parameters fitted internally, using an orthogonalized version of the
original design matrix, not the original coefficients (if this means
nothing to you, as it might well, just accept that these are transformed
versions of the parameters)."

So, could someone help me understanding these "results files" that I get
when running glmmadmb outside R, and how to get the estimates, SE and
significance of the covariates?

Thank you very much.

Diego






-- 
*Diego Pav?n Jord?n*

*Finnish Museum of Natural History*
*PO BOX 17 *

*Helsinki. Finland*



*0445061210https://tuhat.halvi.helsinki.fi/portal/en/persons/diego-pavon-jordan%288d5db37c-eddd-4fca-92cd-9c9956a42b4a%29.html
<https://tuhat.halvi.helsinki.fi/portal/en/persons/diego-pavon-jordan%288d5db37c-eddd-4fca-92cd-9c9956a42b4a%29.html>http://www.linkedin.com/profile/view?id=170617924&trk=nav_responsive_tab_profile
<http://www.linkedin.com/profile/view?id=170617924&trk=nav_responsive_tab_profile>https://helsinki.academia.edu/DiegoPavon
<https://helsinki.academia.edu/DiegoPavon>*

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Wed Jan 13 01:01:18 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 12 Jan 2016 19:01:18 -0500
Subject: [R-sig-ME] glmmadmb - problems understanding the output when
 run in cmd console
In-Reply-To: <CAD93_FrRR-LStJ9ykBcQ2k8L4JMdZ_H=PV2xogCYotE5BmvG6g@mail.gmail.com>
References: <CAD93_FrRR-LStJ9ykBcQ2k8L4JMdZ_H=PV2xogCYotE5BmvG6g@mail.gmail.com>
Message-ID: <CABghstTZYMmnwTkhgZjS3VafE=iGM_AAEpXWfByQL9QoMESteg@mail.gmail.com>

  In the long run it would probably be better to figure out why
glmmADMB isn't running in RStudio ... it might also be interesting to
see whether the glmmTMB (aka the New Hotness,
https://github.com/glmmTMB/glmmTMB) might work.

  In the short run, what you need to know is probably documented in
?admbControl:

if you want to try running the ADMB model outside of R,
              use ?run=FALSE? and set ?save.dir? in the ?glmmADMB?
              arguments.  This will result in a saved directory
              containing the ?glmmadmb? executable (on systems other
              than Windows) as well as ?glmmadmb.pin? and
              ?glmmadmb.dat? files.  You can then run the ?glmmadmb?
              executable with appropriate command-line arguments in a
              command shell or terminal window.  If you run the same R
              command again (i.e. still with ?run=FALSE? and with the
              same ?save.dir?) ?glmmADMB? will find any output files
              that have been produced and read them into a ?glmmadmb?
              object.

In other words, if you run

   glmmadmb(...,run=FALSE)

after running the ADMB executable manually from the shell, then the
function should automatically locate the output files and read them
into a well-set-up object for you.

  Ben Bolker

On Tue, Jan 12, 2016 at 4:31 PM, Diego Pavon
<diego.pavonjordan at gmail.com> wrote:
> Hello all
>
> I am trying to run GLMM, ZIP and ZINB using glmmadmb package. I have count
> data and I want to see whether there is an effect of temperature on the
> counts of 12 species over a period of 29 years (counts in ca. 2000 sites).
> I am having troubles getting the models converged when running them in
> RStudio. For some reason, they do run nicely when I use the windows command
> line (cmd).
> Then I get all these files with the results (.eval, .par, .std, etc...).
>
> My question is, how do I read those? I can open them in R or a text editor,
> but I can't understand what is what. In the glmmadmb website there is a
> small description but I can't really understand. for instance, the betas
> that I get in the .par file are not the actual estimates. In the website,
> It says:
>
> "BETA: fixed-effect parameter estimates: note that these are the versions
> of the parameters fitted internally, using an orthogonalized version of the
> original design matrix, not the original coefficients (if this means
> nothing to you, as it might well, just accept that these are transformed
> versions of the parameters)."
>
> So, could someone help me understanding these "results files" that I get
> when running glmmadmb outside R, and how to get the estimates, SE and
> significance of the covariates?
>
> Thank you very much.
>
> Diego
>
>
>
>
>
>
> --
> *Diego Pav?n Jord?n*
>
> *Finnish Museum of Natural History*
> *PO BOX 17 *
>
> *Helsinki. Finland*
>
>
>
> *0445061210https://tuhat.halvi.helsinki.fi/portal/en/persons/diego-pavon-jordan%288d5db37c-eddd-4fca-92cd-9c9956a42b4a%29.html
> <https://tuhat.halvi.helsinki.fi/portal/en/persons/diego-pavon-jordan%288d5db37c-eddd-4fca-92cd-9c9956a42b4a%29.html>http://www.linkedin.com/profile/view?id=170617924&trk=nav_responsive_tab_profile
> <http://www.linkedin.com/profile/view?id=170617924&trk=nav_responsive_tab_profile>https://helsinki.academia.edu/DiegoPavon
> <https://helsinki.academia.edu/DiegoPavon>*
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From T.M.Spatgens at uva.nl  Fri Jan 15 14:55:49 2016
From: T.M.Spatgens at uva.nl (=?iso-8859-1?Q?Sp=E4tgens=2C_Tessa?=)
Date: Fri, 15 Jan 2016 13:55:49 +0000
Subject: [R-sig-ME] Removing random correlation parameter for categorical
 variable in lmer
Message-ID: <390163A25E622A43932D1B0730EECEA04D1A39C4@MBX04.uva.nl>

Dear all,

I am still learning how to use mixed effects models and ran into a problem I cannot seem to solve using the advice I can find in articles and on various platforms, so I am hoping someone on the list can help me out!

For a study involving a semantic priming experiment, I want to extract the random slope terms as a measure of semantic priming, using lmer. The related and unrelated items are coded 0 and 1, respectively. I want to extract the individual random slope terms for the unrelated items, as a deviation from the related items (intercept). However, the model shows a perfect correlation between the random intercepts and slopes. I have read that this means that the model is overspecified and that, if removing the random effects altogether is not an option, you can try to remove the correlation parameter itself. However, trying to do this using the formula (0+x|y) still yields a correlation parameter for the subjects with variance 0. The formula looks like this:


Response_time ~ relatedness +  mean_response_time + (0 + relatedness | subject) + (1 | word) + (1 | group)

I think this has to do with the fact that it is a categorical variable, as trying it with a (different) continuous variable does work. Is there any way to remove the correlation parameter in this situation?

Thanks in advance for any advice!

Tessa


	[[alternative HTML version deleted]]


From bates at stat.wisc.edu  Fri Jan 15 18:02:24 2016
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 15 Jan 2016 17:02:24 +0000
Subject: [R-sig-ME] Removing random correlation parameter for
 categorical variable in lmer
In-Reply-To: <390163A25E622A43932D1B0730EECEA04D1A39C4@MBX04.uva.nl>
References: <390163A25E622A43932D1B0730EECEA04D1A39C4@MBX04.uva.nl>
Message-ID: <CAO7JsnSb0FpkWxwbkn4B4dvcReZmobgOtG1wy89YoX+OV1JZCg@mail.gmail.com>

For a numeric covariate x, the model expression 0 + x produces a model
matrix with a single column so you can suppress the correlation by using
terms like (1 | subject) + (0 + x | subject).  However, for a categorical
covariate like relatedness with 2 levels the expressions (g | subject), (1
+ g | subect) and (0 + g | subject) are equivalent in terms of the
predictions from the fitted model.  The only difference between the last
two is in the "contrasts" that they generate.

If you really want independent random effects for intercept and for the
effect of relatedness you should convert the relatedness factor to a
numeric covariate.  One possibility is (1 | subject) + (0 +
I(as.integer(relatedness) - 1) | subject).

The fact the the first model you fit (with the correlation) is degenerate
is indeed a warning that you are overfitting the data.  I would try to do
some graphical exploration of the data, or perhaps of the residuals from
fitting a model like

Response_time ~ relatedness + mean_response_time +  (1|subject) +
(1|word) + (1|group)

before trying to incorporate exotic random slopes.

On Fri, Jan 15, 2016 at 7:56 AM Sp?tgens, Tessa <T.M.Spatgens at uva.nl> wrote:

> Dear all,
>
> I am still learning how to use mixed effects models and ran into a problem
> I cannot seem to solve using the advice I can find in articles and on
> various platforms, so I am hoping someone on the list can help me out!
>
> For a study involving a semantic priming experiment, I want to extract the
> random slope terms as a measure of semantic priming, using lmer. The
> related and unrelated items are coded 0 and 1, respectively. I want to
> extract the individual random slope terms for the unrelated items, as a
> deviation from the related items (intercept). However, the model shows a
> perfect correlation between the random intercepts and slopes. I have read
> that this means that the model is overspecified and that, if removing the
> random effects altogether is not an option, you can try to remove the
> correlation parameter itself. However, trying to do this using the formula
> (0+x|y) still yields a correlation parameter for the subjects with variance
> 0. The formula looks like this:
>
>
> Response_time ~ relatedness +  mean_response_time + (0 + relatedness |
> subject) + (1 | word) + (1 | group)
>
> I think this has to do with the fact that it is a categorical variable, as
> trying it with a (different) continuous variable does work. Is there any
> way to remove the correlation parameter in this situation?
>
> Thanks in advance for any advice!
>
> Tessa
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Sun Jan 17 02:04:09 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 17 Jan 2016 01:04:09 +0000 (UTC)
Subject: [R-sig-ME] Removing random correlation parameter for
	categorical variable in lmer
References: <390163A25E622A43932D1B0730EECEA04D1A39C4@MBX04.uva.nl>
	<CAO7JsnSb0FpkWxwbkn4B4dvcReZmobgOtG1wy89YoX+OV1JZCg@mail.gmail.com>
Message-ID: <loom.20160117T013530-958@post.gmane.org>

Douglas Bates <bates at ...> writes:

> 
> For a numeric covariate x, the model expression 0 + x produces a model
> matrix with a single column so you can suppress the correlation by using
> terms like (1 | subject) + (0 + x | subject).  However, for a categorical
> covariate like relatedness with 2 levels the expressions (g | subject), (1
> + g | subect) and (0 + g | subject) are equivalent in terms of the
> predictions from the fitted model.  The only difference between the last
> two is in the "contrasts" that they generate.
> 
> If you really want independent random effects for intercept and for the
> effect of relatedness you should convert the relatedness factor to a
> numeric covariate.  One possibility is (1 | subject) + (0 +
> I(as.integer(relatedness) - 1) | subject).


   Another (equivalent but more readable) possibility is to use the dummy

(1|subject) + (0+dummy(unrelated,"1"))

  (I *think* this is right -- if I am reading correctly, you have
 a categorical variable coded as "0", "1", which is a little confusing ...


From bbolker at gmail.com  Sun Jan 17 18:49:48 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 17 Jan 2016 12:49:48 -0500
Subject: [R-sig-ME] lme4 for Win_10
In-Reply-To: <DUB117-W54B9B9C9B4A2251FB5E5E8CFCF0@phx.gbl>
References: <DUB117-W54B9B9C9B4A2251FB5E5E8CFCF0@phx.gbl>
Message-ID: <569BD43C.2040306@gmail.com>


   I'm forwarding this to r-sig-mixed-models as I think it might be of 
general interest.

   There doesn't seem to be anything wrong with the lme4 directory on 
CRAN <https://cran.r-project.org/web/packages/lme4/index.html>; my best 
guess is that your problem has something to do with the recent release 
of 3.2.3 and some kind of delay in the CRAN/mirror-updating pipeline. 
(As far as I know there should be no differences between Windows 7 and 
10 in this respect.)  I would try again with a different mirror, 
possibly even with the main CRAN site (cran.r-project.org).

    Looking at the CRAN page cited above, version 1.1-10 is available 
for all operating systems/versions of R (except R version 3.1.x on MacOS 
Snow Leopard, which is still back at 1.1-7). Installing from source can 
be helpful in edge cases (it does require compilation tools).

   Ben Bolker

On 16-01-17 08:05 AM, andre wolanowski wrote:
> Dear Sir,
> ---
> The package lme4 did not cause any problems in my computer with
> OS Windows_7. However, an attempt to install it in a computer equipped
> with Windows_10 and R_x64_3.2.3 resulted in the message ?This package is
> not available for R_3.2.3?.
> ---
> It would be appreciated if you could kindly advise me of:  (a) When a
> Windows_10 compatible version of lme4 is likely to be available? (b)
> Which version of R will it be available for?
> ---
> Thank you in advance for your cooperation. Yours sincerely ? Andrew
> Wolanowski


From timothy.s.lau at gmail.com  Mon Jan 18 07:52:27 2016
From: timothy.s.lau at gmail.com (Timothy Lau)
Date: Mon, 18 Jan 2016 01:52:27 -0500
Subject: [R-sig-ME] Unstandardizing GLMM Regression Coefficients
Message-ID: <CAJM8_pnDb5QFVtofkgBmzCHgmYSK1m-dN21+nHg4VimxMq-jWQ@mail.gmail.com>

Hello,

lme4 mentioned that I should consider rescaling some of my predictor
variables because of the scale differences:

*fit warnings:*
*Some predictor variables are on very different scales: consider rescaling*



I have since been trying to expand Ben Bolker's function:

*#
http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon
<http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon>*
*rescale.coefs <- function(beta,mu,sigma) {*
*  beta2 <- beta ## inherit names etc.*
*  beta2[-1] <- sigma[1]*beta[-1]/sigma[-1]*
*  beta2[1]  <- sigma[1]*beta[1]+mu[1]-sum(beta2[-1]*mu[-1])*
*  beta2*
*}*
*# regular model*
*m1 <- lm(Illiteracy~.,as.data.frame(state.x77))*
*b1 <- coef(m1)*
*# Make a scaled version of the data*
*ss <- scale(state.x77)*
*# Scaled coefficients:*
*m1S <- update(m1,data=as.data.frame(ss))*
*b1S <- coef(m1S)*
*# rescaling*
*icol <- which(colnames(state.x77)=="Illiteracy")*
*p.order <- c(icol,(1:ncol(state.x77))[-icol])*
*m <- colMeans(state.x77)[p.order]*
*s <- apply(state.x77,2,sd)[p.order]*
*all.equal(b1,rescale.coefs(b1S,m,s))  ## TRUE*



to work with GLMM models (e.g., count data) that can't have the outcome
standardized, only the predictors:
*# function assumes scaled data is within model; no NA's and only used
variables are in orig.data*
*unscale.coef.mer <- function(model, orig.data) {*
*  require(lme4)*
*  ran.ef <- as.character(attr(attr(model at frame,
"terms"),"predvars.random"))[-1:-2] # random effects*
*  offset <- attr(attr(model at frame, "terms"), "offset")*
*  scaled.data <- model at frame[,!names(model at frame) %in% c(ran.ef, offset)]
# the scaled covariate(s) and outcome used in the model*
*  orig.data <- orig.data[,!names(orig.data) %in% c(ran.ef, offset)]*
*  beta <- fixef(model) # the fixed effects*
*  outvar <- names(model at frame)[attr(attr(m3 at frame, "terms"), "response")]
# the outcoem var. name*
*  icol <- which(colnames(orig.data) == outvar) # outcome variable column
number*
*  p.order <- c(icol, (1:ncol(orig.data))[-icol]) # full list of variable
names with outcome 1st*
*  mu <- colMeans(x = orig.data)[p.order] # variable means with outcome 1st*
*  sigma <- apply(X = orig.data, MARGIN = 2, FUN = sd)[p.order] # variable
SDs with outcome 1st*
*  beta2 <- beta # inherit names etc.*
*  beta2[-1] <- sigma[1] * beta[-1] / sigma[-1] # the fixed effects except
intercept*
*  beta2[1]  <- sigma[1] * beta[1] + mu[1] - sum(beta2[-1] * mu[-1]) # the
FE intercept*
*  return(beta2)*
*}*


?Could someone help me ?with backing out or unscaling the coefficeints so
that m2 coef matches m1 even though m2 used the standardized data (example
data):
*# warnings due to scaling*
*m1 <- glmer(formula = awards ~ 1 + write + (1 | cid), family = poisson,
data = foreign::read.dta(file =
"http://www.ats.ucla.edu/stat/data/hsbdemo.dta
<http://www.ats.ucla.edu/stat/data/hsbdemo.dta>"))*

*?# fixed scaling but need to back out coefficients to original scale for
interpretative reasons?m?2? <- glmer(formula = awards ~ 1 +
?scale(?write?)? + (1 | cid), family = poisson, data =
foreign::read.dta(file = "http://www.ats.ucla.edu/stat/data/hsbdemo.dta
<http://www.ats.ucla.edu/stat/data/hsbdemo.dta>"))*



Best,
Timothy



"The will to win means nothing without the will to prepare."
http://en.wikipedia.org/wiki/Juma_Ikangaa

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Jan 18 09:10:53 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 18 Jan 2016 09:10:53 +0100
Subject: [R-sig-ME] Unstandardizing GLMM Regression Coefficients
In-Reply-To: <CAJM8_pnDb5QFVtofkgBmzCHgmYSK1m-dN21+nHg4VimxMq-jWQ@mail.gmail.com>
References: <CAJM8_pnDb5QFVtofkgBmzCHgmYSK1m-dN21+nHg4VimxMq-jWQ@mail.gmail.com>
Message-ID: <CAJuCY5wkcQsOnehjc61rnM-SWc=s=bW1xkkTqYVzfXw8=MQMUA@mail.gmail.com>

Dear Timothy,

Please don't post in HTML. It makes code unreadable.
Please post code that can be easily reproduced: a simple copy/paste should
work.

You might want to consider a very simple scaling by just using expressing
the variables in another unit (= multiplying by some power of 10). E.g.
kilometers to meters or gram to decagram.

Best regards,

Thierry


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-01-18 7:52 GMT+01:00 Timothy Lau <timothy.s.lau at gmail.com>:

> Hello,
>
> lme4 mentioned that I should consider rescaling some of my predictor
> variables because of the scale differences:
>
> *fit warnings:*
> *Some predictor variables are on very different scales: consider rescaling*
>
>
>
> I have since been trying to expand Ben Bolker's function:
>
> *#
>
> http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon
> <
> http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon
> >*
> *rescale.coefs <- function(beta,mu,sigma) {*
> *  beta2 <- beta ## inherit names etc.*
> *  beta2[-1] <- sigma[1]*beta[-1]/sigma[-1]*
> *  beta2[1]  <- sigma[1]*beta[1]+mu[1]-sum(beta2[-1]*mu[-1])*
> *  beta2*
> *}*
> *# regular model*
> *m1 <- lm(Illiteracy~.,as.data.frame(state.x77))*
> *b1 <- coef(m1)*
> *# Make a scaled version of the data*
> *ss <- scale(state.x77)*
> *# Scaled coefficients:*
> *m1S <- update(m1,data=as.data.frame(ss))*
> *b1S <- coef(m1S)*
> *# rescaling*
> *icol <- which(colnames(state.x77)=="Illiteracy")*
> *p.order <- c(icol,(1:ncol(state.x77))[-icol])*
> *m <- colMeans(state.x77)[p.order]*
> *s <- apply(state.x77,2,sd)[p.order]*
> *all.equal(b1,rescale.coefs(b1S,m,s))  ## TRUE*
>
>
>
> to work with GLMM models (e.g., count data) that can't have the outcome
> standardized, only the predictors:
> *# function assumes scaled data is within model; no NA's and only used
> variables are in orig.data*
> *unscale.coef.mer <- function(model, orig.data) {*
> *  require(lme4)*
> *  ran.ef <- as.character(attr(attr(model at frame,
> "terms"),"predvars.random"))[-1:-2] # random effects*
> *  offset <- attr(attr(model at frame, "terms"), "offset")*
> *  scaled.data <- model at frame[,!names(model at frame) %in% c(ran.ef, offset)]
> # the scaled covariate(s) and outcome used in the model*
> *  orig.data <- orig.data[,!names(orig.data) %in% c(ran.ef, offset)]*
> *  beta <- fixef(model) # the fixed effects*
> *  outvar <- names(model at frame)[attr(attr(m3 at frame, "terms"), "response")]
> # the outcoem var. name*
> *  icol <- which(colnames(orig.data) == outvar) # outcome variable column
> number*
> *  p.order <- c(icol, (1:ncol(orig.data))[-icol]) # full list of variable
> names with outcome 1st*
> *  mu <- colMeans(x = orig.data)[p.order] # variable means with outcome
> 1st*
> *  sigma <- apply(X = orig.data, MARGIN = 2, FUN = sd)[p.order] # variable
> SDs with outcome 1st*
> *  beta2 <- beta # inherit names etc.*
> *  beta2[-1] <- sigma[1] * beta[-1] / sigma[-1] # the fixed effects except
> intercept*
> *  beta2[1]  <- sigma[1] * beta[1] + mu[1] - sum(beta2[-1] * mu[-1]) # the
> FE intercept*
> *  return(beta2)*
> *}*
>
>
> ?Could someone help me ?with backing out or unscaling the coefficeints so
> that m2 coef matches m1 even though m2 used the standardized data (example
> data):
> *# warnings due to scaling*
> *m1 <- glmer(formula = awards ~ 1 + write + (1 | cid), family = poisson,
> data = foreign::read.dta(file =
> "http://www.ats.ucla.edu/stat/data/hsbdemo.dta
> <http://www.ats.ucla.edu/stat/data/hsbdemo.dta>"))*
>
> *?# fixed scaling but need to back out coefficients to original scale for
> interpretative reasons?m?2? <- glmer(formula = awards ~ 1 +
> ?scale(?write?)? + (1 | cid), family = poisson, data =
> foreign::read.dta(file = "http://www.ats.ucla.edu/stat/data/hsbdemo.dta
> <http://www.ats.ucla.edu/stat/data/hsbdemo.dta>"))*
>
>
>
> Best,
> Timothy
>
>
>
> "The will to win means nothing without the will to prepare."
> http://en.wikipedia.org/wiki/Juma_Ikangaa
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From timothy.s.lau at gmail.com  Mon Jan 18 09:43:57 2016
From: timothy.s.lau at gmail.com (Timothy Lau)
Date: Mon, 18 Jan 2016 03:43:57 -0500
Subject: [R-sig-ME] Unstandardizing GLMM Regression Coefficients
In-Reply-To: <CAJuCY5wkcQsOnehjc61rnM-SWc=s=bW1xkkTqYVzfXw8=MQMUA@mail.gmail.com>
References: <CAJM8_pnDb5QFVtofkgBmzCHgmYSK1m-dN21+nHg4VimxMq-jWQ@mail.gmail.com>
	<CAJuCY5wkcQsOnehjc61rnM-SWc=s=bW1xkkTqYVzfXw8=MQMUA@mail.gmail.com>
Message-ID: <CAJM8_p=omzTSA7foF5hO_4TOLJX1eh+9j5kBWvTxgzMSXFq-2A@mail.gmail.com>

Hello Thierry,

My apologies for the HTML formatting, I am guessing gmail did that
because I italicized the the code sections for readability.

Below is the unitalicized version:

**************************************
Hello,

lme4 mentioned that I should consider rescaling some of my predictor
variables because of the scale differences:

fit warnings:
Some predictor variables are on very different scales: consider rescaling



I have since been trying to expand Ben Bolker's function:

# http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon
rescale.coefs <- function(beta,mu,sigma) {
  beta2 <- beta ## inherit names etc.
  beta2[-1] <- sigma[1]*beta[-1]/sigma[-1]
  beta2[1]  <- sigma[1]*beta[1]+mu[1]-sum(beta2[-1]*mu[-1])
  beta2
}
# regular model
m1 <- lm(Illiteracy~.,as.data.frame(state.x77))
b1 <- coef(m1)
# Make a scaled version of the data
ss <- scale(state.x77)
# Scaled coefficients:
m1S <- update(m1,data=as.data.frame(ss))
b1S <- coef(m1S)
# rescaling
icol <- which(colnames(state.x77)=="Illiteracy")
p.order <- c(icol,(1:ncol(state.x77))[-icol])
m <- colMeans(state.x77)[p.order]
s <- apply(state.x77,2,sd)[p.order]
all.equal(b1,rescale.coefs(b1S,m,s))  ## TRUE



to work with GLMM models (e.g., count data) that can't have the
outcome standardized, only the predictors:
# function assumes scaled data is within model; no NA's and only used
variables are in orig.data
unscale.coef.mer <- function(model, orig.data) {
  require(lme4)
  ran.ef <- as.character(attr(attr(model at frame,
"terms"),"predvars.random"))[-1:-2] # random effects
  offset <- attr(attr(model at frame, "terms"), "offset")
  scaled.data <- model at frame[,!names(model at frame) %in% c(ran.ef,
offset)] # the scaled covariate(s) and outcome used in the model
  orig.data <- orig.data[,!names(orig.data) %in% c(ran.ef, offset)]
  beta <- fixef(model) # the fixed effects
  outvar <- names(model at frame)[attr(attr(m3 at frame, "terms"),
"response")] # the outcoem var. name
  icol <- which(colnames(orig.data) == outvar) # outcome variable column number
  p.order <- c(icol, (1:ncol(orig.data))[-icol]) # full list of
variable names with outcome 1st
  mu <- colMeans(x = orig.data)[p.order] # variable means with outcome 1st
  sigma <- apply(X = orig.data, MARGIN = 2, FUN = sd)[p.order] #
variable SDs with outcome 1st
  beta2 <- beta # inherit names etc.
  beta2[-1] <- sigma[1] * beta[-1] / sigma[-1] # the fixed effects
except intercept
  beta2[1]  <- sigma[1] * beta[1] + mu[1] - sum(beta2[-1] * mu[-1]) #
the FE intercept
  return(beta2)
}


Could someone help me with backing out or unscaling the coefficeints
so that m2 coef matches m1 even though m2 used the standardized data
(example data):
# warnings due to scaling
m1 <- glmer(formula = awards ~ 1 + write + (1 | cid), family =
poisson, data = foreign::read.dta(file =
"http://www.ats.ucla.edu/stat/data/hsbdemo.dta"))
# fixed scaling but need to back out coefficients to original scale
for interpretative reasons
m2 <- glmer(formula = awards ~ 1 + scale(write) + (1 | cid), family =
poisson, data = foreign::read.dta(file =
"http://www.ats.ucla.edu/stat/data/hsbdemo.dta"))



Best,
Timothy



"The will to win means nothing without the will to prepare."
http://en.wikipedia.org/wiki/Juma_Ikangaa


On Mon, Jan 18, 2016 at 3:10 AM, Thierry Onkelinx
<thierry.onkelinx at inbo.be> wrote:
> Dear Timothy,
>
> Please don't post in HTML. It makes code unreadable.
> Please post code that can be easily reproduced: a simple copy/paste should
> work.
>
> You might want to consider a very simple scaling by just using expressing
> the variables in another unit (= multiplying by some power of 10). E.g.
> kilometers to meters or gram to decagram.
>
> Best regards,
>
> Thierry
>
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more than
> asking him to perform a post-mortem examination: he may be able to say what
> the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-01-18 7:52 GMT+01:00 Timothy Lau <timothy.s.lau at gmail.com>:
>>
>> Hello,
>>
>> lme4 mentioned that I should consider rescaling some of my predictor
>> variables because of the scale differences:
>>
>> *fit warnings:*
>> *Some predictor variables are on very different scales: consider
>> rescaling*
>>
>>
>>
>> I have since been trying to expand Ben Bolker's function:
>>
>> *#
>>
>> http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon
>>
>> <http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon>*
>> *rescale.coefs <- function(beta,mu,sigma) {*
>> *  beta2 <- beta ## inherit names etc.*
>> *  beta2[-1] <- sigma[1]*beta[-1]/sigma[-1]*
>> *  beta2[1]  <- sigma[1]*beta[1]+mu[1]-sum(beta2[-1]*mu[-1])*
>> *  beta2*
>> *}*
>> *# regular model*
>> *m1 <- lm(Illiteracy~.,as.data.frame(state.x77))*
>> *b1 <- coef(m1)*
>> *# Make a scaled version of the data*
>> *ss <- scale(state.x77)*
>> *# Scaled coefficients:*
>> *m1S <- update(m1,data=as.data.frame(ss))*
>> *b1S <- coef(m1S)*
>> *# rescaling*
>> *icol <- which(colnames(state.x77)=="Illiteracy")*
>> *p.order <- c(icol,(1:ncol(state.x77))[-icol])*
>> *m <- colMeans(state.x77)[p.order]*
>> *s <- apply(state.x77,2,sd)[p.order]*
>> *all.equal(b1,rescale.coefs(b1S,m,s))  ## TRUE*
>>
>>
>>
>> to work with GLMM models (e.g., count data) that can't have the outcome
>> standardized, only the predictors:
>> *# function assumes scaled data is within model; no NA's and only used
>> variables are in orig.data*
>> *unscale.coef.mer <- function(model, orig.data) {*
>> *  require(lme4)*
>> *  ran.ef <- as.character(attr(attr(model at frame,
>> "terms"),"predvars.random"))[-1:-2] # random effects*
>> *  offset <- attr(attr(model at frame, "terms"), "offset")*
>> *  scaled.data <- model at frame[,!names(model at frame) %in% c(ran.ef, offset)]
>> # the scaled covariate(s) and outcome used in the model*
>> *  orig.data <- orig.data[,!names(orig.data) %in% c(ran.ef, offset)]*
>> *  beta <- fixef(model) # the fixed effects*
>> *  outvar <- names(model at frame)[attr(attr(m3 at frame, "terms"), "response")]
>> # the outcoem var. name*
>> *  icol <- which(colnames(orig.data) == outvar) # outcome variable column
>> number*
>> *  p.order <- c(icol, (1:ncol(orig.data))[-icol]) # full list of variable
>> names with outcome 1st*
>> *  mu <- colMeans(x = orig.data)[p.order] # variable means with outcome
>> 1st*
>> *  sigma <- apply(X = orig.data, MARGIN = 2, FUN = sd)[p.order] # variable
>> SDs with outcome 1st*
>> *  beta2 <- beta # inherit names etc.*
>> *  beta2[-1] <- sigma[1] * beta[-1] / sigma[-1] # the fixed effects except
>> intercept*
>> *  beta2[1]  <- sigma[1] * beta[1] + mu[1] - sum(beta2[-1] * mu[-1]) # the
>> FE intercept*
>> *  return(beta2)*
>> *}*
>>
>>
>> Could someone help me with backing out or unscaling the coefficeints so
>> that m2 coef matches m1 even though m2 used the standardized data (example
>> data):
>> *# warnings due to scaling*
>> *m1 <- glmer(formula = awards ~ 1 + write + (1 | cid), family = poisson,
>> data = foreign::read.dta(file =
>> "http://www.ats.ucla.edu/stat/data/hsbdemo.dta
>> <http://www.ats.ucla.edu/stat/data/hsbdemo.dta>"))*
>>
>> *# fixed scaling but need to back out coefficients to original scale for
>> interpretative reasonsm2 <- glmer(formula = awards ~ 1 +
>> scale(write) + (1 | cid), family = poisson, data =
>> foreign::read.dta(file = "http://www.ats.ucla.edu/stat/data/hsbdemo.dta
>> <http://www.ats.ucla.edu/stat/data/hsbdemo.dta>"))*
>>
>>
>>
>> Best,
>> Timothy
>>
>>
>>
>> "The will to win means nothing without the will to prepare."
>> http://en.wikipedia.org/wiki/Juma_Ikangaa
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


From singmann at psychologie.uzh.ch  Mon Jan 18 14:00:52 2016
From: singmann at psychologie.uzh.ch (Henrik Singmann)
Date: Mon, 18 Jan 2016 14:00:52 +0100
Subject: [R-sig-ME] Removing random correlation parameter for
 categorical variable in lmer
In-Reply-To: <loom.20160117T013530-958@post.gmane.org>
References: <390163A25E622A43932D1B0730EECEA04D1A39C4@MBX04.uva.nl>
	<CAO7JsnSb0FpkWxwbkn4B4dvcReZmobgOtG1wy89YoX+OV1JZCg@mail.gmail.com>
	<loom.20160117T013530-958@post.gmane.org>
Message-ID: <569CE204.6000609@psychologie.uzh.ch>

Yet another possibility is to use the lmer_alt function from my afex 
package. It transform a categorical variable into a model matrix of 
numerical variables prior to fitting the model and then passes the so 
created model matrix to lmer/glmer. This completely suppresses all 
correlations among random effects as long as the "||" syntax is used. So 
in your case something along the following lines should work:

require(afex)
lmer_alt(Response_time ~ relatedness +  mean_response_time + 
(relatedness || subject) + (1 | word) + (1 | group))

Hope that helps,
Henrik


Am 17.01.2016 um 02:04 schrieb Ben Bolker:
> Douglas Bates <bates at ...> writes:
>
>>
>> For a numeric covariate x, the model expression 0 + x produces a model
>> matrix with a single column so you can suppress the correlation by using
>> terms like (1 | subject) + (0 + x | subject).  However, for a categorical
>> covariate like relatedness with 2 levels the expressions (g | subject), (1
>> + g | subect) and (0 + g | subject) are equivalent in terms of the
>> predictions from the fitted model.  The only difference between the last
>> two is in the "contrasts" that they generate.
>>
>> If you really want independent random effects for intercept and for the
>> effect of relatedness you should convert the relatedness factor to a
>> numeric covariate.  One possibility is (1 | subject) + (0 +
>> I(as.integer(relatedness) - 1) | subject).
>
>
>     Another (equivalent but more readable) possibility is to use the dummy
>
> (1|subject) + (0+dummy(unrelated,"1"))
>
>    (I *think* this is right -- if I am reading correctly, you have
>   a categorical variable coded as "0", "1", which is a little confusing ...
>


From bbolker at gmail.com  Mon Jan 18 14:45:03 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 18 Jan 2016 08:45:03 -0500
Subject: [R-sig-ME] Unstandardizing GLMM Regression Coefficients
In-Reply-To: <CAJM8_pnDb5QFVtofkgBmzCHgmYSK1m-dN21+nHg4VimxMq-jWQ@mail.gmail.com>
References: <CAJM8_pnDb5QFVtofkgBmzCHgmYSK1m-dN21+nHg4VimxMq-jWQ@mail.gmail.com>
Message-ID: <569CEC5F.6050707@gmail.com>



On 16-01-18 01:52 AM, Timothy Lau wrote:
> Hello,
>
> lme4 mentioned that I should consider rescaling some of my predictor
> variables because of the scale differences:
>
> *fit warnings:*
> *Some predictor variables are on very different scales: consider rescaling*
>
>
>
> I have since been trying to expand Ben Bolker's function:

A brief note -- I've updated the end of the StackOverflow answer you 
reference 
<http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon> 
to read:

 > If you scale only the predictors and not the response, then submit 
c(0,mean(predictors)) for m and c(1,sd(predictors)) for s

   that is, you shouldn't need to modify the given function very much ...

   Ben Bolker


From ashleyhcohen at gmail.com  Mon Jan 18 21:20:26 2016
From: ashleyhcohen at gmail.com (Ashley Cohen)
Date: Mon, 18 Jan 2016 15:20:26 -0500
Subject: [R-sig-ME] lmer vs. nlme
Message-ID: <CANWdAc+6B9cK3bYpN14+x9zuHZ1yXk1BJ0k-yqhmKSEARphF0g@mail.gmail.com>

Hi Mixed Models group;



I was wondering if you might be able to clarify some confusion I am having
with results from lmer.



I am running a mixed model with fixed effects for treatment, time and an
interaction between treatment and time. A random effect has been added in
for subject (for repeated measures).



I was under the impression, based on the response from Ben Bolker here,
that lmer would be estimating an unstructured correlation matrix:

http://stats.stackexchange.com/questions/86958/variance-covariance-structure-for-random-effects-in-glmer



However, when I ran the model with lme, specifying a compound symmetric
correlation structure and random effect, I got the exact same results as
from lmer.



I am not sure about what is going on and was wondering if you had any
thoughts on this. I also couldn?t find a way to pull out the correlation
matrix from lmer.



Thank you,


Ashley

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Jan 18 21:25:48 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 18 Jan 2016 21:25:48 +0100
Subject: [R-sig-ME] lmer vs. nlme
In-Reply-To: <CANWdAc+6B9cK3bYpN14+x9zuHZ1yXk1BJ0k-yqhmKSEARphF0g@mail.gmail.com>
References: <CANWdAc+6B9cK3bYpN14+x9zuHZ1yXk1BJ0k-yqhmKSEARphF0g@mail.gmail.com>
Message-ID: <CAJuCY5w5TU3QdRpsiX=Mvtc7S5-QQv74m65U1zvMHgPjh=8O0g@mail.gmail.com>

Dear Ashley,

Please add some (reproducible) examples. Without that we can only guess
which models you fitted and why the results are identical. Please do add
the output of sessionInfo() as well.

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-01-18 21:20 GMT+01:00 Ashley Cohen <ashleyhcohen at gmail.com>:

> Hi Mixed Models group;
>
>
>
> I was wondering if you might be able to clarify some confusion I am having
> with results from lmer.
>
>
>
> I am running a mixed model with fixed effects for treatment, time and an
> interaction between treatment and time. A random effect has been added in
> for subject (for repeated measures).
>
>
>
> I was under the impression, based on the response from Ben Bolker here,
> that lmer would be estimating an unstructured correlation matrix:
>
>
> http://stats.stackexchange.com/questions/86958/variance-covariance-structure-for-random-effects-in-glmer
>
>
>
> However, when I ran the model with lme, specifying a compound symmetric
> correlation structure and random effect, I got the exact same results as
> from lmer.
>
>
>
> I am not sure about what is going on and was wondering if you had any
> thoughts on this. I also couldn?t find a way to pull out the correlation
> matrix from lmer.
>
>
>
> Thank you,
>
>
> Ashley
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From ashleyhcohen at gmail.com  Mon Jan 18 21:48:05 2016
From: ashleyhcohen at gmail.com (Ashley Cohen)
Date: Mon, 18 Jan 2016 15:48:05 -0500
Subject: [R-sig-ME] lmer vs. nlme
In-Reply-To: <CAJuCY5w5TU3QdRpsiX=Mvtc7S5-QQv74m65U1zvMHgPjh=8O0g@mail.gmail.com>
References: <CANWdAc+6B9cK3bYpN14+x9zuHZ1yXk1BJ0k-yqhmKSEARphF0g@mail.gmail.com>
	<CAJuCY5w5TU3QdRpsiX=Mvtc7S5-QQv74m65U1zvMHgPjh=8O0g@mail.gmail.com>
Message-ID: <CANWdAcL8mwYZ9uvw22Kq52WrKa272TAns80T3oxOmvJx=RAmbg@mail.gmail.com>

Hi All,

Unfortunately, as this data isn't published yet I can't give you the output
results.

However, the models are structured as follows:

model1 <- lmer(outcome ~ group + visit + group *visit + (1 | id), data =
data)
model2<- lme(outcome ~group + visit + group *visit ,
data=data,random=~1|id, na.action = na.exclude, correlation = corCompSymm())

The random effects components are identical in each:

Random effects:

Groups   Name        Variance Std.Dev.

id       (Intercept) 1.280    1.131

 Residual             1.271    1.127

Number of obs: 195, groups:  id, 59


Ashley


On Mon, Jan 18, 2016 at 3:25 PM, Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Ashley,
>
> Please add some (reproducible) examples. Without that we can only guess
> which models you fitted and why the results are identical. Please do add
> the output of sessionInfo() as well.
>
> Best regards,
>
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-01-18 21:20 GMT+01:00 Ashley Cohen <ashleyhcohen at gmail.com>:
>
>> Hi Mixed Models group;
>>
>>
>>
>> I was wondering if you might be able to clarify some confusion I am having
>> with results from lmer.
>>
>>
>>
>> I am running a mixed model with fixed effects for treatment, time and an
>> interaction between treatment and time. A random effect has been added in
>> for subject (for repeated measures).
>>
>>
>>
>> I was under the impression, based on the response from Ben Bolker here,
>> that lmer would be estimating an unstructured correlation matrix:
>>
>>
>> http://stats.stackexchange.com/questions/86958/variance-covariance-structure-for-random-effects-in-glmer
>>
>>
>>
>> However, when I ran the model with lme, specifying a compound symmetric
>> correlation structure and random effect, I got the exact same results as
>> from lmer.
>>
>>
>>
>> I am not sure about what is going on and was wondering if you had any
>> thoughts on this. I also couldn?t find a way to pull out the correlation
>> matrix from lmer.
>>
>>
>>
>> Thank you,
>>
>>
>> Ashley
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Mon Jan 18 23:02:30 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 18 Jan 2016 17:02:30 -0500
Subject: [R-sig-ME] lmer vs. nlme
In-Reply-To: <CANWdAcL8mwYZ9uvw22Kq52WrKa272TAns80T3oxOmvJx=RAmbg@mail.gmail.com>
References: <CANWdAc+6B9cK3bYpN14+x9zuHZ1yXk1BJ0k-yqhmKSEARphF0g@mail.gmail.com>
	<CAJuCY5w5TU3QdRpsiX=Mvtc7S5-QQv74m65U1zvMHgPjh=8O0g@mail.gmail.com>
	<CANWdAcL8mwYZ9uvw22Kq52WrKa272TAns80T3oxOmvJx=RAmbg@mail.gmail.com>
Message-ID: <CABghstQ4jOZTxwe8qX_zTumJxqxZOXZNs6Z6WE81i_VTDWVDKw@mail.gmail.com>

  I think what's going on here is that with this particular
specification *both* models are effectively fitting _positive_
compound symmetry models.  (This is about to get a little bit
technical, and I might not be using exactly the right vocabulary ...)

    if I use a term like (1|id) in either an lmer or an lme model,
it's equivalent to adding a term b_i to the expected value for an
observation, where i indexes the individual (id) that observation came
from.  The b_i are assumed to be exchangeable, more specifically iid
(independent, identically distributed), and more specifically Normally
distributed. The variance must be non-negative, so the correlation
among values *within* a group must be non-negative too: the
corresponding correlation matrix is block diagonal, with one block for
the observations in each group (observations drawn from different
groups are independent of each other), where the off-diagonal elements
in each block are all equal to 1>= rho >= 0.

   When you ask for a compound symmetric model, you now have the same
block structure, but the value of rho may be anywhere between -1 and
1. I'm not actually sure how to set that up for a simple
(intercept-only) model: the only example given in Pinheiro and Bates
2000 is one where we have treatments (Variety) repeated across
different blocks.  From the output below it's clear that pdCompSymm()
forces the correlation *among treatments* to be identical for all
treatments (it also forces the variances to be identical: in principle
one could allow heterogeneous variances with a single fixed
correlation).

  It would be possible if one wanted to badly enough to muck around
with the correlation structures in lme4, but it's not for the faint of
heart.

  The fact that you got rho=0 in your output (not shown) for lme also
indicates that you're not setting up the model you think you are.

  Can you say more about the statistical/subject-area model you're
trying to fit?

  Ben Bolker




set.seed(101)
dd <- data.frame(y=rnorm(100),f=factor(rep(1:10,10)),all=1)
library(nlme)
data(Oats)
m1 <- lme(yield ~ nitro, data = Oats,
          random=list(Block=pdCompSymm(~Variety-1)))
m2 <- lme(yield ~ nitro, data = Oats,
          random=list(Block=pdLogChol(~Variety-1)))
library(lme4)
m3 <- lmer(yield ~ nitro + (Variety-1|Block), data = Oats)

## lme, compound symmetry

VarCorr(m1)
Block = pdCompSymm(Variety - 1)
                   Variance StdDev   Corr
VarietyGolden Rain 331.5271 18.20788
VarietyMarvellous  331.5271 18.20788 0.635
VarietyVictory     331.5271 18.20788 0.635 0.635
Residual           165.5585 12.86695

## lme, unstructured

> VarCorr(m2)
Block = pdLogChol(Variety - 1)
                   Variance StdDev   Corr
VarietyGolden Rain 263.7242 16.23959 VrtyGR VrtyMr
VarietyMarvellous  232.8230 15.25854 0.594
VarietyVictory     537.9363 23.19345 0.936  0.484
Residual           165.5585 12.86695

## lmer, unstructured
VarCorr(m3)
 Groups   Name               Std.Dev. Corr
 Block    VarietyGolden Rain 16.240
          VarietyMarvellous  15.259   0.594
          VarietyVictory     23.193   0.936 0.484
 Residual                    12.867




On Mon, Jan 18, 2016 at 3:48 PM, Ashley Cohen <ashleyhcohen at gmail.com> wrote:
> Hi All,
>
> Unfortunately, as this data isn't published yet I can't give you the output
> results.
>
> However, the models are structured as follows:
>
> model1 <- lmer(outcome ~ group + visit + group *visit + (1 | id), data =
> data)
> model2<- lme(outcome ~group + visit + group *visit ,
> data=data,random=~1|id, na.action = na.exclude, correlation = corCompSymm())
>
> The random effects components are identical in each:
>
> Random effects:
>
> Groups   Name        Variance Std.Dev.
>
> id       (Intercept) 1.280    1.131
>
>  Residual             1.271    1.127
>
> Number of obs: 195, groups:  id, 59
>
>
> Ashley
>
>
> On Mon, Jan 18, 2016 at 3:25 PM, Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Ashley,
>>
>> Please add some (reproducible) examples. Without that we can only guess
>> which models you fitted and why the results are identical. Please do add
>> the output of sessionInfo() as well.
>>
>> Best regards,
>>
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
>> Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> 2016-01-18 21:20 GMT+01:00 Ashley Cohen <ashleyhcohen at gmail.com>:
>>
>>> Hi Mixed Models group;
>>>
>>>
>>>
>>> I was wondering if you might be able to clarify some confusion I am having
>>> with results from lmer.
>>>
>>>
>>>
>>> I am running a mixed model with fixed effects for treatment, time and an
>>> interaction between treatment and time. A random effect has been added in
>>> for subject (for repeated measures).
>>>
>>>
>>>
>>> I was under the impression, based on the response from Ben Bolker here,
>>> that lmer would be estimating an unstructured correlation matrix:
>>>
>>>
>>> http://stats.stackexchange.com/questions/86958/variance-covariance-structure-for-random-effects-in-glmer
>>>
>>>
>>>
>>> However, when I ran the model with lme, specifying a compound symmetric
>>> correlation structure and random effect, I got the exact same results as
>>> from lmer.
>>>
>>>
>>>
>>> I am not sure about what is going on and was wondering if you had any
>>> thoughts on this. I also couldn?t find a way to pull out the correlation
>>> matrix from lmer.
>>>
>>>
>>>
>>> Thank you,
>>>
>>>
>>> Ashley
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From andrew.allyn at gmail.com  Tue Jan 19 01:59:36 2016
From: andrew.allyn at gmail.com (Andrew Allyn)
Date: Mon, 18 Jan 2016 19:59:36 -0500
Subject: [R-sig-ME] Opinions on model structure: fixed and random effects
Message-ID: <569D8A78.6030101@gmail.com>

Dear mixed model experts,

I am hoping to get opinions on a model structure, and specifically, 
whether Year, Season and Strata variables should be included as random 
effects, fixed effects, or both?

In a nutshell, I am building a species distribution model using 30+ 
years of fisheries trawl data with the main objective of using the model 
to predict fish distributions under future climate scenarios and the 
secondary objective of evaluating the relative importance of temperature 
predictor variables compared to static, landscape variables (e.g., 
depth, bottom type). The unit of observation is a trawl tow, which has 
an associated date (year, month, day), season (fall or spring) and 
strata (spatial identifier, where strata is a unique region based on 
biophysical characteristics and used for stratified random sampling 
purposes). Within our dataset we have multiple tows from the same strata 
within the same season and year. We will likely examine a few different 
frameworks (e.g., GLMM, GAMM, Boosted Regression Trees, Random Forests).

Taking the GLMM as an example, my plan is to do the following:
1) Include YEAR as a random effect. Although we are somewhat interested 
in the variability among all years, we are not specifically interested 
in completing year to year comparison between all years. However, if we 
were, it sounds like an interesting approach would be to include year as 
both a random and fixed effect, which would allow us to look at 
variability among years (random component) as well as trend and change 
over years (fixed component).
2) Include STRATA as a random effect. Strata, in many ways, is similar 
to the idea of a plot in a traditional plot-based or split-plot sampling 
design. Including it as a random effect accounts for the fact that 
multiple samples from the same strata are not truly independent. 
Additionally, we are not explicitly interested in comparing among 
strata. Therefore, including it as a random effect makes the most sense.
3) Include SEASON as a fixed effect. With only two options, it does not 
make sense to include season as a random effect. Additionally, we are 
interested in seasonal differences. On a related note, what if you had a 
temperature variable measured at a seasonal scale (i.e., spring or fall 
mean temp)? Would you drop season as a factor in the hopes that the 
seasonal variability was captured by the temperature variable?

Does this approach make sense?

Thank you in advance for your time and insight.

Sincerely,

Andrew


	[[alternative HTML version deleted]]


From timothy.s.lau at gmail.com  Tue Jan 19 20:27:32 2016
From: timothy.s.lau at gmail.com (Timothy Lau)
Date: Tue, 19 Jan 2016 14:27:32 -0500
Subject: [R-sig-ME] Generalized Poisson Regression (Underdispersion)
Message-ID: <CAJM8_pmhBx1VUXXrBULat2AhzYfcEMGTNjV_pfcAbC_cAjN0kQ@mail.gmail.com>

Hello,

I noticed that STATA has some functionality for handling
underdispersion with a theta estimate (similar to glmer.nb ?).
http://www.stata-journal.com/sjpdf.html?articlenum=st0279

What are some options that are available to me in R and the GLMM
particularly for similar situations?



Best,
Timothy



"The will to win means nothing without the will to prepare."
http://en.wikipedia.org/wiki/Juma_Ikangaa


From bbolker at gmail.com  Tue Jan 19 21:32:24 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 19 Jan 2016 15:32:24 -0500
Subject: [R-sig-ME] Generalized Poisson Regression (Underdispersion)
In-Reply-To: <CAJM8_pmhBx1VUXXrBULat2AhzYfcEMGTNjV_pfcAbC_cAjN0kQ@mail.gmail.com>
References: <CAJM8_pmhBx1VUXXrBULat2AhzYfcEMGTNjV_pfcAbC_cAjN0kQ@mail.gmail.com>
Message-ID: <569E9D58.2030104@gmail.com>


   Depends whether you mean in a standard (extended) GLM with only fixed 
effects (in which case the question is a little bit off-topic here ...) 
or in a GLMM.

   In the GLM case you can use the quasipoisson distribution in glm(); I 
think you have lots of other choices (including the ComPoissonReg 
package).  library("sos"); ?findFn is your friend.

   In the GLMM case your options (that I know of) are a lot more 
limited. You could use MASS::glmmPQL with a quasi-Poisson response (but 
I would definitely test it on simulated data with known characteristic 
first!).  You could use a GLMM with a Poisson response and adjust the 
standard errors yourself.  Other distributions could be built into any 
of the 'toolbox' packages (glmmADMB, glmmTMB, brms), but you'd have to 
do it yourself or convince someone else ...
    You might get somewhere with the hglm (hierarchical GLM) package as 
well, although I don't immediately see an underdispersed Poisson family.
   Ordinal response models (e.g. the ordinal package) are another 
alternative ...

   Looking forward to hearing other people's answers

   cheers
     Ben Bolker


On 16-01-19 02:27 PM, Timothy Lau wrote:
> Hello,
>
> I noticed that STATA has some functionality for handling
> underdispersion with a theta estimate (similar to glmer.nb ?).
> http://www.stata-journal.com/sjpdf.html?articlenum=st0279
>
> What are some options that are available to me in R and the GLMM
> particularly for similar situations?
>
>
>
> Best,
> Timothy
>
>
>
> "The will to win means nothing without the will to prepare."
> http://en.wikipedia.org/wiki/Juma_Ikangaa
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From ashleyhcohen at gmail.com  Tue Jan 19 21:37:48 2016
From: ashleyhcohen at gmail.com (Ashley Cohen)
Date: Tue, 19 Jan 2016 15:37:48 -0500
Subject: [R-sig-ME] lmer vs. nlme
In-Reply-To: <CABghstQ4jOZTxwe8qX_zTumJxqxZOXZNs6Z6WE81i_VTDWVDKw@mail.gmail.com>
References: <CANWdAc+6B9cK3bYpN14+x9zuHZ1yXk1BJ0k-yqhmKSEARphF0g@mail.gmail.com>
	<CAJuCY5w5TU3QdRpsiX=Mvtc7S5-QQv74m65U1zvMHgPjh=8O0g@mail.gmail.com>
	<CANWdAcL8mwYZ9uvw22Kq52WrKa272TAns80T3oxOmvJx=RAmbg@mail.gmail.com>
	<CABghstQ4jOZTxwe8qX_zTumJxqxZOXZNs6Z6WE81i_VTDWVDKw@mail.gmail.com>
Message-ID: <CANWdAcK7xGqSpkYq0n2+bkvgvOPzKq=-DVGRJaFR=c=waKCO-g@mail.gmail.com>

Thank you very much for your response, Ben.

I had more luck using lme to play around with the correlation structure, as
you mentioned, and was able to compare compound symmetry, AR(1) and
unstructured.

I think because I am trying to specify it (I don't know that I want to use
unstructured as we are lacking data) it is best not to 'muck around' with
the correlation matrix in lme4, as you mentioned. However, there doesn't
even seem to be much within-subject correlation anyways.

Ideally, I think an AR(1) correlation matrix works best for this data.

On Mon, Jan 18, 2016 at 5:02 PM, Ben Bolker <bbolker at gmail.com> wrote:

>   I think what's going on here is that with this particular
> specification *both* models are effectively fitting _positive_
> compound symmetry models.  (This is about to get a little bit
> technical, and I might not be using exactly the right vocabulary ...)
>
>     if I use a term like (1|id) in either an lmer or an lme model,
> it's equivalent to adding a term b_i to the expected value for an
> observation, where i indexes the individual (id) that observation came
> from.  The b_i are assumed to be exchangeable, more specifically iid
> (independent, identically distributed), and more specifically Normally
> distributed. The variance must be non-negative, so the correlation
> among values *within* a group must be non-negative too: the
> corresponding correlation matrix is block diagonal, with one block for
> the observations in each group (observations drawn from different
> groups are independent of each other), where the off-diagonal elements
> in each block are all equal to 1>= rho >= 0.
>
>    When you ask for a compound symmetric model, you now have the same
> block structure, but the value of rho may be anywhere between -1 and
> 1. I'm not actually sure how to set that up for a simple
> (intercept-only) model: the only example given in Pinheiro and Bates
> 2000 is one where we have treatments (Variety) repeated across
> different blocks.  From the output below it's clear that pdCompSymm()
> forces the correlation *among treatments* to be identical for all
> treatments (it also forces the variances to be identical: in principle
> one could allow heterogeneous variances with a single fixed
> correlation).
>
>   It would be possible if one wanted to badly enough to muck around
> with the correlation structures in lme4, but it's not for the faint of
> heart.
>
>   The fact that you got rho=0 in your output (not shown) for lme also
> indicates that you're not setting up the model you think you are.
>
>   Can you say more about the statistical/subject-area model you're
> trying to fit?
>
>   Ben Bolker
>
>
>
>
> set.seed(101)
> dd <- data.frame(y=rnorm(100),f=factor(rep(1:10,10)),all=1)
> library(nlme)
> data(Oats)
> m1 <- lme(yield ~ nitro, data = Oats,
>           random=list(Block=pdCompSymm(~Variety-1)))
> m2 <- lme(yield ~ nitro, data = Oats,
>           random=list(Block=pdLogChol(~Variety-1)))
> library(lme4)
> m3 <- lmer(yield ~ nitro + (Variety-1|Block), data = Oats)
>
> ## lme, compound symmetry
>
> VarCorr(m1)
> Block = pdCompSymm(Variety - 1)
>                    Variance StdDev   Corr
> VarietyGolden Rain 331.5271 18.20788
> VarietyMarvellous  331.5271 18.20788 0.635
> VarietyVictory     331.5271 18.20788 0.635 0.635
> Residual           165.5585 12.86695
>
> ## lme, unstructured
>
> > VarCorr(m2)
> Block = pdLogChol(Variety - 1)
>                    Variance StdDev   Corr
> VarietyGolden Rain 263.7242 16.23959 VrtyGR VrtyMr
> VarietyMarvellous  232.8230 15.25854 0.594
> VarietyVictory     537.9363 23.19345 0.936  0.484
> Residual           165.5585 12.86695
>
> ## lmer, unstructured
> VarCorr(m3)
>  Groups   Name               Std.Dev. Corr
>  Block    VarietyGolden Rain 16.240
>           VarietyMarvellous  15.259   0.594
>           VarietyVictory     23.193   0.936 0.484
>  Residual                    12.867
>
>
>
>
> On Mon, Jan 18, 2016 at 3:48 PM, Ashley Cohen <ashleyhcohen at gmail.com>
> wrote:
> > Hi All,
> >
> > Unfortunately, as this data isn't published yet I can't give you the
> output
> > results.
> >
> > However, the models are structured as follows:
> >
> > model1 <- lmer(outcome ~ group + visit + group *visit + (1 | id), data =
> > data)
> > model2<- lme(outcome ~group + visit + group *visit ,
> > data=data,random=~1|id, na.action = na.exclude, correlation =
> corCompSymm())
> >
> > The random effects components are identical in each:
> >
> > Random effects:
> >
> > Groups   Name        Variance Std.Dev.
> >
> > id       (Intercept) 1.280    1.131
> >
> >  Residual             1.271    1.127
> >
> > Number of obs: 195, groups:  id, 59
> >
> >
> > Ashley
> >
> >
> > On Mon, Jan 18, 2016 at 3:25 PM, Thierry Onkelinx <
> thierry.onkelinx at inbo.be>
> > wrote:
> >
> >> Dear Ashley,
> >>
> >> Please add some (reproducible) examples. Without that we can only guess
> >> which models you fitted and why the results are identical. Please do add
> >> the output of sessionInfo() as well.
> >>
> >> Best regards,
> >>
> >>
> >> ir. Thierry Onkelinx
> >> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and
> >> Forest
> >> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> >> Kliniekstraat 25
> >> 1070 Anderlecht
> >> Belgium
> >>
> >> To call in the statistician after the experiment is done may be no more
> >> than asking him to perform a post-mortem examination: he may be able to
> say
> >> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> >> The plural of anecdote is not data. ~ Roger Brinner
> >> The combination of some data and an aching desire for an answer does not
> >> ensure that a reasonable answer can be extracted from a given body of
> data.
> >> ~ John Tukey
> >>
> >> 2016-01-18 21:20 GMT+01:00 Ashley Cohen <ashleyhcohen at gmail.com>:
> >>
> >>> Hi Mixed Models group;
> >>>
> >>>
> >>>
> >>> I was wondering if you might be able to clarify some confusion I am
> having
> >>> with results from lmer.
> >>>
> >>>
> >>>
> >>> I am running a mixed model with fixed effects for treatment, time and
> an
> >>> interaction between treatment and time. A random effect has been added
> in
> >>> for subject (for repeated measures).
> >>>
> >>>
> >>>
> >>> I was under the impression, based on the response from Ben Bolker here,
> >>> that lmer would be estimating an unstructured correlation matrix:
> >>>
> >>>
> >>>
> http://stats.stackexchange.com/questions/86958/variance-covariance-structure-for-random-effects-in-glmer
> >>>
> >>>
> >>>
> >>> However, when I ran the model with lme, specifying a compound symmetric
> >>> correlation structure and random effect, I got the exact same results
> as
> >>> from lmer.
> >>>
> >>>
> >>>
> >>> I am not sure about what is going on and was wondering if you had any
> >>> thoughts on this. I also couldn?t find a way to pull out the
> correlation
> >>> matrix from lmer.
> >>>
> >>>
> >>>
> >>> Thank you,
> >>>
> >>>
> >>> Ashley
> >>>
> >>>         [[alternative HTML version deleted]]
> >>>
> >>> _______________________________________________
> >>> R-sig-mixed-models at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >>
> >>
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Wed Jan 20 12:33:47 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 20 Jan 2016 12:33:47 +0100
Subject: [R-sig-ME] Opinions on model structure: fixed and random effects
In-Reply-To: <569D8A78.6030101@gmail.com>
References: <569D8A78.6030101@gmail.com>
Message-ID: <CAJuCY5yNp7=KiMCcBLkB93+NpvWhgxOxdTyb8Xbyw8jQx-y=KA@mail.gmail.com>

Dear Andrew,

Some quick comments.

- Given that you want to predict, I would add year both the fixed and the
random part.
- Maybe you want to add some random slope to the strata (e.g. year or
season).
- Maybe season fixed and season:year and/or season:strata as random.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-01-19 1:59 GMT+01:00 Andrew Allyn <andrew.allyn at gmail.com>:

> Dear mixed model experts,
>
> I am hoping to get opinions on a model structure, and specifically,
> whether Year, Season and Strata variables should be included as random
> effects, fixed effects, or both?
>
> In a nutshell, I am building a species distribution model using 30+
> years of fisheries trawl data with the main objective of using the model
> to predict fish distributions under future climate scenarios and the
> secondary objective of evaluating the relative importance of temperature
> predictor variables compared to static, landscape variables (e.g.,
> depth, bottom type). The unit of observation is a trawl tow, which has
> an associated date (year, month, day), season (fall or spring) and
> strata (spatial identifier, where strata is a unique region based on
> biophysical characteristics and used for stratified random sampling
> purposes). Within our dataset we have multiple tows from the same strata
> within the same season and year. We will likely examine a few different
> frameworks (e.g., GLMM, GAMM, Boosted Regression Trees, Random Forests).
>
> Taking the GLMM as an example, my plan is to do the following:
> 1) Include YEAR as a random effect. Although we are somewhat interested
> in the variability among all years, we are not specifically interested
> in completing year to year comparison between all years. However, if we
> were, it sounds like an interesting approach would be to include year as
> both a random and fixed effect, which would allow us to look at
> variability among years (random component) as well as trend and change
> over years (fixed component).
> 2) Include STRATA as a random effect. Strata, in many ways, is similar
> to the idea of a plot in a traditional plot-based or split-plot sampling
> design. Including it as a random effect accounts for the fact that
> multiple samples from the same strata are not truly independent.
> Additionally, we are not explicitly interested in comparing among
> strata. Therefore, including it as a random effect makes the most sense.
> 3) Include SEASON as a fixed effect. With only two options, it does not
> make sense to include season as a random effect. Additionally, we are
> interested in seasonal differences. On a related note, what if you had a
> temperature variable measured at a seasonal scale (i.e., spring or fall
> mean temp)? Would you drop season as a factor in the hopes that the
> seasonal variability was captured by the temperature variable?
>
> Does this approach make sense?
>
> Thank you in advance for your time and insight.
>
> Sincerely,
>
> Andrew
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From highstat at highstat.com  Wed Jan 20 12:48:40 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Wed, 20 Jan 2016 11:48:40 +0000
Subject: [R-sig-ME] Generalized Poisson Regression
In-Reply-To: <mailman.3.1453287601.26004.r-sig-mixed-models@r-project.org>
References: <mailman.3.1453287601.26004.r-sig-mixed-models@r-project.org>
Message-ID: <569F7418.60106@highstat.com>



> ------------------------------
>
> Message: 2
> Date: Tue, 19 Jan 2016 15:32:24 -0500
> From: Ben Bolker <bbolker at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Generalized Poisson Regression
> 	(Underdispersion)
> Message-ID: <569E9D58.2030104 at gmail.com>
> Content-Type: text/plain; charset=windows-1252; format=flowed
>
>
>     Depends whether you mean in a standard (extended) GLM with only fixed
> effects (in which case the question is a little bit off-topic here ...)
> or in a GLMM.
>
>     In the GLM case you can use the quasipoisson distribution in glm(); I
> think you have lots of other choices (including the ComPoissonReg
> package).  library("sos"); ?findFn is your friend.
>
>     In the GLMM case your options (that I know of) are a lot more
> limited. You could use MASS::glmmPQL with a quasi-Poisson response (but
> I would definitely test it on simulated data with known characteristic
> first!).  You could use a GLMM with a Poisson response and adjust the
> standard errors yourself.  Other distributions could be built into any
> of the 'toolbox' packages (glmmADMB, glmmTMB, brms), but you'd have to
> do it yourself or convince someone else ...
>      You might get somewhere with the hglm (hierarchical GLM) package as
> well, although I don't immediately see an underdispersed Poisson family.
>     Ordinal response models (e.g. the ordinal package) are another
> alternative ...
>
>     Looking forward to hearing other people's answers

There is a bit of that in Ntzoufras (2010). He explains how you can do 
the GP in MCMC. But unfortunately he only explains it for dealing with 
overdispersion (and the majority of papers on this topic do the same). 
It is easy to program in JAGS with the zero trick. And quite often it 
performs better than the negative binomial GLM. This code should work 
for a GP model to deal with overdispersion.

model{

     #1. Priors beta
     for (i in 1:K) { beta[i] ~ dnorm(0, 0.0001)}

     #1C. Prior for theta parameter of GP distribution
     theta ~ dunif(0, 1)

      #2. Likelihood using the zero trick
      C <- 10000
      for (i in 1:N){
         Zeros[i] ~ dpois(Zeros.mean[i])
         Zeros.mean[i] <- -L[i] + C
       
         l1[i] <- log(1 - theta)
         l2[i] <- log(lambda[i])
         l3[i] <- (Y[i] - 1) * log((1-theta) * lambda[i] + theta * Y[i])
         l4[i] <- - ((1 - theta) * lambda[i] + theta * Y[i])
         l5[i] <- -loggam(Y[i] + 1)
         L[i]  <- l1[i] + l2[i] + l3[i] + l4[i] + l5[i]

         eta[i]    <- inprod(beta[], X[i,])
         lambda[i] <- exp(eta[i])
      }
      
      #Mean and variance
      for (i in 1:N){
      	ExpY[i] <- lambda[i]
      	VarY[i] <- lambda[i] / ((1 - theta)^2)
      	Pres[i] <- (Y[i] - ExpY[i]) / sqrt(VarY[i])
      }
      AIC <- -2 * sum(L[1:N]) + 2 * (K + 1)
}



For underdispersion it gets more ugly. The theta can be between -1 and 
0. But you need to include a restriction when sampling new parameters 
within the MCMC algorithm. Can't remember exactly what it 
was...something with the sum of two squares and being positive (See: 
Modeling Underdispersed Count Data with Generalized Poisson Regression. 
Harris et. (2011)).

In normal words: You need to write your own MCMC algorithm...... Maybe 
someone has done this already? But once you have it running, you can 
easily add random effects, and other correlation structures.

You can use rLGP from RMKdiscrete to simulate GP data. Gives you an 
impression how data from such a distribution looks like.

There is a whole book about the GP distribution....by Consul (1989).

By the way..it is also called the log linear Lagrangian Poisson model. 
Just in case you want to impress your colleagues.


Alain








-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


	[[alternative HTML version deleted]]


From chelucero at uchicago.edu  Thu Jan 21 20:51:18 2016
From: chelucero at uchicago.edu (=?UTF-8?Q?Ch=C3=A9_Lucero?=)
Date: Thu, 21 Jan 2016 19:51:18 +0000
Subject: [R-sig-ME] Estimating 2-level fixed effect slopes for a random
	effect
Message-ID: <CAJOWFj8b-m+3ModDRm2dytnVYFJ8qCfqZeaZ_rrXagwD9-i4PQ@mail.gmail.com>

Hi there.

I have a behavioral dataset that has a structure like this:

dat <- data.frame(Subject = rep(c('John', 'Mary', 'Roberta'), each=2),
Behavior = rep(c('A', 'B'), 3), Count = c(0, 4, 1, 3, 2, 6))

I fit models that looks like

glm.1 <- glmer(Count ~ Behavior + (1+Behavior|Subject), data = dat,
family='poisson')
glm.2 <- update(glm.2, . ~ . - Behavior)

and then do an LRT with
anova(glm.1, glm.2)

My question is about the random effects, and particularly estimation of the
random Behavior slopes for Subjects.

My understanding is that the random intercepts for Subjects models
idiosyncratic over-or-under contributing by Subjects (e.g. John gives A: 35
 B:42). I understand the random Behavior slopes for Subject to be modeling
idiosyncratic differences between the A and B Behavior counts for each
Subject (e.g. John gives A:4 B:23), so that you can detect a genuine
difference between A vs B Counts that isn't driven by particular subjects.

In my dataset, glmer seems to be able to estimate the random slopes. Is it
appropriate to include slopes in the model given that there are only two
datapoints (an A count and a B count) for each subject? Can the model
reasonably estimate by-Subject Behavior slopes appropriately with just the
two observations per Subject?

Also to clarify, is the random 'slope' in the case of a poisson
distribution essentially modeling a Count difference between A and B for
each subject?

random effects output from the model summary, in case it's helpful:

Random effects:
 Groups  Name        Variance Std.Dev. Corr
 Subject (Intercept) 0.8942   0.9456
         BehaviorB      0.4539   0.6737   -0.59
Number of obs: 160, groups: Subject, 80

Thank you!

-Ch?

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Thu Jan 21 21:50:51 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 21 Jan 2016 21:50:51 +0100
Subject: [R-sig-ME] Estimating 2-level fixed effect slopes for a random
	effect
In-Reply-To: <CAJOWFj8b-m+3ModDRm2dytnVYFJ8qCfqZeaZ_rrXagwD9-i4PQ@mail.gmail.com>
References: <CAJOWFj8b-m+3ModDRm2dytnVYFJ8qCfqZeaZ_rrXagwD9-i4PQ@mail.gmail.com>
Message-ID: <CAJuCY5zJ5qaD5+7g7oTBmETwj2buY6YfHd0njxoHoKTiu9NwCg@mail.gmail.com>

Dear Ch?,

Are those the random effects structure from glm.1 or glm.2? Can you post
the output of summary(glm.1)?

I would be cautious to fit a model with random slopes with only two
observations per subject. It allows a perfect fit unless the shrinkage is
strong enough to counterbalance that.

The code below estimates what happens in the more extreme subjects by
comparing the 2.5% and 97.5% quantiles of the random effects. In case of A
the relative difference between 2.5% and 97.5% is about 40, in case of B it
is 20. So a subject with a high (97.5%) random effect for A has an expected
value that is 40 times higher than a subject with a low (2.5%) effect for
A. Assessing whether this is reasonable requires expertise on the subject
of the study.

Best regards,

Thierry

re.var <- c(0.8942, 0.4539)
re.cor <- -0.59
re.covar <- prod(sqrt(re.var)) * re.cor
sigma <- matrix(re.covar, ncol = 2, nrow = 2)
diag(sigma) <- re.var
library(mvtnorm)
re.sim <- rmvnorm(1e5, sigma = sigma)
relative <- data.frame(
  A = exp(re.sim[, 1]),
  B = exp(rowSums(re.sim))
)

library(ggplot2)
ggplot(relative, aes(x = A, y = B)) +
  geom_point(alpha = 0.1)
ggplot(relative, aes(x = A, y = B)) +
  geom_point(alpha = 0.1) +
  scale_x_log10() + scale_y_log10()

re.quant <- apply(relative, 2, quantile, probs = c(0.025, 0.975))
re.quant[2, ] / re.quant[1, ]


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-01-21 20:51 GMT+01:00 Ch? Lucero <chelucero at uchicago.edu>:

> Hi there.
>
> I have a behavioral dataset that has a structure like this:
>
> dat <- data.frame(Subject = rep(c('John', 'Mary', 'Roberta'), each=2),
> Behavior = rep(c('A', 'B'), 3), Count = c(0, 4, 1, 3, 2, 6))
>
> I fit models that looks like
>
> glm.1 <- glmer(Count ~ Behavior + (1+Behavior|Subject), data = dat,
> family='poisson')
> glm.2 <- update(glm.2, . ~ . - Behavior)
>
> and then do an LRT with
> anova(glm.1, glm.2)
>
> My question is about the random effects, and particularly estimation of the
> random Behavior slopes for Subjects.
>
> My understanding is that the random intercepts for Subjects models
> idiosyncratic over-or-under contributing by Subjects (e.g. John gives A: 35
>  B:42). I understand the random Behavior slopes for Subject to be modeling
> idiosyncratic differences between the A and B Behavior counts for each
> Subject (e.g. John gives A:4 B:23), so that you can detect a genuine
> difference between A vs B Counts that isn't driven by particular subjects.
>
> In my dataset, glmer seems to be able to estimate the random slopes. Is it
> appropriate to include slopes in the model given that there are only two
> datapoints (an A count and a B count) for each subject? Can the model
> reasonably estimate by-Subject Behavior slopes appropriately with just the
> two observations per Subject?
>
> Also to clarify, is the random 'slope' in the case of a poisson
> distribution essentially modeling a Count difference between A and B for
> each subject?
>
> random effects output from the model summary, in case it's helpful:
>
> Random effects:
>  Groups  Name        Variance Std.Dev. Corr
>  Subject (Intercept) 0.8942   0.9456
>          BehaviorB      0.4539   0.6737   -0.59
> Number of obs: 160, groups: Subject, 80
>
> Thank you!
>
> -Ch?
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From chelucero at uchicago.edu  Thu Jan 21 22:09:17 2016
From: chelucero at uchicago.edu (=?UTF-8?Q?Ch=C3=A9_Lucero?=)
Date: Thu, 21 Jan 2016 21:09:17 +0000
Subject: [R-sig-ME] Estimating 2-level fixed effect slopes for a random
	effect
In-Reply-To: <CAJuCY5zJ5qaD5+7g7oTBmETwj2buY6YfHd0njxoHoKTiu9NwCg@mail.gmail.com>
References: <CAJOWFj8b-m+3ModDRm2dytnVYFJ8qCfqZeaZ_rrXagwD9-i4PQ@mail.gmail.com>
	<CAJuCY5zJ5qaD5+7g7oTBmETwj2buY6YfHd0njxoHoKTiu9NwCg@mail.gmail.com>
Message-ID: <CAJOWFj-S8X6AgjQdLEa8L=PUVXPSUHkfYEuAFNjSsevMdLM2ww@mail.gmail.com>

Hi Thierry.

Thank you for the code. I believe I understood the re.quant output based on
your description.  Can you help me understand how to interpret the charts
your code produced?

The partial summary was from glm.1.  Here is the full output:

> summary(glm.1)
Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: poisson ( log )
Formula: Count ~ Behavior + (1 + Behavior | Subject)
   Data: dat

      AIC       BIC    logLik  deviance
 638.0368  653.4127 -314.0184  628.0368

Random effects:
 Groups  Name        Variance Std.Dev. Corr
 Subject (Intercept) 0.8942   0.9456
         BehaviorB      0.4539   0.6737   -0.59
Number of obs: 160, groups: Subject, 80

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   0.5379     0.1346   3.997 6.41e-05 ***
BehaviorB       -0.2057     0.1405  -1.464    0.143
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
       (Intr)
BehaviorB -0.592


Regards,

-Ch?


On Thu, Jan 21, 2016 at 2:50 PM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Ch?,
>
> Are those the random effects structure from glm.1 or glm.2? Can you post
> the output of summary(glm.1)?
>
> I would be cautious to fit a model with random slopes with only two
> observations per subject. It allows a perfect fit unless the shrinkage is
> strong enough to counterbalance that.
>
> The code below estimates what happens in the more extreme subjects by
> comparing the 2.5% and 97.5% quantiles of the random effects. In case of A
> the relative difference between 2.5% and 97.5% is about 40, in case of B it
> is 20. So a subject with a high (97.5%) random effect for A has an expected
> value that is 40 times higher than a subject with a low (2.5%) effect for
> A. Assessing whether this is reasonable requires expertise on the subject
> of the study.
>
> Best regards,
>
> Thierry
>
> re.var <- c(0.8942, 0.4539)
> re.cor <- -0.59
> re.covar <- prod(sqrt(re.var)) * re.cor
> sigma <- matrix(re.covar, ncol = 2, nrow = 2)
> diag(sigma) <- re.var
> library(mvtnorm)
> re.sim <- rmvnorm(1e5, sigma = sigma)
> relative <- data.frame(
>   A = exp(re.sim[, 1]),
>   B = exp(rowSums(re.sim))
> )
>
> library(ggplot2)
> ggplot(relative, aes(x = A, y = B)) +
>   geom_point(alpha = 0.1)
> ggplot(relative, aes(x = A, y = B)) +
>   geom_point(alpha = 0.1) +
>   scale_x_log10() + scale_y_log10()
>
> re.quant <- apply(relative, 2, quantile, probs = c(0.025, 0.975))
> re.quant[2, ] / re.quant[1, ]
>
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-01-21 20:51 GMT+01:00 Ch? Lucero <chelucero at uchicago.edu>:
>
>> Hi there.
>>
>> I have a behavioral dataset that has a structure like this:
>>
>> dat <- data.frame(Subject = rep(c('John', 'Mary', 'Roberta'), each=2),
>> Behavior = rep(c('A', 'B'), 3), Count = c(0, 4, 1, 3, 2, 6))
>>
>> I fit models that looks like
>>
>> glm.1 <- glmer(Count ~ Behavior + (1+Behavior|Subject), data = dat,
>> family='poisson')
>> glm.2 <- update(glm.2, . ~ . - Behavior)
>>
>> and then do an LRT with
>> anova(glm.1, glm.2)
>>
>> My question is about the random effects, and particularly estimation of
>> the
>> random Behavior slopes for Subjects.
>>
>> My understanding is that the random intercepts for Subjects models
>> idiosyncratic over-or-under contributing by Subjects (e.g. John gives A:
>> 35
>>  B:42). I understand the random Behavior slopes for Subject to be modeling
>> idiosyncratic differences between the A and B Behavior counts for each
>> Subject (e.g. John gives A:4 B:23), so that you can detect a genuine
>> difference between A vs B Counts that isn't driven by particular subjects.
>>
>> In my dataset, glmer seems to be able to estimate the random slopes. Is it
>> appropriate to include slopes in the model given that there are only two
>> datapoints (an A count and a B count) for each subject? Can the model
>> reasonably estimate by-Subject Behavior slopes appropriately with just the
>> two observations per Subject?
>>
>> Also to clarify, is the random 'slope' in the case of a poisson
>> distribution essentially modeling a Count difference between A and B for
>> each subject?
>>
>> random effects output from the model summary, in case it's helpful:
>>
>> Random effects:
>>  Groups  Name        Variance Std.Dev. Corr
>>  Subject (Intercept) 0.8942   0.9456
>>          BehaviorB      0.4539   0.6737   -0.59
>> Number of obs: 160, groups: Subject, 80
>>
>> Thank you!
>>
>> -Ch?
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>

	[[alternative HTML version deleted]]


From karthikeyanc at ncbs.res.in  Wed Jan 20 14:45:19 2016
From: karthikeyanc at ncbs.res.in (Karthikeyan Chandrasegaran)
Date: Wed, 20 Jan 2016 19:15:19 +0530
Subject: [R-sig-ME] R Query - MANOVA with random effects
Message-ID: <CAM-diJSyd_ZH9+PzgMeeYSTPzdMiagnQLVZZPTmVMPRGQwVWWg@mail.gmail.com>

Dear Prof.Bolker,

Greetings. I am Karthikeyan C, a graduate student in evolutionary ecology
at the National Centre for Biological Sciences (NCBS), Bangalore, India. I
would like to seek your assistance in performing a MANOVA with random
effects incorporated in the model in R.

A sample data set and R code is copied below. It includes 3 dependent
variables (res1, res2, res3) and 3 independent variables (density, nut,
tment). I want a random effect ('rep' nested within 'block') included in
the model. Based on some of your tutorials available online, I tried using
'lmer' to run the model but could not take it forward.

It will be really helpful if you can assist me in taking this analysis
forward.

Thank you so much for your time.

Yours sincerely,
Karthikeyan Chandrasegaran

*Data set*

block density nut tment rep res1 res2 res3
1 high 10 control 1 0.051 0.07 0.096
1 high 10 control 2 0.08 0.134 0.182
1 high 10 control 3 0.075 0.329 0.417
1 high 10 tment 1 0.332 0.046 0.055
1 high 10 tment 2 0.043 0.078 0.107
1 high 10 tment 3 0.164 0.162 0.22
1 high 15 control 1 0.109 0.1 0.142
1 high 15 control 2 0.068 0.087 0.108
1 high 15 control 3 0.067 0.133 0.175
1 high 15 tment 1 0.108 0.238 0.342
1 high 15 tment 2 0.088 0.09 0.124
1 high 15 tment 3 0.056 0.131 0.172
1 low 10 control 1 0.096 0.154 0.195
1 low 10 control 2 0.024 0.092 0.122
1 low 10 control 3 0.172 0.192 0.259
1 low 10 tment 1 0.069 0.079 0.106
1 low 10 tment 2 0.065 0.15 0.192
1 low 10 tment 3 0.13 0.189 0.266
1 low 15 control 1 0.065 0.088 0.118
1 low 15 control 2 0.06 0.099 0.139
1 low 15 control 3 0.056 0.045 0.063
1 low 15 tment 1 0.08 0.052 0.071
1 low 15 tment 2 0.091 0.138 0.179
1 low 15 tment 3 0.047 0.062 0.08
2 high 10 control 1 0.031 0.132 0.168
2 high 10 control 2 0.061 0.162 0.211
2 high 10 control 3 0.12 0.041 0.067
2 high 10 tment 1 0.053 0.063 0.087
2 high 10 tment 2 0.053 0.083 0.107
2 high 10 tment 3 0.086 0.229 0.33
2 high 15 control 1 0.144 0.179 0.238
2 high 15 control 2 0.043 0.073 0.095
2 high 15 control 3 0.1 0.111 0.15
2 high 15 tment 1 0.097 0.139 0.177
2 high 15 tment 2 0.013 0.121 0.165
2 high 15 tment 3 0.047 0.119 0.16
2 low 10 control 1 0.103 0.208 0.282
2 low 10 control 2 0.036 0.168 0.234
2 low 10 control 3 0.072 0.073 0.101
2 low 10 tment 1 0.126 0.129 0.167
2 low 10 tment 2 0.071 0.153 0.2
2 low 10 tment 3 0.026 0.085 0.112
2 low 15 control 1 0.043 0.128 0.133
2 low 15 control 2 0.044 0.072 0.102
2 low 15 control 3 0.064 0.085 0.109
2 low 15 tment 1 0.113 0.3 0.414
2 low 15 tment 2 0.041 0.06 0.07
2 low 15 tment 3 0.126 0.129 0.167

#--------------------------------------------------------------------

*R Code*

> dat <- read.csv(file="sample.data.csv", na.strings=c("NA",""))
>
> #transforming datatype of entries in the dataset
> dat$nut <- factor(dat$nut)
> dat$block <- factor(dat$block)
> dat$rep <- factor(dat$rep)
>
>
> library(lme4)
>
> mod1 <- lmer(cbind(res1,res2,res3) ~ tment*density*nut+ (1|rep)+
(1|block), data=dat)
Error in v/v.e : non-conformable arrays
>
> mod2 <- lmer(cbind(res1,res2,res3) ~ tment*density*nut + (1|block/rep),
data=dat)
Error: updateMu: Size mismatch

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Fri Jan 22 13:13:40 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Fri, 22 Jan 2016 13:13:40 +0100
Subject: [R-sig-ME] Estimating 2-level fixed effect slopes for a random
	effect
In-Reply-To: <CAJOWFj-S8X6AgjQdLEa8L=PUVXPSUHkfYEuAFNjSsevMdLM2ww@mail.gmail.com>
References: <CAJOWFj8b-m+3ModDRm2dytnVYFJ8qCfqZeaZ_rrXagwD9-i4PQ@mail.gmail.com>
	<CAJuCY5zJ5qaD5+7g7oTBmETwj2buY6YfHd0njxoHoKTiu9NwCg@mail.gmail.com>
	<CAJOWFj-S8X6AgjQdLEa8L=PUVXPSUHkfYEuAFNjSsevMdLM2ww@mail.gmail.com>
Message-ID: <CAJuCY5xEhCuBH5GppVhQoW9tFZhsZ12i1o7pjbV8Kg9DwePrLQ@mail.gmail.com>

The charts just display a scatter plot with simulated random effect sizes.
They are back transformed to the original scale. They give an idea of how
strong the random effect can be given the variance covariance matrix.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-01-21 22:09 GMT+01:00 Ch? Lucero <chelucero at uchicago.edu>:

> Hi Thierry.
>
> Thank you for the code. I believe I understood the re.quant output based
> on your description.  Can you help me understand how to interpret the
> charts your code produced?
>
> The partial summary was from glm.1.  Here is the full output:
>
> > summary(glm.1)
> Generalized linear mixed model fit by maximum likelihood ['glmerMod']
>  Family: poisson ( log )
> Formula: Count ~ Behavior + (1 + Behavior | Subject)
>    Data: dat
>
>       AIC       BIC    logLik  deviance
>  638.0368  653.4127 -314.0184  628.0368
>
> Random effects:
>  Groups  Name        Variance Std.Dev. Corr
>  Subject (Intercept) 0.8942   0.9456
>          BehaviorB      0.4539   0.6737   -0.59
> Number of obs: 160, groups: Subject, 80
>
> Fixed effects:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)   0.5379     0.1346   3.997 6.41e-05 ***
> BehaviorB       -0.2057     0.1405  -1.464    0.143
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>        (Intr)
> BehaviorB -0.592
>
>
> Regards,
>
> -Ch?
>
>
> On Thu, Jan 21, 2016 at 2:50 PM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Ch?,
>>
>> Are those the random effects structure from glm.1 or glm.2? Can you post
>> the output of summary(glm.1)?
>>
>> I would be cautious to fit a model with random slopes with only two
>> observations per subject. It allows a perfect fit unless the shrinkage is
>> strong enough to counterbalance that.
>>
>> The code below estimates what happens in the more extreme subjects by
>> comparing the 2.5% and 97.5% quantiles of the random effects. In case of A
>> the relative difference between 2.5% and 97.5% is about 40, in case of B it
>> is 20. So a subject with a high (97.5%) random effect for A has an expected
>> value that is 40 times higher than a subject with a low (2.5%) effect for
>> A. Assessing whether this is reasonable requires expertise on the subject
>> of the study.
>>
>> Best regards,
>>
>> Thierry
>>
>> re.var <- c(0.8942, 0.4539)
>> re.cor <- -0.59
>> re.covar <- prod(sqrt(re.var)) * re.cor
>> sigma <- matrix(re.covar, ncol = 2, nrow = 2)
>> diag(sigma) <- re.var
>> library(mvtnorm)
>> re.sim <- rmvnorm(1e5, sigma = sigma)
>> relative <- data.frame(
>>   A = exp(re.sim[, 1]),
>>   B = exp(rowSums(re.sim))
>> )
>>
>> library(ggplot2)
>> ggplot(relative, aes(x = A, y = B)) +
>>   geom_point(alpha = 0.1)
>> ggplot(relative, aes(x = A, y = B)) +
>>   geom_point(alpha = 0.1) +
>>   scale_x_log10() + scale_y_log10()
>>
>> re.quant <- apply(relative, 2, quantile, probs = c(0.025, 0.975))
>> re.quant[2, ] / re.quant[1, ]
>>
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> 2016-01-21 20:51 GMT+01:00 Ch? Lucero <chelucero at uchicago.edu>:
>>
>>> Hi there.
>>>
>>> I have a behavioral dataset that has a structure like this:
>>>
>>> dat <- data.frame(Subject = rep(c('John', 'Mary', 'Roberta'), each=2),
>>> Behavior = rep(c('A', 'B'), 3), Count = c(0, 4, 1, 3, 2, 6))
>>>
>>> I fit models that looks like
>>>
>>> glm.1 <- glmer(Count ~ Behavior + (1+Behavior|Subject), data = dat,
>>> family='poisson')
>>> glm.2 <- update(glm.2, . ~ . - Behavior)
>>>
>>> and then do an LRT with
>>> anova(glm.1, glm.2)
>>>
>>> My question is about the random effects, and particularly estimation of
>>> the
>>> random Behavior slopes for Subjects.
>>>
>>> My understanding is that the random intercepts for Subjects models
>>> idiosyncratic over-or-under contributing by Subjects (e.g. John gives A:
>>> 35
>>>  B:42). I understand the random Behavior slopes for Subject to be
>>> modeling
>>> idiosyncratic differences between the A and B Behavior counts for each
>>> Subject (e.g. John gives A:4 B:23), so that you can detect a genuine
>>> difference between A vs B Counts that isn't driven by particular
>>> subjects.
>>>
>>> In my dataset, glmer seems to be able to estimate the random slopes. Is
>>> it
>>> appropriate to include slopes in the model given that there are only two
>>> datapoints (an A count and a B count) for each subject? Can the model
>>> reasonably estimate by-Subject Behavior slopes appropriately with just
>>> the
>>> two observations per Subject?
>>>
>>> Also to clarify, is the random 'slope' in the case of a poisson
>>> distribution essentially modeling a Count difference between A and B for
>>> each subject?
>>>
>>> random effects output from the model summary, in case it's helpful:
>>>
>>> Random effects:
>>>  Groups  Name        Variance Std.Dev. Corr
>>>  Subject (Intercept) 0.8942   0.9456
>>>          BehaviorB      0.4539   0.6737   -0.59
>>> Number of obs: 160, groups: Subject, 80
>>>
>>> Thank you!
>>>
>>> -Ch?
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>

	[[alternative HTML version deleted]]


From timothy.s.lau at gmail.com  Sat Jan 23 19:25:41 2016
From: timothy.s.lau at gmail.com (Timothy Lau)
Date: Sat, 23 Jan 2016 13:25:41 -0500
Subject: [R-sig-ME] ICC for the GLMM
Message-ID: <CAJM8_pmMPnmCYKgpPcavN3nyKq5E--F7r9SdfvbYASj_ffR5Vw@mail.gmail.com>

 I've written functions to compute the ICC for most of the GLMM using
the lme4 package:

###################################################
### compute ICC in lme4
###################################################
# use
# https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q3/022538.html
# for example interpretation

# Negative Binomial ICC lme4 (numerator can also be a list of values)
# function based off of http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3916583/
ICC.NB <- function(model, numerator){
  require(lme4)
  mout <- data.frame(VarCorr(model)) # random intercept model variances
  sigma_a2 <- sum(mout[mout$grp %in% numerator, "vcov"]) # random
effect(s) in numerator
  sigma_2 <- sum(mout["vcov"]) # sum of random effects variance in denominator
  beta <- as.numeric(fixef(model)["(Intercept)"]) # fixed effect intercept
  r <- getME(object = model, "glmer.nb.theta") # theta
  icc <- (exp(sigma_a2) - 1) / ((exp(sigma_2) - 1) + (exp(sigma_2) /
r) + (exp(-beta) - (sigma_2 / 2)))
  return(icc)
}
# example (cid = a school)
ICC.NB(model = glmer.nb(formula = awards ~ 1 + (1 | cid), data =
foreign::read.dta(file =
"http://www.ats.ucla.edu/stat/data/hsbdemo.dta")), numerator = "cid")


# Binomial ICC lme4 (numerator can also be a list of values)
# function based off of http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3426610/
ICC.BIN <- function(model, numerator){
  require(lme4)
  mout <- data.frame(VarCorr(model)) # random intercept model variances
  level1 <- pi^2 / 3 # level 1 variance of all binomial models
  sigma_a2 <- sum(mout[mout$grp %in% numerator,"vcov"]) # random
effect(s) in numerator
  sigma_2 <- sum(data.frame(VarCorr(model))[,"vcov"], level1) # sum of
random effects variance in denominator
  icc <- sigma_a2 / sigma_2
  return(icc)
}

# example (herd = a herd of bovine)
ICC.BIN(model = glmer(formula = cbind(incidence, size - incidence) ~ 1
+ (1 | herd), data = cbpp, family = binomial), numerator = "herd")


# Gaussian ICC lme4 (numerator can also be a list of values)
# function based off of http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1540459/
ICC.GAU <- function(model, numerator){
  require(lme4)
  mout <- data.frame(VarCorr(model)) # random intercept model variances
  sigma_a2 <- sum(mout[mout$grp %in% numerator, "vcov"]) # random
effect(s) in numerator
  sigma_2 <- sum(mout["vcov"]) # sum of random effects variance in denominator
  icc <- sigma_a2 / sigma_2
  return(icc)
}
# example (Subject = an individual person)
ICC.GAU(model = lmer(formula = Reaction ~ 1 + (1 | Subject), data =
sleepstudy), numerator = "Subject")


# Poisson ICC lme4 (numerator can also be a list of values)
# function based off of p.22
http://www.ssicentral.com/supermix/Documentation/count_final.pdf
ICC.POI <- function(model, numerator){
  require(lme4)
  mout <- data.frame(VarCorr(model)) # random intercept model variances
  sigma_a2 <- sum(mout[mout$grp %in% numerator, "vcov"]) # random
effect(s) in numerator
  sigma_2 <- sum(mout["vcov"]) # sum of random effects variance in denominator
  icc <- sigma_a2 / (1 + sigma_2)
  return(icc)
}
# example (cid = schools)
ICC.POI(model = glmer(formula = awards ~ 1 + (1 | cid), family =
poisson, data = foreign::read.dta(file =
"http://www.ats.ucla.edu/stat/data/hsbdemo.dta")), numerator = "cid")

###################################################


But I'm stuck on the Gamma distribution and Inverse Gaussian. If
anyone has some suggestions for how to compute the ICC for these
distributions or wants to help more directly via github I've put the
functions here:
https://github.com/timothyslau/ICC.merMod


Best,
Timothy



"The will to win means nothing without the will to prepare."
http://en.wikipedia.org/wiki/Juma_Ikangaa


From beckyannegilbert at gmail.com  Tue Jan 26 17:25:13 2016
From: beckyannegilbert at gmail.com (Becky Gilbert)
Date: Tue, 26 Jan 2016 16:25:13 +0000
Subject: [R-sig-ME] Different lmer results using contrasts() vs numeric
	coding
Message-ID: <CALWifgDaqDhX+JCMgDPfpG3GRy9NojHqmHdiQ2CNQbasghiioQ@mail.gmail.com>

Dear R Mixed Models List,



I'm working on a LMM for psycholinguistic data with a 3x2 fixed effects
structure and crossed subject and item random effects. I ran into a
confusing result when I compared the use of contrasts() for fixed factor
variables vs 'hand-coding' these contrasts into numeric variables (with the
same values assigned using contrasts()).



The summaries for the two models that include all fixed factor terms are
identical. Here is the syntax used for each:



modelHandCoded <- lmer(invRT ~ 1 + type1 + type2 + priming1 +
type1:priming1 + type2:priming1 + (1|pp) + (1|word), data = data)



# numeric variables to define contrasts

table(data$type1)

-2    1

1503 3014

table(data$type2)

 -1    0    1

1520 1503 1494

table(data$priming1)

  -1    1

2253 2264



modelContrasts <- lmer(invRT ~ 1 + type + priming + type:priming + (1|pp) +
(1|word), data = data)



# factor variables with contrasts

contrasts(data$type)             # [,1] = 'type1' above, [,2] = 'type2'
above

       [,1] [,2]

SC   -2    0

IC     1    1

IIH    1   -1

contrasts(data$priming)       # [,1] = 'priming1' above

               [,1]

unprimed   -1

primed       1



However, when I remove the effect of the 3-level fixed factor 'type' (while
still including its interaction with the 2-level factor 'priming'), the two
models no longer produce the same results. Here is the syntax for the two
models without 'type':



modelHandCoded.NoType <- lmer(invRT ~ 1 + priming1 + type1:priming1 +
type2:priming1 + (1|pp) + (1|word), data = data)



modelContrasts.NoType <- lmer(invRT ~ 1 + priming + type:priming + (1|pp) +
(1|word), data = data)



The summary for the hand-coded model includes 3 fixed effects that I
expected (priming1, type1:priming1, type1:priming2):



summary(modelHandCoded.NoType)

...

Fixed effects:

               Estimate Std. Error         df t value Pr(>|t|)

(Intercept)   1.428e+00  3.244e-02  2.900e+01  44.031   <2e-16 ***

priming1      8.878e-03  3.572e-03  4.384e+03   2.485    0.013 *

type1:priming1  8.416e-04  2.527e-03  4.385e+03   0.333    0.739

type2:priming1 -1.790e-04  4.373e-03  4.383e+03  -0.041    0.967



However the summary for the contrasts() model includes additional
interaction terms for each level of priming1:



summary(modelContrasts.NoType)

...

Fixed effects:

                 Estimate Std. Error         df t value Pr(>|t|)

(Intercept)     1.428e+00  3.243e-02  2.900e+01  44.046   <2e-16 ***

priming1        8.872e-03  3.572e-03  4.386e+03   2.484   0.0130 *

priming0:type1  6.289e-03  4.811e-03  3.130e+02   1.307   0.1921

priming1:type1  7.957e-03  4.802e-03  3.110e+02   1.657   0.0985 .

priming0:type2 -9.782e-03  8.323e-03  3.120e+02  -1.175   0.2408

priming1:type2 -1.009e-02  8.316e-03  3.110e+02  -1.213   0.2261



When I compare each reduced model to the full model, I find that there's a
difference between the full model and the reduced hand coded model, but not
between the full model and the reduced model using contrasts(). The
Df/AIC/BIC/LL for the latter two models are identical, so it appears that
removing the 'type' term had no effect. (This is true for comparisons with
both the hand-coded and contrasts() versions of the full model.) Here are
the results of the anova() for each comparison:



Model with full fixed-effects structure vs. hand-coded model with 'type'
removed

       Df    AIC    BIC logLik deviance  Chisq Chi Df Pr(>Chisq)

..1       7 168.14 213.05 -77.07   154.14

object  9 167.20 224.94 -74.60   149.20 4.9406      2    0.08456 .



Model with full fixed-effects structure vs. contrast-coded model with
'type' removed

       Df   AIC    BIC logLik deviance Chisq Chi Df Pr(>Chisq)

object  9 167.2 224.94  -74.6    149.2

..1       9 167.2 224.94  -74.6    149.2     0      0  < 2.2e-16 ***



Can anyone explain why the two reduced models differ depending on whether
the fixed factor variables are hand-coded numeric vs. factors with
contrasts() assigned? Also, why is there no effect of removing a fixed
factor term when contrasts() are used?  Apologies if I'm missing something
obvious!



Thanks,

Becky

_____________________________________________________

Dr Becky Gilbert

Research Associate

Psychology and Language Sciences

University College London

London WC1H 0AP

	[[alternative HTML version deleted]]


From drmccloy at uw.edu  Tue Jan 26 22:48:06 2016
From: drmccloy at uw.edu (Dan McCloy)
Date: Tue, 26 Jan 2016 13:48:06 -0800
Subject: [R-sig-ME] Different lmer results using contrasts() vs numeric
	coding
In-Reply-To: <CALWifgDaqDhX+JCMgDPfpG3GRy9NojHqmHdiQ2CNQbasghiioQ@mail.gmail.com>
References: <CALWifgDaqDhX+JCMgDPfpG3GRy9NojHqmHdiQ2CNQbasghiioQ@mail.gmail.com>
Message-ID: <CAOE0pY=G38a_UR8PTfhJ5G7Hxcu_PB2LC5qM+2mM0GU0RSWSAg@mail.gmail.com>

Using numeric variables is not the same as hand-coding the contrasts. If
you pass in a numeric variable the modeling function will assume it is a
numerically continuous predictor, and you will get one coefficient
regardless of how many "levels" you represented numerically. Try something
like

contrasts(data$type_handcoded) <- as.matrix(cbind(data$type1, data$type2))

to specify the contrast matrix by hand.
Dear R Mixed Models List,



I'm working on a LMM for psycholinguistic data with a 3x2 fixed effects
structure and crossed subject and item random effects. I ran into a
confusing result when I compared the use of contrasts() for fixed factor
variables vs 'hand-coding' these contrasts into numeric variables (with the
same values assigned using contrasts()).



The summaries for the two models that include all fixed factor terms are
identical. Here is the syntax used for each:



modelHandCoded <- lmer(invRT ~ 1 + type1 + type2 + priming1 +
type1:priming1 + type2:priming1 + (1|pp) + (1|word), data = data)



# numeric variables to define contrasts

table(data$type1)

-2    1

1503 3014

table(data$type2)

 -1    0    1

1520 1503 1494

table(data$priming1)

  -1    1

2253 2264



modelContrasts <- lmer(invRT ~ 1 + type + priming + type:priming + (1|pp) +
(1|word), data = data)



# factor variables with contrasts

contrasts(data$type)             # [,1] = 'type1' above, [,2] = 'type2'
above

       [,1] [,2]

SC   -2    0

IC     1    1

IIH    1   -1

contrasts(data$priming)       # [,1] = 'priming1' above

               [,1]

unprimed   -1

primed       1



However, when I remove the effect of the 3-level fixed factor 'type' (while
still including its interaction with the 2-level factor 'priming'), the two
models no longer produce the same results. Here is the syntax for the two
models without 'type':



modelHandCoded.NoType <- lmer(invRT ~ 1 + priming1 + type1:priming1 +
type2:priming1 + (1|pp) + (1|word), data = data)



modelContrasts.NoType <- lmer(invRT ~ 1 + priming + type:priming + (1|pp) +
(1|word), data = data)



The summary for the hand-coded model includes 3 fixed effects that I
expected (priming1, type1:priming1, type1:priming2):



summary(modelHandCoded.NoType)

...

Fixed effects:

               Estimate Std. Error         df t value Pr(>|t|)

(Intercept)   1.428e+00  3.244e-02  2.900e+01  44.031   <2e-16 ***

priming1      8.878e-03  3.572e-03  4.384e+03   2.485    0.013 *

type1:priming1  8.416e-04  2.527e-03  4.385e+03   0.333    0.739

type2:priming1 -1.790e-04  4.373e-03  4.383e+03  -0.041    0.967



However the summary for the contrasts() model includes additional
interaction terms for each level of priming1:



summary(modelContrasts.NoType)

...

Fixed effects:

                 Estimate Std. Error         df t value Pr(>|t|)

(Intercept)     1.428e+00  3.243e-02  2.900e+01  44.046   <2e-16 ***

priming1        8.872e-03  3.572e-03  4.386e+03   2.484   0.0130 *

priming0:type1  6.289e-03  4.811e-03  3.130e+02   1.307   0.1921

priming1:type1  7.957e-03  4.802e-03  3.110e+02   1.657   0.0985 .

priming0:type2 -9.782e-03  8.323e-03  3.120e+02  -1.175   0.2408

priming1:type2 -1.009e-02  8.316e-03  3.110e+02  -1.213   0.2261



When I compare each reduced model to the full model, I find that there's a
difference between the full model and the reduced hand coded model, but not
between the full model and the reduced model using contrasts(). The
Df/AIC/BIC/LL for the latter two models are identical, so it appears that
removing the 'type' term had no effect. (This is true for comparisons with
both the hand-coded and contrasts() versions of the full model.) Here are
the results of the anova() for each comparison:



Model with full fixed-effects structure vs. hand-coded model with 'type'
removed

       Df    AIC    BIC logLik deviance  Chisq Chi Df Pr(>Chisq)

..1       7 168.14 213.05 -77.07   154.14

object  9 167.20 224.94 -74.60   149.20 4.9406      2    0.08456 .



Model with full fixed-effects structure vs. contrast-coded model with
'type' removed

       Df   AIC    BIC logLik deviance Chisq Chi Df Pr(>Chisq)

object  9 167.2 224.94  -74.6    149.2

..1       9 167.2 224.94  -74.6    149.2     0      0  < 2.2e-16 ***



Can anyone explain why the two reduced models differ depending on whether
the fixed factor variables are hand-coded numeric vs. factors with
contrasts() assigned? Also, why is there no effect of removing a fixed
factor term when contrasts() are used?  Apologies if I'm missing something
obvious!



Thanks,

Becky

_____________________________________________________

Dr Becky Gilbert

Research Associate

Psychology and Language Sciences

University College London

London WC1H 0AP

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From beckyannegilbert at gmail.com  Wed Jan 27 15:49:06 2016
From: beckyannegilbert at gmail.com (Becky Gilbert)
Date: Wed, 27 Jan 2016 14:49:06 +0000
Subject: [R-sig-ME] Different lmer results using contrasts() vs numeric
	coding
In-Reply-To: <CAOE0pY=G38a_UR8PTfhJ5G7Hxcu_PB2LC5qM+2mM0GU0RSWSAg@mail.gmail.com>
References: <CALWifgDaqDhX+JCMgDPfpG3GRy9NojHqmHdiQ2CNQbasghiioQ@mail.gmail.com>
	<CAOE0pY=G38a_UR8PTfhJ5G7Hxcu_PB2LC5qM+2mM0GU0RSWSAg@mail.gmail.com>
Message-ID: <CALWifgCH+4YArqiUOEAjccBOJLxoUwUhYcDOWA+8sDdGmTB4kQ@mail.gmail.com>

Thanks very much Dan - that makes sense.

Follow up question: why is there no change to the factor-variable model
after removing 'type' as a fixed factor, while including its interaction
with the 'priming' factor?  For context, this reduced model was run in an
effort to get the main effect of the 'type' factor after accounting for the
effect of 'priming' and the 'type' x 'priming' interaction (i.e. the
ANOVA-like results with type-III SS).  Again, apologies if I'm missing
something obvious here.

Thanks,
Becky
_____________________________________________________
Dr Becky Gilbert
Research Associate
Psychology and Language Sciences
26 Bedford Way
Office: Room 208
University College London
London WC1H 0AP


On 26 January 2016 at 21:48, Dan McCloy <drmccloy at uw.edu> wrote:

> Using numeric variables is not the same as hand-coding the contrasts. If
> you pass in a numeric variable the modeling function will assume it is a
> numerically continuous predictor, and you will get one coefficient
> regardless of how many "levels" you represented numerically. Try something
> like
>
> contrasts(data$type_handcoded) <- as.matrix(cbind(data$type1, data$type2))
>
> to specify the contrast matrix by hand.
> Dear R Mixed Models List,
>
>
>
> I'm working on a LMM for psycholinguistic data with a 3x2 fixed effects
> structure and crossed subject and item random effects. I ran into a
> confusing result when I compared the use of contrasts() for fixed factor
> variables vs 'hand-coding' these contrasts into numeric variables (with the
> same values assigned using contrasts()).
>
>
>
> The summaries for the two models that include all fixed factor terms are
> identical. Here is the syntax used for each:
>
>
>
> modelHandCoded <- lmer(invRT ~ 1 + type1 + type2 + priming1 +
> type1:priming1 + type2:priming1 + (1|pp) + (1|word), data = data)
>
>
>
> # numeric variables to define contrasts
>
> table(data$type1)
>
> -2    1
>
> 1503 3014
>
> table(data$type2)
>
>  -1    0    1
>
> 1520 1503 1494
>
> table(data$priming1)
>
>   -1    1
>
> 2253 2264
>
>
>
> modelContrasts <- lmer(invRT ~ 1 + type + priming + type:priming + (1|pp) +
> (1|word), data = data)
>
>
>
> # factor variables with contrasts
>
> contrasts(data$type)             # [,1] = 'type1' above, [,2] = 'type2'
> above
>
>        [,1] [,2]
>
> SC   -2    0
>
> IC     1    1
>
> IIH    1   -1
>
> contrasts(data$priming)       # [,1] = 'priming1' above
>
>                [,1]
>
> unprimed   -1
>
> primed       1
>
>
>
> However, when I remove the effect of the 3-level fixed factor 'type' (while
> still including its interaction with the 2-level factor 'priming'), the two
> models no longer produce the same results. Here is the syntax for the two
> models without 'type':
>
>
>
> modelHandCoded.NoType <- lmer(invRT ~ 1 + priming1 + type1:priming1 +
> type2:priming1 + (1|pp) + (1|word), data = data)
>
>
>
> modelContrasts.NoType <- lmer(invRT ~ 1 + priming + type:priming + (1|pp) +
> (1|word), data = data)
>
>
>
> The summary for the hand-coded model includes 3 fixed effects that I
> expected (priming1, type1:priming1, type1:priming2):
>
>
>
> summary(modelHandCoded.NoType)
>
> ...
>
> Fixed effects:
>
>                Estimate Std. Error         df t value Pr(>|t|)
>
> (Intercept)   1.428e+00  3.244e-02  2.900e+01  44.031   <2e-16 ***
>
> priming1      8.878e-03  3.572e-03  4.384e+03   2.485    0.013 *
>
> type1:priming1  8.416e-04  2.527e-03  4.385e+03   0.333    0.739
>
> type2:priming1 -1.790e-04  4.373e-03  4.383e+03  -0.041    0.967
>
>
>
> However the summary for the contrasts() model includes additional
> interaction terms for each level of priming1:
>
>
>
> summary(modelContrasts.NoType)
>
> ...
>
> Fixed effects:
>
>                  Estimate Std. Error         df t value Pr(>|t|)
>
> (Intercept)     1.428e+00  3.243e-02  2.900e+01  44.046   <2e-16 ***
>
> priming1        8.872e-03  3.572e-03  4.386e+03   2.484   0.0130 *
>
> priming0:type1  6.289e-03  4.811e-03  3.130e+02   1.307   0.1921
>
> priming1:type1  7.957e-03  4.802e-03  3.110e+02   1.657   0.0985 .
>
> priming0:type2 -9.782e-03  8.323e-03  3.120e+02  -1.175   0.2408
>
> priming1:type2 -1.009e-02  8.316e-03  3.110e+02  -1.213   0.2261
>
>
>
> When I compare each reduced model to the full model, I find that there's a
> difference between the full model and the reduced hand coded model, but not
> between the full model and the reduced model using contrasts(). The
> Df/AIC/BIC/LL for the latter two models are identical, so it appears that
> removing the 'type' term had no effect. (This is true for comparisons with
> both the hand-coded and contrasts() versions of the full model.) Here are
> the results of the anova() for each comparison:
>
>
>
> Model with full fixed-effects structure vs. hand-coded model with 'type'
> removed
>
>        Df    AIC    BIC logLik deviance  Chisq Chi Df Pr(>Chisq)
>
> ..1       7 168.14 213.05 -77.07   154.14
>
> object  9 167.20 224.94 -74.60   149.20 4.9406      2    0.08456 .
>
>
>
> Model with full fixed-effects structure vs. contrast-coded model with
> 'type' removed
>
>        Df   AIC    BIC logLik deviance Chisq Chi Df Pr(>Chisq)
>
> object  9 167.2 224.94  -74.6    149.2
>
> ..1       9 167.2 224.94  -74.6    149.2     0      0  < 2.2e-16 ***
>
>
>
> Can anyone explain why the two reduced models differ depending on whether
> the fixed factor variables are hand-coded numeric vs. factors with
> contrasts() assigned? Also, why is there no effect of removing a fixed
> factor term when contrasts() are used?  Apologies if I'm missing something
> obvious!
>
>
>
> Thanks,
>
> Becky
>
> _____________________________________________________
>
> Dr Becky Gilbert
>
> Research Associate
>
> Psychology and Language Sciences
>
> University College London
>
> London WC1H 0AP
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From saroman at vims.edu  Wed Jan 27 15:51:18 2016
From: saroman at vims.edu (Sally A. Roman)
Date: Wed, 27 Jan 2016 14:51:18 +0000
Subject: [R-sig-ME] question about NAs in nlsList
Message-ID: <F5884184AEB2BD44AC3307D5AD03A40E1D9E0C9E@mboxes.campus.vims.edu>

Hi - I am working through Mixed Effect Models in S & SPlus and Nonlinear Regression with R to learn how to model nonlinear mixed effects models.  I am looking to model fish selectivity with a mixed effect model based on previous selectivity analysis I have already completed following Arthur Millar's work using the SELECT method (https://www.stat.auckland.ac.nz/~millar/selectware/R/trawls/).

I have used the nlsList function in  the nlme package to get parameter estimates by group, StationID, for three different subsets of data.  The subsetted data are based on total catch: 50 total animals, 100 total animals and 200 total animals between the two gear types by StationID.  StationID is a tow where the two gears where towed in a pair.  For learning purposes, I randomly selected 10 StationIDs from each dataset to work with.  The maximum number of StationIDs is 463 depending on the dataset.

For each dataset, the nlsList function does not converge for all StationIDs.  The number of nonconverging models decreases with increased total catch.  For the dataset with 200 or more animals, only one model did not converge.  I have modified the control function in an effort to improve convergence.

For example:

nls200<-nlsList(prop~select_fun(q1,q2,a,b,p,Length),start=start_vec,
               data=new.data_200,control=nls.control(maxiter=500,tol=1e-4,minFactor=1/5000,
               printEval=T))


select_fun<-function(q1,q2,a,b,p,Length){
               expo=exp(a+b*Length)
               r=expo/(1+expo)
               phi=q1*p*r/(q1*p*r+q2*(1-p))
}

a, b, and p are parameters estimated.  q1 and q2 are subsampling fractions. Length is the length class and prop is the proportion of animals at each length class caught in one gear to the total catch in both gears.

Then I am passing the nlsList model output  to nlme.

For example:
nls200.2<-nlme(nls200,random=p~1,control=nlmeControl(maxIter=50,
               minScale=0.001,tolerance=1e-5,niterEM=25,msVerbose=T))

My question is how does nlme treat the NAs from the individual models?  The nlme.nlsList help page states:

na.action
a function that indicates what should happen when the data contain NAs. The default action (na.fail) causes nlme to print an error message and terminate if there are any incomplete observations.

I have changed the na.action to na.omit and na.exclude and the model returned the same parameter estimates for all three cases.  I also believe this na.action is in reference to the data used in the model (data=new.data) not the parameter estimates.

Mixed Effects Models in S & SPlus has an example in Chp 6 with one individual model that does not converge, but does not indicate how it is treated when passed to nlme.

Thanks for any insight.

If this email is a repeat sorry, I thought I emailed it to the list members yesterday, but sent it to  'r-sig-mixed-models-bounces at r-project.org' instead of
r-sig-mixed-models at r-project.org.

Sally Roman
Fisheries Specialist
Virginia Institute of Marine Science
Marine Advisory Services

Phone: 804-684-7165
Fax: 804-684-7161

1375 Greate Road
Gloucester Point, VA 23062


	[[alternative HTML version deleted]]


From smit.reuben at gmail.com  Wed Jan 27 18:53:55 2016
From: smit.reuben at gmail.com (Reuben Smit)
Date: Wed, 27 Jan 2016 09:53:55 -0800
Subject: [R-sig-ME] increasing maximum iterations in glmmadmb function
Message-ID: <CAFFAEtgoPGD+2kWa318YNFxcAUW5Uc6z9e_MmziyY-b6hkZ+jA@mail.gmail.com>

My binomial mixed model is failing to converge, so I would like to increase
the number of iterations. However, upon research I cannot find
solutions for glmmadmb function specifically. I must be missing
something. Any guidance would be appreciated.

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Wed Jan 27 19:53:11 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 27 Jan 2016 13:53:11 -0500
Subject: [R-sig-ME] increasing maximum iterations in glmmadmb function
In-Reply-To: <CAFFAEtgoPGD+2kWa318YNFxcAUW5Uc6z9e_MmziyY-b6hkZ+jA@mail.gmail.com>
References: <CAFFAEtgoPGD+2kWa318YNFxcAUW5Uc6z9e_MmziyY-b6hkZ+jA@mail.gmail.com>
Message-ID: <56A91217.5070300@gmail.com>


    Did you look at ?admbControl  ?

    "failing to converge" is a pretty broad category; if for example the 
warning message says something about a non-positive-definite Hessian at 
the optimum, increasing the number of iterations may not help that much ...


On 16-01-27 12:53 PM, Reuben Smit wrote:
> My binomial mixed model is failing to converge, so I would like to increase
> the number of iterations. However, upon research I cannot find
> solutions for glmmadmb function specifically. I must be missing
> something. Any guidance would be appreciated.
>


From bbolker at gmail.com  Thu Jan 28 00:11:19 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 27 Jan 2016 18:11:19 -0500
Subject: [R-sig-ME] increasing maximum iterations in glmmadmb function
In-Reply-To: <CAFFAEtiHa=bBa_yJJHvQ2xj8H7byOXDTAc2fwUkMn-Yax65o_g@mail.gmail.com>
References: <CAFFAEtgoPGD+2kWa318YNFxcAUW5Uc6z9e_MmziyY-b6hkZ+jA@mail.gmail.com>
	<56A91217.5070300@gmail.com>
	<CAFFAEtiHa=bBa_yJJHvQ2xj8H7byOXDTAc2fwUkMn-Yax65o_g@mail.gmail.com>
Message-ID: <CABghstQAUJC06Fr3qpGzhoEH8T3bftq8vVR3SE6LxtK2KA8O_g@mail.gmail.com>

[Please keep r-sig-mixed-models in the Cc: list ...]

Nothing stands out particularly strongly from this summary; the one
guess would be, since you have rather large parameter estimates
(abs(beta)>5) in a binomial-response model, that you are encountering
a problem of complete separation (i.e., that some combinations of
predictor variables have all- (or nearly-all)-negative or all-positive
responses, leading to infinite parameter estimates under MLE).
However, in my experience usually such cases give even larger
parameter estimates (e.g. abs(beta)>8).

Since this particular model is fairly standard (binomial GLMM,
standard logit link, no zero-inflation) it should be fairly easy to
run it with another platform/package (MASS::glmmPQL, lme4::glmer,
glmmML, MCMCglmm ...) and see if you get similar answers.


On Wed, Jan 27, 2016 at 3:11 PM, Reuben Smit <smit.reuben at gmail.com> wrote:
> Thanks for the help query, that is what I was looking for.
>
> The warning does not include anything but "Convergence failed: log
> likelihood -0.1037". You were right about increasing the iterations not
> helping, though.
>
> The following is the model summary, if you notice anything immediately,
> please let me know, otherwise I will continue with model diagnostics, etc.
> This model is a set of models for model based hypotheses. Thanks again for
> any help you can offer.
>
>
> Call:
> glmmadmb(formula = A_Neis_PA ~ HMU + Dist2bank + DistAC + Sinu +
>     (1 | Site_fact), data = mussel, family = "binomial")
>
>
> Coefficients:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)   6.3850     2.0309    3.14  0.00167 **
> HMUB1         2.6482     0.7720    3.43  0.00060 ***
> HMUB2         2.9078     0.8030    3.62  0.00029 ***
> HMUC         -0.4973     1.2842   -0.39  0.69856
> HMUD          4.2994     1.0260    4.19  2.8e-05 ***
> Dist2bank    -0.1279     0.0337   -3.79  0.00015 ***
> DistAC        0.0165     0.0418    0.40  0.69220
> Sinu         -5.7189     1.6773   -3.41  0.00065 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Number of observations: total=259, Site_fact=20
> Random effect variance(s):
> Group=Site_fact
>             Variance StdDev
> (Intercept)   0.2957 0.5438
>
>
> On Wed, Jan 27, 2016 at 10:53 AM, Ben Bolker <bbolker at gmail.com> wrote:
>>
>>
>>    Did you look at ?admbControl  ?
>>
>>    "failing to converge" is a pretty broad category; if for example the
>> warning message says something about a non-positive-definite Hessian at the
>> optimum, increasing the number of iterations may not help that much ...
>>
>>
>>
>> On 16-01-27 12:53 PM, Reuben Smit wrote:
>>>
>>> My binomial mixed model is failing to converge, so I would like to
>>> increase
>>> the number of iterations. However, upon research I cannot find
>>> solutions for glmmadmb function specifically. I must be missing
>>> something. Any guidance would be appreciated.
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


From alexejpksiren at gmail.com  Thu Jan 28 04:12:41 2016
From: alexejpksiren at gmail.com (Alexej Siren)
Date: Wed, 27 Jan 2016 22:12:41 -0500
Subject: [R-sig-ME] glmmADMB without random effects
Message-ID: <002001d15979$c0a70060$41f50120$@gmail.com>

Hello,

 

I have a question about differences in model output.  If random effects are
not specified for a model using the glmmadmb function then the parameter
estimates should be the same for glm, correct?  Out of curiosity I tested
this on several datasets and found that the magnitude, but not the
direction, of parameter estimates changed for some models.  The only pattern
that I detected that was that this happened for datasets that contain highly
overdispersed data.  Is there a difference in fitting algorithms that would
cause this pattern?  Any insight would be appreciated.  

 

If there is not a straightforward explanation I can provide data.  

 

Thanks,

 

Alex 

 

 


	[[alternative HTML version deleted]]


From highstat at highstat.com  Thu Jan 28 11:34:35 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Thu, 28 Jan 2016 10:34:35 +0000
Subject: [R-sig-ME] Introduction to GAM and GAMM with R course
Message-ID: <56A9EEBB.5050601@highstat.com>

There are various remaining seats available on the following course


Course: Introduction to GAM and GAMM
Where:  University of Konstanz, Konstanz, Germany
When:   7-11 March 2016

Course website: http://www.highstat.com/statscourse.htm
Course flyer: http://highstat.com/Courses/Flyer2016_03Konstanz.pdf



Kind regards,

Alain Zuur



-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From katie.murray at stir.ac.uk  Wed Jan 27 16:02:15 2016
From: katie.murray at stir.ac.uk (Katie Murray)
Date: Wed, 27 Jan 2016 15:02:15 +0000
Subject: [R-sig-ME] Temporal Autocorrelation
Message-ID: <f77ae7f9661446d08d3851422bc6188c@stuley.ad.stir.ac.uk>

Good afternoon,

I was wondering if anyone would be able to give me some guidance with the following:

We have a dataset that investigates the senescence of insect body weight. Observations span several months with measurements of weight taken for individuals on a weekly basis until they die. Each individual has a unique ID; every week we collected weight data and measured some other parameters. We would like to test for and account for temporal autocorrelation in the data set.

We are trying to run a model in lme of this form:

m1<-lme(weight ~ week + var 1 + var 2,  random = ~ 1 + week_checked|ID,
             correlation = corAR1(form = ~ 1 + week_checked|ID),
             data = data)

lme delivers the following error message:

"Error in Initialize.corAR1(X[[i]], ...) :
  covariate must have unique values within groups for "corAR1" objects "

We believe this is caused by the fact that individuals disappear from the dataset as they die, and therefore their ID is no longer represented in the weeks following death. If we truncate the data set before the first individual dies, the model runs fine.

Is there any way to account for temporal autocorrelation in a data set where the number of individuals progressively declines over time due to mortality?

Many thanks in advance,

Katie

Katie Murray
PhD Research Student
Biological and Environmental Sciences
Stirling University
Stirling
FK9 4LA

Katie.murray at stir.ac.uk




-- 
The University is ranked in the QS World Rankings of the top 5% of universities in the world (QS World University Rankings, 2014)
The University of Stirling is a charity registered in Scotland, 
 number SC 011159.


	[[alternative HTML version deleted]]


From wolfgang.viechtbauer at maastrichtuniversity.nl  Fri Jan 29 22:07:12 2016
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Fri, 29 Jan 2016 21:07:12 +0000
Subject: [R-sig-ME] Temporal Autocorrelation
In-Reply-To: <f77ae7f9661446d08d3851422bc6188c@stuley.ad.stir.ac.uk>
References: <f77ae7f9661446d08d3851422bc6188c@stuley.ad.stir.ac.uk>
Message-ID: <33f9058c690a4b469d56f0ca4f94ac8e@UM-MAIL3216.unimaas.nl>

I believe the problem is not related to what you suspect. As the error indicates, your dataset appears to include insects that were measured more than once during the same week (the 'covariate' is the week variable, whose values are not all unique within some insects). Quoting from Pinheiro & Bates (2000): "The position variable must evaluate to an integer vector, with nonrepeated values per group [...]" (p. 235).

I think this will help you find the IDs of those insects:

apply(table(data$ID, data$week_checked) > 1, 1, sum) > 0

By the way, while it probably doesn't matter, the correct syntax should be: corAR1(form = ~ week_checked|ID). No need for the '1 +' part.

Best,
Wolfgang

-- 
Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and    
Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD    
Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com    

> -----Original Message-----
> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> project.org] On Behalf Of Katie Murray
> Sent: Wednesday, January 27, 2016 16:02
> To: 'r-sig-mixed-models at r-project.org'
> Subject: [R-sig-ME] Temporal Autocorrelation
> 
> Good afternoon,
> 
> I was wondering if anyone would be able to give me some guidance with the
> following:
> 
> We have a dataset that investigates the senescence of insect body weight.
> Observations span several months with measurements of weight taken for
> individuals on a weekly basis until they die. Each individual has a
> unique ID; every week we collected weight data and measured some other
> parameters. We would like to test for and account for temporal
> autocorrelation in the data set.
> 
> We are trying to run a model in lme of this form:
> 
> m1<-lme(weight ~ week + var 1 + var 2,  random = ~ 1 + week_checked|ID,
>              correlation = corAR1(form = ~ 1 + week_checked|ID),
>              data = data)
> 
> lme delivers the following error message:
> 
> "Error in Initialize.corAR1(X[[i]], ...) :
>   covariate must have unique values within groups for "corAR1" objects "
> 
> We believe this is caused by the fact that individuals disappear from the
> dataset as they die, and therefore their ID is no longer represented in
> the weeks following death. If we truncate the data set before the first
> individual dies, the model runs fine.
> 
> Is there any way to account for temporal autocorrelation in a data set
> where the number of individuals progressively declines over time due to
> mortality?
> 
> Many thanks in advance,
> 
> Katie
> 
> Katie Murray
> PhD Research Student
> Biological and Environmental Sciences
> Stirling University
> Stirling
> FK9 4LA
> 
> Katie.murray at stir.ac.uk


From m.westinbrook at gmail.com  Fri Jan 29 22:45:39 2016
From: m.westinbrook at gmail.com (M West)
Date: Fri, 29 Jan 2016 16:45:39 -0500
Subject: [R-sig-ME] Plotting best fit lines binomial GLMM
Message-ID: <CAKHcBHpNuwRe4+3921fB4S1XdD0Y3ngQc3pP2gUaxZL24ZHHAQ@mail.gmail.com>

Main questions:
(1) How to extract coefficients from GLMM to plot best fit lines to data?
(2) Are there other options for dealing with these sorts of data besides
mixed effects models (or RM ANOVA)?


Specifics: I have a short time series data across 12 sites over 8 years.
I'd like an omnibus plot that summarizes the main pattern interest in these
data.

The dependent variable is frequency females (data are # smokers out of the
total population). The independent variable is also a frequency (# infected
out of the total population).

Plotting each year separately it's easy to see the positive correlation
between smokers and infection. However, given the variation among years,
plotting all the
data on a single plot obscures the overall pattern....I need to fit
regression lines to
each year.

I know how to do this with lme....but I can't quite find how to do this
with GLMM and I've analyzed the data with a GLMM with a binomial
distribution (following Crawley)
[While the data are binomial, they are not binary (i.e., not 0 and 1)so a
logistic curve doesn't work].


I found this thread on inspecting the residuals but I haven't been able to
find anything on plotting a best fit line for these type of data.

http://stats.stackexchange.com/questions/70783/how-to-assess-the-fit-of-a-binomial-glmm-fitted-with-lme4-1-0


I would *much prefer* to use something other than mixed effects models (I
think the results are not straightforward to interpret and every book or
blog recommends a different approach) for this analysis so if there are
other suggestions they are also welcome!

Thanks,
M.

	[[alternative HTML version deleted]]


From julia.velten at rub.de  Sat Jan 30 09:26:26 2016
From: julia.velten at rub.de (Julia Velten)
Date: Sat, 30 Jan 2016 09:26:26 +0100
Subject: [R-sig-ME] HLM post-hoc power analysis and effect sizes
Message-ID: <29A8CF14-F158-446B-9E45-06052B43CEE8@rub.de>

Dear all, 

please excuse me for contacting you with such a "simple" question.

I have used the nlme package to analyze the agreement between different levels of sexual response in women. 

Now, I have just received feedback from peer-reviewers that would like for me to add effect sizes for my multilevel model (2 levels) and conduct a post hoc power analysis. 

Maybe you can help me with that: 

1. Are you familiar with an R-package for calculating HLM effect sizes and/or a post-hoc power analysis?

2. Do you think that it is a useful or appropriate suggestion from the reviewer to add these data to my paper?

Thank you very much for your support. 

Kind regards from Germany, 
Julia Velten  


From bachlaw01 at outlook.com  Sun Jan 31 06:04:24 2016
From: bachlaw01 at outlook.com (Jonathan Judge)
Date: Sun, 31 Jan 2016 05:04:24 +0000
Subject: [R-sig-ME] alternative prediction methods for MCMCglmm
Message-ID: <CY1PR16MB03458A7314AC7ED153C8854CAFDD0@CY1PR16MB0345.namprd16.prod.outlook.com>

I have a data set with 190k observations, over 100 predictors (including interactions), and 4 groups, each ranging from 100 to 1000 members.  

Lme4 models it in about 3 minutes, but lme4 is of course fast at most such things because it is able to work from maximum likelihood.  

What I'd prefer to use is MCMC.  Stan is overwhelmed by such a model, but MCMCglmm, to its credit, can model its standard 13000 iterations in about 2 hours.  (I suspect MCMCglmm's block sampling is well suited to working with large groups).  The sampling is relatively stable after the 3000 burnin iterations, so I have set thin=1 for the last 10000 iterations to reach effective sample sizes in a reasonable amount of time.  This gives me 10000 saved iterations over the 190k observations.

The problem is that while the model runs, the data is saved, and the parameter values from the model summary look good, the amount of saved data overwhelms the built-in predict function when I try to use posterior = 'all'.  Whether I use a machine with 8 or 64 GB of RAM, on Windows 10 or Linux, a prediction from this model uses up virtually all of the available memory and then pronounces that it cannot fit a vector of ___ size (it varies).  Sometimes the model refuses to do predictions at all; sometimes it will do predictions but chokes on doing intervals along with predictions, and sometimes it will do predictions with no marginalizations but object when I attempt to marginalize only certain groups.  There is no problem if I marginalize off a particular point (mean, median, etc.) but for me that defeats much of the point of using (and waiting for) MCMC.  

In reviewing the traceback on the failed predictions, it seems like the process is choking on the calls the prediction function makes to coda and its calculation of the HDR, as called through the apply function.  I recall Jerrod saying that he considered the predict function to be somewhat crude, and wonder therefore if anyone can suggest either (1) revisions to the predict function that might cut down on memory usage, or (2) alternative / manual ways to marginalize over the predictors in the MCMCglmm fit that would not leak / absorb so much memory. I doubt that I am the first person to face this challenge.  

A dummy set of data is provided below that will substantially replicate the problem.  I am using the version of lme4 and MCMCglmm  2.22.1, R 3.2.3.  

Thanks very much.

Jonathan

***********************************************************

### Prepare Environment ###

rm(list=ls(all=TRUE))

set.seed(1234)

### Create vectors and df ###

  y <- scale(c(rep(0,125000), rep(.4,5000), rep(.65, 45000), rep(1.1,10000), rep(1.6, 5000)), center=T, scale=T)
  b <- as.factor(c(rep(1,100000), rep(2,90000)))
  c <- as.factor(c(rep_len(1:30, 190000)))
  d <- as.factor(c(rep(1,60000), rep(2,50000), rep(3, 40000), rep(4,40000)))
  e <- scale(rnorm(190000,0,1), center=T, scale=T)
  f <- scale(rnorm(190000,0,1), center=T, scale=T)
  g <- scale(log(rpois(190000,100)), center=T, scale=T)
  h <- as.factor(c(rep(1,25000), rep(2,22000), rep(3,26000), rep(4,30000), rep(5,17000), rep(6,15000), rep(7,30000), rep(8,25000)))
  j <- as.factor(rpois(190000,20000))
  k <- as.factor(rpois(190000,7500))
  m <- as.factor(rpois(190000,150))

df <- data.frame(y,b,c,d,e,f,g,h,j,k,m)

### lme4 fit ###

require(lme4)
fit.lme4 <- lmer(y ~ b*c + 
              d*e + 
              f + 
              g +
              e +
              h +
              (1|j) + 
              (1|k) + 
              (1|m),
              data=df)

### MCMCglmm fit ###
require(MCMCglmm)
fit.MCMCglmm <- MCMCglmm(y ~ b*c + 
                           d*e + 
                           f + 
                           g +
                           e +
                           h,
                           random = 
                           ~j + 
                           k + 
                           m,
                         data=df,
                         pr=T,
                         thin=1)

******************************************************

In terms of error codes, they are not always the same, but here is the traceback from an attempt to predict the response with a confidence interval:


> df$preds <- predict(fit.MCMCglmm, type='response', interval='confidence', posterior='all')
There were 14 warnings (use warnings() to see them)
Error in t(apply(object$Sol, 1, function(x) { : 
  error in evaluating the argument 'x' in selecting a method for function 't': Error: cannot allocate vector of size 14.2 Gb
> traceback()
3: t(apply(object$Sol, 1, function(x) {
       (W %*% x)@x
   }))
2: predict.MCMCglmm(fit.MCMCglmm, type = "response", interval = "confidence", 
       posterior = "all")
1: predict(fit.MCMCglmm, type = "response", interval = "confidence", 
       posterior = "all")


From Phillip.Alday at unisa.edu.au  Sun Jan 31 07:05:59 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Sun, 31 Jan 2016 06:05:59 +0000
Subject: [R-sig-ME] Plotting best fit lines binomial GLMM
References: <CAKHcBHpNuwRe4+3921fB4S1XdD0Y3ngQc3pP2gUaxZL24ZHHAQ@mail.gmail.com>
Message-ID: <ff1e6bc101c1418b8cde69f07302ddc9@ITUPW-EXMBOX2C.UniNet.unisa.edu.au>

Addressing the plotting issue: look at the effects package. You can
directly plot effects objects (which will yield lattice plots) or you
can convert them to data frames and plot by hand (e.g. if you want more
control and/or ggplot).

Best,
Phillip

On 30/01/16 08:18, M West wrote:
> Main questions:
> (1) How to extract coefficients from GLMM to plot best fit lines to data?
> (2) Are there other options for dealing with these sorts of data besides
> mixed effects models (or RM ANOVA)?
> 
> 
> Specifics: I have a short time series data across 12 sites over 8 years.
> I'd like an omnibus plot that summarizes the main pattern interest in these
> data.
> 
> The dependent variable is frequency females (data are # smokers out of the
> total population). The independent variable is also a frequency (# infected
> out of the total population).
> 
> Plotting each year separately it's easy to see the positive correlation
> between smokers and infection. However, given the variation among years,
> plotting all the
> data on a single plot obscures the overall pattern....I need to fit
> regression lines to
> each year.
> 
> I know how to do this with lme....but I can't quite find how to do this
> with GLMM and I've analyzed the data with a GLMM with a binomial
> distribution (following Crawley)
> [While the data are binomial, they are not binary (i.e., not 0 and 1)so a
> logistic curve doesn't work].
> 
> 
> I found this thread on inspecting the residuals but I haven't been able to
> find anything on plotting a best fit line for these type of data.
> 
> http://stats.stackexchange.com/questions/70783/how-to-assess-the-fit-of-a-binomial-glmm-fitted-with-lme4-1-0
> 
> 
> I would *much prefer* to use something other than mixed effects models (I
> think the results are not straightforward to interpret and every book or
> blog recommends a different approach) for this analysis so if there are
> other suggestions they are also welcome!
> 
> Thanks,
> M.
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From paul.debes at utu.fi  Sun Jan 31 10:15:50 2016
From: paul.debes at utu.fi (Paul Debes)
Date: Sun, 31 Jan 2016 11:15:50 +0200
Subject: [R-sig-ME] alternative prediction methods for MCMCglmm
In-Reply-To: <CY1PR16MB03458A7314AC7ED153C8854CAFDD0@CY1PR16MB0345.namprd16.prod.outlook.com>
References: <CY1PR16MB03458A7314AC7ED153C8854CAFDD0@CY1PR16MB0345.namprd16.prod.outlook.com>
Message-ID: <op.yb3ioom7sgx3xe@armadillo50>

Hi Jonathan,

This problem reminds me of a problem I run into when I predict random  
effect trends with asreml-R to check if I actually model the right things:  
I also run into memory limitations. What I do in these cases is predicting  
piecewise for the random levels, with each prediction small enough to be  
handled by the memory and combining the results afterwards. Is that an  
option for you?

Another possibility may be using the "lsmeans" package. It is also able to  
predict effect combinations for MCMCglmm models. I think you may have to  
provide additional information to the function to work compared to most  
other package models, but this should be described in the manual.

Hope this helps,
Paul


On Sun, 31 Jan 2016 07:04:24 +0200, Jonathan Judge <bachlaw01 at outlook.com>  
wrote:

> I have a data set with 190k observations, over 100 predictors (including  
> interactions), and 4 groups, each ranging from 100 to 1000 members.
>
> Lme4 models it in about 3 minutes, but lme4 is of course fast at most  
> such things because it is able to work from maximum likelihood.
>
> What I'd prefer to use is MCMC.  Stan is overwhelmed by such a model,  
> but MCMCglmm, to its credit, can model its standard 13000 iterations in  
> about 2 hours.  (I suspect MCMCglmm's block sampling is well suited to  
> working with large groups).  The sampling is relatively stable after the  
> 3000 burnin iterations, so I have set thin=1 for the last 10000  
> iterations to reach effective sample sizes in a reasonable amount of  
> time.  This gives me 10000 saved iterations over the 190k observations.
>
> The problem is that while the model runs, the data is saved, and the  
> parameter values from the model summary look good, the amount of saved  
> data overwhelms the built-in predict function when I try to use  
> posterior = 'all'.  Whether I use a machine with 8 or 64 GB of RAM, on  
> Windows 10 or Linux, a prediction from this model uses up virtually all  
> of the available memory and then pronounces that it cannot fit a vector  
> of ___ size (it varies).  Sometimes the model refuses to do predictions  
> at all; sometimes it will do predictions but chokes on doing intervals  
> along with predictions, and sometimes it will do predictions with no  
> marginalizations but object when I attempt to marginalize only certain  
> groups.  There is no problem if I marginalize off a particular point  
> (mean, median, etc.) but for me that defeats much of the point of using  
> (and waiting for) MCMC.
>
> In reviewing the traceback on the failed predictions, it seems like the  
> process is choking on the calls the prediction function makes to coda  
> and its calculation of the HDR, as called through the apply function.  I  
> recall Jerrod saying that he considered the predict function to be  
> somewhat crude, and wonder therefore if anyone can suggest either (1)  
> revisions to the predict function that might cut down on memory usage,  
> or (2) alternative / manual ways to marginalize over the predictors in  
> the MCMCglmm fit that would not leak / absorb so much memory. I doubt  
> that I am the first person to face this challenge.
>
> A dummy set of data is provided below that will substantially replicate  
> the problem.  I am using the version of lme4 and MCMCglmm  2.22.1, R  
> 3.2.3.
>
> Thanks very much.
>
> Jonathan
>
> ***********************************************************
>
> ### Prepare Environment ###
>
> rm(list=ls(all=TRUE))
>
> set.seed(1234)
>
> ### Create vectors and df ###
>
>   y <- scale(c(rep(0,125000), rep(.4,5000), rep(.65, 45000),  
> rep(1.1,10000), rep(1.6, 5000)), center=T, scale=T)
>   b <- as.factor(c(rep(1,100000), rep(2,90000)))
>   c <- as.factor(c(rep_len(1:30, 190000)))
>   d <- as.factor(c(rep(1,60000), rep(2,50000), rep(3, 40000),  
> rep(4,40000)))
>   e <- scale(rnorm(190000,0,1), center=T, scale=T)
>   f <- scale(rnorm(190000,0,1), center=T, scale=T)
>   g <- scale(log(rpois(190000,100)), center=T, scale=T)
>   h <- as.factor(c(rep(1,25000), rep(2,22000), rep(3,26000),  
> rep(4,30000), rep(5,17000), rep(6,15000), rep(7,30000), rep(8,25000)))
>   j <- as.factor(rpois(190000,20000))
>   k <- as.factor(rpois(190000,7500))
>   m <- as.factor(rpois(190000,150))
>
> df <- data.frame(y,b,c,d,e,f,g,h,j,k,m)
>
> ### lme4 fit ###
>
> require(lme4)
> fit.lme4 <- lmer(y ~ b*c +
>               d*e +
>               f +
>               g +
>               e +
>               h +
>               (1|j) +
>               (1|k) +
>               (1|m),
>               data=df)
>
> ### MCMCglmm fit ###
> require(MCMCglmm)
> fit.MCMCglmm <- MCMCglmm(y ~ b*c +
>                            d*e +
>                            f +
>                            g +
>                            e +
>                            h,
>                            random =
>                            ~j +
>                            k +
>                            m,
>                          data=df,
>                          pr=T,
>                          thin=1)
>
> ******************************************************
>
> In terms of error codes, they are not always the same, but here is the  
> traceback from an attempt to predict the response with a confidence  
> interval:
>
>
>> df$preds <- predict(fit.MCMCglmm, type='response',  
>> interval='confidence', posterior='all')
> There were 14 warnings (use warnings() to see them)
> Error in t(apply(object$Sol, 1, function(x) { :
>   error in evaluating the argument 'x' in selecting a method for  
> function 't': Error: cannot allocate vector of size 14.2 Gb
>> traceback()
> 3: t(apply(object$Sol, 1, function(x) {
>        (W %*% x)@x
>    }))
> 2: predict.MCMCglmm(fit.MCMCglmm, type = "response", interval =  
> "confidence",
>        posterior = "all")
> 1: predict(fit.MCMCglmm, type = "response", interval = "confidence",
>        posterior = "all")
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Paul V. Debes
DFG Research Fellow
University of Turku
Department of Biology
Finland

Email: paul.debes at utu.fi


From karl at huftis.org  Sun Jan 31 17:34:22 2016
From: karl at huftis.org (Karl Ove Hufthammer)
Date: Sun, 31 Jan 2016 17:34:22 +0100
Subject: [R-sig-ME] HLM post-hoc power analysis and effect sizes
In-Reply-To: <29A8CF14-F158-446B-9E45-06052B43CEE8@rub.de>
References: <29A8CF14-F158-446B-9E45-06052B43CEE8@rub.de>
Message-ID: <56AE378E.8070902@huftis.org>

Julia Velten skreiv 30. jan. 2016 09:26:
> 1. Are you familiar with an R-package for calculating HLM effect sizes and/or a post-hoc power analysis?

Are you talking about *standardised* effect sizes? IMO, they don?t make 
much sense, and I would avoid reporting them, if at all possible. See, 
for example, 
http://www.graphpad.com/guides/prism/6/statistics/index.htm?stat_whats_wrong_with_standard_valu.htm

Non-standardised effect sizes *do* make sense, and are useful, and you 
get them directly from the nlme output.

For post-hoc power analysis, are you talking about power-analysis based 
on the *observed* effect? They don?t make any sense *at all*. See this 
paper:

Hoenig J. M. & Heisey D. M., 2001.
?The Abuse of Power: The Pervasive Fallacy of Power Calculations for 
Data Analysis?
The American Statistician, American Statistical Association, vol. 55, 
pages 19?24, February.

It?s available as a PDF here: 
http://www.tc.umn.edu/~alonso/Hoenig_AmericanStat_2001.pdf

-- 
Karl Ove Hufthammer


From m.westinbrook at gmail.com  Sun Jan 31 23:28:58 2016
From: m.westinbrook at gmail.com (M West)
Date: Sun, 31 Jan 2016 17:28:58 -0500
Subject: [R-sig-ME] Plotting best fit lines binomial GLMM
In-Reply-To: <ff1e6bc101c1418b8cde69f07302ddc9@ITUPW-EXMBOX2C.UniNet.unisa.edu.au>
References: <CAKHcBHpNuwRe4+3921fB4S1XdD0Y3ngQc3pP2gUaxZL24ZHHAQ@mail.gmail.com>
	<ff1e6bc101c1418b8cde69f07302ddc9@ITUPW-EXMBOX2C.UniNet.unisa.edu.au>
Message-ID: <CAKHcBHpkHW4ByA3xQmuZVHjp5ugmbknPVQ1zzWaEHQHL9UZEPw@mail.gmail.com>

Thanks for this suggestions Philip  - it looks like the effects package
doesn't work
for GLMMs - it works with glms.....

On Sun, Jan 31, 2016 at 1:05 AM, Phillip Alday <Phillip.Alday at unisa.edu.au>
wrote:

> Addressing the plotting issue: look at the effects package. You can
> directly plot effects objects (which will yield lattice plots) or you
> can convert them to data frames and plot by hand (e.g. if you want more
> control and/or ggplot).
>
> Best,
> Phillip
>
> On 30/01/16 08:18, M West wrote:
> > Main questions:
> > (1) How to extract coefficients from GLMM to plot best fit lines to data?
> > (2) Are there other options for dealing with these sorts of data besides
> > mixed effects models (or RM ANOVA)?
> >
> >
> > Specifics: I have a short time series data across 12 sites over 8 years.
> > I'd like an omnibus plot that summarizes the main pattern interest in
> these
> > data.
> >
> > The dependent variable is frequency females (data are # smokers out of
> the
> > total population). The independent variable is also a frequency (#
> infected
> > out of the total population).
> >
> > Plotting each year separately it's easy to see the positive correlation
> > between smokers and infection. However, given the variation among years,
> > plotting all the
> > data on a single plot obscures the overall pattern....I need to fit
> > regression lines to
> > each year.
> >
> > I know how to do this with lme....but I can't quite find how to do this
> > with GLMM and I've analyzed the data with a GLMM with a binomial
> > distribution (following Crawley)
> > [While the data are binomial, they are not binary (i.e., not 0 and 1)so a
> > logistic curve doesn't work].
> >
> >
> > I found this thread on inspecting the residuals but I haven't been able
> to
> > find anything on plotting a best fit line for these type of data.
> >
> >
> http://stats.stackexchange.com/questions/70783/how-to-assess-the-fit-of-a-binomial-glmm-fitted-with-lme4-1-0
> >
> >
> > I would *much prefer* to use something other than mixed effects models (I
> > think the results are not straightforward to interpret and every book or
> > blog recommends a different approach) for this analysis so if there are
> > other suggestions they are also welcome!
> >
> > Thanks,
> > M.
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Sun Jan 31 23:59:57 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Sun, 31 Jan 2016 22:59:57 +0000
Subject: [R-sig-ME] Plotting best fit lines binomial GLMM
In-Reply-To: <CAKHcBHpkHW4ByA3xQmuZVHjp5ugmbknPVQ1zzWaEHQHL9UZEPw@mail.gmail.com>
References: <CAKHcBHpNuwRe4+3921fB4S1XdD0Y3ngQc3pP2gUaxZL24ZHHAQ@mail.gmail.com>
	<ff1e6bc101c1418b8cde69f07302ddc9@ITUPW-EXMBOX2C.UniNet.unisa.edu.au>
	<CAKHcBHpkHW4ByA3xQmuZVHjp5ugmbknPVQ1zzWaEHQHL9UZEPw@mail.gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F5237D@FHSDB2D11-2.csu.mcmaster.ca>

Dear M.,

The effects package does work with GLMMs fit with glmer() in the lme4 package. See ?Effect. Here's an example adapted from ?glmer:

	library(effects)
	library(lme4)
	library("HSAUR2")
	gm2 <- glmer(outcome~treatment*visit+(1|patientID),
            	 data=toenail, family=binomial, nAGQ=20)
	Effect(c("treatment", "visit"), gm2)

producing

treatment*visit effect
              visit
treatment              1         2          3          4           5           6            7
  itraconazole 0.2236820 0.1155113 0.05588527 0.02612852 0.012014461 0.005481597 0.0024920184
  terbinafine  0.2104865 0.0871212 0.03303451 0.01208159 0.004358651 0.001564643 0.0005606578

I hope this helps,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox




> -----Original Message-----
> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> project.org] On Behalf Of M West
> Sent: January 31, 2016 11:29 PM
> To: Phillip Alday <Phillip.Alday at unisa.edu.au>
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Plotting best fit lines binomial GLMM
> 
> Thanks for this suggestions Philip  - it looks like the effects package doesn't
> work for GLMMs - it works with glms.....
> 
> On Sun, Jan 31, 2016 at 1:05 AM, Phillip Alday <Phillip.Alday at unisa.edu.au>
> wrote:
> 
> > Addressing the plotting issue: look at the effects package. You can
> > directly plot effects objects (which will yield lattice plots) or you
> > can convert them to data frames and plot by hand (e.g. if you want
> > more control and/or ggplot).
> >
> > Best,
> > Phillip
> >
> > On 30/01/16 08:18, M West wrote:
> > > Main questions:
> > > (1) How to extract coefficients from GLMM to plot best fit lines to data?
> > > (2) Are there other options for dealing with these sorts of data
> > > besides mixed effects models (or RM ANOVA)?
> > >
> > >
> > > Specifics: I have a short time series data across 12 sites over 8 years.
> > > I'd like an omnibus plot that summarizes the main pattern interest
> > > in
> > these
> > > data.
> > >
> > > The dependent variable is frequency females (data are # smokers out
> > > of
> > the
> > > total population). The independent variable is also a frequency (#
> > infected
> > > out of the total population).
> > >
> > > Plotting each year separately it's easy to see the positive
> > > correlation between smokers and infection. However, given the
> > > variation among years, plotting all the data on a single plot
> > > obscures the overall pattern....I need to fit regression lines to
> > > each year.
> > >
> > > I know how to do this with lme....but I can't quite find how to do
> > > this with GLMM and I've analyzed the data with a GLMM with a
> > > binomial distribution (following Crawley) [While the data are
> > > binomial, they are not binary (i.e., not 0 and 1)so a logistic curve
> > > doesn't work].
> > >
> > >
> > > I found this thread on inspecting the residuals but I haven't been
> > > able
> > to
> > > find anything on plotting a best fit line for these type of data.
> > >
> > >
> > http://stats.stackexchange.com/questions/70783/how-to-assess-the-fit-o
> > f-a-binomial-glmm-fitted-with-lme4-1-0
> > >
> > >
> > > I would *much prefer* to use something other than mixed effects
> > > models (I think the results are not straightforward to interpret and
> > > every book or blog recommends a different approach) for this
> > > analysis so if there are other suggestions they are also welcome!
> > >
> > > Thanks,
> > > M.
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >
> >
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From m.westinbrook at gmail.com  Mon Feb  1 01:35:30 2016
From: m.westinbrook at gmail.com (M West)
Date: Sun, 31 Jan 2016 19:35:30 -0500
Subject: [R-sig-ME] Plotting best fit lines binomial GLMM
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC810F5237D@FHSDB2D11-2.csu.mcmaster.ca>
References: <CAKHcBHpNuwRe4+3921fB4S1XdD0Y3ngQc3pP2gUaxZL24ZHHAQ@mail.gmail.com>
	<ff1e6bc101c1418b8cde69f07302ddc9@ITUPW-EXMBOX2C.UniNet.unisa.edu.au>
	<CAKHcBHpkHW4ByA3xQmuZVHjp5ugmbknPVQ1zzWaEHQHL9UZEPw@mail.gmail.com>
	<ACD1644AA6C67E4FBD0C350625508EC810F5237D@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <CAKHcBHphyjP-WPVNQaYcvtr8FBDWs7Og5RSftBW9E+L2aPAMvQ@mail.gmail.com>

aha, you are right - sorry, I received a weird error message earlier saying
it wasn't available for glmer....that doesn't appear now. thanks.

On Sun, Jan 31, 2016 at 5:59 PM, Fox, John <jfox at mcmaster.ca> wrote:

> Dear M.,
>
> The effects package does work with GLMMs fit with glmer() in the lme4
> package. See ?Effect. Here's an example adapted from ?glmer:
>
>         library(effects)
>         library(lme4)
>         library("HSAUR2")
>         gm2 <- glmer(outcome~treatment*visit+(1|patientID),
>                  data=toenail, family=binomial, nAGQ=20)
>         Effect(c("treatment", "visit"), gm2)
>
> producing
>
> treatment*visit effect
>               visit
> treatment              1         2          3          4           5
>      6            7
>   itraconazole 0.2236820 0.1155113 0.05588527 0.02612852 0.012014461
> 0.005481597 0.0024920184
>   terbinafine  0.2104865 0.0871212 0.03303451 0.01208159 0.004358651
> 0.001564643 0.0005606578
>
> I hope this helps,
>  John
>
> -----------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> Web: socserv.mcmaster.ca/jfox
>
>
>
>
> > -----Original Message-----
> > From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> > project.org] On Behalf Of M West
> > Sent: January 31, 2016 11:29 PM
> > To: Phillip Alday <Phillip.Alday at unisa.edu.au>
> > Cc: r-sig-mixed-models at r-project.org
> > Subject: Re: [R-sig-ME] Plotting best fit lines binomial GLMM
> >
> > Thanks for this suggestions Philip  - it looks like the effects package
> doesn't
> > work for GLMMs - it works with glms.....
> >
> > On Sun, Jan 31, 2016 at 1:05 AM, Phillip Alday <
> Phillip.Alday at unisa.edu.au>
> > wrote:
> >
> > > Addressing the plotting issue: look at the effects package. You can
> > > directly plot effects objects (which will yield lattice plots) or you
> > > can convert them to data frames and plot by hand (e.g. if you want
> > > more control and/or ggplot).
> > >
> > > Best,
> > > Phillip
> > >
> > > On 30/01/16 08:18, M West wrote:
> > > > Main questions:
> > > > (1) How to extract coefficients from GLMM to plot best fit lines to
> data?
> > > > (2) Are there other options for dealing with these sorts of data
> > > > besides mixed effects models (or RM ANOVA)?
> > > >
> > > >
> > > > Specifics: I have a short time series data across 12 sites over 8
> years.
> > > > I'd like an omnibus plot that summarizes the main pattern interest
> > > > in
> > > these
> > > > data.
> > > >
> > > > The dependent variable is frequency females (data are # smokers out
> > > > of
> > > the
> > > > total population). The independent variable is also a frequency (#
> > > infected
> > > > out of the total population).
> > > >
> > > > Plotting each year separately it's easy to see the positive
> > > > correlation between smokers and infection. However, given the
> > > > variation among years, plotting all the data on a single plot
> > > > obscures the overall pattern....I need to fit regression lines to
> > > > each year.
> > > >
> > > > I know how to do this with lme....but I can't quite find how to do
> > > > this with GLMM and I've analyzed the data with a GLMM with a
> > > > binomial distribution (following Crawley) [While the data are
> > > > binomial, they are not binary (i.e., not 0 and 1)so a logistic curve
> > > > doesn't work].
> > > >
> > > >
> > > > I found this thread on inspecting the residuals but I haven't been
> > > > able
> > > to
> > > > find anything on plotting a best fit line for these type of data.
> > > >
> > > >
> > > http://stats.stackexchange.com/questions/70783/how-to-assess-the-fit-o
> > > f-a-binomial-glmm-fitted-with-lme4-1-0
> > > >
> > > >
> > > > I would *much prefer* to use something other than mixed effects
> > > > models (I think the results are not straightforward to interpret and
> > > > every book or blog recommends a different approach) for this
> > > > analysis so if there are other suggestions they are also welcome!
> > > >
> > > > Thanks,
> > > > M.
> > > >
> > > >       [[alternative HTML version deleted]]
> > > >
> > > > _______________________________________________
> > > > R-sig-mixed-models at r-project.org mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > > >
> > >
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From abfine at gmail.com  Mon Feb  1 03:02:17 2016
From: abfine at gmail.com (Alex Fine)
Date: Sun, 31 Jan 2016 21:02:17 -0500
Subject: [R-sig-ME] Plotting best fit lines binomial GLMM
In-Reply-To: <CAKHcBHphyjP-WPVNQaYcvtr8FBDWs7Og5RSftBW9E+L2aPAMvQ@mail.gmail.com>
References: <CAKHcBHpNuwRe4+3921fB4S1XdD0Y3ngQc3pP2gUaxZL24ZHHAQ@mail.gmail.com>
	<ff1e6bc101c1418b8cde69f07302ddc9@ITUPW-EXMBOX2C.UniNet.unisa.edu.au>
	<CAKHcBHpkHW4ByA3xQmuZVHjp5ugmbknPVQ1zzWaEHQHL9UZEPw@mail.gmail.com>
	<ACD1644AA6C67E4FBD0C350625508EC810F5237D@FHSDB2D11-2.csu.mcmaster.ca>
	<CAKHcBHphyjP-WPVNQaYcvtr8FBDWs7Og5RSftBW9E+L2aPAMvQ@mail.gmail.com>
Message-ID: <CAJ6ui+NyVROEZnJpgKJkbsPHkzTm-jx7uQKPv1sdk3Cd6qYgbw@mail.gmail.com>

I always liked this way of visualizing mixed logit models:
https://hlplab.wordpress.com/2009/01/19/plotting-effects-for-glmer-familybimomial-models/

On Sun, Jan 31, 2016 at 7:35 PM, M West <m.westinbrook at gmail.com> wrote:

> aha, you are right - sorry, I received a weird error message earlier saying
> it wasn't available for glmer....that doesn't appear now. thanks.
>
> On Sun, Jan 31, 2016 at 5:59 PM, Fox, John <jfox at mcmaster.ca> wrote:
>
> > Dear M.,
> >
> > The effects package does work with GLMMs fit with glmer() in the lme4
> > package. See ?Effect. Here's an example adapted from ?glmer:
> >
> >         library(effects)
> >         library(lme4)
> >         library("HSAUR2")
> >         gm2 <- glmer(outcome~treatment*visit+(1|patientID),
> >                  data=toenail, family=binomial, nAGQ=20)
> >         Effect(c("treatment", "visit"), gm2)
> >
> > producing
> >
> > treatment*visit effect
> >               visit
> > treatment              1         2          3          4           5
> >      6            7
> >   itraconazole 0.2236820 0.1155113 0.05588527 0.02612852 0.012014461
> > 0.005481597 0.0024920184
> >   terbinafine  0.2104865 0.0871212 0.03303451 0.01208159 0.004358651
> > 0.001564643 0.0005606578
> >
> > I hope this helps,
> >  John
> >
> > -----------------------------
> > John Fox, Professor
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > Web: socserv.mcmaster.ca/jfox
> >
> >
> >
> >
> > > -----Original Message-----
> > > From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> > > project.org] On Behalf Of M West
> > > Sent: January 31, 2016 11:29 PM
> > > To: Phillip Alday <Phillip.Alday at unisa.edu.au>
> > > Cc: r-sig-mixed-models at r-project.org
> > > Subject: Re: [R-sig-ME] Plotting best fit lines binomial GLMM
> > >
> > > Thanks for this suggestions Philip  - it looks like the effects package
> > doesn't
> > > work for GLMMs - it works with glms.....
> > >
> > > On Sun, Jan 31, 2016 at 1:05 AM, Phillip Alday <
> > Phillip.Alday at unisa.edu.au>
> > > wrote:
> > >
> > > > Addressing the plotting issue: look at the effects package. You can
> > > > directly plot effects objects (which will yield lattice plots) or you
> > > > can convert them to data frames and plot by hand (e.g. if you want
> > > > more control and/or ggplot).
> > > >
> > > > Best,
> > > > Phillip
> > > >
> > > > On 30/01/16 08:18, M West wrote:
> > > > > Main questions:
> > > > > (1) How to extract coefficients from GLMM to plot best fit lines to
> > data?
> > > > > (2) Are there other options for dealing with these sorts of data
> > > > > besides mixed effects models (or RM ANOVA)?
> > > > >
> > > > >
> > > > > Specifics: I have a short time series data across 12 sites over 8
> > years.
> > > > > I'd like an omnibus plot that summarizes the main pattern interest
> > > > > in
> > > > these
> > > > > data.
> > > > >
> > > > > The dependent variable is frequency females (data are # smokers out
> > > > > of
> > > > the
> > > > > total population). The independent variable is also a frequency (#
> > > > infected
> > > > > out of the total population).
> > > > >
> > > > > Plotting each year separately it's easy to see the positive
> > > > > correlation between smokers and infection. However, given the
> > > > > variation among years, plotting all the data on a single plot
> > > > > obscures the overall pattern....I need to fit regression lines to
> > > > > each year.
> > > > >
> > > > > I know how to do this with lme....but I can't quite find how to do
> > > > > this with GLMM and I've analyzed the data with a GLMM with a
> > > > > binomial distribution (following Crawley) [While the data are
> > > > > binomial, they are not binary (i.e., not 0 and 1)so a logistic
> curve
> > > > > doesn't work].
> > > > >
> > > > >
> > > > > I found this thread on inspecting the residuals but I haven't been
> > > > > able
> > > > to
> > > > > find anything on plotting a best fit line for these type of data.
> > > > >
> > > > >
> > > >
> http://stats.stackexchange.com/questions/70783/how-to-assess-the-fit-o
> > > > f-a-binomial-glmm-fitted-with-lme4-1-0
> > > > >
> > > > >
> > > > > I would *much prefer* to use something other than mixed effects
> > > > > models (I think the results are not straightforward to interpret
> and
> > > > > every book or blog recommends a different approach) for this
> > > > > analysis so if there are other suggestions they are also welcome!
> > > > >
> > > > > Thanks,
> > > > > M.
> > > > >
> > > > >       [[alternative HTML version deleted]]
> > > > >
> > > > > _______________________________________________
> > > > > R-sig-mixed-models at r-project.org mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > > > >
> > > >
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
Alex Fine
Ph. (336) 302-3251
web:  http://internal.psychology.illinois.edu/~abfine/
<http://internal.psychology.illinois.edu/~abfine/AlexFineHome.html>

	[[alternative HTML version deleted]]


From m.westinbrook at gmail.com  Mon Feb  1 04:53:33 2016
From: m.westinbrook at gmail.com (M West)
Date: Sun, 31 Jan 2016 22:53:33 -0500
Subject: [R-sig-ME] Plotting best fit lines binomial GLMM
In-Reply-To: <CAJ6ui+NyVROEZnJpgKJkbsPHkzTm-jx7uQKPv1sdk3Cd6qYgbw@mail.gmail.com>
References: <CAKHcBHpNuwRe4+3921fB4S1XdD0Y3ngQc3pP2gUaxZL24ZHHAQ@mail.gmail.com>
	<ff1e6bc101c1418b8cde69f07302ddc9@ITUPW-EXMBOX2C.UniNet.unisa.edu.au>
	<CAKHcBHpkHW4ByA3xQmuZVHjp5ugmbknPVQ1zzWaEHQHL9UZEPw@mail.gmail.com>
	<ACD1644AA6C67E4FBD0C350625508EC810F5237D@FHSDB2D11-2.csu.mcmaster.ca>
	<CAKHcBHphyjP-WPVNQaYcvtr8FBDWs7Og5RSftBW9E+L2aPAMvQ@mail.gmail.com>
	<CAJ6ui+NyVROEZnJpgKJkbsPHkzTm-jx7uQKPv1sdk3Cd6qYgbw@mail.gmail.com>
Message-ID: <CAKHcBHqHnvKjS8kc5k80FpowhYDt_n2VYo-Vq06505D9iE9nsQ@mail.gmail.com>

Ah, that's quite nice! Thanks so much to everyone for sharing all of this
information.

M

On Sun, Jan 31, 2016 at 9:02 PM, Alex Fine <abfine at gmail.com> wrote:

> I always liked this way of visualizing mixed logit models:
> https://hlplab.wordpress.com/2009/01/19/plotting-effects-for-glmer-familybimomial-models/
>
> On Sun, Jan 31, 2016 at 7:35 PM, M West <m.westinbrook at gmail.com> wrote:
>
>> aha, you are right - sorry, I received a weird error message earlier
>> saying
>> it wasn't available for glmer....that doesn't appear now. thanks.
>>
>> On Sun, Jan 31, 2016 at 5:59 PM, Fox, John <jfox at mcmaster.ca> wrote:
>>
>> > Dear M.,
>> >
>> > The effects package does work with GLMMs fit with glmer() in the lme4
>> > package. See ?Effect. Here's an example adapted from ?glmer:
>> >
>> >         library(effects)
>> >         library(lme4)
>> >         library("HSAUR2")
>> >         gm2 <- glmer(outcome~treatment*visit+(1|patientID),
>> >                  data=toenail, family=binomial, nAGQ=20)
>> >         Effect(c("treatment", "visit"), gm2)
>> >
>> > producing
>> >
>> > treatment*visit effect
>> >               visit
>> > treatment              1         2          3          4           5
>> >      6            7
>> >   itraconazole 0.2236820 0.1155113 0.05588527 0.02612852 0.012014461
>> > 0.005481597 0.0024920184
>> >   terbinafine  0.2104865 0.0871212 0.03303451 0.01208159 0.004358651
>> > 0.001564643 0.0005606578
>> >
>> > I hope this helps,
>> >  John
>> >
>> > -----------------------------
>> > John Fox, Professor
>> > McMaster University
>> > Hamilton, Ontario
>> > Canada L8S 4M4
>> > Web: socserv.mcmaster.ca/jfox
>> >
>> >
>> >
>> >
>> > > -----Original Message-----
>> > > From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
>> > > project.org] On Behalf Of M West
>> > > Sent: January 31, 2016 11:29 PM
>> > > To: Phillip Alday <Phillip.Alday at unisa.edu.au>
>> > > Cc: r-sig-mixed-models at r-project.org
>> > > Subject: Re: [R-sig-ME] Plotting best fit lines binomial GLMM
>> > >
>> > > Thanks for this suggestions Philip  - it looks like the effects
>> package
>> > doesn't
>> > > work for GLMMs - it works with glms.....
>> > >
>> > > On Sun, Jan 31, 2016 at 1:05 AM, Phillip Alday <
>> > Phillip.Alday at unisa.edu.au>
>> > > wrote:
>> > >
>> > > > Addressing the plotting issue: look at the effects package. You can
>> > > > directly plot effects objects (which will yield lattice plots) or
>> you
>> > > > can convert them to data frames and plot by hand (e.g. if you want
>> > > > more control and/or ggplot).
>> > > >
>> > > > Best,
>> > > > Phillip
>> > > >
>> > > > On 30/01/16 08:18, M West wrote:
>> > > > > Main questions:
>> > > > > (1) How to extract coefficients from GLMM to plot best fit lines
>> to
>> > data?
>> > > > > (2) Are there other options for dealing with these sorts of data
>> > > > > besides mixed effects models (or RM ANOVA)?
>> > > > >
>> > > > >
>> > > > > Specifics: I have a short time series data across 12 sites over 8
>> > years.
>> > > > > I'd like an omnibus plot that summarizes the main pattern interest
>> > > > > in
>> > > > these
>> > > > > data.
>> > > > >
>> > > > > The dependent variable is frequency females (data are # smokers
>> out
>> > > > > of
>> > > > the
>> > > > > total population). The independent variable is also a frequency (#
>> > > > infected
>> > > > > out of the total population).
>> > > > >
>> > > > > Plotting each year separately it's easy to see the positive
>> > > > > correlation between smokers and infection. However, given the
>> > > > > variation among years, plotting all the data on a single plot
>> > > > > obscures the overall pattern....I need to fit regression lines to
>> > > > > each year.
>> > > > >
>> > > > > I know how to do this with lme....but I can't quite find how to do
>> > > > > this with GLMM and I've analyzed the data with a GLMM with a
>> > > > > binomial distribution (following Crawley) [While the data are
>> > > > > binomial, they are not binary (i.e., not 0 and 1)so a logistic
>> curve
>> > > > > doesn't work].
>> > > > >
>> > > > >
>> > > > > I found this thread on inspecting the residuals but I haven't been
>> > > > > able
>> > > > to
>> > > > > find anything on plotting a best fit line for these type of data.
>> > > > >
>> > > > >
>> > > >
>> http://stats.stackexchange.com/questions/70783/how-to-assess-the-fit-o
>> > > > f-a-binomial-glmm-fitted-with-lme4-1-0
>> > > > >
>> > > > >
>> > > > > I would *much prefer* to use something other than mixed effects
>> > > > > models (I think the results are not straightforward to interpret
>> and
>> > > > > every book or blog recommends a different approach) for this
>> > > > > analysis so if there are other suggestions they are also welcome!
>> > > > >
>> > > > > Thanks,
>> > > > > M.
>> > > > >
>> > > > >       [[alternative HTML version deleted]]
>> > > > >
>> > > > > _______________________________________________
>> > > > > R-sig-mixed-models at r-project.org mailing list
>> > > > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> > > > >
>> > > >
>> > >
>> > >       [[alternative HTML version deleted]]
>> > >
>> > > _______________________________________________
>> > > R-sig-mixed-models at r-project.org mailing list
>> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>
> --
> Alex Fine
> Ph. (336) 302-3251
> web:  http://internal.psychology.illinois.edu/~abfine/
> <http://internal.psychology.illinois.edu/~abfine/AlexFineHome.html>
>

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Mon Feb  1 16:10:45 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 01 Feb 2016 15:10:45 +0000
Subject: [R-sig-ME] MCMCglmm predictions using new data error
In-Reply-To: <CAO+b4j-uRjS3_hGR+hTUxLAchs8-N3Kne9=fxtZDa0q5Jmvtsw@mail.gmail.com>
References: <CAO+b4j8wCNw6UEHocir2W+jeGXucMz9216dC3e-CgQ3y1SpcXQ@mail.gmail.com>
	<052754C5-E195-4867-9A0E-43F9E5E225B1@icloud.com>
	<CAO+b4j-uRjS3_hGR+hTUxLAchs8-N3Kne9=fxtZDa0q5Jmvtsw@mail.gmail.com>
Message-ID: <20160201151045.667361jq82awnf8c@www.staffmail.ed.ac.uk>

Hi Alberto,

having id=1:30 in the prediction grid should work.

I ideally it should work with a constant too, but I need to change the  
source code from

Z<-Z[ordering,]

to

Z<-Z[ordering,,drop=FALSE]

on L576. I will include it in the next update.

Coercing a 1-D matrix into numeric rather than vector has to be one of  
the most annoying design flaws in R!

Cheers,

Jarrod.




Quoting Alberto Gallano <alberto.gc8 at gmail.com> on Fri, 8 Jan 2016  
11:51:33 -0500:

> Hi Sven,
>
> thanks so much. That works. Do you know if there is any documentation as to
> how to set up new data for the MCMCglmm predict method? Adding a constant
> response vector to the prediction grid is different to every other R
> predict method (that I know of).
>
> One further problem i'm having is that I actually have random effects, and
> now the predict function throws an error when these are not entered into
> the prediction grid. It's unclear to me how they should be entered - using
> a constant doesn't work. The number of random effect grouping levels may
> not equal the dimensions of the rest of the prediction grid, so just adding
> a vector with each unique grouping level won't work here either. Any
> thoughts would be much appreciated. See "pred_grid2" at bottom of code
> below:
>
>
> #
> ----------------------------------------------------------------------------------
> library(MCMCglmm)
>
> set.seed(123)
>
> dat <- data.frame(
>     y = rnorm(100),
>     x = rnorm(100),
>     id = factor(rep(paste("ID", 1:20), 5))
>     )
>
> fit <- MCMCglmm(
>     fixed = y ~ x,
>     random = ~ id,
>     rcov = ~ units,
>     data = dat,
>     family = "gaussian",
>     pr = TRUE, pl = TRUE,
>     saveX = TRUE,  saveZ = TRUE,
>     nitt = 1.3e+4, thin = 10, burnin = 3e+3
> )
>
>
> pred_grid1 <- with(dat,
>     data.frame(
>     y = 0,
>     x = seq(-1, 1, length.out = 30)
>     ))
>
> predict(fit, newdata = pred_grid1)
>
> # > Error in buildZ(rmodel.terms[r], data = data, nginverse =
> names(ginverse)) : object id not found
>
>
> pred_grid2 <- with(dat,
>     data.frame(
>     y = 0,
>     x = seq(-1, 1, length.out = 30),
>     id = 0
>     ))
>
> predict(fit, newdata = pred_grid2)
>
> # > Error in MCMCglmm(fixed = object$Fixed$formula, random =
> object$Random$formula,  : trying to get slot "Dim" from an object of a
> basic class ("numeric") with no slots
> #
> ----------------------------------------------------------------------------------
>
> On Thu, Jan 7, 2016 at 3:06 PM, Sven E. Templer <sven.templer at gmail.com>
> wrote:
>
>> Hi Alberto,
>>
>> avoid the error from predict by running
>>
>> pred_grid$y <- 0
>>
>> before
>>
>> predict(fit, newdata = pred_grid)
>>
>> Best,
>> Sven
>>
>> > On 06 Jan 2016, at 23:39, Alberto Gallano <alberto.gc8 at gmail.com> wrote:
>> >
>> > I'm trying to make predictions from an MCMCglmm model using new data.
>> This
>> > is a feature that was recently added to the predict.MCMCglmm function
>> > (version 2.22). However, when I set things up as I would for other
>> predict
>> > methods, I get the following error:
>> >
>> >> Error in eval(expr, envir, enclos) : object 'y' not found
>> >
>> > where 'y' is my response vector. I'm including a simplified replicable
>> > example below. Is the set up of the prediction grid different for the
>> > MCMCglmm predict method compared with other methods?
>> >
>> > best,
>> > Alberto
>> >
>> > #
>> >
>> ----------------------------------------------------------------------------------
>> > library(MCMCglmm)
>> >
>> > set.seed(123)
>> >
>> > dat <- data.frame(x = rnorm(100), y = rnorm(100))
>> >
>> >
>> > fit <- MCMCglmm(
>> >
>> >    fixed = y ~ x,
>> >
>> >    rcov = ~ units,
>> >
>> >    data = dat,
>> >
>> >    family = "gaussian",
>> >
>> >    pr = TRUE, pl = TRUE,
>> >
>> >    saveX = TRUE,  saveZ = TRUE,
>> >
>> >    nitt = 1.3e+4, thin = 10, burnin = 3e+3
>> >
>> > )
>> >
>> >
>> > pred_grid <- data.frame(x = seq(-1, 1, length.out = 30))
>> >
>> >
>> > predict(fit, newdata = pred_grid)
>> >
>> > #
>> >
>> ----------------------------------------------------------------------------------
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From thierry.onkelinx at inbo.be  Wed Feb  3 09:41:50 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 3 Feb 2016 09:41:50 +0100
Subject: [R-sig-ME] [R] Use predict() after lmer() (library lme4)
In-Reply-To: <56B1A777.7060004@yahoo.fr>
References: <56B1A777.7060004@yahoo.fr>
Message-ID: <CAJuCY5z33EEtAt3i4qKYOUHQAfb8FLUHWQQWfjYf1OPDVLmaTA@mail.gmail.com>

Dear Marc,

This question is more suited for R-Sig-mixed models to which I'm forwarding
it.

Your manual predictions for out3 are wrong. Here are the correct manual
predictions.

fixed <- model.matrix(~effect, data = dataF) %*% fixef(out3)
random <- rowSums(model.matrix(~Sector, data = dataF) *
ranef(out3)$Beach[dataF$Beach, ])
range(fixed + random - predict(out3))

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-02-03 8:08 GMT+01:00 Marc Girondot <marc_grt at yahoo.fr>:

> Bonjour, (don't worry, after I will write in English [at least I will try
> ;) ])
>
> I try to understand better mixed models and then I have generated data and
> I try to understand how the fixed and the random effects are used in
> predict(). I understand when the random effect is of the form (1 | rf] but
> I don't understand for the form (rf1 | rf2]. Let do an example. The last
> formula does not give the same than predict().
>
> Thanks if someone could explain what formula use to reproduce the
> predict() results.
>
> Marc
>
> # 1/ Generate data in a data.frame, 1 response (number), one effect
> (effect) and two factors (sector and beach) that I want use as random
> effect. These two factors are hierarchical beach within sector
>
> Sector <- c(rep("I", 100), rep("II", 100))
> Beach <- c(
>   sample(c("A", "B", "C", "D", "E"), 100, replace=TRUE),
>   sample(c("F", "G"), 100, replace=TRUE)
>   )
>
> number <- rnorm(200, 10, 1)
>
> # Sector effect
> number[1:100] <- number[1:100] +0.1
> number[101:200] <- number[101:200] +0.5
>
> # beach effect
> beach.value <- 1:7
> names(beach.value) <- LETTERS[1:7]
> number <- number + unname(beach.value[Beach])
>
>
> dataF <- data.frame(number=number, effect= number/10+runif(200, 0, 2),
>                     Sector=Sector, Beach=Beach)
>
> plot(dataF$number, dataF$effect)
>
> ##############
> library("lme4")
> ##############
>
> ##############
> # 2/ Random effect is (1 | Beach). I can reproduce the predict()
> ##############
>
> out1 <- lmer(formula = number ~ effect + (1 | Sector) , data=dataF)
> head(predict(out1))
> ef <- fixef(out1)
> er <- ranef(out1)
>
> head(ef["(Intercept)"]+dataF$effect*ef["effect"]+
>   er$Sector[dataF$Sector, "(Intercept)"]
>   )
>
> ##############
> # 3/ Random effect is (1 | Beach) + (1 | Sector). I can reproduce the
> predict()
> ##############
>
> out2 <- lmer(formula = number ~ effect + (1 | Sector)  + (1 | Beach),
> data=dataF)
>
> head(predict(out2))
> ef <- fixef(out2)
> er <- ranef(out2)
>
> head(ef["(Intercept)"]+dataF$effect*ef["effect"]+
>        er$Sector[dataF$Sector, "(Intercept)"] +
>        er$Beach[dataF$Beach, "(Intercept)"]
>      )
>
> ##############
> # 4/ Random effect if (Sector | Beach). I don't understand how to
> reproduce the predict()
> ##############
>
> out3 <- lmer(formula = number ~ effect + (Sector | Beach), data=dataF)
>
> head(predict(out3))
> ef <- fixef(out3)
> er <- ranef(out3)
>
> head(ef["(Intercept)"]+dataF$effect*ef["effect"]+
>        er$Beach[dataF$Beach, "(Intercept)"]+
>        er$Beach[dataF$Beach, "SectorII"]
> )
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From vito.muggeo at unipa.it  Fri Feb  5 12:01:26 2016
From: vito.muggeo at unipa.it (Vito M. R. Muggeo)
Date: Fri, 5 Feb 2016 12:01:26 +0100
Subject: [R-sig-ME] R code for segmented mixed models with random
	changepoints
Message-ID: <56B48106.9090802@unipa.it>

dear all,
hopefully it is of interest for someone..

I have written a set of functions to fit segmented mixed models, i.e. 
piecewise linear relationships with random changepoints in a 
likelihood/frequentist framework.

The functions seem to work reasonably well, although some bugs are 
expected.. :-)

Code (and an example dataset) along with some worked examples 
illustrating the functions are available on ResearchGate:

https://www.researchgate.net/publication/292986444 (code and data)

https://www.researchgate.net/publication/292629179 (examples 
illustrating the code)

Bugs, questions, doubts and feedbacks are welcome!

Thanks,
vito



-- 
==============================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 23895240
fax: 091 485726
http://dssm.unipa.it/vmuggeo
Associate Editor, Statistical Modelling


From diamahendra at gmail.com  Fri Feb  5 02:32:30 2016
From: diamahendra at gmail.com (Mahendra Dia)
Date: Thu, 4 Feb 2016 20:32:30 -0500
Subject: [R-sig-ME] F and P values for random factors
Message-ID: <CAFCgp1riQipCgQCnLnSrjCSysFPYTi1=vfbZND0unEtegUm44A@mail.gmail.com>

Hi.

I am reaching you out to learn how to compute F ratio and P values for my
experiment where all the factors are treated as random factors.
Please see the attached file where I explained my treatments and sample
data.

I thank you in advance.

Sincerely,
Mahendra-

From peggy_moore at usgs.gov  Fri Feb  5 22:58:06 2016
From: peggy_moore at usgs.gov (Moore, Peggy)
Date: Fri, 5 Feb 2016 13:58:06 -0800
Subject: [R-sig-ME] glmmADMB error: no PSV file found
Message-ID: <CAPO7DkfyZAT-Yy5=ZFY8OZsxO7DABvK3Z4sXRBFZQvOOjh0hEw@mail.gmail.com>

I am using glmmadmb (glmmADMB_0.8.3.2 and R2admb_0.7.13) with R version
3.2.1 (2015-06-18) to fit a negative binomial mixed effects model to my
data. Response is repeated measures of plant counts following herbicide
treatments. There are 2 fixed effects (year, treatment) and 2 random
effects (block, plot:block). I successfully fit the model when mcmc=FALSE,
but when I add mcmc=TRUE, I receive the error: no PSV file found.

I noticed that when I load glmmADMB, I receive the warning:
Warning in install.packages :
  package ?glmmADMB? is not available (for R version 3.2.1)
Yet I have been running the package and it is in my library. Could there be
a version issue?

I have used save.dir to save output to various locations where I can both
read and write files, including the default location where the binary file
is located. The package writes between 38 and 43 files to the specified
location, including the .dat, .pin. .std files but not the .psv file. It is
not being written anywhere on my drive. I have tried changing many other
settings as well and still receive the error.

I get the same result with the Owls data that comes with the package and an
example from 'Getting Started with glmmADMB.' So, I don't think this is an
issue with the package, per se, but perhaps incompatibilities in my system.

Owls <- transform(Owls, Nest = reorder(Nest, NegPerChick), NCalls =
SiblingNegotiation)
setwd("C:/Temp")

### No MCMC
OwlModel_nb1_bs <- glmmadmb(NCalls ~ (FoodTreatment + ArrivalTime) *
SexParent +
                                 BroodSize + (1 | Nest), data = Owls,
zeroInflation = TRUE, family = "nbinom1",
                                 debug=TRUE,#
admb.opts=admbControl(run=FALSE),
                                 save.dir = "C:/Temp")
summary(OwlModel_nb1_bs)

AIC: 3349.6

Coefficients:
                                    Estimate Std. Error z value Pr(>|z|)
(Intercept)                           4.2896     0.8954    4.79  1.7e-06 ***
FoodTreatmentSatiated                -0.9039     0.1404   -6.44  1.2e-10 ***
ArrivalTime                          -0.1194     0.0343   -3.48   0.0005 ***
SexParentMale                         0.4169     1.0527    0.40   0.6921
BroodSize                             0.1982     0.0714    2.78   0.0055 **
FoodTreatmentSatiated:SexParentMale   0.2599     0.1743    1.49   0.1359
ArrivalTime:SexParentMale            -0.0192     0.0433   -0.44   0.6572
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Number of observations: total=599, Nest=27
Random effect variance(s):
Group=Nest
            Variance StdDev
(Intercept)   0.1301 0.3607

Negative binomial dispersion parameter: 5.9086 (std. err.: 0.55992)
Zero-inflation: 0.089995  (std. err.:  0.028842 )

Log-likelihood: -1664.8


### With mcmc = TRUE
OwlModel_nb1_bs_mcmc <- glmmadmb(NCalls ~ (FoodTreatment + ArrivalTime) *
SexParent +
                                 BroodSize + (1 | Nest), data = Owls,
zeroInflation = TRUE, family = "nbinom1",
                                 mcmc = TRUE, mcmc.opts =
mcmcControl(mcmc=25000, mcsave=25),
                                 debug=TRUE,#
admb.opts=admbControl(run=FALSE),
                                 save.dir = "C:/Temp")
summary(OwlModel_nb1_bs_mcmc)
debug output:
platform: windows 64
executable name: glmmadmb.exe
bin_loc:
C:/Users/peggy_moore/Documents/R/R-3.2.1/library/glmmADMB/bin/windows64/glmmadmb.exe
saving files into directory C:/Temp
changed working directory to C:/Temp
writing .dat and .pin files
working directory: C:/Temp
Command line: glmmadmb -maxfn 500 -maxph 5 -noinit -shess -mcmc 25000
-mcsave 25 -mcmult 1

Response:
Error in R2admb::read_psv(file_name) : no PSV file found
In addition: Warning messages:
1: In glmmadmb(NCalls ~ (FoodTreatment + ArrivalTime) * SexParent +  :
  file glmmadmb.std exists: overwriting
2: running command 'C:\Windows\system32\cmd.exe /c glmmadmb -maxfn 500
-maxph 5 -noinit -shess -mcmc 25000 -mcsave 25 -mcmult 1' had status 42

I have checked online resources and found no resolution and very little
discussion on the subject. I fit the models
with admb.opts=admbControl(run=FALSE) and ran it from the cmd prompt, but I
see the purpose of that exercise is to provide .dat and .pin files to an
ADMB expert to evaluate, and I don't know an expert.

I would really appreciate any tips someone can provide.
Peggy Moore

My sessionInfo in case that's informative:
R version 3.2.1 (2015-06-18)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
States.1252    LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                           LC_TIME=English_United
States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] R2admb_0.7.13    glmmADMB_0.8.3.2 MASS_7.3-43

loaded via a namespace (and not attached):
 [1] Matrix_1.2-2    plyr_1.8.3      magrittr_1.5    tools_3.2.1
coda_0.17-1     Rcpp_0.11.6     stringi_0.5-5
 [8] nlme_3.1-121    grid_3.2.1      stringr_1.0.0   lattice_0.20-33

	[[alternative HTML version deleted]]


From abfine at gmail.com  Sat Feb  6 01:05:17 2016
From: abfine at gmail.com (Alex Fine)
Date: Fri, 5 Feb 2016 19:05:17 -0500
Subject: [R-sig-ME] F and P values for random factors
In-Reply-To: <CAFCgp1riQipCgQCnLnSrjCSysFPYTi1=vfbZND0unEtegUm44A@mail.gmail.com>
References: <CAFCgp1riQipCgQCnLnSrjCSysFPYTi1=vfbZND0unEtegUm44A@mail.gmail.com>
Message-ID: <CAJ6ui+OGnSV0z3RCXLTDAKf-bRhFZ=ArdoFiLzF-jYEbQbv4Ag@mail.gmail.com>

Hi Mahendra,

You can assess the significance of both fixed and random factors using
likelihood ratio tests.  Say you want to test the significance of predictor
A in a model with predictors A and B and random slopes for A.  You can do:

full_model = lmer(y ~ A + B)
model_without_A = lmer(y ~ B)
anova(full_model, model_without_A)

The anova() function in this case will return a chi-squared score (df =
number of predic) and a p-value.  The same procedure can be used for random
effects, e.g.:

full_model_2 = lmer(y ~ A + B + (1+A | random_thing)
model_without_random_slope_for_A = lmer(y ~ A + B + (1 | random_thing)
anova(full_model, model_without_random_slope_for_A)

This works because the log-likelihoods of nested models, in the limit,
approximate a chi-squared distribution.

See:  https://en.wikipedia.org/wiki/Likelihood-ratio_test

Or maybe that wasn't what you were asking at all.

Also I think you forgot to attach the file.

Hope that helps!
Alex


On Thu, Feb 4, 2016 at 8:32 PM, Mahendra Dia <diamahendra at gmail.com> wrote:

> Hi.
>
> I am reaching you out to learn how to compute F ratio and P values for my
> experiment where all the factors are treated as random factors.
> Please see the attached file where I explained my treatments and sample
> data.
>
> I thank you in advance.
>
> Sincerely,
> Mahendra-
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
Alex Fine
Ph. (336) 302-3251
web:  abfine.github.io/

	[[alternative HTML version deleted]]


From tom_philippi at nps.gov  Sat Feb  6 01:16:09 2016
From: tom_philippi at nps.gov (Philippi, Tom)
Date: Fri, 5 Feb 2016 16:16:09 -0800
Subject: [R-sig-ME] F and P values for random factors
In-Reply-To: <CAJ6ui+OGnSV0z3RCXLTDAKf-bRhFZ=ArdoFiLzF-jYEbQbv4Ag@mail.gmail.com>
References: <CAFCgp1riQipCgQCnLnSrjCSysFPYTi1=vfbZND0unEtegUm44A@mail.gmail.com>
	<CAJ6ui+OGnSV0z3RCXLTDAKf-bRhFZ=ArdoFiLzF-jYEbQbv4Ag@mail.gmail.com>
Message-ID: <CAM9kYqhNym3NwNRLShkOw8aGuHCWt1htYZuYODAAPAkaRetb-A@mail.gmail.com>

A perhaps more comprehensive answer is available in the draft
r-sig-mixed-models FAQ:
http://glmm.wikidot.com/faq

Tom 2


On Fri, Feb 5, 2016 at 4:05 PM, Alex Fine <abfine at gmail.com> wrote:

> Hi Mahendra,
>
> You can assess the significance of both fixed and random factors using
> likelihood ratio tests.  Say you want to test the significance of predictor
> A in a model with predictors A and B and random slopes for A.  You can do:
>
> full_model = lmer(y ~ A + B)
> model_without_A = lmer(y ~ B)
> anova(full_model, model_without_A)
>
> The anova() function in this case will return a chi-squared score (df =
> number of predic) and a p-value.  The same procedure can be used for random
> effects, e.g.:
>
> full_model_2 = lmer(y ~ A + B + (1+A | random_thing)
> model_without_random_slope_for_A = lmer(y ~ A + B + (1 | random_thing)
> anova(full_model, model_without_random_slope_for_A)
>
> This works because the log-likelihoods of nested models, in the limit,
> approximate a chi-squared distribution.
>
> See:  https://en.wikipedia.org/wiki/Likelihood-ratio_test
>
> Or maybe that wasn't what you were asking at all.
>
> Also I think you forgot to attach the file.
>
> Hope that helps!
> Alex
>
>
> On Thu, Feb 4, 2016 at 8:32 PM, Mahendra Dia <diamahendra at gmail.com>
> wrote:
>
> > Hi.
> >
> > I am reaching you out to learn how to compute F ratio and P values for my
> > experiment where all the factors are treated as random factors.
> > Please see the attached file where I explained my treatments and sample
> > data.
> >
> > I thank you in advance.
> >
> > Sincerely,
> > Mahendra-
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>
>
> --
> Alex Fine
> Ph. (336) 302-3251
> web:  abfine.github.io/
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From chelucero at uchicago.edu  Sun Feb  7 21:56:32 2016
From: chelucero at uchicago.edu (=?UTF-8?Q?Ch=C3=A9_Lucero?=)
Date: Sun, 07 Feb 2016 20:56:32 +0000
Subject: [R-sig-ME] Plotting means and error bars for fixed effects in a ME
	model
Message-ID: <CAJOWFj8JZj_V-X_yE7A6yK6JfpG03LEkjWtMT8hn0sK4zUZ0YA@mail.gmail.com>

Hello, list.

I have a question about error bars on barplotted means of data that has
been analyzed with mixed effects models.

Mixed effects models allow us to simultaneously model variance from
multiple random-effects sources (such as Subjects and Items), a much needed
solution to a problem pointed out (most potently) in my field by Herb Clark
in 1973.

However, to my knowledge, calculating confidence interval/SEM for means has
not advanced to take this modeling approach into account. A series of
papers - Loftus and Masson 94 (Using confidence intervals in within-subject
designs)  Cousineau 05  (Confidence intervals in within-subject designs: a
simpler solution) and Morey 05 (Confidence Intervals from Normalized Data:
A correction to Cousineau)   has specified ways to calculate error bars in
which the within-Subject error has been accounted for.

That approach to calculating error bars for plotting seems to be well
matched for when the statistical procedure is a simple repeated measures
(for Subjects OR Items) ANOVA. But what about the case where I have both
within-Subjects and within-Items (and within-XYZ random factors too)? Is
there a way to construct standard errors of the mean for levels of a fixed
factor (e.g. Condition) taking more than one random-effect into account as
mixed effects models do? It would be really great to have the charts
visually suggest the same inferences that the ME statistics do, but I can't
find any literature about it.

Thanks for your time.

-Ch?

	[[alternative HTML version deleted]]


From pauljohn32 at gmail.com  Sun Feb  7 22:21:40 2016
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Sun, 7 Feb 2016 15:21:40 -0600
Subject: [R-sig-ME] Plotting means and error bars for fixed effects in a
 ME model
In-Reply-To: <CAJOWFj8JZj_V-X_yE7A6yK6JfpG03LEkjWtMT8hn0sK4zUZ0YA@mail.gmail.com>
References: <CAJOWFj8JZj_V-X_yE7A6yK6JfpG03LEkjWtMT8hn0sK4zUZ0YA@mail.gmail.com>
Message-ID: <CAErODj8rD0vYrqiBf4MP9LKHF2tHP6c3HQr13-tNeVE5u=Cdpg@mail.gmail.com>

On Sun, Feb 7, 2016 at 2:56 PM, Ch? Lucero <chelucero at uchicago.edu> wrote:
> Hello, list.
>
> I have a question about error bars on barplotted means of data that has
> been analyzed with mixed effects models.
>
> Mixed effects models allow us to simultaneously model variance from
> multiple random-effects sources (such as Subjects and Items), a much needed
> solution to a problem pointed out (most potently) in my field by Herb Clark
> in 1973.
>
Hi.

You will get better help if you rephrase your question with a fitted
lme4 object and let us see what you are trying to do.  As it is, your
question asks us to wade throught a lot of ANOVA jargon. I'd rather
see your model, data, and the number for which you want the CI.

It appears that even for glm without random effects, there is no
settled upon idea for CI on predicted values. You can fall down a
rabbit hole reading math articles about that. I don't think the
presence of random effects makes that tougher, its just another
element to add in the error bands.  The model you specify when you fit
dictates what's correlated.

I had to improvise some CIs last semester. In the package "arm"
(support for the Gelman & Hill textbook), there's a function called
se.coef and they do the usual plus or minus 2 of those to make error
bars that look like CIs for predicted values. That works because if
you calculate some linear sum, you can use the variances of individual
elements to get a Wald type CI.

I am pretty sure the best answer is simulation with lme4 objects,
however. But don't have code to share to you.

Good luck, think about showing everybody code and such that they can focus on.

pj

> However, to my knowledge, calculating confidence interval/SEM for means has
> not advanced to take this modeling approach into account. A series of
> papers - Loftus and Masson 94 (Using confidence intervals in within-subject
> designs)  Cousineau 05  (Confidence intervals in within-subject designs: a
> simpler solution) and Morey 05 (Confidence Intervals from Normalized Data:
> A correction to Cousineau)   has specified ways to calculate error bars in
> which the within-Subject error has been accounted for.
>
> That approach to calculating error bars for plotting seems to be well
> matched for when the statistical procedure is a simple repeated measures
> (for Subjects OR Items) ANOVA. But what about the case where I have both
> within-Subjects and within-Items (and within-XYZ random factors too)? Is
> there a way to construct standard errors of the mean for levels of a fixed
> factor (e.g. Condition) taking more than one random-effect into account as
> mixed effects models do? It would be really great to have the charts
> visually suggest the same inferences that the ME statistics do, but I can't
> find any literature about it.
>
> Thanks for your time.
>
> -Ch?
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Paul E. Johnson
Professor, Political Science        Director
1541 Lilac Lane, Room 504      Center for Research Methods
University of Kansas                 University of Kansas
http://pj.freefaculty.org              http://crmda.ku.edu


From bbolker at gmail.com  Sun Feb  7 22:46:36 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 7 Feb 2016 16:46:36 -0500
Subject: [R-sig-ME] [ADMB Users] getting standardized coefficients in
	admb
In-Reply-To: <3843b85a-ca32-41b8-934a-80dc4175963e@admb-project.org>
References: <d8b9a2de-704c-4f6e-adb9-c0f3541ce570@admb-project.org>
	<CAJMx2XU0Egbi=q90c8X6Ccj2tmU-M7Oh7arA5-rCC+VW8t+uCw@mail.gmail.com>
	<56563821.6020603@gmail.com>
	<3843b85a-ca32-41b8-934a-80dc4175963e@admb-project.org>
Message-ID: <CABghstTz_spXTtp=VEUc8gaCq5aDHqYz-SLCsa7BGCNWLAAb6w@mail.gmail.com>

(I think this is more appropriate for r-sig-mixed-models, but I'm
leaving ADMB users cc'd for this last response.)

It's not obvious to me whether there's a simple analogue of
standardizing by response variance in the GLMM world.  I suppose you
*could* still standardize by predictor variance, or you could decide
that the link functions (log for NB/Poisson, logit for binomial)
effectively standardize the prediction side of the model.  It looks
like the last section of Schielzeth's 2010 MEE paper "Simple means
...", "Extensions", discusses this issue, but I haven't read it
carefully/absorbed it/tried to implement that in a function.

  cheers
    Ben


On Sun, Feb 7, 2016 at 2:47 PM, Ellen Robertson <robertsonep at gmail.com> wrote:
> Ben,
>    Sorry for the delayed response. In my earlier email, I was referring to
> your post on
> http://r-sig-mixed-models.r-project.narkive.com/1EtbqR8T/r-sig-me-standardized-coefficients-in-glmer-model
> where you talk about using a function similar to the 'lm.beta' function for
> getting standardized coefficients from lmer models ('lm.beta.lmer') .
>      I'm trying to get standardized beta coefficients from different types
> of glmer models (poisson, binomial, Gaussian) so that I can compare the
> effect sizes from each of these (I'm using all three of these different
> types of glmer models within a piecewise structural equation model and want
> to be able to compare the strengths of different paths).  I know that with
> continuous response/predictor variables I can just scale everything before
> running the model and that will output standardized beta coefficients.  But
> I am unsure of do this with non-continuous variables (such as a binomial
> response variable)?    You show (in the link above) how to scale binomial
> predictor variables (change them to numeric, 0/1, rather than
> male/female..and then scale)...but how would you do this with a binomial
> response variable which has to be 0/1?  I tried your "lm.beta.lmer" function
> and it worked when I had 2 predictors in my model but for some reason it
> didn't work with only one predictor variable.  I also wasn't sure if it
> would work with poisson/binomial models or if it only worked with lmer.
>     Thanks for any help you can give.  Cheers,
> Ellen
>
>
>
>
> On Wednesday, November 25, 2015 at 5:37:25 PM UTC-5, Ben Bolker wrote:
>>
>>  I meant to respond to this earlier (maybe I did, and maybe it fell
>> through the cracks).
>>
>>    Ellen, it's not clear whether you're asking about generic ADMB models
>> or about glmmADMB models: if the latter, then
>> r-sig-mix... at r-project.org is probably the more appropriate venue.
>>  If the former, then I'm not even sure what you would mean by
>> "standardized coefficients", as it would probably depend on the model.
>>
>>   Can you give a link/reference for "Bolker's code for beta.lmer for
>> glmer models"?
>>
>>   The very generic answer to your question is that you can either (1)
>> scale/center your continuous input variables *before* running the model
>> or (2) adjust the coefficients afterward, based on the means and
>> standard deviations of the parameters.  This
>>
>>
>> http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon/23643740#23643740
>>
>> gives a function that rescales parameters -- it should be ecumenical
>> (i.e., apply to any set of coefficients from a linear or generalized
>> linear model, no matter what software it was fitted with).
>>
>>
>> On 15-11-25 05:30 PM, Johnoel Ancheta wrote:
>> > Is this possible?
>> >
>> > On Mon, Nov 23, 2015 at 7:31 AM, Ellen Robertson <rober... at gmail.com>
>> > wrote:
>> >
>> >> Hi everyone,
>> >>    Is it possible to get standardized coefficients from admb models?  I
>> >> know about lm.beta for linear models and saw Bolkner's code for
>> >> beta.lmer
>> >> for glmer models....but I have been unable to get standardized
>> >> coefficients
>> >> from my admb models.  Thanks for your help,
>> >> Ellen
>> >>
>> >> --
>> >> You received this message because you are subscribed to the Google
>> >> Groups
>> >> "ADMB Users" group.
>> >> To unsubscribe from this group and stop receiving emails from it, send
>> >> an
>> >> email to users+un... at admb-project.org.
>> >> For more options, visit
>> >> https://groups.google.com/a/admb-project.org/d/optout.
>> >>
>> >
>>
>


From chelucero at uchicago.edu  Sun Feb  7 22:51:40 2016
From: chelucero at uchicago.edu (=?UTF-8?Q?Ch=C3=A9_Lucero?=)
Date: Sun, 07 Feb 2016 21:51:40 +0000
Subject: [R-sig-ME] Plotting means and error bars for fixed effects in a
 ME model
In-Reply-To: <CAErODj8rD0vYrqiBf4MP9LKHF2tHP6c3HQr13-tNeVE5u=Cdpg@mail.gmail.com>
References: <CAJOWFj8JZj_V-X_yE7A6yK6JfpG03LEkjWtMT8hn0sK4zUZ0YA@mail.gmail.com>
	<CAErODj8rD0vYrqiBf4MP9LKHF2tHP6c3HQr13-tNeVE5u=Cdpg@mail.gmail.com>
Message-ID: <CAJOWFj-bXj_ZmCVweYU+FMw3qgBj3of1Ck4_Q5qoHhAcptFcAA@mail.gmail.com>

Hi PJ.

An example:

library(lme4)
dat <- expand.grid(Condition = c('A', 'B'), Subject = 1:10,
                   Item = c('popcorn', 'butter', 'rosemary', 'salt',
'yeast'))
dat$Subject <- factor(dat$Subject)
size <- nrow(dat)
dat$DV <- numeric(size)
dat[dat$Condition == 'A', 'DV'] <- rnorm(size/2, 8, 1)
dat[dat$Condition == 'B', 'DV'] <- rnorm(size/2, 9, 1)

barplot(tapply(dat$DV, dat$Condition, FUN=mean))

m.1 <- lmer(DV ~ Condition + (1|Subject) + (1|Item), data=dat)
m.2 <- update(m.1, . ~ . - Condition)
anova(m.1, m.2)

I don't want a standard error on anything from the model per se. I want to
build a standard error of the mean for the two Condition means for
charting. My example lmer model takes two random effects into account:
Subject and Item. I want to make a plot in which the standard error of the
mean takes into account both the Subject and Item variance (like my model).
I am only aware of how construct the standard error of the mean taking one
of those into account at a time. Hope I was clearer this time!

Thanks,

-Ch?

On Sun, Feb 7, 2016 at 3:21 PM Paul Johnson <pauljohn32 at gmail.com> wrote:

> On Sun, Feb 7, 2016 at 2:56 PM, Ch? Lucero <chelucero at uchicago.edu> wrote:
> > Hello, list.
> >
> > I have a question about error bars on barplotted means of data that has
> > been analyzed with mixed effects models.
> >
> > Mixed effects models allow us to simultaneously model variance from
> > multiple random-effects sources (such as Subjects and Items), a much
> needed
> > solution to a problem pointed out (most potently) in my field by Herb
> Clark
> > in 1973.
> >
> Hi.
>
> You will get better help if you rephrase your question with a fitted
> lme4 object and let us see what you are trying to do.  As it is, your
> question asks us to wade throught a lot of ANOVA jargon. I'd rather
> see your model, data, and the number for which you want the CI.
>
> It appears that even for glm without random effects, there is no
> settled upon idea for CI on predicted values. You can fall down a
> rabbit hole reading math articles about that. I don't think the
> presence of random effects makes that tougher, its just another
> element to add in the error bands.  The model you specify when you fit
> dictates what's correlated.
>
> I had to improvise some CIs last semester. In the package "arm"
> (support for the Gelman & Hill textbook), there's a function called
> se.coef and they do the usual plus or minus 2 of those to make error
> bars that look like CIs for predicted values. That works because if
> you calculate some linear sum, you can use the variances of individual
> elements to get a Wald type CI.
>
> I am pretty sure the best answer is simulation with lme4 objects,
> however. But don't have code to share to you.
>
> Good luck, think about showing everybody code and such that they can focus
> on.
>
> pj
>
> > However, to my knowledge, calculating confidence interval/SEM for means
> has
> > not advanced to take this modeling approach into account. A series of
> > papers - Loftus and Masson 94 (Using confidence intervals in
> within-subject
> > designs)  Cousineau 05  (Confidence intervals in within-subject designs:
> a
> > simpler solution) and Morey 05 (Confidence Intervals from Normalized
> Data:
> > A correction to Cousineau)   has specified ways to calculate error bars
> in
> > which the within-Subject error has been accounted for.
> >
> > That approach to calculating error bars for plotting seems to be well
> > matched for when the statistical procedure is a simple repeated measures
> > (for Subjects OR Items) ANOVA. But what about the case where I have both
> > within-Subjects and within-Items (and within-XYZ random factors too)? Is
> > there a way to construct standard errors of the mean for levels of a
> fixed
> > factor (e.g. Condition) taking more than one random-effect into account
> as
> > mixed effects models do? It would be really great to have the charts
> > visually suggest the same inferences that the ME statistics do, but I
> can't
> > find any literature about it.
> >
> > Thanks for your time.
> >
> > -Ch?
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
> --
> Paul E. Johnson
> Professor, Political Science        Director
> 1541 Lilac Lane, Room 504      Center for Research Methods
> University of Kansas                 University of Kansas
> http://pj.freefaculty.org              http://crmda.ku.edu
>

	[[alternative HTML version deleted]]


From dan at danmirman.org  Mon Feb  8 11:15:51 2016
From: dan at danmirman.org (Dan Mirman)
Date: Mon, 8 Feb 2016 05:15:51 -0500
Subject: [R-sig-ME] Plotting means and error bars for fixed effects in a
 ME model
In-Reply-To: <CAJOWFj-bXj_ZmCVweYU+FMw3qgBj3of1Ck4_Q5qoHhAcptFcAA@mail.gmail.com>
References: <CAJOWFj8JZj_V-X_yE7A6yK6JfpG03LEkjWtMT8hn0sK4zUZ0YA@mail.gmail.com>
	<CAErODj8rD0vYrqiBf4MP9LKHF2tHP6c3HQr13-tNeVE5u=Cdpg@mail.gmail.com>
	<CAJOWFj-bXj_ZmCVweYU+FMw3qgBj3of1Ck4_Q5qoHhAcptFcAA@mail.gmail.com>
Message-ID: <CAGfVdyrJFwzR1g8RgwcH_55h8OLJ+d1uFc=R4hDvPkr0cahT2Q@mail.gmail.com>

I think both the effects and lsmeans packages can do this for you:

library(effects)
as.data.frame(effect("Condition", m.1))

library(lsmeans)
lsmeans(m.1, "Condition")

On Sun, Feb 7, 2016 at 4:51 PM, Ch? Lucero <chelucero at uchicago.edu> wrote:

> Hi PJ.
>
> An example:
>
> library(lme4)
> dat <- expand.grid(Condition = c('A', 'B'), Subject = 1:10,
>                    Item = c('popcorn', 'butter', 'rosemary', 'salt',
> 'yeast'))
> dat$Subject <- factor(dat$Subject)
> size <- nrow(dat)
> dat$DV <- numeric(size)
> dat[dat$Condition == 'A', 'DV'] <- rnorm(size/2, 8, 1)
> dat[dat$Condition == 'B', 'DV'] <- rnorm(size/2, 9, 1)
>
> barplot(tapply(dat$DV, dat$Condition, FUN=mean))
>
> m.1 <- lmer(DV ~ Condition + (1|Subject) + (1|Item), data=dat)
> m.2 <- update(m.1, . ~ . - Condition)
> anova(m.1, m.2)
>
> I don't want a standard error on anything from the model per se. I want to
> build a standard error of the mean for the two Condition means for
> charting. My example lmer model takes two random effects into account:
> Subject and Item. I want to make a plot in which the standard error of the
> mean takes into account both the Subject and Item variance (like my model).
> I am only aware of how construct the standard error of the mean taking one
> of those into account at a time. Hope I was clearer this time!
>
> Thanks,
>
> -Ch?
>
> On Sun, Feb 7, 2016 at 3:21 PM Paul Johnson <pauljohn32 at gmail.com> wrote:
>
> > On Sun, Feb 7, 2016 at 2:56 PM, Ch? Lucero <chelucero at uchicago.edu>
> wrote:
> > > Hello, list.
> > >
> > > I have a question about error bars on barplotted means of data that has
> > > been analyzed with mixed effects models.
> > >
> > > Mixed effects models allow us to simultaneously model variance from
> > > multiple random-effects sources (such as Subjects and Items), a much
> > needed
> > > solution to a problem pointed out (most potently) in my field by Herb
> > Clark
> > > in 1973.
> > >
> > Hi.
> >
> > You will get better help if you rephrase your question with a fitted
> > lme4 object and let us see what you are trying to do.  As it is, your
> > question asks us to wade throught a lot of ANOVA jargon. I'd rather
> > see your model, data, and the number for which you want the CI.
> >
> > It appears that even for glm without random effects, there is no
> > settled upon idea for CI on predicted values. You can fall down a
> > rabbit hole reading math articles about that. I don't think the
> > presence of random effects makes that tougher, its just another
> > element to add in the error bands.  The model you specify when you fit
> > dictates what's correlated.
> >
> > I had to improvise some CIs last semester. In the package "arm"
> > (support for the Gelman & Hill textbook), there's a function called
> > se.coef and they do the usual plus or minus 2 of those to make error
> > bars that look like CIs for predicted values. That works because if
> > you calculate some linear sum, you can use the variances of individual
> > elements to get a Wald type CI.
> >
> > I am pretty sure the best answer is simulation with lme4 objects,
> > however. But don't have code to share to you.
> >
> > Good luck, think about showing everybody code and such that they can
> focus
> > on.
> >
> > pj
> >
> > > However, to my knowledge, calculating confidence interval/SEM for means
> > has
> > > not advanced to take this modeling approach into account. A series of
> > > papers - Loftus and Masson 94 (Using confidence intervals in
> > within-subject
> > > designs)  Cousineau 05  (Confidence intervals in within-subject
> designs:
> > a
> > > simpler solution) and Morey 05 (Confidence Intervals from Normalized
> > Data:
> > > A correction to Cousineau)   has specified ways to calculate error bars
> > in
> > > which the within-Subject error has been accounted for.
> > >
> > > That approach to calculating error bars for plotting seems to be well
> > > matched for when the statistical procedure is a simple repeated
> measures
> > > (for Subjects OR Items) ANOVA. But what about the case where I have
> both
> > > within-Subjects and within-Items (and within-XYZ random factors too)?
> Is
> > > there a way to construct standard errors of the mean for levels of a
> > fixed
> > > factor (e.g. Condition) taking more than one random-effect into account
> > as
> > > mixed effects models do? It would be really great to have the charts
> > > visually suggest the same inferences that the ME statistics do, but I
> > can't
> > > find any literature about it.
> > >
> > > Thanks for your time.
> > >
> > > -Ch?
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> >
> >
> > --
> > Paul E. Johnson
> > Professor, Political Science        Director
> > 1541 Lilac Lane, Room 504      Center for Research Methods
> > University of Kansas                 University of Kansas
> > http://pj.freefaculty.org              http://crmda.ku.edu
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
-----------------------------------------------------
Dan Mirman
Assistant Professor
Department of Psychology
Drexel University
http://www.danmirman.org
-----------------------------------------------------

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Tue Feb  9 17:03:06 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 9 Feb 2016 11:03:06 -0500
Subject: [R-sig-ME] [ADMB Users] getting standardized coefficients in
	admb
In-Reply-To: <CANm5ooWhnAR2BJbNk0uFB3CC0WFNx8v_yaAQBfEJWe8_Gs8Thg@mail.gmail.com>
References: <d8b9a2de-704c-4f6e-adb9-c0f3541ce570@admb-project.org>
	<CAJMx2XU0Egbi=q90c8X6Ccj2tmU-M7Oh7arA5-rCC+VW8t+uCw@mail.gmail.com>
	<56563821.6020603@gmail.com>
	<3843b85a-ca32-41b8-934a-80dc4175963e@admb-project.org>
	<CABghstTz_spXTtp=VEUc8gaCq5aDHqYz-SLCsa7BGCNWLAAb6w@mail.gmail.com>
	<CANm5ooWhnAR2BJbNk0uFB3CC0WFNx8v_yaAQBfEJWe8_Gs8Thg@mail.gmail.com>
Message-ID: <56BA0DBA.20302@gmail.com>


   Actually, now that I look at the article more carefully it turns out 
that those paragraphs are mostly focused on *mixed* models, and don't 
say too much about how the argument generalizes (so to speak) to the 
generalized-linear (mixed) model case.  Scaling the *estimated 
parameter* by 1/standard deviation of the response is not insane (you 
can't scale the response variable *before* you fit the model in a GLM, 
that doesn't make sense), but doesn't have the same nice interpretation 
as in a linear model.  In general the link functions do put the 
parameters on a simple, dimensionless scale, but I'm not sure about a 
sensible, general way to compare among parameters of models fitted with 
*different* link functions.

On 16-02-09 09:49 AM, Ellen Robertson wrote:
> Thanks very much for your response and for pointing me in the direction
> of that article. Cheers, Ellen
>
> On Sun, Feb 7, 2016 at 4:46 PM, Ben Bolker <bbolker at gmail.com
> <mailto:bbolker at gmail.com>> wrote:
>
>     (I think this is more appropriate for r-sig-mixed-models, but I'm
>     leaving ADMB users cc'd for this last response.)
>
>     It's not obvious to me whether there's a simple analogue of
>     standardizing by response variance in the GLMM world.  I suppose you
>     *could* still standardize by predictor variance, or you could decide
>     that the link functions (log for NB/Poisson, logit for binomial)
>     effectively standardize the prediction side of the model.  It looks
>     like the last section of Schielzeth's 2010 MEE paper "Simple means
>     ...", "Extensions", discusses this issue, but I haven't read it
>     carefully/absorbed it/tried to implement that in a function.
>
>        cheers
>          Ben
>
>
>     On Sun, Feb 7, 2016 at 2:47 PM, Ellen Robertson
>     <robertsonep at gmail.com <mailto:robertsonep at gmail.com>> wrote:
>      > Ben,
>      >    Sorry for the delayed response. In my earlier email, I was
>     referring to
>      > your post on
>      >
>     http://r-sig-mixed-models.r-project.narkive.com/1EtbqR8T/r-sig-me-standardized-coefficients-in-glmer-model
>      > where you talk about using a function similar to the 'lm.beta'
>     function for
>      > getting standardized coefficients from lmer models ('lm.beta.lmer') .
>      >      I'm trying to get standardized beta coefficients from
>     different types
>      > of glmer models (poisson, binomial, Gaussian) so that I can
>     compare the
>      > effect sizes from each of these (I'm using all three of these
>     different
>      > types of glmer models within a piecewise structural equation
>     model and want
>      > to be able to compare the strengths of different paths).  I know
>     that with
>      > continuous response/predictor variables I can just scale
>     everything before
>      > running the model and that will output standardized beta
>     coefficients.  But
>      > I am unsure of do this with non-continuous variables (such as a
>     binomial
>      > response variable)?    You show (in the link above) how to scale
>     binomial
>      > predictor variables (change them to numeric, 0/1, rather than
>      > male/female..and then scale)...but how would you do this with a
>     binomial
>      > response variable which has to be 0/1?  I tried your
>     "lm.beta.lmer" function
>      > and it worked when I had 2 predictors in my model but for some
>     reason it
>      > didn't work with only one predictor variable.  I also wasn't sure
>     if it
>      > would work with poisson/binomial models or if it only worked with
>     lmer.
>      >     Thanks for any help you can give.  Cheers,
>      > Ellen
>      >
>      >
>      >
>      >
>      > On Wednesday, November 25, 2015 at 5:37:25 PM UTC-5, Ben Bolker
>     wrote:
>      >>
>      >>  I meant to respond to this earlier (maybe I did, and maybe it fell
>      >> through the cracks).
>      >>
>      >>    Ellen, it's not clear whether you're asking about generic
>     ADMB models
>      >> or about glmmADMB models: if the latter, then
>      >> r-sig-mix... at r-project.org <mailto:r-sig-mix... at r-project.org>
>     is probably the more appropriate venue.
>      >>  If the former, then I'm not even sure what you would mean by
>      >> "standardized coefficients", as it would probably depend on the
>     model.
>      >>
>      >>   Can you give a link/reference for "Bolker's code for beta.lmer for
>      >> glmer models"?
>      >>
>      >>   The very generic answer to your question is that you can
>     either (1)
>      >> scale/center your continuous input variables *before* running
>     the model
>      >> or (2) adjust the coefficients afterward, based on the means and
>      >> standard deviations of the parameters.  This
>      >>
>      >>
>      >>
>     http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon/23643740#23643740
>      >>
>      >> gives a function that rescales parameters -- it should be ecumenical
>      >> (i.e., apply to any set of coefficients from a linear or generalized
>      >> linear model, no matter what software it was fitted with).
>      >>
>      >>
>      >> On 15-11-25 05:30 PM, Johnoel Ancheta wrote:
>      >> > Is this possible?
>      >> >
>      >> > On Mon, Nov 23, 2015 at 7:31 AM, Ellen Robertson
>     <rober... at gmail.com <mailto:rober... at gmail.com>>
>      >> > wrote:
>      >> >
>      >> >> Hi everyone,
>      >> >>    Is it possible to get standardized coefficients from admb
>     models?  I
>      >> >> know about lm.beta for linear models and saw Bolkner's code for
>      >> >> beta.lmer
>      >> >> for glmer models....but I have been unable to get standardized
>      >> >> coefficients
>      >> >> from my admb models.  Thanks for your help,
>      >> >> Ellen
>      >> >>
>      >> >> --
>      >> >> You received this message because you are subscribed to the
>     Google
>      >> >> Groups
>      >> >> "ADMB Users" group.
>      >> >> To unsubscribe from this group and stop receiving emails from
>     it, send
>      >> >> an
>      >> >> email to users+un... at admb-project.org
>     <mailto:users%2Bun... at admb-project.org>.
>      >> >> For more options, visit
>      >> >> https://groups.google.com/a/admb-project.org/d/optout.
>      >> >>
>      >> >
>      >>
>      >
>
>


From robertsonep at gmail.com  Tue Feb  9 15:49:20 2016
From: robertsonep at gmail.com (Ellen Robertson)
Date: Tue, 9 Feb 2016 09:49:20 -0500
Subject: [R-sig-ME] [ADMB Users] getting standardized coefficients in
	admb
In-Reply-To: <CABghstTz_spXTtp=VEUc8gaCq5aDHqYz-SLCsa7BGCNWLAAb6w@mail.gmail.com>
References: <d8b9a2de-704c-4f6e-adb9-c0f3541ce570@admb-project.org>
	<CAJMx2XU0Egbi=q90c8X6Ccj2tmU-M7Oh7arA5-rCC+VW8t+uCw@mail.gmail.com>
	<56563821.6020603@gmail.com>
	<3843b85a-ca32-41b8-934a-80dc4175963e@admb-project.org>
	<CABghstTz_spXTtp=VEUc8gaCq5aDHqYz-SLCsa7BGCNWLAAb6w@mail.gmail.com>
Message-ID: <CANm5ooWhnAR2BJbNk0uFB3CC0WFNx8v_yaAQBfEJWe8_Gs8Thg@mail.gmail.com>

Thanks very much for your response and for pointing me in the direction of
that article. Cheers, Ellen

On Sun, Feb 7, 2016 at 4:46 PM, Ben Bolker <bbolker at gmail.com> wrote:

> (I think this is more appropriate for r-sig-mixed-models, but I'm
> leaving ADMB users cc'd for this last response.)
>
> It's not obvious to me whether there's a simple analogue of
> standardizing by response variance in the GLMM world.  I suppose you
> *could* still standardize by predictor variance, or you could decide
> that the link functions (log for NB/Poisson, logit for binomial)
> effectively standardize the prediction side of the model.  It looks
> like the last section of Schielzeth's 2010 MEE paper "Simple means
> ...", "Extensions", discusses this issue, but I haven't read it
> carefully/absorbed it/tried to implement that in a function.
>
>   cheers
>     Ben
>
>
> On Sun, Feb 7, 2016 at 2:47 PM, Ellen Robertson <robertsonep at gmail.com>
> wrote:
> > Ben,
> >    Sorry for the delayed response. In my earlier email, I was referring
> to
> > your post on
> >
> http://r-sig-mixed-models.r-project.narkive.com/1EtbqR8T/r-sig-me-standardized-coefficients-in-glmer-model
> > where you talk about using a function similar to the 'lm.beta' function
> for
> > getting standardized coefficients from lmer models ('lm.beta.lmer') .
> >      I'm trying to get standardized beta coefficients from different
> types
> > of glmer models (poisson, binomial, Gaussian) so that I can compare the
> > effect sizes from each of these (I'm using all three of these different
> > types of glmer models within a piecewise structural equation model and
> want
> > to be able to compare the strengths of different paths).  I know that
> with
> > continuous response/predictor variables I can just scale everything
> before
> > running the model and that will output standardized beta coefficients.
> But
> > I am unsure of do this with non-continuous variables (such as a binomial
> > response variable)?    You show (in the link above) how to scale binomial
> > predictor variables (change them to numeric, 0/1, rather than
> > male/female..and then scale)...but how would you do this with a binomial
> > response variable which has to be 0/1?  I tried your "lm.beta.lmer"
> function
> > and it worked when I had 2 predictors in my model but for some reason it
> > didn't work with only one predictor variable.  I also wasn't sure if it
> > would work with poisson/binomial models or if it only worked with lmer.
> >     Thanks for any help you can give.  Cheers,
> > Ellen
> >
> >
> >
> >
> > On Wednesday, November 25, 2015 at 5:37:25 PM UTC-5, Ben Bolker wrote:
> >>
> >>  I meant to respond to this earlier (maybe I did, and maybe it fell
> >> through the cracks).
> >>
> >>    Ellen, it's not clear whether you're asking about generic ADMB models
> >> or about glmmADMB models: if the latter, then
> >> r-sig-mix... at r-project.org is probably the more appropriate venue.
> >>  If the former, then I'm not even sure what you would mean by
> >> "standardized coefficients", as it would probably depend on the model.
> >>
> >>   Can you give a link/reference for "Bolker's code for beta.lmer for
> >> glmer models"?
> >>
> >>   The very generic answer to your question is that you can either (1)
> >> scale/center your continuous input variables *before* running the model
> >> or (2) adjust the coefficients afterward, based on the means and
> >> standard deviations of the parameters.  This
> >>
> >>
> >>
> http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon/23643740#23643740
> >>
> >> gives a function that rescales parameters -- it should be ecumenical
> >> (i.e., apply to any set of coefficients from a linear or generalized
> >> linear model, no matter what software it was fitted with).
> >>
> >>
> >> On 15-11-25 05:30 PM, Johnoel Ancheta wrote:
> >> > Is this possible?
> >> >
> >> > On Mon, Nov 23, 2015 at 7:31 AM, Ellen Robertson <rober... at gmail.com>
> >> > wrote:
> >> >
> >> >> Hi everyone,
> >> >>    Is it possible to get standardized coefficients from admb
> models?  I
> >> >> know about lm.beta for linear models and saw Bolkner's code for
> >> >> beta.lmer
> >> >> for glmer models....but I have been unable to get standardized
> >> >> coefficients
> >> >> from my admb models.  Thanks for your help,
> >> >> Ellen
> >> >>
> >> >> --
> >> >> You received this message because you are subscribed to the Google
> >> >> Groups
> >> >> "ADMB Users" group.
> >> >> To unsubscribe from this group and stop receiving emails from it,
> send
> >> >> an
> >> >> email to users+un... at admb-project.org.
> >> >> For more options, visit
> >> >> https://groups.google.com/a/admb-project.org/d/optout.
> >> >>
> >> >
> >>
> >
>

	[[alternative HTML version deleted]]


From daniel_rubi at ymail.com  Tue Feb  9 00:55:26 2016
From: daniel_rubi at ymail.com (Daniel Rubi)
Date: Mon, 8 Feb 2016 23:55:26 +0000 (UTC)
Subject: [R-sig-ME] A model for repeated treatments and repeated outcomes
References: <1905235601.1179568.1454975726250.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1905235601.1179568.1454975726250.JavaMail.yahoo@mail.yahoo.com>

 I have the following experimental design:Measurements of kidney function (in units called GFR) taken at several time points pre-operation (time points not evenly spaced) and at several time points post-operation (neither evenly spaced).
Here's an example of my data in R code:
set.seed(1)df <- data.frame(patient = letters[1:10],? ? ? ? ? ? ? ? gfr_ten_days_prop = rnorm(10,5,1), gfr_five_days_prop = rnorm(10,10,1), gfr_three_days_prop = rnorm(10,12,1),? ? ? ? ? ? ? ? gfr_one_day_postop = rnorm(10,10,1), gfr_one_day_postop = rnorm(10,5,1), gfr_one_day_postop = rnorm(10,2,1))

I'm looking for a model which will estimate the effect of pre-operation GFR on post-operation GFR, taking into account the different times at which GFRs were measured pre- and post-operation.One additional possible caveat - my data contain missing values (NAs).
I'm having a hard time seeing how a mixed-effects model fits this problem since in all the examples of repeated measures/longitudinal data I came across in each time point the response is measured whereas here it is more a predictive question - how strong does each pre-operation GFR predict pos-operation GFR, where the time at which GFRs were measured may matter.
Thanks a lot,Dan
	[[alternative HTML version deleted]]


From diamahendra at gmail.com  Sat Feb  6 06:04:11 2016
From: diamahendra at gmail.com (Mahendra Dia)
Date: Sat, 6 Feb 2016 00:04:11 -0500
Subject: [R-sig-ME] F and P values for random factors
In-Reply-To: <CAM9kYqhNym3NwNRLShkOw8aGuHCWt1htYZuYODAAPAkaRetb-A@mail.gmail.com>
References: <CAFCgp1riQipCgQCnLnSrjCSysFPYTi1=vfbZND0unEtegUm44A@mail.gmail.com>
	<CAJ6ui+OGnSV0z3RCXLTDAKf-bRhFZ=ArdoFiLzF-jYEbQbv4Ag@mail.gmail.com>
	<CAM9kYqhNym3NwNRLShkOw8aGuHCWt1htYZuYODAAPAkaRetb-A@mail.gmail.com>
Message-ID: <CAFCgp1rC1JBfxAG8Qg3oS3m0=6Lp3GuxcAiLOjQL23+v22XNDw@mail.gmail.com>

Hi Dan, Tom, Ben and Alex.

Thank you for your response. Earlier I was not aware that my attachments
will not go through via r-sig-mixed-models at r-project.org.

I will apply example that Alex showed in his email.
Just FYI I am copying text from the previous attachment along with original
attachment. At least few of you can see my data and question. I wish I
could have webpage where I could put my question and data.

Once again I thank you for the help and guidance.

#################################################################################################
#################################################################################################

I have a RCBD experiment with 4 factors:

   -  ?         Genotype or cultigen (CLT);
   - ?         Location (LC);
   - ?         Year (YR); and
   - ?         Blocks or Replication (RP) within location and year.

The levels of my treatments/factors are

?         Cultigen = 10;

?         Location = 5

?         Year = 2

?         Rep = 4

The response *Rijkr* of the genotype *i* in the location *j*, year *k* and
block *r *is modelled as:

*Rijkr = m + CLTi + LCj + YRk + RPr (LCj YRk) + CLTi *LCj + CLTi *YRk +
LCj*YRk + CLTi*LCj*YRk + eijkr*

*NOTE: **Dependent or response variable in this example is MKMGHA. *



I want to compute ANOVA (analysis of variable) while considering all the
factors as random factors in R.



Below is the part of R code I used while using glm function to compute
ANOVA. But I don?t know how to get the *F* ratio and *p* values.

*#ANOVA using GLM , all factors are considered as random*



*  #convert numeric predictor variable into factors*

  tempa2a <- tempa2

  tempa2a$YR <- as.factor(tempa2a$YR)

  tempa2a$RP <- as.factor(tempa2a$RP)

*  #Get structure of data *

*  #(character vs. numeric vs. matrix vs. vector vs. factor)*

  str(tempa2a)



tempa3 <- glm(MKMGHA ~ LC + YR + LC:YR + RP %in% (LC:YR) + CLT + CLT:LC +

                CLT:YR + CLT:LC:YR, family = gaussian , data= tempa2a )



anova1 <- anova(tempa3)

*#convert anova into data frame*

anova2 <- as.data.frame(anova1)

*#convert rownames into column*

anova2$sov <- rownames(anova2)

*# drop rownames*

rownames(anova2) <- NULL





After gleaning online literature it looks like I need to use lme function
nlme4 package. But I don?t know how to compute the *F* ratio and *p* values
as I never used this function/package earlier.



#################################################################################################
#################################################################################################
Sincerely,
M-

On Fri, Feb 5, 2016 at 7:16 PM, Philippi, Tom <tom_philippi at nps.gov> wrote:

> A perhaps more comprehensive answer is available in the draft
> r-sig-mixed-models FAQ:
> http://glmm.wikidot.com/faq
>
> Tom 2
>
>
> On Fri, Feb 5, 2016 at 4:05 PM, Alex Fine <abfine at gmail.com> wrote:
>
>> Hi Mahendra,
>>
>> You can assess the significance of both fixed and random factors using
>> likelihood ratio tests.  Say you want to test the significance of
>> predictor
>> A in a model with predictors A and B and random slopes for A.  You can do:
>>
>> full_model = lmer(y ~ A + B)
>> model_without_A = lmer(y ~ B)
>> anova(full_model, model_without_A)
>>
>> The anova() function in this case will return a chi-squared score (df =
>> number of predic) and a p-value.  The same procedure can be used for
>> random
>> effects, e.g.:
>>
>> full_model_2 = lmer(y ~ A + B + (1+A | random_thing)
>> model_without_random_slope_for_A = lmer(y ~ A + B + (1 | random_thing)
>> anova(full_model, model_without_random_slope_for_A)
>>
>> This works because the log-likelihoods of nested models, in the limit,
>> approximate a chi-squared distribution.
>>
>> See:  https://en.wikipedia.org/wiki/Likelihood-ratio_test
>>
>> Or maybe that wasn't what you were asking at all.
>>
>> Also I think you forgot to attach the file.
>>
>> Hope that helps!
>> Alex
>>
>>
>> On Thu, Feb 4, 2016 at 8:32 PM, Mahendra Dia <diamahendra at gmail.com>
>> wrote:
>>
>> > Hi.
>> >
>> > I am reaching you out to learn how to compute F ratio and P values for
>> my
>> > experiment where all the factors are treated as random factors.
>> > Please see the attached file where I explained my treatments and sample
>> > data.
>> >
>> > I thank you in advance.
>> >
>> > Sincerely,
>> > Mahendra-
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>>
>>
>>
>> --
>> Alex Fine
>> Ph. (336) 302-3251
>> web:  abfine.github.io/
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>

From kathleen.cote at gmail.com  Tue Feb  9 00:00:36 2016
From: kathleen.cote at gmail.com (=?UTF-8?B?S2F0aGxlZW4gQ8O0dMOp?=)
Date: Mon, 8 Feb 2016 18:00:36 -0500
Subject: [R-sig-ME] syntax for nested random factors in lme
Message-ID: <CA+G7zRh3kg7uWriakX5c1j-5EQis+Cm3f+bf934SRpxNSK9bzg@mail.gmail.com>

Hi,

I've been taught that if I want to nest random factor A into B in an lme
model, the syntax is as follows: lme(y~x+B,random=~1|B/A). This seems to be
confirmed by the information contained in the Introduction to R manual
found here: https://cran.r-project.org/doc/manuals/R-intro.pdf.

In the case of my data, matters seem to be complicated by the fact that B
is a categorical variable with only 2 levels. When I run the lme with the
above syntax, I obtain an NaN p value for B as a fixed factor in the
model. When
I rewrite the random factor as random=~1|A/B, I obtain a p value.

Is the correct format for a nested random factor indeed B/A in this case
(i.e. nesting A into B)? Is it incorrect to write it as A/B?

Additionally, some sources of information I've come across suggest that
having B as a stand-alone fixed factor as well as part of a random factor
is nonsensical. Other people have suggested that it's acceptable as long as
that same factor is part of a nested random factor and not as a stand-alone
random factor, e.g. as I have it in my model, lme(y~x+B,random=~1|B/A). Are
you able to comment on this or confirm whether my model is acceptable?

Thank you,
Kathleen

	[[alternative HTML version deleted]]


From robertsonep at gmail.com  Tue Feb  9 17:52:34 2016
From: robertsonep at gmail.com (Ellen Robertson)
Date: Tue, 9 Feb 2016 11:52:34 -0500
Subject: [R-sig-ME] [ADMB Users] getting standardized coefficients in
	admb
In-Reply-To: <56BA0DBA.20302@gmail.com>
References: <d8b9a2de-704c-4f6e-adb9-c0f3541ce570@admb-project.org>
	<CAJMx2XU0Egbi=q90c8X6Ccj2tmU-M7Oh7arA5-rCC+VW8t+uCw@mail.gmail.com>
	<56563821.6020603@gmail.com>
	<3843b85a-ca32-41b8-934a-80dc4175963e@admb-project.org>
	<CABghstTz_spXTtp=VEUc8gaCq5aDHqYz-SLCsa7BGCNWLAAb6w@mail.gmail.com>
	<CANm5ooWhnAR2BJbNk0uFB3CC0WFNx8v_yaAQBfEJWe8_Gs8Thg@mail.gmail.com>
	<56BA0DBA.20302@gmail.com>
Message-ID: <CANm5ooVgH30VkC6mbomRb-cabvtNSuV6ncz8fNUYhZiCCoBWuQ@mail.gmail.com>

Thanks, yes  I'm not really sure what the best approach is. I'm leaning
towards scaling only my predictor variables for binomial/poisson glmer
models (and possibly scale by 1/sd of the response as you mentioned) .  For
my lmer models, I could do the same or I could scale both predictor and
response variables.  My lmer coefficients are extremely different depending
on which I do: if I scale the lmer response variable I get a beta=-0.25 and
if I don't scale the response I get beta=-12.54.
Best,
Ellen

On Tue, Feb 9, 2016 at 11:03 AM, Ben Bolker <bbolker at gmail.com> wrote:

>
>   Actually, now that I look at the article more carefully it turns out
> that those paragraphs are mostly focused on *mixed* models, and don't say
> too much about how the argument generalizes (so to speak) to the
> generalized-linear (mixed) model case.  Scaling the *estimated parameter*
> by 1/standard deviation of the response is not insane (you can't scale the
> response variable *before* you fit the model in a GLM, that doesn't make
> sense), but doesn't have the same nice interpretation as in a linear
> model.  In general the link functions do put the parameters on a simple,
> dimensionless scale, but I'm not sure about a sensible, general way to
> compare among parameters of models fitted with *different* link functions.
>
> On 16-02-09 09:49 AM, Ellen Robertson wrote:
>
>> Thanks very much for your response and for pointing me in the direction
>> of that article. Cheers, Ellen
>>
>> On Sun, Feb 7, 2016 at 4:46 PM, Ben Bolker <bbolker at gmail.com
>> <mailto:bbolker at gmail.com>> wrote:
>>
>>     (I think this is more appropriate for r-sig-mixed-models, but I'm
>>     leaving ADMB users cc'd for this last response.)
>>
>>     It's not obvious to me whether there's a simple analogue of
>>     standardizing by response variance in the GLMM world.  I suppose you
>>     *could* still standardize by predictor variance, or you could decide
>>     that the link functions (log for NB/Poisson, logit for binomial)
>>     effectively standardize the prediction side of the model.  It looks
>>     like the last section of Schielzeth's 2010 MEE paper "Simple means
>>     ...", "Extensions", discusses this issue, but I haven't read it
>>     carefully/absorbed it/tried to implement that in a function.
>>
>>        cheers
>>          Ben
>>
>>
>>     On Sun, Feb 7, 2016 at 2:47 PM, Ellen Robertson
>>     <robertsonep at gmail.com <mailto:robertsonep at gmail.com>> wrote:
>>      > Ben,
>>      >    Sorry for the delayed response. In my earlier email, I was
>>     referring to
>>      > your post on
>>      >
>>
>> http://r-sig-mixed-models.r-project.narkive.com/1EtbqR8T/r-sig-me-standardized-coefficients-in-glmer-model
>>      > where you talk about using a function similar to the 'lm.beta'
>>     function for
>>      > getting standardized coefficients from lmer models
>> ('lm.beta.lmer') .
>>      >      I'm trying to get standardized beta coefficients from
>>     different types
>>      > of glmer models (poisson, binomial, Gaussian) so that I can
>>     compare the
>>      > effect sizes from each of these (I'm using all three of these
>>     different
>>      > types of glmer models within a piecewise structural equation
>>     model and want
>>      > to be able to compare the strengths of different paths).  I know
>>     that with
>>      > continuous response/predictor variables I can just scale
>>     everything before
>>      > running the model and that will output standardized beta
>>     coefficients.  But
>>      > I am unsure of do this with non-continuous variables (such as a
>>     binomial
>>      > response variable)?    You show (in the link above) how to scale
>>     binomial
>>      > predictor variables (change them to numeric, 0/1, rather than
>>      > male/female..and then scale)...but how would you do this with a
>>     binomial
>>      > response variable which has to be 0/1?  I tried your
>>     "lm.beta.lmer" function
>>      > and it worked when I had 2 predictors in my model but for some
>>     reason it
>>      > didn't work with only one predictor variable.  I also wasn't sure
>>     if it
>>      > would work with poisson/binomial models or if it only worked with
>>     lmer.
>>      >     Thanks for any help you can give.  Cheers,
>>      > Ellen
>>      >
>>      >
>>      >
>>      >
>>      > On Wednesday, November 25, 2015 at 5:37:25 PM UTC-5, Ben Bolker
>>     wrote:
>>      >>
>>      >>  I meant to respond to this earlier (maybe I did, and maybe it
>> fell
>>      >> through the cracks).
>>      >>
>>      >>    Ellen, it's not clear whether you're asking about generic
>>     ADMB models
>>      >> or about glmmADMB models: if the latter, then
>>      >> r-sig-mix... at r-project.org <mailto:r-sig-mix... at r-project.org>
>>     is probably the more appropriate venue.
>>      >>  If the former, then I'm not even sure what you would mean by
>>      >> "standardized coefficients", as it would probably depend on the
>>     model.
>>      >>
>>      >>   Can you give a link/reference for "Bolker's code for beta.lmer
>> for
>>      >> glmer models"?
>>      >>
>>      >>   The very generic answer to your question is that you can
>>     either (1)
>>      >> scale/center your continuous input variables *before* running
>>     the model
>>      >> or (2) adjust the coefficients afterward, based on the means and
>>      >> standard deviations of the parameters.  This
>>      >>
>>      >>
>>      >>
>>
>> http://stackoverflow.com/questions/23642111/how-to-unscale-the-coefficients-from-an-lmer-model-fitted-with-a-scaled-respon/23643740#23643740
>>      >>
>>      >> gives a function that rescales parameters -- it should be
>> ecumenical
>>      >> (i.e., apply to any set of coefficients from a linear or
>> generalized
>>      >> linear model, no matter what software it was fitted with).
>>      >>
>>      >>
>>      >> On 15-11-25 05:30 PM, Johnoel Ancheta wrote:
>>      >> > Is this possible?
>>      >> >
>>      >> > On Mon, Nov 23, 2015 at 7:31 AM, Ellen Robertson
>>     <rober... at gmail.com <mailto:rober... at gmail.com>>
>>      >> > wrote:
>>      >> >
>>      >> >> Hi everyone,
>>      >> >>    Is it possible to get standardized coefficients from admb
>>     models?  I
>>      >> >> know about lm.beta for linear models and saw Bolkner's code for
>>      >> >> beta.lmer
>>      >> >> for glmer models....but I have been unable to get standardized
>>      >> >> coefficients
>>      >> >> from my admb models.  Thanks for your help,
>>      >> >> Ellen
>>      >> >>
>>      >> >> --
>>      >> >> You received this message because you are subscribed to the
>>     Google
>>      >> >> Groups
>>      >> >> "ADMB Users" group.
>>      >> >> To unsubscribe from this group and stop receiving emails from
>>     it, send
>>      >> >> an
>>      >> >> email to users+un... at admb-project.org
>>     <mailto:users%2Bun... at admb-project.org>.
>>      >> >> For more options, visit
>>      >> >> https://groups.google.com/a/admb-project.org/d/optout.
>>      >> >>
>>      >> >
>>      >>
>>      >
>>
>>
>>

	[[alternative HTML version deleted]]


From jake987722 at hotmail.com  Tue Feb  9 19:13:21 2016
From: jake987722 at hotmail.com (Jake Westfall)
Date: Tue, 9 Feb 2016 18:13:21 +0000
Subject: [R-sig-ME] syntax for nested random factors in lme
In-Reply-To: <CA+G7zRh3kg7uWriakX5c1j-5EQis+Cm3f+bf934SRpxNSK9bzg@mail.gmail.com>
References: <CA+G7zRh3kg7uWriakX5c1j-5EQis+Cm3f+bf934SRpxNSK9bzg@mail.gmail.com>
Message-ID: <DM3PR19MB05222C5737E83A1009027C48CBD60@DM3PR19MB0522.namprd19.prod.outlook.com>

Kathleen,

In R formulae, the "B/A" syntax gets expanded to "B + A:B" -- in other words, effects are estimated for the Bs and for the As-within-Bs. But since you already have B in the fixed part of the model, this means your model is trying to estimate both fixed and random effects of B, which is certainly the problem. Try replacing "B/A" with "A:B" and see if that works better.

Jake

________________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Kathleen C?t? <kathleen.cote at gmail.com>
Sent: Monday, February 8, 2016 5:00 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] syntax for nested random factors in lme

Hi,

I've been taught that if I want to nest random factor A into B in an lme
model, the syntax is as follows: lme(y~x+B,random=~1|B/A). This seems to be
confirmed by the information contained in the Introduction to R manual
found here: https://cran.r-project.org/doc/manuals/R-intro.pdf.

In the case of my data, matters seem to be complicated by the fact that B
is a categorical variable with only 2 levels. When I run the lme with the
above syntax, I obtain an NaN p value for B as a fixed factor in the
model. When
I rewrite the random factor as random=~1|A/B, I obtain a p value.

Is the correct format for a nested random factor indeed B/A in this case
(i.e. nesting A into B)? Is it incorrect to write it as A/B?

Additionally, some sources of information I've come across suggest that
having B as a stand-alone fixed factor as well as part of a random factor
is nonsensical. Other people have suggested that it's acceptable as long as
that same factor is part of a nested random factor and not as a stand-alone
random factor, e.g. as I have it in my model, lme(y~x+B,random=~1|B/A). Are
you able to comment on this or confirm whether my model is acceptable?

Thank you,
Kathleen

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From thierry.onkelinx at inbo.be  Wed Feb 10 09:54:55 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 10 Feb 2016 09:54:55 +0100
Subject: [R-sig-ME] A model for repeated treatments and repeated outcomes
In-Reply-To: <1905235601.1179568.1454975726250.JavaMail.yahoo@mail.yahoo.com>
References: <1905235601.1179568.1454975726250.JavaMail.yahoo.ref@mail.yahoo.com>
	<1905235601.1179568.1454975726250.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAJuCY5yF9wBQ_PR27F7is4KJ2t6iKnDTgzZ9CpT7cZ_Xwc4pgQ@mail.gmail.com>

Dear Dan,

You'll need to provide more information. What is the global pattern that
you expect (linear? quadratic? non-linear?) How to you thing that the
operation can effect the GFR? You need to answer those kind of questions so
that you can make a sensible fixed effects part of the model.

The random effect is probably just ~1|Patient. And a corCAR1(form = ~ Time)
can handle the temporal correlation within the patient.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-02-09 0:55 GMT+01:00 Daniel Rubi via R-sig-mixed-models <
r-sig-mixed-models at r-project.org>:

>  I have the following experimental design:Measurements of kidney function
> (in units called GFR) taken at several time points pre-operation (time
> points not evenly spaced) and at several time points post-operation
> (neither evenly spaced).
> Here's an example of my data in R code:
> set.seed(1)df <- data.frame(patient = letters[1:10],
> gfr_ten_days_prop = rnorm(10,5,1), gfr_five_days_prop = rnorm(10,10,1),
> gfr_three_days_prop = rnorm(10,12,1),                gfr_one_day_postop =
> rnorm(10,10,1), gfr_one_day_postop = rnorm(10,5,1), gfr_one_day_postop =
> rnorm(10,2,1))
>
> I'm looking for a model which will estimate the effect of pre-operation
> GFR on post-operation GFR, taking into account the different times at which
> GFRs were measured pre- and post-operation.One additional possible caveat -
> my data contain missing values (NAs).
> I'm having a hard time seeing how a mixed-effects model fits this problem
> since in all the examples of repeated measures/longitudinal data I came
> across in each time point the response is measured whereas here it is more
> a predictive question - how strong does each pre-operation GFR predict
> pos-operation GFR, where the time at which GFRs were measured may matter.
> Thanks a lot,Dan
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From surajkeshri at gmail.com  Thu Feb 11 02:39:08 2016
From: surajkeshri at gmail.com (suraj keshri)
Date: Wed, 10 Feb 2016 20:39:08 -0500
Subject: [R-sig-ME] Low rank fixed effect design matrix LMER package
Message-ID: <CABkv+1ct0QY4QwoGM1qNp4yEYHnApvQZeHHN4F5dXvgu6Nk2hA@mail.gmail.com>

Hi,

I'm trying to implement some pieces of LMER algorithm in Python. I'm facing
low rank fixed effect design matrix issue. One way to take care of low rank
is to detect linearly dependent columns using ideas from SVD/QR
decomposition and remove it. My design matrix is very big and sparse.
Therefore, instead of doing SVD of the design matrix (X), I do SVD of X^T *
X. However, one has to decide a threshold for the singular values. When I
look at the singular values of X^T*X, the range is very large (1e+12 to
1e-4). In this situation, how does one decide the threshold so that X^T*X
is invertible (after removing linearly dependent columns)? How does LMER
package solve this problem?

Thank you so much for any help!!


Suraj

	[[alternative HTML version deleted]]


From lorenzo.vignali88 at gmail.com  Thu Feb 11 11:00:57 2016
From: lorenzo.vignali88 at gmail.com (Lorenzo Vignali)
Date: Thu, 11 Feb 2016 11:00:57 +0100
Subject: [R-sig-ME] Model sintax unbalanced design
Message-ID: <CAE9hYqri0=kU3HqK3TnQBqr9j6jfvRZPcOLC=7kM8vqdpwgSoA@mail.gmail.com>

Dear all,

I am trying to use the R package lme4 to analyse behavioral results from a
brain stimulation experiment.
I would be thankful to receive opinions and comments on how would you
structure the model.

We tested two groups of 20 participants each (between factor), one with
brain stimulation over the left hemisphere and one with brain stimulation
over the right hemisphere - factor (Hemi)

Each participant (within each group) was tested twice, once with sham
stimulation, and once with cathodal stimulation - factor (tDCS)

Participants were asked to perform a task by pressing a button (we have
therefore both accuracy rates and reaction times) in 4 different conditions
(100 trials per condition) - factor (Conditions)

Two of this conditions, had an additional manipulation, they had first or
last constraining trigrams.
- factor (Trigrams). Is an unbalanced design and the conditions which have
no Trigrams manipulation are were defined as NA in the R data.frame

So far I tried different models, but one of the best ones is this one (ACC
stands for accuracy, measure a number of correct responses):

basic_ACC <- lmer(Correct_Resp ~ Hemi + tDCS * Conditions * Trigrams + (1 +
Hemi + tdcs + Conditions + Trigrams | sbj), data = tot.all)

as a result I get:

REML criterion at convergence: 13567.9

Scaled residuals:
     Min       1Q   Median       3Q      Max
-2.90479 -0.95169  0.07642  0.46143  1.96212

Random effects:
 Groups   Name        Variance  Std.Dev. Corr
 sbj      (Intercept) 2.321e-03 0.048177
          Hemiright   5.193e-03 0.072063 -0.63
          tdcsstim    6.602e-03 0.081254 -0.70 -0.04
          Conditions  6.976e-04 0.026412 -0.25 -0.18  0.15
          Trigrams    4.323e-05 0.006575 -0.07  0.79 -0.64 -0.17
 Residual             1.373e-01 0.370542
Number of obs: 15682, groups:  sbj, 20

Fixed effects:
                              Estimate Std. Error t value
(Intercept)                   1.158451   0.031748   36.49
Hemiright                     0.003500   0.017166    0.20
tdcsstim                      0.011775   0.045735    0.26
Conditions                   -0.180447   0.014495  -12.45
Trigrams                      0.024563   0.018864    1.30
tdcsstim:Conditions          -0.014195   0.018700   -0.76
tdcsstim:Trigrams            -0.001830   0.026569   -0.07
Conditions:Trigrams          -0.035831   0.008378   -4.28
tdcsstim:Conditions:Trigrams  0.005560   0.011837    0.47

Correlation of Fixed Effects:
            (Intr) Hmrght tdcsst Cndtns Trgrms tdcs:C tdcs:T Cndt:T
Hemiright   -0.234
tdcsstim    -0.703 -0.015
Conditions  -0.799 -0.068  0.556
Trigrams    -0.887  0.058  0.595  0.768
tdcsstm:Cnd  0.593  0.000 -0.822 -0.646 -0.599
tdcsstm:Trg  0.629  0.000 -0.871 -0.549 -0.706  0.849
Cndtns:Trgr  0.795  0.000 -0.552 -0.866 -0.893  0.671  0.634
tdcsstm:C:T -0.563  0.000  0.779  0.613  0.632 -0.949 -0.895 -0.708

it looks fine to me, as we get the expected main effects and the expected
interactions. But I just want to be sure that nothing is too weird, as the
design is quite a tricky one and I am rather inexperienced with lmer.
Thank you all in advance.


Kind regards,

Lorenzo

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Thu Feb 11 17:07:26 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 11 Feb 2016 11:07:26 -0500
Subject: [R-sig-ME] Low rank fixed effect design matrix LMER package
In-Reply-To: <CABkv+1ct0QY4QwoGM1qNp4yEYHnApvQZeHHN4F5dXvgu6Nk2hA@mail.gmail.com>
References: <CABkv+1ct0QY4QwoGM1qNp4yEYHnApvQZeHHN4F5dXvgu6Nk2hA@mail.gmail.com>
Message-ID: <56BCB1BE.6070400@gmail.com>


  The place to start looking is line 222 of 
https://github.com/lme4/lme4/blob/master/R/modular.R . We call the 
Matrix::rankMatrix() and stats::qr() functions with a default tolerance 
of 1e-7.  The comment in the code at that point specifies

## Perform the qr-decomposition of X using LINPACK method,
## as we need the "good" pivots (and the same as lm()):
## FIXME: strongly prefer rankMatrix(X, method= "qr.R")

   Hope that helps.

   Ben Bolker


On 16-02-10 08:39 PM, suraj keshri wrote:
> Hi,
>
> I'm trying to implement some pieces of LMER algorithm in Python. I'm facing
> low rank fixed effect design matrix issue. One way to take care of low rank
> is to detect linearly dependent columns using ideas from SVD/QR
> decomposition and remove it. My design matrix is very big and sparse.
> Therefore, instead of doing SVD of the design matrix (X), I do SVD of X^T *
> X. However, one has to decide a threshold for the singular values. When I
> look at the singular values of X^T*X, the range is very large (1e+12 to
> 1e-4). In this situation, how does one decide the threshold so that X^T*X
> is invertible (after removing linearly dependent columns)? How does LMER
> package solve this problem?
>
> Thank you so much for any help!!
>
>
> Suraj
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From gitumbui at gmail.com  Fri Feb 12 11:44:59 2016
From: gitumbui at gmail.com (Gitu wa Mbui)
Date: Fri, 12 Feb 2016 20:44:59 +1000
Subject: [R-sig-ME] Reporting coefficients from model averaged GAM (M)
 models with smoothed terms
Message-ID: <CAFPRYfDJC0DF03i8j3yghG7v-14ZxzLPcZvjYYXwUBaZeM=y1g@mail.gmail.com>

Having run several GAM (M) models with smoothed terms in model selection, I
am wondering how best to report the coefficients (standardized) associated
with each predictor - where each predictor is repeated across >1 models. I
have used model averaging (MuMIn package) to average the models but the
coefficients are returned for each knot rather than overall for each
predictor (for example, when k=5, it returns coefficient for k1 to k4). Is
it Ok to report averaged coefficients regardless of the knot (k) level, and
how would you go about obtaining these?

~Gitu

	[[alternative HTML version deleted]]


From mdevoto at agro.uba.ar  Fri Feb 12 12:59:54 2016
From: mdevoto at agro.uba.ar (Mariano Devoto)
Date: Fri, 12 Feb 2016 08:59:54 -0300
Subject: [R-sig-ME] Plotting effects in original units
Message-ID: <CAJRWjBYkRMKvdOzpkY1m7uJfJnPE=2Rvf3SuWvrUiyO7jBaWqw@mail.gmail.com>

Hi everyone. I am using 'glmer' function in lme4 package to run a glmm with
rescaled explanatory variables (i.e. I used 'scale' function to centre and
standardise each variable). I am now trying to use the output of the
'allEffects' function in the 'effects' package to calculate and plot the
effects. For each explanatory variable I want to extract the fitted values
(and the confidence interval) and plot them but labeling the axes in the
original units. I don't want to use the default plot function because I am
trying to make custom-made figures. Is there a way around having to use the
inverse of the link function and then backtransforming using the inverse of
the scaling to get the values I am looking for? I'd appreciate any tips.

Best,

Mariano

___
Dr. Mariano Devoto
Investigador Adjunto del CONICET
C?tedra de Bot?nica General
Facultad de Agronom?a de la U.B.A.
Av. San Martin 4453
C1417DSE
C.A. de Buenos Aires
Argentina

http://www.agro.uba.ar/users/mdevoto/

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Fri Feb 12 14:36:53 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Fri, 12 Feb 2016 13:36:53 +0000
Subject: [R-sig-ME] Plotting effects in original units
In-Reply-To: <CAJRWjBYkRMKvdOzpkY1m7uJfJnPE=2Rvf3SuWvrUiyO7jBaWqw@mail.gmail.com>
References: <CAJRWjBYkRMKvdOzpkY1m7uJfJnPE=2Rvf3SuWvrUiyO7jBaWqw@mail.gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F56F0D@FHSDB2D11-2.csu.mcmaster.ca>

Dear Mariano,

The objects returned by the Effect() function, which is called repeatedly by allEffects(), contains all the information you need to create a custom plot. The fitted values for each effect on the scale of the linear predictor (i.e., not transformed to the scale of the response) -- that is, e.g., on the logit scale rather than the probability scale for a logit model --  are in the 'fit' component of the object. As I understand it, this is what you want to plot. The object also contains the combinations of values of the predictors at which the fit was evaluated in the 'x' component. Similarly, the confidence limits are in 'lower' and 'upper', also on the scale of the linear predictor. See ?Effect for more information about the returned object.

As to plotting on the original scales of the predictors rather than the standardized scales, you should be able easily to back-transform each predictor. But why bother? Why not just fit the model with the unstandardized predictors?

Finally, although the print() method for effect objects by default transforms effects to the scale of the response, and the plot() method plots on the scale of the linear predictor but labels the response axis on the scale of the response, you can modify this default behaviour. See ?print.eff and ?plot.eff (and in particular the type argument).

I hope this helps,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
web: socserv.mcmaster.ca/jfox


________________________________________
From: R-sig-mixed-models [r-sig-mixed-models-bounces at r-project.org] on behalf of Mariano Devoto [mdevoto at agro.uba.ar]
Sent: February 12, 2016 6:59 AM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Plotting effects in original units

Hi everyone. I am using 'glmer' function in lme4 package to run a glmm with
rescaled explanatory variables (i.e. I used 'scale' function to centre and
standardise each variable). I am now trying to use the output of the
'allEffects' function in the 'effects' package to calculate and plot the
effects. For each explanatory variable I want to extract the fitted values
(and the confidence interval) and plot them but labeling the axes in the
original units. I don't want to use the default plot function because I am
trying to make custom-made figures. Is there a way around having to use the
inverse of the link function and then backtransforming using the inverse of
the scaling to get the values I am looking for? I'd appreciate any tips.

Best,

Mariano

___
Dr. Mariano Devoto
Investigador Adjunto del CONICET
C?tedra de Bot?nica General
Facultad de Agronom?a de la U.B.A.
Av. San Martin 4453
C1417DSE
C.A. de Buenos Aires
Argentina

http://www.agro.uba.ar/users/mdevoto/

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From chirleu at gmail.com  Fri Feb 12 15:23:05 2016
From: chirleu at gmail.com (=?UTF-8?Q?David_Villegas_R=C3=ADos?=)
Date: Fri, 12 Feb 2016 15:23:05 +0100
Subject: [R-sig-ME] long format and residual covariance matrix in MCMCglmm
Message-ID: <CALC46t9mnZi2Yy2K0tTYFgb+pWtjXAY1ayQOk32+EChh255wBw@mail.gmail.com>

Hello,
I'm trying to organize a dataset to run a multivariate mixed model in
MCMCglmm. I have several response traits and since I have unequal number of
replicates per individual for each trait, I have been suggested to arrange
the data in a long format.

I have 5 response traits. Three of them were recorded as follows:

- Day 1: trait A
- Day 2: trait A and trait B
- Day 3: trait A and trait C
- Day 4: trait A and trait B
- Day 5: trait A and trait C

During the following 6-7 days I didn' t measure anything. And then after
that, the other two traits (trait D and trait E) were recorded weekly
during 54 weeks.

I want to run a multivariate model in MCMCglmm to estimate the random
var-cov matrix and the residual var-cov matrix. The later however must have
some constrains as for example, trait B and C do not covary at the residual
(within-individual level) and traits A, B and C do not covary at the
residual  level with traits D and E. So the residual var-cov matrix should
be something like:

   A B C D E
A 1
B x 1
C x 0 1
D 0 0 0 1
E 0 0 0 x 1

Where x represents the covariances that should be estimated, and 0 the
constrained covariances.

Questions:


- Is it possible to fit such a residual var-cov matrix  in MCMCglmm? Maybe
with an antedependence model?
- How can I structure the dataset for the analysis? Do I need a "time"
column? I could use "day" for the first three traits, but then what about
traits D and E?
- Does it make sense at all to run this model, or would it be more
meaningful to get a mean value of traits A, B and C and used them as fixed
effects, since I have a very low number of replicates of them? In that case
I'd of course investigate the main effects of A, C and C on E and D, and
not the covariation amontg them...but it could be ok as well.

Thanks!

	[[alternative HTML version deleted]]


From daniel_rubi at ymail.com  Fri Feb 12 02:01:44 2016
From: daniel_rubi at ymail.com (Daniel Rubi)
Date: Fri, 12 Feb 2016 01:01:44 +0000 (UTC)
Subject: [R-sig-ME] A model for repeated treatments and repeated outcomes
References: <496230188.2872226.1455238904712.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <496230188.2872226.1455238904712.JavaMail.yahoo@mail.yahoo.com>

Dear Thierry,

Thanks a lot for the help.

Please allow me to explain better.

What I'm trying to estimate is whether the trend in pre-operation GFR affects the levels of post-operation GFR.
The GFR is a measurement that assesses kidney function based on a blood sample and for that reason it reflects the functions of both kidneys.
The operation is either partial or total removal of one kidney - that is affected. Therefore it is supposed to prevent GFR from escalating.

What I'm interesting in estimating is whether the trend of pre-operation GFR (along the time pre-operation time points) is a good predictor of the post-operatoin GFR. It's not clear at which post-operation time point it is correct to measure GFR and therefore I have 3 time points.

The trivial thing to do is to estimate the slope of pre-operation GFR and individually regress each of the post-operation GFRs against thos.

But I'm wondering whether there's anything better?

Thanks a lot,
Dan


--------------------------------------------
On Wed, 2/10/16, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:

 Subject: Re: [R-sig-ME] A model for repeated treatments and repeated outcomes
 To: "Daniel Rubi" <daniel_rubi at ymail.com>
 Cc: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
 Date: Wednesday, February 10, 2016, 3:54 AM

 Dear
 Dan,
 You'll need to
 provide more information. What is the global pattern that
 you expect (linear? quadratic? non-linear?) How to you thing
 that the operation can effect the GFR? You need to answer
 those kind of questions so that you can make a sensible
 fixed effects part of the model.
 The random effect is probably just
 ~1|Patient. And a corCAR1(form = ~ Time) can handle the
 temporal correlation within the patient.
 Best regards,
 ir. Thierry Onkelinx
 Instituut voor natuur- en bosonderzoek /
 Research Institute for Nature and Forest 
 team Biometrie & Kwaliteitszorg / team
 Biometrics & Quality Assurance 
 Kliniekstraat 25
 1070
 Anderlecht
 Belgium

 To call in the statistician after the
 experiment is done may be no more than asking him to perform
 a post-mortem examination: he may be able to say what the
 experiment died of. ~ Sir Ronald Aylmer Fisher
 The plural of anecdote is not data. ~ Roger
 Brinner 
 The combination of some data and an
 aching desire for an answer does not ensure that a
 reasonable answer can be extracted from a given body of
 data. ~ John Tukey

 2016-02-09 0:55 GMT+01:00
 Daniel Rubi via R-sig-mixed-models <r-sig-mixed-models at r-project.org>:
 ?I have the following experimental
 design:Measurements of kidney function (in units called GFR)
 taken at several time points pre-operation (time points not
 evenly spaced) and at several time points post-operation
 (neither evenly spaced).

 Here's an example of my data in R code:

 set.seed(1)df <- data.frame(patient = letters[1:10],? ?
 ? ? ? ? ? ? gfr_ten_days_prop = rnorm(10,5,1),
 gfr_five_days_prop = rnorm(10,10,1), gfr_three_days_prop =
 rnorm(10,12,1),? ? ? ? ? ? ? ? gfr_one_day_postop =
 rnorm(10,10,1), gfr_one_day_postop = rnorm(10,5,1),
 gfr_one_day_postop = rnorm(10,2,1))



 I'm looking for a model which will estimate the effect
 of pre-operation GFR on post-operation GFR, taking into
 account the different times at which GFRs were measured pre-
 and post-operation.One additional possible caveat - my data
 contain missing values (NAs).

 I'm having a hard time seeing how a mixed-effects model
 fits this problem since in all the examples of repeated
 measures/longitudinal data I came across in each time point
 the response is measured whereas here it is more a
 predictive question - how strong does each pre-operation GFR
 predict pos-operation GFR, where the time at which GFRs were
 measured may matter.

 Thanks a lot,Dan

 ? ? ? ? [[alternative HTML version deleted]]



 _______________________________________________

 R-sig-mixed-models at r-project.org
 mailing list

 https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From dakotajudo at mac.com  Fri Feb 12 18:43:42 2016
From: dakotajudo at mac.com (Peter Claussen)
Date: Fri, 12 Feb 2016 11:43:42 -0600
Subject: [R-sig-ME] A model for repeated treatments and repeated outcomes
In-Reply-To: <496230188.2872226.1455238904712.JavaMail.yahoo@mail.yahoo.com>
References: <496230188.2872226.1455238904712.JavaMail.yahoo.ref@mail.yahoo.com>
	<496230188.2872226.1455238904712.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <567A4DC0-34DD-4981-92C9-68D2A812A657@mac.com>

Dan,

If I may jump in.

If I read this correctly, you have two random samples per patient - a random sample of pre-operation GRF and a random sample post-operation. I think these should be considered random since you state the points are not evenly spaced in either case.

So you add a factor for sampling period (SamplePeriod = c(?Pre?,?Post?)) and model random effect as (SamplePeriod | Patient).

That?s my first thought - I?m not sure given three points per sample period you can reasonably measure trends.

Peter

> On Feb 11, 2016, at 7:01 PM, Daniel Rubi via R-sig-mixed-models <r-sig-mixed-models at r-project.org> wrote:
> 
> Dear Thierry,
> 
> Thanks a lot for the help.
> 
> Please allow me to explain better.
> 
> What I'm trying to estimate is whether the trend in pre-operation GFR affects the levels of post-operation GFR.
> The GFR is a measurement that assesses kidney function based on a blood sample and for that reason it reflects the functions of both kidneys.
> The operation is either partial or total removal of one kidney - that is affected. Therefore it is supposed to prevent GFR from escalating.
> 
> What I'm interesting in estimating is whether the trend of pre-operation GFR (along the time pre-operation time points) is a good predictor of the post-operatoin GFR. It's not clear at which post-operation time point it is correct to measure GFR and therefore I have 3 time points.
> 
> The trivial thing to do is to estimate the slope of pre-operation GFR and individually regress each of the post-operation GFRs against thos.
> 
> But I'm wondering whether there's anything better?
> 
> Thanks a lot,
> Dan
> 
> 
> --------------------------------------------
> On Wed, 2/10/16, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
> 
> Subject: Re: [R-sig-ME] A model for repeated treatments and repeated outcomes
> To: "Daniel Rubi" <daniel_rubi at ymail.com>
> Cc: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
> Date: Wednesday, February 10, 2016, 3:54 AM
> 
> Dear
> Dan,
> You'll need to
> provide more information. What is the global pattern that
> you expect (linear? quadratic? non-linear?) How to you thing
> that the operation can effect the GFR? You need to answer
> those kind of questions so that you can make a sensible
> fixed effects part of the model.
> The random effect is probably just
> ~1|Patient. And a corCAR1(form = ~ Time) can handle the
> temporal correlation within the patient.
> Best regards,
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek /
> Research Institute for Nature and Forest 
> team Biometrie & Kwaliteitszorg / team
> Biometrics & Quality Assurance 
> Kliniekstraat 25
> 1070
> Anderlecht
> Belgium
> 
> To call in the statistician after the
> experiment is done may be no more than asking him to perform
> a post-mortem examination: he may be able to say what the
> experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger
> Brinner 
> The combination of some data and an
> aching desire for an answer does not ensure that a
> reasonable answer can be extracted from a given body of
> data. ~ John Tukey
> 
> 2016-02-09 0:55 GMT+01:00
> Daniel Rubi via R-sig-mixed-models <r-sig-mixed-models at r-project.org>:
>  I have the following experimental
> design:Measurements of kidney function (in units called GFR)
> taken at several time points pre-operation (time points not
> evenly spaced) and at several time points post-operation
> (neither evenly spaced).
> 
> Here's an example of my data in R code:
> 
> set.seed(1)df <- data.frame(patient = letters[1:10],   
>             gfr_ten_days_prop = rnorm(10,5,1),
> gfr_five_days_prop = rnorm(10,10,1), gfr_three_days_prop =
> rnorm(10,12,1),                gfr_one_day_postop =
> rnorm(10,10,1), gfr_one_day_postop = rnorm(10,5,1),
> gfr_one_day_postop = rnorm(10,2,1))
> 
> 
> 
> I'm looking for a model which will estimate the effect
> of pre-operation GFR on post-operation GFR, taking into
> account the different times at which GFRs were measured pre-
> and post-operation.One additional possible caveat - my data
> contain missing values (NAs).
> 
> I'm having a hard time seeing how a mixed-effects model
> fits this problem since in all the examples of repeated
> measures/longitudinal data I came across in each time point
> the response is measured whereas here it is more a
> predictive question - how strong does each pre-operation GFR
> predict pos-operation GFR, where the time at which GFRs were
> measured may matter.
> 
> Thanks a lot,Dan
> 
>         [[alternative HTML version deleted]]
> 
> 
> 
> _______________________________________________
> 
> R-sig-mixed-models at r-project.org
> mailing list
> 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From power.julian.chen at gmail.com  Fri Feb 12 19:58:58 2016
From: power.julian.chen at gmail.com (Julian Chen)
Date: Fri, 12 Feb 2016 13:58:58 -0500
Subject: [R-sig-ME] Random effects in GAMs
Message-ID: <CABch2GsLAem7=4j142c8jT+ScCrtN8JELqv89va4s3SDEaHytg@mail.gmail.com>

I am now trying to use random effects in GAMs developed by Professor Simon
Wood. Prof Wood uses  s(...,bs="re") to account for the random effects.
Random intercepts models or random slopes models are two different types of
mixed linear models or general random effects model (Cameron and Trivedi,
2005). I wonder if the Wood's method includes both random intercepts and
random slopes. Based on my understanding, this method does? Anyone can help
me clarify this method? The following is the link of Random effects in GAM
developed by Prof Wood.

https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/random.effects.html

-- 
Life is nothing but a dream,
     and if we are artists,
then we can create our life with Love,
     and our dream becomes
       a masterpiece of art.

	[[alternative HTML version deleted]]


From tom_philippi at nps.gov  Fri Feb 12 20:40:21 2016
From: tom_philippi at nps.gov (Philippi, Tom)
Date: Fri, 12 Feb 2016 11:40:21 -0800
Subject: [R-sig-ME] Random effects in GAMs
In-Reply-To: <CABch2GsLAem7=4j142c8jT+ScCrtN8JELqv89va4s3SDEaHytg@mail.gmail.com>
References: <CABch2GsLAem7=4j142c8jT+ScCrtN8JELqv89va4s3SDEaHytg@mail.gmail.com>
Message-ID: <CAM9kYqirLedpiuGPHe5asL+VOaPa7=BPn8Ma=xYSraQ2vPX+6g@mail.gmail.com>

Julian--
Without further information on your data generating process and what
structure you are trying to capture with your model, we don't know what
your random effects are accounting for. For instance, my frequent use case
is GAM for smoothing across time, and sites as random effects, which
requires mgcv::gamm or gamm4:gamm4 (the first method in your linked page)
instead of gam() with s(...,bs='re').   I don't think that anyone can give
you helpful answers without that further information.

I highly recommend Simon Wood's book "Generalized Additive Models".  It
starts with clear explanations of linear models and generalized linear
models, builds through gams, and ends with a chapter on mixed models and
gamms.  I find it to be a valuable reference for LM, GLM, GAM, and GAMM,
all in a single book.

Tom 2



On Fri, Feb 12, 2016 at 10:58 AM, Julian Chen <power.julian.chen at gmail.com>
wrote:

> I am now trying to use random effects in GAMs developed by Professor Simon
> Wood. Prof Wood uses  s(...,bs="re") to account for the random effects.
> Random intercepts models or random slopes models are two different types of
> mixed linear models or general random effects model (Cameron and Trivedi,
> 2005). I wonder if the Wood's method includes both random intercepts and
> random slopes. Based on my understanding, this method does? Anyone can help
> me clarify this method? The following is the link of Random effects in GAM
> developed by Prof Wood.
>
> https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/random.effects.html
>
> --
> Life is nothing but a dream,
>      and if we are artists,
> then we can create our life with Love,
>      and our dream becomes
>        a masterpiece of art.
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From power.julian.chen at gmail.com  Fri Feb 12 20:51:19 2016
From: power.julian.chen at gmail.com (Julian Chen)
Date: Fri, 12 Feb 2016 14:51:19 -0500
Subject: [R-sig-ME] Random effects in GAMs
In-Reply-To: <CABch2GsLAem7=4j142c8jT+ScCrtN8JELqv89va4s3SDEaHytg@mail.gmail.com>
References: <CABch2GsLAem7=4j142c8jT+ScCrtN8JELqv89va4s3SDEaHytg@mail.gmail.com>
Message-ID: <CABch2GvYoj8y6R=5+ayHhoZ-cBxD2Vmuqk3O+dM030v_oOgRHw@mail.gmail.com>

I am now using random effects of GAM to predict crash frequency at
intersections. The family is negative binomial distribution. I wonder if
this term bs="re" can account for both random intercepts and random slopes
effects together?

It is longitudinal data. The number of crashes, traffic volume on major
roads and traffic volume on minor roads are observed repeatedly three years
for each different intersection as below.

crash main.traffic.volume minor.traffic.volume  ID
1 38122 2789 1
0 40216 2930 1
0 41231 3022 1
2 12890 3401 2
3 13241 3211 2
2 13568 3322 2
1 46889 5688 3
2 48232 5843 3
0 52301 6012 3
*... ...         ...         ...*

I wonder if my code below can account for both random intercepts and random
slopes effects together? or only random slopes effects? Any suggestions?
#########################################################################################
gam(crash~s(main.traffic.volume, ID, bs=?re")+s(minor.traffic.volume, ID,
bs="re"), family=nb(),data=mydata3sg)
#########################################################################################

Many thanks!
Julian

On Fri, Feb 12, 2016 at 1:58 PM, Julian Chen <power.julian.chen at gmail.com>
wrote:

> I am now trying to use random effects in GAMs developed by Professor Simon
> Wood. Prof Wood uses  s(...,bs="re") to account for the random effects.
> Random intercepts models or random slopes models are two different types of
> mixed linear models or general random effects model (Cameron and Trivedi,
> 2005). I wonder if the Wood's method includes both random intercepts and
> random slopes. Based on my understanding, this method does? Anyone can help
> me clarify this method? The following is the link of Random effects in GAM
> developed by Prof Wood.
>
> https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/random.effects.html
>
> --
> Life is nothing but a dream,
>      and if we are artists,
> then we can create our life with Love,
>      and our dream becomes
>        a masterpiece of art.
>



-- 
Life is nothing but a dream,
     and if we are artists,
then we can create our life with Love,
     and our dream becomes
       a masterpiece of art.

	[[alternative HTML version deleted]]


From daniel_rubi at ymail.com  Fri Feb 12 20:16:01 2016
From: daniel_rubi at ymail.com (Daniel Rubi)
Date: Fri, 12 Feb 2016 19:16:01 +0000 (UTC)
Subject: [R-sig-ME] A model for repeated treatments and repeated outcomes
References: <2095815309.3277205.1455304561523.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <2095815309.3277205.1455304561523.JavaMail.yahoo@mail.yahoo.com>

Thanks a lot for the advice Peter.

Just to make sure I got it correctly. Will the model would be this multivariate model:

gfrs_post_op ~ gfr_pre_op1 + gfr_pre_op2 + gfr_pre_op3 + SamplePeriod + (SamplePeriod | Patient)

where gfrs_post-op is a matrix of patient x gfr_post_op

Thanks a lot,
Dan

--------------------------------------------
On Fri, 2/12/16, Peter Claussen <dakotajudo at mac.com> wrote:

 Subject: Re: [R-sig-ME] A model for repeated treatments and repeated outcomes
 To: "Daniel Rubi" <daniel_rubi at ymail.com>
 Cc: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
 Date: Friday, February 12, 2016, 12:43 PM

 Dan,

 If I may jump in.

 If I read this correctly, you have two random
 samples per patient - a random sample of pre-operation GRF
 and a random sample post-operation. I think these should be
 considered random since you state the points are not evenly
 spaced in either case.

 So
 you add a factor for sampling period (SamplePeriod =
 c(?Pre?,?Post?)) and model random effect as
 (SamplePeriod | Patient).

 That?s my first thought - I?m not sure
 given three points per sample period you can reasonably
 measure trends.

 Peter

 > On Feb 11, 2016, at 7:01
 PM, Daniel Rubi via R-sig-mixed-models <r-sig-mixed-models at r-project.org>
 wrote:
 > 
 > Dear
 Thierry,
 > 
 > Thanks a
 lot for the help.
 > 
 >
 Please allow me to explain better.
 > 
 > What I'm trying to estimate is whether
 the trend in pre-operation GFR affects the levels of
 post-operation GFR.
 > The GFR is a
 measurement that assesses kidney function based on a blood
 sample and for that reason it reflects the functions of both
 kidneys.
 > The operation is either
 partial or total removal of one kidney - that is affected.
 Therefore it is supposed to prevent GFR from escalating.
 > 
 > What I'm
 interesting in estimating is whether the trend of
 pre-operation GFR (along the time pre-operation time points)
 is a good predictor of the post-operatoin GFR. It's not
 clear at which post-operation time point it is correct to
 measure GFR and therefore I have 3 time points.
 > 
 > The trivial thing to
 do is to estimate the slope of pre-operation GFR and
 individually regress each of the post-operation GFRs against
 thos.
 > 
 > But I'm
 wondering whether there's anything better?
 > 
 > Thanks a lot,
 > Dan
 > 
 > 
 >
 --------------------------------------------
 > On Wed, 2/10/16, Thierry Onkelinx <thierry.onkelinx at inbo.be>
 wrote:
 > 
 > Subject:
 Re: [R-sig-ME] A model for repeated treatments and repeated
 outcomes
 > To: "Daniel Rubi"
 <daniel_rubi at ymail.com>
 > Cc: "r-sig-mixed-models at r-project.org"
 <r-sig-mixed-models at r-project.org>
 > Date: Wednesday, February 10, 2016, 3:54
 AM
 > 
 > Dear
 > Dan,
 > You'll need
 to
 > provide more information. What is
 the global pattern that
 > you expect
 (linear? quadratic? non-linear?) How to you thing
 > that the operation can effect the GFR? You
 need to answer
 > those kind of questions
 so that you can make a sensible
 > fixed
 effects part of the model.
 > The random
 effect is probably just
 > ~1|Patient. And
 a corCAR1(form = ~ Time) can handle the
 >
 temporal correlation within the patient.
 > Best regards,
 > ir.
 Thierry Onkelinx
 > Instituut voor natuur-
 en bosonderzoek /
 > Research Institute
 for Nature and Forest 
 > team Biometrie
 & Kwaliteitszorg / team
 > Biometrics
 & Quality Assurance 
 > Kliniekstraat
 25
 > 1070
 >
 Anderlecht
 > Belgium
 >

 > To call in the statistician after
 the
 > experiment is done may be no more
 than asking him to perform
 > a
 post-mortem examination: he may be able to say what the
 > experiment died of. ~ Sir Ronald Aylmer
 Fisher
 > The plural of anecdote is not
 data. ~ Roger
 > Brinner 
 > The combination of some data and an
 > aching desire for an answer does not
 ensure that a
 > reasonable answer can be
 extracted from a given body of
 > data. ~
 John Tukey
 > 
 >
 2016-02-09 0:55 GMT+01:00
 > Daniel Rubi
 via R-sig-mixed-models <r-sig-mixed-models at r-project.org>:
 >? I have the following experimental
 > design:Measurements of kidney function (in
 units called GFR)
 > taken at several time
 points pre-operation (time points not
 >
 evenly spaced) and at several time points post-operation
 > (neither evenly spaced).
 > 
 > Here's an
 example of my data in R code:
 > 
 > set.seed(1)df <- data.frame(patient =
 letters[1:10],???
 >? ? ? ?
 ? ???gfr_ten_days_prop = rnorm(10,5,1),
 > gfr_five_days_prop = rnorm(10,10,1),
 gfr_three_days_prop =
 > rnorm(10,12,1),?
 ? ? ? ? ? ? ? gfr_one_day_postop =
 > rnorm(10,10,1), gfr_one_day_postop =
 rnorm(10,5,1),
 > gfr_one_day_postop =
 rnorm(10,2,1))
 > 
 >

 > 
 > I'm looking
 for a model which will estimate the effect
 > of pre-operation GFR on post-operation
 GFR, taking into
 > account the different
 times at which GFRs were measured pre-
 >
 and post-operation.One additional possible caveat - my
 data
 > contain missing values (NAs).
 > 
 > I'm having a
 hard time seeing how a mixed-effects model
 > fits this problem since in all the
 examples of repeated
 >
 measures/longitudinal data I came across in each time
 point
 > the response is measured whereas
 here it is more a
 > predictive question -
 how strong does each pre-operation GFR
 >
 predict pos-operation GFR, where the time at which GFRs
 were
 > measured may matter.
 > 
 > Thanks a lot,Dan
 > 
 >? ? ?
 ???[[alternative HTML version deleted]]
 > 
 > 
 > 
 >
 _______________________________________________
 > 
 > R-sig-mixed-models at r-project.org
 > mailing list
 > 
 > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
 >

 >
 _______________________________________________
 > R-sig-mixed-models at r-project.org
 mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From peggy_moore at usgs.gov  Sat Feb 13 01:55:04 2016
From: peggy_moore at usgs.gov (Moore, Peggy)
Date: Fri, 12 Feb 2016 16:55:04 -0800
Subject: [R-sig-ME] glmmADMB error: Invalid index for array
Message-ID: <CAPO7DkfM7KZ1SC_8aE=TpcPOa-pN3RAFMHD_AGPir5-eOv8v1A@mail.gmail.com>

I am fitting models to investigate the relationship between herbicide
treatment and species response using the R package glmmADMB with 2 fixed
(treatment, year) and 2 random (block, plot) effects. I would like to use
MCMC to improve estimates and confidence limits. Model fit and output is
sensible without mcmc, but when mcmc=TRUE, the response is simply 'no PSV
file found', i.e., no output, and model output is not produced. When I run
the executable from the cmd prompt, I receive an invalid-index error.
For example, using the Owls data set that comes with the package, the
following model from Getting Started with glmmADMB by Bolker et al., and
this same process, the error is: Invalid index 11 used for array range [1,
10] in "double& dvector::operator [] (int i)":

OwlModel_nb1_bs <- glmmadmb(NCalls ~ (FoodTreatment + ArrivalTime) *
SexParent +
                                 BroodSize + (1 | Nest), data = Owls,
zeroInflation = TRUE,
                                 family = "nbinom1",
                                 mcmc = TRUE, mcmc.opts =
mcmcControl(mcmc=25000, mcsave=25),
                                 debug=TRUE,
admb.opts=admbControl(run=TRUE,
                                 noinit=FALSE, shess=FALSE),
                                 save.dir = "C:/Temp/glmm")
I would appreciate any recommendations on how I might avoid this error.
Peggy
My sessionInfo()
R version 3.2.1 (2015-06-18)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] pastecs_1.3-18   boot_1.3-17      MCMCglmm_2.21    ape_3.3
 coda_0.17-1      Matrix_1.2-2     ggplot2_1.0.1
[8] glmmADMB_0.8.3.3 MASS_7.3-43

loaded via a namespace (and not attached):
 [1] Rcpp_0.11.6      magrittr_1.5     munsell_0.4.2    colorspace_1.2-6
lattice_0.20-33  stringr_1.0.0    plyr_1.8.3
 [8] tools_3.2.1      grid_3.2.1       gtable_0.1.2     nlme_3.1-121
corpcor_1.6.8    htmltools_0.2.6  yaml_2.1.13
[15] digest_0.6.8     tensorA_0.36     R2admb_0.7.5.3   reshape2_1.4.1
rmarkdown_0.8    stringi_0.5-5    scales_0.2.5
[22] proto_0.3-10

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Mon Feb 15 05:46:50 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 15 Feb 2016 04:46:50 +0000 (UTC)
Subject: [R-sig-ME] glmmADMB error: Invalid index for array
References: <CAPO7DkfM7KZ1SC_8aE=TpcPOa-pN3RAFMHD_AGPir5-eOv8v1A@mail.gmail.com>
Message-ID: <loom.20160215T045256-449@post.gmane.org>

Moore, Peggy <peggy_moore at ...> writes:

> 
> I am fitting models to investigate the relationship between herbicide
> treatment and species response using the R package glmmADMB with 2 fixed
> (treatment, year) and 2 random (block, plot) effects. I would like to use
> MCMC to improve estimates and confidence limits. Model fit and output is
> sensible without mcmc, but when mcmc=TRUE, the response is simply 'no PSV
> file found', i.e., no output, and model output is not produced. When I run
> the executable from the cmd prompt, I receive an invalid-index error.
> For example, using the Owls data set that comes with the package, the
> following model from Getting Started with glmmADMB by Bolker et al., and
> this same process, the error is: Invalid index 11 used for array range [1,
> 10] in "double& dvector::operator [] (int i)":
> 
> OwlModel_nb1_bs <- glmmadmb(NCalls ~ (FoodTreatment + ArrivalTime) *
> SexParent +
>                                  BroodSize + (1 | Nest), data = Owls,
> zeroInflation = TRUE,
>                                  family = "nbinom1",
>                                  mcmc = TRUE, mcmc.opts =
> mcmcControl(mcmc=25000, mcsave=25),
>                                  debug=TRUE,
> admb.opts=admbControl(run=TRUE,
>                                  noinit=FALSE, shess=FALSE),
>                                  save.dir = "C:/Temp/glmm")
> I would appreciate any recommendations on how I might avoid this error.
> Peggy


Hmm.  I just ran approximately the same thing on my machine, and couldn't
get it to break.  I didn't run it for as long, but otherwise everything else
was (I think) identical.  Did you look in your temp directory?  is there
a .PSV file there?


OwlModel_nb1_bs <- glmmadmb(SiblingNegotiation ~ 
              (FoodTreatment + ArrivalTime) *
                             SexParent +
                                 BroodSize + (1 | Nest), data = Owls,
                               zeroInflation = TRUE,
                                 family = "nbinom1",
                                 mcmc = TRUE, mcmc.opts =
                            mcmcControl(mcmc=100, mcsave=5),
                                 debug=TRUE,
                         admb.opts=admbControl(run=TRUE,
                                 noinit=FALSE, shess=FALSE),
                                 save.dir="tmpadmb")


> My sessionInfo()
> R version 3.2.1 (2015-06-18)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 7 x64 (build 7601) Service Pack 1
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] pastecs_1.3-18   boot_1.3-17      MCMCglmm_2.21    ape_3.3
>  coda_0.17-1      Matrix_1.2-2     ggplot2_1.0.1
> [8] glmmADMB_0.8.3.3 MASS_7.3-43
> 
> loaded via a namespace (and not attached):
>  [1] Rcpp_0.11.6      magrittr_1.5     munsell_0.4.2    colorspace_1.2-6
> lattice_0.20-33  stringr_1.0.0    plyr_1.8.3
>  [8] tools_3.2.1      grid_3.2.1       gtable_0.1.2     nlme_3.1-121
> corpcor_1.6.8    htmltools_0.2.6  yaml_2.1.13
> [15] digest_0.6.8     tensorA_0.36     R2admb_0.7.5.3   reshape2_1.4.1
> rmarkdown_0.8    stringi_0.5-5    scales_0.2.5
> [22] proto_0.3-10
> 
> 	[[alternative HTML version deleted]]
> 
>


From m.ramon.fernandez at gmail.com  Mon Feb 15 08:59:42 2016
From: m.ramon.fernandez at gmail.com (Manuel Ramon)
Date: Mon, 15 Feb 2016 08:59:42 +0100
Subject: [R-sig-ME] Help on random regression model with correlated measures
 on consecutive days
Message-ID: <CAHB8JpK_Qf8LWJ6Qg3Uxkb_4vgxuWZ3rAUK=CF6EGWRuZU1e-g@mail.gmail.com>

Dear list members,
I want to examine the effect of climate on individual behavior (in my case
animal productive performance). My model has different levels of variation:
- 1st: individual production level, being high at the beginning and falling
with time. It is expected that climate effect was higher when individuals
yield at higher levels.
- 2nd: day of climate measurement from day of production measurement. I
have climate measures for the day of production measurement and the
previous 30 days. It is expected that the effect of climate conditions on
the day of production measurement was higher that on the day 15 previous to
production measurement. At this point, it is also important to take into
account that the effect of climate conditions when temperature equals, for
example, to 20?C changed depending on whether in the days before it was
cold or hot (that is, is not the same 20?C from 30?C than from 10?C).

To address this study, I run a random regression model that included, in
addition to other fixed effects, a term for production (quadratic
polynomial), a term for average temperature on the day of production
measurement and the 3 previous days (quadratic polynomial) and a random
individual effect with nested quadratic polynomials for production and
average temperature.

The model run well, but I think that production and temperature effects are
confounded and that I am not considering the lagged effect of temperature
on days previous to production measurement, especially if previous days
were hotter or colder.

My question is how to design a model that include the production trend, the
effect of temperature depending on production level and how this
temperature effect changes depending on the day in which temperature was
measured.

Thanks for your help,
Manuel

--

	[[alternative HTML version deleted]]


From jenyourkavitch at yahoo.com  Mon Feb 15 20:22:40 2016
From: jenyourkavitch at yahoo.com (Jennifer Yourkavitch)
Date: Mon, 15 Feb 2016 14:22:40 -0500
Subject: [R-sig-ME] new R user struggling with error and convergence issues
Message-ID: <61816917-B0DD-46BC-9FF7-5D5A615BE599@yahoo.com>

Hello!
I am fitting multilevel log-binomial models for a very large dataset (>53,000 observations), using glmer. I am getting this--  Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate.
And the model is not converging. 
I think that increasing the number of iterations may remediate the convergence issue. But I can?t find the right syntax for that online.
Any advice re these two issues?
Many thanks,
Jennifer


From bbolker at gmail.com  Mon Feb 15 21:21:03 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 15 Feb 2016 15:21:03 -0500
Subject: [R-sig-ME] new R user struggling with error and convergence
	issues
In-Reply-To: <61816917-B0DD-46BC-9FF7-5D5A615BE599@yahoo.com>
References: <61816917-B0DD-46BC-9FF7-5D5A615BE599@yahoo.com>
Message-ID: <CABghstTNReOwtNzobDcZMHzMxq_-FmxYf4Ub8Ak1HVxuUYPJXw@mail.gmail.com>

It is admittedly hard to increase the "maxstephalfit" parameter, but
part of the reason we haven't put much effort into making it easier is
that in our experience it rarely helps.  Log-binomial models in
particular are tricky because the log link (i.e. the exponential
inverse-link) doesn't naturally constrain the response to the allowed
range of (0,1), so these models are hard to fit (the same applies to
inverse-link Gamma models).  The bigger your data set, the more likely
you are to run into cases where the predicted value is >1.  We could
admittedly do a much better job giving useful warnings about when and
where this problem occurred, and (more controversially) allow users to
clamp the output of the inverse-link function to the allowable domain
...

library("lme4")
set.seed(101)
dd <- data.frame(x=seq(-4,4,length=500),
                 f=sample(letters[1:10],size=500,replace=TRUE))
dd$y <- simulate(~x+(1|f),family=binomial,
                 newdata=dd,newparams=list(beta=c(0,1),theta=1))[[1]]
library("ggplot2")
ggplot(dd,aes(x,y,colour=f))+geom_point()+
    geom_smooth(method="glm",method.args=list(family=binomial))

m1 <- glmer(y~x+(1|f), dd, family=binomial)
m2 <- glmer(y~x+(1|f), dd, family=binomial(link="log"))  ## PIRLS
step-halving problem

On Mon, Feb 15, 2016 at 2:22 PM, Jennifer Yourkavitch via
R-sig-mixed-models <r-sig-mixed-models at r-project.org> wrote:
> Hello!
> I am fitting multilevel log-binomial models for a very large dataset (>53,000 observations), using glmer. I am getting this--  Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate.
> And the model is not converging.
> I think that increasing the number of iterations may remediate the convergence issue. But I can?t find the right syntax for that online.
> Any advice re these two issues?
> Many thanks,
> Jennifer
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From jes-w at hotmail.com  Tue Feb 16 16:45:22 2016
From: jes-w at hotmail.com (Jessica Witt)
Date: Tue, 16 Feb 2016 15:45:22 +0000
Subject: [R-sig-ME] Mixed Models
Message-ID: <BLU436-SMTP16402D91D9130BAE8650014E9AD0@phx.gbl>

Hi,

I?m very new to mixed modelling so please forgive me for this! I am trying to see the repeatability of a behaviour for individuals e.g. the area travelled, but I?ve also got a number of variables that could confound this so am I right in including these as fixed factors? 

Furthermore I?ve written the code but it?s not giving me anything and I?m pretty unsure what the error message means, if someone could explain to me any modifications I need in my R code to get the repeatability of area travelled that would be amazing!

areamod<-lmer(Area~1+Sex+Temp+Mass+Length+Dodgy+Tank+Order+(1|ID), data=platy, na.action=na.exclude, REML=TRUE)
fixed-effect model matrix is rank deficient so dropping 1 column / coefficient

From neilandertal at gmail.com  Tue Feb 16 16:57:59 2016
From: neilandertal at gmail.com (Neil French Collier)
Date: Tue, 16 Feb 2016 16:57:59 +0100
Subject: [R-sig-ME] Mixed Models
In-Reply-To: <BLU436-SMTP16402D91D9130BAE8650014E9AD0@phx.gbl>
References: <BLU436-SMTP16402D91D9130BAE8650014E9AD0@phx.gbl>
Message-ID: <CAOVghqoWyw0pWGnJXO5WSUyiU9zBOMEdLFTv=szbboXRdSD1tg@mail.gmail.com>

Hi Jessica,

It's likely that you don't have enough data to fit the model you described.
These are more like stats questions (i.e. what factors to include), but I
think if you provide more information about your problem (A reproducible
example) then you might get more help. See here for how to do this:
http://adv-r.had.co.nz/Reproducibility.html.

Cheers,

Neil

On Tue, Feb 16, 2016 at 4:45 PM, Jessica Witt <jes-w at hotmail.com> wrote:

> Hi,
>
> I?m very new to mixed modelling so please forgive me for this! I am trying
> to see the repeatability of a behaviour for individuals e.g. the area
> travelled, but I?ve also got a number of variables that could confound this
> so am I right in including these as fixed factors?
>
> Furthermore I?ve written the code but it?s not giving me anything and I?m
> pretty unsure what the error message means, if someone could explain to me
> any modifications I need in my R code to get the repeatability of area
> travelled that would be amazing!
>
> areamod<-lmer(Area~1+Sex+Temp+Mass+Length+Dodgy+Tank+Order+(1|ID),
> data=platy, na.action=na.exclude, REML=TRUE)
> fixed-effect model matrix is rank deficient so dropping 1 column /
> coefficient
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




-- 
Cheers,

Neil

Neil French Collier, PhD
Faculty of Sustainability
Leuphana University L?neburg
Rotenbleicher Weg 67
21335 Lueneburg
Germany

Twitter: @foodsecbio
email: collier at leuphana.de
Google Scholar
<https://scholar.google.com.au/citations?hl=en&user=xVdc-dsAAAAJ&view_op=list_works&sortby=pubdate>
Ideas for Sustainability <https://ideas4sustainability.wordpress.com>

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Tue Feb 16 17:06:33 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 16 Feb 2016 11:06:33 -0500
Subject: [R-sig-ME] Mixed Models
In-Reply-To: <CAOVghqoWyw0pWGnJXO5WSUyiU9zBOMEdLFTv=szbboXRdSD1tg@mail.gmail.com>
References: <BLU436-SMTP16402D91D9130BAE8650014E9AD0@phx.gbl>
	<CAOVghqoWyw0pWGnJXO5WSUyiU9zBOMEdLFTv=szbboXRdSD1tg@mail.gmail.com>
Message-ID: <56C34909.7060608@gmail.com>

    More specifically, "fixed effect model matrix is rank-deficient" 
means that several of your predictor variables are perfectly collinear; 
one way this could happen, e.g. is if "Dodgy" is a binary variable that 
varies only across tanks (i.e. some tanks are dodgy, some are not), so 
that once Tank is in the model as a fixed effect, Dodgy gives no further 
information.

fixef(areamod,add.dropped=TRUE)  should give you some more hints about 
which variables were dropped (but not directly which ones they were 
collinear with, although you may able to guess at that point).

   It does seem like a good idea to include confounders/moderators as 
fixed effects.  Perhaps you should consider including Tank as a random 
effect (i.e. (1|Tank)), which might also take care of your collinearity 
problem?

On 16-02-16 10:57 AM, Neil French Collier wrote:
> Hi Jessica,
>
> It's likely that you don't have enough data to fit the model you described.
> These are more like stats questions (i.e. what factors to include), but I
> think if you provide more information about your problem (A reproducible
> example) then you might get more help. See here for how to do this:
> http://adv-r.had.co.nz/Reproducibility.html.
>
> Cheers,
>
> Neil
>
> On Tue, Feb 16, 2016 at 4:45 PM, Jessica Witt <jes-w at hotmail.com> wrote:
>
>> Hi,
>>
>> I?m very new to mixed modelling so please forgive me for this! I am trying
>> to see the repeatability of a behaviour for individuals e.g. the area
>> travelled, but I?ve also got a number of variables that could confound this
>> so am I right in including these as fixed factors?
>>
>> Furthermore I?ve written the code but it?s not giving me anything and I?m
>> pretty unsure what the error message means, if someone could explain to me
>> any modifications I need in my R code to get the repeatability of area
>> travelled that would be amazing!
>>
>> areamod<-lmer(Area~1+Sex+Temp+Mass+Length+Dodgy+Tank+Order+(1|ID),
>> data=platy, na.action=na.exclude, REML=TRUE)
>> fixed-effect model matrix is rank deficient so dropping 1 column /
>> coefficient
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
>


From bbolker at gmail.com  Tue Feb 16 17:11:53 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 16 Feb 2016 11:11:53 -0500
Subject: [R-sig-ME] hurdle modfels in glmmadmb
In-Reply-To: <CAEJqwPD-jzRBU6q9OOC965Thfn4UB9eEFYi+WLgh1JPmZ7+x9w@mail.gmail.com>
References: <CAEJqwPD-jzRBU6q9OOC965Thfn4UB9eEFYi+WLgh1JPmZ7+x9w@mail.gmail.com>
Message-ID: <56C34A49.1080006@mcmaster.ca>

[cc'ing r-sig-mixed-models]

if you assume that the zero-causing process and the non-zero 
count-determining process are independent (which is not necessarily 
true, but which I think you're more or less forced to assume by 
identifiability issues in the absence of more information), then you 
should just be able to add the two distinct AIC values for the two 
components of the model to get a single AIC value.  (Since the processes 
are independent, the log-likelihoods are additive, and the number of 
parameters in the combined model is clearly additive too.)
    You could cross-check by checking what the pscl package does (it 
deals with zero-inflated GLMs too, although not mixed zero-inflated GLMs).

On 16-02-16 03:35 AM, Panagiotis Theodorou wrote:
> Dear Ben Bolker,
>
> I would like to thank you very much for developing the r package
> glmmadmb that I am currently using to model my zero inflated data.
> I have one small question regarding using two part (hurdle) models
> within the glmmadmb package.
> (http://glmmadmb.r-forge.r-project.org/glmmADMB.html)
> Is there a way to evaluate the fit (e.g. AIC) of a two part model as a
> whole and justify using it?
> Or the only way to justify using a hurdle model comes from prior
> hypothesis of two processes; one causing zeros versus non-zeros, and
> another process explaining the non-zero counts?
>
> Thank you in advance
>
> Panagiotis Theodorou
>


From emmanuel.curis at parisdescartes.fr  Tue Feb 16 17:14:02 2016
From: emmanuel.curis at parisdescartes.fr (Emmanuel Curis)
Date: Tue, 16 Feb 2016 17:14:02 +0100
Subject: [R-sig-ME] Mixed Models
In-Reply-To: <BLU436-SMTP16402D91D9130BAE8650014E9AD0@phx.gbl>
References: <BLU436-SMTP16402D91D9130BAE8650014E9AD0@phx.gbl>
Message-ID: <20160216161402.GA31298@info124.pharmacie.univ-paris5.fr>

Hi Jessica,

It is very difficult to help about the error message without more
details without more details on your dataset, but basically it means
that you have redundant information in your 'fixed effects' variables
(counfounding variables).

For instance, you may have only male (so "sex" is useless, redundant
with 1), or "dodgy" may be 1 only for males and 0 only for femalees,
to the two variables are in fact the same...

More subtle, ? mass ? may be an affine function of ? length ? (like
mass = 3 * length + 4 for instance), here again making these two
variables redundant.

Including confounding variables as fixed effects seems otherwise a
good starting point, but here again without any details about their
meaning, it's impossible to say if you are right in the syntax and so
on.

Best regards,

On Tue, Feb 16, 2016 at 03:45:22PM +0000, Jessica Witt wrote:
? Hi,
? 
? I?m very new to mixed modelling so please forgive me for this! I am trying to see the repeatability of a behaviour for individuals e.g. the area travelled, but I?ve also got a number of variables that could confound this so am I right in including these as fixed factors? 
? 
? Furthermore I?ve written the code but it?s not giving me anything and I?m pretty unsure what the error message means, if someone could explain to me any modifications I need in my R code to get the repeatability of area travelled that would be amazing!
? 
? areamod<-lmer(Area~1+Sex+Temp+Mass+Length+Dodgy+Tank+Order+(1|ID), data=platy, na.action=na.exclude, REML=TRUE)
? fixed-effect model matrix is rank deficient so dropping 1 column / coefficient
? _______________________________________________
? R-sig-mixed-models at r-project.org mailing list
? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From paul.debes at utu.fi  Tue Feb 16 18:23:40 2016
From: paul.debes at utu.fi (Paul Debes)
Date: Tue, 16 Feb 2016 19:23:40 +0200
Subject: [R-sig-ME] Mixed Models
In-Reply-To: <BLU436-SMTP16402D91D9130BAE8650014E9AD0@phx.gbl>
References: <BLU436-SMTP16402D91D9130BAE8650014E9AD0@phx.gbl>
Message-ID: <op.ycxrxhdcsgx3xe@armadillo50>

Hi Jessica,

There is a very useful paper on repeatability that explains how to  
estimate it for many experimental settings and also deals with the  
inclusion of confounding fixed effects. Hence, it may answer some of your  
questions:

Nakagawa S, Schielzeth H (2010) Repeatability for Gaussian and  
non-Gaussian data: a practical guide for biologists. Biological Reviews of  
the Cambridge Philosophical Society, 85, 935-956.

Best,
Paul

On Tue, 16 Feb 2016 17:45:22 +0200, Jessica Witt <jes-w at hotmail.com> wrote:

> Hi,
>
> I?m very new to mixed modelling so please forgive me for this! I am  
> trying to see the repeatability of a behaviour for individuals e.g. the  
> area travelled, but I?ve also got a number of variables that could  
> confound this so am I right in including these as fixed factors?
>
> Furthermore I?ve written the code but it?s not giving me anything and  
> I?m pretty unsure what the error message means, if someone could explain  
> to me any modifications I need in my R code to get the repeatability of  
> area travelled that would be amazing!
>
> areamod<-lmer(Area~1+Sex+Temp+Mass+Length+Dodgy+Tank+Order+(1|ID),  
> data=platy, na.action=na.exclude, REML=TRUE)
> fixed-effect model matrix is rank deficient so dropping 1 column /  
> coefficient
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Paul V. Debes
DFG Research Fellow
University of Turku
Department of Biology
Finland


From HDoran at air.org  Tue Feb 16 18:40:02 2016
From: HDoran at air.org (Doran, Harold)
Date: Tue, 16 Feb 2016 17:40:02 +0000
Subject: [R-sig-ME] [R] Comparing variance components
In-Reply-To: <477CE007-DA6D-4031-B86F-FCAB156578EB@gmail.com>
References: <477CE007-DA6D-4031-B86F-FCAB156578EB@gmail.com>
Message-ID: <B08B6AF0CF8CA44F81B9983EEBDCD686012BB58ADB@DC1VEX10MB01.air.org>

(adding R mixed group). You actually do not want to do this test, and there is no "shrinkage" here on these variances. First, there are conditional variances and marginal variances in the mixed model. What you are have below as "A" is the marginal variances of the random effects and there is no shrinkage on these, per se.

The conditional means of the random effects have shrinkage and each conditional mean (or BLUP) has a conditional variance. 

Now, it seems very odd to want to compare the variance between A and then what you have as sigma2_e, which is presumably the residual variance. These are variances of two completely different things, so a test comparing them seems strange, though I suppose some theoretical reason could exists justifying it, I cannot imagine one though. 





-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wen Huang
Sent: Tuesday, February 16, 2016 10:57 AM
To: r-help at r-project.org
Subject: [R] Comparing variance components

Dear R-help members,

Say I have two data sets collected at different times with the same design. I fit a mixed model using in R using lmer

lmer(y ~ (1|A))

to these data sets and get two estimates of sigma2_A and sigma2_e

What would be a good way to compare sigma2_A and sigma2_e for these two data sets and obtain a P value for the hypothesis that sigma2_A1 = sigma2_A2? There is obvious shrinkage on these estimates, should I be worried about the differential levels of shrinkage on these estimates and how to account for that?

Thank you for your thoughts and inputs!



	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From whuang.ustc at gmail.com  Tue Feb 16 18:56:50 2016
From: whuang.ustc at gmail.com (Wen Huang)
Date: Tue, 16 Feb 2016 12:56:50 -0500
Subject: [R-sig-ME] [R] Comparing variance components
In-Reply-To: <B08B6AF0CF8CA44F81B9983EEBDCD686012BB58ADB@DC1VEX10MB01.air.org>
References: <477CE007-DA6D-4031-B86F-FCAB156578EB@gmail.com>
	<B08B6AF0CF8CA44F81B9983EEBDCD686012BB58ADB@DC1VEX10MB01.air.org>
Message-ID: <7516BB49-2EF2-4F5D-9418-B507B4809DF9@gmail.com>

Hi Harold,

Thank you for your input. I was not very clear. I wanted to compare the sigma2_A?s from the same model fitted to two different data sets. The same for sigma2_e?s. The motivation is when I did the same experiment at two different times, whether the variance due to A (sigma2_A) is bigger at one time versus another. The same for sigma2_e, whether the residual variance is bigger for one experiment versus another.

Thanks,
Wen

> On Feb 16, 2016, at 12:40 PM, Doran, Harold <HDoran at air.org> wrote:
> 
> (adding R mixed group). You actually do not want to do this test, and there is no "shrinkage" here on these variances. First, there are conditional variances and marginal variances in the mixed model. What you are have below as "A" is the marginal variances of the random effects and there is no shrinkage on these, per se.
> 
> The conditional means of the random effects have shrinkage and each conditional mean (or BLUP) has a conditional variance. 
> 
> Now, it seems very odd to want to compare the variance between A and then what you have as sigma2_e, which is presumably the residual variance. These are variances of two completely different things, so a test comparing them seems strange, though I suppose some theoretical reason could exists justifying it, I cannot imagine one though. 
> 
> 
> 
> 
> 
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wen Huang
> Sent: Tuesday, February 16, 2016 10:57 AM
> To: r-help at r-project.org
> Subject: [R] Comparing variance components
> 
> Dear R-help members,
> 
> Say I have two data sets collected at different times with the same design. I fit a mixed model using in R using lmer
> 
> lmer(y ~ (1|A))
> 
> to these data sets and get two estimates of sigma2_A and sigma2_e
> 
> What would be a good way to compare sigma2_A and sigma2_e for these two data sets and obtain a P value for the hypothesis that sigma2_A1 = sigma2_A2? There is obvious shrinkage on these estimates, should I be worried about the differential levels of shrinkage on these estimates and how to account for that?
> 
> Thank you for your thoughts and inputs!
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Paul.Thompson at sanfordhealth.org  Tue Feb 16 19:18:42 2016
From: Paul.Thompson at sanfordhealth.org (Thompson,Paul)
Date: Tue, 16 Feb 2016 18:18:42 +0000
Subject: [R-sig-ME] [R] Comparing variance components
In-Reply-To: <7516BB49-2EF2-4F5D-9418-B507B4809DF9@gmail.com>
References: <477CE007-DA6D-4031-B86F-FCAB156578EB@gmail.com>
	<B08B6AF0CF8CA44F81B9983EEBDCD686012BB58ADB@DC1VEX10MB01.air.org>
	<7516BB49-2EF2-4F5D-9418-B507B4809DF9@gmail.com>
Message-ID: <9B75E7CF385CB94EAD6587DD96AC2D9701D16E4B2E@SFSMCEXMBX3.sanfordhealth.org>

Are you computing two estimates of reliability and wishing to compare them? One possible method is to set both into the same design, treat the design effect (Exp 1, Exp 2) as a fixed effect, and compare them with a standard F test. 

-----Original Message-----
From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Wen Huang
Sent: Tuesday, February 16, 2016 11:57 AM
To: Doran, Harold
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] [R] Comparing variance components

Hi Harold,

Thank you for your input. I was not very clear. I wanted to compare the sigma2_A?s from the same model fitted to two different data sets. The same for sigma2_e?s. The motivation is when I did the same experiment at two different times, whether the variance due to A (sigma2_A) is bigger at one time versus another. The same for sigma2_e, whether the residual variance is bigger for one experiment versus another.

Thanks,
Wen

> On Feb 16, 2016, at 12:40 PM, Doran, Harold <HDoran at air.org> wrote:
> 
> (adding R mixed group). You actually do not want to do this test, and there is no "shrinkage" here on these variances. First, there are conditional variances and marginal variances in the mixed model. What you are have below as "A" is the marginal variances of the random effects and there is no shrinkage on these, per se.
> 
> The conditional means of the random effects have shrinkage and each conditional mean (or BLUP) has a conditional variance. 
> 
> Now, it seems very odd to want to compare the variance between A and then what you have as sigma2_e, which is presumably the residual variance. These are variances of two completely different things, so a test comparing them seems strange, though I suppose some theoretical reason could exists justifying it, I cannot imagine one though. 
> 
> 
> 
> 
> 
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wen 
> Huang
> Sent: Tuesday, February 16, 2016 10:57 AM
> To: r-help at r-project.org
> Subject: [R] Comparing variance components
> 
> Dear R-help members,
> 
> Say I have two data sets collected at different times with the same 
> design. I fit a mixed model using in R using lmer
> 
> lmer(y ~ (1|A))
> 
> to these data sets and get two estimates of sigma2_A and sigma2_e
> 
> What would be a good way to compare sigma2_A and sigma2_e for these two data sets and obtain a P value for the hypothesis that sigma2_A1 = sigma2_A2? There is obvious shrinkage on these estimates, should I be worried about the differential levels of shrinkage on these estimates and how to account for that?
> 
> Thank you for your thoughts and inputs!
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
-----------------------------------------------------------------------
Confidentiality Notice: This e-mail message, including any attachments,
is for the sole use of the intended recipient(s) and may contain
privileged and confidential information.  Any unauthorized review, use,
disclosure or distribution is prohibited.  If you are not the intended
recipient, please contact the sender by reply e-mail and destroy
all copies of the original message.

From bgunter.4567 at gmail.com  Tue Feb 16 19:07:11 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 16 Feb 2016 10:07:11 -0800
Subject: [R-sig-ME] [R] Comparing variance components
In-Reply-To: <1B7ADF10-E2B4-4633-B790-32698DCFA5BF@gmail.com>
References: <477CE007-DA6D-4031-B86F-FCAB156578EB@gmail.com>
	<B08B6AF0CF8CA44F81B9983EEBDCD686012BB58ADB@DC1VEX10MB01.air.org>
	<1B7ADF10-E2B4-4633-B790-32698DCFA5BF@gmail.com>
Message-ID: <CAGxFJbRoY41xsaXMXLirwuujoT2OX-53oKVSnTeQ3C0xbdS_aQ@mail.gmail.com>

I'll save you the trouble.

Yes, they're bigger. Or smaller. Certainly differ between experiments.  So
what? That is just the way things work.

 Google "weighting in meta-analysis" or similar for ways folks try to deal
with this.

Cheers,

Bert

On Tuesday, February 16, 2016, Wen Huang <whuang.ustc at gmail.com> wrote:

> Hi Harold,
> R
> Thank you for your input. I was not very clear. I wanted to compare the
> sigma2_A?s from the same model fitted to two different data sets. The same
> for sigma2_e?s. The motivation is when I did the same experiment at two
> different times, whether the variance due to A (sigma2_A) is bigger at one
> time versus another. The same for sigma2_e, whether the residual variance
> is bigger for one experiment versus another.
>
> Thanks,
> Wen
>
> > On Feb 16, 2016, at 12:40 PM, Doran, Harold <HDoran at air.org
> <javascript:;>> wrote:
> >
> > (adding R mixed group). You actually do not want to do this test, and
> there is no "shrinkage" here on these variances. First, there are
> conditional variances and marginal variances in the mixed model. What you
> are have below as "A" is the marginal variances of the random effects and
> there is no shrinkage on these, per se.
> >
> > The conditional means of the random effects have shrinkage and each
> conditional mean (or BLUP) has a conditional variance.
> >
> > Now, it seems very odd to want to compare the variance between A and
> then what you have as sigma2_e, which is presumably the residual variance.
> These are variances of two completely different things, so a test comparing
> them seems strange, though I suppose some theoretical reason could exists
> justifying it, I cannot imagine one though.
> >
> >
> >
> >
> >
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org <javascript:;>] On
> Behalf Of Wen Huang
> > Sent: Tuesday, February 16, 2016 10:57 AM
> > To: r-help at r-project.org <javascript:;>
> > Subject: [R] Comparing variance components
> >
> > Dear R-help members,
> >
> > Say I have two data sets collected at different times with the same
> design. I fit a mixed model using in R using lmer
> >
> > lmer(y ~ (1|A))
> >
> > to these data sets and get two estimates of sigma2_A and sigma2_e
> >
> > What would be a good way to compare sigma2_A and sigma2_e for these two
> data sets and obtain a P value for the hypothesis that sigma2_A1 =
> sigma2_A2? There is obvious shrinkage on these estimates, should I be
> worried about the differential levels of shrinkage on these estimates and
> how to account for that?
> >
> > Thank you for your thoughts and inputs!
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

	[[alternative HTML version deleted]]


From chuath.ums at gmail.com  Mon Feb 15 22:30:33 2016
From: chuath.ums at gmail.com (Tock Chua)
Date: Tue, 16 Feb 2016 05:30:33 +0800
Subject: [R-sig-ME] glmmADMB
Message-ID: <CALNF8VVZD9_5_KsCQs8_s7VG0aL3_7hfQ5TaEM_82cuxLzmwvQ@mail.gmail.com>

Dear Prof Bolker

Yes, I did run (but did not include in the message I sent earlier):

> library("glmmADMB")
> bb <- glmmADMB:::get_bin_loc()[["bin_loc"]]
Error in get(name, envir = asNamespace(pkg), inherits = FALSE) :
  object 'get_bin_loc' not found

Thank you for your help

ChuaTH

	[[alternative HTML version deleted]]


From jenyourkavitch at yahoo.com  Mon Feb 15 21:11:47 2016
From: jenyourkavitch at yahoo.com (Jennifer Yourkavitch)
Date: Mon, 15 Feb 2016 15:11:47 -0500
Subject: [R-sig-ME] new R user struggling with error and convergence
	issues
In-Reply-To: <61816917-B0DD-46BC-9FF7-5D5A615BE599@yahoo.com>
References: <61816917-B0DD-46BC-9FF7-5D5A615BE599@yahoo.com>
Message-ID: <27287EAC-87E0-4E3E-9815-0627C891CB4E@yahoo.com>

Follow-up: glmmPQL works like a charm. I don?t really understand the difference between the algorithms used by glmmPQL and glmer. Should I be concerned that the models ran with glmmPQL but not glmer?
Thanks,
Jennifer


On Feb 15, 2016, at 2:22 PM, Jennifer Yourkavitch <jenyourkavitch at yahoo.com> wrote:

> Hello!
> I am fitting multilevel log-binomial models for a very large dataset (>53,000 observations), using glmer. I am getting this--  Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate.
> And the model is not converging. 
> I think that increasing the number of iterations may remediate the convergence issue. But I can?t find the right syntax for that online.
> Any advice re these two issues?
> Many thanks,
> Jennifer
> 


From whuang.ustc at gmail.com  Tue Feb 16 18:54:08 2016
From: whuang.ustc at gmail.com (Wen Huang)
Date: Tue, 16 Feb 2016 12:54:08 -0500
Subject: [R-sig-ME] [R] Comparing variance components
In-Reply-To: <B08B6AF0CF8CA44F81B9983EEBDCD686012BB58ADB@DC1VEX10MB01.air.org>
References: <477CE007-DA6D-4031-B86F-FCAB156578EB@gmail.com>
	<B08B6AF0CF8CA44F81B9983EEBDCD686012BB58ADB@DC1VEX10MB01.air.org>
Message-ID: <1B7ADF10-E2B4-4633-B790-32698DCFA5BF@gmail.com>

Hi Harold,

Thank you for your input. I was not very clear. I wanted to compare the sigma2_A?s from the same model fitted to two different data sets. The same for sigma2_e?s. The motivation is when I did the same experiment at two different times, whether the variance due to A (sigma2_A) is bigger at one time versus another. The same for sigma2_e, whether the residual variance is bigger for one experiment versus another.

Thanks,
Wen

> On Feb 16, 2016, at 12:40 PM, Doran, Harold <HDoran at air.org> wrote:
> 
> (adding R mixed group). You actually do not want to do this test, and there is no "shrinkage" here on these variances. First, there are conditional variances and marginal variances in the mixed model. What you are have below as "A" is the marginal variances of the random effects and there is no shrinkage on these, per se.
> 
> The conditional means of the random effects have shrinkage and each conditional mean (or BLUP) has a conditional variance. 
> 
> Now, it seems very odd to want to compare the variance between A and then what you have as sigma2_e, which is presumably the residual variance. These are variances of two completely different things, so a test comparing them seems strange, though I suppose some theoretical reason could exists justifying it, I cannot imagine one though. 
> 
> 
> 
> 
> 
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wen Huang
> Sent: Tuesday, February 16, 2016 10:57 AM
> To: r-help at r-project.org
> Subject: [R] Comparing variance components
> 
> Dear R-help members,
> 
> Say I have two data sets collected at different times with the same design. I fit a mixed model using in R using lmer
> 
> lmer(y ~ (1|A))
> 
> to these data sets and get two estimates of sigma2_A and sigma2_e
> 
> What would be a good way to compare sigma2_A and sigma2_e for these two data sets and obtain a P value for the hypothesis that sigma2_A1 = sigma2_A2? There is obvious shrinkage on these estimates, should I be worried about the differential levels of shrinkage on these estimates and how to account for that?
> 
> Thank you for your thoughts and inputs!
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From whuang.ustc at gmail.com  Tue Feb 16 21:28:31 2016
From: whuang.ustc at gmail.com (Wen Huang)
Date: Tue, 16 Feb 2016 15:28:31 -0500
Subject: [R-sig-ME] [R] Comparing variance components
In-Reply-To: <9B75E7CF385CB94EAD6587DD96AC2D9701D16E4B2E@SFSMCEXMBX3.sanfordhealth.org>
References: <477CE007-DA6D-4031-B86F-FCAB156578EB@gmail.com>
	<B08B6AF0CF8CA44F81B9983EEBDCD686012BB58ADB@DC1VEX10MB01.air.org>
	<7516BB49-2EF2-4F5D-9418-B507B4809DF9@gmail.com>
	<9B75E7CF385CB94EAD6587DD96AC2D9701D16E4B2E@SFSMCEXMBX3.sanfordhealth.org>
Message-ID: <DF8B53E0-12D1-451D-B1D8-1995802FFE94@gmail.com>

Hi Paul,

Thank you. That is a neat idea. How would you implement that? Could you write an example code on how the model should be fitted? Sorry for my ignorance.

Thanks,
Wen

> On Feb 16, 2016, at 1:18 PM, Thompson,Paul <Paul.Thompson at SanfordHealth.org> wrote:
> 
> Are you computing two estimates of reliability and wishing to compare them? One possible method is to set both into the same design, treat the design effect (Exp 1, Exp 2) as a fixed effect, and compare them with a standard F test. 
> 
> -----Original Message-----
> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Wen Huang
> Sent: Tuesday, February 16, 2016 11:57 AM
> To: Doran, Harold
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] [R] Comparing variance components
> 
> Hi Harold,
> 
> Thank you for your input. I was not very clear. I wanted to compare the sigma2_A?s from the same model fitted to two different data sets. The same for sigma2_e?s. The motivation is when I did the same experiment at two different times, whether the variance due to A (sigma2_A) is bigger at one time versus another. The same for sigma2_e, whether the residual variance is bigger for one experiment versus another.
> 
> Thanks,
> Wen
> 
>> On Feb 16, 2016, at 12:40 PM, Doran, Harold <HDoran at air.org> wrote:
>> 
>> (adding R mixed group). You actually do not want to do this test, and there is no "shrinkage" here on these variances. First, there are conditional variances and marginal variances in the mixed model. What you are have below as "A" is the marginal variances of the random effects and there is no shrinkage on these, per se.
>> 
>> The conditional means of the random effects have shrinkage and each conditional mean (or BLUP) has a conditional variance. 
>> 
>> Now, it seems very odd to want to compare the variance between A and then what you have as sigma2_e, which is presumably the residual variance. These are variances of two completely different things, so a test comparing them seems strange, though I suppose some theoretical reason could exists justifying it, I cannot imagine one though. 
>> 
>> 
>> 
>> 
>> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wen 
>> Huang
>> Sent: Tuesday, February 16, 2016 10:57 AM
>> To: r-help at r-project.org
>> Subject: [R] Comparing variance components
>> 
>> Dear R-help members,
>> 
>> Say I have two data sets collected at different times with the same 
>> design. I fit a mixed model using in R using lmer
>> 
>> lmer(y ~ (1|A))
>> 
>> to these data sets and get two estimates of sigma2_A and sigma2_e
>> 
>> What would be a good way to compare sigma2_A and sigma2_e for these two data sets and obtain a P value for the hypothesis that sigma2_A1 = sigma2_A2? There is obvious shrinkage on these estimates, should I be worried about the differential levels of shrinkage on these estimates and how to account for that?
>> 
>> Thank you for your thoughts and inputs!
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> -----------------------------------------------------------------------
> Confidentiality Notice: This e-mail message, including any attachments,
> is for the sole use of the intended recipient(s) and may contain
> privileged and confidential information.  Any unauthorized review, use,
> disclosure or distribution is prohibited.  If you are not the intended
> recipient, please contact the sender by reply e-mail and destroy
> all copies of the original message.


From j.hadfield at ed.ac.uk  Tue Feb 16 21:59:33 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 16 Feb 2016 20:59:33 +0000
Subject: [R-sig-ME] [R] Comparing variance components
In-Reply-To: <DF8B53E0-12D1-451D-B1D8-1995802FFE94@gmail.com>
References: <477CE007-DA6D-4031-B86F-FCAB156578EB@gmail.com>
	<B08B6AF0CF8CA44F81B9983EEBDCD686012BB58ADB@DC1VEX10MB01.air.org>
	<7516BB49-2EF2-4F5D-9418-B507B4809DF9@gmail.com>
	<9B75E7CF385CB94EAD6587DD96AC2D9701D16E4B2E@SFSMCEXMBX3.sanfordhealth.org>
	<DF8B53E0-12D1-451D-B1D8-1995802FFE94@gmail.com>
Message-ID: <56C38DB5.2010709@ed.ac.uk>

Hi Wen,

The question sounds sensible to me, but you can't do what you want to do 
in lmer because it does not allow heterogenous variances for the 
residuals. You can do it in nlme:

model.lme.a<- lme(y~Exp, random=~1|G,  data=my_data)
model.lme.b<- lme(y~Exp, random=~0+Exp|G, 
weights=varIdent(form=~1|Exp),  data=my_data)

or MCMCglmm (or asreml if you have it):

model.mcmc.a<- MCMCglmm(y~Exp, random=~G,  data=my_data)
model.mcmc.b<- MCMCglmm(y~Exp, random=~idh(Exp):G, rcov=~idh(Exp):units, 
data=my_data)

The first model assumes common variances for each experiment, the second 
allows the variances to differ. You can comapre model.lme.a and 
model.lme.b using a likelihood ratio test (2 parameters) or you can 
compare the posterior distributions in the Bayesian model.

Note that this assumes that the levels of the random effect differ in 
the two epxeriments (and they have been given separate lables). If there 
is overlap then an additional assumption of model.a is that the random 
effects have a correlation of 1 between the two experiments when they 
are associated with the same factor level.

Cheers,

Jarrod



On 16/02/2016 20:28, Wen Huang wrote:
> Hi Paul,
>
> Thank you. That is a neat idea. How would you implement that? Could you write an example code on how the model should be fitted? Sorry for my ignorance.
>
> Thanks,
> Wen
>
>> On Feb 16, 2016, at 1:18 PM, Thompson,Paul <Paul.Thompson at SanfordHealth.org> wrote:
>>
>> Are you computing two estimates of reliability and wishing to compare them? One possible method is to set both into the same design, treat the design effect (Exp 1, Exp 2) as a fixed effect, and compare them with a standard F test.
>>
>> -----Original Message-----
>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Wen Huang
>> Sent: Tuesday, February 16, 2016 11:57 AM
>> To: Doran, Harold
>> Cc: r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] [R] Comparing variance components
>>
>> Hi Harold,
>>
>> Thank you for your input. I was not very clear. I wanted to compare the sigma2_A?s from the same model fitted to two different data sets. The same for sigma2_e?s. The motivation is when I did the same experiment at two different times, whether the variance due to A (sigma2_A) is bigger at one time versus another. The same for sigma2_e, whether the residual variance is bigger for one experiment versus another.
>>
>> Thanks,
>> Wen
>>
>>> On Feb 16, 2016, at 12:40 PM, Doran, Harold <HDoran at air.org> wrote:
>>>
>>> (adding R mixed group). You actually do not want to do this test, and there is no "shrinkage" here on these variances. First, there are conditional variances and marginal variances in the mixed model. What you are have below as "A" is the marginal variances of the random effects and there is no shrinkage on these, per se.
>>>
>>> The conditional means of the random effects have shrinkage and each conditional mean (or BLUP) has a conditional variance.
>>>
>>> Now, it seems very odd to want to compare the variance between A and then what you have as sigma2_e, which is presumably the residual variance. These are variances of two completely different things, so a test comparing them seems strange, though I suppose some theoretical reason could exists justifying it, I cannot imagine one though.
>>>
>>>
>>>
>>>
>>>
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wen
>>> Huang
>>> Sent: Tuesday, February 16, 2016 10:57 AM
>>> To: r-help at r-project.org
>>> Subject: [R] Comparing variance components
>>>
>>> Dear R-help members,
>>>
>>> Say I have two data sets collected at different times with the same
>>> design. I fit a mixed model using in R using lmer
>>>
>>> lmer(y ~ (1|A))
>>>
>>> to these data sets and get two estimates of sigma2_A and sigma2_e
>>>
>>> What would be a good way to compare sigma2_A and sigma2_e for these two data sets and obtain a P value for the hypothesis that sigma2_A1 = sigma2_A2? There is obvious shrinkage on these estimates, should I be worried about the differential levels of shrinkage on these estimates and how to account for that?
>>>
>>> Thank you for your thoughts and inputs!
>>>
>>>
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> -----------------------------------------------------------------------
>> Confidentiality Notice: This e-mail message, including any attachments,
>> is for the sole use of the intended recipient(s) and may contain
>> privileged and confidential information.  Any unauthorized review, use,
>> disclosure or distribution is prohibited.  If you are not the intended
>> recipient, please contact the sender by reply e-mail and destroy
>> all copies of the original message.
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From peggy_moore at usgs.gov  Tue Feb 16 22:28:47 2016
From: peggy_moore at usgs.gov (Moore, Peggy)
Date: Tue, 16 Feb 2016 13:28:47 -0800
Subject: [R-sig-ME] glmmADMB error: Invalid index for array
Message-ID: <CAPO7Dkc+6S2xKpgrXm+b0sQdK2BT5+Ht7rtQUHjCn5XVbYOYJw@mail.gmail.com>

Ben,
As a follow-up to a response I sent earlier to day, I wanted to point out
that our statistician tried fitting the same model to the Owls data with
glmmADMB and is seeing the same pattern. We both tried model fits that were
really pared down, but received the same errors as long as mcmc=TRUE. After
it performed the Newton-Raphson steps when run from the cmd prompt, it
produced the error of an 'invalid index for array.' With two different data
sets and three different models, the error always indicates, if I'm reading
it correctly, that the index is one value greater than appropriate for the
array (i.e., 9 if array is [1, 8] or 19 if array is [1, 18].
Our statistician is not co-located with me - she is 150 miles away - but we
are both on US Geological Survey machines, so we could have settings in
common. I see no trouble with writing files to the save.dir location,
however, such as the .dat, .pin, .std, and .exe files and 40 more. Is there
anything else you can suggest?
Peggy

Peggy Moore
U.S. Geological Survey
Western Ecological Research Center
Yosemite Field Station - El Portal Office
5083 Foresta Rd, El Portal, CA 95318
peggy_moore at usgs.gov
209.379.1309 (Mon, Tue, Fri)
209.617.7775 (Wed, Thur)
www.werc.usgs.gov/yosemite

	[[alternative HTML version deleted]]


From dmcastil at umail.iu.edu  Wed Feb 17 17:15:12 2016
From: dmcastil at umail.iu.edu (Dean Castillo)
Date: Wed, 17 Feb 2016 11:15:12 -0500
Subject: [R-sig-ME] [R] Comparing variance components
Message-ID: <CAMmHrnxQ60irKAHJ13SURAdmZd=+N=cNvFzFBqbowmW0agCVJg@mail.gmail.com>

Hi Jarrod,

I have been trying to test a similar hypothesis for a while with only
limited success, so I wanted to thank you for your answer to Wen's
question. I did have a few additional questions of clarification, some
specific to my own data analysis.

For model.mcmc.a I think it is pretty straightforward to compare the
posterior distributions. For model.mcmc.b, now that you explicitly modeled
heterogenous variances for the residuals is it more informative to examine
the IC correlation for G for each Exp rather than the variances themselves?

For my specific problem I have binomial data. I have been modeling the data
as proportions (bounded by 0-1 but can logit or arcsinsqrt transform) as
well as modeling it using "multinomial2" on the raw data.

The issue I am running into is that the variance of G for one of the
experimental blocks is very close to the boundary condition, while the
other is larger, when modeled as a proportion, and the HPD are very wide. I
have been using inv-gamma priors and will play around with the parameter
expanded priors as suggested in your course notes.

For the multinomial2 model should the priors for the variance be the same?
The posterior means are not as close to the boundary condition (Exp1:G=0.8,
Exp2:G=0.3) but the HPD are still very wide (1e-17,1.13) for Exp2:G.

Any help is greatly appreciated

Dean



> Date: Tue, 16 Feb 2016 20:59:33 +0000
> From: Jarrod Hadfield <j.hadfield at ed.ac.uk>
> To: Wen Huang <whuang.ustc at gmail.com>,  "Thompson,Paul"
>         <Paul.Thompson at SanfordHealth.org>
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] [R] Comparing variance components
> Message-ID: <56C38DB5.2010709 at ed.ac.uk>
> Content-Type: text/plain; charset=utf-8; format=flowed
>
> Hi Wen,
>
> The question sounds sensible to me, but you can't do what you want to do
> in lmer because it does not allow heterogenous variances for the
> residuals. You can do it in nlme:
>
> model.lme.a<- lme(y~Exp, random=~1|G,  data=my_data)
> model.lme.b<- lme(y~Exp, random=~0+Exp|G,
> weights=varIdent(form=~1|Exp),  data=my_data)
>
> or MCMCglmm (or asreml if you have it):
>
> model.mcmc.a<- MCMCglmm(y~Exp, random=~G,  data=my_data)
> model.mcmc.b<- MCMCglmm(y~Exp, random=~idh(Exp):G, rcov=~idh(Exp):units,
> data=my_data)
>
> The first model assumes common variances for each experiment, the second
> allows the variances to differ. You can comapre model.lme.a and
> model.lme.b using a likelihood ratio test (2 parameters) or you can
> compare the posterior distributions in the Bayesian model.
>
> Note that this assumes that the levels of the random effect differ in
> the two epxeriments (and they have been given separate lables). If there
> is overlap then an additional assumption of model.a is that the random
> effects have a correlation of 1 between the two experiments when they
> are associated with the same factor level.
>
> Cheers,
>
> Jarrod
>
>
>
> On 16/02/2016 20:28, Wen Huang wrote:
> > Hi Paul,
> >
> > Thank you. That is a neat idea. How would you implement that? Could you
> write an example code on how the model should be fitted? Sorry for my
> ignorance.
> >
> > Thanks,
> > Wen
> >
> >> On Feb 16, 2016, at 1:18 PM, Thompson,Paul
> <Paul.Thompson at SanfordHealth.org> wrote:
> >>
> >> Are you computing two estimates of reliability and wishing to compare
> them? One possible method is to set both into the same design, treat the
> design effect (Exp 1, Exp 2) as a fixed effect, and compare them with a
> standard F test.
> >>
> >> -----Original Message-----
> >> From: R-sig-mixed-models [mailto:
> r-sig-mixed-models-bounces at r-project.org] On Behalf Of Wen Huang
> >> Sent: Tuesday, February 16, 2016 11:57 AM
> >> To: Doran, Harold
> >> Cc: r-sig-mixed-models at r-project.org
> >> Subject: Re: [R-sig-ME] [R] Comparing variance components
> >>
> >> Hi Harold,
> >>
> >> Thank you for your input. I was not very clear. I wanted to compare the
> sigma2_A?s from the same model fitted to two different data sets. The same
> for sigma2_e?s. The motivation is when I did the same experiment at two
> different times, whether the variance due to A (sigma2_A) is bigger at one
> time versus another. The same for sigma2_e, whether the residual variance
> is bigger for one experiment versus another.
> >>
> >> Thanks,
> >> Wen
> >>
> >>> On Feb 16, 2016, at 12:40 PM, Doran, Harold <HDoran at air.org> wrote:
> >>>
> >>> (adding R mixed group). You actually do not want to do this test, and
> there is no "shrinkage" here on these variances. First, there are
> conditional variances and marginal variances in the mixed model. What you
> are have below as "A" is the marginal variances of the random effects and
> there is no shrinkage on these, per se.
> >>>
> >>> The conditional means of the random effects have shrinkage and each
> conditional mean (or BLUP) has a conditional variance.
> >>>
> >>> Now, it seems very odd to want to compare the variance between A and
> then what you have as sigma2_e, which is presumably the residual variance.
> These are variances of two completely different things, so a test comparing
> them seems strange, though I suppose some theoretical reason could exists
> justifying it, I cannot imagine one though.
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> -----Original Message-----
> >>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wen
> >>> Huang
> >>> Sent: Tuesday, February 16, 2016 10:57 AM
> >>> To: r-help at r-project.org
> >>> Subject: [R] Comparing variance components
> >>>
> >>> Dear R-help members,
> >>>
> >>> Say I have two data sets collected at different times with the same
> >>> design. I fit a mixed model using in R using lmer
> >>>
> >>> lmer(y ~ (1|A))
> >>>
> >>> to these data sets and get two estimates of sigma2_A and sigma2_e
> >>>
> >>> What would be a good way to compare sigma2_A and sigma2_e for these
> two data sets and obtain a P value for the hypothesis that sigma2_A1 =
> sigma2_A2? There is obvious shrinkage on these estimates, should I be
> worried about the differential levels of shrinkage on these estimates and
> how to account for that?
> >>>
> >>> Thank you for your thoughts and inputs!
> >>>
> >>>
> >>>
> >>>     [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> -----------------------------------------------------------------------
> >> Confidentiality Notice: This e-mail message, including any attachments,
> >> is for the sole use of the intended recipient(s) and may contain
> >> privileged and confidential information.  Any unauthorized review, use,
> >> disclosure or distribution is prohibited.  If you are not the intended
> >> recipient, please contact the sender by reply e-mail and destroy
> >> all copies of the original message.
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>

	[[alternative HTML version deleted]]


From sandra.hamel at uit.no  Wed Feb 17 10:32:05 2016
From: sandra.hamel at uit.no (Sandra Hamel)
Date: Wed, 17 Feb 2016 10:32:05 +0100
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 110, Issue 16
In-Reply-To: <mailman.1109.1455658147.3865.r-sig-mixed-models@r-project.org>
References: <mailman.1109.1455658147.3865.r-sig-mixed-models@r-project.org>
Message-ID: <56C43E15.4080201@uit.no>

Hi Wen,

Another potential option is to use the package "flexmix" which fits 
mixture of mixed models and allows variance to be heterogeneous. For 
example, fitting a linear mixed model with 2 clusters (k=2) should allow 
estimating sigma for each experiment:

flexmix(y~x|Exp, model=FLXMRlmm(.~., random=~G), data=my_data, k=2, nrep=10)

And you can use the bootstrap function in flexmix to evaluate whether 
these two sigma values differ or not between the two clusters, see this 
pdf for details on how to do this: 
https://cran.r-project.org/web/packages/flexmix/vignettes/bootstrapping.pdf

Sandra

On 2016-02-16 22:29, r-sig-mixed-models-request at r-project.org wrote:
>
> Date: Tue, 16 Feb 2016 20:59:33 +0000
> From: Jarrod Hadfield <j.hadfield at ed.ac.uk>
> To: Wen Huang <whuang.ustc at gmail.com>,	"Thompson,Paul"
> 	<Paul.Thompson at SanfordHealth.org>
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] [R] Comparing variance components
> Message-ID: <56C38DB5.2010709 at ed.ac.uk>
> Content-Type: text/plain; charset=utf-8; format=flowed
>
> Hi Wen,
>
> The question sounds sensible to me, but you can't do what you want to do
> in lmer because it does not allow heterogenous variances for the
> residuals. You can do it in nlme:
>
> model.lme.a<- lme(y~Exp, random=~1|G,  data=my_data)
> model.lme.b<- lme(y~Exp, random=~0+Exp|G,
> weights=varIdent(form=~1|Exp),  data=my_data)
>
> or MCMCglmm (or asreml if you have it):
>
> model.mcmc.a<- MCMCglmm(y~Exp, random=~G,  data=my_data)
> model.mcmc.b<- MCMCglmm(y~Exp, random=~idh(Exp):G, rcov=~idh(Exp):units,
> data=my_data)
>
> The first model assumes common variances for each experiment, the second
> allows the variances to differ. You can comapre model.lme.a and
> model.lme.b using a likelihood ratio test (2 parameters) or you can
> compare the posterior distributions in the Bayesian model.
>
> Note that this assumes that the levels of the random effect differ in
> the two epxeriments (and they have been given separate lables). If there
> is overlap then an additional assumption of model.a is that the random
> effects have a correlation of 1 between the two experiments when they
> are associated with the same factor level.
>
> Cheers,
>
> Jarrod
>
>
>
> On 16/02/2016 20:28, Wen Huang wrote:
>> Hi Paul,
>>
>> Thank you. That is a neat idea. How would you implement that? Could you write an example code on how the model should be fitted? Sorry for my ignorance.
>>
>> Thanks,
>> Wen
>>
>>> On Feb 16, 2016, at 1:18 PM, Thompson,Paul <Paul.Thompson at SanfordHealth.org> wrote:
>>>
>>> Are you computing two estimates of reliability and wishing to compare them? One possible method is to set both into the same design, treat the design effect (Exp 1, Exp 2) as a fixed effect, and compare them with a standard F test.
>>>
>>> -----Original Message-----
>>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Wen Huang
>>> Sent: Tuesday, February 16, 2016 11:57 AM
>>> To: Doran, Harold
>>> Cc: r-sig-mixed-models at r-project.org
>>> Subject: Re: [R-sig-ME] [R] Comparing variance components
>>>
>>> Hi Harold,
>>>
>>> Thank you for your input. I was not very clear. I wanted to compare the sigma2_A?s from the same model fitted to two different data sets. The same for sigma2_e?s. The motivation is when I did the same experiment at two different times, whether the variance due to A (sigma2_A) is bigger at one time versus another. The same for sigma2_e, whether the residual variance is bigger for one experiment versus another.
>>>
>>> Thanks,
>>> Wen
>>>
>>>> On Feb 16, 2016, at 12:40 PM, Doran, Harold <HDoran at air.org> wrote:
>>>>
>>>> (adding R mixed group). You actually do not want to do this test, and there is no "shrinkage" here on these variances. First, there are conditional variances and marginal variances in the mixed model. What you are have below as "A" is the marginal variances of the random effects and there is no shrinkage on these, per se.
>>>>
>>>> The conditional means of the random effects have shrinkage and each conditional mean (or BLUP) has a conditional variance.
>>>>
>>>> Now, it seems very odd to want to compare the variance between A and then what you have as sigma2_e, which is presumably the residual variance. These are variances of two completely different things, so a test comparing them seems strange, though I suppose some theoretical reason could exists justifying it, I cannot imagine one though.
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> -----Original Message-----
>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wen
>>>> Huang
>>>> Sent: Tuesday, February 16, 2016 10:57 AM
>>>> To: r-help at r-project.org
>>>> Subject: [R] Comparing variance components
>>>>
>>>> Dear R-help members,
>>>>
>>>> Say I have two data sets collected at different times with the same
>>>> design. I fit a mixed model using in R using lmer
>>>>
>>>> lmer(y ~ (1|A))
>>>>
>>>> to these data sets and get two estimates of sigma2_A and sigma2_e
>>>>
>>>> What would be a good way to compare sigma2_A and sigma2_e for these two data sets and obtain a P value for the hypothesis that sigma2_A1 = sigma2_A2? There is obvious shrinkage on these estimates, should I be worried about the differential levels of shrinkage on these estimates and how to account for that?
>>>>
>>>> Thank you for your thoughts and inputs!
>>>>
>>>>
>>>>
>>>> 	[[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> -----------------------------------------------------------------------
>>> Confidentiality Notice: This e-mail message, including any attachments,
>>> is for the sole use of the intended recipient(s) and may contain
>>> privileged and confidential information.  Any unauthorized review, use,
>>> disclosure or distribution is prohibited.  If you are not the intended
>>> recipient, please contact the sender by reply e-mail and destroy
>>> all copies of the original message.
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From jenyourkavitch at yahoo.com  Wed Feb 17 14:35:03 2016
From: jenyourkavitch at yahoo.com (Jennifer Yourkavitch)
Date: Wed, 17 Feb 2016 08:35:03 -0500
Subject: [R-sig-ME] new R user struggling with error and convergence
	issues
In-Reply-To: <CABghstTNReOwtNzobDcZMHzMxq_-FmxYf4Ub8Ak1HVxuUYPJXw@mail.gmail.com>
References: <61816917-B0DD-46BC-9FF7-5D5A615BE599@yahoo.com>
	<CABghstTNReOwtNzobDcZMHzMxq_-FmxYf4Ub8Ak1HVxuUYPJXw@mail.gmail.com>
Message-ID: <CD305FEE-55F0-4F03-AE3A-8C423DB0ED7D@yahoo.com>

Thank you!
What do you think about using glmmPQL? The model converged.
Jennifer

On Feb 15, 2016, at 3:21 PM, Ben Bolker <bbolker at gmail.com> wrote:

> It is admittedly hard to increase the "maxstephalfit" parameter, but
> part of the reason we haven't put much effort into making it easier is
> that in our experience it rarely helps.  Log-binomial models in
> particular are tricky because the log link (i.e. the exponential
> inverse-link) doesn't naturally constrain the response to the allowed
> range of (0,1), so these models are hard to fit (the same applies to
> inverse-link Gamma models).  The bigger your data set, the more likely
> you are to run into cases where the predicted value is >1.  We could
> admittedly do a much better job giving useful warnings about when and
> where this problem occurred, and (more controversially) allow users to
> clamp the output of the inverse-link function to the allowable domain
> ...
> 
> library("lme4")
> set.seed(101)
> dd <- data.frame(x=seq(-4,4,length=500),
>                 f=sample(letters[1:10],size=500,replace=TRUE))
> dd$y <- simulate(~x+(1|f),family=binomial,
>                 newdata=dd,newparams=list(beta=c(0,1),theta=1))[[1]]
> library("ggplot2")
> ggplot(dd,aes(x,y,colour=f))+geom_point()+
>    geom_smooth(method="glm",method.args=list(family=binomial))
> 
> m1 <- glmer(y~x+(1|f), dd, family=binomial)
> m2 <- glmer(y~x+(1|f), dd, family=binomial(link="log"))  ## PIRLS
> step-halving problem
> 
> On Mon, Feb 15, 2016 at 2:22 PM, Jennifer Yourkavitch via
> R-sig-mixed-models <r-sig-mixed-models at r-project.org> wrote:
>> Hello!
>> I am fitting multilevel log-binomial models for a very large dataset (>53,000 observations), using glmer. I am getting this--  Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate.
>> And the model is not converging.
>> I think that increasing the number of iterations may remediate the convergence issue. But I can?t find the right syntax for that online.
>> Any advice re these two issues?
>> Many thanks,
>> Jennifer
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From jbukoski1 at gmail.com  Wed Feb 17 21:27:33 2016
From: jbukoski1 at gmail.com (Jacob Bukoski)
Date: Wed, 17 Feb 2016 15:27:33 -0500
Subject: [R-sig-ME] "Non-positive definite approximate variance-covariance"
	output
Message-ID: <CAOES0VhrWFysqUs7RXt2x0v61hHr5=t7ua8=R_FNpDzFZ5KDQQ@mail.gmail.com>

Hi all,

I was in a discussion with my advisor today, and the issue of "non-positive
definite approximate variance-covariance" errors in R when using the lme()
function arose.

Depending on the varFunc specification that I use, I receive the
"non-positive definite approximate variance-covariance" output when calling
myModel$apVar -- however, there are standard errors returned for my model
coefficients in the model summary.

My understanding is that if the variance-covariance cannot be approximated,
neither should the standard errors... how is the lme() function returning
standard errors without a variance-covariance approximation?

Many thanks,
Jacob

-- 
Jacob J. Bukoski
Master of Environmental Science Candidate, 2016
School of Forestry and Environmental Studies, Yale University
jbukoski1 at gmail.com | jacob.bukoski at yale.edu | LinkedIn
<https://www.linkedin.com/profile/view?id=AAIAAAdWVW8BMzqU_2EGNbEkyuy8O7K1Jyhd8ps&trk=nav_responsive_tab_profile_pic>

	[[alternative HTML version deleted]]


From bates at stat.wisc.edu  Wed Feb 17 21:43:56 2016
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 17 Feb 2016 20:43:56 +0000
Subject: [R-sig-ME] "Non-positive definite approximate
 variance-covariance" output
In-Reply-To: <CAOES0VhrWFysqUs7RXt2x0v61hHr5=t7ua8=R_FNpDzFZ5KDQQ@mail.gmail.com>
References: <CAOES0VhrWFysqUs7RXt2x0v61hHr5=t7ua8=R_FNpDzFZ5KDQQ@mail.gmail.com>
Message-ID: <CAO7JsnR=mus-9y8KoSmwh6cZTjMjZOOZB_fodj2sTKEPaXo3Cw@mail.gmail.com>

The standard errors are for the fixed-effects parameters.  The apVar
component includes approximate variances of the estimators of (functions
of) the variance components, which is where the positive-definiteness
failure may be occurring..

On Wed, Feb 17, 2016 at 2:28 PM Jacob Bukoski <jbukoski1 at gmail.com> wrote:

> Hi all,
>
> I was in a discussion with my advisor today, and the issue of "non-positive
> definite approximate variance-covariance" errors in R when using the lme()
> function arose.
>
> Depending on the varFunc specification that I use, I receive the
> "non-positive definite approximate variance-covariance" output when calling
> myModel$apVar -- however, there are standard errors returned for my model
> coefficients in the model summary.
>
> My understanding is that if the variance-covariance cannot be approximated,
> neither should the standard errors... how is the lme() function returning
> standard errors without a variance-covariance approximation?
>
> Many thanks,
> Jacob
>
> --
> Jacob J. Bukoski
> Master of Environmental Science Candidate, 2016
> School of Forestry and Environmental Studies, Yale University
> jbukoski1 at gmail.com | jacob.bukoski at yale.edu | LinkedIn
> <
> https://www.linkedin.com/profile/view?id=AAIAAAdWVW8BMzqU_2EGNbEkyuy8O7K1Jyhd8ps&trk=nav_responsive_tab_profile_pic
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From nirvana4lf at gmail.com  Wed Feb 17 23:26:16 2016
From: nirvana4lf at gmail.com (Sam H)
Date: Wed, 17 Feb 2016 14:26:16 -0800
Subject: [R-sig-ME] Specifying a model with a link function in MCMCglmm
Message-ID: <CAE2_W_kUYmv8Oo5SU2qBsZAmbw=Ry98hh1_63cMW50FCJy3gcQ@mail.gmail.com>

Hello all,


Before jumping into my question, let me first briefly explain my model to
give context. Here is how I currently have specified the model using
MCMCglmm, first specifying a model in lmer and extracting the variance
estimates for my G prior:

full_lmer <- lmer(RT ~ AgeGroup*cue*cong + (cue + cong|PID), data =
vsdataset)

sigma2 <- sigma(full_lmer)^2

Lambda <- getME(full_lmer, "Lambda")

Sigma <- sigma2*tcrossprod(Lambda)

Gmer <- Sigma[1:4,1:4] #Extracting the VCV parameters from the block
diagonalized Sigma

full_mcmc <- MCMCglmm(RT ~ AgeGroup*cue*cong, random = ~us(1 + cue +
cong):PID, thin = 10, nitt = 20000, data = vsdataset, prior = list(G =
list(G1 = list(V = diag(diag(Gmer)), nu = 5))))

Note: I used V = diag(diag(Gmer)) due to the fact that Sigma/Gmer was not
positive definite, which MCMCglmm would not accept.


Quick explanation of model terms:

RT --> response time in msec, very positively skewed even after outlier
removal, inverse transform seems to center it

AgeGroup --> 2 level factorial var (Old and Young, between-subjects)

cue --> 3 level factorial var (within-subjects)

cong --> 2 level factorial var (within-subjects)

PID --> participant ID number


I want to specify this model with an inverse/reciprocal link function
(Gaussian family). However, I can't figure out how to specify the link
function. In the help section for the MCMCglmm function, they mention a
"linking.function" for the random effects terms, but it doesn't seem to
have anything to with specifying a link function for the response variable.
According to the course notes from the MCMCglmm package, "there are many
different types of distribution and link functions and those supported by
MCMCglmm can be found in Table 7.1." However, Table 7.1 seems to just list
the families and their PDFs, there's no column listing "supported" link
functions.

So, how do you specify a link function using MCMCglmm? If you can't
directly specify a link function, is there something else I need to do such
as specifying the prior a certain way, or is it valid to just specify the
model as having a gaussian response and leave the mode as I've specified
it? After plotting the model, I noticed that several of the parameter
distributions were extremely skewed (some left, some right).


As a side note, I originally tried two alternatives:

1) using lmer with an inverse transform

2) using glmer with family = gaussian(link = "inverse") and family =
inverse.gaussian(link = "identity")


#1 seems problematic due to the fact that I need to convert the response
variable units back to the original units, which not only flips any
confidence intervals but also makes them uneven. I'm not sure if converting
these CI's is even appropriate as they were computed with different
units/distribution. I also don't know of any way to validly convert the
standard error back since that is certainly not valid once I
back-transform.

#2 gave me some issues: first, I had to scale down RT by a factor of 1000
(from ms to s) when using gaussian(link = "inverse") otherwise I would get
an error about the downdated VtV not being positive definite. But after
dividing RT by 1000, it was able to continue, but the model did not fully
converge (I think the max abs gradient was approximately .02). I decided to
rerun the model after changing the contrasts on my variables from the
default dummy coding to effect coding (using contr.sum). The same thing
happened, except this time the max gradient was a little higher (about
.0375) and in addition, I got the "model is nearly unidentifiable" warning
due to a large eigenvalue. When I ran the model with inverse.gaussian(link
= "identity"), it worked without scaling down RT by 1000 but I a bunch of
optimizer warnings so I scaled it down and this time it wasn't able to
converge because the max abs gradient value was about .0247.


Any help on this would be greatly appreciated!


- Sam

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Thu Feb 18 09:48:35 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 18 Feb 2016 08:48:35 +0000
Subject: [R-sig-ME] Specifying a model with a link function in MCMCglmm
In-Reply-To: <CAE2_W_kUYmv8Oo5SU2qBsZAmbw=Ry98hh1_63cMW50FCJy3gcQ@mail.gmail.com>
References: <CAE2_W_kUYmv8Oo5SU2qBsZAmbw=Ry98hh1_63cMW50FCJy3gcQ@mail.gmail.com>
Message-ID: <56C58563.3050506@ed.ac.uk>

Hi Sam,

Each distribution has a fixed link function in MCMCglmm, and an inverse 
link function for a Gaussian response would be hard to implement.

Also, using a REML estimate as a prior, and then having quite a strong 
degree of belief parameter is a bit odd. Many, including myself, would 
consider this double-dipping.

Cheers,

Jarrod


On 17/02/2016 22:26, Sam H wrote:
> Hello all,
>
>
> Before jumping into my question, let me first briefly explain my model to
> give context. Here is how I currently have specified the model using
> MCMCglmm, first specifying a model in lmer and extracting the variance
> estimates for my G prior:
>
> full_lmer <- lmer(RT ~ AgeGroup*cue*cong + (cue + cong|PID), data =
> vsdataset)
>
> sigma2 <- sigma(full_lmer)^2
>
> Lambda <- getME(full_lmer, "Lambda")
>
> Sigma <- sigma2*tcrossprod(Lambda)
>
> Gmer <- Sigma[1:4,1:4] #Extracting the VCV parameters from the block
> diagonalized Sigma
>
> full_mcmc <- MCMCglmm(RT ~ AgeGroup*cue*cong, random = ~us(1 + cue +
> cong):PID, thin = 10, nitt = 20000, data = vsdataset, prior = list(G =
> list(G1 = list(V = diag(diag(Gmer)), nu = 5))))
>
> Note: I used V = diag(diag(Gmer)) due to the fact that Sigma/Gmer was not
> positive definite, which MCMCglmm would not accept.
>
>
> Quick explanation of model terms:
>
> RT --> response time in msec, very positively skewed even after outlier
> removal, inverse transform seems to center it
>
> AgeGroup --> 2 level factorial var (Old and Young, between-subjects)
>
> cue --> 3 level factorial var (within-subjects)
>
> cong --> 2 level factorial var (within-subjects)
>
> PID --> participant ID number
>
>
> I want to specify this model with an inverse/reciprocal link function
> (Gaussian family). However, I can't figure out how to specify the link
> function. In the help section for the MCMCglmm function, they mention a
> "linking.function" for the random effects terms, but it doesn't seem to
> have anything to with specifying a link function for the response variable.
> According to the course notes from the MCMCglmm package, "there are many
> different types of distribution and link functions and those supported by
> MCMCglmm can be found in Table 7.1." However, Table 7.1 seems to just list
> the families and their PDFs, there's no column listing "supported" link
> functions.
>
> So, how do you specify a link function using MCMCglmm? If you can't
> directly specify a link function, is there something else I need to do such
> as specifying the prior a certain way, or is it valid to just specify the
> model as having a gaussian response and leave the mode as I've specified
> it? After plotting the model, I noticed that several of the parameter
> distributions were extremely skewed (some left, some right).
>
>
> As a side note, I originally tried two alternatives:
>
> 1) using lmer with an inverse transform
>
> 2) using glmer with family = gaussian(link = "inverse") and family =
> inverse.gaussian(link = "identity")
>
>
> #1 seems problematic due to the fact that I need to convert the response
> variable units back to the original units, which not only flips any
> confidence intervals but also makes them uneven. I'm not sure if converting
> these CI's is even appropriate as they were computed with different
> units/distribution. I also don't know of any way to validly convert the
> standard error back since that is certainly not valid once I
> back-transform.
>
> #2 gave me some issues: first, I had to scale down RT by a factor of 1000
> (from ms to s) when using gaussian(link = "inverse") otherwise I would get
> an error about the downdated VtV not being positive definite. But after
> dividing RT by 1000, it was able to continue, but the model did not fully
> converge (I think the max abs gradient was approximately .02). I decided to
> rerun the model after changing the contrasts on my variables from the
> default dummy coding to effect coding (using contr.sum). The same thing
> happened, except this time the max gradient was a little higher (about
> .0375) and in addition, I got the "model is nearly unidentifiable" warning
> due to a large eigenvalue. When I ran the model with inverse.gaussian(link
> = "identity"), it worked without scaling down RT by 1000 but I a bunch of
> optimizer warnings so I scaled it down and this time it wasn't able to
> converge because the max abs gradient value was about .0247.
>
>
> Any help on this would be greatly appreciated!
>
>
> - Sam
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From jbukoski1 at gmail.com  Thu Feb 18 18:33:57 2016
From: jbukoski1 at gmail.com (Jacob Bukoski)
Date: Thu, 18 Feb 2016 12:33:57 -0500
Subject: [R-sig-ME] "Non-positive definite approximate
 variance-covariance" output
In-Reply-To: <CAO7JsnR=mus-9y8KoSmwh6cZTjMjZOOZB_fodj2sTKEPaXo3Cw@mail.gmail.com>
References: <CAOES0VhrWFysqUs7RXt2x0v61hHr5=t7ua8=R_FNpDzFZ5KDQQ@mail.gmail.com>
	<CAO7JsnR=mus-9y8KoSmwh6cZTjMjZOOZB_fodj2sTKEPaXo3Cw@mail.gmail.com>
Message-ID: <CAOES0ViNfOYCX34eWWK3XmL=V4zLGKvFQ0GybhncNb9D5+vj1w@mail.gmail.com>

?Great -- that makes sense. Thank you!

Jacob?


On Feb 17, 2016 3:44 PM, "Douglas Bates" <bates at stat.wisc.edu> wrote:

> The standard errors are for the fixed-effects parameters.  The apVar
> component includes approximate variances of the estimators of (functions
> of) the variance components, which is where the positive-definiteness
> failure may be occurring..
>
> On Wed, Feb 17, 2016 at 2:28 PM Jacob Bukoski <jbukoski1 at gmail.com> wrote:
>
>> Hi all,
>>
>> I was in a discussion with my advisor today, and the issue of
>> "non-positive
>> definite approximate variance-covariance" errors in R when using the lme()
>> function arose.
>>
>> Depending on the varFunc specification that I use, I receive the
>> "non-positive definite approximate variance-covariance" output when
>> calling
>> myModel$apVar -- however, there are standard errors returned for my model
>> coefficients in the model summary.
>>
>> My understanding is that if the variance-covariance cannot be
>> approximated,
>> neither should the standard errors... how is the lme() function returning
>> standard errors without a variance-covariance approximation?
>>
>> Many thanks,
>> Jacob
>>
>> --
>> Jacob J. Bukoski
>> Master of Environmental Science Candidate, 2016
>> School of Forestry and Environmental Studies, Yale University
>> jbukoski1 at gmail.com | jacob.bukoski at yale.edu | LinkedIn
>> <
>> https://www.linkedin.com/profile/view?id=AAIAAAdWVW8BMzqU_2EGNbEkyuy8O7K1Jyhd8ps&trk=nav_responsive_tab_profile_pic
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From nirvana4lf at gmail.com  Thu Feb 18 18:57:55 2016
From: nirvana4lf at gmail.com (Sam H)
Date: Thu, 18 Feb 2016 09:57:55 -0800
Subject: [R-sig-ME] Specifying a model with a link function in MCMCglmm
In-Reply-To: <56C58563.3050506@ed.ac.uk>
References: <CAE2_W_kUYmv8Oo5SU2qBsZAmbw=Ry98hh1_63cMW50FCJy3gcQ@mail.gmail.com>
	<56C58563.3050506@ed.ac.uk>
Message-ID: <CAE2_W_=W_vBjUXaQv5VkCG6mNkH514tADaE9AJn=hrjrgGamUA@mail.gmail.com>

Thanks for your response Jarrod! Regarding the prior specification, I
forgot to mention the reason I did that. Originally I specified the model
with no prior, but it kept failing just before the 1000th iteration with
the error message "ill-conditioned G/R structure". I was not very sure what
to do since I don't really have any estimates to use for a prior, so I
figured I would just use estimates produced by lmer. Should I just set the
G structure to an identity matrix, or does it not really matter since nu is
high?

On Thu, Feb 18, 2016 at 12:48 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
wrote:

> Hi Sam,
>
> Each distribution has a fixed link function in MCMCglmm, and an inverse
> link function for a Gaussian response would be hard to implement.
>
> Also, using a REML estimate as a prior, and then having quite a strong
> degree of belief parameter is a bit odd. Many, including myself, would
> consider this double-dipping.
>
> Cheers,
>
> Jarrod
>
>
>
> On 17/02/2016 22:26, Sam H wrote:
>
>> Hello all,
>>
>>
>> Before jumping into my question, let me first briefly explain my model to
>> give context. Here is how I currently have specified the model using
>> MCMCglmm, first specifying a model in lmer and extracting the variance
>> estimates for my G prior:
>>
>> full_lmer <- lmer(RT ~ AgeGroup*cue*cong + (cue + cong|PID), data =
>> vsdataset)
>>
>> sigma2 <- sigma(full_lmer)^2
>>
>> Lambda <- getME(full_lmer, "Lambda")
>>
>> Sigma <- sigma2*tcrossprod(Lambda)
>>
>> Gmer <- Sigma[1:4,1:4] #Extracting the VCV parameters from the block
>> diagonalized Sigma
>>
>> full_mcmc <- MCMCglmm(RT ~ AgeGroup*cue*cong, random = ~us(1 + cue +
>> cong):PID, thin = 10, nitt = 20000, data = vsdataset, prior = list(G =
>> list(G1 = list(V = diag(diag(Gmer)), nu = 5))))
>>
>> Note: I used V = diag(diag(Gmer)) due to the fact that Sigma/Gmer was not
>> positive definite, which MCMCglmm would not accept.
>>
>>
>> Quick explanation of model terms:
>>
>> RT --> response time in msec, very positively skewed even after outlier
>> removal, inverse transform seems to center it
>>
>> AgeGroup --> 2 level factorial var (Old and Young, between-subjects)
>>
>> cue --> 3 level factorial var (within-subjects)
>>
>> cong --> 2 level factorial var (within-subjects)
>>
>> PID --> participant ID number
>>
>>
>> I want to specify this model with an inverse/reciprocal link function
>> (Gaussian family). However, I can't figure out how to specify the link
>> function. In the help section for the MCMCglmm function, they mention a
>> "linking.function" for the random effects terms, but it doesn't seem to
>> have anything to with specifying a link function for the response
>> variable.
>> According to the course notes from the MCMCglmm package, "there are many
>> different types of distribution and link functions and those supported by
>> MCMCglmm can be found in Table 7.1." However, Table 7.1 seems to just list
>> the families and their PDFs, there's no column listing "supported" link
>> functions.
>>
>> So, how do you specify a link function using MCMCglmm? If you can't
>> directly specify a link function, is there something else I need to do
>> such
>> as specifying the prior a certain way, or is it valid to just specify the
>> model as having a gaussian response and leave the mode as I've specified
>> it? After plotting the model, I noticed that several of the parameter
>> distributions were extremely skewed (some left, some right).
>>
>>
>> As a side note, I originally tried two alternatives:
>>
>> 1) using lmer with an inverse transform
>>
>> 2) using glmer with family = gaussian(link = "inverse") and family =
>> inverse.gaussian(link = "identity")
>>
>>
>> #1 seems problematic due to the fact that I need to convert the response
>> variable units back to the original units, which not only flips any
>> confidence intervals but also makes them uneven. I'm not sure if
>> converting
>> these CI's is even appropriate as they were computed with different
>> units/distribution. I also don't know of any way to validly convert the
>> standard error back since that is certainly not valid once I
>> back-transform.
>>
>> #2 gave me some issues: first, I had to scale down RT by a factor of 1000
>> (from ms to s) when using gaussian(link = "inverse") otherwise I would get
>> an error about the downdated VtV not being positive definite. But after
>> dividing RT by 1000, it was able to continue, but the model did not fully
>> converge (I think the max abs gradient was approximately .02). I decided
>> to
>> rerun the model after changing the contrasts on my variables from the
>> default dummy coding to effect coding (using contr.sum). The same thing
>> happened, except this time the max gradient was a little higher (about
>> .0375) and in addition, I got the "model is nearly unidentifiable" warning
>> due to a large eigenvalue. When I ran the model with inverse.gaussian(link
>> = "identity"), it worked without scaling down RT by 1000 but I a bunch of
>> optimizer warnings so I scaled it down and this time it wasn't able to
>> converge because the max abs gradient value was about .0247.
>>
>>
>> Any help on this would be greatly appreciated!
>>
>>
>> - Sam
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>

	[[alternative HTML version deleted]]


From 538280 at gmail.com  Thu Feb 18 21:19:04 2016
From: 538280 at gmail.com (Greg Snow)
Date: Thu, 18 Feb 2016 13:19:04 -0700
Subject: [R-sig-ME] Specifying a model with a link function in MCMCglmm
In-Reply-To: <CAE2_W_=W_vBjUXaQv5VkCG6mNkH514tADaE9AJn=hrjrgGamUA@mail.gmail.com>
References: <CAE2_W_kUYmv8Oo5SU2qBsZAmbw=Ry98hh1_63cMW50FCJy3gcQ@mail.gmail.com>
	<56C58563.3050506@ed.ac.uk>
	<CAE2_W_=W_vBjUXaQv5VkCG6mNkH514tADaE9AJn=hrjrgGamUA@mail.gmail.com>
Message-ID: <CAFEqCdzQjSDArkr=kgxNOox2Srm86gddTVrCu9Rq=8n1sOcT8A@mail.gmail.com>

You can follow my example and create the shortcut and  then have R
open minimized (you set that in the windows shortcut properties).

I don't know of a way to not have any R window open.

On Thu, Feb 18, 2016 at 10:57 AM, Sam H <nirvana4lf at gmail.com> wrote:
> Thanks for your response Jarrod! Regarding the prior specification, I
> forgot to mention the reason I did that. Originally I specified the model
> with no prior, but it kept failing just before the 1000th iteration with
> the error message "ill-conditioned G/R structure". I was not very sure what
> to do since I don't really have any estimates to use for a prior, so I
> figured I would just use estimates produced by lmer. Should I just set the
> G structure to an identity matrix, or does it not really matter since nu is
> high?
>
> On Thu, Feb 18, 2016 at 12:48 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
> wrote:
>
>> Hi Sam,
>>
>> Each distribution has a fixed link function in MCMCglmm, and an inverse
>> link function for a Gaussian response would be hard to implement.
>>
>> Also, using a REML estimate as a prior, and then having quite a strong
>> degree of belief parameter is a bit odd. Many, including myself, would
>> consider this double-dipping.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>> On 17/02/2016 22:26, Sam H wrote:
>>
>>> Hello all,
>>>
>>>
>>> Before jumping into my question, let me first briefly explain my model to
>>> give context. Here is how I currently have specified the model using
>>> MCMCglmm, first specifying a model in lmer and extracting the variance
>>> estimates for my G prior:
>>>
>>> full_lmer <- lmer(RT ~ AgeGroup*cue*cong + (cue + cong|PID), data =
>>> vsdataset)
>>>
>>> sigma2 <- sigma(full_lmer)^2
>>>
>>> Lambda <- getME(full_lmer, "Lambda")
>>>
>>> Sigma <- sigma2*tcrossprod(Lambda)
>>>
>>> Gmer <- Sigma[1:4,1:4] #Extracting the VCV parameters from the block
>>> diagonalized Sigma
>>>
>>> full_mcmc <- MCMCglmm(RT ~ AgeGroup*cue*cong, random = ~us(1 + cue +
>>> cong):PID, thin = 10, nitt = 20000, data = vsdataset, prior = list(G =
>>> list(G1 = list(V = diag(diag(Gmer)), nu = 5))))
>>>
>>> Note: I used V = diag(diag(Gmer)) due to the fact that Sigma/Gmer was not
>>> positive definite, which MCMCglmm would not accept.
>>>
>>>
>>> Quick explanation of model terms:
>>>
>>> RT --> response time in msec, very positively skewed even after outlier
>>> removal, inverse transform seems to center it
>>>
>>> AgeGroup --> 2 level factorial var (Old and Young, between-subjects)
>>>
>>> cue --> 3 level factorial var (within-subjects)
>>>
>>> cong --> 2 level factorial var (within-subjects)
>>>
>>> PID --> participant ID number
>>>
>>>
>>> I want to specify this model with an inverse/reciprocal link function
>>> (Gaussian family). However, I can't figure out how to specify the link
>>> function. In the help section for the MCMCglmm function, they mention a
>>> "linking.function" for the random effects terms, but it doesn't seem to
>>> have anything to with specifying a link function for the response
>>> variable.
>>> According to the course notes from the MCMCglmm package, "there are many
>>> different types of distribution and link functions and those supported by
>>> MCMCglmm can be found in Table 7.1." However, Table 7.1 seems to just list
>>> the families and their PDFs, there's no column listing "supported" link
>>> functions.
>>>
>>> So, how do you specify a link function using MCMCglmm? If you can't
>>> directly specify a link function, is there something else I need to do
>>> such
>>> as specifying the prior a certain way, or is it valid to just specify the
>>> model as having a gaussian response and leave the mode as I've specified
>>> it? After plotting the model, I noticed that several of the parameter
>>> distributions were extremely skewed (some left, some right).
>>>
>>>
>>> As a side note, I originally tried two alternatives:
>>>
>>> 1) using lmer with an inverse transform
>>>
>>> 2) using glmer with family = gaussian(link = "inverse") and family =
>>> inverse.gaussian(link = "identity")
>>>
>>>
>>> #1 seems problematic due to the fact that I need to convert the response
>>> variable units back to the original units, which not only flips any
>>> confidence intervals but also makes them uneven. I'm not sure if
>>> converting
>>> these CI's is even appropriate as they were computed with different
>>> units/distribution. I also don't know of any way to validly convert the
>>> standard error back since that is certainly not valid once I
>>> back-transform.
>>>
>>> #2 gave me some issues: first, I had to scale down RT by a factor of 1000
>>> (from ms to s) when using gaussian(link = "inverse") otherwise I would get
>>> an error about the downdated VtV not being positive definite. But after
>>> dividing RT by 1000, it was able to continue, but the model did not fully
>>> converge (I think the max abs gradient was approximately .02). I decided
>>> to
>>> rerun the model after changing the contrasts on my variables from the
>>> default dummy coding to effect coding (using contr.sum). The same thing
>>> happened, except this time the max gradient was a little higher (about
>>> .0375) and in addition, I got the "model is nearly unidentifiable" warning
>>> due to a large eigenvalue. When I ran the model with inverse.gaussian(link
>>> = "identity"), it worked without scaling down RT by 1000 but I a bunch of
>>> optimizer warnings so I scaled it down and this time it wasn't able to
>>> converge because the max abs gradient value was about .0247.
>>>
>>>
>>> Any help on this would be greatly appreciated!
>>>
>>>
>>> - Sam
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From j.hadfield at ed.ac.uk  Fri Feb 19 07:53:23 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 19 Feb 2016 06:53:23 +0000
Subject: [R-sig-ME] [R] Comparing variance components
In-Reply-To: <CAMmHrnxQ60irKAHJ13SURAdmZd=+N=cNvFzFBqbowmW0agCVJg@mail.gmail.com>
References: <CAMmHrnxQ60irKAHJ13SURAdmZd=+N=cNvFzFBqbowmW0agCVJg@mail.gmail.com>
Message-ID: <56C6BBE3.1000404@ed.ac.uk>

Hi,

Whether you should focus on differences in the intra-class correlation 
or the raw variances depends on the question. If it is suspected that 
the difference is due  to differences in scale then I guess testing the 
IC would make more sense, because a change in scale would increase both 
variance components proportionally and the IC should remain the same. 
There is actually a way of just modelling scale differences in MCMCglmm 
using path analysis. It sounds odd but jut regress the response against 
itself in one of the experimental blocks. This works because y = y*beta 
+ eta after a little rearangement becomes

y = eta/(1-beta)

and so 1/(1-beta) where beta is the path coefficient is an estimate of 
the scale (or 1/(1-beta)^2 the change in variance).  For example, lets 
say Vg=Ve=1 in the first block but Vg=Ve=4 in the second (i.e. the scale 
is 2).

G<-gl(50,6)
g<-rnorm(50)
Exp<-gl(2,150)

y<-rnorm(300)+g[G]

y[which(Exp==2)]<-2*y[which(Exp==2)]

my_data<-data.frame(y=y, Exp=Exp, G=G)

model.mcmc.c<-MCMCglmm(y~sir(~units:at.level(Exp,2),~units:at.level(Exp,2)), 
random=~G, rcov=~units, data=my_data)

plot(1/(1-model.mcmc.c$Lambda))

Note that you can't just add y as a predictor in standard (g)lm(m) 
software because the likelihood is not in standard form.

Regarding the variances, I usually use scaled F(1,1) priors in binomial 
models too. However, the long tail can be a bit of a problem in these 
models if the data set is small. In extreme cases it can lead to 
numerical problems: check the latent variables don't lie outside the 
range -25/25 for logit models.  Reducing the scale in the prior can help.

Cheers,

Jarrod




On 17/02/2016 16:15, Dean Castillo wrote:
> Hi Jarrod,
>
> I have been trying to test a similar hypothesis for a while with only 
> limited success, so I wanted to thank you for your answer to Wen's 
> question. I did have a few additional questions of clarification, some 
> specific to my own data analysis.
>
> For model.mcmc.a I think it is pretty straightforward to compare the 
> posterior distributions. For model.mcmc.b, now that you explicitly 
> modeled heterogenous variances for the residuals is it more 
> informative to examine the IC correlation for G for each Exp rather 
> than the variances themselves?
>
> For my specific problem I have binomial data. I have been modeling the 
> data as proportions (bounded by 0-1 but can logit or arcsinsqrt 
> transform) as well as modeling it using "multinomial2" on the raw data.
>
> The issue I am running into is that the variance of G for one of the 
> experimental blocks is very close to the boundary condition, while the 
> other is larger, when modeled as a proportion, and the HPD are very 
> wide. I have been using inv-gamma priors and will play around with the 
> parameter expanded priors as suggested in your course notes.
>
> For the multinomial2 model should the priors for the variance be the 
> same? The posterior means are not as close to the boundary condition 
> (Exp1:G=0.8, Exp2:G=0.3) but the HPD are still very wide (1e-17,1.13) 
> for Exp2:G.
>
> Any help is greatly appreciated
>
> Dean
>
>     Date: Tue, 16 Feb 2016 20:59:33 +0000
>     From: Jarrod Hadfield <j.hadfield at ed.ac.uk
>     <mailto:j.hadfield at ed.ac.uk>>
>     To: Wen Huang <whuang.ustc at gmail.com
>     <mailto:whuang.ustc at gmail.com>>, "Thompson,Paul"
>             <Paul.Thompson at SanfordHealth.org>
>     Cc: r-sig-mixed-models at r-project.org
>     <mailto:r-sig-mixed-models at r-project.org>
>     Subject: Re: [R-sig-ME] [R] Comparing variance components
>     Message-ID: <56C38DB5.2010709 at ed.ac.uk
>     <mailto:56C38DB5.2010709 at ed.ac.uk>>
>     Content-Type: text/plain; charset=utf-8; format=flowed
>
>     Hi Wen,
>
>     The question sounds sensible to me, but you can't do what you want
>     to do
>     in lmer because it does not allow heterogenous variances for the
>     residuals. You can do it in nlme:
>
>     model.lme.a<- lme(y~Exp, random=~1|G,  data=my_data)
>     model.lme.b<- lme(y~Exp, random=~0+Exp|G,
>     weights=varIdent(form=~1|Exp),  data=my_data)
>
>     or MCMCglmm (or asreml if you have it):
>
>     model.mcmc.a<- MCMCglmm(y~Exp, random=~G, data=my_data)
>     model.mcmc.b<- MCMCglmm(y~Exp, random=~idh(Exp):G,
>     rcov=~idh(Exp):units,
>     data=my_data)
>
>     The first model assumes common variances for each experiment, the
>     second
>     allows the variances to differ. You can comapre model.lme.a and
>     model.lme.b using a likelihood ratio test (2 parameters) or you can
>     compare the posterior distributions in the Bayesian model.
>
>     Note that this assumes that the levels of the random effect differ in
>     the two epxeriments (and they have been given separate lables). If
>     there
>     is overlap then an additional assumption of model.a is that the random
>     effects have a correlation of 1 between the two experiments when they
>     are associated with the same factor level.
>
>     Cheers,
>
>     Jarrod
>
>
>
>     On 16/02/2016 20:28, Wen Huang wrote:
>     > Hi Paul,
>     >
>     > Thank you. That is a neat idea. How would you implement that?
>     Could you write an example code on how the model should be fitted?
>     Sorry for my ignorance.
>     >
>     > Thanks,
>     > Wen
>     >
>     >> On Feb 16, 2016, at 1:18 PM, Thompson,Paul
>     <Paul.Thompson at SanfordHealth.org> wrote:
>     >>
>     >> Are you computing two estimates of reliability and wishing to
>     compare them? One possible method is to set both into the same
>     design, treat the design effect (Exp 1, Exp 2) as a fixed effect,
>     and compare them with a standard F test.
>     >>
>     >> -----Original Message-----
>     >> From: R-sig-mixed-models
>     [mailto:r-sig-mixed-models-bounces at r-project.org
>     <mailto:r-sig-mixed-models-bounces at r-project.org>] On Behalf Of
>     Wen Huang
>     >> Sent: Tuesday, February 16, 2016 11:57 AM
>     >> To: Doran, Harold
>     >> Cc: r-sig-mixed-models at r-project.org
>     <mailto:r-sig-mixed-models at r-project.org>
>     >> Subject: Re: [R-sig-ME] [R] Comparing variance components
>     >>
>     >> Hi Harold,
>     >>
>     >> Thank you for your input. I was not very clear. I wanted to
>     compare the sigma2_A?s from the same model fitted to two different
>     data sets. The same for sigma2_e?s. The motivation is when I did
>     the same experiment at two different times, whether the variance
>     due to A (sigma2_A) is bigger at one time versus another. The same
>     for sigma2_e, whether the residual variance is bigger for one
>     experiment versus another.
>     >>
>     >> Thanks,
>     >> Wen
>     >>
>     >>> On Feb 16, 2016, at 12:40 PM, Doran, Harold <HDoran at air.org
>     <mailto:HDoran at air.org>> wrote:
>     >>>
>     >>> (adding R mixed group). You actually do not want to do this
>     test, and there is no "shrinkage" here on these variances. First,
>     there are conditional variances and marginal variances in the
>     mixed model. What you are have below as "A" is the marginal
>     variances of the random effects and there is no shrinkage on
>     these, per se.
>     >>>
>     >>> The conditional means of the random effects have shrinkage and
>     each conditional mean (or BLUP) has a conditional variance.
>     >>>
>     >>> Now, it seems very odd to want to compare the variance between
>     A and then what you have as sigma2_e, which is presumably the
>     residual variance. These are variances of two completely different
>     things, so a test comparing them seems strange, though I suppose
>     some theoretical reason could exists justifying it, I cannot
>     imagine one though.
>     >>>
>     >>>
>     >>>
>     >>>
>     >>>
>     >>> -----Original Message-----
>     >>> From: R-help [mailto:r-help-bounces at r-project.org
>     <mailto:r-help-bounces at r-project.org>] On Behalf Of Wen
>     >>> Huang
>     >>> Sent: Tuesday, February 16, 2016 10:57 AM
>     >>> To: r-help at r-project.org <mailto:r-help at r-project.org>
>     >>> Subject: [R] Comparing variance components
>     >>>
>     >>> Dear R-help members,
>     >>>
>     >>> Say I have two data sets collected at different times with the
>     same
>     >>> design. I fit a mixed model using in R using lmer
>     >>>
>     >>> lmer(y ~ (1|A))
>     >>>
>     >>> to these data sets and get two estimates of sigma2_A and sigma2_e
>     >>>
>     >>> What would be a good way to compare sigma2_A and sigma2_e for
>     these two data sets and obtain a P value for the hypothesis that
>     sigma2_A1 = sigma2_A2? There is obvious shrinkage on these
>     estimates, should I be worried about the differential levels of
>     shrinkage on these estimates and how to account for that?
>     >>>
>     >>> Thank you for your thoughts and inputs!
>     >>>
>     >>>
>     >>>
>     >>>     [[alternative HTML version deleted]]
>     >>>
>     >>> ______________________________________________
>     >>> R-help at r-project.org <mailto:R-help at r-project.org> mailing
>     list -- To UNSUBSCRIBE and more, see
>     >>> https://stat.ethz.ch/mailman/listinfo/r-help
>     >>> PLEASE do read the posting guide
>     >>> http://www.R-project.org/posting-guide.html
>     >>> and provide commented, minimal, self-contained, reproducible code.
>     >> _______________________________________________
>     >> R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>     >>
>     -----------------------------------------------------------------------
>     >> Confidentiality Notice: This e-mail message, including any
>     attachments,
>     >> is for the sole use of the intended recipient(s) and may contain
>     >> privileged and confidential information.  Any unauthorized
>     review, use,
>     >> disclosure or distribution is prohibited.  If you are not the
>     intended
>     >> recipient, please contact the sender by reply e-mail and destroy
>     >> all copies of the original message.
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>     --
>     The University of Edinburgh is a charitable body, registered in
>     Scotland, with registration number SC005336.
>
>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20160219/c6290bdc/attachment-0001.pl>

From j.hadfield at ed.ac.uk  Fri Feb 19 08:27:53 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 19 Feb 2016 07:27:53 +0000
Subject: [R-sig-ME] Specifying a model with a link function in MCMCglmm
In-Reply-To: <CAE2_W_=W_vBjUXaQv5VkCG6mNkH514tADaE9AJn=hrjrgGamUA@mail.gmail.com>
References: <CAE2_W_kUYmv8Oo5SU2qBsZAmbw=Ry98hh1_63cMW50FCJy3gcQ@mail.gmail.com>
	<56C58563.3050506@ed.ac.uk>
	<CAE2_W_=W_vBjUXaQv5VkCG6mNkH514tADaE9AJn=hrjrgGamUA@mail.gmail.com>
Message-ID: <56C6C3F9.30708@ed.ac.uk>

Hi Sam,

Priors for covariance matrices are tricky but having nu=k (where k is 
the dimension of the matrix) is not a good idea. I tend to use parameter 
expanded priors of the form

list(V=diag(k), nu=k, alpha.mu=rep(0,k), alpha.V=diag(k)*1000)

The marginal priors for the variances are scaled F(1,1), but I'm not 
sure what they are for the covariances/correlation. For correlations I 
think it would be flatish but with peaks close to -1 and 1. Upping 
nu=k+2 probably makes the marginal prior for the correlation close to 
uniform over the -1/1 interval. Unfortunatley there seems to be little 
or no theoretical work on this prior in a multivariate context: I use it 
because it *seems* to have nice properties. For exampe, if I fitted a 
set of random effects which in reality are all zero, then it gives 
posterior support for the correlation across the range -1/1.

I would also add that I don't attempt to fit k>2 covariance matrices 
unless my data are up to the task.

Cheers,

Jarrod

k<-2
Ve<-rIW(diag(k), 10)
y<-MASS::mvrnorm(100, rep(0,k), Ve)
rfac<-gl(25,4)

#  the data are simply y~MVN(0, Ve)

my_data<-data.frame(y1=y[,1], y2=y[,2], rfac=rfac)

prior1<-list(R=list(V=diag(k), nu=0), G=list(G1=list(V=diag(k), nu=k, 
alpha.mu=rep(0,k), alpha.V=diag(k)*1000)))

m1<-MCMCglmm(cbind(y1,y2)~trait-1, random=~us(trait):rfac, 
rcov=~us(trait):units, data=my_data, prior=prior1,family=rep("gaussian", k))

hist(m1$VCV[,2]/sqrt(m1$VCV[,1]*m1$VCV[,4]), breaks=30)
# posterior correlation of the random effects: nice

prior2<-list(R=list(V=diag(k), nu=0), G=list(G1=list(V=diag(k), nu=k)))

m2<-MCMCglmm(cbind(y1,y2)~trait-1, random=~us(trait):rfac, 
rcov=~us(trait):units, data=my_data, prior=prior2,family=rep("gaussian", k))

hist(m2$VCV[,2]/sqrt(m2$VCV[,1]*m2$VCV[,4]), breaks=30)
# bad - look at the variances too!







On 18/02/2016 17:57, Sam H wrote:
> Thanks for your response Jarrod! Regarding the prior specification, I 
> forgot to mention the reason I did that. Originally I specified the 
> model with no prior, but it kept failing just before the 1000th 
> iteration with the error message "ill-conditioned G/R structure". I 
> was not very sure what to do since I don't really have any estimates 
> to use for a prior, so I figured I would just use estimates produced 
> by lmer. Should I just set the G structure to an identity matrix, or 
> does it not really matter since nu is high?
>
> On Thu, Feb 18, 2016 at 12:48 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk 
> <mailto:j.hadfield at ed.ac.uk>> wrote:
>
>     Hi Sam,
>
>     Each distribution has a fixed link function in MCMCglmm, and an
>     inverse link function for a Gaussian response would be hard to
>     implement.
>
>     Also, using a REML estimate as a prior, and then having quite a
>     strong degree of belief parameter is a bit odd. Many, including
>     myself, would consider this double-dipping.
>
>     Cheers,
>
>     Jarrod
>
>
>
>     On 17/02/2016 22:26, Sam H wrote:
>
>         Hello all,
>
>
>         Before jumping into my question, let me first briefly explain
>         my model to
>         give context. Here is how I currently have specified the model
>         using
>         MCMCglmm, first specifying a model in lmer and extracting the
>         variance
>         estimates for my G prior:
>
>         full_lmer <- lmer(RT ~ AgeGroup*cue*cong + (cue + cong|PID),
>         data =
>         vsdataset)
>
>         sigma2 <- sigma(full_lmer)^2
>
>         Lambda <- getME(full_lmer, "Lambda")
>
>         Sigma <- sigma2*tcrossprod(Lambda)
>
>         Gmer <- Sigma[1:4,1:4] #Extracting the VCV parameters from the
>         block
>         diagonalized Sigma
>
>         full_mcmc <- MCMCglmm(RT ~ AgeGroup*cue*cong, random = ~us(1 +
>         cue +
>         cong):PID, thin = 10, nitt = 20000, data = vsdataset, prior =
>         list(G =
>         list(G1 = list(V = diag(diag(Gmer)), nu = 5))))
>
>         Note: I used V = diag(diag(Gmer)) due to the fact that
>         Sigma/Gmer was not
>         positive definite, which MCMCglmm would not accept.
>
>
>         Quick explanation of model terms:
>
>         RT --> response time in msec, very positively skewed even
>         after outlier
>         removal, inverse transform seems to center it
>
>         AgeGroup --> 2 level factorial var (Old and Young,
>         between-subjects)
>
>         cue --> 3 level factorial var (within-subjects)
>
>         cong --> 2 level factorial var (within-subjects)
>
>         PID --> participant ID number
>
>
>         I want to specify this model with an inverse/reciprocal link
>         function
>         (Gaussian family). However, I can't figure out how to specify
>         the link
>         function. In the help section for the MCMCglmm function, they
>         mention a
>         "linking.function" for the random effects terms, but it
>         doesn't seem to
>         have anything to with specifying a link function for the
>         response variable.
>         According to the course notes from the MCMCglmm package,
>         "there are many
>         different types of distribution and link functions and those
>         supported by
>         MCMCglmm can be found in Table 7.1." However, Table 7.1 seems
>         to just list
>         the families and their PDFs, there's no column listing
>         "supported" link
>         functions.
>
>         So, how do you specify a link function using MCMCglmm? If you
>         can't
>         directly specify a link function, is there something else I
>         need to do such
>         as specifying the prior a certain way, or is it valid to just
>         specify the
>         model as having a gaussian response and leave the mode as I've
>         specified
>         it? After plotting the model, I noticed that several of the
>         parameter
>         distributions were extremely skewed (some left, some right).
>
>
>         As a side note, I originally tried two alternatives:
>
>         1) using lmer with an inverse transform
>
>         2) using glmer with family = gaussian(link = "inverse") and
>         family =
>         inverse.gaussian(link = "identity")
>
>
>         #1 seems problematic due to the fact that I need to convert
>         the response
>         variable units back to the original units, which not only
>         flips any
>         confidence intervals but also makes them uneven. I'm not sure
>         if converting
>         these CI's is even appropriate as they were computed with
>         different
>         units/distribution. I also don't know of any way to validly
>         convert the
>         standard error back since that is certainly not valid once I
>         back-transform.
>
>         #2 gave me some issues: first, I had to scale down RT by a
>         factor of 1000
>         (from ms to s) when using gaussian(link = "inverse") otherwise
>         I would get
>         an error about the downdated VtV not being positive definite.
>         But after
>         dividing RT by 1000, it was able to continue, but the model
>         did not fully
>         converge (I think the max abs gradient was approximately .02).
>         I decided to
>         rerun the model after changing the contrasts on my variables
>         from the
>         default dummy coding to effect coding (using contr.sum). The
>         same thing
>         happened, except this time the max gradient was a little
>         higher (about
>         .0375) and in addition, I got the "model is nearly
>         unidentifiable" warning
>         due to a large eigenvalue. When I ran the model with
>         inverse.gaussian(link
>         = "identity"), it worked without scaling down RT by 1000 but I
>         a bunch of
>         optimizer warnings so I scaled it down and this time it wasn't
>         able to
>         converge because the max abs gradient value was about .0247.
>
>
>         Any help on this would be greatly appreciated!
>
>
>         - Sam
>
>                 [[alternative HTML version deleted]]
>
>         _______________________________________________
>         R-sig-mixed-models at r-project.org
>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
>     -- 
>     The University of Edinburgh is a charitable body, registered in
>     Scotland, with registration number SC005336.
>
>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20160219/69af2092/attachment.pl>

From j.hadfield at ed.ac.uk  Fri Feb 19 09:09:03 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 19 Feb 2016 08:09:03 +0000
Subject: [R-sig-ME] Specifying a model with a link function in MCMCglmm
In-Reply-To: <56C6C3F9.30708@ed.ac.uk>
References: <CAE2_W_kUYmv8Oo5SU2qBsZAmbw=Ry98hh1_63cMW50FCJy3gcQ@mail.gmail.com>
	<56C58563.3050506@ed.ac.uk>
	<CAE2_W_=W_vBjUXaQv5VkCG6mNkH514tADaE9AJn=hrjrgGamUA@mail.gmail.com>
	<56C6C3F9.30708@ed.ac.uk>
Message-ID: <56C6CD9F.3030006@ed.ac.uk>

Hi Sam,

Sorry, I should have said nu=k+1 (not nu=k+2) probably makes the 
marginal prior for the correlation close to uniform over the -1/1 
interval. This was a guess given the properties of the inverse-Wishart 
(see P72 of the CourseNotes) but it seems to hold .... below is a 
function for simulating from a PX prior:

V2<-rpx(10000, V=diag(2), nu=2, alpha.mu=rep(0,2), alpha.V=diag(2)*1000)
V3<-rpx(10000, V=diag(2), nu=2+1, alpha.mu=rep(0,2), alpha.V=diag(2)*1000)

par(mfrow=c(1,2))
hist(V2[,2]/sqrt(V2[,1]*V2[,4]))
hist(V3[,2]/sqrt(V3[,1]*V3[,4]))

Note that in multiparameter models having marginal priors that are flat 
does NOT mean that they will have no influence on the marginal posterior.

Some simulation/analytic work on the multivariate PX prior would be a 
nice thing for someone to do!

Cheers,

Jarrod


rpx<-function(n=1, V, nu, alpha.mu, alpha.V){

k<-nrow(V)
alpha<-MASS::mvrnorm(n, alpha.mu, alpha.V)
Valpha<-MCMCglmm::rIW(V, nu=nu, n=n)

Vp<-sapply(1:n, 
FUN=function(i){diag(alpha[i,])%*%matrix(Valpha[i,],k,k)%*%diag(alpha[i,])})

if(k==1){
return(Vp)
}else{
return(t(Vp))
}
}






On 19/02/2016 07:27, Jarrod Hadfield wrote:
> Hi Sam,
>
> Priors for covariance matrices are tricky but having nu=k (where k is 
> the dimension of the matrix) is not a good idea. I tend to use 
> parameter expanded priors of the form
>
> list(V=diag(k), nu=k, alpha.mu=rep(0,k), alpha.V=diag(k)*1000)
>
> The marginal priors for the variances are scaled F(1,1), but I'm not 
> sure what they are for the covariances/correlation. For correlations I 
> think it would be flatish but with peaks close to -1 and 1. Upping 
> nu=k+2 probably makes the marginal prior for the correlation close to 
> uniform over the -1/1 interval. Unfortunatley there seems to be little 
> or no theoretical work on this prior in a multivariate context: I use 
> it because it *seems* to have nice properties. For exampe, if I fitted 
> a set of random effects which in reality are all zero, then it gives 
> posterior support for the correlation across the range -1/1.
>
> I would also add that I don't attempt to fit k>2 covariance matrices 
> unless my data are up to the task.
>
> Cheers,
>
> Jarrod
>
> k<-2
> Ve<-rIW(diag(k), 10)
> y<-MASS::mvrnorm(100, rep(0,k), Ve)
> rfac<-gl(25,4)
>
> #  the data are simply y~MVN(0, Ve)
>
> my_data<-data.frame(y1=y[,1], y2=y[,2], rfac=rfac)
>
> prior1<-list(R=list(V=diag(k), nu=0), G=list(G1=list(V=diag(k), nu=k, 
> alpha.mu=rep(0,k), alpha.V=diag(k)*1000)))
>
> m1<-MCMCglmm(cbind(y1,y2)~trait-1, random=~us(trait):rfac, 
> rcov=~us(trait):units, data=my_data, 
> prior=prior1,family=rep("gaussian", k))
>
> hist(m1$VCV[,2]/sqrt(m1$VCV[,1]*m1$VCV[,4]), breaks=30)
> # posterior correlation of the random effects: nice
>
> prior2<-list(R=list(V=diag(k), nu=0), G=list(G1=list(V=diag(k), nu=k)))
>
> m2<-MCMCglmm(cbind(y1,y2)~trait-1, random=~us(trait):rfac, 
> rcov=~us(trait):units, data=my_data, 
> prior=prior2,family=rep("gaussian", k))
>
> hist(m2$VCV[,2]/sqrt(m2$VCV[,1]*m2$VCV[,4]), breaks=30)
> # bad - look at the variances too!
>
>
>
>
>
>
>
> On 18/02/2016 17:57, Sam H wrote:
>> Thanks for your response Jarrod! Regarding the prior specification, I 
>> forgot to mention the reason I did that. Originally I specified the 
>> model with no prior, but it kept failing just before the 1000th 
>> iteration with the error message "ill-conditioned G/R structure". I 
>> was not very sure what to do since I don't really have any estimates 
>> to use for a prior, so I figured I would just use estimates produced 
>> by lmer. Should I just set the G structure to an identity matrix, or 
>> does it not really matter since nu is high?
>>
>> On Thu, Feb 18, 2016 at 12:48 AM, Jarrod Hadfield 
>> <j.hadfield at ed.ac.uk <mailto:j.hadfield at ed.ac.uk>> wrote:
>>
>>     Hi Sam,
>>
>>     Each distribution has a fixed link function in MCMCglmm, and an
>>     inverse link function for a Gaussian response would be hard to
>>     implement.
>>
>>     Also, using a REML estimate as a prior, and then having quite a
>>     strong degree of belief parameter is a bit odd. Many, including
>>     myself, would consider this double-dipping.
>>
>>     Cheers,
>>
>>     Jarrod
>>
>>
>>
>>     On 17/02/2016 22:26, Sam H wrote:
>>
>>         Hello all,
>>
>>
>>         Before jumping into my question, let me first briefly explain
>>         my model to
>>         give context. Here is how I currently have specified the model
>>         using
>>         MCMCglmm, first specifying a model in lmer and extracting the
>>         variance
>>         estimates for my G prior:
>>
>>         full_lmer <- lmer(RT ~ AgeGroup*cue*cong + (cue + cong|PID),
>>         data =
>>         vsdataset)
>>
>>         sigma2 <- sigma(full_lmer)^2
>>
>>         Lambda <- getME(full_lmer, "Lambda")
>>
>>         Sigma <- sigma2*tcrossprod(Lambda)
>>
>>         Gmer <- Sigma[1:4,1:4] #Extracting the VCV parameters from the
>>         block
>>         diagonalized Sigma
>>
>>         full_mcmc <- MCMCglmm(RT ~ AgeGroup*cue*cong, random = ~us(1 +
>>         cue +
>>         cong):PID, thin = 10, nitt = 20000, data = vsdataset, prior =
>>         list(G =
>>         list(G1 = list(V = diag(diag(Gmer)), nu = 5))))
>>
>>         Note: I used V = diag(diag(Gmer)) due to the fact that
>>         Sigma/Gmer was not
>>         positive definite, which MCMCglmm would not accept.
>>
>>
>>         Quick explanation of model terms:
>>
>>         RT --> response time in msec, very positively skewed even
>>         after outlier
>>         removal, inverse transform seems to center it
>>
>>         AgeGroup --> 2 level factorial var (Old and Young,
>>         between-subjects)
>>
>>         cue --> 3 level factorial var (within-subjects)
>>
>>         cong --> 2 level factorial var (within-subjects)
>>
>>         PID --> participant ID number
>>
>>
>>         I want to specify this model with an inverse/reciprocal link
>>         function
>>         (Gaussian family). However, I can't figure out how to specify
>>         the link
>>         function. In the help section for the MCMCglmm function, they
>>         mention a
>>         "linking.function" for the random effects terms, but it
>>         doesn't seem to
>>         have anything to with specifying a link function for the
>>         response variable.
>>         According to the course notes from the MCMCglmm package,
>>         "there are many
>>         different types of distribution and link functions and those
>>         supported by
>>         MCMCglmm can be found in Table 7.1." However, Table 7.1 seems
>>         to just list
>>         the families and their PDFs, there's no column listing
>>         "supported" link
>>         functions.
>>
>>         So, how do you specify a link function using MCMCglmm? If you
>>         can't
>>         directly specify a link function, is there something else I
>>         need to do such
>>         as specifying the prior a certain way, or is it valid to just
>>         specify the
>>         model as having a gaussian response and leave the mode as I've
>>         specified
>>         it? After plotting the model, I noticed that several of the
>>         parameter
>>         distributions were extremely skewed (some left, some right).
>>
>>
>>         As a side note, I originally tried two alternatives:
>>
>>         1) using lmer with an inverse transform
>>
>>         2) using glmer with family = gaussian(link = "inverse") and
>>         family =
>>         inverse.gaussian(link = "identity")
>>
>>
>>         #1 seems problematic due to the fact that I need to convert
>>         the response
>>         variable units back to the original units, which not only
>>         flips any
>>         confidence intervals but also makes them uneven. I'm not sure
>>         if converting
>>         these CI's is even appropriate as they were computed with
>>         different
>>         units/distribution. I also don't know of any way to validly
>>         convert the
>>         standard error back since that is certainly not valid once I
>>         back-transform.
>>
>>         #2 gave me some issues: first, I had to scale down RT by a
>>         factor of 1000
>>         (from ms to s) when using gaussian(link = "inverse") otherwise
>>         I would get
>>         an error about the downdated VtV not being positive definite.
>>         But after
>>         dividing RT by 1000, it was able to continue, but the model
>>         did not fully
>>         converge (I think the max abs gradient was approximately .02).
>>         I decided to
>>         rerun the model after changing the contrasts on my variables
>>         from the
>>         default dummy coding to effect coding (using contr.sum). The
>>         same thing
>>         happened, except this time the max gradient was a little
>>         higher (about
>>         .0375) and in addition, I got the "model is nearly
>>         unidentifiable" warning
>>         due to a large eigenvalue. When I ran the model with
>>         inverse.gaussian(link
>>         = "identity"), it worked without scaling down RT by 1000 but I
>>         a bunch of
>>         optimizer warnings so I scaled it down and this time it wasn't
>>         able to
>>         converge because the max abs gradient value was about .0247.
>>
>>
>>         Any help on this would be greatly appreciated!
>>
>>
>>         - Sam
>>
>>                 [[alternative HTML version deleted]]
>>
>>         _______________________________________________
>>         R-sig-mixed-models at r-project.org
>>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>>     --     The University of Edinburgh is a charitable body, 
>> registered in
>>     Scotland, with registration number SC005336.
>>
>>
>
>
>
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20160219/62f45105/attachment.pl>

From selebatsom at yahoo.co.uk  Fri Feb 19 19:37:11 2016
From: selebatsom at yahoo.co.uk (moses selebatso)
Date: Fri, 19 Feb 2016 18:37:11 +0000 (UTC)
Subject: [R-sig-ME] glmer error (maxstephalfit) PIRLS step-halvings failed
 to reduce deviance in pwrssUpdate)
References: <1439566156.1701076.1455907031839.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1439566156.1701076.1455907031839.JavaMail.yahoo@mail.yahoo.com>

Hello
I am running a model on my data, and getting error message. Below is the script and the out put. Any ideas what the problem could be? I can share the data, if required.

> b<-read.csv("clipboard",sep="\t")
> attach(b)
> modelb<-glmer(Biomass1~Habitat+Season + (1|Location/Replica), family=Gamma)
(bG) nan @ pos 12: y= 40.4; mu=-429094; mu inv=-2.33049e-006; wt=1; y/mu=-9.4152e-005; log(y/mu) =1.#QNAN
(bG) nan @ pos 12: y= 40.4; mu=-858187; mu inv=-1.16525e-006; wt=1; y/mu=-4.7076e-005; log(y/mu) =1.#QNAN
(bG) nan @ pos 12: y= 40.4; mu=-1.71637e+006; mu inv=-5.82624e-007; wt=1; y/mu=-2.3538e-005; log(y/mu) =1.#QNAN
(bG) nan @ pos 12: y= 40.4; mu=-3.43275e+006; mu inv=-2.91312e-007; wt=1; y/mu=-1.1769e-005; log(y/mu) =1.#QNAN
(bG) nan @ pos 12: y= 40.4; mu=-6.8655e+006; mu inv=-1.45656e-007; wt=1; y/mu=-5.8845e-006; log(y/mu) =1.#QNAN
(bG) nan @ pos 12: y= 40.4; mu=-1.3731e+007; mu inv=-7.28279e-008; wt=1; y/mu=-2.94225e-006; log(y/mu) =1.#QNAN
(bG) nan @ pos 12: y= 40.4; mu=-2.7462e+007; mu inv=-3.6414e-008; wt=1; y/mu=-1.47112e-006; log(y/mu) =1.#QNAN
(bG) nan @ pos 12: y= 40.4; mu=-5.4924e+007; mu inv=-1.8207e-008; wt=1; y/mu=-7.35562e-007; log(y/mu) =1.#QNAN
(bG) nan @ pos 12: y= 40.4; mu=-1.09848e+008; mu inv=-9.10349e-009; wt=1; y/mu=-3.67781e-007; log(y/mu) =1.#QNAN
(bG) nan @ pos 12: y= 40.4; mu=-2.19696e+008; mu inv=-4.55175e-009; wt=1; y/mu=-1.83891e-007; log(y/mu) =1.#QNAN
(bG) nan @ pos 12: y= 40.4; mu=-4.39392e+008; mu inv=-2.27587e-009; wt=1; y/mu=-9.19453e-008; log(y/mu) =1.#QNAN
Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate
Thank you,
Moses 
	[[alternative HTML version deleted]]


From bbolker at gmail.com  Sat Feb 20 17:06:48 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 20 Feb 2016 16:06:48 +0000 (UTC)
Subject: [R-sig-ME]
	=?utf-8?q?new_R_user_struggling_with_error_and_converg?=
	=?utf-8?q?ence=09issues?=
References: <61816917-B0DD-46BC-9FF7-5D5A615BE599@yahoo.com>
	<27287EAC-87E0-4E3E-9815-0627C891CB4E@yahoo.com>
Message-ID: <loom.20160220T170030-556@post.gmane.org>

Jennifer Yourkavitch via R-sig-mixed-models <r-sig-mixed-models at ...> writes:

> 
> Follow-up: glmmPQL works like a charm. I don?t really
>  understand the difference between the algorithms
> used by glmmPQL and glmer. Should I be concerned that 
> the models ran with glmmPQL but not glmer?
> Thanks,
> Jennifer
> 

  The main case where Laplace approximation (the default glmer
algorithm) is significantly better than PQL is when there are a
small number of values per group; PQL depends more strongly than
Laplace approximation on the approximate Normality of the sampling
distributions of the conditional modes (predictions of the random
effects for particular groups).
   Do the predicted values make sense?

  Ben Bolker
 

From bbolker at gmail.com  Sat Feb 20 17:16:53 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 20 Feb 2016 11:16:53 -0500
Subject: [R-sig-ME] glmer error (maxstephalfit) PIRLS step-halvings
 failed to reduce deviance in pwrssUpdate)
In-Reply-To: <1439566156.1701076.1455907031839.JavaMail.yahoo@mail.yahoo.com>
References: <1439566156.1701076.1455907031839.JavaMail.yahoo.ref@mail.yahoo.com>
	<1439566156.1701076.1455907031839.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <56C89175.2000509@gmail.com>


	

On 16-02-19 01:37 PM, moses selebatso wrote:
> Hello
> I am running a model on my data, and getting error message. Below is the script and the out put. Any ideas what the problem could be? I can share the data, if required.
>
>> b<-read.csv("clipboard",sep="\t")
>> attach(b)
>> modelb<-glmer(Biomass1~Habitat+Season + (1|Location/Replica), family=Gamma)
> (bG) nan @ pos 12: y= 40.4; mu=-429094; mu inv=-2.33049e-006; wt=1; y/mu=-9.4152e-005; log(y/mu) =1.#QNAN
> (bG) nan @ pos 12: y= 40.4; mu=-858187; mu inv=-1.16525e-006; wt=1; y/mu=-4.7076e-005; log(y/mu) =1.#QNAN
> (bG) nan @ pos 12: y= 40.4; mu=-1.71637e+006; mu inv=-5.82624e-007; wt=1; y/mu=-2.3538e-005; log(y/mu) =1.#QNAN
> (bG) nan @ pos 12: y= 40.4; mu=-3.43275e+006; mu inv=-2.91312e-007; wt=1; y/mu=-1.1769e-005; log(y/mu) =1.#QNAN
> (bG) nan @ pos 12: y= 40.4; mu=-6.8655e+006; mu inv=-1.45656e-007; wt=1; y/mu=-5.8845e-006; log(y/mu) =1.#QNAN
> (bG) nan @ pos 12: y= 40.4; mu=-1.3731e+007; mu inv=-7.28279e-008; wt=1; y/mu=-2.94225e-006; log(y/mu) =1.#QNAN
> (bG) nan @ pos 12: y= 40.4; mu=-2.7462e+007; mu inv=-3.6414e-008; wt=1; y/mu=-1.47112e-006; log(y/mu) =1.#QNAN
> (bG) nan @ pos 12: y= 40.4; mu=-5.4924e+007; mu inv=-1.8207e-008; wt=1; y/mu=-7.35562e-007; log(y/mu) =1.#QNAN
> (bG) nan @ pos 12: y= 40.4; mu=-1.09848e+008; mu inv=-9.10349e-009; wt=1; y/mu=-3.67781e-007; log(y/mu) =1.#QNAN
> (bG) nan @ pos 12: y= 40.4; mu=-2.19696e+008; mu inv=-4.55175e-009; wt=1; y/mu=-1.83891e-007; log(y/mu) =1.#QNAN
> (bG) nan @ pos 12: y= 40.4; mu=-4.39392e+008; mu inv=-2.27587e-009; wt=1; y/mu=-9.19453e-008; log(y/mu) =1.#QNAN
> Error: (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate
> Thank you,
> Mose

When using `lme4` to fit GLMMs with link functions that do not 
automatically constrain the response to the allowable range of the 
distributional family (e.g. binomial models with a log link, where the 
estimated probability can be >1, or inverse-Gamma models, where the 
estimated mean can be negative), it is not unusual to get the error
```
PIRLS step-halvings failed to reduce deviance in pwrssUpdate
```
This occurs because `lme4` doesn't do anything to constrain the 
predicted values, so `NaN` values pop up, which aren't handled 
gracefully. If possible, switch to a link function to one that 
constrains the response (e.g. logit link for binomial or log link for 
Gamma).

===============

This also serves as an advertisement for the updated version of the GLMM 
FAQ, which I have rewritten in R markdown and am in the process of 
moving from wikidot.com to Github. For the rendered HTML, see

https://rawgit.com/bbolker/mixedmodels-misc/master/glmmFAQ.html

For the source, see 
https://github.com/bbolker/mixedmodels-misc/blob/master/glmmFAQ.rmd

Suggestions and corrections (particularly in the form of pull requests, 
but also in the form of issues 
<https://github.com/bbolker/mixedmodels-misc/issues>) welcome ...


From bbolker at gmail.com  Sat Feb 20 17:18:15 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 20 Feb 2016 16:18:15 +0000 (UTC)
Subject: [R-sig-ME] glmmADMB
References: <CALNF8VVZD9_5_KsCQs8_s7VG0aL3_7hfQ5TaEM_82cuxLzmwvQ@mail.gmail.com>
Message-ID: <loom.20160220T171716-722@post.gmane.org>

Tock Chua <chuath.ums at ...> writes:

> 
> Dear Prof Bolker
> 
> Yes, I did run (but did not include in the message I sent earlier):
> 
> > library("glmmADMB")
> > bb <- glmmADMB:::get_bin_loc()[["bin_loc"]]
> Error in get(name, envir = asNamespace(pkg), inherits = FALSE) :
>   object 'get_bin_loc' not found
> 
> Thank you for your help
> 
> ChuaTH


  I don't necessarily expect get_bin_loc() to work for
earlier versions of glmmADMB, but if you are getting
"object not found" errors when running glmmadmb(...) and
you have already run library("glmmADMB"), then something
really weird is happening ...


From bachlaw01 at outlook.com  Sun Feb 21 22:09:46 2016
From: bachlaw01 at outlook.com (Jonathan Judge)
Date: Sun, 21 Feb 2016 21:09:46 +0000
Subject: [R-sig-ME] horseshoe priors in MCMCglmm?
Message-ID: <CY1PR16MB034554011A5B5B6AA4B698C6AFA20@CY1PR16MB0345.namprd16.prod.outlook.com>

Certain authors have been reporting impressive shrinkage performance from incorporation of the so-called "horseshoe" (http://www.jmlr.org/proceedings/papers/v5/carvalho09a/carvalho09a.pdf)  and "horseshoe plus" (http://arxiv.org/pdf/1502.00560v2.pdf) priors into their MCMC sampling.

I've seen a fair amount of attention to incorporating these into Stan programming, but I still prefer to use MCMCglmm at times, particularly with large groups.  Has anyone successfully incorporated any of the horseshoe priors for usage in MCMCglmm, through parameter expansion or otherwise?

Warm regards,
Jonathan

	[[alternative HTML version deleted]]


From highstat at highstat.com  Mon Feb 22 09:23:14 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Mon, 22 Feb 2016 08:23:14 +0000
Subject: [R-sig-ME] Genoa course: Introduction to Zero Inflated Models
Message-ID: <56CAC572.2070506@highstat.com>

We would like to announce the following statistics course:

Course: Introduction to Zero Inflated Models
Where:  Italian National Antarctic Museum, Genoa, Italy
When:   9-13 May 2016

Course website: http://www.highstat.com/statscourse.htm
Course flyer: http://highstat.com/Courses/Flyers/Flyer2016_5Genoa.pdf



Kind regards,

Alain Zuur

-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL: www.highstat.com


From j.hadfield at ed.ac.uk  Mon Feb 22 17:15:25 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 22 Feb 2016 16:15:25 +0000
Subject: [R-sig-ME] horseshoe priors in MCMCglmm?
In-Reply-To: <CY1PR16MB034554011A5B5B6AA4B698C6AFA20@CY1PR16MB0345.namprd16.prod.outlook.com>
References: <CY1PR16MB034554011A5B5B6AA4B698C6AFA20@CY1PR16MB0345.namprd16.prod.outlook.com>
Message-ID: <56CB341D.6020006@ed.ac.uk>

Hi,

I could be wrong, but it looks like the horseshoe prior is a mixture of 
normals, whose standard deviations are drawn from a scaled 1-df Cauchy 
distribution.

Since 1-df Cauchy is equal to the 1-df t, and t^2 is equal to the F, 
isn't this just the same as treating each `fixed' effect as a random 
effect and using a parameter expended prior? You could trick MCMCglmm 
into doing this ...

dat<-data.frame(y=rnorm(100), x1=rnorm(100), x2=rnorm(100))

prior<-list(R=list(V=1, nu=0), G=list(G1=list(V=diag(2), nu=1, 
alpha.mu=c(0,0), V=diag(2)*100)))
# Horseshoe priors on x1 and x2 with scale=100 ?????

m1<-MCMCglmm(y~1, random=~idh(x1+x2):trait, pr=T, data=dat, prior=prior)

summary(m1, random=TRUE)

I haven't got time to check the logic thoroughly, but it might be worth 
investigating.

Cheers,

Jarrod

On 21/02/16 21:09, Jonathan Judge wrote:
> Certain authors have been reporting impressive shrinkage performance from incorporation of the so-called "horseshoe" (http://www.jmlr.org/proceedings/papers/v5/carvalho09a/carvalho09a.pdf)  and "horseshoe plus" (http://arxiv.org/pdf/1502.00560v2.pdf) priors into their MCMC sampling.
>
> I've seen a fair amount of attention to incorporating these into Stan programming, but I still prefer to use MCMCglmm at times, particularly with large groups.  Has anyone successfully incorporated any of the horseshoe priors for usage in MCMCglmm, through parameter expansion or otherwise?
>
> Warm regards,
> Jonathan
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From peggy_moore at usgs.gov  Mon Feb 22 20:28:30 2016
From: peggy_moore at usgs.gov (Moore, Peggy)
Date: Mon, 22 Feb 2016 11:28:30 -0800
Subject: [R-sig-ME] glmmADMB error: Invalid index for array
Message-ID: <CAPO7DkcnrAOfK4zLGQgBiajAbPbgWRaVEFGJo30R-0zgoAndYQ@mail.gmail.com>

I seem to have addressed the errors I was getting with glmmADMB by going to
the buildbot page for glmmADMB at R Project (instructions and link at
http://glmmadmb.r-forge.r-project.org/) and downloading the binary file for
Windows 8. I run Windows 7, but the Windows 8 version is allowing
glmmadmb() to write .psv and 3 other output files for MCMC and is not
giving me the error, "Invalid index for array."
Thank you to those who have been assisting in trying to identify the
problem.
Peggy Moore

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Mon Feb 22 20:39:58 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 22 Feb 2016 14:39:58 -0500
Subject: [R-sig-ME] glmmADMB error: Invalid index for array
In-Reply-To: <CAPO7DkcnrAOfK4zLGQgBiajAbPbgWRaVEFGJo30R-0zgoAndYQ@mail.gmail.com>
References: <CAPO7DkcnrAOfK4zLGQgBiajAbPbgWRaVEFGJo30R-0zgoAndYQ@mail.gmail.com>
Message-ID: <56CB640E.3000508@gmail.com>

   Glad you got it sorted, and thanks for the information -- should be 
helpful to future users too.

On 16-02-22 02:28 PM, Moore, Peggy wrote:
> I seem to have addressed the errors I was getting with glmmADMB by going to
> the buildbot page for glmmADMB at R Project (instructions and link at
> http://glmmadmb.r-forge.r-project.org/) and downloading the binary file for
> Windows 8. I run Windows 7, but the Windows 8 version is allowing
> glmmadmb() to write .psv and 3 other output files for MCMC and is not
> giving me the error, "Invalid index for array."
> Thank you to those who have been assisting in trying to identify the
> problem.
> Peggy Moore
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From steven.v.miller at gmail.com  Mon Feb 22 21:46:17 2016
From: steven.v.miller at gmail.com (svm)
Date: Mon, 22 Feb 2016 15:46:17 -0500
Subject: [R-sig-ME] For what can I use a correlation of fixed effects from
	(g)lmer?
Message-ID: <CABafbirrRmpH3Hd+BxZsnfRogGB353g9vUWhtA4fA5S3do9i7A@mail.gmail.com>

Hi all,

I have a question that concerns how could I possibly use a correlation of
fixed effects that comes standard with every (g)lmer call. I'll explain the
situation I'm encountering briefly.


   - I used mixed effects models mostly for cross-national survey research.
   I have both individual-level fixed effects and country-level fixed effects.
   - My interest is mostly the country-level fixed effects. The
   individual-level stuff tends to be standard "controls" that reviewers want
   to see.
   - I'm not convinced the individual-level fixed effects are entirely
   necessary. My hunch is they just make for inefficient estimates of the
   country-level fixed effects that interest me. The individual-level
   variables just create missing data problems. However, they're stuff that
   reviewers insist on seeing absent any other information about what a mixed
   effects model is doing.


I have a project (manuscript here:
https://www.dropbox.com/s/harb6ylpcxdpalr/etst.pdf?dl=0 | appendix here:
https://www.dropbox.com/s/pq8gmr7v1xvvu2h/etst-appendix.pdf?dl=0) that
reviewers rejected because the country-level fixed effects were rendered
statistically insignificant (i.e. not discernible from zero) upon the
inclusion of the individual-level variables. They said that one
individual-level attribute (which by itself contributes to listwise
deletion of 30% of the data) somehow made the country-level fixed effects
"spurious" to its inclusion. This already strikes me as a bold claim for
theoretical and statistical reasons, but here's what I did to circumvent
this claim:


   - Estimate just the country-level fixed effects.
   - Use multiple imputation to generate five full data sets and merge in
   the macro-level information after the imputation. The results for the
   country-level fixed effects were almost identical to the analyses with just
   the country-level fixed effects.
   - Omit the offending individual-level variables that contribute the most
   missingness. These results were consistent with the results from the other
   two estimation strategies.


However, the reviewers just didn't buy it and torpedoed the manuscript.

Is this something that the correlation of fixed effects could be useful in
addressing? Here's the correlation of fixed effects (without the
intercepts) for the analysis in question. In this analysis, the three
variables at the bottom row (i.e. the two threat indices and the level of
democracy) are the country-level variables for this cross-national survey
analysis. The other variables are individual-level attributes. It's worth
reiterating that every variable that is not binary is scaled by two
standard deviations to create a meaningful zero.

http://i.imgur.com/eIiZH9b.png

Notice that the bottom-left quadrant is entirely white (i.e. the
correlation of the individual-level fixed effects with the country-level
fixed effects is basically zero). Is this telling me that the correlation
for any one individual-level fixed effect and a country-level fixed effect
is almost zero (i.e. they have almost no bearing on each other)? The most
I've seen anyone discuss this correlation matrix is here:

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q1/001941.html

It is an approximate correlation of the estimator of the fixed
effects.  (I include the word "approximate" because I should but in
this case the approximation is very good.)  I'm not sure how to
explain it better than that.  Suppose that you took an MCMC sample
from the parameters in the model, then you would expect the sample of
the fixed-effects parameters to display a correlation structure like
this matrix.


and here (
http://stats.stackexchange.com/questions/57240/how-do-i-interpret-the-correlations-of-fixed-effects-in-my-glmer-output
):


The "correlation of fixed effects" output doesn't have the intuitive
meaning that most would ascribe to it. Specifically, is not about the
correlation of the variables (as OP notes). It is in fact about the
expected correlation of the regression coefficients. Although this may
speak to multicollinearity it does not necessarily.


I should add that I've estimated hundreds of mixed effects models with
individual-level and country-level variables and they all have fixed
effects correlation matrices that resemble these. I have a strong hunch
that individual-level variables don't meaningfully influence the parameter
estimates for country-level variables beyond inefficiency introduced by
missing data. In research projects where individual-level attributes don't
concern the project, I'd like to ignore them for that reason. They just
create estimation problems and slow down computation.

I might be mistaken, which is why I ask here. I thank you for your time.

- Steve

	[[alternative HTML version deleted]]


From cmammides at outlook.com  Mon Feb 22 23:39:15 2016
From: cmammides at outlook.com (christos mammides)
Date: Tue, 23 Feb 2016 00:39:15 +0200
Subject: [R-sig-ME] How to correctly specify a mixed model
Message-ID: <BLU436-SMTP152C5DE0EC8EE19A31EB24BA8A30@phx.gbl>

Dear all,

I have a possibly na?ve question on how to correctly specify a mixed 
model. I would appreciate any help you can provide.

Let?s say I have data on plant growth from several individuals from 7 
different areas (n=96), and I want to test the effect of two climatic 
variables (temperature and rain) on growth. For each of the 7 areas I 
have one measurement for temperature and one for rain. For example, the 
first few lines of my data look like this:

Individual 	Growth 	Temperature 	Rain 	Area
1 	10 	15 	300 	A
2 	12 	15 	300 	A
3 	20 	15 	300 	A
4 	16 	25 	500 	B
5 	29 	25 	500 	B
6 	10 	25 	500 	B
? 	? 	? 	? 	?

Would the following model be appropriate (in terms of the way the random 
effect is specified)?

Model <- lmer(Growth~Temperature+Rain+(1|Area), data=Data)

It was suggested to me that since I only have one measurement for each 
climatic variable per area it?s probably better to take the average of 
the plant growth for each area and run a simple regression model such as 
this: Model <- lm(AveragedGrowth~Temp+Rain, data=AveragedData).

I am right to think that in doing that I am losing information, by 
averaging my plant growth data, and I am also reducing my sample size 
(n=7) to a point that it would be too difficult to run a regression?

Hope my question makes sense.

Thank you in advance,

Christos

?

	[[alternative HTML version deleted]]


From francescobryanromano at gmail.com  Tue Feb 23 02:12:20 2016
From: francescobryanromano at gmail.com (Francesco Romano)
Date: Tue, 23 Feb 2016 02:12:20 +0100
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
Message-ID: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>

Dear all,

I'm trying to report my analysis replicating the method in the following
papers:

Cai, Pickering, and Branigan (2012). Mapping concepts to syntax: Evidence
from structural priming in Mandarin Chinese. Journal of Memory and Language 66
(2012) 833?849. (looking at pg. 842, "Combined analysis of Experiments 1
and 2" section)

Filiaci, Sorace, and Carreiras (2013). Anaphoric biases of null and overt
subjects in Italian and Spanish: a cross-linguistic comparison. Language,
Cognition, and Neuroscience  DOI:10.1080/01690965.2013.801502  (looking at
pg.11, first two paragraphs)

This is because I have a glmer model with three fixed effects, two random
intercepts modeling a binary outcome, exactly as in the articles mentioned.

The difficulty I'm finding is with locating information on commands
generating coefficients, SE, z, and p values (e.g. maximum likelihood
(Laplace Approximation)) to report main effects and interactions with the
anova or afex:mixed commands, following application of effect coding. I
have looked in several places, including Ben Bolker's FAQ
http://glmm.wikidot.com/faq and past posts on the topic in this r-sig.
Although there appears to be a plethora of material for lmer, I can't seem
to locate anything in the right direction for glmer.

Many thanks for any help.




-- 
Frank Romano Ph.D.

*LinkedIn*
https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162

*Academia.edu*
https://sheffield.academia.edu/FrancescoRomano

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Tue Feb 23 02:27:15 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 22 Feb 2016 20:27:15 -0500
Subject: [R-sig-ME] How to correctly specify a mixed model
In-Reply-To: <BLU436-SMTP152C5DE0EC8EE19A31EB24BA8A30@phx.gbl>
References: <BLU436-SMTP152C5DE0EC8EE19A31EB24BA8A30@phx.gbl>
Message-ID: <56CBB573.1050809@gmail.com>


    You should get exactly the same answer whichever way you do it (try 
it!).  The only thing you lose by aggregating is the estimate of the 
variance among observations within areas (which you might not care about 
anyway).  The advantage is a simpler model, which is easier to do 
inference on and harder to screw up. This is the idea of Murtaugh's 2007 
paper in Ecology, "Simplicity and Complexity in Ecological Data 
Analysis".  The only reasons *not* to aggregate would be:

- you're interested in the within-area variance;
- you're doing a GLMM (count/binary responses can't always be aggregated 
as simply as Normal responses)
- you have individual-level covariates that vary within areas
- you have unbalanced data (this can be often be handled by assigning 
non-equal weights)

   A sample size of 7 is indeed somewhat low for a regression with 2 
inputs, but whether you aggregate or not won't make a difference.

On 16-02-22 05:39 PM, christos mammides wrote:
> Dear all,
>
> I have a possibly na?ve question on how to correctly specify a mixed
> model. I would appreciate any help you can provide.
>
> Let?s say I have data on plant growth from several individuals from 7
> different areas (n=96), and I want to test the effect of two climatic
> variables (temperature and rain) on growth. For each of the 7 areas I
> have one measurement for temperature and one for rain. For example, the
> first few lines of my data look like this:
>
> Individual 	Growth 	Temperature 	Rain 	Area
> 1 	10 	15 	300 	A
> 2 	12 	15 	300 	A
> 3 	20 	15 	300 	A
> 4 	16 	25 	500 	B
> 5 	29 	25 	500 	B
> 6 	10 	25 	500 	B
> ? 	? 	? 	? 	?
>
> Would the following model be appropriate (in terms of the way the random
> effect is specified)?
>
> Model <- lmer(Growth~Temperature+Rain+(1|Area), data=Data)
>
> It was suggested to me that since I only have one measurement for each
> climatic variable per area it?s probably better to take the average of
> the plant growth for each area and run a simple regression model such as
> this: Model <- lm(AveragedGrowth~Temp+Rain, data=AveragedData).
>
> I am right to think that in doing that I am losing information, by
> averaging my plant growth data, and I am also reducing my sample size
> (n=7) to a point that it would be too difficult to run a regression?
>
> Hope my question makes sense.
>
> Thank you in advance,
>
> Christos
>
> ?
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From ludovicofrate at hotmail.it  Tue Feb 23 09:58:39 2016
From: ludovicofrate at hotmail.it (Ludovico Frate)
Date: Tue, 23 Feb 2016 09:58:39 +0100
Subject: [R-sig-ME] glmmadmb beta - error
Message-ID: <DUB126-W5256D4E695B95DA5076216D6A40@phx.gbl>

Dear list,I have got the following error message trying to fit this model:
glmmadmb<-(cover~time+summit+direction+(1|plot_id), family = "beta", data = cover_lf)
Error in glmmadmb(cover~time+summit+direction+(1|plot_id),  :   The function maximizer failed (couldn't find parameter file) Troubleshooting steps include (1) run with 'save.dir' set and inspect output files; (2) change run parameters: see '?admbControl';(3) re-run with debug=TRUE for more information on failure modeIn addition: Warning message:running command 'C:\Windows\system32\cmd.exe /c glmmadmb -maxfn 500 -maxph 5 -noinit -shess' had status 1 
str(cover_lf)'data.frame':	50 obs. of  5 variables: $ plot_id  : Factor w/ 48 levels "FEME11","FEME13",..: 5 6 7 8 1 2 3 4 9 10 ... $ summit   : Factor w/ 2 levels "FEM","MAC": 1 1 1 1 1 1 1 1 1 1 ... $ direction: Factor w/ 4 levels "E","N","S","W": 2 2 2 2 1 1 1 1 3 3 ... $ time     : Factor w/ 2 levels "2001","2015": 1 1 1 1 1 1 1 1 1 1 ... $ cover   : num  0.023 0.071 0.063 0.014 0.003 0.035 0.055 0.01 0.065 0.04 ...
cover is the dependent variable that is percentage of cover (0-100) divided by 100 to match the beta distribution. The random effect was included to allow repeated measure on each plot for two time period (time treated as factor).
Thank you in advance,Regards
Ludovico


                                                                                                                                
Dott. For. Ludovico
Frate, Ph.D.
University of Molise - Italy
Environmetrics Lab
http://www.distat.unimol.it/STAT/environmetrica/organico/collaboratori/ludovico-frate-1
Department of Biosciences and Territory - DiBT
Universit? del Molise.
Contrada Fonte
Lappone, 
86090 -  Pesche (IS)
ITALIA.
Cel: ++39
3333767557
Fax: ++39 (0874) 404123
E-mail ludovico.frate at unimol.it
ludovicofrate at hotmail.it
https://www.researchgate.net/profile/Ludovico_Frate
 		 	   		  

From singmann at psychologie.uzh.ch  Tue Feb 23 10:28:58 2016
From: singmann at psychologie.uzh.ch (Henrik Singmann)
Date: Tue, 23 Feb 2016 10:28:58 +0100
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
Message-ID: <56CC265A.2040008@psychologie.uzh.ch>

Hi Francesco,

As far as I see it, there are basically two ways to get these tests easily.

1) afex::mixed()
You simple pass your model as you would to glmer() while also specifying 
that you want likelihood-ratio tests as method for testing the effects, 
e.g.,
m1 <- mixed(x~y + (y|z), data = d, method = "LRT", family = binomial)
m1 # prints tests of effects
nice(m1) # same, see also anova(m1)
summary(m1) # shows summary of full model

Note that mixed already takes care of the correct coding (i.e., uses 
sum.coding as default) and uses type 3 tests as default. You can also 
obtain bootstrapped p-values instead of approximate likelihood ratio 
tests by specifying method = "PB" in the call to mixed (but this might 
take some time, see the mixed examples of how to use multicore).

2) car::Anova()
You first need to fit the model but need to specify the correct coding 
and then pass it to car::Anova() to obtain Wald-tests of the effects, e.g.,:

afex::set_sum_contrasts() # sets sum coding globally
m2 <- glmer(x~y + (y|z), data = d, family = binomial)
car::Anova(m2, type = 3)


The difference between the two options is that mixed() fits a model for 
each effect to be tested while car::Anova() only reuqires one model to 
be fitted. The consequence is that the former might take quite longer 
(again, you can distribute the fitting of the individual models across 
cores, see the mixed examples). On the other hand, 
likelihood-ratio-tests are in principle less problematic than Wald tests 
(http://glmm.wikidot.com/faq#toc4).

Finally, mixed allows to easily suppress the estimation of correlation 
among random slopes for factors by using the expand_re argument which 
may help with model convergence:
m3 <- mixed(x~y + (y||z), data = d, method = "LRT", family = binomial, 
expand_re = TRUE)

What I would report would probably the Chi-square values and associated 
p-values for the tests of effects. Reporting coefficients and associated 
standard errors usually only makes sense if there are two groups. And 
for interactions they can also be difficult to interpret. So I would go 
with the chi-square values in the first place.

Hope that helps,
Henrik


Am 23.02.2016 um 02:12 schrieb Francesco Romano:
> Dear all,
>
> I'm trying to report my analysis replicating the method in the following
> papers:
>
> Cai, Pickering, and Branigan (2012). Mapping concepts to syntax: Evidence
> from structural priming in Mandarin Chinese. Journal of Memory and Language 66
> (2012) 833?849. (looking at pg. 842, "Combined analysis of Experiments 1
> and 2" section)
>
> Filiaci, Sorace, and Carreiras (2013). Anaphoric biases of null and overt
> subjects in Italian and Spanish: a cross-linguistic comparison. Language,
> Cognition, and Neuroscience  DOI:10.1080/01690965.2013.801502  (looking at
> pg.11, first two paragraphs)
>
> This is because I have a glmer model with three fixed effects, two random
> intercepts modeling a binary outcome, exactly as in the articles mentioned.
>
> The difficulty I'm finding is with locating information on commands
> generating coefficients, SE, z, and p values (e.g. maximum likelihood
> (Laplace Approximation)) to report main effects and interactions with the
> anova or afex:mixed commands, following application of effect coding. I
> have looked in several places, including Ben Bolker's FAQ
> http://glmm.wikidot.com/faq and past posts on the topic in this r-sig.
> Although there appears to be a plethora of material for lmer, I can't seem
> to locate anything in the right direction for glmer.
>
> Many thanks for any help.
>
>
>
>


From francescobryanromano at gmail.com  Tue Feb 23 10:33:10 2016
From: francescobryanromano at gmail.com (Francesco Romano)
Date: Tue, 23 Feb 2016 10:33:10 +0100
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
Message-ID: <CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>

Yes. An ANOVA with my final bglmer model yields:

> anova(recallmodel4x6a)

Analysis of Variance Table

                   Df Sum Sq Mean Sq F value
syntax12            1 1.7670  1.7670  1.7670
animacy12           1 3.4036  3.4036  3.4036
group123            2 5.7213  2.8607  2.8607
animacy12:group123  2 4.5546  2.2773  2.2773
syntax12:group123   2 8.1732  4.0866  4.0866

which is counterintuitively not what the authors of the papers
apparently used to generate coefficients to report their main effects
and interactions. It looks to me more like ML fitting. Elsewhere,
and more typically, main effects and interactions are obtained by comparing
a

model with the main fixed effect to a model without the

main fixed effect in terms of log-likelihood ratio tests

(Raffray et al., 2013, http://dx.doi.org/10.1016/j.jml.2013.09.004, p.6).


I understand obtaining p-values from a summary
of linear mixed models fit by lmer is a contentious issue

https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html

but I guess I might be missing something here.






On Tue, Feb 23, 2016 at 2:21 AM, Phillip Alday <Phillip.Alday at unisa.edu.au>
wrote:

> Have you looked at car::Anova() ?
>
> Best,
> Phillip
>
> [forgot to cc the list]
>
> > On 23 Feb 2016, at 11:42, Francesco Romano <
> francescobryanromano at gmail.com> wrote:
> >
> > Dear all,
> >
> > I'm trying to report my analysis replicating the method in the following
> > papers:
> >
> > Cai, Pickering, and Branigan (2012). Mapping concepts to syntax: Evidence
> > from structural priming in Mandarin Chinese. Journal of Memory and
> Language 66
> > (2012) 833?849. (looking at pg. 842, "Combined analysis of Experiments 1
> > and 2" section)
> >
> > Filiaci, Sorace, and Carreiras (2013). Anaphoric biases of null and overt
> > subjects in Italian and Spanish: a cross-linguistic comparison. Language,
> > Cognition, and Neuroscience  DOI:10.1080/01690965.2013.801502  (looking
> at
> > pg.11, first two paragraphs)
> >
> > This is because I have a glmer model with three fixed effects, two random
> > intercepts modeling a binary outcome, exactly as in the articles
> mentioned.
> >
> > The difficulty I'm finding is with locating information on commands
> > generating coefficients, SE, z, and p values (e.g. maximum likelihood
> > (Laplace Approximation)) to report main effects and interactions with the
> > anova or afex:mixed commands, following application of effect coding. I
> > have looked in several places, including Ben Bolker's FAQ
> > http://glmm.wikidot.com/faq and past posts on the topic in this r-sig.
> > Although there appears to be a plethora of material for lmer, I can't
> seem
> > to locate anything in the right direction for glmer.
> >
> > Many thanks for any help.
> >
> >
> >
> >
> > --
> > Frank Romano Ph.D.
> >
> > *LinkedIn*
> > https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162
> >
> > *Academia.edu*
> > https://sheffield.academia.edu/FrancescoRomano
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


-- 
Frank Romano Ph.D.

Tel. +39 3911639149

*LinkedIn*
https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162

*Academia.edu*
https://sheffield.academia.edu/FrancescoRomano

	[[alternative HTML version deleted]]


From Phillip.Alday at unisa.edu.au  Tue Feb 23 10:51:37 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Tue, 23 Feb 2016 09:51:37 +0000
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
Message-ID: <818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>

lme4:anova() is not the same thing as car::Anova()!

A quick R note that might have avoided the confusion:
The :: syntax in R refers to scope, so you can specify a function
unambiguously via package::function.name(). Moreover, R is case
sensitive, so Anova() and anova() are generally different things.

Henrik's message (posted to the list so if you don't suscribe, you need
to look here:
https://mailman.stat.ethz.ch/pipermail/r-sig-mixed-models/2016q1/024465.html
) describes how to do this with either his afex package (for
likelihood-ratio tests) or John Fox's car package (for analysis of
deviance / Wald tests).

If you just want to perform likelihood-ratio tests in lme4, then you
should look at the drop1() function or you can use anova(reduced.model,
full.model). Henrik also does a nice job summarizing some of the issues
here, so I won't repeat them.

One final note: not everything that holds for normal LMM holds for GLMM
-- GLMM tends to be much more complicated. :-(

Best,
Phillip

On 23/02/16 20:03, Francesco Romano wrote:
> Yes. An ANOVA with my final bglmer model yields:
> 
>> anova(recallmodel4x6a)
> 
> Analysis of Variance Table
> 
>                    Df Sum Sq Mean Sq F value
> syntax12            1 1.7670  1.7670  1.7670
> animacy12           1 3.4036  3.4036  3.4036
> group123            2 5.7213  2.8607  2.8607
> animacy12:group123  2 4.5546  2.2773  2.2773
> syntax12:group123   2 8.1732  4.0866  4.0866
> 
> which is counterintuitively not what the authors of the papers 
> apparently used to generate coefficients to report their main effects 
> and interactions. It looks to me more like ML fitting. Elsewhere, 
> and more typically, main effects and interactions are obtained by
> comparing a
> 
> model with the main fixed effect to a model without the
> 
> main fixed effect in terms of log-likelihood ratio tests 
> 
> (Raffray et al., 2013, http://dx.doi.org/10.1016/j.jml.2013.09.004, p.6).
> 
> 
> I understand obtaining p-values from a summary
> of linear mixed models fit by lmer is a contentious issue
> 
> https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html
> 
> but I guess I might be missing something here.
> 
> 
> 
> 
> 
> 
> On Tue, Feb 23, 2016 at 2:21 AM, Phillip Alday
> <Phillip.Alday at unisa.edu.au <mailto:Phillip.Alday at unisa.edu.au>> wrote:
> 
>     Have you looked at car::Anova() ?
> 
>     Best,
>     Phillip
> 
>     [forgot to cc the list]
> 
>     > On 23 Feb 2016, at 11:42, Francesco Romano <francescobryanromano at gmail.com
>     <mailto:francescobryanromano at gmail.com>> wrote:
>     >
>     > Dear all,
>     >
>     > I'm trying to report my analysis replicating the method in the
>     following
>     > papers:
>     >
>     > Cai, Pickering, and Branigan (2012). Mapping concepts to syntax:
>     Evidence
>     > from structural priming in Mandarin Chinese. Journal of Memory and
>     Language 66
>     > (2012) 833?849 <tel:%282012%29%20833%E2%80%93849>. (looking at pg.
>     842, "Combined analysis of Experiments 1
>     > and 2" section)
>     >
>     > Filiaci, Sorace, and Carreiras (2013). Anaphoric biases of null
>     and overt
>     > subjects in Italian and Spanish: a cross-linguistic comparison.
>     Language,
>     > Cognition, and Neuroscience  DOI:10.1080/01690965.2013.801502 
>     (looking at
>     > pg.11, first two paragraphs)
>     >
>     > This is because I have a glmer model with three fixed effects, two
>     random
>     > intercepts modeling a binary outcome, exactly as in the articles
>     mentioned.
>     >
>     > The difficulty I'm finding is with locating information on commands
>     > generating coefficients, SE, z, and p values (e.g. maximum likelihood
>     > (Laplace Approximation)) to report main effects and interactions
>     with the
>     > anova or afex:mixed commands, following application of effect
>     coding. I
>     > have looked in several places, including Ben Bolker's FAQ
>     > http://glmm.wikidot.com/faq and past posts on the topic in this r-sig.
>     > Although there appears to be a plethora of material for lmer, I
>     can't seem
>     > to locate anything in the right direction for glmer.
>     >
>     > Many thanks for any help.
>     >
>     >
>     >
>     >
>     > --
>     > Frank Romano Ph.D.
>     >
>     > *LinkedIn*
>     > https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162
>     >
>     > *Academia.edu*
>     > https://sheffield.academia.edu/FrancescoRomano
>     >
>     >       [[alternative HTML version deleted]]
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 
> 
> -- 
> Frank Romano Ph.D.
> 
> Tel. +39 3911639149
> 
> /LinkedIn/
> https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162
> 
> /Academia.edu/
> https://sheffield.academia.edu/FrancescoRomano


From highstat at highstat.com  Tue Feb 23 12:14:55 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Tue, 23 Feb 2016 11:14:55 +0000
Subject: [R-sig-ME] glmmadmb beta - error
In-Reply-To: <mailman.1474.1456221132.3865.r-sig-mixed-models@r-project.org>
References: <mailman.1474.1456221132.3865.r-sig-mixed-models@r-project.org>
Message-ID: <56CC3F2F.7080707@highstat.com>




>
> ------------------------------
>
> Message: 2
> Date: Tue, 23 Feb 2016 09:58:39 +0100
> From: Ludovico Frate <ludovicofrate at hotmail.it>
> To: "r-sig-mixed-models at r-project.org"
> 	<r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] glmmadmb beta - error
> Message-ID: <DUB126-W5256D4E695B95DA5076216D6A40 at phx.gbl>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Dear list,I have got the following error message trying to fit this model:
> glmmadmb<-(cover~time+summit+direction+(1|plot_id), family = "beta", data = cover_lf)
> Error in glmmadmb(cover~time+summit+direction+(1|plot_id),  :   The function maximizer failed (couldn't find parameter file) Troubleshooting steps include (1) run with 'save.dir' set and inspect output files; (2) change run parameters: see '?admbControl';(3) re-run with debug=TRUE for more information on failure modeIn addition: Warning message:running command 'C:\Windows\system32\cmd.exe /c glmmadmb -maxfn 500 -maxph 5 -noinit -shess' had status 1
> str(cover_lf)'data.frame':	50 obs. of  5 variables: $ plot_id  : Factor w/ 48 levels "FEME11","FEME13",..: 5 6 7 8 1 2 3 4 9 10 ... $ summit   : Factor w/ 2 levels "FEM","MAC": 1 1 1 1 1 1 1 1 1 1 ... $ direction: Factor w/ 4 levels "E","N","S","W": 2 2 2 2 1 1 1 1 3 3 ... $ time     : Factor w/ 2 levels "2001","2015": 1 1 1 1 1 1 1 1 1 1 ... $ cover   : num  0.023 0.071 0.063 0.014 0.003 0.035 0.055 0.01 0.065 0.04 ...
> cover is the dependent variable that is percentage of cover (0-100) divided by 100 to match the beta distribution. The random effect was included to allow repeated measure on each plot for two time period (time treated as factor).
> Thank you in advance,Regards
> Ludovico

Do I read correctly in the error message the phrase '50 obs. of 5 
variables"? Does that mean that you only have 50 observations? If that 
is the case then it is no wonder that a beta model with random effects 
(48 levels?) and 6 or 7 regressions parameters is crashing.

Kind regards,

Alain
>


>                                                                                                                                  
> Dott. For. Ludovico
> Frate, Ph.D.
> University of Molise - Italy
> Environmetrics Lab
> http://www.distat.unimol.it/STAT/environmetrica/organico/collaboratori/ludovico-frate-1
> Department of Biosciences and Territory - DiBT
> Universit? del Molise.
> Contrada Fonte
> Lappone,
> 86090 -  Pesche (IS)
> ITALIA.
> Cel: ++39
> 3333767557
> Fax: ++39 (0874) 404123
> E-mail ludovico.frate at unimol.it
> ludovicofrate at hotmail.it
> https://www.researchgate.net/profile/Ludovico_Frate
>   		


>

-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From ludovicofrate at hotmail.it  Tue Feb 23 13:01:47 2016
From: ludovicofrate at hotmail.it (Ludovico Frate)
Date: Tue, 23 Feb 2016 13:01:47 +0100
Subject: [R-sig-ME] glmmadmb beta - error
In-Reply-To: <56CC3F2F.7080707@highstat.com>
References: <mailman.1474.1456221132.3865.r-sig-mixed-models@r-project.org>,
	<56CC3F2F.7080707@highstat.com>
Message-ID: <DUB126-W560053D590B0CCCF1E25CD6A40@phx.gbl>

I'm sorry,the structure of the data is
data.frame':	50 obs. of  5 variables: $ plot_id  : Factor w/ 25 levels "FEME11","FEME13",..: 5 6 7 8 1 2 3 4 9 10 ... $ summit   : Factor w/ 2 levels "FEM","MAC": 1 1 1 1 1 1 1 1 1 1 ... $ direction: Factor w/ 4 levels "E","N","S","W": 2 2 2 2 1 1 1 1 3 3 ... $ time     : int  2001 2001 2001 2001 2001 2001 2001 2001 2001 2001 ... $ cover    : num  0.023 0.071 0.063 0.014 0.003 0.035 0.055 0.01 0.065 0.04 ...
Ludovico
                                                                                                                                
Dott. For. Ludovico
Frate, Ph.D.
University of Molise - Italy
Environmetrics Lab
http://www.distat.unimol.it/STAT/environmetrica/organico/collaboratori/ludovico-frate-1
Department of Biosciences and Territory - DiBT
Universit? del Molise.
Contrada Fonte
Lappone, 
86090 -  Pesche (IS)
ITALIA.
Cel: ++39
3333767557
Fax: ++39 (0874) 404123
E-mail ludovico.frate at unimol.it
ludovicofrate at hotmail.it
https://www.researchgate.net/profile/Ludovico_Frate


> To: r-sig-mixed-models at r-project.org
> From: highstat at highstat.com
> Date: Tue, 23 Feb 2016 11:14:55 +0000
> Subject: Re: [R-sig-ME] glmmadmb beta - error
> 
> 
> 
> 
> >
> > ------------------------------
> >
> > Message: 2
> > Date: Tue, 23 Feb 2016 09:58:39 +0100
> > From: Ludovico Frate <ludovicofrate at hotmail.it>
> > To: "r-sig-mixed-models at r-project.org"
> > 	<r-sig-mixed-models at r-project.org>
> > Subject: [R-sig-ME] glmmadmb beta - error
> > Message-ID: <DUB126-W5256D4E695B95DA5076216D6A40 at phx.gbl>
> > Content-Type: text/plain; charset="iso-8859-1"
> >
> > Dear list,I have got the following error message trying to fit this model:
> > glmmadmb<-(cover~time+summit+direction+(1|plot_id), family = "beta", data = cover_lf)
> > Error in glmmadmb(cover~time+summit+direction+(1|plot_id),  :   The function maximizer failed (couldn't find parameter file) Troubleshooting steps include (1) run with 'save.dir' set and inspect output files; (2) change run parameters: see '?admbControl';(3) re-run with debug=TRUE for more information on failure modeIn addition: Warning message:running command 'C:\Windows\system32\cmd.exe /c glmmadmb -maxfn 500 -maxph 5 -noinit -shess' had status 1
> > str(cover_lf)'data.frame':	50 obs. of  5 variables: $ plot_id  : Factor w/ 48 levels "FEME11","FEME13",..: 5 6 7 8 1 2 3 4 9 10 ... $ summit   : Factor w/ 2 levels "FEM","MAC": 1 1 1 1 1 1 1 1 1 1 ... $ direction: Factor w/ 4 levels "E","N","S","W": 2 2 2 2 1 1 1 1 3 3 ... $ time     : Factor w/ 2 levels "2001","2015": 1 1 1 1 1 1 1 1 1 1 ... $ cover   : num  0.023 0.071 0.063 0.014 0.003 0.035 0.055 0.01 0.065 0.04 ...
> > cover is the dependent variable that is percentage of cover (0-100) divided by 100 to match the beta distribution. The random effect was included to allow repeated measure on each plot for two time period (time treated as factor).
> > Thank you in advance,Regards
> > Ludovico
> 
> Do I read correctly in the error message the phrase '50 obs. of 5 
> variables"? Does that mean that you only have 50 observations? If that 
> is the case then it is no wonder that a beta model with random effects 
> (48 levels?) and 6 or 7 regressions parameters is crashing.
> 
> Kind regards,
> 
> Alain
> >
> 
> 
> >                                                                                                                                  
> > Dott. For. Ludovico
> > Frate, Ph.D.
> > University of Molise - Italy
> > Environmetrics Lab
> > http://www.distat.unimol.it/STAT/environmetrica/organico/collaboratori/ludovico-frate-1
> > Department of Biosciences and Territory - DiBT
> > Universit? del Molise.
> > Contrada Fonte
> > Lappone,
> > 86090 -  Pesche (IS)
> > ITALIA.
> > Cel: ++39
> > 3333767557
> > Fax: ++39 (0874) 404123
> > E-mail ludovico.frate at unimol.it
> > ludovicofrate at hotmail.it
> > https://www.researchgate.net/profile/Ludovico_Frate
> >   		
> 
> 
> >
> 
> -- 
> Dr. Alain F. Zuur
> 
> First author of:
> 1. Beginner's Guide to GAMM with R (2014).
> 2. Beginner's Guide to GLM and GLMM with R (2013).
> 3. Beginner's Guide to GAM with R (2012).
> 4. Zero Inflated Models and GLMM with R (2012).
> 5. A Beginner's Guide to R (2009).
> 6. Mixed effects models and extensions in ecology with R (2009).
> 7. Analysing Ecological Data (2007).
> 
> Highland Statistics Ltd.
> 9 St Clair Wynd
> UK - AB41 6DZ Newburgh
> Tel:   0044 1358 788177
> Email: highstat at highstat.com
> URL:   www.highstat.com
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
 		 	   		  
	[[alternative HTML version deleted]]


From francescobryanromano at gmail.com  Tue Feb 23 13:06:18 2016
From: francescobryanromano at gmail.com (Francesco Romano)
Date: Tue, 23 Feb 2016 13:06:18 +0100
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
Message-ID: <CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>

Thanks to Henrik and Phillip for the quick reply.
Your suggestions have been helpful in making progress.

On the one hand Henrik is right about
reporting coefficients and standard errors when
there are only two levels for the each predictor. This is
consistent with two of the sources I mentioned so far.
I infer that the authors reported directly from the summary(m1)
after use of the mixed function (not car::Anova which yields chi
square tests).

On the other hand, I don't understand how Cai et al. (2012) p.842,
"combined analysis experiments 1 and 2", reported the main effect
of a factor with 4 levels via a single estimate, SE, z, p coefficient.
How did they obtain this and is this the right way?

Finally, after running analysis both ways, I get slightly different
p-values, with the car::Anova method being more conservative
(it yields less significant predictors). Is this normal?

Frank



On Tue, Feb 23, 2016 at 10:51 AM, Phillip Alday <Phillip.Alday at unisa.edu.au>
wrote:

> lme4:anova() is not the same thing as car::Anova()!
>
> A quick R note that might have avoided the confusion:
> The :: syntax in R refers to scope, so you can specify a function
> unambiguously via package::function.name(). Moreover, R is case
> sensitive, so Anova() and anova() are generally different things.
>
> Henrik's message (posted to the list so if you don't suscribe, you need
> to look here:
>
> https://mailman.stat.ethz.ch/pipermail/r-sig-mixed-models/2016q1/024465.html
> ) describes how to do this with either his afex package (for
> likelihood-ratio tests) or John Fox's car package (for analysis of
> deviance / Wald tests).
>
> If you just want to perform likelihood-ratio tests in lme4, then you
> should look at the drop1() function or you can use anova(reduced.model,
> full.model). Henrik also does a nice job summarizing some of the issues
> here, so I won't repeat them.
>
> One final note: not everything that holds for normal LMM holds for GLMM
> -- GLMM tends to be much more complicated. :-(
>
> Best,
> Phillip
>
> On 23/02/16 20:03, Francesco Romano wrote:
> > Yes. An ANOVA with my final bglmer model yields:
> >
> >> anova(recallmodel4x6a)
> >
> > Analysis of Variance Table
> >
> >                    Df Sum Sq Mean Sq F value
> > syntax12            1 1.7670  1.7670  1.7670
> > animacy12           1 3.4036  3.4036  3.4036
> > group123            2 5.7213  2.8607  2.8607
> > animacy12:group123  2 4.5546  2.2773  2.2773
> > syntax12:group123   2 8.1732  4.0866  4.0866
> >
> > which is counterintuitively not what the authors of the papers
> > apparently used to generate coefficients to report their main effects
> > and interactions. It looks to me more like ML fitting. Elsewhere,
> > and more typically, main effects and interactions are obtained by
> > comparing a
> >
> > model with the main fixed effect to a model without the
> >
> > main fixed effect in terms of log-likelihood ratio tests
> >
> > (Raffray et al., 2013, http://dx.doi.org/10.1016/j.jml.2013.09.004,
> p.6).
> >
> >
> > I understand obtaining p-values from a summary
> > of linear mixed models fit by lmer is a contentious issue
> >
> > https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html
> >
> > but I guess I might be missing something here.
> >
> >
> >
> >
> >
> >
> > On Tue, Feb 23, 2016 at 2:21 AM, Phillip Alday
> > <Phillip.Alday at unisa.edu.au <mailto:Phillip.Alday at unisa.edu.au>> wrote:
> >
> >     Have you looked at car::Anova() ?
> >
> >     Best,
> >     Phillip
> >
> >     [forgot to cc the list]
> >
> >     > On 23 Feb 2016, at 11:42, Francesco Romano <
> francescobryanromano at gmail.com
> >     <mailto:francescobryanromano at gmail.com>> wrote:
> >     >
> >     > Dear all,
> >     >
> >     > I'm trying to report my analysis replicating the method in the
> >     following
> >     > papers:
> >     >
> >     > Cai, Pickering, and Branigan (2012). Mapping concepts to syntax:
> >     Evidence
> >     > from structural priming in Mandarin Chinese. Journal of Memory and
> >     Language 66
> >     > (2012) 833?849 <tel:%282012%29%20833%E2%80%93849>. (looking at pg.
> >     842, "Combined analysis of Experiments 1
> >     > and 2" section)
> >     >
> >     > Filiaci, Sorace, and Carreiras (2013). Anaphoric biases of null
> >     and overt
> >     > subjects in Italian and Spanish: a cross-linguistic comparison.
> >     Language,
> >     > Cognition, and Neuroscience  DOI:10.1080/01690965.2013.801502
> >     (looking at
> >     > pg.11, first two paragraphs)
> >     >
> >     > This is because I have a glmer model with three fixed effects, two
> >     random
> >     > intercepts modeling a binary outcome, exactly as in the articles
> >     mentioned.
> >     >
> >     > The difficulty I'm finding is with locating information on commands
> >     > generating coefficients, SE, z, and p values (e.g. maximum
> likelihood
> >     > (Laplace Approximation)) to report main effects and interactions
> >     with the
> >     > anova or afex:mixed commands, following application of effect
> >     coding. I
> >     > have looked in several places, including Ben Bolker's FAQ
> >     > http://glmm.wikidot.com/faq and past posts on the topic in this
> r-sig.
> >     > Although there appears to be a plethora of material for lmer, I
> >     can't seem
> >     > to locate anything in the right direction for glmer.
> >     >
> >     > Many thanks for any help.
> >     >
> >     >
> >     >
> >     >
> >     > --
> >     > Frank Romano Ph.D.
> >     >
> >     > *LinkedIn*
> >     > https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162
> >     >
> >     > *Academia.edu*
> >     > https://sheffield.academia.edu/FrancescoRomano
> >     >
> >     >       [[alternative HTML version deleted]]
> >     >
> >     > _______________________________________________
> >     > R-sig-mixed-models at r-project.org
> >     <mailto:R-sig-mixed-models at r-project.org> mailing list
> >     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> >
> >
> >
> > --
> > Frank Romano Ph.D.
> >
> > Tel. +39 3911639149
> >
> > /LinkedIn/
> > https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162
> >
> > /Academia.edu/
> > https://sheffield.academia.edu/FrancescoRomano
>



-- 
Frank Romano Ph.D.

Tel. +39 3911639149

*LinkedIn*
https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162

*Academia.edu*
https://sheffield.academia.edu/FrancescoRomano

	[[alternative HTML version deleted]]


From highstat at highstat.com  Tue Feb 23 13:08:29 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Tue, 23 Feb 2016 12:08:29 +0000
Subject: [R-sig-ME] glmmadmb beta - error
In-Reply-To: <DUB126-W560053D590B0CCCF1E25CD6A40@phx.gbl>
References: <mailman.1474.1456221132.3865.r-sig-mixed-models@r-project.org>
	<56CC3F2F.7080707@highstat.com>
	<DUB126-W560053D590B0CCCF1E25CD6A40@phx.gbl>
Message-ID: <56CC4BBD.3030901@highstat.com>

glmmadmb was just telling you in a nice way to get more data. Normally, 
I would say 'try to simplify your model'....but even then you may have 
problems with such a small data set.

You are having around 6 regression parameters, a beta distribution and a 
random effect that has around 2 observations per level.

Alain




On 23/02/2016 12:01, Ludovico Frate wrote:
> I'm sorry,
> the structure of the data is
>
> data.frame':50 obs. of  5 variables:
> $ plot_id  : Factor w/ 25 levels "FEME11","FEME13",..: 5 6 7 8 1 2 3 4 
> 9 10 ...
>  $ summit   : Factor w/ 2 levels "FEM","MAC": 1 1 1 1 1 1 1 1 1 1 ...
>  $ direction: Factor w/ 4 levels "E","N","S","W": 2 2 2 2 1 1 1 1 3 3 ...
>  $ time     : int  2001 2001 2001 2001 2001 2001 2001 2001 2001 2001 ...
>  $ cover    : num  0.023 0.071 0.063 0.014 0.003 0.035 0.055 0.01 
> 0.065 0.04 ...
>
> Ludovico
> __
> Dott. For. Ludovico Frate, Ph.D.
> University of Molise - Italy
> Environmetrics Lab
> http://www.distat.unimol.it/STAT/environmetrica/organico/collaboratori/ludovico-frate-1
> Department of Biosciences and Territory- DiBT
> Universit? del Molise.
> Contrada Fonte Lappone,
> 86090 - Pesche (IS)
> ITALIA.
> Cel: ++39 3333767557
> Fax: ++39 (0874) 404123
> E-mail ludovico.frate at unimol.it
> ludovicofrate at hotmail.it
> https://www.researchgate.net/profile/Ludovico_Frate
>
>
> > To: r-sig-mixed-models at r-project.org
> > From: highstat at highstat.com
> > Date: Tue, 23 Feb 2016 11:14:55 +0000
> > Subject: Re: [R-sig-ME] glmmadmb beta - error
> >
> >
> >
> >
> > >
> > > ------------------------------
> > >
> > > Message: 2
> > > Date: Tue, 23 Feb 2016 09:58:39 +0100
> > > From: Ludovico Frate <ludovicofrate at hotmail.it>
> > > To: "r-sig-mixed-models at r-project.org"
> > > <r-sig-mixed-models at r-project.org>
> > > Subject: [R-sig-ME] glmmadmb beta - error
> > > Message-ID: <DUB126-W5256D4E695B95DA5076216D6A40 at phx.gbl>
> > > Content-Type: text/plain; charset="iso-8859-1"
> > >
> > > Dear list,I have got the following error message trying to fit 
> this model:
> > > glmmadmb<-(cover~time+summit+direction+(1|plot_id), family = 
> "beta", data = cover_lf)
> > > Error in glmmadmb(cover~time+summit+direction+(1|plot_id), : The 
> function maximizer failed (couldn't find parameter file) 
> Troubleshooting steps include (1) run with 'save.dir' set and inspect 
> output files; (2) change run parameters: see '?admbControl';(3) re-run 
> with debug=TRUE for more information on failure modeIn addition: 
> Warning message:running command 'C:\Windows\system32\cmd.exe /c 
> glmmadmb -maxfn 500 -maxph 5 -noinit -shess' had status 1
> > > str(cover_lf)'data.frame': 50 obs. of 5 variables: $ plot_id : 
> Factor w/ 48 levels "FEME11","FEME13",..: 5 6 7 8 1 2 3 4 9 10 ... $ 
> summit : Factor w/ 2 levels "FEM","MAC": 1 1 1 1 1 1 1 1 1 1 ... $ 
> direction: Factor w/ 4 levels "E","N","S","W": 2 2 2 2 1 1 1 1 3 3 ... 
> $ time : Factor w/ 2 levels "2001","2015": 1 1 1 1 1 1 1 1 1 1 ... $ 
> cover : num 0.023 0.071 0.063 0.014 0.003 0.035 0.055 0.01 0.065 0.04 ...
> > > cover is the dependent variable that is percentage of cover 
> (0-100) divided by 100 to match the beta distribution. The random 
> effect was included to allow repeated measure on each plot for two 
> time period (time treated as factor).
> > > Thank you in advance,Regards
> > > Ludovico
> >
> > Do I read correctly in the error message the phrase '50 obs. of 5
> > variables"? Does that mean that you only have 50 observations? If that
> > is the case then it is no wonder that a beta model with random effects
> > (48 levels?) and 6 or 7 regressions parameters is crashing.
> >
> > Kind regards,
> >
> > Alain
> > >
> >
> >
> > >
> > > Dott. For. Ludovico
> > > Frate, Ph.D.
> > > University of Molise - Italy
> > > Environmetrics Lab
> > > 
> http://www.distat.unimol.it/STAT/environmetrica/organico/collaboratori/ludovico-frate-1
> > > Department of Biosciences and Territory - DiBT
> > > Universit? del Molise.
> > > Contrada Fonte
> > > Lappone,
> > > 86090 - Pesche (IS)
> > > ITALIA.
> > > Cel: ++39
> > > 3333767557
> > > Fax: ++39 (0874) 404123
> > > E-mail ludovico.frate at unimol.it
> > > ludovicofrate at hotmail.it
> > > https://www.researchgate.net/profile/Ludovico_Frate
> > >
> >
> >
> > >
> >
> > --
> > Dr. Alain F. Zuur
> >
> > First author of:
> > 1. Beginner's Guide to GAMM with R (2014).
> > 2. Beginner's Guide to GLM and GLMM with R (2013).
> > 3. Beginner's Guide to GAM with R (2012).
> > 4. Zero Inflated Models and GLMM with R (2012).
> > 5. A Beginner's Guide to R (2009).
> > 6. Mixed effects models and extensions in ecology with R (2009).
> > 7. Analysing Ecological Data (2007).
> >
> > Highland Statistics Ltd.
> > 9 St Clair Wynd
> > UK - AB41 6DZ Newburgh
> > Tel: 0044 1358 788177
> > Email: highstat at highstat.com
> > URL: www.highstat.com
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


	[[alternative HTML version deleted]]


From ludovicofrate at hotmail.it  Tue Feb 23 13:43:44 2016
From: ludovicofrate at hotmail.it (Ludovico Frate)
Date: Tue, 23 Feb 2016 13:43:44 +0100
Subject: [R-sig-ME] glmmadmb beta - error
In-Reply-To: <56CC4BBD.3030901@highstat.com>
References: <mailman.1474.1456221132.3865.r-sig-mixed-models@r-project.org>,
	<56CC3F2F.7080707@highstat.com>
	<DUB126-W560053D590B0CCCF1E25CD6A40@phx.gbl>,
	<56CC4BBD.3030901@highstat.com>
Message-ID: <DUB126-W1976CF8BD43BC45FCF35BD6A40@phx.gbl>

Thanks for your reply!I successfully fitted another model with the same number of observation (and structure) and the only difference is that cover values were more centered on the mean values.
I just tried to fit the same model by adding a constant cover value to all observations (+0.20) and the model works
I think that the issue derives from very small cover values...
summary(cover_lf$cover) (divided by 100)
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 0.0000  0.0050  0.0130  0.0252  0.0385  0.1300 
Call:glmmadmb(formula = cover ~ time + summit + direction + (1 | plot_id),     data = cover_lf, family = "beta")
AIC: -228.7 
Coefficients:            Estimate Std. Error z value Pr(>|z|)    (Intercept)  -1.2241     0.0476  -25.72   <2e-16 ***time2015     -0.0688     0.0337   -2.04   0.0410 *  summitMAC    -0.1407     0.0437   -3.22   0.0013 ** directionN    0.1028     0.0574    1.79   0.0733 .  directionS    0.0760     0.0624    1.22   0.2238    directionW    0.0986     0.0574    1.72   0.0860 .  ---Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
Number of observations: total=50, plot_id=25 Random effect variance(s):Group=plot_id            Variance  StdDev(Intercept) 0.003394 0.05826
Beta dispersion parameter: 403.43 (std. err.: 0.00012419)
Log-likelihood: 122.34  		 	   		  
	[[alternative HTML version deleted]]


From highstat at highstat.com  Tue Feb 23 13:49:51 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Tue, 23 Feb 2016 12:49:51 +0000
Subject: [R-sig-ME] glmmadmb beta - error
In-Reply-To: <DUB126-W1976CF8BD43BC45FCF35BD6A40@phx.gbl>
References: <mailman.1474.1456221132.3865.r-sig-mixed-models@r-project.org>
	<56CC3F2F.7080707@highstat.com>
	<DUB126-W560053D590B0CCCF1E25CD6A40@phx.gbl>
	<56CC4BBD.3030901@highstat.com>
	<DUB126-W1976CF8BD43BC45FCF35BD6A40@phx.gbl>
Message-ID: <56CC556F.2090608@highstat.com>

The fact that the software runs properly, or not, is irrelevant. You 
need to have at least 10 - 15 - 20 observations per regression parameter 
for an ordinary regression model. Using only 50 observations for the 
beta mixed model is not the best thing to do. Maybe this is not what you 
wanted to hear.

Alain

On 23/02/2016 12:43, Ludovico Frate wrote:
> Thanks for your reply!
> I successfully fitted another model with the same number of 
> observation (and structure) and the only difference is that cover 
> values were more centered on the mean values.
>
> I just tried to fit the same model by adding a constant cover value to 
> all observations (+0.20) and the model works
>
> I think that the issue derives from very small cover values...
>
> summary(cover_lf$cover) (divided by 100)
>
> Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> 0.0000  0.0050  0.0130  0.0252  0.0385  0.1300
>
> Call:
> glmmadmb(formula = cover ~ time + summit + direction + (1 | plot_id),
>     data = cover_lf, family = "beta")
>
> AIC: -228.7
>
> Coefficients:
>             Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -1.2241     0.0476  -25.72   <2e-16 ***
> time2015     -0.0688     0.0337   -2.04   0.0410 *
> summitMAC    -0.1407     0.0437   -3.22   0.0013 **
> directionN    0.1028     0.0574    1.79   0.0733 .
> directionS    0.0760     0.0624    1.22   0.2238
> directionW    0.0986     0.0574    1.72   0.0860 .
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Number of observations: total=50, plot_id=25
> Random effect variance(s):
> Group=plot_id
>             Variance  StdDev
> (Intercept) 0.003394 0.05826
>
> Beta dispersion parameter: 403.43 (std. err.: 0.00012419)
>
> Log-likelihood: 122.34

-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


	[[alternative HTML version deleted]]


From Phillip.Alday at unisa.edu.au  Tue Feb 23 13:54:05 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Tue, 23 Feb 2016 12:54:05 +0000
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
	<CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
Message-ID: <0293aa014cc14df59eec46f84120532e@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>

In my experience, car::Anova is slightly less conservative (as Wald
tests are known to be somewhat anti-conservative).

Are you using Type-III tests for everything? The differences between
Type-II and Type-III can actually make a big difference in terms of
which predictors are significant.

Speaking of Type-III -- although it's the default in some popular
commercial packages, Type-II (marginal tests) is actually the type that
makes the most sense in terms of statistical interpretation and
hypotheses tested. But that's a topic for another time ....

Best,
Phillip

On 23/02/16 22:41, Francesco Romano wrote:
> Thanks to Henrik and Phillip for the quick reply.
> Your suggestions have been helpful in making progress.
> 
> On the one hand Henrik is right about
> reporting coefficients and standard errors when
> there are only two levels for the each predictor. This is
> consistent with two of the sources I mentioned so far.
> I infer that the authors reported directly from the summary(m1)
> after use of the mixed function (not car::Anova which yields chi
> square tests).
> 
> On the other hand, I don't understand how Cai et al. (2012) p.842,
> "combined analysis experiments 1 and 2", reported the main effect
> of a factor with 4 levels via a single estimate, SE, z, p coefficient.
> How did they obtain this and is this the right way?
> 
> Finally, after running analysis both ways, I get slightly different
> p-values, with the car::Anova method being more conservative
> (it yields less significant predictors). Is this normal?
> 
> Frank
> 
> 
> 
> On Tue, Feb 23, 2016 at 10:51 AM, Phillip Alday <Phillip.Alday at unisa.edu.au>
> wrote:
> 
>> lme4:anova() is not the same thing as car::Anova()!
>>
>> A quick R note that might have avoided the confusion:
>> The :: syntax in R refers to scope, so you can specify a function
>> unambiguously via package::function.name(). Moreover, R is case
>> sensitive, so Anova() and anova() are generally different things.
>>
>> Henrik's message (posted to the list so if you don't suscribe, you need
>> to look here:
>>
>> https://mailman.stat.ethz.ch/pipermail/r-sig-mixed-models/2016q1/024465.html
>> ) describes how to do this with either his afex package (for
>> likelihood-ratio tests) or John Fox's car package (for analysis of
>> deviance / Wald tests).
>>
>> If you just want to perform likelihood-ratio tests in lme4, then you
>> should look at the drop1() function or you can use anova(reduced.model,
>> full.model). Henrik also does a nice job summarizing some of the issues
>> here, so I won't repeat them.
>>
>> One final note: not everything that holds for normal LMM holds for GLMM
>> -- GLMM tends to be much more complicated. :-(
>>
>> Best,
>> Phillip
>>
>> On 23/02/16 20:03, Francesco Romano wrote:
>>> Yes. An ANOVA with my final bglmer model yields:
>>>
>>>> anova(recallmodel4x6a)
>>>
>>> Analysis of Variance Table
>>>
>>>                    Df Sum Sq Mean Sq F value
>>> syntax12            1 1.7670  1.7670  1.7670
>>> animacy12           1 3.4036  3.4036  3.4036
>>> group123            2 5.7213  2.8607  2.8607
>>> animacy12:group123  2 4.5546  2.2773  2.2773
>>> syntax12:group123   2 8.1732  4.0866  4.0866
>>>
>>> which is counterintuitively not what the authors of the papers
>>> apparently used to generate coefficients to report their main effects
>>> and interactions. It looks to me more like ML fitting. Elsewhere,
>>> and more typically, main effects and interactions are obtained by
>>> comparing a
>>>
>>> model with the main fixed effect to a model without the
>>>
>>> main fixed effect in terms of log-likelihood ratio tests
>>>
>>> (Raffray et al., 2013, http://dx.doi.org/10.1016/j.jml.2013.09.004,
>> p.6).
>>>
>>>
>>> I understand obtaining p-values from a summary
>>> of linear mixed models fit by lmer is a contentious issue
>>>
>>> https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html
>>>
>>> but I guess I might be missing something here.
>>>
>>>
>>>
>>>
>>>
>>>
>>> On Tue, Feb 23, 2016 at 2:21 AM, Phillip Alday
>>> <Phillip.Alday at unisa.edu.au <mailto:Phillip.Alday at unisa.edu.au>> wrote:
>>>
>>>     Have you looked at car::Anova() ?
>>>
>>>     Best,
>>>     Phillip
>>>
>>>     [forgot to cc the list]
>>>
>>>     > On 23 Feb 2016, at 11:42, Francesco Romano <
>> francescobryanromano at gmail.com
>>>     <mailto:francescobryanromano at gmail.com>> wrote:
>>>     >
>>>     > Dear all,
>>>     >
>>>     > I'm trying to report my analysis replicating the method in the
>>>     following
>>>     > papers:
>>>     >
>>>     > Cai, Pickering, and Branigan (2012). Mapping concepts to syntax:
>>>     Evidence
>>>     > from structural priming in Mandarin Chinese. Journal of Memory and
>>>     Language 66
>>>     > (2012) 833?849 <tel:%282012%29%20833%E2%80%93849>. (looking at pg.
>>>     842, "Combined analysis of Experiments 1
>>>     > and 2" section)
>>>     >
>>>     > Filiaci, Sorace, and Carreiras (2013). Anaphoric biases of null
>>>     and overt
>>>     > subjects in Italian and Spanish: a cross-linguistic comparison.
>>>     Language,
>>>     > Cognition, and Neuroscience  DOI:10.1080/01690965.2013.801502
>>>     (looking at
>>>     > pg.11, first two paragraphs)
>>>     >
>>>     > This is because I have a glmer model with three fixed effects, two
>>>     random
>>>     > intercepts modeling a binary outcome, exactly as in the articles
>>>     mentioned.
>>>     >
>>>     > The difficulty I'm finding is with locating information on commands
>>>     > generating coefficients, SE, z, and p values (e.g. maximum
>> likelihood
>>>     > (Laplace Approximation)) to report main effects and interactions
>>>     with the
>>>     > anova or afex:mixed commands, following application of effect
>>>     coding. I
>>>     > have looked in several places, including Ben Bolker's FAQ
>>>     > http://glmm.wikidot.com/faq and past posts on the topic in this
>> r-sig.
>>>     > Although there appears to be a plethora of material for lmer, I
>>>     can't seem
>>>     > to locate anything in the right direction for glmer.
>>>     >
>>>     > Many thanks for any help.
>>>     >
>>>     >
>>>     >
>>>     >
>>>     > --
>>>     > Frank Romano Ph.D.
>>>     >
>>>     > *LinkedIn*
>>>     > https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162
>>>     >
>>>     > *Academia.edu*
>>>     > https://sheffield.academia.edu/FrancescoRomano
>>>     >
>>>     >       [[alternative HTML version deleted]]
>>>     >
>>>     > _______________________________________________
>>>     > R-sig-mixed-models at r-project.org
>>>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>>>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>>
>>>
>>> --
>>> Frank Romano Ph.D.
>>>
>>> Tel. +39 3911639149
>>>
>>> /LinkedIn/
>>> https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162
>>>
>>> /Academia.edu/
>>> https://sheffield.academia.edu/FrancescoRomano
>>
> 
> 
> 


From jfox at mcmaster.ca  Tue Feb 23 14:30:28 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Tue, 23 Feb 2016 13:30:28 +0000
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F59E74@FHSDB2D11-2.csu.mcmaster.ca>

Dear Francesco,

The Anova() function in the car package might do what you want. See ?Anova. "Effect coding" in R is implemented by contr.sum().

I hope this helps,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
web: socserv.mcmaster.ca/jfox


________________________________________
From: R-sig-mixed-models [r-sig-mixed-models-bounces at r-project.org] on behalf of Francesco Romano [francescobryanromano at gmail.com]
Sent: February 22, 2016 8:12 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM

Dear all,

I'm trying to report my analysis replicating the method in the following
papers:

Cai, Pickering, and Branigan (2012). Mapping concepts to syntax: Evidence
from structural priming in Mandarin Chinese. Journal of Memory and Language 66
(2012) 833?849. (looking at pg. 842, "Combined analysis of Experiments 1
and 2" section)

Filiaci, Sorace, and Carreiras (2013). Anaphoric biases of null and overt
subjects in Italian and Spanish: a cross-linguistic comparison. Language,
Cognition, and Neuroscience  DOI:10.1080/01690965.2013.801502  (looking at
pg.11, first two paragraphs)

This is because I have a glmer model with three fixed effects, two random
intercepts modeling a binary outcome, exactly as in the articles mentioned.

The difficulty I'm finding is with locating information on commands
generating coefficients, SE, z, and p values (e.g. maximum likelihood
(Laplace Approximation)) to report main effects and interactions with the
anova or afex:mixed commands, following application of effect coding. I
have looked in several places, including Ben Bolker's FAQ
http://glmm.wikidot.com/faq and past posts on the topic in this r-sig.
Although there appears to be a plethora of material for lmer, I can't seem
to locate anything in the right direction for glmer.

Many thanks for any help.




--
Frank Romano Ph.D.

*LinkedIn*
https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162

*Academia.edu*
https://sheffield.academia.edu/FrancescoRomano

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Tue Feb 23 14:33:13 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 23 Feb 2016 08:33:13 -0500
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC810F59E74@FHSDB2D11-2.csu.mcmaster.ca>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<ACD1644AA6C67E4FBD0C350625508EC810F59E74@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <56CC5F99.7090901@gmail.com>


   And don't forget car::contr.Sum(), which is just the same as 
contr.sum from base R but provides saner contrast names ...


On 16-02-23 08:30 AM, Fox, John wrote:
> Dear Francesco,
>
> The Anova() function in the car package might do what you want. See
> ?Anova. "Effect coding" in R is implemented by contr.sum().
>
> I hope this helps, John
>
> ----------------------------- John Fox, Professor McMaster
> University Hamilton, Ontario Canada L8S 4M4 web:
> socserv.mcmaster.ca/jfox
>
>
> ________________________________________ From: R-sig-mixed-models
> [r-sig-mixed-models-bounces at r-project.org] on behalf of Francesco
> Romano [francescobryanromano at gmail.com] Sent: February 22, 2016 8:12
> PM To: r-sig-mixed-models at r-project.org Subject: [R-sig-ME]
> Replicating type III anova tests for glmer/GLMM
>
> Dear all,
>
> I'm trying to report my analysis replicating the method in the
> following papers:
>
> Cai, Pickering, and Branigan (2012). Mapping concepts to syntax:
> Evidence from structural priming in Mandarin Chinese. Journal of
> Memory and Language 66 (2012) 833?849. (looking at pg. 842, "Combined
> analysis of Experiments 1 and 2" section)
>
> Filiaci, Sorace, and Carreiras (2013). Anaphoric biases of null and
> overt subjects in Italian and Spanish: a cross-linguistic comparison.
> Language, Cognition, and Neuroscience
> DOI:10.1080/01690965.2013.801502  (looking at pg.11, first two
> paragraphs)
>
> This is because I have a glmer model with three fixed effects, two
> random intercepts modeling a binary outcome, exactly as in the
> articles mentioned.
>
> The difficulty I'm finding is with locating information on commands
> generating coefficients, SE, z, and p values (e.g. maximum
> likelihood (Laplace Approximation)) to report main effects and
> interactions with the anova or afex:mixed commands, following
> application of effect coding. I have looked in several places,
> including Ben Bolker's FAQ http://glmm.wikidot.com/faq and past posts
> on the topic in this r-sig. Although there appears to be a plethora
> of material for lmer, I can't seem to locate anything in the right
> direction for glmer.
>
> Many thanks for any help.
>
>
>
>
> -- Frank Romano Ph.D.
>
> *LinkedIn*
> https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162
>
> *Academia.edu* https://sheffield.academia.edu/FrancescoRomano
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From mlilleeng at gmail.com  Tue Feb 23 15:02:48 2016
From: mlilleeng at gmail.com (Marte Lilleeng)
Date: Tue, 23 Feb 2016 15:02:48 +0100
Subject: [R-sig-ME] glmmadmb Negative binomial dispersion parameter
Message-ID: <CAM-hW5cun5sXzyf-WrCLF2CGJjfQeD1nTnnccAZ213kbSqDSvA@mail.gmail.com>

Hello everyone.
I wonder if you can help me with the following questions?

1) How is the Negative binomial dispersion parameter calculated in glmmadmb?

2) Is there,as for mixed effects poisson models (glmer), a *rule of thumb*
for when the dispersion parameter is representing trouble (overdispersion)?
I learned from Zuur and Ileno that for a mixed effects poisson mod the
dispersion is ok as long as it is not above 1.3-1.4(calculated this way;

E <- residuals (modelname)

pb <- length(fixef(modelname)+1) # +1 due to random intercept variance

overdisp <- sum(E^2)/(nrow(dataset)-pb)

3) How do you recommend to do the model validation for glmmadmb with
negative binomial error structure?

Best regards from
"New to mixed models with NB", Marte Lilleeng

	[[alternative HTML version deleted]]


From emmanuel.curis at parisdescartes.fr  Tue Feb 23 16:32:23 2016
From: emmanuel.curis at parisdescartes.fr (Emmanuel Curis)
Date: Tue, 23 Feb 2016 16:32:23 +0100
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
	<CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
Message-ID: <20160223153223.GA16587@info124.pharmacie.univ-paris5.fr>

On Tue, Feb 23, 2016 at 01:06:18PM +0100, Francesco Romano wrote:
? On the other hand, I don't understand how Cai et al. (2012) p.842,
? "combined analysis experiments 1 and 2", reported the main effect
? of a factor with 4 levels via a single estimate, SE, z, p coefficient.
? How did they obtain this and is this the right way?

It's just a guess, but any sum-of-square can be seen as a particular
contrast, that is a particular combination of the coefficients in the
model (or of the different means, expressed another way) that is
tested against 0. So I guess this single estimate is the value of the
contrast associated to the corresponding sum-of-squares, and SE/z/p
are derived similarly.

You can play with multcomp::glht to test this, but knowing which
contrast is tested by which sum of square in a specific desing may be
tricky: it depends on the coding, on the (un)balance...

Kowing if this is the ? right ? way is I think the same debate that
knowing which kind of sum-of-square should be used and the question is
very application dependent. Just, if you don't know what this single
estimate estimates really, interpretation is at best difficult...

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From mbrooks at ufl.edu  Tue Feb 23 16:47:53 2016
From: mbrooks at ufl.edu (Mollie Brooks)
Date: Tue, 23 Feb 2016 16:47:53 +0100
Subject: [R-sig-ME] glmmadmb Negative binomial dispersion parameter
In-Reply-To: <CAM-hW5cun5sXzyf-WrCLF2CGJjfQeD1nTnnccAZ213kbSqDSvA@mail.gmail.com>
References: <CAM-hW5cun5sXzyf-WrCLF2CGJjfQeD1nTnnccAZ213kbSqDSvA@mail.gmail.com>
Message-ID: <D022CABA-3BB7-4129-AD27-8C5C1B82D0AB@ufl.edu>

Hi Marte,

Overdispersion isn?t usually a concern with the negative binomial distribution. It?s a problem with distributions where the variance is strictly tied to the mean, like the Poisson. 

There are two different ways that glmmadmb lets the variance relate to the mean. See the details of the help file for descriptions of "nbinom" and "nbinom1". You could compare the AIC of models fit with each of these if you want to see which one is best. I don?t know of other diagnostics, but maybe someone else will chime in.

cheers,
Mollie

------------------------
Mollie Brooks, PhD
Postdoctoral Researcher, Population Ecology Research Group
Department of Evolutionary Biology & Environmental Studies, University of Z?rich
http://www.popecol.org/team/mollie-brooks/


> On 23Feb 2016, at 15:02, Marte Lilleeng <mlilleeng at gmail.com> wrote:
> 
> Hello everyone.
> I wonder if you can help me with the following questions?
> 
> 1) How is the Negative binomial dispersion parameter calculated in glmmadmb?
> 
> 2) Is there,as for mixed effects poisson models (glmer), a *rule of thumb*
> for when the dispersion parameter is representing trouble (overdispersion)?
> I learned from Zuur and Ileno that for a mixed effects poisson mod the
> dispersion is ok as long as it is not above 1.3-1.4(calculated this way;
> 
> E <- residuals (modelname)
> 
> pb <- length(fixef(modelname)+1) # +1 due to random intercept variance
> 
> overdisp <- sum(E^2)/(nrow(dataset)-pb)
> 
> 3) How do you recommend to do the model validation for glmmadmb with
> negative binomial error structure?
> 
> Best regards from
> "New to mixed models with NB", Marte Lilleeng
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


	[[alternative HTML version deleted]]


From highstat at highstat.com  Tue Feb 23 17:01:03 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Tue, 23 Feb 2016 16:01:03 +0000
Subject: [R-sig-ME] glmmadmb beta - error
In-Reply-To: <DUB126-W7233948155059B1B8B00D8D6A40@phx.gbl>
References: <mailman.1474.1456221132.3865.r-sig-mixed-models@r-project.org>
	<56CC3F2F.7080707@highstat.com>
	<DUB126-W560053D590B0CCCF1E25CD6A40@phx.gbl>
	<56CC4BBD.3030901@highstat.com>
	<DUB126-W1976CF8BD43BC45FCF35BD6A40@phx.gbl>
	<56CC556F.2090608@highstat.com>
	<DUB126-W7233948155059B1B8B00D8D6A40@phx.gbl>
Message-ID: <56CC823F.2020308@highstat.com>

Ludovico...it would be nice if you can cc the mixed modelling mailing list.

Whatever you do, you need to get more data. The model below is applying 
a model with 5 regression parameters on 25 observations. And you do it 
twice. And you may still need a beta distribution.

What you can do is fit your original beta (or Gaussian) mixed model and 
do some simulations from the model, and see whether you are able to 
estimate similar parameter estimates from the simulated data. If you are 
lucky and you have strong patterns in your data, than perhaps you may 
get away with your low sample size.


I understand that for some studies 50 observations represent a lot of 
work/time/money, but what is the point in spending all that time if at 
the end of the day the sample size is too low to answer your questions?

Alain




On 23/02/2016 15:42, Ludovico Frate wrote:
> Thank you!
> Since I have repeated measure for each plot (two observation for 
> plot), could be an idea subtracting the cover values in pairs (i.e. 
> plot 1 cover 2015-cover 2001, plot 2 cover 2015-cover 2001, and so on) 
> and then try to fit a simple linear model with the new dependent 
> variable?
>
> lm(cover_delta~1 + direction+time)
>
> Regards,
> Ludovico
>
> ------------------------------------------------------------------------
> Subject: Re: [R-sig-ME] glmmadmb beta - error
> To: ludovicofrate at hotmail.it; r-sig-mixed-models at r-project.org
> From: highstat at highstat.com
> Date: Tue, 23 Feb 2016 12:49:51 +0000
>
> The fact that the software runs properly, or not, is irrelevant. You 
> need to have at least 10 - 15 - 20 observations per regression 
> parameter for an ordinary regression model. Using only 50 observations 
> for the beta mixed model is not the best thing to do. Maybe this is 
> not what you wanted to hear.
>
> Alain
>
> On 23/02/2016 12:43, Ludovico Frate wrote:
>
>     Thanks for your reply!
>     I successfully fitted another model with the same number of
>     observation (and structure) and the only difference is that cover
>     values were more centered on the mean values.
>
>     I just tried to fit the same model by adding a constant cover
>     value to all observations (+0.20) and the model works
>
>     I think that the issue derives from very small cover values...
>
>     summary(cover_lf$cover) (divided by 100)
>
>     Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>     0.0000  0.0050  0.0130  0.0252  0.0385  0.1300
>
>     Call:
>     glmmadmb(formula = cover ~ time + summit + direction + (1 | plot_id),
>         data = cover_lf, family = "beta")
>
>     AIC: -228.7
>
>     Coefficients:
>                 Estimate Std. Error z value Pr(>|z|)
>     (Intercept)  -1.2241     0.0476  -25.72 <2e-16 ***
>     time2015     -0.0688     0.0337   -2.04   0.0410 *
>     summitMAC    -0.1407     0.0437   -3.22   0.0013 **
>     directionN    0.1028     0.0574    1.79   0.0733 .
>     directionS    0.0760     0.0624    1.22   0.2238
>     directionW    0.0986     0.0574    1.72   0.0860 .
>     ---
>     Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>     Number of observations: total=50, plot_id=25
>     Random effect variance(s):
>     Group=plot_id
>                 Variance  StdDev
>     (Intercept) 0.003394 0.05826
>
>     Beta dispersion parameter: 403.43 (std. err.: 0.00012419)
>
>     Log-likelihood: 122.34
>
>
> -- 
> Dr. Alain F. Zuur
>
> First author of:
> 1. Beginner's Guide to GAMM with R (2014).
> 2. Beginner's Guide to GLM and GLMM with R (2013).
> 3. Beginner's Guide to GAM with R (2012).
> 4. Zero Inflated Models and GLMM with R (2012).
> 5. A Beginner's Guide to R (2009).
> 6. Mixed effects models and extensions in ecology with R (2009).
> 7. Analysing Ecological Data (2007).
>
> Highland Statistics Ltd.
> 9 St Clair Wynd
> UK - AB41 6DZ Newburgh
> Tel:   0044 1358 788177
> Email:highstat at highstat.com <mailto:highstat at highstat.com>
> URL:www.highstat.com <http://www.highstat.com>

-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Tue Feb 23 17:15:02 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Tue, 23 Feb 2016 16:15:02 +0000
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <20160223153223.GA16587@info124.pharmacie.univ-paris5.fr>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
	<CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
	<20160223153223.GA16587@info124.pharmacie.univ-paris5.fr>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F5A17C@FHSDB2D11-2.csu.mcmaster.ca>

Dear Emmanuel,

With proper contrast coding (i.e., a coding that's orthogonal in the *basis* of the design, such as provided by contr.sum() ), a "type-III" test is just a test that the corresponding parameters are 0. The models in question are generalized linear (mixed) models and so sums of squares aren't really involved, but one could do the corresponding Wald (like car::Anova) or LR test. The Wald test is what you'd get with multcomp:glht or car:linearHypothesis. BTW, I don't think that it would be hard for car::Anova to be extended to provide LR tests in this case.

Best,
 John

> -----Original Message-----
> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> project.org] On Behalf Of Emmanuel Curis
> Sent: February 23, 2016 10:32 AM
> To: Francesco Romano <francescobryanromano at gmail.com>
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
> 
> On Tue, Feb 23, 2016 at 01:06:18PM +0100, Francesco Romano wrote:
> < On the other hand, I don't understand how Cai et al. (2012) p.842, <
> "combined analysis experiments 1 and 2", reported the main effect < of a
> factor with 4 levels via a single estimate, SE, z, p coefficient.
> < How did they obtain this and is this the right way?
> 
> It's just a guess, but any sum-of-square can be seen as a particular contrast,
> that is a particular combination of the coefficients in the model (or of the
> different means, expressed another way) that is tested against 0. So I guess
> this single estimate is the value of the contrast associated to the
> corresponding sum-of-squares, and SE/z/p are derived similarly.
> 
> You can play with multcomp::glht to test this, but knowing which contrast is
> tested by which sum of square in a specific desing may be
> tricky: it depends on the coding, on the (un)balance...
> 
> Kowing if this is the < right > way is I think the same debate that knowing
> which kind of sum-of-square should be used and the question is very
> application dependent. Just, if you don't know what this single estimate
> estimates really, interpretation is at best difficult...
> 
> --
>                                 Emmanuel CURIS
>                                 emmanuel.curis at parisdescartes.fr
> 
> Page WWW: http://emmanuel.curis.online.fr/index.html
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From paul.johnson at glasgow.ac.uk  Tue Feb 23 17:33:36 2016
From: paul.johnson at glasgow.ac.uk (Paul Johnson)
Date: Tue, 23 Feb 2016 16:33:36 +0000
Subject: [R-sig-ME] glmmadmb Negative binomial dispersion parameter
In-Reply-To: <CAM-hW5cun5sXzyf-WrCLF2CGJjfQeD1nTnnccAZ213kbSqDSvA@mail.gmail.com>
References: <CAM-hW5cun5sXzyf-WrCLF2CGJjfQeD1nTnnccAZ213kbSqDSvA@mail.gmail.com>
Message-ID: <0756934B2C0D624DB20E135A92ACC524041CC298@CMS12-01.campus.gla.ac.uk>

Hi Marte,

In answer to point 2 (adapted from a comment i made on the R-space FB group)...

My general approach is not to bother testing for overdispersion in GLMMs, but to assume it's there and model it as a matter of course, using either the OLRE approach with glmer (see Xavier Harrison's paper https://peerj.com/articles/616/) or negative binomial with glmmADMB (I've never used glmer.nb - the help file used to give a health warning, but I see that's gone). Modelling overdispersion is simply modelling unexplained variation at the observation level, so to fit a Poisson or binomial GLMM that doesn't allow for overdispersion is effectively assuming that the model explains all the variation, which is almost never a reasonable assumption (at least in biology). I don't agree with the approach of ignoring it if either it isn't significant or if the OD index you showed is low (< 1.3-1.4). This still seems to me a Ignoring a potentially substantial amount of variance and ignoring it could still have negative consequences such as an inflated false positive rate for tests and over-optimistic (narrow) CIs.

Including a term for overdispersion (when possible) in a GLMM is pretty much the same as including a residual error term in a simple OLS linear regression model, except that in the linear regression model there is no other source of random variation so we'd never consider leaving it out.

A caveat is that overdispersion can be caused by a mis-specified model so it's important to try to identify this before assuming that all the overdispersion is due to unexplained variation.

All the best,
Paul

On Tue, Feb 23, 2016 at 2:07 p.m., Marte Lilleeng <mlilleeng at gmail.com<mailto:mlilleeng at gmail.com>> wrote:

Hello everyone.
I wonder if you can help me with the following questions?

1) How is the Negative binomial dispersion parameter calculated in glmmadmb?

2) Is there,as for mixed effects poisson models (glmer), a *rule of thumb*
for when the dispersion parameter is representing trouble (overdispersion)?
I learned from Zuur and Ileno that for a mixed effects poisson mod the
dispersion is ok as long as it is not above 1.3-1.4(calculated this way;

E <- residuals (modelname)

pb <- length(fixef(modelname)+1) # +1 due to random intercept variance

overdisp <- sum(E^2)/(nrow(dataset)-pb)

3) How do you recommend to do the model validation for glmmadmb with
negative binomial error structure?

Best regards from
"New to mixed models with NB", Marte Lilleeng

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From chenjeraikay at gmail.com  Tue Feb 23 16:51:14 2016
From: chenjeraikay at gmail.com (Kathy)
Date: Tue, 23 Feb 2016 15:51:14 +0000 (UTC)
Subject: [R-sig-ME] glmmADMB error: no PSV file found
References: <CAPO7DkfyZAT-Yy5=ZFY8OZsxO7DABvK3Z4sXRBFZQvOOjh0hEw@mail.gmail.com>
Message-ID: <loom.20160223T164934-960@post.gmane.org>

Have you found a solution yet Peggy? I am also facing the same issue.


Moore, Peggy <peggy_moore at ...> writes:

> 
> I am using glmmadmb (glmmADMB_0.8.3.2 and R2admb_0.7.13) with R 
version
> 3.2.1 (2015-06-18) to fit a negative binomial mixed effects model to 
my
> data. Response is repeated measures of plant counts following 
herbicide
> treatments. There are 2 fixed effects (year, treatment) and 2 random
> effects (block, plot:block). I successfully fit the model when 
mcmc=FALSE,
> but when I add mcmc=TRUE, I receive the error: no PSV file found.
>


From emmanuel.curis at parisdescartes.fr  Tue Feb 23 17:49:47 2016
From: emmanuel.curis at parisdescartes.fr (Emmanuel Curis)
Date: Tue, 23 Feb 2016 17:49:47 +0100
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC810F5A17C@FHSDB2D11-2.csu.mcmaster.ca>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
	<CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
	<20160223153223.GA16587@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A17C@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <20160223164947.GA29156@info124.pharmacie.univ-paris5.fr>

Dear Pr Fox,

Thanks for your precision. But to summarize this test of, let's say 3
parameters to 0 for a 4-levels factor, by a single value with its SE,
as mentionned in Francesco's mail, the linear combination of these
parameters that is practically tested by this sum of square is needed,
isn't it ?

I mean, if really the parameters are all 0, whatever linear
combination could do the job, but type III sum of square just tests
one of all possible linear combinations, right?

By the way, I was always very annoyed by the fact that Type III sum of
squares are so dependent on coding, but that's another debate...

Best regards,

On Tue, Feb 23, 2016 at 04:15:02PM +0000, Fox, John wrote:
? Dear Emmanuel,
? 
? With proper contrast coding (i.e., a coding that's orthogonal in the *basis* of the design, such as provided by contr.sum() ), a "type-III" test is just a test that the corresponding parameters are 0. The models in question are generalized linear (mixed) models and so sums of squares aren't really involved, but one could do the corresponding Wald (like car::Anova) or LR test. The Wald test is what you'd get with multcomp:glht or car:linearHypothesis. BTW, I don't think that it would be hard for car::Anova to be extended to provide LR tests in this case.
? 
? Best,
?  John

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From jfox at mcmaster.ca  Tue Feb 23 18:17:29 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Tue, 23 Feb 2016 17:17:29 +0000
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <20160223164947.GA29156@info124.pharmacie.univ-paris5.fr>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
	<CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
	<20160223153223.GA16587@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A17C@FHSDB2D11-2.csu.mcmaster.ca>
	<20160223164947.GA29156@info124.pharmacie.univ-paris5.fr>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F5A21D@FHSDB2D11-2.csu.mcmaster.ca>

Dear Emmanuel,

First, the relevant linear hypothesis is for several coefficients simultaneously -- for example, all 3 coefficients for the contrasts representing a 4-level factor -- not for a single contrast. Although it's true that any linear combination of parameters that are 0 is 0, the converse isn't true. Second, for a GLMM, we really should be talking about type-III tests not type-III sums of squares.

Type-III tests are dependent on coding in the full-rank parametrization of linear (and similar) models used in R, to make the tests correspond to reasonable hypotheses. The invariance of type-II tests with respect to coding is attractive, but shouldn't distract from the fundamental issues, which are the hypotheses that are tested and the power of the tests. 

Best,
 John

> -----Original Message-----
> From: Emmanuel Curis [mailto:emmanuel.curis at parisdescartes.fr]
> Sent: February 23, 2016 11:50 AM
> To: Fox, John <jfox at mcmaster.ca>
> Cc: Francesco Romano <francescobryanromano at gmail.com>; r-sig-mixed-
> models at r-project.org
> Subject: Re: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
> 
> Dear Pr Fox,
> 
> Thanks for your precision. But to summarize this test of, let's say 3
> parameters to 0 for a 4-levels factor, by a single value with its SE, as
> mentionned in Francesco's mail, the linear combination of these parameters
> that is practically tested by this sum of square is needed, isn't it ?
> 
> I mean, if really the parameters are all 0, whatever linear combination could
> do the job, but type III sum of square just tests one of all possible linear
> combinations, right?
> 
> By the way, I was always very annoyed by the fact that Type III sum of
> squares are so dependent on coding, but that's another debate...
> 
> Best regards,
> 
> On Tue, Feb 23, 2016 at 04:15:02PM +0000, Fox, John wrote:
> < Dear Emmanuel,
> <
> < With proper contrast coding (i.e., a coding that's orthogonal in the *basis*
> of the design, such as provided by contr.sum() ), a "type-III" test is just a test
> that the corresponding parameters are 0. The models in question are
> generalized linear (mixed) models and so sums of squares aren't really
> involved, but one could do the corresponding Wald (like car::Anova) or LR
> test. The Wald test is what you'd get with multcomp:glht or
> car:linearHypothesis. BTW, I don't think that it would be hard for car::Anova
> to be extended to provide LR tests in this case.
> <
> < Best,
> <  John
> 
> --
>                                 Emmanuel CURIS
>                                 emmanuel.curis at parisdescartes.fr
> 
> Page WWW: http://emmanuel.curis.online.fr/index.html


From highstat at highstat.com  Tue Feb 23 19:43:12 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Tue, 23 Feb 2016 18:43:12 +0000
Subject: [R-sig-ME] glmmadmb Negative binomial dispersion parameter
In-Reply-To: <mailman.1503.1456245240.3865.r-sig-mixed-models@r-project.org>
References: <mailman.1503.1456245240.3865.r-sig-mixed-models@r-project.org>
Message-ID: <56CCA840.7050304@highstat.com>



> Message: 1
> Date: Tue, 23 Feb 2016 15:02:48 +0100
> From: Marte Lilleeng <mlilleeng at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] glmmadmb Negative binomial dispersion parameter
> Message-ID:
> 	<CAM-hW5cun5sXzyf-WrCLF2CGJjfQeD1nTnnccAZ213kbSqDSvA at mail.gmail.com>
> Content-Type: text/plain; charset="UTF-8"
>
> Hello everyone.
> I wonder if you can help me with the following questions?
>
> 1) How is the Negative binomial dispersion parameter calculated in glmmadmb?
That is a very good question. While writing this email I have a series 
of help files open which explain how mgcv is estimating the theta, and 
how glm.nb from MASS is doing it. Performance iteration, outer 
iteration, estimating the theta so that the dispersion statistic is 1, 
or via AIC. So..the answer to your question depends very much which 
strategy the programmers of glmmADMB have followed. Have a look at the 
http://admb-project.org/ website. Maybe you can find your answer there.

GLMs in which the variance term contains an extra parameter (Gaussian, 
Gamma, beta-binomial) cannot be overdispersed. The Poisson and the 
binomial GLM don't have this facility, so they can be overdispersed. 
Strictly speaking the NB GLM is not a GLM (it is not part of the 
exponential family).

The algorithm for NB GLM ping-pongs between a glm algorithm and an 
algorithm for getting the theta (at least this is what glm.nb is doing).
So..strictly speaking when the algorithm is doing its glm leg, it can be 
overdispersed (dispersion statistic > 1).




> 2) Is there,as for mixed effects poisson models (glmer), a *rule of thumb*
> for when the dispersion parameter is representing trouble (overdispersion)?
> I learned from Zuur and Ileno that for a mixed effects poisson mod the
> dispersion is ok as long as it is not above 1.3-1.4(calculated this way;
>
> E <- residuals (modelname)
>
> pb <- length(fixef(modelname)+1) # +1 due to random intercept variance
>
> overdisp <- sum(E^2)/(nrow(dataset)-pb)

You have to add another '+1' due to the theta from the NB GLM.

It is perhaps better to simulate 10,000 times the response variable from 
the fitted model and see how the dispersion statistic varies. Make a 
histogram of the 10,000 dispersion statistics and add a big dot for the 
one you found for your original data set.

You can do the same for the number of zeros, maximum value, etc.


>
> 3) How do you recommend to do the model validation for glmmadmb with
> negative binomial error structure?
As usual. Plot residuals vs everything you have. It is also interesting 
to apply table() on each of the 10,000 simulated data sets, or calculate 
an average table(). And plot that vs the table() for the original data.

Kind regards,

Alain


> Best regards from
> "New to mixed models with NB", Marte Lilleeng
>
> 	[[alternative HTML version deleted]]
>
>


-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From highstat at highstat.com  Tue Feb 23 19:51:10 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Tue, 23 Feb 2016 18:51:10 +0000
Subject: [R-sig-ME] glmmadmb Negative binomial dispersion parameter
In-Reply-To: <mailman.1503.1456245240.3865.r-sig-mixed-models@r-project.org>
References: <mailman.1503.1456245240.3865.r-sig-mixed-models@r-project.org>
Message-ID: <56CCAA1E.1000704@highstat.com>



> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 23 Feb 2016 15:02:48 +0100
> From: Marte Lilleeng <mlilleeng at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] glmmadmb Negative binomial dispersion parameter
> Message-ID:
> 	<CAM-hW5cun5sXzyf-WrCLF2CGJjfQeD1nTnnccAZ213kbSqDSvA at mail.gmail.com>
> Content-Type: text/plain; charset="UTF-8"
>
> Hello everyone.
> I wonder if you can help me with the following questions?
>
> 1) How is the Negative binomial dispersion parameter calculated in glmmadmb?
>
> 2) Is there,as for mixed effects poisson models (glmer), a *rule of thumb*
> for when the dispersion parameter is representing trouble (overdispersion)?
> I learned from Zuur and Ileno that for a mixed effects poisson mod the
> dispersion is ok as long as it is not above 1.3-1.4(calculated this way;
>
> E <- residuals (modelname)
>
> pb <- length(fixef(modelname)+1) # +1 due to random intercept variance
>
> overdisp <- sum(E^2)/(nrow(dataset)-pb)
>
> 3) How do you recommend to do the model validation for glmmadmb with
> negative binomial error structure?
>
> Best regards from
> "New to mixed models with NB", Marte Lilleeng
>
> 	[[alternative HTML version deleted]]
>
>
I forgot to mention that the more and more I look at the NB GLM(M) the 
less I like it. You should only go for a NB GLM(M) is the cause of the 
overdispersion is large variation. If there is something else that is 
causing overdispersion (e.g. non-linear patterns, zero inflation, 
missing covariate, wrong link function), then the parameter theta is 
going to consume that information. And I still need to see the first 
data set for which the NB GLM(M) gives predictions with decent 
confidence intervals.

Maybe you want to have a look at the generalized Poisson GLM(M)...it 
tends to perform better than the NB GLMM. But the problem is that you 
may have to go Bayesian in that case.

Alain








-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From paul.johnson at glasgow.ac.uk  Tue Feb 23 20:25:50 2016
From: paul.johnson at glasgow.ac.uk (Paul Johnson)
Date: Tue, 23 Feb 2016 19:25:50 +0000
Subject: [R-sig-ME] glmmadmb Negative binomial dispersion parameter
In-Reply-To: <56CCAA1E.1000704@highstat.com>
References: <mailman.1503.1456245240.3865.r-sig-mixed-models@r-project.org>
	<56CCAA1E.1000704@highstat.com>
Message-ID: <2ACBEDDD-F797-42BE-967F-C6989175942C@glasgow.ac.uk>

Hi Alain,

>> I forgot to mention that the more and more I look at the NB GLM(M) the less I like it. You should only go for a NB GLM(M) is the cause of the overdispersion is large variation. If there is something else that is causing overdispersion (e.g. non-linear patterns, zero inflation, missing covariate, wrong link function), then the parameter theta is going to consume that information.

I agree that the overdispersion term will mop up all the ?unexplained" variation, whether this is due to bad explanation (= a poorly specified model, e.g. nonlinearity, ZI, etc) or just the fact that even good explanations have limits, and we should of course be careful to avoid bad explanations. However in my experience of biological count (or any) data, even if we take care to fit the model carefully there will generally be a substantial amount of unexplained (and unexplainable) variation, which is why I tend to include an overdispersion term as a matter of course, whether using negative binomial or Poisson-lognormal GLMMs. This doesn?t stop us looking for zero-inflation etc. E.g. if there?s real zero inflation, a NB GLMM with ZI should still fit better that a NB GLMM without ZI.

>> And I still need to see the first data set for which the NB GLM(M) gives predictions with decent confidence intervals.

Concerning ? I?d be interested in knowing more. How are you calculating the CIs? Are you talking about NB GLM(M)s in general, or glmmADMB? 

All the best,
Paul


From francescobryanromano at gmail.com  Tue Feb 23 20:34:08 2016
From: francescobryanromano at gmail.com (Francesco Romano)
Date: Tue, 23 Feb 2016 20:34:08 +0100
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC810F5A21D@FHSDB2D11-2.csu.mcmaster.ca>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
	<CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
	<20160223153223.GA16587@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A17C@FHSDB2D11-2.csu.mcmaster.ca>
	<20160223164947.GA29156@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A21D@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <CABZN5+GE=9ZK-L3Z=Vpj+fDVE+L1pcENu=+n8STs_=b_H=nWvA@mail.gmail.com>

John,

I tried the Anova() function in the car package implemented with
contr.sum() but it doesn't produce beta, SE, z, and p.
To be more precise, R requires that either the F or Chi sq statistic be
used. The model I used was termed "mod", here is the error:

> Anova(mod, type=c("III"),
+     test.statistic=c("LR"))
Error in match.arg(test.statistic) : 'arg' should be one of ?Chisq?, ?F?

Chi square produces the following output:

> Anova(mod, type=c("III"),
+     test.statistic=c("Chisq"))
Analysis of Deviance Table (Type III Wald chisquare tests)

Response: Correct
                              Chisq Df Pr(>Chisq)
(Intercept)                 67.7409  1  < 2.2e-16 ***
Syntax                       0.2856  1   0.593083
Animacy                      6.2575  1   0.012367 *
Prof.group.2                 2.9888  2   0.224379
Syntax:Animacy               0.0970  1   0.755521
Syntax:Prof.group.2          9.3054  2   0.009536 **
Animacy:Prof.group.2         4.7633  2   0.092399 .
Syntax:Animacy:Prof.group.2  1.3704  2   0.503997

So I still don't know how Raffrey et al. reported beta, SE, z, and p for a
main effect of factor with 4 levels.
If reviewers ask me to do this, I will argue that reporting chi square
tests with corresponding p-values is
a more accurate way of reporting main effects and interactions.

If I haven't abused enough of your time, it would be beneficial to
understand which of the two
methods suggested by Henrik I should adopt. I attach my data.

The predictors of interest are Syntax (2 levels), Animacy (2 levels),
Prof.group.2 (3 levels),
and the outcome 'correct', while the random effects are 'Part.name' and
'Item'. The best model fit is a
bglmer with glmerControl(optimizer = "bobyqa") and nAGQ=1

> summary(recallmodel4bisB3)
Cov prior  : Part.name ~ wishart(df = 3.5, scale = Inf, posterior.scale =
cov, common.scale = TRUE)
           : Item ~ wishart(df = 3.5, scale = Inf, posterior.scale = cov,
common.scale = TRUE)
Prior dev  : 1.3565

Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['bglmerMod']
 Family: binomial  ( logit )
Formula: Correct ~ Syntax * Animacy * Prof.group.2 + (1 | Part.name) +
 (1 | Item)
   Data: recall
Control: glmerControl(optimizer = "bobyqa")

     AIC      BIC   logLik deviance df.resid
   313.3    372.9   -142.6    285.3      509

Scaled residuals:
    Min      1Q  Median      3Q     Max
-1.3517 -0.2926 -0.1802 -0.1137  9.3666

Random effects:
 Groups    Name        Variance Std.Dev.
 Part.name (Intercept) 0.8046   0.8970
 Item      (Intercept) 0.5031   0.7093
Number of obs: 523, groups:  Part.name, 42; Item, 16

Fixed effects:
                                       Estimate Std. Error z value Pr(>|z|)

(Intercept)                             -0.8960     0.6317  -1.418 0.156071

Syntaxs                                 -2.0713     0.9447  -2.193 0.028342
*
Animacy+AN -AN                          -3.0539     1.2548  -2.434 0.014941
*
Prof.group.2int                         -2.5594     0.9473  -2.702 0.006898
**
Prof.group.2ns                          -1.8673     0.7634  -2.446 0.014442
*
Syntaxs:Animacy+AN -AN                   1.8642     1.8202   1.024 0.305750

Syntaxs:Prof.group.2int                  4.1704     1.1676   3.572 0.000355
***
Syntaxs:Prof.group.2ns                   2.4244     1.0483   2.313 0.020736
*
Animacy+AN -AN:Prof.group.2int           3.0067     1.5528   1.936 0.052824
.
Animacy+AN -AN:Prof.group.2ns            1.3245     1.6071   0.824 0.409848

Syntaxs:Animacy+AN -AN:Prof.group.2int  -2.2056     2.0550  -1.073 0.283162

Syntaxs:Animacy+AN -AN:Prof.group.2ns   -2.3249     2.3108  -1.006 0.314360

---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


Henrik's first method via afex:mixed leads to:

> m4<-mixed(recallmodel4bisB3, data = recall, family = binomial, method =
"LRT")
Formula (the first argument) converted to formula.
Fitting 8 (g)lmer() models:
(8 warnings omitted)

> anova(m4)
Mixed Model Anova Table (Type 3 tests)

Model: Correct ~ Syntax * Animacy * Prof.group.2 + (1 | Part.name) +
Model:     (1 | Item)
Data: recall
Df full model: 14
                            Df   Chisq Chi Df Pr(>Chisq)
Syntax                      13  5.5659      1   0.018313 *
Animacy                     13  8.4710      1   0.003609 **
Prof.group.2                12 10.5099      2   0.005222 **
Syntax:Animacy              13  0.9832      1   0.321400
Syntax:Prof.group.2         12 15.8094      2   0.000369 ***
Animacy:Prof.group.2        12  3.9188      2   0.140945
Syntax:Animacy:Prof.group.2 12  1.2240      2   0.542272
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

The result is a main effect of Syntax, Animacy, Prof.group.2, and
interaction
between Syntax and Prof.Group.2. The summary(m4) is perfectly interpretable.

Henrik's second method yields:

*set contrasts*
>Syntax01 <- as.factor(1*(recall$Syntax=="of") + 2*(recall$Syntax=="''s"))
>Animacy01 <- as.factor(1*(recall$Animacy=="-AN +AN") +
2*(recall$Animacy=="+AN -AN"))
>Group012 <- as.factor(1*(recall$Prof.group.2=="adv") +
2*(recall$Prof.group.2=="int") + 3*(recall$Prof.group.2=="ns"))
>contrasts(Syntax01) <- contr.sum
>contrasts(Animacy01) <- contr.sum
>contrasts(Group012) <- contr.sum

*try second method*
> m5 <- bglmer (Correct~Syntax01 * Animacy01 * Group012 + (1 | Part.name) +
     (1 | Item), data = recall, control = glmerControl(optimizer =
"bobyqa"), nAGQ=1, family=binomial, expand_re= T)
Warning message:
extra argument(s) ?expand_re? disregarded
> car::Anova(m5, type = 3)
Analysis of Deviance Table (Type III Wald chisquare tests)

Response: Correct
                              Chisq Df Pr(>Chisq)
(Intercept)                 67.7409  1  < 2.2e-16 ***
Syntax01                     0.2856  1   0.593083
Animacy01                    6.2575  1   0.012367 *
Group012                     2.9888  2   0.224379
Syntax01:Animacy01           0.0970  1   0.755521
Syntax01:Group012            9.3054  2   0.009536 **
Animacy01:Group012           4.7633  2   0.092399 .
Syntax01:Animacy01:Group012  1.3704  2   0.503997

The result this time is a main effect of what was Animacy and interaction
between what was Syntax and Prof.Group.2 ?!

The summary(m5) is perfectly interpretable.



On Tue, Feb 23, 2016 at 6:17 PM, Fox, John <jfox at mcmaster.ca> wrote:

> Dear Emmanuel,
>
> First, the relevant linear hypothesis is for several coefficients
> simultaneously -- for example, all 3 coefficients for the contrasts
> representing a 4-level factor -- not for a single contrast. Although it's
> true that any linear combination of parameters that are 0 is 0, the
> converse isn't true. Second, for a GLMM, we really should be talking about
> type-III tests not type-III sums of squares.
>
> Type-III tests are dependent on coding in the full-rank parametrization of
> linear (and similar) models used in R, to make the tests correspond to
> reasonable hypotheses. The invariance of type-II tests with respect to
> coding is attractive, but shouldn't distract from the fundamental issues,
> which are the hypotheses that are tested and the power of the tests.
>
> Best,
>  John
>
> > -----Original Message-----
> > From: Emmanuel Curis [mailto:emmanuel.curis at parisdescartes.fr]
> > Sent: February 23, 2016 11:50 AM
> > To: Fox, John <jfox at mcmaster.ca>
> > Cc: Francesco Romano <francescobryanromano at gmail.com>; r-sig-mixed-
> > models at r-project.org
> > Subject: Re: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
> >
> > Dear Pr Fox,
> >
> > Thanks for your precision. But to summarize this test of, let's say 3
> > parameters to 0 for a 4-levels factor, by a single value with its SE, as
> > mentionned in Francesco's mail, the linear combination of these
> parameters
> > that is practically tested by this sum of square is needed, isn't it ?
> >
> > I mean, if really the parameters are all 0, whatever linear combination
> could
> > do the job, but type III sum of square just tests one of all possible
> linear
> > combinations, right?
> >
> > By the way, I was always very annoyed by the fact that Type III sum of
> > squares are so dependent on coding, but that's another debate...
> >
> > Best regards,
> >
> > On Tue, Feb 23, 2016 at 04:15:02PM +0000, Fox, John wrote:
> > < Dear Emmanuel,
> > <
> > < With proper contrast coding (i.e., a coding that's orthogonal in the
> *basis*
> > of the design, such as provided by contr.sum() ), a "type-III" test is
> just a test
> > that the corresponding parameters are 0. The models in question are
> > generalized linear (mixed) models and so sums of squares aren't really
> > involved, but one could do the corresponding Wald (like car::Anova) or LR
> > test. The Wald test is what you'd get with multcomp:glht or
> > car:linearHypothesis. BTW, I don't think that it would be hard for
> car::Anova
> > to be extended to provide LR tests in this case.
> > <
> > < Best,
> > <  John
> >
> > --
> >                                 Emmanuel CURIS
> >                                 emmanuel.curis at parisdescartes.fr
> >
> > Page WWW: http://emmanuel.curis.online.fr/index.html
>



-- 
Frank Romano Ph.D.

Tel. +39 3911639149

*LinkedIn*
https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162

*Academia.edu*
https://sheffield.academia.edu/FrancescoRomano

From peggy_moore at usgs.gov  Tue Feb 23 20:48:39 2016
From: peggy_moore at usgs.gov (Moore, Peggy)
Date: Tue, 23 Feb 2016 11:48:39 -0800
Subject: [R-sig-ME] glmmADMB error: no PSV file found
Message-ID: <CAPO7DkeXAH8eOYraTqV0jmgjQY--LqJTJ3jc2u+Va7p8xc100Q@mail.gmail.com>

Kathy:  My solution was to go to follow the instructions at:
http://glmmadmb.r-forge.r-project.org/
They indicate you should 1) go to the 'buildbot' page (link provided),
download the latest version of glmmadmb for your version of R and your
version of Windows, 2) save that file to the location of your glmmadmb.exe
file obtained with :
glmmADMB:::get_bin_loc()
They list other options for trouble shooting; however, they neglect to
point out the minor step that, after backing up your .exe file, you need to
rename the file you download to glmmadmb.exe.
Once I did that, I found the program then wrote the 4 mcmc output files to
the save.dir location: .hst, .ecm, .mcm, and .psv.
Peggy


> Message: 1
> Date: Tue, 23 Feb 2016 15:51:14 +0000 (UTC)
> From: Kathy <chenjeraikay at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] glmmADMB error: no PSV file found
> Message-ID: <loom.20160223T164934-960 at post.gmane.org>
> Content-Type: text/plain; charset=us-ascii
>
> Have you found a solution yet Peggy? I am also facing the same issue.
>
>
> Moore, Peggy <peggy_moore at ...> writes:
>
> >
> > I am using glmmadmb (glmmADMB_0.8.3.2 and R2admb_0.7.13) with R
> version
> > 3.2.1 (2015-06-18) to fit a negative binomial mixed effects model to
> my
> > data. Response is repeated measures of plant counts following
> herbicide
> > treatments. There are 2 fixed effects (year, treatment) and 2 random
> > effects (block, plot:block). I successfully fit the model when
> mcmc=FALSE,
> > but when I add mcmc=TRUE, I receive the error: no PSV file found.
> >
>

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Tue Feb 23 21:10:08 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Tue, 23 Feb 2016 20:10:08 +0000
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <CABZN5+GE=9ZK-L3Z=Vpj+fDVE+L1pcENu=+n8STs_=b_H=nWvA@mail.gmail.com>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
	<CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
	<20160223153223.GA16587@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A17C@FHSDB2D11-2.csu.mcmaster.ca>
	<20160223164947.GA29156@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A21D@FHSDB2D11-2.csu.mcmaster.ca>
	<CABZN5+GE=9ZK-L3Z=Vpj+fDVE+L1pcENu=+n8STs_=b_H=nWvA@mail.gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F5A31A@FHSDB2D11-2.csu.mcmaster.ca>

Dear Francesco,

For a 1-df test, the Wald chi-square is just Z^2, but the chi-square is more general. When a term in the model has more than 1 df, there is more than one beta (hat) and one SE (and covariances) for the coefficients in the term. If you want to see the individual coefficient estimates, then summary(mod) will show you each coefficient estimate, the SE for each estimate, Z, and p. Why one would want to look at the individual effect-coded coefficients and tests in this context escapes me. 

Best,
 John

> -----Original Message-----
> From: Francesco Romano [mailto:francescobryanromano at gmail.com]
> Sent: February 23, 2016 2:34 PM
> To: Fox, John <jfox at mcmaster.ca>
> Cc: Emmanuel Curis <emmanuel.curis at parisdescartes.fr>; r-sig-mixed-
> models at r-project.org
> Subject: Re: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
> 
> John,
> 
> I tried the Anova() function in the car package implemented with contr.sum()
> but it doesn't produce beta, SE, z, and p.
> To be more precise, R requires that either the F or Chi sq statistic be used.
> The model I used was termed "mod", here is the error:
> 
> 
> > Anova(mod, type=c("III"),
> +     test.statistic=c("LR"))
> Error in match.arg(test.statistic) : 'arg' should be one of ?Chisq?, ?F?
> 
> Chi square produces the following output:
> 
> > Anova(mod, type=c("III"),
> +     test.statistic=c("Chisq"))
> Analysis of Deviance Table (Type III Wald chisquare tests)
> 
> Response: Correct
>                               Chisq Df Pr(>Chisq)
> (Intercept)                 67.7409  1  < 2.2e-16 ***
> Syntax                       0.2856  1   0.593083
> Animacy                      6.2575  1   0.012367 *
> Prof.group.2                 2.9888  2   0.224379
> Syntax:Animacy               0.0970  1   0.755521
> Syntax:Prof.group.2          9.3054  2   0.009536 **
> Animacy:Prof.group.2         4.7633  2   0.092399 .
> Syntax:Animacy:Prof.group.2  1.3704  2   0.503997
> 
> So I still don't know how Raffrey et al. reported beta, SE, z, and p for a main
> effect of factor with 4 levels.
> If reviewers ask me to do this, I will argue that reporting chi square tests with
> corresponding p-values is a more accurate way of reporting main effects and
> interactions.
> 
> 
> If I haven't abused enough of your time, it would be beneficial to understand
> which of the two
> methods suggested by Henrik I should adopt. I attach my data.
> 
> The predictors of interest are Syntax (2 levels), Animacy (2 levels),
> Prof.group.2 (3 levels),
> and the outcome 'correct', while the random effects are 'Part.name' and
> 'Item'. The best model fit is a
> bglmer with glmerControl(optimizer = "bobyqa") and nAGQ=1
> 
> > summary(recallmodel4bisB3)
> Cov prior  : Part.name ~ wishart(df = 3.5, scale = Inf, posterior.scale = cov,
> common.scale = TRUE)
>            : Item ~ wishart(df = 3.5, scale = Inf, posterior.scale = cov,
> common.scale = TRUE)
> Prior dev  : 1.3565
> 
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['bglmerMod']
>  Family: binomial  ( logit )
> Formula: Correct ~ Syntax * Animacy * Prof.group.2 + (1 | Part.name) +      (1
> | Item)
>    Data: recall
> Control: glmerControl(optimizer = "bobyqa")
> 
>      AIC      BIC   logLik deviance df.resid
>    313.3    372.9   -142.6    285.3      509
> 
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -1.3517 -0.2926 -0.1802 -0.1137  9.3666
> 
> Random effects:
>  Groups    Name        Variance Std.Dev.
>  Part.name (Intercept) 0.8046   0.8970
>  Item      (Intercept) 0.5031   0.7093
> Number of obs: 523, groups:  Part.name, 42; Item, 16
> 
> Fixed effects:
>                                        Estimate Std. Error z value Pr(>|z|)
> (Intercept)                             -0.8960     0.6317  -1.418 0.156071
> Syntaxs                                 -2.0713     0.9447  -2.193 0.028342 *
> Animacy+AN -AN                          -3.0539     1.2548  -2.434 0.014941 *
> Prof.group.2int                         -2.5594     0.9473  -2.702 0.006898 **
> Prof.group.2ns                          -1.8673     0.7634  -2.446 0.014442 *
> Syntaxs:Animacy+AN -AN                   1.8642     1.8202   1.024 0.305750
> Syntaxs:Prof.group.2int                  4.1704     1.1676   3.572 0.000355 ***
> Syntaxs:Prof.group.2ns                   2.4244     1.0483   2.313 0.020736 *
> Animacy+AN -AN:Prof.group.2int           3.0067     1.5528   1.936 0.052824 .
> Animacy+AN -AN:Prof.group.2ns            1.3245     1.6071   0.824 0.409848
> Syntaxs:Animacy+AN -AN:Prof.group.2int  -2.2056     2.0550  -1.073 0.283162
> Syntaxs:Animacy+AN -AN:Prof.group.2ns   -2.3249     2.3108  -1.006 0.314360
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> 
> Henrik's first method via afex:mixed leads to:
> 
> > m4<-mixed(recallmodel4bisB3, data = recall, family = binomial, method =
> "LRT")
> Formula (the first argument) converted to formula.
> Fitting 8 (g)lmer() models:
> (8 warnings omitted)
> 
> > anova(m4)
> Mixed Model Anova Table (Type 3 tests)
> 
> Model: Correct ~ Syntax * Animacy * Prof.group.2 + (1 | Part.name) +
> Model:     (1 | Item)
> Data: recall
> Df full model: 14
>                             Df   Chisq Chi Df Pr(>Chisq)
> Syntax                      13  5.5659      1   0.018313 *
> Animacy                     13  8.4710      1   0.003609 **
> Prof.group.2                12 10.5099      2   0.005222 **
> Syntax:Animacy              13  0.9832      1   0.321400
> Syntax:Prof.group.2         12 15.8094      2   0.000369 ***
> Animacy:Prof.group.2        12  3.9188      2   0.140945
> Syntax:Animacy:Prof.group.2 12  1.2240      2   0.542272
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> The result is a main effect of Syntax, Animacy, Prof.group.2, and interaction
> between Syntax and Prof.Group.2. The summary(m4) is perfectly
> interpretable.
> 
> Henrik's second method yields:
> 
> *set contrasts*
> >Syntax01 <- as.factor(1*(recall$Syntax=="of") + 2*(recall$Syntax=="''s"))
> 
> >Animacy01 <- as.factor(1*(recall$Animacy=="-AN +AN") +
> 2*(recall$Animacy=="+AN -AN"))
> 
> >Group012 <- as.factor(1*(recall$Prof.group.2=="adv") +
> 2*(recall$Prof.group.2=="int") + 3*(recall$Prof.group.2=="ns"))
> >contrasts(Syntax01) <- contr.sum
> 
> >contrasts(Animacy01) <- contr.sum
> 
> >contrasts(Group012) <- contr.sum
> 
> 
> *try second method*
> > m5 <- bglmer (Correct~Syntax01 * Animacy01 * Group012 + (1 | Part.name)
> +      (1 | Item), data = recall, control = glmerControl(optimizer = "bobyqa"),
> nAGQ=1, family=binomial, expand_re= T)
> Warning message:
> extra argument(s) ?expand_re? disregarded
> > car::Anova(m5, type = 3)
> Analysis of Deviance Table (Type III Wald chisquare tests)
> 
> Response: Correct
>                               Chisq Df Pr(>Chisq)
> (Intercept)                 67.7409  1  < 2.2e-16 ***
> Syntax01                     0.2856  1   0.593083
> Animacy01                    6.2575  1   0.012367 *
> Group012                     2.9888  2   0.224379
> Syntax01:Animacy01           0.0970  1   0.755521
> Syntax01:Group012            9.3054  2   0.009536 **
> Animacy01:Group012           4.7633  2   0.092399 .
> Syntax01:Animacy01:Group012  1.3704  2   0.503997
> 
> The result this time is a main effect of what was Animacy and interaction
> between what was Syntax and Prof.Group.2 ?!
> 
> The summary(m5) is perfectly interpretable.
> 
> 
> 
> On Tue, Feb 23, 2016 at 6:17 PM, Fox, John <jfox at mcmaster.ca
> <mailto:jfox at mcmaster.ca> > wrote:
> 
> 
> 	Dear Emmanuel,
> 
> 	First, the relevant linear hypothesis is for several coefficients
> simultaneously -- for example, all 3 coefficients for the contrasts
> representing a 4-level factor -- not for a single contrast. Although it's true
> that any linear combination of parameters that are 0 is 0, the converse isn't
> true. Second, for a GLMM, we really should be talking about type-III tests not
> type-III sums of squares.
> 
> 	Type-III tests are dependent on coding in the full-rank
> parametrization of linear (and similar) models used in R, to make the tests
> correspond to reasonable hypotheses. The invariance of type-II tests with
> respect to coding is attractive, but shouldn't distract from the fundamental
> issues, which are the hypotheses that are tested and the power of the tests.
> 
> 	Best,
> 	 John
> 
> 	> -----Original Message-----
> 	> From: Emmanuel Curis [mailto:emmanuel.curis at parisdescartes.fr
> <mailto:emmanuel.curis at parisdescartes.fr> ]
> 	> Sent: February 23, 2016 11:50 AM
> 	> To: Fox, John <jfox at mcmaster.ca <mailto:jfox at mcmaster.ca> >
> 	> Cc: Francesco Romano <francescobryanromano at gmail.com
> <mailto:francescobryanromano at gmail.com> >; r-sig-mixed-
> 	> models at r-project.org <mailto:models at r-project.org>
> 	> Subject: Re: [R-sig-ME] Replicating type III anova tests for
> glmer/GLMM
> 	>
> 
> 	> Dear Pr Fox,
> 	>
> 	> Thanks for your precision. But to summarize this test of, let's say 3
> 	> parameters to 0 for a 4-levels factor, by a single value with its SE, as
> 	> mentionned in Francesco's mail, the linear combination of these
> parameters
> 	> that is practically tested by this sum of square is needed, isn't it ?
> 	>
> 	> I mean, if really the parameters are all 0, whatever linear
> combination could
> 	> do the job, but type III sum of square just tests one of all possible
> linear
> 	> combinations, right?
> 	>
> 	> By the way, I was always very annoyed by the fact that Type III sum
> of
> 	> squares are so dependent on coding, but that's another debate...
> 	>
> 	> Best regards,
> 	>
> 	> On Tue, Feb 23, 2016 at 04:15:02PM +0000, Fox, John wrote:
> 	> < Dear Emmanuel,
> 	> <
> 	> < With proper contrast coding (i.e., a coding that's orthogonal in the
> *basis*
> 	> of the design, such as provided by contr.sum() ), a "type-III" test is
> just a test
> 	> that the corresponding parameters are 0. The models in question
> are
> 	> generalized linear (mixed) models and so sums of squares aren't
> really
> 	> involved, but one could do the corresponding Wald (like
> car::Anova) or LR
> 	> test. The Wald test is what you'd get with multcomp:glht or
> 	> car:linearHypothesis. BTW, I don't think that it would be hard for
> car::Anova
> 	> to be extended to provide LR tests in this case.
> 	> <
> 	> < Best,
> 	> <  John
> 	>
> 	> --
> 	>                                 Emmanuel CURIS
> 	>                                 emmanuel.curis at parisdescartes.fr
> <mailto:emmanuel.curis at parisdescartes.fr>
> 	>
> 	> Page WWW: http://emmanuel.curis.online.fr/index.html
> 
> 
> 
> 
> 
> --
> 
> Frank Romano Ph.D.
> 
> Tel. +39 3911639149
> 
> 
> LinkedIn
> https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162
> 
> 
> Academia.edu
> https://sheffield.academia.edu/FrancescoRomano


From russell-lenth at uiowa.edu  Tue Feb 23 21:31:01 2016
From: russell-lenth at uiowa.edu (Lenth, Russell V)
Date: Tue, 23 Feb 2016 20:31:01 +0000
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLM
Message-ID: <BY2PR0401MB09196B77E14FCB65250A45D2F1A40@BY2PR0401MB0919.namprd04.prod.outlook.com>

Type III tests were originally formulated in terms of contrasts that give equal weight to levels of any interactions that contain the effect in question. This in turn depends on all those contrasts being estimable. Such contrast-based tests are possible to obtain (one at a time) using the lsmeans package - which does support glmer models. It goes something like this:
 
    require(lsmeans)
    rg <- ref.grid(model)
    str(rg at nbasis)

If the previous result is a matrix with one or more columns, then that means some contrasts are not estimable; and this throws off some or all of the type III tests obtained using the method below. Otherwise, you can proceed. For example, the type III test of an interaction of factors A and B is obtained by:

    test(contrast(lsmeans(rg, ~ A*B), interaction = "trt.vs.ctrl"), joint = TRUE)

Provided everything is estimable, this does NOT depend on a particular coding of the factors.

If there are non-estimable contrasts, the results (and even the degrees of freedom) vary depending on what factor coding is used, and what contrast family (e.g., "trt.vs.ctrl", "consec", "pairwise", etc.) is used. To make any sense at all of them in such a case, you'd have to look at the contrasts themselves to see what is being tested. The same is true in SAS, which offers "type IV" tests, which are also dependent on coding, ordering of levels, etc.

Russ


Russell V. Lenth  -  Professor Emeritus
Department of Statistics and Actuarial Science   
The University of Iowa  -  Iowa City, IA 52242  USA   
Voice (319)335-0712 (Dept. office)  -  FAX (319)335-3017

Just because you have numbers, that doesn't necessarily mean you have data.

> Dear Pr Fox,
>
> Thanks for your precision. But to summarize this test of, let's say 3 parameters to 0 for a 4-levels factor, by a single value with its SE, as mentionned in Francesco's mail, the linear combination of these parameters that is practically tested by this sum of square is needed, isn't it ?
> 
> I mean, if really the parameters are all 0, whatever linear combination could do the job, but type III sum of square just tests one of all possible linear combinations, right?
> 
> By the way, I was always very annoyed by the fact that Type III sum of squares are so dependent on coding, but that's another debate...
> 
> Best regards,
> 
> -- 
>                                 Emmanuel CURIS
>                                 emmanuel.curis at parisdescartes.fr


From emmanuel.curis at parisdescartes.fr  Wed Feb 24 10:54:07 2016
From: emmanuel.curis at parisdescartes.fr (Emmanuel Curis)
Date: Wed, 24 Feb 2016 10:54:07 +0100
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC810F5A21D@FHSDB2D11-2.csu.mcmaster.ca>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
	<CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
	<20160223153223.GA16587@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A17C@FHSDB2D11-2.csu.mcmaster.ca>
	<20160223164947.GA29156@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A21D@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <20160224095407.GA24770@info124.pharmacie.univ-paris5.fr>

Dear Pr Fox,

Thanks for taking time for this discussion. I think I made a few
shortcuts that are wrong, and I still have some not understood issues
about the kind of tests even in the simplest case of linear models...

First, I think I mixed contrast and quadratic forms expectations in my
answer, I apologize for that; what I had in mind when answering
Francesco was in fact the expectation of the quadratic form, and I too
quickly deduced that there was an equivalent linear combination of the
parameters as its ? square root ?, but this was obviously wrong since
the L matrix in a Lt W L quadratic form does not have to be a column
matrix. Am I wrong thinking that typically in such tests, the L matrix
is precisely a multi-column matrix (hence also several degrees of
freedom associated), and that several contrasts are tested
simultaneously?

I precise that I call ? contrast ? a linear combination of the model
parameters with the constraint that the coefficients of this
combination sum to 0 ? this is the definition in French (? contraste ?),
but I may use it wrongly in English?

Second, I may have wrongly understood the definitions of the various
tests, and especially how they generalize from linear model to
GLM/GLMM...

I thought type I was by taking the squared distance of the successive
orthogonal projections on the subspaces generated by the various
terms, in the order given in the model; type II, by ensuring that
the term tested was the last amongst terms of same order, after
terms of lower order but before terms of higher order; type III, by
projecting on the subspace after removal of the basis vectors for the
term tested ? hence its strong dependency on the coding scheme, and
the ? drop1 ? trick to get them.

Is this definition correct? Does it generalize to other kind models,
or is another definition required? Is it unambiguous? The SAS doc
itself suggests that various procedures call "type II" different kind
of things

However, I cannot see clearly which hypothesis is indeed tested in
each case, especially in terms of cell means or marginal means (and,
when I really need it, I start from them and select the contrasts I
need).  Is there any package/software that allows to print the
hypotheses testeds in terms of means starting from the model formula?
Or is there any good reference that makes the link between the two?
For instance, a demonstration that the comparison of marginal means
? always ? leads to a type XXX sum of square?

Best regards,

On Tue, Feb 23, 2016 at 05:17:29PM +0000, Fox, John wrote:
? Dear Emmanuel,
? 
? First, the relevant linear hypothesis is for several coefficients simultaneously -- for example, all 3 coefficients for the contrasts representing a 4-level factor -- not for a single contrast. Although it's true that any linear combination of parameters that are 0 is 0, the converse isn't true. Second, for a GLMM, we really should be talking about type-III tests not type-III sums of squares.
? 
? Type-III tests are dependent on coding in the full-rank parametrization of linear (and similar) models used in R, to make the tests correspond to reasonable hypotheses. The invariance of type-II tests with respect to coding is attractive, but shouldn't distract from the fundamental issues, which are the hypotheses that are tested and the power of the tests. 
? 
? Best,
?  John

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From bates at stat.wisc.edu  Tue Feb 23 18:41:40 2016
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 23 Feb 2016 17:41:40 +0000
Subject: [R-sig-ME] Fitting LMMs using the MixedModels package for Julia
Message-ID: <CAO7JsnRoXi4GRdjbMkQRS_5AC22d9yw-91vEBDb0TcHvg0FPWA@mail.gmail.com>

As many readers of this list are aware, most of my development effort for
the last few years has been in the Julia language, in particular the
MixedModels package for Julia.  There are several aspects of the Julia
language that allow for writing faster code than in R, especially for
iterative fitting of models to large data sets.  The downside for the user
of switching to another language is, well, switching to another language.

Some users have taken the plunge and used Julia because the model fits in R
using lme4 were taking a long time, as in many hours.  They have seen one
to two orders of magnitude differences in speed which, when things are
taking that long, is worth the pain of switching.  If the fit in R is only
taking a few seconds then it is not worthwhile learning a new language just
to make that faster.

I think that performing an lmer-like fit in Julia is now sufficiently
straightforward that it will be worthwhile for others to try doing so.  We
have developed a Julia package called RCall which allows a Julia user to
run an R process from within Julia.  In particular, RCall makes it easy to
create a copy of an R data.frame as a Julia DataFrame object and use that
to fit a linear mixed model.

The steps are reasonably straightforward.  I will illustrate with data on
ratings of  about 3700 movies by about 6000 users.  Of course, not every
user rates every movie.  There are about 1,000,000 ratings in the data set.

These data are available as the MovieLens 1M Dataset at
http://grouplens.org/datasets/movielens/  and as the ml1m data set in the
Timings  R package available at https://github.com/Stat990-033/Timings.
That package is not on CRAN because the data sets are too large.  You must
install it in R with

install.packages("devtools")
devtools::Install_github("Stat990-033/Timings")

Downloads of Julia itself are at http://julialang.org/downloads/  After
installing Julia start it and add the packages

Pkg.add("RCall")
Pkg.add("MixedModels")

The actual model fit is performed as

julia> using DataFrames, MixedModels, RCall

julia> ml1m = rcopy("Timings::ml1m");

julia> @time m1 = fit!(lmm(Y ~ 1 + (1|G) + (1|H), ml1m))
 24.956579 seconds (39.95 M allocations: 1.398 GB, 1.06% gc time)
Linear mixed model fit by maximum likelihood
 logLik: -1331986.005811, deviance: 2663972.011622, AIC: 2663980.011622,
BIC: 2664027.274500

Variance components:
           Variance   Std.Dev.
 G        0.12985210 0.36034996
 H        0.36879694 0.60728654
 Residual 0.81390867 0.90216887
 Number of obs: 1000209; levels of grouping factors: 6040, 3706

  Fixed-effects parameters:
             Estimate Std.Error z value
(Intercept)   3.33902 0.0114624 291.302


The "using" directive is similar to the "library" or "require" functions in
R. The named Julia packages are, in R terminology, loaded and attached.

The "Timings::ml1m" expression is an R expression.  It accesses the ml1m
object in the Timings package, loading the package first, if necessary.
The call to the Julia function lmm is similar to lmer but only creates the
model.  The call to fit! is what causes the model to be fit.

As you can see, this fit takes about 25 seconds.  A similar fit using lmer
takes about 40 minutes on the same machine.

I would be happy to answer questions about the MixedModels package but I
don't think this forum would be appropriate.  It is a forum for questions
about fitting mixed-effects models with R.  For the time being I would
suggest asking questions on the Google group called julia-stats
https://groups.google.com/forum/#!forum/julia-stats to which I am sending a
copy of this message.

	[[alternative HTML version deleted]]


From bates at stat.wisc.edu  Tue Feb 23 19:06:35 2016
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 23 Feb 2016 18:06:35 +0000
Subject: [R-sig-ME] Fitting LMMs using the MixedModels package for Julia
In-Reply-To: <CAO7JsnRoXi4GRdjbMkQRS_5AC22d9yw-91vEBDb0TcHvg0FPWA@mail.gmail.com>
References: <CAO7JsnRoXi4GRdjbMkQRS_5AC22d9yw-91vEBDb0TcHvg0FPWA@mail.gmail.com>
Message-ID: <CAO7JsnTZnnRVhQUGP4nbxdM8uEtrwta5H9J_GJ0mUf=VNDPEEw@mail.gmail.com>

My statement that fitting this model using lmer on the same machine took
about 40 minutes was hearsay and quite inaccurate.  When I checked it took
less than 3 minutes

> system.time(m1 <- lmer(Y ~ 1 + (1|G) + (1|H), ml1m, REML=FALSE))
   user  system elapsed
341.216 225.748 162.243
> m1
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: Y ~ 1 + (1 | G) + (1 | H)
   Data: ml1m
     AIC      BIC   logLik deviance df.resid
 2663980  2664027 -1331986  2663972  1000205
Random effects:
 Groups   Name        Std.Dev.
 G        (Intercept) 0.3603
 H        (Intercept) 0.6073
 Residual             0.9022
Number of obs: 1000209, groups:  G, 6040; H, 3706
Fixed Effects:
(Intercept)
      3.339


On Tue, Feb 23, 2016 at 11:41 AM Douglas Bates <bates at stat.wisc.edu> wrote:

> As many readers of this list are aware, most of my development effort for
> the last few years has been in the Julia language, in particular the
> MixedModels package for Julia.  There are several aspects of the Julia
> language that allow for writing faster code than in R, especially for
> iterative fitting of models to large data sets.  The downside for the user
> of switching to another language is, well, switching to another language.
>
> Some users have taken the plunge and used Julia because the model fits in
> R using lme4 were taking a long time, as in many hours.  They have seen one
> to two orders of magnitude differences in speed which, when things are
> taking that long, is worth the pain of switching.  If the fit in R is only
> taking a few seconds then it is not worthwhile learning a new language just
> to make that faster.
>
> I think that performing an lmer-like fit in Julia is now sufficiently
> straightforward that it will be worthwhile for others to try doing so.  We
> have developed a Julia package called RCall which allows a Julia user to
> run an R process from within Julia.  In particular, RCall makes it easy to
> create a copy of an R data.frame as a Julia DataFrame object and use that
> to fit a linear mixed model.
>
> The steps are reasonably straightforward.  I will illustrate with data on
> ratings of  about 3700 movies by about 6000 users.  Of course, not every
> user rates every movie.  There are about 1,000,000 ratings in the data set.
>
> These data are available as the MovieLens 1M Dataset at
> http://grouplens.org/datasets/movielens/  and as the ml1m data set in the
> Timings  R package available at https://github.com/Stat990-033/Timings.
> That package is not on CRAN because the data sets are too large.  You must
> install it in R with
>
> install.packages("devtools")
> devtools::Install_github("Stat990-033/Timings")
>
> Downloads of Julia itself are at http://julialang.org/downloads/  After
> installing Julia start it and add the packages
>
> Pkg.add("RCall")
> Pkg.add("MixedModels")
>
> The actual model fit is performed as
>
> julia> using DataFrames, MixedModels, RCall
>
> julia> ml1m = rcopy("Timings::ml1m");
>
> julia> @time m1 = fit!(lmm(Y ~ 1 + (1|G) + (1|H), ml1m))
>  24.956579 seconds (39.95 M allocations: 1.398 GB, 1.06% gc time)
> Linear mixed model fit by maximum likelihood
>  logLik: -1331986.005811, deviance: 2663972.011622, AIC: 2663980.011622,
> BIC: 2664027.274500
>
> Variance components:
>            Variance   Std.Dev.
>  G        0.12985210 0.36034996
>  H        0.36879694 0.60728654
>  Residual 0.81390867 0.90216887
>  Number of obs: 1000209; levels of grouping factors: 6040, 3706
>
>   Fixed-effects parameters:
>              Estimate Std.Error z value
> (Intercept)   3.33902 0.0114624 291.302
>
>
> The "using" directive is similar to the "library" or "require" functions
> in R. The named Julia packages are, in R terminology, loaded and attached.
>
> The "Timings::ml1m" expression is an R expression.  It accesses the ml1m
> object in the Timings package, loading the package first, if necessary.
> The call to the Julia function lmm is similar to lmer but only creates the
> model.  The call to fit! is what causes the model to be fit.
>
> As you can see, this fit takes about 25 seconds.  A similar fit using lmer
> takes about 40 minutes on the same machine.
>
> I would be happy to answer questions about the MixedModels package but I
> don't think this forum would be appropriate.  It is a forum for questions
> about fitting mixed-effects models with R.  For the time being I would
> suggest asking questions on the Google group called julia-stats
> https://groups.google.com/forum/#!forum/julia-stats to which I am sending
> a copy of this message.
>
>

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Wed Feb 24 17:08:42 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Wed, 24 Feb 2016 16:08:42 +0000
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <20160224095407.GA24770@info124.pharmacie.univ-paris5.fr>
References: <CABZN5+E5Uj1su4eW=Tqm5mtyod_CcqvPcjAoGvu+QuEt0hs+1A@mail.gmail.com>
	<5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
	<CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
	<20160223153223.GA16587@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A17C@FHSDB2D11-2.csu.mcmaster.ca>
	<20160223164947.GA29156@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A21D@FHSDB2D11-2.csu.mcmaster.ca>
	<20160224095407.GA24770@info124.pharmacie.univ-paris5.fr>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F5A897@FHSDB2D11-2.csu.mcmaster.ca>

Dear Emmanuel,

The questions you raise are sufficiently complicated that it's difficult to address them adequately in an email. My Applied Regression and Generalized Linear Models text, for example, takes about 15 pages to explain the relationships among regressor codings, hypotheses, and tests in 2-way ANOVA, working with the full-rank parametrization of the model, and it's possible (as Russell Lenth indicated) to work things out even more generally. 

I'll try to answer briefly, however.

> -----Original Message-----
> From: Emmanuel Curis [mailto:emmanuel.curis at parisdescartes.fr]
> Sent: February 24, 2016 4:54 AM
> To: Fox, John <jfox at mcmaster.ca>
> Cc: Francesco Romano <francescobryanromano at gmail.com>; r-sig-mixed-
> models at r-project.org
> Subject: Re: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
> 
> Dear Pr Fox,
> 
> Thanks for taking time for this discussion. I think I made a few shortcuts that
> are wrong, and I still have some not understood issues about the kind of
> tests even in the simplest case of linear models...
> 
> First, I think I mixed contrast and quadratic forms expectations in my answer,
> I apologize for that; what I had in mind when answering Francesco was in fact

No need to apologize. I don't think that these are simple ideas.

> the expectation of the quadratic form, and I too quickly deduced that there
> was an equivalent linear combination of the parameters as its ? square root
> ?, but this was obviously wrong since the L matrix in a Lt W L quadratic form
> does not have to be a column matrix. Am I wrong thinking that typically in
> such tests, the L matrix is precisely a multi-column matrix (hence also several
> degrees of freedom associated), and that several contrasts are tested
> simultaneously?

Thinking in terms of the full-rank parametrization, as used in R, each type-III hypothesis is that several coefficients are simultaneously 0, which can be simply formulated as a linear hypothesis assuming an appropriate coding of the regressors for a factor. Type-II hypotheses can also be formulated as linear hypotheses, but doing so is more complicated. The Anova() function uses a kind of projection, in effect defining a type-II test as the most powerful test of a conditional hypothesis such as no A main effect given that the A:B interaction is absent in the model y ~ A*B. This works both for linear models, where (unless there is a complication like missing cells), the resulting test corresponds to the test produced by comparing the models y ~ A and y ~ A + B, using Y ~ A*B for the estimate of error variance (i.e., the denominator MS), and more generally for models with linear predictors, where it's in general possible to formulate the (Wald) tests in terms of the coefficient estimates and their covariance matrix.

> 
> I precise that I call ? contrast ? a linear combination of the model parameters
> with the constraint that the coefficients of this combination sum to 0 ? this is
> the definition in French (? contraste ?), but I may use it wrongly in English?

I'd define a "contrast" as the weights associated with the levels of a factor for formulating a hypothesis, where the weights traditionally are constrained to sum to 0, and to differentiate this from a column of the model matrix, which I'd more generally term a "regressor." Often, a traditional set of contrasts for a factor, one less than the number of levels, are defined not only to sum to 0  but also to be orthogonal in the basis of the design. The usage in R is more general, where "contrasts" mean the set of regressors used to represent a factor. Thus, contr.sum() generates regressors that satisfy the traditional definition of contrasts, as do contr.poly() and contr.helmert(), but the default contr.treatment() generates 0/1 dummy-coded regressors that don't satisfy the traditional definition of contrasts.

> 
> Second, I may have wrongly understood the definitions of the various tests,
> and especially how they generalize from linear model to GLM/GLMM...
> 
> I thought type I was by taking the squared distance of the successive
> orthogonal projections on the subspaces generated by the various terms, in
> the order given in the model; type II, by ensuring that the term tested was
> the last amongst terms of same order, after terms of lower order but before
> terms of higher order; type III, by projecting on the subspace after removal
> of the basis vectors for the term tested ? hence its strong dependency on
> the coding scheme, and the ? drop1 ? trick to get them.
> 
> Is this definition correct? Does it generalize to other kind models, or is
> another definition required? Is it unambiguous? The SAS doc itself suggests
> that various procedures call "type II" different kind of things

Yes, if I've followed this correctly, it's correct, and it explains why it's possible to formulate the different types of tests in linear models independently of the contrasts (regressors) used to code the factors -- because fundamentally what's important is the subspace spanned by the regressors in each model, which is independent of coding. This approach, however, doesn't generalize easily beyond linear models fit by least squares. The approach taken in Anova() corresponds to this approach in linear models fit by least squares as long as the models remain full-rank and for type-III tests as long as the contrasts are properly formulated, and generalizes to other models with linear predictors.

> 
> However, I cannot see clearly which hypothesis is indeed tested in each case,
> especially in terms of cell means or marginal means (and, when I really need
> it, I start from them and select the contrasts I need).  Is there any
> package/software that allows to print the hypotheses testeds in terms of
> means starting from the model formula?
> Or is there any good reference that makes the link between the two?
> For instance, a demonstration that the comparison of marginal means ?
> always ? leads to a type XXX sum of square?

This is where a complete explanation gets too lengthy for an email, but a shorthand formulation, e.g., for the model y ~ A*B, is that type-I tests correspond to the hypotheses A|(B = 0, AB = 0), B | AB = 0, AB = 0; type-II tests to A | AB = 0, B | AB = 0, AB = 0; and type-III tests to A = 0, B = 0, AB = 0. Here, e.g., | AB = 0 means assuming no AB interactions, so, e.g., the hypothesis A | AB = 0 means no A main effects assuming no AB interactions. A hypothesis like A = 0 is indeed formulated in terms of marginal means, understood as cell means for A averaging over the levels of B (not level means of A ignoring B).

I realize that this is far from a complete explanation.

Best,
 John

> 
> Best regards,
> 
> On Tue, Feb 23, 2016 at 05:17:29PM +0000, Fox, John wrote:
> ? Dear Emmanuel,
> ?
> ? First, the relevant linear hypothesis is for several coefficients
> simultaneously -- for example, all 3 coefficients for the contrasts
> representing a 4-level factor -- not for a single contrast. Although it's true
> that any linear combination of parameters that are 0 is 0, the converse isn't
> true. Second, for a GLMM, we really should be talking about type-III tests not
> type-III sums of squares.
> ?
> ? Type-III tests are dependent on coding in the full-rank parametrization of
> linear (and similar) models used in R, to make the tests correspond to
> reasonable hypotheses. The invariance of type-II tests with respect to coding
> is attractive, but shouldn't distract from the fundamental issues, which are
> the hypotheses that are tested and the power of the tests.
> ?
> ? Best,
> ?  John
> 
> --
>                                 Emmanuel CURIS
>                                 emmanuel.curis at parisdescartes.fr
> 
> Page WWW: http://emmanuel.curis.online.fr/index.html

From pauljohn32 at gmail.com  Wed Feb 24 18:12:00 2016
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Wed, 24 Feb 2016 11:12:00 -0600
Subject: [R-sig-ME] Troublesome example of lmer fitting an overidentified
 model; Stata bad too.
Message-ID: <CAErODj8qFHD+BHFTSJjNRg5fByPgENauWRxLj5+DZ0ni-30Vqg@mail.gmail.com>

I'm teaching mixed models, working through a lot of homework problems
in the 2 volume set Multilevel and Longitudinal Modeling using Stata,
by S Rave-Hesketh and A. Skrondal. If you did not look at it yet, I
expect you'll find lots of useful, interesting exercises.

Their Exercise 4.4 is about yield from 10 wheat varieties.  The variables are:

"plot"     "variety"  "yield"    "moist"

variety is the grouping variable in the exercise, but it comes in as a
floating point number. I thought that was the root of the surprise
described here, but now I don't think so.

I was reading one student's homework in Stata and I was
stunned/surprised that Stata's "mixed" function would fit a random
effects model that included the grouping variable as a fixed effect
predictor as well.  This appeared to be grossly overidentified to me.

Here's the stata input and output,

. mixed yield c.moist##i.variety || variety:moist, mle stddev

Performing EM optimization:

Performing gradient-based optimization:

Iteration 0:   log likelihood = -42.123671
Iteration 1:   log likelihood = -41.540417
Iteration 2:   log likelihood = -41.520012
Iteration 3:   log likelihood = -41.520012

Computing standard errors:

Mixed-effects ML regression                     Number of obs     =         60
Group variable: variety                         Number of groups  =         10

                                                Obs per group:
                                                              min =          6
                                                              avg =        6.0
                                                              max =          6

                                                Wald chi2(19)     =   37795.42
Log likelihood = -41.520012                     Prob > chi2       =     0.0000

---------------------------------------------------------------------------------
          yield |      Coef.   Std. Err.      z    P>|z|     [95%
Conf. Interval]
----------------+----------------------------------------------------------------
          moist |   .6077128   .0124644    48.76   0.000      .583283
  .6321425
                |
        variety |
             2  |  -2.844581   .8429087    -3.37   0.001    -4.496652
 -1.192511
             3  |  -1.871277   .7903544    -2.37   0.018    -3.420343
 -.3222105
             4  |  -.3432833   .7323671    -0.47   0.639    -1.778696
   1.09213
             5  |   .2294764   1.121912     0.20   0.838     -1.96943
  2.428383
             6  |    3.46207    .690504     5.01   0.000     2.108707
  4.815433
             7  |  -12.05622   .6729943   -17.91   0.000    -13.37527
 -10.73718
             8  |   1.175528   .7302294     1.61   0.107    -.2556957
  2.606751
             9  |  -1.434985   .7917805    -1.81   0.070    -2.986846
  .1168767
            10  |   3.494313   1.392236     2.51   0.012     .7655807
  6.223044
                |
variety#c.moist |
             2  |  -.0354326   .0260328    -1.36   0.173    -.0864559
  .0155908
             3  |   .1297872   .0190785     6.80   0.000     .0923941
  .1671804
             4  |   .0264085   .0210603     1.25   0.210    -.0148689
   .067686
             5  |   .0284318   .0240342     1.18   0.237    -.0186744
   .075538
             6  |   .0790988   .0161154     4.91   0.000     .0475132
  .1106844
             7  |   .1164752   .0189384     6.15   0.000     .0793566
  .1535938
             8  |   .0791545   .0188487     4.20   0.000     .0422118
  .1160972
             9  |    .080266   .0190236     4.22   0.000     .0429804
  .1175516
            10  |   .0027604   .0282729     0.10   0.922    -.0526535
  .0581742
                |
          _cons |   34.58378   .5478187    63.13   0.000     33.51007
  35.65748
---------------------------------------------------------------------------------

------------------------------------------------------------------------------
  Random-effects Parameters  |   Estimate   Std. Err.     [95% Conf. Interval]
-----------------------------+------------------------------------------------
variety: Independent         |
                   sd(moist) |   3.46e-15   2.27e-14      8.96e-21    1.34e-09
                   sd(_cons) |   1.18e-11   1.34e-07             0           .
-----------------------------+------------------------------------------------
                sd(Residual) |   .4833867   .0441285      .4041925    .5780976
------------------------------------------------------------------------------
LR test vs. linear model: chi2(2) = 0.00                  Prob > chi2 = 1.0000

Note: LR test is conservative and provided only for reference.


As you can see, Stata deals with the overidentification by reporting
miniscule estimates for the random effects.

I think its a bug, wondered what lme4 would do. I predicted it would
refuse to fit at all, as soon as we declare variety as a factor
(varietyf).  However, that does not happen. The full R runable example
is below, but here's the bad part:

> m.wrong <- lmer(yield ~ moist*varietyf + (moist|varietyf), data = dat)
> summary(m.wrong)
Linear mixed model fit by REML ['lmerMod']
Formula: yield ~ moist * varietyf + (moist | varietyf)
   Data: dat

REML criterion at convergence: 157.7

Scaled residuals:
     Min       1Q   Median       3Q      Max
-1.74078 -0.52638 -0.00039  0.56385  1.79231

Random effects:
 Groups   Name        Variance Std.Dev. Corr
 varietyf (Intercept) 0.3505   0.592
          moist       0.3505   0.592    0.00
 Residual             0.3505   0.592
Number of obs: 60, groups:  varietyf, 10

Fixed effects:
                  Estimate Std. Error t value
(Intercept)       34.58378    0.89479   38.65
moist              0.60771    0.59222    1.03
varietyf2         -2.84458    1.32918   -2.14
varietyf3         -1.87128    1.27984   -1.46
varietyf4         -0.34328    1.22700   -0.28
varietyf5          0.22948    1.60904    0.14
varietyf6          3.46207    1.19003    2.91
varietyf7        -12.05622    1.17489  -10.26
varietyf8          1.17553    1.22509    0.96
varietyf9         -1.43498    1.28116   -1.12
varietyf10         3.49431    1.89960    1.84
moist:varietyf2   -0.03543    0.83786   -0.04
moist:varietyf3    0.12979    0.83758    0.15
moist:varietyf4    0.02641    0.83765    0.03
moist:varietyf5    0.02843    0.83777    0.03
moist:varietyf6    0.07910    0.83748    0.09
moist:varietyf7    0.11647    0.83757    0.14
moist:varietyf8    0.07915    0.83757    0.09
moist:varietyf9    0.08027    0.83757    0.10
moist:varietyf10   0.00276    0.83797    0.00




Here' s my full R


## Paul Johnson <pauljohn at ku.edu>
## 20160224
library(foreign)
dat <- read.dta("http://www.stata-press.com/data/mlmus3/wheat.dta")
## write.dta(dat, file = "whead.dta12")
## dat <- read.dta("wheat.dta12")
summary(dat)
## Variety should be an integer, so should plot, but we don't use it
dat$varietyf <- as.factor(dat$variety)

library(lme4)

## This is obviously overidentified/incoherent.
## lmer should bounce the user out.
m.wrong <- lmer(yield ~ moist*varietyf + (moist|varietyf), data = dat)
summary(m.wrong)

## Continue with the homework
m1 <-  lmer(yield ~ moist + (1|varietyf), data = dat)

m2 <- lmer(yield ~ moist + (moist|varietyf), data = dat)
library(lattice)
dotplot(ranef(m2, condVar = TRUE))

## Here's a shocker
anova(m2, m1)
## Really? Look at the pictures!

## Here's a spaghetti plot.
plot(m2)
m2coef <- coef(m2)[["varietyf"]]
m2coef$variety <- rownames(m2coef)
plot(yield ~ moist, data = dat, col = dat$variety)
apply(m2coef, 1, function(x){ browser();  abline(a = x[1], b = x[2],
col = x["variety"])})

## If Var(e) is small enough, even trivial slope differences are
## "statistically significant".






-- 
Paul E. Johnson
Professor, Political Science        Director
1541 Lilac Lane, Room 504      Center for Research Methods
University of Kansas                 University of Kansas
http://pj.freefaculty.org              http://crmda.ku.edu


From bates at stat.wisc.edu  Wed Feb 24 18:48:32 2016
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 24 Feb 2016 17:48:32 +0000
Subject: [R-sig-ME] Troublesome example of lmer fitting an
 overidentified model; Stata bad too.
In-Reply-To: <CAErODj8qFHD+BHFTSJjNRg5fByPgENauWRxLj5+DZ0ni-30Vqg@mail.gmail.com>
References: <CAErODj8qFHD+BHFTSJjNRg5fByPgENauWRxLj5+DZ0ni-30Vqg@mail.gmail.com>
Message-ID: <CAO7JsnTP7JXBQ_+jZFr8W7nQK7T3hM3mhtdxdqdFRW54FZaagw@mail.gmail.com>

I agree that the model is overspecified.  What is happening in the two fits
is that the likelihood surface is flat, up to round-off, in certain
directions so the optimizer converges close to the starting estimates.
Stata is using an EM algorithm and after one step the fixed-effects terms
involving variety will have taken out the variation due to variety.  That's
why the estimates of the variances of the random effects from Stata are
close to zero.

In lmer the starting estimates for the covariance parameters correspond to
a relative covariance factor of the identity.   This, in turn, corresponds
to a covariance matrix for the random effects which is sigma * I.  Notice
that estimates of the residual standard error and both standard errors of
the random effects have converged to 0.592

Detecting this situation is not trivial.  You can do it symbolically but
that involves a lot of symbolic analysis to catch all the possibilities.
You can try to do it numerically by comparing columns in the fixed-effects
model matrix corresponding to model terms and those in the random-effects
model matrix but any such numeric procedure must involve a tolerance and it
is not clear how to set that.  Anyone who has taken a linear algebra course
knows that the rank of a matrix is well defined.  In practice, using
floating-point arithmetic, reliably determining the rank of a matrix is a
notoriously difficult problem - probably an unsolveable problem.

Writing code to detect ill-posed models or failure to converge or other
problematic conditions is a game of whack-a-mole.  You have to guess in
what way the model will be ill-posed then write code to detect this, etc.
This leads to code bloat.  Worse it slows things down, takes up memory,
etc. for all model fits and you need to contend with false positives, which
has been an ongoing issue with the convergence checks in lme4.

Eventually it comes down to the extent to which the developers of the code
feel the need to protect users from themselves. Ben is much more inclined
to do this that I am.  I take the approach of telling the user "don't do
that".  Of course, that doesn't help the next user who tries to do the same
thing.

On Wed, Feb 24, 2016 at 11:18 AM Paul Johnson <pauljohn32 at gmail.com> wrote:

> I'm teaching mixed models, working through a lot of homework problems
> in the 2 volume set Multilevel and Longitudinal Modeling using Stata,
> by S Rave-Hesketh and A. Skrondal. If you did not look at it yet, I
> expect you'll find lots of useful, interesting exercises.
>
> Their Exercise 4.4 is about yield from 10 wheat varieties.  The variables
> are:
>
> "plot"     "variety"  "yield"    "moist"
>
> variety is the grouping variable in the exercise, but it comes in as a
> floating point number. I thought that was the root of the surprise
> described here, but now I don't think so.
>
> I was reading one student's homework in Stata and I was
> stunned/surprised that Stata's "mixed" function would fit a random
> effects model that included the grouping variable as a fixed effect
> predictor as well.  This appeared to be grossly overidentified to me.
>
> Here's the stata input and output,
>
> . mixed yield c.moist##i.variety || variety:moist, mle stddev
>
> Performing EM optimization:
>
> Performing gradient-based optimization:
>
> Iteration 0:   log likelihood = -42.123671
> Iteration 1:   log likelihood = -41.540417
> Iteration 2:   log likelihood = -41.520012
> Iteration 3:   log likelihood = -41.520012
>
> Computing standard errors:
>
> Mixed-effects ML regression                     Number of obs     =
>  60
> Group variable: variety                         Number of groups  =
>  10
>
>                                                 Obs per group:
>                                                               min =
>   6
>                                                               avg =
> 6.0
>                                                               max =
>   6
>
>                                                 Wald chi2(19)     =
>  37795.42
> Log likelihood = -41.520012                     Prob > chi2       =
>  0.0000
>
>
> ---------------------------------------------------------------------------------
>           yield |      Coef.   Std. Err.      z    P>|z|     [95%
> Conf. Interval]
>
> ----------------+----------------------------------------------------------------
>           moist |   .6077128   .0124644    48.76   0.000      .583283
>   .6321425
>                 |
>         variety |
>              2  |  -2.844581   .8429087    -3.37   0.001    -4.496652
>  -1.192511
>              3  |  -1.871277   .7903544    -2.37   0.018    -3.420343
>  -.3222105
>              4  |  -.3432833   .7323671    -0.47   0.639    -1.778696
>    1.09213
>              5  |   .2294764   1.121912     0.20   0.838     -1.96943
>   2.428383
>              6  |    3.46207    .690504     5.01   0.000     2.108707
>   4.815433
>              7  |  -12.05622   .6729943   -17.91   0.000    -13.37527
>  -10.73718
>              8  |   1.175528   .7302294     1.61   0.107    -.2556957
>   2.606751
>              9  |  -1.434985   .7917805    -1.81   0.070    -2.986846
>   .1168767
>             10  |   3.494313   1.392236     2.51   0.012     .7655807
>   6.223044
>                 |
> variety#c.moist |
>              2  |  -.0354326   .0260328    -1.36   0.173    -.0864559
>   .0155908
>              3  |   .1297872   .0190785     6.80   0.000     .0923941
>   .1671804
>              4  |   .0264085   .0210603     1.25   0.210    -.0148689
>    .067686
>              5  |   .0284318   .0240342     1.18   0.237    -.0186744
>    .075538
>              6  |   .0790988   .0161154     4.91   0.000     .0475132
>   .1106844
>              7  |   .1164752   .0189384     6.15   0.000     .0793566
>   .1535938
>              8  |   .0791545   .0188487     4.20   0.000     .0422118
>   .1160972
>              9  |    .080266   .0190236     4.22   0.000     .0429804
>   .1175516
>             10  |   .0027604   .0282729     0.10   0.922    -.0526535
>   .0581742
>                 |
>           _cons |   34.58378   .5478187    63.13   0.000     33.51007
>   35.65748
>
> ---------------------------------------------------------------------------------
>
>
> ------------------------------------------------------------------------------
>   Random-effects Parameters  |   Estimate   Std. Err.     [95% Conf.
> Interval]
>
> -----------------------------+------------------------------------------------
> variety: Independent         |
>                    sd(moist) |   3.46e-15   2.27e-14      8.96e-21
> 1.34e-09
>                    sd(_cons) |   1.18e-11   1.34e-07             0
>    .
>
> -----------------------------+------------------------------------------------
>                 sd(Residual) |   .4833867   .0441285      .4041925
> .5780976
>
> ------------------------------------------------------------------------------
> LR test vs. linear model: chi2(2) = 0.00                  Prob > chi2 =
> 1.0000
>
> Note: LR test is conservative and provided only for reference.
>
>
> As you can see, Stata deals with the overidentification by reporting
> miniscule estimates for the random effects.
>
> I think its a bug, wondered what lme4 would do. I predicted it would
> refuse to fit at all, as soon as we declare variety as a factor
> (varietyf).  However, that does not happen. The full R runable example
> is below, but here's the bad part:
>
> > m.wrong <- lmer(yield ~ moist*varietyf + (moist|varietyf), data = dat)
> > summary(m.wrong)
> Linear mixed model fit by REML ['lmerMod']
> Formula: yield ~ moist * varietyf + (moist | varietyf)
>    Data: dat
>
> REML criterion at convergence: 157.7
>
> Scaled residuals:
>      Min       1Q   Median       3Q      Max
> -1.74078 -0.52638 -0.00039  0.56385  1.79231
>
> Random effects:
>  Groups   Name        Variance Std.Dev. Corr
>  varietyf (Intercept) 0.3505   0.592
>           moist       0.3505   0.592    0.00
>  Residual             0.3505   0.592
> Number of obs: 60, groups:  varietyf, 10
>
> Fixed effects:
>                   Estimate Std. Error t value
> (Intercept)       34.58378    0.89479   38.65
> moist              0.60771    0.59222    1.03
> varietyf2         -2.84458    1.32918   -2.14
> varietyf3         -1.87128    1.27984   -1.46
> varietyf4         -0.34328    1.22700   -0.28
> varietyf5          0.22948    1.60904    0.14
> varietyf6          3.46207    1.19003    2.91
> varietyf7        -12.05622    1.17489  -10.26
> varietyf8          1.17553    1.22509    0.96
> varietyf9         -1.43498    1.28116   -1.12
> varietyf10         3.49431    1.89960    1.84
> moist:varietyf2   -0.03543    0.83786   -0.04
> moist:varietyf3    0.12979    0.83758    0.15
> moist:varietyf4    0.02641    0.83765    0.03
> moist:varietyf5    0.02843    0.83777    0.03
> moist:varietyf6    0.07910    0.83748    0.09
> moist:varietyf7    0.11647    0.83757    0.14
> moist:varietyf8    0.07915    0.83757    0.09
> moist:varietyf9    0.08027    0.83757    0.10
> moist:varietyf10   0.00276    0.83797    0.00
>
>
>
>
> Here' s my full R
>
>
> ## Paul Johnson <pauljohn at ku.edu>
> ## 20160224
> library(foreign)
> dat <- read.dta("http://www.stata-press.com/data/mlmus3/wheat.dta")
> ## write.dta(dat, file = "whead.dta12")
> ## dat <- read.dta("wheat.dta12")
> summary(dat)
> ## Variety should be an integer, so should plot, but we don't use it
> dat$varietyf <- as.factor(dat$variety)
>
> library(lme4)
>
> ## This is obviously overidentified/incoherent.
> ## lmer should bounce the user out.
> m.wrong <- lmer(yield ~ moist*varietyf + (moist|varietyf), data = dat)
> summary(m.wrong)
>
> ## Continue with the homework
> m1 <-  lmer(yield ~ moist + (1|varietyf), data = dat)
>
> m2 <- lmer(yield ~ moist + (moist|varietyf), data = dat)
> library(lattice)
> dotplot(ranef(m2, condVar = TRUE))
>
> ## Here's a shocker
> anova(m2, m1)
> ## Really? Look at the pictures!
>
> ## Here's a spaghetti plot.
> plot(m2)
> m2coef <- coef(m2)[["varietyf"]]
> m2coef$variety <- rownames(m2coef)
> plot(yield ~ moist, data = dat, col = dat$variety)
> apply(m2coef, 1, function(x){ browser();  abline(a = x[1], b = x[2],
> col = x["variety"])})
>
> ## If Var(e) is small enough, even trivial slope differences are
> ## "statistically significant".
>
>
>
>
>
>
> --
> Paul E. Johnson
> Professor, Political Science        Director
> 1541 Lilac Lane, Room 504      Center for Research Methods
> University of Kansas                 University of Kansas
> http://pj.freefaculty.org              http://crmda.ku.edu
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From M.Fairbrother at bristol.ac.uk  Wed Feb 24 22:04:48 2016
From: M.Fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Wed, 24 Feb 2016 21:04:48 +0000
Subject: [R-sig-ME] For what can I use a correlation of fixed effects
	from (g)lmer?
Message-ID: <CAAH-yP_MKy-aSqBt-jUVhty=PH=dwEnpxq8ZMNd5PCpBGHYOUA@mail.gmail.com>

Dear Steve,
There's a lot in your question. A couple thoughts:
(1) I'm not clear whether you *country-mean-centered* your individual-level
covariates. If not, the country means of those variables could (i.e.,
almost certainly will) correlate to some degree with your country-level
variables. This will almost certainly just confuse matters, such that it
would be best to do the mean-centering. (If you want to include, say,
national mean education as a covariate, in addition to de-meaned individual
level education, you can do so... But that will probably correlate a lot
with, say, GDP/capita.) From what you say, mean-centering will get you what
you want, and it actually might also help you deal with the unhelpful
reviewer comments you're getting. (I totally agree with your reactions to
those. Given what appears to the paucity of logic behind their comments,
surreptitiously not doing what they're saying but appearing to do what
they're saying seems a reasonable strategy. Implicitly including country
means increasing your degrees of freedom at the country level, causing a
reduction in efficiency, as you suggest... Though it's an issue of
collinearity, not just missingness.)
So I think you're wrong that "individual-level variables don't meaningfully
influence the parameter estimates for country-level variables beyond
inefficiency introduced by missing data." But I think you can nonetheless
ignore them--because only the country mean components are having the
impacts you describe, and you seem to have substantive reasons to remove
those components.
(2) Like you, I've never found the "correlation of fixed effects" output
very useful. I generally just suppress/ignore it.
Hope that helps.
- Malcolm


Dr Malcolm Fairbrother
Senior Lecturer in Global Policy and Politics
School of Geographical Sciences
University of Bristol




Date: Mon, 22 Feb 2016 15:46:17 -0500
> From: svm <steven.v.miller at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] For what can I use a correlation of fixed effects
>         from    (g)lmer?
>
> Hi all,
>
> I have a question that concerns how could I possibly use a correlation of
> fixed effects that comes standard with every (g)lmer call. I'll explain the
> situation I'm encountering briefly.
>
>
>    - I used mixed effects models mostly for cross-national survey research.
>    I have both individual-level fixed effects and country-level fixed
> effects.
>    - My interest is mostly the country-level fixed effects. The
>    individual-level stuff tends to be standard "controls" that reviewers
> want
>    to see.
>    - I'm not convinced the individual-level fixed effects are entirely
>    necessary. My hunch is they just make for inefficient estimates of the
>    country-level fixed effects that interest me. The individual-level
>    variables just create missing data problems. However, they're stuff that
>    reviewers insist on seeing absent any other information about what a
> mixed
>    effects model is doing.
>
>
> I have a project (manuscript here:
> https://www.dropbox.com/s/harb6ylpcxdpalr/etst.pdf?dl=0 | appendix here:
> https://www.dropbox.com/s/pq8gmr7v1xvvu2h/etst-appendix.pdf?dl=0) that
> reviewers rejected because the country-level fixed effects were rendered
> statistically insignificant (i.e. not discernible from zero) upon the
> inclusion of the individual-level variables. They said that one
> individual-level attribute (which by itself contributes to listwise
> deletion of 30% of the data) somehow made the country-level fixed effects
> "spurious" to its inclusion. This already strikes me as a bold claim for
> theoretical and statistical reasons, but here's what I did to circumvent
> this claim:
>
>
>    - Estimate just the country-level fixed effects.
>    - Use multiple imputation to generate five full data sets and merge in
>    the macro-level information after the imputation. The results for the
>    country-level fixed effects were almost identical to the analyses with
> just
>    the country-level fixed effects.
>    - Omit the offending individual-level variables that contribute the most
>    missingness. These results were consistent with the results from the
> other
>    two estimation strategies.
>
>
> However, the reviewers just didn't buy it and torpedoed the manuscript.
>
> Is this something that the correlation of fixed effects could be useful in
> addressing? Here's the correlation of fixed effects (without the
> intercepts) for the analysis in question. In this analysis, the three
> variables at the bottom row (i.e. the two threat indices and the level of
> democracy) are the country-level variables for this cross-national survey
> analysis. The other variables are individual-level attributes. It's worth
> reiterating that every variable that is not binary is scaled by two
> standard deviations to create a meaningful zero.
>
> http://i.imgur.com/eIiZH9b.png
>
> Notice that the bottom-left quadrant is entirely white (i.e. the
> correlation of the individual-level fixed effects with the country-level
> fixed effects is basically zero). Is this telling me that the correlation
> for any one individual-level fixed effect and a country-level fixed effect
> is almost zero (i.e. they have almost no bearing on each other)? The most
> I've seen anyone discuss this correlation matrix is here:
>
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q1/001941.html
>
> It is an approximate correlation of the estimator of the fixed
> effects.  (I include the word "approximate" because I should but in
> this case the approximation is very good.)  I'm not sure how to
> explain it better than that.  Suppose that you took an MCMC sample
> from the parameters in the model, then you would expect the sample of
> the fixed-effects parameters to display a correlation structure like
> this matrix.
>
>
> and here (
>
> http://stats.stackexchange.com/questions/57240/how-do-i-interpret-the-correlations-of-fixed-effects-in-my-glmer-output
> ):
>
>
> The "correlation of fixed effects" output doesn't have the intuitive
> meaning that most would ascribe to it. Specifically, is not about the
> correlation of the variables (as OP notes). It is in fact about the
> expected correlation of the regression coefficients. Although this may
> speak to multicollinearity it does not necessarily.
>
>
> I should add that I've estimated hundreds of mixed effects models with
> individual-level and country-level variables and they all have fixed
> effects correlation matrices that resemble these. I have a strong hunch
> that individual-level variables don't meaningfully influence the parameter
> estimates for country-level variables beyond inefficiency introduced by
> missing data. In research projects where individual-level attributes don't
> concern the project, I'd like to ignore them for that reason. They just
> create estimation problems and slow down computation.
>
> I might be mistaken, which is why I ask here. I thank you for your time.
>
> - Steve
>

	[[alternative HTML version deleted]]


From steven.v.miller at gmail.com  Wed Feb 24 23:08:53 2016
From: steven.v.miller at gmail.com (svm)
Date: Wed, 24 Feb 2016 17:08:53 -0500
Subject: [R-sig-ME] For what can I use a correlation of fixed effects
	from (g)lmer?
In-Reply-To: <CAAH-yP_MKy-aSqBt-jUVhty=PH=dwEnpxq8ZMNd5PCpBGHYOUA@mail.gmail.com>
References: <CAAH-yP_MKy-aSqBt-jUVhty=PH=dwEnpxq8ZMNd5PCpBGHYOUA@mail.gmail.com>
Message-ID: <CABafbirYk3Y2kyKWhHYYdK=n=pBA5cahzDcQ2cwpf-ab=+_q2w@mail.gmail.com>

Hi Malcolm,

Thanks for the response. I actually cite your 2014 PSRM piece in defense of
that argument. I know we're not using the same language in a similar
approach, but I remember you arguing for the exclusion of individual-level
covariates because it would just contribute to missingness and not help
with your overall research question. We're both using WVS data too.
Sometimes, missingness is not random (e.g. WVS not asking about respondent
ideology in several important countries [like China] in one of my projects).

I did want to clarify that my mean-centering approach is inspired by Gelman
(2008), who argues to scale by two standard deviations. When I have two or
three waves, the inclusion of one or more predictors may drop out an entire
wave (e.g. EVS not asking about respondent's education levels until the
third wave). So, I try to scale on the country-wave (for individual-level
variables like age) or the survey wave (at the macro-level attributes like
a country's level of democracy). For example, here's what I do with a
respondent's age in EVS:

EVS <- ddply(EVS, c("ccode","wave"), transform, zg.age = arm::rescale(x003))

and here's what I do with level of democracy (UDS data).

Macro.EVS <- ddply(Macro.EVS, c("wave"), transform, zg.udsmean =
arm::rescale(udsmean))

In the example I cite above, only the bottom three rows in the correlation
matrix are country-level (really: country-year-level) covariates.
Everything else is an individual-level variable.

And yeah, I've never used a correlation of fixed effects before for
anything concerning the model I estimate. I'm curious if I actually could
use that as justification to stop flooding models with individual-level
variables that (I think) don't meaningfully influence the country-level
variables of interest to me (beyond introducing non-random missingness).

- Steve


On Wed, Feb 24, 2016 at 4:04 PM, Malcolm Fairbrother <
M.Fairbrother at bristol.ac.uk> wrote:

> Dear Steve,
> There's a lot in your question. A couple thoughts:
> (1) I'm not clear whether you *country-mean-centered* your
> individual-level covariates. If not, the country means of those variables
> could (i.e., almost certainly will) correlate to some degree with your
> country-level variables. This will almost certainly just confuse matters,
> such that it would be best to do the mean-centering. (If you want to
> include, say, national mean education as a covariate, in addition to
> de-meaned individual level education, you can do so... But that will
> probably correlate a lot with, say, GDP/capita.) From what you say,
> mean-centering will get you what you want, and it actually might also help
> you deal with the unhelpful reviewer comments you're getting. (I totally
> agree with your reactions to those. Given what appears to the paucity of
> logic behind their comments, surreptitiously not doing what they're saying
> but appearing to do what they're saying seems a reasonable strategy.
> Implicitly including country means increasing your degrees of freedom at
> the country level, causing a reduction in efficiency, as you suggest...
> Though it's an issue of collinearity, not just missingness.)
> So I think you're wrong that "individual-level variables don't
> meaningfully influence the parameter estimates for country-level variables
> beyond inefficiency introduced by missing data." But I think you can
> nonetheless ignore them--because only the country mean components are
> having the impacts you describe, and you seem to have substantive reasons
> to remove those components.
> (2) Like you, I've never found the "correlation of fixed effects" output
> very useful. I generally just suppress/ignore it.
> Hope that helps.
> - Malcolm
>
>
> Dr Malcolm Fairbrother
> Senior Lecturer in Global Policy and Politics
> School of Geographical Sciences
> University of Bristol
>
>
>
>
> Date: Mon, 22 Feb 2016 15:46:17 -0500
>> From: svm <steven.v.miller at gmail.com>
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] For what can I use a correlation of fixed effects
>>         from    (g)lmer?
>>
>> Hi all,
>>
>> I have a question that concerns how could I possibly use a correlation of
>> fixed effects that comes standard with every (g)lmer call. I'll explain
>> the
>> situation I'm encountering briefly.
>>
>>
>>    - I used mixed effects models mostly for cross-national survey
>> research.
>>    I have both individual-level fixed effects and country-level fixed
>> effects.
>>    - My interest is mostly the country-level fixed effects. The
>>    individual-level stuff tends to be standard "controls" that reviewers
>> want
>>    to see.
>>    - I'm not convinced the individual-level fixed effects are entirely
>>    necessary. My hunch is they just make for inefficient estimates of the
>>    country-level fixed effects that interest me. The individual-level
>>    variables just create missing data problems. However, they're stuff
>> that
>>    reviewers insist on seeing absent any other information about what a
>> mixed
>>    effects model is doing.
>>
>>
>> I have a project (manuscript here:
>> https://www.dropbox.com/s/harb6ylpcxdpalr/etst.pdf?dl=0 | appendix here:
>> https://www.dropbox.com/s/pq8gmr7v1xvvu2h/etst-appendix.pdf?dl=0) that
>> reviewers rejected because the country-level fixed effects were rendered
>> statistically insignificant (i.e. not discernible from zero) upon the
>> inclusion of the individual-level variables. They said that one
>> individual-level attribute (which by itself contributes to listwise
>> deletion of 30% of the data) somehow made the country-level fixed effects
>> "spurious" to its inclusion. This already strikes me as a bold claim for
>> theoretical and statistical reasons, but here's what I did to circumvent
>> this claim:
>>
>>
>>    - Estimate just the country-level fixed effects.
>>    - Use multiple imputation to generate five full data sets and merge in
>>    the macro-level information after the imputation. The results for the
>>    country-level fixed effects were almost identical to the analyses with
>> just
>>    the country-level fixed effects.
>>    - Omit the offending individual-level variables that contribute the
>> most
>>
>>    missingness. These results were consistent with the results from the
>> other
>>    two estimation strategies.
>>
>>
>> However, the reviewers just didn't buy it and torpedoed the manuscript.
>>
>> Is this something that the correlation of fixed effects could be useful in
>> addressing? Here's the correlation of fixed effects (without the
>> intercepts) for the analysis in question. In this analysis, the three
>> variables at the bottom row (i.e. the two threat indices and the level of
>> democracy) are the country-level variables for this cross-national survey
>> analysis. The other variables are individual-level attributes. It's worth
>> reiterating that every variable that is not binary is scaled by two
>> standard deviations to create a meaningful zero.
>>
>> http://i.imgur.com/eIiZH9b.png
>>
>> Notice that the bottom-left quadrant is entirely white (i.e. the
>> correlation of the individual-level fixed effects with the country-level
>> fixed effects is basically zero). Is this telling me that the correlation
>> for any one individual-level fixed effect and a country-level fixed effect
>> is almost zero (i.e. they have almost no bearing on each other)? The most
>> I've seen anyone discuss this correlation matrix is here:
>>
>> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q1/001941.html
>>
>> It is an approximate correlation of the estimator of the fixed
>> effects.  (I include the word "approximate" because I should but in
>> this case the approximation is very good.)  I'm not sure how to
>> explain it better than that.  Suppose that you took an MCMC sample
>> from the parameters in the model, then you would expect the sample of
>> the fixed-effects parameters to display a correlation structure like
>> this matrix.
>>
>>
>> and here (
>>
>> http://stats.stackexchange.com/questions/57240/how-do-i-interpret-the-correlations-of-fixed-effects-in-my-glmer-output
>> ):
>>
>>
>> The "correlation of fixed effects" output doesn't have the intuitive
>> meaning that most would ascribe to it. Specifically, is not about the
>> correlation of the variables (as OP notes). It is in fact about the
>> expected correlation of the regression coefficients. Although this may
>> speak to multicollinearity it does not necessarily.
>>
>>
>> I should add that I've estimated hundreds of mixed effects models with
>> individual-level and country-level variables and they all have fixed
>> effects correlation matrices that resemble these. I have a strong hunch
>> that individual-level variables don't meaningfully influence the parameter
>> estimates for country-level variables beyond inefficiency introduced by
>> missing data. In research projects where individual-level attributes don't
>> concern the project, I'd like to ignore them for that reason. They just
>> create estimation problems and slow down computation.
>>
>> I might be mistaken, which is why I ask here. I thank you for your time.
>>
>>
>>

	[[alternative HTML version deleted]]


From gitumbui at gmail.com  Thu Feb 25 14:09:36 2016
From: gitumbui at gmail.com (Gitu wa Mbui)
Date: Fri, 26 Feb 2016 00:09:36 +1100
Subject: [R-sig-ME] Generating partial plots from glmmADMB models with
	splines
Message-ID: <CAFPRYfCkuTqi5A4Gao+GmoGAnfO5kiKe-Njn9jziAeWNyuuGpQ@mail.gmail.com>

Hi list,

I am wondering whether it is possible to create partial plots of the
smoothed terms from glmmADMB models of the following structure (**note the
splines).

model <- glmmadmb(y~ns(x0,5)+ns(x1,5)+ns(x2,5)+ns(x3,5)+(1|g),
       family="beta",link="logit",data=dat)


Thanks,

Gitu

	[[alternative HTML version deleted]]


From francescobryanromano at gmail.com  Fri Feb 26 12:28:37 2016
From: francescobryanromano at gmail.com (Francesco Romano)
Date: Fri, 26 Feb 2016 12:28:37 +0100
Subject: [R-sig-ME] Interpreting three-way interaction with categorical
 predictors and binary outcome (GLMER)
Message-ID: <CABZN5+G7PSiFUruyU97MygXYgEYKF6RB8d_w1Zo1eB299Qom-A@mail.gmail.com>

One more question regarding the interpretation of interactions.
I am having difficulty finding information on how to obtain a specific
contrast in a 3-way interaction of three categorical variables, syntax
(levels s and of), animacy (levels +AN -AN and -AN +AN), and group (int,
adv, ns), predicting a binary outcome (correct or inversion).

The anova yielded a significant effect for animacy and an interaction
between syntax:group. Everything else was non-significant. As I am
interested in comparing two cells at a time from the three-way table, I
kept all 3 predictors and their interactions within the model. The best fit
was given by inclusion of random intercepts for participants and items only.

Here is the output of the final model, dummy coded.

> summary(recallmodel4bisB3)
Cov prior  : Part.name ~ wishart(df = 3.5, scale = Inf, posterior.scale =
cov, common.scale = TRUE)
           : Item ~ wishart(df = 3.5, scale = Inf, posterior.scale = cov,
common.scale = TRUE)
Prior dev  : 1.3565

Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['bglmerMod']
 Family: binomial  ( logit )
Formula: Correct ~ Syntax * Animacy * Prof.group.2 + (1 | Part.name) +
 (1 | Item)
   Data: recall
Control: glmerControl(optimizer = "bobyqa")

     AIC      BIC   logLik deviance df.resid
   313.3    372.9   -142.6    285.3      509

Scaled residuals:
    Min      1Q  Median      3Q     Max
-1.3517 -0.2926 -0.1802 -0.1137  9.3666

Random effects:
 Groups    Name        Variance Std.Dev.
 Part.name (Intercept) 0.8046   0.8970
 Item      (Intercept) 0.5031   0.7093
Number of obs: 523, groups:  Part.name, 42; Item, 16

Fixed effects:
                                       Estimate Std. Error z value Pr(>|z|)

(Intercept)                             -0.8960     0.6317  -1.418 0.156071

Syntaxs                                 -2.0713     0.9447  -2.193 0.028342
*
Animacy+AN -AN                          -3.0539     1.2548  -2.434 0.014941
*
Prof.group.2int                         -2.5594     0.9473  -2.702 0.006898
**
Prof.group.2ns                          -1.8673     0.7634  -2.446 0.014442
*
Syntaxs:Animacy+AN -AN                   1.8642     1.8202   1.024 0.305750

Syntaxs:Prof.group.2int                  4.1704     1.1676   3.572 0.000355
***
Syntaxs:Prof.group.2ns                   2.4244     1.0483   2.313 0.020736
*
Animacy+AN -AN:Prof.group.2int           3.0067     1.5528   1.936 0.052824
.
Animacy+AN -AN:Prof.group.2ns            1.3245     1.6071   0.824 0.409848

Syntaxs:Animacy+AN -AN:Prof.group.2int  -2.2056     2.0550  -1.073 0.283162

Syntaxs:Animacy+AN -AN:Prof.group.2ns   -2.3249     2.3108  -1.006 0.314360

-- 

Now, I am interested in two comparisons:

1. pairwise comparisons between (+AN -AN 's and -AN +AN) as well as (+AN
-AN of and -AN +AN s)  for each of the three groups, int, adv, and ns.

2. difference between two groups at a time for the same contrasts.

In the first case, I don't need to compare across groups but within them
and I don't want to keep animacy or syntax constant in these comparisons.

In the second, I am interested in significant differences of the
differences in 1.

The best information I could find on the matter is here but is not specific
to logistic regression. Therefore, I am not 100% sure it applies.

http://talklab.psy.gla.ac.uk/tvw/catpred/

According to the source, in the case of the model above, a dummy coded
syntax:animacy:group interaction will yield a coefficient representing the
following differences:

(s +AN -AN adv)-(of +AN -AN adv) - (s -AN +AN adv) - (of -AN +AN adv)

Any suggestions on how to obtain the necessary contrasts would be greatly
appreciated. I am currently stuck.



Frank Romano Ph.D.



*LinkedIn*
https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162

*Academia.edu*
https://sheffield.academia.edu/FrancescoRomano

	[[alternative HTML version deleted]]


From emmanuel.curis at parisdescartes.fr  Fri Feb 26 13:47:55 2016
From: emmanuel.curis at parisdescartes.fr (Emmanuel Curis)
Date: Fri, 26 Feb 2016 13:47:55 +0100
Subject: [R-sig-ME] Interpreting three-way interaction with categorical
 predictors and binary outcome (GLMER)
In-Reply-To: <CABZN5+G7PSiFUruyU97MygXYgEYKF6RB8d_w1Zo1eB299Qom-A@mail.gmail.com>
References: <CABZN5+G7PSiFUruyU97MygXYgEYKF6RB8d_w1Zo1eB299Qom-A@mail.gmail.com>
Message-ID: <20160226124755.GA1095@info124.pharmacie.univ-paris5.fr>

If you're patient, you may use the tedious but sure way of
 1) check how the model is defined exactly
 2) write each cell mean using this model
 3) write down the exact comparison you want in term of cell means
 4) using 2), rewrite this comparison in term of the model
    coefficients
  ==> this will give you the linear combination to test (using
 multcomp:glht for instance).

This apply for any model that has a linear part: lm, glm [replace ? by
ln odds in that case], lmer...

Exemple, for a 2-way 2?2 design with interaction, with cell means

   A    B
1  ?1A  ?1B
                  and comparison to do let's say (?2A - ?1B) =?  (?1A-?1B)
2  ?2A  ?2B

Using the default contrats with a reference cell (A,1), the
coefficients in the model are ?0, dB, d2, d2:B with
  ?1A = ?0
  ?1B = ?0 + dB
  ?2A = ?0 + d2
  ?2B = ?0 + d2 + dB + d2:B

Then, you're comparison is
 (?2A - ?1B) - (?1A-?2B) =
 ((?0 + d2) - (?0 + dB)) -( (?0) - (?0 + d2 + dB + d2:B) ) =
 d2 - dB - d2 + dB - d2:B =
 d2:B

and your ? contrast ? to use to write your test is
 ?0  dA  d2  d2:B
 0    0   0     1

This is quite tedious with a three-way interaction and several levels,
and you need to known how were defined each coefficient in the model,
but at least you're certain of what you're testing in the end...

On Fri, Feb 26, 2016 at 12:28:37PM +0100, Francesco Romano wrote:
? One more question regarding the interpretation of interactions.
? I am having difficulty finding information on how to obtain a specific
? contrast in a 3-way interaction of three categorical variables, syntax
? (levels s and of), animacy (levels +AN -AN and -AN +AN), and group (int,
? adv, ns), predicting a binary outcome (correct or inversion).
? 
? The anova yielded a significant effect for animacy and an interaction
? between syntax:group. Everything else was non-significant. As I am
? interested in comparing two cells at a time from the three-way table, I
? kept all 3 predictors and their interactions within the model. The best fit
? was given by inclusion of random intercepts for participants and items only.

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From francescobryanromano at gmail.com  Fri Feb 26 17:37:57 2016
From: francescobryanromano at gmail.com (Francesco Romano)
Date: Fri, 26 Feb 2016 17:37:57 +0100
Subject: [R-sig-ME] Interpreting three-way interaction with categorical
 predictors and binary outcome (GLMER)
In-Reply-To: <20160226124755.GA1095@info124.pharmacie.univ-paris5.fr>
References: <CABZN5+G7PSiFUruyU97MygXYgEYKF6RB8d_w1Zo1eB299Qom-A@mail.gmail.com>
	<20160226124755.GA1095@info124.pharmacie.univ-paris5.fr>
Message-ID: <CABZN5+FhdrqYf_dtwuzMD2aHfJCZa0Nr7MQDpTpXPR5GkPt8vg@mail.gmail.com>

Dear Emmanuel, thanks for sharing your expertise with me. Unfortunately I
can't seem to understand several details in your explanation but I do
appreciate this work can be done manually. At this time I realize the
question is ill-placed within this R-sig so I am closing the ticket and
asking the question elsewhere. Apologies for cross-posting.

On Fri, Feb 26, 2016 at 1:47 PM, Emmanuel Curis <
emmanuel.curis at parisdescartes.fr> wrote:

> If you're patient, you may use the tedious but sure way of
>  1) check how the model is defined exactly
>  2) write each cell mean using this model
>  3) write down the exact comparison you want in term of cell means
>  4) using 2), rewrite this comparison in term of the model
>     coefficients
>   ==> this will give you the linear combination to test (using
>  multcomp:glht for instance).
>
> This apply for any model that has a linear part: lm, glm [replace ? by
> ln odds in that case], lmer...
>
> Exemple, for a 2-way 2?2 design with interaction, with cell means
>
>    A    B
> 1  ?1A  ?1B
>                   and comparison to do let's say (?2A - ?1B) =?  (?1A-?1B)
> 2  ?2A  ?2B
>
> Using the default contrats with a reference cell (A,1), the
> coefficients in the model are ?0, dB, d2, d2:B with
>   ?1A = ?0
>   ?1B = ?0 + dB
>   ?2A = ?0 + d2
>   ?2B = ?0 + d2 + dB + d2:B
>
> Then, you're comparison is
>  (?2A - ?1B) - (?1A-?2B) =
>  ((?0 + d2) - (?0 + dB)) -( (?0) - (?0 + d2 + dB + d2:B) ) =
>  d2 - dB - d2 + dB - d2:B =
>  d2:B
>
> and your ? contrast ? to use to write your test is
>  ?0  dA  d2  d2:B
>  0    0   0     1
>
> This is quite tedious with a three-way interaction and several levels,
> and you need to known how were defined each coefficient in the model,
> but at least you're certain of what you're testing in the end...
>
> On Fri, Feb 26, 2016 at 12:28:37PM +0100, Francesco Romano wrote:
> ? One more question regarding the interpretation of interactions.
> ? I am having difficulty finding information on how to obtain a specific
> ? contrast in a 3-way interaction of three categorical variables, syntax
> ? (levels s and of), animacy (levels +AN -AN and -AN +AN), and group (int,
> ? adv, ns), predicting a binary outcome (correct or inversion).
> ?
> ? The anova yielded a significant effect for animacy and an interaction
> ? between syntax:group. Everything else was non-significant. As I am
> ? interested in comparing two cells at a time from the three-way table, I
> ? kept all 3 predictors and their interactions within the model. The best
> fit
> ? was given by inclusion of random intercepts for participants and items
> only.
>
> --
>                                 Emmanuel CURIS
>                                 emmanuel.curis at parisdescartes.fr
>
> Page WWW: http://emmanuel.curis.online.fr/index.html
>



-- 
Frank Romano Ph.D.

Tel. +39 3911639149

*LinkedIn*
https://it.linkedin.com/pub/francesco-bryan-romano/33/1/162

*Academia.edu*
https://sheffield.academia.edu/FrancescoRomano

	[[alternative HTML version deleted]]


From emmanuel.curis at parisdescartes.fr  Fri Feb 26 17:52:04 2016
From: emmanuel.curis at parisdescartes.fr (Emmanuel Curis)
Date: Fri, 26 Feb 2016 17:52:04 +0100
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC810F5A897@FHSDB2D11-2.csu.mcmaster.ca>
References: <5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
	<CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
	<20160223153223.GA16587@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A17C@FHSDB2D11-2.csu.mcmaster.ca>
	<20160223164947.GA29156@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A21D@FHSDB2D11-2.csu.mcmaster.ca>
	<20160224095407.GA24770@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A897@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <20160226165204.GB8308@info124.pharmacie.univ-paris5.fr>

Dear Pr Fox,

Thanks for the time taken clarifying things. I'll take time to read
your text, and think over things, but I think that until that I'll
stay with the writing of the comparisons in terms of means and deduce
the linear hypothesis to test, to be sure of what I do.

I don't understand well, in your answer, the part saying ? it explains
why it's possible to formulate the different types of tests in linear
models independently of the contrasts (regressors) used to code the
factors -- because fundamentally what's important is the subspace
spanned by the regressors in each model, which is independent of
coding. ?.

As I understood the model, if we have a 2?2 design (A?B) for instance,
the subspace spanned by all predictors is a 4-dimensionnal space. In
this space, each dimension can be assigned to A, B, their interaction
and a constant. That means, each predictor is associated with a
different basis vector of this 4-dimensionnal space. But there is
several ways of defining the basis, defining different sub-spaces
associated with A, B and A?B, and this corresponds to the different
codings. For instance, I can say (with 4 points)

? A   B A?B       or  ?  A  B  A?B
1 -1 -1  +1           1  0  0  0
1 -1 +1  -1           0  0  1  0
1 +1 -1  -1	      0  1  0  0
1 +1 +1  +1	      0  1  1  1

and the sub-spaces associated with ?, A, B, and A?B are different in
these two codings (but in whole, the 4-dimensionnal space is the
same).  I may miss something trivial, but I would say that the coding
instead defines the subspace spanned by the regressor, and not that
they are independant.

Am I too stuck with coding? But then, how is defined the subspace
associated to a regressor ? absolutly ??


On Wed, Feb 24, 2016 at 04:08:42PM +0000, Fox, John wrote:
? Dear Emmanuel,
? 
? The questions you raise are sufficiently complicated that it's difficult to address them adequately in an email. My Applied Regression and Generalized Linear Models text, for example, takes about 15 pages to explain the relationships among regressor codings, hypotheses, and tests in 2-way ANOVA, working with the full-rank parametrization of the model, and it's possible (as Russell Lenth indicated) to work things out even more generally. 
? 
? I'll try to answer briefly, however.
? 
? 
? No need to apologize. I don't think that these are simple ideas.
? 
? > the expectation of the quadratic form, and I too quickly deduced that there
? > was an equivalent linear combination of the parameters as its ? square root
? > ?, but this was obviously wrong since the L matrix in a Lt W L quadratic form
? > does not have to be a column matrix. Am I wrong thinking that typically in
? > such tests, the L matrix is precisely a multi-column matrix (hence also several
? > degrees of freedom associated), and that several contrasts are tested
? > simultaneously?
? 
? Thinking in terms of the full-rank parametrization, as used in R, each type-III hypothesis is that several coefficients are simultaneously 0, which can be simply formulated as a linear hypothesis assuming an appropriate coding of the regressors for a factor. Type-II hypotheses can also be formulated as linear hypotheses, but doing so is more complicated. The Anova() function uses a kind of projection, in effect defining a type-II test as the most powerful test of a conditional hypothesis such as no A main effect given that the A:B interaction is absent in the model y ~ A*B. This works both for linear models, where (unless there is a complication like missing cells), the resulting test corresponds to the test produced by comparing the models y ~ A and y ~ A + B, using Y ~ A*B for the estimate of error variance (i.e., the denominator MS), and more generally for models with linear predictors, where it's in general possible to formulate the (Wald) tests in terms of the coefficient estimates and their covariance matrix.
? 
? > 
? > I precise that I call ? contrast ? a linear combination of the model parameters
? > with the constraint that the coefficients of this combination sum to 0 ? this is
? > the definition in French (? contraste ?), but I may use it wrongly in English?
? 
? I'd define a "contrast" as the weights associated with the levels of a factor for formulating a hypothesis, where the weights traditionally are constrained to sum to 0, and to differentiate this from a column of the model matrix, which I'd more generally term a "regressor." Often, a traditional set of contrasts for a factor, one less than the number of levels, are defined not only to sum to 0  but also to be orthogonal in the basis of the design. The usage in R is more general, where "contrasts" mean the set of regressors used to represent a factor. Thus, contr.sum() generates regressors that satisfy the traditional definition of contrasts, as do contr.poly() and contr.helmert(), but the default contr.treatment() generates 0/1 dummy-coded regressors that don't satisfy the traditional definition of contrasts.
? 
? > 
? > Second, I may have wrongly understood the definitions of the various tests,
? > and especially how they generalize from linear model to GLM/GLMM...
? > 
? > I thought type I was by taking the squared distance of the successive
? > orthogonal projections on the subspaces generated by the various terms, in
? > the order given in the model; type II, by ensuring that the term tested was
? > the last amongst terms of same order, after terms of lower order but before
? > terms of higher order; type III, by projecting on the subspace after removal
? > of the basis vectors for the term tested ? hence its strong dependency on
? > the coding scheme, and the ? drop1 ? trick to get them.
? > 
? > Is this definition correct? Does it generalize to other kind models, or is
? > another definition required? Is it unambiguous? The SAS doc itself suggests
? > that various procedures call "type II" different kind of things
? 
? Yes, if I've followed this correctly, it's correct, and it explains why it's possible to formulate the different types of tests in linear models independently of the contrasts (regressors) used to code the factors -- because fundamentally what's important is the subspace spanned by the regressors in each model, which is independent of coding. This approach, however, doesn't generalize easily beyond linear models fit by least squares. The approach taken in Anova() corresponds to this approach in linear models fit by least squares as long as the models remain full-rank and for type-III tests as long as the contrasts are properly formulated, and generalizes to other models with linear predictors.
? 
? > 
? > However, I cannot see clearly which hypothesis is indeed tested in each case,
? > especially in terms of cell means or marginal means (and, when I really need
? > it, I start from them and select the contrasts I need).  Is there any
? > package/software that allows to print the hypotheses testeds in terms of
? > means starting from the model formula?
? > Or is there any good reference that makes the link between the two?
? > For instance, a demonstration that the comparison of marginal means ?
? > always ? leads to a type XXX sum of square?
? 
? This is where a complete explanation gets too lengthy for an email, but a shorthand formulation, e.g., for the model y ~ A*B, is that type-I tests correspond to the hypotheses A|(B = 0, AB = 0), B | AB = 0, AB = 0; type-II tests to A | AB = 0, B | AB = 0, AB = 0; and type-III tests to A = 0, B = 0, AB = 0. Here, e.g., | AB = 0 means assuming no AB interactions, so, e.g., the hypothesis A | AB = 0 means no A main effects assuming no AB interactions. A hypothesis like A = 0 is indeed formulated in terms of marginal means, understood as cell means for A averaging over the levels of B (not level means of A ignoring B).
? 
? I realize that this is far from a complete explanation.
? 
? Best,
?  John

-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From jfox at mcmaster.ca  Sun Feb 28 16:34:06 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Sun, 28 Feb 2016 15:34:06 +0000
Subject: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
In-Reply-To: <20160226165204.GB8308@info124.pharmacie.univ-paris5.fr>
References: <5463B7A6-DF6E-4F9E-8D84-1D1B7C340890@unisa.edu.au>
	<CABZN5+GPE3r7yNJray+2pgS6v6=q+zvcT4jjoPTD7UviM2aKBQ@mail.gmail.com>
	<818c63c642e44869897199720a040514@ITUPW-EXMBOX3C.UniNet.unisa.edu.au>
	<CABZN5+HvjboCjp3UMao2q+nu0n9+-zxg3gMgwWNSrutqALN8ww@mail.gmail.com>
	<20160223153223.GA16587@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A17C@FHSDB2D11-2.csu.mcmaster.ca>
	<20160223164947.GA29156@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A21D@FHSDB2D11-2.csu.mcmaster.ca>
	<20160224095407.GA24770@info124.pharmacie.univ-paris5.fr>
	<ACD1644AA6C67E4FBD0C350625508EC810F5A897@FHSDB2D11-2.csu.mcmaster.ca>
	<20160226165204.GB8308@info124.pharmacie.univ-paris5.fr>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F5BA52@FHSDB2D11-2.csu.mcmaster.ca>

Dear Emmanuel,

Again, I'll respond briefly, and not in the detail that your questions really require:

> -----Original Message-----
> From: Emmanuel Curis [mailto:emmanuel.curis at parisdescartes.fr]
> Sent: February 26, 2016 11:52 AM
> To: Fox, John <jfox at mcmaster.ca>
> Cc: Francesco Romano <francescobryanromano at gmail.com>; r-sig-mixed-
> models at r-project.org
> Subject: Re: [R-sig-ME] Replicating type III anova tests for glmer/GLMM
> 
> Dear Pr Fox,
> 
> Thanks for the time taken clarifying things. I'll take time to read your text,
> and think over things, but I think that until that I'll stay with the writing of the
> comparisons in terms of means and deduce the linear hypothesis to test, to
> be sure of what I do.
> 
> I don't understand well, in your answer, the part saying ? it explains why it's
> possible to formulate the different types of tests in linear models
> independently of the contrasts (regressors) used to code the factors --
> because fundamentally what's important is the subspace spanned by the
> regressors in each model, which is independent of coding. ?.

The subspace spanned by the regressors in a model like y ~ A*B or y ~ A + B is independent of the coding of the regressors.

> 
> As I understood the model, if we have a 2?2 design (A?B) for instance, the
> subspace spanned by all predictors is a 4-dimensionnal space. In this space,
> each dimension can be assigned to A, B, their interaction and a constant. That
> means, each predictor is associated with a different basis vector of this 4-
> dimensionnal space. But there is several ways of defining the basis, defining
> different sub-spaces associated with A, B and A?B, and this corresponds to
> the different codings. For instance, I can say (with 4 points)
> 
> ? A   B A?B       or  ?  A  B  A?B
> 1 -1 -1  +1           1  0  0  0
> 1 -1 +1  -1           0  0  1  0
> 1 +1 -1  -1	      0  1  0  0
> 1 +1 +1  +1	      0  1  1  1
> 

In both cases, models like y ~ A*B and y ~ A + B produce the same y-hat vectors and hence same SSs. The situation is a bit more complicated for models that violate marginality, but that situation can be handled by more general approaches, like estimable functions or close attention to the hypotheses tested. All tests can be formulated linear hypothesis in the parameters of the full, full-rank model, but different parametrizations make the tests simpler or more difficult.

You've shown the row-basis for the model matrix in the cases of effect ("contr.sum") coding and dummy ("contr.treatment") coding. Call the basis matrix X_B. Then, because these are full-rank parametrizations, as long as no cells are empty, you can solve for the cell means in terms of the model parameters. Call the parameter vector corresponding to the basis beta_B and the ravelled vector of cell means mu. Then mu = X_B beta_B and (because X_B is nonsingular), beta_B = X_B^-1 mu. This allows you to see the composition of each parameter in terms of cell means and thus the hypothesis tested by the (type-III) test that the parameter is 0. In the case of effect coding, the columns of X_B are orthogonal and so its inverse is particularly simple, with each row equal to a column of X_B up to a constant factor.

> and the sub-spaces associated with ?, A, B, and A?B are different in these
> two codings (but in whole, the 4-dimensionnal space is the same).  I may miss
> something trivial, but I would say that the coding instead defines the
> subspace spanned by the regressor, and not that they are independant.
> 
> Am I too stuck with coding? But then, how is defined the subspace
> associated to a regressor ? absolutly ??

It's not, but because the model matrix spans the same subspace, it's possible to test the same hypotheses in full-rank formulations of the same model. One way to see that is to work backwards from beta_B = X_B^-1 mu (that is, define X_B^-1 as the contrasts that you want to test) to mu = X_B beta_B. As mentioned, this is particularly simple when the *rows* of X_B^-1 are orthogonal contrasts.

John

> 
> 
> On Wed, Feb 24, 2016 at 04:08:42PM +0000, Fox, John wrote:
> ? Dear Emmanuel,
> ?
> ? The questions you raise are sufficiently complicated that it's difficult to
> address them adequately in an email. My Applied Regression and
> Generalized Linear Models text, for example, takes about 15 pages to explain
> the relationships among regressor codings, hypotheses, and tests in 2-way
> ANOVA, working with the full-rank parametrization of the model, and it's
> possible (as Russell Lenth indicated) to work things out even more generally.
> ?
> ? I'll try to answer briefly, however.
> ?
> ?
> ? No need to apologize. I don't think that these are simple ideas.
> ?
> ? > the expectation of the quadratic form, and I too quickly deduced that
> there ? > was an equivalent linear combination of the parameters as its ?
> square root ? > ?, but this was obviously wrong since the L matrix in a Lt W L
> quadratic form ? > does not have to be a column matrix. Am I wrong thinking
> that typically in ? > such tests, the L matrix is precisely a multi-column matrix
> (hence also several ? > degrees of freedom associated), and that several
> contrasts are tested ? > simultaneously?
> ?
> ? Thinking in terms of the full-rank parametrization, as used in R, each type-
> III hypothesis is that several coefficients are simultaneously 0, which can be
> simply formulated as a linear hypothesis assuming an appropriate coding of
> the regressors for a factor. Type-II hypotheses can also be formulated as
> linear hypotheses, but doing so is more complicated. The Anova() function
> uses a kind of projection, in effect defining a type-II test as the most
> powerful test of a conditional hypothesis such as no A main effect given that
> the A:B interaction is absent in the model y ~ A*B. This works both for linear
> models, where (unless there is a complication like missing cells), the resulting
> test corresponds to the test produced by comparing the models y ~ A and y ~
> A + B, using Y ~ A*B for the estimate of error variance (i.e., the denominator
> MS), and more generally for models with linear predictors, where it's in
> general possible to formulate the (Wald) tests in terms of the coefficient
> estimates and their covariance matrix.
> ?
> ? >
> ? > I precise that I call ? contrast ? a linear combination of the model
> parameters ? > with the constraint that the coefficients of this combination
> sum to 0 ? this is ? > the definition in French (? contraste ?), but I may use it
> wrongly in English?
> ?
> ? I'd define a "contrast" as the weights associated with the levels of a factor
> for formulating a hypothesis, where the weights traditionally are constrained
> to sum to 0, and to differentiate this from a column of the model matrix,
> which I'd more generally term a "regressor." Often, a traditional set of
> contrasts for a factor, one less than the number of levels, are defined not
> only to sum to 0  but also to be orthogonal in the basis of the design. The
> usage in R is more general, where "contrasts" mean the set of regressors
> used to represent a factor. Thus, contr.sum() generates regressors that
> satisfy the traditional definition of contrasts, as do contr.poly() and
> contr.helmert(), but the default contr.treatment() generates 0/1 dummy-
> coded regressors that don't satisfy the traditional definition of contrasts.
> ?
> ? >
> ? > Second, I may have wrongly understood the definitions of the various
> tests, ? > and especially how they generalize from linear model to
> GLM/GLMM...
> ? >
> ? > I thought type I was by taking the squared distance of the successive ? >
> orthogonal projections on the subspaces generated by the various terms, in
> ? > the order given in the model; type II, by ensuring that the term tested
> was ? > the last amongst terms of same order, after terms of lower order but
> before ? > terms of higher order; type III, by projecting on the subspace
> after removal ? > of the basis vectors for the term tested ? hence its strong
> dependency on ? > the coding scheme, and the ? drop1 ? trick to get them.
> ? >
> ? > Is this definition correct? Does it generalize to other kind models, or is ? >
> another definition required? Is it unambiguous? The SAS doc itself suggests ?
> > that various procedures call "type II" different kind of things ? ? Yes, if I've
> followed this correctly, it's correct, and it explains why it's possible to
> formulate the different types of tests in linear models independently of the
> contrasts (regressors) used to code the factors -- because fundamentally
> what's important is the subspace spanned by the regressors in each model,
> which is independent of coding. This approach, however, doesn't generalize
> easily beyond linear models fit by least squares. The approach taken in
> Anova() corresponds to this approach in linear models fit by least squares as
> long as the models remain full-rank and for type-III tests as long as the
> contrasts are properly formulated, and generalizes to other models with
> linear predictors.
> ?
> ? >
> ? > However, I cannot see clearly which hypothesis is indeed tested in each
> case, ? > especially in terms of cell means or marginal means (and, when I
> really need ? > it, I start from them and select the contrasts I need).  Is there
> any ? > package/software that allows to print the hypotheses testeds in
> terms of ? > means starting from the model formula?
> ? > Or is there any good reference that makes the link between the two?
> ? > For instance, a demonstration that the comparison of marginal means ? ?
> > always ? leads to a type XXX sum of square?
> ?
> ? This is where a complete explanation gets too lengthy for an email, but a
> shorthand formulation, e.g., for the model y ~ A*B, is that type-I tests
> correspond to the hypotheses A|(B = 0, AB = 0), B | AB = 0, AB = 0; type-II
> tests to A | AB = 0, B | AB = 0, AB = 0; and type-III tests to A = 0, B = 0, AB = 0.
> Here, e.g., | AB = 0 means assuming no AB interactions, so, e.g., the
> hypothesis A | AB = 0 means no A main effects assuming no AB interactions.
> A hypothesis like A = 0 is indeed formulated in terms of marginal means,
> understood as cell means for A averaging over the levels of B (not level
> means of A ignoring B).
> ?
> ? I realize that this is far from a complete explanation.
> ?
> ? Best,
> ?  John
> 
> --
>                                 Emmanuel CURIS
>                                 emmanuel.curis at parisdescartes.fr
> 
> Page WWW: http://emmanuel.curis.online.fr/index.html

From danprec at hotmail.com  Mon Feb 29 11:22:31 2016
From: danprec at hotmail.com (Daniel Preciado)
Date: Mon, 29 Feb 2016 11:22:31 +0100
Subject: [R-sig-ME] R lme() - MEEM error (singularity in Backsolve) due to
 user-specified contrasts amount (?)
Message-ID: <BLU436-SMTP1776734DB8DF878FFB5AF14A0BA0@phx.gbl>

Hello, 

I am trying to use lme() to fit and compare different models to data from an experiment in a repeated measures design. My dependent variable is response time (RT, in milliseconds); and I have 2 factors: F_A (2 levels) and F_B (3 Levels). For F_B, I have specified the following contrasts: 
F_B_C1 <- c(1, -1, 0)      # Contrast prize 1 and 2 levels 
F_B_C2 <- c(1, 0, -1)      # Contrast prize 1 with Neutral (no prize) 
F_B_C3 <- c(1, 0, -1)      # Contrast prize 2 with Neutral (no prize) 
F_B_C4 <- c(1, 1, -2)      # Contrast prize with Neutral 
contrasts(Data$F_B, how.many=4) <- cbind(F_B_C1, F_B_C2, F_B_C3, F_B_C4) 
Conditions 1 and 2 are 2 levels of the same manipulation, condition 3 is a neutral control. I am interested in the effect of each level (individually) on RT, and overall in the difference between the experimental manipulation (pooling the first 2 conditions of factor B) and the control condition (final condition of factor B). 

I defined the lme() models step-wise, starting with a Baseline model, and then updating that one to include each factor individually, and finally the interaction: 
RT_Base <- lme(RT ~ 1, random = ~1|SubjID/F_A/F_B, data=Data, method="ML")  #Baseline model 
RT_F_A <- update(RT_Base, .~. + F_A)            #Baseline + F_A 
RT_F_B <- update(RT_F_A, .~. + F_B)             #(Baseline+F_A) + F_B 
RT_Full <- update(RT_F_B, .~. + F_A:F_B)        #Full model (+ interaction) 
However, when I execute the code involving F_B, I get an 
"Error in MEEM (...): Singularity in Backsolve at level 0, block 1). 
I can still inspect the results of the model, but I would like to understand where is this error coming from, what does it mean, and how to avoid it. Furthermore, I realized that if I reduce the amount of contrasts to the default 2, the code runs without any error, so I can only assume that it has something to do with the user-specified comparison pairs. Also, the specified contrasts are not displayed (only the default first 2). 

I also read in some answer that the intercept needed to be suppressed in order to prevent this error (by adding RT ~ 0+Factors to the model formulae). I tried that, but it produces the same error. 

I would appreciate any feedback regarding this, Thanks! 


	[[alternative HTML version deleted]]


From nicolas.no.olano at dexia.com  Mon Feb 29 15:49:46 2016
From: nicolas.no.olano at dexia.com (OLANO NO Nicolas (Exterieur.DCL))
Date: Mon, 29 Feb 2016 14:49:46 +0000
Subject: [R-sig-ME] Saving object into a Matrix/List
Message-ID: <0AE1288CB7FB2C47B64B9DEF76DA1D332278F9@FCEXSEXM020P.dcl.int.dexwired.net>

Hello,

First of all, thank you very much for the package!

I'm using the new version of lme4 and the function glmer and I would like to know if you could help me with one doubt.

I want to compute the likelihood ratio test p value for each variable  (normally two or three variables) so run gm_original <- glmer (...~ X1 +  X2...), then gm_minus_X1 <- glmer (...~ X1...), finally anova(gm_original, gm_minus_X1).

As I have to compute it in a for loop with a lot of combinations of  variables, I would like to know if there is any way to save the  results (gm_original) into a matrix or list to then use them to calculate the  p values (when I add the objet to a list, and then I to run an anova,  I receive the error that it cannot be computed from a list objet).

Furthermore, is there any more efficient way to compute the likelihood  ratio test p value ?

Thank you very much,

Regards,

Nicolas Olano
Risk Quantification & Pricing
Dexia Group
(+32) (0)2 213 5873

-------------------------------------------------------------------------
Dexia disclaimer:

http://www.dexia.com/maildisclaimer.htm <http://www.dexia.com/maildisclaimer.htm>
-------------------------------------------------------------------------

	[[alternative HTML version deleted]]


From marko.bachl at uni-hohenheim.de  Mon Feb 29 19:52:49 2016
From: marko.bachl at uni-hohenheim.de (Marko Bachl)
Date: Mon, 29 Feb 2016 19:52:49 +0100
Subject: [R-sig-ME] Saving object into a Matrix/List
In-Reply-To: <0AE1288CB7FB2C47B64B9DEF76DA1D332278F9@FCEXSEXM020P.dcl.int.dexwired.net>
References: <0AE1288CB7FB2C47B64B9DEF76DA1D332278F9@FCEXSEXM020P.dcl.int.dexwired.net>
Message-ID: <CAE5vbru38Qc=akswmAOUN=bPfsBxFdV1t9gDSFZWMRD2KxJGPQ@mail.gmail.com>

Hi Nicolas,
I guess drop1(fitted_model, test = "Chisq") will give you the desired
result for omitting each term:

library(lme4)
fm <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
drop1(fm, test = "Chisq")

If you want to make more adjustments, you could save the fitted models
to a list and then use do.call():
fm_list = list()
fm_list[1] <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
fm_list[2] <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy)
do.call("anova", fm_list)

Best regards
Marko




2016-02-29 15:49 GMT+01:00 OLANO NO Nicolas (Exterieur.DCL)
<nicolas.no.olano at dexia.com>:
>
> Hello,
>
> First of all, thank you very much for the package!
>
> I'm using the new version of lme4 and the function glmer and I would like to know if you could help me with one doubt.
>
> I want to compute the likelihood ratio test p value for each variable  (normally two or three variables) so run gm_original <- glmer (...~ X1 +  X2...), then gm_minus_X1 <- glmer (...~ X1...), finally anova(gm_original, gm_minus_X1).
>
> As I have to compute it in a for loop with a lot of combinations of  variables, I would like to know if there is any way to save the  results (gm_original) into a matrix or list to then use them to calculate the  p values (when I add the objet to a list, and then I to run an anova,  I receive the error that it cannot be computed from a list objet).
>
> Furthermore, is there any more efficient way to compute the likelihood  ratio test p value ?
>
> Thank you very much,
>
> Regards,
>
> Nicolas Olano
> Risk Quantification & Pricing
> Dexia Group
> (+32) (0)2 213 5873
>
> -------------------------------------------------------------------------
> Dexia disclaimer:
>
> http://www.dexia.com/maildisclaimer.htm <http://www.dexia.com/maildisclaimer.htm>
> -------------------------------------------------------------------------
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




-- 
Dr. Marko Bachl
Universit?t Hohenheim
Institut f?r Kommunikationswissenschaft (540C)
T 0711 459 228 66
M marko.bachl at uni-hohenheim.de
W www.komm.uni-hohenheim.de/bachl


From bbolker at gmail.com  Mon Feb 29 19:54:20 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 29 Feb 2016 13:54:20 -0500
Subject: [R-sig-ME] R lme() - MEEM error (singularity in Backsolve) due
 to user-specified contrasts amount (?)
In-Reply-To: <BLU436-SMTP1776734DB8DF878FFB5AF14A0BA0@phx.gbl>
References: <BLU436-SMTP1776734DB8DF878FFB5AF14A0BA0@phx.gbl>
Message-ID: <CABghstSShjs=emvRTmGviKsb=SUMhNcFZ0cBKNZ8-WsRM8zj=Q@mail.gmail.com>

Are your F_B_C2 and F_B_C3 contrasts really identical, or is that a typo?

A reproducible example would be nice ...

On Mon, Feb 29, 2016 at 5:22 AM, Daniel Preciado <danprec at hotmail.com> wrote:
> Hello,
>
> I am trying to use lme() to fit and compare different models to data from an experiment in a repeated measures design. My dependent variable is response time (RT, in milliseconds); and I have 2 factors: F_A (2 levels) and F_B (3 Levels). For F_B, I have specified the following contrasts:
> F_B_C1 <- c(1, -1, 0)      # Contrast prize 1 and 2 levels
> F_B_C2 <- c(1, 0, -1)      # Contrast prize 1 with Neutral (no prize)
> F_B_C3 <- c(1, 0, -1)      # Contrast prize 2 with Neutral (no prize)
> F_B_C4 <- c(1, 1, -2)      # Contrast prize with Neutral
> contrasts(Data$F_B, how.many=4) <- cbind(F_B_C1, F_B_C2, F_B_C3, F_B_C4)
> Conditions 1 and 2 are 2 levels of the same manipulation, condition 3 is a neutral control. I am interested in the effect of each level (individually) on RT, and overall in the difference between the experimental manipulation (pooling the first 2 conditions of factor B) and the control condition (final condition of factor B).
>
> I defined the lme() models step-wise, starting with a Baseline model, and then updating that one to include each factor individually, and finally the interaction:
> RT_Base <- lme(RT ~ 1, random = ~1|SubjID/F_A/F_B, data=Data, method="ML")  #Baseline model
> RT_F_A <- update(RT_Base, .~. + F_A)            #Baseline + F_A
> RT_F_B <- update(RT_F_A, .~. + F_B)             #(Baseline+F_A) + F_B
> RT_Full <- update(RT_F_B, .~. + F_A:F_B)        #Full model (+ interaction)
> However, when I execute the code involving F_B, I get an
> "Error in MEEM (...): Singularity in Backsolve at level 0, block 1).
> I can still inspect the results of the model, but I would like to understand where is this error coming from, what does it mean, and how to avoid it. Furthermore, I realized that if I reduce the amount of contrasts to the default 2, the code runs without any error, so I can only assume that it has something to do with the user-specified comparison pairs. Also, the specified contrasts are not displayed (only the default first 2).
>
> I also read in some answer that the intercept needed to be suppressed in order to prevent this error (by adding RT ~ 0+Factors to the model formulae). I tried that, but it produces the same error.
>
> I would appreciate any feedback regarding this, Thanks!
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Mon Feb 29 19:57:11 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 29 Feb 2016 13:57:11 -0500
Subject: [R-sig-ME] R lme() - MEEM error (singularity in Backsolve) due
 to user-specified contrasts amount (?)
In-Reply-To: <CABghstSShjs=emvRTmGviKsb=SUMhNcFZ0cBKNZ8-WsRM8zj=Q@mail.gmail.com>
References: <BLU436-SMTP1776734DB8DF878FFB5AF14A0BA0@phx.gbl>
	<CABghstSShjs=emvRTmGviKsb=SUMhNcFZ0cBKNZ8-WsRM8zj=Q@mail.gmail.com>
Message-ID: <CABghstSX=qHgOC3mBb1SBm4a8r4KaqBT=UYaO=_bVchuOLR0pw@mail.gmail.com>

PS if you have three levels of factor B, you can have at most two
contrasts associated with the factor *when you fit the model*.  You
can use the effects or lsmeans or contrast packages (or probably
others) to compute the values and inferential statistics on the
contrasts after you've fitted the model.

On Mon, Feb 29, 2016 at 1:54 PM, Ben Bolker <bbolker at gmail.com> wrote:
> Are your F_B_C2 and F_B_C3 contrasts really identical, or is that a typo?
>
> A reproducible example would be nice ...
>
> On Mon, Feb 29, 2016 at 5:22 AM, Daniel Preciado <danprec at hotmail.com> wrote:
>> Hello,
>>
>> I am trying to use lme() to fit and compare different models to data from an experiment in a repeated measures design. My dependent variable is response time (RT, in milliseconds); and I have 2 factors: F_A (2 levels) and F_B (3 Levels). For F_B, I have specified the following contrasts:
>> F_B_C1 <- c(1, -1, 0)      # Contrast prize 1 and 2 levels
>> F_B_C2 <- c(1, 0, -1)      # Contrast prize 1 with Neutral (no prize)
>> F_B_C3 <- c(1, 0, -1)      # Contrast prize 2 with Neutral (no prize)
>> F_B_C4 <- c(1, 1, -2)      # Contrast prize with Neutral
>> contrasts(Data$F_B, how.many=4) <- cbind(F_B_C1, F_B_C2, F_B_C3, F_B_C4)
>> Conditions 1 and 2 are 2 levels of the same manipulation, condition 3 is a neutral control. I am interested in the effect of each level (individually) on RT, and overall in the difference between the experimental manipulation (pooling the first 2 conditions of factor B) and the control condition (final condition of factor B).
>>
>> I defined the lme() models step-wise, starting with a Baseline model, and then updating that one to include each factor individually, and finally the interaction:
>> RT_Base <- lme(RT ~ 1, random = ~1|SubjID/F_A/F_B, data=Data, method="ML")  #Baseline model
>> RT_F_A <- update(RT_Base, .~. + F_A)            #Baseline + F_A
>> RT_F_B <- update(RT_F_A, .~. + F_B)             #(Baseline+F_A) + F_B
>> RT_Full <- update(RT_F_B, .~. + F_A:F_B)        #Full model (+ interaction)
>> However, when I execute the code involving F_B, I get an
>> "Error in MEEM (...): Singularity in Backsolve at level 0, block 1).
>> I can still inspect the results of the model, but I would like to understand where is this error coming from, what does it mean, and how to avoid it. Furthermore, I realized that if I reduce the amount of contrasts to the default 2, the code runs without any error, so I can only assume that it has something to do with the user-specified comparison pairs. Also, the specified contrasts are not displayed (only the default first 2).
>>
>> I also read in some answer that the intercept needed to be suppressed in order to prevent this error (by adding RT ~ 0+Factors to the model formulae). I tried that, but it produces the same error.
>>
>> I would appreciate any feedback regarding this, Thanks!
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From y.shinohara at aoni.waseda.jp  Tue Mar  1 10:14:46 2016
From: y.shinohara at aoni.waseda.jp (Yasuaki SHINOHARA)
Date: Tue, 01 Mar 2016 18:14:46 +0900
Subject: [R-sig-ME] Post-hoc analysis of logistic mixed effects model
Message-ID: <web-65556239@besv02.spw.secure-premium.ne.jp>

Dear list,
I am analyzing data with a logistic mixed effects model.
I created the following model.

mod1<-glmer(binomial_resposne ~ FactorA*FactorB*FactorC 
+(1+FactorA|subject)+(1+FactorA|speaker/item),family=binomial,data=mydata,control=glmerControl(optimizer="bobyqa", 
optCtrl=list(maxfun=1000)))

FactorA is a categorical variable with 2 levels, "A1" and "A2".
FactorB is a categorical variable with 3 levels, "B1", "B2", and "B3".
FactorC is a continuous variable.

To analyze the main effect of each factor and their interactions, I 
used Anova() with the car package, and reported the results of main 
effects of each factor in a paper (e.g., ?2(2) = 20.11, p < 0.001).
To do post-hoc analyses, I made orthogonal contrasts for each factor, 
and used the summary() function. I reported the b-value, SE, z-value 
and p-value for the significant effects (e.g.,? = 0.04, SE = 0.01, z = 
2.76, p < 0.01).

Is it possible to stick to one method of the analyses, rather than 
using two different methods (chi-square and regression coefficients)?
So my questions are as follows.
1. If I stick to the regression coefficient method with the summary() 
function, how could I get the results of the main analysis for FactorB 
with 3 levels?
2. If I stick to the chi-square method with the Anova() function, how 
could I do post-hoc tests with the same method (not lsmeans or glht)?

Thank you very much for your help in advance.

Best wishes,
Yasu


From danprec at hotmail.com  Wed Mar  2 12:09:45 2016
From: danprec at hotmail.com (Daniel Preciado)
Date: Wed, 2 Mar 2016 12:09:45 +0100
Subject: [R-sig-ME] R lme() - MEEM error (singularity in Backsolve) due
 to user-specified contrasts amount (?)
Message-ID: <SNT405-EAS4207BDEDECC61F5A2298A0CA0BC0@phx.gbl>

Hello Ben,

Thanks a lot for your feedback! I get it now, I think. My biggest concern was that the fitting itself, getting such an error made me a bit distrustful of the fitting result. But if I understand you correctly, I can leave the contrast out while fitting the model, and then just "manually" check the contrasts.

I can forward sample data and the code if you think it is helpful (thanks for making me realize about the typo in the contrasts, should be 0, 1, -1), but you have already solved the issue for me.

Thanks again,

On 29 Feb 2016 19:57, Ben Bolker <bbolker at gmail.com> wrote:
>
> >
> > PS if you have three levels of factor B, you can have at most two
> > contrasts associated with the factor *when you fit the model*.? You
> > can use the effects or lsmeans or contrast packages (or probably
> > others) to compute the values and inferential statistics on the
> > contrasts after you've fitted the model.
> >
> > On Mon, Feb 29, 2016 at 1:54 PM, Ben Bolker <bbolker at gmail.com> wrote:
> > > Are your F_B_C2 and F_B_C3 contrasts really identical, or is that a typo?
> > >
> > > A reproducible example would be nice ...
> > >
> > > On Mon, Feb 29, 2016 at 5:22 AM, Daniel Preciado <danprec at hotmail.com> wrote:
> > >> Hello,
> > >>
> > >> I am trying to use lme() to fit and compare different models to data from an experiment in a repeated measures design. My dependent variable is response time (RT, in milliseconds); and I have 2 factors: F_A (2 levels) and F_B (3 Levels). For F_B, I have specified the following contrasts:
> > >> F_B_C1 <- c(1, -1, 0)????? # Contrast prize 1 and 2 levels
> > >> F_B_C2 <- c(1, 0, -1)????? # Contrast prize 1 with Neutral (no prize)
> > >> F_B_C3 <- c(1, 0, -1)????? # Contrast prize 2 with Neutral (no prize)
> > >> F_B_C4 <- c(1, 1, -2)????? # Contrast prize with Neutral
> > >> contrasts(Data$F_B, how.many=4) <- cbind(F_B_C1, F_B_C2, F_B_C3, F_B_C4)
> > >> Conditions 1 and 2 are 2 levels of the same manipulation, condition 3 is a neutral control. I am interested in the effect of each level (individually) on RT, and overall in the difference between the experimental manipulation (pooling the first 2 conditions of factor B) and the control condition (final condition of factor B).
> > >>
> > >> I defined the lme() models step-wise, starting with a Baseline model, and then updating that one to include each factor individually, and finally the interaction:
> > >> RT_Base <- lme(RT ~ 1, random = ~1|SubjID/F_A/F_B, data=Data, method="ML")? #Baseline model
> > >> RT_F_A <- update(RT_Base, .~. + F_A)??????????? #Baseline + F_A
> > >> RT_F_B <- update(RT_F_A, .~. + F_B)???????????? #(Baseline+F_A) + F_B
> > >> RT_Full <- update(RT_F_B, .~. + F_A:F_B)??????? #Full model (+ interaction)
> > >> However, when I execute the code involving F_B, I get an
> > >> "Error in MEEM (...): Singularity in Backsolve at level 0, block 1).
> > >> I can still inspect the results of the model, but I would like to understand where is this error coming from, what does it mean, and how to avoid it. Furthermore, I realized that if I reduce the amount of contrasts to the default 2, the code runs without any error, so I can only assume that it has something to do with the user-specified comparison pairs. Also, the specified contrasts are not displayed (only the default first 2).
> > >>
> > >> I also read in some answer that the intercept needed to be suppressed in order to prevent this error (by adding RT ~ 0+Factors to the model formulae). I tried that, but it produces the same error.
> > >>
> > >> I would appreciate any feedback regarding this, Thanks!
> > >>
> > >>
> > >>???????? [[alternative HTML version deleted]]
> > >>
> > >> _______________________________________________
> > >> R-sig-mixed-models at r-project.org mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From nicolas.no.olano at dexia.com  Tue Mar  1 11:10:35 2016
From: nicolas.no.olano at dexia.com (OLANO NO Nicolas (Exterieur.DCL))
Date: Tue, 1 Mar 2016 10:10:35 +0000
Subject: [R-sig-ME] Saving object into a Matrix/List
In-Reply-To: <CAE5vbru38Qc=akswmAOUN=bPfsBxFdV1t9gDSFZWMRD2KxJGPQ@mail.gmail.com>
References: <0AE1288CB7FB2C47B64B9DEF76DA1D332278F9@FCEXSEXM020P.dcl.int.dexwired.net>
	<CAE5vbru38Qc=akswmAOUN=bPfsBxFdV1t9gDSFZWMRD2KxJGPQ@mail.gmail.com>
Message-ID: <0AE1288CB7FB2C47B64B9DEF76DA1D33227960@FCEXSEXM020P.dcl.int.dexwired.net>

Thank you very much Marko! I'll try this tomorrow.

Regards,

Nicolas Olano
Risk Quantification & Pricing
Dexia Group
(+32) (0)2?213 5873

-----Original Message-----
From: marko.bachl at gmail.com [mailto:marko.bachl at gmail.com] On Behalf Of Marko Bachl
Sent: lundi 29 f?vrier 2016 19:53
To: OLANO NO Nicolas (Exterieur.DCL)
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Saving object into a Matrix/List

Hi Nicolas,
I guess drop1(fitted_model, test = "Chisq") will give you the desired result for omitting each term:

library(lme4)
fm <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy) drop1(fm, test = "Chisq")

If you want to make more adjustments, you could save the fitted models to a list and then use do.call():
fm_list = list()
fm_list[1] <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy) fm_list[2] <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy) do.call("anova", fm_list)

Best regards
Marko




2016-02-29 15:49 GMT+01:00 OLANO NO Nicolas (Exterieur.DCL)
<nicolas.no.olano at dexia.com>:
>
> Hello,
>
> First of all, thank you very much for the package!
>
> I'm using the new version of lme4 and the function glmer and I would like to know if you could help me with one doubt.
>
> I want to compute the likelihood ratio test p value for each variable  (normally two or three variables) so run gm_original <- glmer (...~ X1 +  X2...), then gm_minus_X1 <- glmer (...~ X1...), finally anova(gm_original, gm_minus_X1).
>
> As I have to compute it in a for loop with a lot of combinations of  variables, I would like to know if there is any way to save the  results (gm_original) into a matrix or list to then use them to calculate the  p values (when I add the objet to a list, and then I to run an anova,  I receive the error that it cannot be computed from a list objet).
>
> Furthermore, is there any more efficient way to compute the likelihood  ratio test p value ?
>
> Thank you very much,
>
> Regards,
>
> Nicolas Olano
> Risk Quantification & Pricing
> Dexia Group
> (+32) (0)2 213 5873
>
> ----------------------------------------------------------------------
> ---
> Dexia disclaimer:
>
> http://www.dexia.com/maildisclaimer.htm 
> <http://www.dexia.com/maildisclaimer.htm>
> ----------------------------------------------------------------------
> ---
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




--
Dr. Marko Bachl
Universit?t Hohenheim
Institut f?r Kommunikationswissenschaft (540C) T 0711 459 228 66 M marko.bachl at uni-hohenheim.de W www.komm.uni-hohenheim.de/bachl
-------------------------------------------------------------------------
Dexia disclaimer:

http://www.dexia.com/maildisclaimer.htm <http://www.dexia.com/maildisclaimer.htm>
-------------------------------------------------------------------------

From a.debelle at sussex.ac.uk  Fri Mar  4 14:30:19 2016
From: a.debelle at sussex.ac.uk (Allan Debelle)
Date: Fri, 04 Mar 2016 13:30:19 +0000
Subject: [R-sig-ME] Syntax for a multivariate & multilevel MCMCglmm model
Message-ID: <56D98DEB.9090903@sussex.ac.uk>

Hello everyone,

I am trying to analyse a dataset using a multivariate MCMCglmm model, 
but I cannot solve a syntax problem related to the structure of my dataset.

Basically, 50 individuals were exposed to either an A or a B treatment, 
and then 4 traits were measured on each individual, as well as 2 
covariates. This experiment was replicated 4 times, meaning that I have 
4 replicates (rep1, rep2, rep3 and rep4) for each of the two treatments 
(A and B).

The dataset looks something like this, with 'treat_rep' just being a 
unique identifier for each combination of treatment x replicate (I 
simplified it and generated random numbers, just FYI):

varA    varB    varC    varD    cov1    cov2    treatment    replicate   
  treat_rep

0.109488619    0.675591081    0.940580782    0.366980736    
0.911079042    0.468285642    A    rep1    A1
0.400887809    0.43162503    0.823351715    0.76152362    0.003221553    
0.622398329    A    rep2    A2
0.176948608    0.074676865    0.91156597    0.747663021    
0.028740661    0.856395358    A    rep3    A3
0.16468968    0.776755434    0.367321135    0.886352101    
0.083174005    0.246884218    A    rep4    A4
0.090596435    0.785823554    0.061103075    0.894436144    
0.989249957    0.50533554    B    rep1    B1
0.839349822    0.639784216    0.293986243    0.55812818    
0.307394708    0.036590982    B    rep2    B2
0.711702334    0.174145468    0.634296876    0.442112773    
0.480754889    0.863199351    B    rep3    B3
0.052352675    0.492622311    0.668647151    0.683243455    
0.958397833    0.857768988    B    rep4    B4
...

##############################

What I want to do is to test for the effect of the treatment on the 
traits I measured, while taking into account the effects of my 
covariates, so something like this:

varA + varB +varC +varD ~ treatment + cov1 + cov2

The problem is that I have to deal with a nested data structure. Each 
individual belongs to a replicate of a treatment (e.g. rep1 of treatment 
A, or rep3 of treatment B, etc.), and I am not sure how to specify this 
correctly in MCMCglmm. I tried a few things, but something is wrong 
either in the prior specification or/and in my specification of the 
variance structure of the random effects (and potentially of the 
residuals too).

##############################

Among other things, I tried to use the treat_rep variable as a grouping 
factor:

prior<- list(R=list(V=diag(4),nu=4),G=list(G1=list(V=diag(4),nu=4)))

   model <- MCMCglmm(

   fixed = cbind(varA, varB, varC, varD) ~ -1 + treatment + cov1 + cov2,

   random = ~ us(trait):treat_rep,

   rcov = ~ us(trait):units,

   prior = prior,

   family = c("gaussian", "gaussian", "gaussian", "gaussian"), nitt = 
100000, burnin = 5000,

   thin = 25, data = dataset)

However, and unless I am wrong, it is not nesting 'replicate' within 
'treatment'. Which means I end up with no effect of 'treatment', whereas 
I do have this effect when I run independent models using lme4 for each 
of the 4 traits (and in that case the nesting syntax is more 
straightforward).

##############################

I also tried this:

   prior<- 
list(R=list(V=diag(16),nu=16),G=list(G1=list(V=diag(8),nu=8))) # 4 
traits x 2 treatments x 4 replicates // 4 traits x 2 treatments

   model <- MCMCglmm(

   fixed = cbind(varA, varB, varC, varD) ~ -1 + treatment + cov1 + cov2,

   random = ~ us(trait:treatment):replicate,

   rcov = ~ us(trait:treatment:replicate):units,

   prior = prior,
   family = c("gaussian", "gaussian", "gaussian", "gaussian"), nitt = 
100000, burnin = 5000,
   thin = 25, data = dataset)

but I get the following error message:

Error in `[.data.frame`(data, , components[[1]]) :
   undefined columns selected

##############################

So, my first question is: What would be the syntax for this kind of 
nested structure (replicate nested within treatment) in MCMCglmm?

My second question is: What would be the syntax if there was another 
level of variance related to this nested structure? I'm thinking of the 
situation where there would be a sort of treatment nesting within each 
replicate (e.g. if each of the four pairs of replicates was kept in a 
different incubator, or if each pair had been done at a different moment 
in time, etc.). Would you just add a 'replicate' random effect in the 
model? How?

Any help with this would be fantastic.

Thanks a lot in advance.

Al

	[[alternative HTML version deleted]]


From paul.debes at utu.fi  Fri Mar  4 15:59:17 2016
From: paul.debes at utu.fi (Paul Debes)
Date: Fri, 04 Mar 2016 16:59:17 +0200
Subject: [R-sig-ME] Syntax for a multivariate & multilevel MCMCglmm model
In-Reply-To: <56D98DEB.9090903@sussex.ac.uk>
References: <56D98DEB.9090903@sussex.ac.uk>
Message-ID: <op.yds2k3opsgx3xe@armadillo50>

Hi Allan,

This model (explicitly nesting random replicates within treatments):

fixed = var ~ 1 + treatment + cov1 + cov2,
random = ~ treatment:replicate,
rcov = ~ units

expanded to a multivariate model with unstructured covariances in G and R  
may be (you missed the "trait" interaction in the fixed statement of your  
first model):

fixed = cbind(varA, varB, varC, varD) ~ trait + trait:treatment +  
trait:cov1 + trait:cov2,
random = ~ us(trait):treatment:replicate,
rcov = ~ us(trait):units

Adding the trait interactions probably gets you back the treatment effects  
you did see in the univariate models.

How you add an additional random term, nested within replicates, will  
depend on how your random treatment:replicate effects are correlated  
between traits (e.g., "us" above may be reduced to "diag" if not  
correlated) and how the sub-replicates are correlated with the replicates  
and the traits. Plotting random effects and nested model testing may be  
required to find a useful solution.

Paul


On Fri, 04 Mar 2016 15:30:19 +0200, Allan Debelle <a.debelle at sussex.ac.uk>  
wrote:

> Hello everyone,
>
> I am trying to analyse a dataset using a multivariate MCMCglmm model,
> but I cannot solve a syntax problem related to the structure of my  
> dataset.
>
> Basically, 50 individuals were exposed to either an A or a B treatment,
> and then 4 traits were measured on each individual, as well as 2
> covariates. This experiment was replicated 4 times, meaning that I have
> 4 replicates (rep1, rep2, rep3 and rep4) for each of the two treatments
> (A and B).
>
> The dataset looks something like this, with 'treat_rep' just being a
> unique identifier for each combination of treatment x replicate (I
> simplified it and generated random numbers, just FYI):
>
> varA    varB    varC    varD    cov1    cov2    treatment    replicate
>   treat_rep
>
> 0.109488619    0.675591081    0.940580782    0.366980736
> 0.911079042    0.468285642    A    rep1    A1
> 0.400887809    0.43162503    0.823351715    0.76152362    0.003221553
> 0.622398329    A    rep2    A2
> 0.176948608    0.074676865    0.91156597    0.747663021
> 0.028740661    0.856395358    A    rep3    A3
> 0.16468968    0.776755434    0.367321135    0.886352101
> 0.083174005    0.246884218    A    rep4    A4
> 0.090596435    0.785823554    0.061103075    0.894436144
> 0.989249957    0.50533554    B    rep1    B1
> 0.839349822    0.639784216    0.293986243    0.55812818
> 0.307394708    0.036590982    B    rep2    B2
> 0.711702334    0.174145468    0.634296876    0.442112773
> 0.480754889    0.863199351    B    rep3    B3
> 0.052352675    0.492622311    0.668647151    0.683243455
> 0.958397833    0.857768988    B    rep4    B4
> ...
>
> ##############################
>
> What I want to do is to test for the effect of the treatment on the
> traits I measured, while taking into account the effects of my
> covariates, so something like this:
>
> varA + varB +varC +varD ~ treatment + cov1 + cov2
>
> The problem is that I have to deal with a nested data structure. Each
> individual belongs to a replicate of a treatment (e.g. rep1 of treatment
> A, or rep3 of treatment B, etc.), and I am not sure how to specify this
> correctly in MCMCglmm. I tried a few things, but something is wrong
> either in the prior specification or/and in my specification of the
> variance structure of the random effects (and potentially of the
> residuals too).
>
> ##############################
>
> Among other things, I tried to use the treat_rep variable as a grouping
> factor:
>
> prior<- list(R=list(V=diag(4),nu=4),G=list(G1=list(V=diag(4),nu=4)))
>
>    model <- MCMCglmm(
>
>    fixed = cbind(varA, varB, varC, varD) ~ -1 + treatment + cov1 + cov2,
>
>    random = ~ us(trait):treat_rep,
>
>    rcov = ~ us(trait):units,
>
>    prior = prior,
>
>    family = c("gaussian", "gaussian", "gaussian", "gaussian"), nitt =
> 100000, burnin = 5000,
>
>    thin = 25, data = dataset)
>
> However, and unless I am wrong, it is not nesting 'replicate' within
> 'treatment'. Which means I end up with no effect of 'treatment', whereas
> I do have this effect when I run independent models using lme4 for each
> of the 4 traits (and in that case the nesting syntax is more
> straightforward).
>
> ##############################
>
> I also tried this:
>
>    prior<-
> list(R=list(V=diag(16),nu=16),G=list(G1=list(V=diag(8),nu=8))) # 4
> traits x 2 treatments x 4 replicates // 4 traits x 2 treatments
>
>    model <- MCMCglmm(
>
>    fixed = cbind(varA, varB, varC, varD) ~ -1 + treatment + cov1 + cov2,
>
>    random = ~ us(trait:treatment):replicate,
>
>    rcov = ~ us(trait:treatment:replicate):units,
>
>    prior = prior,
>    family = c("gaussian", "gaussian", "gaussian", "gaussian"), nitt =
> 100000, burnin = 5000,
>    thin = 25, data = dataset)
>
> but I get the following error message:
>
> Error in `[.data.frame`(data, , components[[1]]) :
>    undefined columns selected
>
> ##############################
>
> So, my first question is: What would be the syntax for this kind of
> nested structure (replicate nested within treatment) in MCMCglmm?
>
> My second question is: What would be the syntax if there was another
> level of variance related to this nested structure? I'm thinking of the
> situation where there would be a sort of treatment nesting within each
> replicate (e.g. if each of the four pairs of replicates was kept in a
> different incubator, or if each pair had been done at a different moment
> in time, etc.). Would you just add a 'replicate' random effect in the
> model? How?
>
> Any help with this would be fantastic.
>
> Thanks a lot in advance.
>
> Al
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
Paul V. Debes
DFG Research Fellow
University of Turku
Department of Biology
Finland

Email: paul.debes at utu.fi


From christoph.huber-huber at univie.ac.at  Fri Mar  4 17:21:00 2016
From: christoph.huber-huber at univie.ac.at (Christoph Huber-Huber)
Date: Fri, 4 Mar 2016 17:21:00 +0100
Subject: [R-sig-ME] bootstrapped CIs for fixed effects in binomial model
 with random slopes
In-Reply-To: <mailman.1.1456484401.11612.r-sig-mixed-models@r-project.org>
References: <mailman.1.1456484401.11612.r-sig-mixed-models@r-project.org>
Message-ID: <56D9B5EC.8020607@univie.ac.at>

Dear list,

I've made a puzzling observation concerning the bootstrap distribution 
of fixed effects in a logistic model after incorporating random slopes. 
I'm wondering whether that is actually to be expected or whether my 
model speficification is problematic in some way.

I have a datset with a binomial response variable, a couple of 
categorical predictors and one grouping variable.
I estimate the model with lme4 by:

m10 <- glmer(dv ~ cr * inter + (cr | vpnr),
                   family = binomial(link = "logit"),
                   data = dat,
                   control = glmerControl(optCtrl = list(maxfun = 
1e+07), optimizer = "bobyqa"))

The model converges and the output seems to be reasonable.

I'm specifically interested in whether random slope correlate. I use the 
"bootMer" function to get bootstrapped confidence intervals for the 
parameters of the variance-covariance matrix, and also for the fixed 
effects.
Now, the puzzling thing is that the bootstrapped distributions of the 
fixed effects are all centered on zero - for all fixed effects, the 
"bias" cancells out the the "original" value - although the fixed effect 
parameters all have a z-value between -6 and -11.

Bootstrap Statistics :   t1* to t12* are fixed effects
          original        bias    std. error
t1*   0.507682933 -5.065802e-01  0.05356729
t2*  -0.410385799  4.111768e-01  0.06641469
t3*  -0.632799315  6.329604e-01  0.07389701
...
t11*  0.046429240 -4.954121e-02  0.14428656
t12*  0.008759285 -7.260454e-03  0.05337620
t13*  1.000000000  0.000000e+00  0.00000000
t14*  0.276959335 -4.755610e-03  0.04368839
t15* -0.107974393  2.175404e-03  0.03900303
t16* -0.173489566  2.103498e-03  0.04720509
...
t34*  0.050002275 -1.520703e-03  0.01094700

When I remove the random slopes in the model, by replacing the 'cr' in 
the random term with '1':

m11 <- udpate(m10, dv ~ cr * inter + (1 | vpnr))

the corresponding bootstraped distributions for the fixed effects are 
now centered around the "original" parameter estimates, and not around 
zero anymore. The "bias" here is much smaller than the "original" 
parameter values.

Bootstrap Statistics :   t1* to t12* are fixed effects
         original        bias    std. error
t1*   0.48741049  1.603870e-04  0.05357392
t2*  -0.40698631 -6.327798e-04  0.05329463
t3*  -0.57912898  2.843670e-04  0.05492087
...
t11*  0.05052645 -1.667216e-03  0.12505046
t12*  0.01646606 -4.656007e-04  0.03281307
t13*  1.00000000  0.000000e+00  0.00000000
t14*  0.25614874 -4.534802e-03  0.04021043


I guess this change in the location/bias of the bootstrapped 
distribution of the fixed effect parameters has something to do with the 
somehow special residual variance term in a binomial model (sigma(m10) = 
1, sigma(m11) = 1) in connection with how bootMer works (sampling the 
residuals, etc.), but I'm far away from tracing down the exact reason.
Are both models m10 and m11 actually ok, or does this shift in the 
boostrapped distributions rather indicate any problems entailed by the 
random slopes? Should a different boostrap procedure (something else 
than bootMer) be used in this case?

Any advice is greatly appreciated.

Thank you very much,
Christoph



-- 
Christoph Huber-Huber, BA MSc
University of Vienna
Department of Basic Psychological Research and Research Methods
Liebiggasse 5
1010 Vienna
Austria

e-mail: christoph.huber-huber at univie.ac.at
phone: +43 1 4277 47147
www: http://psychologie.univie.ac.at/en/


From a.debelle at sussex.ac.uk  Fri Mar  4 18:00:21 2016
From: a.debelle at sussex.ac.uk (Allan Debelle)
Date: Fri, 04 Mar 2016 17:00:21 +0000
Subject: [R-sig-ME] Syntax for a multivariate & multilevel MCMCglmm model
In-Reply-To: <op.yds2k3opsgx3xe@armadillo50>
References: <56D98DEB.9090903@sussex.ac.uk> <op.yds2k3opsgx3xe@armadillo50>
Message-ID: <56D9BF25.5030805@sussex.ac.uk>

Hi Paul,

Thank you very much for taking the time to have a look at this.

No need to modify the residual variance structure then? Just "rcov = ~ 
us(trait):units" ? And I don't need to change the syntax of the prior?

Thank you.

Al

> Paul Debes <mailto:paul.debes at utu.fi>
> 4 March 2016 14:59
> Hi Allan,
>
> This model (explicitly nesting random replicates within treatments):
>
> fixed = var ~ 1 + treatment + cov1 + cov2,
> random = ~ treatment:replicate,
> rcov = ~ units
>
> expanded to a multivariate model with unstructured covariances in G 
> and R may be (you missed the "trait" interaction in the fixed 
> statement of your first model):
>
> fixed = cbind(varA, varB, varC, varD) ~ trait + trait:treatment + 
> trait:cov1 + trait:cov2,
> random = ~ us(trait):treatment:replicate,
> rcov = ~ us(trait):units
>
> Adding the trait interactions probably gets you back the treatment 
> effects you did see in the univariate models.
>
> How you add an additional random term, nested within replicates, will 
> depend on how your random treatment:replicate effects are correlated 
> between traits (e.g., "us" above may be reduced to "diag" if not 
> correlated) and how the sub-replicates are correlated with the 
> replicates and the traits. Plotting random effects and nested model 
> testing may be required to find a useful solution.
>
> Paul
>
>
> On Fri, 04 Mar 2016 15:30:19 +0200, Allan Debelle 
> <a.debelle at sussex.ac.uk> wrote:
>
>
>
> Allan Debelle <mailto:a.debelle at sussex.ac.uk>
> 4 March 2016 13:30
> Hello everyone,
>
> I am trying to analyse a dataset using a multivariate MCMCglmm model,
> but I cannot solve a syntax problem related to the structure of my 
> dataset.
>
> Basically, 50 individuals were exposed to either an A or a B treatment,
> and then 4 traits were measured on each individual, as well as 2
> covariates. This experiment was replicated 4 times, meaning that I have
> 4 replicates (rep1, rep2, rep3 and rep4) for each of the two treatments
> (A and B).
>
> The dataset looks something like this, with 'treat_rep' just being a
> unique identifier for each combination of treatment x replicate (I
> simplified it and generated random numbers, just FYI):
>
> varA varB varC varD cov1 cov2 treatment replicate
> treat_rep
>
> 0.109488619 0.675591081 0.940580782 0.366980736
> 0.911079042 0.468285642 A rep1 A1
> 0.400887809 0.43162503 0.823351715 0.76152362 0.003221553
> 0.622398329 A rep2 A2
> 0.176948608 0.074676865 0.91156597 0.747663021
> 0.028740661 0.856395358 A rep3 A3
> 0.16468968 0.776755434 0.367321135 0.886352101
> 0.083174005 0.246884218 A rep4 A4
> 0.090596435 0.785823554 0.061103075 0.894436144
> 0.989249957 0.50533554 B rep1 B1
> 0.839349822 0.639784216 0.293986243 0.55812818
> 0.307394708 0.036590982 B rep2 B2
> 0.711702334 0.174145468 0.634296876 0.442112773
> 0.480754889 0.863199351 B rep3 B3
> 0.052352675 0.492622311 0.668647151 0.683243455
> 0.958397833 0.857768988 B rep4 B4
> ...
>
> ##############################
>
> What I want to do is to test for the effect of the treatment on the
> traits I measured, while taking into account the effects of my
> covariates, so something like this:
>
> varA + varB +varC +varD ~ treatment + cov1 + cov2
>
> The problem is that I have to deal with a nested data structure. Each
> individual belongs to a replicate of a treatment (e.g. rep1 of treatment
> A, or rep3 of treatment B, etc.), and I am not sure how to specify this
> correctly in MCMCglmm. I tried a few things, but something is wrong
> either in the prior specification or/and in my specification of the
> variance structure of the random effects (and potentially of the
> residuals too).
>
> ##############################
>
> Among other things, I tried to use the treat_rep variable as a grouping
> factor:
>
> prior<- list(R=list(V=diag(4),nu=4),G=list(G1=list(V=diag(4),nu=4)))
>
> model <- MCMCglmm(
>
> fixed = cbind(varA, varB, varC, varD) ~ -1 + treatment + cov1 + cov2,
>
> random = ~ us(trait):treat_rep,
>
> rcov = ~ us(trait):units,
>
> prior = prior,
>
> family = c("gaussian", "gaussian", "gaussian", "gaussian"), nitt =
> 100000, burnin = 5000,
>
> thin = 25, data = dataset)
>
> However, and unless I am wrong, it is not nesting 'replicate' within
> 'treatment'. Which means I end up with no effect of 'treatment', whereas
> I do have this effect when I run independent models using lme4 for each
> of the 4 traits (and in that case the nesting syntax is more
> straightforward).
>
> ##############################
>
> I also tried this:
>
> prior<-
> list(R=list(V=diag(16),nu=16),G=list(G1=list(V=diag(8),nu=8))) # 4
> traits x 2 treatments x 4 replicates // 4 traits x 2 treatments
>
> model <- MCMCglmm(
>
> fixed = cbind(varA, varB, varC, varD) ~ -1 + treatment + cov1 + cov2,
>
> random = ~ us(trait:treatment):replicate,
>
> rcov = ~ us(trait:treatment:replicate):units,
>
> prior = prior,
> family = c("gaussian", "gaussian", "gaussian", "gaussian"), nitt =
> 100000, burnin = 5000,
> thin = 25, data = dataset)
>
> but I get the following error message:
>
> Error in `[.data.frame`(data, , components[[1]]) :
> undefined columns selected
>
> ##############################
>
> So, my first question is: What would be the syntax for this kind of
> nested structure (replicate nested within treatment) in MCMCglmm?
>
> My second question is: What would be the syntax if there was another
> level of variance related to this nested structure? I'm thinking of the
> situation where there would be a sort of treatment nesting within each
> replicate (e.g. if each of the four pairs of replicates was kept in a
> different incubator, or if each pair had been done at a different moment
> in time, etc.). Would you just add a 'replicate' random effect in the
> model? How?
>
> Any help with this would be fantastic.
>
> Thanks a lot in advance.
>
> Al
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Allan Debelle - PhD
Research Fellow at University of Sussex <http://www.sussex.ac.uk/>
Phone: +44 (0)748 095 3874 <callto:415-526-2339>
Web: sites.google.com/site/allandebelle
LinkedIn: fr.linkedin.com/in/allandebelle



From paul.debes at utu.fi  Fri Mar  4 20:54:06 2016
From: paul.debes at utu.fi (Paul Debes)
Date: Fri, 04 Mar 2016 21:54:06 +0200
Subject: [R-sig-ME] Syntax for a multivariate & multilevel MCMCglmm model
In-Reply-To: <56D9BF25.5030805@sussex.ac.uk>
References: <56D98DEB.9090903@sussex.ac.uk> <op.yds2k3opsgx3xe@armadillo50>
	<56D9BF25.5030805@sussex.ac.uk>
Message-ID: <op.ydtf8gahsgx3xe@armadillo50>

Hi Allan,

Maybe take a look at the residuals, if they show independence and a lovely  
distribution for each trait (or even across traits after standardisation)  
when plotted the "us(trait):units" works for you. You assume each trait  
has a different residual variance and residuals are correlated between  
trait pairs within individuals. You can, of course, relax this or make  
this more complicated - only model diagnostics and comparisons can tell  
you.

I don't dare giving advice on priors (being completely clueless myself  
once correlations are involved), but the "course notes" by Jarrod Hadfield  
are extremely informative on many modeling matters and also include  
information about covariance structures and priors. You can find them here:

https://cran.r-project.org/web/packages/MCMCglmm/vignettes/CourseNotes.pdf

Best,
Paul



On Fri, 04 Mar 2016 19:00:21 +0200, Allan Debelle <a.debelle at sussex.ac.uk>  
wrote:

> Hi Paul,
>
> Thank you very much for taking the time to have a look at this.
> No need to modify the residual variance structure then? Just "rcov = ~  
> us(trait):units" ? And I don't need to change the syntax of the prior?
>
> Thank you.
>
> Al
>>> 	>>Paul Debes	>>4 March 2016 14:59

>> Hi Allan,
>> This model (explicitly nesting random replicates within treatments):
>> fixed = var ~ 1 + treatment + cov1 + cov2,random = ~  
>> treatment:replicate,rcov = ~ units
>> expanded to a multivariate model with unstructured covariances in G and  
>> R may be (you missed the "trait" interaction in the fixed >>statement  
>> of your first model):
>> fixed = cbind(varA, varB, varC, varD) ~ trait + trait:treatment +  
>> trait:cov1 + trait:cov2,random = ~ us(trait):treatment:replicate,rcov =  
>> ~ us(trait):units
>> Adding the trait interactions probably gets you back the treatment  
>> effects you did see in the univariate models.
>> How you add an additional random term, nested within replicates, will  
>> depend on how your random treatment:replicate effects >>are correlated  
>> between traits (e.g., "us" above may be reduced to "diag" if not  
>> correlated) and how the sub-replicates are >>correlated with the  
>> replicates and the traits. Plotting random effects and nested model  
>> testing may be required to find a useful >>solution.
>> Paul
>>
>> On Fri, 04 Mar 2016 15:30:19 +0200, Allan Debelle  
>> <a.debelle at sussex.ac.uk> wrote:
>>
>>>> 	>>Allan Debelle	>>4 March 2016 13:30

>> Hello everyone,
>>
>> I am trying to analyse a dataset using a multivariate MCMCglmm model, 
>> but I cannot solve a syntax problem related to the structure of my  
>> dataset.
>>
>> Basically, 50 individuals were exposed to either an A or a B treatment, 
>> and then 4 traits were measured on each individual, as well as 2 
>> covariates. This experiment was replicated 4 times, meaning that I have 
>> 4 replicates (rep1, rep2, rep3 and rep4) for each of the two treatments 
>> (A and B).
>>
>> The dataset looks something like this, with 'treat_rep' just being a 
>> unique identifier for each combination of treatment x replicate (I 
>> simplified it and generated random numbers, just FYI):
>>
>> varA varB varC varD cov1 cov2 treatment replicatetreat_rep
>>
>> 0.109488619 0.675591081 0.940580782 0.3669807360.911079042 0.468285642  
>> A rep1 A1
>> 0.400887809 0.43162503 0.823351715 0.76152362 0.0032215530.622398329 A  
>> rep2 A2
>> 0.176948608 0.074676865 0.91156597 0.7476630210.028740661 0.856395358 A  
>> rep3 A3
>> 0.16468968 0.776755434 0.367321135 0.8863521010.083174005 0.246884218 A  
>> rep4 A4
>> 0.090596435 0.785823554 0.061103075 0.8944361440.989249957 0.50533554 B  
>> rep1 B1
>> 0.839349822 0.639784216 0.293986243 0.558128180.307394708 0.036590982 B  
>> rep2 B2
>> 0.711702334 0.174145468 0.634296876 0.4421127730.480754889 0.863199351  
>> B rep3 B3
>> 0.052352675 0.492622311 0.668647151 0.6832434550.958397833 0.857768988  
>> B rep4 B4
>> ...
>>
>> ##############################
>>
>> What I want to do is to test for the effect of the treatment on the 
>> traits I measured, while taking into account the effects of my 
>> covariates, so something like this:
>>
>> varA + varB +varC +varD ~ treatment + cov1 + cov2
>>
>> The problem is that I have to deal with a nested data structure. Each 
>> individual belongs to a replicate of a treatment (e.g. rep1 of treatment 
>> A, or rep3 of treatment B, etc.), and I am not sure how to specify this 
>> correctly in MCMCglmm. I tried a few things, but something is wrong 
>> either in the prior specification or/and in my specification of the 
>> variance structure of the random effects (and potentially of the 
>> residuals too).
>>
>> ##############################
>>
>> Among other things, I tried to use the treat_rep variable as a grouping 
>> factor:
>>
>> prior<- list(R=list(V=diag(4),nu=4),G=list(G1=list(V=diag(4),nu=4)))
>>
>> model <- MCMCglmm(
>>
>> fixed = cbind(varA, varB, varC, varD) ~ -1 + treatment + cov1 + cov2,
>>
>> random = ~ us(trait):treat_rep,
>>
>> rcov = ~ us(trait):units,
>>
>> prior = prior,
>>
>> family = c("gaussian", "gaussian", "gaussian", "gaussian"), nitt = 
>> 100000, burnin = 5000,
>>
>> thin = 25, data = dataset)
>>
>> However, and unless I am wrong, it is not nesting 'replicate' within 
>> 'treatment'. Which means I end up with no effect of 'treatment', whereas 
>> I do have this effect when I run independent models using lme4 for each 
>> of the 4 traits (and in that case the nesting syntax is more 
>> straightforward).
>>
>> ##############################
>>
>> I also tried this:
>>
>> prior<-list(R=list(V=diag(16),nu=16),G=list(G1=list(V=diag(8),nu=8))) #  
>> 4traits x 2 treatments x 4 replicates // 4 traits x 2 treatments
>>
>> model <- MCMCglmm(
>>
>> fixed = cbind(varA, varB, varC, varD) ~ -1 + treatment + cov1 + cov2,
>>
>> random = ~ us(trait:treatment):replicate,
>>
>> rcov = ~ us(trait:treatment:replicate):units,
>>
>> prior = prior,
>> family = c("gaussian", "gaussian", "gaussian", "gaussian"), nitt = 
>> 100000, burnin = 5000,
>> thin = 25, data = dataset)
>>
>> but I get the following error message:
>>
>> Error in `[.data.frame`(data, , components[[1]]) :
>> undefined columns selected
>>
>> ##############################
>>
>> So, my first question is: What would be the syntax for this kind of 
>> nested structure (replicate nested within treatment) in MCMCglmm?
>>
>> My second question is: What would be the syntax if there was another 
>> level of variance related to this nested structure? I'm thinking of the 
>> situation where there would be a sort of treatment nesting within each 
>> replicate (e.g. if each of the four pairs of replicates was kept in a 
>> different incubator, or if each pair had been done at a different moment 
>> in time, etc.). Would you just add a 'replicate' random effect in the 
>> model? How?
>>
>> Any help with this would be fantastic.
>>
>> Thanks a lot in advance.
>>
>> Al
>>
>> [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> --Allan Debelle - PhDResearch Fellow at University of Sussex
> Phone: +44 (0)748 095 3874
> Web: sites.google.com/site/allandebelle
> LinkedIn: fr.linkedin.com/in/allandebelle
>
>



-- 
Paul V. Debes
DFG Research Fellow
University of Turku
Department of Biology
Finland

Email: paul.debes at utu.fi

From selebatsom at yahoo.co.uk  Sun Mar  6 22:43:54 2016
From: selebatsom at yahoo.co.uk (moses selebatso)
Date: Sun, 6 Mar 2016 21:43:54 +0000 (UTC)
Subject: [R-sig-ME] Failure to converge with glmer on gamma data
References: <159358923.6911237.1457300634490.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <159358923.6911237.1457300634490.JavaMail.yahoo@mail.yahoo.com>

Hello
I m trying to determine the effects of season and habitat on forage protein content. With my replica nested within location. 
model<-glmer(Protein~Habitat*Season + (1|Location/Replica), family=Gamma)
I am getting a warning when running my model. I have tried read on previous post and most?of the time?I get lost because I?fairly new to R.?I?understand there is a maximum "maxlgradl" of 0.001 that should be accepted in this kind of warnings. However, mine?is a?little higher?than that. Can someone advise how I can?proceed here and get an acceptable?output.
I also tried using "....family =Gamma(log)"? instead of just "....family = Gamma" and it doesn't give me the warning. Can this be an acceptable option? 
Model and output below. I can provide data sample if?needed.
> model<-glmer(Protein~Habitat*Season + (1|Location/Replica), family=Gamma)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,? :
? Model failed to converge with max|grad| = 0.00123611 (tol = 0.001, component 1)
> model2<-glmer(Protein~Habitat+Season + (1|Location/Replica), family=Gamma)
Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,? :
? Model failed to converge with max|grad| = 0.00254076 (tol = 0.001, component 1)


?Thank you

Moses SELEBATSO Home:?? ?(+267) 318 5219 (H)??Mobile: ?(+267) 716 39370? or? (+267) 738 39370
	[[alternative HTML version deleted]]


From daveauty at gmail.com  Sun Mar  6 23:16:02 2016
From: daveauty at gmail.com (Dave Auty)
Date: Sun, 6 Mar 2016 15:16:02 -0700
Subject: [R-sig-ME] predict() fails on new data for nlme model object
Message-ID: <CAEEcED7khCe+N=CrcD0vu76mzYBjSWzEbditBUOVdQ9wq0ydZg@mail.gmail.com>

Dear R-Mixed-Modellers,

I've come across some unusual behavior when using the predict function in
nlme_3.1-125. If I allow fixed effects parameters to vary by levels of a
categorical variable, the predict function fails on new data. I made a
reproducible example in the hope that someone might know what's going on:

#########
library(ggplot2)
library(dplyr)
library(nlme)

# Simulate density data with some random noise to fit Michaelis-Menten
equation as function of age. TreeIDs grouped by SP (spacing)
set.seed(1)

df <- data.frame(SP = rep(LETTERS[1:5], 250), expand.grid(TreeID =
factor(1:25), age = seq(1, 50, 1))) %>%
            group_by(TreeID) %>%
            mutate(dens = ((runif(1,10,20)*age)/(runif(1,9,10)+age)) +
rnorm(50, 0, 1))

# plot data by SP treatment:
ggplot(df) +
    geom_line(aes(age, dens, group=TreeID)) +
    facet_wrap(~SP)

# fit mixed-effects model:
fit1 <-nlme(dens ~ a*age/(b+age),
                        fixed=a+b~1,
                        start=c(a=15, b=5),
                        random=a~1|TreeID,
                        data=df)
summary(fit1)

# allow fixed effects parameters to vary by SP:
fit2 <-nlme(dens ~ a*age/(b+age),
                        fixed=list(a~SP, b~SP),
                        start=c(a=rep(14, 5), b=rep(4, 5)),
                        random=a~1|TreeID,
                        data=df)
summary(fit2)

# make new data for predictions:
newdat2 <- expand.grid(SP = LETTERS[1:5], age = seq(1, 50, 1))

df$newpred1 <- predict(fit1, newdat2, level=0) # works fine
df$newpred2 <- predict(fit2, newdat2, level=0) # throws the error:

Error in eval(expr, envir, enclos) : object 'SP' not found

This worked fine in previous versions of nlme. I'd appreciate any insight
into what might be happening here.

Many thanks.

Dave Auty, PhD.
Assistant Professor of Wood Science and Utilization
School of Forestry
Northern Arizona University


sessionInfo()
# R version 3.2.3 (2015-12-10)
# Platform: x86_64-w64-mingw32/x64 (64-bit)
# Running under: Windows >= 8 x64 (build 9200)
#
# locale:
# [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
States.1252    LC_MONETARY=English_United States.1252
# [4] LC_NUMERIC=C                           LC_TIME=English_United
States.1252
#
# attached base packages:
# [1] stats     graphics  grDevices utils     datasets  methods   base
#
# other attached packages:
# [1] dplyr_0.4.3   ggplot2_2.0.0 nlme_3.1-124
#
# loaded via a namespace (and not attached):
#     [1] Rcpp_0.12.3      lattice_0.20-33  assertthat_0.1
grid_3.2.3       plyr_1.8.3       R6_2.1.2         gtable_0.2.0
#   [8] DBI_0.3.1        magrittr_1.5     scales_0.4.0     lazyeval_0.1.10
labeling_0.3     tools_3.2.3      munsell_0.4.3
#  [15] parallel_3.2.3   colorspace_1.2-6

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Mon Mar  7 02:55:58 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 6 Mar 2016 20:55:58 -0500
Subject: [R-sig-ME] Failure to converge with glmer on gamma data
In-Reply-To: <159358923.6911237.1457300634490.JavaMail.yahoo@mail.yahoo.com>
References: <159358923.6911237.1457300634490.JavaMail.yahoo.ref@mail.yahoo.com>
	<159358923.6911237.1457300634490.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CABghstQc+awKuAavXXMgGZ7CyQgGkac_pdxxtK1gh9VboLa0Qw@mail.gmail.com>

  Short answers:

(1) the decision whether to use a log or an inverse link function is
at root a scientific one (i.e., is one functional form or the other
more sensible for your problem?), although it is generally the case
that log links are more stable.  You could also use AIC or
log-likelihood to choose among links if you wanted.
(2) I would say that 0.0025 is *probably* acceptable, although it
would be best to try a different optimizer and see if you get similar
results; have you read the ?convergence manual page?

On Sun, Mar 6, 2016 at 4:43 PM, moses selebatso <selebatsom at yahoo.co.uk> wrote:
> Hello
> I m trying to determine the effects of season and habitat on forage protein content. With my replica nested within location.
> model<-glmer(Protein~Habitat*Season + (1|Location/Replica), family=Gamma)
> I am getting a warning when running my model. I have tried read on previous post and most of the time I get lost because I fairly new to R. I understand there is a maximum "maxlgradl" of 0.001 that should be accepted in this kind of warnings. However, mine is a little higher than that. Can someone advise how I can proceed here and get an acceptable output.
> I also tried using "....family =Gamma(log)"  instead of just "....family = Gamma" and it doesn't give me the warning. Can this be an acceptable option?
> Model and output below. I can provide data sample if needed.
>> model<-glmer(Protein~Habitat*Season + (1|Location/Replica), family=Gamma)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.00123611 (tol = 0.001, component 1)
>> model2<-glmer(Protein~Habitat+Season + (1|Location/Replica), family=Gamma)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 0.00254076 (tol = 0.001, component 1)
>
>
>  Thank you
>
> Moses SELEBATSO Home:    (+267) 318 5219 (H)  Mobile:  (+267) 716 39370  or  (+267) 738 39370


From selebatsom at yahoo.co.uk  Mon Mar  7 07:53:30 2016
From: selebatsom at yahoo.co.uk (moses selebatso)
Date: Mon, 7 Mar 2016 06:53:30 +0000 (UTC)
Subject: [R-sig-ME] Failure to converge with glmer on gamma data
In-Reply-To: <CABghstQc+awKuAavXXMgGZ7CyQgGkac_pdxxtK1gh9VboLa0Qw@mail.gmail.com>
References: <159358923.6911237.1457300634490.JavaMail.yahoo.ref@mail.yahoo.com>
	<159358923.6911237.1457300634490.JavaMail.yahoo@mail.yahoo.com>
	<CABghstQc+awKuAavXXMgGZ7CyQgGkac_pdxxtK1gh9VboLa0Qw@mail.gmail.com>
Message-ID: <1116194329.7107656.1457333610058.JavaMail.yahoo@mail.yahoo.com>

Thank you for the response. I have just gone through the convergence manual in?lme4 February 2016. I ran the singularity test?and this is the result
> diag.vals <- getME(model,"theta")[getME(model,"lower") == 0]
> any(diag.vals < 1e-6) # FALSE
[1] FALSE

My interpretation is that there is one (1) diagonal element?that is zero or very small. Is that correct? If?so, the manual says then "...the convergence??testing methods may be inappropriate.." I do not know what this means exactly. Does it mean I should not use the test or there is not need to do tests, and so accept the model? 
Thank you for your support and patience.
,
Moses SELEBATSO Home:?? ?(+267) 318 5219 (H)??Mobile: ?(+267) 716 39370? or? (+267) 738 39370 

    On Monday, 7 March 2016, 3:55, Ben Bolker <bbolker at gmail.com> wrote:
 
 

 ? Short answers:

(1) the decision whether to use a log or an inverse link function is
at root a scientific one (i.e., is one functional form or the other
more sensible for your problem?), although it is generally the case
that log links are more stable.? You could also use AIC or
log-likelihood to choose among links if you wanted.
(2) I would say that 0.0025 is *probably* acceptable, although it
would be best to try a different optimizer and see if you get similar
results; have you read the ?convergence manual page?

On Sun, Mar 6, 2016 at 4:43 PM, moses selebatso <selebatsom at yahoo.co.uk> wrote:
> Hello
> I m trying to determine the effects of season and habitat on forage protein content. With my replica nested within location.
> model<-glmer(Protein~Habitat*Season + (1|Location/Replica), family=Gamma)
> I am getting a warning when running my model. I have tried read on previous post and most of the time I get lost because I fairly new to R. I understand there is a maximum "maxlgradl" of 0.001 that should be accepted in this kind of warnings. However, mine is a little higher than that. Can someone advise how I can proceed here and get an acceptable output.
> I also tried using "....family =Gamma(log)"? instead of just "....family = Gamma" and it doesn't give me the warning. Can this be an acceptable option?
> Model and output below. I can provide data sample if needed.
>> model<-glmer(Protein~Habitat*Season + (1|Location/Replica), family=Gamma)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,? :
>? Model failed to converge with max|grad| = 0.00123611 (tol = 0.001, component 1)
>> model2<-glmer(Protein~Habitat+Season + (1|Location/Replica), family=Gamma)
> Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,? :
>? Model failed to converge with max|grad| = 0.00254076 (tol = 0.001, component 1)
>
>
>? Thank you
>
> Moses SELEBATSO Home:? ? (+267) 318 5219 (H)? Mobile:? (+267) 716 39370? or? (+267) 738 39370

 
  
	[[alternative HTML version deleted]]


From bbolker at gmail.com  Mon Mar  7 22:40:23 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 7 Mar 2016 16:40:23 -0500
Subject: [R-sig-ME] Failure to converge with glmer on gamma data
In-Reply-To: <1116194329.7107656.1457333610058.JavaMail.yahoo@mail.yahoo.com>
References: <159358923.6911237.1457300634490.JavaMail.yahoo.ref@mail.yahoo.com>
	<159358923.6911237.1457300634490.JavaMail.yahoo@mail.yahoo.com>
	<CABghstQc+awKuAavXXMgGZ7CyQgGkac_pdxxtK1gh9VboLa0Qw@mail.gmail.com>
	<1116194329.7107656.1457333610058.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CABghstRoQuMpOkKhKFALCcMxi5-U+SDb4ed0OyqqPYxbatRcLA@mail.gmail.com>

  I think you've got this backwards:  FALSE means that the model
*isn't* singular (please re-read the docs, and if you think they're
unclear or wrong, post an issue at
http://github.com/lme4/lme4/issues).  So your model doesn't have that
particular problem.  (It's also fairly easy to identify singularity
for models with only scalar or 2-dimensional random effects -- just
look for zero variances or +/- 1 correlations.)


On Mon, Mar 7, 2016 at 1:53 AM, moses selebatso <selebatsom at yahoo.co.uk> wrote:
> Thank you for the response. I have just gone through the convergence manual
> in lme4 February 2016. I ran the singularity test and this is the result
>
>> diag.vals <- getME(model,"theta")[getME(model,"lower") == 0]
>> any(diag.vals < 1e-6) # FALSE
> [1] FALSE
>
> My interpretation is that there is one (1) diagonal element that is zero or
> very small. Is that correct? If so, the manual says then "...the convergence
> testing methods may be inappropriate.." I do not know what this means
> exactly. Does it mean I should not use the test or there is not need to do
> tests, and so accept the model?
>
> Thank you for your support and patience.
>
> ,
> Moses SELEBATSO
> Home:    (+267) 318 5219 (H)
>  Mobile:  (+267) 716 39370  or  (+267) 738 39370
>
>
> On Monday, 7 March 2016, 3:55, Ben Bolker <bbolker at gmail.com> wrote:
>
>
>
>   Short answers:
>
> (1) the decision whether to use a log or an inverse link function is
> at root a scientific one (i.e., is one functional form or the other
> more sensible for your problem?), although it is generally the case
> that log links are more stable.  You could also use AIC or
> log-likelihood to choose among links if you wanted.
> (2) I would say that 0.0025 is *probably* acceptable, although it
> would be best to try a different optimizer and see if you get similar
> results; have you read the ?convergence manual page?
>
> On Sun, Mar 6, 2016 at 4:43 PM, moses selebatso <selebatsom at yahoo.co.uk>
> wrote:
>> Hello
>> I m trying to determine the effects of season and habitat on forage
>> protein content. With my replica nested within location.
>> model<-glmer(Protein~Habitat*Season + (1|Location/Replica), family=Gamma)
>> I am getting a warning when running my model. I have tried read on
>> previous post and most of the time I get lost because I fairly new to R. I
>> understand there is a maximum "maxlgradl" of 0.001 that should be accepted
>> in this kind of warnings. However, mine is a little higher than that. Can
>> someone advise how I can proceed here and get an acceptable output.
>> I also tried using "....family =Gamma(log)"  instead of just "....family =
>> Gamma" and it doesn't give me the warning. Can this be an acceptable option?
>> Model and output below. I can provide data sample if needed.
>>> model<-glmer(Protein~Habitat*Season + (1|Location/Replica), family=Gamma)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>  Model failed to converge with max|grad| = 0.00123611 (tol = 0.001,
>> component 1)
>>> model2<-glmer(Protein~Habitat+Season + (1|Location/Replica),
>>> family=Gamma)
>> Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>  Model failed to converge with max|grad| = 0.00254076 (tol = 0.001,
>> component 1)
>>
>>
>>  Thank you
>>
>> Moses SELEBATSO Home:    (+267) 318 5219 (H)  Mobile:  (+267) 716 39370
>> or  (+267) 738 39370
>
>


From marion.germain at univ-lyon1.fr  Fri Mar 11 10:29:32 2016
From: marion.germain at univ-lyon1.fr (GERMAIN MARION)
Date: Fri, 11 Mar 2016 09:29:32 +0000
Subject: [R-sig-ME] Zero inflated negative binomial mixed models - glmmADMB
	help
Message-ID: <8C0F41F0-07A6-46D2-9A7A-BD1CBA57922F@univ-lyon1.fr>

Dear all,
I am trying to fit zero inflated negative binomial models (using the package glmmADMB) but with no success.
My response variable is the lifetime reproductive success of a bird species.
The explanatory variable are sex (M vs. F), natal dispersal status (D vs. P), age (Y vs. O), brood size manipulation status (three levels) and the one order interaction between natal dispersal status and brood size manipulation as fixed effects. I also include a random effect which is the first year of breeding for a bird.

I have several errors messages such as:
Parameters were estimated, but standard errors were not: the most likely problem is that the curvature at MLE was zero or negative
Erreur dans glmmadmb(LRS ~ sex + nataldisp + agecat + manipcat.control +  :
  The function maximizer failed (couldn't find parameter file) Troubleshooting steps include (1) run with 'save.dir' set and inspect output files; (2) change run parameters: see '?admbControl';(3) re-run with debug=TRUE for more information on failure mode
Furthermore : Warning message :
l'ex?cution de la commande './glmmadmb -maxfn 500 -maxph 5' renvoie un statut 1

I have been trying several solutions you already suggested such as:
- admb.opts=admbControl(shess=FALSE,noinit=FALSE)
- admb.opts=admbControl(shess=FALSE,noinit=FALSE, impSamp=200,maxfn=1000,imaxfn=500,maxph=5)

I also tried to downgrading the package to an earlier version (0.7) but I did not succeed. I followed your instruction (found here http://lists.admb-project.org/pipermail/users/2012-January/001667.html) but it doesn?t work.

I therefore have several questions. First, how could I know if I need to use a zero inflated model (until now, I used negative binomial mixed models). Second, do you have an idea of the reason of this error message ? I tried a zero inflated with a poisson distribution and I do not have any error message. I was wondering if the model could be overparametrized (n = 827). Finally, what could I do to resolve these problems ?
Thank you for helping,
Best
Marion


	[[alternative HTML version deleted]]


From ecp52 at cornell.edu  Sat Mar  5 17:20:00 2016
From: ecp52 at cornell.edu (Evan Palmer-Young)
Date: Sat, 5 Mar 2016 16:20:00 +0000 (UTC)
Subject: [R-sig-ME] LSmean: transformed error bars
Message-ID: <loom.20160305T171127-181@post.gmane.org>

Dear R gurus,
I am using the "lsmeans" function to extract predicted marginal means from 
a Poisson family mixed model with a log link function. When I ask for the 
"lsmeans" on the scale of the linear predictor, the standard errors and 
confidence limits are symmetrical about the mean. 
However, I would like to plot the responses on the original scale, so that 
I can depict "number of eggs" rather than "ln(number of eggs)". 

The function "lsmeans" offers this reverse transformation via the argument 
"type='response'". However, I don't understand why there is only one 
standard error after the response. When I manually exponentiate the 
predicted means and CI's from the scale of the linear predictor to the 
scale of the original response variable, my calculations match perfectly. 
However, when I exponentiate the "mean + se" and "mean - se" for plotting 
on the original scale, I get asymmetric bars.

So in lsmeans, what does the "SE" mean when using "type="response""? 
Shouldn't there be separate + and - SE values? 

The SE value seems to be very close to (upperCI-lowerCI)/4, but not 
exactly. Should I plot mean +/- SE using "type="response"" for symmetric 
error bars, or use my manually exponentiated mean +/-SE?

Thanks for your help!
Evan


From davef at otter-rsch.com  Fri Mar 11 18:14:26 2016
From: davef at otter-rsch.com (dave fournier)
Date: Fri, 11 Mar 2016 09:14:26 -0800
Subject: [R-sig-ME] Zero inflated negative binomial mixed models -
 glmmADMB help
In-Reply-To: <8C0F41F0-07A6-46D2-9A7A-BD1CBA57922F@univ-lyon1.fr>
References: <8C0F41F0-07A6-46D2-9A7A-BD1CBA57922F@univ-lyon1.fr>
Message-ID: <56E2FCF2.2030608@otter-rsch.com>

I like to use to save.dir option of glmmadmb and run models from the 
command line
using the file glmmadmb.dat and glmmadmb.pin.

If you run the model from the command line and look at the
eigenvalues of the Hessian in the file glmmadmb.eva you will see
     unsorted:     0.8299209075 0.8342404973 0.7590373248 0.7698518484 
0.7273714834 0.8736779466 0.6861389165 0.6374567379 0.9724466982 
-4.668064218e-06 0.004088060076 0.001527386956 0.2224415205
       sorted:     -4.668064218e-06 0.001527386956 0.004088060076 
0.2224415205 0.6374567379 0.6861389165 0.7273714834 0.7590373248 
0.7698518484 0.8299209075 0.8342404973 0.8736779466 0.9724466982

So there is one (slightly) negative eigenvalue.
there are a number of things one can try such as restarting the model 
from the
final estimates with

    ./glmmadmb -maxph 5 -phase 5 -binp glmmadmb.bar

but this does not solve the problem. It turns out that running the model
from the beginning with a stricter convergence criterion does produce
a positive definite Hessian.

  ./glmmadmb -maxph 5  -crit 1.e-8

gives a glmmadmb.eva file with
     unsorted:     0.7502190194 0.7626161871 0.7216672407 0.8250394871 
0.8315459284 0.8631699305 0.6817678360 0.6341003013 0.9476416352 
0.2247847266 9.978823284e-06 0.001435942771 0.004023239570
       sorted:     9.978823284e-06 0.001435942771 0.004023239570 
0.2247847266 0.6341003013 0.6817678360 0.7216672407 0.7502190194 
0.7626161871 0.8250394871 0.8315459284 0.8631699305 0.9476416352


adding the option -eigvec includes the eigenvectors of the Hessian.

  ./glmmadmb -maxph 5 -crit 1.e-8 -eigvec

then the eva file looks like
     unsorted:     0.7502190194 0.7626161871 0.7216672407 0.8250394871 
0.8315459284 0.8631699305 0.6817678360 0.6341003013 0.9476416352 
0.2247847266 9.978823284e-06 0.001435942771 0.004023239570
    0.7502190194   0.7626161871   0.7216672407   0.8250394871 
0.8315459284   0.8631699305   0.6817678360   0.6341003013 0.9476416352   
0.2247847266 9.978823284e-06 0.001435942771 0.004023239570
  3.145751248e-05 -0.02229426701  -0.6439166902   0.4165291489 
-0.2349975122   0.3378538346  -0.3627220631  -0.1375767378 0.2092747212  
-0.2021974178 -0.08252179794 0.004892490007 0.0007365404429
  -0.0001627581714 -0.02292054239  -0.2533111690   0.3418924618 
0.03267828005  -0.7301316450   0.1209804739   0.4006587038 
-0.05379681904  -0.1784520056  -0.2726715930 0.0006368161115 0.002106698598
  6.905271438e-05 -0.01094136597  -0.1616308706  -0.1170460172 
0.7988984662  0.06758963263  0.02241602484  -0.1049373653 0.02046180512  
-0.5386081221   0.1233033278 0.003460024640 0.001204016299
  0.0003480524365 0.003719457042   0.1111906466   0.1428914105 
-0.08189325289  -0.4118421907   0.1726200229  -0.7685910283 0.4039533028 
-0.04965863296  0.06856538114 0.002760235874 -0.001145571029
  0.0003922122329 -0.001039509669   0.2633796300  0.01139584203 
-0.4343527695  0.06770277471   0.1747624011   0.2663334861 0.1867854796  
-0.6627218740   0.4021039370 0.0007643489613 -0.001181539429
  -0.0001962079647 -0.006193603101 -0.03602279601  -0.3456553725 
-0.2750993786  -0.1978416834  -0.3326903098  -0.3278134635 
-0.6216010012  -0.3601442675  -0.1735738092 0.0005875992569 0.0009502102918
  -1.801921268e-05  0.02586284264   0.5897627612   0.2877836172 
0.1046648012   0.1632217687  -0.2925115862  0.01356523316 0.1518946287  
-0.2138105138  -0.6136230469 0.005151299186 -0.001335903862
  5.543672656e-05  0.05185388996 -0.006611299676   0.4170181174 
-0.02602451139   0.3035901652   0.6582827082  -0.1964552396 
-0.4819848060 -0.07565901091  -0.1433677854 -0.0003094013723 
-0.0004959787966
  -0.0009915469361  0.03224097595   0.2492459149   0.5458924492 
0.1508893953  -0.1243502158  -0.4036042697 -0.03639258080 
-0.3348481523   0.1191998766   0.5568003123 0.004954228709 0.001733212413
  0.008029938584  -0.9971258920  0.04538382883  0.03359505616 
0.003830672944  0.02411983117  0.02119394632 -0.01714560983 
-0.03042769214  0.01163740729 0.002302755985 -0.003233574812 0.008398091508
    0.9928784299 0.006992077633 -0.0004539774337 0.0003402247550 
0.0003997625959 -0.0005470094690 -0.0009410663612 0.0003558416009 
-0.0006377696486 0.0002107506719 0.0003414248073 -0.006399120457 
-0.1187443667
   -0.1177436265 -0.009601421062 -0.001660259044 0.0004870104068 
0.001146729502 -0.001348434808 -0.0006452870809 0.0009410093944 
-0.002049145507 0.0002844653223 0.0006418773714  0.08304885626 -0.9895125153
   0.01621880363 -0.002512232778 -0.0006114832900 -0.006059549890 
-0.002298758632 -0.0001654826189 0.005049402423 0.002819847138 
-0.001161996960 0.004339845308 7.078884217e-05   0.9964714971 0.08172608698

where I have deleted some lines so that only the unsorted eigenvalues and
the eigenvectors (in the same order) are included.  The small
eigenvalue is third from the last so that corresponding eigenvector is
    0.9928784299 0.006992077633 -0.0004539774337 0.0003402247550 
0.0003997625959 -0.0005470094690 -0.0009410663612 0.0003558416009 
-0.0006377696486 0.0002107506719 0.0003414248073 -0.006399120457 
-0.1187443667

Note that is is almost entirely the first component.  check the par file,
glmmadmb.par
# Number of parameters = 13  Objective function value = 1183.16 Maximum 
gradient component = 1.55143e-07
# pz:
0.0267768011007
# beta:
  3.80217042534 2.99365228295 2.14620977832 0.693168837684 1.48188509460 
1.41704108724 -1.20474431384 -2.72872529388 1.22958281355 0.907826960877
# tmpL:
  -1.27434816223
# tmpL1:
  0.000100000000000
# log_alpha:
0.365496270825
# u:
  0.883421752157 0.725093568164 0.515818718059 -0.252695513091 
0.376963076134 0.139381634514 1.50020943187 0.277631810913 
0.130374031442 0.289427299208 0.581103164117 0.174844539661 
-0.322991242902 -0.119327510643 0.967346420700 1.31812402912 
-0.502539361904 -1.03457222137 -1.14964617640 -0.0617951773726 
-0.866160795582 -1.16221633966 -1.65553226247

and you will see that it corresponds to the first parameter pz which is the
zero inflation parameter.  So it is saying that the model response to the
zero inflation is very small. and slightly confounded with the last 
parameter,
which is the overdispersion parameter.  This makes sense as it is saying 
that
the model can account for extra zeroes either my adding zero inflation or
by changing the overdispersion a bit.

Now lets turn off zero inflation and fit the model.
the par file is

# Number of parameters = 12  Objective function value = 1183.21 Maximum 
gradient component = 1.43850e-07
# pz:
0.0200000000000
# beta:
  3.06300647659 3.03459572371 2.10955237321 0.651987307491 1.53607380969 
1.51288805518 -1.24056997621 -2.65942119162 1.20265939317 0.869632910773
# tmpL:
  -1.26543720173
# tmpL1:
  0.000100000000000
# log_alpha:
0.393160445395
# u:
  0.921344073078 0.774702834115 0.492811643199 -0.243814794142 
0.387638751447 0.139004323059 1.50021679322 0.271472152608 
0.130845912349 0.258548576465 0.585278999357 0.157099308388 
-0.345577809207 -0.130590600526 0.977640503202 1.32244233265 
-0.485656349901 -1.03509274755 -1.14782075617 -0.0389045698289 
-0.878875283217 -1.16385512772 -1.64533137464

So comparing the log-likelihoods (minus Objective function values)
one sees that the difference is only 0.05, so that zero inflation is
not significant.

# Number of parameters = 13  Objective function value = 1183.16 Maximum 
gradient component = 1.55143e-07
# pz:
# Number of parameters = 12  Objective function value = 1183.21 Maximum 
gradient component = 1.43850e-07


From annieclaude.letendre at gmail.com  Fri Mar 11 22:59:25 2016
From: annieclaude.letendre at gmail.com (Annie-Claude Letendre)
Date: Fri, 11 Mar 2016 13:59:25 -0800
Subject: [R-sig-ME] glth function with lme returns error Error in
 contrMat(table(mf[[nm]]), type = types[pm]) : less than two groups
Message-ID: <CAPZCg-um8St1osxT_NK1gH+Q2XtX3r2KJf63WLyJRvtXMA3r0w@mail.gmail.com>

Hi,

I am trying to run a *posthoc* on a lme with the glht function and I get
this error that I can't figure out "Error in contrMat(table(mf[[nm]]), type
= types[pm]) :
  less than two groups"

My experiment had 45 indivuduals (rep) each of these had 1 treatment
(treat) applied to it and then the variable (logcov) was measured twice for
each individual at week 6 and 12.

I've attached the csv I am using , thanks for any help you can offer !
Is there any other posthoc test that could work for me ?

Annie

>
spl<-read.table("C:/Users/Annie/Desktop/splcover.csv",header=T,sep=",",quote="")
> attach(spl)
> names(spl)
 [1] "rep"     "treat"  "logcov"  "week"


> rep<-factor(rep)
> treat<-factor(treat)
> week<-factor(week)
> levels(treat)
[1] "c" "f" "g" "p" "r"
> levels(week)
[1] "6"  "12"
> levels(rep)
 [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10" "11" "12" "13" "14"
"15"
[16] "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29"
"30"
[31] "31" "32" "33" "34" "35" "36" "37" "38" "39" "40" "41" "42" "43" "44"
"45"

> library(nlme)
Warning message:
package ?nlme? was built under R version 3.1.3

> summary(modcover<-lme(logcov~treat*week, random= ~1|rep/week))
Linear mixed-effects model fit by REML
 Data: NULL
        AIC       BIC   logLik
  -33.51971 -2.553367 29.75986

Random effects:
 Formula: ~1 | rep
        (Intercept)
StdDev:  0.09534288

 Formula: ~1 | week %in% rep
        (Intercept)   Residual
StdDev:   0.1095635 0.04379889

Fixed effects: logcov ~ treat * week
                   Value  Std.Error DF   t-value p-value
(Intercept)   -2.0000000 0.05056654 40 -39.55185  0.0000
treatf         1.5944444 0.07151188 40  22.29622  0.0000
treatg         0.0000000 0.07151188 40   0.00000  1.0000
treatp         1.3944444 0.07151188 40  19.49948  0.0000
treatr         1.5744444 0.07151188 40  22.01654  0.0000
week12         0.1400000 0.05562275 40   2.51696  0.0159
treatf:week12 -0.0266667 0.07866245 40  -0.33900  0.7364
treatg:week12  0.1888889 0.07866245 40   2.40126  0.0211
treatp:week12 -0.0222222 0.07866245 40  -0.28250  0.7790
treatr:week12  0.0077778 0.07866245 40   0.09888  0.9217
 Correlation:
              (Intr) treatf treatg treatp treatr week12 trtf:12 trtg:12
trtp:12
treatf        -0.707

treatg        -0.707  0.500

treatp        -0.707  0.500  0.500

treatr        -0.707  0.500  0.500  0.500

week12        -0.550  0.389  0.389  0.389  0.389

treatf:week12  0.389 -0.550 -0.275 -0.275 -0.275 -0.707

treatg:week12  0.389 -0.275 -0.550 -0.275 -0.275 -0.707  0.500

treatp:week12  0.389 -0.275 -0.275 -0.550 -0.275 -0.707  0.500   0.500

treatr:week12  0.389 -0.275 -0.275 -0.275 -0.550 -0.707  0.500   0.500
0.500

Standardized Within-Group Residuals:
         Min           Q1          Med           Q3          Max
-0.741681806 -0.152850434 -0.008438999  0.126023769  0.836897713

Number of Observations: 90
Number of Groups:
          rep week %in% rep
           45            90
> anova(modcover)
            numDF denDF   F-value p-value
(Intercept)     1    40 2817.8201  <.0001
treat           4    40  366.2476  <.0001
week            1    40   46.4611  <.0001
treat:week      4    40    2.6323  0.0483
> require(multcomp)
Loading required package: multcomp
Loading required package: mvtnorm
Loading required package: survival
Loading required package: splines
Loading required package: TH.data


> summary(glht(modcover, linfct=mcp(treat="Tukey")))
Error in contrMat(table(mf[[nm]]), type = types[pm]) :
  less than two groups

> is.factor(treat)
[1] TRUE

> sessionInfo()
R version 3.1.1 (2014-07-10)
Platform: x86_64-w64-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=English_Canada.1252  LC_CTYPE=English_Canada.1252
[3] LC_MONETARY=English_Canada.1252 LC_NUMERIC=C
[5] LC_TIME=English_Canada.1252

attached base packages:
[1] splines   stats     graphics  grDevices utils     datasets  methods
[8] base

other attached packages:
[1] multcomp_1.3-6  TH.data_1.0-3   survival_2.37-7 mvtnorm_1.0-0
[5] nlme_3.1-125

loaded via a namespace (and not attached):
 [1] colorspace_1.2-4 ggplot2_2.0.0    grid_3.1.1       gtable_0.1.2
 [5] lattice_0.20-33  munsell_0.4.2    plyr_1.8.3       Rcpp_0.12.2
 [9] sandwich_2.3-2   scales_0.3.0     zoo_1.7-11

From jbukoski1 at gmail.com  Sun Mar 13 02:33:38 2016
From: jbukoski1 at gmail.com (Jacob Bukoski)
Date: Sat, 12 Mar 2016 20:33:38 -0500
Subject: [R-sig-ME] Non-significant fixed effect allows approximation of
 variance-covariance matrix
Message-ID: <CAOES0Vjh3vEb=w9VaORWOP0MRTgDFuABpoGCSBAfa0WXnH6t6g@mail.gmail.com>

Dear all,

I am using lme() to run a mixed effects model on soil carbon observations,
with fixed effects specified for latitude (continuous), and dominant genera
of tree (factor with five levels), and random effects specified for site.

The data is heteroscedastic, which I can account for well with a varIdent
weights specification; however, when I do so I receive a "non-positive
definite approx. var-covar matrix" output from the call to $apVar.

When I add in a third fixed effect (Geomorphic setting, a factor with three
levels), the variance-covariance matrix is approximated successfully, but
the levels of the added third fixed effect are non significant.

I've been trying to read up on why this might be occurring, but can't for
the life of me figure out why a more complex model (including
non-significant predictors) would allow for the successful approximation of
the variance-covariance matrix.

I'm hoping to use the model for predictive purposes, and ideally would not
include non-significant effects in its final form.

Does anyone have any ideas on why this might be occurring, or
intermediate-level resources per non-positive definite variance-covariance
matrices that I could look into?

Many kind thanks,
Jacob

P.S. If it helps, the model specification is here:

*lme.C.density <- lme(C.density ~ Latitude + Genus + Geomorph,
random=(~1|Site), weights=varIdent(form=~Genus|Site), data=model.c.dens,
method = "REML")*

-- 
Jacob J. Bukoski
Master of Environmental Science Candidate, 2016
School of Forestry and Environmental Studies, Yale University
jbukoski1 at gmail.com | jacob.bukoski at yale.edu | LinkedIn
<https://www.linkedin.com/profile/view?id=AAIAAAdWVW8BMzqU_2EGNbEkyuy8O7K1Jyhd8ps&trk=nav_responsive_tab_profile_pic>

	[[alternative HTML version deleted]]


From alexandre.courtiol at gmail.com  Sun Mar 13 12:22:43 2016
From: alexandre.courtiol at gmail.com (Alexandre Courtiol)
Date: Sun, 13 Mar 2016 12:22:43 +0100
Subject: [R-sig-ME] Covariance matrix specification in MCMCglmm
Message-ID: <CAERMt4egV6NuvzBz4QTchm=hxPAguG75AmXHi95zPEoY9d2ksA@mail.gmail.com>

Dear all,

I am trying to figure out the syntax required to constrain a covariance
matrix in a particular way with MCMCglmm.

I have a multivariate problem, so let's say my response variables are "y1"
and "y2".
I have a fixed effect coding a treatment called "treatment" with "A" or "B".
I want the covariance matrix characterizing individual variation for each
trait.

So I wrote something along:

model <- MCMCglmm(
    fixed = cbind(y1, y2) ~  trait:treatment,
    random = ~ us(trait:treatment):individuals,
    rcov = ~ us(trait):units, ...)

and in the VCV object of the model, I can find something like:

|sigmaA1A1, sigmaA1A2, sigmaA1B1, sigmaA1B2|
|sigmaA2A1, sigmaA2A2, sigmaA2B1, sigmaA2B2|
|sigmaB1A1, sigmaB1A2, sigmaB1B1, sigmaB1B2|
|sigmaB2A1, sigmaB2A2, sigmaB2B1, sigmaB2B2|

which is almost what I want.

The only problem is that my individuals are never in both treatments, so I
would like to constrain the previous matrix to be:

|sigmaA1A1, sigmaA1A2,         0,         0|
|sigmaA2A1, sigmaA2A2,         0,         0|
|        0,         0, sigmaB1B1, sigmaB1B2|
|        0,         0, sigmaB2B1, sigmaB2B2|

instead.

In practice the covariances that I would rather not to estimate are close
to zero, which makes sense, so I could stick to my solution... but I could
gain a bit of extra power if I could constrain those to be exactly zero and
it would look a bit cleaner.

Any idea would be appreciated.

Thanks,

Alex

-- 
Alexandre Courtiol

http://sites.google.com/site/alexandrecourtiol/home

*"Science is the belief in the ignorance of experts"*, R. Feynman

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Sun Mar 13 12:40:41 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sun, 13 Mar 2016 11:40:41 +0000
Subject: [R-sig-ME] Covariance matrix specification in MCMCglmm
In-Reply-To: <CAERMt4egV6NuvzBz4QTchm=hxPAguG75AmXHi95zPEoY9d2ksA@mail.gmail.com>
References: <CAERMt4egV6NuvzBz4QTchm=hxPAguG75AmXHi95zPEoY9d2ksA@mail.gmail.com>
Message-ID: <56E551B9.1090204@ed.ac.uk>

Hi,

random = ~ 
us(trait:at.level(treatment,1)):individuals+us(trait:at.level(treatment,2)):individuals

does what you want.

Cheers,

Jarrod



On 13/03/2016 11:22, Alexandre Courtiol wrote:
> random = ~ us(trait:treatment):individuals,


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bbolker at gmail.com  Sun Mar 13 18:08:49 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 13 Mar 2016 13:08:49 -0400
Subject: [R-sig-ME] Non-significant fixed effect allows approximation of
 variance-covariance matrix
In-Reply-To: <CAOES0Vjh3vEb=w9VaORWOP0MRTgDFuABpoGCSBAfa0WXnH6t6g@mail.gmail.com>
References: <CAOES0Vjh3vEb=w9VaORWOP0MRTgDFuABpoGCSBAfa0WXnH6t6g@mail.gmail.com>
Message-ID: <56E59EA1.50306@gmail.com>

   Hard to say in general without looking at the data in detail. 
Perhaps the Geomorphic information helps explain some of the variation 
in the response that is not otherwise well explained (e.g., leaving out 
the Geomorphic information makes the site-level BLUPs/conditional modes 
less well approximated by a normal distribution).  This is not 
necessarily inconsistent with a non-significant effect overall ...

   I would (1) plot your data to see if there are any outliers or 
patterns that might not be getting picked up by the model; (2) consider 
glmmLasso, if you really want to use the model for prediction; (3) try 
the model in lme4, which is a tiny bit more stable.

   heterogeneous variance models are not built in to lme4; you can make 
it happen for *each* site by adding an observation-level random effect 
that only applies at one level of a site, by adding an observation-level 
factor

   my_data$obs <- factor(seq(nrow(my_data))

and then adding

  +(0+dummy(site,"site2")|obs)

   ... but this is tedious when there are many sites.

   Can you address heteroscedasticity by transformation instead ...?

   How many sites and how many total observations in your data set ... ?


On 16-03-12 08:33 PM, Jacob Bukoski wrote:
> Dear all,
>
> I am using lme() to run a mixed effects model on soil carbon observations,
> with fixed effects specified for latitude (continuous), and dominant genera
> of tree (factor with five levels), and random effects specified for site.
>
> The data is heteroscedastic, which I can account for well with a varIdent
> weights specification; however, when I do so I receive a "non-positive
> definite approx. var-covar matrix" output from the call to $apVar.
>
> When I add in a third fixed effect (Geomorphic setting, a factor with three
> levels), the variance-covariance matrix is approximated successfully, but
> the levels of the added third fixed effect are non significant.
>
> I've been trying to read up on why this might be occurring, but can't for
> the life of me figure out why a more complex model (including
> non-significant predictors) would allow for the successful approximation of
> the variance-covariance matrix.
>
> I'm hoping to use the model for predictive purposes, and ideally would not
> include non-significant effects in its final form.
>
> Does anyone have any ideas on why this might be occurring, or
> intermediate-level resources per non-positive definite variance-covariance
> matrices that I could look into?
>
> Many kind thanks,
> Jacob
>
> P.S. If it helps, the model specification is here:
>
> *lme.C.density <- lme(C.density ~ Latitude + Genus + Geomorph,
> random=(~1|Site), weights=varIdent(form=~Genus|Site), data=model.c.dens,
> method = "REML")*
>


From denis.haine at gmail.com  Sun Mar 13 19:30:40 2016
From: denis.haine at gmail.com (Denis Haine)
Date: Sun, 13 Mar 2016 14:30:40 -0400
Subject: [R-sig-ME] confint.merMod, bootstrap and weights
Message-ID: <CAO1JZaN9A3p_E4D-Vy2wX=GHjbfyHDqUpM7QYoib4NERFVXB=Q@mail.gmail.com>

Hello,

I ran a model as

glmer(y ~ x, family = poisson, data, weights = w)

and then tried to get confidence intervals with the following:

confint(model, method = "boot", parallel = "multicore", ncpus = 4)

However I'm getting the following warning message that I'm not receiving
when using method "Wald" instead of "boot":

Warning message:
In sfun(object, nsim = 1, ftd = rep_len(musim, n * nsim), wts = weights) :
  ignoring prior weights

What's the meaning of this message?

Thanks for your help,

Denis

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Sun Mar 13 20:31:20 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 13 Mar 2016 15:31:20 -0400
Subject: [R-sig-ME] confint.merMod, bootstrap and weights
In-Reply-To: <CAO1JZaN9A3p_E4D-Vy2wX=GHjbfyHDqUpM7QYoib4NERFVXB=Q@mail.gmail.com>
References: <CAO1JZaN9A3p_E4D-Vy2wX=GHjbfyHDqUpM7QYoib4NERFVXB=Q@mail.gmail.com>
Message-ID: <56E5C008.9040401@gmail.com>


   The simulation function (sfun()) that's at the core of the parametric 
bootstrap algorithm is ignoring your specified prior weights.  Poisson 
models with weights are somewhat unusual; what are the weights in your 
model supposed to signify?  If you were simulating the data, how would 
you incorporate the weights in the simulation procedure?

   Ben Bolker


On 16-03-13 02:30 PM, Denis Haine wrote:
> Hello,
>
> I ran a model as
>
> glmer(y ~ x, family = poisson, data, weights = w)
>
> and then tried to get confidence intervals with the following:
>
> confint(model, method = "boot", parallel = "multicore", ncpus = 4)
>
> However I'm getting the following warning message that I'm not receiving
> when using method "Wald" instead of "boot":
>
> Warning message:
> In sfun(object, nsim = 1, ftd = rep_len(musim, n * nsim), wts = weights) :
>    ignoring prior weights
>
> What's the meaning of this message?
>
> Thanks for your help,
>
> Denis
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From alexandre.courtiol at gmail.com  Sun Mar 13 21:55:26 2016
From: alexandre.courtiol at gmail.com (Alexandre Courtiol)
Date: Sun, 13 Mar 2016 21:55:26 +0100
Subject: [R-sig-ME] Extending chains in MCMCglmm?
Message-ID: <CAERMt4eu0jeUYD1=ees_MSnnVfbCzDfObjv8wqMagoQtvis49w@mail.gmail.com>

Dear all,

Is there a way to extend MCMC chains once a MCMCglmm fit is over?

I am using a relatively big dataset (35000 rows) and a rather complex model
(4 response variables, several fixed and random effects). So it is (as it
should be) slow.

Therefore after a run of one week, if the MCMC chains did not converge
well, I would like to know how I could continue to run the model starting
from the end point of the chains I have obtained. Ideally, I would love to
be able to consider the full length of the chains (so old + new ones) and
to have to possibility to redefine the burn-in period for the analyses.

Apologies if it is in the manual/course notes/tutorial... I looked for it
but may have missed it.

Best,

Alex


-- 
Alexandre Courtiol

http://sites.google.com/site/alexandrecourtiol/home

*"Science is the belief in the ignorance of experts"*, R. Feynman

	[[alternative HTML version deleted]]


From iyakoven at ucalgary.ca  Mon Mar 14 03:37:42 2016
From: iyakoven at ucalgary.ca (Igor Yakovenko)
Date: Sun, 13 Mar 2016 22:37:42 -0400
Subject: [R-sig-ME] Nlme corAR1 error: Model won't converge
Message-ID: <1d4d601d17d9a$9750de30$c5f29a90$@ucalgary.ca>

Hi Everyone,

 

I'm absolutely stuck trying to figure this out, so I thought brighter minds
here may be able to shed some light on the situation. I'm relatively new to
mixed modeling in R, so it could be something obvious. I'm trying to fit a
longitudinal mixed model in Nlme, specifically to be able to model the
covariance structure. The data itself is based on an RCT intervention with
two groups followed over three time points (baseline, 3 and 6 months).
Before I even started adding the group predictor or any other factors, I
started building the basic model with just the fixed intercept, random
intercept, then a fixed time and random slopes. After building these first 4
models, which went fine, I modeled corAR1 on top of the previous model and
got a convergence error. Here is the syntax I used, including the resulting
error:

 

model1 = gls(PGSI3months_Total ~1, data = vse, method = "ML", na.action =
"na.omit")

model2 = lme(PGSI3months_Total ~1, data = vse, method = "ML", na.action =
"na.omit", random = ~1|id)

timeRI = update(model2, .~. + Time)

timeRS = update(timeRI, random = ~Time|id)

ARModel = update(timeRS, correlation = corAR1(0, form = ~Time|id))

Error in lme.formula(fixed = PGSI3months_Total ~ Time, data = vse, random =
~Time |  : 

  nlminb problem, convergence error code = 1

  message = iteration limit reached without convergence (10)

 

Time is my time variable, id is the subject number variable, distance
between time points is equal. Changing the maximum number of iterations or
the optimizer did not resolve this issue. 

 

Here is the summary for the timeRS model. The degrees of freedom and sample
size appear to be appropriate to model additional parameters, unless I
missed something, so I can't understand why trying to model autocorrelation
produces an error.

 

Linear mixed-effects model fit by maximum likelihood
 Data: vse 
       AIC      BIC    logLik
  3391.798 3417.239 -1689.899
 
Random effects:
 Formula: ~Time | id
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev      Corr  
(Intercept) 3.869742861 (Intr)
Time        0.005132771 -0.001
Residual    5.588313454       
 
Fixed effects: PGSI3months_Total ~ Time 
                Value Std.Error  DF   t-value p-value
(Intercept) 18.208305 0.6840868 307  26.61695       0
Time        -4.693759 0.3065688 307 -15.31062       0
 Correlation: 
     (Intr)
Time -0.839
 
Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.26592087 -0.68602172 -0.06530166  0.60976438  2.47559836 
 
Number of Observations: 513
Number of Groups: 205 

 

Happy to provide any additional information. 

 

Thank you in advance for your help.


	[[alternative HTML version deleted]]


From timothy.gregoire at yale.edu  Mon Mar 14 05:06:18 2016
From: timothy.gregoire at yale.edu (Gregoire, Timothy)
Date: Mon, 14 Mar 2016 04:06:18 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 111, Issue 9
In-Reply-To: <mailman.3.1457866801.25748.r-sig-mixed-models@r-project.org>
References: <mailman.3.1457866801.25748.r-sig-mixed-models@r-project.org>
Message-ID: <8A2DF497F2B80E4B9BE3662392CD10D80B3D3FF7@X10-MBX9.yu.yale.edu>

Hi Jacob,

I will defer to Doug, as he is the ultimate source. 

I would not fret. It is all about numerical instability when trying to invert (Xprime\Sigma X) matrix, I suspect. 

Tim

Timothy G. Gregoire
J. P. Weyerhaeuser Professor of Forest Management
School of Forestry & Environmental Studies
Yale University
360 Prospect St, New Haven, CT, U.S.A. 06511
Ph: 1.203.432.9398 mob: 1.203.508.4014? fax:1.203.432.3809


-----Original Message-----
From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of r-sig-mixed-models-request at r-project.org
Sent: Sunday, March 13, 2016 7:00 AM
To: r-sig-mixed-models at r-project.org
Subject: R-sig-mixed-models Digest, Vol 111, Issue 9

Send R-sig-mixed-models mailing list submissions to
	r-sig-mixed-models at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
	https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=AwICAg&c=-dg2m7zWuuDZ0MUcV7Sdqw&r=atRKEKX5W2zm-GsIgYzLo4oYM9D-Qn-eMFObHZtnEnI&m=V1qtLiJ8zqDTfpgXN00RSeCx0zncNSopJhLyHBW2k9g&s=Hwxjnz5xf7ce9AVi3vCWhaHLjuN1WjVn9aq2gZ2z7cA&e=
or, via email, send a message with subject or body 'help' to
	r-sig-mixed-models-request at r-project.org

You can reach the person managing the list at
	r-sig-mixed-models-owner at r-project.org

When replying, please edit your Subject line so it is more specific than "Re: Contents of R-sig-mixed-models digest..."


Today's Topics:

   1. Non-significant fixed effect allows approximation of
      variance-covariance matrix (Jacob Bukoski)


----------------------------------------------------------------------

Message: 1
Date: Sat, 12 Mar 2016 20:33:38 -0500
From: Jacob Bukoski <jbukoski1 at gmail.com>
To: "r-sig-mixed-models at r-project.org"
	<r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Non-significant fixed effect allows approximation
	of variance-covariance matrix
Message-ID:
	<CAOES0Vjh3vEb=w9VaORWOP0MRTgDFuABpoGCSBAfa0WXnH6t6g at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

Dear all,

I am using lme() to run a mixed effects model on soil carbon observations, with fixed effects specified for latitude (continuous), and dominant genera of tree (factor with five levels), and random effects specified for site.

The data is heteroscedastic, which I can account for well with a varIdent weights specification; however, when I do so I receive a "non-positive definite approx. var-covar matrix" output from the call to $apVar.

When I add in a third fixed effect (Geomorphic setting, a factor with three levels), the variance-covariance matrix is approximated successfully, but the levels of the added third fixed effect are non significant.

I've been trying to read up on why this might be occurring, but can't for the life of me figure out why a more complex model (including non-significant predictors) would allow for the successful approximation of the variance-covariance matrix.

I'm hoping to use the model for predictive purposes, and ideally would not include non-significant effects in its final form.

Does anyone have any ideas on why this might be occurring, or intermediate-level resources per non-positive definite variance-covariance matrices that I could look into?

Many kind thanks,
Jacob

P.S. If it helps, the model specification is here:

*lme.C.density <- lme(C.density ~ Latitude + Genus + Geomorph, random=(~1|Site), weights=varIdent(form=~Genus|Site), data=model.c.dens, method = "REML")*

--
Jacob J. Bukoski
Master of Environmental Science Candidate, 2016 School of Forestry and Environmental Studies, Yale University jbukoski1 at gmail.com | jacob.bukoski at yale.edu | LinkedIn <https://urldefense.proofpoint.com/v2/url?u=https-3A__www.linkedin.com_profile_view-3Fid-3DAAIAAAdWVW8BMzqU-5F2EGNbEkyuy8O7K1Jyhd8ps-26trk-3Dnav-5Fresponsive-5Ftab-5Fprofile-5Fpic&d=AwICAg&c=-dg2m7zWuuDZ0MUcV7Sdqw&r=atRKEKX5W2zm-GsIgYzLo4oYM9D-Qn-eMFObHZtnEnI&m=V1qtLiJ8zqDTfpgXN00RSeCx0zncNSopJhLyHBW2k9g&s=3DY9F_FwFIGYXmf7IW5gKhKEFL7oBCG_vb5ZW2ReKdU&e= >

	[[alternative HTML version deleted]]



------------------------------

Subject: Digest Footer

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dsig-2Dmixed-2Dmodels&d=AwICAg&c=-dg2m7zWuuDZ0MUcV7Sdqw&r=atRKEKX5W2zm-GsIgYzLo4oYM9D-Qn-eMFObHZtnEnI&m=V1qtLiJ8zqDTfpgXN00RSeCx0zncNSopJhLyHBW2k9g&s=Hwxjnz5xf7ce9AVi3vCWhaHLjuN1WjVn9aq2gZ2z7cA&e= 

------------------------------

End of R-sig-mixed-models Digest, Vol 111, Issue 9


From thierry.onkelinx at inbo.be  Mon Mar 14 09:11:32 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 14 Mar 2016 09:11:32 +0100
Subject: [R-sig-ME] Nlme corAR1 error: Model won't converge
In-Reply-To: <1d4d601d17d9a$9750de30$c5f29a90$@ucalgary.ca>
References: <1d4d601d17d9a$9750de30$c5f29a90$@ucalgary.ca>
Message-ID: <CAJuCY5xpx1nyVBWzcKy7gJb2Ovy6uArE7u4MUAiC17eyuCehhg@mail.gmail.com>

Dear Igor,

Note that corAR1() assumes that time is integer and increases by one after
each time point. You need to check that.

Apart from that. A random slope along time and an AR1 model on the noise
will compete for the same information. When you have a lot of time points
per group, the model can decide with of the two is the most important.
Since you have only 3 time points, the model has difficulties with that.

So you probably better choose between the random slope and the AR1 instead
of using them both.

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-03-14 3:37 GMT+01:00 Igor Yakovenko <iyakoven at ucalgary.ca>:

> Hi Everyone,
>
>
>
> I'm absolutely stuck trying to figure this out, so I thought brighter minds
> here may be able to shed some light on the situation. I'm relatively new to
> mixed modeling in R, so it could be something obvious. I'm trying to fit a
> longitudinal mixed model in Nlme, specifically to be able to model the
> covariance structure. The data itself is based on an RCT intervention with
> two groups followed over three time points (baseline, 3 and 6 months).
> Before I even started adding the group predictor or any other factors, I
> started building the basic model with just the fixed intercept, random
> intercept, then a fixed time and random slopes. After building these first
> 4
> models, which went fine, I modeled corAR1 on top of the previous model and
> got a convergence error. Here is the syntax I used, including the resulting
> error:
>
>
>
> model1 = gls(PGSI3months_Total ~1, data = vse, method = "ML", na.action =
> "na.omit")
>
> model2 = lme(PGSI3months_Total ~1, data = vse, method = "ML", na.action =
> "na.omit", random = ~1|id)
>
> timeRI = update(model2, .~. + Time)
>
> timeRS = update(timeRI, random = ~Time|id)
>
> ARModel = update(timeRS, correlation = corAR1(0, form = ~Time|id))
>
> Error in lme.formula(fixed = PGSI3months_Total ~ Time, data = vse, random =
> ~Time |  :
>
>   nlminb problem, convergence error code = 1
>
>   message = iteration limit reached without convergence (10)
>
>
>
> Time is my time variable, id is the subject number variable, distance
> between time points is equal. Changing the maximum number of iterations or
> the optimizer did not resolve this issue.
>
>
>
> Here is the summary for the timeRS model. The degrees of freedom and sample
> size appear to be appropriate to model additional parameters, unless I
> missed something, so I can't understand why trying to model autocorrelation
> produces an error.
>
>
>
> Linear mixed-effects model fit by maximum likelihood
>  Data: vse
>        AIC      BIC    logLik
>   3391.798 3417.239 -1689.899
>
> Random effects:
>  Formula: ~Time | id
>  Structure: General positive-definite, Log-Cholesky parametrization
>             StdDev      Corr
> (Intercept) 3.869742861 (Intr)
> Time        0.005132771 -0.001
> Residual    5.588313454
>
> Fixed effects: PGSI3months_Total ~ Time
>                 Value Std.Error  DF   t-value p-value
> (Intercept) 18.208305 0.6840868 307  26.61695       0
> Time        -4.693759 0.3065688 307 -15.31062       0
>  Correlation:
>      (Intr)
> Time -0.839
>
> Standardized Within-Group Residuals:
>         Min          Q1         Med          Q3         Max
> -2.26592087 -0.68602172 -0.06530166  0.60976438  2.47559836
>
> Number of Observations: 513
> Number of Groups: 205
>
>
>
> Happy to provide any additional information.
>
>
>
> Thank you in advance for your help.
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Mar 14 09:19:25 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 14 Mar 2016 09:19:25 +0100
Subject: [R-sig-ME] glth function with lme returns error Error in
 contrMat(table(mf[[nm]]), type = types[pm]) : less than two groups
In-Reply-To: <CAPZCg-um8St1osxT_NK1gH+Q2XtX3r2KJf63WLyJRvtXMA3r0w@mail.gmail.com>
References: <CAPZCg-um8St1osxT_NK1gH+Q2XtX3r2KJf63WLyJRvtXMA3r0w@mail.gmail.com>
Message-ID: <CAJuCY5zk2DzkO-5qkX7AWo4PjXyhj0gr0U_RZOiDD-fGMrh+9A@mail.gmail.com>

Dear Annie,

Please don't use attach(). It's a recipe for disaster. Use the data
argument instead.

glht() can't handle interaction terms. You will have to specify the
contrasts manually. A workaround is to create a new variable that
holds the interaction an use that in the model.

spl$Interaction <- interaction(spl$treat, spl$week)
modcover <- lme(logcov ~ Interaction, random= ~1 | rep / week, data = spl)
summary(glht(modcover, linfct = mcp(Interaction = "Tukey")))

Best regards,

Thierry

Op 11-mrt.-2016 23:03 schreef "Annie-Claude Letendre"
<annieclaude.letendre at gmail.com>:
>
> Hi,
>
> I am trying to run a *posthoc* on a lme with the glht function and I get
> this error that I can't figure out "Error in contrMat(table(mf[[nm]]), type
> = types[pm]) :
>   less than two groups"
>
> My experiment had 45 indivuduals (rep) each of these had 1 treatment
> (treat) applied to it and then the variable (logcov) was measured twice for
> each individual at week 6 and 12.
>
> I've attached the csv I am using , thanks for any help you can offer !
> Is there any other posthoc test that could work for me ?
>
> Annie
>
> >
> spl<-read.table("C:/Users/Annie/Desktop/splcover.csv",header=T,sep=",",quote="")
> > attach(spl)
> > names(spl)
>  [1] "rep"     "treat"  "logcov"  "week"
>
>
> > rep<-factor(rep)
> > treat<-factor(treat)
> > week<-factor(week)
> > levels(treat)
> [1] "c" "f" "g" "p" "r"
> > levels(week)
> [1] "6"  "12"
> > levels(rep)
>  [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10" "11" "12" "13" "14"
> "15"
> [16] "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29"
> "30"
> [31] "31" "32" "33" "34" "35" "36" "37" "38" "39" "40" "41" "42" "43" "44"
> "45"
>
> > library(nlme)
> Warning message:
> package ?nlme? was built under R version 3.1.3
>
> > summary(modcover<-lme(logcov~treat*week, random= ~1|rep/week))
> Linear mixed-effects model fit by REML
>  Data: NULL
>         AIC       BIC   logLik
>   -33.51971 -2.553367 29.75986
>
> Random effects:
>  Formula: ~1 | rep
>         (Intercept)
> StdDev:  0.09534288
>
>  Formula: ~1 | week %in% rep
>         (Intercept)   Residual
> StdDev:   0.1095635 0.04379889
>
> Fixed effects: logcov ~ treat * week
>                    Value  Std.Error DF   t-value p-value
> (Intercept)   -2.0000000 0.05056654 40 -39.55185  0.0000
> treatf         1.5944444 0.07151188 40  22.29622  0.0000
> treatg         0.0000000 0.07151188 40   0.00000  1.0000
> treatp         1.3944444 0.07151188 40  19.49948  0.0000
> treatr         1.5744444 0.07151188 40  22.01654  0.0000
> week12         0.1400000 0.05562275 40   2.51696  0.0159
> treatf:week12 -0.0266667 0.07866245 40  -0.33900  0.7364
> treatg:week12  0.1888889 0.07866245 40   2.40126  0.0211
> treatp:week12 -0.0222222 0.07866245 40  -0.28250  0.7790
> treatr:week12  0.0077778 0.07866245 40   0.09888  0.9217
>  Correlation:
>               (Intr) treatf treatg treatp treatr week12 trtf:12 trtg:12
> trtp:12
> treatf        -0.707
>
> treatg        -0.707  0.500
>
> treatp        -0.707  0.500  0.500
>
> treatr        -0.707  0.500  0.500  0.500
>
> week12        -0.550  0.389  0.389  0.389  0.389
>
> treatf:week12  0.389 -0.550 -0.275 -0.275 -0.275 -0.707
>
> treatg:week12  0.389 -0.275 -0.550 -0.275 -0.275 -0.707  0.500
>
> treatp:week12  0.389 -0.275 -0.275 -0.550 -0.275 -0.707  0.500   0.500
>
> treatr:week12  0.389 -0.275 -0.275 -0.275 -0.550 -0.707  0.500   0.500
> 0.500
>
> Standardized Within-Group Residuals:
>          Min           Q1          Med           Q3          Max
> -0.741681806 -0.152850434 -0.008438999  0.126023769  0.836897713
>
> Number of Observations: 90
> Number of Groups:
>           rep week %in% rep
>            45            90
> > anova(modcover)
>             numDF denDF   F-value p-value
> (Intercept)     1    40 2817.8201  <.0001
> treat           4    40  366.2476  <.0001
> week            1    40   46.4611  <.0001
> treat:week      4    40    2.6323  0.0483
> > require(multcomp)
> Loading required package: multcomp
> Loading required package: mvtnorm
> Loading required package: survival
> Loading required package: splines
> Loading required package: TH.data
>
>
> > summary(glht(modcover, linfct=mcp(treat="Tukey")))
> Error in contrMat(table(mf[[nm]]), type = types[pm]) :
>   less than two groups
>
> > is.factor(treat)
> [1] TRUE
>
> > sessionInfo()
> R version 3.1.1 (2014-07-10)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
>
> locale:
> [1] LC_COLLATE=English_Canada.1252  LC_CTYPE=English_Canada.1252
> [3] LC_MONETARY=English_Canada.1252 LC_NUMERIC=C
> [5] LC_TIME=English_Canada.1252
>
> attached base packages:
> [1] splines   stats     graphics  grDevices utils     datasets  methods
> [8] base
>
> other attached packages:
> [1] multcomp_1.3-6  TH.data_1.0-3   survival_2.37-7 mvtnorm_1.0-0
> [5] nlme_3.1-125
>
> loaded via a namespace (and not attached):
>  [1] colorspace_1.2-4 ggplot2_2.0.0    grid_3.1.1       gtable_0.1.2
>  [5] lattice_0.20-33  munsell_0.4.2    plyr_1.8.3       Rcpp_0.12.2
>  [9] sandwich_2.3-2   scales_0.3.0     zoo_1.7-11
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From munjalpatel85 at gmail.com  Mon Mar 14 15:11:56 2016
From: munjalpatel85 at gmail.com (Munjal Patel)
Date: Mon, 14 Mar 2016 10:11:56 -0400
Subject: [R-sig-ME] Error produced while fitting nlmer Emax function
Message-ID: <CAOWjiK8vm_KCfaCMi6hmnxGNNb+3drinwR_bhX1xB=V_hd3w1Q@mail.gmail.com>

Dear R-Users and Developers of lme4,

Good Morning.



I am an Intermediate level R-user.



I do follow Blogs and comments in variety of sources for e.g RPubs,Github
etc.

I am facing some technical problem and wanted to ask one question.

It will be great if you can provide me some thoughts behind it.



I am trying to fit the Emax Model with nlmer.My model description is as
follow.



*EmaxM <- function(BASE,EMAX,EC50,CONC) {*

*         BASE+( EMAX*CONCs/(EC50+CONC) ) }*



*EmaxG <- deriv(body(EmaxM)[[2]],*

*               function.arg=EmaxM,*

*               namevec = c("BASE","EMAX","EC50","CONC"))*



*startvec<-c(BASE=60,EMAX=10,EC50=7.7)*





I am unable to write the nlmer function from the above mentioned arguemtns.

I have tried the following format but it gives me an Error.



*#The Error producing model fit formula*



*fit <- nlmer(HR~
EmaxG(BASE,EMAX,EC50,CONC)~(BASE+EMAX+EC50)~(BASE+EMAX|Subject),nAGQ = 0,*

*              data,start=startvec,verbose = TRUE)*





*I was unable to find the supporting material especially for the NLMM
models.Your input will be very helpful in this if you explain how to
gradually build up the NLMM model in terms of the Model Syntax.*

*I also seek help in the reading material which gives more idea about the
implementation of Random and fixed effects syntax using nlmer.*



*Thank you very much for your kind support and help in advance.*
Munjal

	[[alternative HTML version deleted]]


From denis.haine at gmail.com  Mon Mar 14 15:17:18 2016
From: denis.haine at gmail.com (Denis Haine)
Date: Mon, 14 Mar 2016 10:17:18 -0400
Subject: [R-sig-ME] confint.merMod, bootstrap and weights
Message-ID: <CAO1JZaO4REtu9=2AnpHxcR099fyf9kA1i5enqQ3-mt1qB37Ecg@mail.gmail.com>

That's what I thought, that the weighting was not taken into account.
The weights are used to estimate an underlying causal model. The Poisson
model is used as a Cox regression model with 2-level random effects.
I believe I have to write my own boostrap function, or be happy with the
Wald method for confint.

Denis

> The simulation function (sfun()) that's at the core of the parametric
> bootstrap algorithm is ignoring your specified prior weights.  Poisson
> models with weights are somewhat unusual; what are the weights in your
> model supposed to signify?  If you were simulating the data, how would
> you incorporate the weights in the simulation procedure?
>
>   Ben Bolker


> On 16-03-13 02:30 PM, Denis Haine wrote:
>> Hello,
>>
>> I ran a model as
>>
>> glmer(y ~ x, family = poisson, data, weights = w)
>>
>> and then tried to get confidence intervals with the following:
>>
>> confint(model, method = "boot", parallel = "multicore", ncpus = 4)
>>
>> However I'm getting the following warning message that I'm not receiving
>> when using method "Wald" instead of "boot":
>>
>> Warning message:
>> In sfun(object, nsim = 1, ftd = rep_len(musim, n * nsim), wts = weights)
:
>>    ignoring prior weights
>>
>> What's the meaning of this message?
>>
>> Thanks for your help,
>>
>> Denis

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Mon Mar 14 15:57:22 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 14 Mar 2016 10:57:22 -0400
Subject: [R-sig-ME] confint.merMod, bootstrap and weights
In-Reply-To: <CAO1JZaO4REtu9=2AnpHxcR099fyf9kA1i5enqQ3-mt1qB37Ecg@mail.gmail.com>
References: <CAO1JZaO4REtu9=2AnpHxcR099fyf9kA1i5enqQ3-mt1qB37Ecg@mail.gmail.com>
Message-ID: <56E6D152.4000608@gmail.com>


   In general profile confidence intervals (the default method for 
confidence intervals) are a good intermediate between Wald intervals and 
bootstrap intervals, in both computational cost and accuracy.  If you 
can decide how you want weights to be used, you might be able to use 
assignInNamespace() to hack lme4:::simfunList$poisson, which is 
currently defined as

function (object, nsim, ftd = fitted(object), wts = weights(object))
{
     wts <- weights(object)
     if (any(wts != 1))
         warning("ignoring prior weights")
     rpois(nsim * length(ftd), ftd)
}



On 16-03-14 10:17 AM, Denis Haine wrote:
> That's what I thought, that the weighting was not taken into account.
> The weights are used to estimate an underlying causal model. The Poisson
> model is used as a Cox regression model with 2-level random effects.
> I believe I have to write my own boostrap function, or be happy with the
> Wald method for confint.
>
> Denis
>
>> The simulation function (sfun()) that's at the core of the parametric
>> bootstrap algorithm is ignoring your specified prior weights.  Poisson
>> models with weights are somewhat unusual; what are the weights in your
>> model supposed to signify?  If you were simulating the data, how would
>> you incorporate the weights in the simulation procedure?
>>
>>    Ben Bolker
>
>
>> On 16-03-13 02:30 PM, Denis Haine wrote:
>>> Hello,
>>>
>>> I ran a model as
>>>
>>> glmer(y ~ x, family = poisson, data, weights = w)
>>>
>>> and then tried to get confidence intervals with the following:
>>>
>>> confint(model, method = "boot", parallel = "multicore", ncpus = 4)
>>>
>>> However I'm getting the following warning message that I'm not receiving
>>> when using method "Wald" instead of "boot":
>>>
>>> Warning message:
>>> In sfun(object, nsim = 1, ftd = rep_len(musim, n * nsim), wts = weights)
> :
>>>     ignoring prior weights
>>>
>>> What's the meaning of this message?
>>>
>>> Thanks for your help,
>>>
>>> Denis
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From mollieebrooks at gmail.com  Mon Mar 14 17:34:00 2016
From: mollieebrooks at gmail.com (Mollie Brooks)
Date: Mon, 14 Mar 2016 17:34:00 +0100
Subject: [R-sig-ME] Back-transformation of Poisson model
Message-ID: <86E759A5-4107-4730-AC32-7B9694CA2776@ufl.edu>


Hi R-sig-mixed,
I found the below discussion of back-transformations from the log scale and the same formula is on page 46 of the MCMCglmm course notes pdf (version June 20, 2015). I can?t find a copy of the cited book McCulloch CE, Searle SR (2001) or any other source for this information. I have one point that I?m still wondering about?should v, the sum of the variance components, include residual error in any model, i.e. not just residual error that is part of an observation-level random effect for over-dispersed models? In the following example, the formula, exp(Xb+0.5*v) seems to be giving me an estimate that is consistently higher than my population mean. So adding the residual variance will make it even worse. Is this normal?
Now for the R-specific part of the question... is there a version of predict.merMod that gives the the predictions on the response scale averaged over possible realizations of the random effects, i.e. marginalized with respect to the random effects? I can?t get any predictions close to the observed mean in the following example. I expected that a new level of the random effect would be given a marginalized prediction. Maybe this could be added to the documentation.
thanks,
Mollie

library(lme4)

# simulate Poisson data with random intercept
set.seed(11)
dat=data.frame(group=paste0('g', rep(1:10, each=100)))
x=rnorm(10, mean=5,sd=2) 
dat$y = rpois(nrow(dat), exp(x[dat$group]))
mean(dat$y)#observed mean

# fit model
m1=glmer(y ~ (1 | group), family = poisson, dat)

#estract estimates
str(predict(m1,  type="response", re.form=NA))
exp(fixef(m1)+ 0.5*VarCorr(m1)$group[1])#from MCMCglmm Coursenotes p.46
#not very close to the mean

nd=data.frame(group=paste0('g', -5:-1)) #add new levels
str(predict(m1, newdata=nd, type="response", re.form=NULL, allow.new.levels=TRUE))
str(predict(m1, newdata=nd, type="response", re.form=NA, allow.new.levels=TRUE))
str(predict(m1, newdata=nd, type="response", re.form=NA))
str(predict(m1, type="response", re.form=NA))
mean(exp(coef(m1)[[1]][,1]))#only number close to the observed mean

> 
> ----- Forwarded message from j.hadfield at ed.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models> -----
>      Date: Tue, 12 Jan 2010 18:38:06 +0000
>      From: Jarrod Hadfield <j.hadfield at ed.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
> Reply-To: Jarrod Hadfield <j.hadfield at ed.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>   Subject: Re: [R-sig-ME] Back-transformation of Poisson model
>        To: "Kardynal,Kevin [Yel]" <Kevin.Kardynal at EC.gc.ca <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
> 
> Dear Kevin,
> 
> You will have to exponentiate the output from predict.
> 
> It is worth noting there are three types of prediction (on the
> data-scale) you could make:
> 
> a) exp(Xb)
> b) exp(Xb+Zu)
> c) int exp(Xb)du assuming the u's are iid.
> 
> X - fixed effect design matrix  b - fixed effects
> Z - random effect design matrix  u - fixed effects
> 
> You have done the first, but this can be very different from c, which
> is what I expect you want: the predictions averaged over possible
> realisations of the random effects.
> 
> To obtain c use exp(Xb+0.5*v)
> 
> where v is the sum of the variance components. If you use additive
> over-dispersed models you want to include the "residual" variance in
> v, also.
> 
> Cheers,
> 
> Jarrod
> 
> Quoting "Kardynal,Kevin [Yel]" <Kevin.Kardynal at EC.gc.ca <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>:
> 
> > Hello,
> >
> > I'd like to know if predicted values from  a Poisson mixed model require
> > back-transformation to get the 'real' predicted values or if this is
> > done automatically? I assume that the predicted values are already
> > back-transformed since my results are similar to the annual means.
> >
> > My model for estimating bird trends in lme4 is as such:
> >
> > lme3<-lmer(Abundance ~ Year +  (1|Observer) + (1|Site/Station),
> > data=SWTH, family = poisson(link=log))
> >
> > I then took the coefficients from the mixed model and used them in a GLM
> > to derive the predicted values.
> > dd <-fixef(lme3) # gives coefficients
> >
> > lme_glmDEY <- glm(Abundance ~ Year, data=SWTH, family = poisson(link =
> > log)) # for poisson distribution
> > lme_glmDEY$coefficients <- dd
> >
> > # create a data set of predicted values
> > pred <- as.data.frame(predict(lme_glmDEY, SWTH , "response", se.fit=T))
> > pred_val <- as.data.frame(cbind(SWTH,pred))
> >
> > So, do I have to perform a back-transformation on the predicted values
> > (ie., using exp())?
> >
> > Thanks,
> >
> > Kevin
> >
> > 	[[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models> mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> >
> >
> 
> 
> 
> -- 
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
> 
> 
> 
> ----- End forwarded message -----
> 
> 
> -- 
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
> 
------------------------
Mollie Brooks, PhD
Postdoctoral Researcher, Population Ecology Research Group
Department of Evolutionary Biology & Environmental Studies, University of Z?rich
http://www.popecol.org/team/mollie-brooks/ <http://www.popecol.org/team/mollie-brooks/>
	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Mon Mar 14 21:04:15 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 14 Mar 2016 20:04:15 +0000
Subject: [R-sig-ME] Back-transformation of Poisson model
In-Reply-To: <86E759A5-4107-4730-AC32-7B9694CA2776@ufl.edu>
References: <86E759A5-4107-4730-AC32-7B9694CA2776@ufl.edu>
Message-ID: <56E7193F.3040107@ed.ac.uk>

Hi Mollie,

In simple models it is the sum of the variance components, although with 
things such as random regression it gets a bit more complicated.  For 
log-link Poisson models the mean is simply the mean of a log-normal. For 
logit models there is not an anlytical solution, and the approximation 
in the CourseNotes I certainly got from somewhere but can't now remember 
where from: somebody else has pointed out that my McCulloch & Searle 
citation does not have the relevant material so I will try and hunt down 
where the result comes from.

I think the deviation of your predicted and actual mean arises because 
of sampling error in the intercept/variance. The mean of an 
exponentiated sum of two normals will be larger than the exponentiated 
sum of the means of two normals. Consequently an unbiased estimator of 
the intercept and variance would lead to an upwardly biased estimator of 
the data-scale mean. Not sure about this though! Perhaps see if the 
predicted and actual mean become closer if you up the number of random 
effect levels (and so the intercept/variance become more precisely 
estimated).

Regarding Q2 I'm also not sure. In predict.MCMCglmm you can specifiy the 
random effects you want to marginalise over, but I *think* this is not 
implemented in lme4?

Cheers,

Jarrod







On 14/03/2016 16:34, Mollie Brooks wrote:
> Hi R-sig-mixed,
> I found the below discussion of back-transformations from the log scale and the same formula is on page 46 of the MCMCglmm course notes pdf (version June 20, 2015). I can?t find a copy of the cited book McCulloch CE, Searle SR (2001) or any other source for this information. I have one point that I?m still wondering about?should v, the sum of the variance components, include residual error in any model, i.e. not just residual error that is part of an observation-level random effect for over-dispersed models? In the following example, the formula, exp(Xb+0.5*v) seems to be giving me an estimate that is consistently higher than my population mean. So adding the residual variance will make it even worse. Is this normal?
> Now for the R-specific part of the question... is there a version of predict.merMod that gives the the predictions on the response scale averaged over possible realizations of the random effects, i.e. marginalized with respect to the random effects? I can?t get any predictions close to the observed mean in the following example. I expected that a new level of the random effect would be given a marginalized prediction. Maybe this could be added to the documentation.
> thanks,
> Mollie
>
> library(lme4)
>
> # simulate Poisson data with random intercept
> set.seed(11)
> dat=data.frame(group=paste0('g', rep(1:10, each=100)))
> x=rnorm(10, mean=5,sd=2)
> dat$y = rpois(nrow(dat), exp(x[dat$group]))
> mean(dat$y)#observed mean
>
> # fit model
> m1=glmer(y ~ (1 | group), family = poisson, dat)
>
> #estract estimates
> str(predict(m1,  type="response", re.form=NA))
> exp(fixef(m1)+ 0.5*VarCorr(m1)$group[1])#from MCMCglmm Coursenotes p.46
> #not very close to the mean
>
> nd=data.frame(group=paste0('g', -5:-1)) #add new levels
> str(predict(m1, newdata=nd, type="response", re.form=NULL, allow.new.levels=TRUE))
> str(predict(m1, newdata=nd, type="response", re.form=NA, allow.new.levels=TRUE))
> str(predict(m1, newdata=nd, type="response", re.form=NA))
> str(predict(m1, type="response", re.form=NA))
> mean(exp(coef(m1)[[1]][,1]))#only number close to the observed mean
>
>> ----- Forwarded message from j.hadfield at ed.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models> -----
>>       Date: Tue, 12 Jan 2010 18:38:06 +0000
>>       From: Jarrod Hadfield <j.hadfield at ed.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>> Reply-To: Jarrod Hadfield <j.hadfield at ed.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>>    Subject: Re: [R-sig-ME] Back-transformation of Poisson model
>>         To: "Kardynal,Kevin [Yel]" <Kevin.Kardynal at EC.gc.ca <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>>
>> Dear Kevin,
>>
>> You will have to exponentiate the output from predict.
>>
>> It is worth noting there are three types of prediction (on the
>> data-scale) you could make:
>>
>> a) exp(Xb)
>> b) exp(Xb+Zu)
>> c) int exp(Xb)du assuming the u's are iid.
>>
>> X - fixed effect design matrix  b - fixed effects
>> Z - random effect design matrix  u - fixed effects
>>
>> You have done the first, but this can be very different from c, which
>> is what I expect you want: the predictions averaged over possible
>> realisations of the random effects.
>>
>> To obtain c use exp(Xb+0.5*v)
>>
>> where v is the sum of the variance components. If you use additive
>> over-dispersed models you want to include the "residual" variance in
>> v, also.
>>
>> Cheers,
>>
>> Jarrod
>>
>> Quoting "Kardynal,Kevin [Yel]" <Kevin.Kardynal at EC.gc.ca <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>:
>>
>>> Hello,
>>>
>>> I'd like to know if predicted values from  a Poisson mixed model require
>>> back-transformation to get the 'real' predicted values or if this is
>>> done automatically? I assume that the predicted values are already
>>> back-transformed since my results are similar to the annual means.
>>>
>>> My model for estimating bird trends in lme4 is as such:
>>>
>>> lme3<-lmer(Abundance ~ Year +  (1|Observer) + (1|Site/Station),
>>> data=SWTH, family = poisson(link=log))
>>>
>>> I then took the coefficients from the mixed model and used them in a GLM
>>> to derive the predicted values.
>>> dd <-fixef(lme3) # gives coefficients
>>>
>>> lme_glmDEY <- glm(Abundance ~ Year, data=SWTH, family = poisson(link =
>>> log)) # for poisson distribution
>>> lme_glmDEY$coefficients <- dd
>>>
>>> # create a data set of predicted values
>>> pred <- as.data.frame(predict(lme_glmDEY, SWTH , "response", se.fit=T))
>>> pred_val <- as.data.frame(cbind(SWTH,pred))
>>>
>>> So, do I have to perform a back-transformation on the predicted values
>>> (ie., using exp())?
>>>
>>> Thanks,
>>>
>>> Kevin
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models> mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>>>
>>>
>>
>>
>> -- 
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>>
>> ----- End forwarded message -----
>>
>>
>> -- 
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
> ------------------------
> Mollie Brooks, PhD
> Postdoctoral Researcher, Population Ecology Research Group
> Department of Evolutionary Biology & Environmental Studies, University of Z?rich
> http://www.popecol.org/team/mollie-brooks/ <http://www.popecol.org/team/mollie-brooks/>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From mollieebrooks at gmail.com  Tue Mar 15 19:27:18 2016
From: mollieebrooks at gmail.com (Mollie Brooks)
Date: Tue, 15 Mar 2016 19:27:18 +0100
Subject: [R-sig-ME] Back-transformation of Poisson model
In-Reply-To: <56E7193F.3040107@ed.ac.uk>
References: <86E759A5-4107-4730-AC32-7B9694CA2776@ufl.edu>
	<56E7193F.3040107@ed.ac.uk>
Message-ID: <5A184EF2-CC22-4815-8E11-627A2ADCF731@gmail.com>

Thanks Jarrod,

You?re right that the exp(Xb + 0.5v) correction for Jensen?s inequality is better in a model with more levels of the random effect. For 10,000 groups and 100 observations per group, it has about 10% error. I couldn?t do a ton of replicates because it?s slow. I rewrote the simple example (below) in case other people want to play around with it. 

I think the answer to Q2 may be that predict.merMod does not marginalize over random effects to get predictions on the response scale. A lot of people don?t realize this, so I might add an issue about documentation to the lme4 github page, unless the authors chime in.

cheers,
Mollie

library(lme4)

# simulate Poisson data with random intercept
ng=100 #number of groups
dat=data.frame(group=paste0('g', rep(1:ng, each=100)))
x=rnorm(ng, mean=5, sd=2) 
dat$y = rpois(nrow(dat), exp(x[dat$group]))
mean(dat$y)

# fit model
m1=glmer(y ~ (1 | group), family = poisson, dat)
#dat$obs=1:nrow(dat)
#m2=glmer(y ~ (1 | group) + (1|obs), family = poisson, dat)

#estract estimates
summary(m1)
exp(fixef(m1)+ 0.5*VarCorr(m1)$group[1])#from MCMCglmm Coursenotes p.46

nd=data.frame(group=paste0('g', -5:-1)) #add new levels
str(predict(m1, newdata=nd, type="response", re.form=NULL, allow.new.levels=TRUE))
str(predict(m1, newdata=nd, type="response", re.form=NA, allow.new.levels=TRUE))
str(predict(m1, newdata=nd, type="response", re.form=NA))
str(predict(m1, type="response", re.form=NA))

mean(exp(coef(m1)[[1]][,1])) 
------------------------
Mollie Brooks, PhD
Postdoctoral Researcher, Population Ecology Research Group
Department of Evolutionary Biology & Environmental Studies, University of Z?rich
http://www.popecol.org/team/mollie-brooks/

> On 14Mar 2016, at 21:04, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:
> 
> Hi Mollie,
> 
> In simple models it is the sum of the variance components, although with things such as random regression it gets a bit more complicated.  For log-link Poisson models the mean is simply the mean of a log-normal. For logit models there is not an anlytical solution, and the approximation in the CourseNotes I certainly got from somewhere but can't now remember where from: somebody else has pointed out that my McCulloch & Searle citation does not have the relevant material so I will try and hunt down where the result comes from.
> 
> I think the deviation of your predicted and actual mean arises because of sampling error in the intercept/variance. The mean of an exponentiated sum of two normals will be larger than the exponentiated sum of the means of two normals. Consequently an unbiased estimator of the intercept and variance would lead to an upwardly biased estimator of the data-scale mean. Not sure about this though! Perhaps see if the predicted and actual mean become closer if you up the number of random effect levels (and so the intercept/variance become more precisely estimated).
> 
> Regarding Q2 I'm also not sure. In predict.MCMCglmm you can specifiy the random effects you want to marginalise over, but I *think* this is not implemented in lme4?
> 
> Cheers,
> 
> Jarrod
> 
> 
> 
> 
> 
> 
> 
> On 14/03/2016 16:34, Mollie Brooks wrote:
>> Hi R-sig-mixed,
>> I found the below discussion of back-transformations from the log scale and the same formula is on page 46 of the MCMCglmm course notes pdf (version June 20, 2015). I can?t find a copy of the cited book McCulloch CE, Searle SR (2001) or any other source for this information. I have one point that I?m still wondering about?should v, the sum of the variance components, include residual error in any model, i.e. not just residual error that is part of an observation-level random effect for over-dispersed models? In the following example, the formula, exp(Xb+0.5*v) seems to be giving me an estimate that is consistently higher than my population mean. So adding the residual variance will make it even worse. Is this normal?
>> Now for the R-specific part of the question... is there a version of predict.merMod that gives the the predictions on the response scale averaged over possible realizations of the random effects, i.e. marginalized with respect to the random effects? I can?t get any predictions close to the observed mean in the following example. I expected that a new level of the random effect would be given a marginalized prediction. Maybe this could be added to the documentation.
>> thanks,
>> Mollie
>> 
>> library(lme4)
>> 
>> # simulate Poisson data with random intercept
>> set.seed(11)
>> dat=data.frame(group=paste0('g', rep(1:10, each=100)))
>> x=rnorm(10, mean=5,sd=2)
>> dat$y = rpois(nrow(dat), exp(x[dat$group]))
>> mean(dat$y)#observed mean
>> 
>> # fit model
>> m1=glmer(y ~ (1 | group), family = poisson, dat)
>> 
>> #estract estimates
>> str(predict(m1,  type="response", re.form=NA))
>> exp(fixef(m1)+ 0.5*VarCorr(m1)$group[1])#from MCMCglmm Coursenotes p.46
>> #not very close to the mean
>> 
>> nd=data.frame(group=paste0('g', -5:-1)) #add new levels
>> str(predict(m1, newdata=nd, type="response", re.form=NULL, allow.new.levels=TRUE))
>> str(predict(m1, newdata=nd, type="response", re.form=NA, allow.new.levels=TRUE))
>> str(predict(m1, newdata=nd, type="response", re.form=NA))
>> str(predict(m1, type="response", re.form=NA))
>> mean(exp(coef(m1)[[1]][,1]))#only number close to the observed mean
>> 
>>> ----- Forwarded message from j.hadfield at ed.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models> -----
>>>      Date: Tue, 12 Jan 2010 18:38:06 +0000
>>>      From: Jarrod Hadfield <j.hadfield at ed.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>>> Reply-To: Jarrod Hadfield <j.hadfield at ed.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>>>   Subject: Re: [R-sig-ME] Back-transformation of Poisson model
>>>        To: "Kardynal,Kevin [Yel]" <Kevin.Kardynal at EC.gc.ca <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>>> 
>>> Dear Kevin,
>>> 
>>> You will have to exponentiate the output from predict.
>>> 
>>> It is worth noting there are three types of prediction (on the
>>> data-scale) you could make:
>>> 
>>> a) exp(Xb)
>>> b) exp(Xb+Zu)
>>> c) int exp(Xb)du assuming the u's are iid.
>>> 
>>> X - fixed effect design matrix  b - fixed effects
>>> Z - random effect design matrix  u - fixed effects
>>> 
>>> You have done the first, but this can be very different from c, which
>>> is what I expect you want: the predictions averaged over possible
>>> realisations of the random effects.
>>> 
>>> To obtain c use exp(Xb+0.5*v)
>>> 
>>> where v is the sum of the variance components. If you use additive
>>> over-dispersed models you want to include the "residual" variance in
>>> v, also.
>>> 
>>> Cheers,
>>> 
>>> Jarrod
>>> 
>>> Quoting "Kardynal,Kevin [Yel]" <Kevin.Kardynal at EC.gc.ca <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>:
>>> 
>>>> Hello,
>>>> 
>>>> I'd like to know if predicted values from  a Poisson mixed model require
>>>> back-transformation to get the 'real' predicted values or if this is
>>>> done automatically? I assume that the predicted values are already
>>>> back-transformed since my results are similar to the annual means.
>>>> 
>>>> My model for estimating bird trends in lme4 is as such:
>>>> 
>>>> lme3<-lmer(Abundance ~ Year +  (1|Observer) + (1|Site/Station),
>>>> data=SWTH, family = poisson(link=log))
>>>> 
>>>> I then took the coefficients from the mixed model and used them in a GLM
>>>> to derive the predicted values.
>>>> dd <-fixef(lme3) # gives coefficients
>>>> 
>>>> lme_glmDEY <- glm(Abundance ~ Year, data=SWTH, family = poisson(link =
>>>> log)) # for poisson distribution
>>>> lme_glmDEY$coefficients <- dd
>>>> 
>>>> # create a data set of predicted values
>>>> pred <- as.data.frame(predict(lme_glmDEY, SWTH , "response", se.fit=T))
>>>> pred_val <- as.data.frame(cbind(SWTH,pred))
>>>> 
>>>> So, do I have to perform a back-transformation on the predicted values
>>>> (ie., using exp())?
>>>> 
>>>> Thanks,
>>>> 
>>>> Kevin
>>>> 
>>>> 	[[alternative HTML version deleted]]
>>>> 
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models> mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>>>> 
>>>> 
>>> 
>>> 
>>> -- 
>>> The University of Edinburgh is a charitable body, registered in
>>> Scotland, with registration number SC005336.
>>> 
>>> 
>>> 
>>> ----- End forwarded message -----
>>> 
>>> 
>>> -- 
>>> The University of Edinburgh is a charitable body, registered in
>>> Scotland, with registration number SC005336.
>>> 
>> ------------------------
>> Mollie Brooks, PhD
>> Postdoctoral Researcher, Population Ecology Research Group
>> Department of Evolutionary Biology & Environmental Studies, University of Z?rich
>> http://www.popecol.org/team/mollie-brooks/ <http://www.popecol.org/team/mollie-brooks/>
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> -- 
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From bbolker at gmail.com  Tue Mar 15 22:38:11 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 15 Mar 2016 17:38:11 -0400
Subject: [R-sig-ME] Back-transformation of Poisson model
In-Reply-To: <5A184EF2-CC22-4815-8E11-627A2ADCF731@gmail.com>
References: <86E759A5-4107-4730-AC32-7B9694CA2776@ufl.edu>
	<56E7193F.3040107@ed.ac.uk>
	<5A184EF2-CC22-4815-8E11-627A2ADCF731@gmail.com>
Message-ID: <56E880C3.7090202@gmail.com>


   Please do submit an issue.

   Thinking about marginalizing predictions opens up a lot of questions:

   - marginalize by taking the mean of random samples?
   - by doing numerical integration?
   - by using the delta method?  (one might be able to use higher-order 
terms to reduce error ...)



On 16-03-15 02:27 PM, Mollie Brooks wrote:
> Thanks Jarrod,
>
> You?re right that the exp(Xb + 0.5v) correction for Jensen?s
> inequality is better in a model with more levels of the random
> effect. For 10,000 groups and 100 observations per group, it has
> about 10% error. I couldn?t do a ton of replicates because it?s slow.
> I rewrote the simple example (below) in case other people want to
> play around with it.
>
> I think the answer to Q2 may be that predict.merMod does not
> marginalize over random effects to get predictions on the response
> scale. A lot of people don?t realize this, so I might add an issue
> about documentation to the lme4 github page, unless the authors chime
> in.
>
> cheers, Mollie
>
> library(lme4)
>
> # simulate Poisson data with random intercept ng=100 #number of
> groups dat=data.frame(group=paste0('g', rep(1:ng, each=100)))
> x=rnorm(ng, mean=5, sd=2) dat$y = rpois(nrow(dat),
> exp(x[dat$group])) mean(dat$y)
>
> # fit model m1=glmer(y ~ (1 | group), family = poisson, dat)
> #dat$obs=1:nrow(dat) #m2=glmer(y ~ (1 | group) + (1|obs), family =
> poisson, dat)
>
> #estract estimates summary(m1) exp(fixef(m1)+
> 0.5*VarCorr(m1)$group[1])#from MCMCglmm Coursenotes p.46
>
> nd=data.frame(group=paste0('g', -5:-1)) #add new levels
> str(predict(m1, newdata=nd, type="response", re.form=NULL,
> allow.new.levels=TRUE)) str(predict(m1, newdata=nd, type="response",
> re.form=NA, allow.new.levels=TRUE)) str(predict(m1, newdata=nd,
> type="response", re.form=NA)) str(predict(m1, type="response",
> re.form=NA))
>
> mean(exp(coef(m1)[[1]][,1])) ------------------------ Mollie Brooks,
> PhD Postdoctoral Researcher, Population Ecology Research Group
> Department of Evolutionary Biology & Environmental Studies,
> University of Z?rich http://www.popecol.org/team/mollie-brooks/
>
>> On 14Mar 2016, at 21:04, Jarrod Hadfield <j.hadfield at ed.ac.uk>
>> wrote:
>>
>> Hi Mollie,
>>
>> In simple models it is the sum of the variance components, although
>> with things such as random regression it gets a bit more
>> complicated.  For log-link Poisson models the mean is simply the
>> mean of a log-normal. For logit models there is not an anlytical
>> solution, and the approximation in the CourseNotes I certainly got
>> from somewhere but can't now remember where from: somebody else has
>> pointed out that my McCulloch & Searle citation does not have the
>> relevant material so I will try and hunt down where the result
>> comes from.
>>
>> I think the deviation of your predicted and actual mean arises
>> because of sampling error in the intercept/variance. The mean of an
>> exponentiated sum of two normals will be larger than the
>> exponentiated sum of the means of two normals. Consequently an
>> unbiased estimator of the intercept and variance would lead to an
>> upwardly biased estimator of the data-scale mean. Not sure about
>> this though! Perhaps see if the predicted and actual mean become
>> closer if you up the number of random effect levels (and so the
>> intercept/variance become more precisely estimated).
>>
>> Regarding Q2 I'm also not sure. In predict.MCMCglmm you can
>> specifiy the random effects you want to marginalise over, but I
>> *think* this is not implemented in lme4?
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>>
>>
>>
>> On 14/03/2016 16:34, Mollie Brooks wrote:
>>> Hi R-sig-mixed, I found the below discussion of
>>> back-transformations from the log scale and the same formula is
>>> on page 46 of the MCMCglmm course notes pdf (version June 20,
>>> 2015). I can?t find a copy of the cited book McCulloch CE, Searle
>>> SR (2001) or any other source for this information. I have one
>>> point that I?m still wondering about?should v, the sum of the
>>> variance components, include residual error in any model, i.e.
>>> not just residual error that is part of an observation-level
>>> random effect for over-dispersed models? In the following
>>> example, the formula, exp(Xb+0.5*v) seems to be giving me an
>>> estimate that is consistently higher than my population mean. So
>>> adding the residual variance will make it even worse. Is this
>>> normal? Now for the R-specific part of the question... is there a
>>> version of predict.merMod that gives the the predictions on the
>>> response scale averaged over possible realizations of the random
>>> effects, i.e. marginalized with respect to the random effects? I
>>> can?t get any predictions close to the observed mean in the
>>> following example. I expected that a new level of the random
>>> effect would be given a marginalized prediction. Maybe this could
>>> be added to the documentation. thanks, Mollie
>>>
>>> library(lme4)
>>>
>>> # simulate Poisson data with random intercept set.seed(11)
>>> dat=data.frame(group=paste0('g', rep(1:10, each=100)))
>>> x=rnorm(10, mean=5,sd=2) dat$y = rpois(nrow(dat),
>>> exp(x[dat$group])) mean(dat$y)#observed mean
>>>
>>> # fit model m1=glmer(y ~ (1 | group), family = poisson, dat)
>>>
>>> #estract estimates str(predict(m1,  type="response",
>>> re.form=NA)) exp(fixef(m1)+ 0.5*VarCorr(m1)$group[1])#from
>>> MCMCglmm Coursenotes p.46 #not very close to the mean
>>>
>>> nd=data.frame(group=paste0('g', -5:-1)) #add new levels
>>> str(predict(m1, newdata=nd, type="response", re.form=NULL,
>>> allow.new.levels=TRUE)) str(predict(m1, newdata=nd,
>>> type="response", re.form=NA, allow.new.levels=TRUE))
>>> str(predict(m1, newdata=nd, type="response", re.form=NA))
>>> str(predict(m1, type="response", re.form=NA))
>>> mean(exp(coef(m1)[[1]][,1]))#only number close to the observed
>>> mean
>>>
>>>> ----- Forwarded message from j.hadfield at ed.ac.uk
>>>> <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>>>> ----- Date: Tue, 12 Jan 2010 18:38:06 +0000 From: Jarrod
>>>> Hadfield <j.hadfield at ed.ac.uk
>>>> <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>>>> Reply-To: Jarrod Hadfield <j.hadfield at ed.ac.uk
>>>> <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>>>> Subject: Re: [R-sig-ME] Back-transformation of Poisson model
>>>> To: "Kardynal,Kevin [Yel]" <Kevin.Kardynal at EC.gc.ca
>>>> <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>>>>
>>>> Dear Kevin,
>>>>
>>>> You will have to exponentiate the output from predict.
>>>>
>>>> It is worth noting there are three types of prediction (on the
>>>> data-scale) you could make:
>>>>
>>>> a) exp(Xb) b) exp(Xb+Zu) c) int exp(Xb)du assuming the u's are
>>>> iid.
>>>>
>>>> X - fixed effect design matrix  b - fixed effects Z - random
>>>> effect design matrix  u - fixed effects
>>>>
>>>> You have done the first, but this can be very different from c,
>>>> which is what I expect you want: the predictions averaged over
>>>> possible realisations of the random effects.
>>>>
>>>> To obtain c use exp(Xb+0.5*v)
>>>>
>>>> where v is the sum of the variance components. If you use
>>>> additive over-dispersed models you want to include the
>>>> "residual" variance in v, also.
>>>>
>>>> Cheers,
>>>>
>>>> Jarrod
>>>>
>>>> Quoting "Kardynal,Kevin [Yel]" <Kevin.Kardynal at EC.gc.ca
>>>> <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>:
>>>>
>>>>> Hello,
>>>>>
>>>>> I'd like to know if predicted values from  a Poisson mixed
>>>>> model require back-transformation to get the 'real' predicted
>>>>> values or if this is done automatically? I assume that the
>>>>> predicted values are already back-transformed since my
>>>>> results are similar to the annual means.
>>>>>
>>>>> My model for estimating bird trends in lme4 is as such:
>>>>>
>>>>> lme3<-lmer(Abundance ~ Year +  (1|Observer) +
>>>>> (1|Site/Station), data=SWTH, family = poisson(link=log))
>>>>>
>>>>> I then took the coefficients from the mixed model and used
>>>>> them in a GLM to derive the predicted values. dd
>>>>> <-fixef(lme3) # gives coefficients
>>>>>
>>>>> lme_glmDEY <- glm(Abundance ~ Year, data=SWTH, family =
>>>>> poisson(link = log)) # for poisson distribution
>>>>> lme_glmDEY$coefficients <- dd
>>>>>
>>>>> # create a data set of predicted values pred <-
>>>>> as.data.frame(predict(lme_glmDEY, SWTH , "response",
>>>>> se.fit=T)) pred_val <- as.data.frame(cbind(SWTH,pred))
>>>>>
>>>>> So, do I have to perform a back-transformation on the
>>>>> predicted values (ie., using exp())?
>>>>>
>>>>> Thanks,
>>>>>
>>>>> Kevin
>>>>>
>>>>> [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org
>>>>> <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>>>>> mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>> <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>>>>>
>>>>>
>>>>
>>>>
>>>> -- The University of Edinburgh is a charitable body, registered
>>>> in Scotland, with registration number SC005336.
>>>>
>>>>
>>>>
>>>> ----- End forwarded message -----
>>>>
>>>>
>>>> -- The University of Edinburgh is a charitable body, registered
>>>> in Scotland, with registration number SC005336.
>>>>
>>> ------------------------ Mollie Brooks, PhD Postdoctoral
>>> Researcher, Population Ecology Research Group Department of
>>> Evolutionary Biology & Environmental Studies, University of
>>> Z?rich http://www.popecol.org/team/mollie-brooks/
>>> <http://www.popecol.org/team/mollie-brooks/> [[alternative HTML
>>> version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>> -- The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From salnsg at gmail.com  Wed Mar 16 00:32:30 2016
From: salnsg at gmail.com (=?UTF-8?B?0KHQtdGA0LPQtdC5INChLg==?=)
Date: Wed, 16 Mar 2016 02:32:30 +0300
Subject: [R-sig-ME] =?utf-8?q?About_=E2=80=8B_calculation_of_the_gravity_m?=
	=?utf-8?q?odel_in_R_and_STATA_software?=
Message-ID: <CAM9wezp9AaxxO-2DyWg9o0C0z=nROCeww05OssOh6fE=O4eM2g@mail.gmail.com>

?Dear colleagues!

We spent
??
calculation of the gravity model in R and STATA software.
For calculations we used the standard package glmm in R (with parameter
family = quasipoisson)
and ppml in STATA.

Call the calculation procedure in R:

summary(glmm<-glm(formula=exports ~ ln_GDPimporter + ln_GDPexporter +
ln_GDPimppc + ln_GDPexppc + ln_Distance + ln_Tariff + ln_ExchangeRate +
Contig + Comlang + Colony_CIS + EAEU_CIS + EU_European_Union,
family=quasipoisson(link="log"),data=data_pua))

The results of the calculations in R following:

------------------------------------------------------------------------

Coefficients:
                           Estimate    Std. Error  t value Pr(>|t|)
(Intercept)           -12.53224   15.30072  -0.819  0.41357
ln_GDPimporter       0.10180    0.14988   0.679   0.49765
ln_GDPexporter       0.14612    0.79823    0.183   0.85491
ln_GDPimppc           0.34998    0.30247   1.157   0.24840
ln_GDPexppc           0.65811    0.82189    0.801   0.42409
ln_Distance             0.21838    0.16623    1.314   0.19020
ln_Tariff                 -0.05499    0.04913  -1.119  0.26411
ln_ExchangeRate     -0.11748    0.04275  -2.748  0.00646 **
Contig                      1.48321    0.28684   5.171  4.92e-07 ***
Comlang                  1.50727    0.26199    5.753   2.67e-08 ***
Colony_CIS               2.15272    0.46899   4.590  7.16e-06 ***
EAEU_CIS                -0.94417    0.29315  -3.221  0.00146 **
EU_European_Union  -0.08335    0.76733  -0.109  0.91359
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

(Dispersion parameter for quasipoisson family taken to be 2100.979)

    Null deviance: 1886758  on 251  degrees of freedom
Residual deviance:  316332  on 239  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 8

------------------------------------------------------------------------

On the same data, we done calculations in STATA, using ppml procedure.
Call of the calculation procedure in STATA was next:

ppml exports ln_gdpimporter ln_gdpexporter ln_gdpimppc ln_gdpexppc
ln_distance ln_tariff ln_exchangerate contig comlang colony_cis eaeu_cis
eu_european_union

The results of the calculations in STATA were following:

------------------------------------------------------------------------

Iteration 1:   deviance =  425911.3
Iteration 2:   deviance =  327020.8
Iteration 3:   deviance =  316763.3
Iteration 4:   deviance =  316335.1
Iteration 5:   deviance =  316332.3
Iteration 6:   deviance =  316332.3
Iteration 7:   deviance =  316332.3

Number of parameters: 13
Number of observations: 252
Pseudo log-likelihood: -158930.64
R-squared: .75348104
Option strict is: off
-----------------------------------------------------------------------------------
                           |                    Robust
          exports       |      Coef.      Std. Err.          z    P>|z|
   [95% Conf. Interval]
------------------
+----------------------------------------------------------------
   ln_gdpimporter    |   .1018021   .0982091     1.04   0.300
-.0906843    .2942885
   ln_gdpexporter    |   .1461135   1.084255     0.13   0.893
-1.978988    2.271215
      ln_gdpimppc     |    .349982    .201011      1.74   0.082
-.0439924    .7439564
      ln_gdpexppc     |   .6581201   1.098236     0.60   0.549
-1.494383    2.810624
      ln_distance       |   .2183809    .156757     1.39    0.164
-.0888572     .525619
        ln_tariff          |  -.0549914   .0551489    -1.00   0.319
-.1630811    .0530984
  ln_exchangerate    |  -.1174816   .0343881    -3.42   0.001
-.1848812   -.0500821
           contig          |   1.483213    .168467     8.80   0.000
1.153024    1.813402
          comlang       |   1.507272   .2745761     5.49   0.000
.9691126    2.045431
       colony_cis        |   2.152723   .2338133     9.21   0.000
1.694457    2.610988
         eaeu_cis        |  -.9441651   .2469764    -3.82   0.000
-1.42823   -.4601003
eu_european_union |  -.0833477   .4955678    -0.17   0.866     -1.054643
.8879474
            _cons         |  -12.53206   21.18599    -0.59   0.554
-54.05585    28.99172
-----------------------------------------------------------------------------------


As you can see, model coefficients (second column in the results table) are
the same at least until the 4th mark (!)
However, other results (columns in the table of results, since the third)
is not the same.
Could you explain differences in the results?
In particular, why coefficients the same (the first result table columns),
but standard errors is not?
With best regards,
Sergey S.

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Wed Mar 16 13:12:54 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 16 Mar 2016 12:12:54 +0000
Subject: [R-sig-ME] Back-transformation of Poisson model
In-Reply-To: <5A184EF2-CC22-4815-8E11-627A2ADCF731@gmail.com>
References: <86E759A5-4107-4730-AC32-7B9694CA2776@ufl.edu>
	<56E7193F.3040107@ed.ac.uk>
	<5A184EF2-CC22-4815-8E11-627A2ADCF731@gmail.com>
Message-ID: <56E94DC6.7020303@ed.ac.uk>

Hi,

Obtianing the expected mean by integrating over the uncertainity (e.g. 
the posterior) gets a much better estimate

m1.mcmc<-MCMCglmm(y~1, random=~group, data=dat, prior=list(R=list(V=1, 
nu=0.002), G=list(G1=list(V=1, nu=0.002))), family="poisson")

hist(exp(m1.mcmc$Sol+ 0.5*rowSums(m1.mcmc$VCV)), breaks=100)
abline(v=mean(dat$y), col="red")

You might be able to improve on the REML estimate by subtracting 0.5*(Vi 
+Civ+0.25*Vv) before exponentiating. Vi is the samping variance of the 
intercept (standard error^2) Vv the sampling variance of the variance, 
and Civ the sampling covariance bewteen the two. Getting approximate 
values of Vv and Civ from glmer might be difficult though - not sure.

Cheers,

Jarrod




On 15/03/2016 18:27, Mollie Brooks wrote:
> summary(m1)
> exp(fixef(m1)+ 0.5*VarCorr(m1)$group[1])

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20160316/8627fd2f/attachment.pl>

From cddesjardins at gmail.com  Wed Mar 16 18:56:29 2016
From: cddesjardins at gmail.com (Christopher David Desjardins)
Date: Wed, 16 Mar 2016 12:56:29 -0500
Subject: [R-sig-ME] Convergence warning message
Message-ID: <CALrjt7_3oQ8f0Z1L4OtpZ86uMFrohXMo_SY61Dmd+eyzOhJXrA@mail.gmail.com>

I am trying to fit a mixed effects binomial model.

The data consists of
- A dependent variable consisting of Bernoulli trials (outcome)
- A time variable (time), which has been mean centered
- An id variable (id)
- A categorical covariate (cat_cov)
- A blocking variable (block) which id is nested in. I realize in the model
below that it should be (1 | id/block) but I am just trying to troubleshoot
my problem at the moment.

When I run the following:

example_data <- read.csv("https://cddesja.github.io/example_data.csv",
header  = T)
example_data$cat_cov <- as.factor(example_data$cat_cov)
example_data$id <- as.factor(example_data$id)
example_data$block <- as.factor(example_data$block)
main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 | id),
data = example_data, family = "binomial")

That last line of code gives a warning message:

> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 |
id), data = example_data, family = "binomial")
Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 4.36001 (tol = 0.001, component
1)
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?

I am not exactly sure how to proceed. I know the issue is with cat_cov,
though it's unclear to me why. If I swap out in a different categorical
covariate in the model, not included in that data set, I don't get this
message. I am not running into complete separation with cat_cov.  So, I'm a
little perplexed.

Any advice on what I should do or something I could look at it would be
very helpful.

Thanks,
Chris
-- 
https://cddesja.github.io/

	[[alternative HTML version deleted]]


From jackiewood7 at gmail.com  Wed Mar 16 19:24:15 2016
From: jackiewood7 at gmail.com (Jackie Wood)
Date: Wed, 16 Mar 2016 14:24:15 -0400
Subject: [R-sig-ME] Convergence warning message
In-Reply-To: <CALrjt7_3oQ8f0Z1L4OtpZ86uMFrohXMo_SY61Dmd+eyzOhJXrA@mail.gmail.com>
References: <CALrjt7_3oQ8f0Z1L4OtpZ86uMFrohXMo_SY61Dmd+eyzOhJXrA@mail.gmail.com>
Message-ID: <CAOxxGRkSPbQqvgWK1bSGvi34d0Qx5KSKUMnCiYyXXi7NcTUAqw@mail.gmail.com>

Hi Chris,

Try checking ?convergence....coincidentally, I was having a similar problem
just yesterday. There are some step by step
instructions for trouble shooting/double checking convergence warnings. For
example, a bit of example code is provided to run your model using a number
of different optimizers. If all optimizers yield similar values, it's
possible that you could be getting false convergence warnings. I'm not sure
if that's the case with your data, but it might be a place to start!

Jacquelyn

On Wed, Mar 16, 2016 at 1:56 PM, Christopher David Desjardins <
cddesjardins at gmail.com> wrote:

> I am trying to fit a mixed effects binomial model.
>
> The data consists of
> - A dependent variable consisting of Bernoulli trials (outcome)
> - A time variable (time), which has been mean centered
> - An id variable (id)
> - A categorical covariate (cat_cov)
> - A blocking variable (block) which id is nested in. I realize in the model
> below that it should be (1 | id/block) but I am just trying to troubleshoot
> my problem at the moment.
>
> When I run the following:
>
> example_data <- read.csv("https://cddesja.github.io/example_data.csv",
> header  = T)
> example_data$cat_cov <- as.factor(example_data$cat_cov)
> example_data$id <- as.factor(example_data$id)
> example_data$block <- as.factor(example_data$block)
> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 | id),
> data = example_data, family = "binomial")
>
> That last line of code gives a warning message:
>
> > main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 |
> id), data = example_data, family = "binomial")
> Warning messages:
> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model failed to converge with max|grad| = 4.36001 (tol = 0.001, component
> 1)
> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>   Model is nearly unidentifiable: very large eigenvalue
>  - Rescale variables?
>
> I am not exactly sure how to proceed. I know the issue is with cat_cov,
> though it's unclear to me why. If I swap out in a different categorical
> covariate in the model, not included in that data set, I don't get this
> message. I am not running into complete separation with cat_cov.  So, I'm a
> little perplexed.
>
> Any advice on what I should do or something I could look at it would be
> very helpful.
>
> Thanks,
> Chris
> --
> https://cddesja.github.io/
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
Jacquelyn L.A. Wood, PhD.
224 Montrose Avenue
Toronto, ON
M6G 3G7
Phone: (514) 293-7255

	[[alternative HTML version deleted]]


From highstat at highstat.com  Wed Mar 16 20:13:39 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Wed, 16 Mar 2016 20:13:39 +0100
Subject: [R-sig-ME] 8 statistics courses in Australia
Message-ID: <56E9B063.2070100@highstat.com>

We would like to announce a series of 8 statistics courses in Australia 
in June/July/August 2016. For details and registration:

http://highstat.com/statscourse.htm

Kind regards,

Alain Zuur



Course 1: Introduction to Linear Mixed Effects Models and GLMM with R. 
Frequentist and Bayesian approaches.
Murdoch University, Perth, Australia
6-10 June 2016

Course 2: Introduction to Zero Inflated Models
Australian Institute of Marine Science, Perth, Australia
13-17 June 2016

Course 3: Introduction to Linear Mixed Effects Models and GLMM with R. 
Frequentist and Bayesian approaches
University of Adelaide, Adelaide, Australia
20-24 June 2016

Course 4: Introduction to Linear Mixed Effects Models and GLMM with R. 
Frequentist and Bayesian approaches
Deakin University, Melbourne, Australia
4-8 July 2016

Course 5: Introduction to GAM and GAMM using Bayesian and frequentist tools
CSIRO, Canberra, Australia
11-15 July 2016

Course 6: Introduction to Linear Mixed Effects Models and GLMM with R. 
Frequentist and Bayesian approaches
UNSW, Sydney, Australia
18-22 July 2016

Course 7: Introduction to Zero Inflated Models
UNSW, Sydney, Australia
25-29 July 2016

Course 8: Data exploration, regression, GLM & GAM with introduction to R
Charles Darwin University, Alice Springs, Australia
1-5 August 2016




-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From bbolker at gmail.com  Wed Mar 16 20:35:52 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 16 Mar 2016 15:35:52 -0400
Subject: [R-sig-ME] Convergence warning message
In-Reply-To: <CAOxxGRkSPbQqvgWK1bSGvi34d0Qx5KSKUMnCiYyXXi7NcTUAqw@mail.gmail.com>
References: <CALrjt7_3oQ8f0Z1L4OtpZ86uMFrohXMo_SY61Dmd+eyzOhJXrA@mail.gmail.com>
	<CAOxxGRkSPbQqvgWK1bSGvi34d0Qx5KSKUMnCiYyXXi7NcTUAqw@mail.gmail.com>
Message-ID: <56E9B598.8030607@gmail.com>


   Good question.

   I'm afraid that for data sets ~ 100,000 observations or bigger, our 
convergence calculations aren't terribly reliable -- see e.g. the third 
set of figures under https://rpubs.com/bbolker/lme4_convergence ... I 
would follow Jackie's advice ...

On 16-03-16 02:24 PM, Jackie Wood wrote:
> Hi Chris,
>
> Try checking ?convergence....coincidentally, I was having a similar problem
> just yesterday. There are some step by step
> instructions for trouble shooting/double checking convergence warnings. For
> example, a bit of example code is provided to run your model using a number
> of different optimizers. If all optimizers yield similar values, it's
> possible that you could be getting false convergence warnings. I'm not sure
> if that's the case with your data, but it might be a place to start!
>
> Jacquelyn
>
> On Wed, Mar 16, 2016 at 1:56 PM, Christopher David Desjardins <
> cddesjardins at gmail.com> wrote:
>
>> I am trying to fit a mixed effects binomial model.
>>
>> The data consists of
>> - A dependent variable consisting of Bernoulli trials (outcome)
>> - A time variable (time), which has been mean centered
>> - An id variable (id)
>> - A categorical covariate (cat_cov)
>> - A blocking variable (block) which id is nested in. I realize in the model
>> below that it should be (1 | id/block) but I am just trying to troubleshoot
>> my problem at the moment.
>>
>> When I run the following:
>>
>> example_data <- read.csv("https://cddesja.github.io/example_data.csv",
>> header  = T)
>> example_data$cat_cov <- as.factor(example_data$cat_cov)
>> example_data$id <- as.factor(example_data$id)
>> example_data$block <- as.factor(example_data$block)
>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 | id),
>> data = example_data, family = "binomial")
>>
>> That last line of code gives a warning message:
>>
>>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 |
>> id), data = example_data, family = "binomial")
>> Warning messages:
>> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>    Model failed to converge with max|grad| = 4.36001 (tol = 0.001, component
>> 1)
>> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>>    Model is nearly unidentifiable: very large eigenvalue
>>   - Rescale variables?
>>
>> I am not exactly sure how to proceed. I know the issue is with cat_cov,
>> though it's unclear to me why. If I swap out in a different categorical
>> covariate in the model, not included in that data set, I don't get this
>> message. I am not running into complete separation with cat_cov.  So, I'm a
>> little perplexed.
>>
>> Any advice on what I should do or something I could look at it would be
>> very helpful.
>>
>> Thanks,
>> Chris
>> --
>> https://cddesja.github.io/
>>
>>          [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>


From cddesjardins at gmail.com  Wed Mar 16 20:44:04 2016
From: cddesjardins at gmail.com (Christopher David Desjardins)
Date: Wed, 16 Mar 2016 14:44:04 -0500
Subject: [R-sig-ME] Convergence warning message
In-Reply-To: <56E9B598.8030607@gmail.com>
References: <CALrjt7_3oQ8f0Z1L4OtpZ86uMFrohXMo_SY61Dmd+eyzOhJXrA@mail.gmail.com>
	<CAOxxGRkSPbQqvgWK1bSGvi34d0Qx5KSKUMnCiYyXXi7NcTUAqw@mail.gmail.com>
	<56E9B598.8030607@gmail.com>
Message-ID: <CALrjt79awe0qBxkaGNM+G2t6V07xL0F7RA_fgdwT1cumDq+pDA@mail.gmail.com>

Thanks, Jacquelyn and Ben. Jacquelyn, did you mean to attach some code or
just reference the site that Ben did? I had seen Ben's comments on
StackOverflow about potential false convergence messages, so I'll dig a bit
deeper. I just wanted to make sure it wasn't something obvious that I had
overlooked first.

>From what I've read online, glmmPQL is inappropriate with Bernoulli trials.
Is that correct?

Chris



On Wed, Mar 16, 2016 at 2:35 PM, Ben Bolker <bbolker at gmail.com> wrote:

>
>   Good question.
>
>   I'm afraid that for data sets ~ 100,000 observations or bigger, our
> convergence calculations aren't terribly reliable -- see e.g. the third set
> of figures under https://rpubs.com/bbolker/lme4_convergence ... I would
> follow Jackie's advice ...
>
>
> On 16-03-16 02:24 PM, Jackie Wood wrote:
>
>> Hi Chris,
>>
>> Try checking ?convergence....coincidentally, I was having a similar
>> problem
>> just yesterday. There are some step by step
>> instructions for trouble shooting/double checking convergence warnings.
>> For
>> example, a bit of example code is provided to run your model using a
>> number
>> of different optimizers. If all optimizers yield similar values, it's
>> possible that you could be getting false convergence warnings. I'm not
>> sure
>> if that's the case with your data, but it might be a place to start!
>>
>> Jacquelyn
>>
>> On Wed, Mar 16, 2016 at 1:56 PM, Christopher David Desjardins <
>> cddesjardins at gmail.com> wrote:
>>
>> I am trying to fit a mixed effects binomial model.
>>>
>>> The data consists of
>>> - A dependent variable consisting of Bernoulli trials (outcome)
>>> - A time variable (time), which has been mean centered
>>> - An id variable (id)
>>> - A categorical covariate (cat_cov)
>>> - A blocking variable (block) which id is nested in. I realize in the
>>> model
>>> below that it should be (1 | id/block) but I am just trying to
>>> troubleshoot
>>> my problem at the moment.
>>>
>>> When I run the following:
>>>
>>> example_data <- read.csv("https://cddesja.github.io/example_data.csv",
>>> header  = T)
>>> example_data$cat_cov <- as.factor(example_data$cat_cov)
>>> example_data$id <- as.factor(example_data$id)
>>> example_data$block <- as.factor(example_data$block)
>>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 |
>>> id),
>>> data = example_data, family = "binomial")
>>>
>>> That last line of code gives a warning message:
>>>
>>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 |
>>>>
>>> id), data = example_data, family = "binomial")
>>> Warning messages:
>>> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
>>> :
>>>    Model failed to converge with max|grad| = 4.36001 (tol = 0.001,
>>> component
>>> 1)
>>> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
>>> :
>>>    Model is nearly unidentifiable: very large eigenvalue
>>>   - Rescale variables?
>>>
>>> I am not exactly sure how to proceed. I know the issue is with cat_cov,
>>> though it's unclear to me why. If I swap out in a different categorical
>>> covariate in the model, not included in that data set, I don't get this
>>> message. I am not running into complete separation with cat_cov.  So,
>>> I'm a
>>> little perplexed.
>>>
>>> Any advice on what I should do or something I could look at it would be
>>> very helpful.
>>>
>>> Thanks,
>>> Chris
>>> --
>>> https://cddesja.github.io/
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>
>>
>>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
https://cddesja.github.io/

	[[alternative HTML version deleted]]


From jackiewood7 at gmail.com  Wed Mar 16 20:56:50 2016
From: jackiewood7 at gmail.com (Jackie Wood)
Date: Wed, 16 Mar 2016 15:56:50 -0400
Subject: [R-sig-ME] Convergence warning message
In-Reply-To: <CALrjt79awe0qBxkaGNM+G2t6V07xL0F7RA_fgdwT1cumDq+pDA@mail.gmail.com>
References: <CALrjt7_3oQ8f0Z1L4OtpZ86uMFrohXMo_SY61Dmd+eyzOhJXrA@mail.gmail.com>
	<CAOxxGRkSPbQqvgWK1bSGvi34d0Qx5KSKUMnCiYyXXi7NcTUAqw@mail.gmail.com>
	<56E9B598.8030607@gmail.com>
	<CALrjt79awe0qBxkaGNM+G2t6V07xL0F7RA_fgdwT1cumDq+pDA@mail.gmail.com>
Message-ID: <CAOxxGRkNZBGRERFc7snUqjZVEuOyVGc6zYiD9DdJY9jRqcZNug@mail.gmail.com>

Hi Chris,

You can find the example code I was talking about here if you haven't
tracked it down already:

http://127.0.0.1:29918/library/lme4/html/convergence.html

Jackie



On Wed, Mar 16, 2016 at 3:44 PM, Christopher David Desjardins <
cddesjardins at gmail.com> wrote:

> Thanks, Jacquelyn and Ben. Jacquelyn, did you mean to attach some code or
> just reference the site that Ben did? I had seen Ben's comments on
> StackOverflow about potential false convergence messages, so I'll dig a bit
> deeper. I just wanted to make sure it wasn't something obvious that I had
> overlooked first.
>
> >From what I've read online, glmmPQL is inappropriate with Bernoulli
> trials.
> Is that correct?
>
> Chris
>
>
>
> On Wed, Mar 16, 2016 at 2:35 PM, Ben Bolker <bbolker at gmail.com> wrote:
>
> >
> >   Good question.
> >
> >   I'm afraid that for data sets ~ 100,000 observations or bigger, our
> > convergence calculations aren't terribly reliable -- see e.g. the third
> set
> > of figures under https://rpubs.com/bbolker/lme4_convergence ... I would
> > follow Jackie's advice ...
> >
> >
> > On 16-03-16 02:24 PM, Jackie Wood wrote:
> >
> >> Hi Chris,
> >>
> >> Try checking ?convergence....coincidentally, I was having a similar
> >> problem
> >> just yesterday. There are some step by step
> >> instructions for trouble shooting/double checking convergence warnings.
> >> For
> >> example, a bit of example code is provided to run your model using a
> >> number
> >> of different optimizers. If all optimizers yield similar values, it's
> >> possible that you could be getting false convergence warnings. I'm not
> >> sure
> >> if that's the case with your data, but it might be a place to start!
> >>
> >> Jacquelyn
> >>
> >> On Wed, Mar 16, 2016 at 1:56 PM, Christopher David Desjardins <
> >> cddesjardins at gmail.com> wrote:
> >>
> >> I am trying to fit a mixed effects binomial model.
> >>>
> >>> The data consists of
> >>> - A dependent variable consisting of Bernoulli trials (outcome)
> >>> - A time variable (time), which has been mean centered
> >>> - An id variable (id)
> >>> - A categorical covariate (cat_cov)
> >>> - A blocking variable (block) which id is nested in. I realize in the
> >>> model
> >>> below that it should be (1 | id/block) but I am just trying to
> >>> troubleshoot
> >>> my problem at the moment.
> >>>
> >>> When I run the following:
> >>>
> >>> example_data <- read.csv("https://cddesja.github.io/example_data.csv",
> >>> header  = T)
> >>> example_data$cat_cov <- as.factor(example_data$cat_cov)
> >>> example_data$id <- as.factor(example_data$id)
> >>> example_data$block <- as.factor(example_data$block)
> >>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 |
> >>> id),
> >>> data = example_data, family = "binomial")
> >>>
> >>> That last line of code gives a warning message:
> >>>
> >>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 |
> >>>>
> >>> id), data = example_data, family = "binomial")
> >>> Warning messages:
> >>> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
> >>> :
> >>>    Model failed to converge with max|grad| = 4.36001 (tol = 0.001,
> >>> component
> >>> 1)
> >>> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
> >>> :
> >>>    Model is nearly unidentifiable: very large eigenvalue
> >>>   - Rescale variables?
> >>>
> >>> I am not exactly sure how to proceed. I know the issue is with cat_cov,
> >>> though it's unclear to me why. If I swap out in a different categorical
> >>> covariate in the model, not included in that data set, I don't get this
> >>> message. I am not running into complete separation with cat_cov.  So,
> >>> I'm a
> >>> little perplexed.
> >>>
> >>> Any advice on what I should do or something I could look at it would be
> >>> very helpful.
> >>>
> >>> Thanks,
> >>> Chris
> >>> --
> >>> https://cddesja.github.io/
> >>>
> >>>          [[alternative HTML version deleted]]
> >>>
> >>> _______________________________________________
> >>> R-sig-mixed-models at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>>
> >>>
> >>
> >>
> >>
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>
>
> --
> https://cddesja.github.io/
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
Jacquelyn L.A. Wood, PhD.
224 Montrose Avenue
Toronto, ON
M6G 3G7
Phone: (514) 293-7255

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Wed Mar 16 21:12:56 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 16 Mar 2016 21:12:56 +0100
Subject: [R-sig-ME] Convergence warning message
In-Reply-To: <CAOxxGRkNZBGRERFc7snUqjZVEuOyVGc6zYiD9DdJY9jRqcZNug@mail.gmail.com>
References: <CALrjt7_3oQ8f0Z1L4OtpZ86uMFrohXMo_SY61Dmd+eyzOhJXrA@mail.gmail.com>
	<CAOxxGRkSPbQqvgWK1bSGvi34d0Qx5KSKUMnCiYyXXi7NcTUAqw@mail.gmail.com>
	<56E9B598.8030607@gmail.com>
	<CALrjt79awe0qBxkaGNM+G2t6V07xL0F7RA_fgdwT1cumDq+pDA@mail.gmail.com>
	<CAOxxGRkNZBGRERFc7snUqjZVEuOyVGc6zYiD9DdJY9jRqcZNug@mail.gmail.com>
Message-ID: <CAJuCY5xsqWpmAcTmumbi7od8KHdy-SOkVZME6wSsqVFFv+eMQA@mail.gmail.com>

Dear Jackie,

127.0.01 points to localhost, which will work only on your computer.

Best regards,
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no
more than asking him to perform a post-mortem examination: he may be
able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does
not ensure that a reasonable answer can be extracted from a given body
of data. ~ John Tukey


2016-03-16 20:56 GMT+01:00 Jackie Wood <jackiewood7 at gmail.com>:
> Hi Chris,
>
> You can find the example code I was talking about here if you haven't
> tracked it down already:
>
> http://127.0.0.1:29918/library/lme4/html/convergence.html
>
> Jackie
>
>
>
> On Wed, Mar 16, 2016 at 3:44 PM, Christopher David Desjardins <
> cddesjardins at gmail.com> wrote:
>
>> Thanks, Jacquelyn and Ben. Jacquelyn, did you mean to attach some code or
>> just reference the site that Ben did? I had seen Ben's comments on
>> StackOverflow about potential false convergence messages, so I'll dig a bit
>> deeper. I just wanted to make sure it wasn't something obvious that I had
>> overlooked first.
>>
>> >From what I've read online, glmmPQL is inappropriate with Bernoulli
>> trials.
>> Is that correct?
>>
>> Chris
>>
>>
>>
>> On Wed, Mar 16, 2016 at 2:35 PM, Ben Bolker <bbolker at gmail.com> wrote:
>>
>> >
>> >   Good question.
>> >
>> >   I'm afraid that for data sets ~ 100,000 observations or bigger, our
>> > convergence calculations aren't terribly reliable -- see e.g. the third
>> set
>> > of figures under https://rpubs.com/bbolker/lme4_convergence ... I would
>> > follow Jackie's advice ...
>> >
>> >
>> > On 16-03-16 02:24 PM, Jackie Wood wrote:
>> >
>> >> Hi Chris,
>> >>
>> >> Try checking ?convergence....coincidentally, I was having a similar
>> >> problem
>> >> just yesterday. There are some step by step
>> >> instructions for trouble shooting/double checking convergence warnings.
>> >> For
>> >> example, a bit of example code is provided to run your model using a
>> >> number
>> >> of different optimizers. If all optimizers yield similar values, it's
>> >> possible that you could be getting false convergence warnings. I'm not
>> >> sure
>> >> if that's the case with your data, but it might be a place to start!
>> >>
>> >> Jacquelyn
>> >>
>> >> On Wed, Mar 16, 2016 at 1:56 PM, Christopher David Desjardins <
>> >> cddesjardins at gmail.com> wrote:
>> >>
>> >> I am trying to fit a mixed effects binomial model.
>> >>>
>> >>> The data consists of
>> >>> - A dependent variable consisting of Bernoulli trials (outcome)
>> >>> - A time variable (time), which has been mean centered
>> >>> - An id variable (id)
>> >>> - A categorical covariate (cat_cov)
>> >>> - A blocking variable (block) which id is nested in. I realize in the
>> >>> model
>> >>> below that it should be (1 | id/block) but I am just trying to
>> >>> troubleshoot
>> >>> my problem at the moment.
>> >>>
>> >>> When I run the following:
>> >>>
>> >>> example_data <- read.csv("https://cddesja.github.io/example_data.csv",
>> >>> header  = T)
>> >>> example_data$cat_cov <- as.factor(example_data$cat_cov)
>> >>> example_data$id <- as.factor(example_data$id)
>> >>> example_data$block <- as.factor(example_data$block)
>> >>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 |
>> >>> id),
>> >>> data = example_data, family = "binomial")
>> >>>
>> >>> That last line of code gives a warning message:
>> >>>
>> >>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 |
>> >>>>
>> >>> id), data = example_data, family = "binomial")
>> >>> Warning messages:
>> >>> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
>> >>> :
>> >>>    Model failed to converge with max|grad| = 4.36001 (tol = 0.001,
>> >>> component
>> >>> 1)
>> >>> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
>> >>> :
>> >>>    Model is nearly unidentifiable: very large eigenvalue
>> >>>   - Rescale variables?
>> >>>
>> >>> I am not exactly sure how to proceed. I know the issue is with cat_cov,
>> >>> though it's unclear to me why. If I swap out in a different categorical
>> >>> covariate in the model, not included in that data set, I don't get this
>> >>> message. I am not running into complete separation with cat_cov.  So,
>> >>> I'm a
>> >>> little perplexed.
>> >>>
>> >>> Any advice on what I should do or something I could look at it would be
>> >>> very helpful.
>> >>>
>> >>> Thanks,
>> >>> Chris
>> >>> --
>> >>> https://cddesja.github.io/
>> >>>
>> >>>          [[alternative HTML version deleted]]
>> >>>
>> >>> _______________________________________________
>> >>> R-sig-mixed-models at r-project.org mailing list
>> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>>
>> >>>
>> >>
>> >>
>> >>
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>>
>>
>>
>> --
>> https://cddesja.github.io/
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>
> --
> Jacquelyn L.A. Wood, PhD.
> 224 Montrose Avenue
> Toronto, ON
> M6G 3G7
> Phone: (514) 293-7255
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From cddesjardins at gmail.com  Wed Mar 16 21:33:34 2016
From: cddesjardins at gmail.com (Christopher David Desjardins)
Date: Wed, 16 Mar 2016 15:33:34 -0500
Subject: [R-sig-ME] Convergence warning message
In-Reply-To: <CAJuCY5xsqWpmAcTmumbi7od8KHdy-SOkVZME6wSsqVFFv+eMQA@mail.gmail.com>
References: <CALrjt7_3oQ8f0Z1L4OtpZ86uMFrohXMo_SY61Dmd+eyzOhJXrA@mail.gmail.com>
	<CAOxxGRkSPbQqvgWK1bSGvi34d0Qx5KSKUMnCiYyXXi7NcTUAqw@mail.gmail.com>
	<56E9B598.8030607@gmail.com>
	<CALrjt79awe0qBxkaGNM+G2t6V07xL0F7RA_fgdwT1cumDq+pDA@mail.gmail.com>
	<CAOxxGRkNZBGRERFc7snUqjZVEuOyVGc6zYiD9DdJY9jRqcZNug@mail.gmail.com>
	<CAJuCY5xsqWpmAcTmumbi7od8KHdy-SOkVZME6wSsqVFFv+eMQA@mail.gmail.com>
Message-ID: <CALrjt7-C8EdYw3jqv0MC22Fx48C4DGqcUXjnFxw+AwkjHQs5DQ@mail.gmail.com>

Thanks, I'll read through the convergence help page in the lme4 package.
Chris

On Wed, Mar 16, 2016 at 3:12 PM, Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Jackie,
>
> 127.0.01 points to localhost, which will work only on your computer.
>
> Best regards,
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no
> more than asking him to perform a post-mortem examination: he may be
> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does
> not ensure that a reasonable answer can be extracted from a given body
> of data. ~ John Tukey
>
>
> 2016-03-16 20:56 GMT+01:00 Jackie Wood <jackiewood7 at gmail.com>:
> > Hi Chris,
> >
> > You can find the example code I was talking about here if you haven't
> > tracked it down already:
> >
> > http://127.0.0.1:29918/library/lme4/html/convergence.html
> >
> > Jackie
> >
> >
> >
> > On Wed, Mar 16, 2016 at 3:44 PM, Christopher David Desjardins <
> > cddesjardins at gmail.com> wrote:
> >
> >> Thanks, Jacquelyn and Ben. Jacquelyn, did you mean to attach some code
> or
> >> just reference the site that Ben did? I had seen Ben's comments on
> >> StackOverflow about potential false convergence messages, so I'll dig a
> bit
> >> deeper. I just wanted to make sure it wasn't something obvious that I
> had
> >> overlooked first.
> >>
> >> >From what I've read online, glmmPQL is inappropriate with Bernoulli
> >> trials.
> >> Is that correct?
> >>
> >> Chris
> >>
> >>
> >>
> >> On Wed, Mar 16, 2016 at 2:35 PM, Ben Bolker <bbolker at gmail.com> wrote:
> >>
> >> >
> >> >   Good question.
> >> >
> >> >   I'm afraid that for data sets ~ 100,000 observations or bigger, our
> >> > convergence calculations aren't terribly reliable -- see e.g. the
> third
> >> set
> >> > of figures under https://rpubs.com/bbolker/lme4_convergence ... I
> would
> >> > follow Jackie's advice ...
> >> >
> >> >
> >> > On 16-03-16 02:24 PM, Jackie Wood wrote:
> >> >
> >> >> Hi Chris,
> >> >>
> >> >> Try checking ?convergence....coincidentally, I was having a similar
> >> >> problem
> >> >> just yesterday. There are some step by step
> >> >> instructions for trouble shooting/double checking convergence
> warnings.
> >> >> For
> >> >> example, a bit of example code is provided to run your model using a
> >> >> number
> >> >> of different optimizers. If all optimizers yield similar values, it's
> >> >> possible that you could be getting false convergence warnings. I'm
> not
> >> >> sure
> >> >> if that's the case with your data, but it might be a place to start!
> >> >>
> >> >> Jacquelyn
> >> >>
> >> >> On Wed, Mar 16, 2016 at 1:56 PM, Christopher David Desjardins <
> >> >> cddesjardins at gmail.com> wrote:
> >> >>
> >> >> I am trying to fit a mixed effects binomial model.
> >> >>>
> >> >>> The data consists of
> >> >>> - A dependent variable consisting of Bernoulli trials (outcome)
> >> >>> - A time variable (time), which has been mean centered
> >> >>> - An id variable (id)
> >> >>> - A categorical covariate (cat_cov)
> >> >>> - A blocking variable (block) which id is nested in. I realize in
> the
> >> >>> model
> >> >>> below that it should be (1 | id/block) but I am just trying to
> >> >>> troubleshoot
> >> >>> my problem at the moment.
> >> >>>
> >> >>> When I run the following:
> >> >>>
> >> >>> example_data <- read.csv("
> https://cddesja.github.io/example_data.csv",
> >> >>> header  = T)
> >> >>> example_data$cat_cov <- as.factor(example_data$cat_cov)
> >> >>> example_data$id <- as.factor(example_data$id)
> >> >>> example_data$block <- as.factor(example_data$block)
> >> >>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1
> |
> >> >>> id),
> >> >>> data = example_data, family = "binomial")
> >> >>>
> >> >>> That last line of code gives a warning message:
> >> >>>
> >> >>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1
> |
> >> >>>>
> >> >>> id), data = example_data, family = "binomial")
> >> >>> Warning messages:
> >> >>> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
> control$checkConv,
> >> >>> :
> >> >>>    Model failed to converge with max|grad| = 4.36001 (tol = 0.001,
> >> >>> component
> >> >>> 1)
> >> >>> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
> control$checkConv,
> >> >>> :
> >> >>>    Model is nearly unidentifiable: very large eigenvalue
> >> >>>   - Rescale variables?
> >> >>>
> >> >>> I am not exactly sure how to proceed. I know the issue is with
> cat_cov,
> >> >>> though it's unclear to me why. If I swap out in a different
> categorical
> >> >>> covariate in the model, not included in that data set, I don't get
> this
> >> >>> message. I am not running into complete separation with cat_cov.
> So,
> >> >>> I'm a
> >> >>> little perplexed.
> >> >>>
> >> >>> Any advice on what I should do or something I could look at it
> would be
> >> >>> very helpful.
> >> >>>
> >> >>> Thanks,
> >> >>> Chris
> >> >>> --
> >> >>> https://cddesja.github.io/
> >> >>>
> >> >>>          [[alternative HTML version deleted]]
> >> >>>
> >> >>> _______________________________________________
> >> >>> R-sig-mixed-models at r-project.org mailing list
> >> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> >>>
> >> >>>
> >> >>
> >> >>
> >> >>
> >> > _______________________________________________
> >> > R-sig-mixed-models at r-project.org mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> >
> >>
> >>
> >>
> >> --
> >> https://cddesja.github.io/
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >
> >
> >
> > --
> > Jacquelyn L.A. Wood, PhD.
> > 224 Montrose Avenue
> > Toronto, ON
> > M6G 3G7
> > Phone: (514) 293-7255
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
https://cddesja.github.io/

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Wed Mar 16 21:34:46 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 16 Mar 2016 16:34:46 -0400
Subject: [R-sig-ME] Convergence warning message
In-Reply-To: <CAJuCY5xsqWpmAcTmumbi7od8KHdy-SOkVZME6wSsqVFFv+eMQA@mail.gmail.com>
References: <CALrjt7_3oQ8f0Z1L4OtpZ86uMFrohXMo_SY61Dmd+eyzOhJXrA@mail.gmail.com>
	<CAOxxGRkSPbQqvgWK1bSGvi34d0Qx5KSKUMnCiYyXXi7NcTUAqw@mail.gmail.com>
	<56E9B598.8030607@gmail.com>
	<CALrjt79awe0qBxkaGNM+G2t6V07xL0F7RA_fgdwT1cumDq+pDA@mail.gmail.com>
	<CAOxxGRkNZBGRERFc7snUqjZVEuOyVGc6zYiD9DdJY9jRqcZNug@mail.gmail.com>
	<CAJuCY5xsqWpmAcTmumbi7od8KHdy-SOkVZME6wSsqVFFv+eMQA@mail.gmail.com>
Message-ID: <CABghstR+TuoiYxUDecHDTDCCCnDZz52KCFAu9KgvDPDyKfMVsw@mail.gmail.com>

https://github.com/lme4/lme4/blob/master/man/convergence.Rd

On Wed, Mar 16, 2016 at 4:12 PM, Thierry Onkelinx
<thierry.onkelinx at inbo.be> wrote:
> Dear Jackie,
>
> 127.0.01 points to localhost, which will work only on your computer.
>
> Best regards,
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no
> more than asking him to perform a post-mortem examination: he may be
> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does
> not ensure that a reasonable answer can be extracted from a given body
> of data. ~ John Tukey
>
>
> 2016-03-16 20:56 GMT+01:00 Jackie Wood <jackiewood7 at gmail.com>:
>> Hi Chris,
>>
>> You can find the example code I was talking about here if you haven't
>> tracked it down already:
>>
>> http://127.0.0.1:29918/library/lme4/html/convergence.html
>>
>> Jackie
>>
>>
>>
>> On Wed, Mar 16, 2016 at 3:44 PM, Christopher David Desjardins <
>> cddesjardins at gmail.com> wrote:
>>
>>> Thanks, Jacquelyn and Ben. Jacquelyn, did you mean to attach some code or
>>> just reference the site that Ben did? I had seen Ben's comments on
>>> StackOverflow about potential false convergence messages, so I'll dig a bit
>>> deeper. I just wanted to make sure it wasn't something obvious that I had
>>> overlooked first.
>>>
>>> >From what I've read online, glmmPQL is inappropriate with Bernoulli
>>> trials.
>>> Is that correct?
>>>
>>> Chris
>>>
>>>
>>>
>>> On Wed, Mar 16, 2016 at 2:35 PM, Ben Bolker <bbolker at gmail.com> wrote:
>>>
>>> >
>>> >   Good question.
>>> >
>>> >   I'm afraid that for data sets ~ 100,000 observations or bigger, our
>>> > convergence calculations aren't terribly reliable -- see e.g. the third
>>> set
>>> > of figures under https://rpubs.com/bbolker/lme4_convergence ... I would
>>> > follow Jackie's advice ...
>>> >
>>> >
>>> > On 16-03-16 02:24 PM, Jackie Wood wrote:
>>> >
>>> >> Hi Chris,
>>> >>
>>> >> Try checking ?convergence....coincidentally, I was having a similar
>>> >> problem
>>> >> just yesterday. There are some step by step
>>> >> instructions for trouble shooting/double checking convergence warnings.
>>> >> For
>>> >> example, a bit of example code is provided to run your model using a
>>> >> number
>>> >> of different optimizers. If all optimizers yield similar values, it's
>>> >> possible that you could be getting false convergence warnings. I'm not
>>> >> sure
>>> >> if that's the case with your data, but it might be a place to start!
>>> >>
>>> >> Jacquelyn
>>> >>
>>> >> On Wed, Mar 16, 2016 at 1:56 PM, Christopher David Desjardins <
>>> >> cddesjardins at gmail.com> wrote:
>>> >>
>>> >> I am trying to fit a mixed effects binomial model.
>>> >>>
>>> >>> The data consists of
>>> >>> - A dependent variable consisting of Bernoulli trials (outcome)
>>> >>> - A time variable (time), which has been mean centered
>>> >>> - An id variable (id)
>>> >>> - A categorical covariate (cat_cov)
>>> >>> - A blocking variable (block) which id is nested in. I realize in the
>>> >>> model
>>> >>> below that it should be (1 | id/block) but I am just trying to
>>> >>> troubleshoot
>>> >>> my problem at the moment.
>>> >>>
>>> >>> When I run the following:
>>> >>>
>>> >>> example_data <- read.csv("https://cddesja.github.io/example_data.csv",
>>> >>> header  = T)
>>> >>> example_data$cat_cov <- as.factor(example_data$cat_cov)
>>> >>> example_data$id <- as.factor(example_data$id)
>>> >>> example_data$block <- as.factor(example_data$block)
>>> >>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 |
>>> >>> id),
>>> >>> data = example_data, family = "binomial")
>>> >>>
>>> >>> That last line of code gives a warning message:
>>> >>>
>>> >>> main_effects <- glmer(outcome ~ 1 + cat_cov + time + I(time^2) + (1 |
>>> >>>>
>>> >>> id), data = example_data, family = "binomial")
>>> >>> Warning messages:
>>> >>> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
>>> >>> :
>>> >>>    Model failed to converge with max|grad| = 4.36001 (tol = 0.001,
>>> >>> component
>>> >>> 1)
>>> >>> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,
>>> >>> :
>>> >>>    Model is nearly unidentifiable: very large eigenvalue
>>> >>>   - Rescale variables?
>>> >>>
>>> >>> I am not exactly sure how to proceed. I know the issue is with cat_cov,
>>> >>> though it's unclear to me why. If I swap out in a different categorical
>>> >>> covariate in the model, not included in that data set, I don't get this
>>> >>> message. I am not running into complete separation with cat_cov.  So,
>>> >>> I'm a
>>> >>> little perplexed.
>>> >>>
>>> >>> Any advice on what I should do or something I could look at it would be
>>> >>> very helpful.
>>> >>>
>>> >>> Thanks,
>>> >>> Chris
>>> >>> --
>>> >>> https://cddesja.github.io/
>>> >>>
>>> >>>          [[alternative HTML version deleted]]
>>> >>>
>>> >>> _______________________________________________
>>> >>> R-sig-mixed-models at r-project.org mailing list
>>> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> >>>
>>> >>>
>>> >>
>>> >>
>>> >>
>>> > _______________________________________________
>>> > R-sig-mixed-models at r-project.org mailing list
>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>> >
>>>
>>>
>>>
>>> --
>>> https://cddesja.github.io/
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>
>>
>> --
>> Jacquelyn L.A. Wood, PhD.
>> 224 Montrose Avenue
>> Toronto, ON
>> M6G 3G7
>> Phone: (514) 293-7255
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Thu Mar 17 03:43:39 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 16 Mar 2016 22:43:39 -0400
Subject: [R-sig-ME] Convergence warning message
In-Reply-To: <CALrjt7-C8EdYw3jqv0MC22Fx48C4DGqcUXjnFxw+AwkjHQs5DQ@mail.gmail.com>
References: <CALrjt7_3oQ8f0Z1L4OtpZ86uMFrohXMo_SY61Dmd+eyzOhJXrA@mail.gmail.com>
	<CAOxxGRkSPbQqvgWK1bSGvi34d0Qx5KSKUMnCiYyXXi7NcTUAqw@mail.gmail.com>
	<56E9B598.8030607@gmail.com>
	<CALrjt79awe0qBxkaGNM+G2t6V07xL0F7RA_fgdwT1cumDq+pDA@mail.gmail.com>
	<CAOxxGRkNZBGRERFc7snUqjZVEuOyVGc6zYiD9DdJY9jRqcZNug@mail.gmail.com>
	<CAJuCY5xsqWpmAcTmumbi7od8KHdy-SOkVZME6wSsqVFFv+eMQA@mail.gmail.com>
	<CALrjt7-C8EdYw3jqv0MC22Fx48C4DGqcUXjnFxw+AwkjHQs5DQ@mail.gmail.com>
Message-ID: <56EA19DB.6000606@gmail.com>

   To answer this particular question:

   The importance of accuracy in the approximation of the integral over 
the random effects (for which glmmPQL < Laplace < adaptive Gauss-Hermite 
quadrature) depends on the degree to which the sampling distribution of 
the conditional modes (approx "BLUPs", i.e. the values associated with 
deviations of particular grouping factors from the population average) 
are Gaussian.  In particular, if you have Bernoulli responses and small 
numbers of samples per group (which seems to be the case here -- 1 to 4 
observations per individual if I read your data correctly) then yes, 
this will be a concern ...


On 16-03-16 04:33 PM, Christopher David Desjardins wrote:
>  From what I've read online, glmmPQL is inappropriate with Bernoulli
>> >>trials.
>> >>Is that correct?


From iyakoven at ucalgary.ca  Mon Mar 21 04:44:16 2016
From: iyakoven at ucalgary.ca (Igor Yakovenko)
Date: Sun, 20 Mar 2016 23:44:16 -0400
Subject: [R-sig-ME] Graphing lme results using marginal group means
Message-ID: <2c2901d18323$f59a28f0$e0ce7ad0$@ucalgary.ca>

Hello Everyone,

 

I've run a number of mixed models using data from a two-group RCT evaluating
an intervention over 3 time points  and am currently trying to figure out
the best way to graphically present the results to a lay audience. I thought
the simplest and most digestible way would be to graph the marginal group
means (i.e., means controlling for other predictors in the model) at each
time point using line graphs, but I can't figure out how to do this in R.
Does anyone know how to graph the results of a model such as the one below
where X = time points (baseline, 3 months, 6 months), Y = marginal means
with two lines, one for each of the two groups? Different colors for the
group lines would be ideal. Here is a typical model that I would fit in
nlme.

 

Time_squared <-lme(Control_Scale~Time*Condition + I(Time^2), data = vse,
random = ~Time|id, method = "ML",  na.action = na.omit, control =
list(opt="optim"))

 

In addition, I'm also wondering if there is a way to get the same graphs as
above for the regression or count portion of a zero-altered poisson model
that was fit using the MCMCglmm package. Here is a sample model. If it's
helpful, the prior being used to model includes both random intercept and
random slopes for both submodels (i.e., regression and logit) of the
zero-altered model.

 

days.zaglmm3 <- MCMCglmm(Estimate_Days ~ -1 + trait*(Time), 

                         data = vse, family = "zapoisson",

                         random = ~us(at.level(trait,1) +

                                        at.level(trait,1):Time):id + 

                           us(at.level(trait,2) +

                                at.level(trait,2):Time):id,

                         rcov = ~ idh(trait):units,

                         prior = zi.prior3,

                         #nitt = 1050000, burnin = 50000, thin = 1000,

                         verbose = TRUE, pr = TRUE, pl = FALSE)

 

I would appreciate any suggestions for either of these scenarios.
Alternatively, if you have suggestions that would better show the group
trends over time to an audience that is not statistically savvy and simply
wants to visualize the outcome change over time, I would also appreciate it.

 

Thank you,

Igor

 

 

 


	[[alternative HTML version deleted]]


From lizestats at gmail.com  Mon Mar 21 15:33:39 2016
From: lizestats at gmail.com (Lize van der merwe)
Date: Mon, 21 Mar 2016 16:33:39 +0200
Subject: [R-sig-ME] Multiple binary responses per ID and time
Message-ID: <007a01d1837e$aab1c4f0$00154ed0$@gmail.com>

Dear List, 

Please advise.    I cannot get my head around modelling this data.

Study involves 200 individuals with several (not always the same number)
dichotomous outcomes, at 10 different times.  The predictor also has several
(not the same as each other, nor the same as what the individal has at that
time-point) dichotomous outcomes for the same individuals at the the same
timepoints.  There are time-level covariates and also individual level
covariates to include. 

How do I model these?   Not even sure how to lay out the data.  

Binomial pair (x,n) outcome,  for each individual and each time and another
binomial pair for the predictor?

Regards

Lize van der Merwe

 


From thierry.onkelinx at inbo.be  Mon Mar 21 16:29:51 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 21 Mar 2016 16:29:51 +0100
Subject: [R-sig-ME] Multiple binary responses per ID and time
In-Reply-To: <007a01d1837e$aab1c4f0$00154ed0$@gmail.com>
References: <007a01d1837e$aab1c4f0$00154ed0$@gmail.com>
Message-ID: <CAJuCY5xM+wBTDBnQRkC2XjHipmf4vv4-R1DurgmNELpqnqVFKw@mail.gmail.com>

Dear Lize,

So the response for individual i at time i is (x_it, n_it). Does the
predictor has the same amount of trials as the responses (y_it, n_it)? If
so, do you have information on the n_it Bernouilli trials of both the
response and the predictor? If that is the case then you can model the
individual Bernoulli trials.

If you don't have the information at that detail, then you have to turn the
binomial predictor into a proportion. With a Bayesian hierarchical model
you can first model the predictor and then uses this modelled proportion as
a predictor for the response. There is an example in the INLA FAQ:
http://www.r-inla.org/faq#TOC-Can-I-have-the-linear-predictor-from-one-model-as-a-covariate-in-a-different-model-

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-03-21 15:33 GMT+01:00 Lize van der merwe <lizestats at gmail.com>:

> Dear List,
>
> Please advise.    I cannot get my head around modelling this data.
>
> Study involves 200 individuals with several (not always the same number)
> dichotomous outcomes, at 10 different times.  The predictor also has
> several
> (not the same as each other, nor the same as what the individal has at that
> time-point) dichotomous outcomes for the same individuals at the the same
> timepoints.  There are time-level covariates and also individual level
> covariates to include.
>
> How do I model these?   Not even sure how to lay out the data.
>
> Binomial pair (x,n) outcome,  for each individual and each time and another
> binomial pair for the predictor?
>
> Regards
>
> Lize van der Merwe
>
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From martina.giovannella at icfo.es  Mon Mar 21 16:58:49 2016
From: martina.giovannella at icfo.es (Martina Giovannella)
Date: Mon, 21 Mar 2016 16:58:49 +0100
Subject: [R-sig-ME] lme with interaction: how to decide which treatment is
 more efficaceous? how to deal with p value column of the lme summary?
Message-ID: <56F01A39.9020202@icfo.es>


I am using an lme to check the effect of 3 types of treatment (A,B, C) 
on my variable, over time. I do not expect any effect of treatment A, 
which is a fake treatment.

I have measurement over 5 periods,on 2 sides.

My question is: for each treatment and each period, is my variable 
different from the reference point (treatment A, period 1)?

My model is

             lme(var ~ treatment*period+side,
            method="ML",
            random=list(IDlog=~1), na.action=na.omit,
            data=changes)

This is the anova of the model

     numDF denDF  F-value p-value
     (Intercept)          1   473 79.36094  <.0001
     treatment            2   473  3.49473  0.0311
     period               4   473 12.51296  <.0001
     side                 1   473 12.16210  0.0005
     treatment:period     8   473  2.02865  0.0416

and the summary for fixed effects

     Fixed effects: var ~ treatment * period + hemisphere
                        Value Std.Error  DF   t-value p-value
     (Intercept)         4.622038  3.180983 473  1.453022  0.1469
     treatmentB         -1.376755  3.703398 473 -0.371754  0.7102
     treatmentC         -1.113021  3.703398 473 -0.300540  0.7639
     period2             3.799946  4.168792 473  0.911522  0.3625
     period3             6.124463  4.168792 473  1.469122  0.1425
     period4             4.309267  4.168792 473  1.033697  0.3018
     period5             5.482068  4.168792 473  1.315026  0.1891
     sideright          -4.278672  1.226887 473 -3.487420  0.0005
     treatmentB:period2  4.059350  5.190104 473  0.782133  0.4345
     treatmentC:period2  7.508426  5.190104 473  1.446681  0.1486
     treatmentB:period3  1.965207  5.190104 473  0.378645  0.7051
     treatmentC:period3  5.312525  5.190104 473  1.023587  0.3066
     treatmentB:period4  9.031016  5.190104 473  1.740045  0.0825
     treatmentC:period4  7.819397  5.190104 473  1.506597  0.1326
     treatmentB:period5 13.620365  5.190104 473  2.624295  0.0090
     treatmentC:period5  4.224340  5.215896 473  0.809897  0.4184

The anova states the treatment has an effect, period as well, and there 
is an interaction between treatment and period. Sides are different. 
This is what I expected.

How can I use the p values columns to state for which treatment and 
which period I see a difference from the reference value?

What I would like to retrieve is the numbers I obtain when I test each 
treatment separately. I know how to obtain the mean, what I have to sum 
up in the value column. What about the p values?

Treatment A

     Fixed effects: var ~ period + hemisphere
                     Value Std.Error DF    t-value p-value
     (Intercept)      1.849555  3.768596 94  0.4907810  0.6247
     period2          3.799946  4.174739 94  0.9102236  0.3650
     period3          6.124463  4.174739 94  1.4670291  0.1457
     period4          4.309267  4.174739 94  1.0322244  0.3046
     period5          5.482068  4.174739 94  1.3131524  0.1923
     sideright       -1.142925  2.640337 94 -0.4328710  0.6661

Anova

     numDF denDF  F-value p-value
     (Intercept)     1    94 4.967367  0.0282
     period          4    94 0.654946  0.6248
     side            1    94 0.187377  0.6661

Treatment B

     Fixed effects: var ~ period + hemisphere
                     Value Std.Error  DF   t-value p-value
     (Intercept)      4.674863  2.938724 175  1.590780  0.1135
     period2          7.859296  3.241315 175  2.424724  0.0163
     period3          8.089670  3.241315 175  2.495799  0.0135
     period4         13.340283  3.241315 175  4.115700  0.0001
     period5         19.102433  3.241315 175  5.893420  0.0000
     sideright       -7.137831  2.049988 175 -3.481889  0.0006

Anova

             numDF denDF  F-value p-value
     (Intercept)     1   175 34.37766  <.0001
     period          4   175  9.60042  <.0001
     side            1   175 12.12355   6e-04

Treatment C

     Fixed effects: var ~ period + hemisphere
                     Value Std.Error  DF   t-value p-value
      (Intercept)      2.936039  2.166417 173  1.355251  0.1771
     period2         11.308372  2.392119 173  4.727346  0.0000
     period3         11.436988  2.392119 173  4.781112  0.0000
     period4         12.128664  2.392119 173  5.070260  0.0000
     period5          9.949632  2.428331 173  4.097314  0.0001
     sideright       -3.132715  1.520530 173 -2.060278  0.0409

Anova

            numDF denDF  F-value p-value
     (Intercept)     1   173 58.13208  <.0001
     period          4   173  8.99083  <.0001
     side            1   173  4.24474  0.0409

 From this test it is very clear that for treatment A, var is not bigger 
in any period respect to the reference point, while it is for treatment 
B and C; right side smaller than left.

I want to obtain this information using the first model, with the 3 
treatments.


From lizestats at gmail.com  Mon Mar 21 17:29:40 2016
From: lizestats at gmail.com (Lize van der merwe)
Date: Mon, 21 Mar 2016 18:29:40 +0200
Subject: [R-sig-ME] Multiple binary responses per ID and time
In-Reply-To: <CAJuCY5xM+wBTDBnQRkC2XjHipmf4vv4-R1DurgmNELpqnqVFKw@mail.gmail.com>
References: <007a01d1837e$aab1c4f0$00154ed0$@gmail.com>
	<CAJuCY5xM+wBTDBnQRkC2XjHipmf4vv4-R1DurgmNELpqnqVFKw@mail.gmail.com>
Message-ID: <00ab01d1838e$dfcf1420$9f6d3c60$@gmail.com>

Thank you for the quick response, Thierry.

Yes, unfortunately the response n_it_j does not equal the predictor n_it_j.

I will go the INLA route. 

Thanks again

Lize

 

 

 

 

From: Thierry Onkelinx [mailto:thierry.onkelinx at inbo.be] 
Sent: Monday, 21 March 2016 17:30
To: Lize van der merwe
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Multiple binary responses per ID and time

 

Dear Lize,

 

So the response for individual i at time i is (x_it, n_it). Does the predictor has the same amount of trials as the responses (y_it, n_it)? If so, do you have information on the n_it Bernouilli trials of both the response and the predictor? If that is the case then you can model the individual Bernoulli trials.

 

If you don't have the information at that detail, then you have to turn the binomial predictor into a proportion. With a Bayesian hierarchical model you can first model the predictor and then uses this modelled proportion as a predictor for the response. There is an example in the INLA FAQ: http://www.r-inla.org/faq#TOC-Can-I-have-the-linear-predictor-from-one-model-as-a-covariate-in-a-different-model-

 

Best regards,

 

Thierry




ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner 
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey

 

2016-03-21 15:33 GMT+01:00 Lize van der merwe <lizestats at gmail.com <mailto:lizestats at gmail.com> >:

Dear List,

Please advise.    I cannot get my head around modelling this data.

Study involves 200 individuals with several (not always the same number)
dichotomous outcomes, at 10 different times.  The predictor also has several
(not the same as each other, nor the same as what the individal has at that
time-point) dichotomous outcomes for the same individuals at the the same
timepoints.  There are time-level covariates and also individual level
covariates to include.

How do I model these?   Not even sure how to lay out the data.

Binomial pair (x,n) outcome,  for each individual and each time and another
binomial pair for the predictor?

Regards

Lize van der Merwe



_______________________________________________
R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org>  mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Tue Mar 22 11:26:59 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 22 Mar 2016 11:26:59 +0100
Subject: [R-sig-ME] lme with interaction: how to decide which treatment
 is more efficaceous? how to deal with p value column of the lme summary?
In-Reply-To: <56F01A39.9020202@icfo.es>
References: <56F01A39.9020202@icfo.es>
Message-ID: <CAJuCY5zNCucWTg9NyHm0Gt9tQ1pEOOJGCzxwQ=Xn5A7kHpktUw@mail.gmail.com>

Dear Martina,

I would reparametrise the model.

lme(var ~ treatment + treatment:period+treatment:side,
           method="ML",
           random=list(IDlog=~1), na.action=na.omit,
           data=changes)

Another option is to use glht() from the multcomp package and specify
specific contrasts.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-03-21 16:58 GMT+01:00 Martina Giovannella via R-sig-mixed-models <
r-sig-mixed-models at r-project.org>:

>
> I am using an lme to check the effect of 3 types of treatment (A,B, C) on
> my variable, over time. I do not expect any effect of treatment A, which is
> a fake treatment.
>
> I have measurement over 5 periods,on 2 sides.
>
> My question is: for each treatment and each period, is my variable
> different from the reference point (treatment A, period 1)?
>
> My model is
>
>             lme(var ~ treatment*period+side,
>            method="ML",
>            random=list(IDlog=~1), na.action=na.omit,
>            data=changes)
>
> This is the anova of the model
>
>     numDF denDF  F-value p-value
>     (Intercept)          1   473 79.36094  <.0001
>     treatment            2   473  3.49473  0.0311
>     period               4   473 12.51296  <.0001
>     side                 1   473 12.16210  0.0005
>     treatment:period     8   473  2.02865  0.0416
>
> and the summary for fixed effects
>
>     Fixed effects: var ~ treatment * period + hemisphere
>                        Value Std.Error  DF   t-value p-value
>     (Intercept)         4.622038  3.180983 473  1.453022  0.1469
>     treatmentB         -1.376755  3.703398 473 -0.371754  0.7102
>     treatmentC         -1.113021  3.703398 473 -0.300540  0.7639
>     period2             3.799946  4.168792 473  0.911522  0.3625
>     period3             6.124463  4.168792 473  1.469122  0.1425
>     period4             4.309267  4.168792 473  1.033697  0.3018
>     period5             5.482068  4.168792 473  1.315026  0.1891
>     sideright          -4.278672  1.226887 473 -3.487420  0.0005
>     treatmentB:period2  4.059350  5.190104 473  0.782133  0.4345
>     treatmentC:period2  7.508426  5.190104 473  1.446681  0.1486
>     treatmentB:period3  1.965207  5.190104 473  0.378645  0.7051
>     treatmentC:period3  5.312525  5.190104 473  1.023587  0.3066
>     treatmentB:period4  9.031016  5.190104 473  1.740045  0.0825
>     treatmentC:period4  7.819397  5.190104 473  1.506597  0.1326
>     treatmentB:period5 13.620365  5.190104 473  2.624295  0.0090
>     treatmentC:period5  4.224340  5.215896 473 0.809897  0.4184
>
> The anova states the treatment has an effect, period as well, and there is
> an interaction between treatment and period. Sides are different. This is
> what I expected.
>
> How can I use the p values columns to state for which treatment and which
> period I see a difference from the reference value?
>
> What I would like to retrieve is the numbers I obtain when I test each
> treatment separately. I know how to obtain the mean, what I have to sum up
> in the value column. What about the p values?
>
> Treatment A
>
>     Fixed effects: var ~ period + hemisphere
>                     Value Std.Error DF    t-value p-value
>     (Intercept)      1.849555  3.768596 94  0.4907810  0.6247
>     period2          3.799946  4.174739 94  0.9102236  0.3650
>     period3          6.124463  4.174739 94  1.4670291  0.1457
>     period4          4.309267  4.174739 94  1.0322244  0.3046
>     period5          5.482068  4.174739 94  1.3131524  0.1923
>     sideright       -1.142925  2.640337 94 -0.4328710  0.6661
>
> Anova
>
>     numDF denDF  F-value p-value
>     (Intercept)     1    94 4.967367  0.0282
>     period          4    94 0.654946  0.6248
>     side            1    94 0.187377  0.6661
>
> Treatment B
>
>     Fixed effects: var ~ period + hemisphere
>                     Value Std.Error  DF   t-value p-value
>     (Intercept)      4.674863  2.938724 175 1.590780  0.1135
>     period2          7.859296  3.241315 175  2.424724  0.0163
>     period3          8.089670  3.241315 175  2.495799  0.0135
>     period4         13.340283  3.241315 175  4.115700  0.0001
>     period5         19.102433  3.241315 175  5.893420  0.0000
>     sideright       -7.137831  2.049988 175 -3.481889  0.0006
>
> Anova
>
>             numDF denDF  F-value p-value
>     (Intercept)     1   175 34.37766  <.0001
>     period          4 175 9.60042  <.0001
>     side            1   175 12.12355   6e-04
>
> Treatment C
>
>     Fixed effects: var ~ period + hemisphere
>                     Value Std.Error  DF   t-value p-value
>      (Intercept)      2.936039  2.166417 173  1.355251  0.1771
>     period2         11.308372  2.392119 173  4.727346  0.0000
>     period3         11.436988  2.392119 173  4.781112  0.0000
>     period4         12.128664  2.392119 173  5.070260  0.0000
>     period5          9.949632  2.428331 173  4.097314  0.0001
>     sideright       -3.132715  1.520530 173 -2.060278  0.0409
>
> Anova
>
>            numDF denDF  F-value p-value
>     (Intercept)     1   173 58.13208  <.0001
>     period          4 173 8.99083  <.0001
>     side            1   173  4.24474  0.0409
>
> From this test it is very clear that for treatment A, var is not bigger in
> any period respect to the reference point, while it is for treatment B and
> C; right side smaller than left.
>
> I want to obtain this information using the first model, with the 3
> treatments.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From epalmery at cns.umass.edu  Wed Mar 23 22:12:32 2016
From: epalmery at cns.umass.edu (Evan Palmer-Young)
Date: Wed, 23 Mar 2016 17:12:32 -0400
Subject: [R-sig-ME] Model for Preference analysis
Message-ID: <CAAge6+46cwSqkagWB9ErOz8jLPmg_OTXSuELR+Q=hfvimuFwYQ@mail.gmail.com>

Dear Colleagues,
I am trying to analyze an experiment comparing consumption of 2 food types
in a choice-type design. Subjects were offered a choice of 2 food types,
and their daily consumption of each food type was recorded. Each subject
also was assigned to one of 2 Infection treatments.
We want to know-- "
*How does infection affect preference for one food type over another?*

I am wondering if it is acceptable to model this like a repeated measures
design, with 2 observations per subject and timepoint-- one for each food
type. Or is that not acceptable because the 2 food types were given at the
same time?

I attached my actual data and script, and also paste a code to generate a
random data set, to show the general data structure.

####choice analysis for list###
Df<-expand.grid(Time=seq(0,10,1),
        Subject=c("Honeybunch","Buttercup","Rosy",
                          "Sting", "Buzz", "Bumble"))
View(Df)
Df$Infection<-NULL
Df$Infection[1:33]<-"Infected"
Df$Infection[34:66]<-"Uninfected"
Df$Infection

Df$Consumption.Food.A<-rnorm(n=length(Df$Time),mean=30, sd=6)
Df$Consumption.Food.B<-rnorm(n=length(Df$Time),mean=25, sd=8)

library(lme4)
##Analysis with consumption of other food type as covariate
ModelA<-lmer(Consumption.Food.A~ Time + Infection + Consumption.Food.B +
               (1|Subject),data=Df)
ModelB<-lmer(Consumption.Food.B~ Time + Infection + Consumption.Food.A +
             (1|Subject), data=Df)

#option to convert data to long format
library(tidyr)
Df_long<-gather(data=Df, key=Foodtype, value=Amt.eaten,
                Consumption.Food.A,Consumption.Food.B)
View(Df_long)
#Is this model legal, because both foods offered simulateneously
Fullmodel<-lmer(Amt.eaten ~ Foodtype * Infection + Time +
                  (1|Subject) + (1|Time), data=Df_long)



(Originally I was planning to model proportions of the 2 food types eaten,
but there a lot of values close to zero that could make the proportion
estimates highly variable. And the proportional analysis would not account
for the strong trends of decreasing with (a) time and (b) infection
treatment)

Thanks very much for your advice!
Evan



-- 
Department of Biology
221 Morrill Science Center
611 North Pleasant St
Amherst MA 01003
https://sites.google.com/a/cornell.edu/evan-palmer-young/

From fromnorden at gmail.com  Thu Mar 24 19:36:47 2016
From: fromnorden at gmail.com (Andreu Ferrero)
Date: Thu, 24 Mar 2016 19:36:47 +0100
Subject: [R-sig-ME] =?utf-8?q?=C2=BFCan_I_do_an_internal_validation_of_glm?=
 =?utf-8?q?er=28=29=3F_=C2=BFBy_glmer=28=29_-=3E_corrected_AUC_optimism_by?=
 =?utf-8?q?_bootstraping_technic_bootMer=28=29_=5Binternal_validation_of_a?=
 =?utf-8?q?_mixed-effects-model=5D=3F?=
References: <0AC52DE0-B964-4A18-9106-0CF7C8B6A77D@gmail.com>
Message-ID: <14C025D2-6972-4F64-9484-5476B0DD3B86@gmail.com>


? Can I do an internal validation in a mixed-effect-model?

Here is some code:

> 
> 
> 
>> 
>> 
>> I would like to do an internal validation of a discriminative ability of a mixed effects models.
>> 
>> Here is my scrip:
>> 
>> ###########################
>> ####bootMer-> boot AUC#####
>> ###########################
>> 
>> library(lme4)
>> library(lattice)
library(boot)
>> data(cbpp)
>> 
>> #fit a model
>> 
>> cbpp$Y<-cbpp$incidence>=1
>> glmm<-glmer(Y~period + size + (1|herd), family=binomial, data=cbpp)
>> glmm
>> 
>> ##### funcio: versio 3 - no cal posar endpoint en la funcio
>> ##########################################################
>> 
>> 
>> 
>> AUCFun <- function(fit) {
>> library(pROC)
>> pred<-predict(fit, type="response")
>> AUC<-as.numeric(auc(fit at resp$y, pred))
>> }
>> 
>> 
>> #test
>> 
>> (AUCFun(glmm))
>> 
>> ###run bootMer: AUCFun
>> 
>> 
>> 
>> system.time(AUC.boot <- bootMer(glmm,nsim=100,FUN=AUCFun,seed=1982, use.u=TRUE,
>>                               type="parametric", parallel="multicore", ncpus=2))
>> 
>> 
>> #...
>> 
>> (boot.ci(AUC.boot, index =c(1,1), type="norm"))
>> 
>> roc(cbpp$Y, predict(glmm, type="response"))
>> 
>> 
>> #Now it seems more reasonable, bias as "optimism"... but still do not know #if I am just doing a AUC with bootstrap CI 
>> **************************************************************************************************************************************
> 


From iyakoven at ucalgary.ca  Fri Mar 25 05:20:27 2016
From: iyakoven at ucalgary.ca (Igor Yakovenko)
Date: Fri, 25 Mar 2016 00:20:27 -0400
Subject: [R-sig-ME] lme4 categorical outcome probability values at each time
	point
Message-ID: <4a2c501d1864d$ad93bf80$08bb3e80$@ucalgary.ca>

Hello Everyone,

 

Does anyone know how to graph the results of a lme4 longitudinal multilevel
model as a bar graph of Time (3 points) on x-axis with probability of
getting the 1 value on the outcome after taking into account the predictors
as the y-axis? Alternatively, if there is a way to derive the mean
probability of the outcome being 1 (vs. 0) during an intervention at time 1,
2 and 3 divided by Condition (two levels), I can graph those values
separately. The model is based on data from a two-group RCT evaluating an
intervention over 3 time points. Here is a sample model with a dichotomous
outcome and fixed effects of time (continuous), condition (2 levels) and
whether an additional workshop was completed (2 levels).

 

workshop<-glmer(CurrentAsst~Time*Condition + Workshop_completion_reordered +
(1|id), data = vse, family = binomial, nAGQ = 25, control =
glmerControl(optimizer = "bobyqa"))

 

Thank you,

Igor Yakovenko


	[[alternative HTML version deleted]]


From epalmery at cns.umass.edu  Fri Mar 25 10:46:47 2016
From: epalmery at cns.umass.edu (Evan Palmer-Young)
Date: Fri, 25 Mar 2016 05:46:47 -0400
Subject: [R-sig-ME] lme4 categorical outcome probability values at each
 time point
In-Reply-To: <4a2c501d1864d$ad93bf80$08bb3e80$@ucalgary.ca>
References: <4a2c501d1864d$ad93bf80$08bb3e80$@ucalgary.ca>
Message-ID: <CAAge6+6RjjDyPXSwNDVEGoFNeQ1ZvOdYUL2prEGW8cZX4r4cOw@mail.gmail.com>

Dear Igor,
Am new to the list, so defer by all means to more experienced opinions.
My suggestion would be to use the "lsmeans" package and the inverse logit
transformation
(to get back to the scale of probabilities from the hard-to-interpret
logits)

If you have time as a factor this will be no problem.
So before you run the model, specify
vse$Time<-as.factor(vse$Time)
#then use lsmeans by R Lenth
#see
https://cran.r-project.org/web/packages/lsmeans/vignettes/using-lsmeans.pdf
#example
# lsmeans(oranges.lm1, "day", at = list(price1 = 50, price2 = c(40,60), day
= c("2","3","4")) )
Newdf<- summary(lsmeans(workshop, ~Time*Condition))
#or if Time is numeric:
Newdf<- summary(lsmeans(workshop, ~Time*Condition, at=list(Time=c(1, 2, 3))

#if you need to graph standard errors, create a new column with raw std
errors first,
#then transform the mean +/- SE
Newdf$sehi<-Newdf$lsmean+ Newdf$SE
Newdf$selo<-Newdf$lsmeans - Newdf$SE
#now apply inverse logit
library(boot)
Newdf$meanprop<-inv.logit (Newdf$lsmean)
Newdf$prophi<-inv.logit(Newdf$sehi)
Newdf$proplo<-inv.logit(Newdf$selo)

You should be able to plot the resulting variables using ggplot2 or your
package of choice.
You could also use inv.logit on the 95% confidence limits, though the error
bars tend to get pretty enormous with these reverse transformations and may
not seem to reflect any significant differences in your model.
Hope this helps!
Evan


On Fri, Mar 25, 2016 at 12:20 AM, Igor Yakovenko <iyakoven at ucalgary.ca>
wrote:

> Hello Everyone,
>
>
>
> Does anyone know how to graph the results of a lme4 longitudinal multilevel
> model as a bar graph of Time (3 points) on x-axis with probability of
> getting the 1 value on the outcome after taking into account the predictors
> as the y-axis? Alternatively, if there is a way to derive the mean
> probability of the outcome being 1 (vs. 0) during an intervention at time
> 1,
> 2 and 3 divided by Condition (two levels), I can graph those values
> separately. The model is based on data from a two-group RCT evaluating an
> intervention over 3 time points. Here is a sample model with a dichotomous
> outcome and fixed effects of time (continuous), condition (2 levels) and
> whether an additional workshop was completed (2 levels).
>
>
>
> workshop<-glmer(CurrentAsst~Time*Condition + Workshop_completion_reordered
> +
> (1|id), data = vse, family = binomial, nAGQ = 25, control =
> glmerControl(optimizer = "bobyqa"))
>
>
>
> Thank you,
>
> Igor Yakovenko
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
Department of Biology
221 Morrill Science Center
611 North Pleasant St
Amherst MA 01003
https://sites.google.com/a/cornell.edu/evan-palmer-young/

	[[alternative HTML version deleted]]


From marcus.michelangeli at monash.edu  Thu Mar 17 08:03:47 2016
From: marcus.michelangeli at monash.edu (Marcus Michelangeli)
Date: Thu, 17 Mar 2016 18:03:47 +1100
Subject: [R-sig-ME] MCMCglmm and proportion time data
Message-ID: <CAJGZ2nowE+y5E=zbW_L92_qeKjUWWzB0RKPOVMb7aq=MJQ-U=g@mail.gmail.com>

Hello,

Apologies if a similar question has been asked and answered before, but any
help would be extremely appreciated

I measured 4 behavioral traits in the same test over a period of 20
minutes. Each individual went through the test three times so I could
estimate repeatability.

EXAMPLE DATASET

SKINK       TRIAL POPULATION       REGION ACTIVE  BARRIER SHELTER  STILL

1 SKINK22     1 Kuring-gai Chase      N   1134.08  575.38    0.00
630.90

2 SKINK22     2 Kuring-gai Chase      N   246.28   59.61    657.57
896.12

3 SKINK22     3 Kuring-gai Chase      N   281.60   24.68    118.87
1381.20

4 SKINK23     1 Kuring-gai Chase      N   0.00    0.00     1799.97
0.00

5 SKINK23     2 Kuring-gai Chase      N   279.89    0.00    634.61
885.47

6 SKINK23     3 Kuring-gai Chase      N   20.77    0.00     1243.55 535.65




When looking at histograms of each behaviour, the data is heavily skewed to
the left and is obviously non-normally distributed (all distributions look
exponential)

I'm not sure how to tackle this. I basically just want to estimate the
repeatability of each behavioural trait and look at the effects of Region
and Population on behaviour. I'm think my best option is to convert the
data into proportions (i.e. what proportion of time did the individual
spend doing each behavior over the 20 mins) and then run a GLMM (preferably
using MCMCglmm in R or glmer) with a binomial distribution. My questions are

1. Would this be the best way to tackle this data?
2. What distribution should I use in MCMCglmm (is it family "categorical")?
3. Do binomial distributions require different priors. I usually just use
an uninformative prior (V =1, nu =0.002)



Thanks for your time
Marcus

	[[alternative HTML version deleted]]


From urs at kleinholdermann.de  Wed Mar 30 02:32:27 2016
From: urs at kleinholdermann.de (urs)
Date: Wed, 30 Mar 2016 02:32:27 +0200
Subject: [R-sig-ME] How to extend an existing linear mixed effects model
	using lme in R?
Message-ID: <56FB1E9B.9090804@kleinholdermann.de>

Dear list,

I fitted a linear mixed effects model using lme in R on a relatively 
large data set. Now I want to extend this model, introducing an 
additional fixed effect on a subset of the data without altering 
coefficients estimated on the original dataset. I.e.

original model for example:
m1 = lme(y~a+b,random=~1|id,data=D)

now I want to do something similiar to
m2 = update(m1,~.+c,data=d)

Where d is a subset of D. The values of the new predictor c are only 
available for this subset. However, I would like to keep the originally 
estimated coefficients of the model m1 with regard to predictors a and 
b. If I use update as described above coefficients for all predictors 
(a,b,c) are estimated again on the smaller dataset d. Any suggestions on 
how I can estimate the effect of c while keeping the old coefficient 
values for a and b? Or is this for some reason a bad idea altogether?

Urs


From thierry.onkelinx at inbo.be  Wed Mar 30 09:23:02 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 30 Mar 2016 09:23:02 +0200
Subject: [R-sig-ME] How to extend an existing linear mixed effects model
 using lme in R?
In-Reply-To: <56FB1E9B.9090804@kleinholdermann.de>
References: <56FB1E9B.9090804@kleinholdermann.de>
Message-ID: <CAJuCY5yJyxAydLAiWRkKvBUjDgfiBouzVxdbkoV8DSJ6G6NoMw@mail.gmail.com>

Dear Urs,

What you can do is first get the predictions from m1 for dataset d and add
it as a variable to dataset d. Then fit a model on dataset d using the
prediction from m1 as an offset.

d$M1 <- predict(m1, newdata = d, level = 0)
m2 <- lme(y ~ offset(M1) + c, random = ~1|id, data = d)

or

d$M1 <- predict(m1, newdata = d, level = 1)
m2 <- lm(y ~ offset(M1) + c, data = d)

fitting m2 <- lme(y ~ a + b + c, random = ~1|id, data = d) will probably
give a better fit.

Best regards,

Thierry


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-03-30 2:32 GMT+02:00 urs <urs at kleinholdermann.de>:

> Dear list,
>
> I fitted a linear mixed effects model using lme in R on a relatively large
> data set. Now I want to extend this model, introducing an additional fixed
> effect on a subset of the data without altering coefficients estimated on
> the original dataset. I.e.
>
> original model for example:
> m1 = lme(y~a+b,random=~1|id,data=D)
>
> now I want to do something similiar to
> m2 = update(m1,~.+c,data=d)
>
> Where d is a subset of D. The values of the new predictor c are only
> available for this subset. However, I would like to keep the originally
> estimated coefficients of the model m1 with regard to predictors a and b.
> If I use update as described above coefficients for all predictors (a,b,c)
> are estimated again on the smaller dataset d. Any suggestions on how I can
> estimate the effect of c while keeping the old coefficient values for a and
> b? Or is this for some reason a bad idea altogether?
>
> Urs
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Wed Mar 30 09:47:52 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 30 Mar 2016 08:47:52 +0100
Subject: [R-sig-ME] MCMCglmm and proportion time data
In-Reply-To: <CAJGZ2nowE+y5E=zbW_L92_qeKjUWWzB0RKPOVMb7aq=MJQ-U=g@mail.gmail.com>
References: <CAJGZ2nowE+y5E=zbW_L92_qeKjUWWzB0RKPOVMb7aq=MJQ-U=g@mail.gmail.com>
Message-ID: <56FB84A8.8030903@ed.ac.uk>

Hi,

The Dirichlet distribution is probably most appropriate for a 
continous-valued response, but I guess you could discretise it and treat 
it as multinomial. I'm not sure how the results will change depending on 
the scale at which you discretize - hopefully they will just go up by a 
factor but I'm not sure.  The family would be "multinomial4". The model 
is set up like a multivariate model so you need to use trait to define 
the different reposnes. Note that that there are only 3 traits, because 
they are treated as contrasts from the base-line category (the first 
response). The R-structure defined by us(trait):units is parameterised 
by a 3x3 matrix, all elements of which are idenitifiable.

Alternativley you can define a base-line category (it doesn't matter 
which, lets say STILL) and take the log ratio of each cetogory to the 
base-line. This could then be treated as  multivariate normal. The 
problem then will dealing with the zeros. You may want to check out 
Aitchinson's book on compsitional data.

Cheers,

Jarrod

On 17/03/2016 07:03, Marcus Michelangeli wrote:
> Hello,
>
> Apologies if a similar question has been asked and answered before, but any
> help would be extremely appreciated
>
> I measured 4 behavioral traits in the same test over a period of 20
> minutes. Each individual went through the test three times so I could
> estimate repeatability.
>
> EXAMPLE DATASET
>
> SKINK       TRIAL POPULATION       REGION ACTIVE  BARRIER SHELTER  STILL
>
> 1 SKINK22     1 Kuring-gai Chase      N   1134.08  575.38    0.00
> 630.90
>
> 2 SKINK22     2 Kuring-gai Chase      N   246.28   59.61    657.57
> 896.12
>
> 3 SKINK22     3 Kuring-gai Chase      N   281.60   24.68    118.87
> 1381.20
>
> 4 SKINK23     1 Kuring-gai Chase      N   0.00    0.00     1799.97
> 0.00
>
> 5 SKINK23     2 Kuring-gai Chase      N   279.89    0.00    634.61
> 885.47
>
> 6 SKINK23     3 Kuring-gai Chase      N   20.77    0.00     1243.55 535.65
>
>
>
>
> When looking at histograms of each behaviour, the data is heavily skewed to
> the left and is obviously non-normally distributed (all distributions look
> exponential)
>
> I'm not sure how to tackle this. I basically just want to estimate the
> repeatability of each behavioural trait and look at the effects of Region
> and Population on behaviour. I'm think my best option is to convert the
> data into proportions (i.e. what proportion of time did the individual
> spend doing each behavior over the 20 mins) and then run a GLMM (preferably
> using MCMCglmm in R or glmer) with a binomial distribution. My questions are
>
> 1. Would this be the best way to tackle this data?
> 2. What distribution should I use in MCMCglmm (is it family "categorical")?
> 3. Do binomial distributions require different priors. I usually just use
> an uninformative prior (V =1, nu =0.002)
>
>
>
> Thanks for your time
> Marcus
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From jackiewood7 at gmail.com  Thu Mar 31 16:14:41 2016
From: jackiewood7 at gmail.com (Jackie Wood)
Date: Thu, 31 Mar 2016 10:14:41 -0400
Subject: [R-sig-ME] Complex model yields similar results to simpler model,
 but also warnings: can I ignore them?
Message-ID: <CAOxxGRm6qg-o0deD-PaUoxrr_Xkc2qZuntoeDez2Zsq+O2q7RA@mail.gmail.com>

Hello R-users,

I'm attempting to model differences in fork length over time for 4
different cross types of a species of freshwater fish using the most recent
version of nlme . Examining plots of the data, fork length increases
non-linearly over time so I've included a second order polynomial for age
such that the fixed effects portion of the model has the following
specification:

model <- lme(FL ~ cross * age_bin + cross*I(age_bin^2)

Plots of the random effects suggest evidence for random slopes with respect
to family for age and age^2, and further these are correlated with the
intercept.

So I specified the random effects part as:

random = ~age_bin + I(age_bin^2)|fam

Likelihood ratio tests do favor this random effects structure over simpler
structures.

Plotting the residuals, variance in length definitely increases with
increasing age and also appears to vary per cross type so I added the
following variance weighting structure to the model:

weights = varIdent(form = ~ 1|cross*age_bin))

I've performed typical likelihood ratio tests which consistently favor the
model described above over other simpler model specifications (in terms of
random effects specifications, and variance weighting), but with the above
model I also get a few of these types of warnings:

1: In logLik.reStruct(object, conLin) :
  Singular precision matrix in level -1, block 15

Searching online help forums the advice I see is that the model is likely
overparameterized, and indeed if I remove either the variance weighting
completely, or simplify the random effects to 1|fam (any random slope type
random effects specification gives the same warning), everything works just
fine. I also checked the raw data which seems sound to me.

I do feel as though the more complex random effects structure is warranted
from plotting the data and there is definitely heteroscedasticity to
account for. When I run the above model without the variance weights, the
resulting fixed effects coefficients and estimated random effects and
correlations have values that are pretty close to the model with variance
weights (the residual variance is of course different). So my question is
how important are the warnings? If the output seems reasonable and
corresponds pretty closely with the output of a simpler model that runs
just fine, is it justifiable to ignore the warnings or am I asking for
trouble?

I mean, I could get rid of the variance weighting structure and simply
transform fork length, it does help the heteroscedasticity issue, but I do
find the variance interesting and transforming it away wouldn't be my first
choice.

I'd really appreciate your input!

Jackie

-- 
Jacquelyn L.A. Wood, PhD.
224 Montrose Avenue
Toronto, ON
M6G 3G7
Phone: (514) 293-7255

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Thu Mar 31 16:53:10 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 31 Mar 2016 16:53:10 +0200
Subject: [R-sig-ME] Complex model yields similar results to simpler
 model, but also warnings: can I ignore them?
In-Reply-To: <CAOxxGRm6qg-o0deD-PaUoxrr_Xkc2qZuntoeDez2Zsq+O2q7RA@mail.gmail.com>
References: <CAOxxGRm6qg-o0deD-PaUoxrr_Xkc2qZuntoeDez2Zsq+O2q7RA@mail.gmail.com>
Message-ID: <CAJuCY5yQBWbYJmdqa4bCS7bQs6rCDeOGNGnxJDDZnHu8LVoHJQ@mail.gmail.com>

Dear Jackie,

I presume that the heteroscedasticy along age_bin is somewhat smooth. In
such case you use a less parametrised model the variance. Like e.g.
varExp(form = ~age_bin|cross). ?varClasses gives an overview of available
classes. Note that you can combine classes with varComb().

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-03-31 16:14 GMT+02:00 Jackie Wood <jackiewood7 at gmail.com>:

> Hello R-users,
>
> I'm attempting to model differences in fork length over time for 4
> different cross types of a species of freshwater fish using the most recent
> version of nlme . Examining plots of the data, fork length increases
> non-linearly over time so I've included a second order polynomial for age
> such that the fixed effects portion of the model has the following
> specification:
>
> model <- lme(FL ~ cross * age_bin + cross*I(age_bin^2)
>
> Plots of the random effects suggest evidence for random slopes with respect
> to family for age and age^2, and further these are correlated with the
> intercept.
>
> So I specified the random effects part as:
>
> random = ~age_bin + I(age_bin^2)|fam
>
> Likelihood ratio tests do favor this random effects structure over simpler
> structures.
>
> Plotting the residuals, variance in length definitely increases with
> increasing age and also appears to vary per cross type so I added the
> following variance weighting structure to the model:
>
> weights = varIdent(form = ~ 1|cross*age_bin))
>
> I've performed typical likelihood ratio tests which consistently favor the
> model described above over other simpler model specifications (in terms of
> random effects specifications, and variance weighting), but with the above
> model I also get a few of these types of warnings:
>
> 1: In logLik.reStruct(object, conLin) :
>   Singular precision matrix in level -1, block 15
>
> Searching online help forums the advice I see is that the model is likely
> overparameterized, and indeed if I remove either the variance weighting
> completely, or simplify the random effects to 1|fam (any random slope type
> random effects specification gives the same warning), everything works just
> fine. I also checked the raw data which seems sound to me.
>
> I do feel as though the more complex random effects structure is warranted
> from plotting the data and there is definitely heteroscedasticity to
> account for. When I run the above model without the variance weights, the
> resulting fixed effects coefficients and estimated random effects and
> correlations have values that are pretty close to the model with variance
> weights (the residual variance is of course different). So my question is
> how important are the warnings? If the output seems reasonable and
> corresponds pretty closely with the output of a simpler model that runs
> just fine, is it justifiable to ignore the warnings or am I asking for
> trouble?
>
> I mean, I could get rid of the variance weighting structure and simply
> transform fork length, it does help the heteroscedasticity issue, but I do
> find the variance interesting and transforming it away wouldn't be my first
> choice.
>
> I'd really appreciate your input!
>
> Jackie
>
> --
> Jacquelyn L.A. Wood, PhD.
> 224 Montrose Avenue
> Toronto, ON
> M6G 3G7
> Phone: (514) 293-7255
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From jackiewood7 at gmail.com  Thu Mar 31 18:40:45 2016
From: jackiewood7 at gmail.com (Jackie Wood)
Date: Thu, 31 Mar 2016 12:40:45 -0400
Subject: [R-sig-ME] Complex model yields similar results to simpler
 model, but also warnings: can I ignore them?
In-Reply-To: <CAJuCY5yQBWbYJmdqa4bCS7bQs6rCDeOGNGnxJDDZnHu8LVoHJQ@mail.gmail.com>
References: <CAOxxGRm6qg-o0deD-PaUoxrr_Xkc2qZuntoeDez2Zsq+O2q7RA@mail.gmail.com>
	<CAJuCY5yQBWbYJmdqa4bCS7bQs6rCDeOGNGnxJDDZnHu8LVoHJQ@mail.gmail.com>
Message-ID: <CAOxxGRk1A-C7=Mp1B3stQ6y2REWfzBpSF5qEFJbYk6-Zc-tUaQ@mail.gmail.com>

Hi Thierry,

Thanks for the quick response! I've tried the following variance weight
structures:

vf1 <- varIdent (form = ~ 1 | cross*age_bin)

vf2 <- varExp (form = ~ age_bin | cross)

vf3 <- varComb (varIdent(form =~ 1 | cross), varExp (form = ~ age_bin))

vf4 <- varExp (form = ~ age_bin)

vf2 and vf3 yielded the same warnings as for the original model (using
vf1). vf4 ran fine but  I do think the data indicate that there are some
differences in heteroscedasticity among the crosses as well. The AIC value
was quite a bit lower for the model using vf1 but perhaps that is not
entirely trustworthy given the warnings that were generated.

I guess my original question stands about whether this warning means that
the model is bad news and I should go with something like varExp (form = ~
age_bin) regardless of which specification seems preferred via comparison
of AIC values. Or if, because all other relevant output stays fairly
consistent regardless of the variance weighting specification, I can
proceed with the more complex version.

Jackie





On Thu, Mar 31, 2016 at 10:53 AM, Thierry Onkelinx <thierry.onkelinx at inbo.be
> wrote:

> Dear Jackie,
>
> I presume that the heteroscedasticy along age_bin is somewhat smooth. In
> such case you use a less parametrised model the variance. Like e.g.
> varExp(form = ~age_bin|cross). ?varClasses gives an overview of available
> classes. Note that you can combine classes with varComb().
>
> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-03-31 16:14 GMT+02:00 Jackie Wood <jackiewood7 at gmail.com>:
>
>> Hello R-users,
>>
>> I'm attempting to model differences in fork length over time for 4
>> different cross types of a species of freshwater fish using the most
>> recent
>> version of nlme . Examining plots of the data, fork length increases
>> non-linearly over time so I've included a second order polynomial for age
>> such that the fixed effects portion of the model has the following
>> specification:
>>
>> model <- lme(FL ~ cross * age_bin + cross*I(age_bin^2)
>>
>> Plots of the random effects suggest evidence for random slopes with
>> respect
>> to family for age and age^2, and further these are correlated with the
>> intercept.
>>
>> So I specified the random effects part as:
>>
>> random = ~age_bin + I(age_bin^2)|fam
>>
>> Likelihood ratio tests do favor this random effects structure over simpler
>> structures.
>>
>> Plotting the residuals, variance in length definitely increases with
>> increasing age and also appears to vary per cross type so I added the
>> following variance weighting structure to the model:
>>
>> weights = varIdent(form = ~ 1|cross*age_bin))
>>
>> I've performed typical likelihood ratio tests which consistently favor the
>> model described above over other simpler model specifications (in terms of
>> random effects specifications, and variance weighting), but with the above
>> model I also get a few of these types of warnings:
>>
>> 1: In logLik.reStruct(object, conLin) :
>>   Singular precision matrix in level -1, block 15
>>
>> Searching online help forums the advice I see is that the model is likely
>> overparameterized, and indeed if I remove either the variance weighting
>> completely, or simplify the random effects to 1|fam (any random slope type
>> random effects specification gives the same warning), everything works
>> just
>> fine. I also checked the raw data which seems sound to me.
>>
>> I do feel as though the more complex random effects structure is warranted
>> from plotting the data and there is definitely heteroscedasticity to
>> account for. When I run the above model without the variance weights, the
>> resulting fixed effects coefficients and estimated random effects and
>> correlations have values that are pretty close to the model with variance
>> weights (the residual variance is of course different). So my question is
>> how important are the warnings? If the output seems reasonable and
>> corresponds pretty closely with the output of a simpler model that runs
>> just fine, is it justifiable to ignore the warnings or am I asking for
>> trouble?
>>
>> I mean, I could get rid of the variance weighting structure and simply
>> transform fork length, it does help the heteroscedasticity issue, but I do
>> find the variance interesting and transforming it away wouldn't be my
>> first
>> choice.
>>
>> I'd really appreciate your input!
>>
>> Jackie
>>
>> --
>> Jacquelyn L.A. Wood, PhD.
>> 224 Montrose Avenue
>> Toronto, ON
>> M6G 3G7
>> Phone: (514) 293-7255
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>


-- 
Jacquelyn L.A. Wood, PhD.
224 Montrose Avenue
Toronto, ON
M6G 3G7
Phone: (514) 293-7255

	[[alternative HTML version deleted]]


From amirouche.sadoun at cerco.ups-tlse.fr  Wed Mar 30 11:46:35 2016
From: amirouche.sadoun at cerco.ups-tlse.fr (Amirouche Sadoun)
Date: Wed, 30 Mar 2016 11:46:35 +0200
Subject: [R-sig-ME] Questions concerning glmmADMB and hurdle model
Message-ID: <56FBA07B.10904@cerco.ups-tlse.fr>

Dear Sir,

I'm currently a PhD student in cognitive neurosciences at the CerCo 
laboratory, France.

I'm dealing with a problematic trying to analyze my data with the 
glmmADMB function in R.
I thank you in advance for paying attention to my present problem 
regarding the use of a Hurdle model with glmmADMB, because I didn't yet 
found a solution, especially as I am new in using R.
I'm trying to analyze behavioral data containing a given number of 
trials called ab. (Experimental design with repeated measures.  Fixed 
effects: GROUP and DELAY; random effect: the id of the subjects). 
(Please find the data attached).

The response variable is the number of AB responses related to the Total 
number of trials.
In this data, many subjects had 0 values . This requires the use of a 
Zero inflated or Hurdle model with glmmADMB function (), as it was 
recommended to me.
I tried than to do the analysis with this function, even if it seems not 
clear to me, but I get error messages, in addition to that I don't know 
how to deal with both models after (with post hoc analysis).
Please, find the following lines of the R code and the error messages:

datab $ ID <-factor ($ datab ID)
mod1 <-glmmadmb (cbind (AB, Total-AB) ~ * DELAY GROUP + (1 | ID), data = 
subset (datab, AB> 0), family = "truncnbinom1")
datab $ nz <- as.numeric (datab $ AB> 0)
mod2 <-glmmadmb (nz ~ * DELAY GROUP + (1 | ID), data = datab, family = 
"binomial")

The error message: [1] NOTICE: Warning in eval (expr, envir, pen):
sd.est not defined for this family
[2] ERROR:
The maximizer function failed (could not find parameter file) 
Troubleshooting steps include (1) run with
'Save.dir' set and inspect output files; (2) changes run parameters

I hope wholeheartedly find help

Please accept the assurance of my distinguished regards.

A. SADOUN

CerCo, UMR 5549

Pavillon Baudot

Toulouse 31052 FRANCE
-------------- next part --------------
ID	DELAY	GROUP	Total	AB
1	ph1	K	506	0
2	ph1	K	902	6
3	ph1	K	1503	0
4	ph1	K	1958	8
5	ph1	K	806	7
6	ph1	K	770	0
7	ph1	K	502	2
8	ph1	K	1398	0
9	ph1	K	1874	0
10	ph1	K	1432	6
11	ph1	K	689	5
12	ph1	K	1670	2
13	ph1	K	1860	13
14	ph1	W	2314	82
15	ph1	W	2903	33
16	ph1	W	1034	17
17	ph1	W	705	4
18	ph1	W	4305	75
19	ph1	W	1684	66
20	ph1	W	1188	19
21	ph1	W	880	11
22	ph1	W	304	3
23	ph1	W	2108	41
24	ph1	W	2486	132
25	ph1	W	2581	69
1	ph2	K	690	5
2	ph2	K	683	0
3	ph2	K	802	3
4	ph2	K	705	2
5	ph2	K	758	5
6	ph2	K	769	0
7	ph2	K	604	2
8	ph2	K	860	1
9	ph2	K	801	0
10	ph2	K	702	0
11	ph2	K	764	0
12	ph2	K	870	0
13	ph2	K	588	2
14	ph2	W	803	0
15	ph2	W	550	3
16	ph2	W	689	3
17	ph2	W	716	3
18	ph2	W	549	10
19	ph2	W	766	0
20	ph2	W	654	15
21	ph2	W	703	61
22	ph2	W	590	0
23	ph2	W	641	15
24	ph2	W	784	33
25	ph2	W	699	4
1	ph3	K	201	0
2	ph3	K	198	0
3	ph3	K	168	0
4	ph3	K	197	1
5	ph3	K	198	0
6	ph3	K	196	0
7	ph3	K	203	0
8	ph3	K	187	3
9	ph3	K	193	0
10	ph3	K	195	0
11	ph3	K	201	1
12	ph3	K	195	0
13	ph3	K	178	0
14	ph3	W	199	0
15	ph3	W	205	5
16	ph3	W	203	0
17	ph3	W	199	0
18	ph3	W	192	12
19	ph3	W	201	7
20	ph3	W	189	0
21	ph3	W	203	0
22	ph3	W	210	3
23	ph3	W	204	3
24	ph3	W	197	8
25	ph3	W	181	5

