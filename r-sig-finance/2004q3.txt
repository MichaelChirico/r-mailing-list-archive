From acorrea at utad.pt  Thu Jul  1 16:40:14 2004
From: acorrea at utad.pt (Alessandro Correa)
Date: Thu Jul  1 16:52:28 2004
Subject: [R-sig-finance] TAR
Message-ID: <004801c45f79$62bab020$6b9e88c1@utad.pt>

Hi,
I need to make an estimation by using TAR  (Tsay, 1989)Does anyone know how to do it in R?
Thanks,

 
Alessandro de Castro Corrêa

	[[alternative HTML version deleted]]

From Bernhard.Pfaff at drkw.com  Fri Jul  2 09:51:38 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Fri Jul  2 09:52:22 2004
Subject: [R-sig-finance] TAR
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29B9D4@ibfftce505.de.ad.drkw.net>

> Hi,
> I need to make an estimation by using TAR  (Tsay, 1989)Does 
> anyone know how to do it in R?
> Thanks,
> 

Hello Alessandro,

have you considered the approach proposed in
http://fmwww.bc.edu/EC-P/WP325.pdf?

HTH,
Bernhard

>  
> Alessandro de Castro Correa
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-finance
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From jtl at saxobank.com  Fri Jul  2 10:01:25 2004
From: jtl at saxobank.com (Jeffrey Todd Lins)
Date: Fri Jul  2 10:01:32 2004
Subject: [R-sig-finance] TAR
Message-ID: <DC680F53FAEFC34BB70119CA8C545E6D2D236E@Mal2pro.mid.dom>


If you do implement the TAR estimation in R, I would be very interested in seeing it.


Jeff


Med Venlig Hilsen | Yours sincerely

Jeffrey Todd Lins  | Director - Quantitative Analysis

Saxo Bank A/S | Smakkedalen 2 | DK-2820 Gentofte
Company phone: +45 39 77 40 00 | Direct phone: +45 39 77 40 81 | Fax number: +45 39 77 42 00
Please visit our website at: http://www.saxobank.com <http://www.saxobank.com/>





This email may contain confidential and/or privileged inform...{{dropped}}

From ezivot at u.washington.edu  Fri Jul  2 22:26:15 2004
From: ezivot at u.washington.edu (Eric Zivot)
Date: Fri Jul  2 22:27:21 2004
Subject: [R-sig-finance] TAR
In-Reply-To: <DC680F53FAEFC34BB70119CA8C545E6D2D236E@Mal2pro.mid.dom>
Message-ID: <200407022027.i62KRBhJ007052@mailhost2.u.washington.edu>

While somewhat off topic, The TAR models described in the Hansen paper have
been implemented in Splus for the next release of S+FinMetrics. Also
included will be STAR models and Markov switching models. 
ez

-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Jeffrey Todd
Lins
Sent: Friday, July 02, 2004 1:01 AM
To: r-sig-finance@stat.math.ethz.ch
Subject: [R-sig-finance] TAR


If you do implement the TAR estimation in R, I would be very interested in
seeing it.


Jeff


Med Venlig Hilsen | Yours sincerely

Jeffrey Todd Lins  | Director - Quantitative Analysis

Saxo Bank A/S | Smakkedalen 2 | DK-2820 Gentofte Company phone: +45 39 77 40
00 | Direct phone: +45 39 77 40 81 | Fax number: +45 39 77 42 00 Please
visit our website at: http://www.saxobank.com <http://www.saxobank.com/>





This email may contain confidential and/or privileged inform...{{dropped}}

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-finance

From jtl at saxobank.com  Sat Jul  3 09:22:58 2004
From: jtl at saxobank.com (Jeffrey Todd Lins)
Date: Sat Jul  3 09:23:07 2004
Subject: [R-sig-finance] TAR
Message-ID: <DC680F53FAEFC34BB70119CA8C545E6D015AC7@Mal2pro.mid.dom>


Thanks.

I will have a look at the product spec when it is released.

Most of interest would be MS for state space models.

Jeff

-----Original Message-----
From: Eric Zivot [mailto:ezivot@u.washington.edu]
Sent: Friday, July 02, 2004 10:26 PM
To: Jeffrey Todd Lins; r-sig-finance@stat.math.ethz.ch
Subject: RE: [R-sig-finance] TAR


While somewhat off topic, The TAR models described in the Hansen paper have
been implemented in Splus for the next release of S+FinMetrics. Also
included will be STAR models and Markov switching models.
ez

-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Jeffrey Todd
Lins
Sent: Friday, July 02, 2004 1:01 AM
To: r-sig-finance@stat.math.ethz.ch
Subject: [R-sig-finance] TAR


If you do implement the TAR estimation in R, I would be very interested in
seeing it.


Jeff


Med Venlig Hilsen | Yours sincerely

Jeffrey Todd Lins  | Director - Quantitative Analysis

Saxo Bank A/S | Smakkedalen 2 | DK-2820 Gentofte Company phone: +45 39 77 40
00 | Direct phone: +45 39 77 40 81 | Fax number: +45 39 77 42 00 Please
visit our website at: http://www.saxobank.com <http://www.saxobank.com/>





This email may contain confidential and/or privileged inform...{{dropped}}

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-finance



This email may contain confidential and/or privileged inform...{{dropped}}

From wuertz at itp.phys.ethz.ch  Sun Jul  4 16:54:52 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun Jul  4 16:54:59 2004
Subject: [R-sig-finance] Rmetrics 191.10057
Message-ID: <40E81A3C.3070705@itp.phys.ethz.ch>



It is a pleasure for me to announce the new built for Rmetrics Version
191.0057. The source files and Windows binary packages can be downloaded
from www.rmetrics.org .

The new built has also been submitted to the CRAN server. Some new 
functions
and example files have been added. Unfortunately the user guides and 
reference
guides are not yet updated, they have still the status of Version 181.000xx.

Diethelm Wuertz
info@rmetrics.org


________________________________________________________________________________
2004-07-04
    NEW BUILT RMETRICS VERSION 1091.10057

2004-07-04 Rmetrics
    The new version is now proofed to be Debian license conform for all
    its functions.
    
2004-07-04 FAQ
    The FAQ file has been updated, now the FAQ's are providing more
    information about Rmetrics.     
    
2004-07-04 fbasics/R
    In function .FirstLib we set a timezone if none found in
    environment variables or options, as suggested by Dirk Eddelbuettel,
    thanks Dirk.
    
2004-06-30 fExtremes/R
    A new utility function named "gridVector" has been added which
    creates all grid points from two vectors which span a rectangular
    grid.
    
2004-06-29 fOptions/demo
    A new example file named "funDensitiesEBM.R" has been added
    which adds some distributions and related functions which are
    useful in the theory of exponential Brownian Motion.
    The functions compute densities and probabilities for the
    log-Normal distribution, the Gamma distribution, the
    Reciprocal-Gamma distribution, and the Johnson Type-I
    distribution. Functions are made available for the compution
    of moments including the Normal, the log-Normal, the
    Reciprocal-Gamma, and the Asian-Option Density. In addition
    a function is given to compute numerically first and second
    derivatives of a given function.
        
2004-06-29 fOptions/demo
    A new example file named "funSpecFunsEBM.R" has been added
    with special mathematical functions which are used in the
    theory of exponential Brownian Motion. The functions included
    are: In Part I, the Error Function "erf", the Psi or Digamma
    Function "Psi", the Incomplete Gamma Function "igamma", the
    Gamma Function fpr complex arguments, and the Pochhammer Symbol
    "Pochhammer". In Part II, the Confluent Hypergeometric Functions
    of the 1st Kind and 2nd Kind "kummerM" and "kummerU", the
    Whittaker Functions "whittakerM" and "whittakerW" and the
    Hermite Polynomials "hermiteH"

2004-06-29 fOptions/demo
    A new example file named "xmpSpecFunsEBM.R" has been added
    which shows how to use Gamma Functions, Confluent Hypergeometric
    and related functions under R.  
        
2004-06-29 fSeries/R
    New functions to fit the parameters by the maximum log-likelihood
    method for the symmetric and skew Normal, Student-t with unit
    variance, and generalized error distribution have been added.
                
2004-06-28 fBasics/demo
    A new example file "xmpImportForecasts.R" has been added including
    a function named "forecastsImport" to download monthly financial
    market data from the "www.forecasts.org" web site.

2004-06-28 fBasics/R
    A new function named "keystatsImport" has been added which
    downloads key statistic and fundamental data for equities from
    Yahoo's web site.
       
2004-06-25 fBasics/R
    The function "as.timeSeries" got two additional arguments which
    allow to pass dimension names and the timeDate format in POSIX
    notation to the returned "timeSeries" object.

2004-06-25 fSeries/R
    New functions "[dpqr]ged" and "[dpqr]sged" have been added which
    compute density, distribution function, quantile function and
    generate random variates for the symmetric and skew generalized
    error distribution.

2004-06-25 fSeries/R
    New functions "[dpqr]std" and "[dpqr]sstd" have been added which
    compute density, distribution function, quantile function and  
    generate random variates for the symmetric and skew Student-t
    distribution with unit variance.

2004-06-25 fSeries/R
    New functions "[dpqr]snorm" have been added which compute density,
    distribution function, quantile function and generate random
    variates for the skew normal distribution.
        
2004-06-25 fSeries/demo
    A new example file "xmpDistTESTskew.R" has been added with
    integration tests for the skew normal, for the skew Student-t
    with unit variance, and for the skew GED distribution.      
                
2004-06-25 fSeries/R
    New functions have been added which compute the Haeviside "H" and
    related functions; just another sign function "Sign", the delta
    function "delta", the boxcar function "boxcar" and the ramp
    function "ramp".
        
2004-06-24 fOptions/demo
    The 3D Plot functions for the generalized Black-Scholes option
    prices and the sensitivities have been moved to the examples
    located in the demo directory.

2004-06-14 fSeries/data
    The data sets from the book "The Econometric Modelling of
    Financial Time Series" (2nd Edition) written by Terence C.
    Mills have been added to the data directory.
    
    + many other smaller improvements and fixings ...

From edd at debian.org  Sun Jul  4 18:07:09 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun Jul  4 18:07:16 2004
Subject: [R-sig-finance] Re: Rmetrics 191.10057
In-Reply-To: <40E81A3C.3070705@itp.phys.ethz.ch>
References: <40E81A3C.3070705@itp.phys.ethz.ch>
Message-ID: <20040704160709.GA23525@sonny.eddelbuettel.com>


Many thanks to Diethelm for the new Rmetrics release 191.10057, and to the
CRAN masters for including it in the archive.

I have updated the initial packages that had been prepared for and included
in Quantian 0.5.9.2, and just completed uploading them to Debian's incoming/
directory. As brand-new packages, they will have to wait the customary ten
or more days until the ftpmasters insert them into the archive.  Once that
has happened, they will be apt-get'able from you favourite mirror.

In the meantime, you can fetch sources and i386 Debian packages manually
(sorry, no apt-get support here) from

	http://dirk.eddelbuettel.com/code/rmetrics/
	
Happy 4th of July,  Dirk	

-- 
White House officials praised the performance of the controversial 
new Diebold electronic voting machines, which successfully tabulated 
final results from Florida before a single vote was cast.
          -- Andy Borowitz, http://borowitzreport.com, 29 June 2004

From tpapp at axelero.hu  Mon Jul  5 09:29:03 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Mon Jul  5 10:37:45 2004
Subject: [R-sig-finance] Re: Rmetrics 191.10057
In-Reply-To: <20040704160709.GA23525@sonny.eddelbuettel.com>
References: <40E81A3C.3070705@itp.phys.ethz.ch>
	<20040704160709.GA23525@sonny.eddelbuettel.com>
Message-ID: <20040705072903.GA1363@axelero.hu>

On Sun, Jul 04, 2004 at 11:07:09AM -0500, Dirk Eddelbuettel wrote:

> In the meantime, you can fetch sources and i386 Debian packages manually
> (sorry, no apt-get support here) from
> 
> 	http://dirk.eddelbuettel.com/code/rmetrics/

Has anyone tried making the Debian packages for the powerpc
architecture?

Note that I am not asking that somebody compile them for me, I am only
interested in whether they build.  I am considering buying a powerbook
and running debian on it, and it this doesn't work, it would seriously
discourage me.

Thanks,

Tamas

-- 
Tam?s K. Papp
E-mail: tpapp@axelero.hu
        tpapp@princeton.edu
Please try to send only (latin-2) plain text, not HTML or other garbage.

From Jordi.Molins at drkw.com  Mon Jul  5 12:50:43 2004
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Mon Jul  5 12:50:48 2004
Subject: [R-sig-finance] import.data.rte in R?
Message-ID: <AA0BBC8742AFFF4583B91782E958CB660FCCFC@ibfftce121.de.ad.drkw.net>


Hello,

in the fBasics pdf file, page 108, it is said that import.data.rte downloads
data from Reuters Feed. I have both Bloomberg and Reuters, but not S-Plus (I
work with R). Is there a way to have both (or at least one) datafeeds in R?

Another question: I guess that import.data.rte is a function that is called
from an R session, and it downloads data into some R object. Is it possible
to create a script that automatically sends these data into a database?

thank you

Jordi Molins

Short Term Products / Treasury
Capital Markets

> Dresdner Kleinwort Wasserstein 
phone	+49 69 713 15329
fax	+49 69 713 19804
mobile  +49 171 171 64 61
mailto:jordi.molins@drkw.com





--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From edd at debian.org  Mon Jul  5 15:25:26 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon Jul  5 15:25:31 2004
Subject: [R-sig-finance] Re: Rmetrics 191.10057
In-Reply-To: <20040705072903.GA1363@axelero.hu>
References: <40E81A3C.3070705@itp.phys.ethz.ch>
	<20040704160709.GA23525@sonny.eddelbuettel.com>
	<20040705072903.GA1363@axelero.hu>
Message-ID: <20040705132526.GA4477@sonny.eddelbuettel.com>

On Mon, Jul 05, 2004 at 09:29:03AM +0200, Tamas Papp wrote:
> On Sun, Jul 04, 2004 at 11:07:09AM -0500, Dirk Eddelbuettel wrote:
> 
> > In the meantime, you can fetch sources and i386 Debian packages manually
> > (sorry, no apt-get support here) from
> > 
> > 	http://dirk.eddelbuettel.com/code/rmetrics/
> 
> Has anyone tried making the Debian packages for the powerpc
> architecture?

It'll happen automatically once the packages 'are in'. So for real proof you
may need to wait those few days.

> Note that I am not asking that somebody compile them for me, I am only
> interested in whether they build.  I am considering buying a powerbook
> and running debian on it, and it this doesn't work, it would seriously
> discourage me.

I'm pretty sure they'll work as the actual arch-dependent code is in 'only'
a few isolated Fortran or C files to do very specific tasks.  R itself is
pretty good about being portable.  Check for example the buildd on some
other CRAN packages such as VR or Hmisc 

	http://buildd.debian.org/build.php?pkg=vr	       
	http://buildd.debian.org/build.php?pkg=hmisc	       

These packages typically fail only when something is (temporarily) wrong
with the toolchain on that arch.

Dirk

-- 
White House officials praised the performance of the controversial 
new Diebold electronic voting machines, which successfully tabulated 
final results from Florida before a single vote was cast.
          -- Andy Borowitz, http://borowitzreport.com, 29 June 2004

From edd at debian.org  Mon Jul  5 15:30:26 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon Jul  5 15:30:31 2004
Subject: [R-sig-finance] import.data.rte in R?
In-Reply-To: <AA0BBC8742AFFF4583B91782E958CB660FCCFC@ibfftce121.de.ad.drkw.net>
References: <AA0BBC8742AFFF4583B91782E958CB660FCCFC@ibfftce121.de.ad.drkw.net>
Message-ID: <20040705133026.GB4477@sonny.eddelbuettel.com>

On Mon, Jul 05, 2004 at 12:50:43PM +0200, Molins, Jordi wrote:
> 
> Hello,
> 
> in the fBasics pdf file, page 108, it is said that import.data.rte downloads
> data from Reuters Feed. I have both Bloomberg and Reuters, but not S-Plus (I
> work with R). Is there a way to have both (or at least one) datafeeds in R?

I've done it for Bloomberg (and LIM) using their C-level APIs, and talked
about it at UseR. At this point I cannot release the code for it.  We use
Reuters too, but I haven't had a need for it yet.

> Another question: I guess that import.data.rte is a function that is called
> from an R session, and it downloads data into some R object. Is it possible
> to create a script that automatically sends these data into a database?

I'd try the (D)COM interface by Neuwirth and Baier [ and version 1.35 was
just released: http://cran.r-project.org/contrib/extra/dcom/RSrv135.html ]

Other suggestions, anyone?

Dirk

-- 
White House officials praised the performance of the controversial 
new Diebold electronic voting machines, which successfully tabulated 
final results from Florida before a single vote was cast.
          -- Andy Borowitz, http://borowitzreport.com, 29 June 2004

From Jordi.Molins at drkw.com  Mon Jul  5 15:44:48 2004
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Mon Jul  5 15:44:56 2004
Subject: FW: [R-sig-finance] import.data.rte in R?
Message-ID: <AA0BBC8742AFFF4583B91782E958CB660FCD04@ibfftce121.de.ad.drkw.net>


Dirk,

when you say: "At this point I cannot release the code for it.", is it for
copyright issues or for some other reasons? even though currently you cannot
do it, do you think that you will be able to do it in the foreseeable
future?

I am not a professional programmer (I am a trader), and if somebody that is
proficient with the Bloomberg C-API could give me hints, I would be
extremely grateful ... of course, I understand the problems of sharing
propietary algorithms.

I am already using the (D)COM port. It is pretty useful in the corporate
environment. How would you use it for retrieving data into a database? you
would call the import.data.rte, store the data in some array, and then use
VBA code to send it to Access?

About LIM: what is the advantage of LIM above Bloomberg or Reuters?

thx

Jordi


> -----Original Message-----
> From: Dirk Eddelbuettel [mailto:edd@debian.org]
> Sent: 05 July 2004 15:30
> To: Molins, Jordi
> Cc: 'r-sig-finance@stat.math.ethz.ch'; Jordi Molins
> Subject: Re: [R-sig-finance] import.data.rte in R?
> 
> 
> On Mon, Jul 05, 2004 at 12:50:43PM +0200, Molins, Jordi wrote:
> > 
> > Hello,
> > 
> > in the fBasics pdf file, page 108, it is said that 
> import.data.rte downloads
> > data from Reuters Feed. I have both Bloomberg and Reuters, 
> but not S-Plus (I
> > work with R). Is there a way to have both (or at least one) 
> datafeeds in R?
> 
> I've done it for Bloomberg (and LIM) using their C-level 
> APIs, and talked
> about it at UseR. At this point I cannot release the code for 
> it.  We use
> Reuters too, but I haven't had a need for it yet.
> 
> > Another question: I guess that import.data.rte is a 
> function that is called
> > from an R session, and it downloads data into some R 
> object. Is it possible
> > to create a script that automatically sends these data into 
> a database?
> 
> I'd try the (D)COM interface by Neuwirth and Baier [ and 
> version 1.35 was
> just released: 
> http://cran.r-project.org/contrib/extra/dcom/RSrv135.html ]
> 
> Other suggestions, anyone?
> 
> Dirk
> 
> -- 
> White House officials praised the performance of the controversial 
> new Diebold electronic voting machines, which successfully tabulated 
> final results from Florida before a single vote was cast.
>           -- Andy Borowitz, http://borowitzreport.com, 29 June 2004
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From edd at debian.org  Mon Jul  5 16:54:20 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon Jul  5 16:54:25 2004
Subject: FW: [R-sig-finance] import.data.rte in R?
In-Reply-To: <AA0BBC8742AFFF4583B91782E958CB660FCD04@ibfftce121.de.ad.drkw.net>
References: <AA0BBC8742AFFF4583B91782E958CB660FCD04@ibfftce121.de.ad.drkw.net>
Message-ID: <20040705145417.GA5613@sonny.eddelbuettel.com>


Jordi,

On Mon, Jul 05, 2004 at 03:44:48PM +0200, Molins, Jordi wrote:
> 
> Dirk,
> 
> when you say: "At this point I cannot release the code for it.", is it for
> copyright issues or for some other reasons? even though currently you cannot
> do it, do you think that you will be able to do it in the foreseeable
> future?

Yes, sorry, I wasn't very clear.  Both of those statements are true.  

As I did that development at work, it "belongs" to work and it is not
entirely my call if, and when, it gets released. "Intellectual property" is
increasingly seen as a competetive asset, and we are frequently reminded of
that, and even encouraged to think about patenting our work if 'suitable'.

I will need to lobby for my personal view that a connector package is
'merely' infrastructure, and that we'd be fine open source'ing it. But I
have have no idea how long that may take.  So 'future', yes; 'foreseeable
future' maybe not. I will certainly provide updates on that here.

> I am not a professional programmer (I am a trader), and if somebody that is
> proficient with the Bloomberg C-API could give me hints, I would be
> extremely grateful ... of course, I understand the problems of sharing
> propietary algorithms.

Your colleague Bernhard was also interested, and I have pointed him to the
(actually pretty decent) Bloomberg documentation and C API kit. Couple that
with some understanding one needs from hooking C code into R, and it is no
longer unsurmountable.  Maybe Bernhard and you need to find a sponsor to pay
for it inside DRKW, and then let a developer code it up for you. And maybe
you even get the bosses to release it ?

> I am already using the (D)COM port. It is pretty useful in the corporate
> environment. How would you use it for retrieving data into a database? you
> would call the import.data.rte, store the data in some array, and then use
> VBA code to send it to Access?

R can talk directly to many databases, RODBC helps a lot. The key would be
to get it into R first.

> About LIM: what is the advantage of LIM above Bloomberg or Reuters?

It's mostly complementary. LIM is for historical data, as well as the
ability to create / run scenarios in an almost normal language as in (NB:
untested, typing this from home)

	show TY: three day percent change in TY 
	when fedfunds is larger than previous value of fedfunds plus 0.49

See www.lim.com for more.

Dirk

-- 
White House officials praised the performance of the controversial 
new Diebold electronic voting machines, which successfully tabulated 
final results from Florida before a single vote was cast.
          -- Andy Borowitz, http://borowitzreport.com, 28 June 2004

From wuertz at itp.phys.ethz.ch  Mon Jul  5 20:51:06 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon Jul  5 20:51:11 2004
Subject: [R-sig-finance] import.data.rte
Message-ID: <40E9A31A.3090103@itp.phys.ethz.ch>

My script about my lecture course is named "Computing with R and _Splus_ 
for Financial Engineers:
Markets Basic Statistics, Date and Time Management". Unfortunately it is 
not (yet) a (real) "User
Guide" for my Rmetrics packages, but it can be very helpful in many 
aspects in using the Rmetrics
packages although it still covers some SPlus specific topics. Currently 
I'm fighting to keep almost
1'000 pages, which is  the size of my scripts, up to date and I hope 
that I can finish this effort in
spring 2005.

Originally I have written all my functions for SPlus,  and with the move 
of Insightful away from SPlus-2000 I changed to R. The move was a big 
effort (and is not yet complete), since my packages
have a few hundreds of functions. I have renamed all the download 
specific functions from SPlus
style notation like for example "import.data.provider" to 
"importProvider" to make clear also by
renaming that these functions have changed and are now R-specific.

My SPlus function named "import.data.rte" was written originally a few 
years ago for SPlus (not R!)
and required the commercial "rte" Software package. This function 
allowed to download data from
a Reuters feed. I have not added this function to the Rmetrics Software, 
so that you will find
there no functions for the download of Reuters data.  It is not planned 
to reactivate this
function for R because I have currently no access to a Reuters feed 
neither a valid license for the
"rte" Software. However, in principle it should not be too difficult to 
rewrite this function for R along
with the help of the "import.data.rte" code, but it needs some work and 
testing! If somebody
is interested in the original SPlus code, please send me an email. If 
not outdated (for some
reasons)  the functions should still work together with SPlus (not R), 
with "rte", and with the
Reuters feed.

I have to apologize for any misunderstanding caused by my lecture 
scripts (originally written for R
and SPlus) in use together with the Rmetrics software packages.

Diethelm


Molins, Jordi wrote:

> Hello,
>
> in the fBasics pdf file, page 108, it is said that import.data.rte 
> downloads
> data from Reuters Feed. I have both Bloomberg and Reuters, but not 
> S-Plus (I
> work with R). Is there a way to have both (or at least one) datafeeds 
> in R?
>  

It's mentioned  there, that this function is SPlus (not R) specific 
[maybe not clear enough] ...

> Another question: I guess that import.data.rte is a function that is 
> called
> from an R session, and it downloads data into some R object. Is it 
> possible
> to create a script that automatically sends these data into a database?
>  

Your guess is almost right, the function is called from a SPlus session 
(not R!)  and downloads into
a SPlus object which can be stored in a text file are SQL database with 
the appropriate interface
function ...

> thank you
>
> Jordi Molins
>
> Short Term Products / Treasury
> Capital Markets
>
>  
>
>> Dresdner Kleinwort Wasserstein   
>
> phone    +49 69 713 15329
> fax    +49 69 713 19804
> mobile  +49 171 171 64 61
> mailto:jordi.molins@drkw.com
>
>
>
>
>
> -------------------------------------------------------------------------------- 
>
> The information contained herein is confidential and is 
> inte...{{dropped}}
>
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>

From enrique.bengoechea at credit-suisse.com  Tue Jul  6 13:22:49 2004
From: enrique.bengoechea at credit-suisse.com (Enrique Bengoechea)
Date: Tue Jul  6 14:01:35 2004
Subject: [R-sig-finance] Import from Bloomberg
Message-ID: <OFFCB57AA2.D063CACB-ONC1256EC9.003AA111@csintra.net>

Jordi, I'm downloading data from Bloomberg to R sessions every day. It is relatively easy using the Bloomberg ActiveX control and the R(D)Com client package (instead of the lower-level C APIs). I don't know whether something like this is included in
rMetrics. Maybe it would be useful to add it or to bundle it properly and release a specific package (if someone is interested please tell me, when I searched for it some time ago I found nothing). Here's how (to be run only from a PC with Bloomberg
Workstation installed)

The following code within a R session opens the connection and sets some parameters. Timeout is important as sometimes the download hangs. The other parameters are for time series:

      require("RDCOMClient");
      require("chron"); # For Date-to-ComDate transformations

      blCon <<- try(blCon <- COMCreate("Bloomberg.Data.1"), silent=TRUE);
      if (class(blCon) == "try-error") {
            warning(paste("Seems like this is not a Bloomberg Workstation: ", blCon));
      } else {
            blCon[["Timeout"]] <<- 12000;
            # Constants for "DisplayNonTradingDays": Omit=0, Week=64, AllCalendar=128
            blCon[["DisplayNonTradingDays"]] <<- 64;
            # Constants for "NonTradingDayValue": BloombergHandles=0, PreviousDays=256, ShowNoNumber=512
            blCon[["NonTradingDayValue"]] <<- 512;
            # Constants for Periodicity: Daily=1, Weekly=6, Monthly=7, Annual=9
            blCon[["Periodicity"]] <<- .paOptions$frequencies[.paOptions$frequency, "BloombergPeriodicity"];
      }

I normally  open the connection and keep it open on a global variable for subsequent downloads during the session, because closing it and opening again doesn't work straighforwardly. It seems the COM object is not really released until the garbage
collector is run, and if you run the code above twice the Bloomberg connection no longer works. For a safe closing of the connection use:

      rm(blCon, envir=.GlobalEnv);
      # Explicitly invoking the garbage collector is necessary. Otherwise the COM object is not really released
      # and prevents any new connection to Bloomberg to be established later.
      gc();

To download invidual indicators:

      blpFields <- c("Long Comp Name", "Name", "Short Name", "Crncy", "Id Isin", "Ticker", "Exch Code", "Market Sector Des");
      blpTicker <- "TEF SM Equity";

      figures <- try(blCon$BlpSubscribe(Security=blpTicker, Fields=blpFields));

      if (class(figures) != "try-error") {
            blCon$Desubscribe(ids);       # Just in case...

            # Enable access to fields by name. Spaces are substituted by dots.
            names(figures) <- make.names(blpFields);

            # Flatten the list for easier access (figures["Short Name"] instead of figures["Short Name"][[1]])
            figures <- unlist(figures, recursive=FALSE);

            # Turn Bloomberg codes such as '#N/A N Ap' to NAs
            figures <- lapply(figures, FUN=function(x) {  if (x %in% c("#N/A N Ap", "#N/A N.A.")) NA else x; })

            # Check the ticker has been found
            if (figures[1] == "#N/A Sec")
                  stop(paste("Bloomberg ticker", sQuote(blpTicker), "not found"));
      }

Then you can access the downloaded data with figures$Name, figures$Market.Sector.Des, etc.

To download time series, you first need to specify the date range. I use the R 1.9.0 new Date object, which needs to be transformed to COMDate objects, for what I use the following functions adapted from R(D)Com client examples (using POSIXlt instead of
Date requires trivial changes, I used it before R 1.9.0 was released):

      # FUNCTION: as.Date.comDate
      # DESCRIPTION: Transform a Date object to a COMDate object
      # NOTE: Adapted from RCOM client code examples
      as.Date.comDate <- function(comDate, date1904 = FALSE) {
            if(date1904){
                  orig <- c(month=12, day=31, year=1903);
                  off <- 0;
            }
            else {
                  orig <- c(month=12, day=31, year=1899);
                  off <- 1;
            }

            as.Date(chron(as.numeric(comDate) - off, origin = c(month=12, day=31, year=1899)));
      }


      # FUNCTION: as.comDate.Date
      # DESCRIPTION: Tranforms a COMDate object to a Date object
      # NOTE: Adapted from RCOM client code examples
      as.comDate.Date <- function(aDate, date1904 = FALSE)
      {
            chronDate <- chron(unclass(aDate));
            if (date1904){
                  orig <- c(month=12, day=31, year=1903);
                  off <- 0;
            }
            else {
                  orig <- c(month=12, day=31, year=1899);
                  off <- 1;
            }

            if(any(origin(chronDate)!=orig))
                  origin(chronDate) <- orig;

            result <- new("COMDate");
            result[1] <- round(as.numeric(chronDate) + off);

            result;
      }


And then, to download time series into an R matrix:

      # Turn dates to COMDate's
      from <- as.Date("2000-12-31");
      to <- Sys.Date()-1;     # Yesterday
      comFrom <-  as.comDate.Date(from);
      comTo <- as.comDate.Date(to);
      blpTicker <- "TEF SM Equity";

      histData <- try(blCon$BLPGetHistoricalData(Security=blpTicker, Fields="PX_LAST",
            StartDate=comFrom, EndDate=comTo));
      if (class(histData) != "try-error") {
            # Check the ticker has been found
            if (histData[[2]][[1]][1] == "#N/A History") {
                  warning(paste("History not available for Bloomberg ticker", blpTicker, "between", from, "and", to));
            } else {
                  # Transform Bloomberg result to a R matrix whose row names are strings with the dates
                  price <- matrix(as.numeric(histData[[2]][[1]]), nrow=length(histData[[2]][[1]]), ncol=1,
                        dimnames=list(format(as.Date.comDate(histData[[1]][[1]]), "d/M/yyyy"), code));
            }
      }



Then you can move your data in R objects to a database using the RODBC package. This has be adapted to your particular database schema.

Hope this helps.

Enrique
___________________________________________________________________________

Enrique Bengoechea
Investment Consulting - CREDIT SUISSE Spain

From wuertz at itp.phys.ethz.ch  Wed Jul  7 12:51:56 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Wed Jul  7 12:52:44 2004
Subject: [R-sig-finance] Rmetrics Documentation Update
Message-ID: <40EBD5CC.5070504@itp.phys.ethz.ch>


I like to announce that some of the Rmetrics
Documents have been updated to Version
R 191.10057

    Rmetrics Flyer: 
http://www.itp.phys.ethz.ch/econophysics/R/pdf/DocRmetrics.pdf
    Rmetrics Fact Sheet: 
http://www.itp.phys.ethz.ch/econophysics/R/pdf/DocFactsheet.pdf
    Rmetrics Reference Card: 
http://www.itp.phys.ethz.ch/econophysics/R/pdf/DocRefcard.pdf

Unfortunately, the User Guides are still behind, having Version No 1.8.1.
They will be updated in the near future.
   
Best Regards

Diethelm

From enrique.bengoechea at credit-suisse.com  Thu Jul  8 13:53:47 2004
From: enrique.bengoechea at credit-suisse.com (Enrique Bengoechea)
Date: Thu Jul  8 13:54:32 2004
Subject: [R-sig-finance] Import from Bloomberg
Message-ID: <OF896A6BE6.9DAC3BFB-ONC1256ECB.00414EA4@csintra.net>

Jordi, I'm downloading data from Bloomberg to R sessions every day. It is relatively easy using the Bloomberg ActiveX control and the R(D)Com client package (instead of the lower-level C APIs). I don't know whether something like this is included in
rMetrics. Maybe it would be useful to add it or to bundle it properly and release a specific package (if someone is interested please tell me, when I searched for it some time ago I found nothing). Here's how (to be run only from a PC with Bloomberg
Workstation installed)

The following code within a R session opens the connection and sets some parameters. Timeout is important as sometimes the download hangs. The other parameters are for time series:

      require("RDCOMClient");
      require("chron"); # For Date-to-ComDate transformations

      blCon <<- try(blCon <- COMCreate("Bloomberg.Data.1"), silent=TRUE);
      if (class(blCon) == "try-error") {
            warning(paste("Seems like this is not a Bloomberg Workstation: ", blCon));
      } else {
            blCon[["Timeout"]] <<- 12000;
            # Constants for "DisplayNonTradingDays": Omit=0, Week=64, AllCalendar=128
            blCon[["DisplayNonTradingDays"]] <<- 64;
            # Constants for "NonTradingDayValue": BloombergHandles=0, PreviousDays=256, ShowNoNumber=512
            blCon[["NonTradingDayValue"]] <<- 512;
            # Constants for Periodicity: Daily=1, Weekly=6, Monthly=7, Annual=9
            blCon[["Periodicity"]] <<- .paOptions$frequencies[.paOptions$frequency, "BloombergPeriodicity"];
      }

I normally  open the connection and keep it open on a global variable for subsequent downloads during the session, because closing it and opening again doesn't work straighforwardly. It seems the COM object is not really released until the garbage
collector is run, and if you run the code above twice the Bloomberg connection no longer works. For a safe closing of the connection use:

      rm(blCon, envir=.GlobalEnv);
      # Explicitly invoking the garbage collector is necessary. Otherwise the COM object is not really released
      # and prevents any new connection to Bloomberg to be established later.
      gc();

To download invidual indicators:

      blpFields <- c("Long Comp Name", "Name", "Short Name", "Crncy", "Id Isin", "Ticker", "Exch Code", "Market Sector Des");
      blpTicker <- "TEF SM Equity";

      figures <- try(blCon$BlpSubscribe(Security=blpTicker, Fields=blpFields));

      if (class(figures) != "try-error") {
            blCon$Desubscribe(ids);       # Just in case...

            # Enable access to fields by name. Spaces are substituted by dots.
            names(figures) <- make.names(blpFields);

            # Flatten the list for easier access (figures["Short Name"] instead of figures["Short Name"][[1]])
            figures <- unlist(figures, recursive=FALSE);

            # Turn Bloomberg codes such as '#N/A N Ap' to NAs
            figures <- lapply(figures, FUN=function(x) {  if (x %in% c("#N/A N Ap", "#N/A N.A.")) NA else x; })

            # Check the ticker has been found
            if (figures[1] == "#N/A Sec")
                  stop(paste("Bloomberg ticker", sQuote(blpTicker), "not found"));
      }

Then you can access the downloaded data with figures$Name, figures$Market.Sector.Des, etc.

To download time series, you first need to specify the date range. I use the R 1.9.0 new Date object, which needs to be transformed to COMDate objects, for what I use the following functions adapted from R(D)Com client examples (using POSIXlt instead of
Date requires trivial changes, I used it before R 1.9.0 was released):

      # FUNCTION: as.Date.comDate
      # DESCRIPTION: Transform a Date object to a COMDate object
      # NOTE: Adapted from RCOM client code examples
      as.Date.comDate <- function(comDate, date1904 = FALSE) {
            if(date1904){
                  orig <- c(month=12, day=31, year=1903);
                  off <- 0;
            }
            else {
                  orig <- c(month=12, day=31, year=1899);
                  off <- 1;
            }

            as.Date(chron(as.numeric(comDate) - off, origin = c(month=12, day=31, year=1899)));
      }


      # FUNCTION: as.comDate.Date
      # DESCRIPTION: Tranforms a COMDate object to a Date object
      # NOTE: Adapted from RCOM client code examples
      as.comDate.Date <- function(aDate, date1904 = FALSE)
      {
            chronDate <- chron(unclass(aDate));
            if (date1904){
                  orig <- c(month=12, day=31, year=1903);
                  off <- 0;
            }
            else {
                  orig <- c(month=12, day=31, year=1899);
                  off <- 1;
            }

            if(any(origin(chronDate)!=orig))
                  origin(chronDate) <- orig;

            result <- new("COMDate");
            result[1] <- round(as.numeric(chronDate) + off);

            result;
      }


And then, to download time series into an R matrix:

      # Turn dates to COMDate's
      from <- as.Date("2000-12-31");
      to <- Sys.Date()-1;     # Yesterday
      comFrom <-  as.comDate.Date(from);
      comTo <- as.comDate.Date(to);
      blpTicker <- "TEF SM Equity";

      histData <- try(blCon$BLPGetHistoricalData(Security=blpTicker, Fields="PX_LAST",
            StartDate=comFrom, EndDate=comTo));
      if (class(histData) != "try-error") {
            # Check the ticker has been found
            if (histData[[2]][[1]][1] == "#N/A History") {
                  warning(paste("History not available for Bloomberg ticker", blpTicker, "between", from, "and", to));
            } else {
                  # Transform Bloomberg result to a R matrix whose row names are strings with the dates
                  price <- matrix(as.numeric(histData[[2]][[1]]), nrow=length(histData[[2]][[1]]), ncol=1,
                        dimnames=list(format(as.Date.comDate(histData[[1]][[1]]), "d/M/yyyy"), code));
            }
      }



Then you can move your data in R objects to a database using the RODBC package. This has be adapted to your particular database schema.

Hope this helps.

Enrique
___________________________________________________________________________

Enrique Bengoechea
Investment Consulting - CREDIT SUISSE Spain

From enrique.bengoechea at credit-suisse.com  Fri Jul  9 12:10:24 2004
From: enrique.bengoechea at credit-suisse.com (Enrique Bengoechea)
Date: Fri Jul  9 12:11:03 2004
Subject: [R-sig-finance] Re: Import from Bloomberg
Message-ID: <OFE3E78C94.0D96DCF2-ONC1256ECC.003316F8@csintra.net>

Oooppss!!  Thanks to John for pointing the bugs on my example code.  I modified on the fly my own code which has much more stuff and forgot to clean up some things:

> blCon[["Periodicity"]] <<- .paOptions$frequencies[.paOptions$frequency, "BloombergPeriodicity"];

.paOptions is my own global variable with configurarion options. It should be substituted by one of the Bloomberg constants appearing in the comment below the line, e.g.

      # Constants for Periodicity: Daily=1, Weekly=6, Monthly=7, Annual=9
      blCon[["Periodicity"]] <<- 6; # For weekly data

> blCon$Desubscribe(ids);       # Just in case...
> Where is 'ids' defined?

ids is my variable for the Bloomberg tickers, should be substituted by the ticker you have previously subscribed (I think this desubscribing is only necessary when making asynchronous request and in the examples I'm using synchronous requests, but as it
doesn't harm I usually include it just in case...):

      blCon$Desubscribe("TEF SM Equity");

> dimnames=list(format(as.Date.comDate(histData[[1]][[1]]), "d/M/yyyy"), code));
> Where is 'code' defined?

code is also the ticker, in our case, "TEF SM Equity", although you can use whatever you want to identify the column.

> format(as.Date.comDate(histData[[1]][[1]]), "d/M/yyyy")
> didnt seem to make sense. Did you mean something like
> format(as.Date.comDate(histData[[1]][[1]]), "%d/%m/%Y")

Right. And, as with the column name, you can use whatever format you want, or try to transform to a any of the available time series objects in R (ts, irts, etc.)

Regards,

Enrique
___________________________________________________________________________

Enrique Bengoechea
Investment Consulting - CREDIT SUISSE Spain
___________________________________________________________________________






<john.gavin@ubs.com> on 08/07/2004 19:18:06

To:    Enrique Bengoechea/CSPF@PCOMINT
cc:

Subject:    Import from Bloomberg

Hi Enrique,

Thanks for the Bloomberg example.

I tried to run the code and encountered a few bugs,
which I wonder if you can comment on.

blCon[["Periodicity"]] <<- .paOptions$frequencies[.paOptions$frequency, "BloombergPeriodicity"];

What is '.paOptions'?
I ignored this line, so presumably got the Bloomberg defaults.


blCon$Desubscribe(ids);       # Just in case...

Where is 'ids' defined?
Again I ignored it and things seem to run ok.


dimnames=list(format(as.Date.comDate(histData[[1]][[1]]), "d/M/yyyy"), code));

Where is 'code' defined?
Also,

format(as.Date.comDate(histData[[1]][[1]]), "d/M/yyyy")

didnt seem to make sense. Did you mean something like

format(as.Date.comDate(histData[[1]][[1]]), "%d/%m/%Y")

I used

dimnames=list(format(as.Date.comDate(histData[[1]][[1]]), "%d/%m/%Y"), NULL));

to get past this line (not sure about ignoring 'code' though).

Any hints would be welcome but thanks anyway for the example
it should be enough to allow me to play around with Bloomberg,
something that I have been meaning to do but
never got around to.

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Quantitative Risk Models and Statistics,
UBS Investment Bank, 6th floor,
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289
Fax   +44 (0) 207 568 5352

Date: Thu, 8 Jul 2004 13:53:47 +0200
From: Enrique Bengoechea <enrique.bengoechea@credit-suisse.com>
Subject: [R-sig-finance] Import from Bloomberg
To: r-sig-finance@stat.math.ethz.ch
Message-ID: <OF896A6BE6.9DAC3BFB-ONC1256ECB.00414EA4@csintra.net>
Content-Type: text/plain; charset=us-ascii

Jordi, I'm downloading data from Bloomberg to R sessions every day. It is relatively easy using the Bloomberg ActiveX control and the R(D)Com client package (instead of the lower-level C APIs). I don't know whether something like this is included in
rMetrics. Maybe it would be useful to add it or to bundle it properly and release a specific package (if someone is interested please tell me, when I searched for it some time ago I found nothing). Here's how (to be run only from a PC with Bloomberg
Workstation installed)

The following code within a R session opens the connection and sets some parameters. Timeout is important as sometimes the download hangs. The other parameters are for time series:

      require("RDCOMClient");
      require("chron"); # For Date-to-ComDate transformations

      blCon <<- try(blCon <- COMCreate("Bloomberg.Data.1"), silent=TRUE);
      if (class(blCon) == "try-error") {
            warning(paste("Seems like this is not a Bloomberg Workstation: ", blCon));
      } else {
            blCon[["Timeout"]] <<- 12000;
            # Constants for "DisplayNonTradingDays": Omit=0, Week=64, AllCalendar=128
            blCon[["DisplayNonTradingDays"]] <<- 64;
            # Constants for "NonTradingDayValue": BloombergHandles=0, PreviousDays=256, ShowNoNumber=512
            blCon[["NonTradingDayValue"]] <<- 512;
            # Constants for Periodicity: Daily=1, Weekly=6, Monthly=7, Annual=9
            blCon[["Periodicity"]] <<- .paOptions$frequencies[.paOptions$frequency, "BloombergPeriodicity"];
      }

I normally  open the connection and keep it open on a global variable for subsequent downloads during the session, because closing it and opening again doesn't work straighforwardly. It seems the COM object is not really released until the garbage
collector is run, and if you run the code above twice the Bloomberg connection no longer works. For a safe closing of the connection use:

      rm(blCon, envir=.GlobalEnv);
      # Explicitly invoking the garbage collector is necessary. Otherwise the COM object is not really released
      # and prevents any new connection to Bloomberg to be established later.
      gc();

To download invidual indicators:

      blpFields <- c("Long Comp Name", "Name", "Short Name", "Crncy", "Id Isin", "Ticker", "Exch Code", "Market Sector Des");
      blpTicker <- "TEF SM Equity";

      figures <- try(blCon$BlpSubscribe(Security=blpTicker, Fields=blpFields));

      if (class(figures) != "try-error") {
            blCon$Desubscribe(ids);       # Just in case...

            # Enable access to fields by name. Spaces are substituted by dots.
            names(figures) <- make.names(blpFields);

            # Flatten the list for easier access (figures["Short Name"] instead of figures["Short Name"][[1]])
            figures <- unlist(figures, recursive=FALSE);

            # Turn Bloomberg codes such as '#N/A N Ap' to NAs
            figures <- lapply(figures, FUN=function(x) {  if (x %in% c("#N/A N Ap", "#N/A N.A.")) NA else x; })

            # Check the ticker has been found
            if (figures[1] == "#N/A Sec")
                  stop(paste("Bloomberg ticker", sQuote(blpTicker), "not found"));
      }

Then you can access the downloaded data with figures$Name, figures$Market.Sector.Des, etc.

To download time series, you first need to specify the date range. I use the R 1.9.0 new Date object, which needs to be transformed to COMDate objects, for what I use the following functions adapted from R(D)Com client examples (using POSIXlt instead of
Date requires trivial changes, I used it before R 1.9.0 was released):

      # FUNCTION: as.Date.comDate
      # DESCRIPTION: Transform a Date object to a COMDate object
      # NOTE: Adapted from RCOM client code examples
      as.Date.comDate <- function(comDate, date1904 = FALSE) {
            if(date1904){
                  orig <- c(month=12, day=31, year=1903);
                  off <- 0;
            }
            else {
                  orig <- c(month=12, day=31, year=1899);
                  off <- 1;
            }

            as.Date(chron(as.numeric(comDate) - off, origin = c(month=12, day=31, year=1899)));
      }


      # FUNCTION: as.comDate.Date
      # DESCRIPTION: Tranforms a COMDate object to a Date object
      # NOTE: Adapted from RCOM client code examples
      as.comDate.Date <- function(aDate, date1904 = FALSE)
      {
            chronDate <- chron(unclass(aDate));
            if (date1904){
                  orig <- c(month=12, day=31, year=1903);
                  off <- 0;
            }
            else {
                  orig <- c(month=12, day=31, year=1899);
                  off <- 1;
            }

            if(any(origin(chronDate)!=orig))
                  origin(chronDate) <- orig;

            result <- new("COMDate");
            result[1] <- round(as.numeric(chronDate) + off);

            result;
      }


And then, to download time series into an R matrix:

      # Turn dates to COMDate's
      from <- as.Date("2000-12-31");
      to <- Sys.Date()-1;     # Yesterday
      comFrom <-  as.comDate.Date(from);
      comTo <- as.comDate.Date(to);
      blpTicker <- "TEF SM Equity";

      histData <- try(blCon$BLPGetHistoricalData(Security=blpTicker, Fields="PX_LAST",
            StartDate=comFrom, EndDate=comTo));
      if (class(histData) != "try-error") {
            # Check the ticker has been found
            if (histData[[2]][[1]][1] == "#N/A History") {
                  warning(paste("History not available for Bloomberg ticker", blpTicker, "between", from, "and", to));
            } else {
                  # Transform Bloomberg result to a R matrix whose row names are strings with the dates
                  price <- matrix(as.numeric(histData[[2]][[1]]), nrow=length(histData[[2]][[1]]), ncol=1,
                        dimnames=list(format(as.Date.comDate(histData[[1]][[1]]), "d/M/yyyy"), code));
            }
      }



Then you can move your data in R objects to a database using the RODBC package. This has be adapted to your particular database schema.

Hope this helps.

Enrique
___________________________________________________________________________

Enrique Bengoechea
Investment Consulting - CREDIT SUISSE Spain

Visit our website at http://www.ubs.com

This message contains confidential information and is intend...{{dropped}}

From ru68y7s at myrealbox.com  Mon Jul 19 03:58:40 2004
From: ru68y7s at myrealbox.com (s viswanath)
Date: Mon Jul 19 03:58:46 2004
Subject: [R-sig-finance] question on lm test for arma lag specification
Message-ID: <1090202320.d6badc5cru68y7s@myrealbox.com>


hello R experts,

my question is regarding arma modelling and specification. 

in another older, statistics package , after determining stationarity, i would try to work out the number of ar and ma lags  using an lm test. 
to do this i would 

1. regress  my dependant variable on an intercept term then
2. use LM test for serial correlation, and finally
3. use the  p value of the ols residuals to get the maximum lags for  the arma specification.

I am interested to know how to do this LM test in R say using a function, using perhaps the fseries library?

Thank you in advance,

Sri 

Also, thank you very much for the Roptions package I have been using the options functions regularily.

From ru68y7s at myrealbox.com  Mon Jul 19 21:36:25 2004
From: ru68y7s at myrealbox.com (s viswanath)
Date: Mon Jul 19 21:36:31 2004
Subject: [R-sig-finance] Re: lagrange multiplier test for arma modelling
Message-ID: <1090265785.d669261cru68y7s@myrealbox.com>

Dear R experts,
Followup to my earlier post of finding arma specification. I found thruough r posts, the bg test (bresuch godfrey test).

Is there a way for the bgtest to show the individual coefficients and their p values for the order variables.

best,
sri

-----Original Message-----
From: r-sig-finance-request@stat.math.ethz.ch
To: r-sig-finance@stat.math.ethz.ch
Date: Mon, 19 Jul 2004 12:12:31 +0200
Subject: R-sig-finance Digest, Vol 2, Issue 7

Send R-sig-finance mailing list submissions to
	r-sig-finance@stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
	https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-finance
or, via email, send a message with subject or body 'help' to
	r-sig-finance-request@stat.math.ethz.ch

You can reach the person managing the list at
	r-sig-finance-owner@stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-sig-finance digest..."


Today's Topics:

   1. question on lm test for arma lag specification (s viswanath)


----------------------------------------------------------------------

Message: 1
Date: Sun, 18 Jul 2004 18:58:40 -0700
From: "s viswanath" <ru68y7s@myrealbox.com>
Subject: [R-sig-finance] question on lm test for arma lag
	specification
To: r-sig-finance@stat.math.ethz.ch
Message-ID: <1090202320.d6badc5cru68y7s@myrealbox.com>
Content-Type: text/plain; charset="US-ASCII"


hello R experts,

my question is regarding arma modelling and specification. 

in another older, statistics package , after determining stationarity, i would try to work out the number of ar and ma lags  using an lm test. 
to do this i would 

1. regress  my dependant variable on an intercept term then
2. use LM test for serial correlation, and finally
3. use the  p value of the ols residuals to get the maximum lags for  the arma specification.

I am interested to know how to do this LM test in R say using a function, using perhaps the fseries library?

Thank you in advance,

Sri 

Also, thank you very much for the Roptions package I have been using the options functions regularily.



------------------------------

_______________________________________________
R-sig-finance mailing list
R-sig-finance@stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-finance


End of R-sig-finance Digest, Vol 2, Issue 7

From BSCHARG1 at motorola.com  Fri Jul 23 19:25:41 2004
From: BSCHARG1 at motorola.com (Schargorodsky Benjamin-BSCHARG1)
Date: Fri Jul 23 19:37:11 2004
Subject: [R-sig-finance] Import from Bloomberg
Message-ID: <EBF631554F9CD7118D0B00065BF34DCB097A853D@il27exm03.cig.mot.com>



Ben Schargorodsky
847-435-0804

From durgadaskul at yahoo.co.in  Tue Jul 27 16:29:04 2004
From: durgadaskul at yahoo.co.in (=?iso-8859-1?q?durgadas=20kulkarni?=)
Date: Tue Jul 27 16:31:58 2004
Subject: [R-sig-finance] problem of installing packages
In-Reply-To: <200407241011.i6OABsdE002516@hypatia.math.ethz.ch>
Message-ID: <20040727142904.49031.qmail@web8309.mail.in.yahoo.com>

Dear Sir,
 
I wanted to use quadratic optimization package. So I followed the instructions given as to how to install the package. I then checked if the package has been installed by using the command "library()". So the package was installed. 
 
Soon when i started with the examples in the documentation, I always find it giving the error
 
Error: couldn't find function "solve.QP"
 
I don't understand why this is happening.
Please suggest as early as possible.
Warm regards
durgadas


r-sig-finance-request@stat.math.ethz.ch wrote:
Send R-sig-finance mailing list submissions to
r-sig-finance@stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-finance
or, via email, send a message with subject or body 'help' to
r-sig-finance-request@stat.math.ethz.ch

You can reach the person managing the list at
r-sig-finance-owner@stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-sig-finance digest..."
Today's Topics:

1. Import from Bloomberg (Schargorodsky Benjamin-BSCHARG1)


> ATTACHMENT part 3.1 message/rfc822 
From: Schargorodsky Benjamin-BSCHARG1 
To: "'r-sig-finance@stat.math.ethz.ch'" 
Date: Fri, 23 Jul 2004 12:25:41 -0500
Subject: [R-sig-finance] Import from Bloomberg



Ben Schargorodsky
847-435-0804


_______________________________________________
R-sig-finance mailing list
R-sig-finance@stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-finance


	[[alternative HTML version deleted]]

From maechler at stat.math.ethz.ch  Tue Jul 27 17:42:45 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue Jul 27 19:26:41 2004
Subject: [R-sig-finance] problem of installing packages
In-Reply-To: <20040727142904.49031.qmail@web8309.mail.in.yahoo.com>
References: <20040727142904.49031.qmail@web8309.mail.in.yahoo.com>
Message-ID: <16646.30709.240924.239024@gargle.gargle.HOWL>

>>>>> "durgadas" == durgadas kulkarni <durgadaskul@yahoo.co.in>
>>>>>     on Tue, 27 Jul 2004 15:29:04 +0100 (BST) writes:

    durgadas> Dear Sir, I wanted to use quadratic optimization
    durgadas> package. So I followed the instructions given as
    durgadas> to how to install the package. I then checked if
    durgadas> the package has been installed by using the
    durgadas> command "library()". So the package was installed.
 
    durgadas> Soon when i started with the examples in the
    durgadas> documentation, I always find it giving the error
 
    durgadas> Error: couldn't find function "solve.QP"
 
    durgadas> I don't understand why this is happening.  Please
    durgadas> suggest as early as possible.  Warm regards
    durgadas> durgadas

and what has this to do with R-SIG-finance?

You should really use R-help for asking such questions.

Regards,
Martin Maechler

From acorrea at utad.pt  Wed Aug  4 17:41:37 2004
From: acorrea at utad.pt (Alessandro Correa)
Date: Wed Aug  4 17:42:33 2004
Subject: [R-sig-finance] GARCH(1,1)
Message-ID: <009201c47a39$897e5920$6b9e88c1@utad.pt>

I have run a GARCH(1,1) model using Eviews Matlab and R.
The results are too diferent.
In special, take a look at the constant of the mean equation.
Only Eviews returns a contant=mean of the serie (-0.068513).
Does anyone knows the reason for this?

************** EVIEWS 3 *****************************
Dependent Variable: Y    
Method: ML - ARCH
Sample: 1 550    
Included observations: 550    
Convergence achieved after 28 iterations    
    
 Coefficient Std. Error z-Statistic Prob.  
    
C -0.065643  0.090032 -0.729105  0.4659
    
        Variance Equation   
    
C  0.967087  0.446945  2.163770  0.0305
ARCH(1)  0.083785  0.037083  2.259421  0.0239
GARCH(1) 0.684535  0.127228  5.380361  0.0000
    
R-squared -0.000002 Mean dependent var -0.068513
Log likelihood -1171.509 Durbin-Watson stat 1.9427

************** R *****************************************
Model:
GARCH(1,1)

Coefficient(s):
    Estimate  Std. Error  t value Pr(>|t|)    
a0   0.70954     0.38302    1.852   0.0640 .  
a1   0.06994     0.03180    2.199   0.0278 *  
b1   0.75968     0.11354    6.691 2.21e-11 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 


************** MATLAB *********************************
Number of Parameters Estimated: 4

                               Standard          T     
  Parameter       Value          Error       Statistic 
 -----------   -----------   ------------   -----------
          C    -0.05598       0.089745        -0.6238
          K    0.72795        0.39708          1.8333
   GARCH(1)    0.75555        0.11697          6.4594
    ARCH(1)    0.07061        0.033108         2.1327

Alessandro de Castro Corrêa

	[[alternative HTML version deleted]]

From Jordi.Molins at drkw.com  Wed Aug  4 18:01:55 2004
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Wed Aug  4 18:01:59 2004
Subject: [R-sig-finance] c++ and D-COM
Message-ID: <AA0BBC8742AFFF4583B91782E958CB660FCDD3@ibfftce121.de.ad.drkw.net>


Hello,

I would like to call R functions from C++ code. The C++ code is not called
from an R function, but it stands by itself. I have done research through
google and it seems that before it was not possible to do what I wanted, but
now with the D-COM port, it is possible (comment by Prof Ripley). Prof
Ripley says also that in the documentation of the D-COM port there are C
examples. 

I have looked in the folders of the (D)COM Server, but I have not found
anything like an example (there is an scripting folder, but contains
jscript, python and vbs, but nothing of c++).

Is there a simple way to call R functions from C++ code? is it possible to
have a simple example (like 2+2=4)? 

Thank you very much in advance

Jordi



--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From edd at debian.org  Wed Aug  4 18:38:59 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed Aug  4 18:39:08 2004
Subject: [R-sig-finance] c++ and D-COM
In-Reply-To: <AA0BBC8742AFFF4583B91782E958CB660FCDD3@ibfftce121.de.ad.drkw.net>
References: <AA0BBC8742AFFF4583B91782E958CB660FCDD3@ibfftce121.de.ad.drkw.net>
Message-ID: <20040804163858.GA18493@sonny.eddelbuettel.com>


Hi Jordi,

On Wed, Aug 04, 2004 at 06:01:55PM +0200, Molins, Jordi wrote:
> 
> Hello,
> 
> I would like to call R functions from C++ code. The C++ code is not called
> from an R function, but it stands by itself. I have done research through
> google and it seems that before it was not possible to do what I wanted, but
> now with the D-COM port, it is possible (comment by Prof Ripley). Prof
> Ripley says also that in the documentation of the D-COM port there are C
> examples. 
> 
> I have looked in the folders of the (D)COM Server, but I have not found
> anything like an example (there is an scripting folder, but contains
> jscript, python and vbs, but nothing of c++).

The good folks in Vienna (i.e., Thomas Baier and Erich Neuwierth) have a
mailing list for DCOM that may answer this better:

http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l

Do report back, though, I had wondered about it too. :) 

> Is there a simple way to call R functions from C++ code? is it possible to
> have a simple example (like 2+2=4)?

Not from what I understand as you need to initialize the R engine (i.e. the
interpreter) but you'd get that via (D)COM.  Look for comments on the
r-devel list by, say, Thomas Lumley, Brian Ripley or Simon Urbanek.  Simon
also has a tcp/ip connected Rserve project that we worked with, but again,
no C examples there. Works from Java, and is platform neutral.

Hope this helps, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From Abhijit.Mitra at morganstanley.com  Thu Aug  5 13:08:44 2004
From: Abhijit.Mitra at morganstanley.com (Mitra, Abhijit (IM))
Date: Thu Aug  5 13:08:54 2004
Subject: [R-sig-finance] Brand New User
Message-ID: <D2F2509B543EEE41BC246AE0ADECF78D011EA16F@NYWEXMB19.msad.ms.com>

Hi All,
I work in the hedge funds arena, and will be learning S+ coming this
fall as part of my MS program in Statistics at Temple University. I am
just wondering what will be a good way to start getting acquainted with
the program, any suggestions?
Regards,
Abhi 
--------------------------------------------------------
 
NOTICE: If received in error, please destroy and notify sender.  Sender does not waive confidentiality or privilege, and use is prohibited.

From Jordi.Molins at drkw.com  Thu Aug  5 19:07:17 2004
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Thu Aug  5 19:07:35 2004
Subject: [R-sig-finance] multivariate garch?
Message-ID: <AA0BBC8742AFFF4583B91782E958CB660FCDE8@ibfftce121.de.ad.drkw.net>


Hello,

is there some library in R for multivariate garch?

Rgrds

Jordi



--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From pq at wermlands.lu.se  Fri Aug  6 01:13:13 2004
From: pq at wermlands.lu.se (Linus Thand)
Date: Fri Aug  6 01:15:31 2004
Subject: [R-sig-finance] getReturns problem
Message-ID: <4112BF09.4010102@wermlands.lu.se>

I am trying to run getReturns on a timeSeries object (azn.ts), but the 
function fails:

 > is.timeSeries(azn.ts)
[1] TRUE
 > foo = getReturns(azn.ts)
Error in getReturns(azn.ts) : couldn't find function "positions"

I am running R 1.9.1 and the latest RMetrics libraries (downloaded 
August 5th).
Can anyone help me with this?


Kind regards,
Linus Thand

From edd at debian.org  Fri Aug  6 06:18:45 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri Aug  6 06:18:47 2004
Subject: [R-sig-finance] multivariate garch?
In-Reply-To: <AA0BBC8742AFFF4583B91782E958CB660FCDE8@ibfftce121.de.ad.drkw.net>
References: <AA0BBC8742AFFF4583B91782E958CB660FCDE8@ibfftce121.de.ad.drkw.net>
Message-ID: <20040806041845.GA14568@sonny.eddelbuettel.com>


On Thu, Aug 05, 2004 at 07:07:17PM +0200, Molins, Jordi wrote:
>
> Hello,
>
> is there some library in R for multivariate garch?

No, but check the useR 2004 program. Harald Schmidbauer (whom I'll CC) gave
a really nice (short) presentation on bi-variate garch, and said he was
planning to extend it to the multivariate case. A short page is at
http://www.hs-stat.com/  -- I had meant to follow up with Harald, so this is
as good an excuse as any :)

Harald: any news on the garch code?

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From edd at debian.org  Fri Aug 20 18:49:25 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri Aug 20 18:49:27 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
Message-ID: <20040820164925.GA15391@sonny.eddelbuettel.com>


I've been asked to run some 'modern' regressions: vector autoregression,
vector error correction, kalman filter, ...  

Of course, I'd love to do that in R and will probably end up writing some
code for it, but as the platitude goes, I 'need to hit the ground running'.
Last time I looked at Paul Gilbert's dse bundle, it promised most of this,
but felt somewhat cumbersome.  

Does anybody here have any particular recommendations, and in particular,
warnings about software like EViews, Rats, ... in this context ?

Thanks in advance,  Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From edd at debian.org  Fri Aug 20 18:55:33 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri Aug 20 18:55:35 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
In-Reply-To: <20040820164925.GA15391@sonny.eddelbuettel.com>
References: <20040820164925.GA15391@sonny.eddelbuettel.com>
Message-ID: <20040820165533.GA15630@sonny.eddelbuettel.com>


PS  S-Plus Finmetrics is of course also a candidate, and if anybody here has
feedback or comments on it, I'd love to hear those too.

Thanks, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From pvirketis at hbk.com  Fri Aug 20 19:37:27 2004
From: pvirketis at hbk.com (Pijus Virketis)
Date: Fri Aug 20 19:37:11 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
Message-ID: <FB5014C3177D7E44989DA04617A39E1A028D54@nycdc1.hbk.com>

Dear Dirk,

As far as my personal experience goes, I needed to estimate such models
some time ago, when the R toolkit for this sort of thing was still
almost empty, so I chose to invest in STATA: it provides a fairly
complete set of functions to estimate VAR, SVAR and (as of two months
ago) VECM models, validate their results and stability, and calculate
all the frequently-needed derivatives, such as the MA forms (i.e. IRFs,
SIRFs, ...), etc. For what it's worth, I chose STATA over many other
contenders in the field because it seemed to have some of those R-like
pro-active qualities, like frequent updates, knowledgeable and involved
users, and accessible developers (to which I can personally attest after
running into a couple of bugs in the early SVAR code). The R-STATA
intercommunication is made possible by the foreign package, batch modes,
and good old ASCII. ;) STATA programming is a bit laborious, so I always
only farm out the absolute minimum to it, and do the remainder in R. As
you said, STATA let me "hit the ground running", and is really not a bad
compromise.

Of course, today R's own arsenal for time-series econometrics is shaping
up fast as well. Most significantly, there is now the CRAN urca package
by Bernhard Pfaff: it provides the means to estimate VECM models (both
the transitory and long-term flavours) and Johansen's co-integration
tests built on top them. Sadly, VAR/SVAR and associated battery of
helper functions are still not available, as far as I am aware.

As for the Kalman filter, there is the Kalman... family of functions in
stats: perhaps that's a good place to start? Sadly, I have not yet had a
chance to use space-state models in a proper project, so my knowledge of
the available tools and their relative capabilities is modest. Also, if
you can get to it, R. Carmona's neat book "Statistical Analysis of
Financial Series in S-Plus" (Springer, 2004) has a few sections
(6.2-6.7) on state-space models and Kalman filtering thereof (S code
included), with applications to finance. 

Cheers, 

Pijus

> -----Original Message-----
> From: r-sig-finance-bounces@stat.math.ethz.ch 
> [mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of 
> Dirk Eddelbuettel
> Sent: Friday, August 20, 2004 12:49 PM
> To: R-sig-finance@stat.math.ethz.ch
> Subject: [R-sig-finance] VAR, VECM, Kalman,... non-R software 
> recommendations?
> 
> 
> I've been asked to run some 'modern' regressions: vector 
> autoregression,
> vector error correction, kalman filter, ...  
> 
> Of course, I'd love to do that in R and will probably end up 
> writing some
> code for it, but as the platitude goes, I 'need to hit the 
> ground running'.
> Last time I looked at Paul Gilbert's dse bundle, it promised 
> most of this,
> but felt somewhat cumbersome.  
> 
> Does anybody here have any particular recommendations, and in 
> particular,
> warnings about software like EViews, Rats, ... in this context ?
> 
> Thanks in advance,  Dirk
> 
> -- 
> Those are my principles, and if you don't like them... well, 
> I have others.
>                                                 -- Groucho Marx
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> 
>

From jtl at saxobank.com  Fri Aug 20 20:20:05 2004
From: jtl at saxobank.com (Jeffrey Todd Lins)
Date: Fri Aug 20 20:20:10 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
Message-ID: <DC680F53FAEFC34BB70119CA8C545E6D015B43@Mal2pro.mid.dom>


Hi Dirk,

Yes, in stats there is a set of Kalman filter routines and you can use optim for likelihood estimation. 
I have used it for some state space modeling. There is a chapter in Zivot and Wang's book on the topic as well.

In addition initializing in the KF may be an important consideration - see Harvey and/or Durbin and Koopman.

I have never really used DSE for VAR, ended up writing the code elsewhere, outside of R, but you could look at gretl,
which is available under GNU GPL and written in C, it contains quite a few bits and pieces, I am assuming you can get the source.

Jeff





-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch]On Behalf Of Pijus
Virketis
Sent: Friday, August 20, 2004 7:37 PM
To: Dirk Eddelbuettel
Cc: R-sig-finance@stat.math.ethz.ch
Subject: RE: [R-sig-finance] VAR, VECM, Kalman,... non-R software
recommendations?


Dear Dirk,

As far as my personal experience goes, I needed to estimate such models
some time ago, when the R toolkit for this sort of thing was still
almost empty, so I chose to invest in STATA: it provides a fairly
complete set of functions to estimate VAR, SVAR and (as of two months
ago) VECM models, validate their results and stability, and calculate
all the frequently-needed derivatives, such as the MA forms (i.e. IRFs,
SIRFs, ...), etc. For what it's worth, I chose STATA over many other
contenders in the field because it seemed to have some of those R-like
pro-active qualities, like frequent updates, knowledgeable and involved
users, and accessible developers (to which I can personally attest after
running into a couple of bugs in the early SVAR code). The R-STATA
intercommunication is made possible by the foreign package, batch modes,
and good old ASCII. ;) STATA programming is a bit laborious, so I always
only farm out the absolute minimum to it, and do the remainder in R. As
you said, STATA let me "hit the ground running", and is really not a bad
compromise.

Of course, today R's own arsenal for time-series econometrics is shaping
up fast as well. Most significantly, there is now the CRAN urca package
by Bernhard Pfaff: it provides the means to estimate VECM models (both
the transitory and long-term flavours) and Johansen's co-integration
tests built on top them. Sadly, VAR/SVAR and associated battery of
helper functions are still not available, as far as I am aware.

As for the Kalman filter, there is the Kalman... family of functions in
stats: perhaps that's a good place to start? Sadly, I have not yet had a
chance to use space-state models in a proper project, so my knowledge of
the available tools and their relative capabilities is modest. Also, if
you can get to it, R. Carmona's neat book "Statistical Analysis of
Financial Series in S-Plus" (Springer, 2004) has a few sections
(6.2-6.7) on state-space models and Kalman filtering thereof (S code
included), with applications to finance.

Cheers,

Pijus

> -----Original Message-----
> From: r-sig-finance-bounces@stat.math.ethz.ch
> [mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of
> Dirk Eddelbuettel
> Sent: Friday, August 20, 2004 12:49 PM
> To: R-sig-finance@stat.math.ethz.ch
> Subject: [R-sig-finance] VAR, VECM, Kalman,... non-R software
> recommendations?
>
>
> I've been asked to run some 'modern' regressions: vector
> autoregression,
> vector error correction, kalman filter, ... 
>
> Of course, I'd love to do that in R and will probably end up
> writing some
> code for it, but as the platitude goes, I 'need to hit the
> ground running'.
> Last time I looked at Paul Gilbert's dse bundle, it promised
> most of this,
> but felt somewhat cumbersome. 
>
> Does anybody here have any particular recommendations, and in
> particular,
> warnings about software like EViews, Rats, ... in this context ?
>
> Thanks in advance,  Dirk
>
> --
> Those are my principles, and if you don't like them... well,
> I have others.
>                                                 -- Groucho Marx
>
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


This email may contain confidential and/or privileged inform...{{dropped}}

From edd at debian.org  Fri Aug 20 21:33:38 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri Aug 20 21:33:42 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
In-Reply-To: <FB5014C3177D7E44989DA04617A39E1A028D54@nycdc1.hbk.com>
References: <FB5014C3177D7E44989DA04617A39E1A028D54@nycdc1.hbk.com>
Message-ID: <20040820193338.GA17957@sonny.eddelbuettel.com>


Pijus, and Jeffrey,

On Fri, Aug 20, 2004 at 01:37:27PM -0400, Pijus Virketis wrote:
> Dear Dirk,
> 
> As far as my personal experience goes, I needed to estimate such models
> some time ago, when the R toolkit for this sort of thing was still
> almost empty, so I chose to invest in STATA: it provides a fairly
> complete set of functions to estimate VAR, SVAR and (as of two months
> ago) VECM models, validate their results and stability, and calculate
> all the frequently-needed derivatives, such as the MA forms (i.e. IRFs,
> SIRFs, ...), etc. For what it's worth, I chose STATA over many other
> contenders in the field because it seemed to have some of those R-like
> pro-active qualities, like frequent updates, knowledgeable and involved
> users, and accessible developers (to which I can personally attest after
> running into a couple of bugs in the early SVAR code). The R-STATA
> intercommunication is made possible by the foreign package, batch modes,
> and good old ASCII. ;) STATA programming is a bit laborious, so I always
> only farm out the absolute minimum to it, and do the remainder in R. As
> you said, STATA let me "hit the ground running", and is really not a bad
> compromise.

Very nice you're bringing that up -- with my hat of "being my wife's
sysadmin for the home computers" on, I have the new stata 8 (for Linux) at
home to which she just upgraded. For her limited-dependent variable work,
she has been as very happy user of stata (on Linux) since version 4.  I will
take a much closer look at the VAR and VECM stuff in there.  Thanks a bunch
for that finance practioner's vote of confidence, I had been unsure if
stata's offerings in that area are any good.
 
> Of course, today R's own arsenal for time-series econometrics is shaping
> up fast as well. Most significantly, there is now the CRAN urca package
> by Bernhard Pfaff: it provides the means to estimate VECM models (both
> the transitory and long-term flavours) and Johansen's co-integration
> tests built on top them. Sadly, VAR/SVAR and associated battery of
> helper functions are still not available, as far as I am aware.

Yes, I did of course look at that this week (PS nb 2), and it looks very
good. I need to read up on the original Johansen papers / Hamilton book.

> As for the Kalman filter, there is the Kalman... family of functions in
> stats: perhaps that's a good place to start? Sadly, I have not yet had a

PS nb 3: I had actually worked with these a year or two ago, and have the
Durbin and Koopman text here at my desk. I think I had issues with the nb of
obs needed to do what I wanted to do -- I'll revisit.

> chance to use space-state models in a proper project, so my knowledge of
> the available tools and their relative capabilities is modest. Also, if
> you can get to it, R. Carmona's neat book "Statistical Analysis of
> Financial Series in S-Plus" (Springer, 2004) has a few sections
> (6.2-6.7) on state-space models and Kalman filtering thereof (S code
> included), with applications to finance.

Very good pointer too -- a colleague here has the book, maybe I should take
a look.

So nobody voting for Eview or Rats yet?  Maybe I picked a biased crowd ;-)

Thanks again, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From brandt at unt.edu  Fri Aug 20 22:29:24 2004
From: brandt at unt.edu (Patrick Brandt)
Date: Fri Aug 20 22:30:20 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
In-Reply-To: <20040820193338.GA17957@sonny.eddelbuettel.com>
References: <FB5014C3177D7E44989DA04617A39E1A028D54@nycdc1.hbk.com>
	<20040820193338.GA17957@sonny.eddelbuettel.com>
Message-ID: <9FFA7436-F2E7-11D8-B42E-000A95AC74A2@unt.edu>

I've been a RATS user for about 6 years (*NIX and Windows) and a Stata 
user for 10.   RATS is a great package for doing all of the standard 
econometric time series, esp VARs and VECMs.  The good thing is that it 
includes pre-packaged routines for doing impulse responses, 
forecasting, and decompositions.  It is my favorite for time series, 
because it is one of the few packages that does not strive to do 
everything -- it works to do time series well.

For basic multivariate time series modelling, RATS and Stata can do the 
job well.  I have my quibbles with both (such as Stata not having a 
well defined set of time series "objects" or methods that really 
understand how to work with ts data) and RATS "unique" syntax.  Both 
will allow you to do the standard VAR and VECM models in Hamilton or 
Johansen.

That said, programming in RATS is not for the faint at heart.  Using 
the standard routines works well, but once you start doing more exotic 
things (complex, high dimensional SVARs come to mind), or posterior 
simulations for Bayesian VARs (BVARs), things get more complicated (in 
part because the RATS syntax has a combination of old fashioned Fortran 
and C declarations).  My guess is that while these can all be done in 
RATS with some degree of effort, the effort necessary to do them in 
Stata will be monumental.  I find that whenever I work in RATS I have 
to have a set of manuals nearby.

For these reasons (and as part of a larger project to model 
international conflict data and political economy data), I have started 
on an R package that will estimate VARs, Bayesian VARs, and 
Markov-switching BVARs.  This is being done in R for the obvious 
reasons: 1) it is free / open source, 2) R is gaining wider use in the 
social sciences, 3) I can write the computationally intensive functions 
for the BVARs and MS-BVARs in C++ and make them very fast, and 4) the 
object, scope and method aspects of R lend themselves more easily to 
programming these models.

At present, few if any of these VAR / SVAR extensions are present in 
Stata (even with the new VAR routines they have added, one cannot 
estimate the BVAR models, or any error bands for the impulse 
responses).  RATS has the capacity to offer all of these methods.

Another option is Ox, which is open for academic use, and a reasonable 
fee for non-academic use: http://www.nuff.ox.ac.uk/Users/Doornik/  Ox 
can estimate VAR and VECM models, with many specialized addons.  Also, 
the Ox syntax is remarkably similar to C++.  There is a package for Ox 
that will do most of the state-space modeling outlined in Durbin and 
Koopman as well.


Patrick T. Brandt
Assistant Professor
Department of Political Science
University of North Texas
http://www.psci.unt.edu/~brandt

From kriskumar at earthlink.net  Sat Aug 21 02:36:03 2004
From: kriskumar at earthlink.net (krishna kumar)
Date: Fri Aug 20 23:26:29 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
In-Reply-To: <DC680F53FAEFC34BB70119CA8C545E6D015B43@Mal2pro.mid.dom>
References: <DC680F53FAEFC34BB70119CA8C545E6D015B43@Mal2pro.mid.dom>
Message-ID: <412698F3.9070305@earthlink.net>

I have mucked around with the kalman for estimating time-varying betas.  
there was another interest
in cointegration stuff a few weeks back.

I will clean up my code, and put it up someplace.

One suggestion i have for R-SIGGERS is to have a place to post code like 
a repository.
Someplace like the elseiver computer physics code repository

 http://www.cpc.cs.qub.ac.uk/
for which you have to cough up $$$.  The R-SIG repository should be free.

The idea is already in place for some econometric journals where you 
have people uploading their data-sets and routines.
It would be nice to have a facility where one can upload the code with a 
little blurb of what the routines are doing.

any ideas.??.  I am sure there is a opensource thingie that accepts code 
and a little document and that allows users to rate/leave comments?

If anyone knows one let me know. We are going to see more and more of   
"How do I do foo goo in R ?"    or 
"I  know we can do boomoo in math$  but can you do it in R ?"

Just my 2 cents.



Jeffrey Todd Lins wrote:

>Hi Dirk,
>
>Yes, in stats there is a set of Kalman filter routines and you can use optim for likelihood estimation. 
>I have used it for some state space modeling. There is a chapter in Zivot and Wang's book on the topic as well.
>
>In addition initializing in the KF may be an important consideration - see Harvey and/or Durbin and Koopman.
>
>I have never really used DSE for VAR, ended up writing the code elsewhere, outside of R, but you could look at gretl,
>which is available under GNU GPL and written in C, it contains quite a few bits and pieces, I am assuming you can get the source.
>
>Jeff
>
>
>
>
>
>-----Original Message-----
>From: r-sig-finance-bounces@stat.math.ethz.ch
>[mailto:r-sig-finance-bounces@stat.math.ethz.ch]On Behalf Of Pijus
>Virketis
>Sent: Friday, August 20, 2004 7:37 PM
>To: Dirk Eddelbuettel
>Cc: R-sig-finance@stat.math.ethz.ch
>Subject: RE: [R-sig-finance] VAR, VECM, Kalman,... non-R software
>recommendations?
>
>
>Dear Dirk,
>
>As far as my personal experience goes, I needed to estimate such models
>some time ago, when the R toolkit for this sort of thing was still
>almost empty, so I chose to invest in STATA: it provides a fairly
>complete set of functions to estimate VAR, SVAR and (as of two months
>ago) VECM models, validate their results and stability, and calculate
>all the frequently-needed derivatives, such as the MA forms (i.e. IRFs,
>SIRFs, ...), etc. For what it's worth, I chose STATA over many other
>contenders in the field because it seemed to have some of those R-like
>pro-active qualities, like frequent updates, knowledgeable and involved
>users, and accessible developers (to which I can personally attest after
>running into a couple of bugs in the early SVAR code). The R-STATA
>intercommunication is made possible by the foreign package, batch modes,
>and good old ASCII. ;) STATA programming is a bit laborious, so I always
>only farm out the absolute minimum to it, and do the remainder in R. As
>you said, STATA let me "hit the ground running", and is really not a bad
>compromise.
>
>Of course, today R's own arsenal for time-series econometrics is shaping
>up fast as well. Most significantly, there is now the CRAN urca package
>by Bernhard Pfaff: it provides the means to estimate VECM models (both
>the transitory and long-term flavours) and Johansen's co-integration
>tests built on top them. Sadly, VAR/SVAR and associated battery of
>helper functions are still not available, as far as I am aware.
>
>As for the Kalman filter, there is the Kalman... family of functions in
>stats: perhaps that's a good place to start? Sadly, I have not yet had a
>chance to use space-state models in a proper project, so my knowledge of
>the available tools and their relative capabilities is modest. Also, if
>you can get to it, R. Carmona's neat book "Statistical Analysis of
>Financial Series in S-Plus" (Springer, 2004) has a few sections
>(6.2-6.7) on state-space models and Kalman filtering thereof (S code
>included), with applications to finance.
>
>Cheers,
>
>Pijus
>
>  
>
>>-----Original Message-----
>>From: r-sig-finance-bounces@stat.math.ethz.ch
>>[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of
>>Dirk Eddelbuettel
>>Sent: Friday, August 20, 2004 12:49 PM
>>To: R-sig-finance@stat.math.ethz.ch
>>Subject: [R-sig-finance] VAR, VECM, Kalman,... non-R software
>>recommendations?
>>
>>
>>I've been asked to run some 'modern' regressions: vector
>>autoregression,
>>vector error correction, kalman filter, ... 
>>
>>Of course, I'd love to do that in R and will probably end up
>>writing some
>>code for it, but as the platitude goes, I 'need to hit the
>>ground running'.
>>Last time I looked at Paul Gilbert's dse bundle, it promised
>>most of this,
>>but felt somewhat cumbersome. 
>>
>>Does anybody here have any particular recommendations, and in
>>particular,
>>warnings about software like EViews, Rats, ... in this context ?
>>
>>Thanks in advance,  Dirk
>>
>>--
>>Those are my principles, and if you don't like them... well,
>>I have others.
>>    
>>

From jtl at saxobank.com  Sat Aug 21 00:25:02 2004
From: jtl at saxobank.com (Jeffrey Todd Lins)
Date: Sat Aug 21 00:25:07 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
Message-ID: <DC680F53FAEFC34BB70119CA8C545E6D015B46@Mal2pro.mid.dom>



That reminds me, Dirk.

Brandon Whitcher, who wrote the waveslim package, co-authored a book on filters (mostly on wavelets) with Gencay and Selcuk.  They used an example of a structural TS model for time-varying betas I think, in their chapter on the Kalman. 

Anyway, maybe Brandon Whitcher has made some R code too.

Jeff






-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch]On Behalf Of krishna
kumar
Sent: Saturday, August 21, 2004 2:36 AM
To: R-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] VAR, VECM, Kalman,... non-R software
recommendations?


I have mucked around with the kalman for estimating time-varying betas. 
there was another interest
in cointegration stuff a few weeks back.

I will clean up my code, and put it up someplace.

One suggestion i have for R-SIGGERS is to have a place to post code like
a repository.
Someplace like the elseiver computer physics code repository

 http://www.cpc.cs.qub.ac.uk/
for which you have to cough up $$$.  The R-SIG repository should be free.

The idea is already in place for some econometric journals where you
have people uploading their data-sets and routines.
It would be nice to have a facility where one can upload the code with a
little blurb of what the routines are doing.

any ideas.??.  I am sure there is a opensource thingie that accepts code
and a little document and that allows users to rate/leave comments?

If anyone knows one let me know. We are going to see more and more of  
"How do I do foo goo in R ?"    or
"I  know we can do boomoo in math$  but can you do it in R ?"

Just my 2 cents.



Jeffrey Todd Lins wrote:

>Hi Dirk,
>
>Yes, in stats there is a set of Kalman filter routines and you can use optim for likelihood estimation.
>I have used it for some state space modeling. There is a chapter in Zivot and Wang's book on the topic as well.
>
>In addition initializing in the KF may be an important consideration - see Harvey and/or Durbin and Koopman.
>
>I have never really used DSE for VAR, ended up writing the code elsewhere, outside of R, but you could look at gretl,
>which is available under GNU GPL and written in C, it contains quite a few bits and pieces, I am assuming you can get the source.
>
>Jeff
>
>
>
>
>
>-----Original Message-----
>From: r-sig-finance-bounces@stat.math.ethz.ch
>[mailto:r-sig-finance-bounces@stat.math.ethz.ch]On Behalf Of Pijus
>Virketis
>Sent: Friday, August 20, 2004 7:37 PM
>To: Dirk Eddelbuettel
>Cc: R-sig-finance@stat.math.ethz.ch
>Subject: RE: [R-sig-finance] VAR, VECM, Kalman,... non-R software
>recommendations?
>
>
>Dear Dirk,
>
>As far as my personal experience goes, I needed to estimate such models
>some time ago, when the R toolkit for this sort of thing was still
>almost empty, so I chose to invest in STATA: it provides a fairly
>complete set of functions to estimate VAR, SVAR and (as of two months
>ago) VECM models, validate their results and stability, and calculate
>all the frequently-needed derivatives, such as the MA forms (i.e. IRFs,
>SIRFs, ...), etc. For what it's worth, I chose STATA over many other
>contenders in the field because it seemed to have some of those R-like
>pro-active qualities, like frequent updates, knowledgeable and involved
>users, and accessible developers (to which I can personally attest after
>running into a couple of bugs in the early SVAR code). The R-STATA
>intercommunication is made possible by the foreign package, batch modes,
>and good old ASCII. ;) STATA programming is a bit laborious, so I always
>only farm out the absolute minimum to it, and do the remainder in R. As
>you said, STATA let me "hit the ground running", and is really not a bad
>compromise.
>
>Of course, today R's own arsenal for time-series econometrics is shaping
>up fast as well. Most significantly, there is now the CRAN urca package
>by Bernhard Pfaff: it provides the means to estimate VECM models (both
>the transitory and long-term flavours) and Johansen's co-integration
>tests built on top them. Sadly, VAR/SVAR and associated battery of
>helper functions are still not available, as far as I am aware.
>
>As for the Kalman filter, there is the Kalman... family of functions in
>stats: perhaps that's a good place to start? Sadly, I have not yet had a
>chance to use space-state models in a proper project, so my knowledge of
>the available tools and their relative capabilities is modest. Also, if
>you can get to it, R. Carmona's neat book "Statistical Analysis of
>Financial Series in S-Plus" (Springer, 2004) has a few sections
>(6.2-6.7) on state-space models and Kalman filtering thereof (S code
>included), with applications to finance.
>
>Cheers,
>
>Pijus
>
> 
>
>>-----Original Message-----
>>From: r-sig-finance-bounces@stat.math.ethz.ch
>>[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of
>>Dirk Eddelbuettel
>>Sent: Friday, August 20, 2004 12:49 PM
>>To: R-sig-finance@stat.math.ethz.ch
>>Subject: [R-sig-finance] VAR, VECM, Kalman,... non-R software
>>recommendations?
>>
>>
>>I've been asked to run some 'modern' regressions: vector
>>autoregression,
>>vector error correction, kalman filter, ...
>>
>>Of course, I'd love to do that in R and will probably end up
>>writing some
>>code for it, but as the platitude goes, I 'need to hit the
>>ground running'.
>>Last time I looked at Paul Gilbert's dse bundle, it promised
>>most of this,
>>but felt somewhat cumbersome.
>>
>>Does anybody here have any particular recommendations, and in
>>particular,
>>warnings about software like EViews, Rats, ... in this context ?
>>
>>Thanks in advance,  Dirk
>>
>>--
>>Those are my principles, and if you don't like them... well,
>>I have others.
>>   
>>

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


This email may contain confidential and/or privileged inform...{{dropped}}

From Achim.Zeileis at wu-wien.ac.at  Sat Aug 21 01:34:14 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat Aug 21 01:33:36 2004
Subject: [R-sig-finance] VAR, VECM, Kalman, ... non-R software
	recommendations?
In-Reply-To: <412698F3.9070305@earthlink.net>
References: <DC680F53FAEFC34BB70119CA8C545E6D015B43@Mal2pro.mid.dom>
	<412698F3.9070305@earthlink.net>
Message-ID: <20040821013414.3a0ab9da.Achim.Zeileis@wu-wien.ac.at>

On Fri, 20 Aug 2004 17:36:03 -0700 krishna kumar wrote:

> I have mucked around with the kalman for estimating time-varying
> betas.  there was another interest
> in cointegration stuff a few weeks back.
> 
> I will clean up my code, and put it up someplace.
> 
> One suggestion i have for R-SIGGERS is to have a place to post code
> like a repository.
> Someplace like the elseiver computer physics code repository
> 
>  http://www.cpc.cs.qub.ac.uk/
> for which you have to cough up $$$.  The R-SIG repository should be
> free.
> 
> The idea is already in place for some econometric journals where you 
> have people uploading their data-sets and routines.
> It would be nice to have a facility where one can upload the code with
> a little blurb of what the routines are doing.
> 
> any ideas.??.  I am sure there is a opensource thingie that accepts
> code and a little document and that allows users to rate/leave
> comments?
> 
> If anyone knows one let me know. We are going to see more and more of 
>  
> "How do I do foo goo in R ?"    or 
> "I  know we can do boomoo in math$  but can you do it in R ?"

There are several ways of doing that more generally with R (or other
statistics software). The most obvious idea is, of course, to write a
simple R package and post it on CRAN: this can contain the R functions,
data sets, help pages, further documentation in vignettes etc.
To document what your software is doing and how it can be used, there's
the possibility to submit the package plus documentation to the Journal
of Statistical Software
  http://www.jstatsoft.org/
either for the articles section or the code snippets.

Of course, a full package might seem overkill for some simpler things.
Maybe, it would be a good idea to start a snippet package for
computational finance and econometrics with R. Maybe, we can find a
volunteer who could put something together like Greg Warnes does with
the gregmisc package which hosts nifty code snippets by various authors.

Finally, really simple things could also be posted to the list(s) and
will then be archived anyway. But this would have to be really small
pieces of code, probably.

The advantage of a full CRAN package (possibly one containing snippets
by various authors) is that this is regularly checked and hence needs to
be actively maintained...and it comes in a standard format and can be
trivially accessed via install.packages() and update.packages().
In the repositories of the journals you often find old and out-dated
code which wasn't bug-fixed although mistakes might have been
discovered, with documentation that is insufficient or not easy to
access, data sets in different formats etc. 

Best,
Z

> Just my 2 cents.
> 
> 
> 
> Jeffrey Todd Lins wrote:
> 
> >Hi Dirk,
> >
> >Yes, in stats there is a set of Kalman filter routines and you can
> >use optim for likelihood estimation. I have used it for some state
> >space modeling. There is a chapter in Zivot and Wang's book on the
> >topic as well.
> >
> >In addition initializing in the KF may be an important consideration
> >- see Harvey and/or Durbin and Koopman.
> >
> >I have never really used DSE for VAR, ended up writing the code
> >elsewhere, outside of R, but you could look at gretl, which is
> >available under GNU GPL and written in C, it contains quite a few
> >bits and pieces, I am assuming you can get the source.
> >
> >Jeff
> >
> >
> >
> >
> >
> >-----Original Message-----
> >From: r-sig-finance-bounces@stat.math.ethz.ch
> >[mailto:r-sig-finance-bounces@stat.math.ethz.ch]On Behalf Of Pijus
> >Virketis
> >Sent: Friday, August 20, 2004 7:37 PM
> >To: Dirk Eddelbuettel
> >Cc: R-sig-finance@stat.math.ethz.ch
> >Subject: RE: [R-sig-finance] VAR, VECM, Kalman,... non-R software
> >recommendations?
> >
> >
> >Dear Dirk,
> >
> >As far as my personal experience goes, I needed to estimate such
> >models some time ago, when the R toolkit for this sort of thing was
> >still almost empty, so I chose to invest in STATA: it provides a
> >fairly complete set of functions to estimate VAR, SVAR and (as of two
> >months ago) VECM models, validate their results and stability, and
> >calculate all the frequently-needed derivatives, such as the MA forms
> >(i.e. IRFs, SIRFs, ...), etc. For what it's worth, I chose STATA over
> >many other contenders in the field because it seemed to have some of
> >those R-like pro-active qualities, like frequent updates,
> >knowledgeable and involved users, and accessible developers (to which
> >I can personally attest after running into a couple of bugs in the
> >early SVAR code). The R-STATA intercommunication is made possible by
> >the foreign package, batch modes, and good old ASCII. ;) STATA
> >programming is a bit laborious, so I always only farm out the
> >absolute minimum to it, and do the remainder in R. As you said, STATA
> >let me "hit the ground running", and is really not a bad compromise.
> >
> >Of course, today R's own arsenal for time-series econometrics is
> >shaping up fast as well. Most significantly, there is now the CRAN
> >urca package by Bernhard Pfaff: it provides the means to estimate
> >VECM models (both the transitory and long-term flavours) and
> >Johansen's co-integration tests built on top them. Sadly, VAR/SVAR
> >and associated battery of helper functions are still not available,
> >as far as I am aware.
> >
> >As for the Kalman filter, there is the Kalman... family of functions
> >in stats: perhaps that's a good place to start? Sadly, I have not yet
> >had a chance to use space-state models in a proper project, so my
> >knowledge of the available tools and their relative capabilities is
> >modest. Also, if you can get to it, R. Carmona's neat book
> >"Statistical Analysis of Financial Series in S-Plus" (Springer, 2004)
> >has a few sections(6.2-6.7) on state-space models and Kalman
> >filtering thereof (S code included), with applications to finance.
> >
> >Cheers,
> >
> >Pijus
> >
> >  
> >
> >>-----Original Message-----
> >>From: r-sig-finance-bounces@stat.math.ethz.ch
> >>[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of
> >>Dirk Eddelbuettel
> >>Sent: Friday, August 20, 2004 12:49 PM
> >>To: R-sig-finance@stat.math.ethz.ch
> >>Subject: [R-sig-finance] VAR, VECM, Kalman,... non-R software
> >>recommendations?
> >>
> >>
> >>I've been asked to run some 'modern' regressions: vector
> >>autoregression,
> >>vector error correction, kalman filter, ... 
> >>
> >>Of course, I'd love to do that in R and will probably end up
> >>writing some
> >>code for it, but as the platitude goes, I 'need to hit the
> >>ground running'.
> >>Last time I looked at Paul Gilbert's dse bundle, it promised
> >>most of this,
> >>but felt somewhat cumbersome. 
> >>
> >>Does anybody here have any particular recommendations, and in
> >>particular,
> >>warnings about software like EViews, Rats, ... in this context ?
> >>
> >>Thanks in advance,  Dirk
> >>
> >>--
> >>Those are my principles, and if you don't like them... well,
> >>I have others.
> >>    
> >>
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>

From edd at debian.org  Sat Aug 21 02:33:06 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat Aug 21 02:33:11 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
In-Reply-To: <20040821013414.3a0ab9da.Achim.Zeileis@wu-wien.ac.at>
References: <DC680F53FAEFC34BB70119CA8C545E6D015B43@Mal2pro.mid.dom>
	<412698F3.9070305@earthlink.net>
	<20040821013414.3a0ab9da.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <20040821003306.GA21666@sonny.eddelbuettel.com>

On Sat, Aug 21, 2004 at 01:34:14AM +0200, Achim Zeileis wrote:
> > I have mucked around with the kalman for estimating time-varying
> > betas.  there was another interest
> > in cointegration stuff a few weeks back.
[...]
> There are several ways of doing that more generally with R (or other
> statistics software). The most obvious idea is, of course, to write a
> simple R package and post it on CRAN: this can contain the R functions,
> data sets, help pages, further documentation in vignettes etc.
> To document what your software is doing and how it can be used, there's
> the possibility to submit the package plus documentation to the Journal
> of Statistical Software
>   http://www.jstatsoft.org/
> either for the articles section or the code snippets.

A third option would be a wiki. Detlef Steuer once set one up and announced
it on r-help, but somehow it didn't get adopted. Wikis may have lower
barriers to entry, which can be an advantage. In an ideal world, the
community would edit the wiki to purge truly useless code.
 
> Of course, a full package might seem overkill for some simpler things.
> Maybe, it would be a good idea to start a snippet package for
> computational finance and econometrics with R. Maybe, we can find a
> volunteer who could put something together like Greg Warnes does with
> the gregmisc package which hosts nifty code snippets by various authors.

Yes, that may be a very good compromise.

> Finally, really simple things could also be posted to the list(s) and
> will then be archived anyway. But this would have to be really small
> pieces of code, probably.

Yes, also possible, and probably the easiest method. We could start with
that, and even vet / extend code that way. 

So many ways to contribute code ;-)

Regards, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From edd at debian.org  Sat Aug 21 02:41:34 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat Aug 21 02:41:37 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
In-Reply-To: <DC680F53FAEFC34BB70119CA8C545E6D015B46@Mal2pro.mid.dom>
References: <DC680F53FAEFC34BB70119CA8C545E6D015B46@Mal2pro.mid.dom>
Message-ID: <20040821004134.GB21666@sonny.eddelbuettel.com>

On Sat, Aug 21, 2004 at 12:25:02AM +0200, Jeffrey Todd Lins wrote:
> 
> 
> That reminds me, Dirk.
> 
> Brandon Whitcher, who wrote the waveslim package, co-authored a book on filters (mostly on wavelets) with Gencay and Selcuk.  They used an example of a structural TS model for time-varying betas I think, in their chapter on the Kalman. 
> 
> Anyway, maybe Brandon Whitcher has made some R code too.

Sure did:

  http://cran.r-project.org/src/contrib/Descriptions/waveslim.html

Cut&pasted:

  waveslim: Basic wavelet routines for one-, two- and three-dimensional signal
  processing


  Basic wavelet routines for time series, image and 3D signal analysis. The
  coade provided here is based on wavelet methodology developed in Percival and
  Walden (2000) along with Gencay, Selcuk and Whitcher (2001). All figures in
  chapters 4-7 of GSW are reproducible using this package and R code available
  at the book website below. 

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From edd at debian.org  Sat Aug 21 02:52:28 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat Aug 21 02:52:31 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
In-Reply-To: <9FFA7436-F2E7-11D8-B42E-000A95AC74A2@unt.edu>
References: <FB5014C3177D7E44989DA04617A39E1A028D54@nycdc1.hbk.com>
	<20040820193338.GA17957@sonny.eddelbuettel.com>
	<9FFA7436-F2E7-11D8-B42E-000A95AC74A2@unt.edu>
Message-ID: <20040821005227.GC21666@sonny.eddelbuettel.com>


Patrick,

Thanks for yet another very helpful post in this thread!

On Fri, Aug 20, 2004 at 03:29:24PM -0500, Patrick Brandt wrote:
> I've been a RATS user for about 6 years (*NIX and Windows) and a Stata 
> user for 10.   RATS is a great package for doing all of the standard 
> econometric time series, esp VARs and VECMs.  The good thing is that it

Yes, I used it a for little bit a long time ago.

> includes pre-packaged routines for doing impulse responses, 
> forecasting, and decompositions.  It is my favorite for time series, 
> because it is one of the few packages that does not strive to do 
> everything -- it works to do time series well.

That seems to be a consensus view.

> For basic multivariate time series modelling, RATS and Stata can do the 
> job well.  I have my quibbles with both (such as Stata not having a 
> well defined set of time series "objects" or methods that really 
> understand how to work with ts data) and RATS "unique" syntax.  Both 
> will allow you to do the standard VAR and VECM models in Hamilton or 
> Johansen.
> 
> That said, programming in RATS is not for the faint at heart.  Using 
> the standard routines works well, but once you start doing more exotic 
> things (complex, high dimensional SVARs come to mind), or posterior 
> simulations for Bayesian VARs (BVARs), things get more complicated (in 
> part because the RATS syntax has a combination of old fashioned Fortran 
> and C declarations).  My guess is that while these can all be done in 
> RATS with some degree of effort, the effort necessary to do them in 
> Stata will be monumental.  I find that whenever I work in RATS I have 
> to have a set of manuals nearby.
> 
> For these reasons (and as part of a larger project to model 
> international conflict data and political economy data), I have started 
> on an R package that will estimate VARs, Bayesian VARs, and 
> Markov-switching BVARs.  This is being done in R for the obvious 
> reasons: 1) it is free / open source, 2) R is gaining wider use in the 
> social sciences, 3) I can write the computationally intensive functions 
> for the BVARs and MS-BVARs in C++ and make them very fast, and 4) the 
> object, scope and method aspects of R lend themselves more easily to 
> programming these models.

That sounds very intriguing too, and would complement the kalman filter code
in R's base, as well as Bernhard's urca package.  

Any expected timelines?

> At present, few if any of these VAR / SVAR extensions are present in 
> Stata (even with the new VAR routines they have added, one cannot 
> estimate the BVAR models, or any error bands for the impulse 
> responses).  RATS has the capacity to offer all of these methods.
> 
> Another option is Ox, which is open for academic use, and a reasonable 
> fee for non-academic use: http://www.nuff.ox.ac.uk/Users/Doornik/  Ox

Yes, I also looked at Ox, back when it came out and every now and then
afterwards. I find its licensing to be the most annoying -- free for you but
not for me. Weird hybrid.  That said, a possible contender in this too.

> can estimate VAR and VECM models, with many specialized addons.  Also, 
> the Ox syntax is remarkably similar to C++.  There is a package for Ox 
> that will do most of the state-space modeling outlined in Durbin and 
> Koopman as well.

>From what Koopman said, it is actually the same codebase he contributed to
Ox, gave to Brian Ripley for R around release 1.5.0.  Zivot and Wang credit
it explicitly in the Finmetrics book.

Regards, Dirk 

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From ggrothendieck at myway.com  Sat Aug 21 03:39:25 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat Aug 21 03:39:34 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recommendations?
Message-ID: <20040821013925.040F839A1@mprdmxin.myway.com>


Dirk writes:
>A third option would be a wiki. Detlef Steuer once set one up and >announced
>it on r-help, but somehow it didn't get adopted. Wikis may have lower
>barriers to entry, which can be an advantage. In an ideal world, the
>community would edit the wiki to purge truly useless code.

I just checked and its still at:

   http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome

Anyone could add content to it with almost no work at all.  Someone
could easily set up an R-sig-finance subpage on it too.

From Bernhard.Pfaff at drkw.com  Mon Aug 23 12:19:16 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Mon Aug 23 12:19:25 2004
Subject: [R-sig-finance] VAR, VECM, Kalman, ... non-R software recomme
	ndations?
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29BAC8@ibfftce505.de.ad.drkw.net>

Hello to all and in particular to Dirk and Patrick,

stepping in a little bit late into this thread, I am just wondering why
nobody has mentioned GAUSS so far???

As far as RATS is concerned, Dirk you want to have a look at:

http://www.estima.com/catsinfo.shtml

I worked with the early versions of CATS in the mid/end nineties. It was a
**must have** at that time for conducting VECM.

Cheers,
Bernhard 


> 
> 
> Patrick,
> 
> Thanks for yet another very helpful post in this thread!
> 
> On Fri, Aug 20, 2004 at 03:29:24PM -0500, Patrick Brandt wrote:
> > I've been a RATS user for about 6 years (*NIX and Windows) 
> and a Stata 
> > user for 10.   RATS is a great package for doing all of the 
> standard 
> > econometric time series, esp VARs and VECMs.  The good 
> thing is that it
> 
> Yes, I used it a for little bit a long time ago.
> 
> > includes pre-packaged routines for doing impulse responses, 
> > forecasting, and decompositions.  It is my favorite for 
> time series, 
> > because it is one of the few packages that does not strive to do 
> > everything -- it works to do time series well.
> 
> That seems to be a consensus view.
> 
> > For basic multivariate time series modelling, RATS and 
> Stata can do the 
> > job well.  I have my quibbles with both (such as Stata not having a 
> > well defined set of time series "objects" or methods that really 
> > understand how to work with ts data) and RATS "unique" 
> syntax.  Both 
> > will allow you to do the standard VAR and VECM models in 
> Hamilton or 
> > Johansen.
> > 
> > That said, programming in RATS is not for the faint at 
> heart.  Using 
> > the standard routines works well, but once you start doing 
> more exotic 
> > things (complex, high dimensional SVARs come to mind), or posterior 
> > simulations for Bayesian VARs (BVARs), things get more 
> complicated (in 
> > part because the RATS syntax has a combination of old 
> fashioned Fortran 
> > and C declarations).  My guess is that while these can all 
> be done in 
> > RATS with some degree of effort, the effort necessary to do them in 
> > Stata will be monumental.  I find that whenever I work in 
> RATS I have 
> > to have a set of manuals nearby.
> > 
> > For these reasons (and as part of a larger project to model 
> > international conflict data and political economy data), I 
> have started 
> > on an R package that will estimate VARs, Bayesian VARs, and 
> > Markov-switching BVARs.  This is being done in R for the obvious 
> > reasons: 1) it is free / open source, 2) R is gaining wider 
> use in the 
> > social sciences, 3) I can write the computationally 
> intensive functions 
> > for the BVARs and MS-BVARs in C++ and make them very fast, 
> and 4) the 
> > object, scope and method aspects of R lend themselves more 
> easily to 
> > programming these models.
> 
> That sounds very intriguing too, and would complement the 
> kalman filter code
> in R's base, as well as Bernhard's urca package.  
> 
> Any expected timelines?
> 
> > At present, few if any of these VAR / SVAR extensions are 
> present in 
> > Stata (even with the new VAR routines they have added, one cannot 
> > estimate the BVAR models, or any error bands for the impulse 
> > responses).  RATS has the capacity to offer all of these methods.
> > 
> > Another option is Ox, which is open for academic use, and a 
> reasonable 
> > fee for non-academic use: 
http://www.nuff.ox.ac.uk/Users/Doornik/  Ox

Yes, I also looked at Ox, back when it came out and every now and then
afterwards. I find its licensing to be the most annoying -- free for you but
not for me. Weird hybrid.  That said, a possible contender in this too.

> can estimate VAR and VECM models, with many specialized addons.  Also, 
> the Ox syntax is remarkably similar to C++.  There is a package for Ox 
> that will do most of the state-space modeling outlined in Durbin and 
> Koopman as well.

>From what Koopman said, it is actually the same codebase he contributed to
Ox, gave to Brian Ripley for R around release 1.5.0.  Zivot and Wang credit
it explicitly in the Finmetrics book.

Regards, Dirk 

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From brandt at unt.edu  Mon Aug 23 14:45:07 2004
From: brandt at unt.edu (Patrick Brandt)
Date: Mon Aug 23 14:45:50 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recomme ndations?
In-Reply-To: <18D602BD42B7E24EB810D6454A58DB900A29BAC8@ibfftce505.de.ad.drkw.net>
References: <18D602BD42B7E24EB810D6454A58DB900A29BAC8@ibfftce505.de.ad.drkw.net>
Message-ID: <4351B37D-F502-11D8-9A03-000A95AC74A2@unt.edu>


On Aug 23, 2004, at 5:19 AM, Pfaff, Bernhard wrote:

> Hello to all and in particular to Dirk and Patrick,
>
> stepping in a little bit late into this thread, I am just wondering why
> nobody has mentioned GAUSS so far???
>

Umm, frustration with doing time series in Gauss got me to use R in the 
first place (back around R ~= 0.6)?  Gauss has a high license cost?  
Graphics were not very good in GAUSS?  GAUSS was my high level 
programming language of preference for about 5 years, until I realized 
I could do all of the same things faster in other stuff (RATS, R, OX -- 
things with objects that understand time series).

> As far as RATS is concerned, Dirk you want to have a look at:
>
> http://www.estima.com/catsinfo.shtml
>
> I worked with the early versions of CATS in the mid/end nineties. It 
> was a
> **must have** at that time for conducting VECM.
>

CATS is a must -- I would argue that you really cannot go wrong doing 
VECM and VARs in RATS because that is the MAIN GOAL of the software.

Patrick

[SNIP earlier parts of thread]

Patrick T. Brandt
Assistant Professor
Department of Political Science
University of North Texas
http://www.psci.unt.edu/~brandt

From edd at debian.org  Tue Aug 24 15:30:25 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue Aug 24 15:30:32 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recomme ndations?
In-Reply-To: <18D602BD42B7E24EB810D6454A58DB900A29BAC8@ibfftce505.de.ad.drkw.net>
References: <18D602BD42B7E24EB810D6454A58DB900A29BAC8@ibfftce505.de.ad.drkw.net>
Message-ID: <20040824133025.GA10913@sonny.eddelbuettel.com>


Thanks to all for the continued feedback.  I am now leaning towards
Rats/Cats. I had used Rats a bit in grad school and have experienced its
idiosyncratic nature which may well drive me up the wall. But it is focussed
on what I need, and half the price of Stata.

One last question: Between Rats and Eviews, would anybody speak in favour of
Eviews?

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From Whit.Armstrong at tudor.com  Tue Aug 24 15:37:30 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Tue Aug 24 15:43:02 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,... non-R software recomme 
	ndations?
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF02CAD226@tudor.com>

We use eviews extensively here.  We run four of our fx models in eviews.

Its greatest drawback is that it has no debugging utilities.  If you plan to
use only builtin features it should work fine, but if you plan on writing
your own subroutines, then you will find that it is very cumbersome to use.

I have no experience w/ Rats so I can't offer any insights there.

Good luck,
Whit

> -----Original Message-----
> From: r-sig-finance-bounces@stat.math.ethz.ch 
> [mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of 
> Dirk Eddelbuettel
> Sent: Tuesday, August 24, 2004 9:30 AM
> To: Pfaff, Bernhard
> Cc: R-sig-finance@stat.math.ethz.ch
> Subject: Re: [R-sig-finance] VAR, VECM, Kalman,... non-R 
> software recomme ndations?
> 
> 
> 
> Thanks to all for the continued feedback.  I am now leaning 
> towards Rats/Cats. I had used Rats a bit in grad school and 
> have experienced its idiosyncratic nature which may well 
> drive me up the wall. But it is focussed on what I need, and 
> half the price of Stata.
> 
> One last question: Between Rats and Eviews, would anybody 
> speak in favour of Eviews?
> 
> Dirk
> 
> -- 
> Those are my principles, and if you don't like them... well, 
> I have others.
>                                                 -- Groucho Marx
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>

From Bernhard.Pfaff at drkw.com  Tue Aug 24 15:58:46 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Tue Aug 24 15:58:53 2004
Subject: [R-sig-finance] VAR, VECM, Kalman, ... non-R software recomme
	ndations?
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29BAD3@ibfftce505.de.ad.drkw.net>

Hello Dirk,

just to give you a brief glimpse of what you can expect from the built-in
functions in EViews here is a rough excerpt from the manual for estimating a
vector error correction model:

function:
var_name.ec(trend, n) lag_pairs y1 y2 @ x1 x2

"Specify the order of the VEC by entering one or more pairs of lag
intervals, then list the endogenous variables. Note that the lag orders are
those of the first differences, not the levels. If you are comparing results
to another software program, you should be certain that the specifications
for the lag orders are comparable." 

*Ah, never thought that this would matter*

"You may include exogenous variables, such as seasonal dummies, in the VEC
by including an '@'-sign followed by the list of series..."
"You must specify the trend option and the number of cointegrating equations
n to use...in parenthesis..."

Well, as Whit pointed out earlier, if you leave the menue driven area of
EViews and do your own macro programming, EViews is inferior to RATS/CATS,
IMHO.


Bernhard
> 
> 
> Thanks to all for the continued feedback.  I am now leaning towards
> Rats/Cats. I had used Rats a bit in grad school and have 
> experienced its
> idiosyncratic nature which may well drive me up the wall. But 
> it is focussed
> on what I need, and half the price of Stata.
> 
> One last question: Between Rats and Eviews, would anybody 
> speak in favour of
> Eviews?
> 
> Dirk
> 
> -- 
> Those are my principles, and if you don't like them... well, 
> I have others.
>                                                 -- Groucho Marx
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From t.khan at econ.bbk.ac.uk  Wed Aug 25 13:31:51 2004
From: t.khan at econ.bbk.ac.uk (Taher Khan)
Date: Wed Aug 25 13:50:26 2004
Subject: [R-sig-finance] tips and tricks for rolling regressions?
Message-ID: <s12c86ce.099@markets.econ.bbk.ac.uk>

Greetings!

I want to find out if there is a clever way to do a one-step-ahead
moving window rolling regression using a vectorized operation like with
apply, rather than a for loop.

Does anyone have a code snippet they would like to share for how they do
rolling regressions?!?!

Thanks in advance!

From ajayshah at mayin.org  Wed Aug 25 14:00:41 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Wed Aug 25 14:00:20 2004
Subject: [R-sig-finance] tips and tricks for rolling regressions?
In-Reply-To: <s12c86ce.099@markets.econ.bbk.ac.uk>
References: <s12c86ce.099@markets.econ.bbk.ac.uk>
Message-ID: <20040825120041.GK713@igidr.ac.in>

On Wed, Aug 25, 2004 at 12:31:51PM +0100, Taher Khan wrote:
> Greetings!
> 
> I want to find out if there is a clever way to do a one-step-ahead
> moving window rolling regression using a vectorized operation like with
> apply, rather than a for loop.
> 
> Does anyone have a code snippet they would like to share for how they do
> rolling regressions?!?!

Version 1 : my low-quality hacked-up version -- but using simple
programming constructs that even a child can understand --

snip snip ------------------------------------------------------------
# Using moving windows, we do an OLS regression.
#
# The function returns a matrix with T rows. So there are plenty of
# "NA" values in it. The columns are organised as:
#    beta1 s1 beta2 s2 beta3 s3 sigma R^2
# i.e. we have each regression coefficient followed by it's sigma,
# and then at the end we have the residual sigma and the regression R^2.
#
# Example of usage -- to get moving window mean & sigma - 
#  r = movingWindowRegression(E, length(E$date), width, r.usd ~ 1, 1)
# WARNING: for the model, say 'r.usd~1', not 'E$r.usd~1'.
#
# Use the returned matrix like r[width:T,1] as the moving window mean etc.
movingWindowRegression <- function(data, T, width, model, K) {
  results = matrix(nrow=T, ncol=2*K+2)
  for (i in width:T) {
    details <- summary.lm(lm(model, data[(i-width+1):i,]))
    n=1;
    for (j in 1:K) {
      results[i, n:(n+1)]   = details$coefficients[j, 1:2]
      n = n + 2
    }
    results[i, n] = details$sigma
    results[i, n+1] = details$r.squared
  }
  return(results)
}
snip snip ------------------------------------------------------------






And from Douglas Bates (bates@stat.wisc.edu), a beautiful, brilliant
version, that is faster than mine, but that I only dimly understand --
see http://tolstoy.newcastle.edu.au/R/help/04/04/1269.html -- 

snip snip ------------------------------------------------------------
movingWindow2 <- function(formula, data, width, ...) {
    mCall = match.call()
    mCall$width = NULL
    mCall[[1]] = as.name("lm")
    mCall$x = mCall$y = TRUE
    bigfit = eval(mCall, parent.frame())
    ncoef = length(coef(bigfit))
    width = as.integer(width)[1]
    y = bigfit$y
    x = bigfit$x
    nr = nrow(x)
    stopifnot(width >= ncoef, width <= nr)
    terms = bigfit$terms
    inds = embed(seq(nr), width)[, rev(seq(width))]
    sumrys <- lapply(seq(nrow(inds)),
                     function(st) {
                         ind = inds[st,]
                         fit = lm.fit(x[ind, , drop = FALSE], y[ind])
                         fit$terms = terms
                         class(fit) = "lm"
                         summary(fit)
                     })
    list(coefficients = sapply(sumrys, function(sm) coef(sm)[,"Estimate"]),
         Std.Error = sapply(sumrys, function(sm) coef(sm)[,"Std. Error"]),
         sigma = sapply(sumrys, "[[", "sigma"),
         r.squared = sapply(sumrys, "[[", "r.squared"))
}
snip snip ------------------------------------------------------------

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From ggrothendieck at myway.com  Wed Aug 25 14:29:42 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed Aug 25 14:29:47 2004
Subject: [R-sig-finance] tips and tricks for rolling regressions?
Message-ID: <20040825122942.BC0D93983@mprdmxin.myway.com>



Taher Khan <t.khan@econ.bbk.ac.uk> writes:

> I want to find out if there is a clever way to do a one-step-ahead
> moving window rolling regression using a vectorized operation like with
> apply, rather than a for loop.


There was a thread on r-help about this in which there were
a number of solutions including the ones just listed by Ajay.
Check out the following for a few more:

http://tolstoy.newcastle.edu.au/R/help/04/04/1288.html

http://tolstoy.newcastle.edu.au/R/help/04/04/1371.html

From patrick at burns-stat.com  Wed Aug 25 14:34:46 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed Aug 25 14:35:10 2004
Subject: [R-sig-finance] tips and tricks for rolling regressions?
In-Reply-To: <s12c86ce.099@markets.econ.bbk.ac.uk>
References: <s12c86ce.099@markets.econ.bbk.ac.uk>
Message-ID: <412C8766.5000708@burns-stat.com>


Using something like "apply" rather than a for loop is unlikely
to be significantly more efficient (it might even be less) and it
is going to be much less clear what the code is doing.

If a loop is not sufficiently fast for you, there are techniques of
updating the matrix decomposition with each new observation.
I don't know of any such code for R, but I wouldn't be surprised
if there is an implementation or two floating around.

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Taher Khan wrote:

>Greetings!
>
>I want to find out if there is a clever way to do a one-step-ahead
>moving window rolling regression using a vectorized operation like with
>apply, rather than a for loop.
>
>Does anyone have a code snippet they would like to share for how they do
>rolling regressions?!?!
>
>Thanks in advance!
>
>_______________________________________________
>R-sig-finance@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>

From Achim.Zeileis at wu-wien.ac.at  Wed Aug 25 14:45:03 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed Aug 25 14:44:29 2004
Subject: [R-sig-finance] tips and tricks for rolling regressions?
In-Reply-To: <412C8766.5000708@burns-stat.com>
References: <s12c86ce.099@markets.econ.bbk.ac.uk>
	<412C8766.5000708@burns-stat.com>
Message-ID: <20040825144503.7bbcaf06.Achim.Zeileis@wu-wien.ac.at>

On Wed, 25 Aug 2004 13:34:46 +0100 Patrick Burns wrote:

> 
> Using something like "apply" rather than a for loop is unlikely
> to be significantly more efficient (it might even be less) and it
> is going to be much less clear what the code is doing.
> 
> If a loop is not sufficiently fast for you, there are techniques of
> updating the matrix decomposition with each new observation.
> I don't know of any such code for R, but I wouldn't be surprised
> if there is an implementation or two floating around.

For recursive (rather than rolling) regression, there is an
implementation of the recursive updating technique of Brown, Durbin,
Evans (1975, JRSS B) in recresid() in package strucchange. However, I
only return the recursive residuals, not the recursive estimates.

In efp() (with type set to "RE" or "ME") I do recursive and rolling
regresseion (recursive/moving estimates) but also only in a simple for()
loop. But I just call lm.fit() instead of lm() which saves a lot of
computation time when done repeatedly.

best,
Z

> Patrick Burns
> 
> Burns Statistics
> patrick@burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Taher Khan wrote:
> 
> >Greetings!
> >
> >I want to find out if there is a clever way to do a one-step-ahead
> >moving window rolling regression using a vectorized operation like
> >with apply, rather than a for loop.
> >
> >Does anyone have a code snippet they would like to share for how they
> >do rolling regressions?!?!
> >
> >Thanks in advance!
> >
> >_______________________________________________
> >R-sig-finance@stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
> >  
> >
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>

From Gregor.gawron at rmf.ch  Wed Aug 25 16:11:35 2004
From: Gregor.gawron at rmf.ch (Gregor.gawron@rmf.ch)
Date: Wed Aug 25 16:11:41 2004
Subject: [R-sig-finance] tips and tricks for rolling regressions?
Message-ID: <A3A7D687D463B240844F0718418B68BA4F681A@michexmb01.maninvestments.ad.man.com>

There is a 'running' function in the gregmisc package which I slightly
changed to suit my calcualtions. Below the code for doing one-step ahead
GARCH(1,1) estimations. This is just an extraction of a longer code for
various purposes, so it is surely not the nicest one. However, you can
easily replace the 'garch' function with 'lm' and adjust the output to
your needs.
-----------------------------------------
#moving one-step-ahead GARCH(1,1)
library(tseries)

width<-1500 #the size of the sample window
                        
data(EuStockMarkets) #data taken from package 'tseries'
X<- diff(log(EuStockMarkets))[,"DAX"]

n<-length(X)
from<-sapply((1:n)-width+1,function(x) max(x,1))
to<-1:n
all.elements<-apply(cbind(from,to),1,function(x) seq(x[1],x[2])) 
good.elements<-all.elements[width:length(all.elements)]
better.elements<-as.data.frame(good.elements)

#estimating GARCH(1,1) and extracting the fitted coefficients
myfun<-function(i){
    out<-garch(X[better.elements[,i]])
    garchsum<-summary(out)[[2]][1:3]
    names(garchsum)<-NULL
    return(garchsum)
}

res<-sapply(1:length(better.elements),function(i) myfun(i))
    
#adjusting the output
RES<-matrix(unlist(res))
dim(RES)<-dim(res)
dimnames(RES)<-list(c("a0","a1","b1"),NULL)        
(RES<-t(RES))
---------------------------------------------------



-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Taher Khan
Sent: Mittwoch, 25. August 2004 13:32
To: r-sig-finance
Subject: [R-sig-finance] tips and tricks for rolling regressions?


Greetings!

I want to find out if there is a clever way to do a one-step-ahead
moving window rolling regression using a vectorized operation like with
apply, rather than a for loop.

Does anyone have a code snippet they would like to share for how they do
rolling regressions?!?!

Thanks in advance!

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


Any information in this communication is confidential and ma...{{dropped}}

From Daltonmota at aol.com  Wed Aug 25 16:52:51 2004
From: Daltonmota at aol.com (Daltonmota@aol.com)
Date: Wed Aug 25 16:53:09 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,... non-R software
Message-ID: <661E0171.1F7C003E.0ACFBA63@aol.com>

I used e-views in grad school, and i use it professionaly.
Im planning on use R for time series analyses, since its easier to write functions, but my preliminary research found that parameters estimation of a VAR or VEC differ very much from each software, even with the same specifications. I have to figure out whats going on so to use R as a major software for time series.

Dalton.

From ezivot at u.washington.edu  Wed Aug 25 18:15:58 2004
From: ezivot at u.washington.edu (Eric Zivot)
Date: Wed Aug 25 18:16:04 2004
Subject: [R-sig-finance] VAR, VECM, Kalman,
	... non-R software recomme ndations?
In-Reply-To: <20040824133025.GA10913@sonny.eddelbuettel.com>
Message-ID: <004501c48abe$cf7c0f90$7801a8c0@zivotd800>

I have used Eviews for many years, mostly in my graduate time series class.
I find its VAR functionality to be very good and easy to use. You can do
traditional VARs as well as structural VARs with a push of a button. The
cointegration/VECM functionality is also quite good and easy to use. You can
easily impose restrictions on the VECM parameters, produce forecasts, do
backtesting etc. Eviews excels in dynamic multivariate systems estimation.
You can create very flexible multivariate models and estimate them using
GMM, MLE etc. You can also estimate general models by MLE. The main
drawbacks To Eviews are that it lacks a good programming/scripting language
(it has one but it is not intuitive to use); and (2) you cannot write custom
functions - you are stuck with its canned routines (which are very good).
Note: version 5 of Eviews looks like a very good update, particularly if you
are interested in doing dynamic panel data analysis.  The book on
macroeconometrics by Favaro has many advanced VAR modeling examples using
Eviews. 

I find Eviews excellent for teaching and for doing routine time series
regression etc. In fact, Jeff and I modeled the FinMetrics VAR functionality
to match what EViews does. 

I used RATS many years ago. It is less friendly than Eviews but is more
programmable. It is used by many researchers in economics. Also, Walter
Enders has a RATS companion for his applied time series econometrics book.
If you want to do very fancy VAR analysis then RATS is probably better than
Eviews. In particular, RATS can do a variety of Bayesian VAR models
(FinMetrics also).

-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Dirk
Eddelbuettel
Sent: Tuesday, August 24, 2004 5:30 AM
To: Pfaff, Bernhard
Cc: R-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] VAR, VECM, Kalman,... non-R software recomme
ndations?


Thanks to all for the continued feedback.  I am now leaning towards
Rats/Cats. I had used Rats a bit in grad school and have experienced its
idiosyncratic nature which may well drive me up the wall. But it is focussed
on what I need, and half the price of Stata.

One last question: Between Rats and Eviews, would anybody speak in favour of
Eviews?

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance

From ajayshah at mayin.org  Thu Aug 26 07:12:30 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu Aug 26 07:12:06 2004
Subject: [R-sig-finance] tips and tricks for rolling regressions?
In-Reply-To: <A3A7D687D463B240844F0718418B68BA4F681A@michexmb01.maninvestments.ad.man.com>
References: <A3A7D687D463B240844F0718418B68BA4F681A@michexmb01.maninvestments.ad.man.com>
Message-ID: <20040826051230.GD713@igidr.ac.in>

> #moving one-step-ahead GARCH(1,1)
> library(tseries)
> 
> width<-1500 #the size of the sample window
>                         
> data(EuStockMarkets) #data taken from package 'tseries'
> X<- diff(log(EuStockMarkets))[,"DAX"]
> 
> n<-length(X)
> from<-sapply((1:n)-width+1,function(x) max(x,1))
> to<-1:n
> all.elements<-apply(cbind(from,to),1,function(x) seq(x[1],x[2])) 
> good.elements<-all.elements[width:length(all.elements)]
> better.elements<-as.data.frame(good.elements)
> 
> #estimating GARCH(1,1) and extracting the fitted coefficients
> myfun<-function(i){
>     out<-garch(X[better.elements[,i]])
>     garchsum<-summary(out)[[2]][1:3]
>     names(garchsum)<-NULL
>     return(garchsum)
> }
> 
> res<-sapply(1:length(better.elements),function(i) myfun(i))
>     
> #adjusting the output
> RES<-matrix(unlist(res))
> dim(RES)<-dim(res)
> dimnames(RES)<-list(c("a0","a1","b1"),NULL)        
> (RES<-t(RES))

When I run your code, I get lots and lots of 
 ***** FALSE CONVERGENCE *****
error messages from the garch() function in the tseries
library. Should I be worried?

         -ans.

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From Gregor.gawron at rmf.ch  Thu Aug 26 09:06:02 2004
From: Gregor.gawron at rmf.ch (Gregor.gawron@rmf.ch)
Date: Thu Aug 26 09:06:14 2004
Subject: [R-sig-finance] tips and tricks for rolling regressions?
Message-ID: <A3A7D687D463B240844F0718418B68BA4F681C@michexmb01.maninvestments.ad.man.com>

I don't get this messege. Maybe, a different sample lenght will help. In
fact, in my original code I am using a different data than the DAX. I
took DAX just for illustrative purposes since the data is available in
'tseries' package.

Gregor



-----Original Message-----
From: Ajay Shah [mailto:ajayshah@mayin.org] 
Sent: Donnerstag, 26. August 2004 07:13
To: Gawron, Gregor (MI Switzerland)
Cc: t.khan@econ.bbk.ac.uk; r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] tips and tricks for rolling regressions?


> #moving one-step-ahead GARCH(1,1)
> library(tseries)
> 
> width<-1500 #the size of the sample window
>                         
> data(EuStockMarkets) #data taken from package 'tseries'
> X<- diff(log(EuStockMarkets))[,"DAX"]
> 
> n<-length(X)
> from<-sapply((1:n)-width+1,function(x) max(x,1))
> to<-1:n
> all.elements<-apply(cbind(from,to),1,function(x) seq(x[1],x[2]))
> good.elements<-all.elements[width:length(all.elements)]
> better.elements<-as.data.frame(good.elements)
> 
> #estimating GARCH(1,1) and extracting the fitted coefficients 
> myfun<-function(i){
>     out<-garch(X[better.elements[,i]])
>     garchsum<-summary(out)[[2]][1:3]
>     names(garchsum)<-NULL
>     return(garchsum)
> }
> 
> res<-sapply(1:length(better.elements),function(i) myfun(i))
>     
> #adjusting the output
> RES<-matrix(unlist(res))
> dim(RES)<-dim(res)
> dimnames(RES)<-list(c("a0","a1","b1"),NULL)        
> (RES<-t(RES))

When I run your code, I get lots and lots of 
 ***** FALSE CONVERGENCE *****
error messages from the garch() function in the tseries library. Should
I be worried?

         -ans.

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi


Any information in this communication is confidential and ma...{{dropped}}

From ajayshah at mayin.org  Thu Aug 26 09:16:37 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu Aug 26 09:16:09 2004
Subject: [R-sig-finance] Convergence problems in garch(1,1)
In-Reply-To: <A3A7D687D463B240844F0718418B68BA4F681C@michexmb01.maninvestments.ad.man.com>
References: <A3A7D687D463B240844F0718418B68BA4F681C@michexmb01.maninvestments.ad.man.com>
Message-ID: <20040826071637.GL713@igidr.ac.in>

On Thu, Aug 26, 2004 at 09:06:02AM +0200, Gregor.gawron@rmf.ch wrote:
> I don't get this messege. Maybe, a different sample lenght will help. In
> fact, in my original code I am using a different data than the DAX. I
> took DAX just for illustrative purposes since the data is available in
> 'tseries' package.

Hmm, if I do:

     library(tseries)
     data(EuStockMarkets)  
     dax <- diff(log(EuStockMarkets))[,"DAX"]
     m1 <- garch(dax)  # Fit a GARCH(1,1) to DAX returns

all is well. But if I say:

     m2 <- garch(dax[1:1500])

I get:

 ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 


     I     INITIAL X(I)        D(I)

     1     0.738721E-04     0.100E+01
     2     0.500000E-01     0.100E+01
     3     0.500000E-01     0.100E+01

    IT   NF       F        RELDF    PRELDF    RELDX   STPPAR   D*STEP   NPRELDF

     0    1 -0.631E+04
     1    9 -0.631E+04  0.12E-06  0.25E-06  0.1E-05  0.1E+12  0.1E-06  0.16E+05
     2   20 -0.631E+04  0.39E-03  0.89E-03  0.4E+00  0.2E+01  0.1E+00  0.26E+00
     3   22 -0.631E+04  0.73E-03  0.13E-02  0.5E+00  0.2E+01  0.2E+00  0.17E+00
     4   24 -0.631E+04  0.14E-03  0.19E-03  0.8E-01  0.1E+01  0.6E-01  0.34E-03
     5   27 -0.632E+04  0.94E-03  0.66E-03  0.2E+00  0.3E+00  0.2E+00  0.18E-02
     6   39 -0.632E+04  0.75E-04  0.12E-03  0.8E-06  0.4E+01  0.1E-05  0.86E-01
     7   40 -0.632E+04  0.24E-06  0.32E-06  0.8E-06  0.2E+01  0.1E-05  0.93E-01
     8   50 -0.632E+04  0.39E-03  0.36E-03  0.5E-01  0.2E+01  0.6E-01  0.92E-01
     9   52 -0.633E+04  0.69E-03  0.88E-03  0.8E-01  0.2E+01  0.1E+00  0.14E+01
    10   60 -0.633E+04  0.39E-04  0.86E-04  0.2E-06  0.6E+01  0.4E-06  0.35E-03
    11   61 -0.633E+04  0.11E-05  0.12E-05  0.2E-06  0.2E+01  0.4E-06  0.14E-03
    12   70 -0.633E+04  0.94E-11  0.52E-09  0.1E-09  0.1E+02  0.2E-09  0.14E-03
    13   83 -0.633E+04 -0.20E-14  0.43E-13  0.1E-13  0.1E+06  0.2E-13  0.14E-03

 ***** FALSE CONVERGENCE *****

 FUNCTION    -0.632703E+04   RELDX        0.122E-13
 FUNC. EVALS      83         GRAD. EVALS      13
 PRELDF       0.431E-13      NPRELDF      0.137E-03

     I      FINAL X(I)        D(I)          G(I)

     1    0.840058E-05     0.100E+01     0.135E+05
     2    0.699397E-01     0.100E+01     0.721E+02
     3    0.832688E+00     0.100E+01     0.371E+01




The two estimates differ considerably, so it _does_ seem to be a case
of "false convergence":

> m1

Call:
garch(x = dax)

Coefficient(s):
       a0         a1         b1  
4.639e-06  6.833e-02  8.891e-01  

> m2

Call:
garch(x = dax[1:1500])

Coefficient(s):
       a0         a1         b1  
0.0000084  0.0699397  0.8326876  

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From patrick at burns-stat.com  Thu Aug 26 13:18:56 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu Aug 26 13:19:14 2004
Subject: [R-sig-finance] tips and tricks for rolling regressions?
In-Reply-To: <20040826051230.GD713@igidr.ac.in>
References: <A3A7D687D463B240844F0718418B68BA4F681A@michexmb01.maninvestments.ad.man.com>
	<20040826051230.GD713@igidr.ac.in>
Message-ID: <412DC720.5050001@burns-stat.com>

At least some investigation -- if not absolute worry -- is probably
in order.  GARCH can confuse optimizers, so it is entirely believable
that you are not getting optimal results.

Though it doesn't make sense to me, some times changing the scale
of the data can make the optimizer behave better.  Proprietary code
that I wrote liked to have the returns in percent rather than fractional
form.

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Ajay Shah wrote:

>>#moving one-step-ahead GARCH(1,1)
>>library(tseries)
>>
>>width<-1500 #the size of the sample window
>>                        
>>data(EuStockMarkets) #data taken from package 'tseries'
>>X<- diff(log(EuStockMarkets))[,"DAX"]
>>
>>n<-length(X)
>>from<-sapply((1:n)-width+1,function(x) max(x,1))
>>to<-1:n
>>all.elements<-apply(cbind(from,to),1,function(x) seq(x[1],x[2])) 
>>good.elements<-all.elements[width:length(all.elements)]
>>better.elements<-as.data.frame(good.elements)
>>
>>#estimating GARCH(1,1) and extracting the fitted coefficients
>>myfun<-function(i){
>>    out<-garch(X[better.elements[,i]])
>>    garchsum<-summary(out)[[2]][1:3]
>>    names(garchsum)<-NULL
>>    return(garchsum)
>>}
>>
>>res<-sapply(1:length(better.elements),function(i) myfun(i))
>>    
>>#adjusting the output
>>RES<-matrix(unlist(res))
>>dim(RES)<-dim(res)
>>dimnames(RES)<-list(c("a0","a1","b1"),NULL)        
>>(RES<-t(RES))
>>    
>>
>
>When I run your code, I get lots and lots of 
> ***** FALSE CONVERGENCE *****
>error messages from the garch() function in the tseries
>library. Should I be worried?
>
>         -ans.
>
>  
>

From christoph.lehmann at gmx.ch  Mon Aug 30 10:22:37 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon Aug 30 10:23:20 2004
Subject: [R-sig-finance] correlation between two stock market indices
Message-ID: <4132E3CD.7050509@gmx.ch>

Dear finance professionals

As I was asked by a friend, whether we can compute the correlation 
between two stock market indices (e.g. NASDAQ index and Dow Jones 
index), and I am unfortunately NOT an expert in finance:

(1) What model would you recommend for this kind of question?

something like:

library(ts)
arima(x, order=???, xreg=y)

library(nlme)
gls(x~y,correlation=corARMA(p=?,q=?))

what would you recommend, and what about the "?" :)

(2) Furthermore, searching the web, I found, that (sorry, you experts 
certainly know this, but I have no experience with financial data), 
usually the time series are uncorrelated, but show strong "ARCH 
effects", ie., are not independent.

Does this mean, that any kind of correlation analysis with stock market
indices is senseless, since maybe we don't get a sign. correlation, but 
this doesn't mean that the series are independent?

Many thanks for your help

Chris

From Bernhard.Pfaff at drkw.com  Mon Aug 30 11:08:43 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Mon Aug 30 11:08:51 2004
Subject: [R-sig-finance] correlation between two stock market indices
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29BAF4@ibfftce505.de.ad.drkw.net>

> 
> Dear finance professionals
> 
> As I was asked by a friend, whether we can compute the correlation 
> between two stock market indices (e.g. NASDAQ index and Dow Jones 
> index), and I am unfortunately NOT an expert in finance:

Hello Christoph,

you can almost always compute correlations, if these calculations make sense
and are meaningful is a different matter :-)

> 
> (1) What model would you recommend for this kind of question?
> 
> something like:
> 
> library(ts)
> arima(x, order=???, xreg=y)

sure, you can do this and choose the appropriate order as it is outlined by
Box-Jenkins (i.e. check the acf and pacf of the residuals combined with
diagnostic tests for serial uncorrelatedness). Most likely you want/have to
work with differenced data, due to the *trending* character of the ts in
question. The snag is that level information is lost. Hence, you might want
to specify an ECM / VECM and prior to this check the order of integration of
the series involved. Relevant packages to accomplish this would be ts,
tseries, dse and urca; to my knowledge (check
http://www.mayin.org/ajayshah/KB/R/R_for_economists.html for an overview). 

> 
> library(nlme)
> gls(x~y,correlation=corARMA(p=?,q=?))
> 
> what would you recommend, and what about the "?" :)

this would apply if the *error term* is not nicely behaved and would follow
as a second step, hence after checking the residuals from a simple lm() or
arima(), as is described from ?gls

Description:

     This function fits a linear model using generalized least squares.
     The errors are allowed to be correlated and/or have unequal
     variances.

As a side note, in econometrics it is common notation that the response is
named 'y' and the predictor 'x' and not vice versa.

> 
> (2) Furthermore, searching the web, I found, that (sorry, you experts 
> certainly know this, but I have no experience with financial data), 
> usually the time series are uncorrelated, but show strong "ARCH 
> effects", ie., are not independent.

ARCH refers to the behaviour of the variance of the error term
(autoregressive conditional heteroskedasticity). Again, check the residuals
first, if ARCH is prevailent and only then estimate an ARCH, GARCH etc. type
of model. Note, uncorrelatedness and independence are only equivalent in
case of normality. The former does not imply the latter, only if the the
series are normally distributed. But if two series are independent then
these series are also uncorrelated.

A last side note, ask yourself what the model's aim is. What should the
model explain? What's it purpose? After having answered these questions, you
can pick one of methods and not blindly apply either one of them.

HTH,
Bernhard


> 
> Does this mean, that any kind of correlation analysis with 
> stock market
> indices is senseless, since maybe we don't get a sign. 
> correlation, but 
> this doesn't mean that the series are independent?
> 
> Many thanks for your help
> 
> Chris
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From Bernhard.Pfaff at drkw.com  Mon Aug 30 11:56:01 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Mon Aug 30 11:56:05 2004
Subject: FW: [R-sig-finance] correlation between two stock market indices
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29BAF6@ibfftce505.de.ad.drkw.net>


> > 
> > Many thanks, Bernhard!
> > 
> > What do you think about the suggestion, made by another 
> list member, 
> > that I can just compute the correlation for the differentiated data 
> > between the two stock market index series, with no control for 
> > autocorrelation, etc, since according to the effective market 
> > hypothesis 
> > stock market index series don't show autocorrelation at all?
> 
well, here you are superimposing the validity of a 
hypothesis, that should be checked first. By using 
differenced data you are almost always on the *safe side*, 
but again you are giving up the information content of the 
series in levels. This can be circumvented by specifying an 
ECM. Furthermore, you might want to use log data, i.e. a 
transformation that stabilises the variance. As a side effect 
the lm() estimated coefficients can be interpreted as 
elasticities, i.e. the responsiveness of your lhs-variable to 
a unit change of your rhs-variable (in levels).

> 
> > 
> > I think, I will just check, if there isn't an 
> > autocorrelation, checking 
> > acf and pacf, as you suggested. Thanks a lot.
> 
yes, and this tells you the order to specify for arma(), 
given a stationary series:

ar(p): slowly decaying acf (or dampening alternating in case 
of negative ar coeffcient) and a spike at p in the pacf.

ma(q): just like ar(p), but the shape of acf and pacf are 
reversed, i.e. single peak in the acf and slowly decaying 
pacf (or dampening alternating in case of negative ma coeffcient).

HTH,
Bernhard

> 
> > 
> > Cheers
> > 
> > Christoph
> > 
> > Pfaff, Bernhard wrote:
> > >>Dear finance professionals
> > >>
> > >>As I was asked by a friend, whether we can compute the 
> correlation 
> > >>between two stock market indices (e.g. NASDAQ index and Dow Jones 
> > >>index), and I am unfortunately NOT an expert in finance:
> > > 
> > > 
> > > Hello Christoph,
> > > 
> > > you can almost always compute correlations, if these 
> > calculations make sense
> > > and are meaningful is a different matter :-)
> > > 
> > > 
> > >>(1) What model would you recommend for this kind of question?
> > >>
> > >>something like:
> > >>
> > >>library(ts)
> > >>arima(x, order=???, xreg=y)
> > > 
> > > 
> > > sure, you can do this and choose the appropriate order as 
> > it is outlined by
> > > Box-Jenkins (i.e. check the acf and pacf of the residuals 
> > combined with
> > > diagnostic tests for serial uncorrelatedness). Most likely 
> > you want/have to
> > > work with differenced data, due to the *trending* character 
> > of the ts in
> > > question. The snag is that level information is lost. 
> > Hence, you might want
> > > to specify an ECM / VECM and prior to this check the order 
> > of integration of
> > > the series involved. Relevant packages to accomplish this 
> > would be ts,
> > > tseries, dse and urca; to my knowledge (check
> > > http://www.mayin.org/ajayshah/KB/R/R_for_economists.html 
> > for an overview). 
> > > 
> > > 
> > >>library(nlme)
> > >>gls(x~y,correlation=corARMA(p=?,q=?))
> > >>
> > >>what would you recommend, and what about the "?" :)
> > > 
> > > 
> > > this would apply if the *error term* is not nicely behaved 
> > and would follow
> > > as a second step, hence after checking the residuals from a 
> > simple lm() or
> > > arima(), as is described from ?gls
> > > 
> > > Description:
> > > 
> > >      This function fits a linear model using generalized 
> > least squares.
> > >      The errors are allowed to be correlated and/or have unequal
> > >      variances.
> > > 
> > > As a side note, in econometrics it is common notation that 
> > the response is
> > > named 'y' and the predictor 'x' and not vice versa.
> > > 
> > > 
> > >>(2) Furthermore, searching the web, I found, that (sorry, 
> > you experts 
> > >>certainly know this, but I have no experience with 
> financial data), 
> > >>usually the time series are uncorrelated, but show strong "ARCH 
> > >>effects", ie., are not independent.
> > > 
> > > 
> > > ARCH refers to the behaviour of the variance of the error term
> > > (autoregressive conditional heteroskedasticity). Again, 
> > check the residuals
> > > first, if ARCH is prevailent and only then estimate an 
> > ARCH, GARCH etc. type
> > > of model. Note, uncorrelatedness and independence are only 
> > equivalent in
> > > case of normality. The former does not imply the latter, 
> > only if the the
> > > series are normally distributed. But if two series are 
> > independent then
> > > these series are also uncorrelated.
> > > 
> > > A last side note, ask yourself what the model's aim is. 
> > What should the
> > > model explain? What's it purpose? After having answered 
> > these questions, you
> > > can pick one of methods and not blindly apply either one of them.
> > > 
> > > HTH,
> > > Bernhard
> > > 
> > > 
> > > 
> > >>Does this mean, that any kind of correlation analysis with 
> > >>stock market
> > >>indices is senseless, since maybe we don't get a sign. 
> > >>correlation, but 
> > >>this doesn't mean that the series are independent?
> > >>
> > >>Many thanks for your help
> > >>
> > >>Chris
> > >>
> > >>_______________________________________________
> > >>R-sig-finance@stat.math.ethz.ch mailing list
> > >>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > >>
> > > 
> > > 
> > > 
> > > 
> > --------------------------------------------------------------
> > ------------------
> > > The information contained herein is confidential and is 
> > intended solely for the
> > > addressee. Access by any other party is unauthorised 
> > without the express
> > > written permission of the sender. If you are not the 
> > intended recipient, please
> > > contact the sender either via the company switchboard on 
> > +44 (0)20 7623 8000, or
> > > via e-mail return. If you have received this e-mail in 
> > error or wish to read our
> > > e-mail disclaimer statement and monitoring policy, please 
> refer to 
> > > http://www.drkw.com/disc/email/ or contact the sender. 3167
> > > 
> > --------------------------------------------------------------
> > ------------------
> > > 
> > > 
> > 
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From ajayshah at mayin.org  Sun Aug 29 15:45:28 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue Aug 31 06:04:50 2004
Subject: [R-sig-finance] 3-d visualisation of yield curves?
Message-ID: <20040829134528.GK713@igidr.ac.in>

A yield curve is a picture z(t) running (say) from maturity t=0 to
t=10 years. And, the yield curve fluctuates; there's a new one every
day. So you have a time-series of curves.

I have seen very pretty 3d color visualisation of the changing yield
curve. The x axis is time, the y axis is maturity, the z axis is the
interest rate. Low interest rates are coloured blue and high interest
rates are coloured red. I have also seen a user-interface where the
person can change the point of perspective and interactively look at
the scene from different vantage points.

q: How would one do this in R? :-) Have you deployed ggobi or lattice
   graphics for the task?

q: Does it add insight to look at yield curves in this way? Wise men
   on the r-help mailing list generally say that you don't gain too
   much insight from pretty 3d plots, so I have de-emphasised
   them. But one can't help feeling tempted by those pretty pictures!

There's a code fragment at the end of this email, which sets up a
matrix syc.points which has 118 rows (for 118 days) and 100 columns,
where each column is the spot interest rate at one maturity. The
maturities run from 0.1 to 10 years. This may help you play with the
question. In case it's of interest, this is the spot yield curve in
India from 1 April this year onwards, which has been an exciting time.

Thanks,

        -ans.

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi




nsz <- function(b0, b1, b2, tau, t) {
  tmp = t/tau
  tmp2 = exp(-tmp)
  return(b0 + ((b1+b2)*(1-tmp2)/(tmp)) - (b2*tmp2))
}

ns <- structure(list(date = as.integer(c(20040401, 20040402, 20040405, 
20040406, 20040407, 20040408, 20040410, 20040412, 20040413, 20040415, 
20040416, 20040417, 20040419, 20040420, 20040421, 20040422, 20040423, 
20040424, 20040427, 20040428, 20040429, 20040430, 20040503, 20040505, 
20040506, 20040507, 20040508, 20040510, 20040511, 20040512, 20040513, 
20040514, 20040515, 20040517, 20040518, 20040519, 20040520, 20040521, 
20040522, 20040524, 20040525, 20040526, 20040527, 20040528, 20040529, 
20040531, 20040601, 20040602, 20040603, 20040604, 20040605, 20040607, 
20040608, 20040609, 20040610, 20040611, 20040612, 20040614, 20040615, 
20040616, 20040617, 20040618, 20040619, 20040621, 20040622, 20040623, 
20040624, 20040625, 20040626, 20040628, 20040629, 20040630, 20040701, 
20040702, 20040703, 20040705, 20040706, 20040707, 20040708, 20040710, 
20040712, 20040713, 20040714, 20040715, 20040716, 20040717, 20040719, 
20040720, 20040721, 20040722, 20040723, 20040724, 20040726, 20040727, 
20040728, 20040729, 20040730, 20040731, 20040802, 20040803, 20040804, 
20040805, 20040806, 20040807, 20040809, 20040810, 20040811, 20040812, 
20040813, 20040814, 20040816, 20040817, 20040818, 20040819, 20040821, 
20040823, 20040824, 20040825)), b0 = c(7.3422, 7.8217, 11.3901, 
7.4635, 7.2226, 7.2234, 7.3554, 7.3074, 8.468, 7.4894, 7.333, 
7.4633, 7.6058, 7.1231, 7.686, 7.0339, 7.4681, 7.3624, 7.3174, 
7.4157, 7.7941, 7.4091, 7.8367, 7.4442, 7.7108, 7.4472, 7.4185, 
7.1485, 7.0891, 7.3275, 6.9325, 7.0438, 7.0387, 7.2428, 7.7536, 
7.1781, 7.1971, 7.1158, 9.4099, 14.0915, 6.9736, 7.3604, 8.2228, 
8.9938, 9.2337, 14.5484, 11.5983, 14.1417, 11.5225, 7.516, 7.7201, 
7.5108, 7.575, 7.5737, 8.1235, 7.6523, 7.8026, 7.4334, 14.4161, 
7.4442, 8.3493, 7.7468, 7.8207, 7.8434, 8.3185, 8.0729, 7.9769, 
8.1296, 7.4141, 7.7981, 8.023, 8.2652, 7.9386, 7.8601, 7.9348, 
10.3803, 9.4022, 7.7753, 9.1768, 8.8638, 7.8373, 7.421, 7.4414, 
7.3915, 7.2947, 7.1905, 7.379, 6.9234, 7.5494, 6.9746, 7.9473, 
14.7695, 7.3734, 7.3422, 4.6736, 7.4979, 7.4727, 7.3631, 7.8308, 
7.6939, 7.0801, 7.7495, 5.0205, 7.6851, 15.2449, 11.4073, 10.5747, 
7.47, 8.06, 15.7304, 10.26, 7.3, 9.51, 9.32, 8.838, 13.3616, 
15.5854, 11.0715), b1 = c(-2.721, -3.2183, -6.5193, -3.2818, 
-2.4963, -2.6716, -2.4753, -2.8383, -3.9206, -2.8313, -2.7894, 
-2.8732, -3.2573, -2.5732, -2.9594, -2.3505, -2.6395, -2.6339, 
-2.3883, -2.7691, -3.286, -2.5451, -3.0089, -2.9315, -3.2165, 
-2.7868, -1.4352, -2.4697, -2.0563, -2.7682, -2.5612, -2.5146, 
-2.5144, -2.4144, -2.9954, -2.3257, -2.1419, -2.1677, -4.3465, 
-9.4, -2.1492, -2.6646, -3.4617, -4.1504, -3.1783, -9.4, -6.6092, 
-9.4, -6.5229, -2.3708, -2.3271, -2.9659, -2.7749, -2.5683, -3.1604, 
-2.3273, -2.543, -2.7601, -9.4, -2.2319, -3.1117, -3.3958, -2.7336, 
-2.7717, -3.1778, -2.8743, -2.9824, -3.3025, -2.3269, -2.8134, 
-3.2428, -3.4767, -3.2345, -3.019, -2.7832, -5.2133, -4.2563, 
-2.9053, -4.092, -3.7751, -2.9284, -2.381, -2.6428, -2.7985, 
-2.5906, -2.1872, -2.7013, -2.4288, -2.5056, -2.529, -2.9546, 
-9.4, -2.2121, -2.419, -0.0105, -2.6278, -2.7345, -2.278, -2.6838, 
-2.2775, -3.0019, -2.4925, -0.0191, -2.4934, -9.2345, -5.4497, 
-4.4509, -2.88, -2.31, -9.1449, -3.96, -3.95, -3.4, -3.28, -3.1081, 
-6.9305, -9.4, -4.9702), b2 = c(-2.9455, -3.1714, -7.1921, -1.2954, 
-2.9376, -2.3315, -3.3516, -1.9412, -2.8231, -2.9133, -2.3912, 
-2.7164, -0.6094, -2.4146, -3.0891, -2.8961, -3.452, -2.9017, 
-3.525, -2.7331, -2.2974, -3.4923, -3.5535, -2.2847, -2.0384, 
-2.8926, -5.7102, -2.7131, -3.5915, -2.4034, -1.4584, -2.2325, 
-1.6771, -3.3011, -3.3382, -3.2615, -3.7138, -3.5048, -6.5431, 
-8.59, -3.4933, -2.9518, -3.7216, -4.8041, -8.4707, -11.6242, 
-7.4944, -8.6059, -7.4215, -4.0466, -4.4962, -2.4366, -2.9504, 
-3.7409, -3.5684, -4.1972, -4.0236, -1.9687, -9.1264, -3.5223, 
-4.1759, -1.2065, -3.4071, -3.2178, -3.4439, -3.0968, -2.1767, 
-0.6931, -2.2134, -2.3287, -0.1982, -3.198, 0.0274, -1.4918, 
-2.1835, -2.5059, -1.2104, -3e-04, -0.5105, -0.1034, -0.0324, 
-0.0108, -0.0785, -0.0014, 0, -0.4718, -0.0261, 0.036, -0.0113, 
0.0142, -0.0189, -7.7247, -0.0115, -0.0146, 7.0134, 0.0116, 0.0098, 
-0.0168, -0.0012, -0.0064, 0.0239, 0.0032, 6.0088, 0.01, -7.839, 
-2.518, -1.4064, 0.02, -0.01, -9.5194, -2.12, 1.62, -1.98, -0.04, 
-0.0255, -6.7197, -9.2038, -2.9766), tau = c(4.5266, 5.7445, 
10.7687, 6.682, 4.496, 5.1623, 4.8581, 6.3769, 9.7641, 5.8551, 
5.9208, 6.1627, 11.1458, 4.9739, 6.5382, 4.228, 5.2418, 5.5392, 
4.6344, 5.4967, 7.0604, 4.7554, 6.2326, 5.9656, 7.7035, 5.3672, 
3.8092, 4.2389, 3.5272, 4.9461, 4.8159, 4.0655, 4.972, 3.5159, 
4.9192, 3.8536, 3.6992, 3.4197, 5.8819, 14.656, 2.8546, 4.2702, 
5.9606, 6.1133, 4.7138, 10.0547, 9.0578, 12.9292, 8.6205, 3.6096, 
4.4072, 4.5344, 4.5593, 3.6541, 5.1583, 3.7913, 4.2516, 4.5431, 
13.0367, 3.5478, 5.3657, 5.4139, 3.8999, 3.9799, 4.6267, 4.1281, 
4.5191, 6.8646, 2.9362, 3.6888, 7.3445, 3.835, 7.5136, 4.6792, 
5.2262, 15.5, 15.5, 8.7869, 15.5, 15.4999, 8.3618, 7.8984, 5.6957, 
4.3874, 4.4051, 4.7968, 5.0399, 3.0356, 7.3138, 2.8834, 7.6381, 
15.5, 6.3357, 4.7519, 10.974, 3.925, 3.7252, 4.5168, 6.2362, 
6.282, 1.6898, 5.8815, 9.6076, 4.6436, 15.5, 15.5, 15.5, 1.39, 
4.49, 15.5, 12.39, 1.16, 8.23, 13.41, 7.886, 15.5, 13.5623, 15.5
)), .Names = c("date", "b0", "b1", "b2", "tau"), row.names = c("2105", 
"2106", "2107", "2108", "2109", "2110", "2111", "2112", "2113", 
"2114", "2115", "2116", "2117", "2118", "2119", "2120", "2121", 
"2122", "2123", "2124", "2125", "2126", "2127", "2128", "2129", 
"2130", "2131", "2132", "2133", "2134", "2135", "2136", "2137", 
"2138", "2139", "2140", "2141", "2142", "2143", "2144", "2145", 
"2146", "2147", "2148", "2149", "2150", "2151", "2152", "2153", 
"2154", "2155", "2156", "2157", "2158", "2159", "2160", "2161", 
"2162", "2163", "2164", "2165", "2166", "2167", "2168", "2169", 
"2170", "2171", "2172", "2173", "2174", "2175", "2176", "2177", 
"2178", "2179", "2180", "2181", "2182", "2183", "2184", "2185", 
"2186", "2187", "2188", "2189", "2190", "2191", "2192", "2193", 
"2194", "2195", "2196", "2197", "2198", "2199", "2200", "2201", 
"2202", "2203", "2204", "2205", "2206", "2207", "2208", "2209", 
"2210", "2211", "2212", "2213", "2214", "2215", "2216", "2217", 
"2218", "2219", "2220", "2221", "2222"), class = "data.frame")

ns$date = as.Date(as.character(ns$date), format="%Y%m%d")
syc.points <- matrix(nrow=nrow(ns), ncol=100)

# How to do this in a clever vectorised way?
for (i in 1:nrow(ns)) {
  j <- 1
  for (t in seq(from=0.1,length=100,by=0.1)) {
    syc.points[i,j] <- nsz(ns$b0[i], ns$b1[i], ns$b2[i], ns$tau[i], t)
    j <- j + 1
  }
}

From ajayshah at mayin.org  Tue Aug 31 06:42:51 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue Aug 31 06:42:13 2004
Subject: FW: [R-sig-finance] correlation between two stock market indices
In-Reply-To: <18D602BD42B7E24EB810D6454A58DB900A29BAF6@ibfftce505.de.ad.drkw.net>
References: <18D602BD42B7E24EB810D6454A58DB900A29BAF6@ibfftce505.de.ad.drkw.net>
Message-ID: <20040831044251.GW713@igidr.ac.in>

You can go far with short R code fragments, such as :

     library(its)
     x1 <- priceIts(instrument=c("^ftse"), start="1998-01-01", quote = "Close")
     x2 <- priceIts(instrument=c("^gdax"), start="1998-01-01", quote = "Close")
     prices <- intersect(x1,x2)
     names(prices) <- c("FTSE","DAX")
     returns = 100*diff(log(prices))
     cor(returns)

which gives 

          FTSE       DAX
FTSE 1.0000000 0.7455468
DAX  0.7455468 1.0000000

so you see that returns on the FTSE and the DAX have had a correlation
of 0.7455 in the post-1998 period.

I think the above serves as a great demo of R and 'its' in action! :-)

The pitfalls:

  * Timezones matter greatly. If one market closes before another,
    then it will look like one is causing the other, if viewed through
    daily returns.

    So you have to either go 'in' and pick intervals when both markets
    are contemporaneously trading, or you have to zoom 'out' and pick
    fat intervals where the overlaps matter less.

    E.g. India and the US have exactly non-overlapping hours. I find
    it hard to meaningfully interpret correlations of contemporaneous
    or lagged returns of daily data. It makes more sense to discuss
    correlations of weekly data.

  * Be careful to intersect and then compute returns, as done
    above. Be careful to make returns as 100*diff(log(prices)), as
    done above.

  * R TODO:
    The code fragment above works for indexes but not for individual
    stocks, since priceIts does not know how to give us "adjusted
    closing prices". That is, when splits take place, the price gets
    clobbered, and the series generated by priceIts is useless. I
    would be very happy if people on this list are able to propose a
    solution. I think priceIts is fabulous but this is a major gap in
    functionality. Raw closing prices are next to useless since most
    finance starts with returns, and in order to make returns, we need
    adjusted closing prices.

  * Markets are quite efficient for individual products (e.g. index
    futures or stocks) and you don't generally have problems with
    time-series structure. But when you get to indexes, the process of
    making linear combinations of things that trade with different
    timestamps is known to induce suprious autocorrelations. Andy Lo
    and others have papers on this. This problem is particularly acute
    when an index contains illiquid products.

  * Even if you dealt with individual traded products, like stocks or
    index futures, you'd have the problem of time-varying
    correlations.  You can do rolling window correlations and they'll
    help.

    R TODO: 
    There isn't yet a bivariate ARCH implementation in CRAN or in the
    Debian packages.

    R TODO:
    There isn't yet an abstract engine doing rolling window estimation
    in R, where you get to define the estimator (e.g. as is the case
    with 'by' where you get to specify FUN).

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From Bernhard.Pfaff at drkw.com  Tue Aug 31 09:00:06 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Tue Aug 31 09:00:14 2004
Subject: [R-sig-finance] 3-d visualisation of yield curves?
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29BAFB@ibfftce505.de.ad.drkw.net>

Hello Ajay,

you can achieve this task with persp() and an auxiliary function trans3d()
(provided in the example section of the persp() documentation). The function
trans3d() transforms the output of persp() into a plane and this is needed
for the labeling adjustment of the axis.
It is kind of fiddling to get the labels correctly positioned but it works
along the lines:

pmat <- persp(y, x, as.matrix(ypper), xlab="Last 100 periods",
ylab="Maturities (months)", zlab="Yields", zlim=c(3,6), theta=130, phi=15,
col="Seagreen", box=T, ticktype="detailed", shade=0.3, ltheta=270, lphi=30,
r=sqrt(3), expand=0.5, main="Historical yield curves", axes=FALSE)
# Draw x-axis labels; only draw some of the labels to avoid overlap
subset <- seq(lastobs-51,lastobs-1, length=5)
par(xpd=NA)
text(trans3d((1:lastobs)[subset], 120, 3, pmat), labels=paste(" ",
temp.ymd[subset]), srt=310, adj=0)
# Draw y-axis labels; only draw some of the labels to avoid overlap
xnames <-
c("y1m","y3m","y6m","y9m","y12m","y24m","y36m","y48m","y60m","y72m","y84m","
y96m","y108m","y120m")
subset <- c(1,6,9,11,13,14)
text(trans3d(enter position as integer, x[subset], 3, pmat),
labels=paste(xnames, " ")[subset], srt=60, adj=1)
# Redo z-axis
text(trans3d(lastobs, 1, seq(2, 5, 1), pmat), labels=paste(seq(2, 5, 1), "
"), adj=1)
# Redo axis labels
text(trans3d(enter position as integer, 160, 0, pmat), labels="Last 100
periods", srt=40)
text(trans3d(enter position as integer, mean(x), 0, pmat),labels="Maturities
(months)", srt=330)
text(trans3d(enter position as integer, -50, 4.5, pmat), abels="Yields",
srt=95)

see the help archives, too. The coloring could/should be achieved with
conditional arguments. Apologize for not having worked it out with your
sample data, but rather copied the code snippet in here. 

HTH and directs you to the right spot to look at,
-B.

> A yield curve is a picture z(t) running (say) from maturity t=0 to
> t=10 years. And, the yield curve fluctuates; there's a new one every
> day. So you have a time-series of curves.
> 
> I have seen very pretty 3d color visualisation of the changing yield
> curve. The x axis is time, the y axis is maturity, the z axis is the
> interest rate. Low interest rates are coloured blue and high interest
> rates are coloured red. I have also seen a user-interface where the
> person can change the point of perspective and interactively look at
> the scene from different vantage points.
> 
> q: How would one do this in R? :-) Have you deployed ggobi or lattice
>    graphics for the task?
> 
> q: Does it add insight to look at yield curves in this way? Wise men
>    on the r-help mailing list generally say that you don't gain too
>    much insight from pretty 3d plots, so I have de-emphasised
>    them. But one can't help feeling tempted by those pretty pictures!
> 
> There's a code fragment at the end of this email, which sets up a
> matrix syc.points which has 118 rows (for 118 days) and 100 columns,
> where each column is the spot interest rate at one maturity. The
> maturities run from 0.1 to 10 years. This may help you play with the
> question. In case it's of interest, this is the spot yield curve in
> India from 1 April this year onwards, which has been an exciting time.
> 
> Thanks,
> 
>         -ans.
> 
> -- 
> Ajay Shah                                                   Consultant
> ajayshah@mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> 
> 
> 
> 
> nsz <- function(b0, b1, b2, tau, t) {
>   tmp = t/tau
>   tmp2 = exp(-tmp)
>   return(b0 + ((b1+b2)*(1-tmp2)/(tmp)) - (b2*tmp2))
> }
> 
> ns <- structure(list(date = as.integer(c(20040401, 20040402, 
> 20040405, 
> 20040406, 20040407, 20040408, 20040410, 20040412, 20040413, 20040415, 
> 20040416, 20040417, 20040419, 20040420, 20040421, 20040422, 20040423, 
> 20040424, 20040427, 20040428, 20040429, 20040430, 20040503, 20040505, 
> 20040506, 20040507, 20040508, 20040510, 20040511, 20040512, 20040513, 
> 20040514, 20040515, 20040517, 20040518, 20040519, 20040520, 20040521, 
> 20040522, 20040524, 20040525, 20040526, 20040527, 20040528, 20040529, 
> 20040531, 20040601, 20040602, 20040603, 20040604, 20040605, 20040607, 
> 20040608, 20040609, 20040610, 20040611, 20040612, 20040614, 20040615, 
> 20040616, 20040617, 20040618, 20040619, 20040621, 20040622, 20040623, 
> 20040624, 20040625, 20040626, 20040628, 20040629, 20040630, 20040701, 
> 20040702, 20040703, 20040705, 20040706, 20040707, 20040708, 20040710, 
> 20040712, 20040713, 20040714, 20040715, 20040716, 20040717, 20040719, 
> 20040720, 20040721, 20040722, 20040723, 20040724, 20040726, 20040727, 
> 20040728, 20040729, 20040730, 20040731, 20040802, 20040803, 20040804, 
> 20040805, 20040806, 20040807, 20040809, 20040810, 20040811, 20040812, 
> 20040813, 20040814, 20040816, 20040817, 20040818, 20040819, 20040821, 
> 20040823, 20040824, 20040825)), b0 = c(7.3422, 7.8217, 11.3901, 
> 7.4635, 7.2226, 7.2234, 7.3554, 7.3074, 8.468, 7.4894, 7.333, 
> 7.4633, 7.6058, 7.1231, 7.686, 7.0339, 7.4681, 7.3624, 7.3174, 
> 7.4157, 7.7941, 7.4091, 7.8367, 7.4442, 7.7108, 7.4472, 7.4185, 
> 7.1485, 7.0891, 7.3275, 6.9325, 7.0438, 7.0387, 7.2428, 7.7536, 
> 7.1781, 7.1971, 7.1158, 9.4099, 14.0915, 6.9736, 7.3604, 8.2228, 
> 8.9938, 9.2337, 14.5484, 11.5983, 14.1417, 11.5225, 7.516, 7.7201, 
> 7.5108, 7.575, 7.5737, 8.1235, 7.6523, 7.8026, 7.4334, 14.4161, 
> 7.4442, 8.3493, 7.7468, 7.8207, 7.8434, 8.3185, 8.0729, 7.9769, 
> 8.1296, 7.4141, 7.7981, 8.023, 8.2652, 7.9386, 7.8601, 7.9348, 
> 10.3803, 9.4022, 7.7753, 9.1768, 8.8638, 7.8373, 7.421, 7.4414, 
> 7.3915, 7.2947, 7.1905, 7.379, 6.9234, 7.5494, 6.9746, 7.9473, 
> 14.7695, 7.3734, 7.3422, 4.6736, 7.4979, 7.4727, 7.3631, 7.8308, 
> 7.6939, 7.0801, 7.7495, 5.0205, 7.6851, 15.2449, 11.4073, 10.5747, 
> 7.47, 8.06, 15.7304, 10.26, 7.3, 9.51, 9.32, 8.838, 13.3616, 
> 15.5854, 11.0715), b1 = c(-2.721, -3.2183, -6.5193, -3.2818, 
> -2.4963, -2.6716, -2.4753, -2.8383, -3.9206, -2.8313, -2.7894, 
> -2.8732, -3.2573, -2.5732, -2.9594, -2.3505, -2.6395, -2.6339, 
> -2.3883, -2.7691, -3.286, -2.5451, -3.0089, -2.9315, -3.2165, 
> -2.7868, -1.4352, -2.4697, -2.0563, -2.7682, -2.5612, -2.5146, 
> -2.5144, -2.4144, -2.9954, -2.3257, -2.1419, -2.1677, -4.3465, 
> -9.4, -2.1492, -2.6646, -3.4617, -4.1504, -3.1783, -9.4, -6.6092, 
> -9.4, -6.5229, -2.3708, -2.3271, -2.9659, -2.7749, -2.5683, -3.1604, 
> -2.3273, -2.543, -2.7601, -9.4, -2.2319, -3.1117, -3.3958, -2.7336, 
> -2.7717, -3.1778, -2.8743, -2.9824, -3.3025, -2.3269, -2.8134, 
> -3.2428, -3.4767, -3.2345, -3.019, -2.7832, -5.2133, -4.2563, 
> -2.9053, -4.092, -3.7751, -2.9284, -2.381, -2.6428, -2.7985, 
> -2.5906, -2.1872, -2.7013, -2.4288, -2.5056, -2.529, -2.9546, 
> -9.4, -2.2121, -2.419, -0.0105, -2.6278, -2.7345, -2.278, -2.6838, 
> -2.2775, -3.0019, -2.4925, -0.0191, -2.4934, -9.2345, -5.4497, 
> -4.4509, -2.88, -2.31, -9.1449, -3.96, -3.95, -3.4, -3.28, -3.1081, 
> -6.9305, -9.4, -4.9702), b2 = c(-2.9455, -3.1714, -7.1921, -1.2954, 
> -2.9376, -2.3315, -3.3516, -1.9412, -2.8231, -2.9133, -2.3912, 
> -2.7164, -0.6094, -2.4146, -3.0891, -2.8961, -3.452, -2.9017, 
> -3.525, -2.7331, -2.2974, -3.4923, -3.5535, -2.2847, -2.0384, 
> -2.8926, -5.7102, -2.7131, -3.5915, -2.4034, -1.4584, -2.2325, 
> -1.6771, -3.3011, -3.3382, -3.2615, -3.7138, -3.5048, -6.5431, 
> -8.59, -3.4933, -2.9518, -3.7216, -4.8041, -8.4707, -11.6242, 
> -7.4944, -8.6059, -7.4215, -4.0466, -4.4962, -2.4366, -2.9504, 
> -3.7409, -3.5684, -4.1972, -4.0236, -1.9687, -9.1264, -3.5223, 
> -4.1759, -1.2065, -3.4071, -3.2178, -3.4439, -3.0968, -2.1767, 
> -0.6931, -2.2134, -2.3287, -0.1982, -3.198, 0.0274, -1.4918, 
> -2.1835, -2.5059, -1.2104, -3e-04, -0.5105, -0.1034, -0.0324, 
> -0.0108, -0.0785, -0.0014, 0, -0.4718, -0.0261, 0.036, -0.0113, 
> 0.0142, -0.0189, -7.7247, -0.0115, -0.0146, 7.0134, 0.0116, 0.0098, 
> -0.0168, -0.0012, -0.0064, 0.0239, 0.0032, 6.0088, 0.01, -7.839, 
> -2.518, -1.4064, 0.02, -0.01, -9.5194, -2.12, 1.62, -1.98, -0.04, 
> -0.0255, -6.7197, -9.2038, -2.9766), tau = c(4.5266, 5.7445, 
> 10.7687, 6.682, 4.496, 5.1623, 4.8581, 6.3769, 9.7641, 5.8551, 
> 5.9208, 6.1627, 11.1458, 4.9739, 6.5382, 4.228, 5.2418, 5.5392, 
> 4.6344, 5.4967, 7.0604, 4.7554, 6.2326, 5.9656, 7.7035, 5.3672, 
> 3.8092, 4.2389, 3.5272, 4.9461, 4.8159, 4.0655, 4.972, 3.5159, 
> 4.9192, 3.8536, 3.6992, 3.4197, 5.8819, 14.656, 2.8546, 4.2702, 
> 5.9606, 6.1133, 4.7138, 10.0547, 9.0578, 12.9292, 8.6205, 3.6096, 
> 4.4072, 4.5344, 4.5593, 3.6541, 5.1583, 3.7913, 4.2516, 4.5431, 
> 13.0367, 3.5478, 5.3657, 5.4139, 3.8999, 3.9799, 4.6267, 4.1281, 
> 4.5191, 6.8646, 2.9362, 3.6888, 7.3445, 3.835, 7.5136, 4.6792, 
> 5.2262, 15.5, 15.5, 8.7869, 15.5, 15.4999, 8.3618, 7.8984, 5.6957, 
> 4.3874, 4.4051, 4.7968, 5.0399, 3.0356, 7.3138, 2.8834, 7.6381, 
> 15.5, 6.3357, 4.7519, 10.974, 3.925, 3.7252, 4.5168, 6.2362, 
> 6.282, 1.6898, 5.8815, 9.6076, 4.6436, 15.5, 15.5, 15.5, 1.39, 
> 4.49, 15.5, 12.39, 1.16, 8.23, 13.41, 7.886, 15.5, 13.5623, 15.5
> )), .Names = c("date", "b0", "b1", "b2", "tau"), row.names = 
> c("2105", 
> "2106", "2107", "2108", "2109", "2110", "2111", "2112", "2113", 
> "2114", "2115", "2116", "2117", "2118", "2119", "2120", "2121", 
> "2122", "2123", "2124", "2125", "2126", "2127", "2128", "2129", 
> "2130", "2131", "2132", "2133", "2134", "2135", "2136", "2137", 
> "2138", "2139", "2140", "2141", "2142", "2143", "2144", "2145", 
> "2146", "2147", "2148", "2149", "2150", "2151", "2152", "2153", 
> "2154", "2155", "2156", "2157", "2158", "2159", "2160", "2161", 
> "2162", "2163", "2164", "2165", "2166", "2167", "2168", "2169", 
> "2170", "2171", "2172", "2173", "2174", "2175", "2176", "2177", 
> "2178", "2179", "2180", "2181", "2182", "2183", "2184", "2185", 
> "2186", "2187", "2188", "2189", "2190", "2191", "2192", "2193", 
> "2194", "2195", "2196", "2197", "2198", "2199", "2200", "2201", 
> "2202", "2203", "2204", "2205", "2206", "2207", "2208", "2209", 
> "2210", "2211", "2212", "2213", "2214", "2215", "2216", "2217", 
> "2218", "2219", "2220", "2221", "2222"), class = "data.frame")
> 
> ns$date = as.Date(as.character(ns$date), format="%Y%m%d")
> syc.points <- matrix(nrow=nrow(ns), ncol=100)
> 
> # How to do this in a clever vectorised way?
> for (i in 1:nrow(ns)) {
>   j <- 1
>   for (t in seq(from=0.1,length=100,by=0.1)) {
>     syc.points[i,j] <- nsz(ns$b0[i], ns$b1[i], ns$b2[i], ns$tau[i], t)
>     j <- j + 1
>   }
> }
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From patrick at burns-stat.com  Tue Aug 31 12:41:56 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue Aug 31 12:42:35 2004
Subject: [R-sig-finance] correlation between two stock market indices
In-Reply-To: <4132E3CD.7050509@gmx.ch>
References: <4132E3CD.7050509@gmx.ch>
Message-ID: <413455F4.4030404@burns-stat.com>

I'll summarize what I see as the key points -- most of which have
already been covered.

*)  You want to have the correlation of the returns, not the prices.
Hence you want a command similar to:

rets <- diff(log(prices))

*)  You need to be careful of asynchronous data.  In this case there
is no problem since both are American.

*)  There is minimal autocorrelation in financial returns of liquidly
traded assets.  That should be close to the least of your worries
about getting the model wrong.

*)  How correlation should be estimated depends on the use to which
it will be put.  If the time horizon of interest is long -- on the order of
two months or longer, then an ordinary sample correlation should suffice.
If the time horizon is short -- a day or a week, then a GARCH model
is going to be appropriate. 

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Christoph Lehmann wrote:

> Dear finance professionals
>
> As I was asked by a friend, whether we can compute the correlation 
> between two stock market indices (e.g. NASDAQ index and Dow Jones 
> index), and I am unfortunately NOT an expert in finance:
>
> (1) What model would you recommend for this kind of question?
>
> something like:
>
> library(ts)
> arima(x, order=???, xreg=y)
>
> library(nlme)
> gls(x~y,correlation=corARMA(p=?,q=?))
>
> what would you recommend, and what about the "?" :)
>
> (2) Furthermore, searching the web, I found, that (sorry, you experts 
> certainly know this, but I have no experience with financial data), 
> usually the time series are uncorrelated, but show strong "ARCH 
> effects", ie., are not independent.
>
> Does this mean, that any kind of correlation analysis with stock market
> indices is senseless, since maybe we don't get a sign. correlation, 
> but this doesn't mean that the series are independent?
>
> Many thanks for your help
>
> Chris
>
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>

From ggrothendieck at myway.com  Tue Aug 31 15:51:10 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue Aug 31 15:51:16 2004
Subject: FW: [R-sig-finance] correlation between two stock market indices
Message-ID: <20040831135110.666383967@mprdmxin.myway.com>




> There isn't yet an abstract engine doing rolling window estimation
> in R, where you get to define the estimator (e.g. as is the case
> with 'by' where you get to specify FUN).

Note that the following do exist:
- running in package gregmisc
- rollFun, rollMean, rollVar, rollMin, rollMax in package fSeries
  See  http://www.itp.phys.ethz.ch/econophysics/R/pdf/DocRefcard.pdf
  for overview of related functions.
- embed 
- filter

From edd at debian.org  Tue Aug 31 16:12:12 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue Aug 31 16:12:16 2004
Subject: FW: [R-sig-finance] correlation between two stock market indices
In-Reply-To: <20040831044251.GW713@igidr.ac.in>
References: <18D602BD42B7E24EB810D6454A58DB900A29BAF6@ibfftce505.de.ad.drkw.net>
	<20040831044251.GW713@igidr.ac.in>
Message-ID: <20040831141212.GA17040@sonny.eddelbuettel.com>

On Tue, Aug 31, 2004 at 10:12:51AM +0530, Ajay Shah wrote:
>     The code fragment above works for indexes but not for individual
>     stocks, since priceIts does not know how to give us "adjusted
>     closing prices". That is, when splits take place, the price gets

get.hist.quote(0 in tseries was written before Yahoo! added that fifth
column of adjusted values. priceIts() is a pretty straight copy of
get.hist.quote().

If you submit a decent patch to either one, including an update to the .Rd
file, I am sure that it will get integrated into the package in due course.

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From vograno at evafunds.com  Tue Aug 31 17:19:57 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Tue Aug 31 17:20:26 2004
Subject: [R-sig-finance] correlation between two stock market indices
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A563D7BC@phost015.EVAFUNDS.intermedia.net>

> *)  How correlation should be estimated depends on the use to 
> which it will be put.  If the time horizon of interest is 
> long -- on the order of two months or longer, then an 
> ordinary sample correlation should suffice.
> If the time horizon is short -- a day or a week, then a GARCH 
> model is going to be appropriate. 

To my embarrassment I do not understand this (we are talking about the
cross-correlation, aren't we?). Is there a paper I could consult to
close this gap in my education?

Thank you,
Vadim

From kriskumar at earthlink.net  Tue Aug 31 07:28:41 2004
From: kriskumar at earthlink.net (kris kumar)
Date: Tue Aug 31 19:29:25 2004
Subject: [R-sig-finance] correlation between two stock market indices
Message-ID: <41340C89.3090704@earthlink.net>

>>*)  You need to be careful of asynchronous data.  In this case there
>>is no problem since both are American.

More at 
http://ideas.repec.org/p/cdl/ucsdec/97-30r.html

to add to the excellent summary & add my two cents, 
to what Patrick has elaborated.   

If these indices are measured across timezones then correlation will 
vary with sampling frequency.  There is some measurement noise and some 
people have even suggested using filters.

There is a riskmetrics paper from some time ago
which gives some simple expressions and a correlation formula for 
asynchronous data.

See: Adjusting correlation from nonsynchronous data
http://www.riskmetrics.com/journals.html

Regards
Kris

From patrick at burns-stat.com  Wed Sep  1 01:21:58 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed Sep  1 01:22:06 2004
Subject: [R-sig-finance] correlation between two stock market indices
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A563D7BC@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A563D7BC@phost015.EVAFUNDS.intermedia.net>
Message-ID: <41350816.4010908@burns-stat.com>

Yes, we are talking about cross-correlation.  (Before we get in an
even deeper muddle: for those who can't relate to cross-correlation,
ignore it and just think of correlation.)

I can't think of a very good reference at the moment -- maybe someone
else has ideas.

My statement mainly rests on the following assertion:

Multivariate GARCH is a reasonably good model for the variance
matrix of the returns of assets.

This is most true of daily data.  Lower frequency data smooth out some
of the garchiness; things get complicated with intraday data.

More specifically the assertion should be that there exists some 
multivariate
GARCH model which is reasonably good.  There will be many GARCH
models which are not good.  One particular model that almost surely will
not be at the head of the class is a constant correlation model.  These were
created because of the ease of estimation rather than from any empirical or
theoretical motivation.

If you think of CAPM with the market being modeled as GARCH, then
assets will be more highly correlated with each other when the market is
in a high volatility period than when it is in a low volatility period.

Assuming you can believe that correlations change over time (with some
form of continuity), then it shouldn't be too much of a leap to believe that
the time horizon of interest will influence your estimation procedure.

If you knew that GARCH were the correct model, then it would be optimal
(in the estimation sense) to use GARCH for all time horizons.  But as the
time horizon gets longer, all of the estimates approach the unconditional
correlation.  So for long time horizons there is not much sense in going
through the work of fitting a multivariate GARCH model when you will just
end up with the sample correlation anyway.  The more steps you predict
ahead, the more model risk you take.  GARCH is not exactly correct, so
there is definitely model risk to be had.

For short time horizons, the model doesn't have to be so perfect in order
to outperform the sample correlation.

Assuming that you don't have multivariate GARCH available to you, there
are some half-way measures for getting at predictions for short time 
horizons.
A practical option is to use exponential smoothing. 

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Vadim Ogranovich wrote:

>>*)  How correlation should be estimated depends on the use to 
>>which it will be put.  If the time horizon of interest is 
>>long -- on the order of two months or longer, then an 
>>ordinary sample correlation should suffice.
>>If the time horizon is short -- a day or a week, then a GARCH 
>>model is going to be appropriate. 
>>    
>>
>
>To my embarrassment I do not understand this (we are talking about the
>cross-correlation, aren't we?). Is there a paper I could consult to
>close this gap in my education?
>
>Thank you,
>Vadim
>
>  
>

From vograno at evafunds.com  Wed Sep  1 02:06:10 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Wed Sep  1 02:06:41 2004
Subject: [R-sig-finance] correlation between two stock market indices
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A563D82A@phost015.EVAFUNDS.intermedia.net>

Thank you Patrick! This makes a lot of sense. If I understand you
correctly you are talking about time-varying "instantaneous" correlation
(more precisely covariance) matrix and that a good GARCH can capture the
variations.

Thanks,
Vadim

> -----Original Message-----
> From: Patrick Burns [mailto:patrick@burns-stat.com] 
> Sent: Tuesday, August 31, 2004 4:22 PM
> To: Vadim Ogranovich; r-sig-finance
> Subject: Re: [R-sig-finance] correlation between two stock 
> market indices
> 
> Yes, we are talking about cross-correlation.  (Before we get 
> in an even deeper muddle: for those who can't relate to 
> cross-correlation, ignore it and just think of correlation.)
> 
> I can't think of a very good reference at the moment -- maybe 
> someone else has ideas.
> 
> My statement mainly rests on the following assertion:
> 
> Multivariate GARCH is a reasonably good model for the 
> variance matrix of the returns of assets.
> 
> This is most true of daily data.  Lower frequency data smooth 
> out some of the garchiness; things get complicated with intraday data.
> 
> More specifically the assertion should be that there exists 
> some multivariate GARCH model which is reasonably good.  
> There will be many GARCH models which are not good.  One 
> particular model that almost surely will not be at the head 
> of the class is a constant correlation model.  These were 
> created because of the ease of estimation rather than from 
> any empirical or theoretical motivation.
> 
> If you think of CAPM with the market being modeled as GARCH, 
> then assets will be more highly correlated with each other 
> when the market is in a high volatility period than when it 
> is in a low volatility period.
> 
> Assuming you can believe that correlations change over time 
> (with some form of continuity), then it shouldn't be too much 
> of a leap to believe that the time horizon of interest will 
> influence your estimation procedure.
> 
> If you knew that GARCH were the correct model, then it would 
> be optimal (in the estimation sense) to use GARCH for all 
> time horizons.  But as the time horizon gets longer, all of 
> the estimates approach the unconditional correlation.  So for 
> long time horizons there is not much sense in going through 
> the work of fitting a multivariate GARCH model when you will 
> just end up with the sample correlation anyway.  The more 
> steps you predict ahead, the more model risk you take.  GARCH 
> is not exactly correct, so there is definitely model risk to be had.
> 
> For short time horizons, the model doesn't have to be so 
> perfect in order to outperform the sample correlation.
> 
> Assuming that you don't have multivariate GARCH available to 
> you, there are some half-way measures for getting at 
> predictions for short time horizons.
> A practical option is to use exponential smoothing. 
> 
> Patrick Burns
> 
> Burns Statistics
> patrick@burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Vadim Ogranovich wrote:
> 
> >>*)  How correlation should be estimated depends on the use 
> to which it 
> >>will be put.  If the time horizon of interest is long -- on 
> the order 
> >>of two months or longer, then an ordinary sample correlation should 
> >>suffice.
> >>If the time horizon is short -- a day or a week, then a 
> GARCH model is 
> >>going to be appropriate.
> >>    
> >>
> >
> >To my embarrassment I do not understand this (we are talking 
> about the 
> >cross-correlation, aren't we?). Is there a paper I could consult to 
> >close this gap in my education?
> >
> >Thank you,
> >Vadim
> >
> >  
> >
> 
> 
>

From patrick at burns-stat.com  Wed Sep  1 16:55:00 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed Sep  1 16:55:16 2004
Subject: [R-sig-finance] correlation between two stock market indices
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A563D82A@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A563D82A@phost015.EVAFUNDS.intermedia.net>
Message-ID: <4135E2C4.7000509@burns-stat.com>

Precisely.

Vadim Ogranovich wrote:

>Thank you Patrick! This makes a lot of sense. If I understand you
>correctly you are talking about time-varying "instantaneous" correlation
>(more precisely covariance) matrix and that a good GARCH can capture the
>variations.
>
>Thanks,
>Vadim
>
>  
>
>>-----Original Message-----
>>From: Patrick Burns [mailto:patrick@burns-stat.com] 
>>Sent: Tuesday, August 31, 2004 4:22 PM
>>To: Vadim Ogranovich; r-sig-finance
>>Subject: Re: [R-sig-finance] correlation between two stock 
>>market indices
>>
>>Yes, we are talking about cross-correlation.  (Before we get 
>>in an even deeper muddle: for those who can't relate to 
>>cross-correlation, ignore it and just think of correlation.)
>>
>>I can't think of a very good reference at the moment -- maybe 
>>someone else has ideas.
>>
>>My statement mainly rests on the following assertion:
>>
>>Multivariate GARCH is a reasonably good model for the 
>>variance matrix of the returns of assets.
>>
>>This is most true of daily data.  Lower frequency data smooth 
>>out some of the garchiness; things get complicated with intraday data.
>>
>>More specifically the assertion should be that there exists 
>>some multivariate GARCH model which is reasonably good.  
>>There will be many GARCH models which are not good.  One 
>>particular model that almost surely will not be at the head 
>>of the class is a constant correlation model.  These were 
>>created because of the ease of estimation rather than from 
>>any empirical or theoretical motivation.
>>
>>If you think of CAPM with the market being modeled as GARCH, 
>>then assets will be more highly correlated with each other 
>>when the market is in a high volatility period than when it 
>>is in a low volatility period.
>>
>>Assuming you can believe that correlations change over time 
>>(with some form of continuity), then it shouldn't be too much 
>>of a leap to believe that the time horizon of interest will 
>>influence your estimation procedure.
>>
>>If you knew that GARCH were the correct model, then it would 
>>be optimal (in the estimation sense) to use GARCH for all 
>>time horizons.  But as the time horizon gets longer, all of 
>>the estimates approach the unconditional correlation.  So for 
>>long time horizons there is not much sense in going through 
>>the work of fitting a multivariate GARCH model when you will 
>>just end up with the sample correlation anyway.  The more 
>>steps you predict ahead, the more model risk you take.  GARCH 
>>is not exactly correct, so there is definitely model risk to be had.
>>
>>For short time horizons, the model doesn't have to be so 
>>perfect in order to outperform the sample correlation.
>>
>>Assuming that you don't have multivariate GARCH available to 
>>you, there are some half-way measures for getting at 
>>predictions for short time horizons.
>>A practical option is to use exponential smoothing. 
>>
>>Patrick Burns
>>
>>Burns Statistics
>>patrick@burns-stat.com
>>+44 (0)20 8525 0696
>>http://www.burns-stat.com
>>(home of S Poetry and "A Guide for the Unwilling S User")
>>
>>Vadim Ogranovich wrote:
>>
>>    
>>
>>>>*)  How correlation should be estimated depends on the use 
>>>>        
>>>>
>>to which it 
>>    
>>
>>>>will be put.  If the time horizon of interest is long -- on 
>>>>        
>>>>
>>the order 
>>    
>>
>>>>of two months or longer, then an ordinary sample correlation should 
>>>>suffice.
>>>>If the time horizon is short -- a day or a week, then a 
>>>>        
>>>>
>>GARCH model is 
>>    
>>
>>>>going to be appropriate.
>>>>   
>>>>
>>>>        
>>>>
>>>To my embarrassment I do not understand this (we are talking 
>>>      
>>>
>>about the 
>>    
>>
>>>cross-correlation, aren't we?). Is there a paper I could consult to 
>>>close this gap in my education?
>>>
>>>Thank you,
>>>Vadim
>>>
>>> 
>>>
>>>      
>>>
>>
>>    
>>
>
>  
>

From elw at stderr.org  Sun Sep 12 01:23:01 2004
From: elw at stderr.org (elijah wright)
Date: Sun Sep 12 02:23:11 2004
Subject: [R-sig-finance] diffusion of innovations modeling?
Message-ID: <Pine.LNX.4.61.0409111818460.5005@illuminati.stderr.org>


howdy.

first off, i realize that this is almost off-topic for this list.  I'm 
asking here because I think there might be a couple (business-educated) 
folks here who would know the answers.

i'm looking for implementations of algorithms for the modeling of 
diffusion of innovations - often studied by folks in marketing 
departments.  one particular resource i've seen is Vijay Mahajan's "Models 
for Innovation Diffusion", published by Sage press in the 1985/1986 
timeframe.

Has anyone done modeling of this kind in R?  enough to have tips or scraps 
of code that I could leech?  :)

I'm just not familiar enough with modeling to know where to dig in.  I'd 
appreciate any help folks could proffer.

thanks,

--elijah

School of Library and Information Science
Indiana University, Bloomington

From stefan.albrecht at allianz.com  Sun Sep 12 16:03:57 2004
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Sun Sep 12 16:04:01 2004
Subject: [R-sig-finance] Stefan Albrecht/HV/Finanzen/Allianz-Sach ist
 =?iso-8859-1?q?au=DF?=
 =?iso-8859-1?q?er_Haus=2E_=3A_R-sig-finance_Digest=2C_Vol_4=2C_Issue_3?=
Message-ID: <OF11393250.78B25BB6-ONC1256F0D.004D43F0@inside.allianz.de>






Ich werde ab  09/08/2004 nicht im Büro sein. Ich kehre zurück am
09/27/2004.

Ich werde Ihre Nachricht nach meiner Rückkehr beantworten.

Ursprüngliches Thema: R-sig-finance Digest, Vol 4, Issue 3
	[[alternative HTML version deleted]]

From flatman at swing.be  Sun Sep 12 22:51:22 2004
From: flatman at swing.be (Flatman)
Date: Sun Sep 12 22:51:29 2004
Subject: [R-sig-finance] function "fields"
Message-ID: <81681460-04FD-11D9-91D3-000D932DCBE4@swing.be>

hello !

I'm new to R and Rmetrics .
I'm intensively learning both but I'm in the very beginning ...

so here's my question :

when I launch the examples in fBasics for downloading quotes from yahoo 
, there's a call to a function named "fields" that seems to be 
unknown/not defined . I can't figure where it is ...

thanks for help

Erik

From Daltonmota at aol.com  Mon Sep 13 05:08:21 2004
From: Daltonmota at aol.com (Daltonmota@aol.com)
Date: Mon Sep 13 05:08:30 2004
Subject: [R-sig-finance] RE: diffusion of innovations modeling?
Message-ID: <06ED09CE.74E21DD7.0ACFBA63@aol.com>

I would suggest you to take a look at Princeton Blackboard site and MIT openware site. Both of them offer lots of online course materials from their classes on many subjects, including finance and marketing. I have found specific R / S-Plus code from some of these courses...its a matter of mining, but its free to try out and find what you are looking for. Best luck.

Dalto Mota.

From ajayshah at mayin.org  Sun Sep 12 17:33:36 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon Sep 13 07:30:42 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last traded
	price for NA)
Message-ID: <20040912153336.GD713@igidr.ac.in>

I have 3 different daily time-series. Using union() in the "its"
package, I can make a long matrix, where rows are created when even
one of the three time-series is observed:

massive <- union(nifty.its, union(inrusd.its, infosys.its))

Now in this, I want to replace NA values for prices by the
most-recently observed price. I can do this painfully --

for (i in 2:nrow(massive)) {
  for (j in 1:3) {
    if (is.na(massive[i,j])) {
      massive[i,j] = massive[i-1,j]
    }
  }
}

But this is horribly slow. Is there a more clever way?

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From patrick at burns-stat.com  Mon Sep 13 11:35:57 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon Sep 13 11:36:32 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last traded
	price for NA)
In-Reply-To: <20040912153336.GD713@igidr.ac.in>
References: <20040912153336.GD713@igidr.ac.in>
Message-ID: <414569FD.3030702@burns-stat.com>

This seems like a perfect example of how to move from C
style code towards the vectorization that R likes.  All else
being equal, R will be faster the fewer function calls there are.
So even if you don't see a way out of all of the loops that you
have, the less that is in the loops, the better.

The first thing to notice here is that the missing values can be
done all in one go.  So we can have:

mass.na <- is.na(massive)
nc.mass <- ncol(massive)
for(i in 2:nrow(massive)) {
    for(j in 1:nc.mass) {
       if(mass.na[i,j]) {
            massive[i,j] <- massive[i-1, j]
       }
    }
}

But at this point we can notice that we can do more than one
substitution at a time within each column.  Also if we have all
missing values before some spot (very common for prices),
then we are never going to have a value for that spot. So let's
write a useful subfunction and rearrange the computation:

subfun.miss.use <- function(x) {
  mis <- which(is.na(x))
  mis[mis != seq(along=mis)]
}

for(j in 1:ncol(massive)) {
   while(length(this.mis <- subfun.miss.use(massive[, j]))) {
        massive[this.mis, j] <- massive[this.mis-1, j]
    }
}

Caution: this code hasn't been tested so there may be bugs in it.

You may want to limit how far back to look for a value.  One way
of doing this is to put a count on the "while" loop.  A more rigorous
way of doing it so that you don't partially fill long gaps is to use "rle".

Another feature that is common is to remove rows that have all missing
values, which apparently will not happen in the current setting.


Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Ajay Shah wrote:

>I have 3 different daily time-series. Using union() in the "its"
>package, I can make a long matrix, where rows are created when even
>one of the three time-series is observed:
>
>massive <- union(nifty.its, union(inrusd.its, infosys.its))
>
>Now in this, I want to replace NA values for prices by the
>most-recently observed price. I can do this painfully --
>
>for (i in 2:nrow(massive)) {
>  for (j in 1:3) {
>    if (is.na(massive[i,j])) {
>      massive[i,j] = massive[i-1,j]
>    }
>  }
>}
>
>But this is horribly slow. Is there a more clever way?
>
>  
>

From ggrothendieck at myway.com  Mon Sep 13 13:29:01 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon Sep 13 13:29:06 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last traded
	price for NA)
Message-ID: <20040913112901.4740B394E@mprdmxin.myway.com>



This is referred to as LOCF, last observation carried forward.
First create some test data -- we are using ts here but using
its would be analogous:

   v1 <- c(1, NA, NA, 2, 3, NA, 5)
   v2 <- rev(v1)
   vv <- ts.union(ts(v1), ts(v2, start = 2))

Now define LOCF as a function and apply it:

   LOCF <- function(v) {
      L <- !is.na(v)
      v[c(NA,which(L))[cumsum(L)+1]]
   }
   apply(vv, 2, LOCF) 



Date:   	Sun, 12 Sep 2004 21:03:36 +0530
From:   	Ajay Shah <ajayshah@mayin.org>
To:   	r-sig-finance <r-sig-finance@stat.math.ethz.ch>
Subject:   	[R-sig-finance] How can I do this better? (Filling in last traded price for NA)

I have 3 different daily time-series. Using union() in the "its"
package, I can make a long matrix, where rows are created when even
one of the three time-series is observed:

massive <- union(nifty.its, union(inrusd.its, infosys.its))

Now in this, I want to replace NA values for prices by the
most-recently observed price. I can do this painfully --

for (i in 2:nrow(massive)) {
for (j in 1:3) {
if (is.na(massive[i,j])) {
massive[i,j] = massive[i-1,j]
}
}
}

But this is horribly slow. Is there a more clever way?

-- 
Ajay Shah Consultant
ajayshah@mayin.org Department of Economic Affairs
http://www.mayin.org/ajayshah Ministry of Finance, New Delhi

From Rich at mango-solutions.com  Mon Sep 13 14:12:34 2004
From: Rich at mango-solutions.com (Rich@mango-solutions.com)
Date: Mon Sep 13 14:12:21 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last
	tradedprice for NA)
Message-ID: <200409131212.i8DCCH9E022305@hypatia.math.ethz.ch>

Here's a function that does what you want . haven't look at it for a while
so it may need work.  Should be pretty fast, since it just uses
position-based referencing (ie. no cumsums etc).

retain <- function(vec) {
 isAv <- !is.na(vec)
 if (all(isAv) | all(!isAv)) return(vec)
 posAvToDiff <- posAv <- (1:length(vec))[isAv]
 if (max(posAvToDiff) != length(vec))  posAvToDiff <- c(posAvToDiff,
length(vec)+1)
 repLengths <- diff(posAvToDiff)
 if (max(posAvToDiff) == length(vec)) repLengths <- c(repLengths, 1)
 myOut <- rep(vec[posAv], repLengths)
 if (posAv[1] != 1)  myOut <- c(rep(NA, posAv[1]-1), myOut)
 myOut
}
myVec <- sample(c(1:5, NA), 50, T)
myVec
retain(myVec)

Hope this helps,
Rich.

R and S Consulting and Training for the Finance Industry
mangosolutions
Tel   +44 118 902 6617
Fax  +44 118 902 6401

-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Gabor
Grothendieck
Sent: 13 September 2004 12:29
To: ajayshah@mayin.org; r-sig-finance@stat.math.ethz.ch
Subject: RE: [R-sig-finance] How can I do this better? (Filling in last
tradedprice for NA)



This is referred to as LOCF, last observation carried forward.
First create some test data -- we are using ts here but using
its would be analogous:

   v1 <- c(1, NA, NA, 2, 3, NA, 5)
   v2 <- rev(v1)
   vv <- ts.union(ts(v1), ts(v2, start = 2))

Now define LOCF as a function and apply it:

   LOCF <- function(v) {
      L <- !is.na(v)
      v[c(NA,which(L))[cumsum(L)+1]]
   }
   apply(vv, 2, LOCF) 



Date:   	Sun, 12 Sep 2004 21:03:36 +0530
From:   	Ajay Shah <ajayshah@mayin.org>
To:   	r-sig-finance <r-sig-finance@stat.math.ethz.ch>
Subject:   	[R-sig-finance] How can I do this better? (Filling in last
traded price for NA)

I have 3 different daily time-series. Using union() in the "its"
package, I can make a long matrix, where rows are created when even
one of the three time-series is observed:

massive <- union(nifty.its, union(inrusd.its, infosys.its))

Now in this, I want to replace NA values for prices by the
most-recently observed price. I can do this painfully --

for (i in 2:nrow(massive)) {
for (j in 1:3) {
if (is.na(massive[i,j])) {
massive[i,j] = massive[i-1,j]
}
}
}

But this is horribly slow. Is there a more clever way?

-- 
Ajay Shah Consultant
ajayshah@mayin.org Department of Economic Affairs
http://www.mayin.org/ajayshah Ministry of Finance, New Delhi

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


	[[alternative HTML version deleted]]

From john.gavin at ubs.com  Mon Sep 13 17:57:41 2004
From: john.gavin at ubs.com (john.gavin@ubs.com)
Date: Mon Sep 13 17:58:17 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last traded
	price for NA)
Message-ID: <012821F286ED1D4ABDC72F9E1DD63D0C041EA974@NLDNC003PEX1.ubsgs.ubsgroup.net>

Hi Ajay,

You will probably get other suggestions
along the following lines,
which use 'rle' and 'rep' to speed things up.

fillIn2 <- function(x)
{ bef <- x # keep a copy for display purposes only.
  xRle <- rle(is.na(x))
  # get indices where each NA seq starts (low) and stops (upp)
  upp <- (sumX <- cumsum(xRle$lengths))[xRle$values]
  low <- sumX[which(xRle$values)-1]+1
  # special case: NA at start _only_ i.e. c(NA, ..., NA, notNa, ..., notNA)
  if (length(low) == 0) return(cbind(before = x , after = x))
  # special case: NA at start and else where 
  if (length(upp) == length(low)+1) upp <- upp[-1]
  # Critical bit is 'rep' on RHS. 
  # On LHS, dont replace NAs at the start, if any.
  ind <- low[1]-1
  x[ind + which(is.na(x[-seq(ind)]))] <- x[rep(low-1, upp-low+1)]
  cbind(before = bef , after = x) # show off before and after effect
}
set.seed(123)
x <- 1:10
x[sample(length(x), floor(length(x)/2))] <- NA
fillIn2(x)

should produce

> fillIn2(x)
      before after
 [1,]      1     1
 [2,]      2     2
 [3,]     NA     2
 [4,]     NA     2
 [5,]      5     5
 [6,]     NA     5
 [7,]     NA     5
 [8,]     NA     5
 [9,]      9     9
[10,]     10    10

The code seems clunky and has special cases
so it is probably not optimal.

However, it is faster than, say, using 'mapply'

fillIn <- function(x)
{ bef <- x
  xRle <- rle(is.na(x))
  upp <- cumsum(xRle$lengths)[xRle$values]
  low <- cumsum(xRle$lengths)[which(xRle$values)-1]+1
  if (length(upp) == length(low)+1) upp <- upp[-1]
  mapply(function(l, u) x[l:u] <<- x[l-1], low, upp)
  cbind(before = bef , after = x) # show off before and after effect
}
fillIn(x)

Some simulations to compare times,
based on vectors of varying lengths with 50% of elements set to NA

simFillIn <- function(n, method = c("rep", "mapply"))
{ aa <- rpois(n, 5)
  aa[sample(seq(n), floor(n * .5))] <- NA
  method = match.arg(method)
  ansTime <- system.time(ans <- 
    switch(method,
      mapply = fillIn(aa),
      rep = fillIn2(aa), 
      stop("wrong method")
  )) # switch system.time
  list(time = ansTime) # ans = ans, 
}
ans <- lapply(c(2e4, 1e4, 1e3, 1e2, 1e1), simFillIn, method = "mapply")
lapply(ans, "[[", "time")
ans <- lapply(c(2e4, 1e4, 1e3, 1e2, 1e1), simFillIn, method = "rep")
lapply(ans, "[[", "time")

simFillIn (with 'mapply') seems at least 10 times slower
than simFillIn2 (with 'rep').

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Quantitative Risk Models and Statistics,
UBS Investment Bank, 6th floor, 
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289
Fax   +44 (0) 207 568 5352


Ajay Shah wrote:

>I have 3 different daily time-series. Using union() in the "its"
>package, I can make a long matrix, where rows are created when even
>one of the three time-series is observed:
>
>massive <- union(nifty.its, union(inrusd.its, infosys.its))
>
>Now in this, I want to replace NA values for prices by the
>most-recently observed price. I can do this painfully --
>
>for (i in 2:nrow(massive)) {
>  for (j in 1:3) {
>    if (is.na(massive[i,j])) {
>      massive[i,j] = massive[i-1,j]
>    }
>  }
>}
>
>But this is horribly slow. Is there a more clever way?

Visit our website at http://www.ubs.com

This message contains confidential information and is intend...{{dropped}}

From mdowle at concordiafunds.com  Mon Sep 13 22:06:23 2004
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Mon Sep 13 22:02:46 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last tr
	aded price for NA)
Message-ID: <78166BFC5165D811AA0400065BF0324B314898@wisconsin.concordia>


Isn't C the right tool for this job? Something like this (cobbling cumsum
itself)? This is untested and very unlikely to be exactly correct.

static SEXP fillna(SEXP x, SEXP s)
{
    int i=0;
    double last=R_NA;
    while (i<length(x)) {
	if (!(ISNAN(REAL(x)[i])))
	    last = REAL(x)[i];
	REAL(s)[i++] = last;
    }
    return s;
}

Even Gabor's LOCF involved just 3 calls: is.na(), which() and cumsum(), but
if I understand correctly it involves 3 loops (internally) over the entire
vector plus the associated memory copies of each call. fillna as above
should be as fast as one call to cumsum(), requiring much less working
memory than any R solution. If this is the case, perhaps something like it
could be added to R?

Regards,
Matthew

-----Original Message-----
From: john.gavin@ubs.com [mailto:john.gavin@ubs.com] 
Sent: 13 September 2004 16:58
To: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] How can I do this better? (Filling in last
traded price for NA)


Hi Ajay,

You will probably get other suggestions
along the following lines,
which use 'rle' and 'rep' to speed things up.

fillIn2 <- function(x)
{ bef <- x # keep a copy for display purposes only.
  xRle <- rle(is.na(x))
  # get indices where each NA seq starts (low) and stops (upp)
  upp <- (sumX <- cumsum(xRle$lengths))[xRle$values]
  low <- sumX[which(xRle$values)-1]+1
  # special case: NA at start _only_ i.e. c(NA, ..., NA, notNa, ..., notNA)
  if (length(low) == 0) return(cbind(before = x , after = x))
  # special case: NA at start and else where 
  if (length(upp) == length(low)+1) upp <- upp[-1]
  # Critical bit is 'rep' on RHS. 
  # On LHS, dont replace NAs at the start, if any.
  ind <- low[1]-1
  x[ind + which(is.na(x[-seq(ind)]))] <- x[rep(low-1, upp-low+1)]
  cbind(before = bef , after = x) # show off before and after effect }
set.seed(123)
x <- 1:10
x[sample(length(x), floor(length(x)/2))] <- NA
fillIn2(x)

should produce

> fillIn2(x)
      before after
 [1,]      1     1
 [2,]      2     2
 [3,]     NA     2
 [4,]     NA     2
 [5,]      5     5
 [6,]     NA     5
 [7,]     NA     5
 [8,]     NA     5
 [9,]      9     9
[10,]     10    10

The code seems clunky and has special cases
so it is probably not optimal.

However, it is faster than, say, using 'mapply'

fillIn <- function(x)
{ bef <- x
  xRle <- rle(is.na(x))
  upp <- cumsum(xRle$lengths)[xRle$values]
  low <- cumsum(xRle$lengths)[which(xRle$values)-1]+1
  if (length(upp) == length(low)+1) upp <- upp[-1]
  mapply(function(l, u) x[l:u] <<- x[l-1], low, upp)
  cbind(before = bef , after = x) # show off before and after effect }
fillIn(x)

Some simulations to compare times,
based on vectors of varying lengths with 50% of elements set to NA

simFillIn <- function(n, method = c("rep", "mapply"))
{ aa <- rpois(n, 5)
  aa[sample(seq(n), floor(n * .5))] <- NA
  method = match.arg(method)
  ansTime <- system.time(ans <- 
    switch(method,
      mapply = fillIn(aa),
      rep = fillIn2(aa), 
      stop("wrong method")
  )) # switch system.time
  list(time = ansTime) # ans = ans, 
}
ans <- lapply(c(2e4, 1e4, 1e3, 1e2, 1e1), simFillIn, method = "mapply")
lapply(ans, "[[", "time") ans <- lapply(c(2e4, 1e4, 1e3, 1e2, 1e1),
simFillIn, method = "rep") lapply(ans, "[[", "time")

simFillIn (with 'mapply') seems at least 10 times slower
than simFillIn2 (with 'rep').

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Quantitative Risk Models and Statistics,
UBS Investment Bank, 6th floor, 
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289
Fax   +44 (0) 207 568 5352


Ajay Shah wrote:

>I have 3 different daily time-series. Using union() in the "its" 
>package, I can make a long matrix, where rows are created when even one 
>of the three time-series is observed:
>
>massive <- union(nifty.its, union(inrusd.its, infosys.its))
>
>Now in this, I want to replace NA values for prices by the 
>most-recently observed price. I can do this painfully --
>
>for (i in 2:nrow(massive)) {
>  for (j in 1:3) {
>    if (is.na(massive[i,j])) {
>      massive[i,j] = massive[i-1,j]
>    }
>  }
>}
>
>But this is horribly slow. Is there a more clever way?

Visit our website at http://www.ubs.com

This message contains confidential information and is intend...{{dropped}}

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance

From patrick at burns-stat.com  Mon Sep 13 22:20:06 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon Sep 13 22:20:35 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last tr
	aded price for NA)
In-Reply-To: <78166BFC5165D811AA0400065BF0324B314898@wisconsin.concordia>
References: <78166BFC5165D811AA0400065BF0324B314898@wisconsin.concordia>
Message-ID: <414600F6.80003@burns-stat.com>

I'm not convinced.  If you should be lucky enough to have enough
data that Gabor's solution will take more than a few milliseconds,
then I would think you might want to limit the length of filling.


Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Matthew Dowle wrote:

>Isn't C the right tool for this job? Something like this (cobbling cumsum
>itself)? This is untested and very unlikely to be exactly correct.
>
>static SEXP fillna(SEXP x, SEXP s)
>{
>    int i=0;
>    double last=R_NA;
>    while (i<length(x)) {
>	if (!(ISNAN(REAL(x)[i])))
>	    last = REAL(x)[i];
>	REAL(s)[i++] = last;
>    }
>    return s;
>}
>
>Even Gabor's LOCF involved just 3 calls: is.na(), which() and cumsum(), but
>if I understand correctly it involves 3 loops (internally) over the entire
>vector plus the associated memory copies of each call. fillna as above
>should be as fast as one call to cumsum(), requiring much less working
>memory than any R solution. If this is the case, perhaps something like it
>could be added to R?
>
>Regards,
>Matthew
>
>-----Original Message-----
>From: john.gavin@ubs.com [mailto:john.gavin@ubs.com] 
>Sent: 13 September 2004 16:58
>To: r-sig-finance@stat.math.ethz.ch
>Subject: Re: [R-sig-finance] How can I do this better? (Filling in last
>traded price for NA)
>
>
>Hi Ajay,
>
>You will probably get other suggestions
>along the following lines,
>which use 'rle' and 'rep' to speed things up.
>
>fillIn2 <- function(x)
>{ bef <- x # keep a copy for display purposes only.
>  xRle <- rle(is.na(x))
>  # get indices where each NA seq starts (low) and stops (upp)
>  upp <- (sumX <- cumsum(xRle$lengths))[xRle$values]
>  low <- sumX[which(xRle$values)-1]+1
>  # special case: NA at start _only_ i.e. c(NA, ..., NA, notNa, ..., notNA)
>  if (length(low) == 0) return(cbind(before = x , after = x))
>  # special case: NA at start and else where 
>  if (length(upp) == length(low)+1) upp <- upp[-1]
>  # Critical bit is 'rep' on RHS. 
>  # On LHS, dont replace NAs at the start, if any.
>  ind <- low[1]-1
>  x[ind + which(is.na(x[-seq(ind)]))] <- x[rep(low-1, upp-low+1)]
>  cbind(before = bef , after = x) # show off before and after effect }
>set.seed(123)
>x <- 1:10
>x[sample(length(x), floor(length(x)/2))] <- NA
>fillIn2(x)
>
>should produce
>
>  
>
>>fillIn2(x)
>>    
>>
>      before after
> [1,]      1     1
> [2,]      2     2
> [3,]     NA     2
> [4,]     NA     2
> [5,]      5     5
> [6,]     NA     5
> [7,]     NA     5
> [8,]     NA     5
> [9,]      9     9
>[10,]     10    10
>
>The code seems clunky and has special cases
>so it is probably not optimal.
>
>However, it is faster than, say, using 'mapply'
>
>fillIn <- function(x)
>{ bef <- x
>  xRle <- rle(is.na(x))
>  upp <- cumsum(xRle$lengths)[xRle$values]
>  low <- cumsum(xRle$lengths)[which(xRle$values)-1]+1
>  if (length(upp) == length(low)+1) upp <- upp[-1]
>  mapply(function(l, u) x[l:u] <<- x[l-1], low, upp)
>  cbind(before = bef , after = x) # show off before and after effect }
>fillIn(x)
>
>Some simulations to compare times,
>based on vectors of varying lengths with 50% of elements set to NA
>
>simFillIn <- function(n, method = c("rep", "mapply"))
>{ aa <- rpois(n, 5)
>  aa[sample(seq(n), floor(n * .5))] <- NA
>  method = match.arg(method)
>  ansTime <- system.time(ans <- 
>    switch(method,
>      mapply = fillIn(aa),
>      rep = fillIn2(aa), 
>      stop("wrong method")
>  )) # switch system.time
>  list(time = ansTime) # ans = ans, 
>}
>ans <- lapply(c(2e4, 1e4, 1e3, 1e2, 1e1), simFillIn, method = "mapply")
>lapply(ans, "[[", "time") ans <- lapply(c(2e4, 1e4, 1e3, 1e2, 1e1),
>simFillIn, method = "rep") lapply(ans, "[[", "time")
>
>simFillIn (with 'mapply') seems at least 10 times slower
>than simFillIn2 (with 'rep').
>
>Regards,
>
>John.
>
>John Gavin <john.gavin at ubs.com>,
>Quantitative Risk Models and Statistics,
>UBS Investment Bank, 6th floor, 
>100 Liverpool St., London EC2M 2RH, UK.
>Phone +44 (0) 207 567 4289
>Fax   +44 (0) 207 568 5352
>
>
>Ajay Shah wrote:
>
>  
>
>>I have 3 different daily time-series. Using union() in the "its" 
>>package, I can make a long matrix, where rows are created when even one 
>>of the three time-series is observed:
>>
>>massive <- union(nifty.its, union(inrusd.its, infosys.its))
>>
>>Now in this, I want to replace NA values for prices by the 
>>most-recently observed price. I can do this painfully --
>>
>>for (i in 2:nrow(massive)) {
>> for (j in 1:3) {
>>   if (is.na(massive[i,j])) {
>>     massive[i,j] = massive[i-1,j]
>>   }
>> }
>>}
>>
>>But this is horribly slow. Is there a more clever way?
>>    
>>
>
>Visit our website at http://www.ubs.com
>
>This message contains confidential information and is intend...{{dropped}}
>
>_______________________________________________
>R-sig-finance@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>_______________________________________________
>R-sig-finance@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>

From rpugh at mango-solutions.com  Mon Sep 13 14:03:21 2004
From: rpugh at mango-solutions.com (Richard Pugh)
Date: Mon Sep 13 22:24:40 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last
	tradedprice for NA)
In-Reply-To: <20040913112901.4740B394E@mprdmxin.myway.com>
Message-ID: <200409131203.i8DC37us019099@hypatia.math.ethz.ch>

Here's a function that does what you want . haven't look at it for a while
so it may need work.  Should be pretty fast, since it just uses
position-based referencing (ie. no cumsums etc).

retain <- function(vec) {
 isAv <- !is.na(vec)
 if (all(isAv) | all(!isAv)) return(vec)
 posAvToDiff <- posAv <- (1:length(vec))[isAv]
 if (max(posAvToDiff) != length(vec))  posAvToDiff <- c(posAvToDiff,
length(vec)+1)
 repLengths <- diff(posAvToDiff)
 if (max(posAvToDiff) == length(vec)) repLengths <- c(repLengths, 1)
 myOut <- rep(vec[posAv], repLengths)
 if (posAv[1] != 1)  myOut <- c(rep(NA, posAv[1]-1), myOut)
 myOut
}
myVec <- sample(c(1:5, NA), 50, T)
myVec
retain(myVec)

Hope this helps,
Rich.

R and S Consulting and Training for the Finance Industry
mangosolutions
Tel   +44 118 902 6617
Fax  +44 118 902 6401

-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Gabor
Grothendieck
Sent: 13 September 2004 12:29
To: ajayshah@mayin.org; r-sig-finance@stat.math.ethz.ch
Subject: RE: [R-sig-finance] How can I do this better? (Filling in last
tradedprice for NA)



This is referred to as LOCF, last observation carried forward.
First create some test data -- we are using ts here but using
its would be analogous:

   v1 <- c(1, NA, NA, 2, 3, NA, 5)
   v2 <- rev(v1)
   vv <- ts.union(ts(v1), ts(v2, start = 2))

Now define LOCF as a function and apply it:

   LOCF <- function(v) {
      L <- !is.na(v)
      v[c(NA,which(L))[cumsum(L)+1]]
   }
   apply(vv, 2, LOCF) 



Date:   	Sun, 12 Sep 2004 21:03:36 +0530
From:   	Ajay Shah <ajayshah@mayin.org>
To:   	r-sig-finance <r-sig-finance@stat.math.ethz.ch>
Subject:   	[R-sig-finance] How can I do this better? (Filling in last
traded price for NA)

I have 3 different daily time-series. Using union() in the "its"
package, I can make a long matrix, where rows are created when even
one of the three time-series is observed:

massive <- union(nifty.its, union(inrusd.its, infosys.its))

Now in this, I want to replace NA values for prices by the
most-recently observed price. I can do this painfully --

for (i in 2:nrow(massive)) {
for (j in 1:3) {
if (is.na(massive[i,j])) {
massive[i,j] = massive[i-1,j]
}
}
}

But this is horribly slow. Is there a more clever way?

-- 
Ajay Shah Consultant
ajayshah@mayin.org Department of Economic Affairs
http://www.mayin.org/ajayshah Ministry of Finance, New Delhi

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


	[[alternative HTML version deleted]]

From edd at debian.org  Tue Sep 14 02:03:11 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue Sep 14 02:03:19 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last traded
	price for NA)
In-Reply-To: <20040912153336.GD713@igidr.ac.in>
References: <20040912153336.GD713@igidr.ac.in>
Message-ID: <20040914000311.GA1694@sonny.eddelbuettel.com>

Great email thread in response to Ajay's question! [ I am having some mail
delays at one of the @debian.org mail hosts so I may not have seen all
posts, but I did check the web archive of the list. ]

What is surprising, though, is that nobody looked at _sources_ of its which
has a function locf() to do just this:

#locf-function---------------------------------------------------
locf <- function(x)
    {
    if (!inherits(x, "its")) stop("function is only valid for objects of class 'its'")
    y <- x
    jna <- which(apply(is.na(x),2,any))
    for(j in jna)
        {
        y[,j] <- y[most.recent(!is.na(y[,j])),j]
        }
    return(y)
    }
					    
It uses this function

#most.recent-function--------------------------------------------
most.recent <- function(x) 
    {
    # return a vector of indices of the most recent TRUE value (thanks to Tony Plate)
    if (!is.logical(x)) stop("x must be logical")
    x.pos <- which(x)
    if (length(x.pos)==0 || x.pos[1] != 1) x.pos <- c(1, x.pos)
    rep(x.pos, c(diff(x.pos), length(x) - x.pos[length(x.pos)] + 1))
    }

Would anybody care to time the different approaches we've seen submitted?


Regards,

Dirk
(who, before the list existed, had asked Giles about this)

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From ggrothendieck at myway.com  Tue Sep 14 03:50:51 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue Sep 14 03:51:05 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last traded
	price for NA)
Message-ID: <20040914015051.B783D397D@mprdmxin.myway.com>


Dirk,

I was not aware that locf was in "its" but was aware of Tony's
solution as we had discussed both it and a forerunner of the solution
in my last post at that time.  See the thread beginning with:

https://stat.ethz.ch/pipermail/r-help/2003-November/040603.html

The two solutions are the same except for the inner portion
which calculates the indices of the LOCF of a logical 
vector.  Simplifying slightly:

most.recent.1 <- function(L) {
	if (length(L) > 1) L[1] <- TRUE
	w <- which(c(L,T))
	rep(w[-length(w)], diff(w))
}

most.recent.2 <- function(L) {
	which(c(NA,L))[cumsum(L)+1]
}

so the key operations are which, rep and diff in #1 and which, [
and cumsum in #2.  This suggests they are about equal in speed
and, in fact, some timings I did fluctuated from run to run but
in general they seemed to run at about the same speed with #1
running faster sometimes and #2 running faster other times
(even though the same input was used on every run).

From mdowle at concordiafunds.com  Tue Sep 14 05:31:12 2004
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Tue Sep 14 05:27:47 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last tr
	aded price for NA)
Message-ID: <78166BFC5165D811AA0400065BF0324B31489B@wisconsin.concordia>


How about this in C? With the example data (190MB), 11+ secs (variable) is
reduced to under 1 second consistently. Even when LOCF stops working, the C
method carries on consistently since it requires no working memory. There
would be some more work required to make it 'safe'. If a copy is required, a
copy can be taken first (or save(,compress=TRUE)'d out).

EXPORT void fillna (double *ans, int *rows, int *cols)
{
   int r=0, c=0;
   double last;
   for (c=0; c<*cols; c++) {
      last = *ans++;
      for (r=1; r<*rows; r++) {
         if (!ISNA(*ans)) last = *ans;
         *ans++ = last;
      }
   }
}

fill.na.byref = function(m)
{
   if (!is.matrix(m) || storage.mode(m)!="double") {
      stop("input must be a matrix, storage mode double")
   }
   invisible(.C("fillna", m, as.integer(nrow(m)), as.integer(ncol(m)),
DUP=FALSE, NAOK=TRUE))
}

For example:

> M = matrix(as.double(sample(100)), nrow=10)
> M[sample(100,50)] = NA
> M
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]   88   NA   NA   NA   27   57   NA   NA   NA    99
 [2,]   72   NA   83   NA   24   54   20   32   NA    NA
 [3,]   60   85   33   NA   NA   61    2   NA   50    NA
 [4,]   91   NA    8    5   NA   51   NA   39   NA    45
 [5,]   NA   93   21   NA   NA   48   NA   69   12    56
 [6,]   NA   NA   10   NA   NA   NA   14   53   NA    NA
 [7,]   NA   15   95   NA   43   NA   34   NA   75    90
 [8,]   NA   NA   NA   NA   37   NA   19   NA    7    96
 [9,]   81   NA   NA   NA   NA   89   36   NA   NA    87
[10,]   NA   77   NA   11   NA   18   NA   28   74    NA
> fill.na.byref(M)
> M
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]   88   NA   NA   NA   27   57   NA   NA   NA    99
 [2,]   72   NA   83   NA   24   54   20   32   NA    99
 [3,]   60   85   33   NA   24   61    2   32   50    99
 [4,]   91   85    8    5   24   51    2   39   50    45
 [5,]   91   93   21    5   24   48    2   69   12    56
 [6,]   91   93   10    5   24   48   14   53   12    56
 [7,]   91   15   95    5   43   48   34   53   75    90
 [8,]   91   15   95    5   37   48   19   53    7    96
 [9,]   81   15   95    5   37   89   36   53    7    87
[10,]   81   77   95   11   37   18   36   28   74    87
> 

With some 'large' data (190MB), LOCF works once (11 secs) then runs out of
memory :

> M = matrix(as.double(sample(5000*5000)), nrow=5000)
> object.size(M)/1024^2
[1] 190.7350
> M[sample(5000*5000, 5000)] = NA
> system.time(filled <<- apply(M, 2, LOCF))[3]
[1] 11.33
> system.time(filled <<- apply(M, 2, LOCF))[3]
Error: cannot allocate vector of size 195312 Kb
In addition: Warning message: 
Reached total allocation of 1200Mb: see help(memory.size) 
Timing stopped at: 10.28 0.25 10.67 NA NA 
> gc()
            used  (Mb) gc trigger  (Mb)
Ncells    427770  11.5     741108  19.8
Vcells 100105874 763.8  125499641 957.5
> system.time(filled <<- apply(M, 2, LOCF))[3]
Error: cannot allocate vector of size 195312 Kb
In addition: Warning message: 
Reached total allocation of 1200Mb: see help(memory.size) 
Timing stopped at: 0 0 0 NA NA 
> 

Continuing with the same session, trying the C function :

> identical(filled, M)
[1] FALSE
> system.time(fill.na.byref(M))[3]
[1] 0.92
> identical(filled, M)
[1] TRUE

Running several times (shouldn't matter that the NAs are already filled
since the same work has to be done) :

> system.time(fill.na.byref(M))[3]
[1] 0.9
> system.time(fill.na.byref(M))[3]
[1] 0.89
> system.time(fill.na.byref(M))[3]
[1] 0.87
> system.time(fill.na.byref(M))[3]
[1] 0.89
> system.time(fill.na.byref(M))[3]
[1] 0.89
> 

Since no working memory is required (afaik), the garbage collector isn't
involved and we get consistent, fast, timings.

Its possible there is something wrong with my setup/config which means the
LOCF method takes longer and fails. If anyone can point me in the right
direction (changing vcell options?) I'm happy to try in that direction.


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck@myway.com] 
Sent: 14 September 2004 02:51
To: edd@debian.org; ajayshah@mayin.org
Cc: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] How can I do this better? (Filling in last
traded price for NA)



Dirk,

I was not aware that locf was in "its" but was aware of Tony's solution as
we had discussed both it and a forerunner of the solution in my last post at
that time.  See the thread beginning with:

https://stat.ethz.ch/pipermail/r-help/2003-November/040603.html

The two solutions are the same except for the inner portion which calculates
the indices of the LOCF of a logical 
vector.  Simplifying slightly:

most.recent.1 <- function(L) {
	if (length(L) > 1) L[1] <- TRUE
	w <- which(c(L,T))
	rep(w[-length(w)], diff(w))
}

most.recent.2 <- function(L) {
	which(c(NA,L))[cumsum(L)+1]
}

so the key operations are which, rep and diff in #1 and which, [ and cumsum
in #2.  This suggests they are about equal in speed and, in fact, some
timings I did fluctuated from run to run but in general they seemed to run
at about the same speed with #1 running faster sometimes and #2 running
faster other times (even though the same input was used on every run).

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance

From ggrothendieck at myway.com  Tue Sep 14 10:56:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue Sep 14 10:56:27 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last tr
	aded price for NA)
Message-ID: <20040914085623.7C31D12D84@mprdmxin.myway.com>



The memory error is due to apply rather than LOCF, itself.
You can do it largely in place in R thereby eliminating
memory errors by replacing the apply with a for:

system.time({ for(j in 1:ncol(M)) M[,j] <<- LOCF(M[,j]) })

Of course, if your data really is this large you are not 
only going to have challenges here but it will also be
problematic doing any subsequent analyses that are not 
trivial.


Date:   	Tue, 14 Sep 2004 04:31:12 +0100
From:   	Matthew Dowle <mdowle@concordiafunds.com>
To:   	'ggrothendieck@myway.com' <ggrothendieck@myway.com>, <edd@debian.org>, <ajayshah@mayin.org>
Cc:   	<r-sig-finance@stat.math.ethz.ch>
Subject:   	RE: [R-sig-finance] How can I do this better? (Filling in last tr aded price for NA)


How about this in C? With the example data (190MB), 11+ secs (variable) is
reduced to under 1 second consistently. Even when LOCF stops working, the C
method carries on consistently since it requires no working memory. There
would be some more work required to make it 'safe'. If a copy is required, a
copy can be taken first (or save(,compress=TRUE)'d out).

EXPORT void fillna (double *ans, int *rows, int *cols)
{
int r=0, c=0;
double last;
for (c=0; c<*cols; c++) {
last = *ans++;
for (r=1; r<*rows; r++) {
if (!ISNA(*ans)) last = *ans;
*ans++ = last;
}
}
}

fill.na.byref = function(m)
{
if (!is.matrix(m) || storage.mode(m)!="double") {
stop("input must be a matrix, storage mode double")
}
invisible(.C("fillna", m, as.integer(nrow(m)), as.integer(ncol(m)),
DUP=FALSE, NAOK=TRUE))
}

For example:

> M = matrix(as.double(sample(100)), nrow=10)
> M[sample(100,50)] = NA
> M
[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,] 88 NA NA NA 27 57 NA NA NA 99
[2,] 72 NA 83 NA 24 54 20 32 NA NA
[3,] 60 85 33 NA NA 61 2 NA 50 NA
[4,] 91 NA 8 5 NA 51 NA 39 NA 45
[5,] NA 93 21 NA NA 48 NA 69 12 56
[6,] NA NA 10 NA NA NA 14 53 NA NA
[7,] NA 15 95 NA 43 NA 34 NA 75 90
[8,] NA NA NA NA 37 NA 19 NA 7 96
[9,] 81 NA NA NA NA 89 36 NA NA 87
[10,] NA 77 NA 11 NA 18 NA 28 74 NA
> fill.na.byref(M)
> M
[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,] 88 NA NA NA 27 57 NA NA NA 99
[2,] 72 NA 83 NA 24 54 20 32 NA 99
[3,] 60 85 33 NA 24 61 2 32 50 99
[4,] 91 85 8 5 24 51 2 39 50 45
[5,] 91 93 21 5 24 48 2 69 12 56
[6,] 91 93 10 5 24 48 14 53 12 56
[7,] 91 15 95 5 43 48 34 53 75 90
[8,] 91 15 95 5 37 48 19 53 7 96
[9,] 81 15 95 5 37 89 36 53 7 87
[10,] 81 77 95 11 37 18 36 28 74 87
>

With some 'large' data (190MB), LOCF works once (11 secs) then runs out of
memory :

> M = matrix(as.double(sample(5000*5000)), nrow=5000)
> object.size(M)/1024^2
[1] 190.7350
> M[sample(5000*5000, 5000)] = NA
> system.time(filled <<- apply(M, 2, LOCF))[3]
[1] 11.33
> system.time(filled <<- apply(M, 2, LOCF))[3]
Error: cannot allocate vector of size 195312 Kb
In addition: Warning message:
Reached total allocation of 1200Mb: see help(memory.size)
Timing stopped at: 10.28 0.25 10.67 NA NA
> gc()
used (Mb) gc trigger (Mb)
Ncells 427770 11.5 741108 19.8
Vcells 100105874 763.8 125499641 957.5
> system.time(filled <<- apply(M, 2, LOCF))[3]
Error: cannot allocate vector of size 195312 Kb
In addition: Warning message:
Reached total allocation of 1200Mb: see help(memory.size)
Timing stopped at: 0 0 0 NA NA
>

Continuing with the same session, trying the C function :

> identical(filled, M)
[1] FALSE
> system.time(fill.na.byref(M))[3]
[1] 0.92
> identical(filled, M)
[1] TRUE

Running several times (shouldn't matter that the NAs are already filled
since the same work has to be done) :

> system.time(fill.na.byref(M))[3]
[1] 0.9
> system.time(fill.na.byref(M))[3]
[1] 0.89
> system.time(fill.na.byref(M))[3]
[1] 0.87
> system.time(fill.na.byref(M))[3]
[1] 0.89
> system.time(fill.na.byref(M))[3]
[1] 0.89
>

Since no working memory is required (afaik), the garbage collector isn't
involved and we get consistent, fast, timings.

Its possible there is something wrong with my setup/config which means the
LOCF method takes longer and fails. If anyone can point me in the right
direction (changing vcell options?) I'm happy to try in that direction.


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck@myway.com]
Sent: 14 September 2004 02:51
To: edd@debian.org; ajayshah@mayin.org
Cc: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] How can I do this better? (Filling in last
traded price for NA)



Dirk,

I was not aware that locf was in "its" but was aware of Tony's solution as
we had discussed both it and a forerunner of the solution in my last post at
that time. See the thread beginning with:

https://stat.ethz.ch/pipermail/r-help/2003-November/040603.html

The two solutions are the same except for the inner portion which calculates
the indices of the LOCF of a logical
vector. Simplifying slightly:

most.recent.1 <- function(L) {
     if (length(L) > 1) L[1] <- TRUE
     w <- which(c(L,T))
     rep(w[-length(w)], diff(w))
}

most.recent.2 <- function(L) {
     which(c(NA,L))[cumsum(L)+1]
}

so the key operations are which, rep and diff in #1 and which, [ and cumsum
in #2. This suggests they are about equal in speed and, in fact, some
timings I did fluctuated from run to run but in general they seemed to run
at about the same speed with #1 running faster sometimes and #2 running
faster other times (even though the same input was used on every run).

From mdowle at concordiafunds.com  Tue Sep 14 12:11:52 2004
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Tue Sep 14 12:07:36 2004
Subject: [R-sig-finance] How can I do this better? (Filling in last tr
	aded price for NA)
Message-ID: <78166BFC5165D811AA0400065BF0324B31489F@wisconsin.concordia>


Yes, that fixes the memory problem for me. Its running in a consistent time
now, about 10 times slower than the C method.

I'm wondering why you think 190MB is large? memory.limit() states 1200MB so
the data is 1/6 of the memory available to R. The box has 2GB main memory.

The 'ref' package looks good. This is intended for the challenges you refer
to isn't it ?


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck@myway.com] 
Sent: 14 September 2004 09:56
To: mdowle@concordiafunds.com; ggrothendieck@myway.com; edd@debian.org;
ajayshah@mayin.org
Cc: r-sig-finance@stat.math.ethz.ch
Subject: RE: [R-sig-finance] How can I do this better? (Filling in last tr
aded price for NA)




The memory error is due to apply rather than LOCF, itself.
You can do it largely in place in R thereby eliminating
memory errors by replacing the apply with a for:

system.time({ for(j in 1:ncol(M)) M[,j] <<- LOCF(M[,j]) })

Of course, if your data really is this large you are not 
only going to have challenges here but it will also be problematic doing any
subsequent analyses that are not 
trivial.


Date:   	Tue, 14 Sep 2004 04:31:12 +0100
From:   	Matthew Dowle <mdowle@concordiafunds.com>
To:   	'ggrothendieck@myway.com' <ggrothendieck@myway.com>,
<edd@debian.org>, <ajayshah@mayin.org>
Cc:   	<r-sig-finance@stat.math.ethz.ch>
Subject:   	RE: [R-sig-finance] How can I do this better? (Filling in
last tr aded price for NA)


How about this in C? With the example data (190MB), 11+ secs (variable) is
reduced to under 1 second consistently. Even when LOCF stops working, the C
method carries on consistently since it requires no working memory. There
would be some more work required to make it 'safe'. If a copy is required, a
copy can be taken first (or save(,compress=TRUE)'d out).

EXPORT void fillna (double *ans, int *rows, int *cols)
{
int r=0, c=0;
double last;
for (c=0; c<*cols; c++) {
last = *ans++;
for (r=1; r<*rows; r++) {
if (!ISNA(*ans)) last = *ans;
*ans++ = last;
}
}
}

fill.na.byref = function(m)
{
if (!is.matrix(m) || storage.mode(m)!="double") {
stop("input must be a matrix, storage mode double")
}
invisible(.C("fillna", m, as.integer(nrow(m)), as.integer(ncol(m)),
DUP=FALSE, NAOK=TRUE)) }

For example:

> M = matrix(as.double(sample(100)), nrow=10)
> M[sample(100,50)] = NA
> M
[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,] 88 NA NA NA 27 57 NA NA NA 99
[2,] 72 NA 83 NA 24 54 20 32 NA NA
[3,] 60 85 33 NA NA 61 2 NA 50 NA
[4,] 91 NA 8 5 NA 51 NA 39 NA 45
[5,] NA 93 21 NA NA 48 NA 69 12 56
[6,] NA NA 10 NA NA NA 14 53 NA NA
[7,] NA 15 95 NA 43 NA 34 NA 75 90
[8,] NA NA NA NA 37 NA 19 NA 7 96
[9,] 81 NA NA NA NA 89 36 NA NA 87
[10,] NA 77 NA 11 NA 18 NA 28 74 NA
> fill.na.byref(M)
> M
[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,] 88 NA NA NA 27 57 NA NA NA 99
[2,] 72 NA 83 NA 24 54 20 32 NA 99
[3,] 60 85 33 NA 24 61 2 32 50 99
[4,] 91 85 8 5 24 51 2 39 50 45
[5,] 91 93 21 5 24 48 2 69 12 56
[6,] 91 93 10 5 24 48 14 53 12 56
[7,] 91 15 95 5 43 48 34 53 75 90
[8,] 91 15 95 5 37 48 19 53 7 96
[9,] 81 15 95 5 37 89 36 53 7 87
[10,] 81 77 95 11 37 18 36 28 74 87
>

With some 'large' data (190MB), LOCF works once (11 secs) then runs out of
memory :

> M = matrix(as.double(sample(5000*5000)), nrow=5000) 
> object.size(M)/1024^2
[1] 190.7350
> M[sample(5000*5000, 5000)] = NA
> system.time(filled <<- apply(M, 2, LOCF))[3]
[1] 11.33
> system.time(filled <<- apply(M, 2, LOCF))[3]
Error: cannot allocate vector of size 195312 Kb
In addition: Warning message:
Reached total allocation of 1200Mb: see help(memory.size) Timing stopped at:
10.28 0.25 10.67 NA NA
> gc()
used (Mb) gc trigger (Mb)
Ncells 427770 11.5 741108 19.8
Vcells 100105874 763.8 125499641 957.5
> system.time(filled <<- apply(M, 2, LOCF))[3]
Error: cannot allocate vector of size 195312 Kb
In addition: Warning message:
Reached total allocation of 1200Mb: see help(memory.size) Timing stopped at:
0 0 0 NA NA
>

Continuing with the same session, trying the C function :

> identical(filled, M)
[1] FALSE
> system.time(fill.na.byref(M))[3]
[1] 0.92
> identical(filled, M)
[1] TRUE

Running several times (shouldn't matter that the NAs are already filled
since the same work has to be done) :

> system.time(fill.na.byref(M))[3]
[1] 0.9
> system.time(fill.na.byref(M))[3]
[1] 0.89
> system.time(fill.na.byref(M))[3]
[1] 0.87
> system.time(fill.na.byref(M))[3]
[1] 0.89
> system.time(fill.na.byref(M))[3]
[1] 0.89
>

Since no working memory is required (afaik), the garbage collector isn't
involved and we get consistent, fast, timings.

Its possible there is something wrong with my setup/config which means the
LOCF method takes longer and fails. If anyone can point me in the right
direction (changing vcell options?) I'm happy to try in that direction.


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck@myway.com]
Sent: 14 September 2004 02:51
To: edd@debian.org; ajayshah@mayin.org
Cc: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] How can I do this better? (Filling in last
traded price for NA)



Dirk,

I was not aware that locf was in "its" but was aware of Tony's solution as
we had discussed both it and a forerunner of the solution in my last post at
that time. See the thread beginning with:

https://stat.ethz.ch/pipermail/r-help/2003-November/040603.html

The two solutions are the same except for the inner portion which calculates
the indices of the LOCF of a logical vector. Simplifying slightly:

most.recent.1 <- function(L) {
     if (length(L) > 1) L[1] <- TRUE
     w <- which(c(L,T))
     rep(w[-length(w)], diff(w))
}

most.recent.2 <- function(L) {
     which(c(NA,L))[cumsum(L)+1]
}

so the key operations are which, rep and diff in #1 and which, [ and cumsum
in #2. This suggests they are about equal in speed and, in fact, some
timings I did fluctuated from run to run but in general they seemed to run
at about the same speed with #1 running faster sometimes and #2 running
faster other times (even though the same input was used on every run).

From Daltonmota at aol.com  Tue Sep 14 19:22:07 2004
From: Daltonmota at aol.com (Daltonmota@aol.com)
Date: Tue Sep 14 19:22:22 2004
Subject: [R-sig-finance] RE: R-sig-finance Digest, Vol 4, Issue 4
Message-ID: <0BDBAB17.70B05FEC.0ACFBA63@aol.com>

Hi Abhijit Mitra

For MIT openware go to :

http://ocw.mit.edu/OcwWeb/index.htm

and browse the site to find what you want.

For Princeton BlackBoard, first go to :

https://www.blackboard.princeton.edu/webapps/login

click on Preview button (if you do not have a princeton ID) then click on courses folder. Use the search box on left to search for specific subjects. Use keywords such as "statistics", "microeconomics" , "corporate finance", so on. Choose a course and go to course materials. Most of courses allow you to download readings, documents, exams and papers, but not all of them. Sometimes for a given course, the 2003 class material is available but not the 2002 or 2001 , that depends on who was the professor. So you may have to spend a long time hunting.

Hope that helps.
Dalton Mota

From edd at debian.org  Wed Sep 22 03:34:09 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed Sep 22 03:34:11 2004
Subject: [R-sig-finance] Posting on behalf of ...
Message-ID: <20040922013409.GA1796@sonny.eddelbuettel.com>

... "Mitra, Abhijit (IM)" <Abhijit.Mitra@morganstanley.com> whose message
landed in my listmaster lap, presumably because he is not subscribed here:

  Date: Tue, 21 Sep 2004 10:23:53 -0400
  From: "Mitra, Abhijit (IM)" <Abhijit.Mitra@morganstanley.com>
  To: <r-sig-finance-bounces@stat.math.ethz.ch>
  Subject:

  Can someone please help me what's the below code means (I have a very
  vague idea)  and where I can get some information on these commands

  cex=1.4),

  > superpanel=panel.pairs.hh, subpanel.scales=list(cex=.5),

  > cex=.7, panel.cex=1)
  --------------------------------------------------------
  
  NOTICE: If received in error, please destroy and notify sender.  Sender does
  not waive confidentiality or privilege, and use is prohibited.


These are obtions to lattice plots, and there is nothing 'finance'-specific
about them. See 

  > library(lattice)
  > library(help=lattice)
  > help(Lattice)
  
as a start. 

  > demo(lattice) 
  
is also a good idea.  More specifically, the code above seems to set pairs
plots for each of the panels in a lattice plot, with specific option for
fonts size (cex does that).

Hope that helps, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From elw at stderr.org  Wed Sep 22 22:46:21 2004
From: elw at stderr.org (elijah wright)
Date: Wed Sep 22 22:47:35 2004
Subject: [R-sig-finance] RE: diffusion of innovations modeling?
In-Reply-To: <06ED09CE.74E21DD7.0ACFBA63@aol.com>
References: <06ED09CE.74E21DD7.0ACFBA63@aol.com>
Message-ID: <Pine.LNX.4.61.0409221545450.20467@illuminati.stderr.org>



> Subject: [R-sig-finance] RE: diffusion of innovations modeling?
> 
> I would suggest you to take a look at Princeton Blackboard site and MIT 
> openware site. Both of them offer lots of online course materials from 
> their classes on many subjects, including finance and marketing. I have 
> found specific R / S-Plus code from some of these courses...its a matter 
> of mining, but its free to try out and find what you are looking for. 
> Best luck.

i was hoping someone would have specific bits in mind.  oh well.

the blackboard and opencourseware sites are very useful, aren't they? :)


elijah

From lisa at infinito.it  Thu Sep 23 05:12:08 2004
From: lisa at infinito.it (Lisa)
Date: Thu Sep 23 05:14:11 2004
Subject: [R-sig-finance] aparchFit()$fitted.value
Message-ID: <000901c4a11b$1c94f6d0$ba0f3152@userxwov7q21jr>

Dear R - finance people,
i'm not able to have residuals, fitted.value .... components from an
aparchFit() estimation as explain in the section Value of aparchFit Help,
package
fSeries.

Could someone help me?
Thanks in advance for any insight.
Lisa

From ajayshah at mayin.org  Mon Sep 27 07:24:31 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon Sep 27 07:23:15 2004
Subject: [R-sig-finance] SUMMARY: Copying previous observation for NA values
	(LOCF)
Message-ID: <E1CBnzz-0003QY-00@sanna.igidr.ac.in>


A few weeks ago, I had asked questions about situations like:

> small
           p.nifty p.inrusd p.infosys
1996-11-06  866.57    35.72     21.25
1996-11-07  874.89    35.73     21.56
1996-11-08  884.64    35.70     21.69
1996-11-10  880.36       NA     21.78
1996-11-11  891.27       NA     21.41
1996-11-13  896.84    35.73     21.72
1996-11-14  889.39    35.81     21.81

which happen a lot in finance, where we'd want to copy the last traded
price (LTP) on to the next day if the next day has an NA price.

There were a series of extremely valuable responses. See
https://stat.ethz.ch/pipermail/r-sig-finance/2004q3/thread.html and
look for "How can I do this better? (Filling in last tradedprice for
NA)". I did some reading and thinking in putting them together.

Here's a quick summary. I have the slowest machine in the world - an
IBM X20 Celeron@500 MHz notebook running linux. My problem size was
2178 rows by 3 columns. For this, I have clear data on 4 alternatives:

  ------------------------------------------------------------
  Version                                       Time (seconds)
  ------------------------------------------------------------
  My dumb loops solution                             250

  Patrick Burns' 1st solution                         13.71
  Patrick Burns' 2nd solution                          1.16

  The locf() function of ITS (pointed out by           0.32
      Dirk)
  ------------------------------------------------------------

locf() rocks! It's great and all of us should use it. ITS has a
strange format where when you say library(help=its) you don't get a
list of the functions. So locf() is hidden inside ?itsInterp. It
shouldn't be tucked away like this. Makes me wonder what else about
its is unknown to me!

There were also nice solutions proposed by Gabor, Richard Pugh, and
John Gavin (look at this thread in the mailing list archives). As far
as I could see, they weren't working in the context of the ITS
package. As an example, Richard Pugh's retain() function clearly works
for his example --

> apply(vv, 2, retain)
     ts(v1) ts(v2, start = 2)
[1,]      1                NA
[2,]      1                 5
[3,]      1                 5
[4,]      2                 3
[5,]      3                 2
[6,]      3                 2
[7,]      5                 2
[8,]      5                 1

but not when fed an ITS object --

> apply(small, 3, retain)
Error in apply(small, 3, retain) : subscript out of bounds

It would be nice if we could somehow have a generic locf() method
which worked for data frames, matrices and ITS objects.

These solutions on the mailing list, which don't work with ITS, do
have their role for the many situations where one might have data
which is not an ITS object, and it's great that all this knowledge has
been given to google on the mailing list archive.

Finally, Matthew Dowle had a solution in C which I didn't experiment
with, since I don't yet know how to marry C and R.





R code which puts together the above 4 solutions and compares their
performance is placed ahead. It uses 3 data files. You can pickup
these data files at /home/ajayshah/public_html/datafiles.tar.bz2
but I won't leave them there indefinitely.

------------------------------------------------------------ snip snip
library(its)

infosys.its <- its(readcsvIts(filename="infosys.text", header=F, sep=",",
                              col.names=c("date", "p.infosys"),
                              informat=its.format("%m/%d/%Y"),
                              outformat=its.format("%Y-%m-%d")))
inrusd.its <- its(readcsvIts(filename="inrusd.text", header=F, sep="|",
                             col.names=c("date", "p.inrusd"),
                             informat=its.format("%d %b %Y"),
                             outformat=its.format("%Y-%m-%d")))
nifty.its <- its(readcsvIts(filename="nifty.text", header=F, sep="|",
                            col.names=c("date", "p.nifty"),
                            informat=its.format("%d %b %Y"),
                            outformat=its.format("%Y-%m-%d")))
massive <- union(nifty.its, union(inrusd.its, infosys.its))
small <- massive[215:230,]

# column 1 is missing until 165. it starts from 166.
# column 2 is missing until 11, it starts from 12.
# column 3 is there from 1 onwards.

# Dumbest solution - my starting point --
loops.solution <- function(X) {
  for (i in 2:nrow(X)) {
    for (j in 1:ncol(X)) {
      if (is.na(X[i,j])) {
        X[i,j] = X[i-1,j]
      }
    }
  }
  return(X)
}

# First solution proposed by Patrick Burns --
pburns <- function(X) {
  mass.na <- is.na(X)
  for (i in 2:nrow(X)) {
    for (j in 1:ncol(X)) {
      if (mass.na[i,j]) {
        X[i,j] <- X[i-1, j]
      }
    }
  }
  return(X)
}

# Second solution proposed by Patrick Burns --
pburns.columnatatime <- function(X) {
  subfun.miss.use <- function(x) {
    missing <- which(is.na(x))   # Makes a vector of indexes of missing data
    return(missing[missing != seq(along=missing)]) # Don't understand this.
  }
  for (j in 1:ncol(X)) {
    while (length(this.mis <- subfun.miss.use(X[, j]))) {
      X[this.mis, j] <- X[this.mis-1, j]
    }   # Does this correctly handle situations with row=1?
  }     # For those, we shouldn't be copy from row=0.
  return(X)
}

# If you want to recreate it --
system.time(S0 <- loops.solution(massive)) #  249.15  0.19 258.87   0.00   0.00
#   save(S0, file="quickly.rda")
# If you want to just read it in to save time --
# load("quickly.rda")

system.time(S1 <- pburns(massive)) # 13.71  0.00 14.00  0.00  0.00
which(S0!=S1)
system.time(S2 <- pburns.columnatatime(massive)) # 1.16 0.00 1.16 0.00 0.00
which(S0!=S2)

# The 3rd solution is to use the locf() function from its, as suggested
# by Dirk. "Its in its but its documentation isn't". :-)
# "locf" = "last observation carried forward".
system.time(S3 <- locf(massive)) # 0.32 0.00 0.32 0.00 0.00
which(S0!=S3)
------------------------------------------------------------ snip snip

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From ajayshah at mayin.org  Mon Sep 27 11:03:13 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon Sep 27 11:01:50 2004
Subject: [R-sig-finance] SUMMARY: Copying previous observation for NA
	values (LOCF)
In-Reply-To: <16727.54501.918621.339689@gargle.gargle.HOWL>
References: <E1CBnzz-0003QY-00@sanna.igidr.ac.in>
	<16727.54501.918621.339689@gargle.gargle.HOWL>
Message-ID: <20040927090313.GZ713@igidr.ac.in>

On Mon, Sep 27, 2004 at 10:52:53AM +0200, Martin Maechler wrote:
>     Ajay> R code which puts together the above 4 solutions and compares their
>     Ajay> performance is placed ahead. It uses 3 data files. You can pickup
>     Ajay> these data files at /home/ajayshah/public_html/datafiles.tar.bz2
> 
> oops! I'm pretty sure you meant
>       http://www.mayin.org/ajayshah/datafiles.tar.bz2
> 
> BUT,
> the file is a bit misnomed :
> 
> After downloading
>   {maechler}202> bunzip2 ~/datafiles.tar.bz2
>   bunzip2: /u/maechler/datafiles.tar.bz2 is not a bzip2 file.
>   {maechler}203> file ~/datafiles.tar.bz2
>   /u/maechler/datafiles.tar.bz2: gzip compressed data, from Unix
>   {maechler}204> mv ~/datafiles.tar.bz2 ~/datafiles.tar.gz
>   {maechler}205> gunzip datafiles.tar.gz
> 
> i.e., I think it should have been named  datafiles.tar.gz

Ouch! Sorry for the mistake. You are of course right.

$ file ~/public_html/datafiles.tar.bz2 
/home/ajayshah/public_html/datafiles.tar.bz2: gzip compressed data, from Unix
$ cd public_html/    
$ cp datafiles.tar.bz2 datafiles.tar.gz  
$ update-page
ajayshah@66.216.80.49's password: 

So the old file (datafiles.tar.bz2) is still there as is the correct
file datafiles.tar.gz

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From patrick at burns-stat.com  Mon Sep 27 12:36:51 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon Sep 27 12:37:07 2004
Subject: [R-sig-finance] POP version 2 released
Message-ID: <4157ED43.1080200@burns-stat.com>

Version 2 of POP Portfolio Construction Suite has just been released.
This runs under both R and S-PLUS, and there is a license fee for it.

What undoubtedly will become the most important feature is the ability
to generate random portfolios that obey a set of constraints but pay no
heed to the utility.  These can be used sort of like a bootstrap or a
permutation test.  The POP User's Manual (freely available on the Burns
Statistics website) has a few ideas on how they can be used.  The
working paper "Does my beta look big  in this?", also on the website,
is an example of using them to test the efficacy of constraints.

This forum seems like a likely source of further ideas of how they can be
used.

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

From Jordi.Molins at drkw.com  Mon Sep 27 17:45:49 2004
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Mon Sep 27 17:45:58 2004
Subject: [R-sig-finance] Common project
Message-ID: <AA0BBC8742AFFF4583B91782E958CB660FCFDD@ibfftce121.de.ad.drkw.net>


Hello to everybody,

some time ago in this list, it appeared to me that some people wanted to
involve others into common projects. I want to add to this idea with a
project:

I have read in diagonal the following paper:	

 <<Waelti_Contagion_Central_Europe.pdf>> 
and I find it interesting (I define "interesting" both as something
intellectually interesting and something with a possibility to be applied to
real trading). However, as many interesting things in life, it requires to
devote a lot of time implementing it. And I do not have so much time.
However, I know that sharing efforts between people could make the project
doable.

The project, in few words, would be the following:

develop the model described in the paper, consisting mainly in the shocks
produced by US monetary policy into Central European economies. The
relationship among these economies is also of interest. And, in the end,
reproduce fully their results.

Is there somebody interested? maybe somebody has already taken a look at the
paper? or maybe somebody has worked out a similar example that could be
applied more or less directly to this one?

Since I guess everybody is pretty busy here, I suggest just to send R code
to r-sig-finance@stat.math.ethz.ch

I have not written any code yet, but if I see that there are enough people
interested, I promise to add to the project.

Rgrds and hoping to hear from the finance list soon

Jordi



--------------------------------------------------------------------------------
The information contained herein is confidential and is intended solely for the
addressee. Access by any other party is unauthorised without the express 
written permission of the sender. If you are not the intended recipient, please 
contact the sender either via the company switchboard on +44 (0)20 7623 8000, or
via e-mail return. If you have received this e-mail in error or wish to read our
e-mail disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender. 3166
--------------------------------------------------------------------------------

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Waelti_Contagion_Central_Europe.pdf
Type: application/octet-stream
Size: 297254 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20040927/a8236e24/Waelti_Contagion_Central_Europe.obj
From Abhijit.Mitra at morganstanley.com  Wed Sep 29 15:52:38 2004
From: Abhijit.Mitra at morganstanley.com (Mitra, Abhijit (IM))
Date: Wed Sep 29 16:16:17 2004
Subject: [R-sig-finance] (no subject)
Message-ID: <D2F2509B543EEE41BC246AE0ADECF78D01F7ACF1@NYWEXMB19.msad.ms.com>

Hi,
I want to know how I add an axis label to my bwplot using S+,any help
will be appreciated, thanks

A

-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of
r-sig-finance-request@stat.math.ethz.ch
Sent: Tuesday, September 28, 2004 6:21 AM
To: r-sig-finance@stat.math.ethz.ch
Subject: R-sig-finance Digest, Vol 4, Issue 10

Send R-sig-finance mailing list submissions to
	r-sig-finance@stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
or, via email, send a message with subject or body 'help' to
	r-sig-finance-request@stat.math.ethz.ch

You can reach the person managing the list at
	r-sig-finance-owner@stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific than
"Re: Contents of R-sig-finance digest..."


Today's Topics:

   1. POP version 2 released (Patrick Burns)
   2. Common project (Molins, Jordi)


----------------------------------------------------------------------

Message: 1
Date: Mon, 27 Sep 2004 11:36:51 +0100
From: Patrick Burns <patrick@burns-stat.com>
Subject: [R-sig-finance] POP version 2 released
To: r-sig-finance <r-sig-finance@stat.math.ethz.ch>
Message-ID: <4157ED43.1080200@burns-stat.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Version 2 of POP Portfolio Construction Suite has just been released.
This runs under both R and S-PLUS, and there is a license fee for it.

What undoubtedly will become the most important feature is the ability
to generate random portfolios that obey a set of constraints but pay no
heed to the utility.  These can be used sort of like a bootstrap or a
permutation test.  The POP User's Manual (freely available on the Burns
Statistics website) has a few ideas on how they can be used.  The
working paper "Does my beta look big  in this?", also on the website, is
an example of using them to test the efficacy of constraints.

This forum seems like a likely source of further ideas of how they can
be used.

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



------------------------------

Message: 2
Date: Mon, 27 Sep 2004 17:45:49 +0200
From: "Molins, Jordi" <Jordi.Molins@drkw.com>
Subject: [R-sig-finance] Common project
To: r-sig-finance@stat.math.ethz.ch
Cc: "Pfaff, Bernhard" <Bernhard.Pfaff@drkw.com>
Message-ID:
	
<AA0BBC8742AFFF4583B91782E958CB660FCFDD@ibfftce121.de.ad.drkw.net>
Content-Type: text/plain; charset="iso-8859-1"


Hello to everybody,

some time ago in this list, it appeared to me that some people wanted to
involve others into common projects. I want to add to this idea with a
project:

I have read in diagonal the following paper:	

 <<Waelti_Contagion_Central_Europe.pdf>>
and I find it interesting (I define "interesting" both as something
intellectually interesting and something with a possibility to be
applied to real trading). However, as many interesting things in life,
it requires to devote a lot of time implementing it. And I do not have
so much time.
However, I know that sharing efforts between people could make the
project doable.

The project, in few words, would be the following:

develop the model described in the paper, consisting mainly in the
shocks produced by US monetary policy into Central European economies.
The relationship among these economies is also of interest. And, in the
end, reproduce fully their results.

Is there somebody interested? maybe somebody has already taken a look at
the paper? or maybe somebody has worked out a similar example that could
be applied more or less directly to this one?

Since I guess everybody is pretty busy here, I suggest just to send R
code to r-sig-finance@stat.math.ethz.ch

I have not written any code yet, but if I see that there are enough
people interested, I promise to add to the project.

Rgrds and hoping to hear from the finance list soon

Jordi



------------------------------------------------------------------------
--------
The information contained herein is confidential and is intended solely
for the addressee. Access by any other party is unauthorised without the
express written permission of the sender. If you are not the intended
recipient, please contact the sender either via the company switchboard
on +44 (0)20 7623 8000, or via e-mail return. If you have received this
e-mail in error or wish to read our e-mail disclaimer statement and
monitoring policy, please refer to http://www.drkw.com/disc/email/ or
contact the sender. 3166
------------------------------------------------------------------------
--------

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Waelti_Contagion_Central_Europe.pdf
Type: application/octet-stream
Size: 297254 bytes
Desc: not available
Url :
https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20040927/a8236e
24/Waelti_Contagion_Central_Europe-0001.obj

------------------------------

_______________________________________________
R-sig-finance mailing list
R-sig-finance@stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


End of R-sig-finance Digest, Vol 4, Issue 10
******************************************** 
--------------------------------------------------------
 
NOTICE: If received in error, please destroy and notify sender.  Sender does not waive confidentiality or privilege, and use is prohibited.

From t.khan at econ.bbk.ac.uk  Thu Sep 30 16:22:31 2004
From: t.khan at econ.bbk.ac.uk (Taher Khan)
Date: Thu Sep 30 16:43:34 2004
Subject: [R-sig-finance] How to set up an ITS object, just the basics?
Message-ID: <s15c24c5.011@markets.econ.bbk.ac.uk>

Greetings!

Apologize for bringing up a really simple question, unfortunately I
cannot make sense of the help file for ITS.

I have a dataframe dfNAV which consists of a POSIXct date value %d/%m/%Y
and two values, some of which are NA's.

I want to set up an ITS object so that I can use locf().

Can someone please explain the syntax for setting it up? For example:

dfNAV.its <- its(as.matrix(dfNAV), format = '%d-%m-%Y')

Is it not working because I already have a date value, should I convert
it to a string before trying to make it an ITS?

Best regards and thanks in advance,

Taher.

From edd at debian.org  Thu Sep 30 17:15:27 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu Sep 30 17:15:31 2004
Subject: [R-sig-finance] How to set up an ITS object, just the basics?
In-Reply-To: <s15c24c5.011@markets.econ.bbk.ac.uk>
References: <s15c24c5.011@markets.econ.bbk.ac.uk>
Message-ID: <20040930151527.GA6934@sonny.eddelbuettel.com>

(Post manually approved; Taher you may want to subscribe to the list with
this address --Dirk)

On Thu, Sep 30, 2004 at 03:22:31PM +0100, Taher Khan wrote:
> Greetings!
> 
> Apologize for bringing up a really simple question, unfortunately I
> cannot make sense of the help file for ITS.
> 
> I have a dataframe dfNAV which consists of a POSIXct date value %d/%m/%Y
> and two values, some of which are NA's.
> 
> I want to set up an ITS object so that I can use locf().
> 
> Can someone please explain the syntax for setting it up? For example:
> 
> dfNAV.its <- its(as.matrix(dfNAV), format = '%d-%m-%Y')
> 
> Is it not working because I already have a date value, should I convert
> it to a string before trying to make it an ITS?

It is really all there in the docs. The example section of help(its) has

   mat <- structure(1:6,dim=c(2,3),dimnames=list(c("2003-01-01","2003-01-04"),letters[1:3]))
   
which creates a matrix. Try class(mat), and then try  class(its(mat)).
Similarly 

   times <- as.POSIXct(strptime(c("1999-12-31 01:00:00","2000-01-01 02:00:00"),format="%Y-%m-%d %X"))

creates a POSIXct object which you can then use as the second argument to
its() -- try class(times) and class(its(mat,times)).

So in a nutshell, you can create its objects either way.  I tend to fill the
data and time components as two components, so 

   myIts <- its(myData, myDatesAsPosix) 
   
would be my choice.

Hope this helps, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From ggrothendieck at myway.com  Thu Sep 30 21:18:33 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu Sep 30 21:18:43 2004
Subject: [R-sig-finance] How to set up an ITS object, just the basics?
Message-ID: <20040930191833.6C6C539B5@mprdmxin.myway.com>



1. First define a test data frame.  2. Then convert it to its
and perform a locf.  3. Do the same thing using the upcoming
version of zoo (not yet on CRAN).  4. If you want to eliminate
dependence on external packages note that LOCF can be defined
in a one line function so finally we define our own mylocf and 
run it directly on the data frame.

R> # 1. define test data
R> tt <- seq(Sys.time(), len = 5, by = "month")
R> x1 <- c(1,NA,NA,4:5)
R> x2 <- c(1:4,NA)
R> dfNAV <- data.frame(tt, x1, x2)

R> # 2. using its 
R> require(its)
[1] TRUE
R> itsNAV <- as.its(data.matrix(dfNAV))
R> itsNAV <- locf(itsNAV)
R> itsNAV
           x1 x2
2004-09-30  1  1
2004-10-30  1  2
2004-11-30  1  3
2004-12-30  4  4
2005-01-30  5  4

R> # 3. uses the upcoming version of zoo (not yet on CRAN)
R> require(zoo)
[1] TRUE
R> dfNAV.zoo <- zoo(dfNAV[,2:3], dfNAV[,1])
R> dfNAV.zoo <- LOCF(dfNAV.zoo)
R> dfNAV.zoo
                    x1 x2
2004-09-30 15:13:42 1  1 
2004-10-30 15:13:42 1  2 
2004-11-30 15:13:42 1  3 
2004-12-30 15:13:42 4  4 
2005-01-30 15:13:42 5  4 

R> # 4. define own locf function and apply to data frame
R> mylocf<- function(x) x[c(NA,which(!is.na(x)))[cumsum(!is.na(x))+1]]
R> dfNAV[,2:3] <- apply(dfNAV[,2:3], 2, mylocf)
R> dfNAV
                   tt x1 x2
1 2004-09-30 15:13:42  1  1
2 2004-10-30 15:13:42  1  2
3 2004-11-30 15:13:42  1  3
4 2004-12-30 15:13:42  4  4
5 2005-01-30 15:13:42  5  4






Date:   	Thu, 30 Sep 2004 15:22:31 +0100
From:   	Taher Khan <t.khan@econ.bbk.ac.uk>
To:   	r-sig-finance <r-sig-finance@stat.math.ethz.ch>
Subject:   	[R-sig-finance] How to set up an ITS object, just the basics?

Greetings!

Apologize for bringing up a really simple question, unfortunately I
cannot make sense of the help file for ITS.

I have a dataframe dfNAV which consists of a POSIXct date value %d/%m/%Y
and two values, some of which are NA's.

I want to set up an ITS object so that I can use locf().

Can someone please explain the syntax for setting it up? For example:

dfNAV.its <- its(as.matrix(dfNAV), format = '%d-%m-%Y')

Is it not working because I already have a date value, should I convert
it to a string before trying to make it an ITS?

Best regards and thanks in advance,

Taher.

