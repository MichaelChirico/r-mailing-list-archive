From josh.m.ulrich at gmail.com  Tue Jul  1 04:53:59 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Mon, 30 Jun 2008 21:53:59 -0500
Subject: [R-SIG-Finance] Hourly quotations from 1 minutes quotations.
In-Reply-To: <453388.71856.qm@web28103.mail.ukl.yahoo.com>
References: <453388.71856.qm@web28103.mail.ukl.yahoo.com>
Message-ID: <8cca69990806301953x605d777bof9983a646c9b2bf4@mail.gmail.com>

Hi Pierre8r,

Your script does not assign the output from to.minutes5() and
to.hourly() to q5mns and qHourly.  Try this:

q5mns <- to.minutes5(q1mn)
qHourly <- to.hourly(q1mn)

barChart(q1mn,TA=NULL)
barChart(q5mns,TA=NULL)
barChart(qHourly,TA=NULL)

Best,
Josh

--
http://quantemplation.blogspot.com


On Mon, Jun 30, 2008 at 3:12 PM,  <pierre8r-list at yahoo.fr> wrote:
> Hello,
>
> I am trying to do 5 minutes quotations and to hourly quotations.
> >From 1 minute quotations.
> I wrote a script that does not work.
>
> Thank you in advance,
>
> Pierre8r
>
> Here's my script that does not work:
>
> library(xts)
> library(quantmod)
>
> quotes <- read.csv2("E:\\00001-Compare\\Import\\ImportIB\\TestGBPUSD-1mn.txt",
> header = TRUE, sep = ",", dec=".")
>
> q1mn <- xts(as.matrix(quotes[,-(1:2)]),as.POSIXct(paste(quotes[,1],quotes[,2]),
> format='%m/%d/%Y%H:%M'))
>
> colnames(q1mn) <- c('Open','High','Low','Close','Volume')
>
> q5mns <- q1mn
> to.minutes5(q5mns)
>
> qHourly <- q1mn
> to.hourly(qHourly)
>
> barChart(q1mn,TA=NULL)
> barChart(q5mns,TA=NULL)
> barChart(qHourly,TA=NULL)
>
>
> Some datas :
>
> 01/08/2007,00:00,1.93025,1.93025,1.93015,1.9302,-1
> 01/08/2007,00:01,1.9302,1.9302,1.93015,1.9302,-1
> 01/08/2007,00:02,1.9302,1.93025,1.9302,1.9302,-1
> 01/08/2007,00:03,1.9302,1.93025,1.9302,1.9302,-1
> 01/08/2007,00:04,1.9302,1.93025,1.9302,1.9302,-1
> 01/08/2007,00:05,1.9302,1.93025,1.9302,1.9302,-1
> 01/08/2007,00:06,1.9302,1.93025,1.93015,1.93015,-1
> 01/08/2007,00:07,1.93015,1.93025,1.93015,1.9302,-1
> 01/08/2007,00:08,1.9302,1.93035,1.9302,1.9303,-1
> 01/08/2007,00:09,1.9303,1.9303,1.9303,1.9303,-1
> 01/08/2007,00:10,1.9303,1.9303,1.9303,1.9303,-1
> 01/08/2007,00:11,1.9303,1.9303,1.9303,1.9303,-1
> 01/08/2007,00:12,1.9303,1.9303,1.9302,1.9302,-1
> 01/08/2007,00:13,1.9302,1.9302,1.9302,1.9302,-1
> 01/08/2007,00:14,1.9302,1.9302,1.9302,1.9302,-1
> 01/08/2007,00:15,1.9302,1.9302,1.9302,1.9302,-1
> 01/08/2007,00:16,1.9302,1.9302,1.9302,1.9302,-1
> 01/08/2007,00:17,1.9302,1.9302,1.9302,1.9302,-1
> 01/08/2007,00:18,1.9302,1.9302,1.9302,1.9302,-1
> 01/08/2007,00:19,1.9302,1.9302,1.9302,1.9302,-1
> 01/08/2007,00:20,1.9302,1.9302,1.9302,1.9302,-1
> 01/08/2007,00:21,1.9302,1.9302,1.9302,1.9302,-1
> 01/08/2007,00:22,1.9302,1.9302,1.9302,1.9302,-1
> 01/08/2007,00:23,1.9302,1.9302,1.9302,1.9302,-1
> 01/08/2007,00:24,1.9302,1.9302,1.93015,1.93015,-1
> 01/08/2007,00:25,1.93015,1.93015,1.93015,1.93015,-1
> 01/08/2007,00:26,1.93015,1.93015,1.93015,1.93015,-1
> 01/08/2007,00:27,1.93015,1.93015,1.93015,1.93015,-1
> 01/08/2007,00:28,1.93015,1.93015,1.93015,1.93015,-1
> 01/08/2007,00:29,1.93015,1.93015,1.93005,1.93005,-1
> 01/08/2007,00:30,1.93005,1.93005,1.93005,1.93005,-1
> 01/08/2007,00:31,1.93005,1.93005,1.93005,1.93005,-1
> 01/08/2007,00:32,1.93005,1.93005,1.93005,1.93005,-1
> 01/08/2007,00:33,1.93005,1.93005,1.93005,1.93005,-1
> 01/08/2007,00:34,1.93005,1.93005,1.93005,1.93005,-1
> 01/08/2007,00:35,1.93005,1.93005,1.93005,1.93005,-1
> 01/08/2007,00:36,1.93005,1.93015,1.93005,1.93005,-1
> 01/08/2007,00:37,1.93005,1.9301,1.93005,1.93005,-1
> 01/08/2007,00:38,1.93005,1.93015,1.93005,1.93015,-1
> 01/08/2007,00:39,1.93015,1.93015,1.93015,1.93015,-1
> 01/08/2007,00:40,1.93015,1.93015,1.9301,1.9301,-1
> 01/08/2007,00:41,1.9301,1.9302,1.9301,1.9302,-1
> 01/08/2007,00:42,1.9302,1.9302,1.9301,1.9301,-1
> 01/08/2007,00:43,1.9301,1.9301,1.9301,1.9301,-1
> 01/08/2007,00:44,1.9301,1.9301,1.9301,1.9301,-1
> 01/08/2007,00:45,1.9301,1.9301,1.93005,1.9301,-1
> 01/08/2007,00:46,1.9301,1.9301,1.9301,1.9301,-1
> 01/08/2007,00:47,1.9301,1.9301,1.93005,1.9301,-1
> 01/08/2007,00:48,1.9301,1.9301,1.9301,1.9301,-1
> 01/08/2007,00:49,1.9301,1.9301,1.9301,1.9301,-1
> 01/08/2007,00:50,1.9301,1.9301,1.9301,1.9301,-1
> 01/08/2007,00:51,1.9301,1.9301,1.92995,1.93,-1
> 01/08/2007,00:52,1.93,1.93015,1.93,1.93005,-1
> 01/08/2007,00:53,1.93005,1.93005,1.92995,1.93,-1
> 01/08/2007,00:54,1.93,1.93,1.92995,1.93,-1
> 01/08/2007,00:55,1.93,1.93,1.92995,1.93,-1
>
>
>
>
>      ____________________________________________________________
> ente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From enricoschumann at yahoo.de  Tue Jul  1 09:02:18 2008
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Tue, 1 Jul 2008 09:02:18 +0200
Subject: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
In-Reply-To: <15722777.post@talk.nabble.com>
Message-ID: <732668.44638.bm@omp218.mail.ukl.yahoo.com>

how about bootstrapping? keeping the cross-sectional correlation in the data
is fairly simple by sampling whole rows from your returns matrix (assumed of
dimension observations times returns), but the serial dependence is more
difficult. if you have an idea how this serial dependence looks like (or,
say, you know what parts you want to reproduce in your scenario sets) you
may fit a regression model capturing this dependence and then resample from
the residuals. if you want a rather non-parametric approach, block
bootstrapping may be a technique to look at. 

i think patrick burns has a tutorial on bootstrapping on his homepage
http://www.burns-stat.com/

enrico

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von maratikus
Gesendet: Mittwoch, 27. Februar 2008 21:49
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization


I am exploring robust portfolio optimization.  I have historical daily data
for 20 stocks over 2 year period.  i'd like to simulate 1,000 datasets of 1
year each that have autocorrelation and cross-correlation properties similar
to those of the historical data.  Then I'd like to find allocation that
maximizes minimum risk-adjusted return over 1,000 datasets.  All suggestions
are appreciated!
--
View this message in context:
http://www.nabble.com/robust-portfolio-optimization-tp15722777p15722777.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG. 

28.06.2008
19:42


From station.ning at gmail.com  Tue Jul  1 09:54:34 2008
From: station.ning at gmail.com (ning zhang)
Date: Tue, 1 Jul 2008 08:54:34 +0100
Subject: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
In-Reply-To: <732668.44638.bm@omp218.mail.ukl.yahoo.com>
References: <15722777.post@talk.nabble.com>
	<732668.44638.bm@omp218.mail.ukl.yahoo.com>
Message-ID: <9519fd9f0807010054s8215ei7bf9c549e15f3522@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080701/a8f7dfcd/attachment.ksh>

From enricoschumann at yahoo.de  Tue Jul  1 10:10:11 2008
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Tue, 1 Jul 2008 10:10:11 +0200
Subject: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
In-Reply-To: <9519fd9f0807010054s8215ei7bf9c549e15f3522@mail.gmail.com>
Message-ID: <555700.40036.bm@omp220.mail.ukl.yahoo.com>

there are lots of choices how to obtain the `robust solution' you want.
maybe optimise the weights to give the *mean* sharpe/sortino whatever, or to
maximise a quantile (or the lowest) of the objective functions of your 1,000
data sets.

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von ning zhang
Gesendet: Dienstag, 1. Juli 2008 09:55
An: Enrico Schumann
Cc: r-sig-finance at stat.math.ethz.ch
Betreff: Re: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization

you could sign the random weight to each assets first, and then calculated
portfolio variance as well as portfolio return. Finally, you could use monte
carlo simulation to optimise the weight of each asset, which gives you the
best sharp ratio.



On Tue, Jul 1, 2008 at 8:02 AM, Enrico Schumann <enricoschumann at yahoo.de>
wrote:

> how about bootstrapping? keeping the cross-sectional correlation in 
> the data is fairly simple by sampling whole rows from your returns 
> matrix (assumed of dimension observations times returns), but the 
> serial dependence is more difficult. if you have an idea how this 
> serial dependence looks like (or, say, you know what parts you want to 
> reproduce in your scenario sets) you may fit a regression model 
> capturing this dependence and then resample from the residuals. if you 
> want a rather non-parametric approach, block bootstrapping may be a 
> technique to look at.
>
> i think patrick burns has a tutorial on bootstrapping on his homepage 
> http://www.burns-stat.com/
>
> enrico
>
> -----Urspr|ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von 
> maratikus
> Gesendet: Mittwoch, 27. Februar 2008 21:49
> An: r-sig-finance at stat.math.ethz.ch
> Betreff: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
>
>
> I am exploring robust portfolio optimization.  I have historical daily 
> data for 20 stocks over 2 year period.  i'd like to simulate 1,000 
> datasets of 1 year each that have autocorrelation and 
> cross-correlation properties similar to those of the historical data.  
> Then I'd like to find allocation that maximizes minimum risk-adjusted 
> return over 1,000 datasets.  All suggestions are appreciated!
> --
> View this message in context:
>
> http://www.nabble.com/robust-portfolio-optimization-tp15722777p1572277
> 7.html Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> No virus found in this incoming message.
> Checked by AVG.
>
> 28.06.2008
> 19:42
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

	[[alternative HTML version deleted]]


No virus found in this incoming message.
Checked by AVG. 

30.06.2008
08:43


From patrick at burns-stat.com  Tue Jul  1 11:44:08 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 01 Jul 2008 10:44:08 +0100
Subject: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
In-Reply-To: <555700.40036.bm@omp220.mail.ukl.yahoo.com>
References: <555700.40036.bm@omp220.mail.ukl.yahoo.com>
Message-ID: <4869FC68.8030507@burns-stat.com>

I pretty much understand all of the solutions
that have been offered.  What I don't understand
is the original question.

How do you know if your solution is
good or not?  Given that you have two years of data
and you are talking about samples of one year, a
reasonable plan would be to test an out-of-sample
period (a day, a week, ...) and then move the in-sample
data that amount.  Only a year of out-of-sample data
seems rather short to me.

You want to get good returns.  I think it is safe to say
that the amount of predictive power in a year of daily
returns is infinitesimal, no matter how much fancy footwork
you do.  A test of optimization technology without a good
predictive model for returns is going to be driven by noise
unless you are creating minimum variance portfolios (or
some other minimum risk).


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


Enrico Schumann wrote:
> there are lots of choices how to obtain the `robust solution' you want.
> maybe optimise the weights to give the *mean* sharpe/sortino whatever, or to
> maximise a quantile (or the lowest) of the objective functions of your 1,000
> data sets.
>
> -----Urspr?ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von ning zhang
> Gesendet: Dienstag, 1. Juli 2008 09:55
> An: Enrico Schumann
> Cc: r-sig-finance at stat.math.ethz.ch
> Betreff: Re: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
>
> you could sign the random weight to each assets first, and then calculated
> portfolio variance as well as portfolio return. Finally, you could use monte
> carlo simulation to optimise the weight of each asset, which gives you the
> best sharp ratio.
>
>
>
> On Tue, Jul 1, 2008 at 8:02 AM, Enrico Schumann <enricoschumann at yahoo.de>
> wrote:
>
>   
>> how about bootstrapping? keeping the cross-sectional correlation in 
>> the data is fairly simple by sampling whole rows from your returns 
>> matrix (assumed of dimension observations times returns), but the 
>> serial dependence is more difficult. if you have an idea how this 
>> serial dependence looks like (or, say, you know what parts you want to 
>> reproduce in your scenario sets) you may fit a regression model 
>> capturing this dependence and then resample from the residuals. if you 
>> want a rather non-parametric approach, block bootstrapping may be a 
>> technique to look at.
>>
>> i think patrick burns has a tutorial on bootstrapping on his homepage 
>> http://www.burns-stat.com/
>>
>> enrico
>>
>> -----Urspr|ngliche Nachricht-----
>> Von: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von 
>> maratikus
>> Gesendet: Mittwoch, 27. Februar 2008 21:49
>> An: r-sig-finance at stat.math.ethz.ch
>> Betreff: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
>>
>>
>> I am exploring robust portfolio optimization.  I have historical daily 
>> data for 20 stocks over 2 year period.  i'd like to simulate 1,000 
>> datasets of 1 year each that have autocorrelation and 
>> cross-correlation properties similar to those of the historical data.  
>> Then I'd like to find allocation that maximizes minimum risk-adjusted 
>> return over 1,000 datasets.  All suggestions are appreciated!
>> --
>> View this message in context:
>>
>> http://www.nabble.com/robust-portfolio-optimization-tp15722777p1572277
>> 7.html Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>> No virus found in this incoming message.
>> Checked by AVG.
>>
>> 28.06.2008
>> 19:42
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>     
>
> 	[[alternative HTML version deleted]]
>
>
> No virus found in this incoming message.
> Checked by AVG. 
>
> 30.06.2008
> 08:43
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
>


From station.ning at gmail.com  Tue Jul  1 15:58:05 2008
From: station.ning at gmail.com (ning zhang)
Date: Tue, 1 Jul 2008 14:58:05 +0100
Subject: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
In-Reply-To: <4869FC68.8030507@burns-stat.com>
References: <555700.40036.bm@omp220.mail.ukl.yahoo.com>
	<4869FC68.8030507@burns-stat.com>
Message-ID: <9519fd9f0807010658h142e1178r886d2d8560016f79@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080701/6a4519aa/attachment.pl>

From brian at braverock.com  Tue Jul  1 16:38:40 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 01 Jul 2008 09:38:40 -0500
Subject: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
In-Reply-To: <9519fd9f0807010658h142e1178r886d2d8560016f79@mail.gmail.com>
References: <555700.40036.bm@omp220.mail.ukl.yahoo.com>	<4869FC68.8030507@burns-stat.com>
	<9519fd9f0807010658h142e1178r886d2d8560016f79@mail.gmail.com>
Message-ID: <486A4170.3070500@braverock.com>

Patrick's point is still correct.  With only two years of data, you have 
basically one year of in-sample data and one year of out-of-sample data. 
  This is hardly enough to build any reliable statistical inference on 
for anything other than a minimum risk portfolio (and even that is 
highly questionable).

I'd say that the first step should be to get more data.  Ideally this 
would be by getting additional history on the target investments, and 
doing the optimization over a longer time frame.

If you still don't have enough data, you could combine a distributional 
assumption from a "similar" industry or company and do a Bayesian or 
Monte Carlo historical simulation, although you'd want to try to do 
conditional/dependent variance to get reasonable covariances. 
Basically, this then becomes a very hard problem.

We also haven't addresses what the meaning of "robust" is here, but I'll 
let the original poster comment on what they meant without inferring too 
much.

Regards,

   - Brian

ning zhang wrote:
> you could split the data into training and testing periods. Find the
> optimised weight by using the training dataset, then apply this weights into
> the testing period to see whether the performance is consistance. If it is
> consistance, then it is a good model.
> 
> On Tue, Jul 1, 2008 at 10:44 AM, Patrick Burns <patrick at burns-stat.com>
> wrote:
> 
>> I pretty much understand all of the solutions
>> that have been offered.  What I don't understand
>> is the original question.
>>
>> How do you know if your solution is
>> good or not?  Given that you have two years of data
>> and you are talking about samples of one year, a
>> reasonable plan would be to test an out-of-sample
>> period (a day, a week, ...) and then move the in-sample
>> data that amount.  Only a year of out-of-sample data
>> seems rather short to me.
>>
>> You want to get good returns.  I think it is safe to say
>> that the amount of predictive power in a year of daily
>> returns is infinitesimal, no matter how much fancy footwork
>> you do.  A test of optimization technology without a good
>> predictive model for returns is going to be driven by noise
>> unless you are creating minimum variance portfolios (or
>> some other minimum risk).
>>
>>
>> Patrick Burns
>> patrick at burns-stat.com
>> +44 (0)20 8525 0696
>> http://www.burns-stat.com
>> (home of S Poetry and "A Guide for the Unwilling S User")
>>
>>
>> Enrico Schumann wrote:
>>
>>> there are lots of choices how to obtain the `robust solution' you want.
>>> maybe optimise the weights to give the *mean* sharpe/sortino whatever, or
>>> to
>>> maximise a quantile (or the lowest) of the objective functions of your
>>> 1,000
>>> data sets.
>>>
>>> -----Urspr?ngliche Nachricht-----
>>> Von: r-sig-finance-bounces at stat.math.ethz.ch
>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von ning
>>> zhang
>>> Gesendet: Dienstag, 1. Juli 2008 09:55
>>> An: Enrico Schumann
>>> Cc: r-sig-finance at stat.math.ethz.ch
>>> Betreff: Re: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
>>>
>>> you could sign the random weight to each assets first, and then calculated
>>> portfolio variance as well as portfolio return. Finally, you could use
>>> monte
>>> carlo simulation to optimise the weight of each asset, which gives you the
>>> best sharp ratio.
>>>
>>>
>>>
>>> On Tue, Jul 1, 2008 at 8:02 AM, Enrico Schumann <enricoschumann at yahoo.de>
>>> wrote:
>>>
>>>
>>>
>>>> how about bootstrapping? keeping the cross-sectional correlation in the
>>>> data is fairly simple by sampling whole rows from your returns matrix
>>>> (assumed of dimension observations times returns), but the serial dependence
>>>> is more difficult. if you have an idea how this serial dependence looks like
>>>> (or, say, you know what parts you want to reproduce in your scenario sets)
>>>> you may fit a regression model capturing this dependence and then resample
>>>> from the residuals. if you want a rather non-parametric approach, block
>>>> bootstrapping may be a technique to look at.
>>>>
>>>> i think patrick burns has a tutorial on bootstrapping on his homepage
>>>> http://www.burns-stat.com/
>>>>
>>>> enrico
>>>>
>>>> -----Urspr|ngliche Nachricht-----
>>>> Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von
>>>> maratikus
>>>> Gesendet: Mittwoch, 27. Februar 2008 21:49
>>>> An: r-sig-finance at stat.math.ethz.ch
>>>> Betreff: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
>>>>
>>>>
>>>> I am exploring robust portfolio optimization.  I have historical daily
>>>> data for 20 stocks over 2 year period.  i'd like to simulate 1,000 datasets
>>>> of 1 year each that have autocorrelation and cross-correlation properties
>>>> similar to those of the historical data.  Then I'd like to find allocation
>>>> that maximizes minimum risk-adjusted return over 1,000 datasets.  All
>>>> suggestions are appreciated!


From station.ning at gmail.com  Tue Jul  1 16:46:07 2008
From: station.ning at gmail.com (ning zhang)
Date: Tue, 1 Jul 2008 15:46:07 +0100
Subject: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
In-Reply-To: <486A4170.3070500@braverock.com>
References: <555700.40036.bm@omp220.mail.ukl.yahoo.com>
	<4869FC68.8030507@burns-stat.com>
	<9519fd9f0807010658h142e1178r886d2d8560016f79@mail.gmail.com>
	<486A4170.3070500@braverock.com>
Message-ID: <9519fd9f0807010746t41089ad8ybcf4fad3263f6cf8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080701/8dbb78a3/attachment.pl>

From yuri.volchik at gmail.com  Tue Jul  1 17:27:07 2008
From: yuri.volchik at gmail.com (Yuri Volchik)
Date: Tue, 1 Jul 2008 08:27:07 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Error when trying to load fOptions
	package
Message-ID: <18218710.post@talk.nabble.com>


Hi,

I get the following error when trying to load fOptions package:

Rmetrics Package fCalendar (262.73) loaded.
Rmetrics Package fSeries (270.73) loaded.
Rmetrics Package fImport (270.73) loaded.
Rmetrics Package fBasics (270.73) loaded.

Package fOptions (260.72) loaded.
Rmetrics - Basics of Option Valuation
Rmetrics, (C) 1999-2007, Diethelm Wuertz, GPL

Error : package 'fImport' does not have a name space
Error in as.environment(pos) : 
  no item called "newtable" on the search list



Version info:
$platform
[1] "i386-pc-mingw32"

$arch
[1] "i386"

$os
[1] "mingw32"

$system
[1] "i386, mingw32"

$status
[1] ""

$major
[1] "2"

$minor
[1] "6.2"

$year
[1] "2008"

$month
[1] "02"

$day
[1] "08"

$`svn rev`
[1] "44383"

$language
[1] "R"

$version.string
[1] "R version 2.6.2 (2008-02-08)"



-- 
View this message in context: http://www.nabble.com/Error-when-trying-to-load-fOptions-package-tp18218710p18218710.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From chalabi at phys.ethz.ch  Wed Jul  2 09:15:35 2008
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Wed, 2 Jul 2008 09:15:35 +0200
Subject: [R-SIG-Finance] [R-sig-finance] Error when trying to load
 fOptions package
In-Reply-To: <18218710.post@talk.nabble.com>
References: <18218710.post@talk.nabble.com>
Message-ID: <20080702091535.52ec38fd@mimi>

>>>> "YV" == Yuri Volchik <yuri.volchik at gmail.com>
>>>> on Tue, 1 Jul 2008 08:27:07 -0700 (PDT)

   YV> Rmetrics Package fCalendar (262.73) loaded.
   YV> Rmetrics Package fSeries (270.73) loaded.
   YV> Rmetrics Package fImport (270.73) loaded.
   YV> Rmetrics Package fBasics (270.73) loaded.

Dear Yuri,

you are mixing packages from CRAN and R-Forge which are
development packages.

I am currently adding name space to all Rmetrics packages and some of
them are available at R-Forge. So do not be surprised if you have problem
when mixing different versions with development packages.

regards,
Yohan

-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch
www.rmetrics.org

NOTE:
Rmetrics Workshop: http://www.rmetrics.org/meielisalp.htm
June 29th - July 3rd Meielisalp, Lake Thune, Switzerland


From pierre8r-list at yahoo.fr  Wed Jul  2 20:48:14 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Wed, 2 Jul 2008 18:48:14 +0000 (GMT)
Subject: [R-SIG-Finance] (no subject)
Message-ID: <460582.77721.qm@web28107.mail.ukl.yahoo.com>

Helllo,

My script creates an xts object.


First question:
I would like to compute for each Open Close from this time-series, the percentage change.
Is there a function that realizes this?

Second question:
I wish also to calculate the percentage change between two consecutive Close.
Is there a function that realizes this?

Thank you,

Pierre8r


      ____________________________________________________
intelligente http://mail.yahoo.fr


From pierre8r-list at yahoo.fr  Wed Jul  2 20:56:25 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Wed, 2 Jul 2008 18:56:25 +0000 (GMT)
Subject: [R-SIG-Finance] How to compute percentage change of xts ?
Message-ID: <469053.17661.qm@web28108.mail.ukl.yahoo.com>

Helllo,

Sorry, I repost my e-mail because I forgot the title.

My script creates an xts object.


First question:
I would like to compute for each Open Close from this time-series, the percentage change.
Is there a function that realizes this?

Second question:
I wish also to calculate the percentage change between two consecutive Close.
Is there a function that realizes this?

Thank you,

Pierre8r



      ____________________________________________________________
ente http://mail.yahoo.fr


From markleeds at verizon.net  Wed Jul  2 21:16:08 2008
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 02 Jul 2008 14:16:08 -0500 (CDT)
Subject: [R-SIG-Finance] How to compute percentage change of xts ?
Message-ID: <14761615.8100831215026168566.JavaMail.javamailuser@localhost>

  hi: below is a function that takes a series of numbers ( assumed to be 
prices ) and calculates the associated series of returns.
you have to either call it 4 times for open, high,low and close  or 
modify it to deal with multiple series simultaneously. I imagine there 
have to
be R  packages that do below in a much fancier way such as you're asking 
about but below works in the simple case where you just have
a series of prices ( numeric vectors. not xts objects ).

#==============================================
# function calculates arithmetic cumulative returns given a vector of 
prices

calcreturns <-
function(prices)
{
     P1 <- prices[-length(prices)]
     P2 <- prices[-1]
     returns <- P2 / P1 - 1
     c(0,returns)
}

 
#=========================================================================

On Wed, Jul 2, 2008 at  2:56 PM, pierre8r-list at yahoo.fr wrote:

> Helllo,
>
> Sorry, I repost my e-mail because I forgot the title.
>
> My script creates an xts object.
>
>
> First question:
> I would like to compute for each Open Close from this time-series, the 
> percentage change.
> Is there a function that realizes this?
>
> Second question:
> I wish also to calculate the percentage change between two consecutive 
> Close.
> Is there a function that realizes this?
>
> Thank you,
>
> Pierre8r
>
>
>
>       ____________________________________________________________
> ente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From jorge.nieves at moorecap.com  Wed Jul  2 21:24:34 2008
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Wed, 2 Jul 2008 15:24:34 -0400
Subject: [R-SIG-Finance] How to compute percentage change of xts ?
In-Reply-To: <469053.17661.qm@web28108.mail.ukl.yahoo.com>
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF14546E@NYC-XCH3.win.moorecap.com>

Take a look at 
Return.calculate,Return.cumulative, and Return.annualized in the
PerformnaceAnalytics package.

http://cran.r-project.org/web/packages/PerformanceAnalytics/PerformanceA
nalytics.pdf

 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
pierre8r-list at yahoo.fr
Sent: Wednesday, July 02, 2008 02:56 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] How to compute percentage change of xts ?

Helllo,

Sorry, I repost my e-mail because I forgot the title.

My script creates an xts object.


First question:
I would like to compute for each Open Close from this time-series, the
percentage change.
Is there a function that realizes this?

Second question:
I wish also to calculate the percentage change between two consecutive
Close.
Is there a function that realizes this?

Thank you,

Pierre8r



      ____________________________________________________________
ente http://mail.yahoo.fr

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From ggrothendieck at gmail.com  Wed Jul  2 22:06:50 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 2 Jul 2008 16:06:50 -0400
Subject: [R-SIG-Finance] How to compute percentage change of xts ?
In-Reply-To: <469053.17661.qm@web28108.mail.ukl.yahoo.com>
References: <469053.17661.qm@web28108.mail.ukl.yahoo.com>
Message-ID: <971536df0807021306g50036686k6836377ea054b2b5@mail.gmail.com>

Assuming Open and Close are in columns 1 and 4:

100 * x[, 4] / x[, 1] - 100

logx <- log(x)
100 * (logx[, 4] - logx[, 1]) # common approximation

100 * diff(x[, c(1,4)], arith = FALSE) - 100

100 * diff(log(x[, c(1,4)])) # common approximation


On Wed, Jul 2, 2008 at 2:56 PM,  <pierre8r-list at yahoo.fr> wrote:
> Helllo,
>
> Sorry, I repost my e-mail because I forgot the title.
>
> My script creates an xts object.
>
>
> First question:
> I would like to compute for each Open Close from this time-series, the percentage change.
> Is there a function that realizes this?
>
> Second question:
> I wish also to calculate the percentage change between two consecutive Close.
> Is there a function that realizes this?
>
> Thank you,
>
> Pierre8r
>
>
>
>      ____________________________________________________________
> ente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Thu Jul  3 01:11:50 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 2 Jul 2008 18:11:50 -0500
Subject: [R-SIG-Finance] How to compute percentage change of xts ?
In-Reply-To: <469053.17661.qm@web28108.mail.ukl.yahoo.com>
References: <469053.17661.qm@web28108.mail.ukl.yahoo.com>
Message-ID: <e8e755250807021611v303c4b0bk67e20256c3dd581@mail.gmail.com>

In quantmod:

OpCl will get you the Open to Close

and ClCl will do the same for Close to Close

They are wrappers to the Delt function, which allow for both
arithmetic and log returns.

There is also periodReturn in quantmod to provide the ClCl
functionality as well.

HTH
Jeff

On Wed, Jul 2, 2008 at 1:56 PM,  <pierre8r-list at yahoo.fr> wrote:
> Helllo,
>
> Sorry, I repost my e-mail because I forgot the title.
>
> My script creates an xts object.
>
>
> First question:
> I would like to compute for each Open Close from this time-series, the percentage change.
> Is there a function that realizes this?
>
> Second question:
> I wish also to calculate the percentage change between two consecutive Close.
> Is there a function that realizes this?
>
> Thank you,
>
> Pierre8r
>
>
>
>      ____________________________________________________________
> ente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From jeff.a.ryan at gmail.com  Thu Jul  3 01:14:40 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 2 Jul 2008 18:14:40 -0500
Subject: [R-SIG-Finance] How to compute percentage change of xts ?
In-Reply-To: <e8e755250807021611v303c4b0bk67e20256c3dd581@mail.gmail.com>
References: <469053.17661.qm@web28108.mail.ukl.yahoo.com>
	<e8e755250807021611v303c4b0bk67e20256c3dd581@mail.gmail.com>
Message-ID: <e8e755250807021614m1853475i9bac246b88f85788@mail.gmail.com>

One note on the previous reply -- the OpCl functions expect the column
names to contain 'Open' and 'Close' somewhere in the name.

There are also many other function to do similar calculations, see ?Op

Jeff

On Wed, Jul 2, 2008 at 6:11 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> In quantmod:
>
> OpCl will get you the Open to Close
>
> and ClCl will do the same for Close to Close
>
> They are wrappers to the Delt function, which allow for both
> arithmetic and log returns.
>
> There is also periodReturn in quantmod to provide the ClCl
> functionality as well.
>
> HTH
> Jeff
>
> On Wed, Jul 2, 2008 at 1:56 PM,  <pierre8r-list at yahoo.fr> wrote:
>> Helllo,
>>
>> Sorry, I repost my e-mail because I forgot the title.
>>
>> My script creates an xts object.
>>
>>
>> First question:
>> I would like to compute for each Open Close from this time-series, the percentage change.
>> Is there a function that realizes this?
>>
>> Second question:
>> I wish also to calculate the percentage change between two consecutive Close.
>> Is there a function that realizes this?
>>
>> Thank you,
>>
>> Pierre8r
>>
>>
>>
>>      ____________________________________________________________
>> ente http://mail.yahoo.fr
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From pierre8r-gmane at yahoo.fr  Thu Jul  3 18:35:16 2008
From: pierre8r-gmane at yahoo.fr (Pierre8r)
Date: Thu, 3 Jul 2008 16:35:16 +0000 (UTC)
Subject: [R-SIG-Finance] How to compute percentage change of xts ?
References: <469053.17661.qm@web28108.mail.ukl.yahoo.com>
Message-ID: <loom.20080703T163242-153@post.gmane.org>

Thanks to all for the answers,

Pierre8r


From molyboga at gmail.com  Tue Jul  1 15:39:28 2008
From: molyboga at gmail.com (Marat Molyboga)
Date: Tue, 1 Jul 2008 08:39:28 -0500
Subject: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
In-Reply-To: <4869FC68.8030507@burns-stat.com>
References: <555700.40036.bm@omp220.mail.ukl.yahoo.com>
	<4869FC68.8030507@burns-stat.com>
Message-ID: <6c124f870807010639qbab2498x6d526f452399d3b7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080701/e15c388b/attachment.pl>

From fvtrade at libero.it  Wed Jul  2 10:58:19 2008
From: fvtrade at libero.it (fvtrade)
Date: Wed, 2 Jul 2008 01:58:19 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Multiplicative error model ?
In-Reply-To: <9587318.167231214316727716.JavaMail.root@calliope.stern.nyu.edu>
References: <c17037a10806240046r105f80e0g6a7ac5f772717412@mail.gmail.com>
	<9587318.167231214316727716.JavaMail.root@calliope.stern.nyu.edu>
Message-ID: <18233075.post@talk.nabble.com>


Good morning Sirs. I need a help about dynamo package installation.
I followed theese steps:
1- i choose a cram mirror
2- i downloaded the package
3- i choose to install it from the downloaded resident zip file
but after that when i try to charge dynamo, the system returns me an error
message cause "libgsl.dll" not found.

Where did i go wrong?  Tank you in advance for the answer

fvtrade


Christian Brownlees wrote:
> 
> 
> I'm actually working on package for R called dynamo which allows to
> estimate univaraite MEMs. 
> 
> The package is on CRAN but on windows there are some CRAN dependent
> linking
> problems which have to be solved.
> 
> However, I have a version of the package for windows that runs.
> 
> The packages is still under development but estimation, inference,
> prediction, simulation of the model
> are already there more or less.
> 
> Best,
> Christian
> 
>> Dear all:
>> 
>> Does anyone know whether the multiplicative error mode (MEM) is
>> implemented
>> in R or other languages? I tried to write the program but got
>> non-converging
>> problems. I checked the paper in :
>> www.core.ucl.ac.be/archives/CORE.ETRICSfiles/2005-06/gallo.pdf
>> But I found that the stuff there seems too complicated for me to handle.
>> Can anyone give a hint here? Thanks for help of any kind.
>> 
>> ShyhWeir
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Multiplicative-error-model---tp18085707p18233075.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From duenisch at stud.uni-frankfurt.de  Fri Jul  4 13:28:31 2008
From: duenisch at stud.uni-frankfurt.de (Eli Duenisch)
Date: Fri, 04 Jul 2008 13:28:31 +0200
Subject: [R-SIG-Finance] convert row-vector to column-vector/compose matrix
	from two vectors
Message-ID: <486E095F.9030104@stud.uni-frankfurt.de>

Hi,
i'm just a newbie and there are two problems i couldn't google:
1.) How to convert a row-vector to a column-vector (and vice versa)?
2.)How to compose a matrix from two vectors (that have the same length)?
Thank you for your patience and best regards,
Eli D.


From liuxiaofengas at gmail.com  Fri Jul  4 17:32:20 2008
From: liuxiaofengas at gmail.com (Liu Xiaofeng)
Date: Fri, 4 Jul 2008 08:32:20 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Multiplicative error model ?
In-Reply-To: <18233075.post@talk.nabble.com>
References: <c17037a10806240046r105f80e0g6a7ac5f772717412@mail.gmail.com>
	<9587318.167231214316727716.JavaMail.root@calliope.stern.nyu.edu>
	<18233075.post@talk.nabble.com>
Message-ID: <18281608.post@talk.nabble.com>


I had same problem. :(


fvtrade wrote:
> 
> Good morning Sirs. I need a help about dynamo package installation.
> I followed theese steps:
> 1- i choose a cram mirror
> 2- i downloaded the package
> 3- i choose to install it from the downloaded resident zip file
> but after that when i try to charge dynamo, the system returns me an error
> message cause "libgsl.dll" not found.
> 
> Where did i go wrong?  Tank you in advance for the answer
> 
> fvtrade
> 
> 
> Christian Brownlees wrote:
>> 
>> 
>> I'm actually working on package for R called dynamo which allows to
>> estimate univaraite MEMs. 
>> 
>> The package is on CRAN but on windows there are some CRAN dependent
>> linking
>> problems which have to be solved.
>> 
>> However, I have a version of the package for windows that runs.
>> 
>> The packages is still under development but estimation, inference,
>> prediction, simulation of the model
>> are already there more or less.
>> 
>> Best,
>> Christian
>> 
>>> Dear all:
>>> 
>>> Does anyone know whether the multiplicative error mode (MEM) is
>>> implemented
>>> in R or other languages? I tried to write the program but got
>>> non-converging
>>> problems. I checked the paper in :
>>> www.core.ucl.ac.be/archives/CORE.ETRICSfiles/2005-06/gallo.pdf
>>> But I found that the stuff there seems too complicated for me to handle.
>>> Can anyone give a hint here? Thanks for help of any kind.
>>> 
>>> ShyhWeir
>>> 
>>> 	[[alternative HTML version deleted]]
>>> 
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>> 
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>> 
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/Multiplicative-error-model---tp18085707p18281608.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ryan.sheftel at malbecpartners.com  Sat Jul  5 22:20:20 2008
From: ryan.sheftel at malbecpartners.com (ryan.sheftel at malbecpartners.com)
Date: Sat, 5 Jul 2008 16:20:20 -0400
Subject: [R-SIG-Finance] convert row-vector to column-vector/compose
 matrix	from two vectors
In-Reply-To: <486E095F.9030104@stud.uni-frankfurt.de>
Message-ID: <OF56673BAD.99CE6105-ON8525747D.006F451A-8525747D.006FBBAA@malbecpartners.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080705/bccea092/attachment.pl>

From el_eli at gmx.de  Sun Jul  6 22:57:01 2008
From: el_eli at gmx.de (el_eli at gmx.de)
Date: Sun, 06 Jul 2008 22:57:01 +0200
Subject: [R-SIG-Finance] Symbolic computation in R
Message-ID: <4871319D.4000107@gmx.de>

Hi,

I'm a R-newbie and not very familiar with the concepts of numeric and 
symbolic programming environments.
As an undergraduate student i have to compute a lot of symbolic stuff 
like partial derivatives or integrals.
For that purpose Maple or Mathematica are quite good tools, but i want 
to stick to R.
The following functions seem to be the ones to get symbolic derivatives:
D (expr, name)
deriv(expr, ...)
deriv3(expr, ...)
But i could'nt find any other functions for symbolic computation in R.
So here are my questions:

Is there a function to perform a symbolic integration?

Is there a way to solve an expresion like 2*x=y for x symbolically?

Best,

Eli


From ggrothendieck at gmail.com  Sun Jul  6 23:08:29 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 6 Jul 2008 17:08:29 -0400
Subject: [R-SIG-Finance] Symbolic computation in R
In-Reply-To: <4871319D.4000107@gmx.de>
References: <4871319D.4000107@gmx.de>
Message-ID: <971536df0807061408h46c31cd3w6b52ac95a323204d@mail.gmail.com>

The Ryacas package provides an interface to the yacas symbolic
algebra system.  yacas is not very sophisticated when it comes to
integration but for the very simplest of cases it should be ok:
For more info see home page at
http://ryacas.googlecode.com

> library(Ryacas)
Loading required package: XML
> x <- Sym("x")
> Integrate(x*x, x)
[1] "Starting Yacas!"
expression(x^3/3)
> y <- Sym("y")
> Solve(2*y == x, x)
expression(list(x == 2 * y))
> detach()
Thank you for using yacas


On Sun, Jul 6, 2008 at 4:57 PM, el_eli at gmx.de <el_eli at gmx.de> wrote:
> Hi,
>
> I'm a R-newbie and not very familiar with the concepts of numeric and
> symbolic programming environments.
> As an undergraduate student i have to compute a lot of symbolic stuff like
> partial derivatives or integrals.
> For that purpose Maple or Mathematica are quite good tools, but i want to
> stick to R.
> The following functions seem to be the ones to get symbolic derivatives:
> D (expr, name)
> deriv(expr, ...)
> deriv3(expr, ...)
> But i could'nt find any other functions for symbolic computation in R.
> So here are my questions:
>
> Is there a function to perform a symbolic integration?
>
> Is there a way to solve an expresion like 2*x=y for x symbolically?
>
> Best,
>
> Eli
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From binabina at bellsouth.net  Mon Jul  7 02:51:32 2008
From: binabina at bellsouth.net (bina)
Date: Sun, 06 Jul 2008 20:51:32 -0400
Subject: [R-SIG-Finance] yahooSeries - how to access the date field?
Message-ID: <48716894.8050802@bellsouth.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080706/a4721526/attachment.pl>

From m_olshansky at yahoo.com  Mon Jul  7 03:12:24 2008
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Sun, 6 Jul 2008 18:12:24 -0700 (PDT)
Subject: [R-SIG-Finance] yahooSeries - how to access the date field?
In-Reply-To: <48716894.8050802@bellsouth.net>
Message-ID: <984773.58946.qm@web32202.mail.mud.yahoo.com>

Try 
rownames(zdata)


--- On Mon, 7/7/08, bina <binabina at bellsouth.net> wrote:

> From: bina <binabina at bellsouth.net>
> Subject: [R-SIG-Finance] yahooSeries - how to access the date field?
> To: r-sig-finance at stat.math.ethz.ch
> Received: Monday, 7 July, 2008, 10:51 AM
> hello, extracting data sets from the yahooSeries function,
> see below.  I 
> return a data frame successfully, however how do i access
> the date 
> field.  When viewing the structure i don't see the
> field, but using head 
> i do see the date field. 
> 
> Can someone provide me some example plot code to plot the
> data in a time 
> series format, with the dates going across the X axis
> labels? 
> -zubin
> 
> 
> 
> zdata <-     yahooSeries(symbols =
> c("^NDX","UYG"), from =
> '2006-01-01', 
> to = '2008-07-06', quote = c("Open",
> "High", "Low", "Close",      
>   
>      "Volume"), aggregation = c("d"),
> returnClass = c("data.frame"))
> 
> str(zdata)
> 'data.frame':    653 obs. of  10 variables:
>  $ X.NDX.Open  : num  1654 1684 1698 1718 1734 ...
>  $ X.NDX.High  : num  1687 1697 1708 1736 1745 ...
>  $ X.NDX.Low   : num  1634 1682 1697 1711 1730 ...
>  $ X.NDX.Close : num  1680 1696 1705 1735 1742 ...
>  $ X.NDX.Volume: num  2.00e+09 1.88e+09 1.89e+09 2.23e+09
> 1.95e+09 ...
>  $ UYG.Open    : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ UYG.High    : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ UYG.Low     : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ UYG.Close   : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ UYG.Volume  : num  NA NA NA NA NA NA NA NA NA NA ...
> 
> head(zdata)
>            X.NDX.Open X.NDX.High X.NDX.Low
> 2006-01-03    1654.39    1686.59   1633.62
> 2006-01-04    1684.05    1696.83   1681.85
> 2006-01-05    1697.80    1707.71   1697.33
> 2006-01-06    1718.17    1735.50   1710.86
> 2006-01-09    1734.50    1745.12   1729.60
> 2006-01-10    1732.25    1744.47   1730.25
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Mon Jul  7 03:40:58 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sun, 6 Jul 2008 20:40:58 -0500
Subject: [R-SIG-Finance] yahooSeries - how to access the date field?
In-Reply-To: <48716894.8050802@bellsouth.net>
References: <48716894.8050802@bellsouth.net>
Message-ID: <e8e755250807061840v7ea5e971i6c084d7c0dc43cf2@mail.gmail.com>

An alternate method that offers a bit of simplicity for plotting -- as
well as the use of true time-indexing:

library(quantmod)

getSymbols("^NDX;UYG", from='2006-01-01')

# will load 2 objects - NDX and UYG into your environment

# then you can plot any component by selecting the column, or
alternately the whole OHLCV series:

plot(NDX) #plots the Open, and a warning :)
plot(as.zoo(NDX)) # plots O,H,L,C,V


plot(Cl(UYG))  # plots the Close


# or

barChart(NDX)
candleChart(UYG)

See http://www.quantmod.com/examples for some additional quantmod
tools that may be of use.

HTH
Jeff

On Sun, Jul 6, 2008 at 7:51 PM, bina <binabina at bellsouth.net> wrote:
> hello, extracting data sets from the yahooSeries function, see below.  I
> return a data frame successfully, however how do i access the date
> field.  When viewing the structure i don't see the field, but using head
> i do see the date field.
>
> Can someone provide me some example plot code to plot the data in a time
> series format, with the dates going across the X axis labels?
> -zubin
>
>
>
> zdata <-     yahooSeries(symbols = c("^NDX","UYG"), from = '2006-01-01',
> to = '2008-07-06', quote = c("Open", "High", "Low", "Close",
>     "Volume"), aggregation = c("d"), returnClass = c("data.frame"))
>
> str(zdata)
> 'data.frame':    653 obs. of  10 variables:
>  $ X.NDX.Open  : num  1654 1684 1698 1718 1734 ...
>  $ X.NDX.High  : num  1687 1697 1708 1736 1745 ...
>  $ X.NDX.Low   : num  1634 1682 1697 1711 1730 ...
>  $ X.NDX.Close : num  1680 1696 1705 1735 1742 ...
>  $ X.NDX.Volume: num  2.00e+09 1.88e+09 1.89e+09 2.23e+09 1.95e+09 ...
>  $ UYG.Open    : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ UYG.High    : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ UYG.Low     : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ UYG.Close   : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ UYG.Volume  : num  NA NA NA NA NA NA NA NA NA NA ...
>
> head(zdata)
>           X.NDX.Open X.NDX.High X.NDX.Low
> 2006-01-03    1654.39    1686.59   1633.62
> 2006-01-04    1684.05    1696.83   1681.85
> 2006-01-05    1697.80    1707.71   1697.33
> 2006-01-06    1718.17    1735.50   1710.86
> 2006-01-09    1734.50    1745.12   1729.60
> 2006-01-10    1732.25    1744.47   1730.25
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From pierre8r-gmane at yahoo.fr  Mon Jul  7 21:36:52 2008
From: pierre8r-gmane at yahoo.fr (Pierre8r)
Date: Mon, 7 Jul 2008 19:36:52 +0000 (UTC)
Subject: [R-SIG-Finance] quantmod addTA() How to compute his own indicator ?
Message-ID: <loom.20080707T192258-319@post.gmane.org>

Hello,

How to compute his own indicator ?
I want to use addTA() from quantmod with my own indicator.
How to compute my own indicator ?

Please give me some full R code sample.

The sample code can be some Moving Average sample code or other I don't care.
For example Close + 1 
I just need something I can start from.

Thanks,

Pierre8r

Here some Java code to compute Moving Average.
But the sample code can be simpler.

Java code :

    public double calculate() {
        int endBar = qh.size() - 1;
        int startBar = endBar - length;
        double sma = 0;

        for (int bar = startBar; bar <= endBar; bar++) {
            sma += qh.getPriceBar(bar).getClose();
        }

        value = sma / (endBar - startBar + 1);
        return value;
    }


From jeff.a.ryan at gmail.com  Mon Jul  7 22:35:31 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 7 Jul 2008 15:35:31 -0500
Subject: [R-SIG-Finance] quantmod addTA() How to compute his own
	indicator ?
In-Reply-To: <loom.20080707T192258-319@post.gmane.org>
References: <loom.20080707T192258-319@post.gmane.org>
Message-ID: <e8e755250807071335o493f8c5ahedcd461c699ac13@mail.gmail.com>

Bonjour Pierre,

The site now has some examples with respect to the new addTA and newTA
functionality

http://www.quantmod.com/examples/charting/#technicals

Apart from that the basic idea is that addTA will take the result of a
function call, and apply it to the active chart, whereas newTA will
let you create an indicator much like the built-in ones.

A quick example _not_ on the site using Guppy Multiple Moving Averages
(soon to be added to TTR or quantmod):

library(quantmod)
library(TTR) # for the EMA function - make sure to use the TTR version
and not the fTrading version, as TTR prepends NAs

getSymbols("YHOO")

 `GMMA` <- function(x, short=c(3,5,8,10,12,15),
long=c(30,35,40,45,50,60), type='EMA') {
    gmma <- sapply(c(short,long), function(g) do.call(type, list(x,n=g)))
    colnames(gmma) <- c(paste('short lag',short),paste('long lag',long))
    gmma
  }

  addGMMA <- newTA(GMMA, preFUN=Cl,
legend.name='GMMA',col=c(rep(c('green','red'), each=6)))

  barChart(YHOO)
  addGMMA()

  # alternately you can use addTA for one-off chart additions:

 addTA(GMMA(Cl(YHOO)),col=c(rep(c('blue','white'),each=6)))

In future additions there will be more control over titles, though for
the most part this works well even now.

addTA requires either a vector or matrix with NROW == NROW of the
charted series, OR alternately it can be an
xts object within the range of the original series.  Most graphical
parameters are handled by both functions, as well as the ability to
automatically recycle args and plot multiple lines.  The "on" argument
lets you add an overlay to the main window by specifying "on=1".
Eventually all windows will allow for this.

# add an overlay
addTA(EMA(Cl(YHOO)), on=1, col='yellow')


Hope that helps,
Jeff



On Mon, Jul 7, 2008 at 2:36 PM, Pierre8r <pierre8r-gmane at yahoo.fr> wrote:
> Hello,
>
> How to compute his own indicator ?
> I want to use addTA() from quantmod with my own indicator.
> How to compute my own indicator ?
>
> Please give me some full R code sample.
>
> The sample code can be some Moving Average sample code or other I don't care.
> For example Close + 1
> I just need something I can start from.
>
> Thanks,
>
> Pierre8r
>
> Here some Java code to compute Moving Average.
> But the sample code can be simpler.
>
> Java code :
>
>    public double calculate() {
>        int endBar = qh.size() - 1;
>        int startBar = endBar - length;
>        double sma = 0;
>
>        for (int bar = startBar; bar <= endBar; bar++) {
>            sma += qh.getPriceBar(bar).getClose();
>        }
>
>        value = sma / (endBar - startBar + 1);
>        return value;
>    }
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From DAvery at marketingleverage.com  Tue Jul  8 13:25:33 2008
From: DAvery at marketingleverage.com (Dan Avery)
Date: Tue, 8 Jul 2008 07:25:33 -0400
Subject: [R-SIG-Finance] Market stats data sources
Message-ID: <E390E12F18C406459525A2C25BA9FC67B7F6@SBS-MLI.MLI.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080708/0865be3b/attachment.pl>

From brian at braverock.com  Tue Jul  8 14:15:04 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 08 Jul 2008 07:15:04 -0500
Subject: [R-SIG-Finance] Market stats data sources
In-Reply-To: <E390E12F18C406459525A2C25BA9FC67B7F6@SBS-MLI.MLI.local>
References: <E390E12F18C406459525A2C25BA9FC67B7F6@SBS-MLI.MLI.local>
Message-ID: <48735A48.2000800@braverock.com>

Dan Avery wrote:
> I'm interested in finding historical data on market stats - new highs, new lows, advances,declines, unchanged, advancing volume, declining volume and unchanged for the NYSE, AMEX and NASDAQ. This is the data in the Market Diary of the WSJ. Anyone know of a souce that could be accessed using something like:
> 
> library(quantmod)
> 
> getSymbols(c("list of symbols for data above"), from='1970-01-01')

Normally I would calculate the stats you are talking about from the 
price history.  Both 'quantmod' and 'PerformanceAnalytics' have a number 
of functions to pull out trends, runs, and drawdown periods from a time 
series of prices or returns.

Regards,

   - Brian


From jeff.a.ryan at gmail.com  Tue Jul  8 16:37:30 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 8 Jul 2008 09:37:30 -0500
Subject: [R-SIG-Finance] quantmod addTA() How to compute his own
	indicator ?
In-Reply-To: <e8e755250807071335o493f8c5ahedcd461c699ac13@mail.gmail.com>
References: <loom.20080707T192258-319@post.gmane.org>
	<e8e755250807071335o493f8c5ahedcd461c699ac13@mail.gmail.com>
Message-ID: <e8e755250807080737s4e614682jb22c0dd35f00a5d4@mail.gmail.com>

One adjustment to my reply.

Version 0.3-6 'newTA' makes use of an argument called 'tFUN' instead
of 'preFUN'.  The yet-to-be-released version has some additional args
and minor naming changes to make it more flexible. This should be out
in a few weeks.

So for 0.3-6 this should work...

 addGMMA <- newTA(GMMA, tFUN=Cl,
legend.name='GMMA',col=c(rep(c('green','red'), each=6)))

Jeff

On Mon, Jul 7, 2008 at 3:35 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Bonjour Pierre,
>
> The site now has some examples with respect to the new addTA and newTA
> functionality
>
> http://www.quantmod.com/examples/charting/#technicals
>
> Apart from that the basic idea is that addTA will take the result of a
> function call, and apply it to the active chart, whereas newTA will
> let you create an indicator much like the built-in ones.
>
> A quick example _not_ on the site using Guppy Multiple Moving Averages
> (soon to be added to TTR or quantmod):
>
> library(quantmod)
> library(TTR) # for the EMA function - make sure to use the TTR version
> and not the fTrading version, as TTR prepends NAs
>
> getSymbols("YHOO")
>
>  `GMMA` <- function(x, short=c(3,5,8,10,12,15),
> long=c(30,35,40,45,50,60), type='EMA') {
>    gmma <- sapply(c(short,long), function(g) do.call(type, list(x,n=g)))
>    colnames(gmma) <- c(paste('short lag',short),paste('long lag',long))
>    gmma
>  }
>
>  addGMMA <- newTA(GMMA, preFUN=Cl,
> legend.name='GMMA',col=c(rep(c('green','red'), each=6)))
>
>  barChart(YHOO)
>  addGMMA()
>
>  # alternately you can use addTA for one-off chart additions:
>
>  addTA(GMMA(Cl(YHOO)),col=c(rep(c('blue','white'),each=6)))
>
> In future additions there will be more control over titles, though for
> the most part this works well even now.
>
> addTA requires either a vector or matrix with NROW == NROW of the
> charted series, OR alternately it can be an
> xts object within the range of the original series.  Most graphical
> parameters are handled by both functions, as well as the ability to
> automatically recycle args and plot multiple lines.  The "on" argument
> lets you add an overlay to the main window by specifying "on=1".
> Eventually all windows will allow for this.
>
> # add an overlay
> addTA(EMA(Cl(YHOO)), on=1, col='yellow')
>
>
> Hope that helps,
> Jeff
>
>
>
> On Mon, Jul 7, 2008 at 2:36 PM, Pierre8r <pierre8r-gmane at yahoo.fr> wrote:
>> Hello,
>>
>> How to compute his own indicator ?
>> I want to use addTA() from quantmod with my own indicator.
>> How to compute my own indicator ?
>>
>> Please give me some full R code sample.
>>
>> The sample code can be some Moving Average sample code or other I don't care.
>> For example Close + 1
>> I just need something I can start from.
>>
>> Thanks,
>>
>> Pierre8r
>>
>> Here some Java code to compute Moving Average.
>> But the sample code can be simpler.
>>
>> Java code :
>>
>>    public double calculate() {
>>        int endBar = qh.size() - 1;
>>        int startBar = endBar - length;
>>        double sma = 0;
>>
>>        for (int bar = startBar; bar <= endBar; bar++) {
>>            sma += qh.getPriceBar(bar).getClose();
>>        }
>>
>>        value = sma / (endBar - startBar + 1);
>>        return value;
>>    }
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From Trevor.L.Davis at frb.gov  Tue Jul  8 22:39:14 2008
From: Trevor.L.Davis at frb.gov (Trevor Davis)
Date: Tue, 08 Jul 2008 16:39:14 -0400
Subject: [R-SIG-Finance] Symbolic computation in R
In-Reply-To: <4871319D.4000107@gmx.de>
References: <4871319D.4000107@gmx.de>
Message-ID: <g50j9n$ao0$1@ger.gmane.org>

Ryacas is the way to go if you need to stay in R, however R is 
relatively a much stronger numeric programming environment than symbolic 
programming environment.  For your symbolic math needs you might also 
want to check out SAGE which brings together many free open-source 
mathematics programs.  I find it more natural to do symbolic 
calculations in SAGE (just as I find it more natural to do statistics in 
  R).  Technically SAGE even comes with R although I prefer to maintain 
a separate additional installation of R.

Trevor Davis
Federal Reserve Board of Governors

el_eli at gmx.de wrote:
> Hi,
> 
> I'm a R-newbie and not very familiar with the concepts of numeric and 
> symbolic programming environments.
> As an undergraduate student i have to compute a lot of symbolic stuff 
> like partial derivatives or integrals.
> For that purpose Maple or Mathematica are quite good tools, but i want 
> to stick to R.
> The following functions seem to be the ones to get symbolic derivatives:
> D (expr, name)
> deriv(expr, ...)
> deriv3(expr, ...)
> But i could'nt find any other functions for symbolic computation in R.
> So here are my questions:
> 
> Is there a function to perform a symbolic integration?
> 
> Is there a way to solve an expresion like 2*x=y for x symbolically?
> 
> Best,
> 
> Eli
>


From eowiesny at allstontrading.com  Tue Jul  8 23:50:11 2008
From: eowiesny at allstontrading.com (Eric Owiesny)
Date: Tue, 08 Jul 2008 16:50:11 -0500
Subject: [R-SIG-Finance] RBloomberg Best Bid Higher than Best Ask
Message-ID: <4873E113.8080003@allstontrading.com>

Hello,

This is more a question about Bloomberg than about RBloomberg, but I've 
been unable to get a good answer out of the Bloomberg people so I was 
wondering if anyone else had run into similar problems.  I've been 
gathering intraday data on several Equities from Bloomberg using 
RBloomberg with the fields of BEST_BID and BEST_ASK, barfields of OPEN 
and LAST_PRICE, and barsize of 1 minute.    I saw mentioned in an 
earlier message on this list that BEST_BID and BEST_ASK are better than 
BID and ASK as fields and have generally found that to be true myself as 
well.  I've noticed that sometimes using these gives BEST_BID as being 
higher than BEST_ASK for both the open and last price.  For example, 
using the symbol ATPG the last price BEST_BID at 04/16/08 09:20:00 is 
31.05 while the BEST_ASK is 31.01.  I'm curious about the degree to 
which I can trust the Bloomberg data and whether anyone has seen a 
decent explanation for this.  I hope that this question is more about 
Bloomberg isn't a problem for this list.  If it is, I apologize.

Thanks,
Eric Owiesny

------------------------------------------------------------------------------------------
This message is for the named person(s) use only. It may contain confidential proprietary or legally privileged information. No confidentiality or privilege is waived or lost by any mistransmission. If you receive this message in error, please immediately delete it and all copies of it from your system, destroy any hard copies of it and notify the sender. You must not, directly or indirectly use, disclose, distribute, print, or copy any part of this message if you are not the intended recipient. Allston Trading LLC and its subsidiaries and affiliates each reserve the right to monitor all e-mail communications through its networks. Any views expressed in this message are those of the individual sender, except where the message states otherwise and the sender is authorized to state them to be the views of any such entity.


From niheaven at hotmail.com  Wed Jul  9 07:08:59 2008
From: niheaven at hotmail.com (Hsiao-nan Cheung)
Date: Wed, 9 Jul 2008 13:08:59 +0800
Subject: [R-SIG-Finance]  Get Status of Derivatives as Options and Futures
Message-ID: <BAY126-DS2F04439501FFC9A0581E6D9960@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080709/a870b248/attachment.pl>

From robert at sanctumfi.com  Wed Jul  9 13:09:59 2008
From: robert at sanctumfi.com (Robert Sams)
Date: Wed, 9 Jul 2008 12:09:59 +0100
Subject: [R-SIG-Finance] RBloomberg Best Bid Higher than Best Ask
References: <SANCTUMFISERVERPZrI000035d8@sanctumfi.com>
Message-ID: <SANCTUMFISERVERekVg00003705@sanctumfi.com>

Hi Eric,

You're right, this is question for Bloomberg, not RBloomberg. 

It is common to see data "errors" like locked markets, etc in Bloomberg
data, intraday and even daily historical. Series from futures exchanges
tend to be much cleaner than OTC data (bonds, swap tenors, etc). 
You can explore other data vendors (e.g, http://www.lim.com/ is
popular), but data errors are probably inevitable whatever you buy. If
your analysis is sensitive to these, you'll need to write some
data-error algorithms. This shouldn't be hard. Check out the NA handling
tools in Achim's and Gabor's wonderful zoo package. 

By the way, if you care to inform Bloomberg about a dodgy data point,
find it on the QRM screen; that way you can grab it and send it to them.
They will fix it, although I personally find it not worth the effort as
I automate the data cleaning on my end using R.

Cheers,
Robert

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Eric
Owiesny
Sent: 08 July 2008 22:50
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] RBloomberg Best Bid Higher than Best Ask

Hello,

This is more a question about Bloomberg than about RBloomberg, but I've
been unable to get a good answer out of the Bloomberg people so I was
wondering if anyone else had run into similar problems.  I've been
gathering intraday data on several Equities from Bloomberg using
RBloomberg with the fields of BEST_BID and BEST_ASK, barfields of OPEN 
and LAST_PRICE, and barsize of 1 minute.    I saw mentioned in an 
earlier message on this list that BEST_BID and BEST_ASK are better than
BID and ASK as fields and have generally found that to be true myself as
well.  I've noticed that sometimes using these gives BEST_BID as being
higher than BEST_ASK for both the open and last price.  For example,
using the symbol ATPG the last price BEST_BID at 04/16/08 09:20:00 is
31.05 while the BEST_ASK is 31.01.  I'm curious about the degree to
which I can trust the Bloomberg data and whether anyone has seen a
decent explanation for this.  I hope that this question is more about
Bloomberg isn't a problem for this list.  If it is, I apologize.

Thanks,
Eric Owiesny

------------------------------------------------------------------------
------------------
This message is for the named person(s) use only. It may contain
confidential proprietary or legally privileged information. No
confidentiality or privilege is waived or lost by any mistransmission.
If you receive this message in error, please immediately delete it and
all copies of it from your system, destroy any hard copies of it and
notify the sender. You must not, directly or indirectly use, disclose,
distribute, print, or copy any part of this message if you are not the
intended recipient. Allston Trading LLC and its subsidiaries and
affiliates each reserve the right to monitor all e-mail communications
through its networks. Any views expressed in this message are those of
the individual sender, except where the message states otherwise and the
sender is authorized to state them to be the views of any such entity.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From davidr at rhotrading.com  Wed Jul  9 15:14:59 2008
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 9 Jul 2008 08:14:59 -0500
Subject: [R-SIG-Finance] RBloomberg Best Bid Higher than Best Ask
In-Reply-To: <SANCTUMFISERVERekVg00003705@sanctumfi.com>
References: <SANCTUMFISERVERPZrI000035d8@sanctumfi.com>
	<SANCTUMFISERVERekVg00003705@sanctumfi.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE77011DFA14@rhopost.rhotrading.com>

My two cents worth: You may be able to improve the quality of your 
bid/ask data by restricting to a single source. With combined markets 
in equities that trade on several exchanges, there can be crossed 
markets, but almost always from different sources.
Try IBM UN Equity QRM vs IBM US Equity QRM to see the difference.

HTH,
-- David

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Robert
Sams
Sent: Wednesday, July 09, 2008 6:10 AM
To: Eric Owiesny; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] RBloomberg Best Bid Higher than Best Ask

Hi Eric,

You're right, this is question for Bloomberg, not RBloomberg. 

It is common to see data "errors" like locked markets, etc in Bloomberg
data, intraday and even daily historical. Series from futures exchanges
tend to be much cleaner than OTC data (bonds, swap tenors, etc). 
You can explore other data vendors (e.g, http://www.lim.com/ is
popular), but data errors are probably inevitable whatever you buy. If
your analysis is sensitive to these, you'll need to write some
data-error algorithms. This shouldn't be hard. Check out the NA handling
tools in Achim's and Gabor's wonderful zoo package. 

By the way, if you care to inform Bloomberg about a dodgy data point,
find it on the QRM screen; that way you can grab it and send it to them.
They will fix it, although I personally find it not worth the effort as
I automate the data cleaning on my end using R.

Cheers,
Robert

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Eric
Owiesny
Sent: 08 July 2008 22:50
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] RBloomberg Best Bid Higher than Best Ask

Hello,

This is more a question about Bloomberg than about RBloomberg, but I've
been unable to get a good answer out of the Bloomberg people so I was
wondering if anyone else had run into similar problems.  I've been
gathering intraday data on several Equities from Bloomberg using
RBloomberg with the fields of BEST_BID and BEST_ASK, barfields of OPEN 
and LAST_PRICE, and barsize of 1 minute.    I saw mentioned in an 
earlier message on this list that BEST_BID and BEST_ASK are better than
BID and ASK as fields and have generally found that to be true myself as
well.  I've noticed that sometimes using these gives BEST_BID as being
higher than BEST_ASK for both the open and last price.  For example,
using the symbol ATPG the last price BEST_BID at 04/16/08 09:20:00 is
31.05 while the BEST_ASK is 31.01.  I'm curious about the degree to
which I can trust the Bloomberg data and whether anyone has seen a
decent explanation for this.  I hope that this question is more about
Bloomberg isn't a problem for this list.  If it is, I apologize.

Thanks,
Eric Owiesny

------------------------------------------------------------------------
------------------
This message is for the named person(s) use only. It may contain
confidential proprietary or legally privileged information. No
confidentiality or privilege is waived or lost by any mistransmission.
If you receive this message in error, please immediately delete it and
all copies of it from your system, destroy any hard copies of it and
notify the sender. You must not, directly or indirectly use, disclose,
distribute, print, or copy any part of this message if you are not the
intended recipient. Allston Trading LLC and its subsidiaries and
affiliates each reserve the right to monitor all e-mail communications
through its networks. Any views expressed in this message are those of
the individual sender, except where the message states otherwise and the
sender is authorized to state them to be the views of any such entity.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From davidr at rhotrading.com  Wed Jul  9 15:32:28 2008
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 9 Jul 2008 08:32:28 -0500
Subject: [R-SIG-Finance] Get Status of Derivatives as Options and Futures
In-Reply-To: <BAY126-DS2F04439501FFC9A0581E6D9960@phx.gbl>
References: <BAY126-DS2F04439501FFC9A0581E6D9960@phx.gbl>
Message-ID: <F9F2A641C593D7408925574C05A7BE77011DFA23@rhopost.rhotrading.com>

My suggestion would be to go to the relevant exchange's website and see 
if you can get what you need there, at least as a first step. Then if 
you can't get what you want from there, look to free data sites (which 
have been discussed a few times here - look in the archives), and 
finally, you may have to pay money for the data you want.
HTH,
-- David


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Hsiao-nan Cheung
Sent: Wednesday, July 09, 2008 12:09 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Get Status of Derivatives as Options and Futures

Hi,

 

Is there any method to get some information about active traded derivatives
such as several options and futures? Since I??m doing a research about FX
futures, but I don't know how to get the data.

 

 
Hsiao-nan Cheung

 

----------------------------------------------------------------------------
-------

                      ??????????????????????????????????????

----------------------------------------------------------------------------
-------




	[[alternative HTML version deleted]]


From j at jtoll.com  Wed Jul  9 23:55:32 2008
From: j at jtoll.com (James)
Date: Wed, 9 Jul 2008 15:55:32 -0600
Subject: [R-SIG-Finance] Specifying data by date
Message-ID: <74D45BCE-E096-4334-A934-E1B97CE53EB3@jtoll.com>

Hi,

I have a dataframe with data from the yahooSeries function, I am  
trying to figure out if there is a way to calculate the standard  
deviation on a specific time period by simply entering a beginning  
and end date.  For example, could I calculate the standard deviation  
for the month of March 2007, by simply giving a beginning date and an  
end date?  Or better yet, the SD for March for the past 20 years.

Really, I guess I just need a way to coax the appropriate range of  
data from the dataframe into a vector so that I can perform the  
necessary calculation.  Any suggestions?  Thanks.

James


From markleeds at verizon.net  Thu Jul 10 00:00:11 2008
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 09 Jul 2008 17:00:11 -0500 (CDT)
Subject: [R-SIG-Finance] portfolio optimization
Message-ID: <17396679.8785751215640811803.JavaMail.javamailuser@localhost>

does anyone know of academic literature on optimization in finance where 
one has the probabilities of the assets being up or down rather than the
expected returns of the assets ? ( and also covariance matrix).  i think 
i  saw something in the past somewhere but i could be mistaken. thanks.


From josh.m.ulrich at gmail.com  Thu Jul 10 00:24:01 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Wed, 9 Jul 2008 17:24:01 -0500
Subject: [R-SIG-Finance] Specifying data by date
In-Reply-To: <74D45BCE-E096-4334-A934-E1B97CE53EB3@jtoll.com>
References: <74D45BCE-E096-4334-A934-E1B97CE53EB3@jtoll.com>
Message-ID: <8cca69990807091524o1c0a312s8719828cd984b904@mail.gmail.com>

Hi James,

The xts package was created with this problem in mind.  You can index
xts objects using POSIX-style character strings, with ranges separated
by either ":" or "/".  period.apply can give you the standard
deviation of a series by month.  The xts vignette describes all this -
and much more - in thorough detail.

require(fImport)
require(xts)

> df <- yahooSeries('QQQQ', returnClass=c("data.frame"))
trying URL 'http://chart.yahoo.com/table.csv?s=QQQQ&a=6&b=10&c=2007&d=6&e=09&f=2008&g=d&x=.csv'
Content type 'text/csv' length 200 bytes
opened URL
downloaded 12 Kb

> x <- as.xts(df)
> sd(x['2008-01','QQQQ.Close'])
QQQQ.Close
   2.20967
> sd(x['2008-01/2008-02','QQQQ.Close'])
QQQQ.Close
  2.110933
> period.apply(x$QQQQ.Close, INDEX=endpoints(x,'months'), FUN=function(x) sd(x))
                   x
2007-07-31 0.8553352
2007-08-31 0.8954604
2007-09-28 1.0446701
2007-10-31 0.8624363
2007-11-30 1.8838374
2007-12-31 0.8676462
2008-01-31 2.2096702
2008-02-29 0.6359294
2008-03-31 0.9582882
2008-04-30 1.0761549
2008-05-30 0.6399203
2008-06-30 1.3976623
2008-07-08 0.6155458
>

Best,
Josh

--
http://quantemplation.blogspot.com


On Wed, Jul 9, 2008 at 4:55 PM, James <j at jtoll.com> wrote:
> Hi,
>
> I have a dataframe with data from the yahooSeries function, I am trying to
> figure out if there is a way to calculate the standard deviation on a
> specific time period by simply entering a beginning and end date.  For
> example, could I calculate the standard deviation for the month of March
> 2007, by simply giving a beginning date and an end date?  Or better yet, the
> SD for March for the past 20 years.
>
> Really, I guess I just need a way to coax the appropriate range of data from
> the dataframe into a vector so that I can perform the necessary calculation.
>  Any suggestions?  Thanks.
>
> James
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From j at jtoll.com  Thu Jul 10 00:30:43 2008
From: j at jtoll.com (James)
Date: Wed, 9 Jul 2008 16:30:43 -0600
Subject: [R-SIG-Finance] Specifying data by date
In-Reply-To: <8cca69990807091524o1c0a312s8719828cd984b904@mail.gmail.com>
References: <74D45BCE-E096-4334-A934-E1B97CE53EB3@jtoll.com>
	<8cca69990807091524o1c0a312s8719828cd984b904@mail.gmail.com>
Message-ID: <4247E14D-98D5-4C31-9996-78A9E8748516@jtoll.com>

Josh,

Thanks for the help.  That's exactly what I was looking for.  Thanks  
again.

James



On Jul 9, 2008, at 4:24 PM, Josh Ulrich wrote:

> Hi James,
>
> The xts package was created with this problem in mind.  You can index
> xts objects using POSIX-style character strings, with ranges separated
> by either ":" or "/".  period.apply can give you the standard
> deviation of a series by month.  The xts vignette describes all this -
> and much more - in thorough detail.
>
> require(fImport)
> require(xts)
>
>> df <- yahooSeries('QQQQ', returnClass=c("data.frame"))
> trying URL 'http://chart.yahoo.com/table.csv? 
> s=QQQQ&a=6&b=10&c=2007&d=6&e=09&f=2008&g=d&x=.csv'
> Content type 'text/csv' length 200 bytes
> opened URL
> downloaded 12 Kb
>
>> x <- as.xts(df)
>> sd(x['2008-01','QQQQ.Close'])
> QQQQ.Close
>    2.20967
>> sd(x['2008-01/2008-02','QQQQ.Close'])
> QQQQ.Close
>   2.110933
>> period.apply(x$QQQQ.Close, INDEX=endpoints(x,'months'),  
>> FUN=function(x) sd(x))
>                    x
> 2007-07-31 0.8553352
> 2007-08-31 0.8954604
> 2007-09-28 1.0446701
> 2007-10-31 0.8624363
> 2007-11-30 1.8838374
> 2007-12-31 0.8676462
> 2008-01-31 2.2096702
> 2008-02-29 0.6359294
> 2008-03-31 0.9582882
> 2008-04-30 1.0761549
> 2008-05-30 0.6399203
> 2008-06-30 1.3976623
> 2008-07-08 0.6155458
>>
>
> Best,
> Josh
>
> --
> http://quantemplation.blogspot.com
>
>
> On Wed, Jul 9, 2008 at 4:55 PM, James <j at jtoll.com> wrote:
>> Hi,
>>
>> I have a dataframe with data from the yahooSeries function, I am  
>> trying to
>> figure out if there is a way to calculate the standard deviation on a
>> specific time period by simply entering a beginning and end date.   
>> For
>> example, could I calculate the standard deviation for the month of  
>> March
>> 2007, by simply giving a beginning date and an end date?  Or  
>> better yet, the
>> SD for March for the past 20 years.
>>
>> Really, I guess I just need a way to coax the appropriate range of  
>> data from
>> the dataframe into a vector so that I can perform the necessary  
>> calculation.
>>  Any suggestions?  Thanks.
>>
>> James


From kriskumar at earthlink.net  Thu Jul 10 02:42:22 2008
From: kriskumar at earthlink.net (kriskumar at earthlink.net)
Date: Thu, 10 Jul 2008 00:42:22 +0000
Subject: [R-SIG-Finance] portfolio optimization
Message-ID: <1463008249-1215650636-cardhu_decombobulator_blackberry.rim.net-128675317-@bxe190.bisx.prod.on.blackberry>

There is an interesting paper on optimization with sorts by Almgren and Chriss. This takes ordering information to compute the optimal portfolio. There was an earlier post here on this which you will find in the list archives.

Cheers
Krishna

------Original Message------
From: markleeds at verizon.net
Sender: 
To: r-sig-finance at stat.math.ethz.ch
Sent: Jul 9, 2008 18:00
Subject: [R-SIG-Finance] portfolio optimization

does anyone know of academic literature on optimization in finance where 
one has the probabilities of the assets being up or down rather than the
expected returns of the assets ? ( and also covariance matrix).  i think 
i  saw something in the past somewhere but i could be mistaken. thanks.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


----
"When I get a little money, I buy books and if any 
      is left, I buy food and clothes."  -- Erasmus

From Xiaochen.Sun at brunel.ac.uk  Thu Jul 10 10:51:58 2008
From: Xiaochen.Sun at brunel.ac.uk (Xiao Sun)
Date: Thu, 10 Jul 2008 09:51:58 +0100
Subject: [R-SIG-Finance] portfolio optimization
References: <1463008249-1215650636-cardhu_decombobulator_blackberry.rim.net-128675317-@bxe190.bisx.prod.on.blackberry>
Message-ID: <E386E504246A9249A9176B5BEEC13B6F7B9006@UXEXMBU116.academic.windsor>

It might be help,
 
Portfolio Optimisation Models and Properties of Return Distributions, (2005) G Mitra, D Roman, KH Darby-Dowman, Mathematical Programming Journal,
 
Integrating market and credit risk: A simulation and optimisation perspective, (2005) N J Jobst, G Mitra, S Zenios to appear in Journal of Banking and Finance.
 

Quadratic programming for portfolio planning: Insights into algorithmic and computational issues
Part I - Solving a family of QP models


Quadratic programming for portfolio planning: Insights into algorithmic and computational issues Part II - Processing of portfolio planning models with discrete constraints

regards,
MC

________________________________

From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of kriskumar at earthlink.net
Sent: Thu 10/07/2008 01:42
To: markleeds at verizon.net; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] portfolio optimization



There is an interesting paper on optimization with sorts by Almgren and Chriss. This takes ordering information to compute the optimal portfolio. There was an earlier post here on this which you will find in the list archives.

Cheers
Krishna

------Original Message------
From: markleeds at verizon.net
Sender:
To: r-sig-finance at stat.math.ethz.ch
Sent: Jul 9, 2008 18:00
Subject: [R-SIG-Finance] portfolio optimization

does anyone know of academic literature on optimization in finance where
one has the probabilities of the assets being up or down rather than the
expected returns of the assets ? ( and also covariance matrix).  i think
i  saw something in the past somewhere but i could be mistaken. thanks.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


----
"When I get a little money, I buy books and if any
      is left, I buy food and clothes."  -- Erasmus
_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From Markus.Gesmann at libero.uk.com  Thu Jul 10 13:04:34 2008
From: Markus.Gesmann at libero.uk.com (Markus Gesmann)
Date: Thu, 10 Jul 2008 12:04:34 +0100
Subject: [R-SIG-Finance] CVaR, fExtremes
In-Reply-To: <E386E504246A9249A9176B5BEEC13B6F7B9006@UXEXMBU116.academic.windsor>
References: <1463008249-1215650636-cardhu_decombobulator_blackberry.rim.net-128675317-@bxe190.bisx.prod.on.blackberry>
	<E386E504246A9249A9176B5BEEC13B6F7B9006@UXEXMBU116.academic.windsor>
Message-ID: <5B78F43018FC3D488411E5F4903362E319284CC9D6@OBG-EXC-01.OBG>

Hi all,

I struggle to understand the output of the CVaR function in the fExtremes package.
The output of VaR (Value at Risk) gives me results I expect to see. However the output of CVaR is less than the output of VaR. From my understanding CVaR gives the mean over a given threshold and should therefore always be bigger than VaR.
The fowling example shows the output of VaR and CVaR:

library(fExtremes)
n <- 1000000
loss.ratio <- rlnorm(n, -0.3479, 0.104)
VaR(loss.ratio, 0.995) # same as quantile(loss.ratio, 0.995)
#    99.5%
# 0.9588572
CVaR(loss.ratio, 0.995)
#    99.5%
#0.7088572

I expected an output more like this:
mean(loss.ratio[loss.ratio > 0.995])
# 1.021089
mean(loss.ratio[loss.ratio > 0.995]) - VaR(loss.ratio, 0.995)
#  99.5%
#0.09783733

Maybe I am just a little bit confused and mix up terminologies.


Markus



This message is intended for the personal and confidential use for the designated recipient(s) named above.  If you are not the intended recipient of this message you are hereby notified that any review, dissemination,  distribution or copying of this message is strictly prohibited. This communication is for information purposes only and should not be regarded as an offer to sell or as a solicitation of an offer to buy any financial product, an official confirmation of any transaction or as an official statement of Libero Ventures Ltd.  Email transmissions cannot be guaranteed to be secure or error-free. Therefore we do not represent that this information is complete or accurate and it should not be relied upon as such.  All information is subject to change without notice.


From ianseow at gmail.com  Fri Jul 11 04:39:08 2008
From: ianseow at gmail.com (Ian Seow)
Date: Fri, 11 Jul 2008 10:39:08 +0800
Subject: [R-SIG-Finance] timeDate class query
Message-ID: <1865010807101939u217fb775t90173bb15ce6fab7@mail.gmail.com>

Hi, I have a query on the timeDate class in the fCalendar Package.
I have a large data set of intraday observations (in GMT) containing
407,100 POSIXct objects which I want to convert into timeDate objects
and then view the result in my local time zone ('Asia/Singapore').
The conversion works fine, however, it seems that timeDate does not
have 'Dims' and hence does not allow for length(), head() and tail()
to be performed.
In this case, how do I go about printing say the last 6 elements in
this object? If I convert the timeDate object into another class, the
timezone conversion information is lost and I get my return object in
"GMT" which is not what I want. Appreciate any ideas on this?

Here's the code to illustrate my point:


Original Object in POSIXct
==========================

>   class(index(d))
[1] "POSIXt"  "POSIXct"
>   length(index(d))
[1] 407100

>   tail(index(d))
[1] "2008-06-02 12:30:00 Malay Peninsula Standard Time" "2008-06-02
12:35:00 Malay Peninsula Standard Time"
[3] "2008-06-02 12:40:00 Malay Peninsula Standard Time" "2008-06-02
12:45:00 Malay Peninsula Standard Time"
[5] "2008-06-02 12:50:00 Malay Peninsula Standard Time" "2008-06-02
12:55:00 Malay Peninsula Standard Time"
>

Converted object into timeDate class
====================================

>   myTZ = timeDate(index(d), zone='GMT', FinCenter='Asia/Singapore')
>   length(myTZ) ## fails
Error in length.timeDate(myTZ) :
  no slot of name "Dim" for this object of class "timeDate"
>   tail(myTZ) ## fails
Error in length.timeDate(x) :
  no slot of name "Dim" for this object of class "timeDate"
>


Converted object into character Class, but loses timezone conversion
information. Result returned is back in GMT
================================================================================================================

>   tail(as.character(myTZ))
[1] "2008-06-02 12:30:00" "2008-06-02 12:35:00" "2008-06-02 12:40:00"
"2008-06-02 12:45:00" "2008-06-02 12:50:00" "2008-06-02 12:55:00"


Thanks!

Rgds
Ian Seow


From enricoschumann at yahoo.de  Fri Jul 11 13:17:28 2008
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Fri, 11 Jul 2008 13:17:28 +0200
Subject: [R-SIG-Finance] CVaR, fExtremes
In-Reply-To: <5B78F43018FC3D488411E5F4903362E319284CC9D6@OBG-EXC-01.OBG>
Message-ID: <115120.68383.bm@omp207.mail.ukl.yahoo.com>


it seems to me there is a sign problem when it comes to upper and lower
tail. try the following (sorry that I change your example, but for the
Gaussian its easier to check)


require(fExtremes)
testData <- rnorm(100000)

# the function
CVaRtest <- function (x, alpha = 0.05, type = "sample", tail = c("lower", 
    "upper")) 
{
    x = as.matrix(x)
    tail = match.arg(tail)
    VaR = VaR(x, alpha, type, tail)
    if (type == "sample") {
        CVaR = NULL
	  if (tail=="lower"){
        for (i in 1:ncol(x)) {
            X = as.vector(x[, i])
		Z <- VaR[i] - X
            CVaR = c(CVaR, VaR[i] - 0.5 * mean((Z + 
                abs(Z)))/alpha)
        }
	  }else{
        for (i in 1:ncol(x)) {
            X = as.vector(x[, i])
		Z <- VaR[i] - X
            CVaR = c(CVaR, VaR[i] - 0.5 * mean((Z - 
                abs(Z)))/alpha)
        }
	  }
    }
    CVaR
}

VaR(testData,.05,tail="lower")
CVaR(testData,.05,tail="lower")
CVaRtest(testData,.05,tail="lower")

VaR(testData,.05,tail="upper")
CVaR(testData,.05,tail="upper")
CVaRtest(testData,.05,tail="upper")


# so, for your data
n <- 1000000
loss.ratio <- rlnorm(n, -0.3479, 0.104)

VaR(loss.ratio,.05,tail="lower")
CVaR(loss.ratio,.05,tail="lower")
CVaRtest(loss.ratio,.05,tail="lower")

VaR(loss.ratio,.05,tail="upper")
CVaR(loss.ratio,.05,tail="upper")
CVaRtest(loss.ratio,.05,tail="upper")

# a quick check
mean(loss.ratio[loss.ratio > quantile(loss.ratio,.95)])
mean(loss.ratio[loss.ratio < quantile(loss.ratio,.05)])

(btw. the documentation states that alpha is `a numeric value, the
confidence interval', but this should rather be `confidence interval =
1-\alpha')


best, enrico

cc: diethelm,yohan


-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Markus
Gesmann
Gesendet: Donnerstag, 10. Juli 2008 13:05
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] CVaR, fExtremes

Hi all,

I struggle to understand the output of the CVaR function in the fExtremes
package.
The output of VaR (Value at Risk) gives me results I expect to see. However
the output of CVaR is less than the output of VaR. From my understanding
CVaR gives the mean over a given threshold and should therefore always be
bigger than VaR.
The fowling example shows the output of VaR and CVaR:

library(fExtremes)
n <- 1000000
loss.ratio <- rlnorm(n, -0.3479, 0.104)
VaR(loss.ratio, 0.995) # same as quantile(loss.ratio, 0.995)
#    99.5%
# 0.9588572
CVaR(loss.ratio, 0.995)
#    99.5%
#0.7088572

I expected an output more like this:
mean(loss.ratio[loss.ratio > 0.995])
# 1.021089
mean(loss.ratio[loss.ratio > 0.995]) - VaR(loss.ratio, 0.995) #  99.5%
#0.09783733

Maybe I am just a little bit confused and mix up terminologies.


Markus



This message is intended for the personal and confidential use for the
designated recipient(s) named above.  If you are not the intended recipient
of this message you are hereby notified that any review, dissemination,
distribution or copying of this message is strictly prohibited. This
communication is for information purposes only and should not be regarded as
an offer to sell or as a solicitation of an offer to buy any financial
product, an official confirmation of any transaction or as an official
statement of Libero Ventures Ltd.  Email transmissions cannot be guaranteed
to be secure or error-free. Therefore we do not represent that this
information is complete or accurate and it should not be relied upon as
such.  All information is subject to change without notice.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG - http://www.avg.com

09.07.2008
18:32


From dario.nicodemi at proximaalfa.com  Fri Jul 11 14:49:04 2008
From: dario.nicodemi at proximaalfa.com (Dario Nicodemi)
Date: Fri, 11 Jul 2008 14:49:04 +0200
Subject: [R-SIG-Finance] Stress Testing
Message-ID: <8DC2BE23593EC34EA1E731E3AE14705F01297428@MAH-EU-EXM-01.eu.corp.proximaalfa.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080711/a9e8224f/attachment.pl>

From dutt.debashis at gmail.com  Fri Jul 11 20:17:40 2008
From: dutt.debashis at gmail.com (Debashis Dutta)
Date: Fri, 11 Jul 2008 21:17:40 +0300
Subject: [R-SIG-Finance] Stress Testing
In-Reply-To: <8DC2BE23593EC34EA1E731E3AE14705F01297428@MAH-EU-EXM-01.eu.corp.proximaalfa.com>
References: <8DC2BE23593EC34EA1E731E3AE14705F01297428@MAH-EU-EXM-01.eu.corp.proximaalfa.com>
Message-ID: <37673c2d0807111117l6bd379fcmfcd3b152273776ef@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080711/ba05e184/attachment.pl>

From dario.nicodemi at proximaalfa.com  Mon Jul 14 10:11:19 2008
From: dario.nicodemi at proximaalfa.com (Dario Nicodemi)
Date: Mon, 14 Jul 2008 10:11:19 +0200
Subject: [R-SIG-Finance] Stress Testing
In-Reply-To: <37673c2d0807111117l6bd379fcmfcd3b152273776ef@mail.gmail.com>
References: <8DC2BE23593EC34EA1E731E3AE14705F01297428@MAH-EU-EXM-01.eu.corp.proximaalfa.com>
	<37673c2d0807111117l6bd379fcmfcd3b152273776ef@mail.gmail.com>
Message-ID: <8DC2BE23593EC34EA1E731E3AE14705F01297539@MAH-EU-EXM-01.eu.corp.proximaalfa.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080714/774f4148/attachment.pl>

From pierre8r-gmane at yahoo.fr  Mon Jul 14 12:29:14 2008
From: pierre8r-gmane at yahoo.fr (Pierre8r)
Date: Mon, 14 Jul 2008 10:29:14 +0000 (UTC)
Subject: [R-SIG-Finance] xts library Can I put my test data into the R code
	( without reading a file ) ?
Message-ID: <loom.20080714T102035-920@post.gmane.org>

Hello,


I want to work with some test datas like that ones :

2007-12-03  100.00  110.00  90.00  110.00
2007-12-04  110.00  120.00 100.00  120.00
2007-12-05  120.00  130.00 110.00  130.00
2007-12-06  130.00  140.00 120.00  140.00


I know I can create a CSV file and use this kind of R code :


library(xts)
library(quantmod)
quotes <-
read.csv2("E:\\00001-Compare\\Output\\OutputJBacktesting\\Quotes1HOUR.txt",
header = FALSE, sep = ",", dec=".")

x <- xts(as.matrix(quotes[,-1]),as.POSIXct(paste(quotes[,1]),format='%m/%d/%Y
%H:%M'))
colnames(x) <- c('Open','High','Low','Close','Volume')

It works, but I like to know if I could do the job a other way.
Something like that :

library(xts)

x <- xts(
2007-12-03  100.00  110.00  90.00  110.00
2007-12-04  110.00  120.00 100.00  120.00
2007-12-05  120.00  130.00 110.00  130.00
2007-12-06  130.00  140.00 120.00  140.00
)

x

Can I put my test data into the R code ( without reading a file ).


Thanks,

Pierre8r


From ggrothendieck at gmail.com  Mon Jul 14 13:57:57 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 14 Jul 2008 07:57:57 -0400
Subject: [R-SIG-Finance] xts library Can I put my test data into the R
	code ( without reading a file ) ?
In-Reply-To: <loom.20080714T102035-920@post.gmane.org>
References: <loom.20080714T102035-920@post.gmane.org>
Message-ID: <971536df0807140457h65e832deoa149a2b2fa0c7fad@mail.gmail.com>

Try this:

library(xts) # this also pulls in zoo

Lines <- "2007-12-03  100.00  110.00  90.00  110.00
2007-12-04  110.00  120.00 100.00  120.00
2007-12-05  120.00  130.00 110.00  130.00
2007-12-06  130.00  140.00 120.00  140.00"

x <- read.zoo(textConnection(Lines))
x <- as.xts(x)

On Mon, Jul 14, 2008 at 6:29 AM, Pierre8r <pierre8r-gmane at yahoo.fr> wrote:
> Hello,
>
>
> I want to work with some test datas like that ones :
>
> 2007-12-03  100.00  110.00  90.00  110.00
> 2007-12-04  110.00  120.00 100.00  120.00
> 2007-12-05  120.00  130.00 110.00  130.00
> 2007-12-06  130.00  140.00 120.00  140.00
>
>
> I know I can create a CSV file and use this kind of R code :
>
>
> library(xts)
> library(quantmod)
> quotes <-
> read.csv2("E:\\00001-Compare\\Output\\OutputJBacktesting\\Quotes1HOUR.txt",
> header = FALSE, sep = ",", dec=".")
>
> x <- xts(as.matrix(quotes[,-1]),as.POSIXct(paste(quotes[,1]),format='%m/%d/%Y
> %H:%M'))
> colnames(x) <- c('Open','High','Low','Close','Volume')
>
> It works, but I like to know if I could do the job a other way.
> Something like that :
>
> library(xts)
>
> x <- xts(
> 2007-12-03  100.00  110.00  90.00  110.00
> 2007-12-04  110.00  120.00 100.00  120.00
> 2007-12-05  120.00  130.00 110.00  130.00
> 2007-12-06  130.00  140.00 120.00  140.00
> )
>
> x
>
> Can I put my test data into the R code ( without reading a file ).
>
>
> Thanks,
>
> Pierre8r
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From pierre8r-gmane at yahoo.fr  Mon Jul 14 22:36:04 2008
From: pierre8r-gmane at yahoo.fr (Pierre8r)
Date: Mon, 14 Jul 2008 20:36:04 +0000 (UTC)
Subject: [R-SIG-Finance]
	=?utf-8?q?xts_library_Can_I_put_my_test_data_into?=
	=?utf-8?q?_the_R=09code_=28_without_reading_a_file_=29_=3F?=
References: <loom.20080714T102035-920@post.gmane.org>
	<971536df0807140457h65e832deoa149a2b2fa0c7fad@mail.gmail.com>
Message-ID: <loom.20080714T203532-430@post.gmane.org>

Thanks Gabor.


From pierre8r-gmane at yahoo.fr  Mon Jul 14 22:38:31 2008
From: pierre8r-gmane at yahoo.fr (Pierre8r)
Date: Mon, 14 Jul 2008 20:38:31 +0000 (UTC)
Subject: [R-SIG-Finance] Why this R code dont work ?
Message-ID: <loom.20080714T203621-556@post.gmane.org>

Hello,


Why this R code dont work ?

Error in French :
Erreur dans if (xCl[1] < xCl[2]) { : l'argument est de longueur nulle

Thanks,

Pierre8r

library(quantmod) # this also pulls in zoo
library(xts) # this also pulls in zoo

Lines <- 
"2007-12-03  100.00  110.00  90.00  110.00  10.0
2007-12-04  110.00  120.00 100.00  120.00 10.0
2007-12-05  120.00  130.00 110.00  130.00 10.0
2007-12-06  130.00  140.00 120.00  140.00 10.0
2007-12-07  140.00  140.00 120.00  140.00 10.0
2007-12-08  130.00  130.00 110.00  120.00 10.0
2007-12-09  120.00  120.00 120.00  140.00 10.0
2007-12-10  110.00  130.00 120.00  140.00 10.0
2007-12-11  100.00  120.00 120.00  140.00 10.0 "


x <- read.zoo(textConnection(Lines))

x <- as.xts(x)

colnames(x) <- c('Open','High','Low','Close','Volume')

xCl  <- Cl(x)

if (xCl[1] < xCl[2] ) {
  print("xCl[1] < xCl[2] OK")
} else
{
  print("xCl[1] < xCl[2] KO")
}


From josh.m.ulrich at gmail.com  Mon Jul 14 22:59:12 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Mon, 14 Jul 2008 15:59:12 -0500
Subject: [R-SIG-Finance] Why this R code dont work ?
In-Reply-To: <loom.20080714T203621-556@post.gmane.org>
References: <loom.20080714T203621-556@post.gmane.org>
Message-ID: <8cca69990807141359w7dbeb9f0wb66c0d69f0c320aa@mail.gmail.com>

Ops.xts (like Ops.zoo) requires both observations to have the same
index value.  You want to compare observations with different index
values... so you should use:

coredata(xCl[1]) < coredata(xCl[2])

Best,
Josh
--
http://quantemplation.blogspot.com


On Mon, Jul 14, 2008 at 3:38 PM, Pierre8r <pierre8r-gmane at yahoo.fr> wrote:
> Hello,
>
>
> Why this R code dont work ?
>
> Error in French :
> Erreur dans if (xCl[1] < xCl[2]) { : l'argument est de longueur nulle
>
> Thanks,
>
> Pierre8r
>
> library(quantmod) # this also pulls in zoo
> library(xts) # this also pulls in zoo
>
> Lines <-
> "2007-12-03  100.00  110.00  90.00  110.00  10.0
> 2007-12-04  110.00  120.00 100.00  120.00 10.0
> 2007-12-05  120.00  130.00 110.00  130.00 10.0
> 2007-12-06  130.00  140.00 120.00  140.00 10.0
> 2007-12-07  140.00  140.00 120.00  140.00 10.0
> 2007-12-08  130.00  130.00 110.00  120.00 10.0
> 2007-12-09  120.00  120.00 120.00  140.00 10.0
> 2007-12-10  110.00  130.00 120.00  140.00 10.0
> 2007-12-11  100.00  120.00 120.00  140.00 10.0 "
>
>
> x <- read.zoo(textConnection(Lines))
>
> x <- as.xts(x)
>
> colnames(x) <- c('Open','High','Low','Close','Volume')
>
> xCl  <- Cl(x)
>
> if (xCl[1] < xCl[2] ) {
>  print("xCl[1] < xCl[2] OK")
> } else
> {
>  print("xCl[1] < xCl[2] KO")
> }
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Mon Jul 14 23:02:17 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 14 Jul 2008 17:02:17 -0400
Subject: [R-SIG-Finance] Why this R code dont work ?
In-Reply-To: <loom.20080714T203621-556@post.gmane.org>
References: <loom.20080714T203621-556@post.gmane.org>
Message-ID: <971536df0807141402u461e7e8v75dd6745536e7cb1@mail.gmail.com>

xCl[1] and xCl[2] are xts/zoo time series and when you try to compare
them they get merged and aligned implicitly and then values at
the same times in each are compared.  In this case they are time
series of length 1 that occur at different times so the intersection
of their times is empty.  Try:

coredata(xCl[1]) > coredata(xCl[2])

On Mon, Jul 14, 2008 at 4:38 PM, Pierre8r <pierre8r-gmane at yahoo.fr> wrote:
> Hello,
>
>
> Why this R code dont work ?
>
> Error in French :
> Erreur dans if (xCl[1] < xCl[2]) { : l'argument est de longueur nulle
>
> Thanks,
>
> Pierre8r
>
> library(quantmod) # this also pulls in zoo
> library(xts) # this also pulls in zoo
>
> Lines <-
> "2007-12-03  100.00  110.00  90.00  110.00  10.0
> 2007-12-04  110.00  120.00 100.00  120.00 10.0
> 2007-12-05  120.00  130.00 110.00  130.00 10.0
> 2007-12-06  130.00  140.00 120.00  140.00 10.0
> 2007-12-07  140.00  140.00 120.00  140.00 10.0
> 2007-12-08  130.00  130.00 110.00  120.00 10.0
> 2007-12-09  120.00  120.00 120.00  140.00 10.0
> 2007-12-10  110.00  130.00 120.00  140.00 10.0
> 2007-12-11  100.00  120.00 120.00  140.00 10.0 "
>
>
> x <- read.zoo(textConnection(Lines))
>
> x <- as.xts(x)
>
> colnames(x) <- c('Open','High','Low','Close','Volume')
>
> xCl  <- Cl(x)
>
> if (xCl[1] < xCl[2] ) {
>  print("xCl[1] < xCl[2] OK")
> } else
> {
>  print("xCl[1] < xCl[2] KO")
> }
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From josh.m.ulrich at gmail.com  Mon Jul 14 23:09:05 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Mon, 14 Jul 2008 16:09:05 -0500
Subject: [R-SIG-Finance] Why this R code dont work ?
In-Reply-To: <8cca69990807141359w7dbeb9f0wb66c0d69f0c320aa@mail.gmail.com>
References: <loom.20080714T203621-556@post.gmane.org>
	<8cca69990807141359w7dbeb9f0wb66c0d69f0c320aa@mail.gmail.com>
Message-ID: <8cca69990807141409g5ef7dcc3recfa4e10b6cf30bb@mail.gmail.com>

You could also use lag.zoo (which may be easier to understand):

xCl > as.xts(lag(xCl,-1,na.pad=TRUE))


On Mon, Jul 14, 2008 at 3:59 PM, Josh Ulrich <josh.m.ulrich at gmail.com> wrote:
> Ops.xts (like Ops.zoo) requires both observations to have the same
> index value.  You want to compare observations with different index
> values... so you should use:
>
> coredata(xCl[1]) < coredata(xCl[2])
>
> Best,
> Josh
> --
> http://quantemplation.blogspot.com
>
>
> On Mon, Jul 14, 2008 at 3:38 PM, Pierre8r <pierre8r-gmane at yahoo.fr> wrote:
>> Hello,
>>
>>
>> Why this R code dont work ?
>>
>> Error in French :
>> Erreur dans if (xCl[1] < xCl[2]) { : l'argument est de longueur nulle
>>
>> Thanks,
>>
>> Pierre8r
>>
>> library(quantmod) # this also pulls in zoo
>> library(xts) # this also pulls in zoo
>>
>> Lines <-
>> "2007-12-03  100.00  110.00  90.00  110.00  10.0
>> 2007-12-04  110.00  120.00 100.00  120.00 10.0
>> 2007-12-05  120.00  130.00 110.00  130.00 10.0
>> 2007-12-06  130.00  140.00 120.00  140.00 10.0
>> 2007-12-07  140.00  140.00 120.00  140.00 10.0
>> 2007-12-08  130.00  130.00 110.00  120.00 10.0
>> 2007-12-09  120.00  120.00 120.00  140.00 10.0
>> 2007-12-10  110.00  130.00 120.00  140.00 10.0
>> 2007-12-11  100.00  120.00 120.00  140.00 10.0 "
>>
>>
>> x <- read.zoo(textConnection(Lines))
>>
>> x <- as.xts(x)
>>
>> colnames(x) <- c('Open','High','Low','Close','Volume')
>>
>> xCl  <- Cl(x)
>>
>> if (xCl[1] < xCl[2] ) {
>>  print("xCl[1] < xCl[2] OK")
>> } else
>> {
>>  print("xCl[1] < xCl[2] KO")
>> }
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From dutt.debashis at gmail.com  Mon Jul 14 23:31:16 2008
From: dutt.debashis at gmail.com (Debashis Dutta)
Date: Tue, 15 Jul 2008 00:31:16 +0300
Subject: [R-SIG-Finance] Stress Testing
In-Reply-To: <8DC2BE23593EC34EA1E731E3AE14705F01297539@MAH-EU-EXM-01.eu.corp.proximaalfa.com>
References: <8DC2BE23593EC34EA1E731E3AE14705F01297428@MAH-EU-EXM-01.eu.corp.proximaalfa.com>
	<37673c2d0807111117l6bd379fcmfcd3b152273776ef@mail.gmail.com>
	<8DC2BE23593EC34EA1E731E3AE14705F01297539@MAH-EU-EXM-01.eu.corp.proximaalfa.com>
Message-ID: <37673c2d0807141431g11f7da88u4a6f6971182fe6d3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080715/615593b6/attachment.pl>

From ggrothendieck at gmail.com  Tue Jul 15 03:06:26 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 14 Jul 2008 21:06:26 -0400
Subject: [R-SIG-Finance] Why this R code dont work ?
In-Reply-To: <8cca69990807141409g5ef7dcc3recfa4e10b6cf30bb@mail.gmail.com>
References: <loom.20080714T203621-556@post.gmane.org>
	<8cca69990807141359w7dbeb9f0wb66c0d69f0c320aa@mail.gmail.com>
	<8cca69990807141409g5ef7dcc3recfa4e10b6cf30bb@mail.gmail.com>
Message-ID: <971536df0807141806k4e43239ak9b5ee75f942599d6@mail.gmail.com>

Or diff.

On Mon, Jul 14, 2008 at 5:09 PM, Josh Ulrich <josh.m.ulrich at gmail.com> wrote:
> You could also use lag.zoo (which may be easier to understand):
>
> xCl > as.xts(lag(xCl,-1,na.pad=TRUE))
>
>
> On Mon, Jul 14, 2008 at 3:59 PM, Josh Ulrich <josh.m.ulrich at gmail.com> wrote:
>> Ops.xts (like Ops.zoo) requires both observations to have the same
>> index value.  You want to compare observations with different index
>> values... so you should use:
>>
>> coredata(xCl[1]) < coredata(xCl[2])
>>
>> Best,
>> Josh
>> --
>> http://quantemplation.blogspot.com
>>
>>
>> On Mon, Jul 14, 2008 at 3:38 PM, Pierre8r <pierre8r-gmane at yahoo.fr> wrote:
>>> Hello,
>>>
>>>
>>> Why this R code dont work ?
>>>
>>> Error in French :
>>> Erreur dans if (xCl[1] < xCl[2]) { : l'argument est de longueur nulle
>>>
>>> Thanks,
>>>
>>> Pierre8r
>>>
>>> library(quantmod) # this also pulls in zoo
>>> library(xts) # this also pulls in zoo
>>>
>>> Lines <-
>>> "2007-12-03  100.00  110.00  90.00  110.00  10.0
>>> 2007-12-04  110.00  120.00 100.00  120.00 10.0
>>> 2007-12-05  120.00  130.00 110.00  130.00 10.0
>>> 2007-12-06  130.00  140.00 120.00  140.00 10.0
>>> 2007-12-07  140.00  140.00 120.00  140.00 10.0
>>> 2007-12-08  130.00  130.00 110.00  120.00 10.0
>>> 2007-12-09  120.00  120.00 120.00  140.00 10.0
>>> 2007-12-10  110.00  130.00 120.00  140.00 10.0
>>> 2007-12-11  100.00  120.00 120.00  140.00 10.0 "
>>>
>>>
>>> x <- read.zoo(textConnection(Lines))
>>>
>>> x <- as.xts(x)
>>>
>>> colnames(x) <- c('Open','High','Low','Close','Volume')
>>>
>>> xCl  <- Cl(x)
>>>
>>> if (xCl[1] < xCl[2] ) {
>>>  print("xCl[1] < xCl[2] OK")
>>> } else
>>> {
>>>  print("xCl[1] < xCl[2] KO")
>>> }
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From dario.nicodemi at proximaalfa.com  Tue Jul 15 08:42:15 2008
From: dario.nicodemi at proximaalfa.com (Dario Nicodemi)
Date: Tue, 15 Jul 2008 08:42:15 +0200
Subject: [R-SIG-Finance] Stress Testing
In-Reply-To: <37673c2d0807141431g11f7da88u4a6f6971182fe6d3@mail.gmail.com>
References: <8DC2BE23593EC34EA1E731E3AE14705F01297428@MAH-EU-EXM-01.eu.corp.proximaalfa.com>
	<37673c2d0807111117l6bd379fcmfcd3b152273776ef@mail.gmail.com>
	<8DC2BE23593EC34EA1E731E3AE14705F01297539@MAH-EU-EXM-01.eu.corp.proximaalfa.com>
	<37673c2d0807141431g11f7da88u4a6f6971182fe6d3@mail.gmail.com>
Message-ID: <8DC2BE23593EC34EA1E731E3AE14705F0129770B@MAH-EU-EXM-01.eu.corp.proximaalfa.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080715/7e4a6264/attachment.pl>

From rbali at ufmg.br  Tue Jul 15 14:36:15 2008
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Tue, 15 Jul 2008 08:36:15 -0400
Subject: [R-SIG-Finance] Tests of Conditional Predictive Ability in R
References: <8DC2BE23593EC34EA1E731E3AE14705F01297428@MAH-EU-EXM-01.eu.corp.proximaalfa.com>
Message-ID: <F031F55CCBFE43A8BF44C0253557271F@HPR>

Dear All,

Does any of you know if the CPA test (Giacomini, R. & White, H. (2006). 
"Tests of Conditional Predictive Ability." Econometrica, 74(6), 1545-1578.) 
is available in R packages? Or someone has the code for personal use that 
could share with the list? The authors have a Matlab code 
http://www.econ.ucla.edu/giacomin/Files/CPAtest.m
Any help will be appreciated.
Regards

Robert
ri2162 at columbia.edu


From j at jtoll.com  Wed Jul 16 00:09:04 2008
From: j at jtoll.com (James)
Date: Tue, 15 Jul 2008 16:09:04 -0600
Subject: [R-SIG-Finance] XTS - plot 8 years by month
Message-ID: <54554183-24A8-46F2-9783-EF96B9228E64@jtoll.com>


Hi,

I have an xts object with eight years of monthly calculations, so a  
total of 96 pieces of data.  What I would like to do is to plot 8  
lines, one for each year, with each months data aligned along the x  
axis.  So the x axis would be each month, from Jan to Dec.  I tried  
using monthplot but it creates 12 lines for each of the months, not  
what I want.

I know I can pull each years data by z['2000'].
 > z['2000']

2000-01-21 0.2694149
2000-02-18 0.2404871
2000-03-17 0.2756291
2000-04-21 0.2959471
2000-05-19 0.2736563
2000-06-16 0.2056082
2000-07-21 0.1621791
2000-08-18 0.1382985
2000-09-15 0.1333139
2000-10-20 0.2556851
2000-11-17 0.2216605
2000-12-15 0.2704414

Does anyone know of an easy way to do this or do I have to create a  
new data.frame with months by years?  Thanks.

James


From ggrothendieck at gmail.com  Wed Jul 16 00:24:23 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 15 Jul 2008 18:24:23 -0400
Subject: [R-SIG-Finance] XTS - plot 8 years by month
In-Reply-To: <54554183-24A8-46F2-9783-EF96B9228E64@jtoll.com>
References: <54554183-24A8-46F2-9783-EF96B9228E64@jtoll.com>
Message-ID: <971536df0807151524v583e2b62r6e3b0df3690b747a@mail.gmail.com>

On Tue, Jul 15, 2008 at 6:09 PM, James <j at jtoll.com> wrote:
>
> Hi,
>
> I have an xts object with eight years of monthly calculations, so a total of
> 96 pieces of data.  What I would like to do is to plot 8 lines, one for each
> year, with each months data aligned along the x axis.  So the x axis would
> be each month, from Jan to Dec.  I tried using monthplot but it creates 12
> lines for each of the months, not what I want.
>
> I know I can pull each years data by z['2000'].
>> z['2000']
>
> 2000-01-21 0.2694149
> 2000-02-18 0.2404871
> 2000-03-17 0.2756291
> 2000-04-21 0.2959471
> 2000-05-19 0.2736563
> 2000-06-16 0.2056082
> 2000-07-21 0.1621791
> 2000-08-18 0.1382985
> 2000-09-15 0.1333139
> 2000-10-20 0.2556851
> 2000-11-17 0.2216605
> 2000-12-15 0.2704414
>
> Does anyone know of an easy way to do this or do I have to create a new
> data.frame with months by years?  Thanks.
>

Try this:

# sample data
z <- zooreg(1:96, start = as.Date("2000-01-01"))

matplot(matrix(coredata(z), 12), type = "o", xaxt = "n", pch = paste(0:7))
axis(1, 1:12, month.abb)


From j at jtoll.com  Wed Jul 16 00:29:23 2008
From: j at jtoll.com (James)
Date: Tue, 15 Jul 2008 16:29:23 -0600
Subject: [R-SIG-Finance] XTS - plot 8 years by month
In-Reply-To: <971536df0807151524v583e2b62r6e3b0df3690b747a@mail.gmail.com>
References: <54554183-24A8-46F2-9783-EF96B9228E64@jtoll.com>
	<971536df0807151524v583e2b62r6e3b0df3690b747a@mail.gmail.com>
Message-ID: <A438EBA5-7BE3-4982-BF6B-08C733C37773@jtoll.com>


On Jul 15, 2008, at 4:24 PM, Gabor Grothendieck wrote:

> On Tue, Jul 15, 2008 at 6:09 PM, James <j at jtoll.com> wrote:
>>
>> Hi,
>>
>> I have an xts object with eight years of monthly calculations, so  
>> a total of
>> 96 pieces of data.  What I would like to do is to plot 8 lines,  
>> one for each
>> year, with each months data aligned along the x axis.  So the x  
>> axis would
>> be each month, from Jan to Dec.  I tried using monthplot but it  
>> creates 12
>> lines for each of the months, not what I want.
>>
>> I know I can pull each years data by z['2000'].
>>> z['2000']
>>
>> 2000-01-21 0.2694149
>> 2000-02-18 0.2404871
>> 2000-03-17 0.2756291
>> 2000-04-21 0.2959471
>> 2000-05-19 0.2736563
>> 2000-06-16 0.2056082
>> 2000-07-21 0.1621791
>> 2000-08-18 0.1382985
>> 2000-09-15 0.1333139
>> 2000-10-20 0.2556851
>> 2000-11-17 0.2216605
>> 2000-12-15 0.2704414
>>
>> Does anyone know of an easy way to do this or do I have to create  
>> a new
>> data.frame with months by years?  Thanks.
>>
>
> Try this:
>
> # sample data
> z <- zooreg(1:96, start = as.Date("2000-01-01"))
>
> matplot(matrix(coredata(z), 12), type = "o", xaxt = "n", pch = paste 
> (0:7))
> axis(1, 1:12, month.abb)

Thanks Gabor,

That does exactly what I wanted.

James


From Swaroop.Yalla at MorganStanley.com  Wed Jul 16 13:55:39 2008
From: Swaroop.Yalla at MorganStanley.com (Yalla, Swaroop (FID))
Date: Wed, 16 Jul 2008 07:55:39 -0400
Subject: [R-SIG-Finance] time series question
Message-ID: <507725E31752A74CB00728FA6C17D99C0A52237C@NYWEXMB29.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080716/19544b6d/attachment.pl>

From Swaroop.Yalla at MorganStanley.com  Wed Jul 16 14:12:31 2008
From: Swaroop.Yalla at MorganStanley.com (Yalla, Swaroop (FID))
Date: Wed, 16 Jul 2008 08:12:31 -0400
Subject: [R-SIG-Finance] time series question
Message-ID: <507725E31752A74CB00728FA6C17D99C0A522387@NYWEXMB29.msad.ms.com>

I am using the time series package and have a regularly spaced daily
time-series. How do I convert it to a monthly or quarterly time-series
(i.e. up-sample the time series). Is there a method to do that??
 
Also, as a side questions - what is the best method to find maximum
correlation between two time-series, one of which lags the other.
 
thanks
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/sell the securities/instruments mentioned or an official confirmation.  Morgan Stanley may deal as principal in or own or act as market maker for securities/instruments mentioned or may advise the issuers.  This is not research and is not from MS Research but it may refer to a research analyst/research report.  Unless indicated, these views are the author's and may differ from those of Morgan Stanley research or others in the Firm.  We do not represent this is accurate or complete and we may not update this.  Past performance is not indicative of future returns.  For additional information, research reports and important disclosures, contact me or see https://secure.ms.com/servlet/cls.  You should not use e-mail to request, authorize or effect the purchase or sale of any security or instrument, to send transfer instructions, or to effect any other transactions.  We cannot guarantee that any such requests received via e-mail will be processed in a timely manner.  This communication is solely for the addressee(s) and may contain confidential information.  We do not waive confidentiality by mistransmission.  Contact me if you do not wish to receive these communications.  In the UK, this communication is directed in the UK to those persons who are professional and eligible counterparties (as defined in the UK Financial Services Authority's rules).


From jeff.a.ryan at gmail.com  Wed Jul 16 14:32:07 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 16 Jul 2008 07:32:07 -0500
Subject: [R-SIG-Finance] time series question
In-Reply-To: <507725E31752A74CB00728FA6C17D99C0A52237C@NYWEXMB29.msad.ms.com>
References: <507725E31752A74CB00728FA6C17D99C0A52237C@NYWEXMB29.msad.ms.com>
Message-ID: <e8e755250807160532l69ffac44j1f64181d7f372e56@mail.gmail.com>

I am not entirely sure which package you are referring to, but I will
assume you mean the timeSeries class from fSeries.

>From almost any time-series class you can use the to.period function
in the 'xts' package.  See ?to.period

library(fSeries)
library(quantmod)

getSymbols("QQQQ", ret='timeSeries')
#will load QQQQ into your workspace

to.period(QQQQ, 'months')
#or
to.monthly(QQQQ)

#or for quarterly

to.quarterly(QQQQ)

to.period is smart enough to make sure the class you pass in is the
class you get back.

With respect to 'maximum correlation' I am not too sure what you mean
by that... but see ?lag and ?Lag for a start.

Jeff



On Wed, Jul 16, 2008 at 6:55 AM, Yalla, Swaroop (FID)
<Swaroop.Yalla at morganstanley.com> wrote:
> I am using the time series package and have a regularly spaced daily
> time-series. How do I convert it to a monthly or quarterly time-series
> (i.e. up-sample the time series). Is there a method to do that??
>
> Also, as a side questions - what is the best method to find maximum
> correlation between two time-series, one of which lags the other.
>
> thanks
> --------------------------------------------------------
>
> This is not an offer (or solicitation of an offer) to bu...{{dropped:23}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From pierre8r-list at yahoo.fr  Wed Jul 16 16:32:26 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Wed, 16 Jul 2008 14:32:26 +0000 (GMT)
Subject: [R-SIG-Finance] How to format a CSV file output ?
Message-ID: <673372.5378.qm@web28104.mail.ukl.yahoo.com>

Hello,

Here my R code :

library(xts)
library(quantmod)

quotes <- read.csv2("E:\\00001-Compare\\Output\\OutputJBacktesting\\InputIndic.txt",
header = FALSE, sep = ",", dec=".")

x <- xts(as.matrix(quotes[,-1]),as.POSIXct(paste(quotes[,1]),format='%m/%d/%Y
%H:%M'))
colnames(x) <- c('Indic')
x

write.csv(x, file = "OutputIndic.csv", quote = FALSE, row.names = FALSE)
                                   
The x give this output to the R Console :

> colnames(x) <- c('Indic')
> x
                      Indic
2007-01-08 00:59:00 1.93025
2007-01-08 01:59:00 1.92960
2007-01-08 02:59:00 1.92805


My goal is to generate this kind of CSV output :

01/08/2007,00:59, 1.93025
01/08/2007,01:59, 1.92955
01/08/2007,02:59, 1.92885

My write.csv generate this CSV output :

Indic
1.93025
1.9296
1.92805

How to generate this kind of output ?

01/08/2007,00:59, 1.93025
01/08/2007,01:59, 1.92955

Thanks,

Pierre8r



      ____________________________________________________________
ente http://mail.yahoo.fr


From Matthias.Koberstein at hsbctrinkaus.de  Wed Jul 16 16:48:50 2008
From: Matthias.Koberstein at hsbctrinkaus.de (Matthias.Koberstein at hsbctrinkaus.de)
Date: Wed, 16 Jul 2008 16:48:50 +0200
Subject: [R-SIG-Finance] Optimize question
Message-ID: <OF96DBC418.0EB2107C-ONC1257488.0050E085-C1257488.00515AFA@hsbctrinkaus.de>


Hi,

I use the command optim and optimize in a function.
Unfortunatley the standard method needs a lot of time and in accordance to
the manual is the slowest (Nelder-Maed).
A more "dirty" optimization qould be sufficient for my purposes as long as
it is faster. The function provides 4 other methods ("BFGS", "CG",
"L-BFGS-B", "SANN")
but which one is the fastest? Does anyone have eperience with that?

Thank you very much in advance

Matthias

**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential...{{dropped:18}}


From Wayne.W.Jones at shell.com  Wed Jul 16 17:04:49 2008
From: Wayne.W.Jones at shell.com (Wayne.W.Jones at shell.com)
Date: Wed, 16 Jul 2008 16:04:49 +0100
Subject: [R-SIG-Finance] Optimize question
In-Reply-To: <OF96DBC418.0EB2107C-ONC1257488.0050E085-C1257488.00515AFA@hsbctrinkaus.de>
Message-ID: <77693D6263D9B94AA3C6384F1474E26A034A25E7@wyt-s-019.europe.shell.com>


Hi, 

Nelder and Mead is slow to converge but has the advantage that objective function derivatives need not be calculated. 

Sometimes speed to convergence is dependent on the problem in hand, so I suggest you set up a test optimisation and try each method in turn to benchmark each method. 


Regards

Wayne



-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of
Matthias.Koberstein at hsbctrinkaus.de
Sent: 16 July 2008 15:49
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Optimize question



Hi,

I use the command optim and optimize in a function.
Unfortunatley the standard method needs a lot of time and in accordance to
the manual is the slowest (Nelder-Maed).
A more "dirty" optimization qould be sufficient for my purposes as long as
it is faster. The function provides 4 other methods ("BFGS", "CG",
"L-BFGS-B", "SANN")
but which one is the fastest? Does anyone have eperience with that?

Thank you very much in advance

Matthias

**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential...{{dropped:18}}

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Wed Jul 16 17:05:29 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 16 Jul 2008 10:05:29 -0500
Subject: [R-SIG-Finance] How to format a CSV file output ?
In-Reply-To: <673372.5378.qm@web28104.mail.ukl.yahoo.com>
References: <673372.5378.qm@web28104.mail.ukl.yahoo.com>
Message-ID: <e8e755250807160805t44eae328q4a224e606e7e21ff@mail.gmail.com>

Hi Pierre,

try:

x <- xts(matrix(runif(10,1.9,2)), Sys.time()+1:10)

x <- matrix(c(strftime(as.POSIXlt(index(x)),'%m/%d/%Y,%H:%M'),x),dimnames=list(NULL,c('','Indec')),nc=2)

write.csv(x, row.names=FALSE, quote=FALSE)

,Indec
07/16/2008,14:59,1.92998949340545
07/16/2008,14:59,1.92604529005475
07/16/2008,14:59,1.90992500772700
07/16/2008,14:59,1.90918286179658
07/16/2008,14:59,1.98319136707578
07/16/2008,14:59,1.95377100475598
07/16/2008,14:59,1.90766719337553
07/16/2008,14:59,1.9713675866602
07/16/2008,14:59,1.94339190139435
07/16/2008,14:59,1.95427884196397

HTH
Jeff

On Wed, Jul 16, 2008 at 9:32 AM,  <pierre8r-list at yahoo.fr> wrote:
> Hello,
>
> Here my R code :
>
> library(xts)
> library(quantmod)
>
> quotes <- read.csv2("E:\\00001-Compare\\Output\\OutputJBacktesting\\InputIndic.txt",
> header = FALSE, sep = ",", dec=".")
>
> x <- xts(as.matrix(quotes[,-1]),as.POSIXct(paste(quotes[,1]),format='%m/%d/%Y
> %H:%M'))
> colnames(x) <- c('Indic')
> x
>
> write.csv(x, file = "OutputIndic.csv", quote = FALSE, row.names = FALSE)
>
> The x give this output to the R Console :
>
>> colnames(x) <- c('Indic')
>> x
>                      Indic
> 2007-01-08 00:59:00 1.93025
> 2007-01-08 01:59:00 1.92960
> 2007-01-08 02:59:00 1.92805
>
>
> My goal is to generate this kind of CSV output :
>
> 01/08/2007,00:59, 1.93025
> 01/08/2007,01:59, 1.92955
> 01/08/2007,02:59, 1.92885
>
> My write.csv generate this CSV output :
>
> Indic
> 1.93025
> 1.9296
> 1.92805
>
> How to generate this kind of output ?
>
> 01/08/2007,00:59, 1.93025
> 01/08/2007,01:59, 1.92955
>
> Thanks,
>
> Pierre8r
>
>
>
>      ____________________________________________________________
> ente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From josh.m.ulrich at gmail.com  Wed Jul 16 17:10:34 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Wed, 16 Jul 2008 10:10:34 -0500
Subject: [R-SIG-Finance] How to format a CSV file output ?
In-Reply-To: <673372.5378.qm@web28104.mail.ukl.yahoo.com>
References: <673372.5378.qm@web28104.mail.ukl.yahoo.com>
Message-ID: <8cca69990807160810y76e31758p9827e52a25659060@mail.gmail.com>

write.csv(x, file = "OutputIndic.csv", quote = FALSE, row.names =
format(index(x), "%m/%d/%Y,%H:%M:%S"))

--
http://quantemplation.blogspot.com


On Wed, Jul 16, 2008 at 9:32 AM,  <pierre8r-list at yahoo.fr> wrote:
> Hello,
>
> Here my R code :
>
> library(xts)
> library(quantmod)
>
> quotes <- read.csv2("E:\\00001-Compare\\Output\\OutputJBacktesting\\InputIndic.txt",
> header = FALSE, sep = ",", dec=".")
>
> x <- xts(as.matrix(quotes[,-1]),as.POSIXct(paste(quotes[,1]),format='%m/%d/%Y
> %H:%M'))
> colnames(x) <- c('Indic')
> x
>
> write.csv(x, file = "OutputIndic.csv", quote = FALSE, row.names = FALSE)
>
> The x give this output to the R Console :
>
>> colnames(x) <- c('Indic')
>> x
>                      Indic
> 2007-01-08 00:59:00 1.93025
> 2007-01-08 01:59:00 1.92960
> 2007-01-08 02:59:00 1.92805
>
>
> My goal is to generate this kind of CSV output :
>
> 01/08/2007,00:59, 1.93025
> 01/08/2007,01:59, 1.92955
> 01/08/2007,02:59, 1.92885
>
> My write.csv generate this CSV output :
>
> Indic
> 1.93025
> 1.9296
> 1.92805
>
> How to generate this kind of output ?
>
> 01/08/2007,00:59, 1.93025
> 01/08/2007,01:59, 1.92955
>
> Thanks,
>
> Pierre8r
>
>
>
>      ____________________________________________________________
> ente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Wed Jul 16 17:47:15 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 16 Jul 2008 11:47:15 -0400
Subject: [R-SIG-Finance] time series question
In-Reply-To: <507725E31752A74CB00728FA6C17D99C0A52237C@NYWEXMB29.msad.ms.com>
References: <507725E31752A74CB00728FA6C17D99C0A52237C@NYWEXMB29.msad.ms.com>
Message-ID: <971536df0807160847g3a5cbea2tdd649437a56d7420@mail.gmail.com>

Its not clear from your question what "time series" package refers to
as there are many such packages in R but if your series is a ts series
then see ?aggregate.ts and if its a zoo series see ?aggregate.zoo .
The latter has specific examples of monthly and quarterly aggregations.
Jeff has already mentioned xts and for the fame package see ?convert

On Wed, Jul 16, 2008 at 7:55 AM, Yalla, Swaroop (FID)
<Swaroop.Yalla at morganstanley.com> wrote:
> I am using the time series package and have a regularly spaced daily
> time-series. How do I convert it to a monthly or quarterly time-series
> (i.e. up-sample the time series). Is there a method to do that??
>
> Also, as a side questions - what is the best method to find maximum
> correlation between two time-series, one of which lags the other.
>
> thanks
> --------------------------------------------------------
>
> This is not an offer (or solicitation of an offer) to bu...{{dropped:23}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From patrick at burns-stat.com  Wed Jul 16 18:58:12 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed, 16 Jul 2008 17:58:12 +0100
Subject: [R-SIG-Finance] Optimize question
In-Reply-To: <77693D6263D9B94AA3C6384F1474E26A034A25E7@wyt-s-019.europe.shell.com>
References: <77693D6263D9B94AA3C6384F1474E26A034A25E7@wyt-s-019.europe.shell.com>
Message-ID: <487E28A4.20406@burns-stat.com>

Also note that there is convergence, and convergence
to an answer that is good enough.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Wayne.W.Jones at shell.com wrote:
> Hi, 
>
> Nelder and Mead is slow to converge but has the advantage that objective function derivatives need not be calculated. 
>
> Sometimes speed to convergence is dependent on the problem in hand, so I suggest you set up a test optimisation and try each method in turn to benchmark each method. 
>
>
> Regards
>
> Wayne
>
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of
> Matthias.Koberstein at hsbctrinkaus.de
> Sent: 16 July 2008 15:49
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Optimize question
>
>
>
> Hi,
>
> I use the command optim and optimize in a function.
> Unfortunatley the standard method needs a lot of time and in accordance to
> the manual is the slowest (Nelder-Maed).
> A more "dirty" optimization qould be sufficient for my purposes as long as
> it is faster. The function provides 4 other methods ("BFGS", "CG",
> "L-BFGS-B", "SANN")
> but which one is the fastest? Does anyone have eperience with that?
>
> Thank you very much in advance
>
> Matthias
>
> **** Ressourcen schonen, weniger drucken - Think before you print! ****
>
> ---------------------------------------------------------------------
> Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
> rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
> sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
> sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
> oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
> gestattet.
>
> This e-mail and any attachments may contain confidential...{{dropped:18}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
>


From j at jtoll.com  Wed Jul 16 23:30:31 2008
From: j at jtoll.com (James)
Date: Wed, 16 Jul 2008 15:30:31 -0600
Subject: [R-SIG-Finance] XTS - endpoints omits price changes
Message-ID: <8FDF9F21-93F8-41A5-94E1-C4099666491E@jtoll.com>

Hi,


I've been learning to use the XTS package and have run into a  
problem.  If I calculate monthly log normal price relatives as such:

 > df<-yahooSeries("QQQQ", from = "2007-01-01", to = "2007-12-31",  
returnClass=c("data.frame"))
 > x<-as.xts(df)
 > names(df)<-c("Open","High","Low","Close","Volume")
 > period.apply(x$Close, INDEX=endpoints(x, 'months'), FUN=function 
(x) sum(diff(log(x))))
2007-01-31   2007-02-28   2007-03-30   2007-04-30   2007-05-31    
2007-06-29   2007-07-31   2007-08-31   2007-09-28
  0.019013286 -0.015344398  0.009231545  0.052943687  0.027803331   
0.003367007 -0.010464725  0.020048207  0.050668995
   2007-10-31   2007-11-30   2007-12-31
  0.056634772 -0.051098382  0.006660162

What happens is that the price change between the last day of the  
previous month and the first day of the current month is ignored for  
all 12 months.  This is a problem because I should, at least in my  
opinion, be able to add all twelve monthly changes to get the yearly  
change.  And that should be the same as:

 > period.apply(x$Close, INDEX=endpoints(x, 'years'), FUN=function(x)  
sum(diff(log(x))))
2007-12-31
  0.1693641

But it's not, because 12 daily returns have been left out.  Is there  
a way to change this behavior, so that any given month, or period,  
will include all the price changes?

Thanks,

James


From josh.m.ulrich at gmail.com  Thu Jul 17 00:12:02 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Wed, 16 Jul 2008 17:12:02 -0500
Subject: [R-SIG-Finance] XTS - endpoints omits price changes
In-Reply-To: <8FDF9F21-93F8-41A5-94E1-C4099666491E@jtoll.com>
References: <8FDF9F21-93F8-41A5-94E1-C4099666491E@jtoll.com>
Message-ID: <8cca69990807161512p4ccc534ctee45d56983b7e300@mail.gmail.com>

Hi James,

period.apply works *within* the given interval, so it will not use the
previous month's values in any calculation.  The diff function omits
the first value for each month by default, which are your missing
days' returns.

To achieve your desired result you could calculate daily returns
before calling period.apply:
period.apply(diff(log(x$Close),na.pad=TRUE), INDEX=endpoints(x,
'months'), FUN=sum)
# returns a 'zoo' series

Or you can calculate returns on the end-of-month values only:
diff(log(x[endpoints(x,'months'),'Close']))

But you need the Close for the last day of 2006-12 in order to
calculate the return for the first day of 2007 and therefore calculate
the year's return by your definition.

Best,
Josh

--
http://quantemplation.blogspot.com


On Wed, Jul 16, 2008 at 4:30 PM, James <j at jtoll.com> wrote:
> Hi,
>
>
> I've been learning to use the XTS package and have run into a problem.  If I
> calculate monthly log normal price relatives as such:
>
>> df<-yahooSeries("QQQQ", from = "2007-01-01", to = "2007-12-31",
>> returnClass=c("data.frame"))
>> x<-as.xts(df)
>> names(df)<-c("Open","High","Low","Close","Volume")
>> period.apply(x$Close, INDEX=endpoints(x, 'months'), FUN=function(x)
>> sum(diff(log(x))))
> 2007-01-31   2007-02-28   2007-03-30   2007-04-30   2007-05-31   2007-06-29
>   2007-07-31   2007-08-31   2007-09-28
>  0.019013286 -0.015344398  0.009231545  0.052943687  0.027803331
>  0.003367007 -0.010464725  0.020048207  0.050668995
>  2007-10-31   2007-11-30   2007-12-31
>  0.056634772 -0.051098382  0.006660162
>
> What happens is that the price change between the last day of the previous
> month and the first day of the current month is ignored for all 12 months.
>  This is a problem because I should, at least in my opinion, be able to add
> all twelve monthly changes to get the yearly change.  And that should be the
> same as:
>
>> period.apply(x$Close, INDEX=endpoints(x, 'years'), FUN=function(x)
>> sum(diff(log(x))))
> 2007-12-31
>  0.1693641
>
> But it's not, because 12 daily returns have been left out.  Is there a way
> to change this behavior, so that any given month, or period, will include
> all the price changes?
>
> Thanks,
>
> James
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From m_olshansky at yahoo.com  Thu Jul 17 01:33:36 2008
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 16 Jul 2008 16:33:36 -0700 (PDT)
Subject: [R-SIG-Finance] Optimize question
In-Reply-To: <OF96DBC418.0EB2107C-ONC1257488.0050E085-C1257488.00515AFA@hsbctrinkaus.de>
Message-ID: <903158.39182.qm@web32204.mail.mud.yahoo.com>

This may depend on your problem: how many dimensions do you have, is it a constrained or unconstrained optimization, does your function have smooth first (and second) derivatives, can you compute them analytically, etc.?


--- On Thu, 17/7/08, Matthias.Koberstein at hsbctrinkaus.de <Matthias.Koberstein at hsbctrinkaus.de> wrote:

> From: Matthias.Koberstein at hsbctrinkaus.de <Matthias.Koberstein at hsbctrinkaus.de>
> Subject: [R-SIG-Finance] Optimize question
> To: r-sig-finance at stat.math.ethz.ch
> Received: Thursday, 17 July, 2008, 12:48 AM
> Hi,
> 
> I use the command optim and optimize in a function.
> Unfortunatley the standard method needs a lot of time and
> in accordance to
> the manual is the slowest (Nelder-Maed).
> A more "dirty" optimization qould be sufficient
> for my purposes as long as
> it is faster. The function provides 4 other methods
> ("BFGS", "CG",
> "L-BFGS-B", "SANN")
> but which one is the fastest? Does anyone have eperience
> with that?
> 
> Thank you very much in advance
> 
> Matthias
> 
> **** Ressourcen schonen, weniger drucken - Think before you
> print! ****
> 
> ---------------------------------------------------------------------
> Diese E-Mail sowie eventuelle Anh?nge enthalten
> vertrauliche und / oder
> rechtlich gesch?tzte Informationen. Wenn Sie nicht der
> richtige Adressat
> sind oder diese E-Mail irrt?mlich erhalten haben,
> informieren Sie bitte
> sofort den Absender und vernichten Sie diese Mail. Das
> unerlaubte Kopieren
> oder Speichern sowie die unbefugte Weitergabe dieser E-Mail
> sind nicht
> gestattet.
> 
> This e-mail and any attachments may contain
> confidential...{{dropped:18}}
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Thu Jul 17 03:39:17 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 16 Jul 2008 20:39:17 -0500
Subject: [R-SIG-Finance] XTS - endpoints omits price changes
In-Reply-To: <8FDF9F21-93F8-41A5-94E1-C4099666491E@jtoll.com>
References: <8FDF9F21-93F8-41A5-94E1-C4099666491E@jtoll.com>
Message-ID: <e8e755250807161839ma3bfc12l4d1c242ad71ce72c@mail.gmail.com>

An alternate (simpler) approach using 'quantmod':

> monthlyReturn(x[,'Close'],type='log')
         monthly.returns
Jan 2007     0.019013286
Feb 2007    -0.016934043
Mar 2007     0.004605120
Apr 2007     0.054321097
May 2007     0.031061724
Jun 2007     0.003999584
Jul 2007    -0.001471671
Aug 2007     0.027802621
Sep 2007     0.050668995
Oct 2007     0.068045785
Nov 2007    -0.069992826
Dec 2007    -0.001755584

> sum(monthlyReturn(x[,'Close'],type='log'))
[1] 0.1693641

> yearlyReturn(x[,'Close'],type='log')
           yearly.returns
2007-12-31      0.1693641

What is being called by periodReturn internally:
> Delt(Cl(to.monthly(x)),type='log')
           Delt.1.log
Jan 2007           NA
Feb 2007 -0.016934043
Mar 2007  0.004605120
Apr 2007  0.054321097
May 2007  0.031061724
Jun 2007  0.003999584
Jul 2007 -0.001471671
Aug 2007  0.027802621
Sep 2007  0.050668995
Oct 2007  0.068045785
Nov 2007 -0.069992826
Dec 2007 -0.001755584

see ?periodReturn for more info on formats and options.

Jeff
On Wed, Jul 16, 2008 at 4:30 PM, James <j at jtoll.com> wrote:
> Hi,
>
>
> I've been learning to use the XTS package and have run into a problem.  If I
> calculate monthly log normal price relatives as such:
>
>> df<-yahooSeries("QQQQ", from = "2007-01-01", to = "2007-12-31",
>> returnClass=c("data.frame"))
>> x<-as.xts(df)
>> names(df)<-c("Open","High","Low","Close","Volume")
>> period.apply(x$Close, INDEX=endpoints(x, 'months'), FUN=function(x)
>> sum(diff(log(x))))
> 2007-01-31   2007-02-28   2007-03-30   2007-04-30   2007-05-31   2007-06-29
>   2007-07-31   2007-08-31   2007-09-28
>  0.019013286 -0.015344398  0.009231545  0.052943687  0.027803331
>  0.003367007 -0.010464725  0.020048207  0.050668995
>  2007-10-31   2007-11-30   2007-12-31
>  0.056634772 -0.051098382  0.006660162
>
> What happens is that the price change between the last day of the previous
> month and the first day of the current month is ignored for all 12 months.
>  This is a problem because I should, at least in my opinion, be able to add
> all twelve monthly changes to get the yearly change.  And that should be the
> same as:
>
>> period.apply(x$Close, INDEX=endpoints(x, 'years'), FUN=function(x)
>> sum(diff(log(x))))
> 2007-12-31
>  0.1693641
>
> But it's not, because 12 daily returns have been left out.  Is there a way
> to change this behavior, so that any given month, or period, will include
> all the price changes?
>
> Thanks,
>
> James
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From pierre8r-list at yahoo.fr  Thu Jul 17 12:24:13 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Thu, 17 Jul 2008 10:24:13 +0000 (GMT)
Subject: [R-SIG-Finance] How to format a CSV file output ?
Message-ID: <128512.88006.qm@web28104.mail.ukl.yahoo.com>

Jeff Ryan <jeff.a.ryan <at> gmail.com> writes:

> 
> Hi Pierre,
> 
> try:
> 
> x <- xts(matrix(runif(10,1.9,2)), Sys.time()+1:10)
> 
> x <-
matrix(c(strftime(as.POSIXlt(index(x)),'%m/%d/%Y,%H:%M'),x),dimnames=list(NULL,c('','Indec')),nc=2)
> 
> write.csv(x, row.names=FALSE, quote=FALSE)
> 
> ,Indec
> 07/16/2008,14:59,1.92998949340545
> 07/16/2008,14:59,1.92604529005475
> 07/16/2008,14:59,1.90992500772700
> 07/16/2008,14:59,1.90918286179658


Hi jeff,

Thanks for the answer.

Because I need to compare two files, I have to format the csv file.
To compare the two files I use WinMerge :
http://winmerge.org/

I have slightly changed your code to :

x <- matrix(c(strftime(as.POSIXlt(index(x)),'%m/%d/%Y,%H:%M'),
 format(x, nsmall = 5)), ncol=2)

write.csv(x, row.names=FALSE, quote=FALSE)

But I need a space between the comma and the double, like that :
The current output :
01/08/2007,00:59,1.93025
01/08/2007,01:59,1.92955
01/08/2007,02:59,1.92885

My target :
01/08/2007,00:59, 1.93025
01/08/2007,01:59, 1.92955
01/08/2007,02:59, 1.92885

Also is it possible to remove the first line ( the V1,V2 stuff ) ?

V1,V2
07/17/2008,12:18,1.960615
07/17/2008,12:18,1.940416
07/17/2008,12:18,1.931146


Thanks,

Pierre8r


      ____________________________________________________
intelligente http://mail.yahoo.fr


From Matthias.Koberstein at hsbctrinkaus.de  Thu Jul 17 14:41:25 2008
From: Matthias.Koberstein at hsbctrinkaus.de (Matthias.Koberstein at hsbctrinkaus.de)
Date: Thu, 17 Jul 2008 14:41:25 +0200
Subject: [R-SIG-Finance] Antwort: Re:  Optimize question
In-Reply-To: <903158.39182.qm@web32204.mail.mud.yahoo.com>
Message-ID: <OFB6ECB5EA.67F9E9C1-ONC1257489.00449265-C1257489.0045B087@hsbctrinkaus.de>

Dear all,

thank you for your help regarding the optim speed question.
I just wanted to share my results with you.
After testing all optimization methods, the BFGS algorithm proved the
fastest by quite a bit.
The algorithms performances on a three dimensional problem with ~2200
observations were

BFGS 0:44 min.
L-BFGS -B 1:06 min
NM: 1:28 minutes
CS >3 min.
SANN >3 min.

(all with default convergence)

Matthias S. Koberstein
__________________________________
HSBC Trinkaus
Structured Solutions Group
K?nigsalle 21/23, 40212 D?sseldorf

Phone: +49 211 910 4412
e-mail: matthias.koberstein at hsbctrinkaus.de



**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential and / or
privileged information. If you are not the intended recipient or have
received this e-mail in error, please notify the sender immediately and
destroy this e-mail . Any unauthorized copying, storing, disclosure or
distribution of the contents of this e-mail is strictly forbidden.

---------------------------------------------------------------------
HSBC Trinkaus & Burkhardt AG
Sitz: D?sseldorf, K?nigsallee 21/23, 40212 D?sseldorf, Handelsregister:
Amtsgericht D?sseldorf HRB 54447
Mitglieder des Vorstands: Andreas Schmitz (Sprecher), Paul Hagen, Dr. Olaf
Huth, Carola Gr?fin v. Schmettow
Vorsitzender des Aufsichtsrats: Dr. Sieghardt Rometsch


                                                                           
             Moshe Olshansky                                               
             <m_olshansky at yaho                                             
             o.com>                                                     An 
             Gesendet von:               r-sig-finance at stat.math.ethz.ch,  
             r-sig-finance-bou           Matthias.Koberstein at hsbctrinkaus. 
             nces at stat.math.et           de                                
             hz.ch                                                   Kopie 
                                                                           
             Fax-Deckblatt:                                          Thema 
             HSBCTuB                     Re: [R-SIG-Finance] Optimize      
             17.07.2008 01:35            question                          
                                                                           
              Bitte antworten                                              
                    an                                                     
             m_olshansky at yahoo                                             
                   .com                                                    
                                                                           
                                                                           




This may depend on your problem: how many dimensions do you have, is it a
constrained or unconstrained optimization, does your function have smooth
first (and second) derivatives, can you compute them analytically, etc.?


--- On Thu, 17/7/08, Matthias.Koberstein at hsbctrinkaus.de
<Matthias.Koberstein at hsbctrinkaus.de> wrote:

> From: Matthias.Koberstein at hsbctrinkaus.de
<Matthias.Koberstein at hsbctrinkaus.de>
> Subject: [R-SIG-Finance] Optimize question
> To: r-sig-finance at stat.math.ethz.ch
> Received: Thursday, 17 July, 2008, 12:48 AM
> Hi,
>
> I use the command optim and optimize in a function.
> Unfortunatley the standard method needs a lot of time and
> in accordance to
> the manual is the slowest (Nelder-Maed).
> A more "dirty" optimization qould be sufficient
> for my purposes as long as
> it is faster. The function provides 4 other methods
> ("BFGS", "CG",
> "L-BFGS-B", "SANN")
> but which one is the fastest? Does anyone have eperience
> with that?
>
> Thank you very much in advance
>
> Matthias
>
> **** Ressourcen schonen, weniger drucken - Think before you
> print! ****
>
> ---------------------------------------------------------------------
> Diese E-Mail sowie eventuelle Anh?nge enthalten
> vertrauliche und / oder
> rechtlich gesch?tzte Informationen. Wenn Sie nicht der
> richtige Adressat
> sind oder diese E-Mail irrt?mlich erhalten haben,
> informieren Sie bitte
> sofort den Absender und vernichten Sie diese Mail. Das
> unerlaubte Kopieren
> oder Speichern sowie die unbefugte Weitergabe dieser E-Mail
> sind nicht
> gestattet.
>
> This e-mail and any attachments may contain
> confidential...{{dropped:18}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From Wayne.W.Jones at shell.com  Thu Jul 17 15:27:00 2008
From: Wayne.W.Jones at shell.com (Wayne.W.Jones at shell.com)
Date: Thu, 17 Jul 2008 14:27:00 +0100
Subject: [R-SIG-Finance] Antwort: Re:  Optimize question
In-Reply-To: <OFB6ECB5EA.67F9E9C1-ONC1257489.00449265-C1257489.0045B087@hsbctrinkaus.de>
Message-ID: <77693D6263D9B94AA3C6384F1474E26A034A25F4@wyt-s-019.europe.shell.com>


Thanks for the feedback,

Interesting to see the difference in the methods. 

If you play with the convergence criteria then you may well improve on these times. Sometimes the optim algortithms search hard for a very small change in objective value and effectiveley only change the final parameter estimates by miniscule amounts. 

Regards

Wayne





-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of
Matthias.Koberstein at hsbctrinkaus.de
Sent: 17 July 2008 13:41
To: m_olshansky at yahoo.com
Cc: r-sig-finance at stat.math.ethz.ch;
r-sig-finance-bounces at stat.math.ethz.ch
Subject: [R-SIG-Finance] Antwort: Re: Optimize question


Dear all,

thank you for your help regarding the optim speed question.
I just wanted to share my results with you.
After testing all optimization methods, the BFGS algorithm proved the
fastest by quite a bit.
The algorithms performances on a three dimensional problem with ~2200
observations were

BFGS 0:44 min.
L-BFGS -B 1:06 min
NM: 1:28 minutes
CS >3 min.
SANN >3 min.

(all with default convergence)

Matthias S. Koberstein
__________________________________
HSBC Trinkaus
Structured Solutions Group
K?nigsalle 21/23, 40212 D?sseldorf

Phone: +49 211 910 4412
e-mail: matthias.koberstein at hsbctrinkaus.de



**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential and / or
privileged information. If you are not the intended recipient or have
received this e-mail in error, please notify the sender immediately and
destroy this e-mail . Any unauthorized copying, storing, disclosure or
distribution of the contents of this e-mail is strictly forbidden.

---------------------------------------------------------------------
HSBC Trinkaus & Burkhardt AG
Sitz: D?sseldorf, K?nigsallee 21/23, 40212 D?sseldorf, Handelsregister:
Amtsgericht D?sseldorf HRB 54447
Mitglieder des Vorstands: Andreas Schmitz (Sprecher), Paul Hagen, Dr. Olaf
Huth, Carola Gr?fin v. Schmettow
Vorsitzender des Aufsichtsrats: Dr. Sieghardt Rometsch


                                                                           
             Moshe Olshansky                                               
             <m_olshansky at yaho                                             
             o.com>                                                     An 
             Gesendet von:               r-sig-finance at stat.math.ethz.ch,  
             r-sig-finance-bou           Matthias.Koberstein at hsbctrinkaus. 
             nces at stat.math.et           de                                
             hz.ch                                                   Kopie 
                                                                           
             Fax-Deckblatt:                                          Thema 
             HSBCTuB                     Re: [R-SIG-Finance] Optimize      
             17.07.2008 01:35            question                          
                                                                           
              Bitte antworten                                              
                    an                                                     
             m_olshansky at yahoo                                             
                   .com                                                    
                                                                           
                                                                           




This may depend on your problem: how many dimensions do you have, is it a
constrained or unconstrained optimization, does your function have smooth
first (and second) derivatives, can you compute them analytically, etc.?


--- On Thu, 17/7/08, Matthias.Koberstein at hsbctrinkaus.de
<Matthias.Koberstein at hsbctrinkaus.de> wrote:

> From: Matthias.Koberstein at hsbctrinkaus.de
<Matthias.Koberstein at hsbctrinkaus.de>
> Subject: [R-SIG-Finance] Optimize question
> To: r-sig-finance at stat.math.ethz.ch
> Received: Thursday, 17 July, 2008, 12:48 AM
> Hi,
>
> I use the command optim and optimize in a function.
> Unfortunatley the standard method needs a lot of time and
> in accordance to
> the manual is the slowest (Nelder-Maed).
> A more "dirty" optimization qould be sufficient
> for my purposes as long as
> it is faster. The function provides 4 other methods
> ("BFGS", "CG",
> "L-BFGS-B", "SANN")
> but which one is the fastest? Does anyone have eperience
> with that?
>
> Thank you very much in advance
>
> Matthias
>
> **** Ressourcen schonen, weniger drucken - Think before you
> print! ****
>
> ---------------------------------------------------------------------
> Diese E-Mail sowie eventuelle Anh?nge enthalten
> vertrauliche und / oder
> rechtlich gesch?tzte Informationen. Wenn Sie nicht der
> richtige Adressat
> sind oder diese E-Mail irrt?mlich erhalten haben,
> informieren Sie bitte
> sofort den Absender und vernichten Sie diese Mail. Das
> unerlaubte Kopieren
> oder Speichern sowie die unbefugte Weitergabe dieser E-Mail
> sind nicht
> gestattet.
>
> This e-mail and any attachments may contain
> confidential...{{dropped:18}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Thu Jul 17 16:32:50 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 17 Jul 2008 09:32:50 -0500
Subject: [R-SIG-Finance] How to format a CSV file output ?
In-Reply-To: <128512.88006.qm@web28104.mail.ukl.yahoo.com>
References: <128512.88006.qm@web28104.mail.ukl.yahoo.com>
Message-ID: <e8e755250807170732i7165b38dw932d85e7cf8607a1@mail.gmail.com>

Hi Pierre,

Using Josh's better suggestion:

write.table(format(x,nsmall=5),
 quote=FALSE,
 col.names=FALSE,
 row.names=format(index(x),"%m/%d/%Y, %H:%M"),
 sep=", ")

should get you what you want.  Note that there is a comma and then a
space in the sep= arg.

07/17/2008, 14:26, 1.940937
07/17/2008, 14:26, 1.936670
07/17/2008, 14:26, 1.921130
07/17/2008, 14:26, 1.965353
07/17/2008, 14:26, 1.958934
07/17/2008, 14:26, 1.914515
07/17/2008, 14:26, 1.991515
07/17/2008, 14:26, 1.970007
07/17/2008, 14:26, 1.994710
07/17/2008, 14:26, 1.942495


Jeff

On Thu, Jul 17, 2008 at 5:24 AM,  <pierre8r-list at yahoo.fr> wrote:
> Jeff Ryan <jeff.a.ryan <at> gmail.com> writes:
>
>>
>> Hi Pierre,
>>
>> try:
>>
>> x <- xts(matrix(runif(10,1.9,2)), Sys.time()+1:10)
>>
>> x <-
> matrix(c(strftime(as.POSIXlt(index(x)),'%m/%d/%Y,%H:%M'),x),dimnames=list(NULL,c('','Indec')),nc=2)
>>
>> write.csv(x, row.names=FALSE, quote=FALSE)
>>
>> ,Indec
>> 07/16/2008,14:59,1.92998949340545
>> 07/16/2008,14:59,1.92604529005475
>> 07/16/2008,14:59,1.90992500772700
>> 07/16/2008,14:59,1.90918286179658
>
>
> Hi jeff,
>
> Thanks for the answer.
>
> Because I need to compare two files, I have to format the csv file.
> To compare the two files I use WinMerge :
> http://winmerge.org/
>
> I have slightly changed your code to :
>
> x <- matrix(c(strftime(as.POSIXlt(index(x)),'%m/%d/%Y,%H:%M'),
>  format(x, nsmall = 5)), ncol=2)
>
> write.csv(x, row.names=FALSE, quote=FALSE)
>
> But I need a space between the comma and the double, like that :
> The current output :
> 01/08/2007,00:59,1.93025
> 01/08/2007,01:59,1.92955
> 01/08/2007,02:59,1.92885
>
> My target :
> 01/08/2007,00:59, 1.93025
> 01/08/2007,01:59, 1.92955
> 01/08/2007,02:59, 1.92885
>
> Also is it possible to remove the first line ( the V1,V2 stuff ) ?
>
> V1,V2
> 07/17/2008,12:18,1.960615
> 07/17/2008,12:18,1.940416
> 07/17/2008,12:18,1.931146
>
>
> Thanks,
>
> Pierre8r
>
>
>      ____________________________________________________
> intelligente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From j at jtoll.com  Thu Jul 17 16:57:36 2008
From: j at jtoll.com (James)
Date: Thu, 17 Jul 2008 08:57:36 -0600
Subject: [R-SIG-Finance] XTS - endpoints omits price changes
In-Reply-To: <e8e755250807161839ma3bfc12l4d1c242ad71ce72c@mail.gmail.com>
References: <8FDF9F21-93F8-41A5-94E1-C4099666491E@jtoll.com>
	<e8e755250807161839ma3bfc12l4d1c242ad71ce72c@mail.gmail.com>
Message-ID: <1D058FE4-0119-43A9-A733-71D04616D106@jtoll.com>

Thanks for the suggestions.  Between your help and Josh's help I  
think I can get it to work.  But I need to get it working using the  
time periods aligned to the option expiration cycle.  I'll keep  
working on it.  Thanks again for your help.

James




On Jul 16, 2008, at 7:39 PM, Jeff Ryan wrote:

> An alternate (simpler) approach using 'quantmod':
>
>> monthlyReturn(x[,'Close'],type='log')
>          monthly.returns
> Jan 2007     0.019013286
> Feb 2007    -0.016934043
> Mar 2007     0.004605120
> Apr 2007     0.054321097
> May 2007     0.031061724
> Jun 2007     0.003999584
> Jul 2007    -0.001471671
> Aug 2007     0.027802621
> Sep 2007     0.050668995
> Oct 2007     0.068045785
> Nov 2007    -0.069992826
> Dec 2007    -0.001755584
>
>> sum(monthlyReturn(x[,'Close'],type='log'))
> [1] 0.1693641
>
>> yearlyReturn(x[,'Close'],type='log')
>            yearly.returns
> 2007-12-31      0.1693641
>
> What is being called by periodReturn internally:
>> Delt(Cl(to.monthly(x)),type='log')
>            Delt.1.log
> Jan 2007           NA
> Feb 2007 -0.016934043
> Mar 2007  0.004605120
> Apr 2007  0.054321097
> May 2007  0.031061724
> Jun 2007  0.003999584
> Jul 2007 -0.001471671
> Aug 2007  0.027802621
> Sep 2007  0.050668995
> Oct 2007  0.068045785
> Nov 2007 -0.069992826
> Dec 2007 -0.001755584
>
> see ?periodReturn for more info on formats and options.
>
> Jeff
> On Wed, Jul 16, 2008 at 4:30 PM, James <j at jtoll.com> wrote:
>> Hi,
>>
>>
>> I've been learning to use the XTS package and have run into a  
>> problem.  If I
>> calculate monthly log normal price relatives as such:
>>
>>> df<-yahooSeries("QQQQ", from = "2007-01-01", to = "2007-12-31",
>>> returnClass=c("data.frame"))
>>> x<-as.xts(df)
>>> names(df)<-c("Open","High","Low","Close","Volume")
>>> period.apply(x$Close, INDEX=endpoints(x, 'months'), FUN=function(x)
>>> sum(diff(log(x))))
>> 2007-01-31   2007-02-28   2007-03-30   2007-04-30   2007-05-31    
>> 2007-06-29
>>   2007-07-31   2007-08-31   2007-09-28
>>  0.019013286 -0.015344398  0.009231545  0.052943687  0.027803331
>>  0.003367007 -0.010464725  0.020048207  0.050668995
>>  2007-10-31   2007-11-30   2007-12-31
>>  0.056634772 -0.051098382  0.006660162
>>
>> What happens is that the price change between the last day of the  
>> previous
>> month and the first day of the current month is ignored for all 12  
>> months.
>>  This is a problem because I should, at least in my opinion, be  
>> able to add
>> all twelve monthly changes to get the yearly change.  And that  
>> should be the
>> same as:
>>
>>> period.apply(x$Close, INDEX=endpoints(x, 'years'), FUN=function(x)
>>> sum(diff(log(x))))
>> 2007-12-31
>>  0.1693641
>>
>> But it's not, because 12 daily returns have been left out.  Is  
>> there a way
>> to change this behavior, so that any given month, or period, will  
>> include
>> all the price changes?
>>
>> Thanks,
>>
>> James
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> -- 
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com


From burkett at uri.edu  Thu Jul 17 18:10:54 2008
From: burkett at uri.edu (John P. Burkett)
Date: Thu, 17 Jul 2008 12:10:54 -0400
Subject: [R-SIG-Finance] quantmod, getSymbols, csv
Message-ID: <487F6F0E.8050903@uri.edu>

Making a first attempt use quantmod's getSymbols to load data from a 
.csv file, I have encountered a puzzling error message.
The response to
getSymbols("CREFglobaleq", from="1992-05-01", to="2008-06-30", src="csv")
is
Error in fromchar(x) :
   character string is not in a standard unambiguous format

My basic problem is ignorance about what is acceptable as "a standard 
unambiguous format."  Explanations or references to documentation would 
be greatly appreciated.

The first few rows of my CREFglobaleq.csv file read as follows:
fund     ,  price  ,  date      ,  id
CREFglob , 25.8429 , 1992-05-01 , 199205
CREFglob , 25.9774 , 1992-05-04 , 199205
CREFglob , 25.9964 , 1992-05-05 , 199205   	
CREFglob , 26.2335 , 1992-05-06 , 199205
CREFglob , 26.3368 , 1992-05-07 , 199205

Best regards,
John



-- 
John P. Burkett
Department of Environmental and Natural Resource Economics
and Department of Economics
University of Rhode Island
Kingston, RI 02881-0808
USA

phone (401) 874-9195


From jeff.a.ryan at gmail.com  Thu Jul 17 19:05:03 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 17 Jul 2008 12:05:03 -0500
Subject: [R-SIG-Finance] quantmod, getSymbols, csv
In-Reply-To: <487F6F0E.8050903@uri.edu>
References: <487F6F0E.8050903@uri.edu>
Message-ID: <e8e755250807171005y58001bb4ia82f845aa667e4e9@mail.gmail.com>

Hi John,

getSymbols.csv is not very robust at this time.  What needs to be done
is a clean generalization of the call, for cases that can't be covered
(like the date in the 3rd col, and superfluous data in cols 1 and 4,
if I am reading your data correctly).

I suspect something to identify the timestamp column, and the columns
you would like to include would make this more useful.  The other
option is to create your own custom getSymbols.John function, so
getSymbols('FILE', src="John") would do exactly what you want.  Simply
adding the below in the appropriate places to the getSymbols.csv
source would be all that is required.

For now:

x <- read.csv('CREFglobeq.csv',header=TRUE)
x <- xts(matrix(x$price,dimnames=list(index(x),'price')),as.Date(x$date))

             price
1992-05-01 25.8429
1992-05-04 25.9774
1992-05-05 25.9964
1992-05-06 26.2335
1992-05-07 26.3368

xts can be replaced by zoo, or any other time-series constructor you like.

Jeff


On Thu, Jul 17, 2008 at 11:10 AM, John P. Burkett <burkett at uri.edu> wrote:
> Making a first attempt use quantmod's getSymbols to load data from a .csv
> file, I have encountered a puzzling error message.
> The response to
> getSymbols("CREFglobaleq", from="1992-05-01", to="2008-06-30", src="csv")
> is
> Error in fromchar(x) :
>  character string is not in a standard unambiguous format
>
> My basic problem is ignorance about what is acceptable as "a standard
> unambiguous format."  Explanations or references to documentation would be
> greatly appreciated.
>
> The first few rows of my CREFglobaleq.csv file read as follows:
> fund     ,  price  ,  date      ,  id
> CREFglob , 25.8429 , 1992-05-01 , 199205
> CREFglob , 25.9774 , 1992-05-04 , 199205
> CREFglob , 25.9964 , 1992-05-05 , 199205
> CREFglob , 26.2335 , 1992-05-06 , 199205
> CREFglob , 26.3368 , 1992-05-07 , 199205
>
> Best regards,
> John
>
>
>
> --
> John P. Burkett
> Department of Environmental and Natural Resource Economics
> and Department of Economics
> University of Rhode Island
> Kingston, RI 02881-0808
> USA
>
> phone (401) 874-9195
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From jeff.a.ryan at gmail.com  Thu Jul 17 19:14:08 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 17 Jul 2008 12:14:08 -0500
Subject: [R-SIG-Finance] quantmod, getSymbols, csv
In-Reply-To: <e8e755250807171005y58001bb4ia82f845aa667e4e9@mail.gmail.com>
References: <487F6F0E.8050903@uri.edu>
	<e8e755250807171005y58001bb4ia82f845aa667e4e9@mail.gmail.com>
Message-ID: <e8e755250807171014s115748acn672d46eeae342c83@mail.gmail.com>

Something like:

> getSymbols.John
function (Symbols, env, dir = "", return.class = "xts", extension = "csv",
    ...)
{
    importDefaults("getSymbols.csv")
    this.env <- environment()
    for (var in names(list(...))) {
        assign(var, list(...)[[var]], this.env)
    }
    default.return.class <- return.class
    default.dir <- dir
    default.extension <- extension
    if (missing(verbose))
        verbose <- FALSE
    if (missing(auto.assign))
        auto.assign <- TRUE
    for (i in 1:length(Symbols)) {
        return.class <- getSymbolLookup()[[Symbols[[i]]]]$return.class
        return.class <- ifelse(is.null(return.class), default.return.class,
            return.class)
        dir <- getSymbolLookup()[[Symbols[[i]]]]$dir
        dir <- ifelse(is.null(dir), default.dir, dir)
        extension <- getSymbolLookup()[[Symbols[[i]]]]$extension
        extension <- ifelse(is.null(extension), default.extension,
            extension)
        if (verbose)
            cat("loading ", Symbols[[i]], ".....")
        if (dir == "") {
            sym.file <- paste(Symbols[[i]], extension, sep = ".")
        }
        else {
            sym.file <- file.path(dir, paste(Symbols[[i]], extension,
                sep = "."))
        }
        if (!file.exists(sym.file)) {
            cat("\nfile ", paste(Symbols[[i]], "csv", sep = "."),
                " does not exist ", "in ", dir, "....skipping\n")
            next
        }
        fr <- read.csv(sym.file, header=TRUE) #added header=TRUE
        if (verbose)
            cat("done.\n")
        fr <- xts(matrix(fr$price, dimnames=list(index(x),'price')),
as.Date(fr$date),   # added for getSymbols.John
            src = "csv", updated = Sys.time()) # added for getSymbols.John

# removed colnames call from original getSymbols.csv

        fr <- convert.time.series(fr = fr, return.class = return.class)
        Symbols[[i]] <- toupper(gsub("\\^", "", Symbols[[i]]))
        if (auto.assign)
            assign(Symbols[[i]], fr, env)
    }
    if (auto.assign)
        return(Symbols)
    return(fr)
}
<environment: namespace:quantmod>

> getSymbols('CREFglobaleq', src='John')
[1] "CREFGLOBALEQ"
> CREFGLOBALEQ
             price
1992-05-01 25.8429
1992-05-04 25.9774
1992-05-05 25.9964
1992-05-06 26.2335
1992-05-07 26.3368

I am also planning on making a skeleton function/functionality much
like newTA does for the charts - so hand modifications will be less
important.

Jeff

On Thu, Jul 17, 2008 at 12:05 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Hi John,
>
> getSymbols.csv is not very robust at this time.  What needs to be done
> is a clean generalization of the call, for cases that can't be covered
> (like the date in the 3rd col, and superfluous data in cols 1 and 4,
> if I am reading your data correctly).
>
> I suspect something to identify the timestamp column, and the columns
> you would like to include would make this more useful.  The other
> option is to create your own custom getSymbols.John function, so
> getSymbols('FILE', src="John") would do exactly what you want.  Simply
> adding the below in the appropriate places to the getSymbols.csv
> source would be all that is required.
>
> For now:
>
> x <- read.csv('CREFglobeq.csv',header=TRUE)
> x <- xts(matrix(x$price,dimnames=list(index(x),'price')),as.Date(x$date))
>
>             price
> 1992-05-01 25.8429
> 1992-05-04 25.9774
> 1992-05-05 25.9964
> 1992-05-06 26.2335
> 1992-05-07 26.3368
>
> xts can be replaced by zoo, or any other time-series constructor you like.
>
> Jeff
>
>
> On Thu, Jul 17, 2008 at 11:10 AM, John P. Burkett <burkett at uri.edu> wrote:
>> Making a first attempt use quantmod's getSymbols to load data from a .csv
>> file, I have encountered a puzzling error message.
>> The response to
>> getSymbols("CREFglobaleq", from="1992-05-01", to="2008-06-30", src="csv")
>> is
>> Error in fromchar(x) :
>>  character string is not in a standard unambiguous format
>>
>> My basic problem is ignorance about what is acceptable as "a standard
>> unambiguous format."  Explanations or references to documentation would be
>> greatly appreciated.
>>
>> The first few rows of my CREFglobaleq.csv file read as follows:
>> fund     ,  price  ,  date      ,  id
>> CREFglob , 25.8429 , 1992-05-01 , 199205
>> CREFglob , 25.9774 , 1992-05-04 , 199205
>> CREFglob , 25.9964 , 1992-05-05 , 199205
>> CREFglob , 26.2335 , 1992-05-06 , 199205
>> CREFglob , 26.3368 , 1992-05-07 , 199205
>>
>> Best regards,
>> John
>>
>>
>>
>> --
>> John P. Burkett
>> Department of Environmental and Natural Resource Economics
>> and Department of Economics
>> University of Rhode Island
>> Kingston, RI 02881-0808
>> USA
>>
>> phone (401) 874-9195
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From mittalgirish at gmail.com  Sun Jul 20 13:03:19 2008
From: mittalgirish at gmail.com (Girish Mittal)
Date: Sun, 20 Jul 2008 16:33:19 +0530
Subject: [R-SIG-Finance] Using NAG Callback Functions in R
Message-ID: <cb43cf660807200403v7d61f9aeg83cf6978ec77f888@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080720/077b0e97/attachment.pl>

From alexander.f.moreno at gmail.com  Mon Jul 21 19:05:29 2008
From: alexander.f.moreno at gmail.com (Alexander Moreno)
Date: Mon, 21 Jul 2008 12:05:29 -0500
Subject: [R-SIG-Finance] portfolio optimization-autocorrelation in asset
	returns
Message-ID: <3303a4570807211005v10ea6a23v11d6d4e9f8a9c516@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080721/44f9575d/attachment.pl>

From markleeds at verizon.net  Mon Jul 21 19:22:24 2008
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 21 Jul 2008 12:22:24 -0500 (CDT)
Subject: [R-SIG-Finance] [R]  portfolio optimization problem - use R
Message-ID: <33358447.961321216660944194.JavaMail.javamailuser@localhost>

  Hi: Patrick Burns POP software solves the problem below. I don't think 
quadprog does because of the transaction cost term but you can check out 
quadprog to confirm. I'm ccing sig-finance because someone over there 
may say/know more ?


On Mon, Jul 21, 2008 at 10:56 AM, fzp2008 wrote:

> How to use R to solve the optimisaton problem
>
> Minimize:
> ?*w^T*omega*w+mu^T*w+c^T(w-w0) for w>w0 long position
> ?*w^T*omega*w+mu^T*w-c^T(w-w0) for w<w0 short position
>
> W: is the update weight of portfolio
> Wo is the initial weight of portfolio
>
> Omega is the variance covariance matrix
> mu is the vector of return rate of stocks in the portfolio
>
> C is the vector coefficient of transaction cost
>  Is it a quandratic programming problem? Then how to write the 
> objective
> function? Or any other method to solve this?
>
> -- 
> View this message in context: 
> http://www.nabble.com/portfolio-optimization-problem---use-R-tp18570399p18570399.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From patrick at burns-stat.com  Mon Jul 21 19:48:52 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon, 21 Jul 2008 18:48:52 +0100
Subject: [R-SIG-Finance] portfolio optimization-autocorrelation in asset
 returns
In-Reply-To: <3303a4570807211005v10ea6a23v11d6d4e9f8a9c516@mail.gmail.com>
References: <3303a4570807211005v10ea6a23v11d6d4e9f8a9c516@mail.gmail.com>
Message-ID: <4884CC04.2070200@burns-stat.com>

Does the smoothing parameter of your covariance
matrix affect the results substantially?  You might
also try 'factor.model.stat' that is available in the
Public Domain area of http://www.burns-stat.com

Though it is somewhat out of character for me to
discourage minimum variance portfolios, perhaps
you want to include a prediction of the returns in
your optimization.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Alexander Moreno wrote:
> Hi,
>
> I'm running a Markowitz Optimization using an EWMA correlation forecast and
> weekly data to find the minimum variance portfolios, updated every week, for
> a basket of currencies.  I'm finding that the performance is somewhat
> wanting, but when I remove currencies with significant positive
> autocorrelation over the sample, the performance over the same sample
> improves substantially (I know, my description is somewhat vague and this is
> also cheating).  However, I believe this is due to autocorrelation violating
> assumptions in the Markowitz Optimization framework, and I'm wondering if
> anyone could point me towards the best ways to get around this problem that
> don't involve looking at the autocorrelation over a sample and then removing
> the currency from the optimization for the same sample.
>
> Thanks,
> Alex
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
>


From denisexifaras at googlemail.com  Tue Jul 22 09:34:18 2008
From: denisexifaras at googlemail.com (Denise Xifara)
Date: Tue, 22 Jul 2008 08:34:18 +0100
Subject: [R-SIG-Finance] fama-macbeth
Message-ID: <98a7ecd90807220034r52bc9ccaj5a2413d31bc6f677@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080722/a3afebad/attachment.pl>

From Christian.Prinoth at epsilonsgr.it  Tue Jul 22 09:48:58 2008
From: Christian.Prinoth at epsilonsgr.it (Christian Prinoth)
Date: Tue, 22 Jul 2008 09:48:58 +0200
Subject: [R-SIG-Finance] [R]  portfolio optimization problem - use R
In-Reply-To: <33358447.961321216660944194.JavaMail.javamailuser@localhost>
References: <33358447.961321216660944194.JavaMail.javamailuser@localhost>
Message-ID: <8D64D4652EB17048B874B0503309CFCA025E5036@epsilon2003.epsilonsgr.it>


This problem can be solved with quadprog. You just  have to introduce a few auxiliary variables. Have a look at this paper:
http://www.stanford.edu/~boyd/papers/portfolio.html

Cheers
Christian

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of markleeds at verizon.net
Sent: Monday, July 21, 2008 19:22
To: fzp2008; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] [R] portfolio optimization problem - use R

  Hi: Patrick Burns POP software solves the problem below. I don't think quadprog does because of the transaction cost term but you can check out quadprog to confirm. I'm ccing sig-finance because someone over there may say/know more ?


On Mon, Jul 21, 2008 at 10:56 AM, fzp2008 wrote:

> How to use R to solve the optimisaton problem
>
> Minimize:
> ?*w^T*omega*w+mu^T*w+c^T(w-w0) for w>w0 long position
> ?*w^T*omega*w+mu^T*w-c^T(w-w0) for w<w0 short position
>
> W: is the update weight of portfolio
> Wo is the initial weight of portfolio
>
> Omega is the variance covariance matrix mu is the vector of return
> rate of stocks in the portfolio
>
> C is the vector coefficient of transaction cost  Is it a quandratic
> programming problem? Then how to write the objective function? Or any
> other method to solve this?
>
> --
> View this message in context:
> http://www.nabble.com/portfolio-optimization-problem---use-R-tp1857039
> 9p18570399.html Sent from the R help mailing list archive at
> Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

DISCLAIMER:\ L'utilizzo non autorizzato del presente mes...{{dropped:16}}


From neil.gup at gmail.com  Tue Jul 22 15:24:12 2008
From: neil.gup at gmail.com (Neil Gupta)
Date: Tue, 22 Jul 2008 08:24:12 -0500
Subject: [R-SIG-Finance] Correlation on Tick Data
Message-ID: <a51fe2df0807220624he9d7784q958b77d9154e01bd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080722/0880735d/attachment.pl>

From Matthieu.Stigler at gmail.com  Tue Jul 22 16:14:22 2008
From: Matthieu.Stigler at gmail.com (Matthieu Stigler)
Date: Tue, 22 Jul 2008 16:14:22 +0200
Subject: [R-SIG-Finance] Correlation on Tick Data
In-Reply-To: <a51fe2df0807220624he9d7784q958b77d9154e01bd@mail.gmail.com>
References: <a51fe2df0807220624he9d7784q958b77d9154e01bd@mail.gmail.com>
Message-ID: <4885EB3E.9050003@gmail.com>

Hello

If ES and YM are time series, you maybe should first test for 
auto-correlation of the series. High auto-correlated series can lead to 
the phenomen called as spurious regression, and then the correlation 
coefficient is "too high".

Hope this helps

Mat

Neil Gupta a ?crit :
> Hello R users.
>
> I was using R to calculate correlation of midquote returns on ES and YM. ES
> and YM are highly correlated at close to .97. However when I run the
> correlation on the MQ returns the correlation is close to 0. Should I be
> expecting this or am I doing something wrong? Others have told me this
> should happen, but I do not understand why. If anyone can please explain I
> would really appreciate.
>
> Many Thanks,
> Neil
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From denisexifaras at googlemail.com  Tue Jul 22 16:26:56 2008
From: denisexifaras at googlemail.com (Denise Xifara)
Date: Tue, 22 Jul 2008 15:26:56 +0100
Subject: [R-SIG-Finance] loop for multiple regressions
In-Reply-To: <98a7ecd90807220620l1b54413eie326a2ea1ba0f8b0@mail.gmail.com>
References: <98a7ecd90807220618w6fdb6c4bjb8282aa014707e8e@mail.gmail.com>
	<98a7ecd90807220620l1b54413eie326a2ea1ba0f8b0@mail.gmail.com>
Message-ID: <98a7ecd90807220726g591e3787rcb34ea9ad88605c6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080722/f449ae40/attachment.pl>

From Matthias.Koberstein at hsbctrinkaus.de  Tue Jul 22 17:24:32 2008
From: Matthias.Koberstein at hsbctrinkaus.de (Matthias.Koberstein at hsbctrinkaus.de)
Date: Tue, 22 Jul 2008 17:24:32 +0200
Subject: [R-SIG-Finance] Antwort:  loop for multiple regressions
In-Reply-To: <98a7ecd90807220726g591e3787rcb34ea9ad88605c6@mail.gmail.com>
Message-ID: <OFB0E34AE2.2982DE5B-ONC125748E.005417BD-C125748E.00549F8C@hsbctrinkaus.de>

Hi,
Basically the problem is that the regression returns a LIST not a single
value.
You can access the single values of the list with $valuename.
Please look into help(lm) for the values contained in the list.
If you select for example
Example[i]<-lm(y~x)$coefficient[1]

it should save the intercept to the matrix Example at place i.

Cheers

Matthias S. Koberstein
__________________________________
HSBC Trinkaus
Structured Solutions Group
K?nigsalle 21/23, 40212 D?sseldorf

Phone: +49 211 910 4412
e-mail: matthias.koberstein at hsbctrinkaus.de



**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential and / or
privileged information. If you are not the intended recipient or have
received this e-mail in error, please notify the sender immediately and
destroy this e-mail . Any unauthorized copying, storing, disclosure or
distribution of the contents of this e-mail is strictly forbidden.

---------------------------------------------------------------------
HSBC Trinkaus & Burkhardt AG
Sitz: D?sseldorf, K?nigsallee 21/23, 40212 D?sseldorf, Handelsregister:
Amtsgericht D?sseldorf HRB 54447
Mitglieder des Vorstands: Andreas Schmitz (Sprecher), Paul Hagen, Dr. Olaf
Huth, Carola Gr?fin v. Schmettow
Vorsitzender des Aufsichtsrats: Dr. Sieghardt Rometsch


                                                                           
             "Denise Xifara"                                               
             <denisexifaras at go                                             
             oglemail.com>                                              An 
             Gesendet von:               r-sig-finance at stat.math.ethz.ch   
             r-sig-finance-bou                                       Kopie 
             nces at stat.math.et                                             
             hz.ch                                                   Thema 
                                         [R-SIG-Finance] loop for multiple 
             Fax-Deckblatt:              regressions                       
             HSBCTuB                                                       
             22.07.2008 17:05                                              
                                                                           
                                                                           
                                                                           
                                                                           




   Dear all,
I have the following data in excel:

    day1   y 1 2 3 2 3 x1 0.2 0.3 0.4 0.3 0.2 x2 7 3.4 2 8 6  day2   y 2 4
3
2 2 x1 0.4 0.5 0.3 0.3 0.2 x2 7 8 9.1 6 5

I have the following problems:
first of all, when I ask R to read the file (with the package xlsReadWrite
and the command read.xls) it has a problem with the fact that the left most
corner is labelled the same way, so in order for it to work I need to do:



 day1



 y1
 1
 2
 3
 2
 3
 x11
 0.2
 0.3
 0.4
 0.3
 0.2
 x21
 7
 3.4
 2
 8
 6


 day2



 y2
 2
 4
 3
 2
 2
 x12
 0.4
 0.5
 0.3
 0.3
 0.2
 x22
 7
 8
 9.1
 6
 5

Next I need to do the regression y~ x1+x2, for the different days.
I have converted the above data in the form of a matrix and labelled it
mat.
The following loop works:

results<-c(0,0,0,0,0)
for (i in c(1,5)){
results[i]<-lm(mat[i,]~mat[i+1,]+mat[i+2,])}
results

And the results are:

Warning messages:
1: In results[i] <- lm(mat[i, ] ~ mat[i + 1, ] + mat[i + 2, ]) :
  number of items to replace is not a multiple of replacement length
2: In results[i] <- lm(mat[i, ] ~ mat[i + 1, ] + mat[i + 2, ]) :
  number of items to replace is not a multiple of replacement length
> results
[[1]]
 (Intercept) mat[i + 1, ] mat[i + 2, ]
   2.4918415    1.5151515   -0.1356220
[[2]]
[1] 0
[[3]]
[1] 0
[[4]]
[1] 0
[[5]]
 (Intercept) mat[i + 1, ] mat[i + 2, ]
  -0.3741935    3.4193548    0.2580645

The method does work but it is not ideal.  The problem is that the real
data
that I will be using will involve regressions of 20 or so variables over
3000 days. I also want to avoid finding the exact values that "i" should
take inside the loop.
Is there a better way of carrying out these regressions so that I do not
need to relabel everything, the results are faster and it is clear and easy
to isolate the results of the linear models for each day?

Thank you very much everyone in advance,
Regards,
Denise

             [[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From markleeds at verizon.net  Tue Jul 22 17:38:44 2008
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 22 Jul 2008 10:38:44 -0500 (CDT)
Subject: [R-SIG-Finance] Correlation on Tick Data
Message-ID: <23870565.5176361216741124953.JavaMail.javamailuser@localhost>

  just to elaborate a bit more on what matt said.

you need to make sure you time series are stationary before you 
correlate them.  usually one does this by using a unit root test but, in 
your case, since you are dealing with currencies, as long as you are 
dealing with the returns streamsand not the prices themselves, there's 
really no need to use the unit root test.  returns should be stationary 
( in general ).   if you're dealing with prices, then correlations don't 
make sense because prices aren't ( in general ),
or atleast i've never seen prices that were.



On Tue, Jul 22, 2008 at 10:14 AM, Matthieu Stigler wrote:

> Hello
>
> If ES and YM are time series, you maybe should first test for 
> auto-correlation of the series. High auto-correlated series can lead 
> to the phenomen called as spurious regression, and then the 
> correlation coefficient is "too high".
>
> Hope this helps
>
> Mat
>
> Neil Gupta a ?crit :
>> Hello R users.
>>
>> I was using R to calculate correlation of midquote returns on ES and 
>> YM. ES
>> and YM are highly correlated at close to .97. However when I run the
>> correlation on the MQ returns the correlation is close to 0. Should I 
>> be
>> expecting this or am I doing something wrong? Others have told me 
>> this
>> should happen, but I do not understand why. If anyone can please 
>> explain I
>> would really appreciate.
>>
>> Many Thanks,
>> Neil
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From ksspriggs at gmail.com  Tue Jul 22 18:09:38 2008
From: ksspriggs at gmail.com (Kenneth Spriggs)
Date: Tue, 22 Jul 2008 11:09:38 -0500
Subject: [R-SIG-Finance] Correlation on Tick Data
In-Reply-To: <23870565.5176361216741124953.JavaMail.javamailuser@localhost>
References: <23870565.5176361216741124953.JavaMail.javamailuser@localhost>
Message-ID: <82b060880807220909h5841b3ffr9347db78577f3966@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080722/8d47f6e6/attachment.pl>

From samohyl at yahoo.com  Tue Jul 22 18:29:16 2008
From: samohyl at yahoo.com (BOB SAMOHYL)
Date: Tue, 22 Jul 2008 09:29:16 -0700 (PDT)
Subject: [R-SIG-Finance] Correlation on Tick Data
In-Reply-To: <23870565.5176361216741124953.JavaMail.javamailuser@localhost>
Message-ID: <493001.61939.qm@web54108.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080722/3c1c9fdf/attachment.pl>

From burkett at uri.edu  Tue Jul 22 18:32:21 2008
From: burkett at uri.edu (John P. Burkett)
Date: Tue, 22 Jul 2008 12:32:21 -0400
Subject: [R-SIG-Finance] fPortfolio, as.timeSeries
Message-ID: <48860B95.6090204@uri.edu>

Using fPortfolio as part of R version 2.6.1 running under Gentoo Linux, 
I'm having trouble loading returns data in a format recognized by the 
package.

In a separate R program I generated 22 time series named IENAX.ts,..., 
WHVintl.ts, and assembled them with a command of the form
returns <- ts.intersect(IENAX.ts,..., WHVintl.ts). The period covered by 
returns is Oct 2003 - March 2008. I saved the result in two formats, as 
follows:
write.csv(returns, file="returns.csv")
save(returns, file="returns.RData")

My troubles start when I try to get fPortfolio to process the returns 
data.  If I give the commands
library(fPortfolio)
returnsdata <- read.csv("returns.csv",header=T,sep=",")
attach(returnsdata)
Data = as.timeSeries(returnsdata)
the response is
Warning messages:
1: In .whichFormat(charvec, ...) : Could not determine time(date) format
2: In .whichFormat(charvec, ...) : Could not determine time(date) format
The rows of Data are labeled with "dummy dates" starting with 1970-01-01.

If I give the commands
library(fPortfolio)
load("returns.RData")
Data = as.timeSeries(returns)
then I find that returns (a 54x22 matrix with row labels Oct 2003,..., 
Mar 2008) has been turned into Data (a vector of 1188 elements assembled 
by stacking the columns of returns and labeled with the dummy dates 
starting with 1970-01-01).

Suggestions for properly loading and transforming data would be most 
welcome. If .R, .csv, or .RData files would be of diagnostic value, I 
will happily supply them.

John







-- 
John P. Burkett
Department of Environmental and Natural Resource Economics
and Department of Economics
University of Rhode Island
Kingston, RI 02881-0808
USA

phone (401) 874-9195


From markleeds at verizon.net  Tue Jul 22 18:43:59 2008
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 22 Jul 2008 11:43:59 -0500 (CDT)
Subject: [R-SIG-Finance] Correlation on Tick Data
Message-ID: <11732214.5285691216745039641.JavaMail.javamailuser@localhost>

  Thanks for pointing that out. I understand the linearity ( you're 
saying i think that correlation will only pick up linear relation ) but 
why normality ?


On Tue, Jul 22, 2008 at 12:29 PM, BOB SAMOHYL wrote:

> I would go farther than just stationarity and include normality, and 
> linearity in the relation, three suppositions for the correlation 
> coefficient that are rarely examined.
> Robert Wayne Samohyl, Ph.D. www.qualimetria.ufsc.br
> fones: 55-48-3721-7001 University 55-48-9608-5056 celular  ??
>
> --- Em ter, 22/7/08, markleeds at verizon.net <markleeds at verizon.net> 
> escreveu:
> De: markleeds at verizon.net <markleeds at verizon.net>
> Assunto: Re: [R-SIG-Finance] Correlation on Tick Data
> Para: "Matthieu Stigler" <Matthieu.Stigler at gmail.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Data: Ter??a-feira, 22 de Julho de 2008, 12:38
>
> just to elaborate a bit more on what matt said.
>
> you need to make sure you time series are stationary before you 
> correlate them.  usually one does this by using a unit root test but, 
> in your case, since you are dealing with currencies, as long as you 
> are dealing with the returns streamsand not the prices themselves, 
> there's really no need to use the unit root test.  returns should be 
> stationary ( in general ).   if you're dealing with prices, then 
> correlations
> don't make sense because prices aren't ( in general ),
> or atleast i've never seen prices that were.
>
>
>
> On Tue, Jul 22, 2008 at 10:14 AM, Matthieu Stigler wrote:
>
>> Hello
>>
>> If ES and YM are time series, you maybe should first test for 
>> auto-correlation of the series. High auto-correlated series can lead 
>> to the phenomen called as spurious regression, and then the 
>> correlation coefficient is "too high".
>>
>> Hope this helps
>>
>> Mat
>>
>> Neil Gupta a ??crit :
>>> Hello R users.
>>>
>>> I was using R to calculate correlation of midquote returns on ES and 
>>> YM. ES
>>> and YM are highly correlated at close to .97. However when I run the
>>> correlation on the MQ returns the correlation is close to 0. Should 
>>> I be
>>> expecting this or am I doing something wrong? Others have told me 
>>> this
>>> should happen, but I do not understand why. If anyone can please 
>>> explain I
>>> would really appreciate.
>>>
>>> Many Thanks,
>>> Neil
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
> [[elided Yahoo spam]]
> ovo com a sua cara @ymail.com ou @rocketmail.com.
>
> 	[[alternative HTML version deleted]]
>
>
>
>      ------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From samohyl at yahoo.com  Tue Jul 22 19:21:50 2008
From: samohyl at yahoo.com (BOB SAMOHYL)
Date: Tue, 22 Jul 2008 10:21:50 -0700 (PDT)
Subject: [R-SIG-Finance] Correlation on Tick Data
In-Reply-To: <11732214.5285691216745039641.JavaMail.javamailuser@localhost>
Message-ID: <579761.22314.qm@web54102.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080722/a28ae2c4/attachment.pl>

From markleeds at verizon.net  Tue Jul 22 19:25:27 2008
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 22 Jul 2008 12:25:27 -0500 (CDT)
Subject: [R-SIG-Finance] Correlation on Tick Data
Message-ID: <24221559.5347871216747527348.JavaMail.javamailuser@localhost>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080722/36a5ea7c/attachment.pl>

From ksspriggs at gmail.com  Tue Jul 22 20:22:08 2008
From: ksspriggs at gmail.com (Kenneth Spriggs)
Date: Tue, 22 Jul 2008 13:22:08 -0500
Subject: [R-SIG-Finance] Correlation on Tick Data
In-Reply-To: <24221559.5347871216747527348.JavaMail.javamailuser@localhost>
References: <24221559.5347871216747527348.JavaMail.javamailuser@localhost>
Message-ID: <82b060880807221122r4f8a7abek6eaa21a2b1e13b4b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080722/1d39bd9f/attachment.pl>

From patrick at burns-stat.com  Tue Jul 22 20:25:46 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 22 Jul 2008 19:25:46 +0100
Subject: [R-SIG-Finance] Correlation on Tick Data
In-Reply-To: <579761.22314.qm@web54102.mail.re2.yahoo.com>
References: <579761.22314.qm@web54102.mail.re2.yahoo.com>
Message-ID: <4886262A.5000006@burns-stat.com>

If you are doing hypothesis testing, then much better
is to use a permutation test -- no distribution assumption
required.  Permutation tests are talked about in
http://www.burns-stat.com/pages/Tutor/bootstrap_resampling.html

Box-Cox transformation is not going to help with long-tails,
which is how returns are distributed, especially intraday returns.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

BOB SAMOHYL wrote:
> Mark, 
>
> To reject the null of no correlation, an hypothsis test based on the normal distribution. If normality is not the base assumption your working from then p-values, significance tests and conf. intervals dont mean much (the value of the coefficient is not reliable) or youll have to find a more appropriate distribution for your data. Or transform the data to normal. There??s a box-cox transformation in R that can resolve this. Normality may represent returns data most of the time (maybe not) but imagine?  that it doesnt in a specific case and this discrepancy finds its way into a portfolio. Bob
>  
>
> --- Em ter, 22/7/08, markleeds at verizon.net <markleeds at verizon.net> escreveu:
> De: markleeds at verizon.net <markleeds at verizon.net>
> Assunto: Re: [R-SIG-Finance] Correlation on Tick Data
> Para: samohyl at yahoo.com
> Cc: r-sig-finance at stat.math.ethz.ch
> Data: Ter??a-feira, 22 de Julho de 2008, 13:43
>
> Thanks for pointing that out. I understand the linearity ( you're 
> saying i think that correlation will only pick up linear relation ) but 
> why normality ?
>
>
> On Tue, Jul 22, 2008 at 12:29 PM, BOB SAMOHYL wrote:
>
>   
>> I would go farther than just stationarity and include normality, and 
>> linearity in the relation, three suppositions for the correlation 
>> coefficient that are rarely examined.
>> Robert Wayne Samohyl, Ph.D. www.qualimetria.ufsc.br
>> fones: 55-48-3721-7001 University 55-48-9608-5056 celular  ??????
>>
>> --- Em ter, 22/7/08, markleeds at verizon.net <markleeds at verizon.net> 
>> escreveu:
>> De: markleeds at verizon.net <markleeds at verizon.net>
>> Assunto: Re: [R-SIG-Finance] Correlation on Tick Data
>> Para: "Matthieu Stigler" <Matthieu.Stigler at gmail.com>
>> Cc: r-sig-finance at stat.math.ethz.ch
>> Data: Ter??????a-feira, 22 de Julho de 2008, 12:38
>>
>> just to elaborate a bit more on what matt said.
>>
>> you need to make sure you time series are stationary before you 
>> correlate them.  usually one does this by using a unit root test but, 
>> in your case, since you are dealing with currencies, as long as you 
>> are dealing with the returns streamsand not the prices themselves, 
>> there's really no need to use the unit root test.  returns should be 
>> stationary ( in general ).   if you're dealing with prices, then 
>> correlations
>> don't make sense because prices aren't ( in general ),
>> or atleast i've never seen prices that were.
>>
>>
>>
>> On Tue, Jul 22, 2008 at 10:14 AM, Matthieu Stigler wrote:
>>
>>     
>>> Hello
>>>
>>> If ES and YM are time series, you maybe should first test for 
>>> auto-correlation of the series. High auto-correlated series can lead 
>>> to the phenomen called as spurious regression, and then the 
>>> correlation coefficient is "too high".
>>>
>>> Hope this helps
>>>
>>> Mat
>>>
>>> Neil Gupta a ??????crit :
>>>       
>>>> Hello R users.
>>>>
>>>> I was using R to calculate correlation of midquote returns on ES
>>>>         
> and 
>   
>>>> YM. ES
>>>> and YM are highly correlated at close to .97. However when I run
>>>>         
> the
>   
>>>> correlation on the MQ returns the correlation is close to 0.
>>>>         
> Should 
>   
>>>> I be
>>>> expecting this or am I doing something wrong? Others have told me 
>>>> this
>>>> should happen, but I do not understand why. If anyone can please 
>>>> explain I
>>>> would really appreciate.
>>>>
>>>> Many Thanks,
>>>> Neil
>>>>
>>>> 	[[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>         
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>       
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>> [[elided Yahoo spam]]
>> ovo com a sua cara @ymail.com ou @rocketmail.com.
>>
>> 	[[alternative HTML version deleted]]
>>
>>
>>
>>      ------------------------------
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>     
>
>
>       Novos endere??os, o Yahoo! que voc?? conhece. Crie um email novo com a sua cara @ymail.com ou @rocketmail.com.
> http://br.new.mail.yahoo.com/addresses
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From ezivot at u.washington.edu  Tue Jul 22 21:37:21 2008
From: ezivot at u.washington.edu (Eric Zivot)
Date: Tue, 22 Jul 2008 12:37:21 -0700 (PDT)
Subject: [R-SIG-Finance] Correlation on Tick Data
In-Reply-To: <82b060880807220909h5841b3ffr9347db78577f3966@mail.gmail.com>
Message-ID: <Pine.LNX.4.43.0807221237210.3079@hymn11.u.washington.edu>

Computing correlation on tick data is very tricky. You should consult the realized covariance literature (e.g. papers by Neil shephard and his co-authors) for an introduction to the issues of getting sensible correlations from tick data. Many issues are involved such as aligning data to a common time clock, how often to sample (how many ticks to use), stale quotes for some assets, how to deal with microstructure noise (bid ask bounce etc), etc. just computing an naive correlation on tick data is not a good idea.

****************************************************************
*  Eric Zivot                  			               *
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *                                                           *
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Tue, 22 Jul 2008, Kenneth Spriggs wrote:

> Neil,
>
> I don't quite understand your data set.
>
>> I performed this on tick data at different tick time frequencies. For
> instance I can run a whole day's tick updates(quotes) at an >interval of
> 50,000 ticks.
>
> What does this mean exactly?
>
> You're doing cor(YM, ES, method = "whatever")  I assume.  So ES and YM have
> to have the same number of points.  If you take the returns from 50,000
> ticks this could be drastically different time frames.  Do you see my
> concern?
>
>
>
> On Tue, Jul 22, 2008 at 10:38 AM, <markleeds at verizon.net> wrote:
>
>>  just to elaborate a bit more on what matt said.
>>
>> you need to make sure you time series are stationary before you correlate
>> them.  usually one does this by using a unit root test but, in your case,
>> since you are dealing with currencies, as long as you are dealing with the
>> returns streamsand not the prices themselves, there's really no need to use
>> the unit root test.  returns should be stationary ( in general ).   if
>> you're dealing with prices, then correlations don't make sense because
>> prices aren't ( in general ),
>> or atleast i've never seen prices that were.
>>
>>
>>
>>
>> On Tue, Jul 22, 2008 at 10:14 AM, Matthieu Stigler wrote:
>>
>>  Hello
>>>
>>> If ES and YM are time series, you maybe should first test for
>>> auto-correlation of the series. High auto-correlated series can lead to the
>>> phenomen called as spurious regression, and then the correlation coefficient
>>> is "too high".
>>>
>>> Hope this helps
>>>
>>> Mat
>>>
>>> Neil Gupta a ?crit :
>>>
>>>> Hello R users.
>>>>
>>>> I was using R to calculate correlation of midquote returns on ES and YM.
>>>> ES
>>>> and YM are highly correlated at close to .97. However when I run the
>>>> correlation on the MQ returns the correlation is close to 0. Should I be
>>>> expecting this or am I doing something wrong? Others have told me this
>>>> should happen, but I do not understand why. If anyone can please explain
>>>> I
>>>> would really appreciate.
>>>>
>>>> Many Thanks,
>>>> Neil
>>>>
>>>>        [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> 	[[alternative HTML version deleted]]
>
>


From ezivot at u.washington.edu  Tue Jul 22 21:51:23 2008
From: ezivot at u.washington.edu (Eric Zivot)
Date: Tue, 22 Jul 2008 12:51:23 -0700 (PDT)
Subject: [R-SIG-Finance] Correlation on Tick Data
In-Reply-To: <24221559.5347871216747527348.JavaMail.javamailuser@localhost>
Message-ID: <Pine.LNX.4.43.0807221251230.8790@hymn31.u.washington.edu>

I forgot to mention that my former Phd student Scott Payseur wrote the Realized R package that has functions to compute realized variance and covariance using all of the latest techniques.


****************************************************************
*  Eric Zivot                  			               *
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *                                                           *
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Tue, 22 Jul 2008 markleeds at verizon.net wrote:

>
>  gotcha. so what you are saying is that you need normality for
> hypothesis testing, statistical inference purposes. that's fine and
> thanks again
> for pointing that out.
>
>
>
>
> On Tue, Jul 22, 2008 at  1:21 PM, BOB SAMOHYL wrote:
>
>
> Mark,
>
> To reject the null of no correlation, an hypothsis test based on the
> normal distribution. If normality is not the base assumption your
> working from then p-values, significance tests and conf. intervals dont
> mean much (the value of the coefficient is not reliable) or youll have
> to find a more appropriate distribution for your data. Or transform the
> data to normal. There??s a box-cox transformation in R that can resolve
> this. Normality may represent returns data most of the time (maybe not)
> but imagine  that it doesnt in a specific case and this discrepancy
> finds its way into a portfolio. Bob
>
>
> --- Em ter, 22/7/08, markleeds at verizon.net <markleeds at verizon.net>
> escreveu:
> De:  markleeds at verizon.net <markleeds at verizon.net>
> Assunto: Re: [R-SIG-Finance] Correlation on Tick Data
> Para: samohyl at yahoo.com
> Cc: r-sig-finance at stat.math.ethz.ch
> Data: Ter??a-feira, 22 de Julho de 2008, 13:43
>
> Thanks for pointing that out. I understand the linearity ( you're
> saying i think that correlation will only pick up linear relation ) but
> why normality ?
>
> On Tue, Jul 22, 2008 at 12:29 PM, BOB SAMOHYL wrote:
>
>> I would go farther than just stationarity and include normality, and
>> linearity in the relation, three suppositions for the correlation
>> coefficient that are rarely examined. Robert Wayne Samohyl, Ph.D.
>> www.qualimetria.ufsc.br fones: 55-48-3721-7001 University
>> 55-48-9608-5056 celular  ??????
>> --- Em ter, 22/7/08, markleeds at verizon.net <markleeds at verizon.net>
>> escreveu: De: markleeds at verizon.net
>  <markleeds at verizon.net>
>> Assunto: Re: [R-SIG-Finance] Correlation on Tick Data Para: "Matthieu
>> Stigler" <Matthieu.Stigler at gmail.com> Cc:
>> r-sig-finance at stat.math.ethz.ch Data: Ter??????a-feira, 22 de Julho de
>> 2008, 12:38
>> just to elaborate a bit more on what matt said.
>> you need to make sure you time series are stationary before you
>> correlate them.  usually one does this by using a unit root test but,
>> in your case, since you are dealing with currencies, as long as you
>> are dealing with the returns streamsand not the prices themselves,
>> there's really no need to use the unit root test.  returns should be
>> stationary ( in general ).   if you're dealing with prices, then
>> correlations don't make sense because prices aren't ( in general ), or
>> atleast i've never seen prices that were.
>>
>>
>> On
>  Tue, Jul 22, 2008 at 10:14 AM, Matthieu Stigler wrote:
>>
>>> Hello
>>> If ES and YM are time series, you maybe should first test for
>>> auto-correlation of the series. High auto-correlated series can lead
>>> to the phenomen called as spurious regression, and then the
>>> correlation coefficient is "too high".
>>> Hope this helps
>>> Mat
>>> Neil Gupta a ??????crit :
>>>> Hello R users.
>>>> I was using R to calculate correlation of midquote returns on ES
> and
>>>> YM. ES and YM are highly correlated at close to .97. However when I
>>>> run
> the
>>>> correlation on the MQ returns the correlation is close to 0.
> Should
>>>> I be expecting this or am I doing something wrong? Others have told
>>>> me this
>  should happen, but I do not understand why. If anyone can please
>>>> explain I would really appreciate.
>>>> Many Thanks, Neil
>>>> 	[[alternative HTML version deleted]]
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance --
>>>> Subscriber-posting only. -- If you want to post, subscribe first.
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance --
>>> Subscriber-posting only. -- If you want to post, subscribe first.
>>
>> _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance --
>> Subscriber-posting only. -- If you want to post, subscribe first.
>>
>> [[elided Yahoo spam]] ovo com a sua cara @ymail.com ou
>> @rocketmail.com.
>> 	[[alternative HTML version deleted]]
>>
>>
>>      ------------------------------
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance --
>> Subscriber-posting only. -- If you want to post, subscribe first.
> ___________________________________
>
> Novos endere??os, o Yahoo! que voc?? conhece. Crie um email novo
> <http://br.rd.yahoo.com/mail/taglines/mail/*http://br.new.mail.yahoo.com/addresses>
> com a sua cara @ymail.com ou @rocketmail.com.
>
> 	[[alternative HTML version deleted]]
>
>


From giuseppe1.milicia at hsbcib.com  Wed Jul 23 11:11:06 2008
From: giuseppe1.milicia at hsbcib.com (giuseppe1.milicia at hsbcib.com)
Date: Wed, 23 Jul 2008 10:11:06 +0100
Subject: [R-SIG-Finance] (no subject)
Message-ID: <OFFEA5F53E.23B7E2EE-ON8025748F.003054AC-8025748F.003275D6@hsbcib.com>


Alex,

In the next few days I'm going to do something very similar :)

I was wondering if you could share some more information? What R
packages/functions did you use? Just quadprog, or did you find something
better suited?

// Giuseppe

Message: 1
Date: Mon, 21 Jul 2008 12:05:29 -0500
From: "Alexander Moreno" <alexander.f.moreno at gmail.com>
Subject: [R-SIG-Finance] portfolio optimization-autocorrelation in
             asset             returns
To: r-sig-finance at stat.math.ethz.ch
Message-ID:
             <3303a4570807211005v10ea6a23v11d6d4e9f8a9c516 at mail.gmail.com>
Content-Type: text/plain

Hi,

I'm running a Markowitz Optimization using an EWMA correlation forecast and
weekly data to find the minimum variance portfolios, updated every week,
for
a basket of currencies.  I'm finding that the performance is somewhat
wanting, but when I remove currencies with significant positive
autocorrelation over the sample, the performance over the same sample
improves substantially (I know, my description is somewhat vague and this
is
also cheating).  However, I believe this is due to autocorrelation
violating
assumptions in the Markowitz Optimization framework, and I'm wondering if
anyone could point me towards the best ways to get around this problem that
don't involve looking at the autocorrelation over a sample and then
removing
the currency from the optimization for the same sample.

Thanks,
Alex

************************************************************
HSBC Bank plc may be solicited in the course of its placement efforts for a
new issue, by investment clients of the firm for whom the Bank as a firm
already provides other services. It may equally decide to allocate to its
own proprietary book or with an associate of HSBC Group. This represents a
potential conflict of interest. HSBC Bank plc has internal arrangements
designed to ensure that the firm would give unbiased and full advice to the
corporate finance client about the valuation and pricing of the offering as
well as internal systems, controls and procedures to identify and manage
conflicts of interest.

HSBC Bank plc
Registered Office: 8 Canada Square, London E14 5HQ, United Kingdom
Registered in England - Number 14259
Authorised and regulated by the Financial Services Authority.
************************************************************


-----------------------------------------
SAVE PAPER - THINK BEFORE YOU PRINT!

This transmission has been issued by a member of the HSBC Group
"HSBC" for the information of the addressee only and should not be
reproduced and/or distributed to any other person. Each page
attached hereto must be read in conjunction with any disclaimer
which forms part of it. Unless otherwise stated, this transmission
is neither an offer nor the solicitation of an offer to sell or
purchase any investment. Its contents are based on information
obtained from sources believed to be reliable but HSBC makes no
representation and accepts no responsibility or liability as to its
completeness or accuracy.


From pierre8r-list at yahoo.fr  Wed Jul 23 12:35:52 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Wed, 23 Jul 2008 10:35:52 +0000 (GMT)
Subject: [R-SIG-Finance] Feature request to the xls package. Add to.4hours(x,
	name)
Message-ID: <681406.91906.qm@web28106.mail.ukl.yahoo.com>

Hello,

Feature request to the xls package.

Add 
to.4hours(x,name) to the xls package.

Regards,

Pierre8r


      ____________________________________________________
intelligente http://mail.yahoo.fr


From chalabi at phys.ethz.ch  Wed Jul 23 14:09:31 2008
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Wed, 23 Jul 2008 14:09:31 +0200
Subject: [R-SIG-Finance] fPortfolio, as.timeSeries
In-Reply-To: <48860B95.6090204@uri.edu>
References: <48860B95.6090204@uri.edu>
Message-ID: <20080723140931.198aab44@mimi>

>>>> "JPB" == "John P. Burkett" <burkett at uri.edu>
>>>> on Tue, 22 Jul 2008 12:32:21 -0400

   JPB> My troubles start when I try to get fPortfolio to process the returns 
   JPB> data.  If I give the commands
   JPB> library(fPortfolio)
   JPB> returnsdata <- read.csv("returns.csv",header=T,sep=",")
   JPB> attach(returnsdata)
   JPB> Data = as.timeSeries(returnsdata)
   JPB> the response is
   JPB> Warning messages:
   JPB> 1: In .whichFormat(charvec, ...) : Could not determine time(date) format
   JPB> 2: In .whichFormat(charvec, ...) : Could not determine time(date) format
   JPB> The rows of Data are labeled with "dummy dates" starting with 1970-01-01.

You need to specify the format of your charvec in timeSeries()

this could be done with

Data = timeSeries(data = returnsdata, format = "[with the format of your timestamps]")

hope this helps,
Yohan

-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch
www.rmetrics.org


From r_sig_finance at greenmail.ch  Wed Jul 23 14:24:42 2008
From: r_sig_finance at greenmail.ch (r_sig_finance)
Date: Wed, 23 Jul 2008 14:24:42 +0200
Subject: [R-SIG-Finance]  estimating non-linear state space models
Message-ID: <39F7C064586B194EB8AAE4D74AB096A1B32812@EXVS02.mcis.agrinet.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080723/784f20e0/attachment.pl>

From chalabi at phys.ethz.ch  Wed Jul 23 14:25:11 2008
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Wed, 23 Jul 2008 14:25:11 +0200
Subject: [R-SIG-Finance] timeDate class query
In-Reply-To: <1865010807101939u217fb775t90173bb15ce6fab7@mail.gmail.com>
References: <1865010807101939u217fb775t90173bb15ce6fab7@mail.gmail.com>
Message-ID: <20080723142511.0f966bf8@mimi>

>>>> "IS" == "Ian Seow" <ianseow at gmail.com>
>>>> on Fri, 11 Jul 2008 10:39:08 +0800

   IS> Hi, I have a query on the timeDate class in the fCalendar Package.
   IS> I have a large data set of intraday observations (in GMT) containing
   IS> 407,100 POSIXct objects which I want to convert into timeDate objects
   IS> and then view the result in my local time zone ('Asia/Singapore').
   IS> The conversion works fine, however, it seems that timeDate does not
   IS> have 'Dims' and hence does not allow for length(), head() and tail()
   IS> to be performed.

Do you have the latest version of fCalendar?

length(timeCalendar()) works fine with the current version of fCalendar available at CRAN.


regards,
Yohan

-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch
www.rmetrics.org


From cwrward at gmail.com  Wed Jul 23 14:30:03 2008
From: cwrward at gmail.com (Charles Ward)
Date: Wed, 23 Jul 2008 13:30:03 +0100
Subject: [R-SIG-Finance] portfolio optimization-autocorrelation in asset
	returns
In-Reply-To: <3303a4570807211005v10ea6a23v11d6d4e9f8a9c516@mail.gmail.com>
References: <3303a4570807211005v10ea6a23v11d6d4e9f8a9c516@mail.gmail.com>
Message-ID: <bd9aa36b0807230530y45a93cdcja275a74483920f4d@mail.gmail.com>

Alex

Autocorrelaton in returns often occurs in thin markets, especially in
real estate. One conventional response in real estate is to use a
filter to de-smooth the returns.
One such filter is De-smoothed R(t) = ( ObsR(t) - k*ObsR(t-1) )/ (1-k)
where k = either the coefficient of the autocorrelation or something
less (chosen subjectively) and ObsR = the observed or reported
returns.  The result is to increase the variance of the return series.
The underlying assumption is that the observed returns reported are a
weighted average of the "true" de-smoothed return and the previous
period's return. In some cases, the filter will increase the
correlation between the series and other market returns. With a highly
autocorrelated series, the de-smoothed series becomes implausibly
volatile so researchers have adapted the approach to modify the effect
by reducing k until the de-smoothed series looks "reasonable".

In a study for the UK Investment Property Forum, "Index Smoothing and
the Volatility of UK Commercial Property", March 2007, the authors
reported that in a survey of 13 of the largest real estate fund
managers in the UK, 9 adjusted the returns using this kind of filter.

Charles Ward

2008/7/21 Alexander Moreno <alexander.f.moreno at gmail.com>:
> Hi,
>
> I'm running a Markowitz Optimization using an EWMA correlation forecast and
> weekly data to find the minimum variance portfolios, updated every week, for
> a basket of currencies.  I'm finding that the performance is somewhat
> wanting, but when I remove currencies with significant positive
> autocorrelation over the sample, the performance over the same sample
> improves substantially (I know, my description is somewhat vague and this is
> also cheating).  However, I believe this is due to autocorrelation violating
> assumptions in the Markowitz Optimization framework, and I'm wondering if
> anyone could point me towards the best ways to get around this problem that
> don't involve looking at the autocorrelation over a sample and then removing
> the currency from the optimization for the same sample.
>
> Thanks,
> Alex
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Charles Ward
ICMA Centre - The Business School for Financial Markets
The University of Reading
Whiteknights Park
Reading RG6 6BA - UK

Web:www.icmacentre.rdg.ac.uk
Tel: +44 (0)118 378 6292

Email: c.w.r.ward at reading.ac.uk
------------------------------------------------------------------------
----------------

The information contained in this communication is confi...{{dropped:12}}


From r_sig_finance at greenmail.ch  Wed Jul 23 15:07:13 2008
From: r_sig_finance at greenmail.ch (r_sig_finance)
Date: Wed, 23 Jul 2008 15:07:13 +0200
Subject: [R-SIG-Finance]  estimating non-linear state space models
References: <39F7C064586B194EB8AAE4D74AB096A1B32812@EXVS02.mcis.agrinet.local>
Message-ID: <39F7C064586B194EB8AAE4D74AB096A1B32813@EXVS02.mcis.agrinet.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080723/4a26071d/attachment.pl>

From jeff.a.ryan at gmail.com  Wed Jul 23 15:08:00 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 23 Jul 2008 08:08:00 -0500
Subject: [R-SIG-Finance] Feature request to the xls package. Add
	to.4hours(x, name)
In-Reply-To: <681406.91906.qm@web28106.mail.ukl.yahoo.com>
References: <681406.91906.qm@web28106.mail.ukl.yahoo.com>
Message-ID: <e8e755250807230608j5f3cffd2u824604cc1f83e562@mail.gmail.com>

Hi Pierre,

I suspect you mean the 'xts' package.  If so try:

to.hourly(x, k=4)

The 'k' arg to the to.period function (which to.XXXX calls) is for that:

?to.period

It gets passed in the ... args, so you must name it.  I will see if
adding it to the formal args makes sense.

Jeff

On Wed, Jul 23, 2008 at 5:35 AM,  <pierre8r-list at yahoo.fr> wrote:
> Hello,
>
> Feature request to the xls package.
>
> Add
> to.4hours(x,name) to the xls package.
>
> Regards,
>
> Pierre8r
>
>
>      ____________________________________________________
> intelligente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From brian at braverock.com  Wed Jul 23 16:03:57 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 23 Jul 2008 09:03:57 -0500
Subject: [R-SIG-Finance] estimating non-linear state space models
In-Reply-To: <39F7C064586B194EB8AAE4D74AB096A1B32813@EXVS02.mcis.agrinet.local>
References: <39F7C064586B194EB8AAE4D74AB096A1B32812@EXVS02.mcis.agrinet.local>
	<39F7C064586B194EB8AAE4D74AB096A1B32813@EXVS02.mcis.agrinet.local>
Message-ID: <48873A4D.7060503@braverock.com>

Andreas wrote:
> I am trying to estimate the dynamic model for equity fund's alphas and betas described here: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=389740 <http://papers.ssrn.com/sol3/papers.cfm?abstract_id=389740>  . The nonlinear state space model is described by equations (6) and (11). (For those in a hurry: The one dimensional state follows an AR1 process. The observation equation has similarities with CAPM, but is extended to depend quadratically on the state)
> 
> So far I have tried to work with the packages sspir and dse, but they don't seem to support non-linear models. I then tried to implement my own EKF code, it works for state estimation but so far I couldn't get the parameter and variance estimation running reliably.

You might try posting your code here, and being very specific about what 
help you need.  That way everyone can benefit from an implementation of 
these models in R.

Regards,

   - Brian


From spayseur at u.washington.edu  Wed Jul 23 19:59:30 2008
From: spayseur at u.washington.edu (Scott Payseur)
Date: Wed, 23 Jul 2008 18:59:30 +0100
Subject: [R-SIG-Finance] Correlation on Tick Data
In-Reply-To: <Pine.LNX.4.43.0807221251230.8790@hymn31.u.washington.edu>
References: <24221559.5347871216747527348.JavaMail.javamailuser@localhost>
	<Pine.LNX.4.43.0807221251230.8790@hymn31.u.washington.edu>
Message-ID: <9feef0f90807231059wd1811aan5927f4d8f97301b4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080723/c4e7d2f9/attachment.pl>

From ianseow at gmail.com  Thu Jul 24 03:16:25 2008
From: ianseow at gmail.com (Ian Seow)
Date: Thu, 24 Jul 2008 09:16:25 +0800
Subject: [R-SIG-Finance] timeDate class query
In-Reply-To: <20080723142511.0f966bf8@mimi>
References: <1865010807101939u217fb775t90173bb15ce6fab7@mail.gmail.com>
	<20080723142511.0f966bf8@mimi>
Message-ID: <1865010807231816i3292d78bse82e87eb28a5b6c4@mail.gmail.com>

Hi Yohan thanks for your reply. Are you referring to version 270.75
for fCalendar? This is the latest version I could find and it is the
version I am currently using. I must be missing something, because
when I try length(timeCalendar( ) ) I get the following error:

> test=timeCalendar(m = c(9, 1, 8, 2), d = c(28, 15, 30, 9),
+                 y = c(1989, 2001, 2004, 1990), FinCenter = "GMT")

> test
GMT
[1] [1989-09-28] [2001-01-15] [2004-08-30] [1990-02-09]

> length(test)
Error in prod(x at Dim) : no slot of name "Dim" for this object of class "timeDate"


On 7/23/08, Yohan Chalabi <chalabi at phys.ethz.ch> wrote:
> >>>> "IS" == "Ian Seow" <ianseow at gmail.com>
> >>>> on Fri, 11 Jul 2008 10:39:08 +0800
>
>   IS> Hi, I have a query on the timeDate class in the fCalendar Package.
>   IS> I have a large data set of intraday observations (in GMT) containing
>   IS> 407,100 POSIXct objects which I want to convert into timeDate objects
>   IS> and then view the result in my local time zone ('Asia/Singapore').
>   IS> The conversion works fine, however, it seems that timeDate does not
>   IS> have 'Dims' and hence does not allow for length(), head() and tail()
>   IS> to be performed.
>
> Do you have the latest version of fCalendar?
>
> length(timeCalendar()) works fine with the current version of fCalendar available at CRAN.
>
>
> regards,
> Yohan
>
> --
> PhD student
> Swiss Federal Institute of Technology
> Zurich
>
> www.ethz.ch
> www.rmetrics.org
>


From rbali at ufmg.br  Thu Jul 24 03:21:09 2008
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Wed, 23 Jul 2008 21:21:09 -0400
Subject: [R-SIG-Finance] estimating non-linear state space models
References: <39F7C064586B194EB8AAE4D74AB096A1B32812@EXVS02.mcis.agrinet.local><39F7C064586B194EB8AAE4D74AB096A1B32813@EXVS02.mcis.agrinet.local>
	<48873A4D.7060503@braverock.com>
Message-ID: <C4DFF417DAED416EBA949AB0E277BDF4@HPR>

Just for those interested. The final version of that paper was published in 
Review of Financial Studies 2008 21(1):233-264; doi:10.1093/rfs/hhm049.
Regards

Robert Iquiapaza
ri2162 at columbia.edu

--------------------------------------------------
From: "Brian G. Peterson" <brian at braverock.com>
Sent: Wednesday, July 23, 2008 10:03 AM
To: "r_sig_finance" <r_sig_finance at greenmail.ch>
Cc: <r-sig-finance at stat.math.ethz.ch>
Subject: Re: [R-SIG-Finance] estimating non-linear state space models

> Andreas wrote:
>> I am trying to estimate the dynamic model for equity fund's alphas and 
>> betas described here: 
>> http://papers.ssrn.com/sol3/papers.cfm?abstract_id=389740 
>> <http://papers.ssrn.com/sol3/papers.cfm?abstract_id=389740>  . The 
>> nonlinear state space model is described by equations (6) and (11). (For 
>> those in a hurry: The one dimensional state follows an AR1 process. The 
>> observation equation has similarities with CAPM, but is extended to 
>> depend quadratically on the state)
>>
>> So far I have tried to work with the packages sspir and dse, but they 
>> don't seem to support non-linear models. I then tried to implement my own 
>> EKF code, it works for state estimation but so far I couldn't get the 
>> parameter and variance estimation running reliably.
>
> You might try posting your code here, and being very specific about what 
> help you need.  That way everyone can benefit from an implementation of 
> these models in R.
>
> Regards,
>
>   - Brian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From Matthias.Koberstein at hsbctrinkaus.de  Thu Jul 24 11:10:20 2008
From: Matthias.Koberstein at hsbctrinkaus.de (Matthias.Koberstein at hsbctrinkaus.de)
Date: Thu, 24 Jul 2008 11:10:20 +0200
Subject: [R-SIG-Finance] Diagram Question
Message-ID: <OF209073A6.85366CF2-ONC1257490.0032041C-C1257490.003262AF@hsbctrinkaus.de>


Hi,

I am trying to plot a Line Diagram with two Y Axes Scales and display one
time series on the left one and the second on the right one.
Does anyone have an idea how to achieve that?

Cheers

Matthias

**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential...{{dropped:18}}


From enricoschumann at yahoo.de  Thu Jul 24 11:28:57 2008
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Thu, 24 Jul 2008 11:28:57 +0200
Subject: [R-SIG-Finance] Diagram Question
In-Reply-To: <OF209073A6.85366CF2-ONC1257490.0032041C-C1257490.003262AF@hsbctrinkaus.de>
Message-ID: <898481.37027.bm@omp201.mail.ukl.yahoo.com>

maybe the zoo-faq can help you

http://cran.r-project.org/web/packages/zoo/vignettes/zoo-faq.pdf 

regards,
enrico
-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von
Matthias.Koberstein at hsbctrinkaus.de
Gesendet: Donnerstag, 24. Juli 2008 11:10
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] Diagram Question


Hi,

I am trying to plot a Line Diagram with two Y Axes Scales and display one
time series on the left one and the second on the right one.
Does anyone have an idea how to achieve that?

Cheers

Matthias

**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential...{{dropped:18}}

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG - http://www.avg.com

24.07.2008
06:59


From Matthieu.Stigler at gmail.com  Thu Jul 24 11:41:38 2008
From: Matthieu.Stigler at gmail.com (Matthieu Stigler)
Date: Thu, 24 Jul 2008 11:41:38 +0200
Subject: [R-SIG-Finance] Diagram Question
In-Reply-To: <OF209073A6.85366CF2-ONC1257490.0032041C-C1257490.003262AF@hsbctrinkaus.de>
References: <OF209073A6.85366CF2-ONC1257490.0032041C-C1257490.003262AF@hsbctrinkaus.de>
Message-ID: <48884E52.5040808@gmail.com>

See ?axis to add an axis

Mat

Matthias.Koberstein at hsbctrinkaus.de a ?crit :
> Hi,
>
> I am trying to plot a Line Diagram with two Y Axes Scales and display one
> time series on the left one and the second on the right one.
> Does anyone have an idea how to achieve that?
>
> Cheers
>
> Matthias
>
> **** Ressourcen schonen, weniger drucken - Think before you print! ****
>
> ---------------------------------------------------------------------
> Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
> rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
> sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
> sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
> oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
> gestattet.
>
> This e-mail and any attachments may contain confidential...{{dropped:18}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From r_sig_finance at greenmail.ch  Thu Jul 24 11:46:15 2008
From: r_sig_finance at greenmail.ch (Andreas)
Date: Thu, 24 Jul 2008 11:46:15 +0200
Subject: [R-SIG-Finance]  estimating non-linear state space models
References: <39F7C064586B194EB8AAE4D74AB096A1B32812@EXVS02.mcis.agrinet.local><39F7C064586B194EB8AAE4D74AB096A1B32813@EXVS02.mcis.agrinet.local>
	<48873A4D.7060503@braverock.com>
	<C4DFF417DAED416EBA949AB0E277BDF4@HPR>
Message-ID: <39F7C064586B194EB8AAE4D74AB096A1B32814@EXVS02.mcis.agrinet.local>

I had to clean up my EKF code first, after trying many different ideas I ended up with a huge mess... It's still heavily under development, but I think it could serve as a starting point.

 

As I wrote before, state estimation seems to run OK given the true parameters.

 

When I try to estimate the parameters by maximizing the likelihood, I end up with rather random results depending on the initial parameters I start optimizing with.

 

I don't know if there's an error in the calculation of the likelihood, or if I'm just overstraining the ML-method by estimating model parameters and noise variances at the same time. Is this even possible? Or maybe I'm just expecting too precise results...

 

Regards

Andreas

 

 


________________________________

Von: Robert Iquiapaza [mailto:rbali at ufmg.br]
Gesendet: Do 24.07.2008 03:21
An: Andreas
Cc: r-sig-finance at stat.math.ethz.ch
Betreff: Re: [R-SIG-Finance] estimating non-linear state space models



Just for those interested. The final version of that paper was published in
Review of Financial Studies 2008 21(1):233-264; doi:10.1093/rfs/hhm049.
Regards

Robert Iquiapaza
ri2162 at columbia.edu

--------------------------------------------------
From: "Brian G. Peterson" <brian at braverock.com>
Sent: Wednesday, July 23, 2008 10:03 AM
To: "r_sig_finance" <r_sig_finance at greenmail.ch>
Cc: <r-sig-finance at stat.math.ethz.ch>
Subject: Re: [R-SIG-Finance] estimating non-linear state space models

> Andreas wrote:
>> I am trying to estimate the dynamic model for equity fund's alphas and
>> betas described here:
>> http://papers.ssrn.com/sol3/papers.cfm?abstract_id=389740
>> <http://papers.ssrn.com/sol3/papers.cfm?abstract_id=389740>  . The
>> nonlinear state space model is described by equations (6) and (11). (For
>> those in a hurry: The one dimensional state follows an AR1 process. The
>> observation equation has similarities with CAPM, but is extended to
>> depend quadratically on the state)
>>
>> So far I have tried to work with the packages sspir and dse, but they
>> don't seem to support non-linear models. I then tried to implement my own
>> EKF code, it works for state estimation but so far I couldn't get the
>> parameter and variance estimation running reliably.
>
> You might try posting your code here, and being very specific about what
> help you need.  That way everyone can benefit from an implementation of
> these models in R.
>
> Regards,
>
>   - Brian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080724/b25169e1/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ekf.R
Type: application/octet-stream
Size: 6166 bytes
Desc: ekf.R
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080724/b25169e1/attachment.obj>

From pierre8r-list at yahoo.fr  Thu Jul 24 11:52:14 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Thu, 24 Jul 2008 09:52:14 +0000 (GMT)
Subject: [R-SIG-Finance] Got a error message with a OpCl(x).
Message-ID: <441641.64141.qm@web28107.mail.ukl.yahoo.com>

Hi,

I have this message error :

Error in attr(x, "tsp") <- c(1, NROW(x), 1) : 
  attempt to set an attribute on NULL


With this R code :


library(quantmod) # this also pulls in zoo
library(xts) # this also pulls in zoo

Lines <-
"2007-12-03  100.00  110.00  90.00  110.00  10.0
2007-12-04  110.00  120.00 100.00  120.00 10.0
2007-12-05  120.00  130.00 110.00  130.00 10.0
2007-12-06  130.00  140.00 120.00  140.00 10.0
2007-12-07  140.00  140.00 120.00  140.00 10.0
2007-12-08  130.00  130.00 110.00  120.00 10.0
2007-12-09  120.00  120.00 120.00  140.00 10.0
2007-12-10  110.00  130.00 120.00  140.00 10.0
2007-12-11  100.00  120.00 120.00  140.00 10.0"


x <- read.zoo(textConnection(Lines))
x <- as.xts(x)

OpCl(x)





      ____________________________________________________________
ente http://mail.yahoo.fr


From pierre8r-list at yahoo.fr  Thu Jul 24 12:02:28 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Thu, 24 Jul 2008 10:02:28 +0000 (GMT)
Subject: [R-SIG-Finance] Re :  Got a error message with a OpCl(x).
In-Reply-To: <441641.64141.qm@web28107.mail.ukl.yahoo.com>
Message-ID: <678665.37173.qm@web28104.mail.ukl.yahoo.com>

Answer to myself :

Add :
colnames(x) <- c('Open','High','Low','Close','Volume')
before 
OpCl(x)



--- En date de?: Jeu 24.7.08, pierre8r-list at yahoo.fr <pierre8r-list at yahoo.fr> a ?crit?:

> De: pierre8r-list at yahoo.fr <pierre8r-list at yahoo.fr>
> Objet: [R-SIG-Finance] Got a error message with a OpCl(x).
> ?: r-sig-finance at stat.math.ethz.ch
> Date: Jeudi 24 Juillet 2008, 11h52
> Hi,
> 
> I have this message error :
> 
> Error in attr(x, "tsp") <- c(1, NROW(x), 1) : 
>   attempt to set an attribute on NULL
> 
> 
> With this R code :
> 
> 
> library(quantmod) # this also pulls in zoo
> library(xts) # this also pulls in zoo
> 
> Lines <-
> "2007-12-03  100.00  110.00  90.00  110.00  10.0
> 2007-12-04  110.00  120.00 100.00  120.00 10.0
> 2007-12-05  120.00  130.00 110.00  130.00 10.0
> 2007-12-06  130.00  140.00 120.00  140.00 10.0
> 2007-12-07  140.00  140.00 120.00  140.00 10.0
> 2007-12-08  130.00  130.00 110.00  120.00 10.0
> 2007-12-09  120.00  120.00 120.00  140.00 10.0
> 2007-12-10  110.00  130.00 120.00  140.00 10.0
> 2007-12-11  100.00  120.00 120.00  140.00 10.0"
> 
> 
> x <- read.zoo(textConnection(Lines))
> x <- as.xts(x)
> 
> OpCl(x)
> 
> 
> 
> 
> 
>      
> ____________________________________________________________
> ente http://mail.yahoo.fr
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


      _____________________________________________________________________________ 
Envoyez avec

From imanpreet at gmail.com  Thu Jul 24 12:44:27 2008
From: imanpreet at gmail.com (Imanpreet)
Date: Thu, 24 Jul 2008 16:14:27 +0530
Subject: [R-SIG-Finance] Finance Benchmarks/Workload
Message-ID: <c26b95920807240344g1a0e525oa6d502d680eced97@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080724/c0d5f761/attachment.pl>

From r_sig_finance at greenmail.ch  Thu Jul 24 14:00:48 2008
From: r_sig_finance at greenmail.ch (Andreas)
Date: Thu, 24 Jul 2008 14:00:48 +0200
Subject: [R-SIG-Finance]   estimating non-linear state space models
References: <39F7C064586B194EB8AAE4D74AB096A1B32812@EXVS02.mcis.agrinet.local><39F7C064586B194EB8AAE4D74AB096A1B32813@EXVS02.mcis.agrinet.local><48873A4D.7060503@braverock.com><C4DFF417DAED416EBA949AB0E277BDF4@HPR>
	<39F7C064586B194EB8AAE4D74AB096A1B32814@EXVS02.mcis.agrinet.local>
Message-ID: <39F7C064586B194EB8AAE4D74AB096A1B32816@EXVS02.mcis.agrinet.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080724/5510c398/attachment.pl>

From vlanschot at yahoo.com  Thu Jul 24 16:36:10 2008
From: vlanschot at yahoo.com (R@Nabble)
Date: Thu, 24 Jul 2008 07:36:10 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] feasiblePortfolio in fPortfolio
Message-ID: <18633174.post@talk.nabble.com>


Hi,

What could be the reason that feasiblePortfolio, which worked fine in
version 260.72, no longer seems to work in the June-version of 270.74?  Do I
need to upgrade to the latest version?

Thx.

P.S.: I have R 2.7.0 installed
-- 
View this message in context: http://www.nabble.com/feasiblePortfolio-in-fPortfolio-tp18633174p18633174.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From kafka at centras.lt  Thu Jul 24 17:20:23 2008
From: kafka at centras.lt (kafkaz)
Date: Thu, 24 Jul 2008 08:20:23 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] xts and barChart (quantmod)
Message-ID: <18617785.post@talk.nabble.com>


My data is stored in a MySQL database. So, I'm using RMySQL to load
information about price performance. After that, I'm using xts to convert
selected data into XTS object. Example:

rs <- dbSendQuery(con, "select ....");
dji<- fetch(rs, n = -1) 
dji2<-xts(dji,order.by=as.POSIXct(dji[,1]))
colnames(dji2)<-c('Datetime', 'Open', 'High', 'Low', 'Close', 'Volume')
At this point I have a xts object:

class(dji2)
[1] "xts" "zoo"

 >str(dji2)
An ?xts? object from 2004-01-07 15:50:00 to 2004-01-09 11:45:00 containing:
  Data: chr [1:50, 1:6] "2004-01-07 15:50:00" "2004-01-07 15:55:00"
"2004-01-07 16:00:00" "2004-01-07 16:05:00" ...
 - attr(*, "dimnames")=List of 2
  ..$ : chr [1:50] "2004-01-07 15:50:00" "2004-01-07 15:55:00" "2004-01-07
16:00:00" "2004-01-07 16:05:00" ...
  ..$ : chr [1:6] "Datetime" "Open" "High" "Low" ...
  Indexed by:  POSIXct[1:50], format: "2004-01-07 15:50:00" "2004-01-07
15:55:00" "2004-01-07 16:00:00" "2004-01-07 16:05:00" ...
  xts Attributes:  
 NULL

dji2[1:4,]
                    Datetime            Open  High  Low   Close Volume
2004-01-07 15:50:00 2004-01-07 15:50:00 10483 10483 10480 10480   0   
2004-01-07 15:55:00 2004-01-07 15:55:00 10479 10479 10465 10465   0   
2004-01-07 16:00:00 2004-01-07 16:00:00 10464 10464 10460 10460   0   
2004-01-07 16:05:00 2004-01-07 16:05:00 10459 10459 10442 10442   0 

If I try to use barChart function (quantmod package), I will get this error:
Error in checkSlotAssignment(object, name, value) : 
  assignment of an object of class "character" is not valid for slot
"yrange" in an object of class "chob"; is(value, "numeric") is not TRUE

The problem lies in the type of columns - open, high, low and close have
type character, but quantmod function expects integer:

if (is.OHLC(x)) {
        chob at yrange <- c(min(Lo(x), na.rm = TRUE), max(Hi(x), 
            na.rm = TRUE))
    }
    else chob at yrange <- range(x[, 1], na.rm = TRUE)

is.character(dji2[,5])
[1] TRUE

Any idea how to fix it?
Thank you.
-- 
View this message in context: http://www.nabble.com/xts-and-barChart-%28quantmod%29-tp18617785p18617785.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From brian at braverock.com  Thu Jul 24 18:05:48 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 24 Jul 2008 11:05:48 -0500
Subject: [R-SIG-Finance] Finance Benchmarks/Workload
In-Reply-To: <c26b95920807240344g1a0e525oa6d502d680eced97@mail.gmail.com>
References: <c26b95920807240344g1a0e525oa6d502d680eced97@mail.gmail.com>
Message-ID: <4888A85C.6050804@braverock.com>

Imanpreet wrote:
>                  Could anyone point out any existing benchmarks or workloads
> for R, specifically targeting financial applications?

That's a pretty vague request.

I've got functions that run in microseconds, and some portfolio 
optimizations that take hours.

I'm aware of organizations that use huge clusters to do financial 
analysis with R.

Perhaps you could be a little more specific about what you are trying to 
benchmark?

Regards,

   - Brian


From jeff.a.ryan at gmail.com  Thu Jul 24 18:16:51 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 24 Jul 2008 11:16:51 -0500
Subject: [R-SIG-Finance] [R-sig-finance] xts and barChart (quantmod)
In-Reply-To: <18617785.post@talk.nabble.com>
References: <18617785.post@talk.nabble.com>
Message-ID: <e8e755250807240916g30528e09j96c6e1af1ebc4f93@mail.gmail.com>

The issue is with your original dji object.

The first column of you data.frame is forcing the zoo/xts matrix to be
a character one.

Try:

dji2 <- xts( dji[,-1], order.by= as.POSIXct( dji[,1])
colnames(dji2)<-c('Open', 'High', 'Low', 'Close', 'Volume')

The only issue remaining is that your original data.frame may be all
characters, though I assume that is not the case here.

Jeff (with input from Josh)

On Thu, Jul 24, 2008 at 10:20 AM, kafkaz <kafka at centras.lt> wrote:
>
> My data is stored in a MySQL database. So, I'm using RMySQL to load
> information about price performance. After that, I'm using xts to convert
> selected data into XTS object. Example:
>
> rs <- dbSendQuery(con, "select ....");
> dji<- fetch(rs, n = -1)
> dji2<-xts(dji,order.by=as.POSIXct(dji[,1]))
> colnames(dji2)<-c('Datetime', 'Open', 'High', 'Low', 'Close', 'Volume')
> At this point I have a xts object:
>
> class(dji2)
> [1] "xts" "zoo"
>
>  >str(dji2)
> An 'xts' object from 2004-01-07 15:50:00 to 2004-01-09 11:45:00 containing:
>  Data: chr [1:50, 1:6] "2004-01-07 15:50:00" "2004-01-07 15:55:00"
> "2004-01-07 16:00:00" "2004-01-07 16:05:00" ...
>  - attr(*, "dimnames")=List of 2
>  ..$ : chr [1:50] "2004-01-07 15:50:00" "2004-01-07 15:55:00" "2004-01-07
> 16:00:00" "2004-01-07 16:05:00" ...
>  ..$ : chr [1:6] "Datetime" "Open" "High" "Low" ...
>  Indexed by:  POSIXct[1:50], format: "2004-01-07 15:50:00" "2004-01-07
> 15:55:00" "2004-01-07 16:00:00" "2004-01-07 16:05:00" ...
>  xts Attributes:
>  NULL
>
> dji2[1:4,]
>                    Datetime            Open  High  Low   Close Volume
> 2004-01-07 15:50:00 2004-01-07 15:50:00 10483 10483 10480 10480   0
> 2004-01-07 15:55:00 2004-01-07 15:55:00 10479 10479 10465 10465   0
> 2004-01-07 16:00:00 2004-01-07 16:00:00 10464 10464 10460 10460   0
> 2004-01-07 16:05:00 2004-01-07 16:05:00 10459 10459 10442 10442   0
>
> If I try to use barChart function (quantmod package), I will get this error:
> Error in checkSlotAssignment(object, name, value) :
>  assignment of an object of class "character" is not valid for slot
> "yrange" in an object of class "chob"; is(value, "numeric") is not TRUE
>
> The problem lies in the type of columns - open, high, low and close have
> type character, but quantmod function expects integer:
>
> if (is.OHLC(x)) {
>        chob at yrange <- c(min(Lo(x), na.rm = TRUE), max(Hi(x),
>            na.rm = TRUE))
>    }
>    else chob at yrange <- range(x[, 1], na.rm = TRUE)
>
> is.character(dji2[,5])
> [1] TRUE
>
> Any idea how to fix it?
> Thank you.
> --
> View this message in context: http://www.nabble.com/xts-and-barChart-%28quantmod%29-tp18617785p18617785.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From pierre8r-list at yahoo.fr  Thu Jul 24 18:54:50 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Thu, 24 Jul 2008 16:54:50 +0000 (GMT)
Subject: [R-SIG-Finance] Don't success to create xts from lines in code
Message-ID: <13186.62105.qm@web28101.mail.ukl.yahoo.com>

Hi,

Don't success to create xts from lines in code



Here my R code :
---------------

library(quantmod)
library(xts)

Lines <-
"01/08/2007 00:59:00,1.930250,1.930350,1.929300,1.929600,-60
01/08/2007 01:59:00,1.929600,1.930100,1.926250,1.928050,-60
01/08/2007 02:59:00,1.928050,1.929700,1.927700,1.927800,-60
01/08/2007 03:59:00,1.927800,1.928350,1.927200,1.928250,-60
01/08/2007 04:59:00,1.928250,1.929600,1.928250,1.929000,-60
01/08/2007 05:59:00,1.929000,1.929250,1.928650,1.929250,-60
01/08/2007 06:59:00,1.929250,1.930850,1.928700,1.930650,-60
01/08/2007 07:59:00,1.930650,1.933100,1.930250,1.933000,-60
01/08/2007 08:59:00,1.933000,1.934350,1.931050,1.931100,-60
01/08/2007 09:59:00,1.931100,1.934750,1.930350,1.934000,-60
01/08/2007 10:59:00,1.934000,1.934650,1.932300,1.932400,-60
01/08/2007 11:59:00,1.932400,1.933500,1.931200,1.933050,-60
01/08/2007 12:59:00,1.933050,1.933850,1.932100,1.933600,-60"

quotes <- read.zoo(textConnection(Lines))

x <- xts(as.matrix(quotes[,-1]),as.POSIXct(paste(quotes[,1]),
format='%m/%d/%Y %H:%M:%S'))

colnames(x) <- c('Open','High','Low','Close','Volume')

x


The message error :
-------------------

> 
> quotes <- read.zoo(textConnection(Lines))
Warning message:
In zoo(rval, ix) :
  some methods for ?zoo? objects do not work if the index entries in ?order.by? are not unique
> 
> x <- xts(as.matrix(quotes[,-1]),as.POSIXct(paste(quotes[,1]),
+ format='%m/%d/%Y %H:%M:%S'))
Warning messages:
1: In zoo(rval[i], x.index[i]) :
  some methods for ?zoo? objects do not work if the index entries in ?order.by? are not unique
2: In zoo(x = x, order.by = order.by, frequency = frequency) :
  some methods for ?zoo? objects do not work if the index entries in ?order.by? are not unique
3: In zoo(rval[i], x.index[i]) :
  some methods for ?zoo? objects do not work if the index entries in ?order.by? are not unique







      ____________________________________________________________
ente http://mail.yahoo.fr


From josh.m.ulrich at gmail.com  Thu Jul 24 19:12:04 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Thu, 24 Jul 2008 12:12:04 -0500
Subject: [R-SIG-Finance] Don't success to create xts from lines in code
In-Reply-To: <13186.62105.qm@web28101.mail.ukl.yahoo.com>
References: <13186.62105.qm@web28101.mail.ukl.yahoo.com>
Message-ID: <8cca69990807241012g7627fcbap7443eb4d8b10f36c@mail.gmail.com>

See ?read.zoo

1) You need to specify the correct delimiter for your file.
2) read.zoo is converting your index to Date (the default), use
POSIXct instead and be sure to specify the correct format of the index
in your file.

quotes <- read.zoo(textConnection(Lines), sep=',',
FUN=as.POSIXct, format='%m/%d/%Y %H:%M:%S')
x <- as.xts(quotes)
colnames(x) <- c('Open','High','Low','Close','Volume')

--
http://quantemplation.blogspot.com



On Thu, Jul 24, 2008 at 11:54 AM,  <pierre8r-list at yahoo.fr> wrote:
> Hi,
>
> Don't success to create xts from lines in code
>
>
>
> Here my R code :
> ---------------
>
> library(quantmod)
> library(xts)
>
> Lines <-
> "01/08/2007 00:59:00,1.930250,1.930350,1.929300,1.929600,-60
> 01/08/2007 01:59:00,1.929600,1.930100,1.926250,1.928050,-60
> 01/08/2007 02:59:00,1.928050,1.929700,1.927700,1.927800,-60
> 01/08/2007 03:59:00,1.927800,1.928350,1.927200,1.928250,-60
> 01/08/2007 04:59:00,1.928250,1.929600,1.928250,1.929000,-60
> 01/08/2007 05:59:00,1.929000,1.929250,1.928650,1.929250,-60
> 01/08/2007 06:59:00,1.929250,1.930850,1.928700,1.930650,-60
> 01/08/2007 07:59:00,1.930650,1.933100,1.930250,1.933000,-60
> 01/08/2007 08:59:00,1.933000,1.934350,1.931050,1.931100,-60
> 01/08/2007 09:59:00,1.931100,1.934750,1.930350,1.934000,-60
> 01/08/2007 10:59:00,1.934000,1.934650,1.932300,1.932400,-60
> 01/08/2007 11:59:00,1.932400,1.933500,1.931200,1.933050,-60
> 01/08/2007 12:59:00,1.933050,1.933850,1.932100,1.933600,-60"
>
> quotes <- read.zoo(textConnection(Lines))
>
> x <- xts(as.matrix(quotes[,-1]),as.POSIXct(paste(quotes[,1]),
> format='%m/%d/%Y %H:%M:%S'))
>
> colnames(x) <- c('Open','High','Low','Close','Volume')
>
> x
>
>
> The message error :
> -------------------
>
>>
>> quotes <- read.zoo(textConnection(Lines))
> Warning message:
> In zoo(rval, ix) :
>  some methods for "zoo" objects do not work if the index entries in 'order.by' are not unique
>>
>> x <- xts(as.matrix(quotes[,-1]),as.POSIXct(paste(quotes[,1]),
> + format='%m/%d/%Y %H:%M:%S'))
> Warning messages:
> 1: In zoo(rval[i], x.index[i]) :
>  some methods for "zoo" objects do not work if the index entries in 'order.by' are not unique
> 2: In zoo(x = x, order.by = order.by, frequency = frequency) :
>  some methods for "zoo" objects do not work if the index entries in 'order.by' are not unique
> 3: In zoo(rval[i], x.index[i]) :
>  some methods for "zoo" objects do not work if the index entries in 'order.by' are not unique
>
>
>
>
>
>
>
>      ____________________________________________________________
> ente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From chalabi at phys.ethz.ch  Thu Jul 24 20:05:42 2008
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Thu, 24 Jul 2008 20:05:42 +0200
Subject: [R-SIG-Finance] timeDate class query
In-Reply-To: <1865010807231816i3292d78bse82e87eb28a5b6c4@mail.gmail.com>
References: <1865010807101939u217fb775t90173bb15ce6fab7@mail.gmail.com>
	<20080723142511.0f966bf8@mimi>
	<1865010807231816i3292d78bse82e87eb28a5b6c4@mail.gmail.com>
Message-ID: <20080724200542.663bd92b@mimi>

>>>> "IS" == "Ian Seow" <ianseow at gmail.com>
>>>> on Thu, 24 Jul 2008 09:16:25 +0800

   IS> Hi Yohan thanks for your reply. Are you referring to version 270.75
   IS> for fCalendar? This is the latest version I could find and it is the
   IS> version I am currently using. I must be missing something, because
   IS> when I try length(timeCalendar( ) ) I get the following error:
   IS> 
   IS> > test=timeCalendar(m = c(9, 1, 8, 2), d = c(28, 15, 30, 9),  
   IS> +                 y = c(1989, 2001, 2004, 1990), FinCenter = "GMT")
   IS> 
   IS> > test  
   IS> GMT
   IS> [1] [1989-09-28] [2001-01-15] [2004-08-30] [1990-02-09]
   IS> 
   IS> > length(test)  
   IS> Error in prod(x at Dim) : no slot of name "Dim" for this object of class "timeDate"

timeDate class used to have a @Dim two or three versions ago. As far
as I can see, all references to @Dim have been removed.

the function length.timeDate is defined in v270.75 as :

length.timeDate <- 
    function(x) 
{   
    # A function implemented by Diethelm Wuertz

    # Description:
    #   Gets the length of a 'timeDate' vector

    # Arguments:
    #   x - a 'timeDate' object
    
    # Value:
    #   Returns the lengths of an object of class 'timeDate'.

    # FUNCTION:
    
    # Length:
    ans = length(x at Data)
    
    # Return Value:
    ans
}

as you can see there is no reference to @Dim.

Could you please show us how your length.timeDate looks like.

regards,
Yohan 


-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch
www.rmetrics.org


From pierre8r-list at yahoo.fr  Thu Jul 24 20:59:38 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Thu, 24 Jul 2008 18:59:38 +0000 (GMT)
Subject: [R-SIG-Finance] Don't success to create xts from lines in code
In-Reply-To: <8cca69990807241012g7627fcbap7443eb4d8b10f36c@mail.gmail.com>
Message-ID: <959951.52489.qm@web28103.mail.ukl.yahoo.com>

Hi Josh,

Thanks, it works.

If it don't boring you, I want to read a other data format.
There is a comma between Date and Time. 

2008.07.14,04:00,1.5894,1.5916,1.5893,1.5914,276
2008.07.14,05:00,1.5912,1.5912,1.5902,1.5904,205
2008.07.14,06:00,1.5903,1.5911,1.5898,1.5899,239
2008.07.14,07:00,1.5898,1.5902,1.5865,1.5882,541
2008.07.14,08:00,1.5883,1.5910,1.5868,1.5870,689
2008.07.14,09:00,1.5869,1.5890,1.5860,1.5877,763
2008.07.14,10:00,1.5878,1.5881,1.5841,1.5871,835
2008.07.14,11:00,1.5870,1.5876,1.5854,1.5859,533

Thanks,

Pierre8r


> See ?read.zoo
> 
> 1) You need to specify the correct delimiter for your file.
> 2) read.zoo is converting your index to Date (the default),
> use
> POSIXct instead and be sure to specify the correct format
> of the index
> in your file.
> 
> quotes <- read.zoo(textConnection(Lines),
> sep=',',
> FUN=as.POSIXct, format='%m/%d/%Y %H:%M:%S')
> x <- as.xts(quotes)
> colnames(x) <-
> c('Open','High','Low','Close','Volume')
> 
> --
> http://quantemplation.blogspot.com
> 
> 



      ____________________________________________________________
ente http://mail.yahoo.fr


From imanpreet at gmail.com  Thu Jul 24 21:31:58 2008
From: imanpreet at gmail.com (Imanpreet)
Date: Fri, 25 Jul 2008 01:01:58 +0530
Subject: [R-SIG-Finance] Finance Benchmarks/Workload
In-Reply-To: <4888A85C.6050804@braverock.com>
References: <c26b95920807240344g1a0e525oa6d502d680eced97@mail.gmail.com>
	<4888A85C.6050804@braverock.com>
Message-ID: <c26b95920807241231u5384379cw3f85ab6ed4e11c8d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080725/ca0f638d/attachment.pl>

From josh.m.ulrich at gmail.com  Thu Jul 24 22:02:39 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Thu, 24 Jul 2008 15:02:39 -0500
Subject: [R-SIG-Finance] Don't success to create xts from lines in code
In-Reply-To: <959951.52489.qm@web28103.mail.ukl.yahoo.com>
References: <8cca69990807241012g7627fcbap7443eb4d8b10f36c@mail.gmail.com>
	<959951.52489.qm@web28103.mail.ukl.yahoo.com>
Message-ID: <8cca69990807241302m5927efbw4bf2717eec40f206@mail.gmail.com>

I would read it as a data.frame, since the index has a comma, then
convert to xts.

quotes <- read.csv(textConnection(lines), header=FALSE)
x <- as.xts(quotes[,-(1:2)],
as.POSIXct(paste(quotes[,1],quotes[,2]),format='%Y.%m.%d %H:%M'))
colnames(x) <- c('Open','High','Low','Close','Volume')

--
http://quantemplation.blogspot.com



On Thu, Jul 24, 2008 at 1:59 PM,  <pierre8r-list at yahoo.fr> wrote:
> Hi Josh,
>
> Thanks, it works.
>
> If it don't boring you, I want to read a other data format.
> There is a comma between Date and Time.
>
> 2008.07.14,04:00,1.5894,1.5916,1.5893,1.5914,276
> 2008.07.14,05:00,1.5912,1.5912,1.5902,1.5904,205
> 2008.07.14,06:00,1.5903,1.5911,1.5898,1.5899,239
> 2008.07.14,07:00,1.5898,1.5902,1.5865,1.5882,541
> 2008.07.14,08:00,1.5883,1.5910,1.5868,1.5870,689
> 2008.07.14,09:00,1.5869,1.5890,1.5860,1.5877,763
> 2008.07.14,10:00,1.5878,1.5881,1.5841,1.5871,835
> 2008.07.14,11:00,1.5870,1.5876,1.5854,1.5859,533
>
> Thanks,
>
> Pierre8r
>
>
>> See ?read.zoo
>>
>> 1) You need to specify the correct delimiter for your file.
>> 2) read.zoo is converting your index to Date (the default),
>> use
>> POSIXct instead and be sure to specify the correct format
>> of the index
>> in your file.
>>
>> quotes <- read.zoo(textConnection(Lines),
>> sep=',',
>> FUN=as.POSIXct, format='%m/%d/%Y %H:%M:%S')
>> x <- as.xts(quotes)
>> colnames(x) <-
>> c('Open','High','Low','Close','Volume')
>>
>> --
>> http://quantemplation.blogspot.com
>>
>>
>
>
>
>      ____________________________________________________________
> ente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From swtzang at gmail.com  Fri Jul 25 05:25:00 2008
From: swtzang at gmail.com (ShyhWeir Tzang)
Date: Fri, 25 Jul 2008 11:25:00 +0800
Subject: [R-SIG-Finance] dynamo installation
Message-ID: <c17037a10807242025mb2f7182gddf2437bb2fcdd63@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080725/f5779358/attachment.pl>

From pierre8r-list at yahoo.fr  Fri Jul 25 08:23:31 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Fri, 25 Jul 2008 06:23:31 +0000 (GMT)
Subject: [R-SIG-Finance] Don't success to create xts from lines in code
In-Reply-To: <8cca69990807241302m5927efbw4bf2717eec40f206@mail.gmail.com>
Message-ID: <925153.45202.qm@web28105.mail.ukl.yahoo.com>


> I would read it as a data.frame, since the index has a
> comma, then
> convert to xts.
> 
> quotes <- read.csv(textConnection(lines), header=FALSE)
> x <- as.xts(quotes[,-(1:2)],
> as.POSIXct(paste(quotes[,1],quotes[,2]),format='%Y.%m.%d
> %H:%M'))
> colnames(x) <-
> c('Open','High','Low','Close','Volume')
> 

Thanks Josh. 
I just changed lines to Lines and it works.

This data format is the data format for historical quotations from MetaTrader 4 :
http://www.metaquotes.net/

Anybody can download MT4 client and open a account with a e-mail for free.
Then download historical Forex datas for free.

In french :
Menu / Outils / Archives

In english something near that :
Menu / Tools / Historical data

Maybe this R code could be added to show how to import for free Forex quotations into R.

Regards,

Pierre8r



      ____________________________________________________________
ente http://mail.yahoo.fr


From pierre8r-list at yahoo.fr  Fri Jul 25 09:57:03 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Fri, 25 Jul 2008 07:57:03 +0000 (GMT)
Subject: [R-SIG-Finance] Feature request to the xls package. Add
	to.4hours(x, name)
In-Reply-To: <e8e755250807230608j5f3cffd2u824604cc1f83e562@mail.gmail.com>
Message-ID: <802158.3325.qm@web28106.mail.ukl.yahoo.com>

Hi Jeff, 

> I suspect you mean the 'xts' package.  If so try:

Well guess!  ;-)

I have downloaded  hourly and four hours Forex datas from 
http://www.metaquotes.net/

Then I have used :
to.hourly(x, k=4)
with the hourly datas to compare.
But the results are not the same, why ?


My R code :
----------

library(quantmod)
library(xts)

Lines <-
"2008.07.07,00:00,1.5691,1.5705,1.5691,1.5702,58
2008.07.07,01:00,1.5701,1.5702,1.5691,1.5694,94
2008.07.07,02:00,1.5693,1.5693,1.5650,1.5652,278
2008.07.07,03:00,1.5651,1.5667,1.5633,1.5643,515
2008.07.07,04:00,1.5642,1.5643,1.5631,1.5637,255
2008.07.07,05:00,1.5638,1.5647,1.5634,1.5645,301
2008.07.07,06:00,1.5644,1.5653,1.5640,1.5640,341
2008.07.07,07:00,1.5641,1.5642,1.5625,1.5641,380
2008.07.07,08:00,1.5640,1.5646,1.5617,1.5627,590
2008.07.07,09:00,1.5628,1.5639,1.5611,1.5623,810
2008.07.07,10:00,1.5622,1.5647,1.5611,1.5643,808
2008.07.07,11:00,1.5642,1.5663,1.5642,1.5658,850
2008.07.07,12:00,1.5659,1.5667,1.5627,1.5663,802
2008.07.07,13:00,1.5665,1.5671,1.5651,1.5654,436
2008.07.07,14:00,1.5653,1.5659,1.5641,1.5649,599
2008.07.07,15:00,1.5648,1.5651,1.5621,1.5634,823
2008.07.07,16:00,1.5633,1.5663,1.5631,1.5663,694
2008.07.07,17:00,1.5662,1.5674,1.5645,1.5645,815
2008.07.07,18:00,1.5646,1.5718,1.5645,1.5717,840
2008.07.07,19:00,1.5716,1.5753,1.5715,1.5745,790
2008.07.07,20:00,1.5744,1.5748,1.5725,1.5730,510
2008.07.07,21:00,1.5729,1.5730,1.5707,1.5718,438
2008.07.07,22:00,1.5719,1.5730,1.5716,1.5726,223
2008.07.07,23:00,1.5725,1.5728,1.5720,1.5722,69"

quotes <- read.csv(textConnection(Lines), header=FALSE)
xH <- as.xts(quotes[,-(1:2)],
as.POSIXct(paste(quotes[,1],quotes[,2]),format='%Y.%m.%d %H:%M'))
colnames(xH) <- c('Open','High','Low','Close','Volume')
xH

x4H <- to.hourly(xH, k=4) 
x4H



Datas 60 minutes from  http://www.metaquotes.net/ :
--------------------------------------------------
2008.07.07,00:00,1.5691,1.5705,1.5691,1.5702,58
2008.07.07,01:00,1.5701,1.5702,1.5691,1.5694,94
2008.07.07,02:00,1.5693,1.5693,1.5650,1.5652,278
2008.07.07,03:00,1.5651,1.5667,1.5633,1.5643,515
2008.07.07,04:00,1.5642,1.5643,1.5631,1.5637,255
2008.07.07,05:00,1.5638,1.5647,1.5634,1.5645,301
2008.07.07,06:00,1.5644,1.5653,1.5640,1.5640,341
2008.07.07,07:00,1.5641,1.5642,1.5625,1.5641,380
2008.07.07,08:00,1.5640,1.5646,1.5617,1.5627,590
2008.07.07,09:00,1.5628,1.5639,1.5611,1.5623,810
2008.07.07,10:00,1.5622,1.5647,1.5611,1.5643,808
2008.07.07,11:00,1.5642,1.5663,1.5642,1.5658,850
2008.07.07,12:00,1.5659,1.5667,1.5627,1.5663,802
2008.07.07,13:00,1.5665,1.5671,1.5651,1.5654,436
2008.07.07,14:00,1.5653,1.5659,1.5641,1.5649,599
2008.07.07,15:00,1.5648,1.5651,1.5621,1.5634,823
2008.07.07,16:00,1.5633,1.5663,1.5631,1.5663,694
2008.07.07,17:00,1.5662,1.5674,1.5645,1.5645,815
2008.07.07,18:00,1.5646,1.5718,1.5645,1.5717,840
2008.07.07,19:00,1.5716,1.5753,1.5715,1.5745,790
2008.07.07,20:00,1.5744,1.5748,1.5725,1.5730,510
2008.07.07,21:00,1.5729,1.5730,1.5707,1.5718,438
2008.07.07,22:00,1.5719,1.5730,1.5716,1.5726,223
2008.07.07,23:00,1.5725,1.5728,1.5720,1.5722,69


Datas 240 minutes from  http://www.metaquotes.net/ :
--------------------------------------------------
2008.07.07,00:00,1.5691,1.5705,1.5633,1.5643,945
2008.07.07,04:00,1.5642,1.5653,1.5625,1.5641,1277
2008.07.07,08:00,1.5640,1.5663,1.5611,1.5658,3058
2008.07.07,12:00,1.5659,1.5671,1.5621,1.5634,2660
2008.07.07,16:00,1.5633,1.5753,1.5631,1.5745,3139
2008.07.07,20:00,1.5744,1.5748,1.5707,1.5722,1240


Output datas from the R code :
-----------------------------
                    xH.Open xH.High xH.Low xH.Close xH.Volume
2008-07-07 01:00:00  1.5691  1.5705 1.5691   1.5694       152
2008-07-07 05:00:00  1.5693  1.5693 1.5631   1.5645      1349
2008-07-07 09:00:00  1.5644  1.5653 1.5611   1.5623      2121
2008-07-07 13:00:00  1.5622  1.5671 1.5611   1.5654      2896
2008-07-07 17:00:00  1.5653  1.5674 1.5621   1.5645      2931
2008-07-07 21:00:00  1.5646  1.5753 1.5645   1.5718      2578
2008-07-07 23:00:00  1.5719  1.5730 1.5716   1.5722       292



















      ____________________________________________________________

From jeff.a.ryan at gmail.com  Fri Jul 25 18:19:35 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 25 Jul 2008 11:19:35 -0500
Subject: [R-SIG-Finance] dynamo installation
In-Reply-To: <c17037a10807242025mb2f7182gddf2437bb2fcdd63@mail.gmail.com>
References: <c17037a10807242025mb2f7182gddf2437bb2fcdd63@mail.gmail.com>
Message-ID: <e8e755250807250919i740c967cve7430aff31ff64b9@mail.gmail.com>

Have you attempted to contact the maintainer?

You should also follow the posting guide and include some (all!)
information about your installation ( sessionInfo() output is a start)
as well as making sure you convey that you have the dependencies as
required (gsl version 1.5 or greater).

More of a help list, not so much a solve my problem list :)

How exactly did you do:
> I checked the dll file and it's OK.
???

Jeff

On Thu, Jul 24, 2008 at 10:25 PM, ShyhWeir Tzang <swtzang at gmail.com> wrote:
> Dear all:
>
> I tried to install the package dynamo but with the following messages:
>
> ....Error in inDL(x, as.logical(local), as.logical(now), ...) :
> cannot load the... ?uC:/PROGRA~1/R/R-27~1.1/library/dynamo/libs/dynamo.dll?v?G
>  LoadLibrary failure:  ....
>
> I checked the dll file and it's OK. Can anyone help me find out what went
> wrong with the installation?
>
> Thanks for help.
>
> SWT
>
>        [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From rbali at ufmg.br  Fri Jul 25 18:31:54 2008
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Fri, 25 Jul 2008 12:31:54 -0400
Subject: [R-SIG-Finance] dynamo installation
References: <c17037a10807242025mb2f7182gddf2437bb2fcdd63@mail.gmail.com>
	<e8e755250807250919i740c967cve7430aff31ff64b9@mail.gmail.com>
Message-ID: <A66EB2892B0A4F33B3120350074F5450@HPR>

ShyhWeir,

Are you trying to install on Windows Vista? If it is the case, you need to 
run R as administrator (if not done yet)...

--------------------------------------------------
From: "Jeff Ryan" <jeff.a.ryan at gmail.com>
Sent: Friday, July 25, 2008 12:19 PM
To: "ShyhWeir Tzang" <swtzang at gmail.com>
Cc: <r-sig-finance at stat.math.ethz.ch>
Subject: Re: [R-SIG-Finance] dynamo installation

> Have you attempted to contact the maintainer?
>
> You should also follow the posting guide and include some (all!)
> information about your installation ( sessionInfo() output is a start)
> as well as making sure you convey that you have the dependencies as
> required (gsl version 1.5 or greater).
>
> More of a help list, not so much a solve my problem list :)
>
> How exactly did you do:
>> I checked the dll file and it's OK.
> ???
>
> Jeff
>
> On Thu, Jul 24, 2008 at 10:25 PM, ShyhWeir Tzang <swtzang at gmail.com> 
> wrote:
>> Dear all:
>>
>> I tried to install the package dynamo but with the following messages:
>>
>> ....Error in inDL(x, as.logical(local), as.logical(now), ...) :
>> cannot load the... 
>> ?uC:/PROGRA~1/R/R-27~1.1/library/dynamo/libs/dynamo.dll?v?G
>>  LoadLibrary failure:  ....
>>
>> I checked the dll file and it's OK. Can anyone help me find out what went
>> wrong with the installation?
>>
>> Thanks for help.
>>
>> SWT
>>
>>        [[alternative HTML version deleted]]
>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first. 


From burkett at uri.edu  Fri Jul 25 22:42:52 2008
From: burkett at uri.edu (John P. Burkett)
Date: Fri, 25 Jul 2008 16:42:52 -0400
Subject: [R-SIG-Finance] fPortfolio, portfolioFrontier,
 data as mean vector and covariance matrix
Message-ID: <488A3ACC.50202@uri.edu>

The reference manual for the fPortfolio package version 260.72 (February 
16, 2008) states that the data argument of the portfolioFrontier 
function "can be either the mean vector and the covariance matrix in a 
equivalently named list or a time series table" (p. 7). So far I have 
succeeded only with a time series table.  My attempts to use a mean 
vector and covariance matrix elicit error messages.

Discussion of a similar issue in March is archived in Nabble at 
http://www.nabble.com/Direct-Specification-of-Mu-and-Sigma-in-fportfolio-td16392702.html
I wonder whether there is anything new to say now.

As a concrete example of the problem, consider the following code and 
error message:

library(fPortfolio)
returnsdata <- read.csv("realreturns.csv",header=T,sep=",")
attach(returnsdata)
mu <- mean(returnsdata[,2:23]) # unweighted mean returns
Sigma <-  cov(returnsdata[,2:23]) #covariance matrix of returns
lms <- list(mu, Sigma)
Spec = portfolioSpec()
setNFrontierPoints(Spec) = 100
Spec
Constraint = c("minW[1:nAssets]=0")
frontier = portfolioFrontier(lms, Spec, Constraint)
Error: class(data) == "timeSeries" is not TRUE

John

-- 
John P. Burkett
Department of Environmental and Natural Resource Economics
and Department of Economics
University of Rhode Island
Kingston, RI 02881-0808
USA

phone (401) 874-9195


From jeff.a.ryan at gmail.com  Sat Jul 26 16:18:23 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sat, 26 Jul 2008 09:18:23 -0500
Subject: [R-SIG-Finance] Feature request to the xls package. Add
	to.4hours(x, name)
In-Reply-To: <802158.3325.qm@web28106.mail.ukl.yahoo.com>
References: <e8e755250807230608j5f3cffd2u824604cc1f83e562@mail.gmail.com>
	<802158.3325.qm@web28106.mail.ukl.yahoo.com>
Message-ID: <e8e755250807260718w6a6a81bai6592bb8a761d00cf@mail.gmail.com>

Hi Pierre:

In my version of xts (which may be slightly updated from the CRAN
version --- though available from R-forge).

> to.hourly(xH,k=4)
                   xH.Open xH.High xH.Low xH.Close xH.Volume
2008-07-07 03:00:00  1.5691  1.5705 1.5633   1.5643       945
2008-07-07 07:00:00  1.5642  1.5653 1.5625   1.5641      1277
2008-07-07 11:00:00  1.5640  1.5663 1.5611   1.5658      3058
2008-07-07 15:00:00  1.5659  1.5671 1.5621   1.5634      2660
2008-07-07 19:00:00  1.5633  1.5753 1.5631   1.5745      3139
2008-07-07 23:00:00  1.5744  1.5748 1.5707   1.5722      1240

That matches the output you have.

The ability to use the indexAt argument has just been added.  So now
you can match the output exactly.

> to.hourly(xH,k=4, indexAt='startof')
                   xH.Open xH.High xH.Low xH.Close xH.Volume
2008-07-07 00:00:00  1.5691  1.5705 1.5633   1.5643       945
2008-07-07 04:00:00  1.5642  1.5653 1.5625   1.5641      1277
2008-07-07 08:00:00  1.5640  1.5663 1.5611   1.5658      3058
2008-07-07 12:00:00  1.5659  1.5671 1.5621   1.5634      2660
2008-07-07 16:00:00  1.5633  1.5753 1.5631   1.5745      3139
2008-07-07 20:00:00  1.5744  1.5748 1.5707   1.5722      1240



On Fri, Jul 25, 2008 at 2:57 AM,  <pierre8r-list at yahoo.fr> wrote:
> Hi Jeff,
>
>> I suspect you mean the 'xts' package.  If so try:
>
> Well guess!  ;-)
>
> I have downloaded  hourly and four hours Forex datas from
> http://www.metaquotes.net/
>
> Then I have used :
> to.hourly(x, k=4)
> with the hourly datas to compare.
> But the results are not the same, why ?
>
>
> My R code :
> ----------
>
> library(quantmod)
> library(xts)
>
> Lines <-
> "2008.07.07,00:00,1.5691,1.5705,1.5691,1.5702,58
> 2008.07.07,01:00,1.5701,1.5702,1.5691,1.5694,94
> 2008.07.07,02:00,1.5693,1.5693,1.5650,1.5652,278
> 2008.07.07,03:00,1.5651,1.5667,1.5633,1.5643,515
> 2008.07.07,04:00,1.5642,1.5643,1.5631,1.5637,255
> 2008.07.07,05:00,1.5638,1.5647,1.5634,1.5645,301
> 2008.07.07,06:00,1.5644,1.5653,1.5640,1.5640,341
> 2008.07.07,07:00,1.5641,1.5642,1.5625,1.5641,380
> 2008.07.07,08:00,1.5640,1.5646,1.5617,1.5627,590
> 2008.07.07,09:00,1.5628,1.5639,1.5611,1.5623,810
> 2008.07.07,10:00,1.5622,1.5647,1.5611,1.5643,808
> 2008.07.07,11:00,1.5642,1.5663,1.5642,1.5658,850
> 2008.07.07,12:00,1.5659,1.5667,1.5627,1.5663,802
> 2008.07.07,13:00,1.5665,1.5671,1.5651,1.5654,436
> 2008.07.07,14:00,1.5653,1.5659,1.5641,1.5649,599
> 2008.07.07,15:00,1.5648,1.5651,1.5621,1.5634,823
> 2008.07.07,16:00,1.5633,1.5663,1.5631,1.5663,694
> 2008.07.07,17:00,1.5662,1.5674,1.5645,1.5645,815
> 2008.07.07,18:00,1.5646,1.5718,1.5645,1.5717,840
> 2008.07.07,19:00,1.5716,1.5753,1.5715,1.5745,790
> 2008.07.07,20:00,1.5744,1.5748,1.5725,1.5730,510
> 2008.07.07,21:00,1.5729,1.5730,1.5707,1.5718,438
> 2008.07.07,22:00,1.5719,1.5730,1.5716,1.5726,223
> 2008.07.07,23:00,1.5725,1.5728,1.5720,1.5722,69"
>
> quotes <- read.csv(textConnection(Lines), header=FALSE)
> xH <- as.xts(quotes[,-(1:2)],
> as.POSIXct(paste(quotes[,1],quotes[,2]),format='%Y.%m.%d %H:%M'))
> colnames(xH) <- c('Open','High','Low','Close','Volume')
> xH
>
> x4H <- to.hourly(xH, k=4)
> x4H
>
>
>
> Datas 60 minutes from  http://www.metaquotes.net/ :
> --------------------------------------------------
> 2008.07.07,00:00,1.5691,1.5705,1.5691,1.5702,58
> 2008.07.07,01:00,1.5701,1.5702,1.5691,1.5694,94
> 2008.07.07,02:00,1.5693,1.5693,1.5650,1.5652,278
> 2008.07.07,03:00,1.5651,1.5667,1.5633,1.5643,515
> 2008.07.07,04:00,1.5642,1.5643,1.5631,1.5637,255
> 2008.07.07,05:00,1.5638,1.5647,1.5634,1.5645,301
> 2008.07.07,06:00,1.5644,1.5653,1.5640,1.5640,341
> 2008.07.07,07:00,1.5641,1.5642,1.5625,1.5641,380
> 2008.07.07,08:00,1.5640,1.5646,1.5617,1.5627,590
> 2008.07.07,09:00,1.5628,1.5639,1.5611,1.5623,810
> 2008.07.07,10:00,1.5622,1.5647,1.5611,1.5643,808
> 2008.07.07,11:00,1.5642,1.5663,1.5642,1.5658,850
> 2008.07.07,12:00,1.5659,1.5667,1.5627,1.5663,802
> 2008.07.07,13:00,1.5665,1.5671,1.5651,1.5654,436
> 2008.07.07,14:00,1.5653,1.5659,1.5641,1.5649,599
> 2008.07.07,15:00,1.5648,1.5651,1.5621,1.5634,823
> 2008.07.07,16:00,1.5633,1.5663,1.5631,1.5663,694
> 2008.07.07,17:00,1.5662,1.5674,1.5645,1.5645,815
> 2008.07.07,18:00,1.5646,1.5718,1.5645,1.5717,840
> 2008.07.07,19:00,1.5716,1.5753,1.5715,1.5745,790
> 2008.07.07,20:00,1.5744,1.5748,1.5725,1.5730,510
> 2008.07.07,21:00,1.5729,1.5730,1.5707,1.5718,438
> 2008.07.07,22:00,1.5719,1.5730,1.5716,1.5726,223
> 2008.07.07,23:00,1.5725,1.5728,1.5720,1.5722,69
>
>
> Datas 240 minutes from  http://www.metaquotes.net/ :
> --------------------------------------------------
> 2008.07.07,00:00,1.5691,1.5705,1.5633,1.5643,945
> 2008.07.07,04:00,1.5642,1.5653,1.5625,1.5641,1277
> 2008.07.07,08:00,1.5640,1.5663,1.5611,1.5658,3058
> 2008.07.07,12:00,1.5659,1.5671,1.5621,1.5634,2660
> 2008.07.07,16:00,1.5633,1.5753,1.5631,1.5745,3139
> 2008.07.07,20:00,1.5744,1.5748,1.5707,1.5722,1240
>
>
> Output datas from the R code :
> -----------------------------
>                    xH.Open xH.High xH.Low xH.Close xH.Volume
> 2008-07-07 01:00:00  1.5691  1.5705 1.5691   1.5694       152
> 2008-07-07 05:00:00  1.5693  1.5693 1.5631   1.5645      1349
> 2008-07-07 09:00:00  1.5644  1.5653 1.5611   1.5623      2121
> 2008-07-07 13:00:00  1.5622  1.5671 1.5611   1.5654      2896
> 2008-07-07 17:00:00  1.5653  1.5674 1.5621   1.5645      2931
> 2008-07-07 21:00:00  1.5646  1.5753 1.5645   1.5718      2578
> 2008-07-07 23:00:00  1.5719  1.5730 1.5716   1.5722       292
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>      ____________________________________________________________
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From spencer.graves at pdf.com  Mon Jul 28 06:24:59 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 27 Jul 2008 21:24:59 -0700
Subject: [R-SIG-Finance] estimating non-linear state space models
In-Reply-To: <39F7C064586B194EB8AAE4D74AB096A1B32816@EXVS02.mcis.agrinet.local>
References: <39F7C064586B194EB8AAE4D74AB096A1B32812@EXVS02.mcis.agrinet.local><39F7C064586B194EB8AAE4D74AB096A1B32813@EXVS02.mcis.agrinet.local><48873A4D.7060503@braverock.com><C4DFF417DAED416EBA949AB0E277BDF4@HPR>	<39F7C064586B194EB8AAE4D74AB096A1B32814@EXVS02.mcis.agrinet.local>
	<39F7C064586B194EB8AAE4D74AB096A1B32816@EXVS02.mcis.agrinet.local>
Message-ID: <488D4A1B.8060106@pdf.com>

      Have you looked at the 'dlm' package for "Bayesian and Likelihood 
Analysis of Dynamic Linear Models"?  It has optional parameters 'JFF', 
'JV', 'JGG', 'JW', and 'X' to support nonlinear modifications of the 
standard Kalman terms 'm0', 'C0', 'FF', 'V', 'GG', 'W'.  Moreover, it 
has a vignette that helps learning its capabilities.  This should 
support your needs provided the (standardized) residuals can plausibly 
be assumed to be normally distributed. 

      Hope this helps. 
      Spencer

Andreas wrote:
> PS: The ML-calculation in the EFK-function shoud be:
>  
> return(sum(log(Inno)) + sum(Kerror^2/Inno))
>  
> instead of Perror, I missed that line while renaming in the version I attached earlier.
>  
>
> ________________________________
>
> Von: r-sig-finance-bounces at stat.math.ethz.ch im Auftrag von Andreas
> Gesendet: Do 24.07.2008 11:46
> An: r-sig-finance at stat.math.ethz.ch
> Betreff: [R-SIG-Finance] estimating non-linear state space models
>
>
> I had to clean up my EKF code first, after trying many different ideas I ended up with a huge mess... It's still heavily under development, but I think it could serve as a starting point.
>
>
>
> As I wrote before, state estimation seems to run OK given the true parameters.
>
>
>
> When I try to estimate the parameters by maximizing the likelihood, I end up with rather random results depending on the initial parameters I start optimizing with.
>
>
>
> I don't know if there's an error in the calculation of the likelihood, or if I'm just overstraining the ML-method by estimating model parameters and noise variances at the same time. Is this even possible? Or maybe I'm just expecting too precise results...
>
>
>
> Regards
>
> Andreas
>
>
>
>
>
>
> ________________________________
>
> Von: Robert Iquiapaza [mailto:rbali at ufmg.br]
> Gesendet: Do 24.07.2008 03:21
> An: Andreas
> Cc: r-sig-finance at stat.math.ethz.ch
> Betreff: Re: [R-SIG-Finance] estimating non-linear state space models
>
>
>
> Just for those interested. The final version of that paper was published in
> Review of Financial Studies 2008 21(1):233-264; doi:10.1093/rfs/hhm049.
> Regards
>
> Robert Iquiapaza
> ri2162 at columbia.edu
>
> --------------------------------------------------
> From: "Brian G. Peterson" <brian at braverock.com>
> Sent: Wednesday, July 23, 2008 10:03 AM
> To: "r_sig_finance" <r_sig_finance at greenmail.ch>
> Cc: <r-sig-finance at stat.math.ethz.ch>
> Subject: Re: [R-SIG-Finance] estimating non-linear state space models
>
>   
>> Andreas wrote:
>>     
>>> I am trying to estimate the dynamic model for equity fund's alphas and
>>> betas described here:
>>> http://papers.ssrn.com/sol3/papers.cfm?abstract_id=389740
>>> <http://papers.ssrn.com/sol3/papers.cfm?abstract_id=389740>  . The
>>> nonlinear state space model is described by equations (6) and (11). (For
>>> those in a hurry: The one dimensional state follows an AR1 process. The
>>> observation equation has similarities with CAPM, but is extended to
>>> depend quadratically on the state)
>>>
>>> So far I have tried to work with the packages sspir and dse, but they
>>> don't seem to support non-linear models. I then tried to implement my own
>>> EKF code, it works for state estimation but so far I couldn't get the
>>> parameter and variance estimation running reliably.
>>>       
>> You might try posting your code here, and being very specific about what
>> help you need.  That way everyone can benefit from an implementation of
>> these models in R.
>>
>> Regards,
>>
>>   - Brian
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>     
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From pierre8r-list at yahoo.fr  Mon Jul 28 20:28:35 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Mon, 28 Jul 2008 18:28:35 +0000 (GMT)
Subject: [R-SIG-Finance] Feature request to the xls package. Add
	to.4hours(x, name)
In-Reply-To: <e8e755250807260718w6a6a81bai6592bb8a761d00cf@mail.gmail.com>
Message-ID: <559207.36139.qm@web28108.mail.ukl.yahoo.com>

It woks.
Thanks Jeff.




      ____________________________________________________________
ente http://mail.yahoo.fr


From feanor0 at hotmail.com  Wed Jul 30 17:36:02 2008
From: feanor0 at hotmail.com (Murali Menon)
Date: Wed, 30 Jul 2008 15:36:02 +0000
Subject: [R-SIG-Finance] VaR.Beyond() etc.
Message-ID: <BLU105-W37B41A013807722B36FBC6EE7D0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080730/20c23217/attachment.pl>

From brian at braverock.com  Wed Jul 30 18:32:36 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 30 Jul 2008 11:32:36 -0500
Subject: [R-SIG-Finance] VaR.Beyond() etc.
In-Reply-To: <BLU105-W37B41A013807722B36FBC6EE7D0@phx.gbl>
References: <BLU105-W37B41A013807722B36FBC6EE7D0@phx.gbl>
Message-ID: <489097A4.2040903@braverock.com>

Murali Menon wrote:
> I see that the definition of VaR.Beyond() in the PerformanceAnalytics 
> package includes a parameter called periods, to determine the expected 
> shortfall across various time horizons. However, it is not used within 
> the function, at least not in PerformanceAnalytics_0.9.6, which is what 
> I have. Has this been remedied?
>  
> Likewise, does it make sense to have a multiperiod VaR method as well?

First of all, we're including new CVaR/ES functions in 
PerformanceAnalytics 0.9.7 that provide more functionality than the 
original VaR.Beyond function provides, including an implementation of a 
Modified Cornish Fisher Expected Shortfall that we developed for an 
upcoming paper in the Journal of Risk (to appear Winter 2008).

PerformanceAnalytics 0.9.7 will be released as soon as we finish the 
documentation for a few of the new functions.

> I'm not sure how to incorporate the notion of horizons into the 
> computation or I'd have done this myself. Is there any paper that helps 
> to explain the concept?

There are multiple papers on this topic, which I'll look up later if I 
have time, but I'll summarize some of the relevant approaches here.

The simplest methods incorporate some sort of decay function.  If you 
are assuming a normal distribution, as Gaussian parametric VaR and ES 
do, then you would multiply the single period VaR number by the square 
root of the number of periods to scale over, as is commonly done with 
variance.  This is the method that was implemented in an earlier version 
of the VaR.Beyond function, although I believe I took it out because it 
was having issues. (I'm also unconvinced of the accuracy of this 
approach, although it is most likely true if you assume a Gaussian 
distribution)

Another major method looks at conditional or unconditional risk against 
one or more factors over some period, and scales the risk metric that 
way.  (I think this is likely the most accurate, but also the most 
difficult approach)

A third method is typically based on Monte Carlo simulation, where the 
multiperiod risk can be taken directly as a quantile of the simulated 
results.  (This is the method most often employed by banks as part of 
their approach for calculating regulatory capital, in my experience.)

There have also been papers about using GARCH models to scale risk 
measure to multiple periods, but I recall reading a paper by Embrechts 
and McNeil that called that methodology into question.

Regards,

   - Brian


From josh.m.ulrich at gmail.com  Wed Jul 30 21:14:39 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Wed, 30 Jul 2008 14:14:39 -0500
Subject: [R-SIG-Finance] Invitation to join the R-Finance Group on LinkedIn
In-Reply-To: <8cca69990807301201i7c22204dm23e2d169c30f3cb4@mail.gmail.com>
References: <8cca69990807301201i7c22204dm23e2d169c30f3cb4@mail.gmail.com>
Message-ID: <8cca69990807301214g5c49bb6w455582555b76d5f6@mail.gmail.com>

Hello All,

Those of you who keep a professional network on LinkedIn are invited
to join the newly-created R-Finance group.  Joining will allow you to
find and contact other R-Finance members on LinkedIn.  The goal of
this group is to help members:

- Reach other finance useRs (you decide if group members can contact
you directly)
- Accelerate careers/business through referrals from R-Finance group members
- Know more than a name; view other members' rich, professional profiles

Here's the link to join:
http://www.linkedin.com/e/gis/155029/10C6A51CE56D

Hope to see you in the group,
Josh Ulrich (TTR, xts, opentick)
Jeff Ryan (quantmod, xts, IBrokers)


From pierre8r-list at yahoo.fr  Wed Jul 30 21:28:54 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Wed, 30 Jul 2008 19:28:54 +0000 (GMT)
Subject: [R-SIG-Finance] Can't load the quantmod library, without errors.
Message-ID: <462538.90347.qm@web28102.mail.ukl.yahoo.com>

Hello,

If I try to load the quantmod library, I got these errors messages :


> 
> library(quantmod)
Loading required package: xts
Loading required package: zoo
Error in loadNamespace(i, c(lib.loc, .libPaths())) : 
  there is no package called 'lattice'
Error: package 'zoo' could not be loaded
> 


Thanks in advance.

Pierre8r





      ____________________________________________________________
ente http://mail.yahoo.fr


From jeff.a.ryan at gmail.com  Wed Jul 30 21:57:20 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 30 Jul 2008 14:57:20 -0500
Subject: [R-SIG-Finance] Can't load the quantmod library, without errors.
In-Reply-To: <462538.90347.qm@web28102.mail.ukl.yahoo.com>
References: <462538.90347.qm@web28102.mail.ukl.yahoo.com>
Message-ID: <e8e755250807301257m50e8cb12xf60a4a046a942dd3@mail.gmail.com>

zoo depends on:
Depends:	R (? 2.4.1), stats
Imports:	stats, utils, graphics, grDevices, lattice

So you will need lattice --- which should be in the default install of R.

Take a look at your R install itself.  Or try library(lattice) and see
what you get.

Jeff

On Wed, Jul 30, 2008 at 2:28 PM,  <pierre8r-list at yahoo.fr> wrote:
> Hello,
>
> If I try to load the quantmod library, I got these errors messages :
>
>
>>
>> library(quantmod)
> Loading required package: xts
> Loading required package: zoo
> Error in loadNamespace(i, c(lib.loc, .libPaths())) :
>  there is no package called 'lattice'
> Error: package 'zoo' could not be loaded
>>
>
>
> Thanks in advance.
>
> Pierre8r
>
>
>
>
>
>      ____________________________________________________________
> ente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com

From pierre8r-list at yahoo.fr  Wed Jul 30 22:21:54 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Wed, 30 Jul 2008 20:21:54 +0000 (GMT)
Subject: [R-SIG-Finance] Can't load the quantmod library, without errors.
In-Reply-To: <e8e755250807301257m50e8cb12xf60a4a046a942dd3@mail.gmail.com>
Message-ID: <712315.62987.qm@web28107.mail.ukl.yahoo.com>


> zoo depends on:
> Depends:	R (? 2.4.1), stats
> Imports:	stats, utils, graphics, grDevices, lattice
> 
> So you will need lattice --- which should be in the default
> install of R.


I have reinstalled lattice.

No more errors messages.

But it seen I got the old wrong results.


My R code :
----------
library(quantmod)
library(xts)

Lines <-                                                                         
"2008.07.07,00:00,1.5691,1.5705,1.5691,1.5702,58
2008.07.07,01:00,1.5701,1.5702,1.5691,1.5694,94
2008.07.07,02:00,1.5693,1.5693,1.5650,1.5652,278
2008.07.07,03:00,1.5651,1.5667,1.5633,1.5643,515
2008.07.07,04:00,1.5642,1.5643,1.5631,1.5637,255
2008.07.07,05:00,1.5638,1.5647,1.5634,1.5645,301
2008.07.07,06:00,1.5644,1.5653,1.5640,1.5640,341
2008.07.07,07:00,1.5641,1.5642,1.5625,1.5641,380
2008.07.07,08:00,1.5640,1.5646,1.5617,1.5627,590
2008.07.07,09:00,1.5628,1.5639,1.5611,1.5623,810
2008.07.07,10:00,1.5622,1.5647,1.5611,1.5643,808
2008.07.07,11:00,1.5642,1.5663,1.5642,1.5658,850
2008.07.07,12:00,1.5659,1.5667,1.5627,1.5663,802
2008.07.07,13:00,1.5665,1.5671,1.5651,1.5654,436
2008.07.07,14:00,1.5653,1.5659,1.5641,1.5649,599
2008.07.07,15:00,1.5648,1.5651,1.5621,1.5634,823
2008.07.07,16:00,1.5633,1.5663,1.5631,1.5663,694
2008.07.07,17:00,1.5662,1.5674,1.5645,1.5645,815
2008.07.07,18:00,1.5646,1.5718,1.5645,1.5717,840
2008.07.07,19:00,1.5716,1.5753,1.5715,1.5745,790
2008.07.07,20:00,1.5744,1.5748,1.5725,1.5730,510
2008.07.07,21:00,1.5729,1.5730,1.5707,1.5718,438
2008.07.07,22:00,1.5719,1.5730,1.5716,1.5726,223
2008.07.07,23:00,1.5725,1.5728,1.5720,1.5722,69"

quotes <- read.csv(textConnection(Lines), header=FALSE)
xH <- as.xts(quotes[,-(1:2)],
as.POSIXct(paste(quotes[,1],quotes[,2]),format='%Y.%m.%d %H:%M'))
colnames(xH) <- c('Open','High','Low','Close','Volume')
xH

x4H <- to.hourly(xH,k=4, indexAt='startof')
x4H


Datas 240 minutes from  http://www.metaquotes.net/ :
--------------------------------------------------
2008.07.07,00:00,1.5691,1.5705,1.5633,1.5643,945
2008.07.07,04:00,1.5642,1.5653,1.5625,1.5641,1277
2008.07.07,08:00,1.5640,1.5663,1.5611,1.5658,3058
2008.07.07,12:00,1.5659,1.5671,1.5621,1.5634,2660
2008.07.07,16:00,1.5633,1.5753,1.5631,1.5745,3139
2008.07.07,20:00,1.5744,1.5748,1.5707,1.5722,1240


Output datas from the R code :
-----------------------------
                    xH.Open xH.High xH.Low xH.Close xH.Volume
2008-07-07 00:00:00  1.5691  1.5705 1.5691   1.5694       152
2008-07-07 02:00:00  1.5693  1.5693 1.5631   1.5645      1349
2008-07-07 06:00:00  1.5644  1.5653 1.5611   1.5623      2121
2008-07-07 10:00:00  1.5622  1.5671 1.5611   1.5654      2896
2008-07-07 14:00:00  1.5653  1.5674 1.5621   1.5645      2931
2008-07-07 18:00:00  1.5646  1.5753 1.5645   1.5718      2578
2008-07-07 22:00:00  1.5719  1.5730 1.5716   1.5722       292


Here my installed packages :
----------------------------

> 
> installed.packages()
           Package      LibPath                          Version   Priority      Bundle Contains                 
ade4       "ade4"       "p:/PROGRA~1/R/R-27~1.1/library" "1.4-9"   NA            NA     NA                       
base       "base"       "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
boot       "boot"       "p:/PROGRA~1/R/R-27~1.1/library" "1.2-33"  "recommended" NA     NA                       
class      "class"      "p:/PROGRA~1/R/R-27~1.1/library" "7.2-42"  "recommended" "VR"   "MASS class nnet spatial"
cluster    "cluster"    "p:/PROGRA~1/R/R-27~1.1/library" "1.11.11" "recommended" NA     NA                       
codetools  "codetools"  "p:/PROGRA~1/R/R-27~1.1/library" "0.2-1"   "recommended" NA     NA                       
datasets   "datasets"   "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
Defaults   "Defaults"   "p:/PROGRA~1/R/R-27~1.1/library" "1.1-1"   NA            NA     NA                       
foreign    "foreign"    "p:/PROGRA~1/R/R-27~1.1/library" "0.8-27"  "recommended" NA     NA                       
graphics   "graphics"   "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
grDevices  "grDevices"  "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
grid       "grid"       "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
iplots     "iplots"     "p:/PROGRA~1/R/R-27~1.1/library" "1.1-2"   NA            NA     NA                       
JavaGD     "JavaGD"     "p:/PROGRA~1/R/R-27~1.1/library" "0.5-1"   NA            NA     NA                       
JGR        "JGR"        "p:/PROGRA~1/R/R-27~1.1/library" "1.6-2"   NA            NA     NA                       
KernSmooth "KernSmooth" "p:/PROGRA~1/R/R-27~1.1/library" "2.22-22" "recommended" NA     NA                       
lattice    "lattice"    "p:/PROGRA~1/R/R-27~1.1/library" "0.17-12" "recommended" NA     NA                       
MASS       "MASS"       "p:/PROGRA~1/R/R-27~1.1/library" "7.2-42"  "recommended" "VR"   "MASS class nnet spatial"
methods    "methods"    "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
mgcv       "mgcv"       "p:/PROGRA~1/R/R-27~1.1/library" "1.4-1"   "recommended" NA     NA                       
nlme       "nlme"       "p:/PROGRA~1/R/R-27~1.1/library" "3.1-89"  "recommended" NA     NA                       
nnet       "nnet"       "p:/PROGRA~1/R/R-27~1.1/library" "7.2-42"  "recommended" "VR"   "MASS class nnet spatial"
pixmap     "pixmap"     "p:/PROGRA~1/R/R-27~1.1/library" "0.4-9"   NA            NA     NA                       
quadprog   "quadprog"   "p:/PROGRA~1/R/R-27~1.1/library" "1.4-11"  NA            NA     NA                       
quantmod   "quantmod"   "p:/PROGRA~1/R/R-27~1.1/library" "0.3-6"   NA            NA     NA                       
rJava      "rJava"      "p:/PROGRA~1/R/R-27~1.1/library" "0.5-1"   NA            NA     NA                       
rpart      "rpart"      "p:/PROGRA~1/R/R-27~1.1/library" "3.1-41"  "recommended" NA     NA                       
spatial    "spatial"    "p:/PROGRA~1/R/R-27~1.1/library" "7.2-42"  "recommended" "VR"   "MASS class nnet spatial"
splines    "splines"    "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
stats      "stats"      "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
stats4     "stats4"     "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
survival   "survival"   "p:/PROGRA~1/R/R-27~1.1/library" "2.34-1"  "recommended" NA     NA                       
tcltk      "tcltk"      "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
tools      "tools"      "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
tseries    "tseries"    "p:/PROGRA~1/R/R-27~1.1/library" "0.10-16" NA            NA     NA                       
TTR        "TTR"        "p:/PROGRA~1/R/R-27~1.1/library" "0.14-0"  NA            NA     NA                       
utils      "utils"      "p:/PROGRA~1/R/R-27~1.1/library" "2.7.1"   "base"        NA     NA                       
xts        "xts"        "p:/PROGRA~1/R/R-27~1.1/library" "0.0-16"  NA            NA     NA                       
zoo        "zoo"        "p:/PROGRA~1/R/R-27~1.1/library" "1.5-4"   NA            NA     NA                       








      ____________________________________________________________

From pierre8r-list at yahoo.fr  Thu Jul 31 11:13:55 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Thu, 31 Jul 2008 09:13:55 +0000 (GMT)
Subject: [R-SIG-Finance] Can't load the quantmod library, without errors.
In-Reply-To: <712315.62987.qm@web28107.mail.ukl.yahoo.com>
Message-ID: <711899.85217.qm@web28105.mail.ukl.yahoo.com>

Maybe the errors come because I did a packages update.
Can't remember which packages I updated.
	
Is it normal that xts is still release 0.0-15 on CRAN ?

Pierre8r


      ____________________________________________________
intelligente http://mail.yahoo.fr


From jeff.a.ryan at gmail.com  Thu Jul 31 14:31:24 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 31 Jul 2008 07:31:24 -0500
Subject: [R-SIG-Finance] Can't load the quantmod library, without errors.
In-Reply-To: <711899.85217.qm@web28105.mail.ukl.yahoo.com>
References: <712315.62987.qm@web28107.mail.ukl.yahoo.com>
	<711899.85217.qm@web28105.mail.ukl.yahoo.com>
Message-ID: <e8e755250807310531r12e3c863v403e9c7526bcba5e@mail.gmail.com>

Hi Pierre,

Yes 0.0-15 is the released version.  The one you had prior that worked
the way you wanted (with the indexAt arg for hours) is 0.0-16.  That
source is on R-forge.

I will make sure that 0.0-16 passes R CMD check today and upload that
to CRAN, which should then post in a day or so.

Thanks for pointing that out.


Jeff

On Thu, Jul 31, 2008 at 4:13 AM,  <pierre8r-list at yahoo.fr> wrote:
> Maybe the errors come because I did a packages update.
> Can't remember which packages I updated.
>
> Is it normal that xts is still release 0.0-15 on CRAN ?
>
> Pierre8r
>
>
>      ____________________________________________________
> intelligente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From pierre8r-list at yahoo.fr  Thu Jul 31 15:34:23 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Thu, 31 Jul 2008 13:34:23 +0000 (GMT)
Subject: [R-SIG-Finance] Can't load the quantmod library, without errors.
In-Reply-To: <e8e755250807310531r12e3c863v403e9c7526bcba5e@mail.gmail.com>
Message-ID: <292072.74094.qm@web28104.mail.ukl.yahoo.com>




> Hi Pierre,
> 
> Yes 0.0-15 is the released version.  The one you had prior
> that worked
> the way you wanted (with the indexAt arg for hours) is
> 0.0-16.  That
> source is on R-forge.
> 
> I will make sure that 0.0-16 passes R CMD check today and
> upload that
> to CRAN, which should then post in a day or so.

Hi Jeff,

Do you feel it will resolve my question about the x4H data, data I expected and data I get ?
x4H <- to.hourly(xH,k=4, indexAt='startof')

Pierre8r


> 
> Thanks for pointing that out.
> 
> 
> Jeff
> 



      ____________________________________________________________
ente http://mail.yahoo.fr


From jeff.a.ryan at gmail.com  Thu Jul 31 16:20:32 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 31 Jul 2008 09:20:32 -0500
Subject: [R-SIG-Finance] Can't load the quantmod library, without errors.
In-Reply-To: <292072.74094.qm@web28104.mail.ukl.yahoo.com>
References: <e8e755250807310531r12e3c863v403e9c7526bcba5e@mail.gmail.com>
	<292072.74094.qm@web28104.mail.ukl.yahoo.com>
Message-ID: <e8e755250807310720x1e09fa48v6e99f47f8bb38122@mail.gmail.com>

That should solve your issue.

Jeff
On Thu, Jul 31, 2008 at 8:34 AM,  <pierre8r-list at yahoo.fr> wrote:
>
>
>
>> Hi Pierre,
>>
>> Yes 0.0-15 is the released version.  The one you had prior
>> that worked
>> the way you wanted (with the indexAt arg for hours) is
>> 0.0-16.  That
>> source is on R-forge.
>>
>> I will make sure that 0.0-16 passes R CMD check today and
>> upload that
>> to CRAN, which should then post in a day or so.
>
> Hi Jeff,
>
> Do you feel it will resolve my question about the x4H data, data I expected and data I get ?
> x4H <- to.hourly(xH,k=4, indexAt='startof')
>
> Pierre8r
>
>
>>
>> Thanks for pointing that out.
>>
>>
>> Jeff
>>
>
>
>
>      ____________________________________________________________
> ente http://mail.yahoo.fr
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From imanpreet at gmail.com  Thu Jul 31 23:30:55 2008
From: imanpreet at gmail.com (Imanpreet)
Date: Fri, 1 Aug 2008 03:00:55 +0530
Subject: [R-SIG-Finance] Finance Benchmarks/Workload
In-Reply-To: <c26b95920807241231u5384379cw3f85ab6ed4e11c8d@mail.gmail.com>
References: <c26b95920807240344g1a0e525oa6d502d680eced97@mail.gmail.com>
	<4888A85C.6050804@braverock.com>
	<c26b95920807241231u5384379cw3f85ab6ed4e11c8d@mail.gmail.com>
Message-ID: <c26b95920807311430x46aff623t2dcd7504953d186a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080801/bb233fde/attachment.pl>

From pierre8r-list at yahoo.fr  Fri Aug  1 10:10:07 2008
From: pierre8r-list at yahoo.fr (pierre8r-list at yahoo.fr)
Date: Fri, 1 Aug 2008 08:10:07 +0000 (GMT)
Subject: [R-SIG-Finance] How to change the way R format his output ?
Message-ID: <611017.88958.qm@web28108.mail.ukl.yahoo.com>

Hi,

I try to :
Load 1 minute quotes from http://www.metaquotes.net/ into R.
Compute 15 mns quotes, 1 hour, daily quotes, etc. with R from 1 mn quotes.
Write to files these quotes.
Compare quotes from http://www.metaquotes.net/  with quotes from R, with Winmerge ( http://winmerge.org/ ), I compare directories.

How to change the way R format his output, and than Winmerge will not show any difference between the two files ?


Datas 15 minutes from  http://www.metaquotes.net/ :
-------------------------------------------------- 
2008.07.01,00:00,1.5754,1.5757,1.5753,1.5757,23
2008.07.01,00:15,1.5756,1.5758,1.5753,1.5758,48
2008.07.01,00:30,1.5757,1.5758,1.5752,1.5754,32
2008.07.01,00:45,1.5755,1.5755,1.5747,1.5748,29
2008.07.01,01:00,1.5747,1.5758,1.5747,1.5757,66
2008.07.01,01:15,1.5756,1.5757,1.5752,1.5755,46
2008.07.01,01:30,1.5754,1.5756,1.5751,1.5754,65


Output 15 minutes datas from the R code :
----------------------------------------
2008.07.01,00:14,  1.5754,  1.5757,  1.5753,  1.5757, 23.0000
2008.07.01,00:29,  1.5756,  1.5758,  1.5753,  1.5758, 48.0000
2008.07.01,00:44,  1.5757,  1.5758,  1.5752,  1.5754, 32.0000
2008.07.01,00:59,  1.5755,  1.5755,  1.5747,  1.5748, 29.0000
2008.07.01,01:14,  1.5747,  1.5758,  1.5747,  1.5757, 66.0000
2008.07.01,01:29,  1.5756,  1.5757,  1.5752,  1.5755, 46.0000
2008.07.01,01:44,  1.5754,  1.5756,  1.5751,  1.5754, 65.0000



My R code :
----------
#-------------------------------------------------------------------------#

"InputQuotesMT4" <-
function( FicIn = "ErrorFicIn", RepIn = "ErrorRepIn" ){

  InputFile <- paste(collapse="", c(RepIn, FicIn))

  quotes <- read.csv(InputFile, header=FALSE)
  IQuotes <- as.xts(quotes[,-(1:2)],
  as.POSIXct(paste(quotes[,1],quotes[,2]),format='%Y.%m.%d %H:%M'))
  colnames(IQuotes) <- c('Open','High','Low','Close','Volume')

  return( IQuotes )
}

#-------------------------------------------------------------------------#

"OutputToCSV" <-
function(xFile = xFil , OD = OutputDirectory , OF = OutputFile ) {
  nameFile <- paste(collapse="", c(OD, OF,".csv"))

  write.table(format(xFile,nsmall=4, justify = "left"),
    file = nameFile, quote=FALSE,  col.names=FALSE,
  row.names=format(index(xFile),"%Y.%m.%d,%H:%M"),  sep=",") 

}

#-------------------------------------------------------------------------#
                                                       
library(xts)
library(quantmod)

InputDir <- "K:\\MT4Input\\"
InputFile <- "EURUSD1.csv"

OutputDir <- "K:\\MT4Output\\"

Quotes1MN <- InputQuotesMT4( FicIn = InputFile, RepIn = InputDir ) 
OutputToCSV(xFile = Quotes1MN, OD = OutputDir, OF = "EURUSD1")

Quotes5MNS <- to.minutes5(Quotes1MN)
OutputToCSV(xFile = Quotes5MNS, OD = OutputDir, OF = "EURUSD5")

Quotes15MNS <- to.minutes15(Quotes5MNS)
OutputToCSV(Quotes15MNS, OD = OutputDir, OF = "EURUSD15")

Quotes1H <- to.hourly(Quotes5MNS)
OutputToCSV(xFile = Quotes1H,OD = OutputDir, OF = "EURUSD60")

Quotes4H <- to.hourly(Quotes5MNS, k=4, indexAt='startof')
OutputToCSV(xFile = Quotes4H,OD = OutputDir, OF = "EURUSD240")

QuotesDay <- to.daily (Quotes1H)
OutputToCSV(xFile = QuotesDay,OD = OutputDir, OF = "EURUSD1440")

QuotesWeek <- to.weekly(QuotesDay)
OutputToCSV(xFile = QuotesWeek,OD = OutputDir, OF = "EURUSD10080")





      ____________________________________________________________

From dario.nicodemi at proximaalfa.com  Fri Aug  1 12:00:32 2008
From: dario.nicodemi at proximaalfa.com (Dario Nicodemi)
Date: Fri, 1 Aug 2008 12:00:32 +0200
Subject: [R-SIG-Finance] Probability of Default - from CDS
Message-ID: <8DC2BE23593EC34EA1E731E3AE14705F013E8129@MAH-EU-EXM-01.eu.corp.proximaalfa.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080801/67f45388/attachment.pl>

From brian at braverock.com  Fri Aug  1 17:41:54 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 01 Aug 2008 10:41:54 -0500
Subject: [R-SIG-Finance] Probability of Default - from CDS
In-Reply-To: <8DC2BE23593EC34EA1E731E3AE14705F013E8129@MAH-EU-EXM-01.eu.corp.proximaalfa.com>
References: <8DC2BE23593EC34EA1E731E3AE14705F013E8129@MAH-EU-EXM-01.eu.corp.proximaalfa.com>
Message-ID: <48932EC2.8090509@braverock.com>

Dario Nicodemi wrote:
> I was wondering if someone has already worked out some code to calculate
> the probability of default from CDS.

Perhaps a few more details are in order?

What methods would you like to use to value these CDS contracts?  What 
information do you have about the underlying debt?  Are these 
semi-standardized instruments?  What information do you know about the 
contract terms?  are they insured?  If so, by whom, and under what 
terms? etc, etc, etc

All these things go into the probability of default.

So, if you have a particular paper or methodology that you'd like to 
replicate, please do tell, so that the list can help you find code for 
specific techniques or methods.

Also, just to show my open collaboration bent, sharing what you've done 
so far  (and the final working results) generally produces more answers 
on this list.

Regards,

   - Brian


From brian at braverock.com  Sun Aug  3 17:53:35 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Sun, 03 Aug 2008 10:53:35 -0500
Subject: [R-SIG-Finance] Finance Benchmarks/Workload
In-Reply-To: <c26b95920807311430x46aff623t2dcd7504953d186a@mail.gmail.com>
References: <c26b95920807240344g1a0e525oa6d502d680eced97@mail.gmail.com>	
	<4888A85C.6050804@braverock.com>	
	<c26b95920807241231u5384379cw3f85ab6ed4e11c8d@mail.gmail.com>
	<c26b95920807311430x46aff623t2dcd7504953d186a@mail.gmail.com>
Message-ID: <4895D47F.80107@braverock.com>

"huge amounts of data and computation" doesn't say much.  You still 
haven't told the list what it is you are trying to do with your system 
so that we can give you *relevant* benchmarks.  Descriptive things like 
"building a Markowitz portfolio optimizer for 4000 instruments" or 
"calculating daily risk capital for a bank on 200,000 positions" or 
"conducting intra-day pricing on 2000 monitored municipal bonds" or 
"performing Monte Carlo risk calculations on 3 million simulated 
portfolios"etc.  All of my toy examples here have very different 
computational and data requirements. Without more clarity on what you 
are trying to do, we don't have enough information to give you relevant 
benchmark numbers.

Cheers,

    - Brian

Imanpreet wrote:
> Hello Brian,
> 
>                     If you could /still /provide any pointers for the 
> benchmarks or workloads :). Do tell if you need any more info?
> 
> Regards,
> 
>           Imanpreet Singh Arora
> 
> On Fri, Jul 25, 2008 at 1:01 AM, Imanpreet <imanpreet at gmail.com 
> <mailto:imanpreet at gmail.com>> wrote:
> 
> 
>     Hello Brian,
> 
>                 I am working on porting R to a new architecture which
>     would be most optimized only when we are benchmarking it with huge
>     amounts of data and computation. So, I will be more interested
>     towards benchmarks or workloads which would be sufficiently
>     complex.. Could you provide some pointers for the same?
> 
>     TIA
> 
>     Regards,
> 
>               Imanpreet Singh Arora
> 
> 
>     On Thu, Jul 24, 2008 at 9:35 PM, Brian G. Peterson
>     <brian at braverock.com <mailto:brian at braverock.com>> wrote:
> 
>         Imanpreet wrote:
> 
>                             Could anyone point out any existing
>             benchmarks or workloads
>             for R, specifically targeting financial applications?
> 
> 
>         That's a pretty vague request.
> 
>         I've got functions that run in microseconds, and some portfolio
>         optimizations that take hours.
> 
>         I'm aware of organizations that use huge clusters to do
>         financial analysis with R.
> 
>         Perhaps you could be a little more specific about what you are
>         trying to benchmark?
> 
>         Regards,
> 
>          - Brian


From niheaven at hotmail.com  Mon Aug  4 09:22:15 2008
From: niheaven at hotmail.com (Hsiao-nan Cheung)
Date: Mon, 4 Aug 2008 15:22:15 +0800
Subject: [R-SIG-Finance] Monte Carlo function in package 'fOptions'
Message-ID: <BAY126-DS6C9226D79FFE85E3EC984D9780@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080804/e93f0215/attachment.pl>

From hongchux at gmail.com  Mon Aug  4 15:27:53 2008
From: hongchux at gmail.com (Hongchuan Xia)
Date: Mon, 4 Aug 2008 15:27:53 +0200
Subject: [R-SIG-Finance] TARN
Message-ID: <2f7adeb90808040627s5ca9962dhffcea5cd9f19c476@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080804/19bbb4ac/attachment.pl>

From pierre8r-nabble at yahoo.fr  Mon Aug  4 19:35:15 2008
From: pierre8r-nabble at yahoo.fr (Pierre8rou)
Date: Mon, 4 Aug 2008 10:35:15 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] How to change the way R format
	his output ?
In-Reply-To: <611017.88958.qm@web28108.mail.ukl.yahoo.com>
References: <611017.88958.qm@web28108.mail.ukl.yahoo.com>
Message-ID: <18815951.post@talk.nabble.com>


I answer to myself.

Pierre8r

#-------------------------------------------------------------------------#

"InputQuotesMT4" <-
function( FicIn = "ErrorFicIn", RepIn = "ErrorRepIn" ){

  InputFile <- paste(collapse="", c(RepIn, FicIn))

  quotes <- read.csv(InputFile, header=FALSE)
  IQuotes <- as.xts(quotes[,-(1:2)],
  as.POSIXct(paste(quotes[,1],quotes[,2]),format='%Y.%m.%d %H:%M'))
  colnames(IQuotes) <- c('Open','High','Low','Close','Volume')

  return( IQuotes )
}

#-------------------------------------------------------------------------#

"OutputToCSV" <-
function(xFile = xFil , OD = OutputDirectory , OF = OutputFile ) {
  nameFile <- paste(collapse="", c(OD, OF,".csv"))
  
  colnames(xFile) <- c('Open','High','Low','Close','Volume')

  xFile$Open <- sprintf("%5.04f", xFile$Open)
  xFile$High <- sprintf("%5.04f", xFile$High)
  xFile$Low <- sprintf("%5.04f", xFile$Low)
  xFile$Close <- sprintf("%5.04f", xFile$Close)
  
  xFile$Volume <- sprintf("%.0f", xFile$Volume)
  
  write.table(xFile,file = nameFile, quote=FALSE,  col.names=FALSE,
  row.names=format(index(xFile),"%Y.%m.%d,%H:%M"),  sep=",") 

}

#-------------------------------------------------------------------------#
                                                       
library(xts)
library(quantmod)

InputDir <- "K:\\MT4Input\\"
InputFile <- "EURUSD1.csv"

OutputDir <- "K:\\MT4Output\\"

Quotes1MN <- InputQuotesMT4( FicIn = InputFile, RepIn = InputDir ) 
OutputToCSV(xFile = Quotes1MN, OD = OutputDir, OF = "EURUSD1")

Quotes5MNS <- to.minutes5(Quotes1MN)
OutputToCSV(xFile = Quotes5MNS, OD = OutputDir, OF = "EURUSD5")

Quotes15MNS <- to.minutes15(Quotes5MNS)
OutputToCSV(Quotes15MNS, OD = OutputDir, OF = "EURUSD15")

Quotes1H <- to.hourly(Quotes5MNS)
OutputToCSV(xFile = Quotes1H,OD = OutputDir, OF = "EURUSD60")

Quotes4H <- to.hourly(Quotes5MNS, k=4, indexAt='startof')
OutputToCSV(xFile = Quotes4H,OD = OutputDir, OF = "EURUSD240")

QuotesDay <- to.daily (Quotes1H)
OutputToCSV(xFile = QuotesDay,OD = OutputDir, OF = "EURUSD1440")

QuotesWeek <- to.weekly(QuotesDay)
OutputToCSV(xFile = QuotesWeek,OD = OutputDir, OF = "EURUSD10080")



-- 
View this message in context: http://www.nabble.com/How-to-change-the-way-R-format-his-output---tp18770099p18815951.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From pierre8r-nabble at yahoo.fr  Mon Aug  4 20:04:41 2008
From: pierre8r-nabble at yahoo.fr (Pierre8rou)
Date: Mon, 4 Aug 2008 11:04:41 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] xts. Change the way time series is
 build ( minutes ).
Message-ID: <18816524.post@talk.nabble.com>


Hi,

Follow a sample showing how is build a hourly time frame from xts and from a
trading software 
There is a difference with the minutes.
Is it possible to change the way time frame is build with xts ?

Hourly time frame build with a trading software 
--------------------------------------------
2008.07.01,00:00,1.5754,1.5758,1.5747,1.5748,132
2008.07.01,01:00,1.5747,1.5761,1.5747,1.5760,242
2008.07.01,02:00,1.5761,1.5766,1.5745,1.5746,309
2008.07.01,03:00,1.5745,1.5751,1.5727,1.5739,333

Hourly time frame build with xts
------------------------------
2008.07.01,00:59,1.5754,1.5758,1.5747,1.5748,132
2008.07.01,01:59,1.5747,1.5761,1.5747,1.5760,242
2008.07.01,02:59,1.5761,1.5766,1.5745,1.5746,309
2008.07.01,03:59,1.5745,1.5751,1.5727,1.5739,333


Pierre8r
-- 
View this message in context: http://www.nabble.com/xts.-Change-the-way-time-series-is-build-%28-minutes-%29.-tp18816524p18816524.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Mon Aug  4 20:27:52 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 4 Aug 2008 13:27:52 -0500
Subject: [R-SIG-Finance] [R-sig-finance] xts. Change the way time series
	is build ( minutes ).
In-Reply-To: <18816524.post@talk.nabble.com>
References: <18816524.post@talk.nabble.com>
Message-ID: <e8e755250808041127o500d27beh67beb2226a5d106d@mail.gmail.com>

Using minute data from Interactive Brokers interface in R (IBrokers)

> library(IBrokers)
Loading required package: xts
Loading required package: zoo

Attaching package: 'zoo'


        The following object(s) are masked from package:base :

         as.Date.numeric

IBrokers version 0.1-2: (pre-alpha)

This software comes with NO WARRANTY.  Not intended for production use!
See ?IBrokers for details
> tws <- twsConnect()
> QQQQ <- reqHistoricalData(tws, twsEquity("QQQQ"), barSize='1 min', duration='1 D')
waiting for TWS reply ...... done.
> head(QQQQ)
                     Open  High   Low Close Volume   WAP hasGaps Count
2008-08-04 08:30:00 44.88 44.92 44.82 44.82   9491 44.88       0   884
2008-08-04 08:31:00 44.82 44.83 44.52 44.57  31075 44.67       0  3102
2008-08-04 08:32:00 44.58 44.66 44.50 44.63  16498 44.57       0  1853
2008-08-04 08:33:00 44.63 44.68 44.62 44.66   3604 44.65       0   625
2008-08-04 08:34:00 44.67 44.72 44.67 44.69   3803 44.70       0   547
2008-08-04 08:35:00 44.69 44.75 44.68 44.75   2539 44.70       0   306

> to.hourly(QQQQ[,1:5],name='QQQQ')
                    QQQQ.Open QQQQ.High QQQQ.Low QQQQ.Close QQQQ.Volume
2008-08-04 08:59:00     44.88     44.92    44.50      44.68      148632
2008-08-04 09:59:00     44.67     44.71    44.38      44.49      176134
2008-08-04 10:59:00     44.48     44.84    44.41      44.77      185493
2008-08-04 11:59:00     44.77     44.82    44.59      44.60       61961
2008-08-04 12:59:00     44.60     44.69    44.51      44.62       62291
2008-08-04 13:12:00     44.61     44.64    44.57      44.62       11080

> to.hourly(QQQQ[,1:5],name='QQQQ',indexAt='startof')
                    QQQQ.Open QQQQ.High QQQQ.Low QQQQ.Close QQQQ.Volume
2008-08-04 08:30:00     44.88     44.92    44.50      44.68      148632
2008-08-04 09:00:00     44.67     44.71    44.38      44.49      176134
2008-08-04 10:00:00     44.48     44.84    44.41      44.77      185493
2008-08-04 11:00:00     44.77     44.82    44.59      44.60       61961
2008-08-04 12:00:00     44.60     44.69    44.51      44.62       62291
2008-08-04 13:00:00     44.61     44.64    44.57      44.62       11080

Like previous posts.. the indexAt arg is the key.  For hourly you are
going to have to use the R-forge version, or the source (all open!)
from r-forge as well.  You can also modify the code to your liking, if
you'd be so inclined.  Though the latter would be unnecessary in this
case as it does what you want.

If you have the windows tools (I am assuming that is your platform) to
build R packages, just grab the source from r-forge.  If not, you'll
have to wait until I clean it so that it passes R CMD check and makes
it to CRAN.

Here are the links:

http://r-forge.r-project.org/bin/windows/contrib/latest/xts_0.0-16.zip
http://r-forge.r-project.org/src/contrib/xts_0.0-16.tar.gz

Or from R:

install.packages('xts', type='source', repos='http://r-forge.r-project.org')


Jeff


On Mon, Aug 4, 2008 at 1:04 PM, Pierre8rou <pierre8r-nabble at yahoo.fr> wrote:
>
> Hi,
>
> Follow a sample showing how is build a hourly time frame from xts and from a
> trading software
> There is a difference with the minutes.
> Is it possible to change the way time frame is build with xts ?
>
> Hourly time frame build with a trading software
> --------------------------------------------
> 2008.07.01,00:00,1.5754,1.5758,1.5747,1.5748,132
> 2008.07.01,01:00,1.5747,1.5761,1.5747,1.5760,242
> 2008.07.01,02:00,1.5761,1.5766,1.5745,1.5746,309
> 2008.07.01,03:00,1.5745,1.5751,1.5727,1.5739,333
>
> Hourly time frame build with xts
> ------------------------------
> 2008.07.01,00:59,1.5754,1.5758,1.5747,1.5748,132
> 2008.07.01,01:59,1.5747,1.5761,1.5747,1.5760,242
> 2008.07.01,02:59,1.5761,1.5766,1.5745,1.5746,309
> 2008.07.01,03:59,1.5745,1.5751,1.5727,1.5739,333
>
>
> Pierre8r
> --
> View this message in context: http://www.nabble.com/xts.-Change-the-way-time-series-is-build-%28-minutes-%29.-tp18816524p18816524.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From Achim.Zeileis at wu-wien.ac.at  Mon Aug  4 21:41:29 2008
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 4 Aug 2008 21:41:29 +0200 (CEST)
Subject: [R-SIG-Finance] JSS: Econometrics in R
Message-ID: <Pine.LNX.4.64.0808042128380.495@paninaro.stat-math.wu-wien.ac.at>

Dear useRs:

the Journal of Statistical Software (JSS, http://www.jstatsoft.org/) has 
just published a special volume on "Econometrics in R", edited by Roger 
Koenker and myself. All papers and accompanying software (including 
scripts that replicate the analysis from the paper) are freely available 
(under CC licenses) from

   http://www.jstatsoft.org/v27/

There are seven papers plus a short intro:

    - Achim Zeileis, Roger Koenker
      Econometrics in R: Past, Present, and Future
    - Yves Croissant, Giovanni Millo
      Panel Data Econometrics in R: The plm Package
    - Rob J. Hyndman, Yeasmin Khandakar
      Automatic Time Series Forecasting: The forecast Package for R
    - Bernhard Pfaff
      VAR, SVAR and SVEC Models: Implementation Within R Package vars
    - Tristen Hayfield, Jeffrey S. Racine
      Nonparametric Econometrics: The np Package
    - Roger Koenker
      Censored Quantile Regression Redux
    - Arne Henningsen, Ott Toomet
      Sample Selection Models in R: Package sampleSelection
    - Achim Zeileis, Christian Kleiber, Simon Jackman
      Regression Models for Count Data in R

I hope that many of the readers on this list will find (some of) the 
papers in this special volume interesting and helpful for using the 
associated packages for their work and research.

Best wishes,
Z


From kriskumar at earthlink.net  Wed Aug  6 04:23:23 2008
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Tue, 05 Aug 2008 22:23:23 -0400
Subject: [R-SIG-Finance] TARN
In-Reply-To: <2f7adeb90808040627s5ca9962dhffcea5cd9f19c476@mail.gmail.com>
References: <2f7adeb90808040627s5ca9962dhffcea5cd9f19c476@mail.gmail.com>
Message-ID: <48990B1B.7020207@earthlink.net>

Hongchuan Xia wrote:
> Hi,
>
> Does anyone has codeing experience with respect to Target redemption note?
>
> TIA
>
> Hong
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>   
sorry but this is way off-topic, Most tarns can be priced with MC and it 
is fairly straightforward to generate a path in R with flat vol, 
deterministic rates.
Have you had a go at this?.


From Zeno.Adams at ebs.edu  Wed Aug  6 12:07:57 2008
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Wed, 6 Aug 2008 12:07:57 +0200
Subject: [R-SIG-Finance] Principal Component Analysis
Message-ID: <9064522880125945B98983BBAECBA1CC21D1F4@exchsrv001.ebs.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080806/3bba0a3b/attachment.pl>

From pgrahl at gmail.com  Wed Aug  6 15:10:21 2008
From: pgrahl at gmail.com (Paulo Grahl)
Date: Wed, 6 Aug 2008 10:10:21 -0300
Subject: [R-SIG-Finance] Principal Component Analysis
In-Reply-To: <9064522880125945B98983BBAECBA1CC21D1F4@exchsrv001.ebs.local>
References: <9064522880125945B98983BBAECBA1CC21D1F4@exchsrv001.ebs.local>
Message-ID: <755f575b0808060610k683ab432oa79d29d484caff2e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080806/65485f3d/attachment.pl>

From tolga.i.uzuner at jpmorgan.com  Fri Aug  8 14:18:20 2008
From: tolga.i.uzuner at jpmorgan.com (tolga.i.uzuner at jpmorgan.com)
Date: Fri, 8 Aug 2008 13:18:20 +0100
Subject: [R-SIG-Finance] reliable Hurst exponent estimation
Message-ID: <OF72329931.1460B737-ON8025749F.00434503-8025749F.004398B9@jpmchase.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080808/a256b937/attachment.pl>

From maechler at stat.math.ethz.ch  Fri Aug  8 19:02:11 2008
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 8 Aug 2008 19:02:11 +0200
Subject: [R-SIG-Finance] reliable Hurst exponent estimation
In-Reply-To: <OF72329931.1460B737-ON8025749F.00434503-8025749F.004398B9@jpmchase.com>
References: <OF72329931.1460B737-ON8025749F.00434503-8025749F.004398B9@jpmchase.com>
Message-ID: <18588.31763.411357.658216@stat.math.ethz.ch>

>>>>> "tiu" == tolga i uzuner <tolga.i.uzuner at jpmorgan.com>
>>>>>     on Fri, 8 Aug 2008 13:18:20 +0100 writes:

    tiu> Dear R Finance Users, I am trying to reliably estimate
    tiu> the Hurst exponent using the ***fit functions in
    tiu> fArma. The estimates appear to be substantially
    tiu> different depending on the estimator used and in some
    tiu> cases, are outside the bounds of 0 and 1.

    tiu> I was wondering if anyone has considered in detail
    tiu> whether a "meta" estimator in which one takes the
    tiu> median amongst all estimators after excluding any which
    tiu> do not fall into the [0,1] range has any known
    tiu> statistical properties ? Any other recommendations also
    tiu> welcome.

    tiu> Thanks in advance, Tolga

And where is the small reproducible example code?

What does fracdiff give?


From tolga.i.uzuner at jpmorgan.com  Fri Aug  8 19:23:55 2008
From: tolga.i.uzuner at jpmorgan.com (tolga.i.uzuner at jpmorgan.com)
Date: Fri, 8 Aug 2008 18:23:55 +0100
Subject: [R-SIG-Finance] reliable Hurst exponent estimation
In-Reply-To: <18588.31763.411357.658216@stat.math.ethz.ch>
Message-ID: <OFAABAE529.87BCBF90-ON8025749F.005F4E29-8025749F.005F9309@jpmchase.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080808/5acafa1e/attachment.pl>

From niheaven at hotmail.com  Sun Aug 10 10:17:39 2008
From: niheaven at hotmail.com (Hsiao-nan Cheung)
Date: Sun, 10 Aug 2008 16:17:39 +0800
Subject: [R-SIG-Finance] Models Choosing
Message-ID: <BAY126-DS36E1E77C674C8FE28ED07D9760@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080810/e0e39ec6/attachment.pl>

From ezivot at u.washington.edu  Sun Aug 10 18:42:28 2008
From: ezivot at u.washington.edu (Eric Zivot)
Date: Sun, 10 Aug 2008 09:42:28 -0700
Subject: [R-SIG-Finance] Models Choosing
In-Reply-To: <BAY126-DS36E1E77C674C8FE28ED07D9760@phx.gbl>
Message-ID: <200808101642.m7AGgTYg012274@smtp.washington.edu>

I suggest you read the book Nonlinear models in Empirical Finance by Franses
and van Dijk. All of the answers to your questions are there. 
ez

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Hsiao-nan
Cheung
Sent: Sunday, August 10, 2008 1:18 AM
To: R-SIG-FINANCE
Subject: [R-SIG-Finance] Models Choosing

Hi,

 

I have a time series data, which has obviously structure switching property.
Now I want to use a model to capture its characteristics and find all
influencing factors, then which model should I choose?

 

Also, what!/s the difference when a stochastic model, a GARCH model and a
Markov-Switching model is used to describe the time series? And what!/s the
most important when I choose from these models? Is there a transformation
mechanism between these model?

 

Thanks!

 

Hsiao-nan Cheung

 

2008/08/10

 


	[[alternative HTML version deleted]]


From mvstatistics at yahoo.com.br  Tue Aug 12 13:42:57 2008
From: mvstatistics at yahoo.com.br (=?iso-8859-1?Q?Marcus_Vin=EDcius_Soares?=)
Date: Tue, 12 Aug 2008 04:42:57 -0700 (PDT)
Subject: [R-SIG-Finance] cointegrated series with Data Missing
Message-ID: <319030.86778.qm@web65716.mail.ac4.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080812/2dc5809c/attachment.pl>

From brian at braverock.com  Tue Aug 12 14:07:38 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 12 Aug 2008 07:07:38 -0500
Subject: [R-SIG-Finance] cointegrated series with Data Missing
In-Reply-To: <319030.86778.qm@web65716.mail.ac4.yahoo.com>
References: <319030.86778.qm@web65716.mail.ac4.yahoo.com>
Message-ID: <48A17D0A.5040603@braverock.com>

Marcus Vin?cius Soares wrote:
> Hi there!
>  
> First, I?m from Brazil, so sorry for any English mistake.
> I have two cointegrated series with Data Missing in both. They are two finance series, one from USA and another from Brazil, so in holidays here I have data for USA serie and same for holidays in USA.
> I think usual Data Missing theories do not apply, since they are time series, and are also cointegrated.
> Can someone help me?

There have been many discussions on this list regarding missing data in 
financial timeseries, so I suggest that you search the list archives.

I would also suggest that you look at the methods for dealing with 
missing data in the 'xts' package (and by extension, from 'zoo'), and 
see if a an locf or interpolated solution would work.

Regards,

    - Brian


From frainj at tcd.ie  Tue Aug 12 16:01:43 2008
From: frainj at tcd.ie (John Frain)
Date: Tue, 12 Aug 2008 15:01:43 +0100
Subject: [R-SIG-Finance] cointegrated series with Data Missing
In-Reply-To: <cfdde1650808120700p7d76700cp87ff165313685010@mail.gmail.com>
References: <319030.86778.qm@web65716.mail.ac4.yahoo.com>
	<cfdde1650808120700p7d76700cp87ff165313685010@mail.gmail.com>
Message-ID: <cfdde1650808120701w17a1e20ah803854bf4b6ed49c@mail.gmail.com>

---------- Forwarded message ----------
From: John Frain <frainj at tcd.ie>
Date: 2008/8/12
Subject: Re: [R-SIG-Finance] cointegrated series with Data Missing
To: mvstatistics at yahoo.com.br


It is not clear what you aim to do with the series.  Are you

1) Trying to test for cointegration

2) Estimating a cointegrating relationship

3) Estimating missing values.  If so how are you going to use the
estimates.  Perhaps a) estimate a VECM, b) fill in missing values, c)
reestimate and d) cycle to convergence.

Best Regards

John





2008/8/12 Marcus Vin?cius Soares <mvstatistics at yahoo.com.br>:
> Hi there!
>
> First, I?m from Brazil, so sorry for any English mistake.
> I have two cointegrated series with Data Missing in both. They are two finance series, one from USA and another from Brazil, so in holidays here I have data for USA serie and same for holidays in USA.
> I think usual Data Missing thecnics do not apply, since they are time series, and are also cointegrated.
> Can someone help me?
>
> Thanks very much,
>
> Marcus Vinicius
>
>
>
>      Novos endere?os, o Yahoo! que voc? conhece. Crie um email novo com a sua cara @ymail.com ou @rocketmail.com.
> http://br.new.mail.yahoo.com/addresses
>        [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



--
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com



-- 
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

From niheaven at hotmail.com  Wed Aug 13 23:23:34 2008
From: niheaven at hotmail.com (Hsiao-nan Cheung)
Date: Thu, 14 Aug 2008 05:23:34 +0800
Subject: [R-SIG-Finance] Granger Causality Test
Message-ID: <BAY126-DS7E20A6DA1FACCBF1D02E1D9730@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080814/bc883cd4/attachment.pl>

From patrick.t.brandt at gmail.com  Wed Aug 13 23:53:52 2008
From: patrick.t.brandt at gmail.com (Patrick Brandt)
Date: Wed, 13 Aug 2008 16:53:52 -0500
Subject: [R-SIG-Finance] Granger Causality Test
In-Reply-To: <BAY126-DS7E20A6DA1FACCBF1D02E1D9730@phx.gbl>
References: <BAY126-DS7E20A6DA1FACCBF1D02E1D9730@phx.gbl>
Message-ID: <232e3f240808131453k15d26036vcebbd4b5646c2e9f@mail.gmail.com>

The following cites cover much of the basics of Granger causality for
I(d) variables:


@ARTICLE{Dolado&Lutkepohl96,
  author = {Dolado, Juan J. and Helmut Lutkepohl},
  year = 1996,
  title = {Making Wald Tests Work for Cointegrated {VAR} Systems},
  journal = {Econometric Reviews},
  volume = 15,
  number = 4,
  pages = {369-386}
}

@Article{Zapata&Rambaldi1997,
  author = 	 {Hector O. Zapata and Alicia N. Rambaldi},
  title = 	 {Monte Carlo Evidence on Cointegration and Causation},
  journal = 	 {Oxford Bulletin of Economics and Statistics},
  year = 	 1997,
  volume =	 59,
  number =	 2,
  pages =	 {285-298},
  month =	 {May},
}

@ARTICLE{Engle&Granger87,
  author = {Engle, Robert F. and C. W. J. Granger},
  year = 1987,
  title = {Co-Integration and Error Correction: Representation, Estimation and
          Testing},
  journal = {Econometrica},
  volume = 55,
  pages = {251-76}
}

On Wed, Aug 13, 2008 at 4:23 PM, Hsiao-nan Cheung <niheaven at hotmail.com> wrote:
> Hi,
>
>
>
> I have some question that whether only stationary series could do Granger
> Causality Test. Is there any exception?
>
>
>
> Since I??ve found somewhere that to make a time series stationary, the
> differential series may has little economic meaning.
>
>
>
> Hsiao-nan Cheung
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Patrick Brandt
Assistant Professor
Political Science
School of Economic, Political and Policy Sciences
University of Texas at Dallas
Personal site: http://www.utdallas.edu/~pbrandt
MSBVAR site: http://yule.utdallas.edu


From ezivot at u.washington.edu  Thu Aug 14 00:00:42 2008
From: ezivot at u.washington.edu (Eric Zivot)
Date: Wed, 13 Aug 2008 15:00:42 -0700
Subject: [R-SIG-Finance] Granger Causality Test
In-Reply-To: <BAY126-DS7E20A6DA1FACCBF1D02E1D9730@phx.gbl>
Message-ID: <200808132200.m7DM0gPr002684@smtp.washington.edu>

Granger causality only makes sense between stationary variables or between
nonstationary variables that are cointegrated. See the excellent textbook A
New Introduction to Multiple Time Series  by Helmut Lutkepohl for everything
you ever wanted to know about causality testing
 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Hsiao-nan
Cheung
Sent: Wednesday, August 13, 2008 2:24 PM
To: R-SIG-FINANCE
Subject: [R-SIG-Finance] Granger Causality Test

Hi,

 

I have some question that whether only stationary series could do Granger
Causality Test. Is there any exception?

 

Since I!/ve found somewhere that to make a time series stationary, the
differential series may has little economic meaning.

 

Hsiao-nan Cheung

 

 


	[[alternative HTML version deleted]]


From markleeds at verizon.net  Thu Aug 14 00:01:05 2008
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 13 Aug 2008 17:01:05 -0500 (CDT)
Subject: [R-SIG-Finance] Granger Causality Test
Message-ID: <11625533.15301711218664866064.JavaMail.javamailuser@localhost>

  i think your two statements deserve separate answers.

1)

granger causailty is only used ( as far as i know ) in the context of 
deciding on which variables in a previusly estimated VAR cause another
in the granger sense. ( defined in Lutkepohl ) . But, since it's a n 
already estimated VAR,  you should have already decided whether the
series under study needed to be differenced or not in order that the VAR 
estimation is done on variables that are stationary.

So, granger causality doesn't really have much to do with stationarity, 
I don't think,  because that should have been worked out during the
estimation fo the VAR.

2)

in the 70's before cointegration was discovered, people used to 
difference time series variables in order to make them stationary (
for example , if they were prices, they would make them differenced 
prices ). but, it was realized that, if one did this, then the 
information about levels of the
series was lost and it was always felt that there must be a better 
approach so the levels info was not abruptly discarded.

then, engle and granger figured out in the early 80's that there were 
cases where two variables could be non-stationary and yet
the regression of the two on each other could still be valid 
statistically ( cointegration ). So, they were able to figure out a way 
to
keep the levels in the specification by rewriting the regression 
relationship in the form of an error correction model. This was then 
extended beyond the
bivariate case to what is called a vector-error correction model ( VECM 
) which is like a VAR but slightly different and I couldn't do the 
description  of a VECM justice even if  i tried so I won't.

There is a lot of econometrics literautre in this area that talks about 
this in a much more eloquent way than I could.

Some good books in order of increasing difficulty ( atleast to me ).

Enders
Zivot ( S+Finmetrics book )
Hayashi
Lutkepohl
Hamilton

To me, Hamilton and Lutkpohl are similar in difficulty and require a 
larger time investment than the others.  Enders is the most basic but it 
gives nice intuition and is good for an intro. Zivot is more general in 
that it covers various  time series topics but also provides a brief but 
nice discussion on the topics above.





On Wed, Aug 13, 2008 at  5:23 PM, Hsiao-nan Cheung wrote:

> Hi,
>
>
> I have some question that whether only stationary series could do 
> Granger
> Causality Test. Is there any exception?
>
>
> Since I??ve found somewhere that to make a time series stationary, the
> differential series may has little economic meaning.
>
>
> Hsiao-nan Cheung
>
>
>
>
> 	[[alternative HTML version deleted]]
>
>
>
>      ------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From jh8080 at hotmail.com  Thu Aug 14 00:09:24 2008
From: jh8080 at hotmail.com (Jae Kim)
Date: Wed, 13 Aug 2008 22:09:24 +0000
Subject: [R-SIG-Finance] Granger Causality Test
In-Reply-To: <11625533.15301711218664866064.JavaMail.javamailuser@localhost>
References: <11625533.15301711218664866064.JavaMail.javamailuser@localhost>
Message-ID: <BAY108-W190CA5362D2A5DBAEF3348CD730@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080813/d53b578f/attachment.pl>

From frainj at tcd.ie  Thu Aug 14 01:12:12 2008
From: frainj at tcd.ie (John Frain)
Date: Thu, 14 Aug 2008 00:12:12 +0100
Subject: [R-SIG-Finance] Granger Causality Test
In-Reply-To: <BAY126-DS7E20A6DA1FACCBF1D02E1D9730@phx.gbl>
References: <BAY126-DS7E20A6DA1FACCBF1D02E1D9730@phx.gbl>
Message-ID: <cfdde1650808131612jca4ce76td4d6b8f566638f9e@mail.gmail.com>

You can do Granger Causality tests in VECM systems. See, for example
Lutkepohl (2005), New introduction to multiple time series analysis,
Springer. Section 7.6.

Best Regards

John Frain

2008/8/13 Hsiao-nan Cheung <niheaven at hotmail.com>:
> Hi,
>
>
>
> I have some question that whether only stationary series could do Granger
> Causality Test. Is there any exception?
>
>
>
> Since I??ve found somewhere that to make a time series stationary, the
> differential series may has little economic meaning.
>
>
>
> Hsiao-nan Cheung
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

From mvstatistics at yahoo.com.br  Thu Aug 14 16:08:45 2008
From: mvstatistics at yahoo.com.br (=?iso-8859-1?Q?Marcus_Vin=EDcius_Soares?=)
Date: Thu, 14 Aug 2008 07:08:45 -0700 (PDT)
Subject: [R-SIG-Finance] cointegrated series with Data Missing
In-Reply-To: <cfdde1650808120700p7d76700cp87ff165313685010@mail.gmail.com>
Message-ID: <180015.24350.qm@web65707.mail.ac4.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080814/7d484147/attachment.pl>

From brian at braverock.com  Thu Aug 14 18:09:25 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 14 Aug 2008 11:09:25 -0500
Subject: [R-SIG-Finance] cointegrated series with Data Missing
In-Reply-To: <180015.24350.qm@web65707.mail.ac4.yahoo.com>
References: <180015.24350.qm@web65707.mail.ac4.yahoo.com>
Message-ID: <48A458B5.7010409@braverock.com>

I don't believe that the missing data methodologies for cointegrated 
time series are really any different than for other (irregular) time 
series that you want to compare to each other.

As I said in an earlier post, xts and zoo contain multiple methods for 
merging the date indexes from multiple series and then additional 
methods to apply to NA handling, like last observation carried forward 
and interpolated.

Regards,

   - Brian


Marcus Vin?cius Soares wrote:
> Dear John,
>  
> I?m trying to test for cointegration. I thought that if I use an MA as estimation with last data before each missing, perhaps one or two weeks, it could do the trick. But I can find some week sazonality, since market in a Friday may be more or less active than in a Monday. The fact is that I can't test cointegration with missing, and I wouldn't just delete the unpaired data.
> Sugestions?
>  
> I searched in the list of historical mails of this list and didn't find anything that fits on this case.
> 
> Rgds,
> Marcus
> 
> --- Em ter, 12/8/08, John Frain <frainj at tcd.ie> escreveu:
> 
> De: John Frain <frainj at tcd.ie>
> Assunto: Re: [R-SIG-Finance] cointegrated series with Data Missing
> Para: mvstatistics at yahoo.com.br
> Data: Ter?a-feira, 12 de Agosto de 2008, 11:00
> 
> It is not clear what you aim to do with the series.  Are you
> 
> 1) Trying to test for cointegration
> 
> 2) Estimating a cointegrating relationship
> 
> 3) Estimating missing values.  If so how are you going to use the
> estimates.  Perhaps a) estimate a VECM, b) fill in missing values, c)
> reestimate and d) cycle to convergence.
> 
> Best Regards
> 
> John
> 
> 
> 
> 
> 
> 2008/8/12 Marcus Vin?cius Soares <mvstatistics at yahoo.com.br>:
>> Hi there!
>>
>> First, I?m from Brazil, so sorry for any English mistake.
>> I have two cointegrated series with Data Missing in both. They are two
> finance series, one from USA and another from Brazil, so in holidays here I have
> data for USA serie and same for holidays in USA.
>> I think usual Data Missing thecnics do not apply, since they are time
> series, and are also cointegrated.
>> Can someone help me?
>>
>> Thanks very much,
>>
>> Marcus Vinicius
>>
>>
>>
>>      Novos endere?os, o Yahoo! que voc? conhece. Crie um email novo com
> a sua cara @ymail.com ou @rocketmail.com.
>> http://br.new.mail.yahoo.com/addresses
>>        [[alternative HTML version deleted]]
>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> 
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From ezivot at u.washington.edu  Thu Aug 14 18:49:01 2008
From: ezivot at u.washington.edu (Eric Zivot)
Date: Thu, 14 Aug 2008 09:49:01 -0700
Subject: [R-SIG-Finance] cointegrated series with Data Missing
In-Reply-To: <48A458B5.7010409@braverock.com>
Message-ID: <200808141649.m7EGn1Qg031882@smtp.washington.edu>

With time series data, filling in missing data can be tricky. State space
models are particulary well suited for this problem because you can specify
explicitly the law of motion and the Kalman filter will automatically
fill-in the missing values in an optimal way.
ez 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
Peterson
Sent: Thursday, August 14, 2008 9:09 AM
To: mvstatistics at yahoo.com.br; R-SIG-Finance
Subject: Re: [R-SIG-Finance] cointegrated series with Data Missing

I don't believe that the missing data methodologies for cointegrated time
series are really any different than for other (irregular) time series that
you want to compare to each other.

As I said in an earlier post, xts and zoo contain multiple methods for
merging the date indexes from multiple series and then additional methods to
apply to NA handling, like last observation carried forward and
interpolated.

Regards,

   - Brian


Marcus Vin?cius Soares wrote:
> Dear John,
>  
> I?m trying to test for cointegration. I thought that if I use an MA as
estimation with last data before each missing, perhaps one or two weeks, it
could do the trick. But I can find some week sazonality, since market in a
Friday may be more or less active than in a Monday. The fact is that I can't
test cointegration with missing, and I wouldn't just delete the unpaired
data.
> Sugestions?
>  
> I searched in the list of historical mails of this list and didn't find
anything that fits on this case.
> 
> Rgds,
> Marcus
> 
> --- Em ter, 12/8/08, John Frain <frainj at tcd.ie> escreveu:
> 
> De: John Frain <frainj at tcd.ie>
> Assunto: Re: [R-SIG-Finance] cointegrated series with Data Missing
> Para: mvstatistics at yahoo.com.br
> Data: Ter?a-feira, 12 de Agosto de 2008, 11:00
> 
> It is not clear what you aim to do with the series.  Are you
> 
> 1) Trying to test for cointegration
> 
> 2) Estimating a cointegrating relationship
> 
> 3) Estimating missing values.  If so how are you going to use the 
> estimates.  Perhaps a) estimate a VECM, b) fill in missing values, c) 
> reestimate and d) cycle to convergence.
> 
> Best Regards
> 
> John
> 
> 
> 
> 
> 
> 2008/8/12 Marcus Vin?cius Soares <mvstatistics at yahoo.com.br>:
>> Hi there!
>>
>> First, I?m from Brazil, so sorry for any English mistake.
>> I have two cointegrated series with Data Missing in both. They are 
>> two
> finance series, one from USA and another from Brazil, so in holidays 
> here I have data for USA serie and same for holidays in USA.
>> I think usual Data Missing thecnics do not apply, since they are time
> series, and are also cointegrated.
>> Can someone help me?
>>
>> Thanks very much,
>>
>> Marcus Vinicius
>>
>>
>>
>>      Novos endere?os, o Yahoo! que voc? conhece. Crie um email novo 
>> com
> a sua cara @ymail.com ou @rocketmail.com.
>> http://br.new.mail.yahoo.com/addresses
>>        [[alternative HTML version deleted]]
>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> 
> 
> 
> ----------------------------------------------------------------------
> --
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From bastian2507hk at yahoo.co.uk  Sun Aug 17 14:19:00 2008
From: bastian2507hk at yahoo.co.uk (Bastian Offermann)
Date: Sun, 17 Aug 2008 14:19:00 +0200
Subject: [R-SIG-Finance] How to get data from different time zones (e.g. USA,
	EU, Asia) in the same format
Message-ID: <519958.11395.bm@omp204.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080817/e8dc6f38/attachment.pl>

From ggrothendieck at gmail.com  Sun Aug 17 14:53:11 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 17 Aug 2008 08:53:11 -0400
Subject: [R-SIG-Finance] How to get data from different time zones (e.g.
	USA, EU, Asia) in the same format
In-Reply-To: <519958.11395.bm@omp204.mail.ukl.yahoo.com>
References: <519958.11395.bm@omp204.mail.ukl.yahoo.com>
Message-ID: <971536df0808170553i3fe0671fv234b83e18eccd4bb@mail.gmail.com>

merge.zoo does that:

merge(z1, z2)

See ?merge.zoo

Note that unlike zoo in the core of R which can only merge two objects
at a time merge.zoo
can merge multiple objects so if L is a list of such zoo objects you can write:

do.call("merge.zoo", L)



On Sun, Aug 17, 2008 at 8:19 AM, Bastian Offermann
<bastian2507hk at yahoo.co.uk> wrote:
> Hello,
>
>
>
> I am currently analyzing market data retrieving data sets from yahoo with
> the get.hist.quote function in the Performance Analytics package.
>
>
>
> ############ Data Import Sheet ##############
>
>
>
> ticker<-c("^GSPC", "^GDAXI")
>
>
>
>
>
> # Time horizon
>
>
>
> from <- "1992-01-03"              # of type "YYYY-MM-DD"
>
> till <- "2007-12-31"
>
>
>
> # Return frequency
>
>
>
> freque <- c("d", "w", "m", "y")
>
> fq <- 1                             # input 1 for "d", 2 for "w" etc.
>
>
>
>
>
> # Benchmark
>
> benchmark <- get.hist.quote(ticker[1], start = from, end = till,
> compression=freque[fq], quote="Close")
>
> benchmark2 <- get.hist.quote(ticker[2], start = from, end = till,
> compression=freque[fq], quote="Close")
>
>
>
> Now, both benchmarks are zoo objects with an attached index. Unfortunately,
> due to holidays etc both series do neither have identical indices nor the
> same length (in most cases, sometimes they do by chance).
>
> I have been trying to address this problem for quite some time now, but
> cannot find a solution to it.
>
>
>
> The problems arise as I usually store each series in a matrix format where
> the rows contain the assets and the columns the respective quotes or returns
> for a certain trading interval.
>
>
>
>            Quote 1 .. Quote 2 .. Quote N
>
> Asset1
>
>
>
> Asset 2
>
>
>
> ..
>
> ..
>
> ..
>
>
>
> Asset N
>
>
>
> Etc.
>
>
>
> What I ultimately want to do is to match both series, i.e. same time indices
> and same length.
>
>
>
> Do you have an idea how to overcome this? Thanks in advance.
>
>
>
> regards
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Sun Aug 17 15:20:17 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 17 Aug 2008 09:20:17 -0400
Subject: [R-SIG-Finance] How to get data from different time zones (e.g.
	USA, EU, Asia) in the same format
In-Reply-To: <971536df0808170553i3fe0671fv234b83e18eccd4bb@mail.gmail.com>
References: <519958.11395.bm@omp204.mail.ukl.yahoo.com>
	<971536df0808170553i3fe0671fv234b83e18eccd4bb@mail.gmail.com>
Message-ID: <971536df0808170620l3a2e0ad7y34c227140818dba0@mail.gmail.com>

That should be do.call("merge", L) since merge.zoo is not exported.  I assume
your objects are all POSIXct with time zones so they are all
internally represented
with respect to GMT anyways.  See R News 4/1.

On Sun, Aug 17, 2008 at 8:53 AM, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> merge.zoo does that:
>
> merge(z1, z2)
>
> See ?merge.zoo
>
> Note that unlike zoo in the core of R which can only merge two objects
> at a time merge.zoo
> can merge multiple objects so if L is a list of such zoo objects you can write:
>
> do.call("merge.zoo", L)
>
>
>
> On Sun, Aug 17, 2008 at 8:19 AM, Bastian Offermann
> <bastian2507hk at yahoo.co.uk> wrote:
>> Hello,
>>
>>
>>
>> I am currently analyzing market data retrieving data sets from yahoo with
>> the get.hist.quote function in the Performance Analytics package.
>>
>>
>>
>> ############ Data Import Sheet ##############
>>
>>
>>
>> ticker<-c("^GSPC", "^GDAXI")
>>
>>
>>
>>
>>
>> # Time horizon
>>
>>
>>
>> from <- "1992-01-03"              # of type "YYYY-MM-DD"
>>
>> till <- "2007-12-31"
>>
>>
>>
>> # Return frequency
>>
>>
>>
>> freque <- c("d", "w", "m", "y")
>>
>> fq <- 1                             # input 1 for "d", 2 for "w" etc.
>>
>>
>>
>>
>>
>> # Benchmark
>>
>> benchmark <- get.hist.quote(ticker[1], start = from, end = till,
>> compression=freque[fq], quote="Close")
>>
>> benchmark2 <- get.hist.quote(ticker[2], start = from, end = till,
>> compression=freque[fq], quote="Close")
>>
>>
>>
>> Now, both benchmarks are zoo objects with an attached index. Unfortunately,
>> due to holidays etc both series do neither have identical indices nor the
>> same length (in most cases, sometimes they do by chance).
>>
>> I have been trying to address this problem for quite some time now, but
>> cannot find a solution to it.
>>
>>
>>
>> The problems arise as I usually store each series in a matrix format where
>> the rows contain the assets and the columns the respective quotes or returns
>> for a certain trading interval.
>>
>>
>>
>>            Quote 1 .. Quote 2 .. Quote N
>>
>> Asset1
>>
>>
>>
>> Asset 2
>>
>>
>>
>> ..
>>
>> ..
>>
>> ..
>>
>>
>>
>> Asset N
>>
>>
>>
>> Etc.
>>
>>
>>
>> What I ultimately want to do is to match both series, i.e. same time indices
>> and same length.
>>
>>
>>
>> Do you have an idea how to overcome this? Thanks in advance.
>>
>>
>>
>> regards
>>
>>
>>        [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From matthieu.stigler at gmail.com  Sun Aug 17 22:29:35 2008
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Sun, 17 Aug 2008 22:29:35 +0200
Subject: [R-SIG-Finance] How to contribute my threshold contegration
	functions in R?
Message-ID: <48A88A2F.6000701@gmail.com>

Dear members of the R-Finance list and other persons potentially concerned

This is actually a question not directly related to existing functions 
in R but rather a question on how to add new ones.

For my master thesis in economics, I had to write myself functions in R 
for threshold cointegration. I found a job after it and hoped to be able 
simultaneously to make my functions available for R , but I realized 
that it takes much more than I expected to make my raw code end-user 
suitable. I will soon finish this actual job and, before searching a 
real work, I'm thinking about spending some time to write and improve 
the functions in order to contribute them in package tsDyn (see 
http://code.google.com/p/tsdyn/wiki/ThresholdCointegration for the 
actual features).

This is why I ask you if you have an idea about how I could find a way 
to develop and improve the package?  Maybe some institute or university 
might be interested in fostering the development of a package for 
threshold cointegration and get practice in applying it, and therefore 
offer a 2-3 months job or stage place?
Cointegration analysis has become an indispensable tool/step for 
considering relationships between multivariate time series. Threshold 
cointegration extends the usual linear cointegration to cases where the 
adjustment towards long-run equilibrium does not occur after each small 
deviation but more realistically only when the deviations exceed some 
critical thresholds, thus taking transaction costs and asymmetries in 
price transmission into account .

Threshold cointegration can improve significantly the estimation and 
forecast accuracy and has been widely applied to study the relationship 
between interest rates, exchange rates, oil prices, international 
agricultural markets, and to test the purchasing power parity theory and 
the law of one price.

In spite of its wide interest in theoretical and empirical work, there 
isn't, to my knowledge, any comprehensive package or software for it, 
except some codes for Gauss and Matlab.

So I appreciate very much any idea or advice about a 2-3 month job or 
stage place to develop this package! Thank a lot!

Matthieu


From icos.atropa at gmail.com  Mon Aug 18 21:20:01 2008
From: icos.atropa at gmail.com (icosa atropa)
Date: Mon, 18 Aug 2008 13:20:01 -0600
Subject: [R-SIG-Finance] How to get data from different time zones
Message-ID: <681d07c20808181220p52678259sd05bb343ec12dd0e@mail.gmail.com>

"do.call("merge", L) "

Thanks so much for that info - very helpful!
-christian gunning
-- 
Far better an approximate answer to the right question, which is often
vague, than the exact answer to the wrong question, which can always
be made precise -- j.w. tukey


From CVorlow at eurobank.gr  Tue Aug 19 16:58:49 2008
From: CVorlow at eurobank.gr (Vorlow Constantinos)
Date: Tue, 19 Aug 2008 17:58:49 +0300
Subject: [R-SIG-Finance] intradaily data from bloomberg EXCEL file (ITS)
Message-ID: <BCEA70B53E1BE64290556580EA0708D6393B22@EH002EXC.eurobank.efg.gr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080819/6a54c1fb/attachment.pl>

From ggrothendieck at gmail.com  Tue Aug 19 17:39:23 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Aug 2008 11:39:23 -0400
Subject: [R-SIG-Finance] intradaily data from bloomberg EXCEL file (ITS)
In-Reply-To: <BCEA70B53E1BE64290556580EA0708D6393B22@EH002EXC.eurobank.efg.gr>
References: <BCEA70B53E1BE64290556580EA0708D6393B22@EH002EXC.eurobank.efg.gr>
Message-ID: <971536df0808190839k7a50d675q10312ec494efd3d2@mail.gmail.com>

This works for me:

Lines <- "1/2/2008 18:21,90.75
1/2/2008 18:22,89.29
1/2/2008 18:24,89.3
1/2/2008 18:27,89.33
1/2/2008 19:22,88.6
1/2/2008 19:52,88.26
1/2/2008 20:03,88.12
1/2/2008 21:28,88.45
1/2/2008 21:43,88.37
2/2/2008 0:14,88.37
4/2/2008 16:35,88.37
4/2/2008 17:09,88.33
4/2/2008 17:23,89.05
4/2/2008 17:52,88.91
4/2/2008 18:30,89.13
4/2/2008 18:49,89
4/2/2008 18:54,88.95
4/2/2008 21:27,89.3"

library(its)
x <- readcsvIts(textConnection(Lines), header = FALSE, informat
="%d/%m/%Y %H:%M")
y <- its(x)

# as does this

library(z)
z <- read.zoo(textConnection(Lines), header = FALSE,
	tz = "", format = "%d/%m/%Y %H:%M")
y2 <- as.its(z)


On Tue, Aug 19, 2008 at 10:58 AM, Vorlow Constantinos
<CVorlow at eurobank.gr> wrote:
> Hi,
>
> I have a .CSV file that goes like this:
>
> 1/2/2008 18:21,90.75
> 1/2/2008 18:22,89.29
> 1/2/2008 18:24,89.3
> 1/2/2008 18:27,89.33
> 1/2/2008 19:22,88.6
> 1/2/2008 19:52,88.26
> 1/2/2008 20:03,88.12
> 1/2/2008 21:28,88.45
> 1/2/2008 21:43,88.37
> 2/2/2008 0:14,88.37
> 4/2/2008 16:35,88.37
> 4/2/2008 17:09,88.33
> 4/2/2008 17:23,89.05
> 4/2/2008 17:52,88.91
> 4/2/2008 18:30,89.13
> 4/2/2008 18:49,89
> 4/2/2008 18:54,88.95
> 4/2/2008 21:27,89.3
>
> of high freq. intradaily data (prices by the minute of an asset
> downloaded from Bloomberg LP) and I am trying to use th following lines
> to read them into R:
>
>
>>    its.format("%d/%m/%Y %H:%M")
>
>
> It seems that R reads them ok (readcsvIts(filename="a.csv")) as it
> recognizes times and dates and aligns them to the corresponding prices :
>
> 19/08/2008 12:18 112.03
> 19/08/2008 12:19 112.05
> 19/08/2008 12:20 112.14
> 19/08/2008 12:21 112.23
> 19/08/2008 12:22 112.16
> 19/08/2008 12:23 112.15
> 19/08/2008 12:24 112.18
> 19/08/2008 12:25 112.18
> 19/08/2008 12:27 112.14
> 19/08/2008 12:28 112.09
> 19/08/2008 12:31 111.94
> 19/08/2008 12:32 111.95
> 19/08/2008 12:33 111.95
>
>
> However, when I try to pass them to a variable I get:
>
>
>> CL1<-its(readcsvIts(filename="a.csv"))
> Error in validObject(.Object) :
>  invalid class "its" object: Missing values in dates
>>
>
>
> I checked the data file and everything is fine. I think I have a problem
> with the format (date/time)...
>
> Can anybody help?
>
> Also:
>
> Can I read them into a ZOO object instad of an ITS???
>
> Many TKS in advance,
>
> Costas
>
>
>
>
> P Think before you print.
>
> Disclaimer:
> This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
> EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
> EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Tue Aug 19 17:53:27 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Aug 2008 11:53:27 -0400
Subject: [R-SIG-Finance] intradaily data from bloomberg EXCEL file (ITS)
In-Reply-To: <971536df0808190839k7a50d675q10312ec494efd3d2@mail.gmail.com>
References: <BCEA70B53E1BE64290556580EA0708D6393B22@EH002EXC.eurobank.efg.gr>
	<971536df0808190839k7a50d675q10312ec494efd3d2@mail.gmail.com>
Message-ID: <971536df0808190853v150bf8f9i3e0fdb1d2f0b1203@mail.gmail.com>

One correction.  See below.

On Tue, Aug 19, 2008 at 11:39 AM, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> This works for me:
>
> Lines <- "1/2/2008 18:21,90.75
> 1/2/2008 18:22,89.29
> 1/2/2008 18:24,89.3
> 1/2/2008 18:27,89.33
> 1/2/2008 19:22,88.6
> 1/2/2008 19:52,88.26
> 1/2/2008 20:03,88.12
> 1/2/2008 21:28,88.45
> 1/2/2008 21:43,88.37
> 2/2/2008 0:14,88.37
> 4/2/2008 16:35,88.37
> 4/2/2008 17:09,88.33
> 4/2/2008 17:23,89.05
> 4/2/2008 17:52,88.91
> 4/2/2008 18:30,89.13
> 4/2/2008 18:49,89
> 4/2/2008 18:54,88.95
> 4/2/2008 21:27,89.3"
>
> library(its)
> x <- readcsvIts(textConnection(Lines), header = FALSE, informat
> ="%d/%m/%Y %H:%M")
> y <- its(x)
>
> # as does this
>
> library(z)
> z <- read.zoo(textConnection(Lines), header = FALSE,
>        tz = "", format = "%d/%m/%Y %H:%M")

Last statement should be:

z <- read.zoo(textConnection(Lines), header = FALSE,
        tz = "", format = "%d/%m/%Y %H:%M", sep = ",")

> y2 <- as.its(z)
>
>
> On Tue, Aug 19, 2008 at 10:58 AM, Vorlow Constantinos
> <CVorlow at eurobank.gr> wrote:
>> Hi,
>>
>> I have a .CSV file that goes like this:
>>
>> 1/2/2008 18:21,90.75
>> 1/2/2008 18:22,89.29
>> 1/2/2008 18:24,89.3
>> 1/2/2008 18:27,89.33
>> 1/2/2008 19:22,88.6
>> 1/2/2008 19:52,88.26
>> 1/2/2008 20:03,88.12
>> 1/2/2008 21:28,88.45
>> 1/2/2008 21:43,88.37
>> 2/2/2008 0:14,88.37
>> 4/2/2008 16:35,88.37
>> 4/2/2008 17:09,88.33
>> 4/2/2008 17:23,89.05
>> 4/2/2008 17:52,88.91
>> 4/2/2008 18:30,89.13
>> 4/2/2008 18:49,89
>> 4/2/2008 18:54,88.95
>> 4/2/2008 21:27,89.3
>>
>> of high freq. intradaily data (prices by the minute of an asset
>> downloaded from Bloomberg LP) and I am trying to use th following lines
>> to read them into R:
>>
>>
>>>    its.format("%d/%m/%Y %H:%M")
>>
>>
>> It seems that R reads them ok (readcsvIts(filename="a.csv")) as it
>> recognizes times and dates and aligns them to the corresponding prices :
>>
>> 19/08/2008 12:18 112.03
>> 19/08/2008 12:19 112.05
>> 19/08/2008 12:20 112.14
>> 19/08/2008 12:21 112.23
>> 19/08/2008 12:22 112.16
>> 19/08/2008 12:23 112.15
>> 19/08/2008 12:24 112.18
>> 19/08/2008 12:25 112.18
>> 19/08/2008 12:27 112.14
>> 19/08/2008 12:28 112.09
>> 19/08/2008 12:31 111.94
>> 19/08/2008 12:32 111.95
>> 19/08/2008 12:33 111.95
>>
>>
>> However, when I try to pass them to a variable I get:
>>
>>
>>> CL1<-its(readcsvIts(filename="a.csv"))
>> Error in validObject(.Object) :
>>  invalid class "its" object: Missing values in dates
>>>
>>
>>
>> I checked the data file and everything is fine. I think I have a problem
>> with the format (date/time)...
>>
>> Can anybody help?
>>
>> Also:
>>
>> Can I read them into a ZOO object instad of an ITS???
>>
>> Many TKS in advance,
>>
>> Costas
>>
>>
>>
>>
>> P Think before you print.
>>
>> Disclaimer:
>> This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
>> EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
>> EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.
>>
>>
>>        [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From benverschuere at hotmail.com  Wed Aug 20 19:01:09 2008
From: benverschuere at hotmail.com (Verschuere Benjamin)
Date: Wed, 20 Aug 2008 19:01:09 +0200
Subject: [R-SIG-Finance] SSPIR  noob question
Message-ID: <BLU109-W2800159D3E7CB3C5178534B8680@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080820/51f9edfd/attachment.pl>

From benverschuere at hotmail.com  Wed Aug 20 19:40:18 2008
From: benverschuere at hotmail.com (Verschuere Benjamin)
Date: Wed, 20 Aug 2008 19:40:18 +0200
Subject: [R-SIG-Finance] SSPIR  noob question
In-Reply-To: <10693469.22283651219252854219.JavaMail.javamailuser@localhost>
References: <10693469.22283651219252854219.JavaMail.javamailuser@localhost>
Message-ID: <BLU109-W568782045BC6D1C0E8F528B8680@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080820/93a66ed7/attachment.pl>

From benverschuere at hotmail.com  Wed Aug 20 21:33:17 2008
From: benverschuere at hotmail.com (Verschuere Benjamin)
Date: Wed, 20 Aug 2008 21:33:17 +0200
Subject: [R-SIG-Finance] How to use the debugger/broser in a R package
	function
Message-ID: <BLU109-W34B79187954117C1961D00B8680@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080820/a146b660/attachment.pl>

From jeff.a.ryan at gmail.com  Wed Aug 20 21:40:52 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 20 Aug 2008 14:40:52 -0500
Subject: [R-SIG-Finance] How to use the debugger/broser in a R package
	function
In-Reply-To: <BLU109-W34B79187954117C1961D00B8680@phx.gbl>
References: <BLU109-W34B79187954117C1961D00B8680@phx.gbl>
Message-ID: <e8e755250808201240m72e7e909vdd97ae7a7863bd5c@mail.gmail.com>

This is probably (certainly) a question for R-help, but since
cross-posting isn't too nice, I'll answer.

You need to find the method that is being dispatched on.

methods(kfilter)

> methods(kfilter)
[1] kfilter.SS  kfilter.ssm

These can then be viewed or used in the debug() call:

debug(kfilter.SS)  or debug(kfilter.ssm)

undebug will stop the debugging.

see ?debug

HTH
Jeff

Those with an asterisk will be unexported, so you'll need to prefix with

On Wed, Aug 20, 2008 at 2:33 PM, Verschuere Benjamin
<benverschuere at hotmail.com> wrote:
>
> Hi,
>
> I have small question.  Basically I am trying to see what is going inside function from a R package (in my case the "kfilter" function in sspir package).
> The problem is that when I try to see what is going inside "kfilter" I have that:
>> kfilter
> function (ss)
> {
>    UseMethod("kfilter")
> }
> <environment: namespace:sspir>
>
> I have been told that the debugger might help, but I dont know how to use it. Or maybe there is a better way?
>
> Thanks in advance for the help.
>
> Benjamin
>
> _________________________________________________________________
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From giuseppe1.milicia at hsbcib.com  Thu Aug 21 15:06:09 2008
From: giuseppe1.milicia at hsbcib.com (giuseppe1.milicia at hsbcib.com)
Date: Thu, 21 Aug 2008 14:06:09 +0100
Subject: [R-SIG-Finance] fPortfolio and leverage
Message-ID: <OF5503C45E.6674F9A7-ON802574AC.0043C017-802574AC.00480166@hsbcib.com>


Guys,

I'm playing a bit with fPortfolio and looking at the examples and unit
tests, it seems that the weights returned always sum up to 1.

I was wondering whether there is a way to have a leveraged portfolio with
weights summing up to W > 1. Say I target a certain risk level R and I want
the weights to be totally unconstrained. From the docs I see that
Constraints = "Short" should given me unconstrained weights:

"Short": This selection defines the case of unlimited short selling. i.e.
each weight may range
between -Inf and Inf. Consequently, there are no group constraints. Risk
budget constraints are
not included in the portfolio optimization.

But, say if I try this:

# Load Data and Convert to timeSeries Object:
Data = as.timeSeries(data(smallcap.ts))
Data = Data[, c("BKE", "GG", "GYMB", "KRON")]
Data

# Set Default Specifications:
Spec = portfolioSpec()
Spec

setTargetAlpha(Spec) = 0.6

# Allow for unlimiConstraints = "Short"ted Short Selling:
Constraints = "Short"

# Compute Short Selling Minimum Variance Portfolio
frontier = portfolioFrontier(Data, Spec, Constraint)

I seem to get always weights adding up to 1, no matter what I do...

I tried:

frontier = portfolioFrontier(Data, Spec, "maxsumW[1:22]=2")

Weights add up to 1 again.

frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=2")
frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=0.1")

The last two calls give back no portfolio. I wonder why? Is it no possible
to be leveraged/under invested?

I can't find anything of that sort on the unit tests either.

Cheers,

// Giuseppe

************************************************************
HSBC Bank plc may be solicited in the course of its placement efforts for a
new issue, by investment clients of the firm for whom the Bank as a firm
already provides other services. It may equally decide to allocate to its
own proprietary book or with an associate of HSBC Group. This represents a
potential conflict of interest. HSBC Bank plc has internal arrangements
designed to ensure that the firm would give unbiased and full advice to the
corporate finance client about the valuation and pricing of the offering as
well as internal systems, controls and procedures to identify and manage
conflicts of interest.

HSBC Bank plc
Registered Office: 8 Canada Square, London E14 5HQ, United Kingdom
Registered in England - Number 14259
Authorised and regulated by the Financial Services Authority.
************************************************************


-----------------------------------------
SAVE PAPER - THINK BEFORE YOU PRINT!

This transmission has been issued by a member of the HSBC Group
"HSBC" for the information of the addressee only and should not be
reproduced and/or distributed to any other person. Each page
attached hereto must be read in conjunction with any disclaimer
which forms part of it. Unless otherwise stated, this transmission
is neither an offer nor the solicitation of an offer to sell or
purchase any investment. Its contents are based on information
obtained from sources believed to be reliable but HSBC makes no
representation and accepts no responsibility or liability as to its
completeness or accuracy.


From brian at braverock.com  Thu Aug 21 15:31:03 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 21 Aug 2008 08:31:03 -0500
Subject: [R-SIG-Finance] fPortfolio and leverage
In-Reply-To: <OF5503C45E.6674F9A7-ON802574AC.0043C017-802574AC.00480166@hsbcib.com>
References: <OF5503C45E.6674F9A7-ON802574AC.0043C017-802574AC.00480166@hsbcib.com>
Message-ID: <48AD6E17.2040009@braverock.com>

your example can still account for leverage.

the w vector can be interpreted as percentage allocations from your 
total dollars to invest.

Your leverage is unconstrained from the optimization.  Whether you have 
100 euros to invest or 200 million euros to invest, you will still apply 
the weights from the output of the optimization.

Regards,

   - Brian

giuseppe1.milicia at hsbcib.com wrote:
> Guys,
> 
> I'm playing a bit with fPortfolio and looking at the examples and unit
> tests, it seems that the weights returned always sum up to 1.
> 
> I was wondering whether there is a way to have a leveraged portfolio with
> weights summing up to W > 1. Say I target a certain risk level R and I want
> the weights to be totally unconstrained. From the docs I see that
> Constraints = "Short" should given me unconstrained weights:
> 
> "Short": This selection defines the case of unlimited short selling. i.e.
> each weight may range
> between -Inf and Inf. Consequently, there are no group constraints. Risk
> budget constraints are
> not included in the portfolio optimization.
> 
> But, say if I try this:
> 
> # Load Data and Convert to timeSeries Object:
> Data = as.timeSeries(data(smallcap.ts))
> Data = Data[, c("BKE", "GG", "GYMB", "KRON")]
> Data
> 
> # Set Default Specifications:
> Spec = portfolioSpec()
> Spec
> 
> setTargetAlpha(Spec) = 0.6
> 
> # Allow for unlimiConstraints = "Short"ted Short Selling:
> Constraints = "Short"
> 
> # Compute Short Selling Minimum Variance Portfolio
> frontier = portfolioFrontier(Data, Spec, Constraint)
> 
> I seem to get always weights adding up to 1, no matter what I do...
> 
> I tried:
> 
> frontier = portfolioFrontier(Data, Spec, "maxsumW[1:22]=2")
> 
> Weights add up to 1 again.
> 
> frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=2")
> frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=0.1")
> 
> The last two calls give back no portfolio. I wonder why? Is it no possible
> to be leveraged/under invested?
> 
> I can't find anything of that sort on the unit tests either.
> 
> Cheers,
> 
> // Giuseppe


From giuseppe1.milicia at hsbcib.com  Thu Aug 21 15:41:09 2008
From: giuseppe1.milicia at hsbcib.com (giuseppe1.milicia at hsbcib.com)
Date: Thu, 21 Aug 2008 14:41:09 +0100
Subject: [R-SIG-Finance] fPortfolio and leverage
In-Reply-To: <48AD6E17.2040009@braverock.com>
Message-ID: <OFB6AF07F7.263967DF-ON802574AC.004A95A9-802574AC.004B35BE@hsbcib.com>

Brian,

You are right. But I was thinking of a slighly different setup for the
problem. Say you want to leverage at different levels for each of the
assets, with only a global risk target as goal. I thought that the easiest
way out was to leave that to the portfolio optimizer. My assets are not
equities and I assume they are traded on margin.

I believe that approach was taken, for instance, in "Portfolio optimization
with drawdown constraints" Checkhlov, Uryasec and Zabrankin.

From the paper:

"As for the technological constraints (8), we chose x_min = 0.2 and x_max
=0.8  . This choice was
dictated by the need to have the resultant margin-to-equity ratio in the
account within admissible
bounds, which are specific for a particular portfolio. In this futures
trading setup, these
constraints are analogous to the ?fully-invested? condition from classical
Sharpe-Markowitz
theory. They define bounds on the leverage of the strategy and make an
efficient frontier to be
concave. If all positions are equal to the lower bound 0.2, then the sum of
the positions equals
0.2 ?32 = 6.4 and the minimal leverage equals 6.4. However, if all
positions are equal to the
upper bound 0.8, then the sum of the positions equals 0.8? 32 = 25.6 and
the maximal leverage
equals 25.6. The optimal allocation of weights picks both the optimal
leverage and proportions
between instruments."

Cheers,

// Giuseppe



                                                                           
             "Brian G.                                                     
             Peterson"                                                     
             <brian at braverock.                                          To 
             com>                      Giuseppe1 MILICIA/IBEU/HSBC at HSBC    
                                                                        cc 
             21/08/2008 14:31          r-sig-finance at stat.math.ethz.ch     
             Mail Size: 5454                                       Subject 
                                       Re: [R-SIG-Finance] fPortfolio and  
                                       leverage                            
                                                                    Entity 
                                       Investment Banking Europe - IBEU    
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




your example can still account for leverage.

the w vector can be interpreted as percentage allocations from your
total dollars to invest.

Your leverage is unconstrained from the optimization.  Whether you have
100 euros to invest or 200 million euros to invest, you will still apply
the weights from the output of the optimization.

Regards,

   - Brian

giuseppe1.milicia at hsbcib.com wrote:
> Guys,
>
> I'm playing a bit with fPortfolio and looking at the examples and unit
> tests, it seems that the weights returned always sum up to 1.
>
> I was wondering whether there is a way to have a leveraged portfolio with
> weights summing up to W > 1. Say I target a certain risk level R and I
want
> the weights to be totally unconstrained. From the docs I see that
> Constraints = "Short" should given me unconstrained weights:
>
> "Short": This selection defines the case of unlimited short selling. i.e.
> each weight may range
> between -Inf and Inf. Consequently, there are no group constraints. Risk
> budget constraints are
> not included in the portfolio optimization.
>
> But, say if I try this:
>
> # Load Data and Convert to timeSeries Object:
> Data = as.timeSeries(data(smallcap.ts))
> Data = Data[, c("BKE", "GG", "GYMB", "KRON")]
> Data
>
> # Set Default Specifications:
> Spec = portfolioSpec()
> Spec
>
> setTargetAlpha(Spec) = 0.6
>
> # Allow for unlimiConstraints = "Short"ted Short Selling:
> Constraints = "Short"
>
> # Compute Short Selling Minimum Variance Portfolio
> frontier = portfolioFrontier(Data, Spec, Constraint)
>
> I seem to get always weights adding up to 1, no matter what I do...
>
> I tried:
>
> frontier = portfolioFrontier(Data, Spec, "maxsumW[1:22]=2")
>
> Weights add up to 1 again.
>
> frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=2")
> frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=0.1")
>
> The last two calls give back no portfolio. I wonder why? Is it no
possible
> to be leveraged/under invested?
>
> I can't find anything of that sort on the unit tests either.
>
> Cheers,
>
> // Giuseppe



************************************************************
HSBC Bank plc may be solicited in the course of its placement efforts for a
new issue, by investment clients of the firm for whom the Bank as a firm
already provides other services. It may equally decide to allocate to its
own proprietary book or with an associate of HSBC Group. This represents a
potential conflict of interest. HSBC Bank plc has internal arrangements
designed to ensure that the firm would give unbiased and full advice to the
corporate finance client about the valuation and pricing of the offering as
well as internal systems, controls and procedures to identify and manage
conflicts of interest.

HSBC Bank plc
Registered Office: 8 Canada Square, London E14 5HQ, United Kingdom
Registered in England - Number 14259
Authorised and regulated by the Financial Services Authority.
************************************************************

-----------------------------------------
SAVE PAPER - THINK BEFORE YOU PRINT!

This transmission has been issued by a member of the HSBC Group
"HSBC" for the information of the addressee only and should not be
reproduced and/or distributed to any other person. Each page
attached hereto must be read in conjunction with any disclaimer
which forms part of it. Unless otherwise stated, this transmission
is neither an offer nor the solicitation of an offer to sell or
purchase any investment. Its contents are based on information
obtained from sources believed to be reliable but HSBC makes no
representation and accepts no responsibility or liability as to its
completeness or accuracy.

From brian at braverock.com  Thu Aug 21 17:16:25 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 21 Aug 2008 10:16:25 -0500
Subject: [R-SIG-Finance] fPortfolio and leverage
In-Reply-To: <OFB6AF07F7.263967DF-ON802574AC.004A95A9-802574AC.004B35BE@hsbcib.com>
References: <OFB6AF07F7.263967DF-ON802574AC.004A95A9-802574AC.004B35BE@hsbcib.com>
Message-ID: <48AD86C9.7090008@braverock.com>

The Chekhlov, Uryasev, and Zabarankin paper you reference can be found here:

http://www.ise.ufl.edu/uryasev/drawdown.pdf

for anyone else who is playing along.

Note how on page 8 of the paper, which you quote, they set limits on the 
total weight range to develop a particular leverage model, but not on 
the alpha or performance of the model.

So, to your original example:

# Load Data and Convert to timeSeries Object:
Data = as.timeSeries(data(smallcap.ts))
Data = Data[, c("BKE", "GG", "GYMB", "KRON")]

# Set Default Specifications:
Spec = portfolioSpec()

setTargetAlpha(Spec) = 0.6

# Allow for unlimiConstraints = "Short"ted Short Selling:
Constraints = "Short"

# Compute Short Selling Minimum Variance Portfolio
frontier = portfolioFrontier(Data, Spec, Constraint)

#I seem to get always weights adding up to 1, no matter what I do...

#I tried:

frontier = portfolioFrontier(Data, Spec, "maxsumW[1:22]=2")

#Weights add up to 1 again.

frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=2")
frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=0.1")

# The last two calls give back no portfolio. I wonder why?
# Is it not possible to be leveraged/under invested?

This suggests that you should remove your setTargetAlpha constraint, and 
see what the optimizer does only with constraints of maxsumW[1:22]=2 
*and* minsumW[1:22]=0.1, which I believe can be specified in your 
portfolioSpec() call.

Regards,

   - Brian

giuseppe1.milicia at hsbcib.com wrote:
> Brian,
> 
> You are right. But I was thinking of a slighly different setup for the
> problem. Say you want to leverage at different levels for each of the
> assets, with only a global risk target as goal. I thought that the easiest
> way out was to leave that to the portfolio optimizer. My assets are not
> equities and I assume they are traded on margin.
> 
> I believe that approach was taken, for instance, in "Portfolio optimization
> with drawdown constraints" Checkhlov, Uryasec and Zabrankin.
> 
> From the paper:
> 
> "As for the technological constraints (8), we chose x_min = 0.2 and x_max
> =0.8  . This choice was
> dictated by the need to have the resultant margin-to-equity ratio in the
> account within admissible
> bounds, which are specific for a particular portfolio. In this futures
> trading setup, these
> constraints are analogous to the ?fully-invested? condition from classical
> Sharpe-Markowitz
> theory. They define bounds on the leverage of the strategy and make an
> efficient frontier to be
> concave. If all positions are equal to the lower bound 0.2, then the sum of
> the positions equals
> 0.2 ?32 = 6.4 and the minimal leverage equals 6.4. However, if all
> positions are equal to the
> upper bound 0.8, then the sum of the positions equals 0.8? 32 = 25.6 and
> the maximal leverage
> equals 25.6. The optimal allocation of weights picks both the optimal
> leverage and proportions
> between instruments."
> 
> Cheers,
> 
> // Giuseppe
>> Brian G. Peterson wrote:
>> your example can still account for leverage.
>> 
>> the w vector can be interpreted as percentage allocations from your
>> total dollars to invest.
>> 
>> Your leverage is unconstrained from the optimization.  Whether you have
>> 100 euros to invest or 200 million euros to invest, you will still apply
>> the weights from the output of the optimization.
>> 
>> Regards,
>> 
>>    - Brian
>> 
>>> giuseppe1.milicia at hsbcib.com wrote:
>>> Guys,
>>>
>>> I'm playing a bit with fPortfolio and looking at the examples and unit
>>> tests, it seems that the weights returned always sum up to 1.
>>>
>>> I was wondering whether there is a way to have a leveraged portfolio with
>>> weights summing up to W > 1. Say I target a certain risk level R and I
>>> want the weights to be totally unconstrained. From the docs I see that
>>> Constraints = "Short" should given me unconstrained weights:
>>> "Short": This selection defines the case of unlimited short selling. i.e.
>>> each weight may range
>>> between -Inf and Inf. Consequently, there are no group constraints. Risk
>>> budget constraints are
>>> not included in the portfolio optimization.
<...>


From giuseppe1.milicia at hsbcib.com  Thu Aug 21 19:12:22 2008
From: giuseppe1.milicia at hsbcib.com (giuseppe1.milicia at hsbcib.com)
Date: Thu, 21 Aug 2008 18:12:22 +0100
Subject: [R-SIG-Finance] fPortfolio and leverage
In-Reply-To: <48AD86C9.7090008@braverock.com>
Message-ID: <OF6FA7B5C4.27F2FEA3-ON802574AC.0059A3FB-802574AC.005E8C5B@hsbcib.com>

Brian,

I don't think you can remove the target alpha. When you create a portfolio
spec, it's there by default.

I tried the constraints you mentioned:

frontier = portfolioFrontier(Data, Spec,
c("minsumW[1:22]=0.5","maxsumW[1:22]=2"))

and

frontier = portfolioFrontier(Data, Spec,
c("minW[1:22]=0.2","maxW[1:22]=0.6"))

which puts the constraints on the single weights.

In the second case there is no solution. In the first again all the weights
add to 1

I debugged the whole thing and it seems that .setBoxGroupConstraints fixes
a budget constraints forcing the weights to add up to 1. It seems that the
constratin sum(w_i)=1 is added automatically and cannot be removed.

I guess I should really set up the QP problem myself if I want to
experiment with the sort of thing... :(

Cheers,

// Giuseppe



                                                                           
             "Brian G.                                                     
             Peterson"                                                     
             <brian at braverock.                                          To 
             com>                      Giuseppe1 MILICIA/IBEU/HSBC at HSBC    
                                                                        cc 
             21/08/2008 16:16          r-sig-finance at stat.math.ethz.ch     
             Mail Size: 7779                                       Subject 
                                       Re: [R-SIG-Finance] fPortfolio and  
                                       leverage                            
                                                                    Entity 
                                       Investment Banking Europe - IBEU    
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




The Chekhlov, Uryasev, and Zabarankin paper you reference can be found
here:

http://www.ise.ufl.edu/uryasev/drawdown.pdf

for anyone else who is playing along.

Note how on page 8 of the paper, which you quote, they set limits on the
total weight range to develop a particular leverage model, but not on
the alpha or performance of the model.

So, to your original example:

# Load Data and Convert to timeSeries Object:
Data = as.timeSeries(data(smallcap.ts))
Data = Data[, c("BKE", "GG", "GYMB", "KRON")]

# Set Default Specifications:
Spec = portfolioSpec()

setTargetAlpha(Spec) = 0.6

# Allow for unlimiConstraints = "Short"ted Short Selling:
Constraints = "Short"

# Compute Short Selling Minimum Variance Portfolio
frontier = portfolioFrontier(Data, Spec, Constraint)

#I seem to get always weights adding up to 1, no matter what I do...

#I tried:

frontier = portfolioFrontier(Data, Spec, "maxsumW[1:22]=2")

#Weights add up to 1 again.

frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=2")
frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=0.1")

# The last two calls give back no portfolio. I wonder why?
# Is it not possible to be leveraged/under invested?

This suggests that you should remove your setTargetAlpha constraint, and
see what the optimizer does only with constraints of maxsumW[1:22]=2
*and* minsumW[1:22]=0.1, which I believe can be specified in your
portfolioSpec() call.

Regards,

   - Brian

giuseppe1.milicia at hsbcib.com wrote:
> Brian,
>
> You are right. But I was thinking of a slighly different setup for the
> problem. Say you want to leverage at different levels for each of the
> assets, with only a global risk target as goal. I thought that the
easiest
> way out was to leave that to the portfolio optimizer. My assets are not
> equities and I assume they are traded on margin.
>
> I believe that approach was taken, for instance, in "Portfolio
optimization
> with drawdown constraints" Checkhlov, Uryasec and Zabrankin.
>
> From the paper:
>
> "As for the technological constraints (8), we chose x_min = 0.2 and x_max
> =0.8  . This choice was
> dictated by the need to have the resultant margin-to-equity ratio in the
> account within admissible
> bounds, which are specific for a particular portfolio. In this futures
> trading setup, these
> constraints are analogous to the ?fully-invested? condition from
classical
> Sharpe-Markowitz
> theory. They define bounds on the leverage of the strategy and make an
> efficient frontier to be
> concave. If all positions are equal to the lower bound 0.2, then the sum
of
> the positions equals
> 0.2 ?32 = 6.4 and the minimal leverage equals 6.4. However, if all
> positions are equal to the
> upper bound 0.8, then the sum of the positions equals 0.8? 32 = 25.6 and
> the maximal leverage
> equals 25.6. The optimal allocation of weights picks both the optimal
> leverage and proportions
> between instruments."
>
> Cheers,
>
> // Giuseppe
>> Brian G. Peterson wrote:
>> your example can still account for leverage.
>>
>> the w vector can be interpreted as percentage allocations from your
>> total dollars to invest.
>>
>> Your leverage is unconstrained from the optimization.  Whether you have
>> 100 euros to invest or 200 million euros to invest, you will still apply
>> the weights from the output of the optimization.
>>
>> Regards,
>>
>>    - Brian
>>
>>> giuseppe1.milicia at hsbcib.com wrote:
>>> Guys,
>>>
>>> I'm playing a bit with fPortfolio and looking at the examples and unit
>>> tests, it seems that the weights returned always sum up to 1.
>>>
>>> I was wondering whether there is a way to have a leveraged portfolio
with
>>> weights summing up to W > 1. Say I target a certain risk level R and I
>>> want the weights to be totally unconstrained. From the docs I see that
>>> Constraints = "Short" should given me unconstrained weights:
>>> "Short": This selection defines the case of unlimited short selling.
i.e.
>>> each weight may range
>>> between -Inf and Inf. Consequently, there are no group constraints.
Risk
>>> budget constraints are
>>> not included in the portfolio optimization.
<...>



************************************************************
HSBC Bank plc may be solicited in the course of its placement efforts for a
new issue, by investment clients of the firm for whom the Bank as a firm
already provides other services. It may equally decide to allocate to its
own proprietary book or with an associate of HSBC Group. This represents a
potential conflict of interest. HSBC Bank plc has internal arrangements
designed to ensure that the firm would give unbiased and full advice to the
corporate finance client about the valuation and pricing of the offering as
well as internal systems, controls and procedures to identify and manage
conflicts of interest.

HSBC Bank plc
Registered Office: 8 Canada Square, London E14 5HQ, United Kingdom
Registered in England - Number 14259
Authorised and regulated by the Financial Services Authority.
************************************************************

-----------------------------------------
SAVE PAPER - THINK BEFORE YOU PRINT!

This transmission has been issued by a member of the HSBC Group
"HSBC" for the information of the addressee only and should not be
reproduced and/or distributed to any other person. Each page
attached hereto must be read in conjunction with any disclaimer
which forms part of it. Unless otherwise stated, this transmission
is neither an offer nor the solicitation of an offer to sell or
purchase any investment. Its contents are based on information
obtained from sources believed to be reliable but HSBC makes no
representation and accepts no responsibility or liability as to its
completeness or accuracy.

From brian at braverock.com  Thu Aug 21 19:21:10 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 21 Aug 2008 12:21:10 -0500
Subject: [R-SIG-Finance] fPortfolio and leverage
In-Reply-To: <OF6FA7B5C4.27F2FEA3-ON802574AC.0059A3FB-802574AC.005E8C5B@hsbcib.com>
References: <OF6FA7B5C4.27F2FEA3-ON802574AC.0059A3FB-802574AC.005E8C5B@hsbcib.com>
Message-ID: <48ADA406.7080001@braverock.com>

then I would recommend downloading the source function file and changing 
the code directly, or changing the constraint inside the debugger.  The 
debugger is interactive, so you could set sum(w_i)=2 or whatever you 
need from inside the debugger.

Hopefully Yohan or Diethelm will chime in here and help out.

Regards,

   - Brian

giuseppe1.milicia at hsbcib.com wrote:
> Brian,
> 
> I don't think you can remove the target alpha. When you create a portfolio
> spec, it's there by default.
> 
> I tried the constraints you mentioned:
> 
> frontier = portfolioFrontier(Data, Spec,
> c("minsumW[1:22]=0.5","maxsumW[1:22]=2"))
> 
> and
> 
> frontier = portfolioFrontier(Data, Spec,
> c("minW[1:22]=0.2","maxW[1:22]=0.6"))
> 
> which puts the constraints on the single weights.
> 
> In the second case there is no solution. In the first again all the weights
> add to 1
> 
> I debugged the whole thing and it seems that .setBoxGroupConstraints fixes
> a budget constraints forcing the weights to add up to 1. It seems that the
> constratin sum(w_i)=1 is added automatically and cannot be removed.
> 
> I guess I should really set up the QP problem myself if I want to
> experiment with the sort of thing... :(
> 
> Cheers,
> 
> // Giuseppe
> 
> 
> 
>                                                                            
>              "Brian G.                                                     
>              Peterson"                                                     
>              <brian at braverock.                                          To 
>              com>                      Giuseppe1 MILICIA/IBEU/HSBC at HSBC    
>                                                                         cc 
>              21/08/2008 16:16          r-sig-finance at stat.math.ethz.ch     
>              Mail Size: 7779                                       Subject 
>                                        Re: [R-SIG-Finance] fPortfolio and  
>                                        leverage                            
>                                                                     Entity 
>                                        Investment Banking Europe - IBEU    
>                                                                            
>                                                                            
>                                                                            
>                                                                            
>                                                                            
>                                                                            
> 
> 
> 
> 
> The Chekhlov, Uryasev, and Zabarankin paper you reference can be found
> here:
> 
> http://www.ise.ufl.edu/uryasev/drawdown.pdf
> 
> for anyone else who is playing along.
> 
> Note how on page 8 of the paper, which you quote, they set limits on the
> total weight range to develop a particular leverage model, but not on
> the alpha or performance of the model.
> 
> So, to your original example:
> 
> # Load Data and Convert to timeSeries Object:
> Data = as.timeSeries(data(smallcap.ts))
> Data = Data[, c("BKE", "GG", "GYMB", "KRON")]
> 
> # Set Default Specifications:
> Spec = portfolioSpec()
> 
> setTargetAlpha(Spec) = 0.6
> 
> # Allow for unlimiConstraints = "Short"ted Short Selling:
> Constraints = "Short"
> 
> # Compute Short Selling Minimum Variance Portfolio
> frontier = portfolioFrontier(Data, Spec, Constraint)
> 
> #I seem to get always weights adding up to 1, no matter what I do...
> 
> #I tried:
> 
> frontier = portfolioFrontier(Data, Spec, "maxsumW[1:22]=2")
> 
> #Weights add up to 1 again.
> 
> frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=2")
> frontier = portfolioFrontier(Data, Spec, "minsumW[1:22]=0.1")
> 
> # The last two calls give back no portfolio. I wonder why?
> # Is it not possible to be leveraged/under invested?
> 
> This suggests that you should remove your setTargetAlpha constraint, and
> see what the optimizer does only with constraints of maxsumW[1:22]=2
> *and* minsumW[1:22]=0.1, which I believe can be specified in your
> portfolioSpec() call.
> 
> Regards,
> 
>    - Brian
> 
> giuseppe1.milicia at hsbcib.com wrote:
>> Brian,
>>
>> You are right. But I was thinking of a slighly different setup for the
>> problem. Say you want to leverage at different levels for each of the
>> assets, with only a global risk target as goal. I thought that the
> easiest
>> way out was to leave that to the portfolio optimizer. My assets are not
>> equities and I assume they are traded on margin.
>>
>> I believe that approach was taken, for instance, in "Portfolio
> optimization
>> with drawdown constraints" Checkhlov, Uryasec and Zabrankin.
>>
>> From the paper:
>>
>> "As for the technological constraints (8), we chose x_min = 0.2 and x_max
>> =0.8  . This choice was
>> dictated by the need to have the resultant margin-to-equity ratio in the
>> account within admissible
>> bounds, which are specific for a particular portfolio. In this futures
>> trading setup, these
>> constraints are analogous to the ?fully-invested? condition from
> classical
>> Sharpe-Markowitz
>> theory. They define bounds on the leverage of the strategy and make an
>> efficient frontier to be
>> concave. If all positions are equal to the lower bound 0.2, then the sum
> of
>> the positions equals
>> 0.2 ?32 = 6.4 and the minimal leverage equals 6.4. However, if all
>> positions are equal to the
>> upper bound 0.8, then the sum of the positions equals 0.8? 32 = 25.6 and
>> the maximal leverage
>> equals 25.6. The optimal allocation of weights picks both the optimal
>> leverage and proportions
>> between instruments."
>>
>> Cheers,
>>
>> // Giuseppe
>>> Brian G. Peterson wrote:
>>> your example can still account for leverage.
>>>
>>> the w vector can be interpreted as percentage allocations from your
>>> total dollars to invest.
>>>
>>> Your leverage is unconstrained from the optimization.  Whether you have
>>> 100 euros to invest or 200 million euros to invest, you will still apply
>>> the weights from the output of the optimization.
>>>
>>> Regards,
>>>
>>>    - Brian
>>>
>>>> giuseppe1.milicia at hsbcib.com wrote:
>>>> Guys,
>>>>
>>>> I'm playing a bit with fPortfolio and looking at the examples and unit
>>>> tests, it seems that the weights returned always sum up to 1.
>>>>
>>>> I was wondering whether there is a way to have a leveraged portfolio
> with
>>>> weights summing up to W > 1. Say I target a certain risk level R and I
>>>> want the weights to be totally unconstrained. From the docs I see that
>>>> Constraints = "Short" should given me unconstrained weights:
>>>> "Short": This selection defines the case of unlimited short selling.
> i.e.
>>>> each weight may range
>>>> between -Inf and Inf. Consequently, there are no group constraints.
> Risk
>>>> budget constraints are
>>>> not included in the portfolio optimization.
> <...>
> 
> 
> 
> ************************************************************
> HSBC Bank plc may be solicited in the course of its placement efforts for a
> new issue, by investment clients of the firm for whom the Bank as a firm
> already provides other services. It may equally decide to allocate to its
> own proprietary book or with an associate of HSBC Group. This represents a
> potential conflict of interest. HSBC Bank plc has internal arrangements
> designed to ensure that the firm would give unbiased and full advice to the
> corporate finance client about the valuation and pricing of the offering as
> well as internal systems, controls and procedures to identify and manage
> conflicts of interest.
> 
> HSBC Bank plc
> Registered Office: 8 Canada Square, London E14 5HQ, United Kingdom
> Registered in England - Number 14259
> Authorised and regulated by the Financial Services Authority.
> ************************************************************
> 
> -----------------------------------------
> SAVE PAPER - THINK BEFORE YOU PRINT!
> 
> This transmission has been issued by a member of the HSBC Group
> "HSBC" for the information of the addressee only and should not be
> reproduced and/or distributed to any other person. Each page
> attached hereto must be read in conjunction with any disclaimer
> which forms part of it. Unless otherwise stated, this transmission
> is neither an offer nor the solicitation of an offer to sell or
> purchase any investment. Its contents are based on information
> obtained from sources believed to be reliable but HSBC makes no
> representation and accepts no responsibility or liability as to its
> completeness or accuracy.


From elise at predictionimpact.com  Fri Aug 22 07:58:52 2008
From: elise at predictionimpact.com (Elise Johnson)
Date: Thu, 21 Aug 2008 22:58:52 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Predictive Analytics event Sept
	24-25, Chicago
Message-ID: <19101655.post@talk.nabble.com>


Hi, I wanted to make sure you were all aware of these upcoming events. There
is a seminar in Predictive Analytics on Sept 24-25 in Chicago, and two
following in D.C. (Oct.) and San Francisco (Nov.).  This is intensive
training for managers, marketers, and IT people who need to make sense of
customer data to predict buying behavior, profit, etc.  Past attendees have
given rave reviews. 

You can find more info at
http://www.predictionimpact.com/predictive-analytics-training.html, e-mail
training at predictionimpact.com, or call (415) 683-1146.

thanks --Elise Johnson, Prediction Impact

-- 
View this message in context: http://www.nabble.com/Predictive-Analytics-event-Sept-24-25%2C-Chicago-tp19101655p19101655.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From zornitsa.luleva at gmail.com  Mon Aug 25 15:50:52 2008
From: zornitsa.luleva at gmail.com (Zornitsa Luleva)
Date: Mon, 25 Aug 2008 15:50:52 +0200
Subject: [R-SIG-Finance] Parabolic cylinder function
Message-ID: <e67ad1a20808250650p27d22449p259d3af3ff8ac5d2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080825/6e664679/attachment.pl>

From adrian_d at eskimo.com  Mon Aug 25 16:22:57 2008
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Mon, 25 Aug 2008 07:22:57 -0700 (PDT)
Subject: [R-SIG-Finance] Parabolic cylinder function
In-Reply-To: <e67ad1a20808250650p27d22449p259d3af3ff8ac5d2@mail.gmail.com>
References: <e67ad1a20808250650p27d22449p259d3af3ff8ac5d2@mail.gmail.com>
Message-ID: <Pine.SUN.4.58.0808250717290.28178@eskimo.com>


Check hypergeo package for an implementation of the hypergeometric
function.  You can express the solution to the parabolic cylinder equation
in terms of the hypergeometric function.  See Abramowits&Stegun.

Adrian

On Mon, 25 Aug 2008, Zornitsa Luleva wrote:

> Dear all,
>
> I need your advice since I am looking for an implementation of the parabolic
> cylinder function in R. I found implemantations of the hypergeometric
> functions (the Whittaker and the confluent hypogeometric functions) in the
> package fAsianOptions but the parabolic cylinder function was unfortunately
> not there. Do you know of such implementation?
>
> Thank you very much for your advice.
>
> Cheers,
> Zoe
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From benverschuere at hotmail.com  Mon Aug 25 18:18:21 2008
From: benverschuere at hotmail.com (Verschuere Benjamin)
Date: Mon, 25 Aug 2008 18:18:21 +0200
Subject: [R-SIG-Finance] Parabolic cylinder function
In-Reply-To: <e67ad1a20808250650p27d22449p259d3af3ff8ac5d2@mail.gmail.com>
References: <e67ad1a20808250650p27d22449p259d3af3ff8ac5d2@mail.gmail.com>
Message-ID: <BLU109-W480DC392735438943F0170B8670@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080825/b6d3166a/attachment.pl>

From Matthias.Koberstein at hsbctrinkaus.de  Tue Aug 26 17:53:18 2008
From: Matthias.Koberstein at hsbctrinkaus.de (Matthias.Koberstein at hsbctrinkaus.de)
Date: Tue, 26 Aug 2008 17:53:18 +0200
Subject: [R-SIG-Finance] Generating Distributions with set skewness and
	kurtosis
Message-ID: <OF62C4844B.17259C3C-ONC12574B1.0056CD0F-C12574B1.00574664@hsbctrinkaus.de>


Hello,

I am reaching out to you for help since I am struggeling to find a function
to generate distributions with a set statistical properties as kurtosis and
skewdness.
Lets say I want to generate random variables following a "normal"
distribution, but with skewness 2 and kurtosis 5.
How would I do that, the most efficient way? Are there any packages for
that? I had a quick look but were only able to find packages which
calculate statistical
distribution properties after having the data.

Thank you very much

Matthias

**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential...{{dropped:18}}


From frainj at tcd.ie  Tue Aug 26 18:37:39 2008
From: frainj at tcd.ie (John Frain)
Date: Tue, 26 Aug 2008 17:37:39 +0100
Subject: [R-SIG-Finance] Generating Distributions with set skewness and
	kurtosis
In-Reply-To: <OF62C4844B.17259C3C-ONC12574B1.0056CD0F-C12574B1.00574664@hsbctrinkaus.de>
References: <OF62C4844B.17259C3C-ONC12574B1.0056CD0F-C12574B1.00574664@hsbctrinkaus.de>
Message-ID: <cfdde1650808260937q26cea3f6tb859a401b8719767@mail.gmail.com>

The normal distribution is symmetric and therefore can not be skewed.
The common measure of skewness when applied to the normal is 0.  The
kurtosis of the normal distribution is also fixed at 3.  If you want
to simulate a distribution with skew and heavy tails there are many
ways to do so.  A lot depends on what you want to do with the skewed
distributions.  You may look at mixtures of normal distributions or at
distributions which naturally allow skew such as the alpha-stable
distribution.  This latter has infinite variance so be careful as this
may or may not be what you want.  If you want to independently fix
location, spread, skew and kurtosis you will need a probability
distribution with 4 parameters.

John Frain

2008/8/26  <Matthias.Koberstein at hsbctrinkaus.de>:
>
> Hello,
>
> I am reaching out to you for help since I am struggeling to find a function
> to generate distributions with a set statistical properties as kurtosis and
> skewdness.
> Lets say I want to generate random variables following a "normal"
> distribution, but with skewness 2 and kurtosis 5.
> How would I do that, the most efficient way? Are there any packages for
> that? I had a quick look but were only able to find packages which
> calculate statistical
> distribution properties after having the data.
>
> Thank you very much
>
> Matthias
>
> **** Ressourcen schonen, weniger drucken - Think before you print! ****
>
> ---------------------------------------------------------------------
> Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
> rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
> sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
> sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
> oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
> gestattet.
>
> This e-mail and any attachments may contain confidential...{{dropped:18}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>



-- 
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

From jnoble1 at mmm.com  Tue Aug 26 19:01:42 2008
From: jnoble1 at mmm.com (jnoble1 at mmm.com)
Date: Tue, 26 Aug 2008 12:01:42 -0500
Subject: [R-SIG-Finance] R for Individual Stock Trading Analysis
Message-ID: <OF445A1890.5BFEA5E4-ON862574B1.005C5286-862574B1.005D8AA0@mmm.com>


Based on the discussion the past couple of months, it's obvious that this
list is primarily geared towards serious quantitative finance and
econometrics.  I was wondering about the activities of R users focusing on
individual stock trading.  Obviously, one is capable with R of bring some
highly sophisticated toolsets to the problems of trend and pattern
recognition in time series (not to spark a debate on random walk vs.
predictability, etc.).   Being a newbie to R, I was wondering if there are
any blogs, papers, discussions, etc. geared towards someone who's
technically inclined but beginning with R and wanting to perform individual
equity analysis for trading purposes.
Thanks,

Jonathan Noble
jnoble1 at mmm.com


From patrick at burns-stat.com  Tue Aug 26 19:47:45 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 26 Aug 2008 18:47:45 +0100
Subject: [R-SIG-Finance] Generating Distributions with set skewness and
 kurtosis
In-Reply-To: <cfdde1650808260937q26cea3f6tb859a401b8719767@mail.gmail.com>
References: <OF62C4844B.17259C3C-ONC12574B1.0056CD0F-C12574B1.00574664@hsbctrinkaus.de>
	<cfdde1650808260937q26cea3f6tb859a401b8719767@mail.gmail.com>
Message-ID: <48B441C1.6000408@burns-stat.com>

I suspect that the 'sn' package (as in skew-normal)
is the natural target of the search.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

John Frain wrote:
> The normal distribution is symmetric and therefore can not be skewed.
> The common measure of skewness when applied to the normal is 0.  The
> kurtosis of the normal distribution is also fixed at 3.  If you want
> to simulate a distribution with skew and heavy tails there are many
> ways to do so.  A lot depends on what you want to do with the skewed
> distributions.  You may look at mixtures of normal distributions or at
> distributions which naturally allow skew such as the alpha-stable
> distribution.  This latter has infinite variance so be careful as this
> may or may not be what you want.  If you want to independently fix
> location, spread, skew and kurtosis you will need a probability
> distribution with 4 parameters.
>
> John Frain
>
> 2008/8/26  <Matthias.Koberstein at hsbctrinkaus.de>:
>   
>> Hello,
>>
>> I am reaching out to you for help since I am struggeling to find a function
>> to generate distributions with a set statistical properties as kurtosis and
>> skewdness.
>> Lets say I want to generate random variables following a "normal"
>> distribution, but with skewness 2 and kurtosis 5.
>> How would I do that, the most efficient way? Are there any packages for
>> that? I had a quick look but were only able to find packages which
>> calculate statistical
>> distribution properties after having the data.
>>
>> Thank you very much
>>
>> Matthias
>>
>> **** Ressourcen schonen, weniger drucken - Think before you print! ****
>>
>> ---------------------------------------------------------------------
>> Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
>> rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
>> sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
>> sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
>> oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
>> gestattet.
>>
>> This e-mail and any attachments may contain confidential...{{dropped:18}}
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>>     
>
>
>
>


From davidr at rhotrading.com  Tue Aug 26 19:54:05 2008
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 26 Aug 2008 12:54:05 -0500
Subject: [R-SIG-Finance] Generating Distributions with set skewness and
	kurtosis
In-Reply-To: <OF62C4844B.17259C3C-ONC12574B1.0056CD0F-C12574B1.00574664@hsbctrinkaus.de>
References: <OF62C4844B.17259C3C-ONC12574B1.0056CD0F-C12574B1.00574664@hsbctrinkaus.de>
Message-ID: <F9F2A641C593D7408925574C05A7BE7701564189@rhopost.rhotrading.com>

You might try the Johnson distributions, algorithms 99 and 100 from APPL. STATIST. (1976) VOL.25, P.180ff
I don't have this any more, but maybe you have access to JSTOR.

David L. Reiner, PhD
Head Quant
Rho Trading Securities, LLC


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Matthias.Koberstein at hsbctrinkaus.de
Sent: Tuesday, August 26, 2008 10:53 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Generating Distributions with set skewness andkurtosis


Hello,

I am reaching out to you for help since I am struggeling to find a function
to generate distributions with a set statistical properties as kurtosis and
skewdness.
Lets say I want to generate random variables following a "normal"
distribution, but with skewness 2 and kurtosis 5.
How would I do that, the most efficient way? Are there any packages for
that? I had a quick look but were only able to find packages which
calculate statistical
distribution properties after having the data.

Thank you very much

Matthias

**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential...{{dropped:18}}

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From brian at braverock.com  Tue Aug 26 20:10:48 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 26 Aug 2008 13:10:48 -0500
Subject: [R-SIG-Finance] R for Individual Stock Trading Analysis
In-Reply-To: <OF445A1890.5BFEA5E4-ON862574B1.005C5286-862574B1.005D8AA0@mmm.com>
References: <OF445A1890.5BFEA5E4-ON862574B1.005C5286-862574B1.005D8AA0@mmm.com>
Message-ID: <48B44728.1020809@braverock.com>

jnoble1 at mmm.com wrote:
> Based on the discussion the past couple of months, it's obvious that this
> list is primarily geared towards serious quantitative finance and
> econometrics.  I was wondering about the activities of R users focusing on
> individual stock trading.  Obviously, one is capable with R of bring some
> highly sophisticated toolsets to the problems of trend and pattern
> recognition in time series (not to spark a debate on random walk vs.
> predictability, etc.).   Being a newbie to R, I was wondering if there are
> any blogs, papers, discussions, etc. geared towards someone who's
> technically inclined but beginning with R and wanting to perform individual
> equity analysis for trading purposes.

Jonathan,

You are correct that most of the posters on this list are professionals 
or academics in quantitative finance.  I suspect that this list would 
still be useful for the beginner interested in these topics, but 
probably not a great place to ask completely uninformed questions.

For general R help questions, the R-help list has a lot of new R users 
on it, and straightforward trend and factor analysis questions would not 
be out of place there (or here, if asked correctly, see below).

Several of the packages developed by the professionals on this list will 
probably enter into your toolkit at some point.  Packages like 
portfolio, quantmod, PerformanceAnalytics, RMetrics, etc. will have 
already implemented a wide array of techniques that you will see used 
for systematic equity analysis and trading.

If you haven't already seen it, take a look at the Empirical Finance 
Task View here:

http://cran.r-project.org/web/views/Finance.html

for an overview of some of the packages that you may find useful.

If you're looking for a good book on quantitative finance that covers a 
lot of the theoretical underpinnings without getting lost in the math of 
the advanced techniques, I recommend picking up a copy of

Statistics and Finance: An Introduction
by David Ruppert
http://www.amazon.com/Statistics-Finance-Introduction-David-Ruppert/dp/0387202706

R code for all of his examples is available online.

To maximize your benefit from this list, and the signal-to-noise ratio 
for the long-term list members, try to ask very specific questions with 
your test code to back them up.

I recommend reading the R project list posting guide here:

http://www.r-project.org/posting-guide.html

and "How to ask questions the smart way." here:

http://www.catb.org/~esr/faqs/smart-questions.html

Generally, clearly describing what you're trying to do, what you've 
tried to get there, links to any relevant papers or books, and some 
sample code and data will help the members of R-help or this list 
respond to your question in the least possible time (for us) with the 
best possible answer (for you).

Regards,

   - Brian


From jeff.a.ryan at gmail.com  Tue Aug 26 21:00:10 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 26 Aug 2008 14:00:10 -0500
Subject: [R-SIG-Finance] R for Individual Stock Trading Analysis
In-Reply-To: <48B44728.1020809@braverock.com>
References: <OF445A1890.5BFEA5E4-ON862574B1.005C5286-862574B1.005D8AA0@mmm.com>
	<48B44728.1020809@braverock.com>
Message-ID: <e8e755250808261200v29ca4e1g1c5cf0e48b227cad@mail.gmail.com>

I'll second Brian's comments, and add a few more.

Looking through the list archives will provide tremendous insight  ---
make that a habit.  All the R-lists.

I think a reasonable number of packages on CRAN, as well as on the
Finance Task Views, include pdf vignettes.  These are usually quite
valuable as well for a general take on what you can do.  The
PerformanceAnalytics and portfolio ones are especially nice.

quantmod has some examples at:
http://www.quantmod.com/examples/

quantmod also has full charting ability with technical indicators
(from the TTR package) as well as a simple mechanism to add your own.
There are also some 'coming soon' links that are to be posted... some
day.

quantmod uses the 'xts' package, which is very 'zoo'-oriented.  Both
these packages have nice vignettes to make data management,
manageable.

In terms of non-professional data I also have an interface to
Interactive Brokers available on CRAN.  This at present allows for
historical and RT data capture/playback from IB.  The development
version has a INcomplete mechanism to execute trades through that
platform as well.

Of course Yahoo, Google, Federal Reserve, and Oanda (FX) data is
available in a few packages (quantmod included).  Additional data
tools include packages 'opentick' [access to opentick data in R], and
'RCSI' [www.csidata.com].  Both of those are on R-forge.  Bloomberg
API is also available, though I suspect that is not in the private
trader arsenal :)

Hope that helps,
Jeff


On Tue, Aug 26, 2008 at 1:10 PM, Brian G. Peterson <brian at braverock.com> wrote:
> jnoble1 at mmm.com wrote:
>>
>> Based on the discussion the past couple of months, it's obvious that this
>> list is primarily geared towards serious quantitative finance and
>> econometrics.  I was wondering about the activities of R users focusing on
>> individual stock trading.  Obviously, one is capable with R of bring some
>> highly sophisticated toolsets to the problems of trend and pattern
>> recognition in time series (not to spark a debate on random walk vs.
>> predictability, etc.).   Being a newbie to R, I was wondering if there are
>> any blogs, papers, discussions, etc. geared towards someone who's
>> technically inclined but beginning with R and wanting to perform
>> individual
>> equity analysis for trading purposes.
>
> Jonathan,
>
> You are correct that most of the posters on this list are professionals or
> academics in quantitative finance.  I suspect that this list would still be
> useful for the beginner interested in these topics, but probably not a great
> place to ask completely uninformed questions.
>
> For general R help questions, the R-help list has a lot of new R users on
> it, and straightforward trend and factor analysis questions would not be out
> of place there (or here, if asked correctly, see below).
>
> Several of the packages developed by the professionals on this list will
> probably enter into your toolkit at some point.  Packages like portfolio,
> quantmod, PerformanceAnalytics, RMetrics, etc. will have already implemented
> a wide array of techniques that you will see used for systematic equity
> analysis and trading.
>
> If you haven't already seen it, take a look at the Empirical Finance Task
> View here:
>
> http://cran.r-project.org/web/views/Finance.html
>
> for an overview of some of the packages that you may find useful.
>
> If you're looking for a good book on quantitative finance that covers a lot
> of the theoretical underpinnings without getting lost in the math of the
> advanced techniques, I recommend picking up a copy of
>
> Statistics and Finance: An Introduction
> by David Ruppert
> http://www.amazon.com/Statistics-Finance-Introduction-David-Ruppert/dp/0387202706
>
> R code for all of his examples is available online.
>
> To maximize your benefit from this list, and the signal-to-noise ratio for
> the long-term list members, try to ask very specific questions with your
> test code to back them up.
>
> I recommend reading the R project list posting guide here:
>
> http://www.r-project.org/posting-guide.html
>
> and "How to ask questions the smart way." here:
>
> http://www.catb.org/~esr/faqs/smart-questions.html
>
> Generally, clearly describing what you're trying to do, what you've tried to
> get there, links to any relevant papers or books, and some sample code and
> data will help the members of R-help or this list respond to your question
> in the least possible time (for us) with the best possible answer (for you).
>
> Regards,
>
>  - Brian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From matthewcleggphd at gmail.com  Wed Aug 27 02:49:52 2008
From: matthewcleggphd at gmail.com (Matthew Clegg)
Date: Tue, 26 Aug 2008 20:49:52 -0400
Subject: [R-SIG-Finance] Generating Distributions with set skewness and
	kurtosis
In-Reply-To: <OF62C4844B.17259C3C-ONC12574B1.0056CD0F-C12574B1.00574664@hsbctrinkaus.de>
References: <OF62C4844B.17259C3C-ONC12574B1.0056CD0F-C12574B1.00574664@hsbctrinkaus.de>
Message-ID: <e199b8e70808261749q20494c8bm2288098b84a474fe@mail.gmail.com>

On Tue, Aug 26, 2008 at 11:53 AM,  <Matthias.Koberstein at hsbctrinkaus.de> wrote:
>
> Hello,
>
> I am reaching out to you for help since I am struggeling to find a function
> to generate distributions with a set statistical properties as kurtosis and
> skewdness.
> Lets say I want to generate random variables following a "normal"
> distribution, but with skewness 2 and kurtosis 5.
> How would I do that, the most efficient way? Are there any packages for
> that? I had a quick look but were only able to find packages which
> calculate statistical
> distribution properties after having the data.
>
> Thank you very much
>
> Matthias
>

The skewness and kurtosis of the normal distribution are fixed,
but there are many continuous univariate distributions defined
on the entire real line for which the skewness and kurtosis can be
varied.

One possible choice is the Pearson Type IV distribution.
This distribution has the nice feature that the skewness and
kurtosis can be easily formulated in terms of the distributional
parameters (and vice versa).  The Wikipedia entry of the Pearson
distributions is fairly informative:

http://en.wikipedia.org/wiki/Pearson_distribution

Joel Heinrich has written up a nice implementation guide:

http://www-cdf.fnal.gov/publications/cdf6820_pearson4.pdf

The translation into R is fairly straightforward.

There are many other options for distributions that allow for
arbitrary skewness and kurtosis, but relating the parameters of the
distribution to the skewness and kurtosis can be a challenge.
If you are willing to resort to numerical methods to determine
the skewness and kurtosis from the distributional parameters,
here are a few choices.

One easy option is the skewed-t distribution of F?rnandez and Steel.
See the "skewt" package by Robert King and Emily Anderson.  The
F?rnandez and Steel approach is elegant in that it provides a way to
transform any symmetric continuous distribution into a skewed distribution.
However, working out the exact skewness and kurtosis from the
parameter values can be a challenge.

As mentioned by John Frain, the stable distribution is a good
choice when the tails are especially heavy.  See John Nolan's
web site for a wealth of information:

http://academic2.american.edu/~jpnolan/stable/stable.html

For an R implementation, see Jim Lindsey's web page:

http://popgen.unimaas.nl/~jlindsey/rcode.html

(Also, although the stable distributions are skewed and
heavy-tailed, the traditional definitions of skewness and kurtosis
can't be applied to them, because the 2nd and higher moments
are not defined.)

On the other hand, if you are interested in a distribution that
has thinner tails than the normal, you might want to consider
the skew GED distribution.  See Diethelm Wuertz's fGarch
package:

http://www.rmetrics.org

This package also contains implementations of skew normal
and skew student-t distributions, again using F?rnandez and
Steel's approach.

The normal inverse Gaussian, and its cousin the generalized
hyperbolic distribution, has received a fair amount of recent
attention.  I believe an implementation can be found in the
"ghyp" package of Wolfgang Breymann and David Luethi.

This does not by any means exhaust the space of possibilities,
but it should at least give you a start.

BTW, here are a few R functions that will help you to explore
the skewness and kurtosis of arbitrary distributions:

# Calculate mean of an arbitrary density
Mean <- function(f, ...) { integrate(function (x) { f(x, ...) * x },
-Inf, Inf)$value }
# Calculate k-th central moment of an arbitrary density
M <- function (f, ..., k=1, xm = Mean(f, ...)) { integrate(function(x)
{(x - xm)^k*f(x,...)}, -Inf, Inf)$value }
# Calculate skewness of an arbitrary density
SK <- function(f, ...) { M(f, ..., k=3) / (M(f, ..., k=2)^1.5) }
# Calculate excess kurtosis of an arbitrary density
KU <- function(f, ...) { M(f, ..., k=4) / (M(f, ..., k=2)^2) - 3}

> SK(dnorm)
[1] 0   # normal distribution has skewness of 0
> KU(dnorm)
[1] 1.625367e-13  # good enough for government work

# Test gamma distribution with shape=1
> Mean(dgamma, 1)
[1] 1  # good
> SK(dgamma, 1)
[1] 2  # good
> KU(dgamma, 1)
[1] 6  # good

> library(skewt)
> KU(dskt, 5, 1)
[1] 6   # Agrees with theory ... skewness of t with 5 d.f. should be 6
> SK(dskt, 5, 1.5)
[1] 1.516366
> SK(dskt, 5, 1/1.5)
[1] -1.516366


-- 
Matt Clegg
matthewcleggphd at gmail.com


From Matthias.Koberstein at hsbctrinkaus.de  Wed Aug 27 08:51:26 2008
From: Matthias.Koberstein at hsbctrinkaus.de (Matthias.Koberstein at hsbctrinkaus.de)
Date: Wed, 27 Aug 2008 08:51:26 +0200
Subject: [R-SIG-Finance] Antwort: Re: Generating Distributions with set
 skewness and kurtosis
In-Reply-To: <e199b8e70808261749q20494c8bm2288098b84a474fe@mail.gmail.com>
Message-ID: <OFF9728F2E.7DD9D075-ONC12574B2.00253070-C12574B2.0025AA3E@hsbctrinkaus.de>

Hello,

thank you very much for your help on this topic. It is really appreciated.
There have been some great replies and I guess it will take me some time to
go through all the implementation options given by you guys.
Thanks again

Matthias

P.S.:       I was aware of the fact that the normal distribution has a
fixed skew and kurtosis (therefore I wrote "normal" ;))
      I just wanted to give an example what I am trying to do, I guess it
wasnt really descriptive :)



**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential and / or
privileged information. If you are not the intended recipient or have
received this e-mail in error, please notify the sender immediately and
destroy this e-mail . Any unauthorized copying, storing, disclosure or
distribution of the contents of this e-mail is strictly forbidden.

---------------------------------------------------------------------
HSBC Trinkaus & Burkhardt AG
Sitz: D?sseldorf, K?nigsallee 21/23, 40212 D?sseldorf, Handelsregister:
Amtsgericht D?sseldorf HRB 54447
Mitglieder des Vorstands: Andreas Schmitz (Sprecher), Paul Hagen, Dr. Olaf
Huth, Carola Gr?fin v. Schmettow
Vorsitzender des Aufsichtsrats: Dr. Sieghardt Rometsch


                                                                           
             "Matthew Clegg"                                               
             <matthewcleggphd@                                             
             gmail.com>                                                 An 
             Fax-Deckblatt:              Matthias.Koberstein at hsbctrinkaus. 
             HSBCTuB                     de                                
             27.08.2008 02:50                                        Kopie 
                                         r-sig-finance at stat.math.ethz.ch   
                                                                     Thema 
                                         Re: [R-SIG-Finance] Generating    
                                         Distributions with set skewness   
                                         and kurtosis                      
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




The skewness and kurtosis of the normal distribution are fixed,
but there are many continuous univariate distributions defined
on the entire real line for which the skewness and kurtosis can be
varied.

One possible choice is the Pearson Type IV distribution.
This distribution has the nice feature that the skewness and
kurtosis can be easily formulated in terms of the distributional
parameters (and vice versa).  The Wikipedia entry of the Pearson
distributions is fairly informative:

http://en.wikipedia.org/wiki/Pearson_distribution

Joel Heinrich has written up a nice implementation guide:

http://www-cdf.fnal.gov/publications/cdf6820_pearson4.pdf

The translation into R is fairly straightforward.

There are many other options for distributions that allow for
arbitrary skewness and kurtosis, but relating the parameters of the
distribution to the skewness and kurtosis can be a challenge.
If you are willing to resort to numerical methods to determine
the skewness and kurtosis from the distributional parameters,
here are a few choices.

One easy option is the skewed-t distribution of F?rnandez and Steel.
See the "skewt" package by Robert King and Emily Anderson.  The
F?rnandez and Steel approach is elegant in that it provides a way to
transform any symmetric continuous distribution into a skewed distribution.
However, working out the exact skewness and kurtosis from the
parameter values can be a challenge.

As mentioned by John Frain, the stable distribution is a good
choice when the tails are especially heavy.  See John Nolan's
web site for a wealth of information:

http://academic2.american.edu/~jpnolan/stable/stable.html

For an R implementation, see Jim Lindsey's web page:

http://popgen.unimaas.nl/~jlindsey/rcode.html

(Also, although the stable distributions are skewed and
heavy-tailed, the traditional definitions of skewness and kurtosis
can't be applied to them, because the 2nd and higher moments
are not defined.)

On the other hand, if you are interested in a distribution that
has thinner tails than the normal, you might want to consider
the skew GED distribution.  See Diethelm Wuertz's fGarch
package:

http://www.rmetrics.org

This package also contains implementations of skew normal
and skew student-t distributions, again using F?rnandez and
Steel's approach.

The normal inverse Gaussian, and its cousin the generalized
hyperbolic distribution, has received a fair amount of recent
attention.  I believe an implementation can be found in the
"ghyp" package of Wolfgang Breymann and David Luethi.

This does not by any means exhaust the space of possibilities,
but it should at least give you a start.

BTW, here are a few R functions that will help you to explore
the skewness and kurtosis of arbitrary distributions:

# Calculate mean of an arbitrary density
Mean <- function(f, ...) { integrate(function (x) { f(x, ...) * x },
-Inf, Inf)$value }
# Calculate k-th central moment of an arbitrary density
M <- function (f, ..., k=1, xm = Mean(f, ...)) { integrate(function(x)
{(x - xm)^k*f(x,...)}, -Inf, Inf)$value }
# Calculate skewness of an arbitrary density
SK <- function(f, ...) { M(f, ..., k=3) / (M(f, ..., k=2)^1.5) }
# Calculate excess kurtosis of an arbitrary density
KU <- function(f, ...) { M(f, ..., k=4) / (M(f, ..., k=2)^2) - 3}

> SK(dnorm)
[1] 0   # normal distribution has skewness of 0
> KU(dnorm)
[1] 1.625367e-13  # good enough for government work

# Test gamma distribution with shape=1
> Mean(dgamma, 1)
[1] 1  # good
> SK(dgamma, 1)
[1] 2  # good
> KU(dgamma, 1)
[1] 6  # good

> library(skewt)
> KU(dskt, 5, 1)
[1] 6   # Agrees with theory ... skewness of t with 5 d.f. should be 6
> SK(dskt, 5, 1.5)
[1] 1.516366
> SK(dskt, 5, 1/1.5)
[1] -1.516366


--
Matt Clegg
matthewcleggphd at gmail.com


From daler at uic.edu  Wed Aug 27 16:18:15 2008
From: daler at uic.edu (Dale W.R. Rosenthal)
Date: Wed, 27 Aug 2008 09:18:15 -0500
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 51, Issue 19
In-Reply-To: <mailman.1.1219831201.12245.r-sig-finance@stat.math.ethz.ch>
References: <mailman.1.1219831201.12245.r-sig-finance@stat.math.ethz.ch>
Message-ID: <48B56227.5010305@uic.edu>

Matthias --

You can try a few approaches.

1) You could find a four-parameter distribution.  Some of those have 
been mentioned; and, I'd add the Normal Inverse Gaussian as a 
suggestion.  Barndorff-Nielsen and Sorensen have both looked at the 
properties of this distribution as applied to finance.

2) Another option would be to go with an Edgeworth expansion -- very 
similar to a Taylor expansion of the characteristic function.  The 
advantage is that Edgeworth expansions are designed precisely for 
matching cumulants (almost the same as moments, up to the fourth).  The 
disadvantage is that Edgeworth expansions can yield "pdf"s which go 
negative for a short time.  However, if you have a specific case in 
mind; you could check for that.

3) You could also try something like an Edgeworth expansion of the 
log-density; then, exponentiate the result.  You will get a pdf 
approximation that is non-negative; but, these expansions can explode in 
the tails -- so they need even more careful handling.  They can also be 
tough to integrate.

The reference for the Edgeworth expansions is McCullagh's _Tensor 
Methods in Statistics_ or Kolassa's _Series Approximation Methods in 
Statistics, 3d Ed._.  McCullagh uses tensor notation -- which some 
people find hard to get used to; his book can also be tough to find 
outside of libraries.  Kolassa doesn't cover the log-density expansion; 
but, his development is free from tensor notation.

4) You could also try using a saddlepoint method with an approximate 
cumulant generating function.  Easton and Ronchetti (JASA, 1986) would 
be the reference if you want to try this method.  I haven't tried it 
myself; but, if it works for you, it would probably be a very nice 
approximation.


Hope that helps.

Dale

-- 
Dale W.R. Rosenthal
Assistant Professor, Department of Finance
University of Illinois at Chicago
http://tigger.uic.edu/~daler
SSRN: http://ssrn.com/author=906862 

> Message: 1
> Date: Tue, 26 Aug 2008 17:53:18 +0200
> From: Matthias.Koberstein at hsbctrinkaus.de
> Subject: [R-SIG-Finance] Generating Distributions with set skewness
> 	and	kurtosis
> To: r-sig-finance at stat.math.ethz.ch
>
>
> Hello,
>
> I am reaching out to you for help since I am struggeling to find a function
> to generate distributions with a set statistical properties as kurtosis and
> skewdness.
> Lets say I want to generate random variables following a "normal"
> distribution, but with skewness 2 and kurtosis 5.
> How would I do that, the most efficient way? Are there any packages for
> that? I had a quick look but were only able to find packages which
> calculate statistical
> distribution properties after having the data.
>
> Thank you very much
>
> Matthias
>
>


From zornitsa.luleva at gmail.com  Wed Aug 27 18:12:23 2008
From: zornitsa.luleva at gmail.com (Zornitsa Luleva)
Date: Wed, 27 Aug 2008 18:12:23 +0200
Subject: [R-SIG-Finance] Compound Poisson process
Message-ID: <e67ad1a20808270912s3ab277ebhc060ee088a1a3d40@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080827/f580fa38/attachment.pl>

From fgochez at mango-solutions.com  Wed Aug 27 18:24:31 2008
From: fgochez at mango-solutions.com (Francisco Gochez)
Date: Wed, 27 Aug 2008 17:24:31 +0100
Subject: [R-SIG-Finance] Announcement: BLCOP package
Message-ID: <3CBFCFB1FEFFA841BA83ADF2F2A9C6FA1EE1A3@mango-data1.Mango.local>

Hello R-Sig-Financers,

I uploaded a beta version (0.2.2) of a modest new package I call "BLCOP"
to CRAN last week.  It implements the Black-Litterman and (in my view
more interesting) copula opinion pooling frameworks.  Additional methods
such as entropy pooling may also be added in the future.  I hope that
some of you find it useful, and feedback and/or bug reports are always
appreciated.

Regards,

Francisco Gochez

Mango Solutions, Chippenham
+44 (0)1249 767700  


From finbref.2006 at gmail.com  Wed Aug 27 23:16:32 2008
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Wed, 27 Aug 2008 23:16:32 +0200
Subject: [R-SIG-Finance] Compound Poisson process
In-Reply-To: <e67ad1a20808270912s3ab277ebhc060ee088a1a3d40@mail.gmail.com>
References: <e67ad1a20808270912s3ab277ebhc060ee088a1a3d40@mail.gmail.com>
Message-ID: <d0f55a670808271416o259af0f4ta6f9b4e8166aa02e@mail.gmail.com>

Hi,

Your question is a math-question, not an R-Finance question. So don't
expect too many replies.

Your price process is the sum of a brownian motion and two jump
processes, right?
I'm not sure if the beta distribution is infinitely divisible (Pareto
is). This can put you in big trouble, you will not have stationary
increments anymore.
I'm not totally sure, but the Pareto will give you a simple jump
diffusion (finite activity), which can be sampled easily.

Thomas




2008/8/27 Zornitsa Luleva <zornitsa.luleva at gmail.com>:
> Dear all,
>
> i would like to implement two compound Poisson processes that simulate
> upwards and downwards jumps respectively. Thereby, the up jump magnitudes
> are Pareto distributed and the down jump magnitudes are Beta distributed.
> These jump processes as well as a Brownian motion are a part of a model
> describing security prices.
> My goal is to simulate the two processes (independently of each other) with
> known values of the parameters - the Poisson lambdas and the Beta / Pareto
> parameters. Afterwards, I want to take the simulated data and try to
> estimate the parameters using Maximum likelihood estimation (MLE) to see how
> well an estimation algorithm that I implemnted is working.
>
> My questions:
>
> 1) Is it right to simulate exponentially distributed waiting times between
> the jumps and then just "jump" with a Beta / Pareto distributed magnitude?
>
> 2) When I simulate the down jumps and take zero as a starting point, then I
> get negative values. The MLE does not like it, neither do I, because it
> means, that I simulate negative prices. Can I take another starting value
> for the process (for example one) ?
>
> I am very grateful for a concise answer.
>
> Cheers,
> Zoe
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From mkeller at fam.tuwien.ac.at  Thu Aug 28 10:53:18 2008
From: mkeller at fam.tuwien.ac.at (Martin Keller-Ressel)
Date: Thu, 28 Aug 2008 08:53:18 -0000
Subject: [R-SIG-Finance] Compound Poisson process
In-Reply-To: <e67ad1a20808270912s3ab277ebhc060ee088a1a3d40@mail.gmail.com>
References: <e67ad1a20808270912s3ab277ebhc060ee088a1a3d40@mail.gmail.com>
Message-ID: <op.ugk9m4vi0efqu7@neyman.fam.tuwien.ac.at>

Hi Zornitsa,

On Wed, 27 Aug 2008 16:12:23 -0000, Zornitsa Luleva  
<zornitsa.luleva at gmail.com> wrote:

> 2) When I simulate the down jumps and take zero as a starting point,  
> then I
> get negative values. The MLE does not like it, neither do I, because it
> means, that I simulate negative prices. Can I take another starting value
> for the process (for example one) ?

The usual approach here would be to model the log-price by a Brownian  
motion plus jumps. This process can of course become negative, but once  
you take the exponential you get a strictly positive price process. Your  
jumps then describe relative change in prices, not absolute change. Such  
models are called exponential Levy models, in 'Financial Modelling with  
jumps' by Rama Cont and Peter Tankov you will find a lot of examples and  
background on this.

best regards,
Martin Keller-Ressel


-- 
Martin Keller-Ressel
Research Unit of Financial and Actuarial Mathematics, TU Vienna
http://www.fam.tuwien.ac.at/~mkeller


From martin.becker at mx.uni-saarland.de  Thu Aug 28 11:06:52 2008
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Thu, 28 Aug 2008 11:06:52 +0200
Subject: [R-SIG-Finance] Compound Poisson process
In-Reply-To: <e67ad1a20808270912s3ab277ebhc060ee088a1a3d40@mail.gmail.com>
References: <e67ad1a20808270912s3ab277ebhc060ee088a1a3d40@mail.gmail.com>
Message-ID: <48B66AAC.2060900@mx.uni-saarland.de>

Dear Zornitsa,

Zornitsa Luleva wrote:
> 1) Is it right to simulate exponentially distributed waiting times between
> the jumps and then just "jump" with a Beta / Pareto distributed magnitude?
>   

to complement what Thomas and Martin already wrote:
Depending on what parts of the process are needed (e.g. final values 
only, discrete approximation of process path,...) different techniques 
(which are mentioned in the monograph by Cont/Tankov) may be favourable:

- final values only: sample the number of jumps from 
Poisson(lambda[i]*T) distribution (when T denotes final time) for Beta 
[1] and Pareto [2] jumps and sum up simulated jump magnitudes 
accordingly (should be especially efficient when implemented in R 
without using C calls). [If jump times are needed: they are distributed 
uniformly on (0,T).]

- discrete approximation: Beta and Pareto distributed jumps may be 
concentrated in one CP process:
  * draw exponential waiting times (rate = lambda[1]+lambda[2])
  * for each jump: with probability lambda[1]/(lambda[1]+lambda[2]) draw 
Beta jump; draw Pareto jump else.

Kind regards,

  Martin


-- 
Dr. Martin Becker
Saarland University
Statistics and Econometrics


From RPadayachee at investec.co.za  Thu Aug 28 11:14:58 2008
From: RPadayachee at investec.co.za (Ruvashen Padayachee)
Date: Thu, 28 Aug 2008 11:14:58 +0200
Subject: [R-SIG-Finance] Relative Date Question
Message-ID: <F2F0B4F4D798D1499DC8D3AFE36E1D4472FC4D@mbzajhb02.za.corp.investec.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080828/5b115684/attachment.pl>

From peter at braverock.com  Thu Aug 28 14:02:13 2008
From: peter at braverock.com (Peter Carl)
Date: Thu, 28 Aug 2008 07:02:13 -0500
Subject: [R-SIG-Finance] Relative Date Question
In-Reply-To: <F2F0B4F4D798D1499DC8D3AFE36E1D4472FC4D@mbzajhb02.za.corp.investec.com>
References: <F2F0B4F4D798D1499DC8D3AFE36E1D4472FC4D@mbzajhb02.za.corp.investec.com>
Message-ID: <200808280702.13489.peter@braverock.com>

On Thursday 28 August 2008 4:14:58 am Ruvashen Padayachee wrote:
> For example, how can I find the date one month after "2008/09/30" ?

The class 'Date' provides a 'seq.Date' function for creating date sequences.
> seq(as.Date("2008-09-01"),length.out=12, by="1 month")
> seq(ISOdate(2008,9,30), by = "month", length = 4)

Note that this second example, which attempts to create a sequence of 
end-of-month dates, doesn't work very well.  How to find last day of the 
month in a date sequence?  Unfortunately, the solution is so trivial that it 
might not be obvious.

> seq(as.Date("2008-9-01"),length.out=4, by="1 month")-1

HTH

pcc
-- 
Peter Carl
145 Scottswood Rd
Riverside, IL 60546
312 307 6346
http://www.braverock.com/~peter


From ggrothendieck at gmail.com  Thu Aug 28 15:00:03 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 28 Aug 2008 09:00:03 -0400
Subject: [R-SIG-Finance] Relative Date Question
In-Reply-To: <200808280702.13489.peter@braverock.com>
References: <F2F0B4F4D798D1499DC8D3AFE36E1D4472FC4D@mbzajhb02.za.corp.investec.com>
	<200808280702.13489.peter@braverock.com>
Message-ID: <971536df0808280600s455e2431i656053fdb48621ed@mail.gmail.com>

Here are a couple of possibilities.

library(zoo) # as.yearmon, as.Date.yearmon

# test data
x <- seq(as.Date("2000-02-01"), by = "month", length = 6)-1
x

# use same number of days past start of next month
ans1 <- as.Date(as.yearmon(x) + 1/12) + as.numeric(format(x, "%d")) - 1
ans1

# or if you don't want to go beyond the end of the next month:
pmin(ans1, as.Date(as.yearmon(x) + 2/12)-1)


On Thu, Aug 28, 2008 at 8:02 AM, Peter Carl <peter at braverock.com> wrote:
> On Thursday 28 August 2008 4:14:58 am Ruvashen Padayachee wrote:
>> For example, how can I find the date one month after "2008/09/30" ?
>
> The class 'Date' provides a 'seq.Date' function for creating date sequences.
>> seq(as.Date("2008-09-01"),length.out=12, by="1 month")
>> seq(ISOdate(2008,9,30), by = "month", length = 4)
>
> Note that this second example, which attempts to create a sequence of
> end-of-month dates, doesn't work very well.  How to find last day of the
> month in a date sequence?  Unfortunately, the solution is so trivial that it
> might not be obvious.
>
>> seq(as.Date("2008-9-01"),length.out=4, by="1 month")-1
>
> HTH
>
> pcc
> --
> Peter Carl
> 145 Scottswood Rd
> Riverside, IL 60546
> 312 307 6346
> http://www.braverock.com/~peter
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Thu Aug 28 16:05:46 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 28 Aug 2008 09:05:46 -0500
Subject: [R-SIG-Finance] Relative Date Question
In-Reply-To: <F2F0B4F4D798D1499DC8D3AFE36E1D4472FC4D@mbzajhb02.za.corp.investec.com>
References: <F2F0B4F4D798D1499DC8D3AFE36E1D4472FC4D@mbzajhb02.za.corp.investec.com>
Message-ID: <e8e755250808280705g60ec8fe2wac7c06197eab9a6a@mail.gmail.com>

using xts:

> library(xts)

> timeBasedSeq('2000-02/2001-01',"Date")-1
 [1] "2000-01-31" "2000-02-29" "2000-03-31" "2000-04-30" "2000-05-31"
 [6] "2000-06-30" "2000-07-31" "2000-08-31" "2000-09-30" "2000-10-31"
[11] "2000-11-30" "2000-12-31"

timeBasedSeq uses the maximal resolution you specify to build the
sequence, so by including only the CCYY-MM you get months.

The trick is that months always start on the first, and are always
preceded by the last of the previous month.

?timeBasedSeq
?endpoints

also:

> Sys.setenv(TZ='GMT')   # watch for TZ issues...
> as.Date(lastof(2000,1:12))
 [1] "2000-01-31" "2000-02-28" "2000-03-31" "2000-04-30" "2000-05-31"
 [6] "2000-06-30" "2000-07-31" "2000-08-31" "2000-09-30" "2000-10-31"
[11] "2000-11-30" "2000-12-31"

> as.Date(lastof(rep(2000:2001,each=12),1:12))
 [1] "2000-01-31" "2000-02-28" "2000-03-31" "2000-04-30" "2000-05-31"
 [6] "2000-06-30" "2000-07-31" "2000-08-31" "2000-09-30" "2000-10-31"
[11] "2000-11-30" "2000-12-31" "2001-01-31" "2001-02-28" "2001-03-31"
[16] "2001-04-30" "2001-05-31" "2001-06-30" "2001-07-31" "2001-08-31"
[21] "2001-09-30" "2001-10-31" "2001-11-30" "2001-12-31"


HTH,
Jeff

On Thu, Aug 28, 2008 at 4:14 AM, Ruvashen Padayachee
<RPadayachee at investec.co.za> wrote:
> Hi there. I have a quick query. What is the best way in R to find out a
> date relative to another date?
>
> For example, how can I find the date one month after "2008/09/30" ?
>
> If there is a method, would it take into account different days in the
> month?
>
> Thank you for your help.
>
>
>
> http://www.investec.com/EmailDisclaimer/emaildisclaimer.htm
>
> The disclaimer also provides our corporate information and names of our directors as required by law.
>
> The disclaimer is deemed to form part of this message in terms of Section 11 of the Electronic Communications and Transactions Act 25 of 2002.
> If you cannot access the disclaimer, please obtain a copy thereof from us by sending an email to: disclaimer at investec.co.za
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From atp at piskorski.com  Thu Aug 28 16:24:10 2008
From: atp at piskorski.com (Andrew Piskorski)
Date: Thu, 28 Aug 2008 10:24:10 -0400
Subject: [R-SIG-Finance] Relative Date Question
In-Reply-To: <F2F0B4F4D798D1499DC8D3AFE36E1D4472FC4D@mbzajhb02.za.corp.investec.com>
References: <F2F0B4F4D798D1499DC8D3AFE36E1D4472FC4D@mbzajhb02.za.corp.investec.com>
Message-ID: <20080828142410.GA81757@piskorski.com>

On Thu, Aug 28, 2008 at 11:14:58AM +0200, Ruvashen Padayachee wrote:
> Hi there. I have a quick query. What is the best way in R to find out a
> date relative to another date? 
>  
> For example, how can I find the date one month after "2008/09/30" ? 

One way is to simply fork off a small Tcl script, as Tcl's 'clock'
command includes pretty good support for relative date calculations,
getting the last day of the month, etc.  E.g.:

  my.next.month <- function(month) {
     # Given a single date anywhere in a month, get the next month date.
     # We assume here that 'month' is a string in YYYY-MM-DD format.
     tcl.code <-
        paste('set month_d [clock scan {'  ,month  ,'}]'
              ,'\n'  ,'puts [clock format [clock scan {+ 1 month} -base $month_d] -format {%Y-%m-%d}]'
              ,sep="")
     system("tclsh" ,input=tcl.code ,intern=T)
  }

  > my.next.month("2008-09-30")
  [1] "2008-10-30"
  > my.next.month("2008-01-29")
  [1] "2008-02-29"
  > my.next.month("2007-01-29")
  [1] "2007-02-28"

Instead of forking a script with system(), you could also probably use
.Tcl(), which gives you a single persistent in-processs Tcl
interpreter, and would presumably be more efficient.  E.g.:

  > capabilities("tcltk")
  tcltk
   TRUE
  > require(tcltk)
  Loading required package: tcltk
  Loading Tcl/Tk interface ... done
  > .Tcl("set x 1") ; .Tcl("incr x") ; .Tcl("incr x")
  <Tcl> 1
  <Tcl> 2
  <Tcl> 3

-- 
Andrew Piskorski <atp at piskorski.com>
http://www.piskorski.com/


From rory.winston at gmail.com  Thu Aug 28 23:07:41 2008
From: rory.winston at gmail.com (Rory Winston)
Date: Thu, 28 Aug 2008 22:07:41 +0100
Subject: [R-SIG-Finance] R for Individual Stock Trading Analysis
Message-ID: <3f446aa30808281407y24e5c3edr6eff0c240cb03ec4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080828/2e9ef176/attachment.pl>

From kriskumar at earthlink.net  Fri Aug 29 05:17:18 2008
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Thu, 28 Aug 2008 23:17:18 -0400
Subject: [R-SIG-Finance] Generating Distributions with set skewness and
 kurtosis
In-Reply-To: <e199b8e70808261749q20494c8bm2288098b84a474fe@mail.gmail.com>
References: <OF62C4844B.17259C3C-ONC12574B1.0056CD0F-C12574B1.00574664@hsbctrinkaus.de>
	<e199b8e70808261749q20494c8bm2288098b84a474fe@mail.gmail.com>
Message-ID: <48B76A3E.1000202@earthlink.net>

Just to add to this the Johnson Family is simillar to the pearson family 
and is obtained by a simple transformation of the normal distribution.

The R package SuppDists has great support for the Johnson Distribution. 
In particular if you have a specific skew and kurtosis
 >require(SuppDists)
 >parms<-JohnsonFit(c(0,1,-0.2,4),moment="use")
 >hist(rJohnson(100,parms)

where the 0,1,-0.2,3  are respectively the  first four central moments 
of the distribution (mean ,variance etc)
The hist function above then shows you the skew in the resulting 
distribution.

Hope this helps,

Thanks
Kris






Matthew Clegg wrote:
> On Tue, Aug 26, 2008 at 11:53 AM,  <Matthias.Koberstein at hsbctrinkaus.de> wrote:
>   
>> Hello,
>>
>> I am reaching out to you for help since I am struggeling to find a function
>> to generate distributions with a set statistical properties as kurtosis and
>> skewdness.
>> Lets say I want to generate random variables following a "normal"
>> distribution, but with skewness 2 and kurtosis 5.
>> How would I do that, the most efficient way? Are there any packages for
>> that? I had a quick look but were only able to find packages which
>> calculate statistical
>> distribution properties after having the data.
>>
>> Thank you very much
>>
>> Matthias
>>
>>     
>
> The skewness and kurtosis of the normal distribution are fixed,
> but there are many continuous univariate distributions defined
> on the entire real line for which the skewness and kurtosis can be
> varied.
>
> One possible choice is the Pearson Type IV distribution.
> This distribution has the nice feature that the skewness and
> kurtosis can be easily formulated in terms of the distributional
> parameters (and vice versa).  The Wikipedia entry of the Pearson
> distributions is fairly informative:
>
> http://en.wikipedia.org/wiki/Pearson_distribution
>
> Joel Heinrich has written up a nice implementation guide:
>
> http://www-cdf.fnal.gov/publications/cdf6820_pearson4.pdf
>
> The translation into R is fairly straightforward.
>
> There are many other options for distributions that allow for
> arbitrary skewness and kurtosis, but relating the parameters of the
> distribution to the skewness and kurtosis can be a challenge.
> If you are willing to resort to numerical methods to determine
> the skewness and kurtosis from the distributional parameters,
> here are a few choices.
>
> One easy option is the skewed-t distribution of F?rnandez and Steel.
> See the "skewt" package by Robert King and Emily Anderson.  The
> F?rnandez and Steel approach is elegant in that it provides a way to
> transform any symmetric continuous distribution into a skewed distribution.
> However, working out the exact skewness and kurtosis from the
> parameter values can be a challenge.
>
> As mentioned by John Frain, the stable distribution is a good
> choice when the tails are especially heavy.  See John Nolan's
> web site for a wealth of information:
>
> http://academic2.american.edu/~jpnolan/stable/stable.html
>
> For an R implementation, see Jim Lindsey's web page:
>
> http://popgen.unimaas.nl/~jlindsey/rcode.html
>
> (Also, although the stable distributions are skewed and
> heavy-tailed, the traditional definitions of skewness and kurtosis
> can't be applied to them, because the 2nd and higher moments
> are not defined.)
>
> On the other hand, if you are interested in a distribution that
> has thinner tails than the normal, you might want to consider
> the skew GED distribution.  See Diethelm Wuertz's fGarch
> package:
>
> http://www.rmetrics.org
>
> This package also contains implementations of skew normal
> and skew student-t distributions, again using F?rnandez and
> Steel's approach.
>
> The normal inverse Gaussian, and its cousin the generalized
> hyperbolic distribution, has received a fair amount of recent
> attention.  I believe an implementation can be found in the
> "ghyp" package of Wolfgang Breymann and David Luethi.
>
> This does not by any means exhaust the space of possibilities,
> but it should at least give you a start.
>
> BTW, here are a few R functions that will help you to explore
> the skewness and kurtosis of arbitrary distributions:
>
> # Calculate mean of an arbitrary density
> Mean <- function(f, ...) { integrate(function (x) { f(x, ...) * x },
> -Inf, Inf)$value }
> # Calculate k-th central moment of an arbitrary density
> M <- function (f, ..., k=1, xm = Mean(f, ...)) { integrate(function(x)
> {(x - xm)^k*f(x,...)}, -Inf, Inf)$value }
> # Calculate skewness of an arbitrary density
> SK <- function(f, ...) { M(f, ..., k=3) / (M(f, ..., k=2)^1.5) }
> # Calculate excess kurtosis of an arbitrary density
> KU <- function(f, ...) { M(f, ..., k=4) / (M(f, ..., k=2)^2) - 3}
>
>   
>> SK(dnorm)
>>     
> [1] 0   # normal distribution has skewness of 0
>   
>> KU(dnorm)
>>     
> [1] 1.625367e-13  # good enough for government work
>
> # Test gamma distribution with shape=1
>   
>> Mean(dgamma, 1)
>>     
> [1] 1  # good
>   
>> SK(dgamma, 1)
>>     
> [1] 2  # good
>   
>> KU(dgamma, 1)
>>     
> [1] 6  # good
>
>   
>> library(skewt)
>> KU(dskt, 5, 1)
>>     
> [1] 6   # Agrees with theory ... skewness of t with 5 d.f. should be 6
>   
>> SK(dskt, 5, 1.5)
>>     
> [1] 1.516366
>   
>> SK(dskt, 5, 1/1.5)
>>     
> [1] -1.516366
>
>
>


From kriskumar at earthlink.net  Fri Aug 29 05:25:54 2008
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Thu, 28 Aug 2008 23:25:54 -0400
Subject: [R-SIG-Finance] Compound Poisson process
In-Reply-To: <e67ad1a20808270912s3ab277ebhc060ee088a1a3d40@mail.gmail.com>
References: <e67ad1a20808270912s3ab277ebhc060ee088a1a3d40@mail.gmail.com>
Message-ID: <48B76C42.2040703@earthlink.net>

In my little research on this calibrating a symmetric merton type jump 
process (lognormal jump size with poissson jumps) is in itself a little 
difficult.
The likelihood surface tends to have multiple maxima making it a 
stretch. Is there a reason for the assymetric jumps?. {in reality i can 
see why but the calibration is tricky}


Zornitsa Luleva wrote:
> Dear all,
>
> i would like to implement two compound Poisson processes that simulate
> upwards and downwards jumps respectively. Thereby, the up jump magnitudes
> are Pareto distributed and the down jump magnitudes are Beta distributed.
> These jump processes as well as a Brownian motion are a part of a model
> describing security prices.
> My goal is to simulate the two processes (independently of each other) with
> known values of the parameters - the Poisson lambdas and the Beta / Pareto
> parameters. Afterwards, I want to take the simulated data and try to
> estimate the parameters using Maximum likelihood estimation (MLE) to see how
> well an estimation algorithm that I implemnted is working.
>
> My questions:
>
> 1) Is it right to simulate exponentially distributed waiting times between
> the jumps and then just "jump" with a Beta / Pareto distributed magnitude?
>
> 2) When I simulate the down jumps and take zero as a starting point, then I
> get negative values. The MLE does not like it, neither do I, because it
> means, that I simulate negative prices. Can I take another starting value
> for the process (for example one) ?
>
> I am very grateful for a concise answer.
>
> Cheers,
> Zoe
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From ezivot at u.washington.edu  Sat Aug 30 03:04:41 2008
From: ezivot at u.washington.edu (Eric Zivot)
Date: Fri, 29 Aug 2008 18:04:41 -0700 (PDT)
Subject: [R-SIG-Finance] R for Individual Stock Trading Analysis
In-Reply-To: <3f446aa30808281407y24e5c3edr6eff0c240cb03ec4@mail.gmail.com>
Message-ID: <Pine.LNX.4.43.0808291804410.4158@hymn34.u.washington.edu>

Just to let you know, the second edition of Statistics and Finance: An Introductdion is under works and I will be joining David Ruppert as a co-author. All of the examples in the book will now be done in R.
ez

****************************************************************
*  Eric Zivot                  			               *
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *                                                           *
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Thu, 28 Aug 2008, Rory Winston wrote:

>> <snip>
>> If you're looking for a good book on quantitative finance that covers a
>> lot of the theoretical underpinnings without getting lost in the math of
>> the advanced techniques, I recommend picking up a copy of
>>
>> Statistics and Finance: An Introduction
>> by David Ruppert
>>
>> http://www.amazon.com/Statistics-Finance-Introduction-David-Ruppert/dp/0387202706
>>
>> R code for all of his examples is available online.
>> </snip>
>
>
>
> Great to hear someone else likes this book! It's one of my favourites - very
> clear and with a nice statistician slant.
>
> Rory
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From smaydan at scottwoodcapital.com  Fri Jul 18 17:18:55 2008
From: smaydan at scottwoodcapital.com (Stan Maydan)
Date: Fri, 18 Jul 2008 15:18:55 -0000
Subject: [R-SIG-Finance] Is there a way to overcome 2 gigabytes data set
	limit in R?
Message-ID: <893C528B72100B4DA79A0129B58D4D3D012DC76A@SCWOOD02.corp.scottwoodcapital.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080718/4b2e82d5/attachment.pl>

From pierre8r-gmane at yahoo.fr  Wed Jul 16 15:40:19 2008
From: pierre8r-gmane at yahoo.fr (pierre8r-gmane at yahoo.fr)
Date: Wed, 16 Jul 2008 13:40:19 -0000
Subject: [R-SIG-Finance] How to format a CSV file output  ?
Message-ID: <754979.77160.qm@web28105.mail.ukl.yahoo.com>

Hello,

Here my R code :

library(xts)
library(quantmod)

quotes <- read.csv2("E:\\00001-Compare\\Output\\OutputJBacktesting\\InputIndic.txt",
header = FALSE, sep = ",", dec=".")

x <- xts(as.matrix(quotes[,-1]),as.POSIXct(paste(quotes[,1]),format='%m/%d/%Y
%H:%M'))
colnames(x) <- c('Indic')
x

write.csv(x, file = "OutputIndic.csv", quote = FALSE, row.names = FALSE)
                                   
The x give this output to the R Console :

> colnames(x) <- c('Indic')
> x
                      Indic
2007-01-08 00:59:00 1.93025
2007-01-08 01:59:00 1.92960
2007-01-08 02:59:00 1.92805


My goal is to generate this kind of CSV output :

01/08/2007,00:59, 1.93025 
01/08/2007,01:59, 1.92955 
01/08/2007,02:59, 1.92885 

My write.csv generate this CSV output :

Indic
1.93025
1.9296
1.92805

How to generate this kind of output ?

01/08/2007,00:59, 1.93025 
01/08/2007,01:59, 1.92955 

Thanks,

Pierre8r


      ____________________________________________________
intelligente http://mail.yahoo.fr


From Yunlei.Hu at barclayscapital.com  Mon Jul 21 17:46:56 2008
From: Yunlei.Hu at barclayscapital.com (Yunlei.Hu at barclayscapital.com)
Date: Mon, 21 Jul 2008 15:46:56 -0000
Subject: [R-SIG-Finance] Urgent on the help
Message-ID: <B9AEC8265E202D4BA6540EA32F49579004771D1B@LDNPCMEU301VEUA.INTRANET.BARCAPINT.COM>

Dear all
I have a optimization problem as follows. And would appreaciated if
someone can give me the reply soon.
 I aim to optimize the portfolio in considering the transaction cost.
Hence the objective function is: 
Min: 1/2 w^T* omega*w-mu^T*w-c^T*(w-w0) when w[i]>w0[i] 
       1/2 w^T* omega*w-mu^T*w+c^T*(w0-w) when w[i]<w0[i] 

Where w is the update weight vector of the portfolio
omiga is the variance-covariance matrix 
mu is the vector of the return rate 
w0 is the initial vector weight 
C is the coefficient vector of transaction cost

 It is in a bit of emergency. I would be really appreciated if anybody
can give me the reply ASAP. 

Many thanks
 Yunlei 

_______________________________________________

This e-mail may contain information that is confidential, privileged or otherwise protected from disclosure. If you are not an intended recipient of this e-mail, do not duplicate or redistribute it by any means. Please delete it and any attachments and notify the sender that you have received it in error. Unless specifically indicated, this e-mail is not an offer to buy or sell or a solicitation to buy or sell any securities, investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Barclays. Any views or opinions presented are solely those of the author and do not necessarily represent those of Barclays. This e-mail is subject to terms available at the following link: www.barcap.com/emaildisclaimer. By messaging with Barclays you consent to the foregoing.  Barclays Capital is the investment banking division of Barclays Bank PLC, a company registered in England (number 1026167) with its registered office at 1 Churchill Place, London, E14 5HP.  This email may relate to or be sent from other members of the Barclays Group.


From Yunlei.Hu at barclayscapital.com  Tue Jul 22 10:17:26 2008
From: Yunlei.Hu at barclayscapital.com (Yunlei.Hu at barclayscapital.com)
Date: Tue, 22 Jul 2008 08:17:26 -0000
Subject: [R-SIG-Finance] Urgent on the help
Message-ID: <B9AEC8265E202D4BA6540EA32F49579004771D2C@LDNPCMEU301VEUA.INTRANET.BARCAPINT.COM>


 Dear all

I am using quadratic programming to solve the portfolio optimization in
cosidering transaction cost. Is there any R optimization package can do
this? 

Solve.QP require the positive definite matrix in Dmat, while in my case,
this matrix in the objective function is not positive definite.

>  It is in a bit of emergency. I would be really appreciated if anybody
> can give me the reply ASAP. 
> 
> Many thanks
>  Yunlei 
> 
_______________________________________________

This e-mail may contain information that is confidential, privileged or otherwise protected from disclosure. If you are not an intended recipient of this e-mail, do not duplicate or redistribute it by any means. Please delete it and any attachments and notify the sender that you have received it in error. Unless specifically indicated, this e-mail is not an offer to buy or sell or a solicitation to buy or sell any securities, investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Barclays. Any views or opinions presented are solely those of the author and do not necessarily represent those of Barclays. This e-mail is subject to terms available at the following link: www.barcap.com/emaildisclaimer. By messaging with Barclays you consent to the foregoing.  Barclays Capital is the investment banking division of Barclays Bank PLC, a company registered in England (number 1026167) with its registered office at 1 Churchill Place, London, E14 5HP.  This email may relate to or be sent from other members of the Barclays Group.


From kafka at centras.lt  Wed Jul 23 21:03:44 2008
From: kafka at centras.lt (kafkaz)
Date: Wed, 23 Jul 2008 19:03:44 -0000
Subject: [R-SIG-Finance] [R-sig-finance] xts and barChart (quantmod)
Message-ID: <18617785.post@talk.nabble.com>


My data is stored in a MySQL database. So, I'm using RMySQL to load
information about price performance. After that, I'm using xts to convert
selected data into XTS object. Example:

rs <- dbSendQuery(con, "select ....");
dji<- fetch(rs, n = -1) 
dji2<-xts(dji,order.by=as.POSIXct(dji[,1]))
colnames(dji2)<-c('Datetime', 'Open', 'High', 'Low', 'Close', 'Volume')
At this point I have a xts object:

class(dji2)
[1] "xts" "zoo"

 >str(dji2)
An ?xts? object from 2004-01-07 15:50:00 to 2004-01-09 11:45:00 containing:
  Data: chr [1:50, 1:6] "2004-01-07 15:50:00" "2004-01-07 15:55:00"
"2004-01-07 16:00:00" "2004-01-07 16:05:00" ...
 - attr(*, "dimnames")=List of 2
  ..$ : chr [1:50] "2004-01-07 15:50:00" "2004-01-07 15:55:00" "2004-01-07
16:00:00" "2004-01-07 16:05:00" ...
  ..$ : chr [1:6] "Datetime" "Open" "High" "Low" ...
  Indexed by:  POSIXct[1:50], format: "2004-01-07 15:50:00" "2004-01-07
15:55:00" "2004-01-07 16:00:00" "2004-01-07 16:05:00" ...
  xts Attributes:  
 NULL

dji2[1:4,]
                    Datetime            Open  High  Low   Close Volume
2004-01-07 15:50:00 2004-01-07 15:50:00 10483 10483 10480 10480   0   
2004-01-07 15:55:00 2004-01-07 15:55:00 10479 10479 10465 10465   0   
2004-01-07 16:00:00 2004-01-07 16:00:00 10464 10464 10460 10460   0   
2004-01-07 16:05:00 2004-01-07 16:05:00 10459 10459 10442 10442   0 

If I try to use barChart function (quantmod package), I will get this error:
Error in checkSlotAssignment(object, name, value) : 
  assignment of an object of class "character" is not valid for slot
"yrange" in an object of class "chob"; is(value, "numeric") is not TRUE

The problem lies in the type of columns - open, high, low and close have
type character, but quantmod function expects integer:

if (is.OHLC(x)) {
        chob at yrange <- c(min(Lo(x), na.rm = TRUE), max(Hi(x), 
            na.rm = TRUE))
    }
    else chob at yrange <- range(x[, 1], na.rm = TRUE)

is.character(dji2[,5])
[1] TRUE

Any idea how to fix it?
Thank you.
-- 
View this message in context: http://www.nabble.com/xts-and-barChart-%28quantmod%29-tp18617785p18617785.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From tolga.i.uzuner at jpmorgan.com  Mon Aug  4 22:08:37 2008
From: tolga.i.uzuner at jpmorgan.com (tolga.i.uzuner at jpmorgan.com)
Date: Mon, 04 Aug 2008 20:08:37 -0000
Subject: [R-SIG-Finance] [R] Long Range Dependence: Hurst exponent
	estimation
In-Reply-To: <07FF9DD79EF8DD4C876FEC3DE0B6AF1D7DF343@MLNYC20MB057.amrs.win.ml.com>
Message-ID: <OF23359B5D.FE7DB3D9-ON8025749B.006E0EF7-8025749B.006E83B5@jpmchase.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080804/3362afb0/attachment.pl>

From picnic101 at gmail.com  Tue Aug 12 19:19:29 2008
From: picnic101 at gmail.com (swkim)
Date: Tue, 12 Aug 2008 17:19:29 -0000
Subject: [R-SIG-Finance] [R-sig-finance] error message from fPortfolio
	(270.74)
Message-ID: <18948789.post@talk.nabble.com>


Hi, R-users.

The example codes in the file "runit.solveRdonlp2.R" of the unitTests
directory was run 
using fPortfolio (270.74) on R 2.7.1. But I got the following error message.

>         # Default Constraints:
>         constraints = "LongOnly"
>         print(constraints)
[1] "LongOnly"
>      
>         # Optimization:
>         portfolio = solveRdonlp2(data, spec, constraints)
Loading required package: Cdonlp2

Rdonlp2 Package missing
Please download package from Rmetrics Server
Error in .Call("call_donlp2", as.double(par), as.integer(num.lin),
as.integer(num.nlin),  : 
  C symbol name "call_donlp2" not in DLL for package "Cdonlp2"
In addition: Warning message:
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return
= TRUE,  :
  there is no package called 'Cdonlp2'
Error in .Call("teardown", 0, PACKAGE = "Cdonlp2") : 
  C symbol name "teardown" not in DLL for package "Cdonlp2"

I know Rdonlp2 Package is on the search list. 
R is complaining that the package "Cdonlp2" is missing. Where is it?
Any help would be appreciated.

Sangwhan Kim
Chungbuk National University, South Korea

-- 
View this message in context: http://www.nabble.com/error-message-from-fPortfolio-%28270.74%29-tp18948789p18948789.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From collonil at gmx.de  Tue Aug 19 10:25:26 2008
From: collonil at gmx.de (Jule M)
Date: Tue, 19 Aug 2008 10:25:26 +0200
Subject: [R-SIG-Finance] (no subject)
Message-ID: <20080819082526.212540@gmx.net>


Hello,

somebody from nabble send me here. May be you can help me:

i want to fit a GARCH model with a extern regressor (without arma
components), so i found the following function in package fGarch. I tryed
out a lot of things but usually I get this Error.

> garchFit(formula=y~x, formula.var=~garch(1,1),data=w)
Error in .garchFit(formula.mean, formula.var, series = x, init.rec, delta,
:
  Algorithm only supported for mci Recursion

I think i use the function the wrong way, but cannot find my mistake. Can
somebody help me?

Thanks a lot, Collonil
-- 
View this message in context:
http://www.nabble.com/another-GARCH-problem-tp19033057p19033057.html
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
--


From balta.nunes at gmail.com  Wed Aug 20 01:03:09 2008
From: balta.nunes at gmail.com (Baltazar Nunes)
Date: Tue, 19 Aug 2008 16:03:09 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] AutocorTest and Missing values
Message-ID: <19060340.post@talk.nabble.com>


Hi,
My question is about AutocorTest() function of FinTS package.
Does this function handle missing values (NA values in the time series)? And
if so can anyone help clarify how it's done?
Many thanks for your attention.
Best regards,

Baltazar Nunes
Departamento de Epidemiologia
Instituto Nacional de Sa?de Dr. Ricardo Jorge
Lisboa, Portugal
-- 
View this message in context: http://www.nabble.com/AutocorTest-and-Missing-values-tp19060340p19060340.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From markleeds at verizon.net  Mon Sep  1 00:14:17 2008
From: markleeds at verizon.net (Mark Leeds)
Date: Sun, 31 Aug 2008 18:14:17 -0400
Subject: [R-SIG-Finance] Urgent on the help
In-Reply-To: <B9AEC8265E202D4BA6540EA32F49579004771D2C@LDNPCMEU301VEUA.INTRANET.BARCAPINT.COM>
Message-ID: <000001c90bb6$e998fa00$6600a8c0@coresystem>

A few months back, someone mentioned  a paper on the internet that showed
how to modify the QP to handle transaction costs. I have the paper at work
but I don't remember the name of it and I haven't read it. If you check the
archives, I imagine the old thread must be there and the link also.





-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
Yunlei.Hu at barclayscapital.com
Sent: Tuesday, July 22, 2008 4:17 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Urgent on the help


 Dear all

I am using quadratic programming to solve the portfolio optimization in
cosidering transaction cost. Is there any R optimization package can do
this? 

Solve.QP require the positive definite matrix in Dmat, while in my case,
this matrix in the objective function is not positive definite.

>  It is in a bit of emergency. I would be really appreciated if anybody
> can give me the reply ASAP. 
> 
> Many thanks
>  Yunlei 
> 
_______________________________________________

This e-mail may contain information that is confidential, privileged or
otherwise protected from disclosure. If you are not an intended recipient of
this e-mail, do not duplicate or redistribute it by any means. Please delete
it and any attachments and notify the sender that you have received it in
error. Unless specifically indicated, this e-mail is not an offer to buy or
sell or a solicitation to buy or sell any securities, investment products or
other financial product or service, an official confirmation of any
transaction, or an official statement of Barclays. Any views or opinions
presented are solely those of the author and do not necessarily represent
those of Barclays. This e-mail is subject to terms available at the
following link: www.barcap.com/emaildisclaimer. By messaging with Barclays
you consent to the foregoing.  Barclays Capital is the investment banking
division of Barclays Bank PLC, a company registered in England (number
1026167) with its registered offi!
 ce at 1 Churchill Place, London, E14 5HP.  This email may relate to or be
sent from other members of the Barclays Group.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From dutt.debashis at gmail.com  Mon Sep  1 00:38:49 2008
From: dutt.debashis at gmail.com (Debashis Dutta)
Date: Mon, 1 Sep 2008 01:38:49 +0300
Subject: [R-SIG-Finance] Urgent on the help
In-Reply-To: <37673c2d0808311533t46b34da5s5e33d1c4da097015@mail.gmail.com>
References: <B9AEC8265E202D4BA6540EA32F49579004771D2C@LDNPCMEU301VEUA.INTRANET.BARCAPINT.COM>
	<37673c2d0808311533t46b34da5s5e33d1c4da097015@mail.gmail.com>
Message-ID: <37673c2d0808311538j12f7a26fj44d6352636191a5a@mail.gmail.com>

Hi Yuilei,

For immediate reference, I enclose one papers. See if  this comes to your
use. give me a little time to run through the codes.

Kind Regards,
Debashis


On 01/09/2008, Debashis Dutta <dutt.debashis at gmail.com> wrote:
>
> Hi Yuilei,
>
> For immediate reference, I enclose two papers. See if  these come to your
> use. give me a little time to run through the codes.
>
> Kind Regards,
> Debashis
>
>
>  On 22/07/2008, Yunlei.Hu at barclayscapital.com <
> Yunlei.Hu at barclayscapital.com> wrote:
>>
>>
>> Dear all
>>
>> I am using quadratic programming to solve the portfolio optimization in
>> cosidering transaction cost. Is there any R optimization package can do
>> this?
>>
>> Solve.QP require the positive definite matrix in Dmat, while in my case,
>> this matrix in the objective function is not positive definite.
>>
>> >  It is in a bit of emergency. I would be really appreciated if anybody
>> > can give me the reply ASAP.
>> >
>> > Many thanks
>> >  Yunlei
>> >
>> _______________________________________________
>>
>> This e-mail may contain information that is confidential, privileged or
>> otherwise protected from disclosure. If you are not an intended recipient of
>> this e-mail, do not duplicate or redistribute it by any means. Please delete
>> it and any attachments and notify the sender that you have received it in
>> error. Unless specifically indicated, this e-mail is not an offer to buy or
>> sell or a solicitation to buy or sell any securities, investment products or
>> other financial product or service, an official confirmation of any
>> transaction, or an official statement of Barclays. Any views or opinions
>> presented are solely those of the author and do not necessarily represent
>> those of Barclays. This e-mail is subject to terms available at the
>> following link: www.barcap.com/emaildisclaimer. By messaging with
>> Barclays you consent to the foregoing.  Barclays Capital is the investment
>> banking division of Barclays Bank PLC, a company registered in England
>> (number 1026167) with its registered offi!
>> ce at 1 Churchill Place, London, E14 5HP.  This email may relate to or be
>> sent from other members of the Barclays Group.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080901/c40ff5d6/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: paper_nag.pdf
Type: application/pdf
Size: 192447 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080901/c40ff5d6/attachment.pdf>

From dutt.debashis at gmail.com  Mon Sep  1 01:15:37 2008
From: dutt.debashis at gmail.com (Debashis Dutta)
Date: Mon, 1 Sep 2008 02:15:37 +0300
Subject: [R-SIG-Finance] Urgent on the help
In-Reply-To: <B9AEC8265E202D4BA6540EA32F49579004771D2C@LDNPCMEU301VEUA.INTRANET.BARCAPINT.COM>
References: <B9AEC8265E202D4BA6540EA32F49579004771D2C@LDNPCMEU301VEUA.INTRANET.BARCAPINT.COM>
Message-ID: <37673c2d0808311615y768b82b9r762af6041d8628d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080901/637ba8c6/attachment.pl>

From kriskumar at earthlink.net  Mon Sep  1 02:46:32 2008
From: kriskumar at earthlink.net (kriskumar at earthlink.net)
Date: Mon, 1 Sep 2008 00:46:32 +0000
Subject: [R-SIG-Finance] Urgent on the help
In-Reply-To: <B9AEC8265E202D4BA6540EA32F49579004771D2C@LDNPCMEU301VEUA.INTRANET.BARCAPINT.COM>
References: <B9AEC8265E202D4BA6540EA32F49579004771D2C@LDNPCMEU301VEUA.INTRANET.BARCAPINT.COM>
Message-ID: <85734274-1220230010-cardhu_decombobulator_blackberry.rim.net-545929391-@bxe190.bisx.prod.on.blackberry>

Hi Yunlei,

There are many recipes to find the nearest non-singular matrix. Peter Jackel has a nice paper outlining three popular approaches to this. 

The R package corpcor has an implementation of Higham's approach.

>require(corpcor)
>posDefmat<-make.positive.definite(mat,tol)

Should do the trick now it  would be a good idea to compares this to Rebonato's method, which  is essentially setting the negative or close to zero eigen values to a small positive number (1e-5) and rescale to get the non-singular matrix back. This is a good practice  just to compare these things and see what magic dust(:-)). higham's procedure adds to the original matrix.

Hth

Best
Krishna
----
"When I get a little money, I buy books and if any 
      is left, I buy food and clothes."  -- Erasmus

-----Original Message-----
From: <Yunlei.Hu at barclayscapital.com>

Date: Tue, 22 Jul 2008 09:17:11 
To: <r-sig-finance at stat.math.ethz.ch>
Subject: [R-SIG-Finance] Urgent on the help



 Dear all

I am using quadratic programming to solve the portfolio optimization in
cosidering transaction cost. Is there any R optimization package can do
this? 

Solve.QP require the positive definite matrix in Dmat, while in my case,
this matrix in the objective function is not positive definite.

>  It is in a bit of emergency. I would be really appreciated if anybody
> can give me the reply ASAP. 
> 
> Many thanks
>  Yunlei 
> 
_______________________________________________

This e-mail may contain information that is confidential, privileged or otherwise protected from disclosure. If you are not an intended recipient of this e-mail, do not duplicate or redistribute it by any means. Please delete it and any attachments and notify the sender that you have received it in error. Unless specifically indicated, this e-mail is not an offer to buy or sell or a solicitation to buy or sell any securities, investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Barclays. Any views or opinions presented are solely those of the author and do not necessarily represent those of Barclays. This e-mail is subject to terms available at the following link: www.barcap.com/emaildisclaimer. By messaging with Barclays you consent to the foregoing.  Barclays Capital is the investment banking division of Barclays Bank PLC, a company registered in England (number 1026167) with its registered offi!
 ce at 1 Churchill Place, London, E14 5HP.  This email may relate to or be sent from other members of the Barclays Group.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

From landronimirc at gmail.com  Mon Sep  1 06:45:20 2008
From: landronimirc at gmail.com (Liviu Andronic)
Date: Mon, 1 Sep 2008 06:45:20 +0200
Subject: [R-SIG-Finance] Is there a way to overcome 2 gigabytes data set
	limit in R?
In-Reply-To: <893C528B72100B4DA79A0129B58D4D3D012DC76A@SCWOOD02.corp.scottwoodcapital.com>
References: <893C528B72100B4DA79A0129B58D4D3D012DC76A@SCWOOD02.corp.scottwoodcapital.com>
Message-ID: <68b1e2610808312145s13c70cddi623e9c6af2d0eed7@mail.gmail.com>

On Fri, Jul 18, 2008 at 5:18 PM, Stan Maydan
<smaydan at scottwoodcapital.com> wrote:
> I am doing a survival analysis of the large group of loans and would
> like to know if there is a way to do it in R with the dataset of more
> than 2 Gigabytes, specifically on 64 bits OS.
>
You will find this thread [1] interesting. You'd also want to check
packages filehash, ff and sqldf.
Regards,
Liviu

[1] http://www.nabble.com/How-to-read-HUGE-data-sets--tt15729830.html


From enricoschumann at yahoo.de  Mon Sep  1 09:32:32 2008
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Mon, 1 Sep 2008 09:32:32 +0200
Subject: [R-SIG-Finance] Is there a way to overcome 2 gigabytes data
	setlimit in R?
In-Reply-To: <893C528B72100B4DA79A0129B58D4D3D012DC76A@SCWOOD02.corp.scottwoodcapital.com>
Message-ID: <904547.57192.bm@omp220.mail.ukl.yahoo.com>

you may want to have a look at the ff-package available on CRAN.

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Stan Maydan
Gesendet: Freitag, 18. Juli 2008 17:19
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] Is there a way to overcome 2 gigabytes data
setlimit in R?

Hello,

 

I am doing a survival analysis of the large group of loans and would like to
know if there is a way to do it in R with the dataset of more than 2
Gigabytes, specifically on 64 bits OS.

 

Thank you


	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG - http://www.avg.com

31.08.2008
16:59


From stefano.iacus at unimi.it  Mon Sep  1 19:26:14 2008
From: stefano.iacus at unimi.it (stefano iacus)
Date: Mon, 01 Sep 2008 19:26:14 +0200
Subject: [R-SIG-Finance] mysedata
Message-ID: <B495A54A-64D3-4CC4-8FD9-609E6D0B5A95@unimi.it>

Hi,

do you know any package which includes a function to gete data from

http://www.nyxdata.com/nysedata/default.aspx?tabID=749

assumed you have an account

stefano


From rory.winston at gmail.com  Mon Sep  1 23:18:36 2008
From: rory.winston at gmail.com (Rory Winston)
Date: Mon, 01 Sep 2008 22:18:36 +0100
Subject: [R-SIG-Finance] Is there a way to overcome 2 gigabytes,
 data set	limit in R?
In-Reply-To: <mailman.3.1220263201.30131.r-sig-finance@stat.math.ethz.ch>
References: <mailman.3.1220263201.30131.r-sig-finance@stat.math.ethz.ch>
Message-ID: <48BC5C2C.9020507@gmail.com>

There has been a lot of work on this area over the last year or more. 
R.ff is a well-known package designed to handle this, but there is also 
a comparative package called bigmemory, which may be slightly easier to use.

Cheers
Rory
>  
>
> I am doing a survival analysis of the large group of loans and would
> like to know if there is a way to do it in R with the dataset of more
> than 2 Gigabytes, specifically on 64 bits OS.
>


From vlanschot at yahoo.com  Tue Sep  2 11:36:50 2008
From: vlanschot at yahoo.com (R@Nabble)
Date: Tue, 2 Sep 2008 02:36:50 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] OT: R plug-in for Amibroker
Message-ID: <19266969.post@talk.nabble.com>


This mail is intended for R-users who are looking to extend the
R-functionality within an advanced trading environment. With that in mind,
I'd like to inform you that an R plug-in for Amibroker has been made
available, called RMath. I make this plug-in freely available to all
registered Amibroker-users. Amibroker is a trading software package with a
C-type language, called AFL. It offers advanced backtesting, optimization,
and charting functionality. For more details, please see the website:

http://www.amibroker.com/

The plug-in itself is available from the Members-zone on the official
Amibroker website:

http://www.amibroker.com/members/3rdparty/RPlugIn.zip

Users will need to supply a valid AmIBroker member's zone user/password.

The purpose of the plug-in for the R-user is to allow you to apply your R
requirements within a dedicated trading platform which offers more
functionality than any of the R-packages available currently. I hope that
this plug-in, combined with Amibroker, will enhance your trading/analysis
skills, like it has mine.

For all clarity, I have no affiliation, commercial or otherwise, with
Amibroker beyond being a long-term, and very satisfied customer.

PS
-- 
View this message in context: http://www.nabble.com/OT%3A-R-plug-in-for-Amibroker-tp19266969p19266969.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From daler at uic.edu  Tue Sep  2 18:00:40 2008
From: daler at uic.edu (Rosenthal, Dale W.R.)
Date: Tue, 2 Sep 2008 11:00:40 -0500 (CDT)
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 52, Issue 2
In-Reply-To: <mailman.3.1220349602.2673.r-sig-finance@stat.math.ethz.ch>
References: <mailman.3.1220349602.2673.r-sig-finance@stat.math.ethz.ch>
Message-ID: <52642.99.140.244.0.1220371240.squirrel@webmail.uic.edu>


While not the most user-friendly process, you could also work with you
sysadmin to get a version of R compiled with 64-bit libraries.  That was
my method for beating the 2GB limit; and, it worked very well.

While your at it, you also might ask your sysadmin to compile and link in
GotoBLAS for the linear algebra routines.  That way you would get threaded
performance on some calculations.

Dale
-- 
Dale W.R. Rosenthal
Assistant Professor, Department of Finance
University of Illinois at Chicago
http://tigger.uic.edu/~daler
SSRN: http://ssrn.com/author=906862

On Tue, September 2, 2008 05:00, r-sig-finance-request at stat.math.ethz.ch
wrote:
> Message: 2
> Date: Mon, 01 Sep 2008 22:18:36 +0100
> From: Rory Winston <rory.winston at gmail.com>
> Subject: Re: [R-SIG-Finance] Is there a way to overcome 2 gigabytes,
> 	data set	limit in R?
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID: <48BC5C2C.9020507 at gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> There has been a lot of work on this area over the last year or more.
R.ff is a well-known package designed to handle this, but there is also
a comparative package called bigmemory, which may be slightly easier to
use.
>
> Cheers
> Rory
>> I am doing a survival analysis of the large group of loans and would
like to know if there is a way to do it in R with the dataset of more
than 2 Gigabytes, specifically on 64 bits OS.
>
>






Dale W.R. Rosenthal
Assistant Professor, Department of Finance
University of Illinois at Chicago
http://tigger.uic.edu/~daler
SSRN: http://ssrn.com/author=906862


From jorge.nieves at moorecap.com  Tue Sep  2 23:09:21 2008
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Tue, 2 Sep 2008 17:09:21 -0400
Subject: [R-SIG-Finance] Use apply/lapply/tapply functions
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF75D980@NYC-XCH3.win.moorecap.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080902/0274185c/attachment.pl>

From gmain.20.phftt at xoxy.net  Wed Sep  3 16:52:24 2008
From: gmain.20.phftt at xoxy.net (Rob Steele)
Date: Wed, 03 Sep 2008 10:52:24 -0400
Subject: [R-SIG-Finance] Use apply/lapply/tapply functions
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF75D980@NYC-XCH3.win.moorecap.com>
References: <D595C0E05185614C90515F1E8A2D4CBF75D980@NYC-XCH3.win.moorecap.com>
Message-ID: <g9m8b9$uib$1@ger.gmane.org>

The looping functions (apply/lapply/tapply) can make your code cleaner 
and easier to read but they can't speed it up.  For that you need to 
make the stuff in the loop faster, perhaps by vectorizing parts you're 
currently doing serially.


Jorge Nieves wrote:
> Hi,
> 
> I have  a function that takes in a dataset ( a matrix of m rows by n
> columns), and five additional "constant" parameters, p1,p2,y1,y2,y3. The
> function perform a series of operations and transformations on the
> dataset, and returns a table of results.
> 
> I have tested the function repeatedly and it works fine.
> 
> However, I would like to generate a grid of results from myfunction for
> different values of two of the input parameters: p1, and p2.
> 
> I have tried using for loops, and they work, but the computation time is
> a too long.  I would like to use the apply/lapply/tapply functions to
> avoid using for loops, what ever works !!!
> 
> Can someone recommend how to use these function to parameterize only a
> subset of the inputs into the function, i.e p1, and p2?
> 
> Any tips/recommendations will be appreciated.
> 
> 
> 
> myfunction = function (dataset, p1,p2,y1,y2,y3)
> {
> 
> Line1
> Line2
> Line3
> ::::::::
> :::::::
> :::::::
> Return(res.table)
> }
> 
>


From enricoschumann at yahoo.de  Wed Sep  3 17:27:46 2008
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Wed, 3 Sep 2008 17:27:46 +0200
Subject: [R-SIG-Finance] Use apply/lapply/tapply functions
In-Reply-To: <g9m8b9$uib$1@ger.gmane.org>
Message-ID: <653537.56386.bm@omp214.mail.ukl.yahoo.com>

i suppose that this is rather an r-help question; however, if you can use a
function from the ``apply-family'', it should usually be far faster than a
loop. 

try
N <- 100000
x <- array(0,dim=c(N,1))

set.seed(1284357)
# loop
pcm <- proc.time()
for (i in 1:N){
x[i] <- rnorm(1)
}
p1 <- proc.time()-pcm

set.seed(1284357)
# apply
y <- array(0,dim=c(N,1))
pcm <- proc.time()
y <- apply(y,2,rnorm)
p2 <- proc.time()-pcm

# compare time needed
p1
p2
# compare results
sum(x!=y)


but, if you can use apply, then you probably did not really need a loop in
the first place, as your procedure is not really sequential (in the sense
that the computation in i+1 really required the computation from step i)


-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Rob Steele
Gesendet: Mittwoch, 3. September 2008 16:52
An: r-sig-finance at stat.math.ethz.ch
Betreff: Re: [R-SIG-Finance] Use apply/lapply/tapply functions

The looping functions (apply/lapply/tapply) can make your code cleaner and
easier to read but they can't speed it up.  For that you need to make the
stuff in the loop faster, perhaps by vectorizing parts you're currently
doing serially.


Jorge Nieves wrote:
> Hi,
> 
> I have  a function that takes in a dataset ( a matrix of m rows by n 
> columns), and five additional "constant" parameters, p1,p2,y1,y2,y3. 
> The function perform a series of operations and transformations on the 
> dataset, and returns a table of results.
> 
> I have tested the function repeatedly and it works fine.
> 
> However, I would like to generate a grid of results from myfunction 
> for different values of two of the input parameters: p1, and p2.
> 
> I have tried using for loops, and they work, but the computation time 
> is a too long.  I would like to use the apply/lapply/tapply functions 
> to avoid using for loops, what ever works !!!
> 
> Can someone recommend how to use these function to parameterize only a 
> subset of the inputs into the function, i.e p1, and p2?
> 
> Any tips/recommendations will be appreciated.
> 
> 
> 
> myfunction = function (dataset, p1,p2,y1,y2,y3) {
> 
> Line1
> Line2
> Line3
> ::::::::
> :::::::
> :::::::
> Return(res.table)
> }
> 
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG - http://www.avg.com

03.09.2008
07:15


From jorge.nieves at moorecap.com  Wed Sep  3 17:58:21 2008
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Wed, 3 Sep 2008 11:58:21 -0400
Subject: [R-SIG-Finance] Use apply/lapply/tapply functions
In-Reply-To: <162133.97742.bm@omp204.mail.ukl.yahoo.com>
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF75D985@NYC-XCH3.win.moorecap.com>

Thanks for your suggestions.

I just started using R recently. I am trying to figure out my way around the system. I tested your suggestions and the speed of "apply" is definitely better that of the "for" loop.

The two function inputs (out of a total five) that I am trying to parameterize (loop trough) are not time dependent. Therefore, I believe I could use some function from the apply family, but I do not know how to set it up. The references in the help do not show how to select ONLY a subset (two in his case) of the variables that go into my function. Say if the function takes in (x1,p1,p2,y1,y2), my problem is to determine how to APPLY p1 and p2 only?


What will be the equivalent in the APPLY space to the following for loop code?

for p1 in 1:100
 {
  for p2 in 1:100
  {
   test = myfunction(x1,p1,p2,y1,y2)
  }
}

Where:

myfunction = function (dataset, p1,p2,y1,y2,y3) 
{

Line1
Line2
Line3
::::::::
:::::::
:::::::
Return(res.table)
}
 
res.table is a n by m matrix

Jorge

-----Original Message-----
From: Enrico Schumann [mailto:enricoschumann at yahoo.de] 
Sent: Wednesday, September 03, 2008 11:28 AM
To: 'Rob Steele'; Jorge Nieves
Cc: r-sig-finance at stat.math.ethz.ch
Subject: AW: [R-SIG-Finance] Use apply/lapply/tapply functions

i suppose that this is rather an r-help question; however, if you can use a function from the ``apply-family'', it should usually be far faster than a loop. 

try
N <- 100000
x <- array(0,dim=c(N,1))

set.seed(1284357)
# loop
pcm <- proc.time()
for (i in 1:N){
x[i] <- rnorm(1)
}
p1 <- proc.time()-pcm

set.seed(1284357)
# apply
y <- array(0,dim=c(N,1))
pcm <- proc.time()
y <- apply(y,2,rnorm)
p2 <- proc.time()-pcm

# compare time needed
p1
p2
# compare results
sum(x!=y)


but, if you can use apply, then you probably did not really need a loop in the first place, as your procedure is not really sequential (in the sense that the computation in i+1 really required the computation from step i)


-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Rob Steele
Gesendet: Mittwoch, 3. September 2008 16:52
An: r-sig-finance at stat.math.ethz.ch
Betreff: Re: [R-SIG-Finance] Use apply/lapply/tapply functions

The looping functions (apply/lapply/tapply) can make your code cleaner and easier to read but they can't speed it up.  For that you need to make the stuff in the loop faster, perhaps by vectorizing parts you're currently doing serially.


Jorge Nieves wrote:
> Hi,
> 
> I have  a function that takes in a dataset ( a matrix of m rows by n 
> columns), and five additional "constant" parameters, p1,p2,y1,y2,y3.
> The function perform a series of operations and transformations on the 
> dataset, and returns a table of results.
> 
> I have tested the function repeatedly and it works fine.
> 
> However, I would like to generate a grid of results from myfunction 
> for different values of two of the input parameters: p1, and p2.
> 
> I have tried using for loops, and they work, but the computation time 
> is a too long.  I would like to use the apply/lapply/tapply functions 
> to avoid using for loops, what ever works !!!
> 
> Can someone recommend how to use these function to parameterize only a 
> subset of the inputs into the function, i.e p1, and p2?
> 
> Any tips/recommendations will be appreciated.
> 
> 
> 
> myfunction = function (dataset, p1,p2,y1,y2,y3) {
> 
> Line1
> Line2
> Line3
> ::::::::
> :::::::
> :::::::
> Return(res.table)
> }
> 
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG - http://www.avg.com

03.09.2008
07:15


From patrick at burns-stat.com  Wed Sep  3 21:12:23 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed, 03 Sep 2008 20:12:23 +0100
Subject: [R-SIG-Finance] Use apply/lapply/tapply functions
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF75D986@NYC-XCH3.win.moorecap.com>
References: <D595C0E05185614C90515F1E8A2D4CBF75D986@NYC-XCH3.win.moorecap.com>
Message-ID: <48BEE196.5000506@burns-stat.com>

Jorge,

This is not a good example at all.  First off,  using
'system.time' is easier and better (because it does
garbage collection at the start by default).  Here
are timings I get:

 > N <- 100000
 > xf <- array(0, c(N, 1))
 > xa <- array(0, c(N, 1))

 > set.seed(3)
 > system.time(xar2 <- apply(xa, 2, rnorm))
   user  system elapsed
   0.07    0.00    0.06
 > set.seed(3)
 > system.time(xar1 <- apply(xa, 1, rnorm))
   user  system elapsed
   3.23    0.00    3.24
 > set.seed(3)
 > system.time(for(i in 1:N) xf[i] <- rnorm(1))
   user  system elapsed
   2.62    0.00    2.64

The apply on the second dimension is really a
vectorized operation -- it is only calling 'rnorm'
once.  This works because:

 > rnorm(c(0,0,0))
[1] 1.4397447 0.9339142 1.9902305

'rnorm' vectorizes on the length of the first argument
if it is of length greater than one.  Using 'apply' on the
first dimension takes longer than the loop and doesn't
really do anything.

The proper 'apply' call would be:

 > xa1 <- array(1, c(N, 1))
 > set.seed(3)
 > system.time(xar1b <- apply(xa1, 1, rnorm))
   user  system elapsed
   4.28    0.01    4.41

which takes longer still.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Jorge Nieves wrote:
> Ah  ok
>
> Thanks for your comments Patrick.
>
> I run the example sent to me by Enrico Schumann, and I see quite different processing times. The question of course is if the processing time escalates linearly as the "loop size" increases in both scenarios.
>
> Here is Enrico Schumann [enricoschumann at yahoo.de] email:
>
> ##################
>
> i suppose that this is rather an r-help question; however, if you can use a function from the ``apply-family'', it should usually be far faster than a loop. 
>
> try
> N <- 100000
> x <- array(0,dim=c(N,1))
>
> set.seed(1284357)
> # loop
> pcm <- proc.time()
> for (i in 1:N){
> x[i] <- rnorm(1)
> }
> p1 <- proc.time()-pcm
>
> set.seed(1284357)
> # apply
> y <- array(0,dim=c(N,1))
> pcm <- proc.time()
> y <- apply(y,2,rnorm)
> p2 <- proc.time()-pcm
>
> # compare time needed
> p1
> p2
> # compare results
> sum(x!=y)
>
>
> but, if you can use apply, then you probably did not really need a loop in the first place, as your procedure is not really sequential (in the sense that the computation in i+1 really required the computation from step i)
>
> ############
>  
>
> -----Original Message-----
> From: Patrick Burns [mailto:patrick at burns-stat.com] 
> Sent: Wednesday, September 03, 2008 01:48 PM
> To: Jorge Nieves
> Subject: Re: [R-SIG-Finance] Use apply/lapply/tapply functions
>
> Jorge,
>
> I'd be quite interested to see an example where 'apply' is significantly faster than the corresponding 'for' loop because I don't think that it is possible.
> 'apply' has a 'for' loop in its definition.
>
> S Poetry may be useful to you in learning R.
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Jorge Nieves wrote:
>   
>> Thanks for your suggestions.
>>
>> I just started using R recently. I am trying to figure out my way around the system. I tested your suggestions and the speed of "apply" is definitely better that of the "for" loop.
>>
>> The two function inputs (out of a total five) that I am trying to parameterize (loop trough) are not time dependent. Therefore, I believe I could use some function from the apply family, but I do not know how to set it up. The references in the help do not show how to select ONLY a subset (two in his case) of the variables that go into my function. Say if the function takes in (x1,p1,p2,y1,y2), my problem is to determine how to APPLY p1 and p2 only?
>>
>>
>> What will be the equivalent in the APPLY space to the following for loop code?
>>
>> for p1 in 1:100
>>  {
>>   for p2 in 1:100
>>   {
>>    test = myfunction(x1,p1,p2,y1,y2)
>>   }
>> }
>>
>> Where:
>>
>> myfunction = function (dataset, p1,p2,y1,y2,y3) {
>>
>> Line1
>> Line2
>> Line3
>> ::::::::
>> :::::::
>> :::::::
>> Return(res.table)
>> }
>>  
>> res.table is a n by m matrix
>>
>> Jorge
>>
>> -----Original Message-----
>> From: Enrico Schumann [mailto:enricoschumann at yahoo.de]
>> Sent: Wednesday, September 03, 2008 11:28 AM
>> To: 'Rob Steele'; Jorge Nieves
>> Cc: r-sig-finance at stat.math.ethz.ch
>> Subject: AW: [R-SIG-Finance] Use apply/lapply/tapply functions
>>
>> i suppose that this is rather an r-help question; however, if you can use a function from the ``apply-family'', it should usually be far faster than a loop. 
>>
>> try
>> N <- 100000
>> x <- array(0,dim=c(N,1))
>>
>> set.seed(1284357)
>> # loop
>> pcm <- proc.time()
>> for (i in 1:N){
>> x[i] <- rnorm(1)
>> }
>> p1 <- proc.time()-pcm
>>
>> set.seed(1284357)
>> # apply
>> y <- array(0,dim=c(N,1))
>> pcm <- proc.time()
>> y <- apply(y,2,rnorm)
>> p2 <- proc.time()-pcm
>>
>> # compare time needed
>> p1
>> p2
>> # compare results
>> sum(x!=y)
>>
>>
>> but, if you can use apply, then you probably did not really need a 
>> loop in the first place, as your procedure is not really sequential 
>> (in the sense that the computation in i+1 really required the 
>> computation from step i)
>>
>>
>> -----Urspr?ngliche Nachricht-----
>> Von: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Rob 
>> Steele
>> Gesendet: Mittwoch, 3. September 2008 16:52
>> An: r-sig-finance at stat.math.ethz.ch
>> Betreff: Re: [R-SIG-Finance] Use apply/lapply/tapply functions
>>
>> The looping functions (apply/lapply/tapply) can make your code cleaner and easier to read but they can't speed it up.  For that you need to make the stuff in the loop faster, perhaps by vectorizing parts you're currently doing serially.
>>
>>
>> Jorge Nieves wrote:
>>   
>>     
>>> Hi,
>>>
>>> I have  a function that takes in a dataset ( a matrix of m rows by n 
>>> columns), and five additional "constant" parameters, p1,p2,y1,y2,y3.
>>> The function perform a series of operations and transformations on 
>>> the dataset, and returns a table of results.
>>>
>>> I have tested the function repeatedly and it works fine.
>>>
>>> However, I would like to generate a grid of results from myfunction 
>>> for different values of two of the input parameters: p1, and p2.
>>>
>>> I have tried using for loops, and they work, but the computation time 
>>> is a too long.  I would like to use the apply/lapply/tapply functions 
>>> to avoid using for loops, what ever works !!!
>>>
>>> Can someone recommend how to use these function to parameterize only 
>>> a subset of the inputs into the function, i.e p1, and p2?
>>>
>>> Any tips/recommendations will be appreciated.
>>>
>>>
>>>
>>> myfunction = function (dataset, p1,p2,y1,y2,y3) {
>>>
>>> Line1
>>> Line2
>>> Line3
>>> ::::::::
>>> :::::::
>>> :::::::
>>> Return(res.table)
>>> }
>>>
>>>
>>>     
>>>       
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>> No virus found in this incoming message.
>> Checked by AVG - http://www.avg.com
>>
>> 03.09.2008
>> 07:15
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>>   
>>     
>
>
>


From enricoschumann at yahoo.de  Wed Sep  3 22:35:45 2008
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Wed, 3 Sep 2008 22:35:45 +0200
Subject: [R-SIG-Finance] Use apply/lapply/tapply functions
In-Reply-To: <48BEE196.5000506@burns-stat.com>
Message-ID: <138692.29029.bm@omp210.mail.ukl.yahoo.com>

right; sorry, i `applied' on the column, not the rows. 

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Patrick
Burns
Gesendet: Mittwoch, 3. September 2008 21:12
An: Jorge Nieves
Cc: R-sig-finance at stat.math.ethz.ch
Betreff: Re: [R-SIG-Finance] Use apply/lapply/tapply functions

Jorge,

This is not a good example at all.  First off,  using 'system.time' is
easier and better (because it does garbage collection at the start by
default).  Here are timings I get:

 > N <- 100000
 > xf <- array(0, c(N, 1))
 > xa <- array(0, c(N, 1))

 > set.seed(3)
 > system.time(xar2 <- apply(xa, 2, rnorm))
   user  system elapsed
   0.07    0.00    0.06
 > set.seed(3)
 > system.time(xar1 <- apply(xa, 1, rnorm))
   user  system elapsed
   3.23    0.00    3.24
 > set.seed(3)
 > system.time(for(i in 1:N) xf[i] <- rnorm(1))
   user  system elapsed
   2.62    0.00    2.64

The apply on the second dimension is really a vectorized operation -- it is
only calling 'rnorm'
once.  This works because:

 > rnorm(c(0,0,0))
[1] 1.4397447 0.9339142 1.9902305

'rnorm' vectorizes on the length of the first argument if it is of length
greater than one.  Using 'apply' on the first dimension takes longer than
the loop and doesn't really do anything.

The proper 'apply' call would be:

 > xa1 <- array(1, c(N, 1))
 > set.seed(3)
 > system.time(xar1b <- apply(xa1, 1, rnorm))
   user  system elapsed
   4.28    0.01    4.41

which takes longer still.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Jorge Nieves wrote:
> Ah  ok
>
> Thanks for your comments Patrick.
>
> I run the example sent to me by Enrico Schumann, and I see quite different
processing times. The question of course is if the processing time escalates
linearly as the "loop size" increases in both scenarios.
>
> Here is Enrico Schumann [enricoschumann at yahoo.de] email:
>
> ##################
>
> i suppose that this is rather an r-help question; however, if you can use
a function from the ``apply-family'', it should usually be far faster than a
loop. 
>
> try
> N <- 100000
> x <- array(0,dim=c(N,1))
>
> set.seed(1284357)
> # loop
> pcm <- proc.time()
> for (i in 1:N){
> x[i] <- rnorm(1)
> }
> p1 <- proc.time()-pcm
>
> set.seed(1284357)
> # apply
> y <- array(0,dim=c(N,1))
> pcm <- proc.time()
> y <- apply(y,2,rnorm)
> p2 <- proc.time()-pcm
>
> # compare time needed
> p1
> p2
> # compare results
> sum(x!=y)
>
>
> but, if you can use apply, then you probably did not really need a 
> loop in the first place, as your procedure is not really sequential 
> (in the sense that the computation in i+1 really required the 
> computation from step i)
>
> ############
>  
>
> -----Original Message-----
> From: Patrick Burns [mailto:patrick at burns-stat.com]
> Sent: Wednesday, September 03, 2008 01:48 PM
> To: Jorge Nieves
> Subject: Re: [R-SIG-Finance] Use apply/lapply/tapply functions
>
> Jorge,
>
> I'd be quite interested to see an example where 'apply' is significantly
faster than the corresponding 'for' loop because I don't think that it is
possible.
> 'apply' has a 'for' loop in its definition.
>
> S Poetry may be useful to you in learning R.
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Jorge Nieves wrote:
>   
>> Thanks for your suggestions.
>>
>> I just started using R recently. I am trying to figure out my way around
the system. I tested your suggestions and the speed of "apply" is definitely
better that of the "for" loop.
>>
>> The two function inputs (out of a total five) that I am trying to
parameterize (loop trough) are not time dependent. Therefore, I believe I
could use some function from the apply family, but I do not know how to set
it up. The references in the help do not show how to select ONLY a subset
(two in his case) of the variables that go into my function. Say if the
function takes in (x1,p1,p2,y1,y2), my problem is to determine how to APPLY
p1 and p2 only?
>>
>>
>> What will be the equivalent in the APPLY space to the following for loop
code?
>>
>> for p1 in 1:100
>>  {
>>   for p2 in 1:100
>>   {
>>    test = myfunction(x1,p1,p2,y1,y2)
>>   }
>> }
>>
>> Where:
>>
>> myfunction = function (dataset, p1,p2,y1,y2,y3) {
>>
>> Line1
>> Line2
>> Line3
>> ::::::::
>> :::::::
>> :::::::
>> Return(res.table)
>> }
>>  
>> res.table is a n by m matrix
>>
>> Jorge
>>
>> -----Original Message-----
>> From: Enrico Schumann [mailto:enricoschumann at yahoo.de]
>> Sent: Wednesday, September 03, 2008 11:28 AM
>> To: 'Rob Steele'; Jorge Nieves
>> Cc: r-sig-finance at stat.math.ethz.ch
>> Subject: AW: [R-SIG-Finance] Use apply/lapply/tapply functions
>>
>> i suppose that this is rather an r-help question; however, if you can use
a function from the ``apply-family'', it should usually be far faster than a
loop. 
>>
>> try
>> N <- 100000
>> x <- array(0,dim=c(N,1))
>>
>> set.seed(1284357)
>> # loop
>> pcm <- proc.time()
>> for (i in 1:N){
>> x[i] <- rnorm(1)
>> }
>> p1 <- proc.time()-pcm
>>
>> set.seed(1284357)
>> # apply
>> y <- array(0,dim=c(N,1))
>> pcm <- proc.time()
>> y <- apply(y,2,rnorm)
>> p2 <- proc.time()-pcm
>>
>> # compare time needed
>> p1
>> p2
>> # compare results
>> sum(x!=y)
>>
>>
>> but, if you can use apply, then you probably did not really need a 
>> loop in the first place, as your procedure is not really sequential 
>> (in the sense that the computation in i+1 really required the 
>> computation from step i)
>>
>>
>> -----Urspr?ngliche Nachricht-----
>> Von: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Rob 
>> Steele
>> Gesendet: Mittwoch, 3. September 2008 16:52
>> An: r-sig-finance at stat.math.ethz.ch
>> Betreff: Re: [R-SIG-Finance] Use apply/lapply/tapply functions
>>
>> The looping functions (apply/lapply/tapply) can make your code cleaner
and easier to read but they can't speed it up.  For that you need to make
the stuff in the loop faster, perhaps by vectorizing parts you're currently
doing serially.
>>
>>
>> Jorge Nieves wrote:
>>   
>>     
>>> Hi,
>>>
>>> I have  a function that takes in a dataset ( a matrix of m rows by n 
>>> columns), and five additional "constant" parameters, p1,p2,y1,y2,y3.
>>> The function perform a series of operations and transformations on 
>>> the dataset, and returns a table of results.
>>>
>>> I have tested the function repeatedly and it works fine.
>>>
>>> However, I would like to generate a grid of results from myfunction 
>>> for different values of two of the input parameters: p1, and p2.
>>>
>>> I have tried using for loops, and they work, but the computation 
>>> time is a too long.  I would like to use the apply/lapply/tapply 
>>> functions to avoid using for loops, what ever works !!!
>>>
>>> Can someone recommend how to use these function to parameterize only 
>>> a subset of the inputs into the function, i.e p1, and p2?
>>>
>>> Any tips/recommendations will be appreciated.
>>>
>>>
>>>
>>> myfunction = function (dataset, p1,p2,y1,y2,y3) {
>>>
>>> Line1
>>> Line2
>>> Line3
>>> ::::::::
>>> :::::::
>>> :::::::
>>> Return(res.table)
>>> }
>>>
>>>
>>>     
>>>       
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>> No virus found in this incoming message.
>> Checked by AVG - http://www.avg.com
>>
>> 03.09.2008
>> 07:15
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>>   
>>     
>
>
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG - http://www.avg.com

03.09.2008
07:15


From jorge.nieves at moorecap.com  Wed Sep  3 22:41:32 2008
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Wed, 3 Sep 2008 16:41:32 -0400
Subject: [R-SIG-Finance] Use apply/lapply/tapply functions
In-Reply-To: <138692.29029.bm@omp210.mail.ukl.yahoo.com>
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF75D98E@NYC-XCH3.win.moorecap.com>

Ok...

So in short, there is really not much computational advantage in terms of time savings when using apply versus for loop, right? Is it fair to say advantage of apply over "for loops"  is the "verorizing' and compactness of the code?

Thanks,

Jorge

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Enrico Schumann
Sent: Wednesday, September 03, 2008 04:36 PM
To: 'Patrick Burns'
Cc: R-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Use apply/lapply/tapply functions

right; sorry, i `applied' on the column, not the rows. 

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Patrick Burns
Gesendet: Mittwoch, 3. September 2008 21:12
An: Jorge Nieves
Cc: R-sig-finance at stat.math.ethz.ch
Betreff: Re: [R-SIG-Finance] Use apply/lapply/tapply functions

Jorge,

This is not a good example at all.  First off,  using 'system.time' is easier and better (because it does garbage collection at the start by default).  Here are timings I get:

 > N <- 100000
 > xf <- array(0, c(N, 1))
 > xa <- array(0, c(N, 1))

 > set.seed(3)
 > system.time(xar2 <- apply(xa, 2, rnorm))
   user  system elapsed
   0.07    0.00    0.06
 > set.seed(3)
 > system.time(xar1 <- apply(xa, 1, rnorm))
   user  system elapsed
   3.23    0.00    3.24
 > set.seed(3)
 > system.time(for(i in 1:N) xf[i] <- rnorm(1))
   user  system elapsed
   2.62    0.00    2.64

The apply on the second dimension is really a vectorized operation -- it is only calling 'rnorm'
once.  This works because:

 > rnorm(c(0,0,0))
[1] 1.4397447 0.9339142 1.9902305

'rnorm' vectorizes on the length of the first argument if it is of length greater than one.  Using 'apply' on the first dimension takes longer than the loop and doesn't really do anything.

The proper 'apply' call would be:

 > xa1 <- array(1, c(N, 1))
 > set.seed(3)
 > system.time(xar1b <- apply(xa1, 1, rnorm))
   user  system elapsed
   4.28    0.01    4.41

which takes longer still.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Jorge Nieves wrote:
> Ah  ok
>
> Thanks for your comments Patrick.
>
> I run the example sent to me by Enrico Schumann, and I see quite 
> different
processing times. The question of course is if the processing time escalates linearly as the "loop size" increases in both scenarios.
>
> Here is Enrico Schumann [enricoschumann at yahoo.de] email:
>
> ##################
>
> i suppose that this is rather an r-help question; however, if you can 
> use
a function from the ``apply-family'', it should usually be far faster than a loop. 
>
> try
> N <- 100000
> x <- array(0,dim=c(N,1))
>
> set.seed(1284357)
> # loop
> pcm <- proc.time()
> for (i in 1:N){
> x[i] <- rnorm(1)
> }
> p1 <- proc.time()-pcm
>
> set.seed(1284357)
> # apply
> y <- array(0,dim=c(N,1))
> pcm <- proc.time()
> y <- apply(y,2,rnorm)
> p2 <- proc.time()-pcm
>
> # compare time needed
> p1
> p2
> # compare results
> sum(x!=y)
>
>
> but, if you can use apply, then you probably did not really need a 
> loop in the first place, as your procedure is not really sequential 
> (in the sense that the computation in i+1 really required the 
> computation from step i)
>
> ############
>  
>
> -----Original Message-----
> From: Patrick Burns [mailto:patrick at burns-stat.com]
> Sent: Wednesday, September 03, 2008 01:48 PM
> To: Jorge Nieves
> Subject: Re: [R-SIG-Finance] Use apply/lapply/tapply functions
>
> Jorge,
>
> I'd be quite interested to see an example where 'apply' is 
> significantly
faster than the corresponding 'for' loop because I don't think that it is possible.
> 'apply' has a 'for' loop in its definition.
>
> S Poetry may be useful to you in learning R.
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Jorge Nieves wrote:
>   
>> Thanks for your suggestions.
>>
>> I just started using R recently. I am trying to figure out my way 
>> around
the system. I tested your suggestions and the speed of "apply" is definitely better that of the "for" loop.
>>
>> The two function inputs (out of a total five) that I am trying to
parameterize (loop trough) are not time dependent. Therefore, I believe I could use some function from the apply family, but I do not know how to set it up. The references in the help do not show how to select ONLY a subset (two in his case) of the variables that go into my function. Say if the function takes in (x1,p1,p2,y1,y2), my problem is to determine how to APPLY
p1 and p2 only?
>>
>>
>> What will be the equivalent in the APPLY space to the following for 
>> loop
code?
>>
>> for p1 in 1:100
>>  {
>>   for p2 in 1:100
>>   {
>>    test = myfunction(x1,p1,p2,y1,y2)
>>   }
>> }
>>
>> Where:
>>
>> myfunction = function (dataset, p1,p2,y1,y2,y3) {
>>
>> Line1
>> Line2
>> Line3
>> ::::::::
>> :::::::
>> :::::::
>> Return(res.table)
>> }
>>  
>> res.table is a n by m matrix
>>
>> Jorge
>>
>> -----Original Message-----
>> From: Enrico Schumann [mailto:enricoschumann at yahoo.de]
>> Sent: Wednesday, September 03, 2008 11:28 AM
>> To: 'Rob Steele'; Jorge Nieves
>> Cc: r-sig-finance at stat.math.ethz.ch
>> Subject: AW: [R-SIG-Finance] Use apply/lapply/tapply functions
>>
>> i suppose that this is rather an r-help question; however, if you can 
>> use
a function from the ``apply-family'', it should usually be far faster than a loop. 
>>
>> try
>> N <- 100000
>> x <- array(0,dim=c(N,1))
>>
>> set.seed(1284357)
>> # loop
>> pcm <- proc.time()
>> for (i in 1:N){
>> x[i] <- rnorm(1)
>> }
>> p1 <- proc.time()-pcm
>>
>> set.seed(1284357)
>> # apply
>> y <- array(0,dim=c(N,1))
>> pcm <- proc.time()
>> y <- apply(y,2,rnorm)
>> p2 <- proc.time()-pcm
>>
>> # compare time needed
>> p1
>> p2
>> # compare results
>> sum(x!=y)
>>
>>
>> but, if you can use apply, then you probably did not really need a 
>> loop in the first place, as your procedure is not really sequential 
>> (in the sense that the computation in i+1 really required the 
>> computation from step i)
>>
>>
>> -----Urspr?ngliche Nachricht-----
>> Von: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Rob 
>> Steele
>> Gesendet: Mittwoch, 3. September 2008 16:52
>> An: r-sig-finance at stat.math.ethz.ch
>> Betreff: Re: [R-SIG-Finance] Use apply/lapply/tapply functions
>>
>> The looping functions (apply/lapply/tapply) can make your code 
>> cleaner
and easier to read but they can't speed it up.  For that you need to make the stuff in the loop faster, perhaps by vectorizing parts you're currently doing serially.
>>
>>
>> Jorge Nieves wrote:
>>   
>>     
>>> Hi,
>>>
>>> I have  a function that takes in a dataset ( a matrix of m rows by n 
>>> columns), and five additional "constant" parameters, p1,p2,y1,y2,y3.
>>> The function perform a series of operations and transformations on 
>>> the dataset, and returns a table of results.
>>>
>>> I have tested the function repeatedly and it works fine.
>>>
>>> However, I would like to generate a grid of results from myfunction 
>>> for different values of two of the input parameters: p1, and p2.
>>>
>>> I have tried using for loops, and they work, but the computation 
>>> time is a too long.  I would like to use the apply/lapply/tapply 
>>> functions to avoid using for loops, what ever works !!!
>>>
>>> Can someone recommend how to use these function to parameterize only 
>>> a subset of the inputs into the function, i.e p1, and p2?
>>>
>>> Any tips/recommendations will be appreciated.
>>>
>>>
>>>
>>> myfunction = function (dataset, p1,p2,y1,y2,y3) {
>>>
>>> Line1
>>> Line2
>>> Line3
>>> ::::::::
>>> :::::::
>>> :::::::
>>> Return(res.table)
>>> }
>>>
>>>
>>>     
>>>       
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>> No virus found in this incoming message.
>> Checked by AVG - http://www.avg.com
>>
>> 03.09.2008
>> 07:15
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>>   
>>     
>
>
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG - http://www.avg.com

03.09.2008
07:15

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From brian at braverock.com  Wed Sep  3 22:57:05 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 03 Sep 2008 15:57:05 -0500
Subject: [R-SIG-Finance] Use apply/lapply/tapply functions
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF75D985@NYC-XCH3.win.moorecap.com>
References: <D595C0E05185614C90515F1E8A2D4CBF75D985@NYC-XCH3.win.moorecap.com>
Message-ID: <48BEFA21.9000508@braverock.com>

as others have already hinted at, you really need to rework the *inside*
of your function to not repeat code that doesn't need to be repeated.
Either apply or a loop might be a rational answer there, but you'll
first need to dissect your code to figure out what can be looped inside
the function, while avoiding repeating stuff that's already been
calculated.  For example, many calculations can be vectorized or turned
into matrix operations. Then, pass in a vector of values for p1 and p2,
and have the internals of the function do the work.

Without the actual internals of your function, no one here is going to
be able to speak in more than generalities.  If you're replicating a
published paper, perhaps you should consider that the model you are
evaluating is already "in the wild", and everyone could benefit from an
optimized R model for same.  If, of course, you're creating a new model
from scratch, then we all understand that you cannot show the internals.

Regards,

   - Brian

Jorge Nieves wrote:
> Thanks for your suggestions.
> 
> I just started using R recently. I am trying to figure out my way around the system. I tested your suggestions and the speed of "apply" is definitely better that of the "for" loop.
> 
> The two function inputs (out of a total five) that I am trying to parameterize (loop trough) are not time dependent. Therefore, I believe I could use some function from the apply family, but I do not know how to set it up. The references in the help do not show how to select ONLY a subset (two in his case) of the variables that go into my function. Say if the function takes in (x1,p1,p2,y1,y2), my problem is to determine how to APPLY p1 and p2 only?
> 
> 
> What will be the equivalent in the APPLY space to the following for loop code?
> 
> for p1 in 1:100
>  {
>   for p2 in 1:100
>   {
>    test = myfunction(x1,p1,p2,y1,y2)
>   }
> }
> 
> Where:
> 
> myfunction = function (dataset, p1,p2,y1,y2,y3) 
> {
> 
> Line1
> Line2
> Line3
> ::::::::
> :::::::
> :::::::
> Return(res.table)
> }
>  
> res.table is a n by m matrix
> 
> Jorge
> 
> -----Original Message-----
> From: Enrico Schumann [mailto:enricoschumann at yahoo.de] 
> Sent: Wednesday, September 03, 2008 11:28 AM
> To: 'Rob Steele'; Jorge Nieves
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: AW: [R-SIG-Finance] Use apply/lapply/tapply functions
> 
> i suppose that this is rather an r-help question; however, if you can use a function from the ``apply-family'', it should usually be far faster than a loop. 
> 
> try
> N <- 100000
> x <- array(0,dim=c(N,1))
> 
> set.seed(1284357)
> # loop
> pcm <- proc.time()
> for (i in 1:N){
> x[i] <- rnorm(1)
> }
> p1 <- proc.time()-pcm
> 
> set.seed(1284357)
> # apply
> y <- array(0,dim=c(N,1))
> pcm <- proc.time()
> y <- apply(y,2,rnorm)
> p2 <- proc.time()-pcm
> 
> # compare time needed
> p1
> p2
> # compare results
> sum(x!=y)
> 
> 
> but, if you can use apply, then you probably did not really need a loop in the first place, as your procedure is not really sequential (in the sense that the computation in i+1 really required the computation from step i)
> 
> 
> -----Urspr?ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Rob Steele
> Gesendet: Mittwoch, 3. September 2008 16:52
> An: r-sig-finance at stat.math.ethz.ch
> Betreff: Re: [R-SIG-Finance] Use apply/lapply/tapply functions
> 
> The looping functions (apply/lapply/tapply) can make your code cleaner and easier to read but they can't speed it up.  For that you need to make the stuff in the loop faster, perhaps by vectorizing parts you're currently doing serially.
> 
> 
> Jorge Nieves wrote:
>> Hi,
>>
>> I have  a function that takes in a dataset ( a matrix of m rows by n 
>> columns), and five additional "constant" parameters, p1,p2,y1,y2,y3.
>> The function perform a series of operations and transformations on the 
>> dataset, and returns a table of results.
>>
>> I have tested the function repeatedly and it works fine.
>>
>> However, I would like to generate a grid of results from myfunction 
>> for different values of two of the input parameters: p1, and p2.
>>
>> I have tried using for loops, and they work, but the computation time 
>> is a too long.  I would like to use the apply/lapply/tapply functions 
>> to avoid using for loops, what ever works !!!
>>
>> Can someone recommend how to use these function to parameterize only a 
>> subset of the inputs into the function, i.e p1, and p2?
>>
>> Any tips/recommendations will be appreciated.
>>
>>
>>
>> myfunction = function (dataset, p1,p2,y1,y2,y3) {
>>
>> Line1
>> Line2
>> Line3
>> ::::::::
>> :::::::
>> :::::::
>> Return(res.table)
>> }
>>
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> No virus found in this incoming message.
> Checked by AVG - http://www.avg.com
> 
> 03.09.2008
> 07:15
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From jorge.nieves at moorecap.com  Wed Sep  3 22:59:17 2008
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Wed, 3 Sep 2008 16:59:17 -0400
Subject: [R-SIG-Finance] Use apply/lapply/tapply functions
In-Reply-To: <48BEFA21.9000508@braverock.com>
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF75D991@NYC-XCH3.win.moorecap.com>

Thanks Brian,

I am creating a new model form scratch.

Jorge 

-----Original Message-----
From: Brian G. Peterson [mailto:brian at braverock.com] 
Sent: Wednesday, September 03, 2008 04:57 PM
To: Jorge Nieves
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Use apply/lapply/tapply functions

as others have already hinted at, you really need to rework the *inside* of your function to not repeat code that doesn't need to be repeated.
Either apply or a loop might be a rational answer there, but you'll first need to dissect your code to figure out what can be looped inside the function, while avoiding repeating stuff that's already been calculated.  For example, many calculations can be vectorized or turned into matrix operations. Then, pass in a vector of values for p1 and p2, and have the internals of the function do the work.

Without the actual internals of your function, no one here is going to be able to speak in more than generalities.  If you're replicating a published paper, perhaps you should consider that the model you are evaluating is already "in the wild", and everyone could benefit from an optimized R model for same.  If, of course, you're creating a new model from scratch, then we all understand that you cannot show the internals.

Regards,

   - Brian

Jorge Nieves wrote:
> Thanks for your suggestions.
> 
> I just started using R recently. I am trying to figure out my way around the system. I tested your suggestions and the speed of "apply" is definitely better that of the "for" loop.
> 
> The two function inputs (out of a total five) that I am trying to parameterize (loop trough) are not time dependent. Therefore, I believe I could use some function from the apply family, but I do not know how to set it up. The references in the help do not show how to select ONLY a subset (two in his case) of the variables that go into my function. Say if the function takes in (x1,p1,p2,y1,y2), my problem is to determine how to APPLY p1 and p2 only?
> 
> 
> What will be the equivalent in the APPLY space to the following for loop code?
> 
> for p1 in 1:100
>  {
>   for p2 in 1:100
>   {
>    test = myfunction(x1,p1,p2,y1,y2)
>   }
> }
> 
> Where:
> 
> myfunction = function (dataset, p1,p2,y1,y2,y3) {
> 
> Line1
> Line2
> Line3
> ::::::::
> :::::::
> :::::::
> Return(res.table)
> }
>  
> res.table is a n by m matrix
> 
> Jorge
> 
> -----Original Message-----
> From: Enrico Schumann [mailto:enricoschumann at yahoo.de]
> Sent: Wednesday, September 03, 2008 11:28 AM
> To: 'Rob Steele'; Jorge Nieves
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: AW: [R-SIG-Finance] Use apply/lapply/tapply functions
> 
> i suppose that this is rather an r-help question; however, if you can use a function from the ``apply-family'', it should usually be far faster than a loop. 
> 
> try
> N <- 100000
> x <- array(0,dim=c(N,1))
> 
> set.seed(1284357)
> # loop
> pcm <- proc.time()
> for (i in 1:N){
> x[i] <- rnorm(1)
> }
> p1 <- proc.time()-pcm
> 
> set.seed(1284357)
> # apply
> y <- array(0,dim=c(N,1))
> pcm <- proc.time()
> y <- apply(y,2,rnorm)
> p2 <- proc.time()-pcm
> 
> # compare time needed
> p1
> p2
> # compare results
> sum(x!=y)
> 
> 
> but, if you can use apply, then you probably did not really need a 
> loop in the first place, as your procedure is not really sequential 
> (in the sense that the computation in i+1 really required the 
> computation from step i)
> 
> 
> -----Urspr?ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Rob 
> Steele
> Gesendet: Mittwoch, 3. September 2008 16:52
> An: r-sig-finance at stat.math.ethz.ch
> Betreff: Re: [R-SIG-Finance] Use apply/lapply/tapply functions
> 
> The looping functions (apply/lapply/tapply) can make your code cleaner and easier to read but they can't speed it up.  For that you need to make the stuff in the loop faster, perhaps by vectorizing parts you're currently doing serially.
> 
> 
> Jorge Nieves wrote:
>> Hi,
>>
>> I have  a function that takes in a dataset ( a matrix of m rows by n 
>> columns), and five additional "constant" parameters, p1,p2,y1,y2,y3.
>> The function perform a series of operations and transformations on 
>> the dataset, and returns a table of results.
>>
>> I have tested the function repeatedly and it works fine.
>>
>> However, I would like to generate a grid of results from myfunction 
>> for different values of two of the input parameters: p1, and p2.
>>
>> I have tried using for loops, and they work, but the computation time 
>> is a too long.  I would like to use the apply/lapply/tapply functions 
>> to avoid using for loops, what ever works !!!
>>
>> Can someone recommend how to use these function to parameterize only 
>> a subset of the inputs into the function, i.e p1, and p2?
>>
>> Any tips/recommendations will be appreciated.
>>
>>
>>
>> myfunction = function (dataset, p1,p2,y1,y2,y3) {
>>
>> Line1
>> Line2
>> Line3
>> ::::::::
>> :::::::
>> :::::::
>> Return(res.table)
>> }
>>
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> No virus found in this incoming message.
> Checked by AVG - http://www.avg.com
> 
> 03.09.2008
> 07:15
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From gmain.20.phftt at xoxy.net  Wed Sep  3 23:22:03 2008
From: gmain.20.phftt at xoxy.net (Rob Steele)
Date: Wed, 03 Sep 2008 17:22:03 -0400
Subject: [R-SIG-Finance] Use apply/lapply/tapply functions
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF75D98E@NYC-XCH3.win.moorecap.com>
References: <138692.29029.bm@omp210.mail.ukl.yahoo.com>
	<D595C0E05185614C90515F1E8A2D4CBF75D98E@NYC-XCH3.win.moorecap.com>
Message-ID: <g9mv5t$nc1$1@ger.gmane.org>

Jorge Nieves wrote:
> Ok...
> 
> So in short, there is really not much computational advantage in terms of time savings when using apply versus for loop, right? Is it fair to say advantage of apply over "for loops"  is the "verorizing' and compactness of the code?
> 
> Thanks,
> 
> Jorge

Um, no.  Or rather, you're half right.  apply() and it's siblings are 
mere syntactic sugar to help you write clean code.  They are implicit 
loops that still have to loop over whatever it is you would loop over in 
an explicit loop.  Vectorization is something else altogether and is a 
big win if you can use it.  It can eliminate loops altogether, or rather 
push them down into a hyper-optimized numerical library.

Good luck.


From UriShimron at optiver.com  Thu Sep  4 12:06:20 2008
From: UriShimron at optiver.com (Uri Shimron)
Date: Thu, 4 Sep 2008 12:06:20 +0200
Subject: [R-SIG-Finance] Use apply/lapply/tapply functions
In-Reply-To: <g9mv5t$nc1$1@ger.gmane.org>
Message-ID: <061264DF11708C44BAF02C0F3F4EA28F034D8D4D@opamms0002.comp.optiver.com>


I think that the R Help Desk article in the latest R News
(http://cran.r-project.org/doc/Rnews/Rnews_2008-1.pdf) on 'How can I
avoid this loop or make it faster' is a very nice and succinct article
on this issue.

In your case you could try to use
#WARNING: the line below has not been tested at all!
test <-
mapply(myfunction,p1=1:100,p2=1:100,MoreArgs=list(dataset=x1,y1=y1,y2=y2
,y3=y3))
#you may have to play a bit with SIMPLIFY
#see ?mapply

Hope this helps!

Uri Shimron


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Rob Steele
Sent: Wednesday 03 September 2008 23:22
To: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Use apply/lapply/tapply functions

Jorge Nieves wrote:
> Ok...
> 
> So in short, there is really not much computational advantage in terms
of time savings when using apply versus for loop, right? Is it fair to
say advantage of apply over "for loops"  is the "verorizing' and
compactness of the code?
> 
> Thanks,
> 
> Jorge

Um, no.  Or rather, you're half right.  apply() and it's siblings are 
mere syntactic sugar to help you write clean code.  They are implicit 
loops that still have to loop over whatever it is you would loop over in

an explicit loop.  Vectorization is something else altogether and is a 
big win if you can use it.  It can eliminate loops altogether, or rather

push them down into a hyper-optimized numerical library.

Good luck.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

***************************************************************************
This email and any files transmitted with it are confide...{{dropped:11}}


From albertosantini at gmail.com  Thu Sep  4 18:57:28 2008
From: albertosantini at gmail.com (Alberto Santini)
Date: Thu, 4 Sep 2008 09:57:28 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] portfolio.optim and assets with
 weigth equals to zero...
Message-ID: <19315076.post@talk.nabble.com>


After posting  on R-Help it makes sense posting on r-sig-finance too.

I don't understand a particular output of portfolio.optim (tseries).
I have 4 assets and the portfolio.optim returns an asset with weight equals
to zero.
If I do a portfolio.optim with 3 assets, without the asset with weight
equals to zero,
it returns a completely different result.

That's I would expected the same weights as the run with 4 assets.

Below the code.

Thanks in advance,
Alberto Santini


-------------------------------------

require(tseries)

f.mi <- coredata(get.hist.quote("F.MI", start="2006-09-03", compression="w",
quote="Close"))
eng.mi <- coredata(get.hist.quote("ENG.MI", start="2006-09-03",
compression="w", quote="Close"))
tis.mi <- coredata(get.hist.quote("TIS.MI", start="2006-09-03",
compression="w", quote="Close"))
spmib <- coredata(get.hist.quote("^SPMIB", start="2006-09-03",
compression="w", quote="Close"))

f.mi.rets <- diff(log(f.mi[1:(length(f.mi)-1)]))
eng.mi.rets <- diff(log(eng.mi[1:(length(eng.mi)-1)]))
tis.mi.rets <- diff(log(tis.mi[1:(length(tis.mi)-1)]))
spmib.rets <- diff(log(spmib[1:(length(spmib)-1)]))

x <- cbind(f.mi.rets, eng.mi.rets, tis.mi.rets, spmib.rets)
res <- portfolio.optim(x);
res$pw

x2 <- cbind(f.mi.rets, eng.mi.rets, spmib.rets)
res <- portfolio.optim(x2);
res$pw 

----------------------------------------


I tried with Rmetrics too, but the behaviour is the same. So I am missing
something...
Very interesting to understand why. :)

I think it's a numerical problem: maybe, zero is not zero, it's very near to
zero.

Regards,
Alberto Santini

-----------------------------

require(fPortfolio)
require(tseries)

assets <- c(
    "F.MI",
    "ENG.MI",
    "TIS.MI",
    "^SPMIB"
)

f.mi <- get.hist.quote("F.MI", start="2006-09-03", end="2008-09-03",
    compression="w", quote="Close")
eng.mi <- get.hist.quote("ENG.MI", start="2006-09-03", end="2008-09-03",
    compression="w", quote="Close")
tis.mi <- get.hist.quote("TIS.MI", start="2006-09-03", end="2008-09-03",
    compression="w", quote="Close")  
spmib <- get.hist.quote("^SPMIB", start="2006-09-03", end="2008-09-03",
    compression="w", quote="Close")

X <- cbind(f.mi[1:(length(f.mi)-1)], eng.mi[1:(length(eng.mi)-1)],
tis.mi[1:(length(tis.mi)-1)], spmib[1:(length(spmib)-1)])
colnames(X) <- assets

R <- as.timeSeries(returns(X))

# Spec = portfolioSpec(model = list(type = c("MV"),
#  estimator = c("mean", "cov"), tailRisk = list(), params = list()),
#  portfolio = list(weights = NULL, targetReturn = 0,
#    targetRisk = 0, targetAlpha = 0.05, riskFreeRate = NULL,
#    nFrontierPoints = 50),
#  solver = list(solver = c("quadprog"), trace = FALSE))
Spec = portfolioSpec()
frontier <- portfolioFrontier(R, Spec, c("minW[1:nAssets]=0"))
# weightsSlider(frontier)

ptf <-
frontier at portfolio$weights[which.min(getTargetRisk(frontier)[,1])+1,]*100
ptf

assets2 <- c(
    "ENG.MI",
    "^SPMIB"
)

X2 <- cbind(eng.mi[1:(length(eng.mi)-1)], spmib[1:(length(spmib)-1)])
colnames(X2) <- assets2

R2 <- as.timeSeries(returns(X2))
frontier <- portfolioFrontier(R2, Spec, c("minW[1:nAssets]=0"))

ptf <-
frontier at portfolio$weights[which.min(getTargetRisk(frontier)[,1])+1,]*100
ptf

-----------------------------

The asset allocation (in Rmetrics):


    F.MI   ENG.MI   TIS.MI   ^SPMIB
 0.00000 24.30756  0.00000 75.69244

 ENG.MI   ^SPMIB
24.48980 75.51020


> cov(R)

               F.MI       ENG.MI       TIS.MI       ^SPMIB
F.MI   0.0029905040 0.0008974621 0.0010289887 0.0009954717
ENG.MI 0.0008974621 0.0014935276 0.0004278133 0.0002809398
TIS.MI 0.0010289887 0.0004278133 0.0052708621 0.0007186357
^SPMIB 0.0009954717 0.0002809398 0.0007186357 0.0005617209
> cov(R2)
             ENG.MI       ^SPMIB
ENG.MI 0.0014197889 0.0002578205
^SPMIB 0.0002578205 0.0005791276
 
I would think that if a weight is within epsilon of zero, then removing it
from the computation should have minimal impact on the result.

> eigen(cov(R))
$values
[1] 0.0060129474 0.0030233942 0.0010997423 0.0001805307

$vectors
           [,1]       [,2]        [,3]        [,4]
[1,] -0.4145084  0.7702681  0.36304798 -0.32103879
[2,] -0.1770771  0.3541481 -0.91735850  0.04094150
[3,] -0.8700674 -0.4871272 -0.02330958 -0.07173836
[4,] -0.1995222  0.2096978  0.16157418  0.94345721

> eigen(cov(R2))
$values
[1] 0.0014925599 0.0005063566

$vectors
           [,1]       [,2]
[1,] -0.9623985  0.2716415
[2,] -0.2716415 -0.9623985

The difference of the asset allocation is small. It is not so for
portfolio.optim. I will investigate the difference between Rmetric and
tseries, because in my context the solver is the same (quadprog).

Maybe the measure of the risk used in Rmetrics is not the variance as used
in tseries and it is less sensitive. In fact, the two asset allocations are
different.

I have short constraints and the weights don't change if I change the order
of the assets.


Thanks in advance,
Alberto Santini
-- 
View this message in context: http://www.nabble.com/portfolio.optim-and-assets-with-weigth-equals-to-zero...-tp19315076p19315076.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From zornitsa.luleva at gmail.com  Thu Sep  4 19:13:15 2008
From: zornitsa.luleva at gmail.com (Zornitsa Luleva)
Date: Thu, 4 Sep 2008 19:13:15 +0200
Subject: [R-SIG-Finance] Compound Poisson process - parameter estimation
Message-ID: <e67ad1a20809041013k1f60d47n22ab52ba02b73da7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080904/acd243a5/attachment.pl>

From guy.yollin at rotellacapital.com  Thu Sep  4 19:52:41 2008
From: guy.yollin at rotellacapital.com (Guy Yollin)
Date: Thu, 4 Sep 2008 13:52:41 -0400
Subject: [R-SIG-Finance] [R-sig-finance] portfolio.optim and assets with
	weigth equals to zero...
In-Reply-To: <19315076.post@talk.nabble.com>
References: <19315076.post@talk.nabble.com>
Message-ID: <E634AF2410E42246A35865D8C0C784D901A7DDCE@MI8NYCMAIL09.Mi8.com>

Alberto,

When passing just a matrix to portfolio.optim, the required portfolio
return defaults to the mean of the matrix.

Since mean(x) and mean(x2) are different, your weights are different.

You would get the same weights in the 2nd call if it were:

res <- portfolio.optim(x2,pm=mean(x))

Best,

-- G


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Alberto
Santini
Sent: Thursday, September 04, 2008 9:57 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] [R-sig-finance] portfolio.optim and assets with
weigth equals to zero...


After posting  on R-Help it makes sense posting on r-sig-finance too.

I don't understand a particular output of portfolio.optim (tseries).
I have 4 assets and the portfolio.optim returns an asset with weight
equals
to zero.
If I do a portfolio.optim with 3 assets, without the asset with weight
equals to zero,
it returns a completely different result.

That's I would expected the same weights as the run with 4 assets.

Below the code.

Thanks in advance,
Alberto Santini


-------------------------------------

require(tseries)

f.mi <- coredata(get.hist.quote("F.MI", start="2006-09-03",
compression="w",
quote="Close"))
eng.mi <- coredata(get.hist.quote("ENG.MI", start="2006-09-03",
compression="w", quote="Close"))
tis.mi <- coredata(get.hist.quote("TIS.MI", start="2006-09-03",
compression="w", quote="Close"))
spmib <- coredata(get.hist.quote("^SPMIB", start="2006-09-03",
compression="w", quote="Close"))

f.mi.rets <- diff(log(f.mi[1:(length(f.mi)-1)]))
eng.mi.rets <- diff(log(eng.mi[1:(length(eng.mi)-1)]))
tis.mi.rets <- diff(log(tis.mi[1:(length(tis.mi)-1)]))
spmib.rets <- diff(log(spmib[1:(length(spmib)-1)]))

x <- cbind(f.mi.rets, eng.mi.rets, tis.mi.rets, spmib.rets)
res <- portfolio.optim(x);
res$pw

x2 <- cbind(f.mi.rets, eng.mi.rets, spmib.rets)
res <- portfolio.optim(x2);
res$pw 

----------------------------------------


I tried with Rmetrics too, but the behaviour is the same. So I am
missing
something...
Very interesting to understand why. :)

I think it's a numerical problem: maybe, zero is not zero, it's very
near to
zero.

Regards,
Alberto Santini

-----------------------------

require(fPortfolio)
require(tseries)

assets <- c(
    "F.MI",
    "ENG.MI",
    "TIS.MI",
    "^SPMIB"
)

f.mi <- get.hist.quote("F.MI", start="2006-09-03", end="2008-09-03",
    compression="w", quote="Close")
eng.mi <- get.hist.quote("ENG.MI", start="2006-09-03", end="2008-09-03",
    compression="w", quote="Close")
tis.mi <- get.hist.quote("TIS.MI", start="2006-09-03", end="2008-09-03",
    compression="w", quote="Close")  
spmib <- get.hist.quote("^SPMIB", start="2006-09-03", end="2008-09-03",
    compression="w", quote="Close")

X <- cbind(f.mi[1:(length(f.mi)-1)], eng.mi[1:(length(eng.mi)-1)],
tis.mi[1:(length(tis.mi)-1)], spmib[1:(length(spmib)-1)])
colnames(X) <- assets

R <- as.timeSeries(returns(X))

# Spec = portfolioSpec(model = list(type = c("MV"),
#  estimator = c("mean", "cov"), tailRisk = list(), params = list()),
#  portfolio = list(weights = NULL, targetReturn = 0,
#    targetRisk = 0, targetAlpha = 0.05, riskFreeRate = NULL,
#    nFrontierPoints = 50),
#  solver = list(solver = c("quadprog"), trace = FALSE))
Spec = portfolioSpec()
frontier <- portfolioFrontier(R, Spec, c("minW[1:nAssets]=0"))
# weightsSlider(frontier)

ptf <-
frontier at portfolio$weights[which.min(getTargetRisk(frontier)[,1])+1,]*10
0
ptf

assets2 <- c(
    "ENG.MI",
    "^SPMIB"
)

X2 <- cbind(eng.mi[1:(length(eng.mi)-1)], spmib[1:(length(spmib)-1)])
colnames(X2) <- assets2

R2 <- as.timeSeries(returns(X2))
frontier <- portfolioFrontier(R2, Spec, c("minW[1:nAssets]=0"))

ptf <-
frontier at portfolio$weights[which.min(getTargetRisk(frontier)[,1])+1,]*10
0
ptf

-----------------------------

The asset allocation (in Rmetrics):


    F.MI   ENG.MI   TIS.MI   ^SPMIB
 0.00000 24.30756  0.00000 75.69244

 ENG.MI   ^SPMIB
24.48980 75.51020


> cov(R)

               F.MI       ENG.MI       TIS.MI       ^SPMIB
F.MI   0.0029905040 0.0008974621 0.0010289887 0.0009954717
ENG.MI 0.0008974621 0.0014935276 0.0004278133 0.0002809398
TIS.MI 0.0010289887 0.0004278133 0.0052708621 0.0007186357
^SPMIB 0.0009954717 0.0002809398 0.0007186357 0.0005617209
> cov(R2)
             ENG.MI       ^SPMIB
ENG.MI 0.0014197889 0.0002578205
^SPMIB 0.0002578205 0.0005791276

I would think that if a weight is within epsilon of zero, then removing
it
from the computation should have minimal impact on the result.

> eigen(cov(R))
$values
[1] 0.0060129474 0.0030233942 0.0010997423 0.0001805307

$vectors
           [,1]       [,2]        [,3]        [,4]
[1,] -0.4145084  0.7702681  0.36304798 -0.32103879
[2,] -0.1770771  0.3541481 -0.91735850  0.04094150
[3,] -0.8700674 -0.4871272 -0.02330958 -0.07173836
[4,] -0.1995222  0.2096978  0.16157418  0.94345721

> eigen(cov(R2))
$values
[1] 0.0014925599 0.0005063566

$vectors
           [,1]       [,2]
[1,] -0.9623985  0.2716415
[2,] -0.2716415 -0.9623985

The difference of the asset allocation is small. It is not so for
portfolio.optim. I will investigate the difference between Rmetric and
tseries, because in my context the solver is the same (quadprog).

Maybe the measure of the risk used in Rmetrics is not the variance as
used
in tseries and it is less sensitive. In fact, the two asset allocations
are
different.

I have short constraints and the weights don't change if I change the
order
of the assets.


Thanks in advance,
Alberto Santini
-- 
View this message in context:
http://www.nabble.com/portfolio.optim-and-assets-with-weigth-equals-to-z
ero...-tp19315076p19315076.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Thu Sep  4 20:43:03 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 4 Sep 2008 13:43:03 -0500
Subject: [R-SIG-Finance] First R/Finance/Chicago Meeting October 3rd
Message-ID: <e8e755250809041143v338a479r683537c247120a8b@mail.gmail.com>

R-finance useRs [[Chicago]]:

I'm happy to announce the first big meeting of R/Finance users in Chicago
on October 3rd, 2008.

In coordination with major R finance contributors and the support of the
University of Illinois at Chicago, we invite the local community to an
informal social gathering after the market close on Friday October 3rd.

The gathering will be at Jak's Tap in the West Loop and will start with a
casual reception at 3:30.  Details and links can be found at the bottom of
this email.

The idea is to bring together industry practitioners and academics with a
common interest in R, from across Chicago-land, to meet face to face.  We
hope to increase discussion about the future of R and finance.  We will
also be discussing a much bigger event for the Spring and would like
feedback as to possible direction and content.

The afternoon/evening will be mostly informal, with formal presentations
starting around 5.

The formal agenda:

? We will have a small presentation from some of the locally-based
contributors as to the state of the R community in finance, as well as a
vision for the future.  The current presenters include:

 Jeff Ryan [quantmod, xts, IBrokers]
 Dirk Eddelbuettel [CRAN Empirical Finance task View, RQuantLib, etc]
 Brian Peterson & Peter Carl [PerformanceAnalytics]
 Josh Ulrich [TTR, xts, opentick]

? A presentation by UIC's Finance Department and International Center for
Futures and Derivatives on: (1) how UIC fits into the R-finance mix, and
(2) what they have to offer the larger quantitative/trading community in
Chicago.

Following the formal presentations, we will have the room(s) until 10PM to
allow for discussion and collaboration among the attendees.

Please RSVP to jeffrey.ryan at insightalgo.com or jeff.a.ryan at gmail.com

What: First R/Finance Chicago Meeting
Where: Jak's Tap. 901 W. Jackson, Chicago IL  http://jakstap.com/
When: Friday, October 3, 2008
Time: 3:30 - 10:00PM (presentations start at 5:00PM)


-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From rbali at ufmg.br  Thu Sep  4 21:02:47 2008
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Thu, 4 Sep 2008 15:02:47 -0400
Subject: [R-SIG-Finance] [R-sig-finance] portfolio.optim and assets with
	weigth equals to zero...
References: <19315076.post@talk.nabble.com>
Message-ID: <45E2767FFD024545ACE0830D3E6AD395@HPR>

Alberto,
Your data is a little bit mess because of NAs. The different 'treatment' 
given to NAs by "coredata(get.hist.quote" and "as.timeSeries(returns" may 
explain some of the differences in the results. You are passing different 
set of returns. The first method passes a wrong returns matrix.
Good luck
Robert

--------------------------------------------------
From: "Alberto Santini" <albertosantini at gmail.com>
Sent: Thursday, September 04, 2008 12:57 PM
To: <r-sig-finance at stat.math.ethz.ch>
Subject: [R-SIG-Finance] [R-sig-finance] portfolio.optim and assets with 
weigth equals to zero...

>
> After posting  on R-Help it makes sense posting on r-sig-finance too.
>
> I don't understand a particular output of portfolio.optim (tseries).
> I have 4 assets and the portfolio.optim returns an asset with weight 
> equals
> to zero.
> If I do a portfolio.optim with 3 assets, without the asset with weight
> equals to zero,
> it returns a completely different result.
>
> That's I would expected the same weights as the run with 4 assets.
>
> Below the code.
>
> Thanks in advance,
> Alberto Santini
>
>
> -------------------------------------
>
> require(tseries)
>
> f.mi <- coredata(get.hist.quote("F.MI", start="2006-09-03", 
> compression="w",
> quote="Close"))
> eng.mi <- coredata(get.hist.quote("ENG.MI", start="2006-09-03",
> compression="w", quote="Close"))
> tis.mi <- coredata(get.hist.quote("TIS.MI", start="2006-09-03",
> compression="w", quote="Close"))
> spmib <- coredata(get.hist.quote("^SPMIB", start="2006-09-03",
> compression="w", quote="Close"))
>
> f.mi.rets <- diff(log(f.mi[1:(length(f.mi)-1)]))
> eng.mi.rets <- diff(log(eng.mi[1:(length(eng.mi)-1)]))
> tis.mi.rets <- diff(log(tis.mi[1:(length(tis.mi)-1)]))
> spmib.rets <- diff(log(spmib[1:(length(spmib)-1)]))
>
> x <- cbind(f.mi.rets, eng.mi.rets, tis.mi.rets, spmib.rets)
> res <- portfolio.optim(x);
> res$pw
>
> x2 <- cbind(f.mi.rets, eng.mi.rets, spmib.rets)
> res <- portfolio.optim(x2);
> res$pw
>
> ----------------------------------------
>
>
> I tried with Rmetrics too, but the behaviour is the same. So I am missing
> something...
> Very interesting to understand why. :)
>
> I think it's a numerical problem: maybe, zero is not zero, it's very near 
> to
> zero.
>
> Regards,
> Alberto Santini
>
> -----------------------------
>
> require(fPortfolio)
> require(tseries)
>
> assets <- c(
>    "F.MI",
>    "ENG.MI",
>    "TIS.MI",
>    "^SPMIB"
> )
>
> f.mi <- get.hist.quote("F.MI", start="2006-09-03", end="2008-09-03",
>    compression="w", quote="Close")
> eng.mi <- get.hist.quote("ENG.MI", start="2006-09-03", end="2008-09-03",
>    compression="w", quote="Close")
> tis.mi <- get.hist.quote("TIS.MI", start="2006-09-03", end="2008-09-03",
>    compression="w", quote="Close")
> spmib <- get.hist.quote("^SPMIB", start="2006-09-03", end="2008-09-03",
>    compression="w", quote="Close")
>
> X <- cbind(f.mi[1:(length(f.mi)-1)], eng.mi[1:(length(eng.mi)-1)],
> tis.mi[1:(length(tis.mi)-1)], spmib[1:(length(spmib)-1)])
> colnames(X) <- assets
>
> R <- as.timeSeries(returns(X))
>
> # Spec = portfolioSpec(model = list(type = c("MV"),
> #  estimator = c("mean", "cov"), tailRisk = list(), params = list()),
> #  portfolio = list(weights = NULL, targetReturn = 0,
> #    targetRisk = 0, targetAlpha = 0.05, riskFreeRate = NULL,
> #    nFrontierPoints = 50),
> #  solver = list(solver = c("quadprog"), trace = FALSE))
> Spec = portfolioSpec()
> frontier <- portfolioFrontier(R, Spec, c("minW[1:nAssets]=0"))
> # weightsSlider(frontier)
>
> ptf <-
> frontier at portfolio$weights[which.min(getTargetRisk(frontier)[,1])+1,]*100
> ptf
>
> assets2 <- c(
>    "ENG.MI",
>    "^SPMIB"
> )
>
> X2 <- cbind(eng.mi[1:(length(eng.mi)-1)], spmib[1:(length(spmib)-1)])
> colnames(X2) <- assets2
>
> R2 <- as.timeSeries(returns(X2))
> frontier <- portfolioFrontier(R2, Spec, c("minW[1:nAssets]=0"))
>
> ptf <-
> frontier at portfolio$weights[which.min(getTargetRisk(frontier)[,1])+1,]*100
> ptf
>
> -----------------------------
>
> The asset allocation (in Rmetrics):
>
>
>    F.MI   ENG.MI   TIS.MI   ^SPMIB
> 0.00000 24.30756  0.00000 75.69244
>
> ENG.MI   ^SPMIB
> 24.48980 75.51020
>
>
>> cov(R)
>
>               F.MI       ENG.MI       TIS.MI       ^SPMIB
> F.MI   0.0029905040 0.0008974621 0.0010289887 0.0009954717
> ENG.MI 0.0008974621 0.0014935276 0.0004278133 0.0002809398
> TIS.MI 0.0010289887 0.0004278133 0.0052708621 0.0007186357
> ^SPMIB 0.0009954717 0.0002809398 0.0007186357 0.0005617209
>> cov(R2)
>             ENG.MI       ^SPMIB
> ENG.MI 0.0014197889 0.0002578205
> ^SPMIB 0.0002578205 0.0005791276
>
> I would think that if a weight is within epsilon of zero, then removing it
> from the computation should have minimal impact on the result.
>
>> eigen(cov(R))
> $values
> [1] 0.0060129474 0.0030233942 0.0010997423 0.0001805307
>
> $vectors
>           [,1]       [,2]        [,3]        [,4]
> [1,] -0.4145084  0.7702681  0.36304798 -0.32103879
> [2,] -0.1770771  0.3541481 -0.91735850  0.04094150
> [3,] -0.8700674 -0.4871272 -0.02330958 -0.07173836
> [4,] -0.1995222  0.2096978  0.16157418  0.94345721
>
>> eigen(cov(R2))
> $values
> [1] 0.0014925599 0.0005063566
>
> $vectors
>           [,1]       [,2]
> [1,] -0.9623985  0.2716415
> [2,] -0.2716415 -0.9623985
>
> The difference of the asset allocation is small. It is not so for
> portfolio.optim. I will investigate the difference between Rmetric and
> tseries, because in my context the solver is the same (quadprog).
>
> Maybe the measure of the risk used in Rmetrics is not the variance as used
> in tseries and it is less sensitive. In fact, the two asset allocations 
> are
> different.
>
> I have short constraints and the weights don't change if I change the 
> order
> of the assets.
>
>
> Thanks in advance,
> Alberto Santini
> -- 
> View this message in context: 
> http://www.nabble.com/portfolio.optim-and-assets-with-weigth-equals-to-zero...-tp19315076p19315076.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From albertosantini at gmail.com  Thu Sep  4 23:05:25 2008
From: albertosantini at gmail.com (Alberto Santini)
Date: Thu, 4 Sep 2008 14:05:25 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] portfolio.optim and assets with
 weigth equals to zero...
In-Reply-To: <E634AF2410E42246A35865D8C0C784D901A7DDCE@MI8NYCMAIL09.Mi8.com>
References: <19315076.post@talk.nabble.com>
	<E634AF2410E42246A35865D8C0C784D901A7DDCE@MI8NYCMAIL09.Mi8.com>
Message-ID: <19319949.post@talk.nabble.com>



Guy Yollin-2 wrote:
> 
> Alberto,
> 
> When passing just a matrix to portfolio.optim, the required portfolio
> return defaults to the mean of the matrix.
> 
> Since mean(x) and mean(x2) are different, your weights are different.
> 

Simple and plain: this is the answer. Thanks a lot.


Guy Yollin-2 wrote:
> 
> You would get the same weights in the 2nd call if it were:
> 
> res <- portfolio.optim(x2,pm=mean(x))
> 

Correct, if I set the same target return the weights returned are the same.
I was concentrated only on the covariance matrix. 

Thanks again,
Alberto Santini
-- 
View this message in context: http://www.nabble.com/portfolio.optim-and-assets-with-weigth-equals-to-zero...-tp19315076p19319949.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From albertosantini at gmail.com  Thu Sep  4 23:13:14 2008
From: albertosantini at gmail.com (Alberto Santini)
Date: Thu, 4 Sep 2008 14:13:14 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] portfolio.optim and assets with
 weigth equals to zero...
In-Reply-To: <45E2767FFD024545ACE0830D3E6AD395@HPR>
References: <19315076.post@talk.nabble.com>
	<45E2767FFD024545ACE0830D3E6AD395@HPR>
Message-ID: <19320091.post@talk.nabble.com>




Robert Iquiapaza wrote:
> 
> Alberto,
> Your data is a little bit mess because of NAs. The different 'treatment' 
> given to NAs by "coredata(get.hist.quote" and "as.timeSeries(returns" may 
> explain some of the differences in the results. You are passing different 
> set of returns. The first method passes a wrong returns matrix.
> 

Sorry... but I don't see any NA. Never mind because I get the right hint to
understand the difference about the asset allocation with or without an
asset with weight equals to zero. See the thread.

About the asset allocation difference between tseries and Rmetrics, using
the same parameters (returns, solver, etc.), I will check your suggestion.

Thanks a lot for your support,
Alberto Santini
-- 
View this message in context: http://www.nabble.com/portfolio.optim-and-assets-with-weigth-equals-to-zero...-tp19315076p19320091.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Fri Sep  5 01:42:14 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 4 Sep 2008 19:42:14 -0400
Subject: [R-SIG-Finance] [R-sig-finance] portfolio.optim and assets with
	weigth equals to zero...
In-Reply-To: <19315076.post@talk.nabble.com>
References: <19315076.post@talk.nabble.com>
Message-ID: <971536df0809041642l3b5b82bdx5d2ea1601a48a5b@mail.gmail.com>

Your question has already been answered but note that the code
can be simplified somewhat giving the following (which is not
exactly the same as yours but may be what you meant):

library(tseries)
qq <- do.call(cbind, sapply(sym, get.hist.quote, start = "2006-09-03",
	compression = "w", quote = "Close", simplify = FALSE))
res <- portfolio.optim(na.omit(diff(log(qq))))
res$pw


On Thu, Sep 4, 2008 at 12:57 PM, Alberto Santini
<albertosantini at gmail.com> wrote:
>
> After posting  on R-Help it makes sense posting on r-sig-finance too.
>
> I don't understand a particular output of portfolio.optim (tseries).
> I have 4 assets and the portfolio.optim returns an asset with weight equals
> to zero.
> If I do a portfolio.optim with 3 assets, without the asset with weight
> equals to zero,
> it returns a completely different result.
>
> That's I would expected the same weights as the run with 4 assets.
>
> Below the code.
>
> Thanks in advance,
> Alberto Santini
>
>
> -------------------------------------
>
> require(tseries)
>
> f.mi <- coredata(get.hist.quote("F.MI", start="2006-09-03", compression="w",
> quote="Close"))
> eng.mi <- coredata(get.hist.quote("ENG.MI", start="2006-09-03",
> compression="w", quote="Close"))
> tis.mi <- coredata(get.hist.quote("TIS.MI", start="2006-09-03",
> compression="w", quote="Close"))
> spmib <- coredata(get.hist.quote("^SPMIB", start="2006-09-03",
> compression="w", quote="Close"))
>
> f.mi.rets <- diff(log(f.mi[1:(length(f.mi)-1)]))
> eng.mi.rets <- diff(log(eng.mi[1:(length(eng.mi)-1)]))
> tis.mi.rets <- diff(log(tis.mi[1:(length(tis.mi)-1)]))
> spmib.rets <- diff(log(spmib[1:(length(spmib)-1)]))
>
> x <- cbind(f.mi.rets, eng.mi.rets, tis.mi.rets, spmib.rets)
> res <- portfolio.optim(x);
> res$pw
>
> x2 <- cbind(f.mi.rets, eng.mi.rets, spmib.rets)
> res <- portfolio.optim(x2);
> res$pw
>
> ----------------------------------------
>
>
> I tried with Rmetrics too, but the behaviour is the same. So I am missing
> something...
> Very interesting to understand why. :)
>
> I think it's a numerical problem: maybe, zero is not zero, it's very near to
> zero.
>
> Regards,
> Alberto Santini
>
> -----------------------------
>
> require(fPortfolio)
> require(tseries)
>
> assets <- c(
>    "F.MI",
>    "ENG.MI",
>    "TIS.MI",
>    "^SPMIB"
> )
>
> f.mi <- get.hist.quote("F.MI", start="2006-09-03", end="2008-09-03",
>    compression="w", quote="Close")
> eng.mi <- get.hist.quote("ENG.MI", start="2006-09-03", end="2008-09-03",
>    compression="w", quote="Close")
> tis.mi <- get.hist.quote("TIS.MI", start="2006-09-03", end="2008-09-03",
>    compression="w", quote="Close")
> spmib <- get.hist.quote("^SPMIB", start="2006-09-03", end="2008-09-03",
>    compression="w", quote="Close")
>
> X <- cbind(f.mi[1:(length(f.mi)-1)], eng.mi[1:(length(eng.mi)-1)],
> tis.mi[1:(length(tis.mi)-1)], spmib[1:(length(spmib)-1)])
> colnames(X) <- assets
>
> R <- as.timeSeries(returns(X))
>
> # Spec = portfolioSpec(model = list(type = c("MV"),
> #  estimator = c("mean", "cov"), tailRisk = list(), params = list()),
> #  portfolio = list(weights = NULL, targetReturn = 0,
> #    targetRisk = 0, targetAlpha = 0.05, riskFreeRate = NULL,
> #    nFrontierPoints = 50),
> #  solver = list(solver = c("quadprog"), trace = FALSE))
> Spec = portfolioSpec()
> frontier <- portfolioFrontier(R, Spec, c("minW[1:nAssets]=0"))
> # weightsSlider(frontier)
>
> ptf <-
> frontier at portfolio$weights[which.min(getTargetRisk(frontier)[,1])+1,]*100
> ptf
>
> assets2 <- c(
>    "ENG.MI",
>    "^SPMIB"
> )
>
> X2 <- cbind(eng.mi[1:(length(eng.mi)-1)], spmib[1:(length(spmib)-1)])
> colnames(X2) <- assets2
>
> R2 <- as.timeSeries(returns(X2))
> frontier <- portfolioFrontier(R2, Spec, c("minW[1:nAssets]=0"))
>
> ptf <-
> frontier at portfolio$weights[which.min(getTargetRisk(frontier)[,1])+1,]*100
> ptf
>
> -----------------------------
>
> The asset allocation (in Rmetrics):
>
>
>    F.MI   ENG.MI   TIS.MI   ^SPMIB
>  0.00000 24.30756  0.00000 75.69244
>
>  ENG.MI   ^SPMIB
> 24.48980 75.51020
>
>
>> cov(R)
>
>               F.MI       ENG.MI       TIS.MI       ^SPMIB
> F.MI   0.0029905040 0.0008974621 0.0010289887 0.0009954717
> ENG.MI 0.0008974621 0.0014935276 0.0004278133 0.0002809398
> TIS.MI 0.0010289887 0.0004278133 0.0052708621 0.0007186357
> ^SPMIB 0.0009954717 0.0002809398 0.0007186357 0.0005617209
>> cov(R2)
>             ENG.MI       ^SPMIB
> ENG.MI 0.0014197889 0.0002578205
> ^SPMIB 0.0002578205 0.0005791276
>
> I would think that if a weight is within epsilon of zero, then removing it
> from the computation should have minimal impact on the result.
>
>> eigen(cov(R))
> $values
> [1] 0.0060129474 0.0030233942 0.0010997423 0.0001805307
>
> $vectors
>           [,1]       [,2]        [,3]        [,4]
> [1,] -0.4145084  0.7702681  0.36304798 -0.32103879
> [2,] -0.1770771  0.3541481 -0.91735850  0.04094150
> [3,] -0.8700674 -0.4871272 -0.02330958 -0.07173836
> [4,] -0.1995222  0.2096978  0.16157418  0.94345721
>
>> eigen(cov(R2))
> $values
> [1] 0.0014925599 0.0005063566
>
> $vectors
>           [,1]       [,2]
> [1,] -0.9623985  0.2716415
> [2,] -0.2716415 -0.9623985
>
> The difference of the asset allocation is small. It is not so for
> portfolio.optim. I will investigate the difference between Rmetric and
> tseries, because in my context the solver is the same (quadprog).
>
> Maybe the measure of the risk used in Rmetrics is not the variance as used
> in tseries and it is less sensitive. In fact, the two asset allocations are
> different.
>
> I have short constraints and the weights don't change if I change the order
> of the assets.
>
>
> Thanks in advance,
> Alberto Santini
> --
> View this message in context: http://www.nabble.com/portfolio.optim-and-assets-with-weigth-equals-to-zero...-tp19315076p19315076.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From Weiyang.Lim at henderson.com  Fri Sep  5 10:14:50 2008
From: Weiyang.Lim at henderson.com (Weiyang Lim)
Date: Fri, 5 Sep 2008 09:14:50 +0100
Subject: [R-SIG-Finance] Difficulty getting desired returns with
	returns(fSeries) function
Message-ID: <AC9CF1A3B3582B4A847998B84DE457E4B7118D9A41@UKLONEXCPDV40.HDS.Int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080905/7c4fd785/attachment.pl>

From agehr at mozart.depaul.edu  Fri Sep  5 15:12:41 2008
From: agehr at mozart.depaul.edu (Adam Gehr)
Date: Fri, 5 Sep 2008 08:12:41 -0500 (CDT)
Subject: [R-SIG-Finance] Difficulty getting desired returns with
 returns(fSeries) function
In-Reply-To: <AC9CF1A3B3582B4A847998B84DE457E4B7118D9A41@UKLONEXCPDV40.HDS.Int>
References: <AC9CF1A3B3582B4A847998B84DE457E4B7118D9A41@UKLONEXCPDV40.HDS.Int>
Message-ID: <Pine.LNX.4.63.0809050811290.32286@mozart.depaul.edu>

The program is calculating ln(Pt/Pt-1) not (Pt/Pt-1)-1



On Fri, 5 Sep 2008, Weiyang Lim wrote:

> Dear users,
>
> I tried to use the getReturns/returns function available with the fSeries package but it did not appear to give me the results generated by the same function in S-Plus finmetrics. I am not sure what is happening or what goes behind the calculations.
>
> I have the code
>
> my.ts <- timeSeries(data = DataSeries) which generates the time series
>
>                        37
> 1970-01-01        NA
> 1970-01-02        NA
> 1970-01-03        NA
> 1970-01-04        NA
> 1970-01-05        NA
> 1970-01-06        NA
> 1970-01-07        NA
> 1970-01-08        NA
> 1970-01-09        NA
> 1970-01-10        NA
> 1970-01-11        6500
> 1970-01-12        8100
> 1970-01-13        8010
> 1970-01-14        7930
> .                       .
> .                       .
> .                       .
> .                       .
> .                       .
>
> Then I used
>
> dummystock.rs <- getReturns(my.ts, type = "discrete", na.rm = F, trim = F, percentage = T) which generates the returns
>
>                        37
> 1970-01-01        NA
> 1970-01-02        NA
> 1970-01-03        NA
> 1970-01-04        NA
> 1970-01-05        NA
> 1970-01-06        NA
> 1970-01-07        NA
> 1970-01-08        NA
> 1970-01-09        NA
> 1970-01-10        NA
> 1970-01-11        NA
> 1970-01-12        22.0061885
> 1970-01-13        -1.1173301
> 1970-01-14        -1.0037725
> .                       .
> .                       .
> .                       .
> .                       .
>
> However, what I really want is 24.61, -1.111, and -0.99 respectively rather than the returns above. The calculation I wanted for returns was (8100-6500)/6500 * 100 (for the first return) = 24.61 rather than 22.0061885.
>
> I am using R version 2.7.0.
>
> I wonder if anyone can give me any kind advice as to how to achieve the returns I hope to have.
>
> Many thanks.
>
> Best Regards,
> Wy
>
>
>
>
>
> **********************************************************
>
> The information provided in this e-mail is confidential and is for the sole use of the recipient. It may not be disclosed, copied or distributed in any form without the express permission of Henderson Global Investors and to the extent that it is passed on, care must be taken to ensure that this is in a form which accurately reflects the information presented here.
>
> Whilst Henderson Global Investors believe that the information is correct at the date of this e-mail, no warranty or representation is given to this effect and no responsibility can be accepted by Henderson Global Investors to any end users for any action taken on the basis of this information.
>
> Henderson Global Investors is the name under which Henderson Global Investors Limited (registered no. 906355), Henderson Fund Management plc (registered no. 2607112), Henderson Investment Funds Limited (registered no. 2678531), Henderson Investment Management Limited (registered no. 1795354) Henderson Alternative Investment Advisor Limited (registered no. 962757) and Henderson Equity Partners Limited (registered no.2606646) (each incorporated and registered in England and Wales with registered office at 4 Broadgate, London EC2M 2DA and authorised and regulated by the Financial Services Authority) provide investment products and services.  Henderson Secretarial Services Limited (incorporated and registered in England and Wales, registered no. 1471624, registered office 4 Broadgate, London EC2M 2DA) is the name under which company secretarial services are provided. All these companies are wholly owned subsidiaries of Henderson Group plc (incorporated and registered in England
  !
> and Wales, registered no. 2072534, registered office 4 Broadgate, London EC2M 2DA).
>
> We may record telephone calls or email for our mutual protection and to improve customer service.
>
> **********************************************************
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From albertosantini at gmail.com  Fri Sep  5 15:53:00 2008
From: albertosantini at gmail.com (Alberto Santini)
Date: Fri, 5 Sep 2008 06:53:00 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] portfolio.optim and assets with
 weigth equals to zero...
In-Reply-To: <45E2767FFD024545ACE0830D3E6AD395@HPR>
References: <19315076.post@talk.nabble.com>
	<45E2767FFD024545ACE0830D3E6AD395@HPR>
Message-ID: <19331721.post@talk.nabble.com>




Robert Iquiapaza wrote:
> 
> Alberto,
> Your data is a little bit mess because of NAs. The different 'treatment' 
> given to NAs by "coredata(get.hist.quote" and "as.timeSeries(returns" may 
> explain some of the differences in the results. You are passing different 
> set of returns. The first method passes a wrong returns matrix.
> 

Ok... I verified again and I note NAs. In my Rmetrics example I introduced
NAs in the return time series, because, in the week, the day of the close
price is different for each asset.

Thanks,
Alberto
-- 
View this message in context: http://www.nabble.com/portfolio.optim-and-assets-with-weigth-equals-to-zero...-tp19315076p19331721.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Fri Sep  5 16:31:51 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 5 Sep 2008 09:31:51 -0500
Subject: [R-SIG-Finance] Difficulty getting desired returns with
	returns(fSeries) function
In-Reply-To: <AC9CF1A3B3582B4A847998B84DE457E4B7118D9A41@UKLONEXCPDV40.HDS.Int>
References: <AC9CF1A3B3582B4A847998B84DE457E4B7118D9A41@UKLONEXCPDV40.HDS.Int>
Message-ID: <e8e755250809050731y51cc0e5aoa6dc9d6c88f861f8@mail.gmail.com>

Try:

library(quantmod)

my.ts <- timeSeries(c(rep(NA,10),6500,8100,8010,7930),
as.Date('1970-01-01')+0:13)

Delt(my.ts)
           Delt.1.arithmetic
1970-01-01                NA
1970-01-02                NA
1970-01-03                NA
1970-01-04                NA
1970-01-05                NA
1970-01-06                NA
1970-01-07                NA
1970-01-08                NA
1970-01-09                NA
1970-01-10                NA
1970-01-11                NA
1970-01-12       0.246153846
1970-01-13      -0.011111111
1970-01-14      -0.009987516

What you are getting, as Adam points out, is log returns.

> Delt(my.ts, type='log')
            Delt.1.log
1970-01-01          NA
1970-01-02          NA
1970-01-03          NA
1970-01-04          NA
1970-01-05          NA
1970-01-06          NA
1970-01-07          NA
1970-01-08          NA
1970-01-09          NA
1970-01-10          NA
1970-01-11          NA
1970-01-12  0.22006188
1970-01-13 -0.01117330
1970-01-14 -0.01003773

The xts library also has a family of functions to calculate period
returns (and much more).

?periodReturn

HTH
Jeff
On Fri, Sep 5, 2008 at 3:14 AM, Weiyang Lim <Weiyang.Lim at henderson.com> wrote:
> Dear users,
>
> I tried to use the getReturns/returns function available with the fSeries package but it did not appear to give me the results generated by the same function in S-Plus finmetrics. I am not sure what is happening or what goes behind the calculations.
>
> I have the code
>
> my.ts <- timeSeries(data = DataSeries) which generates the time series
>
>                        37
> 1970-01-01        NA
> 1970-01-02        NA
> 1970-01-03        NA
> 1970-01-04        NA
> 1970-01-05        NA
> 1970-01-06        NA
> 1970-01-07        NA
> 1970-01-08        NA
> 1970-01-09        NA
> 1970-01-10        NA
> 1970-01-11        6500
> 1970-01-12        8100
> 1970-01-13        8010
> 1970-01-14        7930
> .                       .
> .                       .
> .                       .
> .                       .
> .                       .
>
> Then I used
>
> dummystock.rs <- getReturns(my.ts, type = "discrete", na.rm = F, trim = F, percentage = T) which generates the returns
>
>                        37
> 1970-01-01        NA
> 1970-01-02        NA
> 1970-01-03        NA
> 1970-01-04        NA
> 1970-01-05        NA
> 1970-01-06        NA
> 1970-01-07        NA
> 1970-01-08        NA
> 1970-01-09        NA
> 1970-01-10        NA
> 1970-01-11        NA
> 1970-01-12        22.0061885
> 1970-01-13        -1.1173301
> 1970-01-14        -1.0037725
> .                       .
> .                       .
> .                       .
> .                       .
>
> However, what I really want is 24.61, -1.111, and -0.99 respectively rather than the returns above. The calculation I wanted for returns was (8100-6500)/6500 * 100 (for the first return) = 24.61 rather than 22.0061885.
>
> I am using R version 2.7.0.
>
> I wonder if anyone can give me any kind advice as to how to achieve the returns I hope to have.
>
> Many thanks.
>
> Best Regards,
> Wy
>
>
>
>
>
> **********************************************************
>
> The information provided in this e-mail is confidential and is for the sole use of the recipient. It may not be disclosed, copied or distributed in any form without the express permission of Henderson Global Investors and to the extent that it is passed on, care must be taken to ensure that this is in a form which accurately reflects the information presented here.
>
> Whilst Henderson Global Investors believe that the information is correct at the date of this e-mail, no warranty or representation is given to this effect and no responsibility can be accepted by Henderson Global Investors to any end users for any action taken on the basis of this information.
>
> Henderson Global Investors is the name under which Henderson Global Investors Limited (registered no. 906355), Henderson Fund Management plc (registered no. 2607112), Henderson Investment Funds Limited (registered no. 2678531), Henderson Investment Management Limited (registered no. 1795354) Henderson Alternative Investment Advisor Limited (registered no. 962757) and Henderson Equity Partners Limited (registered no.2606646) (each incorporated and registered in England and Wales with registered office at 4 Broadgate, London EC2M 2DA and authorised and regulated by the Financial Services Authority) provide investment products and services.  Henderson Secretarial Services Limited (incorporated and registered in England and Wales, registered no. 1471624, registered office 4 Broadgate, London EC2M 2DA) is the name under which company secretarial services are provided. All these companies are wholly owned subsidiaries of Henderson Group plc (incorporated and registered in England !
>  and Wales, registered no. 2072534, registered office 4 Broadgate, London EC2M 2DA).
>
> We may record telephone calls or email for our mutual protection and to improve customer service.
>
> **********************************************************
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From rbali at ufmg.br  Fri Sep  5 16:33:56 2008
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Fri, 5 Sep 2008 10:33:56 -0400
Subject: [R-SIG-Finance] Difficulty getting desired returns
	withreturns(fSeries) function
References: <AC9CF1A3B3582B4A847998B84DE457E4B7118D9A41@UKLONEXCPDV40.HDS.Int>
Message-ID: <26DFAD08BD4A44808CEB9EE287AB7D57@HPR>

Try 'method' instead of 'type'. This works

getReturns(MSFT, method = "discrete", na.rm = F, trim = F, percentage = T)

Robert

--------------------------------------------------
From: "Weiyang Lim" <Weiyang.Lim at henderson.com>
Sent: Friday, September 05, 2008 4:14 AM
To: <r-sig-finance at stat.math.ethz.ch>
Subject: [R-SIG-Finance] Difficulty getting desired returns 
withreturns(fSeries) function

> Dear users,
>
> I tried to use the getReturns/returns function available with the fSeries 
> package but it did not appear to give me the results generated by the same 
> function in S-Plus finmetrics. I am not sure what is happening or what 
> goes behind the calculations.
>
> I have the code
>
> my.ts <- timeSeries(data = DataSeries) which generates the time series
>
>                        37
> 1970-01-01        NA
> 1970-01-02        NA
> 1970-01-03        NA
> 1970-01-04        NA
> 1970-01-05        NA
> 1970-01-06        NA
> 1970-01-07        NA
> 1970-01-08        NA
> 1970-01-09        NA
> 1970-01-10        NA
> 1970-01-11        6500
> 1970-01-12        8100
> 1970-01-13        8010
> 1970-01-14        7930
> .                       .
> .                       .
> .                       .
> .                       .
> .                       .
>
> Then I used
>
> dummystock.rs <- getReturns(my.ts, type = "discrete", na.rm = F, trim = F, 
> percentage = T) which generates the returns
>
>                        37
> 1970-01-01        NA
> 1970-01-02        NA
> 1970-01-03        NA
> 1970-01-04        NA
> 1970-01-05        NA
> 1970-01-06        NA
> 1970-01-07        NA
> 1970-01-08        NA
> 1970-01-09        NA
> 1970-01-10        NA
> 1970-01-11        NA
> 1970-01-12        22.0061885
> 1970-01-13        -1.1173301
> 1970-01-14        -1.0037725
> .                       .
> .                       .
> .                       .
> .                       .
>
> However, what I really want is 24.61, -1.111, and -0.99 respectively 
> rather than the returns above. The calculation I wanted for returns was 
> (8100-6500)/6500 * 100 (for the first return) = 24.61 rather than 
> 22.0061885.
>
> I am using R version 2.7.0.
>
> I wonder if anyone can give me any kind advice as to how to achieve the 
> returns I hope to have.
>
> Many thanks.
>
> Best Regards,
> Wy
>


From Weiyang.Lim at henderson.com  Mon Sep  8 10:08:29 2008
From: Weiyang.Lim at henderson.com (Weiyang Lim)
Date: Mon, 8 Sep 2008 09:08:29 +0100
Subject: [R-SIG-Finance] Summary Difficulty getting desired returns with
 returns(fSeries) function
Message-ID: <AC9CF1A3B3582B4A847998B84DE457E4B7118D9B42@UKLONEXCPDV40.HDS.Int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080908/0fc839ee/attachment.pl>

From jorge.nieves at moorecap.com  Wed Sep 10 16:49:47 2008
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Wed, 10 Sep 2008 10:49:47 -0400
Subject: [R-SIG-Finance] Sequence ID
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF75D9EF@NYC-XCH3.win.moorecap.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080910/d40e2153/attachment.pl>

From martin.becker at mx.uni-saarland.de  Wed Sep 10 17:04:10 2008
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Wed, 10 Sep 2008 17:04:10 +0200
Subject: [R-SIG-Finance] Sequence ID
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF75D9EF@NYC-XCH3.win.moorecap.com>
References: <D595C0E05185614C90515F1E8A2D4CBF75D9EF@NYC-XCH3.win.moorecap.com>
Message-ID: <48C7E1EA.5000707@mx.uni-saarland.de>

Dear Jorge,

try rle():

test = c(0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0)
myrle<-rle(test)
myrle$values[myrle$values==1]<-1000*seq_len(sum(myrle$values==1))
inverse.rle(myrle)

Best wishes,

  Martin


Jorge Nieves wrote:
> Hi,
>
> First I would like to apologize if this is not the right forum for the
> following question. 
>
> I have the following sequence  of ZEROS  and ONES. I would like to ID
> each  "package" of  ONES with a unique identifier.
>
> test = c(0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0)
>
>  An example could be the following vector
>
> id
> =c(0,0,0,0,1000,1000,1000,1000,0,0,0,2000,2000,2000,2000,2000,0,0,0,3000
> ,3000,3000,0)
>
> Where all the consecutive ONES have the same ID. I can create the ID
> using a loop, but I was wondering if anyone could recommend  another
> method. I suspect that something in the area of factors might help, but
> I do not know what will be the best approach. Any recommendations will
> be highly appreciated.
>
>
>
> Jorge Nieves
> Phone 212.782.7083
> Fax 212.642.7644
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>   

-- 
Dr. Martin Becker
Statistics and Econometrics
Saarland University
Campus C3 1, Room 206
66123 Saarbruecken
Germany


From ggrothendieck at gmail.com  Wed Sep 10 17:35:43 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 10 Sep 2008 11:35:43 -0400
Subject: [R-SIG-Finance] Sequence ID
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF75D9EF@NYC-XCH3.win.moorecap.com>
References: <D595C0E05185614C90515F1E8A2D4CBF75D9EF@NYC-XCH3.win.moorecap.com>
Message-ID: <971536df0809100835uc376a3fxde30a6e54b2f31a2@mail.gmail.com>

Try this:

1000 * cumsum(pmax(0, diff(c(0, test)))) * test

On Wed, Sep 10, 2008 at 10:49 AM, Jorge Nieves
<jorge.nieves at moorecap.com> wrote:
>
> Hi,
>
> First I would like to apologize if this is not the right forum for the
> following question.
>
> I have the following sequence  of ZEROS  and ONES. I would like to ID
> each  "package" of  ONES with a unique identifier.
>
> test = c(0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0)
>
>  An example could be the following vector
>
> id
> =c(0,0,0,0,1000,1000,1000,1000,0,0,0,2000,2000,2000,2000,2000,0,0,0,3000
> ,3000,3000,0)
>
> Where all the consecutive ONES have the same ID. I can create the ID
> using a loop, but I was wondering if anyone could recommend  another
> method. I suspect that something in the area of factors might help, but
> I do not know what will be the best approach. Any recommendations will
> be highly appreciated.
>
>
>
> Jorge Nieves
> Phone 212.782.7083
> Fax 212.642.7644
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jorge.nieves at moorecap.com  Wed Sep 10 17:38:51 2008
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Wed, 10 Sep 2008 11:38:51 -0400
Subject: [R-SIG-Finance] Sequence ID
In-Reply-To: <971536df0809100835uc376a3fxde30a6e54b2f31a2@mail.gmail.com>
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF75D9F1@NYC-XCH3.win.moorecap.com>

Thanks,

I received two suggestions that work..

One by Martin Becker

test = c(0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0)
myrle<-rle(test)
myrle$values[myrle$values==1]<-1000*seq_len(sum(myrle$values==1))
inverse.rle(myrle)

And the other by Gabor (see bellow) 

Thanks a lot !!!!

Jorge

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Wednesday, September 10, 2008 11:36 AM
To: Jorge Nieves
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Sequence ID

Try this:

1000 * cumsum(pmax(0, diff(c(0, test)))) * test

On Wed, Sep 10, 2008 at 10:49 AM, Jorge Nieves
<jorge.nieves at moorecap.com> wrote:
>
> Hi,
>
> First I would like to apologize if this is not the right forum for the

> following question.
>
> I have the following sequence  of ZEROS  and ONES. I would like to ID 
> each  "package" of  ONES with a unique identifier.
>
> test = c(0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0)
>
>  An example could be the following vector
>
> id
> =c(0,0,0,0,1000,1000,1000,1000,0,0,0,2000,2000,2000,2000,2000,0,0,0,30
> 00
> ,3000,3000,0)
>
> Where all the consecutive ONES have the same ID. I can create the ID 
> using a loop, but I was wondering if anyone could recommend  another 
> method. I suspect that something in the area of factors might help, 
> but I do not know what will be the best approach. Any recommendations 
> will be highly appreciated.
>
>
>
> Jorge Nieves
> Phone 212.782.7083
> Fax 212.642.7644
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From chicane at bluewin.ch  Wed Sep 10 17:55:26 2008
From: chicane at bluewin.ch (Manuel Gasser)
Date: Wed, 10 Sep 2008 17:55:26 +0200
Subject: [R-SIG-Finance] Simple Problem using hngarchFit (fOptions)
Message-ID: <48C7EDEE.50002@bluewin.ch>

Dear all,

I'm stranding on a quite simple problem trying to work with a 
Heston-Nandi-Garch Model built in fOptions (Rmetrics):

The manual states the following:

hngarchFit(*x*, model = list(lambda = -0.5, omega = var(x), alpha =
0.1 * var(x), beta = 0.1, gamma = 0, rf = 0), symmetric = TRUE,
trace = FALSE, title = NULL, description = NULL, ...)


where *x *is, according to the Arguments,  "an univariate vector or time 
series". Maybe that's a complete newbie question to ask, but WHAT SORT 
OF DATA DOES X NEED? daily returns, daily logreturns, daily log(St) ?

Any help is appreciated!

Best Regards


From ag at jet-sa.ch  Mon Sep  8 15:21:10 2008
From: ag at jet-sa.ch (Arno gaboury)
Date: Mon, 8 Sep 2008 15:21:10 +0200
Subject: [R-SIG-Finance] get historical quote
Message-ID: <005201c911b5$c27c7960$47756c20$@ch>

Hi,

 

First of all, let me tell you I am a very beginner in the R-Project, and it
is far from my usual business. I am a futures trader trying to build a
risk/management tool. I know little about statistics & programming
languages. Please, be kind in all your answers!

I want to create a correlation matrix for about 25 future products, and will
need to download historical data each day.

I am a CQG user, and interested in retrieving the quotes from my system
better than from Yahoo. Does anyone has ever wrote something about using CQG
?

Then, I need to indicate a time stamp as I want a daily close of all markets
in the same time, otherwise returns won't be comparable. I have no idea how
to indicate this.

 

Thank You in advance for any hints,

 

Arno

 

 

 

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080908/532742e9/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/x-pkcs7-signature
Size: 3068 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080908/532742e9/attachment.bin>

From dutt.debashis at gmail.com  Mon Sep  1 00:33:23 2008
From: dutt.debashis at gmail.com (Debashis Dutta)
Date: Mon, 1 Sep 2008 01:33:23 +0300
Subject: [R-SIG-Finance] Urgent on the help
In-Reply-To: <B9AEC8265E202D4BA6540EA32F49579004771D2C@LDNPCMEU301VEUA.INTRANET.BARCAPINT.COM>
References: <B9AEC8265E202D4BA6540EA32F49579004771D2C@LDNPCMEU301VEUA.INTRANET.BARCAPINT.COM>
Message-ID: <37673c2d0808311533t46b34da5s5e33d1c4da097015@mail.gmail.com>

Hi Yuilei,

For immediate reference, I enclose two papers. See if  these come to your
use. give me a little time to run through the codes.

Kind Regards,
Debashis


On 22/07/2008, Yunlei.Hu at barclayscapital.com <Yunlei.Hu at barclayscapital.com>
wrote:
>
>
> Dear all
>
> I am using quadratic programming to solve the portfolio optimization in
> cosidering transaction cost. Is there any R optimization package can do
> this?
>
> Solve.QP require the positive definite matrix in Dmat, while in my case,
> this matrix in the objective function is not positive definite.
>
> >  It is in a bit of emergency. I would be really appreciated if anybody
> > can give me the reply ASAP.
> >
> > Many thanks
> >  Yunlei
> >
> _______________________________________________
>
> This e-mail may contain information that is confidential, privileged or
> otherwise protected from disclosure. If you are not an intended recipient of
> this e-mail, do not duplicate or redistribute it by any means. Please delete
> it and any attachments and notify the sender that you have received it in
> error. Unless specifically indicated, this e-mail is not an offer to buy or
> sell or a solicitation to buy or sell any securities, investment products or
> other financial product or service, an official confirmation of any
> transaction, or an official statement of Barclays. Any views or opinions
> presented are solely those of the author and do not necessarily represent
> those of Barclays. This e-mail is subject to terms available at the
> following link: www.barcap.com/emaildisclaimer. By messaging with Barclays
> you consent to the foregoing.  Barclays Capital is the investment banking
> division of Barclays Bank PLC, a company registered in England (number
> 1026167) with its registered offi!
> ce at 1 Churchill Place, London, E14 5HP.  This email may relate to or be
> sent from other members of the Barclays Group.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080901/048dabfc/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: paper_nag.pdf
Type: application/pdf
Size: 192447 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080901/048dabfc/attachment.pdf>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: po.pdf
Type: application/pdf
Size: 235469 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080901/048dabfc/attachment-0001.pdf>

From Rory.WINSTON at rbs.com  Thu Sep 11 10:18:49 2008
From: Rory.WINSTON at rbs.com (Rory.WINSTON at rbs.com)
Date: Thu, 11 Sep 2008 09:18:49 +0100
Subject: [R-SIG-Finance] get historical quote
In-Reply-To: <005201c911b5$c27c7960$47756c20$@ch>
References: <005201c911b5$c27c7960$47756c20$@ch>
Message-ID: <F5AB27C222D4164382737207F9869F2B51FDEB581B@LONMC01014.rbsres07.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080911/835a251e/attachment.pl>

From jeff.a.ryan at gmail.com  Thu Sep 11 14:54:00 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 11 Sep 2008 07:54:00 -0500
Subject: [R-SIG-Finance] get historical quote
In-Reply-To: <005201c911b5$c27c7960$47756c20$@ch>
References: <005201c911b5$c27c7960$47756c20$@ch>
Message-ID: <e8e755250809110554k7c4eec7di48e3cdcb50475fc8@mail.gmail.com>

More hints might be available if you worked up an example yourself (in
pseudocode???) before asking the list to do the same.

You could aggregate to daily data using to.daily in the 'xts' package.
 This would remove the time element (assuming you have one, once again
an example of two from you would help).

Jeff

>
> Then, I need to indicate a time stamp as I want a daily close of all markets
> in the same time, otherwise returns won't be comparable. I have no idea how
> to indicate this.
>

-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From swtzang at gmail.com  Thu Sep 11 15:20:46 2008
From: swtzang at gmail.com (ShyhWeir Tzang)
Date: Thu, 11 Sep 2008 21:20:46 +0800
Subject: [R-SIG-Finance] Heston Nandi Garch Option pricing
Message-ID: <c17037a10809110620k1c871d9cgd89d108e97c32621@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080911/f3abba1f/attachment.pl>

From jeff.a.ryan at gmail.com  Thu Sep 11 16:33:48 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 11 Sep 2008 09:33:48 -0500
Subject: [R-SIG-Finance] get historical quote
In-Reply-To: <00a101c9141a$0fb9a660$2f2cf320$@ch>
References: <005201c911b5$c27c7960$47756c20$@ch>
	<F5AB27C222D4164382737207F9869F2B51FDEB581B@LONMC01014.rbsres07.net>
	<00a101c9141a$0fb9a660$2f2cf320$@ch>
Message-ID: <e8e755250809110733k55ffd4cctfe140d5ed2b7d4b5@mail.gmail.com>

Looking at the CQG site leads me to think that there is no public API,
so Rory's suggestion of exporting/importing is probably the only
plausible solution.

You can extend getSymbols in quantmod to handle that process, or
obviously just write a stand-alone function to do the same.  If you
either, reposting your code here would be beneficial to all.

Both Bloomberg (RBloomberg) and IB (IBrokers) have R-based APIs for
most data functionality.  IBrokers is also acquiring access to the
order routing side of IB as well.

HTH
Jeff

On Thu, Sep 11, 2008 at 9:24 AM, Arno gaboury <ag at jet-sa.ch> wrote:
> I will try to see what I can do with the to.daily function.
> As for the use of CQG, I was just wandering if someone had already been
> using it with R. I am rather more comfortable downloading data from my feed
> vendor better than from Yahoo. Can't find any literature about it. It seems
> people use more Bloomberg or IB.
>
>
>
>
>
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From etheber at gmx.de  Fri Sep 12 10:03:16 2008
From: etheber at gmx.de (Thomas Etheber)
Date: Fri, 12 Sep 2008 10:03:16 +0200
Subject: [R-SIG-Finance] Reverse Optimisation
Message-ID: <20080912080316.153980@gmx.net>

Dear All,

i am trying to do a reverse portfolio optimisation with R to calculate the implied return vector of the corresponding assetclasses as recommended in Sharpe 1974. Of course the optimisation problem is in a way restricted in form of minimum and maximum assetclass weights.

I am looking for a way to formulate the lagrange function with the relevant restrictions in R. Do you know of some R functionality to handle this problem? 


Thx in advance for your help

Thomas
--


From rkevinburton at charter.net  Sat Sep 13 00:27:40 2008
From: rkevinburton at charter.net (rkevinburton at charter.net)
Date: Fri, 12 Sep 2008 18:27:40 -0400
Subject: [R-SIG-Finance] Seasonal time-series.
Message-ID: <20080912182740.YKV32.449952.root@mp07>

I have a general question that although I will speak in specifics, what I am really after is a process.

I have a time series that I have removed the trend and seasonality from using the 'R' method 'stl. What remains is the "remainder" and that is the subject of this question. 

First I looked at the ACF and PACF of the series and it plot seem to die down. The ACF seems to die down in more or less a linear fashion. The PACF oscilates but seems to die down at what looks like an exponential rate.

Based on this information I tried fitting an ARIMA model with p = 1, d = 1, and q = 1. The residuals of this fit and a Box-Ljung test indicated that this didn't fit properly. I looked again at the PACF and noticed that the first lag that was below 0.05 was at lag 10 so I set p = 10 and it still didn't seem to fit (based on the tests of the residuals and the Box-Ljung test). I am not sure where to proceed from here. I have dabbled a little in ARCH models and I have run what Tsay in his book "Analysis of Financial Time Series" says are ARCH effect tests. These tests seem to indicate that there is an ARCH effect in this "remainder" data. I would rather not go there if I can help it a) I don't have alot of experience with ARCH models b) there isn't alot of data (examples) that I have seen on developing those models with 'R'. The other possibility that I have not tried is to not use 'stl' to remove the seasonality and instead let the ARIMA model incorporate it into the fitted model. 

My eventual goal would be to combine all of this into a model that I could use for forecast. Using stl it seems that I am almost there. The trend fits to an ARMA model readily, of course the seasonal component can be readily added to a forcast model. The only thing that remains is the pesky "remainder".

Thoughts?

Thank you.

Kevin


From niheaven at hotmail.com  Mon Sep 15 16:03:13 2008
From: niheaven at hotmail.com (Hsiao-nan Cheung)
Date: Mon, 15 Sep 2008 22:03:13 +0800
Subject: [R-SIG-Finance] Financial Econometrics
Message-ID: <BAY126-DS4D246EDE8A9121FA37C9ED9520@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080915/c51a84e1/attachment.pl>

From ggrothendieck at gmail.com  Mon Sep 15 16:45:26 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 15 Sep 2008 10:45:26 -0400
Subject: [R-SIG-Finance] Financial Econometrics
In-Reply-To: <BAY126-DS4D246EDE8A9121FA37C9ED9520@phx.gbl>
References: <BAY126-DS4D246EDE8A9121FA37C9ED9520@phx.gbl>
Message-ID: <971536df0809150745i694b0a6at1437ae4a15cd9665@mail.gmail.com>

Check out the FinTS package on CRAN (and associated book).

On Mon, Sep 15, 2008 at 10:03 AM, Hsiao-nan Cheung <niheaven at hotmail.com> wrote:
> Hi,
>
>
>
> I??ll be a assistant professor on econometrics in the following term. Since
> the students are mainly of dept. of finance, my professor want the teaching
> materials be about finance and not about macroeconomics. Is there any good
> books about financial econometrics (not something about time series
> analysis)? Something by examples is the best.
>
>
>
> Thanks
>
>
>
> Hsiao-nan Cheung
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From tobias.verbeke at gmail.com  Mon Sep 15 17:46:17 2008
From: tobias.verbeke at gmail.com (Tobias Verbeke)
Date: Mon, 15 Sep 2008 17:46:17 +0200
Subject: [R-SIG-Finance] Financial Econometrics
In-Reply-To: <BAY126-DS4D246EDE8A9121FA37C9ED9520@phx.gbl>
References: <BAY126-DS4D246EDE8A9121FA37C9ED9520@phx.gbl>
Message-ID: <48CE8349.2000907@telenet.be>

Hi,

> I??ll be a assistant professor on econometrics in the following term. Since
> the students are mainly of dept. of finance, my professor want the teaching
> materials be about finance and not about macroeconomics. Is there any good
> books about financial econometrics (not something about time series
> analysis)? Something by examples is the best.

Could the book 'Applied Econometrics with R' by Christian Kleiber and
Achim Zeileis be useful ?

http://www.springer.com/economics/econometrics/book/978-0-387-77316-2

HTH,
Tobias


From Xiaochen.Sun at brunel.ac.uk  Mon Sep 15 18:03:22 2008
From: Xiaochen.Sun at brunel.ac.uk (Xiaochen Sun)
Date: Mon, 15 Sep 2008 17:03:22 +0100
Subject: [R-SIG-Finance] Financial Econometrics
References: <BAY126-DS4D246EDE8A9121FA37C9ED9520@phx.gbl>
	<48CE8349.2000907@telenet.be>
Message-ID: <E386E504246A9249A9176B5BEEC13B6F7B9056@UXEXMBU116.academic.windsor>

Maybe David Ruppert's , "Statistics and Finance An Introduction" , http://www.amazon.com/Statistics-Finance-Introduction-David-Ruppert/dp/0387202706
 

________________________________

From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of Tobias Verbeke
Sent: Mon 15/09/2008 16:46
To: Hsiao-nan Cheung
Cc: R-SIG-FINANCE
Subject: Re: [R-SIG-Finance] Financial Econometrics



Hi,

> I??ll be a assistant professor on econometrics in the following term. Since
> the students are mainly of dept. of finance, my professor want the teaching
> materials be about finance and not about macroeconomics. Is there any good
> books about financial econometrics (not something about time series
> analysis)? Something by examples is the best.

Could the book 'Applied Econometrics with R' by Christian Kleiber and
Achim Zeileis be useful ?

http://www.springer.com/economics/econometrics/book/978-0-387-77316-2

HTH,
Tobias

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From BChiquoine at tiff.org  Mon Sep 15 19:49:38 2008
From: BChiquoine at tiff.org (Chiquoine, Ben)
Date: Mon, 15 Sep 2008 13:49:38 -0400
Subject: [R-SIG-Finance] Compound Currency Options
Message-ID: <AC36F1084FDE8E4CAFD5CF2409F051D56359EB@vsw3exch2.tiff.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080915/8b1d6752/attachment.pl>

From ezivot at u.washington.edu  Mon Sep 15 20:26:30 2008
From: ezivot at u.washington.edu (Eric Zivot)
Date: Mon, 15 Sep 2008 11:26:30 -0700
Subject: [R-SIG-Finance] Financial Econometrics
In-Reply-To: <971536df0809150745i694b0a6at1437ae4a15cd9665@mail.gmail.com>
Message-ID: <200809151826.m8FIQU0b001604@smtp.washington.edu>

Depends on the level of your students. Are these Phd students, MBA students,
undergrads, in the US or outside? What is the course title? Without this
information, any recommendation is essentially useless.

There are many excellent Phd level books and a few good applied books for
MBA students and advanced undergrads. 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Gabor
Grothendieck
Sent: Monday, September 15, 2008 7:45 AM
To: Hsiao-nan Cheung
Cc: R-SIG-FINANCE
Subject: Re: [R-SIG-Finance] Financial Econometrics

Check out the FinTS package on CRAN (and associated book).

On Mon, Sep 15, 2008 at 10:03 AM, Hsiao-nan Cheung <niheaven at hotmail.com>
wrote:
> Hi,
>
>
>
> I??ll be a assistant professor on econometrics in the following term. 
> Since the students are mainly of dept. of finance, my professor want 
> the teaching materials be about finance and not about macroeconomics. 
> Is there any good books about financial econometrics (not something 
> about time series analysis)? Something by examples is the best.
>
>
>
> Thanks
>
>
>
> Hsiao-nan Cheung
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From BChiquoine at tiff.org  Mon Sep 15 22:56:13 2008
From: BChiquoine at tiff.org (Chiquoine, Ben)
Date: Mon, 15 Sep 2008 16:56:13 -0400
Subject: [R-SIG-Finance] Iterative equation solver
Message-ID: <AC36F1084FDE8E4CAFD5CF2409F051D5635A8E@vsw3exch2.tiff.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080915/d18b6431/attachment.pl>

From m_olshansky at yahoo.com  Tue Sep 16 03:58:02 2008
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Mon, 15 Sep 2008 18:58:02 -0700 (PDT)
Subject: [R-SIG-Finance] Iterative equation solver
In-Reply-To: <AC36F1084FDE8E4CAFD5CF2409F051D5635A8E@vsw3exch2.tiff.local>
Message-ID: <13490.38248.qm@web32203.mail.mud.yahoo.com>

Hi Ben,

The function is non-linear but it is of only one argument, so you can use uniroot to find it's root. Moreover, the function is monotone, so there should be no problem finding it's zero. For reasonable strike prices it should behave quite well, otherwise it can be very flat and finding a very accurate root may be difficult, but first of all you can overcome this by using either midpoint or linear interpolation between the current end-points (you may need to compute the function values quite accurately), and secondly, if the function is very flat and your zero is not very accurate, it will have a very small effect on the option price.

Regards,

Moshe.


--- On Tue, 16/9/08, Chiquoine, Ben <BChiquoine at tiff.org> wrote:

> From: Chiquoine, Ben <BChiquoine at tiff.org>
> Subject: [R-SIG-Finance] Iterative equation solver
> To: r-sig-finance at stat.math.ethz.ch
> Received: Tuesday, 16 September, 2008, 6:56 AM
> Hi
> 
>  
> 
> I am trying to solve for the "Critical Price" in
> a compound option.
> This involves solving a very non linear equation that I
> think can only
> be solved iteratively.  My question is, is there an
> iterative solve
> function (similar to the solver in excel) written anywhere
> for R?  I've
> seen a couple of posts that seem similar to this one but so
> far I have
> been unable to find an existing function.  Any feedback
> would be greatly
> appreciated!
> 
>  
> 
> Thanks,
> 
>  
> 
> Ben
> 
> 
> ___________________________________________
> This message and any attached documents contain
> information which may be confidential, subject to 
> privilege or exempt from disclosure under applicable
> law. These materials are solely for the use of the 
> intended recipient. If you are not the intended 
> recipient of this transmission, you are hereby 
> notified that any distribution, disclosure, printing, 
> copying, storage, modification or the taking of any
> action in reliance upon this transmission is strictly
> prohibited. Delivery of this message to any person
> other than the intended recipient shall not
> compromise or waive such confidentiality, privilege
> or exemption from disclosure as to this 
> communication. 
> 
> If you have received this communication in error, 
> please notify the sender immediately and delete
> this message from your system. 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From ddyudd at sina.com  Tue Sep 16 05:11:37 2008
From: ddyudd at sina.com (ddyudd at sina.com)
Date: Tue, 16 Sep 2008 11:11:37 +0800 
Subject: [R-SIG-Finance] Help(R(com)_server)
Message-ID: <20080916031137.5C010AA661A@mail3-126.sinamail.sina.com.cn>


-------------- ???? --------------


.sinamailpaper-0{cursor:text;}.sinamailpaper-0 td,.sinamailpaper-0 textarea,.sinamailpaper-0 input,.sinamailpaper-0 br,.sinamailpaper-0 div,.sinamailpaper-0 span{font-size:14px;font-family:"??",Verdana,Arial,Helvetica,sans-serif;line-height:1.5;}.sinamailpaper-0 p{*margin:0.2em auto;}.sinamailpaper-0 img{border:0;}.sinamailpaper-0 pre{white-space:normal;}.sinamailpaper-0 form{margin:0;}

 Hi,
   I chose to use C#.NET for the interface language to build some financial application with the R packages.Since there was not a .NET component bundled with R, I used the R-(D) COM interface for the automation. Thank what you make very much to dedicate unselfishly . There have two questions :
1) Connecting Error!   Run envrionment :Visualstudio2005  ,WindowsXp
using STATCONNECTORCLNTLib;
using StatConnectorCommonLib;
using STATCONNECTORSRVLib;
StatConnector conR = new STATCONNECTORSRVLib.StatConnectorClass();   
 conR.Init("R");
    conR.EvaluateNoReturn("library(fCalendar)");
    conR.Evaluate("x1
#  Sys.timeDate() is a function  belong to  "fCalendar" package , and "fCalendar" package has already been  downloaded on the local machine. The code above can be run  in  "R"  platform right.
2) If the "source" funcion can be used ?
conR.EvaluateNoReturn("source(\" d:\\myfile.r \",echo=T)"); // occur " this object is static ,don't allow this operation! "
Thanks a  lot.
 
Bryan
 
 
 


???????????????????

-------------------------------------------------------------------
???????????????????(http://space.sina.com.cn/ )

From niheaven at hotmail.com  Tue Sep 16 06:56:52 2008
From: niheaven at hotmail.com (Hsiao-nan Cheung)
Date: Tue, 16 Sep 2008 12:56:52 +0800
Subject: [R-SIG-Finance] Financial Econometrics
In-Reply-To: <200809151826.m8FIQU0b001604@smtp.washington.edu>
References: <971536df0809150745i694b0a6at1437ae4a15cd9665@mail.gmail.com>
	<200809151826.m8FIQU0b001604@smtp.washington.edu>
Message-ID: <BAY126-DS2F09A4D4D526F24095CBCD94D0@phx.gbl>

My students are mainly MBA students, and partially PhDs. The course title is 'Advanced Econometrics'. Although economic examples are okay, I prefer some examples on finance since this is their major. Next year they'll take a lecture named 'Time Series Analysis', so in this class there is less contents about TS.

> -----Original Message-----
> From: Eric Zivot [mailto:ezivot at u.washington.edu]
> Sent: Tuesday, September 16, 2008 2:27 AM
> To: 'Hsiao-nan Cheung'
> Cc: 'R-SIG-FINANCE'
> Subject: RE: [R-SIG-Finance] Financial Econometrics
> 
> Depends on the level of your students. Are these Phd students, MBA
> students,
> undergrads, in the US or outside? What is the course title? Without
> this
> information, any recommendation is essentially useless.
> 
> There are many excellent Phd level books and a few good applied books
> for
> MBA students and advanced undergrads.
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Gabor
> Grothendieck
> Sent: Monday, September 15, 2008 7:45 AM
> To: Hsiao-nan Cheung
> Cc: R-SIG-FINANCE
> Subject: Re: [R-SIG-Finance] Financial Econometrics
> 
> Check out the FinTS package on CRAN (and associated book).
> 
> On Mon, Sep 15, 2008 at 10:03 AM, Hsiao-nan Cheung
> <niheaven at hotmail.com>
> wrote:
> > Hi,
> >
> >
> >
> > I??ll be a assistant professor on econometrics in the following term.
> > Since the students are mainly of dept. of finance, my professor want
> > the teaching materials be about finance and not about macroeconomics.
> > Is there any good books about financial econometrics (not something
> > about time series analysis)? Something by examples is the best.
> >
> >
> >
> > Thanks
> >
> >
> >
> > Hsiao-nan Cheung
> >
> >
> >
> >
> >
> >
> >        [[alternative HTML version deleted]]
> >
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From niheaven at hotmail.com  Tue Sep 16 07:41:19 2008
From: niheaven at hotmail.com (Hsiao-nan Cheung)
Date: Tue, 16 Sep 2008 13:41:19 +0800
Subject: [R-SIG-Finance] Financial Econometrics
In-Reply-To: <27496806.12831181221490627282.JavaMail.javamailuser@localhost>
References: <27496806.12831181221490627282.JavaMail.javamailuser@localhost>
Message-ID: <BAY126-DS2E8997BEFB6A186ACC160D94D0@phx.gbl>

Thanks, and are these book about traditional econometrics or time series?

Maybe it's just a illusion to find some econometrics on finance without ts...

HC

> -----Original Message-----
> From: markleeds at verizon.net [mailto:markleeds at verizon.net]
> Sent: Monday, September 15, 2008 10:57 PM
> To: Hsiao-nan Cheung
> Subject: RE: [R-SIG-Finance] Financial Econometrics
> 
> authors of texts  are below but i don't know the text names off the top
> of my head.
> 
> tsay
> zivot and wang
> hayashi
> mckinlay & lo
> gueriorox and monfort
> stephen taylor  ( out of print )
> 
> 
> On Mon, Sep 15, 2008 at 10:03 AM, Hsiao-nan Cheung wrote:
> 
> > Hi,
> >
> >
> > I??ll be a assistant professor on econometrics in the following term.
> > Since
> > the students are mainly of dept. of finance, my professor want the
> > teaching
> > materials be about finance and not about macroeconomics. Is there any
> > good
> > books about financial econometrics (not something about time series
> > analysis)? Something by examples is the best.
> >
> >
> > Thanks
> >
> >
> > Hsiao-nan Cheung
> >
> >
> >
> >
> > 	[[alternative HTML version deleted]]
> >
> >
> >
> >      ------------------------------
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.


From shane.conway at gmail.com  Tue Sep 16 15:24:06 2008
From: shane.conway at gmail.com (Shane Conway)
Date: Tue, 16 Sep 2008 09:24:06 -0400
Subject: [R-SIG-Finance] Financial Econometrics
Message-ID: <dd3243090809160624h53cce231m10782236562e4bc1@mail.gmail.com>

I recommend:

* Kennedy: "A Guide to Econometrics"
* Greene: "Econometric Analysis"


------------------------------

Thanks, and are these book about traditional econometrics or time series?

Maybe it's just a illusion to find some econometrics on finance without ts...

HC

> -----Original Message-----
> From: markleeds at verizon.net [mailto:markleeds at verizon.net]
> Sent: Monday, September 15, 2008 10:57 PM
> To: Hsiao-nan Cheung
> Subject: RE: [R-SIG-Finance] Financial Econometrics
>
> authors of texts  are below but i don't know the text names off the top
> of my head.
>
> tsay
> zivot and wang
> hayashi
> mckinlay & lo
> gueriorox and monfort
> stephen taylor  ( out of print )
>
>
> On Mon, Sep 15, 2008 at 10:03 AM, Hsiao-nan Cheung wrote:
>
> > Hi,
> >
> >
> > I??ll be a assistant professor on econometrics in the following term.
> > Since
> > the students are mainly of dept. of finance, my professor want the
> > teaching
> > materials be about finance and not about macroeconomics. Is there any
> > good
> > books about financial econometrics (not something about time series
> > analysis)? Something by examples is the best.
> >
> >
> > Thanks
> >
> >
> > Hsiao-nan Cheung
> >
> >
> >
> >
> > 	[[alternative HTML version deleted]]
> >
> >
> >
> >      ------------------------------
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.

From baj2107 at columbia.edu  Tue Sep 16 15:46:32 2008
From: baj2107 at columbia.edu (Bernd Jagla)
Date: Tue, 16 Sep 2008 09:46:32 -0400
Subject: [R-SIG-Finance] Morningstar rating
In-Reply-To: <dd3243090809160624h53cce231m10782236562e4bc1@mail.gmail.com>
References: <dd3243090809160624h53cce231m10782236562e4bc1@mail.gmail.com>
Message-ID: <6C5CFEF8DBE34B5D8B2FE8D0F65FF551@cgc.cpmc.columbia.edu>

I would like to download the Morningstar rating for a given stock. What
package do I need and how do I do it?

Thanks,

BJ


From hkahra at gmail.com  Tue Sep 16 15:52:26 2008
From: hkahra at gmail.com (Hannu Kahra)
Date: Tue, 16 Sep 2008 16:52:26 +0300
Subject: [R-SIG-Finance] Financial Econometrics
In-Reply-To: <dd3243090809160624h53cce231m10782236562e4bc1@mail.gmail.com>
References: <dd3243090809160624h53cce231m10782236562e4bc1@mail.gmail.com>
Message-ID: <3d35a2ca0809160652x76692aa3pb1f961fc4c52afcf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080916/26a869ee/attachment.pl>

From frainj at tcd.ie  Tue Sep 16 16:30:13 2008
From: frainj at tcd.ie (John Frain)
Date: Tue, 16 Sep 2008 15:30:13 +0100
Subject: [R-SIG-Finance] Financial Econometrics
In-Reply-To: <dd3243090809160624h53cce231m10782236562e4bc1@mail.gmail.com>
References: <dd3243090809160624h53cce231m10782236562e4bc1@mail.gmail.com>
Message-ID: <cfdde1650809160730w7b570444qf9c316235a8b7878@mail.gmail.com>

It does really depend on the prior
mathematical/statistical/econometric knowledge of your students.  If
it is really advanced econometrics then you might look at Hayashi or
Green or Hendry.   I would think of three possible divisions in
econometrics

1) macroeconometrics
2) microeconometrics
3) financial econometrics.

All three are based, to a large extent, on variations and
generalisations of the linear model and have a considerable overlap.

Macroeconometrics can be divided into two groups - structural
econometrics and what I might call atheoretical  econometrics which
includes ARMA GARCH Var and similar models.  ( I know that  some of
these models can include structural features).   Many structural
models include temporal dependencies which require time-series
methods.   What is important is that one understands that pure
time-series methods can only be used for forecasting.  Structural
methods are required for policy evaluation.  Generally samples size is
small and economic theory has a very important input.   Many
economists do not understand this point.

Microeconometrics usually involves the analysis of larger
cross-sections or panels of data.  As a cross-section is a picture of
a large numer of units taken at one time there is no temporal
dependence in the data.  As the sample size can be very big asymptotic
theory is generally sufficient.   In panel data the number of units is
very large relative to the number of time periods and most of the
asymptotic theory is in terms of the number of units.  There is a
limited amount of theory dealing with temporal dependence.  Standard
references include Wooldridge or Cameron and Trivedi.  Applications in
finance include credit rating and evaluation of new products.

Financial econometrics is more like the atheoretical macroeconometrics
with large samples.  One is generally concerned with forecasting
returns, variances and correlations for input to various procedures.
The emphasis here is slightly different.

At a more elementary level you might have a look at Heij, De Boer,
Frances, Kloek and van Dijk(2004), Econometric methods with
Applications in Business and Economics,Oxford or Brooks (2008),
Introductory Econometrics for Finance, Cambridge.  Both have a lot of
applied examples which you might find useful.

I have looked a the R library for 'Applied Econometrics with R' by
Christian Kleiber and Achim Zeileis and have ordered it from Amazon in
America as it is marked not available yet in Europe.  It looks very
interesting.  I have not mentioned the classic intermediate books  by
Maddala, Baltagi, Berndt, Dinaro and Johnson,  etc.  Any of these
might be worth considering.  It is important that one has a clear view
of the capabilities of the class and of the aims of the course.

Best Regards

John


2008/9/16 Shane Conway <shane.conway at gmail.com>:
> I recommend:
>
> * Kennedy: "A Guide to Econometrics"
> * Greene: "Econometric Analysis"
>
>
> ------------------------------
>
> Thanks, and are these book about traditional econometrics or time series?
>
> Maybe it's just a illusion to find some econometrics on finance without ts...
>
> HC
>
>> -----Original Message-----
>> From: markleeds at verizon.net [mailto:markleeds at verizon.net]
>> Sent: Monday, September 15, 2008 10:57 PM
>> To: Hsiao-nan Cheung
>> Subject: RE: [R-SIG-Finance] Financial Econometrics
>>
>> authors of texts  are below but i don't know the text names off the top
>> of my head.
>>
>> tsay
>> zivot and wang
>> hayashi
>> mckinlay & lo
>> gueriorox and monfort
>> stephen taylor  ( out of print )
>>
>>
>> On Mon, Sep 15, 2008 at 10:03 AM, Hsiao-nan Cheung wrote:
>>
>> > Hi,
>> >
>> >
>> > I??ll be a assistant professor on econometrics in the following term.
>> > Since
>> > the students are mainly of dept. of finance, my professor want the
>> > teaching
>> > materials be about finance and not about macroeconomics. Is there any
>> > good
>> > books about financial econometrics (not something about time series
>> > analysis)? Something by examples is the best.
>> >
>> >
>> > Thanks
>> >
>> >
>> > Hsiao-nan Cheung
>> >
>> >
>> >
>> >
>> >     [[alternative HTML version deleted]]
>> >
>> >
>> >
>> >      ------------------------------
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.



-- 
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

From Rory.WINSTON at rbs.com  Tue Sep 16 17:01:23 2008
From: Rory.WINSTON at rbs.com (Rory.WINSTON at rbs.com)
Date: Tue, 16 Sep 2008 16:01:23 +0100
Subject: [R-SIG-Finance] Financial Econometrics
In-Reply-To: <3d35a2ca0809160652x76692aa3pb1f961fc4c52afcf@mail.gmail.com>
References: <dd3243090809160624h53cce231m10782236562e4bc1@mail.gmail.com>
	<3d35a2ca0809160652x76692aa3pb1f961fc4c52afcf@mail.gmail.com>
Message-ID: <F5AB27C222D4164382737207F9869F2B51FDEB5836@LONMC01014.rbsres07.net>

I concur with Shane, Verbeek is a great book. I picked up the 2nd edition a while ago and it is one of the most lucid econometrics books I have seen so far.

Rory


Rory Winston
RBS Global Banking & Markets
Office: +44 20 7085 4476

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Hannu Kahra
Sent: 16 September 2008 14:52
To: Shane Conway
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Financial Econometrics

In addition to the previously mentioned books, I recommend Verbeek: A Guide to Modern Econometrics http://www.econ.kuleuven.ac.be/gme/

The 3rd edition of the book is now available http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470517697.html
I used the 2nd edition in my "advanced statistical methods" course and found the book very suitable for the course.

-Hannu

On Tue, Sep 16, 2008 at 4:24 PM, Shane Conway <shane.conway at gmail.com>wrote:

> I recommend:
>
> * Kennedy: "A Guide to Econometrics"
> * Greene: "Econometric Analysis"
>
>
> ------------------------------
>
> Thanks, and are these book about traditional econometrics or time series?
>
> Maybe it's just a illusion to find some econometrics on finance
> without ts...
>
> HC
>
> > -----Original Message-----
> > From: markleeds at verizon.net [mailto:markleeds at verizon.net]
> > Sent: Monday, September 15, 2008 10:57 PM
> > To: Hsiao-nan Cheung
> > Subject: RE: [R-SIG-Finance] Financial Econometrics
> >
> > authors of texts  are below but i don't know the text names off the
> > top of my head.
> >
> > tsay
> > zivot and wang
> > hayashi
> > mckinlay & lo
> > gueriorox and monfort
> > stephen taylor  ( out of print )
> >
> >
> > On Mon, Sep 15, 2008 at 10:03 AM, Hsiao-nan Cheung wrote:
> >
> > > Hi,
> > >
> > >
> > > I  ll be a assistant professor on econometrics in the following term.
> > > Since
> > > the students are mainly of dept. of finance, my professor want the
> > > teaching materials be about finance and not about macroeconomics.
> > > Is there any good books about financial econometrics (not
> > > something about time series analysis)? Something by examples is
> > > the best.
> > >
> > >
> > > Thanks
> > >
> > >
> > > Hsiao-nan Cheung
> > >
> > >
> > >
> > >
> > >     [[alternative HTML version deleted]]
> > >
> > >
> > >
> > >      ------------------------------
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

        [[alternative HTML version deleted]]


***********************************************************************************
The Royal Bank of Scotland plc. Registered in Scotland No 90312. Registered Office: 36 St Andrew Square, Edinburgh EH2 2YB. 
Authorised and regulated by the Financial Services Authority 

This e-mail message is confidential and for use by the=2...{{dropped:22}}


From jorge.nieves at moorecap.com  Tue Sep 16 17:41:08 2008
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Tue, 16 Sep 2008 11:41:08 -0400
Subject: [R-SIG-Finance] Help with help
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF75DA4E@NYC-XCH3.win.moorecap.com>
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF75DA4F@NYC-XCH3.win.moorecap.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080916/92de26ec/attachment.pl>

From BChiquoine at tiff.org  Tue Sep 16 18:09:35 2008
From: BChiquoine at tiff.org (Chiquoine, Ben)
Date: Tue, 16 Sep 2008 12:09:35 -0400
Subject: [R-SIG-Finance] Help with help
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF75DA4F@NYC-XCH3.win.moorecap.com>
References: <D595C0E05185614C90515F1E8A2D4CBF75DA4E@NYC-XCH3.win.moorecap.com>
	<D595C0E05185614C90515F1E8A2D4CBF75DA4F@NYC-XCH3.win.moorecap.com>
Message-ID: <AC36F1084FDE8E4CAFD5CF2409F051D5635BCF@vsw3exch2.tiff.local>

I had the same problem when I first installed r.  I ended up
reinstalling selecting custom setup then when it asked me about the help
files I selected the html option (a change from the default).  This
seemed to fix the problem for me and was well worth my time as the
documentation for R actually seems to be pretty good.

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Jorge
Nieves
Sent: Tuesday, September 16, 2008 11:41 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Help with help


> Hi,
> 
> I recently installed R version 2.7.2(2008-08-25)  
> 
> I am having some difficulty with the help command. 
> 
> For example, I am using to use the TTR package. When I run help(SMA)
> in the browser, I get a blank page. I am not sure if something went
> wrong in the installation. Does anyone have any tips about how to fix
> fix this problem?
> 
> Thanks,
> 
> Jorge
> 
> 
> 
> 
> 

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
___________________________________________
This message and any attached documents contain
information which may be confidential, subject to 
privilege or exempt from disclosure under applicable
law. These materials are solely for the use of the 
intended recipient. If you are not the intended 
recipient of this transmission, you are hereby 
notified that any distribution, disclosure, printing, 
copying, storage, modification or the taking of any
action in reliance upon this transmission is strictly
prohibited. Delivery of this message to any person
other than the intended recipient shall not
compromise or waive such confidentiality, privilege
or exemption from disclosure as to this 
communication. 

If you have received this communication in error, 
please notify the sender immediately and delete
this message from your system. 



From ezivot at u.washington.edu  Tue Sep 16 19:03:25 2008
From: ezivot at u.washington.edu (Eric Zivot)
Date: Tue, 16 Sep 2008 10:03:25 -0700
Subject: [R-SIG-Finance] Financial Econometrics
In-Reply-To: <BAY126-DS2E8997BEFB6A186ACC160D94D0@phx.gbl>
Message-ID: <200809161703.m8GH3PaR002498@smtp.washington.edu>

Well, this is a bit of a challenge. The best overall book on financial
econometrics is Campbell, Lo and MacKinlay's book The Econometrics of
Financial Markets. They cover a very wide variety of topics on the
econometric analysis of financial models (return predictability, high
frequency and market microstructure, event studies, portfolio theory, factor
models, derivatives models, etc). Since most financial data is times series
(or panel oriented) the emphasis is on time series applications but there
are plenty of cross section topics as well. You might supplement the
material by a general discussion of GMM techniques as well as panel data
techniques. I would suggest using Cochrane's Asset Pricing Book together
with Hall's GMM book for this material. Unfortunately, this material is
probabably too advanced for most MBA students without a good stat
background. You might also consider Ruppert's book Statistics and Finance:
An Introduction or Carmona's book Statistical Analysis of Financial Data in
S-PLUS and pick the chapters that are not so much time series oriented. I
personally like these books quite a lot. Finally, another choice might be
Chris Brooks' book Introductory Econometridcs for Finance. However, this
book is fairly time series oriented but does have a solid treatment of
regression topics.
Hope this helps.
ez 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Hsiao-nan
Cheung
Sent: Monday, September 15, 2008 10:41 PM
To: markleeds at verizon.net
Cc: R-SIG-FINANCE
Subject: Re: [R-SIG-Finance] Financial Econometrics

Thanks, and are these book about traditional econometrics or time series?

Maybe it's just a illusion to find some econometrics on finance without
ts...

HC

> -----Original Message-----
> From: markleeds at verizon.net [mailto:markleeds at verizon.net]
> Sent: Monday, September 15, 2008 10:57 PM
> To: Hsiao-nan Cheung
> Subject: RE: [R-SIG-Finance] Financial Econometrics
> 
> authors of texts  are below but i don't know the text names off the 
> top of my head.
> 
> tsay
> zivot and wang
> hayashi
> mckinlay & lo
> gueriorox and monfort
> stephen taylor  ( out of print )
> 
> 
> On Mon, Sep 15, 2008 at 10:03 AM, Hsiao-nan Cheung wrote:
> 
> > Hi,
> >
> >
> > I??ll be a assistant professor on econometrics in the following term.
> > Since
> > the students are mainly of dept. of finance, my professor want the 
> > teaching materials be about finance and not about macroeconomics. Is 
> > there any good books about financial econometrics (not something 
> > about time series analysis)? Something by examples is the best.
> >
> >
> > Thanks
> >
> >
> > Hsiao-nan Cheung
> >
> >
> >
> >
> > 	[[alternative HTML version deleted]]
> >
> >
> >
> >      ------------------------------
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From wssecn at uol.com.br  Wed Sep 17 12:56:17 2008
From: wssecn at uol.com.br (Washington Santos da Silva)
Date: Wed, 17 Sep 2008 07:56:17 -0300
Subject: [R-SIG-Finance] Financial Econometrics
Message-ID: <K7C6DT$890151AB9A39E772C5951BF1E53BF8D7@uol.com.br>

  Another option:

 Introductory Econometrics for Finance
 Chris Brooks
http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521790185&ss=res

 Washington S. Silva


> I recommend:
> 
> * Kennedy: "A Guide to Econometrics"
> * Greene: "Econometric Analysis"
> 
> 
> ------------------------------
> 
> Thanks, and are these book about traditional econometrics or time series?
> 
> Maybe it's just a illusion to find some econometrics on finance without ts...
> 
> HC
> 
> > -----Original Message-----
> > From: markleeds at verizon.net [mailto:markleeds at verizon.net]
> > Sent: Monday, September 15, 2008 10:57 PM
> > To: Hsiao-nan Cheung
> > Subject: RE: [R-SIG-Finance] Financial Econometrics
> >
> > authors of texts  are below but i don't know the text names off the top
> > of my head.
> >
> > tsay
> > zivot and wang
> > hayashi
> > mckinlay & lo
> > gueriorox and monfort
> > stephen taylor  ( out of print )
> >
> >
> > On Mon, Sep 15, 2008 at 10:03 AM, Hsiao-nan Cheung wrote:
> >
> > > Hi,
> > >
> > >
> > > I??ll be a assistant professor on econometrics in the following term.
> > > Since
> > > the students are mainly of dept. of finance, my professor want the
> > > teaching
> > > materials be about finance and not about macroeconomics. Is there any
> > > good
> > > books about financial econometrics (not something about time series
> > > analysis)? Something by examples is the best.
> > >
> > >
> > > Thanks
> > >
> > >
> > > Hsiao-nan Cheung
> > >
> > >
> > >
> > >
> > > 	[[alternative HTML version deleted]]
> > >
> > >
> > >
> > >      ------------------------------
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From niheaven at hotmail.com  Wed Sep 17 15:56:07 2008
From: niheaven at hotmail.com (Hsiao-nan Cheung)
Date: Wed, 17 Sep 2008 21:56:07 +0800
Subject: [R-SIG-Finance] Financial Econometrics
In-Reply-To: <26883093.14665531221545002995.JavaMail.javamailuser@localhost>
References: <26883093.14665531221545002995.JavaMail.javamailuser@localhost>
Message-ID: <BAY126-DS44DA289D9F89ADBFC1D5CD94C0@phx.gbl>

Maybe I should specify detailed contents of my class. It is CLRM's OLS, BLUE, Dummy Variables, Collinearity, Heteroscedasticity, Autocorrelation, IV, Dynamic Econometric Models and Simultaneous-Equations Models. What I need is relating them with financial application.

And the educational background of the students? Er, it's sure they all have solid mathematic basis, and most of them should be really quantitative MBAs. This is just a essential advanced econometrics lecture and they'll get a more deeper lesson on time series.

Again, thank you for your help.

> -----Original Message-----
> From: markleeds at verizon.net [mailto:markleeds at verizon.net]
> Sent: 2008?9?16? 14:03
> To: Hsiao-nan Cheung; ezivot at u.washington.edu
> Subject: RE: [R-SIG-Finance] Financial Econometrics
> 
> Hi:  I guess you'd call them a mixture ? if you are mostly teaching mba
> students, then hayashi is not applicable. tsay might be but only for
> REALLY QUANTITATIVE MBA's. Stephen Taylor's new book ( I can't remember
> the name ) might not be bad.  i have it but I haven't looked at it so
> carefully.
> 
> I'm not sure what you mean by econometrics in finance without time
> series ? Wooldridge covers cross sectional type stuff
> and panel data but not necessarily finance related ? Mckinlay and Lo is
> not all time series. Maybe you mean like generalized method of moments
> books with respect to finance problems ?  I think there's a book by
> Hall
> that covers GMM and it may have some financial
> applications in it ?
> 
> Maybe Eric can answer better because I'm not  sure what you mean and I
> don't  want to give you bad advice. I will cc eric here because  he is
> much more equipped to answer. because I'm not clear on what topics you
> are trying to cover.  Also, if it's mostly MBA's, then you are
> definitely more limited in the texts that you can use so I would be
> EXTREMELY careful in deciding on a text, regardless of the topics you
> are trying
> to cover.
> 
> 
> 
> 
> 
> 
> 
> On Tue, Sep 16, 2008 at  1:41 AM, Hsiao-nan Cheung wrote:
> 
> > Thanks, and are these book about traditional econometrics or time
> > series?
> >
> > Maybe it's just a illusion to find some econometrics on finance
> > without ts...
> >
> > HC
> >
> >> -----Original Message-----
> >> From: markleeds at verizon.net [mailto:markleeds at verizon.net]
> >> Sent: Monday, September 15, 2008 10:57 PM
> >> To: Hsiao-nan Cheung
> >> Subject: RE: [R-SIG-Finance] Financial Econometrics
> >>
> >> authors of texts  are below but i don't know the text names off the
> >> top
> >> of my head.
> >>
> >> tsay
> >> zivot and wang
> >> hayashi
> >> mckinlay & lo
> >> gueriorox and monfort
> >> stephen taylor  ( out of print )
> >>
> >>
> >> On Mon, Sep 15, 2008 at 10:03 AM, Hsiao-nan Cheung wrote:
> >>
> >>> Hi,
> >>>
> >>>
> >>> I??ll be a assistant professor on econometrics in the following
> >>> term.
> >>> Since
> >>> the students are mainly of dept. of finance, my professor want the
> >>> teaching
> >>> materials be about finance and not about macroeconomics. Is there
> >>> any
> >>> good
> >>> books about financial econometrics (not something about time series
> >>> analysis)? Something by examples is the best.
> >>>
> >>>
> >>> Thanks
> >>>
> >>>
> >>> Hsiao-nan Cheung
> >>>
> >>>
> >>>
> >>>
> >>> 	[[alternative HTML version deleted]]
> >>>
> >>>
> >>>
> >>>      ------------------------------
> >>>
> >>> _______________________________________________
> >>> R-SIG-Finance at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >>> -- Subscriber-posting only.
> >>> -- If you want to post, subscribe first.


From liuzhigang626 at gmail.com  Thu Sep 18 05:29:19 2008
From: liuzhigang626 at gmail.com (=?GB2312?B?wfXWvrjV?=)
Date: Thu, 18 Sep 2008 11:29:19 +0800
Subject: [R-SIG-Finance] Winsorization
Message-ID: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080918/4456079c/attachment.pl>

From ajayshah at mayin.org  Thu Sep 18 06:31:00 2008
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu, 18 Sep 2008 10:01:00 +0530
Subject: [R-SIG-Finance] Winsorization
In-Reply-To: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com>
References: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com>
Message-ID: <20080918043100.GN311@lubyanka.local>

On Thu, Sep 18, 2008 at 11:29:19AM +0800, ?????? wrote:
> Dear all,
>        I am dealing with a data set with many outliers value. And it is said
> that a technique named winsorization or winsorising can
> reduce the influence of those extreme values. Did anyone use this skill
> before? And how to do it in S+ or R? Thank you.

Winsorisation is not a great idea. It is an adhoc procedure. Your test
statistics are all suspect if you have preprocessed the data in this
fashion.

If you can do robust regressions (e.g. use the R package `robust')
that is far better. Get on the r-sig-robust mailing list and start
learning! (At least, that's what I'm doing).

If you must do it, here's some code:

winsorise <- function(x, cutoff=0.01) {
  stopifnot(length(x)>0, cutoff>0)
  osd <-  sd(x)
  values <- quantile(x, p=c(cutoff,1-cutoff), na.rm=TRUE)
  winsorised.left <- x<values[1]
  winsorised.right <- x>values[2]       # From here on, I start writing into x
  x[winsorised.left] <- values[1]
  x[winsorised.right] <- values[2]
  list(winsorised=x,
       values=values,
       osd=osd, nsd=sd(x),
       winsorised.left=winsorised.left, winsorised.right=winsorised.right)
}

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From mel at altk.com  Thu Sep 18 08:07:59 2008
From: mel at altk.com (mel)
Date: Thu, 18 Sep 2008 08:07:59 +0200
Subject: [R-SIG-Finance] metastock
Message-ID: <48D1F03F.6090706@altk.com>

Hello,
Does anybody know if there may exist a R (or C/C++)
function for reading metastock data files ?
(or the metastock format is really too closed).
Thanks
Vincent


From patrick at burns-stat.com  Thu Sep 18 12:00:57 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 18 Sep 2008 11:00:57 +0100
Subject: [R-SIG-Finance] Winsorization
In-Reply-To: <20080918043100.GN311@lubyanka.local>
References: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com>
	<20080918043100.GN311@lubyanka.local>
Message-ID: <48D226D9.9020203@burns-stat.com>

I disagree with Ajay about the value of Winsorization.
Yes, it is ad hoc but it is simple to understand and
often results in reasonable answers.

It certainly depends on the context but if we are talking
about financial returns, then I haven't had positive
experience with traditional statistical robustness. 
(Given that my thesis was on robustness, I don't say
this lightly.)  Robustness often gives inferior answers
in finance (in my experience) even when it is obvious
that it "should" be the proper thing to do.  This is
a phenomenon that I don't understand.

The code that Ajay gives always truncates some fraction
of data in each tail.  Often Winsorization is thought of as
truncating only data that are too far from the center.  A
simple version of this is:

function(x, winsorize=5)
{
    s <- mad(x) * winsorize
    top <- median(x) +  s
    bot <- median(x) -  s
    x[x > top] <- top
    x[x < bot] <- bot
    x
}

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Ajay Shah wrote:
> On Thu, Sep 18, 2008 at 11:29:19AM +0800, ?????? wrote:
>   
>> Dear all,
>>        I am dealing with a data set with many outliers value. And it is said
>> that a technique named winsorization or winsorising can
>> reduce the influence of those extreme values. Did anyone use this skill
>> before? And how to do it in S+ or R? Thank you.
>>     
>
> Winsorisation is not a great idea. It is an adhoc procedure. Your test
> statistics are all suspect if you have preprocessed the data in this
> fashion.
>
> If you can do robust regressions (e.g. use the R package `robust')
> that is far better. Get on the r-sig-robust mailing list and start
> learning! (At least, that's what I'm doing).
>
> If you must do it, here's some code:
>
> winsorise <- function(x, cutoff=0.01) {
>   stopifnot(length(x)>0, cutoff>0)
>   osd <-  sd(x)
>   values <- quantile(x, p=c(cutoff,1-cutoff), na.rm=TRUE)
>   winsorised.left <- x<values[1]
>   winsorised.right <- x>values[2]       # From here on, I start writing into x
>   x[winsorised.left] <- values[1]
>   x[winsorised.right] <- values[2]
>   list(winsorised=x,
>        values=values,
>        osd=osd, nsd=sd(x),
>        winsorised.left=winsorised.left, winsorised.right=winsorised.right)
> }
>
>


From brian at braverock.com  Thu Sep 18 12:36:10 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 18 Sep 2008 05:36:10 -0500
Subject: [R-SIG-Finance] Winsorization
In-Reply-To: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com>
References: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com>
Message-ID: <1221734170.15019.14.camel@ubuntu.braverock.com>

On Thu, 2008-09-18 at 11:29 +0800, ??? wrote:
>        I am dealing with a data set with many outliers value. And it is said
> that a technique named winsorization or winsorising can
> reduce the influence of those extreme values. Did anyone use this skill
> before? And how to do it in S+ or R? Thank you.

We used this technique in an upcoming (Winter 2008) paper for the
Journal of Risk.  The robust data cleaning methods we propose will be in
that paper, and the code will be in the version of PerformanceAnalytics
that we are preparing for release right now.  

If you want the functions right now, please let me know, and I'll send
you the .R files you need prior to the release of the package.

Regards,

  - Brian

-- 
http://braverock.com/brian/
Ph: 773-459-4973 
IM: bgpbraverock


From brian at braverock.com  Thu Sep 18 12:47:13 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 18 Sep 2008 05:47:13 -0500
Subject: [R-SIG-Finance] Winsorization
In-Reply-To: <48D226D9.9020203@burns-stat.com>
References: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com>
	<20080918043100.GN311@lubyanka.local> <48D226D9.9020203@burns-stat.com>
Message-ID: <1221734833.15019.24.camel@ubuntu.braverock.com>

On Thu, 2008-09-18 at 11:00 +0100, Patrick Burns wrote:
> I disagree with Ajay about the value of Winsorization.
> Yes, it is ad hoc but it is simple to understand and
> often results in reasonable answers.
> 
> It certainly depends on the context but if we are talking
> about financial returns, then I haven't had positive
> experience with traditional statistical robustness. 
> (Given that my thesis was on robustness, I don't say
> this lightly.)  Robustness often gives inferior answers
> in finance (in my experience) even when it is obvious
> that it "should" be the proper thing to do.  This is
> a phenomenon that I don't understand.

I have to agree with Patrick.  We proposed an extension above and beyond
classic Winsorization that would only reduce the outliers that occured
beyond a certain confidence level (e.g. 95% or 99%).  Traditional robust
methods have a tendency to ignore outliers rather than simply reduce
their influence.  In measuring risk, this is clearly quite dangerous.
We found that by only cleaning outliers beyond a certain confidence, we
got much more stable and accurate out of sample predictions on a variety
of risk measures (as well as predictions that compared well to kernel
estimation and Monte Carlo methods with lower computational burden).

Like I said in my previous email, code and documentation available upon
request or in the next version of PerformanceAnalytics.

Regards,

    - Brian

-- 
http://braverock.com/brian/
Ph: 773-459-4973 
IM: bgpbraverock


From Zeno.Adams at ebs.edu  Thu Sep 18 13:21:17 2008
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Thu, 18 Sep 2008 13:21:17 +0200
Subject: [R-SIG-Finance] Winsorization
References: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com>
Message-ID: <9064522880125945B98983BBAECBA1CC21CADA@exchsrv001.ebs.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080918/04cce4b1/attachment.pl>

From tobias.verbeke at gmail.com  Thu Sep 18 13:56:43 2008
From: tobias.verbeke at gmail.com (Tobias Verbeke)
Date: Thu, 18 Sep 2008 13:56:43 +0200
Subject: [R-SIG-Finance] Winsorization
In-Reply-To: <9064522880125945B98983BBAECBA1CC21CADA@exchsrv001.ebs.local>
References: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com>
	<9064522880125945B98983BBAECBA1CC21CADA@exchsrv001.ebs.local>
Message-ID: <48D241FB.3060000@telenet.be>

Adams, Zeno wrote:

> what method are you planning to use?
> 
> If you want to do a regression analysis, there are better methods for this.
> The quantreg package can give you the least absolute deviation (LAD) estimate, (this corresponds to a quantile regression with the quantile tau = 0.5)
> which is robust to outliers.

For a structured overview of robust methods available in R,
see the robust statistics task view at

http://cran.r-project.org/web/views/Robust.html

HTH,
Tobias

> Another interesting approach is weighted least squares available in S-Plus. 
> See Martin, R. and Simin, T. (2003) "Outlier-Resistant Estimates of Beta" Financial Analysts Journal, September, pp. 56-69.
> 
> Zeno
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of ???
> Sent: Thu 9/18/2008 5:29 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Winsorization
>  
> Dear all,
>        I am dealing with a data set with many outliers value. And it is said
> that a technique named winsorization or winsorising can
> reduce the influence of those extreme values. Did anyone use this skill
> before? And how to do it in S+ or R? Thank you.
> 
> Zhigang Liu
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 
> 
> 
> 
> EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrung: Prof. Dr. Christopher Jahns, Rektor; Dr. Reimar Palte, Kanzler;  Sabine Fuchs, Prokuristin; Verwaltungsrat: Dr. Hellmut K. Albrecht, Vorsitzender
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
>


From Rory.WINSTON at rbs.com  Thu Sep 18 14:38:03 2008
From: Rory.WINSTON at rbs.com (Rory.WINSTON at rbs.com)
Date: Thu, 18 Sep 2008 13:38:03 +0100
Subject: [R-SIG-Finance] Winsorization
In-Reply-To: <48D226D9.9020203@burns-stat.com>
References: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com>
	<20080918043100.GN311@lubyanka.local> <48D226D9.9020203@burns-stat.com>
Message-ID: <F5AB27C222D4164382737207F9869F2B51FDEB584D@LONMC01014.rbsres07.net>

Hi Patrick

This is interesting - by inferior, do you mean that robust methods make assumptions about the distribution shape or variance that are violated by the type of distributions seen in financial returns, for instance?

Rory


Rory Winston
RBS Global Banking & Markets
Office: +44 20 7085 4476

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
Sent: 18 September 2008 11:01
To: Ajay Shah
Cc: r-sig-finance at stat.math.ethz.ch; ??????
Subject: Re: [R-SIG-Finance] Winsorization

I disagree with Ajay about the value of Winsorization.
Yes, it is ad hoc but it is simple to understand and often results in reasonable answers.

It certainly depends on the context but if we are talking about financial returns, then I haven't had positive experience with traditional statistical robustness.
(Given that my thesis was on robustness, I don't say this lightly.)  Robustness often gives inferior answers in finance (in my experience) even when it is obvious that it "should" be the proper thing to do.  This is a phenomenon that I don't understand.

The code that Ajay gives always truncates some fraction of data in each tail.  Often Winsorization is thought of as truncating only data that are too far from the center.  A simple version of this is:


function(x, winsorize=5)
{
    s <- mad(x) * winsorize
    top <- median(x) +  s
    bot <- median(x) -  s
    x[x > top] <- top
    x[x < bot] <- bot
    x
}

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Ajay Shah wrote:
> On Thu, Sep 18, 2008 at 11:29:19AM +0800, ?????? wrote:
>
>> Dear all,
>>        I am dealing with a data set with many outliers value. And it
>> is said that a technique named winsorization or winsorising can
>> reduce the influence of those extreme values. Did anyone use this
>> skill before? And how to do it in S+ or R? Thank you.
>>
>
> Winsorisation is not a great idea. It is an adhoc procedure. Your test
> statistics are all suspect if you have preprocessed the data in this
> fashion.
>
> If you can do robust regressions (e.g. use the R package `robust')
> that is far better. Get on the r-sig-robust mailing list and start
> learning! (At least, that's what I'm doing).
>
> If you must do it, here's some code:
>
> winsorise <- function(x, cutoff=0.01) {
>   stopifnot(length(x)>0, cutoff>0)
>   osd <-  sd(x)
>   values <- quantile(x, p=c(cutoff,1-cutoff), na.rm=TRUE)
>   winsorised.left <- x<values[1]
>   winsorised.right <- x>values[2]       # From here on, I start writing into x
>   x[winsorised.left] <- values[1]
>   x[winsorised.right] <- values[2]
>   list(winsorised=x,
>        values=values,
>        osd=osd, nsd=sd(x),
>        winsorised.left=winsorised.left,
> winsorised.right=winsorised.right)
> }
>
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

***********************************************************************************
The Royal Bank of Scotland plc. Registered in Scotland No 90312. Registered Office: 36 St Andrew Square, Edinburgh EH2 2YB. 
Authorised and regulated by the Financial Services Authority 

This e-mail message is confidential and for use by the=2...{{dropped:22}}


From ngottlieb at marinercapital.com  Thu Sep 18 16:27:38 2008
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Thu, 18 Sep 2008 10:27:38 -0400
Subject: [R-SIG-Finance] Winsorization
In-Reply-To: <1221734833.15019.24.camel@ubuntu.braverock.com>
References: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com><20080918043100.GN311@lubyanka.local>
	<48D226D9.9020203@burns-stat.com>
	<1221734833.15019.24.camel@ubuntu.braverock.com>
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D8801DBD3F9@500MAIL.goldbox.com>

Patrick:

Have you looked at Rousseau's, minimum volume ellipsoids (MVE) for
handling outliers?

Curious if so how you found this for handling outliers? 

Neil

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
Peterson
Sent: Thursday, September 18, 2008 6:47 AM
To: Patrick Burns
Cc: r-sig-finance at stat.math.ethz.ch; ??????
Subject: Re: [R-SIG-Finance] Winsorization

On Thu, 2008-09-18 at 11:00 +0100, Patrick Burns wrote:
> I disagree with Ajay about the value of Winsorization.
> Yes, it is ad hoc but it is simple to understand and often results in 
> reasonable answers.
> 
> It certainly depends on the context but if we are talking about 
> financial returns, then I haven't had positive experience with 
> traditional statistical robustness.
> (Given that my thesis was on robustness, I don't say this lightly.)  
> Robustness often gives inferior answers in finance (in my experience) 
> even when it is obvious that it "should" be the proper thing to do.  
> This is a phenomenon that I don't understand.

I have to agree with Patrick.  We proposed an extension above and beyond
classic Winsorization that would only reduce the outliers that occured
beyond a certain confidence level (e.g. 95% or 99%).  Traditional robust
methods have a tendency to ignore outliers rather than simply reduce
their influence.  In measuring risk, this is clearly quite dangerous.
We found that by only cleaning outliers beyond a certain confidence, we
got much more stable and accurate out of sample predictions on a variety
of risk measures (as well as predictions that compared well to kernel
estimation and Monte Carlo methods with lower computational burden).

Like I said in my previous email, code and documentation available upon
request or in the next version of PerformanceAnalytics.

Regards,

    - Brian

--
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
--------------------------------------------------------



This information is being sent at the recipient's reques...{{dropped:16}}


From ajayshah at mayin.org  Thu Sep 18 17:36:16 2008
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu, 18 Sep 2008 21:06:16 +0530
Subject: [R-SIG-Finance] Outliers in the market model that's used to
	estimate `beta' of a stock
Message-ID: <20080918153616.GA6633@lubyanka.local>

In continuation of the discussion on `Winsorisation' that has taken
place on r-sig-finance today, I thought I'd present all of you with an
interesting dataset and a question.

This data is the daily stock returns of the large Indian software firm
`Infosys'. (This is the symbol `INFY' on NASDAQ). It is a large number
of observations of daily returns (i.e. percentage changes of the
adjusted stock price).

Load the data in --

    print(load(url("http://www.mayin.org/ajayshah/tmp/infosys_mm.rda")))
    str(x)
    summary(x)
    sd(x)

The name `rj' is used for returns on Infosys, and `rM' is used for
returns on the stock market index (Nifty). There are three really
weird observations in this.

    weird.rj <- c(1896,2395)
    weird.rM <- 2672
    x[weird.rj,]
    x[weird.rM,]

As you can see, these observations are quite remarkable given the
small standard deviations that we saw above. There is absolutely no
measurement error here. These things actually happened.

Now consider a typical application: using this to estimate a market
model. The goal here is to estimate the coefficient of a regression of
rj on rM.

    # A regression with all obs
    summary(lm(rj ~ rM, data=x))

    # Drop the weird rj --
    summary(lm(rj ~ rM, data=x[-weird.rj,]))

    # Drop the weird rM --
    summary(lm(rj ~ rM, data=x[-weird.rM,]))

    # Drop both kinds of weird observations --
    summary(lm(rj ~ rM, data=x[-c(weird.rM,weird.rj),]))

    # Robust regressions
    library(MASS)
    summary(rlm(rj ~ rM, data=x))
    summary(rlm(rj ~ rM, method="MM", data=x))
    library(robust)
    summary(lmRob(rj ~ rM, data=x))
    library(quantreg)
    summary(rq(rj ~ rM, tau=0.5, data=x))

So you see, we have a variety of different estimates for the slope
(which is termed `beta' in finance). What value would you trust the
most?

And, would winsorisation using either my code
(https://stat.ethz.ch/pipermail/r-sig-finance/2008q3/002921.html) or
Patrick Burns' code
(https://stat.ethz.ch/pipermail/r-sig-finance/2008q3/002923.html) be a
good idea here?

I'm instinctively unhappy with any scheme based on discarding
observations that I'm absolutely sure have no measurement error. We
have to model the weirdness of this data generating process, not
ignore it.

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From patrick at burns-stat.com  Thu Sep 18 18:12:24 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 18 Sep 2008 17:12:24 +0100
Subject: [R-SIG-Finance] Winsorization
In-Reply-To: <F5AB27C222D4164382737207F9869F2B51FDEB584D@LONMC01014.rbsres07.net>
References: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com>	<20080918043100.GN311@lubyanka.local>
	<48D226D9.9020203@burns-stat.com>
	<F5AB27C222D4164382737207F9869F2B51FDEB584D@LONMC01014.rbsres07.net>
Message-ID: <48D27DE8.40809@burns-stat.com>

Rory,

No, quite the reverse.  Returns are unabashedly
long-tailed, which makes robustness the "right"
thing to do.  However, as Brian has said elsewhere
in this thread, a robust technique can be disastrously
worse than non-robust or silly ad hoc procedures.

Asking two questions is a good thing to do:

1) What do I want to do?
(It is surprising how often this seemingly obvious start
is slighted.)

2) Does the technique I'm using work well OUT OF
SAMPLE for THIS task?

Finance is a wild and wonderful place, and seems to have
it in for theoreticians.

Pat

Rory.WINSTON at rbs.com wrote:
> Hi Patrick
>
> This is interesting - by inferior, do you mean that robust methods make assumptions about the distribution shape or variance that are violated by the type of distributions seen in financial returns, for instance?
>
> Rory
>
>
> Rory Winston
> RBS Global Banking & Markets
> Office: +44 20 7085 4476
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
> Sent: 18 September 2008 11:01
> To: Ajay Shah
> Cc: r-sig-finance at stat.math.ethz.ch; ??????
> Subject: Re: [R-SIG-Finance] Winsorization
>
> I disagree with Ajay about the value of Winsorization.
> Yes, it is ad hoc but it is simple to understand and often results in reasonable answers.
>
> It certainly depends on the context but if we are talking about financial returns, then I haven't had positive experience with traditional statistical robustness.
> (Given that my thesis was on robustness, I don't say this lightly.)  Robustness often gives inferior answers in finance (in my experience) even when it is obvious that it "should" be the proper thing to do.  This is a phenomenon that I don't understand.
>
> The code that Ajay gives always truncates some fraction of data in each tail.  Often Winsorization is thought of as truncating only data that are too far from the center.  A simple version of this is:
>
>
> function(x, winsorize=5)
> {
>     s <- mad(x) * winsorize
>     top <- median(x) +  s
>     bot <- median(x) -  s
>     x[x > top] <- top
>     x[x < bot] <- bot
>     x
> }
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Ajay Shah wrote:
>   
>> On Thu, Sep 18, 2008 at 11:29:19AM +0800, ?????? wrote:
>>
>>     
>>> Dear all,
>>>        I am dealing with a data set with many outliers value. And it
>>> is said that a technique named winsorization or winsorising can
>>> reduce the influence of those extreme values. Did anyone use this
>>> skill before? And how to do it in S+ or R? Thank you.
>>>
>>>       
>> Winsorisation is not a great idea. It is an adhoc procedure. Your test
>> statistics are all suspect if you have preprocessed the data in this
>> fashion.
>>
>> If you can do robust regressions (e.g. use the R package `robust')
>> that is far better. Get on the r-sig-robust mailing list and start
>> learning! (At least, that's what I'm doing).
>>
>> If you must do it, here's some code:
>>
>> winsorise <- function(x, cutoff=0.01) {
>>   stopifnot(length(x)>0, cutoff>0)
>>   osd <-  sd(x)
>>   values <- quantile(x, p=c(cutoff,1-cutoff), na.rm=TRUE)
>>   winsorised.left <- x<values[1]
>>   winsorised.right <- x>values[2]       # From here on, I start writing into x
>>   x[winsorised.left] <- values[1]
>>   x[winsorised.right] <- values[2]
>>   list(winsorised=x,
>>        values=values,
>>        osd=osd, nsd=sd(x),
>>        winsorised.left=winsorised.left,
>> winsorised.right=winsorised.right)
>> }
>>
>>
>>     
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> ***********************************************************************************
> The Royal Bank of Scotland plc. Registered in Scotland No 90312. Registered Office: 36 St Andrew Square, Edinburgh EH2 2YB. 
> Authorised and regulated by the Financial Services Authority 
>  
> This e-mail message is confidential and for use by the 
> addressee only. If the message is received by anyone other 
> than the addressee, please return the message to the sender 
> by replying to it and then delete the message from your 
> computer. Internet e-mails are not necessarily secure. The 
> Royal Bank of Scotland plc does not accept responsibility for 
> changes made to this message after it was sent. 
>
> Whilst all reasonable care has been taken to avoid the 
> transmission of viruses, it is the responsibility of the recipient to 
> ensure that the onward transmission, opening or use of this 
> message and any attachments will not adversely affect its 
> systems or data. No responsibility is accepted by The 
> Royal Bank of Scotland plc in this regard and the recipient should carry 
> out such virus and other checks as it considers appropriate. 
> Visit our websites at: 
> www.rbs.com
> www.rbs.com/gbm
> www.rbsgc.com
> ***********************************************************************************
>
>
>
>


From patrick at burns-stat.com  Thu Sep 18 18:46:35 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 18 Sep 2008 17:46:35 +0100
Subject: [R-SIG-Finance] Winsorization
In-Reply-To: <0946E293C7C22A45A0E33BA14FAA8D8801DBD3F9@500MAIL.goldbox.com>
References: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com><20080918043100.GN311@lubyanka.local>
	<48D226D9.9020203@burns-stat.com>
	<1221734833.15019.24.camel@ubuntu.braverock.com>
	<0946E293C7C22A45A0E33BA14FAA8D8801DBD3F9@500MAIL.goldbox.com>
Message-ID: <48D285EB.2070304@burns-stat.com>

Neil,

I don't recall having the opportunity to think of MVE
in finance.  But I have tried the moral equivalent in
regression (when building risk models).  The results were
that high-breakdown regression did worst.  Best was
Huber M-estimation with quite mild robustness.
Least squares was almost as good as the best, and
better than almost all of the robust regressions.

Pat

ngottlieb at marinercapital.com wrote:
> Patrick:
>
> Have you looked at Rousseau's, minimum volume ellipsoids (MVE) for
> handling outliers?
>
> Curious if so how you found this for handling outliers? 
>
> Neil
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
> Peterson
> Sent: Thursday, September 18, 2008 6:47 AM
> To: Patrick Burns
> Cc: r-sig-finance at stat.math.ethz.ch; ??????
> Subject: Re: [R-SIG-Finance] Winsorization
>
> On Thu, 2008-09-18 at 11:00 +0100, Patrick Burns wrote:
>   
>> I disagree with Ajay about the value of Winsorization.
>> Yes, it is ad hoc but it is simple to understand and often results in 
>> reasonable answers.
>>
>> It certainly depends on the context but if we are talking about 
>> financial returns, then I haven't had positive experience with 
>> traditional statistical robustness.
>> (Given that my thesis was on robustness, I don't say this lightly.)  
>> Robustness often gives inferior answers in finance (in my experience) 
>> even when it is obvious that it "should" be the proper thing to do.  
>> This is a phenomenon that I don't understand.
>>     
>
> I have to agree with Patrick.  We proposed an extension above and beyond
> classic Winsorization that would only reduce the outliers that occured
> beyond a certain confidence level (e.g. 95% or 99%).  Traditional robust
> methods have a tendency to ignore outliers rather than simply reduce
> their influence.  In measuring risk, this is clearly quite dangerous.
> We found that by only cleaning outliers beyond a certain confidence, we
> got much more stable and accurate out of sample predictions on a variety
> of risk measures (as well as predictions that compared well to kernel
> estimation and Monte Carlo methods with lower computational burden).
>
> Like I said in my previous email, code and documentation available upon
> request or in the next version of PerformanceAnalytics.
>
> Regards,
>
>     - Brian
>
> --
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> --------------------------------------------------------
>
>  
>  
> This information is being sent at the recipient's requ...{{dropped:18}}


From markleeds at verizon.net  Thu Sep 18 18:58:02 2008
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 18 Sep 2008 11:58:02 -0500 (CDT)
Subject: [R-SIG-Finance] Outliers in the market model that's used to
 estimate `beta' of a stock
Message-ID: <15833186.24323431221757082212.JavaMail.javamailuser@localhost>

Hi: i don't know if you read "fooled by randomness" by Nassim Taleb ( 
spelling )  but he essentially says using very non statistical arguments 
but
strong nevertheless. ( it's not a stat or a quant finance book )  that 
outliers in finance are not modellable and don't claim that you can 
model
them because you'd be lying. In fact, he would say that a model works 
until it doesn't.

Anyway, it's an interesting book that sort of indirectly talks ( for a 
little too long actually. you can get what's he saying in the first 50 
pages and
  it's about 200 pages )  about your comment below so I figured I would 
just mention it in case you were interested.


On Thu, Sep 18, 2008 at 11:36 AM, Ajay Shah wrote:

> In continuation of the discussion on `Winsorisation' that has taken
> place on r-sig-finance today, I thought I'd present all of you with an
> interesting dataset and a question.
>
> This data is the daily stock returns of the large Indian software firm
> `Infosys'. (This is the symbol `INFY' on NASDAQ). It is a large number
> of observations of daily returns (i.e. percentage changes of the
> adjusted stock price).
>
> Load the data in --
>
> 
> print(load(url("http://www.mayin.org/ajayshah/tmp/infosys_mm.rda")))
>     str(x)
>     summary(x)
>     sd(x)
>
> The name `rj' is used for returns on Infosys, and `rM' is used for
> returns on the stock market index (Nifty). There are three really
> weird observations in this.
>
>     weird.rj <- c(1896,2395)
>     weird.rM <- 2672
>     x[weird.rj,]
>     x[weird.rM,]
>
> As you can see, these observations are quite remarkable given the
> small standard deviations that we saw above. There is absolutely no
> measurement error here. These things actually happened.
>
> Now consider a typical application: using this to estimate a market
> model. The goal here is to estimate the coefficient of a regression of
> rj on rM.
>
>     # A regression with all obs
>     summary(lm(rj ~ rM, data=x))
>
>     # Drop the weird rj --
>     summary(lm(rj ~ rM, data=x[-weird.rj,]))
>
>     # Drop the weird rM --
>     summary(lm(rj ~ rM, data=x[-weird.rM,]))
>
>     # Drop both kinds of weird observations --
>     summary(lm(rj ~ rM, data=x[-c(weird.rM,weird.rj),]))
>
>     # Robust regressions
>     library(MASS)
>     summary(rlm(rj ~ rM, data=x))
>     summary(rlm(rj ~ rM, method="MM", data=x))
>     library(robust)
>     summary(lmRob(rj ~ rM, data=x))
>     library(quantreg)
>     summary(rq(rj ~ rM, tau=0.5, data=x))
>
> So you see, we have a variety of different estimates for the slope
> (which is termed `beta' in finance). What value would you trust the
> most?
>
> And, would winsorisation using either my code
> (https://stat.ethz.ch/pipermail/r-sig-finance/2008q3/002921.html) or
> Patrick Burns' code
> (https://stat.ethz.ch/pipermail/r-sig-finance/2008q3/002923.html) be a
> good idea here?
>
> I'm instinctively unhappy with any scheme based on discarding
> observations that I'm absolutely sure have no measurement error. We
> have to model the weirdness of this data generating process, not
> ignore it.
>
> -- 
> Ajay Shah 
> http://www.mayin.org/ajayshah  ajayshah at mayin.org 
> http://ajayshahblog.blogspot.com
> <*(:-? - wizard who doesn't know the answer.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From ngottlieb at marinercapital.com  Thu Sep 18 19:16:36 2008
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Thu, 18 Sep 2008 13:16:36 -0400
Subject: [R-SIG-Finance] Winsorization
In-Reply-To: <48D285EB.2070304@burns-stat.com>
References: <26c73e350809172029s37606db8t4d053feefcc1d8aa@mail.gmail.com><20080918043100.GN311@lubyanka.local>
	<48D226D9.9020203@burns-stat.com>
	<1221734833.15019.24.camel@ubuntu.braverock.com>
	<0946E293C7C22A45A0E33BA14FAA8D8801DBD3F9@500MAIL.goldbox.com>
	<48D285EB.2070304@burns-stat.com>
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D8801DBD3FA@500MAIL.goldbox.com>

Hi Pat:

I remember trying MVE many years ago when doing some things in finance
using Principal components
To create correlations first then do PCA.

Might be worth a look at Rousseau's old paper on MVE, been awhile but
sure easy to find again if you web search.

Interesting about least squares versus robust regression. Do you have
any studies showing
This, would be interested.

Neil 

-----Original Message-----
From: Patrick Burns [mailto:patrick at burns-stat.com] 
Sent: Thursday, September 18, 2008 12:47 PM
To: Gottlieb, Neil
Cc: brian at braverock.com; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Winsorization

Neil,

I don't recall having the opportunity to think of MVE in finance.  But I
have tried the moral equivalent in regression (when building risk
models).  The results were that high-breakdown regression did worst.
Best was Huber M-estimation with quite mild robustness.
Least squares was almost as good as the best, and better than almost all
of the robust regressions.

Pat

ngottlieb at marinercapital.com wrote:
> Patrick:
>
> Have you looked at Rousseau's, minimum volume ellipsoids (MVE) for 
> handling outliers?
>
> Curious if so how you found this for handling outliers? 
>
> Neil
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
> Peterson
> Sent: Thursday, September 18, 2008 6:47 AM
> To: Patrick Burns
> Cc: r-sig-finance at stat.math.ethz.ch; ??????
> Subject: Re: [R-SIG-Finance] Winsorization
>
> On Thu, 2008-09-18 at 11:00 +0100, Patrick Burns wrote:
>   
>> I disagree with Ajay about the value of Winsorization.
>> Yes, it is ad hoc but it is simple to understand and often results in

>> reasonable answers.
>>
>> It certainly depends on the context but if we are talking about 
>> financial returns, then I haven't had positive experience with 
>> traditional statistical robustness.
>> (Given that my thesis was on robustness, I don't say this lightly.) 
>> Robustness often gives inferior answers in finance (in my experience)

>> even when it is obvious that it "should" be the proper thing to do.
>> This is a phenomenon that I don't understand.
>>     
>
> I have to agree with Patrick.  We proposed an extension above and 
> beyond classic Winsorization that would only reduce the outliers that 
> occured beyond a certain confidence level (e.g. 95% or 99%).  
> Traditional robust methods have a tendency to ignore outliers rather 
> than simply reduce their influence.  In measuring risk, this is
clearly quite dangerous.
> We found that by only cleaning outliers beyond a certain confidence, 
> we got much more stable and accurate out of sample predictions on a 
> variety of risk measures (as well as predictions that compared well to

> kernel estimation and Monte Carlo methods with lower computational
burden).
>
> Like I said in my previous email, code and documentation available 
> upon request or in the next version of PerformanceAnalytics.
>
> Regards,
>
>     - Brian
>
> --
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> --------------------------------------------------------
>
>  
>  
> This information is being sent at the recipient's request or with
their specific understanding. The recipient acknowledges that by sending
this information via electronic means, there is no absolute assurance
that the information will be free from third party access, use, or
further dissemination. This e-mail contains information that is
privileged and/or confidential and may be subject to legal restrictions
and penalties regarding its unauthorized disclosure or other use. You
are prohibited from copying, distributing or otherwise using this
information if you are not the intended recipient. Past performance is
not necessarily indicative of future results. This is not an offer of or
the solicitation for any security which will be made only by private
placement memorandum that may be obtained from the applicable hedge
fund. If you have received this e-mail in error, please notify us
immediately by return e-mail and delete this e-mail and all attachments
from your system. Thank You.
>
>
>
--------------------------------------------------------



This information is being sent at the recipient's request or with their specific understanding. The recipient acknowledges that by sending this information via electronic means, there is no absolute assurance that the information will be free from third party access, use, or further dissemination. This e-mail contains information that is privileged and/or confidential and may be subject to legal restrictions and penalties regarding its unauthorized disclosure or other use. You are prohibited from copying, distributing or otherwise using this information if you are not the intended recipient. Past performance is not necessarily indicative of future results. This is not an offer of or the solicitation for any security which will be made only by private placement memorandum that may be obtained from the applicable hedge fund. If you have received this e-mail in error, please notify us immediately by return e-mail and delete this e-mail and all attachments from your system. Thank You.


From markleeds at verizon.net  Thu Sep 18 19:47:29 2008
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 18 Sep 2008 12:47:29 -0500 (CDT)
Subject: [R-SIG-Finance] [RsR] Outliers in the market model that's used
 to estimate `beta' of a stock
Message-ID: <30342326.24417231221760049994.JavaMail.javamailuser@localhost>

  Hi Matias: yes, he wasn't dissing statistics for the most part. He was 
definitely talking about the miuses also but I think he was claiming
that models , be it statistics, physics and even non quant models  in 
finance are kind of assumed to be right until they don't work.

That's true in all science but it puts finance on quite shaky ground 
because there are people trading serious money based on the idea that 
what they are doing is valid and working  correctly. This is obviously 
kind of relevant  to what's going on right now. Thanks for your 
references also.


 
mark


On Thu, Sep 18, 2008 at  1:29 PM, Matias Salibian-Barrera wrote:

> Haven't read "Fooled by randomness", but did start reading Black Swan, 
> and although in general I like provocative books that challenge my 
> points of view, I found his main thesis to be too short to warrant so 
> many words... I took it that his main argument was with those who 
> misinterpret and misuse statistics (particularly when they do it for 
> their own benefit), not with statistics itself, which is always based 
> on assumptions etc.
>
>> [snip] In fact, he would say that a model works until it doesn't.
>
> Which is a fair statement, that also applies to science in general, 
> "theories work until they are proved wrong", and the whole 
> "falsifiability" argument (cf. Popper vs. Kuhn vs. Feyerabend vs...). 
> I believe robust statistics can help you determine when your model 
> (theory) has stopped to work.
>
> In any case, with respect to the old "data cleaning versus robust 
> estimators" discussion, I would point the interest reader to the first 
> chapter of Maronna, Martin and Yohai's book 
> (http://books.google.com/books?id=YD--AAAACAAJ&dq=martin+maronna+yohai), 
> and for some more specific inference implications, to the first 
> chapter of my PhD dissertation. Essentially, a couple of main issues 
> are: (a) detecting outliers using non-robust estimators does not work 
> well in general (but even if / when it does, see my next point); (b) 
> if you remove (or alter) observations, all subsequent probabilistic 
> statements (p-values, standard errors, etc) are all conditional on the 
> very non-linear cleaning operation you did, and thus both wrong at 
> face value, and not easy to correct. Robust estimators incorporate the 
> down-weighting and its effect on the corresponding inference at once, 
> and are thus, IMHO, to be preferred.
>
> Matias
>
> markleeds at verizon.net wrote:
>> Hi: i don't know if you read "fooled by randomness" by Nassim Taleb ( 
>> spelling )  but he essentially says using very non statistical 
>> arguments but
>> strong nevertheless. ( it's not a stat or a quant finance book ) 
>> that outliers in finance are not modellable and don't claim that you 
>> can model
>> them because you'd be lying. In fact, he would say that a model works 
>> until it doesn't.
>>
>> Anyway, it's an interesting book that sort of indirectly talks ( for 
>> a little too long actually. you can get what's he saying in the first 
>> 50 pages and
>>  it's about 200 pages )  about your comment below so I figured I 
>> would just mention it in case you were interested.
>>
>>
>> On Thu, Sep 18, 2008 at 11:36 AM, Ajay Shah wrote:
>>
>>> In continuation of the discussion on `Winsorisation' that has taken
>>> place on r-sig-finance today, I thought I'd present all of you with 
>>> an
>>> interesting dataset and a question.
>>>
>>> This data is the daily stock returns of the large Indian software 
>>> firm
>>> `Infosys'. (This is the symbol `INFY' on NASDAQ). It is a large 
>>> number
>>> of observations of daily returns (i.e. percentage changes of the
>>> adjusted stock price).
>>>
>>> Load the data in --
>>>
>>>
>>> print(load(url("http://www.mayin.org/ajayshah/tmp/infosys_mm.rda")))
>>>     str(x)
>>>     summary(x)
>>>     sd(x)
>>>
>>> The name `rj' is used for returns on Infosys, and `rM' is used for
>>> returns on the stock market index (Nifty). There are three really
>>> weird observations in this.
>>>
>>>     weird.rj <- c(1896,2395)
>>>     weird.rM <- 2672
>>>     x[weird.rj,]
>>>     x[weird.rM,]
>>>
>>> As you can see, these observations are quite remarkable given the
>>> small standard deviations that we saw above. There is absolutely no
>>> measurement error here. These things actually happened.
>>>
>>> Now consider a typical application: using this to estimate a market
>>> model. The goal here is to estimate the coefficient of a regression 
>>> of
>>> rj on rM.
>>>
>>>     # A regression with all obs
>>>     summary(lm(rj ~ rM, data=x))
>>>
>>>     # Drop the weird rj --
>>>     summary(lm(rj ~ rM, data=x[-weird.rj,]))
>>>
>>>     # Drop the weird rM --
>>>     summary(lm(rj ~ rM, data=x[-weird.rM,]))
>>>
>>>     # Drop both kinds of weird observations --
>>>     summary(lm(rj ~ rM, data=x[-c(weird.rM,weird.rj),]))
>>>
>>>     # Robust regressions
>>>     library(MASS)
>>>     summary(rlm(rj ~ rM, data=x))
>>>     summary(rlm(rj ~ rM, method="MM", data=x))
>>>     library(robust)
>>>     summary(lmRob(rj ~ rM, data=x))
>>>     library(quantreg)
>>>     summary(rq(rj ~ rM, tau=0.5, data=x))
>>>
>>> So you see, we have a variety of different estimates for the slope
>>> (which is termed `beta' in finance). What value would you trust the
>>> most?
>>>
>>> And, would winsorisation using either my code
>>> (https://stat.ethz.ch/pipermail/r-sig-finance/2008q3/002921.html) or
>>> Patrick Burns' code
>>> (https://stat.ethz.ch/pipermail/r-sig-finance/2008q3/002923.html) be 
>>> a
>>> good idea here?
>>>
>>> I'm instinctively unhappy with any scheme based on discarding
>>> observations that I'm absolutely sure have no measurement error. We
>>> have to model the weirdness of this data generating process, not
>>> ignore it.
>>>
>>> -- 
>>> Ajay Shah http://www.mayin.org/ajayshah  ajayshah at mayin.org 
>>> http://ajayshahblog.blogspot.com
>>> <*(:-? - wizard who doesn't know the answer.
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>
>> _______________________________________________
>> R-SIG-Robust at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-robust
>
> -- 
> _____________________________________________________
> Matias Salibian-Barrera - Department of Statistics
> The University of British Columbia
> Phone: (604) 822-3410 - Fax: (604) 822-6960
> "The plural of anecdote is not data" (George Stigler?)


From rkevinburton at charter.net  Thu Sep 18 23:22:00 2008
From: rkevinburton at charter.net (rkevinburton at charter.net)
Date: Thu, 18 Sep 2008 14:22:00 -0700
Subject: [R-SIG-Finance] Time Series Decomposition
Message-ID: <20080918172200.FAH62.91960.root@mp13>

Hi: 

I've been struggling with the following problem and was just 
wondering if someone knew of any solid references in the literature 
for what i'm attempting. I think what i'm attempting is fairly 
straightforward: I have a time series that has a level, a trend and 
seasonal component. I want to model all three of the pieces and put them 
back together to build a forecasting model. I tried 
to use stl but the seasonal component (in the data) is clearly chaging over time so 
the stl models the seasonal piece incorrectly/incompletely (with the amplitude basically fixed for each "season") and this causes all the 
other components to be subsequently modelled incorrectly also. So, I was 
looking for somewhat of a less black boxy approach that 
I can really get my hands around as far as what's going on. Then, with 
that knowledge, maybe I can figure out a way to deal with 
the fact that the seasonal component ( the magnitude ) is changing over 
time. I have heard that I can use DLMs ( dynamic linear 
models for this ) but they are pretty complicated so I was wondering if 
anyone knew of simpler approaches or something in the literature 
that I can read and understand. I have been looking at going the arima 
route but the time series texts that I have don't do in depth coverage 
of a time series decomposition. They talk about how to use the arima 
framework to model a seasonal time series but I have a trend in my 
series also. And I also want to model the level after the seasonal and trend is 
taken out ? So, if anyone could help me out with references for 
what I want to do, it would be greatly appreciated. 
 
Kevin
rkevinburton at charter.net


From jh8080 at hotmail.com  Fri Sep 19 00:13:12 2008
From: jh8080 at hotmail.com (Jae Kim)
Date: Thu, 18 Sep 2008 22:13:12 +0000
Subject: [R-SIG-Finance] Time Series Decomposition
In-Reply-To: <20080918172200.FAH62.91960.root@mp13>
References: <20080918172200.FAH62.91960.root@mp13>
Message-ID: <BAY108-W3733BE1E6FDD6313D8EC01CD4F0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080918/364febcb/attachment.pl>

From Zeno.Adams at ebs.edu  Fri Sep 19 09:58:48 2008
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Fri, 19 Sep 2008 09:58:48 +0200
Subject: [R-SIG-Finance] Outliers in the market model that's used
	toestimate `beta' of a stock
References: <20080918153616.GA6633@lubyanka.local>
Message-ID: <9064522880125945B98983BBAECBA1CC21CADE@exchsrv001.ebs.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080919/8e231709/attachment.pl>

From BChiquoine at tiff.org  Fri Sep 19 20:18:11 2008
From: BChiquoine at tiff.org (Chiquoine, Ben)
Date: Fri, 19 Sep 2008 14:18:11 -0400
Subject: [R-SIG-Finance] Cumulative Multivariate Normal Distribution
Message-ID: <AC36F1084FDE8E4CAFD5CF2409F051D5789637@vsw3exch2.tiff.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080919/4cbf4441/attachment.pl>

From davidr at rhotrading.com  Fri Sep 19 20:40:53 2008
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Fri, 19 Sep 2008 13:40:53 -0500
Subject: [R-SIG-Finance] Cumulative Multivariate Normal Distribution
In-Reply-To: <AC36F1084FDE8E4CAFD5CF2409F051D5789637@vsw3exch2.tiff.local>
References: <AC36F1084FDE8E4CAFD5CF2409F051D5789637@vsw3exch2.tiff.local>
Message-ID: <F9F2A641C593D7408925574C05A7BE77017B7D95@rhopost.rhotrading.com>

Espen Haug has code for a trivariate cum norm in his second edition, p.
482, section 13.4, due to Alan Genz.
(You also need cor_23)
HTH,

David L. Reiner, PhD
Head Quant
Rho Trading Securities, LLC

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Chiquoine,
Ben
Sent: Friday, September 19, 2008 1:18 PM
To: r-sig-finance
Subject: [R-SIG-Finance] Cumulative Multivariate Normal Distribution

Hi,

 

Does anyone know of a function similar to the CBND function for
multivariate distributions?  I've seen a couple but what I am really
looking for is one that will take (x1,x2,x3,cor112,cor113) as inputs.
Alternatively, does anyone know of a package with built in functions for
pricing a worst of three color rainbow options?  Any help along these
lines would be greatly appreciated.  

 

Thanks,


Ben


___________________________________________
This message and any attached documents contain
information which may be confidential, subject to 
privilege or exempt from disclosure under applicable
law. These materials are solely for the use of the 
intended recipient. If you are not the intended 
recipient of this transmission, you are hereby 
notified that any distribution, disclosure, printing, 
copying, storage, modification or the taking of any
action in reliance upon this transmission is strictly
prohibited. Delivery of this message to any person
other than the intended recipient shall not
compromise or waive such confidentiality, privilege
or exemption from disclosure as to this 
communication. 

If you have received this communication in error, 
please notify the sender immediately and delete
this message from your system. 

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From rkevinburton at charter.net  Fri Sep 19 23:03:59 2008
From: rkevinburton at charter.net (rkevinburton at charter.net)
Date: Fri, 19 Sep 2008 14:03:59 -0700
Subject: [R-SIG-Finance] Seasonsal ARIMA
Message-ID: <20080919170359.5FGV4.10147.root@mp20>

First off let me say that from 'R' calling arima(xdata,    order=c(1,1,1), seasonal=list(order=c(2,1,1), period=12) (an ARIMA(1,1,1)X(2.1.1) 12 model) works just fine (provided that data is monthly). But for my data I have daily data so there are 365 obeservations per year and I have about 4 years of data. With this data and replacing 12 with 365 gives me an error indicating that I cannot set the lag above 350. So I gradually worked my way down and now I have 52 observations pwr year and I am calling arima like:

arima(x,    order=c(1,1,1), seasonal=list(order=c(2,1,1), period=52) 
(an ARIMA(1,1,1)X(2.1.1) 52 model)

Now I am getting an error that I don't understand. It must have something to do with the practicle/numeric limitations of the implementation. 

<simpleError in optim(init[mask], armaCSS, method = "BFGS", hessian = FALSE,     control = optim.control): initial value in 'vmmin' is not finite>

So my question to this group is, "what are the practicle limits for using arima to fit data to a seasonal model?" One, I found that I cannot specify a lag larger than 350 either specifically or implicitly with the model that I am building. If the fit takes longer than say 5 minutes I would say that it is impracticle and I need to look for a different solution. Obviously if I get numerical errors like shown above that would be another practicle limitation of the function. So darwing on the experience of this group rather than me going through trial and error what would be the practicle limits of arima? Can these limitations be overcome by possibly another model or other parameters? 

Let me give an example. If I want to brute force find 100! at first the response would be that it is impossible, that big of number can't be represented. But there have been a number of solutions to make findiing 100! "practicle". I am looking for the same kind of advice with arima. 

Thank you.

Kevin


From kriskumar at earthlink.net  Sat Sep 20 01:00:30 2008
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Fri, 19 Sep 2008 19:00:30 -0400
Subject: [R-SIG-Finance] Cumulative Multivariate Normal Distribution
In-Reply-To: <AC36F1084FDE8E4CAFD5CF2409F051D5789637@vsw3exch2.tiff.local>
References: <AC36F1084FDE8E4CAFD5CF2409F051D5789637@vsw3exch2.tiff.local>
Message-ID: <48D42F0E.9020307@earthlink.net>

Hi Ben,

Have you looked at pmvnorm  in package mvtnorm.? This is Genz's method, 
I am working with Jan-Dash on a new approximation
for cumulative normal and I have some semi-tested code of that, we are 
currently testing it for Worst-of and other rainbow type options.
http://arxiv.org/pdf/cs.CE/0611061

The alternative to price these would be to use monte-carlo to do the 
integration and just price them using that, see for example
 the attached R code which generates multi-variate paths. You can then 
use these paths to then compute the discounted payoff function.

Hope this helps,

Best
Krishna

ps:  the drift is simply the risk-neutral drift however for currencies 
you have to factor in the quanto adjustment.
Also this piece of code was written about 4 years ago so pardon the 
coding standards.

 >assetPath<-multiassetpath(c(2,3),c(-0.1,-0.2),2,matrix(c(1,0.2,0.2,1),2,2),1,100,2)









Chiquoine, Ben wrote:
> Hi,
>
>  
>
> Does anyone know of a function similar to the CBND function for
> multivariate distributions?  I've seen a couple but what I am really
> looking for is one that will take (x1,x2,x3,cor112,cor113) as inputs.
> Alternatively, does anyone know of a package with built in functions for
> pricing a worst of three color rainbow options?  Any help along these
> lines would be greatly appreciated.  
>
>  
>
> Thanks,
>
>
> Ben
>
>
> ___________________________________________
> This message and any attached documents contain
> information which may be confidential, subject to 
> privilege or exempt from disclosure under applicable
> law. These materials are solely for the use of the 
> intended recipient. If you are not the intended 
> recipient of this transmission, you are hereby 
> notified that any distribution, disclosure, printing, 
> copying, storage, modification or the taking of any
> action in reliance upon this transmission is strictly
> prohibited. Delivery of this message to any person
> other than the intended recipient shall not
> compromise or waive such confidentiality, privilege
> or exemption from disclosure as to this 
> communication. 
>
> If you have received this communication in error, 
> please notify the sender immediately and delete
> this message from your system. 
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>   

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: worstof.r
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080919/ef51ae38/attachment.pl>

From jh8080 at hotmail.com  Sat Sep 20 05:15:10 2008
From: jh8080 at hotmail.com (Jae Kim)
Date: Sat, 20 Sep 2008 03:15:10 +0000
Subject: [R-SIG-Finance] Seasonsal ARIMA
In-Reply-To: <20080919170359.5FGV4.10147.root@mp20>
References: <20080919170359.5FGV4.10147.root@mp20>
Message-ID: <BAY108-W2737B90BCE2F1AF445B40BCD490@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080920/342049be/attachment.pl>

From rkevinburton at charter.net  Sat Sep 20 05:55:16 2008
From: rkevinburton at charter.net (rkevinburton at charter.net)
Date: Fri, 19 Sep 2008 20:55:16 -0700
Subject: [R-SIG-Finance] Seasonsal ARIMA
In-Reply-To: <BAY108-W2737B90BCE2F1AF445B40BCD490@phx.gbl>
Message-ID: <20080919235516.172KG.129912.root@mp12>

No I have 52 observations per year (which is once a week) so isn't the periodicty 52? The original data had 365 observations per year and arima could not handle that. I tried splitting the year in half (having 182 observations per period/half year) and then splitting the year into quarters (91 observations per year) but that all proved too much for arima to handle (seasonal wise). So the next logical step was to split the data into weeks. Summing the daily data so that there are 52 observations per year. This kind of works in that there are not as many errors. I still get errors from 'optim' but I think eventually with trial and error I will be able to get rid of those few.

Hopefully I have made it a little clearer on what I am trying to do.

Thank you for your comments.

Kevin
---- Jae Kim <jh8080 at hotmail.com> wrote: 
> 
> Hi,
>  
> If you have daily 7-day a week data, you may have weekly, bi-weekly or monthly seasonality. That is, 7-day cycle.
> This means that the value of your periodicity may have to be 7.
>  
> But I guess you can inspect the SACF to confirm the presence of this periodicity.
>  
> JHK
> > Date: Fri, 19 Sep 2008 14:03:59 -0700> From: rkevinburton at charter.net> To: r-sig-finance at stat.math.ethz.ch> Subject: [R-SIG-Finance] Seasonsal ARIMA> > First off let me say that from 'R' calling arima(xdata, order=c(1,1,1), seasonal=list(order=c(2,1,1), period=12) (an ARIMA(1,1,1)X(2.1.1) 12 model) works just fine (provided that data is monthly). But for my data I have daily data so there are 365 obeservations per year and I have about 4 years of data. With this data and replacing 12 with 365 gives me an error indicating that I cannot set the lag above 350. So I gradually worked my way down and now I have 52 observations pwr year and I am calling arima like:> > arima(x, order=c(1,1,1), seasonal=list(order=c(2,1,1), period=52) > (an ARIMA(1,1,1)X(2.1.1) 52 model)> > Now I am getting an error that I don't understand. It must have something to do with the practicle/numeric limitations of the implementation. > > <simpleError in optim(init[mask], armaCSS, method = "BFGS", hessian = FALSE, control = optim.control): initial value in 'vmmin' is not finite>> > So my question to this group is, "what are the practicle limits for using arima to fit data to a seasonal model?" One, I found that I cannot specify a lag larger than 350 either specifically or implicitly with the model that I am building. If the fit takes longer than say 5 minutes I would say that it is impracticle and I need to look for a different solution. Obviously if I get numerical errors like shown above that would be another practicle limitation of the function. So darwing on the experience of this group rather than me going through trial and error what would be the practicle limits of arima? Can these limitations be overcome by possibly another model or other parameters? > > Let me give an example. If I want to brute force find 100! at first the response would be that it is impossible, that big of number can't be represented. But there have been a number of solutions to make findiing 100! "practicle". I am looking for the same kind of advice with arima. > > Thank you.> > Kevin> > _______________________________________________> R-SIG-Finance at stat.math.ethz.ch mailing list> https://stat.ethz.ch/mailman/listinfo/r-sig-finance> -- Subscriber-posting only.> -- If you want to post, subscribe first.
> _________________________________________________________________
> It's simple! Sell your car for just $40 at CarPoint.com.au
> http://a.ninemsn.com.au/b.aspx?URL=http%3A%2F%2Fsecure%2Dau%2Eimrworldwide%2Ecom%2Fcgi%2Dbin%2Fa%2Fci%5F450304%2Fet%5F2%2Fcg%5F801459%2Fpi%5F1004813%2Fai%5F859641&_t=762955845&_r=tig_OCT07&_m=EXT


From seanpor at acm.org  Sat Sep 20 09:29:51 2008
From: seanpor at acm.org (Sean O'Riordain)
Date: Sat, 20 Sep 2008 08:29:51 +0100
Subject: [R-SIG-Finance] Seasonsal ARIMA
In-Reply-To: <20080919170359.5FGV4.10147.root@mp20>
References: <20080919170359.5FGV4.10147.root@mp20>
Message-ID: <8ed68eed0809200029q767c931fidacee9feb86355d6@mail.gmail.com>

Kevin,
I'm a complete beginner at timeseries analysis, so with that said..
I've been looking at the forecast package, and it appears to be able
to work in automatic mode if you so desire.

please please take the following with a grain of salt... I don't know
how to manipulate timeseries, so I'm just being a bit "blunt" here...
no doubt somebody will jump in with improvements! :-)

library(forecast)
# btw. gold has a length of 1108 ... but just to make things longer
and add a bit of "seasonal repetition"...
g1 <- as.numeric(unclass(gold))
g2 <- c(g1,g1,g1,g1)
# it appears that forecast doesn't like NAs so I'll just bluntly
interpolate them for the purposes of this demo
fg2 <- forecast(na.interp(g2))
plot(fg2)
# or for a more detailed plot
nl <- length(g2)
plot(fg2, xlim=c(nl-250, nl))

Rob J. Hyndman has a paper "Automatic time series forecasting: the
forecast package for R" at
http://ideas.repec.org/p/msh/ebswps/2007-6.html

Regards,
Sean O'Riordain
Dublin, Ireland
seanpor at acm.org


On Fri, Sep 19, 2008 at 10:03 PM,  <rkevinburton at charter.net> wrote:
> First off let me say that from 'R' calling arima(xdata,    order=c(1,1,1), seasonal=list(order=c(2,1,1), period=12) (an ARIMA(1,1,1)X(2.1.1) 12 model) works just fine (provided that data is monthly). But for my data I have daily data so there are 365 obeservations per year and I have about 4 years of data. With this data and replacing 12 with 365 gives me an error indicating that I cannot set the lag above 350. So I gradually worked my way down and now I have 52 observations pwr year and I am calling arima like:
>
> arima(x,    order=c(1,1,1), seasonal=list(order=c(2,1,1), period=52)
> (an ARIMA(1,1,1)X(2.1.1) 52 model)
>
> Now I am getting an error that I don't understand. It must have something to do with the practicle/numeric limitations of the implementation.
>
> <simpleError in optim(init[mask], armaCSS, method = "BFGS", hessian = FALSE,     control = optim.control): initial value in 'vmmin' is not finite>
>
> So my question to this group is, "what are the practicle limits for using arima to fit data to a seasonal model?" One, I found that I cannot specify a lag larger than 350 either specifically or implicitly with the model that I am building. If the fit takes longer than say 5 minutes I would say that it is impracticle and I need to look for a different solution. Obviously if I get numerical errors like shown above that would be another practicle limitation of the function. So darwing on the experience of this group rather than me going through trial and error what would be the practicle limits of arima? Can these limitations be overcome by possibly another model or other parameters?
>
> Let me give an example. If I want to brute force find 100! at first the response would be that it is impossible, that big of number can't be represented. But there have been a number of solutions to make findiing 100! "practicle". I am looking for the same kind of advice with arima.
>
> Thank you.
>
> Kevin
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From kriskumar at earthlink.net  Sat Sep 20 14:07:55 2008
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Sat, 20 Sep 2008 08:07:55 -0400
Subject: [R-SIG-Finance] Financial Econometrics
In-Reply-To: <BAY126-DS4D246EDE8A9121FA37C9ED9520@phx.gbl>
References: <BAY126-DS4D246EDE8A9121FA37C9ED9520@phx.gbl>
Message-ID: <48D4E79B.50201@earthlink.net>

I like these three books and think they are worth looking at Tsay, Mills 
and Franses in no particular order.
They are quite accessible another book that has a few good chapters is 
Carol Alexander's book.
http://www.amazon.com/exec/obidos/ASIN/0471899755/kriskumar-20

Eric Zivot's book has again excellent treatment and examples and could 
be a good way to do this course.

Cheers
Krishna



Hsiao-nan Cheung wrote:
> Hi,
>
>  
>
> I??ll be a assistant professor on econometrics in the following term. Since
> the students are mainly of dept. of finance, my professor want the teaching
> materials be about finance and not about macroeconomics. Is there any good
> books about financial econometrics (not something about time series
> analysis)? Something by examples is the best.
>
>  
>
> Thanks
>
>  
>
> Hsiao-nan Cheung
>
>  
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From BChiquoine at tiff.org  Sun Sep 21 00:55:20 2008
From: BChiquoine at tiff.org (Chiquoine, Ben)
Date: Sat, 20 Sep 2008 18:55:20 -0400
Subject: [R-SIG-Finance] Using monte carlo simulation to price options
Message-ID: <AC36F1084FDE8E4CAFD5CF2409F051D57896E8@vsw3exch2.tiff.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080920/0d605155/attachment.pl>

From ben.domingue at gmail.com  Sun Sep 21 05:17:19 2008
From: ben.domingue at gmail.com (Ben Domingue)
Date: Sat, 20 Sep 2008 21:17:19 -0600
Subject: [R-SIG-Finance] Ng-Perron test for unit roots
Message-ID: <9c15a6240809202017q5e7db542j36adecefd3c3e64d@mail.gmail.com>

Hello,
I've searched all the standard spots, and I can't find any
implementation of the Ng-Perron test for unit roots.  I am aware of
the PP tests in urca.  Anybody know of something I missed?
Some one from R-help suggested I ask here as well.
Thanks,
Ben


From m_olshansky at yahoo.com  Mon Sep 22 03:26:09 2008
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Sun, 21 Sep 2008 18:26:09 -0700 (PDT)
Subject: [R-SIG-Finance] Using monte carlo simulation to price options
In-Reply-To: <AC36F1084FDE8E4CAFD5CF2409F051D57896E8@vsw3exch2.tiff.local>
Message-ID: <65609.81542.qm@web32204.mail.mud.yahoo.com>

You can use mvrnorm from MASS package to generate multivariate normal distribution. Then if your option is European, using Monte Carlo to price it is straightforward, but this is not the case if the option is American.


--- On Sun, 21/9/08, Chiquoine, Ben <BChiquoine at tiff.org> wrote:

> From: Chiquoine, Ben <BChiquoine at tiff.org>
> Subject: [R-SIG-Finance] Using monte carlo simulation to price options
> To: "r-sig-finance" <r-sig-finance at stat.math.ethz.ch>
> Received: Sunday, 21 September, 2008, 8:55 AM
> Hi,
> 
>  
> 
> I'm trying to use monte carlo simulation to price a
> worst of three asset
> currency option.  I'm trying to follow what Haug does
> in his book "The
> Complete Guide to Option Pricing Formulas" 
> Unfortunately he only
> carries the monte carlo  simulation logic out to an option
> on two assets
> and I'm not able to infer how the underlying asset
> prices should be
> modeled for an option with 3 underlying assets.  I know
> this might not
> be the perfect forum for this question but it was the best
> I could think
> of.  Any help you can provide would be greatly appreciated.
>  The formula
> that Haug gives for simulating two correlated asset prices
> which follow
> geometric Brownian motion is....
> 
>  
> 
> S1+ ChangeS1 = S1exp((u1-1/2*std1^2)*t + std1*a1*sqrt(t))
> 
> S2+ ChangeS2 = S2exp((u2-1/2*std2^2)*t + std2*a2*sqrt(t))
> 
> a1=e1
> 
> a2=rho12*e1+ e2*sqrt(1-rho^2)
> 
> where e1 and e2 are independent random numbers from
> standard normal
> distributions
> 
>  
> 
> Again, Im basically just trying to add an S3 to this model
> so that
> rho12, rho13, and rho23 are all incorporated. Thanks again
> for any
> insight you can provide!
> 
>  
> 
> Ben
> 
>  
> 
>  
> 
> 
> ___________________________________________
> This message and any attached documents contain
> information which may be confidential, subject to 
> privilege or exempt from disclosure under applicable
> law. These materials are solely for the use of the 
> intended recipient. If you are not the intended 
> recipient of this transmission, you are hereby 
> notified that any distribution, disclosure, printing, 
> copying, storage, modification or the taking of any
> action in reliance upon this transmission is strictly
> prohibited. Delivery of this message to any person
> other than the intended recipient shall not
> compromise or waive such confidentiality, privilege
> or exemption from disclosure as to this 
> communication. 
> 
> If you have received this communication in error, 
> please notify the sender immediately and delete
> this message from your system. 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From davidr at rhotrading.com  Mon Sep 22 15:11:24 2008
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Mon, 22 Sep 2008 08:11:24 -0500
Subject: [R-SIG-Finance] Cumulative Multivariate Normal Distribution
In-Reply-To: <48D42F0E.9020307@earthlink.net>
References: <AC36F1084FDE8E4CAFD5CF2409F051D5789637@vsw3exch2.tiff.local>
	<48D42F0E.9020307@earthlink.net>
Message-ID: <F9F2A641C593D7408925574C05A7BE77017B7E74@rhopost.rhotrading.com>

Just be aware that the output of pmvnorm is not deterministic.
Genz's method for trivariate (given in Haug) is deterministic.

David L. Reiner, PhD
Head Quant
Rho Trading Securities, LLC


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Krishna
Kumar
Sent: Friday, September 19, 2008 6:01 PM
To: Chiquoine, Ben
Cc: r-sig-finance
Subject: Re: [R-SIG-Finance] Cumulative Multivariate Normal Distribution

Hi Ben,

Have you looked at pmvnorm  in package mvtnorm.? This is Genz's method, 
I am working with Jan-Dash on a new approximation
for cumulative normal and I have some semi-tested code of that, we are 
currently testing it for Worst-of and other rainbow type options.
http://arxiv.org/pdf/cs.CE/0611061

The alternative to price these would be to use monte-carlo to do the 
integration and just price them using that, see for example
 the attached R code which generates multi-variate paths. You can then 
use these paths to then compute the discounted payoff function.

Hope this helps,

Best
Krishna

ps:  the drift is simply the risk-neutral drift however for currencies 
you have to factor in the quanto adjustment.
Also this piece of code was written about 4 years ago so pardon the 
coding standards.

 
>assetPath<-multiassetpath(c(2,3),c(-0.1,-0.2),2,matrix(c(1,0.2,0.2,1),2
,2),1,100,2)









Chiquoine, Ben wrote:
> Hi,
>
>  
>
> Does anyone know of a function similar to the CBND function for
> multivariate distributions?  I've seen a couple but what I am really
> looking for is one that will take (x1,x2,x3,cor112,cor113) as inputs.
> Alternatively, does anyone know of a package with built in functions
for
> pricing a worst of three color rainbow options?  Any help along these
> lines would be greatly appreciated.  
>
>  
>
> Thanks,
>
>
> Ben
>
>
> ___________________________________________
> This message and any attached documents contain
> information which may be confidential, subject to 
> privilege or exempt from disclosure under applicable
> law. These materials are solely for the use of the 
> intended recipient. If you are not the intended 
> recipient of this transmission, you are hereby 
> notified that any distribution, disclosure, printing, 
> copying, storage, modification or the taking of any
> action in reliance upon this transmission is strictly
> prohibited. Delivery of this message to any person
> other than the intended recipient shall not
> compromise or waive such confidentiality, privilege
> or exemption from disclosure as to this 
> communication. 
>
> If you have received this communication in error, 
> please notify the sender immediately and delete
> this message from your system. 
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>   


From rkevinburton at charter.net  Mon Sep 22 19:15:31 2008
From: rkevinburton at charter.net (rkevinburton at charter.net)
Date: Mon, 22 Sep 2008 10:15:31 -0700
Subject: [R-SIG-Finance] Frequency too high for ets?
Message-ID: <20080922131531.XFRPG.143497.root@mp06>

I have a time series that is basically weekly data for a year. So the frequency on the time-series is 52 (52 weeks (obeservations)/year). I have 3+ years of data. I am trying to fit a model to this data using ets in the forecast package (exponential smoothing) but I get:

Error in ets(.sublist$TimeSeries) : Frequency too high

I was looking in the documentation for 'ets' and there was no mention of the limits but apparently 52 is "too high".  Any suggestions?

Thank you.

Kevin


From josh.m.ulrich at gmail.com  Mon Sep 22 19:26:30 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Mon, 22 Sep 2008 12:26:30 -0500
Subject: [R-SIG-Finance] Frequency too high for ets?
In-Reply-To: <20080922131531.XFRPG.143497.root@mp06>
References: <20080922131531.XFRPG.143497.root@mp06>
Message-ID: <8cca69990809221026m2276f2e3ma8105fc52b1ec973@mail.gmail.com>

Look at the source:

> x <- ts(rnorm(52*100),frequency=52)
> debug(ets)
> e <- ets(x)
<snip>
debug: m <- frequency(y)
Browse[1]>
debug: if (m > 24) stop("Frequency too high")
Browse[1]>
Error in ets(x) : Frequency too high

--
http://quantemplation.blogspot.com


On Mon, Sep 22, 2008 at 12:15 PM,  <rkevinburton at charter.net> wrote:
> I have a time series that is basically weekly data for a year. So the frequency on the time-series is 52 (52 weeks (obeservations)/year). I have 3+ years of data. I am trying to fit a model to this data using ets in the forecast package (exponential smoothing) but I get:
>
> Error in ets(.sublist$TimeSeries) : Frequency too high
>
> I was looking in the documentation for 'ets' and there was no mention of the limits but apparently 52 is "too high".  Any suggestions?
>
> Thank you.
>
> Kevin
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From rkevinburton at charter.net  Mon Sep 22 20:38:12 2008
From: rkevinburton at charter.net (rkevinburton at charter.net)
Date: Mon, 22 Sep 2008 14:38:12 -0400
Subject: [R-SIG-Finance] Frequency too high for ets?
In-Reply-To: <8cca69990809221026m2276f2e3ma8105fc52b1ec973@mail.gmail.com>
Message-ID: <20080922143812.7E5NQ.146526.root@mp06>

Thank you. I did get a valiuable technique for debugging that I didn't know before from your comments.

So from your comments I gather the limit for exponential smoothing is 24. Not having much experience with exponential smoothing is it unreasonable to expect an algorithm to handle frequencies > 24? I started out with daily data (365 observations per year). I quickly realized that there were not any available fitting algorithms that could handle that degree of resolution. I tried splitting the year in half and splitting the year into quarters (91 observations per quarter). I finally found that if I have 52 observations per year I can use arima to fit and ARIMA model to my data. That seemed like a compromise that I could live with but it was still a compromise. Now if I want to fit an exponential smoothing model I need to further reduce the frequency by more than a half. Are there other packages that can handle this? I would like to be able to forecast down to any given week in the year if possible. In other words if I feed in something like week 48 I would like to be able to handle this. Reommendations?

Thank you.

Kevin

---- Josh Ulrich <josh.m.ulrich at gmail.com> wrote: 
> Look at the source:
> 
> > x <- ts(rnorm(52*100),frequency=52)
> > debug(ets)
> > e <- ets(x)
> <snip>
> debug: m <- frequency(y)
> Browse[1]>
> debug: if (m > 24) stop("Frequency too high")
> Browse[1]>
> Error in ets(x) : Frequency too high
> 
> --
> http://quantemplation.blogspot.com
> 
> 
> On Mon, Sep 22, 2008 at 12:15 PM,  <rkevinburton at charter.net> wrote:
> > I have a time series that is basically weekly data for a year. So the frequency on the time-series is 52 (52 weeks (obeservations)/year). I have 3+ years of data. I am trying to fit a model to this data using ets in the forecast package (exponential smoothing) but I get:
> >
> > Error in ets(.sublist$TimeSeries) : Frequency too high
> >
> > I was looking in the documentation for 'ets' and there was no mention of the limits but apparently 52 is "too high".  Any suggestions?
> >
> > Thank you.
> >
> > Kevin
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >


From ezivot at u.washington.edu  Tue Sep 23 00:06:45 2008
From: ezivot at u.washington.edu (Eric Zivot)
Date: Mon, 22 Sep 2008 15:06:45 -0700
Subject: [R-SIG-Finance] Frequency too high for ets?
In-Reply-To: <20080922143812.7E5NQ.146526.root@mp06>
Message-ID: <200809222206.m8MM6kL8021990@smtp.washington.edu>

This problem is just a limitation of representing your data as a ts object,
not a problem with any exponential smoothing algorithm. The ts class in R is
severely limited in the types of regularly spaced time series data that can
be represented (annual, quaterly or monthly). A simple fix is to redefine
your weekly time series so that it has a frequency of 1 instead of 52; that
is, disregard the fact that you have weekly data and just treat each week as
one observation. 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
rkevinburton at charter.net
Sent: Monday, September 22, 2008 11:38 AM
To: Josh Ulrich
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Frequency too high for ets?

Thank you. I did get a valiuable technique for debugging that I didn't know
before from your comments.

So from your comments I gather the limit for exponential smoothing is 24.
Not having much experience with exponential smoothing is it unreasonable to
expect an algorithm to handle frequencies > 24? I started out with daily
data (365 observations per year). I quickly realized that there were not any
available fitting algorithms that could handle that degree of resolution. I
tried splitting the year in half and splitting the year into quarters (91
observations per quarter). I finally found that if I have 52 observations
per year I can use arima to fit and ARIMA model to my data. That seemed like
a compromise that I could live with but it was still a compromise. Now if I
want to fit an exponential smoothing model I need to further reduce the
frequency by more than a half. Are there other packages that can handle
this? I would like to be able to forecast down to any given week in the year
if possible. In other words if I feed in something like week 48 I would like
to be able to !
 handle this. Reommendations?

Thank you.

Kevin

---- Josh Ulrich <josh.m.ulrich at gmail.com> wrote: 
> Look at the source:
> 
> > x <- ts(rnorm(52*100),frequency=52)
> > debug(ets)
> > e <- ets(x)
> <snip>
> debug: m <- frequency(y)
> Browse[1]>
> debug: if (m > 24) stop("Frequency too high") Browse[1]> Error in 
> ets(x) : Frequency too high
> 
> --
> http://quantemplation.blogspot.com
> 
> 
> On Mon, Sep 22, 2008 at 12:15 PM,  <rkevinburton at charter.net> wrote:
> > I have a time series that is basically weekly data for a year. So the
frequency on the time-series is 52 (52 weeks (obeservations)/year). I have
3+ years of data. I am trying to fit a model to this data using ets in the
forecast package (exponential smoothing) but I get:
> >
> > Error in ets(.sublist$TimeSeries) : Frequency too high
> >
> > I was looking in the documentation for 'ets' and there was no mention of
the limits but apparently 52 is "too high".  Any suggestions?
> >
> > Thank you.
> >
> > Kevin
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From elise at predictionimpact.com  Tue Sep 23 09:38:32 2008
From: elise at predictionimpact.com (Elise Johnson)
Date: Tue, 23 Sep 2008 00:38:32 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Predictive Analytics event Oct
 24-25 (DC) and Nov 6-7 (SF)
Message-ID: <19622739.post@talk.nabble.com>


Hi, I wanted to make sure you were all aware of these upcoming events. There
is a seminar in Predictive Analytics on Oct. 24-25 in DC, and in San
Francisco Nov 6-7.  This is intensive training for managers, marketers, and
IT people who need to make sense of customer data to predict buying
behavior, profit, etc.  Past attendees have given rave reviews. 

You can find more info at
http://www.predictionimpact.com/predictive-analytics-training.html, e-mail
training at predictionimpact.com, or call (415) 683-1146.

thanks --Elise Johnson, Prediction Impact

-- 
View this message in context: http://www.nabble.com/Predictive-Analytics-event-Oct-24-25-%28DC%29-and-Nov-6-7-%28SF%29-tp19622739p19622739.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From supersubu123 at gmail.com  Wed Sep 24 08:07:01 2008
From: supersubu123 at gmail.com (subramanian R)
Date: Wed, 24 Sep 2008 11:37:01 +0530
Subject: [R-SIG-Finance] Use of sorting in Financial Domain
Message-ID: <b2d9df730809232307t2d643cc2jd0b7e81e9b6abb23@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080924/eac89861/attachment.pl>

From brian at braverock.com  Wed Sep 24 12:08:08 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 24 Sep 2008 05:08:08 -0500
Subject: [R-SIG-Finance] Use of sorting in Financial Domain
In-Reply-To: <b2d9df730809232307t2d643cc2jd0b7e81e9b6abb23@mail.gmail.com>
References: <b2d9df730809232307t2d643cc2jd0b7e81e9b6abb23@mail.gmail.com>
Message-ID: <1222250889.15019.84.camel@ubuntu.braverock.com>

On Wed, 2008-09-24 at 11:37 +0530, subramanian R wrote:
> Hi All,
> The *sort() *command in R has two methods (method specification is an
> optional parameter)
> 1.shell sort(default)
> 2.quick sort
> and order() has 1 extra method
> 3.radix sort.
> 
> In our multicore architecture we have some optimized implementations of
> several sorting methods like Bitonic sort that is not supported by R.It is
> faster than quick sort in most cases in our architecture.
> 
> I have two questions in this regard with respect to financial domain.
> 
> How much sorting is used in this domain?
> In those cases what type of sorting is used?
> Will it help if a seperate R command for bitonic sort is implemented and
> override the default one?
> Are there any specific packages used by Financial sector where optimized
> sorting techniques have been implemented?

Sorting is often used in financial time series: e.g. to examine
drawdowns, or to extract a quantile of an observed series or a monte
carlo simulation.

Implementation of additional parallel sort algorithms in R such as
Batcher's bitonic sort seems as though it could be valuable in financial
time series.

I'm not aware of additional sorting implementations in R specific to
finance, most finance specific functionality uses R general functions
such as sort() and order() 

It seems that your implementation would be best proposed as a patch to
one of these functions on the r-devel list, or alternately contained in
an optimized sorting package that could overload the standard functions
with a new implementation.

Regards,

   - Brian


From josh at gghc.com  Wed Sep 24 14:59:39 2008
From: josh at gghc.com (Joshua Reich)
Date: Wed, 24 Sep 2008 08:59:39 -0400
Subject: [R-SIG-Finance] Use of sorting in Financial Domain
In-Reply-To: <1222250889.15019.84.camel@ubuntu.braverock.com>
References: <b2d9df730809232307t2d643cc2jd0b7e81e9b6abb23@mail.gmail.com>
	<1222250889.15019.84.camel@ubuntu.braverock.com>
Message-ID: <C20EA84D76C94F4E999DC041E81C0D11057226BB@exchange2k3.ny.gghc.com>

Extending the question further, I assume that bitonic sorting makes
sense when you have access to a SIMD platform, such as in a GPU.

I have been playing with porting some of my code to NVIDA's CUDA
platform, and I'd love to hear if other people have had much success. If
I recall correctly, I had discussed this on the list in the past and we
discussed using CUDA BLAS over default installations. Has anyone moved
CUDA into a production environment? If so, how has it fared? If not, why
not?

Josh Reich

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
Peterson
Sent: Wednesday, September 24, 2008 6:08 AM
To: subramanian R
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Use of sorting in Financial Domain

On Wed, 2008-09-24 at 11:37 +0530, subramanian R wrote:
> Hi All,
> The *sort() *command in R has two methods (method specification is an
> optional parameter)
> 1.shell sort(default)
> 2.quick sort
> and order() has 1 extra method
> 3.radix sort.
> 
> In our multicore architecture we have some optimized implementations
of
> several sorting methods like Bitonic sort that is not supported by
R.It is
> faster than quick sort in most cases in our architecture.
> 
> I have two questions in this regard with respect to financial domain.
> 
> How much sorting is used in this domain?
> In those cases what type of sorting is used?
> Will it help if a seperate R command for bitonic sort is implemented
and
> override the default one?
> Are there any specific packages used by Financial sector where
optimized
> sorting techniques have been implemented?

Sorting is often used in financial time series: e.g. to examine
drawdowns, or to extract a quantile of an observed series or a monte
carlo simulation.

Implementation of additional parallel sort algorithms in R such as
Batcher's bitonic sort seems as though it could be valuable in financial
time series.

I'm not aware of additional sorting implementations in R specific to
finance, most finance specific functionality uses R general functions
such as sort() and order() 

It seems that your implementation would be best proposed as a patch to
one of these functions on the r-devel list, or alternately contained in
an optimized sorting package that could overload the standard functions
with a new implementation.

Regards,

   - Brian

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From imanpreet at gmail.com  Thu Sep 25 06:22:48 2008
From: imanpreet at gmail.com (Imanpreet)
Date: Thu, 25 Sep 2008 09:52:48 +0530
Subject: [R-SIG-Finance] Use of sorting in Financial Domain
In-Reply-To: <C20EA84D76C94F4E999DC041E81C0D11057226BB@exchange2k3.ny.gghc.com>
References: <b2d9df730809232307t2d643cc2jd0b7e81e9b6abb23@mail.gmail.com>
	<1222250889.15019.84.camel@ubuntu.braverock.com>
	<C20EA84D76C94F4E999DC041E81C0D11057226BB@exchange2k3.ny.gghc.com>
Message-ID: <c26b95920809242122s6ae8bb9cmb5bb6554e688351d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080925/e38a1c98/attachment.pl>

From jeff.a.ryan at gmail.com  Fri Sep 26 20:55:10 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 26 Sep 2008 13:55:10 -0500
Subject: [R-SIG-Finance] REMINDER: R/Finance/Chicago October 3rd
Message-ID: <e8e755250809261155j27c0476dxc4816e45624553bb@mail.gmail.com>

R-finance useRs [[Chicago]]:

***A quick reminder of next Friday's event!  Hope to see everyone there!***

I'm happy to announce the first big meeting of R/Finance users in Chicago
on October 3rd, 2008.

In coordination with major R finance contributors and the support of the
University of Illinois at Chicago, we invite the local community to an
informal social gathering after the market close on Friday October 3rd.

The gathering will be at Jak's Tap in the West Loop and will start with a
casual reception at 3:30.  Details and links can be found at the bottom of
this email.

The idea is to bring together industry practitioners and academics with a
common interest in R, from across Chicago-land, to meet face to face.  We
hope to increase discussion about the future of R and finance.  We will
also be discussing a much bigger event for the Spring and would like
feedback as to possible direction and content.

The afternoon/evening will be mostly informal, with formal presentations
starting around 5.

The formal agenda:

? We will have a small presentation from some of the locally-based
contributors as to the state of the R community in finance, as well as a
vision for the future.  The current presenters include:

 Jeff Ryan [quantmod, xts, IBrokers]
 Dirk Eddelbuettel [CRAN Empirical Finance task View, RQuantLib, etc]
 Brian Peterson & Peter Carl [PerformanceAnalytics]
 Josh Ulrich [TTR, xts, opentick]

? A presentation by UIC's Finance Department and International Center for
Futures and Derivatives on: (1) how UIC fits into the R-finance mix, and
(2) what they have to offer the larger quantitative/trading community in
Chicago.

Following the formal presentations, we will have the room(s) until 10PM to
allow for discussion and collaboration among the attendees.

Please RSVP to jeffrey.ryan at insightalgo.com or jeff.a.ryan at gmail.com

What: First R/Finance Chicago Meeting
Where: Jak's Tap. 901 W. Jackson, Chicago IL  http://jakstap.com/
When: Friday, October 3, 2008
Time: 3:30 - 10:00PM (presentations start at 5:00PM)


-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From rory.winston at gmail.com  Sun Sep 28 12:42:30 2008
From: rory.winston at gmail.com (Rory Winston)
Date: Sun, 28 Sep 2008 11:42:30 +0100
Subject: [R-SIG-Finance] REMINDER: R/Finance/Chicago October 3rd
In-Reply-To: <mailman.3.1222509601.7427.r-sig-finance@stat.math.ethz.ch>
References: <mailman.3.1222509601.7427.r-sig-finance@stat.math.ethz.ch>
Message-ID: <48DF5F96.9060907@gmail.com>

Just as a matter of interest, are there any London-based R users in 
finance who are interested in a similar event?

Rory


r-sig-finance-request at stat.math.ethz.ch wrote:
> Send R-SIG-Finance mailing list submissions to
> 	r-sig-finance at stat.math.ethz.ch
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> or, via email, send a message with subject or body 'help' to
> 	r-sig-finance-request at stat.math.ethz.ch
>
> You can reach the person managing the list at
> 	r-sig-finance-owner at stat.math.ethz.ch
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-SIG-Finance digest..."
>
>
> Today's Topics:
>
>    1. REMINDER: R/Finance/Chicago October 3rd (Jeff Ryan)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Fri, 26 Sep 2008 13:55:10 -0500
> From: "Jeff Ryan" <jeff.a.ryan at gmail.com>
> Subject: [R-SIG-Finance] REMINDER: R/Finance/Chicago October 3rd
> To: R-SIG-Finance <r-sig-finance at stat.math.ethz.ch>
> Message-ID:
> 	<e8e755250809261155j27c0476dxc4816e45624553bb at mail.gmail.com>
> Content-Type: text/plain; charset=WINDOWS-1252
>
> R-finance useRs [[Chicago]]:
>
> ***A quick reminder of next Friday's event!  Hope to see everyone there!***
>
> I'm happy to announce the first big meeting of R/Finance users in Chicago
> on October 3rd, 2008.
>
> In coordination with major R finance contributors and the support of the
> University of Illinois at Chicago, we invite the local community to an
> informal social gathering after the market close on Friday October 3rd.
>
> The gathering will be at Jak's Tap in the West Loop and will start with a
> casual reception at 3:30.  Details and links can be found at the bottom of
> this email.
>
> The idea is to bring together industry practitioners and academics with a
> common interest in R, from across Chicago-land, to meet face to face.  We
> hope to increase discussion about the future of R and finance.  We will
> also be discussing a much bigger event for the Spring and would like
> feedback as to possible direction and content.
>
> The afternoon/evening will be mostly informal, with formal presentations
> starting around 5.
>
> The formal agenda:
>
> ? We will have a small presentation from some of the locally-based
> contributors as to the state of the R community in finance, as well as a
> vision for the future.  The current presenters include:
>
>  Jeff Ryan [quantmod, xts, IBrokers]
>  Dirk Eddelbuettel [CRAN Empirical Finance task View, RQuantLib, etc]
>  Brian Peterson & Peter Carl [PerformanceAnalytics]
>  Josh Ulrich [TTR, xts, opentick]
>
> ? A presentation by UIC's Finance Department and International Center for
> Futures and Derivatives on: (1) how UIC fits into the R-finance mix, and
> (2) what they have to offer the larger quantitative/trading community in
> Chicago.
>
> Following the formal presentations, we will have the room(s) until 10PM to
> allow for discussion and collaboration among the attendees.
>
> Please RSVP to jeffrey.ryan at insightalgo.com or jeff.a.ryan at gmail.com
>
> What: First R/Finance Chicago Meeting
> Where: Jak's Tap. 901 W. Jackson, Chicago IL  http://jakstap.com/
> When: Friday, October 3, 2008
> Time: 3:30 - 10:00PM (presentations start at 5:00PM)
>
>
>


From ryan.sheftel at malbecpartners.com  Sun Sep 28 15:15:42 2008
From: ryan.sheftel at malbecpartners.com (ryan.sheftel at malbecpartners.com)
Date: Sun, 28 Sep 2008 09:15:42 -0400
Subject: [R-SIG-Finance] Omega calculation in PerformanceAnalytics package
Message-ID: <OF09BE7644.75FD63CC-ON852574D2.0041FE09-852574D2.00490B20@malbecpartners.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080928/84fe6e7d/attachment.pl>

From binabina at bellsouth.net  Mon Sep 29 00:49:16 2008
From: binabina at bellsouth.net (zubin)
Date: Sun, 28 Sep 2008 18:49:16 -0400
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 52, Issue 21
In-Reply-To: <mailman.3.1222509601.7427.r-sig-finance@stat.math.ethz.ch>
References: <mailman.3.1222509601.7427.r-sig-finance@stat.math.ethz.ch>
Message-ID: <48E009EC.9090801@bellsouth.net>

I would love to come to this, however I live in Atlanta - I may come, 
office I work for is in Chicago.  -zubin


r-sig-finance-request at stat.math.ethz.ch wrote:
> Send R-SIG-Finance mailing list submissions to
> 	r-sig-finance at stat.math.ethz.ch
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> or, via email, send a message with subject or body 'help' to
> 	r-sig-finance-request at stat.math.ethz.ch
>
> You can reach the person managing the list at
> 	r-sig-finance-owner at stat.math.ethz.ch
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-SIG-Finance digest..."
>
>
> Today's Topics:
>
>    1. REMINDER: R/Finance/Chicago October 3rd (Jeff Ryan)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Fri, 26 Sep 2008 13:55:10 -0500
> From: "Jeff Ryan" <jeff.a.ryan at gmail.com>
> Subject: [R-SIG-Finance] REMINDER: R/Finance/Chicago October 3rd
> To: R-SIG-Finance <r-sig-finance at stat.math.ethz.ch>
> Message-ID:
> 	<e8e755250809261155j27c0476dxc4816e45624553bb at mail.gmail.com>
> Content-Type: text/plain; charset=WINDOWS-1252
>
> R-finance useRs [[Chicago]]:
>
> ***A quick reminder of next Friday's event!  Hope to see everyone there!***
>
> I'm happy to announce the first big meeting of R/Finance users in Chicago
> on October 3rd, 2008.
>
> In coordination with major R finance contributors and the support of the
> University of Illinois at Chicago, we invite the local community to an
> informal social gathering after the market close on Friday October 3rd.
>
> The gathering will be at Jak's Tap in the West Loop and will start with a
> casual reception at 3:30.  Details and links can be found at the bottom of
> this email.
>
> The idea is to bring together industry practitioners and academics with a
> common interest in R, from across Chicago-land, to meet face to face.  We
> hope to increase discussion about the future of R and finance.  We will
> also be discussing a much bigger event for the Spring and would like
> feedback as to possible direction and content.
>
> The afternoon/evening will be mostly informal, with formal presentations
> starting around 5.
>
> The formal agenda:
>
> ? We will have a small presentation from some of the locally-based
> contributors as to the state of the R community in finance, as well as a
> vision for the future.  The current presenters include:
>
>  Jeff Ryan [quantmod, xts, IBrokers]
>  Dirk Eddelbuettel [CRAN Empirical Finance task View, RQuantLib, etc]
>  Brian Peterson & Peter Carl [PerformanceAnalytics]
>  Josh Ulrich [TTR, xts, opentick]
>
> ? A presentation by UIC's Finance Department and International Center for
> Futures and Derivatives on: (1) how UIC fits into the R-finance mix, and
> (2) what they have to offer the larger quantitative/trading community in
> Chicago.
>
> Following the formal presentations, we will have the room(s) until 10PM to
> allow for discussion and collaboration among the attendees.
>
> Please RSVP to jeffrey.ryan at insightalgo.com or jeff.a.ryan at gmail.com
>
> What: First R/Finance Chicago Meeting
> Where: Jak's Tap. 901 W. Jackson, Chicago IL  http://jakstap.com/
> When: Friday, October 3, 2008
> Time: 3:30 - 10:00PM (presentations start at 5:00PM)
>
>
>


From binabina at bellsouth.net  Mon Sep 29 05:07:10 2008
From: binabina at bellsouth.net (zubin)
Date: Sun, 28 Sep 2008 23:07:10 -0400
Subject: [R-SIG-Finance] xts time series object - merge
Message-ID: <48E0465E.8060908@bellsouth.net>

Hello, using R to extract data from yahoo using getSymbols.  I have a 
few sets of symbols I need to merge together to conduct some modeling.  
How does one merge multiple xts data objects into one equivalent data 
frame representation so I can explore and model>?


From ggrothendieck at gmail.com  Mon Sep 29 05:53:08 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 28 Sep 2008 23:53:08 -0400
Subject: [R-SIG-Finance] xts time series object - merge
In-Reply-To: <48E0465E.8060908@bellsouth.net>
References: <48E0465E.8060908@bellsouth.net>
Message-ID: <971536df0809282053i7c524ff4uaf6f37ee3c8d4c48@mail.gmail.com>

Try this:

library(quantmod)
getSymbols(c("GOOG", "IBM", "MSFT"))
tech <- merge(GOOG, IBM, MSFT)
?merge.zoo


On Sun, Sep 28, 2008 at 11:07 PM, zubin <binabina at bellsouth.net> wrote:
> Hello, using R to extract data from yahoo using getSymbols.  I have a few
> sets of symbols I need to merge together to conduct some modeling.  How does
> one merge multiple xts data objects into one equivalent data frame
> representation so I can explore and model>?


From binabina at bellsouth.net  Mon Sep 29 06:01:57 2008
From: binabina at bellsouth.net (zubin)
Date: Mon, 29 Sep 2008 00:01:57 -0400
Subject: [R-SIG-Finance] xts time series object - merge
In-Reply-To: <971536df0809282053i7c524ff4uaf6f37ee3c8d4c48@mail.gmail.com>
References: <48E0465E.8060908@bellsouth.net>
	<971536df0809282053i7c524ff4uaf6f37ee3c8d4c48@mail.gmail.com>
Message-ID: <48E05335.7080103@bellsouth.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080929/e49c652a/attachment.pl>

From ggrothendieck at gmail.com  Mon Sep 29 06:39:55 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 29 Sep 2008 00:39:55 -0400
Subject: [R-SIG-Finance] xts time series object - merge
In-Reply-To: <48E054FE.4020409@dowlaty.com>
References: <48E0465E.8060908@bellsouth.net>
	<971536df0809282053i7c524ff4uaf6f37ee3c8d4c48@mail.gmail.com>
	<48E05335.7080103@bellsouth.net> <48E054FE.4020409@dowlaty.com>
Message-ID: <971536df0809282139i19d00596qbc7689f1f290d0ba@mail.gmail.com>

See ?last or ?window.zoo

Be sure to read the xts vignette and the 3 zoo vignettes.

On Mon, Sep 29, 2008 at 12:09 AM, zubin <zubin at dowlaty.com> wrote:
> I did notice the merge produces a 'zoo' object.  How would i subset this new
> zoo object by date, i like to use the subset command similar to
> chartSeries,  where you state for example: 'last 4 months'?
>
>          UYG.Close GLD.Close VIX.Close
> 2007-01-03        NA     62.28     12.04
> 2007-01-04        NA     61.65     11.51
> 2007-01-05        NA     60.17     12.14
> 2007-01-08        NA     60.48     12.00
> 2007-01-09        NA     60.85     11.91
> 2007-01-10        NA     60.59     11.47
>
> zubin wrote:
>
> Sweet, this works well!  thx
>
>
> Gabor Grothendieck wrote:
>
> Try this:
> library(quantmod)
> getSymbols(c("GOOG", "IBM", "MSFT"))
> tech <- merge(GOOG, IBM, MSFT)
> ?merge.zoo
> On Sun, Sep 28, 2008 at 11:07 PM, zubin <binabina at bellsouth.net> wrote:
>
>
> Hello, using R to extract data from yahoo using getSymbols.  I have a few
> sets of symbols I need to merge together to conduct some modeling.  How does
> one merge multiple xts data objects into one equivalent data frame
> representation so I can explore and model>?
>
>
>


From brian at braverock.com  Mon Sep 29 16:23:18 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 29 Sep 2008 09:23:18 -0500
Subject: [R-SIG-Finance] Omega calculation in PerformanceAnalytics
	package
In-Reply-To: <OF09BE7644.75FD63CC-ON852574D2.0041FE09-852574D2.00490B20@malbecpartners.com>
References: <OF09BE7644.75FD63CC-ON852574D2.0041FE09-852574D2.00490B20@malbecpartners.com>
Message-ID: <1222698198.15019.101.camel@ubuntu.braverock.com>

Ryan,

Thank you for spotting this.  I've corrected the function in CVS, and it
will be part of version 0.9.7 of PerformanceAnalytics that we are
currently testing for release to CRAN.

After implementing the function and some initial testing, we didn't find
a lot of utility in the Omega ratio (this could be because of the error
in our calculation!, although the interpolated method gave results which
we also found of questionable utility).

Would you mind sharing why/how you find Omega useful?

Thanks again,

   - Brian

On Sun, 2008-09-28 at 09:15 -0400, ryan.sheftel at malbecpartners.com
wrote:
> 
> I am looking at the Omega() function in the PerformanceAnalytics
> package and it appears there may be an issue with the code and the
> answers it gives. It is entirely possible that it is just my
> understanding of the Omega ratio, but here is what I think needs to be
> fixed. Please correct me if I am wrong: 
> 
> R version 2.5.0 
> PerformanceAnalytics package v 0.9.5 
> 
> I am using the function Omega with the following inputs and result: 
> 
> > Omega(-2:3, L=0, method='simple') 
> [1] 1.5 
> 
> This answer did not seem correct, so I went into the code and see that
> the calculation is: 
> 
> ... 
>     switch(method, simple = { 
>         numerator = exp(-rf) * mean(max(x - L, 0)) 
>         denominator = exp(-rf) * mean(max(L - x, 0)) 
>         omega = numerator/denominator 
> ... 
> 
> From my understanding of the Omega ratio, the problem lies in the use
> of the "max" in the code and not "pmax". The max function returns on
> the single largest value from the vector, so the mean(max()) is the
> same as just max(). I believe the proper code should be: 
> 
> ... 
>     switch(method, simple = { 
>         numerator = exp(-rf) * mean(pmax(x - L, 0)) 
>         denominator = exp(-rf) * mean(pmax(L - x, 0)) 
>         omega = numerator/denominator 
> ... 
> 
> Which would give the proper result of 2. This is because the Omega is
> an average of all the values above the threshold and average of all
> the values below, and not just the extreme values. 
> 
> If this has been fixed in a later version of PerformanceAnalytics, my
> apologies. 
> 
> 
> Ryan Sheftel 
> Malbec Quantys Fund 
> 200 Park Avenue 
> New York, NY 10166 
> (212) 271-4006 
> ryan.sheftel at malbecpartners.com


From ryan.sheftel at malbecpartners.com  Mon Sep 29 16:52:38 2008
From: ryan.sheftel at malbecpartners.com (ryan.sheftel at malbecpartners.com)
Date: Mon, 29 Sep 2008 10:52:38 -0400
Subject: [R-SIG-Finance] Omega calculation in PerformanceAnalytics
	package
In-Reply-To: <1222698198.15019.101.camel@ubuntu.braverock.com>
Message-ID: <OF1559F618.20A30571-ON852574D3.004FECB2-852574D3.0051EB6C@malbecpartners.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080929/7c31671d/attachment.pl>

From jorge.nieves at moorecap.com  Mon Sep 29 19:09:29 2008
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Mon, 29 Sep 2008 13:09:29 -0400
Subject: [R-SIG-Finance] tapply and rownames
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF014EB48C@NYC-XCH3.win.moorecap.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080929/1bee4aaa/attachment.pl>

From julianlhe at gmail.com  Mon Sep 29 19:22:46 2008
From: julianlhe at gmail.com (Julian Lee)
Date: Tue, 30 Sep 2008 01:22:46 +0800
Subject: [R-SIG-Finance] Career/Educational Advice - First Post
Message-ID: <dc0981050809291022s41e426e1k7ec206edb1231d0@mail.gmail.com>

Dear all,

I thought that this would be an excellent place to post a question on
career/educational advice for a bioinformatics specialist who is
interested in stepping foot into the world of quants. However, if the
moderators feel that this post is inappropriate, and perhaps out of
topic, feel free to delete it.

Before i begin, i probably ought to introduce myself. I completed my
undergraduate degree from the National University of SIngapore in the
field of COmputational Science Specializing in Life Science. My main
areas of study was engineering mathematics, programming, and molecular
biology/biochemistry/structural biology. My first job was at a local
polytechnic teaching science and mathematics to first year students. I
then moved to National Cancer Centre as Bioinformatics Specialist,
where i specialize in the analysis of microarrays, genomic data. Some
of you maybe familiar with R. Gentleman's Bioconductor project, and
I've been utilizing many of BIoconductor's packages in my work. My
main project is in the prediction of chemosensitivity of Gastric
Cancer from genomic data. The biologists are currently validating some
of my SVM (support vector machine) computer model and if all goes
well, i will soon make my first publication!

I have been using R for the past 2 years and am quite addicted to it.
In playing around with R/Bioconductor, I bumped into Rmetrics and am
amazed with the work by Diethelm W?rtz and his team of developers.
That got me interested in the field of financial
engineering/computational finance.

Here are some of my questions, (i apologize if there is some
over-simplification on my part)

1. I have been looking at some masters graduate programmes and they
come in all shapes and sizes. Some are more mathematical (theoretical)
than others, while some are computational, ie NYU's mathematical
finance program versus CMU's Computational Finance graduate degree. As
a quant in the industry, does the content of these degrees matter?

2. How much of open-source technology, R/Rmetrics is being used by the
financial industry? Correct me if i'm wrong, but I'm informed that the
standard tool for quants is C++ and VBA, with a wee bit of Matlab.
There is significant difference among the tools mentioned, but I would
like to gauge how much R/Rmetrics has been adopted into the financial
industry. Or has this been placed into the field of academia? I ask
because I think R/bioconductor had similar problems, most complain
about its "user-unfriendlyness", ie lack of GUI, hence difficulty in
using it in their work (many resort to Excel)

3. Is it possible for someone to be a quant without any post-graduate
training? I've just recently joined this forum, and have learnt quite
a bit (i drew my first stock market graph of the KLSE the other day),
from the use of Rmetrics. Simply put, can someone be a quant from
being and a superb R/Rmetrics user, ie thorough understanding of the
functions, when to use them and how to use them etc.

Thank you
regards

julian lee


From dave at kanecap.com  Mon Sep 29 21:17:29 2008
From: dave at kanecap.com (David Kane)
Date: Mon, 29 Sep 2008 15:17:29 -0400
Subject: [R-SIG-Finance] Career/Educational Advice - First Post
In-Reply-To: <dc0981050809291022s41e426e1k7ec206edb1231d0@mail.gmail.com>
References: <dc0981050809291022s41e426e1k7ec206edb1231d0@mail.gmail.com>
Message-ID: <e8ec70e40809291217x330526ccp1abe05b1fe13d655@mail.gmail.com>

My personal opinions . . .

> 1. I have been looking at some masters graduate programmes and they
> come in all shapes and sizes. ...  As
> a quant in the industry, does the content of these degrees matter?

No.

> 2. How much of open-source technology, R/Rmetrics is being used by the
> financial industry?

Lots. If you aren't using R now, you will be someday. Further thoughts at:

http://papers.ssrn.com/sol3/papers.cfm?abstract_id=966354

> 3. Is it possible for someone to be a quant without any post-graduate
> training?

Yes. Your best bet is to get involved with an open source finance
project. That will allow you to build your skills and gain some
expertise, thereby making a job search much easier (although no job
search will be easy this year.

Good luck,

Dave Kane


From dsmith at viciscapital.com  Mon Sep 29 21:56:13 2008
From: dsmith at viciscapital.com (Dale Smith)
Date: Mon, 29 Sep 2008 15:56:13 -0400
Subject: [R-SIG-Finance] Career/Educational Advice - First Post
In-Reply-To: <e8ec70e40809291217x330526ccp1abe05b1fe13d655@mail.gmail.com>
References: <dc0981050809291022s41e426e1k7ec206edb1231d0@mail.gmail.com>
	<e8ec70e40809291217x330526ccp1abe05b1fe13d655@mail.gmail.com>
Message-ID: <AAD6506F4E0FC9448EB9F608C657242F2965E4@vicsrv4.viciscapital.com>

You are apparently working now. I agree, get involved with QuantLib or
an R project and learn what you can. If you know any C++, get Joshi's
"C++ Design Patterns and Derivatives Pricing" (2nd edition). Be prepared
to answer questions about R, C++, and quantitative analysis. RServe is a
fine project to investigate for distributed R calculations: distributed
calculation services may not come up in your interview but it's worth
knowing something as it may come up on the job.

It's going to be a while before hiring picks up.

Dale Smith, Ph.D.
Vicis Capital LLC
Voice: 212-909-4635
Email: dsmith at viciscapital.com
AIM: dsmith11701

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of David Kane
Sent: Monday, September 29, 2008 3:17 PM
To: Julian Lee
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post

My personal opinions . . .

> 1. I have been looking at some masters graduate programmes and they
> come in all shapes and sizes. ...  As
> a quant in the industry, does the content of these degrees matter?

No.

> 2. How much of open-source technology, R/Rmetrics is being used by the
> financial industry?

Lots. If you aren't using R now, you will be someday. Further thoughts
at:

http://papers.ssrn.com/sol3/papers.cfm?abstract_id=966354

> 3. Is it possible for someone to be a quant without any post-graduate
> training?

Yes. Your best bet is to get involved with an open source finance
project. That will allow you to build your skills and gain some
expertise, thereby making a job search much easier (although no job
search will be easy this year.

Good luck,

Dave Kane

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

All e-mail sent to or from this address will be received or otherwise recorded by Vicis Capital, LLC and is subject to archival, monitoring and/or review, by and/or disclosure to, someone other than the recipient.  This message is intended only for the use of the person(s) ("intended recipient") to whom it is addressed.  It may contain information that is privileged and confidential.  If you are not the intended recipient, please contact the sender as soon as possible and delete the message without reading it or making a copy.  Any dissemination, distribution, copying, or other use of this message or any of its content by any person other than the intended recipient is strictly prohibited.  Vicis Capital, LLC only transacts business in states where it is properly registered or notice filed, or excluded or exempted from registration or notice filing requirements.


From mapgxcs at brunel.ac.uk  Tue Sep 30 11:58:18 2008
From: mapgxcs at brunel.ac.uk (mapgxcs at brunel.ac.uk)
Date: Tue, 30 Sep 2008 09:58:18 +0000
Subject: [R-SIG-Finance] Career/Educational Advice - First Post
In-Reply-To: <AAD6506F4E0FC9448EB9F608C657242F2965E4@vicsrv4.viciscapital.com>
References: <dc0981050809291022s41e426e1k7ec206edb1231d0@mail.gmail.com><e8ec70e40809291217x330526ccp1abe05b1fe13d655@mail.gmail.com><AAD6506F4E0FC9448EB9F608C657242F2965E4@vicsrv4.viciscapital.com>
Message-ID: <90182110-1222768979-cardhu_decombobulator_blackberry.rim.net-1696336593-@bxe073.bisx.produk.on.blackberry>

Hi all,
Is R/S+ widely used within the financial institutions? Or it's quants' personal preference?
Regards,
Mc
Michael
Sent from my BlackBerry? wireless device

-----Original Message-----
From: "Dale Smith" <dsmith at viciscapital.com>

Date: Mon, 29 Sep 2008 15:56:13 
To: <r-sig-finance at stat.math.ethz.ch>
Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post


You are apparently working now. I agree, get involved with QuantLib or
an R project and learn what you can. If you know any C++, get Joshi's
"C++ Design Patterns and Derivatives Pricing" (2nd edition). Be prepared
to answer questions about R, C++, and quantitative analysis. RServe is a
fine project to investigate for distributed R calculations: distributed
calculation services may not come up in your interview but it's worth
knowing something as it may come up on the job.

It's going to be a while before hiring picks up.

Dale Smith, Ph.D.
Vicis Capital LLC
Voice: 212-909-4635
Email: dsmith at viciscapital.com
AIM: dsmith11701

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of David Kane
Sent: Monday, September 29, 2008 3:17 PM
To: Julian Lee
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post

My personal opinions . . .

> 1. I have been looking at some masters graduate programmes and they
> come in all shapes and sizes. ...  As
> a quant in the industry, does the content of these degrees matter?

No.

> 2. How much of open-source technology, R/Rmetrics is being used by the
> financial industry?

Lots. If you aren't using R now, you will be someday. Further thoughts
at:

http://papers.ssrn.com/sol3/papers.cfm?abstract_id=966354

> 3. Is it possible for someone to be a quant without any post-graduate
> training?

Yes. Your best bet is to get involved with an open source finance
project. That will allow you to build your skills and gain some
expertise, thereby making a job search much easier (although no job
search will be easy this year.

Good luck,

Dave Kane

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

All e-mail sent to or from this address will be received or otherwise recorded by Vicis Capital, LLC and is subject to archival, monitoring and/or review, by and/or disclosure to, someone other than the recipient.  This message is intended only for the use of the person(s) ("intended recipient") to whom it is addressed.  It may contain information that is privileged and confidential.  If you are not the intended recipient, please contact the sender as soon as possible and delete the message without reading it or making a copy.  Any dissemination, distribution, copying, or other use of this message or any of its content by any person other than the intended recipient is strictly prohibited.  Vicis Capital, LLC only transacts business in states where it is properly registered or notice filed, or excluded or exempted from registration or notice filing requirements.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

From nicolas.mougeot at db.com  Tue Sep 30 12:26:12 2008
From: nicolas.mougeot at db.com (Nicolas Mougeot)
Date: Tue, 30 Sep 2008 11:26:12 +0100
Subject: [R-SIG-Finance] Career/Educational Advice - First Post
In-Reply-To: <90182110-1222768979-cardhu_decombobulator_blackberry.rim.net-1696336593-@bxe073.bisx.produk.on.blackberry>
Message-ID: <OF8089A853.A6E84228-ON802574D4.0038F49A-802574D4.003938E0@db.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080930/6e2486a5/attachment.pl>

From jtl at saxobank.com  Tue Sep 30 12:55:48 2008
From: jtl at saxobank.com (Jeffrey Todd Lins)
Date: Tue, 30 Sep 2008 10:55:48 +0000
Subject: [R-SIG-Finance] Career/Educational Advice - First Post
In-Reply-To: <OF8089A853.A6E84228-ON802574D4.0038F49A-802574D4.003938E0@db.com>
References: <90182110-1222768979-cardhu_decombobulator_blackberry.rim.net-1696336593-@bxe073.bisx.produk.on.blackberry>
	<OF8089A853.A6E84228-ON802574D4.0038F49A-802574D4.003938E0@db.com>
Message-ID: <C88C12BE4A7E7C44841FD636D869B3F6026D2530CE@MALMB3-DK.mid.dom>

We concur.


Jeffrey Todd Lins  | Executive Director - Quantitative Analysis and Advanced Research Center

Saxo Bank A/S | Smakkedalen 2 | DK-2820 Gentofte
Company phone: +45 39 77 40 00 | Direct phone: +45 39 77 40 81 | Fax number: +45 39 77 42 00






-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Nicolas Mougeot
Sent: Tuesday, September 30, 2008 12:26 PM
To: mapgxcs at brunel.ac.uk
Cc: r-sig-finance at stat.math.ethz.ch; r-sig-finance-bounces at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post

R is free, free is good.
and it's flexible, and it benefits from inputs from a lot of people used in a number of quant teams in investment banks

________________________________________
Nicolas Mougeot
Managing Director

Head of Global Equity Derivatives Strategy - Europe Deutsche Bank AG London

+44 207 545 2198
+ 44 79 612 18375

eDerivatives.db.com -- it's where vol lives

Please click on this link and read the disclaimer:
https://ederivatives.db.com/static/edsg_disclaimer.html




mapgxcs at brunel.ac.uk
Sent by: r-sig-finance-bounces at stat.math.ethz.ch
09/30/2008 10:58 AM
Please respond to
mapgxcs at brunel.ac.uk


To
"Dale Smith" <dsmith at viciscapital.com>,
r-sig-finance-bounces at stat.math.ethz.ch, r-sig-finance at stat.math.ethz.ch cc

Subject
Re: [R-SIG-Finance] Career/Educational Advice - First Post






Hi all,
Is R/S+ widely used within the financial institutions? Or it's quants'
personal preference?
Regards,
Mc
Michael
Sent from my BlackBerry(r) wireless device

-----Original Message-----
From: "Dale Smith" <dsmith at viciscapital.com>

Date: Mon, 29 Sep 2008 15:56:13
To: <r-sig-finance at stat.math.ethz.ch>
Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post


You are apparently working now. I agree, get involved with QuantLib or an R project and learn what you can. If you know any C++, get Joshi's "C++ Design Patterns and Derivatives Pricing" (2nd edition). Be prepared to answer questions about R, C++, and quantitative analysis. RServe is a fine project to investigate for distributed R calculations: distributed calculation services may not come up in your interview but it's worth knowing something as it may come up on the job.

It's going to be a while before hiring picks up.

Dale Smith, Ph.D.
Vicis Capital LLC
Voice: 212-909-4635
Email: dsmith at viciscapital.com
AIM: dsmith11701

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of David Kane
Sent: Monday, September 29, 2008 3:17 PM
To: Julian Lee
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post

My personal opinions . . .

> 1. I have been looking at some masters graduate programmes and they
> come in all shapes and sizes. ...  As a quant in the industry, does
> the content of these degrees matter?

No.

> 2. How much of open-source technology, R/Rmetrics is being used by the
> financial industry?

Lots. If you aren't using R now, you will be someday. Further thoughts
at:

http://papers.ssrn.com/sol3/papers.cfm?abstract_id=966354

> 3. Is it possible for someone to be a quant without any post-graduate
> training?

Yes. Your best bet is to get involved with an open source finance project. That will allow you to build your skills and gain some expertise, thereby making a job search much easier (although no job search will be easy this year.

Good luck,

Dave Kane

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

All e-mail sent to or from this address will be received or otherwise recorded by Vicis Capital, LLC and is subject to archival, monitoring and/or review, by and/or disclosure to, someone other than the recipient.
This message is intended only for the use of the person(s) ("intended
recipient") to whom it is addressed.  It may contain information that is privileged and confidential.  If you are not the intended recipient, please contact the sender as soon as possible and delete the message without reading it or making a copy.  Any dissemination, distribution, copying, or other use of this message or any of its content by any person other than the intended recipient is strictly prohibited.  Vicis Capital, LLC only transacts business in states where it is properly registered or notice filed, or excluded or exempted from registration or notice filing requirements.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.




---

This e-mail may contain confidential and/or privileged i...{{dropped:11}}


This email may contain confidential and/or privileged in...{{dropped:12}}


From patrick at burns-stat.com  Tue Sep 30 13:18:48 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 30 Sep 2008 12:18:48 +0100
Subject: [R-SIG-Finance] Career/Educational Advice - First Post
In-Reply-To: <90182110-1222768979-cardhu_decombobulator_blackberry.rim.net-1696336593-@bxe073.bisx.produk.on.blackberry>
References: <dc0981050809291022s41e426e1k7ec206edb1231d0@mail.gmail.com><e8ec70e40809291217x330526ccp1abe05b1fe13d655@mail.gmail.com><AAD6506F4E0FC9448EB9F608C657242F2965E4@vicsrv4.viciscapital.com>
	<90182110-1222768979-cardhu_decombobulator_blackberry.rim.net-1696336593-@bxe073.bisx.produk.on.blackberry>
Message-ID: <48E20B18.8020305@burns-stat.com>

mapgxcs at brunel.ac.uk wrote:
> Hi all,
> Is R/S+ widely used within the financial institutions? Or it's quants' personal preference?
>   

Keep in mind that none of us sees the entire elephant,
so reality may be somewhat different than I picture it.

The short answer is that R has made inroads into finance
that are much larger than I've been expecting.  Perhaps
I've become too cynical (though given present news, that
seems hardly likely).

Earlier in this thread there was the idea that C++ and
Matlab are widely used.  That is both true and false.

In derivatives pricing and related activities, C++ seems
to be the thing.  (But note that there has been some effort
lately to bring R in as the frontend.)

Matlab tends to be heavily used in fixed income.  Fixed
income is more mathematical (as opposed to statistical)
than equities, so Matlab makes more sense there.  Plus
a lot of such fixed income work was started before the S
language took hold.  But R is being used some in fixed
income (which is statistical as well as mathematical).

Some firms use SAS.  These tend to be firms that were
among the first to realize that finance is a statistical field.
It boggles my mind that they still use SAS, but there you
go.

There is a massive amount of things done in Excel that
would better be done in R.  See
http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
for a more extended rant on that topic.

To answer the question of R being quants' personal
preference: Sometimes, but not always.  However,
R almost always becomes the personal preference of
those who have had it imposed upon them.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")
> Regards,
> Mc
> Michael
> Sent from my BlackBerry? wireless device
>
> -----Original Message-----
> From: "Dale Smith" <dsmith at viciscapital.com>
>
> Date: Mon, 29 Sep 2008 15:56:13 
> To: <r-sig-finance at stat.math.ethz.ch>
> Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post
>
>
> You are apparently working now. I agree, get involved with QuantLib or
> an R project and learn what you can. If you know any C++, get Joshi's
> "C++ Design Patterns and Derivatives Pricing" (2nd edition). Be prepared
> to answer questions about R, C++, and quantitative analysis. RServe is a
> fine project to investigate for distributed R calculations: distributed
> calculation services may not come up in your interview but it's worth
> knowing something as it may come up on the job.
>
> It's going to be a while before hiring picks up.
>
> Dale Smith, Ph.D.
> Vicis Capital LLC
> Voice: 212-909-4635
> Email: dsmith at viciscapital.com
> AIM: dsmith11701
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of David Kane
> Sent: Monday, September 29, 2008 3:17 PM
> To: Julian Lee
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post
>
> My personal opinions . . .
>
>   
>> 1. I have been looking at some masters graduate programmes and they
>> come in all shapes and sizes. ...  As
>> a quant in the industry, does the content of these degrees matter?
>>     
>
> No.
>
>   
>> 2. How much of open-source technology, R/Rmetrics is being used by the
>> financial industry?
>>     
>
> Lots. If you aren't using R now, you will be someday. Further thoughts
> at:
>
> http://papers.ssrn.com/sol3/papers.cfm?abstract_id=966354
>
>   
>> 3. Is it possible for someone to be a quant without any post-graduate
>> training?
>>     
>
> Yes. Your best bet is to get involved with an open source finance
> project. That will allow you to build your skills and gain some
> expertise, thereby making a job search much easier (although no job
> search will be easy this year.
>
> Good luck,
>
> Dave Kane
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> All e-mail sent to or from this address will be received or otherwise recorded by Vicis Capital, LLC and is subject to archival, monitoring and/or review, by and/or disclosure to, someone other than the recipient.  This message is intended only for the use of the person(s) ("intended recipient") to whom it is addressed.  It may contain information that is privileged and confidential.  If you are not the intended recipient, please contact the sender as soon as possible and delete the message without reading it or making a copy.  Any dissemination, distribution, copying, or other use of this message or any of its content by any person other than the intended recipient is strictly prohibited.  Vicis Capital, LLC only transacts business in states where it is properly registered or notice filed, or excluded or exempted from registration or notice filing requirements.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
>


From mapgxcs at brunel.ac.uk  Tue Sep 30 15:01:21 2008
From: mapgxcs at brunel.ac.uk (mapgxcs at brunel.ac.uk)
Date: Tue, 30 Sep 2008 13:01:21 +0000
Subject: [R-SIG-Finance] Career/Educational Advice - First Post
In-Reply-To: <48E20B18.8020305@burns-stat.com>
References: <dc0981050809291022s41e426e1k7ec206edb1231d0@mail.gmail.com><e8ec70e40809291217x330526ccp1abe05b1fe13d655@mail.gmail.com><AAD6506F4E0FC9448EB9F608C657242F2965E4@vicsrv4.viciscapital.com>
	<90182110-1222768979-cardhu_decombobulator_blackberry.rim.net-1696336593-@bxe073.bisx.produk.on.blackberry><48E20B18.8020305@burns-stat.com>
Message-ID: <911976394-1222779962-cardhu_decombobulator_blackberry.rim.net-700402227-@bxe073.bisx.produk.on.blackberry>

Thanks Patrick. Very helpful comments.
Also thanks to all other replies!
Will sticky on R :-)
Regards
Michael
Sent from my BlackBerry? wireless device

-----Original Message-----
From: Patrick Burns <patrick at burns-stat.com>

Date: Tue, 30 Sep 2008 12:18:48 
To: <XIAOCHEN.Sun at brunel.ac.uk>
Cc: Dale Smith<dsmith at viciscapital.com>; <r-sig-finance at stat.math.ethz.ch>
Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post


mapgxcs at brunel.ac.uk wrote:
> Hi all,
> Is R/S+ widely used within the financial institutions? Or it's quants' personal preference?
>   

Keep in mind that none of us sees the entire elephant,
so reality may be somewhat different than I picture it.

The short answer is that R has made inroads into finance
that are much larger than I've been expecting.  Perhaps
I've become too cynical (though given present news, that
seems hardly likely).

Earlier in this thread there was the idea that C++ and
Matlab are widely used.  That is both true and false.

In derivatives pricing and related activities, C++ seems
to be the thing.  (But note that there has been some effort
lately to bring R in as the frontend.)

Matlab tends to be heavily used in fixed income.  Fixed
income is more mathematical (as opposed to statistical)
than equities, so Matlab makes more sense there.  Plus
a lot of such fixed income work was started before the S
language took hold.  But R is being used some in fixed
income (which is statistical as well as mathematical).

Some firms use SAS.  These tend to be firms that were
among the first to realize that finance is a statistical field.
It boggles my mind that they still use SAS, but there you
go.

There is a massive amount of things done in Excel that
would better be done in R.  See
http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
for a more extended rant on that topic.

To answer the question of R being quants' personal
preference: Sometimes, but not always.  However,
R almost always becomes the personal preference of
those who have had it imposed upon them.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")
> Regards,
> Mc
> Michael
> Sent from my BlackBerry? wireless device
>
> -----Original Message-----
> From: "Dale Smith" <dsmith at viciscapital.com>
>
> Date: Mon, 29 Sep 2008 15:56:13 
> To: <r-sig-finance at stat.math.ethz.ch>
> Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post
>
>
> You are apparently working now. I agree, get involved with QuantLib or
> an R project and learn what you can. If you know any C++, get Joshi's
> "C++ Design Patterns and Derivatives Pricing" (2nd edition). Be prepared
> to answer questions about R, C++, and quantitative analysis. RServe is a
> fine project to investigate for distributed R calculations: distributed
> calculation services may not come up in your interview but it's worth
> knowing something as it may come up on the job.
>
> It's going to be a while before hiring picks up.
>
> Dale Smith, Ph.D.
> Vicis Capital LLC
> Voice: 212-909-4635
> Email: dsmith at viciscapital.com
> AIM: dsmith11701
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of David Kane
> Sent: Monday, September 29, 2008 3:17 PM
> To: Julian Lee
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post
>
> My personal opinions . . .
>
>   
>> 1. I have been looking at some masters graduate programmes and they
>> come in all shapes and sizes. ...  As
>> a quant in the industry, does the content of these degrees matter?
>>     
>
> No.
>
>   
>> 2. How much of open-source technology, R/Rmetrics is being used by the
>> financial industry?
>>     
>
> Lots. If you aren't using R now, you will be someday. Further thoughts
> at:
>
> http://papers.ssrn.com/sol3/papers.cfm?abstract_id=966354
>
>   
>> 3. Is it possible for someone to be a quant without any post-graduate
>> training?
>>     
>
> Yes. Your best bet is to get involved with an open source finance
> project. That will allow you to build your skills and gain some
> expertise, thereby making a job search much easier (although no job
> search will be easy this year.
>
> Good luck,
>
> Dave Kane
>
>_______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> All e-mail sent to or from this address will be received or otherwise recorded by Vicis Capital, LLC and is subject to archival, monitoring and/or review, by and/or disclosure to, someone other than the recipient.  This message is intended only for the use of the person(s) ("intended recipient") to whom it is addressed.  It may contain information that is privileged and confidential.  If you are not the intended recipient, please contact the sender as soon as possible and delete the message without reading it or making a copy.  Any dissemination, distribution, copying, or other use of this message or any of its content by any person other than the intended recipient is strictly prohibited.  Vicis Capital, LLC only transacts business in states where it is properly registered or notice filed, or excluded or exempted from registration or notice filing requirements.
>
>_______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>_______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
>   

From fgochez at mango-solutions.com  Tue Sep 30 15:30:23 2008
From: fgochez at mango-solutions.com (Francisco Gochez)
Date: Tue, 30 Sep 2008 14:30:23 +0100
Subject: [R-SIG-Finance] Open source in finance (was: RE: Career/Educational
	Advice - First Post)
References: <dc0981050809291022s41e426e1k7ec206edb1231d0@mail.gmail.com>
	<e8ec70e40809291217x330526ccp1abe05b1fe13d655@mail.gmail.com>
Message-ID: <3CBFCFB1FEFFA841BA83ADF2F2A9C6FA067C00@mango-data1.Mango.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080930/6dd1151b/attachment.pl>

From davidr at rhotrading.com  Tue Sep 30 17:05:55 2008
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 30 Sep 2008 10:05:55 -0500
Subject: [R-SIG-Finance] Career/Educational Advice - First Post
In-Reply-To: <48E20B18.8020305@burns-stat.com>
References: <dc0981050809291022s41e426e1k7ec206edb1231d0@mail.gmail.com><e8ec70e40809291217x330526ccp1abe05b1fe13d655@mail.gmail.com><AAD6506F4E0FC9448EB9F608C657242F2965E4@vicsrv4.viciscapital.com><90182110-1222768979-cardhu_decombobulator_blackberry.rim.net-1696336593-@bxe073.bisx.produk.on.blackberry>
	<48E20B18.8020305@burns-stat.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE77018CA7D7@rhopost.rhotrading.com>

Much of what happens in the financial world is proprietary, 
and some firms are quite paranoid, so any sharing is out.

A firm I worked at in the late eighties and early nineties used 
S then S-Plus very heavily in many aspects of analysis, but very 
little in production. Most places I've worked since then have had 
S-Plus or I've used R. A few have had Matlab as well. 

However, all real-time option valuation, etc., must be done in a 
fast, complied language, so there's always going to be something 
_like_ C required for quant development for production.


David L. Reiner, PhD
Head Quant
Rho Trading Securities, LLC


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Patrick
Burns
Sent: Tuesday, September 30, 2008 6:19 AM
To: mapgxcs at brunel.ac.uk
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post

mapgxcs at brunel.ac.uk wrote:
> Hi all,
> Is R/S+ widely used within the financial institutions? Or it's quants'
personal preference?
>   

Keep in mind that none of us sees the entire elephant,
so reality may be somewhat different than I picture it.

The short answer is that R has made inroads into finance
that are much larger than I've been expecting.  Perhaps
I've become too cynical (though given present news, that
seems hardly likely).

Earlier in this thread there was the idea that C++ and
Matlab are widely used.  That is both true and false.

In derivatives pricing and related activities, C++ seems
to be the thing.  (But note that there has been some effort
lately to bring R in as the frontend.)

Matlab tends to be heavily used in fixed income.  Fixed
income is more mathematical (as opposed to statistical)
than equities, so Matlab makes more sense there.  Plus
a lot of such fixed income work was started before the S
language took hold.  But R is being used some in fixed
income (which is statistical as well as mathematical).

Some firms use SAS.  These tend to be firms that were
among the first to realize that finance is a statistical field.
It boggles my mind that they still use SAS, but there you
go.

There is a massive amount of things done in Excel that
would better be done in R.  See
http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
for a more extended rant on that topic.

To answer the question of R being quants' personal
preference: Sometimes, but not always.  However,
R almost always becomes the personal preference of
those who have had it imposed upon them.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")
> Regards,
> Mc
> Michael
> Sent from my BlackBerry(r) wireless device
>
> -----Original Message-----
> From: "Dale Smith" <dsmith at viciscapital.com>
>
> Date: Mon, 29 Sep 2008 15:56:13 
> To: <r-sig-finance at stat.math.ethz.ch>
> Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post
>
>
> You are apparently working now. I agree, get involved with QuantLib or
> an R project and learn what you can. If you know any C++, get Joshi's
> "C++ Design Patterns and Derivatives Pricing" (2nd edition). Be
prepared
> to answer questions about R, C++, and quantitative analysis. RServe is
a
> fine project to investigate for distributed R calculations:
distributed
> calculation services may not come up in your interview but it's worth
> knowing something as it may come up on the job.
>
> It's going to be a while before hiring picks up.
>
> Dale Smith, Ph.D.
> Vicis Capital LLC
> Voice: 212-909-4635
> Email: dsmith at viciscapital.com
> AIM: dsmith11701
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of David
Kane
> Sent: Monday, September 29, 2008 3:17 PM
> To: Julian Lee
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Career/Educational Advice - First Post
>
> My personal opinions . . .
>
>   
>> 1. I have been looking at some masters graduate programmes and they
>> come in all shapes and sizes. ...  As
>> a quant in the industry, does the content of these degrees matter?
>>     
>
> No.
>
>   
>> 2. How much of open-source technology, R/Rmetrics is being used by
the
>> financial industry?
>>     
>
> Lots. If you aren't using R now, you will be someday. Further thoughts
> at:
>
> http://papers.ssrn.com/sol3/papers.cfm?abstract_id=966354
>
>   
>> 3. Is it possible for someone to be a quant without any post-graduate
>> training?
>>     
>
> Yes. Your best bet is to get involved with an open source finance
> project. That will allow you to build your skills and gain some
> expertise, thereby making a job search much easier (although no job
> search will be easy this year.
>
> Good luck,
>
> Dave Kane
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> All e-mail sent to or from this address will be received or otherwise
recorded by Vicis Capital, LLC and is subject to archival, monitoring
and/or review, by and/or disclosure to, someone other than the
recipient.  This message is intended only for the use of the person(s)
("intended recipient") to whom it is addressed.  It may contain
information that is privileged and confidential.  If you are not the
intended recipient, please contact the sender as soon as possible and
delete the message without reading it or making a copy.  Any
dissemination, distribution, copying, or other use of this message or
any of its content by any person other than the intended recipient is
strictly prohibited.  Vicis Capital, LLC only transacts business in
states where it is properly registered or notice filed, or excluded or
exempted from registration or notice filing requirements.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From ryan.sheftel at malbecpartners.com  Tue Sep 30 20:36:20 2008
From: ryan.sheftel at malbecpartners.com (ryan.sheftel at malbecpartners.com)
Date: Tue, 30 Sep 2008 14:36:20 -0400
Subject: [R-SIG-Finance] PerformanceAnalytics and the UpsidePotentialRatio
Message-ID: <OFCA7BB79E.41CA5672-ON852574D4.00650CFC-852574D4.00666670@malbecpartners.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080930/d2dc8809/attachment.pl>

From brian at braverock.com  Tue Sep 30 20:59:04 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 30 Sep 2008 13:59:04 -0500
Subject: [R-SIG-Finance] PerformanceAnalytics and the
	UpsidePotentialRatio
In-Reply-To: <OFCA7BB79E.41CA5672-ON852574D4.00650CFC-852574D4.00666670@malbecpartners.com>
References: <OFCA7BB79E.41CA5672-ON852574D4.00650CFC-852574D4.00666670@malbecpartners.com>
Message-ID: <1222801144.11730.18.camel@ubuntu.braverock.com>

Ryan,

We previously encountered this problem in the DownsideDeviation
function.  When looking at Sortino's various writings on the subject, we
found that both interpretations (full series versus subset series) had
been used in published papers.  

For DownsideDeviation, we implemented a "method" argument to allow the
user to choose a "full" or "subset" series to be applied to the
calculation, but did not carry that through to UpsidePotentialRatio.

I'll give it some thought and extend the "method" argument to the
UpsidePotentialRatio function as well.  I'll try to post a patched
function here that supports the "method" argument soon.

Thank you very much for your thoughtful comments:  questions,
suggestions, and reports from users improve PerformanceAnalytics for all
of us.  Keep it coming!

Regards,

   - Brian

On Tue, 2008-09-30 at 14:36 -0400, ryan.sheftel at malbecpartners.com
wrote:
> 
> Continuing to look at the PerformanceAnalytic package and this time
> the function UpsidePotentialRatio 
> 
> The code in the package is: 
> 
> > UpsidePotentialRatio 
> function (Ra, MAR = 0) 
> { 
>     Ra = checkData(Ra, method = "vector") 
>     r = subset(Ra, Ra > MAR) 
>     return((sum(r - MAR)/(length(Ra)))/DownsideDeviation(Ra, MAR)) 
> } 
> 
> I believe there are two problems with this calculation according to my
> understanding of the ratio. 
> 
> - I think the numerator should be the average return of the
> observations in excess of the MAR, so the lenght(Ra) should be
> lenght(r). 
> 
>         (sum(r - MAR)/(length(r))) 
> 
> - Second the denominator should the the deviation below the MAR, but
> again only for the observations below the MAR. The DownsideDeviation
> function looks to again be using all observations: 
> 
> > DownsideDeviation 
> function (Ra, MAR = 0) 
> { 
>     Ra = checkDataVector(Ra) 
>     r = subset(Ra, Ra < MAR) 
>     return(sqrt(sum((r - MAR)^2)/(length(Ra)))) 
> } 
> 
> In here the final "length(Ra)" should be "length(r)" 
> 
> 
> Thanks for taking a look. The package is very impressive.


From brian at braverock.com  Tue Sep 30 23:14:55 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 30 Sep 2008 16:14:55 -0500
Subject: [R-SIG-Finance] PerformanceAnalytics and the
	UpsidePotentialRatio
In-Reply-To: <OFCA7BB79E.41CA5672-ON852574D4.00650CFC-852574D4.00666670@malbecpartners.com>
References: <OFCA7BB79E.41CA5672-ON852574D4.00650CFC-852574D4.00666670@malbecpartners.com>
Message-ID: <1222809295.11730.35.camel@ubuntu.braverock.com>

Ryan,

I've updated the code to allow the use of a "method" argument for all
the related functions.  I was wondering if I could impose upon you to
check my understanding of something.

In June of 2007, I committed changes to the DownsideDeviation function
with the comment: "fixed to use length of entire series, per Platinga,
van der Meer, Sortino 2001", and in August 2007, I added the method
argument so that users could choose.  

If you have the time and inclination, would you be willing to take a
look at the reference and see if yourread of the paper matches my
understanding that the length of the full series should be the default?
 
Plantinga, A., van der Meer, R. and Sortino, F. The Impact of Downside
Risk on Risk-Adjusted Performance of Mutual Funds in the Euronext
Markets. July 19, 2001. Available at SSRN:
http://ssrn.com/abstract=277352


I'm mostly concerned with setting the best default here.  All the
functions now support the method argument, I just want to make sure that
the default is as rational as possible.  I've copied the list in case
any one else wants to chime in as well.

Regards,

   - Brian

On Tue, 2008-09-30 at 14:36 -0400, ryan.sheftel at malbecpartners.com
wrote:
> 
> Continuing to look at the PerformanceAnalytic package and this time
> the function UpsidePotentialRatio 
> 
> The code in the package is: 
> 
> > UpsidePotentialRatio 
> function (Ra, MAR = 0) 
> { 
>     Ra = checkData(Ra, method = "vector") 
>     r = subset(Ra, Ra > MAR) 
>     return((sum(r - MAR)/(length(Ra)))/DownsideDeviation(Ra, MAR)) 
> } 
> 
> I believe there are two problems with this calculation according to my
> understanding of the ratio. 
> 
> - I think the numerator should be the average return of the
> observations in excess of the MAR, so the lenght(Ra) should be
> lenght(r). 
> 
>         (sum(r - MAR)/(length(r))) 
> 
> - Second the denominator should the the deviation below the MAR, but
> again only for the observations below the MAR. The DownsideDeviation
> function looks to again be using all observations: 
> 
> > DownsideDeviation 
> function (Ra, MAR = 0) 
> { 
>     Ra = checkDataVector(Ra) 
>     r = subset(Ra, Ra < MAR) 
>     return(sqrt(sum((r - MAR)^2)/(length(Ra)))) 
> } 
> 
> In here the final "length(Ra)" should be "length(r)" 
> 
> 
> Thanks for taking a look. The package is very impressive.


From ryan.sheftel at malbecpartners.com  Tue Sep 30 23:23:22 2008
From: ryan.sheftel at malbecpartners.com (ryan.sheftel at malbecpartners.com)
Date: Tue, 30 Sep 2008 17:23:22 -0400
Subject: [R-SIG-Finance] [Maybe Content Spam] Re: PerformanceAnalytics
 and the UpsidePotentialRatio
In-Reply-To: <1222809295.11730.35.camel@ubuntu.braverock.com>
Message-ID: <OFF5D872D8.D0AF3AD9-ON852574D4.00757AA7-852574D4.0075B13D@malbecpartners.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080930/dff98310/attachment.pl>

