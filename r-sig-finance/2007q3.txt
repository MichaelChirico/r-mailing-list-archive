From cgb at datanalytics.com  Sun Jul  1 01:16:37 2007
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Sun, 01 Jul 2007 00:16:37 +0100
Subject: [R-SIG-Finance] Method dispatch in functions?
In-Reply-To: <972441.14518.qm@web35410.mail.mud.yahoo.com>
References: <972441.14518.qm@web35410.mail.mud.yahoo.com>
Message-ID: <1183245397.6000.6.camel@localhost>

Look at the UseMethod function. The help for the "print" method, a
heavily overloaded function, can also help.

Regards,

Carlos J. Gil Bellosta
http://www.datanalytics.com

On Thu, 2007-06-28 at 09:05 -0700, John McHenry wrote:
> Hi,
> 
> Could someone point me in the right direction for documentation on the following question? 
> 
> Let's say I have two objects a and b of classes A and B, respectively.
> Now let's say I write a function foo that does something similar to 
> objects of type A and B. Basically I want to overload the function
> in C++ talk, so if I give foo and object of type A something (and this
> is my question) dispatches the call to, say, foo.A, and if I give foo
> and object of type B something dispatches the call to, say, foo.B.
> 
> I want to write foo.A and foo.B. How to I perform the method 
> dispatch? From what I understand there are two ways in R:
> S3 and S4. What is the simple S3 way?
> 
> Thanks!
> 
> Jack.
> 
>        
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.


From rory.winston at gmail.com  Mon Jul  2 22:30:59 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Mon, 2 Jul 2007 21:30:59 +0100
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 38, Issue 1
In-Reply-To: <mailman.13.1183284006.19627.r-sig-finance@stat.math.ethz.ch>
References: <mailman.13.1183284006.19627.r-sig-finance@stat.math.ethz.ch>
Message-ID: <3f446aa30707021330s69a89738te7139ec9bb6f55ac@mail.gmail.com>

Hi all

I have a question re: adding legends to a series of plots. I am
creating a lag plot of 100 days worth of EUR/USD log returns. I just
type lag.plot(logR, 100), and it creates a nice array of subplots. Now
what I would like to do is add a legend (or X label, alternatively) to
each plot, showing the correlation coefficient for each lag. So each
subplot would have a legend or label saying p=0.9....etc. The trouble
is, I have no idea how to do this. Does anyone have any experience
with annotating subplots? Or would I need to do it all in one big loop
(set up the subplot window using par(), plot one lag at a time,
calculate rho and then add a legend)?

Thanks
Rory


From stoicsam at gmail.com  Tue Jul  3 00:37:17 2007
From: stoicsam at gmail.com (Samit Shah)
Date: Mon, 2 Jul 2007 23:37:17 +0100
Subject: [R-SIG-Finance] New to R . .
Message-ID: <a464c480707021537v59ee3d5bq39edb11a1d1f951e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070702/15f27b85/attachment.pl 

From ggrothendieck at gmail.com  Tue Jul  3 03:50:09 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 2 Jul 2007 21:50:09 -0400
Subject: [R-SIG-Finance] New to R . .
In-Reply-To: <a464c480707021537v59ee3d5bq39edb11a1d1f951e@mail.gmail.com>
References: <a464c480707021537v59ee3d5bq39edb11a1d1f951e@mail.gmail.com>
Message-ID: <971536df0707021850v1f81d30tf4fabda61dd296a8@mail.gmail.com>

Read the Introduction to R manual and various sources pointed at here:
https://stat.ethz.ch/pipermail/r-help/2007-June/134096.html

Also the zoo package vignette has many examples of time
series manipulation.  After installing zoo, from within R:
library(zoo)
vignette("zoo")

On 7/2/07, Samit Shah <stoicsam at gmail.com> wrote:
> Hello,
> I just started using R last week and come from having used mostly SAS. I am
> using it more for data mining and analysis rather than technical statistics
> (just basic descriptive statistics). There are some things I want to get out
> of the data but I think I am stuck because a lack of understanding the
> language (and vectors too - i usually work in data frames). I have time
> series data of funds, their cashflows, the type of cashflow (3 levels), the
> specific date of the cashflow in addition to the fund year, fund size and
> the classification of fund. I want to be able to group the funds together by
> fund year and then group that subset by cashflow type or by fund size
> (summing the cashflow by type by fund year and fund id). It would be nice to
> see the distribution of fund sizes around specific vintage years as well as
> the ratio of calls to fund size for a given time period.
>
> Also, I would like to subtract the cashflow dates from the first cashflow
> date by fund and then group the funds by their cashflow date bucket and see
> how they compare to each other (aggregating over cashflow type, etc)
>
> If there are good books that go more into the code with some illustrative
> examples, I would appreciate the title and/or reference info. I have found
> one online datamining source but it would be nice to get some more
> resources. If you know of any R training centers or professionals in London
> I can be put in touch with I would appreciate that too.
>
> Thanks,
> Samit
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ianseow at gmail.com  Tue Jul  3 09:20:51 2007
From: ianseow at gmail.com (Ian Seow)
Date: Tue, 3 Jul 2007 15:20:51 +0800
Subject: [R-SIG-Finance] Intraday data with RBloomberg
Message-ID: <1865010707030020x1332441dpd8a90371138e905c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070703/eb5e0b0b/attachment.pl 

From ianseow at gmail.com  Tue Jul  3 09:25:17 2007
From: ianseow at gmail.com (Ian Seow)
Date: Tue, 3 Jul 2007 15:25:17 +0800
Subject: [R-SIG-Finance] Intraday data with RBloomberg
In-Reply-To: <1865010707030020x1332441dpd8a90371138e905c@mail.gmail.com>
References: <1865010707030020x1332441dpd8a90371138e905c@mail.gmail.com>
Message-ID: <1865010707030025o5f4adee6j12ead01b979a6913@mail.gmail.com>

Hi, I'm currently trying to implement an intraday currency model using
a live feed from bloomberg.
I hit the following error when I attempt to download intraday 2 min ticks:

> usdjpy=blpGetData(conn,securities='USDJPY Curncy',fields=c('BID','ASK'), start= as.chron(Sys.time()-6000), end=as.chron(Sys.time()),barsize=2, barfields='OPEN')

> usdjpy
                    BID.OPEN ASK.OPEN
(07/03/07 15:01:45)       NA       NA

blpGetData works fine for historical price data and seems to work fine
for commodity futures ( e.g. the example above works fine for ED1
Comdty), so I'm puzzled why this function does not return a result for
Curncy. I double-checked the "USDJPY Curncy" intraday bloomberg API
feed in excel and it works.

Also, is it a good idea in general to avoid implementing such models
in R?  Would C/C++ be a better alternative?
Any ideas / insights would be greatly appreciated! Thanks.


Best Regards
Ian Seow


From Bernhard_Pfaff at fra.invesco.com  Tue Jul  3 09:29:24 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Tue, 3 Jul 2007 08:29:24 +0100
Subject: [R-SIG-Finance] New to R . .
In-Reply-To: <a464c480707021537v59ee3d5bq39edb11a1d1f951e@mail.gmail.com>
References: <a464c480707021537v59ee3d5bq39edb11a1d1f951e@mail.gmail.com>
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C30991A8D2@GBHENXMB02.corp.amvescap.net>

>Hello,
>I just started using R last week and come from having used 
>mostly SAS. I am
>using it more for data mining and analysis rather than 
>technical statistics
>(just basic descriptive statistics). There are some things I 
>want to get out
>of the data but I think I am stuck because a lack of understanding the
>language (and vectors too - i usually work in data frames). I have time
>series data of funds, their cashflows, the type of cashflow (3 
>levels), the
>specific date of the cashflow in addition to the fund year, 
>fund size and
>the classification of fund. I want to be able to group the 
>funds together by
>fund year and then group that subset by cashflow type or by fund size
>(summing the cashflow by type by fund year and fund id). It 
>would be nice to
>see the distribution of fund sizes around specific vintage 
>years as well as
>the ratio of calls to fund size for a given time period.
>
>Also, I would like to subtract the cashflow dates from the 
>first cashflow
>date by fund and then group the funds by their cashflow date 
>bucket and see
>how they compare to each other (aggregating over cashflow type, etc)
>
>If there are good books that go more into the code with some 
>illustrative
>examples, I would appreciate the title and/or reference info. 
>I have found
>one online datamining source but it would be nice to get some more
>resources. If you know of any R training centers or 
>professionals in London
>I can be put in touch with I would appreciate that too.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


Hello Samit,

you might want to contact Patrick Burns: http://www.burns-stat.com/

Best,
Bernhard

>
>Thanks,
>Samit
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From josh at gghc.com  Tue Jul  3 14:52:48 2007
From: josh at gghc.com (Joshua Reich)
Date: Tue, 3 Jul 2007 08:52:48 -0400
Subject: [R-SIG-Finance] New to R . .
In-Reply-To: <a464c480707021537v59ee3d5bq39edb11a1d1f951e@mail.gmail.com>
References: <a464c480707021537v59ee3d5bq39edb11a1d1f951e@mail.gmail.com>
Message-ID: <C20EA84D76C94F4E999DC041E81C0D110256E825@exchange2k3.ny.gghc.com>

Although I have been using R for a while, "Econometrics in R" gave me
the best introduction to the language, especially in terms of learning
how to group, select and sort data. See
http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf -
section 2.1 in particular for selecting specific rows or columns from a
matrix (e.g., wealth[wealth$year==1980,]). From there, the various
'apply' functions can be used to perform custom aggregations on your
selections.

Josh

 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Samit Shah
Sent: Monday, July 02, 2007 6:37 PM
To: R-SIG-Finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] New to R . .

Hello,
I just started using R last week and come from having used mostly SAS. I
am using it more for data mining and analysis rather than technical
statistics (just basic descriptive statistics). There are some things I
want to get out of the data but I think I am stuck because a lack of
understanding the language (and vectors too - i usually work in data
frames). I have time series data of funds, their cashflows, the type of
cashflow (3 levels), the specific date of the cashflow in addition to
the fund year, fund size and the classification of fund. I want to be
able to group the funds together by fund year and then group that subset
by cashflow type or by fund size (summing the cashflow by type by fund
year and fund id). It would be nice to see the distribution of fund
sizes around specific vintage years as well as the ratio of calls to
fund size for a given time period.

Also, I would like to subtract the cashflow dates from the first
cashflow date by fund and then group the funds by their cashflow date
bucket and see how they compare to each other (aggregating over cashflow
type, etc)

If there are good books that go more into the code with some
illustrative examples, I would appreciate the title and/or reference
info. I have found one online datamining source but it would be nice to
get some more resources. If you know of any R training centers or
professionals in London I can be put in touch with I would appreciate
that too.

Thanks,
Samit

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From davidr at rhotrading.com  Tue Jul  3 15:30:47 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 3 Jul 2007 08:30:47 -0500
Subject: [R-SIG-Finance] Intraday data with RBloomberg
In-Reply-To: <1865010707030025o5f4adee6j12ead01b979a6913@mail.gmail.com>
References: <1865010707030020x1332441dpd8a90371138e905c@mail.gmail.com>
	<1865010707030025o5f4adee6j12ead01b979a6913@mail.gmail.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE77395240@rhopost.rhotrading.com>

Almost there! You just have to set the barfields; see the example at the
end of ?blpGetData.
HTH,

David L. Reiner
Rho Trading Securities, LLC
550 W. Jackson Blvd #1000
Chicago, IL 60661-5704
 
312-244-4610 direct
312-244-4500 main
312-244-4501 fax
 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Seow
Sent: Tuesday, July 03, 2007 2:25 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Intraday data with RBloomberg

Hi, I'm currently trying to implement an intraday currency model using
a live feed from bloomberg.
I hit the following error when I attempt to download intraday 2 min
ticks:

> usdjpy=blpGetData(conn,securities='USDJPY
Curncy',fields=c('BID','ASK'), start= as.chron(Sys.time()-6000),
end=as.chron(Sys.time()),barsize=2, barfields='OPEN')

> usdjpy
                    BID.OPEN ASK.OPEN
(07/03/07 15:01:45)       NA       NA

blpGetData works fine for historical price data and seems to work fine
for commodity futures ( e.g. the example above works fine for ED1
Comdty), so I'm puzzled why this function does not return a result for
Curncy. I double-checked the "USDJPY Curncy" intraday bloomberg API
feed in excel and it works.

Also, is it a good idea in general to avoid implementing such models
in R?  Would C/C++ be a better alternative?
Any ideas / insights would be greatly appreciated! Thanks.


Best Regards
Ian Seow

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From ianseow at gmail.com  Wed Jul  4 01:53:01 2007
From: ianseow at gmail.com (Ian Seow)
Date: Wed, 4 Jul 2007 07:53:01 +0800
Subject: [R-SIG-Finance] Intraday data with RBloomberg
In-Reply-To: <F9F2A641C593D7408925574C05A7BE77395240@rhopost.rhotrading.com>
References: <1865010707030020x1332441dpd8a90371138e905c@mail.gmail.com>
	<1865010707030025o5f4adee6j12ead01b979a6913@mail.gmail.com>
	<F9F2A641C593D7408925574C05A7BE77395240@rhopost.rhotrading.com>
Message-ID: <1865010707031653j23edb395xb249178b9009b633@mail.gmail.com>

Hi David, that doesn't seem to work either.

In the example below, I cut and paste the example from ?blpGetData,
replacing only the security field with 'USDJPY Curncy'. Notice that
the example works great when we use 'ED1 Comdty'. Also, when I try the
historical data example, it works great for USDJPY Curncy daily
prices. I'm totally stumped.


> edc=blpGetData(conn,'USDJPY Curncy', c('BID','ASK'), start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)

> edc
                    BID.OPEN ASK.OPEN
(07/04/07 07:46:10)       NA       NA

> edc=blpGetData(conn,'ED1 Comdty', c('BID','ASK'), start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
> edc
                    BID.OPEN ASK.OPEN
(07/03/07 22:38:00)   94.660   94.665
(07/03/07 22:40:00)   94.660   94.665
(07/03/07 22:42:00)   94.660   94.665
(07/03/07 22:44:00)   94.660   94.665
(07/03/07 22:46:00)   94.660   94.665
(07/03/07 22:48:00)   94.660   94.665
(07/03/07 22:50:00)   94.660   94.665
(07/03/07 22:52:00)   94.660   94.665
(07/03/07 22:54:00)   94.660   94.665
(07/03/07 22:56:00)   94.660   94.665
(07/03/07 22:58:00)   94.660   94.665
etc....



On 7/3/07, davidr at rhotrading.com <davidr at rhotrading.com> wrote:
> Almost there! You just have to set the barfields; see the example at the
> end of ?blpGetData.
> HTH,
>
> David L. Reiner
> Rho Trading Securities, LLC
> 550 W. Jackson Blvd #1000
> Chicago, IL 60661-5704
>
> 312-244-4610 direct
> 312-244-4500 main
> 312-244-4501 fax
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Seow
> Sent: Tuesday, July 03, 2007 2:25 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Intraday data with RBloomberg
>
> Hi, I'm currently trying to implement an intraday currency model using
> a live feed from bloomberg.
> I hit the following error when I attempt to download intraday 2 min
> ticks:
>
> > usdjpy=blpGetData(conn,securities='USDJPY
> Curncy',fields=c('BID','ASK'), start= as.chron(Sys.time()-6000),
> end=as.chron(Sys.time()),barsize=2, barfields='OPEN')
>
> > usdjpy
>                     BID.OPEN ASK.OPEN
> (07/03/07 15:01:45)       NA       NA
>
> blpGetData works fine for historical price data and seems to work fine
> for commodity futures ( e.g. the example above works fine for ED1
> Comdty), so I'm puzzled why this function does not return a result for
> Curncy. I double-checked the "USDJPY Curncy" intraday bloomberg API
> feed in excel and it works.
>
> Also, is it a good idea in general to avoid implementing such models
> in R?  Would C/C++ be a better alternative?
> Any ideas / insights would be greatly appreciated! Thanks.
>
>
> Best Regards
> Ian Seow
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From davidr at rhotrading.com  Thu Jul  5 16:56:25 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Thu, 5 Jul 2007 09:56:25 -0500
Subject: [R-SIG-Finance] [SPAM] - Re: Intraday data with RBloomberg -
	Email found in subject
In-Reply-To: <1865010707031653j23edb395xb249178b9009b633@mail.gmail.com>
References: <1865010707030020x1332441dpd8a90371138e905c@mail.gmail.com>
	<1865010707030025o5f4adee6j12ead01b979a6913@mail.gmail.com>
	<F9F2A641C593D7408925574C05A7BE77395240@rhopost.rhotrading.com>
	<1865010707031653j23edb395xb249178b9009b633@mail.gmail.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE773F2A14@rhopost.rhotrading.com>

I can verify similar behavior.
I can get the data via VBA behind Excel.
Between versions of Bloomberg and R and the difficulty I seem to have
getting RDCOMClient installed correctly, I have sort of given up on this
approach, unfortunately so, since it seems so useful. I pretty much use
VBA/Excel or C# to generate text files to read into R.

David L. Reiner
Rho Trading Securities, LLC
550 W. Jackson Blvd #1000
Chicago, IL 60661-5704
 
312-244-4610 direct
312-244-4500 main
312-244-4501 fax
 

-----Original Message-----
From: Ian Seow [mailto:ianseow at gmail.com] 
Sent: Tuesday, July 03, 2007 6:53 PM
To: David Reiner <davidr at rhotrading.com>
Cc: r-sig-finance at stat.math.ethz.ch
Subject: [SPAM] - Re: [R-SIG-Finance] Intraday data with RBloomberg -
Email found in subject

Hi David, that doesn't seem to work either.

In the example below, I cut and paste the example from ?blpGetData,
replacing only the security field with 'USDJPY Curncy'. Notice that
the example works great when we use 'ED1 Comdty'. Also, when I try the
historical data example, it works great for USDJPY Curncy daily
prices. I'm totally stumped.


> edc=blpGetData(conn,'USDJPY Curncy', c('BID','ASK'),
start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)

> edc
                    BID.OPEN ASK.OPEN
(07/04/07 07:46:10)       NA       NA

> edc=blpGetData(conn,'ED1 Comdty', c('BID','ASK'),
start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
> edc
                    BID.OPEN ASK.OPEN
(07/03/07 22:38:00)   94.660   94.665
(07/03/07 22:40:00)   94.660   94.665
(07/03/07 22:42:00)   94.660   94.665
(07/03/07 22:44:00)   94.660   94.665
(07/03/07 22:46:00)   94.660   94.665
(07/03/07 22:48:00)   94.660   94.665
(07/03/07 22:50:00)   94.660   94.665
(07/03/07 22:52:00)   94.660   94.665
(07/03/07 22:54:00)   94.660   94.665
(07/03/07 22:56:00)   94.660   94.665
(07/03/07 22:58:00)   94.660   94.665
etc....



On 7/3/07, davidr at rhotrading.com <davidr at rhotrading.com> wrote:
> Almost there! You just have to set the barfields; see the example at
the
> end of ?blpGetData.
> HTH,
>
> David L. Reiner
> Rho Trading Securities, LLC
> 550 W. Jackson Blvd #1000
> Chicago, IL 60661-5704
>
> 312-244-4610 direct
> 312-244-4500 main
> 312-244-4501 fax
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Seow
> Sent: Tuesday, July 03, 2007 2:25 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Intraday data with RBloomberg
>
> Hi, I'm currently trying to implement an intraday currency model using
> a live feed from bloomberg.
> I hit the following error when I attempt to download intraday 2 min
> ticks:
>
> > usdjpy=blpGetData(conn,securities='USDJPY
> Curncy',fields=c('BID','ASK'), start= as.chron(Sys.time()-6000),
> end=as.chron(Sys.time()),barsize=2, barfields='OPEN')
>
> > usdjpy
>                     BID.OPEN ASK.OPEN
> (07/03/07 15:01:45)       NA       NA
>
> blpGetData works fine for historical price data and seems to work fine
> for commodity futures ( e.g. the example above works fine for ED1
> Comdty), so I'm puzzled why this function does not return a result for
> Curncy. I double-checked the "USDJPY Curncy" intraday bloomberg API
> feed in excel and it works.
>
> Also, is it a good idea in general to avoid implementing such models
> in R?  Would C/C++ be a better alternative?
> Any ideas / insights would be greatly appreciated! Thanks.
>
>
> Best Regards
> Ian Seow
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From edd at debian.org  Thu Jul  5 17:54:02 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 5 Jul 2007 10:54:02 -0500
Subject: [R-SIG-Finance] [SPAM] - Re: Intraday data with RBloomberg
	-	Email found in subject
In-Reply-To: <F9F2A641C593D7408925574C05A7BE773F2A14@rhopost.rhotrading.com>
References: <1865010707030020x1332441dpd8a90371138e905c@mail.gmail.com>
	<1865010707030025o5f4adee6j12ead01b979a6913@mail.gmail.com>
	<F9F2A641C593D7408925574C05A7BE77395240@rhopost.rhotrading.com>
	<1865010707031653j23edb395xb249178b9009b633@mail.gmail.com>
	<F9F2A641C593D7408925574C05A7BE773F2A14@rhopost.rhotrading.com>
Message-ID: <18061.5146.455992.808579@basebud.nulle.part>


On 5 July 2007 at 09:56, davidr at rhotrading.com wrote:
| I can verify similar behavior.
| I can get the data via VBA behind Excel.
| Between versions of Bloomberg and R and the difficulty I seem to have
| getting RDCOMClient installed correctly, I have sort of given up on this
| approach, unfortunately so, since it seems so useful. I pretty much use
| VBA/Excel or C# to generate text files to read into R.

Well, the C/C++ code I wrote two employers ago, and which is hence still
owned by that firm and hence no shareable, worked pretty well.

It shouldn't take too long to rewrite this, starting from the Bloomberg C API
examples. As I don't currently use or need Bloomberg, I can't help.  Maybe
somebody else can though.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From icos.atropa at gmail.com  Fri Jul  6 05:52:43 2007
From: icos.atropa at gmail.com (icosa atropa)
Date: Thu, 5 Jul 2007 21:52:43 -0600
Subject: [R-SIG-Finance] Timeseries data, lattice, and model formulas?
Message-ID: <681d07c20707052052w36d16f3fv94f54cf3703487a7@mail.gmail.com>

Hello All,

I'm using R for a rather large, multivariate timeseries dataset of ~4
million records.  So far I've divided the set and used conventional
data frames with good success at visualizing, summary statistics, etc.

I've experimented with dedicated timeseries packages like its and zoo
for more complicated analysis, but support for things like factors,
lattice and model formulas seems lacking.  Am I missing something, or
are data frames the most appropriate vehicle for large, multivariate
timeseries?

thanks so much
christian
university of new mexico
-- 
Far better an approximate answer to the right question, which is often
vague, than the exact answer to the wrong question, which can always
be made precise -- j.w. tukey


From ggrothendieck at gmail.com  Fri Jul  6 06:09:57 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 6 Jul 2007 00:09:57 -0400
Subject: [R-SIG-Finance] Timeseries data, lattice, and model formulas?
In-Reply-To: <681d07c20707052052w36d16f3fv94f54cf3703487a7@mail.gmail.com>
References: <681d07c20707052052w36d16f3fv94f54cf3703487a7@mail.gmail.com>
Message-ID: <971536df0707052109n69d5f48bid29cb6a16883877a@mail.gmail.com>

On 7/5/07, icosa atropa <icos.atropa at gmail.com> wrote:
> Hello All,
>
> I'm using R for a rather large, multivariate timeseries dataset of ~4
> million records.  So far I've divided the set and used conventional
> data frames with good success at visualizing, summary statistics, etc.
>
> I've experimented with dedicated timeseries packages like its and zoo
> for more complicated analysis, but support for things like factors,

Don't know what you are looking for with respect to factors but if you create
a zoo object from a factor it remembers where it came from:

   > zf <- zoo(factor(c(1,1,2)))
   > class(zf)
   [1] "zoo"
   > str(zf)
   atomic [1:3] 1 1 2
    - attr(*, "levels")= chr [1:2] "1" "2"
    - attr(*, "oclass")= chr "factor"
    - attr(*, "index")= int [1:3] 1 2 3

> lattice and

zoo explicitly supports lattice with xyplot.zoo, e.g.

library(zoo)
library(lattice)
example(xyplot.zoo)

> model formulas seems lacking.  Am I missing something, or

In conjuction with dyn or dynlm zoo supports model formula:

library(dyn)
set.seed(1)
z <- zoo(rnorm(10))
dyn$lm(z ~ lag(z, -1))


From icos.atropa at gmail.com  Fri Jul  6 07:41:45 2007
From: icos.atropa at gmail.com (icosa atropa)
Date: Thu, 5 Jul 2007 23:41:45 -0600
Subject: [R-SIG-Finance] Timeseries data, lattice, and model formulas?
In-Reply-To: <971536df0707052109n69d5f48bid29cb6a16883877a@mail.gmail.com>
References: <681d07c20707052052w36d16f3fv94f54cf3703487a7@mail.gmail.com>
	<971536df0707052109n69d5f48bid29cb6a16883877a@mail.gmail.com>
Message-ID: <681d07c20707052241w224044d5vf3a93bfd470c8222@mail.gmail.com>

> Don't know what you are looking for with respect to factors but if you create
> a zoo object from a factor it remembers where it came from:

Thanks for the reply.  I need to read up on dynlm.
R.e. factors, I have something that looks like this - the first 3
columns have identifying info, and are the factors that I give to
lattice, whereas the last column is the actual timeseries.

> summary(M.full)
    unit_id  well_num              sampled_on                      dtw_m
 M1     :5   N:5        Min.   :2005-08-04 15:30:00    Min.   :-1.571
 M2     :0   S:0       1st Qu.:2005-08-04 15:45:00    1st Qu.:-1.570

Would the most logical way to use zoo be to create an object for each
element in the factor matrix, i.e. M1N, M1S, M2N, M2S, ... , and
create a list or environment of the objects?

Thanks!
christian

>
>    > zf <- zoo(factor(c(1,1,2)))
>    > class(zf)
>    [1] "zoo"
>    > str(zf)
>    atomic [1:3] 1 1 2
>     - attr(*, "levels")= chr [1:2] "1" "2"
>     - attr(*, "oclass")= chr "factor"
>     - attr(*, "index")= int [1:3] 1 2 3
>
> > lattice and
>
> zoo explicitly supports lattice with xyplot.zoo, e.g.
>
> library(zoo)
> library(lattice)
> example(xyplot.zoo)
>
> > model formulas seems lacking.  Am I missing something, or
>
> In conjuction with dyn or dynlm zoo supports model formula:
>
> library(dyn)
> set.seed(1)
> z <- zoo(rnorm(10))
> dyn$lm(z ~ lag(z, -1))
>


-- 
Far better an approximate answer to the right question, which is often
vague, than the exact answer to the wrong question, which can always
be made precise -- j.w. tukey


From a.trapletti at swissonline.ch  Fri Jul  6 12:47:17 2007
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Fri, 06 Jul 2007 12:47:17 +0200
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 38, Issue 4
In-Reply-To: <mailman.17.1183716015.20772.r-sig-finance@stat.math.ethz.ch>
References: <mailman.17.1183716015.20772.r-sig-finance@stat.math.ethz.ch>
Message-ID: <468E1DB5.7050006@swissonline.ch>

Just my 2 cents: It might be more useful to split the data part from the 
analysis or trading or whatever part. In the first step, data is 
received and stored in a database. Typically a C/C++ or Java application 
receives data and saves it in a database. In a second step, 
applications, e.g., R, may access the data through the database, either 
access historical data or data in realtime. If well designed, the delay 
of receiving data in realtime through the database instead of a direct 
connection is negligible. And you gain a lot of flexibility with this 
design.

Best regards
Adrian

>------------------------------
>
>Message: 2
>Date: Thu, 5 Jul 2007 10:54:02 -0500
>From: Dirk Eddelbuettel <edd at debian.org>
>Subject: Re: [R-SIG-Finance] [SPAM] - Re: Intraday data with
>	RBloomberg	-	Email found in subject
>To: <davidr at rhotrading.com>
>Cc: r-sig-finance at stat.math.ethz.ch, Ian Seow <ianseow at gmail.com>
>Message-ID: <18061.5146.455992.808579 at basebud.nulle.part>
>Content-Type: text/plain; charset=us-ascii
>
>
>On 5 July 2007 at 09:56, davidr at rhotrading.com wrote:
>| I can verify similar behavior.
>| I can get the data via VBA behind Excel.
>| Between versions of Bloomberg and R and the difficulty I seem to have
>| getting RDCOMClient installed correctly, I have sort of given up on this
>| approach, unfortunately so, since it seems so useful. I pretty much use
>| VBA/Excel or C# to generate text files to read into R.
>
>Well, the C/C++ code I wrote two employers ago, and which is hence still
>owned by that firm and hence no shareable, worked pretty well.
>
>It shouldn't take too long to rewrite this, starting from the Bloomberg C API
>examples. As I don't currently use or need Bloomberg, I can't help.  Maybe
>somebody else can though.
>
>Dirk
>
>  
>

-- 
Adrian Trapletti
Wildsbergstrasse 31
8610 Uster
Switzerland

Phone :   +41 (0) 44 9945630
Mobile :  +41 (0) 76 3705631

Email :   a.trapletti at swissonline.ch


From a.trapletti at swissonline.ch  Fri Jul  6 12:50:34 2007
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Fri, 06 Jul 2007 12:50:34 +0200
Subject: [R-SIG-Finance] Intraday data with RBloomberg
Message-ID: <468E1E7A.8010906@swissonline.ch>

Just my 2 cents: It might be more useful to split the data part from the 
analysis or trading or whatever part. In the first step, data is 
received and stored in a database. Typically a C/C++ or Java application 
receives data and saves it in a database. In a second step, 
applications, e.g., R, may access the data through the database, either 
access historical data or data in realtime. If well designed, the delay 
of receiving data in realtime through the database instead of a direct 
connection is negligible. And you gain a lot of flexibility with this 
design.

Best regards
Adrian

> ------------------------------
>
> Message: 2
> Date: Thu, 5 Jul 2007 10:54:02 -0500
> From: Dirk Eddelbuettel <edd at debian.org>
> Subject: Re: [R-SIG-Finance] [SPAM] - Re: Intraday data with
>     RBloomberg    -    Email found in subject
> To: <davidr at rhotrading.com>
> Cc: r-sig-finance at stat.math.ethz.ch, Ian Seow <ianseow at gmail.com>
> Message-ID: <18061.5146.455992.808579 at basebud.nulle.part>
> Content-Type: text/plain; charset=us-ascii
>
>
> On 5 July 2007 at 09:56, davidr at rhotrading.com wrote:
> | I can verify similar behavior.
> | I can get the data via VBA behind Excel.
> | Between versions of Bloomberg and R and the difficulty I seem to have
> | getting RDCOMClient installed correctly, I have sort of given up on 
> this
> | approach, unfortunately so, since it seems so useful. I pretty much use
> | VBA/Excel or C# to generate text files to read into R.
>
> Well, the C/C++ code I wrote two employers ago, and which is hence still
> owned by that firm and hence no shareable, worked pretty well.
>
> It shouldn't take too long to rewrite this, starting from the 
> Bloomberg C API
> examples. As I don't currently use or need Bloomberg, I can't help.  
> Maybe
> somebody else can though.
>
> Dirk 


-- 
Adrian Trapletti
Wildsbergstrasse 31
8610 Uster
Switzerland

Phone :   +41 (0) 44 9945630
Mobile :  +41 (0) 76 3705631

Email :   a.trapletti at swissonline.ch


From drodriguez007 at gmail.com  Fri Jul  6 14:38:04 2007
From: drodriguez007 at gmail.com (David Rodriguez)
Date: Fri, 6 Jul 2007 08:38:04 -0400
Subject: [R-SIG-Finance] Intraday data with RBloomberg
In-Reply-To: <468E1E7A.8010906@swissonline.ch>
References: <468E1E7A.8010906@swissonline.ch>
Message-ID: <8dab790b0707060538i2e50f1bg1d0c2215daf00f71@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070706/125d1c9e/attachment.pl 

From robert at sanctumfi.com  Fri Jul  6 15:30:24 2007
From: robert at sanctumfi.com (Robert Sams)
Date: Fri, 6 Jul 2007 14:30:24 +0100
Subject: [R-SIG-Finance] Intraday data with RBloomberg
References: <1865010707030020x1332441dpd8a90371138e905c@mail.gmail.com>
	<1865010707030025o5f4adee6j12ead01b979a6913@mail.gmail.com>
	<F9F2A641C593D7408925574C05A7BE77395240@rhopost.rhotrading.com>
	<1865010707031653j23edb395xb249178b9009b633@mail.gmail.com>
	<SANCTUMFISERVER049u00003140@sanctumfi.com>
Message-ID: <SANCTUMFISERVERPzg90000337f@sanctumfi.com>

David,

"sort of given up"? I'm disappointed in you ;)

Let's look at Ian's original example. 

 blpGetData(conn,'USDJPY Curncy', c('BID'),
start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
                    BID.OPEN
(07/06/07 14:02:50)       NA

Indeed, this doesn't seem right. A little more transparency:

blpGetData(conn,'USDJPY Curncy', 'BID', start=as.chron(Sys.time()-3600),
barfields='OPEN', barsize=2, retval="raw")
[[1]]
[[1]][[1]]
[[1]][[1]][[1]]
[[1]][[1]][[1]][[1]]
An object of class "COMDate"
[1] 39269.59




[[2]]
[[2]][[1]]
[[2]][[1]][[1]]
[[2]][[1]][[1]][[1]]
[1] "#N/A History"




attr(,"num.of.date.cols")
[1] 1
attr(,"class")
[1] "BlpCOMReturn"
attr(,"securities")
[1] "USDJPY CURNCY"
attr(,"fields")
[1] "BID"
attr(,"barfields")
[1] "OPEN"
> 

The existence of a "no available history" Bloomberg error in the return
value of the underlying COM method makes me think that this isn't really
an RBloomberg or RDCOMClient package error. 

It's always useful to experiment with different fields.. in a few
minutes I discovered this:

blpGetData(conn,'USDJPY Curncy', 'BEST_BID',
start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
                    BEST_BID.OPEN
(07/06/07 12:07:00)       123.230
(07/06/07 12:09:00)       123.230
(07/06/07 12:11:00)       123.240
(07/06/07 12:13:00)       123.230
(07/06/07 12:15:00)       123.215
(07/06/07 12:17:00)       123.230
(07/06/07 12:19:00)       123.260
(07/06/07 12:21:00)       123.250
(07/06/07 12:23:00)       123.260
(07/06/07 12:25:00)       123.270
(07/06/07 12:27:00)       123.280
(07/06/07 12:29:00)       123.320
etc..

Does this call work for you Ian?

Now, I don't know why BID does not work but BEST_BID does, but it seems
to me that the answer lies on the Bloomberg API side of things rather
than with RBloomberg. But maybe I'm wrong. Can somebody send me a VBA
snippet that shows a successful call to
GetHistoricalData(Security='USDJPY Curncy', Fields='BID', ... in that
language's binding of the COM interface? If so, it's worth looking into.


I haven't touched the RBloomberg code for well over a year but use it
every day in a trading environment without any problems. I will however
set aside some time over the next week to clean up the code and
incorporate any bug fixes or features that RBloombergers care to submit,
so please keep an eye on cran for an updated version(s). 

Cheers,

Robert





-----Original Message-----
From: davidr at rhotrading.com [mailto:davidr at rhotrading.com] 
Sent: 05 July 2007 15:56
To: Ian Seow
Cc: r-sig-finance at stat.math.ethz.ch; Robert Sams
Subject: RE: [SPAM] - Re: [R-SIG-Finance] Intraday data with RBloomberg
- Email found in subject

I can verify similar behavior.
I can get the data via VBA behind Excel.
Between versions of Bloomberg and R and the difficulty I seem to have
getting RDCOMClient installed correctly, I have sort of given up on this
approach, unfortunately so, since it seems so useful. I pretty much use
VBA/Excel or C# to generate text files to read into R.

David L. Reiner
Rho Trading Securities, LLC
550 W. Jackson Blvd #1000
Chicago, IL 60661-5704
 
312-244-4610 direct
312-244-4500 main
312-244-4501 fax
 

-----Original Message-----
From: Ian Seow [mailto:ianseow at gmail.com]
Sent: Tuesday, July 03, 2007 6:53 PM
To: David Reiner <davidr at rhotrading.com>
Cc: r-sig-finance at stat.math.ethz.ch
Subject: [SPAM] - Re: [R-SIG-Finance] Intraday data with RBloomberg -
Email found in subject

Hi David, that doesn't seem to work either.

In the example below, I cut and paste the example from ?blpGetData,
replacing only the security field with 'USDJPY Curncy'. Notice that the
example works great when we use 'ED1 Comdty'. Also, when I try the
historical data example, it works great for USDJPY Curncy daily prices.
I'm totally stumped.


> edc=blpGetData(conn,'USDJPY Curncy', c('BID','ASK'),
start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)

> edc
                    BID.OPEN ASK.OPEN
(07/04/07 07:46:10)       NA       NA

> edc=blpGetData(conn,'ED1 Comdty', c('BID','ASK'),
start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
> edc
                    BID.OPEN ASK.OPEN
(07/03/07 22:38:00)   94.660   94.665
(07/03/07 22:40:00)   94.660   94.665
(07/03/07 22:42:00)   94.660   94.665
(07/03/07 22:44:00)   94.660   94.665
(07/03/07 22:46:00)   94.660   94.665
(07/03/07 22:48:00)   94.660   94.665
(07/03/07 22:50:00)   94.660   94.665
(07/03/07 22:52:00)   94.660   94.665
(07/03/07 22:54:00)   94.660   94.665
(07/03/07 22:56:00)   94.660   94.665
(07/03/07 22:58:00)   94.660   94.665
etc....



On 7/3/07, davidr at rhotrading.com <davidr at rhotrading.com> wrote:
> Almost there! You just have to set the barfields; see the example at
the
> end of ?blpGetData.
> HTH,
>
> David L. Reiner
> Rho Trading Securities, LLC
> 550 W. Jackson Blvd #1000
> Chicago, IL 60661-5704
>
> 312-244-4610 direct
> 312-244-4500 main
> 312-244-4501 fax
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Seow
> Sent: Tuesday, July 03, 2007 2:25 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Intraday data with RBloomberg
>
> Hi, I'm currently trying to implement an intraday currency model using

> a live feed from bloomberg.
> I hit the following error when I attempt to download intraday 2 min
> ticks:
>
> > usdjpy=blpGetData(conn,securities='USDJPY
> Curncy',fields=c('BID','ASK'), start= as.chron(Sys.time()-6000), 
> end=as.chron(Sys.time()),barsize=2, barfields='OPEN')
>
> > usdjpy
>                     BID.OPEN ASK.OPEN
> (07/03/07 15:01:45)       NA       NA
>
> blpGetData works fine for historical price data and seems to work fine

> for commodity futures ( e.g. the example above works fine for ED1 
> Comdty), so I'm puzzled why this function does not return a result for

> Curncy. I double-checked the "USDJPY Curncy" intraday bloomberg API 
> feed in excel and it works.
>
> Also, is it a good idea in general to avoid implementing such models 
> in R?  Would C/C++ be a better alternative?
> Any ideas / insights would be greatly appreciated! Thanks.
>
>
> Best Regards
> Ian Seow
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Fri Jul  6 15:35:41 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 6 Jul 2007 09:35:41 -0400
Subject: [R-SIG-Finance] Timeseries data, lattice, and model formulas?
In-Reply-To: <681d07c20707052241w224044d5vf3a93bfd470c8222@mail.gmail.com>
References: <681d07c20707052052w36d16f3fv94f54cf3703487a7@mail.gmail.com>
	<971536df0707052109n69d5f48bid29cb6a16883877a@mail.gmail.com>
	<681d07c20707052241w224044d5vf3a93bfd470c8222@mail.gmail.com>
Message-ID: <971536df0707060635m3541e714iab2ed81892ab177a@mail.gmail.com>

On 7/6/07, icosa atropa <icos.atropa at gmail.com> wrote:
> > Don't know what you are looking for with respect to factors but if you create
> > a zoo object from a factor it remembers where it came from:
>
> Thanks for the reply.  I need to read up on dynlm.

To use dyn just preface lm, glm, etc. with dyn$


> R.e. factors, I have something that looks like this - the first 3
> columns have identifying info, and are the factors that I give to
> lattice, whereas the last column is the actual timeseries.
>
> > summary(M.full)
>    unit_id  well_num              sampled_on                      dtw_m
>  M1     :5   N:5        Min.   :2005-08-04 15:30:00    Min.   :-1.571
>  M2     :0   S:0       1st Qu.:2005-08-04 15:45:00    1st Qu.:-1.570
>
> Would the most logical way to use zoo be to create an object for each
> element in the factor matrix, i.e. M1N, M1S, M2N, M2S, ... , and
> create a list or environment of the objects?
>
> Thanks!
> christian

Please provide something reproducible and show what you want to do without
zoo.


From davidr at rhotrading.com  Fri Jul  6 15:49:46 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Fri, 6 Jul 2007 08:49:46 -0500
Subject: [R-SIG-Finance] Intraday data with RBloomberg
In-Reply-To: <SANCTUMFISERVERPzg90000337f@sanctumfi.com>
References: <1865010707030020x1332441dpd8a90371138e905c@mail.gmail.com>
	<1865010707030025o5f4adee6j12ead01b979a6913@mail.gmail.com>
	<F9F2A641C593D7408925574C05A7BE77395240@rhopost.rhotrading.com>
	<1865010707031653j23edb395xb249178b9009b633@mail.gmail.com>
	<SANCTUMFISERVER049u00003140@sanctumfi.com>
	<SANCTUMFISERVERPzg90000337f@sanctumfi.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE773F2AB9@rhopost.rhotrading.com>

Well, I've not given up completely ;-)

I'd forgotten that I DO use Best_Bid for intraday bars, and they say
that for bars, you should use best_bid and best_ask, and last_price.

But for intraday _raw_, it depends on the exchange; Bloomberg help
people suggest requesting both in general and using the one that returns
data (of course if you know which to use, it's simpler to just ask for
the one type.)

(For some reason I can't get RDCOMClient to install correctly in 2.5.0,
hence my frustration, but that's for another thread when I have more
time.)

David L. Reiner
Rho Trading Securities, LLC
550 W. Jackson Blvd #1000
Chicago, IL 60661-5704
 
312-244-4610 direct
312-244-4500 main
312-244-4501 fax
 

-----Original Message-----
From: Robert Sams [mailto:robert at sanctumfi.com] 
Sent: Friday, July 06, 2007 8:30 AM
To: David Reiner <davidr at rhotrading.com>; Ian Seow
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Intraday data with RBloomberg 

David,

"sort of given up"? I'm disappointed in you ;)

Let's look at Ian's original example. 

 blpGetData(conn,'USDJPY Curncy', c('BID'),
start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
                    BID.OPEN
(07/06/07 14:02:50)       NA

Indeed, this doesn't seem right. A little more transparency:

blpGetData(conn,'USDJPY Curncy', 'BID', start=as.chron(Sys.time()-3600),
barfields='OPEN', barsize=2, retval="raw")
[[1]]
[[1]][[1]]
[[1]][[1]][[1]]
[[1]][[1]][[1]][[1]]
An object of class "COMDate"
[1] 39269.59




[[2]]
[[2]][[1]]
[[2]][[1]][[1]]
[[2]][[1]][[1]][[1]]
[1] "#N/A History"




attr(,"num.of.date.cols")
[1] 1
attr(,"class")
[1] "BlpCOMReturn"
attr(,"securities")
[1] "USDJPY CURNCY"
attr(,"fields")
[1] "BID"
attr(,"barfields")
[1] "OPEN"
> 

The existence of a "no available history" Bloomberg error in the return
value of the underlying COM method makes me think that this isn't really
an RBloomberg or RDCOMClient package error. 

It's always useful to experiment with different fields.. in a few
minutes I discovered this:

blpGetData(conn,'USDJPY Curncy', 'BEST_BID',
start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
                    BEST_BID.OPEN
(07/06/07 12:07:00)       123.230
(07/06/07 12:09:00)       123.230
(07/06/07 12:11:00)       123.240
(07/06/07 12:13:00)       123.230
(07/06/07 12:15:00)       123.215
(07/06/07 12:17:00)       123.230
(07/06/07 12:19:00)       123.260
(07/06/07 12:21:00)       123.250
(07/06/07 12:23:00)       123.260
(07/06/07 12:25:00)       123.270
(07/06/07 12:27:00)       123.280
(07/06/07 12:29:00)       123.320
etc..

Does this call work for you Ian?

Now, I don't know why BID does not work but BEST_BID does, but it seems
to me that the answer lies on the Bloomberg API side of things rather
than with RBloomberg. But maybe I'm wrong. Can somebody send me a VBA
snippet that shows a successful call to
GetHistoricalData(Security='USDJPY Curncy', Fields='BID', ... in that
language's binding of the COM interface? If so, it's worth looking into.


I haven't touched the RBloomberg code for well over a year but use it
every day in a trading environment without any problems. I will however
set aside some time over the next week to clean up the code and
incorporate any bug fixes or features that RBloombergers care to submit,
so please keep an eye on cran for an updated version(s). 

Cheers,

Robert





-----Original Message-----
From: davidr at rhotrading.com [mailto:davidr at rhotrading.com] 
Sent: 05 July 2007 15:56
To: Ian Seow
Cc: r-sig-finance at stat.math.ethz.ch; Robert Sams
Subject: RE: [SPAM] - Re: [R-SIG-Finance] Intraday data with RBloomberg
- Email found in subject

I can verify similar behavior.
I can get the data via VBA behind Excel.
Between versions of Bloomberg and R and the difficulty I seem to have
getting RDCOMClient installed correctly, I have sort of given up on this
approach, unfortunately so, since it seems so useful. I pretty much use
VBA/Excel or C# to generate text files to read into R.

David L. Reiner
Rho Trading Securities, LLC
550 W. Jackson Blvd #1000
Chicago, IL 60661-5704
 
312-244-4610 direct
312-244-4500 main
312-244-4501 fax
 

-----Original Message-----
From: Ian Seow [mailto:ianseow at gmail.com]
Sent: Tuesday, July 03, 2007 6:53 PM
To: David Reiner <davidr at rhotrading.com>
Cc: r-sig-finance at stat.math.ethz.ch
Subject: [SPAM] - Re: [R-SIG-Finance] Intraday data with RBloomberg -
Email found in subject

Hi David, that doesn't seem to work either.

In the example below, I cut and paste the example from ?blpGetData,
replacing only the security field with 'USDJPY Curncy'. Notice that the
example works great when we use 'ED1 Comdty'. Also, when I try the
historical data example, it works great for USDJPY Curncy daily prices.
I'm totally stumped.


> edc=blpGetData(conn,'USDJPY Curncy', c('BID','ASK'),
start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)

> edc
                    BID.OPEN ASK.OPEN
(07/04/07 07:46:10)       NA       NA

> edc=blpGetData(conn,'ED1 Comdty', c('BID','ASK'),
start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
> edc
                    BID.OPEN ASK.OPEN
(07/03/07 22:38:00)   94.660   94.665
(07/03/07 22:40:00)   94.660   94.665
(07/03/07 22:42:00)   94.660   94.665
(07/03/07 22:44:00)   94.660   94.665
(07/03/07 22:46:00)   94.660   94.665
(07/03/07 22:48:00)   94.660   94.665
(07/03/07 22:50:00)   94.660   94.665
(07/03/07 22:52:00)   94.660   94.665
(07/03/07 22:54:00)   94.660   94.665
(07/03/07 22:56:00)   94.660   94.665
(07/03/07 22:58:00)   94.660   94.665
etc....



On 7/3/07, davidr at rhotrading.com <davidr at rhotrading.com> wrote:
> Almost there! You just have to set the barfields; see the example at
the
> end of ?blpGetData.
> HTH,
>
> David L. Reiner
> Rho Trading Securities, LLC
> 550 W. Jackson Blvd #1000
> Chicago, IL 60661-5704
>
> 312-244-4610 direct
> 312-244-4500 main
> 312-244-4501 fax
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Seow
> Sent: Tuesday, July 03, 2007 2:25 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Intraday data with RBloomberg
>
> Hi, I'm currently trying to implement an intraday currency model using

> a live feed from bloomberg.
> I hit the following error when I attempt to download intraday 2 min
> ticks:
>
> > usdjpy=blpGetData(conn,securities='USDJPY
> Curncy',fields=c('BID','ASK'), start= as.chron(Sys.time()-6000), 
> end=as.chron(Sys.time()),barsize=2, barfields='OPEN')
>
> > usdjpy
>                     BID.OPEN ASK.OPEN
> (07/03/07 15:01:45)       NA       NA
>
> blpGetData works fine for historical price data and seems to work fine

> for commodity futures ( e.g. the example above works fine for ED1 
> Comdty), so I'm puzzled why this function does not return a result for

> Curncy. I double-checked the "USDJPY Curncy" intraday bloomberg API 
> feed in excel and it works.
>
> Also, is it a good idea in general to avoid implementing such models 
> in R?  Would C/C++ be a better alternative?
> Any ideas / insights would be greatly appreciated! Thanks.
>
>
> Best Regards
> Ian Seow
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ianseow at gmail.com  Sat Jul  7 02:11:22 2007
From: ianseow at gmail.com (Ian Seow)
Date: Sat, 7 Jul 2007 08:11:22 +0800
Subject: [R-SIG-Finance] Intraday data with RBloomberg
In-Reply-To: <SANCTUMFISERVERPzg90000337f@sanctumfi.com>
References: <1865010707030020x1332441dpd8a90371138e905c@mail.gmail.com>
	<1865010707030025o5f4adee6j12ead01b979a6913@mail.gmail.com>
	<F9F2A641C593D7408925574C05A7BE77395240@rhopost.rhotrading.com>
	<1865010707031653j23edb395xb249178b9009b633@mail.gmail.com>
	<SANCTUMFISERVER049u00003140@sanctumfi.com>
	<SANCTUMFISERVERPzg90000337f@sanctumfi.com>
Message-ID: <1865010707061711va203160u95d7f843aebdb22e@mail.gmail.com>

Hi Robert, this definitely works for me! 'BEST_BID' worked like a
charm, thank you so much.

Another point of curiousity... I am currently trying to build a model
in R which uses bloomberg price feeds (minute data) to obtain buy/sell
orders and execute them via an electronic trading platform API. I've
done something similar successfully using daily data, but this is the
first time I'm doing this on an intraday basis. My main worry is in
the time lag I've been observing when getting the bloomberg feeds
(this lag is present, regardless of whether I'm using VBA or
RBloomberg)...

Could anyone with similar experiences give me some advice? Or perhaps
I should rethink the system architecture altogether, or perhaps use
another price feed more suited for this purpose?

Regards
Ian Seow

On 7/6/07, Robert Sams <robert at sanctumfi.com> wrote:
> David,
>
> "sort of given up"? I'm disappointed in you ;)
>
> Let's look at Ian's original example.
>
>  blpGetData(conn,'USDJPY Curncy', c('BID'),
> start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
>                     BID.OPEN
> (07/06/07 14:02:50)       NA
>
> Indeed, this doesn't seem right. A little more transparency:
>
> blpGetData(conn,'USDJPY Curncy', 'BID', start=as.chron(Sys.time()-3600),
> barfields='OPEN', barsize=2, retval="raw")
> [[1]]
> [[1]][[1]]
> [[1]][[1]][[1]]
> [[1]][[1]][[1]][[1]]
> An object of class "COMDate"
> [1] 39269.59
>
>
>
>
> [[2]]
> [[2]][[1]]
> [[2]][[1]][[1]]
> [[2]][[1]][[1]][[1]]
> [1] "#N/A History"
>
>
>
>
> attr(,"num.of.date.cols")
> [1] 1
> attr(,"class")
> [1] "BlpCOMReturn"
> attr(,"securities")
> [1] "USDJPY CURNCY"
> attr(,"fields")
> [1] "BID"
> attr(,"barfields")
> [1] "OPEN"
> >
>
> The existence of a "no available history" Bloomberg error in the return
> value of the underlying COM method makes me think that this isn't really
> an RBloomberg or RDCOMClient package error.
>
> It's always useful to experiment with different fields.. in a few
> minutes I discovered this:
>
> blpGetData(conn,'USDJPY Curncy', 'BEST_BID',
> start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
>                     BEST_BID.OPEN
> (07/06/07 12:07:00)       123.230
> (07/06/07 12:09:00)       123.230
> (07/06/07 12:11:00)       123.240
> (07/06/07 12:13:00)       123.230
> (07/06/07 12:15:00)       123.215
> (07/06/07 12:17:00)       123.230
> (07/06/07 12:19:00)       123.260
> (07/06/07 12:21:00)       123.250
> (07/06/07 12:23:00)       123.260
> (07/06/07 12:25:00)       123.270
> (07/06/07 12:27:00)       123.280
> (07/06/07 12:29:00)       123.320
> etc..
>
> Does this call work for you Ian?
>
> Now, I don't know why BID does not work but BEST_BID does, but it seems
> to me that the answer lies on the Bloomberg API side of things rather
> than with RBloomberg. But maybe I'm wrong. Can somebody send me a VBA
> snippet that shows a successful call to
> GetHistoricalData(Security='USDJPY Curncy', Fields='BID', ... in that
> language's binding of the COM interface? If so, it's worth looking into.
>
>
> I haven't touched the RBloomberg code for well over a year but use it
> every day in a trading environment without any problems. I will however
> set aside some time over the next week to clean up the code and
> incorporate any bug fixes or features that RBloombergers care to submit,
> so please keep an eye on cran for an updated version(s).
>
> Cheers,
>
> Robert
>
>
>
>
>
> -----Original Message-----
> From: davidr at rhotrading.com [mailto:davidr at rhotrading.com]
> Sent: 05 July 2007 15:56
> To: Ian Seow
> Cc: r-sig-finance at stat.math.ethz.ch; Robert Sams
> Subject: RE: [SPAM] - Re: [R-SIG-Finance] Intraday data with RBloomberg
> - Email found in subject
>
> I can verify similar behavior.
> I can get the data via VBA behind Excel.
> Between versions of Bloomberg and R and the difficulty I seem to have
> getting RDCOMClient installed correctly, I have sort of given up on this
> approach, unfortunately so, since it seems so useful. I pretty much use
> VBA/Excel or C# to generate text files to read into R.
>
> David L. Reiner
> Rho Trading Securities, LLC
> 550 W. Jackson Blvd #1000
> Chicago, IL 60661-5704
>
> 312-244-4610 direct
> 312-244-4500 main
> 312-244-4501 fax
>
>
> -----Original Message-----
> From: Ian Seow [mailto:ianseow at gmail.com]
> Sent: Tuesday, July 03, 2007 6:53 PM
> To: David Reiner <davidr at rhotrading.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: [SPAM] - Re: [R-SIG-Finance] Intraday data with RBloomberg -
> Email found in subject
>
> Hi David, that doesn't seem to work either.
>
> In the example below, I cut and paste the example from ?blpGetData,
> replacing only the security field with 'USDJPY Curncy'. Notice that the
> example works great when we use 'ED1 Comdty'. Also, when I try the
> historical data example, it works great for USDJPY Curncy daily prices.
> I'm totally stumped.
>
>
> > edc=blpGetData(conn,'USDJPY Curncy', c('BID','ASK'),
> start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
>
> > edc
>                     BID.OPEN ASK.OPEN
> (07/04/07 07:46:10)       NA       NA
>
> > edc=blpGetData(conn,'ED1 Comdty', c('BID','ASK'),
> start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
> > edc
>                     BID.OPEN ASK.OPEN
> (07/03/07 22:38:00)   94.660   94.665
> (07/03/07 22:40:00)   94.660   94.665
> (07/03/07 22:42:00)   94.660   94.665
> (07/03/07 22:44:00)   94.660   94.665
> (07/03/07 22:46:00)   94.660   94.665
> (07/03/07 22:48:00)   94.660   94.665
> (07/03/07 22:50:00)   94.660   94.665
> (07/03/07 22:52:00)   94.660   94.665
> (07/03/07 22:54:00)   94.660   94.665
> (07/03/07 22:56:00)   94.660   94.665
> (07/03/07 22:58:00)   94.660   94.665
> etc....
>
>
>
> On 7/3/07, davidr at rhotrading.com <davidr at rhotrading.com> wrote:
> > Almost there! You just have to set the barfields; see the example at
> the
> > end of ?blpGetData.
> > HTH,
> >
> > David L. Reiner
> > Rho Trading Securities, LLC
> > 550 W. Jackson Blvd #1000
> > Chicago, IL 60661-5704
> >
> > 312-244-4610 direct
> > 312-244-4500 main
> > 312-244-4501 fax
> >
> >
> > -----Original Message-----
> > From: r-sig-finance-bounces at stat.math.ethz.ch
> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian Seow
> > Sent: Tuesday, July 03, 2007 2:25 AM
> > To: r-sig-finance at stat.math.ethz.ch
> > Subject: [R-SIG-Finance] Intraday data with RBloomberg
> >
> > Hi, I'm currently trying to implement an intraday currency model using
>
> > a live feed from bloomberg.
> > I hit the following error when I attempt to download intraday 2 min
> > ticks:
> >
> > > usdjpy=blpGetData(conn,securities='USDJPY
> > Curncy',fields=c('BID','ASK'), start= as.chron(Sys.time()-6000),
> > end=as.chron(Sys.time()),barsize=2, barfields='OPEN')
> >
> > > usdjpy
> >                     BID.OPEN ASK.OPEN
> > (07/03/07 15:01:45)       NA       NA
> >
> > blpGetData works fine for historical price data and seems to work fine
>
> > for commodity futures ( e.g. the example above works fine for ED1
> > Comdty), so I'm puzzled why this function does not return a result for
>
> > Curncy. I double-checked the "USDJPY Curncy" intraday bloomberg API
> > feed in excel and it works.
> >
> > Also, is it a good idea in general to avoid implementing such models
> > in R?  Would C/C++ be a better alternative?
> > Any ideas / insights would be greatly appreciated! Thanks.
> >
> >
> > Best Regards
> > Ian Seow
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From robert at sanctumfi.com  Mon Jul  9 12:31:02 2007
From: robert at sanctumfi.com (Robert Sams)
Date: Mon, 9 Jul 2007 11:31:02 +0100
Subject: [R-SIG-Finance] Intraday data with RBloomberg
References: <1865010707030020x1332441dpd8a90371138e905c@mail.gmail.com>
	<1865010707030025o5f4adee6j12ead01b979a6913@mail.gmail.com>
	<F9F2A641C593D7408925574C05A7BE77395240@rhopost.rhotrading.com>
	<1865010707031653j23edb395xb249178b9009b633@mail.gmail.com>
	<SANCTUMFISERVER049u00003140@sanctumfi.com>
	<SANCTUMFISERVERPzg90000337f@sanctumfi.com>
	<SANCTUMFISERVER9eab000034e3@sanctumfi.com>
Message-ID: <SANCTUMFISERVEREc830000365b@sanctumfi.com>

Hi Ian,

I'm not aware of any delay in the intraday historical calls. For
example, I made this call at just before 11:07am London time:

 blpGetData(conn, "ERU9 COMDTY", "BEST_BID",
start=chron("7/9/7","11:05:00"), barsize=0)
                    BEST_BID
(07/09/07 11:05:18)    95.22
(07/09/07 11:05:33)    95.22
(07/09/07 11:05:33)    95.22
(07/09/07 11:05:36)    95.22
(07/09/07 11:05:43)    95.22
(07/09/07 11:05:44)    95.22
(07/09/07 11:05:46)    95.22
(07/09/07 11:05:47)    95.22
(07/09/07 11:05:51)    95.22
(07/09/07 11:06:11)    95.22
(07/09/07 11:06:13)    95.22
(07/09/07 11:06:15)    95.22
(07/09/07 11:06:15)    95.22
(07/09/07 11:06:23)    95.22
(07/09/07 11:06:23)    95.22
(07/09/07 11:06:34)    95.22
(07/09/07 11:06:42)    95.22
(07/09/07 11:06:42)    95.22
(07/09/07 11:06:47)    95.22
(07/09/07 11:06:48)    95.22
(07/09/07 11:06:57)    95.22
(07/09/07 11:06:58)    95.22
> 

Perhaps you need to enable your terminal for live feeds on the product
under question?

As for the integration of RBloomberg with a trading system.. I guess it
depends upon what you are trying to do. I wouldn't have thought that R
is the appropriate platform on which to build an algorithmic trading
program, but I would be very interested to hear about the successful
implementation of such a thing. 

For analysis, backtesting, etc I think the Bloomberg/R solution is
ideal. But for trade execution, I prefer using feeds from the platform I
use to trade (e.g., TradingTechnologies for futures trading, TradeWeb
for OTC). There are many such platforms on the market; as I don't trade
FX, I cannot give you any useful advice here. Just about all of them
provide at least a C API, so if you have R code that you want to use in
an algorithmic trading environment, I'm sure you can integrate it with
whatever platform you end up using.

FYI.. I will put a new version of RBloomberg on cran at some point this
week.

Robert
  

-----Original Message-----
From: Ian Seow [mailto:ianseow at gmail.com] 
Sent: 07 July 2007 01:11
To: Robert Sams
Cc: davidr at rhotrading.com; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Intraday data with RBloomberg

Hi Robert, this definitely works for me! 'BEST_BID' worked like a charm,
thank you so much.

Another point of curiousity... I am currently trying to build a model in
R which uses bloomberg price feeds (minute data) to obtain buy/sell
orders and execute them via an electronic trading platform API. I've
done something similar successfully using daily data, but this is the
first time I'm doing this on an intraday basis. My main worry is in the
time lag I've been observing when getting the bloomberg feeds (this lag
is present, regardless of whether I'm using VBA or RBloomberg)...

Could anyone with similar experiences give me some advice? Or perhaps I
should rethink the system architecture altogether, or perhaps use
another price feed more suited for this purpose?

Regards
Ian Seow

On 7/6/07, Robert Sams <robert at sanctumfi.com> wrote:
> David,
>
> "sort of given up"? I'm disappointed in you ;)
>
> Let's look at Ian's original example.
>
>  blpGetData(conn,'USDJPY Curncy', c('BID'), 
> start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
>                     BID.OPEN
> (07/06/07 14:02:50)       NA
>
> Indeed, this doesn't seem right. A little more transparency:
>
> blpGetData(conn,'USDJPY Curncy', 'BID', 
> start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2, 
> retval="raw") [[1]] [[1]][[1]] [[1]][[1]][[1]] [[1]][[1]][[1]][[1]] An

> object of class "COMDate"
> [1] 39269.59
>
>
>
>
> [[2]]
> [[2]][[1]]
> [[2]][[1]][[1]]
> [[2]][[1]][[1]][[1]]
> [1] "#N/A History"
>
>
>
>
> attr(,"num.of.date.cols")
> [1] 1
> attr(,"class")
> [1] "BlpCOMReturn"
> attr(,"securities")
> [1] "USDJPY CURNCY"
> attr(,"fields")
> [1] "BID"
> attr(,"barfields")
> [1] "OPEN"
> >
>
> The existence of a "no available history" Bloomberg error in the 
> return value of the underlying COM method makes me think that this 
> isn't really an RBloomberg or RDCOMClient package error.
>
> It's always useful to experiment with different fields.. in a few 
> minutes I discovered this:
>
> blpGetData(conn,'USDJPY Curncy', 'BEST_BID', 
> start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
>                     BEST_BID.OPEN
> (07/06/07 12:07:00)       123.230
> (07/06/07 12:09:00)       123.230
> (07/06/07 12:11:00)       123.240
> (07/06/07 12:13:00)       123.230
> (07/06/07 12:15:00)       123.215
> (07/06/07 12:17:00)       123.230
> (07/06/07 12:19:00)       123.260
> (07/06/07 12:21:00)       123.250
> (07/06/07 12:23:00)       123.260
> (07/06/07 12:25:00)       123.270
> (07/06/07 12:27:00)       123.280
> (07/06/07 12:29:00)       123.320
> etc..
>
> Does this call work for you Ian?
>
> Now, I don't know why BID does not work but BEST_BID does, but it 
> seems to me that the answer lies on the Bloomberg API side of things 
> rather than with RBloomberg. But maybe I'm wrong. Can somebody send me

> a VBA snippet that shows a successful call to 
> GetHistoricalData(Security='USDJPY Curncy', Fields='BID', ... in that 
> language's binding of the COM interface? If so, it's worth looking
into.
>
>
> I haven't touched the RBloomberg code for well over a year but use it 
> every day in a trading environment without any problems. I will 
> however set aside some time over the next week to clean up the code 
> and incorporate any bug fixes or features that RBloombergers care to 
> submit, so please keep an eye on cran for an updated version(s).
>
> Cheers,
>
> Robert
>
>
>
>
>
> -----Original Message-----
> From: davidr at rhotrading.com [mailto:davidr at rhotrading.com]
> Sent: 05 July 2007 15:56
> To: Ian Seow
> Cc: r-sig-finance at stat.math.ethz.ch; Robert Sams
> Subject: RE: [SPAM] - Re: [R-SIG-Finance] Intraday data with 
> RBloomberg
> - Email found in subject
>
> I can verify similar behavior.
> I can get the data via VBA behind Excel.
> Between versions of Bloomberg and R and the difficulty I seem to have 
> getting RDCOMClient installed correctly, I have sort of given up on 
> this approach, unfortunately so, since it seems so useful. I pretty 
> much use VBA/Excel or C# to generate text files to read into R.
>
> David L. Reiner
> Rho Trading Securities, LLC
> 550 W. Jackson Blvd #1000
> Chicago, IL 60661-5704
>
> 312-244-4610 direct
> 312-244-4500 main
> 312-244-4501 fax
>
>
> -----Original Message-----
> From: Ian Seow [mailto:ianseow at gmail.com]
> Sent: Tuesday, July 03, 2007 6:53 PM
> To: David Reiner <davidr at rhotrading.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: [SPAM] - Re: [R-SIG-Finance] Intraday data with RBloomberg - 
> Email found in subject
>
> Hi David, that doesn't seem to work either.
>
> In the example below, I cut and paste the example from ?blpGetData, 
> replacing only the security field with 'USDJPY Curncy'. Notice that 
> the example works great when we use 'ED1 Comdty'. Also, when I try the

> historical data example, it works great for USDJPY Curncy daily
prices.
> I'm totally stumped.
>
>
> > edc=blpGetData(conn,'USDJPY Curncy', c('BID','ASK'),
> start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
>
> > edc
>                     BID.OPEN ASK.OPEN
> (07/04/07 07:46:10)       NA       NA
>
> > edc=blpGetData(conn,'ED1 Comdty', c('BID','ASK'),
> start=as.chron(Sys.time()-3600), barfields='OPEN', barsize=2)
> > edc
>                     BID.OPEN ASK.OPEN
> (07/03/07 22:38:00)   94.660   94.665
> (07/03/07 22:40:00)   94.660   94.665
> (07/03/07 22:42:00)   94.660   94.665
> (07/03/07 22:44:00)   94.660   94.665
> (07/03/07 22:46:00)   94.660   94.665
> (07/03/07 22:48:00)   94.660   94.665
> (07/03/07 22:50:00)   94.660   94.665
> (07/03/07 22:52:00)   94.660   94.665
> (07/03/07 22:54:00)   94.660   94.665
> (07/03/07 22:56:00)   94.660   94.665
> (07/03/07 22:58:00)   94.660   94.665
> etc....
>
>
>
> On 7/3/07, davidr at rhotrading.com <davidr at rhotrading.com> wrote:
> > Almost there! You just have to set the barfields; see the example at
> the
> > end of ?blpGetData.
> > HTH,
> >
> > David L. Reiner
> > Rho Trading Securities, LLC
> > 550 W. Jackson Blvd #1000
> > Chicago, IL 60661-5704
> >
> > 312-244-4610 direct
> > 312-244-4500 main
> > 312-244-4501 fax
> >
> >
> > -----Original Message-----
> > From: r-sig-finance-bounces at stat.math.ethz.ch
> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ian 
> > Seow
> > Sent: Tuesday, July 03, 2007 2:25 AM
> > To: r-sig-finance at stat.math.ethz.ch
> > Subject: [R-SIG-Finance] Intraday data with RBloomberg
> >
> > Hi, I'm currently trying to implement an intraday currency model 
> > using
>
> > a live feed from bloomberg.
> > I hit the following error when I attempt to download intraday 2 min
> > ticks:
> >
> > > usdjpy=blpGetData(conn,securities='USDJPY
> > Curncy',fields=c('BID','ASK'), start= as.chron(Sys.time()-6000), 
> > end=as.chron(Sys.time()),barsize=2, barfields='OPEN')
> >
> > > usdjpy
> >                     BID.OPEN ASK.OPEN
> > (07/03/07 15:01:45)       NA       NA
> >
> > blpGetData works fine for historical price data and seems to work 
> > fine
>
> > for commodity futures ( e.g. the example above works fine for ED1 
> > Comdty), so I'm puzzled why this function does not return a result 
> > for
>
> > Curncy. I double-checked the "USDJPY Curncy" intraday bloomberg API 
> > feed in excel and it works.
> >
> > Also, is it a good idea in general to avoid implementing such models

> > in R?  Would C/C++ be a better alternative?
> > Any ideas / insights would be greatly appreciated! Thanks.
> >
> >
> > Best Regards
> > Ian Seow
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From rory.winston at gmail.com  Wed Jul 11 13:24:36 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Wed, 11 Jul 2007 12:24:36 +0100
Subject: [R-SIG-Finance] Simple TWAP Algorithm in R, Part II
Message-ID: <3f446aa30707110424l245ce3fdh4dcaf742d93dcdd4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070711/0681155e/attachment.pl 

From David.Brahm at geodecapital.com  Wed Jul 11 20:11:13 2007
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Wed, 11 Jul 2007 14:11:13 -0400
Subject: [R-SIG-Finance] Simple TWAP Algorithm in R, Part II
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D80781D1F3@MSGBOSCLF2WIN.DMN1.FMR.COM>

The [-1] in your tapply formula is what associates each spread with the
PRIOR time difference.  If you want to associate it with the NEXT time
difference, just chop off the last element instead of the first.

This function may be handy:
chop <- function(x) rev(rev(x)[-1])

Then:
twp <- tapply( diff(pricedata$time), chop(pricedata$spread), sum )

-- David Brahm (brahm at alum.mit.edu) 



-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Rory
Winston
Sent: Wednesday, July 11, 2007 7:25 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Simple TWAP Algorithm in R, Part II

Hi all

I recently posted a question regarding a simple time-averaged
calculation in
R. I have an input data file which is a historical list of prices and
their
time:

Time                  Bid        Ask
1183480907.042 1.36150 1.36160
1183480919.045 1.36150 1.36170
1183480923.044 1.36150 1.36160
....

After some data conversion and spread calculations, I use R to calculate
a
time-weighted average (based on a clever tapply() command graciously
supplied by Patrick Burns):

twp <- tapply( diff(pricedata$time), pricedata$spread[-1], sum )

I can then calculate the time-weighted average by dividing twp by
sum(diff(pricedata$time)).

When I compared my numbers with someone else who had done the
calculations,
it became apparent that I need to make a slight modification to the
tapply()
command : the current logic basically computes a running sum as spreads
[S_n] += T_{n} - T_{n-1}, whereas the correct calculation is spreads
[S_{n-1}] += T_{n} - T_{n-1}. In other words, I need to tell tapply()
somehow to offset the spread vector by one. I have fiddled around with
various permutations of tapply() trying to get it to do this, but with
no
success so far. I guess I could rebase the vector of spreads before
calculating, but this seems like an unecessary hassle. Does anyone out
there
know how I can get tapply() (or a similar command) to do what I want?

Thanks
Rory

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From ggrothendieck at gmail.com  Thu Jul 12 02:54:48 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 11 Jul 2007 20:54:48 -0400
Subject: [R-SIG-Finance] Simple TWAP Algorithm in R, Part II
In-Reply-To: <4DD6F8B8782D584FABF50BF3A32B03D80781D1F3@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <4DD6F8B8782D584FABF50BF3A32B03D80781D1F3@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <971536df0707111754h59851bb0re53be39682fea43f@mail.gmail.com>

chop(x) below is the same as head(x, -1)

On 7/11/07, Brahm, David <David.Brahm at geodecapital.com> wrote:
> The [-1] in your tapply formula is what associates each spread with the
> PRIOR time difference.  If you want to associate it with the NEXT time
> difference, just chop off the last element instead of the first.
>
> This function may be handy:
> chop <- function(x) rev(rev(x)[-1])
>
> Then:
> twp <- tapply( diff(pricedata$time), chop(pricedata$spread), sum )
>
> -- David Brahm (brahm at alum.mit.edu)
>
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Rory
> Winston
> Sent: Wednesday, July 11, 2007 7:25 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Simple TWAP Algorithm in R, Part II
>
> Hi all
>
> I recently posted a question regarding a simple time-averaged
> calculation in
> R. I have an input data file which is a historical list of prices and
> their
> time:
>
> Time                  Bid        Ask
> 1183480907.042 1.36150 1.36160
> 1183480919.045 1.36150 1.36170
> 1183480923.044 1.36150 1.36160
> ....
>
> After some data conversion and spread calculations, I use R to calculate
> a
> time-weighted average (based on a clever tapply() command graciously
> supplied by Patrick Burns):
>
> twp <- tapply( diff(pricedata$time), pricedata$spread[-1], sum )
>
> I can then calculate the time-weighted average by dividing twp by
> sum(diff(pricedata$time)).
>
> When I compared my numbers with someone else who had done the
> calculations,
> it became apparent that I need to make a slight modification to the
> tapply()
> command : the current logic basically computes a running sum as spreads
> [S_n] += T_{n} - T_{n-1}, whereas the correct calculation is spreads
> [S_{n-1}] += T_{n} - T_{n-1}. In other words, I need to tell tapply()
> somehow to offset the spread vector by one. I have fiddled around with
> various permutations of tapply() trying to get it to do this, but with
> no
> success so far. I guess I could rebase the vector of spreads before
> calculating, but this seems like an unecessary hassle. Does anyone out
> there
> know how I can get tapply() (or a similar command) to do what I want?
>
> Thanks
> Rory
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From rory.winston at gmail.com  Thu Jul 12 12:33:38 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Thu, 12 Jul 2007 11:33:38 +0100
Subject: [R-SIG-Finance] Simple TWAP Algorithm in R, Part II
In-Reply-To: <971536df0707111754h59851bb0re53be39682fea43f@mail.gmail.com>
References: <4DD6F8B8782D584FABF50BF3A32B03D80781D1F3@MSGBOSCLF2WIN.DMN1.FMR.COM>
	<971536df0707111754h59851bb0re53be39682fea43f@mail.gmail.com>
Message-ID: <3f446aa30707120333k48680345ic59550ac8a928308@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070712/8fd0e070/attachment.pl 

From dtsmith at mindspring.com  Sat Jul 14 17:06:14 2007
From: dtsmith at mindspring.com (Dale Smith)
Date: Sat, 14 Jul 2007 11:06:14 -0400
Subject: [R-SIG-Finance]  Rmetrics gpdFit & fitting an entire distribution
Message-ID: <6e2ae3605acc3cd1e0c3d604e6f9e590@mindspring.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070714/942f02e3/attachment.pl 

From brian at braverock.com  Sat Jul 14 18:04:46 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Sat, 14 Jul 2007 11:04:46 -0500
Subject: [R-SIG-Finance] Rmetrics gpdFit & fitting an entire distribution
In-Reply-To: <6e2ae3605acc3cd1e0c3d604e6f9e590@mindspring.com>
References: <6e2ae3605acc3cd1e0c3d604e6f9e590@mindspring.com>
Message-ID: <4698F41E.3070109@braverock.com>

Dale Smith wrote:
> Hi there,
> 
> We are interested in fitting a generalized pareto distribution to the 
> tails of our
> stock returns, with a kernel fit to the remainder. We looked at gpdFit, 
> but it fits
> tails, instead of the entire distribution. We thought gpdFit worked 
> like the corresponding
> function in FinMetrics ? gpd.tail. Alas, gpdFit in fExtremes works 
> differently,
> by fitting tails only, instead of the entire distribution.
> 
> I looked at the archives for information on this problem, but didn?t 
> find anything.
> At this point, we plan to fit each tail separately, and fit the middle 
> of the distribution
> with another non-parametric method. We are really looking for the 
> quantiles of the
> fitted distribution.
> 
> Has anyone done this before, and might be willing to share the 
> algorithm they used?

On first inspection, it looks like the underlying function 'gpd' would
fit the entire distribution.

You may also wish to examine the code for the VaR.gpd function from
package VaR to get another log-liklihood gpd estimate

Cheers,

    - Brian


From dtsmith at mindspring.com  Sat Jul 14 18:42:12 2007
From: dtsmith at mindspring.com (Dale Smith)
Date: Sat, 14 Jul 2007 12:42:12 -0400
Subject: [R-SIG-Finance] Rmetrics gpdFit & fitting an entire distribution
In-Reply-To: <4698F41E.3070109@braverock.com>
References: <6e2ae3605acc3cd1e0c3d604e6f9e590@mindspring.com>
	<4698F41E.3070109@braverock.com>
Message-ID: <5add922264769615b04e8b7258a63358@mindspring.com>

Ok thanks very much, I will do that.

Dale

On Jul 14, 2007, at 12:04 PM, Brian G. Peterson wrote:

> Dale Smith wrote:
>> Hi there,
>>
>> We are interested in fitting a generalized pareto distribution to the
>> tails of our
>> stock returns, with a kernel fit to the remainder. We looked at 
>> gpdFit,
>> but it fits
>> tails, instead of the entire distribution. We thought gpdFit worked
>> like the corresponding
>> function in FinMetrics ? gpd.tail. Alas, gpdFit in fExtremes works
>> differently,
>> by fitting tails only, instead of the entire distribution.
>>
>> I looked at the archives for information on this problem, but didn?t
>> find anything.
>> At this point, we plan to fit each tail separately, and fit the middle
>> of the distribution
>> with another non-parametric method. We are really looking for the
>> quantiles of the
>> fitted distribution.
>>
>> Has anyone done this before, and might be willing to share the
>> algorithm they used?
>
> On first inspection, it looks like the underlying function 'gpd' would
> fit the entire distribution.
>
> You may also wish to examine the code for the VaR.gpd function from
> package VaR to get another log-liklihood gpd estimate
>
> Cheers,
>
>     - Brian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. -- If you want to post, subscribe first.
>


From ezivot at u.washington.edu  Sun Jul 15 00:44:17 2007
From: ezivot at u.washington.edu (Eric Zivot)
Date: Sat, 14 Jul 2007 15:44:17 -0700
Subject: [R-SIG-Finance] Rmetrics gpdFit & fitting an entire distribution
In-Reply-To: <6e2ae3605acc3cd1e0c3d604e6f9e590@mindspring.com>
Message-ID: <200707142244.l6EMiHGh030807@smtp.washington.edu>

Rene Carmona has his EVENASCA splus library available for free download and
this has the gpd function for nonparametrically fitting the middle of the
distribution. See
http://www.orfe.princeton.edu/~rcarmona/SVbook/evanesce.zip


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Dale Smith
Sent: Saturday, July 14, 2007 8:06 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Rmetrics gpdFit & fitting an entire distribution

Hi there,

We are interested in fitting a generalized pareto distribution to the tails
of our stock returns, with a kernel fit to the remainder. We looked at
gpdFit, but it fits tails, instead of the entire distribution. We thought
gpdFit worked like the corresponding function in FinMetrics  gpd.tail.
Alas, gpdFit in fExtremes works differently, by fitting tails only, instead
of the entire distribution.

I looked at the archives for information on this problem, but didnt find
anything.
At this point, we plan to fit each tail separately, and fit the middle of
the distribution with another non-parametric method. We are really looking
for the quantiles of the fitted distribution.

Has anyone done this before, and might be willing to share the algorithm
they used?

Thanks very much,
Dale Smith
dtsmith at mindspring.com
	[[alternative text/enriched version deleted]]


From sankalp.upadhyay at gmail.com  Sun Jul 15 14:55:09 2007
From: sankalp.upadhyay at gmail.com (Sankalp Upadhyay)
Date: Sun, 15 Jul 2007 18:25:09 +0530
Subject: [R-SIG-Finance] SPD and RND estimation
Message-ID: <ff91746c0707150555x67f4ab2fndb42ccdd0e273fc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070715/0578351f/attachment.pl 

From brian at braverock.com  Sun Jul 15 15:44:36 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Sun, 15 Jul 2007 08:44:36 -0500
Subject: [R-SIG-Finance] SPD and RND estimation
In-Reply-To: <ff91746c0707150555x67f4ab2fndb42ccdd0e273fc@mail.gmail.com>
References: <ff91746c0707150555x67f4ab2fndb42ccdd0e273fc@mail.gmail.com>
Message-ID: <469A24C4.7000808@braverock.com>

Sankalp Upadhyay wrote:
> Hi,
> 
> I am trying to do an estimation of State Price Density (SPD) and Risk
> Neutral Density (RND) from a set of option prices - preferably in a
> non-parametric way.
> Is there some package in R that can help? fOptions does not seem to be have
> this.
> 
> Alternatively, would you know a standard method or a very good research
> paper/article/reference on this topic? The utility being that a good
> paper/article can be changed to R code easily.

The function 'ksmooth' provides a nonparametric kernel regression 
smoothing algorithm which should be sufficient for the estimation of the 
density function. Kernel smoothing regression is the method utilized in 
the original Ait-Sahalia and Lo paper you reference.

The last paper referenced below uses several other distribution fitting 
mechanisms, and may be informative.

A quick citation survey indicates that the original paper is referenced 
several times, but no one seems to take up a direct implementation of 
the techniques described in the 1995 paper.

R contains a multitude of distribution fitting methods, so if the kernel 
smoothing method is not sufficient, I'd read the last paper referenced 
here first, and then look for R methods of doing those fittings.

As always, please consider sharing your resulting R code and functions 
with the community.

Regards,

   - Brian


Ref:
Ait-Sahalia, Yacine and Lo, Andrew W., "Nonparametric Estimation of 
State-Price Densities Implicit in Financial Asset Prices" (November 
1995). NBER Working Paper No. W5351.
Available at SSRN: http://ssrn.com/abstract=225414

Ait-Sahalia, Yacine and Lo, Andrew W., "Non-Parametric Risk Management 
and Implied Risk Aversion" (January 4, 1998). CRSP Working Paper No. 468.
Available at SSRN: http://ssrn.com/abstract=94133

Chang, P H Kevin and Melick, William R
"Workshop on estimating and interpreting probability density functions"
14 June 1999
http://www.bis.org/publ/bisp06_p1.pdf

Campa, Jose M. et. al.
"Implied Exchange Rate Distributions: Evidence from OTC Option Markets"
Working Paper, April 1997
http://finance.math.biu.ac.il/readings/bonds/3%20implied%20exch%20rates.pdf


From evgeny.panov at citi.com  Mon Jul 16 14:58:55 2007
From: evgeny.panov at citi.com (Panov, Evgeny )
Date: Mon, 16 Jul 2007 08:58:55 -0400
Subject: [R-SIG-Finance] SPD and RND estimation
Message-ID: <370AE4BE62A7674C8299149A951D1D580B969B75@EXNJMB61.nam.nsroot.net>

Dear Sankalp,

As a cooking recipe, will probably benefit a lot from first converting the prices into Black-Scholes implied volatilities and then smoothing the implied volatilities, after which you can go back into price space (don't worry about unknown dividend yield - you can back it out from put-call parity).

Best regards,
Gene

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of Sankalp
Upadhyay
Sent: Sunday, July 15, 2007 8:55 AM
To: R-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] SPD and RND estimation


Hi,

I am trying to do an estimation of State Price Density (SPD) and Risk
Neutral Density (RND) from a set of option prices - preferably in a
non-parametric way.
Is there some package in R that can help? fOptions does not seem to be have
this.

Alternatively, would you know a standard method or a very good research
paper/article/reference on this topic? The utility being that a good
paper/article can be changed to R code easily.

Many thanks,

Sankalp

-- 
--
Sankalp Upadhyay

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From brian at braverock.com  Mon Jul 16 15:41:15 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 16 Jul 2007 08:41:15 -0500
Subject: [R-SIG-Finance] SPD and RND estimation
In-Reply-To: <370AE4BE62A7674C8299149A951D1D580B969B75@EXNJMB61.nam.nsroot.net>
References: <370AE4BE62A7674C8299149A951D1D580B969B75@EXNJMB61.nam.nsroot.net>
Message-ID: <469B757B.7080208@braverock.com>

Panov, Evgeny wrote:
> As a cooking recipe, will probably benefit a lot from first converting 
 > the prices into Black-Scholes implied volatilities and then smoothing
 > the implied volatilities, after which you can go back into price space
 > (don't worry about unknown dividend yield - you can back it out from
 > put-call parity).

The problem with this approach is that the point of the cited 
Ait-Sahalia and Lo paper is to do a non-parametric estimation of the 
density function from prices. They propose the use of kernel smoothing 
regression, although other non-parametric methods are cited in other papers.

Regards,

   - Brian

Ref:
Ait-Sahalia, Yacine and Lo, Andrew W., "Nonparametric Estimation of
State-Price Densities Implicit in Financial Asset Prices" (November
1995). NBER Working Paper No. W5351.
Available at SSRN: http://ssrn.com/abstract=225414

> -----Original Message-----
> From: Sankalp Upadhyay
> Sent: Sunday, July 15, 2007 8:55 AM
> 
> Hi,
> 
> I am trying to do an estimation of State Price Density (SPD) and Risk
> Neutral Density (RND) from a set of option prices - preferably in a
> non-parametric way.
> Is there some package in R that can help? fOptions does not seem to be have
> this.
> 
> Alternatively, would you know a standard method or a very good research
> paper/article/reference on this topic? The utility being that a good
> paper/article can be changed to R code easily.
> 
> Many thanks,
> 
> Sankalp


From evgeny.panov at citi.com  Mon Jul 16 15:52:14 2007
From: evgeny.panov at citi.com (Panov, Evgeny )
Date: Mon, 16 Jul 2007 09:52:14 -0400
Subject: [R-SIG-Finance] SPD and RND estimation
Message-ID: <370AE4BE62A7674C8299149A951D1D580C1B7F50@EXNJMB61.nam.nsroot.net>

Sorry, you are right. The concept of Black-Scholes implied volatility imposes parametric approach.
I tried to refer to it as a "cooking recipe" because this approach is indeed not perfect theoretically.

-----Original Message-----
From: Brian G. Peterson [mailto:brian at braverock.com]
Sent: Monday, July 16, 2007 9:41 AM
To: Panov, Evgeny [CMB-EQTY]
Cc: Sankalp Upadhyay; R-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] SPD and RND estimation


Panov, Evgeny wrote:
> As a cooking recipe, will probably benefit a lot from first converting 
 > the prices into Black-Scholes implied volatilities and then smoothing
 > the implied volatilities, after which you can go back into price space
 > (don't worry about unknown dividend yield - you can back it out from
 > put-call parity).

The problem with this approach is that the point of the cited 
Ait-Sahalia and Lo paper is to do a non-parametric estimation of the 
density function from prices. They propose the use of kernel smoothing 
regression, although other non-parametric methods are cited in other papers.

Regards,

   - Brian

Ref:
Ait-Sahalia, Yacine and Lo, Andrew W., "Nonparametric Estimation of
State-Price Densities Implicit in Financial Asset Prices" (November
1995). NBER Working Paper No. W5351.
Available at SSRN: http://ssrn.com/abstract=225414

> -----Original Message-----
> From: Sankalp Upadhyay
> Sent: Sunday, July 15, 2007 8:55 AM
> 
> Hi,
> 
> I am trying to do an estimation of State Price Density (SPD) and Risk
> Neutral Density (RND) from a set of option prices - preferably in a
> non-parametric way.
> Is there some package in R that can help? fOptions does not seem to be have
> this.
> 
> Alternatively, would you know a standard method or a very good research
> paper/article/reference on this topic? The utility being that a good
> paper/article can be changed to R code easily.
> 
> Many thanks,
> 
> Sankalp


From mario.aignertorres at gmail.com  Mon Jul 16 17:17:35 2007
From: mario.aignertorres at gmail.com (Mario Aigner-Torres)
Date: Mon, 16 Jul 2007 12:17:35 -0300
Subject: [R-SIG-Finance] books on financial data and R
Message-ID: <af34d0c00707160817p4309ec2as65c977880a996a25@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070716/b0d382a8/attachment.pl 

From adrian_d at eskimo.com  Mon Jul 16 17:43:10 2007
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Mon, 16 Jul 2007 08:43:10 -0700 (PDT)
Subject: [R-SIG-Finance] books on financial data and R
In-Reply-To: <af34d0c00707160817p4309ec2as65c977880a996a25@mail.gmail.com>
References: <af34d0c00707160817p4309ec2as65c977880a996a25@mail.gmail.com>
Message-ID: <Pine.SUN.4.58.0707160842050.9844@eskimo.com>


You can check out these two.


Pfaff, Bernhard -- Analysis of integrated and cointegrated time series
  with R.  	QA280.P5 2006

Shumway, Robert H. -- Time series analysis and its applications: with
  R examples / Robert H. Shumway, David S. Stoffer.  2nd [updated]
  ed. 2006.  QA280.S585 2006

Adrian Dragulescu


On Mon, 16 Jul 2007, Mario Aigner-Torres wrote:

> Dear All,
>
> are any books on financial data/financial econometrics using R available?
>
> I appreciate your kind answer.
>
> Best regards,
>
> Mario Aigner-Torres
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From frednovo at pipeline.com  Mon Jul 16 17:45:10 2007
From: frednovo at pipeline.com (Frederick Novomestky)
Date: Mon, 16 Jul 2007 11:45:10 -0400
Subject: [R-SIG-Finance] books on financial data and R
In-Reply-To: <af34d0c00707160817p4309ec2as65c977880a996a25@mail.gmail.co m>
References: <af34d0c00707160817p4309ec2as65c977880a996a25@mail.gmail.com>
Message-ID: <6.2.1.2.2.20070716114443.03beba90@pop.pipeline.com>

The Rene Carmona book which I use in my courses in Finance and Risk 
Engineering can be adapted to using R.

Best regards,

Fred

At 11:17 AM 7/16/2007, Mario Aigner-Torres wrote:
>Dear All,
>
>are any books on financial data/financial econometrics using R available?
>
>I appreciate your kind answer.
>
>Best regards,
>
>Mario Aigner-Torres
>
>         [[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.

Frederick Novomestky, Ph.D.
Novomestky Associates LLC
41 Eastover Drive
East Northport, NY 11731-4330
Vox: 1.631.368.0701
Fax: 1.631.368.1696

Confidentiality Notice: This electronic mail transmission, i...{{dropped}}


From davidr at rhotrading.com  Mon Jul 16 18:28:50 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Mon, 16 Jul 2007 11:28:50 -0500
Subject: [R-SIG-Finance] books on financial data and R
In-Reply-To: <6.2.1.2.2.20070716114443.03beba90@pop.pipeline.com>
References: <af34d0c00707160817p4309ec2as65c977880a996a25@mail.gmail.com>
	<6.2.1.2.2.20070716114443.03beba90@pop.pipeline.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE773F2FB9@rhopost.rhotrading.com>

How do you get around the use of finmetrics in the scripts?
(It's mentioned in a review.) What is the real link to the datasets?

Thanks,

David L. Reiner
Rho Trading Securities, LLC
550 W. Jackson Blvd #1000
Chicago, IL 60661-5704
 
312-244-4610 direct
312-244-4500 main
312-244-4501 fax
 
-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Frederick
Novomestky
Sent: Monday, July 16, 2007 10:45 AM
To: Mario Aigner-Torres; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] books on financial data and R

The Rene Carmona book which I use in my courses in Finance and Risk 
Engineering can be adapted to using R.

Best regards,

Fred

At 11:17 AM 7/16/2007, Mario Aigner-Torres wrote:
>Dear All,
>
>are any books on financial data/financial econometrics using R
available?
>
>I appreciate your kind answer.
>
>Best regards,
>
>Mario Aigner-Torres
>
>         [[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.

Frederick Novomestky, Ph.D.
Novomestky Associates LLC
41 Eastover Drive
East Northport, NY 11731-4330
Vox: 1.631.368.0701
Fax: 1.631.368.1696

Confidentiality Notice: This electronic mail transmission,\ ...{{dropped}}


From kriskumar at earthlink.net  Tue Jul 17 03:18:09 2007
From: kriskumar at earthlink.net (kriskumar at earthlink.net)
Date: Tue, 17 Jul 2007 01:18:09 +0000
Subject: [R-SIG-Finance] SPD and RND estimation
In-Reply-To: <ff91746c0707150555x67f4ab2fndb42ccdd0e273fc@mail.gmail.com>
References: <ff91746c0707150555x67f4ab2fndb42ccdd0e273fc@mail.gmail.com>
Message-ID: <1743517288-1184635140-cardhu_decombobulator_blackberry.rim.net-1673914250-@bxe001.bisx.prod.on.blackberry>

Assuming you have a continuoum of call and put prices the RND could be directly estimated from numerical differentiation dC^2/dK^2 not clean and stable but a poor man's RND.

The problem one always has is with the wings and how to extrapolate in there and that is not an easy one to solve. 

Best
Kris 
 
??it is more important to have beauty in one?s equations than to have them fit experiment?It seems that if one is working from the point of view of getting beauty in one?s equations, and if one has really a sound insight, one is on a sure line of progress.? -Dirac

-----Original Message-----
From: "Sankalp Upadhyay" <sankalp.upadhyay at gmail.com>

Date: Sun, 15 Jul 2007 18:25:09 
To:R-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] SPD and RND estimation


Hi,

I am trying to do an estimation of State Price Density (SPD) and Risk
Neutral Density (RND) from a set of option prices - preferably in a
non-parametric way.
Is there some package in R that can help? fOptions does not seem to be have
this.

Alternatively, would you know a standard method or a very good research
paper/article/reference on this topic? The utility being that a good
paper/article can be changed to R code easily.

Many thanks,

Sankalp

-- 
--
Sankalp Upadhyay

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From joedtka at yahoo.com  Tue Jul 17 04:01:27 2007
From: joedtka at yahoo.com (Joseph Khalil)
Date: Mon, 16 Jul 2007 19:01:27 -0700 (PDT)
Subject: [R-SIG-Finance] Pricing option using Explicit Finite Difference
	method
Message-ID: <593762.69418.qm@web56212.mail.re3.yahoo.com>

Hi,

Does anyone have R code to calculate option prices (European, plain vanilla) using Explicit Finite
Difference method?   

Thanks,
Joe


From joedtka at yahoo.com  Tue Jul 17 15:56:20 2007
From: joedtka at yahoo.com (Joseph Khalil)
Date: Tue, 17 Jul 2007 06:56:20 -0700 (PDT)
Subject: [R-SIG-Finance] Pricing option using Explicit Finite Difference
	method
Message-ID: <514526.98851.qm@web56215.mail.re3.yahoo.com>

I am a new user of R and new to this mailing list.         I am using 
the book "Implementing Derivatives Models" by Les Clewlow and Chris
Strickland, and I am trying to simulate

the example on pricing options using the explicit finite difference
method in  chapter 3. I have tried the following to price plain European calls  but I am getting errors.  





K<-100      #Strike price

T<-1        #Time of maturity

t<-0        #Current time

sigma<-0.2  #Volatility

r<-0.06     #Interest rate

div<-0.03   # Dividend yield

S<-100      #Central stock price

dx<-0.02     #Space step around central asset price (logarithmic measurement exp(dx))

Nj<-30      #Number of space steps

N<-500      #Number of time steps until T



dt<-T/N # Time step

nu<-r-div-0.5*sigma^2

pu<-0.5*dt*((sigma/dx)^2+nu/dx)      # Probability for go up

pm<-1-dt*(sigma/dx)^2-r*dt           # Probability for stay the same

pd<-0.5*dt*((sigma/dx)^2-nu/dx)      # Probability for go down

# Initialise matrices

St<-matrix(data=0,nrow=2*Nj+1,ncol=1)

Call<-matrix(data=0,nrow=2*Nj+1,ncol=N+1)



#Compute independent values

for (j in (-Nj:Nj)) {

St[Nj-j+1,1]<-S*exp(j*dx)                 # Initialise asset prices at maturity

Call[Nj-j+1,N+1]<-max(0,St[Nj-j+1,1]-K) }  # Initialise option prices at maturity



#Compute dependent option values

    for (i in (N:1)) {       

# Explicit compution of option values (backwards)

        for (j in (-Nj+1:Nj-1))  { 

           
Call[j+Nj+1,i]<-as.matrix(pu*(as.matrix(Call[j+Nj,i+1])))+as.matrix(pm*Call[j+Nj+1,i+1])+as.matrix(pd*Call[j+Nj+2,i+1])


                                  }    

Call[1,i]<-Call[2,i]+St[1,1]-St[2,1]  # Upper boundary condition

Call[2*Nj+1,i]<-Call[2*Nj,i]          #Lower boundary condition

}

----- Original Message ----
From: Joseph Khalil <joedtka at yahoo.com>
To: r-sig-finance at stat.math.ethz.ch
Sent: Monday, July 16, 2007 10:01:27 PM
Subject: [R-SIG-Finance] Pricing option using Explicit Finite Difference method

Hi,

Does anyone have R code to calculate option prices (European, plain vanilla) using Explicit Finite
Difference method?   

Thanks,
Joe

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From evgeny.panov at citi.com  Tue Jul 17 16:26:08 2007
From: evgeny.panov at citi.com (Panov, Evgeny )
Date: Tue, 17 Jul 2007 10:26:08 -0400
Subject: [R-SIG-Finance] Pricing option using Explicit Finite
	Differencemethod
Message-ID: <370AE4BE62A7674C8299149A951D1D580B969B88@EXNJMB61.nam.nsroot.net>

Dear Joseph,

It seems like you need to price a European option.

Doing it R with finite-difference method will be very inefficient (will probably take more than one second per option), because R is inefficient with "for" loops (unless you can represent these multiplications that you are doing within the loop as one or two a huge matrix multiplications).

However, if it is just a European option, you can do everything in closed-form (Black-Scholes for lognormal RND or Black-Scholes with moment-matching for non-lognormal with discrete dividends will do the job), and it will take less than a second in R. 

So maybe instead of debugging finite-differences in R, you may consider not using them at all (if you want to do finite-differences, it can be done much more efficiently in c++, .net or java).

Best regards,

Gene

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of Joseph
Khalil
Sent: Tuesday, July 17, 2007 9:56 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Pricing option using Explicit Finite
Differencemethod


I am a new user of R and new to this mailing list.         I am using 
the book "Implementing Derivatives Models" by Les Clewlow and Chris
Strickland, and I am trying to simulate

the example on pricing options using the explicit finite difference
method in  chapter 3. I have tried the following to price plain European calls  but I am getting errors.  





K<-100      #Strike price

T<-1        #Time of maturity

t<-0        #Current time

sigma<-0.2  #Volatility

r<-0.06     #Interest rate

div<-0.03   # Dividend yield

S<-100      #Central stock price

dx<-0.02     #Space step around central asset price (logarithmic measurement exp(dx))

Nj<-30      #Number of space steps

N<-500      #Number of time steps until T



dt<-T/N # Time step

nu<-r-div-0.5*sigma^2

pu<-0.5*dt*((sigma/dx)^2+nu/dx)      # Probability for go up

pm<-1-dt*(sigma/dx)^2-r*dt           # Probability for stay the same

pd<-0.5*dt*((sigma/dx)^2-nu/dx)      # Probability for go down

# Initialise matrices

St<-matrix(data=0,nrow=2*Nj+1,ncol=1)

Call<-matrix(data=0,nrow=2*Nj+1,ncol=N+1)



#Compute independent values

for (j in (-Nj:Nj)) {

St[Nj-j+1,1]<-S*exp(j*dx)                 # Initialise asset prices at maturity

Call[Nj-j+1,N+1]<-max(0,St[Nj-j+1,1]-K) }  # Initialise option prices at maturity



#Compute dependent option values

    for (i in (N:1)) {       

# Explicit compution of option values (backwards)

        for (j in (-Nj+1:Nj-1))  { 

           
Call[j+Nj+1,i]<-as.matrix(pu*(as.matrix(Call[j+Nj,i+1])))+as.matrix(pm*Call[j+Nj+1,i+1])+as.matrix(pd*Call[j+Nj+2,i+1])


                                  }    

Call[1,i]<-Call[2,i]+St[1,1]-St[2,1]  # Upper boundary condition

Call[2*Nj+1,i]<-Call[2*Nj,i]          #Lower boundary condition

}

----- Original Message ----
From: Joseph Khalil <joedtka at yahoo.com>
To: r-sig-finance at stat.math.ethz.ch
Sent: Monday, July 16, 2007 10:01:27 PM
Subject: [R-SIG-Finance] Pricing option using Explicit Finite Difference method

Hi,

Does anyone have R code to calculate option prices (European, plain vanilla) using Explicit Finite
Difference method?   

Thanks,
Joe

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From evgeny.panov at citi.com  Tue Jul 17 16:29:03 2007
From: evgeny.panov at citi.com (Panov, Evgeny )
Date: Tue, 17 Jul 2007 10:29:03 -0400
Subject: [R-SIG-Finance] Pricing option using Explicit Finite
	Differencemethod
Message-ID: <370AE4BE62A7674C8299149A951D1D580B969B89@EXNJMB61.nam.nsroot.net>

By the way, if you decide to pursue the finite-difference route, you may consider using the implicit method, because it needs much less time steps to achieve the same precision (mainly because the explicit method for heat equation is diverges unless number of time steps is very high).

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of Joseph
Khalil
Sent: Tuesday, July 17, 2007 9:56 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Pricing option using Explicit Finite
Differencemethod


I am a new user of R and new to this mailing list.         I am using 
the book "Implementing Derivatives Models" by Les Clewlow and Chris
Strickland, and I am trying to simulate

the example on pricing options using the explicit finite difference
method in  chapter 3. I have tried the following to price plain European calls  but I am getting errors.  





K<-100      #Strike price

T<-1        #Time of maturity

t<-0        #Current time

sigma<-0.2  #Volatility

r<-0.06     #Interest rate

div<-0.03   # Dividend yield

S<-100      #Central stock price

dx<-0.02     #Space step around central asset price (logarithmic measurement exp(dx))

Nj<-30      #Number of space steps

N<-500      #Number of time steps until T



dt<-T/N # Time step

nu<-r-div-0.5*sigma^2

pu<-0.5*dt*((sigma/dx)^2+nu/dx)      # Probability for go up

pm<-1-dt*(sigma/dx)^2-r*dt           # Probability for stay the same

pd<-0.5*dt*((sigma/dx)^2-nu/dx)      # Probability for go down

# Initialise matrices

St<-matrix(data=0,nrow=2*Nj+1,ncol=1)

Call<-matrix(data=0,nrow=2*Nj+1,ncol=N+1)



#Compute independent values

for (j in (-Nj:Nj)) {

St[Nj-j+1,1]<-S*exp(j*dx)                 # Initialise asset prices at maturity

Call[Nj-j+1,N+1]<-max(0,St[Nj-j+1,1]-K) }  # Initialise option prices at maturity



#Compute dependent option values

    for (i in (N:1)) {       

# Explicit compution of option values (backwards)

        for (j in (-Nj+1:Nj-1))  { 

           
Call[j+Nj+1,i]<-as.matrix(pu*(as.matrix(Call[j+Nj,i+1])))+as.matrix(pm*Call[j+Nj+1,i+1])+as.matrix(pd*Call[j+Nj+2,i+1])


                                  }    

Call[1,i]<-Call[2,i]+St[1,1]-St[2,1]  # Upper boundary condition

Call[2*Nj+1,i]<-Call[2*Nj,i]          #Lower boundary condition

}

----- Original Message ----
From: Joseph Khalil <joedtka at yahoo.com>
To: r-sig-finance at stat.math.ethz.ch
Sent: Monday, July 16, 2007 10:01:27 PM
Subject: [R-SIG-Finance] Pricing option using Explicit Finite Difference method

Hi,

Does anyone have R code to calculate option prices (European, plain vanilla) using Explicit Finite
Difference method?   

Thanks,
Joe

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From davidr at rhotrading.com  Tue Jul 17 16:37:08 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 17 Jul 2007 09:37:08 -0500
Subject: [R-SIG-Finance] Pricing option using Explicit Finite
	Differencemethod
In-Reply-To: <514526.98851.qm@web56215.mail.re3.yahoo.com>
References: <514526.98851.qm@web56215.mail.re3.yahoo.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE773F3012@rhopost.rhotrading.com>

Operator precedence strikes again!

> Nj <- 5
> -Nj+1:Nj-1
[1] -5 -4 -3 -2 -1
> (-Nj+1):(Nj-1)
[1] -4 -3 -2 -1  0  1  2  3  4
>

David L. Reiner
Rho Trading Securities, LLC
550 W. Jackson Blvd #1000
Chicago, IL 60661-5704
 
312-244-4610 direct
312-244-4500 main
312-244-4501 fax
 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Joseph
Khalil
Sent: Tuesday, July 17, 2007 8:56 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Pricing option using Explicit Finite
Differencemethod

I am a new user of R and new to this mailing list.         I am using 
the book "Implementing Derivatives Models" by Les Clewlow and Chris
Strickland, and I am trying to simulate

the example on pricing options using the explicit finite difference
method in  chapter 3. I have tried the following to price plain European
calls  but I am getting errors.  





K<-100      #Strike price

T<-1        #Time of maturity

t<-0        #Current time

sigma<-0.2  #Volatility

r<-0.06     #Interest rate

div<-0.03   # Dividend yield

S<-100      #Central stock price

dx<-0.02     #Space step around central asset price (logarithmic
measurement exp(dx))

Nj<-30      #Number of space steps

N<-500      #Number of time steps until T



dt<-T/N # Time step

nu<-r-div-0.5*sigma^2

pu<-0.5*dt*((sigma/dx)^2+nu/dx)      # Probability for go up

pm<-1-dt*(sigma/dx)^2-r*dt           # Probability for stay the same

pd<-0.5*dt*((sigma/dx)^2-nu/dx)      # Probability for go down

# Initialise matrices

St<-matrix(data=0,nrow=2*Nj+1,ncol=1)

Call<-matrix(data=0,nrow=2*Nj+1,ncol=N+1)



#Compute independent values

for (j in (-Nj:Nj)) {

St[Nj-j+1,1]<-S*exp(j*dx)                 # Initialise asset prices at
maturity

Call[Nj-j+1,N+1]<-max(0,St[Nj-j+1,1]-K) }  # Initialise option prices at
maturity



#Compute dependent option values

    for (i in (N:1)) {       

# Explicit compution of option values (backwards)

        for (j in (-Nj+1:Nj-1))  { 

           
Call[j+Nj+1,i]<-as.matrix(pu*(as.matrix(Call[j+Nj,i+1])))+as.matrix(pm*C
all[j+Nj+1,i+1])+as.matrix(pd*Call[j+Nj+2,i+1])


                                  }    

Call[1,i]<-Call[2,i]+St[1,1]-St[2,1]  # Upper boundary condition

Call[2*Nj+1,i]<-Call[2*Nj,i]          #Lower boundary condition

}

----- Original Message ----
From: Joseph Khalil <joedtka at yahoo.com>
To: r-sig-finance at stat.math.ethz.ch
Sent: Monday, July 16, 2007 10:01:27 PM
Subject: [R-SIG-Finance] Pricing option using Explicit Finite Difference
method

Hi,

Does anyone have R code to calculate option prices (European, plain
vanilla) using Explicit Finite
Difference method?   

Thanks,
Joe

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From fabricemcshort at hotmail.com  Tue Jul 17 18:39:54 2007
From: fabricemcshort at hotmail.com (Fabrice McShort)
Date: Tue, 17 Jul 2007 18:39:54 +0200
Subject: [R-SIG-Finance] Multi-asset portfolio VaR
Message-ID: <BAY129-W13934F753255A38F242666CBF90@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070717/7c11ae40/attachment.pl 

From brian at braverock.com  Tue Jul 17 19:13:52 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 17 Jul 2007 12:13:52 -0500
Subject: [R-SIG-Finance] Multi-asset portfolio VaR
In-Reply-To: <BAY129-W13934F753255A38F242666CBF90@phx.gbl>
References: <BAY129-W13934F753255A38F242666CBF90@phx.gbl>
Message-ID: <469CF8D0.6030406@braverock.com>

Fabrice McShort wrote:
> I tried to calculate the VaR of a multi-assets portfolio.
 > I have the historical data of the assets and their weights in the 
portfolio.
 > My impression is that the Performance Analytics Package calculate 
only the
 > VaR of each asset. Not the VaR of the portfolio. But, when I use 
fPortfolio
 > Package, I have the VaR (Historical simulation) of the portfolio. 
Could you
 > confirm my first impression or indicate me how to calculate the
 > VaR of this portfolio with the Performance Analytics Package.
> Thanks for your help.

Fabrice,

You are correct.  The parametric VaR methods in PerformanceAnalytics are 
all univariate.  fPortfolio calculates historical mean-VaR from the 
quantiles of observed returns.  The simplest method to do what you want 
  is to combine functions from the two packages.

Use pfolioReturn() from fPortfolio to construct the return series of 
your historical portfolio.  Use this as the input to the parametric VaR 
calculations in PerformanceAnalytics.  This may be sufficient to answer 
your requirements (it's how I calculate VaR of a historical portfolio in 
most cases.)

Now, a true multivariate parametric estimation is somewhat more 
difficult.  I'll take the case of traditional mean-VaR first, and then 
extend that to the Cornish-Fisher expansion.

In the case of traditional mean-VaR, to make a parametric estimate of 
VaR that is multivariate, you need to construct the return series, 
possibly using some robust estimator so that you get a more robust mean 
to use as the input.  The difficulty lies with the variance.  You may in 
this case wish to scale your observed component returns by the 
historical weights and use one of the multivariate methods to come up 
with a better multivariate covariance measure.  See the Multivariate 
Statistics CRAN Task view here:

http://cran.r-project.org/src/contrib/Views/Multivariate.html

Now, extending this to multivariate skewness and kurtosis is yet more 
complex.  PerformanceAnalytics provides functions for coskewness and 
cokurtosis, but those aren't the actual inputs into the Cornish Fisher 
expansion.  The "moments" package provides slightly more options for 
calculating skewness and kurtosis than those used in 
PerformanceAnalytics and fBasics, but still doesn't provide a true 
multivariate estimate of skewness and kurtosis.  There is quite a lot of 
literature on multivariate skewness and kurtosis, and if you're 
interested, we would certainly welcome collaboration on creating a set 
of functions for providing these in R.

For most purposes, the univariate case seems to be sufficient, but I 
wanted to make sure to cover the issues in the true multivariate case.

Regards,

   - Brian


From fabricemcshort at hotmail.com  Wed Jul 18 10:13:09 2007
From: fabricemcshort at hotmail.com (Fabrice McShort)
Date: Wed, 18 Jul 2007 10:13:09 +0200
Subject: [R-SIG-Finance]  Multi-asset portfolio VaR
Message-ID: <BAY129-W27837B77B7A8C0D7B51E47CBFA0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070718/f7394422/attachment.pl 

From jordi.molins.coronado at gmail.com  Thu Jul 19 14:01:56 2007
From: jordi.molins.coronado at gmail.com (Jordi Molins)
Date: Thu, 19 Jul 2007 14:01:56 +0200
Subject: [R-SIG-Finance] rolling window
Message-ID: <d2b785860707190501h5e9d05c9q3faefde9ed471c81@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070719/d90d22e0/attachment.pl 

From ggrothendieck at gmail.com  Thu Jul 19 14:33:42 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 19 Jul 2007 08:33:42 -0400
Subject: [R-SIG-Finance] rolling window
In-Reply-To: <d2b785860707190501h5e9d05c9q3faefde9ed471c81@mail.gmail.com>
References: <d2b785860707190501h5e9d05c9q3faefde9ed471c81@mail.gmail.com>
Message-ID: <971536df0707190533l663c1c5p8d16cfd26fd14953@mail.gmail.com>

On 7/19/07, Jordi Molins <jordi.molins.coronado at gmail.com> wrote:
> I want to do a rolling window calculation for multivariate data. In other
> words: rollFun in fMultivar makes univariate calculations, ie, given for
> example mydata[1:5], mydata[2:6], ? I get one number (the function applied
> to mydata[1:5]) every time. rollingFunction in PerformanceAnalytics does the
> same but for possibly multivariate data, but one column at a time. rapply in
> zoo also does rolling window for univariate data (at least, all examples are
> with univariate data).

rollapply (which was called rapply in old versions of zoo but was renamed
to avoid collisions with a new function of the same name in the core of R)
in zoo can handle multivariate zoo series either one column at a time (default)
or combined (via by.column = FALSE):

> library(zoo)
> z <- zoo(matrix(1:24, 12))
> z

1   1 13
2   2 14
3   3 15
4   4 16
5   5 17
6   6 18
7   7 19
8   8 20
9   9 21
10 10 22
11 11 23
12 12 24
> rollapply(z, 3, mean, by.column = TRUE) # can omit by.column here

2   2 14
3   3 15
4   4 16
5   5 17
6   6 18
7   7 19
8   8 20
9   9 21
10 10 22
11 11 23
> rollapply(z, 3, mean, by.column = FALSE)
 2  3  4  5  6  7  8  9 10 11
 8  9 10 11 12 13 14 15 16 17


From brian at braverock.com  Thu Jul 19 14:55:19 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 19 Jul 2007 07:55:19 -0500
Subject: [R-SIG-Finance] rolling window
In-Reply-To: <971536df0707190533l663c1c5p8d16cfd26fd14953@mail.gmail.com>
References: <d2b785860707190501h5e9d05c9q3faefde9ed471c81@mail.gmail.com>
	<971536df0707190533l663c1c5p8d16cfd26fd14953@mail.gmail.com>
Message-ID: <469F5F37.5030103@braverock.com>

Gabor Grothendieck wrote:
> On 7/19/07, Jordi Molins <jordi.molins.coronado at gmail.com> wrote:
>> I want to do a rolling window calculation for multivariate data. In other
>> words: rollFun in fMultivar makes univariate calculations, ie, given for
>> example mydata[1:5], mydata[2:6], ? I get one number (the function applied
>> to mydata[1:5]) every time. rollingFunction in PerformanceAnalytics does the
>> same but for possibly multivariate data, but one column at a time. rapply in
>> zoo also does rolling window for univariate data (at least, all examples are
>> with univariate data).
> 
> rollapply (which was called rapply in old versions of zoo but was renamed
> to avoid collisions with a new function of the same name in the core of R)
> in zoo can handle multivariate zoo series either one column at a time (default)
> or combined (via by.column = FALSE):

the parameter '...' in rollingFunction() will allow you to pass in the 
by.column parameter described by Gabor.

   - Brian


From Achim.Zeileis at wu-wien.ac.at  Thu Jul 19 15:31:06 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 19 Jul 2007 15:31:06 +0200 (CEST)
Subject: [R-SIG-Finance] rolling window
In-Reply-To: <971536df0707190533l663c1c5p8d16cfd26fd14953@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0707191517040.7607-100000@disco.wu-wien.ac.at>

As a follow-up to Gabor's comment. A more elaborate illustration might be
a rolling regression, e.g.,

  ## set up multivariate zoo series with
  ## number of UK driver deaths and lags 1 and 12
  seat <- as.zoo(log(UKDriverDeaths))
  time(seat) <- as.yearmon(time(seat))
  seat <- merge(y = seat, y1 = lag(seat, k = -1),
    y12 = lag(seat, k = -12), all = FALSE)

  ## run a rolling regression with a 3-year time window
  ## (similar to a SARIMA(1,0,0)(1,0,0)_12 fitted by OLS)
  fm <- rollapply(seat, width = 36,
    FUN = function(z) coef(lm(y ~ y1 + y12, data = as.data.frame(z))),
    by.column = FALSE, align = "right")

  ## plot the changes in coefficients
  plot(fm)
  ## showing the shifts after the oil crisis in Oct 1973
  ## and after the seatbelt legislation change in Jan 1983

Maybe we should add such an example to the rollaply() man page?

Best,
Z

On Thu, 19 Jul 2007, Gabor Grothendieck wrote:

> On 7/19/07, Jordi Molins <jordi.molins.coronado at gmail.com> wrote:
> > I want to do a rolling window calculation for multivariate data. In other
> > words: rollFun in fMultivar makes univariate calculations, ie, given for
> > example mydata[1:5], mydata[2:6], 
 I get one number (the function applied
> > to mydata[1:5]) every time. rollingFunction in PerformanceAnalytics does the
> > same but for possibly multivariate data, but one column at a time. rapply in
> > zoo also does rolling window for univariate data (at least, all examples are
> > with univariate data).
>
> rollapply (which was called rapply in old versions of zoo but was renamed
> to avoid collisions with a new function of the same name in the core of R)
> in zoo can handle multivariate zoo series either one column at a time (default)
> or combined (via by.column = FALSE):
>
> > library(zoo)
> > z <- zoo(matrix(1:24, 12))
> > z
>
> 1   1 13
> 2   2 14
> 3   3 15
> 4   4 16
> 5   5 17
> 6   6 18
> 7   7 19
> 8   8 20
> 9   9 21
> 10 10 22
> 11 11 23
> 12 12 24
> > rollapply(z, 3, mean, by.column = TRUE) # can omit by.column here
>
> 2   2 14
> 3   3 15
> 4   4 16
> 5   5 17
> 6   6 18
> 7   7 19
> 8   8 20
> 9   9 21
> 10 10 22
> 11 11 23
> > rollapply(z, 3, mean, by.column = FALSE)
>  2  3  4  5  6  7  8  9 10 11
>  8  9 10 11 12 13 14 15 16 17
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From julio.ferreira at reliance.com.br  Thu Jul 19 20:49:43 2007
From: julio.ferreira at reliance.com.br (Julio Ferreira)
Date: Thu, 19 Jul 2007 15:49:43 -0300
Subject: [R-SIG-Finance] problem with fPortfolio
Message-ID: <FBD3BAC024356E4796E7420ED739C4B75093BC@EXCHANGEREL.domainrel.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070719/dca51739/attachment.pl 

From brian at braverock.com  Fri Jul 20 16:58:31 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 20 Jul 2007 09:58:31 -0500
Subject: [R-SIG-Finance] problem with fPortfolio
In-Reply-To: <FBD3BAC024356E4796E7420ED739C4B75093BC@EXCHANGEREL.domainrel.com>
References: <FBD3BAC024356E4796E7420ED739C4B75093BC@EXCHANGEREL.domainrel.com>
Message-ID: <46A0CD97.7020809@braverock.com>

Julio Ferreira wrote:
> While trying to use several functions (e.g.  assetsMeanCov(x,
> method="cov") ) from fPortfolio package, I get error messages stating
> that it is  "unable to find function isPositiveDefinite". I checked and
> apparently all required packages (dependencies) were installed and
> loaded. Looking at the documentations available I was unable to find any
> reference to this function. Has anyone get this problem before? How to
> solve it?

Have you tried calling update.packages() ?

Regards,

   - Brian


From brian at braverock.com  Fri Jul 20 21:03:36 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 20 Jul 2007 14:03:36 -0500
Subject: [R-SIG-Finance] [Fwd: Re:  problem with fPortfolio]
Message-ID: <46A10708.8090405@braverock.com>

Please address responses to the list

-------- Original Message --------
From: Julio Ferreira <julio.ferreira at reliance.com.br>
To: Brian G. Peterson <brian at braverock.com>

Brian,

Yes, I did after I sent the message calling for help. Actually I found 
out that "isPositiveDefinite" is part of fEcofin, which apparently 
should have been downloaded (I used "dependencies=TRUE", when downloaded 
fPortfolio). I tried to download fEcofin from another mirror and it 
solved the problem. Maybe the first downloaded was (somehow) 
"corrupted"????Is it possible ?? the checks seemed to be ok....

Now i?m getting another strange problem: when calling

  >assetsMeanCov(berndtAssets.ts,method="mcd")

I?m getting error message:

Erro em names(mu) = colnames(x) : tentativa de especificar um atributo 
em um NULL

translation: "attempted to specify an attribute to a  NULL"

The problem is that I am just reproducing what was on the fPortfolio 
documentation. Any guess what was going on?

I thank you for your reply, and apologize for "abusing" from you 
"goodwill", but I am learning R by myself, so don?t have no one else to 
ask for help...

-------- End Original Message --------

Julio,

Try with a different data set, or check to make sure that there are no 
NA's in the data you're using to test the functions.  Rmetrics is a set 
of functions and data which have accumulated over several years, and 
fEcofin is a relative newcomer to the package set.  In some cases, this 
means that examples in documentation, which are not automatically 
checked on package build, may no longer work properly.

In the specific case you cite above, it looks like your input data may 
not have had named columns.

I'm not sure that's the case though, because when I try with a different 
data set which I know is named and symetric in length of all columns, I 
get a similar error to the one you describe:

 > data(edhec)
 > assetsMeanCov(edhec,method="mcd")

Error in `names<-.default`(`*tmp*`, value = c("Convertible.Arbitrage", 
:        attempt to set an attribute on NULL

When I try with a different data set with timeseries of differing 
lengths, I get a different error:

 > data(managers)
 > assetsMeanCov(managers,method="mcd")
Error in MASS::cov.rob(x, method = "mcd") :
         missing or infinite values are not allowed

When I try with a different method parameter, I get the error you 
reported initially.

 > assetsMeanCov(edhec,method="cov")
Error in assetsMeanCov(edhec, method = "cov") :
         could not find function "isPositiveDefinite"

So, I'm suspecting that there may be a bug in the naming section (or 
elsewhere) in the assetsMeanCov function.

Perhaps you should try the more primitive functions mean() and cov() 
before working with wrapper functions.

Regards,

   - Brian


From julio.ferreira at reliance.com.br  Fri Jul 20 22:28:07 2007
From: julio.ferreira at reliance.com.br (Julio Ferreira)
Date: Fri, 20 Jul 2007 17:28:07 -0300
Subject: [R-SIG-Finance] RES: [Fwd: Re:  problem with fPortfolio]
Message-ID: <FBD3BAC024356E4796E7420ED739C4B7509740@EXCHANGEREL.domainrel.com>

Brian,

Checking the code for assetsMeanCov, I found out that when method="mcd"
the code does this:


else if (method == "mve") {
        ans = MASS::cov.rob(x, method = "mve")
        mu = ans$mu
        Sigma = ans$Omega
    }
    else if (method == "mcd") {
        ans = MASS::cov.rob(x, method = "mcd")
        mu = ans$mu
        Sigma = ans$Omega


When the correct should be this:

else if (method == "mve") {
        ans = MASS::cov.rob(x, method = "mve")
        mu = ans$center
        Sigma = ans$cov
    }
    else if (method == "mcd") {
        ans = MASS::cov.rob(x, method = "mcd")
        mu = ans$center
        Sigma = ans$cov

As the cov.rob function (package MASS) returns $center(instead of $mu) and $cov (instead of $Omega), the objects mu and Omega were NULL, and then the error message makes sense ("trying to attribute to a NULL")
In the case of the other methods choices, the code is correct.
I am a newcomer to R, so would you pls indicate how should I warn the mantainer to get this code fixed on the package??
Rgds




-----Mensagem original-----
De: Brian G. Peterson [mailto:brian at braverock.com] 
Enviada em: sexta-feira, 20 de julho de 2007 16:04
Para: R-SIG-Finance
Cc: Julio Ferreira
Assunto: [Fwd: Re: [R-SIG-Finance] problem with fPortfolio]

Please address responses to the list

-------- Original Message --------
From: Julio Ferreira <julio.ferreira at reliance.com.br>
To: Brian G. Peterson <brian at braverock.com>

Brian,

Yes, I did after I sent the message calling for help. Actually I found 
out that "isPositiveDefinite" is part of fEcofin, which apparently 
should have been downloaded (I used "dependencies=TRUE", when downloaded 
fPortfolio). I tried to download fEcofin from another mirror and it 
solved the problem. Maybe the first downloaded was (somehow) 
"corrupted"????Is it possible ?? the checks seemed to be ok....

Now i?m getting another strange problem: when calling

  >assetsMeanCov(berndtAssets.ts,method="mcd")

I?m getting error message:

Erro em names(mu) = colnames(x) : tentativa de especificar um atributo 
em um NULL

translation: "attempted to specify an attribute to a  NULL"

The problem is that I am just reproducing what was on the fPortfolio 
documentation. Any guess what was going on?

I thank you for your reply, and apologize for "abusing" from you 
"goodwill", but I am learning R by myself, so don?t have no one else to 
ask for help...

-------- End Original Message --------

Julio,

Try with a different data set, or check to make sure that there are no 
NA's in the data you're using to test the functions.  Rmetrics is a set 
of functions and data which have accumulated over several years, and 
fEcofin is a relative newcomer to the package set.  In some cases, this 
means that examples in documentation, which are not automatically 
checked on package build, may no longer work properly.

In the specific case you cite above, it looks like your input data may 
not have had named columns.

I'm not sure that's the case though, because when I try with a different 
data set which I know is named and symetric in length of all columns, I 
get a similar error to the one you describe:

 > data(edhec)
 > assetsMeanCov(edhec,method="mcd")

Error in `names<-.default`(`*tmp*`, value = c("Convertible.Arbitrage", 
:        attempt to set an attribute on NULL

When I try with a different data set with timeseries of differing 
lengths, I get a different error:

 > data(managers)
 > assetsMeanCov(managers,method="mcd")
Error in MASS::cov.rob(x, method = "mcd") :
         missing or infinite values are not allowed

When I try with a different method parameter, I get the error you 
reported initially.

 > assetsMeanCov(edhec,method="cov")
Error in assetsMeanCov(edhec, method = "cov") :
         could not find function "isPositiveDefinite"

So, I'm suspecting that there may be a bug in the naming section (or 
elsewhere) in the assetsMeanCov function.

Perhaps you should try the more primitive functions mean() and cov() 
before working with wrapper functions.

Regards,

   - Brian


From julio.ferreira at reliance.com.br  Fri Jul 20 23:00:34 2007
From: julio.ferreira at reliance.com.br (Julio Ferreira)
Date: Fri, 20 Jul 2007 18:00:34 -0300
Subject: [R-SIG-Finance] installation of old verisons of fPortfolio
Message-ID: <FBD3BAC024356E4796E7420ED739C4B750976C@EXCHANGEREL.domainrel.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070720/545e9156/attachment.pl 

From brian at braverock.com  Fri Jul 20 23:35:15 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 20 Jul 2007 16:35:15 -0500
Subject: [R-SIG-Finance] installation of old verisons of fPortfolio
In-Reply-To: <FBD3BAC024356E4796E7420ED739C4B750976C@EXCHANGEREL.domainrel.com>
References: <FBD3BAC024356E4796E7420ED739C4B750976C@EXCHANGEREL.domainrel.com>
Message-ID: <46A12A93.9090303@braverock.com>

Julio Ferreira wrote:
> I want to install an old version of package fPortfolio, which is
> available on Cran-Packages-archives as a .tar.gz file. 
> 
> Is it possible?
> 
> How should I proceed to avoid conflicts with the current version of
> fPortfolio that I have already installed?

In Windows, I have no idea how it's done but I'm sure it's possible.

In *nix, you would:

R CMD INSTALL somepackage.tar.gz

from a shell.

Regards,

   - Brian


From julio.ferreira at reliance.com.br  Fri Jul 20 23:40:15 2007
From: julio.ferreira at reliance.com.br (Julio Ferreira)
Date: Fri, 20 Jul 2007 18:40:15 -0300
Subject: [R-SIG-Finance] RES:  installation of old verisons of fPortfolio
Message-ID: <FBD3BAC024356E4796E7420ED739C4B7509797@EXCHANGEREL.domainrel.com>

Supposing i discover how to do it on windows, should I rename it first
(to avoid conflicts resulting of two packeages with the same name)??
rgds

-----Mensagem original-----
De: Brian G. Peterson [mailto:brian at braverock.com] 
Enviada em: sexta-feira, 20 de julho de 2007 18:35
Para: Julio Ferreira
Cc: r-sig-finance at stat.math.ethz.ch
Assunto: Re: [R-SIG-Finance] installation of old verisons of fPortfolio

Julio Ferreira wrote:
> I want to install an old version of package fPortfolio, which is
> available on Cran-Packages-archives as a .tar.gz file. 
> 
> Is it possible?
> 
> How should I proceed to avoid conflicts with the current version of
> fPortfolio that I have already installed?

In Windows, I have no idea how it's done but I'm sure it's possible.

In *nix, you would:

R CMD INSTALL somepackage.tar.gz

from a shell.

Regards,

   - Brian


From maechler at stat.math.ethz.ch  Sat Jul 21 13:31:47 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 21 Jul 2007 13:31:47 +0200
Subject: [R-SIG-Finance] RES: installation of old verisons of fPortfolio
In-Reply-To: <FBD3BAC024356E4796E7420ED739C4B7509797@EXCHANGEREL.domainrel.com>
References: <FBD3BAC024356E4796E7420ED739C4B7509797@EXCHANGEREL.domainrel.com>
Message-ID: <18081.61091.586506.798@stat.math.ethz.ch>

>>>>> "JF" == Julio Ferreira <julio.ferreira at reliance.com.br>
>>>>>     on Fri, 20 Jul 2007 18:40:15 -0300 writes:

    JF> Supposing i discover how to do it on windows, should I
    JF> rename it first (to avoid conflicts resulting of two
    JF> packeages with the same name)??  rgds

Well, you 'd have to first unpack the zip file,
rename the resulting directory and zip it again, then "install
from local Zip file"; but I guess that still won't work.

Instead, do install your 'fPortfolio' package into a different
*library* (a Windows folder that you'd typically create first)
than the one the other 'fPortfolio' is in.

Read the help pages    
     ?.libPaths
and  ?library

and then use one of them, probably

  library(fPortfolio, lib = ".. your othter library directory")

Regards,
Martin


    JF> -----Mensagem original----- De: Brian G. Peterson
    JF> [mailto:brian at braverock.com] Enviada em: sexta-feira, 20
    JF> de julho de 2007 18:35 Para: Julio Ferreira Cc:
    JF> r-sig-finance at stat.math.ethz.ch Assunto: Re:
    JF> [R-SIG-Finance] installation of old verisons of
    JF> fPortfolio

    JF> Julio Ferreira wrote:
    >> I want to install an old version of package fPortfolio,
    >> which is available on Cran-Packages-archives as a .tar.gz
    >> file.
    >> 
    >> Is it possible?
    >> 
    >> How should I proceed to avoid conflicts with the current
    >> version of fPortfolio that I have already installed?

    JF> In Windows, I have no idea how it's done but I'm sure
    JF> it's possible.

    JF> In *nix, you would:

    JF> R CMD INSTALL somepackage.tar.gz

    JF> from a shell.

    JF> Regards,

    JF>    - Brian

    JF> _______________________________________________
    JF> R-SIG-Finance at stat.math.ethz.ch mailing list
    JF> https://stat.ethz.ch/mailman/listinfo/r-sig-finance --
    JF> Subscriber-posting only.  -- If you want to post,
    JF> subscribe first.


From icos.atropa at gmail.com  Sun Jul 22 14:47:44 2007
From: icos.atropa at gmail.com (icosa atropa)
Date: Sun, 22 Jul 2007 06:47:44 -0600
Subject: [R-SIG-Finance] Zoo NA handling documentation?
Message-ID: <681d07c20707220547g4aaaa3f9jf81502ae8b36f757@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070722/a1106c4f/attachment.pl 

From ggrothendieck at gmail.com  Sun Jul 22 15:22:02 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 22 Jul 2007 09:22:02 -0400
Subject: [R-SIG-Finance] Zoo NA handling documentation?
In-Reply-To: <681d07c20707220547g4aaaa3f9jf81502ae8b36f757@mail.gmail.com>
References: <681d07c20707220547g4aaaa3f9jf81502ae8b36f757@mail.gmail.com>
Message-ID: <971536df0707220622p1b867f7br7abf77cbc41e793d@mail.gmail.com>

There are some example here:

https://stat.ethz.ch/pipermail/r-sig-finance/2006q2/


On 7/22/07, icosa atropa <icos.atropa at gmail.com> wrote:
> Thank you so much for your previous help with zoo and lattice.  Following
> your guidance, I've found the zoo package very powerful.  The rollapply
> demos were also illuminating!
>
> I'm wondering if I've found some inconsistency with zoo documentation r.e.
> NA handling.
> Both na.omit and na.contiguous are mentioned as methods in the current CRAN
> pdf reference documentation.  The hyperlink for na.omit on p18 lead to p1,
> and there are no function defs for either in the docs.  Similarly, both
> ?na.omit and ?na.contiguous lead to function defs.  Are these unimplemented
> as generic functions?  I'm a bit confused as to the proper context in which
> to use na.omit and na.contiguous.
>
> best,
> christian
>
> On 7/8/07, Gabor Grothendieck <ggrothendieck at gmail.com > wrote:
> > Try this (and see ?xyplot.zoo for more info):
> >
> > # first run code in your post that creates the experiment object
> >
> > # create a time series from experiment
> > z <- zoo(matrix(experiment$meas, 25, byrow = TRUE))
> > colnames(z) <- c("NC", "NT", "SC", "ST")
> >
> > # add grid to existing plot
> > addGrid <- function() for(i in 1:2) {
> >   trellis.focus("panel", i, 1); panel.grid(); trellis.unfocus()
> > }
> >
> > zz <- z
> > colnames(zz) <- c("N", "NT", "S", "ST")
> > xyplot(zz, screens = c(1, 1, 2, 2), col = 1:2, layout = 2:1, type = "b")
> > addGrid()
> >
> > zz <- z
> > colnames(zz) <- c("C", "T", "SC", "ST")
> > xyplot(zz, screens = 1:2, col = c(1, 1, 2, 2), layout = 2:1, type = "b")
> > addGrid()
> >
> >
> > On 7/8/07, icosa atropa <icos.atropa at gmail.com> wrote:
> > > > Please provide something reproducible and
> > > > show what you want to do without zoo.
> > >
> > > The following example is contrived, but closely resembles my data.
> > >
> > > I pull my data from a database using RODBC - it comes in as a data.frame
> > > with factors, timestamp, and data all globbed together.  This is very
> > > convenient for plotting with lattice, but complicates time series
> analysis.
> > > I can't turn the object into a zoo directly due to the mixed
> factor/data.
> > > I've looked at fCalendar, whose timeSeries object has an recordtID
> slot...
> > >
> > >
> > > # Everything from here down tested with R 2.5.0
> > >
> > > experiment.layout = expand.grid( tx=c('control', 'treatment'), rep=c(
> 'N',
> > > 'S'), time=1:25)
> > > # ^^ block design experiment
> > >
> > > experiment = data.frame( experiment.layout, meas=rnorm(100) )
> > > # ^^the time-indexed dataframe of observations by
> > >
> > > experiment$meas=experiment$meas + c(1,-1,2,-2)
> > > # ^^experimental effects differ by bloco and treatment
> > >
> > > summary(experiment)
> > >
> > > require(lattice)
> > > #
> > > #First split condition on rep, grouping by tx
> > > #
> > > xyplot(meas ~ time | rep, data=experiment, groups=tx,
> > >                 panel=function(x, y, subscripts, groups) {
> > >                         panel.grid()
> > >                         panel.superpose(x, y, subscripts, groups)
> > >                  }
> > > )
> > >
> > > #
> > > #Next condition on tx, grouping by experiment
> > > #
> > > xyplot(meas ~ time | tx, data=experiment, groups=rep,
> > >                 panel=function(x, y, subscripts, groups) {
> > >                         panel.grid()
> > >                         panel.superpose(x, y, subscripts, groups)
> > >                  }
> > > )
> > >
> > > thanks again for your time!
> > > christian
> > >
> >
>
>
>
> --
> Far better an approximate answer to the right question, which is often
> vague, than the exact answer to the wrong question, which can always be made
> precise -- j.w. tukey


From ggrothendieck at gmail.com  Sun Jul 22 15:23:17 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 22 Jul 2007 09:23:17 -0400
Subject: [R-SIG-Finance] Zoo NA handling documentation?
In-Reply-To: <971536df0707220622p1b867f7br7abf77cbc41e793d@mail.gmail.com>
References: <681d07c20707220547g4aaaa3f9jf81502ae8b36f757@mail.gmail.com>
	<971536df0707220622p1b867f7br7abf77cbc41e793d@mail.gmail.com>
Message-ID: <971536df0707220623y6c2b8d91m13e1397a9aa15e67@mail.gmail.com>

Sorry, bad link.  Here it is again.

https://stat.ethz.ch/pipermail/r-sig-finance/2006q2/000777.html

On 7/22/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> There are some example here:
>
> https://stat.ethz.ch/pipermail/r-sig-finance/2006q2/
>
>
> On 7/22/07, icosa atropa <icos.atropa at gmail.com> wrote:
> > Thank you so much for your previous help with zoo and lattice.  Following
> > your guidance, I've found the zoo package very powerful.  The rollapply
> > demos were also illuminating!
> >
> > I'm wondering if I've found some inconsistency with zoo documentation r.e.
> > NA handling.
> > Both na.omit and na.contiguous are mentioned as methods in the current CRAN
> > pdf reference documentation.  The hyperlink for na.omit on p18 lead to p1,
> > and there are no function defs for either in the docs.  Similarly, both
> > ?na.omit and ?na.contiguous lead to function defs.  Are these unimplemented
> > as generic functions?  I'm a bit confused as to the proper context in which
> > to use na.omit and na.contiguous.
> >
> > best,
> > christian
> >
> > On 7/8/07, Gabor Grothendieck <ggrothendieck at gmail.com > wrote:
> > > Try this (and see ?xyplot.zoo for more info):
> > >
> > > # first run code in your post that creates the experiment object
> > >
> > > # create a time series from experiment
> > > z <- zoo(matrix(experiment$meas, 25, byrow = TRUE))
> > > colnames(z) <- c("NC", "NT", "SC", "ST")
> > >
> > > # add grid to existing plot
> > > addGrid <- function() for(i in 1:2) {
> > >   trellis.focus("panel", i, 1); panel.grid(); trellis.unfocus()
> > > }
> > >
> > > zz <- z
> > > colnames(zz) <- c("N", "NT", "S", "ST")
> > > xyplot(zz, screens = c(1, 1, 2, 2), col = 1:2, layout = 2:1, type = "b")
> > > addGrid()
> > >
> > > zz <- z
> > > colnames(zz) <- c("C", "T", "SC", "ST")
> > > xyplot(zz, screens = 1:2, col = c(1, 1, 2, 2), layout = 2:1, type = "b")
> > > addGrid()
> > >
> > >
> > > On 7/8/07, icosa atropa <icos.atropa at gmail.com> wrote:
> > > > > Please provide something reproducible and
> > > > > show what you want to do without zoo.
> > > >
> > > > The following example is contrived, but closely resembles my data.
> > > >
> > > > I pull my data from a database using RODBC - it comes in as a data.frame
> > > > with factors, timestamp, and data all globbed together.  This is very
> > > > convenient for plotting with lattice, but complicates time series
> > analysis.
> > > > I can't turn the object into a zoo directly due to the mixed
> > factor/data.
> > > > I've looked at fCalendar, whose timeSeries object has an recordtID
> > slot...
> > > >
> > > >
> > > > # Everything from here down tested with R 2.5.0
> > > >
> > > > experiment.layout = expand.grid( tx=c('control', 'treatment'), rep=c(
> > 'N',
> > > > 'S'), time=1:25)
> > > > # ^^ block design experiment
> > > >
> > > > experiment = data.frame( experiment.layout, meas=rnorm(100) )
> > > > # ^^the time-indexed dataframe of observations by
> > > >
> > > > experiment$meas=experiment$meas + c(1,-1,2,-2)
> > > > # ^^experimental effects differ by bloco and treatment
> > > >
> > > > summary(experiment)
> > > >
> > > > require(lattice)
> > > > #
> > > > #First split condition on rep, grouping by tx
> > > > #
> > > > xyplot(meas ~ time | rep, data=experiment, groups=tx,
> > > >                 panel=function(x, y, subscripts, groups) {
> > > >                         panel.grid()
> > > >                         panel.superpose(x, y, subscripts, groups)
> > > >                  }
> > > > )
> > > >
> > > > #
> > > > #Next condition on tx, grouping by experiment
> > > > #
> > > > xyplot(meas ~ time | tx, data=experiment, groups=rep,
> > > >                 panel=function(x, y, subscripts, groups) {
> > > >                         panel.grid()
> > > >                         panel.superpose(x, y, subscripts, groups)
> > > >                  }
> > > > )
> > > >
> > > > thanks again for your time!
> > > > christian
> > > >
> > >
> >
> >
> >
> > --
> > Far better an approximate answer to the right question, which is often
> > vague, than the exact answer to the wrong question, which can always be made
> > precise -- j.w. tukey
>


From ggrothendieck at gmail.com  Sun Jul 22 15:44:55 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 22 Jul 2007 09:44:55 -0400
Subject: [R-SIG-Finance] Zoo NA handling documentation?
In-Reply-To: <971536df0707220623y6c2b8d91m13e1397a9aa15e67@mail.gmail.com>
References: <681d07c20707220547g4aaaa3f9jf81502ae8b36f757@mail.gmail.com>
	<971536df0707220622p1b867f7br7abf77cbc41e793d@mail.gmail.com>
	<971536df0707220623y6c2b8d91m13e1397a9aa15e67@mail.gmail.com>
Message-ID: <971536df0707220644n7a7ad9e3sc578ac2c7971733f@mail.gmail.com>

One other comment.  In addition to the 5 NA handling routines
for zoo objects illustrated in the link below there is also a
6th NA handling routine in the stinepack package:

library(stinepack)
?na.stinterp

On 7/22/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Sorry, bad link.  Here it is again.
>
> https://stat.ethz.ch/pipermail/r-sig-finance/2006q2/000777.html
>
> On 7/22/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > There are some example here:
> >
> > https://stat.ethz.ch/pipermail/r-sig-finance/2006q2/
> >
> >
> > On 7/22/07, icosa atropa <icos.atropa at gmail.com> wrote:
> > > Thank you so much for your previous help with zoo and lattice.  Following
> > > your guidance, I've found the zoo package very powerful.  The rollapply
> > > demos were also illuminating!
> > >
> > > I'm wondering if I've found some inconsistency with zoo documentation r.e.
> > > NA handling.
> > > Both na.omit and na.contiguous are mentioned as methods in the current CRAN
> > > pdf reference documentation.  The hyperlink for na.omit on p18 lead to p1,
> > > and there are no function defs for either in the docs.  Similarly, both
> > > ?na.omit and ?na.contiguous lead to function defs.  Are these unimplemented
> > > as generic functions?  I'm a bit confused as to the proper context in which
> > > to use na.omit and na.contiguous.
> > >
> > > best,
> > > christian
> > >
> > > On 7/8/07, Gabor Grothendieck <ggrothendieck at gmail.com > wrote:
> > > > Try this (and see ?xyplot.zoo for more info):
> > > >
> > > > # first run code in your post that creates the experiment object
> > > >
> > > > # create a time series from experiment
> > > > z <- zoo(matrix(experiment$meas, 25, byrow = TRUE))
> > > > colnames(z) <- c("NC", "NT", "SC", "ST")
> > > >
> > > > # add grid to existing plot
> > > > addGrid <- function() for(i in 1:2) {
> > > >   trellis.focus("panel", i, 1); panel.grid(); trellis.unfocus()
> > > > }
> > > >
> > > > zz <- z
> > > > colnames(zz) <- c("N", "NT", "S", "ST")
> > > > xyplot(zz, screens = c(1, 1, 2, 2), col = 1:2, layout = 2:1, type = "b")
> > > > addGrid()
> > > >
> > > > zz <- z
> > > > colnames(zz) <- c("C", "T", "SC", "ST")
> > > > xyplot(zz, screens = 1:2, col = c(1, 1, 2, 2), layout = 2:1, type = "b")
> > > > addGrid()
> > > >
> > > >
> > > > On 7/8/07, icosa atropa <icos.atropa at gmail.com> wrote:
> > > > > > Please provide something reproducible and
> > > > > > show what you want to do without zoo.
> > > > >
> > > > > The following example is contrived, but closely resembles my data.
> > > > >
> > > > > I pull my data from a database using RODBC - it comes in as a data.frame
> > > > > with factors, timestamp, and data all globbed together.  This is very
> > > > > convenient for plotting with lattice, but complicates time series
> > > analysis.
> > > > > I can't turn the object into a zoo directly due to the mixed
> > > factor/data.
> > > > > I've looked at fCalendar, whose timeSeries object has an recordtID
> > > slot...
> > > > >
> > > > >
> > > > > # Everything from here down tested with R 2.5.0
> > > > >
> > > > > experiment.layout = expand.grid( tx=c('control', 'treatment'), rep=c(
> > > 'N',
> > > > > 'S'), time=1:25)
> > > > > # ^^ block design experiment
> > > > >
> > > > > experiment = data.frame( experiment.layout, meas=rnorm(100) )
> > > > > # ^^the time-indexed dataframe of observations by
> > > > >
> > > > > experiment$meas=experiment$meas + c(1,-1,2,-2)
> > > > > # ^^experimental effects differ by bloco and treatment
> > > > >
> > > > > summary(experiment)
> > > > >
> > > > > require(lattice)
> > > > > #
> > > > > #First split condition on rep, grouping by tx
> > > > > #
> > > > > xyplot(meas ~ time | rep, data=experiment, groups=tx,
> > > > >                 panel=function(x, y, subscripts, groups) {
> > > > >                         panel.grid()
> > > > >                         panel.superpose(x, y, subscripts, groups)
> > > > >                  }
> > > > > )
> > > > >
> > > > > #
> > > > > #Next condition on tx, grouping by experiment
> > > > > #
> > > > > xyplot(meas ~ time | tx, data=experiment, groups=rep,
> > > > >                 panel=function(x, y, subscripts, groups) {
> > > > >                         panel.grid()
> > > > >                         panel.superpose(x, y, subscripts, groups)
> > > > >                  }
> > > > > )
> > > > >
> > > > > thanks again for your time!
> > > > > christian
> > > > >
> > > >
> > >
> > >
> > >
> > > --
> > > Far better an approximate answer to the right question, which is often
> > > vague, than the exact answer to the wrong question, which can always be made
> > > precise -- j.w. tukey
> >
>


From ggrothendieck at gmail.com  Sun Jul 22 15:58:25 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 22 Jul 2007 09:58:25 -0400
Subject: [R-SIG-Finance] Zoo NA handling documentation?
In-Reply-To: <971536df0707220644n7a7ad9e3sc578ac2c7971733f@mail.gmail.com>
References: <681d07c20707220547g4aaaa3f9jf81502ae8b36f757@mail.gmail.com>
	<971536df0707220622p1b867f7br7abf77cbc41e793d@mail.gmail.com>
	<971536df0707220623y6c2b8d91m13e1397a9aa15e67@mail.gmail.com>
	<971536df0707220644n7a7ad9e3sc578ac2c7971733f@mail.gmail.com>
Message-ID: <971536df0707220658u3b88b52dy33c57bcff79cde5f@mail.gmail.com>

Hmm.  In fact, there is yet another one.  We will have to bring all this
together properly but for now here is a summary.  (My previous posts
did not include na.spline.) Note that there is a help page in zoo for
?na.approx and ?na.spline  and in stinepackfor ?na.stinterp .

library(zoo)
zz <- zoo(c(NA, NA, 1:2, NA, 3:5, NA, 6, NA))
na.approx(zz)  # linear interpolation
na.contiguous(zz)  # longest stretch without NAs
na.locf(zz)   # last occurence (that is not NA) carried forward
na.omit(zz)  # omit the NAs
na.spline(zz) # spline interpolation
na.trim(zz)  # remove NAs off beginning and end but not interior ones
library(stinepack)
na.stinterp(zz) # stineman interpolation

> library(zoo)
> zz <- zoo(c(NA, NA, 1:2, NA, 3:5, NA, 6, NA))
> na.approx(zz)  # linear interpolation
  3   4   5   6   7   8   9  10
1.0 2.0 2.5 3.0 4.0 5.0 5.5 6.0
> na.contiguous(zz)  # longest stretch without NAs
6 7 8
3 4 5
> na.locf(zz)   # last occurence (that is not NA) carried forward
 3  4  5  6  7  8  9 10 11
 1  2  2  3  4  5  5  6  6
> na.omit(zz)  # omit the NAs
 3  4  6  7  8 10
 1  2  3  4  5  6
> na.spline(zz) # spline interpolation
        1         2         3         4         5         6         7
       8         9        10        11
-4.676471 -1.058824  1.000000  2.000000  2.463235  3.000000  4.000000
5.000000  5.742647  6.000000  5.522059
> na.trim(zz)  # remove NAs off beginning and end but not interior ones
 3  4  5  6  7  8  9 10
 1  2 NA  3  4  5 NA  6
> library(stinepack)
> na.stinterp(zz) # stineman interpolation
       3        4        5        6        7        8        9       10
1.000000 2.000000 2.500000 3.000000 4.000000 5.000000 5.600269 6.000000


From patrick at burns-stat.com  Tue Jul 24 11:33:40 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 24 Jul 2007 10:33:40 +0100
Subject: [R-SIG-Finance] [OT] Engle lecture video
Message-ID: <46A5C774.4050905@burns-stat.com>

Somewhat off-topic but probably of interest to a
reasonable fraction of the list.  The Financial Times
has a series of short videos of Rob Engle talking about
volatility this week.  A new one comes out each day.

http://www.ft.com/engle

(The video works on Internet Explorer, results may
vary with other browsers.)

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


From johnputz3655 at yahoo.com  Tue Jul 24 17:00:51 2007
From: johnputz3655 at yahoo.com (John Putz)
Date: Tue, 24 Jul 2007 08:00:51 -0700 (PDT)
Subject: [R-SIG-Finance]  Energy Industry Groups?
Message-ID: <74653.1102.qm@web50704.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070724/f078f6bc/attachment.pl 

From brian at braverock.com  Tue Jul 24 17:15:03 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 24 Jul 2007 10:15:03 -0500
Subject: [R-SIG-Finance] Energy Industry Groups?
In-Reply-To: <74653.1102.qm@web50704.mail.re2.yahoo.com>
References: <74653.1102.qm@web50704.mail.re2.yahoo.com>
Message-ID: <46A61777.9010900@braverock.com>

John Putz wrote:
> Do you know if there're any energy-industry-specific groups for discussing energy related topics in Rmetrics?  I'm currently interested in options for modeling a (gas-fired) power plant as a spread option.

I think this is it.  The (vocal) R finance community is small enough 
that having it all in one place seems to work well.  Also, applications 
to one asset class are generally usable on other asset classes as well, 
so I think we all benefit from the cross-pollination of ideas and code.

So, please feel free to describe your particular application and 
implementation issues (and successes!) here.

Regards,

    - Brian


From adrian_d at eskimo.com  Tue Jul 24 17:16:12 2007
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Tue, 24 Jul 2007 08:16:12 -0700 (PDT)
Subject: [R-SIG-Finance] Energy Industry Groups?
In-Reply-To: <74653.1102.qm@web50704.mail.re2.yahoo.com>
References: <74653.1102.qm@web50704.mail.re2.yahoo.com>
Message-ID: <Pine.SUN.4.58.0707240809080.16304@eskimo.com>



John,

I don't know of any energy related group.  Common methods for pricing
spread options you can find in Haug's book.  You can also check Carmona's
papers, he has an enhancement to what you'll find in Haug.  However, a
blind application of out of the box methods to price optionality of power
plants can give you numbers that will be misleading.

Best,
Adrian




On Tue, 24 Jul 2007, John Putz wrote:

> Hello,
>
> Do you know if there're any energy-industry-specific groups for discussing energy related topics in Rmetrics?  I'm currently interested in options for modeling a (gas-fired) power plant as a spread option.
>
> Thank, John.
>
> email: johnputz3655 at yahoo.com
> home: 206-632-6522
> cell: 206-910-5229
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From samkemp at predictedmarkets.com  Tue Jul 24 13:34:33 2007
From: samkemp at predictedmarkets.com (Samuel Kemp)
Date: Tue, 24 Jul 2007 12:34:33 +0100
Subject: [R-SIG-Finance] Problems collecting Intraday data
Message-ID: <200707241134.l6OBYbjI001816@hypatia.math.ethz.ch>

Hi,

I have been using the blpGetData() function to fetch intraday High and Low
data (10min intervals) i.e.

conn <- blpConnect()
BBCode <- 'VGU7 Index'   # Eurostoxx 50 Futures Sept 07 Contract
Date <- "06/28/07"       # which day
Open <- "07:00:00"	 # from time
Close <- "21:00:00"	 # to time
Interval <- 10		 # time interval between values
dat   <- blpGetData(conn, BBCode, c('LAST_PRICE'),start=as.chron(Date,
Open), end=as.chron(Date,Close),barfields=c('HIGH','LOW'), barsize=Interval)
blpDisconnect(conn)

However, sometimes it works and then sometimes (using exactly the same
inputs) it does not - I get the following error...

Error in substr(x, as.integer(start), as.integer(stop)) : 
        invalid substring argument(s) in substr()

Does anyone have a solution to this problem and/or explanation?

Many thanks in advance,

Sam

p.s. I am not sure if it is relevant but my computer is running XP.


____________________________________________
Samuel Kemp
Predicted Markets Ltd.
t?? +44(0)1428 729743
m?? +44(0)7880 740220
e?? samkemp at predictedmarkets.com  
w?? www.predictedmarkets.com  

This message is intended solely for the individual addressee named above. If
you are not the intended recipient, please delete. The information contained
within this e-mail is confidential and may be legally privileged. Messages
sent by e-mail may be subject to delays, non-delivery and un-authorised
alteration. The information contained herein is from sources thought to be
reliable but we do not represent that it is accurate or complete. Therefore,
Predicted Markets Limited and its staff cannot be held responsible or liable
for the contents of this message. This is not an offer or solicitation to
buy and sell any instrument, security or investment. Registered in England
No. 5851947. Predicted Markets Limited, Ladybrook, Burgh Hill, Bramshott,
Hants GU30 7RQ is Authorised and Regulated by the Financial Services
Authority.


From topkatz at msn.com  Wed Jul 25 19:26:36 2007
From: topkatz at msn.com (Talbot Katz)
Date: Wed, 25 Jul 2007 13:26:36 -0400
Subject: [R-SIG-Finance] fCopulae Availability?
Message-ID: <BAY108-F40C3960F94FD6619DB18F3AAF10@phx.gbl>

Hi.

I see the fCopulae package listed on the Contributed Packages page of the 
CRAN website (http://cran.cnr.berkeley.edu/); however, fCopulae doesn't 
appear to be available to download from the two CRAN ftp sites I have 
visited.  Is it actually available for use in R, and if so, what is the best 
way to obtain and load it?  (This is not an emergency, I'm just curious.)  
Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From ngottlieb at marinercapital.com  Thu Jul 26 19:09:24 2007
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Thu, 26 Jul 2007 13:09:24 -0400
Subject: [R-SIG-Finance] Problems collecting Intraday data
In-Reply-To: <200707241134.l6OBYbjI001816@hypatia.math.ethz.ch>
References: <200707241134.l6OBYbjI001816@hypatia.math.ethz.ch>
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D88F3892D@500MAIL.goldbox.com>

Sam:

Suggest you try using the BLP function in Excel and see what data is coming back.

Most likely some bad data so the function substr is throwing 
an exception. 

Could happen if value in x is null, empty and the thus subscripts for substr, 
as.integer(start) or as.integer(stop), are negative, zero or stop < start.

See what comes back in Excel with BLP call to Bloomberg.
Perhaps even write some VBA code in Excel to test substr
function of values retrieved.

Enjoy debugging! :)

Neil 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Samuel Kemp
Sent: Tuesday, July 24, 2007 7:35 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Problems collecting Intraday data

Hi,

I have been using the blpGetData() function to fetch intraday High and Low data (10min intervals) i.e.

conn <- blpConnect()
BBCode <- 'VGU7 Index'   # Eurostoxx 50 Futures Sept 07 Contract
Date <- "06/28/07"       # which day
Open <- "07:00:00"	 # from time
Close <- "21:00:00"	 # to time
Interval <- 10		 # time interval between values
dat   <- blpGetData(conn, BBCode, c('LAST_PRICE'),start=as.chron(Date,
Open), end=as.chron(Date,Close),barfields=c('HIGH','LOW'), barsize=Interval)
blpDisconnect(conn)

However, sometimes it works and then sometimes (using exactly the same
inputs) it does not - I get the following error...

Error in substr(x, as.integer(start), as.integer(stop)) : 
        invalid substring argument(s) in substr()

Does anyone have a solution to this problem and/or explanation?

Many thanks in advance,

Sam

p.s. I am not sure if it is relevant but my computer is running XP.


____________________________________________
Samuel Kemp
Predicted Markets Ltd.
t?  +44(0)1428 729743
m?  +44(0)7880 740220
e?  samkemp at predictedmarkets.com
w?  www.predictedmarkets.com  

This message is intended solely for the individual addressee named above. If you are not the intended recipient, please delete. The information contained within this e-mail is confidential and may be legally privileged. Messages sent by e-mail may be subject to delays, non-delivery and un-authorised alteration. The information contained herein is from sources thought to be reliable but we do not represent that it is accurate or complete. Therefore, Predicted Markets Limited and its staff cannot be held responsible or liable for the contents of this message. This is not an offer or solicitation to buy and sell any instrument, security or investment. Registered in England No. 5851947. Predicted Markets Limited, Ladybrook, Burgh Hill, Bramshott, Hants GU30 7RQ is Authorised and Regulated by the Financial Services Authority.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.
--------------------------------------------------------

 
 
This information is being sent at the recipient's request or...{{dropped}}


From frednovo at pipeline.com  Thu Jul 26 21:06:00 2007
From: frednovo at pipeline.com (Frederick Novomestky)
Date: Thu, 26 Jul 2007 15:06:00 -0400
Subject: [R-SIG-Finance] fCopulae Availability?
In-Reply-To: <BAY108-F40C3960F94FD6619DB18F3AAF10@phx.gbl>
References: <BAY108-F40C3960F94FD6619DB18F3AAF10@phx.gbl>
Message-ID: <6.2.1.2.2.20070726150538.03abfc80@pop.pipeline.com>

Check out the copula package.  This is an excellent starting point.

Best regards,

Fred Novomestky

At 01:26 PM 7/25/2007, Talbot Katz wrote:
>Hi.
>
>I see the fCopulae package listed on the Contributed Packages page of the
>CRAN website (http://cran.cnr.berkeley.edu/); however, fCopulae doesn't
>appear to be available to download from the two CRAN ftp sites I have
>visited.  Is it actually available for use in R, and if so, what is the best
>way to obtain and load it?  (This is not an emergency, I'm just curious.)
>Thanks!
>
>--  TMK  --
>212-460-5430    home
>917-656-5351    cell
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.

Frederick Novomestky, Ph.D.
Novomestky Associates LLC
41 Eastover Drive
East Northport, NY 11731-4330
Vox: 1.631.368.0701
Fax: 1.631.368.1696

Confidentiality Notice: This electronic mail transmission, i...{{dropped}}


From brian at braverock.com  Thu Jul 26 21:39:01 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 26 Jul 2007 14:39:01 -0500
Subject: [R-SIG-Finance] fCopulae Availability?
In-Reply-To: <6.2.1.2.2.20070726150538.03abfc80@pop.pipeline.com>
References: <BAY108-F40C3960F94FD6619DB18F3AAF10@phx.gbl>
	<6.2.1.2.2.20070726150538.03abfc80@pop.pipeline.com>
Message-ID: <46A8F855.7030709@braverock.com>

fCopulae installed fine for me via the install.packages("fCopulae") 
function inside R.

Binary packages are also available here:
http://cran.r-project.org/src/contrib/Descriptions/fCopulae.html

Regards,

    - Brian

Frederick Novomestky wrote:
> Check out the copula package.  This is an excellent starting point.
> 
> Best regards,
> 
> Fred Novomestky
> 
> At 01:26 PM 7/25/2007, Talbot Katz wrote:
>> Hi.
>>
>> I see the fCopulae package listed on the Contributed Packages page of the
>> CRAN website (http://cran.cnr.berkeley.edu/); however, fCopulae doesn't
>> appear to be available to download from the two CRAN ftp sites I have
>> visited.  Is it actually available for use in R, and if so, what is the best
>> way to obtain and load it?  (This is not an emergency, I'm just curious.)
>> Thanks!


From topkatz at msn.com  Thu Jul 26 22:25:42 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 26 Jul 2007 16:25:42 -0400
Subject: [R-SIG-Finance] fCopulae Availability?
In-Reply-To: <46A8F855.7030709@braverock.com>
Message-ID: <BAY108-F407E2579BDA11D9A81CFD3AAF20@phx.gbl>

Thank you, Fred and Brian, for the responses!

When I tried installing from the "USA (CA1)" repository I got:

>install.packages("fCopulae")
Warning in download.packages(pkgs, destdir = tmpd, available = available,  :
         no package 'fCopulae' at the repositories
>

Brian, out of curiosity, what repository were you using?  Perhaps the 
package is available on some repositories but not others?  I've only 
installed packages by downloading them from CRAN in an active R window; if I 
download the gzip file from the website, how would I install the package?

Meanwhile, I was able to install the copula package.

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell



>From: "Brian G. Peterson" <brian at braverock.com>
>To: Talbot Katz <topkatz at msn.com>
>CC: r-sig-finance at stat.math.ethz.ch
>Subject: Re: [R-SIG-Finance] fCopulae Availability?
>Date: Thu, 26 Jul 2007 14:39:01 -0500
>
>fCopulae installed fine for me via the install.packages("fCopulae") 
>function inside R.
>
>Binary packages are also available here:
>http://cran.r-project.org/src/contrib/Descriptions/fCopulae.html
>
>Regards,
>
>    - Brian
>
>Frederick Novomestky wrote:
>>Check out the copula package.  This is an excellent starting point.
>>
>>Best regards,
>>
>>Fred Novomestky
>>
>>At 01:26 PM 7/25/2007, Talbot Katz wrote:
>>>Hi.
>>>
>>>I see the fCopulae package listed on the Contributed Packages page of the
>>>CRAN website (http://cran.cnr.berkeley.edu/); however, fCopulae doesn't
>>>appear to be available to download from the two CRAN ftp sites I have
>>>visited.  Is it actually available for use in R, and if so, what is the 
>>>best
>>>way to obtain and load it?  (This is not an emergency, I'm just curious.)
>>>Thanks!


From topkatz at msn.com  Thu Jul 26 23:11:58 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 26 Jul 2007 17:11:58 -0400
Subject: [R-SIG-Finance] fCopulae Availability?
Message-ID: <BAY108-F2400D3C476CDD51F1B0200AAF20@phx.gbl>

Follow-up...

I had not mentioned previously that I was running R 2.4.1.  I just installed 
R 2.5.1 and was able to obtain the fCopulae package.  I didn't even ask for 
it specifically, I was just installing my previous list of libraries, and it 
got added in automatically as a dependency.  (On the downside, one of my 
previous libraries, DAAG, was no longer available, but that's just a 
collection of sample data sets.)

Thanks again!

--  TMK  --
212-460-5430	home
917-656-5351	cell



>From: "Brian G. Peterson" <brian at braverock.com>
>To: Talbot Katz <topkatz at msn.com>
>CC: r-sig-finance at stat.math.ethz.ch
>Subject: Re: [R-SIG-Finance] fCopulae Availability?
>Date: Thu, 26 Jul 2007 14:39:01 -0500
>
>fCopulae installed fine for me via the install.packages("fCopulae") 
>function inside R.
>
>Binary packages are also available here:
>http://cran.r-project.org/src/contrib/Descriptions/fCopulae.html
>
>Regards,
>
>    - Brian
>
>Frederick Novomestky wrote:
>>Check out the copula package.  This is an excellent starting point.
>>
>>Best regards,
>>
>>Fred Novomestky
>>
>>At 01:26 PM 7/25/2007, Talbot Katz wrote:
>>>Hi.
>>>
>>>I see the fCopulae package listed on the Contributed Packages page of the
>>>CRAN website (http://cran.cnr.berkeley.edu/); however, fCopulae doesn't
>>>appear to be available to download from the two CRAN ftp sites I have
>>>visited.  Is it actually available for use in R, and if so, what is the 
>>>best
>>>way to obtain and load it?  (This is not an emergency, I'm just curious.)
>>>Thanks!


From CVorlow at eurobank.gr  Fri Jul 27 13:07:00 2007
From: CVorlow at eurobank.gr (Vorlow Constantinos)
Date: Fri, 27 Jul 2007 14:07:00 +0300
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 38, Issue 19
In-Reply-To: <mailman.17.1185530408.25884.r-sig-finance@stat.math.ethz.ch>
Message-ID: <93843C113DD8914BB1A9A63878E8918CD8AAA5@EH002EXC.eurobank.efg.gr>

Hi,

I have some similar problems. I found out that although the code to
download the intra-daily prices ("last price") was ok the data would not
download properly. Sometimes the rBloomberg script would be executed but
no data would be downloaded, sometimes the script would crash and
sometimes would execute properly. The very same script. 

I finally gathered that it has to do with the Bloomberg data stream
response. Sometimes, if you wait for a few seconds and rerun the script,
everything will execute  smoothly.  I even have problems with EXCEL
sometimes. Maybe it has to do with the quality-type of connection with
Bloomberg as well (i.e., if you are behind a firewall etc. etc.).

Could it be and R-(D)COM problem?

Best,
Costas

http://www.vorlow.org
----------------------------------
Costas Vorlow
Research Economist
Eurobank EFG
Division of Research & Forecasting

-------------------------------------------------------
tel: +30-210-3337273 (ext 17273)
fax: +30-210-3337687
 
 


-----Original Message-----
Message: 1
Date: Tue, 24 Jul 2007 12:34:33 +0100
From: "Samuel Kemp" <samkemp at predictedmarkets.com>
Subject: [R-SIG-Finance] Problems collecting Intraday data
To: <r-sig-finance at stat.math.ethz.ch>
Message-ID: <200707241134.l6OBYbjI001816 at hypatia.math.ethz.ch>
Content-Type: text/plain;	charset="iso-8859-1"

Hi,

I have been using the blpGetData() function to fetch intraday High and
Low data (10min intervals) i.e.

conn <- blpConnect()
BBCode <- 'VGU7 Index'   # Eurostoxx 50 Futures Sept 07 Contract
Date <- "06/28/07"       # which day
Open <- "07:00:00"	 # from time
Close <- "21:00:00"	 # to time
Interval <- 10		 # time interval between values
dat   <- blpGetData(conn, BBCode, c('LAST_PRICE'),start=as.chron(Date,
Open), end=as.chron(Date,Close),barfields=c('HIGH','LOW'),
barsize=Interval)
blpDisconnect(conn)

However, sometimes it works and then sometimes (using exactly the same
inputs) it does not - I get the following error...

Error in substr(x, as.integer(start), as.integer(stop)) : 
        invalid substring argument(s) in substr()

Does anyone have a solution to this problem and/or explanation?

Many thanks in advance,

Sam

p.s. I am not sure if it is relevant but my computer is running XP.

P Think before you print.

Disclaimer:
This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.


From dsmith at viciscapital.com  Fri Jul 27 14:10:13 2007
From: dsmith at viciscapital.com (Dale Smith)
Date: Fri, 27 Jul 2007 08:10:13 -0400
Subject: [R-SIG-Finance] Problems collecting Intraday data
In-Reply-To: <93843C113DD8914BB1A9A63878E8918CD8AAA5@EH002EXC.eurobank.efg.gr>
References: <mailman.17.1185530408.25884.r-sig-finance@stat.math.ethz.ch>
	<93843C113DD8914BB1A9A63878E8918CD8AAA5@EH002EXC.eurobank.efg.gr>
Message-ID: <0E4F0C7EEAAB274F8DC6B1543949F05BEB71C4@vicsrv4.viciscapital.com>

These are known problems with Bloomberg. It's not a problem with R, and
it happens in Excel. Re-run until you get all the prices you need.

Dale Smith
Vicis Capital, LLC
 
-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Vorlow
Constantinos
Sent: Friday, July 27, 2007 7:07 AM
To: r-sig-finance at stat.math.ethz.ch
Cc: samkemp at predictedmarkets.com
Subject: Re: [R-SIG-Finance] R-SIG-Finance Digest, Vol 38, Issue 19

Hi,

I have some similar problems. I found out that although the code to
download the intra-daily prices ("last price") was ok the data would not
download properly. Sometimes the rBloomberg script would be executed but
no data would be downloaded, sometimes the script would crash and
sometimes would execute properly. The very same script. 

I finally gathered that it has to do with the Bloomberg data stream
response. Sometimes, if you wait for a few seconds and rerun the script,
everything will execute  smoothly.  I even have problems with EXCEL
sometimes. Maybe it has to do with the quality-type of connection with
Bloomberg as well (i.e., if you are behind a firewall etc. etc.).

Could it be and R-(D)COM problem?

Best,
Costas

http://www.vorlow.org
----------------------------------
Costas Vorlow
Research Economist
Eurobank EFG
Division of Research & Forecasting

-------------------------------------------------------
tel: +30-210-3337273 (ext 17273)
fax: +30-210-3337687
 
 


-----Original Message-----
Message: 1
Date: Tue, 24 Jul 2007 12:34:33 +0100
From: "Samuel Kemp" <samkemp at predictedmarkets.com>
Subject: [R-SIG-Finance] Problems collecting Intraday data
To: <r-sig-finance at stat.math.ethz.ch>
Message-ID: <200707241134.l6OBYbjI001816 at hypatia.math.ethz.ch>
Content-Type: text/plain;	charset="iso-8859-1"

Hi,

I have been using the blpGetData() function to fetch intraday High and
Low data (10min intervals) i.e.

conn <- blpConnect()
BBCode <- 'VGU7 Index'   # Eurostoxx 50 Futures Sept 07 Contract
Date <- "06/28/07"       # which day
Open <- "07:00:00"	 # from time
Close <- "21:00:00"	 # to time
Interval <- 10		 # time interval between values
dat   <- blpGetData(conn, BBCode, c('LAST_PRICE'),start=as.chron(Date,
Open), end=as.chron(Date,Close),barfields=c('HIGH','LOW'),
barsize=Interval)
blpDisconnect(conn)

However, sometimes it works and then sometimes (using exactly the same
inputs) it does not - I get the following error...

Error in substr(x, as.integer(start), as.integer(stop)) : 
        invalid substring argument(s) in substr()

Does anyone have a solution to this problem and/or explanation?

Many thanks in advance,

Sam

p.s. I am not sure if it is relevant but my computer is running XP.

P Think before you print.

Disclaimer:
This e-mail is confidential. If you are not the intended recipient, you
should not copy it, re-transmit it, use it or disclose its contents, but
should return it to the sender immediately and delete the copy from your
system.
EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any
opinion, recommendation, conclusion, solicitation, offer or agreement or
any information contained in this communication.
EFG Eurobank Ergasias S.A. cannot accept any responsibility for the
accuracy or completeness of this message as it has been transmitted over
a public network. If you suspect that the message may have been
intercepted or amended, please call the sender.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

All e-mail sent to or from this address will be received or otherwise recorded by Vicis Capital, LLC and is subject to archival, monitoring and/or review, by and/or disclosure to, someone other than the recipient.  This message is intended only for the use of the person(s) ("intended recipient") to whom it is addressed.  It may contain information that is privileged and confidential.  If you are not the intended recipient, please contact the sender as soon as possible and delete the message without reading it or making a copy.  Any dissemination, distribution, copying, or other use of this message or any of its content by any person other than the intended recipient is strictly prohibited.  Vicis Capital, LLC only transacts business in states where it is properly registered or notice filed, or excluded or exempted from registration or notice filing requirements.


From jordi.molins.coronado at gmail.com  Tue Jul 31 22:04:45 2007
From: jordi.molins.coronado at gmail.com (Jordi Molins)
Date: Tue, 31 Jul 2007 22:04:45 +0200
Subject: [R-SIG-Finance] zoo-like classes in c++?
Message-ID: <d2b785860707311304n217be78dyb4039ffef0de76c6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070731/fc259532/attachment.pl 

From whit.armstrong at hcmny.com  Tue Jul 31 22:36:36 2007
From: whit.armstrong at hcmny.com (Armstrong, Whit)
Date: Tue, 31 Jul 2007 16:36:36 -0400
Subject: [R-SIG-Finance] zoo-like classes in c++?
In-Reply-To: <d2b785860707311304n217be78dyb4039ffef0de76c6@mail.gmail.com>
Message-ID: <2CF54397DF75234DA13D028A0753C0DE7FBF21@ex3.811t.hcmny.com>

no help files for this package which is why it's not on cran yet, but
I'm happy to walk you through whatever you need to do.

It's basically an R wrapper on top of a c++ class which implements all
the time series functions.  It uses POSIXct for dates.

http://code.google.com/p/rseries/

-Whit
 

> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch 
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of 
> Jordi Molins
> Sent: Tuesday, July 31, 2007 4:05 PM
> To: r-sig-finance at stat.math.ethz.ch; Jordi Molins
> Subject: [R-SIG-Finance] zoo-like classes in c++?
> 
> I usually use zoo for my time series in R. However, sometimes 
> my algorithms are very slow. Maybe my code is inefficient, 
> but I need to add several for and ifs, which I believe slows 
> down the whole calculation.
> 
> I have been thinking that if I could use the C++ STL for 
> intersections (and other operations), in addition to use the 
> fast for and if in c++, the calculation could be much faster. 
> My problem is that I do not know any time and date class in 
> C++ with the same conventions as in zoo and POSIX. I have 
> been googling, and I have found Boost.Date_Time, which 
> apparently does quite a lot of what I want.
> 
> Is zoo using some time and date class from C++? If yes, which 
> one? if not, which one could I use?
> 
> Thank you.
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 




This e-mail message is intended only for the named recipient(s) above. It may contain confidential information. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this e-mail and any attachment(s) is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender by replying to this e-mail and delete the message and any attachment(s) from your system. Thank you.


From seanpor at acm.org  Tue Jul 31 22:37:22 2007
From: seanpor at acm.org (Sean O'Riordain)
Date: Tue, 31 Jul 2007 20:37:22 +0000
Subject: [R-SIG-Finance] zoo-like classes in c++?
In-Reply-To: <d2b785860707311304n217be78dyb4039ffef0de76c6@mail.gmail.com>
References: <d2b785860707311304n217be78dyb4039ffef0de76c6@mail.gmail.com>
Message-ID: <8ed68eed0707311337y56ea4cf2h9a8fd9c01be0c762@mail.gmail.com>

Jordi,

I've been doing some work with big text files (50+mb) recently and I
thought that I could only do the work in for() loops etc... I was ok
up to a certain size then my machine just couldn't take it ran out of
memory and I had to rework my algorithm - staying in R - I work in a
corporate environment where making changes to my desktop is something
that takes weeks and I just didn't have that time on the project...
Eventually I figured out how to vectorise the calculations and on a
small file what took minutes reduced to instantaneous, and from what
was impossible took a mere 10 seconds once vectorised...

Are you really sure that it is really impossible to vectorise this?

cheers,
Sean


On 31/07/07, Jordi Molins <jordi.molins.coronado at gmail.com> wrote:
> I usually use zoo for my time series in R. However, sometimes my algorithms
> are very slow. Maybe my code is inefficient, but I need to add several for
> and ifs, which I believe slows down the whole calculation.
>
> I have been thinking that if I could use the C++ STL for intersections (and
> other operations), in addition to use the fast for and if in c++, the
> calculation could be much faster. My problem is that I do not know any time
> and date class in C++ with the same conventions as in zoo and POSIX. I have
> been googling, and I have found Boost.Date_Time, which apparently does quite
> a lot of what I want.
>
> Is zoo using some time and date class from C++? If yes, which one? if not,
> which one could I use?
>
> Thank you.
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Tue Jul 31 23:09:09 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 Jul 2007 17:09:09 -0400
Subject: [R-SIG-Finance] zoo-like classes in c++?
In-Reply-To: <d2b785860707311304n217be78dyb4039ffef0de76c6@mail.gmail.com>
References: <d2b785860707311304n217be78dyb4039ffef0de76c6@mail.gmail.com>
Message-ID: <971536df0707311409g24e60d08r673addd58ef7805b@mail.gmail.com>

No, zoo does not use builtin time and date classes.  It does not
know anything about Date, chron, POSIXct, etc.  Its completely
general and only assumes that whatever classes you use are ordered
(that's the O in zoo) and have certain methods defined. See ?zoo for
more information on the methods assumed.

For example, here we are using letters as our time/date class.  The
time/date class in this example is not even numeric:

> library(zoo)
> z <- zoo(11:15, letters[1:5])
> merge(z, lag(z, -1))
   z lag(z, -1)
a 11         NA
b 12         11
c 13         12
d 14         13
e 15         14

(Actually there are a few exceptions to the above for convenience, i.e.
"yearqtr" and "yearmon" classes are provided, as.Date.numeric
is provided and read.zoo understands Date and POSIXct classes
but overall the statement that its perfectly general is correct.)


On 7/31/07, Jordi Molins <jordi.molins.coronado at gmail.com> wrote:
> I usually use zoo for my time series in R. However, sometimes my algorithms
> are very slow. Maybe my code is inefficient, but I need to add several for
> and ifs, which I believe slows down the whole calculation.
>
> I have been thinking that if I could use the C++ STL for intersections (and
> other operations), in addition to use the fast for and if in c++, the
> calculation could be much faster. My problem is that I do not know any time
> and date class in C++ with the same conventions as in zoo and POSIX. I have
> been googling, and I have found Boost.Date_Time, which apparently does quite
> a lot of what I want.
>
> Is zoo using some time and date class from C++? If yes, which one? if not,
> which one could I use?
>
> Thank you.
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jordi.molins.coronado at gmail.com  Wed Aug  1 09:45:11 2007
From: jordi.molins.coronado at gmail.com (Jordi Molins)
Date: Wed, 1 Aug 2007 09:45:11 +0200
Subject: [R-SIG-Finance] zoo-like classes in c++?
In-Reply-To: <971536df0707311409g24e60d08r673addd58ef7805b@mail.gmail.com>
References: <d2b785860707311304n217be78dyb4039ffef0de76c6@mail.gmail.com>
	<971536df0707311409g24e60d08r673addd58ef7805b@mail.gmail.com>
Message-ID: <d2b785860708010045s552cd0a9hf644e06b9b651d7b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070801/1953e250/attachment.pl 

From rory.winston at gmail.com  Wed Aug  1 15:03:33 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Wed, 1 Aug 2007 14:03:33 +0100
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
Message-ID: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070801/39d11a3c/attachment.pl 

From ggrothendieck at gmail.com  Wed Aug  1 15:16:24 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 1 Aug 2007 09:16:24 -0400
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
Message-ID: <971536df0708010616h39d87c1ei951a59abcd9812b2@mail.gmail.com>

Something similar was just discussed this morning:
https://www.stat.math.ethz.ch/pipermail/r-help/2007-August/137695.html


On 8/1/07, Rory Winston <rory.winston at gmail.com> wrote:
> Hi all
>
> I have a question about aggegating statistics by time intervals. I have a
> data set with 3 columns : time, bid, and ask. Time is specified as a
> millisecond timestamp since epoch. I would like to compute summary
> statistics for the data set on an hourly basis. Here is what I have tried so
> far:
>
> # Data is in pricedata
>
> t <- ISODatetime(1970, 1, 1, 0, 0, 0) + pricedata$time
> agg <- aggregate(pricedata$spread, list(byhour=format(t, "%Y-%m %H")), mean)
>
> This seems to do what I want - however, what really want to do is more
> specific: I would like to be able to extract a subset of the data frame
> pricedata, and not just the aggregated entries - for instance, instead of
> just extracting pricedata$spread by hour, I would like to extract a slice of
> columns, e.g. pricedata$spread and pricedata$time on an hourly basis, and
> pass these into a function that can compute a time-weighted average spread,
> for instance. Does anyone know an elegant way to do this? I have a feeling
> zoo may do what I want, but I'm new to zoo ...
>
> Cheers
> Rory
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From thomas.harte at yahoo.com  Wed Aug  1 16:06:20 2007
From: thomas.harte at yahoo.com (Thomas Harte)
Date: Wed, 1 Aug 2007 07:06:20 -0700 (PDT)
Subject: [R-SIG-Finance] zoo-like classes in c++?
Message-ID: <679860.2245.qm@web43143.mail.sp1.yahoo.com>

whit,

do you have any regression-testing code to go with either the C++ src
or the R wrapper?

cheers,

thomas.

> no help files for this package which is why it's not on cran yet, but
> I'm happy to walk you through whatever you need to do.
>
> It's basically an R wrapper on top of a c++ class which implements all
> the time series functions.  It uses POSIXct for dates.
>
> http://code.google.com/p/rseries/
>
> -Whit


From whit.armstrong at hcmny.com  Wed Aug  1 16:24:30 2007
From: whit.armstrong at hcmny.com (Armstrong, Whit)
Date: Wed, 1 Aug 2007 10:24:30 -0400
Subject: [R-SIG-Finance] zoo-like classes in c++?
In-Reply-To: <679860.2245.qm@web43143.mail.sp1.yahoo.com>
Message-ID: <2CF54397DF75234DA13D028A0753C0DE7FBF28@ex3.811t.hcmny.com>

There is no test suite written in R, but the c++ class has its own test
suite using the boost unit_test_framework.

I haven't set up any automated builds or regression testing, but help in
that area is certainly welcome.

Here is the link to the c++ library.  The tar is dated, so use the svn
link if you want to examine the code.

https://sourceforge.net/projects/tslib

svn: https://tslib.svn.sourceforge.net/svnroot/tslib

-Whit
 

> -----Original Message-----
> From: Thomas Harte [mailto:thomas.harte at yahoo.com] 
> Sent: Wednesday, August 01, 2007 10:06 AM
> To: Armstrong, Whit
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] zoo-like classes in c++?
> 
> whit,
> 
> do you have any regression-testing code to go with either the 
> C++ src or the R wrapper?
> 
> cheers,
> 
> thomas.
> 
> > no help files for this package which is why it's not on 
> cran yet, but 
> > I'm happy to walk you through whatever you need to do.
> >
> > It's basically an R wrapper on top of a c++ class which 
> implements all 
> > the time series functions.  It uses POSIXct for dates.
> >
> > http://code.google.com/p/rseries/
> >
> > -Whit
> 




This e-mail message is intended only for the named recipient(s) above. It may contain confidential information. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this e-mail and any attachment(s) is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender by replying to this e-mail and delete the message and any attachment(s) from your system. Thank you.


From rory.winston at gmail.com  Fri Aug  3 10:44:15 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Fri, 3 Aug 2007 09:44:15 +0100
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <971536df0708010616h39d87c1ei951a59abcd9812b2@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
	<971536df0708010616h39d87c1ei951a59abcd9812b2@mail.gmail.com>
Message-ID: <3f446aa30708030144s477cd70ahcdc21dc20d47f654@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070803/0c692623/attachment.pl 

From ggrothendieck at gmail.com  Fri Aug  3 12:35:40 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 3 Aug 2007 06:35:40 -0400
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <3f446aa30708030144s477cd70ahcdc21dc20d47f654@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
	<971536df0708010616h39d87c1ei951a59abcd9812b2@mail.gmail.com>
	<3f446aa30708030144s477cd70ahcdc21dc20d47f654@mail.gmail.com>
Message-ID: <971536df0708030335g3b9aa867h7a631299e309f764@mail.gmail.com>

Can you provide a reproducible example that exhibits the warning.
Redoing it in a more easily reproducible way and using the data
in your post gives me no warning

> tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791, 1185882791,
+ 1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
+ 2e-04, 1e-04))
>
> twas <-
+  function(dat) {
+    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
+  sum)/sum(diff(dat$time)) * 100.0)
+ }
> now <- Sys.time()
> epoch <- now - as.numeric(now)
> z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> z
      1e-04    2e-04
07 66.66667 33.33333
> R.version.string # XP
[1] "R version 2.5.1 (2007-06-27)"


Here is input:

tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791, 1185882791,
1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
2e-04, 1e-04))
twas <-
 function(dat) {
   data.frame(tapply(diff(dat$time), head(dat$spread, -1),
 sum)/sum(diff(dat$time)) * 100.0)
}
now <- Sys.time()
epoch <- now - as.numeric(now)
z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
z
R.version.string # XP



On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> Hi
>
> I've been wrestling with this a little bit, using the example in the email
> that Gabor pointed me to as a reference, and I think I have almost got what
> I want...however its still not quite right.
>
> I have a variable, tmp, with two dimensions: time and spread:
>
> > head(tmp$time)
> [1] 1185882786 1185882790 1185882791 1185882791 1185882792 1185882795
>
> > head(tmp$spread)
> [1] 1e-04 1e-04 2e-04 1e-04 2e-04 1e-04
> >
>
> I also have a function that calculates the time-weighted average spread:
>
> > twas
> function(dat) {
>   data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> sum)/sum(diff(dat$time)) * 100.0)
> }
>
> I can combine them using as rbind() and by():
>
> z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
>
> (epoch is just an instance of ISOdatetime)
>
> This gives me a warning:
>
> Warning message:
> number of columns of result
>        is not a multiple of vector length (arg 3) in: rbind(1, "12" = c(
> 91.99207541277, 8.00792458723005), "13" = c(90.1884966797708,
>
> The output from the above command is almost exactly what I need, apart from
> the recycling:
>
>      1e-04     2e-04      3e-04        4e-04
> 12 91.99208  8.007925 91.9920754  8.007924587 <== recycled values
> 13 90.18850  9.337448  0.4218405  0.052214551
> 14 90.59640  9.171417  0.2321811 90.596401668
> 15 89.55771 10.194291  0.2343418  0.013661453
> ...
>
> I can just pass this into a barplot() and get a nice visual breakdown of
> hourly weighted spreads, *but* I dont know how to get these results without
> the recycling. Looking at rbind(), it seems that this will automatically
> recycle. Does anyone know of a function I could use to get these results
> without this problem?
>
> Cheers
> Rory
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> On 8/1/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> > Something similar was just discussed this morning:
> > https://www.stat.math.ethz.ch/pipermail/r-help/2007-August/137695.html
> >
> >
> > On 8/1/07, Rory Winston <rory.winston at gmail.com> wrote:
> > > Hi all
> > >
> > > I have a question about aggegating statistics by time intervals. I have
> > a
> > > data set with 3 columns : time, bid, and ask. Time is specified as a
> > > millisecond timestamp since epoch. I would like to compute summary
> > > statistics for the data set on an hourly basis. Here is what I have
> > tried so
> > > far:
> > >
> > > # Data is in pricedata
> > >
> > > t <- ISODatetime(1970, 1, 1, 0, 0, 0) + pricedata$time
> > > agg <- aggregate(pricedata$spread, list(byhour=format(t, "%Y-%m %H")),
> > mean)
> > >
> > > This seems to do what I want - however, what really want to do is more
> > > specific: I would like to be able to extract a subset of the data frame
> > > pricedata, and not just the aggregated entries - for instance, instead
> > of
> > > just extracting pricedata$spread by hour, I would like to extract a
> > slice of
> > > columns, e.g. pricedata$spread and pricedata$time on an hourly basis,
> > and
> > > pass these into a function that can compute a time-weighted average
> > spread,
> > > for instance. Does anyone know an elegant way to do this? I have a
> > feeling
> > > zoo may do what I want, but I'm new to zoo ...
> > >
> > > Cheers
> > > Rory
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> >
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From rory.winston at gmail.com  Fri Aug  3 14:06:37 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Fri, 3 Aug 2007 13:06:37 +0100
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <971536df0708030335g3b9aa867h7a631299e309f764@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
	<971536df0708010616h39d87c1ei951a59abcd9812b2@mail.gmail.com>
	<3f446aa30708030144s477cd70ahcdc21dc20d47f654@mail.gmail.com>
	<971536df0708030335g3b9aa867h7a631299e309f764@mail.gmail.com>
Message-ID: <3f446aa30708030506m6439a41do53377c3cd39e076e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070803/06744fb6/attachment.pl 

From ggrothendieck at gmail.com  Fri Aug  3 14:20:52 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 3 Aug 2007 08:20:52 -0400
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <3f446aa30708030506m6439a41do53377c3cd39e076e@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
	<971536df0708010616h39d87c1ei951a59abcd9812b2@mail.gmail.com>
	<3f446aa30708030144s477cd70ahcdc21dc20d47f654@mail.gmail.com>
	<971536df0708030335g3b9aa867h7a631299e309f764@mail.gmail.com>
	<3f446aa30708030506m6439a41do53377c3cd39e076e@mail.gmail.com>
Message-ID: <971536df0708030520o492b060ao10bbac04bb3867e5@mail.gmail.com>

I still get no warning.  Please provide complete self contained input
and output.

> tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791, 1185882791,
+  1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
+  2e-04, 3e-04))
>
> twas <-
+  function(dat) {
+    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
+  sum)/sum(diff(dat$time)) * 100.0)
+ }
> now <- Sys.time()
> epoch <- now - as.numeric(now)
> z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> z
      1e-04    2e-04
07 66.66667 33.33333
>
> R.version.string # XP
[1] "R version 2.5.1 (2007-06-27)"


On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> Hi
>
> I have figured out what causes the warning (and recycling), but I am not
> sure how I can fix it. After seeing that it seemed to work for you, I went
> back and tried working with different subsets of the data. I eventually
> found where it occurs - when we get a third unique spread value. To
> reproduce, just change the definition of tmp to be:
>
> tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791, 1185882791,
>  1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
>  2e-04, 3e-04)) <== Added 3e-04
>
> i.e. I have just changed one of the spread values to be a third value - this
> seems to trigger the warning  "Warning message:number of columns of result
> is not a multiple of vector length (arg 3) in: rbind", and the recycling. I
> tried this on R 2.5.0 and 2.5.1
>
> Can anyone see what I am doing wrong here?
>
> Cheers
> Rory
>
>
>
>
>
>
> On 8/3/07, Gabor Grothendieck < ggrothendieck at gmail.com> wrote:
> > Can you provide a reproducible example that exhibits the warning.
> > Redoing it in a more easily reproducible way and using the data
> > in your post gives me no warning
> >
> > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> 1185882791,
> > + 1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > + 2e-04, 1e-04))
> > >
> > > twas <-
> > +  function(dat) {
> > +    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > +  sum)/sum(diff(dat$time)) * 100.0)
> > + }
> > > now <- Sys.time()
> > > epoch <- now - as.numeric(now)
> > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > > z
> >       1e-04    2e-04
> > 07 66.66667 33.33333
> > > R.version.string # XP
> > [1] "R version 2.5.1 (2007-06-27)"
> >
> >
> > Here is input:
> >
> > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791, 1185882791,
> > 1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > 2e-04, 1e-04))
> > twas <-
> > function(dat) {
> >    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > sum)/sum(diff(dat$time)) * 100.0)
> > }
> > now <- Sys.time()
> > epoch <- now - as.numeric(now)
> > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > z
> > R.version.string # XP
> >
> >
> >
> > On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> > > Hi
> > >
> > > I've been wrestling with this a little bit, using the example in the
> email
> > > that Gabor pointed me to as a reference, and I think I have almost got
> what
> > > I want...however its still not quite right.
> > >
> > > I have a variable, tmp, with two dimensions: time and spread:
> > >
> > > > head(tmp$time)
> > > [1] 1185882786 1185882790 1185882791 1185882791 1185882792 1185882795
> > >
> > > > head(tmp$spread)
> > > [1] 1e-04 1e-04 2e-04 1e-04 2e-04 1e-04
> > > >
> > >
> > > I also have a function that calculates the time-weighted average spread:
> > >
> > > > twas
> > > function(dat) {
> > >   data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > sum)/sum(diff(dat$time)) * 100.0)
> > > }
> > >
> > > I can combine them using as rbind() and by():
> > >
> > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > >
> > > (epoch is just an instance of ISOdatetime)
> > >
> > > This gives me a warning:
> > >
> > > Warning message:
> > > number of columns of result
> > >        is not a multiple of vector length (arg 3) in: rbind(1, "12" = c(
> > > 91.99207541277, 8.00792458723005), "13" = c(90.1884966797708,
> > >
> > > The output from the above command is almost exactly what I need, apart
> from
> > > the recycling:
> > >
> > >      1e-04     2e-04      3e-04        4e-04
> > > 12 91.99208  8.007925 91.9920754  8.007924587 <== recycled values
> > > 13 90.18850  9.337448  0.4218405  0.052214551
> > > 14 90.59640  9.171417  0.2321811 90.596401668
> > > 15 89.55771 10.194291  0.2343418  0.013661453
> > > ...
> > >
> > > I can just pass this into a barplot() and get a nice visual breakdown of
> > > hourly weighted spreads, *but* I dont know how to get these results
> without
> > > the recycling. Looking at rbind(), it seems that this will automatically
> > > recycle. Does anyone know of a function I could use to get these results
> > > without this problem?
> > >
> > > Cheers
> > > Rory
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > On 8/1/07, Gabor Grothendieck <ggrothendieck at gmail.com > wrote:
> > > >
> > > > Something similar was just discussed this morning:
> > > >
> https://www.stat.math.ethz.ch/pipermail/r-help/2007-August/137695.html
> > > >
> > > >
> > > > On 8/1/07, Rory Winston <rory.winston at gmail.com> wrote:
> > > > > Hi all
> > > > >
> > > > > I have a question about aggegating statistics by time intervals. I
> have
> > > > a
> > > > > data set with 3 columns : time, bid, and ask. Time is specified as a
> > > > > millisecond timestamp since epoch. I would like to compute summary
> > > > > statistics for the data set on an hourly basis. Here is what I have
> > > > tried so
> > > > > far:
> > > > >
> > > > > # Data is in pricedata
> > > > >
> > > > > t <- ISODatetime(1970, 1, 1, 0, 0, 0) + pricedata$time
> > > > > agg <- aggregate(pricedata$spread, list(byhour=format(t, "%Y-%m
> %H")),
> > > > mean)
> > > > >
> > > > > This seems to do what I want - however, what really want to do is
> more
> > > > > specific: I would like to be able to extract a subset of the data
> frame
> > > > > pricedata, and not just the aggregated entries - for instance,
> instead
> > > > of
> > > > > just extracting pricedata$spread by hour, I would like to extract a
> > > > slice of
> > > > > columns, e.g. pricedata$spread and pricedata$time on an hourly
> basis,
> > > > and
> > > > > pass these into a function that can compute a time-weighted average
> > > > spread,
> > > > > for instance. Does anyone know an elegant way to do this? I have a
> > > > feeling
> > > > > zoo may do what I want, but I'm new to zoo ...
> > > > >
> > > > > Cheers
> > > > > Rory
> > > > >
> > > > >        [[alternative HTML version deleted]]
> > > > >
> > > > > _______________________________________________
> > > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > -- Subscriber-posting only.
> > > > > -- If you want to post, subscribe first.
> > > > >
> > > >
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> >
>
>


From weihanliu2002 at yahoo.com  Fri Aug  3 14:44:20 2007
From: weihanliu2002 at yahoo.com (Wei-han Liu)
Date: Fri, 3 Aug 2007 05:44:20 -0700 (PDT)
Subject: [R-SIG-Finance]  question on analyzing of correlation structure
Message-ID: <633952.8806.qm@web53512.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070803/156610ae/attachment.pl 

From mam3xs at gmail.com  Thu Aug  2 17:03:11 2007
From: mam3xs at gmail.com (Michael Sun)
Date: Thu, 2 Aug 2007 16:03:11 +0100
Subject: [R-SIG-Finance] Estimate parameters of copulas
Message-ID: <fe005e260708020803m77a93ddbs3190e6e8f361e570@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070802/db4ff050/attachment.pl 

From liulu at hotmail.com  Fri Aug  3 14:39:11 2007
From: liulu at hotmail.com (liu lu)
Date: Fri, 3 Aug 2007 20:39:11 +0800
Subject: [R-SIG-Finance]  question on analyzing of correlation structure
Message-ID: <BAY105-DAV122CF31A56BC125F67E32CA6EA0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070803/6d2279f0/attachment.pl 

From rory.winston at gmail.com  Fri Aug  3 15:15:07 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Fri, 3 Aug 2007 14:15:07 +0100
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <971536df0708030520o492b060ao10bbac04bb3867e5@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
	<971536df0708010616h39d87c1ei951a59abcd9812b2@mail.gmail.com>
	<3f446aa30708030144s477cd70ahcdc21dc20d47f654@mail.gmail.com>
	<971536df0708030335g3b9aa867h7a631299e309f764@mail.gmail.com>
	<3f446aa30708030506m6439a41do53377c3cd39e076e@mail.gmail.com>
	<971536df0708030520o492b060ao10bbac04bb3867e5@mail.gmail.com>
Message-ID: <3f446aa30708030615v121e4b4bn6152d5af6545eb33@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070803/d0ecbd0e/attachment.pl 

From ggrothendieck at gmail.com  Fri Aug  3 16:29:43 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 3 Aug 2007 10:29:43 -0400
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <3f446aa30708030615v121e4b4bn6152d5af6545eb33@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
	<971536df0708010616h39d87c1ei951a59abcd9812b2@mail.gmail.com>
	<3f446aa30708030144s477cd70ahcdc21dc20d47f654@mail.gmail.com>
	<971536df0708030335g3b9aa867h7a631299e309f764@mail.gmail.com>
	<3f446aa30708030506m6439a41do53377c3cd39e076e@mail.gmail.com>
	<971536df0708030520o492b060ao10bbac04bb3867e5@mail.gmail.com>
	<3f446aa30708030615v121e4b4bn6152d5af6545eb33@mail.gmail.com>
Message-ID: <971536df0708030729y23d9941dqaa81d1f38c64cfdd@mail.gmail.com>

Try producing it in "long" format using aggregate and then reshaping
it into "wide" format using xtabs, reshape or the reshape package:

twas <- function(x) {
	y <- data.frame(timediff = diff(x$time), head(x, -1))
	aggregate(100 * y[1]/sum(y[1]), y[c("hour", "spread")], sum)
}
tmp2 <- cbind(tmp, hour = fmt(tmp$time))
long <- do.call("rbind", by(tmp2, tmp2["hour"], twas))

# any one of these three:

xtabs(timediff ~., long)

reshape(long, dir = "wide", timevar = "spread", idvar = "hour")

library(reshape)
cast(melt(long, id = 1:2), hour ~ spread)


On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> Hi
>
> Sorry, I'm not sure what happened with that last one. Here is a fully
> contained example (sorry about the line length if this doesnt wrap).
>
> tmp <- data.frame(
> time=c(1185882786,1185882790,1185882791,1185882791,1185882792,1185882795,1185882796,1185882797,1185882797,1185882798,1185882799,1185882800,1185882806,1185882807,1185882809,1185882810,1185882810,1185882811,1185882845,1185882846,1185882906,1185882918,1185882950,1185882951,1185882951,1185882952,1185882953,1185882954,1185882955,1185882956,1185882991,1185882991,1185882995,1185882996,1185882997,1185882997,1185882998,1185882998,1185882999,1185883003,1185883004,1185883006,1185883007,1185883025,1185883026,1185883086,1185883129,1185883129,1185883133,1185883133,1185883137,1185883137,1185883144,1185883145,1185883145,1185883148,1185883148,1185883149,1185883150,1185883151,1185883152,1185883154,1185883154,1185883155,1185883155,1185883175,1185883176,1185883179,1185883179,1185883180,1185883181,1185883181,1185883182,1185883186,1185883187,1185883191,1185883191,1185883200,1185883200,1185883211,1185883212,1185883214,1185883214,1185883215,1185883217,1185883218,1185883219,1185883279,1185883307,1185883307,1185883365,1185883366,1185883366,1185883367,1185883368,1185883368,1185883368,1185883369,1185883373,1185883376),
> spread=c(1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,3e-04,2e-04)
> )
>
> twas <- function (dat)
> {
>     data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> sum)/sum(diff(dat$time)) * 100)
> }
>
> now <- Sys.time()
> epoch <- now - as.numeric(now)
>
> z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
>
> Cheers
> Rory
>
>
> On 8/3/07, Gabor Grothendieck < ggrothendieck at gmail.com> wrote:
> > I still get no warning.  Please provide complete self contained input
> > and output.
> >
> > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> 1185882791,
> > +  1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > +  2e-04, 3e-04))
> > >
> > > twas <-
> > +  function(dat) {
> > +    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > +  sum)/sum(diff(dat$time)) * 100.0)
> > + }
> > > now <- Sys.time()
> > > epoch <- now - as.numeric(now)
> > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > > z
> >       1e-04    2e-04
> > 07 66.66667 33.33333
> > >
> > > R.version.string # XP
> > [1] "R version 2.5.1 (2007-06-27)"
> >
> >
> > On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> > > Hi
> > >
> > > I have figured out what causes the warning (and recycling), but I am not
> > > sure how I can fix it. After seeing that it seemed to work for you, I
> went
> > > back and tried working with different subsets of the data. I eventually
> > > found where it occurs - when we get a third unique spread value. To
> > > reproduce, just change the definition of tmp to be:
> > >
> > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> 1185882791,
> > >  1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > >  2e-04, 3e-04)) <== Added 3e-04
> > >
> > > i.e. I have just changed one of the spread values to be a third value -
> this
> > > seems to trigger the warning  "Warning message:number of columns of
> result
> > > is not a multiple of vector length (arg 3) in: rbind", and the
> recycling. I
> > > tried this on R 2.5.0 and 2.5.1
> > >
> > > Can anyone see what I am doing wrong here?
> > >
> > > Cheers
> > > Rory
> > >
> > >
> > >
> > >
> > >
> > >
> > > On 8/3/07, Gabor Grothendieck < ggrothendieck at gmail.com> wrote:
> > > > Can you provide a reproducible example that exhibits the warning.
> > > > Redoing it in a more easily reproducible way and using the data
> > > > in your post gives me no warning
> > > >
> > > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > > 1185882791,
> > > > + 1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > > + 2e-04, 1e-04))
> > > > >
> > > > > twas <-
> > > > +  function(dat) {
> > > > +     data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > +  sum)/sum(diff(dat$time)) * 100.0)
> > > > + }
> > > > > now <- Sys.time()
> > > > > epoch <- now - as.numeric(now)
> > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > > > > z
> > > >       1e-04    2e-04
> > > > 07 66.66667 33.33333
> > > > > R.version.string # XP
> > > > [1] "R version 2.5.1 (2007-06-27)"
> > > >
> > > >
> > > > Here is input:
> > > >
> > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> 1185882791,
> > > > 1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > > 2e-04, 1e-04))
> > > > twas <-
> > > > function(dat) {
> > > >    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > sum)/sum(diff(dat$time)) * 100.0)
> > > > }
> > > > now <- Sys.time()
> > > > epoch <- now - as.numeric(now)
> > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > > > z
> > > > R.version.string # XP
> > > >
> > > >
> > > >
> > > > On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> > > > > Hi
> > > > >
> > > > > I've been wrestling with this a little bit, using the example in the
> > > email
> > > > > that Gabor pointed me to as a reference, and I think I have almost
> got
> > > what
> > > > > I want...however its still not quite right.
> > > > >
> > > > > I have a variable, tmp, with two dimensions: time and spread:
> > > > >
> > > > > > head(tmp$time)
> > > > > [1] 1185882786 1185882790 1185882791 1185882791 1185882792
> 1185882795
> > > > >
> > > > > > head(tmp$spread)
> > > > > [1] 1e-04 1e-04 2e-04 1e-04 2e-04 1e-04
> > > > > >
> > > > >
> > > > > I also have a function that calculates the time-weighted average
> spread:
> > > > >
> > > > > > twas
> > > > > function(dat) {
> > > > >   data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > sum)/sum(diff(dat$time)) * 100.0)
> > > > > }
> > > > >
> > > > > I can combine them using as rbind() and by():
> > > > >
> > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > > > >
> > > > > (epoch is just an instance of ISOdatetime)
> > > > >
> > > > > This gives me a warning:
> > > > >
> > > > > Warning message:
> > > > > number of columns of result
> > > > >        is not a multiple of vector length (arg 3) in: rbind(1, "12"
> = c(
> > > > > 91.99207541277 , 8.00792458723005), "13" = c(90.1884966797708,
> > > > >
> > > > > The output from the above command is almost exactly what I need,
> apart
> > > from
> > > > > the recycling:
> > > > >
> > > > >      1e-04     2e-04      3e-04        4e-04
> > > > > 12 91.99208  8.007925 91.9920754  8.007924587 <== recycled values
> > > > > 13 90.18850  9.337448  0.4218405  0.052214551
> > > > > 14 90.59640  9.171417  0.2321811 90.596401668
> > > > > 15 89.55771 10.194291  0.2343418  0.013661453
> > > > > ...
> > > > >
> > > > > I can just pass this into a barplot() and get a nice visual
> breakdown of
> > > > > hourly weighted spreads, *but* I dont know how to get these results
> > > without
> > > > > the recycling. Looking at rbind(), it seems that this will
> automatically
> > > > > recycle. Does anyone know of a function I could use to get these
> results
> > > > > without this problem?
> > > > >
> > > > > Cheers
> > > > > Rory
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > > On 8/1/07, Gabor Grothendieck < ggrothendieck at gmail.com > wrote:
> > > > > >
> > > > > > Something similar was just discussed this morning:
> > > > > >
> > >
> https://www.stat.math.ethz.ch/pipermail/r-help/2007-August/137695.html
> > > > > >
> > > > > >
> > > > > > On 8/1/07, Rory Winston <rory.winston at gmail.com > wrote:
> > > > > > > Hi all
> > > > > > >
> > > > > > > I have a question about aggegating statistics by time intervals.
> I
> > > have
> > > > > > a
> > > > > > > data set with 3 columns : time, bid, and ask. Time is specified
> as a
> > > > > > > millisecond timestamp since epoch. I would like to compute
> summary
> > > > > > > statistics for the data set on an hourly basis. Here is what I
> have
> > > > > > tried so
> > > > > > > far:
> > > > > > >
> > > > > > > # Data is in pricedata
> > > > > > >
> > > > > > > t <- ISODatetime(1970, 1, 1, 0, 0, 0) + pricedata$time
> > > > > > > agg <- aggregate(pricedata$spread, list(byhour=format(t, "%Y-%m
> > > %H")),
> > > > > > mean)
> > > > > > >
> > > > > > > This seems to do what I want - however, what really want to do
> is
> > > more
> > > > > > > specific: I would like to be able to extract a subset of the
> data
> > > frame
> > > > > > > pricedata, and not just the aggregated entries - for instance,
> > > instead
> > > > > > of
> > > > > > > just extracting pricedata$spread by hour, I would like to
> extract a
> > > > > > slice of
> > > > > > > columns, e.g. pricedata$spread and pricedata$time on an hourly
> > > basis,
> > > > > > and
> > > > > > > pass these into a function that can compute a time-weighted
> average
> > > > > > spread,
> > > > > > > for instance. Does anyone know an elegant way to do this? I have
> a
> > > > > > feeling
> > > > > > > zoo may do what I want, but I'm new to zoo ...
> > > > > > >
> > > > > > > Cheers
> > > > > > > Rory
> > > > > > >
> > > > > > >        [[alternative HTML version deleted]]
> > > > > > >
> > > > > > > _______________________________________________
> > > > > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > > > >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > > > -- Subscriber-posting only.
> > > > > > > -- If you want to post, subscribe first.
> > > > > > >
> > > > > >
> > > > >
> > > > >        [[alternative HTML version deleted]]
> > > > >
> > > > > _______________________________________________
> > > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > -- Subscriber-posting only.
> > > > > -- If you want to post, subscribe first.
> > > > >
> > > >
> > >
> > >
> >
>
>


From ggrothendieck at gmail.com  Fri Aug  3 16:36:03 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 3 Aug 2007 10:36:03 -0400
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <971536df0708030729y23d9941dqaa81d1f38c64cfdd@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
	<971536df0708010616h39d87c1ei951a59abcd9812b2@mail.gmail.com>
	<3f446aa30708030144s477cd70ahcdc21dc20d47f654@mail.gmail.com>
	<971536df0708030335g3b9aa867h7a631299e309f764@mail.gmail.com>
	<3f446aa30708030506m6439a41do53377c3cd39e076e@mail.gmail.com>
	<971536df0708030520o492b060ao10bbac04bb3867e5@mail.gmail.com>
	<3f446aa30708030615v121e4b4bn6152d5af6545eb33@mail.gmail.com>
	<971536df0708030729y23d9941dqaa81d1f38c64cfdd@mail.gmail.com>
Message-ID: <971536df0708030736j4169fd12s9787675e060dac0b@mail.gmail.com>

I had omitted fmt and epoch.  tmp is as in your post.

twas <- function(x) {
	y <- data.frame(timediff = diff(x$time), head(x, -1))
	aggregate(100 * y[1]/sum(y[1]), y[c("hour", "spread")], sum)
}
now <- Sys.time()
epoch <- now - as.numeric(now)
fmt <- function(x) format(epoch + x, "%H")
tmp2 <- cbind(tmp, hour = fmt(tmp$time))

z <- do.call("rbind", by(tmp2, tmp2["hour"], twas))

# three alternatives

# 1
xtabs(timediff ~., z)

# 2
reshape(z, dir = "wide", timevar = "spread", idvar = "hour")

# 3
library(reshape)
cast(melt(z, id = 1:2), hour ~ spread)


On 8/3/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try producing it in "long" format using aggregate and then reshaping
> it into "wide" format using xtabs, reshape or the reshape package:
>
> twas <- function(x) {
>        y <- data.frame(timediff = diff(x$time), head(x, -1))
>        aggregate(100 * y[1]/sum(y[1]), y[c("hour", "spread")], sum)
> }
> tmp2 <- cbind(tmp, hour = fmt(tmp$time))
> long <- do.call("rbind", by(tmp2, tmp2["hour"], twas))
>
> # any one of these three:
>
> xtabs(timediff ~., long)
>
> reshape(long, dir = "wide", timevar = "spread", idvar = "hour")
>
> library(reshape)
> cast(melt(long, id = 1:2), hour ~ spread)
>
>
> On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> > Hi
> >
> > Sorry, I'm not sure what happened with that last one. Here is a fully
> > contained example (sorry about the line length if this doesnt wrap).
> >
> > tmp <- data.frame(
> > time=c(1185882786,1185882790,1185882791,1185882791,1185882792,1185882795,1185882796,1185882797,1185882797,1185882798,1185882799,1185882800,1185882806,1185882807,1185882809,1185882810,1185882810,1185882811,1185882845,1185882846,1185882906,1185882918,1185882950,1185882951,1185882951,1185882952,1185882953,1185882954,1185882955,1185882956,1185882991,1185882991,1185882995,1185882996,1185882997,1185882997,1185882998,1185882998,1185882999,1185883003,1185883004,1185883006,1185883007,1185883025,1185883026,1185883086,1185883129,1185883129,1185883133,1185883133,1185883137,1185883137,1185883144,1185883145,1185883145,1185883148,1185883148,1185883149,1185883150,1185883151,1185883152,1185883154,1185883154,1185883155,1185883155,1185883175,1185883176,1185883179,1185883179,1185883180,1185883181,1185883181,1185883182,1185883186,1185883187,1185883191,1185883191,1185883200,1185883200,1185883211,1185883212,1185883214,1185883214,1185883215,1185883217,1185883218,1185883219,1185883279,1185883307,1185883307,1185883365,1185883366,1185883366,1185883367,1185883368,1185883368,1185883368,1185883369,1185883373,1185883376),
> > spread=c(1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,3e-04,2e-04)
> > )
> >
> > twas <- function (dat)
> > {
> >     data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > sum)/sum(diff(dat$time)) * 100)
> > }
> >
> > now <- Sys.time()
> > epoch <- now - as.numeric(now)
> >
> > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> >
> > Cheers
> > Rory
> >
> >
> > On 8/3/07, Gabor Grothendieck < ggrothendieck at gmail.com> wrote:
> > > I still get no warning.  Please provide complete self contained input
> > > and output.
> > >
> > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > 1185882791,
> > > +  1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > +  2e-04, 3e-04))
> > > >
> > > > twas <-
> > > +  function(dat) {
> > > +    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > +  sum)/sum(diff(dat$time)) * 100.0)
> > > + }
> > > > now <- Sys.time()
> > > > epoch <- now - as.numeric(now)
> > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > > > z
> > >       1e-04    2e-04
> > > 07 66.66667 33.33333
> > > >
> > > > R.version.string # XP
> > > [1] "R version 2.5.1 (2007-06-27)"
> > >
> > >
> > > On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> > > > Hi
> > > >
> > > > I have figured out what causes the warning (and recycling), but I am not
> > > > sure how I can fix it. After seeing that it seemed to work for you, I
> > went
> > > > back and tried working with different subsets of the data. I eventually
> > > > found where it occurs - when we get a third unique spread value. To
> > > > reproduce, just change the definition of tmp to be:
> > > >
> > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > 1185882791,
> > > >  1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > >  2e-04, 3e-04)) <== Added 3e-04
> > > >
> > > > i.e. I have just changed one of the spread values to be a third value -
> > this
> > > > seems to trigger the warning  "Warning message:number of columns of
> > result
> > > > is not a multiple of vector length (arg 3) in: rbind", and the
> > recycling. I
> > > > tried this on R 2.5.0 and 2.5.1
> > > >
> > > > Can anyone see what I am doing wrong here?
> > > >
> > > > Cheers
> > > > Rory
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > On 8/3/07, Gabor Grothendieck < ggrothendieck at gmail.com> wrote:
> > > > > Can you provide a reproducible example that exhibits the warning.
> > > > > Redoing it in a more easily reproducible way and using the data
> > > > > in your post gives me no warning
> > > > >
> > > > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > > > 1185882791,
> > > > > + 1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > > > + 2e-04, 1e-04))
> > > > > >
> > > > > > twas <-
> > > > > +  function(dat) {
> > > > > +     data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > +  sum)/sum(diff(dat$time)) * 100.0)
> > > > > + }
> > > > > > now <- Sys.time()
> > > > > > epoch <- now - as.numeric(now)
> > > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > > > > > z
> > > > >       1e-04    2e-04
> > > > > 07 66.66667 33.33333
> > > > > > R.version.string # XP
> > > > > [1] "R version 2.5.1 (2007-06-27)"
> > > > >
> > > > >
> > > > > Here is input:
> > > > >
> > > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > 1185882791,
> > > > > 1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > > > 2e-04, 1e-04))
> > > > > twas <-
> > > > > function(dat) {
> > > > >    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > sum)/sum(diff(dat$time)) * 100.0)
> > > > > }
> > > > > now <- Sys.time()
> > > > > epoch <- now - as.numeric(now)
> > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > > > > z
> > > > > R.version.string # XP
> > > > >
> > > > >
> > > > >
> > > > > On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> > > > > > Hi
> > > > > >
> > > > > > I've been wrestling with this a little bit, using the example in the
> > > > email
> > > > > > that Gabor pointed me to as a reference, and I think I have almost
> > got
> > > > what
> > > > > > I want...however its still not quite right.
> > > > > >
> > > > > > I have a variable, tmp, with two dimensions: time and spread:
> > > > > >
> > > > > > > head(tmp$time)
> > > > > > [1] 1185882786 1185882790 1185882791 1185882791 1185882792
> > 1185882795
> > > > > >
> > > > > > > head(tmp$spread)
> > > > > > [1] 1e-04 1e-04 2e-04 1e-04 2e-04 1e-04
> > > > > > >
> > > > > >
> > > > > > I also have a function that calculates the time-weighted average
> > spread:
> > > > > >
> > > > > > > twas
> > > > > > function(dat) {
> > > > > >   data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > > sum)/sum(diff(dat$time)) * 100.0)
> > > > > > }
> > > > > >
> > > > > > I can combine them using as rbind() and by():
> > > > > >
> > > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > > > > >
> > > > > > (epoch is just an instance of ISOdatetime)
> > > > > >
> > > > > > This gives me a warning:
> > > > > >
> > > > > > Warning message:
> > > > > > number of columns of result
> > > > > >        is not a multiple of vector length (arg 3) in: rbind(1, "12"
> > = c(
> > > > > > 91.99207541277 , 8.00792458723005), "13" = c(90.1884966797708,
> > > > > >
> > > > > > The output from the above command is almost exactly what I need,
> > apart
> > > > from
> > > > > > the recycling:
> > > > > >
> > > > > >      1e-04     2e-04      3e-04        4e-04
> > > > > > 12 91.99208  8.007925 91.9920754  8.007924587 <== recycled values
> > > > > > 13 90.18850  9.337448  0.4218405  0.052214551
> > > > > > 14 90.59640  9.171417  0.2321811 90.596401668
> > > > > > 15 89.55771 10.194291  0.2343418  0.013661453
> > > > > > ...
> > > > > >
> > > > > > I can just pass this into a barplot() and get a nice visual
> > breakdown of
> > > > > > hourly weighted spreads, *but* I dont know how to get these results
> > > > without
> > > > > > the recycling. Looking at rbind(), it seems that this will
> > automatically
> > > > > > recycle. Does anyone know of a function I could use to get these
> > results
> > > > > > without this problem?
> > > > > >
> > > > > > Cheers
> > > > > > Rory
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > > On 8/1/07, Gabor Grothendieck < ggrothendieck at gmail.com > wrote:
> > > > > > >
> > > > > > > Something similar was just discussed this morning:
> > > > > > >
> > > >
> > https://www.stat.math.ethz.ch/pipermail/r-help/2007-August/137695.html
> > > > > > >
> > > > > > >
> > > > > > > On 8/1/07, Rory Winston <rory.winston at gmail.com > wrote:
> > > > > > > > Hi all
> > > > > > > >
> > > > > > > > I have a question about aggegating statistics by time intervals.
> > I
> > > > have
> > > > > > > a
> > > > > > > > data set with 3 columns : time, bid, and ask. Time is specified
> > as a
> > > > > > > > millisecond timestamp since epoch. I would like to compute
> > summary
> > > > > > > > statistics for the data set on an hourly basis. Here is what I
> > have
> > > > > > > tried so
> > > > > > > > far:
> > > > > > > >
> > > > > > > > # Data is in pricedata
> > > > > > > >
> > > > > > > > t <- ISODatetime(1970, 1, 1, 0, 0, 0) + pricedata$time
> > > > > > > > agg <- aggregate(pricedata$spread, list(byhour=format(t, "%Y-%m
> > > > %H")),
> > > > > > > mean)
> > > > > > > >
> > > > > > > > This seems to do what I want - however, what really want to do
> > is
> > > > more
> > > > > > > > specific: I would like to be able to extract a subset of the
> > data
> > > > frame
> > > > > > > > pricedata, and not just the aggregated entries - for instance,
> > > > instead
> > > > > > > of
> > > > > > > > just extracting pricedata$spread by hour, I would like to
> > extract a
> > > > > > > slice of
> > > > > > > > columns, e.g. pricedata$spread and pricedata$time on an hourly
> > > > basis,
> > > > > > > and
> > > > > > > > pass these into a function that can compute a time-weighted
> > average
> > > > > > > spread,
> > > > > > > > for instance. Does anyone know an elegant way to do this? I have
> > a
> > > > > > > feeling
> > > > > > > > zoo may do what I want, but I'm new to zoo ...
> > > > > > > >
> > > > > > > > Cheers
> > > > > > > > Rory
> > > > > > > >
> > > > > > > >        [[alternative HTML version deleted]]
> > > > > > > >
> > > > > > > > _______________________________________________
> > > > > > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > > > > >
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > > > > -- Subscriber-posting only.
> > > > > > > > -- If you want to post, subscribe first.
> > > > > > > >
> > > > > > >
> > > > > >
> > > > > >        [[alternative HTML version deleted]]
> > > > > >
> > > > > > _______________________________________________
> > > > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > > -- Subscriber-posting only.
> > > > > > -- If you want to post, subscribe first.
> > > > > >
> > > > >
> > > >
> > > >
> > >
> >
> >
>


From rory.winston at gmail.com  Fri Aug  3 16:54:39 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Fri, 3 Aug 2007 15:54:39 +0100
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <971536df0708030736j4169fd12s9787675e060dac0b@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
	<971536df0708010616h39d87c1ei951a59abcd9812b2@mail.gmail.com>
	<3f446aa30708030144s477cd70ahcdc21dc20d47f654@mail.gmail.com>
	<971536df0708030335g3b9aa867h7a631299e309f764@mail.gmail.com>
	<3f446aa30708030506m6439a41do53377c3cd39e076e@mail.gmail.com>
	<971536df0708030520o492b060ao10bbac04bb3867e5@mail.gmail.com>
	<3f446aa30708030615v121e4b4bn6152d5af6545eb33@mail.gmail.com>
	<971536df0708030729y23d9941dqaa81d1f38c64cfdd@mail.gmail.com>
	<971536df0708030736j4169fd12s9787675e060dac0b@mail.gmail.com>
Message-ID: <3f446aa30708030754s5ffab998u617b19430d73fe09@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070803/16411395/attachment.pl 

From ggrothendieck at gmail.com  Fri Aug  3 17:01:54 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 3 Aug 2007 11:01:54 -0400
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <3f446aa30708030754s5ffab998u617b19430d73fe09@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
	<971536df0708010616h39d87c1ei951a59abcd9812b2@mail.gmail.com>
	<3f446aa30708030144s477cd70ahcdc21dc20d47f654@mail.gmail.com>
	<971536df0708030335g3b9aa867h7a631299e309f764@mail.gmail.com>
	<3f446aa30708030506m6439a41do53377c3cd39e076e@mail.gmail.com>
	<971536df0708030520o492b060ao10bbac04bb3867e5@mail.gmail.com>
	<3f446aa30708030615v121e4b4bn6152d5af6545eb33@mail.gmail.com>
	<971536df0708030729y23d9941dqaa81d1f38c64cfdd@mail.gmail.com>
	<971536df0708030736j4169fd12s9787675e060dac0b@mail.gmail.com>
	<3f446aa30708030754s5ffab998u617b19430d73fe09@mail.gmail.com>
Message-ID: <971536df0708030801l46fbf989sab1f5cff466fca49@mail.gmail.com>

The different invocations of twas were creating data frames of different
numbers of columns because different hours had different numbers
of spreads.  The warning came when it tried to rbind together
data.frames with different numbers of columns.

On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> Wow....thats great. Thank you very much! I appreciate the help greatly. I
> dont quite understand what the issue was though....was it that the data
> frame returned from my initial twas() function was of the wrong order?
>
>
> Cheers
> Rory
>
> On 8/3/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > I had omitted fmt and epoch.  tmp is as in your post.
> >
> > twas <- function(x) {
> >         y <- data.frame(timediff = diff(x$time), head(x, -1))
> >         aggregate(100 * y[1]/sum(y[1]), y[c("hour", "spread")], sum)
> > }
> > now <- Sys.time()
> > epoch <- now - as.numeric(now)
> > fmt <- function(x) format(epoch + x, "%H")
> > tmp2 <- cbind(tmp, hour = fmt(tmp$time))
> >
> > z <- do.call("rbind", by(tmp2, tmp2["hour"], twas))
> >
> > # three alternatives
> >
> > # 1
> > xtabs(timediff ~., z)
> >
> > # 2
> > reshape(z, dir = "wide", timevar = "spread", idvar = "hour")
> >
> > # 3
> > library(reshape)
> > cast(melt(z, id = 1:2), hour ~ spread)
> >
> >
> > On 8/3/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > Try producing it in "long" format using aggregate and then reshaping
> > > it into "wide" format using xtabs, reshape or the reshape package:
> > >
> > > twas <- function(x) {
> > >        y <- data.frame(timediff = diff(x$time), head(x, -1))
> > >        aggregate(100 * y[1]/sum(y[1]), y[c("hour", "spread")], sum)
> > > }
> > > tmp2 <- cbind(tmp, hour = fmt(tmp$time))
> > > long <- do.call("rbind", by(tmp2, tmp2["hour"], twas))
> > >
> > > # any one of these three:
> > >
> > > xtabs(timediff ~., long)
> > >
> > > reshape(long, dir = "wide", timevar = "spread", idvar = "hour")
> > >
> > > library(reshape)
> > > cast(melt(long, id = 1:2), hour ~ spread)
> > >
> > >
> > > On 8/3/07, Rory Winston < rory.winston at gmail.com> wrote:
> > > > Hi
> > > >
> > > > Sorry, I'm not sure what happened with that last one. Here is a fully
> > > > contained example (sorry about the line length if this doesnt wrap).
> > > >
> > > > tmp <- data.frame(
> > > >
> time=c(1185882786,1185882790,1185882791,1185882791,1185882792,1185882795,1185882796,1185882797,1185882797,1185882798,1185882799,1185882800,1185882806,1185882807,1185882809,1185882810,1185882810,1185882811,1185882845,1185882846,1185882906,1185882918,1185882950,1185882951,1185882951,1185882952,1185882953,1185882954,1185882955,1185882956,1185882991,1185882991,1185882995,1185882996,1185882997,1185882997,1185882998,1185882998,1185882999,1185883003,1185883004,1185883006,1185883007,1185883025,1185883026,1185883086,1185883129,1185883129,1185883133,1185883133,1185883137,1185883137,1185883144,1185883145,1185883145,1185883148,1185883148,1185883149,1185883150,1185883151,1185883152,1185883154,1185883154,1185883155,1185883155,1185883175,1185883176,1185883179,1185883179,1185883180,1185883181,1185883181,1185883182,1185883186,1185883187,1185883191,1185883191,1185883200,1185883200,1185883211,1185883212,1185883214,1185883214,1185883215,1185883217,1185883218,1185883219,1185883279,1185883307,1185883307,1185883365,1185883366,1185883366,1185883367,1185883368,1185883368,1185883368,1185883369,1185883373,1185883376),
> > > >
> spread=c(1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,3e-04,2e-04)
> > > > )
> > > >
> > > > twas <- function (dat)
> > > > {
> > > >     data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > sum)/sum(diff(dat$time)) * 100)
> > > > }
> > > >
> > > > now <- Sys.time()
> > > > epoch <- now - as.numeric(now)
> > > >
> > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > > >
> > > > Cheers
> > > > Rory
> > > >
> > > >
> > > > On 8/3/07, Gabor Grothendieck < ggrothendieck at gmail.com> wrote:
> > > > > I still get no warning.  Please provide complete self contained
> input
> > > > > and output.
> > > > >
> > > > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > > > 1185882791,
> > > > > +  1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > > > +  2e-04, 3e-04))
> > > > > >
> > > > > > twas <-
> > > > > +  function(dat) {
> > > > > +    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > +  sum)/sum(diff(dat$time)) * 100.0)
> > > > > + }
> > > > > > now <- Sys.time()
> > > > > > epoch <- now - as.numeric(now)
> > > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"),
> twas))
> > > > > > z
> > > > >       1e-04    2e-04
> > > > > 07 66.66667 33.33333
> > > > > >
> > > > > > R.version.string # XP
> > > > > [1] "R version 2.5.1 (2007-06-27)"
> > > > >
> > > > >
> > > > > On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> > > > > > Hi
> > > > > >
> > > > > > I have figured out what causes the warning (and recycling), but I
> am not
> > > > > > sure how I can fix it. After seeing that it seemed to work for
> you, I
> > > > went
> > > > > > back and tried working with different subsets of the data. I
> eventually
> > > > > > found where it occurs - when we get a third unique spread value.
> To
> > > > > > reproduce, just change the definition of tmp to be:
> > > > > >
> > > > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > > > 1185882791,
> > > > > >  1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > > > >  2e-04, 3e-04)) <== Added 3e-04
> > > > > >
> > > > > > i.e. I have just changed one of the spread values to be a third
> value -
> > > > this
> > > > > > seems to trigger the warning  "Warning message:number of columns
> of
> > > > result
> > > > > > is not a multiple of vector length (arg 3) in: rbind", and the
> > > > recycling. I
> > > > > > tried this on R 2.5.0 and 2.5.1
> > > > > >
> > > > > > Can anyone see what I am doing wrong here?
> > > > > >
> > > > > > Cheers
> > > > > > Rory
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > > On 8/3/07, Gabor Grothendieck < ggrothendieck at gmail.com> wrote:
> > > > > > > Can you provide a reproducible example that exhibits the
> warning.
> > > > > > > Redoing it in a more easily reproducible way and using the data
> > > > > > > in your post gives me no warning
> > > > > > >
> > > > > > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > > > > > 1185882791,
> > > > > > > + 1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04,
> 1e-04,
> > > > > > > + 2e-04, 1e-04))
> > > > > > > >
> > > > > > > > twas <-
> > > > > > > +  function(dat) {
> > > > > > > +     data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > > > +  sum)/sum(diff(dat$time)) * 100.0)
> > > > > > > + }
> > > > > > > > now <- Sys.time()
> > > > > > > > epoch <- now - as.numeric(now)
> > > > > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"),
> twas))
> > > > > > > > z
> > > > > > >       1e-04    2e-04
> > > > > > > 07 66.66667 33.33333
> > > > > > > > R.version.string # XP
> > > > > > > [1] "R version 2.5.1 (2007-06-27)"
> > > > > > >
> > > > > > >
> > > > > > > Here is input:
> > > > > > >
> > > > > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > > > 1185882791,
> > > > > > > 1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > > > > > 2e-04, 1e-04))
> > > > > > > twas <-
> > > > > > > function(dat) {
> > > > > > >    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > > > sum)/sum(diff(dat$time)) * 100.0)
> > > > > > > }
> > > > > > > now <- Sys.time ()
> > > > > > > epoch <- now - as.numeric(now)
> > > > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"),
> twas))
> > > > > > > z
> > > > > > > R.version.string # XP
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > > On 8/3/07, Rory Winston <rory.winston at gmail.com > wrote:
> > > > > > > > Hi
> > > > > > > >
> > > > > > > > I've been wrestling with this a little bit, using the example
> in the
> > > > > > email
> > > > > > > > that Gabor pointed me to as a reference, and I think I have
> almost
> > > > got
> > > > > > what
> > > > > > > > I want...however its still not quite right.
> > > > > > > >
> > > > > > > > I have a variable, tmp, with two dimensions: time and spread:
> > > > > > > >
> > > > > > > > > head(tmp$time)
> > > > > > > > [1] 1185882786 1185882790 1185882791 1185882791 1185882792
> > > > 1185882795
> > > > > > > >
> > > > > > > > > head(tmp$spread)
> > > > > > > > [1] 1e-04 1e-04 2e-04 1e-04 2e-04 1e-04
> > > > > > > > >
> > > > > > > >
> > > > > > > > I also have a function that calculates the time-weighted
> average
> > > > spread:
> > > > > > > >
> > > > > > > > > twas
> > > > > > > > function(dat) {
> > > > > > > >   data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > > > > sum)/sum(diff(dat$time)) * 100.0)
> > > > > > > > }
> > > > > > > >
> > > > > > > > I can combine them using as rbind() and by():
> > > > > > > >
> > > > > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"),
> twas))
> > > > > > > >
> > > > > > > > (epoch is just an instance of ISOdatetime)
> > > > > > > >
> > > > > > > > This gives me a warning:
> > > > > > > >
> > > > > > > > Warning message:
> > > > > > > > number of columns of result
> > > > > > > >        is not a multiple of vector length (arg 3) in: rbind(1,
> "12"
> > > > = c(
> > > > > > > > 91.99207541277 , 8.00792458723005), "13" = c(90.1884966797708,
> > > > > > > >
> > > > > > > > The output from the above command is almost exactly what I
> need,
> > > > apart
> > > > > > from
> > > > > > > > the recycling:
> > > > > > > >
> > > > > > > >      1e-04     2e-04      3e-04        4e-04
> > > > > > > > 12 91.99208  8.007925 91.9920754  8.007924587 <== recycled
> values
> > > > > > > > 13 90.18850  9.337448  0.4218405  0.052214551
> > > > > > > > 14 90.59640  9.171417  0.2321811 90.596401668
> > > > > > > > 15 89.55771 10.194291  0.2343418  0.013661453
> > > > > > > > ...
> > > > > > > >
> > > > > > > > I can just pass this into a barplot() and get a nice visual
> > > > breakdown of
> > > > > > > > hourly weighted spreads, *but* I dont know how to get these
> results
> > > > > > without
> > > > > > > > the recycling. Looking at rbind(), it seems that this will
> > > > automatically
> > > > > > > > recycle. Does anyone know of a function I could use to get
> these
> > > > results
> > > > > > > > without this problem?
> > > > > > > >
> > > > > > > > Cheers
> > > > > > > > Rory
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > > On 8/1/07, Gabor Grothendieck < ggrothendieck at gmail.com >
> wrote:
> > > > > > > > >
> > > > > > > > > Something similar was just discussed this morning:
> > > > > > > > >
> > > > > >
> > > >
> https://www.stat.math.ethz.ch/pipermail/r-help/2007-August/137695.html
> > > > > > > > >
> > > > > > > > >
> > > > > > > > > On 8/1/07, Rory Winston <rory.winston at gmail.com > wrote:
> > > > > > > > > > Hi all
> > > > > > > > > >
> > > > > > > > > > I have a question about aggegating statistics by time
> intervals.
> > > > I
> > > > > > have
> > > > > > > > > a
> > > > > > > > > > data set with 3 columns : time, bid, and ask. Time is
> specified
> > > > as a
> > > > > > > > > > millisecond timestamp since epoch. I would like to compute
> > > > summary
> > > > > > > > > > statistics for the data set on an hourly basis. Here is
> what I
> > > > have
> > > > > > > > > tried so
> > > > > > > > > > far:
> > > > > > > > > >
> > > > > > > > > > # Data is in pricedata
> > > > > > > > > >
> > > > > > > > > > t <- ISODatetime(1970, 1, 1, 0, 0, 0) + pricedata$time
> > > > > > > > > > agg <- aggregate(pricedata$spread, list(byhour=format(t,
> "%Y-%m
> > > > > > %H")),
> > > > > > > > > mean)
> > > > > > > > > >
> > > > > > > > > > This seems to do what I want - however, what really want
> to do
> > > > is
> > > > > > more
> > > > > > > > > > specific: I would like to be able to extract a subset of
> the
> > > > data
> > > > > > frame
> > > > > > > > > > pricedata, and not just the aggregated entries - for
> instance,
> > > > > > instead
> > > > > > > > > of
> > > > > > > > > > just extracting pricedata$spread by hour, I would like to
> > > > extract a
> > > > > > > > > slice of
> > > > > > > > > > columns, e.g. pricedata$spread and pricedata$time on an
> hourly
> > > > > > basis,
> > > > > > > > > and
> > > > > > > > > > pass these into a function that can compute a
> time-weighted
> > > > average
> > > > > > > > > spread,
> > > > > > > > > > for instance. Does anyone know an elegant way to do this?
> I have
> > > > a
> > > > > > > > > feeling
> > > > > > > > > > zoo may do what I want, but I'm new to zoo ...
> > > > > > > > > >
> > > > > > > > > > Cheers
> > > > > > > > > > Rory
> > > > > > > > > >
> > > > > > > > > >        [[alternative HTML version deleted]]
> > > > > > > > > >
> > > > > > > > > >
> _______________________________________________
> > > > > > > > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > > > > > > >
> > > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > > > > > > -- Subscriber-posting only.
> > > > > > > > > > -- If you want to post, subscribe first.
> > > > > > > > > >
> > > > > > > > >
> > > > > > > >
> > > > > > > >        [[alternative HTML version deleted]]
> > > > > > > >
> > > > > > > >
> _______________________________________________
> > > > > > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > > > > >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > > > > -- Subscriber-posting only.
> > > > > > > > -- If you want to post, subscribe first.
> > > > > > > >
> > > > > > >
> > > > > >
> > > > > >
> > > > >
> > > >
> > > >
> > >
> >
>
>


From ggrothendieck at gmail.com  Fri Aug  3 19:15:08 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 3 Aug 2007 13:15:08 -0400
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <971536df0708030801l46fbf989sab1f5cff466fca49@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
	<3f446aa30708030144s477cd70ahcdc21dc20d47f654@mail.gmail.com>
	<971536df0708030335g3b9aa867h7a631299e309f764@mail.gmail.com>
	<3f446aa30708030506m6439a41do53377c3cd39e076e@mail.gmail.com>
	<971536df0708030520o492b060ao10bbac04bb3867e5@mail.gmail.com>
	<3f446aa30708030615v121e4b4bn6152d5af6545eb33@mail.gmail.com>
	<971536df0708030729y23d9941dqaa81d1f38c64cfdd@mail.gmail.com>
	<971536df0708030736j4169fd12s9787675e060dac0b@mail.gmail.com>
	<3f446aa30708030754s5ffab998u617b19430d73fe09@mail.gmail.com>
	<971536df0708030801l46fbf989sab1f5cff466fca49@mail.gmail.com>
Message-ID: <971536df0708031015l1a683c9cpb3c70496768a9146@mail.gmail.com>

Here is one more solution.  Using the tmp from your post, this one uses
SQLite via the sqldf package.  It produces a similar output as z from
our prior solution and then
we can use xtabs, reshape or the reshape package as before to get
the final layout.  The first subselect within the main select sums within
hour and spread and the second sums within hour.  We join the subselects
and take the ratio of the two the sums to get the answer.

In the solutions before we used hour relative to GMT rather than local time.

> library(sqldf)
> sqldf("select ahour, spread,
+   100 * aa.timediff / bb.timediff timediff from
+   (select
+     strftime('%H',a.time__1,'unixepoch') ahour,
+     strftime('%H',b.time__1,'unixepoch') bhour,
+     a.spread spread,
+     sum(b.time__1 - a.time__1) timediff
+    from tmp a, tmp b
+    where a.row_names = b.row_names-1 and ahour = bhour
+    group by ahour, a.spread) aa join
+    (select strftime('%H',c.time__1,'unixepoch') chour,
+       strftime('%H',d.time__1,'unixepoch') dhour,
+       sum(d.time__1 - c.time__1) timediff
+       from tmp c, tmp d
+       where c.row_names = d.row_names-1 and chour = dhour
+       group by chour) bb
+     where ahour = chour
+     group by spread, ahour",
+  row.names = TRUE)
  ahour spread  timediff
1    11  1e-04 91.358025
2    12  1e-04 92.613636
3    11  2e-04  8.641975
4    12  2e-04  5.681818
5    12  3e-04  1.704545

> # old solution for comparison
> twas <- function(x) {
+ y <- data.frame(timediff = diff(x$time), head(x, -1))
+ aggregate(100 * y[1]/sum(y[1]), y[c("hour", "spread")], sum)
+ }
> now <- Sys.time()
> epoch <- now - as.numeric(now)
> fmt <- function(x) format(epoch + x, "%H", tz = "GMT")
> tmp2 <- cbind(tmp, hour = fmt(tmp$time))
>
> z <- do.call("rbind", by(tmp2, tmp2["hour"], twas))
> z
     hour spread  timediff
11.1   11  1e-04 91.358025
11.2   11  2e-04  8.641975
12.1   12  1e-04 92.613636
12.2   12  2e-04  5.681818
12.3   12  3e-04  1.704545

Here is input:

library(sqldf)
sqldf("select ahour, spread,
	100 * aa.timediff / bb.timediff timediff from
	(select
	strftime('%H',a.time__1,'unixepoch') ahour,
	strftime('%H',b.time__1,'unixepoch') bhour,
	a.spread spread,
	sum(b.time__1 - a.time__1) timediff
	from tmp a, tmp b
	where a.row_names = b.row_names-1 and ahour = bhour
	group by ahour, a.spread) aa join
	(select strftime('%H',c.time__1,'unixepoch') chour,
		strftime('%H',d.time__1,'unixepoch') dhour,
		sum(d.time__1 - c.time__1) timediff
		from tmp c, tmp d
		where c.row_names = d.row_names-1 and chour = dhour
		group by chour) bb
	where ahour = chour
	group by spread, ahour",
	row.names = TRUE)




On 8/3/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> The different invocations of twas were creating data frames of different
> numbers of columns because different hours had different numbers
> of spreads.  The warning came when it tried to rbind together
> data.frames with different numbers of columns.
>
> On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> > Wow....thats great. Thank you very much! I appreciate the help greatly. I
> > dont quite understand what the issue was though....was it that the data
> > frame returned from my initial twas() function was of the wrong order?
> >
> >
> > Cheers
> > Rory
> >
> > On 8/3/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > I had omitted fmt and epoch.  tmp is as in your post.
> > >
> > > twas <- function(x) {
> > >         y <- data.frame(timediff = diff(x$time), head(x, -1))
> > >         aggregate(100 * y[1]/sum(y[1]), y[c("hour", "spread")], sum)
> > > }
> > > now <- Sys.time()
> > > epoch <- now - as.numeric(now)
> > > fmt <- function(x) format(epoch + x, "%H")
> > > tmp2 <- cbind(tmp, hour = fmt(tmp$time))
> > >
> > > z <- do.call("rbind", by(tmp2, tmp2["hour"], twas))
> > >
> > > # three alternatives
> > >
> > > # 1
> > > xtabs(timediff ~., z)
> > >
> > > # 2
> > > reshape(z, dir = "wide", timevar = "spread", idvar = "hour")
> > >
> > > # 3
> > > library(reshape)
> > > cast(melt(z, id = 1:2), hour ~ spread)
> > >
> > >
> > > On 8/3/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > Try producing it in "long" format using aggregate and then reshaping
> > > > it into "wide" format using xtabs, reshape or the reshape package:
> > > >
> > > > twas <- function(x) {
> > > >        y <- data.frame(timediff = diff(x$time), head(x, -1))
> > > >        aggregate(100 * y[1]/sum(y[1]), y[c("hour", "spread")], sum)
> > > > }
> > > > tmp2 <- cbind(tmp, hour = fmt(tmp$time))
> > > > long <- do.call("rbind", by(tmp2, tmp2["hour"], twas))
> > > >
> > > > # any one of these three:
> > > >
> > > > xtabs(timediff ~., long)
> > > >
> > > > reshape(long, dir = "wide", timevar = "spread", idvar = "hour")
> > > >
> > > > library(reshape)
> > > > cast(melt(long, id = 1:2), hour ~ spread)
> > > >
> > > >
> > > > On 8/3/07, Rory Winston < rory.winston at gmail.com> wrote:
> > > > > Hi
> > > > >
> > > > > Sorry, I'm not sure what happened with that last one. Here is a fully
> > > > > contained example (sorry about the line length if this doesnt wrap).
> > > > >
> > > > > tmp <- data.frame(
> > > > >
> > time=c(1185882786,1185882790,1185882791,1185882791,1185882792,1185882795,1185882796,1185882797,1185882797,1185882798,1185882799,1185882800,1185882806,1185882807,1185882809,1185882810,1185882810,1185882811,1185882845,1185882846,1185882906,1185882918,1185882950,1185882951,1185882951,1185882952,1185882953,1185882954,1185882955,1185882956,1185882991,1185882991,1185882995,1185882996,1185882997,1185882997,1185882998,1185882998,1185882999,1185883003,1185883004,1185883006,1185883007,1185883025,1185883026,1185883086,1185883129,1185883129,1185883133,1185883133,1185883137,1185883137,1185883144,1185883145,1185883145,1185883148,1185883148,1185883149,1185883150,1185883151,1185883152,1185883154,1185883154,1185883155,1185883155,1185883175,1185883176,1185883179,1185883179,1185883180,1185883181,1185883181,1185883182,1185883186,1185883187,1185883191,1185883191,1185883200,1185883200,1185883211,1185883212,1185883214,1185883214,1185883215,1185883217,1185883218,1185883219,1185883279,1185883307,1185883307,1185883365,1185883366,1185883366,1185883367,1185883368,1185883368,1185883368,1185883369,1185883373,1185883376),
> > > > >
> > spread=c(1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,1e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,1e-04,2e-04,1e-04,2e-04,1e-04,2e-04,3e-04,2e-04)
> > > > > )
> > > > >
> > > > > twas <- function (dat)
> > > > > {
> > > > >     data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > sum)/sum(diff(dat$time)) * 100)
> > > > > }
> > > > >
> > > > > now <- Sys.time()
> > > > > epoch <- now - as.numeric(now)
> > > > >
> > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"), twas))
> > > > >
> > > > > Cheers
> > > > > Rory
> > > > >
> > > > >
> > > > > On 8/3/07, Gabor Grothendieck < ggrothendieck at gmail.com> wrote:
> > > > > > I still get no warning.  Please provide complete self contained
> > input
> > > > > > and output.
> > > > > >
> > > > > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > > > > 1185882791,
> > > > > > +  1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > > > > +  2e-04, 3e-04))
> > > > > > >
> > > > > > > twas <-
> > > > > > +  function(dat) {
> > > > > > +    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > > +  sum)/sum(diff(dat$time)) * 100.0)
> > > > > > + }
> > > > > > > now <- Sys.time()
> > > > > > > epoch <- now - as.numeric(now)
> > > > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"),
> > twas))
> > > > > > > z
> > > > > >       1e-04    2e-04
> > > > > > 07 66.66667 33.33333
> > > > > > >
> > > > > > > R.version.string # XP
> > > > > > [1] "R version 2.5.1 (2007-06-27)"
> > > > > >
> > > > > >
> > > > > > On 8/3/07, Rory Winston <rory.winston at gmail.com> wrote:
> > > > > > > Hi
> > > > > > >
> > > > > > > I have figured out what causes the warning (and recycling), but I
> > am not
> > > > > > > sure how I can fix it. After seeing that it seemed to work for
> > you, I
> > > > > went
> > > > > > > back and tried working with different subsets of the data. I
> > eventually
> > > > > > > found where it occurs - when we get a third unique spread value.
> > To
> > > > > > > reproduce, just change the definition of tmp to be:
> > > > > > >
> > > > > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > > > > 1185882791,
> > > > > > >  1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > > > > >  2e-04, 3e-04)) <== Added 3e-04
> > > > > > >
> > > > > > > i.e. I have just changed one of the spread values to be a third
> > value -
> > > > > this
> > > > > > > seems to trigger the warning  "Warning message:number of columns
> > of
> > > > > result
> > > > > > > is not a multiple of vector length (arg 3) in: rbind", and the
> > > > > recycling. I
> > > > > > > tried this on R 2.5.0 and 2.5.1
> > > > > > >
> > > > > > > Can anyone see what I am doing wrong here?
> > > > > > >
> > > > > > > Cheers
> > > > > > > Rory
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > > On 8/3/07, Gabor Grothendieck < ggrothendieck at gmail.com> wrote:
> > > > > > > > Can you provide a reproducible example that exhibits the
> > warning.
> > > > > > > > Redoing it in a more easily reproducible way and using the data
> > > > > > > > in your post gives me no warning
> > > > > > > >
> > > > > > > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > > > > > > 1185882791,
> > > > > > > > + 1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04,
> > 1e-04,
> > > > > > > > + 2e-04, 1e-04))
> > > > > > > > >
> > > > > > > > > twas <-
> > > > > > > > +  function(dat) {
> > > > > > > > +     data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > > > > +  sum)/sum(diff(dat$time)) * 100.0)
> > > > > > > > + }
> > > > > > > > > now <- Sys.time()
> > > > > > > > > epoch <- now - as.numeric(now)
> > > > > > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"),
> > twas))
> > > > > > > > > z
> > > > > > > >       1e-04    2e-04
> > > > > > > > 07 66.66667 33.33333
> > > > > > > > > R.version.string # XP
> > > > > > > > [1] "R version 2.5.1 (2007-06-27)"
> > > > > > > >
> > > > > > > >
> > > > > > > > Here is input:
> > > > > > > >
> > > > > > > > tmp <- data.frame(time = c(1185882786, 1185882790, 1185882791,
> > > > > 1185882791,
> > > > > > > > 1185882792, 1185882795), spread = c(1e-04, 1e-04, 2e-04, 1e-04,
> > > > > > > > 2e-04, 1e-04))
> > > > > > > > twas <-
> > > > > > > > function(dat) {
> > > > > > > >    data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > > > > sum)/sum(diff(dat$time)) * 100.0)
> > > > > > > > }
> > > > > > > > now <- Sys.time ()
> > > > > > > > epoch <- now - as.numeric(now)
> > > > > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"),
> > twas))
> > > > > > > > z
> > > > > > > > R.version.string # XP
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > > On 8/3/07, Rory Winston <rory.winston at gmail.com > wrote:
> > > > > > > > > Hi
> > > > > > > > >
> > > > > > > > > I've been wrestling with this a little bit, using the example
> > in the
> > > > > > > email
> > > > > > > > > that Gabor pointed me to as a reference, and I think I have
> > almost
> > > > > got
> > > > > > > what
> > > > > > > > > I want...however its still not quite right.
> > > > > > > > >
> > > > > > > > > I have a variable, tmp, with two dimensions: time and spread:
> > > > > > > > >
> > > > > > > > > > head(tmp$time)
> > > > > > > > > [1] 1185882786 1185882790 1185882791 1185882791 1185882792
> > > > > 1185882795
> > > > > > > > >
> > > > > > > > > > head(tmp$spread)
> > > > > > > > > [1] 1e-04 1e-04 2e-04 1e-04 2e-04 1e-04
> > > > > > > > > >
> > > > > > > > >
> > > > > > > > > I also have a function that calculates the time-weighted
> > average
> > > > > spread:
> > > > > > > > >
> > > > > > > > > > twas
> > > > > > > > > function(dat) {
> > > > > > > > >   data.frame(tapply(diff(dat$time), head(dat$spread, -1),
> > > > > > > > > sum)/sum(diff(dat$time)) * 100.0)
> > > > > > > > > }
> > > > > > > > >
> > > > > > > > > I can combine them using as rbind() and by():
> > > > > > > > >
> > > > > > > > > z <- do.call("rbind", by(tmp, format(epoch + tmp$time, "%H"),
> > twas))
> > > > > > > > >
> > > > > > > > > (epoch is just an instance of ISOdatetime)
> > > > > > > > >
> > > > > > > > > This gives me a warning:
> > > > > > > > >
> > > > > > > > > Warning message:
> > > > > > > > > number of columns of result
> > > > > > > > >        is not a multiple of vector length (arg 3) in: rbind(1,
> > "12"
> > > > > = c(
> > > > > > > > > 91.99207541277 , 8.00792458723005), "13" = c(90.1884966797708,
> > > > > > > > >
> > > > > > > > > The output from the above command is almost exactly what I
> > need,
> > > > > apart
> > > > > > > from
> > > > > > > > > the recycling:
> > > > > > > > >
> > > > > > > > >      1e-04     2e-04      3e-04        4e-04
> > > > > > > > > 12 91.99208  8.007925 91.9920754  8.007924587 <== recycled
> > values
> > > > > > > > > 13 90.18850  9.337448  0.4218405  0.052214551
> > > > > > > > > 14 90.59640  9.171417  0.2321811 90.596401668
> > > > > > > > > 15 89.55771 10.194291  0.2343418  0.013661453
> > > > > > > > > ...
> > > > > > > > >
> > > > > > > > > I can just pass this into a barplot() and get a nice visual
> > > > > breakdown of
> > > > > > > > > hourly weighted spreads, *but* I dont know how to get these
> > results
> > > > > > > without
> > > > > > > > > the recycling. Looking at rbind(), it seems that this will
> > > > > automatically
> > > > > > > > > recycle. Does anyone know of a function I could use to get
> > these
> > > > > results
> > > > > > > > > without this problem?
> > > > > > > > >
> > > > > > > > > Cheers
> > > > > > > > > Rory
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > > On 8/1/07, Gabor Grothendieck < ggrothendieck at gmail.com >
> > wrote:
> > > > > > > > > >
> > > > > > > > > > Something similar was just discussed this morning:
> > > > > > > > > >
> > > > > > >
> > > > >
> > https://www.stat.math.ethz.ch/pipermail/r-help/2007-August/137695.html
> > > > > > > > > >
> > > > > > > > > >
> > > > > > > > > > On 8/1/07, Rory Winston <rory.winston at gmail.com > wrote:
> > > > > > > > > > > Hi all
> > > > > > > > > > >
> > > > > > > > > > > I have a question about aggegating statistics by time
> > intervals.
> > > > > I
> > > > > > > have
> > > > > > > > > > a
> > > > > > > > > > > data set with 3 columns : time, bid, and ask. Time is
> > specified
> > > > > as a
> > > > > > > > > > > millisecond timestamp since epoch. I would like to compute
> > > > > summary
> > > > > > > > > > > statistics for the data set on an hourly basis. Here is
> > what I
> > > > > have
> > > > > > > > > > tried so
> > > > > > > > > > > far:
> > > > > > > > > > >
> > > > > > > > > > > # Data is in pricedata
> > > > > > > > > > >
> > > > > > > > > > > t <- ISODatetime(1970, 1, 1, 0, 0, 0) + pricedata$time
> > > > > > > > > > > agg <- aggregate(pricedata$spread, list(byhour=format(t,
> > "%Y-%m
> > > > > > > %H")),
> > > > > > > > > > mean)
> > > > > > > > > > >
> > > > > > > > > > > This seems to do what I want - however, what really want
> > to do
> > > > > is
> > > > > > > more
> > > > > > > > > > > specific: I would like to be able to extract a subset of
> > the
> > > > > data
> > > > > > > frame
> > > > > > > > > > > pricedata, and not just the aggregated entries - for
> > instance,
> > > > > > > instead
> > > > > > > > > > of
> > > > > > > > > > > just extracting pricedata$spread by hour, I would like to
> > > > > extract a
> > > > > > > > > > slice of
> > > > > > > > > > > columns, e.g. pricedata$spread and pricedata$time on an
> > hourly
> > > > > > > basis,
> > > > > > > > > > and
> > > > > > > > > > > pass these into a function that can compute a
> > time-weighted
> > > > > average
> > > > > > > > > > spread,
> > > > > > > > > > > for instance. Does anyone know an elegant way to do this?
> > I have
> > > > > a
> > > > > > > > > > feeling
> > > > > > > > > > > zoo may do what I want, but I'm new to zoo ...
> > > > > > > > > > >
> > > > > > > > > > > Cheers
> > > > > > > > > > > Rory
> > > > > > > > > > >
> > > > > > > > > > >        [[alternative HTML version deleted]]
> > > > > > > > > > >
> > > > > > > > > > >
> > _______________________________________________
> > > > > > > > > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > > > > > > > >
> > > > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > > > > > > > -- Subscriber-posting only.
> > > > > > > > > > > -- If you want to post, subscribe first.
> > > > > > > > > > >
> > > > > > > > > >
> > > > > > > > >
> > > > > > > > >        [[alternative HTML version deleted]]
> > > > > > > > >
> > > > > > > > >
> > _______________________________________________
> > > > > > > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > > > > > >
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > > > > > -- Subscriber-posting only.
> > > > > > > > > -- If you want to post, subscribe first.
> > > > > > > > >
> > > > > > > >
> > > > > > >
> > > > > > >
> > > > > >
> > > > >
> > > > >
> > > >
> > >
> >
> >
>


From rory.winston at gmail.com  Fri Aug  3 19:32:06 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Fri, 3 Aug 2007 18:32:06 +0100
Subject: [R-SIG-Finance] Aggregating Statistics By Time Interval
In-Reply-To: <971536df0708031015l1a683c9cpb3c70496768a9146@mail.gmail.com>
References: <3f446aa30708010603h55a7840w4442ff6e4942e123@mail.gmail.com>
	<971536df0708030335g3b9aa867h7a631299e309f764@mail.gmail.com>
	<3f446aa30708030506m6439a41do53377c3cd39e076e@mail.gmail.com>
	<971536df0708030520o492b060ao10bbac04bb3867e5@mail.gmail.com>
	<3f446aa30708030615v121e4b4bn6152d5af6545eb33@mail.gmail.com>
	<971536df0708030729y23d9941dqaa81d1f38c64cfdd@mail.gmail.com>
	<971536df0708030736j4169fd12s9787675e060dac0b@mail.gmail.com>
	<3f446aa30708030754s5ffab998u617b19430d73fe09@mail.gmail.com>
	<971536df0708030801l46fbf989sab1f5cff466fca49@mail.gmail.com>
	<971536df0708031015l1a683c9cpb3c70496768a9146@mail.gmail.com>
Message-ID: <3f446aa30708031032r4020bad1vd11124023920bf3b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070803/c26e31ac/attachment.pl 

From fabricemcshort at hotmail.com  Mon Aug  6 09:52:47 2007
From: fabricemcshort at hotmail.com (Fabrice McShort)
Date: Mon, 6 Aug 2007 09:52:47 +0200
Subject: [R-SIG-Finance] How to connect R to Business Object
Message-ID: <BAY129-W20ED7ACFEBB9AD24145CF5CBE50@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070806/0393129a/attachment.pl 

From chd850 at gmail.com  Tue Aug  7 05:53:58 2007
From: chd850 at gmail.com (Hung-Te(Stanley) Chu)
Date: Mon, 6 Aug 2007 23:53:58 -0400
Subject: [R-SIG-Finance] How to do multivariate MLE?
References: <mailman.13.1186394405.29159.r-sig-finance@stat.math.ethz.ch>
Message-ID: <001501c7d8a6$96ae13f0$6601a8c0@Stanley>

Hi,

If I generate random variables, X, from the bivariate t distribution with df 
= 3:
    x <- rmvt(n=10, sigma = diag(2), df = 3)

Now I want to perform MLE to "back out" the df. To do so, I define a 
function loglik:

loglik <- function(v){
  a <- 0.5;
  b <- 0.5*v;
  sum( -log( (1+x/v)^-((v+1)/2) ) + log( beta(a,b)*sqrt(v) ) )
}

And then I use "nlminb()" to solve it. Somehow this setup is incorrect and I 
don't know why.

Can anyone give me an example about how to do multivariate MLE?

Thanks a lot.
Stanley


From ajayshah at mayin.org  Tue Aug  7 06:29:47 2007
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue, 7 Aug 2007 09:59:47 +0530
Subject: [R-SIG-Finance] How to do multivariate MLE?
In-Reply-To: <001501c7d8a6$96ae13f0$6601a8c0@Stanley>
References: <mailman.13.1186394405.29159.r-sig-finance@stat.math.ethz.ch>
	<001501c7d8a6$96ae13f0$6601a8c0@Stanley>
Message-ID: <20070807042947.GH288@lubyanka.local>

On Mon, Aug 06, 2007 at 11:53:58PM -0400, Hung-Te(Stanley) Chu wrote:
> Hi,
> 
> If I generate random variables, X, from the bivariate t distribution with df 
> = 3:
>     x <- rmvt(n=10, sigma = diag(2), df = 3)
> 
> Now I want to perform MLE to "back out" the df. To do so, I define a 
> function loglik:
> 
> loglik <- function(v){
>   a <- 0.5;
>   b <- 0.5*v;
>   sum( -log( (1+x/v)^-((v+1)/2) ) + log( beta(a,b)*sqrt(v) ) )
> }
> 
> And then I use "nlminb()" to solve it. Somehow this setup is incorrect and I 
> don't know why.
> 
> Can anyone give me an example about how to do multivariate MLE?

You might find
http://www.mayin.org/ajayshah/KB/R/documents/mle/mle.html to be useful.

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From gyadav at ccilindia.co.in  Tue Aug  7 15:30:59 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Tue, 7 Aug 2007 19:00:59 +0530
Subject: [R-SIG-Finance] How to do multivariate MLE?
In-Reply-To: <001501c7d8a6$96ae13f0$6601a8c0@Stanley>
Message-ID: <OF7B7A32EA.730D4664-ON65257330.004A0DCA-65257330.004A40F3@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070807/dc810cc2/attachment.pl 

From chd850 at gmail.com  Wed Aug  8 14:28:02 2007
From: chd850 at gmail.com (Hung-Te Chu)
Date: Wed, 8 Aug 2007 08:28:02 -0400
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 39, Issue 7
In-Reply-To: <mailman.11.1186567207.12724.r-sig-finance@stat.math.ethz.ch>
References: <mailman.11.1186567207.12724.r-sig-finance@stat.math.ethz.ch>
Message-ID: <5fb794c50708080528waeb6ee8w138dc114462df902@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070808/23a6f730/attachment.pl 

From nicolas.mougeot at db.com  Wed Aug  8 15:57:45 2007
From: nicolas.mougeot at db.com (Nicolas Mougeot)
Date: Wed, 8 Aug 2007 14:57:45 +0100
Subject: [R-SIG-Finance] data upload
Message-ID: <OF21910E0A.B8342F6D-ON80257331.004C7CB7-80257331.004CB36A@db.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070808/864b61a9/attachment.pl 

From brian at braverock.com  Wed Aug  8 16:08:23 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 08 Aug 2007 09:08:23 -0500
Subject: [R-SIG-Finance] data upload
In-Reply-To: <OF21910E0A.B8342F6D-ON80257331.004C7CB7-80257331.004CB36A@db.com>
References: <OF21910E0A.B8342F6D-ON80257331.004C7CB7-80257331.004CB36A@db.com>
Message-ID: <46B9CE57.4030008@braverock.com>

Nicolas Mougeot wrote:
> silly question but can't find an answer:
> I've got a csv file with data (prices, vol,...) and try to download it 
> into R.
> however, read.csv create an object with non numeric argument. What's the 
> easiest way to transform it into a matrix constituted of numeric elements?

Please provide a small (few line) example of your data and the command 
you used that recreates the problem.  Also, please provide the exact 
error.

Without seeing your data and the command you used, I suspect it probably 
has something to do with row and column names, or the delimiter.

You may want to look at:

?read.table

and try different parameters to make it work with your data.

But until you provide a fully contained example that recreates the 
problem, anyone on this list is just guessing.

Regards,

   - Brian


From sf at metrak.com  Sat Aug 11 04:37:49 2007
From: sf at metrak.com (paul sorenson)
Date: Sat, 11 Aug 2007 12:37:49 +1000
Subject: [R-SIG-Finance] making sense of 100's of funds
Message-ID: <46BD20FD.6050607@metrak.com>

This is a bit of an open question but the fund manager my super with has 
over two hundred funds I can move my investment around in.

Using R I typically focus on a handful of funds, plotting MACD's and and 
just relying on visualisation methods like that but I was hoping for 
some pointers on more objective measures re risk, return that are 
practical to apply to several hundred investment funds.  To be fair, 
many of the funds are "me too" so it wouldn't hurt to cull this to a 
significantly smaller set.

I have some code which downloads the daily fund entry and exit prices 
into an sqlite database which I read directly with R.

Any tips for me (an engineer not a statistician) would be most appreciated.

cheers


From brian at braverock.com  Sat Aug 11 04:46:52 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 10 Aug 2007 21:46:52 -0500
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46BD20FD.6050607@metrak.com>
References: <46BD20FD.6050607@metrak.com>
Message-ID: <46BD231C.4040601@braverock.com>

paul sorenson wrote:
> This is a bit of an open question but the fund manager my super with has 
> over two hundred funds I can move my investment around in.
> 
> Using R I typically focus on a handful of funds, plotting MACD's and and 
> just relying on visualisation methods like that but I was hoping for 
> some pointers on more objective measures re risk, return that are 
> practical to apply to several hundred investment funds.  To be fair, 
> many of the funds are "me too" so it wouldn't hurt to cull this to a 
> significantly smaller set.
> 
> I have some code which downloads the daily fund entry and exit prices 
> into an sqlite database which I read directly with R.
> 
> Any tips for me (an engineer not a statistician) would be most appreciated.

Well, your simplest first cut would be to find a good benchmark for each 
style you're interested in.  Then you can do correlations to the 
benchmark for all the funds in each style.  Funds with a very high 
correlation to the benchmark could probably be replaced more cheaply 
with index ETF's, unless they are low-fee index funds.

For risk, you need to decide which risk measures matter to you. 
variance is the most common measure of risk, but it also has many issues 
in assuming a normal distribution.  You may wish to choose a measure 
such as Sortino's Upside Potential Ratio that utilizes a return target 
called the minimum acceptable return.  Then you could stack-rank the 
funds in a given style, or across styles, by UPR to aid your choice.

I wrote a short overview of multiple Performance and Risk measures as 
the package level help for the PerformanceAnalytics package.  You could 
download the pdf documentation from CRAN, and take a look at section 
PerformanceAnalytics-package.

Regards,

   - Brian


From dlincke at lincke.net  Sat Aug 11 05:07:51 2007
From: dlincke at lincke.net (David-Michael Lincke)
Date: Fri, 10 Aug 2007 23:07:51 -0400
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval and
	timeSeries functionality still being maintained at all?
Message-ID: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070810/6f6ef7b1/attachment.pl 

From jeff.a.ryan at gmail.com  Sat Aug 11 06:02:15 2007
From: jeff.a.ryan at gmail.com (jeff.a.ryan at gmail.com)
Date: Sat, 11 Aug 2007 04:02:15 +0000
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval
	andtimeSeries functionality still being maintained at all?
In-Reply-To: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net>
References: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net>
Message-ID: <819791030-1186804987-cardhu_decombobulator_blackberry.rim.net-2085991020-@bxe008.bisx.prod.on.blackberry>

David,

I've had about the same experience - though I haven't really spent enough time with Rmetrics to try and get passed it.

Its certainly not meant to be a replacement - but to at least handle the download part you can download my new quantmod package from CRAN or from http://www.quantmod.com 

getSymbols(c("AAPL","SUNW"), return.class = "timeSeries") will download the appropriate data to your workspace.

See ?getSymbols and ?getSymbols.yahoo for info.  You'll also need my Defaults package.

The former is a work in progress - the latter is much more complete.  Both are fairly well documented, and both are fairly small.

Tell me what you think if you try it out.

Jeff Ryan
Sent via BlackBerry from T-Mobile

-----Original Message-----
From: "David-Michael Lincke" <dlincke at lincke.net>

Date: Fri, 10 Aug 2007 23:07:51 
To:<r-sig-finance at stat.math.ethz.ch>
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval and
	timeSeries functionality still being maintained at all?


Is the functionality in the RMetrics fBasics package for market data
retrieval and timeSeries object functionality still being maintained at all?

A quick test of mine showed that the Yahoo import functions don't work at
all as they expect a different date format from what's being delivered.
Specifically the following changes are necessary in lines 24 to 28 of
yahooImport:

 

        x2 = strsplit(x1[regexpr("-..-..,", x1) > 0], ",")

        x1 = matrix(unlist(x2), byrow = TRUE, nrow = length(x2))

        z = matrix(as.numeric(x1[, -1]), ncol = dim(x1)[2] - 

            1)

        rowNames = as.character(as.Date(x1[, 1]))

 

Once that hurdle is taken, however, it seems that all plotting functionality
on timeSeries objects is broken as well:

 

> plot.timeSeries(asyt.ts)

Error in xy.coords(x, y, xlabel, ylabel, log) : 'x' and 'y' lengths differ

> lines.timeSeries(asyt.ts)

Error in xy.coords(x, y, xlabel, ylabel, log) : 'x' and 'y' lengths differ

> ohlcDailyPlot(asyt.ts)

Error in plotOHLC(X, origin = "1970-01-01", xlab = xlab[1], ylab = ylab[1])
: x is not a open/high/low/close time series

 

If this first experience is representative of the state of things with
RMetrics then I'm not sure this is worth pursuing any further and I'm
probably better off sticking with Matlab.

David


	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From sf at metrak.com  Sat Aug 11 13:08:24 2007
From: sf at metrak.com (paul sorenson)
Date: Sat, 11 Aug 2007 21:08:24 +1000
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46BD20FD.6050607@metrak.com>
References: <46BD20FD.6050607@metrak.com>
Message-ID: <46BD98A8.20703@metrak.com>

Ok - thanks for the tips off list Brian and Patrick.

After reading through some of the PerformanceAnalytics docs one of the 
first things I tried to do was convert the daily unit prices to returns. 
  Just looking at the resulting time series is enlightening in its own 
right (I guess I should be surprised).

The next bit it more of an R style question.  I currently have the data 
in one data frame with the fund name as a factor.

 > names(funds)
  [1] "fundname"   "tier"       "region"     "assetClass" "security"
  [6] "style"      "geared"     "hedged"     "pdate"      "EntryPrice"
[11] "ExitPrice"  "Group"

Plotting the raw prices is almost a trivial matter with xyplot's formula 
interface, with or without groups:

print(xyplot(ExitPrice ~ pdate | fundname,  data=funds, type='l',
              layout=c(2,4),
              par.strip.text=list(cex=0.7)))

It may just be my inexperience with lattice but once I start dealing 
with zoo objects, then lattice doesn't seem to be quite so convenient. 
I could cbind the returns back into the the dataframe and continue using 
xyplot but it seems that would be throwing away the features of zoo.

What do people on the list do?

cheers

paul sorenson wrote:
> This is a bit of an open question but the fund manager my super with has 
> over two hundred funds I can move my investment around in.
> 
> Using R I typically focus on a handful of funds, plotting MACD's and and 
> just relying on visualisation methods like that but I was hoping for 
> some pointers on more objective measures re risk, return that are 
> practical to apply to several hundred investment funds.  To be fair, 
> many of the funds are "me too" so it wouldn't hurt to cull this to a 
> significantly smaller set.
> 
> I have some code which downloads the daily fund entry and exit prices 
> into an sqlite database which I read directly with R.
> 
> Any tips for me (an engineer not a statistician) would be most appreciated.


From ggrothendieck at gmail.com  Sat Aug 11 13:55:10 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 11 Aug 2007 07:55:10 -0400
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46BD98A8.20703@metrak.com>
References: <46BD20FD.6050607@metrak.com> <46BD98A8.20703@metrak.com>
Message-ID: <971536df0708110455h5cfb2f24pbe1c79816b553492@mail.gmail.com>

On 8/11/07, paul sorenson <sf at metrak.com> wrote:
> Ok - thanks for the tips off list Brian and Patrick.
>
> After reading through some of the PerformanceAnalytics docs one of the
> first things I tried to do was convert the daily unit prices to returns.
>  Just looking at the resulting time series is enlightening in its own
> right (I guess I should be surprised).
>
> The next bit it more of an R style question.  I currently have the data
> in one data frame with the fund name as a factor.
>
>  > names(funds)
>  [1] "fundname"   "tier"       "region"     "assetClass" "security"
>  [6] "style"      "geared"     "hedged"     "pdate"      "EntryPrice"
> [11] "ExitPrice"  "Group"
>
> Plotting the raw prices is almost a trivial matter with xyplot's formula
> interface, with or without groups:
>
> print(xyplot(ExitPrice ~ pdate | fundname,  data=funds, type='l',
>              layout=c(2,4),
>              par.strip.text=list(cex=0.7)))
>
> It may just be my inexperience with lattice but once I start dealing
> with zoo objects, then lattice doesn't seem to be quite so convenient.
> I could cbind the returns back into the the dataframe and continue using
> xyplot but it seems that would be throwing away the features of zoo.
>
> What do people on the list do?

Read the documentation in ?xyplot.zoo and ?plot.zoo   .


From tmuhlhof at indiana.edu  Sun Aug 12 17:41:17 2007
From: tmuhlhof at indiana.edu (Tobias Muhlhofer)
Date: Sun, 12 Aug 2007 11:41:17 -0400
Subject: [R-SIG-Finance] making sense of 100's of funds
Message-ID: <46BF2A1D.7010604@indiana.edu>

Paul,

Unless you are looking at index funds, you need to see whether your
funds produce alpha. To do this, pick a set of benchmarks according to
your fund's style and investment strategy, like Morningstar category
index or something like that (or perhaps just the general stock market
plus the two Fama-French factors), regress the fund's returns on the
benchmark returns, and see whether you have a significantly positive
intercept after fees. This is the best way of measuring systematic-risk
adjusted returns.

Being a finance academic (and therefore a cynic), and judging from my
own research, if benchmarked correctly, very few fund managers generate
positively significant alphas, and so I personally buy index funds for
whatever style I want to invest in, and there I choose the one with the
lowest expense ratio.

Best,
	Toby


From wuertz at itp.phys.ethz.ch  Mon Aug 13 00:05:45 2007
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon, 13 Aug 2007 00:05:45 +0200
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval
 and	timeSeries functionality still being maintained at all?
In-Reply-To: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net>
References: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net>
Message-ID: <46BF8439.2050803@itp.phys.ethz.ch>

David-Michael Lincke wrote:
> Is the functionality in the RMetrics fBasics package for market data
> retrieval and timeSeries object functionality still being maintained at all?
>
> A quick test of mine 
What is your test ? --- useless if you don't tell us what you have done
> showed that the Yahoo import functions don't work at
> all as they expect a different date format from what's being delivered.
>   
This is wrong what you tell us.

Have you read the help pages ? --- I think definitely not.

yahooImport() is a function which delivers a web object of the download 
with many
information which is helpful for downloading data on a regular time 
schedule and to
store them in a database. Nota bene, there is a data slot which keeps 
the timeSeries
object.

For simply downloading a time series, you should use the function 
yahooSeries(). This
delivers a 'timeSeries' object.

Note, the download depends on the time format used by Yahoo. This has 
changed a few
months ago, noticed on this mailing list. --- Therefore you should use a 
recent version of
Rmetrics and you should also update your software on a regular base.
> Specifically the following changes are necessary in lines 24 to 28 of
> yahooImport:
>
>  
>
>         x2 = strsplit(x1[regexpr("-..-..,", x1) > 0], ",")
>
>         x1 = matrix(unlist(x2), byrow = TRUE, nrow = length(x2))
>
>         z = matrix(as.numeric(x1[, -1]), ncol = dim(x1)[2] - 
>
>             1)
>
>         rowNames = as.character(as.Date(x1[, 1]))
>
>  
>
> Once that hurdle is taken, however, it seems that all plotting functionality
> on timeSeries objects is broken as well:
>
>  
>   
No, definitely not ...

Try ...

IBM = yahooSeries("IBM")
colnames(IBM)
par(mfrow = c(1,1))
plot(IBM[, "IBM.Open"], type = "l")
lines(IBM[, "IBM.Close"],col = "red")

Rmetrics has also many tailored plotting functions, plot is only
used for univariate timeSeries objects, multivariate timeSeries
objects can be displayed using the additional functions lines()
and points()!

>   
>> plot.timeSeries(asyt.ts)
>>     
>
> Error in xy.coords(x, y, xlabel, ylabel, log) : 'x' and 'y' lengths differ
>
>   
>> lines.timeSeries(asyt.ts)
>>     
>
> Error in xy.coords(x, y, xlabel, ylabel, log) : 'x' and 'y' lengths differ
>
>   
>> ohlcDailyPlot(asyt.ts)
>>     
>
>   

What is asyt.ts ? --- We can only help if you tell us what you have done ...

> Error in plotOHLC(X, origin = "1970-01-01", xlab = xlab[1], ylab = ylab[1])
> : x is not a open/high/low/close time series
>
>  
>
> If this first experience is representative of the state of things with
> RMetrics then I'm not sure this is worth pursuing any further and I'm
> probably better off sticking with Matlab.
>   
I wan't comment this ....

ohlcDailyPlot() expects a timeSeries object with column names c("Open", 
"High", "Low", "Close", "Volume")
They can be in any order, but the names must be exactly the column names 
mentioned.

Try:

colnames(IBM) <- c("Open", "High", "Low", "Close", "Volume")
par(mfrow = c(2, 1))
ohlcDailyPlot(IBM)


... it works fine!

Diethelm

> David
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
>


From dlincke at lincke.net  Mon Aug 13 03:11:39 2007
From: dlincke at lincke.net (David-Michael Lincke)
Date: Sun, 12 Aug 2007 21:11:39 -0400
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval
	and	timeSeries functionality still being maintained at all?
In-Reply-To: <46BF8439.2050803@itp.phys.ethz.ch>
References: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net>
	<46BF8439.2050803@itp.phys.ethz.ch>
Message-ID: <06a001c7dd46$e99b8080$0c01a8c0@lincke.net>

Diethelm,
Thank you for your response. Asyt.ts in my example is a timeSeries object
constructed using yahooSeries(). Trying out the example you provided I get:

> IBM = yahooSeries("IBM")
trying URL
'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
x=.csv'
Content type 'text/csv' length unknown
opened URL
downloaded 12Kb

Read 250 items
Error in matrix(unlist(x2), byrow = TRUE, nrow = length(x2)) : 
        matrix: invalid 'ncol' value (< 0)
Error in .yahooSeries(symbols[1], from = from, to = to, nDaysBack =
nDaysBack,  : 
        cannot get a slot ("data") from an object of type "character"

After applying the patch to yahooImport() that I posted in my initial
message the import works:

> IBM = yahooSeries("IBM")
trying URL
'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
x=.csv'
Content type 'text/csv' length unknown
opened URL
downloaded 12Kb

Read 250 items
Read 7 items

> colnames(IBM)
[1] "IBM.Open"   "IBM.High"   "IBM.Low"    "IBM.Close"  "IBM.Volume"

Given that plot() has been overloaded to deal with timeSeries objects I
would have expected it to have some default functionality when passed a
timeSeries object short of unceremoniously failing in a lower level function
call. I did not realize it only works on single columns/data series.

As for ohlcDailyPlot(), I checked the source code later on and realized that
it expects column names that are different from how the market data
retrieval functions construct their timeSeries objects. This is easily
adjusted for. However, it's not exactly a smooth solution from a design
point of view. Also, the documentation does not mention the need for an
explicit prior call to par(mfrow = c(2, 1)) for it to result in a proper
plot. But I guess maybe that's obvious to a more seasoned R user than me.

Anyway, thank you for pointing me in the right direction. The code in
yahooImport(), however, really is broken as you can clearly verify by taking
a look at the date format delivered by Yahoo which does not have spelled out
three letter month names as expected by the source code.

David

-----Original Message-----
From: Diethelm Wuertz [mailto:wuertz at itp.phys.ethz.ch] 
Sent: Sunday, August 12, 2007 6:06 PM
To: David-Michael Lincke; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] RMetrics fBasics market data retrieval and
timeSeries functionality still being maintained at all?

David-Michael Lincke wrote:
> Is the functionality in the RMetrics fBasics package for market data
> retrieval and timeSeries object functionality still being maintained at
all?
>
> A quick test of mine 
What is your test ? --- useless if you don't tell us what you have done
> showed that the Yahoo import functions don't work at
> all as they expect a different date format from what's being delivered.
>   
This is wrong what you tell us.

Have you read the help pages ? --- I think definitely not.

yahooImport() is a function which delivers a web object of the download 
with many
information which is helpful for downloading data on a regular time 
schedule and to
store them in a database. Nota bene, there is a data slot which keeps 
the timeSeries
object.

For simply downloading a time series, you should use the function 
yahooSeries(). This
delivers a 'timeSeries' object.

Note, the download depends on the time format used by Yahoo. This has 
changed a few
months ago, noticed on this mailing list. --- Therefore you should use a 
recent version of
Rmetrics and you should also update your software on a regular base.
> Specifically the following changes are necessary in lines 24 to 28 of
> yahooImport:
>
>  
>
>         x2 = strsplit(x1[regexpr("-..-..,", x1) > 0], ",")
>
>         x1 = matrix(unlist(x2), byrow = TRUE, nrow = length(x2))
>
>         z = matrix(as.numeric(x1[, -1]), ncol = dim(x1)[2] - 
>
>             1)
>
>         rowNames = as.character(as.Date(x1[, 1]))
>
>  
>
> Once that hurdle is taken, however, it seems that all plotting
functionality
> on timeSeries objects is broken as well:
>
>  
>   
No, definitely not ...

Try ...

IBM = yahooSeries("IBM")
colnames(IBM)
par(mfrow = c(1,1))
plot(IBM[, "IBM.Open"], type = "l")
lines(IBM[, "IBM.Close"],col = "red")

Rmetrics has also many tailored plotting functions, plot is only
used for univariate timeSeries objects, multivariate timeSeries
objects can be displayed using the additional functions lines()
and points()!

>   
>> plot.timeSeries(asyt.ts)
>>     
>
> Error in xy.coords(x, y, xlabel, ylabel, log) : 'x' and 'y' lengths differ
>
>   
>> lines.timeSeries(asyt.ts)
>>     
>
> Error in xy.coords(x, y, xlabel, ylabel, log) : 'x' and 'y' lengths differ
>
>   
>> ohlcDailyPlot(asyt.ts)
>>     
>
>   

What is asyt.ts ? --- We can only help if you tell us what you have done ...

> Error in plotOHLC(X, origin = "1970-01-01", xlab = xlab[1], ylab =
ylab[1])
> : x is not a open/high/low/close time series
>
>  
>
> If this first experience is representative of the state of things with
> RMetrics then I'm not sure this is worth pursuing any further and I'm
> probably better off sticking with Matlab.
>   
I wan't comment this ....

ohlcDailyPlot() expects a timeSeries object with column names c("Open", 
"High", "Low", "Close", "Volume")
They can be in any order, but the names must be exactly the column names 
mentioned.

Try:

colnames(IBM) <- c("Open", "High", "Low", "Close", "Volume")
par(mfrow = c(2, 1))
ohlcDailyPlot(IBM)


... it works fine!

Diethelm

> David
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
>


From sf at metrak.com  Mon Aug 13 10:52:40 2007
From: sf at metrak.com (paul sorenson)
Date: Mon, 13 Aug 2007 18:52:40 +1000
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46BF2A1D.7010604@indiana.edu>
References: <46BF2A1D.7010604@indiana.edu>
Message-ID: <46C01BD8.2020007@metrak.com>

Toby,

Thanks for the tips.  I am somewhat of a cynic also. When it is my own
retirement fund, academic meets real-life in a fairly personal way!  I
have heard figures that something like 98% (from memory) of Australian's
"don't understand superannuation".

I am trying to get myself well into the 2%.  Writing R code leveraging
some of the great packages out there is just a method of learning which
I find works for me.  It can be slow going at times though.

I have picked out 8 funds to crunch through and using the ASX SP200 as
the benchmark for exercising my code.  My "practice" data set is at
http://www.metrak.com/tmp/exitprices.csv and I retrieved the SP200 using
get.hist.quote("^AXJO", start="2002-01-01", quote="Close", retclass="zoo").

I have used CalculateReturns from PerformanceAnalytics to create returns
as zoo objects so hopefully I will be able to calculate the alphas.  If
I understand your comment below, I am looking for a more positive
intercept on my choice of fund compared with the benchmark?

cheers

Tobias Muhlhofer wrote:
> Paul,
> 
> Unless you are looking at index funds, you need to see whether your
> funds produce alpha. To do this, pick a set of benchmarks according to
> your fund's style and investment strategy, like Morningstar category
> index or something like that (or perhaps just the general stock market
> plus the two Fama-French factors), regress the fund's returns on the
> benchmark returns, and see whether you have a significantly positive
> intercept after fees. This is the best way of measuring systematic-risk
> adjusted returns.
> 
> Being a finance academic (and therefore a cynic), and judging from my
> own research, if benchmarked correctly, very few fund managers generate
> positively significant alphas, and so I personally buy index funds for
> whatever style I want to invest in, and there I choose the one with the
> lowest expense ratio.
> 
> Best,
> 	Toby
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.


From martin.becker at mx.uni-saarland.de  Mon Aug 13 11:22:13 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Mon, 13 Aug 2007 11:22:13 +0200
Subject: [R-SIG-Finance] bug in rnig (Rmetrics: fBasics 251.70)
Message-ID: <46C022C5.1070304@mx.uni-saarland.de>

Dear maintainers/users of Rmetrics,


the random generator for NIG random variates (rnig) in fBasics version 
251.70 (and probably prior versions) is faulty, which is easily 
visualised, e.g., by the following code:

  library(fBasics)
  curve(dnig(x,alpha=4,beta=1,delta=3),-2,4)
  lines(density(rnig(n=100000,alpha=4,beta=1,delta=3)),col="red")

The bug is caused by a minor error in the PhD thesis of S. Raible 
(2000), which is the source of the current implementation of rnig.
Please find below a patch for 2B-HyperbolicDistribution.R (which besides 
changes three apparently nonessential global assignments to local ones) 
as well as a documentation patch (for 2B-HyperbolicDistribution.Rd), 
which includes references to a correct implementation.


Kind regards,

   Martin Becker



diff --recursive fBasics.orig/R/2B-HyperbolicDistribution.R 
fBasics/R/2B-HyperbolicDistribution.R
1372,1374c1372,1374
<         z1 <<- function(v, delta, gamma) {
<             delta/gamma + v/(gamma^2) - sqrt( 2*v*delta/(gamma^3) +
<             (v/(gamma^2))^2 )
---
 >         z1 <- function(v, delta, gamma) {
 >             delta/gamma + v/(2*gamma^2) - sqrt( v*delta/(gamma^3) +
 >             (v/(2*gamma^2))^2 )
1376c1376
<         z2 <<- function(v, delta, gamma) {
---
 >         z2 <- function(v, delta, gamma) {
1379c1379
<         pz1 <<- function(v, delta, gamma) {
---
 >         pz1 <- function(v, delta, gamma) {

diff --recursive fBasics.orig/man/2B-HyperbolicDistribution.Rd 
fBasics/man/2B-HyperbolicDistribution.Rd
173,174c173,177
<     The random deviates are calculated with the method described by
<     Raible (2000).
---
 >     The generator \code{rnig} is based on general methods given by
 >     Michael et al. (1976), as implemented in, e.g., Benth et al. 
(2006) for
 >     the case of NIG random variates.
 >     \emph{Caution: up to fBasics v251.70, \code{rnig} was based on the
 >     incorrect implementation described by Raible (2000).}
203a207,216
 > Benth F.E., Groth M., Kettler P.C. (2006);
 >     \emph{A Quasi-Monte Carlo Algorithm for the Normal Inverse Gaussian
 >     Distribution and Valuation of Financial Derivatives},
 >     International Journal of Theoretical and Applied Finance, 9(6), 
843--867.
 >
 > Michael J.R., Schucany W.R., Haas R.W. (1976);
 >     \emph{Generating Random Variates Using Transformations with Multiple
 >     Roots},
 >     The American Statistician, 30(2), 88--90.
 >


From martin.becker at mx.uni-saarland.de  Mon Aug 13 12:25:40 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Mon, 13 Aug 2007 12:25:40 +0200
Subject: [R-SIG-Finance] bug in .garchOxFit (Rmetrics: fSeries 251.70)
Message-ID: <46C031A4.5030602@mx.uni-saarland.de>

Dear maintainers/users of Rmetrics,


a buglet in the GarchOxFit-Interface of Rmetrics, which was already reported for version 220.10063 (https://stat.ethz.ch/pipermail/r-sig-finance/2005q4/000498.html) and version 240.10068 (http://tolstoy.newcastle.edu.au/R/e2/help/07/06/19220.html) is still present in version 251.70.

Please find below a patch for 4D-GarchOxInterface.R (fSeries version 251.70).

Kind regards,


  Martin Becker

 

diff --recursive fSeries.orig/R/4D-GarchOxInterface.R fSeries/R/4D-GarchOxInterface.R
227c227
<     write(x, file = "OxSeries.csv", ncolumns = 1, append = TRUE)                        
---
>     write(x = series, file = "OxSeries.csv", ncolumns = 1, append = TRUE)


From maechler at stat.math.ethz.ch  Tue Aug 14 18:03:06 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 14 Aug 2007 18:03:06 +0200
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval and
	timeSeries functionality still being maintained at all?
In-Reply-To: <06a001c7dd46$e99b8080$0c01a8c0@lincke.net>
References: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net>
	<46BF8439.2050803@itp.phys.ethz.ch>
	<06a001c7dd46$e99b8080$0c01a8c0@lincke.net>
Message-ID: <18113.53818.239520.979117@stat.math.ethz.ch>


>>>>> "DL" == David-Michael Lincke <dlincke at lincke.net>
>>>>>     on Sun, 12 Aug 2007 21:11:39 -0400 writes:

    DL> Diethelm,
    DL> Thank you for your response. Asyt.ts in my example is a timeSeries object
    DL> constructed using yahooSeries(). Trying out the example you provided I get:

    >> IBM = yahooSeries("IBM")
    DL> trying URL
    DL> 'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
    DL> x=.csv'
    DL> Content type 'text/csv' length unknown
    DL> opened URL
    DL> downloaded 12Kb

    DL> Read 250 items
    DL> Error in matrix(unlist(x2), byrow = TRUE, nrow = length(x2)) : 
    DL> matrix: invalid 'ncol' value (< 0)
    DL> Error in .yahooSeries(symbols[1], from = from, to = to, nDaysBack =
    DL> nDaysBack,  : 
    DL> cannot get a slot ("data") from an object of type "character"

David,
I can confirm Diethelm's statement that  yahooSeries()
works without problems 

__IFF__ you use current versions of R and fBasics.

Can you give your
    sessionInfo() ## {after having loaded  fBasics} ?

I'm pretty sure your version of fBasics is not current enough.

    DL> After applying the patch to yahooImport() that I posted in my initial
    DL> message the import works:

    >> IBM = yahooSeries("IBM")
    DL> trying URL
    DL> 'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
    DL> x=.csv'
    DL> Content type 'text/csv' length unknown
    DL> opened URL
    DL> downloaded 12Kb

    DL> Read 250 items
    DL> Read 7 items

    >> colnames(IBM)
    DL> [1] "IBM.Open"   "IBM.High"   "IBM.Low"    "IBM.Close"  "IBM.Volume"

    DL> Given that plot() has been overloaded to deal with timeSeries objects I
    DL> would have expected it to have some default functionality when passed a
    DL> timeSeries object short of unceremoniously failing in a lower level function
    DL> call. I did not realize it only works on single columns/data series.

    DL> As for ohlcDailyPlot(), I checked the source code later on and realized that
    DL> it expects column names that are different from how the market data
    DL> retrieval functions construct their timeSeries objects. This is easily
    DL> adjusted for. However, it's not exactly a smooth solution from a design
    DL> point of view. Also, the documentation does not mention the need for an
    DL> explicit prior call to par(mfrow = c(2, 1)) for it to result in a proper
    DL> plot. But I guess maybe that's obvious to a more seasoned R user than me.

    DL> Anyway, thank you for pointing me in the right direction. The code in
    DL> yahooImport(), however, really is broken as you can clearly verify by taking
    DL> a look at the date format delivered by Yahoo which does not have spelled out
    DL> three letter month names as expected by the source code.


I partly agree with you, that the documentation of several of
the Rmetrics functions is clearly "sub-optimal".
There's currently an ongoing effort to make Rmetrics being
loaded on more shoulders than just Diethelm's and I'd hope that
consequently many things will improve on the documentation front
...

Best regards,
Martin

    DL> David


From raschaoot at gmail.com  Tue Aug 14 18:19:41 2007
From: raschaoot at gmail.com (Jeroen van der Heide)
Date: Tue, 14 Aug 2007 11:19:41 -0500
Subject: [R-SIG-Finance] [garchFit] Fitted GARCH(1,
	1) results in a straight line
Message-ID: <90b8808d0708140919i5c53d8f5jf6bceb1a22f51d38@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070814/8f48ce2d/attachment.pl 

From dlincke at lincke.net  Tue Aug 14 18:22:12 2007
From: dlincke at lincke.net (David-Michael Lincke)
Date: Tue, 14 Aug 2007 12:22:12 -0400
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval and
	timeSeries functionality still being maintained at all?
In-Reply-To: <18113.53818.239520.979117@stat.math.ethz.ch>
References: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net><46BF8439.2050803@itp.phys.ethz.ch><06a001c7dd46$e99b8080$0c01a8c0@lincke.net>
	<18113.53818.239520.979117@stat.math.ethz.ch>
Message-ID: <07b901c7de8f$47fc5fc0$0c01a8c0@lincke.net>

Martin,

I did run update.packages() after I first encountered problems, which
suggested that no newer version was available. The source code of
yahooImport() clearly scans for dates with format along the lines of
2007-Aug-01 while Yahoo actually delivers 2007-08-01. Here is the output
from sessionInfo():

> sessionInfo()
R version 2.4.0 (2006-10-03) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[7] "base"     

other attached packages:
      fBasics     fCalendar       fEcofin          MASS 
"240.10068.1"   "240.10068"   "240.10067"      "7.2-34"

David

-----Original Message-----
From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
Sent: Tuesday, August 14, 2007 12:03 PM
To: David-Michael Lincke
Cc: 'Diethelm Wuertz'; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] RMetrics fBasics market data retrieval and
timeSeries functionality still being maintained at all?


>>>>> "DL" == David-Michael Lincke <dlincke at lincke.net>
>>>>>     on Sun, 12 Aug 2007 21:11:39 -0400 writes:

    DL> Diethelm,
    DL> Thank you for your response. Asyt.ts in my example is a timeSeries
object
    DL> constructed using yahooSeries(). Trying out the example you provided
I get:

    >> IBM = yahooSeries("IBM")
    DL> trying URL
    DL>
'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
    DL> x=.csv'
    DL> Content type 'text/csv' length unknown
    DL> opened URL
    DL> downloaded 12Kb

    DL> Read 250 items
    DL> Error in matrix(unlist(x2), byrow = TRUE, nrow = length(x2)) : 
    DL> matrix: invalid 'ncol' value (< 0)
    DL> Error in .yahooSeries(symbols[1], from = from, to = to, nDaysBack =
    DL> nDaysBack,  : 
    DL> cannot get a slot ("data") from an object of type "character"

David,
I can confirm Diethelm's statement that  yahooSeries()
works without problems 

__IFF__ you use current versions of R and fBasics.

Can you give your
    sessionInfo() ## {after having loaded  fBasics} ?

I'm pretty sure your version of fBasics is not current enough.

    DL> After applying the patch to yahooImport() that I posted in my
initial
    DL> message the import works:

    >> IBM = yahooSeries("IBM")
    DL> trying URL
    DL>
'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
    DL> x=.csv'
    DL> Content type 'text/csv' length unknown
    DL> opened URL
    DL> downloaded 12Kb

    DL> Read 250 items
    DL> Read 7 items

    >> colnames(IBM)
    DL> [1] "IBM.Open"   "IBM.High"   "IBM.Low"    "IBM.Close"  "IBM.Volume"

    DL> Given that plot() has been overloaded to deal with timeSeries
objects I
    DL> would have expected it to have some default functionality when
passed a
    DL> timeSeries object short of unceremoniously failing in a lower level
function
    DL> call. I did not realize it only works on single columns/data series.

    DL> As for ohlcDailyPlot(), I checked the source code later on and
realized that
    DL> it expects column names that are different from how the market data
    DL> retrieval functions construct their timeSeries objects. This is
easily
    DL> adjusted for. However, it's not exactly a smooth solution from a
design
    DL> point of view. Also, the documentation does not mention the need for
an
    DL> explicit prior call to par(mfrow = c(2, 1)) for it to result in a
proper
    DL> plot. But I guess maybe that's obvious to a more seasoned R user
than me.

    DL> Anyway, thank you for pointing me in the right direction. The code
in
    DL> yahooImport(), however, really is broken as you can clearly verify
by taking
    DL> a look at the date format delivered by Yahoo which does not have
spelled out
    DL> three letter month names as expected by the source code.


I partly agree with you, that the documentation of several of
the Rmetrics functions is clearly "sub-optimal".
There's currently an ongoing effort to make Rmetrics being
loaded on more shoulders than just Diethelm's and I'd hope that
consequently many things will improve on the documentation front
...

Best regards,
Martin

    DL> David


From agehr at mozart.depaul.edu  Tue Aug 14 18:35:46 2007
From: agehr at mozart.depaul.edu (Adam Gehr)
Date: Tue, 14 Aug 2007 11:35:46 -0500 (CDT)
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval and
 timeSeries functionality still being maintained at all?
In-Reply-To: <07b901c7de8f$47fc5fc0$0c01a8c0@lincke.net>
References: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net><46BF8439.2050803@itp.phys.ethz.ch><06a001c7dd46$e99b8080$0c01a8c0@lincke.net>
	<18113.53818.239520.979117@stat.math.ethz.ch>
	<07b901c7de8f$47fc5fc0$0c01a8c0@lincke.net>
Message-ID: <Pine.LNX.4.63.0708141134220.24936@mozart.depaul.edu>

On trying the same commands with a recently updated version of R I get:

> sessionInfo()
R version 2.5.1 (2007-06-27)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "tcltk"     "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "methods"   "base"

other attached packages:
   fBasics fCalendar   fEcofin   spatial      MASS
  "251.70"  "251.70"  "251.70"  "7.2-35"  "7.2-35"
> IBM = yahooSeries("IBM")
trying URL 
'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=14&c=2006&d=7&e=14&f=2007&g=d&x=.csv'
Content type 'text/csv' length unknown
opened URL
downloaded 12Kb

and the resulting dataset looks fine.

   Adam Gehr


On Tue, 14 Aug 2007, David-Michael Lincke wrote:

> Martin,
>
> I did run update.packages() after I first encountered problems, which
> suggested that no newer version was available. The source code of
> yahooImport() clearly scans for dates with format along the lines of
> 2007-Aug-01 while Yahoo actually delivers 2007-08-01. Here is the output
> from sessionInfo():
>
>> sessionInfo()
> R version 2.4.0 (2006-10-03)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> other attached packages:
>      fBasics     fCalendar       fEcofin          MASS
> "240.10068.1"   "240.10068"   "240.10067"      "7.2-34"
>
> David
>
> -----Original Message-----
> From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
> Sent: Tuesday, August 14, 2007 12:03 PM
> To: David-Michael Lincke
> Cc: 'Diethelm Wuertz'; r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] RMetrics fBasics market data retrieval and
> timeSeries functionality still being maintained at all?
>
>
>>>>>> "DL" == David-Michael Lincke <dlincke at lincke.net>
>>>>>>     on Sun, 12 Aug 2007 21:11:39 -0400 writes:
>
>    DL> Diethelm,
>    DL> Thank you for your response. Asyt.ts in my example is a timeSeries
> object
>    DL> constructed using yahooSeries(). Trying out the example you provided
> I get:
>
>    >> IBM = yahooSeries("IBM")
>    DL> trying URL
>    DL>
> 'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
>    DL> x=.csv'
>    DL> Content type 'text/csv' length unknown
>    DL> opened URL
>    DL> downloaded 12Kb
>
>    DL> Read 250 items
>    DL> Error in matrix(unlist(x2), byrow = TRUE, nrow = length(x2)) :
>    DL> matrix: invalid 'ncol' value (< 0)
>    DL> Error in .yahooSeries(symbols[1], from = from, to = to, nDaysBack =
>    DL> nDaysBack,  :
>    DL> cannot get a slot ("data") from an object of type "character"
>
> David,
> I can confirm Diethelm's statement that  yahooSeries()
> works without problems
>
> __IFF__ you use current versions of R and fBasics.
>
> Can you give your
>    sessionInfo() ## {after having loaded  fBasics} ?
>
> I'm pretty sure your version of fBasics is not current enough.
>
>    DL> After applying the patch to yahooImport() that I posted in my
> initial
>    DL> message the import works:
>
>    >> IBM = yahooSeries("IBM")
>    DL> trying URL
>    DL>
> 'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
>    DL> x=.csv'
>    DL> Content type 'text/csv' length unknown
>    DL> opened URL
>    DL> downloaded 12Kb
>
>    DL> Read 250 items
>    DL> Read 7 items
>
>    >> colnames(IBM)
>    DL> [1] "IBM.Open"   "IBM.High"   "IBM.Low"    "IBM.Close"  "IBM.Volume"
>
>    DL> Given that plot() has been overloaded to deal with timeSeries
> objects I
>    DL> would have expected it to have some default functionality when
> passed a
>    DL> timeSeries object short of unceremoniously failing in a lower level
> function
>    DL> call. I did not realize it only works on single columns/data series.
>
>    DL> As for ohlcDailyPlot(), I checked the source code later on and
> realized that
>    DL> it expects column names that are different from how the market data
>    DL> retrieval functions construct their timeSeries objects. This is
> easily
>    DL> adjusted for. However, it's not exactly a smooth solution from a
> design
>    DL> point of view. Also, the documentation does not mention the need for
> an
>    DL> explicit prior call to par(mfrow = c(2, 1)) for it to result in a
> proper
>    DL> plot. But I guess maybe that's obvious to a more seasoned R user
> than me.
>
>    DL> Anyway, thank you for pointing me in the right direction. The code
> in
>    DL> yahooImport(), however, really is broken as you can clearly verify
> by taking
>    DL> a look at the date format delivered by Yahoo which does not have
> spelled out
>    DL> three letter month names as expected by the source code.
>
>
> I partly agree with you, that the documentation of several of
> the Rmetrics functions is clearly "sub-optimal".
> There's currently an ongoing effort to make Rmetrics being
> loaded on more shoulders than just Diethelm's and I'd hope that
> consequently many things will improve on the documentation front
> ...
>
> Best regards,
> Martin
>
>    DL> David
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From edd at debian.org  Tue Aug 14 19:30:41 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 14 Aug 2007 12:30:41 -0500
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval
	and	timeSeries functionality still being maintained at all?
In-Reply-To: <07b901c7de8f$47fc5fc0$0c01a8c0@lincke.net>
References: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net>
	<46BF8439.2050803@itp.phys.ethz.ch>
	<06a001c7dd46$e99b8080$0c01a8c0@lincke.net>
	<18113.53818.239520.979117@stat.math.ethz.ch>
	<07b901c7de8f$47fc5fc0$0c01a8c0@lincke.net>
Message-ID: <18113.59073.412514.925644@ron.nulle.part>


On 14 August 2007 at 12:22, David-Michael Lincke wrote:
| Martin,
| 
| I did run update.packages() after I first encountered problems, which
| suggested that no newer version was available. The source code of
| yahooImport() clearly scans for dates with format along the lines of
| 2007-Aug-01 while Yahoo actually delivers 2007-08-01. Here is the output
| from sessionInfo():
| 
| > sessionInfo()
| R version 2.4.0 (2006-10-03) 
| i386-pc-mingw32 

Released last October, as the date shows. You missed a patch release (always
recommended), a new major release (2.5.) and its first patch releases.

| locale:
| LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
| States.1252;LC_MONETARY=English_United
| States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
| 
| attached base packages:
| [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
| [7] "base"     
| 
| other attached packages:
|       fBasics     fCalendar       fEcofin          MASS 
| "240.10068.1"   "240.10068"   "240.10067"      "7.2-34"

These are __current only conditional of the version of R you're running__
i.e. R found you the max(version | R release).  The problem is that you use
an outdated R releases, which leads to the outdated Rmetrics.

Please upgrade, and your problem should go away as suggested by three
different readers of this list.

Hth, Dirk

 
| David
| 
| -----Original Message-----
| From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
| Sent: Tuesday, August 14, 2007 12:03 PM
| To: David-Michael Lincke
| Cc: 'Diethelm Wuertz'; r-sig-finance at stat.math.ethz.ch
| Subject: Re: [R-SIG-Finance] RMetrics fBasics market data retrieval and
| timeSeries functionality still being maintained at all?
| 
| 
| >>>>> "DL" == David-Michael Lincke <dlincke at lincke.net>
| >>>>>     on Sun, 12 Aug 2007 21:11:39 -0400 writes:
| 
|     DL> Diethelm,
|     DL> Thank you for your response. Asyt.ts in my example is a timeSeries
| object
|     DL> constructed using yahooSeries(). Trying out the example you provided
| I get:
| 
|     >> IBM = yahooSeries("IBM")
|     DL> trying URL
|     DL>
| 'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
|     DL> x=.csv'
|     DL> Content type 'text/csv' length unknown
|     DL> opened URL
|     DL> downloaded 12Kb
| 
|     DL> Read 250 items
|     DL> Error in matrix(unlist(x2), byrow = TRUE, nrow = length(x2)) : 
|     DL> matrix: invalid 'ncol' value (< 0)
|     DL> Error in .yahooSeries(symbols[1], from = from, to = to, nDaysBack =
|     DL> nDaysBack,  : 
|     DL> cannot get a slot ("data") from an object of type "character"
| 
| David,
| I can confirm Diethelm's statement that  yahooSeries()
| works without problems 
| 
| __IFF__ you use current versions of R and fBasics.
| 
| Can you give your
|     sessionInfo() ## {after having loaded  fBasics} ?
| 
| I'm pretty sure your version of fBasics is not current enough.
| 
|     DL> After applying the patch to yahooImport() that I posted in my
| initial
|     DL> message the import works:
| 
|     >> IBM = yahooSeries("IBM")
|     DL> trying URL
|     DL>
| 'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
|     DL> x=.csv'
|     DL> Content type 'text/csv' length unknown
|     DL> opened URL
|     DL> downloaded 12Kb
| 
|     DL> Read 250 items
|     DL> Read 7 items
| 
|     >> colnames(IBM)
|     DL> [1] "IBM.Open"   "IBM.High"   "IBM.Low"    "IBM.Close"  "IBM.Volume"
| 
|     DL> Given that plot() has been overloaded to deal with timeSeries
| objects I
|     DL> would have expected it to have some default functionality when
| passed a
|     DL> timeSeries object short of unceremoniously failing in a lower level
| function
|     DL> call. I did not realize it only works on single columns/data series.
| 
|     DL> As for ohlcDailyPlot(), I checked the source code later on and
| realized that
|     DL> it expects column names that are different from how the market data
|     DL> retrieval functions construct their timeSeries objects. This is
| easily
|     DL> adjusted for. However, it's not exactly a smooth solution from a
| design
|     DL> point of view. Also, the documentation does not mention the need for
| an
|     DL> explicit prior call to par(mfrow = c(2, 1)) for it to result in a
| proper
|     DL> plot. But I guess maybe that's obvious to a more seasoned R user
| than me.
| 
|     DL> Anyway, thank you for pointing me in the right direction. The code
| in
|     DL> yahooImport(), however, really is broken as you can clearly verify
| by taking
|     DL> a look at the date format delivered by Yahoo which does not have
| spelled out
|     DL> three letter month names as expected by the source code.
| 
| 
| I partly agree with you, that the documentation of several of
| the Rmetrics functions is clearly "sub-optimal".
| There's currently an ongoing effort to make Rmetrics being
| loaded on more shoulders than just Diethelm's and I'd hope that
| consequently many things will improve on the documentation front
| ...
| 
| Best regards,
| Martin
| 
|     DL> David
| 
| _______________________________________________
| R-SIG-Finance at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-sig-finance
| -- Subscriber-posting only. 
| -- If you want to post, subscribe first.

-- 
Three out of two people have difficulties with fractions.


From hfrisks at gmail.com  Tue Aug 14 19:45:46 2007
From: hfrisks at gmail.com (Praveen Kanakamedala)
Date: Tue, 14 Aug 2007 18:45:46 +0100
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval and
	timeSeries functionality still being maintained at all?
In-Reply-To: <18113.59073.412514.925644@ron.nulle.part>
References: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net>
	<46BF8439.2050803@itp.phys.ethz.ch>
	<06a001c7dd46$e99b8080$0c01a8c0@lincke.net>
	<18113.53818.239520.979117@stat.math.ethz.ch>
	<07b901c7de8f$47fc5fc0$0c01a8c0@lincke.net>
	<18113.59073.412514.925644@ron.nulle.part>
Message-ID: <44dc9fb70708141045h458944d8m74c179d01e82c4d8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070814/b729745a/attachment.pl 

From dlincke at lincke.net  Tue Aug 14 19:49:31 2007
From: dlincke at lincke.net (David-Michael Lincke)
Date: Tue, 14 Aug 2007 13:49:31 -0400
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval
	andtimeSeries functionality still being maintained at all?
In-Reply-To: <18113.59073.412514.925644@ron.nulle.part>
References: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net><46BF8439.2050803@itp.phys.ethz.ch><06a001c7dd46$e99b8080$0c01a8c0@lincke.net><18113.53818.239520.979117@stat.math.ethz.ch><07b901c7de8f$47fc5fc0$0c01a8c0@lincke.net>
	<18113.59073.412514.925644@ron.nulle.part>
Message-ID: <07cf01c7de9b$7ab46b90$0c01a8c0@lincke.net>

Thanks everybody for pointing this out. I did not expect updates to the
packages to be dependent on the very latest release of R being installed
given that my installation is still less then a year old.

David

-----Original Message-----
From: Dirk Eddelbuettel [mailto:edd at debian.org] 
Sent: Tuesday, August 14, 2007 1:31 PM
To: David-Michael Lincke
Cc: 'Martin Maechler'; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] RMetrics fBasics market data retrieval
andtimeSeries functionality still being maintained at all?


On 14 August 2007 at 12:22, David-Michael Lincke wrote:
| Martin,
| 
| I did run update.packages() after I first encountered problems, which
| suggested that no newer version was available. The source code of
| yahooImport() clearly scans for dates with format along the lines of
| 2007-Aug-01 while Yahoo actually delivers 2007-08-01. Here is the output
| from sessionInfo():
| 
| > sessionInfo()
| R version 2.4.0 (2006-10-03) 
| i386-pc-mingw32 

Released last October, as the date shows. You missed a patch release (always
recommended), a new major release (2.5.) and its first patch releases.

| locale:
| LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
| States.1252;LC_MONETARY=English_United
| States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
| 
| attached base packages:
| [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"

| [7] "base"     
| 
| other attached packages:
|       fBasics     fCalendar       fEcofin          MASS 
| "240.10068.1"   "240.10068"   "240.10067"      "7.2-34"

These are __current only conditional of the version of R you're running__
i.e. R found you the max(version | R release).  The problem is that you use
an outdated R releases, which leads to the outdated Rmetrics.

Please upgrade, and your problem should go away as suggested by three
different readers of this list.

Hth, Dirk

 
| David
| 
| -----Original Message-----
| From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
| Sent: Tuesday, August 14, 2007 12:03 PM
| To: David-Michael Lincke
| Cc: 'Diethelm Wuertz'; r-sig-finance at stat.math.ethz.ch
| Subject: Re: [R-SIG-Finance] RMetrics fBasics market data retrieval and
| timeSeries functionality still being maintained at all?
| 
| 
| >>>>> "DL" == David-Michael Lincke <dlincke at lincke.net>
| >>>>>     on Sun, 12 Aug 2007 21:11:39 -0400 writes:
| 
|     DL> Diethelm,
|     DL> Thank you for your response. Asyt.ts in my example is a timeSeries
| object
|     DL> constructed using yahooSeries(). Trying out the example you
provided
| I get:
| 
|     >> IBM = yahooSeries("IBM")
|     DL> trying URL
|     DL>
|
'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
|     DL> x=.csv'
|     DL> Content type 'text/csv' length unknown
|     DL> opened URL
|     DL> downloaded 12Kb
| 
|     DL> Read 250 items
|     DL> Error in matrix(unlist(x2), byrow = TRUE, nrow = length(x2)) : 
|     DL> matrix: invalid 'ncol' value (< 0)
|     DL> Error in .yahooSeries(symbols[1], from = from, to = to, nDaysBack
=
|     DL> nDaysBack,  : 
|     DL> cannot get a slot ("data") from an object of type "character"
| 
| David,
| I can confirm Diethelm's statement that  yahooSeries()
| works without problems 
| 
| __IFF__ you use current versions of R and fBasics.
| 
| Can you give your
|     sessionInfo() ## {after having loaded  fBasics} ?
| 
| I'm pretty sure your version of fBasics is not current enough.
| 
|     DL> After applying the patch to yahooImport() that I posted in my
| initial
|     DL> message the import works:
| 
|     >> IBM = yahooSeries("IBM")
|     DL> trying URL
|     DL>
|
'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
|     DL> x=.csv'
|     DL> Content type 'text/csv' length unknown
|     DL> opened URL
|     DL> downloaded 12Kb
| 
|     DL> Read 250 items
|     DL> Read 7 items
| 
|     >> colnames(IBM)
|     DL> [1] "IBM.Open"   "IBM.High"   "IBM.Low"    "IBM.Close"
"IBM.Volume"
| 
|     DL> Given that plot() has been overloaded to deal with timeSeries
| objects I
|     DL> would have expected it to have some default functionality when
| passed a
|     DL> timeSeries object short of unceremoniously failing in a lower
level
| function
|     DL> call. I did not realize it only works on single columns/data
series.
| 
|     DL> As for ohlcDailyPlot(), I checked the source code later on and
| realized that
|     DL> it expects column names that are different from how the market
data
|     DL> retrieval functions construct their timeSeries objects. This is
| easily
|     DL> adjusted for. However, it's not exactly a smooth solution from a
| design
|     DL> point of view. Also, the documentation does not mention the need
for
| an
|     DL> explicit prior call to par(mfrow = c(2, 1)) for it to result in a
| proper
|     DL> plot. But I guess maybe that's obvious to a more seasoned R user
| than me.
| 
|     DL> Anyway, thank you for pointing me in the right direction. The code
| in
|     DL> yahooImport(), however, really is broken as you can clearly verify
| by taking
|     DL> a look at the date format delivered by Yahoo which does not have
| spelled out
|     DL> three letter month names as expected by the source code.
| 
| 
| I partly agree with you, that the documentation of several of
| the Rmetrics functions is clearly "sub-optimal".
| There's currently an ongoing effort to make Rmetrics being
| loaded on more shoulders than just Diethelm's and I'd hope that
| consequently many things will improve on the documentation front
| ...
| 
| Best regards,
| Martin
| 
|     DL> David
| 
| _______________________________________________
| R-SIG-Finance at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-sig-finance
| -- Subscriber-posting only. 
| -- If you want to post, subscribe first.

-- 
Three out of two people have difficulties with fractions.


From brian at braverock.com  Tue Aug 14 20:10:10 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 14 Aug 2007 13:10:10 -0500
Subject: [R-SIG-Finance] RMetrics fBasics market data
 retrieval	andtimeSeries functionality still being maintained at all?
In-Reply-To: <07cf01c7de9b$7ab46b90$0c01a8c0@lincke.net>
References: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net><46BF8439.2050803@itp.phys.ethz.ch><06a001c7dd46$e99b8080$0c01a8c0@lincke.net><18113.53818.239520.979117@stat.math.ethz.ch><07b901c7de8f$47fc5fc0$0c01a8c0@lincke.net>	<18113.59073.412514.925644@ron.nulle.part>
	<07cf01c7de9b$7ab46b90$0c01a8c0@lincke.net>
Message-ID: <46C1F002.3080600@braverock.com>

David-Michael Lincke wrote:
> Thanks everybody for pointing this out. I did not expect updates to the
> packages to be dependent on the very latest release of R being installed
> given that my installation is still less then a year old.

They aren't required to be tied together.  However, some package authors 
set the R requirement for new package releases at the version that they 
are running, so that they "know it works" with the R version they use. 
I tend to try to keep one machine on an older version, because I know 
people stay a little bit behind, but this has caused package problems 
for me in the past as well.

update.packages() will only find updated versions that are compatible 
with your version of R, not those stating a requirement for a newer R 
than you are running.  Since the newest Rmetrics code requires R 2.5.0+, 
your update.packages didn't find it.

Regards,

   - Brian


From maechler at stat.math.ethz.ch  Wed Aug 15 15:42:39 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 15 Aug 2007 15:42:39 +0200
Subject: [R-SIG-Finance] RMetrics fBasics market data retrieval
	andtimeSeries functionality still being maintained at all?
In-Reply-To: <07cf01c7de9b$7ab46b90$0c01a8c0@lincke.net>
References: <056501c7dbc4$d01357b0$0c01a8c0@lincke.net>
	<46BF8439.2050803@itp.phys.ethz.ch>
	<06a001c7dd46$e99b8080$0c01a8c0@lincke.net>
	<18113.53818.239520.979117@stat.math.ethz.ch>
	<07b901c7de8f$47fc5fc0$0c01a8c0@lincke.net>
	<18113.59073.412514.925644@ron.nulle.part>
	<07cf01c7de9b$7ab46b90$0c01a8c0@lincke.net>
Message-ID: <18115.719.519579.657767@stat.math.ethz.ch>

>>>>> "DL" == David-Michael Lincke <dlincke at lincke.net>
>>>>>     on Tue, 14 Aug 2007 13:49:31 -0400 writes:

    DL> Thanks everybody for pointing this out. I did not expect updates to the
    DL> packages to be dependent on the very latest release of R being installed
    DL> given that my installation is still less then a year old.

They do depend only as long as you use Windoze and install
precompiled aka "binary" R packages.
Precompiled R packages are typically only available for the
latest version of R.

If you'd either use a real operating system or install all the
necessary tools on windows (beware!! that's not at all easy, and
many people have given up trying to do that) such that you can
install *source* (as opposed to "binary") R packages on windows,
you would not see such problems.

Regards,
Martin


    DL> -----Original Message-----
    DL> From: Dirk Eddelbuettel [mailto:edd at debian.org] 
    DL> Sent: Tuesday, August 14, 2007 1:31 PM
    DL> To: David-Michael Lincke
    DL> Cc: 'Martin Maechler'; r-sig-finance at stat.math.ethz.ch
    DL> Subject: Re: [R-SIG-Finance] RMetrics fBasics market data retrieval
    DL> andtimeSeries functionality still being maintained at all?


    DL> On 14 August 2007 at 12:22, David-Michael Lincke wrote:
    DL> | Martin,
    DL> | 
    DL> | I did run update.packages() after I first encountered problems, which
    DL> | suggested that no newer version was available. The source code of
    DL> | yahooImport() clearly scans for dates with format along the lines of
    DL> | 2007-Aug-01 while Yahoo actually delivers 2007-08-01. Here is the output
    DL> | from sessionInfo():
    DL> | 
    DL> | > sessionInfo()
    DL> | R version 2.4.0 (2006-10-03) 
    DL> | i386-pc-mingw32 

    DL> Released last October, as the date shows. You missed a patch release (always
    DL> recommended), a new major release (2.5.) and its first patch releases.

    DL> | locale:
    DL> | LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
    DL> | States.1252;LC_MONETARY=English_United
    DL> | States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
    DL> | 
    DL> | attached base packages:
    DL> | [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"

    DL> | [7] "base"     
    DL> | 
    DL> | other attached packages:
    DL> |       fBasics     fCalendar       fEcofin          MASS 
    DL> | "240.10068.1"   "240.10068"   "240.10067"      "7.2-34"

    DL> These are __current only conditional of the version of R you're running__
    DL> i.e. R found you the max(version | R release).  The problem is that you use
    DL> an outdated R releases, which leads to the outdated Rmetrics.

    DL> Please upgrade, and your problem should go away as suggested by three
    DL> different readers of this list.

    DL> Hth, Dirk

 
    DL> | David
    DL> | 
    DL> | -----Original Message-----
    DL> | From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
    DL> | Sent: Tuesday, August 14, 2007 12:03 PM
    DL> | To: David-Michael Lincke
    DL> | Cc: 'Diethelm Wuertz'; r-sig-finance at stat.math.ethz.ch
    DL> | Subject: Re: [R-SIG-Finance] RMetrics fBasics market data retrieval and
    DL> | timeSeries functionality still being maintained at all?
    DL> | 
    DL> | 
    DL> | >>>>> "DL" == David-Michael Lincke <dlincke at lincke.net>
    DL> | >>>>>     on Sun, 12 Aug 2007 21:11:39 -0400 writes:
    DL> | 
    DL> |     DL> Diethelm,
    DL> |     DL> Thank you for your response. Asyt.ts in my example is a timeSeries
    DL> | object
    DL> |     DL> constructed using yahooSeries(). Trying out the example you
    DL> provided
    DL> | I get:
    DL> | 
    DL> |     >> IBM = yahooSeries("IBM")
    DL> |     DL> trying URL
    DL> |     DL>
    DL> |
    DL> 'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
    DL> |     DL> x=.csv'
    DL> |     DL> Content type 'text/csv' length unknown
    DL> |     DL> opened URL
    DL> |     DL> downloaded 12Kb
    DL> | 
    DL> |     DL> Read 250 items
    DL> |     DL> Error in matrix(unlist(x2), byrow = TRUE, nrow = length(x2)) : 
    DL> |     DL> matrix: invalid 'ncol' value (< 0)
    DL> |     DL> Error in .yahooSeries(symbols[1], from = from, to = to, nDaysBack
    DL> =
    DL> |     DL> nDaysBack,  : 
    DL> |     DL> cannot get a slot ("data") from an object of type "character"
    DL> | 
    DL> | David,
    DL> | I can confirm Diethelm's statement that  yahooSeries()
    DL> | works without problems 
    DL> | 
    DL> | __IFF__ you use current versions of R and fBasics.
    DL> | 
    DL> | Can you give your
    DL> |     sessionInfo() ## {after having loaded  fBasics} ?
    DL> | 
    DL> | I'm pretty sure your version of fBasics is not current enough.
    DL> | 
    DL> |     DL> After applying the patch to yahooImport() that I posted in my
    DL> | initial
    DL> |     DL> message the import works:
    DL> | 
    DL> |     >> IBM = yahooSeries("IBM")
    DL> |     DL> trying URL
    DL> |     DL>
    DL> |
    DL> 'http://chart.yahoo.com/table.csv?s=IBM&a=7&b=12&c=2006&d=7&e=12&f=2007&g=d&
    DL> |     DL> x=.csv'
    DL> |     DL> Content type 'text/csv' length unknown
    DL> |     DL> opened URL
    DL> |     DL> downloaded 12Kb
    DL> | 
    DL> |     DL> Read 250 items
    DL> |     DL> Read 7 items
    DL> | 
    DL> |     >> colnames(IBM)
    DL> |     DL> [1] "IBM.Open"   "IBM.High"   "IBM.Low"    "IBM.Close"
    DL> "IBM.Volume"
    DL> | 
    DL> |     DL> Given that plot() has been overloaded to deal with timeSeries
    DL> | objects I
    DL> |     DL> would have expected it to have some default functionality when
    DL> | passed a
    DL> |     DL> timeSeries object short of unceremoniously failing in a lower
    DL> level
    DL> | function
    DL> |     DL> call. I did not realize it only works on single columns/data
    DL> series.
    DL> | 
    DL> |     DL> As for ohlcDailyPlot(), I checked the source code later on and
    DL> | realized that
    DL> |     DL> it expects column names that are different from how the market
    DL> data
    DL> |     DL> retrieval functions construct their timeSeries objects. This is
    DL> | easily
    DL> |     DL> adjusted for. However, it's not exactly a smooth solution from a
    DL> | design
    DL> |     DL> point of view. Also, the documentation does not mention the need
    DL> for
    DL> | an
    DL> |     DL> explicit prior call to par(mfrow = c(2, 1)) for it to result in a
    DL> | proper
    DL> |     DL> plot. But I guess maybe that's obvious to a more seasoned R user
    DL> | than me.
    DL> | 
    DL> |     DL> Anyway, thank you for pointing me in the right direction. The code
    DL> | in
    DL> |     DL> yahooImport(), however, really is broken as you can clearly verify
    DL> | by taking
    DL> |     DL> a look at the date format delivered by Yahoo which does not have
    DL> | spelled out
    DL> |     DL> three letter month names as expected by the source code.
    DL> | 
    DL> | 
    DL> | I partly agree with you, that the documentation of several of
    DL> | the Rmetrics functions is clearly "sub-optimal".
    DL> | There's currently an ongoing effort to make Rmetrics being
    DL> | loaded on more shoulders than just Diethelm's and I'd hope that
    DL> | consequently many things will improve on the documentation front
    DL> | ...
    DL> | 
    DL> | Best regards,
    DL> | Martin
    DL> | 
    DL> |     DL> David
    DL> | 
    DL> | _______________________________________________
    DL> | R-SIG-Finance at stat.math.ethz.ch mailing list
    DL> | https://stat.ethz.ch/mailman/listinfo/r-sig-finance
    DL> | -- Subscriber-posting only. 
    DL> | -- If you want to post, subscribe first.

    DL> -- 
    DL> Three out of two people have difficulties with fractions.

    DL> _______________________________________________
    DL> R-SIG-Finance at stat.math.ethz.ch mailing list
    DL> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
    DL> -- Subscriber-posting only. 
    DL> -- If you want to post, subscribe first.


From tmuhlhof at indiana.edu  Thu Aug 16 04:15:42 2007
From: tmuhlhof at indiana.edu (Tobias Muhlhofer)
Date: Wed, 15 Aug 2007 22:15:42 -0400
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C01BD8.2020007@metrak.com>
References: <46BF2A1D.7010604@indiana.edu> <46C01BD8.2020007@metrak.com>
Message-ID: <46C3B34E.1050208@indiana.edu>

Paul,

Sorry for the delay in my reply.

Yes you are looking for funds that produce intercepts that are as
strongly positive as possible when their fund returns are regressed on
the returns to the benchmark. Remember to consider standard errors!
Since we are looking at a risk-adjusted return measure, only a fund
which has a *statistically significantly* positive intercept has a
positive intercept. Sorry if this is obvious to you, but many people
miss it.

For choice of benchmark, the ASX SP200 strikes me as a very good
starting point.

In terms of calculating returns, CalculateReturns (which I have never
used, but just looked at the manual) does not seem to take into account
distributions, which is imprecise. The total one-period return at time t is:

r_t = [Price_(t) + Dividends_(t) + CapGainsDist_(t) - Price_(t-1)] /
Price_(t-1)

Here Price_(t) is the price at time t, Dividends_(t) and
CapGainsDist_(t) are respectively the dividends and capital gains
distributions made by the fund between t-1 and t. The distributions may
be substantial, so they will make a difference. If you use "adjusted
closing" prices from Yahoo Finance, you are OK, as these are adjusted
for exactly that (and splits).

Toby


paul sorenson wrote:
> Toby,
> 
> Thanks for the tips.  I am somewhat of a cynic also. When it is my own
> retirement fund, academic meets real-life in a fairly personal way!  I
> have heard figures that something like 98% (from memory) of Australian's
> "don't understand superannuation".
> 
> I am trying to get myself well into the 2%.  Writing R code leveraging
> some of the great packages out there is just a method of learning which
> I find works for me.  It can be slow going at times though.
> 
> I have picked out 8 funds to crunch through and using the ASX SP200 as
> the benchmark for exercising my code.  My "practice" data set is at
> http://www.metrak.com/tmp/exitprices.csv and I retrieved the SP200 using
> get.hist.quote("^AXJO", start="2002-01-01", quote="Close", retclass="zoo").
> 
> I have used CalculateReturns from PerformanceAnalytics to create returns
> as zoo objects so hopefully I will be able to calculate the alphas.  If
> I understand your comment below, I am looking for a more positive
> intercept on my choice of fund compared with the benchmark?
> 
> cheers
> 
> Tobias Muhlhofer wrote:
>> Paul,
>>
>> Unless you are looking at index funds, you need to see whether your
>> funds produce alpha. To do this, pick a set of benchmarks according to
>> your fund's style and investment strategy, like Morningstar category
>> index or something like that (or perhaps just the general stock market
>> plus the two Fama-French factors), regress the fund's returns on the
>> benchmark returns, and see whether you have a significantly positive
>> intercept after fees. This is the best way of measuring systematic-risk
>> adjusted returns.
>>
>> Being a finance academic (and therefore a cynic), and judging from my
>> own research, if benchmarked correctly, very few fund managers generate
>> positively significant alphas, and so I personally buy index funds for
>> whatever style I want to invest in, and there I choose the one with the
>> lowest expense ratio.
>>
>> Best,
>>     Toby
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only. -- If you want to post, subscribe first.
> 
> 
>


From brian at braverock.com  Thu Aug 16 14:35:02 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 16 Aug 2007 07:35:02 -0500
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C3B34E.1050208@indiana.edu>
References: <46BF2A1D.7010604@indiana.edu> <46C01BD8.2020007@metrak.com>
	<46C3B34E.1050208@indiana.edu>
Message-ID: <46C44476.5020308@braverock.com>

Toby,

CalculateReturns works from price series.  That's what it's for.  The 
original poster (Paul) was looking for a calculation to take a price 
series of a mutual fund and turn it into a geometric return series.

In most cases, it is not easy to get historical interest, dividend, and 
capital gains distributions information on mutual funds in a form that 
is useful for analysis (i.e. not on a paper statement), especially of 
hundreds of funds.

It would not be too difficult to add distribution calculations to the 
CalculateReturns function, but users would need to be educated that when 
distributions are included, the price series must be turned into a 
position value (NAV) series or the distributions must be scaled to a 
per-share basis.  In general, I think it is better in practice to use 
this kind of analysis with positions actually held, and to apply a time 
and cash flow weighted return calculation as you've suggested.  In 
practice, because of data availability, the closest proxy for the 
distributions you mention is often to use a total return series on the 
index, which are generally more easily obtainable from 
Ibbotson/Morningstar or similar locations.

Regards,

   - Brian

Tobias Muhlhofer wrote:
> Paul,
> 
> Sorry for the delay in my reply.
> 
> Yes you are looking for funds that produce intercepts that are as
> strongly positive as possible when their fund returns are regressed on
> the returns to the benchmark. Remember to consider standard errors!
> Since we are looking at a risk-adjusted return measure, only a fund
> which has a *statistically significantly* positive intercept has a
> positive intercept. Sorry if this is obvious to you, but many people
> miss it.
> 
> For choice of benchmark, the ASX SP200 strikes me as a very good
> starting point.
> 
> In terms of calculating returns, CalculateReturns (which I have never
> used, but just looked at the manual) does not seem to take into account
> distributions, which is imprecise. The total one-period return at time t is:
> 
> r_t = [Price_(t) + Dividends_(t) + CapGainsDist_(t) - Price_(t-1)] /
> Price_(t-1)
> 
> Here Price_(t) is the price at time t, Dividends_(t) and
> CapGainsDist_(t) are respectively the dividends and capital gains
> distributions made by the fund between t-1 and t. The distributions may
> be substantial, so they will make a difference. If you use "adjusted
> closing" prices from Yahoo Finance, you are OK, as these are adjusted
> for exactly that (and splits).
> 
> Toby
> 
> 
> paul sorenson wrote:
>> Toby,
>>
>> Thanks for the tips.  I am somewhat of a cynic also. When it is my own
>> retirement fund, academic meets real-life in a fairly personal way!  I
>> have heard figures that something like 98% (from memory) of Australian's
>> "don't understand superannuation".
>>
>> I am trying to get myself well into the 2%.  Writing R code leveraging
>> some of the great packages out there is just a method of learning which
>> I find works for me.  It can be slow going at times though.
>>
>> I have picked out 8 funds to crunch through and using the ASX SP200 as
>> the benchmark for exercising my code.  My "practice" data set is at
>> http://www.metrak.com/tmp/exitprices.csv and I retrieved the SP200 using
>> get.hist.quote("^AXJO", start="2002-01-01", quote="Close", retclass="zoo").
>>
>> I have used CalculateReturns from PerformanceAnalytics to create returns
>> as zoo objects so hopefully I will be able to calculate the alphas.  If
>> I understand your comment below, I am looking for a more positive
>> intercept on my choice of fund compared with the benchmark?
>>
>> cheers
>>
>> Tobias Muhlhofer wrote:
>>> Paul,
>>>
>>> Unless you are looking at index funds, you need to see whether your
>>> funds produce alpha. To do this, pick a set of benchmarks according to
>>> your fund's style and investment strategy, like Morningstar category
>>> index or something like that (or perhaps just the general stock market
>>> plus the two Fama-French factors), regress the fund's returns on the
>>> benchmark returns, and see whether you have a significantly positive
>>> intercept after fees. This is the best way of measuring systematic-risk
>>> adjusted returns.
>>>
>>> Being a finance academic (and therefore a cynic), and judging from my
>>> own research, if benchmarked correctly, very few fund managers generate
>>> positively significant alphas, and so I personally buy index funds for
>>> whatever style I want to invest in, and there I choose the one with the
>>> lowest expense ratio.
>>>
>>> Best,
>>>     Toby


From johnputz3655 at yahoo.com  Thu Aug 16 19:28:45 2007
From: johnputz3655 at yahoo.com (John Putz)
Date: Thu, 16 Aug 2007 10:28:45 -0700 (PDT)
Subject: [R-SIG-Finance] best method for rolling forecast based on linear fit
Message-ID: <56582.22999.qm@web50706.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070816/66fd173e/attachment.pl 

From brian at braverock.com  Thu Aug 16 19:44:51 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 16 Aug 2007 12:44:51 -0500
Subject: [R-SIG-Finance] best method for rolling forecast based on
 linear fit
In-Reply-To: <56582.22999.qm@web50706.mail.re2.yahoo.com>
References: <56582.22999.qm@web50706.mail.re2.yahoo.com>
Message-ID: <46C48D13.7070605@braverock.com>

John Putz wrote:
> Hello,
> 
> A basic question.  Can anybody point me towards the best method to use to perform a rolling 1 step ahead forecast of a price series based on a rolling N day linear fit?  I believe I used the dse package in the past, but my recollection is it was somewhat cumbersome.

If you're using a linear model as your predictor, it's not strictly 
speaking a "one step ahead" prediction, but rather an ex post rolling 
window analysis.

"best" is highly subjective depending on how fancy you want to be.

zoo has the rollapply function with a configurable window to roll over.

The rollingRegression function in PerformanceAnalytics might make things 
a little easier to use.

There are also many robust regressors that you might wish to consider, 
especially if you're using a relatively short window.

Regards,

    - Brian


From johnputz3655 at yahoo.com  Thu Aug 16 22:47:00 2007
From: johnputz3655 at yahoo.com (John Putz)
Date: Thu, 16 Aug 2007 13:47:00 -0700 (PDT)
Subject: [R-SIG-Finance] best method for rolling forecast based on
	linear fit
Message-ID: <57999.5298.qm@web50709.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070816/718da7fd/attachment.pl 

From fparlamis at mac.com  Fri Aug 17 09:09:54 2007
From: fparlamis at mac.com (Franklin Parlamis)
Date: Fri, 17 Aug 2007 00:09:54 -0700
Subject: [R-SIG-Finance] Hedge Fund Job Opening
Message-ID: <24360A70-0114-1000-8B90-2032CE42DC3B-Webmail-10010@mac.com>

Hi all.  

I've been lurking and posting on this list for a few years now.  Prior to that, I managed the US Convertible Arbitrage portfolio for Amaranth Advisors.  I recently agreed to manage a similar portfolio for a different hedge fund and am looking for someone to join me, essentially as the principal quant for a new San Francisco office.  I've been very impressed with the posters on this list and am hopeful that some of you will consider submitting your resume for the position.

The job announcement follows.

Franklin Parlamis 

(My apologies for posting in both sig-finance and devel -- finance experience is not mandatory)

***

Statistician/Programmer Sought for San Francisco Hedge Fund Office.

Pine River Capital Management (with assets under management in excess of USD 750 million and offices in Minneapolis, London and Hong Kong) is seeking a Statistician/Programmer for its new San Francisco office. 

You will be responsible for designing and programming modeling tools used in convertible and capital structure arbitrage.  You will also be responsible for analyzing market data to identify trade opportunities as well as promising hedging strategies.  You will work in a small office environment, on the trading desk and directly with the portfolio manager.

Ideally you will have a PhD in Statistics or a closely related discipline and significant experience with a statistical programming language such as S.  Other pluses include a background in Bayesian inference and experience with object-oriented programming.

In addition to base compensation, a performance bonus is anticipated to be paid out of book trading profits.

Candidates should please email their resumes to franklin.parlamis at pinerivercapital.com


From brian at braverock.com  Fri Aug 17 13:55:24 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 17 Aug 2007 06:55:24 -0500
Subject: [R-SIG-Finance] best method for rolling forecast based on
 linear fit
In-Reply-To: <57999.5298.qm@web50709.mail.re2.yahoo.com>
References: <57999.5298.qm@web50709.mail.re2.yahoo.com>
Message-ID: <46C58CAC.2010708@braverock.com>

John Putz wrote:
> When I looked at the rollingRegression function it seems to allow you to 
> compare the performance of the fit over a rolling window.  That seems to 
> be different from what I was looking for which is to perform a fit for a 
> period from t1 to t2 and then see how it performs at predicting 
> the return at t2+1 as the window (t1,t2) rolls through the my data set.  
> Am I misunderstanding what these functions do?  At the end of the day 
> I'm looking for a vector of forecasted values that I can compare to the 
> actual (out-of-sample) values.  
> Thanks again, John.

RollingRegression performs the regression over a rolling window on a 
time series.  Nothing Else.  rollapply in zoo (which is used by 
RollingRegression) would do the same.

Regression, in and of itself, is not a predictive tool.  Unlike GARCH, 
ARIMA, etc, there is not a built-in "prediction" from a regression 
analysis.

The *comparison* of how well the ex post regression result on the 
rolling window compares to the out of sample t+1 performance is your 
(proposed) trading or analytical model.  You would need to write the 
code to do that comparison at the appropriate time lag.

Regards,

    - Brian

> ----- Original Message ----
> From: Brian G. Peterson <brian at braverock.com>
> To: John Putz <johnputz3655 at yahoo.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Sent: Thursday, August 16, 2007 10:44:51 AM
> Subject: Re: [R-SIG-Finance] best method for rolling forecast based on 
> linear fit
> 
> John Putz wrote:
>  > Hello,
>  >
>  > A basic question.  Can anybody point me towards the best method to 
> use to perform a rolling 1 step ahead forecast of a price series based 
> on a rolling N day linear fit?  I believe I used the dse package in the 
> past, but my recollection is it was somewhat cumbersome.
> 
> If you're using a linear model as your predictor, it's not strictly
> speaking a "one step ahead" prediction, but rather an ex post rolling
> window analysis.
> 
> "best" is highly subjective depending on how fancy you want to be.
> 
> zoo has the rollapply function with a configurable window to roll over.
> 
> The rollingRegression function in PerformanceAnalytics might make things
> a little easier to use.
> 
> There are also many robust regressors that you might wish to consider,
> especially if you're using a relatively short window.
> 
> Regards,
> 
>     - Brian
>


From ggrothendieck at gmail.com  Fri Aug 17 14:18:46 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 17 Aug 2007 08:18:46 -0400
Subject: [R-SIG-Finance] best method for rolling forecast based on
	linear fit
In-Reply-To: <56582.22999.qm@web50706.mail.re2.yahoo.com>
References: <56582.22999.qm@web50706.mail.re2.yahoo.com>
Message-ID: <971536df0708170518v57f4daecrb61d2899dd83677c@mail.gmail.com>

Look at ?arima and note the xreg= argument.  Also ?predict.arima
and note the n.ahead= argument.

On 8/16/07, John Putz <johnputz3655 at yahoo.com> wrote:
> Hello,
>
> A basic question.  Can anybody point me towards the best method to use to perform a rolling 1 step ahead forecast of a price series based on a rolling N day linear fit?  I believe I used the dse package in the past, but my recollection is it was somewhat cumbersome.
>
> Thanks, John.
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From sf at metrak.com  Sat Aug 18 13:05:33 2007
From: sf at metrak.com (paul sorenson)
Date: Sat, 18 Aug 2007 21:05:33 +1000
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C44476.5020308@braverock.com>
References: <46BF2A1D.7010604@indiana.edu> <46C01BD8.2020007@metrak.com>
	<46C3B34E.1050208@indiana.edu> <46C44476.5020308@braverock.com>
Message-ID: <46C6D27D.10602@metrak.com>

Brian and Tobias,

Yes I believe the data I have includes everything from fees to dividends.

FWIW the web site I get this data from is 
http://www.colonialfirststate.com.au/ and a significant factor in 
choosing them was because I can download daily prices in CSV format. 
(For any Aussies on the list who might be interested, I can provide some 
python code to download the prices for all the funds), please feel free 
to contact me off the list.

cheers

Brian G. Peterson wrote:
> Toby,
> 
> CalculateReturns works from price series.  That's what it's for.  The 
> original poster (Paul) was looking for a calculation to take a price 
> series of a mutual fund and turn it into a geometric return series.
> 
> In most cases, it is not easy to get historical interest, dividend, and 
> capital gains distributions information on mutual funds in a form that 
> is useful for analysis (i.e. not on a paper statement), especially of 
> hundreds of funds.
> 
> It would not be too difficult to add distribution calculations to the 
> CalculateReturns function, but users would need to be educated that when 
> distributions are included, the price series must be turned into a 
> position value (NAV) series or the distributions must be scaled to a 
> per-share basis.  In general, I think it is better in practice to use 
> this kind of analysis with positions actually held, and to apply a time 
> and cash flow weighted return calculation as you've suggested.  In 
> practice, because of data availability, the closest proxy for the 
> distributions you mention is often to use a total return series on the 
> index, which are generally more easily obtainable from 
> Ibbotson/Morningstar or similar locations.
> 
> Regards,
> 
>   - Brian
> 
> Tobias Muhlhofer wrote:
>> Paul,
>>
>> Sorry for the delay in my reply.
>>
>> Yes you are looking for funds that produce intercepts that are as
>> strongly positive as possible when their fund returns are regressed on
>> the returns to the benchmark. Remember to consider standard errors!
>> Since we are looking at a risk-adjusted return measure, only a fund
>> which has a *statistically significantly* positive intercept has a
>> positive intercept. Sorry if this is obvious to you, but many people
>> miss it.
>>
>> For choice of benchmark, the ASX SP200 strikes me as a very good
>> starting point.
>>
>> In terms of calculating returns, CalculateReturns (which I have never
>> used, but just looked at the manual) does not seem to take into account
>> distributions, which is imprecise. The total one-period return at time 
>> t is:
>>
>> r_t = [Price_(t) + Dividends_(t) + CapGainsDist_(t) - Price_(t-1)] /
>> Price_(t-1)
>>
>> Here Price_(t) is the price at time t, Dividends_(t) and
>> CapGainsDist_(t) are respectively the dividends and capital gains
>> distributions made by the fund between t-1 and t. The distributions may
>> be substantial, so they will make a difference. If you use "adjusted
>> closing" prices from Yahoo Finance, you are OK, as these are adjusted
>> for exactly that (and splits).
>>
>> Toby
>>
>>
>> paul sorenson wrote:
>>> Toby,
>>>
>>> Thanks for the tips.  I am somewhat of a cynic also. When it is my own
>>> retirement fund, academic meets real-life in a fairly personal way!  I
>>> have heard figures that something like 98% (from memory) of Australian's
>>> "don't understand superannuation".
>>>
>>> I am trying to get myself well into the 2%.  Writing R code leveraging
>>> some of the great packages out there is just a method of learning which
>>> I find works for me.  It can be slow going at times though.
>>>
>>> I have picked out 8 funds to crunch through and using the ASX SP200 as
>>> the benchmark for exercising my code.  My "practice" data set is at
>>> http://www.metrak.com/tmp/exitprices.csv and I retrieved the SP200 using
>>> get.hist.quote("^AXJO", start="2002-01-01", quote="Close", 
>>> retclass="zoo").
>>>
>>> I have used CalculateReturns from PerformanceAnalytics to create returns
>>> as zoo objects so hopefully I will be able to calculate the alphas.  If
>>> I understand your comment below, I am looking for a more positive
>>> intercept on my choice of fund compared with the benchmark?
>>>
>>> cheers
>>>
>>> Tobias Muhlhofer wrote:
>>>> Paul,
>>>>
>>>> Unless you are looking at index funds, you need to see whether your
>>>> funds produce alpha. To do this, pick a set of benchmarks according to
>>>> your fund's style and investment strategy, like Morningstar category
>>>> index or something like that (or perhaps just the general stock market
>>>> plus the two Fama-French factors), regress the fund's returns on the
>>>> benchmark returns, and see whether you have a significantly positive
>>>> intercept after fees. This is the best way of measuring systematic-risk
>>>> adjusted returns.
>>>>
>>>> Being a finance academic (and therefore a cynic), and judging from my
>>>> own research, if benchmarked correctly, very few fund managers generate
>>>> positively significant alphas, and so I personally buy index funds for
>>>> whatever style I want to invest in, and there I choose the one with the
>>>> lowest expense ratio.
>>>>
>>>> Best,
>>>>     Toby


From sf at metrak.com  Sat Aug 18 13:30:14 2007
From: sf at metrak.com (paul sorenson)
Date: Sat, 18 Aug 2007 21:30:14 +1000
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C3B34E.1050208@indiana.edu>
References: <46BF2A1D.7010604@indiana.edu> <46C01BD8.2020007@metrak.com>
	<46C3B34E.1050208@indiana.edu>
Message-ID: <46C6D846.4020106@metrak.com>

Tobias,

Thanks for the answer and no problems about the delay (its nothing 
compared with trying to move your "hard earned" from one fund manager to 
another).

I am really just working my way through this stuff and not much is 
"obvious to me".  I will have to go back and check how significant the 
alphas are.

At the moment I am just plotting stuff, printing out tables and doing 
lots of reading.  I am on a fairly steep learning curve with R, LaTeX, 
Sweave as well the core subject matter.  At the end of the day, I want 
to be able to select a portfolio of super funds based on some objective 
measures.

cheers

Tobias Muhlhofer wrote:
> Paul,
> 
> Sorry for the delay in my reply.
> 
> Yes you are looking for funds that produce intercepts that are as
> strongly positive as possible when their fund returns are regressed on
> the returns to the benchmark. Remember to consider standard errors!
> Since we are looking at a risk-adjusted return measure, only a fund
> which has a *statistically significantly* positive intercept has a
> positive intercept. Sorry if this is obvious to you, but many people
> miss it.
> 
> For choice of benchmark, the ASX SP200 strikes me as a very good
> starting point.
> 
> In terms of calculating returns, CalculateReturns (which I have never
> used, but just looked at the manual) does not seem to take into account
> distributions, which is imprecise. The total one-period return at time t is:
> 
> r_t = [Price_(t) + Dividends_(t) + CapGainsDist_(t) - Price_(t-1)] /
> Price_(t-1)
> 
> Here Price_(t) is the price at time t, Dividends_(t) and
> CapGainsDist_(t) are respectively the dividends and capital gains
> distributions made by the fund between t-1 and t. The distributions may
> be substantial, so they will make a difference. If you use "adjusted
> closing" prices from Yahoo Finance, you are OK, as these are adjusted
> for exactly that (and splits).
> 
> Toby
> 
> 
> paul sorenson wrote:
>> Toby,
>>
>> Thanks for the tips.  I am somewhat of a cynic also. When it is my own
>> retirement fund, academic meets real-life in a fairly personal way!  I
>> have heard figures that something like 98% (from memory) of Australian's
>> "don't understand superannuation".
>>
>> I am trying to get myself well into the 2%.  Writing R code leveraging
>> some of the great packages out there is just a method of learning which
>> I find works for me.  It can be slow going at times though.
>>
>> I have picked out 8 funds to crunch through and using the ASX SP200 as
>> the benchmark for exercising my code.  My "practice" data set is at
>> http://www.metrak.com/tmp/exitprices.csv and I retrieved the SP200 using
>> get.hist.quote("^AXJO", start="2002-01-01", quote="Close", retclass="zoo").
>>
>> I have used CalculateReturns from PerformanceAnalytics to create returns
>> as zoo objects so hopefully I will be able to calculate the alphas.  If
>> I understand your comment below, I am looking for a more positive
>> intercept on my choice of fund compared with the benchmark?
>>
>> cheers
>>
>> Tobias Muhlhofer wrote:
>>> Paul,
>>>
>>> Unless you are looking at index funds, you need to see whether your
>>> funds produce alpha. To do this, pick a set of benchmarks according to
>>> your fund's style and investment strategy, like Morningstar category
>>> index or something like that (or perhaps just the general stock market
>>> plus the two Fama-French factors), regress the fund's returns on the
>>> benchmark returns, and see whether you have a significantly positive
>>> intercept after fees. This is the best way of measuring systematic-risk
>>> adjusted returns.
>>>
>>> Being a finance academic (and therefore a cynic), and judging from my
>>> own research, if benchmarked correctly, very few fund managers generate
>>> positively significant alphas, and so I personally buy index funds for
>>> whatever style I want to invest in, and there I choose the one with the
>>> lowest expense ratio.
>>>
>>> Best,
>>>     Toby
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only. -- If you want to post, subscribe first.
>>
>>


From sf at metrak.com  Sat Aug 18 13:52:41 2007
From: sf at metrak.com (paul sorenson)
Date: Sat, 18 Aug 2007 21:52:41 +1000
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C3B34E.1050208@indiana.edu>
References: <46BF2A1D.7010604@indiana.edu> <46C01BD8.2020007@metrak.com>
	<46C3B34E.1050208@indiana.edu>
Message-ID: <46C6DD89.3060604@metrak.com>

Toby,

I ran a pairs plot on the daily fund returns as well as calculating the 
correlation coefficient (Pearson).

The pairs plot is reproduced at http://www.metrak.com/tmp/exch09.png and 
unless I am missing something, some of these "look" significant whereas 
some don't.

cheers

Tobias Muhlhofer wrote:
> Paul,
> 
> Sorry for the delay in my reply.
> 
> Yes you are looking for funds that produce intercepts that are as
> strongly positive as possible when their fund returns are regressed on
> the returns to the benchmark. Remember to consider standard errors!
> Since we are looking at a risk-adjusted return measure, only a fund
> which has a *statistically significantly* positive intercept has a
> positive intercept. Sorry if this is obvious to you, but many people
> miss it.
> 
> For choice of benchmark, the ASX SP200 strikes me as a very good
> starting point.
> 
> In terms of calculating returns, CalculateReturns (which I have never
> used, but just looked at the manual) does not seem to take into account
> distributions, which is imprecise. The total one-period return at time t is:
> 
> r_t = [Price_(t) + Dividends_(t) + CapGainsDist_(t) - Price_(t-1)] /
> Price_(t-1)
> 
> Here Price_(t) is the price at time t, Dividends_(t) and
> CapGainsDist_(t) are respectively the dividends and capital gains
> distributions made by the fund between t-1 and t. The distributions may
> be substantial, so they will make a difference. If you use "adjusted
> closing" prices from Yahoo Finance, you are OK, as these are adjusted
> for exactly that (and splits).
> 
> Toby
> 
> 
> paul sorenson wrote:
>> Toby,
>>
>> Thanks for the tips.  I am somewhat of a cynic also. When it is my own
>> retirement fund, academic meets real-life in a fairly personal way!  I
>> have heard figures that something like 98% (from memory) of Australian's
>> "don't understand superannuation".
>>
>> I am trying to get myself well into the 2%.  Writing R code leveraging
>> some of the great packages out there is just a method of learning which
>> I find works for me.  It can be slow going at times though.
>>
>> I have picked out 8 funds to crunch through and using the ASX SP200 as
>> the benchmark for exercising my code.  My "practice" data set is at
>> http://www.metrak.com/tmp/exitprices.csv and I retrieved the SP200 using
>> get.hist.quote("^AXJO", start="2002-01-01", quote="Close", retclass="zoo").
>>
>> I have used CalculateReturns from PerformanceAnalytics to create returns
>> as zoo objects so hopefully I will be able to calculate the alphas.  If
>> I understand your comment below, I am looking for a more positive
>> intercept on my choice of fund compared with the benchmark?
>>
>> cheers
>>
>> Tobias Muhlhofer wrote:
>>> Paul,
>>>
>>> Unless you are looking at index funds, you need to see whether your
>>> funds produce alpha. To do this, pick a set of benchmarks according to
>>> your fund's style and investment strategy, like Morningstar category
>>> index or something like that (or perhaps just the general stock market
>>> plus the two Fama-French factors), regress the fund's returns on the
>>> benchmark returns, and see whether you have a significantly positive
>>> intercept after fees. This is the best way of measuring systematic-risk
>>> adjusted returns.
>>>
>>> Being a finance academic (and therefore a cynic), and judging from my
>>> own research, if benchmarked correctly, very few fund managers generate
>>> positively significant alphas, and so I personally buy index funds for
>>> whatever style I want to invest in, and there I choose the one with the
>>> lowest expense ratio.
>>>
>>> Best,
>>>     Toby
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only. -- If you want to post, subscribe first.
>>
>>


From brian at braverock.com  Sat Aug 18 16:04:57 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Sat, 18 Aug 2007 09:04:57 -0500
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C6DD89.3060604@metrak.com>
References: <46BF2A1D.7010604@indiana.edu>
	<46C01BD8.2020007@metrak.com>	<46C3B34E.1050208@indiana.edu>
	<46C6DD89.3060604@metrak.com>
Message-ID: <46C6FC89.1040906@braverock.com>

paul sorenson wrote:
> I ran a pairs plot on the daily fund returns as well as calculating the 
> correlation coefficient (Pearson).
> 
> The pairs plot is reproduced at http://www.metrak.com/tmp/exch09.png and 
> unless I am missing something, some of these "look" significant whereas 
> some don't.

The pairs plot will certainly show you funds that closely track the 
index.  A quick check of cor() (or the Pearson correlation coefficient) 
and CAPM.alpha() will do the same.  A pairs plot (and to a lesser extent 
correlation) won't show you anything about systematic out-performance or 
under-performance, while alpha is a good indicator if the benchmark you 
choose is indicative of the investment universe of the fund.  Another 
good indicator is Sortino's Upside Potential Ratio, especially if you 
choose the benchmark index standard deviation as your MAR.

CAPM alpha will not be a good indicator if you choose an index that is 
different from the investment style of the fund.  For example, using a 
SP200 index with a fixed income fund wouldn't make any sense.

Cheers,

    -Brian


From sf at metrak.com  Sun Aug 19 01:04:43 2007
From: sf at metrak.com (paul sorenson)
Date: Sun, 19 Aug 2007 09:04:43 +1000
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C6FC89.1040906@braverock.com>
References: <46BF2A1D.7010604@indiana.edu>
	<46C01BD8.2020007@metrak.com>	<46C3B34E.1050208@indiana.edu>
	<46C6DD89.3060604@metrak.com> <46C6FC89.1040906@braverock.com>
Message-ID: <46C77B0B.5000200@metrak.com>

Probably not unsurprisingly, the correlation between the SP200 and the 
global share fund daily returns goes up (from 0.16 to 0.42) when I 
insert a 1 day lag.

http://www.metrak.com/tmp/exch10.png has plots produced by ccf().

Brian - if I want to look at returns over a different period using 
PerformanceAnalytics when the base data is daily, is the normal strategy 
just to undersample prices before calling CalculateReturns?

I am guessing that the correlation of the SP200 and global share fund 
would increase when looking at a longer time period.  I also want to 
compare it with some other data which comes out monthly.

cheers

BTW sorry if I am boring you guys with what must be very basic stuff - 
just tell me.


Brian G. Peterson wrote:
> paul sorenson wrote:
>> I ran a pairs plot on the daily fund returns as well as calculating 
>> the correlation coefficient (Pearson).
>>
>> The pairs plot is reproduced at http://www.metrak.com/tmp/exch09.png 
>> and unless I am missing something, some of these "look" significant 
>> whereas some don't.
> 
> The pairs plot will certainly show you funds that closely track the 
> index.  A quick check of cor() (or the Pearson correlation coefficient) 
> and CAPM.alpha() will do the same.  A pairs plot (and to a lesser extent 
> correlation) won't show you anything about systematic out-performance or 
> under-performance, while alpha is a good indicator if the benchmark you 
> choose is indicative of the investment universe of the fund.  Another 
> good indicator is Sortino's Upside Potential Ratio, especially if you 
> choose the benchmark index standard deviation as your MAR.
> 
> CAPM alpha will not be a good indicator if you choose an index that is 
> different from the investment style of the fund.  For example, using a 
> SP200 index with a fixed income fund wouldn't make any sense.
> 
> Cheers,
> 
>    -Brian


From patrick at burns-stat.com  Sun Aug 19 11:23:38 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Sun, 19 Aug 2007 10:23:38 +0100
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C77B0B.5000200@metrak.com>
References: <46BF2A1D.7010604@indiana.edu>	<46C01BD8.2020007@metrak.com>	<46C3B34E.1050208@indiana.edu>	<46C6DD89.3060604@metrak.com>
	<46C6FC89.1040906@braverock.com> <46C77B0B.5000200@metrak.com>
Message-ID: <46C80C1A.7050909@burns-stat.com>

paul sorenson wrote:

>Probably not unsurprisingly, the correlation between the SP200 and the 
>global share fund daily returns goes up (from 0.16 to 0.42) when I 
>insert a 1 day lag.
>  
>

Careful.

There are asynchrony issues with global data, which definitely affect
the correlation (it is too low). Presumably the global return series has
asynchrony issues all by itself.

The working paper for Burns, Engle and Mezrich (1998) is available
at http://www.econ.ucsd.edu/papers/dp97.html

That paper talks about the effects of asynchrony and proposes a
method of backing out data without asynchrony.  Our investigation
suggested that using weekly data is adequate for avoiding asynchrony
effects.

>http://www.metrak.com/tmp/exch10.png has plots produced by ccf().
>
>Brian - if I want to look at returns over a different period using 
>PerformanceAnalytics when the base data is daily, is the normal strategy 
>just to undersample prices before calling CalculateReturns?
>
>I am guessing that the correlation of the SP200 and global share fund 
>would increase when looking at a longer time period.  I also want to 
>compare it with some other data which comes out monthly.
>
>cheers
>
>BTW sorry if I am boring you guys with what must be very basic stuff - 
>just tell me.
>  
>

Asynchrony is neither basic (i.e., well-studied) nor very well appreciated.
I don't vote this message boring.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

>
>Brian G. Peterson wrote:
>  
>
>>paul sorenson wrote:
>>    
>>
>>>I ran a pairs plot on the daily fund returns as well as calculating 
>>>the correlation coefficient (Pearson).
>>>
>>>The pairs plot is reproduced at http://www.metrak.com/tmp/exch09.png 
>>>and unless I am missing something, some of these "look" significant 
>>>whereas some don't.
>>>      
>>>
>>The pairs plot will certainly show you funds that closely track the 
>>index.  A quick check of cor() (or the Pearson correlation coefficient) 
>>and CAPM.alpha() will do the same.  A pairs plot (and to a lesser extent 
>>correlation) won't show you anything about systematic out-performance or 
>>under-performance, while alpha is a good indicator if the benchmark you 
>>choose is indicative of the investment universe of the fund.  Another 
>>good indicator is Sortino's Upside Potential Ratio, especially if you 
>>choose the benchmark index standard deviation as your MAR.
>>
>>CAPM alpha will not be a good indicator if you choose an index that is 
>>different from the investment style of the fund.  For example, using a 
>>SP200 index with a fixed income fund wouldn't make any sense.
>>
>>Cheers,
>>
>>   -Brian
>>    
>>
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From bbands at gmail.com  Sun Aug 19 16:20:37 2007
From: bbands at gmail.com (BBands)
Date: Sun, 19 Aug 2007 07:20:37 -0700
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C80C1A.7050909@burns-stat.com>
References: <46BF2A1D.7010604@indiana.edu> <46C01BD8.2020007@metrak.com>
	<46C3B34E.1050208@indiana.edu> <46C6DD89.3060604@metrak.com>
	<46C6FC89.1040906@braverock.com> <46C77B0B.5000200@metrak.com>
	<46C80C1A.7050909@burns-stat.com>
Message-ID: <6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>

On 8/19/07, Patrick Burns <patrick at burns-stat.com> wrote:
> That paper talks about the effects of asynchrony and proposes a
> method of backing out data without asynchrony.  Our investigation
> suggested that using weekly data is adequate for avoiding asynchrony
> effects.

We have done a lot of work on this problem and agree with the choice
of weekly data.

Paul,

The use of benchmarks may not be the optimal path in this application,
relatively simple ranking might be more viable. As a compromise, you
might try looking at ranked Sharpe ratios...

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From brian at braverock.com  Sun Aug 19 18:01:03 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Sun, 19 Aug 2007 11:01:03 -0500
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>
References: <46BF2A1D.7010604@indiana.edu>
	<46C01BD8.2020007@metrak.com>	<46C3B34E.1050208@indiana.edu>
	<46C6DD89.3060604@metrak.com>	<46C6FC89.1040906@braverock.com>
	<46C77B0B.5000200@metrak.com>	<46C80C1A.7050909@burns-stat.com>
	<6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>
Message-ID: <46C8693F.1040609@braverock.com>

BBands wrote:
> On 8/19/07, Patrick Burns <patrick at burns-stat.com> wrote:
>> That paper talks about the effects of asynchrony and proposes a
>> method of backing out data without asynchrony.  Our investigation
>> suggested that using weekly data is adequate for avoiding asynchrony
>> effects.
> 
> We have done a lot of work on this problem and agree with the choice
> of weekly data.
> 
> Paul,
> 
> The use of benchmarks may not be the optimal path in this application,
> relatively simple ranking might be more viable. As a compromise, you
> might try looking at ranked Sharpe ratios...

A stack ranking of risk/reward ratios is a good idea.  I would recommend 
using either a Cornish Fisher modified Sharpe ratio (to take possible 
non-normality of distributions into account) or Sortino's Upside 
Potential Ratio.  Even Sharpe himself recommends the use of Information 
Ratio preferentially to the original Sharpe ratio, but old habits die 
hard...

To answer an earlier question on sub-sampling:  Yes, from daily *price* 
data you can construct a weekly or monthly series by simply taking the 
price at the end of the week or month, and constructing your returns 
series from the end of period closing price.  The zoo library also has a 
good implementation of the aggregate() function for timeseries data to 
help you automate the sampling while maintaining your original daily data.

Regards,

    - Brian


From bbands at gmail.com  Sun Aug 19 21:27:02 2007
From: bbands at gmail.com (BBands)
Date: Sun, 19 Aug 2007 12:27:02 -0700
Subject: [R-SIG-Finance]  making sense of 100's of funds
In-Reply-To: <6e8360ad0708191225x72a91bdbw1a67f31cbf2e7b02@mail.gmail.com>
References: <46BF2A1D.7010604@indiana.edu> <46C01BD8.2020007@metrak.com>
	<46C3B34E.1050208@indiana.edu> <46C6DD89.3060604@metrak.com>
	<46C6FC89.1040906@braverock.com> <46C77B0B.5000200@metrak.com>
	<46C80C1A.7050909@burns-stat.com>
	<6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>
	<46C8693F.1040609@braverock.com>
	<6e8360ad0708191225x72a91bdbw1a67f31cbf2e7b02@mail.gmail.com>
Message-ID: <6e8360ad0708191227w7d3c52afs39c53d3ad18c712c@mail.gmail.com>

BBands wrote:
> > The use of benchmarks may not be the optimal path in this application,
> > relatively simple ranking might be more viable. As a compromise, you
> > might try looking at ranked Sharpe ratios...

On 8/19/07, Brian G. Peterson <brian at braverock.com> wrote:
> A stack ranking of risk/reward ratios is a good idea.  I would recommend
> using either a Cornish Fisher modified Sharpe ratio (to take possible
> non-normality of distributions into account) or Sortino's Upside
> Potential Ratio.  Even Sharpe himself recommends the use of Information
> Ratio preferentially to the original Sharpe ratio, but old habits die
> hard...

Old habits do die hard... For those interested, Bob Fulks has done a
lot of interesting work with the Sharpe ratio. A quick search on his
name might be useful.

    jab
--
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From sf at metrak.com  Mon Aug 20 00:26:23 2007
From: sf at metrak.com (paul sorenson)
Date: Mon, 20 Aug 2007 08:26:23 +1000
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C80C1A.7050909@burns-stat.com>
References: <46BF2A1D.7010604@indiana.edu>	<46C01BD8.2020007@metrak.com>	<46C3B34E.1050208@indiana.edu>	<46C6DD89.3060604@metrak.com>	<46C6FC89.1040906@braverock.com>
	<46C77B0B.5000200@metrak.com> <46C80C1A.7050909@burns-stat.com>
Message-ID: <46C8C38F.1020302@metrak.com>

Patrick,

That is an interesting paper, in this case I was asking myself the 
question, "if I know that today's Melbourne prices are correlated to 
yesterdays  NY prices, is there a possibility of taking advantage of 
that?"  That is, not so much from putting a portfolio together, but 
daily investment decisions.

cheers

Patrick Burns wrote:
> paul sorenson wrote:
> 
>> Probably not unsurprisingly, the correlation between the SP200 and the 
>> global share fund daily returns goes up (from 0.16 to 0.42) when I 
>> insert a 1 day lag.
>>  
>>
> 
> Careful.
> 
> There are asynchrony issues with global data, which definitely affect
> the correlation (it is too low). Presumably the global return series has
> asynchrony issues all by itself.
> 
> The working paper for Burns, Engle and Mezrich (1998) is available
> at http://www.econ.ucsd.edu/papers/dp97.html
> 
> That paper talks about the effects of asynchrony and proposes a
> method of backing out data without asynchrony.  Our investigation
> suggested that using weekly data is adequate for avoiding asynchrony
> effects.
> 
>> http://www.metrak.com/tmp/exch10.png has plots produced by ccf().
>>
>> Brian - if I want to look at returns over a different period using 
>> PerformanceAnalytics when the base data is daily, is the normal 
>> strategy just to undersample prices before calling CalculateReturns?
>>
>> I am guessing that the correlation of the SP200 and global share fund 
>> would increase when looking at a longer time period.  I also want to 
>> compare it with some other data which comes out monthly.
>>
>> cheers
>>
>> BTW sorry if I am boring you guys with what must be very basic stuff - 
>> just tell me.
>>  
>>
> 
> Asynchrony is neither basic (i.e., well-studied) nor very well appreciated.
> I don't vote this message boring.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
>>
>> Brian G. Peterson wrote:
>>  
>>
>>> paul sorenson wrote:
>>>   
>>>> I ran a pairs plot on the daily fund returns as well as calculating 
>>>> the correlation coefficient (Pearson).
>>>>
>>>> The pairs plot is reproduced at http://www.metrak.com/tmp/exch09.png 
>>>> and unless I am missing something, some of these "look" significant 
>>>> whereas some don't.
>>>>     
>>> The pairs plot will certainly show you funds that closely track the 
>>> index.  A quick check of cor() (or the Pearson correlation 
>>> coefficient) and CAPM.alpha() will do the same.  A pairs plot (and to 
>>> a lesser extent correlation) won't show you anything about systematic 
>>> out-performance or under-performance, while alpha is a good indicator 
>>> if the benchmark you choose is indicative of the investment universe 
>>> of the fund.  Another good indicator is Sortino's Upside Potential 
>>> Ratio, especially if you choose the benchmark index standard 
>>> deviation as your MAR.
>>>
>>> CAPM alpha will not be a good indicator if you choose an index that 
>>> is different from the investment style of the fund.  For example, 
>>> using a SP200 index with a fixed income fund wouldn't make any sense.
>>>
>>> Cheers,
>>>
>>>   -Brian
>>>   
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only. -- If you want to post, subscribe first.
>>
>>
>>  
>>


From sf at metrak.com  Mon Aug 20 00:27:11 2007
From: sf at metrak.com (paul sorenson)
Date: Mon, 20 Aug 2007 08:27:11 +1000
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>
References: <46BF2A1D.7010604@indiana.edu>
	<46C01BD8.2020007@metrak.com>	<46C3B34E.1050208@indiana.edu>
	<46C6DD89.3060604@metrak.com>	<46C6FC89.1040906@braverock.com>
	<46C77B0B.5000200@metrak.com>	<46C80C1A.7050909@burns-stat.com>
	<6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>
Message-ID: <46C8C3BF.1090404@metrak.com>

Jab,

Thanks for the tip, and hopefully that is something like where I will
end up.

Ultimately I want to create some sort of personal investment portfolio
but the learning exercise to get there is also important to me,
including trying techniques that may be less applicable and
understanding why.

cheers

BBands wrote:
> On 8/19/07, Patrick Burns <patrick at burns-stat.com> wrote:
>> That paper talks about the effects of asynchrony and proposes a
>> method of backing out data without asynchrony.  Our investigation
>> suggested that using weekly data is adequate for avoiding asynchrony
>> effects.
> 
> We have done a lot of work on this problem and agree with the choice
> of weekly data.
> 
> Paul,
> 
> The use of benchmarks may not be the optimal path in this application,
> relatively simple ranking might be more viable. As a compromise, you
> might try looking at ranked Sharpe ratios...
> 
>     jab


From sf at metrak.com  Mon Aug 20 01:58:33 2007
From: sf at metrak.com (paul sorenson)
Date: Mon, 20 Aug 2007 09:58:33 +1000
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C8693F.1040609@braverock.com>
References: <46BF2A1D.7010604@indiana.edu>	<46C01BD8.2020007@metrak.com>	<46C3B34E.1050208@indiana.edu>	<46C6DD89.3060604@metrak.com>	<46C6FC89.1040906@braverock.com>	<46C77B0B.5000200@metrak.com>	<46C80C1A.7050909@burns-stat.com>	<6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>
	<46C8693F.1040609@braverock.com>
Message-ID: <46C8D929.2040305@metrak.com>

Brian,

A few weeks ago this would have been "all greeks to me".  I am still a 
long way off to contemplate making a career change from engineering to 
econometrics.

You also answered my next question, where is "the R function" to 
subsample to monthly data.  Weekly is pretty trivial (holidays aside) 
but monthly requires a few tricks and I didn't want to reinvent the wheel.

cheers

Brian G. Peterson wrote:
> BBands wrote:
>> On 8/19/07, Patrick Burns <patrick at burns-stat.com> wrote:
>>> That paper talks about the effects of asynchrony and proposes a
>>> method of backing out data without asynchrony.  Our investigation
>>> suggested that using weekly data is adequate for avoiding asynchrony
>>> effects.
>> We have done a lot of work on this problem and agree with the choice
>> of weekly data.
>>
>> Paul,
>>
>> The use of benchmarks may not be the optimal path in this application,
>> relatively simple ranking might be more viable. As a compromise, you
>> might try looking at ranked Sharpe ratios...
> 
> A stack ranking of risk/reward ratios is a good idea.  I would recommend 
> using either a Cornish Fisher modified Sharpe ratio (to take possible 
> non-normality of distributions into account) or Sortino's Upside 
> Potential Ratio.  Even Sharpe himself recommends the use of Information 
> Ratio preferentially to the original Sharpe ratio, but old habits die 
> hard...
> 
> To answer an earlier question on sub-sampling:  Yes, from daily *price* 
> data you can construct a weekly or monthly series by simply taking the 
> price at the end of the week or month, and constructing your returns 
> series from the end of period closing price.  The zoo library also has a 
> good implementation of the aggregate() function for timeseries data to 
> help you automate the sampling while maintaining your original daily data.
> 
> Regards,
> 
>     - Brian
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.


From sf at metrak.com  Mon Aug 20 02:22:22 2007
From: sf at metrak.com (paul sorenson)
Date: Mon, 20 Aug 2007 10:22:22 +1000
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <6e8360ad0708191227w7d3c52afs39c53d3ad18c712c@mail.gmail.com>
References: <46BF2A1D.7010604@indiana.edu>
	<46C01BD8.2020007@metrak.com>	<46C3B34E.1050208@indiana.edu>
	<46C6DD89.3060604@metrak.com>	<46C6FC89.1040906@braverock.com>
	<46C77B0B.5000200@metrak.com>	<46C80C1A.7050909@burns-stat.com>	<6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>	<46C8693F.1040609@braverock.com>	<6e8360ad0708191225x72a91bdbw1a67f31cbf2e7b02@mail.gmail.com>
	<6e8360ad0708191227w7d3c52afs39c53d3ad18c712c@mail.gmail.com>
Message-ID: <46C8DEBE.4080804@metrak.com>

John,

The ranking idea sounds quite attractive.  If I understand you right 
though it wouldn't necessarily give me "diversity" metrics whatever they 
might be.  Ie as well as risk/reward of individual funds I would somehow 
want to achieve a mix of funds that did *not* correlate well 
(performance aside).

So I am thinking along the lines of, when faced with 200+ funds:

	- Put them in groups of highly correlated returns.

	- Select from each group based on my preferred performance criteria. 
Maybe at this stage I would focus more on reward than risk.

	- Then put together some kind of portfolio from this much smaller set 
based on holistic metrics with a balance of risk and reward that I am 
comfortable with.

Then presumably repeat parts of the process at intervals yet to be 
determined.

cheers

BBands wrote:
> BBands wrote:
>>> The use of benchmarks may not be the optimal path in this application,
>>> relatively simple ranking might be more viable. As a compromise, you
>>> might try looking at ranked Sharpe ratios...
> 
> On 8/19/07, Brian G. Peterson <brian at braverock.com> wrote:
>> A stack ranking of risk/reward ratios is a good idea.  I would recommend
>> using either a Cornish Fisher modified Sharpe ratio (to take possible
>> non-normality of distributions into account) or Sortino's Upside
>> Potential Ratio.  Even Sharpe himself recommends the use of Information
>> Ratio preferentially to the original Sharpe ratio, but old habits die
>> hard...
> 
> Old habits do die hard... For those interested, Bob Fulks has done a
> lot of interesting work with the Sharpe ratio. A quick search on his
> name might be useful.
> 
>     jab
> --
> John Bollinger, CFA, CMT
> www.BollingerBands.com
> 
> If you advance far enough, you arrive at the beginning.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.


From ravis at ambaresearch.com  Mon Aug 20 10:05:04 2007
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Mon, 20 Aug 2007 13:35:04 +0530
Subject: [R-SIG-Finance] Implied Probability Distribution
Message-ID: <A36876D3F8A5734FA84A4338135E7CC30252EE9B@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070820/d6e19be5/attachment.pl 

From patrick at burns-stat.com  Mon Aug 20 10:33:08 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon, 20 Aug 2007 09:33:08 +0100
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46C8C38F.1020302@metrak.com>
References: <46BF2A1D.7010604@indiana.edu>	<46C01BD8.2020007@metrak.com>	<46C3B34E.1050208@indiana.edu>	<46C6DD89.3060604@metrak.com>	<46C6FC89.1040906@braverock.com>
	<46C77B0B.5000200@metrak.com> <46C80C1A.7050909@burns-stat.com>
	<46C8C38F.1020302@metrak.com>
Message-ID: <46C951C4.3060305@burns-stat.com>

If I'm correct that New York and Sydney do not have
any overlapping hours, then you have hopped back out
of the asynchrony problem.

Pat

paul sorenson wrote:

> Patrick,
>
> That is an interesting paper, in this case I was asking myself the 
> question, "if I know that today's Melbourne prices are correlated to 
> yesterdays  NY prices, is there a possibility of taking advantage of 
> that?"  That is, not so much from putting a portfolio together, but 
> daily investment decisions.
>
> cheers
>
> Patrick Burns wrote:
>
>> paul sorenson wrote:
>>
>>> Probably not unsurprisingly, the correlation between the SP200 and 
>>> the global share fund daily returns goes up (from 0.16 to 0.42) when 
>>> I insert a 1 day lag.
>>>  
>>>
>>
>> Careful.
>>
>> There are asynchrony issues with global data, which definitely affect
>> the correlation (it is too low). Presumably the global return series has
>> asynchrony issues all by itself.
>>
>> The working paper for Burns, Engle and Mezrich (1998) is available
>> at http://www.econ.ucsd.edu/papers/dp97.html
>>
>> That paper talks about the effects of asynchrony and proposes a
>> method of backing out data without asynchrony.  Our investigation
>> suggested that using weekly data is adequate for avoiding asynchrony
>> effects.
>>
>>> http://www.metrak.com/tmp/exch10.png has plots produced by ccf().
>>>
>>> Brian - if I want to look at returns over a different period using 
>>> PerformanceAnalytics when the base data is daily, is the normal 
>>> strategy just to undersample prices before calling CalculateReturns?
>>>
>>> I am guessing that the correlation of the SP200 and global share 
>>> fund would increase when looking at a longer time period.  I also 
>>> want to compare it with some other data which comes out monthly.
>>>
>>> cheers
>>>
>>> BTW sorry if I am boring you guys with what must be very basic stuff 
>>> - just tell me.
>>>  
>>>
>>
>> Asynchrony is neither basic (i.e., well-studied) nor very well 
>> appreciated.
>> I don't vote this message boring.
>>
>> Patrick Burns
>> patrick at burns-stat.com
>> +44 (0)20 8525 0696
>> http://www.burns-stat.com
>> (home of S Poetry and "A Guide for the Unwilling S User")
>>
>>>
>>> Brian G. Peterson wrote:
>>>  
>>>
>>>> paul sorenson wrote:
>>>>  
>>>>
>>>>> I ran a pairs plot on the daily fund returns as well as 
>>>>> calculating the correlation coefficient (Pearson).
>>>>>
>>>>> The pairs plot is reproduced at 
>>>>> http://www.metrak.com/tmp/exch09.png and unless I am missing 
>>>>> something, some of these "look" significant whereas some don't.
>>>>>     
>>>>
>>>> The pairs plot will certainly show you funds that closely track the 
>>>> index.  A quick check of cor() (or the Pearson correlation 
>>>> coefficient) and CAPM.alpha() will do the same.  A pairs plot (and 
>>>> to a lesser extent correlation) won't show you anything about 
>>>> systematic out-performance or under-performance, while alpha is a 
>>>> good indicator if the benchmark you choose is indicative of the 
>>>> investment universe of the fund.  Another good indicator is 
>>>> Sortino's Upside Potential Ratio, especially if you choose the 
>>>> benchmark index standard deviation as your MAR.
>>>>
>>>> CAPM alpha will not be a good indicator if you choose an index that 
>>>> is different from the investment style of the fund.  For example, 
>>>> using a SP200 index with a fixed income fund wouldn't make any sense.
>>>>
>>>> Cheers,
>>>>
>>>>   -Brian
>>>>   
>>>
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only. -- If you want to post, subscribe first.
>>>
>>>
>>>  
>>>
>
>
>


From Xiaochen.Sun at brunel.ac.uk  Tue Aug 21 01:26:16 2007
From: Xiaochen.Sun at brunel.ac.uk (Xiaochen Sun)
Date: Tue, 21 Aug 2007 00:26:16 +0100
Subject: [R-SIG-Finance] question of fitMvdc and fitCopula functions
Message-ID: <E386E504246A9249A9176B5BEEC13B6F0E499E@UXEXMBU116.academic.windsor>

Dear all,
 
I wonder if anyone has experience on copula package regarding the copula parameter estimation.
I was reading "Enjoy the Joy of Copulas" and the manual of the package "copula", 
 
The example code:
fit <- fitMvdc(dat, myMvd, start = start, optim.control = list(trace=TRUE,maxit=200)

the item myMvd reflects the copula functions, but it is already defined by
 
myMvd <- mvdc(copula=ellipCopula(family= "nomal", param=0.5),margins=c("gamma","gamma"), paramMargins=list(list(shape=2, scale=1),list(shape=3,scale=2)))
 
My question is suppose I have a series data X1....Xn;Y1....Yn, however without knowing the margins, and I intend to estimate the parameter of the chosen copula, how can I run this function????
 
Same question to fitCopula function......fitCopula(eu, myMvd at copula <mailto:myMvd at copula> , start = a.0)
 
If you go to "QRMlib" package, you will find similar function 
fit.gausscopula and fit.Archcopula2d, in both case, you can simply code as fit.Archcopula2d(Udata, "clayton")
.....
 
[Ref: Enjoy the Joy of Coulas, Jun Yan, 2006]
 
Appreciate for any reply.
 
Cheers
Michael


From Xiaochen.Sun at brunel.ac.uk  Tue Aug 21 11:14:44 2007
From: Xiaochen.Sun at brunel.ac.uk (Xiaochen Sun)
Date: Tue, 21 Aug 2007 10:14:44 +0100
Subject: [R-SIG-Finance] fit.gausscopula function
Message-ID: <E386E504246A9249A9176B5BEEC13B6F0E499F@UXEXMBU116.academic.windsor>

I do not know why, fit.gausscopula( ) function in QRMlib doesn't work.....
 
If you run the example code in help file, Here is the message I've got:
 
> data(ftse100);
> data(smi);
> TS1 <- cut(ftse100, "1990-11-08", "2004-03-25");
> TS1Augment <- alignDailySeries(TS1, method="before");
> TS2Augment <- alignDailySeries(smi, method="before");
> INDEXES.RAW <- merge(TS1Augment,TS2Augment);
> #Cleanup:
> rm(TS1, TS1Augment, TS2Augment);
> INDEXES <- mk.returns(INDEXES.RAW);
> PARTIALINDEXES <- cut(INDEXES, "1993-12-31", "2003-12-31");
> #Now create a data matrix from the just-created timeSeries 
> data <- seriesData(PARTIALINDEXES);
> #Keep only the data items which are non-zero for both smi and ftse100
> data <- data[data[,1]!=0 & data[,2] !=0,];
> # Construct pseudo copula data. The 2nd parameter is MARGIN=2 
> #when applying to columns and 1 applied to rows. Hence this says to
> #apply the 'edf()' empirical distribtion function() to the columns
> #of the data. 
> Udata <- apply(data,2,edf,adjust=1);
> mod.gauss <- fit.gausscopula(Udata); 
Error in dmnorm(Qdata, rep(0, d), P, logvalue = TRUE) : 
        unused argument(s) (logvalue = TRUE)

However it did work once:
X3X2 <- cbind(x3,x2)
U3U2 <- apply(X3X2,2,edf,adjust=1)
mod.gaussU3U2 <- fit.gausscopula(U3U2)
mod.gaussU3U2

> mod.gaussU3U2[1]
$P
          [,1]      [,2]
[1,] 1.0000000 0.2770152
[2,] 0.2770152 1.0000000
I can not figure out:(((( 
 
Could anyone tell me why?
 
Cheers
Mc


From barth at tac-financial.com  Tue Aug 21 12:39:19 2007
From: barth at tac-financial.com (Sylvain BARTHELEMY)
Date: Tue, 21 Aug 2007 12:39:19 +0200
Subject: [R-SIG-Finance] fit.gausscopula function
In-Reply-To: <E386E504246A9249A9176B5BEEC13B6F0E499F@UXEXMBU116.academic.windsor>
References: <E386E504246A9249A9176B5BEEC13B6F0E499F@UXEXMBU116.academic.windsor>
Message-ID: <006101c7e3df$8840aaa0$98c1ffe0$@com>

Dear Xiaochen,

I think that there are conflicting versions of packages on your machine, as
the R code you provide works well on two of my machines, with R 2.5 and
QRMlib v2.5.1. 

You should update all your R packages by using the "update packages" menu.

Regards.

---
Sylvain Barth?l?my
Research Director, TAC
www.tac-financial.com | www.sylbarth.com

-----Message d'origine-----
De?: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de Xiaochen Sun
Envoy??: mardi 21 ao?t 2007 11:15
??: r-sig-finance at stat.math.ethz.ch
Objet?: [R-SIG-Finance] fit.gausscopula function

I do not know why, fit.gausscopula( ) function in QRMlib doesn't work.....
 
If you run the example code in help file, Here is the message I've got:
 
> data(ftse100);
> data(smi);
> TS1 <- cut(ftse100, "1990-11-08", "2004-03-25");
> TS1Augment <- alignDailySeries(TS1, method="before");
> TS2Augment <- alignDailySeries(smi, method="before");
> INDEXES.RAW <- merge(TS1Augment,TS2Augment);
> #Cleanup:
> rm(TS1, TS1Augment, TS2Augment);
> INDEXES <- mk.returns(INDEXES.RAW);
> PARTIALINDEXES <- cut(INDEXES, "1993-12-31", "2003-12-31");
> #Now create a data matrix from the just-created timeSeries 
> data <- seriesData(PARTIALINDEXES);
> #Keep only the data items which are non-zero for both smi and ftse100
> data <- data[data[,1]!=0 & data[,2] !=0,];
> # Construct pseudo copula data. The 2nd parameter is MARGIN=2 
> #when applying to columns and 1 applied to rows. Hence this says to
> #apply the 'edf()' empirical distribtion function() to the columns
> #of the data. 
> Udata <- apply(data,2,edf,adjust=1);
> mod.gauss <- fit.gausscopula(Udata); 
Error in dmnorm(Qdata, rep(0, d), P, logvalue = TRUE) : 
        unused argument(s) (logvalue = TRUE)

However it did work once:
X3X2 <- cbind(x3,x2)
U3U2 <- apply(X3X2,2,edf,adjust=1)
mod.gaussU3U2 <- fit.gausscopula(U3U2)
mod.gaussU3U2

> mod.gaussU3U2[1]
$P
          [,1]      [,2]
[1,] 1.0000000 0.2770152
[2,] 0.2770152 1.0000000
I can not figure out:(((( 
 
Could anyone tell me why?
 
Cheers
Mc

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From brian at braverock.com  Tue Aug 21 14:37:50 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 21 Aug 2007 07:37:50 -0500
Subject: [R-SIG-Finance] pointers on using VaR.gpd with return series?
Message-ID: <46CADC9E.6040406@braverock.com>

Does anyone have any hints on utilizing VaR.gpd on return series instead 
of price series?

I have tried converting a return series to a wealth index (using 
cumprod), but this still seems to cause problems with the VaR package.

for example:

library(PerformanceAnalytics)
data(edhec)
wi<-cumprod.column(1+edhec)
library(VaR)
vt<-VaR.gpd(wi[,1])

Error in optim(init, gpd.liklhd, hessian = TRUE, method = "Nelder-Mead") :
         non-finite finite-difference value [2]
In addition: There were 50 or more warnings (use warnings() to see the 
first 50)

A message to the maintainer of the package has without response.

If I don't get any usable feedback, I'll probably move to using fit.GPD 
from QRMlib (which I have had some luck with in the past) to add a 
general Pareto method to the VaR functions in PerformanceAnalytics, as I 
feel that parametric VaR functions on a broader set of distributions and 
copulae should be more widely available.

Regards,

    - Brian


From barth at tac-financial.com  Tue Aug 21 16:19:56 2007
From: barth at tac-financial.com (Sylvain BARTHELEMY)
Date: Tue, 21 Aug 2007 16:19:56 +0200
Subject: [R-SIG-Finance] pointers on using VaR.gpd with return series?
In-Reply-To: <46CADC9E.6040406@braverock.com>
References: <46CADC9E.6040406@braverock.com>
Message-ID: <006501c7e3fe$59ffe790$0dffb6b0$@com>

I think that there is a problem with the VaR.gpd function, as it works on
USD/EUR and not on EUR/USD values

library(PerformanceAnalytics)
eurusd <- get.hist.quote("EUR/USD", provider="oanda", start = "2006-01-01")
usdeur <- get.hist.quote("USD/EUR", provider="oanda", start = "2006-01-01")

library(VaR)
v1 <- VaR.gpd(as.vector(eurusd))
v2 <- VaR.gpd(as.vector(usdeur))


output:

> v1 <- VaR.gpd(as.vector(eurusd))
Error in optim(init, gpd.liklhd, hessian = TRUE, method = "Nelder-Mead") :
         non-finite finite-difference value [2]
In addition: There were 50 or more warnings (use warnings() to see the 
first 50)

> v2 <- VaR.gpd(as.vector(usdeur))
There were 11 warnings (use warnings() to see)

---
Sylvain Barth?l?my
Research Director, TAC
www.tac-financial.com | www.sylbarth.com


-----Message d'origine-----
De?: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de Brian G.
Peterson
Envoy??: mardi 21 ao?t 2007 14:38
??: R-SIG-Finance
Objet?: [R-SIG-Finance] pointers on using VaR.gpd with return series?

Does anyone have any hints on utilizing VaR.gpd on return series instead 
of price series?

I have tried converting a return series to a wealth index (using 
cumprod), but this still seems to cause problems with the VaR package.

for example:

library(PerformanceAnalytics)
data(edhec)
wi<-cumprod.column(1+edhec)
library(VaR)
vt<-VaR.gpd(wi[,1])

Error in optim(init, gpd.liklhd, hessian = TRUE, method = "Nelder-Mead") :
         non-finite finite-difference value [2]
In addition: There were 50 or more warnings (use warnings() to see the 
first 50)

A message to the maintainer of the package has without response.

If I don't get any usable feedback, I'll probably move to using fit.GPD 
from QRMlib (which I have had some luck with in the past) to add a 
general Pareto method to the VaR functions in PerformanceAnalytics, as I 
feel that parametric VaR functions on a broader set of distributions and 
copulae should be more widely available.

Regards,

    - Brian

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From bbands at gmail.com  Tue Aug 21 17:08:36 2007
From: bbands at gmail.com (BBands)
Date: Tue, 21 Aug 2007 08:08:36 -0700
Subject: [R-SIG-Finance]  making sense of 100's of funds
In-Reply-To: <6e8360ad0708200906h6a0756fboe8b2dbb54baa2a5a@mail.gmail.com>
References: <46BF2A1D.7010604@indiana.edu> <46C6FC89.1040906@braverock.com>
	<46C77B0B.5000200@metrak.com> <46C80C1A.7050909@burns-stat.com>
	<6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>
	<46C8693F.1040609@braverock.com>
	<6e8360ad0708191225x72a91bdbw1a67f31cbf2e7b02@mail.gmail.com>
	<6e8360ad0708191227w7d3c52afs39c53d3ad18c712c@mail.gmail.com>
	<46C8DEBE.4080804@metrak.com>
	<6e8360ad0708200906h6a0756fboe8b2dbb54baa2a5a@mail.gmail.com>
Message-ID: <6e8360ad0708210808v61aaff56t9276623ced43e16c@mail.gmail.com>

On 8/19/07, paul sorenson <sf at metrak.com> wrote:
> John,
>
> The ranking idea sounds quite attractive.  If I understand you right
> though it wouldn't necessarily give me "diversity" metrics whatever they
> might be.  Ie as well as risk/reward of individual funds I would somehow
> want to achieve a mix of funds that did *not* correlate well
> (performance aside).
>
> So I am thinking along the lines of, when faced with 200+ funds:
>
>         - Put them in groups of highly correlated returns.
>
>         - Select from each group based on my preferred performance criteria.
> Maybe at this stage I would focus more on reward than risk.
>
>         - Then put together some kind of portfolio from this much smaller set
> based on holistic metrics with a balance of risk and reward that I am
> comfortable with.
>
> Then presumably repeat parts of the process at intervals yet to be
> determined.

I feared we'd get here.

The benefits of diversification are a myth, or, more properly, a
nightmare. (They did exist once upon a time, but that was long, long
ago.) In today's markets on the way up diversification averages down
returns, while on the way down diversification offers no benefits as
correlations converge on one.

Having said that, I'll crawl into my bunker and await the incoming.

    jab
--
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From barth at tac-financial.com  Tue Aug 21 18:50:00 2007
From: barth at tac-financial.com (Sylvain BARTHELEMY)
Date: Tue, 21 Aug 2007 18:50:00 +0200
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <6e8360ad0708210808v61aaff56t9276623ced43e16c@mail.gmail.com>
References: <46BF2A1D.7010604@indiana.edu>
	<46C6FC89.1040906@braverock.com>	<46C77B0B.5000200@metrak.com>
	<46C80C1A.7050909@burns-stat.com>	<6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>	<46C8693F.1040609@braverock.com>	<6e8360ad0708191225x72a91bdbw1a67f31cbf2e7b02@mail.gmail.com>	<6e8360ad0708191227w7d3c52afs39c53d3ad18c712c@mail.gmail.com>	<46C8DEBE.4080804@metrak.com>	<6e8360ad0708200906h6a0756fboe8b2dbb54baa2a5a@mail.gmail.com>
	<6e8360ad0708210808v61aaff56t9276623ced43e16c@mail.gmail.com>
Message-ID: <008801c7e413$512a59b0$f37f0d10$@com>

John,


>> The benefits of diversification are a myth, or, more properly, a
>> nightmare. (They did exist once upon a time, but that was long, long
>> ago.) 

No, I don't agree with that. The benefits of diversification are not easy to
quantify and maybe less important than in the past, but I would not say that
it is a myth or even a nightmare. 

It is true that financial markets are more and more integrated and that
contagion is usually observed during financial crises, especially on
emerging markets. But the impact of large events and crises are less
important on well diversified portfolio (geographically, different
instruments, different sectors,...). 

I don't think that diversification disappear, but that the way to construct
a diversified portfolio changes over time, as financial markets change.


>> In today's markets on the way up diversification averages down
>> returns, while on the way down diversification offers no benefits as
>> correlations converge on one.

Yes, maybe the correlation should converge on one as financial markets are
more and more integrated. 

But the fact is that correlation measures usually show very unstable
process, and they can changes very rapidly from uncorrelated one to high
correlated markets, especially during crises (see correlations between
emerging markets during the 97 crisis). Then, all this is very different
from a smooth trend toward less diversification gains and more correlation
between world markets.


>> Having said that, I'll crawl into my bunker and await the incoming.

Don't crawl into your bunker, it is an interesting topic, and not only for
banks and portfolio managers. I would be interested to know more about your
ideas on that.


---
Sylvain Barth?l?my
Research Director, TAC
www.tac-financial.com | www.sylbarth.com


-----Message d'origine-----
De?: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de BBands
Envoy??: mardi 21 ao?t 2007 17:09
??: R-sig-finance
Objet?: [R-SIG-Finance] making sense of 100's of funds

On 8/19/07, paul sorenson <sf at metrak.com> wrote:
> John,
>
> The ranking idea sounds quite attractive.  If I understand you right
> though it wouldn't necessarily give me "diversity" metrics whatever they
> might be.  Ie as well as risk/reward of individual funds I would somehow
> want to achieve a mix of funds that did *not* correlate well
> (performance aside).
>
> So I am thinking along the lines of, when faced with 200+ funds:
>
>         - Put them in groups of highly correlated returns.
>
>         - Select from each group based on my preferred performance
criteria.
> Maybe at this stage I would focus more on reward than risk.
>
>         - Then put together some kind of portfolio from this much smaller
set
> based on holistic metrics with a balance of risk and reward that I am
> comfortable with.
>
> Then presumably repeat parts of the process at intervals yet to be
> determined.

I feared we'd get here.

The benefits of diversification are a myth, or, more properly, a
nightmare. (They did exist once upon a time, but that was long, long
ago.) In today's markets on the way up diversification averages down
returns, while on the way down diversification offers no benefits as
correlations converge on one.

Having said that, I'll crawl into my bunker and await the incoming.

    jab
--
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From dcielen at vub.ac.be  Tue Aug 21 19:58:03 2007
From: dcielen at vub.ac.be (Davy)
Date: Tue, 21 Aug 2007 19:58:03 +0200
Subject: [R-SIG-Finance] making sense of 100's of funds
Message-ID: <1187719083.11923.24.camel@davy-desktop>

John, Sylvain and colleagues,

I find both your remarks very interesting and to contain some truth, I
have included a working paper from Angela, ea (2003). Where they use a
time varying two factor model to test for contagion. They find that in
the Asian Crisis the correlations are indeed increased however no strong
evidence is found for contagion in the Mexican crisis. Now my question
is do you think that the use of time-varying models or stochastic models
(such as a Markov VAR) can reduce risk in a crisis by switching to safer
portfolios and by switching to high risk (and hopefully high yielding)
portfolios in better times?

I'm very interested to hear your opinions,

Davy Cielen
dcielen at vub.ac.be
Student Business Engineering, 
International Master in Management Science,
Solvay Business School

Angela, Bekeart and Campbell, 2003, Market Integration and contagion,
working paper, available from http://www.nber.org/papers/w9510


From bbands at gmail.com  Tue Aug 21 21:46:38 2007
From: bbands at gmail.com (BBands)
Date: Tue, 21 Aug 2007 12:46:38 -0700
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <008801c7e413$512a59b0$f37f0d10$@com>
References: <46BF2A1D.7010604@indiana.edu> <46C80C1A.7050909@burns-stat.com>
	<6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>
	<46C8693F.1040609@braverock.com>
	<6e8360ad0708191225x72a91bdbw1a67f31cbf2e7b02@mail.gmail.com>
	<6e8360ad0708191227w7d3c52afs39c53d3ad18c712c@mail.gmail.com>
	<46C8DEBE.4080804@metrak.com>
	<6e8360ad0708200906h6a0756fboe8b2dbb54baa2a5a@mail.gmail.com>
	<6e8360ad0708210808v61aaff56t9276623ced43e16c@mail.gmail.com>
	<008801c7e413$512a59b0$f37f0d10$@com>
Message-ID: <6e8360ad0708211246k226f6d74r74706bd8dd509bc@mail.gmail.com>

On 8/21/07, Sylvain BARTHELEMY <barth at tac-financial.com> wrote:
> John,
>
>
> >> The benefits of diversification are a myth, or, more properly, a
> >> nightmare. (They did exist once upon a time, but that was long, long
> >> ago.)
>
> No, I don't agree with that. The benefits of diversification are not easy to
> quantify and maybe less important than in the past, but I would not say that
> it is a myth or even a nightmare.
>
> It is true that financial markets are more and more integrated and that
> contagion is usually observed during financial crises, especially on
> emerging markets. But the impact of large events and crises are less
> important on well diversified portfolio (geographically, different
> instruments, different sectors,...).

In practice that doesn't seem to be the case. In this cycle in
particular most things were sold off with a maddening similarity. The
following was sent to me off list:

"""
I fully agree with you....the myth of diversification is great for
academic purposes.  We run a portfolio of hedge funds and try to look at
correlations of our managers in a portfolio...but the pattern you
describe below is exactly what we see...detract from performance in an
up market and as correlations go to 1, don't offer enough protection in
a down market.  The only time it may work is a nontrending market, in
which case it may be better to have a diversified portfolio?
"""

> I don't think that diversification disappear, but that the way to construct
> a diversified portfolio changes over time, as financial markets change.

The problem is that the reversals are so rapid that is may well be
impossible to dynamically adjust.

> >> In today's markets on the way up diversification averages down
> >> returns, while on the way down diversification offers no benefits as
> >> correlations converge on one.
>
> Yes, maybe the correlation should converge on one as financial markets are
> more and more integrated.
>
> But the fact is that correlation measures usually show very unstable
> process, and they can changes very rapidly from uncorrelated one to high
> correlated markets, especially during crises (see correlations between
> emerging markets during the 97 crisis). Then, all this is very different
> from a smooth trend toward less diversification gains and more correlation
> between world markets.

My point is that in today's markets diversification dumbs down a
portfolio during expansions--I'll grant that in some cases that there
may be some pickup in risk-adjusted returns. However, when it comes to
decline such as the one we just saw, diversification offers cold
comfort to the practitioner as correlations converge. Unfortunately it
is exactly in these times that the benefits of diversification are
most counted on and most missed.

> >> Having said that, I'll crawl into my bunker and await the incoming.
>
> Don't crawl into your bunker, it is an interesting topic, and not only for
> banks and portfolio managers. I would be interested to know more about your
> ideas on that.

I've found that when tackling shibboleths, a bunker offers some comfort.

To paraphrase one of Dirk's signatures.

Hell, there are no rules here, we are trying to make money.

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From kriskumar at earthlink.net  Wed Aug 22 03:42:40 2007
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Tue, 21 Aug 2007 21:42:40 -0400
Subject: [R-SIG-Finance] pointers on using VaR.gpd with return series?
In-Reply-To: <006501c7e3fe$59ffe790$0dffb6b0$@com>
References: <46CADC9E.6040406@braverock.com>
	<006501c7e3fe$59ffe790$0dffb6b0$@com>
Message-ID: <46CB9490.5070802@earthlink.net>

Sylvain BARTHELEMY wrote:
> I think that there is a problem with the VaR.gpd function, as it works on
> USD/EUR and not on EUR/USD values
>
> library(PerformanceAnalytics)
> eurusd <- get.hist.quote("EUR/USD", provider="oanda", start = "2006-01-01")
> usdeur <- get.hist.quote("USD/EUR", provider="oanda", start = "2006-01-01")
>
> library(VaR)
> v1 <- VaR.gpd(as.vector(eurusd))
> v2 <- VaR.gpd(as.vector(usdeur))
>
>
> output:
>
>   
>> v1 <- VaR.gpd(as.vector(eurusd))
>>     
> Error in optim(init, gpd.liklhd, hessian = TRUE, method = "Nelder-Mead") :
>          non-finite finite-difference value [2]
> In addition: There were 50 or more warnings (use warnings() to see the 
> first 50)
>
>   
>
Ouch the default parameters there are two possible fixes setting the 
cut-off threshold using p.tr helps.

(a)  doing the following call on your data comes back with some results..

 > v1 <- VaR.gpd(as.vector(eurusd),p.tr=0.95)

(b) The other alternate is to rewrite VaR.gpd and set  hessian=FALSE 
where it makes the call to maximize the log-likelihood.

optim(init, gpd.liklhd, *hessian = TRUE*, method = "Nelder-Mead") :


Neither of these are "the solution"  as this is more an Art than a 
science. Method (a) relates to the question of  how to pick the 
threshold. Very few and
 you have biased fit and too many you are no longer fitting the tail.

In this context I would point you towards the evir library and the 
excellent book by Alex Mcneill on this but doing the following should 
give you some hints..

 >require(evir)
 >shape(danish)

Hope this helps,

Best
Krishna


From brian at braverock.com  Wed Aug 22 03:57:37 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 21 Aug 2007 20:57:37 -0500
Subject: [R-SIG-Finance] pointers on using VaR.gpd with return series?
In-Reply-To: <46CB9490.5070802@earthlink.net>
References: <46CADC9E.6040406@braverock.com>	<006501c7e3fe$59ffe790$0dffb6b0$@com>
	<46CB9490.5070802@earthlink.net>
Message-ID: <46CB9811.8010606@braverock.com>

Krishna Kumar wrote:
> Sylvain BARTHELEMY wrote:
>> I think that there is a problem with the VaR.gpd function, as it works on
>> USD/EUR and not on EUR/USD values
>>
>> library(PerformanceAnalytics)
>> eurusd <- get.hist.quote("EUR/USD", provider="oanda", start = "2006-01-01")
>> usdeur <- get.hist.quote("USD/EUR", provider="oanda", start = "2006-01-01")
>>
>> library(VaR)
>> v1 <- VaR.gpd(as.vector(eurusd))
>> v2 <- VaR.gpd(as.vector(usdeur))
>>
>>
>> output:
>>
>>   
>>> v1 <- VaR.gpd(as.vector(eurusd))
>>>     
>> Error in optim(init, gpd.liklhd, hessian = TRUE, method = "Nelder-Mead") :
>>          non-finite finite-difference value [2]
>> In addition: There were 50 or more warnings (use warnings() to see the 
>> first 50)
>>
>>   
>>
> Ouch the default parameters there are two possible fixes setting the 
> cut-off threshold using p.tr helps.
> 
> (a)  doing the following call on your data comes back with some results..
> 
>  > v1 <- VaR.gpd(as.vector(eurusd),p.tr=0.95)
> 
> (b) The other alternate is to rewrite VaR.gpd and set  hessian=FALSE 
> where it makes the call to maximize the log-likelihood.
> 
> optim(init, gpd.liklhd, *hessian = TRUE*, method = "Nelder-Mead") :
> 
> 
> Neither of these are "the solution"  as this is more an Art than a 
> science. Method (a) relates to the question of  how to pick the 
> threshold. Very few and
>  you have biased fit and too many you are no longer fitting the tail.
> 
> In this context I would point you towards the evir library and the 
> excellent book by Alex Mcneill on this but doing the following should 
> give you some hints..
> 
>  >require(evir)
>  >shape(danish)

Kris and Sylvain,

Thanks for the pointers.  I would have assumed that a VaR function would 
set some reasonable defaults for threshold and p value, but I guess not. 
  Basically, I *know* that a GPD distribution is fitable in a reasonable 
fashion to this data, as I've done it.  I was hoping to cut down on my 
implementation difficulties by using functions already written.

I'll try fitting with evir and QRMlib. (the QRMlib package is the R port 
for functions from McNiel's book, which is well worth owning)

Stay tuned...  as always, we'll share our results.

Regards,

   - Brian


From kriskumar at earthlink.net  Wed Aug 22 04:07:29 2007
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Tue, 21 Aug 2007 22:07:29 -0400
Subject: [R-SIG-Finance] Implied Probability Distribution
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC30252EE9B@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC30252EE9B@BAN-MAILSRV03.Amba.com>
Message-ID: <46CB9A61.9050208@earthlink.net>


> Using non overlapping data of one month call option I have estimated the
> implied volatility using the GBSVolatility of fOptions. Using these
> implied volatilities I have obtained delta using GBSGreeks of the same
> package. 
>
> Using the natural splines I have plotted the implied volatility and
> delta.
>
>  
>
> I have two questions (this might be trivial but would be glad for
> insight):
>
> 1) How do I extrapolate to obtain the tail?
>   
I am afraid there is no easy answer to this either you could flat 
forward extrapolate or use your fitted functional form/spline to 
extrapolate and obtain the vols for those strikes in the wings. 
Stability is the key and using something that is too flexible could 
sometimes hurt in this case.

> 2) How do I back out the option price? (We can use the Black- Scholes
> model (GBSOption of fOptions), however my understanding is it requires
> implied volatility and strike, but we have implied volatility and delta)
>
>
>   
I couldn't find a version of the Breeden & Litzenberger paper but here 
is an easy read on this use eqn 3 on Page-4. 
http://www.bcb.gov.br/ingles/estabilidade/2002_nov/ref200201c62i.pdf

In essence you use a continuum of Call prices and take the 2nd 
derivative with respect to Strike(K) and for this take the centered 
difference
for starters there are some higher order differences that you can mess 
with later.

Also if you have the delta you can always map that to a strike K using 
the formula in http://www.mathfinance.org/formulas_u/Vanilla/node20.html
If you need further help it would help if you provide further 
information and possibly the data that you are using for this list to be 
of further help.

Best
Krishna





>  
>
> Thank you in advance for the help.
>
> Regards,
>
> Ravi
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
>


From kriskumar at earthlink.net  Wed Aug 22 04:21:02 2007
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Tue, 21 Aug 2007 22:21:02 -0400
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <1187719083.11923.24.camel@davy-desktop>
References: <1187719083.11923.24.camel@davy-desktop>
Message-ID: <46CB9D8E.90308@earthlink.net>

Davy wrote:
> John, Sylvain and colleagues,
>
> I find both your remarks very interesting and to contain some truth, I
> have included a working paper from Angela, ea (2003). Where they use a
> time varying two factor model to test for contagion. They find that in
> the Asian Crisis the correlations are indeed increased however no strong
> evidence is found for contagion in the Mexican crisis. Now my question
> is do you think that the use of time-varying models or stochastic models
> (such as a Markov VAR) can reduce risk in a crisis by switching to safer
> portfolios and by switching to high risk (and hopefully high yielding)
> portfolios in better times?
>
> I'm very interested to hear your opinions,
>
>   
I broadly agree with what John and others have said on this but here is 
my two centavos and this is related to the other thread on Copula 
functions. We repeatedly see that the marginal distributions are 
non-normal and there is asymmetry in the returns. e.g. Markets tank 
together but go up in a de-correlated fashion that is not quite captured 
in a correlation estimate. {Also recall Correlation(Pearson's) is a 
linear measure of dependence}

So perhaps this is the sort of thing that is best modeled using Copula 
functions with non-Gaussian marginals. Someone with a little spare time 
could perhaps back-test the performance of risk models that use other 
measure of dependence besides correlation and see how they measure up.


From barth at tac-financial.com  Wed Aug 22 10:34:59 2007
From: barth at tac-financial.com (Sylvain BARTHELEMY)
Date: Wed, 22 Aug 2007 10:34:59 +0200
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <6e8360ad0708211246k226f6d74r74706bd8dd509bc@mail.gmail.com>
References: <46BF2A1D.7010604@indiana.edu>
	<46C80C1A.7050909@burns-stat.com>	<6e8360ad0708190720q767ccbeepdff0d043bab0f35e@mail.gmail.com>	<46C8693F.1040609@braverock.com>	<6e8360ad0708191225x72a91bdbw1a67f31cbf2e7b02@mail.gmail.com>	<6e8360ad0708191227w7d3c52afs39c53d3ad18c712c@mail.gmail.com>	<46C8DEBE.4080804@metrak.com>	<6e8360ad0708200906h6a0756fboe8b2dbb54baa2a5a@mail.gmail.com>	<6e8360ad0708210808v61aaff56t9276623ced43e16c@mail.gmail.com>	<008801c7e413$512a59b0$f37f0d10$@com>
	<6e8360ad0708211246k226f6d74r74706bd8dd509bc@mail.gmail.com>
Message-ID: <006901c7e497$542c1c00$fc845400$@com>


>> I fully agree with you....the myth of diversification is great for
>> academic purposes.  We run a portfolio of hedge funds and try to look at
>> correlations of our managers in a portfolio...but the pattern you
>> describe below is exactly what we see...detract from performance in an
>> up market and as correlations go to 1, don't offer enough protection in
>> a down market.  The only time it may work is a nontrending market, in
>> which case it may be better to have a diversified portfolio?

Yes, I agree with the fact that it does not offer enough protection, but it
offers some, even if portfolio managers expect more. 

You were talking about recent events. So, imagine what would be the impact
of the US sub-prime crisis in Europe if the banks and funds were less
diversified. The liquidity problem could become a solvency problem and a
crash much more rapidly than it does.

Diversification is like an airbag, and the only thing you know is that if
you have SP500 portfolio, in case of crisis, you may lose less money than
unlucky managers but maybe more than lucky ones. The rest is a matter of
price. What is the price/return that you would put on that?

Managers usually expect too much about the benefits of diversification and
would like high returns without any risk... but unfortunately, it is not
possible! Today, the price of diversification is high but it offers more
stable return even if it this protection low in case of crashes.

Finally, the horizon and objectives of practitioners matters. Then, the
diversification benefits are very different from the point of view of the
portfolio manager of an hedge fund, a international bank (doing retail of
investment banking) or the CEO of a multinational company.


>> The problem is that the reversals are so rapid that is may well be
>> impossible to dynamically adjust.

I fully agree with that. 


>> My point is that in today's markets diversification dumbs down a
>> portfolio during expansions--I'll grant that in some cases that there
>> may be some pickup in risk-adjusted returns. However, when it comes to
>> decline such as the one we just saw, diversification offers cold
>> comfort to the practitioner as correlations converge. Unfortunately it
>> is exactly in these times that the benefits of diversification are
>> most counted on and most missed.

Again, I fully agree with that but my answer is not that diversification is
a myth: there is a big difference between "cold comfort" and "no comfort".


>> Hell, there are no rules here, we are trying to make money.

"In theory, there is no difference between theory and practice. But, in
practice, there is"


---
Sylvain Barth?l?my
Research Director, TAC
www.tac-financial.com | www.sylbarth.com


-----Message d'origine-----
De?: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de BBands
Envoy??: mardi 21 ao?t 2007 21:47
??: R-sig-finance
Objet?: Re: [R-SIG-Finance] making sense of 100's of funds

On 8/21/07, Sylvain BARTHELEMY <barth at tac-financial.com> wrote:
> John,
>
>
> >> The benefits of diversification are a myth, or, more properly, a
> >> nightmare. (They did exist once upon a time, but that was long, long
> >> ago.)
>
> No, I don't agree with that. The benefits of diversification are not easy
to
> quantify and maybe less important than in the past, but I would not say
that
> it is a myth or even a nightmare.
>
> It is true that financial markets are more and more integrated and that
> contagion is usually observed during financial crises, especially on
> emerging markets. But the impact of large events and crises are less
> important on well diversified portfolio (geographically, different
> instruments, different sectors,...).

In practice that doesn't seem to be the case. In this cycle in
particular most things were sold off with a maddening similarity. The
following was sent to me off list:

"""
I fully agree with you....the myth of diversification is great for
academic purposes.  We run a portfolio of hedge funds and try to look at
correlations of our managers in a portfolio...but the pattern you
describe below is exactly what we see...detract from performance in an
up market and as correlations go to 1, don't offer enough protection in
a down market.  The only time it may work is a nontrending market, in
which case it may be better to have a diversified portfolio?
"""

> I don't think that diversification disappear, but that the way to
construct
> a diversified portfolio changes over time, as financial markets change.

The problem is that the reversals are so rapid that is may well be
impossible to dynamically adjust.

> >> In today's markets on the way up diversification averages down
> >> returns, while on the way down diversification offers no benefits as
> >> correlations converge on one.
>
> Yes, maybe the correlation should converge on one as financial markets are
> more and more integrated.
>
> But the fact is that correlation measures usually show very unstable
> process, and they can changes very rapidly from uncorrelated one to high
> correlated markets, especially during crises (see correlations between
> emerging markets during the 97 crisis). Then, all this is very different
> from a smooth trend toward less diversification gains and more correlation
> between world markets.

My point is that in today's markets diversification dumbs down a
portfolio during expansions--I'll grant that in some cases that there
may be some pickup in risk-adjusted returns. However, when it comes to
decline such as the one we just saw, diversification offers cold
comfort to the practitioner as correlations converge. Unfortunately it
is exactly in these times that the benefits of diversification are
most counted on and most missed.

> >> Having said that, I'll crawl into my bunker and await the incoming.
>
> Don't crawl into your bunker, it is an interesting topic, and not only for
> banks and portfolio managers. I would be interested to know more about
your
> ideas on that.

I've found that when tackling shibboleths, a bunker offers some comfort.

To paraphrase one of Dirk's signatures.

Hell, there are no rules here, we are trying to make money.

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From barth at tac-financial.com  Wed Aug 22 12:08:29 2007
From: barth at tac-financial.com (Sylvain BARTHELEMY)
Date: Wed, 22 Aug 2007 12:08:29 +0200
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <1187719083.11923.24.camel@davy-desktop>
References: <1187719083.11923.24.camel@davy-desktop>
Message-ID: <007001c7e4a4$64580cd0$2d082670$@com>

Dear Davy,

Thank you the reference article.


>> evidence is found for contagion in the Mexican crisis. Now my question
>> is do you think that the use of time-varying models or stochastic models
>> (such as a Markov VAR) can reduce risk in a crisis by switching to safer
>> portfolios and by switching to high risk (and hopefully high yielding)
>> portfolios in better times?

I have many doubts that it would be helpful, especially on emerging markets,
where the quality and availability of data is low and markets are not
liquid.

But I know that there are a lot of research on that and some of my
collegues/practitioners are trying to use this kind of time varying models
and/or extended Kalman filters to do that. I think that if these models are
very helpful to understand a stochastic process and regimes ex-post, they
are very difficult to use to elaborate scenarios ex-ante, especially in case
of switching regime (during crises and large events).



---
Sylvain Barth?l?my
Research Director, TAC
www.tac-financial.com | www.sylbarth.com


-----Message d'origine-----
De?: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de Davy
Envoy??: mardi 21 ao?t 2007 19:58
??: 'R-sig-finance'
Objet?: Re: [R-SIG-Finance] making sense of 100's of funds

John, Sylvain and colleagues,

I find both your remarks very interesting and to contain some truth, I
have included a working paper from Angela, ea (2003). Where they use a
time varying two factor model to test for contagion. They find that in
the Asian Crisis the correlations are indeed increased however no strong
evidence is found for contagion in the Mexican crisis. Now my question
is do you think that the use of time-varying models or stochastic models
(such as a Markov VAR) can reduce risk in a crisis by switching to safer
portfolios and by switching to high risk (and hopefully high yielding)
portfolios in better times?

I'm very interested to hear your opinions,

Davy Cielen
dcielen at vub.ac.be
Student Business Engineering, 
International Master in Management Science,
Solvay Business School

Angela, Bekeart and Campbell, 2003, Market Integration and contagion,
working paper, available from http://www.nber.org/papers/w9510

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From a.trapletti at swissonline.ch  Wed Aug 22 15:19:45 2007
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Wed, 22 Aug 2007 15:19:45 +0200
Subject: [R-SIG-Finance] making sense of 100's of funds
Message-ID: <46CC37F1.6080500@swissonline.ch>

My two cents come in the form of an R-script:

# Germany vs USA
dax <- get.hist.quote(instrument = "^gdaxi", start = "1980-01-01", quote 
= "Close")
sp <- get.hist.quote(instrument = "^gspc", start = "1980-01-01", quote = 
"Close")

# Synchronize data
x <- merge(dax, sp, all=F)

# Avoid synchronization problems due to trading in different time zones
dax <- embed(as.vector(diff(log(x[,1]))), 5)
dax <- apply(dax, 1, sum)

sp <- embed(as.vector(diff(log(x[,2]))), 5)
sp <- apply(sp, 1, sum)

# Compute running correlation with a quaterly window
cov <- apply(embed(sp*dax, 60), 1, mean)
s1 <- sqrt(apply(embed(dax^2, 60), 1, mean))
s2 <- sqrt(apply(embed(sp^2, 60), 1, mean))
cor <- cov/(s1*s2)
idx <- 1:length(cor)

# Plot running correlation over time and loess average
# Correlation increases over time which is not surprising
# with today's levels of globalization
plot(idx, cor, type="l")
lines(idx, predict(loess(cor~idx)), col="green")

# Plot running correlation vs running volatility and loess average
# Correlation is higher during volatile periods, i.e., during market 
corrections
plot(s1, cor)
lines(s1, predict(md <- loess(cor~s1)), col="green")

# Plot running correlation vs running volatility and loess average
# Correlation is higher during volatile periods, i.e., during market 
corrections
plot(s2, cor)
lines(s2, predict(loess(cor~s2)), col="green")

# Correlation increase over time is somewhat down-biased in the past few 
years
# as the past few years exhibited exceptionally low volatility
# Plot running correlation over time with a "bias-corrected" running 
correlation
# correcting for different levels of volatility
ccor <- pmin(1, cor+(cor-predict(md, s1)))

plot(idx, cor, type="l", ylim=c(-1, 1))
lines(idx, ccor, col="blue")
lines(idx, predict(loess(ccor~idx)), col="green")

 From my point of view, the only way to find true diversification is 
investing in different sectors such as bonds or real estate or investing 
in different strategies, e.g., hedge funds being long volatility (but 
also in these examples correlations increased in the past few years, and 
it is difficult to find true diversification).

Best
Adrian

>Message: 12
>Date: Wed, 22 Aug 2007 10:34:59 +0200
>From: "Sylvain BARTHELEMY" <barth at tac-financial.com>
>Subject: Re: [R-SIG-Finance] making sense of 100's of funds
>To: "'R-sig-finance'" <r-sig-finance at stat.math.ethz.ch>
>Message-ID: <006901c7e497$542c1c00$fc845400$@com>
>Content-Type: text/plain;	charset="iso-8859-1"
>
>
>  
>
>>>>> I fully agree with you....the myth of diversification is great for
>>>>> academic purposes.  We run a portfolio of hedge funds and try to look at
>>>>> correlations of our managers in a portfolio...but the pattern you
>>>>> describe below is exactly what we see...detract from performance in an
>>>>> up market and as correlations go to 1, don't offer enough protection in
>>>>> a down market.  The only time it may work is a nontrending market, in
>>>>> which case it may be better to have a diversified portfolio?
>>>      
>>>
>
>Yes, I agree with the fact that it does not offer enough protection, but it
>offers some, even if portfolio managers expect more. 
>
>You were talking about recent events. So, imagine what would be the impact
>of the US sub-prime crisis in Europe if the banks and funds were less
>diversified. The liquidity problem could become a solvency problem and a
>crash much more rapidly than it does.
>
>Diversification is like an airbag, and the only thing you know is that if
>you have SP500 portfolio, in case of crisis, you may lose less money than
>unlucky managers but maybe more than lucky ones. The rest is a matter of
>price. What is the price/return that you would put on that?
>
>Managers usually expect too much about the benefits of diversification and
>would like high returns without any risk... but unfortunately, it is not
>possible! Today, the price of diversification is high but it offers more
>stable return even if it this protection low in case of crashes.
>
>Finally, the horizon and objectives of practitioners matters. Then, the
>diversification benefits are very different from the point of view of the
>portfolio manager of an hedge fund, a international bank (doing retail of
>investment banking) or the CEO of a multinational company.
>
>
>  
>
>>>>> The problem is that the reversals are so rapid that is may well be
>>>>> impossible to dynamically adjust.
>>>      
>>>
>
>I fully agree with that. 
>
>
>  
>
>>>>> My point is that in today's markets diversification dumbs down a
>>>>> portfolio during expansions--I'll grant that in some cases that there
>>>>> may be some pickup in risk-adjusted returns. However, when it comes to
>>>>> decline such as the one we just saw, diversification offers cold
>>>>> comfort to the practitioner as correlations converge. Unfortunately it
>>>>> is exactly in these times that the benefits of diversification are
>>>>> most counted on and most missed.
>>>      
>>>
>
>Again, I fully agree with that but my answer is not that diversification is
>a myth: there is a big difference between "cold comfort" and "no comfort".
>
>
>  
>
>>>>> Hell, there are no rules here, we are trying to make money.
>>>      
>>>
>
>"In theory, there is no difference between theory and practice. But, in
>practice, there is"
>
>
>---
>Sylvain Barth?l?my
>Research Director, TAC
>www.tac-financial.com | www.sylbarth.com
>
>
>-----Message d'origine-----
>De?: r-sig-finance-bounces at stat.math.ethz.ch
>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de BBands
>Envoy??: mardi 21 ao?t 2007 21:47
>??: R-sig-finance
>Objet?: Re: [R-SIG-Finance] making sense of 100's of funds
>
>On 8/21/07, Sylvain BARTHELEMY <barth at tac-financial.com> wrote:
>  
>
>>> John,
>>>
>>>
>>    
>>
>>>>> >> The benefits of diversification are a myth, or, more properly, a
>>>>> >> nightmare. (They did exist once upon a time, but that was long, long
>>>>> >> ago.)
>>>>        
>>>>
>>>
>>> No, I don't agree with that. The benefits of diversification are not easy
>>    
>>
>to
>  
>
>>> quantify and maybe less important than in the past, but I would not say
>>    
>>
>that
>  
>
>>> it is a myth or even a nightmare.
>>>
>>> It is true that financial markets are more and more integrated and that
>>> contagion is usually observed during financial crises, especially on
>>> emerging markets. But the impact of large events and crises are less
>>> important on well diversified portfolio (geographically, different
>>> instruments, different sectors,...).
>>    
>>
>
>In practice that doesn't seem to be the case. In this cycle in
>particular most things were sold off with a maddening similarity. The
>following was sent to me off list:
>
>"""
>I fully agree with you....the myth of diversification is great for
>academic purposes.  We run a portfolio of hedge funds and try to look at
>correlations of our managers in a portfolio...but the pattern you
>describe below is exactly what we see...detract from performance in an
>up market and as correlations go to 1, don't offer enough protection in
>a down market.  The only time it may work is a nontrending market, in
>which case it may be better to have a diversified portfolio?
>"""
>
>  
>
>>> I don't think that diversification disappear, but that the way to
>>    
>>
>construct
>  
>
>>> a diversified portfolio changes over time, as financial markets change.
>>    
>>
>
>The problem is that the reversals are so rapid that is may well be
>impossible to dynamically adjust.
>
>  
>
>>>>> >> In today's markets on the way up diversification averages down
>>>>> >> returns, while on the way down diversification offers no benefits as
>>>>> >> correlations converge on one.
>>>>        
>>>>
>>>
>>> Yes, maybe the correlation should converge on one as financial markets are
>>> more and more integrated.
>>>
>>> But the fact is that correlation measures usually show very unstable
>>> process, and they can changes very rapidly from uncorrelated one to high
>>> correlated markets, especially during crises (see correlations between
>>> emerging markets during the 97 crisis). Then, all this is very different
>>> from a smooth trend toward less diversification gains and more correlation
>>> between world markets.
>>    
>>
>
>My point is that in today's markets diversification dumbs down a
>portfolio during expansions--I'll grant that in some cases that there
>may be some pickup in risk-adjusted returns. However, when it comes to
>decline such as the one we just saw, diversification offers cold
>comfort to the practitioner as correlations converge. Unfortunately it
>is exactly in these times that the benefits of diversification are
>most counted on and most missed.
>
>  
>
>>>>> >> Having said that, I'll crawl into my bunker and await the incoming.
>>>>        
>>>>
>>>
>>> Don't crawl into your bunker, it is an interesting topic, and not only for
>>> banks and portfolio managers. I would be interested to know more about
>>    
>>
>your
>  
>
>>> ideas on that.
>>    
>>
>
>I've found that when tackling shibboleths, a bunker offers some comfort.
>
>To paraphrase one of Dirk's signatures.
>
>Hell, there are no rules here, we are trying to make money.
>
>    jab
> -- John Bollinger, CFA, CMT www.BollingerBands.com If you advance far 
> enough, you arrive at the beginning. 
> _______________________________________________ 
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance -- 
> Subscriber-posting only. -- If you want to post, subscribe first.
>

-- 
Adrian Trapletti
Wildsbergstrasse 31
8610 Uster
Switzerland

Phone :   +41 (0) 44 9945630
Mobile :  +41 (0) 76 3705631

Email :   a.trapletti at swissonline.ch


From ggrothendieck at gmail.com  Wed Aug 22 17:28:58 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 22 Aug 2007 11:28:58 -0400
Subject: [R-SIG-Finance] making sense of 100's of funds
In-Reply-To: <46CC37F1.6080500@swissonline.ch>
References: <46CC37F1.6080500@swissonline.ch>
Message-ID: <971536df0708220828p4d01f15dwc7f8d2224e39bdae@mail.gmail.com>

Just wanted to point out that some of these calculations could be streamlined
using facilities in zoo and dyn.   In particular we use rollmean instead of
embed and apply and we can directly plot the zoo series using plot.zoo.
Also using
  p <- dyn$loess(...)
the result becomes a dyn object so that predict(p) retains its zoo class.
(dyn does not know anything about the modelling function and simply
assumes that the it, loess in this case, works like lm and this is sufficiently
true for this example.)

There may be some small difference between what we have below and the
original since I did this quickly and did not take care that it be absolutely
identical.

library(tseries)
library(dyn)

dax <- get.hist.quote("^gdaxi", start = "1980-01-01", quote = "Close")
sp  <- get.hist.quote("^gspc",  start = "1980-01-01", quote = "Close")

x <- merge(dax, sp, all = FALSE)

x5 <- rollmean(diff(log(x)), 5)

s1 <- sqrt(rollmean(x5[,1] ^2, 60))
s2 <- sqrt(rollmean(x5[,2] ^2, 60))
Cov <- rollmean(x5[,1] * x5[,2], 60)
Cor <- Cov / (s1 * s2)
tt <- unclass(time(Cov))

opar <- par(mfrow = c(2,2))

plot(Cor)
tt <- as.numeric(time(Cov))
lines(p <- predict(dyn$loess(Cor ~ tt)), col = "green")

plot(s1, Cor)
lines(s1, predict(md <- dyn$loess(Cor ~ s1)), col = "green")

plot(s2, Cor)
lines(s2, predict(dyn$loess(Cor ~ s2)), col = "green")

ccor <- pmin(Cor+(Cor-predict(md)), 1)
plot(merge(Cor, ccor, p), col = c(1, 3, 4), plot.type = "single")

par(opar)


On 8/22/07, Adrian Trapletti <a.trapletti at swissonline.ch> wrote:
> My two cents come in the form of an R-script:
>
> # Germany vs USA
> dax <- get.hist.quote(instrument = "^gdaxi", start = "1980-01-01", quote
> = "Close")
> sp <- get.hist.quote(instrument = "^gspc", start = "1980-01-01", quote =
> "Close")
>
> # Synchronize data
> x <- merge(dax, sp, all=F)
>
> # Avoid synchronization problems due to trading in different time zones
> dax <- embed(as.vector(diff(log(x[,1]))), 5)
> dax <- apply(dax, 1, sum)
>
> sp <- embed(as.vector(diff(log(x[,2]))), 5)
> sp <- apply(sp, 1, sum)
>
> # Compute running correlation with a quaterly window
> cov <- apply(embed(sp*dax, 60), 1, mean)
> s1 <- sqrt(apply(embed(dax^2, 60), 1, mean))
> s2 <- sqrt(apply(embed(sp^2, 60), 1, mean))
> cor <- cov/(s1*s2)
> idx <- 1:length(cor)
>
> # Plot running correlation over time and loess average
> # Correlation increases over time which is not surprising
> # with today's levels of globalization
> plot(idx, cor, type="l")
> lines(idx, predict(loess(cor~idx)), col="green")
>
> # Plot running correlation vs running volatility and loess average
> # Correlation is higher during volatile periods, i.e., during market
> corrections
> plot(s1, cor)
> lines(s1, predict(md <- loess(cor~s1)), col="green")
>
> # Plot running correlation vs running volatility and loess average
> # Correlation is higher during volatile periods, i.e., during market
> corrections
> plot(s2, cor)
> lines(s2, predict(loess(cor~s2)), col="green")
>
> # Correlation increase over time is somewhat down-biased in the past few
> years
> # as the past few years exhibited exceptionally low volatility
> # Plot running correlation over time with a "bias-corrected" running
> correlation
> # correcting for different levels of volatility
> ccor <- pmin(1, cor+(cor-predict(md, s1)))
>
> plot(idx, cor, type="l", ylim=c(-1, 1))
> lines(idx, ccor, col="blue")
> lines(idx, predict(loess(ccor~idx)), col="green")
>
>  From my point of view, the only way to find true diversification is
> investing in different sectors such as bonds or real estate or investing
> in different strategies, e.g., hedge funds being long volatility (but
> also in these examples correlations increased in the past few years, and
> it is difficult to find true diversification).
>
> Best
> Adrian
>
> >Message: 12
> >Date: Wed, 22 Aug 2007 10:34:59 +0200
> >From: "Sylvain BARTHELEMY" <barth at tac-financial.com>
> >Subject: Re: [R-SIG-Finance] making sense of 100's of funds
> >To: "'R-sig-finance'" <r-sig-finance at stat.math.ethz.ch>
> >Message-ID: <006901c7e497$542c1c00$fc845400$@com>
> >Content-Type: text/plain;      charset="iso-8859-1"
> >
> >
> >
> >
> >>>>> I fully agree with you....the myth of diversification is great for
> >>>>> academic purposes.  We run a portfolio of hedge funds and try to look at
> >>>>> correlations of our managers in a portfolio...but the pattern you
> >>>>> describe below is exactly what we see...detract from performance in an
> >>>>> up market and as correlations go to 1, don't offer enough protection in
> >>>>> a down market.  The only time it may work is a nontrending market, in
> >>>>> which case it may be better to have a diversified portfolio?
> >>>
> >>>
> >
> >Yes, I agree with the fact that it does not offer enough protection, but it
> >offers some, even if portfolio managers expect more.
> >
> >You were talking about recent events. So, imagine what would be the impact
> >of the US sub-prime crisis in Europe if the banks and funds were less
> >diversified. The liquidity problem could become a solvency problem and a
> >crash much more rapidly than it does.
> >
> >Diversification is like an airbag, and the only thing you know is that if
> >you have SP500 portfolio, in case of crisis, you may lose less money than
> >unlucky managers but maybe more than lucky ones. The rest is a matter of
> >price. What is the price/return that you would put on that?
> >
> >Managers usually expect too much about the benefits of diversification and
> >would like high returns without any risk... but unfortunately, it is not
> >possible! Today, the price of diversification is high but it offers more
> >stable return even if it this protection low in case of crashes.
> >
> >Finally, the horizon and objectives of practitioners matters. Then, the
> >diversification benefits are very different from the point of view of the
> >portfolio manager of an hedge fund, a international bank (doing retail of
> >investment banking) or the CEO of a multinational company.
> >
> >
> >
> >
> >>>>> The problem is that the reversals are so rapid that is may well be
> >>>>> impossible to dynamically adjust.
> >>>
> >>>
> >
> >I fully agree with that.
> >
> >
> >
> >
> >>>>> My point is that in today's markets diversification dumbs down a
> >>>>> portfolio during expansions--I'll grant that in some cases that there
> >>>>> may be some pickup in risk-adjusted returns. However, when it comes to
> >>>>> decline such as the one we just saw, diversification offers cold
> >>>>> comfort to the practitioner as correlations converge. Unfortunately it
> >>>>> is exactly in these times that the benefits of diversification are
> >>>>> most counted on and most missed.
> >>>
> >>>
> >
> >Again, I fully agree with that but my answer is not that diversification is
> >a myth: there is a big difference between "cold comfort" and "no comfort".
> >
> >
> >
> >
> >>>>> Hell, there are no rules here, we are trying to make money.
> >>>
> >>>
> >
> >"In theory, there is no difference between theory and practice. But, in
> >practice, there is"
> >
> >
> >---
> >Sylvain Barth?l?my
> >Research Director, TAC
> >www.tac-financial.com | www.sylbarth.com
> >
> >
> >-----Message d'origine-----
> >De?: r-sig-finance-bounces at stat.math.ethz.ch
> >[mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de BBands
> >Envoy??: mardi 21 ao?t 2007 21:47
> >??: R-sig-finance
> >Objet?: Re: [R-SIG-Finance] making sense of 100's of funds
> >
> >On 8/21/07, Sylvain BARTHELEMY <barth at tac-financial.com> wrote:
> >
> >
> >>> John,
> >>>
> >>>
> >>
> >>
> >>>>> >> The benefits of diversification are a myth, or, more properly, a
> >>>>> >> nightmare. (They did exist once upon a time, but that was long, long
> >>>>> >> ago.)
> >>>>
> >>>>
> >>>
> >>> No, I don't agree with that. The benefits of diversification are not easy
> >>
> >>
> >to
> >
> >
> >>> quantify and maybe less important than in the past, but I would not say
> >>
> >>
> >that
> >
> >
> >>> it is a myth or even a nightmare.
> >>>
> >>> It is true that financial markets are more and more integrated and that
> >>> contagion is usually observed during financial crises, especially on
> >>> emerging markets. But the impact of large events and crises are less
> >>> important on well diversified portfolio (geographically, different
> >>> instruments, different sectors,...).
> >>
> >>
> >
> >In practice that doesn't seem to be the case. In this cycle in
> >particular most things were sold off with a maddening similarity. The
> >following was sent to me off list:
> >
> >"""
> >I fully agree with you....the myth of diversification is great for
> >academic purposes.  We run a portfolio of hedge funds and try to look at
> >correlations of our managers in a portfolio...but the pattern you
> >describe below is exactly what we see...detract from performance in an
> >up market and as correlations go to 1, don't offer enough protection in
> >a down market.  The only time it may work is a nontrending market, in
> >which case it may be better to have a diversified portfolio?
> >"""
> >
> >
> >
> >>> I don't think that diversification disappear, but that the way to
> >>
> >>
> >construct
> >
> >
> >>> a diversified portfolio changes over time, as financial markets change.
> >>
> >>
> >
> >The problem is that the reversals are so rapid that is may well be
> >impossible to dynamically adjust.
> >
> >
> >
> >>>>> >> In today's markets on the way up diversification averages down
> >>>>> >> returns, while on the way down diversification offers no benefits as
> >>>>> >> correlations converge on one.
> >>>>
> >>>>
> >>>
> >>> Yes, maybe the correlation should converge on one as financial markets are
> >>> more and more integrated.
> >>>
> >>> But the fact is that correlation measures usually show very unstable
> >>> process, and they can changes very rapidly from uncorrelated one to high
> >>> correlated markets, especially during crises (see correlations between
> >>> emerging markets during the 97 crisis). Then, all this is very different
> >>> from a smooth trend toward less diversification gains and more correlation
> >>> between world markets.
> >>
> >>
> >
> >My point is that in today's markets diversification dumbs down a
> >portfolio during expansions--I'll grant that in some cases that there
> >may be some pickup in risk-adjusted returns. However, when it comes to
> >decline such as the one we just saw, diversification offers cold
> >comfort to the practitioner as correlations converge. Unfortunately it
> >is exactly in these times that the benefits of diversification are
> >most counted on and most missed.
> >
> >
> >
> >>>>> >> Having said that, I'll crawl into my bunker and await the incoming.
> >>>>
> >>>>
> >>>
> >>> Don't crawl into your bunker, it is an interesting topic, and not only for
> >>> banks and portfolio managers. I would be interested to know more about
> >>
> >>
> >your
> >
> >
> >>> ideas on that.
> >>
> >>
> >
> >I've found that when tackling shibboleths, a bunker offers some comfort.
> >
> >To paraphrase one of Dirk's signatures.
> >
> >Hell, there are no rules here, we are trying to make money.
> >
> >    jab
> > -- John Bollinger, CFA, CMT www.BollingerBands.com If you advance far
> > enough, you arrive at the beginning.
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance --
> > Subscriber-posting only. -- If you want to post, subscribe first.
> >
>
> --
> Adrian Trapletti
> Wildsbergstrasse 31
> 8610 Uster
> Switzerland
>
> Phone :   +41 (0) 44 9945630
> Mobile :  +41 (0) 76 3705631
>
> Email :   a.trapletti at swissonline.ch
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From marcl at svquant.com  Thu Aug 23 02:16:52 2007
From: marcl at svquant.com (Marc E Levitt)
Date: Wed, 22 Aug 2007 17:16:52 -0700 (PDT)
Subject: [R-SIG-Finance] Diversification Comments...
Message-ID: <358604.70838.qm@web32209.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070822/88dc5be7/attachment.pl 

From VOSSK at kochind.com  Thu Aug 23 05:48:26 2007
From: VOSSK at kochind.com (Voss, Kent)
Date: Wed, 22 Aug 2007 22:48:26 -0500
Subject: [R-SIG-Finance] rollapply and cummin
Message-ID: <030B041DE2D0A34F8C7283D96877A44F01CA25B1@hou0mbx01.kochind.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070822/01c73602/attachment.pl 

From josh.m.ulrich at gmail.com  Thu Aug 23 06:26:31 2007
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Wed, 22 Aug 2007 23:26:31 -0500
Subject: [R-SIG-Finance] rollapply and cummin
In-Reply-To: <030B041DE2D0A34F8C7283D96877A44F01CA25B1@hou0mbx01.kochind.com>
References: <030B041DE2D0A34F8C7283D96877A44F01CA25B1@hou0mbx01.kochind.com>
Message-ID: <8cca69990708222126g5e23daf0n9131435905c02a6e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070822/c8c92b36/attachment.pl 

From ggrothendieck at gmail.com  Thu Aug 23 06:46:31 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Aug 2007 00:46:31 -0400
Subject: [R-SIG-Finance] rollapply and cummin
In-Reply-To: <030B041DE2D0A34F8C7283D96877A44F01CA25B1@hou0mbx01.kochind.com>
References: <030B041DE2D0A34F8C7283D96877A44F01CA25B1@hou0mbx01.kochind.com>
Message-ID: <971536df0708222146v2b23424r836d5477d982543d@mail.gmail.com>

The usual case is that the function used in rollapply returns a single
number; however, if you really want to return a vector of 5 numbers try
the latest version of rollapply which was fixed about 2 weeks ago and
is not yet on CRAN but can be downloaded from the development
repository.  From within R:

library(zoo)
source("http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/*checkout*/pkg/R/rollapply.R?rev=363&root=zoo")
# ... now run your commands ...



On 8/22/07, Voss, Kent <VOSSK at kochind.com> wrote:
> I am getting some unexpected results using rollapply and the cummin function that I am hoping someone can help me understand.  I'm just trying to get a rolling cumulative minimum of a time series.  Think of it as the maximum loss over a rolling window.
>
> Here's an example
> # Create a dummy sequence to use with a set of dates used to create a zoo object
> x <- c(0,rep(c(seq(1,5,by=1),seq(-1,-3, by=-1)),2))
> DateList <- as.Date(seq(ISOdate(1990,1,1), length.out=length(x), by="1 day"), '%Y-%m-%d')
>
> z <- zoo(x,DateList)
>
> # Doing the following gets expected results, a rolling 5 day sum.  So far so good
> zl <- rollapply(z, 5, align='left',sum)
>
> # The following however creates an n x n matrix where n = the length of z, that I can't quite figure out
> zl <- rollapply(z, 5, align='left',cummin)
>
>
> Now I'm pretty new at this stuff, so I'm sure there's something I'm missing, but I can't make heads or tails around the results of the cummin.  Any help would be greatly appreciated.  Thanks in advance.
>
> Kent
>
>
>
>
>
> "EMF <kochind.com>" made the following annotations.
> ------------------------------------------------------------------------------
> The information in this e-mail and any attachments is confidential and intended solely for the attention and use of the named addressee(s). It must not be disclosed to any person without proper authority. If you are not the intended recipient, or a person responsible for delivering it to the intended recipient, you are not authorized to and must not disclose, copy, distribute, or retain this message or any part of it.
>
> ==============================================================================
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From Achim.Zeileis at wu-wien.ac.at  Thu Aug 23 10:50:13 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 23 Aug 2007 10:50:13 +0200 (CEST)
Subject: [R-SIG-Finance] rollapply and cummin
In-Reply-To: <971536df0708222146v2b23424r836d5477d982543d@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0708231047110.6792-100000@disco.wu-wien.ac.at>

On Thu, 23 Aug 2007, Gabor Grothendieck wrote:

> The usual case is that the function used in rollapply returns a single
> number; however, if you really want to return a vector of 5 numbers try
> the latest version of rollapply which was fixed about 2 weeks ago and
> is not yet on CRAN but can be downloaded from the development
> repository.  From within R:
>
> library(zoo)
> source("http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/*checkout*/pkg/R/rollapply.R?rev=363&root=zoo")
> # ... now run your commands ...

Just a comment on using code from the R-Forge repository. Usually
  install.packages("zoo", repos = "http://R-Forge.R-project.org/")
would be the easiest way to obtain the latest development version. But
currently the "zoo" package is not being built, I'll try to fix that soon.
(I guess it is due to missing style files for the vignette but have to
check.)

Best wishes,
Z

>
>
> On 8/22/07, Voss, Kent <VOSSK at kochind.com> wrote:
> > I am getting some unexpected results using rollapply and the cummin function that I am hoping someone can help me understand.  I'm just trying to get a rolling cumulative minimum of a time series.  Think of it as the maximum loss over a rolling window.
> >
> > Here's an example
> > # Create a dummy sequence to use with a set of dates used to create a zoo object
> > x <- c(0,rep(c(seq(1,5,by=1),seq(-1,-3, by=-1)),2))
> > DateList <- as.Date(seq(ISOdate(1990,1,1), length.out=length(x), by="1 day"), '%Y-%m-%d')
> >
> > z <- zoo(x,DateList)
> >
> > # Doing the following gets expected results, a rolling 5 day sum.  So far so good
> > zl <- rollapply(z, 5, align='left',sum)
> >
> > # The following however creates an n x n matrix where n = the length of z, that I can't quite figure out
> > zl <- rollapply(z, 5, align='left',cummin)
> >
> >
> > Now I'm pretty new at this stuff, so I'm sure there's something I'm missing, but I can't make heads or tails around the results of the cummin.  Any help would be greatly appreciated.  Thanks in advance.
> >
> > Kent
> >
> >
> >
> >
> >
> > "EMF <kochind.com>" made the following annotations.
> > ------------------------------------------------------------------------------
> > The information in this e-mail and any attachments is confidential and intended solely for the attention and use of the named addressee(s). It must not be disclosed to any person without proper authority. If you are not the intended recipient, or a person responsible for delivering it to the intended recipient, you are not authorized to and must not disclose, copy, distribute, or retain this message or any part of it.
> >
> > ==============================================================================
> >
> >        [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From alexander.wurzer at fin4cast.com  Thu Aug 23 13:47:37 2007
From: alexander.wurzer at fin4cast.com (Alexander Wurzer)
Date: Thu, 23 Aug 2007 13:47:37 +0200
Subject: [R-SIG-Finance] Bloomberg: Polygon with color transition
Message-ID: <001701c7e57b$6734cc40$ad04d30a@backoffice>

Hi,

Probably this is not the best mailing-list for my problem, but I don't know
where I should ask instead...
Has anybody of you an idea how I can create a polygon with color transition
like on Bloomberg?
Attached you find an example what I mean.

Kind regards,
Alex


-------------- next part --------------
A non-text attachment was scrubbed...
Name: sg425236.gif
Type: application/octet-stream
Size: 25829 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070823/5ceaf279/attachment.obj 

From finbref.2006 at gmail.com  Thu Aug 23 14:07:40 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Thu, 23 Aug 2007 14:07:40 +0200
Subject: [R-SIG-Finance] Bloomberg: Polygon with color transition
In-Reply-To: <001701c7e57b$6734cc40$ad04d30a@backoffice>
References: <001701c7e57b$6734cc40$ad04d30a@backoffice>
Message-ID: <d0f55a670708230507t43f2139x21641eaea956e78@mail.gmail.com>

you can make this easily with SVG and a gradient.
Thomas


From VOSSK at kochind.com  Thu Aug 23 14:29:34 2007
From: VOSSK at kochind.com (Voss, Kent)
Date: Thu, 23 Aug 2007 07:29:34 -0500
Subject: [R-SIG-Finance] rollapply and cummin
In-Reply-To: <971536df0708222146v2b23424r836d5477d982543d@mail.gmail.com>
References: <030B041DE2D0A34F8C7283D96877A44F01CA25B1@hou0mbx01.kochind.com>
	<971536df0708222146v2b23424r836d5477d982543d@mail.gmail.com>
Message-ID: <030B041DE2D0A34F8C7283D96877A44F01CA25B2@hou0mbx01.kochind.com>

Thank you so much, the new rollapply worked like a charm.  I retrospect
what I was really trying to do (which the new rollapply works perfectly
for), is calculate the maximum cumulative loss over some period.

For the benefit of anyone else who has this problem...
So with the new rollapply, the following returns an n x m matrix where m
in the width specified in rollapply and is a cumulative sum over the
width
zl <- rollapply(z, 5, align='left',cumsum)

Then doing an apply gives the minimum for each row, and is the maximum
cumulative loss over the window.
zm <- apply(zl, 1, min) 

Thank you so much Gabor and Z!!



 

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Wednesday, August 22, 2007 11:47 PM
To: Voss, Kent
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] rollapply and cummin

The usual case is that the function used in rollapply returns a single
number; however, if you really want to return a vector of 5 numbers try
the latest version of rollapply which was fixed about 2 weeks ago and is
not yet on CRAN but can be downloaded from the development repository.
>From within R:

library(zoo)
source("http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/*checkou
t*/pkg/R/rollapply.R?rev=363&root=zoo")
# ... now run your commands ...



On 8/22/07, Voss, Kent <VOSSK at kochind.com> wrote:
> I am getting some unexpected results using rollapply and the cummin
function that I am hoping someone can help me understand.  I'm just
trying to get a rolling cumulative minimum of a time series.  Think of
it as the maximum loss over a rolling window.
>
> Here's an example
> # Create a dummy sequence to use with a set of dates used to create a 
> zoo object x <- c(0,rep(c(seq(1,5,by=1),seq(-1,-3, by=-1)),2)) 
> DateList <- as.Date(seq(ISOdate(1990,1,1), length.out=length(x), by="1

> day"), '%Y-%m-%d')
>
> z <- zoo(x,DateList)
>
> # Doing the following gets expected results, a rolling 5 day sum.  So 
> far so good zl <- rollapply(z, 5, align='left',sum)
>
> # The following however creates an n x n matrix where n = the length 
> of z, that I can't quite figure out zl <- rollapply(z, 5, 
> align='left',cummin)
>
>
> Now I'm pretty new at this stuff, so I'm sure there's something I'm
missing, but I can't make heads or tails around the results of the
cummin.  Any help would be greatly appreciated.  Thanks in advance.
>
> Kent
>
>
>
>
>
> "EMF <kochind.com>" made the following annotations.
> ----------------------------------------------------------------------
> -------- The information in this e-mail and any attachments is 
> confidential and intended solely for the attention and use of the
named addressee(s). It must not be disclosed to any person without
proper authority. If you are not the intended recipient, or a person
responsible for delivering it to the intended recipient, you are not
authorized to and must not disclose, copy, distribute, or retain this
message or any part of it.
>
> ======================================================================
> ========
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


"EMF <kochind.com>" made the following annotations.
------------------------------------------------------------------------------
The information in this e-mail and any attachments is confidential and intended solely for the attention and use of the named addressee(s). It must not be disclosed to any person without proper authority. If you are not the intended recipient, or a person responsible for delivering it to the intended recipient, you are not authorized to and must not disclose, copy, distribute, or retain this message or any part of it.


From ggrothendieck at gmail.com  Thu Aug 23 14:55:36 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Aug 2007 08:55:36 -0400
Subject: [R-SIG-Finance] rollapply and cummin
In-Reply-To: <030B041DE2D0A34F8C7283D96877A44F01CA25B2@hou0mbx01.kochind.com>
References: <030B041DE2D0A34F8C7283D96877A44F01CA25B1@hou0mbx01.kochind.com>
	<971536df0708222146v2b23424r836d5477d982543d@mail.gmail.com>
	<030B041DE2D0A34F8C7283D96877A44F01CA25B2@hou0mbx01.kochind.com>
Message-ID: <971536df0708230555t3b062ac8u8c3a84d47668cee@mail.gmail.com>

In that case you might prefer:

# test data
set.seed(1)
z <- zoo(rnorm(25))

rollapply(z, 5, function(x) min(cumsum(x)), align = "left")

which (1) works with the old rollapply too since the function
returns a single number and (2) has the advantage of returning
a zoo object rather than a numeric vector so the time information
is not lost.

On 8/23/07, Voss, Kent <VOSSK at kochind.com> wrote:
> Thank you so much, the new rollapply worked like a charm.  I retrospect
> what I was really trying to do (which the new rollapply works perfectly
> for), is calculate the maximum cumulative loss over some period.
>
> For the benefit of anyone else who has this problem...
> So with the new rollapply, the following returns an n x m matrix where m
> in the width specified in rollapply and is a cumulative sum over the
> width
> zl <- rollapply(z, 5, align='left',cumsum)
>
> Then doing an apply gives the minimum for each row, and is the maximum
> cumulative loss over the window.
> zm <- apply(zl, 1, min)
>
> Thank you so much Gabor and Z!!
>
>
>
>
>
> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Wednesday, August 22, 2007 11:47 PM
> To: Voss, Kent
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] rollapply and cummin
>
> The usual case is that the function used in rollapply returns a single
> number; however, if you really want to return a vector of 5 numbers try
> the latest version of rollapply which was fixed about 2 weeks ago and is
> not yet on CRAN but can be downloaded from the development repository.
> From within R:
>
> library(zoo)
> source("http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/*checkou
> t*/pkg/R/rollapply.R?rev=363&root=zoo")
> # ... now run your commands ...
>
>
>
> On 8/22/07, Voss, Kent <VOSSK at kochind.com> wrote:
> > I am getting some unexpected results using rollapply and the cummin
> function that I am hoping someone can help me understand.  I'm just
> trying to get a rolling cumulative minimum of a time series.  Think of
> it as the maximum loss over a rolling window.
> >
> > Here's an example
> > # Create a dummy sequence to use with a set of dates used to create a
> > zoo object x <- c(0,rep(c(seq(1,5,by=1),seq(-1,-3, by=-1)),2))
> > DateList <- as.Date(seq(ISOdate(1990,1,1), length.out=length(x), by="1
>
> > day"), '%Y-%m-%d')
> >
> > z <- zoo(x,DateList)
> >
> > # Doing the following gets expected results, a rolling 5 day sum.  So
> > far so good zl <- rollapply(z, 5, align='left',sum)
> >
> > # The following however creates an n x n matrix where n = the length
> > of z, that I can't quite figure out zl <- rollapply(z, 5,
> > align='left',cummin)
> >
> >
> > Now I'm pretty new at this stuff, so I'm sure there's something I'm
> missing, but I can't make heads or tails around the results of the
> cummin.  Any help would be greatly appreciated.  Thanks in advance.
> >
> > Kent
> >
> >
> >
> >
> >
> > "EMF <kochind.com>" made the following annotations.
> > ----------------------------------------------------------------------
> > -------- The information in this e-mail and any attachments is
> > confidential and intended solely for the attention and use of the
> named addressee(s). It must not be disclosed to any person without
> proper authority. If you are not the intended recipient, or a person
> responsible for delivering it to the intended recipient, you are not
> authorized to and must not disclose, copy, distribute, or retain this
> message or any part of it.
> >
> > ======================================================================
> > ========
> >
> >        [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
>
> "EMF <kochind.com>" made the following annotations.
> ------------------------------------------------------------------------------
> The information in this e-mail and any attachments is confidential and intended solely for the attention and use of the named addressee(s). It must not be disclosed to any person without proper authority. If you are not the intended recipient, or a person responsible for delivering it to the intended recipient, you are not authorized to and must not disclose, copy, distribute, or retain this message or any part of it.
>
> ==============================================================================
>
>


From VOSSK at kochind.com  Thu Aug 23 15:03:35 2007
From: VOSSK at kochind.com (Voss, Kent)
Date: Thu, 23 Aug 2007 08:03:35 -0500
Subject: [R-SIG-Finance] rollapply and cummin
In-Reply-To: <971536df0708230555t3b062ac8u8c3a84d47668cee@mail.gmail.com>
References: <030B041DE2D0A34F8C7283D96877A44F01CA25B1@hou0mbx01.kochind.com>
	<971536df0708222146v2b23424r836d5477d982543d@mail.gmail.com>
	<030B041DE2D0A34F8C7283D96877A44F01CA25B2@hou0mbx01.kochind.com>
	<971536df0708230555t3b062ac8u8c3a84d47668cee@mail.gmail.com>
Message-ID: <030B041DE2D0A34F8C7283D96877A44F01CA25B4@hou0mbx01.kochind.com>

Ah, I had never created a function that way in any of the *apply
functions, I learned something new.  Thank you so much.  R and the R
community is just phenomenal...thank you.


Kent Voss
Koch Quantitative Trading
20 East Greenway Plaza, Suite 450
Houston, TX  77046
Phone: 713.544.5140
Fax: 713.544.6506
kent.voss at kochind.com


 

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Thursday, August 23, 2007 7:56 AM
To: Voss, Kent
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] rollapply and cummin

In that case you might prefer:

# test data
set.seed(1)
z <- zoo(rnorm(25))

rollapply(z, 5, function(x) min(cumsum(x)), align = "left")

which (1) works with the old rollapply too since the function returns a
single number and (2) has the advantage of returning a zoo object rather
than a numeric vector so the time information is not lost.

On 8/23/07, Voss, Kent <VOSSK at kochind.com> wrote:
> Thank you so much, the new rollapply worked like a charm.  I 
> retrospect what I was really trying to do (which the new rollapply 
> works perfectly for), is calculate the maximum cumulative loss over
some period.
>
> For the benefit of anyone else who has this problem...
> So with the new rollapply, the following returns an n x m matrix where

> m in the width specified in rollapply and is a cumulative sum over the

> width zl <- rollapply(z, 5, align='left',cumsum)
>
> Then doing an apply gives the minimum for each row, and is the maximum

> cumulative loss over the window.
> zm <- apply(zl, 1, min)
>
> Thank you so much Gabor and Z!!
>
>
>
>
>
> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Wednesday, August 22, 2007 11:47 PM
> To: Voss, Kent
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] rollapply and cummin
>
> The usual case is that the function used in rollapply returns a single

> number; however, if you really want to return a vector of 5 numbers 
> try the latest version of rollapply which was fixed about 2 weeks ago 
> and is not yet on CRAN but can be downloaded from the development
repository.
> From within R:
>
> library(zoo)
> source("http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/*check
> ou
> t*/pkg/R/rollapply.R?rev=363&root=zoo")
> # ... now run your commands ...
>
>
>
> On 8/22/07, Voss, Kent <VOSSK at kochind.com> wrote:
> > I am getting some unexpected results using rollapply and the cummin
> function that I am hoping someone can help me understand.  I'm just 
> trying to get a rolling cumulative minimum of a time series.  Think of

> it as the maximum loss over a rolling window.
> >
> > Here's an example
> > # Create a dummy sequence to use with a set of dates used to create 
> > a zoo object x <- c(0,rep(c(seq(1,5,by=1),seq(-1,-3, by=-1)),2)) 
> > DateList <- as.Date(seq(ISOdate(1990,1,1), length.out=length(x), 
> > by="1
>
> > day"), '%Y-%m-%d')
> >
> > z <- zoo(x,DateList)
> >
> > # Doing the following gets expected results, a rolling 5 day sum.  
> > So far so good zl <- rollapply(z, 5, align='left',sum)
> >
> > # The following however creates an n x n matrix where n = the length

> > of z, that I can't quite figure out zl <- rollapply(z, 5,
> > align='left',cummin)
> >
> >
> > Now I'm pretty new at this stuff, so I'm sure there's something I'm
> missing, but I can't make heads or tails around the results of the 
> cummin.  Any help would be greatly appreciated.  Thanks in advance.
> >
> > Kent
> >
> >
> >
> >
> >
> > "EMF <kochind.com>" made the following annotations.
> > --------------------------------------------------------------------
> > --
> > -------- The information in this e-mail and any attachments is 
> > confidential and intended solely for the attention and use of the
> named addressee(s). It must not be disclosed to any person without 
> proper authority. If you are not the intended recipient, or a person 
> responsible for delivering it to the intended recipient, you are not 
> authorized to and must not disclose, copy, distribute, or retain this 
> message or any part of it.
> >
> > ====================================================================
> > ==
> > ========
> >
> >        [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
>
> "EMF <kochind.com>" made the following annotations.
> ----------------------------------------------------------------------
> -------- The information in this e-mail and any attachments is 
> confidential and intended solely for the attention and use of the
named addressee(s). It must not be disclosed to any person without
proper authority. If you are not the intended recipient, or a person
responsible for delivering it to the intended recipient, you are not
authorized to and must not disclose, copy, distribute, or retain this
message or any part of it.
>
> ======================================================================
> ========
>
>


"EMF <kochind.com>" made the following annotations.
------------------------------------------------------------------------------
The information in this e-mail and any attachments is confidential and intended solely for the attention and use of the named addressee(s). It must not be disclosed to any person without proper authority. If you are not the intended recipient, or a person responsible for delivering it to the intended recipient, you are not authorized to and must not disclose, copy, distribute, or retain this message or any part of it.


From barth at tac-financial.com  Thu Aug 23 15:08:14 2007
From: barth at tac-financial.com (Sylvain BARTHELEMY)
Date: Thu, 23 Aug 2007 15:08:14 +0200
Subject: [R-SIG-Finance] Bloomberg: Polygon with color transition
In-Reply-To: <001701c7e57b$6734cc40$ad04d30a@backoffice>
References: <001701c7e57b$6734cc40$ad04d30a@backoffice>
Message-ID: <006c01c7e586$aab86ca0$002945e0$@com>

If this is for a PHP website, you can do that with jpgraph:
 http://www.aditus.nu/jpgraph/

---
Sylvain Barth?l?my
Research Director, TAC
www.tac-financial.com | www.sylbarth.com


-----Message d'origine-----
De?: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de Alexander
Wurzer
Envoy??: jeudi 23 ao?t 2007 13:48
??: r-sig-finance at stat.math.ethz.ch
Objet?: [R-SIG-Finance] Bloomberg: Polygon with color transition

Hi,

Probably this is not the best mailing-list for my problem, but I don't know
where I should ask instead...
Has anybody of you an idea how I can create a polygon with color transition
like on Bloomberg?
Attached you find an example what I mean.

Kind regards,
Alex


From finbref.2006 at gmail.com  Thu Aug 23 15:10:36 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Thu, 23 Aug 2007 15:10:36 +0200
Subject: [R-SIG-Finance] Bloomberg: Polygon with color transition
In-Reply-To: <002001c7e581$001ed130$ad04d30a@backoffice>
References: <d0f55a670708230507t43f2139x21641eaea956e78@mail.gmail.com>
	<002001c7e581$001ed130$ad04d30a@backoffice>
Message-ID: <d0f55a670708230610l34eb6066s417a1352d6990ee0@mail.gmail.com>

> thanks for your reply. Can you just explain in more detail how this works? I
> know the SVG packages, but I have no idea how to implement a graphic like on
> bloomberg. Maybe you can give me a short example if it's not too
> time-consuming?

I am busy right now, but perhaps this quick help works ;)

* Plot your data with R using the polygone like here (code on this
page as well): http://commons.wikimedia.org/wiki/Image:VaR_graph.png
* save it as a SVG
* add by hand a gradient (
http://de.wikipedia.org/wiki/Gradient_(Grafik) ) eg with Inkscape. I
think that you cannot do this directly with the SVG package.

If you need further help, let me know, perhaps I find some time on the weekend.

BTW: Did you study TM in Vienna?
Best
Thomas


From alexander.wurzer at fin4cast.com  Thu Aug 23 15:34:29 2007
From: alexander.wurzer at fin4cast.com (Alexander Wurzer)
Date: Thu, 23 Aug 2007 15:34:29 +0200
Subject: [R-SIG-Finance] Bloomberg: Polygon with color transition
In-Reply-To: <d0f55a670708230610l34eb6066s417a1352d6990ee0@mail.gmail.com>
Message-ID: <002101c7e58a$54a7d040$ad04d30a@backoffice>

thanks for your advices! 
unfortunately, it doesn't solve my problem.

I dynamically generate a PDF-performance-report for many assets and
portfolios. So adding a gradient manually would be very inconvenient.
The data-connection, calculation and the plot-functions are written in R, so
the PHP solution is not an appropriate solution for me. Thanks anyway!

Everything works fine - it would be just an optical improvement for our
investors...

It would be great if anybody has an idea how I can create such a plot
directly in R.

Best,
Alex

P.S.: I studied "Management Science" at Vienna University of Economics and
Business Administration

-----Original Message-----
From: Thomas Steiner [mailto:finbref.2006 at gmail.com] 
Sent: Thursday, 23. August 2007 15:11
To: alexander.wurzer at fin4cast.com
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Bloomberg: Polygon with color transition


> thanks for your reply. Can you just explain in more detail how this 
> works? I know the SVG packages, but I have no idea how to implement a 
> graphic like on bloomberg. Maybe you can give me a short example if 
> it's not too time-consuming?

I am busy right now, but perhaps this quick help works ;)

* Plot your data with R using the polygone like here (code on this page as
well): http://commons.wikimedia.org/wiki/Image:VaR_graph.png
* save it as a SVG
* add by hand a gradient (
http://de.wikipedia.org/wiki/Gradient_(Grafik) ) eg with Inkscape. I think
that you cannot do this directly with the SVG package.

If you need further help, let me know, perhaps I find some time on the
weekend.

BTW: Did you study TM in Vienna?
Best
Thomas


From ggrothendieck at gmail.com  Thu Aug 23 16:26:16 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Aug 2007 10:26:16 -0400
Subject: [R-SIG-Finance] rollapply and cummin
In-Reply-To: <030B041DE2D0A34F8C7283D96877A44F01CA25B4@hou0mbx01.kochind.com>
References: <030B041DE2D0A34F8C7283D96877A44F01CA25B1@hou0mbx01.kochind.com>
	<971536df0708222146v2b23424r836d5477d982543d@mail.gmail.com>
	<030B041DE2D0A34F8C7283D96877A44F01CA25B2@hou0mbx01.kochind.com>
	<971536df0708230555t3b062ac8u8c3a84d47668cee@mail.gmail.com>
	<030B041DE2D0A34F8C7283D96877A44F01CA25B4@hou0mbx01.kochind.com>
Message-ID: <971536df0708230726n1a4ae4d2q7087dd3f057ebcdc@mail.gmail.com>

Using gsubfn its also possible to write it in a way which allows the
function body to be specified as a formula for compactness.  Note
the fn$ prefix:

   library(gsubfn)
   fn$rollapply(z, 5, ~ min(cumsum(x)), align = "left")

More at
   http://gsubfn.googlecode.com

On 8/23/07, Voss, Kent <VOSSK at kochind.com> wrote:
> Ah, I had never created a function that way in any of the *apply
> functions, I learned something new.  Thank you so much.  R and the R
> community is just phenomenal...thank you.
>
>
> Kent Voss
> Koch Quantitative Trading
> 20 East Greenway Plaza, Suite 450
> Houston, TX  77046
> Phone: 713.544.5140
> Fax: 713.544.6506
> kent.voss at kochind.com
>
>
>
>
> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Thursday, August 23, 2007 7:56 AM
> To: Voss, Kent
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] rollapply and cummin
>
> In that case you might prefer:
>
> # test data
> set.seed(1)
> z <- zoo(rnorm(25))
>
> rollapply(z, 5, function(x) min(cumsum(x)), align = "left")
>
> which (1) works with the old rollapply too since the function returns a
> single number and (2) has the advantage of returning a zoo object rather
> than a numeric vector so the time information is not lost.
>
> On 8/23/07, Voss, Kent <VOSSK at kochind.com> wrote:
> > Thank you so much, the new rollapply worked like a charm.  I
> > retrospect what I was really trying to do (which the new rollapply
> > works perfectly for), is calculate the maximum cumulative loss over
> some period.
> >
> > For the benefit of anyone else who has this problem...
> > So with the new rollapply, the following returns an n x m matrix where
>
> > m in the width specified in rollapply and is a cumulative sum over the
>
> > width zl <- rollapply(z, 5, align='left',cumsum)
> >
> > Then doing an apply gives the minimum for each row, and is the maximum
>
> > cumulative loss over the window.
> > zm <- apply(zl, 1, min)
> >
> > Thank you so much Gabor and Z!!
> >
> >
> >
> >
> >
> > -----Original Message-----
> > From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> > Sent: Wednesday, August 22, 2007 11:47 PM
> > To: Voss, Kent
> > Cc: r-sig-finance at stat.math.ethz.ch
> > Subject: Re: [R-SIG-Finance] rollapply and cummin
> >
> > The usual case is that the function used in rollapply returns a single
>
> > number; however, if you really want to return a vector of 5 numbers
> > try the latest version of rollapply which was fixed about 2 weeks ago
> > and is not yet on CRAN but can be downloaded from the development
> repository.
> > From within R:
> >
> > library(zoo)
> > source("http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/*check
> > ou
> > t*/pkg/R/rollapply.R?rev=363&root=zoo")
> > # ... now run your commands ...
> >
> >
> >
> > On 8/22/07, Voss, Kent <VOSSK at kochind.com> wrote:
> > > I am getting some unexpected results using rollapply and the cummin
> > function that I am hoping someone can help me understand.  I'm just
> > trying to get a rolling cumulative minimum of a time series.  Think of
>
> > it as the maximum loss over a rolling window.
> > >
> > > Here's an example
> > > # Create a dummy sequence to use with a set of dates used to create
> > > a zoo object x <- c(0,rep(c(seq(1,5,by=1),seq(-1,-3, by=-1)),2))
> > > DateList <- as.Date(seq(ISOdate(1990,1,1), length.out=length(x),
> > > by="1
> >
> > > day"), '%Y-%m-%d')
> > >
> > > z <- zoo(x,DateList)
> > >
> > > # Doing the following gets expected results, a rolling 5 day sum.
> > > So far so good zl <- rollapply(z, 5, align='left',sum)
> > >
> > > # The following however creates an n x n matrix where n = the length
>
> > > of z, that I can't quite figure out zl <- rollapply(z, 5,
> > > align='left',cummin)
> > >
> > >
> > > Now I'm pretty new at this stuff, so I'm sure there's something I'm
> > missing, but I can't make heads or tails around the results of the
> > cummin.  Any help would be greatly appreciated.  Thanks in advance.
> > >
> > > Kent
> > >
> > >
> > >
> > >
> > >
> > > "EMF <kochind.com>" made the following annotations.
> > > --------------------------------------------------------------------
> > > --
> > > -------- The information in this e-mail and any attachments is
> > > confidential and intended solely for the attention and use of the
> > named addressee(s). It must not be disclosed to any person without
> > proper authority. If you are not the intended recipient, or a person
> > responsible for delivering it to the intended recipient, you are not
> > authorized to and must not disclose, copy, distribute, or retain this
> > message or any part of it.
> > >
> > > ====================================================================
> > > ==
> > > ========
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> >
> >
> > "EMF <kochind.com>" made the following annotations.
> > ----------------------------------------------------------------------
> > -------- The information in this e-mail and any attachments is
> > confidential and intended solely for the attention and use of the
> named addressee(s). It must not be disclosed to any person without
> proper authority. If you are not the intended recipient, or a person
> responsible for delivering it to the intended recipient, you are not
> authorized to and must not disclose, copy, distribute, or retain this
> message or any part of it.
> >
> > ======================================================================
> > ========
> >
> >
>
>
> "EMF <kochind.com>" made the following annotations.
> ------------------------------------------------------------------------------
> The information in this e-mail and any attachments is confidential and intended solely for the attention and use of the named addressee(s). It must not be disclosed to any person without proper authority. If you are not the intended recipient, or a person responsible for delivering it to the intended recipient, you are not authorized to and must not disclose, copy, distribute, or retain this message or any part of it.
>
> ==============================================================================
>
>


From dcielen at vub.ac.be  Thu Aug 23 19:14:05 2007
From: dcielen at vub.ac.be (Davy)
Date: Thu, 23 Aug 2007 19:14:05 +0200
Subject: [R-SIG-Finance] Bloomberg
Message-ID: <1187889245.16307.11.camel@davy-desktop>

Dearest,

On this page (1) you can find an example of a (pseudo) gradient, you
need to make the intervals closer and then change the direction. I think
however that such an approach will slow down your code.

(1) http://commons.wikimedia.org/wiki/Image:Quantile_graph.png
The R code is included at the bottom of the page.

I hope this can be of any help for your project,

Sincerely yours,

Davy


From bbands at gmail.com  Thu Aug 23 22:10:40 2007
From: bbands at gmail.com (BBands)
Date: Thu, 23 Aug 2007 13:10:40 -0700
Subject: [R-SIG-Finance] Diversification Comments...
In-Reply-To: <358604.70838.qm@web32209.mail.mud.yahoo.com>
References: <358604.70838.qm@web32209.mail.mud.yahoo.com>
Message-ID: <6e8360ad0708231310m50473353u189e42ecb3415c9b@mail.gmail.com>

On 8/22/07, Marc E Levitt <marcl at svquant.com> wrote:
> Consider the question you are looking to answer before we dismiss diversification and how much diversification is useful. There are many papers that explore diversification in terms of terminal wealth standard deviation which is quite important for those in the asset-liability management world, e.g. pension funds, where a short-fall at any given instance is perhaps most important. Most conclusions in this area is that more diversification even with high correlation coefficients (of course not 1.0) between managers (instruments) pays off depending on implementation costs. I looked at this in terms of trend following CTAs and found that diversification continues to pay off when looking at terminal wealth vs time series standard deviations well beyond what the standard rules of thumb tell you.

Terminal wealth means nothing in the presence of large drawdowns.

See Ralph Vince's new book for some consideration of the above idea.

http://www.amazon.com/Handbook-Portfolio-Mathematics-Formulas-Allocation/dp/0471757683/

"Risk is the probability of being ruined."

"Ruin is touching or penetrating some lower barrier on your equity."

When correlations converge on one and that lower barrier is touched
long-term goals mean nothing as investing stops and liquidation
starts. Many examples of this in the last two weeks news.

Also of interest, the 23 August ISI Quantitative Research piece
entitled "Market Direction and Sector Correlation".

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From finbref.2006 at gmail.com  Thu Aug 23 22:38:00 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Thu, 23 Aug 2007 22:38:00 +0200
Subject: [R-SIG-Finance] Bloomberg
In-Reply-To: <1187889245.16307.11.camel@davy-desktop>
References: <1187889245.16307.11.camel@davy-desktop>
Message-ID: <d0f55a670708231338w414e916axb248a6b76c03f285@mail.gmail.com>

> (1) http://commons.wikimedia.org/wiki/Image:Quantile_graph.png
> The R code is included at the bottom of the page.

:) my image :))
but it's really very "pseudo".
Thomas


From alexander.wurzer at fin4cast.com  Fri Aug 24 08:03:44 2007
From: alexander.wurzer at fin4cast.com (Alexander Wurzer)
Date: Fri, 24 Aug 2007 08:03:44 +0200
Subject: [R-SIG-Finance] Bloomberg: Polygon with color transition
In-Reply-To: <d0f55a670708231329p17ef0ba4wd012703a6049dee3@mail.gmail.com>
Message-ID: <000201c7e614$86fe0fc0$ad04d30a@backoffice>

Thanks for the numerous ideas!
I'll try to include some of them in my scripts.

Alex

-----Original Message-----
From: Thomas Steiner [mailto:finbref.2006 at gmail.com] 
Sent: Thursday, 23. August 2007 22:29
To: alexander.wurzer at fin4cast.com
Subject: Re: [R-SIG-Finance] Bloomberg: Polygon with color transition


> It would be great if anybody has an idea how I can create such a plot 
> directly in R.

you add the gradient
(http://www.w3schools.com/svg/svg_grad_linear.asp) to the svg package
;)
or you ask the original author. or some other svg-geeks. good luck ;) Thomas


From davison at stats.ox.ac.uk  Fri Aug 24 13:11:24 2007
From: davison at stats.ox.ac.uk (Dan Davison)
Date: Fri, 24 Aug 2007 12:11:24 +0100
Subject: [R-SIG-Finance] Bloomberg
In-Reply-To: <d0f55a670708231338w414e916axb248a6b76c03f285@mail.gmail.com>
References: <1187889245.16307.11.camel@davy-desktop>
	<d0f55a670708231338w414e916axb248a6b76c03f285@mail.gmail.com>
Message-ID: <20070824111124.GD19118@stats.ox.ac.uk>

On Thu, Aug 23, 2007 at 10:38:00PM +0200, Thomas Steiner wrote:
> > (1) http://commons.wikimedia.org/wiki/Image:Quantile_graph.png
> > The R code is included at the bottom of the page.
> 
> :) my image :))
> but it's really very "pseudo".

This approach does give a good pure R solution doesn't it? e.g. something like

gradient.under.graph <- function(n=100, y0=0, mu=-0.7, sd=1, nrects=1000) {
    y <- y0 + cumsum(rnorm(n, mean=mu, sd=sd))
    plot(NA, xlim=c(1,n), ylim=range(y), bty="n")
    col <- colorRampPalette(colors=c("dark blue","light blue"))(nrects)
    incr <- (max(y) - min(y)) / nrects
    rect(0, seq(min(y), max(y)-incr, length=nrects), n, seq(min(y)+incr, max(y), length=nrects), col=col, border=NA)
    polygon(x=c(1,1:n,n), y=c(max(y), y, max(y)), col="white", border=NULL)
}

is not slow.

Dan




> Thomas
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.


From nbryant at optonline.net  Fri Aug 24 17:12:09 2007
From: nbryant at optonline.net (Nathan Bryant)
Date: Fri, 24 Aug 2007 11:12:09 -0400
Subject: [R-SIG-Finance] ARIMA(0,1,0)+c results estimate incorrect drift
Message-ID: <46CEF549.2060107@optonline.net>


Hi,

I seem to have found a problem. Integrated ARIMA with drift uses a
linear regression on xreg and the results don't look numerically stable.
Forecasts are inconsistent with an equivalent arima(0,0,0) on
differenced data and with rwf. Maybe someone knows if there is a
rational explanation?

Try this,

library(forecast)
set.seed(3)
rand <- rnorm(1000, mean=.2)
mean(rand)
fit1 <- arima(diffinv(rand), order=c(0,1,0), include.drift=T)
coef(fit1)
fit2 <- arima(rand, order=c(0,0,0))
coef(fit2)



Without seed you may have to repeat the experiment a few times to see
the nuisance, seed is included to show an example of the problem:

> library(forecast)
> set.seed(3)
> rand <- rnorm(1000, mean=.2)
> mean(rand)
[1] 0.2063965
> fit1 <- arima(diffinv(rand), order=c(0,1,0), include.drift=T)
> coef(fit1)
    drift
0.2073960
> fit2 <- arima(rand, order=c(0,0,0))
> coef(fit2)
intercept
0.2063965




Nathan Bryant


From dcielen at vub.ac.be  Fri Aug 24 23:36:35 2007
From: dcielen at vub.ac.be (Davy Cielen)
Date: Fri, 24 Aug 2007 23:36:35 +0200
Subject: [R-SIG-Finance] Bloomberg
In-Reply-To: <20070824111124.GD19118@stats.ox.ac.uk>
References: <1187889245.16307.11.camel@davy-desktop>
	<d0f55a670708231338w414e916axb248a6b76c03f285@mail.gmail.com>
	<20070824111124.GD19118@stats.ox.ac.uk>
Message-ID: <1187991395.15359.10.camel@davy-desktop>

It looks very nice and is fast indeed,
I tried to build a gradient with 100.000 intervals,

it was build in no time.

Can this type of solution be converted to an overal gradient independent
of the direction and then be added to some library?  

Great work Tom and Dan,

Davy

-- 
   One suggestion as to tentative expected return and risk is to use the
observed return and risk for some period of the past. I believe that
better methods, which take into account more information, can be found.
I believe that what is needed is essentially a "probabilistic"
reformulation of security analysis. I wi]l not pursue this subject here,
for this is "another story." It is a story of which I have read only the
first page of the first chapter.

"Markowitz"

-------------- next part --------------
An embedded message was scrubbed...
From: Dan Davison <davison at stats.ox.ac.uk>
Subject: Re: [R-SIG-Finance] Bloomberg
Date: Fri, 24 Aug 2007 12:11:24 +0100
Size: 2673
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070824/924cf505/attachment.mht 

From William.Alpert at barrons.com  Sat Aug 25 17:38:34 2007
From: William.Alpert at barrons.com (Alpert, William)
Date: Sat, 25 Aug 2007 11:38:34 -0400
Subject: [R-SIG-Finance] Tabloid journalism using R !
In-Reply-To: <mailman.15.1188036006.2590.r-sig-finance@stat.math.ethz.ch>
Message-ID: <C39A02DFF9023744AC627E11F528C45B042CE0@SBKMXSMB04.win.dowjones.net>

Hey list !

Esteemed list-member Pat Burns helped the stock market tabloid Barron's
do a cover story last week that tested the returns to the stock picks
that Jim Cramer makes on his CNBC show Mad Money.  I think the story is
accessible without subscription at our website www.barrons.com, but I'll
send copies to anyone who cares and can't find it.  Since today's
Saturday, it's now the past week's edition.

We used R to test various investment strategies, such as buying the
morning after Cramer's 6 pm show, buying on the second day after the
show, etc. One reason we kept testing new strategies is that Cramer and
CNBC kept shifting their claim about when Cramer advises his audience to
Buy, as we showed them how each successive strategy lagged the market.
They also made weasely arguments that you should only count certain of
his recommendations -- as if viewers would know that his fingers are
crossed some of the times that he tells you to Buy or Sell.  As it
happened, the segment of his show that he argued most shrilly for us to
deselect (his Lightning Round recommendations to phone callers) was the
only segment with statistically significant good excess returns -- in
this case, on the Sell recommendations. His prepared Sell
recommendations went up.

We did find that you might make an interesting 20-day return by shorting
his Buys the morning after his shows, with offsetting S&P500 positions.

We downloaded our stock histories from Edgar Online's I-Metrix service,
using some Excel macros cooked up by Edgar Online analyst Elias-John
Kies and my neighbor Madison Mcgaffin, who's a rising sophomore at
Tufts. My story credits to them got cut in the editing.

Pat bootstrapped our 95% confidence intervals. We also did some event
study style analyses, incorporating each stock's average return over its
prior 250 days. These suggest that Cramer is a momentum guy who probably
rips his ideas from the headlines.

Very few statistical details made it into the final story. Nor did my
appreciative credit to Tim Hesterberg, of Insightful in Seattle, who
gave us some good advice but who bears no responsibility for any
woebegone errors.

Hey Pat ! Feel free to correct any misreporting done here by this
misanthropic layman reporter.

Bill Alpert
Barron's

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
r-sig-finance-request at stat.math.ethz.ch
Sent: Saturday, August 25, 2007 6:00 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: R-SIG-Finance Digest, Vol 39, Issue 23


Send R-SIG-Finance mailing list submissions to
	r-sig-finance at stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
or, via email, send a message with subject or body 'help' to
	r-sig-finance-request at stat.math.ethz.ch

You can reach the person managing the list at
	r-sig-finance-owner at stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific than
"Re: Contents of R-SIG-Finance digest..."


Today's Topics:

   1. Re: Bloomberg (Dan Davison)
   2. ARIMA(0,1,0)+c results estimate incorrect drift (Nathan Bryant)
   3. Re: Bloomberg (Davy Cielen)


----------------------------------------------------------------------

Message: 1
Date: Fri, 24 Aug 2007 12:11:24 +0100
From: Dan Davison <davison at stats.ox.ac.uk>
Subject: Re: [R-SIG-Finance] Bloomberg
To: Thomas Steiner <finbref.2006 at gmail.com>
Cc: r-sig-finance at stat.math.ethz.ch
Message-ID: <20070824111124.GD19118 at stats.ox.ac.uk>
Content-Type: text/plain; charset=us-ascii

On Thu, Aug 23, 2007 at 10:38:00PM +0200, Thomas Steiner wrote:
> > (1) http://commons.wikimedia.org/wiki/Image:Quantile_graph.png
> > The R code is included at the bottom of the page.
> 
> :) my image :))
> but it's really very "pseudo".

This approach does give a good pure R solution doesn't it? e.g.
something like

gradient.under.graph <- function(n=100, y0=0, mu=-0.7, sd=1,
nrects=1000) {
    y <- y0 + cumsum(rnorm(n, mean=mu, sd=sd))
    plot(NA, xlim=c(1,n), ylim=range(y), bty="n")
    col <- colorRampPalette(colors=c("dark blue","light blue"))(nrects)
    incr <- (max(y) - min(y)) / nrects
    rect(0, seq(min(y), max(y)-incr, length=nrects), n, seq(min(y)+incr,
max(y), length=nrects), col=col, border=NA)
    polygon(x=c(1,1:n,n), y=c(max(y), y, max(y)), col="white",
border=NULL) }

is not slow.

Dan




> Thomas
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.



------------------------------

Message: 2
Date: Fri, 24 Aug 2007 11:12:09 -0400
From: Nathan Bryant <nbryant at optonline.net>
Subject: [R-SIG-Finance] ARIMA(0,1,0)+c results estimate incorrect
	drift
To: r-sig-finance at stat.math.ethz.ch
Message-ID: <46CEF549.2060107 at optonline.net>
Content-Type: text/plain; charset=UTF-8; format=flowed


Hi,

I seem to have found a problem. Integrated ARIMA with drift uses a
linear regression on xreg and the results don't look numerically stable.
Forecasts are inconsistent with an equivalent arima(0,0,0) on
differenced data and with rwf. Maybe someone knows if there is a
rational explanation?

Try this,

library(forecast)
set.seed(3)
rand <- rnorm(1000, mean=.2)
mean(rand)
fit1 <- arima(diffinv(rand), order=c(0,1,0), include.drift=T)
coef(fit1)
fit2 <- arima(rand, order=c(0,0,0))
coef(fit2)



Without seed you may have to repeat the experiment a few times to see
the nuisance, seed is included to show an example of the problem:

> library(forecast)
> set.seed(3)
> rand <- rnorm(1000, mean=.2)
> mean(rand)
[1] 0.2063965
> fit1 <- arima(diffinv(rand), order=c(0,1,0), include.drift=T)
> coef(fit1)
    drift
0.2073960
> fit2 <- arima(rand, order=c(0,0,0))
> coef(fit2)
intercept
0.2063965




Nathan Bryant



------------------------------

Message: 3
Date: Fri, 24 Aug 2007 23:36:35 +0200
From: Davy Cielen <dcielen at vub.ac.be>
Subject: Re: [R-SIG-Finance] Bloomberg
To: Dan Davison <davison at stats.ox.ac.uk>
Cc: r-sig-finance at stat.math.ethz.ch
Message-ID: <1187991395.15359.10.camel at davy-desktop>
Content-Type: text/plain; charset="us-ascii"

It looks very nice and is fast indeed,
I tried to build a gradient with 100.000 intervals,

it was build in no time.

Can this type of solution be converted to an overal gradient independent
of the direction and then be added to some library?  

Great work Tom and Dan,

Davy

-- 
   One suggestion as to tentative expected return and risk is to use the
observed return and risk for some period of the past. I believe that
better methods, which take into account more information, can be found.
I believe that what is needed is essentially a "probabilistic"
reformulation of security analysis. I wi]l not pursue this subject here,
for this is "another story." It is a story of which I have read only the
first page of the first chapter.

"Markowitz"

-------------- next part --------------
An embedded message was scrubbed...
From: Dan Davison <davison at stats.ox.ac.uk>
Subject: Re: [R-SIG-Finance] Bloomberg
Date: Fri, 24 Aug 2007 12:11:24 +0100
Size: 2673
Url:
https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070824/924cf5
05/attachment-0001.mht 

------------------------------

_______________________________________________
R-SIG-Finance mailing list
R-SIG-Finance at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


End of R-SIG-Finance Digest, Vol 39, Issue 23


From edd at debian.org  Sat Aug 25 18:57:27 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 25 Aug 2007 11:57:27 -0500
Subject: [R-SIG-Finance] Tabloid journalism using R !
In-Reply-To: <C39A02DFF9023744AC627E11F528C45B042CE0@SBKMXSMB04.win.dowjones.net>
References: <mailman.15.1188036006.2590.r-sig-finance@stat.math.ethz.ch>
	<C39A02DFF9023744AC627E11F528C45B042CE0@SBKMXSMB04.win.dowjones.net>
Message-ID: <18128.24439.747663.913213@ron.nulle.part>


Bill,

That's a _great_ story.  I'd suggest to marginally extend the email with
another paragraph or two, maybe a summary chart, and send it to R News!!

Cheers, Dirk

-- 
Three out of two people have difficulties with fractions.


From ggrothendieck at gmail.com  Sat Aug 25 19:19:53 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 25 Aug 2007 13:19:53 -0400
Subject: [R-SIG-Finance] Tabloid journalism using R !
In-Reply-To: <C39A02DFF9023744AC627E11F528C45B042CE0@SBKMXSMB04.win.dowjones.net>
References: <mailman.15.1188036006.2590.r-sig-finance@stat.math.ethz.ch>
	<C39A02DFF9023744AC627E11F528C45B042CE0@SBKMXSMB04.win.dowjones.net>
Message-ID: <971536df0708251019l1bc10fddi9967c9a6d222f8ba@mail.gmail.com>

On 8/25/07, Alpert, William <William.Alpert at barrons.com> wrote:
> Esteemed list-member Pat Burns helped the stock market tabloid Barron's
> do a cover story last week that tested the returns to the stock picks
> that Jim Cramer makes on his CNBC show Mad Money.  I think the story is
> accessible without subscription at our website www.barrons.com, but I'll
> send copies to anyone who cares and can't find it.  Since today's
> Saturday, it's now the past week's edition.

Its here:

http://online.barrons.com/article/SB118681265755995100.html?mod=9_0002_b_this_weeks_magazine_home_top


From finbref.2006 at gmail.com  Sun Aug 26 07:43:36 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Sun, 26 Aug 2007 07:43:36 +0200
Subject: [R-SIG-Finance] Bloomberg
In-Reply-To: <1187991395.15359.10.camel@davy-desktop>
References: <1187889245.16307.11.camel@davy-desktop>
	<d0f55a670708231338w414e916axb248a6b76c03f285@mail.gmail.com>
	<20070824111124.GD19118@stats.ox.ac.uk>
	<1187991395.15359.10.camel@davy-desktop>
Message-ID: <d0f55a670708252243t79ebbeecv8867c496fd134031@mail.gmail.com>

> Can this type of solution be converted to an overal gradient independent
> of the direction and then be added to some library?

perhaps this is close: did you check out the function "gradient.rect"
in the "plotrix Package"
(http://cran.r-project.org/src/contrib/Descriptions/plotrix.html)?
Thomas


From shiazy at gmail.com  Sun Aug 26 09:08:49 2007
From: shiazy at gmail.com (Shiazy Fuzzy)
Date: Sun, 26 Aug 2007 09:08:49 +0200
Subject: [R-SIG-Finance] Error on "fBasics::dstable"
Message-ID: <9d3ef91d0708260008y1ac833e6x368284fc4b7d318a@mail.gmail.com>

Hi,

I get this error on fBasics::dstable function:

--- [CUT HERE] ---
Error in dstable(y, alpha = alpha, beta = beta, gamma = gamma, delta =
delta) : object "result" not found
--- [/CUT HERE] ---

Looking at the code ("2A-StableDistribution.R"), it seems it is
sufficient to add an initialization instruction, for instance, at line
442:

--- [CUT HERE] ---
    result = 0; # ADDED INITIALIZATION
    # Special Cases:
    if (alpha == 2) {
        result = dnorm(x, mean = 0, sd = sqrt(2))
    }
    if (alpha == 1 & beta == 0) {
        result = dcauchy(x)
    }
--- [/CUT HERE] ---

Furthermore, I think the test on "beta" ("... & abs(beta) <= 1") at
line 454 is useless because of test at lines 425 and 426 ("if (beta  <
-1) ...." and "if (beta  > -1) ...", respectively).

My system is:
CPU: AMD Turion x2 TL-52
RAM: 1GB
OS: Fedora 7 x86_64
R: v. 2.5.1
fBasics. fCalendar, fEcofin: v. 251.70

Sincerely,

-- Marco


From shiazy at gmail.com  Sun Aug 26 15:43:27 2007
From: shiazy at gmail.com (Shiazy Fuzzy)
Date: Sun, 26 Aug 2007 15:43:27 +0200
Subject: [R-SIG-Finance] Error on "fBasics::dstable"
In-Reply-To: <9d3ef91d0708260008y1ac833e6x368284fc4b7d318a@mail.gmail.com>
References: <9d3ef91d0708260008y1ac833e6x368284fc4b7d318a@mail.gmail.com>
Message-ID: <9d3ef91d0708260643o749e82b1jcbd9319d9b7f7885@mail.gmail.com>

On 8/26/07, Shiazy Fuzzy <shiazy at gmail.com> wrote:
> Hi,
>
> I get this error on fBasics::dstable function:
>
> --- [CUT HERE] ---
> Error in dstable(y, alpha = alpha, beta = beta, gamma = gamma, delta =
> delta) : object "result" not found
> --- [/CUT HERE] ---
>
> Looking at the code ("2A-StableDistribution.R"), it seems it is
> sufficient to add an initialization instruction, for instance, at line
> 442:
>
> --- [CUT HERE] ---
>     result = 0; # ADDED INITIALIZATION
>     # Special Cases:
>     if (alpha == 2) {
>         result = dnorm(x, mean = 0, sd = sqrt(2))
>     }
>     if (alpha == 1 & beta == 0) {
>         result = dcauchy(x)
>     }
> --- [/CUT HERE] ---
>
> Furthermore, I think the test on "beta" ("... & abs(beta) <= 1") at
> line 454 is useless because of test at lines 425 and 426 ("if (beta  <
> -1) ...." and "if (beta  > -1) ...", respectively).
>
> My system is:
> CPU: AMD Turion x2 TL-52
> RAM: 1GB
> OS: Fedora 7 x86_64
> R: v. 2.5.1
> fBasics. fCalendar, fEcofin: v. 251.70
>
> Sincerely,
>
> -- Marco
>

Here is a reproducible example:

--- [CUT HERE] ---
library(fBasics);
x <- c( 492408, 1897, 66795, 4014, 14, 11, 5770, 4284, 672, 50, 769,
21, 28, 13, 76, 29, 37, 28, 114, 63, 1479, 166, 999, 117, 8, 14, 206,
231, 85, 74684, 1130, 475, 657, 27, 230, 607, 1460, 116, 125, 2058,
3413, 69110, 20, 43, 406, 147, 314, 69, 310, 21, 1059, 986 );
stableFit( x,type=c("mle"),doplot=F );
--- [/CUT HERE] ---

With the above corrections, the problem seems to be partially solved
... Partially because I found out if you specifiy 'doplot = TRUE', the
same error happens in "qstable"  method:

--- [CUT HERE] ---
Error in qstable(0.01, alpha, beta) : object "result" not found
--- [/CUT HERE] ---

Note this error appears only if you have already corrected the
"dstable" method; otherwise execution stops before.
I've tried to fix it with the same kind of corrections done for
"dstable" (added "result=NULL;" to line 767. However, I got a new
error (in place of the above):

--- [CUT HERE] ---
Error in if (from == to) rep.int(from, length.out) else as.vector(c(from,  :
        argument is of length zero
--- [/CUT HERE] ---

Let me know if there's a better solution or if I'm wrong.

Thank you so much!

Sincerely,

-- Marco


From shiazy at gmail.com  Sun Aug 26 16:00:40 2007
From: shiazy at gmail.com (Shiazy Fuzzy)
Date: Sun, 26 Aug 2007 16:00:40 +0200
Subject: [R-SIG-Finance] Error on "fBasics::dstable"
In-Reply-To: <9d3ef91d0708260643o749e82b1jcbd9319d9b7f7885@mail.gmail.com>
References: <9d3ef91d0708260008y1ac833e6x368284fc4b7d318a@mail.gmail.com>
	<9d3ef91d0708260643o749e82b1jcbd9319d9b7f7885@mail.gmail.com>
Message-ID: <9d3ef91d0708260700r2c0509a6nab96019a3c1c7cfb@mail.gmail.com>

On 8/26/07, Shiazy Fuzzy <shiazy at gmail.com> wrote:
> On 8/26/07, Shiazy Fuzzy <shiazy at gmail.com> wrote:
> > Hi,
> >
> > I get this error on fBasics::dstable function:
> >
> > --- [CUT HERE] ---
> > Error in dstable(y, alpha = alpha, beta = beta, gamma = gamma, delta =
> > delta) : object "result" not found
> > --- [/CUT HERE] ---
> >
> > Looking at the code ("2A-StableDistribution.R"), it seems it is
> > sufficient to add an initialization instruction, for instance, at line
> > 442:
> >
> > --- [CUT HERE] ---
> >     result = 0; # ADDED INITIALIZATION
> >     # Special Cases:
> >     if (alpha == 2) {
> >         result = dnorm(x, mean = 0, sd = sqrt(2))
> >     }
> >     if (alpha == 1 & beta == 0) {
> >         result = dcauchy(x)
> >     }
> > --- [/CUT HERE] ---
> >
> > Furthermore, I think the test on "beta" ("... & abs(beta) <= 1") at
> > line 454 is useless because of test at lines 425 and 426 ("if (beta  <
> > -1) ...." and "if (beta  > -1) ...", respectively).
> >
> > My system is:
> > CPU: AMD Turion x2 TL-52
> > RAM: 1GB
> > OS: Fedora 7 x86_64
> > R: v. 2.5.1
> > fBasics. fCalendar, fEcofin: v. 251.70
> >
> > Sincerely,
> >
> > -- Marco
> >
>
> Here is a reproducible example:
>
> --- [CUT HERE] ---
> library(fBasics);
> x <- c( 492408, 1897, 66795, 4014, 14, 11, 5770, 4284, 672, 50, 769,
> 21, 28, 13, 76, 29, 37, 28, 114, 63, 1479, 166, 999, 117, 8, 14, 206,
> 231, 85, 74684, 1130, 475, 657, 27, 230, 607, 1460, 116, 125, 2058,
> 3413, 69110, 20, 43, 406, 147, 314, 69, 310, 21, 1059, 986 );
> stableFit( x,type=c("mle"),doplot=F );
> --- [/CUT HERE] ---
>
> With the above corrections, the problem seems to be partially solved
> ... Partially because I found out if you specifiy 'doplot = TRUE', the
> same error happens in "qstable"  method:
>
> --- [CUT HERE] ---
> Error in qstable(0.01, alpha, beta) : object "result" not found
> --- [/CUT HERE] ---
>
> Note this error appears only if you have already corrected the
> "dstable" method; otherwise execution stops before.
> I've tried to fix it with the same kind of corrections done for
> "dstable" (added "result=NULL;" to line 767. However, I got a new
> error (in place of the above):
>
> --- [CUT HERE] ---
> Error in if (from == to) rep.int(from, length.out) else as.vector(c(from,  :
>         argument is of length zero
> --- [/CUT HERE] ---
>
> Let me know if there's a better solution or if I'm wrong.
>
> Thank you so much!
>
> Sincerely,
>
> -- Marco
>

Hmmm ... another error, this time in "pstable":

--- [CUT HERE] ---
Error in attr(ans, "control") = cbind.data.frame(dist = "stable",
alpha = alpha,  :
        attempt to set an attribute on NULL
--- [/CUT HERE] ---

To reproduce:
--- [CUT HERE] ---
library(fBasics);
x <- c( 492408, 1897, 66795, 4014, 14, 11, 5770, 4284, 672, 50, 769,
21, 28, 13, 76, 29, 37, 28, 114, 63, 1479, 166, 999, 117, 8, 14, 206,
231, 85, 74684, 1130, 475, 657, 27, 230, 607, 1460, 116, 125, 2058,
3413, 69110, 20, 43, 406, 147, 314, 69, 310, 21, 1059, 986 );
fit <- stableFit( x,type=c("mle"),doplot=F );
pstable( x, alpha=fit at fit$estimate["alpha"],
beta=fit at fit$estimate["beta"], gamma=fit at fit$estimate["gamma"],
delta=fit at fit$estimate["delta"] );
--- [/CUT HERE] ---

-- Marco


From ecjbosu at aol.com  Sun Aug 26 19:33:29 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Sun, 26 Aug 2007 12:33:29 -0500
Subject: [R-SIG-Finance] rcoplua.gauss sim problem
Message-ID: <fasdhl$o96$1@sea.gmane.org>

I am trying to replicate the copulae simulation from the SAS docs using 
gamlss, fCopulae, and copula

My problem is when I try and run rcopula.gauss I get an error message, 
sigma needs to be a correlation matrix in the test for sum(diag(sigma))=d.

No matter what I give rcopula.gauss, correlation matrix or cov matrix I 
get this error.  Even when I pass d=sum(diag(cop.fit$cor)) I get the 
error.  If I comment out the test for a corr matrix the simulation seems 
to work.

Any help is appreciated.

Thank you
Joe


require(VGAM)
require(QRMlib)
require(gamlss)
require(fCopulae)
require(copula)
require(date)

df<-7.5
sig1<-.5
var2<-2.5
seed<-1234
set.seed(seed);
#base on sas code from web site
#generate t distributed data
tdata<-data.frame(date=seq(as.numeric(as.date('06/01/2001')),as.numeric(as.date('11/01/2002')),1))
tdata$tdata<-.05*tdata$date+rTF(length(tdata$date),mu=5000,sigma=sig1,nu=df)

#estimate model data~a * date + constant
     fam<-TF
 
y1<-gamlss(tdata~date+1,data=tdata,family=fam,na.rm=T,nu.start=16.58,sigma.formula=~1
       ,nu.formula=~1)
     y1<-gamlss(tdata~date+1,data=tdata,family=TF(mu.link = 
identity,sigma.link = log, nu.link = log))
     plot(y1,na.rm=T)


  #generate normal garch distributed data
  set.seed(12345)
 
ndata<-data.frame(date=seq(as.numeric(as.date('06/01/2001')),as.numeric(as.date('11/01/2002')),1))
   ndata$rnd<-rnorm(length(ndata$date))
   ndata$v<-NaN
   ndata$e<-NaN
   ndata$v[1]<-.0001+.2*var2^2 +.75* var2
   ndata$e[1]<-sqrt(ndata$v[1])*ndata$rnd[1]
   for (i in 2:length(ndata$date) ) {
     ndata$v[i]<- 0.0001 + 0.2 * ndata$e[(i - 1)]^2 + 0.75 * ndata$v[(i 
- 1)]
     ndata$e[i]<-sqrt(ndata$v[i])+ndata$rnd[i]
   }
   # add constant 25
   ndata$ndata<-25+ndata$e
   # estimate garch 1,1 model of normally distributed data
   yn<-garchFit(ndata~garch(1,1),data=ndata)

  #y = rcauchy(519, loc=-4, scale=1)
  # generate cauchy data
  set.seed(seed);
 
cdata<-data.frame(date=seq(as.numeric(as.date('06/01/2001')),as.numeric(as.date('11/01/2002')),1))
  #cdata$cdata<-unlist(-4+tan(runif(length(cdata$date))-.5)*pi)
  cdata$cdata<-rcauchy(length(cdata$date),loc=-4,scale=1)
  #estimate cauchy model
  yc<-vglm(cdata ~ 1 , data=cdata,cauchy1(lloc="identity"), trace=TRUE, 
crit="c")



  #create transformed uniform residuals for all estimated series' residuals
 
resids<-data.frame(date=tdata$date,tresids=y1$residuals,nresids=yn at residuals,cresids=yc at residuals)
 
resids$tresids<-pnorm(pTF(resids$tresids/exp(y1$sigma.coefficient),y1$nu.coefficients))
  resids$cresids<-pnorm(pcauchy(resids$cresids,yc at coefficients[1]))
 
resids$nresids<-pnorm(resids$nresids,mean(resids$nresids),sd=sd(resids$nresids))
 
yall<-merge(tdata,merge(cdata,ndata,by.x="date",by.y="date",all=T),by.x="date",by.y="date",all=T)
 
rnds<-data.frame(rmvnorm(2000,sigma=cov(resids[-1]),mean=mean(resids[-1])))
  names(rnds)<-c('T',"Normal",'Cauchy')
#fit a normal copula to the uniform residuals
  cop.fit<-fit.norm(resids[,2:4])
#simulate the normal copula
# cop.sim1<-rmnorm(2000, Sigma=cop.fit$Sigma, mu=cop.fit$mu)


#************Here is my problem
  cop.sim<-rcopula.gauss(2000, Sigma=cop.fit$Sigma)

#*******




  #,d=sum(diag(cop.fit$cor)))#, mu=cop.fit$mu)
#transform the simulated correlated normal rv' to uncorrelated 
individual distributions
  sims<-data.frame(cop.sim)
  names(sims)<- c('T',"Normal",'Cauchy')
 
sims$T<-qTF(pnorm(sims$T),mu=y1$mu.coefficients[1],sigma=exp(y1$sigma.coefficients), 
nu=y1$nu.coefficients)
  sims$Normal<-qnorm(sims$Normal,mean(resids$nresids),sd=sd(resids$nresids))
  sims$Cauchy<-qcauchy(pnorm(sims$Cauchy),location=yc at coefficients[1])
 
sims$T<-seq(as.numeric(as.date('11/01/2002'))+1,as.numeric(as.date('11/01/2002'))+2000,1) 
*
    y1$mu.coefficients[2] + sims$T


# tt <- seq(0,10, len=21)                     cauchy1(lloc="loge")
#ncp <- seq(0,6, len=31)
#ptn <- outer(tt,ncp, function(t,d) pt(t, df = 3, ncp=d))
#image(tt,ncp,ptn, zlim=c(0,1),main=t.tit <- "Non-central t - 
Probabilities")
#persp(tt,ncp,ptn, zlim=0:1, r=2, phi=20, theta=200, main=t.tit,
#      xlab = "t", ylab = "non-centrality parameter", zlab = "Pr(T <= t)")


##rcopula.t<-function (n, df, Sigma = equicorr(d, rho), d = 2, rho = 0.7)
##{
##    d <- dim(Sigma)[1]
##    if (sum(diag(Sigma)) != d)
##        stop("Sigma should be correlation matrix")
##    tmp <- rmt(n, df, Sigma)
##    matrix(pt(tmp, df), ncol = d)
##}


#************ rcopula.gauss that works
rcopula.gauss<-function (n, Sigma = equicorr(d, rho), d = 2, rho = 0.7)
{
     d <- dim(Sigma)[1]
#    if (sum(diag(Sigma)) != d)
#        stop("Sigma should be correlation matrix")
     mnorm <- rmnorm(n, Sigma = Sigma)
     matrix(pnorm(mnorm), ncol = d)
}


From alexander.wurzer at fin4cast.com  Mon Aug 27 09:26:18 2007
From: alexander.wurzer at fin4cast.com (Alexander Wurzer)
Date: Mon, 27 Aug 2007 09:26:18 +0200
Subject: [R-SIG-Finance] Bloomberg
In-Reply-To: <20070824111124.GD19118@stats.ox.ac.uk>
Message-ID: <001401c7e87b$8f6b3de0$ad04d30a@backoffice>

Hi,

this is a really good solution for my problem!

Thanks a lot!

Alex

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Dan Davison
Sent: Friday, 24. August 2007 13:11
To: Thomas Steiner
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Bloomberg


On Thu, Aug 23, 2007 at 10:38:00PM +0200, Thomas Steiner wrote:
> > (1) http://commons.wikimedia.org/wiki/Image:Quantile_graph.png
> > The R code is included at the bottom of the page.
> 
> :) my image :))
> but it's really very "pseudo".

This approach does give a good pure R solution doesn't it? e.g. something
like

gradient.under.graph <- function(n=100, y0=0, mu=-0.7, sd=1, nrects=1000) {
    y <- y0 + cumsum(rnorm(n, mean=mu, sd=sd))
    plot(NA, xlim=c(1,n), ylim=range(y), bty="n")
    col <- colorRampPalette(colors=c("dark blue","light blue"))(nrects)
    incr <- (max(y) - min(y)) / nrects
    rect(0, seq(min(y), max(y)-incr, length=nrects), n, seq(min(y)+incr,
max(y), length=nrects), col=col, border=NA)
    polygon(x=c(1,1:n,n), y=c(max(y), y, max(y)), col="white", border=NULL)
}

is not slow.

Dan




> Thomas
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From spencer.graves at pdf.com  Wed Aug 29 19:51:03 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 29 Aug 2007 10:51:03 -0700
Subject: [R-SIG-Finance] =?iso-8859-1?q?UseR!_2008=2C_August_12-14=2C_Univ?=
 =?iso-8859-1?q?ersit=E4t_Dortmund=2C_Germany?=
Message-ID: <46D5B207.4080608@pdf.com>

      On behalf of the program committee for the UseR! conference next 
year, I'd like to invite you to mark your calendar for this event, 
consider attending and possibly making a presentation.  Important 
deadlines for the conference (from the web site 
"www.statistik.uni-dortmund.de/useR-2008" or www.r-project.org -> News:  
UseR! 2008) are as follows: 

      2007-10-31:  submission deadline for tutorial proposals

      December 2007:  start of registration and online abstract 
submission. 

      2008-03-31:  deadlines for abstract submission and early 
registration. 

      2008-05-15:  notification of acceptance of abstracts

      2008-05-31:  deadline for regular registration

      2008-07-25:  deadline for advanced registration (though on site 
registrations will still be accepted) 

      2008-08-11:  pre-conference tutorials

      2008-08-12 / 2008-08-14:  conference

      More later. 
      Best Wishes,
      Spencer Graves


From rich7804 at yahoo.com  Thu Aug 30 03:31:16 2007
From: rich7804 at yahoo.com (Rich Ghazarian)
Date: Wed, 29 Aug 2007 18:31:16 -0700 (PDT)
Subject: [R-SIG-Finance] garchFit()  with dummies
Message-ID: <111749.69237.qm@web30815.mail.mud.yahoo.com>

I am trying to add monthly dummies and a threshold parameter to the garchFit() function, and I have seen the following post by Diethelm Wuertz
   
   myGarch = function(data, p, q)
    {
        form = as.formula(paste("~garch(", p, ",", q, ")"))
        garchFit(formula.var = form, series = data)
    }
   
    myGarch(data = garchSim(n = 1000, rseed = 4711), p = 1, q = 1)


how can O introduce the following binary dummies to be estimated 
Dummies=wi,sp,su,fa,th




Thank you




       
____________________________________________________________________________________

Comedy with an Edge to see what's on, when.


From ravis at ambaresearch.com  Thu Aug 30 10:42:36 2007
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Thu, 30 Aug 2007 14:12:36 +0530
Subject: [R-SIG-Finance] Implied Probability Distribution
References: <A36876D3F8A5734FA84A4338135E7CC30252EE9B@BAN-MAILSRV03.Amba.com>
	<46CB9A61.9050208@earthlink.net>
Message-ID: <A36876D3F8A5734FA84A4338135E7CC30265973B@BAN-MAILSRV03.Amba.com>

Hi R users,

First let me thank Krishna and Anup.

Based on the suggestions I have given the r code along with the sample
data.

Sample data of the nifty index call options that I am using.

Date	Expiry	Strike	Price	spot	InterestRate	Time_Mat
6/28/2002	25/07/2002	1090	7.5	1057.8	0.0698
0.075396825
6/28/2002	25/07/2002	1120	2.25	1057.8	0.0698
0.075396825
6/28/2002	25/07/2002	1110	3.5	1057.8	0.0698
0.075396825
6/28/2002	25/07/2002	1100	5.65	1057.8	0.0698
0.075396825
6/28/2002	25/07/2002	1080	9.55	1057.8	0.0698
0.075396825
6/28/2002	25/07/2002	1070	13	1057.8	0.0698
0.075396825
6/28/2002	25/07/2002	1060	17.7	1057.8	0.0698
0.075396825
6/28/2002	25/07/2002	1050	23.35	1057.8	0.0698
0.075396825
6/28/2002	25/07/2002	1040	28.1	1057.8	0.0698
0.075396825
6/28/2002	25/07/2002	1020	45	1057.8	0.0698
0.075396825
6/28/2002	25/07/2002	1030	38.5	1057.8	0.0698
0.075396825

And the sample code that I am using.

indexdata<-read.table("clipboard",header=T)
n<-length(indexdata[,6])
impvol=delta=0
for(i in 1:n)
{
impvol[i]<-GBSVolatility(indexdata[i,4],"c",indexdata[i,5],indexdata[i,3
],indexdata[i,7],indexdata[i,6],0,0.0001,10000)
}
indexdata<-data.frame(indexdata,impvol)
for(i in 1:n)
{
 
delta[i]<-GBSGreeks("delta","c",indexdata[i,5],indexdata[i,3],indexdata[
i,7],indexdata[i,6],0,indexdata[i,8])
}
indexdata<-data.frame(indexdata,delta)
tt<-splinefun(indexdata$delta,indexdata$impvol,method="natural",ties="me
an")
splinecoef <- get("z", envir = environment(tt))

k<-indexdata$spot*exp(((indexdata$InterestRate)+0.5*(splinecoef$y)^2)*(i
ndexdata$Time_Mat)-(qnorm(splinecoef$x)*(splinecoef$y)*sqrt(indexdata$Ti
me_Mat)))
 call_price<-GBSOption(TypeFlag = "c", S = indexdata[1,5], X =k, Time
=indexdata[1,7], r =indexdata[1,6], b = 0, sigma =splinecoef$y)
hh<-data.frame(indexdata$Strike,indexdata$Price,indexdata$spot)
 mm<-data.frame(k,call_price at price,indexdata$spot)
final_data<-data.frame(rbind(as.matrix(hh),as.matrix(mm)))
final_data1<-final_data[order(final_data$indexdata.Strike),]
i=final_data1$indexdata.spot-final_data1$indexdata.Strike
final_data1<-data.frame(final_data1,i)
c2<-final_data1[(order(abs(final_data1[,4]))[1]),2]
c3<-final_data1[((order(abs(final_data1[,4]))[1])+1),2]
 c1<-final_data1[((order(abs(final_data1[,4]))[1])-1),2]
imp_prob<-exp(indexdata[1,6]*indexdata[1,7])*((c1+c3-(2*c2))/16)

Now I have the following question:

The differences between the strikes are not even. I have used (4)^2 in
computing imp_prob.

Please tell me if I am proceeding in the correct direction in my
computing of implied probability.

Thank you in advance for your response.
Regards,

Ravi Shankar S


-----Original Message-----
From: Krishna Kumar [mailto:kriskumar at earthlink.net] 
Sent: Wednesday, August 22, 2007 7:37 AM
To: Ravi S. Shankar
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Implied Probability Distribution


> Using non overlapping data of one month call option I have estimated
the
> implied volatility using the GBSVolatility of fOptions. Using these
> implied volatilities I have obtained delta using GBSGreeks of the same
> package. 
>
> Using the natural splines I have plotted the implied volatility and
> delta.
>
>  
>
> I have two questions (this might be trivial but would be glad for
> insight):
>
> 1) How do I extrapolate to obtain the tail?
>   
I am afraid there is no easy answer to this either you could flat 
forward extrapolate or use your fitted functional form/spline to 
extrapolate and obtain the vols for those strikes in the wings. 
Stability is the key and using something that is too flexible could 
sometimes hurt in this case.

> 2) How do I back out the option price? (We can use the Black- Scholes
> model (GBSOption of fOptions), however my understanding is it requires
> implied volatility and strike, but we have implied volatility and
delta)
>
>
>   
I couldn't find a version of the Breeden & Litzenberger paper but here 
is an easy read on this use eqn 3 on Page-4. 
http://www.bcb.gov.br/ingles/estabilidade/2002_nov/ref200201c62i.pdf

In essence you use a continuum of Call prices and take the 2nd 
derivative with respect to Strike(K) and for this take the centered 
difference
for starters there are some higher order differences that you can mess 
with later.

Also if you have the delta you can always map that to a strike K using 
the formula in http://www.mathfinance.org/formulas_u/Vanilla/node20.html
If you need further help it would help if you provide further 
information and possibly the data that you are using for this list to be

of further help.

Best
Krishna





>  
>
> Thank you in advance for the help.
>
> Regards,
>
> Ravi
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
>


From sankalp.upadhyay at gmail.com  Fri Aug 31 14:39:06 2007
From: sankalp.upadhyay at gmail.com (Sankalp Upadhyay)
Date: Fri, 31 Aug 2007 18:09:06 +0530
Subject: [R-SIG-Finance] Implied Probability Distribution
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC30265973B@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC30252EE9B@BAN-MAILSRV03.Amba.com>
	<46CB9A61.9050208@earthlink.net>
	<A36876D3F8A5734FA84A4338135E7CC30265973B@BAN-MAILSRV03.Amba.com>
Message-ID: <ff91746c0708310539u4cef0a61rd4fc3137cc4e87fc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070831/f14f1f3b/attachment.pl 

From mikekocurek at gmail.com  Fri Aug 31 20:32:42 2007
From: mikekocurek at gmail.com (Mike Kocurek)
Date: Fri, 31 Aug 2007 14:32:42 -0400
Subject: [R-SIG-Finance] fSeries GARCH Prediction Questions
Message-ID: <95f94a3d0708311132lccc8fbfjc34b5735af5b2a05@mail.gmail.com>

Hi,

I'm hoping to use the fSeries GARCH modeling to perform multi-period
predictions. However, the predict.fGARCH() function seems to be pretty
sparsely documented. It seems like it's only able to predict from the
end of the training data onwards, not based on new data that I
provide. For example, if I train it on daily data from, say, 1/1/1990
- 12/31/2005, calling predict() will give me predictions starting at
1/1/2006, with (if I'm reading the source right) all subsequent error
terms assumed to be zero. I'd like to be able to pass it an array of
new data to predict over, so that I can use the model to predict, say,
February of 2006 or June of 2000. This seems to be possible if you use
the tSeries library (via the newdata parameter), but not with the
fSeries library (which I need to use, since garchFit() is more robust
than garch()). Besides writing my own GARCH predictor, is there any
way to accomplish this with the provided code? This seems like a very
common thing to do, but I can't seem to find it anywhere. Any help
would be greatly appreciated.

Thanks,
-Mike


From brian at braverock.com  Sat Sep  1 04:09:24 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 31 Aug 2007 21:09:24 -0500
Subject: [R-SIG-Finance] fSeries GARCH Prediction Questions
In-Reply-To: <95f94a3d0708311132lccc8fbfjc34b5735af5b2a05@mail.gmail.com>
References: <95f94a3d0708311132lccc8fbfjc34b5735af5b2a05@mail.gmail.com>
Message-ID: <46D8C9D4.2000005@braverock.com>

Mike Kocurek wrote:
> I'm hoping to use the fSeries GARCH modeling to perform multi-period
> predictions. However, the predict.fGARCH() function seems to be pretty
> sparsely documented. It seems like it's only able to predict from the
> end of the training data onwards, not based on new data that I
> provide. For example, if I train it on daily data from, say, 1/1/1990
> - 12/31/2005, calling predict() will give me predictions starting at
> 1/1/2006, with (if I'm reading the source right) all subsequent error
> terms assumed to be zero. I'd like to be able to pass it an array of
> new data to predict over, so that I can use the model to predict, say,
> February of 2006 or June of 2000. This seems to be possible if you use
> the tSeries library (via the newdata parameter), but not with the
> fSeries library (which I need to use, since garchFit() is more robust
> than garch()). Besides writing my own GARCH predictor, is there any
> way to accomplish this with the provided code? This seems like a very
> common thing to do, but I can't seem to find it anywhere. Any help
> would be greatly appreciated.

It would be typical to train the model over a rolling window, and make a 
constant 'n' step ahead prediction, hopefully learning from the prior 
history.  As in real life, you 'in sample' period continues to grow, and 
you can see how well the model performs 'out of sample' 'n' steps ahead.

Try rollapply or one of its cousins, with whatever appropriate window.

Regards,

   - Brian


From gmain.20.phftt at xoxy.net  Mon Sep  3 15:41:54 2007
From: gmain.20.phftt at xoxy.net (Rob Steele)
Date: Mon, 03 Sep 2007 09:41:54 -0400
Subject: [R-SIG-Finance] Tabloid journalism using R !
In-Reply-To: <C39A02DFF9023744AC627E11F528C45B042CE0@SBKMXSMB04.win.dowjones.net>
References: <mailman.15.1188036006.2590.r-sig-finance@stat.math.ethz.ch>
	<C39A02DFF9023744AC627E11F528C45B042CE0@SBKMXSMB04.win.dowjones.net>
Message-ID: <46DC0F22.8050702@xoxy.net>

Very cool!  I love the expressions "the screaming, chair-throwing 
character that Cramer plays on TV" and "regulators have frisked Cramer's 
trading records for duplicity".  I also love the way you give him due 
credit for his many accomplishments.  Congratulations and 
congratulations to Pat Burns.  Nice work!

Rob


From ianseow at gmail.com  Tue Sep  4 02:20:37 2007
From: ianseow at gmail.com (Ian Seow)
Date: Tue, 4 Sep 2007 08:20:37 +0800
Subject: [R-SIG-Finance] Website authentication with R
Message-ID: <1865010709031720x3e4e6a90j45825302cae7483f@mail.gmail.com>

Hi guys, I'm currently running daily batch jobs to pull various data
from the web (using R CMD BATCH, in conjunction with cron jobs) and
then parsing them with R functions for stat analysis. However, I
recently encountered problems doing so on websites requiring
authentication. In Perl this would be done something like:
$ua->credentials( $netloc, $realm, $uname, $pass )..... Is there an
equivalent for this in R? I see that there is a HTTPUserAgent setting
in options( ), but unfortunately there's no username/password field.
Has anyone else encountered similar problems?

Thanks!

Best Regards
Ian Seow


From edd at debian.org  Tue Sep  4 02:51:31 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 3 Sep 2007 19:51:31 -0500
Subject: [R-SIG-Finance] Website authentication with R
In-Reply-To: <1865010709031720x3e4e6a90j45825302cae7483f@mail.gmail.com>
References: <1865010709031720x3e4e6a90j45825302cae7483f@mail.gmail.com>
Message-ID: <18140.44051.871662.425337@ron.nulle.part>


On 4 September 2007 at 08:20, Ian Seow wrote:
| Hi guys, I'm currently running daily batch jobs to pull various data
| from the web (using R CMD BATCH, in conjunction with cron jobs) and
| then parsing them with R functions for stat analysis. However, I
| recently encountered problems doing so on websites requiring
| authentication. In Perl this would be done something like:
| $ua->credentials( $netloc, $realm, $uname, $pass )..... Is there an
| equivalent for this in R? I see that there is a HTTPUserAgent setting
| in options( ), but unfortunately there's no username/password field.
| Has anyone else encountered similar problems?

While this is not really an r-sig-finance question, I can think of 

i)   Switch to using wget as your download agent; you can the 'drive' all
     authentication from wget and test it indepently.  See ?download.file 
     and esp. ?download.file.method

ii)  Try the RCurl package which provides all the goodness.

iii) If you can't get it done with R, use a first batch job to download
     with Perl and use R for subsequent analysis.


Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From Bernhard_Pfaff at fra.invesco.com  Tue Sep  4 09:53:58 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Tue, 4 Sep 2007 08:53:58 +0100
Subject: [R-SIG-Finance] Announcement: CRAN packages 'urca' and 'vars' on
	R-Forge
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C30ABF07ED@GBHENXMB02.corp.amvescap.net>

Dear useR,

the CRAN packages 'urca' and 'vars' are now hosted on R-Forge as
projects 'AICTS I' and 'AICTS II', respectively. The packages' summary
page can be directly accessed via:

AICTS I:
http://r-forge.r-project.org/projects/urca/

AICTS II:
http://r-forge.r-project.org/projects/vars/

All R-Forge features for both projects have been enabled. For more
information about R-Forge, visit: http://r-forge.r-project.org/


Best,
Bernhard
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From br44114 at gmail.com  Tue Sep  4 17:18:18 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 4 Sep 2007 11:18:18 -0400
Subject: [R-SIG-Finance] Diversification Comments...
Message-ID: <8d5a36350709040818j3cb664c0u8758d27cf580db3e@mail.gmail.com>

> When correlations converge on one and that lower barrier is touched
> long-term goals mean nothing as investing stops and liquidation
> starts. Many examples of this in the last two weeks news.

A nice article on the same theme:
http://www.economist.com/daily/columns/marketview/PrinterFriendly.cfm?story_id=9749127



> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of BBands
> Sent: Thursday, August 23, 2007 4:11 PM
> To: R-sig-finance
> Subject: Re: [R-SIG-Finance] Diversification Comments...
>
> Terminal wealth means nothing in the presence of large drawdowns.
>
> See Ralph Vince's new book for some consideration of the above idea.
>
> http://www.amazon.com/Handbook-Portfolio-Mathematics-Formulas-
> Allocation/dp/0471757683/
>
> "Risk is the probability of being ruined."
>
> "Ruin is touching or penetrating some lower barrier on your equity."
>
> When correlations converge on one and that lower barrier is touched
> long-term goals mean nothing as investing stops and liquidation
> starts. Many examples of this in the last two weeks news.
>
> Also of interest, the 23 August ISI Quantitative Research piece
> entitled "Market Direction and Sector Correlation".
>
>     jab
> --
> John Bollinger, CFA, CMT
> www.BollingerBands.com
>
> If you advance far enough, you arrive at the beginning.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jefe_goode at yahoo.com  Wed Sep  5 00:12:53 2007
From: jefe_goode at yahoo.com (jefe goode)
Date: Tue, 4 Sep 2007 15:12:53 -0700 (PDT)
Subject: [R-SIG-Finance] get.hist.quote() and Yahoo
Message-ID: <198730.58061.qm@web61024.mail.yahoo.com>

Hi 

I am using get.hist.quote() to download data from
Yahoo. But there are problems.

1) Many tickers eg III.L download cleanly with 100% of
the timeseries OK.

2) But for others eg HMSO.L the timeseries truncates
at 10-Aug for some reason. (should be data to today)

3) for yet others there are missing elements in the
timeseries

Why is this working for some tickers and not others?
PLEASE NOTE : ALL ticker symbols are VALID, and
checked on interactive Yahoo.finance graphs as having
data to date. ie This is a problem with
get.hist.quote() and/or Yahoo provision of data to
get.hist.quote()

eg Code below:

get.hist.quote(symbol, start    = "2007-08-01",
	               compress = "d")

Thanks in advance

Jefe


From nbryant at optonline.net  Wed Sep  5 00:24:31 2007
From: nbryant at optonline.net (Nathan Bryant)
Date: Tue, 04 Sep 2007 18:24:31 -0400
Subject: [R-SIG-Finance] get.hist.quote() and Yahoo
In-Reply-To: <198730.58061.qm@web61024.mail.yahoo.com>
References: <198730.58061.qm@web61024.mail.yahoo.com>
Message-ID: <46DDDB1F.9050002@optonline.net>

jefe goode wrote:
> Hi 
>
> I am using get.hist.quote() to download data from
> Yahoo. But there are problems.
>
> 1) Many tickers eg III.L download cleanly with 100% of
> the timeseries OK.
>
> 2) But for others eg HMSO.L the timeseries truncates
> at 10-Aug for some reason. (should be data to today)
>
> 3) for yet others there are missing elements in the
> timeseries
>   
There are bigger problems than this. You can't rely on Yahoo's "Adjusted 
Close" column. The formula they use to adjust for splits and dividends 
is wrong, so I'm told. Occasionally I've even seen them load their 
previously buggered "adjusted close" data into their "close" column, 
when reloading their database. This seems to only occur for certain 
ticker symbols and is probably a case of operator error...


From jeff.a.ryan at gmail.com  Wed Sep  5 00:49:27 2007
From: jeff.a.ryan at gmail.com (jeff.a.ryan at gmail.com)
Date: Tue, 4 Sep 2007 22:49:27 +0000
Subject: [R-SIG-Finance] get.hist.quote() and Yahoo
In-Reply-To: <46DDDB1F.9050002@optonline.net>
References: <198730.58061.qm@web61024.mail.yahoo.com><46DDDB1F.9050002@optonline.net>
Message-ID: <75921923-1188946248-cardhu_decombobulator_blackberry.rim.net-614106316-@bxe008.bisx.prod.on.blackberry>

I'd agree that yahoo data isn't great.  Google has a nice database that 'seems' to be adjusted throughout for splits (i.e. OHLC (V) is split adjusted throughout the series - no Adjusted column per se.)

The problem is with some dates - specifically Dec 29,30,31 of 2003 is returned as the 29,30,31 of 2004 - though wedged into Dec of 03.  Essentially those three dates are incorrect and missing.

You can try my new package quantmod (on CRAN) or from www.quantmod.com. Basically one function to download daily data from yahoo or google - getSymbols. Also can access Federal Reserve's FRED database.  

Still quite beta-like, but feedback is greatly desired (as to quality and future direction.)

Jeff Ryan


Sent via BlackBerry from T-Mobile

-----Original Message-----
From: Nathan Bryant <nbryant at optonline.net>

Date: Tue, 04 Sep 2007 18:24:31 
To:jefe goode <jefe_goode at yahoo.com>
Cc:r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] get.hist.quote() and Yahoo


jefe goode wrote:
> Hi 
>
> I am using get.hist.quote() to download data from
> Yahoo. But there are problems.
>
> 1) Many tickers eg III.L download cleanly with 100% of
> the timeseries OK.
>
> 2) But for others eg HMSO.L the timeseries truncates
> at 10-Aug for some reason. (should be data to today)
>
> 3) for yet others there are missing elements in the
> timeseries
>   
There are bigger problems than this. You can't rely on Yahoo's "Adjusted 
Close" column. The formula they use to adjust for splits and dividends 
is wrong, so I'm told. Occasionally I've even seen them load their 
previously buggered "adjusted close" data into their "close" column, 
when reloading their database. This seems to only occur for certain 
ticker symbols and is probably a case of operator error...

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From sergio.correia at gmail.com  Wed Sep  5 05:58:58 2007
From: sergio.correia at gmail.com (Sergio Correia)
Date: Tue, 4 Sep 2007 22:58:58 -0500
Subject: [R-SIG-Finance] get.hist.quote() and Yahoo
In-Reply-To: <75921923-1188946248-cardhu_decombobulator_blackberry.rim.net-614106316-@bxe008.bisx.prod.on.blackberry>
References: <198730.58061.qm@web61024.mail.yahoo.com>
	<46DDDB1F.9050002@optonline.net>
	<75921923-1188946248-cardhu_decombobulator_blackberry.rim.net-614106316-@bxe008.bisx.prod.on.blackberry>
Message-ID: <f1aa6d810709042058x1232d46bm593e8f95d10501e7@mail.gmail.com>

About the Yahoo data, the summary page is ok but there is something
*wrong* with the historical data:

Summary: http://finance.yahoo.com/q?s=HMSO.L
Historical: http://finance.yahoo.com/q/hp?s=HMSO.L

I loaded the historical data and the last date was Aug 10, 2007. I
reloaded and now the last date is Aug 31, 2007. Strange indeed.

(Update, I just reloaded again and now the last date is 24-Aug-07 !!!)

About the adjusted close data.. is there a way to use the
dividend/split historical data to correctly generate the adjusted
close information? Anyone has more info about that?

Also, are there other sources available? I think google does not allow
downloading to CSV and doesn't have much data (SP500 index only since
2004).

I've been using python to download the data and interact with the DB,
but getSymbols seems very interesting, great job Jeff. I'll send you
my feedback.

Thanks,

Sergio

On 9/4/07, jeff.a.ryan at gmail.com <jeff.a.ryan at gmail.com> wrote:
> I'd agree that yahoo data isn't great.  Google has a nice database that 'seems' to be adjusted throughout for splits (i.e. OHLC (V) is split adjusted throughout the series - no Adjusted column per se.)
>
> The problem is with some dates - specifically Dec 29,30,31 of 2003 is returned as the 29,30,31 of 2004 - though wedged into Dec of 03.  Essentially those three dates are incorrect and missing.
>
> You can try my new package quantmod (on CRAN) or from www.quantmod.com. Basically one function to download daily data from yahoo or google - getSymbols. Also can access Federal Reserve's FRED database.
>
> Still quite beta-like, but feedback is greatly desired (as to quality and future direction.)
>
> Jeff Ryan
>
>
> Sent via BlackBerry from T-Mobile
>
> -----Original Message-----
> From: Nathan Bryant <nbryant at optonline.net>
>
> Date: Tue, 04 Sep 2007 18:24:31
> To:jefe goode <jefe_goode at yahoo.com>
> Cc:r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] get.hist.quote() and Yahoo
>
>
> jefe goode wrote:
> > Hi
> >
> > I am using get.hist.quote() to download data from
> > Yahoo. But there are problems.
> >
> > 1) Many tickers eg III.L download cleanly with 100% of
> > the timeseries OK.
> >
> > 2) But for others eg HMSO.L the timeseries truncates
> > at 10-Aug for some reason. (should be data to today)
> >
> > 3) for yet others there are missing elements in the
> > timeseries
> >
> There are bigger problems than this. You can't rely on Yahoo's "Adjusted
> Close" column. The formula they use to adjust for splits and dividends
> is wrong, so I'm told. Occasionally I've even seen them load their
> previously buggered "adjusted close" data into their "close" column,
> when reloading their database. This seems to only occur for certain
> ticker symbols and is probably a case of operator error...
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From alexander.f.moreno at gmail.com  Wed Sep  5 21:12:35 2007
From: alexander.f.moreno at gmail.com (Alexander Moreno)
Date: Wed, 5 Sep 2007 14:12:35 -0500
Subject: [R-SIG-Finance] money market rates download: get.hist.quote
Message-ID: <3303a4570709051212s218a8b5o4406e569d2b140f7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070905/57942d3f/attachment.pl 

From josh at gghc.com  Wed Sep  5 21:43:11 2007
From: josh at gghc.com (Joshua Reich)
Date: Wed, 5 Sep 2007 15:43:11 -0400
Subject: [R-SIG-Finance] Job posting
In-Reply-To: <3303a4570709051212s218a8b5o4406e569d2b140f7@mail.gmail.com>
References: <3303a4570709051212s218a8b5o4406e569d2b140f7@mail.gmail.com>
Message-ID: <C20EA84D76C94F4E999DC041E81C0D110256EBB0@exchange2k3.ny.gghc.com>

Hi List,

We've been looking to fill the position below for a few weeks now and I
figured that this list might be a good source of smart cookies. 
 
We are a small research group attached to an established asset manager
with over 40 years in business, and we need more super smart programmers
and innovative thinkers to fill positions in our Manhattan office. Our
firm follows a fundamental, growth-oriented approach to investment and
we manage over $5bn in global equity positions. 

As a member of our small, self-directed team, it will be part of your
job to come up with interesting projects that you think would benefit
the firm. For example, some things we've been working on include
applying data mining to online auctions to monitor consumer demand,
using hidden Markov models to infer stock liquidity, and performing text
analysis on corporate filings for fine-grained industry classification.
We develop and prototype in a pragmatic mix of Perl, PHP, C, R/S-Plus,
Matlab/Octave and Java. We don't really care what language you code in
as we believe smart coders are flexible and are best left to choose
their own tools. 

You will be expected to work well in a loosely structured environment,
capable of setting and working towards your own goals. The majority of
your time will be spent on long range projects, with approximately 20%
spent on helping portfolio managers analyze investment decisions. 

You should be intimately familiar with database design, web development,
Linux and Windows. A decent background, academic or practical, in
statistics or data mining is a plus. You should believe that running
code speaks louder than design documents and that attracting a user base
speaks loudest of all.  Solid communication skills are essential as you
may find yourself writing research reports or giving presentations to
portfolio managers. 

We offer competitive starting salaries, excellent benefits, an informal
environment, and, most importantly, interesting problems to solve.

Although we hold a general fondness for robots, we don't like robotic
applicants. Please *distinguish yourself* with a cover letter or subject
that includes an interesting or quirky fact about mathematics, computer
science, or yourself!

Thanks,

Josh


From jeff.a.ryan at gmail.com  Wed Sep  5 21:42:27 2007
From: jeff.a.ryan at gmail.com (jeff.a.ryan at gmail.com)
Date: Wed, 5 Sep 2007 19:42:27 +0000
Subject: [R-SIG-Finance] money market rates download: get.hist.quote
In-Reply-To: <3303a4570709051212s218a8b5o4406e569d2b140f7@mail.gmail.com>
References: <3303a4570709051212s218a8b5o4406e569d2b140f7@mail.gmail.com>
Message-ID: <1669736236-1189021428-cardhu_decombobulator_blackberry.rim.net-1035901967-@bxe008.bisx.prod.on.blackberry>

Look at package:quantmod - www.quantmod.com, specifically getSymbols.FRED. Single command access to about 11000 macro series from the Federal Reserve.  Not 100% what your aiming for - but if you know a source that is readliy parsable - tell me - I'll make getSymbols work for it.

Rmetrics had some economagic access - but I have found economagic to be more than a little irratating as you need to parse out the data from nice HTML.

FRED is free and nice.

Jeff
Sent via BlackBerry from T-Mobile

-----Original Message-----
From: "Alexander Moreno" <alexander.f.moreno at gmail.com>

Date: Wed, 5 Sep 2007 14:12:35 
To:r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] money market rates download: get.hist.quote


Hi,

I'm wondering how I can download historical money market rates, i.e. libor,
sovereign debt (from a variety of countries) directly into R using the
get.hist.quote or other function.  Any help would be much appreciated.

Regards,
Alex

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From ecjbosu at aol.com  Thu Sep  6 12:29:31 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Thu, 06 Sep 2007 05:29:31 -0500
Subject: [R-SIG-Finance] rcoplua.gauss sim problem
Message-ID: <46DFD68B.7060709@aol.com>

Rich,

I may not be the best for answering your questions, but will try. 
garchFit may allow exogenous variable in the mean function thru the 
parameter xreg (see arima and arimaFit for more information).  The 
variance equation in garchFit, to my knowledge, is limited to garch/arch 
variance specification based on references in the help files.

I might recommend gamlss package for your question.  It allows 
specifications of exogenous variables in the variance equations and will 
allow you to fit many different distributions to your data.  It provides 
AIC stats for model specification tests.  I used gamlss in the example.

Also, be patient with rmetrics, they will respond.  I am still waiting 
on help with the rcopula question.

Good luck
Joe


Rich Ghazarian wrote:
 > Joe,
 >
 > I am new to R and enjoy reading your posting on [R-SIG-Finance] .  I 
am looking at your code and my limited ability is not allowing me to 
help you with your problem, but I find it an excellent learning tool.  I 
have a question related to a GARCH estimation problem, looking at your 
code I am trying to figure out if it is possible to impose seasonality 
into the garchFit() or any function.  I would like monthly dummies to 
enter the garch process via alpha,.gamma, and  beta.   I know this is 
done in Splus, but can't seem to figure it out in  R.  Given your 
experience in R and Energy markets, I am hoping that you might have 
encountered such problem in the past.
 >
 >  I have tried to contact the owner of rmetrics, but I have not heard 
anything.   Your help is much appreciated
 >
 > Best,
 >
 > Rich Ghazarian
 >
 >
 >
 >
 >
 >
 > 

From josh.m.ulrich at gmail.com  Thu Sep  6 16:21:53 2007
From: josh.m.ulrich at gmail.com (josh.m.ulrich at gmail.com)
Date: Thu, 6 Sep 2007 09:21:53 -0500
Subject: [R-SIG-Finance] get.hist.quote() and Yahoo
In-Reply-To: <f1aa6d810709042058x1232d46bm593e8f95d10501e7@mail.gmail.com>
References: <198730.58061.qm@web61024.mail.yahoo.com>
	<46DDDB1F.9050002@optonline.net>
	<75921923-1188946248-cardhu_decombobulator_blackberry.rim.net-614106316-@bxe008.bisx.prod.on.blackberry>
	<f1aa6d810709042058x1232d46bm593e8f95d10501e7@mail.gmail.com>
Message-ID: <8cca69990709060721v2032400ct327cad5ac3de43fd@mail.gmail.com>

On 9/4/07, Sergio Correia <sergio.correia at gmail.com> wrote:
>
> About the adjusted close data.. is there a way to use the
> dividend/split historical data to correctly generate the adjusted
> close information? Anyone has more info about that?

I have been working on a technical analysis package (TTR).  It has a
function ('yahoo.data') to pull OHLC, volume, split, and dividend
information from Yahoo! Finance and to correctly adjust the OHLC and
volume data for splits and dividends.  Comments and suggestions are
welcome.  You can find the package here:
http://bodanker.t35.com/ttr.html

Best,
Josh


From bbands at gmail.com  Thu Sep  6 21:58:11 2007
From: bbands at gmail.com (BBands)
Date: Thu, 6 Sep 2007 12:58:11 -0700
Subject: [R-SIG-Finance] get.hist.quote() and Yahoo
In-Reply-To: <8cca69990709060721v2032400ct327cad5ac3de43fd@mail.gmail.com>
References: <198730.58061.qm@web61024.mail.yahoo.com>
	<46DDDB1F.9050002@optonline.net>
	<75921923-1188946248-cardhu_decombobulator_blackberry.rim.net-614106316-@bxe008.bisx.prod.on.blackberry>
	<f1aa6d810709042058x1232d46bm593e8f95d10501e7@mail.gmail.com>
	<8cca69990709060721v2032400ct327cad5ac3de43fd@mail.gmail.com>
Message-ID: <6e8360ad0709061258m47ff2617y70a3b34c1bf5c4bd@mail.gmail.com>

On 9/6/07, josh.m.ulrich at gmail.com <josh.m.ulrich at gmail.com> wrote:
> I have been working on a technical analysis package (TTR).  It has a
> function ('yahoo.data') to pull OHLC, volume, split, and dividend
> information from Yahoo! Finance and to correctly adjust the OHLC and
> volume data for splits and dividends.  Comments and suggestions are
> welcome.  You can find the package here:
> http://bodanker.t35.com/ttr.html

That's a nice start. I'll do some checking for you when I get a chance.

What are your plans?

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From johnputz3655 at yahoo.com  Thu Sep  6 22:43:52 2007
From: johnputz3655 at yahoo.com (John Putz)
Date: Thu, 6 Sep 2007 13:43:52 -0700 (PDT)
Subject: [R-SIG-Finance] holidayNYSE missing some
Message-ID: <328548.67997.qm@web50706.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070906/bfbe0add/attachment.pl 

From Charles.Naylor at nikkoam.com  Thu Sep  6 22:51:49 2007
From: Charles.Naylor at nikkoam.com (Charles Naylor)
Date: Thu, 6 Sep 2007 16:51:49 -0400
Subject: [R-SIG-Finance] holidayNYSE missing some
In-Reply-To: <328548.67997.qm@web50706.mail.re2.yahoo.com>
References: <328548.67997.qm@web50706.mail.re2.yahoo.com>
Message-ID: <A4678959B3D65D449266DE8D3825E0821F7F47@nycmsg501.nikkoam.com>

This is deliberate behavior.  If you check the code, the third-to-last
line is as follows:
	ans = ans[!(as.POSIXlt(ans at Data)$wday == 0 |
as.POSIXlt(ans at Data)$wday== 6)]


You could make an alternate version of holidayNYSE that omits this line,
if you like.

-CN

Charles Naylor
Assistant Vice President
Global Dynamic Asset Allocation Group
Nikko Alternative Asset Management, Inc.
535 Madison Avenue, Suite 2500
New York, NY 10022
T: 212.610.6158
F: 212.610.6148
E: charles.naylor at nikkoam.com

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of John Putz
Sent: Thursday, September 06, 2007 4:44 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] holidayNYSE missing some

Hello,

I'm not sure if this is the correct list to email this too, but it
appears that at least in R 2.5.0 that the holidayNYSE function in
fCalendar does not include holidays that occur on Saturday.  E.g.
holidayNYSE(2004) does not list Christmas.

Thanks, John.
	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

IMPORTANT NOTICE:  The information in this message (and any attachments) is intended for the addressee(s) only and may contain confidential and/or privileged information; any unauthorized copying, disclosure, distribution or use is strictly forbidden.  If you are not the intended recipient, or have received this e-mail in error, please notify the sender immediately and destroy this e-mail.  E-mail transmissions are not secure and may contain viruses; we accept no liability for viruses, errors in transmission, delayed transmission, or other transmission-related errors.   This e-mail is neither a recommendation nor a solicitation of an offer to buy or sell securities or other financial instruments.


From johnputz3655 at yahoo.com  Fri Sep  7 16:20:57 2007
From: johnputz3655 at yahoo.com (John Putz)
Date: Fri, 7 Sep 2007 07:20:57 -0700 (PDT)
Subject: [R-SIG-Finance] holidayNYSE missing some
Message-ID: <719928.15491.qm@web50703.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070907/aa89b857/attachment.pl 

From peter at braverock.com  Sat Sep  8 04:16:30 2007
From: peter at braverock.com (Peter Carl)
Date: Fri, 7 Sep 2007 21:16:30 -0500
Subject: [R-SIG-Finance] Getmansky et al. Smoothing Index
Message-ID: <200709072116.30628.peter@braverock.com>

I am working on implementing a measure for evaluating the relative amount of 
serial correlation caused by smoothing in a return series as described in:

Getmansky, M., A. W. Lo, and I. Makarov. ?An Econometric Model of Serial 
Correlation and Illiquidity in Hedge Fund Returns.? Journal of Financial 
Economics 74 (2004), 529-609.

In that paper, the authors argue that there are three possible 
sources of serial correlation in hedge fund returns: time-varying expected 
returns, time-varying leverage and incentive fees with high-water marks. ?
They carefully go through all three to argue that none of these can 
effectively explain the high levels of observed serial correlation in the 
context of hedge funds. ?With that, they turn their focus towards the 
combination of illiquidity and smoothed returns. 

The remainder of the paper argues that serial correlation can be considered a 
proxy for illiquidity and return smoothing. Even though illiquidity and 
smoothing are two distinct phenomena, they argue to consider them together 
since one facilitates the other. ?The basic arguement goes that 
return-smoothing behavior yields a more consistent set of returns over time, 
with lower volatility and, therefore, a higher Sharpe ratio, but it also 
produces serial correlation as a side effect. ?Part of the motivation here is 
that such a measure would give us a way to compare the relative smoothing 
among our managers.

To measure and alleviate the effects of smoothing, they offer a rather 
complicated solution. ?The first part involves estimating the smoothing 
profile using maximum likelihood estimation (MLE) in a fashion similar to the 
estimation of standard moving-average time series models. ?They define 
a "smoothing profile" as a vector of coefficients for an MLE fit on returns 
using a two-period moving-average process. ?The coefficients, ?j, are then 
normalized to sum to interpreted as a "weighted average of the fund?s true 
returns over the most recent k + 1 periods, including the current period." ?
In other words, the "information generated at date t may not be fully 
impounded into prices until several periods later." ?If the first coefficient 
(?0) was 0.719, it would imply that only 71.9% of that fund?s true current 
monthly return was reported, with the remaining 28.1% distributed over the 
next two months (recall the constraint that ?0 + ?1 + ?2 = 1). The estimates 
0.201 and 0.080 for ?1 and ?2 imply that on average, the current reported 
return also includes 20% of last months true return and 8% of the previous 
month's return.

The measure probably does capture some essence of serial correlation from a 
return series. ?If these weights are disproportionately centered on a small 
number of lags, relatively little serial correlation will be induced. 
However, if the weights are evenly distributed among many lags, this would 
show higher serial correlation. ?The Herfindahl Index was originally 
developed to measure concentration of manufacturers or suppliers in a 
marketplace, using market share of member companies in an industry -- and has 
very little to do with the measure. ?Getmansky, et al. simply use it to scale 
the coefficients, or "smoothing profile", into a single number, or "smoothing 
index". ?In the context of smoothed returns, a lower value of the smoothing 
index implies more smoothing, and the upper bound of 1 implies no smoothing. 

There are a number of issues for implementers lurking in their methodology. ?
The first and probably most obvious issue comes from fitting a model to the 
returns series. ?The methodology proposed is difficult to understand and 
implement correctly. ?Fortunately, there are functions in most popular 
statistics packages that can fit such a model. ?There are variations in 
exactly how those algorithms are implemented that may cause the results to be 
difficult to repeat exactly. ?But, for the moment, let's pretend I found 
something that comes close to their methodology to use. ?

In my tests, the smoothing index that I calculate is not particularly stable 
through time. ?When measured over a 36- or 60-month rolling window, values 
wiggle through regions where you might expect and then suddenly spike. ?Those 
spikes don't mean that the manager suddenly found a pool of liquidity, or was 
on vacation for a few months and couldn't smooth the returns - they mean that 
the model was mis-specified and the measure isn't valid through that period.

Getmansky, et al. comment on the possibility of mis-specification, noting that 
the smoothing index "does not always perform well in small samples or when 
the underlying distribution of true returns is not normal as hypothesized." ?
They offer three tests for specification: Did the fit converge? Are all of 
the estimated smoothing coefficients positive? and Is it wildly different 
than the estimates from a linear regression approach (which I didn't 
implement)?

The second issue is that they don't normalize the fit coeficients to [0,1], so 
the resulting 'smoothing index' is not limited to that range either. ?As a 
result, all we can say is that lower values are "less liquid" and higher 
values are "more liquid" or mis-specified.

This group also wrote a second paper that updated the observations of the 
first. ?"Systemic Risk and Hedge Funds," by Nicholas Chan, Mila Getmansky, 
Shane M. Haas, and Andrew W. Lo, which was published as an NBER Working Paper 
(No. 11200) in March 2005. ?I would note that their reported experience with 
this measure seems much more consistent than mine, which suggests that the 
fitting methodology I'm using is incorrect or more prone to 
mis-specification.

My current draft of the code is attached below.  I'm using the arima() 
function to fit an MA(2) model as follows:

MA2=arima(ra, order=c(0,0,2), method="ML", transform.pars=TRUE, 
include.mean=FALSE)

I'm still scratching my head about whether I'm doing this correctly.  I've 
noticed that the fits are very unstable through time (which makes sense, 
given the normality assumption buried in here), but that would limit it's 
utility.  I've noticed that if I extend the model to order=c(0,0,3) it helps 
some, but not a lot.

Three questions:
- Am I using the arima fit function correctly?
- Has someone else implemented this with more rigor?
- Has anyone else found this to be a useful measure?

Thanks in advance,

pcc

`SmoothingIndex` <-
function (ra, ...)
{ # @author Peter Carl

    # Description:
    # SmoothingIndex

    # ra    log return vector

    # Function:

    ra = checkData(ra, method="vector", na.omit=TRUE)

    MA2 = NULL
    thetas = 0
    SmoothingIndex = 0

    # First, create a a maximum likelihood estimation fit for an MA process.

    # include.mean: Getmansky, et al. JFE 2004 p 555 "By applying the above
    # procedure to observed de-meaned returns...", so set parameter to FALSE
    # transform.pars: ibid, "we impose the additional restriction that the   
    # estimated MA(k)  process be invertible." so set the parameter to TRUE
    MA2 = arima(ra, order=c(0,0,2), method="ML", transform.pars=TRUE, 
include.mean=FALSE)

    thetas = as.numeric((MA2$coef)/sum(MA2$coef))

    SmoothingIndex = sum(thetas^2) 

    return(SmoothingIndex)

}
-- 
Peter Carl


From Jens.Fricke1 at gmx.de  Sun Sep  9 21:19:59 2007
From: Jens.Fricke1 at gmx.de (Jens Fricke)
Date: Sun, 9 Sep 2007 21:19:59 +0200
Subject: [R-SIG-Finance] GARCH with variance targeting, stable distribution
Message-ID: <98C7DBAD-565D-460A-A0D9-70C72A42DB00@gmx.de>

Dear All,

I am searching for a possibility to use the garch estimation with  
variance targeting, i.e. first estimate the parameter a_0 in a garch  
model (h_i=a_0+a_1*r_i-1^2+b_1*h_i-1) based on the unconditional  
variance sigma^2: a_0=sigma^2*(1-a_1-b_1). Then, in a second step,  
optimize the other parameters a_1, b_1. Is there a way to use the  
"garch" routines or to incorporate that into the current functions?

In addition, are there possibilities to estimate GARCH models with  
assumed stable distribution?

Thank you very much for your help.

Best regards,

Jens


From patrick at burns-stat.com  Mon Sep 10 11:18:40 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon, 10 Sep 2007 10:18:40 +0100
Subject: [R-SIG-Finance] GARCH with variance targeting,
	stable distribution
In-Reply-To: <98C7DBAD-565D-460A-A0D9-70C72A42DB00@gmx.de>
References: <98C7DBAD-565D-460A-A0D9-70C72A42DB00@gmx.de>
Message-ID: <46E50BF0.7050508@burns-stat.com>

In answer to the second question, stable distributions
have infinite variance (except for the Gaussian), so garch
assuming a stable distribution is not particularly interesting.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Jens Fricke wrote:

>Dear All,
>
>I am searching for a possibility to use the garch estimation with  
>variance targeting, i.e. first estimate the parameter a_0 in a garch  
>model (h_i=a_0+a_1*r_i-1^2+b_1*h_i-1) based on the unconditional  
>variance sigma^2: a_0=sigma^2*(1-a_1-b_1). Then, in a second step,  
>optimize the other parameters a_1, b_1. Is there a way to use the  
>"garch" routines or to incorporate that into the current functions?
>
>In addition, are there possibilities to estimate GARCH models with  
>assumed stable distribution?
>
>Thank you very much for your help.
>
>Best regards,
>
>Jens
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From fontana at unive.it  Thu Sep 13 21:36:00 2007
From: fontana at unive.it (fontana at unive.it)
Date: Thu, 13 Sep 2007 21:36:00 +0200 (CEST)
Subject: [R-SIG-Finance] r beginner..I need to
Message-ID: <49294.136.167.255.127.1189712160.squirrel@helios1.unive.it>

 I am a Matlab beginner and I need to estimate the term structure of
interest rate susing Nelson and Siegel 1987 approach.
I am wondering if someone has a reference or an file for the topic because
I don't know how to start or how to proceed.
Any kind of help would be greatly appreciated
Best regards
-- 
Alessandro Fontana
PhD student in Economics and Organisation (SSAV)
Venice International University
Isola di San Servolo
30100 Venice
Italy
email: fontana at unive.it


From fontana at unive.it  Thu Sep 13 21:44:11 2007
From: fontana at unive.it (fontana at unive.it)
Date: Thu, 13 Sep 2007 21:44:11 +0200 (CEST)
Subject: [R-SIG-Finance] [Fwd: r beginner..I need to]
Message-ID: <49309.136.167.255.127.1189712651.squirrel@helios1.unive.it>



-------------------------- Messaggio originale ---------------------------
Oggetto: r beginner..I need to
Da:      fontana at unive.it
Data:    Gio, 13 Settembre 2007 9:36 pm
A:       r-sig-finance at stat.math.ethz.ch
--------------------------------------------------------------------------

 I am a R   (not Matlab...sorry for the mistake) beginner and I need to
estimate the term structure of
interest rate susing Nelson and Siegel 1987 approach.
I am wondering if someone has a reference or an file for the topic because
I don't know how to start or how to proceed.
Any kind of help would be greatly appreciated
Best regards
-- 
Alessandro Fontana
PhD student in Economics and Organisation (SSAV)
Venice International University
Isola di San Servolo
30100 Venice
Italy
email: fontana at unive.it



-- 
Alessandro Fontana
PhD student in Economics and Organisation (SSAV)
Venice International University
Isola di San Servolo
30100 Venice
Italy
email: fontana at unive.it


From jeff.a.ryan at gmail.com  Thu Sep 13 22:03:08 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 13 Sep 2007 15:03:08 -0500
Subject: [R-SIG-Finance] [Fwd: r beginner..I need to]
In-Reply-To: <49309.136.167.255.127.1189712651.squirrel@helios1.unive.it>
References: <49309.136.167.255.127.1189712651.squirrel@helios1.unive.it>
Message-ID: <e8e755250709131303l7dddf9fdx5c67f768cf5d8ee9@mail.gmail.com>

A quick google of "Nelson and Siegel R" yields:

https://stat.ethz.ch/pipermail/r-sig-finance/2006q1/000631.html

Which seems quite comprehensive, and fairly easy to find.

Jeff Ryan

On 9/13/07, fontana at unive.it <fontana at unive.it> wrote:
>
>
> -------------------------- Messaggio originale ---------------------------
> Oggetto: r beginner..I need to
> Da:      fontana at unive.it
> Data:    Gio, 13 Settembre 2007 9:36 pm
> A:       r-sig-finance at stat.math.ethz.ch
> --------------------------------------------------------------------------
>
>  I am a R   (not Matlab...sorry for the mistake) beginner and I need to
> estimate the term structure of
> interest rate susing Nelson and Siegel 1987 approach.
> I am wondering if someone has a reference or an file for the topic because
> I don't know how to start or how to proceed.
> Any kind of help would be greatly appreciated
> Best regards
> --
> Alessandro Fontana
> PhD student in Economics and Organisation (SSAV)
> Venice International University
> Isola di San Servolo
> 30100 Venice
> Italy
> email: fontana at unive.it
>
>
>
> --
> Alessandro Fontana
> PhD student in Economics and Organisation (SSAV)
> Venice International University
> Isola di San Servolo
> 30100 Venice
> Italy
> email: fontana at unive.it
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Fri Sep 14 07:03:08 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 14 Sep 2007 00:03:08 -0500
Subject: [R-SIG-Finance] r beginner..I need to
In-Reply-To: <49294.136.167.255.127.1189712160.squirrel@helios1.unive.it>
References: <49294.136.167.255.127.1189712160.squirrel@helios1.unive.it>
Message-ID: <e8e755250709132203l59fb4bb8y7765ce8852263bc5@mail.gmail.com>

Even better:

http://cran.r-project.org/src/contrib/Descriptions/termstrc.html



On 9/13/07, fontana at unive.it <fontana at unive.it> wrote:
>  I am a Matlab beginner and I need to estimate the term structure of
> interest rate susing Nelson and Siegel 1987 approach.
> I am wondering if someone has a reference or an file for the topic because
> I don't know how to start or how to proceed.
> Any kind of help would be greatly appreciated
> Best regards
> --
> Alessandro Fontana
> PhD student in Economics and Organisation (SSAV)
> Venice International University
> Isola di San Servolo
> 30100 Venice
> Italy
> email: fontana at unive.it
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From alexander.f.moreno at gmail.com  Sat Sep 15 07:43:30 2007
From: alexander.f.moreno at gmail.com (Alexander Moreno)
Date: Sat, 15 Sep 2007 00:43:30 -0500
Subject: [R-SIG-Finance] portfolio optimization using higher moments
Message-ID: <3303a4570709142243u33640d12kf9f8e8a29bd6b632@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070915/57e3c4fa/attachment.pl 

From brian at braverock.com  Sat Sep 15 12:34:14 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Sat, 15 Sep 2007 05:34:14 -0500
Subject: [R-SIG-Finance] portfolio optimization using higher moments
In-Reply-To: <3303a4570709142243u33640d12kf9f8e8a29bd6b632@mail.gmail.com>
References: <3303a4570709142243u33640d12kf9f8e8a29bd6b632@mail.gmail.com>
Message-ID: <46EBB526.8020108@braverock.com>

Alexander Moreno wrote:
> Is there any R canned package which will optimize a portfolio using all four
> moments?  Or are LPM, CVaR, and VaR optimization the best one can do in R (
> i.e. no Kurtosis)?

There are multiple different methods which have been proposed for 
optimizing a portfolio using higher moments.  Unfortunately, most of 
them are not amenable to an analytical solution, so you are down to some 
sort of sampling method, possibly with hill climbing, or brute force 
with a large number of processors.

Is there a particular paper that you are trying to replicate?

This is one of my current areas of research. I would be interested in 
collaborating if you are also working on this.

Regards,

   - Brian


From kshimbo at gmail.com  Mon Sep 17 05:17:16 2007
From: kshimbo at gmail.com (Kazuhiro Shimbo)
Date: Sun, 16 Sep 2007 23:17:16 -0400
Subject: [R-SIG-Finance] Candlestick chart
Message-ID: <f8cc95220709162017o6a917d58kf6c11e1ae0f1c7c6@mail.gmail.com>

Is there a function or coded example of drawing candlestick chart
based on open,high,low,and close?

Thanks,


From sf at metrak.com  Mon Sep 17 14:40:29 2007
From: sf at metrak.com (paul sorenson)
Date: Mon, 17 Sep 2007 22:40:29 +1000
Subject: [R-SIG-Finance] daily vs weekly returns
Message-ID: <46EE75BD.70901@metrak.com>

In a previous thread ("making sense of 100's of funds") there was 
discussion of asynchony and whether it was more favourable to use daily 
vs weekly returns for analysis.

I was curious about this and although probably an agricultural approach, 
I plotted the annualized weekly vs daily standard deviations for 80 of 
the funds that happen to be available at my superannuation provider.

The plot is at http://metrak.com/tmp/exch02.png.  The dashed line is y=x.

cheers


From luda at zhaw.ch  Mon Sep 17 17:16:38 2007
From: luda at zhaw.ch (=?iso-8859-1?Q?L=FCthi_David_=28luda=29?=)
Date: Mon, 17 Sep 2007 17:16:38 +0200
Subject: [R-SIG-Finance] WG:  portfolio optimization using higher moments
Message-ID: <206E81C5DB06934283543543C0E5CDCB5BFC93@langouste.zhaw.ch>

Alexander Moreno wrote:
> Is there any R canned package which will optimize a portfolio using 
> all four moments?  Or are LPM, CVaR, and VaR optimization the best one 
> can do in R ( i.e. no Kurtosis)?

Try package ghyp where you can fit a generalized hyerbolic distribution (which is able to describe skewed and leptocurtic behaviour) to multivariate return data and optimize a portfolio with respect to the variance, VaR or CVaR. 

Best regards,
David

P.s.: A new version of ghyp is on the way to CRAN.


From ianseow at gmail.com  Tue Sep 18 03:41:35 2007
From: ianseow at gmail.com (Ian Seow)
Date: Tue, 18 Sep 2007 09:41:35 +0800
Subject: [R-SIG-Finance] Candlestick chart
Message-ID: <1865010709171841n4870c5a0r3591f8d11e91b769@mail.gmail.com>

Hi Kazuhiro,

There's a nice implementation of candle-stick charting here:-
http://addictedtor.free.fr/graphiques/graphcode.php?graph=65

You can modify the candles themselves in the plotBollingerBars()
function to look more like a regular candlestick chart.

Hope this helps!

best rgds
Ian Seow

> Message: 1
> Date: Sun, 16 Sep 2007 23:17:16 -0400
> From: "Kazuhiro Shimbo" <kshimbo at gmail.com>
> Subject: [R-SIG-Finance] Candlestick chart
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID:
>         <f8cc95220709162017o6a917d58kf6c11e1ae0f1c7c6 at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
>
> Is there a function or coded example of drawing candlestick chart
> based on open,high,low,and close?
>
> Thanks,
>


From davidr at rhotrading.com  Tue Sep 18 18:18:57 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 18 Sep 2007 11:18:57 -0500
Subject: [R-SIG-Finance] [R] Problem in extracting EQY_DVD_HIST from
	Bloomberg
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3027D7DF3@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3027D7DF3@BAN-MAILSRV03.Amba.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE774E752D@rhopost.rhotrading.com>

I'd guess start and end are confusing the bulk data call (it doesn't
take those params). This works for me:

div <- blpGetData(con, "IBM Equity", "EQY_DVD_HIST", retval="raw")

You need the "raw" retval when anything is not numeric; then you need to
extract the bits you want from the nested list returned.

HTH,
David

David L. Reiner
Rho Trading Securities, LLC

-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of Shubha Vishwanath Karanth
Sent: Tuesday, September 18, 2007 1:08 AM
To: R-SIG-Finance at stat.math.ethz.ch; r-help at stat.math.ethz.ch
Subject: [R] Problem in extracting EQY_DVD_HIST from Bloomberg

Hi R,

 

Again the problem in Bloomberg, I give the below code,

 

> con =
blpConnect(show.days="trading",na.action="previous.days",periodicity="da
ily")# connecting Bloomberg
> div <- blpGetData(con,"IBM US
Equity","EQY_DVD_HIST",start=as.chron(as.Date("01/01/2005",
"%m/%d/%Y")),end=as.chron(Sys.Date()))
> blpDisconnect(con)


used (Mb) gc trigger (Mb) max used (Mb)
Ncells 480141 12.9 818163 21.9 818163 21.9
Vcells 798590 6.1 1445757 11.1 1441593 11.0



> div
EQY_DVD_HIST
(09/17/07 19:34:49) NA

 

I get all NA data. What could be the problem? I have the corresponding
data in Bloomberg....but not able to extract the data from R....

 

Thanks in advance,

Shubha


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bbands at gmail.com  Tue Sep 18 18:39:28 2007
From: bbands at gmail.com (BBands)
Date: Tue, 18 Sep 2007 09:39:28 -0700
Subject: [R-SIG-Finance] Candlestick chart
In-Reply-To: <1865010709171841n4870c5a0r3591f8d11e91b769@mail.gmail.com>
References: <1865010709171841n4870c5a0r3591f8d11e91b769@mail.gmail.com>
Message-ID: <6e8360ad0709180939h6a1ac035sf5f1ce49ee5cab6e@mail.gmail.com>

On 9/17/07, Ian Seow <ianseow at gmail.com> wrote:
> Hi Kazuhiro,
>
> There's a nice implementation of candle-stick charting here:-
> http://addictedtor.free.fr/graphiques/graphcode.php?graph=65
>
> You can modify the candles themselves in the plotBollingerBars()
> function to look more like a regular candlestick chart.
>
> Hope this helps!
>
> best rgds
> Ian Seow

I do wish they'd get the attribution on that code right. Dirk and I co-wrote it.

Bollinger Bars were designed as a replacement for both Western bars
and Japanese candlesticks. They encode all the information in both
prior charting styles via the use of color in a bar that can be just a
pixel wide--both Western bars and candlesticks must be at least three
pixels wide. Dirk did a really nice job coding them!

    jab
-
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From davidr at rhotrading.com  Tue Sep 18 19:43:16 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 18 Sep 2007 12:43:16 -0500
Subject: [R-SIG-Finance] [R] ISIN numbers into Bloomberg tickers
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3027D7E4A@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3027D7E4A@BAN-MAILSRV03.Amba.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE774E753F@rhopost.rhotrading.com>

I can't get that ISIN to be recognized by Bloomberg (ID US4009703799<go>
gives 'Security not found'.)

This works for me:
> con <- blpConnect(show.days="trading", na.action="previous.days",
periodicity="daily")
> isin <- "US912828HA15"
> dat <- blpGetData(con, paste(isin,"Govt"), "PX_LAST",
start=chron("9/10/2007"), end=chron("9/18/2007"), retval="data.frame")
> dat
  [DATETIME]  PX_LAST
1   09/10/07 103.4062
2   09/11/07 103.0312
3   09/12/07 102.6875
4   09/13/07 102.2344
5   09/14/07 102.3125
6   09/17/07 102.2812
7   09/18/07 102.0156

David L. Reiner
Rho Trading Securities, LLC

ps. please try not to cross-post.

-----Original Message-----
From: Shubha Vishwanath Karanth [mailto:shubhak at ambaresearch.com] 
Sent: Tuesday, September 18, 2007 1:34 AM
To: r-help at stat.math.ethz.ch; R-SIG-Finance at stat.math.ethz.ch; David
Reiner <davidr at rhotrading.com>
Subject: FW: [R] ISIN numbers into Bloomberg tickers

Hi David,

I tried the following and get the below error messages....


con =
blpConnect(show.days="trading",na.action="previous.days",periodicity="da
ily")# connecting Bloomberg
> dat <- blpGetData(con,"US4009703799
Equity","PX_LAST",start=as.chron(as.Date("01/01/2005",
"%m/%d/%Y"),end=as.chron(Sys.Date()),retval="data.frame")
> blpDisconnect(con)

used (Mb) gc trigger (Mb) max used (Mb)
Ncells 480150 12.9 818163 21.9 818163 21.9
Vcells 797171 6.1 1445757 11.1 1441593 11.0

> dat
[DATETIME] PX_LAST
day day (09/17/07 19:43:45) NA


I don't get the data...but the same statement tried with ticker works.
So, any problem with ISIN's?

-----Original Message-----
From: davidr at rhotrading.com [mailto:davidr at rhotrading.com] 
Sent: Friday, September 14, 2007 10:24 PM
To: Shubha Vishwanath Karanth; r-help at stat.math.ethz.ch
Subject: RE: [R] ISIN numbers into Bloomberg tickers

You can try

> blpGetData(conn, "US912828HA15 Govt", 
    c("ticker", "cpn", "maturity", "market_sector_des"), retval="raw")

and paste together the parts.

HTH,

David L. Reiner
Rho Trading Securities, LLC


-----Original Message-----
From: r-help-bounces at r-project.org [mailto:r-help-bounces at r-project.org]
On Behalf Of Shubha Vishwanath Karanth
Sent: Friday, September 14, 2007 8:42 AM
To: r-help at stat.math.ethz.ch
Subject: [R] ISIN numbers into Bloomberg tickers

Hi R,

 

Can I convert ISIN numbers into Bloomberg tickers in the RBloomberg
package?

 

BR, Shubha


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From m_fornasier at finint.it  Wed Sep 19 12:32:34 2007
From: m_fornasier at finint.it (Fornasier Matteo)
Date: Wed, 19 Sep 2007 12:32:34 +0200
Subject: [R-SIG-Finance] Bloomberg in LAN
Message-ID: <36CFF5AEEAE9E74DA023CCE6C00CDD0BD838D7@ITC1EX01.finit.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070919/07aee19e/attachment.pl 

From ecjbosu at aol.com  Wed Sep 19 12:46:18 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Wed, 19 Sep 2007 05:46:18 -0500
Subject: [R-SIG-Finance] rcopula error
In-Reply-To: <fasdhl$o96$1@sea.gmane.org>
References: <fasdhl$o96$1@sea.gmane.org>
Message-ID: <fcqumb$76i$1@sea.gmane.org>

This is a modified rcopula function.  The commented lines of code are 
the  only way I could get it to work.

I know there is a problem with the if statement when a covariance matrix 
is passed to this function.



#************ rcopula.gauss that works
rcopula.gauss<-function (n, Sigma = equicorr(d, rho), d = 2, rho = 0.7)
{
      d <- dim(Sigma)[1]
#    if (sum(diag(Sigma)) != d)
#        stop("Sigma should be correlation matrix")
      mnorm <- rmnorm(n, Sigma = Sigma)
      matrix(pnorm(mnorm), ncol = d)
}


Here is my example.


require(VGAM)
require(QRMlib)
require(gamlss)
require(fCopulae)
require(copula)
require(date)

df<-7.5
sig1<-.5
var2<-2.5
seed<-1234
set.seed(seed);
#base on sas code from web site
#generate t distributed data
tdata<-data.frame(date=seq(as.numeric(as.date('06/01/2001')),as.numeric(as.date('11/01/2002')),1))
tdata$tdata<-.05*tdata$date+rTF(length(tdata$date),mu=5000,sigma=sig1,nu=df)

#estimate model data~a * date + constant
      fam<-TF

y1<-gamlss(tdata~date+1,data=tdata,family=fam,na.rm=T,nu.start=16.58,sigma.formula=~1
        ,nu.formula=~1)
      y1<-gamlss(tdata~date+1,data=tdata,family=TF(mu.link =
identity,sigma.link = log, nu.link = log))
      plot(y1,na.rm=T)


   #generate normal garch distributed data
   set.seed(12345)

ndata<-data.frame(date=seq(as.numeric(as.date('06/01/2001')),as.numeric(as.date('11/01/2002')),1))
    ndata$rnd<-rnorm(length(ndata$date))
    ndata$v<-NaN
    ndata$e<-NaN
    ndata$v[1]<-.0001+.2*var22 +.75* var2
    ndata$e[1]<-sqrt(ndata$v[1])*ndata$rnd[1]
    for (i in 2:length(ndata$date) ) {
      ndata$v[i]<- 0.0001 + 0.2 * ndata$e[(i - 1)]2 + 0.75 * ndata$v[(i
- 1)]
      ndata$e[i]<-sqrt(ndata$v[i])+ndata$rnd[i]
    }
    # add constant 25
    ndata$ndata<-25+ndata$e
    # estimate garch 1,1 model of normally distributed data
    yn<-garchFit(ndata~garch(1,1),data=ndata)

   #y = rcauchy(519, loc=-4, scale=1)
   # generate cauchy data
   set.seed(seed);

cdata<-data.frame(date=seq(as.numeric(as.date('06/01/2001')),as.numeric(as.date('11/01/2002')),1))
   #cdata$cdata<-unlist(-4+tan(runif(length(cdata$date))-.5)*pi)
   cdata$cdata<-rcauchy(length(cdata$date),loc=-4,scale=1)
   #estimate cauchy model
   yc<-vglm(cdata ~ 1 , data=cdata,cauchy1(lloc="identity"), trace=TRUE,
crit="c")



   #create transformed uniform residuals for all estimated series' residuals

resids<-data.frame(date=tdata$date,tresids=y1$residuals,nresids=yn at residuals,cresids=yc at residuals)

resids$tresids<-pnorm(pTF(resids$tresids/exp(y1$sigma.coefficient),y1$nu.coefficients))
   resids$cresids<-pnorm(pcauchy(resids$cresids,yc at coefficients[1]))

resids$nresids<-pnorm(resids$nresids,mean(resids$nresids),sd=sd(resids$nresids))

yall<-merge(tdata,merge(cdata,ndata,by.x="date",by.y="date",all=T),by.x="date",by.y="date",all=T)

rnds<-data.frame(rmvnorm(2000,sigma=cov(resids[-1]),mean=mean(resids[-1])))
   names(rnds)<-c('T',"Normal",'Cauchy')
#fit a normal copula to the uniform residuals
   cop.fit<-fit.norm(resids[,2:4])
#simulate the normal copula
# cop.sim1<-rmnorm(2000, Sigma=cop.fit$Sigma, mu=cop.fit$mu)


#************Here is my problem
   cop.sim<-rcopula.gauss(2000, Sigma=cop.fit$Sigma)

#*******

   #,d=sum(diag(cop.fit$cor)))#, mu=cop.fit$mu)
#transform the simulated correlated normal rv' to uncorrelated
individual distributions
   sims<-data.frame(cop.sim)
   names(sims)<- c('T',"Normal",'Cauchy')

sims$T<-qTF(pnorm(sims$T),mu=y1$mu.coefficients[1],sigma=exp(y1$sigma.coefficients), 

nu=y1$nu.coefficients)
 
sims$Normal<-qnorm(sims$Normal,mean(resids$nresids),sd=sd(resids$nresids))
   sims$Cauchy<-qcauchy(pnorm(sims$Cauchy),location=yc at coefficients[1])

sims$T<-seq(as.numeric(as.date('11/01/2002'))+1,as.numeric(as.date('11/01/2002'))+2000,1)


From davidr at rhotrading.com  Wed Sep 19 15:09:22 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 19 Sep 2007 08:09:22 -0500
Subject: [R-SIG-Finance] Bloomberg in LAN
In-Reply-To: <36CFF5AEEAE9E74DA023CCE6C00CDD0BD838D7@ITC1EX01.finit.local>
References: <36CFF5AEEAE9E74DA023CCE6C00CDD0BD838D7@ITC1EX01.finit.local>
Message-ID: <F9F2A641C593D7408925574C05A7BE774E75A3@rhopost.rhotrading.com>

You are only authorized to fetch and use Bloomberg data on a machine that is paying for a license, either full or as a 'traveler.' If you have serverAPI or data license you can get data other ways, but these also cost.

David L. Reiner
Rho Trading Securities, LLC
550 W. Jackson Blvd #1000
Chicago, IL 60661-5704
 
312-244-4610 direct
312-244-4500 main
312-244-4501 fax
 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Fornasier Matteo
Sent: Wednesday, September 19, 2007 5:33 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Bloomberg in LAN

Hi all,
I'm using R in my workstation, I want to know if I can get data from Bloomberg (with Rbloomberg package) that is in another workstation in the LAN, or I have to install R in that workstation and launch the scripts in there.
Thanks all,
 
Regards
 
Matteo Fornasier
Finanziaria Internazionale Alternative Investment SGR S.p.A.
V. Vittorio Alfieri, 1
31015 Conegliano (TV), Italy
tel. +39 0438 360655
fax. +39 0438 694565
m_fornasier at finint.it

Ogni informazione contenuta nel messaggio ? confidenziale e di esclusivo interesse del destinatario. La distribuzione, l'utilizzo e/o la divulgazione, anche parziale, sono proibiti. Le comunicazioni su Internet non sono sicure e quindi Finanziaria Internazionale non accetta responsabilit? legali per i contenuti di questo messaggio.

The information in this Internet E-mail is confidential. It is intended solely for the addressee. Access to this Internet E-mail by anyone else is unautorised. If you are not intended recipient, any disclosure, copying or distribution is prohibited and may be unlawful. Internet communications are not secure and therefore Finanziaria Internazionale does not accept legal responsability for the contents of this message.

 

	[[alternative HTML version deleted]]


From dryeraser at yahoo.com  Thu Sep 20 20:12:11 2007
From: dryeraser at yahoo.com (Dry Eraser)
Date: Thu, 20 Sep 2007 11:12:11 -0700 (PDT)
Subject: [R-SIG-Finance] get.hist.quote and mysql
Message-ID: <169640.6669.qm@web30502.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070920/a74fcc00/attachment.pl 

From dsmith at viciscapital.com  Thu Sep 20 20:16:00 2007
From: dsmith at viciscapital.com (Dale Smith)
Date: Thu, 20 Sep 2007 14:16:00 -0400
Subject: [R-SIG-Finance] get.hist.quote and mysql
In-Reply-To: <169640.6669.qm@web30502.mail.mud.yahoo.com>
References: <169640.6669.qm@web30502.mail.mud.yahoo.com>
Message-ID: <0E4F0C7EEAAB274F8DC6B1543949F05B0113E10D@vicsrv4.viciscapital.com>

Try the RMySQL package. It works pretty well.

http://cran.r-project.org/src/contrib/Descriptions/RMySQL.html

Dale Smith
Vicis Capital, LLC

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Dry Eraser
Sent: Thursday, September 20, 2007 2:12 PM
To: r-sig-finance at r-project.org
Subject: [R-SIG-Finance] get.hist.quote and mysql

I'm doing a small project to identity particular stock movements out of
the universe of stock quote histories from Yahoo.  Basically, I would be
iterating a screen over several thousand time series.

I could save and use several thousand CSV files saved by get.hist.quote,
but storing the data in a database like MySQL seems to be a better idea,
especially since more stock prices would be added in the future.

I googled r lists for this subject, read r manuals, and skimmed the TOC
of one or two Times Series using R books and didn't turn up much in the
way of concrete application of MySQL.  The best I found was a hint by
Bollinger on this list pointing to a number of packages (including
python-related).

Any suggestions on where to look and ways of approaching my project?

Thanks,
Ming


 
________________________________________________________________________
____________

, and more!

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

All e-mail sent to or from this address will be received or otherwise recorded by Vicis Capital, LLC and is subject to archival, monitoring and/or review, by and/or disclosure to, someone other than the recipient.  This message is intended only for the use of the person(s) ("intended recipient") to whom it is addressed.  It may contain information that is privileged and confidential.  If you are not the intended recipient, please contact the sender as soon as possible and delete the message without reading it or making a copy.  Any dissemination, distribution, copying, or other use of this message or any of its content by any person other than the intended recipient is strictly prohibited.  Vicis Capital, LLC only transacts business in states where it is properly registered or notice filed, or excluded or exempted from registration or notice filing requirements.


From dryeraser at yahoo.com  Thu Sep 20 20:53:27 2007
From: dryeraser at yahoo.com (Dry Eraser)
Date: Thu, 20 Sep 2007 11:53:27 -0700 (PDT)
Subject: [R-SIG-Finance] get.hist.quotes and MySQL
Message-ID: <509275.16917.qm@web30515.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070920/a7f3347a/attachment.pl 

From josh at gghc.com  Thu Sep 20 21:12:30 2007
From: josh at gghc.com (Joshua Reich)
Date: Thu, 20 Sep 2007 15:12:30 -0400
Subject: [R-SIG-Finance] get.hist.quotes and MySQL
References: <509275.16917.qm@web30515.mail.mud.yahoo.com>
Message-ID: <C20EA84D76C94F4E999DC041E81C0D1104AB5FC1@exchange2k3.ny.gghc.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070920/bf28d08c/attachment.pl 

From jeff.a.ryan at gmail.com  Thu Sep 20 21:14:25 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 20 Sep 2007 14:14:25 -0500
Subject: [R-SIG-Finance] get.hist.quote and mysql
In-Reply-To: <169640.6669.qm@web30502.mail.mud.yahoo.com>
References: <169640.6669.qm@web30502.mail.mud.yahoo.com>
Message-ID: <e8e755250709201214u5328d500n11d0a61bf28762d4@mail.gmail.com>

A bit of python that should facilitate the yahoo to MySQL:

Should work - though you'll need a database set up, and the
appropriate password/host/etc.

At least will get you started in the right direction.

Basically calls a file 'symbolsFile' in the same directory,
which is just a text file with one yahoo symbol per line.

db has columns:
date,o,h,l,c,v,a with the appropriate MySQL types of course.


Python requires proper indentation, so copy paste may not be perfect.
And it requires (this script) the package MySQLdb... A word of warning
- it has been a bit since I've used this, so double check your results
at the very least.


#!/usr/bin/env python2.5

import urllib
import re
import MySQLdb
import string
import time

sym = open("symbolsFile").readlines()
regx = re.compile('(.+?)\\n')

today = time.localtime()
yr = time.strftime('%Y',today)
mon = time.strftime('%m',today)
day = time.strftime('%d',today)

db = MySQLdb.connect(user='username',passwd='password',db='databasename', \
                     host='localhost', unix_socket='/path/to/socket')
cursor = db.cursor()

for s in sym:
    symbol = regx.findall(s)[0]
    yahoourl = "http://ichart.finance.yahoo.com/table.csv?s="+symbol + \
                   "&a=00&b="+day+"&c="+"1970"+"&d="+mon+"&e="+day+ \
                   "&f="+yr+"&g=d&ignore=.csv"
    data = urllib.urlopen(yahoourl).readlines()
    sql = "INSERT IGNORE INTO "+string.strip(symbol,'^')+"
(date,o,h,l,c,v,a)" + \
          " VALUES(%s,%s,%s,%s,%s,%s,%s)"
    mult=[]
    for d in data[1:]:
        dd = string.split(d,',')
        dd[6] = string.strip(dd[6],'\n')
        string.join(dd,',')
        mult[len(mult):] = [dd]
    cursor.executemany(sql,mult)

And because I like to plug my R software (feedback is my goal...), check out
www.quantmod.com or the quantmod package on CRAN.  Some nice wrappers to
data handling and modellig stuff.  Still a work in progress, but it IS
in progress...

Jeff Ryan


On 9/20/07, Dry Eraser <dryeraser at yahoo.com> wrote:
> I'm doing a small project to identity particular stock movements out of the universe of stock quote histories from Yahoo.  Basically, I would be iterating a screen over several thousand time series.
>
> I could save and use several thousand CSV files saved by get.hist.quote, but storing the data in a database like MySQL seems to be a better idea, especially since more stock prices would be added in the future.
>
> I googled r lists for this subject, read r manuals, and skimmed the TOC of one or two Times Series using R books and didn't turn up much in the way of concrete application of MySQL.  The best I found was a hint by Bollinger on this list pointing to a number of packages (including python-related).
>
> Any suggestions on where to look and ways of approaching my project?
>
> Thanks,
> Ming
>
>
>       ____________________________________________________________________________________
>
> , and more!
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Thu Sep 20 21:47:04 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 20 Sep 2007 14:47:04 -0500
Subject: [R-SIG-Finance] get.hist.quote and mysql
In-Reply-To: <e8e755250709201214u5328d500n11d0a61bf28762d4@mail.gmail.com>
References: <169640.6669.qm@web30502.mail.mud.yahoo.com>
	<e8e755250709201214u5328d500n11d0a61bf28762d4@mail.gmail.com>
Message-ID: <e8e755250709201247g1bbcec04la45950b8b392795e@mail.gmail.com>

That bit of python - if you are downloading a lot of symbols, you'll
probably want to add some sort of timeout between request - lest you
get throttled by yahoo...

As I copied it it will retrieve data from 1970 (or the earliest
available) to the present, adjust the dates to to your needs....

...and in case I was unclear - your mileage may vary when run : )


I like that Postgresql idea too..


Jeff

On 9/20/07, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> A bit of python that should facilitate the yahoo to MySQL:
>
> Should work - though you'll need a database set up, and the
> appropriate password/host/etc.
>
> At least will get you started in the right direction.
>
> Basically calls a file 'symbolsFile' in the same directory,
> which is just a text file with one yahoo symbol per line.
>
> db has columns:
> date,o,h,l,c,v,a with the appropriate MySQL types of course.
>
>
> Python requires proper indentation, so copy paste may not be perfect.
> And it requires (this script) the package MySQLdb... A word of warning
> - it has been a bit since I've used this, so double check your results
> at the very least.
>
>
> #!/usr/bin/env python2.5
>
> import urllib
> import re
> import MySQLdb
> import string
> import time
>
> sym = open("symbolsFile").readlines()
> regx = re.compile('(.+?)\\n')
>
> today = time.localtime()
> yr = time.strftime('%Y',today)
> mon = time.strftime('%m',today)
> day = time.strftime('%d',today)
>
> db = MySQLdb.connect(user='username',passwd='password',db='databasename', \
>                      host='localhost', unix_socket='/path/to/socket')
> cursor = db.cursor()
>
> for s in sym:
>     symbol = regx.findall(s)[0]
>     yahoourl = "http://ichart.finance.yahoo.com/table.csv?s="+symbol + \
>                    "&a=00&b="+day+"&c="+"1970"+"&d="+mon+"&e="+day+ \
>                    "&f="+yr+"&g=d&ignore=.csv"
>     data = urllib.urlopen(yahoourl).readlines()
>     sql = "INSERT IGNORE INTO "+string.strip(symbol,'^')+"
> (date,o,h,l,c,v,a)" + \
>           " VALUES(%s,%s,%s,%s,%s,%s,%s)"
>     mult=[]
>     for d in data[1:]:
>         dd = string.split(d,',')
>         dd[6] = string.strip(dd[6],'\n')
>         string.join(dd,',')
>         mult[len(mult):] = [dd]
>     cursor.executemany(sql,mult)
>
> And because I like to plug my R software (feedback is my goal...), check out
> www.quantmod.com or the quantmod package on CRAN.  Some nice wrappers to
> data handling and modellig stuff.  Still a work in progress, but it IS
> in progress...
>
> Jeff Ryan
>
>
> On 9/20/07, Dry Eraser <dryeraser at yahoo.com> wrote:
> > I'm doing a small project to identity particular stock movements out of the universe of stock quote histories from Yahoo.  Basically, I would be iterating a screen over several thousand time series.
> >
> > I could save and use several thousand CSV files saved by get.hist.quote, but storing the data in a database like MySQL seems to be a better idea, especially since more stock prices would be added in the future.
> >
> > I googled r lists for this subject, read r manuals, and skimmed the TOC of one or two Times Series using R books and didn't turn up much in the way of concrete application of MySQL.  The best I found was a hint by Bollinger on this list pointing to a number of packages (including python-related).
> >
> > Any suggestions on where to look and ways of approaching my project?
> >
> > Thanks,
> > Ming
> >
> >
> >       ____________________________________________________________________________________
> >
> > , and more!
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From dryeraser at yahoo.com  Thu Sep 20 22:01:00 2007
From: dryeraser at yahoo.com (Dry Eraser)
Date: Thu, 20 Sep 2007 13:01:00 -0700 (PDT)
Subject: [R-SIG-Finance] get.hist.quote and mysql
Message-ID: <399749.24268.qm@web30513.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070920/5e1eaab5/attachment.pl 

From jeff.a.ryan at gmail.com  Thu Sep 20 22:10:58 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 20 Sep 2007 15:10:58 -0500
Subject: [R-SIG-Finance] get.hist.quote and mysql
In-Reply-To: <399749.24268.qm@web30513.mail.mud.yahoo.com>
References: <399749.24268.qm@web30513.mail.mud.yahoo.com>
Message-ID: <e8e755250709201310o5812cbdbhd77b03b27b54271d@mail.gmail.com>

BeanCounter is from Dirk - the maintainer the finance task view on
CRAN.  Probably quite solid would be my guess : )

Perl just brings me to tears too often...

On 9/20/07, Dry Eraser <dryeraser at yahoo.com> wrote:
> thanks for all the input.  a friend of mine who works at a hedge fund recommended postgres with R (or S?) a while back for the reasons mentioned in this thread.
>
> i'll try the python script (with the timeouts)... i also found the Finance::Beancounter module on CPAN
>
> http://search.cpan.org/~edd/Finance-BeanCounter-0.8.7/
>
> looks promising.  it supports postgres by default and has companion modules to initialize the database.
>
> -Ming
>
> ----- Original Message ----
> From: Jeff Ryan <jeff.a.ryan at gmail.com>
> To: Dry Eraser <dryeraser at yahoo.com>
> Cc: r-sig-finance at r-project.org
> Sent: Thursday, September 20, 2007 3:47:04 PM
> Subject: Re: [R-SIG-Finance] get.hist.quote and mysql
>
>
> That bit of python - if you are downloading a lot of symbols, you'll
> probably want to add some sort of timeout between request - lest you
> get throttled by yahoo...
>
> As I copied it it will retrieve data from 1970 (or the earliest
> available) to the present, adjust the dates to to your needs....
>
> ...and in case I was unclear - your mileage may vary when run : )
>
>
> I like that Postgresql idea too..
>
>
> Jeff
>
> On 9/20/07, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> > A bit of python that should facilitate the yahoo to MySQL:
> >
> > Should work - though you'll need a database set up, and the
> > appropriate password/host/etc.
> >
> > At least will get you started in the right direction.
> >
> > Basically calls a file 'symbolsFile' in the same directory,
> > which is just a text file with one yahoo symbol per line.
> >
> > db has columns:
> > date,o,h,l,c,v,a with the appropriate MySQL types of course.
> >
> >
> > Python requires proper indentation, so copy paste may not be perfect.
> > And it requires (this script) the package MySQLdb... A word of warning
> > - it has been a bit since I've used this, so double check your results
> > at the very least.
> >
> >
> > #!/usr/bin/env python2.5
> >
> > import urllib
> > import re
> > import MySQLdb
> > import string
> > import time
> >
> > sym = open("symbolsFile").readlines()
> > regx = re.compile('(.+?)\\n')
> >
> > today = time.localtime()
> > yr = time.strftime('%Y',today)
> > mon = time.strftime('%m',today)
> > day = time.strftime('%d',today)
> >
> > db = MySQLdb.connect(user='username',passwd='password',db='databasename', \
> >                      host='localhost', unix_socket='/path/to/socket')
> > cursor = db.cursor()
> >
> > for s in sym:
> >     symbol = regx.findall(s)[0]
> >     yahoourl = "http://ichart.finance.yahoo.com/table.csv?s="+symbol + \
> >                    "&a=00&b="+day+"&c="+"1970"+"&d="+mon+"&e="+day+ \
> >                    "&f="+yr+"&g=d&ignore=.csv"
> >     data = urllib.urlopen(yahoourl).readlines()
> >     sql = "INSERT IGNORE INTO "+string.strip(symbol,'^')+"
> > (date,o,h,l,c,v,a)" + \
> >           " VALUES(%s,%s,%s,%s,%s,%s,%s)"
> >     mult=[]
> >     for d in data[1:]:
> >         dd = string.split(d,',')
> >         dd[6] = string.strip(dd[6],'\n')
> >         string.join(dd,',')
> >         mult[len(mult):] = [dd]
> >     cursor.executemany(sql,mult)
> >
> > And because I like to plug my R software (feedback is my goal...), check out
> > www.quantmod.com or the quantmod package on CRAN.  Some nice wrappers to
> > data handling and modellig stuff.  Still a work in progress, but it IS
> > in progress...
> >
> > Jeff Ryan
> >
> >
> > On 9/20/07, Dry Eraser <dryeraser at yahoo.com> wrote:
> > > I'm doing a small project to identity particular stock movements out of the universe of stock quote histories from Yahoo.  Basically, I would be iterating a screen over several thousand time series.
> > >
> > > I could save and use several thousand CSV files saved by get.hist.quote, but storing the data in a database like MySQL seems to be a better idea, especially since more stock prices would be added in the future.
> > >
> > > I googled r lists for this subject, read r manuals, and skimmed the TOC of one or two Times Series using R books and didn't turn up much in the way of concrete application of MySQL.  The best I found was a hint by Bollinger on this list pointing to a number of packages (including python-related).
> > >
> > > Any suggestions on where to look and ways of approaching my project?
> > >
> > > Thanks,
> > > Ming
> > >
> > >
> > >       ____________________________________________________________________________________
> > >
> > > , and more!
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> >
>
>
>
> ____________________________________________________________________________________
> Boardwalk for $500? In 2007? Ha! Play Monopoly Here and Now (it's updated f
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From edd at debian.org  Thu Sep 20 22:17:53 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 20 Sep 2007 15:17:53 -0500
Subject: [R-SIG-Finance] get.hist.quote and mysql
In-Reply-To: <399749.24268.qm@web30513.mail.mud.yahoo.com>
References: <399749.24268.qm@web30513.mail.mud.yahoo.com>
Message-ID: <18162.54641.188646.54102@ron.nulle.part>


On 20 September 2007 at 13:01, Dry Eraser wrote:
| http://search.cpan.org/~edd/Finance-BeanCounter-0.8.7/

Yes, main page is 
	http://dirk.eddelbuettel.com/code/beancounter.html
and you get to pick Postgresql, Mysql or SQLite all of which work out of the
box.  It's rather robust, with a number of users who have been using it to
fetch daily data for a number of years now.  It has its warts too --
historical FX data is not as reliable etc pp.

I have yet to really integrate it with R -- the analysis part (done in Perl
just like the rest) covers just the basics (but does two VaR measures and
what have you) -- but I want to do it in a way that leaves the Perl-only
users unaffected, ie start from scratch with an analysis modules, maybe with
a little gWidgets GUI. I always thought it would make for a nice backend for
PerformanceAnalytics, so here's Hi to Brian and Peter :)

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From ksspriggs at gmail.com  Thu Sep 20 22:51:23 2007
From: ksspriggs at gmail.com (Kenneth Spriggs)
Date: Thu, 20 Sep 2007 15:51:23 -0500
Subject: [R-SIG-Finance] get.hist.quotes and MySQL
In-Reply-To: <509275.16917.qm@web30515.mail.mud.yahoo.com>
References: <509275.16917.qm@web30515.mail.mud.yahoo.com>
Message-ID: <82b060880709201351u1c7a189o7c14b2c5beedccb9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070920/2e93b935/attachment.pl 

From sf at metrak.com  Fri Sep 21 11:16:43 2007
From: sf at metrak.com (paul sorenson)
Date: Fri, 21 Sep 2007 19:16:43 +1000
Subject: [R-SIG-Finance] get.hist.quotes and MySQL
In-Reply-To: <509275.16917.qm@web30515.mail.mud.yahoo.com>
References: <509275.16917.qm@web30515.mail.mud.yahoo.com>
Message-ID: <46F38BFB.3010104@metrak.com>

I am collecting hundreds of time series and storing them in sqlite.  I 
am downloading and storing with python then reading them out with R.

I don't know if the MySQL and PostgreSQL interfaces are smarter, but 
with SQLite the data all comes out as character, you need to perform 
some massaging after extracting data from SQLite.  On the other hand 
SQLite is zero administration.

cheers

Dry Eraser wrote:
> I want to screen the universe of stock price histories available from
> Yahoo for certain patterns.  That means iterating a screen over
> several thousand time series.  get.hist.quotes saves time series to
> CSV.  Putting everything into a database like MySQL seems to be a
> better idea than working with thousands of CSV files, especially when
> I might want to add new data points in the future.
> 
> I scoured the R and R packages manuals, the TOC of the Times Series
> Analysis Using R book, and googled r lists for information on
> get.hist.quotes and MySQL and came up only almost nothing on the
> practical use of MySQL.  (I did find a tantalizing pointer by
> Bollinger to a bunch of python, r and gnuplot packages for his own
> projects.  I think I might need to leverage python or perl to load up
> the database and then R packages manipulate the data.)
> 
> Any suggestions on where to learn more -- and find examples -- on
> using MySQL for ts and any comments on my type of project using R in
> general?
> 
> Thanks, Ming
> 
> 
> 
> ____________________________________________________________________________________
> 
> 
> eChase.
> 
> [[alternative HTML version deleted]]
> 
> _______________________________________________ 
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance --
> Subscriber-posting only. -- If you want to post, subscribe first.


From brian at braverock.com  Fri Sep 21 12:00:49 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 21 Sep 2007 05:00:49 -0500
Subject: [R-SIG-Finance] get.hist.quotes and MySQL
In-Reply-To: <46F38BFB.3010104@metrak.com>
References: <509275.16917.qm@web30515.mail.mud.yahoo.com>
	<46F38BFB.3010104@metrak.com>
Message-ID: <46F39651.3050002@braverock.com>

paul sorenson wrote:
> I am collecting hundreds of time series and storing them in sqlite.  I 
> am downloading and storing with python then reading them out with R.
> 
> I don't know if the MySQL and PostgreSQL interfaces are smarter, but 
> with SQLite the data all comes out as character, you need to perform 
> some massaging after extracting data from SQLite.  On the other hand 
> SQLite is zero administration.


Just for the archive:  yes MySQL, PostgreSQL, Oracle, DB2, etc can store 
data types better than sqlite.  As Paul notes, the main reason to use 
sqlite is that you don't need to actually administer a real database 
environment.

   - Brian


> Dry Eraser wrote:
>> I want to screen the universe of stock price histories available from
>> Yahoo for certain patterns.  That means iterating a screen over
>> several thousand time series.  get.hist.quotes saves time series to
>> CSV.  Putting everything into a database like MySQL seems to be a
>> better idea than working with thousands of CSV files, especially when
>> I might want to add new data points in the future.
>>
>> I scoured the R and R packages manuals, the TOC of the Times Series
>> Analysis Using R book, and googled r lists for information on
>> get.hist.quotes and MySQL and came up only almost nothing on the
>> practical use of MySQL.  (I did find a tantalizing pointer by
>> Bollinger to a bunch of python, r and gnuplot packages for his own
>> projects.  I think I might need to leverage python or perl to load up
>> the database and then R packages manipulate the data.)
>>
>> Any suggestions on where to learn more -- and find examples -- on
>> using MySQL for ts and any comments on my type of project using R in
>> general?
>>
>> Thanks, Ming


From strongthinking at gmail.com  Sat Sep 22 16:30:27 2007
From: strongthinking at gmail.com (Strong)
Date: Sat, 22 Sep 2007 23:30:27 +0900 (JST)
Subject: [R-SIG-Finance] Problem with the timeDate function
Message-ID: <20070922.233027.226732933.strongthinking@srv.cc.hit-u.ac.jp>


Dear All

I am a beginner of Rmetrics.  It is the first time I posts on the mailing list. 

I am having a problem with the use of timeDate function. 

I tried to use timeDate function in the fCalendar package to convert a character string of date to into the timeDate object.

My script is something like the followings:

< date <- c("1990.01", "1990.02")

< date.td <- timeDate(date, format="%Y.%m")

And then, I got the following error message:

Error in if (sum(lt$sec + lt$min + lt$hour) == 0) isoFormat = "%Y-%m-%d" : 
	missing value where TRUE/FALSE needed
In addition: Warning message:
Unknown Format Specification in: .whichFormat(charvec) 

I don't know what's wrong with my format specification.  It seems the reason is I am not using the standard iso format,  although I do believe the flexible specification of time format is allowed by the timeDate function just like in S-Plus.

Can anyone help me?

Thanks in advance.


Strong Chen
phd Student,  Graduate School of Commerce and Management;
Hitotusbashi University, Tokyo, Japan


From icos.atropa at gmail.com  Sun Sep 23 02:45:23 2007
From: icos.atropa at gmail.com (icosa atropa)
Date: Sat, 22 Sep 2007 18:45:23 -0600
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 40, Issue 16
In-Reply-To: <mailman.1.1190455202.5220.r-sig-finance@stat.math.ethz.ch>
References: <mailman.1.1190455202.5220.r-sig-finance@stat.math.ethz.ch>
Message-ID: <681d07c20709221745i39306d6ay5533cf7be180df1d@mail.gmail.com>

I settled on Postgresql to store my timeseries prior to R processing,
and I've been very pleased with my choice.  It has extensive datatype
support - In particular, I've used POSIX , float, text, and even GIS
extensions with great success.  Postgresl documentation is __very__
good.  Admin requires some time and resources (not VPS friendly by
default), but it's been a very satisfying learning experience for me.

I consider postgresql the "designers" choice of database to interface
with R.  It supports cool features like foreign keys and views that I
now take for granted.  As previously mentioned, server-side use of
perl and/or python functions defined and triggered within the database
can be very useful. Overall, if the product is to be used long-term,
by more than one person, or is developed by someone who enjoys
programming, then I consider Postgreql a very good choice.

I use RODBC to connect R to postgresql (extra datatype coersion can
occur here).  The unixODBC docs (dongle between RODBC and postgresql)
are decent and generally complete.  I preprocess my data with Perl -
the perl database interface docs are high-quality; i imagine python is
similar.

Some interesting advantages of postgresql (also apply to mysql?) that
I've found:
I can easily interface with different front ends
  - php and html
  - Openoffice base
  - psql, etc

A final note - Relational databases don't "understand" timeseries per
se, and are not guaranteed to return rows in a given order unless
specifically requested.  Adding a "unique" constraint on the time
index column and sorting on the time index gives the table the
appearance of being a timeseries.  As far as the database is
concerned, however, its just a heap of tuples.   Hence the importance
of pre-processing text data with a text-friendly language like python
or perl before db insertion.

hope this helps,
christian

> paul sorenson wrote:
> > I am collecting hundreds of time series and storing them in sqlite.  I
> > am downloading and storing with python then reading them out with R.
>
> Just for the archive:  yes MySQL, PostgreSQL, Oracle, DB2, etc can store
> data types better than sqlite.  As Paul notes, the main reason to use
> sqlite is that you don't need to actually administer a real database
> environment.
>
>    - Brian


From bjthelen at umich.edu  Sun Sep 23 04:30:13 2007
From: bjthelen at umich.edu (Brian Thelen)
Date: Sat, 22 Sep 2007 22:30:13 -0400
Subject: [R-SIG-Finance] "Error in fmin" when running evCopulaFit (in
	fCopulae)
Message-ID: <46F5CFB5.7070906@umich.edu>

Hi,
I am getting an error message when I run evCopulaFit, a function in Package
fCopulae.  Specifically, I have the following input/output from R.  Is 
there
any advice out there about what is going on here -- is there a problem with
the code, or am I doing something wrong.

 > evcopfit <- evCopulaFit(runif(25,0,1),runif(25,0,1),type='gumbel')
Error in fmin(function(arg) f(arg, ...), lower, upper, tol) :
        invalid 'xmax' value

No matter what sort of input I use, I get the same error message as above.
thanks for any help that is out there,
Brian


From alexander.f.moreno at gmail.com  Sun Sep 23 06:15:07 2007
From: alexander.f.moreno at gmail.com (Alexander Moreno)
Date: Sat, 22 Sep 2007 23:15:07 -0500
Subject: [R-SIG-Finance] moving average
Message-ID: <3303a4570709222115s46890c30j709c16f582aa1841@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070922/1a0888e1/attachment.pl 

From patrick at burns-stat.com  Sun Sep 23 09:39:18 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Sun, 23 Sep 2007 08:39:18 +0100
Subject: [R-SIG-Finance] moving average
In-Reply-To: <3303a4570709222115s46890c30j709c16f582aa1841@mail.gmail.com>
References: <3303a4570709222115s46890c30j709c16f582aa1841@mail.gmail.com>
Message-ID: <46F61826.2060703@burns-stat.com>

Asking if your code does what you think it does is
an excellent thing to do.

In this case I would try it out on something like:

A <- rnorm(30)
A[15] <- 100

And then see if the result is what I hoped.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Alexander Moreno wrote:

>Hi,
>
>If I want to make a 12 day exponentially weighted moving average using only
>past data on a time series A, will this do the trick?
>
>filter(A,exp(-c(1:12)/12)/sum(exp(-c(1:12)/12)),method="convolution",sides=1)
>
>i.e. does it include today in the calculation?  Or do I need to do this
>(which is what I think)?
>
>filter(A,c(0,exp(-c(1:12)/12)/sum(exp(-c(1:12)/12))),method="convolution",sides=1)
>
>I'm trying to play around with an MACD model and want to see the daily
>returns of days where the 12-day moving average < 26 day moving average and
>12-day > 26 day
>
>Thanks,
>Alex
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From m_olshansky at yahoo.com  Mon Sep 24 02:14:04 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Sun, 23 Sep 2007 17:14:04 -0700 (PDT)
Subject: [R-SIG-Finance] Problem with the timeDate function
In-Reply-To: <20070922.233027.226732933.strongthinking@srv.cc.hit-u.ac.jp>
Message-ID: <468647.79905.qm@web32202.mail.mud.yahoo.com>

Hello,

I am also new to Rmetrics so take what I say with
caution.
The string "1990.01" is not a valid date - you must
add a day also (even if you do not care about the day
add a dummy one, let say 1, so that you date is
"1990.01.01"). This is a valid date but then you have
another problem (as indicated by your error message) -
if you specify date only then the format is assumed to
be "%Y-%m-%d" and so a period (.) can not be a
separator. So you can either specify the date as
"1990-01-01" or supply a valid time as well, i.e.
timeDate("1990.02.01.00.00.01",format="%Y.%m.%d.%H.%M.%S").

Regards,

Moshe.
 
--- Strong <strongthinking at gmail.com> wrote:

> 
> Dear All
> 
> I am a beginner of Rmetrics.  It is the first time I
> posts on the mailing list. 
> 
> I am having a problem with the use of timeDate
> function. 
> 
> I tried to use timeDate function in the fCalendar
> package to convert a character string of date to
> into the timeDate object.
> 
> My script is something like the followings:
> 
> < date <- c("1990.01", "1990.02")
> 
> < date.td <- timeDate(date, format="%Y.%m")
> 
> And then, I got the following error message:
> 
> Error in if (sum(lt$sec + lt$min + lt$hour) == 0)
> isoFormat = "%Y-%m-%d" : 
> 	missing value where TRUE/FALSE needed
> In addition: Warning message:
> Unknown Format Specification in:
> .whichFormat(charvec) 
> 
> I don't know what's wrong with my format
> specification.  It seems the reason is I am not
> using the standard iso format,  although I do
> believe the flexible specification of time format is
> allowed by the timeDate function just like in
> S-Plus.
> 
> Can anyone help me?
> 
> Thanks in advance.
> 
> 
> Strong Chen
> phd Student,  Graduate School of Commerce and
> Management;
> Hitotusbashi University, Tokyo, Japan
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From strongthinking at gmail.com  Mon Sep 24 10:22:49 2007
From: strongthinking at gmail.com (Thinking Strong)
Date: Mon, 24 Sep 2007 17:22:49 +0900
Subject: [R-SIG-Finance] Expanding data to higher frequency
Message-ID: <9b5c86780709240122x48427b6cy20b713674bd6b902@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070924/43bf2ffa/attachment.pl 

From nicolas.mougeot at db.com  Mon Sep 24 13:44:21 2007
From: nicolas.mougeot at db.com (Nicolas Mougeot)
Date: Mon, 24 Sep 2007 13:44:21 +0200
Subject: [R-SIG-Finance] R-squared or t-stat?
Message-ID: <OFC9FEF692.23A46BD3-ON80257360.0040213D-C1257360.00407C1C@db.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070924/17ed1fe4/attachment.pl 

From edd at debian.org  Mon Sep 24 15:17:34 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 24 Sep 2007 08:17:34 -0500
Subject: [R-SIG-Finance] R-squared or t-stat?
In-Reply-To: <OFC9FEF692.23A46BD3-ON80257360.0040213D-C1257360.00407C1C@db.com>
References: <OFC9FEF692.23A46BD3-ON80257360.0040213D-C1257360.00407C1C@db.com>
Message-ID: <20070924131734.GA6327@eddelbuettel.com>

On Mon, Sep 24, 2007 at 01:44:21PM +0200, Nicolas Mougeot wrote:
> Hi,
> 
> let us assume that you want to regress Y on either X and Z or W, ie you 
> hesitate between 1 bivariate model and a univariate one. As W is different 
> from X and Z, usual criterion such as AIC or BIC do not work. 
> if you had to quickly choose between the two models, would you rely more 
> on the R-squared or on the t-stat?
> my issue is that model with X and Z provide a higher adjusted R-squared 
> but low and non significant t-stat while W yields a significant t-stat but 
> a lower R-squared

IIRC the only proper way to do this comparison is to set up an
encompassing model, ie y ~ x + z + w.  There is a lot in the
literature, search for example for 'J test'. 

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From patrick at burns-stat.com  Mon Sep 24 17:22:55 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon, 24 Sep 2007 16:22:55 +0100
Subject: [R-SIG-Finance] R-squared or t-stat?
In-Reply-To: <OFC9FEF692.23A46BD3-ON80257360.0040213D-C1257360.00407C1C@db.com>
References: <OFC9FEF692.23A46BD3-ON80257360.0040213D-C1257360.00407C1C@db.com>
Message-ID: <46F7D64F.5000605@burns-stat.com>

I'm not so sure that your analysis of AIC and BIC
is correct.  But in any case I would suggest cross
validation, which is discussed in

http://www.burns-stat.com/pages/Tutor/bootstrap_resampling.html

If that is too slow in general for your purpose (you'd
need to be in a big hurry, I think), then you could
experiment to see which criterion tended to agree best
with cross validation.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Nicolas Mougeot wrote:

>Hi,
>
>let us assume that you want to regress Y on either X and Z or W, ie you 
>hesitate between 1 bivariate model and a univariate one. As W is different 
>from X and Z, usual criterion such as AIC or BIC do not work. 
>if you had to quickly choose between the two models, would you rely more 
>on the R-squared or on the t-stat?
>my issue is that model with X and Z provide a higher adjusted R-squared 
>but low and non significant t-stat while W yields a significant t-stat but 
>a lower R-squared
>
>Kind regards,
>
>Nicolas
>
>
>---
>
>This e-mail may contain confidential and/or privileged infor...{{dropped}}
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From strongthinking at gmail.com  Mon Sep 24 18:00:35 2007
From: strongthinking at gmail.com (Strong)
Date: Tue, 25 Sep 2007 01:00:35 +0900 (JST)
Subject: [R-SIG-Finance] Expanding data to higher frequency
In-Reply-To: <9b5c86780709240122x48427b6cy20b713674bd6b902@mail.gmail.com>
References: <9b5c86780709240122x48427b6cy20b713674bd6b902@mail.gmail.com>
Message-ID: <20070925.010035.149801197.strongthinking@srv.cc.hit-u.ac.jp>

From: "Thinking Strong" <strongthinking at gmail.com>
Subject: [R-SIG-Finance] Expanding data to higher frequency
Date: Mon, 24 Sep 2007 17:22:49 +0900

Maybe I should put my question more clearly.

What I am trying to ask is whether or not there is a function in Rmetrics to help me expand the semi-annual dividend series to the monthly data by setting the value of each month to the dividend for that  semi-annual period.

Suppose I have a dataframe shown below.  

is there an easy way to set all the dividend value from Jan 2002 through June 2002 to the same value of 100? 


   Time      Dividend
  2002.01     100
  2002.02     NA
  2002.03     NA
  2002.04     NA
  2002.05     NA
  2002.06     NA
  2002.07     200 
  2002.08     NA
  2002.09     NA
  2002.10     NA
  2002.11     NA
  2002.12     NA

And, one more related question. 

if I have the dataframe originally like this instead of the above one, what should I do with my job. 

   Time      Dividend
  2002.01     100
  2002.07     200 
 
 
Regards


Strong Chen
phd Student,  Graduate School of Commerce and Management;
Hitotusbashi University, Tokyo, Japan


> Dear All
> 
> I have difficulty with expanding the semi-annual dividend data to the
> monthly data by setting  the value of each month to the semi-annual data for
> that period.
> 
> I am wondering there is a function in R to do this kind of job.
> 
> Thanks in advance.
> 
> 
> Strong Chen
> phd Student,  Graduate School of Commerce and Management;
> Hitotusbashi University, Tokyo, Japan
> 
> 	[[alternative HTML version deleted]]
> 
>


From strongthinking at gmail.com  Mon Sep 24 18:06:35 2007
From: strongthinking at gmail.com (Strong)
Date: Tue, 25 Sep 2007 01:06:35 +0900 (JST)
Subject: [R-SIG-Finance] Problem with the timeDate function
In-Reply-To: <468647.79905.qm@web32202.mail.mud.yahoo.com>
References: <20070922.233027.226732933.strongthinking@srv.cc.hit-u.ac.jp>
	<468647.79905.qm@web32202.mail.mud.yahoo.com>
Message-ID: <20070925.010635.55831062.strongthinking@srv.cc.hit-u.ac.jp>

From: Moshe Olshansky <m_olshansky at yahoo.com>
Subject: Re: [R-SIG-Finance] Problem with the timeDate function
Date: Sun, 23 Sep 2007 17:14:04 -0700 (PDT)

Thanks for your reply.

But,  what should I do to add a day to my original 1990.01 date string within R.

And, is there a way to convert the period in my date string to hyphen?



Strong Chen 

> Hello,
> 
> I am also new to Rmetrics so take what I say with
> caution.
> The string "1990.01" is not a valid date - you must
> add a day also (even if you do not care about the day
> add a dummy one, let say 1, so that you date is
> "1990.01.01"). This is a valid date but then you have
> another problem (as indicated by your error message) -
> if you specify date only then the format is assumed to
> be "%Y-%m-%d" and so a period (.) can not be a
> separator. So you can either specify the date as
> "1990-01-01" or supply a valid time as well, i.e.
> timeDate("1990.02.01.00.00.01",format="%Y.%m.%d.%H.%M.%S").
> 
> Regards,
> 
> Moshe.
>  
> --- Strong <strongthinking at gmail.com> wrote:
> 
> > 
> > Dear All
> > 
> > I am a beginner of Rmetrics.  It is the first time I
> > posts on the mailing list. 
> > 
> > I am having a problem with the use of timeDate
> > function. 
> > 
> > I tried to use timeDate function in the fCalendar
> > package to convert a character string of date to
> > into the timeDate object.
> > 
> > My script is something like the followings:
> > 
> > < date <- c("1990.01", "1990.02")
> > 
> > < date.td <- timeDate(date, format="%Y.%m")
> > 
> > And then, I got the following error message:
> > 
> > Error in if (sum(lt$sec + lt$min + lt$hour) == 0)
> > isoFormat = "%Y-%m-%d" : 
> > 	missing value where TRUE/FALSE needed
> > In addition: Warning message:
> > Unknown Format Specification in:
> > .whichFormat(charvec) 
> > 
> > I don't know what's wrong with my format
> > specification.  It seems the reason is I am not
> > using the standard iso format,  although I do
> > believe the flexible specification of time format is
> > allowed by the timeDate function just like in
> > S-Plus.
> > 
> > Can anyone help me?
> > 
> > Thanks in advance.
> > 
> > 
> > Strong Chen
> > phd Student,  Graduate School of Commerce and
> > Management;
> > Hitotusbashi University, Tokyo, Japan
> > 
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only. 
> > -- If you want to post, subscribe first.
> >
> 
>


From feanor0 at hotmail.com  Mon Sep 24 18:45:32 2007
From: feanor0 at hotmail.com (Murali Menon)
Date: Mon, 24 Sep 2007 16:45:32 +0000
Subject: [R-SIG-Finance] Rolling functions on matrices
Message-ID: <BLU105-W354C81F3D745E01A81DC7BEEB60@phx.gbl>


Folks,

I'm a bit stymied by the following problem. I have a matrix of returns for various equities, and a matrix of implied vols for these equities. So, e.g.:

returns <- matrix(runif(40), ncol = 4)
colnames(returns) <- c("IBM", "MOT", "NOK", "INTC")
vols <- matrix(rnorm(40, 7, 1), ncol = 4)
colnames(vols) <- colnames(returns)

#I need weighted MA over lags of 2, 16, 32

lagvals <- c(2, 16, 32)
coeff <- sapply(lagvals , function(n) c(rep(0, max(lagvals ) - n), 1 : n))
colnames(coeff) <- as.character(paste("MA", lagvals , sep = ""))

Now I'd like to compute weighted returns for each equity i and each lag index j such that at each point in time (t), I have:

wtRet[t, i, j] = returns[t, i] / vols[t, i] * average.vol.over.past.lag.days

So for lag = 32, i would compute average.vol.over.past.lag.days = mean(vols[(t - 32 + 1) : t, i]),

and so on for the other lags.

And then, finally, do a rolling weighted mean where the weighting for each lag is given by the coeff matrix above.

I hope this is not incomprehensible!

Any suggestions? I'm getting lost with all the indexation required between equity and lags, and
indeed the rolling function to apply.

Also, for my real application, the matrices are biggish (10-20 years), so if I can minimise any computational time, that would be an added bonus.

Thanks very much,
Murali
_________________________________________________________________

 Hotmail?. NOW with 5GB storage.

ration_HM_mini_5G_0907

From brian at braverock.com  Mon Sep 24 19:06:13 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 24 Sep 2007 12:06:13 -0500
Subject: [R-SIG-Finance] Rolling functions on matrices
In-Reply-To: <BLU105-W354C81F3D745E01A81DC7BEEB60@phx.gbl>
References: <BLU105-W354C81F3D745E01A81DC7BEEB60@phx.gbl>
Message-ID: <46F7EE85.9060108@braverock.com>

There are many examples on the archives of this list for using the 
rollapply function.  Perhaps you could start there.

Regards,

    - Brian

Murali Menon wrote:
> Folks,
> 
> I'm a bit stymied by the following problem. I have a matrix of returns for various equities, and a matrix of implied vols for these equities. So, e.g.:
> 
> returns <- matrix(runif(40), ncol = 4)
> colnames(returns) <- c("IBM", "MOT", "NOK", "INTC")
> vols <- matrix(rnorm(40, 7, 1), ncol = 4)
> colnames(vols) <- colnames(returns)
> 
> #I need weighted MA over lags of 2, 16, 32
> 
> lagvals <- c(2, 16, 32)
> coeff <- sapply(lagvals , function(n) c(rep(0, max(lagvals ) - n), 1 : n))
> colnames(coeff) <- as.character(paste("MA", lagvals , sep = ""))
> 
> Now I'd like to compute weighted returns for each equity i and each lag index j such that at each point in time (t), I have:
> 
> wtRet[t, i, j] = returns[t, i] / vols[t, i] * average.vol.over.past.lag.days
> 
> So for lag = 32, i would compute average.vol.over.past.lag.days = mean(vols[(t - 32 + 1) : t, i]),
> 
> and so on for the other lags.
> 
> And then, finally, do a rolling weighted mean where the weighting for each lag is given by the coeff matrix above.
> 
> I hope this is not incomprehensible!
> 
> Any suggestions? I'm getting lost with all the indexation required between equity and lags, and
> indeed the rolling function to apply.
> 
> Also, for my real application, the matrices are biggish (10-20 years), so if I can minimise any computational time, that would be an added bonus.
> 
> Thanks very much,
> Murali
> _________________________________________________________________
> 
>  Hotmail?. NOW with 5GB storage.
> 
> ration_HM_mini_5G_0907
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.


From strongthinking at gmail.com  Tue Sep 25 14:14:55 2007
From: strongthinking at gmail.com (Strong)
Date: Tue, 25 Sep 2007 21:14:55 +0900 (JST)
Subject: [R-SIG-Finance] Problem with the timeDate function
In-Reply-To: <20070925.010635.55831062.strongthinking@srv.cc.hit-u.ac.jp>
References: <20070922.233027.226732933.strongthinking@srv.cc.hit-u.ac.jp>
	<468647.79905.qm@web32202.mail.mud.yahoo.com>
	<20070925.010635.55831062.strongthinking@srv.cc.hit-u.ac.jp>
Message-ID: <20070925.211455.123970069.strongthinking@srv.cc.hit-u.ac.jp>

From: Strong <strongthinking at gmail.com>
Subject: Re: [R-SIG-Finance] Problem with the timeDate function
Date: Tue, 25 Sep 2007 01:06:35 +0900 (JST)

Dear all

I have already found a way to do it myself.

Suppose we have a character vector date like the following:

"2006.01" "2006.02" "2006.03" "2006.04" "2006.05" "2006.06"

First, use the function gsub to replace the "." with "-". 

date <- gsub("/", "-", date)

And then, use the function paste to add a date to it, say "01".

paste(date, "-01", sep="")

Finally, you can convert it to timeDate object by using function timeDate without any error message!

date.td <- timeDate(date, format="%Y-%m-%d")



Regards

Strong Chen


> From: Moshe Olshansky <m_olshansky at yahoo.com>
> Subject: Re: [R-SIG-Finance] Problem with the timeDate function
> Date: Sun, 23 Sep 2007 17:14:04 -0700 (PDT)
> 
> Thanks for your reply.
> 
> But,  what should I do to add a day to my original 1990.01 date string within R.
> 
> And, is there a way to convert the period in my date string to hyphen?
> 
> 
> 
> Strong Chen 
> 
> > Hello,
> > 
> > I am also new to Rmetrics so take what I say with
> > caution.
> > The string "1990.01" is not a valid date - you must
> > add a day also (even if you do not care about the day
> > add a dummy one, let say 1, so that you date is
> > "1990.01.01"). This is a valid date but then you have
> > another problem (as indicated by your error message) -
> > if you specify date only then the format is assumed to
> > be "%Y-%m-%d" and so a period (.) can not be a
> > separator. So you can either specify the date as
> > "1990-01-01" or supply a valid time as well, i.e.
> > timeDate("1990.02.01.00.00.01",format="%Y.%m.%d.%H.%M.%S").
> > 
> > Regards,
> > 
> > Moshe.
> >  
> > --- Strong <strongthinking at gmail.com> wrote:
> > 
> > > 
> > > Dear All
> > > 
> > > I am a beginner of Rmetrics.  It is the first time I
> > > posts on the mailing list. 
> > > 
> > > I am having a problem with the use of timeDate
> > > function. 
> > > 
> > > I tried to use timeDate function in the fCalendar
> > > package to convert a character string of date to
> > > into the timeDate object.
> > > 
> > > My script is something like the followings:
> > > 
> > > < date <- c("1990.01", "1990.02")
> > > 
> > > < date.td <- timeDate(date, format="%Y.%m")
> > > 
> > > And then, I got the following error message:
> > > 
> > > Error in if (sum(lt$sec + lt$min + lt$hour) == 0)
> > > isoFormat = "%Y-%m-%d" : 
> > > 	missing value where TRUE/FALSE needed
> > > In addition: Warning message:
> > > Unknown Format Specification in:
> > > .whichFormat(charvec) 
> > > 
> > > I don't know what's wrong with my format
> > > specification.  It seems the reason is I am not
> > > using the standard iso format,  although I do
> > > believe the flexible specification of time format is
> > > allowed by the timeDate function just like in
> > > S-Plus.
> > > 
> > > Can anyone help me?
> > > 
> > > Thanks in advance.
> > > 
> > > 
> > > Strong Chen
> > > phd Student,  Graduate School of Commerce and
> > > Management;
> > > Hitotusbashi University, Tokyo, Japan
> > > 
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only. 
> > > -- If you want to post, subscribe first.
> > >
> > 
> >
> 
>


From alexander.f.moreno at gmail.com  Wed Sep 26 04:52:03 2007
From: alexander.f.moreno at gmail.com (Alexander Moreno)
Date: Tue, 25 Sep 2007 21:52:03 -0500
Subject: [R-SIG-Finance] are R packages open-source?
Message-ID: <3303a4570709251952h6105616fh2960458011869fdb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070925/a1413728/attachment.pl 

From jeff.a.ryan at gmail.com  Wed Sep 26 05:09:54 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 25 Sep 2007 22:09:54 -0500
Subject: [R-SIG-Finance] are R packages open-source?
In-Reply-To: <3303a4570709251952h6105616fh2960458011869fdb@mail.gmail.com>
References: <3303a4570709251952h6105616fh2960458011869fdb@mail.gmail.com>
Message-ID: <e8e755250709252009gff06198gd659a63b5c489a45@mail.gmail.com>

Alex,

Would be best to be more specific - i.e. maybe which package/functions...

That said, all packages on CRAN are source code, with binaries built
on post-submission for Mac and Windows.  Simply download the .tar.gz
file from CRAN's contributed section, and untar:

Unix and Mac:

tar xzf name_of_package.tar.gz

_OR_

gtar xzf name_of_package.tar.gz

Windows must have similar facility, though I personally have no idea.

Anyway all code would then be somewhere in the directory that follows
the tar command.  Source meant to be compiled (C and Fortan) is in the
src directory.

Good luck.
Jeff

On 9/25/07, Alexander Moreno <alexander.f.moreno at gmail.com> wrote:
> Hi,
>
> I'm trying to modify some of the commands in add-on packages, and I'm
> wondering if they are open source, which would allow me to do that?  If so,
> does anyone know how I could access the code?
>
> Best,
> Alex
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Wed Sep 26 05:11:34 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 25 Sep 2007 22:11:34 -0500
Subject: [R-SIG-Finance] are R packages open-source?
In-Reply-To: <3303a4570709251952h6105616fh2960458011869fdb@mail.gmail.com>
References: <3303a4570709251952h6105616fh2960458011869fdb@mail.gmail.com>
Message-ID: <e8e755250709252011x51aabac8w4ba8a4e4abe2c52@mail.gmail.com>

An aside.

Open source != open to modification.  Make sure to read the license.

Jeff

On 9/25/07, Alexander Moreno <alexander.f.moreno at gmail.com> wrote:
> Hi,
>
> I'm trying to modify some of the commands in add-on packages, and I'm
> wondering if they are open source, which would allow me to do that?  If so,
> does anyone know how I could access the code?
>
> Best,
> Alex
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From adschai at optonline.net  Wed Sep 26 05:42:04 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Wed, 26 Sep 2007 03:42:04 +0000 (GMT)
Subject: [R-SIG-Finance] backtesting engine (full-blown) application
Message-ID: <e4fc8deafe3b.46f9d50c@optonline.net>

Hi - I guess you might have experience or suggestion on this subject. I am looking for an application vendor that supports strategy creation, backtesting and also offers actual AES functionality. Also, pre-trade and post-trade analysis should be supported. I'm wondering if there is any vendor that provides a good framework at reasonable $$ or not. Any suggestion would be really appreciated. Thank you.

- adschai


From m_olshansky at yahoo.com  Wed Sep 26 06:34:22 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Tue, 25 Sep 2007 21:34:22 -0700 (PDT)
Subject: [R-SIG-Finance] RBloomberg equity options prices
Message-ID: <896988.77256.qm@web32209.mail.mud.yahoo.com>

Hello,

Is it possible to get historical option prices using
RBloomberg? Is it possible to get intraday data?

Thank you!

Moshe Olshansky.


From m_olshansky at yahoo.com  Wed Sep 26 07:48:25 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Tue, 25 Sep 2007 22:48:25 -0700 (PDT)
Subject: [R-SIG-Finance] backtesting engine (full-blown) application
In-Reply-To: <e4fc8deafe3b.46f9d50c@optonline.net>
Message-ID: <429869.96297.qm@web32201.mail.mud.yahoo.com>

Hi Adschai,

About 2 years ago (while working for Brevan Howard) we
looked at some such applications and even had a couple
of them installed. I can ask my former colleagues for
more details.
However, keep in mind that it will take time to learn
to use the application and to get sure that you can
trust there analysis. Moreover, depending on how
complex (mathematically) your trading strategy is,
they may or may not have all the
(mathematical/statistical) functions you need
So if you can easily get the historical data and need
to check just a few strategies you may end up doing it
faster using R (or Matlab).

Regards,

Moshe.

--- adschai at optonline.net wrote:

> Hi - I guess you might have experience or suggestion
> on this subject. I am looking for an application
> vendor that supports strategy creation, backtesting
> and also offers actual AES functionality. Also,
> pre-trade and post-trade analysis should be
> supported. I'm wondering if there is any vendor that
> provides a good framework at reasonable $$ or not.
> Any suggestion would be really appreciated. Thank
> you.
> 
> - adschai
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From brian at braverock.com  Wed Sep 26 13:26:46 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 26 Sep 2007 06:26:46 -0500
Subject: [R-SIG-Finance] are R packages open-source?
In-Reply-To: <3303a4570709251952h6105616fh2960458011869fdb@mail.gmail.com>
References: <3303a4570709251952h6105616fh2960458011869fdb@mail.gmail.com>
Message-ID: <46FA41F6.7080107@braverock.com>

Alexander Moreno wrote:
> I'm trying to modify some of the commands in add-on packages, and I'm
> wondering if they are open source, which would allow me to do that?  If so,
> does anyone know how I could access the code?

Jeff has already mostly answered your question, but I'll provide what 
may be a bit of clarification.

All R core packages and R itself are released under the GNU Public 
License (GPL), and you are thus free to modify their code.  There are 
other restrictions on attribution and distribution in the GPL that are 
spelled out in the license but basically come down to crediting the 
source, noting your changes, and distributing your modifications always 
under the GPL as well.

The vast majority of CRAN packages for R are also distributed under the 
GPL, also allowing you to change the source.  As Jeff has pointed out, 
you need to check the License section for the package you're looking at 
either on CRAN or in the downloaded package itself.  Feel free to ask on 
this list if you're not clear on the licensing status of any particular 
license or package.

In something that might otherwise go unsaid: If you make changes to a 
package to either fix a bug or enhance the functionality, *please* give 
back those changes to the community that you have benefited so much from 
by sending them to the package maintainer at least, and 
possibly/probably also posting your modifications to this list if they 
are finance-related. Everyone benefits when we share our work and code 
on published analytical techniques, even though the professional 
investors among us will continue to compete in the markets.

Regards,

   - Brian


From brian at braverock.com  Wed Sep 26 13:39:25 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 26 Sep 2007 06:39:25 -0500
Subject: [R-SIG-Finance] backtesting engine (full-blown) application
In-Reply-To: <e4fc8deafe3b.46f9d50c@optonline.net>
References: <e4fc8deafe3b.46f9d50c@optonline.net>
Message-ID: <46FA44ED.2060804@braverock.com>

adschai at optonline.net wrote:
> Hi - I guess you might have experience or suggestion on this subject. I am looking for an application vendor that supports strategy creation, backtesting and also offers actual AES functionality. Also, pre-trade and post-trade analysis should be supported. I'm wondering if there is any vendor that provides a good framework at reasonable $$ or not. Any suggestion would be really appreciated. Thank you.


There are many many commercial portfolio back testing engines available. 
  They range from pretty good to worthless.
One key question you'll need to ask is what instruments, data feeds, 
other systems, etc. you need to integrate to.  Many of the less 
expensive systems have very limited capabilities for getting data in an 
out, placing all the effort on you to get the data, feed it into the 
system, make your trades, and then feed the results back into the 
optimizer.  This is why many small-to-medium sized asset managers build 
their own infrastructure.

There are some R packages available to aid you in the build-your-own 
approach, and I'm sure the community would continue to contribute to 
building an open framework for doing portfolio analysis.

Regards,

   - Brian


From ryan.sheftel at malbecpartners.com  Wed Sep 26 13:54:52 2007
From: ryan.sheftel at malbecpartners.com (Ryan Sheftel)
Date: Wed, 26 Sep 2007 07:54:52 -0400
Subject: [R-SIG-Finance] backtesting engine (full-blown) application
In-Reply-To: <46FA44ED.2060804@braverock.com>
Message-ID: <OFB6F25CB6.560D0267-ON85257362.0040A839-85257362.00418C63@fftw.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070926/54f09da4/attachment.pl 

From brian at braverock.com  Wed Sep 26 14:04:15 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 26 Sep 2007 07:04:15 -0500
Subject: [R-SIG-Finance] backtesting engine (full-blown) application
In-Reply-To: <OFB6F25CB6.560D0267-ON85257362.0040A839-85257362.00418C63@fftw.com>
References: <OFB6F25CB6.560D0267-ON85257362.0040A839-85257362.00418C63@fftw.com>
Message-ID: <46FA4ABF.5040503@braverock.com>

Ryan Sheftel wrote:
> For live trading R (or even matlab) would not work stand alone if you 
> are monitoring a large number of markets because they are single 
> threaded and you would need to wrap it somehow with an event based 
> language or would need one R process running per market, which could 
> lead to 500 R processes if you are monitoring and trading the stocks in 
> the SP 500.

R from the command line interpreter is single-command-at-a-time (which 
is different from single-threaded, many R core analytical routines are 
multi-threaded).  There are also many methods available to make R run as 
a server process listening on a socket or on a cluster. It would be 
incorrect to simply classify R as a single-threaded application.  There 
is, as Ryan points out, programming effort involved to work in a 
clustered or multi-core environment, and wrapping R/DCOM with an 
event-based language is one approach to achieving this, but not the only 
one.

Regards,

    - Brian


From ryan.sheftel at malbecpartners.com  Wed Sep 26 14:08:14 2007
From: ryan.sheftel at malbecpartners.com (Ryan Sheftel)
Date: Wed, 26 Sep 2007 08:08:14 -0400
Subject: [R-SIG-Finance] backtesting engine (full-blown) application
In-Reply-To: <46FA4ABF.5040503@braverock.com>
Message-ID: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070926/bf5e8056/attachment.pl 

From brian at braverock.com  Wed Sep 26 14:40:35 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 26 Sep 2007 07:40:35 -0500
Subject: [R-SIG-Finance] Running R as a server or in a cluster
In-Reply-To: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>
References: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>
Message-ID: <46FA5343.1020101@braverock.com>

Ryan Sheftel wrote:
> "There are also many methods available to make R run as
> a server process listening on a socket or on a cluster. It would be
> incorrect to simply classify R as a single-threaded application.  There
> is, as Ryan points out, programming effort involved to work in a
> clustered or multi-core environment, and wrapping R/DCOM with an
> event-based language is one approach to achieving this, but not the only
> one."
> 
> Can you share your favorite method? I would be interested to learn as 
> this is a problem we are facing. If it is proprietary of involved I 
> understand.

I believe in sharing "generally useful" approaches (and code).  Trading 
models are, of course, proprietary. ;)

Several groups I know make extensive use of the Rserve package
http://cran.r-project.org/src/contrib/Descriptions/Rserve.html
The Rserve model fits in well with integration to some other programming 
language that would drive the interactions and fire off requests to some 
large server.

There is also R.rsp for ASP/JSP like programming:
http://cran.r-project.org/src/contrib/Descriptions/R.rsp.html

One of the simplest approaches to clustering is to use the snow library:

http://cran.r-project.org/src/contrib/Descriptions/snow.html

Since this doesn't require a formal cluster.

Dirk's Quantian project installs the Mosix cluster and R by default.

For true grid/cluster integration, the most advanced set of features I'm 
aware of is offered by the various Parallel-R libraries, which are used 
extensively in scientific computing clusters:

http://rss.acs.unt.edu/Rdoc/library/taskPR/html/00Index.html

Perhaps others on the list will fill us in on approaches I've missed.

Regards,

   - Brian


From adrian_d at eskimo.com  Wed Sep 26 15:08:12 2007
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Wed, 26 Sep 2007 06:08:12 -0700 (PDT)
Subject: [R-SIG-Finance] Running R as a server or in a cluster
In-Reply-To: <46FA5343.1020101@braverock.com>
References: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>
	<46FA5343.1020101@braverock.com>
Message-ID: <Pine.SUN.4.58.0709260604270.4940@eskimo.com>


We have set up a Condor cluster, see http://www.cs.wisc.edu/condor/ and we
submit R jobs to the cluster.  It works well because Condor has very
advanced scheduling capabilities, job monitoring, etc.

Adrian


On Wed, 26 Sep 2007, Brian G. Peterson wrote:

> Ryan Sheftel wrote:
> > "There are also many methods available to make R run as
> > a server process listening on a socket or on a cluster. It would be
> > incorrect to simply classify R as a single-threaded application.  There
> > is, as Ryan points out, programming effort involved to work in a
> > clustered or multi-core environment, and wrapping R/DCOM with an
> > event-based language is one approach to achieving this, but not the only
> > one."
> >
> > Can you share your favorite method? I would be interested to learn as
> > this is a problem we are facing. If it is proprietary of involved I
> > understand.
>
> I believe in sharing "generally useful" approaches (and code).  Trading
> models are, of course, proprietary. ;)
>
> Several groups I know make extensive use of the Rserve package
> http://cran.r-project.org/src/contrib/Descriptions/Rserve.html
> The Rserve model fits in well with integration to some other programming
> language that would drive the interactions and fire off requests to some
> large server.
>
> There is also R.rsp for ASP/JSP like programming:
> http://cran.r-project.org/src/contrib/Descriptions/R.rsp.html
>
> One of the simplest approaches to clustering is to use the snow library:
>
> http://cran.r-project.org/src/contrib/Descriptions/snow.html
>
> Since this doesn't require a formal cluster.
>
> Dirk's Quantian project installs the Mosix cluster and R by default.
>
> For true grid/cluster integration, the most advanced set of features I'm
> aware of is offered by the various Parallel-R libraries, which are used
> extensively in scientific computing clusters:
>
> http://rss.acs.unt.edu/Rdoc/library/taskPR/html/00Index.html
>
> Perhaps others on the list will fill us in on approaches I've missed.
>
> Regards,
>
>    - Brian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From brian at braverock.com  Wed Sep 26 15:24:53 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 26 Sep 2007 08:24:53 -0500
Subject: [R-SIG-Finance] Running R as a server or in a cluster
In-Reply-To: <Pine.SUN.4.58.0709260604270.4940@eskimo.com>
References: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>
	<46FA5343.1020101@braverock.com>
	<Pine.SUN.4.58.0709260604270.4940@eskimo.com>
Message-ID: <46FA5DA5.70101@braverock.com>

Adrian Dragulescu wrote:
> We have set up a Condor cluster, see http://www.cs.wisc.edu/condor/ and we
> submit R jobs to the cluster.  It works well because Condor has very
> advanced scheduling capabilities, job monitoring, etc.

Adrian,

Could you provide more details?  Are you running Rserve on the cluster, 
running "R CMD BATCH", or using Parallel-R?

I'd like to suggest that we use this thread to continue to develop the 
collective knowledge of the r-sig-finance community on distributed or 
high-throughput R calculations.

Regards,

    - Brian


From jeff.a.ryan at gmail.com  Wed Sep 26 16:08:18 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 26 Sep 2007 09:08:18 -0500
Subject: [R-SIG-Finance] Running R as a server or in a cluster
In-Reply-To: <46FA5DA5.70101@braverock.com>
References: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>
	<46FA5343.1020101@braverock.com>
	<Pine.SUN.4.58.0709260604270.4940@eskimo.com>
	<46FA5DA5.70101@braverock.com>
Message-ID: <e8e755250709260708j68b0566dqeff74f0e03b7fecf@mail.gmail.com>

Hi all,

Short of answers, but I do wonder if anyone has used Sun Microsystems
www.network.com for grid work with R.  At 1USD a CPU hr, with R
already built - and a working example script on the service - it seems
like a path worth exploring.

Has anyone given it a try.  I set up an account, but have yet to get
the opportunity to try it out.

Here is the link:

http://www.network.com/apps/r_project.html

Jeff Ryan

On 9/26/07, Brian G. Peterson <brian at braverock.com> wrote:
> Adrian Dragulescu wrote:
> > We have set up a Condor cluster, see http://www.cs.wisc.edu/condor/ and we
> > submit R jobs to the cluster.  It works well because Condor has very
> > advanced scheduling capabilities, job monitoring, etc.
>
> Adrian,
>
> Could you provide more details?  Are you running Rserve on the cluster,
> running "R CMD BATCH", or using Parallel-R?
>
> I'd like to suggest that we use this thread to continue to develop the
> collective knowledge of the r-sig-finance community on distributed or
> high-throughput R calculations.
>
> Regards,
>
>     - Brian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Wed Sep 26 16:13:25 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 26 Sep 2007 09:13:25 -0500
Subject: [R-SIG-Finance] Running R as a server or in a cluster
In-Reply-To: <e8e755250709260708j68b0566dqeff74f0e03b7fecf@mail.gmail.com>
References: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>
	<46FA5343.1020101@braverock.com>
	<Pine.SUN.4.58.0709260604270.4940@eskimo.com>
	<46FA5DA5.70101@braverock.com>
	<e8e755250709260708j68b0566dqeff74f0e03b7fecf@mail.gmail.com>
Message-ID: <e8e755250709260713i224ea0c8g4e88811ca8fcb6df@mail.gmail.com>

About Sun's service,

Even better (not for me, as I've already signed up), is that they seem
to have a promo for a free 200 hrs on the www.network.com cluster.

Jeff

On 9/26/07, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Hi all,
>
> Short of answers, but I do wonder if anyone has used Sun Microsystems
> www.network.com for grid work with R.  At 1USD a CPU hr, with R
> already built - and a working example script on the service - it seems
> like a path worth exploring.
>
> Has anyone given it a try.  I set up an account, but have yet to get
> the opportunity to try it out.
>
> Here is the link:
>
> http://www.network.com/apps/r_project.html
>
> Jeff Ryan
>
> On 9/26/07, Brian G. Peterson <brian at braverock.com> wrote:
> > Adrian Dragulescu wrote:
> > > We have set up a Condor cluster, see http://www.cs.wisc.edu/condor/ and we
> > > submit R jobs to the cluster.  It works well because Condor has very
> > > advanced scheduling capabilities, job monitoring, etc.
> >
> > Adrian,
> >
> > Could you provide more details?  Are you running Rserve on the cluster,
> > running "R CMD BATCH", or using Parallel-R?
> >
> > I'd like to suggest that we use this thread to continue to develop the
> > collective knowledge of the r-sig-finance community on distributed or
> > high-throughput R calculations.
> >
> > Regards,
> >
> >     - Brian
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From josh at gghc.com  Wed Sep 26 16:13:15 2007
From: josh at gghc.com (Joshua Reich)
Date: Wed, 26 Sep 2007 10:13:15 -0400
Subject: [R-SIG-Finance] Running R as a server or in a cluster
In-Reply-To: <e8e755250709260708j68b0566dqeff74f0e03b7fecf@mail.gmail.com>
References: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com><46FA5343.1020101@braverock.com><Pine.SUN.4.58.0709260604270.4940@eskimo.com><46FA5DA5.70101@braverock.com>
	<e8e755250709260708j68b0566dqeff74f0e03b7fecf@mail.gmail.com>
Message-ID: <C20EA84D76C94F4E999DC041E81C0D110256EC81@exchange2k3.ny.gghc.com>

We recently set up a similar environment using Amazon's EC2 service.
They charge $0.1 per CPU hour. I can't say what our results have been
like yet - still ironing out the kinks in our R code. But I will
certainly let you all know how it goes.

Our 'clustering' mechanism is very simple. We have written perl scripts
that receive data over HTTP, start R, process the data, and then post
the results back via HTTP to a central server. 

Josh 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Jeff Ryan
Sent: Wednesday, September 26, 2007 10:08 AM
To: Brian G. Peterson
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Running R as a server or in a cluster

Hi all,

Short of answers, but I do wonder if anyone has used Sun Microsystems
www.network.com for grid work with R.  At 1USD a CPU hr, with R already
built - and a working example script on the service - it seems like a
path worth exploring.

Has anyone given it a try.  I set up an account, but have yet to get the
opportunity to try it out.

Here is the link:

http://www.network.com/apps/r_project.html

Jeff Ryan

On 9/26/07, Brian G. Peterson <brian at braverock.com> wrote:
> Adrian Dragulescu wrote:
> > We have set up a Condor cluster, see http://www.cs.wisc.edu/condor/ 
> > and we submit R jobs to the cluster.  It works well because Condor 
> > has very advanced scheduling capabilities, job monitoring, etc.
>
> Adrian,
>
> Could you provide more details?  Are you running Rserve on the 
> cluster, running "R CMD BATCH", or using Parallel-R?
>
> I'd like to suggest that we use this thread to continue to develop the

> collective knowledge of the r-sig-finance community on distributed or 
> high-throughput R calculations.
>
> Regards,
>
>     - Brian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Wed Sep 26 16:17:47 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 26 Sep 2007 09:17:47 -0500
Subject: [R-SIG-Finance] Running R as a server or in a cluster
In-Reply-To: <C20EA84D76C94F4E999DC041E81C0D110256EC81@exchange2k3.ny.gghc.com>
References: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>
	<46FA5343.1020101@braverock.com>
	<Pine.SUN.4.58.0709260604270.4940@eskimo.com>
	<46FA5DA5.70101@braverock.com>
	<e8e755250709260708j68b0566dqeff74f0e03b7fecf@mail.gmail.com>
	<C20EA84D76C94F4E999DC041E81C0D110256EC81@exchange2k3.ny.gghc.com>
Message-ID: <e8e755250709260717l342473abvd6d4a2f9246e972b@mail.gmail.com>

I do know the Sun one is using their grid software, and is supposedly
highly secure.  Basically have access to a 2000 node opteron cluster.

The Amazon one seems to be more of using a machine, one at a time.  Is
that correct?


On 9/26/07, Joshua Reich <josh at gghc.com> wrote:
> We recently set up a similar environment using Amazon's EC2 service.
> They charge $0.1 per CPU hour. I can't say what our results have been
> like yet - still ironing out the kinks in our R code. But I will
> certainly let you all know how it goes.
>
> Our 'clustering' mechanism is very simple. We have written perl scripts
> that receive data over HTTP, start R, process the data, and then post
> the results back via HTTP to a central server.
>
> Josh
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Jeff Ryan
> Sent: Wednesday, September 26, 2007 10:08 AM
> To: Brian G. Peterson
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Running R as a server or in a cluster
>
> Hi all,
>
> Short of answers, but I do wonder if anyone has used Sun Microsystems
> www.network.com for grid work with R.  At 1USD a CPU hr, with R already
> built - and a working example script on the service - it seems like a
> path worth exploring.
>
> Has anyone given it a try.  I set up an account, but have yet to get the
> opportunity to try it out.
>
> Here is the link:
>
> http://www.network.com/apps/r_project.html
>
> Jeff Ryan
>
> On 9/26/07, Brian G. Peterson <brian at braverock.com> wrote:
> > Adrian Dragulescu wrote:
> > > We have set up a Condor cluster, see http://www.cs.wisc.edu/condor/
> > > and we submit R jobs to the cluster.  It works well because Condor
> > > has very advanced scheduling capabilities, job monitoring, etc.
> >
> > Adrian,
> >
> > Could you provide more details?  Are you running Rserve on the
> > cluster, running "R CMD BATCH", or using Parallel-R?
> >
> > I'd like to suggest that we use this thread to continue to develop the
>
> > collective knowledge of the r-sig-finance community on distributed or
> > high-throughput R calculations.
> >
> > Regards,
> >
> >     - Brian
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From josh at gghc.com  Wed Sep 26 16:26:58 2007
From: josh at gghc.com (Joshua Reich)
Date: Wed, 26 Sep 2007 10:26:58 -0400
Subject: [R-SIG-Finance] Running R as a server or in a cluster
In-Reply-To: <e8e755250709260717l342473abvd6d4a2f9246e972b@mail.gmail.com>
References: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>
	<46FA5343.1020101@braverock.com>
	<Pine.SUN.4.58.0709260604270.4940@eskimo.com>
	<46FA5DA5.70101@braverock.com>
	<e8e755250709260708j68b0566dqeff74f0e03b7fecf@mail.gmail.com>
	<C20EA84D76C94F4E999DC041E81C0D110256EC81@exchange2k3.ny.gghc.com>
	<e8e755250709260717l342473abvd6d4a2f9246e972b@mail.gmail.com>
Message-ID: <C20EA84D76C94F4E999DC041E81C0D110256EC83@exchange2k3.ny.gghc.com>

Yes - its JBOC (just a bunch of computers). You provide them with a disk
image (of sorts) and they will load it on to as many computers as you
request. Images are loaded and machines are requested via a web services
API. Initially you can request up to 20 machines - but if you email them
you can ask for more. All network bandwidth between machines is free,
but there is a per GB transfer charge for external connectivity - I
can't recall what the rate is, but it is very reasonable. 

Not being a specialized grid environment, all inter-node communication
and scheduling has to be handled by your own application. But for the
price, that's not too bad.

While I was aware of SNOW, I'm not familiar with the other clustering
approaches mentioned earlier in this thread. What special sauce does Sun
provide to make running on a grid easier than running on a JBOC style
setup?

Josh

-----Original Message-----
From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com] 
Sent: Wednesday, September 26, 2007 10:18 AM
To: Joshua Reich
Cc: Brian G. Peterson; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Running R as a server or in a cluster

I do know the Sun one is using their grid software, and is supposedly
highly secure.  Basically have access to a 2000 node opteron cluster.

The Amazon one seems to be more of using a machine, one at a time.  Is
that correct?


On 9/26/07, Joshua Reich <josh at gghc.com> wrote:
> We recently set up a similar environment using Amazon's EC2 service.
> They charge $0.1 per CPU hour. I can't say what our results have been 
> like yet - still ironing out the kinks in our R code. But I will 
> certainly let you all know how it goes.
>
> Our 'clustering' mechanism is very simple. We have written perl 
> scripts that receive data over HTTP, start R, process the data, and 
> then post the results back via HTTP to a central server.
>
> Josh
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Jeff 
> Ryan
> Sent: Wednesday, September 26, 2007 10:08 AM
> To: Brian G. Peterson
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Running R as a server or in a cluster
>
> Hi all,
>
> Short of answers, but I do wonder if anyone has used Sun Microsystems 
> www.network.com for grid work with R.  At 1USD a CPU hr, with R 
> already built - and a working example script on the service - it seems

> like a path worth exploring.
>
> Has anyone given it a try.  I set up an account, but have yet to get 
> the opportunity to try it out.
>
> Here is the link:
>
> http://www.network.com/apps/r_project.html
>
> Jeff Ryan
>
> On 9/26/07, Brian G. Peterson <brian at braverock.com> wrote:
> > Adrian Dragulescu wrote:
> > > We have set up a Condor cluster, see 
> > > http://www.cs.wisc.edu/condor/ and we submit R jobs to the 
> > > cluster.  It works well because Condor has very advanced
scheduling capabilities, job monitoring, etc.
> >
> > Adrian,
> >
> > Could you provide more details?  Are you running Rserve on the 
> > cluster, running "R CMD BATCH", or using Parallel-R?
> >
> > I'd like to suggest that we use this thread to continue to develop 
> > the
>
> > collective knowledge of the r-sig-finance community on distributed 
> > or high-throughput R calculations.
> >
> > Regards,
> >
> >     - Brian
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Wed Sep 26 16:39:30 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 26 Sep 2007 09:39:30 -0500
Subject: [R-SIG-Finance] Running R as a server or in a cluster
In-Reply-To: <C20EA84D76C94F4E999DC041E81C0D110256EC83@exchange2k3.ny.gghc.com>
References: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>
	<46FA5343.1020101@braverock.com>
	<Pine.SUN.4.58.0709260604270.4940@eskimo.com>
	<46FA5DA5.70101@braverock.com>
	<e8e755250709260708j68b0566dqeff74f0e03b7fecf@mail.gmail.com>
	<C20EA84D76C94F4E999DC041E81C0D110256EC81@exchange2k3.ny.gghc.com>
	<e8e755250709260717l342473abvd6d4a2f9246e972b@mail.gmail.com>
	<C20EA84D76C94F4E999DC041E81C0D110256EC83@exchange2k3.ny.gghc.com>
Message-ID: <e8e755250709260739s5945f199u4bd105f604e3bef3@mail.gmail.com>

Unfortunately I don't really know, I have one Sun personally - so
clustering my one was easy : )

I think the problem is the same regardless of platform - R isn't
multithreaded, so some additional glue is required, be it SNOW or some
other method.

I will try out the service as soon as I can - maybe in the next week
or so.  Maybe with the 'free' time, a few more could give it a shot
and we could get some sort of feedback.  My dream is to have some sort
of R package to handle the whole process within a local package
framework, though clearly a only a dream at the moment.

Again, has anyone given this a try yet?

Jeff

On 9/26/07, Joshua Reich <josh at gghc.com> wrote:
> Yes - its JBOC (just a bunch of computers). You provide them with a disk
> image (of sorts) and they will load it on to as many computers as you
> request. Images are loaded and machines are requested via a web services
> API. Initially you can request up to 20 machines - but if you email them
> you can ask for more. All network bandwidth between machines is free,
> but there is a per GB transfer charge for external connectivity - I
> can't recall what the rate is, but it is very reasonable.
>
> Not being a specialized grid environment, all inter-node communication
> and scheduling has to be handled by your own application. But for the
> price, that's not too bad.
>
> While I was aware of SNOW, I'm not familiar with the other clustering
> approaches mentioned earlier in this thread. What special sauce does Sun
> provide to make running on a grid easier than running on a JBOC style
> setup?
>
> Josh
>
> -----Original Message-----
> From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com]
> Sent: Wednesday, September 26, 2007 10:18 AM
> To: Joshua Reich
> Cc: Brian G. Peterson; r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Running R as a server or in a cluster
>
> I do know the Sun one is using their grid software, and is supposedly
> highly secure.  Basically have access to a 2000 node opteron cluster.
>
> The Amazon one seems to be more of using a machine, one at a time.  Is
> that correct?
>
>
> On 9/26/07, Joshua Reich <josh at gghc.com> wrote:
> > We recently set up a similar environment using Amazon's EC2 service.
> > They charge $0.1 per CPU hour. I can't say what our results have been
> > like yet - still ironing out the kinks in our R code. But I will
> > certainly let you all know how it goes.
> >
> > Our 'clustering' mechanism is very simple. We have written perl
> > scripts that receive data over HTTP, start R, process the data, and
> > then post the results back via HTTP to a central server.
> >
> > Josh
> >
> > -----Original Message-----
> > From: r-sig-finance-bounces at stat.math.ethz.ch
> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Jeff
> > Ryan
> > Sent: Wednesday, September 26, 2007 10:08 AM
> > To: Brian G. Peterson
> > Cc: r-sig-finance at stat.math.ethz.ch
> > Subject: Re: [R-SIG-Finance] Running R as a server or in a cluster
> >
> > Hi all,
> >
> > Short of answers, but I do wonder if anyone has used Sun Microsystems
> > www.network.com for grid work with R.  At 1USD a CPU hr, with R
> > already built - and a working example script on the service - it seems
>
> > like a path worth exploring.
> >
> > Has anyone given it a try.  I set up an account, but have yet to get
> > the opportunity to try it out.
> >
> > Here is the link:
> >
> > http://www.network.com/apps/r_project.html
> >
> > Jeff Ryan
> >
> > On 9/26/07, Brian G. Peterson <brian at braverock.com> wrote:
> > > Adrian Dragulescu wrote:
> > > > We have set up a Condor cluster, see
> > > > http://www.cs.wisc.edu/condor/ and we submit R jobs to the
> > > > cluster.  It works well because Condor has very advanced
> scheduling capabilities, job monitoring, etc.
> > >
> > > Adrian,
> > >
> > > Could you provide more details?  Are you running Rserve on the
> > > cluster, running "R CMD BATCH", or using Parallel-R?
> > >
> > > I'd like to suggest that we use this thread to continue to develop
> > > the
> >
> > > collective knowledge of the r-sig-finance community on distributed
> > > or high-throughput R calculations.
> > >
> > > Regards,
> > >
> > >     - Brian
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From brian at braverock.com  Wed Sep 26 16:53:22 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 26 Sep 2007 09:53:22 -0500
Subject: [R-SIG-Finance] Running R as a server or in a cluster
In-Reply-To: <C20EA84D76C94F4E999DC041E81C0D110256EC83@exchange2k3.ny.gghc.com>
References: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>
	<46FA5343.1020101@braverock.com>
	<Pine.SUN.4.58.0709260604270.4940@eskimo.com>
	<46FA5DA5.70101@braverock.com>
	<e8e755250709260708j68b0566dqeff74f0e03b7fecf@mail.gmail.com>
	<C20EA84D76C94F4E999DC041E81C0D110256EC81@exchange2k3.ny.gghc.com>
	<e8e755250709260717l342473abvd6d4a2f9246e972b@mail.gmail.com>
	<C20EA84D76C94F4E999DC041E81C0D110256EC83@exchange2k3.ny.gghc.com>
Message-ID: <46FA7262.5020400@braverock.com>

Joshua Reich wrote:
> Yes - its JBOC (just a bunch of computers). You provide them with a disk
> image (of sorts) and they will load it on to as many computers as you
> request. Images are loaded and machines are requested via a web services
> API. Initially you can request up to 20 machines - but if you email them
> you can ask for more. All network bandwidth between machines is free,
> but there is a per GB transfer charge for external connectivity - I
> can't recall what the rate is, but it is very reasonable. 
> 
> Not being a specialized grid environment, all inter-node communication
> and scheduling has to be handled by your own application. But for the
> price, that's not too bad.
> 
> While I was aware of SNOW, I'm not familiar with the other clustering
> approaches mentioned earlier in this thread. What special sauce does Sun
> provide to make running on a grid easier than running on a JBOC style
> setup?

Sun and HP both contributed to the development of Parallel-R, I believe. 
  So I would assume that the sun cluster provides these capabilities.

One simple approach for highly-parallizable calculations that I've seen 
has been to use a parallel version of the apply function.

I think that a cluster-aware portfolio optimization package framework 
should be relatively straightforward to put together in R.  Other 
analyses would need to be taken on a case-by-case basis.

Rserve can work well in front of a cluster environment to run individual 
self-contained queries without having to program for a cluster 
environment.  Other analysis would/might require that your code be 
cluster-aware, and send out and collect distributed jobs.

Regards,

   - Brian


From davidr at rhotrading.com  Wed Sep 26 18:33:40 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 26 Sep 2007 11:33:40 -0500
Subject: [R-SIG-Finance] RBloomberg equity options prices
In-Reply-To: <896988.77256.qm@web32209.mail.mud.yahoo.com>
References: <896988.77256.qm@web32209.mail.mud.yahoo.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE774E7AAB@rhopost.rhotrading.com>

Yes and yes. See ?blpGetData.
You just need the option ticker, e.g., "IBM 10 P105 Equity" is the Oct
105 put for IBM.

David L. Reiner
Rho Trading Securities, LLC


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Moshe
Olshansky
Sent: Tuesday, September 25, 2007 11:34 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] RBloomberg equity options prices

Hello,

Is it possible to get historical option prices using
RBloomberg? Is it possible to get intraday data?

Thank you!

Moshe Olshansky.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From adrian_d at eskimo.com  Wed Sep 26 23:29:28 2007
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Wed, 26 Sep 2007 14:29:28 -0700 (PDT)
Subject: [R-SIG-Finance] Running R as a server or in a cluster
In-Reply-To: <46FA5DA5.70101@braverock.com>
References: <OF4292D8EB.00AAAA54-ON85257362.00428BE6-85257362.0042C5D6@fftw.com>
	<46FA5343.1020101@braverock.com>
	<Pine.SUN.4.58.0709260604270.4940@eskimo.com>
	<46FA5DA5.70101@braverock.com>
Message-ID: <Pine.SUN.4.58.0709261419270.27812@eskimo.com>


Brian,

We're running Rscript, the "newer" version of R CMD BATCH.  We
have a big dependency graph, with many R jobs (+1000), spanning several
projects.  We split big problems into smaller ones and aggregate latter.
Since one project depends on others running successfuly, it's
important to monitor, recover jobs, restart the queue, etc.  Condor allows
us to do this quite well.

Regards,
Adrian


On Wed, 26 Sep 2007, Brian G. Peterson wrote:

> Adrian Dragulescu wrote:
> > We have set up a Condor cluster, see http://www.cs.wisc.edu/condor/ and we
> > submit R jobs to the cluster.  It works well because Condor has very
> > advanced scheduling capabilities, job monitoring, etc.
>
> Adrian,
>
> Could you provide more details?  Are you running Rserve on the cluster,
> running "R CMD BATCH", or using Parallel-R?
>
> I'd like to suggest that we use this thread to continue to develop the
> collective knowledge of the r-sig-finance community on distributed or
> high-throughput R calculations.
>
> Regards,
>
>     - Brian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ashish.kumar at stanford.edu  Thu Sep 27 00:45:59 2007
From: ashish.kumar at stanford.edu (Ashish Kumar)
Date: Wed, 26 Sep 2007 15:45:59 -0700 (PDT)
Subject: [R-SIG-Finance] Using blockMaxima in fExtremes package for
	preprocessing the Data
Message-ID: <12897457.post@talk.nabble.com>


Hello, I am trying to find how blockMaxima works ? I run this on danish data
and i get the following :-

> summary(danish)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  1.000   1.321   1.778   3.385   2.967 263.300 

> blockMaxima(danish, block=500)
       82       472       388       356       121 
263.25037  57.41064  32.46753 152.41321 144.65759 
> 

I guess there are 5 blocks of following sizes
       82       472       388       356       121 
with the maximum value of each block being..
263.25037  57.41064  32.46753 152.41321 144.65759 

But if you add all these block i.e. 
      82     +  472  +    388  +    356  +     121  = 1419..

> length(danish)
[1] 2167

The sample size of danish is 2167 which is not equal to 1419...
 
Can someone explain how blockMaxima is preprocessing the tails of danish
data..?


Ashish

-- 
View this message in context: http://www.nabble.com/Using-blockMaxima-in-fExtremes-package-for-preprocessing-the-Data-tf4521138.html#a12897457
Sent from the Rmetrics mailing list archive at Nabble.com.


From ngottlieb at marinercapital.com  Thu Sep 27 22:28:10 2007
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Thu, 27 Sep 2007 16:28:10 -0400
Subject: [R-SIG-Finance] Which package has Efficient frontier
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D880151E335@500MAIL.goldbox.com>

Dear Fellow R Finance Users:

I forgot which package has efficient frontier function and ability to
plot it.

I am sure I downloaded it and isntalled... just don't remember the
package name. 
Can someone remind me?

Thanks,
Neil


From edd at debian.org  Thu Sep 27 23:22:21 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 27 Sep 2007 21:22:21 +0000
Subject: [R-SIG-Finance] Which package has Efficient frontier
In-Reply-To: <0946E293C7C22A45A0E33BA14FAA8D880151E335@500MAIL.goldbox.com>
References: <0946E293C7C22A45A0E33BA14FAA8D880151E335@500MAIL.goldbox.com>
Message-ID: <20070927212221.GA20453@master.debian.org>

On Thu, Sep 27, 2007 at 04:28:10PM -0400, ngottlieb at marinercapital.com wrote:
> Dear Fellow R Finance Users:
> 
> I forgot which package has efficient frontier function and ability to
> plot it.
> 
> I am sure I downloaded it and isntalled... just don't remember the
> package name. 
> Can someone remind me?

You probably look for 'tseries'.

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From ngottlieb at marinercapital.com  Thu Sep 27 23:29:40 2007
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Thu, 27 Sep 2007 17:29:40 -0400
Subject: [R-SIG-Finance] Which package has Efficient frontier
In-Reply-To: <20070927212221.GA20453@master.debian.org>
References: <0946E293C7C22A45A0E33BA14FAA8D880151E335@500MAIL.goldbox.com>
	<20070927212221.GA20453@master.debian.org>
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D880151E336@500MAIL.goldbox.com>

Thanks. I have that loaded.

Actually may be simpler than just a cruve fit as I have
In one vector the returns (mu) and standard deviation(sigma) in the
other.

Any suggestions which package and function to use based on already
having above data?

Thanks,
Neil 

-----Original Message-----
From: Dirk Eddelbuettel [mailto:edd at master.debian.org] On Behalf Of Dirk
Eddelbuettel
Sent: Thursday, September 27, 2007 5:22 PM
To: Gottlieb, Neil
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Which package has Efficient frontier

On Thu, Sep 27, 2007 at 04:28:10PM -0400, ngottlieb at marinercapital.com
wrote:
> Dear Fellow R Finance Users:
> 
> I forgot which package has efficient frontier function and ability to 
> plot it.
> 
> I am sure I downloaded it and isntalled... just don't remember the 
> package name.
> Can someone remind me?

You probably look for 'tseries'.

Hth, Dirk

--
Three out of two people have difficulties with fractions.
--------------------------------------------------------



This information is being sent at the recipient's request or...{{dropped}}


From finbref.2006 at gmail.com  Thu Sep 27 23:34:26 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Thu, 27 Sep 2007 23:34:26 +0200
Subject: [R-SIG-Finance] Which package has Efficient frontier
In-Reply-To: <0946E293C7C22A45A0E33BA14FAA8D880151E335@500MAIL.goldbox.com>
References: <0946E293C7C22A45A0E33BA14FAA8D880151E335@500MAIL.goldbox.com>
Message-ID: <d0f55a670709271434r71478b39ge0b15080154007d2@mail.gmail.com>

fPortfolio?
http://cran.r-project.org/src/contrib/Descriptions/fPortfolio.html
Thomas


From m_olshansky at yahoo.com  Fri Sep 28 02:33:28 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Thu, 27 Sep 2007 17:33:28 -0700 (PDT)
Subject: [R-SIG-Finance] RBloomberg equity options prices
In-Reply-To: <F9F2A641C593D7408925574C05A7BE774E7AAB@rhopost.rhotrading.com>
Message-ID: <218968.89570.qm@web32208.mail.mud.yahoo.com>

Thank you - this really works!

I was unable to guess that for the option data one
still must add the word "Equity" at the end.

Is there any place where such information can be
found?

Moshe.

--- davidr at rhotrading.com wrote:

> Yes and yes. See ?blpGetData.
> You just need the option ticker, e.g., "IBM 10 P105
> Equity" is the Oct
> 105 put for IBM.
> 
> David L. Reiner
> Rho Trading Securities, LLC
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On
> Behalf Of Moshe
> Olshansky
> Sent: Tuesday, September 25, 2007 11:34 PM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] RBloomberg equity options
> prices
> 
> Hello,
> 
> Is it possible to get historical option prices using
> RBloomberg? Is it possible to get intraday data?
> 
> Thank you!
> 
> Moshe Olshansky.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From edd at debian.org  Fri Sep 28 02:49:07 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 27 Sep 2007 19:49:07 -0500
Subject: [R-SIG-Finance] RBloomberg equity options prices
In-Reply-To: <218968.89570.qm@web32208.mail.mud.yahoo.com>
References: <F9F2A641C593D7408925574C05A7BE774E7AAB@rhopost.rhotrading.com>
	<218968.89570.qm@web32208.mail.mud.yahoo.com>
Message-ID: <20070928004907.GA8622@eddelbuettel.com>

On Thu, Sep 27, 2007 at 05:33:28PM -0700, Moshe Olshansky wrote:
> Thank you - this really works!
> 
> I was unable to guess that for the option data one
> still must add the word "Equity" at the end.
> 
> Is there any place where such information can be
> found?

The Bloomberg terminal itself?  If you can;t get 'info' reliably for a
symbol you can get the time series either.  Recall that in interactive
mode, the 'yellow key' matters, and that is what corresponds to the
'Equity' here.  

Dirk

-- 
Three out of two people have difficulties with fractions.


From davidr at rhotrading.com  Fri Sep 28 15:36:06 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Fri, 28 Sep 2007 08:36:06 -0500
Subject: [R-SIG-Finance] RBloomberg equity options prices
In-Reply-To: <218968.89570.qm@web32208.mail.mud.yahoo.com>
References: <F9F2A641C593D7408925574C05A7BE774E7AAB@rhopost.rhotrading.com>
	<218968.89570.qm@web32208.mail.mud.yahoo.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE774E7C16@rhopost.rhotrading.com>

I would suggest reading the Bloomberg API help.
You can start at the Bloomberg professional (terminal) and type
WAPI<go>.
You can munge around there, or download WAPI Lite so you can look at the
docs offline. Much of the time, starting in Excel with the wizards will
get you to the point where you know the tickers and fields you want.
Then you can move over to programming instead of using Excel and
formulas. One other function in Bloomberg I use quite often is FPRP<go>
once you have an instrument active. This tells you everything that is
available for that ticker and what its current value is, as well as the
API filed names.

Have fun!

David L. Reiner
Rho Trading Securities, LLC


-----Original Message-----
From: Moshe Olshansky [mailto:m_olshansky at yahoo.com] 
Sent: Thursday, September 27, 2007 7:33 PM
To: David Reiner <davidr at rhotrading.com>;
r-sig-finance at stat.math.ethz.ch
Subject: RE: [R-SIG-Finance] RBloomberg equity options prices

Thank you - this really works!

I was unable to guess that for the option data one
still must add the word "Equity" at the end.

Is there any place where such information can be
found?

Moshe.

--- davidr at rhotrading.com wrote:

> Yes and yes. See ?blpGetData.
> You just need the option ticker, e.g., "IBM 10 P105
> Equity" is the Oct
> 105 put for IBM.
> 
> David L. Reiner
> Rho Trading Securities, LLC
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On
> Behalf Of Moshe
> Olshansky
> Sent: Tuesday, September 25, 2007 11:34 PM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] RBloomberg equity options
> prices
> 
> Hello,
> 
> Is it possible to get historical option prices using
> RBloomberg? Is it possible to get intraday data?
> 
> Thank you!
> 
> Moshe Olshansky.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 


From bradford.n.cross at gmail.com  Sun Sep 30 04:14:12 2007
From: bradford.n.cross at gmail.com (Bradford Cross)
Date: Sat, 29 Sep 2007 19:14:12 -0700
Subject: [R-SIG-Finance] Fwd: smart updates and rolling windows
In-Reply-To: <ea7d6a710709291008y60013d7bv7f7ce7abb8fb0c19@mail.gmail.com>
References: <ea7d6a710709291008y60013d7bv7f7ce7abb8fb0c19@mail.gmail.com>
Message-ID: <ea7d6a710709291914q3bba623cx41e4827646d5469@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20070929/60468bf8/attachment.pl 

From ajayshah at mayin.org  Sun Sep 30 20:36:06 2007
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 1 Oct 2007 00:06:06 +0530
Subject: [R-SIG-Finance] Bug report on mgarchBEKK
Message-ID: <20070930183606.GU1358@lubyanka.local>

The example supplied for mvBEKK.sim() doesn't work with R 2.5.1:

> library(mgarchBEKK)

        `mgarchBEKK' version: 0.07-8 
> sim = mvBEKK.sim(series.count = 3, T = 2500)
the part of the args list of 'list' being evaluated was:
   (length = T, series.count = series.count, order = order, params =
> params, true.params = buff.par, eigenvalues = eigenvalues,
> uncond.cov.matrix = sigma, white.noise = nu, eps = eps.list, cor =
> cor, sd = sd, )
Error in mvBEKK.sim(series.count = 3, T = 2500) : 
        element 12 is empty

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


