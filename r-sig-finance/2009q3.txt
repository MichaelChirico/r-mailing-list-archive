From ajayshah at mayin.org  Wed Jul  1 05:19:20 2009
From: ajayshah at mayin.org (Ajay Shah)
Date: Wed, 1 Jul 2009 08:49:20 +0530
Subject: [R-SIG-Finance] How can I do this better (handling "realtime"
	macroeconomic data)?
Message-ID: <20090701031920.GA5324@ajay-shahs-macbook-pro.local>

Folks,

One problem faced with macroeconomic data is that of data revision. On
date t1 you're told a value x, and then on date t2 you're told that
same value has been changed to y. It is often fine to merely ignore
older values. But in some problems, it becomes important to track the
time-series as observed at past dates. In order to address this
problem, we need to store not just the time-series as a set of
(date,value) pairs, but also an additional field "infodate" which is
the date on which a given record was observed.

Here's an example of this data representation:

  a <- structure(list(date = c("2007-04-01", "2007-04-01", "2007-05-01", 
    "2007-04-01", "2007-05-01", "2007-06-01", "2007-05-01", "2007-06-01", 
    "2007-07-01"), infodate = structure(c(13634, 13665, 13665, 13695, 
    13695, 13695, 13726, 13726, 13726), class = "Date"), value = c(42L, 
    43L, 55L, 49L, 55L, 66L, 56L, 67L, 77L)), .Names = c("date", 
    "infodate", "value"), row.names = c(NA, -9L), class = "data.frame")
  a
  str(a)

So this is a dataset containing date (a string), infodate (a Date) and value.

Using this representation, I wrote a function which queries the
dataset and reports the time series as seen on a given date. If a
value for ondate is supplied, only records with infodate <= ondate are
utilised.

  fetch.ts <- function(d, ondate=NULL) {
    if (!is.null(ondate)) {
      d <- subset(d, d$infodate <= ondate)
    }
  
    # Now we walk through the series, and every time a new value for
    # a given date shows up, we overwrite the previous version.
    x <- a$value[1]; names(x)[1] <- d$date[1]
    for (i in 2:nrow(d)) {
      x[d$date[i]] <- d$value[i]
    }
    x
  }

This seems to work okay:

  fetch.ts(a)
  all.equal(fetch.ts(a), structure(c(49L, 56L, 67L, 77L),
                                   .Names = c("2007-04-01",
                                     "2007-05-01", "2007-06-01", "2007-07-01")))
  fetch.ts(a, "2007-07-01")
  all.equal(fetch.ts(a, "2007-07-01"),
            structure(c(49L, 55L, 66L),
                      .Names = c("2007-04-01", "2007-05-01", "2007-06-01")))

but I'm not happy at my loops-intensive solution. Also, the use of
associative arrays (using the names in R) might be quite
expensive. How would you improve on this?

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From ggrothendieck at gmail.com  Wed Jul  1 09:24:12 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 1 Jul 2009 03:24:12 -0400
Subject: [R-SIG-Finance] How can I do this better (handling "realtime"
	macroeconomic data)?
In-Reply-To: <20090701031920.GA5324@ajay-shahs-macbook-pro.local>
References: <20090701031920.GA5324@ajay-shahs-macbook-pro.local>
Message-ID: <971536df0907010024l12acd17er90643f485d52cd6d@mail.gmail.com>

Try this:

fetch2 <- function(d, as.of.date = Inf) {
	do.call(rbind, by(d, d$date, function(x)
		x[which.max(x[x$infodate <= as.of.date, ]$infodate), ]
))}


On Tue, Jun 30, 2009 at 11:19 PM, Ajay Shah<ajayshah at mayin.org> wrote:
> Folks,
>
> One problem faced with macroeconomic data is that of data revision. On
> date t1 you're told a value x, and then on date t2 you're told that
> same value has been changed to y. It is often fine to merely ignore
> older values. But in some problems, it becomes important to track the
> time-series as observed at past dates. In order to address this
> problem, we need to store not just the time-series as a set of
> (date,value) pairs, but also an additional field "infodate" which is
> the date on which a given record was observed.
>
> Here's an example of this data representation:
>
> ?a <- structure(list(date = c("2007-04-01", "2007-04-01", "2007-05-01",
> ? ?"2007-04-01", "2007-05-01", "2007-06-01", "2007-05-01", "2007-06-01",
> ? ?"2007-07-01"), infodate = structure(c(13634, 13665, 13665, 13695,
> ? ?13695, 13695, 13726, 13726, 13726), class = "Date"), value = c(42L,
> ? ?43L, 55L, 49L, 55L, 66L, 56L, 67L, 77L)), .Names = c("date",
> ? ?"infodate", "value"), row.names = c(NA, -9L), class = "data.frame")
> ?a
> ?str(a)
>
> So this is a dataset containing date (a string), infodate (a Date) and value.
>
> Using this representation, I wrote a function which queries the
> dataset and reports the time series as seen on a given date. If a
> value for ondate is supplied, only records with infodate <= ondate are
> utilised.
>
> ?fetch.ts <- function(d, ondate=NULL) {
> ? ?if (!is.null(ondate)) {
> ? ? ?d <- subset(d, d$infodate <= ondate)
> ? ?}
>
> ? ?# Now we walk through the series, and every time a new value for
> ? ?# a given date shows up, we overwrite the previous version.
> ? ?x <- a$value[1]; names(x)[1] <- d$date[1]
> ? ?for (i in 2:nrow(d)) {
> ? ? ?x[d$date[i]] <- d$value[i]
> ? ?}
> ? ?x
> ?}
>
> This seems to work okay:
>
> ?fetch.ts(a)
> ?all.equal(fetch.ts(a), structure(c(49L, 56L, 67L, 77L),
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? .Names = c("2007-04-01",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? "2007-05-01", "2007-06-01", "2007-07-01")))
> ?fetch.ts(a, "2007-07-01")
> ?all.equal(fetch.ts(a, "2007-07-01"),
> ? ? ? ? ? ?structure(c(49L, 55L, 66L),
> ? ? ? ? ? ? ? ? ? ? ?.Names = c("2007-04-01", "2007-05-01", "2007-06-01")))
>
> but I'm not happy at my loops-intensive solution. Also, the use of
> associative arrays (using the names in R) might be quite
> expensive. How would you improve on this?
>
> --
> Ajay Shah ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?http://www.mayin.org/ajayshah
> ajayshah at mayin.org ? ? ? ? ? ? ? ? ? ? ? ? ? ? http://ajayshahblog.blogspot.com
> <*(:-? - wizard who doesn't know the answer.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From rbali at ufmg.br  Wed Jul  1 12:37:21 2009
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Wed, 1 Jul 2009 07:37:21 -0300
Subject: [R-SIG-Finance] Value-at-Risk
In-Reply-To: <817126.55167.qm@web53501.mail.re2.yahoo.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>
Message-ID: <ABA800AA882E492DB9466E1CC92EAAF6@DellPC>

See for example "Accurate value-at-risk forecasting based on the 
normal-GARCH model" by C Hartz, S Mittnik, M Paolella - Computational 
Statistics and Data Analysis, 2006

best

--------------------------------------------------
From: "Wei-han Liu" <weihanliu2002 at yahoo.com>
Sent: Tuesday, June 30, 2009 12:16 PM
To: <R-SIG-Finance at stat.math.ethz.ch>
Subject: [R-SIG-Finance] Value-at-Risk

> Dear R-users:
>
> Several questions please on Value-at-Risk.
>
> Is Value-at-Risk designed for forecasting purpose?
>
> I wonder if Value-at-Risk estimated by in-sample data can be used for 
> out-of-sample forecasting?
>
> If in-sample Value-at-Risk is estimated by several methods, is it 
> appropriate to do the model comparisons based on out--of-sample 
> performance?
>
> Wei-han Liu


From weihanliu2002 at yahoo.com  Wed Jul  1 14:02:06 2009
From: weihanliu2002 at yahoo.com (Wei-han Liu)
Date: Wed, 1 Jul 2009 05:02:06 -0700 (PDT)
Subject: [R-SIG-Finance] Fw:  Value-at-Risk
In-Reply-To: <817126.55167.qm@web53501.mail.re2.yahoo.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>
Message-ID: <190882.92554.qm@web53507.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090701/6cdee740/attachment.pl>

From dutt.debashis at gmail.com  Wed Jul  1 15:25:24 2009
From: dutt.debashis at gmail.com (Debashis Dutta)
Date: Wed, 1 Jul 2009 16:25:24 +0300
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <190882.92554.qm@web53507.mail.re2.yahoo.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>
	<190882.92554.qm@web53507.mail.re2.yahoo.com>
Message-ID: <37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090701/97729ee1/attachment.pl>

From Murali.MENON at fortisinvestments.com  Wed Jul  1 15:41:31 2009
From: Murali.MENON at fortisinvestments.com (Murali.MENON at fortisinvestments.com)
Date: Wed, 1 Jul 2009 15:41:31 +0200
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>
	<190882.92554.qm@web53507.mail.re2.yahoo.com> 
	<37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>
Message-ID: <5A3D018CBDC36B4F8FF6DB52DDF3B82DA2AC76@BE-S0500-V22.adroot.local>

Hi,
Regarding the use of EVT-based VaR, I think it is dependent on the asset
class. In exchange rates, e.g., you may generally be served well with
EVT for the major currencies, but might do quite badly with emerging
market currencies. Likewise, in the credit markets, EVT may not do too
well. The assumption of a constant tail index results in misleading VaR
at the extreme tails, especially when there are several regime shifts.
Take a look at the paper "Testing for Multiple Regimes in the Tail
Behavior of Emerging Currency Returns" by B. Candelon and S. Straetmans,
LIFE Working Paper 03-035.
Cheers,
Murali


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Debashis
Dutta
Sent: 01 July 2009 14:25
To: Wei-han Liu
Cc: R-SIG-Finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Fw: Value-at-Risk

Dear Wei-han,

I believe EVT based VaR would provide a better solution specially in
stressed situation like the present one, modeling the extremal behaviour
in the tail. I used Peaks Over Threshold (POT) based VaR method in my
doctoral dissertation.

Back testing and comparing the new method to existing ones on real
financial events show that this POT based VaR method provides a rather
realistic model for the extremal behavior of financial processes,
enabling a precise estimation of risk measures. Through the GPD , the
model provides a way of estimating the tail behaviour of the random
variables without knowledge of the true distribution and as such it is a
good candidate for Vale at Risk computation.

Most common at this moment is the tail-fitting approach based on the
second theorem in extreme value theory (Theorem II Pickands(1975),
Balkema and de Haan(1974)). In general this conforms to the first
theorem in extreme value theory (Theorem I Fisher and Tippett(1928), and
Gnedenko (1943)).The difference between the two theorems is due to the
nature of the data generation.

 For theorem I the data are generated in full range, while in theorem II
data is only generated when it surpasses a certain threshold (POT's
models or Peak Over Threshold). The POT approach has been developed
largely in the insurance business, where only losses (pay outs) above a
certain threshold are accessible to the insurance company.

 Kind Regards,
Debashis

On 01/07/2009, Wei-han Liu <weihanliu2002 at yahoo.com> wrote:
>
> Thanks a lot, Robert.
>
> I know GARCH models has its forecasting capacity as the reference you 
> shared indicates.
>
> I wonder if the Value-at-Risk estimated by extreme value theory can 
> also be used for forecasting purpose. Is there some theory background 
> in this regard?
>
> Wei-han
>
>
>
> ----- Forwarded Message ----
> From: Robert Iquiapaza <rbali at ufmg.br>
> To: Wei-han Liu <weihanliu2002 at yahoo.com>; "
> r-sig-finance at stat.math.ethz.ch" <R-SIG-Finance at stat.math.ethz.ch>
> Sent: Wednesday, July 1, 2009 6:37:21 PM
> Subject: Re: [R-SIG-Finance] Value-at-Risk
>
> See for example "Accurate value-at-risk forecasting based on the 
> normal-GARCH model" by C Hartz, S Mittnik, M Paolella - Computational 
> Statistics and Data Analysis, 2006
>
> best
>
> --------------------------------------------------
>
> Sent: Tuesday, June 30, 2009 12:16 PM
> To: <R-SIG-Finance at stat.math.ethz.ch>
> Subject: [R-SIG-Finance] Value-at-Risk
>
> > Dear R-users:
> >
> > Several questions please on Value-at-Risk.
> >
> > Is Value-at-Risk designed for forecasting purpose?
> >
> > I wonder if Value-at-Risk estimated by in-sample data can be used 
> > for
> out-of-sample forecasting?
> >
> > If in-sample Value-at-Risk is estimated by several methods, is it
> appropriate to do the model comparisons based on out--of-sample
performance?
> >
> > Wei-han Liu
>
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From john.kerpel at gmail.com  Wed Jul  1 17:27:33 2009
From: john.kerpel at gmail.com (John Kerpel)
Date: Wed, 1 Jul 2009 10:27:33 -0500
Subject: [R-SIG-Finance] Calculating returns
Message-ID: <6555fd730907010827l7c66c4e1yd7c1ca83304fb3ca@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090701/c330a3f6/attachment.pl>

From brian at braverock.com  Wed Jul  1 20:39:02 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 01 Jul 2009 13:39:02 -0500
Subject: [R-SIG-Finance] Fw:  Value-at-Risk
In-Reply-To: <190882.92554.qm@web53507.mail.re2.yahoo.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>
	<190882.92554.qm@web53507.mail.re2.yahoo.com>
Message-ID: <4A4BAD46.9040100@braverock.com>

Wei-han Liu wrote:
> I know GARCH models has its forecasting capacity as the reference you shared indicates.
>
> I wonder if the Value-at-Risk estimated by extreme value theory can also be used for forecasting purpose. Is there some theory background in this regard?
>   
There are a myriad of Value at Risk methods, many of the most useful 
ones (and some of their portfolio component risk decomposition 
extensions) are already implemented in R.

The literature on this topic is huge.  Jorion's book comes to mind, 
though t is neither comprehensive nor recent.  EVT, Cornish Fisher, 
Shew-t, GPD, MCMC, GARCH, VEC/VAR, etc may all be used to calculate a 
VaR (note the capitalization difference) model that has out of sample 
estimation capabilities.

I suggest searching the R documentation and the internet on Value at 
Risk so that you can ask a more actionable question.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From brian at braverock.com  Wed Jul  1 20:43:22 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 01 Jul 2009 13:43:22 -0500
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>	<190882.92554.qm@web53507.mail.re2.yahoo.com>
	<37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>
Message-ID: <4A4BAE4A.8050901@braverock.com>

Debashis Dutta wrote:
> Dear Wei-han,
>
> I believe EVT based VaR would provide a better solution specially
> in stressed situation like the present one, modeling the extremal behaviour
> in the tail. I used Peaks Over Threshold (POT) based VaR method in my
> doctoral dissertation.
>
> Back testing and comparing the new method to existing ones on real financial
> events show that this POT based VaR method provides a rather realistic model
> for the extremal behavior of financial processes, enabling a precise
> estimation of risk measures. Through the GPD , the model provides a way of
> estimating the tail behaviour of the random variables without knowledge of
> the true distribution and as such it is a good candidate for Vale at Risk
> computation.
>
> Most common at this moment is the tail-fitting approach based on the second
> theorem in extreme value theory (Theorem II Pickands(1975), Balkema and de
> Haan(1974)). In general this conforms to the first theorem in extreme value
> theory (Theorem I Fisher and Tippett(1928), and Gnedenko (1943)).The
> difference between the two theorems is due to the nature of the data
> generation.
>
>  For theorem I the data are generated in full range, while in theorem II
> data is only generated when it surpasses a certain threshold (POT's models
> or Peak Over Threshold). The POT approach has been developed largely in the
> insurance business, where only losses (pay outs) above a certain threshold
> are accessible to the insurance company.
Debashis,

Could you please post a link to your dissertation and the code used to implement it?

Regards,

  - Brian 


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From dutt.debashis at gmail.com  Wed Jul  1 21:05:29 2009
From: dutt.debashis at gmail.com (Debashis Dutta)
Date: Wed, 1 Jul 2009 22:05:29 +0300
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <5A3D018CBDC36B4F8FF6DB52DDF3B82DA2AC76@BE-S0500-V22.adroot.local>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>
	<190882.92554.qm@web53507.mail.re2.yahoo.com>
	<37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>
	<5A3D018CBDC36B4F8FF6DB52DDF3B82DA2AC76@BE-S0500-V22.adroot.local>
Message-ID: <37673c2d0907011205l995e7afga29653f7615fc0ff@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090701/38b1e249/attachment.pl>

From comtech.usa at gmail.com  Wed Jul  1 21:44:19 2009
From: comtech.usa at gmail.com (Michael)
Date: Wed, 1 Jul 2009 12:44:19 -0700
Subject: [R-SIG-Finance] IBrokers and automatic submission of orders?
Message-ID: <b1f16d9d0907011244v5068aac4pe732b95d7e0cdc3e@mail.gmail.com>

Hi all,

What's the best way to set up a timer in IBrokers, and take a snapshot
of market data at fixed interval, and then place some orders, and run
other tasks?

Also,  reqMktData(tws, twsEquity("QQQQ")) returns data continuously,

is there a way for it to only return one snapshot of data at one time,

just need to sample at fixed interval using a timer...

Thank you very much!

-----------

And how to submit order such as "market open", "market close", and
"good-until-after-some-time" in IBrokers?

Thanks a lot!


From patrick at burns-stat.com  Wed Jul  1 22:10:33 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed, 01 Jul 2009 21:10:33 +0100
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <37673c2d0907011205l995e7afga29653f7615fc0ff@mail.gmail.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>	<190882.92554.qm@web53507.mail.re2.yahoo.com>	<37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>	<5A3D018CBDC36B4F8FF6DB52DDF3B82DA2AC76@BE-S0500-V22.adroot.local>
	<37673c2d0907011205l995e7afga29653f7615fc0ff@mail.gmail.com>
Message-ID: <4A4BC2B9.8070905@burns-stat.com>

In doing the forecasting there are two
things to get right: the distribution
and the changes in volatility.  In the
research that I've done, getting the
volatility changes right appeared to be
much more important than getting the
distribution right.

I have no problem with bringing extreme
value theory in, but I don't see how the
volatility issue can be avoided.  There
is the problem of heteroskedasticity
when estimating the tails that makes
EVT tricky to apply.

The work I did was in equities, but I
suspect that the situation wouldn't
be all that different for other asset
classes.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of "The R Inferno" and "A Guide for the Unwilling S User")

Debashis Dutta wrote:
> Dear Murali,
> 
> I fundamentally disagree with you on your comments ??The assumption of a
> constant tail index results in misleading VaR at the extreme tails,
> especially when there are several regime shifts.? The works exhibited by
> Researchers and study made by Practitioners proved the contrary.
> 
> Please see a recent paper titled ?The Extreme Value Theory Value Theory
> Performance in the event of major financial crisis" by Adrian F. Rossignolo
> Uiversity of BuenosAirs, February 2009.
> 
> 
> 
> The key outcome:
> 
> 
> 
> Extreme Value Theory (EVT) provides a method to estimate VaR at high
> quantiles of the distribution, consequently focusing on extraordinary and
> unusual circumstances. ? EVT to calculate VaR for six stock market indices
> belonging to developed and emerging markets in two different ways:
> Unconditional EVT on raw returns and Conditional EVT which blends
> Quasi-Maximum-Likelihood fitting of GARCH models to estimate current dynamic
> volatility and EVT for estimating the tails of the innovation distribution
> of the GARCH residuals (both tails independently). Backtesting EVT
> representations using turmoil recorded in 2008, and comparing their
> performance with that of the most popular representations nowadays in vogue,
> it is found that EVT schemes could help institutions to avoid huge losses
> arising from market disasters. A simple exercise on the constitution of
> Regulatory Capital illustrates the advantages of EVT.
> 
> 
> 
> I being a practitioner agree with Adrain.
> 
> 
> 
> There is also a recent study on POT in GCC.
> 
> 
> 
> The paper ? The tail behavior of extreme stock returns in the Gulf emerging
> markets: An implication for financial risk management? by Aktham I.
> Maghyereh and Haitham A. Al-Zoubi , Studies in Economics and
> Finance<http://www.emeraldinsight.com/1086-7376.htm>,
> 2008, Vol. - 25, Issue 1, 21-37.
> 
> 
> 
> Findings ? Not only is the heavy tail found to be a facial appearance in
> these markets, but also POT method of modelling extreme tail quantiles is
> more accurate than conventional methodologies (historical simulation and
> normal distribution models) in estimating the tail behavior of the Gulf
> markets returns. Across all return series, it is found that left and right
> tails behave very different across countries.
> 
> 
> 
> I am sorry to disagree with your comment. ?Likewise, in the credit markets,
> EVT may not do too well.?
> 
> Please see the Basel Paper ?Extreme tails for linear portfolio credit risk
> models? by Andr? Lucas, Pieter Klaassen,Peter Spreij and Stefan Straetmans.
> 
> 
> 
> Concluding Remarks of the paper:
> 
> Upon comparing the analytic tail probabilities with their extreme value
> counterparts, we found that the extreme value probabilities come close to
> their true values provided one goes very far into the credit loss tail.
> 
> 
> 
> The word of caution that is also mentioned in the concluding remarks
> 
> 
> 
>  ?We conclude that standard use of EVT methods as applied in, for example,
> the market risk context is inappropriate in the credit risk context.?
> Possibly you have mistaken this comment.
> 
> 
> 
> Kind Regards,
> 
> Debashis
> 
> 
> 
> 2009/7/1 <Murali.MENON at fortisinvestments.com>
> 
>> Hi,
>> Regarding the use of EVT-based VaR, I think it is dependent on the asset
>> class. In exchange rates, e.g., you may generally be served well with
>> EVT for the major currencies, but might do quite badly with emerging
>> market currencies. Likewise, in the credit markets, EVT may not do too
>> well. The assumption of a constant tail index results in misleading VaR
>> at the extreme tails, especially when there are several regime shifts.
>> Take a look at the paper "Testing for Multiple Regimes in the Tail
>> Behavior of Emerging Currency Returns" by B. Candelon and S. Straetmans,
>> LIFE Working Paper 03-035.
>> Cheers,
>> Murali
>>
>>
>> -----Original Message-----
>> From: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Debashis
>> Dutta
>> Sent: 01 July 2009 14:25
>> To: Wei-han Liu
>> Cc: R-SIG-Finance at stat.math.ethz.ch
>> Subject: Re: [R-SIG-Finance] Fw: Value-at-Risk
>>
>> Dear Wei-han,
>>
>> I believe EVT based VaR would provide a better solution specially in
>> stressed situation like the present one, modeling the extremal behaviour
>> in the tail. I used Peaks Over Threshold (POT) based VaR method in my
>> doctoral dissertation.
>>
>> Back testing and comparing the new method to existing ones on real
>> financial events show that this POT based VaR method provides a rather
>> realistic model for the extremal behavior of financial processes,
>> enabling a precise estimation of risk measures. Through the GPD , the
>> model provides a way of estimating the tail behaviour of the random
>> variables without knowledge of the true distribution and as such it is a
>> good candidate for Vale at Risk computation.
>>
>> Most common at this moment is the tail-fitting approach based on the
>> second theorem in extreme value theory (Theorem II Pickands(1975),
>> Balkema and de Haan(1974)). In general this conforms to the first
>> theorem in extreme value theory (Theorem I Fisher and Tippett(1928), and
>> Gnedenko (1943)).The difference between the two theorems is due to the
>> nature of the data generation.
>>
>>  For theorem I the data are generated in full range, while in theorem II
>> data is only generated when it surpasses a certain threshold (POT's
>> models or Peak Over Threshold). The POT approach has been developed
>> largely in the insurance business, where only losses (pay outs) above a
>> certain threshold are accessible to the insurance company.
>>
>>  Kind Regards,
>> Debashis
>>
>> On 01/07/2009, Wei-han Liu <weihanliu2002 at yahoo.com> wrote:
>>> Thanks a lot, Robert.
>>>
>>> I know GARCH models has its forecasting capacity as the reference you
>>> shared indicates.
>>>
>>> I wonder if the Value-at-Risk estimated by extreme value theory can
>>> also be used for forecasting purpose. Is there some theory background
>>> in this regard?
>>>
>>> Wei-han
>>>
>>>
>>>
>>> ----- Forwarded Message ----
>>> From: Robert Iquiapaza <rbali at ufmg.br>
>>> To: Wei-han Liu <weihanliu2002 at yahoo.com>; "
>>> r-sig-finance at stat.math.ethz.ch" <R-SIG-Finance at stat.math.ethz.ch>
>>> Sent: Wednesday, July 1, 2009 6:37:21 PM
>>> Subject: Re: [R-SIG-Finance] Value-at-Risk
>>>
>>> See for example "Accurate value-at-risk forecasting based on the
>>> normal-GARCH model" by C Hartz, S Mittnik, M Paolella - Computational
>>> Statistics and Data Analysis, 2006
>>>
>>> best
>>>
>>> --------------------------------------------------
>>>
>>> Sent: Tuesday, June 30, 2009 12:16 PM
>>> To: <R-SIG-Finance at stat.math.ethz.ch>
>>> Subject: [R-SIG-Finance] Value-at-Risk
>>>
>>>> Dear R-users:
>>>>
>>>> Several questions please on Value-at-Risk.
>>>>
>>>> Is Value-at-Risk designed for forecasting purpose?
>>>>
>>>> I wonder if Value-at-Risk estimated by in-sample data can be used
>>>> for
>>> out-of-sample forecasting?
>>>> If in-sample Value-at-Risk is estimated by several methods, is it
>>> appropriate to do the model comparisons based on out--of-sample
>> performance?
>>>> Wei-han Liu
>>>
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>        [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From andyzhu35 at yahoo.com  Wed Jul  1 23:54:06 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Wed, 1 Jul 2009 14:54:06 -0700 (PDT)
Subject: [R-SIG-Finance] Calculating returns
Message-ID: <383781.5134.qm@web56201.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090701/1558fc9c/attachment.pl>

From dutt.debashis at gmail.com  Thu Jul  2 01:09:55 2009
From: dutt.debashis at gmail.com (Debashis Dutta)
Date: Thu, 2 Jul 2009 02:09:55 +0300
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <4A4BC2B9.8070905@burns-stat.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>
	<190882.92554.qm@web53507.mail.re2.yahoo.com>
	<37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>
	<5A3D018CBDC36B4F8FF6DB52DDF3B82DA2AC76@BE-S0500-V22.adroot.local>
	<37673c2d0907011205l995e7afga29653f7615fc0ff@mail.gmail.com>
	<4A4BC2B9.8070905@burns-stat.com>
Message-ID: <37673c2d0907011609p22f6f379m5d2d0471eaf15c9b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090702/09edeb51/attachment.pl>

From rrburns at cox.net  Thu Jul  2 01:25:10 2009
From: rrburns at cox.net (Ron Burns)
Date: Wed, 01 Jul 2009 16:25:10 -0700
Subject: [R-SIG-Finance] garchFit help page examples do not converge
Message-ID: <4A4BF056.6070102@cox.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090701/7b3fafc7/attachment.html>

From cedrick at cedrickjohnson.com  Thu Jul  2 03:12:51 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Wed, 01 Jul 2009 21:12:51 -0400
Subject: [R-SIG-Finance] IBrokers and automatic submission of orders?
In-Reply-To: <b1f16d9d0907011244v5068aac4pe732b95d7e0cdc3e@mail.gmail.com>
References: <b1f16d9d0907011244v5068aac4pe732b95d7e0cdc3e@mail.gmail.com>
Message-ID: <4A4C0993.5050503@cedrickjohnson.com>

Probably a roundabout way to do this would be to use Rserver and write a 
Java app using the Rserver wrapper to issue command reqHistoricalData at 
X intervals (setup a Java timer object or loop) and place orders (via 
the same java program connected to the RS wrapper) using twsOrder and 
placeOrder.

I'm not too familiar with callbacks and such in the IBrokers package.

hth,
c

Michael wrote:
> Hi all,
>
> What's the best way to set up a timer in IBrokers, and take a snapshot
> of market data at fixed interval, and then place some orders, and run
> other tasks?
>
> Also,  reqMktData(tws, twsEquity("QQQQ")) returns data continuously,
>
> is there a way for it to only return one snapshot of data at one time,
>
> just need to sample at fixed interval using a timer...
>
> Thank you very much!
>
> -----------
>
> And how to submit order such as "market open", "market close", and
> "good-until-after-some-time" in IBrokers?
>
> Thanks a lot!
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From rhelpacc at gmail.com  Thu Jul  2 03:47:14 2009
From: rhelpacc at gmail.com (R_help Help)
Date: Wed, 1 Jul 2009 21:47:14 -0400
Subject: [R-SIG-Finance] Earliest available data on yahoo to download
Message-ID: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com>

Hi - I used xts and zoo to try to download stock data from yahoo. I
set the start date to 2005-01-01. I got only portion starting from
2007-01-03. However, if I got to yahoo and manually click to download
historical data it will give me everything. So is this a known problem
with xts and zoo? Or is it a problem with yahoo? Is there anyway
around? Thank you in advance.

ads


From josh.m.ulrich at gmail.com  Thu Jul  2 03:53:51 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 1 Jul 2009 20:53:51 -0500
Subject: [R-SIG-Finance] Earliest available data on yahoo to download
In-Reply-To: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com>
References: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com>
Message-ID: <8cca69990907011853t2157192w3b81dbe75440c1df@mail.gmail.com>

It would really help if you provided reproducible code, especially
since neither zoo or xts have methods to access Yahoo data.

Best,
Josh
--
http://www.fosstrading.com



On Wed, Jul 1, 2009 at 8:47 PM, R_help Help<rhelpacc at gmail.com> wrote:
> Hi - I used xts and zoo to try to download stock data from yahoo. I
> set the start date to 2005-01-01. I got only portion starting from
> 2007-01-03. However, if I got to yahoo and manually click to download
> historical data it will give me everything. So is this a known problem
> with xts and zoo? Or is it a problem with yahoo? Is there anyway
> around? Thank you in advance.
>
> ads
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From seancarmody at gmail.com  Thu Jul  2 04:01:32 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Thu, 2 Jul 2009 12:01:32 +1000
Subject: [R-SIG-Finance] Earliest available data on yahoo to download
In-Reply-To: <8cca69990907011853t2157192w3b81dbe75440c1df@mail.gmail.com>
References: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com>
	<8cca69990907011853t2157192w3b81dbe75440c1df@mail.gmail.com>
Message-ID: <ce6bbb9d0907011901w1d01a533wf0184721d90059ce@mail.gmail.com>

Assuming you are using quantmod, try the from argument. For example:

getSymbols("^AORD", from="2000-01-01")

will retrieve data going back to 2000 from yahoo while

getSymbols("^AORD")

defaults to just retrieving data from the last two years.

Sean.

On Thu, Jul 2, 2009 at 11:53 AM, Joshua Ulrich<josh.m.ulrich at gmail.com> wrote:
> It would really help if you provided reproducible code, especially
> since neither zoo or xts have methods to access Yahoo data.
>
> Best,
> Josh
> --
> http://www.fosstrading.com
>
>
>
> On Wed, Jul 1, 2009 at 8:47 PM, R_help Help<rhelpacc at gmail.com> wrote:
>> Hi - I used xts and zoo to try to download stock data from yahoo. I
>> set the start date to 2005-01-01. I got only portion starting from
>> 2007-01-03. However, if I got to yahoo and manually click to download
>> historical data it will give me everything. So is this a known problem
>> with xts and zoo? Or is it a problem with yahoo? Is there anyway
>> around? Thank you in advance.
>>
>> ads
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Sean Carmody

The Stubborn Mule
http://www.stubbornmule.net
http://twitter.com/seancarmody


From rhelpacc at gmail.com  Thu Jul  2 04:04:55 2009
From: rhelpacc at gmail.com (R_help Help)
Date: Wed, 1 Jul 2009 22:04:55 -0400
Subject: [R-SIG-Finance] Earliest available data on yahoo to download
In-Reply-To: <8cca69990907011853t2157192w3b81dbe75440c1df@mail.gmail.com>
References: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com>
	<8cca69990907011853t2157192w3b81dbe75440c1df@mail.gmail.com>
Message-ID: <ad1ead5f0907011904oaefb74es6fcf69303c342481@mail.gmail.com>

Sorry when I said xts I meant quantmod. Please see my code below. If
you ever use quantmod getSymbols, the code below should be clear to
you. Another package was tseries. Somehow zoo vignette has an example
on it. So I just called it zoo. Regardless of which, the problem is
the same. Thanks.

> getSymbols("JPM",start="2006-01-01")
[1] "JPM"
> head(JPM)
           JPM.Open JPM.High JPM.Low JPM.Close JPM.Volume JPM.Adjusted
2007-01-03    48.00    48.37   47.59     48.07   14244700        44.71
2007-01-04    48.05    48.55   47.75     48.19    9471500        44.82
2007-01-05    48.17    48.25   47.63     47.79   10760500        44.45
2007-01-08    47.57    48.06   47.32     47.95    8239200        44.60
2007-01-09    47.90    48.11   47.36     47.75    9276700        44.41
2007-01-10    47.47    48.12   47.44     48.10   15597000        44.74


On Wed, Jul 1, 2009 at 9:53 PM, Joshua Ulrich<josh.m.ulrich at gmail.com> wrote:
> It would really help if you provided reproducible code, especially
> since neither zoo or xts have methods to access Yahoo data.
>
> Best,
> Josh
> --
> http://www.fosstrading.com
>
>
>
> On Wed, Jul 1, 2009 at 8:47 PM, R_help Help<rhelpacc at gmail.com> wrote:
>> Hi - I used xts and zoo to try to download stock data from yahoo. I
>> set the start date to 2005-01-01. I got only portion starting from
>> 2007-01-03. However, if I got to yahoo and manually click to download
>> historical data it will give me everything. So is this a known problem
>> with xts and zoo? Or is it a problem with yahoo? Is there anyway
>> around? Thank you in advance.
>>
>> ads
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From rhelpacc at gmail.com  Thu Jul  2 04:05:47 2009
From: rhelpacc at gmail.com (R_help Help)
Date: Wed, 1 Jul 2009 22:05:47 -0400
Subject: [R-SIG-Finance] Earliest available data on yahoo to download
In-Reply-To: <ce6bbb9d0907011901w1d01a533wf0184721d90059ce@mail.gmail.com>
References: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com>
	<8cca69990907011853t2157192w3b81dbe75440c1df@mail.gmail.com>
	<ce6bbb9d0907011901w1d01a533wf0184721d90059ce@mail.gmail.com>
Message-ID: <ad1ead5f0907011905u55291450x1b3343fd8f329d1d@mail.gmail.com>

Is there anyway to avoid this default at all? Thank you.

On Wed, Jul 1, 2009 at 10:01 PM, Sean Carmody<seancarmody at gmail.com> wrote:
> Assuming you are using quantmod, try the from argument. For example:
>
> getSymbols("^AORD", from="2000-01-01")
>
> will retrieve data going back to 2000 from yahoo while
>
> getSymbols("^AORD")
>
> defaults to just retrieving data from the last two years.
>
> Sean.
>
> On Thu, Jul 2, 2009 at 11:53 AM, Joshua Ulrich<josh.m.ulrich at gmail.com> wrote:
>> It would really help if you provided reproducible code, especially
>> since neither zoo or xts have methods to access Yahoo data.
>>
>> Best,
>> Josh
>> --
>> http://www.fosstrading.com
>>
>>
>>
>> On Wed, Jul 1, 2009 at 8:47 PM, R_help Help<rhelpacc at gmail.com> wrote:
>>> Hi - I used xts and zoo to try to download stock data from yahoo. I
>>> set the start date to 2005-01-01. I got only portion starting from
>>> 2007-01-03. However, if I got to yahoo and manually click to download
>>> historical data it will give me everything. So is this a known problem
>>> with xts and zoo? Or is it a problem with yahoo? Is there anyway
>>> around? Thank you in advance.
>>>
>>> ads
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Sean Carmody
>
> The Stubborn Mule
> http://www.stubbornmule.net
> http://twitter.com/seancarmody
>


From seancarmody at gmail.com  Thu Jul  2 04:10:04 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Thu, 2 Jul 2009 12:10:04 +1000
Subject: [R-SIG-Finance] Earliest available data on yahoo to download
In-Reply-To: <ad1ead5f0907011904oaefb74es6fcf69303c342481@mail.gmail.com>
References: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com>
	<8cca69990907011853t2157192w3b81dbe75440c1df@mail.gmail.com>
	<ad1ead5f0907011904oaefb74es6fcf69303c342481@mail.gmail.com>
Message-ID: <ce6bbb9d0907011910o5c9aad62h36cf8f8599688827@mail.gmail.com>

Use "from" rather than "start" and you'll be fine.
Sean.

On Thu, Jul 2, 2009 at 12:04 PM, R_help Help<rhelpacc at gmail.com> wrote:
> Sorry when I said xts I meant quantmod. Please see my code below. If
> you ever use quantmod getSymbols, the code below should be clear to
> you. Another package was tseries. Somehow zoo vignette has an example
> on it. So I just called it zoo. Regardless of which, the problem is
> the same. Thanks.
>
>> getSymbols("JPM",start="2006-01-01")
> [1] "JPM"
>> head(JPM)
> ? ? ? ? ? JPM.Open JPM.High JPM.Low JPM.Close JPM.Volume JPM.Adjusted
> 2007-01-03 ? ?48.00 ? ?48.37 ? 47.59 ? ? 48.07 ? 14244700 ? ? ? ?44.71
> 2007-01-04 ? ?48.05 ? ?48.55 ? 47.75 ? ? 48.19 ? ?9471500 ? ? ? ?44.82
> 2007-01-05 ? ?48.17 ? ?48.25 ? 47.63 ? ? 47.79 ? 10760500 ? ? ? ?44.45
> 2007-01-08 ? ?47.57 ? ?48.06 ? 47.32 ? ? 47.95 ? ?8239200 ? ? ? ?44.60
> 2007-01-09 ? ?47.90 ? ?48.11 ? 47.36 ? ? 47.75 ? ?9276700 ? ? ? ?44.41
> 2007-01-10 ? ?47.47 ? ?48.12 ? 47.44 ? ? 48.10 ? 15597000 ? ? ? ?44.74
>
>
> On Wed, Jul 1, 2009 at 9:53 PM, Joshua Ulrich<josh.m.ulrich at gmail.com> wrote:
>> It would really help if you provided reproducible code, especially
>> since neither zoo or xts have methods to access Yahoo data.
>>
>> Best,
>> Josh
>> --
>> http://www.fosstrading.com
>>
>>
>>
>> On Wed, Jul 1, 2009 at 8:47 PM, R_help Help<rhelpacc at gmail.com> wrote:
>>> Hi - I used xts and zoo to try to download stock data from yahoo. I
>>> set the start date to 2005-01-01. I got only portion starting from
>>> 2007-01-03. However, if I got to yahoo and manually click to download
>>> historical data it will give me everything. So is this a known problem
>>> with xts and zoo? Or is it a problem with yahoo? Is there anyway
>>> around? Thank you in advance.
>>>
>>> ads
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Sean Carmody

The Stubborn Mule
http://www.stubbornmule.net
http://twitter.com/seancarmody


From rhelpacc at gmail.com  Thu Jul  2 04:22:17 2009
From: rhelpacc at gmail.com (R_help Help)
Date: Wed, 1 Jul 2009 22:22:17 -0400
Subject: [R-SIG-Finance] Earliest available data on yahoo to download
In-Reply-To: <ce6bbb9d0907011910o5c9aad62h36cf8f8599688827@mail.gmail.com>
References: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com>
	<8cca69990907011853t2157192w3b81dbe75440c1df@mail.gmail.com>
	<ad1ead5f0907011904oaefb74es6fcf69303c342481@mail.gmail.com>
	<ce6bbb9d0907011910o5c9aad62h36cf8f8599688827@mail.gmail.com>
Message-ID: <ad1ead5f0907011922g7af0aac1p1f26e821d4147e7b@mail.gmail.com>

Very cool Sean. Thank you!

On Wed, Jul 1, 2009 at 10:10 PM, Sean Carmody<seancarmody at gmail.com> wrote:
> Use "from" rather than "start" and you'll be fine.
> Sean.
>
> On Thu, Jul 2, 2009 at 12:04 PM, R_help Help<rhelpacc at gmail.com> wrote:
>> Sorry when I said xts I meant quantmod. Please see my code below. If
>> you ever use quantmod getSymbols, the code below should be clear to
>> you. Another package was tseries. Somehow zoo vignette has an example
>> on it. So I just called it zoo. Regardless of which, the problem is
>> the same. Thanks.
>>
>>> getSymbols("JPM",start="2006-01-01")
>> [1] "JPM"
>>> head(JPM)
>> ? ? ? ? ? JPM.Open JPM.High JPM.Low JPM.Close JPM.Volume JPM.Adjusted
>> 2007-01-03 ? ?48.00 ? ?48.37 ? 47.59 ? ? 48.07 ? 14244700 ? ? ? ?44.71
>> 2007-01-04 ? ?48.05 ? ?48.55 ? 47.75 ? ? 48.19 ? ?9471500 ? ? ? ?44.82
>> 2007-01-05 ? ?48.17 ? ?48.25 ? 47.63 ? ? 47.79 ? 10760500 ? ? ? ?44.45
>> 2007-01-08 ? ?47.57 ? ?48.06 ? 47.32 ? ? 47.95 ? ?8239200 ? ? ? ?44.60
>> 2007-01-09 ? ?47.90 ? ?48.11 ? 47.36 ? ? 47.75 ? ?9276700 ? ? ? ?44.41
>> 2007-01-10 ? ?47.47 ? ?48.12 ? 47.44 ? ? 48.10 ? 15597000 ? ? ? ?44.74
>>
>>
>> On Wed, Jul 1, 2009 at 9:53 PM, Joshua Ulrich<josh.m.ulrich at gmail.com> wrote:
>>> It would really help if you provided reproducible code, especially
>>> since neither zoo or xts have methods to access Yahoo data.
>>>
>>> Best,
>>> Josh
>>> --
>>> http://www.fosstrading.com
>>>
>>>
>>>
>>> On Wed, Jul 1, 2009 at 8:47 PM, R_help Help<rhelpacc at gmail.com> wrote:
>>>> Hi - I used xts and zoo to try to download stock data from yahoo. I
>>>> set the start date to 2005-01-01. I got only portion starting from
>>>> 2007-01-03. However, if I got to yahoo and manually click to download
>>>> historical data it will give me everything. So is this a known problem
>>>> with xts and zoo? Or is it a problem with yahoo? Is there anyway
>>>> around? Thank you in advance.
>>>>
>>>> ads
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Sean Carmody
>
> The Stubborn Mule
> http://www.stubbornmule.net
> http://twitter.com/seancarmody
>


From josh.m.ulrich at gmail.com  Thu Jul  2 04:47:45 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 1 Jul 2009 21:47:45 -0500
Subject: [R-SIG-Finance] Earliest available data on yahoo to download
In-Reply-To: <ad1ead5f0907011905u55291450x1b3343fd8f329d1d@mail.gmail.com>
References: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com> 
	<8cca69990907011853t2157192w3b81dbe75440c1df@mail.gmail.com> 
	<ce6bbb9d0907011901w1d01a533wf0184721d90059ce@mail.gmail.com> 
	<ad1ead5f0907011905u55291450x1b3343fd8f329d1d@mail.gmail.com>
Message-ID: <8cca69990907011947r771794cara9b7f07dc82e9ec0@mail.gmail.com>

On Wed, Jul 1, 2009 at 9:05 PM, R_help Help<rhelpacc at gmail.com> wrote:
> Is there anyway to avoid this default at all? Thank you.
>

You can use the Defaults package (it's loaded with quantmod) to set
your own preferred default arguments to many functions.  See the
Defaults package vignette for details, but this should get you
started:

> library(quantmod)
> setDefaults(getSymbols.yahoo,from='1900-01-01')
> getSymbols("AAPL")
[1] "AAPL"
> head(AAPL)
           AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted
1984-09-07     26.50     26.87    26.25      26.50     2981600          3.02
1984-09-10     26.50     26.62    25.87      26.37     2346400          3.01
1984-09-11     26.62     27.37    26.62      26.87     5444000          3.07
1984-09-12     26.87     27.00    26.12      26.12     4773600          2.98
1984-09-13     27.50     27.62    27.50      27.50     7429600          3.14
1984-09-14     27.62     28.50    27.62      27.87     8826400          3.18

HTH,
Josh
--
http://www.fosstrading.com

> On Wed, Jul 1, 2009 at 10:01 PM, Sean Carmody<seancarmody at gmail.com> wrote:
>> Assuming you are using quantmod, try the from argument. For example:
>>
>> getSymbols("^AORD", from="2000-01-01")
>>
>> will retrieve data going back to 2000 from yahoo while
>>
>> getSymbols("^AORD")
>>
>> defaults to just retrieving data from the last two years.
>>
>> Sean.
>>
>> --
>> Sean Carmody
>>
>> The Stubborn Mule
>> http://www.stubbornmule.net
>> http://twitter.com/seancarmody
>>
>


From ggrothendieck at gmail.com  Thu Jul  2 06:01:57 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 2 Jul 2009 00:01:57 -0400
Subject: [R-SIG-Finance] Earliest available data on yahoo to download
In-Reply-To: <ad1ead5f0907011904oaefb74es6fcf69303c342481@mail.gmail.com>
References: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com> 
	<8cca69990907011853t2157192w3b81dbe75440c1df@mail.gmail.com> 
	<ad1ead5f0907011904oaefb74es6fcf69303c342481@mail.gmail.com>
Message-ID: <971536df0907012101h12ec7bb8v713918f263887781@mail.gmail.com>

The zoo vignette has an example of get.hist.quote from tseries, not
getSymbols from quantmod.

On Wed, Jul 1, 2009 at 10:04 PM, R_help Help<rhelpacc at gmail.com> wrote:
> Sorry when I said xts I meant quantmod. Please see my code below. If
> you ever use quantmod getSymbols, the code below should be clear to
> you. Another package was tseries. Somehow zoo vignette has an example
> on it. So I just called it zoo. Regardless of which, the problem is
> the same. Thanks.
>
>> getSymbols("JPM",start="2006-01-01")
> [1] "JPM"
>> head(JPM)
> ? ? ? ? ? JPM.Open JPM.High JPM.Low JPM.Close JPM.Volume JPM.Adjusted
> 2007-01-03 ? ?48.00 ? ?48.37 ? 47.59 ? ? 48.07 ? 14244700 ? ? ? ?44.71
> 2007-01-04 ? ?48.05 ? ?48.55 ? 47.75 ? ? 48.19 ? ?9471500 ? ? ? ?44.82
> 2007-01-05 ? ?48.17 ? ?48.25 ? 47.63 ? ? 47.79 ? 10760500 ? ? ? ?44.45
> 2007-01-08 ? ?47.57 ? ?48.06 ? 47.32 ? ? 47.95 ? ?8239200 ? ? ? ?44.60
> 2007-01-09 ? ?47.90 ? ?48.11 ? 47.36 ? ? 47.75 ? ?9276700 ? ? ? ?44.41
> 2007-01-10 ? ?47.47 ? ?48.12 ? 47.44 ? ? 48.10 ? 15597000 ? ? ? ?44.74
>
>
> On Wed, Jul 1, 2009 at 9:53 PM, Joshua Ulrich<josh.m.ulrich at gmail.com> wrote:
>> It would really help if you provided reproducible code, especially
>> since neither zoo or xts have methods to access Yahoo data.
>>
>> Best,
>> Josh
>> --
>> http://www.fosstrading.com
>>
>>
>>
>> On Wed, Jul 1, 2009 at 8:47 PM, R_help Help<rhelpacc at gmail.com> wrote:
>>> Hi - I used xts and zoo to try to download stock data from yahoo. I
>>> set the start date to 2005-01-01. I got only portion starting from
>>> 2007-01-03. However, if I got to yahoo and manually click to download
>>> historical data it will give me everything. So is this a known problem
>>> with xts and zoo? Or is it a problem with yahoo? Is there anyway
>>> around? Thank you in advance.
>>>
>>> ads
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From weihanliu2002 at yahoo.com  Thu Jul  2 07:01:17 2009
From: weihanliu2002 at yahoo.com (Wei-han Liu)
Date: Wed, 1 Jul 2009 22:01:17 -0700 (PDT)
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <4A4BC2B9.8070905@burns-stat.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>
	<190882.92554.qm@web53507.mail.re2.yahoo.com>
	<37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>
	<5A3D018CBDC36B4F8FF6DB52DDF3B82DA2AC76@BE-S0500-V22.adroot.local>
	<37673c2d0907011205l995e7afga29653f7615fc0ff@mail.gmail.com>
	<4A4BC2B9.8070905@burns-stat.com>
Message-ID: <762033.19687.qm@web53508.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090701/bd8b4d23/attachment.pl>

From patrick at burns-stat.com  Thu Jul  2 09:16:13 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 02 Jul 2009 08:16:13 +0100
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <37673c2d0907011609p22f6f379m5d2d0471eaf15c9b@mail.gmail.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>	
	<190882.92554.qm@web53507.mail.re2.yahoo.com>	
	<37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>	
	<5A3D018CBDC36B4F8FF6DB52DDF3B82DA2AC76@BE-S0500-V22.adroot.local>	
	<37673c2d0907011205l995e7afga29653f7615fc0ff@mail.gmail.com>	
	<4A4BC2B9.8070905@burns-stat.com>
	<37673c2d0907011609p22f6f379m5d2d0471eaf15c9b@mail.gmail.com>
Message-ID: <4A4C5EBD.1080106@burns-stat.com>

Debashis Dutta wrote:
> Dear Patrick,
>  

[ ... some interesting stuff ...]

>  
>  Patrick,  I am  interesed in your work in eqities. Could you kindly 
> share with me?

"The Quality of Value at Risk via Univariate GARCH"
http://www.burns-stat.com/pages/Working/varunigar.pdf

Pat

>  
> Kind Regards,
> Debashis
> 
> 

[... more stuff ...]


From jeff.a.ryan at gmail.com  Thu Jul  2 09:17:23 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 2 Jul 2009 02:17:23 -0500
Subject: [R-SIG-Finance] IBrokers and automatic submission of orders?
In-Reply-To: <4A4C0993.5050503@cedrickjohnson.com>
References: <b1f16d9d0907011244v5068aac4pe732b95d7e0cdc3e@mail.gmail.com>
	<4A4C0993.5050503@cedrickjohnson.com>
Message-ID: <e8e755250907020017y2ee57a6bo2712b7d2bfcf508c@mail.gmail.com>

I just presented some slides about this at R/Rmetrics 2009 in
Meielisalp about 2 days ago.

This should give you a bit more information on the current development
of IBrokers, and how you can accomplish this all in R.

HTH,
Jeff

On Wed, Jul 1, 2009 at 8:12 PM, Cedrick
Johnson<cedrick at cedrickjohnson.com> wrote:
> Probably a roundabout way to do this would be to use Rserver and write a
> Java app using the Rserver wrapper to issue command reqHistoricalData at X
> intervals (setup a Java timer object or loop) and place orders (via the same
> java program connected to the RS wrapper) using twsOrder and placeOrder.
>
> I'm not too familiar with callbacks and such in the IBrokers package.
>
> hth,
> c
>
> Michael wrote:
>>
>> Hi all,
>>
>> What's the best way to set up a timer in IBrokers, and take a snapshot
>> of market data at fixed interval, and then place some orders, and run
>> other tasks?
>>
>> Also, ?reqMktData(tws, twsEquity("QQQQ")) returns data continuously,
>>
>> is there a way for it to only return one snapshot of data at one time,
>>
>> just need to sample at fixed interval using a timer...
>>
>> Thank you very much!
>>
>> -----------
>>
>> And how to submit order such as "market open", "market close", and
>> "good-until-after-some-time" in IBrokers?
>>
>> Thanks a lot!
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From patrick at burns-stat.com  Thu Jul  2 09:22:21 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 02 Jul 2009 08:22:21 +0100
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <762033.19687.qm@web53508.mail.re2.yahoo.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>	<190882.92554.qm@web53507.mail.re2.yahoo.com>	<37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>	<5A3D018CBDC36B4F8FF6DB52DDF3B82DA2AC76@BE-S0500-V22.adroot.local>	<37673c2d0907011205l995e7afga29653f7615fc0ff@mail.gmail.com>	<4A4BC2B9.8070905@burns-stat.com>
	<762033.19687.qm@web53508.mail.re2.yahoo.com>
Message-ID: <4A4C602D.4080807@burns-stat.com>

The issue of in-sample versus out-of-sample
is key.  Performance in-sample is not at all
a useful measure -- we need to look out of
sample.  There is another issue with evaluating
VaR since we only know if the prediction
exceeded the actual or not.  It takes a LOT
of replications to see if a VaR method is
working well or not.

Pat


Wei-han Liu wrote:
> Hi,
> 
> Thanks to Patrick. I think he raised the question that I had in mind. My initial idea is that if? we want to do the forecasting, the distriubution change and the volatility process are the two key issues. That is part of reasons that GARCH models are the best candidates when it comes to Value-at-Risk forecasting. However, I have no idea if extreme value theory has developed in this two issues. To me, extreme value theory can do a pretty good job in in-sample fitting. What about in out-sample forecasting? I wonder.
> 
> Or what we can do is to assume the constant return distribution and the n-period ahead forecasts are calculated as the last-period estimate multiplied by the squart root of the number of periods ahead.
> 
> Please be generous enough to share your opinions, suggestion, or corrections.
> 
> Wei-han
> 
> 
> 
> ________________________________
> From: Patrick Burns <patrick at burns-stat.com>
> To: Debashis Dutta <dutt.debashis at gmail.com>
> Cc: R-SIG-Finance at stat.math.ethz.ch; Murali.MENON at fortisinvestments.com
> Sent: Thursday, July 2, 2009 4:10:33 AM
> Subject: Re: [R-SIG-Finance] Fw: Value-at-Risk
> 
> In doing the forecasting there are two
> things to get right: the distribution
> and the changes in volatility.?  In the
> research that I've done, getting the
> volatility changes right appeared to be
> much more important than getting the
> distribution right.
> 
> I have no problem with bringing extreme
> value theory in, but I don't see how the
> volatility issue can be avoided.?  There
> is the problem of heteroskedasticity
> when estimating the tails that makes
> EVT tricky to apply.
> 
> The work I did was in equities, but I
> suspect that the situation wouldn't
> be all that different for other asset
> classes.
> 
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of "The R Inferno" and "A Guide for the Unwilling S User")
> 
> Debashis Dutta wrote:
>> Dear Murali,
>>
>> I fundamentally disagree with you on your comments ??????The assumption of a
>> constant tail index results in misleading VaR at the extreme tails,
>> especially when there are several regime shifts.??? The works exhibited by
>> Researchers and study made by Practitioners proved the contrary.
>>
>> Please see a recent paper titled ???The Extreme Value Theory Value Theory
>> Performance in the event of major financial crisis" by Adrian F. Rossignolo
>> Uiversity of BuenosAirs, February 2009.
>>
>>
>>
>> The key outcome:
>>
>>
>>
>> Extreme Value Theory (EVT) provides a method to estimate VaR at high
>> quantiles of the distribution, consequently focusing on extraordinary and
>> unusual circumstances. ??? EVT to calculate VaR for six stock market indices
>> belonging to developed and emerging markets in two different ways:
>> Unconditional EVT on raw returns and Conditional EVT which blends
>> Quasi-Maximum-Likelihood fitting of GARCH models to estimate current dynamic
>> volatility and EVT for estimating the tails of the innovation distribution
>> of the GARCH residuals (both tails independently). Backtesting EVT
>> representations using turmoil recorded in 2008, and comparing their
>> performance with that of the most popular representations nowadays in vogue,
>> it is found that EVT schemes could help institutions to avoid huge losses
>> arising from market disasters. A simple exercise on the constitution of
>> Regulatory Capital illustrates the advantages of EVT.
>>
>>
>>
>> I being a practitioner agree with Adrain.
>>
>>
>>
>> There is also a recent study on POT in GCC.
>>
>>
>>
>> The paper ??? The tail behavior of extreme stock returns in the Gulf emerging
>> markets: An implication for financial risk management??? by Aktham I.
>> Maghyereh and Haitham A. Al-Zoubi , Studies in Economics and
>> Finance<http://www.emeraldinsight.com/1086-7376.htm>,
>> 2008, Vol. - 25, Issue 1, 21-37.
>>
>>
>>
>> Findings ??? Not only is the heavy tail found to be a facial appearance in
>> these markets, but also POT method of modelling extreme tail quantiles is
>> more accurate than conventional methodologies (historical simulation and
>> normal distribution models) in estimating the tail behavior of the Gulf
>> markets returns. Across all return series, it is found that left and right
>> tails behave very different across countries.
>>
>>
>>
>> I am sorry to disagree with your comment. ???Likewise, in the credit markets,
>> EVT may not do too well.???
>>
>> Please see the Basel Paper ???Extreme tails for linear portfolio credit risk
>> models??? by Andr?? Lucas, Pieter Klaassen,Peter Spreij and Stefan Straetmans.
>>
>>
>>
>> Concluding Remarks of the paper:
>>
>> Upon comparing the analytic tail probabilities with their extreme value
>> counterparts, we found that the extreme value probabilities come close to
>> their true values provided one goes very far into the credit loss tail.
>>
>>
>>
>> The word of caution that is also mentioned in the concluding remarks
>>
>>
>>
>> ?  ???We conclude that standard use of EVT methods as applied in, for example,
>> the market risk context is inappropriate in the credit risk context.???
>> Possibly you have mistaken this comment.
>>
>>
>>
>> Kind Regards,
>>
>> Debashis
>>
>>
>>
>> 2009/7/1 <Murali.MENON at fortisinvestments.com>
>>
>>> Hi,
>>> Regarding the use of EVT-based VaR, I think it is dependent on the asset
>>> class. In exchange rates, e.g., you may generally be served well with
>>> EVT for the major currencies, but might do quite badly with emerging
>>> market currencies. Likewise, in the credit markets, EVT may not do too
>>> well. The assumption of a constant tail index results in misleading VaR
>>> at the extreme tails, especially when there are several regime shifts.
>>> Take a look at the paper "Testing for Multiple Regimes in the Tail
>>> Behavior of Emerging Currency Returns" by B. Candelon and S. Straetmans,
>>> LIFE Working Paper 03-035.
>>> Cheers,
>>> Murali
>>>
>>>
>>> -----Original Message-----
>>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Debashis
>>> Dutta
>>> Sent: 01 July 2009 14:25
>>> To: Wei-han Liu
>>> Cc: R-SIG-Finance at stat.math.ethz.ch
>>> Subject: Re: [R-SIG-Finance] Fw: Value-at-Risk
>>>
>>> Dear Wei-han,
>>>
>>> I believe EVT based VaR would provide a better solution specially in
>>> stressed situation like the present one, modeling the extremal behaviour
>>> in the tail. I used Peaks Over Threshold (POT) based VaR method in my
>>> doctoral dissertation.
>>>
>>> Back testing and comparing the new method to existing ones on real
>>> financial events show that this POT based VaR method provides a rather
>>> realistic model for the extremal behavior of financial processes,
>>> enabling a precise estimation of risk measures. Through the GPD , the
>>> model provides a way of estimating the tail behaviour of the random
>>> variables without knowledge of the true distribution and as such it is a
>>> good candidate for Vale at Risk computation.
>>>
>>> Most common at this moment is the tail-fitting approach based on the
>>> second theorem in extreme value theory (Theorem II Pickands(1975),
>>> Balkema and de Haan(1974)). In general this conforms to the first
>>> theorem in extreme value theory (Theorem I Fisher and Tippett(1928), and
>>> Gnedenko (1943)).The difference between the two theorems is due to the
>>> nature of the data generation.
>>>
>>> ?  For theorem I the data are generated in full range, while in theorem II
>>> data is only generated when it surpasses a certain threshold (POT's
>>> models or Peak Over Threshold). The POT approach has been developed
>>> largely in the insurance business, where only losses (pay outs) above a
>>> certain threshold are accessible to the insurance company.
>>>
>>> ?  Kind Regards,
>>> Debashis
>>>
> 
>>>> Thanks a lot, Robert.
>>>>
>>>> I know GARCH models has its forecasting capacity as the reference you
>>>> shared indicates.
>>>>
>>>> I wonder if the Value-at-Risk estimated by extreme value theory can
>>>> also be used for forecasting purpose. Is there some theory background
>>>> in this regard?
>>>>
>>>> Wei-han
>>>>
>>>>
>>>>
>>>> ----- Forwarded Message ----
>>>> From: Robert Iquiapaza <rbali at ufmg.br>
> 
>>>> r-sig-finance at stat.math.ethz.ch" <R-SIG-Finance at stat.math.ethz.ch>
>>>> Sent: Wednesday, July 1, 2009 6:37:21 PM
>>>> Subject: Re: [R-SIG-Finance] Value-at-Risk
>>>>
>>>> See for example "Accurate value-at-risk forecasting based on the
>>>> normal-GARCH model" by C Hartz, S Mittnik, M Paolella - Computational
>>>> Statistics and Data Analysis, 2006
>>>>
>>>> best
>>>>
>>>> --------------------------------------------------
>>>>
>>>> Sent: Tuesday, June 30, 2009 12:16 PM
>>>> To: <R-SIG-Finance at stat.math.ethz.ch>
>>>> Subject: [R-SIG-Finance] Value-at-Risk
>>>>
>>>>> Dear R-users:
>>>>>
>>>>> Several questions please on Value-at-Risk.
>>>>>
>>>>> Is Value-at-Risk designed for forecasting purpose?
>>>>>
>>>>> I wonder if Value-at-Risk estimated by in-sample data can be used
>>>>> for
>>>> out-of-sample forecasting?
>>>>> If in-sample Value-at-Risk is estimated by several methods, is it
>>>> appropriate to do the model comparisons based on out--of-sample
>>> performance?
>>>>> Wei-han Liu
>>>>
>>>> ?  ?  ?  ?  [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>> ?  ?  ?  ?  [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>> ? ? ?  [[alternative HTML version deleted]]
>>
>>
>>
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 
> 
>       
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Thu Jul  2 09:30:28 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 2 Jul 2009 02:30:28 -0500
Subject: [R-SIG-Finance] Earliest available data on yahoo to download
In-Reply-To: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com>
References: <ad1ead5f0907011847o7401b727gc9a7eb29819c7bcb@mail.gmail.com>
Message-ID: <e8e755250907020030w38062c08p64f7948b47494157@mail.gmail.com>

R_help Help:

While I know this has been answered, I'll just make a few comments
w.r.t. getting the most out of R-finance.

You should really use your real name.  We are all actual people, who
spend actual time on writing, documenting, and supporting our code.  I
realize it isn't a universal standard, but real names are generally
appreciated.

The help files [very well written, mind you ;-) ], are the first place
to always start.  From there a look at the args() or source is
helpful. Following that a search of previous posts is a good next
step.  After all that fails, posting an well written question with a
self-contained example is really the only way to ask for help.

?getSymbols
example(getSymbols)
args(getSymbols.yahoo)

There is also a reasonable amount of code/examples on http://www.quantmod.com


All would have pointed out that the function recognizes 'start' as
much as it does 'nonsense_madeup_arg'.

In general using the list isn't a 'do my homework', but rather a 'I've
done my homework, and I still can't figure it out' tool.

HTH
Jeff

On Wed, Jul 1, 2009 at 8:47 PM, R_help Help<rhelpacc at gmail.com> wrote:
> Hi - I used xts and zoo to try to download stock data from yahoo. I
> set the start date to 2005-01-01. I got only portion starting from
> 2007-01-03. However, if I got to yahoo and manually click to download
> historical data it will give me everything. So is this a known problem
> with xts and zoo? Or is it a problem with yahoo? Is there anyway
> around? Thank you in advance.
>
> ads
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From weihanliu2002 at yahoo.com  Thu Jul  2 10:00:34 2009
From: weihanliu2002 at yahoo.com (Wei-han Liu)
Date: Thu, 2 Jul 2009 01:00:34 -0700 (PDT)
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <4A4C602D.4080807@burns-stat.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>
	<190882.92554.qm@web53507.mail.re2.yahoo.com>
	<37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>
	<5A3D018CBDC36B4F8FF6DB52DDF3B82DA2AC76@BE-S0500-V22.adroot.local>
	<37673c2d0907011205l995e7afga29653f7615fc0ff@mail.gmail.com>
	<4A4BC2B9.8070905@burns-stat.com>
	<762033.19687.qm@web53508.mail.re2.yahoo.com>
	<4A4C602D.4080807@burns-stat.com>
Message-ID: <942201.15462.qm@web53504.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090702/b9167c51/attachment.pl>

From patrick at burns-stat.com  Thu Jul  2 12:27:49 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 02 Jul 2009 11:27:49 +0100
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <942201.15462.qm@web53504.mail.re2.yahoo.com>
References: <817126.55167.qm@web53501.mail.re2.yahoo.com>	<190882.92554.qm@web53507.mail.re2.yahoo.com>	<37673c2d0907010625q4e810c7byf4a850f0696848c8@mail.gmail.com>	<5A3D018CBDC36B4F8FF6DB52DDF3B82DA2AC76@BE-S0500-V22.adroot.local>	<37673c2d0907011205l995e7afga29653f7615fc0ff@mail.gmail.com>	<4A4BC2B9.8070905@burns-stat.com>	<762033.19687.qm@web53508.mail.re2.yahoo.com>	<4A4C602D.4080807@burns-stat.com>
	<942201.15462.qm@web53504.mail.re2.yahoo.com>
Message-ID: <4A4C8BA5.6080105@burns-stat.com>

If your body temperature varies like
market volatility, I suggest that you
consult an endocrinologist without
delay.

Pat


Wei-han Liu wrote:
> Hi again:
> 
> I'd say performance in-sample IS a useful measure because Value-at-Risk is risk measure of the risk level of the portfolio's mark-to-market value, as the definition quoted form WiKi shows. Thermometer can tell us the current body temperature even though it can hardly forecast when we will have a health condition.
> 
> Defition: VaR is defined as a threshold value such that the probability that the mark-to-market loss on the portfolio over the given time horizon exceeds this value? is the given probability level.
> 
> Wei-han
> 
> 
> ________________________________
> From: Patrick Burns <patrick at burns-stat.com>
> 
> Cc: R-SIG-Finance at stat.math.ethz.ch
> Sent: Thursday, July 2, 2009 3:22:21 PM
> Subject: Re: [R-SIG-Finance] Fw: Value-at-Risk
> 
> The issue of in-sample versus out-of-sample
> is key.?  Performance in-sample is not at all
> a useful measure -- we need to look out of
> sample.?  There is another issue with evaluating
> VaR since we only know if the prediction
> exceeded the actual or not.?  It takes a LOT
> of replications to see if a VaR method is
> working well or not.
> 
> Pat
> 
> 
> Wei-han Liu wrote:
>> Hi,
>>
>> Thanks to Patrick. I think he raised the question that I had in mind. My initial idea is that if?? we want to do the forecasting, the distriubution change and the volatility process are the two key issues. That is part of reasons that GARCH models are the best candidates when it comes to Value-at-Risk forecasting. However, I have no idea if extreme value theory has developed in this two issues. To me, extreme value theory can do a pretty good job in in-sample fitting. What about in out-sample forecasting? I wonder.
>>
>> Or what we can do is to assume the constant return distribution and the n-period ahead forecasts are calculated as the last-period estimate multiplied by the squart root of the number of periods ahead.
>>
>> Please be generous enough to share your opinions, suggestion, or corrections.
>>
>> Wei-han
>>
>>
>>
>> ________________________________
>> From: Patrick Burns <patrick at burns-stat.com>
>> To: Debashis Dutta <dutt.debashis at gmail.com>
>> Cc: R-SIG-Finance at stat.math.ethz.ch; Murali.MENON at fortisinvestments.com
>> Sent: Thursday, July 2, 2009 4:10:33 AM
>> Subject: Re: [R-SIG-Finance] Fw: Value-at-Risk
>>
>> In doing the forecasting there are two
>> things to get right: the distribution
>> and the changes in volatility.???  In the
>> research that I've done, getting the
>> volatility changes right appeared to be
>> much more important than getting the
>> distribution right.
>>
>> I have no problem with bringing extreme
>> value theory in, but I don't see how the
>> volatility issue can be avoided.???  There
>> is the problem of heteroskedasticity
>> when estimating the tails that makes
>> EVT tricky to apply.
>>
>> The work I did was in equities, but I
>> suspect that the situation wouldn't
>> be all that different for other asset
>> classes.
>>
>>
>> Patrick Burns
>> patrick at burns-stat.com
>> +44 (0)20 8525 0696
>> http://www.burns-stat.com
>> (home of "The R Inferno" and "A Guide for the Unwilling S User")
>>
>> Debashis Dutta wrote:
>>> Dear Murali,
>>>
>>> I fundamentally disagree with you on your comments ??????????????The assumption of a
>>> constant tail index results in misleading VaR at the extreme tails,
>>> especially when there are several regime shifts.???????? The works exhibited by
>>> Researchers and study made by Practitioners proved the contrary.
>>>
>>> Please see a recent paper titled ???????The Extreme Value Theory Value Theory
>>> Performance in the event of major financial crisis" by Adrian F. Rossignolo
>>> Uiversity of BuenosAirs, February 2009.
>>>
>>>
>>>
>>> The key outcome:
>>>
>>>
>>>
>>> Extreme Value Theory (EVT) provides a method to estimate VaR at high
>>> quantiles of the distribution, consequently focusing on extraordinary and
>>> unusual circumstances. ??????? EVT to calculate VaR for six stock market indices
>>> belonging to developed and emerging markets in two different ways:
>>> Unconditional EVT on raw returns and Conditional EVT which blends
>>> Quasi-Maximum-Likelihood fitting of GARCH models to estimate current dynamic
>>> volatility and EVT for estimating the tails of the innovation distribution
>>> of the GARCH residuals (both tails independently). Backtesting EVT
>>> representations using turmoil recorded in 2008, and comparing their
>>> performance with that of the most popular representations nowadays in vogue,
>>> it is found that EVT schemes could help institutions to avoid huge losses
>>> arising from market disasters. A simple exercise on the constitution of
>>> Regulatory Capital illustrates the advantages of EVT.
>>>
>>>
>>>
>>> I being a practitioner agree with Adrain.
>>>
>>>
>>>
>>> There is also a recent study on POT in GCC.
>>>
>>>
>>>
>>> The paper ??????? The tail behavior of extreme stock returns in the Gulf emerging
>>> markets: An implication for financial risk management???????? by Aktham I.
>>> Maghyereh and Haitham A. Al-Zoubi , Studies in Economics and
>>> Finance<http://www.emeraldinsight.com/1086-7376.htm>,
>>> 2008, Vol. - 25, Issue 1, 21-37.
>>>
>>>
>>>
>>> Findings ???????? Not only is the heavy tail found to be a facial appearance in
>>> these markets, but also POT method of modelling extreme tail quantiles is
>>> more accurate than conventional methodologies (historical simulation and
>>> normal distribution models) in estimating the tail behavior of the Gulf
>>> markets returns. Across all return series, it is found that left and right
>>> tails behave very different across countries.
>>>
>>>
>>>
>>> I am sorry to disagree with your comment. ???????Likewise, in the credit markets,
>>> EVT may not do too well.????????
>>>
>>> Please see the Basel Paper ???????Extreme tails for linear portfolio credit risk
>>> models???????? by Andr???? Lucas, Pieter Klaassen,Peter Spreij and Stefan Straetmans.
>>>
>>>
>>>
>>> Concluding Remarks of the paper:
>>>
>>> Upon comparing the analytic tail probabilities with their extreme value
>>> counterparts, we found that the extreme value probabilities come close to
>>> their true values provided one goes very far into the credit loss tail.
>>>
>>>
>>>
>>> The word of caution that is also mentioned in the concluding remarks
>>>
>>>
>>>
>>> ???  ???????We conclude that standard use of EVT methods as applied in, for example,
>>> the market risk context is inappropriate in the credit risk context.????????
>>> Possibly you have mistaken this comment.
>>>
>>>
>>>
>>> Kind Regards,
>>>
>>> Debashis
>>>
>>>
>>>
>>> 2009/7/1 <Murali.MENON at fortisinvestments.com>
>>>
>>>> Hi,
>>>> Regarding the use of EVT-based VaR, I think it is dependent on the asset
>>>> class. In exchange rates, e.g., you may generally be served well with
>>>> EVT for the major currencies, but might do quite badly with emerging
>>>> market currencies. Likewise, in the credit markets, EVT may not do too
>>>> well. The assumption of a constant tail index results in misleading VaR
>>>> at the extreme tails, especially when there are several regime shifts.
>>>> Take a look at the paper "Testing for Multiple Regimes in the Tail
>>>> Behavior of Emerging Currency Returns" by B. Candelon and S. Straetmans,
>>>> LIFE Working Paper 03-035.
>>>> Cheers,
>>>> Murali
>>>>
>>>>
>>>> -----Original Message-----
>>>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Debashis
>>>> Dutta
>>>> Sent: 01 July 2009 14:25
>>>> To: Wei-han Liu
>>>> Cc: R-SIG-Finance at stat.math.ethz.ch
>>>> Subject: Re: [R-SIG-Finance] Fw: Value-at-Risk
>>>>
>>>> Dear Wei-han,
>>>>
>>>> I believe EVT based VaR would provide a better solution specially in
>>>> stressed situation like the present one, modeling the extremal behaviour
>>>> in the tail. I used Peaks Over Threshold (POT) based VaR method in my
>>>> doctoral dissertation.
>>>>
>>>> Back testing and comparing the new method to existing ones on real
>>>> financial events show that this POT based VaR method provides a rather
>>>> realistic model for the extremal behavior of financial processes,
>>>> enabling a precise estimation of risk measures. Through the GPD , the
>>>> model provides a way of estimating the tail behaviour of the random
>>>> variables without knowledge of the true distribution and as such it is a
>>>> good candidate for Vale at Risk computation.
>>>>
>>>> Most common at this moment is the tail-fitting approach based on the
>>>> second theorem in extreme value theory (Theorem II Pickands(1975),
>>>> Balkema and de Haan(1974)). In general this conforms to the first
>>>> theorem in extreme value theory (Theorem I Fisher and Tippett(1928), and
>>>> Gnedenko (1943)).The difference between the two theorems is due to the
>>>> nature of the data generation.
>>>>
>>>> ???  For theorem I the data are generated in full range, while in theorem II
>>>> data is only generated when it surpasses a certain threshold (POT's
>>>> models or Peak Over Threshold). The POT approach has been developed
>>>> largely in the insurance business, where only losses (pay outs) above a
>>>> certain threshold are accessible to the insurance company.
>>>>
>>>> ???  Kind Regards,
>>>> Debashis
>>>>
>>>>> Thanks a lot, Robert.
>>>>>
>>>>> I know GARCH models has its forecasting capacity as the reference you
>>>>> shared indicates.
>>>>>
>>>>> I wonder if the Value-at-Risk estimated by extreme value theory can
>>>>> also be used for forecasting purpose. Is there some theory background
>>>>> in this regard?
>>>>>
>>>>> Wei-han
>>>>>
>>>>>
>>>>>
>>>>> ----- Forwarded Message ----
>>>>> From: Robert Iquiapaza <rbali at ufmg.br>
>>>>> r-sig-finance at stat.math.ethz.ch" <R-SIG-Finance at stat.math.ethz.ch>
>>>>> Sent: Wednesday, July 1, 2009 6:37:21 PM
>>>>> Subject: Re: [R-SIG-Finance] Value-at-Risk
>>>>>
>>>>> See for example "Accurate value-at-risk forecasting based on the
>>>>> normal-GARCH model" by C Hartz, S Mittnik, M Paolella - Computational
>>>>> Statistics and Data Analysis, 2006
>>>>>
>>>>> best
>>>>>
>>>>> --------------------------------------------------
>>>>>
>>>>> Sent: Tuesday, June 30, 2009 12:16 PM
>>>>> To: <R-SIG-Finance at stat.math.ethz.ch>
>>>>> Subject: [R-SIG-Finance] Value-at-Risk
>>>>>
>>>>>> Dear R-users:
>>>>>>
>>>>>> Several questions please on Value-at-Risk.
>>>>>>
>>>>>> Is Value-at-Risk designed for forecasting purpose?
>>>>>>
>>>>>> I wonder if Value-at-Risk estimated by in-sample data can be used
>>>>>> for
>>>>> out-of-sample forecasting?
>>>>>> If in-sample Value-at-Risk is estimated by several methods, is it
>>>>> appropriate to do the model comparisons based on out--of-sample
>>>> performance?
>>>>>> Wei-han Liu
>>>>> ???  ???  ???  ???  [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>> ???  ???  ???  ???  [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>> ?? ?? ???  [[alternative HTML version deleted]]
>>>
>>>
>>>
>>> ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>>
>> ?  ?  ?  
>> ? ? ?  [[alternative HTML version deleted]]
>>
>>
>>
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
> 
> 
> 
>       
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From cedrick at cedrickjohnson.com  Thu Jul  2 14:13:24 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Thu, 02 Jul 2009 08:13:24 -0400
Subject: [R-SIG-Finance] IBrokers and automatic submission of orders?
In-Reply-To: <e8e755250907020017y2ee57a6bo2712b7d2bfcf508c@mail.gmail.com>
References: <b1f16d9d0907011244v5068aac4pe732b95d7e0cdc3e@mail.gmail.com>	
	<4A4C0993.5050503@cedrickjohnson.com>
	<e8e755250907020017y2ee57a6bo2712b7d2bfcf508c@mail.gmail.com>
Message-ID: <4A4CA464.2040106@cedrickjohnson.com>

Thanks Jeff-

I'll check out the R/Rmetrics conference page. Unfortunately I missed 
that (and the Chicago one) this year.

Speaking of which, when is there going to be another Chicago gathering 
of R users, say for a meetup at a downtown pub?

-c

Jeff Ryan wrote:
> I just presented some slides about this at R/Rmetrics 2009 in
> Meielisalp about 2 days ago.
>
> This should give you a bit more information on the current development
> of IBrokers, and how you can accomplish this all in R.
>
> HTH,
> Jeff
>
> On Wed, Jul 1, 2009 at 8:12 PM, Cedrick
> Johnson<cedrick at cedrickjohnson.com> wrote:
>   
>> Probably a roundabout way to do this would be to use Rserver and write a
>> Java app using the Rserver wrapper to issue command reqHistoricalData at X
>> intervals (setup a Java timer object or loop) and place orders (via the same
>> java program connected to the RS wrapper) using twsOrder and placeOrder.
>>
>> I'm not too familiar with callbacks and such in the IBrokers package.
>>
>> hth,
>> c
>>
>> Michael wrote:
>>     
>>> Hi all,
>>>
>>> What's the best way to set up a timer in IBrokers, and take a snapshot
>>> of market data at fixed interval, and then place some orders, and run
>>> other tasks?
>>>
>>> Also,  reqMktData(tws, twsEquity("QQQQ")) returns data continuously,
>>>
>>> is there a way for it to only return one snapshot of data at one time,
>>>
>>> just need to sample at fixed interval using a timer...
>>>
>>> Thank you very much!
>>>
>>> -----------
>>>
>>> And how to submit order such as "market open", "market close", and
>>> "good-until-after-some-time" in IBrokers?
>>>
>>> Thanks a lot!
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>       
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>     
>
>
>
>


From ezivot at u.washington.edu  Thu Jul  2 17:19:53 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Thu, 2 Jul 2009 08:19:53 -0700 (PDT)
Subject: [R-SIG-Finance] Fw: Value-at-Risk
In-Reply-To: <762033.19687.qm@web53508.mail.re2.yahoo.com>
Message-ID: <Pine.LNX.4.43.0907020819530.15140@hymn13.u.washington.edu>

There have  been many papers that combine EVT with GARCH to give out-of-sample VaR predictions. The seminal paper on this technique

McNeil, A. J., and R. Frey. (2000). ??Estimation of Tail-related Risk Measures for Heteroscedastic Financial Time Series: An Extreme Value Approach.?? Journal of Empirical Finance 7:271?300.


I have a brief description of this technique in my class slides on extreme value theory

http://faculty.washington.edu/ezivot/econ512/econ512extremevalue.pdf


****************************************************************
*  Eric Zivot                  			               *
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Adjunct Professor of Finance                                *
*  Adjunct Professor of Statistics
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *                                                           *
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Wed, 1 Jul 2009, Wei-han Liu wrote:

> Hi,
>
> Thanks to Patrick. I think he raised the question that I had in mind. My initial idea is that if??we want to do the forecasting, the distriubution change and the volatility process are the two key issues. That is part of reasons that GARCH models are the best candidates when it comes to Value-at-Risk forecasting. However, I have no idea if extreme value theory has developed in this two issues. To me, extreme value theory can do a pretty good job in in-sample fitting. What about in out-sample forecasting? I wonder.
>
> Or what we can do is to assume the constant return distribution and the n-period ahead forecasts are calculated as the last-period estimate multiplied by the squart root of the number of periods ahead.
>
> Please be generous enough to share your opinions, suggestion, or corrections.
>
> Wei-han
>
>
>
> ________________________________
> From: Patrick Burns <patrick at burns-stat.com>
> To: Debashis Dutta <dutt.debashis at gmail.com>
> Cc: R-SIG-Finance at stat.math.ethz.ch; Murali.MENON at fortisinvestments.com
> Sent: Thursday, July 2, 2009 4:10:33 AM
> Subject: Re: [R-SIG-Finance] Fw: Value-at-Risk
>
> In doing the forecasting there are two
> things to get right: the distribution
> and the changes in volatility.?? In the
> research that I've done, getting the
> volatility changes right appeared to be
> much more important than getting the
> distribution right.
>
> I have no problem with bringing extreme
> value theory in, but I don't see how the
> volatility issue can be avoided.?? There
> is the problem of heteroskedasticity
> when estimating the tails that makes
> EVT tricky to apply.
>
> The work I did was in equities, but I
> suspect that the situation wouldn't
> be all that different for other asset
> classes.
>
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of "The R Inferno" and "A Guide for the Unwilling S User")
>
> Debashis Dutta wrote:
>> Dear Murali,
>>
>> I fundamentally disagree with you on your comments ??????The assumption of a
>> constant tail index results in misleading VaR at the extreme tails,
>> especially when there are several regime shifts.??? The works exhibited by
>> Researchers and study made by Practitioners proved the contrary.
>>
>> Please see a recent paper titled ???The Extreme Value Theory Value Theory
>> Performance in the event of major financial crisis" by Adrian F. Rossignolo
>> Uiversity of BuenosAirs, February 2009.
>>
>>
>>
>> The key outcome:
>>
>>
>>
>> Extreme Value Theory (EVT) provides a method to estimate VaR at high
>> quantiles of the distribution, consequently focusing on extraordinary and
>> unusual circumstances. ??? EVT to calculate VaR for six stock market indices
>> belonging to developed and emerging markets in two different ways:
>> Unconditional EVT on raw returns and Conditional EVT which blends
>> Quasi-Maximum-Likelihood fitting of GARCH models to estimate current dynamic
>> volatility and EVT for estimating the tails of the innovation distribution
>> of the GARCH residuals (both tails independently). Backtesting EVT
>> representations using turmoil recorded in 2008, and comparing their
>> performance with that of the most popular representations nowadays in vogue,
>> it is found that EVT schemes could help institutions to avoid huge losses
>> arising from market disasters. A simple exercise on the constitution of
>> Regulatory Capital illustrates the advantages of EVT.
>>
>>
>>
>> I being a practitioner agree with Adrain.
>>
>>
>>
>> There is also a recent study on POT in GCC.
>>
>>
>>
>> The paper ??? The tail behavior of extreme stock returns in the Gulf emerging
>> markets: An implication for financial risk management??? by Aktham I.
>> Maghyereh and Haitham A. Al-Zoubi , Studies in Economics and
>> Finance<http://www.emeraldinsight.com/1086-7376.htm>,
>> 2008, Vol. - 25, Issue 1, 21-37.
>>
>>
>>
>> Findings ??? Not only is the heavy tail found to be a facial appearance in
>> these markets, but also POT method of modelling extreme tail quantiles is
>> more accurate than conventional methodologies (historical simulation and
>> normal distribution models) in estimating the tail behavior of the Gulf
>> markets returns. Across all return series, it is found that left and right
>> tails behave very different across countries.
>>
>>
>>
>> I am sorry to disagree with your comment. ???Likewise, in the credit markets,
>> EVT may not do too well.???
>>
>> Please see the Basel Paper ???Extreme tails for linear portfolio credit risk
>> models??? by Andr?? Lucas, Pieter Klaassen,Peter Spreij and Stefan Straetmans.
>>
>>
>>
>> Concluding Remarks of the paper:
>>
>> Upon comparing the analytic tail probabilities with their extreme value
>> counterparts, we found that the extreme value probabilities come close to
>> their true values provided one goes very far into the credit loss tail.
>>
>>
>>
>> The word of caution that is also mentioned in the concluding remarks
>>
>>
>>
>> ?? ???We conclude that standard use of EVT methods as applied in, for example,
>> the market risk context is inappropriate in the credit risk context.???
>> Possibly you have mistaken this comment.
>>
>>
>>
>> Kind Regards,
>>
>> Debashis
>>
>>
>>
>> 2009/7/1 <Murali.MENON at fortisinvestments.com>
>>
>>> Hi,
>>> Regarding the use of EVT-based VaR, I think it is dependent on the asset
>>> class. In exchange rates, e.g., you may generally be served well with
>>> EVT for the major currencies, but might do quite badly with emerging
>>> market currencies. Likewise, in the credit markets, EVT may not do too
>>> well. The assumption of a constant tail index results in misleading VaR
>>> at the extreme tails, especially when there are several regime shifts.
>>> Take a look at the paper "Testing for Multiple Regimes in the Tail
>>> Behavior of Emerging Currency Returns" by B. Candelon and S. Straetmans,
>>> LIFE Working Paper 03-035.
>>> Cheers,
>>> Murali
>>>
>>>
>>> -----Original Message-----
>>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Debashis
>>> Dutta
>>> Sent: 01 July 2009 14:25
>>> To: Wei-han Liu
>>> Cc: R-SIG-Finance at stat.math.ethz.ch
>>> Subject: Re: [R-SIG-Finance] Fw: Value-at-Risk
>>>
>>> Dear Wei-han,
>>>
>>> I believe EVT based VaR would provide a better solution specially in
>>> stressed situation like the present one, modeling the extremal behaviour
>>> in the tail. I used Peaks Over Threshold (POT) based VaR method in my
>>> doctoral dissertation.
>>>
>>> Back testing and comparing the new method to existing ones on real
>>> financial events show that this POT based VaR method provides a rather
>>> realistic model for the extremal behavior of financial processes,
>>> enabling a precise estimation of risk measures. Through the GPD , the
>>> model provides a way of estimating the tail behaviour of the random
>>> variables without knowledge of the true distribution and as such it is a
>>> good candidate for Vale at Risk computation.
>>>
>>> Most common at this moment is the tail-fitting approach based on the
>>> second theorem in extreme value theory (Theorem II Pickands(1975),
>>> Balkema and de Haan(1974)). In general this conforms to the first
>>> theorem in extreme value theory (Theorem I Fisher and Tippett(1928), and
>>> Gnedenko (1943)).The difference between the two theorems is due to the
>>> nature of the data generation.
>>>
>>> ?? For theorem I the data are generated in full range, while in theorem II
>>> data is only generated when it surpasses a certain threshold (POT's
>>> models or Peak Over Threshold). The POT approach has been developed
>>> largely in the insurance business, where only losses (pay outs) above a
>>> certain threshold are accessible to the insurance company.
>>>
>>> ?? Kind Regards,
>>> Debashis
>>>
>
>>>> Thanks a lot, Robert.
>>>>
>>>> I know GARCH models has its forecasting capacity as the reference you
>>>> shared indicates.
>>>>
>>>> I wonder if the Value-at-Risk estimated by extreme value theory can
>>>> also be used for forecasting purpose. Is there some theory background
>>>> in this regard?
>>>>
>>>> Wei-han
>>>>
>>>>
>>>>
>>>> ----- Forwarded Message ----
>>>> From: Robert Iquiapaza <rbali at ufmg.br>
>
>>>> r-sig-finance at stat.math.ethz.ch" <R-SIG-Finance at stat.math.ethz.ch>
>>>> Sent: Wednesday, July 1, 2009 6:37:21 PM
>>>> Subject: Re: [R-SIG-Finance] Value-at-Risk
>>>>
>>>> See for example "Accurate value-at-risk forecasting based on the
>>>> normal-GARCH model" by C Hartz, S Mittnik, M Paolella - Computational
>>>> Statistics and Data Analysis, 2006
>>>>
>>>> best
>>>>
>>>> --------------------------------------------------
>>>>
>>>> Sent: Tuesday, June 30, 2009 12:16 PM
>>>> To: <R-SIG-Finance at stat.math.ethz.ch>
>>>> Subject: [R-SIG-Finance] Value-at-Risk
>>>>
>>>>> Dear R-users:
>>>>>
>>>>> Several questions please on Value-at-Risk.
>>>>>
>>>>> Is Value-at-Risk designed for forecasting purpose?
>>>>>
>>>>> I wonder if Value-at-Risk estimated by in-sample data can be used
>>>>> for
>>>> out-of-sample forecasting?
>>>>> If in-sample Value-at-Risk is estimated by several methods, is it
>>>> appropriate to do the model comparisons based on out--of-sample
>>> performance?
>>>>> Wei-han Liu
>>>>
>>>>
>>>> ?? ?? ?? ?? [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>> ?? ?? ?? ?? [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> ?????? [[alternative HTML version deleted]]
>>
>>
>>
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
>
>
> 	[[alternative HTML version deleted]]
>
>


From briankim19 at yahoo.com  Thu Jul  2 18:33:46 2009
From: briankim19 at yahoo.com (B Kim)
Date: Thu, 2 Jul 2009 09:33:46 -0700 (PDT)
Subject: [R-SIG-Finance] IBrokers and automatic submission of orders?
In-Reply-To: <e8e755250907020017y2ee57a6bo2712b7d2bfcf508c@mail.gmail.com>
References: <b1f16d9d0907011244v5068aac4pe732b95d7e0cdc3e@mail.gmail.com>
	<4A4C0993.5050503@cedrickjohnson.com>
	<e8e755250907020017y2ee57a6bo2712b7d2bfcf508c@mail.gmail.com>
Message-ID: <39090.13741.qm@web62501.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090702/63a556e0/attachment.pl>

From spencer.graves at prodsyse.com  Fri Jul  3 01:31:25 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Thu, 02 Jul 2009 16:31:25 -0700
Subject: [R-SIG-Finance] Help on constrained regression
In-Reply-To: <ad1ead5f0906292018w8158fc4od0e507cff9d25a3@mail.gmail.com>
References: <ad1ead5f0906292018w8158fc4od0e507cff9d25a3@mail.gmail.com>
Message-ID: <4A4D434D.6090402@prodsyse.com>

      Have you considered writing the model in terms of log(a) = g, say: 


           y[t] = exp(g)*y[t-1]+b+epsilon? 


      With this, you could estimate "g" and "b" using "nls".  With 
multiple series, you could use the "nlme" function in the "nmle" 
package.  For the "nlme" package, an excellent reference in Pinheiro and 
Bates (2000) Mixed-Effects Models in S and S-PLUS (Springer). 


      Hope this helps.
      Spencer Graves
    

R_help Help wrote:
> Hi,
>
> I have an AR(1) model
>
> y[t] = ay[t-1]+b+epsilon
>
> I'm trying to force a to be positive. So I did the constrained
> regression with constraints 0 < a < 1. I used pcls in package mgcv.
> However, I found that the solution is not so stable. Most of my lag 1
> autocorrelation is negative. Forcing a to positive value makes the
> optimizer to stick a to the boundary value. All it does is varying b.
> I there anyway to solve this problem? I think the problem might be due
> to my initial value is not a smart choice.
>
> Thank you.
>
> adschai
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From rhelpacc at gmail.com  Fri Jul  3 01:36:50 2009
From: rhelpacc at gmail.com (R_help Help)
Date: Thu, 2 Jul 2009 19:36:50 -0400
Subject: [R-SIG-Finance] Help on constrained regression
In-Reply-To: <4A4D434D.6090402@prodsyse.com>
References: <ad1ead5f0906292018w8158fc4od0e507cff9d25a3@mail.gmail.com>
	<4A4D434D.6090402@prodsyse.com>
Message-ID: <ad1ead5f0907021636ubefcca7q79d1460d12246e5f@mail.gmail.com>

I did. The problem was the underlying process that's negative AR(1).
So I just have to find other way to model it. Thank you.

On Thu, Jul 2, 2009 at 7:31 PM, spencerg<spencer.graves at prodsyse.com> wrote:
> ? ? Have you considered writing the model in terms of log(a) = g, say:
>
> ? ? ? ? ?y[t] = exp(g)*y[t-1]+b+epsilon?
>
> ? ? With this, you could estimate "g" and "b" using "nls". ?With multiple
> series, you could use the "nlme" function in the "nmle" package. ?For the
> "nlme" package, an excellent reference in Pinheiro and Bates (2000)
> Mixed-Effects Models in S and S-PLUS (Springer).
>
> ? ? Hope this helps.
> ? ? Spencer Graves
>
> R_help Help wrote:
>>
>> Hi,
>>
>> I have an AR(1) model
>>
>> y[t] = ay[t-1]+b+epsilon
>>
>> I'm trying to force a to be positive. So I did the constrained
>> regression with constraints 0 < a < 1. I used pcls in package mgcv.
>> However, I found that the solution is not so stable. Most of my lag 1
>> autocorrelation is negative. Forcing a to positive value makes the
>> optimizer to stick a to the boundary value. All it does is varying b.
>> I there anyway to solve this problem? I think the problem might be due
>> to my initial value is not a smart choice.
>>
>> Thank you.
>>
>> adschai
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
>


From spencer.graves at prodsyse.com  Fri Jul  3 02:19:19 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Thu, 02 Jul 2009 17:19:19 -0700
Subject: [R-SIG-Finance] Help on constrained regression
In-Reply-To: <ad1ead5f0907021636ubefcca7q79d1460d12246e5f@mail.gmail.com>
References: <ad1ead5f0906292018w8158fc4od0e507cff9d25a3@mail.gmail.com>	
	<4A4D434D.6090402@prodsyse.com>
	<ad1ead5f0907021636ubefcca7q79d1460d12246e5f@mail.gmail.com>
Message-ID: <4A4D4E87.4080004@prodsyse.com>



           1.  Have you made normal probability plots of the data 
(function "qqnorm")?  This could reveal outliers or a need for a 
transformation. 


           2.  Have you considered using "lme" with "corAR1", allowing 
"a" to be negative?  The examples in the help file for "corAR1" show how 
to estimate this model with a different "b" for each series but with one 
"a" shared between all series.  Then an analysis of residuals should 
help you identify which series seem least consistent with the model. 


           3.  Have you considered dropping one observation in each 
series and using "lme" to estimate a distinct "a" and "b" for each 
series, allowing  "a" to be negative?  If the grand mean of the 
estimates for "a" are positive, this will reduce the number of negative 
"a" values by shrinking them all towards the grand mean.  Then you can 
look more carefully at the series that still have negative "a" values.  
They will either contain outliers, or you will likely identify some 
other series to include in the model so it becomes something more like 
the following: 


           y[t] = a*y[t-1] + b + c*x[t] + e


      As your comment indicated, the seriously negative estimates for 
"a" suggest some substantive violations of the assumptions you used to 
conclude that "a" should be positive. 


      Hope this helps. 
      Spencer


R_help Help wrote:
> I did. The problem was the underlying process that's negative AR(1).
> So I just have to find other way to model it. Thank you.
>
> On Thu, Jul 2, 2009 at 7:31 PM, spencerg<spencer.graves at prodsyse.com> wrote:
>   
>>     Have you considered writing the model in terms of log(a) = g, say:
>>
>>          y[t] = exp(g)*y[t-1]+b+epsilon?
>>
>>     With this, you could estimate "g" and "b" using "nls".  With multiple
>> series, you could use the "nlme" function in the "nmle" package.  For the
>> "nlme" package, an excellent reference in Pinheiro and Bates (2000)
>> Mixed-Effects Models in S and S-PLUS (Springer).
>>
>>     Hope this helps.
>>     Spencer Graves
>>
>> R_help Help wrote:
>>     
>>> Hi,
>>>
>>> I have an AR(1) model
>>>
>>> y[t] = ay[t-1]+b+epsilon
>>>
>>> I'm trying to force a to be positive. So I did the constrained
>>> regression with constraints 0 < a < 1. I used pcls in package mgcv.
>>> However, I found that the solution is not so stable. Most of my lag 1
>>> autocorrelation is negative. Forcing a to positive value makes the
>>> optimizer to stick a to the boundary value. All it does is varying b.
>>> I there anyway to solve this problem? I think the problem might be due
>>> to my initial value is not a smart choice.
>>>
>>> Thank you.
>>>
>>> adschai
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>
>>>       
>>     
>
>


From ron_michael70 at yahoo.com  Fri Jul  3 03:22:23 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Thu, 2 Jul 2009 18:22:23 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Rank
Message-ID: <24316324.post@talk.nabble.com>


Hi, i have a small matrix related question which most of you find trivial
however I am not getting through. Suppose I have a matrix of dimension
(nxm), n < m. Is it in principle possible to have the rank of that matrix
greater than n? Is it possible to have some example?

Thanks,
-- 
View this message in context: http://www.nabble.com/Rank-tp24316324p24316324.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From enricoschumann at yahoo.de  Fri Jul  3 09:22:38 2009
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Fri, 3 Jul 2009 09:22:38 +0200
Subject: [R-SIG-Finance] [R-sig-finance] Rank
In-Reply-To: <24316324.post@talk.nabble.com>
Message-ID: <200907030724.n637NdvH019170@hypatia.math.ethz.ch>

that's not a finance question, but the rank can at most be the min of n and
m.

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
Gesendet: Freitag, 3. Juli 2009 03:22
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] [R-sig-finance] Rank


Hi, i have a small matrix related question which most of you find trivial
however I am not getting through. Suppose I have a matrix of dimension
(nxm), n < m. Is it in principle possible to have the rank of that matrix
greater than n? Is it possible to have some example?

Thanks,
--
View this message in context:
http://www.nabble.com/Rank-tp24316324p24316324.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
Checked by AVG - www.avg.com


18:06:00


From ron_michael70 at yahoo.com  Fri Jul  3 09:50:40 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Fri, 3 Jul 2009 00:50:40 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Rank
In-Reply-To: <200907030724.n637NdvH019170@hypatia.math.ethz.ch>
References: <24316324.post@talk.nabble.com>
	<200907030724.n637NdvH019170@hypatia.math.ethz.ch>
Message-ID: <24319081.post@talk.nabble.com>


This is a finance related question in the sense that I have come accross this
kind of problem in Co-Integration matrix construction in a VECM. I am
explaing how :

Suppose I have 2 endogeneous variables and 3 exogeneous variable all are
I(1) and assumed to have cointegration relationships among them. Let say the
DGP is

y[t] = alpha * t(beta) * (y[t-1] : x[t-1]) + ..................

pi = alpha * t(beta)

Obviously dimension of y vector is 2 and x vector is 3. Therefore there
could be more than 2 cointegrating relationships in that. Hence rank of pi
is in principle more than 2. As number of co-integrating relationships is
estimated on looking at rank of pi matrix. However number of rows there is :
2. I am trying to understand this scenario here. In this case, can usual
VECM estimation procedure work? More important to me is to understand rank
of pi is more than it's row number.

Thanks





Enrico Schumann wrote:
> 
> that's not a finance question, but the rank can at most be the min of n
> and
> m.
> 
> -----Urspr?ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
> Gesendet: Freitag, 3. Juli 2009 03:22
> An: r-sig-finance at stat.math.ethz.ch
> Betreff: [R-SIG-Finance] [R-sig-finance] Rank
> 
> 
> Hi, i have a small matrix related question which most of you find trivial
> however I am not getting through. Suppose I have a matrix of dimension
> (nxm), n < m. Is it in principle possible to have the rank of that matrix
> greater than n? Is it possible to have some example?
> 
> Thanks,
> --
> View this message in context:
> http://www.nabble.com/Rank-tp24316324p24316324.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> Checked by AVG - www.avg.com
> 
> 
> 18:06:00
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Rank-tp24316324p24319081.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From chalabi at phys.ethz.ch  Fri Jul  3 10:52:04 2009
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Fri, 3 Jul 2009 10:52:04 +0200
Subject: [R-SIG-Finance] 3rd R/Rmetrics Workshop 2009 - Presentation Online
Message-ID: <20090703105204.7f2b9748@mimi>

Dear list,

For those who missed the third R/Rmetrics User and Developer Workshop,
you can find the presentations, abstracts and program on

www.rmetrics.org .

It was great to bring together developers, practitioners,
researchers and students to discuss and exchange ideas on how to use R
and Rmetrics in Finance.

We thank all participants and sponsors (ETHZ, Finance Online,
REvolution Computing and Marcel Fehr) who made this special event a
success.

We are looking forward to meeting you at the next edition.

On behalf of the organisation team, 
Yohan Chalabi

-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch


From matthieu.stigler at gmail.com  Fri Jul  3 12:08:16 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Fri, 3 Jul 2009 12:08:16 +0200
Subject: [R-SIG-Finance] [R-sig-finance] Rank
In-Reply-To: <24319081.post@talk.nabble.com>
References: <24316324.post@talk.nabble.com>
	<200907030724.n637NdvH019170@hypatia.math.ethz.ch>
	<24319081.post@talk.nabble.com>
Message-ID: <111060c20907030308y2222ef94u462667c48db29602@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090703/054cbf02/attachment.pl>

From ron_michael70 at yahoo.com  Fri Jul  3 12:57:36 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Fri, 3 Jul 2009 03:57:36 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Rank
In-Reply-To: <111060c20907030308y2222ef94u462667c48db29602@mail.gmail.com>
References: <24316324.post@talk.nabble.com>
	<200907030724.n637NdvH019170@hypatia.math.ethz.ch>
	<24319081.post@talk.nabble.com>
	<111060c20907030308y2222ef94u462667c48db29602@mail.gmail.com>
Message-ID: <24321300.post@talk.nabble.com>


I am not sure why you are saying c.i. relationships can not be more than n.
Quote from Lutkepohl, page : 408 : "Because the error correction term now
involves all the cointegration relations between the endogenous and
unmodelled variables,it is possible that r>K. ", here he defined K as number
of endo. variables in the system................any idea?

However your 1st point is valid, I should have added diff. operator on the
left side, it was a typo.

PS. I understand some ppl here previously suggested not to read Lutkepohl
1st, however except few things I am getting comfortable-reading on that,
atleast easier than Hamilton, perhaps I have only softcopy of Hamilton ;). 


matifou wrote:
> 
> 2009/7/3 RON70 <ron_michael70 at yahoo.com>
> 
>>
>> This is a finance related question in the sense that I have come accross
>> this
>> kind of problem in Co-Integration matrix construction in a VECM. I am
>> explaing how :
>>
>> Suppose I have 2 endogeneous variables and 3 exogeneous variable all are
>> I(1) and assumed to have cointegration relationships among them. Let say
>> the
>> DGP is
>>
> what do you mean by exogenous?
> 
> 
>>
>> y[t] = alpha * t(beta) * (y[t-1] : x[t-1]) + ..................
> 
> left should be differenced
> 
>>
>>
>> pi = alpha * t(beta)
>>
>> Obviously dimension of y vector is 2 and x vector is 3. Therefore there
>> could be more than 2 cointegrating relationships in that.
> 
> if you have more than two cointegrating relationships: I would say x is
> not
> exogeneous
> 
> Hence rank of pi
>> is in principle more than 2. As number of co-integrating relationships is
>> estimated on looking at rank of pi matrix. However number of rows there
>> is
>> :
>> 2.
>>
> I am trying to understand this scenario here. In this case, can usual
>> VECM estimation procedure work? More important to me is to understand
>> rank
>> of pi is more than it's row number.
>>
>> Thanks
>>
>>
>>
>>
>>
>> Enrico Schumann wrote:
>> >
>> > that's not a finance question, but the rank can at most be the min of n
>> > and
>> > m.
>> >
>> > -----Urspr?ngliche Nachricht-----
>> > Von: r-sig-finance-bounces at stat.math.ethz.ch
>> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>> > Gesendet: Freitag, 3. Juli 2009 03:22
>> > An: r-sig-finance at stat.math.ethz.ch
>> > Betreff: [R-SIG-Finance] [R-sig-finance] Rank
>> >
>> >
>> > Hi, i have a small matrix related question which most of you find
>> trivial
>> > however I am not getting through. Suppose I have a matrix of dimension
>> > (nxm), n < m. Is it in principle possible to have the rank of that
>> matrix
>> > greater than n? Is it possible to have some example?
>> >
>> > Thanks,
>> > --
>> > View this message in context:
>> > http://www.nabble.com/Rank-tp24316324p24316324.html
>> > Sent from the Rmetrics mailing list archive at Nabble.com.
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> > Checked by AVG - www.avg.com
>> >
>> >
>> > 18:06:00
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>> >
>>
>> --
>> View this message in context:
>> http://www.nabble.com/Rank-tp24316324p24319081.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 

-- 
View this message in context: http://www.nabble.com/Rank-tp24316324p24321300.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From frainj at tcd.ie  Fri Jul  3 17:54:29 2009
From: frainj at tcd.ie (John Frain)
Date: Fri, 3 Jul 2009 16:54:29 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Rank
In-Reply-To: <24321300.post@talk.nabble.com>
References: <24316324.post@talk.nabble.com>
	<200907030724.n637NdvH019170@hypatia.math.ethz.ch>
	<24319081.post@talk.nabble.com>
	<111060c20907030308y2222ef94u462667c48db29602@mail.gmail.com>
	<24321300.post@talk.nabble.com>
Message-ID: <cfdde1650907030854r4e8a8d45u2dd6fd80e5cc348e@mail.gmail.com>

Your pi matrix is 2 by 5 and therefore must be of rank <= 2 and you
can not have more than two cointegrating relationships betwween the
y's.   Page 408 of my copy of Lutkepohl(2005) deals with Multiplier
analysis and Optimal Control

Best Regards

John

2009/7/3 RON70 <ron_michael70 at yahoo.com>:
>
> I am not sure why you are saying c.i. relationships can not be more than n.
> Quote from Lutkepohl, page : 408 : "Because the error correction term now
> involves all the cointegration relations between the endogenous and
> unmodelled variables,it is possible that r>K. ", here he defined K as number
> of endo. variables in the system................any idea?
>
> However your 1st point is valid, I should have added diff. operator on the
> left side, it was a typo.
>
> PS. I understand some ppl here previously suggested not to read Lutkepohl
> 1st, however except few things I am getting comfortable-reading on that,
> atleast easier than Hamilton, perhaps I have only softcopy of Hamilton ;).
>
>
> matifou wrote:
>>
>> 2009/7/3 RON70 <ron_michael70 at yahoo.com>
>>
>>>
>>> This is a finance related question in the sense that I have come accross
>>> this
>>> kind of problem in Co-Integration matrix construction in a VECM. I am
>>> explaing how :
>>>
>>> Suppose I have 2 endogeneous variables and 3 exogeneous variable all are
>>> I(1) and assumed to have cointegration relationships among them. Let say
>>> the
>>> DGP is
>>>
>> what do you mean by exogenous?
>>
>>
>>>
>>> y[t] = alpha * t(beta) * (y[t-1] : x[t-1]) + ..................
>>
>> left should be differenced
>>
>>>
>>>
>>> pi = alpha * t(beta)
>>>
>>> Obviously dimension of y vector is 2 and x vector is 3. Therefore there
>>> could be more than 2 cointegrating relationships in that.
>>
>> if you have more than two cointegrating relationships: I would say x is
>> not
>> exogeneous
>>
>> Hence rank of pi
>>> is in principle more than 2. As number of co-integrating relationships is
>>> estimated on looking at rank of pi matrix. However number of rows there
>>> is
>>> :
>>> 2.
>>>
>> I am trying to understand this scenario here. In this case, can usual
>>> VECM estimation procedure work? More important to me is to understand
>>> rank
>>> of pi is more than it's row number.
>>>
>>> Thanks
>>>
>>>
>>>
>>>
>>>
>>> Enrico Schumann wrote:
>>> >
>>> > that's not a finance question, but the rank can at most be the min of n
>>> > and
>>> > m.
>>> >
>>> > -----Urspr?ngliche Nachricht-----
>>> > Von: r-sig-finance-bounces at stat.math.ethz.ch
>>> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>> > Gesendet: Freitag, 3. Juli 2009 03:22
>>> > An: r-sig-finance at stat.math.ethz.ch
>>> > Betreff: [R-SIG-Finance] [R-sig-finance] Rank
>>> >
>>> >
>>> > Hi, i have a small matrix related question which most of you find
>>> trivial
>>> > however I am not getting through. Suppose I have a matrix of dimension
>>> > (nxm), n < m. Is it in principle possible to have the rank of that
>>> matrix
>>> > greater than n? Is it possible to have some example?
>>> >
>>> > Thanks,
>>> > --
>>> > View this message in context:
>>> > http://www.nabble.com/Rank-tp24316324p24316324.html
>>> > Sent from the Rmetrics mailing list archive at Nabble.com.
>>> >
>>> > _______________________________________________
>>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> > -- Subscriber-posting only.
>>> > -- If you want to post, subscribe first.
>>> > Checked by AVG - www.avg.com
>>> >
>>> >
>>> > 18:06:00
>>> >
>>> > _______________________________________________
>>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> > -- Subscriber-posting only.
>>> > -- If you want to post, subscribe first.
>>> >
>>> >
>>>
>>> --
>>> View this message in context:
>>> http://www.nabble.com/Rank-tp24316324p24319081.html
>>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> ? ? ? [[alternative HTML version deleted]]
>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> --
> View this message in context: http://www.nabble.com/Rank-tp24316324p24321300.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.



-- 
John C Frain, Ph.D.
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


From comtech.usa at gmail.com  Fri Jul  3 18:07:02 2009
From: comtech.usa at gmail.com (Michael)
Date: Fri, 3 Jul 2009 09:07:02 -0700
Subject: [R-SIG-Finance] IBrokers date/time index?
Message-ID: <b1f16d9d0907030907l193e5abdrde9958caebe7874@mail.gmail.com>

Hi all,

I requested historical data using IBrokers. However, there should be
time in the indices of the time series, not just only the dates.
That's to say, the indices should be of the format: 2009-05-04
HH:MM:SS.

Could anybody tell me how to access those times in the indices?

Thanks!

                SPX.Open SPX.High SPX.Low SPX.Close SPX.Volume SPX.WAP
SPX.hasGaps
2009-05-04   884.56   907.88  882.90    907.88          0       0           0
2009-05-05   905.01   907.92  897.27    903.69          0       0           0


From ron_michael70 at yahoo.com  Fri Jul  3 18:29:14 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Fri, 3 Jul 2009 09:29:14 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Rank
In-Reply-To: <cfdde1650907030854r4e8a8d45u2dd6fd80e5cc348e@mail.gmail.com>
References: <24316324.post@talk.nabble.com>
	<200907030724.n637NdvH019170@hypatia.math.ethz.ch>
	<24319081.post@talk.nabble.com>
	<111060c20907030308y2222ef94u462667c48db29602@mail.gmail.com>
	<24321300.post@talk.nabble.com>
	<cfdde1650907030854r4e8a8d45u2dd6fd80e5cc348e@mail.gmail.com>
Message-ID: <24325699.post@talk.nabble.com>


Oh John, it is page 399, sorry.



John C. Frain wrote:
> 
> Your pi matrix is 2 by 5 and therefore must be of rank <= 2 and you
> can not have more than two cointegrating relationships betwween the
> y's.   Page 408 of my copy of Lutkepohl(2005) deals with Multiplier
> analysis and Optimal Control
> 
> Best Regards
> 
> John
> 
> 2009/7/3 RON70 <ron_michael70 at yahoo.com>:
>>
>> I am not sure why you are saying c.i. relationships can not be more than
>> n.
>> Quote from Lutkepohl, page : 408 : "Because the error correction term now
>> involves all the cointegration relations between the endogenous and
>> unmodelled variables,it is possible that r>K. ", here he defined K as
>> number
>> of endo. variables in the system................any idea?
>>
>> However your 1st point is valid, I should have added diff. operator on
>> the
>> left side, it was a typo.
>>
>> PS. I understand some ppl here previously suggested not to read Lutkepohl
>> 1st, however except few things I am getting comfortable-reading on that,
>> atleast easier than Hamilton, perhaps I have only softcopy of Hamilton
>> ;).
>>
>>
>> matifou wrote:
>>>
>>> 2009/7/3 RON70 <ron_michael70 at yahoo.com>
>>>
>>>>
>>>> This is a finance related question in the sense that I have come
>>>> accross
>>>> this
>>>> kind of problem in Co-Integration matrix construction in a VECM. I am
>>>> explaing how :
>>>>
>>>> Suppose I have 2 endogeneous variables and 3 exogeneous variable all
>>>> are
>>>> I(1) and assumed to have cointegration relationships among them. Let
>>>> say
>>>> the
>>>> DGP is
>>>>
>>> what do you mean by exogenous?
>>>
>>>
>>>>
>>>> y[t] = alpha * t(beta) * (y[t-1] : x[t-1]) + ..................
>>>
>>> left should be differenced
>>>
>>>>
>>>>
>>>> pi = alpha * t(beta)
>>>>
>>>> Obviously dimension of y vector is 2 and x vector is 3. Therefore there
>>>> could be more than 2 cointegrating relationships in that.
>>>
>>> if you have more than two cointegrating relationships: I would say x is
>>> not
>>> exogeneous
>>>
>>> Hence rank of pi
>>>> is in principle more than 2. As number of co-integrating relationships
>>>> is
>>>> estimated on looking at rank of pi matrix. However number of rows there
>>>> is
>>>> :
>>>> 2.
>>>>
>>> I am trying to understand this scenario here. In this case, can usual
>>>> VECM estimation procedure work? More important to me is to understand
>>>> rank
>>>> of pi is more than it's row number.
>>>>
>>>> Thanks
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> Enrico Schumann wrote:
>>>> >
>>>> > that's not a finance question, but the rank can at most be the min of
>>>> n
>>>> > and
>>>> > m.
>>>> >
>>>> > -----Urspr?ngliche Nachricht-----
>>>> > Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>>> > Gesendet: Freitag, 3. Juli 2009 03:22
>>>> > An: r-sig-finance at stat.math.ethz.ch
>>>> > Betreff: [R-SIG-Finance] [R-sig-finance] Rank
>>>> >
>>>> >
>>>> > Hi, i have a small matrix related question which most of you find
>>>> trivial
>>>> > however I am not getting through. Suppose I have a matrix of
>>>> dimension
>>>> > (nxm), n < m. Is it in principle possible to have the rank of that
>>>> matrix
>>>> > greater than n? Is it possible to have some example?
>>>> >
>>>> > Thanks,
>>>> > --
>>>> > View this message in context:
>>>> > http://www.nabble.com/Rank-tp24316324p24316324.html
>>>> > Sent from the Rmetrics mailing list archive at Nabble.com.
>>>> >
>>>> > _______________________________________________
>>>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> > -- Subscriber-posting only.
>>>> > -- If you want to post, subscribe first.
>>>> > Checked by AVG - www.avg.com
>>>> >
>>>> >
>>>> > 18:06:00
>>>> >
>>>> > _______________________________________________
>>>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> > -- Subscriber-posting only.
>>>> > -- If you want to post, subscribe first.
>>>> >
>>>> >
>>>>
>>>> --
>>>> View this message in context:
>>>> http://www.nabble.com/Rank-tp24316324p24319081.html
>>>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>> ? ? ? [[alternative HTML version deleted]]
>>>
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> --
>> View this message in context:
>> http://www.nabble.com/Rank-tp24316324p24321300.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
> 
> 
> 
> -- 
> John C Frain, Ph.D.
> Trinity College Dublin
> Dublin 2
> Ireland
> www.tcd.ie/Economics/staff/frainj/home.htm
> mailto:frainj at tcd.ie
> mailto:frainj at gmail.com
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 

-- 
View this message in context: http://www.nabble.com/Rank-tp24316324p24325699.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ezivot at u.washington.edu  Fri Jul  3 19:20:09 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Fri, 3 Jul 2009 10:20:09 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Rank
In-Reply-To: <24325699.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.43.0907031020090.20426@hymn32.u.washington.edu>

I suggest that you look at Johansen's book on cointegration

http://www.amazon.com/Likelihood-Based-Inference-Cointegrated-Autoregressive-Econometrics/dp/0198774508/ref=sr_1_1?ie=UTF8&s=books&qid=1246640896&sr=8-1

His treatment is the most complete and will answer all of your questions. 
A nice empirical/practical follow up to this book is the recent one by K. Jusalius (his wife)

http://www.amazon.com/Cointegrated-VAR-Model-Applications-Econometrics/dp/0199285675/ref=pd_sim_b_2

The issue here is because the Pi matrix has rank 2 there are only two cointegrating relationships among the Y's of the form beta1'(Y1, Y2, X1, X2, X3) and beta2'(Y1, Y2, X1, X2, X3) where the coefficients on the X's are not all zero.

The X variables are unmodeled - which in the cointegration literature means that they are weakly exogenous wrt to the cointegration parameters in the VECM. If there is now feedback from the Ys to the Xs then the reduced form relationship for DX(t) does not involve Y and the Xs are then strongly exogenous. In particular, the error correction coefficients on these variables (alphas) are zero so that the system has the form like


DY1(t) = a1*beta1'(Y1, Y2, X1, X2, X3) + lags of DY(t) and DX(t) + e1(t)
DY2(t) = a2*beta2'(Y1, Y2, X1, X2, X3) + lags of DY(t) and DX(t) + e1(t)
DX(t) = lags of DX(t) + e3(t)

Notice in this type of representation there is no cointegration among the Xs because the reduced form for the Xs is not a VECM. 
Now because the X's are unmodeled, there is the possibility that there are cointegration relationships among the X's that do not involve the Ys. I think this is causing the confusion. In general, it is assumed that such relationships do not exist when the VECM is specified with unmodeled variables.

None of this discussion has to do with finance



On Fri, 3 Jul 2009, RON70 wrote:

> 
Oh John, it is page 399, sorry.



John C. Frain wrote:
> 
> Your pi matrix is 2 by 5 and therefore must be of rank <= 2 and you
> can not have more than two cointegrating relationships betwween the
> y's.   Page 408 of my copy of Lutkepohl(2005) deals with Multiplier
> analysis and Optimal Control
> 
> Best Regards
> 
> John
> 
> 2009/7/3 RON70 <ron_michael70 at yahoo.com>:
>>
>> I am not sure why you are saying c.i. relationships can not be more than
>> n.
>> Quote from Lutkepohl, page : 408 : "Because the error correction term now
>> involves all the cointegration relations between the endogenous and
>> unmodelled variables,it is possible that r>K. ", here he defined K as
>> number
>> of endo. variables in the system................any idea?
>>
>> However your 1st point is valid, I should have added diff. operator on
>> the
>> left side, it was a typo.
>>
>> PS. I understand some ppl here previously suggested not to read Lutkepohl
>> 1st, however except few things I am getting comfortable-reading on that,
>> atleast easier than Hamilton, perhaps I have only softcopy of Hamilton
>> ;).
>>
>>
>> matifou wrote:
>>>
>>> 2009/7/3 RON70 <ron_michael70 at yahoo.com>
>>>
>>>>
>>>> This is a finance related question in the sense that I have come
>>>> accross
>>>> this
>>>> kind of problem in Co-Integration matrix construction in a VECM. I am
>>>> explaing how :
>>>>
>>>> Suppose I have 2 endogeneous variables and 3 exogeneous variable all
>>>> are
>>>> I(1) and assumed to have cointegration relationships among them. Let
>>>> say
>>>> the
>>>> DGP is
>>>>
>>> what do you mean by exogenous?
>>>
>>>
>>>>
>>>> y[t] = alpha * t(beta) * (y[t-1] : x[t-1]) + ..................
>>>
>>> left should be differenced
>>>
>>>>
>>>>
>>>> pi = alpha * t(beta)
>>>>
>>>> Obviously dimension of y vector is 2 and x vector is 3. Therefore there
>>>> could be more than 2 cointegrating relationships in that.
>>>
>>> if you have more than two cointegrating relationships: I would say x is
>>> not
>>> exogeneous
>>>
>>> Hence rank of pi
>>>> is in principle more than 2. As number of co-integrating relationships
>>>> is
>>>> estimated on looking at rank of pi matrix. However number of rows there
>>>> is
>>>> :
>>>> 2.
>>>>
>>> I am trying to understand this scenario here. In this case, can usual
>>>> VECM estimation procedure work? More important to me is to understand
>>>> rank
>>>> of pi is more than it's row number.
>>>>
>>>> Thanks
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> Enrico Schumann wrote:
>>>> >
>>>> > that's not a finance question, but the rank can at most be the min of
>>>> n
>>>> > and
>>>> > m.
>>>> >
>>>> > -----Urspr?ngliche Nachricht-----
>>>> > Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>>> > Gesendet: Freitag, 3. Juli 2009 03:22
>>>> > An: r-sig-finance at stat.math.ethz.ch
>>>> > Betreff: [R-SIG-Finance] [R-sig-finance] Rank
>>>> >
>>>> >
>>>> > Hi, i have a small matrix related question which most of you find
>>>> trivial
>>>> > however I am not getting through. Suppose I have a matrix of
>>>> dimension
>>>> > (nxm), n < m. Is it in principle possible to have the rank of that
>>>> matrix
>>>> > greater than n? Is it possible to have some example?
>>>> >
>>>> > Thanks,
>>>> > --
>>>> > View this message in context:
>>>> > http://www.nabble.com/Rank-tp24316324p24316324.html
>>>> > Sent from the Rmetrics mailing list archive at Nabble.com.
>>>> >
>>>> > _______________________________________________
>>>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> > -- Subscriber-posting only.
>>>> > -- If you want to post, subscribe first.
>>>> > Checked by AVG - www.avg.com
>>>> >
>>>> >
>>>> > 18:06:00
>>>> >
>>>> > _______________________________________________
>>>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> > -- Subscriber-posting only.
>>>> > -- If you want to post, subscribe first.
>>>> >
>>>> >
>>>>
>>>> --
>>>> View this message in context:
>>>> http://www.nabble.com/Rank-tp24316324p24319081.html
>>>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>> ? ? ? [[alternative HTML version deleted]]
>>>
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> --
>> View this message in context:
>> http://www.nabble.com/Rank-tp24316324p24321300.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
> 
> 
> 
> -- 
> John C Frain, Ph.D.
> Trinity College Dublin
> Dublin 2
> Ireland
> www.tcd.ie/Economics/staff/frainj/home.htm
> mailto:frainj at tcd.ie
> mailto:frainj at gmail.com
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

-- 
View this message in context: http://www.nabble.com/Rank-tp24316324p24325699.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Fri Jul  3 20:25:35 2009
From: jeff.a.ryan at gmail.com (J Ryan)
Date: Fri, 3 Jul 2009 13:25:35 -0500
Subject: [R-SIG-Finance] IBrokers date/time index?
In-Reply-To: <b1f16d9d0907030907l193e5abdrde9958caebe7874@mail.gmail.com>
References: <b1f16d9d0907030907l193e5abdrde9958caebe7874@mail.gmail.com>
Message-ID: <17AEC7DA-FC6F-4928-B632-DF3536EC7F93@gmail.com>

Daily data (as in your example) doesn't include seconds that matter.

You can of course do anything you like by simply reading the source  
and writing your own code.

HTH
Jeff


Jeffrey A. Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com

On Jul 3, 2009, at 11:07 AM, Michael <comtech.usa at gmail.com> wrote:

> Hi all,
>
> I requested historical data using IBrokers. However, there should be
> time in the indices of the time series, not just only the dates.
> That's to say, the indices should be of the format: 2009-05-04
> HH:MM:SS.
>
> Could anybody tell me how to access those times in the indices?
>
> Thanks!
>
>                SPX.Open SPX.High SPX.Low SPX.Close SPX.Volume SPX.WAP
> SPX.hasGaps
> 2009-05-04   884.56   907.88  882.90    907.88          0        
> 0           0
> 2009-05-05   905.01   907.92  897.27    903.69          0        
> 0           0
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From sklai3 at hotmail.com  Fri Jul  3 21:43:26 2009
From: sklai3 at hotmail.com (sk lai)
Date: Fri, 3 Jul 2009 19:43:26 +0000
Subject: [R-SIG-Finance] Does any one know how to program
 difference-in-difference in panel data with R-program ?
Message-ID: <BAY102-W36F626DC7163EA2665F561802C0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090703/3fcb1f59/attachment.pl>

From frainj at tcd.ie  Fri Jul  3 22:29:44 2009
From: frainj at tcd.ie (John Frain)
Date: Fri, 3 Jul 2009 21:29:44 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Rank
In-Reply-To: <Pine.LNX.4.43.0907031020090.20426@hymn32.u.washington.edu>
References: <24325699.post@talk.nabble.com>
	<Pine.LNX.4.43.0907031020090.20426@hymn32.u.washington.edu>
Message-ID: <cfdde1650907031329q6144e58bsa420d840ab950e3d@mail.gmail.com>

I agree with what is said below.  Even though the rank of pi is 2 it
can be factorised into a product of two matrices alpha which is 2 by 3
and a beta which is 3 by 5.  In macroeconometrics, in practice,  such
a scheme has been used to incorporate a disequilibrium in one sector
of a macro model (modelling the X,s) into another sector (modelling
the Y's).  We did necessarily even use the first differences of the
Y's inmodelling the X's.

I agree that this is probably a matter of time series macoeconometrics
rather than finance and I should probably leave it at that,

Best Regards

John

2009/7/3 Eric Zivot <ezivot at u.washington.edu>:
> I suggest that you look at Johansen's book on cointegration
>
> http://www.amazon.com/Likelihood-Based-Inference-Cointegrated-Autoregressive-Econometrics/dp/0198774508/ref=sr_1_1?ie=UTF8&s=books&qid=1246640896&sr=8-1
>
> His treatment is the most complete and will answer all of your questions. A
> nice empirical/practical follow up to this book is the recent one by K.
> Jusalius (his wife)
>
> http://www.amazon.com/Cointegrated-VAR-Model-Applications-Econometrics/dp/0199285675/ref=pd_sim_b_ot nec2
>
> The issue here is because the Pi matrix has rank 2 there are only two
> cointegrating relationships among the Y's of the form beta1'(Y1, Y2, X1, X2,
> X3) and beta2'(Y1, Y2, X1, X 2, X3) where the coefficients on the X's are not
> all zero.
>
> The X variables are unmodeled - which in the cointegration literature means
> that they are weakly exogenous wrt to the cointegration parameters in the
> VECM. If there is now feedback from the Ys to the Xs then the reduced form
> relationship for DX(t) does not involve Y and the Xs are then strongly
> exogenous. In particular, the error correction coefficients on these
> variables (alphas) are zero so that the system has the form like
>
>
> DY1(t) = a1*beta1'(Y1, Y2, X1, X2, X3) + lags of DY(t) and DX(t) + e1(t)
> DY2(t) = a2*beta2'(Y1, Y2, X1, X2, X3) + lags of DY(t) and DX(t) + e1(t)
> DX(t) = lags of DX(t) + e3(t)
>
> Notice in this type of representation there is no cointegration among the Xs
> because the reduced form for the Xs is not a VECM. Now because the X's are
> unmodeled, there is the possibility that there are cointegration
> relationships among the X's that do not involve the Ys. I think this is
> causing the confusion. In general, it is assumed that such relationships do
> not exist when the VECM is specified with unmodeled variables.
>
> None of this discussion has to do with finance
>
>
>
> On Fri, 3 Jul 2009, RON70 wrote:
>
>>
> Oh John, it is page 399, sorry.
>
>
>
> John C. Frain wrote:
>>
>> Your pi matrix is 2 by 5 and therefore must be of rank <= 2 and you
>> can not have more than two cointegrating relationships betwween the
>> y's. ? Page 408 of my copy of Lutkepohl(2005) deals with Multiplier
>> analysis and Optimal Control
>>
>> Best Regards
>>
>> John
>>
>> 2009/7/3 RON70 <ron_michael70 at yahoo.com>:
>>>
>>> I am not sure why you are saying c.i. relationships can not be more than
>>> n.
>>> Quote from Lutkepohl, page : 408 : "Because the error correction term now
>>> involves all the cointegration relations between the endogenous and
>>> unmodelled variables,it is possible that r>K. ", here he defined K as
>>> number
>>> of endo. variables in the system................any idea?
>>>
>>> However your 1st point is valid, I should have added diff. operator on
>>> the
>>> left side, it was a typo.
>>>
>>> PS. I understand some ppl here previously suggested not to read Lutkepohl
>>> 1st, however except few things I am getting comfortable-reading on that,
>>> atleast easier than Hamilton, perhaps I have only softcopy of Hamilton
>>> ;).
>>>
>>>
>>> matifou wrote:
>>>>
>>>> 2009/7/3 RON70 <ron_michael70 at yahoo.com>
>>>>
>>>>>
>>>>> This is a finance related question in the sense that I have come
>>>>> accross
>>>>> this
>>>>> kind of problem in Co-Integration matrix construction in a VECM. I am
>>>>> explaing how :
>>>>>
>>>>> Suppose I have 2 endogeneous variables and 3 exogeneous variable all
>>>>> are
>>>>> I(1) and assumed to have cointegration relationships among them. Let
>>>>> say
>>>>> the
>>>>> DGP is
>>>>>
>>>> what do you mean by exogenous?
>>>>
>>>>
>>>>>
>>>>> y[t] = alpha * t(beta) * (y[t-1] : x[t-1]) + ..................
>>>>
>>>> left should be differenced
>>>>
>>>>>
>>>>>
>>>>> pi = alpha * t(beta)
>>>>>
>>>>> Obviously dimension of y vector is 2 and x vector is 3. Therefore there
>>>>> could be more than 2 cointegrating relationships in that.
>>>>
>>>> if you have more than two cointegrating relationships: I would say x is
>>>> not
>>>> exogeneous
>>>>
>>>> Hence rank of pi
>>>>>
>>>>> is in principle more than 2. As number of co-integrating relationships
>>>>> is
>>>>> estimated on looking at rank of pi matrix. However number of rows there
>>>>> is
>>>>> :
>>>>> 2.
>>>>>
>>>> I am trying to understand this scenario here. In this case, can usual
>>>>>
>>>>> VECM estimation procedure work? More important to me is to understand
>>>>> rank
>>>>> of pi is more than it's row number.
>>>>>
>>>>> Thanks
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> Enrico Schumann wrote:
>>>>> >
>>>>> > that's not a finance question, but the rank can at most be the min of
>>>>> n
>>>>> > and
>>>>> > m.
>>>>> >
>>>>> > -----Urspr?ngliche Nachricht-----
>>>>> > Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>>> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von RON70
>>>>> > Gesendet: Freitag, 3. Juli 2009 03:22
>>>>> > An: r-sig-finance at stat.math.ethz.ch
>>>>> > Betreff: [R-SIG-Finance] [R-sig-finance] Rank
>>>>> >
>>>>> >
>>>>> > Hi, i have a small matrix related question which most of you find
>>>>> trivial
>>>>> > however I am not getting through. Suppose I have a matrix of
>>>>> dimension
>>>>> > (nxm), n < m. Is it in principle possible to have the rank of that
>>>>> matrix
>>>>> > greater than n? Is it possible to have some example?
>>>>> >
>>>>> > Thanks,
>>>>> > --
>>>>> > View this message in context:
>>>>> > http://www.nabble.com/Rank-tp24316324p24316324.html
>>>>> > Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>> >
>>>>> > _______________________________________________
>>>>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> > -- Subscriber-posting only.
>>>>> > -- If you want to post, subscribe first.
>>>>> > Checked by AVG - www.avg.com
>>>>> >
>>>>> >
>>>>> > 18:06:00
>>>>> >
>>>>> > _______________________________________________
>>>>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> > -- Subscriber-posting only.
>>>>> > -- If you want to post, subscribe first.
>>>>> >
>>>>> >
>>>>>
>>>>> --
>>>>> View this message in context:
>>>>> http://www.nabble.com/Rank-tp24316324p24319081.html
>>>>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>
>>>> ? ? ? [[alternative HTML version deleted]]
>>>>
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>> --
>>> View this message in context:
>>> http://www.nabble.com/Rank-tp24316324p24321300.html
>>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>
>>
>>
>> --
>> John C Frain, Ph.D.
>> Trinity College Dublin
>> Dublin 2
>> Ireland
>> www.tcd.ie/Economics/staff/frainj/home.htm
>> mailto:frainj at tcd.ie
>> mailto:frainj at gmail.com
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> --
> View this message in context:
> http://www.nabble.com/Rank-tp24316324p24325699.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.



-- 
John C Frain, Ph.D.
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


From comtech.usa at gmail.com  Sat Jul  4 02:01:18 2009
From: comtech.usa at gmail.com (Michael)
Date: Fri, 3 Jul 2009 17:01:18 -0700
Subject: [R-SIG-Finance] insert element in IBrokers/XTS time series?
Message-ID: <b1f16d9d0907031701p766e9cb2s7a9f61acc205b606@mail.gmail.com>

Hi all,

Let's say I have the following timeseres in IBrokers and XTS:

2009-06-25      18038      18350     18011       18327         5396     18183
2009-06-26      18420      18708     18331       18515         7367     18499
2009-06-29      18546      18688     18423       18577        28728     18555
2009-07-03      17951      18198     17866       18149        38019     18059

I want to add a few more rows, indexed by

"2009-06-30", "2009-07-01", "2009-07-02", and using NA's,

I did the following:

myhist["2009-07-01",]=c(NA, NA, NA, NA, NA, NA, NA, NA)

But it didn't really work.

What's the good way to insert missing points in time series?

Thanks!


From comtech.usa at gmail.com  Sat Jul  4 02:33:18 2009
From: comtech.usa at gmail.com (Michael)
Date: Fri, 3 Jul 2009 17:33:18 -0700
Subject: [R-SIG-Finance] insert element in IBrokers/XTS time series?
In-Reply-To: <b1f16d9d0907031701p766e9cb2s7a9f61acc205b606@mail.gmail.com>
References: <b1f16d9d0907031701p766e9cb2s7a9f61acc205b606@mail.gmail.com>
Message-ID: <b1f16d9d0907031733j7a7c6809t5c460b86d5a83505@mail.gmail.com>

And how to remove one row via index?

Thanks!

On Fri, Jul 3, 2009 at 5:01 PM, Michael<comtech.usa at gmail.com> wrote:
> Hi all,
>
> Let's say I have the following timeseres in IBrokers and XTS:
>
> 2009-06-25 ? ? ?18038 ? ? ?18350 ? ? 18011 ? ? ? 18327 ? ? ? ? 5396 ? ? 18183
> 2009-06-26 ? ? ?18420 ? ? ?18708 ? ? 18331 ? ? ? 18515 ? ? ? ? 7367 ? ? 18499
> 2009-06-29 ? ? ?18546 ? ? ?18688 ? ? 18423 ? ? ? 18577 ? ? ? ?28728 ? ? 18555
> 2009-07-03 ? ? ?17951 ? ? ?18198 ? ? 17866 ? ? ? 18149 ? ? ? ?38019 ? ? 18059
>
> I want to add a few more rows, indexed by
>
> "2009-06-30", "2009-07-01", "2009-07-02", and using NA's,
>
> I did the following:
>
> myhist["2009-07-01",]=c(NA, NA, NA, NA, NA, NA, NA, NA)
>
> But it didn't really work.
>
> What's the good way to insert missing points in time series?
>
> Thanks!
>


From seancarmody at gmail.com  Sat Jul  4 11:26:10 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Sat, 4 Jul 2009 19:26:10 +1000
Subject: [R-SIG-Finance] insert element in IBrokers/XTS time series?
In-Reply-To: <b1f16d9d0907031701p766e9cb2s7a9f61acc205b606@mail.gmail.com>
References: <b1f16d9d0907031701p766e9cb2s7a9f61acc205b606@mail.gmail.com>
Message-ID: <ce6bbb9d0907040226n1559dc51w97b0a88b3e9af0b3@mail.gmail.com>

Eliminating a row is straightforward, e.g.

myhist[time(myhist) != as.Date("2009-06-29"),]

will return everything except the entry for 2009-06-29.

Adding a row is, I believe, a bit more fiddly. You can use

rbind(myhist, xts(data.frame(rep(NA,6), order.by=as.Date("2009-08-01")))

although this will give a warning message about column names
differing. If you have an additional xts with the same column names,
you can simply use merge()

Sean.

On Sat, Jul 4, 2009 at 10:01 AM, Michael<comtech.usa at gmail.com> wrote:
> Hi all,
>
> Let's say I have the following timeseres in IBrokers and XTS:
>
> 2009-06-25 ? ? ?18038 ? ? ?18350 ? ? 18011 ? ? ? 18327 ? ? ? ? 5396 ? ? 18183
> 2009-06-26 ? ? ?18420 ? ? ?18708 ? ? 18331 ? ? ? 18515 ? ? ? ? 7367 ? ? 18499
> 2009-06-29 ? ? ?18546 ? ? ?18688 ? ? 18423 ? ? ? 18577 ? ? ? ?28728 ? ? 18555
> 2009-07-03 ? ? ?17951 ? ? ?18198 ? ? 17866 ? ? ? 18149 ? ? ? ?38019 ? ? 18059
>
> I want to add a few more rows, indexed by
>
> "2009-06-30", "2009-07-01", "2009-07-02", and using NA's,
>
> I did the following:
>
> myhist["2009-07-01",]=c(NA, NA, NA, NA, NA, NA, NA, NA)
>
> But it didn't really work.
>
> What's the good way to insert missing points in time series?
>
> Thanks!
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Sean Carmody

The Stubborn Mule
http://www.stubbornmule.net
http://twitter.com/seancarmody


From comtech.usa at gmail.com  Sat Jul  4 18:47:04 2009
From: comtech.usa at gmail.com (Michael)
Date: Sat, 4 Jul 2009 09:47:04 -0700
Subject: [R-SIG-Finance] insert element in IBrokers/XTS time series?
In-Reply-To: <ce6bbb9d0907040226n1559dc51w97b0a88b3e9af0b3@mail.gmail.com>
References: <b1f16d9d0907031701p766e9cb2s7a9f61acc205b606@mail.gmail.com>
	<ce6bbb9d0907040226n1559dc51w97b0a88b3e9af0b3@mail.gmail.com>
Message-ID: <b1f16d9d0907040947y57fb8119j6e8d6aa598f81dac@mail.gmail.com>

I got an error:

Error in xts(data.frame(rep(NA, 6), order.by = as.Date("2009-08-01"))) :
  order.by requires an appropriate time-based object

On Sat, Jul 4, 2009 at 2:26 AM, Sean Carmody<seancarmody at gmail.com> wrote:
> Eliminating a row is straightforward, e.g.
>
> myhist[time(myhist) != as.Date("2009-06-29"),]
>
> will return everything except the entry for 2009-06-29.
>
> Adding a row is, I believe, a bit more fiddly. You can use
>
> rbind(myhist, xts(data.frame(rep(NA,6), order.by=as.Date("2009-08-01")))
>
> although this will give a warning message about column names
> differing. If you have an additional xts with the same column names,
> you can simply use merge()
>
> Sean.
>
> On Sat, Jul 4, 2009 at 10:01 AM, Michael<comtech.usa at gmail.com> wrote:
>> Hi all,
>>
>> Let's say I have the following timeseres in IBrokers and XTS:
>>
>> 2009-06-25 ? ? ?18038 ? ? ?18350 ? ? 18011 ? ? ? 18327 ? ? ? ? 5396 ? ? 18183
>> 2009-06-26 ? ? ?18420 ? ? ?18708 ? ? 18331 ? ? ? 18515 ? ? ? ? 7367 ? ? 18499
>> 2009-06-29 ? ? ?18546 ? ? ?18688 ? ? 18423 ? ? ? 18577 ? ? ? ?28728 ? ? 18555
>> 2009-07-03 ? ? ?17951 ? ? ?18198 ? ? 17866 ? ? ? 18149 ? ? ? ?38019 ? ? 18059
>>
>> I want to add a few more rows, indexed by
>>
>> "2009-06-30", "2009-07-01", "2009-07-02", and using NA's,
>>
>> I did the following:
>>
>> myhist["2009-07-01",]=c(NA, NA, NA, NA, NA, NA, NA, NA)
>>
>> But it didn't really work.
>>
>> What's the good way to insert missing points in time series?
>>
>> Thanks!
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Sean Carmody
>
> The Stubborn Mule
> http://www.stubbornmule.net
> http://twitter.com/seancarmody
>


From comtech.usa at gmail.com  Mon Jul  6 07:09:21 2009
From: comtech.usa at gmail.com (Michael)
Date: Sun, 5 Jul 2009 22:09:21 -0700
Subject: [R-SIG-Finance] insert element in IBrokers/XTS time series?
In-Reply-To: <ce6bbb9d0907041410n2cf4c1adjf90ffa82a560a39a@mail.gmail.com>
References: <b1f16d9d0907031701p766e9cb2s7a9f61acc205b606@mail.gmail.com>
	<ce6bbb9d0907040226n1559dc51w97b0a88b3e9af0b3@mail.gmail.com>
	<b1f16d9d0907040947y57fb8119j6e8d6aa598f81dac@mail.gmail.com>
	<ce6bbb9d0907041410n2cf4c1adjf90ffa82a560a39a@mail.gmail.com>
Message-ID: <b1f16d9d0907052209l458dd768h49e6aedf668fe376@mail.gmail.com>

still not working...

On Sat, Jul 4, 2009 at 2:10 PM, Sean Carmody<seancarmody at gmail.com> wrote:
> Sorry, try this
>
> ?rbind(myhist, xts(data.frame(NA,NA,NA,NA,NA,NA),
> order.by=as.Date("2009-08-01")))
>
> also, check class(time(myhist)) and if it is not "Date" you may need
> to replace as.Date with the appropriate constructor for your time
> class.
>
> Sean.
>
> On Sun, Jul 5, 2009 at 2:47 AM, Michael<comtech.usa at gmail.com> wrote:
>> I got an error:
>>
>> Error in xts(data.frame(rep(NA, 6), order.by = as.Date("2009-08-01"))) :
>> ?order.by requires an appropriate time-based object
>>
>> On Sat, Jul 4, 2009 at 2:26 AM, Sean Carmody<seancarmody at gmail.com> wrote:
>>> Eliminating a row is straightforward, e.g.
>>>
>>> myhist[time(myhist) != as.Date("2009-06-29"),]
>>>
>>> will return everything except the entry for 2009-06-29.
>>>
>>> Adding a row is, I believe, a bit more fiddly. You can use
>>>
>>> rbind(myhist, xts(data.frame(rep(NA,6), order.by=as.Date("2009-08-01")))
>>>
>>> although this will give a warning message about column names
>>> differing. If you have an additional xts with the same column names,
>>> you can simply use merge()
>>>
>>> Sean.
>>>
>>> On Sat, Jul 4, 2009 at 10:01 AM, Michael<comtech.usa at gmail.com> wrote:
>>>> Hi all,
>>>>
>>>> Let's say I have the following timeseres in IBrokers and XTS:
>>>>
>>>> 2009-06-25 ? ? ?18038 ? ? ?18350 ? ? 18011 ? ? ? 18327 ? ? ? ? 5396 ? ? 18183
>>>> 2009-06-26 ? ? ?18420 ? ? ?18708 ? ? 18331 ? ? ? 18515 ? ? ? ? 7367 ? ? 18499
>>>> 2009-06-29 ? ? ?18546 ? ? ?18688 ? ? 18423 ? ? ? 18577 ? ? ? ?28728 ? ? 18555
>>>> 2009-07-03 ? ? ?17951 ? ? ?18198 ? ? 17866 ? ? ? 18149 ? ? ? ?38019 ? ? 18059
>>>>
>>>> I want to add a few more rows, indexed by
>>>>
>>>> "2009-06-30", "2009-07-01", "2009-07-02", and using NA's,
>>>>
>>>> I did the following:
>>>>
>>>> myhist["2009-07-01",]=c(NA, NA, NA, NA, NA, NA, NA, NA)
>>>>
>>>> But it didn't really work.
>>>>
>>>> What's the good way to insert missing points in time series?
>>>>
>>>> Thanks!
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>>
>>>
>>> --
>>> Sean Carmody
>>>
>>> The Stubborn Mule
>>> http://www.stubbornmule.net
>>> http://twitter.com/seancarmody
>>>
>>
>
>
>
> --
> Sean Carmody
>
> The Stubborn Mule
> http://www.stubbornmule.net
> http://twitter.com/seancarmody
>


From seancarmody at gmail.com  Mon Jul  6 07:45:24 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Mon, 6 Jul 2009 15:45:24 +1000
Subject: [R-SIG-Finance] insert element in IBrokers/XTS time series?
In-Reply-To: <b1f16d9d0907052209l458dd768h49e6aedf668fe376@mail.gmail.com>
References: <b1f16d9d0907031701p766e9cb2s7a9f61acc205b606@mail.gmail.com>
	<ce6bbb9d0907040226n1559dc51w97b0a88b3e9af0b3@mail.gmail.com>
	<b1f16d9d0907040947y57fb8119j6e8d6aa598f81dac@mail.gmail.com>
	<ce6bbb9d0907041410n2cf4c1adjf90ffa82a560a39a@mail.gmail.com>
	<b1f16d9d0907052209l458dd768h49e6aedf668fe376@mail.gmail.com>
Message-ID: <ce6bbb9d0907052245l6dba897bj96267b29da267888@mail.gmail.com>

It works for me (albeit with the warning about column names I
mentioned)! What do you get from

class(time(myhist))?

Sean.

On Mon, Jul 6, 2009 at 3:09 PM, Michael<comtech.usa at gmail.com> wrote:
> still not working...
>
> On Sat, Jul 4, 2009 at 2:10 PM, Sean Carmody<seancarmody at gmail.com> wrote:
>> Sorry, try this
>>
>> ?rbind(myhist, xts(data.frame(NA,NA,NA,NA,NA,NA),
>> order.by=as.Date("2009-08-01")))
>>
>> also, check class(time(myhist)) and if it is not "Date" you may need
>> to replace as.Date with the appropriate constructor for your time
>> class.
>>
>> Sean.
>>
>> On Sun, Jul 5, 2009 at 2:47 AM, Michael<comtech.usa at gmail.com> wrote:
>>> I got an error:
>>>
>>> Error in xts(data.frame(rep(NA, 6), order.by = as.Date("2009-08-01"))) :
>>> ?order.by requires an appropriate time-based object
>>>
>>> On Sat, Jul 4, 2009 at 2:26 AM, Sean Carmody<seancarmody at gmail.com> wrote:
>>>> Eliminating a row is straightforward, e.g.
>>>>
>>>> myhist[time(myhist) != as.Date("2009-06-29"),]
>>>>
>>>> will return everything except the entry for 2009-06-29.
>>>>
>>>> Adding a row is, I believe, a bit more fiddly. You can use
>>>>
>>>> rbind(myhist, xts(data.frame(rep(NA,6), order.by=as.Date("2009-08-01")))
>>>>
>>>> although this will give a warning message about column names
>>>> differing. If you have an additional xts with the same column names,
>>>> you can simply use merge()
>>>>
>>>> Sean.
>>>>
>>>> On Sat, Jul 4, 2009 at 10:01 AM, Michael<comtech.usa at gmail.com> wrote:
>>>>> Hi all,
>>>>>
>>>>> Let's say I have the following timeseres in IBrokers and XTS:
>>>>>
>>>>> 2009-06-25 ? ? ?18038 ? ? ?18350 ? ? 18011 ? ? ? 18327 ? ? ? ? 5396 ? ? 18183
>>>>> 2009-06-26 ? ? ?18420 ? ? ?18708 ? ? 18331 ? ? ? 18515 ? ? ? ? 7367 ? ? 18499
>>>>> 2009-06-29 ? ? ?18546 ? ? ?18688 ? ? 18423 ? ? ? 18577 ? ? ? ?28728 ? ? 18555
>>>>> 2009-07-03 ? ? ?17951 ? ? ?18198 ? ? 17866 ? ? ? 18149 ? ? ? ?38019 ? ? 18059
>>>>>
>>>>> I want to add a few more rows, indexed by
>>>>>
>>>>> "2009-06-30", "2009-07-01", "2009-07-02", and using NA's,
>>>>>
>>>>> I did the following:
>>>>>
>>>>> myhist["2009-07-01",]=c(NA, NA, NA, NA, NA, NA, NA, NA)
>>>>>
>>>>> But it didn't really work.
>>>>>
>>>>> What's the good way to insert missing points in time series?
>>>>>
>>>>> Thanks!
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Sean Carmody
>>>>
>>>> The Stubborn Mule
>>>> http://www.stubbornmule.net
>>>> http://twitter.com/seancarmody
>>>>
>>>
>>
>>
>>
>> --
>> Sean Carmody
>>
>> The Stubborn Mule
>> http://www.stubbornmule.net
>> http://twitter.com/seancarmody
>>
>



-- 
Sean Carmody

The Stubborn Mule
http://www.stubbornmule.net
http://twitter.com/seancarmody


From seancarmody at gmail.com  Mon Jul  6 07:52:01 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Mon, 6 Jul 2009 15:52:01 +1000
Subject: [R-SIG-Finance] insert element in IBrokers/XTS time series?
In-Reply-To: <b1f16d9d0907052209l458dd768h49e6aedf668fe376@mail.gmail.com>
References: <b1f16d9d0907031701p766e9cb2s7a9f61acc205b606@mail.gmail.com>
	<ce6bbb9d0907040226n1559dc51w97b0a88b3e9af0b3@mail.gmail.com>
	<b1f16d9d0907040947y57fb8119j6e8d6aa598f81dac@mail.gmail.com>
	<ce6bbb9d0907041410n2cf4c1adjf90ffa82a560a39a@mail.gmail.com>
	<b1f16d9d0907052209l458dd768h49e6aedf668fe376@mail.gmail.com>
Message-ID: <ce6bbb9d0907052252y7ab0d9a7m5397be8ee4c76b08@mail.gmail.com>

Here's a more elegant approach that should not generate the error message:

xts(rbind(coredata(myhist), rep(NA,dim(myhist)[2])),
order.by=c(index(myhist), as.Date("2009-01-01")))

Note that if class(index(myhist)) is not type "Date" you will need to
replace as.Date with the appropriate constructor, e.g. as.POSIXct or
as.chron.

Regards,
Sean.

On Mon, Jul 6, 2009 at 3:09 PM, Michael<comtech.usa at gmail.com> wrote:
> still not working...
>
> On Sat, Jul 4, 2009 at 2:10 PM, Sean Carmody<seancarmody at gmail.com> wrote:
>> Sorry, try this
>>
>> ?rbind(myhist, xts(data.frame(NA,NA,NA,NA,NA,NA),
>> order.by=as.Date("2009-08-01")))
>>
>> also, check class(time(myhist)) and if it is not "Date" you may need
>> to replace as.Date with the appropriate constructor for your time
>> class.
>>
>> Sean.
>>
>> On Sun, Jul 5, 2009 at 2:47 AM, Michael<comtech.usa at gmail.com> wrote:
>>> I got an error:
>>>
>>> Error in xts(data.frame(rep(NA, 6), order.by = as.Date("2009-08-01"))) :
>>> ?order.by requires an appropriate time-based object
>>>
>>> On Sat, Jul 4, 2009 at 2:26 AM, Sean Carmody<seancarmody at gmail.com> wrote:
>>>> Eliminating a row is straightforward, e.g.
>>>>
>>>> myhist[time(myhist) != as.Date("2009-06-29"),]
>>>>
>>>> will return everything except the entry for 2009-06-29.
>>>>
>>>> Adding a row is, I believe, a bit more fiddly. You can use
>>>>
>>>> rbind(myhist, xts(data.frame(rep(NA,6), order.by=as.Date("2009-08-01")))
>>>>
>>>> although this will give a warning message about column names
>>>> differing. If you have an additional xts with the same column names,
>>>> you can simply use merge()
>>>>
>>>> Sean.
>>>>
>>>> On Sat, Jul 4, 2009 at 10:01 AM, Michael<comtech.usa at gmail.com> wrote:
>>>>> Hi all,
>>>>>
>>>>> Let's say I have the following timeseres in IBrokers and XTS:
>>>>>
>>>>> 2009-06-25 ? ? ?18038 ? ? ?18350 ? ? 18011 ? ? ? 18327 ? ? ? ? 5396 ? ? 18183
>>>>> 2009-06-26 ? ? ?18420 ? ? ?18708 ? ? 18331 ? ? ? 18515 ? ? ? ? 7367 ? ? 18499
>>>>> 2009-06-29 ? ? ?18546 ? ? ?18688 ? ? 18423 ? ? ? 18577 ? ? ? ?28728 ? ? 18555
>>>>> 2009-07-03 ? ? ?17951 ? ? ?18198 ? ? 17866 ? ? ? 18149 ? ? ? ?38019 ? ? 18059
>>>>>
>>>>> I want to add a few more rows, indexed by
>>>>>
>>>>> "2009-06-30", "2009-07-01", "2009-07-02", and using NA's,
>>>>>
>>>>> I did the following:
>>>>>
>>>>> myhist["2009-07-01",]=c(NA, NA, NA, NA, NA, NA, NA, NA)
>>>>>
>>>>> But it didn't really work.
>>>>>
>>>>> What's the good way to insert missing points in time series?
>>>>>
>>>>> Thanks!
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Sean Carmody
>>>>
>>>> The Stubborn Mule
>>>> http://www.stubbornmule.net
>>>> http://twitter.com/seancarmody
>>>>
>>>
>>
>>
>>
>> --
>> Sean Carmody
>>
>> The Stubborn Mule
>> http://www.stubbornmule.net
>> http://twitter.com/seancarmody
>>
>



-- 
Sean Carmody

The Stubborn Mule
http://www.stubbornmule.net
http://twitter.com/seancarmody


From brian at braverock.com  Mon Jul  6 14:07:57 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 06 Jul 2009 07:07:57 -0500
Subject: [R-SIG-Finance] Does any one know how to program
 difference-in-difference in panel data with R-program ?
In-Reply-To: <BAY102-W36F626DC7163EA2665F561802C0@phx.gbl>
References: <BAY102-W36F626DC7163EA2665F561802C0@phx.gbl>
Message-ID: <4A51E91D.60109@braverock.com>

sk lai wrote:
> Does any one know how to program difference-in-difference in panel data with R-program ?
>
> Please advise,
>  
> Much thanks
>
> New user : SK Lai
>   
We will be more able to assist you if you provide some details on what 
you are trying to do, and ideally a academic reference or two for the 
technique you are trying to replicate in R.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From rbali at ufmg.br  Mon Jul  6 14:13:13 2009
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Mon, 6 Jul 2009 09:13:13 -0300
Subject: [R-SIG-Finance] Does any one know how to program
	difference-in-difference in panel data with R-program ?
In-Reply-To: <BAY102-W36F626DC7163EA2665F561802C0@phx.gbl>
References: <BAY102-W36F626DC7163EA2665F561802C0@phx.gbl>
Message-ID: <89397A24E2A249BDB8E5F260663B1ECC@DellPC>

Panel Data Econometrics in R: The plm Package


--------------------------------------------------
From: "sk lai" <sklai3 at hotmail.com>
Sent: Friday, July 03, 2009 4:43 PM
To: <r-sig-finance at stat.math.ethz.ch>
Subject: [R-SIG-Finance] Does any one know how to program 
difference-in-difference in panel data with R-program ?

>
> Dear All,
> Does any one know how to program difference-in-difference in panel data 
> with R-program ?
> Please advise,
> Much thanks

>


From jeff.a.ryan at gmail.com  Mon Jul  6 15:54:49 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 6 Jul 2009 08:54:49 -0500
Subject: [R-SIG-Finance] insert element in IBrokers/XTS time series?
In-Reply-To: <b1f16d9d0907031701p766e9cb2s7a9f61acc205b606@mail.gmail.com>
References: <b1f16d9d0907031701p766e9cb2s7a9f61acc205b606@mail.gmail.com>
Message-ID: <e8e755250907060654u692c99d9o5e79c9c5d93c3914@mail.gmail.com>

If you are simply looking to make an irregular series 'regular' per
some time-scale (e.g. days), you can merge with a zero-width xts
object or a time-based vector:

?merge.xts and example(merge.xts) will give you some tips.

> reg.time <- timeBasedSeq('20090625/20090703')    # creates a 'Date' vector, use retclass to override if needed
> reg.time
[1] "2009-06-25" "2009-06-26" "2009-06-27" "2009-06-28" "2009-06-29"
[6] "2009-06-30" "2009-07-01" "2009-07-02" "2009-07-03"

# get some daily data
> getSymbols("AAPL", from='2009-06-25')
> AAPL
           AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted
2009-06-25    135.75    140.20   135.21     139.86    21051700        139.86
2009-06-26    139.79    143.56   139.74     142.44    15692300        142.44
2009-06-29    143.46    143.95   141.54     141.97    20272000        141.97
2009-06-30    142.58    143.80   141.80     142.43    15508000        142.43
2009-07-01    143.50    144.66   142.52     142.83    14792100        142.83
2009-07-02    141.25    142.83   139.79     140.02    13231400        140.02

> merge(AAPL,reg.time)
           AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted
2009-06-25    135.75    140.20   135.21     139.86    21051700        139.86
2009-06-26    139.79    143.56   139.74     142.44    15692300        142.44
2009-06-27        NA        NA       NA         NA          NA            NA
2009-06-28        NA        NA       NA         NA          NA            NA
2009-06-29    143.46    143.95   141.54     141.97    20272000        141.97
2009-06-30    142.58    143.80   141.80     142.43    15508000        142.43
2009-07-01    143.50    144.66   142.52     142.83    14792100        142.83
2009-07-02    141.25    142.83   139.79     140.02    13231400        140.02
2009-07-03        NA        NA       NA         NA          NA            NA

HTH,
Jeff
On Fri, Jul 3, 2009 at 7:01 PM, Michael<comtech.usa at gmail.com> wrote:
> Hi all,
>
> Let's say I have the following timeseres in IBrokers and XTS:
>
> 2009-06-25 ? ? ?18038 ? ? ?18350 ? ? 18011 ? ? ? 18327 ? ? ? ? 5396 ? ? 18183
> 2009-06-26 ? ? ?18420 ? ? ?18708 ? ? 18331 ? ? ? 18515 ? ? ? ? 7367 ? ? 18499
> 2009-06-29 ? ? ?18546 ? ? ?18688 ? ? 18423 ? ? ? 18577 ? ? ? ?28728 ? ? 18555
> 2009-07-03 ? ? ?17951 ? ? ?18198 ? ? 17866 ? ? ? 18149 ? ? ? ?38019 ? ? 18059
>
> I want to add a few more rows, indexed by
>
> "2009-06-30", "2009-07-01", "2009-07-02", and using NA's,
>
> I did the following:
>
> myhist["2009-07-01",]=c(NA, NA, NA, NA, NA, NA, NA, NA)
>
> But it didn't really work.
>
> What's the good way to insert missing points in time series?
>
> Thanks!
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From joshcchien at yahoo.com  Mon Jul  6 23:46:10 2009
From: joshcchien at yahoo.com (Josh C. Chien)
Date: Mon, 6 Jul 2009 14:46:10 -0700 (PDT)
Subject: [R-SIG-Finance] Is there any great & efficient Financial
	engineering package ?
Message-ID: <486418.82859.qm@web110103.mail.gq1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090706/c59125b3/attachment.pl>

From armstrong.whit at gmail.com  Mon Jul  6 23:57:32 2009
From: armstrong.whit at gmail.com (Whit Armstrong)
Date: Mon, 6 Jul 2009 17:57:32 -0400
Subject: [R-SIG-Finance] Is there any great & efficient Financial
	engineering package ?
In-Reply-To: <486418.82859.qm@web110103.mail.gq1.yahoo.com>
References: <486418.82859.qm@web110103.mail.gq1.yahoo.com>
Message-ID: <8ec76080907061457l6cd80a87g121c0e5047132652@mail.gmail.com>

http://dirk.eddelbuettel.com/code/rquantlib.html

and Fincad can also be made to talk to R, if you know the right people.

-Whit


On Mon, Jul 6, 2009 at 5:46 PM, Josh C. Chien<joshcchien at yahoo.com> wrote:
> Hi R-Finance,
> I'm new user for R.
> For work, I need to pricing fixed income products and analyze fixed income, like duration, DVO1, curve fitting, etc.
> Besides, I'm also working on quantitative numerical methods in risk.
> Does anyone recommend me to install what package and how to make use of R efficiently.
> I'm a junior risk quant for insurance co. In past experience, I used VBA much.
> Now, I chose R being my daily implement tool for analysis in market risk or credit risk.
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From brian at braverock.com  Mon Jul  6 23:58:14 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 06 Jul 2009 16:58:14 -0500
Subject: [R-SIG-Finance] Is there any great & efficient
 Financial	engineering package ?
In-Reply-To: <486418.82859.qm@web110103.mail.gq1.yahoo.com>
References: <486418.82859.qm@web110103.mail.gq1.yahoo.com>
Message-ID: <4A527376.9020603@braverock.com>

There are many pricing and risk packages available for R.  I suggest 
checking the task views, doing your homework, and asking a more specific 
question.

http://cran.r-project.org/web/views/Finance.html
http://cran.r-project.org/web/views/Econometrics.html
http://cran.r-project.org/web/views/TimeSeries.html

Good Luck,

   - Brian

Josh C. Chien wrote:
> Hi R-Finance,
> I'm new user for R.
> For work, I need to pricing fixed income products and analyze fixed income, like duration, DVO1, curve fitting, etc.
> Besides, I'm also working on quantitative numerical methods in risk.
> Does anyone recommend me to install what package and how to make use of R efficiently.
> I'm a junior risk quant for insurance co. In past experience, I used VBA much. 
> Now, I chose R being my daily implement tool for analysis in market risk or credit risk.
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From sergeyg at gmail.com  Tue Jul  7 16:55:23 2009
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Tue, 7 Jul 2009 16:55:23 +0200
Subject: [R-SIG-Finance] RBloomberg warning message
Message-ID: <7cb007bd0907070755m13efd18dkafe6cd6b727b40f1@mail.gmail.com>

Hello, everyone

I have this very long script, where I call blpGetData many times (I
get some data, I do some computations, I output results in and Excel
file, I call some other data....)

As the code grew, I started to get the same warning message more and more often:

"Warning message:
In as.matrix.BlpCOMReturn(x) NAs are introduced by coersion"

Basically, blpGetData sometimes does not work!

This message comes up in different parts of the code (in different
calls to blpGetData), and since script is very long and runs
considerable amount of time, one such error
completely messes up the end results.

Does anyone know why blpGetData sometimes fails to execute?

Thanks in advance for help!

Best,
Sergey


From nelson.ana at gmail.com  Tue Jul  7 18:50:42 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Tue, 7 Jul 2009 17:50:42 +0100
Subject: [R-SIG-Finance] RBloomberg warning message
In-Reply-To: <7cb007bd0907070755m13efd18dkafe6cd6b727b40f1@mail.gmail.com>
References: <7cb007bd0907070755m13efd18dkafe6cd6b727b40f1@mail.gmail.com>
Message-ID: <a7d6d2740907070950y6fc2210m1c6f95f9f85951d7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090707/b09932dc/attachment.pl>

From icos.atropa at gmail.com  Wed Jul  8 08:26:47 2009
From: icos.atropa at gmail.com (Christian Gunning)
Date: Tue, 7 Jul 2009 23:26:47 -0700
Subject: [R-SIG-Finance] getting xts endpoints() and apply.daiy() to work
	with local TZ?
Message-ID: <681d07c20907072326r73042b8cv11018633836200e8@mail.gmail.com>

I'm looking for the best way to get apply.daily() to use breaks
according to local TZ.  Sorry, i know timezone handling is perennial
issue.  I reviewed R News 4/1, R-sig-fin, ?indexTZ, and
?DateTimeClasses and xts R source.

Currently, apply.daily() in a non-GMT timezone computes breaks
according to GMT (A note about this in apply.daily() or endpoints()
might preempt user confusion?).

Below, I compute the difference between local time and GMT in seconds,
and add it into the call to which() in endpoints()  to get my desired
answer. Is endpoints() support of TZ on the map, and is there a better
way than this to add it (particularly, calculating tmpdiff below)? Is
this a bad idea altogether?

thanks,
christian gunning
university of new mexico

###
oldtz = Sys.getenv('TZ')
Sys.setenv(TZ="GMT+7")

tmpind = as.POSIXct(seq(from=0, to=5*24*60*60-60, by=60*60),
origin='2000-01-01')

tmpx = xts(1:(5*24), tmpind)
indexTZ(tmpx)      ### correctly reports TZ as "GMT+7"
tmpind[1]          ### format.POSIXlt? reports TZ as "GMT",
tmpx[endpoints(tmpx, on='days')]  ### daybreaks as per GMT


### from endpoints(), added tmpdiff, daybreaks as per local TZ
tmpdiff = as.integer(as.POSIXct(0, origin='2000-01-01', tz='GMT')) -
as.integer(as.POSIXct(0, origin='2000-01-01'))

tmpdays = as.integer(c(0,
which(diff((.index(tmpx)+tmpdiff)%/%86400L%/%k + 1) != 0),
NROW(tmpx)))

tmpx[tmpdays]

Sys.setenv(TZ=oldtz)
###


From Murali.MENON at fortisinvestments.com  Wed Jul  8 12:38:45 2009
From: Murali.MENON at fortisinvestments.com (Murali.MENON at fortisinvestments.com)
Date: Wed, 8 Jul 2009 12:38:45 +0200
Subject: [R-SIG-Finance] Preventing active-neutral-active bouncing signals
	by staying neutral
Message-ID: <5A3D018CBDC36B4F8FF6DB52DDF3B82DB1AD40@BE-S0500-V22.adroot.local>

Folks,
 
I have a series of active (+1, -1) and neutral (0) signals, and I want
to prevent bouncing from active to neutral to active (same sign). E.g.,
if I am -1 yesterday and neutral today, I will neutralise, but if the
signal becomes -1 again tomorrow, I want to remain neutral; if it does
remain -1 the following day as well, I will accept that and go short.
 
So if I have:
 
> x <- zoo(matrix(c(1,0,1,1,0,-1,0,1,-1,0,1,1),ncol=2), as.Date(1:6))
> x
1970-01-02  1  0
1970-01-03  0  1
1970-01-04  1 -1
1970-01-05  1  0
1970-01-06  0  1
1970-01-07 -1  1
 
in the first column, I'll neutralise my position on 03-Jan-70, ignore
the new active signal on 04-Jan-70, and go long again until 05-Jan-1970;
neutral again on 06-Jan-70, but will go short on 07-Jan-70, because the
signal is opposite to what it was before I neutralised.
 
Likewise, in column 2, I'll go neutral on 05-Jan-1970 but accept the
long signal the very next day because it is opposite to my previous
position.
 
How best to convert this zoo into a new zoo with ZEROES in place of
active signals where I have determined I want to continue to remain
neutral?

In general, I may want to remain neutral for 'n' consecutive days, and
then only accept a signal in the same direction as I started with after
I get 'm' successive identical active signals; but I will accept a
signal in the opposite direction at once if it appears after the 'n'
neutrals.
 
Is there a way to code this generalisation?

(All I could think of was rather roundabout: determine the dates of
inception of each signal (separately for +1s and -1s), and if the
difference between these dates is n-1, then neutralise the signal on the
latter date. But this doesn't take care of the 'm' successive active
signals before activating.)
 
Cheers,
Murali


From sergeyg at gmail.com  Wed Jul  8 13:37:39 2009
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Wed, 8 Jul 2009 13:37:39 +0200
Subject: [R-SIG-Finance] RBloomberg warning message
Message-ID: <7cb007bd0907080437o7d98613cm62f7ab0a059a2c7f@mail.gmail.com>

Hi, Ana,

Thank you for answering.
In each load I download only numerical data (prices), I am not
combining different types of data.
I use retval="zoo", because I find it most useful for my purposes.
Moreover, I've tried "data.frame" before and there were problems with
this setting for retval.
I cannot provide an example script, firstly because it is proprietary,
secondly, because it is very long, and thirdly, because providing
something illustrative is difficult as the problem pops up in
different places (I call blpGetData many times in the script and at
different places) or not at all sometimes!

Since I load daily data for last 5 years and for up to 30 instruments
sometimes, I think the load size may be responsible. But then only
partly, because the same problem pops up when I load 4 years of daily
data for just one ticker. Also the problem does not always pop up when
I load data for 30 tickers at the same time.

Maybe it is some kind of function overload? Before I used to connect
at the beginning of the script, get some data, process it, output
results, get some more data, etc... without disconnecting after each
data retreival call. From today I
connect-retreive-disconnect-process-connect-...

Basically, I am not sure why the problem pops up from time to time.

Kind Regards,
Sergey





Hi, Sergey,
A matrix can only handle 1 kind of data. Everything has to be a numeric, or
everything has to be a string. Are you trying to combine different types of
data?
It might be better to try retval="data.frame".
Otherwise, can you provide an example script which reproduces the problem?
Regards,
Ana

On Tue, Jul 7, 2009 at 3:55 PM, Sergey Goriatchev <sergeyg at gmail.com> wrote:
> Hello, everyone
>
> I have this very long script, where I call blpGetData many times (I
> get some data, I do some computations, I output results in and Excel
> file, I call some other data....)
>
> As the code grew, I started to get the same warning message more and more
> often:
>
> "Warning message:
> In as.matrix.BlpCOMReturn(x) NAs are introduced by coersion"
>
> Basically, blpGetData sometimes does not work!
>
> This message comes up in different parts of the code (in different
> calls to blpGetData), and since script is very long and runs
> considerable amount of time, one such error
> completely messes up the end results.
>
> Does anyone know why blpGetData sometimes fails to execute?
>
> Thanks in advance for help!
>
> Best,
> Sergey



--
I'm not young enough to know everything. /Oscar Wilde
Experience is one thing you can't get for nothing. /Oscar Wilde
When you are finished changing, you're finished. /Benjamin Franklin
Tell me and I forget, teach me and I remember, involve me and I learn.
/Benjamin Franklin
Luck is where preparation meets opportunity. /George Patten


From jeff.a.ryan at gmail.com  Wed Jul  8 14:33:16 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 8 Jul 2009 07:33:16 -0500
Subject: [R-SIG-Finance] getting xts endpoints() and apply.daiy() to
	work with local TZ?
In-Reply-To: <681d07c20907072326r73042b8cv11018633836200e8@mail.gmail.com>
References: <681d07c20907072326r73042b8cv11018633836200e8@mail.gmail.com>
Message-ID: <e8e755250907080533r18ed66a7odc8c785a44b8d11@mail.gmail.com>

Hi Christian,

endpoints was patched about two weeks ago on R-forge for this issue.
You can install from there, or there will be a new CRAN version coming
in the next few weeks (after Rennes).

The fixed version with commit log:

http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/pkg/R/endpoints.R?rev=417&root=xts&view=markup



To install from R-forge:

install.packages("xts", repos="http://r-forge.r-project.org")


Thanks again for the report and suggestion!

Jeff


On Wed, Jul 8, 2009 at 1:26 AM, Christian Gunning<icos.atropa at gmail.com> wrote:
> I'm looking for the best way to get apply.daily() to use breaks
> according to local TZ. ?Sorry, i know timezone handling is perennial
> issue. ?I reviewed R News 4/1, R-sig-fin, ?indexTZ, and
> ?DateTimeClasses and xts R source.
>
> Currently, apply.daily() in a non-GMT timezone computes breaks
> according to GMT (A note about this in apply.daily() or endpoints()
> might preempt user confusion?).
>
> Below, I compute the difference between local time and GMT in seconds,
> and add it into the call to which() in endpoints() ?to get my desired
> answer. Is endpoints() support of TZ on the map, and is there a better
> way than this to add it (particularly, calculating tmpdiff below)? Is
> this a bad idea altogether?
>
> thanks,
> christian gunning
> university of new mexico
>
> ###
> oldtz = Sys.getenv('TZ')
> Sys.setenv(TZ="GMT+7")
>
> tmpind = as.POSIXct(seq(from=0, to=5*24*60*60-60, by=60*60),
> origin='2000-01-01')
>
> tmpx = xts(1:(5*24), tmpind)
> indexTZ(tmpx) ? ? ?### correctly reports TZ as "GMT+7"
> tmpind[1] ? ? ? ? ?### format.POSIXlt? reports TZ as "GMT",
> tmpx[endpoints(tmpx, on='days')] ?### daybreaks as per GMT
>
>
> ### from endpoints(), added tmpdiff, daybreaks as per local TZ
> tmpdiff = as.integer(as.POSIXct(0, origin='2000-01-01', tz='GMT')) -
> as.integer(as.POSIXct(0, origin='2000-01-01'))
>
> tmpdays = as.integer(c(0,
> which(diff((.index(tmpx)+tmpdiff)%/%86400L%/%k + 1) != 0),
> NROW(tmpx)))
>
> tmpx[tmpdays]
>
> Sys.setenv(TZ=oldtz)
> ###
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From robert at sanctumfi.com  Wed Jul  8 14:36:41 2009
From: robert at sanctumfi.com (Robert Sams)
Date: Wed, 8 Jul 2009 13:36:41 +0100
Subject: [R-SIG-Finance] RBloomberg warning message
References: <SANCTUMFISERVER8vWy00000761@sanctumfi.com>
Message-ID: <SANCTUMFISERVERasSm000007cc@sanctumfi.com>

Sergey, you need to do better than this. 

> Hi, Ana,
> 
> Thank you for answering.
> In each load I download only numerical data (prices), I am 
> not combining different types of data.
> I use retval="zoo", because I find it most useful for my purposes.
> Moreover, I've tried "data.frame" before and there were 
> problems with this setting for retval.
> I cannot provide an example script, firstly because it is 
> proprietary, secondly, because it is very long, and thirdly, 
> because providing something illustrative is difficult as the 
> problem pops up in different places (I call blpGetData many 
> times in the script and at different places) or not at all sometimes!

If your script is proprietary, then rewrite for this list a line or two
of code that replicates a case of the problem. If you can't replicate
the problem and it seems to be an emerging property of your big script,
I'd suggest you rewrite the script in a more modular way so that (1) the
problem goes away or (2) you are able to replicate the problem at least
stochastically with some code you can post here.

> Since I load daily data for last 5 years and for up to 30 
> instruments sometimes, I think the load size may be 
> responsible. But then only partly, because the same problem 
> pops up when I load 4 years of daily data for just one 
> ticker. Also the problem does not always pop up when I load 
> data for 30 tickers at the same time.

Are you aware of Bloomberg's "Extended Rules and Usage Limits"? 

> 
> Maybe it is some kind of function overload? Before I used to 
> connect at the beginning of the script, get some data, 
> process it, output results, get some more data, etc... 
> without disconnecting after each data retreival call. From 
> today I connect-retreive-disconnect-process-connect-...

Did this problem exist when your script was making just one connection
request? Do you have a good reason for making multiple
connect/disconnect calls? I'm sure it's possible to torture the
bloomberg connection control to the point where the API does weird
things.
 
> Basically, I am not sure why the problem pops up from time to time.

Without examples, neither are we.


Robert

> 
> Kind Regards,
> Sergey
> 
> 
> 
> 
> 
> Hi, Sergey,
> A matrix can only handle 1 kind of data. Everything has to be 
> a numeric, or everything has to be a string. Are you trying 
> to combine different types of data?
> It might be better to try retval="data.frame".
> Otherwise, can you provide an example script which reproduces 
> the problem?
> Regards,
> Ana
> 
> On Tue, Jul 7, 2009 at 3:55 PM, Sergey Goriatchev 
> <sergeyg at gmail.com> wrote:
> > Hello, everyone
> >
> > I have this very long script, where I call blpGetData many times (I 
> > get some data, I do some computations, I output results in 
> and Excel 
> > file, I call some other data....)
> >
> > As the code grew, I started to get the same warning message 
> more and 
> > more
> > often:
> >
> > "Warning message:
> > In as.matrix.BlpCOMReturn(x) NAs are introduced by coersion"
> >
> > Basically, blpGetData sometimes does not work!
> >
> > This message comes up in different parts of the code (in different 
> > calls to blpGetData), and since script is very long and runs 
> > considerable amount of time, one such error completely 
> messes up the 
> > end results.
> >
> > Does anyone know why blpGetData sometimes fails to execute?
> >
> > Thanks in advance for help!
> >
> > Best,
> > Sergey
> 
> 
> 
> --
> I'm not young enough to know everything. /Oscar Wilde 
> Experience is one thing you can't get for nothing. /Oscar 
> Wilde When you are finished changing, you're finished. 
> /Benjamin Franklin Tell me and I forget, teach me and I 
> remember, involve me and I learn.
> /Benjamin Franklin
> Luck is where preparation meets opportunity. /George Patten
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 


From olivier.schmitt at gmail.com  Wed Jul  8 16:19:44 2009
From: olivier.schmitt at gmail.com (Olivier Schmitt)
Date: Wed, 8 Jul 2009 10:19:44 -0400
Subject: [R-SIG-Finance] Date format with RBloomberg and timeSeries
Message-ID: <2c9fcb830907080719i33a95ca9q169c853a92fb9d89@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090708/3463e726/attachment.pl>

From nelson.ana at gmail.com  Wed Jul  8 16:37:47 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Wed, 8 Jul 2009 15:37:47 +0100
Subject: [R-SIG-Finance] Date format with RBloomberg and timeSeries
In-Reply-To: <2c9fcb830907080719i33a95ca9q169c853a92fb9d89@mail.gmail.com>
References: <2c9fcb830907080719i33a95ca9q169c853a92fb9d89@mail.gmail.com>
Message-ID: <a7d6d2740907080737q1118d947t54f0e0fec9c5c96a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090708/dfd6c44a/attachment.pl>

From ggrothendieck at gmail.com  Wed Jul  8 21:05:12 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 8 Jul 2009 15:05:12 -0400
Subject: [R-SIG-Finance] Preventing active-neutral-active bouncing
	signals by staying neutral
In-Reply-To: <5A3D018CBDC36B4F8FF6DB52DDF3B82DB1AD40@BE-S0500-V22.adroot.local>
References: <5A3D018CBDC36B4F8FF6DB52DDF3B82DB1AD40@BE-S0500-V22.adroot.local>
Message-ID: <971536df0907081205p5c1c8462s744cbc7e3af9ebc0@mail.gmail.com>

The rules are not clear but maybe this code will help in
constructing whatever it is. newstate takes the current state
and action and produces the new state from those two such
that the state is the signed number of signals in the current
direction (since the last signal in the other direction).

newstate <- function(state, action) {
	if (action == 0) state
	else if (action * state > 0) state <- state+action
	else action
}
Reduce(newstate, coredata(x), 0, acc = TRUE)




On Wed, Jul 8, 2009 at 6:38 AM, <Murali.MENON at fortisinvestments.com> wrote:
> Folks,
>
> I have a series of active (+1, -1) and neutral (0) signals, and I want
> to prevent bouncing from active to neutral to active (same sign). E.g.,
> if I am -1 yesterday and neutral today, I will neutralise, but if the
> signal becomes -1 again tomorrow, I want to remain neutral; if it does
> remain -1 the following day as well, I will accept that and go short.
>
> So if I have:
>
>> x <- zoo(matrix(c(1,0,1,1,0,-1,0,1,-1,0,1,1),ncol=2), as.Date(1:6))
>> x
> 1970-01-02 ?1 ?0
> 1970-01-03 ?0 ?1
> 1970-01-04 ?1 -1
> 1970-01-05 ?1 ?0
> 1970-01-06 ?0 ?1
> 1970-01-07 -1 ?1
>
> in the first column, I'll neutralise my position on 03-Jan-70, ignore
> the new active signal on 04-Jan-70, and go long again until 05-Jan-1970;
> neutral again on 06-Jan-70, but will go short on 07-Jan-70, because the
> signal is opposite to what it was before I neutralised.
>
> Likewise, in column 2, I'll go neutral on 05-Jan-1970 but accept the
> long signal the very next day because it is opposite to my previous
> position.
>
> How best to convert this zoo into a new zoo with ZEROES in place of
> active signals where I have determined I want to continue to remain
> neutral?
>
> In general, I may want to remain neutral for 'n' consecutive days, and
> then only accept a signal in the same direction as I started with after
> I get 'm' successive identical active signals; but I will accept a
> signal in the opposite direction at once if it appears after the 'n'
> neutrals.
>
> Is there a way to code this generalisation?
>
> (All I could think of was rather roundabout: determine the dates of
> inception of each signal (separately for +1s and -1s), and if the
> difference between these dates is n-1, then neutralise the signal on the
> latter date. But this doesn't take care of the 'm' successive active
> signals before activating.)
>
> Cheers,
> Murali
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From olivier.schmitt at gmail.com  Wed Jul  8 22:27:09 2009
From: olivier.schmitt at gmail.com (Olivier Schmitt)
Date: Wed, 8 Jul 2009 16:27:09 -0400
Subject: [R-SIG-Finance] Date format with RBloomberg and timeSeries
In-Reply-To: <a7d6d2740907080737q1118d947t54f0e0fec9c5c96a@mail.gmail.com>
References: <2c9fcb830907080719i33a95ca9q169c853a92fb9d89@mail.gmail.com>
	<a7d6d2740907080737q1118d947t54f0e0fec9c5c96a@mail.gmail.com>
Message-ID: <2c9fcb830907081327i1583210awe82bc28cd3405fa7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090708/55f661b4/attachment.pl>

From megh700004 at yahoo.com  Wed Jul  8 23:03:09 2009
From: megh700004 at yahoo.com (megh)
Date: Wed, 8 Jul 2009 14:03:09 -0700 (PDT)
Subject: [R-SIG-Finance] Re[R-sig-finance] uters data
Message-ID: <24399329.post@talk.nabble.com>


Hi, is there any R package/function which downloads data from Reuters
terminal?

Regards,
-- 
View this message in context: http://www.nabble.com/Reuters-data-tp24399329p24399329.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ksabol at dsaco.com  Thu Jul  9 02:05:18 2009
From: ksabol at dsaco.com (Keith Sabol)
Date: Wed, 8 Jul 2009 18:05:18 -0600
Subject: [R-SIG-Finance] Coercion problem in RBloomberg
Message-ID: <BF4613BA62AAD6459F2C1FB9FF9C1EDB01D8EF5A@orange.davis.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090708/5c333ffc/attachment.pl>

From VOSSK at kochind.com  Thu Jul  9 03:53:12 2009
From: VOSSK at kochind.com (Voss, Kent)
Date: Wed, 8 Jul 2009 18:53:12 -0700
Subject: [R-SIG-Finance] RBloomberg warning message
In-Reply-To: <a7d6d2740907070950y6fc2210m1c6f95f9f85951d7@mail.gmail.com>
References: <7cb007bd0907070755m13efd18dkafe6cd6b727b40f1@mail.gmail.com>
	<a7d6d2740907070950y6fc2210m1c6f95f9f85951d7@mail.gmail.com>
Message-ID: <13CB99597D0BAA43B07ABFCE29E45F6B0262FE27@phx0mbx01.kochind.com>

Sergey,

I used to have this exact same problem as well.  I put it down to
Bloomberg just hiccups sometimes (it's not just Rbloomberg, I've had the
problem with calls to their API from other languages as well).

To work around it, I wrote a script that essentially downloads the data
to a local text file the first time you successfully get the data, and
then it doesn't go back to Bloomberg for that day.  So in the morning I
have a dedicated script that just goes out to Bloomberg to get the data
I need, and I re-run that script until it runs without warnings...ie.
until I have all the data I need.  Then I run my analysis script which I
don't have to worry about data not being there.  I am not a programmer,
so the script may look awful, but it does work.  

Hopefully you can customize it to suit your purpose.  Good luck.

Kent

# Script (also in attached file)
# Standardized function to simplify downloading Bloomberg data
# The essence of this function is to standardize calls (for me at least)
to Bloomberg
# and also to store retrieved data as a flat file.
# Storing the data as a flat file allows you to run a data retrieval
script
# multiple times if necessary to ensure all the data is downloaded,
before running the analytics
# Once a data element (Instrument + Field) has been downloaded for the
day, the script will get the
# downloaded data and not go back to Bloomberg.  This ensures that once
you've gotten the data for
# the day, that your analytic script won't bomb out b/c the Bloomberg
retrieval hiccupped.
getBBData <- function(Instruments, Field, StartDate, EndDate=Sys.Date(),
ShowDays="week", NAaction="na", Periodicity="daily") {
  library(RBloomberg)

  # Set the path and naming convention for files to stor retrieved data
  filename <- paste("c:/blp/data/",Instruments," ",Field,".txt",sep="")

  # Get the file information to see if it exists
  fileinfo <- file.info(filename)
  mdate <- as.Date(strptime(fileinfo$mtime, format="%Y-%m-%d"))

  # If the file exists, move on to the next step to check the datestamp
  if(!is.na(fileinfo$size[1])) {
    if(fileinfo$size[1] > 20) {
      validfile = TRUE
    } else {
      validfile = FALSE
    }
  } else {
    validfile = FALSE
  }
  # If the file exists and the last modified date is today, read it in
  if((length(Instruments) == 1) & validfile & ((Sys.Date() - mdate[1])
== 0)) {
      Data <- read.zoo(file=filename,header=TRUE,format="%Y-%m-%d")
      Name <- strsplit(Instruments," ")[[1]][1]
      Data <- zoo(data.frame(Data = coredata(Data)), time(Data))
      colnames(Data) <- Name
  } else { # If the file doesn't exist, go out to Bloomberg and get the
data

    # Variables used for function testing
  #  Instruments <- c('SPTR Index','MXEA Index','SPGSCITR Index','FNERTR
Index')
  #  Field <- "PX_LAST"
  #  StartDate <- as.Date('2008-1-1', '%Y-%m-%d')
  #  EndDate <- Sys.Date()

    # Convert the EndDate to a datetime object
    EndTime <- as.POSIXct(EndDate)

    # Calculate the length in days to get data
    BBDateLen <- as.numeric(EndDate - StartDate)

    ## Conect to Bloomberg
    conn <- blpConnect(iface="COM", timeout=12000, show.days=ShowDays,
na.action=NAaction, periodicity=Periodicity)

    ## Get the historical data
    Data <- blpGetData(conn, Instruments, Field, start=as.chron(EndTime
- 86400 * BBDateLen))

    ## Disconnect from Bloomberg
    blpDisconnect(conn)

    ## Convert it to a zoo object  with the appropriate time/dates
    Data <- zoo(coredata(Data),as.Date(time(Data),'%m/%d/%y'))

    # Write the data to a file for subsequent retrieval
    if(length(Instruments) == 1) {
      Name <- strsplit(Instruments," ")[[1]][1]
      Data <- zoo(data.frame(Data = coredata(Data)), time(Data))
      colnames(Data) <- Name
      write.zoo(Data, file=filename)
    }
  } # END if(!is.na(fileinfo$size) & ((Sys.Date() - mdate) == 0)) {

  return(Data)

} #END getBBData <- function(Instruments, Field, StartDate,
EndDate=Sys.Date()) (

-----Original Message-----
From: Ana Nelson [mailto:nelson.ana at gmail.com] 
Sent: Tuesday, July 07, 2009 9:51 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] RBloomberg warning message

Hi, Sergey,

A matrix can only handle 1 kind of data. Everything has to be a numeric,
or everything has to be a string. Are you trying to combine different
types of data?

It might be better to try retval="data.frame".

Otherwise, can you provide an example script which reproduces the
problem?

Regards,
Ana



On Tue, Jul 7, 2009 at 3:55 PM, Sergey Goriatchev <sergeyg at gmail.com>
wrote:

> Hello, everyone
>
> I have this very long script, where I call blpGetData many times (I 
> get some data, I do some computations, I output results in and Excel 
> file, I call some other data....)
>
> As the code grew, I started to get the same warning message more and 
> more
> often:
>
> "Warning message:
> In as.matrix.BlpCOMReturn(x) NAs are introduced by coersion"
>
> Basically, blpGetData sometimes does not work!
>
> This message comes up in different parts of the code (in different 
> calls to blpGetData), and since script is very long and runs 
> considerable amount of time, one such error completely messes up the 
> end results.
>
> Does anyone know why blpGetData sometimes fails to execute?
>
> Thanks in advance for help!
>
> Best,
> Sergey
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

	[[alternative HTML version deleted]]



"EMF <kochind.com>" made the following annotations.
------------------------------------------------------------------------------
The information in this e-mail and any attachments is confidential and intended solely for the attention and use of the named addressee(s). It must not be disclosed to any person without proper authority. If you are not the intended recipient, or a person responsible for delivering it to the intended recipient, you are not authorized to and must not disclose, copy, distribute, or retain this message or any part of it.

==============================================================================
-------------- next part --------------
A non-text attachment was scrubbed...
Name: getBBData.R
Type: application/octet-stream
Size: 3163 bytes
Desc: getBBData.R
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090708/c078db69/attachment.obj>

From sergeyg at gmail.com  Thu Jul  9 08:23:30 2009
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Thu, 9 Jul 2009 08:23:30 +0200
Subject: [R-SIG-Finance] RBloomberg warning message
In-Reply-To: <13CB99597D0BAA43B07ABFCE29E45F6B0262FE27@phx0mbx01.kochind.com>
References: <7cb007bd0907070755m13efd18dkafe6cd6b727b40f1@mail.gmail.com>
	<a7d6d2740907070950y6fc2210m1c6f95f9f85951d7@mail.gmail.com>
	<13CB99597D0BAA43B07ABFCE29E45F6B0262FE27@phx0mbx01.kochind.com>
Message-ID: <7cb007bd0907082323h44e9c9a9t5da0d57ef694d4f1@mail.gmail.com>

Dear Kent,

Thank you for this suggestion!
In fact, I had a second version of the script, based on Excel files. I
loaded data in excel (manually thru bloomberg excel update functions)
then ran the main script on these files. That never fails to work.
I moved to RBloomberg to automate the process, but if I continue to
get the hiccups then I will consider your way.

Regards,
Sergey

On Thu, Jul 9, 2009 at 03:53, Voss, Kent<VOSSK at kochind.com> wrote:
> Sergey,
>
> I used to have this exact same problem as well. ?I put it down to
> Bloomberg just hiccups sometimes (it's not just Rbloomberg, I've had the
> problem with calls to their API from other languages as well).
>
> To work around it, I wrote a script that essentially downloads the data
> to a local text file the first time you successfully get the data, and
> then it doesn't go back to Bloomberg for that day. ?So in the morning I
> have a dedicated script that just goes out to Bloomberg to get the data
> I need, and I re-run that script until it runs without warnings...ie.
> until I have all the data I need. ?Then I run my analysis script which I
> don't have to worry about data not being there. ?I am not a programmer,
> so the script may look awful, but it does work.
>
> Hopefully you can customize it to suit your purpose. ?Good luck.
>
> Kent
>
> # Script (also in attached file)
> # Standardized function to simplify downloading Bloomberg data
> # The essence of this function is to standardize calls (for me at least)
> to Bloomberg
> # and also to store retrieved data as a flat file.
> # Storing the data as a flat file allows you to run a data retrieval
> script
> # multiple times if necessary to ensure all the data is downloaded,
> before running the analytics
> # Once a data element (Instrument + Field) has been downloaded for the
> day, the script will get the
> # downloaded data and not go back to Bloomberg. ?This ensures that once
> you've gotten the data for
> # the day, that your analytic script won't bomb out b/c the Bloomberg
> retrieval hiccupped.
> getBBData <- function(Instruments, Field, StartDate, EndDate=Sys.Date(),
> ShowDays="week", NAaction="na", Periodicity="daily") {
> ?library(RBloomberg)
>
> ?# Set the path and naming convention for files to stor retrieved data
> ?filename <- paste("c:/blp/data/",Instruments," ",Field,".txt",sep="")
>
> ?# Get the file information to see if it exists
> ?fileinfo <- file.info(filename)
> ?mdate <- as.Date(strptime(fileinfo$mtime, format="%Y-%m-%d"))
>
> ?# If the file exists, move on to the next step to check the datestamp
> ?if(!is.na(fileinfo$size[1])) {
> ? ?if(fileinfo$size[1] > 20) {
> ? ? ?validfile = TRUE
> ? ?} else {
> ? ? ?validfile = FALSE
> ? ?}
> ?} else {
> ? ?validfile = FALSE
> ?}
> ?# If the file exists and the last modified date is today, read it in
> ?if((length(Instruments) == 1) & validfile & ((Sys.Date() - mdate[1])
> == 0)) {
> ? ? ?Data <- read.zoo(file=filename,header=TRUE,format="%Y-%m-%d")
> ? ? ?Name <- strsplit(Instruments," ")[[1]][1]
> ? ? ?Data <- zoo(data.frame(Data = coredata(Data)), time(Data))
> ? ? ?colnames(Data) <- Name
> ?} else { # If the file doesn't exist, go out to Bloomberg and get the
> data
>
> ? ?# Variables used for function testing
> ?# ?Instruments <- c('SPTR Index','MXEA Index','SPGSCITR Index','FNERTR
> Index')
> ?# ?Field <- "PX_LAST"
> ?# ?StartDate <- as.Date('2008-1-1', '%Y-%m-%d')
> ?# ?EndDate <- Sys.Date()
>
> ? ?# Convert the EndDate to a datetime object
> ? ?EndTime <- as.POSIXct(EndDate)
>
> ? ?# Calculate the length in days to get data
> ? ?BBDateLen <- as.numeric(EndDate - StartDate)
>
> ? ?## Conect to Bloomberg
> ? ?conn <- blpConnect(iface="COM", timeout=12000, show.days=ShowDays,
> na.action=NAaction, periodicity=Periodicity)
>
> ? ?## Get the historical data
> ? ?Data <- blpGetData(conn, Instruments, Field, start=as.chron(EndTime
> - 86400 * BBDateLen))
>
> ? ?## Disconnect from Bloomberg
> ? ?blpDisconnect(conn)
>
> ? ?## Convert it to a zoo object ?with the appropriate time/dates
> ? ?Data <- zoo(coredata(Data),as.Date(time(Data),'%m/%d/%y'))
>
> ? ?# Write the data to a file for subsequent retrieval
> ? ?if(length(Instruments) == 1) {
> ? ? ?Name <- strsplit(Instruments," ")[[1]][1]
> ? ? ?Data <- zoo(data.frame(Data = coredata(Data)), time(Data))
> ? ? ?colnames(Data) <- Name
> ? ? ?write.zoo(Data, file=filename)
> ? ?}
> ?} # END if(!is.na(fileinfo$size) & ((Sys.Date() - mdate) == 0)) {
>
> ?return(Data)
>
> } #END getBBData <- function(Instruments, Field, StartDate,
> EndDate=Sys.Date()) (
>
> -----Original Message-----
> From: Ana Nelson [mailto:nelson.ana at gmail.com]
> Sent: Tuesday, July 07, 2009 9:51 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] RBloomberg warning message
>
> Hi, Sergey,
>
> A matrix can only handle 1 kind of data. Everything has to be a numeric,
> or everything has to be a string. Are you trying to combine different
> types of data?
>
> It might be better to try retval="data.frame".
>
> Otherwise, can you provide an example script which reproduces the
> problem?
>
> Regards,
> Ana
>
>
>
> On Tue, Jul 7, 2009 at 3:55 PM, Sergey Goriatchev <sergeyg at gmail.com>
> wrote:
>
>> Hello, everyone
>>
>> I have this very long script, where I call blpGetData many times (I
>> get some data, I do some computations, I output results in and Excel
>> file, I call some other data....)
>>
>> As the code grew, I started to get the same warning message more and
>> more
>> often:
>>
>> "Warning message:
>> In as.matrix.BlpCOMReturn(x) NAs are introduced by coersion"
>>
>> Basically, blpGetData sometimes does not work!
>>
>> This message comes up in different parts of the code (in different
>> calls to blpGetData), and since script is very long and runs
>> considerable amount of time, one such error completely messes up the
>> end results.
>>
>> Does anyone know why blpGetData sometimes fails to execute?
>>
>> Thanks in advance for help!
>>
>> Best,
>> Sergey
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
>
>
> "EMF <kochind.com>" made the following annotations.
> ------------------------------------------------------------------------------
> The information in this e-mail and any attachments is ...{{dropped:23}}


From sergeyg at gmail.com  Thu Jul  9 08:30:05 2009
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Thu, 9 Jul 2009 08:30:05 +0200
Subject: [R-SIG-Finance] RBloomberg warning message
In-Reply-To: <SANCTUMFISERVERasSm000007cc@sanctumfi.com>
References: <SANCTUMFISERVER8vWy00000761@sanctumfi.com>
	<SANCTUMFISERVERasSm000007cc@sanctumfi.com>
Message-ID: <7cb007bd0907082330r2032c8aeo1b641e0db312c0a8@mail.gmail.com>

Dear Robert,

I know you are right about providing examples etc
It is just not possible in this case.

WHen I was only made one connection request I did not get any errors.
My code is already very modular, that is also one of the reasons to
call blpGetData in different places in the code. The other reason, I
am managing my RAM.

I'll try to figure something out.

Thank you for your reply!

Regards,
Sergey

On Wed, Jul 8, 2009 at 14:36, Robert Sams<robert at sanctumfi.com> wrote:
> Sergey, you need to do better than this.
>
>> Hi, Ana,
>>
>> Thank you for answering.
>> In each load I download only numerical data (prices), I am
>> not combining different types of data.
>> I use retval="zoo", because I find it most useful for my purposes.
>> Moreover, I've tried "data.frame" before and there were
>> problems with this setting for retval.
>> I cannot provide an example script, firstly because it is
>> proprietary, secondly, because it is very long, and thirdly,
>> because providing something illustrative is difficult as the
>> problem pops up in different places (I call blpGetData many
>> times in the script and at different places) or not at all sometimes!
>
> If your script is proprietary, then rewrite for this list a line or two
> of code that replicates a case of the problem. If you can't replicate
> the problem and it seems to be an emerging property of your big script,
> I'd suggest you rewrite the script in a more modular way so that (1) the
> problem goes away or (2) you are able to replicate the problem at least
> stochastically with some code you can post here.
>
>> Since I load daily data for last 5 years and for up to 30
>> instruments sometimes, I think the load size may be
>> responsible. But then only partly, because the same problem
>> pops up when I load 4 years of daily data for just one
>> ticker. Also the problem does not always pop up when I load
>> data for 30 tickers at the same time.
>
> Are you aware of Bloomberg's "Extended Rules and Usage Limits"?
>
>>
>> Maybe it is some kind of function overload? Before I used to
>> connect at the beginning of the script, get some data,
>> process it, output results, get some more data, etc...
>> without disconnecting after each data retreival call. From
>> today I connect-retreive-disconnect-process-connect-...
>
> Did this problem exist when your script was making just one connection
> request? Do you have a good reason for making multiple
> connect/disconnect calls? I'm sure it's possible to torture the
> bloomberg connection control to the point where the API does weird
> things.
>
>> Basically, I am not sure why the problem pops up from time to time.
>
> Without examples, neither are we.
>
>
> Robert
>
>>
>> Kind Regards,
>> Sergey
>>
>>
>>
>>
>>
>> Hi, Sergey,
>> A matrix can only handle 1 kind of data. Everything has to be
>> a numeric, or everything has to be a string. Are you trying
>> to combine different types of data?
>> It might be better to try retval="data.frame".
>> Otherwise, can you provide an example script which reproduces
>> the problem?
>> Regards,
>> Ana
>>
>> On Tue, Jul 7, 2009 at 3:55 PM, Sergey Goriatchev
>> <sergeyg at gmail.com> wrote:
>> > Hello, everyone
>> >
>> > I have this very long script, where I call blpGetData many times (I
>> > get some data, I do some computations, I output results in
>> and Excel
>> > file, I call some other data....)
>> >
>> > As the code grew, I started to get the same warning message
>> more and
>> > more
>> > often:
>> >
>> > "Warning message:
>> > In as.matrix.BlpCOMReturn(x) NAs are introduced by coersion"
>> >
>> > Basically, blpGetData sometimes does not work!
>> >
>> > This message comes up in different parts of the code (in different
>> > calls to blpGetData), and since script is very long and runs
>> > considerable amount of time, one such error completely
>> messes up the
>> > end results.
>> >
>> > Does anyone know why blpGetData sometimes fails to execute?
>> >
>> > Thanks in advance for help!
>> >
>> > Best,
>> > Sergey
>>
>>
>>
>> --
>> I'm not young enough to know everything. /Oscar Wilde
>> Experience is one thing you can't get for nothing. /Oscar
>> Wilde When you are finished changing, you're finished.
>> /Benjamin Franklin Tell me and I forget, teach me and I
>> remember, involve me and I learn.
>> /Benjamin Franklin
>> Luck is where preparation meets opportunity. /George Patten
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>



-- 
I'm not young enough to know everything. /Oscar Wilde
Experience is one thing you can't get for nothing. /Oscar Wilde
When you are finished changing, you're finished. /Benjamin Franklin
Tell me and I forget, teach me and I remember, involve me and I learn.
/Benjamin Franklin
Luck is where preparation meets opportunity. /George Patten


From nelson.ana at gmail.com  Thu Jul  9 11:35:54 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Thu, 9 Jul 2009 10:35:54 +0100
Subject: [R-SIG-Finance] Re[R-sig-finance] uters data
In-Reply-To: <24399329.post@talk.nabble.com>
References: <24399329.post@talk.nabble.com>
Message-ID: <a7d6d2740907090235g49083506p5809a965cc7252f1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090709/7fc41569/attachment.pl>

From enrique.bengoechea at credit-suisse.com  Thu Jul  9 12:25:46 2009
From: enrique.bengoechea at credit-suisse.com (=?iso-8859-1?Q?Bengoechea_Bartolom=E9_Enrique_=28SIES_73=29?=)
Date: Thu, 9 Jul 2009 12:25:46 +0200
Subject: [R-SIG-Finance] RBloomberg warning message
In-Reply-To: <mailman.1.1247133602.27074.r-sig-finance@stat.math.ethz.ch>
Message-ID: <19811401A1D8174CB3EAD7F6072E9B50023F3AF1@chsa1025.share.beluni.net>

Hi,

One thing I do when requesting much data from Bloomberg is to split it into "lots": instead of a big request, several smaller requests that are then aggregated in R. I've found that big requests tend to fail, and that individual requests (say, ticker-per-ticker) tend to be too slow. Somewhere in-between provides the best solution to me. I have this built-in into my own library to access Bloomberg, with user-definable lot size. Don't know whether such a feature is available in RBloomberg.

Enrique

------------------------------

Message: 2
Date: Wed, 8 Jul 2009 13:37:39 +0200
From: Sergey Goriatchev <sergeyg at gmail.com>
Subject: Re: [R-SIG-Finance] RBloomberg warning message
To: nelson.ana at gmail.com, r-sig-finance at stat.math.ethz.ch
Message-ID:
	<7cb007bd0907080437o7d98613cm62f7ab0a059a2c7f at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

Hi, Ana,

Thank you for answering.
In each load I download only numerical data (prices), I am not combining different types of data.
I use retval="zoo", because I find it most useful for my purposes.
Moreover, I've tried "data.frame" before and there were problems with this setting for retval.
I cannot provide an example script, firstly because it is proprietary, secondly, because it is very long, and thirdly, because providing something illustrative is difficult as the problem pops up in different places (I call blpGetData many times in the script and at different places) or not at all sometimes!

Since I load daily data for last 5 years and for up to 30 instruments sometimes, I think the load size may be responsible. But then only partly, because the same problem pops up when I load 4 years of daily data for just one ticker. Also the problem does not always pop up when I load data for 30 tickers at the same time.

Maybe it is some kind of function overload? Before I used to connect at the beginning of the script, get some data, process it, output results, get some more data, etc... without disconnecting after each data retreival call. From today I connect-retreive-disconnect-process-connect-...

Basically, I am not sure why the problem pops up from time to time.

Kind Regards,
Sergey





Hi, Sergey,
A matrix can only handle 1 kind of data. Everything has to be a numeric, or everything has to be a string. Are you trying to combine different types of data?
It might be better to try retval="data.frame".
Otherwise, can you provide an example script which reproduces the problem?
Regards,
Ana

On Tue, Jul 7, 2009 at 3:55 PM, Sergey Goriatchev <sergeyg at gmail.com> wrote:
> Hello, everyone
>
> I have this very long script, where I call blpGetData many times (I 
> get some data, I do some computations, I output results in and Excel 
> file, I call some other data....)
>
> As the code grew, I started to get the same warning message more and 
> more
> often:
>
> "Warning message:
> In as.matrix.BlpCOMReturn(x) NAs are introduced by coersion"
>
> Basically, blpGetData sometimes does not work!
>
> This message comes up in different parts of the code (in different 
> calls to blpGetData), and since script is very long and runs 
> considerable amount of time, one such error completely messes up the 
> end results.
>
> Does anyone know why blpGetData sometimes fails to execute?
>
> Thanks in advance for help!
>
> Best,
> Sergey



--
I'm not young enough to know everything. /Oscar Wilde Experience is one thing you can't get for nothing. /Oscar Wilde When you are finished changing, you're finished. /Benjamin Franklin Tell me and I forget, teach me and I remember, involve me and I learn.
/Benjamin Franklin
Luck is where preparation meets opportunity. /George Patten


From nelson.ana at gmail.com  Thu Jul  9 12:36:10 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Thu, 9 Jul 2009 11:36:10 +0100
Subject: [R-SIG-Finance] RBloomberg warning message
In-Reply-To: <19811401A1D8174CB3EAD7F6072E9B50023F3AF1@chsa1025.share.beluni.net>
References: <mailman.1.1247133602.27074.r-sig-finance@stat.math.ethz.ch>
	<19811401A1D8174CB3EAD7F6072E9B50023F3AF1@chsa1025.share.beluni.net>
Message-ID: <a7d6d2740907090336v5689e74cxfb73990341d28f80@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090709/60afbeb8/attachment.pl>

From sergeyg at gmail.com  Thu Jul  9 12:39:18 2009
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Thu, 9 Jul 2009 12:39:18 +0200
Subject: [R-SIG-Finance] RBloomberg warning message
In-Reply-To: <19811401A1D8174CB3EAD7F6072E9B50023F3AF1@chsa1025.share.beluni.net>
References: <mailman.1.1247133602.27074.r-sig-finance@stat.math.ethz.ch>
	<19811401A1D8174CB3EAD7F6072E9B50023F3AF1@chsa1025.share.beluni.net>
Message-ID: <7cb007bd0907090339i605afa87kabe679214d3f1a55@mail.gmail.com>

Hi, Bartolom?,

Yes, that is another solution. I actually use this solution in another
script, setting maximum number of tickers to 20 ( I sequence length of
ticker vector by 20 and use that). 20 seems to work 99% of the time
for 5-years of weekly/daily data.

Kind Regards,
Sergey

On Thu, Jul 9, 2009 at 12:25, Bengoechea Bartolom? Enrique (SIES
73)<enrique.bengoechea at credit-suisse.com> wrote:
> Hi,
>
> One thing I do when requesting much data from Bloomberg is to split it into "lots": instead of a big request, several smaller requests that are then aggregated in R. I've found that big requests tend to fail, and that individual requests (say, ticker-per-ticker) tend to be too slow. Somewhere in-between provides the best solution to me. I have this built-in into my own library to access Bloomberg, with user-definable lot size. Don't know whether such a feature is available in RBloomberg.
>
> Enrique
>
> ------------------------------
>
> Message: 2
> Date: Wed, 8 Jul 2009 13:37:39 +0200
> From: Sergey Goriatchev <sergeyg at gmail.com>
> Subject: Re: [R-SIG-Finance] RBloomberg warning message
> To: nelson.ana at gmail.com, r-sig-finance at stat.math.ethz.ch
> Message-ID:
> ? ? ? ?<7cb007bd0907080437o7d98613cm62f7ab0a059a2c7f at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
>
> Hi, Ana,
>
> Thank you for answering.
> In each load I download only numerical data (prices), I am not combining different types of data.
> I use retval="zoo", because I find it most useful for my purposes.
> Moreover, I've tried "data.frame" before and there were problems with this setting for retval.
> I cannot provide an example script, firstly because it is proprietary, secondly, because it is very long, and thirdly, because providing something illustrative is difficult as the problem pops up in different places (I call blpGetData many times in the script and at different places) or not at all sometimes!
>
> Since I load daily data for last 5 years and for up to 30 instruments sometimes, I think the load size may be responsible. But then only partly, because the same problem pops up when I load 4 years of daily data for just one ticker. Also the problem does not always pop up when I load data for 30 tickers at the same time.
>
> Maybe it is some kind of function overload? Before I used to connect at the beginning of the script, get some data, process it, output results, get some more data, etc... without disconnecting after each data retreival call. From today I connect-retreive-disconnect-process-connect-...
>
> Basically, I am not sure why the problem pops up from time to time.
>
> Kind Regards,
> Sergey
>
>
>
>
>
> Hi, Sergey,
> A matrix can only handle 1 kind of data. Everything has to be a numeric, or everything has to be a string. Are you trying to combine different types of data?
> It might be better to try retval="data.frame".
> Otherwise, can you provide an example script which reproduces the problem?
> Regards,
> Ana
>
> On Tue, Jul 7, 2009 at 3:55 PM, Sergey Goriatchev <sergeyg at gmail.com> wrote:
>> Hello, everyone
>>
>> I have this very long script, where I call blpGetData many times (I
>> get some data, I do some computations, I output results in and Excel
>> file, I call some other data....)
>>
>> As the code grew, I started to get the same warning message more and
>> more
>> often:
>>
>> "Warning message:
>> In as.matrix.BlpCOMReturn(x) NAs are introduced by coersion"
>>
>> Basically, blpGetData sometimes does not work!
>>
>> This message comes up in different parts of the code (in different
>> calls to blpGetData), and since script is very long and runs
>> considerable amount of time, one such error completely messes up the
>> end results.
>>
>> Does anyone know why blpGetData sometimes fails to execute?
>>
>> Thanks in advance for help!
>>
>> Best,
>> Sergey
>
>
>
> --
> I'm not young enough to know everything. /Oscar Wilde Experience is one thing you can't get for nothing. /Oscar Wilde When you are finished changing, you're finished. /Benjamin Franklin Tell me and I forget, teach me and I remember, involve me and I learn.
> /Benjamin Franklin
> Luck is where preparation meets opportunity. /George Patten
>
>
>
> ------------------------------
>



-- 
I'm not young enough to know everything. /Oscar Wilde
Experience is one thing you can't get for nothing. /Oscar Wilde
When you are finished changing, you're finished. /Benjamin Franklin
Tell me and I forget, teach me and I remember, involve me and I learn.
/Benjamin Franklin
Luck is where preparation meets opportunity. /George Patten


From stvienna at gmail.com  Thu Jul  9 17:00:12 2009
From: stvienna at gmail.com (stvienna wiener)
Date: Thu, 9 Jul 2009 17:00:12 +0200
Subject: [R-SIG-Finance] financial series: waveslim, brainwaver,
	wavetresh and fractal
Message-ID: <b9bcd4e10907090800v1bcd0d3cr5ad68a041755b3fd@mail.gmail.com>

Greetings to all!


# Short-Version
I am looking for advanced R packages similar to "waveslim",
"brainwaver", "wavethresh", "fractal", etc.
in order to analyze financial times series (e.g. stock prices).


Currently I am writing code to show/demonstrate the use of wavelets
and fractal dimension
to analyze financial time series (its a school project for my master
degree in CS).

The purpose is, for example, to find a correlation between IBM stock
prices and the dow jones index. Or to find a changepoint in
stock prices, correlating to unusual events e.g Yahoo stock prices in 2008 while
there was an acquisition attempt.

I have been using the "brainwaver" package to calculate correlations
of us stock prices to the dow jones.
It seems to work, but I am not 100% sure. (could provide so code but
its just fragments at the moment)

So my questions at the moment would be:
1)? Is there a specific R package that would be interesting and
helpful that I missed?
2)? Is there a package to find "changepoints" in time series with
wavelets or fractal-approach?
I know there is a function in "waveslim" to find a changepoint in the
wavelet-variance,
but I could not yet get it working.
If not is there a way by other methods e.g. bayesian interference?


As references I was looking at the task views of finance, econometrics
and time series.

As well as the following to very helpful books (aside of the standard
wavelet literature):

(2001) An Introduction to Wavelets and Other Filtering Methods in
Finance and Economics.
Wavelet Methods in Statistics with R [ would be looking for a newer
reference ... .. ]
(2008) Wavelet Methods in Statistics with R


THANK you very much for any help or pointers!!


With the best regards to all,
Steve


From hammeda at ffc.co.za  Thu Jul  9 19:14:40 2009
From: hammeda at ffc.co.za (Hammed)
Date: Thu, 9 Jul 2009 10:14:40 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Normalization and Cointegrating
 Vectors from VECM analysis
Message-ID: <24414323.post@talk.nabble.com>


On page 24 of Bernhard Pfaff's  "VAR, SVAR and SVEC Models: Implementation
Within R Package vars" the analysis noted the existence of one cointegration
relationship. Subsequently a VECM with the restriction that r=1 and a
normalization of the long-run relationship with respect to total wages was
re-estimated. This was done via

R>vecm <-cajo(Canada[, c("rw", "prod", "e", "U"], type="trace",
ecdet="trend", K=3, spec="transitory")
R> vecmR1 <-cajorls(vecm, r=1). 

Here are the questions I have:

1. Does placing "rw" first signal that it is the variable on which the VECM
estimation will be normalized? If  not, how is normalization conducted?

2. How does one go about obtaining the t-statistics (and standard errors) of
the alpha and beta parameters as provided in Table 5? I have searched the
net and noted that other users have had similar issues in obtaining
something similar to Table 5 when one gets results such attached below

 

>Coefficients:
  LY.d        LPVI.d      LGC.d       LPI.d     

ect1      -1.281e-01   2.857e-01  -7.791e-01   1.412e+00

> 

>constant   1.585e+02  -3.532e+02   9.635e+02  -1.746e+03

> 

>LY.dl1     7.591e-01   3.701e-01   1.370e+00  -4.387e-01

> 

>LPVI.dl1   8.272e-02   4.471e-01   2.071e-01   6.531e-02

> 

>LGC.dl1    2.629e-02   2.016e-01  -6.392e-06   3.304e-02

> 

>LPI.dl1   -6.621e-03  -1.903e-02   6.491e-03   3.498e-01

> 

> 

> 

>$beta

> 

>                ect1

> 

>LY.l1     1.00000000

> 

>LPVI.l1  -0.29140737

> 

>LGC.l1    0.20201033

> 

>LPI.l1   -0.02771818

> 

>trend.l1 -0.56077672

> 

> 

> 

>> summary(vecm.r1$rlm)

> 

>Response LY.d :

> 

> 

> 

>Call:

> 

>lm(formula = substitute(LY.d), data = data.mat)

> 

> 

> 

>Residuals:

> 

>      Min        1Q    Median        3Q       Max 

> 

>-1.556232 -0.212279 -0.006118  0.281033  1.025720 

> 

> 

> 

>Coefficients:

> 

>           Estimate Std. Error t value Pr(>|t|)    

> 

>ect1      -0.128062   0.082565  -1.551    0.127    

> 

>constant 158.511805 102.096882   1.553    0.127    

> 

>LY.dl1     0.759144   0.161426   4.703 1.93e-05 ***

> 

>LPVI.dl1   0.082721   0.060251   1.373    0.176    

> 

>LGC.dl1    0.026292   0.047268   0.556    0.580    

> 

>LPI.dl1   -0.006621   0.013322  -0.497    0.621    

> 

>---

> 

>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

> 

> 

> 

>Residual standard error: 0.4673 on 52 degrees of freedom

> 

>Multiple R-squared: 0.807,      Adjusted R-squared: 0.7847 

> 

>F-statistic: 36.23 on 6 and 52 DF,  p-value: < 2.2e-16 

> 

> 

> 

>Response LPVI.d :

> 

> 

> 

>Call:

> 

>lm(formula = substitute(LPVI.d), data = data.mat)

> 

> 

> 

>Residuals:

> 

>    Min      1Q  Median      3Q     Max 

> 

>-1.8874 -0.5372 -0.0947  0.4634  2.9403 

> 

> 

> 

>Coefficients:

> 

>           Estimate Std. Error t value Pr(>|t|)    

> 

>ect1        0.28566    0.15822   1.806 0.076780 .  

> 

>constant -353.15002  195.64231  -1.805 0.076853 .  

> 

>LY.dl1      0.37008    0.30933   1.196 0.236966    

> 

>LPVI.dl1    0.44706    0.11546   3.872 0.000303 ***

> 

>LGC.dl1     0.20157    0.09058   2.225 0.030415 *  

> 

>LPI.dl1    -0.01903    0.02553  -0.745 0.459427    

> 

>---

> 

>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

> 

> 

> 

>Residual standard error: 0.8955 on 52 degrees of freedom

> 

>Multiple R-squared: 0.8414,     Adjusted R-squared: 0.8231 

> 

>F-statistic: 45.97 on 6 and 52 DF,  p-value: < 2.2e-16 

> 

> 

> 

> 

> 

>Response LGC.d :

> 

> 

> 

>Call:

> 

>lm(formula = substitute(LGC.d), data = data.mat)

> 

> 

> 

>Residuals:

> 

>    Min      1Q  Median      3Q     Max 

> 

>-2.4355 -0.8068 -0.1286  0.9971  2.3484 

> 

> 

> 

>Coefficients:

> 

>           Estimate Std. Error   t value Pr(>|t|)    

> 

>ect1     -7.791e-01  2.163e-01    -3.601 0.000707 ***

> 

>constant  9.635e+02  2.675e+02     3.602 0.000706 ***

> 

>LY.dl1    1.370e+00  4.230e-01     3.240 0.002088 ** 

> 

>LPVI.dl1  2.071e-01  1.579e-01     1.312 0.195284    

> 

>LGC.dl1  -6.392e-06  1.238e-01 -5.16e-05 0.999959    

> 

>LPI.dl1   6.491e-03  3.491e-02     0.186 0.853192    

> 

>---

> 

>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

> 

> 

> 

>Residual standard error: 1.224 on 52 degrees of freedom

> 

>Multiple R-squared: 0.4406,     Adjusted R-squared: 0.3761 

> 

>F-statistic: 6.827 on 6 and 52 DF,  p-value: 2.219e-05 

> 

> 

> 

> 

> 

>Response LPI.d :

> 

> 

> 

>Call:

> 

>lm(formula = substitute(LPI.d), data = data.mat)

> 

> 

> 

>Residuals:

> 

>     Min       1Q   Median       3Q      Max 

> 

>-17.2248  -1.7118  -0.1967   2.2343   8.0607 

> 

> 

> 

>Coefficients:

> 

>           Estimate Std. Error t value Pr(>|t|)   

> 

>ect1      1.412e+00  7.243e-01   1.950  0.05661 . 

> 

>constant -1.746e+03  8.956e+02  -1.950  0.05660 . 

> 

>LY.dl1   -4.386e-01  1.416e+00  -0.310  0.75798   

> 

>LPVI.dl1  6.531e-02  5.285e-01   0.124  0.90214   

> 

>LGC.dl1   3.304e-02  4.147e-01   0.080  0.93680   

> 

>LPI.dl1   3.498e-01  1.169e-01   2.993  0.00421 **

> 

>---

> 

>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

> 

> 

> 

>Residual standard error: 4.1 on 52 degrees of freedom

> 

>Multiple R-squared: 0.3707,     Adjusted R-squared: 0.2981 

> 

>F-statistic: 5.105 on 6 and 52 DF,  p-value: 0.0003471

>
-- 
View this message in context: http://www.nabble.com/Normalization-and-Cointegrating-Vectors-from-VECM-analysis-tp24414323p24414323.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From nodecorum at yahoo.com  Thu Jul  9 22:30:51 2009
From: nodecorum at yahoo.com (tradenet)
Date: Thu, 9 Jul 2009 13:30:51 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] rmetrics portfolio backtesting
	limitations question
Message-ID: <24417340.post@talk.nabble.com>


I have been working with the portfolioBacktesting function in Rmetrics and it
seems to be a very powerful and useful function.  There seems to be some
limitations/bugs that seriously limits the utility of the function and I was
hoping someone has found workarounds:

1.) If I specify a large number of assets, say 50, in the formula, e.g. SPX
~ asset1 + asset2....+asset50
then it seems only the first 29 assets are used in the analysis. 
ncol(backtestPortfolios$weights) always returns 29 and assets near the end
of the list get nonzero weights if I move their names to the beginning of
the formula string, otherwise those assets do not appear in the results.  Is
there a way to NOT compare results to a benchmark?  I'd like to just supply
a timeseries of asset returns for a date range and run the analysis using
all assets in the timeseries as portfolio candidates.

BTW, if anyone knows an easy way to dump out the weights to a file I'd
appreciate the insight

2.) the 12m window horizon seems to be the only one that works:
setWindowsHorizon(backtestBT)<-"12m" works, but <-"3m" yields an error
message.

I thought this might have to do with the lambda smoothing, so I set the
smoothing to "1m" and still no go

3.) does anyone know how to turn smoothing completely off?  I would like to
see the raw, unsmoothed weights

4.) is there anyway to plot the results without performing weight smoothing
-- I don't want to have to do smoothing to look at all the great plots and
results.

Rmetrics is great stuff, so close to being beyond perfect!  I am very
grateful to the developers and the community.

Warm regards,

Andrew
-- 
View this message in context: http://www.nabble.com/rmetrics-portfolio-backtesting-limitations-question-tp24417340p24417340.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From nodecorum at yahoo.com  Fri Jul 10 16:16:15 2009
From: nodecorum at yahoo.com (tradenet)
Date: Fri, 10 Jul 2009 07:16:15 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] rmetrics portfolio backtesting
 limitations question
In-Reply-To: <24417340.post@talk.nabble.com>
References: <24417340.post@talk.nabble.com>
Message-ID: <24428274.post@talk.nabble.com>


I found the problem with the truncation of the asset list -- R only allows
500 characters in a formula.

My next step is to copy, rename and modify the original portfolioBacktest
function to add another parameter, a vector of strings which are the asset
names.  Can I just execute the code for the new function in the GUI and
thereby make it available to my script?

If I can figure out how to do that, perhaps I can fix the windowing function
to allow shorter window sizes and no smoothing of weights (our strategies
are very short term so rebalancing smoothness is not an issue).

Thank you,

Andrew




tradenet wrote:
> 
> I have been working with the portfolioBacktesting function in Rmetrics and
> it seems to be a very powerful and useful function.  There seems to be
> some limitations/bugs that seriously limits the utility of the function
> and I was hoping someone has found workarounds:
> 
> 1.) If I specify a large number of assets, say 50, in the formula, e.g.
> SPX ~ asset1 + asset2....+asset50
> then it seems only the first 29 assets are used in the analysis. 
> ncol(backtestPortfolios$weights) always returns 29 and assets near the end
> of the list get nonzero weights if I move their names to the beginning of
> the formula string, otherwise those assets do not appear in the results. 
> Is there a way to NOT compare results to a benchmark?  I'd like to just
> supply a timeseries of asset returns for a date range and run the analysis
> using all assets in the timeseries as portfolio candidates.
> 
> BTW, if anyone knows an easy way to dump out the weights to a file I'd
> appreciate the insight
> 
> 2.) the 12m window horizon seems to be the only one that works:
> setWindowsHorizon(backtestBT)<-"12m" works, but <-"3m" yields an error
> message.
> 
> I thought this might have to do with the lambda smoothing, so I set the
> smoothing to "1m" and still no go
> 
> 3.) does anyone know how to turn smoothing completely off?  I would like
> to see the raw, unsmoothed weights
> 
> 4.) is there anyway to plot the results without performing weight
> smoothing -- I don't want to have to do smoothing to look at all the great
> plots and results.
> 
> Rmetrics is great stuff, so close to being beyond perfect!  I am very
> grateful to the developers and the community.
> 
> Warm regards,
> 
> Andrew
> 

-- 
View this message in context: http://www.nabble.com/rmetrics-portfolio-backtesting-limitations-question-tp24417340p24428274.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From guy.yollin at rotellacapital.com  Fri Jul 10 18:41:04 2009
From: guy.yollin at rotellacapital.com (Guy Yollin)
Date: Fri, 10 Jul 2009 11:41:04 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Normalization and Cointegrating
 Vectors from VECM analysis
In-Reply-To: <24414323.post@talk.nabble.com>
References: <24414323.post@talk.nabble.com>
Message-ID: <E4259A82356E7F46B4F911FB27B0D72517D1B463B5@AUSP01VMBX02.collaborationhost.net>

Hammed,

Pfaff presents the details of the SVEC model in section 8.3 of his text "Analysis of Integrated and Cointegrated Time Series with R".

All of the scripts used in the text are available from his website:

http://www.pfaffikus.de/ex2.html

Rcode-8-8.R through Rcode-8-17.R perform the analysis of the Canada dataset

Specifically, Rcode-8-13.R performs the calculation of t-stats and SEs for alpha and beta.

Best,

-- G


From BChiquoine at tiff.org  Fri Jul 10 19:31:09 2009
From: BChiquoine at tiff.org (Chiquoine, Ben)
Date: Fri, 10 Jul 2009 13:31:09 -0400
Subject: [R-SIG-Finance] PDF Reader
Message-ID: <E71E6D5B2274B341B26B6DF3D34D8028014F0DAB@vsw3exch2.tiff.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090710/7805a081/attachment.pl>

From adrian_d at eskimo.com  Fri Jul 10 19:51:22 2009
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Fri, 10 Jul 2009 10:51:22 -0700 (PDT)
Subject: [R-SIG-Finance] PDF Reader
In-Reply-To: <E71E6D5B2274B341B26B6DF3D34D8028014F0DAB@vsw3exch2.tiff.local>
References: <E71E6D5B2274B341B26B6DF3D34D8028014F0DAB@vsw3exch2.tiff.local>
Message-ID: <Pine.LNX.4.64.0907101048420.1125@shell.eskimo.com>


You can use pdftotext to convert to text files and then parse your text 
files. http://en.wikipedia.org/wiki/Pdftotext

It works well in simple cases.

I have a wrapper something like this:

read.pdf <- function(filein, fileout=NULL, layout=TRUE, first=NULL,
                      last=NULL, eol=NULL, opw=NULL, upw=NULL)
{
   cmd <- "the/path/to/pdftotext "

   if (layout) cmd <- paste(cmd, "-layout")
   if (!is.null(first)) cmd <- paste(cmd, "-f", first)
   if (!is.null(last))  cmd <- paste(cmd, "-l", last)
   if (!is.null(eol))   cmd <- paste(cmd, "-l", eol)
   if (!is.null(opw))   cmd <- paste(cmd, "-l", opw)
   if (!is.null(upw))   cmd <- paste(cmd, "-l", upw)

   if (!file.exists(filein)){
     stop(paste("Cannot find file: ", filein))
   } else {
     cmd <- paste(cmd, shQuote(filein))
   }

   if (is.null(fileout)){
     cmd <- paste(cmd, "-")
     res <- system(cmd, intern=TRUE)
   } else {
     cmd <- paste(cmd, fileout)
     res <- system(cmd)
     if (res != 0)
       stop("Pdf conversion to txt failed.\n")
   }

   return(res)
}

Adrian



On Fri, 10 Jul 2009, Chiquoine, Ben wrote:

> Hi,
>
>
>
> First let me appoligize if this is the wrong venue for this question...
>
> I work for a small financial company and we often receive statements
> that are in pdf form.  Pulling the data from these can be quite time
> consuming and I'm wondering if anyone on the list knows of a way to read
> a pdf in as text in R.  I know that google has come out with a few tools
> that allow you to search the text of pdfs which has given me hope that
> something along these lines may be possible but I've been unable to find
> any R documentation on inputing data from PDFs.  Any
> thoughts/suggestions would be much appreciated.
>
>
>
> Thanks,
>
>
>
> Ben
>
>
> ___________________________________________
> This message and any attached documents contain
> information which may be confidential, subject to
> privilege or exempt from disclosure under applicable
> law. These materials are solely for the use of the
> intended recipient. If you are not the intended
> recipient of this transmission, you are hereby
> notified that any distribution, disclosure, printing,
> copying, storage, modification or the taking of any
> action in reliance upon this transmission is strictly
> prohibited. Delivery of this message to any person
> other than the intended recipient shall not
> compromise or waive such confidentiality, privilege
> or exemption from disclosure as to this
> communication.
>
> If you have received this communication in error,
> please notify the sender immediately and delete
> this message from your system.
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From brian at braverock.com  Fri Jul 10 19:57:24 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 10 Jul 2009 12:57:24 -0500
Subject: [R-SIG-Finance] PDF Reader
In-Reply-To: <E71E6D5B2274B341B26B6DF3D34D8028014F0DAB@vsw3exch2.tiff.local>
References: <E71E6D5B2274B341B26B6DF3D34D8028014F0DAB@vsw3exch2.tiff.local>
Message-ID: <4A578104.5040503@braverock.com>

Ben,

I wouldn't really consider this the appropriate forum for your query, 
but I'll answer it anyway, with emphasis on the finance-specific bits.

There has existed for many years a utility called "pdf2txt".  Note that 
this will extract text from a pdf, but may not do a great job with 
maintaining the column structure.  In the past, I have had to resort to 
perl, php, or python to use  regular expression matching to put the data 
into a tabular format that would be suitable for analysis in R or some 
other processing environment.

Also, most fund managers, trustees, administrators, markets, brokerages, 
etc do have better data formats available for their investors/clients.  
Call them up and tell them that you need the data in machine-readable 
form, whether CSV, fixed width, Excel, whatever.  Almost all of your 
sources should be able to provide this, though it may take some work.  
You may not get to choose the format, but any machine-readable format 
should be coercible into R or other analysis environments.

Regards,

  - Brian

Chiquoine, Ben wrote:
> Hi,
>
>  
>
> First let me appoligize if this is the wrong venue for this question...
>
> I work for a small financial company and we often receive statements
> that are in pdf form.  Pulling the data from these can be quite time
> consuming and I'm wondering if anyone on the list knows of a way to read
> a pdf in as text in R.  I know that google has come out with a few tools
> that allow you to search the text of pdfs which has given me hope that
> something along these lines may be possible but I've been unable to find
> any R documentation on inputing data from PDFs.  Any
> thoughts/suggestions would be much appreciated.
>   
-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From BChiquoine at tiff.org  Fri Jul 10 20:14:32 2009
From: BChiquoine at tiff.org (Chiquoine, Ben)
Date: Fri, 10 Jul 2009 14:14:32 -0400
Subject: [R-SIG-Finance] PDF Reader
In-Reply-To: <4A578104.5040503@braverock.com>
References: <E71E6D5B2274B341B26B6DF3D34D8028014F0DAB@vsw3exch2.tiff.local>
	<4A578104.5040503@braverock.com>
Message-ID: <E71E6D5B2274B341B26B6DF3D34D8028014F0DD4@vsw3exch2.tiff.local>

Thanks Brian and Adrian for your helpful suggestions.  pdf2txt looks
like it might do the trick (especially with that great wrapper you put
on in adrian).  I've found many hedge fund managers reluctant to give
data out in forms other then pdf because they feel PDFs help them to
prevent redistribution... maybe I should be pushing harder.

Thanks again,

Ben

-----Original Message-----
From: Brian G. Peterson [mailto:brian at braverock.com] 
Sent: Friday, July 10, 2009 1:57 PM
To: Chiquoine, Ben
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] PDF Reader

Ben,

I wouldn't really consider this the appropriate forum for your query, 
but I'll answer it anyway, with emphasis on the finance-specific bits.

There has existed for many years a utility called "pdf2txt".  Note that 
this will extract text from a pdf, but may not do a great job with 
maintaining the column structure.  In the past, I have had to resort to 
perl, php, or python to use  regular expression matching to put the data

into a tabular format that would be suitable for analysis in R or some 
other processing environment.

Also, most fund managers, trustees, administrators, markets, brokerages,

etc do have better data formats available for their investors/clients.  
Call them up and tell them that you need the data in machine-readable 
form, whether CSV, fixed width, Excel, whatever.  Almost all of your 
sources should be able to provide this, though it may take some work.  
You may not get to choose the format, but any machine-readable format 
should be coercible into R or other analysis environments.

Regards,

  - Brian

Chiquoine, Ben wrote:
> Hi,
>
>  
>
> First let me appoligize if this is the wrong venue for this
question...
>
> I work for a small financial company and we often receive statements
> that are in pdf form.  Pulling the data from these can be quite time
> consuming and I'm wondering if anyone on the list knows of a way to
read
> a pdf in as text in R.  I know that google has come out with a few
tools
> that allow you to search the text of pdfs which has given me hope that
> something along these lines may be possible but I've been unable to
find
> any R documentation on inputing data from PDFs.  Any
> thoughts/suggestions would be much appreciated.
>   
-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock

___________________________________________
This message and any attached documents contain
information which may be confidential, subject to 
privilege or exempt from disclosure under applicable
law. These materials are solely for the use of the 
intended recipient. If you are not the intended 
recipient of this transmission, you are hereby 
notified that any distribution, disclosure, printing, 
copying, storage, modification or the taking of any
action in reliance upon this transmission is strictly
prohibited. Delivery of this message to any person
other than the intended recipient shall not
compromise or waive such confidentiality, privilege
or exemption from disclosure as to this 
communication. 

If you have received this communication in error, 
please notify the sender immediately and delete
this message from your system. 



From doi.murilo at gmail.com  Fri Jul 10 22:21:33 2009
From: doi.murilo at gmail.com (Murilo Eiji Doi)
Date: Fri, 10 Jul 2009 13:21:33 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Extracting AIC or
 Log-Likelihood from a fitted GARCH
Message-ID: <24361175.post@talk.nabble.com>


Hi, 

to extract AIC , BIC or Log-likelihood of model 

fitmodel <- garchFit(~arma(3,3)+garch(1,1), data)
fitmodel at fit$ics   #is a vector with Information Criterion Statistics
> fitmodel at fit$ics[1]  #is AIC 
> fitmodel at fit$ics[2]  #is BIC 
> fitmodel at fit$value  #is Log-likelihood

Murilo Eiji Doi


-----

Murilo Eiji Doi
http://twitter.com/murilodoi
http://pt.beezzer.com/rproject (help do R-project em portugu?s)

-- 
View this message in context: http://www.nabble.com/Extracting-AIC-or-Log-Likelihood-from-a-fitted-GARCH-tp22934266p24361175.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From spencer.graves at prodsyse.com  Sat Jul 11 04:55:07 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Fri, 10 Jul 2009 19:55:07 -0700
Subject: [R-SIG-Finance] Coercion problem in RBloomberg
In-Reply-To: <BF4613BA62AAD6459F2C1FB9FF9C1EDB01D8EF5A@orange.davis.local>
References: <BF4613BA62AAD6459F2C1FB9FF9C1EDB01D8EF5A@orange.davis.local>
Message-ID: <4A57FF0B.1000009@prodsyse.com>

      I have not seen a reply to this, so I will offer a comment.  I do 
not have access to Bloomberg, so I can not recreate the problem.  
However, I can suggest two things: 


      1.  Have you tried "debug(blpGetData)"?  When you next execute a 
call to "blpGetData", this will put you in the environment of the 
function, allowing you to walk through the function line by line, 
looking at and changing things as you wish.  To support this, I suggest 
you download the package source file, "RBloomberg_0.1-11.tar.gz", from 
"http://cran.fhcrc.org/web/packages/RBloomberg/index.html".  This is 
often useful, because any comments in the source code are deleted in the 
copy that is attached with "library(RBloomberg)". 


      2.  Have you considered setting "options(warn=2)"?  This will turn 
warnings into errors, making it easier to find the problem that 
generated the warning message. 


      Hope this helps. 
      Spencer Graves


Keith Sabol wrote:
> Essentially, I'm trying to re-create the data displayed Bloomberg's QRMD
> function.  When I run the following I get the error below:
>
>  
>
> blpGetData(conn,c("X 4 Corp"), c("VOLUME","LAST_PRICE"),
> ,start=as.chron(14432.375),end=as.chron(14433.55), barsize=0)
>
>                     VOLUME LAST_PRICE
>
> (07/08/09 10:15:01) 150000 150000.000
>
> (07/08/09 11:05:01) 350000    123.026
>
> Warning message:
>
> In matrix(vec, nrow = nr, ncol = nc) :
>
>   data length [11] is not a sub-multiple or multiple of the number of
> rows [2]
>
>  
>
>  
>
> When I run the data request to return the "raw" data I get:
>
>  
>
>  
>
> blpGetData(conn,"X 4 Corp", c("VOLUME","LAST_PRICE"),
> ,start=as.chron(14432.375),end=as.chron(14433.55),
> barsize=0,retval="raw")
>
> [[1]]
>
> [[1]][[1]]
>
> [[1]][[1]][[1]]
>
> An object of class "COMDate"
>
> [1] 40001.64
>
>  
>
> [[1]][[1]][[2]]
>
> An object of class "COMDate"
>
> [1] 40002.42
>
>  
>
> [[1]][[1]][[3]]
>
> An object of class "COMDate"
>
> [1] 40002.43
>
>  
>
> [[1]][[1]][[4]]
>
> An object of class "COMDate"
>
> [1] 40002.46
>
>  
>
>  
>
>  
>
> [[2]]
>
> [[2]][[1]]
>
> [[2]][[1]][[1]]
>
> NULL
>
>  
>
> [[2]][[1]][[2]]
>
> [1] 150000
>
>  
>
> [[2]][[1]][[3]]
>
> [1] 350000
>
>  
>
> [[2]][[1]][[4]]
>
> [1] 150000
>
>  
>
>  
>
>  
>
> [[3]]
>
> [[3]][[1]]
>
> [[3]][[1]][[1]]
>
> [1] 123.026
>
>  
>
> [[3]][[1]][[2]]
>
> [1] 118.107
>
>  
>
> [[3]][[1]][[3]]
>
> [1] 118.475
>
>  
>
> [[3]][[1]][[4]]
>
> [1] 118.75
>
>  
>
>  
>
> Coercions to "zoo" or "matrix" fail because of the NULL that is
> returned. Does anyone know how to get around this?
>
>  
>
> Thanks in advance!
>
>  
>
>  
>
>  
>
> Keith Sabol
>
>  
>
> Email Disclosure Notice
> This message (including any attachments) is for the addr...{{dropped:12}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From malcolm.croucher at gmail.com  Sat Jul 11 10:40:48 2009
From: malcolm.croucher at gmail.com (malcolm Crouch)
Date: Sat, 11 Jul 2009 10:40:48 +0200
Subject: [R-SIG-Finance] Finance Data
In-Reply-To: <e8e755250906151152x4bad25ffj8f3117be444d358d@mail.gmail.com>
References: <386fa5610906151130p1e4a8abdgd1b6e6efd2ebd54a@mail.gmail.com>
	<e8e755250906151152x4bad25ffj8f3117be444d358d@mail.gmail.com>
Message-ID: <386fa5610907110140n3a099e1cxe2bf7e2610501b4c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090711/e6f0a6c4/attachment.pl>

From icos.atropa at gmail.com  Sat Jul 11 11:29:31 2009
From: icos.atropa at gmail.com (Christian Gunning)
Date: Sat, 11 Jul 2009 02:29:31 -0700
Subject: [R-SIG-Finance] Fwd: [Plr-general] request for input -- good
	example needed for PGDay lightning-talk
In-Reply-To: <4A4B81B2.7010009@joeconway.com>
References: <4A4B81B2.7010009@joeconway.com>
Message-ID: <681d07c20907110229q4577d8aeu546138c9fd423a89@mail.gmail.com>

PLR is a great tool, and Joe's a great developer. If anyone has a good
postgres example sitting on the shelf, i'm sure he'd be happy to hear
of it.

-christian
university of new mexico


---------- Forwarded message ----------
From: Joe Conway <mail at joeconway.com>
Date: Wed, Jul 1, 2009 at 8:33 AM
Subject: [Plr-general] request for input -- good example needed for
PGDay lightning-talk
To: PLR MailList <plr-general at pgfoundry.org>


Hi,

On 19 July I'm scheduled to give a 5 minute "lightning talk" at PGDay
in San Jose. My intent is to quickly cover:

- What is R, what is PL/R? (1 slide, 1 minute)
- What are the newest features (1 slide, 1 minute)
? ? ? ?- Added RPostgreSQL compatibility functions
? ? ? ?- Added ability to send serialized R objects to Postgres as
? ? ? ? ?bytea return values
? ? ? ?- Added ability to convert bytea arguments from Postgres back
? ? ? ? ?into the original R object
? ? ? ?- Added function to unserialize bytea value in order to restore
? ? ? ? ?object outside of R (useful for image data)
- Quick pre-staged demo (3 minutes)

For the demo, it would be nice to have a compelling real-life example
(even if the actual data is anonymized in some way). Here is what I
had in mind:

1. Show R script that:
?- uses RPostgreSQL to select data out of PostgreSQL tables
?- does some intermediate calculation to produce a dataframe
?- does a final calculation
?- uses the final data to produce a chart, output as a file
2. Cut-and-paste that R script into a PL/R function, show that it works
? unmodified
3. Show a modified PL/R function (or possibly split into two functions) that:
?- stores the intermediate dataframe in a PostgreSQL table
?- retrieves and recreates the dataframe
?- does the final calculation
?- uses the final data to produce a chart, streamed back to PostgreSQL
? as bytea data
4. Show a PHP page that runs the PL/R and displays the chart

Perhaps ambitious for a 5 minute talk, but what the heck ;-)

The help I'm asking for is the compelling example, complete with data
if possible. I will modify the R script to use PostgreSQL as required.
Does anyone have something they can share?

Thanks,

Joe
_______________________________________________
Plr-general mailing list
Plr-general at pgfoundry.org
http://pgfoundry.org/mailman/listinfo/plr-general


From josh.m.ulrich at gmail.com  Sat Jul 11 13:10:21 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Sat, 11 Jul 2009 06:10:21 -0500
Subject: [R-SIG-Finance] Finance Data
In-Reply-To: <386fa5610907110140n3a099e1cxe2bf7e2610501b4c@mail.gmail.com>
References: <386fa5610906151130p1e4a8abdgd1b6e6efd2ebd54a@mail.gmail.com> 
	<e8e755250906151152x4bad25ffj8f3117be444d358d@mail.gmail.com> 
	<386fa5610907110140n3a099e1cxe2bf7e2610501b4c@mail.gmail.com>
Message-ID: <8cca69990907110410i2537680ek7e42ebee7c5879f@mail.gmail.com>

Malcolm,

What's quoted in the press is usually the Blue Chip Forecasts:
http://www.aspenpublishers.com/blue-chip-publications.htm

Best,
Josh
--
http://www.fosstrading.com



On Sat, Jul 11, 2009 at 3:40 AM, malcolm
Crouch<malcolm.croucher at gmail.com> wrote:
> Hi ,
>
> Thanks for this , this has been a great help . I found exactly what I am
> looking for .
> However I was wondering the following : I notice in the financial press that
> with each figure there is an estimate attached :
>
> e.g Unemployment expected 9.45 currently at 9.4 .
>
> Does ?anyone know of a reliable place to get estimates ?
>
> Regards
>
> Malcolm
>
>
> On Mon, Jun 15, 2009 at 8:52 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>
>> Try FRED
>>
>> http://research.stlouisfed.org/fred2/search/vehicle/1
>> http://research.stlouisfed.org/fred2/search/retail+sales/1
>>
>> library(quantmod)
>> > getSymbols("RETAIL", src="FRED")
>> [1] "RETAIL"
>> > head(RETAIL)
>> ? ? ? ? ? RETAIL
>> 1947-01-01 ? 9583
>> 1947-02-01 ? 9852
>> 1947-03-01 ? 9769
>> 1947-04-01 ? 9947
>> 1947-05-01 ?10061
>> 1947-06-01 ?10146
>> >
>>
>> HTH
>> Jeff
>>
>> On Mon, Jun 15, 2009 at 1:30 PM, malcolm
>> Crouch<malcolm.croucher at gmail.com> wrote:
>> > Hi ,
>> >
>> > I am looking for US GDP Figures , non-pharm Payrolls , retail sales ,
>> motor
>> > vehicle sales ect.
>> >
>> > Does anyone know where i can find data like that ?
>> >
>> > i have found :
>> >
>> > http://www.bls.gov/data/#employment ? ?- ?Labour Data
>> > http://www.economicindicators.gov/ ?- Some Data
>> > http://www.bea.gov/ ? -- Economic Data
>> >
>> > im still struggling to find a csv or excel data source for payrolls Motor
>> > Vehicle Sales and Retail Sales.
>> >
>> > Regards
>> >
>> > Malcolm
>> >
>> > ? ? ? ?[[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>
>
>
> --
> Malcolm A.B Croucher
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From Heiko-Mayer at gmx.de  Sat Jul 11 16:28:45 2009
From: Heiko-Mayer at gmx.de (Heiko Mayer)
Date: Sat, 11 Jul 2009 16:28:45 +0200
Subject: [R-SIG-Finance] PerformanceAnalytics - Error in UseMethod("time<-")
Message-ID: <20090711142845.109680@gmx.net>

Hi,

I have tried to create the PerformanceSummary chart in PerformanceAnalytics but always got an error. Even the example doesn't work showing the same error. I only get german error messages, but in english it should look like:
'Error in UseMethod("time<-") : no applicable method for "time<-"'

> data(edhec)
> charts.PerformanceSummary(edhec[,c("Funds.of.Funds","Long.Short.Equity")])
Fehler in UseMethod("time<-") : keine anwendbare Methode f?r "time<-"
 
Has anyone an idea?

Thanks,
Heiko
-- 

f?r nur 19,99 Euro/mtl.!* http://portal.gmx.net/de/go/dsl02


From jeff.a.ryan at gmail.com  Sat Jul 11 18:21:30 2009
From: jeff.a.ryan at gmail.com (J Ryan)
Date: Sat, 11 Jul 2009 11:21:30 -0500
Subject: [R-SIG-Finance] PerformanceAnalytics - Error in
	UseMethod("time<-")
In-Reply-To: <20090711142845.109680@gmx.net>
References: <20090711142845.109680@gmx.net>
Message-ID: <4BDCE574-6499-4740-9139-2A8FB71C95D7@gmail.com>

For PerformanceAnalytics time<- is being defined in the zoo package.

You haven't included your sessionInfo, so beyond that it's not  
possible to see where the actual problem is.

Please post the all the requested info so we can see where conflicts/ 
issues are occurring.  My guess is a conflict in the use of time from  
another package.

Jeff

Jeffrey A. Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com

On Jul 11, 2009, at 9:28 AM, "Heiko Mayer" <Heiko-Mayer at gmx.de> wrote:

> Hi,
>
> I have tried to create the PerformanceSummary chart in  
> PerformanceAnalytics but always got an error. Even the example  
> doesn't work showing the same error. I only get german error  
> messages, but in english it should look like:
> 'Error in UseMethod("time<-") : no applicable method for "time<-"'
>
>> data(edhec)
>> charts.PerformanceSummary(edhec[,c 
>> ("Funds.of.Funds","Long.Short.Equity")])
> Fehler in UseMethod("time<-") : keine anwendbare Methode f?r "time<-"
>
> Has anyone an idea?
>
> Thanks,
> Heiko
> -- 
>
> f?r nur 19,99 Euro/mtl.!* http://portal.gmx.net/de/go/dsl02
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From ggrothendieck at gmail.com  Sat Jul 11 18:39:30 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 11 Jul 2009 12:39:30 -0400
Subject: [R-SIG-Finance] PerformanceAnalytics - Error in
	UseMethod("time<-")
In-Reply-To: <20090711142845.109680@gmx.net>
References: <20090711142845.109680@gmx.net>
Message-ID: <971536df0907110939u687dcceej35693172168019ce@mail.gmail.com>

Always state what commands you are using so that others
can reproduce the problem.    I suspect there is something
wrong with your setup as this works for me:

library(PerformanceAnalytics)
example(charts.PerformanceSummary)

> packageDescription("PerformanceAnalytics")$Version
[1] "0.9.7.1"
> R.version.string
[1] "R version 2.9.1 Patched (2009-07-01 r48886)"

>From your error message it seems that the zoo package
may not be present.  Make sure its installed and loaded.

search()

should list zoo.

On Sat, Jul 11, 2009 at 10:28 AM, Heiko Mayer<Heiko-Mayer at gmx.de> wrote:
> Hi,
>
> I have tried to create the PerformanceSummary chart in PerformanceAnalytics but always got an error. Even the example doesn't work showing the same error. I only get german error messages, but in english it should look like:
> 'Error in UseMethod("time<-") : no applicable method for "time<-"'
>
>> data(edhec)
>> charts.PerformanceSummary(edhec[,c("Funds.of.Funds","Long.Short.Equity")])
> Fehler in UseMethod("time<-") : keine anwendbare Methode f?r "time<-"
>
> Has anyone an idea?
>
> Thanks,
> Heiko
> --
>
> f?r nur 19,99 Euro/mtl.!* http://portal.gmx.net/de/go/dsl02
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From spencer.graves at prodsyse.com  Sun Jul 12 03:32:53 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sat, 11 Jul 2009 18:32:53 -0700
Subject: [R-SIG-Finance] financial series: waveslim, brainwaver,
 wavetresh and fractal
In-Reply-To: <b9bcd4e10907090800v1bcd0d3cr5ad68a041755b3fd@mail.gmail.com>
References: <b9bcd4e10907090800v1bcd0d3cr5ad68a041755b3fd@mail.gmail.com>
Message-ID: <4A593D45.6060100@prodsyse.com>

I can't help you directly, but have you tried "RSiteSearch.function" in 
the "RSiteSearch" package?  Consider the following: 


library(RSiteSearch)

fcp <- RSiteSearch.function('fractal changepoint')
fcps <- RSiteSearch.function('fractal changepoints')

wcp <- RSiteSearch.function('wavelet changepoint')
wcps <- RSiteSearch.function('wavelet changepoints')
HTML(wcp)


      This sequence returned 1 match for the third query, and HTML(wcp) 
displayed that match in a web browser with a link to, in this case, the 
index page for the "WaveCGH" package. 


      Hope this helps. 
      Spencer Graves 

stvienna wiener wrote:
> Greetings to all!
>
>
> # Short-Version
> I am looking for advanced R packages similar to "waveslim",
> "brainwaver", "wavethresh", "fractal", etc.
> in order to analyze financial times series (e.g. stock prices).
>
>
> Currently I am writing code to show/demonstrate the use of wavelets
> and fractal dimension
> to analyze financial time series (its a school project for my master
> degree in CS).
>
> The purpose is, for example, to find a correlation between IBM stock
> prices and the dow jones index. Or to find a changepoint in
> stock prices, correlating to unusual events e.g Yahoo stock prices in 2008 while
> there was an acquisition attempt.
>
> I have been using the "brainwaver" package to calculate correlations
> of us stock prices to the dow jones.
> It seems to work, but I am not 100% sure. (could provide so code but
> its just fragments at the moment)
>
> So my questions at the moment would be:
> 1)  Is there a specific R package that would be interesting and
> helpful that I missed?
> 2)  Is there a package to find "changepoints" in time series with
> wavelets or fractal-approach?
> I know there is a function in "waveslim" to find a changepoint in the
> wavelet-variance,
> but I could not yet get it working.
> If not is there a way by other methods e.g. bayesian interference?
>
>
> As references I was looking at the task views of finance, econometrics
> and time series.
>
> As well as the following to very helpful books (aside of the standard
> wavelet literature):
>
> (2001) An Introduction to Wavelets and Other Filtering Methods in
> Finance and Economics.
> Wavelet Methods in Statistics with R [ would be looking for a newer
> reference ... .. ]
> (2008) Wavelet Methods in Statistics with R
>
>
> THANK you very much for any help or pointers!!
>
>
> With the best regards to all,
> Steve
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From Heiko-Mayer at gmx.de  Sun Jul 12 13:03:32 2009
From: Heiko-Mayer at gmx.de (Heiko Mayer)
Date: Sun, 12 Jul 2009 13:03:32 +0200
Subject: [R-SIG-Finance] PerformanceAnalytics - Error in
 UseMethod("time<-")
In-Reply-To: <4BDCE574-6499-4740-9139-2A8FB71C95D7@gmail.com>
References: <20090711142845.109680@gmx.net>
	<4BDCE574-6499-4740-9139-2A8FB71C95D7@gmail.com>
Message-ID: <20090712110332.129030@gmx.net>

Thank you all for the feedback. I was able to replicate the error and I know when it occurs, but have no clue why.
I have installed the latest R version (2.9.1) and libraries PerformanceAnalytics and zoo. Under that environment, "charts.PerformanceSummary" worked. 
However, after installing Rmetrics packages according to "https://wiki.rmetrics.org/install_rmetrics"
   source("http://www.rmetrics.org/Rmetrics.R")
   install.Rmetrics()
it doesn't work anymore when you load library "fPortfolio" (in addition to PerformanceAnalytics and zoo).
Does anyone experienced problems with fPortfolio before?

Below please find the sessionInfo:
R version 2.9.1 (2009-06-26) 
i386-pc-mingw32 

locale:
LC_COLLATE=German_Germany.1252;LC_CTYPE=German_Germany.1252;LC_MONETARY=German_Germany.1252;LC_NUMERIC=C;LC_TIME=German_Germany.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] fPortfolio_2100.77           Rsymphony_0.1-9              Rglpk_0.3-1                  slam_0.1-1                   quadprog_1.4-11             
 [6] fAssets_2100.77              robustbase_0.4-5             fCopulae_2100.76             adapt_1.0-4                  sn_0.4-12                   
[11] mnormt_1.3-3                 fBasics_2100.77              timeSeries_2100.83           timeDate_290.85              MASS_7.2-47                 
[16] PerformanceAnalytics_0.9.7.1 zoo_1.5-6                   

loaded via a namespace (and not attached):
[1] grid_2.9.1      lattice_0.17-25 tools_2.9.1    

Thank you,
Heiko


-------- Original-Nachricht --------
> Datum: Sat, 11 Jul 2009 11:21:30 -0500
> Von: J Ryan <jeff.a.ryan at gmail.com>
> An: Heiko Mayer <Heiko-Mayer at gmx.de>
> CC: "r-sig-finance at stat.math.ethz.ch" <r-sig-finance at stat.math.ethz.ch>
> Betreff: Re: [R-SIG-Finance] PerformanceAnalytics - Error in UseMethod("time<-")

> For PerformanceAnalytics time<- is being defined in the zoo package.
> 
> You haven't included your sessionInfo, so beyond that it's not  
> possible to see where the actual problem is.
> 
> Please post the all the requested info so we can see where conflicts/ 
> issues are occurring.  My guess is a conflict in the use of time from  
> another package.
> 
> Jeff
> 
> Jeffrey A. Ryan
> jeffrey.ryan at insightalgo.com
> 
> ia: insight algorithmics
> www.insightalgo.com
> 
> On Jul 11, 2009, at 9:28 AM, "Heiko Mayer" <Heiko-Mayer at gmx.de> wrote:
> 
> > Hi,
> >
> > I have tried to create the PerformanceSummary chart in  
> > PerformanceAnalytics but always got an error. Even the example  
> > doesn't work showing the same error. I only get german error  
> > messages, but in english it should look like:
> > 'Error in UseMethod("time<-") : no applicable method for "time<-"'
> >
> >> data(edhec)
> >> charts.PerformanceSummary(edhec[,c 
> >> ("Funds.of.Funds","Long.Short.Equity")])
> > Fehler in UseMethod("time<-") : keine anwendbare Methode f?r "time<-"
> >
> > Has anyone an idea?
> >
> > Thanks,
> > Heiko
> > -- 
> >
> > f?r nur 19,99 Euro/mtl.!* http://portal.gmx.net/de/go/dsl02
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.

-- 

f?r nur 19,99 Euro/mtl.!* http://portal.gmx.net/de/go/dsl02


From ggrothendieck at gmail.com  Sun Jul 12 13:23:39 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 12 Jul 2009 07:23:39 -0400
Subject: [R-SIG-Finance] PerformanceAnalytics - Error in
	UseMethod("time<-")
In-Reply-To: <971536df0907110939u687dcceej35693172168019ce@mail.gmail.com>
References: <20090711142845.109680@gmx.net>
	<971536df0907110939u687dcceej35693172168019ce@mail.gmail.com>
Message-ID: <971536df0907120423s3f179877p2e17f0dca0f59b1b@mail.gmail.com>

This seems to cause the problem:

library(PerformanceAnalytics)
library(fPortfolio)
example(charts.PerformanceSummary)

whereas this works (same statements different order):

library(fPortfolio)
library(PerformanceAnalytics)
example(charts.PerformanceSummary)

It appears that fPortfolio or one of the associated rmetrics packages
is masking time<-.zoo.  I suspect an S4 problem either with S4 itself
or with the implementation in rmetrics.

At any rate the workaround seems to be to load PerformanceAnalytics
after fPortfolio rather than before.

> packageDescription("PerformanceAnalytics")$Version
[1] "0.9.7.1"
> R.version.string
[1] "R version 2.9.1 Patched (2009-07-01 r48886)"
> packageDescription("fPortfolio")$Version
[1] "2100.77"


On Sat, Jul 11, 2009 at 12:39 PM, Gabor
Grothendieck<ggrothendieck at gmail.com> wrote:
> Always state what commands you are using so that others
> can reproduce the problem. ? ?I suspect there is something
> wrong with your setup as this works for me:
>
> library(PerformanceAnalytics)
> example(charts.PerformanceSummary)
>
>> packageDescription("PerformanceAnalytics")$Version
> [1] "0.9.7.1"
>> R.version.string
> [1] "R version 2.9.1 Patched (2009-07-01 r48886)"
>
> From your error message it seems that the zoo package
> may not be present. ?Make sure its installed and loaded.
>
> search()
>
> should list zoo.
>
> On Sat, Jul 11, 2009 at 10:28 AM, Heiko Mayer<Heiko-Mayer at gmx.de> wrote:
>> Hi,
>>
>> I have tried to create the PerformanceSummary chart in PerformanceAnalytics but always got an error. Even the example doesn't work showing the same error. I only get german error messages, but in english it should look like:
>> 'Error in UseMethod("time<-") : no applicable method for "time<-"'
>>
>>> data(edhec)
>>> charts.PerformanceSummary(edhec[,c("Funds.of.Funds","Long.Short.Equity")])
>> Fehler in UseMethod("time<-") : keine anwendbare Methode f?r "time<-"
>>
>> Has anyone an idea?
>>
>> Thanks,
>> Heiko
>> --
>>
>> f?r nur 19,99 Euro/mtl.!* http://portal.gmx.net/de/go/dsl02
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From Heiko-Mayer at gmx.de  Sun Jul 12 14:07:51 2009
From: Heiko-Mayer at gmx.de (Heiko Mayer)
Date: Sun, 12 Jul 2009 14:07:51 +0200
Subject: [R-SIG-Finance] PerformanceAnalytics - Error in
	UseMethod("time<-")
In-Reply-To: <971536df0907120423s3f179877p2e17f0dca0f59b1b@mail.gmail.com>
References: <20090711142845.109680@gmx.net>
	<971536df0907110939u687dcceej35693172168019ce@mail.gmail.com>
	<971536df0907120423s3f179877p2e17f0dca0f59b1b@mail.gmail.com>
Message-ID: <20090712120751.129030@gmx.net>

Gabor,

Thank you very much. That quick fix works excellent. I will cc Rmetrics, maybe they can have a look at it.

Regards,
Heiko

-------- Original-Nachricht --------
> Datum: Sun, 12 Jul 2009 07:23:39 -0400
> Von: Gabor Grothendieck <ggrothendieck at gmail.com>
> An: Heiko Mayer <Heiko-Mayer at gmx.de>
> CC: r-sig-finance at stat.math.ethz.ch
> Betreff: Re: [R-SIG-Finance] PerformanceAnalytics - Error in 	UseMethod("time<-")

> This seems to cause the problem:
> 
> library(PerformanceAnalytics)
> library(fPortfolio)
> example(charts.PerformanceSummary)
> 
> whereas this works (same statements different order):
> 
> library(fPortfolio)
> library(PerformanceAnalytics)
> example(charts.PerformanceSummary)
> 
> It appears that fPortfolio or one of the associated rmetrics packages
> is masking time<-.zoo.  I suspect an S4 problem either with S4 itself
> or with the implementation in rmetrics.
> 
> At any rate the workaround seems to be to load PerformanceAnalytics
> after fPortfolio rather than before.
> 
> > packageDescription("PerformanceAnalytics")$Version
> [1] "0.9.7.1"
> > R.version.string
> [1] "R version 2.9.1 Patched (2009-07-01 r48886)"
> > packageDescription("fPortfolio")$Version
> [1] "2100.77"
> 
> 
> On Sat, Jul 11, 2009 at 12:39 PM, Gabor
> Grothendieck<ggrothendieck at gmail.com> wrote:
> > Always state what commands you are using so that others
> > can reproduce the problem. ? ?I suspect there is something
> > wrong with your setup as this works for me:
> >
> > library(PerformanceAnalytics)
> > example(charts.PerformanceSummary)
> >
> >> packageDescription("PerformanceAnalytics")$Version
> > [1] "0.9.7.1"
> >> R.version.string
> > [1] "R version 2.9.1 Patched (2009-07-01 r48886)"
> >
> > From your error message it seems that the zoo package
> > may not be present. ?Make sure its installed and loaded.
> >
> > search()
> >
> > should list zoo.
> >
> > On Sat, Jul 11, 2009 at 10:28 AM, Heiko Mayer<Heiko-Mayer at gmx.de> wrote:
> >> Hi,
> >>
> >> I have tried to create the PerformanceSummary chart in
> PerformanceAnalytics but always got an error. Even the example doesn't work showing the
> same error. I only get german error messages, but in english it should look
> like:
> >> 'Error in UseMethod("time<-") : no applicable method for "time<-"'
> >>
> >>> data(edhec)
> >>>
> charts.PerformanceSummary(edhec[,c("Funds.of.Funds","Long.Short.Equity")])
> >> Fehler in UseMethod("time<-") : keine anwendbare Methode f?r "time<-"
> >>
> >> Has anyone an idea?
> >>
> >> Thanks,
> >> Heiko
> >> --
> >>
> >> f?r nur 19,99 Euro/mtl.!* http://portal.gmx.net/de/go/dsl02
> >>
> >> _______________________________________________
> >> R-SIG-Finance at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >> -- Subscriber-posting only.
> >> -- If you want to post, subscribe first.
> >>
> >

-- 
GRATIS f?r alle GMX-Mitglieder: Die maxdome Movie-FLAT!


From brian at braverock.com  Sun Jul 12 17:50:40 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Sun, 12 Jul 2009 10:50:40 -0500
Subject: [R-SIG-Finance] Finance Data
In-Reply-To: <386fa5610907110140n3a099e1cxe2bf7e2610501b4c@mail.gmail.com>
References: <386fa5610906151130p1e4a8abdgd1b6e6efd2ebd54a@mail.gmail.com>	<e8e755250906151152x4bad25ffj8f3117be444d358d@mail.gmail.com>
	<386fa5610907110140n3a099e1cxe2bf7e2610501b4c@mail.gmail.com>
Message-ID: <4A5A0650.6000908@braverock.com>

malcolm Crouch wrote:
> Thanks for this , this has been a great help . I found exactly what I am
> looking for .
> However I was wondering the following : I notice in the financial press that
> with each figure there is an estimate attached :
> 
> e.g Unemployment expected 9.45 currently at 9.4 .
> 
> Does  anyone know of a reliable place to get estimates ?

Malcolm,

The "estimate" is typically a consensus average of analyst or economist 
estimates on the statistic in question.  Several commercial data providers sell 
  access to these estimates, or you can take them out of the paper.  There is 
no free source that I am aware of.

Regards,

    - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From spencer.graves at prodsyse.com  Mon Jul 13 01:57:23 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Sun, 12 Jul 2009 16:57:23 -0700
Subject: [R-SIG-Finance] [R-sig-finance] rmetrics portfolio backtesting
 limitations question
In-Reply-To: <24417340.post@talk.nabble.com>
References: <24417340.post@talk.nabble.com>
Message-ID: <4A5A7863.4070904@prodsyse.com>

Hello, Andrew: 


      I'm not familiar with either "portfolioBacktest" nor 
"setWindowsHorizon".  Moreover, using RSiteSearch produced nothing for 
either.  You might get more help from this list if you don't require 
respondents to understand these terms. 


      Have you considered using the "debug" function to walk through 
code line by line, looking at what it does, changing things at will?  
You can often learn enough doing this to see what you need to do to get 
it to do what you want, provided there is enough information in the 
data.  By doing this, you should be able to get the weights outputted in 
many different ways. 


      However, I'd look carefully at any algorithms that produced 
distinct portfolio weights for many assets.  As a check, I suggest you 
compute "eigen" of "corr" on your favorite 50 assets and look at 
"eigen(...)$values".  If the smallest eigenvalue exceeds, say, 0.0001 
times the largest, you are probably OK.  Otherwise, your use of 
individually estimated weights could be worse than using constant 
weights. In particular, if you have fewer than 50 observations, the 
smallest eigenvalue may be negative, which says that that portion of the 
variability, and probably more than that, is driven by round-off error.  
I'd rather not base investment decisions on round-off. 


      Hope this helps. 
      Spencer Graves
p.s.  Are you aware that you can get the source code for any CRAN 
package?  For example, the source for the "timeDate" package is 
available in a file "timeDate_290.85.tar.gz" downloadable from 
"http://cran.fhcrc.org/web/packages/timeDate/index.html".  If the people 
who wrote a particular functions included comments in their code, they 
will appear in the *.tar.gz file but not in the version you get by 
typing the function name. 


tradenet wrote:
> I have been working with the portfolioBacktesting function in Rmetrics and it
> seems to be a very powerful and useful function.  There seems to be some
> limitations/bugs that seriously limits the utility of the function and I was
> hoping someone has found workarounds:
>
> 1.) If I specify a large number of assets, say 50, in the formula, e.g. SPX
> ~ asset1 + asset2....+asset50
> then it seems only the first 29 assets are used in the analysis. 
> ncol(backtestPortfolios$weights) always returns 29 and assets near the end
> of the list get nonzero weights if I move their names to the beginning of
> the formula string, otherwise those assets do not appear in the results.  Is
> there a way to NOT compare results to a benchmark?  I'd like to just supply
> a timeseries of asset returns for a date range and run the analysis using
> all assets in the timeseries as portfolio candidates.
>
> BTW, if anyone knows an easy way to dump out the weights to a file I'd
> appreciate the insight
>
> 2.) the 12m window horizon seems to be the only one that works:
> setWindowsHorizon(backtestBT)<-"12m" works, but <-"3m" yields an error
> message.
>
> I thought this might have to do with the lambda smoothing, so I set the
> smoothing to "1m" and still no go
>
> 3.) does anyone know how to turn smoothing completely off?  I would like to
> see the raw, unsmoothed weights
>
> 4.) is there anyway to plot the results without performing weight smoothing
> -- I don't want to have to do smoothing to look at all the great plots and
> results.
>
> Rmetrics is great stuff, so close to being beyond perfect!  I am very
> grateful to the developers and the community.
>
> Warm regards,
>
> Andrew
>


From breman.mark at gmail.com  Mon Jul 13 08:57:14 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Mon, 13 Jul 2009 08:57:14 +0200
Subject: [R-SIG-Finance] xts question: how to get previous row?
Message-ID: <5e6a2e670907122357o72053e6xeea57746a695558@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090713/691c9016/attachment.pl>

From edd at debian.org  Mon Jul 13 11:26:59 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 13 Jul 2009 04:26:59 -0500
Subject: [R-SIG-Finance] xts question: how to get previous row?
In-Reply-To: <5e6a2e670907122357o72053e6xeea57746a695558@mail.gmail.com>
References: <5e6a2e670907122357o72053e6xeea57746a695558@mail.gmail.com>
Message-ID: <19034.64995.222465.374671@ron.nulle.part>


On 13 July 2009 at 08:57, Mark Breman wrote:
| I have a really basic question but I can't find an answer for it.
| 
| Supose I have this xts timeseries called aapl:
|            AAPL.Close
| 2007-04-20      90.97
| 2007-04-23      93.51
| 2007-04-24      93.24
| 2007-04-25      95.35
| 2007-04-26      98.84
| 2007-04-27      99.92
| 2007-04-30      99.80
| 2007-05-01      99.47
| 2007-05-02     100.39
| 2007-05-03     100.40
| 2007-05-04     100.81
| 
| and supose I have a reference to a row in this series (i.e.
| aapl["2007-04-30"]), what is the easiest way to select the previous row
| (i.e. the row with index 2007-04-27) from the series?

This has nothing to do with Finance. You may want to read up on the R functions

	index()
	which()
	as.Date()

to see just how easy xts makes the indexing.   In a nutshell, use index() to
extract the position of the date(s) you want. This gives you the same 'type'
as the indexing is done one. Here, these are dates which you can add/subtract
from so a simple '- 1' gives you the previous day.   You could use lag
operators.

R> Z <- xts(100+cumsum(rnorm(10)), order.by=as.Date("2009-07-01")+0:9)
R> Z
             [,1]
2009-07-01 100.08
2009-07-02  99.83
2009-07-03 100.00
2009-07-04  98.81
2009-07-05  97.62
2009-07-06  97.84
2009-07-07  98.82
2009-07-08  98.56
2009-07-09  98.37
2009-07-10  98.06
R> Z[ index(Z["2009-07-07"]) - 1, ]
            [,1]
2009-07-06 97.84
R> 


Hth,  Dirk

-- 
Three out of two people have difficulties with fractions.


From Murali.MENON at fortisinvestments.com  Mon Jul 13 11:47:05 2009
From: Murali.MENON at fortisinvestments.com (Murali.MENON at fortisinvestments.com)
Date: Mon, 13 Jul 2009 11:47:05 +0200
Subject: [R-SIG-Finance] xts question: how to get previous row?
In-Reply-To: <5e6a2e670907122357o72053e6xeea57746a695558@mail.gmail.com>
References: <5e6a2e670907122357o72053e6xeea57746a695558@mail.gmail.com>
Message-ID: <5A3D018CBDC36B4F8FF6DB52DDF3B82DB1B3DC@BE-S0500-V22.adroot.local>

This appears to work:

> aapl <- as.xts(read.zoo(textConnection("2007-04-20,      90.97
+ 2007-04-23,      93.51
+ 2007-04-24,      93.24
+ 2007-04-25,      95.35
+ 2007-04-26,      98.84
+ 2007-04-27,      99.92
+ 2007-04-30,      99.80
+ 2007-05-01,      99.47
+ 2007-05-02,     100.39
+ 2007-05-03,     100.40
+ 2007-05-04,     100.81"), sep=","))

> aapl[grep("2007-04-30", index(aapl)) - 1]
            [,1]
2007-04-27 99.92

Murali

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Mark
Breman
Sent: 13 July 2009 07:57
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] xts question: how to get previous row?

I have a really basic question but I can't find an answer for it.

Supose I have this xts timeseries called aapl:
           AAPL.Close
2007-04-20      90.97
2007-04-23      93.51
2007-04-24      93.24
2007-04-25      95.35
2007-04-26      98.84
2007-04-27      99.92
2007-04-30      99.80
2007-05-01      99.47
2007-05-02     100.39
2007-05-03     100.40
2007-05-04     100.81

and supose I have a reference to a row in this series (i.e.
aapl["2007-04-30"]), what is the easiest way to select the previous row
(i.e. the row with index 2007-04-27) from the series?

Thanks,

-Mark-

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From sandor.benczik at crabel.ro  Mon Jul 13 13:22:28 2009
From: sandor.benczik at crabel.ro (Sandor Benczik)
Date: Mon, 13 Jul 2009 14:22:28 +0300
Subject: [R-SIG-Finance] xts question: how to get previous row?
Message-ID: <1DD3BDB66DD5A34BAB9D8C9AA6A422EF5340F5@crabelmain.crabel.ro>

You may try this:
> getSymbols("AAPL")
> prev.idx <- which(index(AAPL["2007-01-08"])==index(AAPL))-1
> AAPL[prev.idx]

It should be slightly faster than grep-ing, but still does not do binary
search.

OTOH, I don't think Dirk's suggestion will work if you have gaps
(missing days) in the data. Say:
> Z <- xts(100+cumsum(rnorm(10)), order.by=as.Date("2009-07-01")+0:9*2)
> Z[ index(Z["2009-07-07"]) - 1, ]
Data:
numeric(0)

Index:
integer(0)

HTH,
Sandor

| -----Original Message-----
| From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-
| bounces at stat.math.ethz.ch] On Behalf Of Dirk Eddelbuettel
| Sent: Monday, July 13, 2009 12:27 PM
| To: Mark Breman
| Cc: r-sig-finance at stat.math.ethz.ch
| Subject: Re: [R-SIG-Finance] xts question: how to get previous row?
| 
| 
| On 13 July 2009 at 08:57, Mark Breman wrote:
| | I have a really basic question but I can't find an answer for it.
| |
| | Supose I have this xts timeseries called aapl:
| |            AAPL.Close
| | 2007-04-20      90.97
| | 2007-04-23      93.51
| | 2007-04-24      93.24
| | 2007-04-25      95.35
| | 2007-04-26      98.84
| | 2007-04-27      99.92
| | 2007-04-30      99.80
| | 2007-05-01      99.47
| | 2007-05-02     100.39
| | 2007-05-03     100.40
| | 2007-05-04     100.81
| |
| | and supose I have a reference to a row in this series (i.e.
| | aapl["2007-04-30"]), what is the easiest way to select the previous
row
| | (i.e. the row with index 2007-04-27) from the series?
| 
| This has nothing to do with Finance. You may want to read up on the R
| functions
| 
| 	index()
| 	which()
| 	as.Date()
| 
| to see just how easy xts makes the indexing.   In a nutshell, use
index()
| to
| extract the position of the date(s) you want. This gives you the same
| 'type'
| as the indexing is done one. Here, these are dates which you can
| add/subtract
| from so a simple '- 1' gives you the previous day.   You could use lag
| operators.
| 
| R> Z <- xts(100+cumsum(rnorm(10)), order.by=as.Date("2009-07-01")+0:9)
| R> Z
|              [,1]
| 2009-07-01 100.08
| 2009-07-02  99.83
| 2009-07-03 100.00
| 2009-07-04  98.81
| 2009-07-05  97.62
| 2009-07-06  97.84
| 2009-07-07  98.82
| 2009-07-08  98.56
| 2009-07-09  98.37
| 2009-07-10  98.06
| R> Z[ index(Z["2009-07-07"]) - 1, ]
|             [,1]
| 2009-07-06 97.84
| R>
| 
| 
| Hth,  Dirk
| 
| --
| Three out of two people have difficulties with fractions.
| 
| _______________________________________________
| R-SIG-Finance at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-sig-finance
| -- Subscriber-posting only.
| -- If you want to post, subscribe first.


From breman.mark at gmail.com  Mon Jul 13 13:02:58 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Mon, 13 Jul 2009 13:02:58 +0200
Subject: [R-SIG-Finance] xts question: how to get previous row?
In-Reply-To: <5A3D018CBDC36B4F8FF6DB52DDF3B82DB1B3DC@BE-S0500-V22.adroot.local>
References: <5e6a2e670907122357o72053e6xeea57746a695558@mail.gmail.com>
	<5A3D018CBDC36B4F8FF6DB52DDF3B82DB1B3DC@BE-S0500-V22.adroot.local>
Message-ID: <5e6a2e670907130402l1b83d0dfkc594567f519f83e5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090713/2f740a54/attachment.pl>

From ravis at ambaresearch.com  Tue Jul 14 14:57:25 2009
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Tue, 14 Jul 2009 18:27:25 +0530
Subject: [R-SIG-Finance] Extracting implied probabilities
Message-ID: <B8948310FD4828478CBCC2093E5096B726CDFB@BAN-MAILSRV02.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090714/b62bdcb1/attachment.pl>

From kagba2006 at yahoo.com  Tue Jul 14 16:56:11 2009
From: kagba2006 at yahoo.com (FMH)
Date: Tue, 14 Jul 2009 07:56:11 -0700 (PDT)
Subject: [R-SIG-Finance] Problem with GroupedData
In-Reply-To: <mailman.1.1247565601.18918.r-sig-finance@stat.math.ethz.ch>
References: <mailman.1.1247565601.18918.r-sig-finance@stat.math.ethz.ch>
Message-ID: <240002.32066.qm@web38301.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090714/0c244a96/attachment.pl>

From robert at sanctumfi.com  Tue Jul 14 17:12:11 2009
From: robert at sanctumfi.com (Robert Sams)
Date: Tue, 14 Jul 2009 16:12:11 +0100
Subject: [R-SIG-Finance] Extracting implied probabilities
References: <SANCTUMFISERVEREB4H00001761@sanctumfi.com>
Message-ID: <SANCTUMFISERVERO5fY0000180e@sanctumfi.com>

I use the simple, model-free approach of letting (P[i] - P[i+1]) /
(X[i+1] - X[i]) be the probability that the price of the underlying is
*at or above* X[i+1] at expiry and P[i] is the premium of call X[i].
Solve for all i's up the strike chain. Basically, we construct call
spreads from consecutive strikes up the call chain and treat each spread
as a binary option struck at the top strike. The probability is the
spread price divided by the strike difference.

Of course, the resolution of the price intervals is limited by the size
of your strike chain. Your case of three doesn't get you very far,
unfortunately.

Note, you can also check which puts have open interest and using
put-call parity you might be able to fill in some gaps with "shadow"
call values.

Robert 

> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch 
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of 
> Ravi S. Shankar
> Sent: 14 July 2009 13:57
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Extracting implied probabilities
> 
> Hi R,
> 
>  
> 
> I am trying to extract the implied probability from options 
> on individual stocks.
> 
>  
> 
> I have taken the near month (1 month) call options (American) 
> and removed strikes with zero volume. But I end with just 
> three strikes and all of them OTM.
> 
>  
> 
> Date
> 
> Expiry
> 
> Strike.Price
> 
> Settle.Price
> 
> Spot
> 
> 29-Dec-06
> 
> 25-Jan-2007
> 
> 900
> 
> 27.75
> 
> 891.5
> 
> 29-Dec-06
> 
> 25-Jan-2007
> 
> 940
> 
> 12.5
> 
> 891.5
> 
> 29-Dec-06
> 
> 25-Jan-2007
> 
> 920
> 
> 19.75
> 
> 891.5
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
>  
> 
> I will compute the implied volatility and delta using the 
> fOptions package. I intend to use the splines package to 
> interpolate in the implied volatility-delta space. My 
> question is are just 3 strikes sufficient to interpolate and 
> extract a meaningful implied probability?
> Any pointers or paper that deals with extracting implied 
> probability from thin options markets would be helpful.
> 
>  
> 
>  
> 
> Thank you,
> 
> Regards,
> 
>  
> 
> Ravi Shankar S
> 
> This e-mail may contain confidential and/or privileged 
> i...{{dropped:13}}
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 


From brian at braverock.com  Tue Jul 14 17:21:56 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 14 Jul 2009 10:21:56 -0500
Subject: [R-SIG-Finance] Extracting the n-step-ahead estimates from fGarch
Message-ID: <4A5CA294.6030703@braverock.com>

I've been experimenting with fitting a garch model to some instrument 
data.  It appears that the  conditional volatility for the instrument in 
question behaves pretty well in a GARCH(p,q) model.

I'm wondering if anyone has combined garchFit() and predict() from 
fGarch to effectively produce a "rolling prediction" on the series?

 gtest=garchFit(~garch(1,1),garchSim())
 predict(gtest, n.ahead = 10)

Will fit a GARCH(1,1) model to the data, and then predict 10 steps ahead 
on that model.  This is fine, but in the real world you'd want to 
predict on a rolling basis "out of sample", potentially refitting as 
each new observation comes in, or not, depending on the stability of 
your series.

Barring any input from the list, I'll probably do this using 
apply.fromstart from PerformanceAnalytics (wouldn't it be nice if 
rollapply in xts and zoo could fix the starting point for the roll?), 
and then collect the prediction at time t for time t+n, and then line up 
the predictions with the observed series when I'm done.

I'm hoping somebody has already written a wrapper like this and can save 
me some time.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From brian at braverock.com  Tue Jul 14 19:11:33 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 14 Jul 2009 12:11:33 -0500
Subject: [R-SIG-Finance] Problem with GroupedData
In-Reply-To: <240002.32066.qm@web38301.mail.mud.yahoo.com>
References: <mailman.1.1247565601.18918.r-sig-finance@stat.math.ethz.ch>
	<240002.32066.qm@web38301.mail.mud.yahoo.com>
Message-ID: <4A5CBC45.3030203@braverock.com>

Try sending this to r-help. This list is for finance related discussion, 
and while your data may be finance data (even that isn't clear), you're 
asking a novice R question that is unrelated to finance. Please direct 
it to the appropriate forum.

Regards,

    - Brian

FMH wrote:
> Hi,
>  
> I have an original data frame with 8 columns of variables, which are stored in 'data1' frame.
>  
> data1 <- read.csv("E:\\PHD GLASGOW UNIVERSITY\\Data\\R\\Colin\\Cailness21.csv")
> attach(data1)
> names(data1)
>  
> [1] "Date"       "d"          "m"          "y"          "Time"      
> [6] "Depth"      "Temp"       "Group_Time"
>  
>  
> Recently, i try two add a column into this data frame, but when i try to use the 'groupedData' function with this new data frame via command below, it reported the problem stated after the command below.
>  
> library(nlme)
> tmp <- groupedData(Temp ~ d | Depth, data2, order.groups = TRUE, labels = list(x = "Month", y = "Temperature"), units = list(y = "0C"))
>
> Error in nfGroupedData(formula = Temp ~ d | Depth, data = data2, order.groups = TRUE,  : 
>   second argument to groupedData must inherit from data.frame
>  
>  
> Could someone help me to sort out on this problem, please?
>  
> Thank you
> Fir
>


From sarswat at gmail.com  Tue Jul 14 19:42:43 2009
From: sarswat at gmail.com (suneel)
Date: Tue, 14 Jul 2009 23:12:43 +0530
Subject: [R-SIG-Finance]  process order-book/trade data
In-Reply-To: <925D7CDA-64B6-4F54-8337-93D1AC1140F6@unimi.it>
References: <b1f16d9d0905221724q3950bda8s9010e8d845e7d926@mail.gmail.com>
	<925D7CDA-64B6-4F54-8337-93D1AC1140F6@unimi.it>
Message-ID: <4A5CC393.10202@gmail.com>

Hi Friends,
                 Currently I am handling high-frequency data for my 
research work, and I have two specific query regarding that.
1) I want convert trade data into minute-wise data with for column 
(high, low, open, close), since my data for each day is around 600MB, 
"scan" or "read.table" is not the best way to work around for reading 
the data. Any suggestion/program would be helpful, since I am new to R.
2) For order data I have to convert them into trade data and order 
snap-shots. any help?
Thanks,
Suneel


From cgb at datanalytics.com  Tue Jul 14 19:51:53 2009
From: cgb at datanalytics.com (Carlos J. Gil Bellosta )
Date: Tue, 14 Jul 2009 19:51:53 +0200
Subject: [R-SIG-Finance] process order-book/trade data
In-Reply-To: <4A5CC393.10202@gmail.com>
References: <b1f16d9d0905221724q3950bda8s9010e8d845e7d926@mail.gmail.com>
	<925D7CDA-64B6-4F54-8337-93D1AC1140F6@unimi.it>
	<4A5CC393.10202@gmail.com>
Message-ID: <b028350f0907141051mb8a6ac1l3bcd916c605c67ae@mail.gmail.com>

2009/7/14 suneel <sarswat at gmail.com>:
> Hi Friends,
> ? ? ? ? ? ? ? ?Currently I am handling high-frequency data for my research
> work, and I have two specific query regarding that.
> 1) I want convert trade data into minute-wise data with for column (high,
> low, open, close), since my data for each day is around 600MB, "scan" or
> "read.table" is not the best way to work around for reading the data. Any
> suggestion/program would be helpful, since I am new to R.

For that, you may want to check my package "colbycol". You can install
it according to the instructions at

http://r-forge.r-project.org/projects/colbycol/

I tried to keep the functions interface there as close as possible to
"read.table" and friends. However, files are read column by column.
The process may be a bit slower, but it allows you to fit into memory
datasets which, as text files, were originally bigger than your RAM.

Best regards,

Carlos J. Gil Bellosta
http://www.datanalytics.com


From brian at braverock.com  Tue Jul 14 19:52:15 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 14 Jul 2009 12:52:15 -0500
Subject: [R-SIG-Finance] process order-book/trade data
In-Reply-To: <4A5CC393.10202@gmail.com>
References: <b1f16d9d0905221724q3950bda8s9010e8d845e7d926@mail.gmail.com>	<925D7CDA-64B6-4F54-8337-93D1AC1140F6@unimi.it>
	<4A5CC393.10202@gmail.com>
Message-ID: <4A5CC5CF.7040509@braverock.com>

- Use xts, it can handle millions of observations in memory
- Process one symbol or group of symbols at a time
- use to.period() to create OHLC(V) data from tick data
- for bid/ask quote data, use to.period on the bids and asks separately.

Regards,

   - Brian

suneel wrote:
> Hi Friends,
>                 Currently I am handling high-frequency data for my 
> research work, and I have two specific query regarding that.
> 1) I want convert trade data into minute-wise data with for column 
> (high, low, open, close), since my data for each day is around 600MB, 
> "scan" or "read.table" is not the best way to work around for reading 
> the data. Any suggestion/program would be helpful, since I am new to R.
> 2) For order data I have to convert them into trade data and order 
> snap-shots. any help?
> Thanks,
> Suneel

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From Murali.MENON at fortisinvestments.com  Wed Jul 15 11:55:09 2009
From: Murali.MENON at fortisinvestments.com (Murali.MENON at fortisinvestments.com)
Date: Wed, 15 Jul 2009 11:55:09 +0200
Subject: [R-SIG-Finance] comparison of trading rules
Message-ID: <5A3D018CBDC36B4F8FF6DB52DDF3B82DBFE262@BE-S0500-V22.adroot.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090715/9ad94994/attachment.pl>

From stvienna at gmail.com  Wed Jul 15 14:19:54 2009
From: stvienna at gmail.com (stvienna wiener)
Date: Wed, 15 Jul 2009 14:19:54 +0200
Subject: [R-SIG-Finance] financial series: waveslim, brainwaver,
	wavetresh 	and fractal
In-Reply-To: <4A593D45.6060100@prodsyse.com>
References: <b9bcd4e10907090800v1bcd0d3cr5ad68a041755b3fd@mail.gmail.com>
	<4A593D45.6060100@prodsyse.com>
Message-ID: <b9bcd4e10907150519y475b0dd7jb33a79d11b76a8aa@mail.gmail.com>

Hello Spencer,


thank you for your reply.

I tried to install the package "WaveCGH" today, however I got an error.

> install.packages("WaveCGH")

In getDependencies(pkgs, dependencies, available, lib) :
  package ?WaveCGH? is not available


Its somehow strange, because on the R-Website:
http://cran.r-project.org/web/packages/WaveCGH/

It is written:
"Package ?WaveCGH? was removed from the CRAN repository."

So I think the possibilities are:
a) the package was remove from CRAN
b) there are some dependencies. But I installed the newest R version!
c) I spelled the package wrong (e.g. small/capital letters )


Do you have maybe an idea?


Currently I am planning to use the following packages:
  library(R.utils)

  library(tseries)
  library(wmtsa)        #Wavelets
  library(waveslim)     #Wavelet Correlation
  library(brainwaver)   #Wavelet Correlation

  library(QRMlib)       # time series plotting

   library(wavethresh)  # wavelet thresholding
   library(CVThresh)    # wavelet thresholding


Any suggestions welcome ;-)

Regards,
Steve



2009/7/12 spencerg <spencer.graves at prodsyse.com>:
> I can't help you directly, but have you tried "RSiteSearch.function" in the
> "RSiteSearch" package? ?Consider the following:
>
> library(RSiteSearch)
>
> fcp <- RSiteSearch.function('fractal changepoint')
> fcps <- RSiteSearch.function('fractal changepoints')
>
> wcp <- RSiteSearch.function('wavelet changepoint')
> wcps <- RSiteSearch.function('wavelet changepoints')
> HTML(wcp)
>
>
> ? ? This sequence returned 1 match for the third query, and HTML(wcp)
> displayed that match in a web browser with a link to, in this case, the
> index page for the "WaveCGH" package.
>
> ? ? Hope this helps. ? ? Spencer Graves
> stvienna wiener wrote:
>>
>> Greetings to all!
>>
>>
>> # Short-Version
>> I am looking for advanced R packages similar to "waveslim",
>> "brainwaver", "wavethresh", "fractal", etc.
>> in order to analyze financial times series (e.g. stock prices).
>>
>>
>> Currently I am writing code to show/demonstrate the use of wavelets
>> and fractal dimension
>> to analyze financial time series (its a school project for my master
>> degree in CS).
>>
>> The purpose is, for example, to find a correlation between IBM stock
>> prices and the dow jones index. Or to find a changepoint in
>> stock prices, correlating to unusual events e.g Yahoo stock prices in 2008
>> while
>> there was an acquisition attempt.
>>
>> I have been using the "brainwaver" package to calculate correlations
>> of us stock prices to the dow jones.
>> It seems to work, but I am not 100% sure. (could provide so code but
>> its just fragments at the moment)
>>
>> So my questions at the moment would be:
>> 1) ?Is there a specific R package that would be interesting and
>> helpful that I missed?
>> 2) ?Is there a package to find "changepoints" in time series with
>> wavelets or fractal-approach?
>> I know there is a function in "waveslim" to find a changepoint in the
>> wavelet-variance,
>> but I could not yet get it working.
>> If not is there a way by other methods e.g. bayesian interference?
>>
>>
>> As references I was looking at the task views of finance, econometrics
>> and time series.
>>
>> As well as the following to very helpful books (aside of the standard
>> wavelet literature):
>>
>> (2001) An Introduction to Wavelets and Other Filtering Methods in
>> Finance and Economics.
>> Wavelet Methods in Statistics with R [ would be looking for a newer
>> reference ... .. ]
>> (2008) Wavelet Methods in Statistics with R
>>
>>
>> THANK you very much for any help or pointers!!
>>
>>
>> With the best regards to all,
>> Steve
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
>


From josh.m.ulrich at gmail.com  Wed Jul 15 15:11:28 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 15 Jul 2009 08:11:28 -0500
Subject: [R-SIG-Finance] xts question: how to get previous row?
In-Reply-To: <5e6a2e670907130402l1b83d0dfkc594567f519f83e5@mail.gmail.com>
References: <5e6a2e670907122357o72053e6xeea57746a695558@mail.gmail.com> 
	<5A3D018CBDC36B4F8FF6DB52DDF3B82DB1B3DC@BE-S0500-V22.adroot.local> 
	<5e6a2e670907130402l1b83d0dfkc594567f519f83e5@mail.gmail.com>
Message-ID: <8cca69990907150611h5953bc55oc08d5485189284ae@mail.gmail.com>

Mark,

I think the lag function does what you're asking.  Please see ?lag.xts
and note the differences between lag.xts and other time-series lag
functions.

> merge(aapl,lag(aapl))
            aapl lag.aapl.
2007-04-20  90.97        NA
2007-04-23  93.51     90.97
2007-04-24  93.24     93.51
2007-04-25  95.35     93.24
2007-04-26  98.84     95.35
2007-04-27  99.92     98.84
2007-04-30  99.80     99.92
2007-05-01  99.47     99.80
2007-05-02 100.39     99.47
2007-05-03 100.40    100.39
2007-05-04 100.81    100.40
> merge(aapl,lag(aapl))['2007-04-30',2]
          lag.aapl.
2007-04-30     99.92

Best,
Josh
--
http://www.fosstrading.com



On Mon, Jul 13, 2009 at 6:02 AM, Mark Breman<breman.mark at gmail.com> wrote:
> Thank you Murali, that does it for me.
>
> 2009/7/13 <Murali.MENON at fortisinvestments.com>
>
>> This appears to work:
>>
>> > aapl <- as.xts(read.zoo(textConnection("2007-04-20, ? ? ?90.97
>> + 2007-04-23, ? ? ?93.51
>> + 2007-04-24, ? ? ?93.24
>> + 2007-04-25, ? ? ?95.35
>> + 2007-04-26, ? ? ?98.84
>> + 2007-04-27, ? ? ?99.92
>> + 2007-04-30, ? ? ?99.80
>> + 2007-05-01, ? ? ?99.47
>> + 2007-05-02, ? ? 100.39
>> + 2007-05-03, ? ? 100.40
>> + 2007-05-04, ? ? 100.81"), sep=","))
>>
>> > aapl[grep("2007-04-30", index(aapl)) - 1]
>> ? ? ? ? ? ?[,1]
>> 2007-04-27 99.92
>>
>> Murali
>>
>> -----Original Message-----
>> From: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Mark
>> Breman
>> Sent: 13 July 2009 07:57
>> To: r-sig-finance at stat.math.ethz.ch
>> Subject: [R-SIG-Finance] xts question: how to get previous row?
>>
>> I have a really basic question but I can't find an answer for it.
>>
>> Supose I have this xts timeseries called aapl:
>> ? ? ? ? ? AAPL.Close
>> 2007-04-20 ? ? ?90.97
>> 2007-04-23 ? ? ?93.51
>> 2007-04-24 ? ? ?93.24
>> 2007-04-25 ? ? ?95.35
>> 2007-04-26 ? ? ?98.84
>> 2007-04-27 ? ? ?99.92
>> 2007-04-30 ? ? ?99.80
>> 2007-05-01 ? ? ?99.47
>> 2007-05-02 ? ? 100.39
>> 2007-05-03 ? ? 100.40
>> 2007-05-04 ? ? 100.81
>>
>> and supose I have a reference to a row in this series (i.e.
>> aapl["2007-04-30"]), what is the easiest way to select the previous row
>> (i.e. the row with index 2007-04-27) from the series?
>>
>> Thanks,
>>
>> -Mark-
>>
>> ? ? ? ? [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From hammeda at ffc.co.za  Wed Jul 15 15:21:29 2009
From: hammeda at ffc.co.za (Hammed Amusa)
Date: Wed, 15 Jul 2009 15:21:29 +0200
Subject: [R-SIG-Finance] Obtaining short-run parameters and error correction
	coefficients
Message-ID: <2957FBD3A69203429FE5D75BDC07E7F991662F@ffc-srv01.ffc.co.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090715/750ca2c9/attachment.pl>

From spencer.graves at prodsyse.com  Wed Jul 15 16:13:13 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Wed, 15 Jul 2009 07:13:13 -0700
Subject: [R-SIG-Finance] financial series: waveslim, brainwaver,
 wavetresh 	and fractal
In-Reply-To: <b9bcd4e10907150519y475b0dd7jb33a79d11b76a8aa@mail.gmail.com>
References: <b9bcd4e10907090800v1bcd0d3cr5ad68a041755b3fd@mail.gmail.com>	
	<4A593D45.6060100@prodsyse.com>
	<b9bcd4e10907150519y475b0dd7jb33a79d11b76a8aa@mail.gmail.com>
Message-ID: <4A5DE3F9.8030808@prodsyse.com>

Hello, Steve: 


      I see two options: 


           1.  Send an email to the package authors / mainteners to ask 
about the status.  I found an email address using 
"RSiteSearch('WaveCGH')".  If that email fails, you could try to find 
them via Google. 


           2.  You cited a web page that stated that, "Package ?WaveCGH? 
was removed from the CRAN repository."  That web page also says, 
"Formerly available versions can be obtained from the archive." If you 
click on the word "archive", it should take you to a web page from which 
you can download "WaveCGH_2.01.tar.gz".  This is a source file for the 
package.  With this, you can try "R CMD INSTALL WaveCGH_2.01.tar.gz" at 
a commands prompt.  This will work under Windows IF you have Rtools 
installed, and I believe it will work under Linux.  From within R, I 
believe "install.packages('WaveCGH_2.01.tar.gz')" may also work.  If 
that fails, you could try another post to this list. 


      Hope this helps. 
      Spencer Graves


stvienna wiener wrote:
> Hello Spencer,
>
>
> thank you for your reply.
>
> I tried to install the package "WaveCGH" today, however I got an error.
>
>   
>> install.packages("WaveCGH")
>>     
>
> In getDependencies(pkgs, dependencies, available, lib) :
>   package ?WaveCGH? is not available
>
>
> Its somehow strange, because on the R-Website:
> http://cran.r-project.org/web/packages/WaveCGH/
>
> It is written:
> "Package ?WaveCGH? was removed from the CRAN repository."
>
> So I think the possibilities are:
> a) the package was remove from CRAN
> b) there are some dependencies. But I installed the newest R version!
> c) I spelled the package wrong (e.g. small/capital letters )
>
>
> Do you have maybe an idea?
>
>
> Currently I am planning to use the following packages:
>   library(R.utils)
>
>   library(tseries)
>   library(wmtsa)        #Wavelets
>   library(waveslim)     #Wavelet Correlation
>   library(brainwaver)   #Wavelet Correlation
>
>   library(QRMlib)       # time series plotting
>
>    library(wavethresh)  # wavelet thresholding
>    library(CVThresh)    # wavelet thresholding
>
>
> Any suggestions welcome ;-)
>
> Regards,
> Steve
>
>
>
> 2009/7/12 spencerg <spencer.graves at prodsyse.com>:
>   
>> I can't help you directly, but have you tried "RSiteSearch.function" in the
>> "RSiteSearch" package?  Consider the following:
>>
>> library(RSiteSearch)
>>
>> fcp <- RSiteSearch.function('fractal changepoint')
>> fcps <- RSiteSearch.function('fractal changepoints')
>>
>> wcp <- RSiteSearch.function('wavelet changepoint')
>> wcps <- RSiteSearch.function('wavelet changepoints')
>> HTML(wcp)
>>
>>
>>     This sequence returned 1 match for the third query, and HTML(wcp)
>> displayed that match in a web browser with a link to, in this case, the
>> index page for the "WaveCGH" package.
>>
>>     Hope this helps.     Spencer Graves
>> stvienna wiener wrote:
>>     
>>> Greetings to all!
>>>
>>>
>>> # Short-Version
>>> I am looking for advanced R packages similar to "waveslim",
>>> "brainwaver", "wavethresh", "fractal", etc.
>>> in order to analyze financial times series (e.g. stock prices).
>>>
>>>
>>> Currently I am writing code to show/demonstrate the use of wavelets
>>> and fractal dimension
>>> to analyze financial time series (its a school project for my master
>>> degree in CS).
>>>
>>> The purpose is, for example, to find a correlation between IBM stock
>>> prices and the dow jones index. Or to find a changepoint in
>>> stock prices, correlating to unusual events e.g Yahoo stock prices in 2008
>>> while
>>> there was an acquisition attempt.
>>>
>>> I have been using the "brainwaver" package to calculate correlations
>>> of us stock prices to the dow jones.
>>> It seems to work, but I am not 100% sure. (could provide so code but
>>> its just fragments at the moment)
>>>
>>> So my questions at the moment would be:
>>> 1)  Is there a specific R package that would be interesting and
>>> helpful that I missed?
>>> 2)  Is there a package to find "changepoints" in time series with
>>> wavelets or fractal-approach?
>>> I know there is a function in "waveslim" to find a changepoint in the
>>> wavelet-variance,
>>> but I could not yet get it working.
>>> If not is there a way by other methods e.g. bayesian interference?
>>>
>>>
>>> As references I was looking at the task views of finance, econometrics
>>> and time series.
>>>
>>> As well as the following to very helpful books (aside of the standard
>>> wavelet literature):
>>>
>>> (2001) An Introduction to Wavelets and Other Filtering Methods in
>>> Finance and Economics.
>>> Wavelet Methods in Statistics with R [ would be looking for a newer
>>> reference ... .. ]
>>> (2008) Wavelet Methods in Statistics with R
>>>
>>>
>>> THANK you very much for any help or pointers!!
>>>
>>>
>>> With the best regards to all,
>>> Steve
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>
>>>       
>>     
>
>


From markleeds at verizon.net  Wed Jul 15 16:38:28 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 15 Jul 2009 09:38:28 -0500 (CDT)
Subject: [R-SIG-Finance] comparison of trading rules
Message-ID: <169578133.1178710.1247668708054.JavaMail.root@vms227.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090715/6e21362d/attachment.html>

From stvienna at gmail.com  Wed Jul 15 18:24:10 2009
From: stvienna at gmail.com (stvienna wiener)
Date: Wed, 15 Jul 2009 18:24:10 +0200
Subject: [R-SIG-Finance] financial series: waveslim, brainwaver,
	wavetresh 	and fractal
In-Reply-To: <4A5DE3F9.8030808@prodsyse.com>
References: <b9bcd4e10907090800v1bcd0d3cr5ad68a041755b3fd@mail.gmail.com>
	<4A593D45.6060100@prodsyse.com>
	<b9bcd4e10907150519y475b0dd7jb33a79d11b76a8aa@mail.gmail.com>
	<4A5DE3F9.8030808@prodsyse.com>
Message-ID: <b9bcd4e10907150924t748d8edal150a8765d55da71f@mail.gmail.com>

Hi,


thank you again. It seems I got another function in "waveslim" working
to find change points.
The function is called "testing.hov". The problem was there is not
example provided.
I think my code works now.

So very happy at the moment!

Regards,
Steve


P.S.: In case someone is interested in the code or sees any errors:

 ## Finding Change-Points

  # ! ZERO GUARANTEE that this is useful or correct !

  # For more info look at the book in chaper 7.3 / page 247:
  # Gencay, Selcuk, Whitcher (2001) An Introduction to Wavelets
  #  and Other Filtering Methods in Finance and Economics

  # load lib
  library(waveslim)
  # if not installed, run:   install.packages("waveslim")

  # ibm stock data, calculate returns (used in finance)
  data(ibm)
  ibm.returns <- diff(log(ibm))

  # pad series with zeros to get length 512 (for wavelet-transformation)
  lenibm <- length(ibm.returns)
  ibmnew <- 1:512
  ibmnew[1:512] <- 0
  ibmnew[1:lenibm] <- ibm.returns[1:lenibm]


  J <- 4 # wavelet decomposition up to scale 4

  # not sure if this is a good choice:
  wf <- "la8" # Daubechies orthonormal Wavelet of length 8


  # Locate Changepoint in ibm series
  # find significant changpoints considering the wavelet-variance of the series

  # (calculated at different scales, at scale number one there are 2
changepoints,
  # higher scales have no significant changepoints)

  changepoints <- testing.hov(ibmnew, wf, J=4, debug=TRUE)

  # plot series
  ts.plot(ibmnew)

  # draw lines at change points
  changepoint1 <- changepoints[1,4]
  changepoint2 <- changepoints[2,4]
  abline(v=c(changepoint1),lty=3, col="red")
  abline(v=c(changepoint2),lty=3, col="red")

  # you should see now a time series with two
  # red lines approx. at time 243 and 378



2009/7/15 spencerg <spencer.graves at prodsyse.com>:
> Hello, Steve:
>
> ? ? I see two options:
>
> ? ? ? ? ?1. ?Send an email to the package authors / mainteners to ask about
> the status. ?I found an email address using "RSiteSearch('WaveCGH')". ?If
> that email fails, you could try to find them via Google.
>
> ? ? ? ? ?2. ?You cited a web page that stated that, "Package ?WaveCGH? was
> removed from the CRAN repository." ?That web page also says, "Formerly
> available versions can be obtained from the archive." If you click on the
> word "archive", it should take you to a web page from which you can download
> "WaveCGH_2.01.tar.gz". ?This is a source file for the package. ?With this,
> you can try "R CMD INSTALL WaveCGH_2.01.tar.gz" at a commands prompt. ?This
> will work under Windows IF you have Rtools installed, and I believe it will
> work under Linux. ?From within R, I believe
> "install.packages('WaveCGH_2.01.tar.gz')" may also work. ?If that fails, you
> could try another post to this list.
>
> ? ? Hope this helps. ? ? Spencer Graves
>
>
> stvienna wiener wrote:
>>
>> Hello Spencer,
>>
>>
>> thank you for your reply.
>>
>> I tried to install the package "WaveCGH" today, however I got an error.
>>
>>
>>>
>>> install.packages("WaveCGH")
>>>
>>
>> In getDependencies(pkgs, dependencies, available, lib) :
>> ?package ?WaveCGH? is not available
>>
>>
>> Its somehow strange, because on the R-Website:
>> http://cran.r-project.org/web/packages/WaveCGH/
>>
>> It is written:
>> "Package ?WaveCGH? was removed from the CRAN repository."
>>
>> So I think the possibilities are:
>> a) the package was remove from CRAN
>> b) there are some dependencies. But I installed the newest R version!
>> c) I spelled the package wrong (e.g. small/capital letters )
>>
>>
>> Do you have maybe an idea?
>>
>>
>> Currently I am planning to use the following packages:
>> ?library(R.utils)
>>
>> ?library(tseries)
>> ?library(wmtsa) ? ? ? ?#Wavelets
>> ?library(waveslim) ? ? #Wavelet Correlation
>> ?library(brainwaver) ? #Wavelet Correlation
>>
>> ?library(QRMlib) ? ? ? # time series plotting
>>
>> ? library(wavethresh) ?# wavelet thresholding
>> ? library(CVThresh) ? ?# wavelet thresholding
>>
>>
>> Any suggestions welcome ;-)
>>
>> Regards,
>> Steve
>>
>>
>>
>> 2009/7/12 spencerg <spencer.graves at prodsyse.com>:
>>
>>>
>>> I can't help you directly, but have you tried "RSiteSearch.function" in
>>> the
>>> "RSiteSearch" package? ?Consider the following:
>>>
>>> library(RSiteSearch)
>>>
>>> fcp <- RSiteSearch.function('fractal changepoint')
>>> fcps <- RSiteSearch.function('fractal changepoints')
>>>
>>> wcp <- RSiteSearch.function('wavelet changepoint')
>>> wcps <- RSiteSearch.function('wavelet changepoints')
>>> HTML(wcp)
>>>
>>>
>>> ? ?This sequence returned 1 match for the third query, and HTML(wcp)
>>> displayed that match in a web browser with a link to, in this case, the
>>> index page for the "WaveCGH" package.
>>>
>>> ? ?Hope this helps. ? ? Spencer Graves
>>> stvienna wiener wrote:
>>>
>>>>
>>>> Greetings to all!
>>>>
>>>>
>>>> # Short-Version
>>>> I am looking for advanced R packages similar to "waveslim",
>>>> "brainwaver", "wavethresh", "fractal", etc.
>>>> in order to analyze financial times series (e.g. stock prices).
>>>>
>>>>
>>>> Currently I am writing code to show/demonstrate the use of wavelets
>>>> and fractal dimension
>>>> to analyze financial time series (its a school project for my master
>>>> degree in CS).
>>>>
>>>> The purpose is, for example, to find a correlation between IBM stock
>>>> prices and the dow jones index. Or to find a changepoint in
>>>> stock prices, correlating to unusual events e.g Yahoo stock prices in
>>>> 2008
>>>> while
>>>> there was an acquisition attempt.
>>>>
>>>> I have been using the "brainwaver" package to calculate correlations
>>>> of us stock prices to the dow jones.
>>>> It seems to work, but I am not 100% sure. (could provide so code but
>>>> its just fragments at the moment)
>>>>
>>>> So my questions at the moment would be:
>>>> 1) ?Is there a specific R package that would be interesting and
>>>> helpful that I missed?
>>>> 2) ?Is there a package to find "changepoints" in time series with
>>>> wavelets or fractal-approach?
>>>> I know there is a function in "waveslim" to find a changepoint in the
>>>> wavelet-variance,
>>>> but I could not yet get it working.
>>>> If not is there a way by other methods e.g. bayesian interference?
>>>>
>>>>
>>>> As references I was looking at the task views of finance, econometrics
>>>> and time series.
>>>>
>>>> As well as the following to very helpful books (aside of the standard
>>>> wavelet literature):
>>>>
>>>> (2001) An Introduction to Wavelets and Other Filtering Methods in
>>>> Finance and Economics.
>>>> Wavelet Methods in Statistics with R [ would be looking for a newer
>>>> reference ... .. ]
>>>> (2008) Wavelet Methods in Statistics with R
>>>>
>>>>
>>>> THANK you very much for any help or pointers!!
>>>>
>>>>
>>>> With the best regards to all,
>>>> Steve
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>>>
>>>
>>>
>>
>>
>
>


From nodecorum at yahoo.com  Wed Jul 15 20:12:50 2009
From: nodecorum at yahoo.com (tradenet)
Date: Wed, 15 Jul 2009 11:12:50 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] rmetrics portfolio backtesting
 limitations question
In-Reply-To: <4A5A7863.4070904@prodsyse.com>
References: <24417340.post@talk.nabble.com> <4A5A7863.4070904@prodsyse.com>
Message-ID: <24503164.post@talk.nabble.com>


Thank you Spencer.  I'll proceed according to your suggestions.

Regards,

Andrew


spencerg wrote:
> 
> Hello, Andrew: 
> 
> 
>       I'm not familiar with either "portfolioBacktest" nor 
> "setWindowsHorizon".  Moreover, using RSiteSearch produced nothing for 
> either.  You might get more help from this list if you don't require 
> respondents to understand these terms. 
> 
> 
>       Have you considered using the "debug" function to walk through 
> code line by line, looking at what it does, changing things at will?  
> You can often learn enough doing this to see what you need to do to get 
> it to do what you want, provided there is enough information in the 
> data.  By doing this, you should be able to get the weights outputted in 
> many different ways. 
> 
> 
>       However, I'd look carefully at any algorithms that produced 
> distinct portfolio weights for many assets.  As a check, I suggest you 
> compute "eigen" of "corr" on your favorite 50 assets and look at 
> "eigen(...)$values".  If the smallest eigenvalue exceeds, say, 0.0001 
> times the largest, you are probably OK.  Otherwise, your use of 
> individually estimated weights could be worse than using constant 
> weights. In particular, if you have fewer than 50 observations, the 
> smallest eigenvalue may be negative, which says that that portion of the 
> variability, and probably more than that, is driven by round-off error.  
> I'd rather not base investment decisions on round-off. 
> 
> 
>       Hope this helps. 
>       Spencer Graves
> p.s.  Are you aware that you can get the source code for any CRAN 
> package?  For example, the source for the "timeDate" package is 
> available in a file "timeDate_290.85.tar.gz" downloadable from 
> "http://cran.fhcrc.org/web/packages/timeDate/index.html".  If the people 
> who wrote a particular functions included comments in their code, they 
> will appear in the *.tar.gz file but not in the version you get by 
> typing the function name. 
> 
> 
> tradenet wrote:
>> I have been working with the portfolioBacktesting function in Rmetrics
>> and it
>> seems to be a very powerful and useful function.  There seems to be some
>> limitations/bugs that seriously limits the utility of the function and I
>> was
>> hoping someone has found workarounds:
>>
>> 1.) If I specify a large number of assets, say 50, in the formula, e.g.
>> SPX
>> ~ asset1 + asset2....+asset50
>> then it seems only the first 29 assets are used in the analysis. 
>> ncol(backtestPortfolios$weights) always returns 29 and assets near the
>> end
>> of the list get nonzero weights if I move their names to the beginning of
>> the formula string, otherwise those assets do not appear in the results. 
>> Is
>> there a way to NOT compare results to a benchmark?  I'd like to just
>> supply
>> a timeseries of asset returns for a date range and run the analysis using
>> all assets in the timeseries as portfolio candidates.
>>
>> BTW, if anyone knows an easy way to dump out the weights to a file I'd
>> appreciate the insight
>>
>> 2.) the 12m window horizon seems to be the only one that works:
>> setWindowsHorizon(backtestBT)<-"12m" works, but <-"3m" yields an error
>> message.
>>
>> I thought this might have to do with the lambda smoothing, so I set the
>> smoothing to "1m" and still no go
>>
>> 3.) does anyone know how to turn smoothing completely off?  I would like
>> to
>> see the raw, unsmoothed weights
>>
>> 4.) is there anyway to plot the results without performing weight
>> smoothing
>> -- I don't want to have to do smoothing to look at all the great plots
>> and
>> results.
>>
>> Rmetrics is great stuff, so close to being beyond perfect!  I am very
>> grateful to the developers and the community.
>>
>> Warm regards,
>>
>> Andrew
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/rmetrics-portfolio-backtesting-limitations-question-tp24417340p24503164.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From Achim.Zeileis at wu-wien.ac.at  Wed Jul 15 23:56:31 2009
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 15 Jul 2009 23:56:31 +0200 (CEST)
Subject: [R-SIG-Finance] Reminder: CFE 2009 Submission Deadline
Message-ID: <Pine.LNX.4.64.0907152355240.16652@paninaro.stat-math.wu-wien.ac.at>

Dear useRs,

just a quick reminder that the

   Submission Deadline: 2009-07-17

for the 3rd International Conference on

   Computational and Financial Econometrics (CFE'09)
   October 29-31, 2009
   Grand Resort Hotel
   Limassol, Cyprus
   http://www.dcs.bbk.ac.uk/cfe09/

is approaching. Invited speakers include Christophe Croux, Siem Jan Koopman, 
and Neil Shephard. If you're interested in the workshop, please consider 
submitting an abstract about computational econometrics and finance in R.

Best wishes,
Z


From robert at sanctumfi.com  Thu Jul 16 13:34:55 2009
From: robert at sanctumfi.com (Robert Sams)
Date: Thu, 16 Jul 2009 12:34:55 +0100
Subject: [R-SIG-Finance] comparison of trading rules
References: <SANCTUMFISERVERBNEZ00001a80@sanctumfi.com>
Message-ID: <SANCTUMFISERVERKAy400001eb7@sanctumfi.com>

Hi Murali,

Trading system performance is a subtle and multi-dimensional subject.
There is no simple metric or suite of metrics that can answer the
question "does system X have a better return-for-risk than system Y"? A
couple of key concepts to consider are:

1) Robustness. Widely used measures like sharpe ratio, max drawdown, etc
are NOT robust. They are very sensitive to the sample period used and
comparisons based on such metrics are not reliable. So, I think
pondering the subject of robust estimators is fundamental to trading
system evaluation.

2) Leverage. Cumulative return on a trading system is a geometric
process, so return and risk are not linear functions of leverage (as is
sometimes thought) but parabolic. Return increases with increased
leverage up to a point and then declines thereafter. Even a system with
a great positive edge can have negative overall performance if it is
over-leveraged. The leveraging assumptions behind two trading systems
must be normalised somehow for a comparison of returns to be meaningful.
Leverage is a ramified subject and raises many important questions in
trading system evaluation. In the literature it sometimes goes under the
title 'money management.'

As for your specific example of two systems with different percentages
of time out of the market, I'm not really sure what you're after. You
mentioned standard deviation. Presumably you would exclude those
observations where the system is out of the market, right? If we're
working with the assumption that you can dynamically invest (withdrawal)
capital from a strategy as it enters (exits) the market, then we
probably don't care about those zero-return periods in annualising risk
and return metrics. We might care about what happens when the system
isn't in the market from the perspective of understanding what
contribution the system's exit rules make to its overall performance,
but that's another matter. 

In short, you need to have a clear idea about what you're trying to
'compare', what questions you are trying to answer.

I hope this helps.

Robert


> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch 
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of 
> Murali.MENON at fortisinvestments.com
> Sent: 15 July 2009 10:55
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] comparison of trading rules
> 
> Folks,
>  
> As far as I can tell, the comparison of competing trading 
> rules seems to be done on the basis of such metrics as Sharpe 
> ratio, drawdowns, hit ratios and the like.
>  
> I wonder if these are entirely fair comparisons.
>  
> For example, if one strategy results in active positions 90% 
> of the time, while the other is active only 50% of the time, 
> the standard deviation of the latter strategy might be lower 
> than the former because of the longer stretches of zeroes in 
> the returns. Should a further adjustment be made for the 
> inactive periods? Would this be something as simple as 
> dividing by the square root of the number of inactive zeroes?
>  
> Are there any other ways of comparing the performance of 
> strategies that you can suggest? Any related literature?
>  
> Thanks,
>  
> Murali
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 


From windspeedo99 at gmail.com  Thu Jul 16 14:38:49 2009
From: windspeedo99 at gmail.com (Wind)
Date: Thu, 16 Jul 2009 20:38:49 +0800
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
Message-ID: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>

Prof. Sornette has spent years forcasting bubble burst with
"log-periodic power law".    The latest paper  gives "a
self-consistent model for explosive financial bubbles, which combines
a mean-reverting volatility process and a stochastic conditional
return which reflects nonlinear positive feedbacks and continuous
updates of the investors' beliefs and sentiments."

And his  latest  predicting is the burst of Chinese equity bubble at
the end of July.     http://arxiv.org/abs/0907.1827

While waiting to see the result, I wonder whether it is possible to
replicate the forcast with R.  The model is in the page 10 of the "A
Consistent Model of `Explosive' Financial Bubbles With Mean-Reversing
Residuals",  http://arxiv.org/abs/0905.0128  .   The output chart is
in the page 3 of "The Chinese Equity Bubble: Ready to Burst",
http://arxiv.org/abs/0907.1827 .   I guess the authors of the latter
paper use the same model as described in the first paper.

Because statistics is still challenging for me though I could use R
for  basic data manipulations,  I wonder which package or function
would be necessary to implement the model in the paper.  The model
seems more complicated than the models in the R tutorials for me.
By the way, the author of the paper used Python and the codes are
private.

Any suggestion would be highly appreciated.

Wind


From brian at braverock.com  Thu Jul 16 15:25:12 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 16 Jul 2009 08:25:12 -0500
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
In-Reply-To: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>
References: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>
Message-ID: <4A5F2A38.9060100@braverock.com>

So first, using your real name and ideally your professional identity, 
ask for the python code.  Better yet, get an academic buddy to do it. 
Usually getting access to the code isn't too tough.  Mention things like 
"repeatable research" and "collaboration" in your email.  Two of the 
authors publish their email addresses in one of the papers you 
reference, so contacting them should be easy.

Next port the python code to R.

If you can't do that, then replicate the model in R "from scratch".  A 
trivial scan of the paper in question lends several techniques that are 
well covered in R: AR, GARCH, power laws, linear regression, stochastic 
discount factor, Ornstein-Uhlenbeck, etc. 

There are volumes of information available on these topics from within 
R, in numerous books, and in the archives of this mailing list and r-help.

You're going to have to do your replication in pieces, probably starting 
with their implementation of the log periodic power law (LPPL), for 
which I do not believe there is an existing direct analogue in R though 
all the component parts necessary to replicate it should be readily 
available.

As you work on each step of the replication, share your code with this 
list and the problems you are having with a particular step.  Ask 
specific, directed questions with code to back them up.  Someone will 
likely help you solve the specific problem.

In R generally, it is not necessary that you be able to *do* the math 
(think pencil and paper), but if you plan to replicate published work, 
it will be necessary to *understand* at least some of how the math 
works, and to be able to pick out the names of techniques that you can 
search for an utilize.

Basically, I'm recommending that you (specifically) and others (more 
generally) should share the process of replicating a technique like 
this, as well as the final product, to give all the rest of us who are 
likely to be helping "you" get all this done. quid pro quo.

Cheers,

  - Brian


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock



Wind wrote:
> Prof. Sornette has spent years forcasting bubble burst with
> "log-periodic power law".    The latest paper  gives "a
> self-consistent model for explosive financial bubbles, which combines
> a mean-reverting volatility process and a stochastic conditional
> return which reflects nonlinear positive feedbacks and continuous
> updates of the investors' beliefs and sentiments."
>
> And his  latest  predicting is the burst of Chinese equity bubble at
> the end of July.     http://arxiv.org/abs/0907.1827
>
> While waiting to see the result, I wonder whether it is possible to
> replicate the forcast with R.  The model is in the page 10 of the "A
> Consistent Model of `Explosive' Financial Bubbles With Mean-Reversing
> Residuals",  http://arxiv.org/abs/0905.0128  .   The output chart is
> in the page 3 of "The Chinese Equity Bubble: Ready to Burst",
> http://arxiv.org/abs/0907.1827 .   I guess the authors of the latter
> paper use the same model as described in the first paper.
>
> Because statistics is still challenging for me though I could use R
> for  basic data manipulations,  I wonder which package or function
> would be necessary to implement the model in the paper.  The model
> seems more complicated than the models in the R tutorials for me.
> By the way, the author of the paper used Python and the codes are
> private.
>
> Any suggestion would be highly appreciated.
>


From windspeedo99 at gmail.com  Thu Jul 16 16:03:41 2009
From: windspeedo99 at gmail.com (Wind)
Date: Thu, 16 Jul 2009 22:03:41 +0800
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
In-Reply-To: <4A5F2A38.9060100@braverock.com>
References: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>
	<4A5F2A38.9060100@braverock.com>
Message-ID: <d718c8210907160703red62523y943f0711d6f903b9@mail.gmail.com>

Thanks Brian.  You're always warm-hearted and very professional.
I will try my best following your detailed instructions.   It would be
a great improvement for myself if I could work out the final solution
with the help of the list.    Since I am just an independent
individual investor with only master degree in finance,  I guess it
would take some time.
Thanks again.   I will post the progress to the list if there is some
progress :)

wind

On Thu, Jul 16, 2009 at 9:25 PM, Brian G. Peterson<brian at braverock.com> wrote:
> So first, using your real name and ideally your professional identity, ask
> for the python code. ?Better yet, get an academic buddy to do it. Usually
> getting access to the code isn't too tough. ?Mention things like "repeatable
> research" and "collaboration" in your email. ?Two of the authors publish
> their email addresses in one of the papers you reference, so contacting them
> should be easy.
>
> Next port the python code to R.
>
> If you can't do that, then replicate the model in R "from scratch". ?A
> trivial scan of the paper in question lends several techniques that are well
> covered in R: AR, GARCH, power laws, linear regression, stochastic discount
> factor, Ornstein-Uhlenbeck, etc.
> There are volumes of information available on these topics from within R, in
> numerous books, and in the archives of this mailing list and r-help.
>
> You're going to have to do your replication in pieces, probably starting
> with their implementation of the log periodic power law (LPPL), for which I
> do not believe there is an existing direct analogue in R though all the
> component parts necessary to replicate it should be readily available.
>
> As you work on each step of the replication, share your code with this list
> and the problems you are having with a particular step. ?Ask specific,
> directed questions with code to back them up. ?Someone will likely help you
> solve the specific problem.
>
> In R generally, it is not necessary that you be able to *do* the math (think
> pencil and paper), but if you plan to replicate published work, it will be
> necessary to *understand* at least some of how the math works, and to be
> able to pick out the names of techniques that you can search for an utilize.
>
> Basically, I'm recommending that you (specifically) and others (more
> generally) should share the process of replicating a technique like this, as
> well as the final product, to give all the rest of us who are likely to be
> helping "you" get all this done. quid pro quo.
>
> Cheers,
>
> ?- Brian
>
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
>
>
> Wind wrote:
>>
>> Prof. Sornette has spent years forcasting bubble burst with
>> "log-periodic power law". ? ?The latest paper ?gives "a
>> self-consistent model for explosive financial bubbles, which combines
>> a mean-reverting volatility process and a stochastic conditional
>> return which reflects nonlinear positive feedbacks and continuous
>> updates of the investors' beliefs and sentiments."
>>
>> And his ?latest ?predicting is the burst of Chinese equity bubble at
>> the end of July. ? ? http://arxiv.org/abs/0907.1827
>>
>> While waiting to see the result, I wonder whether it is possible to
>> replicate the forcast with R. ?The model is in the page 10 of the "A
>> Consistent Model of `Explosive' Financial Bubbles With Mean-Reversing
>> Residuals", ?http://arxiv.org/abs/0905.0128 ?. ? The output chart is
>> in the page 3 of "The Chinese Equity Bubble: Ready to Burst",
>> http://arxiv.org/abs/0907.1827 . ? I guess the authors of the latter
>> paper use the same model as described in the first paper.
>>
>> Because statistics is still challenging for me though I could use R
>> for ?basic data manipulations, ?I wonder which package or function
>> would be necessary to implement the model in the paper. ?The model
>> seems more complicated than the models in the R tutorials for me.
>> By the way, the author of the paper used Python and the codes are
>> private.
>>
>> Any suggestion would be highly appreciated.
>>
>
>
>


From breman.mark at gmail.com  Thu Jul 16 16:15:11 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Thu, 16 Jul 2009 16:15:11 +0200
Subject: [R-SIG-Finance] Backtesting trade systems
Message-ID: <5e6a2e670907160715x4c4c77f8m396557bbda267e92@mail.gmail.com>

Hello,
I have spend quit some time now looking for a package that allows me to
backtest (technical) trading systems based on single financial instruments
with R.

I had a look at Rmetrics, blotter, fTrading, PerformanceAnalytics, backtest,
quantmod, TTR etc, but not one of these fill my requirements. It's not that
they are not usefull, on the contrary, they are all filled with terrific
statistical stuff, but it's not the simple, practical and straightforward
approach that I am looking for as a trader rather than as a statisticus.

So I have decided to build my own solution, reusing as much as possible from
these existing packages. (As a former software engineer I know how much time
and effort goes into buiding reliable software, so the more reuse the
better). As I am quite new to R and statistics in general, there is a lot to
learn for me here...

What I have build so far is a very basic set of functions called
"tradesim.R" (I have attached it to this post). A very basic example of how
these functions can be used for a backtest-run can be found in
"tradesim_example.R". The example runs a backtest with end-of-day data from
AAPL, using a (rather poor) trading system based on the RSI indicator (from
the TTR package).

Now I have read in some older post on this list that others were also
searching for a backtesting package. I even read a post proposing to start a
group effort creating such a package. I suspect that some of you might be
interested in what I made so far and maybe would like to put in a effort
creating such a package together. I certainly know that it's a lot easier to
create good software as a group, rather than by a single person...

So if you are interested have a look at what I got so far and let me know
what you think.

Regards,

-Mark-
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090716/b29a70de/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: tradesim.R
Type: application/octet-stream
Size: 4743 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090716/b29a70de/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: tradesim_example.R
Type: application/octet-stream
Size: 521 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090716/b29a70de/attachment-0001.obj>

From adrian_d at eskimo.com  Thu Jul 16 16:28:23 2009
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Thu, 16 Jul 2009 07:28:23 -0700 (PDT)
Subject: [R-SIG-Finance] Backtesting trade systems
In-Reply-To: <5e6a2e670907160715x4c4c77f8m396557bbda267e92@mail.gmail.com>
References: <5e6a2e670907160715x4c4c77f8m396557bbda267e92@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0907160718390.25985@shell.eskimo.com>


I have been working on such a package too.  I am currently testing it, and 
was planning to have a small user group release in about a month or so.

>From the vignette introduction (that I am still expanding):

The goal of the package {\tt\textbf{AlgoTrader}} is to provide such a
framework using an object oriented approach\footnote{See Section
   \ref{appendixProto} for a discussion on the design decision.}.  There
are two main classes.  An {\tt Algo} class from which all user
algorithms inherit and a {\tt Trader} class used for performace
tracking and reporting.  By convention, the parent classes {\tt Algo}
and {\tt Trader} are capitalized, while lower case names, say {\tt
   algo1} and {\tt trader1} are used for instances of these classes.
Conceptually, a model is represented by an algorithm and a trader.

The functionality provided in this package allows the user to define
multiple trading algorithms.  He could gradually refine these
algorithms by using the inheritance mechanism, or use the same
algorithms with different parameters to instantiate different
\code{Algo} objects.

The \code{Algo} object contains all the historical data necessary, a
set of parameters, functions that generate trading signals, and a set
of functions that modify the positions given a trading signal.  Using
an algorithm {\tt algo}, the user can create a {\tt trader} that uses
the algoritm over a specified time interval to arrive at a trading
decision for each time step.  A trade log is kept by the trader, so
that an analysis of the trading performance can be done.  The
\code{trader} has different methods to summarize/plot/evaluate his
performance.  For strategy selection or for tuning the algorithm
parameters, the performance of several traders can be compared side by
side.  As the \code{AlgoTrader} package does backtesting, it is best
if only deterministic trading algorithms are used so performance
results are reproducible.

Let me know if you want to take part in testing.

Thanks,
Adrian

On Thu, 16 Jul 2009, Mark Breman wrote:

> Hello,
> I have spend quit some time now looking for a package that allows me to
> backtest (technical) trading systems based on single financial instruments
> with R.
>
> I had a look at Rmetrics, blotter, fTrading, PerformanceAnalytics, backtest,
> quantmod, TTR etc, but not one of these fill my requirements. It's not that
> they are not usefull, on the contrary, they are all filled with terrific
> statistical stuff, but it's not the simple, practical and straightforward
> approach that I am looking for as a trader rather than as a statisticus.
>
> So I have decided to build my own solution, reusing as much as possible from
> these existing packages. (As a former software engineer I know how much time
> and effort goes into buiding reliable software, so the more reuse the
> better). As I am quite new to R and statistics in general, there is a lot to
> learn for me here...
>
> What I have build so far is a very basic set of functions called
> "tradesim.R" (I have attached it to this post). A very basic example of how
> these functions can be used for a backtest-run can be found in
> "tradesim_example.R". The example runs a backtest with end-of-day data from
> AAPL, using a (rather poor) trading system based on the RSI indicator (from
> the TTR package).
>
> Now I have read in some older post on this list that others were also
> searching for a backtesting package. I even read a post proposing to start a
> group effort creating such a package. I suspect that some of you might be
> interested in what I made so far and maybe would like to put in a effort
> creating such a package together. I certainly know that it's a lot easier to
> create good software as a group, rather than by a single person...
>
> So if you are interested have a look at what I got so far and let me know
> what you think.
>
> Regards,
>
> -Mark-
>


From robert at sanctumfi.com  Thu Jul 16 16:41:25 2009
From: robert at sanctumfi.com (Robert Sams)
Date: Thu, 16 Jul 2009 15:41:25 +0100
Subject: [R-SIG-Finance] Backtesting trade systems
References: <SANCTUMFISERVER69RK00001ffa@sanctumfi.com>
Message-ID: <SANCTUMFISERVERq6Dq00002024@sanctumfi.com>

Hi Mark,
 
I have started a package called tradesys which, I think, is a clean
solution. The project is registered on r-forge
https://r-forge.r-project.org/projects/tradesys/ and the initial code
base with documentation will be checked-in by the end of the day London
time. As a taster:
 
> library(tradesys)
> data(spx)
> tail(spx)
             Open   High    Low  Close     Volume
2009-05-12 910.52 915.57 896.46 908.35 6871750400
2009-05-13 905.40 905.40 882.80 883.92 7091820000
2009-05-14 884.24 898.36 882.52 893.07 6134870000
2009-05-15 892.76 896.97 878.94 882.88 5439720000
2009-05-18 886.07 910.00 886.07 909.71 5702150000
2009-05-19 909.67 916.39 905.22 908.13 6616270000
> x <- tradesys(spx, el=MA(Close, 60) > MA(Close, 120), es=MA(Close, 60)
<= MA(Close, 120))
> tail(trades(x, uselog=TRUE))
    phase      etime      xtime time nobs  eprice  xprice    pnl
ror
107    EL 2006-09-21 2007-09-12  356  244 1324.89 1471.10 146.21
0.264117052
108    ES 2007-09-12 2007-11-09   58   42 1471.10 1467.59   3.51
0.006027153
109    EL 2007-11-09 2008-01-03   55   36 1467.59 1447.55 -20.04
-0.034689959
110    ES 2008-01-03 2008-06-10  159  109 1447.55 1358.98  88.57
0.159301490
111    EL 2008-06-10 2008-07-21   41   28 1358.98 1261.82 -97.16
-0.187159273
112    ES 2008-07-21 2009-05-19  302  209 1261.82  909.67 352.15
0.825619186
> tail(equity(x, uselog=TRUE))
           trade states    delta    price          ror   equity
2009-05-12   112     -1 1.331220 6.814016  0.018107970 38.02601
2009-05-13   112     -1 1.307543 6.808377  0.007373275 38.30638
2009-05-14   112     -1 1.297973 6.784729  0.030694874 39.48219
2009-05-15   112     -1 1.259318 6.794318 -0.012075942 39.00541
2009-05-18   112     -1 1.274712 6.786796  0.009588169 39.37940
2009-05-19   112     -1 1.262606 6.813082 -0.033188777 38.07244
 
Please checkout and play with the code at your leisure. Anyone
interested in write-access to the repository should contact me directly.


Robert 


________________________________

	From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Mark
Breman
	Sent: 16 July 2009 15:15
	To: r-sig-finance at stat.math.ethz.ch
	Subject: [R-SIG-Finance] Backtesting trade systems
	
	
	Hello, 

	I have spend quit some time now looking for a package that
allows me to backtest (technical) trading systems based on single
financial instruments with R.

	I had a look at Rmetrics, blotter, fTrading,
PerformanceAnalytics, backtest, quantmod, TTR etc, but not one of these
fill my requirements. It's not that they are not usefull, on the
contrary, they are all filled with terrific statistical stuff, but it's
not the simple, practical and straightforward approach that I am looking
for as a trader rather than as a statisticus.

	So I have decided to build my own solution, reusing as much as
possible from these existing packages. (As a former software engineer I
know how much time and effort goes into buiding reliable software, so
the more reuse the better). As I am quite new to R and statistics in
general, there is a lot to learn for me here...
	
	
	What I have build so far is a very basic set of functions called
"tradesim.R" (I have attached it to this post). A very basic example of
how these functions can be used for a backtest-run can be found in
"tradesim_example.R". The example runs a backtest with end-of-day data
from AAPL, using a (rather poor) trading system based on the RSI
indicator (from the TTR package). 

	Now I have read in some older post on this list that others were
also searching for a backtesting package. I even read a post proposing
to start a group effort creating such a package. I suspect that some of
you might be interested in what I made so far and maybe would like to
put in a effort creating such a package together. I certainly know that
it's a lot easier to create good software as a group, rather than by a
single person...

	So if you are interested have a look at what I got so far and
let me know what you think.

	Regards,

	-Mark- 

	   


From breman.mark at gmail.com  Thu Jul 16 16:56:23 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Thu, 16 Jul 2009 16:56:23 +0200
Subject: [R-SIG-Finance] Backtesting trade systems
In-Reply-To: <SANCTUMFISERVERq6Dq00002024@sanctumfi.com>
References: <SANCTUMFISERVER69RK00001ffa@sanctumfi.com>
	<SANCTUMFISERVERq6Dq00002024@sanctumfi.com>
Message-ID: <5e6a2e670907160756p16ed8a1qa32b51598e9fa468@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090716/906fbce7/attachment.pl>

From Murali.MENON at fortisinvestments.com  Thu Jul 16 18:05:02 2009
From: Murali.MENON at fortisinvestments.com (Murali.MENON at fortisinvestments.com)
Date: Thu, 16 Jul 2009 18:05:02 +0200
Subject: [R-SIG-Finance] comparison of trading rules
In-Reply-To: <SANCTUMFISERVERKAy400001eb7@sanctumfi.com>
References: <SANCTUMFISERVERBNEZ00001a80@sanctumfi.com>
	<SANCTUMFISERVERKAy400001eb7@sanctumfi.com>
Message-ID: <5A3D018CBDC36B4F8FF6DB52DDF3B82DBFE684@BE-S0500-V22.adroot.local>

Hi Robert, Mark, Matthias,

Thanks for your responses.

I agree that measures such as IR are not robust, but for whatever
reason, practitioners seem to stick to these. They look at year-to-date
and 1yr and 3yr horizons, and seem to decide that one performance was
better than the other across these horizons. In such a comparison, they
would take daily returns, say, irrespective of whether the strategy was
active or not, and then do a straight (mean - benchmark_mean) / std dev.
Which is why I asked the question if this comparison was fair.

Regarding leverage: if both strategies are scaled to the same risk
budget and invest in the same underlying assets, they should have
roughly similar levels of leverage, I guess? 

Are you saying that over-leverage affects performance because of
increased transaction costs? Or is there some other reason for the
parabolic function?

Cheers,
Murali


-----Original Message-----
From: Robert Sams [mailto:robert at sanctumfi.com] 
Sent: 16 July 2009 12:35
To: MENON Murali; r-sig-finance at stat.math.ethz.ch
Subject: RE: [R-SIG-Finance] comparison of trading rules

Hi Murali,

Trading system performance is a subtle and multi-dimensional subject.
There is no simple metric or suite of metrics that can answer the
question "does system X have a better return-for-risk than system Y"? A
couple of key concepts to consider are:

1) Robustness. Widely used measures like sharpe ratio, max drawdown, etc
are NOT robust. They are very sensitive to the sample period used and
comparisons based on such metrics are not reliable. So, I think
pondering the subject of robust estimators is fundamental to trading
system evaluation.

2) Leverage. Cumulative return on a trading system is a geometric
process, so return and risk are not linear functions of leverage (as is
sometimes thought) but parabolic. Return increases with increased
leverage up to a point and then declines thereafter. Even a system with
a great positive edge can have negative overall performance if it is
over-leveraged. The leveraging assumptions behind two trading systems
must be normalised somehow for a comparison of returns to be meaningful.
Leverage is a ramified subject and raises many important questions in
trading system evaluation. In the literature it sometimes goes under the
title 'money management.'

As for your specific example of two systems with different percentages
of time out of the market, I'm not really sure what you're after. You
mentioned standard deviation. Presumably you would exclude those
observations where the system is out of the market, right? If we're
working with the assumption that you can dynamically invest (withdrawal)
capital from a strategy as it enters (exits) the market, then we
probably don't care about those zero-return periods in annualising risk
and return metrics. We might care about what happens when the system
isn't in the market from the perspective of understanding what
contribution the system's exit rules make to its overall performance,
but that's another matter. 

In short, you need to have a clear idea about what you're trying to
'compare', what questions you are trying to answer.

I hope this helps.

Robert


> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of 
> Murali.MENON at fortisinvestments.com
> Sent: 15 July 2009 10:55
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] comparison of trading rules
> 
> Folks,
>  
> As far as I can tell, the comparison of competing trading rules seems 
> to be done on the basis of such metrics as Sharpe ratio, drawdowns, 
> hit ratios and the like.
>  
> I wonder if these are entirely fair comparisons.
>  
> For example, if one strategy results in active positions 90% of the 
> time, while the other is active only 50% of the time, the standard 
> deviation of the latter strategy might be lower than the former 
> because of the longer stretches of zeroes in the returns. Should a 
> further adjustment be made for the inactive periods? Would this be 
> something as simple as dividing by the square root of the number of 
> inactive zeroes?
>  
> Are there any other ways of comparing the performance of strategies 
> that you can suggest? Any related literature?
>  
> Thanks,
>  
> Murali
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 


From jeff.a.ryan at gmail.com  Thu Jul 16 18:32:31 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 16 Jul 2009 11:32:31 -0500
Subject: [R-SIG-Finance] Backtesting trade systems
In-Reply-To: <SANCTUMFISERVERq6Dq00002024@sanctumfi.com>
References: <SANCTUMFISERVER69RK00001ffa@sanctumfi.com>
	<SANCTUMFISERVERq6Dq00002024@sanctumfi.com>
Message-ID: <e8e755250907160932t41eb4734uef833bd3c371154@mail.gmail.com>

All,

I suspect this topic/conversation is central to most of us -- and
quite important to R and R-Finance in general.

There are two primary issues that we seem to be encountering at this
point in the R-Finance community:

1) consistency of design: so all of our work becomes additive.
2) generality: writing code that works in the most general case possible

The first is quite difficult.  I have personally been involved in
*hundreds* of hours of productive (?) conversation with dozens of
people on this topic, be it on the list, useR, NY, Meielisalp,
R/Finance Chicago and back again at Meielisalp (and many sessions
involving Jak's tap in Chicago).  All of the conversations have been
certainly valuable, though even after all of this I still can't figure
out the best path...

One thing that I have worked on personally is making consistency
possible.  'xts' was born of this issue.  We didn't need another
time-series class.  We had 9 or 10.  We needed (or I needed) a way to
abstract the class and just get the job done.  'xts' does that
reasonably well -- though still far from complete.  The same issue
exists for portfolio level data, as well numerous other 'classes'.

Everyone should be free to use their own data object, what would be
nice (and required for maximum usefulness in my opinion) is if that
choice wasn't imposed on the end-user to use your software.  This
leads to more users, more feedback, better code, and a more 'complete'
landscape of tools that can be used.

As a time series example -- xts lets the developer use one type of
object (simplifying code), and accept *ALL* objects (simplifying the
user experience).

The point of the above is that interface counts, and it counts for a
lot.  If a framework is to work, it needs to be accessible to all
users.

The second point I'll toss out there is one of generality.

This is just as difficult as the first conceptually, though I would
argue possibly even more intractable.  We just can't individually
understand what we collectively require.

quantmod set about in 2007 trying to create a 'framework' and has
failed miserably.  It does some things like data management and
charting quite well, but the often asked question is how does
specifyModel/buildModel/tradeModel work.  The short answer is that it
does, and it doesn't.  It works on one type of strategy (EOD), and
doesn't on others:

> ?tradeModel
>      ## Not run:
>      m <- specifyModel(Next(OpCl(QQQQ)) ~ Lag(OpHi(QQQQ)))
>      m.built <- buildModel(m,method='rpart',training.per=c('2007-01-01','2007-04-01'))
loading required package: rpart
>
>      tradeModel(m.built)

  Model:  rpart1247761259.05370

  C.A.G.R.:  16.96%     H.P.R.:  67.94%

  Returns by period summary:

            weekly monthly quarterly yearly
    Max.    12.29%  18.96%    21.12% 52.23%
    3rd Qu.  2.43%   7.51%     8.84% 28.51%
    Mean     0.49%   2.10%     5.46% 20.50%
    Median   0.47%   0.78%     4.58%  4.80%
    2rd Qu. -1.63%  -3.01%     0.80%  4.63%
    Min.    -8.89% -10.71%    -4.87%  4.46%

  Period to date returns:

             weekly monthly quarterly yearly
             -0.44%   0.75%     0.75%  4.46%
>      tradeModel(m.built,leverage=2)

  Model:  rpart1247761259.05370

  C.A.G.R.:  29.76%     H.P.R.:  136.82%

  Returns by period summary:

             weekly monthly quarterly  yearly
    Max.     25.22%  36.09%    36.23% 105.40%
    3rd Qu.   4.72%  14.80%    17.59%  56.63%
    Mean      0.97%   4.05%     9.56%  39.50%
    Median    0.87%   1.46%     7.90%   7.90%
    2rd Qu.  -3.38%  -6.20%     1.34%   6.57%
    Min.    -17.15% -20.97%   -10.30%   5.24%

  Period to date returns:

             weekly monthly quarterly yearly
             -0.95%   1.35%     1.35%  5.24%
>      ## End(Not run)

Looks great!  But it is useless.  It is a casualty of specificity.  A
generalized framework would be awesome, but I think it is a much
larger task than I can handle.  I suspect that general to one person
is fantastically restrictive to another.

R is the 'general' framework.

What we really need is consistency of the pieces that make
building/testing models in *R* easier.

This comes from projects like blotter I think.  Were we can use parts
that we want, and only those that we want.

As I said at the start, this is a lot more involved than it looks
like.  Obviously best of luck to all who take up the challenge.  This
thread is an awesome start to the (larger) public conversation that
has been taking place over beers for quite some time now.

Best,
Jeff

On Thu, Jul 16, 2009 at 9:41 AM, Robert Sams<robert at sanctumfi.com> wrote:
> Hi Mark,
>
> I have started a package called tradesys which, I think, is a clean
> solution. The project is registered on r-forge
> https://r-forge.r-project.org/projects/tradesys/ and the initial code
> base with documentation will be checked-in by the end of the day London
> time. As a taster:
>
>> library(tradesys)
>> data(spx)
>> tail(spx)
> ? ? ? ? ? ? Open ? High ? ?Low ?Close ? ? Volume
> 2009-05-12 910.52 915.57 896.46 908.35 6871750400
> 2009-05-13 905.40 905.40 882.80 883.92 7091820000
> 2009-05-14 884.24 898.36 882.52 893.07 6134870000
> 2009-05-15 892.76 896.97 878.94 882.88 5439720000
> 2009-05-18 886.07 910.00 886.07 909.71 5702150000
> 2009-05-19 909.67 916.39 905.22 908.13 6616270000
>> x <- tradesys(spx, el=MA(Close, 60) > MA(Close, 120), es=MA(Close, 60)
> <= MA(Close, 120))
>> tail(trades(x, uselog=TRUE))
> ? ?phase ? ? ?etime ? ? ?xtime time nobs ?eprice ?xprice ? ?pnl
> ror
> 107 ? ?EL 2006-09-21 2007-09-12 ?356 ?244 1324.89 1471.10 146.21
> 0.264117052
> 108 ? ?ES 2007-09-12 2007-11-09 ? 58 ? 42 1471.10 1467.59 ? 3.51
> 0.006027153
> 109 ? ?EL 2007-11-09 2008-01-03 ? 55 ? 36 1467.59 1447.55 -20.04
> -0.034689959
> 110 ? ?ES 2008-01-03 2008-06-10 ?159 ?109 1447.55 1358.98 ?88.57
> 0.159301490
> 111 ? ?EL 2008-06-10 2008-07-21 ? 41 ? 28 1358.98 1261.82 -97.16
> -0.187159273
> 112 ? ?ES 2008-07-21 2009-05-19 ?302 ?209 1261.82 ?909.67 352.15
> 0.825619186
>> tail(equity(x, uselog=TRUE))
> ? ? ? ? ? trade states ? ?delta ? ?price ? ? ? ? ?ror ? equity
> 2009-05-12 ? 112 ? ? -1 1.331220 6.814016 ?0.018107970 38.02601
> 2009-05-13 ? 112 ? ? -1 1.307543 6.808377 ?0.007373275 38.30638
> 2009-05-14 ? 112 ? ? -1 1.297973 6.784729 ?0.030694874 39.48219
> 2009-05-15 ? 112 ? ? -1 1.259318 6.794318 -0.012075942 39.00541
> 2009-05-18 ? 112 ? ? -1 1.274712 6.786796 ?0.009588169 39.37940
> 2009-05-19 ? 112 ? ? -1 1.262606 6.813082 -0.033188777 38.07244
>
> Please checkout and play with the code at your leisure. Anyone
> interested in write-access to the repository should contact me directly.
>
>
> Robert
>
>
> ________________________________
>
> ? ? ? ?From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Mark
> Breman
> ? ? ? ?Sent: 16 July 2009 15:15
> ? ? ? ?To: r-sig-finance at stat.math.ethz.ch
> ? ? ? ?Subject: [R-SIG-Finance] Backtesting trade systems
>
>
> ? ? ? ?Hello,
>
> ? ? ? ?I have spend quit some time now looking for a package that
> allows me to backtest (technical) trading systems based on single
> financial instruments with R.
>
> ? ? ? ?I had a look at Rmetrics, blotter, fTrading,
> PerformanceAnalytics, backtest, quantmod, TTR etc, but not one of these
> fill my requirements. It's not that they are not usefull, on the
> contrary, they are all filled with terrific statistical stuff, but it's
> not the simple, practical and straightforward approach that I am looking
> for as a trader rather than as a statisticus.
>
> ? ? ? ?So I have decided to build my own solution, reusing as much as
> possible from these existing packages. (As a former software engineer I
> know how much time and effort goes into buiding reliable software, so
> the more reuse the better). As I am quite new to R and statistics in
> general, there is a lot to learn for me here...
>
>
> ? ? ? ?What I have build so far is a very basic set of functions called
> "tradesim.R" (I have attached it to this post). A very basic example of
> how these functions can be used for a backtest-run can be found in
> "tradesim_example.R". The example runs a backtest with end-of-day data
> from AAPL, using a (rather poor) trading system based on the RSI
> indicator (from the TTR package).
>
> ? ? ? ?Now I have read in some older post on this list that others were
> also searching for a backtesting package. I even read a post proposing
> to start a group effort creating such a package. I suspect that some of
> you might be interested in what I made so far and maybe would like to
> put in a effort creating such a package together. I certainly know that
> it's a lot easier to create good software as a group, rather than by a
> single person...
>
> ? ? ? ?So if you are interested have a look at what I got so far and
> let me know what you think.
>
> ? ? ? ?Regards,
>
> ? ? ? ?-Mark-
>
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From ron_michael70 at yahoo.com  Thu Jul 16 19:06:26 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Thu, 16 Jul 2009 10:06:26 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] practicability of variance
	decomposition
Message-ID: <24520555.post@talk.nabble.com>


Hi all, in multi-variate TS literature, in terms of VAR, VECM etc, I see lot
of stuffs on Variance decomposition, however could not figure out what is
the practical use of that? What practical question it answers? Any idea?

Regards,
-- 
View this message in context: http://www.nabble.com/practicability-of-variance-decomposition-tp24520555p24520555.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From josh.m.ulrich at gmail.com  Thu Jul 16 19:07:30 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Thu, 16 Jul 2009 12:07:30 -0500
Subject: [R-SIG-Finance] Backtesting trade systems
In-Reply-To: <SANCTUMFISERVERq6Dq00002024@sanctumfi.com>
References: <SANCTUMFISERVER69RK00001ffa@sanctumfi.com>
	<SANCTUMFISERVERq6Dq00002024@sanctumfi.com>
Message-ID: <8cca69990907161007j182ecc55oab2453664a34dd6a@mail.gmail.com>

Robert,

In the "details" section of your RollingFunctions documentation, you
note that you want to add "LAG, TR, ATR, EWMA, etc."  All of these
functions, and more, are already in TTR.

TTR uses xts internally, which provides support all major time-series
classes, not just zoo/xts.  Additionally, many of TTR's functions are
very fast because they use compiled code.

Best,
Josh
--
http://www.fosstrading.com



On Thu, Jul 16, 2009 at 9:41 AM, Robert Sams<robert at sanctumfi.com> wrote:
> Hi Mark,
>
> I have started a package called tradesys which, I think, is a clean
> solution. The project is registered on r-forge
> https://r-forge.r-project.org/projects/tradesys/ and the initial code
> base with documentation will be checked-in by the end of the day London
> time. As a taster:
>
>> library(tradesys)
>> data(spx)
>> tail(spx)
> ? ? ? ? ? ? Open ? High ? ?Low ?Close ? ? Volume
> 2009-05-12 910.52 915.57 896.46 908.35 6871750400
> 2009-05-13 905.40 905.40 882.80 883.92 7091820000
> 2009-05-14 884.24 898.36 882.52 893.07 6134870000
> 2009-05-15 892.76 896.97 878.94 882.88 5439720000
> 2009-05-18 886.07 910.00 886.07 909.71 5702150000
> 2009-05-19 909.67 916.39 905.22 908.13 6616270000
>> x <- tradesys(spx, el=MA(Close, 60) > MA(Close, 120), es=MA(Close, 60)
> <= MA(Close, 120))
>> tail(trades(x, uselog=TRUE))
> ? ?phase ? ? ?etime ? ? ?xtime time nobs ?eprice ?xprice ? ?pnl
> ror
> 107 ? ?EL 2006-09-21 2007-09-12 ?356 ?244 1324.89 1471.10 146.21
> 0.264117052
> 108 ? ?ES 2007-09-12 2007-11-09 ? 58 ? 42 1471.10 1467.59 ? 3.51
> 0.006027153
> 109 ? ?EL 2007-11-09 2008-01-03 ? 55 ? 36 1467.59 1447.55 -20.04
> -0.034689959
> 110 ? ?ES 2008-01-03 2008-06-10 ?159 ?109 1447.55 1358.98 ?88.57
> 0.159301490
> 111 ? ?EL 2008-06-10 2008-07-21 ? 41 ? 28 1358.98 1261.82 -97.16
> -0.187159273
> 112 ? ?ES 2008-07-21 2009-05-19 ?302 ?209 1261.82 ?909.67 352.15
> 0.825619186
>> tail(equity(x, uselog=TRUE))
> ? ? ? ? ? trade states ? ?delta ? ?price ? ? ? ? ?ror ? equity
> 2009-05-12 ? 112 ? ? -1 1.331220 6.814016 ?0.018107970 38.02601
> 2009-05-13 ? 112 ? ? -1 1.307543 6.808377 ?0.007373275 38.30638
> 2009-05-14 ? 112 ? ? -1 1.297973 6.784729 ?0.030694874 39.48219
> 2009-05-15 ? 112 ? ? -1 1.259318 6.794318 -0.012075942 39.00541
> 2009-05-18 ? 112 ? ? -1 1.274712 6.786796 ?0.009588169 39.37940
> 2009-05-19 ? 112 ? ? -1 1.262606 6.813082 -0.033188777 38.07244
>
> Please checkout and play with the code at your leisure. Anyone
> interested in write-access to the repository should contact me directly.
>
>
> Robert
>
>
> ________________________________
>
> ? ? ? ?From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Mark
> Breman
> ? ? ? ?Sent: 16 July 2009 15:15
> ? ? ? ?To: r-sig-finance at stat.math.ethz.ch
> ? ? ? ?Subject: [R-SIG-Finance] Backtesting trade systems
>
>
> ? ? ? ?Hello,
>
> ? ? ? ?I have spend quit some time now looking for a package that
> allows me to backtest (technical) trading systems based on single
> financial instruments with R.
>
> ? ? ? ?I had a look at Rmetrics, blotter, fTrading,
> PerformanceAnalytics, backtest, quantmod, TTR etc, but not one of these
> fill my requirements. It's not that they are not usefull, on the
> contrary, they are all filled with terrific statistical stuff, but it's
> not the simple, practical and straightforward approach that I am looking
> for as a trader rather than as a statisticus.
>
> ? ? ? ?So I have decided to build my own solution, reusing as much as
> possible from these existing packages. (As a former software engineer I
> know how much time and effort goes into buiding reliable software, so
> the more reuse the better). As I am quite new to R and statistics in
> general, there is a lot to learn for me here...
>
>
> ? ? ? ?What I have build so far is a very basic set of functions called
> "tradesim.R" (I have attached it to this post). A very basic example of
> how these functions can be used for a backtest-run can be found in
> "tradesim_example.R". The example runs a backtest with end-of-day data
> from AAPL, using a (rather poor) trading system based on the RSI
> indicator (from the TTR package).
>
> ? ? ? ?Now I have read in some older post on this list that others were
> also searching for a backtesting package. I even read a post proposing
> to start a group effort creating such a package. I suspect that some of
> you might be interested in what I made so far and maybe would like to
> put in a effort creating such a package together. I certainly know that
> it's a lot easier to create good software as a group, rather than by a
> single person...
>
> ? ? ? ?So if you are interested have a look at what I got so far and
> let me know what you think.
>
> ? ? ? ?Regards,
>
> ? ? ? ?-Mark-
>
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ezivot at u.washington.edu  Thu Jul 16 19:55:26 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Thu, 16 Jul 2009 10:55:26 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] practicability of variance
 decomposition
In-Reply-To: <24520555.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.43.0907161055260.31406@hymn12.u.washington.edu>

The usefulness of variance decompositions is described in several time series books - Lutkepohl, enders, etc. Favero's applied macroeconometrics book is also very good in this regard. The idea is to see what are the sources of forecast errors. If you are predicting real GDP from a SVAR with 5 variables and 5 identified orthogonal "shocks" then the FEVD tells you which variable is contributing most to the forecast error. Lutkephoh also shows very clearly that the FEVD gives the same information as the impulse response functions but it a different way. FEVD are not particularly useful outside of an identified SVAR because the decomposition of variance only has interpretable meaning when the shocks are uncorrelated.

****************************************************************
*  Eric Zivot                  			               *
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Adjunct Professor of Finance                                *
*  Adjunct Professor of Statistics
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *                                                           *
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Thu, 16 Jul 2009, RON70 wrote:

>
> Hi all, in multi-variate TS literature, in terms of VAR, VECM etc, I see lot
> of stuffs on Variance decomposition, however could not figure out what is
> the practical use of that? What practical question it answers? Any idea?
>
> Regards,
> --
> View this message in context: http://www.nabble.com/practicability-of-variance-decomposition-tp24520555p24520555.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From peter at braverock.com  Fri Jul 17 05:05:29 2009
From: peter at braverock.com (Peter Carl)
Date: Thu, 16 Jul 2009 22:05:29 -0500 (CDT)
Subject: [R-SIG-Finance] Backtesting trade systems
In-Reply-To: <e8e755250907160932t41eb4734uef833bd3c371154@mail.gmail.com>
References: <SANCTUMFISERVER69RK00001ffa@sanctumfi.com>
	<SANCTUMFISERVERq6Dq00002024@sanctumfi.com>
	<e8e755250907160932t41eb4734uef833bd3c371154@mail.gmail.com>
Message-ID: <45455.75.3.89.165.1247799929.squirrel@mail.braverock.com>

I'll add to Jeff's comments a little, but I think he's absolutely right
about working towards integration of various components.

PerformanceAnalytics has always been intended for statistical evaluation
of the resulting returns generated by trading systems, and has been
carefully scoped that way.  Although we started writing it before zoo or
xts came along, we had long wished for a time series package such that we
could concentrate on the objectives of the package rather worry about
underlying mechanics.  For me, zoo and xts are key components not only
because they make development easier, but because they should also make
integration easier.  That's why we're continuing to work to  move away
from our own (legacy?) code and take advantage of all that xts offers.

Blotter is another example of a component.  blotter takes it's name from
the "trade blotter," which is used by traders for recording transactions. 
blotter is *not* about creating trading strategies, but only about
managing trades, positions, and calculating p&l on those trades/positions.
 We will be extending it shortly to handle multiple instrument types and
currencies, as our requirements are for worldwide markets and multiple
different asset classes.  So, in that paradigm, the "trading system" would
generate a list of trades, and 'blotter' would be an interface for
evaluating performance, keeping records, etc., completely agnostic of the
process that generates trades.

Although blotter is still immature code, it's pretty functional already
given that it benefits so much from the existence of xts.  To see how to
use blotter, install it from r-forge and take a look at:
library(blotter)
example(blotter)
demo(turtles)

The example is a relatively simple one with a few trades in a few stocks. 
The demo is a more developed example of a trading system.  In this case,
it's a trend following system originally developed by Josh Ulrich that
uses signals from TTR, charts and prices from quantmod, and blotter to
calculate P&L.  There's a lot to do still, but I think it hints at the
possibilities here.

pcc
-- 
Peter Carl
http://www.braverock.com/~peter

> All,
>
> I suspect this topic/conversation is central to most of us -- and
> quite important to R and R-Finance in general.
>
> There are two primary issues that we seem to be encountering at this
> point in the R-Finance community:
>
> 1) consistency of design: so all of our work becomes additive.
> 2) generality: writing code that works in the most general case possible
>
> The first is quite difficult.  I have personally been involved in
> *hundreds* of hours of productive (?) conversation with dozens of
> people on this topic, be it on the list, useR, NY, Meielisalp,
> R/Finance Chicago and back again at Meielisalp (and many sessions
> involving Jak's tap in Chicago).  All of the conversations have been
> certainly valuable, though even after all of this I still can't figure
> out the best path...
>
> One thing that I have worked on personally is making consistency
> possible.  'xts' was born of this issue.  We didn't need another
> time-series class.  We had 9 or 10.  We needed (or I needed) a way to
> abstract the class and just get the job done.  'xts' does that
> reasonably well -- though still far from complete.  The same issue
> exists for portfolio level data, as well numerous other 'classes'.
>
> Everyone should be free to use their own data object, what would be
> nice (and required for maximum usefulness in my opinion) is if that
> choice wasn't imposed on the end-user to use your software.  This
> leads to more users, more feedback, better code, and a more 'complete'
> landscape of tools that can be used.
>
> As a time series example -- xts lets the developer use one type of
> object (simplifying code), and accept *ALL* objects (simplifying the
> user experience).
>
> The point of the above is that interface counts, and it counts for a
> lot.  If a framework is to work, it needs to be accessible to all
> users.
>
> The second point I'll toss out there is one of generality.
>
> This is just as difficult as the first conceptually, though I would
> argue possibly even more intractable.  We just can't individually
> understand what we collectively require.
>
> quantmod set about in 2007 trying to create a 'framework' and has
> failed miserably.  It does some things like data management and
> charting quite well, but the often asked question is how does
> specifyModel/buildModel/tradeModel work.  The short answer is that it
> does, and it doesn't.  It works on one type of strategy (EOD), and
> doesn't on others:
>
>> ?tradeModel
>>      ## Not run:
>>      m <- specifyModel(Next(OpCl(QQQQ)) ~ Lag(OpHi(QQQQ)))
>>      m.built <-
>> buildModel(m,method='rpart',training.per=c('2007-01-01','2007-04-01'))
> loading required package: rpart
>>
>>      tradeModel(m.built)
>
>   Model:  rpart1247761259.05370
>
>   C.A.G.R.:  16.96%     H.P.R.:  67.94%
>
>   Returns by period summary:
>
>             weekly monthly quarterly yearly
>     Max.    12.29%  18.96%    21.12% 52.23%
>     3rd Qu.  2.43%   7.51%     8.84% 28.51%
>     Mean     0.49%   2.10%     5.46% 20.50%
>     Median   0.47%   0.78%     4.58%  4.80%
>     2rd Qu. -1.63%  -3.01%     0.80%  4.63%
>     Min.    -8.89% -10.71%    -4.87%  4.46%
>
>   Period to date returns:
>
>              weekly monthly quarterly yearly
>              -0.44%   0.75%     0.75%  4.46%
>>      tradeModel(m.built,leverage=2)
>
>   Model:  rpart1247761259.05370
>
>   C.A.G.R.:  29.76%     H.P.R.:  136.82%
>
>   Returns by period summary:
>
>              weekly monthly quarterly  yearly
>     Max.     25.22%  36.09%    36.23% 105.40%
>     3rd Qu.   4.72%  14.80%    17.59%  56.63%
>     Mean      0.97%   4.05%     9.56%  39.50%
>     Median    0.87%   1.46%     7.90%   7.90%
>     2rd Qu.  -3.38%  -6.20%     1.34%   6.57%
>     Min.    -17.15% -20.97%   -10.30%   5.24%
>
>   Period to date returns:
>
>              weekly monthly quarterly yearly
>              -0.95%   1.35%     1.35%  5.24%
>>      ## End(Not run)
>
> Looks great!  But it is useless.  It is a casualty of specificity.  A
> generalized framework would be awesome, but I think it is a much
> larger task than I can handle.  I suspect that general to one person
> is fantastically restrictive to another.
>
> R is the 'general' framework.
>
> What we really need is consistency of the pieces that make
> building/testing models in *R* easier.
>
> This comes from projects like blotter I think.  Were we can use parts
> that we want, and only those that we want.
>
> As I said at the start, this is a lot more involved than it looks
> like.  Obviously best of luck to all who take up the challenge.  This
> thread is an awesome start to the (larger) public conversation that
> has been taking place over beers for quite some time now.
>
> Best,
> Jeff
>
> On Thu, Jul 16, 2009 at 9:41 AM, Robert Sams<robert at sanctumfi.com> wrote:
>> Hi Mark,
>>
>> I have started a package called tradesys which, I think, is a clean
>> solution. The project is registered on r-forge
>> https://r-forge.r-project.org/projects/tradesys/ and the initial code
>> base with documentation will be checked-in by the end of the day London
>> time. As a taster:
>>
>>> library(tradesys)
>>> data(spx)
>>> tail(spx)
>> ? ? ? ? ? ? Open ? High ? ?Low ?Close ? ? Volume
>> 2009-05-12 910.52 915.57 896.46 908.35 6871750400
>> 2009-05-13 905.40 905.40 882.80 883.92 7091820000
>> 2009-05-14 884.24 898.36 882.52 893.07 6134870000
>> 2009-05-15 892.76 896.97 878.94 882.88 5439720000
>> 2009-05-18 886.07 910.00 886.07 909.71 5702150000
>> 2009-05-19 909.67 916.39 905.22 908.13 6616270000
>>> x <- tradesys(spx, el=MA(Close, 60) > MA(Close, 120), es=MA(Close, 60)
>> <= MA(Close, 120))
>>> tail(trades(x, uselog=TRUE))
>> ? ?phase ? ? ?etime ? ? ?xtime time nobs ?eprice ?xprice ? ?pnl
>> ror
>> 107 ? ?EL 2006-09-21 2007-09-12 ?356 ?244 1324.89 1471.10 146.21
>> 0.264117052
>> 108 ? ?ES 2007-09-12 2007-11-09 ? 58 ? 42 1471.10 1467.59 ? 3.51
>> 0.006027153
>> 109 ? ?EL 2007-11-09 2008-01-03 ? 55 ? 36 1467.59 1447.55 -20.04
>> -0.034689959
>> 110 ? ?ES 2008-01-03 2008-06-10 ?159 ?109 1447.55 1358.98 ?88.57
>> 0.159301490
>> 111 ? ?EL 2008-06-10 2008-07-21 ? 41 ? 28 1358.98 1261.82 -97.16
>> -0.187159273
>> 112 ? ?ES 2008-07-21 2009-05-19 ?302 ?209 1261.82 ?909.67 352.15
>> 0.825619186
>>> tail(equity(x, uselog=TRUE))
>> ? ? ? ? ? trade states ? ?delta ? ?price ? ? ? ? ?ror ? equity
>> 2009-05-12 ? 112 ? ? -1 1.331220 6.814016 ?0.018107970 38.02601
>> 2009-05-13 ? 112 ? ? -1 1.307543 6.808377 ?0.007373275 38.30638
>> 2009-05-14 ? 112 ? ? -1 1.297973 6.784729 ?0.030694874 39.48219
>> 2009-05-15 ? 112 ? ? -1 1.259318 6.794318 -0.012075942 39.00541
>> 2009-05-18 ? 112 ? ? -1 1.274712 6.786796 ?0.009588169 39.37940
>> 2009-05-19 ? 112 ? ? -1 1.262606 6.813082 -0.033188777 38.07244
>>
>> Please checkout and play with the code at your leisure. Anyone
>> interested in write-access to the repository should contact me directly.
>>
>>
>> Robert
>>
>>
>> ________________________________
>>
>> ? ? ? ?From: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Mark
>> Breman
>> ? ? ? ?Sent: 16 July 2009 15:15
>> ? ? ? ?To: r-sig-finance at stat.math.ethz.ch
>> ? ? ? ?Subject: [R-SIG-Finance] Backtesting trade systems
>>
>>
>> ? ? ? ?Hello,
>>
>> ? ? ? ?I have spend quit some time now looking for a package that
>> allows me to backtest (technical) trading systems based on single
>> financial instruments with R.
>>
>> ? ? ? ?I had a look at Rmetrics, blotter, fTrading,
>> PerformanceAnalytics, backtest, quantmod, TTR etc, but not one of these
>> fill my requirements. It's not that they are not usefull, on the
>> contrary, they are all filled with terrific statistical stuff, but it's
>> not the simple, practical and straightforward approach that I am looking
>> for as a trader rather than as a statisticus.
>>
>> ? ? ? ?So I have decided to build my own solution, reusing as much as
>> possible from these existing packages. (As a former software engineer I
>> know how much time and effort goes into buiding reliable software, so
>> the more reuse the better). As I am quite new to R and statistics in
>> general, there is a lot to learn for me here...
>>
>>
>> ? ? ? ?What I have build so far is a very basic set of functions called
>> "tradesim.R" (I have attached it to this post). A very basic example of
>> how these functions can be used for a backtest-run can be found in
>> "tradesim_example.R". The example runs a backtest with end-of-day data
>> from AAPL, using a (rather poor) trading system based on the RSI
>> indicator (from the TTR package).
>>
>> ? ? ? ?Now I have read in some older post on this list that others were
>> also searching for a backtesting package. I even read a post proposing
>> to start a group effort creating such a package. I suspect that some of
>> you might be interested in what I made so far and maybe would like to
>> put in a effort creating such a package together. I certainly know that
>> it's a lot easier to create good software as a group, rather than by a
>> single person...
>>
>> ? ? ? ?So if you are interested have a look at what I got so far and
>> let me know what you think.
>>
>> ? ? ? ?Regards,
>>
>> ? ? ? ?-Mark-
>>
>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From windspeedo99 at gmail.com  Fri Jul 17 09:12:24 2009
From: windspeedo99 at gmail.com (Wind)
Date: Fri, 17 Jul 2009 15:12:24 +0800
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
In-Reply-To: <d718c8210907160703red62523y943f0711d6f903b9@mail.gmail.com>
References: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>
	<4A5F2A38.9060100@braverock.com>
	<d718c8210907160703red62523y943f0711d6f903b9@mail.gmail.com>
Message-ID: <d718c8210907170012t643ee348g9eeac6c415fc26fd@mail.gmail.com>

There is a chart of Heng Seng index  in page 24 of Prof. Sornette's paper:
## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
Financial and Economic Crisis
## http://arxiv.org/abs/0905.0220
The picture has also been attached as .hong kong.jpg

The y axis of the chart is log-axis.  And there is a straight line in
the chart.  "This is indeed the long-term behavior of this market, as
shown by the best linear fit represented by the solid straight line,
corresponding to an average constant growth rate of 13.8% per year."
The following codes could not plot the same straight line.  I wonder
how could plot the straight best fit line  in the log plot.


## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
Financial and Economic Crisis
## http://arxiv.org/abs/0905.0220
## chart in Page 24

hsi<-read.csv("http://32xiang.appspot.com/static/hsi-1970.csv",header=TRUE,stringsAsFactors=FALSE)
pr<-hsi$close

plot(pr,type="l",log="y")
grid()

ti<-index(pr)
ti2<-index(pr)^2

lines(lm(pr~ti+ti2)$fit,col="red")
lines(lm(pr~ti)$fit,col="blue")
lines(lm(pr~ti2)$fit,col="pink")

Thanks in advance.

wind
-------------- next part --------------
A non-text attachment was scrubbed...
Name: hong kong .jpg
Type: image/jpeg
Size: 48355 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090717/bbd27fcd/attachment.jpg>

From windspeedo99 at gmail.com  Fri Jul 17 10:11:53 2009
From: windspeedo99 at gmail.com (Wind)
Date: Fri, 17 Jul 2009 16:11:53 +0800
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
In-Reply-To: <d718c8210907170012t643ee348g9eeac6c415fc26fd@mail.gmail.com>
References: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>
	<4A5F2A38.9060100@braverock.com>
	<d718c8210907160703red62523y943f0711d6f903b9@mail.gmail.com>
	<d718c8210907170012t643ee348g9eeac6c415fc26fd@mail.gmail.com>
Message-ID: <d718c8210907170111l6d08ea92s431b47c992069632@mail.gmail.com>

Sorry that I forgot including the first line of the code:
library(quantmod)

So the replicable codes as following:

library(quantmod)
hsi<-read.csv("http://32xiang.appspot.com/static/hsi-1970.csv",header=TRUE,stringsAsFactors=FALSE)
pr<-hsi$close

plot(pr,type="l",log="y")
grid()

ti<-index(pr)
ti2<-index(pr)^2

lines(lm(pr~ti+ti2)$fit,col="red")
lines(lm(pr~ti)$fit,col="blue")
lines(lm(pr~ti2)$fit,col="pink")




On Fri, Jul 17, 2009 at 3:12 PM, Wind<windspeedo99 at gmail.com> wrote:
> There is a chart of Heng Seng index ?in page 24 of Prof. Sornette's paper:
> ## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
> Financial and Economic Crisis
> ## http://arxiv.org/abs/0905.0220
> The picture has also been attached as .hong kong.jpg
>
> The y axis of the chart is log-axis. ?And there is a straight line in
> the chart. ?"This is indeed the long-term behavior of this market, as
> shown by the best linear fit represented by the solid straight line,
> corresponding to an average constant growth rate of 13.8% per year."
> The following codes could not plot the same straight line. ?I wonder
> how could plot the straight best fit line ?in the log plot.
>
>
> ## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
> Financial and Economic Crisis
> ## http://arxiv.org/abs/0905.0220
> ## chart in Page 24
>
> hsi<-read.csv("http://32xiang.appspot.com/static/hsi-1970.csv",header=TRUE,stringsAsFactors=FALSE)
> pr<-hsi$close
>
> plot(pr,type="l",log="y")
> grid()
>
> ti<-index(pr)
> ti2<-index(pr)^2
>
> lines(lm(pr~ti+ti2)$fit,col="red")
> lines(lm(pr~ti)$fit,col="blue")
> lines(lm(pr~ti2)$fit,col="pink")
>
> Thanks in advance.
>
> wind
>


From breman.mark at gmail.com  Fri Jul 17 11:09:44 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Fri, 17 Jul 2009 11:09:44 +0200
Subject: [R-SIG-Finance] Backtesting trade systems
In-Reply-To: <45455.75.3.89.165.1247799929.squirrel@mail.braverock.com>
References: <SANCTUMFISERVER69RK00001ffa@sanctumfi.com>
	<SANCTUMFISERVERq6Dq00002024@sanctumfi.com>
	<e8e755250907160932t41eb4734uef833bd3c371154@mail.gmail.com>
	<45455.75.3.89.165.1247799929.squirrel@mail.braverock.com>
Message-ID: <5e6a2e670907170209x188706caybfae680c01c103de@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090717/5baf9ca2/attachment.pl>

From robert at sanctumfi.com  Fri Jul 17 12:19:38 2009
From: robert at sanctumfi.com (Robert Sams)
Date: Fri, 17 Jul 2009 11:19:38 +0100
Subject: [R-SIG-Finance] Backtesting trade systems
References: <SANCTUMFISERVER69RK00001ffa@sanctumfi.com>
	<SANCTUMFISERVERq6Dq00002024@sanctumfi.com>
	<SANCTUMFISERVERqnWw000022a5@sanctumfi.com>
Message-ID: <SANCTUMFISERVERMum0000022e1@sanctumfi.com>

Hi Josh, 

Ah, yes.. the RollingFunctions code is totally peripheral to the tradesys framework and was thrown in there so that some simple examples could be run without adding package dependencies. But now that you mention it, we can make TTR a suggested package and use its functions liberally in the examples and vignette. I think I'll do that.

Any function can be used in the expressions passed to the el, es, xl, xs, and makecols parameters of tradesys(), and TTR contains many of the functions that one needs. The packages are very complementary, which is great.

Robert

> -----Original Message-----
> From: Joshua Ulrich [mailto:josh.m.ulrich at gmail.com] 
> Sent: 16 July 2009 18:08
> To: Robert Sams
> Cc: Mark Breman; r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Backtesting trade systems
> 
> Robert,
> 
> In the "details" section of your RollingFunctions 
> documentation, you note that you want to add "LAG, TR, ATR, 
> EWMA, etc."  All of these functions, and more, are already in TTR.
> 
> TTR uses xts internally, which provides support all major 
> time-series classes, not just zoo/xts.  Additionally, many of 
> TTR's functions are very fast because they use compiled code.
> 
> Best,
> Josh
> --
> http://www.fosstrading.com
> 
> 
> 
> On Thu, Jul 16, 2009 at 9:41 AM, Robert 
> Sams<robert at sanctumfi.com> wrote:
> > Hi Mark,
> >
> > I have started a package called tradesys which, I think, is a clean 
> > solution. The project is registered on r-forge 
> > https://r-forge.r-project.org/projects/tradesys/ and the 
> initial code 
> > base with documentation will be checked-in by the end of the day 
> > London time. As a taster:
> >
> >> library(tradesys)
> >> data(spx)
> >> tail(spx)
> > ? ? ? ? ? ? Open ? High ? ?Low ?Close ? ? Volume
> > 2009-05-12 910.52 915.57 896.46 908.35 6871750400
> > 2009-05-13 905.40 905.40 882.80 883.92 7091820000
> > 2009-05-14 884.24 898.36 882.52 893.07 6134870000
> > 2009-05-15 892.76 896.97 878.94 882.88 5439720000
> > 2009-05-18 886.07 910.00 886.07 909.71 5702150000
> > 2009-05-19 909.67 916.39 905.22 908.13 6616270000
> >> x <- tradesys(spx, el=MA(Close, 60) > MA(Close, 120), es=MA(Close, 
> >> 60)
> > <= MA(Close, 120))
> >> tail(trades(x, uselog=TRUE))
> > ? ?phase ? ? ?etime ? ? ?xtime time nobs ?eprice ?xprice ? ?pnl ror
> > 107 ? ?EL 2006-09-21 2007-09-12 ?356 ?244 1324.89 1471.10 146.21
> > 0.264117052
> > 108 ? ?ES 2007-09-12 2007-11-09 ? 58 ? 42 1471.10 1467.59 ? 3.51
> > 0.006027153
> > 109 ? ?EL 2007-11-09 2008-01-03 ? 55 ? 36 1467.59 1447.55 -20.04
> > -0.034689959
> > 110 ? ?ES 2008-01-03 2008-06-10 ?159 ?109 1447.55 1358.98 ?88.57 
> > 0.159301490
> > 111 ? ?EL 2008-06-10 2008-07-21 ? 41 ? 28 1358.98 1261.82 -97.16
> > -0.187159273
> > 112 ? ?ES 2008-07-21 2009-05-19 ?302 ?209 1261.82 ?909.67 352.15
> > 0.825619186
> >> tail(equity(x, uselog=TRUE))
> > ? ? ? ? ? trade states ? ?delta ? ?price ? ? ? ? ?ror ? equity
> > 2009-05-12 ? 112 ? ? -1 1.331220 6.814016 ?0.018107970 38.02601
> > 2009-05-13 ? 112 ? ? -1 1.307543 6.808377 ?0.007373275 38.30638
> > 2009-05-14 ? 112 ? ? -1 1.297973 6.784729 ?0.030694874 39.48219
> > 2009-05-15 ? 112 ? ? -1 1.259318 6.794318 -0.012075942 39.00541
> > 2009-05-18 ? 112 ? ? -1 1.274712 6.786796 ?0.009588169 39.37940
> > 2009-05-19 ? 112 ? ? -1 1.262606 6.813082 -0.033188777 38.07244
> >
> > Please checkout and play with the code at your leisure. Anyone 
> > interested in write-access to the repository should contact 
> me directly.
> >
> >
> > Robert
> >
> >
> > ________________________________
> >
> > ? ? ? ?From: r-sig-finance-bounces at stat.math.ethz.ch
> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Mark 
> > Breman
> > ? ? ? ?Sent: 16 July 2009 15:15
> > ? ? ? ?To: r-sig-finance at stat.math.ethz.ch
> > ? ? ? ?Subject: [R-SIG-Finance] Backtesting trade systems
> >
> >
> > ? ? ? ?Hello,
> >
> > ? ? ? ?I have spend quit some time now looking for a package that 
> > allows me to backtest (technical) trading systems based on single 
> > financial instruments with R.
> >
> > ? ? ? ?I had a look at Rmetrics, blotter, fTrading, 
> > PerformanceAnalytics, backtest, quantmod, TTR etc, but not one of 
> > these fill my requirements. It's not that they are not 
> usefull, on the 
> > contrary, they are all filled with terrific statistical stuff, but 
> > it's not the simple, practical and straightforward approach 
> that I am 
> > looking for as a trader rather than as a statisticus.
> >
> > ? ? ? ?So I have decided to build my own solution, reusing 
> as much as 
> > possible from these existing packages. (As a former 
> software engineer 
> > I know how much time and effort goes into buiding reliable 
> software, 
> > so the more reuse the better). As I am quite new to R and 
> statistics 
> > in general, there is a lot to learn for me here...
> >
> >
> > ? ? ? ?What I have build so far is a very basic set of functions 
> > called "tradesim.R" (I have attached it to this post). A very basic 
> > example of how these functions can be used for a 
> backtest-run can be 
> > found in "tradesim_example.R". The example runs a backtest with 
> > end-of-day data from AAPL, using a (rather poor) trading 
> system based 
> > on the RSI indicator (from the TTR package).
> >
> > ? ? ? ?Now I have read in some older post on this list that others 
> > were also searching for a backtesting package. I even read a post 
> > proposing to start a group effort creating such a package. 
> I suspect 
> > that some of you might be interested in what I made so far 
> and maybe 
> > would like to put in a effort creating such a package together. I 
> > certainly know that it's a lot easier to create good software as a 
> > group, rather than by a single person...
> >
> > ? ? ? ?So if you are interested have a look at what I got 
> so far and 
> > let me know what you think.
> >
> > ? ? ? ?Regards,
> >
> > ? ? ? ?-Mark-
> >
> >
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
> 


From michaellibeijing at gmail.com  Fri Jul 17 12:54:37 2009
From: michaellibeijing at gmail.com (michael li)
Date: Fri, 17 Jul 2009 18:54:37 +0800
Subject: [R-SIG-Finance] quantmod:indicators with different colors
Message-ID: <aacf63cb0907170354w63301714x4e4a35a5bab23a52@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090717/5c16204f/attachment.pl>

From ggrothendieck at gmail.com  Fri Jul 17 12:58:53 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 17 Jul 2009 06:58:53 -0400
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
In-Reply-To: <d718c8210907170111l6d08ea92s431b47c992069632@mail.gmail.com>
References: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com> 
	<4A5F2A38.9060100@braverock.com>
	<d718c8210907160703red62523y943f0711d6f903b9@mail.gmail.com> 
	<d718c8210907170012t643ee348g9eeac6c415fc26fd@mail.gmail.com> 
	<d718c8210907170111l6d08ea92s431b47c992069632@mail.gmail.com>
Message-ID: <971536df0907170358p5145ad49g654ffbe8e8a6b3a@mail.gmail.com>

Try this:

lines(exp(fitted(lm(log(pr) ~ ti))), col = "purple")


On Fri, Jul 17, 2009 at 4:11 AM, Wind<windspeedo99 at gmail.com> wrote:
> Sorry that I forgot including the first line of the code:
> library(quantmod)
>
> So the replicable codes as following:
>
> library(quantmod)
> hsi<-read.csv("http://32xiang.appspot.com/static/hsi-1970.csv",header=TRUE,stringsAsFactors=FALSE)
> pr<-hsi$close
>
> plot(pr,type="l",log="y")
> grid()
>
> ti<-index(pr)
> ti2<-index(pr)^2
>
> lines(lm(pr~ti+ti2)$fit,col="red")
> lines(lm(pr~ti)$fit,col="blue")
> lines(lm(pr~ti2)$fit,col="pink")
>
>
>
>
> On Fri, Jul 17, 2009 at 3:12 PM, Wind<windspeedo99 at gmail.com> wrote:
>> There is a chart of Heng Seng index ?in page 24 of Prof. Sornette's paper:
>> ## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
>> Financial and Economic Crisis
>> ## http://arxiv.org/abs/0905.0220
>> The picture has also been attached as .hong kong.jpg
>>
>> The y axis of the chart is log-axis. ?And there is a straight line in
>> the chart. ?"This is indeed the long-term behavior of this market, as
>> shown by the best linear fit represented by the solid straight line,
>> corresponding to an average constant growth rate of 13.8% per year."
>> The following codes could not plot the same straight line. ?I wonder
>> how could plot the straight best fit line ?in the log plot.
>>
>>
>> ## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
>> Financial and Economic Crisis
>> ## http://arxiv.org/abs/0905.0220
>> ## chart in Page 24
>>
>> hsi<-read.csv("http://32xiang.appspot.com/static/hsi-1970.csv",header=TRUE,stringsAsFactors=FALSE)
>> pr<-hsi$close
>>
>> plot(pr,type="l",log="y")
>> grid()
>>
>> ti<-index(pr)
>> ti2<-index(pr)^2
>>
>> lines(lm(pr~ti+ti2)$fit,col="red")
>> lines(lm(pr~ti)$fit,col="blue")
>> lines(lm(pr~ti2)$fit,col="pink")
>>
>> Thanks in advance.
>>
>> wind
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From windspeedo99 at gmail.com  Fri Jul 17 13:05:03 2009
From: windspeedo99 at gmail.com (Wind)
Date: Fri, 17 Jul 2009 19:05:03 +0800
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
In-Reply-To: <971536df0907170358p5145ad49g654ffbe8e8a6b3a@mail.gmail.com>
References: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>
	<4A5F2A38.9060100@braverock.com>
	<d718c8210907160703red62523y943f0711d6f903b9@mail.gmail.com>
	<d718c8210907170012t643ee348g9eeac6c415fc26fd@mail.gmail.com>
	<d718c8210907170111l6d08ea92s431b47c992069632@mail.gmail.com>
	<971536df0907170358p5145ad49g654ffbe8e8a6b3a@mail.gmail.com>
Message-ID: <d718c8210907170405m1bfa4ed7yb47336c9e021e2d2@mail.gmail.com>

It works exactly as the chart.
Thanks Gabor

On Fri, Jul 17, 2009 at 6:58 PM, Gabor
Grothendieck<ggrothendieck at gmail.com> wrote:
> Try this:
>
> lines(exp(fitted(lm(log(pr) ~ ti))), col = "purple")
>
>
> On Fri, Jul 17, 2009 at 4:11 AM, Wind<windspeedo99 at gmail.com> wrote:
>> Sorry that I forgot including the first line of the code:
>> library(quantmod)
>>
>> So the replicable codes as following:
>>
>> library(quantmod)
>> hsi<-read.csv("http://32xiang.appspot.com/static/hsi-1970.csv",header=TRUE,stringsAsFactors=FALSE)
>> pr<-hsi$close
>>
>> plot(pr,type="l",log="y")
>> grid()
>>
>> ti<-index(pr)
>> ti2<-index(pr)^2
>>
>> lines(lm(pr~ti+ti2)$fit,col="red")
>> lines(lm(pr~ti)$fit,col="blue")
>> lines(lm(pr~ti2)$fit,col="pink")
>>
>>
>>
>>
>> On Fri, Jul 17, 2009 at 3:12 PM, Wind<windspeedo99 at gmail.com> wrote:
>>> There is a chart of Heng Seng index ?in page 24 of Prof. Sornette's paper:
>>> ## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
>>> Financial and Economic Crisis
>>> ## http://arxiv.org/abs/0905.0220
>>> The picture has also been attached as .hong kong.jpg
>>>
>>> The y axis of the chart is log-axis. ?And there is a straight line in
>>> the chart. ?"This is indeed the long-term behavior of this market, as
>>> shown by the best linear fit represented by the solid straight line,
>>> corresponding to an average constant growth rate of 13.8% per year."
>>> The following codes could not plot the same straight line. ?I wonder
>>> how could plot the straight best fit line ?in the log plot.
>>>
>>>
>>> ## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
>>> Financial and Economic Crisis
>>> ## http://arxiv.org/abs/0905.0220
>>> ## chart in Page 24
>>>
>>> hsi<-read.csv("http://32xiang.appspot.com/static/hsi-1970.csv",header=TRUE,stringsAsFactors=FALSE)
>>> pr<-hsi$close
>>>
>>> plot(pr,type="l",log="y")
>>> grid()
>>>
>>> ti<-index(pr)
>>> ti2<-index(pr)^2
>>>
>>> lines(lm(pr~ti+ti2)$fit,col="red")
>>> lines(lm(pr~ti)$fit,col="blue")
>>> lines(lm(pr~ti2)$fit,col="pink")
>>>
>>> Thanks in advance.
>>>
>>> wind
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From jeff.a.ryan at gmail.com  Fri Jul 17 16:05:02 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 17 Jul 2009 09:05:02 -0500
Subject: [R-SIG-Finance] quantmod:indicators with different colors
In-Reply-To: <aacf63cb0907170354w63301714x4e4a35a5bab23a52@mail.gmail.com>
References: <aacf63cb0907170354w63301714x4e4a35a5bab23a52@mail.gmail.com>
Message-ID: <e8e755250907170705v122535a2nf290b12c3b970c4@mail.gmail.com>

Try:

library(quantmod)
getSymbols("SBUX")
pos <- which(OpCl(SBUX) > 0)
barChart(SBUX)
addTA(OpCl(SBUX)[pos], col=4, type='h', lwd=1)
addTA(OpCl(SBUX)[-pos]*-1, col='red', type='h', lwd=1, on=3)

HTH,
Jeff

On Fri, Jul 17, 2009 at 5:54 AM, michael li<michaellibeijing at gmail.com> wrote:
> Is it possible to plot an indicator with two different colors in quantmod?
> ?one for positive numbers, and another for negative numbers?
> Just like the style of addCMF()?
>
>>library(quantmod)
>>getSymbols('SBUX')
>>barChart(SBUX)
>>addTA(OpCl(SBUX), col=4, type='h', lwd=2)
>
> The last line plot the indicator in one color only.
>
> Thanks.
>
> Michael
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From michaellibeijing at gmail.com  Fri Jul 17 16:38:29 2009
From: michaellibeijing at gmail.com (michael li)
Date: Fri, 17 Jul 2009 22:38:29 +0800
Subject: [R-SIG-Finance] quantmod:indicators with different colors
In-Reply-To: <e8e755250907170705v122535a2nf290b12c3b970c4@mail.gmail.com>
References: <aacf63cb0907170354w63301714x4e4a35a5bab23a52@mail.gmail.com>
	<e8e755250907170705v122535a2nf290b12c3b970c4@mail.gmail.com>
Message-ID: <aacf63cb0907170738t7b49befctfce3429bef8f836f@mail.gmail.com>

Good idea.
I modified it a bit, then got the effect I want.
>library(quantmod)
>getSymbols("SBUX")
>pos <- which(OpCl(SBUX) > 0)
>barChart(SBUX)
>addTA(OpCl(SBUX), col=4, type='h', lwd=1)
>addTA(OpCl(SBUX)[pos], col=4, type='h', lwd=1,on=3)
>addTA(OpCl(SBUX)[-pos], col='red', type='h', lwd=1, on=3)

Thanks.

Michael



On Fri, Jul 17, 2009 at 10:05 PM, Jeff Ryan<jeff.a.ryan at gmail.com> wrote:
> Try:
>
> library(quantmod)
> getSymbols("SBUX")
> pos <- which(OpCl(SBUX) > 0)
> barChart(SBUX)
> addTA(OpCl(SBUX)[pos], col=4, type='h', lwd=1)
> addTA(OpCl(SBUX)[-pos]*-1, col='red', type='h', lwd=1, on=3)
>
> HTH,
> Jeff
>
> On Fri, Jul 17, 2009 at 5:54 AM, michael li<michaellibeijing at gmail.com> wrote:
>> Is it possible to plot an indicator with two different colors in quantmod?
>> ?one for positive numbers, and another for negative numbers?
>> Just like the style of addCMF()?
>>
>>>library(quantmod)
>>>getSymbols('SBUX')
>>>barChart(SBUX)
>>>addTA(OpCl(SBUX), col=4, type='h', lwd=2)
>>
>> The last line plot the indicator in one color only.
>>
>> Thanks.
>>
>> Michael
>>
>> ? ? ? ?[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>


From swisdom at gmail.com  Fri Jul 17 19:46:19 2009
From: swisdom at gmail.com (Steve Wisdom)
Date: Fri, 17 Jul 2009 13:46:19 -0400
Subject: [R-SIG-Finance] Turtle demo (was 'Backtesting...')
Message-ID: <8890ce50907171046q51df4f92wbeea1fa9cec90dc@mail.gmail.com>

Mark Breman <breman.mark at gmail.com> :

> That all said, I got an error when I tried to run the turtle demo:

> Error in updatePortf(Portfolio = portfolio, StartDate = CurrentDate, EndDate
> = CurrentDate) :
> unused argument(s) (StartDate = 12866, EndDate = 12866)

"Works on my machine" - http://i28.tinypic.com/vf8gfa.png


From djmphd at gmail.com  Sat Jul 18 04:58:53 2009
From: djmphd at gmail.com (David J. Moore, Ph.D.)
Date: Fri, 17 Jul 2009 21:58:53 -0500
Subject: [R-SIG-Finance] Inequality constraints in GMM estimation?
Message-ID: <d94e0b160907171958p6a08b56ckdf7d02a14c33830d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090717/1c61ae2a/attachment.pl>

From james at jtoll.com  Sat Jul 18 05:33:30 2009
From: james at jtoll.com (James Toll)
Date: Fri, 17 Jul 2009 21:33:30 -0600
Subject: [R-SIG-Finance] Random numbers with positive skewness
Message-ID: <D225F5FF-DC2F-4BF7-AB50-60BD349753BB@jtoll.com>

Hi,

I've been debating how to go about acquiring the historical data  
necessary to backtest an indexing idea and I finally decided that  
maybe I should first try it out on some randomly generated time series  
data. So, for starters, I need to generate 100 time series to  
represent 100 different equities, like, for example, the OEX.  To take  
the place of 10 years of closing prices, I thought I could simply  
generate 2520 price relatives using something along the lines of this:

x <- rnorm(2520, mean = 1, sd = 0.02)

But obviously, it's not likely that each of the components of a cap  
weighted index of 100 equities is going to have price relatives with  
an SD of 0.02.  So I'd like to be able to vary the SD for each.  I  
thought I could just as easily randomly generate a vector of SD's for  
use in generating each time series like so:

y<-rnorm(100, mean = 0.025, sd = 0.007)

The problem I'm running into is that when generating the SD's for each  
of the 100 time series my wild guess is that the mean might be  
somewhere between 0.02 and 0.03, and I think the SD might be somewhere  
around 0.007, but I don't think a normal distribution really works at  
all.  I think I need a lot of positive skewness to the distribution.

BTW, all of these wild guesses are simply based upon my experience as  
an option market maker (which may be worthless to this task), and  
there are lots of equities that normally trade between 30 and 50  
volatility, but then I've also traded tech stocks with vols around 80  
and 90.  So basically I think the bulk of the distribution of SD's is  
between 0.02 and 0.03, they taper off on the left side around 0.01,  
maybe a little lower, but then on the right side the long tail goes up  
to around 0.06.  If my assumptions / conclusions are totally off base  
please feel free to tell me.  This is definitely my first attempt at  
any kind of backtesting.

Is there some other method of generating random numbers that will  
allow me to control the skewness of the distribution?  Thanks.

James


From landronimirc at gmail.com  Sat Jul 18 15:03:44 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Sat, 18 Jul 2009 15:03:44 +0200
Subject: [R-SIG-Finance] Inequality constraints in GMM estimation?
In-Reply-To: <d94e0b160907171958p6a08b56ckdf7d02a14c33830d@mail.gmail.com>
References: <d94e0b160907171958p6a08b56ckdf7d02a14c33830d@mail.gmail.com>
Message-ID: <68b1e2610907180603w263f211au9509bb0da2ec00ca@mail.gmail.com>

Hello,

On Sat, Jul 18, 2009 at 4:58 AM, David J. Moore, Ph.D.<djmphd at gmail.com> wrote:
> I have a relatively simple finance application of GMM. Given the moment
> condition:
> E[m*R]=0
>
> where m=m[theta]
>
> I would like to constrain m>0. Any ideas?
>
Check library(gmm).
Liviu


From niheaven at hotmail.com  Sat Jul 18 19:07:59 2009
From: niheaven at hotmail.com (Hsiao-nan Cheung)
Date: Sun, 19 Jul 2009 01:07:59 +0800
Subject: [R-SIG-Finance] termstrc's bonds dataset creation
Message-ID: <BAY126-DS76F253D1CC2A61599CECED91F0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090719/0b2fe742/attachment.pl>

From landronimirc at gmail.com  Sat Jul 18 21:04:37 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Sat, 18 Jul 2009 21:04:37 +0200
Subject: [R-SIG-Finance] Fwd:  Inequality constraints in GMM estimation?
In-Reply-To: <Pine.LNX.4.43.0907181108480.3563@hymn13.u.washington.edu>
References: <68b1e2610907180603w263f211au9509bb0da2ec00ca@mail.gmail.com> 
	<Pine.LNX.4.43.0907181108480.3563@hymn13.u.washington.edu>
Message-ID: <68b1e2610907181204u17750af9m5a1155068637129c@mail.gmail.com>

This e-mail was probably intended to be on-list.
Liviu


---------- Forwarded message ----------
From: Eric Zivot <ezivot at u.washington.edu>
Date: Sat, Jul 18, 2009 at 8:08 PM
Subject: Re: [R-SIG-Finance] Inequality constraints in GMM estimation?
To: Liviu Andronic <landronimirc at gmail.com>


Not sure your inequality constraint makes sense. Do you want theta > 0
or m(theta) > 0

Usually, inequality constraints are put on parameters. If you want to
constrain theta > 0 then just re-parameterize theta as theta =
exp(gamma) and optimize over gamma. You will need to use delta method
to get the correct std errors for theta.hat = exp(gamma.hat).

****************************************************************
* ?Eric Zivot ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?*
* ?Professor and Gary Waterman Distinguished Scholar ? ? ? ? ? *
* ?Department of Economics ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? *
* ?Adjunct Professor of Finance ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?*
* ?Adjunct Professor of Statistics
* ?Box 353330 ? ? ? ? ? ? ? ? ?email: ?ezivot at u.washington.edu *
* ?University of Washington ? ?phone: ?206-543-6715 ? ? ? ? ? ?*
* ?Seattle, WA 98195-3330 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?*
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? *
* ?www: ?http://faculty.washington.edu/ezivot ? ? ? ? ? ? ? ? ?*
****************************************************************

On Sat, 18 Jul 2009, Liviu Andronic wrote:

> Hello,
>
> On Sat, Jul 18, 2009 at 4:58 AM, David J. Moore, Ph.D.<djmphd at gmail.com> wrote:
>>
>> I have a relatively simple finance application of GMM. Given the moment
>> condition:
>> E[m*R]=0
>>
>> where m=m[theta]
>>
>> I would like to constrain m>0. Any ideas?
>>
> Check library(gmm).
> Liviu
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>





-- 
Do you know how to read?
http://www.alienetworks.com/srtest.cfm
Do you know how to write?
http://garbl.home.comcast.net/~garbl/stylemanual/e.htm#e-mail


From djmphd at gmail.com  Sun Jul 19 01:31:35 2009
From: djmphd at gmail.com (David J. Moore, Ph.D.)
Date: Sat, 18 Jul 2009 18:31:35 -0500
Subject: [R-SIG-Finance] Fwd: Fwd: Inequality constraints in GMM estimation?
In-Reply-To: <2EC65577-0A86-4579-B179-C52EA0EC7167@gmail.com>
References: <2EC65577-0A86-4579-B179-C52EA0EC7167@gmail.com>
Message-ID: <d94e0b160907181631q4e3793d5r60e565f310477732@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090718/a9e08a95/attachment.pl>

From binabina at bellsouth.net  Sun Jul 19 02:12:53 2009
From: binabina at bellsouth.net (zubin)
Date: Sat, 18 Jul 2009 20:12:53 -0400
Subject: [R-SIG-Finance] high frequency  data
Message-ID: <4A626505.7010602@bellsouth.net>

hello, using R for some high frequency research. 

-loading data using getSymbols method, but see no options for fetching a 
current data query in 1 minute increments, looks like day is lowest 
level.   Maybe back 100 minutes or such, with the ability to split by 
Premarket, active, post market trading.  Google does provide this real 
time data, i notice it when i get a quote at google before 9:30AM and 
after 4:00PM.   Google also seems to provide real time pricing that 
reconciles pretty well to other feeds i see. 

-which vendor (google or yahoo) provide split adjusted data:  example 
test with symbol (FAS, it recently split) both do not using 
getSymbols('FAS') or getSymbols('FAS',src='google')

basically my question, before I write this code, does this feature exist 
already in a package? 


thx


From jeff.a.ryan at gmail.com  Sun Jul 19 05:08:45 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sat, 18 Jul 2009 22:08:45 -0500
Subject: [R-SIG-Finance] high frequency data
In-Reply-To: <4A626505.7010602@bellsouth.net>
References: <4A626505.7010602@bellsouth.net>
Message-ID: <e8e755250907182008p2eab7c7uc80d08cdba2f40e6@mail.gmail.com>

Hi Zubin,

The 'real-time' (15 min delayed) from Yahoo is available in quantmod
with getQuote.  You can loop over this and 'capture' your data that
way.  Horrible sounding to me...but it has been done.

AFAIK there is no free historical data access for intraday bars.  Many
external providers that you can grep this list archive to find though.

Spit adjusted data is available from Yahoo in the 'Adjusted' column,
accessible with the Ad() function in quantmod. (assuming a yahoo
sourced dataset).

The newest versions of TTR and quantmod on R-forge (soon to be on
CRAN) have a very nice new feature courtesy of Josh's handiwork.

getSymbols on yahoo, with adjust=TRUE will adjust OHLC based on
downloaded split and dividend data.  That function is in TTR, but
enabled via the getSymbols.yahoo interface.

quantmod also has a less robust, though often perfectly adequate
mechanism as well in 'adjustOHLC', one if its methods actually makes
use of the 'better' TTR routine. Due note that the TTR name has
changed, though the quantmod docs don't yet reflect.  The CRAN version
will be correct...

adjustOHLC             package:quantmod             R Documentation

Adjust Open,High,Low,Close Prices For Splits and Dividends


Details:

     This function calculates the adjusted Open, High, Low, and Close
     prices according to split and dividend information.

     There are three methods available to calculate the new OHLC object
     prices.

     By default, getSplits and getDividends are called to retrieve the
     respective information. These may dispatch to custom methods
     following the "." methodology used by quantmod dispatch.  See
     'getSymbols' for information related to extending quantmod. This
     information is passed to 'adjSplitDiv' from the 'TTR' package, and
     the resulting ratio calculations are used to adjust to observed
     historical prices. This is the most precise way to adjust a
     series.

     The second method works only on standard Yahoo! data containing an
     explicit Adjusted column.

     A final method allows for one to pass a 'ratio' into the function
     directly.

     All methods proceed as follows:

     New columns are derived by taking the  ratio of adjusted value to
     original Close, and multiplying by the difference of the
     respective column and the original Close.  This is then added to
     the modified Close column to arrive at the remaining 'adjusted'
     Open, High, Low column values.

     If no adjustment is needed, the function returns the original data
     unaltered.

Value:

     An object of the original class, with prices adjusted for splits
     and dividends.

Warning:

     Using 'use.Adjusted = TRUE' will be less precise than the method
     that employs actual split and dividend information. This is due to
     loss of precision from Yahoo! using Adjusted columns of only two
     decimal places. The advantage is that this can be run offline, and
     for short series or those with few adjustments the loss of
     precision will be small.

     The resulting precision loss will be from row observation to row
     observation, as the calculation will be exact for intraday values.

HTH
Jeff

On Sat, Jul 18, 2009 at 7:12 PM, zubin<binabina at bellsouth.net> wrote:
> hello, using R for some high frequency research.
> -loading data using getSymbols method, but see no options for fetching a
> current data query in 1 minute increments, looks like day is lowest level.
> Maybe back 100 minutes or such, with the ability to split by Premarket,
> active, post market trading. ?Google does provide this real time data, i
> notice it when i get a quote at google before 9:30AM and after 4:00PM.
> Google also seems to provide real time pricing that reconciles pretty well
> to other feeds i see.
> -which vendor (google or yahoo) provide split adjusted data: ?example test
> with symbol (FAS, it recently split) both do not using getSymbols('FAS') or
> getSymbols('FAS',src='google')
>
> basically my question, before I write this code, does this feature exist
> already in a package?
>
> thx
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From enrique.bengoechea at credit-suisse.com  Mon Jul 20 10:40:44 2009
From: enrique.bengoechea at credit-suisse.com (=?iso-8859-1?Q?Bengoechea_Bartolom=E9_Enrique_=28SIES_73=29?=)
Date: Mon, 20 Jul 2009 10:40:44 +0200
Subject: [R-SIG-Finance] R-finance frameworks (was "Backtesting trade
	systems")
In-Reply-To: <mailman.1.1247824801.20587.r-sig-finance@stat.math.ethz.ch>
Message-ID: <19811401A1D8174CB3EAD7F6072E9B500244A041@chsa1025.share.beluni.net>

Hi all,

Jeff's points are really important. I've also spent many hours with 
him and other debating ways of better coordinating our efforts. 
Here are some thoughts and a proposal:

	THOUGHTS:

1) most work in finance is done closed-source. Finance companies
are reluctant to allow their workers to open source their developments,
or share anything, even if they're using open source tools such as R.
I think this is slowly changing, but still an issue arising all the 
time.

2) even for those of us that have started writting public R packages,
the effort has been relatively isolated. There's no real community
around R and finance yet, as far as I can see. Two groups are
arising around the two annual conferences: one in Switzerland
(R-metrics) and another in Chicago (Jeff, Josh, Brian, Peter..., 
those behind xts, TTR, blotter, PerformanceAnalytics, etc)

But still there're few broader conversations about possible ways
forward in a coordinated way (common needs, frameworks, community).

Let's look for an example at the biostatistics community around the 
Bioconductor project. They have long been working with R. They have
a dedicated package repository, several lists, packages providing
general infrastucture, and lots of specialized packages. I starve
for something like that for R-finance.... 

3) I do think that R itself, being a wonderful platform that is
very flexible and promotes rapid prototyping, in a sense 
doesn't precisely fosters collaboration. Why do I say that?

Because there's an obvious lack of conventions, standards
and guidelines for programming in R. Two "official" object systems,
plus several others created by package writters. No clear coding 
conventions. Lack of modularity on the base code, that does not
lend itself to reusability....

And standards, like them or not, are at the heart of team work 
and collaboration.

	A PROPOSAL:

In my opinion, there are two things we can do to improve 
this situation much before talking about complex thing like 
"general frameworks":

1) Establish common ground via a base set of programming 
guidelines and standards. Just a required base for team work.

2) Establish a "common language" of finance within R. Following
current programming trends, we could call it an R-finance DSL 
(Domain Specific Language).

Within R, a "common language" is defined via the usage of 
generic functions. Generics are at the heart of the R language,
and what R-base does is to define this common language to 
manipulate general data via generics ('c', 'rbind', 'cbind', 
'apply', etc.)

What good R packages do is to extend R behaviour by applying these
generics to new data structures: that's why, in my opinion, 'zoo', 
and then 'xts', were outstanding over initial implementations
of other time series packages: because they followed standard R
generics much closely, reducing the cognitive overload required
to start using the new packages. 

The problem with R in finance is that, lacking guidelines and a 
common language, each package author has come up with  completely 
different conventions and names for the same things.

As a simple example, look at one of the most basic calculations
within finance: compute instrument returns. We have more than
5 different names for the same concept in different packages:

* quantmod:
periodReturn(x, period = "monthly", subset = NULL, 
	type = "arithmetic", leading = TRUE, ...)

* timeSeries
returns(x, 
	method = c("continuous", "discrete", "compound", "simple"), 
	percentage = FALSE, ...)
getReturns(...)
returnSeries(...)		
	
* PerformanceAnalytics
Return.calculate(prices, method = c("compound", "simple"))
Return.cumulative(R, geometric = TRUE)	

* TTR
ROC(x, n = 1, type = c("continuous", "discrete"), na = NA)

and I think there're several others I'm now missing. Similar
examples would go for all types of funcions related to portfolios,
simulation, backtesting, risk modelling, etc.


What's the point? These differences start to difficult 
interoperability of code, and we have not yet started to talk 
about modelling instruments, or creating frameworks.

Participation of package authors and detailed documentation 
of the design choices taken would be some of the key issues for
a package providing these common generics. Default methods for
the generics could be included but they would not be so
important for a first step.

Using the common generics, package authors could start to
apply those generics and add methods for them. No need to
remake their older APIs, just add the new generics as wrappers
to existing code and slowly migrate.

NOTE: Jeff and I already started to think about this at the last 
Chicago conference, and produced the simplest prototype, called 
"finbase". All R-finance developers are invited to add to this
effort.


Only after that, we could really start to think further 
(frameworks?)

I look forward to your opinions.

Best,

Enrique

------------------------------
Date: Thu, 16 Jul 2009 11:32:31 -0500
From: Jeff Ryan <jeff.a.ryan at gmail.com>
Subject: Re: [R-SIG-Finance] Backtesting trade systems
To: Robert Sams <robert at sanctumfi.com>
Cc: r-sig-finance at stat.math.ethz.ch
Message-ID:
	<e8e755250907160932t41eb4734uef833bd3c371154 at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1

All,

I suspect this topic/conversation is central to most of us -- and
quite important to R and R-Finance in general.

There are two primary issues that we seem to be encountering at this
point in the R-Finance community:

1) consistency of design: so all of our work becomes additive.
2) generality: writing code that works in the most general case possible

The first is quite difficult.  I have personally been involved in
*hundreds* of hours of productive (?) conversation with dozens of
people on this topic, be it on the list, useR, NY, Meielisalp,
R/Finance Chicago and back again at Meielisalp (and many sessions
involving Jak's tap in Chicago).  All of the conversations have been
certainly valuable, though even after all of this I still can't figure
out the best path...

One thing that I have worked on personally is making consistency
possible.  'xts' was born of this issue.  We didn't need another
time-series class.  We had 9 or 10.  We needed (or I needed) a way to
abstract the class and just get the job done.  'xts' does that
reasonably well -- though still far from complete.  The same issue
exists for portfolio level data, as well numerous other 'classes'.

Everyone should be free to use their own data object, what would be
nice (and required for maximum usefulness in my opinion) is if that
choice wasn't imposed on the end-user to use your software.  This
leads to more users, more feedback, better code, and a more 'complete'
landscape of tools that can be used.

As a time series example -- xts lets the developer use one type of
object (simplifying code), and accept *ALL* objects (simplifying the
user experience).

The point of the above is that interface counts, and it counts for a
lot.  If a framework is to work, it needs to be accessible to all
users.

The second point I'll toss out there is one of generality.

This is just as difficult as the first conceptually, though I would
argue possibly even more intractable.  We just can't individually
understand what we collectively require.

quantmod set about in 2007 trying to create a 'framework' and has
failed miserably.  It does some things like data management and
charting quite well, but the often asked question is how does
specifyModel/buildModel/tradeModel work.  The short answer is that it
does, and it doesn't.  It works on one type of strategy (EOD), and
doesn't on others:

(...)

Looks great!  But it is useless.  It is a casualty of specificity.  A
generalized framework would be awesome, but I think it is a much
larger task than I can handle.  I suspect that general to one person
is fantastically restrictive to another.

R is the 'general' framework.

What we really need is consistency of the pieces that make
building/testing models in *R* easier.

This comes from projects like blotter I think.  Were we can use parts
that we want, and only those that we want.

As I said at the start, this is a lot more involved than it looks
like.  Obviously best of luck to all who take up the challenge.  This
thread is an awesome start to the (larger) public conversation that
has been taking place over beers for quite some time now.

Best,
Jeff

-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From windspeedo99 at gmail.com  Mon Jul 20 10:57:56 2009
From: windspeedo99 at gmail.com (Wind)
Date: Mon, 20 Jul 2009 16:57:56 +0800
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
In-Reply-To: <4A5F2A38.9060100@braverock.com>
References: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>
	<4A5F2A38.9060100@braverock.com>
Message-ID: <d718c8210907200157m1b9c9698me96a6af142d85e87@mail.gmail.com>

Some progress.   The LPPL curve could be plotted with the following codes.
The problem now is how to get the best fit parameters.
Some researchers  use python or matlab for LPPL calibrating.     It
seems that some of them prefer tabu search for optimums locating.   It
seems that there's still no general function for tabu search in R.
At the end of codes, I give the possible parameter combinations to be
searched in, maybe there are other functions for optimum searching in
R.
Any suggestion would be appreciated.


## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
Financial and Economic Crisis
## http://arxiv.org/abs/0905.0220
## Fig. 23 S&P500 index (in logarithmic scale)in  Page 39


library(quantmod)

LPPL1<-function(p,dtc=20,alpha=0.35,omega=0.1,phi=1)
{
	#function in page 26 of http://arxiv.org/abs/0905.0220
	#the basic form of LPPL
	dtc=abs(floor(dtc))
	tc<-length(p)+dtc
	dt<-abs(tc-(1:length(p)))

	x1<-dt^alpha
	x2<-(dt^alpha)*cos(omega*log(dt)+phi)
	
	f<-lm(log(p) ~ x1+x2)
	
	f$para<-list(recno=dim(f$model)[1],dtc=dtc,alpha=alpha,omega=omega,phi=phi,sigma=summary(f)$sigma)
	return(f)
}

opt.lppl<-function(x)
{
	#derived function for optim
	return(LPPL1(p,dtc=x[1],alpha=x[2],omega=x[3],phi=x[4])$para$sigma)
}

LPPL1.x<-function(lpplf,pt=100)
{
	#x axis for predicting
	dt<-abs((lpplf$recno+lpplf$dtc)-(1:(lpplf$recno+lpplf$dtc+pt)))
	dt[dt==0]<-0.5
	
	x1<-dt^lpplf$alpha
	x2<-(dt^lpplf$alpha)*cos(lpplf$omega*log(dt)+lpplf$phi)
	return(list(x1=x1,x2=x2))
	
}

#get the SP500 index
pr<-getSymbols("^GSPC",auto.assign=FALSE,from="2003-10-1",to="2007-05-16")[,4]
p<-as.numeric(pr)

plot(p,type="l",log="y",xlim=c(0,length(p)+100),ylim=c(min(p),max(p)*1.2))
abline(v=length(p),col="green")

#something like the Fig. 23 in  Page 39 of http://arxiv.org/abs/0905.0220
#but obviously the result is not calibrated well
#using optim like this can not calibrate the LPPL model
opts<-sapply(seq(1,50,10),function(x){
			o<-optim(c(x,0.6,20,1),opt.lppl)
			f3<-LPPL1(p,dtc=o$par[1],alpha=o$par[2],omega=o$par[3],phi=o$par[4])
			xp<-LPPL1.x(f3$para,200)
			f3p<-predict(f3,data.frame(x1=xp$x1,x2=xp$x2))
			lines(exp(f3p),col="blue")
			lines(exp(fitted(f3)),col="red")
			f3$para
		})



##crash point after dtc days
dtc<-seq(1,100,1)

##appropraite range of the parametes of LPPL
##according to Dr. W.X. Zhou's new book which is in Chinese
##the increments of the sequences are added according to my own judement
alpha<-seq(0.01,1.2,0.1)
omega<-seq(0,40,1)
phi<-seq(0,7,0.1)

##millions possible combinations
#complete test would be difficult
para<-expand.grid(dtc=dtc,alpha=alpha,omega=omega,phi=phi)
dim(para)
system.time(sigs<-apply(para[1:100,],1,function(x){LPPL1(p,dtc=x[1],alpha=x[2],omega=x[3],phi=x[4])$para$sigma}))

##methods for minimum sigma searching within the parameter combinations
##not implemented yet


wind





On Thu, Jul 16, 2009 at 9:25 PM, Brian G. Peterson<brian at braverock.com> wrote:
> So first, using your real name and ideally your professional identity, ask
> for the python code. ?Better yet, get an academic buddy to do it. Usually
> getting access to the code isn't too tough. ?Mention things like "repeatable
> research" and "collaboration" in your email. ?Two of the authors publish
> their email addresses in one of the papers you reference, so contacting them
> should be easy.
>
> Next port the python code to R.
>
> If you can't do that, then replicate the model in R "from scratch". ?A
> trivial scan of the paper in question lends several techniques that are well
> covered in R: AR, GARCH, power laws, linear regression, stochastic discount
> factor, Ornstein-Uhlenbeck, etc.
> There are volumes of information available on these topics from within R, in
> numerous books, and in the archives of this mailing list and r-help.
>
> You're going to have to do your replication in pieces, probably starting
> with their implementation of the log periodic power law (LPPL), for which I
> do not believe there is an existing direct analogue in R though all the
> component parts necessary to replicate it should be readily available.
>
> As you work on each step of the replication, share your code with this list
> and the problems you are having with a particular step. ?Ask specific,
> directed questions with code to back them up. ?Someone will likely help you
> solve the specific problem.
>
> In R generally, it is not necessary that you be able to *do* the math (think
> pencil and paper), but if you plan to replicate published work, it will be
> necessary to *understand* at least some of how the math works, and to be
> able to pick out the names of techniques that you can search for an utilize.
>
> Basically, I'm recommending that you (specifically) and others (more
> generally) should share the process of replicating a technique like this, as
> well as the final product, to give all the rest of us who are likely to be
> helping "you" get all this done. quid pro quo.
>
> Cheers,
>
> ?- Brian
>
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
>
>
> Wind wrote:
>>
>> Prof. Sornette has spent years forcasting bubble burst with
>> "log-periodic power law". ? ?The latest paper ?gives "a
>> self-consistent model for explosive financial bubbles, which combines
>> a mean-reverting volatility process and a stochastic conditional
>> return which reflects nonlinear positive feedbacks and continuous
>> updates of the investors' beliefs and sentiments."
>>
>> And his ?latest ?predicting is the burst of Chinese equity bubble at
>> the end of July. ? ? http://arxiv.org/abs/0907.1827
>>
>> While waiting to see the result, I wonder whether it is possible to
>> replicate the forcast with R. ?The model is in the page 10 of the "A
>> Consistent Model of `Explosive' Financial Bubbles With Mean-Reversing
>> Residuals", ?http://arxiv.org/abs/0905.0128 ?. ? The output chart is
>> in the page 3 of "The Chinese Equity Bubble: Ready to Burst",
>> http://arxiv.org/abs/0907.1827 . ? I guess the authors of the latter
>> paper use the same model as described in the first paper.
>>
>> Because statistics is still challenging for me though I could use R
>> for ?basic data manipulations, ?I wonder which package or function
>> would be necessary to implement the model in the paper. ?The model
>> seems more complicated than the models in the R tutorials for me.
>> By the way, the author of the paper used Python and the codes are
>> private.
>>
>> Any suggestion would be highly appreciated.
>>
>
>
>


From ravis at ambaresearch.com  Mon Jul 20 11:37:57 2009
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Mon, 20 Jul 2009 15:07:57 +0530
Subject: [R-SIG-Finance] American Option implied volatility
Message-ID: <B8948310FD4828478CBCC2093E5096B726CF62@BAN-MAILSRV02.Amba.com>

Hi R,

I am estimating the implied volatility for American options.
I have split the data based on date. 
dat1[[i]]
         Date     Expiry Strike.Price Settle.Price   Spot  RF rate
Time
7  2007-01-03 2007-01-25          900         28.8 894.55 0.0848
0.06746032
8  2007-01-03 2007-01-25          880         40.0 894.55 0.0848
0.06746032
9  2007-01-03 2007-01-25          840         45.0 894.55 0.0848
0.06746032
10 2007-01-03 2007-01-25          940         11.0 894.55 0.0848
0.06746032
11 2007-01-03 2007-01-25          920         19.0 894.55 0.0848
0.06746032

But when I try 

apply(dat1[[i]],1,function(x){AmericanOptionImpliedVolatility.default("c
all",x[4],x[5],x[3],x[6],x[7],volatility=0.1,dividendYield=0)[[1]]})

I get Error in AmericanOptionImpliedVolatility.default("call", x[4],
x[5], x[3],  : 
  Exception: root not bracketed: f[1e-07,4] ->
[1.428540e+01,3.264693e+02]


 How do I resolve this? Any help would be appreciated

> sessionInfo()
R version 2.9.0 (2009-04-17) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] tcltk     stats     graphics  grDevices utils     datasets  methods
base     

other attached packages:
 [1] RQuantLib_0.2.11    Rcpp_0.6.5          fOptions_251.70
fSeries_251.70      nnet_7.2-46         mgcv_1.3-26
fBasics_240.10068.1
 [8] fCalendar_251.70    MASS_7.2-46         fEcofin_251.70
spatial_7.2-46     

loaded via a namespace (and not attached):
[1] tools_2.9.0


Thank you,
Ravi Shankar S 
This e-mail may contain confidential and/or privileged i...{{dropped:10}}


From breman.mark at gmail.com  Mon Jul 20 13:28:13 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Mon, 20 Jul 2009 13:28:13 +0200
Subject: [R-SIG-Finance] Questions/issues about new Tradesys package
Message-ID: <5e6a2e670907200428n1510d763g92c618166815b8e4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090720/eade3333/attachment.pl>

From breman.mark at gmail.com  Mon Jul 20 14:13:15 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Mon, 20 Jul 2009 14:13:15 +0200
Subject: [R-SIG-Finance] Pattern recognition in timeseries data
Message-ID: <5e6a2e670907200513uc0d318dga31e77cd18a2a683@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090720/82f88526/attachment.pl>

From brian at braverock.com  Mon Jul 20 15:22:06 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 20 Jul 2009 08:22:06 -0500
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
In-Reply-To: <d718c8210907200157m1b9c9698me96a6af142d85e87@mail.gmail.com>
References: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>	
	<4A5F2A38.9060100@braverock.com>
	<d718c8210907200157m1b9c9698me96a6af142d85e87@mail.gmail.com>
Message-ID: <4A646F7E.3090406@braverock.com>

Glad to see you're making progress on this problem.

This paper
http://www.diegm.uniud.it/satt/papers/DiSc06b.pdf

implements a tabu search in R, though they didn't publish their code.  
You might want to contact them for the implementation and permission to 
share their tabu search algorithm/code with the R community.

They also reference an R package called RACE by this gentleman:
http://iridia.ulb.ac.be/~mbiro/
that they use for evaluating the solution.

More generally, from my limited understanding, tabu search is an 
extension and refinement of simulated annealing approaches easily 
implemented in R.  In brief the approach is to take your best 'n' 
solutions from a random space search, and then search "near" those 
solutions.  Simulated annealing and its close cousins have a lot of 
benefits in finance, where a single true optima from a closed form 
problem is not likely to be available.  I personally have rarely found 
'optim' to be usable for my problem space, and have had to use other 
solvers for practical problems in finance.

Regards,

  - Brian

Wind wrote:
> Some progress.   The LPPL curve could be plotted with the following codes.
> The problem now is how to get the best fit parameters.
> Some researchers  use python or matlab for LPPL calibrating.     It
> seems that some of them prefer tabu search for optimums locating.   It
> seems that there's still no general function for tabu search in R.
> At the end of codes, I give the possible parameter combinations to be
> searched in, maybe there are other functions for optimum searching in
> R.
> Any suggestion would be appreciated.
>
>
> ## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
> Financial and Economic Crisis
> ## http://arxiv.org/abs/0905.0220
> ## Fig. 23 S&P500 index (in logarithmic scale)in  Page 39
>
>
> library(quantmod)
>
> LPPL1<-function(p,dtc=20,alpha=0.35,omega=0.1,phi=1)
> {
> 	#function in page 26 of http://arxiv.org/abs/0905.0220
> 	#the basic form of LPPL
> 	dtc=abs(floor(dtc))
> 	tc<-length(p)+dtc
> 	dt<-abs(tc-(1:length(p)))
>
> 	x1<-dt^alpha
> 	x2<-(dt^alpha)*cos(omega*log(dt)+phi)
> 	
> 	f<-lm(log(p) ~ x1+x2)
> 	
> 	f$para<-list(recno=dim(f$model)[1],dtc=dtc,alpha=alpha,omega=omega,phi=phi,sigma=summary(f)$sigma)
> 	return(f)
> }
>
> opt.lppl<-function(x)
> {
> 	#derived function for optim
> 	return(LPPL1(p,dtc=x[1],alpha=x[2],omega=x[3],phi=x[4])$para$sigma)
> }
>
> LPPL1.x<-function(lpplf,pt=100)
> {
> 	#x axis for predicting
> 	dt<-abs((lpplf$recno+lpplf$dtc)-(1:(lpplf$recno+lpplf$dtc+pt)))
> 	dt[dt==0]<-0.5
> 	
> 	x1<-dt^lpplf$alpha
> 	x2<-(dt^lpplf$alpha)*cos(lpplf$omega*log(dt)+lpplf$phi)
> 	return(list(x1=x1,x2=x2))
> 	
> }
>
> #get the SP500 index
> pr<-getSymbols("^GSPC",auto.assign=FALSE,from="2003-10-1",to="2007-05-16")[,4]
> p<-as.numeric(pr)
>
> plot(p,type="l",log="y",xlim=c(0,length(p)+100),ylim=c(min(p),max(p)*1.2))
> abline(v=length(p),col="green")
>
> #something like the Fig. 23 in  Page 39 of http://arxiv.org/abs/0905.0220
> #but obviously the result is not calibrated well
> #using optim like this can not calibrate the LPPL model
> opts<-sapply(seq(1,50,10),function(x){
> 			o<-optim(c(x,0.6,20,1),opt.lppl)
> 			f3<-LPPL1(p,dtc=o$par[1],alpha=o$par[2],omega=o$par[3],phi=o$par[4])
> 			xp<-LPPL1.x(f3$para,200)
> 			f3p<-predict(f3,data.frame(x1=xp$x1,x2=xp$x2))
> 			lines(exp(f3p),col="blue")
> 			lines(exp(fitted(f3)),col="red")
> 			f3$para
> 		})
>
>
>
> ##crash point after dtc days
> dtc<-seq(1,100,1)
>
> ##appropraite range of the parametes of LPPL
> ##according to Dr. W.X. Zhou's new book which is in Chinese
> ##the increments of the sequences are added according to my own judement
> alpha<-seq(0.01,1.2,0.1)
> omega<-seq(0,40,1)
> phi<-seq(0,7,0.1)
>
> ##millions possible combinations
> #complete test would be difficult
> para<-expand.grid(dtc=dtc,alpha=alpha,omega=omega,phi=phi)
> dim(para)
> system.time(sigs<-apply(para[1:100,],1,function(x){LPPL1(p,dtc=x[1],alpha=x[2],omega=x[3],phi=x[4])$para$sigma}))
>
> ##methods for minimum sigma searching within the parameter combinations
> ##not implemented yet
>
>
> wind
>
>
>
>
>
> On Thu, Jul 16, 2009 at 9:25 PM, Brian G. Peterson<brian at braverock.com> wrote:
>   
>> So first, using your real name and ideally your professional identity, ask
>> for the python code.  Better yet, get an academic buddy to do it. Usually
>> getting access to the code isn't too tough.  Mention things like "repeatable
>> research" and "collaboration" in your email.  Two of the authors publish
>> their email addresses in one of the papers you reference, so contacting them
>> should be easy.
>>
>> Next port the python code to R.
>>
>> If you can't do that, then replicate the model in R "from scratch".  A
>> trivial scan of the paper in question lends several techniques that are well
>> covered in R: AR, GARCH, power laws, linear regression, stochastic discount
>> factor, Ornstein-Uhlenbeck, etc.
>> There are volumes of information available on these topics from within R, in
>> numerous books, and in the archives of this mailing list and r-help.
>>
>> You're going to have to do your replication in pieces, probably starting
>> with their implementation of the log periodic power law (LPPL), for which I
>> do not believe there is an existing direct analogue in R though all the
>> component parts necessary to replicate it should be readily available.
>>
>> As you work on each step of the replication, share your code with this list
>> and the problems you are having with a particular step.  Ask specific,
>> directed questions with code to back them up.  Someone will likely help you
>> solve the specific problem.
>>
>> In R generally, it is not necessary that you be able to *do* the math (think
>> pencil and paper), but if you plan to replicate published work, it will be
>> necessary to *understand* at least some of how the math works, and to be
>> able to pick out the names of techniques that you can search for an utilize.
>>
>> Basically, I'm recommending that you (specifically) and others (more
>> generally) should share the process of replicating a technique like this, as
>> well as the final product, to give all the rest of us who are likely to be
>> helping "you" get all this done. quid pro quo.
>>
>> Cheers,
>>
>>  - Brian
>>
>>
>> --
>> Brian G. Peterson
>> http://braverock.com/brian/
>> Ph: 773-459-4973
>> IM: bgpbraverock
>>
>>
>>
>> Wind wrote:
>>     
>>> Prof. Sornette has spent years forcasting bubble burst with
>>> "log-periodic power law".    The latest paper  gives "a
>>> self-consistent model for explosive financial bubbles, which combines
>>> a mean-reverting volatility process and a stochastic conditional
>>> return which reflects nonlinear positive feedbacks and continuous
>>> updates of the investors' beliefs and sentiments."
>>>
>>> And his  latest  predicting is the burst of Chinese equity bubble at
>>> the end of July.     http://arxiv.org/abs/0907.1827
>>>
>>> While waiting to see the result, I wonder whether it is possible to
>>> replicate the forcast with R.  The model is in the page 10 of the "A
>>> Consistent Model of `Explosive' Financial Bubbles With Mean-Reversing
>>> Residuals",  http://arxiv.org/abs/0905.0128  .   The output chart is
>>> in the page 3 of "The Chinese Equity Bubble: Ready to Burst",
>>> http://arxiv.org/abs/0907.1827 .   I guess the authors of the latter
>>> paper use the same model as described in the first paper.
>>>
>>> Because statistics is still challenging for me though I could use R
>>> for  basic data manipulations,  I wonder which package or function
>>> would be necessary to implement the model in the paper.  The model
>>> seems more complicated than the models in the R tutorials for me.
>>> By the way, the author of the paper used Python and the codes are
>>> private.
>>>
>>> Any suggestion would be highly appreciated.
>>>
>>>       
>>
>>     


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From davidr at rhotrading.com  Mon Jul 20 16:22:25 2009
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Mon, 20 Jul 2009 09:22:25 -0500
Subject: [R-SIG-Finance] Random numbers with positive skewness
In-Reply-To: <D225F5FF-DC2F-4BF7-AB50-60BD349753BB@jtoll.com>
References: <D225F5FF-DC2F-4BF7-AB50-60BD349753BB@jtoll.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE7703585908@rhopost.rhotrading.com>

Why not just use the vols you seem to have?
I suppose you could sample from the empirical distribution of the vols, 
or you could guess a distribution, say the gamma, estimate the
parameters, and sample from that.
Any skewed density with support the positive reals would probably be as
good for your purposes,
i.e., equally unrealistic.

David L. Reiner, PhD
Head Quant
Rho Trading Securities, LLC

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of James Toll
Sent: Friday, July 17, 2009 10:34 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Random numbers with positive skewness

Hi,

I've been debating how to go about acquiring the historical data  
necessary to backtest an indexing idea and I finally decided that  
maybe I should first try it out on some randomly generated time series  
data. So, for starters, I need to generate 100 time series to  
represent 100 different equities, like, for example, the OEX.  To take  
the place of 10 years of closing prices, I thought I could simply  
generate 2520 price relatives using something along the lines of this:

x <- rnorm(2520, mean = 1, sd = 0.02)

But obviously, it's not likely that each of the components of a cap  
weighted index of 100 equities is going to have price relatives with  
an SD of 0.02.  So I'd like to be able to vary the SD for each.  I  
thought I could just as easily randomly generate a vector of SD's for  
use in generating each time series like so:

y<-rnorm(100, mean = 0.025, sd = 0.007)

The problem I'm running into is that when generating the SD's for each  
of the 100 time series my wild guess is that the mean might be  
somewhere between 0.02 and 0.03, and I think the SD might be somewhere  
around 0.007, but I don't think a normal distribution really works at  
all.  I think I need a lot of positive skewness to the distribution.

BTW, all of these wild guesses are simply based upon my experience as  
an option market maker (which may be worthless to this task), and  
there are lots of equities that normally trade between 30 and 50  
volatility, but then I've also traded tech stocks with vols around 80  
and 90.  So basically I think the bulk of the distribution of SD's is  
between 0.02 and 0.03, they taper off on the left side around 0.01,  
maybe a little lower, but then on the right side the long tail goes up  
to around 0.06.  If my assumptions / conclusions are totally off base  
please feel free to tell me.  This is definitely my first attempt at  
any kind of backtesting.

Is there some other method of generating random numbers that will  
allow me to control the skewness of the distribution?  Thanks.

James

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


This e-mail and any materials attached hereto, including, without limitation, all content hereof and thereof (collectively, "Rho Content") are confidential and proprietary to Rho Trading Securities, LLC ("Rho") and/or its affiliates, and are protected by intellectual property laws.  Without the prior written consent of Rho, the Rho Content may not (i) be disclosed to any third party or (ii) be reproduced or otherwise used by anyone other than current employees of Rho or its affiliates, on behalf of Rho or its affiliates.

THE RHO CONTENT IS PROVIDED AS IS, WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND.  TO THE MAXIMUM EXTENT PERMISSIBLE UNDER APPLICABLE LAW, RHO HEREBY DISCLAIMS ANY AND ALL WARRANTIES, EXPRESS AND IMPLIED, RELATING TO THE RHO CONTENT, AND NEITHER RHO NOR ANY OF ITS AFFILIATES SHALL IN ANY EVENT BE LIABLE FOR ANY DAMAGES OF ANY NATURE WHATSOEVER, INCLUDING, BUT NOT LIMITED TO, DIRECT, INDIRECT, CONSEQUENTIAL, SPECIAL AND PUNITIVE DAMAGES, LOSS OF PROFITS AND TRADING LOSSES, RESULTING FROM ANY PERSON'S USE OR RELIANCE UPON, OR INABILITY TO USE, ANY RHO CONTENT, EVEN IF RHO IS ADVISED OF THE POSSIBILITY OF SUCH DAMAGES OR IF SUCH DAMAGES WERE FORESEEABLE.


From davidr at rhotrading.com  Mon Jul 20 16:44:50 2009
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Mon, 20 Jul 2009 09:44:50 -0500
Subject: [R-SIG-Finance] American Option implied volatility
In-Reply-To: <B8948310FD4828478CBCC2093E5096B726CF62@BAN-MAILSRV02.Amba.com>
References: <B8948310FD4828478CBCC2093E5096B726CF62@BAN-MAILSRV02.Amba.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE7703585914@rhopost.rhotrading.com>

Just a guess that entry #9 is causing a problem since the option value
is below parity, so there is no implied vol. Probably the function
should check that first before it tries to solve for vol.

David L. Reiner, PhD
Head Quant
Rho Trading Securities, LLC


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ravi S.
Shankar
Sent: Monday, July 20, 2009 4:38 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] American Option implied volatility

Hi R,

I am estimating the implied volatility for American options.
I have split the data based on date. 
dat1[[i]]
         Date     Expiry Strike.Price Settle.Price   Spot  RF rate
Time
7  2007-01-03 2007-01-25          900         28.8 894.55 0.0848
0.06746032
8  2007-01-03 2007-01-25          880         40.0 894.55 0.0848
0.06746032
9  2007-01-03 2007-01-25          840         45.0 894.55 0.0848
0.06746032
10 2007-01-03 2007-01-25          940         11.0 894.55 0.0848
0.06746032
11 2007-01-03 2007-01-25          920         19.0 894.55 0.0848
0.06746032

But when I try 

apply(dat1[[i]],1,function(x){AmericanOptionImpliedVolatility.default("c
all",x[4],x[5],x[3],x[6],x[7],volatility=0.1,dividendYield=0)[[1]]})

I get Error in AmericanOptionImpliedVolatility.default("call", x[4],
x[5], x[3],  : 
  Exception: root not bracketed: f[1e-07,4] ->
[1.428540e+01,3.264693e+02]


 How do I resolve this? Any help would be appreciated

> sessionInfo()
R version 2.9.0 (2009-04-17) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] tcltk     stats     graphics  grDevices utils     datasets  methods
base     

other attached packages:
 [1] RQuantLib_0.2.11    Rcpp_0.6.5          fOptions_251.70
fSeries_251.70      nnet_7.2-46         mgcv_1.3-26
fBasics_240.10068.1
 [8] fCalendar_251.70    MASS_7.2-46         fEcofin_251.70
spatial_7.2-46     

loaded via a namespace (and not attached):
[1] tools_2.9.0


Thank you,
Ravi Shankar S 
This e-mail may contain confidential and/or privileged
i...{{dropped:10}}

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


This e-mail and any materials attached hereto, including, without limitation, all content hereof and thereof (collectively, "Rho Content") are confidential and proprietary to Rho Trading Securities, LLC ("Rho") and/or its affiliates, and are protected by intellectual property laws.  Without the prior written consent of Rho, the Rho Content may not (i) be disclosed to any third party or (ii) be reproduced or otherwise used by anyone other than current employees of Rho or its affiliates, on behalf of Rho or its affiliates.

THE RHO CONTENT IS PROVIDED AS IS, WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND.  TO THE MAXIMUM EXTENT PERMISSIBLE UNDER APPLICABLE LAW, RHO HEREBY DISCLAIMS ANY AND ALL WARRANTIES, EXPRESS AND IMPLIED, RELATING TO THE RHO CONTENT, AND NEITHER RHO NOR ANY OF ITS AFFILIATES SHALL IN ANY EVENT BE LIABLE FOR ANY DAMAGES OF ANY NATURE WHATSOEVER, INCLUDING, BUT NOT LIMITED TO, DIRECT, INDIRECT, CONSEQUENTIAL, SPECIAL AND PUNITIVE DAMAGES, LOSS OF PROFITS AND TRADING LOSSES, RESULTING FROM ANY PERSON'S USE OR RELIANCE UPON, OR INABILITY TO USE, ANY RHO CONTENT, EVEN IF RHO IS ADVISED OF THE POSSIBILITY OF SUCH DAMAGES OR IF SUCH DAMAGES WERE FORESEEABLE.


From spencer.graves at prodsyse.com  Mon Jul 20 16:55:11 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Mon, 20 Jul 2009 08:55:11 -0600
Subject: [R-SIG-Finance] Random numbers with positive skewness
In-Reply-To: <D225F5FF-DC2F-4BF7-AB50-60BD349753BB@jtoll.com>
References: <D225F5FF-DC2F-4BF7-AB50-60BD349753BB@jtoll.com>
Message-ID: <4A64854F.20606@prodsyse.com>

      I believe that GARCH models tend to fit real data better than a 
normal, and GARCH with a skewed t often fits even better.  I'm not 
absolutely certain, but I believe a skewed t distribution can be 
estimated with the "garchFit" function in the "fGarch" package.  If it 
were my problem, I'd try "garchFit" to several real data set using a 
skewed t before I used simulation.  Then you could simulate more numbers 
using "garchSim". 


      To look for capabilities in R for historical price data, I did the 
following: 


 library(RSiteSearch)
hpd <- RSiteSearch.function('historical price data')
HTML(hpd)


      This found 10 help pages in 8 contributed packages containing the 
indicated search term and displayed the results in a browser. 


      Hope this helps. 
      Spencer     


James Toll wrote:
> Hi,
>
> I've been debating how to go about acquiring the historical data 
> necessary to backtest an indexing idea and I finally decided that 
> maybe I should first try it out on some randomly generated time series 
> data. So, for starters, I need to generate 100 time series to 
> represent 100 different equities, like, for example, the OEX.  To take 
> the place of 10 years of closing prices, I thought I could simply 
> generate 2520 price relatives using something along the lines of this:
>
> x <- rnorm(2520, mean = 1, sd = 0.02)
>
> But obviously, it's not likely that each of the components of a cap 
> weighted index of 100 equities is going to have price relatives with 
> an SD of 0.02.  So I'd like to be able to vary the SD for each.  I 
> thought I could just as easily randomly generate a vector of SD's for 
> use in generating each time series like so:
>
> y<-rnorm(100, mean = 0.025, sd = 0.007)
>
> The problem I'm running into is that when generating the SD's for each 
> of the 100 time series my wild guess is that the mean might be 
> somewhere between 0.02 and 0.03, and I think the SD might be somewhere 
> around 0.007, but I don't think a normal distribution really works at 
> all.  I think I need a lot of positive skewness to the distribution.
>
> BTW, all of these wild guesses are simply based upon my experience as 
> an option market maker (which may be worthless to this task), and 
> there are lots of equities that normally trade between 30 and 50 
> volatility, but then I've also traded tech stocks with vols around 80 
> and 90.  So basically I think the bulk of the distribution of SD's is 
> between 0.02 and 0.03, they taper off on the left side around 0.01, 
> maybe a little lower, but then on the right side the long tail goes up 
> to around 0.06.  If my assumptions / conclusions are totally off base 
> please feel free to tell me.  This is definitely my first attempt at 
> any kind of backtesting.
>
> Is there some other method of generating random numbers that will 
> allow me to control the skewness of the distribution?  Thanks.
>
> James
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From marco.zanella at inbox.com  Mon Jul 20 20:31:00 2009
From: marco.zanella at inbox.com (Zanella Marco)
Date: Mon, 20 Jul 2009 10:31:00 -0800
Subject: [R-SIG-Finance] Best practice in trading model reporting
	(PerformanceAnalytics)
Message-ID: <73C4F3B16AE.000002AAmarco.zanella@inbox.com>

Dear Sirs,
I'm working on a standardized report for trading models I'll build. Of course I've found very helpful, my thanks to authors, the PerformanceAnalytics package.

Let me underline that in my opinion the most interesting feature of package is its scalability, or something similar. In fact, you can use the included statistics (Sharpe, maxDD, Var, etc...) or complete the report with statistics of your own or not included in the package.

I would like to ask you what is the best/smartest output format or what kind of output format you usually use for internal model report. In other words: what is the best practice? A single XLS file, TXT report or a complex PDF file (morningstar like) with all the informations about model?

Regards,

Marco


From multeesl at yahoo.co.uk  Mon Jul 20 20:32:02 2009
From: multeesl at yahoo.co.uk (bonjourbc9)
Date: Mon, 20 Jul 2009 11:32:02 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] COPULA package in R ~ need help on
	error message
Message-ID: <24575096.post@talk.nabble.com>


Hi folks, I am a newbie to R and I need some advice on an error message while
running the copula package.
Basically I am trying to model a COPULA- GARCH to study the dependence
across international stock returns.

I ran the "fgarch" to fit a AR(1)-GJR GARCH(1,1) ~skewed t to the returns
and obtained the standardised residuals. using the psstd function, I obtain
the marginal cdf for these residuals, this give me a list of u and v. 

What I did next was to convert the u , v into a data matrix as follows;

>V1<-as.vector(u)
>V2<-as.vector(v)
>x<-cbind( V1, V2)

then when i use the formula from copula package >fit.ml <-
fitCopula(gumbel.cop, x, method="ml") , i get this error message ; 

Error in fitCopula.ml(data, copula, start, lower, upper, optim.control,  :
  trying to get slot "dimension" from an object (class "data.frame") that is
not an S4 object

I am at a loss what this error means. Can anyone please advise >?
-- 
View this message in context: http://www.nabble.com/COPULA-package-in-R-%7E-need-help-on-error-message-tp24575096p24575096.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From peter at braverock.com  Mon Jul 20 21:21:38 2009
From: peter at braverock.com (Peter Carl)
Date: Mon, 20 Jul 2009 14:21:38 -0500 (CDT)
Subject: [R-SIG-Finance] Best practice in trading model reporting
 (PerformanceAnalytics)
In-Reply-To: <73C4F3B16AE.000002AAmarco.zanella@inbox.com>
References: <73C4F3B16AE.000002AAmarco.zanella@inbox.com>
Message-ID: <4417.64.190.216.194.1248117698.squirrel@mail.braverock.com>

Take a look at the slides for a workshop that Brian and I did earlier this
year:

http://rinfinance.quantmod.com/presentations/PA%20Workshop%20Chi%20RFinance%202009-04.pdf

As you note, PA doesn't presume to prescribe an analysis, but contains
many common tools that may be useful.  The presentation might help give a
better understanding of some of the things that we've found useful through
time.

pcc
-- 
Peter Carl
http://www.braverock.com/~peter

> Dear Sirs,
> I'm working on a standardized report for trading models I'll build. Of
> course I've found very helpful, my thanks to authors, the
> PerformanceAnalytics package.
>
> Let me underline that in my opinion the most interesting feature of
> package is its scalability, or something similar. In fact, you can use the
> included statistics (Sharpe, maxDD, Var, etc...) or complete the report
> with statistics of your own or not included in the package.
>
> I would like to ask you what is the best/smartest output format or what
> kind of output format you usually use for internal model report. In other
> words: what is the best practice? A single XLS file, TXT report or a
> complex PDF file (morningstar like) with all the informations about model?
>
> Regards,
>
> Marco
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From spencer.graves at prodsyse.com  Tue Jul 21 04:51:16 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Mon, 20 Jul 2009 20:51:16 -0600
Subject: [R-SIG-Finance] termstrc's bonds dataset creation
In-Reply-To: <BAY126-DS76F253D1CC2A61599CECED91F0@phx.gbl>
References: <BAY126-DS76F253D1CC2A61599CECED91F0@phx.gbl>
Message-ID: <4A652D24.2080805@prodsyse.com>

      I believe you can get an answer to that question by first Googling 
for "Robert Ferstl, Josef Hayden", the authors of the package.  The 
first match allows you to download a 23-page manuscript describing the 
package (http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1307149).  
This paper discusses "clean" vs. "dirty" prices.  From this, you should 
be able to construct a very simple special case that will answer your 
question.  I do not remember now the answer to your question, but I 
believe I did this a couple of months ago and got a clear answer. 


      Hope this helps. 
      Spencer Graves


Hsiao-nan Cheung wrote:
> Hi,
>
>  
>
>          I want to do a bonds analysis and now plan to create my own bonds
> set with package termstrc. But during construction, I got two problems.
>
> 1.       In the given datasets, there??s a domain called PRICE, and should I
> use bonds?? CLEAN price or DIRTY price here?
>
> 2.       In the list CF, the given bonds tend to be coupon bonds, and the
> coupon frequencies are once per year. Then how about zero-coupon (pure
> discounted) bonds? Or coupon frequencies larger then 1?
>
>  
>
> Thanks
>
> Hsiao-nan Cheung
>
> 2009/7/19
>
>
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From andyzhu35 at yahoo.com  Tue Jul 21 07:50:39 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Mon, 20 Jul 2009 22:50:39 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] COPULA package in R ~ need help
	on error message
Message-ID: <286670.66753.qm@web56208.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090720/ec3b8c85/attachment.pl>

From josh.m.ulrich at gmail.com  Tue Jul 21 16:27:04 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Tue, 21 Jul 2009 09:27:04 -0500
Subject: [R-SIG-Finance] Pattern recognition in timeseries data
In-Reply-To: <5e6a2e670907200513uc0d318dga31e77cd18a2a683@mail.gmail.com>
References: <5e6a2e670907200513uc0d318dga31e77cd18a2a683@mail.gmail.com>
Message-ID: <8cca69990907210727l79871e44ta0386ad91cb2c072@mail.gmail.com>

Hi Mark,

I'm not aware of anything like this in R.  I avoided these types of
indicators in TTR because their definitions tend to be subjective.

I wouldn't be opposed to including such indicators in TTR as long they
could be well-defined.  Here are a couple examples of well-defined
head and shoulders patterns:
http://ssrn.com/abstract=217470
http://ssrn.com/abstract=993938

Best,
Josh
--
http://www.fosstrading.com



On Mon, Jul 20, 2009 at 7:13 AM, Mark Breman<breman.mark at gmail.com> wrote:
> Hi,
> I have searched for R packages/functions for recognizing (technical) trading
> patterns in timeseries data, but have not found any so far.
>
> Technical trading patterns could be Head & Shoulders patterns, double tops,
> wedges etc.
>
> Is there really nothing out there?
>
> Kind regards,
>
> -Mark-
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From andyzhu35 at yahoo.com  Tue Jul 21 18:14:57 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Tue, 21 Jul 2009 09:14:57 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] COPULA package in R ~ need help
	on error message
Message-ID: <3811.98601.qm@web56206.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090721/27819fdf/attachment.pl>

From breman.mark at gmail.com  Tue Jul 21 20:03:47 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Tue, 21 Jul 2009 20:03:47 +0200
Subject: [R-SIG-Finance] Pattern recognition in timeseries data
In-Reply-To: <8cca69990907210727l79871e44ta0386ad91cb2c072@mail.gmail.com>
References: <5e6a2e670907200513uc0d318dga31e77cd18a2a683@mail.gmail.com>
	<8cca69990907210727l79871e44ta0386ad91cb2c072@mail.gmail.com>
Message-ID: <5e6a2e670907211103w615667a9r5ac0290309c66573@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090721/3ca28f02/attachment.pl>

From markleeds at verizon.net  Tue Jul 21 20:52:21 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 21 Jul 2009 13:52:21 -0500 (CDT)
Subject: [R-SIG-Finance] Pattern recognition in timeseries data
Message-ID: <603663933.2024763.1248202341251.JavaMail.root@vms170003.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090721/2095add4/attachment.html>

From breman.mark at gmail.com  Tue Jul 21 21:16:20 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Tue, 21 Jul 2009 21:16:20 +0200
Subject: [R-SIG-Finance] Pattern recognition in timeseries data
In-Reply-To: <603663933.2024763.1248202341251.JavaMail.root@vms170003.mailsrvcs.net>
References: <603663933.2024763.1248202341251.JavaMail.root@vms170003.mailsrvcs.net>
Message-ID: <5e6a2e670907211216g51d9b8e9m2a112d61d9445878@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090721/95895afd/attachment.pl>

From daniel.cegielka at gmail.com  Tue Jul 21 21:50:04 2009
From: daniel.cegielka at gmail.com (=?ISO-8859-2?Q?Daniel_Cegie=B3ka?=)
Date: Tue, 21 Jul 2009 21:50:04 +0200
Subject: [R-SIG-Finance] Pattern recognition in timeseries data
In-Reply-To: <5e6a2e670907211103w615667a9r5ac0290309c66573@mail.gmail.com>
References: <5e6a2e670907200513uc0d318dga31e77cd18a2a683@mail.gmail.com>
	<8cca69990907210727l79871e44ta0386ad91cb2c072@mail.gmail.com>
	<5e6a2e670907211103w615667a9r5ac0290309c66573@mail.gmail.com>
Message-ID: <5ddf2c610907211250x50edb23fl834384b410549567@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090721/1311fc03/attachment.pl>

From breman.mark at gmail.com  Tue Jul 21 22:25:08 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Tue, 21 Jul 2009 22:25:08 +0200
Subject: [R-SIG-Finance] Pattern recognition in timeseries data
In-Reply-To: <5ddf2c610907211250x50edb23fl834384b410549567@mail.gmail.com>
References: <5e6a2e670907200513uc0d318dga31e77cd18a2a683@mail.gmail.com>
	<8cca69990907210727l79871e44ta0386ad91cb2c072@mail.gmail.com>
	<5e6a2e670907211103w615667a9r5ac0290309c66573@mail.gmail.com>
	<5ddf2c610907211250x50edb23fl834384b410549567@mail.gmail.com>
Message-ID: <5e6a2e670907211325l7f1bb31s91a96a383d461878@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090721/635a955d/attachment.pl>

From cwrward at gmail.com  Tue Jul 21 22:39:51 2009
From: cwrward at gmail.com (Charles Ward)
Date: Tue, 21 Jul 2009 21:39:51 +0100
Subject: [R-SIG-Finance] Pattern recognition in timeseries data
In-Reply-To: <5e6a2e670907211216g51d9b8e9m2a112d61d9445878@mail.gmail.com>
References: <603663933.2024763.1248202341251.JavaMail.root@vms170003.mailsrvcs.net>
	<5e6a2e670907211216g51d9b8e9m2a112d61d9445878@mail.gmail.com>
Message-ID: <bd9aa36b0907211339g39c21871xf604e3ed3161aa0c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090721/71312a4b/attachment.pl>

From breman.mark at gmail.com  Wed Jul 22 08:40:37 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Wed, 22 Jul 2009 08:40:37 +0200
Subject: [R-SIG-Finance] Pattern recognition in timeseries data
In-Reply-To: <bd9aa36b0907211339g39c21871xf604e3ed3161aa0c@mail.gmail.com>
References: <603663933.2024763.1248202341251.JavaMail.root@vms170003.mailsrvcs.net>
	<5e6a2e670907211216g51d9b8e9m2a112d61d9445878@mail.gmail.com>
	<bd9aa36b0907211339g39c21871xf604e3ed3161aa0c@mail.gmail.com>
Message-ID: <5e6a2e670907212340w96eea71o3f8ec813a320cf34@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090722/4fa920af/attachment.pl>

From spencer.graves at prodsyse.com  Wed Jul 22 18:40:40 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Wed, 22 Jul 2009 10:40:40 -0600
Subject: [R-SIG-Finance] Fwd: Fwd: Inequality constraints in GMM
	estimation?
In-Reply-To: <d94e0b160907181631q4e3793d5r60e565f310477732@mail.gmail.com>
References: <2EC65577-0A86-4579-B179-C52EA0EC7167@gmail.com>
	<d94e0b160907181631q4e3793d5r60e565f310477732@mail.gmail.com>
Message-ID: <4A674108.2010908@prodsyse.com>

      I haven't used "gmm", but if it will pass 'method = "L-BFGS-B"' 
with "lower = 0", you might be able to transform your problem into that 
form as follows: 


      Let m(q) = m q, where m = an n x p matrix, and q = a p x 1 
vector.  If n = p, then let z = m q so q = solve(m, z), and parameterize 
your function "g" in terms of z. 


      If n < p, first compute the qr decomposition of t(m)


m <- matrix(1:2, 1)
mq <- t(qr.Q(qr(t(m)), complete=TRUE))


      Then let mc be an n x n matrix with the first n rows = m and the 
last (p-n) rows being the last (p-n) rows of mq.  Then let z = mc q and 
solve similar to the above. 


      If n > p, I suggest you start by solving the unconstrained 
problem, then find out which constraints are violated, add the 
constraints that seem to be violated the worst perhaps one at a time 
iteratively until you have a solution. 


      Or you could read the code for "gmm" and try to modify it to use 
"constrOptim". 


      Hope this helps. 
      Spencer Graves
     

David J. Moore, Ph.D. wrote:
> I need m(theta) > 0 where m() is a linear function of theta.  While the
> reparameterization is a creative workaround for a parameter constraint,I do
> not believe I can apply it here.  Other thoughts?
>
> David J. Moore, Ph.D.
> Visiting Assistant Professor of Finance
> The University of Memphis
>
>
> On Jul 18, 2009, at 2:04 PM, Liviu Andronic <landronimirc at gmail.com> wrote:
>
>  From: Eric Zivot <ezivot at u.washington.edu>
>   
>> Date: Sat, Jul 18, 2009 at 8:08 PM
>> Subject: Re: [R-SIG-Finance] Inequality constraints in GMM estimation?
>> To: Liviu Andronic <landronimirc at gmail.com>
>>
>>
>> Not sure your inequality constraint makes sense. Do you want theta > 0
>> or m(theta) > 0
>>
>> Usually, inequality constraints are put on parameters. If you want to
>> constrain theta > 0 then just re-parameterize theta as theta =
>> exp(gamma) and optimize over gamma. You will need to use delta method
>> to get the correct std errors for theta.hat = exp(gamma.hat).
>>
>> ****************************************************************
>> *  Eric Zivot                                                  *
>> *  Professor and Gary Waterman Distinguished Scholar           *
>> *  Department of Economics                                     *
>> *  Adjunct Professor of Finance                                *
>> *  Adjunct Professor of Statistics
>> *  Box 353330                  email:  ezivot at u.washington.edu *
>> *  University of Washington    phone:  206-543-6715            *
>> *  Seattle, WA 98195-3330                                      *
>>                                                   *
>> *  www:  http://faculty.washington.edu/ezivot                  *
>> ****************************************************************
>>
>> On Sat, 18 Jul 2009, Liviu Andronic wrote:
>>
>>  Hello,
>>     
>>> On Sat, Jul 18, 2009 at 4:58 AM, David J. Moore, Ph.D.<djmphd at gmail.com>
>>> wrote:
>>>
>>>       
>>>> I have a relatively simple finance application of GMM. Given the moment
>>>> condition:
>>>> E[m*R]=0
>>>>
>>>> where m=m[theta]
>>>>
>>>> I would like to constrain m>0. Any ideas?
>>>>
>>>>  Check library(gmm).
>>>>         
>>> Liviu
>>>
>>>
>>>       
>>
>>
>>     
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From ko.tseng at gmail.com  Thu Jul 23 09:37:52 2009
From: ko.tseng at gmail.com (Tseng Ko)
Date: Thu, 23 Jul 2009 15:37:52 +0800
Subject: [R-SIG-Finance] Index time change when coercing zoo object to xts
Message-ID: <e22594a60907230037x4cc7e99ei543ad82d629beb23@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090723/e9443289/attachment.pl>

From brian at braverock.com  Thu Jul 23 12:45:19 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 23 Jul 2009 05:45:19 -0500
Subject: [R-SIG-Finance] Index time change when coercing zoo object to
 xts
In-Reply-To: <e22594a60907230037x4cc7e99ei543ad82d629beb23@mail.gmail.com>
References: <e22594a60907230037x4cc7e99ei543ad82d629beb23@mail.gmail.com>
Message-ID: <4A683F3F.6010802@braverock.com>

Tseng Ko wrote:
> Dear all,
>
>   I tried to trasnform a zoo object to xts.
>
>   
>> x1
>>     
>                         C   V
> (07/23/09 15:13:00) 19815 238
> (07/23/09 15:14:00) 19805 291
> (07/23/09 15:15:00) 19816 298
> (07/23/09 15:16:00) 19820 357
> (07/23/09 15:17:00) 19813 218
> (07/23/09 15:18:00) 19789 203
>
>   
>> as.xts(x1)
>>     
>                         C   V
> (07/23/09 15:12:59) 19815 238
> (07/23/09 15:13:59) 19805 291
> (07/23/09 15:14:59) 19816 298
> (07/23/09 15:15:59) 19820 357
> (07/23/09 15:17:00) 19813 218
> (07/23/09 15:18:00) 19789 203
>
> Some data elements have 1 sec difference. Is it a bug in xts?
>   
You probably need to give the list more information to help you diagnose 
your problem.  Your test case is not self-contained and repeatable.

- What class is the index of your original object?
- What happens to the display if you set options(digits.secs=6)?
- What versions of R, xts, zoo are you using?
- Can you attach the R object that you use for this test case to your reply?

It seems mst likely to me that the index of your original object is 
something other than POXIXct, and that the representation misses when 
converted, but we won't know that for certain until you add some extra 
details and provide a test object so that others can test and repeat.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From ggrothendieck at gmail.com  Thu Jul 23 14:10:10 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Jul 2009 08:10:10 -0400
Subject: [R-SIG-Finance] Index time change when coercing zoo object to
	xts
In-Reply-To: <e22594a60907230037x4cc7e99ei543ad82d629beb23@mail.gmail.com>
References: <e22594a60907230037x4cc7e99ei543ad82d629beb23@mail.gmail.com>
Message-ID: <971536df0907230510l6c4d587m70c8c0973d58e794@mail.gmail.com>

As another responder mentioned we really need it in a reproducible
form; however, we can guess that the source of the problem is that
zoo represents all times in their original class, presumably chron
in this case, without any modification whereas xts represents all
times in POSIXct no matter what their original form was so as.xts
must map them all to POSIXct and in this conversion can introduce
small differences.

I can't reproduce this (see code below) but if you provide x1 in a
reproducible form then the above hypothesis can be double checked.

> Lines <- "07/23/09 15:13:00,19815,238
+ 07/23/09 15:14:00,19805,291
+ 07/23/09 15:15:00,19816,298
+ 07/23/09 15:16:00,19820,357
+ 07/23/09 15:17:00,19813,218
+ 07/23/09 15:18:00,19789,203"
>
> library(xts)
> library(chron)
> z <- read.zoo(textConnection(Lines), sep = ",", FUN = as.chron,
+ format = "%m/%d/%y %H:%M:%S")
> z
                       V2  V3
(07/23/09 15:13:00) 19815 238
(07/23/09 15:14:00) 19805 291
(07/23/09 15:15:00) 19816 298
(07/23/09 15:16:00) 19820 357
(07/23/09 15:17:00) 19813 218
(07/23/09 15:18:00) 19789 203
> as.xts(z)
                       V2  V3
(07/23/09 15:13:00) 19815 238
(07/23/09 15:14:00) 19805 291
(07/23/09 15:15:00) 19816 298
(07/23/09 15:16:00) 19820 357
(07/23/09 15:17:00) 19813 218
(07/23/09 15:18:00) 19789 203


On Thu, Jul 23, 2009 at 3:37 AM, Tseng Ko<ko.tseng at gmail.com> wrote:
> Dear all,
>
> ?I tried to trasnform a zoo object to xts.
>
>> x1
> ? ? ? ? ? ? ? ? ? ? ? ?C ? V
> (07/23/09 15:13:00) 19815 238
> (07/23/09 15:14:00) 19805 291
> (07/23/09 15:15:00) 19816 298
> (07/23/09 15:16:00) 19820 357
> (07/23/09 15:17:00) 19813 218
> (07/23/09 15:18:00) 19789 203
>
>> as.xts(x1)
> ? ? ? ? ? ? ? ? ? ? ? ?C ? V
> (07/23/09 15:12:59) 19815 238
> (07/23/09 15:13:59) 19805 291
> (07/23/09 15:14:59) 19816 298
> (07/23/09 15:15:59) 19820 357
> (07/23/09 15:17:00) 19813 218
> (07/23/09 15:18:00) 19789 203
>
> Some data elements have 1 sec difference. Is it a bug in xts?
>
> Thanks.
>
> --
> Ko Tseng
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Thu Jul 23 16:10:52 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 23 Jul 2009 09:10:52 -0500
Subject: [R-SIG-Finance] Index time change when coercing zoo object to
	xts
In-Reply-To: <971536df0907230510l6c4d587m70c8c0973d58e794@mail.gmail.com>
References: <e22594a60907230037x4cc7e99ei543ad82d629beb23@mail.gmail.com>
	<971536df0907230510l6c4d587m70c8c0973d58e794@mail.gmail.com>
Message-ID: <e8e755250907230710g3a706dfaqad9ac31282bd7b32@mail.gmail.com>

As the previous posters mentioned, without an example to work from we
can't really tell what the problem is.

Please post the versions of all software you are using, along with
reproducible code for us to try.

As Gabor pointed out, 'xts' converts user time classes internally to a
POSIXct representation.  This is done transparently and for internal
efficiency purposes, as well as to offer identical behavior regardless
of user preference.  Somewhere in the conversion it is possible that
we are losing precision from 'chron' to 'POSIXct'.

That said, I can't replicate your issue, so it may be fixed in recent releases.

HTH,
Jeff

On Thu, Jul 23, 2009 at 7:10 AM, Gabor
Grothendieck<ggrothendieck at gmail.com> wrote:
> As another responder mentioned we really need it in a reproducible
> form; however, we can guess that the source of the problem is that
> zoo represents all times in their original class, presumably chron
> in this case, without any modification whereas xts represents all
> times in POSIXct no matter what their original form was so as.xts
> must map them all to POSIXct and in this conversion can introduce
> small differences.
>
> I can't reproduce this (see code below) but if you provide x1 in a
> reproducible form then the above hypothesis can be double checked.
>
>> Lines <- "07/23/09 15:13:00,19815,238
> + 07/23/09 15:14:00,19805,291
> + 07/23/09 15:15:00,19816,298
> + 07/23/09 15:16:00,19820,357
> + 07/23/09 15:17:00,19813,218
> + 07/23/09 15:18:00,19789,203"
>>
>> library(xts)
>> library(chron)
>> z <- read.zoo(textConnection(Lines), sep = ",", FUN = as.chron,
> + format = "%m/%d/%y %H:%M:%S")
>> z
> ? ? ? ? ? ? ? ? ? ? ? V2 ?V3
> (07/23/09 15:13:00) 19815 238
> (07/23/09 15:14:00) 19805 291
> (07/23/09 15:15:00) 19816 298
> (07/23/09 15:16:00) 19820 357
> (07/23/09 15:17:00) 19813 218
> (07/23/09 15:18:00) 19789 203
>> as.xts(z)
> ? ? ? ? ? ? ? ? ? ? ? V2 ?V3
> (07/23/09 15:13:00) 19815 238
> (07/23/09 15:14:00) 19805 291
> (07/23/09 15:15:00) 19816 298
> (07/23/09 15:16:00) 19820 357
> (07/23/09 15:17:00) 19813 218
> (07/23/09 15:18:00) 19789 203
>
>
> On Thu, Jul 23, 2009 at 3:37 AM, Tseng Ko<ko.tseng at gmail.com> wrote:
>> Dear all,
>>
>> ?I tried to trasnform a zoo object to xts.
>>
>>> x1
>> ? ? ? ? ? ? ? ? ? ? ? ?C ? V
>> (07/23/09 15:13:00) 19815 238
>> (07/23/09 15:14:00) 19805 291
>> (07/23/09 15:15:00) 19816 298
>> (07/23/09 15:16:00) 19820 357
>> (07/23/09 15:17:00) 19813 218
>> (07/23/09 15:18:00) 19789 203
>>
>>> as.xts(x1)
>> ? ? ? ? ? ? ? ? ? ? ? ?C ? V
>> (07/23/09 15:12:59) 19815 238
>> (07/23/09 15:13:59) 19805 291
>> (07/23/09 15:14:59) 19816 298
>> (07/23/09 15:15:59) 19820 357
>> (07/23/09 15:17:00) 19813 218
>> (07/23/09 15:18:00) 19789 203
>>
>> Some data elements have 1 sec difference. Is it a bug in xts?
>>
>> Thanks.
>>
>> --
>> Ko Tseng
>>
>> ? ? ? ?[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From james.klingsporn at gmail.com  Thu Jul 23 17:38:14 2009
From: james.klingsporn at gmail.com (James Klingsporn)
Date: Thu, 23 Jul 2009 11:38:14 -0400
Subject: [R-SIG-Finance] quantmod Charting
Message-ID: <771cf9480907230838y3febfd24xd4e1d665ef588fd7@mail.gmail.com>

Hello,

I am using quantmod to create charts that have buy/sell date times
with a window 40 minutes before and after the buy/sell date time.  I
want to create ps charts and ultimately pdf charts of the trades.  I'm
not concerned with the exit right now.  The number of buy/sell date
times can range between 150 ? 3000+, so I'd clearly like to automate
the process.  The code I?ve been using is attached below.  I've tried
putting the below into a function so I call the function with the
trade number, but this does not produce the full chart (and a similar
problem with 'if' and thus the multipe 'if's below for the same
condition).  If I run the below as Source R code it also doesn't
produce the full chart.  Also, because the code is so long for each
chart, I can't just run the code for all the trades at 1 time; I have
to cut and paste the script for about 75 trades/time.  The code has
run time of about 5-6 trade charts/min which clearly takes a while for
the 3000+ trades and then considering I have also to cut and paste the
code.

Is there any way to speed up or otherwise improve the script/charting?

Thanks,
Jim


R code for charts -

 require(quantmod)

	file = 'C:/Consulting/chart.txt'
	chart_data <- read.zoo(file, header = FALSE, col.names = c("Date",
"Open", "High", "Low", "Close", "MACD1min", "MACD5min", "MACD10min",
"Hist1min", "Hist5min", "Hist10min"), sep = ",", dec = ".", tz = "",
format = "%Y-%m-%d %H:%M:%S")
    ES <- cbind(chart_data$Open, chart_data$High, chart_data$Low,
chart_data$Close)
	ES <- as.quantmod.OHLC(ES, col.names = c("Open", "High", "Low", "Close"))

    time_before = 40
    time_after = 40
    time_before <- time_before * 60
    time_after <- time_after * 60

    enter_times_path = 'C:/Consulting/ES_trades_with_slope_rule.txt'
    enter_times <- read.zoo(enter_times_path, header = FALSE,
col.names = c("Date", "Position"), sep = ",", dec = ".", tz = "",
format = "%Y-%m-%d %H:%M:%S")
	
	ctheme <- chartTheme(theme = 'white', area = 'white', up.col =
'#0C481A', dn.col = '#B12525')

i = 1
# Trade number repeats for number of trades

display_range <- window(ES, start = time(enter_times[i]) - time_before
, end = time(enter_times[i]) + time_after)
m <- window(chart_data, start = time(enter_times[i]) - time_before ,
end = time(enter_times[i]) + time_after)
MACD <- cbind(m$MACD1min, m$MACD5min, m$MACD10min)

chartSeries(display_range, theme = ctheme)

Hist_range_5 = c(min(range(m$Hist5min)[1], - 0.2),
max(range(m$Hist5min)[2], 0.2))
Hist_range_10 = c(min(range(m$Hist10min)[1], - 0.2),
max(range(m$Hist10min)[2], 0.2))

h_steep_slope_1min <- m[abs(diff(m$Hist1min)) >= 0.1]
h_slope_1min <- m[abs(diff(m$Hist1min)) >= 0.05 & abs(diff(m$Hist1min)) < 0.1]
h_no_slope_1min <- m[abs(diff(m$Hist1min)) < 0.05]
h_slope_macd_above_1min <- h_slope_1min[abs(h_slope_1min$Hist1min) > 0.2,]
h_slope_macd_below_1min <- h_slope_1min[abs(h_slope_1min$Hist1min) <= 0.2,]
h_no_slope_above_1min <- h_no_slope_1min[abs(h_no_slope_1min$Hist1min) > 0.2,]
h_no_slope_below_1min <- h_no_slope_1min[abs(h_no_slope_1min$Hist1min) <= 0.2,]
h_steep_slope_above_1min <-
h_steep_slope_1min[abs(h_steep_slope_1min$Hist1min) > 0.2,]
h_steep_slope_below_1min <-
h_steep_slope_1min[abs(h_steep_slope_1min$Hist1min) <= 0.2,]

a = h_slope_macd_below_1min$Hist1min
b = h_no_slope_below_1min$Hist1min
f =	h_steep_slope_below_1min$Hist1min

c = h_steep_slope_above_1min$Hist1min
d = h_no_slope_above_1min$Hist1min
e = h_slope_macd_above_1min$Hist1min

zero_times = time(m)
zero = zoo(0, order.by = zero_times)

addTA(a, on = NA, col = '#C03E15' , type = 'h', lwd = 3, yrange =
range(m$Hist1min))
addTA(b, on = 2, col = '#C03E15', type = 'h', lwd = 1)
addTA(f, on = 2, col = '#C03E15', type = 'h', lwd = 6)
addTA(c, on = 2, col = '#060440', type = 'h', lwd = 3)
addTA(d, on = 2, col = '#060440', type = 'h', lwd = 1)
addTA(e, on = 2, col = '#060440', type = 'h', lwd = 6)
addTA(zero, on = 2, col = 'black', type = 'l', lwd = 1)

h_steep_slope_5min <- m[abs(diff(m$Hist5min)) >= 0.1]
h_slope_5min <- m[abs(diff(m$Hist5min)) >= 0.05 & abs(diff(m$Hist5min)) < 0.1]
h_no_slope_5min <- m[abs(diff(m$Hist5min)) < 0.05]

h_slope_macd_above_5min <- h_slope_5min[abs(h_slope_5min$Hist5min) > 0.7,]
h_slope_macd_below_5min <- h_slope_5min[abs(h_slope_5min$Hist5min) <= 0.7,]

h_no_slope_above_5min <- h_no_slope_5min[abs(h_no_slope_5min$Hist5min) > 0.7,]
h_no_slope_below_5min <- h_no_slope_5min[abs(h_no_slope_5min$Hist5min) <= 0.7,]

h_steep_slope_above_5min <-
h_steep_slope_5min[abs(h_steep_slope_5min$Hist5min) > 0.7,]
h_steep_slope_below_5min <-
h_steep_slope_5min[abs(h_steep_slope_5min$Hist5min) <= 0.7,]

a5 = h_slope_macd_below_5min$Hist5min
b5 = h_no_slope_below_5min$Hist5min
f5 = h_steep_slope_below_5min$Hist5min

c5 = h_steep_slope_above_5min$Hist5min
d5 = h_no_slope_above_5min$Hist5min
e5 = h_slope_macd_above_5min$Hist5min

addTA(a5, on = NA, col = '#C03E15' , type = 'h', lwd = 3, yrange =
range(Hist_range_5))
addTA(b5, on = 3, col = '#C03E15', type = 'h', lwd = 1)
addTA(f5, on = 3, col = '#C03E15', type = 'h', lwd = 6)
addTA(c5, on = 3, col = '#060440', type = 'h', lwd = 3)
addTA(d5, on = 3, col = '#060440', type = 'h', lwd = 1)
addTA(e5, on = 3, col = '#060440', type = 'h', lwd = 6)
addTA(zero, on = 3, col = 'black', type = 'l', lwd = 1)

h_steep_slope_10min <- m[abs(diff(m$Hist10min)) >= 0.1]
h_slope_10min <- m[abs(diff(m$Hist10min)) >= 0.05 &
abs(diff(m$Hist10min)) < 0.1]
h_no_slope_10min <- m[abs(diff(m$Hist10min)) < 0.05]

h_slope_macd_above_10min <- h_slope_10min[abs(h_slope_10min$Hist10min) > 0.7,]
h_slope_macd_below_10min <- h_slope_10min[abs(h_slope_10min$Hist10min) <= 0.7,]

h_no_slope_above_10min <-
h_no_slope_10min[abs(h_no_slope_10min$Hist10min) > 0.7,]
h_no_slope_below_10min <-
h_no_slope_10min[abs(h_no_slope_10min$Hist10min) <= 0.7,]

h_steep_slope_above_10min <-
h_steep_slope_10min[abs(h_steep_slope_10min$Hist10min) > 0.7,]
h_steep_slope_below_10min <-
h_steep_slope_10min[abs(h_steep_slope_10min$Hist10min) <= 0.7,]

a10 = h_slope_macd_below_10min$Hist10min
b10 = h_no_slope_below_10min$Hist10min
f10 = h_steep_slope_below_10min$Hist10min

c10 = h_steep_slope_above_10min$Hist10min
d10 = h_no_slope_above_10min$Hist10min
e10 = h_slope_macd_above_10min$Hist10min

addTA(d10, on = NA, col = '#060440', type = 'h', lwd = 1)
addTA(a10, on = 4, col = '#C03E15' , type = 'h', lwd = 3, yrange =
range(Hist_range_10))
addTA(b10, on = 4, col = '#C03E15', type = 'h', lwd = 1)
addTA(f10, on = 4, col = '#C03E15', type = 'h', lwd = 6)
addTA(c10, on = 4, col = '#060440', type = 'h', lwd = 3)

addTA(e10, on = 4, col = '#060440', type = 'h', lwd = 6)
addTA(zero, on = 4, col = 'black', type = 'l', lwd = 1)

addTA(MACD, on = NA, col = c('blue', 'red', 'green'), type = c('l',
'l', 'l'), lwd = c(4, 2, 1))
addTA(zero, on = 5, col = 'black', type = 'l', lwd = 1)

if (as.character(enter_times[i]) == "Long")	Buy <- ES[time(ES) ==
time(enter_times[i]) + 60]$ES.Open
if (as.character(enter_times[i]) == "Long")	addTA(Buy, on = 1, col =
'red', type = 'p', pch = 6, lwd = 2)
if (as.character(enter_times[i]) == "Long")	addTA(Buy > 0, on =
-(1:5), col = '#FFCAFD', border = NA)

if (as.character(enter_times[i]) == "Short")	Short <- ES[time(ES) ==
time(enter_times[i]) + 60]$ES.Open
if (as.character(enter_times[i]) == "Short")    addTA(Short, on = 1,
col = 'blue', type = 'p', pch = 2, lwd = 2)
if (as.character(enter_times[i]) == "Short")	addTA(Short > 0, on =
-(1:5), col = '#FF7878', border = NA)

# Repeats with i = 2...


From brian at braverock.com  Thu Jul 23 17:53:42 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 23 Jul 2009 10:53:42 -0500
Subject: [R-SIG-Finance] quantmod Charting
In-Reply-To: <771cf9480907230838y3febfd24xd4e1d665ef588fd7@mail.gmail.com>
References: <771cf9480907230838y3febfd24xd4e1d665ef588fd7@mail.gmail.com>
Message-ID: <4A688786.90505@braverock.com>

You haven't sent us a data series that we can reproduce your problem 
with.  Maybe grab one from an instrument that you don't trade, or 
simulate a series that has some of the same properties that you use to 
trade.

I typically have no problem using chartSeries on a day of tick data 
(millions of points) or 15-sec OHLC bars (hundreds of thousands of 
points) beyond the time it takes to actually display/render the chart.  
This includes calling addTA to add extra things to the charts in question.

One thing that comes immediately to mind is that I would probably 
separate the process of creating the indicators from the process of 
charting.  Run through your data series and generate your indicators, 
then again to generate your signals, and *then* chart it from the 
aligned time series.

There have been multiple previous posts to this list regarding calls to 
chartSeries and addTA from within custom functions, perhaps a review of 
the list archives is in order as well.

Regards,

  - Brian

James Klingsporn wrote:
> Hello,
>
> I am using quantmod to create charts that have buy/sell date times
> with a window 40 minutes before and after the buy/sell date time.  I
> want to create ps charts and ultimately pdf charts of the trades.  I'm
> not concerned with the exit right now.  The number of buy/sell date
> times can range between 150 ? 3000+, so I'd clearly like to automate
> the process.  The code I?ve been using is attached below.  I've tried
> putting the below into a function so I call the function with the
> trade number, but this does not produce the full chart (and a similar
> problem with 'if' and thus the multipe 'if's below for the same
> condition).  If I run the below as Source R code it also doesn't
> produce the full chart.  Also, because the code is so long for each
> chart, I can't just run the code for all the trades at 1 time; I have
> to cut and paste the script for about 75 trades/time.  The code has
> run time of about 5-6 trade charts/min which clearly takes a while for
> the 3000+ trades and then considering I have also to cut and paste the
> code.
>
> Is there any way to speed up or otherwise improve the script/charting?
>
> Thanks,
> Jim
>
>
> R code for charts -
>
>  require(quantmod)
>
> 	file = 'C:/Consulting/chart.txt'
> 	chart_data <- read.zoo(file, header = FALSE, col.names = c("Date",
> "Open", "High", "Low", "Close", "MACD1min", "MACD5min", "MACD10min",
> "Hist1min", "Hist5min", "Hist10min"), sep = ",", dec = ".", tz = "",
> format = "%Y-%m-%d %H:%M:%S")
>     ES <- cbind(chart_data$Open, chart_data$High, chart_data$Low,
> chart_data$Close)
> 	ES <- as.quantmod.OHLC(ES, col.names = c("Open", "High", "Low", "Close"))
>
>     time_before = 40
>     time_after = 40
>     time_before <- time_before * 60
>     time_after <- time_after * 60
>
>     enter_times_path = 'C:/Consulting/ES_trades_with_slope_rule.txt'
>     enter_times <- read.zoo(enter_times_path, header = FALSE,
> col.names = c("Date", "Position"), sep = ",", dec = ".", tz = "",
> format = "%Y-%m-%d %H:%M:%S")
> 	
> 	ctheme <- chartTheme(theme = 'white', area = 'white', up.col =
> '#0C481A', dn.col = '#B12525')
>
> i = 1
> # Trade number repeats for number of trades
>
> display_range <- window(ES, start = time(enter_times[i]) - time_before
> , end = time(enter_times[i]) + time_after)
> m <- window(chart_data, start = time(enter_times[i]) - time_before ,
> end = time(enter_times[i]) + time_after)
> MACD <- cbind(m$MACD1min, m$MACD5min, m$MACD10min)
>
> chartSeries(display_range, theme = ctheme)
>
> Hist_range_5 = c(min(range(m$Hist5min)[1], - 0.2),
> max(range(m$Hist5min)[2], 0.2))
> Hist_range_10 = c(min(range(m$Hist10min)[1], - 0.2),
> max(range(m$Hist10min)[2], 0.2))
>
> h_steep_slope_1min <- m[abs(diff(m$Hist1min)) >= 0.1]
> h_slope_1min <- m[abs(diff(m$Hist1min)) >= 0.05 & abs(diff(m$Hist1min)) < 0.1]
> h_no_slope_1min <- m[abs(diff(m$Hist1min)) < 0.05]
> h_slope_macd_above_1min <- h_slope_1min[abs(h_slope_1min$Hist1min) > 0.2,]
> h_slope_macd_below_1min <- h_slope_1min[abs(h_slope_1min$Hist1min) <= 0.2,]
> h_no_slope_above_1min <- h_no_slope_1min[abs(h_no_slope_1min$Hist1min) > 0.2,]
> h_no_slope_below_1min <- h_no_slope_1min[abs(h_no_slope_1min$Hist1min) <= 0.2,]
> h_steep_slope_above_1min <-
> h_steep_slope_1min[abs(h_steep_slope_1min$Hist1min) > 0.2,]
> h_steep_slope_below_1min <-
> h_steep_slope_1min[abs(h_steep_slope_1min$Hist1min) <= 0.2,]
>
> a = h_slope_macd_below_1min$Hist1min
> b = h_no_slope_below_1min$Hist1min
> f =	h_steep_slope_below_1min$Hist1min
>
> c = h_steep_slope_above_1min$Hist1min
> d = h_no_slope_above_1min$Hist1min
> e = h_slope_macd_above_1min$Hist1min
>
> zero_times = time(m)
> zero = zoo(0, order.by = zero_times)
>
> addTA(a, on = NA, col = '#C03E15' , type = 'h', lwd = 3, yrange =
> range(m$Hist1min))
> addTA(b, on = 2, col = '#C03E15', type = 'h', lwd = 1)
> addTA(f, on = 2, col = '#C03E15', type = 'h', lwd = 6)
> addTA(c, on = 2, col = '#060440', type = 'h', lwd = 3)
> addTA(d, on = 2, col = '#060440', type = 'h', lwd = 1)
> addTA(e, on = 2, col = '#060440', type = 'h', lwd = 6)
> addTA(zero, on = 2, col = 'black', type = 'l', lwd = 1)
>
> h_steep_slope_5min <- m[abs(diff(m$Hist5min)) >= 0.1]
> h_slope_5min <- m[abs(diff(m$Hist5min)) >= 0.05 & abs(diff(m$Hist5min)) < 0.1]
> h_no_slope_5min <- m[abs(diff(m$Hist5min)) < 0.05]
>
> h_slope_macd_above_5min <- h_slope_5min[abs(h_slope_5min$Hist5min) > 0.7,]
> h_slope_macd_below_5min <- h_slope_5min[abs(h_slope_5min$Hist5min) <= 0.7,]
>
> h_no_slope_above_5min <- h_no_slope_5min[abs(h_no_slope_5min$Hist5min) > 0.7,]
> h_no_slope_below_5min <- h_no_slope_5min[abs(h_no_slope_5min$Hist5min) <= 0.7,]
>
> h_steep_slope_above_5min <-
> h_steep_slope_5min[abs(h_steep_slope_5min$Hist5min) > 0.7,]
> h_steep_slope_below_5min <-
> h_steep_slope_5min[abs(h_steep_slope_5min$Hist5min) <= 0.7,]
>
> a5 = h_slope_macd_below_5min$Hist5min
> b5 = h_no_slope_below_5min$Hist5min
> f5 = h_steep_slope_below_5min$Hist5min
>
> c5 = h_steep_slope_above_5min$Hist5min
> d5 = h_no_slope_above_5min$Hist5min
> e5 = h_slope_macd_above_5min$Hist5min
>
> addTA(a5, on = NA, col = '#C03E15' , type = 'h', lwd = 3, yrange =
> range(Hist_range_5))
> addTA(b5, on = 3, col = '#C03E15', type = 'h', lwd = 1)
> addTA(f5, on = 3, col = '#C03E15', type = 'h', lwd = 6)
> addTA(c5, on = 3, col = '#060440', type = 'h', lwd = 3)
> addTA(d5, on = 3, col = '#060440', type = 'h', lwd = 1)
> addTA(e5, on = 3, col = '#060440', type = 'h', lwd = 6)
> addTA(zero, on = 3, col = 'black', type = 'l', lwd = 1)
>
> h_steep_slope_10min <- m[abs(diff(m$Hist10min)) >= 0.1]
> h_slope_10min <- m[abs(diff(m$Hist10min)) >= 0.05 &
> abs(diff(m$Hist10min)) < 0.1]
> h_no_slope_10min <- m[abs(diff(m$Hist10min)) < 0.05]
>
> h_slope_macd_above_10min <- h_slope_10min[abs(h_slope_10min$Hist10min) > 0.7,]
> h_slope_macd_below_10min <- h_slope_10min[abs(h_slope_10min$Hist10min) <= 0.7,]
>
> h_no_slope_above_10min <-
> h_no_slope_10min[abs(h_no_slope_10min$Hist10min) > 0.7,]
> h_no_slope_below_10min <-
> h_no_slope_10min[abs(h_no_slope_10min$Hist10min) <= 0.7,]
>
> h_steep_slope_above_10min <-
> h_steep_slope_10min[abs(h_steep_slope_10min$Hist10min) > 0.7,]
> h_steep_slope_below_10min <-
> h_steep_slope_10min[abs(h_steep_slope_10min$Hist10min) <= 0.7,]
>
> a10 = h_slope_macd_below_10min$Hist10min
> b10 = h_no_slope_below_10min$Hist10min
> f10 = h_steep_slope_below_10min$Hist10min
>
> c10 = h_steep_slope_above_10min$Hist10min
> d10 = h_no_slope_above_10min$Hist10min
> e10 = h_slope_macd_above_10min$Hist10min
>
> addTA(d10, on = NA, col = '#060440', type = 'h', lwd = 1)
> addTA(a10, on = 4, col = '#C03E15' , type = 'h', lwd = 3, yrange =
> range(Hist_range_10))
> addTA(b10, on = 4, col = '#C03E15', type = 'h', lwd = 1)
> addTA(f10, on = 4, col = '#C03E15', type = 'h', lwd = 6)
> addTA(c10, on = 4, col = '#060440', type = 'h', lwd = 3)
>
> addTA(e10, on = 4, col = '#060440', type = 'h', lwd = 6)
> addTA(zero, on = 4, col = 'black', type = 'l', lwd = 1)
>
> addTA(MACD, on = NA, col = c('blue', 'red', 'green'), type = c('l',
> 'l', 'l'), lwd = c(4, 2, 1))
> addTA(zero, on = 5, col = 'black', type = 'l', lwd = 1)
>
> if (as.character(enter_times[i]) == "Long")	Buy <- ES[time(ES) ==
> time(enter_times[i]) + 60]$ES.Open
> if (as.character(enter_times[i]) == "Long")	addTA(Buy, on = 1, col =
> 'red', type = 'p', pch = 6, lwd = 2)
> if (as.character(enter_times[i]) == "Long")	addTA(Buy > 0, on =
> -(1:5), col = '#FFCAFD', border = NA)
>
> if (as.character(enter_times[i]) == "Short")	Short <- ES[time(ES) ==
> time(enter_times[i]) + 60]$ES.Open
> if (as.character(enter_times[i]) == "Short")    addTA(Short, on = 1,
> col = 'blue', type = 'p', pch = 2, lwd = 2)
> if (as.character(enter_times[i]) == "Short")	addTA(Short > 0, on =
> -(1:5), col = '#FF7878', border = NA)
>
> # Repeats with i = 2...
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>   


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From james.klingsporn at gmail.com  Thu Jul 23 18:34:28 2009
From: james.klingsporn at gmail.com (James Klingsporn)
Date: Thu, 23 Jul 2009 12:34:28 -0400
Subject: [R-SIG-Finance] quantmod Charting
In-Reply-To: <4A688786.90505@braverock.com>
References: <771cf9480907230838y3febfd24xd4e1d665ef588fd7@mail.gmail.com>
	<4A688786.90505@braverock.com>
Message-ID: <771cf9480907230934h4baaaa12r31373828cf8ead13@mail.gmail.com>

Thanks for the help.  The chartSeries and addTA helped.  Maybe it
wasn't clear from my first email but the indicators are already
calculated, it is just a matter of loading the data from a file.  I'll
take a look at TTR again;  For the add* function, such as MACD, the TA
is calculated from the begining of the chart data, correct?  This
would then be different from what I'm calculating, simply because the
TA continues from uncharted data.

On Thu, Jul 23, 2009 at 11:53 AM, Brian G. Peterson<brian at braverock.com> wrote:
> You haven't sent us a data series that we can reproduce your problem with.
> ?Maybe grab one from an instrument that you don't trade, or simulate a
> series that has some of the same properties that you use to trade.
>
> I typically have no problem using chartSeries on a day of tick data
> (millions of points) or 15-sec OHLC bars (hundreds of thousands of points)
> beyond the time it takes to actually display/render the chart. ?This
> includes calling addTA to add extra things to the charts in question.
>
> One thing that comes immediately to mind is that I would probably separate
> the process of creating the indicators from the process of charting. ?Run
> through your data series and generate your indicators, then again to
> generate your signals, and *then* chart it from the aligned time series.
>
> There have been multiple previous posts to this list regarding calls to
> chartSeries and addTA from within custom functions, perhaps a review of the
> list archives is in order as well.
>
> Regards,
>
> ?- Brian
>
> James Klingsporn wrote:
>>
>> Hello,
>>
>> I am using quantmod to create charts that have buy/sell date times
>> with a window 40 minutes before and after the buy/sell date time. ?I
>> want to create ps charts and ultimately pdf charts of the trades. ?I'm
>> not concerned with the exit right now. ?The number of buy/sell date
>> times can range between 150 ? 3000+, so I'd clearly like to automate
>> the process. ?The code I?ve been using is attached below. ?I've tried
>> putting the below into a function so I call the function with the
>> trade number, but this does not produce the full chart (and a similar
>> problem with 'if' and thus the multipe 'if's below for the same
>> condition). ?If I run the below as Source R code it also doesn't
>> produce the full chart. ?Also, because the code is so long for each
>> chart, I can't just run the code for all the trades at 1 time; I have
>> to cut and paste the script for about 75 trades/time. ?The code has
>> run time of about 5-6 trade charts/min which clearly takes a while for
>> the 3000+ trades and then considering I have also to cut and paste the
>> code.
>>
>> Is there any way to speed up or otherwise improve the script/charting?
>>
>> Thanks,
>> Jim
>>
>>
>> R code for charts -
>>
>> ?require(quantmod)
>>
>> ? ? ? ?file = 'C:/Consulting/chart.txt'
>> ? ? ? ?chart_data <- read.zoo(file, header = FALSE, col.names = c("Date",
>> "Open", "High", "Low", "Close", "MACD1min", "MACD5min", "MACD10min",
>> "Hist1min", "Hist5min", "Hist10min"), sep = ",", dec = ".", tz = "",
>> format = "%Y-%m-%d %H:%M:%S")
>> ? ?ES <- cbind(chart_data$Open, chart_data$High, chart_data$Low,
>> chart_data$Close)
>> ? ? ? ?ES <- as.quantmod.OHLC(ES, col.names = c("Open", "High", "Low",
>> "Close"))
>>
>> ? ?time_before = 40
>> ? ?time_after = 40
>> ? ?time_before <- time_before * 60
>> ? ?time_after <- time_after * 60
>>
>> ? ?enter_times_path = 'C:/Consulting/ES_trades_with_slope_rule.txt'
>> ? ?enter_times <- read.zoo(enter_times_path, header = FALSE,
>> col.names = c("Date", "Position"), sep = ",", dec = ".", tz = "",
>> format = "%Y-%m-%d %H:%M:%S")
>>
>> ? ? ? ?ctheme <- chartTheme(theme = 'white', area = 'white', up.col =
>> '#0C481A', dn.col = '#B12525')
>>
>> i = 1
>> # Trade number repeats for number of trades
>>
>> display_range <- window(ES, start = time(enter_times[i]) - time_before
>> , end = time(enter_times[i]) + time_after)
>> m <- window(chart_data, start = time(enter_times[i]) - time_before ,
>> end = time(enter_times[i]) + time_after)
>> MACD <- cbind(m$MACD1min, m$MACD5min, m$MACD10min)
>>
>> chartSeries(display_range, theme = ctheme)
>>
>> Hist_range_5 = c(min(range(m$Hist5min)[1], - 0.2),
>> max(range(m$Hist5min)[2], 0.2))
>> Hist_range_10 = c(min(range(m$Hist10min)[1], - 0.2),
>> max(range(m$Hist10min)[2], 0.2))
>>
>> h_steep_slope_1min <- m[abs(diff(m$Hist1min)) >= 0.1]
>> h_slope_1min <- m[abs(diff(m$Hist1min)) >= 0.05 & abs(diff(m$Hist1min)) <
>> 0.1]
>> h_no_slope_1min <- m[abs(diff(m$Hist1min)) < 0.05]
>> h_slope_macd_above_1min <- h_slope_1min[abs(h_slope_1min$Hist1min) > 0.2,]
>> h_slope_macd_below_1min <- h_slope_1min[abs(h_slope_1min$Hist1min) <=
>> 0.2,]
>> h_no_slope_above_1min <- h_no_slope_1min[abs(h_no_slope_1min$Hist1min) >
>> 0.2,]
>> h_no_slope_below_1min <- h_no_slope_1min[abs(h_no_slope_1min$Hist1min) <=
>> 0.2,]
>> h_steep_slope_above_1min <-
>> h_steep_slope_1min[abs(h_steep_slope_1min$Hist1min) > 0.2,]
>> h_steep_slope_below_1min <-
>> h_steep_slope_1min[abs(h_steep_slope_1min$Hist1min) <= 0.2,]
>>
>> a = h_slope_macd_below_1min$Hist1min
>> b = h_no_slope_below_1min$Hist1min
>> f = ? ? h_steep_slope_below_1min$Hist1min
>>
>> c = h_steep_slope_above_1min$Hist1min
>> d = h_no_slope_above_1min$Hist1min
>> e = h_slope_macd_above_1min$Hist1min
>>
>> zero_times = time(m)
>> zero = zoo(0, order.by = zero_times)
>>
>> addTA(a, on = NA, col = '#C03E15' , type = 'h', lwd = 3, yrange =
>> range(m$Hist1min))
>> addTA(b, on = 2, col = '#C03E15', type = 'h', lwd = 1)
>> addTA(f, on = 2, col = '#C03E15', type = 'h', lwd = 6)
>> addTA(c, on = 2, col = '#060440', type = 'h', lwd = 3)
>> addTA(d, on = 2, col = '#060440', type = 'h', lwd = 1)
>> addTA(e, on = 2, col = '#060440', type = 'h', lwd = 6)
>> addTA(zero, on = 2, col = 'black', type = 'l', lwd = 1)
>>
>> h_steep_slope_5min <- m[abs(diff(m$Hist5min)) >= 0.1]
>> h_slope_5min <- m[abs(diff(m$Hist5min)) >= 0.05 & abs(diff(m$Hist5min)) <
>> 0.1]
>> h_no_slope_5min <- m[abs(diff(m$Hist5min)) < 0.05]
>>
>> h_slope_macd_above_5min <- h_slope_5min[abs(h_slope_5min$Hist5min) > 0.7,]
>> h_slope_macd_below_5min <- h_slope_5min[abs(h_slope_5min$Hist5min) <=
>> 0.7,]
>>
>> h_no_slope_above_5min <- h_no_slope_5min[abs(h_no_slope_5min$Hist5min) >
>> 0.7,]
>> h_no_slope_below_5min <- h_no_slope_5min[abs(h_no_slope_5min$Hist5min) <=
>> 0.7,]
>>
>> h_steep_slope_above_5min <-
>> h_steep_slope_5min[abs(h_steep_slope_5min$Hist5min) > 0.7,]
>> h_steep_slope_below_5min <-
>> h_steep_slope_5min[abs(h_steep_slope_5min$Hist5min) <= 0.7,]
>>
>> a5 = h_slope_macd_below_5min$Hist5min
>> b5 = h_no_slope_below_5min$Hist5min
>> f5 = h_steep_slope_below_5min$Hist5min
>>
>> c5 = h_steep_slope_above_5min$Hist5min
>> d5 = h_no_slope_above_5min$Hist5min
>> e5 = h_slope_macd_above_5min$Hist5min
>>
>> addTA(a5, on = NA, col = '#C03E15' , type = 'h', lwd = 3, yrange =
>> range(Hist_range_5))
>> addTA(b5, on = 3, col = '#C03E15', type = 'h', lwd = 1)
>> addTA(f5, on = 3, col = '#C03E15', type = 'h', lwd = 6)
>> addTA(c5, on = 3, col = '#060440', type = 'h', lwd = 3)
>> addTA(d5, on = 3, col = '#060440', type = 'h', lwd = 1)
>> addTA(e5, on = 3, col = '#060440', type = 'h', lwd = 6)
>> addTA(zero, on = 3, col = 'black', type = 'l', lwd = 1)
>>
>> h_steep_slope_10min <- m[abs(diff(m$Hist10min)) >= 0.1]
>> h_slope_10min <- m[abs(diff(m$Hist10min)) >= 0.05 &
>> abs(diff(m$Hist10min)) < 0.1]
>> h_no_slope_10min <- m[abs(diff(m$Hist10min)) < 0.05]
>>
>> h_slope_macd_above_10min <- h_slope_10min[abs(h_slope_10min$Hist10min) >
>> 0.7,]
>> h_slope_macd_below_10min <- h_slope_10min[abs(h_slope_10min$Hist10min) <=
>> 0.7,]
>>
>> h_no_slope_above_10min <-
>> h_no_slope_10min[abs(h_no_slope_10min$Hist10min) > 0.7,]
>> h_no_slope_below_10min <-
>> h_no_slope_10min[abs(h_no_slope_10min$Hist10min) <= 0.7,]
>>
>> h_steep_slope_above_10min <-
>> h_steep_slope_10min[abs(h_steep_slope_10min$Hist10min) > 0.7,]
>> h_steep_slope_below_10min <-
>> h_steep_slope_10min[abs(h_steep_slope_10min$Hist10min) <= 0.7,]
>>
>> a10 = h_slope_macd_below_10min$Hist10min
>> b10 = h_no_slope_below_10min$Hist10min
>> f10 = h_steep_slope_below_10min$Hist10min
>>
>> c10 = h_steep_slope_above_10min$Hist10min
>> d10 = h_no_slope_above_10min$Hist10min
>> e10 = h_slope_macd_above_10min$Hist10min
>>
>> addTA(d10, on = NA, col = '#060440', type = 'h', lwd = 1)
>> addTA(a10, on = 4, col = '#C03E15' , type = 'h', lwd = 3, yrange =
>> range(Hist_range_10))
>> addTA(b10, on = 4, col = '#C03E15', type = 'h', lwd = 1)
>> addTA(f10, on = 4, col = '#C03E15', type = 'h', lwd = 6)
>> addTA(c10, on = 4, col = '#060440', type = 'h', lwd = 3)
>>
>> addTA(e10, on = 4, col = '#060440', type = 'h', lwd = 6)
>> addTA(zero, on = 4, col = 'black', type = 'l', lwd = 1)
>>
>> addTA(MACD, on = NA, col = c('blue', 'red', 'green'), type = c('l',
>> 'l', 'l'), lwd = c(4, 2, 1))
>> addTA(zero, on = 5, col = 'black', type = 'l', lwd = 1)
>>
>> if (as.character(enter_times[i]) == "Long") ? ? Buy <- ES[time(ES) ==
>> time(enter_times[i]) + 60]$ES.Open
>> if (as.character(enter_times[i]) == "Long") ? ? addTA(Buy, on = 1, col =
>> 'red', type = 'p', pch = 6, lwd = 2)
>> if (as.character(enter_times[i]) == "Long") ? ? addTA(Buy > 0, on =
>> -(1:5), col = '#FFCAFD', border = NA)
>>
>> if (as.character(enter_times[i]) == "Short") ? ?Short <- ES[time(ES) ==
>> time(enter_times[i]) + 60]$ES.Open
>> if (as.character(enter_times[i]) == "Short") ? ?addTA(Short, on = 1,
>> col = 'blue', type = 'p', pch = 2, lwd = 2)
>> if (as.character(enter_times[i]) == "Short") ? ?addTA(Short > 0, on =
>> -(1:5), col = '#FF7878', border = NA)
>>
>> # Repeats with i = 2...
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
>


From jeff.a.ryan at gmail.com  Thu Jul 23 18:37:06 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 23 Jul 2009 11:37:06 -0500
Subject: [R-SIG-Finance] quantmod Charting
In-Reply-To: <4A688786.90505@braverock.com>
References: <771cf9480907230838y3febfd24xd4e1d665ef588fd7@mail.gmail.com>
	<4A688786.90505@braverock.com>
Message-ID: <e8e755250907230937p44a32cer8f9f66eeeb230ed4@mail.gmail.com>

Some details on how quantmod/chartSeries works are in order:

Looping (or calling) outside of the .GlobalEnv is tricky.  To make
chartSeries and addTA stuff  interactively they do a lot of internal
checking and manipulation.

Calling addTA (or add***) from a non-global env will cause the plot
step to be skipped. To force plotting you need to wrap the add calls
in plot()

Secondly, opening a device first, then looping over these values will
cause a full screen redraw at each iteration -- usually what you want.
 In a pdf() this causes a new plot to be printed.  So if you call
addTA() 10 times on a chart, you'll have a total of 11 charts/pages.

The best way (at present) to manage this is to simply dump the final
chart (after all the additions) to a pdf using:

?saveChart or ?dev.copy2pdf

There are some further nuances to this of course, but that should get
you at least in the right direction.

HTH
Jeff

On Thu, Jul 23, 2009 at 10:53 AM, Brian G. Peterson<brian at braverock.com> wrote:
> You haven't sent us a data series that we can reproduce your problem with.
> ?Maybe grab one from an instrument that you don't trade, or simulate a
> series that has some of the same properties that you use to trade.
>
> I typically have no problem using chartSeries on a day of tick data
> (millions of points) or 15-sec OHLC bars (hundreds of thousands of points)
> beyond the time it takes to actually display/render the chart. ?This
> includes calling addTA to add extra things to the charts in question.
>
> One thing that comes immediately to mind is that I would probably separate
> the process of creating the indicators from the process of charting. ?Run
> through your data series and generate your indicators, then again to
> generate your signals, and *then* chart it from the aligned time series.
>
> There have been multiple previous posts to this list regarding calls to
> chartSeries and addTA from within custom functions, perhaps a review of the
> list archives is in order as well.
>
> Regards,
>
> ?- Brian
>
> James Klingsporn wrote:
>>
>> Hello,
>>
>> I am using quantmod to create charts that have buy/sell date times
>> with a window 40 minutes before and after the buy/sell date time. ?I
>> want to create ps charts and ultimately pdf charts of the trades. ?I'm
>> not concerned with the exit right now. ?The number of buy/sell date
>> times can range between 150 ? 3000+, so I'd clearly like to automate
>> the process. ?The code I?ve been using is attached below. ?I've tried
>> putting the below into a function so I call the function with the
>> trade number, but this does not produce the full chart (and a similar
>> problem with 'if' and thus the multipe 'if's below for the same
>> condition). ?If I run the below as Source R code it also doesn't
>> produce the full chart. ?Also, because the code is so long for each
>> chart, I can't just run the code for all the trades at 1 time; I have
>> to cut and paste the script for about 75 trades/time. ?The code has
>> run time of about 5-6 trade charts/min which clearly takes a while for
>> the 3000+ trades and then considering I have also to cut and paste the
>> code.
>>
>> Is there any way to speed up or otherwise improve the script/charting?
>>
>> Thanks,
>> Jim
>>
>>
>> R code for charts -
>>
>> ?require(quantmod)
>>
>> ? ? ? ?file = 'C:/Consulting/chart.txt'
>> ? ? ? ?chart_data <- read.zoo(file, header = FALSE, col.names = c("Date",
>> "Open", "High", "Low", "Close", "MACD1min", "MACD5min", "MACD10min",
>> "Hist1min", "Hist5min", "Hist10min"), sep = ",", dec = ".", tz = "",
>> format = "%Y-%m-%d %H:%M:%S")
>> ? ?ES <- cbind(chart_data$Open, chart_data$High, chart_data$Low,
>> chart_data$Close)
>> ? ? ? ?ES <- as.quantmod.OHLC(ES, col.names = c("Open", "High", "Low",
>> "Close"))
>>
>> ? ?time_before = 40
>> ? ?time_after = 40
>> ? ?time_before <- time_before * 60
>> ? ?time_after <- time_after * 60
>>
>> ? ?enter_times_path = 'C:/Consulting/ES_trades_with_slope_rule.txt'
>> ? ?enter_times <- read.zoo(enter_times_path, header = FALSE,
>> col.names = c("Date", "Position"), sep = ",", dec = ".", tz = "",
>> format = "%Y-%m-%d %H:%M:%S")
>>
>> ? ? ? ?ctheme <- chartTheme(theme = 'white', area = 'white', up.col =
>> '#0C481A', dn.col = '#B12525')
>>
>> i = 1
>> # Trade number repeats for number of trades
>>
>> display_range <- window(ES, start = time(enter_times[i]) - time_before
>> , end = time(enter_times[i]) + time_after)
>> m <- window(chart_data, start = time(enter_times[i]) - time_before ,
>> end = time(enter_times[i]) + time_after)
>> MACD <- cbind(m$MACD1min, m$MACD5min, m$MACD10min)
>>
>> chartSeries(display_range, theme = ctheme)
>>
>> Hist_range_5 = c(min(range(m$Hist5min)[1], - 0.2),
>> max(range(m$Hist5min)[2], 0.2))
>> Hist_range_10 = c(min(range(m$Hist10min)[1], - 0.2),
>> max(range(m$Hist10min)[2], 0.2))
>>
>> h_steep_slope_1min <- m[abs(diff(m$Hist1min)) >= 0.1]
>> h_slope_1min <- m[abs(diff(m$Hist1min)) >= 0.05 & abs(diff(m$Hist1min)) <
>> 0.1]
>> h_no_slope_1min <- m[abs(diff(m$Hist1min)) < 0.05]
>> h_slope_macd_above_1min <- h_slope_1min[abs(h_slope_1min$Hist1min) > 0.2,]
>> h_slope_macd_below_1min <- h_slope_1min[abs(h_slope_1min$Hist1min) <=
>> 0.2,]
>> h_no_slope_above_1min <- h_no_slope_1min[abs(h_no_slope_1min$Hist1min) >
>> 0.2,]
>> h_no_slope_below_1min <- h_no_slope_1min[abs(h_no_slope_1min$Hist1min) <=
>> 0.2,]
>> h_steep_slope_above_1min <-
>> h_steep_slope_1min[abs(h_steep_slope_1min$Hist1min) > 0.2,]
>> h_steep_slope_below_1min <-
>> h_steep_slope_1min[abs(h_steep_slope_1min$Hist1min) <= 0.2,]
>>
>> a = h_slope_macd_below_1min$Hist1min
>> b = h_no_slope_below_1min$Hist1min
>> f = ? ? h_steep_slope_below_1min$Hist1min
>>
>> c = h_steep_slope_above_1min$Hist1min
>> d = h_no_slope_above_1min$Hist1min
>> e = h_slope_macd_above_1min$Hist1min
>>
>> zero_times = time(m)
>> zero = zoo(0, order.by = zero_times)
>>
>> addTA(a, on = NA, col = '#C03E15' , type = 'h', lwd = 3, yrange =
>> range(m$Hist1min))
>> addTA(b, on = 2, col = '#C03E15', type = 'h', lwd = 1)
>> addTA(f, on = 2, col = '#C03E15', type = 'h', lwd = 6)
>> addTA(c, on = 2, col = '#060440', type = 'h', lwd = 3)
>> addTA(d, on = 2, col = '#060440', type = 'h', lwd = 1)
>> addTA(e, on = 2, col = '#060440', type = 'h', lwd = 6)
>> addTA(zero, on = 2, col = 'black', type = 'l', lwd = 1)
>>
>> h_steep_slope_5min <- m[abs(diff(m$Hist5min)) >= 0.1]
>> h_slope_5min <- m[abs(diff(m$Hist5min)) >= 0.05 & abs(diff(m$Hist5min)) <
>> 0.1]
>> h_no_slope_5min <- m[abs(diff(m$Hist5min)) < 0.05]
>>
>> h_slope_macd_above_5min <- h_slope_5min[abs(h_slope_5min$Hist5min) > 0.7,]
>> h_slope_macd_below_5min <- h_slope_5min[abs(h_slope_5min$Hist5min) <=
>> 0.7,]
>>
>> h_no_slope_above_5min <- h_no_slope_5min[abs(h_no_slope_5min$Hist5min) >
>> 0.7,]
>> h_no_slope_below_5min <- h_no_slope_5min[abs(h_no_slope_5min$Hist5min) <=
>> 0.7,]
>>
>> h_steep_slope_above_5min <-
>> h_steep_slope_5min[abs(h_steep_slope_5min$Hist5min) > 0.7,]
>> h_steep_slope_below_5min <-
>> h_steep_slope_5min[abs(h_steep_slope_5min$Hist5min) <= 0.7,]
>>
>> a5 = h_slope_macd_below_5min$Hist5min
>> b5 = h_no_slope_below_5min$Hist5min
>> f5 = h_steep_slope_below_5min$Hist5min
>>
>> c5 = h_steep_slope_above_5min$Hist5min
>> d5 = h_no_slope_above_5min$Hist5min
>> e5 = h_slope_macd_above_5min$Hist5min
>>
>> addTA(a5, on = NA, col = '#C03E15' , type = 'h', lwd = 3, yrange =
>> range(Hist_range_5))
>> addTA(b5, on = 3, col = '#C03E15', type = 'h', lwd = 1)
>> addTA(f5, on = 3, col = '#C03E15', type = 'h', lwd = 6)
>> addTA(c5, on = 3, col = '#060440', type = 'h', lwd = 3)
>> addTA(d5, on = 3, col = '#060440', type = 'h', lwd = 1)
>> addTA(e5, on = 3, col = '#060440', type = 'h', lwd = 6)
>> addTA(zero, on = 3, col = 'black', type = 'l', lwd = 1)
>>
>> h_steep_slope_10min <- m[abs(diff(m$Hist10min)) >= 0.1]
>> h_slope_10min <- m[abs(diff(m$Hist10min)) >= 0.05 &
>> abs(diff(m$Hist10min)) < 0.1]
>> h_no_slope_10min <- m[abs(diff(m$Hist10min)) < 0.05]
>>
>> h_slope_macd_above_10min <- h_slope_10min[abs(h_slope_10min$Hist10min) >
>> 0.7,]
>> h_slope_macd_below_10min <- h_slope_10min[abs(h_slope_10min$Hist10min) <=
>> 0.7,]
>>
>> h_no_slope_above_10min <-
>> h_no_slope_10min[abs(h_no_slope_10min$Hist10min) > 0.7,]
>> h_no_slope_below_10min <-
>> h_no_slope_10min[abs(h_no_slope_10min$Hist10min) <= 0.7,]
>>
>> h_steep_slope_above_10min <-
>> h_steep_slope_10min[abs(h_steep_slope_10min$Hist10min) > 0.7,]
>> h_steep_slope_below_10min <-
>> h_steep_slope_10min[abs(h_steep_slope_10min$Hist10min) <= 0.7,]
>>
>> a10 = h_slope_macd_below_10min$Hist10min
>> b10 = h_no_slope_below_10min$Hist10min
>> f10 = h_steep_slope_below_10min$Hist10min
>>
>> c10 = h_steep_slope_above_10min$Hist10min
>> d10 = h_no_slope_above_10min$Hist10min
>> e10 = h_slope_macd_above_10min$Hist10min
>>
>> addTA(d10, on = NA, col = '#060440', type = 'h', lwd = 1)
>> addTA(a10, on = 4, col = '#C03E15' , type = 'h', lwd = 3, yrange =
>> range(Hist_range_10))
>> addTA(b10, on = 4, col = '#C03E15', type = 'h', lwd = 1)
>> addTA(f10, on = 4, col = '#C03E15', type = 'h', lwd = 6)
>> addTA(c10, on = 4, col = '#060440', type = 'h', lwd = 3)
>>
>> addTA(e10, on = 4, col = '#060440', type = 'h', lwd = 6)
>> addTA(zero, on = 4, col = 'black', type = 'l', lwd = 1)
>>
>> addTA(MACD, on = NA, col = c('blue', 'red', 'green'), type = c('l',
>> 'l', 'l'), lwd = c(4, 2, 1))
>> addTA(zero, on = 5, col = 'black', type = 'l', lwd = 1)
>>
>> if (as.character(enter_times[i]) == "Long") ? ? Buy <- ES[time(ES) ==
>> time(enter_times[i]) + 60]$ES.Open
>> if (as.character(enter_times[i]) == "Long") ? ? addTA(Buy, on = 1, col =
>> 'red', type = 'p', pch = 6, lwd = 2)
>> if (as.character(enter_times[i]) == "Long") ? ? addTA(Buy > 0, on =
>> -(1:5), col = '#FFCAFD', border = NA)
>>
>> if (as.character(enter_times[i]) == "Short") ? ?Short <- ES[time(ES) ==
>> time(enter_times[i]) + 60]$ES.Open
>> if (as.character(enter_times[i]) == "Short") ? ?addTA(Short, on = 1,
>> col = 'blue', type = 'p', pch = 2, lwd = 2)
>> if (as.character(enter_times[i]) == "Short") ? ?addTA(Short > 0, on =
>> -(1:5), col = '#FF7878', border = NA)
>>
>> # Repeats with i = 2...
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From jeff.a.ryan at gmail.com  Thu Jul 23 18:38:57 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 23 Jul 2009 11:38:57 -0500
Subject: [R-SIG-Finance] quantmod Charting
In-Reply-To: <771cf9480907230934h4baaaa12r31373828cf8ead13@mail.gmail.com>
References: <771cf9480907230838y3febfd24xd4e1d665ef588fd7@mail.gmail.com>
	<4A688786.90505@braverock.com>
	<771cf9480907230934h4baaaa12r31373828cf8ead13@mail.gmail.com>
Message-ID: <e8e755250907230938x7d5bd589y2462b1e197cc439f@mail.gmail.com>

> take a look at TTR again; ?For the add* function, such as MACD, the TA
> is calculated from the begining of the chart data, correct? ?This
> would then be different from what I'm calculating, simply because the
> TA continues from uncharted data.

It is calculated from the beginning of the data that was passed to the
original call.  Using subset= or zoomChart maintains the data (and
allows for full data to be used in calculations), just changes the
window you see.

HTH
Jeff


From seancarmody at gmail.com  Thu Jul 23 23:17:37 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Fri, 24 Jul 2009 07:17:37 +1000
Subject: [R-SIG-Finance] Plot.zoo does not take vector parameter for lwd
Message-ID: <ce6bbb9d0907231417k1523eb90ve63152f7f3a23d3c@mail.gmail.com>

It seems to me that plot.zoo does not treat the linewidth (lwd)
parameter as a vector and instead only uses the first value. Consider
the following example:

x <- zoo(data.frame(X=runif(12), Y=rnorm(12),
x=runif(12)),as.Date(paste("2009",1:12,1,sep="-")))
plot(x, plot.type="single", lwd=c(1,2,3))

The resulting plot has lines all of the same width. In contrast, the
col argument work as expected, for example:

plot(x,plot.type="single", col=rainbow(3))

Is there any way to get around this without plotting each line separately?

On a related subject, I found this post
http://tolstoy.newcastle.edu.au/R/e6/help/09/01/1297.html which
explains how to change the plot.zoo so that no bounding box is drawn.
It would seem a lot easier if plot.zoo would actually use a bty
argument that is passed. Is there any reason why this is not possible?

Regards,
Sean Carmody.


From ggrothendieck at gmail.com  Fri Jul 24 01:44:21 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Jul 2009 18:44:21 -0500
Subject: [R-SIG-Finance] Plot.zoo does not take vector parameter for lwd
In-Reply-To: <ce6bbb9d0907231417k1523eb90ve63152f7f3a23d3c@mail.gmail.com>
References: <ce6bbb9d0907231417k1523eb90ve63152f7f3a23d3c@mail.gmail.com>
Message-ID: <971536df0907231644p483766ecl78d99b74bf96e544@mail.gmail.com>

On Thu, Jul 23, 2009 at 4:17 PM, Sean Carmody<seancarmody at gmail.com> wrote:
> It seems to me that plot.zoo does not treat the linewidth (lwd)
> parameter as a vector and instead only uses the first value. Consider
> the following example:
>
> x <- zoo(data.frame(X=runif(12), Y=rnorm(12),
> x=runif(12)),as.Date(paste("2009",1:12,1,sep="-")))
> plot(x, plot.type="single", lwd=c(1,2,3))
>
> The resulting plot has lines all of the same width. In contrast, the
> col argument work as expected, for example:
>
> plot(x,plot.type="single", col=rainbow(3))
>
> Is there any way to get around this without plotting each line separately?

OK. plot.zoo does not implement every possible parameter in
full generality but since you ask about that one specifically a
fix has been added to the devel version.

library(zoo)
source("http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/*checkout*/pkg/R/plot.zoo.R?rev=603&root=zoo")
plot(zoo(matrix(1:4, 2)), lwd = 1:2)

>
> On a related subject, I found this post
> http://tolstoy.newcastle.edu.au/R/e6/help/09/01/1297.html which
> explains how to change the plot.zoo so that no bounding box is drawn.
> It would seem a lot easier if plot.zoo would actually use a bty
> argument that is passed. Is there any reason why this is not possible?

Here is an easier way:

opar <- par(bty = "n")
plot(zoo(1:5))


From seancarmody at gmail.com  Fri Jul 24 03:39:05 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Fri, 24 Jul 2009 11:39:05 +1000
Subject: [R-SIG-Finance] Plot.zoo does not take vector parameter for lwd
In-Reply-To: <971536df0907231644p483766ecl78d99b74bf96e544@mail.gmail.com>
References: <ce6bbb9d0907231417k1523eb90ve63152f7f3a23d3c@mail.gmail.com>
	<971536df0907231644p483766ecl78d99b74bf96e544@mail.gmail.com>
Message-ID: <ce6bbb9d0907231839o35a3d2a4sa6c660fb81726e5e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090724/ae6e39ef/attachment.pl>

From windspeedo99 at gmail.com  Fri Jul 24 04:13:05 2009
From: windspeedo99 at gmail.com (Wind)
Date: Fri, 24 Jul 2009 10:13:05 +0800
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
In-Reply-To: <4A646F7E.3090406@braverock.com>
References: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>
	<4A5F2A38.9060100@braverock.com>
	<d718c8210907200157m1b9c9698me96a6af142d85e87@mail.gmail.com>
	<4A646F7E.3090406@braverock.com>
Message-ID: <d718c8210907231913m49ce69fnf44289155ec9a81d@mail.gmail.com>

Since I am not good at coding or statistics, genetic algo has been used
instead of tabu search.    Package rgenoud is great and idiot proof.

Attached is the latest analysis on SSEC index, the equity market index of
mainland China.  genoud() has been run for 10 times, generating 10 fits.
The residuals are all stationary according to ADF test from package urca.
But the time window setting is subjective and maybe some minor problems on
parameter conditions such as omega and C.  So,  just for fun.

Thanks for all the encouragement and help from the list.    And thanks for
the detailed instructions on statistics issues by one of the author on LPPL,
Dr. Lin.

wind

On Mon, Jul 20, 2009 at 9:22 PM, Brian G. Peterson <brian at braverock.com>wrote:

> Glad to see you're making progress on this problem.
>
> This paper
> http://www.diegm.uniud.it/satt/papers/DiSc06b.pdf
>
> implements a tabu search in R, though they didn't publish their code.  You
> might want to contact them for the implementation and permission to share
> their tabu search algorithm/code with the R community.
>
> They also reference an R package called RACE by this gentleman:
> http://iridia.ulb.ac.be/~mbiro/
> that they use for evaluating the solution.
>
> More generally, from my limited understanding, tabu search is an extension
> and refinement of simulated annealing approaches easily implemented in R.
>  In brief the approach is to take your best 'n' solutions from a random
> space search, and then search "near" those solutions.  Simulated annealing
> and its close cousins have a lot of benefits in finance, where a single true
> optima from a closed form problem is not likely to be available.  I
> personally have rarely found 'optim' to be usable for my problem space, and
> have had to use other solvers for practical problems in finance.
>
> Regards,
>
>  - Brian
>
>
> Wind wrote:
>
>> Some progress.   The LPPL curve could be plotted with the following codes.
>> The problem now is how to get the best fit parameters.
>> Some researchers  use python or matlab for LPPL calibrating.     It
>> seems that some of them prefer tabu search for optimums locating.   It
>> seems that there's still no general function for tabu search in R.
>> At the end of codes, I give the possible parameter combinations to be
>> searched in, maybe there are other functions for optimum searching in
>> R.
>> Any suggestion would be appreciated.
>>
>>
>> ## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
>> Financial and Economic Crisis
>> ## http://arxiv.org/abs/0905.0220
>> ## Fig. 23 S&P500 index (in logarithmic scale)in  Page 39
>>
>>
>> library(quantmod)
>>
>> LPPL1<-function(p,dtc=20,alpha=0.35,omega=0.1,phi=1)
>> {
>>        #function in page 26 of http://arxiv.org/abs/0905.0220
>>        #the basic form of LPPL
>>        dtc=abs(floor(dtc))
>>        tc<-length(p)+dtc
>>        dt<-abs(tc-(1:length(p)))
>>
>>        x1<-dt^alpha
>>        x2<-(dt^alpha)*cos(omega*log(dt)+phi)
>>
>>        f<-lm(log(p) ~ x1+x2)
>>
>>
>>  f$para<-list(recno=dim(f$model)[1],dtc=dtc,alpha=alpha,omega=omega,phi=phi,sigma=summary(f)$sigma)
>>        return(f)
>> }
>>
>> opt.lppl<-function(x)
>> {
>>        #derived function for optim
>>        return(LPPL1(p,dtc=x[1],alpha=x[2],omega=x[3],phi=x[4])$para$sigma)
>> }
>>
>> LPPL1.x<-function(lpplf,pt=100)
>> {
>>        #x axis for predicting
>>        dt<-abs((lpplf$recno+lpplf$dtc)-(1:(lpplf$recno+lpplf$dtc+pt)))
>>        dt[dt==0]<-0.5
>>
>>        x1<-dt^lpplf$alpha
>>        x2<-(dt^lpplf$alpha)*cos(lpplf$omega*log(dt)+lpplf$phi)
>>        return(list(x1=x1,x2=x2))
>>
>> }
>>
>> #get the SP500 index
>>
>> pr<-getSymbols("^GSPC",auto.assign=FALSE,from="2003-10-1",to="2007-05-16")[,4]
>> p<-as.numeric(pr)
>>
>> plot(p,type="l",log="y",xlim=c(0,length(p)+100),ylim=c(min(p),max(p)*1.2))
>> abline(v=length(p),col="green")
>>
>> #something like the Fig. 23 in  Page 39 of http://arxiv.org/abs/0905.0220
>> #but obviously the result is not calibrated well
>> #using optim like this can not calibrate the LPPL model
>> opts<-sapply(seq(1,50,10),function(x){
>>                        o<-optim(c(x,0.6,20,1),opt.lppl)
>>
>>  f3<-LPPL1(p,dtc=o$par[1],alpha=o$par[2],omega=o$par[3],phi=o$par[4])
>>                        xp<-LPPL1.x(f3$para,200)
>>                        f3p<-predict(f3,data.frame(x1=xp$x1,x2=xp$x2))
>>                        lines(exp(f3p),col="blue")
>>                        lines(exp(fitted(f3)),col="red")
>>                        f3$para
>>                })
>>
>>
>>
>> ##crash point after dtc days
>> dtc<-seq(1,100,1)
>>
>> ##appropraite range of the parametes of LPPL
>> ##according to Dr. W.X. Zhou's new book which is in Chinese
>> ##the increments of the sequences are added according to my own judement
>> alpha<-seq(0.01,1.2,0.1)
>> omega<-seq(0,40,1)
>> phi<-seq(0,7,0.1)
>>
>> ##millions possible combinations
>> #complete test would be difficult
>> para<-expand.grid(dtc=dtc,alpha=alpha,omega=omega,phi=phi)
>> dim(para)
>>
>> system.time(sigs<-apply(para[1:100,],1,function(x){LPPL1(p,dtc=x[1],alpha=x[2],omega=x[3],phi=x[4])$para$sigma}))
>>
>> ##methods for minimum sigma searching within the parameter combinations
>> ##not implemented yet
>>
>>
>> wind
>>
>>
>>
>>
>>
>> On Thu, Jul 16, 2009 at 9:25 PM, Brian G. Peterson<brian at braverock.com>
>> wrote:
>>
>>
>>> So first, using your real name and ideally your professional identity,
>>> ask
>>> for the python code.  Better yet, get an academic buddy to do it. Usually
>>> getting access to the code isn't too tough.  Mention things like
>>> "repeatable
>>> research" and "collaboration" in your email.  Two of the authors publish
>>> their email addresses in one of the papers you reference, so contacting
>>> them
>>> should be easy.
>>>
>>> Next port the python code to R.
>>>
>>> If you can't do that, then replicate the model in R "from scratch".  A
>>> trivial scan of the paper in question lends several techniques that are
>>> well
>>> covered in R: AR, GARCH, power laws, linear regression, stochastic
>>> discount
>>> factor, Ornstein-Uhlenbeck, etc.
>>> There are volumes of information available on these topics from within R,
>>> in
>>> numerous books, and in the archives of this mailing list and r-help.
>>>
>>> You're going to have to do your replication in pieces, probably starting
>>> with their implementation of the log periodic power law (LPPL), for which
>>> I
>>> do not believe there is an existing direct analogue in R though all the
>>> component parts necessary to replicate it should be readily available.
>>>
>>> As you work on each step of the replication, share your code with this
>>> list
>>> and the problems you are having with a particular step.  Ask specific,
>>> directed questions with code to back them up.  Someone will likely help
>>> you
>>> solve the specific problem.
>>>
>>> In R generally, it is not necessary that you be able to *do* the math
>>> (think
>>> pencil and paper), but if you plan to replicate published work, it will
>>> be
>>> necessary to *understand* at least some of how the math works, and to be
>>> able to pick out the names of techniques that you can search for an
>>> utilize.
>>>
>>> Basically, I'm recommending that you (specifically) and others (more
>>> generally) should share the process of replicating a technique like this,
>>> as
>>> well as the final product, to give all the rest of us who are likely to
>>> be
>>> helping "you" get all this done. quid pro quo.
>>>
>>> Cheers,
>>>
>>>  - Brian
>>>
>>>
>>> --
>>> Brian G. Peterson
>>> http://braverock.com/brian/
>>> Ph: 773-459-4973
>>> IM: bgpbraverock
>>>
>>>
>>>
>>> Wind wrote:
>>>
>>>
>>>> Prof. Sornette has spent years forcasting bubble burst with
>>>> "log-periodic power law".    The latest paper  gives "a
>>>> self-consistent model for explosive financial bubbles, which combines
>>>> a mean-reverting volatility process and a stochastic conditional
>>>> return which reflects nonlinear positive feedbacks and continuous
>>>> updates of the investors' beliefs and sentiments."
>>>>
>>>> And his  latest  predicting is the burst of Chinese equity bubble at
>>>> the end of July.     http://arxiv.org/abs/0907.1827
>>>>
>>>> While waiting to see the result, I wonder whether it is possible to
>>>> replicate the forcast with R.  The model is in the page 10 of the "A
>>>> Consistent Model of `Explosive' Financial Bubbles With Mean-Reversing
>>>> Residuals",  http://arxiv.org/abs/0905.0128  .   The output chart is
>>>> in the page 3 of "The Chinese Equity Bubble: Ready to Burst",
>>>> http://arxiv.org/abs/0907.1827 .   I guess the authors of the latter
>>>> paper use the same model as described in the first paper.
>>>>
>>>> Because statistics is still challenging for me though I could use R
>>>> for  basic data manipulations,  I wonder which package or function
>>>> would be necessary to implement the model in the paper.  The model
>>>> seems more complicated than the models in the R tutorials for me.
>>>> By the way, the author of the paper used Python and the codes are
>>>> private.
>>>>
>>>> Any suggestion would be highly appreciated.
>>>>
>>>>
>>>>
>>>
>>>
>>>
>>
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090724/82f44d7f/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ssec final 1.png
Type: image/png
Size: 10268 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090724/82f44d7f/attachment.png>

From joshcchien at yahoo.com  Fri Jul 24 05:57:34 2009
From: joshcchien at yahoo.com (Josh C. Chien)
Date: Thu, 23 Jul 2009 20:57:34 -0700 (PDT)
Subject: [R-SIG-Finance] For pricing Bond Library ?
Message-ID: <28775.63485.qm@web110101.mail.gq1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090723/01872339/attachment.pl>

From knguyen at cs.umb.edu  Fri Jul 24 08:02:02 2009
From: knguyen at cs.umb.edu (Khanh Nguyen)
Date: Fri, 24 Jul 2009 13:02:02 +0700
Subject: [R-SIG-Finance] For pricing Bond Library ?
In-Reply-To: <28775.63485.qm@web110101.mail.gq1.yahoo.com>
References: <28775.63485.qm@web110101.mail.gq1.yahoo.com>
Message-ID: <2871c9e10907232302s44b6e15bg1cd42853889d1a84@mail.gmail.com>

Hi Josh,

You can check out RQuantLib, for some simple bond pricing. Currently,
we have Zero, Fixed Rate, Floating, CallableBond, Convertible
(Floating, Zero, Fixed Rate).

-k

On Fri, Jul 24, 2009 at 10:57 AM, Josh C. Chien<joshcchien at yahoo.com> wrote:
> Hi R-Finance users,
> Does anyone know what library is for pricing bond or fixed income products ?
> Thanks a lot.
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From edd at debian.org  Fri Jul 24 13:22:55 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 24 Jul 2009 06:22:55 -0500
Subject: [R-SIG-Finance] For pricing Bond Library ?
In-Reply-To: <2871c9e10907232302s44b6e15bg1cd42853889d1a84@mail.gmail.com>
References: <28775.63485.qm@web110101.mail.gq1.yahoo.com>
	<2871c9e10907232302s44b6e15bg1cd42853889d1a84@mail.gmail.com>
Message-ID: <19049.39311.35123.407069@ron.nulle.part>


On 24 July 2009 at 13:02, Khanh Nguyen wrote:
| Hi Josh,
| 
| You can check out RQuantLib, for some simple bond pricing. Currently,
| we have Zero, Fixed Rate, Floating, CallableBond, Convertible
| (Floating, Zero, Fixed Rate).

And because RQuantLib is changing pretty rapidly right now during Khanh's
Google Summer of Code work on it, we have no made a new release in a while.
So to take advantage of the new code, you currently need to go to the R-Forge
page at https://r-forge.r-project.org/R/?group_id=117 to get packages or
tarballs.  Thanks to work by Stefan this should now include a working windows
binary.

Dirk

-- 
Three out of two people have difficulties with fractions.


From spencer.graves at prodsyse.com  Fri Jul 24 14:32:22 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Fri, 24 Jul 2009 06:32:22 -0600
Subject: [R-SIG-Finance] For pricing Bond Library ?
In-Reply-To: <19049.39311.35123.407069@ron.nulle.part>
References: <28775.63485.qm@web110101.mail.gq1.yahoo.com>	<2871c9e10907232302s44b6e15bg1cd42853889d1a84@mail.gmail.com>
	<19049.39311.35123.407069@ron.nulle.part>
Message-ID: <4A69A9D6.5080808@prodsyse.com>

Hi, Dirk: 


      Will Quantlib now support the Nelson / Siegel / Svensson and 
spline models for the term structure of interest rates, as does the 
"termstrc" package (and as documented in the companion paper, Ferstl and 
Hayden 2008 "Zero Coupon Yield Curve Estimation with the Package 
termstrc" http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1307149)? 


      Thanks,
      Spencer


Dirk Eddelbuettel wrote:
> On 24 July 2009 at 13:02, Khanh Nguyen wrote:
> | Hi Josh,
> | 
> | You can check out RQuantLib, for some simple bond pricing. Currently,
> | we have Zero, Fixed Rate, Floating, CallableBond, Convertible
> | (Floating, Zero, Fixed Rate).
>
> And because RQuantLib is changing pretty rapidly right now during Khanh's
> Google Summer of Code work on it, we have no made a new release in a while.
> So to take advantage of the new code, you currently need to go to the R-Forge
> page at https://r-forge.r-project.org/R/?group_id=117 to get packages or
> tarballs.  Thanks to work by Stefan this should now include a working windows
> binary.
>
> Dirk
>
>


From brian at braverock.com  Fri Jul 24 14:46:48 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 24 Jul 2009 07:46:48 -0500
Subject: [R-SIG-Finance] For pricing Bond Library ?
In-Reply-To: <4A69A9D6.5080808@prodsyse.com>
References: <28775.63485.qm@web110101.mail.gq1.yahoo.com>	<2871c9e10907232302s44b6e15bg1cd42853889d1a84@mail.gmail.com>	<19049.39311.35123.407069@ron.nulle.part>
	<4A69A9D6.5080808@prodsyse.com>
Message-ID: <4A69AD38.8050305@braverock.com>

spencerg wrote:
> Hi, Dirk:
>
>      Will Quantlib now support the Nelson / Siegel / Svensson and 
> spline models for the term structure of interest rates, as does the 
> "termstrc" package (and as documented in the companion paper, Ferstl 
> and Hayden 2008 "Zero Coupon Yield Curve Estimation with the Package 
> termstrc" http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1307149)?
The quantlib library which RQuantlib wraps includes log-linear 
interpolation for term structure.  I think that the "termstrc" package 
will remain an important part of the toolkit for anyone working in with 
fixed income instruments.  I've also used the "CreditMetrics" package as 
part of developing default scenarios for corporate credit.

Regards,

   - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From djmphd at gmail.com  Fri Jul 24 14:56:24 2009
From: djmphd at gmail.com (David J. Moore, Ph.D.)
Date: Fri, 24 Jul 2009 07:56:24 -0500
Subject: [R-SIG-Finance] Fwd: Fwd: Inequality constraints in GMM
	estimation?
In-Reply-To: <4A674108.2010908@prodsyse.com>
References: <2EC65577-0A86-4579-B179-C52EA0EC7167@gmail.com>
	<d94e0b160907181631q4e3793d5r60e565f310477732@mail.gmail.com>
	<4A674108.2010908@prodsyse.com>
Message-ID: <d94e0b160907240556y40fb63abjd38f1ae4e61d36ab@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090724/80af0b26/attachment.pl>

From knguyen at cs.umb.edu  Fri Jul 24 15:30:35 2009
From: knguyen at cs.umb.edu (Khanh Nguyen)
Date: Fri, 24 Jul 2009 09:30:35 -0400
Subject: [R-SIG-Finance] For pricing Bond Library ?
In-Reply-To: <4A69A9D6.5080808@prodsyse.com>
References: <28775.63485.qm@web110101.mail.gq1.yahoo.com>
	<2871c9e10907232302s44b6e15bg1cd42853889d1a84@mail.gmail.com>
	<19049.39311.35123.407069@ron.nulle.part>
	<4A69A9D6.5080808@prodsyse.com>
Message-ID: <2871c9e10907240630o77bf95f3mab3d50f8ed870d9e@mail.gmail.com>

Is it what you are talking about

https://quantlib.svn.sourceforge.net/svnroot/quantlib/trunk/QuantLib/Examples/FittedBondCurve/FittedBondCurve.cpp

This is also added to RQuantLib a few days ago.

-k


On Fri, Jul 24, 2009 at 8:32 AM, spencerg<spencer.graves at prodsyse.com> wrote:
> Hi, Dirk:
>
> ? ? Will Quantlib now support the Nelson / Siegel / Svensson and spline
> models for the term structure of interest rates, as does the "termstrc"
> package (and as documented in the companion paper, Ferstl and Hayden 2008
> "Zero Coupon Yield Curve Estimation with the Package termstrc"
> http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1307149)?
>
> ? ? Thanks,
> ? ? Spencer
>


From spencer.graves at prodsyse.com  Fri Jul 24 15:52:06 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Fri, 24 Jul 2009 07:52:06 -0600
Subject: [R-SIG-Finance] For pricing Bond Library ?
In-Reply-To: <2871c9e10907240630o77bf95f3mab3d50f8ed870d9e@mail.gmail.com>
References: <28775.63485.qm@web110101.mail.gq1.yahoo.com>	
	<2871c9e10907232302s44b6e15bg1cd42853889d1a84@mail.gmail.com>	
	<19049.39311.35123.407069@ron.nulle.part>	
	<4A69A9D6.5080808@prodsyse.com>
	<2871c9e10907240630o77bf95f3mab3d50f8ed870d9e@mail.gmail.com>
Message-ID: <4A69BC86.2080009@prodsyse.com>

Hi, Khan: 


      Just skimming, it looks good.  It looks like it includes the 
Nelson / Siegel model, plus cubic splines and some others but not the 
Svensson model.  I'd have to try it to know for sure, and I don't have 
time to do that right now.  However, it looks like I need to make time. 


      Thanks again.
      Spencer


Khanh Nguyen wrote:
> Is it what you are talking about
>
> https://quantlib.svn.sourceforge.net/svnroot/quantlib/trunk/QuantLib/Examples/FittedBondCurve/FittedBondCurve.cpp
>
> This is also added to RQuantLib a few days ago.
>
> -k
>
>
> On Fri, Jul 24, 2009 at 8:32 AM, spencerg<spencer.graves at prodsyse.com> wrote:
>   
>> Hi, Dirk:
>>
>>     Will Quantlib now support the Nelson / Siegel / Svensson and spline
>> models for the term structure of interest rates, as does the "termstrc"
>> package (and as documented in the companion paper, Ferstl and Hayden 2008
>> "Zero Coupon Yield Curve Estimation with the Package termstrc"
>> http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1307149)?
>>
>>     Thanks,
>>     Spencer
>>
>>     
>
>


From breman.mark at gmail.com  Fri Jul 24 16:41:48 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Fri, 24 Jul 2009 16:41:48 +0200
Subject: [R-SIG-Finance] Read timeseries from csv file with xts or quantmod
Message-ID: <5e6a2e670907240741n151309d1g4cc2a79c7b068fbd@mail.gmail.com>

Hi,
I have a timeseries in a csv file that I want to read into an xts object.

The csv file looks like:

1948-01-01 51,7
1948-02-01 50,2
1948-03-01 43,3
1948-04-01 45,4
1948-05-01 49,5
1948-06-01 53
1948-07-01 48,4
1948-08-01 45,1
1948-09-01 42,1
1948-10-01 47,2
1948-11-01 42,4
1948-12-01 35
1949-01-01 32,9
1949-02-01 31,3
1949-03-01 34,5
1949-04-01 35,5
1949-05-01 32,6
1949-06-01 31,6
etc...

I have attached it to this post.

I tried to read the file with getSymbols() but it gives me an error:
------------------------------------- start trace
----------------------------------------------
> library(quantmod)
> getSymbols('US_PMI_Monthly', src='csv', header = FALSE, sep = "\t",
dec=",")
Error in dimnames(x) <- dn :
  length of 'dimnames' [2] not equal to array extent
>
------------------------------------ end trace
-------------------------------------------------

Strange because read.csv() can read this file into a data.frame:

-------------------------------------- start trace
-----------------------------------------------
> data = read.csv("US_PMI_Monthly.csv", header = FALSE, sep = "\t", dec=",")
> class(data)
[1] "data.frame"
>
-------------------------------------- end trace
-------------------------------------------------

I tried to be smart and convert the data.frame to an xts object, but this
also gives me an error:

------------------------------------- start trace
-------------------------------------------------
> as.xts(data)
Error in as.POSIXlt.character(x, tz, ...) :
  character string is not in a standard unambiguous format
>
------------------------------------- end trace
---------------------------------------------------

I am using R 2.8.1 for windows with xts 0.6-6 and quantmod 0.3-10.

What am I doing wrong here?

Regards,

-Mark-
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090724/c030bac0/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: US_PMI_Monthly.csv
Type: application/octet-stream
Size: 12407 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090724/c030bac0/attachment.obj>

From Achim.Zeileis at wu-wien.ac.at  Fri Jul 24 16:52:30 2009
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 24 Jul 2009 16:52:30 +0200 (CEST)
Subject: [R-SIG-Finance] Read timeseries from csv file with xts or
	quantmod
In-Reply-To: <5e6a2e670907240741n151309d1g4cc2a79c7b068fbd@mail.gmail.com>
References: <5e6a2e670907240741n151309d1g4cc2a79c7b068fbd@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0907241650550.2890@paninaro.stat-math.wu-wien.ac.at>

On Fri, 24 Jul 2009, Mark Breman wrote:

> Hi,
> I have a timeseries in a csv file that I want to read into an xts object.

You can use read.zoo() from the "zoo" package and the as.xts() to coerce 
to an "xts" series.

   library("xts")
   x <- as.xts(read.zoo("US_PMI_Monthly.csv", dec = ","))

Note that you have to set the decimal separator appropriately.

hth,
Z

> The csv file looks like:
>
> 1948-01-01 51,7
> 1948-02-01 50,2
> 1948-03-01 43,3
> 1948-04-01 45,4
> 1948-05-01 49,5
> 1948-06-01 53
> 1948-07-01 48,4
> 1948-08-01 45,1
> 1948-09-01 42,1
> 1948-10-01 47,2
> 1948-11-01 42,4
> 1948-12-01 35
> 1949-01-01 32,9
> 1949-02-01 31,3
> 1949-03-01 34,5
> 1949-04-01 35,5
> 1949-05-01 32,6
> 1949-06-01 31,6
> etc...
>
> I have attached it to this post.
>
> I tried to read the file with getSymbols() but it gives me an error:
> ------------------------------------- start trace
> ----------------------------------------------
>> library(quantmod)
>> getSymbols('US_PMI_Monthly', src='csv', header = FALSE, sep = "\t",
> dec=",")
> Error in dimnames(x) <- dn :
>  length of 'dimnames' [2] not equal to array extent
>>
> ------------------------------------ end trace
> -------------------------------------------------
>
> Strange because read.csv() can read this file into a data.frame:
>
> -------------------------------------- start trace
> -----------------------------------------------
>> data = read.csv("US_PMI_Monthly.csv", header = FALSE, sep = "\t", dec=",")
>> class(data)
> [1] "data.frame"
>>
> -------------------------------------- end trace
> -------------------------------------------------
>
> I tried to be smart and convert the data.frame to an xts object, but this
> also gives me an error:
>
> ------------------------------------- start trace
> -------------------------------------------------
>> as.xts(data)
> Error in as.POSIXlt.character(x, tz, ...) :
>  character string is not in a standard unambiguous format
>>
> ------------------------------------- end trace
> ---------------------------------------------------
>
> I am using R 2.8.1 for windows with xts 0.6-6 and quantmod 0.3-10.
>
> What am I doing wrong here?
>
> Regards,
>
> -Mark-
>


From jeff.a.ryan at gmail.com  Fri Jul 24 16:56:23 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 24 Jul 2009 09:56:23 -0500
Subject: [R-SIG-Finance] Read timeseries from csv file with xts or
	quantmod
In-Reply-To: <5e6a2e670907240741n151309d1g4cc2a79c7b068fbd@mail.gmail.com>
References: <5e6a2e670907240741n151309d1g4cc2a79c7b068fbd@mail.gmail.com>
Message-ID: <e8e755250907240756s63b1bc4cp99dc0ceefdcc1edc@mail.gmail.com>

Doing nothing wrong, getSymbols.csv isn't general enough to handle this.

You can either read in with read.zoo (best option), and then convert
to xts if needed, or write/modify your own getSymbols.csv code to read
the correct columns.

Essentially getSymbols.csv is a simple function to illustrate the
generalization facility of getSymbols.  The idea is to write
getSymbols.XXXX calls that map to your specific data.

That said, a univariate case is sufficiently common to have
getSymbols.csv be smart enough to handle.  I will make the changes to
do this.

Thanks,

Jeff

On Fri, Jul 24, 2009 at 9:41 AM, Mark Breman<breman.mark at gmail.com> wrote:
> Hi,
> I have a timeseries in a csv file that I want to read into an xts object.
> The csv file looks like:
> 1948-01-01 51,7
> 1948-02-01 50,2
> 1948-03-01 43,3
> 1948-04-01 45,4
> 1948-05-01 49,5
> 1948-06-01 53
> 1948-07-01 48,4
> 1948-08-01 45,1
> 1948-09-01 42,1
> 1948-10-01 47,2
> 1948-11-01 42,4
> 1948-12-01 35
> 1949-01-01 32,9
> 1949-02-01 31,3
> 1949-03-01 34,5
> 1949-04-01 35,5
> 1949-05-01 32,6
> 1949-06-01 31,6
> etc...
> I have attached it to this post.
> I tried to read the file with getSymbols() but it gives me an error:
> ------------------------------------- start trace
> ----------------------------------------------
>> library(quantmod)
>> getSymbols('US_PMI_Monthly', src='csv', header = FALSE, sep = "\t",
>> dec=",")
> Error in dimnames(x) <- dn :
> ??length of 'dimnames' [2] not equal to array extent
>>
> ------------------------------------ end trace
> -------------------------------------------------
> Strange because read.csv() can read this file into a data.frame:
> -------------------------------------- start trace
> -----------------------------------------------
>> data = read.csv("US_PMI_Monthly.csv", header = FALSE, sep = "\t", dec=",")
>> class(data)
> [1] "data.frame"
>>
> -------------------------------------- end trace
> -------------------------------------------------
> I tried to be smart and convert the data.frame to an xts object, but this
> also gives me an error:
> ------------------------------------- start trace
> -------------------------------------------------
>> as.xts(data)
> Error in as.POSIXlt.character(x, tz, ...) :
> ??character string is not in a standard unambiguous format
>>
> ------------------------------------- end trace
> ---------------------------------------------------
> I am using R 2.8.1 for windows with xts 0.6-6 and quantmod 0.3-10.
> What am I doing wrong here?
> Regards,
> -Mark-
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From breman.mark at gmail.com  Fri Jul 24 17:00:59 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Fri, 24 Jul 2009 17:00:59 +0200
Subject: [R-SIG-Finance] Read timeseries from csv file with xts or
	quantmod
In-Reply-To: <Pine.LNX.4.64.0907241650550.2890@paninaro.stat-math.wu-wien.ac.at>
References: <5e6a2e670907240741n151309d1g4cc2a79c7b068fbd@mail.gmail.com>
	<Pine.LNX.4.64.0907241650550.2890@paninaro.stat-math.wu-wien.ac.at>
Message-ID: <5e6a2e670907240800he5e9af6i347acfcb07c99866@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090724/89f970b9/attachment.pl>

From breman.mark at gmail.com  Fri Jul 24 17:07:14 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Fri, 24 Jul 2009 17:07:14 +0200
Subject: [R-SIG-Finance] Read timeseries from csv file with xts or
	quantmod
In-Reply-To: <e8e755250907240756s63b1bc4cp99dc0ceefdcc1edc@mail.gmail.com>
References: <5e6a2e670907240741n151309d1g4cc2a79c7b068fbd@mail.gmail.com>
	<e8e755250907240756s63b1bc4cp99dc0ceefdcc1edc@mail.gmail.com>
Message-ID: <5e6a2e670907240807o2a1f654dr9d10a3aa2f51fb23@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090724/0c6e3e1e/attachment.pl>

From ggrothendieck at gmail.com  Fri Jul 24 17:07:44 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 24 Jul 2009 11:07:44 -0400
Subject: [R-SIG-Finance] Read timeseries from csv file with xts or
	quantmod
In-Reply-To: <5e6a2e670907240741n151309d1g4cc2a79c7b068fbd@mail.gmail.com>
References: <5e6a2e670907240741n151309d1g4cc2a79c7b068fbd@mail.gmail.com>
Message-ID: <971536df0907240807j65f9f4adw24a27ecd1c31106a@mail.gmail.com>

Try this:

Lines <- "1948-01-01 51,7
1948-02-01 50,2
1948-03-01 43,3
1948-04-01 45,4
1948-05-01 49,5
1948-06-01 53
1948-07-01 48,4
1948-08-01 45,1
1948-09-01 42,1
1948-10-01 47,2
1948-11-01 42,4
1948-12-01 35
1949-01-01 32,9
1949-02-01 31,3
1949-03-01 34,5
1949-04-01 35,5
1949-05-01 32,6
1949-06-01 31,6"
library(xts)
# z <- read.zoo("myfile.txt", dec = ",")
z <- read.zoo(textConnection(Lines), dec = ",")
x <- as.xts(z)


On Fri, Jul 24, 2009 at 10:41 AM, Mark Breman<breman.mark at gmail.com> wrote:
> Hi,
> I have a timeseries in a csv file that I want to read into an xts object.
> The csv file looks like:
> 1948-01-01 51,7
> 1948-02-01 50,2
> 1948-03-01 43,3
> 1948-04-01 45,4
> 1948-05-01 49,5
> 1948-06-01 53
> 1948-07-01 48,4
> 1948-08-01 45,1
> 1948-09-01 42,1
> 1948-10-01 47,2
> 1948-11-01 42,4
> 1948-12-01 35
> 1949-01-01 32,9
> 1949-02-01 31,3
> 1949-03-01 34,5
> 1949-04-01 35,5
> 1949-05-01 32,6
> 1949-06-01 31,6
> etc...
> I have attached it to this post.
> I tried to read the file with getSymbols() but it gives me an error:
> ------------------------------------- start trace
> ----------------------------------------------
>> library(quantmod)
>> getSymbols('US_PMI_Monthly', src='csv', header = FALSE, sep = "\t",
>> dec=",")
> Error in dimnames(x) <- dn :
> ??length of 'dimnames' [2] not equal to array extent
>>
> ------------------------------------ end trace
> -------------------------------------------------
> Strange because read.csv() can read this file into a data.frame:
> -------------------------------------- start trace
> -----------------------------------------------
>> data = read.csv("US_PMI_Monthly.csv", header = FALSE, sep = "\t", dec=",")
>> class(data)
> [1] "data.frame"
>>
> -------------------------------------- end trace
> -------------------------------------------------
> I tried to be smart and convert the data.frame to an xts object, but this
> also gives me an error:
> ------------------------------------- start trace
> -------------------------------------------------
>> as.xts(data)
> Error in as.POSIXlt.character(x, tz, ...) :
> ??character string is not in a standard unambiguous format
>>
> ------------------------------------- end trace
> ---------------------------------------------------
> I am using R 2.8.1 for windows with xts 0.6-6 and quantmod 0.3-10.
> What am I doing wrong here?
> Regards,
> -Mark-
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From josh.m.ulrich at gmail.com  Fri Jul 24 17:36:05 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Fri, 24 Jul 2009 10:36:05 -0500
Subject: [R-SIG-Finance] Read timeseries from csv file with xts or
	quantmod
In-Reply-To: <5e6a2e670907240807o2a1f654dr9d10a3aa2f51fb23@mail.gmail.com>
References: <5e6a2e670907240741n151309d1g4cc2a79c7b068fbd@mail.gmail.com> 
	<e8e755250907240756s63b1bc4cp99dc0ceefdcc1edc@mail.gmail.com> 
	<5e6a2e670907240807o2a1f654dr9d10a3aa2f51fb23@mail.gmail.com>
Message-ID: <8cca69990907240836n3a90d852s1949353e7870deae@mail.gmail.com>

Mark,

The as.xts(data.frame) case doesn't work because your data.frame does
not have rownames.  as.xts() converts the rownames into the xts index.
 If you read in your data with the following command, you can then
convert the object to xts.

> data <- read.table("US_PMI_Monthly.csv", sep="\t", dec=",", row.names=1)
> head(as.xts(data))
             V2
1948-01-01 51.7
1948-02-01 50.2
1948-03-01 43.3
1948-04-01 45.4
1948-05-01 49.5
1948-06-01 53.0

I would also note that your data, though in a CSV file, is not in CSV
format.  This may have unforeseen consequences when using commands
written for CSV format files (getSymbols.csv, read.csv, etc.).

HTH,
Josh
--
http://www.fosstrading.com



On Fri, Jul 24, 2009 at 10:07 AM, Mark Breman<breman.mark at gmail.com> wrote:
> Hi Jeff,
> Thanks for explaining it to me.
>
> How about the as.xts(data) call where data is a data.frame type. Is it not
> supposed to work this way?
>
> Regards,
>
> -Mark-
>
> 2009/7/24 Jeff Ryan <jeff.a.ryan at gmail.com>
>
>> Doing nothing wrong, getSymbols.csv isn't general enough to handle this.
>>
>> You can either read in with read.zoo (best option), and then convert
>> to xts if needed, or write/modify your own getSymbols.csv code to read
>> the correct columns.
>>
>> Essentially getSymbols.csv is a simple function to illustrate the
>> generalization facility of getSymbols. ?The idea is to write
>> getSymbols.XXXX calls that map to your specific data.
>>
>> That said, a univariate case is sufficiently common to have
>> getSymbols.csv be smart enough to handle. ?I will make the changes to
>> do this.
>>
>> Thanks,
>>
>> Jeff
>>
>> On Fri, Jul 24, 2009 at 9:41 AM, Mark Breman<breman.mark at gmail.com> wrote:
>> > Hi,
>> > I have a timeseries in a csv file that I want to read into an xts object.
>> > The csv file looks like:
>> > 1948-01-01 51,7
>> > 1948-02-01 50,2
>> > 1948-03-01 43,3
>> > 1948-04-01 45,4
>> > 1948-05-01 49,5
>> > 1948-06-01 53
>> > 1948-07-01 48,4
>> > 1948-08-01 45,1
>> > 1948-09-01 42,1
>> > 1948-10-01 47,2
>> > 1948-11-01 42,4
>> > 1948-12-01 35
>> > 1949-01-01 32,9
>> > 1949-02-01 31,3
>> > 1949-03-01 34,5
>> > 1949-04-01 35,5
>> > 1949-05-01 32,6
>> > 1949-06-01 31,6
>> > etc...
>> > I have attached it to this post.
>> > I tried to read the file with getSymbols() but it gives me an error:
>> > ------------------------------------- start trace
>> > ----------------------------------------------
>> >> library(quantmod)
>> >> getSymbols('US_PMI_Monthly', src='csv', header = FALSE, sep = "\t",
>> >> dec=",")
>> > Error in dimnames(x) <- dn :
>> > ? length of 'dimnames' [2] not equal to array extent
>> >>
>> > ------------------------------------ end trace
>> > -------------------------------------------------
>> > Strange because read.csv() can read this file into a data.frame:
>> > -------------------------------------- start trace
>> > -----------------------------------------------
>> >> data = read.csv("US_PMI_Monthly.csv", header = FALSE, sep = "\t",
>> dec=",")
>> >> class(data)
>> > [1] "data.frame"
>> >>
>> > -------------------------------------- end trace
>> > -------------------------------------------------
>> > I tried to be smart and convert the data.frame to an xts object, but this
>> > also gives me an error:
>> > ------------------------------------- start trace
>> > -------------------------------------------------
>> >> as.xts(data)
>> > Error in as.POSIXlt.character(x, tz, ...) :
>> > ? character string is not in a standard unambiguous format
>> >>
>> > ------------------------------------- end trace
>> > ---------------------------------------------------
>> > I am using R 2.8.1 for windows with xts 0.6-6 and quantmod 0.3-10.
>> > What am I doing wrong here?
>> > Regards,
>> > -Mark-
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From atp at piskorski.com  Fri Jul 24 18:40:54 2009
From: atp at piskorski.com (Andrew Piskorski)
Date: Fri, 24 Jul 2009 12:40:54 -0400
Subject: [R-SIG-Finance] For pricing Bond Library ?
In-Reply-To: <28775.63485.qm@web110101.mail.gq1.yahoo.com>
References: <28775.63485.qm@web110101.mail.gq1.yahoo.com>
Message-ID: <20090724164053.GA16936@piskorski.com>

On Thu, Jul 23, 2009 at 08:57:34PM -0700, Josh C. Chien wrote:
> Hi R-Finance users, 
> Does anyone know what library is for pricing bond or fixed income products ?
> Thanks a lot.

I'm not sure exactly what you're looking for, but you could try the
TIPS Inc. "Standard Securities Calculations Software Library":

  http://www.tipsinc.com/prods.htm

Back in 2000, I used the C version (on Solaris, libTIPSCL.so) for
various standard bond calculations.  I don't know what the software
costs, but it worked fine, and the TIPS folks were helpful and easy to
work with, on the rare occasions when just reading their docs wasn't
enough.

-- 
Andrew Piskorski <atp at piskorski.com>
http://www.piskorski.com/


From breman.mark at gmail.com  Fri Jul 24 19:11:55 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Fri, 24 Jul 2009 19:11:55 +0200
Subject: [R-SIG-Finance] Read timeseries from csv file with xts or
	quantmod
In-Reply-To: <8cca69990907240836n3a90d852s1949353e7870deae@mail.gmail.com>
References: <5e6a2e670907240741n151309d1g4cc2a79c7b068fbd@mail.gmail.com>
	<e8e755250907240756s63b1bc4cp99dc0ceefdcc1edc@mail.gmail.com>
	<5e6a2e670907240807o2a1f654dr9d10a3aa2f51fb23@mail.gmail.com>
	<8cca69990907240836n3a90d852s1949353e7870deae@mail.gmail.com>
Message-ID: <5e6a2e670907241011g63e6a4ffp8d34ba3198b55622@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090724/cf6bdd9a/attachment.pl>

From comtech.usa at gmail.com  Sat Jul 25 05:44:25 2009
From: comtech.usa at gmail.com (Michael)
Date: Fri, 24 Jul 2009 20:44:25 -0700
Subject: [R-SIG-Finance] how to smooth timeseries without the lagging?
Message-ID: <b1f16d9d0907242044m48a1cbb5n77279e9ad1040608@mail.gmail.com>

Hi all,

If I use a moving average, it will smooth the choppy time series, but
it will lead to lagging...

How do I smooth timeseries without the lagging effect?

Thanks!


From rechtsteiner at bgki.net  Sat Jul 25 11:12:11 2009
From: rechtsteiner at bgki.net (Josuah Rechtsteiner)
Date: Sat, 25 Jul 2009 11:12:11 +0200
Subject: [R-SIG-Finance] how to smooth timeseries without the lagging?
In-Reply-To: <b1f16d9d0907242044m48a1cbb5n77279e9ad1040608@mail.gmail.com>
References: <b1f16d9d0907242044m48a1cbb5n77279e9ad1040608@mail.gmail.com>
Message-ID: <FDD44CBB-52A9-4390-A996-065582928CF4@bgki.net>

maybe kalman filter is what you are looking for.



Am 25.07.2009 um 05:44 schrieb Michael:

> Hi all,
>
> If I use a moving average, it will smooth the choppy time series, but
> it will lead to lagging...
>
> How do I smooth timeseries without the lagging effect?
>
> Thanks!
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From seancarmody at gmail.com  Sat Jul 25 11:56:59 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Sat, 25 Jul 2009 19:56:59 +1000
Subject: [R-SIG-Finance] how to smooth timeseries without the lagging?
In-Reply-To: <FDD44CBB-52A9-4390-A996-065582928CF4@bgki.net>
References: <b1f16d9d0907242044m48a1cbb5n77279e9ad1040608@mail.gmail.com>
	<FDD44CBB-52A9-4390-A996-065582928CF4@bgki.net>
Message-ID: <ce6bbb9d0907250256t4dd39c7fw38ea1311752b4352@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090725/c484d37c/attachment.pl>

From michaellibeijing at gmail.com  Sat Jul 25 14:59:08 2009
From: michaellibeijing at gmail.com (michael li)
Date: Sat, 25 Jul 2009 20:59:08 +0800
Subject: [R-SIG-Finance] xts() speed on data with date index
Message-ID: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com>

I like xts because  subsetting is easier.  Something like x["2009-08"].
But it seems that xts() is a little slower than zoo() when converting
data with date index instead of time index.

> pr<-matrix(rnorm(20000),nrow=5000,ncol=4)
> t<-Sys.time()+(1:5000)*24*3600
> d<-as.Date(t)
> system.time(x<-zoo(pr,t))
   user  system elapsed
      0       0       0
> system.time(x<-zoo(pr,d))
   user  system elapsed
      0       0       0
> system.time(x<-xts(pr,t))
   user  system elapsed
      0       0       0
> system.time(x<-xts(pr,d))
   user  system elapsed
   0.34    0.00    0.35

Regards,
Michael


From ggrothendieck at gmail.com  Sat Jul 25 15:19:40 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 25 Jul 2009 09:19:40 -0400
Subject: [R-SIG-Finance] xts() speed on data with date index
In-Reply-To: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com>
References: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com>
Message-ID: <971536df0907250619s441dd025o644118fdec2e9ad7@mail.gmail.com>

The explanation for the time difference is likely that xts
converts all time indexes to POSIXct internally whereas
zoo works with them in their original form.   Thus the
time difference would be expected to be the conversion time.

Once its converted then xts would likely be faster for those
xts operatoins where the xts code is in C.  In contrast zoo
is 100% R although eventually it is anticipated that portions
of zoo will be in C too.

As xts in a subclass of zoo some of its operations are done
in zoo so in that case there would not be much of a difference
in speed or if xts handles the corresponding operation itself
but in R then it would not be expected that there would be
much difference either.

On Sat, Jul 25, 2009 at 8:59 AM, michael li<michaellibeijing at gmail.com> wrote:
> I like xts because ?subsetting is easier. ?Something like x["2009-08"].
> But it seems that xts() is a little slower than zoo() when converting
> data with date index instead of time index.
>
>> pr<-matrix(rnorm(20000),nrow=5000,ncol=4)
>> t<-Sys.time()+(1:5000)*24*3600
>> d<-as.Date(t)
>> system.time(x<-zoo(pr,t))
> ? user ?system elapsed
> ? ? ?0 ? ? ? 0 ? ? ? 0
>> system.time(x<-zoo(pr,d))
> ? user ?system elapsed
> ? ? ?0 ? ? ? 0 ? ? ? 0
>> system.time(x<-xts(pr,t))
> ? user ?system elapsed
> ? ? ?0 ? ? ? 0 ? ? ? 0
>> system.time(x<-xts(pr,d))
> ? user ?system elapsed
> ? 0.34 ? ?0.00 ? ?0.35
>
> Regards,
> Michael
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Sat Jul 25 16:00:31 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sat, 25 Jul 2009 09:00:31 -0500
Subject: [R-SIG-Finance] xts() speed on data with date index
In-Reply-To: <971536df0907250619s441dd025o644118fdec2e9ad7@mail.gmail.com>
References: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com>
	<971536df0907250619s441dd025o644118fdec2e9ad7@mail.gmail.com>
Message-ID: <e8e755250907250700w47d4e7d7waaf66bf17f277c5@mail.gmail.com>

Gabor is correct.

In addition the operation you are looking at is the creation of an xts
object, which is usually happening once during typical use (or at
least far fewer times than other operations).

xts stores all times internally as POSIXct, for internal
efficiency/consistency purposes. The original time class is preserved
in an internal attribute that is used in calls where it makes sense to
convert for the user:

(1)index():
where you are extracting the 'original' index.  Transparently the
POSIXct is converted back to the time class you expect.  This carries
overhead, but again should be only occasionally used.  Direct access
to the index is via .index()

(2)print() or printing by default.
Conversion cost is irrelevant here, as it is assumed you will spend
far more time looking at the output than any cost to convert.

The logic is that everything is 'time', just with slightly different
internal representations.  The user shouldn't have to care about time
choice, just as the user should have to care about big-endian or
little-endian hardware storage.

There are some technical issues that are still not perfect of course.
Timezone handling can get tricky if you do a lot of strange subsetting
with *different* index classes.  This type of handling is entirely
impossible in most other time series classes, so the 'weakness' isn't
really valid, as it is still better than most, but it isn't perfect
... yet.

HTH
Jeff


On Sat, Jul 25, 2009 at 8:19 AM, Gabor
Grothendieck<ggrothendieck at gmail.com> wrote:
> The explanation for the time difference is likely that xts
> converts all time indexes to POSIXct internally whereas
> zoo works with them in their original form. ? Thus the
> time difference would be expected to be the conversion time.
>
> Once its converted then xts would likely be faster for those
> xts operatoins where the xts code is in C. ?In contrast zoo
> is 100% R although eventually it is anticipated that portions
> of zoo will be in C too.
>
> As xts in a subclass of zoo some of its operations are done
> in zoo so in that case there would not be much of a difference
> in speed or if xts handles the corresponding operation itself
> but in R then it would not be expected that there would be
> much difference either.
>
> On Sat, Jul 25, 2009 at 8:59 AM, michael li<michaellibeijing at gmail.com> wrote:
>> I like xts because ?subsetting is easier. ?Something like x["2009-08"].
>> But it seems that xts() is a little slower than zoo() when converting
>> data with date index instead of time index.
>>
>>> pr<-matrix(rnorm(20000),nrow=5000,ncol=4)
>>> t<-Sys.time()+(1:5000)*24*3600
>>> d<-as.Date(t)
>>> system.time(x<-zoo(pr,t))
>> ? user ?system elapsed
>> ? ? ?0 ? ? ? 0 ? ? ? 0
>>> system.time(x<-zoo(pr,d))
>> ? user ?system elapsed
>> ? ? ?0 ? ? ? 0 ? ? ? 0
>>> system.time(x<-xts(pr,t))
>> ? user ?system elapsed
>> ? ? ?0 ? ? ? 0 ? ? ? 0
>>> system.time(x<-xts(pr,d))
>> ? user ?system elapsed
>> ? 0.34 ? ?0.00 ? ?0.35
>>
>> Regards,
>> Michael
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From michael.sankowski at gmail.com  Sat Jul 25 16:28:30 2009
From: michael.sankowski at gmail.com (michael.sankowski at gmail.com)
Date: Sat, 25 Jul 2009 14:28:30 +0000
Subject: [R-SIG-Finance] how to smooth timeseries without the lagging?
In-Reply-To: <ce6bbb9d0907250256t4dd39c7fw38ea1311752b4352@mail.gmail.com>
References: <b1f16d9d0907242044m48a1cbb5n77279e9ad1040608@mail.gmail.com><FDD44CBB-52A9-4390-A996-065582928CF4@bgki.net><ce6bbb9d0907250256t4dd39c7fw38ea1311752b4352@mail.gmail.com>
Message-ID: <1344858837-1248532072-cardhu_decombobulator_blackberry.rim.net-1027381999-@bxe1231.bisx.prod.on.blackberry>

Also try MESA. I was suppposed to do this for josh but have been on other projects. 
Sent via BlackBerry from T-Mobile

-----Original Message-----
From: Sean Carmody <seancarmody at gmail.com>

Date: Sat, 25 Jul 2009 19:56:59 
To: Josuah Rechtsteiner<rechtsteiner at bgki.net>
Cc: <r-sig-finance at stat.math.ethz.ch>
Subject: Re: [R-SIG-Finance] how to smooth timeseries without the lagging?


Or a Henderson filter. If you have a filter that looks forward as well as
back, it will not have the lag effect. Then it'll need special treatment at
the end of the series.

(Sorry for the double email Josuah).

Sean.

On Sat, Jul 25, 2009 at 7:12 PM, Josuah Rechtsteiner
<rechtsteiner at bgki.net>wrote:

> maybe kalman filter is what you are looking for.
>
>
>
> Am 25.07.2009 um 05:44 schrieb Michael:
>
>
>  Hi all,
>>
>> If I use a moving average, it will smooth the choppy time series, but
>> it will lead to lagging...
>>
>> How do I smooth timeseries without the lagging effect?
>>
>> Thanks!
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Sean Carmody

The Stubborn Mule
http://www.stubbornmule.net
http://twitter.com/seancarmody

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

From jeff.a.ryan at gmail.com  Sat Jul 25 16:46:43 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sat, 25 Jul 2009 09:46:43 -0500
Subject: [R-SIG-Finance] xts() speed on data with date index
In-Reply-To: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com>
References: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com>
Message-ID: <e8e755250907250746p2dc673cfn129b46c755af453b@mail.gmail.com>

An example to clarify/confuse ;)

> system.time(px<-xts(pr,t))
   user  system elapsed
  0.001   0.001   0.002
> system.time(dx<-xts(pr,d))
   user  system elapsed
  0.164   0.004   0.168
> indexClass(dx)
[1] "Date"
> indexClass(px)
[1] "POSIXt"  "POSIXct"

##  in POSIXct time as expected
> head(px)
                          [,1]       [,2]       [,3]       [,4]
2009-07-26 09:39:32  0.6739000  0.6124116 -0.5820407  0.1022264
2009-07-27 09:39:32 -0.7806308 -0.4988508 -0.5448239 -0.5649892
2009-07-28 09:39:32  1.0275375 -0.8538655 -0.6098992  1.9661175
2009-07-29 09:39:32  0.6268532 -0.3144514  1.0142005 -0.2303953
2009-07-30 09:39:32 -3.8062200 -0.3875341 -0.9251609 -0.4677076
2009-07-31 09:39:32 -1.1192617 -0.6575085 -0.9918533 -0.8743504

> head(dx)
                 [,1]       [,2]       [,3]       [,4]
2009-07-26  0.6739000  0.6124116 -0.5820407  0.1022264
2009-07-27 -0.7806308 -0.4988508 -0.5448239 -0.5649892
2009-07-28  1.0275375 -0.8538655 -0.6098992  1.9661175
2009-07-29  0.6268532 -0.3144514  1.0142005 -0.2303953
2009-07-30 -3.8062200 -0.3875341 -0.9251609 -0.4677076
2009-07-31 -1.1192617 -0.6575085 -0.9918533 -0.8743504
> indexClass(px) <- "Date"

### simply convert the indexClass -- this is essentially costless,
### and now you have an xts object 'indexed' by Date
> head(px)
                 [,1]       [,2]       [,3]       [,4]
2009-07-26  0.6739000  0.6124116 -0.5820407  0.1022264
2009-07-27 -0.7806308 -0.4988508 -0.5448239 -0.5649892
2009-07-28  1.0275375 -0.8538655 -0.6098992  1.9661175
2009-07-29  0.6268532 -0.3144514  1.0142005 -0.2303953
2009-07-30 -3.8062200 -0.3875341 -0.9251609 -0.4677076
2009-07-31 -1.1192617 -0.6575085 -0.9918533 -0.8743504
> system.time(indexClass(px) <- "Date")
   user  system elapsed
  0.001   0.001   0.001

Now convert back, and see if something like merge cares about the
index class... (hint: it doesn't)

> system.time(indexClass(px) <- c("POSIXt","POSIXct"))
   user  system elapsed
  0.001   0.000   0.000
> indexClass(px)
[1] "POSIXt"  "POSIXct"
> head(px)
                          [,1]       [,2]       [,3]       [,4]
2009-07-26 09:39:32  0.6739000  0.6124116 -0.5820407  0.1022264
2009-07-27 09:39:32 -0.7806308 -0.4988508 -0.5448239 -0.5649892
2009-07-28 09:39:32  1.0275375 -0.8538655 -0.6098992  1.9661175
2009-07-29 09:39:32  0.6268532 -0.3144514  1.0142005 -0.2303953
2009-07-30 09:39:32 -3.8062200 -0.3875341 -0.9251609 -0.4677076
2009-07-31 09:39:32 -1.1192617 -0.6575085 -0.9918533 -0.8743504
> head(dx)
                 [,1]       [,2]       [,3]       [,4]
2009-07-26  0.6739000  0.6124116 -0.5820407  0.1022264
2009-07-27 -0.7806308 -0.4988508 -0.5448239 -0.5649892
2009-07-28  1.0275375 -0.8538655 -0.6098992  1.9661175
2009-07-29  0.6268532 -0.3144514  1.0142005 -0.2303953
2009-07-30 -3.8062200 -0.3875341 -0.9251609 -0.4677076
2009-07-31 -1.1192617 -0.6575085 -0.9918533 -0.8743504
> system.time(merge(dx,dx))
   user  system elapsed
  0.001   0.000   0.002
> system.time(merge(px,px))
   user  system elapsed
  0.001   0.000   0.002

### Caveat: converting back and forth is a bit silly, and possibly
perilous, but it is
### illustrative of what you can do with xts

HTHsomewhat

Jeff

On Sat, Jul 25, 2009 at 7:59 AM, michael li<michaellibeijing at gmail.com> wrote:
> I like xts because ?subsetting is easier. ?Something like x["2009-08"].
> But it seems that xts() is a little slower than zoo() when converting
> data with date index instead of time index.
>
>> pr<-matrix(rnorm(20000),nrow=5000,ncol=4)
>> t<-Sys.time()+(1:5000)*24*3600
>> d<-as.Date(t)
>> system.time(x<-zoo(pr,t))
> ? user ?system elapsed
> ? ? ?0 ? ? ? 0 ? ? ? 0
>> system.time(x<-zoo(pr,d))
> ? user ?system elapsed
> ? ? ?0 ? ? ? 0 ? ? ? 0
>> system.time(x<-xts(pr,t))
> ? user ?system elapsed
> ? ? ?0 ? ? ? 0 ? ? ? 0
>> system.time(x<-xts(pr,d))
> ? user ?system elapsed
> ? 0.34 ? ?0.00 ? ?0.35
>
> Regards,
> Michael
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From ezivot at u.washington.edu  Sat Jul 25 17:31:04 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Sat, 25 Jul 2009 08:31:04 -0700 (PDT)
Subject: [R-SIG-Finance] how to smooth timeseries without the lagging?
In-Reply-To: <1344858837-1248532072-cardhu_decombobulator_blackberry.rim.net-1027381999-@bxe1231.bisx.prod.on.blackberry>
Message-ID: <Pine.LNX.4.43.0907250831040.1294@hymn14.u.washington.edu>

"smoothing" is not a well defined term. In the Kalman filter literature, smoothing refers to state extraction at time t using all available information (before and after t) and typically results in two-sided moving average type algorithms. The whole point of smoothing is to reduce the noise in the data and extract a "smooth" component. This invariably involves some kind of averaging of the observation both before and after the observation. So it doesn't make sense to have a "smoother" that does not involve some lagging effect. Otherwise, you couldn't smooth. See


Harvey, A. C. and Koopman, S. J. (2000). Signal extraction and the formulation of unobserved components
models. Econometrics Journal, Vol. 3, pp. 84-107.

for a nice discussion.

****************************************************************
*  Eric Zivot                  			               *
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Adjunct Professor of Finance                                *
*  Adjunct Professor of Statistics
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *                                                           *
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Sat, 25 Jul 2009 michael.sankowski at gmail.com wrote:

> Also try MESA. I was suppposed to do this for josh but have been on other projects.
> Sent via BlackBerry from T-Mobile
>
> -----Original Message-----
> From: Sean Carmody <seancarmody at gmail.com>
>
> Date: Sat, 25 Jul 2009 19:56:59
> To: Josuah Rechtsteiner<rechtsteiner at bgki.net>
> Cc: <r-sig-finance at stat.math.ethz.ch>
> Subject: Re: [R-SIG-Finance] how to smooth timeseries without the lagging?
>
>
> Or a Henderson filter. If you have a filter that looks forward as well as
> back, it will not have the lag effect. Then it'll need special treatment at
> the end of the series.
>
> (Sorry for the double email Josuah).
>
> Sean.
>
> On Sat, Jul 25, 2009 at 7:12 PM, Josuah Rechtsteiner
> <rechtsteiner at bgki.net>wrote:
>
>> maybe kalman filter is what you are looking for.
>>
>>
>>
>> Am 25.07.2009 um 05:44 schrieb Michael:
>>
>>
>>  Hi all,
>>>
>>> If I use a moving average, it will smooth the choppy time series, but
>>> it will lead to lagging...
>>>
>>> How do I smooth timeseries without the lagging effect?
>>>
>>> Thanks!
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Sean Carmody
>
> The Stubborn Mule
> http://www.stubbornmule.net
> http://twitter.com/seancarmody
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From michaellibeijing at gmail.com  Sun Jul 26 03:38:09 2009
From: michaellibeijing at gmail.com (Michael)
Date: Sun, 26 Jul 2009 09:38:09 +0800
Subject: [R-SIG-Finance] xts() speed on data with date index
In-Reply-To: <e8e755250907250746p2dc673cfn129b46c755af453b@mail.gmail.com>
References: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com>
	<e8e755250907250746p2dc673cfn129b46c755af453b@mail.gmail.com>
Message-ID: <d718c8210907251838j56568a98j8c0fc7c081a9509f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090726/eb3fe18c/attachment.pl>

From ggrothendieck at gmail.com  Sun Jul 26 04:09:54 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 25 Jul 2009 22:09:54 -0400
Subject: [R-SIG-Finance] xts() speed on data with date index
In-Reply-To: <d718c8210907251838j56568a98j8c0fc7c081a9509f@mail.gmail.com>
References: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com> 
	<e8e755250907250746p2dc673cfn129b46c755af453b@mail.gmail.com> 
	<d718c8210907251838j56568a98j8c0fc7c081a9509f@mail.gmail.com>
Message-ID: <971536df0907251909l2092dd69q6f15176da21f369c@mail.gmail.com>

The conversion to or from Date occurs when you store it (as you
noticed) or print it out or otherwise try to use it in a context that
requires a Date.  Your example does not do any of those so no
conversion was needed.  Try this:

> system.time(print(x))
   ...
   user  system elapsed
   0.39    0.00    0.39

> system.time(print(t))
  ...
   user  system elapsed
   0.08    0.00    0.11


On Sat, Jul 25, 2009 at 9:38 PM, Michael<michaellibeijing at gmail.com> wrote:
> Thanks for detailed explanation and demo codes.indexClass(x)<-"Date ?is
> great for its speed. ? And easier than ?as.Date(x). ? as.Date(x) ?will
> change the date to UTC for POSIXct automatically no matter whether you like
> it or not. ? It seems indexClass(x)<-"Date will not change the time zone of
> the date, which is exactly what I need.
> The following codes meet my needs exactly with high speed.
>> library(quantmod)
>>
>> pr<-matrix(rnorm(20000),nrow=5000,ncol=4)
>> t<-Sys.time()+(1:5000)*24*3600
>> system.time(x<-xts(pr,t))
> ? user ?system elapsed
> ? ? ?0 ? ? ? 0 ? ? ? 0
>> head(x)
> ? ? ? ? ? ? ? ? ? ? ? ? ?[,1] ? ? ? [,2] ? ? ? ?[,3] ? ? ? ?[,4]
> 2009-07-27 09:35:39 ?0.6735081 -2.0724029 -2.36234114 -0.74839095
> 2009-07-28 09:35:39 -1.7756642 ?0.7624485 -2.15179721 -0.98905882
> 2009-07-29 09:35:39 ?1.2053857 -1.5182344 ?0.12540278 -0.02926681
> 2009-07-30 09:35:39 -0.5301832 ?0.3087448 -0.46227937 -1.03176589
> 2009-07-31 09:35:39 -0.9061220 ?1.2160150 -0.85206307 -1.13925796
> 2009-08-01 09:35:39 -0.2954440 ?0.7048516 ?0.03260057 ?2.04602758
>> system.time(indexClass(x)<- "Date")
> ? user ?system elapsed
> ? ? ?0 ? ? ? 0 ? ? ? 0
>> head(x)
> ? ? ? ? ? ? ? ? [,1] ? ? ? [,2] ? ? ? ?[,3] ? ? ? ?[,4]
> 2009-07-27 ?0.6735081 -2.0724029 -2.36234114 -0.74839095
> 2009-07-28 -1.7756642 ?0.7624485 -2.15179721 -0.98905882
> 2009-07-29 ?1.2053857 -1.5182344 ?0.12540278 -0.02926681
> 2009-07-30 -0.5301832 ?0.3087448 -0.46227937 -1.03176589
> 2009-07-31 -0.9061220 ?1.2160150 -0.85206307 -1.13925796
> 2009-08-01 -0.2954440 ?0.7048516 ?0.03260057 ?2.04602758
>>
>
> Thanks Jeff and Gabor.
>
> Michael
>
>
> On Sat, Jul 25, 2009 at 10:46 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>
>> An example to clarify/confuse ;)
>>
>> > system.time(px<-xts(pr,t))
>> ? user ?system elapsed
>> ?0.001 ? 0.001 ? 0.002
>> > system.time(dx<-xts(pr,d))
>> ? user ?system elapsed
>> ?0.164 ? 0.004 ? 0.168
>> > indexClass(dx)
>> [1] "Date"
>> > indexClass(px)
>> [1] "POSIXt" ?"POSIXct"
>>
>> ## ?in POSIXct time as expected
>> > head(px)
>> ? ? ? ? ? ? ? ? ? ? ? ? ?[,1] ? ? ? [,2] ? ? ? [,3] ? ? ? [,4]
>> 2009-07-26 09:39:32 ?0.6739000 ?0.6124116 -0.5820407 ?0.1022264
>> 2009-07-27 09:39:32 -0.7806308 -0.4988508 -0.5448239 -0.5649892
>> 2009-07-28 09:39:32 ?1.0275375 -0.8538655 -0.6098992 ?1.9661175
>> 2009-07-29 09:39:32 ?0.6268532 -0.3144514 ?1.0142005 -0.2303953
>> 2009-07-30 09:39:32 -3.8062200 -0.3875341 -0.9251609 -0.4677076
>> 2009-07-31 09:39:32 -1.1192617 -0.6575085 -0.9918533 -0.8743504
>>
>> > head(dx)
>> ? ? ? ? ? ? ? ? [,1] ? ? ? [,2] ? ? ? [,3] ? ? ? [,4]
>> 2009-07-26 ?0.6739000 ?0.6124116 -0.5820407 ?0.1022264
>> 2009-07-27 -0.7806308 -0.4988508 -0.5448239 -0.5649892
>> 2009-07-28 ?1.0275375 -0.8538655 -0.6098992 ?1.9661175
>> 2009-07-29 ?0.6268532 -0.3144514 ?1.0142005 -0.2303953
>> 2009-07-30 -3.8062200 -0.3875341 -0.9251609 -0.4677076
>> 2009-07-31 -1.1192617 -0.6575085 -0.9918533 -0.8743504
>> > indexClass(px) <- "Date"
>>
>> ### simply convert the indexClass -- this is essentially costless,
>> ### and now you have an xts object 'indexed' by Date
>> > head(px)
>> ? ? ? ? ? ? ? ? [,1] ? ? ? [,2] ? ? ? [,3] ? ? ? [,4]
>> 2009-07-26 ?0.6739000 ?0.6124116 -0.5820407 ?0.1022264
>> 2009-07-27 -0.7806308 -0.4988508 -0.5448239 -0.5649892
>> 2009-07-28 ?1.0275375 -0.8538655 -0.6098992 ?1.9661175
>> 2009-07-29 ?0.6268532 -0.3144514 ?1.0142005 -0.2303953
>> 2009-07-30 -3.8062200 -0.3875341 -0.9251609 -0.4677076
>> 2009-07-31 -1.1192617 -0.6575085 -0.9918533 -0.8743504
>> > system.time(indexClass(px) <- "Date")
>> ? user ?system elapsed
>> ?0.001 ? 0.001 ? 0.001
>>
>> Now convert back, and see if something like merge cares about the
>> index class... (hint: it doesn't)
>>
>> > system.time(indexClass(px) <- c("POSIXt","POSIXct"))
>> ? user ?system elapsed
>> ?0.001 ? 0.000 ? 0.000
>> > indexClass(px)
>> [1] "POSIXt" ?"POSIXct"
>> > head(px)
>> ? ? ? ? ? ? ? ? ? ? ? ? ?[,1] ? ? ? [,2] ? ? ? [,3] ? ? ? [,4]
>> 2009-07-26 09:39:32 ?0.6739000 ?0.6124116 -0.5820407 ?0.1022264
>> 2009-07-27 09:39:32 -0.7806308 -0.4988508 -0.5448239 -0.5649892
>> 2009-07-28 09:39:32 ?1.0275375 -0.8538655 -0.6098992 ?1.9661175
>> 2009-07-29 09:39:32 ?0.6268532 -0.3144514 ?1.0142005 -0.2303953
>> 2009-07-30 09:39:32 -3.8062200 -0.3875341 -0.9251609 -0.4677076
>> 2009-07-31 09:39:32 -1.1192617 -0.6575085 -0.9918533 -0.8743504
>> > head(dx)
>> ? ? ? ? ? ? ? ? [,1] ? ? ? [,2] ? ? ? [,3] ? ? ? [,4]
>> 2009-07-26 ?0.6739000 ?0.6124116 -0.5820407 ?0.1022264
>> 2009-07-27 -0.7806308 -0.4988508 -0.5448239 -0.5649892
>> 2009-07-28 ?1.0275375 -0.8538655 -0.6098992 ?1.9661175
>> 2009-07-29 ?0.6268532 -0.3144514 ?1.0142005 -0.2303953
>> 2009-07-30 -3.8062200 -0.3875341 -0.9251609 -0.4677076
>> 2009-07-31 -1.1192617 -0.6575085 -0.9918533 -0.8743504
>> > system.time(merge(dx,dx))
>> ? user ?system elapsed
>> ?0.001 ? 0.000 ? 0.002
>> > system.time(merge(px,px))
>> ? user ?system elapsed
>> ?0.001 ? 0.000 ? 0.002
>>
>> ### Caveat: converting back and forth is a bit silly, and possibly
>> perilous, but it is
>> ### illustrative of what you can do with xts
>>
>> HTHsomewhat
>>
>> Jeff
>>
>> On Sat, Jul 25, 2009 at 7:59 AM, michael li<michaellibeijing at gmail.com>
>> wrote:
>> > I like xts because ?subsetting is easier. ?Something like x["2009-08"].
>> > But it seems that xts() is a little slower than zoo() when converting
>> > data with date index instead of time index.
>> >
>> >> pr<-matrix(rnorm(20000),nrow=5000,ncol=4)
>> >> t<-Sys.time()+(1:5000)*24*3600
>> >> d<-as.Date(t)
>> >> system.time(x<-zoo(pr,t))
>> > ? user ?system elapsed
>> > ? ? ?0 ? ? ? 0 ? ? ? 0
>> >> system.time(x<-zoo(pr,d))
>> > ? user ?system elapsed
>> > ? ? ?0 ? ? ? 0 ? ? ? 0
>> >> system.time(x<-xts(pr,t))
>> > ? user ?system elapsed
>> > ? ? ?0 ? ? ? 0 ? ? ? 0
>> >> system.time(x<-xts(pr,d))
>> > ? user ?system elapsed
>> > ? 0.34 ? ?0.00 ? ?0.35
>> >
>> > Regards,
>> > Michael
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From michaellibeijing at gmail.com  Sun Jul 26 05:18:49 2009
From: michaellibeijing at gmail.com (Michael)
Date: Sun, 26 Jul 2009 11:18:49 +0800
Subject: [R-SIG-Finance] xts() speed on data with date index
In-Reply-To: <971536df0907251909l2092dd69q6f15176da21f369c@mail.gmail.com>
References: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com>
	<e8e755250907250746p2dc673cfn129b46c755af453b@mail.gmail.com>
	<d718c8210907251838j56568a98j8c0fc7c081a9509f@mail.gmail.com>
	<971536df0907251909l2092dd69q6f15176da21f369c@mail.gmail.com>
Message-ID: <d718c8210907252018p28388ff8lbe578d381dcfe0c6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090726/0c510444/attachment.pl>

From john.seppanen99 at gmail.com  Sun Jul 26 10:54:23 2009
From: john.seppanen99 at gmail.com (=?ISO-8859-1?Q?John_Sepp=E4nen?=)
Date: Sun, 26 Jul 2009 11:54:23 +0300
Subject: [R-SIG-Finance] Fitting and testing copula-functions
Message-ID: <f5dfae7c0907260154o5ac75121hf0b2a1dc04336093@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090726/a39e2818/attachment.pl>

From michael.sankowski at gmail.com  Sun Jul 26 20:35:17 2009
From: michael.sankowski at gmail.com (michael.sankowski at gmail.com)
Date: Sun, 26 Jul 2009 18:35:17 +0000
Subject: [R-SIG-Finance] how to smooth timeseries without the lagging?
In-Reply-To: <Pine.LNX.4.43.0907250831040.1294@hymn14.u.washington.edu>
References: <1344858837-1248532072-cardhu_decombobulator_blackberry.rim.net-1027381999-@bxe1231.bisx.prod.on.blackberry><Pine.LNX.4.43.0907250831040.1294@hymn14.u.washington.edu>
Message-ID: <957951299-1248633278-cardhu_decombobulator_blackberry.rim.net-1800350618-@bxe1231.bisx.prod.on.blackberry>

What is the application?  Optimal smoothing is related to extracting high signal to noise. If your application is not dependent on constant or perfectly stable parameter then you can look at optimizing signal to noise rather than choosing unchanging smoothing. 

I typically use MESA to extract cycle information when dealing with a changing cyclical component.   I can post some code in a bit. 
Sent via BlackBerry from T-Mobile

-----Original Message-----
From: Eric Zivot <ezivot at u.washington.edu>

Date: Sat, 25 Jul 2009 08:31:04 
To: <michael.sankowski at gmail.com>
Cc: Sean Carmody<seancarmody at gmail.com>; <r-sig-finance-bounces at stat.math.ethz.ch>; Josuah Rechtsteiner<rechtsteiner at bgki.net>; r-sig-finance at stat.math.ethz.ch<r-sig-finance at stat.math.ethz.ch>
Subject: Re: [R-SIG-Finance] how to smooth timeseries without the lagging?


"smoothing" is not a well defined term. In the Kalman filter literature, smoothing refers to state extraction at time t using all available information (before and after t) and typically results in two-sided moving average type algorithms. The whole point of smoothing is to reduce the noise in the data and extract a "smooth" component. This invariably involves some kind of averaging of the observation both before and after the observation. So it doesn't make sense to have a "smoother" that does not involve some lagging effect. Otherwise, you couldn't smooth. See


Harvey, A. C. and Koopman, S. J. (2000). Signal extraction and the formulation of unobserved components
models. Econometrics Journal, Vol. 3, pp. 84-107.

for a nice discussion.

****************************************************************
*  Eric Zivot                  			               *
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Adjunct Professor of Finance                                *
*  Adjunct Professor of Statistics
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *                                                           *
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Sat, 25 Jul 2009 michael.sankowski at gmail.com wrote:

> Also try MESA. I was suppposed to do this for josh but have been on other projects.
> Sent via BlackBerry from T-Mobile
>
> -----Original Message-----
> From: Sean Carmody <seancarmody at gmail.com>
>
> Date: Sat, 25 Jul 2009 19:56:59
> To: Josuah Rechtsteiner<rechtsteiner at bgki.net>
> Cc: <r-sig-finance at stat.math.ethz.ch>
> Subject: Re: [R-SIG-Finance] how to smooth timeseries without the lagging?
>
>
> Or a Henderson filter. If you have a filter that looks forward as well as
> back, it will not have the lag effect. Then it'll need special treatment at
> the end of the series.
>
> (Sorry for the double email Josuah).
>
> Sean.
>
> On Sat, Jul 25, 2009 at 7:12 PM, Josuah Rechtsteiner
> <rechtsteiner at bgki.net>wrote:
>
>> maybe kalman filter is what you are looking for.
>>
>>
>>
>> Am 25.07.2009 um 05:44 schrieb Michael:
>>
>>
>>  Hi all,
>>>
>>> If I use a moving average, it will smooth the choppy time series, but
>>> it will lead to lagging...
>>>
>>> How do I smooth timeseries without the lagging effect?
>>>
>>> Thanks!
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Sean Carmody
>
> The Stubborn Mule
> http://www.stubbornmule.net
> http://twitter.com/seancarmody
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



From matthieu.stigler at gmail.com  Mon Jul 27 10:13:14 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Mon, 27 Jul 2009 10:13:14 +0200
Subject: [R-SIG-Finance] Version 0.7 of package tsDyn, nonlinear time series
In-Reply-To: <4A6C630A.10405@gmail.com>
References: <4A6C630A.10405@gmail.com>
Message-ID: <111060c20907270113g64505f8clc0ed237d7cfd466d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090727/bc180179/attachment.pl>

From GMKeogh at justice.ie  Mon Jul 27 11:59:24 2009
From: GMKeogh at justice.ie (Gerard M. Keogh)
Date: Mon, 27 Jul 2009 10:59:24 +0100
Subject: [R-SIG-Finance] how to smooth timeseries without the lagging?
In-Reply-To: <Pine.LNX.4.43.0907250831040.1294@hymn14.u.washington.edu>
Message-ID: <OF7A2A2982.CF75471F-ON80257600.00342B93-80257600.0036E0C4@justice.ie>

Sorry guys!

This is not a well posed question.
If you intend to "smooth" a time series all methods mentioned (and that I'm
aware of anyway) use some kind of TIME averaging - the best simple method
is probably kernel smoothing (nadaraya-watson smoother). Lowess (available
in R) locally weighted regression is another alternative.
One way or another you'll be doing some kind of "time averaging" - afterall
the smoothed solution is generally a projection of x_t onto the subspace
spanned by the history (e.g. x_(t-1) etc).

As I see it the only way to avoid "time averaging" is to use regression of
y_t on x1, x2, .. - this once again is the projection of y_t onto the
subspace spanned by x1, x2 etc. - but this is still an "average"!
Here of course you need independent predictors and once again these may
have time effects that are correlated with y_t - e.g. y_t may be correlated
with x_1(t-3), x_1 lagged 3 periods. In this situation you'll need dynamic
regression models.
But, nevertheless using regression is the most straightforward option to
avoid time averaging.

Gerard



                                                                           
             Eric Zivot                                                    
             <ezivot at u.washing                                             
             ton.edu>                                                   To 
             Sent by:                  michael.sankowski at gmail.com         
             r-sig-finance-bou                                          cc 
             nces at stat.math.et         "r-sig-finance at stat.math.ethz.ch"   
             hz.ch                     <r-sig-finance at stat.math.ethz.ch>,  
                                       r-sig-finance-bounces at stat.math.eth 
                                       z.ch                                
             25/07/2009 16:31                                      Subject 
                                       Re: [R-SIG-Finance] how to smooth   
                                       timeseries without the lagging?     
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




"smoothing" is not a well defined term. In the Kalman filter literature,
smoothing refers to state extraction at time t using all available
information (before and after t) and typically results in two-sided moving
average type algorithms. The whole point of smoothing is to reduce the
noise in the data and extract a "smooth" component. This invariably
involves some kind of averaging of the observation both before and after
the observation. So it doesn't make sense to have a "smoother" that does
not involve some lagging effect. Otherwise, you couldn't smooth. See


Harvey, A. C. and Koopman, S. J. (2000). Signal extraction and the
formulation of unobserved components
models. Econometrics Journal, Vol. 3, pp. 84-107.

for a nice discussion.

****************************************************************
*  Eric Zivot
*
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Adjunct Professor of Finance                                *
*  Adjunct Professor of Statistics
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *
*
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Sat, 25 Jul 2009 michael.sankowski at gmail.com wrote:

> Also try MESA. I was suppposed to do this for josh but have been on other
projects.
> Sent via BlackBerry from T-Mobile
>
> -----Original Message-----
> From: Sean Carmody <seancarmody at gmail.com>
>
> Date: Sat, 25 Jul 2009 19:56:59
> To: Josuah Rechtsteiner<rechtsteiner at bgki.net>
> Cc: <r-sig-finance at stat.math.ethz.ch>
> Subject: Re: [R-SIG-Finance] how to smooth timeseries without the
lagging?
>
>
> Or a Henderson filter. If you have a filter that looks forward as well as
> back, it will not have the lag effect. Then it'll need special treatment
at
> the end of the series.
>
> (Sorry for the double email Josuah).
>
> Sean.
>
> On Sat, Jul 25, 2009 at 7:12 PM, Josuah Rechtsteiner
> <rechtsteiner at bgki.net>wrote:
>
>> maybe kalman filter is what you are looking for.
>>
>>
>>
>> Am 25.07.2009 um 05:44 schrieb Michael:
>>
>>
>>  Hi all,
>>>
>>> If I use a moving average, it will smooth the choppy time series, but
>>> it will lead to lagging...
>>>
>>> How do I smooth timeseries without the lagging effect?
>>>
>>> Thanks!
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Sean Carmody
>
> The Stubborn Mule
> http://www.stubbornmule.net
> http://twitter.com/seancarmody
>
>            [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.



**********************************************************************************
The information transmitted is intended only for the person or entity to which it is addressed and may contain confidential and/or privileged material. Any review, retransmission, dissemination or other use of, or taking of any action in reliance upon, this information by persons or entities other than the intended recipient is prohibited. If you received this in error, please contact the sender and delete the material from any computer.  It is the policy of the Department of Justice, Equality and Law Reform and the Agencies and Offices using its IT services to disallow the sending of offensive material.
Should you consider that the material contained in this message is offensive you should contact the sender immediately and also mailminder[at]justice.ie.

Is le haghaidh an duine n? an eintitis ar a bhfuil s? d?rithe, agus le haghaidh an duine n? an eintitis sin amh?in, a bhearta?tear an fhaisn?is a tarchuireadh agus f?adfaidh s? go bhfuil ?bhar faoi r?n agus/n? faoi phribhl?id inti. Toirmisctear aon athbhreithni?, atarchur n? leathadh a dh?anamh ar an bhfaisn?is seo, aon ?s?id eile a bhaint aisti n? aon ghn?omh a dh?anamh ar a hiontaoibh, ag daoine n? ag eintitis seachas an faighteoir beartaithe. M? fuair t? ? seo tr? dhearmad, t?igh i dteagmh?il leis an seolt?ir, le do thoil, agus scrios an t-?bhar as aon r?omhaire. Is ? beartas na Roinne Dl? agus Cirt, Comhionannais agus Athch?irithe Dl?, agus na nOif?g? agus na nGn?omhaireachta? a ?s?ideann seirbh?s? TF na Roinne, seoladh ?bhair chol?il a dh?chead?.
M?s rud ? go measann t? gur ?bhar col?il at? san ?bhar at? sa teachtaireacht seo is ceart duit dul i dteagmh?il leis an seolt?ir l?ithreach agus le mailminder[ag]justice.ie chomh maith. 
***********************************************************************************




From nelson.ana at gmail.com  Mon Jul 27 15:37:53 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Mon, 27 Jul 2009 14:37:53 +0100
Subject: [R-SIG-Finance] Coercion problem in RBloomberg
In-Reply-To: <BF4613BA62AAD6459F2C1FB9FF9C1EDB01D8EF5A@orange.davis.local>
References: <AcoAKPG6AOGrKaVzSMCxytHFvz2OPw==>
	<BF4613BA62AAD6459F2C1FB9FF9C1EDB01D8EF5A@orange.davis.local>
Message-ID: <a7d6d2740907270637s190c7da6m9bce5dc3be8f29fd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090727/b03bd893/attachment.pl>

From burkett at uri.edu  Mon Jul 27 20:04:34 2009
From: burkett at uri.edu (John P. Burkett)
Date: Mon, 27 Jul 2009 14:04:34 -0400
Subject: [R-SIG-Finance] disaggregate from monthly to daily time series
Message-ID: <4A6DEC32.1030803@uri.edu>

I would like to disaggregate a monthly average (Consumer Price Index) to 
create a daily time series.  The new daily series should be smooth 
(i.e.--exhibit no unusual jump from the last day of a month to the first 
day of the next month) and be consistent with the original monthly data 
(i.e.--the average value of the new series for the days of a month 
should equal the given value for that month).

If all months had 30 days and I were using S+Finmetrics, I would try to 
create the daily series with a command such as
disaggregate(CPI, 30, method="spline", how="mean")
where CPI is the monthly data on the Consumer Price Index.
The fact that months are of different lengths complicates matters.

Suggestions for how to accomplish the disaggregation in R would be 
greatly appreciated.

-John

-- 
John P. Burkett
Department of Economics
University of Rhode Island
Kingston, RI 02881-0808
USA

phone (401) 874-9195


From brian at braverock.com  Mon Jul 27 20:22:53 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 27 Jul 2009 13:22:53 -0500
Subject: [R-SIG-Finance] disaggregate from monthly to daily time series
In-Reply-To: <4A6DEC32.1030803@uri.edu>
References: <4A6DEC32.1030803@uri.edu>
Message-ID: <4A6DF07D.4010801@braverock.com>

Well, leaving aside the fact that this seems like a pretty low-utility 
idea (ymmv), na.approx or na.spline will do this if you cbind a monthly 
series to a daily series, and then apply na.approx to the NA's in the 
(formerly) monthly data.

   - Brian

John P. Burkett wrote:
> I would like to disaggregate a monthly average (Consumer Price Index) 
> to create a daily time series.  The new daily series should be smooth 
> (i.e.--exhibit no unusual jump from the last day of a month to the 
> first day of the next month) and be consistent with the original 
> monthly data (i.e.--the average value of the new series for the days 
> of a month should equal the given value for that month).
>
> If all months had 30 days and I were using S+Finmetrics, I would try 
> to create the daily series with a command such as
> disaggregate(CPI, 30, method="spline", how="mean")
> where CPI is the monthly data on the Consumer Price Index.
> The fact that months are of different lengths complicates matters.
>
> Suggestions for how to accomplish the disaggregation in R would be 
> greatly appreciated.
>
> -John
>


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From HodgessE at uhd.edu  Mon Jul 27 20:23:32 2009
From: HodgessE at uhd.edu (Hodgess, Erin)
Date: Mon, 27 Jul 2009 13:23:32 -0500
Subject: [R-SIG-Finance] disaggregate from monthly to daily time series
References: <4A6DEC32.1030803@uri.edu>
Message-ID: <70A5AC06FDB5E54482D19E1C04CDFCF307C3734E@BALI.uhd.campus>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090727/3ce4f5e1/attachment.pl>

From robert at sanctumfi.com  Mon Jul 27 20:27:01 2009
From: robert at sanctumfi.com (Robert Sams)
Date: Mon, 27 Jul 2009 19:27:01 +0100
Subject: [R-SIG-Finance] disaggregate from monthly to daily time series
References: <SANCTUMFISERVERSSDu00003e21@sanctumfi.com>
Message-ID: <SANCTUMFISERVERtivE00003e37@sanctumfi.com>

Hi John,

Does this work for you?

> library(zoo)
> (cpi <- zoo(c(212.174, 213.007, 212.714, 212.671, 212.876, 214.459),
order.by=as.Date(ISOdate(2009, 2:7, 1))))
2009-02-01 2009-03-01 2009-04-01 2009-05-01 2009-06-01 2009-07-01 
   212.174    213.007    212.714    212.671    212.876    214.459 
> na.spline(merge(cpi, foo=zoo(NA, order.by=seq(start(cpi), end(cpi),
"day")))[, 1])
2009-02-01 2009-02-02 2009-02-03 2009-02-04 2009-02-05 2009-02-06
2009-02-07 2009-02-08 2009-02-09 2009-02-10 
  212.1740   212.2373   212.2976   212.3550   212.4096   212.4614
212.5105   212.5568   212.6006   212.6418 
2009-02-11 2009-02-12 2009-02-13 2009-02-14 2009-02-15 2009-02-16
2009-02-17 2009-02-18 2009-02-19 2009-02-20 
  212.6805   212.7167   212.7505   212.7819   212.8110   212.8379
212.8626   212.8851   212.9056   212.9240 
2009-02-21 2009-02-22 2009-02-23 2009-02-24 2009-02-25 2009-02-26
2009-02-27 2009-02-28 2009-03-01 2009-03-02 
  212.9404   212.9549   212.9676   212.9784   212.9874   212.9948
213.0004   213.0045   213.0070   213.0080 
2009-03-03 2009-03-04 2009-03-05 2009-03-06 2009-03-07 2009-03-08
2009-03-09 2009-03-10 2009-03-11 2009-03-12 
  213.0076   213.0059   213.0028   212.9986   212.9932   212.9868
212.9795   212.9712   212.9621   212.9523 
2009-03-13 2009-03-14 2009-03-15 2009-03-16 2009-03-17 2009-03-18
2009-03-19 2009-03-20 2009-03-21 2009-03-22 
  212.9417   212.9306   212.9190   212.9069   212.8944   212.8817
212.8687   212.8556   212.8424   212.8292 
2009-03-23 2009-03-24 2009-03-25 2009-03-26 2009-03-27 2009-03-28
2009-03-29 2009-03-30 2009-03-31 2009-04-01 
  212.8161   212.8031   212.7904   212.7780   212.7659   212.7543
212.7433   212.7328   212.7230   212.7140 
2009-04-02 2009-04-03 2009-04-04 2009-04-05 2009-04-06 2009-04-07
2009-04-08 2009-04-09 2009-04-10 2009-04-11 
  212.7058   212.6984   212.6917   212.6858   212.6805   212.6759
212.6719   212.6686   212.6657   212.6634 
2009-04-12 2009-04-13 2009-04-14 2009-04-15 2009-04-16 2009-04-17
2009-04-18 2009-04-19 2009-04-20 2009-04-21 
  212.6615   212.6601   212.6591   212.6585   212.6583   212.6583
212.6586   212.6591   212.6598   212.6607 
2009-04-22 2009-04-23 2009-04-24 2009-04-25 2009-04-26 2009-04-27
2009-04-28 2009-04-29 2009-04-30 2009-05-01 
  212.6617   212.6629   212.6640   212.6652   212.6664   212.6676
212.6686   212.6696   212.6704   212.6710 
2009-05-02 2009-05-03 2009-05-04 2009-05-05 2009-05-06 2009-05-07
2009-05-08 2009-05-09 2009-05-10 2009-05-11 
  212.6714   212.6717   212.6718   212.6719   212.6720   212.6722
212.6725   212.6731   212.6739   212.6750 
2009-05-12 2009-05-13 2009-05-14 2009-05-15 2009-05-16 2009-05-17
2009-05-18 2009-05-19 2009-05-20 2009-05-21 
  212.6764   212.6783   212.6807   212.6837   212.6872   212.6915
212.6964   212.7022   212.7088   212.7163 
2009-05-22 2009-05-23 2009-05-24 2009-05-25 2009-05-26 2009-05-27
2009-05-28 2009-05-29 2009-05-30 2009-05-31 
  212.7248   212.7343   212.7449   212.7567   212.7696   212.7838
212.7994   212.8163   212.8347   212.8546 
2009-06-01 2009-06-02 2009-06-03 2009-06-04 2009-06-05 2009-06-06
2009-06-07 2009-06-08 2009-06-09 2009-06-10 
  212.8760   212.8991   212.9238   212.9502   212.9784   213.0084
213.0402   213.0738   213.1094   213.1469 
2009-06-11 2009-06-12 2009-06-13 2009-06-14 2009-06-15 2009-06-16
2009-06-17 2009-06-18 2009-06-19 2009-06-20 
  213.1864   213.2280   213.2716   213.3174   213.3653   213.4154
213.4678   213.5224   213.5793   213.6387 
2009-06-21 2009-06-22 2009-06-23 2009-06-24 2009-06-25 2009-06-26
2009-06-27 2009-06-28 2009-06-29 2009-06-30 
  213.7004   213.7646   213.8312   213.9004   213.9722   214.0465
214.1236   214.2033   214.2857   214.3710 
2009-07-01 
  214.4590 
> 
 
Robert

> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch 
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of 
> John P. Burkett
> Sent: 27 July 2009 19:05
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] disaggregate from monthly to daily 
> time series
> 
> I would like to disaggregate a monthly average (Consumer 
> Price Index) to create a daily time series.  The new daily 
> series should be smooth (i.e.--exhibit no unusual jump from 
> the last day of a month to the first day of the next month) 
> and be consistent with the original monthly data (i.e.--the 
> average value of the new series for the days of a month 
> should equal the given value for that month).
> 
> If all months had 30 days and I were using S+Finmetrics, I 
> would try to create the daily series with a command such as 
> disaggregate(CPI, 30, method="spline", how="mean") where CPI 
> is the monthly data on the Consumer Price Index.
> The fact that months are of different lengths complicates matters.
> 
> Suggestions for how to accomplish the disaggregation in R 
> would be greatly appreciated.
> 
> -John
> 
> --
> John P. Burkett
> Department of Economics
> University of Rhode Island
> Kingston, RI 02881-0808
> USA
> 
> phone (401) 874-9195
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 


From josh.m.ulrich at gmail.com  Mon Jul 27 20:32:11 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Mon, 27 Jul 2009 13:32:11 -0500
Subject: [R-SIG-Finance] disaggregate from monthly to daily time series
In-Reply-To: <4A6DF07D.4010801@braverock.com>
References: <4A6DEC32.1030803@uri.edu> <4A6DF07D.4010801@braverock.com>
Message-ID: <8cca69990907271132l62f48ec1xe806e8b6a2adcad1@mail.gmail.com>

John,

Here's some code that follows Brian's suggestion.

library(xts)
x <- xts(cumprod(1+rnorm(999)/100),Sys.Date()-999:1)
m <- timeBasedSeq('2006-11/2009-07', 'Date')
mx <- x[m]
M <- merge(mx,x)

mtod.spline <- xts(spline(1:NROW(M),M[,1],n=NROW(M))$y,index(x))
mtod.linear <- xts(approx(1:NROW(M),M[,1],n=NROW(M))$y,index(x))
mtod <- merge(x,mtod.spline,mtod.linear)
plot.zoo(mtod)

HTH,
Josh
--
http://www.fosstrading.com



On Mon, Jul 27, 2009 at 1:22 PM, Brian G. Peterson<brian at braverock.com> wrote:
> Well, leaving aside the fact that this seems like a pretty low-utility idea
> (ymmv), na.approx or na.spline will do this if you cbind a monthly series to
> a daily series, and then apply na.approx to the NA's in the (formerly)
> monthly data.
>
> ?- Brian
>
> John P. Burkett wrote:
>>
>> I would like to disaggregate a monthly average (Consumer Price Index) to
>> create a daily time series. ?The new daily series should be smooth
>> (i.e.--exhibit no unusual jump from the last day of a month to the first day
>> of the next month) and be consistent with the original monthly data
>> (i.e.--the average value of the new series for the days of a month should
>> equal the given value for that month).
>>
>> If all months had 30 days and I were using S+Finmetrics, I would try to
>> create the daily series with a command such as
>> disaggregate(CPI, 30, method="spline", how="mean")
>> where CPI is the monthly data on the Consumer Price Index.
>> The fact that months are of different lengths complicates matters.
>>
>> Suggestions for how to accomplish the disaggregation in R would be greatly
>> appreciated.
>>
>> -John
>>
>
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From burkett at uri.edu  Mon Jul 27 23:44:18 2009
From: burkett at uri.edu (John P. Burkett)
Date: Mon, 27 Jul 2009 17:44:18 -0400
Subject: [R-SIG-Finance] disaggregate from monthly to daily time series
In-Reply-To: <70A5AC06FDB5E54482D19E1C04CDFCF307C3734E@BALI.uhd.campus>
References: <4A6DEC32.1030803@uri.edu>
	<70A5AC06FDB5E54482D19E1C04CDFCF307C3734E@BALI.uhd.campus>
Message-ID: <4A6E1FB2.201@uri.edu>

Hodgess, Erin wrote:
> I've done disaggregation for annual/quarterly, quarterly/monthly before, but never monthly/daily
> 
> is there a big demand for that, please?
> 
Some people involved with rolling portfolio management and risk 
supervision like to work with daily data on asset prices (Gourieroux and 
Monfort, "The Econometrics of Efficient Portfolios," Journal of 
Empirical Finance, v. 12 (2005) pp. 22-23).  However, to adjust these 
prices for inflation, one should divide them by a general price index 
such as the CPI.  That's the motivation for trying to disaggregate the 
monthly CPI data to daily frequency.

-John



> Thanks,
> Erin
> 
> 
> Erin M. Hodgess, PhD
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgesse at uhd.edu
> 
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of John P. Burkett
> Sent: Mon 7/27/2009 1:04 PM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] disaggregate from monthly to daily time series
>  
> I would like to disaggregate a monthly average (Consumer Price Index) to 
> create a daily time series.  The new daily series should be smooth 
> (i.e.--exhibit no unusual jump from the last day of a month to the first 
> day of the next month) and be consistent with the original monthly data 
> (i.e.--the average value of the new series for the days of a month 
> should equal the given value for that month).
> 
> If all months had 30 days and I were using S+Finmetrics, I would try to 
> create the daily series with a command such as
> disaggregate(CPI, 30, method="spline", how="mean")
> where CPI is the monthly data on the Consumer Price Index.
> The fact that months are of different lengths complicates matters.
> 
> Suggestions for how to accomplish the disaggregation in R would be 
> greatly appreciated.
> 
> -John
> 


-- 
John P. Burkett
Department of Economics
University of Rhode Island
Kingston, RI 02881-0808
USA

phone (401) 874-9195


From burkett at uri.edu  Tue Jul 28 00:10:43 2009
From: burkett at uri.edu (John P. Burkett)
Date: Mon, 27 Jul 2009 18:10:43 -0400
Subject: [R-SIG-Finance] disaggregate from monthly to daily time series
In-Reply-To: <8cca69990907271132l62f48ec1xe806e8b6a2adcad1@mail.gmail.com>
References: <4A6DEC32.1030803@uri.edu> <4A6DF07D.4010801@braverock.com>
	<8cca69990907271132l62f48ec1xe806e8b6a2adcad1@mail.gmail.com>
Message-ID: <4A6E25E3.9060102@uri.edu>

Joshua Ulrich wrote:

> Here's some code that follows Brian's suggestion.
Thank you, Brian and Joshua, for your suggestions. Having tried the code 
on a Ubuntu box running R 2.8.1, I'm inserting my comments below the 
relevant lines.
> 
> library(xts)
> x <- xts(cumprod(1+rnorm(999)/100),Sys.Date()-999:1)
> m <- timeBasedSeq('2006-11/2009-07', 'Date')
This line elicits the message "Error in strftime(x) : wrong class"
However the line seems to work o.k. if shortened to
m <- timeBasedSeq('2006-11/2009-07')

> mx <- x[m]
This line generates 50 or more warnings of the form
"In timeBasedSeq(x, NULL) : NAs introduced by coercion"
Are those the NAs Brian mentioned?
dim(mx) is 0, 1.  Doing "mx" gets the following response:
   Data:
   numeric(0)
   Index:
   integer(0)
Is that what was intended?

> M <- merge(mx,x)
> 
> mtod.spline <- xts(spline(1:NROW(M),M[,1],n=NROW(M))$y,index(x))
> mtod.linear <- xts(approx(1:NROW(M),M[,1],n=NROW(M))$y,index(x))
> mtod <- merge(x,mtod.spline,mtod.linear)
> plot.zoo(mtod)
The end result on my box is three seemingly identical series: x, 
mtod.spline, mtod.linear. I'm still thinking about how they relate the 
problem of disaggregating a monthly series into smooth daily series with 
the original mean.

> 
> HTH,
> Josh
> --
> http://www.fosstrading.com
> 
> 
> 
> On Mon, Jul 27, 2009 at 1:22 PM, Brian G. Peterson<brian at braverock.com> wrote:
>> Well, leaving aside the fact that this seems like a pretty low-utility idea
>> (ymmv), na.approx or na.spline will do this if you cbind a monthly series to
>> a daily series, and then apply na.approx to the NA's in the (formerly)
>> monthly data.
>>
>>  - Brian
>>
>> John P. Burkett wrote:
>>> I would like to disaggregate a monthly average (Consumer Price Index) to
>>> create a daily time series.  The new daily series should be smooth
>>> (i.e.--exhibit no unusual jump from the last day of a month to the first day
>>> of the next month) and be consistent with the original monthly data
>>> (i.e.--the average value of the new series for the days of a month should
>>> equal the given value for that month).
>>>
>>> If all months had 30 days and I were using S+Finmetrics, I would try to
>>> create the daily series with a command such as
>>> disaggregate(CPI, 30, method="spline", how="mean")
>>> where CPI is the monthly data on the Consumer Price Index.
>>> The fact that months are of different lengths complicates matters.
>>>
>>> Suggestions for how to accomplish the disaggregation in R would be greatly
>>> appreciated.
>>>
>>> -John
>>>
>>
>> --
>> Brian G. Peterson
>> http://braverock.com/brian/
>> Ph: 773-459-4973
>> IM: bgpbraverock
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 


-- 
John P. Burkett
Department of Economics
University of Rhode Island
Kingston, RI 02881-0808
USA

phone (401) 874-9195


From josh.m.ulrich at gmail.com  Tue Jul 28 00:37:12 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Mon, 27 Jul 2009 17:37:12 -0500
Subject: [R-SIG-Finance] disaggregate from monthly to daily time series
In-Reply-To: <4A6E25E3.9060102@uri.edu>
References: <4A6DEC32.1030803@uri.edu> <4A6DF07D.4010801@braverock.com> 
	<8cca69990907271132l62f48ec1xe806e8b6a2adcad1@mail.gmail.com> 
	<4A6E25E3.9060102@uri.edu>
Message-ID: <8cca69990907271537n2cb8214aic331db0b8aa2b9a8@mail.gmail.com>

On Mon, Jul 27, 2009 at 5:10 PM, John P. Burkett<burkett at uri.edu> wrote:
> Joshua Ulrich wrote:
>
>> Here's some code that follows Brian's suggestion.
>
> Thank you, Brian and Joshua, for your suggestions. Having tried the code on
> a Ubuntu box running R 2.8.1, I'm inserting my comments below the relevant
> lines.
I would suggest you upgrade to R-2.9.1 and the latest version of xts.

>>
>> library(xts)
>> x <- xts(cumprod(1+rnorm(999)/100),Sys.Date()-999:1)
>> m <- timeBasedSeq('2006-11/2009-07', 'Date')
>
> This line elicits the message "Error in strftime(x) : wrong class"
> However the line seems to work o.k. if shortened to
> m <- timeBasedSeq('2006-11/2009-07')
>
>> mx <- x[m]
>
This won't work because timeBasedSeq will return zoo's "yearmon" class
by default in this case, and you can't subset an xts object via "mmm
yyyy" style strings.  The original code returned a Date class object,
which you can then use to extract the first values of the month from
the xts object "x".

> This line generates 50 or more warnings of the form
> "In timeBasedSeq(x, NULL) : NAs introduced by coercion"
> Are those the NAs Brian mentioned?
They are not.  This line throws an error on R-2.9.1 with the latest xts.
> mx <- x[m]
Error in `[.xts`(x, m) : subscript out of bounds

> dim(mx) is 0, 1. ?Doing "mx" gets the following response:
> ?Data:
> ?numeric(0)
> ?Index:
> ?integer(0)
> Is that what was intended?
>
No, it's an empty xts object - the result of the warnings above.

>> M <- merge(mx,x)
>>
>> mtod.spline <- xts(spline(1:NROW(M),M[,1],n=NROW(M))$y,index(x))
>> mtod.linear <- xts(approx(1:NROW(M),M[,1],n=NROW(M))$y,index(x))
>> mtod <- merge(x,mtod.spline,mtod.linear)
>> plot.zoo(mtod)
>
> The end result on my box is three seemingly identical series: x,
> mtod.spline, mtod.linear. I'm still thinking about how they relate the
> problem of disaggregating a monthly series into smooth daily series with the
> original mean.
>
The "mx" object is empty, so "M" and "x" _are_ identical.  I'm not
familiar with S+Finmetrics and I wasn't aware that's what
disaggregate(..., how="mean") implied.  If my example retains the mean
of the original series, it would be completely unintentional.

Best,
Josh
--
http://www.fosstrading.com


>>
>> HTH,
>> Josh
>> --
>> http://www.fosstrading.com
>>
>>
>>
>> On Mon, Jul 27, 2009 at 1:22 PM, Brian G. Peterson<brian at braverock.com>
>> wrote:
>>>
>>> Well, leaving aside the fact that this seems like a pretty low-utility
>>> idea
>>> (ymmv), na.approx or na.spline will do this if you cbind a monthly series
>>> to
>>> a daily series, and then apply na.approx to the NA's in the (formerly)
>>> monthly data.
>>>
>>> ?- Brian
>>>
>>> John P. Burkett wrote:
>>>>
>>>> I would like to disaggregate a monthly average (Consumer Price Index) to
>>>> create a daily time series. ?The new daily series should be smooth
>>>> (i.e.--exhibit no unusual jump from the last day of a month to the first
>>>> day
>>>> of the next month) and be consistent with the original monthly data
>>>> (i.e.--the average value of the new series for the days of a month
>>>> should
>>>> equal the given value for that month).
>>>>
>>>> If all months had 30 days and I were using S+Finmetrics, I would try to
>>>> create the daily series with a command such as
>>>> disaggregate(CPI, 30, method="spline", how="mean")
>>>> where CPI is the monthly data on the Consumer Price Index.
>>>> The fact that months are of different lengths complicates matters.
>>>>
>>>> Suggestions for how to accomplish the disaggregation in R would be
>>>> greatly
>>>> appreciated.
>>>>
>>>> -John
>>>>
>>>
>>> --
>>> Brian G. Peterson
>>> http://braverock.com/brian/
>>> Ph: 773-459-4973
>>> IM: bgpbraverock
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>
>
> --
> John P. Burkett
> Department of Economics
> University of Rhode Island
> Kingston, RI 02881-0808
> USA
>
> phone (401) 874-9195
>


From hassan_hany_fahmy at yahoo.com  Tue Jul 28 08:38:38 2009
From: hassan_hany_fahmy at yahoo.com (Hassan hany)
Date: Mon, 27 Jul 2009 23:38:38 -0700 (PDT)
Subject: [R-SIG-Finance] R excel
Message-ID: <131014.89736.qm@web112213.mail.gq1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090727/c2eff61d/attachment.pl>

From jeff.a.ryan at gmail.com  Tue Jul 28 17:38:41 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 28 Jul 2009 10:38:41 -0500
Subject: [R-SIG-Finance] R excel
In-Reply-To: <131014.89736.qm@web112213.mail.gq1.yahoo.com>
References: <131014.89736.qm@web112213.mail.gq1.yahoo.com>
Message-ID: <e8e755250907280838yc14c149v39ab5e398512a3fa@mail.gmail.com>

This isn't a _finance_ question.

Please ask the main R-help list for help, as well as read the posting
guide(s) before asking any questions.

Jeff

On Tue, Jul 28, 2009 at 1:38 AM, Hassan hany<hassan_hany_fahmy at yahoo.com> wrote:
> Hi,
> whenever am trying to start R from excel, or by clicking the[ RExcel2007 with RCommander] icon....., i got the following messages boxes :
> SCTools not available
> ?then:
> there seems to be no R process connected
> to excel
> though both r and excel starts after I click the Icon!!
> Please help
> Hassan
>
>
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From brian at braverock.com  Tue Jul 28 21:08:41 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 28 Jul 2009 14:08:41 -0500
Subject: [R-SIG-Finance] error in plot() with fGarch object from saved
	.RData file
Message-ID: <4A6F4CB9.5020003@braverock.com>

On Windows, R saves your workspace automagically on quitting in a file 
in the working directory called .RData

if I have a garchFit object 'gf' it will plot correctly immediately 
after fitting.

However, it will fail after reloading:

 > plot(gf)
Error in as.double(y) :
  cannot coerce type 'S4' to vector of type 'double'

but the corresponding .plot.garch.#() functions will still work.

I can replicate this behavior on linux as well by fitting and then 
saving a garchFit object.  On loading the object, it will fail to S4 
method dispatch on plot() again.

Using R 2.9.1 and the CRAN version of the RMetrics packages

Any ideas on how to avoid this behavior? 

It's only an annoyance, but it does make it difficult for me to share 
data internally if I can't just save the object and have it displayable 
by my coworkers.

Thanks,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From chalabi at phys.ethz.ch  Tue Jul 28 22:01:06 2009
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Tue, 28 Jul 2009 22:01:06 +0200
Subject: [R-SIG-Finance] error in plot() with fGarch object from saved
	.RData file
In-Reply-To: <4A6F4CB9.5020003@braverock.com>
References: <4A6F4CB9.5020003@braverock.com>
Message-ID: <20090728220106.69245e77@mimi>

>>>> "BGP" == "Brian G. Peterson" <brian at braverock.com>
>>>> on Tue, 28 Jul 2009 14:08:41 -0500

   BGP> On Windows, R saves your workspace automagically on quitting
   BGP> in a file
   BGP> in the working directory called .RData
   BGP>
   BGP> if I have a garchFit object 'gf' it will plot correctly
   BGP> immediately
   BGP> after fitting.
   BGP>
   BGP> However, it will fail after reloading:
   BGP>
   BGP> > plot(gf)
   BGP> Error in as.double(y) :
   BGP> cannot coerce type 'S4' to vector of type 'double'

Hi Brian,

I suspect that you did not load 'fGarch' before trying to plot your
saved object 'gf'. You are thus missing plot,fGARCH-method and related
functions.

If you do not want your coworker to load the whole package, you could
include the missing method and functions in the saved workspace.

HTH
Yohan

-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch


From brian at braverock.com  Tue Jul 28 22:06:16 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 28 Jul 2009 15:06:16 -0500
Subject: [R-SIG-Finance] error in plot() with fGarch object from saved
	.RData file
In-Reply-To: <20090728220106.69245e77@mimi>
References: <4A6F4CB9.5020003@braverock.com> <20090728220106.69245e77@mimi>
Message-ID: <4A6F5A38.9070207@braverock.com>

Yohan Chalabi wrote:
>>>>> "BGP" == "Brian G. Peterson" <brian at braverock.com>
>>>>> on Tue, 28 Jul 2009 14:08:41 -0500
>>>>>           
>
>    BGP> On Windows, R saves your workspace automagically on quitting
>    BGP> in a file
>    BGP> in the working directory called .RData
>    BGP>
>    BGP> if I have a garchFit object 'gf' it will plot correctly
>    BGP> immediately
>    BGP> after fitting.
>    BGP>
>    BGP> However, it will fail after reloading:
>    BGP>
>    BGP> > plot(gf)
>    BGP> Error in as.double(y) :
>    BGP> cannot coerce type 'S4' to vector of type 'double'
>
> Hi Brian,
>
> I suspect that you did not load 'fGarch' before trying to plot your
> saved object 'gf'. You are thus missing plot,fGARCH-method and related
> functions.
>
> If you do not want your coworker to load the whole package, you could
> include the missing method and functions in the saved workspace.
>   
Yohan,

Thanks.  I also suspected this, and made sure that

require(fGarch')

was called before I tried to plot.  Also, as I said, I can call the 
private .plot.garch.#() functions directly.

 > class(gf)
[1] "fGARCH"
attr(,"package")
[1] "fGarch"

looks OK

Does anybody know how to debug S4 method dispatch (sorry this is not 
strictly finance, but fGarch clearly is)?

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From chalabi at phys.ethz.ch  Tue Jul 28 23:18:45 2009
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Tue, 28 Jul 2009 23:18:45 +0200
Subject: [R-SIG-Finance] error in plot() with fGarch object from saved
	.RData file
In-Reply-To: <4A6F5A38.9070207@braverock.com>
References: <4A6F4CB9.5020003@braverock.com> <20090728220106.69245e77@mimi>
	<4A6F5A38.9070207@braverock.com>
Message-ID: <20090728231845.3fbdc7e7@mimi>

>>>> "BGP" == "Brian G. Peterson" <brian at braverock.com>
>>>> on Tue, 28 Jul 2009 15:06:16 -0500

   BGP> Thanks.  I also suspected this, and made sure that
   BGP>
   BGP> require(fGarch')
   BGP>
   BGP> was called before I tried to plot.  Also, as I said, I can
   BGP> call the
   BGP> private .plot.garch.#() functions directly.
   BGP>
   BGP> > class(gf)
   BGP> [1] fGARCH
   BGP> attr(,package)
   BGP> [1] fGarch
   BGP>
   BGP> looks OK
   BGP>
   BGP> Does anybody know how to debug S4 method dispatch (sorry this
   BGP> is not
   BGP> strictly finance, but fGarch clearly is)?

It looks like a NAMESPACE problem in 'fGarch' v2100.78. Although the
method is exported in the NAMESPACE, it does not appear after loading
the package...

But it works fine with the dev-version which is on R-forge.

Until we upload the new version to CRAN, you could explicitly add
in the workspace the method as defined in fGarch/pkg/method-plot.R and
save it with your 'gf' object.

HTH,
Yohan


-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch


From roger at bergande.ch  Wed Jul 29 08:34:59 2009
From: roger at bergande.ch (Roger Bergande)
Date: Wed, 29 Jul 2009 08:34:59 +0200
Subject: [R-SIG-Finance] Tier 1 Subordinated Bond
In-Reply-To: <d718c8210907252018p28388ff8lbe578d381dcfe0c6@mail.gmail.com>
References: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com>
	<e8e755250907250746p2dc673cfn129b46c755af453b@mail.gmail.com>
	<d718c8210907251838j56568a98j8c0fc7c081a9509f@mail.gmail.com>
	<971536df0907251909l2092dd69q6f15176da21f369c@mail.gmail.com>
	<d718c8210907252018p28388ff8lbe578d381dcfe0c6@mail.gmail.com>
Message-ID: <20090729083459.13617qej5mqpvhxv@login-3.hoststar.ch>

Dear all

I?m facing the problem of valuing a tier 1 Bond. The Bond has an  
embedded option which allows no coupon payment if the issuer doesn?t  
pay dividends.

Does anybody have an idea how I could model this type of Bond? Please  
let me know if you have a model or a replicating method.

Many thanks in advance and best regards,
Roger


From ICoe at connectcap.com  Thu Jul 30 00:06:10 2009
From: ICoe at connectcap.com (Ian Coe)
Date: Wed, 29 Jul 2009 15:06:10 -0700
Subject: [R-SIG-Finance] Speed issue issue with periodReturn
In-Reply-To: <20090729083459.13617qej5mqpvhxv@login-3.hoststar.ch>
References: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com><e8e755250907250746p2dc673cfn129b46c755af453b@mail.gmail.com><d718c8210907251838j56568a98j8c0fc7c081a9509f@mail.gmail.com><971536df0907251909l2092dd69q6f15176da21f369c@mail.gmail.com><d718c8210907252018p28388ff8lbe578d381dcfe0c6@mail.gmail.com>
	<20090729083459.13617qej5mqpvhxv@login-3.hoststar.ch>
Message-ID: <C92D6BF93B8E2A4B96E206B66040B916E29FAC@CONNCAPSBS.connectcap.local>

Hi,
  I noticed that the periodReturn function seems to a take non-trivial
amount of time to compute weekly returns. 

  The calls below all compute the log returns from 1/1/2007 to now and
they take slightly different amounts of time.

  I'd like to be able to compute weekly returns as fast as possible.
Does anyone have suggestions for minimizing the processing time?
Different data type?  Different function to call?  

Thanks,
Ian  


> yhoo=getSymbols("GOOG",source="yahoo",from = "01-01-2004")
> system.time(weeks<-periodReturn(GOOG,period="weekly",type='log',
subset="2007::"))
   user  system elapsed 
   0.06    0.00    0.06 
> yhoo=getSymbols("GOOG",source="yahoo",from = "01-01-2007")
> system.time(weeks<-periodReturn(GOOG,period="weekly",type='log',
subset="2007::"))
   user  system elapsed 
   0.04    0.00    0.07
> yhoo=getSymbols("GOOG",source="yahoo")
> system.time(weeks<-periodReturn(GOOG,period="weekly",type='log'))
   user  system elapsed 
   0.05    0.00    0.05


From brian at braverock.com  Thu Jul 30 02:42:06 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 29 Jul 2009 19:42:06 -0500
Subject: [R-SIG-Finance] Speed issue issue with periodReturn
In-Reply-To: <C92D6BF93B8E2A4B96E206B66040B916E29FAC@CONNCAPSBS.connectcap.local>
References: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com><e8e755250907250746p2dc673cfn129b46c755af453b@mail.gmail.com><d718c8210907251838j56568a98j8c0fc7c081a9509f@mail.gmail.com><971536df0907251909l2092dd69q6f15176da21f369c@mail.gmail.com><d718c8210907252018p28388ff8lbe578d381dcfe0c6@mail.gmail.com>	<20090729083459.13617qej5mqpvhxv@login-3.hoststar.ch>
	<C92D6BF93B8E2A4B96E206B66040B916E29FAC@CONNCAPSBS.connectcap.local>
Message-ID: <4A70EC5E.80303@braverock.com>

Ian Coe wrote:
> Hi,
>   I noticed that the periodReturn function seems to a take non-trivial
> amount of time to compute weekly returns. 
>
>   The calls below all compute the log returns from 1/1/2007 to now and
> they take slightly different amounts of time.
>
>   I'd like to be able to compute weekly returns as fast as possible.
> Does anyone have suggestions for minimizing the processing time?
> Different data type?  Different function to call?  
>
> Thanks,
> Ian  
>
>
>   
>> yhoo=getSymbols("GOOG",source="yahoo",from = "01-01-2004")
>> system.time(weeks<-periodReturn(GOOG,period="weekly",type='log',
>>     
> subset="2007::"))
>    user  system elapsed 
>    0.06    0.00    0.06 
>   
>> yhoo=getSymbols("GOOG",source="yahoo",from = "01-01-2007")
>> system.time(weeks<-periodReturn(GOOG,period="weekly",type='log',
>>     
> subset="2007::"))
>    user  system elapsed 
>    0.04    0.00    0.07
>   
>> yhoo=getSymbols("GOOG",source="yahoo")
>> system.time(weeks<-periodReturn(GOOG,period="weekly",type='log'))
>>     
>    user  system elapsed 
>    0.05    0.00    0.05
>
>   
Ian,

1/20th of a second for WEEKLY aggregated returns on 2+ years of data 
doesn't seem outrageous.  So might gain a tiny bit by calling log() 
directly, but I doubt it.  I could of course be wrong, but the tiny time 
difference seems trivial, not "non-trivial".  Perhaps you need to 
investigate use of foreach and doMC for your larger problem?

Regards,

   - Brian


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From jeff.a.ryan at gmail.com  Thu Jul 30 04:21:53 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 29 Jul 2009 21:21:53 -0500
Subject: [R-SIG-Finance] Speed issue issue with periodReturn
In-Reply-To: <C92D6BF93B8E2A4B96E206B66040B916E29FAC@CONNCAPSBS.connectcap.local>
References: <aacf63cb0907250559n73edfb92oae27ea2cbc8d290d@mail.gmail.com>
	<e8e755250907250746p2dc673cfn129b46c755af453b@mail.gmail.com>
	<d718c8210907251838j56568a98j8c0fc7c081a9509f@mail.gmail.com>
	<971536df0907251909l2092dd69q6f15176da21f369c@mail.gmail.com>
	<d718c8210907252018p28388ff8lbe578d381dcfe0c6@mail.gmail.com>
	<20090729083459.13617qej5mqpvhxv@login-3.hoststar.ch>
	<C92D6BF93B8E2A4B96E206B66040B916E29FAC@CONNCAPSBS.connectcap.local>
Message-ID: <e8e755250907291921v14b59ebfqde2017d6b91253ca@mail.gmail.com>

I suspect we live in marginally different worlds - as 0.05 seems
'trivial' to me.

While all this software is "free" I'd like to reiterate that it
actually costs someone (usually) lots of time.

A better/helpful post could include some effort to identify a
bottleneck, or maybe show what you've accomplished to make it better.

The above said, a few suggestions:

In one CPU hour (10 cents on Amazon?) you could do periodReturn 60,000 times.

Assuming an hour to better the performance by 50%, and at a price of
to code of say $100/hr you'd need to plan on using the new function
120,000,000 times.  These estimates are also quite unrealistic..
300,000,000 times might be closer.  That is one heck of a universe of
instruments.

If you find yourself calling the function over and over, saving the
results might be useful too.

Part of the above <rant> was to illustrate that there is no free lunch
--- even in the land of free beer.

That said, periodReturn was meant to be easy to use first, and fast a
distant second.

Try:

> t1 <- system.time(weeks2 <- diff(log(Cl(GOOG)[endpoints(GOOG,'weeks')]))['2007/'])
> t2 <- system.time(weeks<-periodReturn(GOOG,period="weekly",type='log',subset="2007/"))
> (t2/t1)[3]
 elapsed
20.71429
> all.equal(coredata(weeks),coredata(weeks2),check.attributes=FALSE)
[1] TRUE

The direct route is 20x faster.

As it turns out, I am refactoring the internal to.period code to be
much faster/memory efficient.  The original was Fortran, which while
very fast, suffers from a lot of required copying in R, so the code is
changing (originally from 2007).  I'll probably also think about
making the simple cases use something more along the direct route.

HTH
Jeff

On Wed, Jul 29, 2009 at 5:06 PM, Ian Coe<ICoe at connectcap.com> wrote:
> Hi,
> ?I noticed that the periodReturn function seems to a take non-trivial
> amount of time to compute weekly returns.
>
> ?The calls below all compute the log returns from 1/1/2007 to now and
> they take slightly different amounts of time.
>
> ?I'd like to be able to compute weekly returns as fast as possible.
> Does anyone have suggestions for minimizing the processing time?
> Different data type? ?Different function to call?
>
> Thanks,
> Ian
>
>
>> yhoo=getSymbols("GOOG",source="yahoo",from = "01-01-2004")
>> system.time(weeks<-periodReturn(GOOG,period="weekly",type='log',
> subset="2007::"))
> ? user ?system elapsed
> ? 0.06 ? ?0.00 ? ?0.06
>> yhoo=getSymbols("GOOG",source="yahoo",from = "01-01-2007")
>> system.time(weeks<-periodReturn(GOOG,period="weekly",type='log',
> subset="2007::"))
> ? user ?system elapsed
> ? 0.04 ? ?0.00 ? ?0.07
>> yhoo=getSymbols("GOOG",source="yahoo")
>> system.time(weeks<-periodReturn(GOOG,period="weekly",type='log'))
> ? user ?system elapsed
> ? 0.05 ? ?0.00 ? ?0.05
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From arun.kumar.saha at gmail.com  Thu Jul 30 19:43:28 2009
From: arun.kumar.saha at gmail.com (Arun.stat)
Date: Thu, 30 Jul 2009 10:43:28 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Quantmod
In-Reply-To: <24739588.post@talk.nabble.com>
References: <24739588.post@talk.nabble.com>
Message-ID: <24743374.post@talk.nabble.com>


I think it is your net-connection problem. It is working fine in my system :

> from.dat <- as.Date("01/01/08", format="%m/%d/%y")
> to.dat <- as.Date("02/19/09", format="%m/%d/%y")
> 
> getSymbols("GE", src="yahoo", from = from.dat, to = to.dat) 
[1] "GE"
> Sys.info()
                     sysname                      release                     
version 
                   "Windows"                      "Vista" "build 6002,
Service Pack 2" 
                    nodename                      machine                       
login 
                      "ARUN"                        "x86"                   
"Arrun's" 
                        user 
                   "Arrun's" 



ehxpieterse wrote:
> 
> Hi there,
> 
> I have recently installed R 2.9.1 and quantmod 0.3.6
> 
> I am trying to get my head around using it, but I keep on getting stopped
> at the first hurdle. As an example, I am trying to test the following
> code:
> 
> > from.dat <- as.Date("01/01/08", format="%m/%d/%y") 
>> to.dat <- as.Date("02/19/09", format="%m/%d/%y") 
>> 
>> getSymbols("GE", src="yahoo", from = from.dat, to = to.dat) 
> But I get the following error:
> 
> Error in download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=", from.m, 
> : 
>   cannot open URL
> 'http://chart.yahoo.com/table.csv?s=GE&a=0&b=01&c=2008&d=1&e=19&f=2009&g=d&q=q&y=0&z=GE&x=.csv'
> In addition: Warning message:
> In download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=", from.m,  :
>   unable to connect to 'chart.yahoo.com' on port 80.
> 
> I am not sure where to start to fix this this problem. Has anyone come
> accross something like this before?
> 
> Thanks,
> Eduard
> 
> 

-- 
View this message in context: http://www.nabble.com/Quantmod-tp24739588p24743374.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Thu Jul 30 19:49:07 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 30 Jul 2009 12:49:07 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Quantmod
In-Reply-To: <24743374.post@talk.nabble.com>
References: <24739588.post@talk.nabble.com> <24743374.post@talk.nabble.com>
Message-ID: <e8e755250907301049o6624adf8y8a9c4379df38c822@mail.gmail.com>

The other issue is that your version is a bit out of date.

The current CRAN version is 0.3-11.  I'll update the website to
reflect this, as well as post a warning that the site has a tendency
to lag behind CRAN or R-forge in terms of most current versions.

Thanks,
Jeff

On Thu, Jul 30, 2009 at 12:43 PM, Arun.stat<arun.kumar.saha at gmail.com> wrote:
>
> I think it is your net-connection problem. It is working fine in my system :
>
>> from.dat <- as.Date("01/01/08", format="%m/%d/%y")
>> to.dat <- as.Date("02/19/09", format="%m/%d/%y")
>>
>> getSymbols("GE", src="yahoo", from = from.dat, to = to.dat)
> [1] "GE"
>> Sys.info()
> ? ? ? ? ? ? ? ? ? ? sysname ? ? ? ? ? ? ? ? ? ? ?release
> version
> ? ? ? ? ? ? ? ? ? "Windows" ? ? ? ? ? ? ? ? ? ? ?"Vista" "build 6002,
> Service Pack 2"
> ? ? ? ? ? ? ? ? ? ?nodename ? ? ? ? ? ? ? ? ? ? ?machine
> login
> ? ? ? ? ? ? ? ? ? ? ?"ARUN" ? ? ? ? ? ? ? ? ? ? ? ?"x86"
> "Arrun's"
> ? ? ? ? ? ? ? ? ? ? ? ?user
> ? ? ? ? ? ? ? ? ? "Arrun's"
>
>
>
> ehxpieterse wrote:
>>
>> Hi there,
>>
>> I have recently installed R 2.9.1 and quantmod 0.3.6
>>
>> I am trying to get my head around using it, but I keep on getting stopped
>> at the first hurdle. As an example, I am trying to test the following
>> code:
>>
>> > from.dat <- as.Date("01/01/08", format="%m/%d/%y")
>>> to.dat <- as.Date("02/19/09", format="%m/%d/%y")
>>>
>>> getSymbols("GE", src="yahoo", from = from.dat, to = to.dat)
>> But I get the following error:
>>
>> Error in download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=", from.m,
>> :
>> ? cannot open URL
>> 'http://chart.yahoo.com/table.csv?s=GE&a=0&b=01&c=2008&d=1&e=19&f=2009&g=d&q=q&y=0&z=GE&x=.csv'
>> In addition: Warning message:
>> In download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=", from.m, ?:
>> ? unable to connect to 'chart.yahoo.com' on port 80.
>>
>> I am not sure where to start to fix this this problem. Has anyone come
>> accross something like this before?
>>
>> Thanks,
>> Eduard
>>
>>
>
> --
> View this message in context: http://www.nabble.com/Quantmod-tp24739588p24743374.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From breman.mark at gmail.com  Fri Jul 31 11:55:22 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Fri, 31 Jul 2009 11:55:22 +0200
Subject: [R-SIG-Finance] Continuous futures series with R
Message-ID: <5e6a2e670907310255k468abb49p5a773f1e7d3e251a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090731/a9da313d/attachment.pl>

From brian at braverock.com  Fri Jul 31 12:36:58 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 31 Jul 2009 05:36:58 -0500
Subject: [R-SIG-Finance] Continuous futures series with R
In-Reply-To: <5e6a2e670907310255k468abb49p5a773f1e7d3e251a@mail.gmail.com>
References: <5e6a2e670907310255k468abb49p5a773f1e7d3e251a@mail.gmail.com>
Message-ID: <4A72C94A.8080102@braverock.com>

Mark Breman wrote:
> Hi,
> I have been looking for existing R code to create a continuous futures
> series from individual futures contract series, but have not found anything
> (yet).
>
> Is there really nothing out there?
>
> Kind regards,
>   
Mark,

I think the biggest issue is that the roll can happen on any number of 
different criteria.  Volume Cross, Midpoint Roll, Date (expiration-n) 
Roll, some other method, etc.  Also, any data provider that you're 
already paying for data (Bloomberg, Reuters, CQG, QAI, etc.) will 
already have one or more continuous series methods available, making the 
potential R code even less useful, and probably specific to one data 
provider. 

Another problem would be instrument/contract descriptors through time, 
though this seems minor, R does not yet have an instrument model for 
reference data, though we're working on that.

Can you be a little more explicit about what you are trying to do?  data 
source/provider, roll method, etc?

Regards,

    - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From eduard.pieterse at macquarie.com  Fri Jul 31 13:39:29 2009
From: eduard.pieterse at macquarie.com (ehxpieterse)
Date: Fri, 31 Jul 2009 04:39:29 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Quantmod
In-Reply-To: <e8e755250907301049o6624adf8y8a9c4379df38c822@mail.gmail.com>
References: <24739588.post@talk.nabble.com> <24743374.post@talk.nabble.com>
	<e8e755250907301049o6624adf8y8a9c4379df38c822@mail.gmail.com>
Message-ID: <24755037.post@talk.nabble.com>


Thanks Jeff,

I have updated quantmod but am still having connectivity issues. I have
searched through various help files, and I have implemented all the
suggestions.

Do you know of a simple guide to check the connectivity setup of R?

Thanks,
Eduard
-- 
View this message in context: http://www.nabble.com/Quantmod-tp24739588p24755037.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From breman.mark at gmail.com  Fri Jul 31 13:45:20 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Fri, 31 Jul 2009 13:45:20 +0200
Subject: [R-SIG-Finance] Continuous futures series with R
In-Reply-To: <4A72C94A.8080102@braverock.com>
References: <5e6a2e670907310255k468abb49p5a773f1e7d3e251a@mail.gmail.com>
	<4A72C94A.8080102@braverock.com>
Message-ID: <5e6a2e670907310445o4d43e46m4693a9bfa6a980b1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090731/ff393e5a/attachment.pl>

From josh.m.ulrich at gmail.com  Fri Jul 31 13:50:32 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Fri, 31 Jul 2009 06:50:32 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Quantmod
In-Reply-To: <24755037.post@talk.nabble.com>
References: <24739588.post@talk.nabble.com> <24743374.post@talk.nabble.com> 
	<e8e755250907301049o6624adf8y8a9c4379df38c822@mail.gmail.com> 
	<24755037.post@talk.nabble.com>
Message-ID: <8cca69990907310450v4c1c297ducd9985d92563e32a@mail.gmail.com>

Eduard

See this section of the Windows FAQ:
http://cran.r-project.org/bin/windows/base/old/2.0.0/rw-FAQ.html#The-internet-download-functions-fail_002e

Post to R-help If you still have problems, since this is no longer
finance-related.  You'll likely get faster responses from that list.

Best,
Josh
--
http://www.fosstrading.com



On Fri, Jul 31, 2009 at 6:39 AM,
ehxpieterse<eduard.pieterse at macquarie.com> wrote:
>
> Thanks Jeff,
>
> I have updated quantmod but am still having connectivity issues. I have
> searched through various help files, and I have implemented all the
> suggestions.
>
> Do you know of a simple guide to check the connectivity setup of R?
>
> Thanks,
> Eduard
> --
> View this message in context: http://www.nabble.com/Quantmod-tp24739588p24755037.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From eduard.pieterse at macquarie.com  Fri Jul 31 13:57:22 2009
From: eduard.pieterse at macquarie.com (ehxpieterse)
Date: Fri, 31 Jul 2009 04:57:22 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Quantmod
In-Reply-To: <8cca69990907310450v4c1c297ducd9985d92563e32a@mail.gmail.com>
References: <24739588.post@talk.nabble.com> <24743374.post@talk.nabble.com>
	<e8e755250907301049o6624adf8y8a9c4379df38c822@mail.gmail.com>
	<24755037.post@talk.nabble.com>
	<8cca69990907310450v4c1c297ducd9985d92563e32a@mail.gmail.com>
Message-ID: <24755240.post@talk.nabble.com>


Will do, thanks Josh.
-- 
View this message in context: http://www.nabble.com/Quantmod-tp24739588p24755240.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From armstrong.whit at gmail.com  Fri Jul 31 14:29:01 2009
From: armstrong.whit at gmail.com (Whit Armstrong)
Date: Fri, 31 Jul 2009 08:29:01 -0400
Subject: [R-SIG-Finance] Continuous futures series with R
In-Reply-To: <5e6a2e670907310445o4d43e46m4693a9bfa6a980b1@mail.gmail.com>
References: <5e6a2e670907310255k468abb49p5a773f1e7d3e251a@mail.gmail.com>
	<4A72C94A.8080102@braverock.com>
	<5e6a2e670907310445o4d43e46m4693a9bfa6a980b1@mail.gmail.com>
Message-ID: <8ec76080907310529n2a20405el9e3e68f175b8d7c5@mail.gmail.com>

if you have high quality data (i.e. no missing vol and oi data), then
the problem isn't difficult.

I'm not sure what the case is with IB, but quite a few futures data
providers have very spotty data then you have to write a lot of code
to deal with the bad data.

typical problems that I've seen from our data provider are:
1) future contracts that do not overlap in time
2) OI data that is missing for some or all contracts
3) OI data that is represented as the total OI for all contracts (but
appended to each contract)
4) contracts that were trading but not the active contract that have
data on holidays (but the active contract doesn't have this date).
This is a problem b/c to create the scratch space for the continuous
series the easiest way to calc the total rows is
unique(unlist(lapply(x,dates))), so you wind up with extra dates b/c
of the bad data.

I have a package for this that's not quite complete.  I'll post it to
github shortly.

-Whit


On Fri, Jul 31, 2009 at 7:45 AM, Mark Breman<breman.mark at gmail.com> wrote:
> Hi Brian,
> Yes I think you are right; the roll strategy can differ for different types
> of continuous contracts.
>
> I think that Ideally you would have a function/package that:
>
> 1) allows the creation of the continuous series to be based on a specified
> roll strategy
> 2) it should be data-source independant (thus expiration dates should also
> be specified)
>
> The problem I face is that my current data provider (IB) does not offer
> continuous contracts...
>
> Regards,
>
> -Mark-
>
>
> 2009/7/31 Brian G. Peterson <brian at braverock.com>
>
>> Mark Breman wrote:
>>
>>> Hi,
>>> I have been looking for existing R code to create a continuous futures
>>> series from individual futures contract series, but have not found
>>> anything
>>> (yet).
>>>
>>> Is there really nothing out there?
>>>
>>> Kind regards,
>>>
>>>
>> Mark,
>>
>> I think the biggest issue is that the roll can happen on any number of
>> different criteria. ?Volume Cross, Midpoint Roll, Date (expiration-n) Roll,
>> some other method, etc. ?Also, any data provider that you're already paying
>> for data (Bloomberg, Reuters, CQG, QAI, etc.) will already have one or more
>> continuous series methods available, making the potential R code even less
>> useful, and probably specific to one data provider.
>> Another problem would be instrument/contract descriptors through time,
>> though this seems minor, R does not yet have an instrument model for
>> reference data, though we're working on that.
>>
>> Can you be a little more explicit about what you are trying to do? ?data
>> source/provider, roll method, etc?
>>
>> Regards,
>>
>> ? - Brian
>>
>> --
>> Brian G. Peterson
>> http://braverock.com/brian/
>> Ph: 773-459-4973
>> IM: bgpbraverock
>>
>>
>>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From breman.mark at gmail.com  Fri Jul 31 15:09:40 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Fri, 31 Jul 2009 15:09:40 +0200
Subject: [R-SIG-Finance] Continuous futures series with R
In-Reply-To: <8ec76080907310529n2a20405el9e3e68f175b8d7c5@mail.gmail.com>
References: <5e6a2e670907310255k468abb49p5a773f1e7d3e251a@mail.gmail.com>
	<4A72C94A.8080102@braverock.com>
	<5e6a2e670907310445o4d43e46m4693a9bfa6a980b1@mail.gmail.com>
	<8ec76080907310529n2a20405el9e3e68f175b8d7c5@mail.gmail.com>
Message-ID: <5e6a2e670907310609v62648595q94d8e1bea2a0ce26@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090731/e194e5ec/attachment.pl>

From eduard.pieterse at macquarie.com  Fri Jul 31 15:28:23 2009
From: eduard.pieterse at macquarie.com (ehxpieterse)
Date: Fri, 31 Jul 2009 06:28:23 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] getSymbols() date range
Message-ID: <24756400.post@talk.nabble.com>


Hi,

Using the following code, I can easily grab the S&P history:

from.dat <- as.Date("01/01/80", format="%m/%d/%y") 
to.dat <- as.Date("07/30/09", format="%m/%d/%y")
getSymbols("^GSPC", src="yahoo", from = from.dat, to = to.dat)

However, I would like to know how to grab all the history, without knowing
beforehand when it starts. Using getSymbols() without the date ranges, only
gives a few years' worth of history. Typically, the Dow started somwhere in
1920 and the S&P somewhere in 1970 (the data that is).

Also, out of interest, when I open an imported data set and view it using
data editor, I can only see the Series.open values. Is this a feature or am
I doing something strange?

Many thanks,
Eduard
-- 
View this message in context: http://www.nabble.com/getSymbols%28%29-date-range-tp24756400p24756400.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From armstrong.whit at gmail.com  Fri Jul 31 16:01:00 2009
From: armstrong.whit at gmail.com (Whit Armstrong)
Date: Fri, 31 Jul 2009 10:01:00 -0400
Subject: [R-SIG-Finance] Continuous futures series with R
In-Reply-To: <5e6a2e670907310609v62648595q94d8e1bea2a0ce26@mail.gmail.com>
References: <5e6a2e670907310255k468abb49p5a773f1e7d3e251a@mail.gmail.com>
	<4A72C94A.8080102@braverock.com>
	<5e6a2e670907310445o4d43e46m4693a9bfa6a980b1@mail.gmail.com>
	<8ec76080907310529n2a20405el9e3e68f175b8d7c5@mail.gmail.com>
	<5e6a2e670907310609v62648595q94d8e1bea2a0ce26@mail.gmail.com>
Message-ID: <8ec76080907310701s66da1642t2431f8f807d3f77d@mail.gmail.com>

the roll code will eventually be moved to c++.

for now it just implements one roll policy, which is keep the current
contract active until expiration.

http://github.com/armstrtw/RCommodity/tree/master

-Whit


On Fri, Jul 31, 2009 at 9:09 AM, Mark Breman<breman.mark at gmail.com> wrote:
> Thanks Whit. I have a look at it when it's available.
> Regards,
> -Mark-
>
> 2009/7/31 Whit Armstrong <armstrong.whit at gmail.com>
>>
>> if you have high quality data (i.e. no missing vol and oi data), then
>> the problem isn't difficult.
>>
>> I'm not sure what the case is with IB, but quite a few futures data
>> providers have very spotty data then you have to write a lot of code
>> to deal with the bad data.
>>
>> typical problems that I've seen from our data provider are:
>> 1) future contracts that do not overlap in time
>> 2) OI data that is missing for some or all contracts
>> 3) OI data that is represented as the total OI for all contracts (but
>> appended to each contract)
>> 4) contracts that were trading but not the active contract that have
>> data on holidays (but the active contract doesn't have this date).
>> This is a problem b/c to create the scratch space for the continuous
>> series the easiest way to calc the total rows is
>> unique(unlist(lapply(x,dates))), so you wind up with extra dates b/c
>> of the bad data.
>>
>> I have a package for this that's not quite complete. ?I'll post it to
>> github shortly.
>>
>> -Whit
>>
>>
>> On Fri, Jul 31, 2009 at 7:45 AM, Mark Breman<breman.mark at gmail.com> wrote:
>> > Hi Brian,
>> > Yes I think you are right; the roll strategy can differ for different
>> > types
>> > of continuous contracts.
>> >
>> > I think that Ideally you would have a function/package that:
>> >
>> > 1) allows the creation of the continuous series to be based on a
>> > specified
>> > roll strategy
>> > 2) it should be data-source independant (thus expiration dates should
>> > also
>> > be specified)
>> >
>> > The problem I face is that my current data provider (IB) does not offer
>> > continuous contracts...
>> >
>> > Regards,
>> >
>> > -Mark-
>> >
>> >
>> > 2009/7/31 Brian G. Peterson <brian at braverock.com>
>> >
>> >> Mark Breman wrote:
>> >>
>> >>> Hi,
>> >>> I have been looking for existing R code to create a continuous futures
>> >>> series from individual futures contract series, but have not found
>> >>> anything
>> >>> (yet).
>> >>>
>> >>> Is there really nothing out there?
>> >>>
>> >>> Kind regards,
>> >>>
>> >>>
>> >> Mark,
>> >>
>> >> I think the biggest issue is that the roll can happen on any number of
>> >> different criteria. ?Volume Cross, Midpoint Roll, Date (expiration-n)
>> >> Roll,
>> >> some other method, etc. ?Also, any data provider that you're already
>> >> paying
>> >> for data (Bloomberg, Reuters, CQG, QAI, etc.) will already have one or
>> >> more
>> >> continuous series methods available, making the potential R code even
>> >> less
>> >> useful, and probably specific to one data provider.
>> >> Another problem would be instrument/contract descriptors through time,
>> >> though this seems minor, R does not yet have an instrument model for
>> >> reference data, though we're working on that.
>> >>
>> >> Can you be a little more explicit about what you are trying to do?
>> >> ?data
>> >> source/provider, roll method, etc?
>> >>
>> >> Regards,
>> >>
>> >> ? - Brian
>> >>
>> >> --
>> >> Brian G. Peterson
>> >> http://braverock.com/brian/
>> >> Ph: 773-459-4973
>> >> IM: bgpbraverock
>> >>
>> >>
>> >>
>> >
>> > ? ? ? ?[[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>
>


From jeff.a.ryan at gmail.com  Fri Jul 31 16:11:07 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 31 Jul 2009 09:11:07 -0500
Subject: [R-SIG-Finance] [R-sig-finance] getSymbols() date range
In-Reply-To: <24756400.post@talk.nabble.com>
References: <24756400.post@talk.nabble.com>
Message-ID: <e8e755250907310711v3607671al2a81bafcb35fee1a@mail.gmail.com>

Eduard,

getSymbols( src="yahoo") will let you set any time as the from parameter.

getSymbols("^GSPC", from="1900-01-01") should do what you want.  The
'to' is always today's date.

HTH
Jeff

P.S. The data editor issue is one you should ask on R-help, as that
would have nothing to do with the quantmod/finance side of things.



On Fri, Jul 31, 2009 at 8:28 AM,
ehxpieterse<eduard.pieterse at macquarie.com> wrote:
>
> Hi,
>
> Using the following code, I can easily grab the S&P history:
>
> from.dat <- as.Date("01/01/80", format="%m/%d/%y")
> to.dat <- as.Date("07/30/09", format="%m/%d/%y")
> getSymbols("^GSPC", src="yahoo", from = from.dat, to = to.dat)
>
> However, I would like to know how to grab all the history, without knowing
> beforehand when it starts. Using getSymbols() without the date ranges, only
> gives a few years' worth of history. Typically, the Dow started somwhere in
> 1920 and the S&P somewhere in 1970 (the data that is).
>
> Also, out of interest, when I open an imported data set and view it using
> data editor, I can only see the Series.open values. Is this a feature or am
> I doing something strange?
>
> Many thanks,
> Eduard
> --
> View this message in context: http://www.nabble.com/getSymbols%28%29-date-range-tp24756400p24756400.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From englishinparis at aim.com  Fri Jul 31 16:18:41 2009
From: englishinparis at aim.com (englishinparis at aim.com)
Date: Fri, 31 Jul 2009 10:18:41 -0400
Subject: [R-SIG-Finance] Question about seasonal parameters in ARIMA model.
Message-ID: <8CBE000F0567FEC-B38-2EA9@webmail-dg04.sysops.aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090731/e78c2570/attachment.pl>

From brian at braverock.com  Fri Jul 31 16:39:31 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 31 Jul 2009 09:39:31 -0500
Subject: [R-SIG-Finance] Question about seasonal parameters in ARIMA
 model.
In-Reply-To: <8CBE000F0567FEC-B38-2EA9@webmail-dg04.sysops.aol.com>
References: <8CBE000F0567FEC-B38-2EA9@webmail-dg04.sysops.aol.com>
Message-ID: <1249051171.1946.9.camel@chiwwp616.breakwater.com>

This message belongs on r-help, not r-sig-finance, as it is not finance
related that I can see.

Regards,

    - Brian

On Fri, 2009-07-31 at 10:18 -0400, englishinparis at aim.com wrote:
> Hi all.
>  
> I am playing with monthly temperature data collected in Seattle from 1931 through 2009 in order to learn about the modeling and analysis of time series. I am using an ARIMA procedure. Beforehand, I ran a Partial Autocorrelation Function analysis and found the obvious winter/summer seasonality of termperatures.

> I am getting 

> Call: 
>  arima(x = temp, order = c(1, 1, 1), seasonal = list(order = c(1, 3, 
>  3), period = 1), 
>  ? ? include.mean = 1) 
>  
> 
> Coefficients: 
>  ? ? ? ? ? ar1 ? ? ?ma1 ? ? sar1 ? ?sma1 ? ? sma2 ? ?sma3 
>  ? ? ? -0.6912 ?-1.0000 ?-0.6912 ?0.0567 ?-0.8644 ?-0.192 
>  s.e. ? ? ?NaN ? 0.0029 ? 0.0069 ? ? NaN ? 0.0070 ? ? NaN 

> So I seem to have a very significant (negative) seasonal auto regressive (SAR1) effect. I would like to know how to interpret this coefficient in layman's terms. Is this is a statement about the temperatures of 2 consecutive months, or 2 nonconsecutive months? What is the difference between SAR1 and SAR2 in an ARIMA?
> 
> I greatly appreciate your help.


From Zeno.Adams at ebs.edu  Fri Jul 31 16:43:07 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Fri, 31 Jul 2009 16:43:07 +0200
Subject: [R-SIG-Finance] Question about seasonal parameters in ARIMA
	model.
In-Reply-To: <8CBE000F0567FEC-B38-2EA9@webmail-dg04.sysops.aol.com>
References: <8CBE000F0567FEC-B38-2EA9@webmail-dg04.sysops.aol.com>
Message-ID: <9064522880125945B98983BBAECBA1CC9855DB@exchsrv001.ebs.local>

Temperature data is not my field but I noticed one thing that appears odd:

You use three times differencing on the seasonal part which suggests some extreme form of nonstationarity in the seasonality. Normally, differencing once for the seasonality should be sufficient. You may want to check the optimal parameter, differencing, and lag setting by running several model specifications. The lowest AIC should tell you which specification to use. The following code (see more in "Applied Econometrics with R", Kleiber & Zeileis (2008)) should help:

optimal_par <- expand.grid(ar = 0:2, diff = 1, ma = 0:2, sar = 0:3, sdiff = 0:3, sma = 0:3)
opt_aic <- rep(0, nrow(optimal_par))
for(i in seq(along = opt_aic)) opt_aic[i] <- AIC(arima(temp,
unlist(optimal_par[i, 1:3]), unlist(optimal_par[i, 4:6])),
k = log(length(temp)))
optimal_par[which.min(opt_aic),]


Zeno




-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von englishinparis at aim.com
Gesendet: Freitag, 31. Juli 2009 16:19
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] Question about seasonal parameters in ARIMA model.

Hi all.

 
I am playing with monthly temperature data collected in Seattle from 1931 through 2009 in order to learn about the modeling and analysis of time series. I am using an ARIMA procedure. Beforehand, I ran a Partial Autocorrelation Function analysis and found the obvious winter/summer seasonality of termperatures.


 
 

I am getting 
 

Call: 
 arima(x = temp, order = c(1, 1, 1), seasonal = list(order = c(1, 3, 
 3), period = 1), 
 ? ? include.mean = 1) 
 

Coefficients: 
 ? ? ? ? ? ar1 ? ? ?ma1 ? ? sar1 ? ?sma1 ? ? sma2 ? ?sma3 
 ? ? ? -0.6912 ?-1.0000 ?-0.6912 ?0.0567 ?-0.8644 ?-0.192 
 s.e. ? ? ?NaN ? 0.0029 ? 0.0069 ? ? NaN ? 0.0070 ? ? NaN 



 

So I seem to have a very significant (negative) seasonal auto regressive (SAR1) effect. I would like to know how to interpret this coefficient in layman's terms. Is this is a statement about the temperatures of 2 consecutive months, or 2 nonconsecutive months? What is the difference between SAR1 and SAR2 in an ARIMA?





I greatly appreciate your help. 

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Praesident; Dr. Reimar Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Aufsichtsrat: Dr. Hellmut K. Albrecht, Vorsitzender

From brian at braverock.com  Fri Jul 31 17:07:35 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 31 Jul 2009 10:07:35 -0500
Subject: [R-SIG-Finance] [R-sig-finance] getSymbols() date range
In-Reply-To: <24756400.post@talk.nabble.com>
References: <24756400.post@talk.nabble.com>
Message-ID: <4A7308B7.9060408@braverock.com>

ehxpieterse wrote:
> Also, out of interest, when I open an imported data set and view it using data editor, I can only see the Series.open values. Is this a feature or am I doing something strange?
>   
I can't get data editor on Windows to work.  I never use it as a rule, 
but I tried just to see.  It is informative that the dialog box asks for 
a 'data frame or matrix'.  View() works fine on xts objects, so you 
might use that instead.

Regards,

   - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From burkett at uri.edu  Fri Jul 31 18:49:23 2009
From: burkett at uri.edu (John P. Burkett)
Date: Fri, 31 Jul 2009 12:49:23 -0400
Subject: [R-SIG-Finance] fPortfolio,  frontierSlider,  weightsSlider
Message-ID: <4A732093.2060204@uri.edu>

The package fPortfolio, when I used it a year ago, included a function 
frontierSlider, which worked well for me.  Judging by the April 20, 2009 
reference manual for fPortfolio, the old frontierSlider function has 
been replaced by a new weightsSlider function.

My attempt to follow the example on p. 57 of the reference manual goes 
as follows:
 > Data = SMALLCAP.RET
 > Data = Data[, c("BKE", "GG", "GYMB", "KRON")]
 > frontier = portfolioFrontier(Data)
 > weightsSlider(frontier)
Error in getTargetRisk(object)[, 1] : incorrect number of dimensions

My attempts to use weightsSlider with my own data end with the same 
error message.

Can anyone diagnosis the error message and/or suggest how to produce an 
interactive portfolio weights plot?

-John


-- 
John P. Burkett
Department of Economics
University of Rhode Island
Kingston, RI 02881-0808
USA

phone (401) 874-9195


From brian at braverock.com  Fri Jul 31 19:01:57 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 31 Jul 2009 12:01:57 -0500
Subject: [R-SIG-Finance] fPortfolio,  frontierSlider,  weightsSlider
In-Reply-To: <4A732093.2060204@uri.edu>
References: <4A732093.2060204@uri.edu>
Message-ID: <4A732385.1020809@braverock.com>

John P. Burkett wrote:
> The package fPortfolio, when I used it a year ago, included a function 
> frontierSlider, which worked well for me.  Judging by the April 20, 
> 2009 reference manual for fPortfolio, the old frontierSlider function 
> has been replaced by a new weightsSlider function.
>
> My attempt to follow the example on p. 57 of the reference manual goes 
> as follows:
> > Data = SMALLCAP.RET
> > Data = Data[, c("BKE", "GG", "GYMB", "KRON")]
> > frontier = portfolioFrontier(Data)
> > weightsSlider(frontier)
> Error in getTargetRisk(object)[, 1] : incorrect number of dimensions
>
> My attempts to use weightsSlider with my own data end with the same 
> error message.
>
> Can anyone diagnosis the error message and/or suggest how to produce 
> an interactive portfolio weights plot?
>
> -John
>
>

Here's how to diagnose the problem and gather more information to share 
with the list:

 >debug(weightsSlider)
 >weightsSlider(frontier)
 
step through the function,
examine what it is doing. 
Go all the way to the failure point once.
Do it again, but this time pay attention as you get close to the failure 
point. 
Collect data to share here on the data being passed into the bit 
(getTargetRisk) that fails.
Examine what the getTargetRisk function normally expects.

Cheers,

  - Brian

-- 

Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From burkett at uri.edu  Fri Jul 31 19:59:36 2009
From: burkett at uri.edu (John P. Burkett)
Date: Fri, 31 Jul 2009 13:59:36 -0400
Subject: [R-SIG-Finance] fPortfolio,  frontierSlider,  weightsSlider
Message-ID: <4A733108.4010809@uri.edu>

Brian G. Peterson wrote:
 > John P. Burkett wrote:
 >> The package fPortfolio, when I used it a year ago, included a 
function frontierSlider, which worked well for me.  Judging by the April 
20, 2009 reference manual for fPortfolio, the old frontierSlider 
function has been replaced by a new weightsSlider function.
 >>
 >> My attempt to follow the example on p. 57 of the reference manual 
goes as follows:
 >> > Data = SMALLCAP.RET
 >> > Data = Data[, c("BKE", "GG", "GYMB", "KRON")]
 >> > frontier = portfolioFrontier(Data)
 >> > weightsSlider(frontier)
 >> Error in getTargetRisk(object)[, 1] : incorrect number of dimensions
 >>
 >> My attempts to use weightsSlider with my own data end with the same 
error message.
 >>
 >> Can anyone diagnosis the error message and/or suggest how to produce 
an interactive portfolio weights plot?
 >>
 >> -John
 >>
 >>
 >
 > Here's how to diagnose the problem and gather more information to 
share with the list:
 >
 >  >debug(weightsSlider)
 >  >weightsSlider(frontier)
 >
 > step through the function,
 > examine what it is doing. Go all the way to the failure point once.
 > Do it again, but this time pay attention as you get close to the 
failure point. Collect data to share here on the data being passed into 
the bit (getTargetRisk) that fails.
 > Examine what the getTargetRisk function normally expects.

Thank you, Brian, for your good suggestions. My attempt to use debug and 
the results are as follows:

 > debug(weightsSlider)
 > weightsSlider(frontier)
debugging in: weightsSlider(frontier)
debug: {
     object <<- object
     nFrontierPoints <- length(getTargetRisk(object)[, 1])
     dim = dim(getWeights(object))[2]

<snip>

         starts = c(Start))
     invisible()
}
Browse[1]> n
debug: object <<- object
Browse[1]>
debug: nFrontierPoints <- length(getTargetRisk(object)[, 1])
Browse[1]>
Error in getTargetRisk(object)[, 1] : incorrect number of dimensions
 >

Trying to learn what the getTargetRisk function normally expect, I did
?getTargetRisk
eliciting the following response:
getDefault            package:fPortfolio            R Documentation
Extractor Functions
Description:
      Extractor functions to get information from objects of class
      fPFOLIODATA, fPFOLIOSPEC, fPFOLIODATA, fPFOLIOVAL, and fPORTFOLIO.
Usage:
      getConstraints(object)
<snip>
      getTargetRisk(object)
<snip>
Arguments:
   object: an object of class 'fPFOLIODATA', 'fPFOLIOSPEC' or
           'fPORTFOLIO'.
      ...: optional arguments to be passed.

Doing "getTargetRisk" got this response:
function (object)
{
     UseMethod("getTargetRisk")
}
<environment: namespace:fPortfolio>

I'd be grateful for help in interpreting the above results and/or 
suggestions for obtaining more information.

Best regards,
John

-- 
John P. Burkett
Department of Economics
University of Rhode Island
Kingston, RI 02881-0808
USA

phone (401) 874-9195


From eduard.pieterse at macquarie.com  Mon Aug  3 12:55:50 2009
From: eduard.pieterse at macquarie.com (ehxpieterse)
Date: Mon, 3 Aug 2009 03:55:50 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] n-period return
Message-ID: <24788735.post@talk.nabble.com>


Apologies if this has been asked before.

I am looking for a function to calculate the n-period return for a time
series. For example, the user would specify 11, 90 or 180 and then generate
an accompanying n-period delta.

Thanks in advance.
Eduard 
-- 
View this message in context: http://www.nabble.com/n-period-return-tp24788735p24788735.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Mon Aug  3 14:37:56 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 3 Aug 2009 07:37:56 -0500
Subject: [R-SIG-Finance] [R-sig-finance] n-period return
In-Reply-To: <24788735.post@talk.nabble.com>
References: <24788735.post@talk.nabble.com>
Message-ID: <e8e755250908030537l70d6b3f5q95bae4dd6cb1a185@mail.gmail.com>

Try:

?diff

library(quantmod)
getSymbols("AAPL")
AAPL.Cl <- Cl(AAPL)

diff(log(AAPL.Cl), lag=11)
diff(log(AAPL.Cl), lag=90)
diff(log(AAPL.Cl), lag=180)

Or using quantmod's Delt:

Delt(Cl(AAPL), k=c(11,90,180), type="log")

HTH
Jeff

On Mon, Aug 3, 2009 at 5:55 AM,
ehxpieterse<eduard.pieterse at macquarie.com> wrote:
>
> Apologies if this has been asked before.
>
> I am looking for a function to calculate the n-period return for a time
> series. For example, the user would specify 11, 90 or 180 and then generate
> an accompanying n-period delta.
>
> Thanks in advance.
> Eduard
> --
> View this message in context: http://www.nabble.com/n-period-return-tp24788735p24788735.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From brian at braverock.com  Mon Aug  3 14:38:54 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 03 Aug 2009 07:38:54 -0500
Subject: [R-SIG-Finance] [R-sig-finance] n-period return
In-Reply-To: <24788735.post@talk.nabble.com>
References: <24788735.post@talk.nabble.com>
Message-ID: <4A76DA5E.301@braverock.com>

ehxpieterse wrote:
> Apologies if this has been asked before.
>
> I am looking for a function to calculate the n-period return for a time
> series. For example, the user would specify 11, 90 or 180 and then generate
> an accompanying n-period delta.
>   
See to.period() in xts to decompose.  Also see period.apply()

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From eduard.pieterse at macquarie.com  Mon Aug  3 17:55:39 2009
From: eduard.pieterse at macquarie.com (ehxpieterse)
Date: Mon, 3 Aug 2009 08:55:39 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] n-period return
In-Reply-To: <e8e755250908030537l70d6b3f5q95bae4dd6cb1a185@mail.gmail.com>
References: <24788735.post@talk.nabble.com>
	<e8e755250908030537l70d6b3f5q95bae4dd6cb1a185@mail.gmail.com>
Message-ID: <24793238.post@talk.nabble.com>


Thanks Jeff,

I'm not too clear on how to use the GetSeries data type, after using diff().

getSymbols("^GSPC", src="yahoo") 
CloseData <- Cl(GSPC) 
Delta <- diff(CloseData, lag=1) 
for (i in 3:length(Delta)) { 
 if (Delta[i]>Delta[i-1]) sum <- sum + Delta   
}

It seems if the variables have no length? Is this something specific to the
way getSymbols create objects?
-- 
View this message in context: http://www.nabble.com/n-period-return-tp24788735p24793238.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From josh.m.ulrich at gmail.com  Mon Aug  3 18:10:13 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Mon, 3 Aug 2009 11:10:13 -0500
Subject: [R-SIG-Finance] [R-sig-finance] n-period return
In-Reply-To: <24793238.post@talk.nabble.com>
References: <24788735.post@talk.nabble.com>
	<e8e755250908030537l70d6b3f5q95bae4dd6cb1a185@mail.gmail.com> 
	<24793238.post@talk.nabble.com>
Message-ID: <8cca69990908030910l32327fbai1731d19b32b00dbb@mail.gmail.com>

This is because xts aligns the series before operations.
(Delta[i]>Delta[i-1]) returns an empty xts object because Delta[i] and
Delta[i-1] don't share any common index values.

Something like this will work:
> lDelta <- lag(Delta)
> for (i in 3:length(Delta)) {
+  if (Delta[i]>lDelta[i]) sum <- sum + Delta
+ }

Though you're going to have the same problem when you try to add "sum"
and "Delta" if they're both xts objects.

HTH,
Josh
--
http://www.fosstrading.com



On Mon, Aug 3, 2009 at 10:55 AM,
ehxpieterse<eduard.pieterse at macquarie.com> wrote:
>
> Thanks Jeff,
>
> I'm not too clear on how to use the GetSeries data type, after using diff().
>
> getSymbols("^GSPC", src="yahoo")
> CloseData <- Cl(GSPC)
> Delta <- diff(CloseData, lag=1)
> for (i in 3:length(Delta)) {
> ?if (Delta[i]>Delta[i-1]) sum <- sum + Delta
> }
>
> It seems if the variables have no length? Is this something specific to the
> way getSymbols create objects?
> --
> View this message in context: http://www.nabble.com/n-period-return-tp24788735p24793238.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From eduard.pieterse at macquarie.com  Tue Aug  4 14:41:09 2009
From: eduard.pieterse at macquarie.com (ehxpieterse)
Date: Tue, 4 Aug 2009 05:41:09 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Moving volatility
Message-ID: <24807675.post@talk.nabble.com>


Hi,

I have found a function online to calculate moving volatility. I am aware of
addVolatility in the quantmod package, but that only adds the vol to a
graph. Does any one know if there exists a better function to use than the
one shown below? I find the current one quite slow when working with large
data sets.

movsd <- function(series,lag) 
{ 
movingsd <- vector(mode="numeric") 
for (i in lag:length(series))

	{
	movingsd[i] <- sd(series[(i-lag+1):i])
	}

assign("movingsd",movingsd,.GlobalEnv) 
}

to.dat <- as.Date(Sys.Date(), format="%m/%d/%y")
getSymbols("^GSPC", src="yahoo", from = "2000-01-01", to = to.dat)
CloseData <- Cl(GSPC)

x <- movsd(Delt(CloseData),40)
xx <- x*100
plot(xx, type="l")

-- 
View this message in context: http://www.nabble.com/Moving-volatility-tp24807675p24807675.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From shane.conway at gmail.com  Tue Aug  4 15:08:35 2009
From: shane.conway at gmail.com (Shane Conway)
Date: Tue, 4 Aug 2009 09:08:35 -0400
Subject: [R-SIG-Finance] [R-sig-finance] Moving volatility
In-Reply-To: <24807675.post@talk.nabble.com>
References: <24807675.post@talk.nabble.com>
Message-ID: <dd3243090908040608y4742f7c3k1a65258d61aa7b0e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090804/67007301/attachment.pl>

From ggrothendieck at gmail.com  Tue Aug  4 15:13:18 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 4 Aug 2009 09:13:18 -0400
Subject: [R-SIG-Finance] [R-sig-finance] Moving volatility
In-Reply-To: <24807675.post@talk.nabble.com>
References: <24807675.post@talk.nabble.com>
Message-ID: <971536df0908040613u46ef1cf9xeba79f05b3958cd3@mail.gmail.com>

There is also rollapply in the zoo package:

set.seed(123)
library(zoo)
x <- zoo(rnorm(25))
z1 <- rollapply(x, 5, sd)

or using rollmean (which is faster)

z2 <- sqrt(5/4 * (rollmean(x*x,5) - rollmean(x,5)^2))

all.equal(z1, z2) # TRUE

On Tue, Aug 4, 2009 at 8:41 AM,
ehxpieterse<eduard.pieterse at macquarie.com> wrote:
>
> Hi,
>
> I have found a function online to calculate moving volatility. I am aware of
> addVolatility in the quantmod package, but that only adds the vol to a
> graph. Does any one know if there exists a better function to use than the
> one shown below? I find the current one quite slow when working with large
> data sets.
>
> movsd <- function(series,lag)
> {
> movingsd <- vector(mode="numeric")
> for (i in lag:length(series))
>
> ? ? ? ?{
> ? ? ? ?movingsd[i] <- sd(series[(i-lag+1):i])
> ? ? ? ?}
>
> assign("movingsd",movingsd,.GlobalEnv)
> }
>
> to.dat <- as.Date(Sys.Date(), format="%m/%d/%y")
> getSymbols("^GSPC", src="yahoo", from = "2000-01-01", to = to.dat)
> CloseData <- Cl(GSPC)
>
> x <- movsd(Delt(CloseData),40)
> xx <- x*100
> plot(xx, type="l")
>
> --
> View this message in context: http://www.nabble.com/Moving-volatility-tp24807675p24807675.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From josh.m.ulrich at gmail.com  Tue Aug  4 18:47:51 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Tue, 4 Aug 2009 11:47:51 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Moving volatility
In-Reply-To: <dd3243090908040608y4742f7c3k1a65258d61aa7b0e@mail.gmail.com>
References: <24807675.post@talk.nabble.com>
	<dd3243090908040608y4742f7c3k1a65258d61aa7b0e@mail.gmail.com>
Message-ID: <8cca69990908040947t528b2e8bq4a79d3334b7ef079@mail.gmail.com>

Eduard,

Please read the documentation.  ?addVolatility notes that it uses the
volatility function in TTR.

TTR's moving window functions (see ?runFun) call compiled code, so
they are very fast.  They also use xts internally, so they accept and
return a variety of time series classes (ts, zoo, timeSeries, etc.).

Best,
Josh
--
http://www.fosstrading.com



On Tue, Aug 4, 2009 at 8:08 AM, Shane Conway<shane.conway at gmail.com> wrote:
> You can use the rollVar() function in rmetrics or the volatility() function
> in TTR.
>
> On Tue, Aug 4, 2009 at 8:41 AM, ehxpieterse
> <eduard.pieterse at macquarie.com>wrote:
>
>>
>> Hi,
>>
>> I have found a function online to calculate moving volatility. I am aware
>> of
>> addVolatility in the quantmod package, but that only adds the vol to a
>> graph. Does any one know if there exists a better function to use than the
>> one shown below? I find the current one quite slow when working with large
>> data sets.
>>
>> movsd <- function(series,lag)
>> {
>> movingsd <- vector(mode="numeric")
>> for (i in lag:length(series))
>>
>> ? ? ? ?{
>> ? ? ? ?movingsd[i] <- sd(series[(i-lag+1):i])
>> ? ? ? ?}
>>
>> assign("movingsd",movingsd,.GlobalEnv)
>> }
>>
>> to.dat <- as.Date(Sys.Date(), format="%m/%d/%y")
>> getSymbols("^GSPC", src="yahoo", from = "2000-01-01", to = to.dat)
>> CloseData <- Cl(GSPC)
>>
>> x <- movsd(Delt(CloseData),40)
>> xx <- x*100
>> plot(xx, type="l")
>>
>> --
>> View this message in context:
>> http://www.nabble.com/Moving-volatility-tp24807675p24807675.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From nands31 at gmail.com  Tue Aug  4 21:06:17 2009
From: nands31 at gmail.com (Subhrangshu Nandi)
Date: Tue, 4 Aug 2009 14:06:17 -0500
Subject: [R-SIG-Finance] Quantmod charting options
Message-ID: <de69a2b90908041206u3a491ab2g584f65ddc3d3ae80@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090804/1edcb7d5/attachment.pl>

From comtech.usa at gmail.com  Thu Aug  6 05:48:10 2009
From: comtech.usa at gmail.com (Michael)
Date: Wed, 5 Aug 2009 20:48:10 -0700
Subject: [R-SIG-Finance] Forecasting FX using R?
Message-ID: <b1f16d9d0908052048hbe73da0x819302ea9febdc07@mail.gmail.com>

Hi all,

Could anybody give me some good pointers about forecasting, predicting
and trading FX? I am thinking of using some data-mining methods to
predict the FX movement... Could anybody point me to some success
stories and codes using R?

Thanks a lot!


From patrick at burns-stat.com  Thu Aug  6 08:58:21 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 06 Aug 2009 07:58:21 +0100
Subject: [R-SIG-Finance] Forecasting FX using R?
In-Reply-To: <b1f16d9d0908052048hbe73da0x819302ea9febdc07@mail.gmail.com>
References: <b1f16d9d0908052048hbe73da0x819302ea9febdc07@mail.gmail.com>
Message-ID: <4A7A7F0D.4060703@burns-stat.com>

Those who say don't know.
Those who know don't say.

     Lao-tzu, Tao Te Ching



Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of "The R Inferno" and "A Guide for the Unwilling S User")

Michael wrote:
> Hi all,
> 
> Could anybody give me some good pointers about forecasting, predicting
> and trading FX? I am thinking of using some data-mining methods to
> predict the FX movement... Could anybody point me to some success
> stories and codes using R?
> 
> Thanks a lot!
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
>


From eduard.pieterse at macquarie.com  Thu Aug  6 10:40:50 2009
From: eduard.pieterse at macquarie.com (ehxpieterse)
Date: Thu, 6 Aug 2009 01:40:50 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Array to xts/zoo
Message-ID: <24841032.post@talk.nabble.com>


Hi,

I have an array which holds the cumulative profit of a trading strategy over
time. I use raw data of class xts/zoo by using quantmod's
getSymbols("^DJI").

My question: How do I transform my array to a zoo/xts class, by also using
the date formatting in the raw data? The array shows the corresponding
profit/loss for each day of data downloaded. Both my array and the raw data
from getSymbols have the same number of elements.

Thanks in advance.
Eduard
-- 
View this message in context: http://www.nabble.com/Array-to-xts-zoo-tp24841032p24841032.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From brian at braverock.com  Thu Aug  6 13:44:12 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 06 Aug 2009 06:44:12 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Array to xts/zoo
In-Reply-To: <24841032.post@talk.nabble.com>
References: <24841032.post@talk.nabble.com>
Message-ID: <4A7AC20C.1070809@braverock.com>

ehxpieterse wrote:
> Hi,
>
> I have an array which holds the cumulative profit of a trading strategy over
> time. I use raw data of class xts/zoo by using quantmod's
> getSymbols("^DJI").
>
> My question: How do I transform my array to a zoo/xts class, by also using
> the date formatting in the raw data? The array shows the corresponding
> profit/loss for each day of data downloaded. Both my array and the raw data
> from getSymbols have the same number of elements.
>
> Thanks in advance.
> Eduard
>   
may I suggest chapter 9 of  
http://www.burns-stat.com/pages/Tutor/R_inferno.pdf

failing that, try cbind()

In the future, I suggest that when dealing with timeseries, keep *all* 
of your data conveniently indexed by time.

Cheers,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From mdowle at mdowle.plus.com  Fri Aug  7 08:51:02 2009
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Fri, 7 Aug 2009 07:51:02 +0100
Subject: [R-SIG-Finance] Video demo of using svSocket with data.table
Message-ID: <h5gj0k$3kl$1@ger.gmane.org>

Hi,
If you haven't already seen this then : 
http://www.youtube.com/watch?v=rvT8XThGA8o
There is no audio, just typing at the console. Please press the HD button 
and maximise.  Its about 8 mins.
Comments/feedback appreciated.
Regards, Matthew


From eduard.pieterse at macquarie.com  Fri Aug  7 15:17:52 2009
From: eduard.pieterse at macquarie.com (ehxpieterse)
Date: Fri, 7 Aug 2009 06:17:52 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Importing intra day data
Message-ID: <24864718.post@talk.nabble.com>


Hi,

I am reading intra day data into R using:

EWW <- read.table("EWW.txt", sep=",", header =TRUE)

EWW text file:

"Date","Time","Open","High","Low","Close","Volume"
07/01/1998,10:07:00,12.63,12.63,12.63,12.63,500
07/01/1998,10:27:00,12.75,12.75,12.75,12.75,100
07/01/1998,10:29:00,12.75,12.75,12.75,12.75,100
07/01/1998,11:07:00,12.75,12.75,12.75,12.75,100
07/01/1998,13:49:00,12.75,12.75,12.75,12.75,1000
07/01/1998,14:15:00,12.94,12.94,12.94,12.94,600

I think I am having trouble converting the data to xts type, due to the date
and time fields being contained in seperate variables. 

> EWW.xts <- as.xts(EWW)
Error in as.POSIXlt.character(x, tz, ...) : 
  character string is not in a standard unambiguous format

Has anyone come across a similar problem before?

Thanks,
Eduard

-- 
View this message in context: http://www.nabble.com/Importing-intra-day-data-tp24864718p24864718.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Fri Aug  7 16:28:38 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 7 Aug 2009 09:28:38 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Importing intra day data
In-Reply-To: <24864718.post@talk.nabble.com>
References: <24864718.post@talk.nabble.com>
Message-ID: <e8e755250908070728k4130d741xa3387896b3f768b0@mail.gmail.com>

The answer lies in the error:


> 07/01/1998,14:15:00,12.94,12.94,12.94,12.94,600
>
> I think I am having trouble converting the data to xts type, due to the date
> and time fields being contained in seperate variables.
>
>> EWW.xts <- as.xts(EWW)
> Error in as.POSIXlt.character(x, tz, ...) :
> ?character string is not in a standard unambiguous format
>

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
07/01/1998 is ambiguous, and not ISO 8601.
http://en.wikipedia.org/wiki/ISO_8601

CCYY-MM-DD would be correct, and CCYY-MM-DD HH:MM:SS is preferred.

try something like:

> xts(EWW[,-c(1,2)], as.POSIXct(strptime(paste(EWW$Date,EWW$Time),"%m/%d/%Y %H:%M:%S")))
                     Open  High   Low Close Volume
1998-07-01 10:07:00 12.63 12.63 12.63 12.63    500
1998-07-01 10:27:00 12.75 12.75 12.75 12.75    100
1998-07-01 10:29:00 12.75 12.75 12.75 12.75    100
1998-07-01 11:07:00 12.75 12.75 12.75 12.75    100
1998-07-01 13:49:00 12.75 12.75 12.75 12.75   1000
1998-07-01 14:15:00 12.94 12.94 12.94 12.94    600

HTH
Jeff



> Has anyone come across a similar problem before?
>
> Thanks,
> Eduard
>
> --
> View this message in context: http://www.nabble.com/Importing-intra-day-data-tp24864718p24864718.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From brian at braverock.com  Fri Aug  7 16:32:46 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 07 Aug 2009 09:32:46 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Importing intra day data
In-Reply-To: <24864718.post@talk.nabble.com>
References: <24864718.post@talk.nabble.com>
Message-ID: <4A7C3B0E.6090401@braverock.com>

ehxpieterse wrote:
> I think I am having trouble converting the data to xts type, due to the date and time fields being contained in seperate variables. 
>   
This question has been asked and answered multiple times on this list.

Please *search the archives* before posting.

See: http://catb.org/esr/faqs/smart-questions.html

Regards,

    - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From brian at braverock.com  Fri Aug  7 20:31:57 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 07 Aug 2009 13:31:57 -0500
Subject: [R-SIG-Finance] Quantmod charting options
In-Reply-To: <de69a2b90908041206u3a491ab2g584f65ddc3d3ae80@mail.gmail.com>
References: <de69a2b90908041206u3a491ab2g584f65ddc3d3ae80@mail.gmail.com>
Message-ID: <4A7C731D.5090304@braverock.com>

Subhrangshu Nandi wrote:
> I have a financial timeseries xts object A.xts, not in an OHLC format, but with multiple columns. I am doing the following:
> *# Step 1*
> *chartSeries(A.xts$Col1, name="User defined name")*
> *
> *
> *# Step 2*
> *addTA(A.xts$Col2, on=1)*
> *
> *
> *# Step 3*
> *addTA(cumsum(A.xts$Col3, col="red")*
>
> I would like to do the following to my charts:
>
>    1. I would like to remove the displaying of the last value of the time series, in the topleft corner.
>    2. I would like to be able to add a legend after step 2, with user defined names.
>    3. I would also like to be able to add subtitles to both or one of the charts
>    4. I would also like to control the font sizes of the x & y-axis
>   
What I've figured out on this so far:

- for panels other than the main panel, using the 'legend' parameter for 
addTA will overwrite the label in the upper left corner of the panel.
- in the main panel, using the 'legend' parameter will write the legend 
on top of the Last value display, but they will be superimposed and 
unreadable

There does not seem to be a chartSeries/quantmod equivalent of the 
legend() function used for plot() that could name each series and 
provide a color coded key.

Regards,

    - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From me at censix.com  Sat Aug  8 11:04:01 2009
From: me at censix.com (me at censix.com)
Date: Sat, 8 Aug 2009 11:04:01 +0200 (CEST)
Subject: [R-SIG-Finance] How to feed minvariancePortfolio with ones own
 covariance matrix (sigma) and means (mu)
Message-ID: <59839.202.86.21.250.1249722241.squirrel@censix.com>

Hi

I have been trying to use some of the efficient frontier functions (in
package fPortfolio) but I need to feed them with my own sigma (covariance
matrix) and mu (vecor of return means) since the source timeseries that
are usually passed as an argument to these functions are not available to
me. (sigma will approximately be a 500x500 matrix).

minvariancePortfolio()
maxreturnPortfolio()
portfolioFrontier()

After looking at the definitions of these functions it is still unclear to
me how to do this. It seems that I need to create an object of class
"fPFOLIODATA" using my own sigma and mu and then feed that object into the
functions above.

Any help on how to do this best is highly appreciated

Cheers

Soren


From marco.zanella at inbox.com  Mon Aug 10 20:03:35 2009
From: marco.zanella at inbox.com (Zanella Marco)
Date: Mon, 10 Aug 2009 10:03:35 -0800
Subject: [R-SIG-Finance] Simple and fast date format conversion
Message-ID: <7B8F5665FD4.00000184marco.zanella@inbox.com>

Gentleman,
I have to convert a date to this format: nov 2009

A) I've loaded a daily dataset of returns like this:
> ret[1,]
        Date Ret
1 20090729  3.08

B) Naturally, I've transformed Date with as.Date() and now I have:
> ret[1,]
        Date Ret
1 2009-07-29 3.08

Ok, now I want to analyse data also accorpated for months, and weeks, not only daily.
I think the ways to do this can be various:
1) In A), where Date is simply a number, truncate the last two digits of Date (es. 200907). Then I can subset data for month thanks to new Date's format.
2) In B), after Date conversion to date format, proceed with a new format conversion (es. lug 2009)

But, how I can do this with R? I tried with truncate() and strptime() but failed.

Thanks.

Marco

____________________________________________________________
GET FREE 5GB EMAIL - Check out spam free email with many cool features!
Visit http://www.inbox.com/email to find out more!


From brian at braverock.com  Mon Aug 10 20:18:06 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 10 Aug 2009 13:18:06 -0500
Subject: [R-SIG-Finance] Simple and fast date format conversion
In-Reply-To: <7B8F5665FD4.00000184marco.zanella@inbox.com>
References: <7B8F5665FD4.00000184marco.zanella@inbox.com>
Message-ID: <4A80645E.9040400@braverock.com>

Zanella Marco wrote:
> Gentleman,
> I have to convert a date to this format: nov 2009
>
> A) I've loaded a daily dataset of returns like this:
>   
>> ret[1,]
>>     
>         Date Ret
> 1 20090729  3.08
>
> B) Naturally, I've transformed Date with as.Date() and now I have:
>   
>> ret[1,]
>>     
>         Date Ret
> 1 2009-07-29 3.08
>
> Ok, now I want to analyse data also accorpated for months, and weeks, not only daily.
> I think the ways to do this can be various:
> 1) In A), where Date is simply a number, truncate the last two digits of Date (es. 200907). Then I can subset data for month thanks to new Date's format.
> 2) In B), after Date conversion to date format, proceed with a new format conversion (es. lug 2009)
>
> But, how I can do this with R? I tried with truncate() and strptime() but failed.
>   
You're working with returns, so you need to aggregate your returns.  You 
can't treat them like prices and simply grab the last price in your 
observation period.

Do you have a price (value) series available?  If not, convert it to a 
wealth index first.

With either price data or a wealth index, you can use to.period in xts 
to convert.  If you need returns for your analysis, the wealth index is 
fungible back to returns.

Further clarification would benefit from a sample data series.

Regards,

    - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From vfulco1 at gmail.com  Tue Aug 11 18:25:43 2009
From: vfulco1 at gmail.com (Vince Fulco)
Date: Tue, 11 Aug 2009 12:25:43 -0400
Subject: [R-SIG-Finance] IBrokers- how to keep target file from being
	re-written?
Message-ID: <34f2770f0908110925q33ed6995w1ba7ab3971d7fd13@mail.gmail.com>

For now, using reqHistoricalData() vs. the new wrapper reqHistory(),
setting endDateTime array and then looping thru writing to same file.
Func overwrites and attempt to add 'a'ppend qualifiers doesn't work.

dfile='/home/foo/'

now<- Sys.Date()-1

tp<- seq(now, length=6, by=paste(-5, 'days'))

# reformat dates to conform with IB needs YYYYMMDD

now2<- as.Date(tp, format="%Y%m%d")

reqHistoricalData(tws,
		    contract,
		    endDateTime=now2,
		    barSize = "5 mins",
		    duration = "5 D",
		    useRTH = "0",
		    whatToShow = "TRADES",
		    time.format = "1",
		    verbose = TRUE,
		    tickerId = "1",
		    #eventHistoricalData,
                    # this line fails
                    #file=paste(dfile,'EStest.dat',sep=''),open='a')
		    file=paste(dfile,'EStest.dat',sep=''))

* Would prefer to work from one file but could write out to numerous
and then merge.

Snippet from reqHistoricalData() suggests write.table would need to be
extended with 'append=TRUE' but sure there is something more
obvious...

#
if (!missing(file)) {
            cm[, 1] <- dts
            write.table(cm, file = file, quote = FALSE, row.names = FALSE,
                col.names = FALSE, sep = ",")
            invisible(return())
#


TIA, Vince


-- 
Vince Fulco, CFA, CAIA
612.424.5477 (universal)
vfulco1 at gmail.com

 A posse ad esse non valet consequentia

?the possibility does not necessarily lead to materialization?


From jeff.a.ryan at gmail.com  Tue Aug 11 19:07:02 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 11 Aug 2009 12:07:02 -0500
Subject: [R-SIG-Finance] IBrokers- how to keep target file from being
	re-written?
In-Reply-To: <34f2770f0908110925q33ed6995w1ba7ab3971d7fd13@mail.gmail.com>
References: <34f2770f0908110925q33ed6995w1ba7ab3971d7fd13@mail.gmail.com>
Message-ID: <e8e755250908111007n520e6c30v745b16bdc403b811@mail.gmail.com>

Hi Vince,

I should probably let a ... arg pass to write.table, but I'll have to
check that doesn't cause issues.

If you pass in an *open* file handle, you will likely get the output you want.

something like:

fh <- file("EStest.dat", open="a")
reqHistoricalData(tws, twsSTK("AAPL"), file=fh)


Alternatively, with reqHistory you can just let the function merge in
memory, and then use write.zoo to save to a csv file.

write.zoo(reqHistory(tws, twsSTK("AAPL")), file="AAPL.csv")
system("head AAPL.csv")
system("wc -l AAPL.csv")

HTH
Jeff

On Tue, Aug 11, 2009 at 11:25 AM, Vince Fulco<vfulco1 at gmail.com> wrote:
> For now, using reqHistoricalData() vs. the new wrapper reqHistory(),
> setting endDateTime array and then looping thru writing to same file.
> Func overwrites and attempt to add 'a'ppend qualifiers doesn't work.
>
> dfile='/home/foo/'
>
> now<- Sys.Date()-1
>
> tp<- seq(now, length=6, by=paste(-5, 'days'))
>
> # reformat dates to conform with IB needs YYYYMMDD
>
> now2<- as.Date(tp, format="%Y%m%d")
>
> reqHistoricalData(tws,
> ? ? ? ? ? ? ? ? ? ?contract,
> ? ? ? ? ? ? ? ? ? ?endDateTime=now2,
> ? ? ? ? ? ? ? ? ? ?barSize = "5 mins",
> ? ? ? ? ? ? ? ? ? ?duration = "5 D",
> ? ? ? ? ? ? ? ? ? ?useRTH = "0",
> ? ? ? ? ? ? ? ? ? ?whatToShow = "TRADES",
> ? ? ? ? ? ? ? ? ? ?time.format = "1",
> ? ? ? ? ? ? ? ? ? ?verbose = TRUE,
> ? ? ? ? ? ? ? ? ? ?tickerId = "1",
> ? ? ? ? ? ? ? ? ? ?#eventHistoricalData,
> ? ? ? ? ? ? ? ? ? ?# this line fails
> ? ? ? ? ? ? ? ? ? ?#file=paste(dfile,'EStest.dat',sep=''),open='a')
> ? ? ? ? ? ? ? ? ? ?file=paste(dfile,'EStest.dat',sep=''))
>
> * Would prefer to work from one file but could write out to numerous
> and then merge.
>
> Snippet from reqHistoricalData() suggests write.table would need to be
> extended with 'append=TRUE' but sure there is something more
> obvious...
>
> #
> if (!missing(file)) {
> ? ? ? ? ? ?cm[, 1] <- dts
> ? ? ? ? ? ?write.table(cm, file = file, quote = FALSE, row.names = FALSE,
> ? ? ? ? ? ? ? ?col.names = FALSE, sep = ",")
> ? ? ? ? ? ?invisible(return())
> #
>
>
> TIA, Vince
>
>
> --
> Vince Fulco, CFA, CAIA
> 612.424.5477 (universal)
> vfulco1 at gmail.com
>
> ?A posse ad esse non valet consequentia
>
> ?the possibility does not necessarily lead to materialization?
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From armstrong.whit at gmail.com  Thu Aug 13 15:50:23 2009
From: armstrong.whit at gmail.com (Whit Armstrong)
Date: Thu, 13 Aug 2009 09:50:23 -0400
Subject: [R-SIG-Finance] methods from Kim/Nelson "State-Space Models with
	Regime Switching"
Message-ID: <8ec76080908130650g2cf8656x5029cc40ebd0e88c@mail.gmail.com>

Has anyone implemented the methods from the Kim/Nelson book:
http://www.amazon.com/State-Space-Models-Regime-Switching-Gibbs-Sampling/dp/0262112388.

Gauss programs here:
http://www.econ.washington.edu/user/cnelson/markov/prgmlist.htm

Any suggestions for existing packages would be welcome.

So far, I see dse, dlm, MSVAR, and FKF.

Thanks,
Whit


From matthieu.stigler at gmail.com  Fri Aug 14 08:28:39 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Fri, 14 Aug 2009 08:28:39 +0200
Subject: [R-SIG-Finance] methods from Kim/Nelson "State-Space Models
	with Regime Switching"
In-Reply-To: <8ec76080908130650g2cf8656x5029cc40ebd0e88c@mail.gmail.com>
References: <8ec76080908130650g2cf8656x5029cc40ebd0e88c@mail.gmail.com>
Message-ID: <111060c20908132328y61499744s722f170cb168c9b9@mail.gmail.com>

well something similar has been done and presdented at user R 2009,
but as I know stil under dev and not released on CRAN, but have a look
and maybe contact the author:s

Estimating Markov-Switching Regression Models in R: An application to
model energy price in Spain

http://www2.agrocampus-ouest.fr/math/useR-2009/slides/Fontdecaba+SanchezEspigares+Munoz.pdf

Mat

2009/8/13 Whit Armstrong <armstrong.whit at gmail.com>:
> Has anyone implemented the methods from the Kim/Nelson book:
> http://www.amazon.com/State-Space-Models-Regime-Switching-Gibbs-Sampling/dp/0262112388.
>
> Gauss programs here:
> http://www.econ.washington.edu/user/cnelson/markov/prgmlist.htm
>
> Any suggestions for existing packages would be welcome.
>
> So far, I see dse, dlm, MSVAR, and FKF.
>
> Thanks,
> Whit
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From tzygmund at googlemail.com  Fri Aug 14 12:13:18 2009
From: tzygmund at googlemail.com (tzygmund mcfarlane)
Date: Fri, 14 Aug 2009 11:13:18 +0100
Subject: [R-SIG-Finance] Whittle estimation for ARMA models
Message-ID: <9c0c14910908140313rd074491qc4803da7f92762e7@mail.gmail.com>

<This is a repost from r-help; I was advised to repost it to the
r-sig-finance list.>

Hi,

Does anyone know of a package/script that will implement the Whittle
(1953) estimator for the parameters of an invertible stationary ARMA
time series model? The estimator is defined on, for example, pg. 378
of Brockwell & Davis (1991).

I assume that the internal call .whittle in this code due to Diethelm
Wuertz can be used, but I am unsure how:
http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/*checkout*/pkg/fArma/R/whittle.R?rev=2307&root=rmetrics

Barring this, could someone point me to a textbook example which I
could try to reproduce (using a publicly available dataset)?

Thanks

@article{whittle1953estimation,
 title={{Estimation and information in stationary time series}},
 author={Whittle, P.},
 journal={Arkiv f{\\"o}r Matematik},
 volume={2},
 number={5},
 pages={423--434},
 year={1953},
 publisher={Springer}
}

@book{brockwell1991time,
 title={{Time series: theory and methods}},
 author={Brockwell, P.J. and Davis, R.A.},
 year={1991},
 publisher={Springer}
}


From binabina at bellsouth.net  Sat Aug 15 03:19:39 2009
From: binabina at bellsouth.net (zubin)
Date: Fri, 14 Aug 2009 21:19:39 -0400
Subject: [R-SIG-Finance] getQuotes can it return seconds?
Message-ID: <4A860D2B.1060207@bellsouth.net>

Hello, using getQuote to return stock prices and storing into a 
database.  Running every 60 seconds. 

However, i noticed there are no seconds in the return time string, only 
minutes is the lowest grain.  Does getQuote provide or does yahoo show 
seconds?


ticker    time              open    bid    ask    volume
XLF    8/11/2009 1:00:00    14.2    13.79    13.8    74269144
XLF    8/11/2009 1:01:00    14.2    13.79    13.8    74390016
XLF    8/11/2009 1:01:00    14.2    13.79    13.8    74435440
XLF    8/11/2009 1:03:00    14.2    13.82    13.83    74907712
XLF    8/11/2009 1:04:00    14.2    13.82    13.83    74928528
XLF    8/11/2009 1:05:00    14.2    13.81    13.82    75067536
XLF    8/11/2009 1:06:00    14.2    13.81    13.82    75076752
XLF    8/11/2009 1:07:00    14.2    13.81    13.82    75219664
XLF    8/11/2009 1:08:00    14.2    13.82    13.83    75569120
XLF    8/11/2009 1:09:00    14.2    13.81    13.82    75663344
XLF    8/11/2009 1:10:00    14.2    13.81    13.82    75691920
XLF    8/11/2009 1:11:00    14.2    13.8    13.81    75777312
XLF    8/11/2009 1:12:00    14.2    13.8    13.81    75788296


From james at jtoll.com  Sat Aug 15 05:50:33 2009
From: james at jtoll.com (James Toll)
Date: Fri, 14 Aug 2009 21:50:33 -0600
Subject: [R-SIG-Finance] SD of simulated index market caps grows too quickly
Message-ID: <104365FE-9A48-46EA-83D2-F8C06E68DB96@jtoll.com>

Hi,

I'm trying to simulate an indexing strategy on an equity index with  
100 constituents.  I'm using mvrnorm from MASS to generate 10 years  
worth of data.  Because of the difficulty in generating a useable  
covariance matrix for Sigma, I've been trying to get by with a 100x100  
covariance matrix and using that to generate 100 time series.

The problem I'm experiencing is that over a ten year period, the  
standard deviation of the market caps in the index is exploding (e.g.  
200-400% increase, sometimes larger).  I don't have data to support  
this opinion, but I don't think this is very realistic of a typical  
index like, for example, the OEX.

My guess as to the problem is that an index, by its very nature, has a  
kind of built-in mean reversion due to the fact that equities that  
lose a significant amount of market cap will be shortly replaced by  
equities that are growing in market cap.  I believe that this would  
tend to prevent the kind of explosion in SD that I'm seeing in my model.

Short of expanding my matrix, and then periodically rebalancing with  
the largest 100, is there a way to control this expansion of the SD of  
the constituent market caps?

Thank you,

James


From ron_michael70 at yahoo.com  Sat Aug 15 17:58:29 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Sat, 15 Aug 2009 08:58:29 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
Message-ID: <24985789.post@talk.nabble.com>


Hi all, in Lutkepohl, page 250, I found that if there are stationary
variables in integrated system then they must be put in upper r-dimension.
My question is, is that the fact? If I do not do it, is there any problem in
estimation and interpretation? I have done few exercises and found that
parameter estimation is not infected with ordering (except IRF estimation).
Even is page 303 an example is presented wherein inflation rate variable is
taken as 2nd variable, although it looks like a stable process. 

Can anyone please clarify that? If really ordering is a problem in presence
of a stationary variable, can anyone provide me an example, with perhaps in
R-code so that I can regenerate? 

Thanks
-- 
View this message in context: http://www.nabble.com/A-question-on-VECM-tp24985789p24985789.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From rvince99 at earthlink.net  Sat Aug 15 19:41:09 2009
From: rvince99 at earthlink.net (R. Vince)
Date: Sat, 15 Aug 2009 13:41:09 -0400
Subject: [R-SIG-Finance] How to feed minvariancePortfolio with ones own
	covariance matrix (sigma) and means (mu)
References: <59839.202.86.21.250.1249722241.squirrel@censix.com>
Message-ID: <3A91D0C1D0284652988AB11A29CC5DBC@XP1>

 I too have been trying to use some of the efficient frontier functions (in 
package fPortfolio). I must be doing something very stupd. Working from the 
example (from the help file page, included herein below), when I go to 
create the Data object, I get:

> Data = SMALLCAP.RET
> Data = Data[, c("GOOG", "MSFT", "JNJ")]
Error: subscript out of bounds

I'm truly at a loss here. Can someone please point me in the right direction 
as to how to create the Data Object? I think there must be a mistake in the 
help file, or I'm just very confused about it. Thanks, RVince

------ Help File Page I am working from:------------------

efficientPortfolio(fPortfolio) R Documentation

Efficient Portfolios
Description
Returns efficient portfolios.

Usage
efficientPortfolio(data, spec = portfolioSpec(), constraints = "LongOnly")

maxratioPortfolio(data, spec = portfolioSpec(), constraints = "LongOnly")
tangencyPortfolio(data, spec = portfolioSpec(), constraints = "LongOnly")

minriskPortfolio(data, spec = portfolioSpec(), constraints = "LongOnly")
minvariancePortfolio(data, spec = portfolioSpec(), constraints = "LongOnly")

maxreturnPortfolio(data, spec = portfolioSpec(), constraints = "LongOnly")

Arguments
constraints a character string vector, containing the constraints of the 
form
"minW[asset]=percentage" for box constraints resp.
"maxsumW[assets]=percentage" for sector constraints.
data a multivariate time series described by an S4 object of class 
timeSeries. If your timeSerie is not a timeSeries object, consult the 
generic function as.timeSeries to convert your time series.
spec an S4 object of class fPFOLIOSPEC as returned by the function 
portfolioSpec.

Details
Efficient Portfolio:

An efficient portfolio is a portfolio which lies on the efficient frontier. 
The efficientPortfolio function returns the properties of the efficient 
portfolio as an S4 object of class fPORTFOLIO.

Minumum Risk or Tangency Portfolio:

The function tangencyPortfolio returns the portfolio with the highest 
return/risk ratio on the efficient frontier. For the Markowitz portfolio 
this is the same as the Sharpe ratio. To find this point on the frontier the 
return/risk ratio calculated from the target return and target risk returned 
by the function efficientPortfolio.

Global minimum risk or Minimum Variance Portfolio:

The function minvariancePortfolio returns the portfolio with the minimal 
risk on the efficient frontier. To find the minimal risk point the target 
risk returned by the function efficientPortfolio is minimized.

Maximum Return Portfolio:

The function maxreturnPortfolio returns the portfolio with the maximal 
return for a fixed target risk.

Value
returns an S4 object of class "fPORTFOLIO".

References
Wuertz, D., Chalabi, Y., Chen W., Ellis A. (2009); Portfolio Optimization 
with R/Rmetrics, Rmetrics eBook, Rmetrics Association and Finance Online, 
Zurich.

Examples
## data -
   Data = SMALLCAP.RET
   Data = Data[, c("BKE", "GG", "GYMB", "KRON")]
   Data

## spec -
   Spec = portfolioSpec()
   setTargetReturn(Spec) = mean(colMeans(Data))
   Spec

## constraints -
   Constraints = "LongOnly"
   Constraints

## efficientPortfolio -
   efficientPortfolio(Data, Spec, Constraints)

## tangency Portfolio -
   tangencyPortfolio(Data, Spec, Constraints)

## minvariancePortfolio -
   minvariancePortfolio(Data, Spec, Constraints)


From cedrick at cedrickjohnson.com  Sat Aug 15 21:59:45 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Sat, 15 Aug 2009 15:59:45 -0400
Subject: [R-SIG-Finance] How to feed minvariancePortfolio with ones own
 covariance matrix (sigma) and means (mu)
In-Reply-To: <3A91D0C1D0284652988AB11A29CC5DBC@XP1>
References: <59839.202.86.21.250.1249722241.squirrel@censix.com>
	<3A91D0C1D0284652988AB11A29CC5DBC@XP1>
Message-ID: <4A8713B1.7070308@cedrickjohnson.com>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090815/78564a70/attachment.html>

From multeesl at yahoo.co.uk  Tue Aug 18 17:25:25 2009
From: multeesl at yahoo.co.uk (Alex Chan)
Date: Tue, 18 Aug 2009 15:25:25 +0000 (GMT)
Subject: [R-SIG-Finance] cdf of skewed t distribution using fGARCH vs skewt
	package
Message-ID: <137956.45540.qm@web28607.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090818/2a72faf5/attachment.pl>

From brian at braverock.com  Tue Aug 18 17:51:07 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 18 Aug 2009 10:51:07 -0500
Subject: [R-SIG-Finance] cdf of skewed t distribution using fGARCH vs
 skewt	package
In-Reply-To: <137956.45540.qm@web28607.mail.ukl.yahoo.com>
References: <137956.45540.qm@web28607.mail.ukl.yahoo.com>
Message-ID: <4A8ACDEB.6010205@braverock.com>

Alex Chan wrote:
> I ran a AR(p)-GARCH (1,1) - skewed t using the fGARCH package. When I tried to use the sstd option in fgarch to return the cdf of the  standardized residuals z and compared it with the results obtained using the skewt package, I obtained different results even though both skewed t dist are based on Fernandez and steel. Since the standardized inovations are supposed to be iid with mean o and variance 1, i used   psstd(z, mean = 0, sd = 1, nu , xi ). Anyone can explain why this might be the case? 
>   
Please read the posting guide.  Include sample code and data if you want 
to maximize your chance of having someone help you figure out the 
(possible) problem you are having.

Depending on the data, the length of your data set, the degree of 
skewness, the random seed, coding issues, and a whole bunch of other 
factors, guessing at what the possible problem is is unlikely to be very 
useful.

Regards,

   - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From spencer.graves at prodsyse.com  Tue Aug 18 18:02:56 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Tue, 18 Aug 2009 09:02:56 -0700
Subject: [R-SIG-Finance] cdf of skewed t distribution using fGARCH vs
 skewt	package
In-Reply-To: <4A8ACDEB.6010205@braverock.com>
References: <137956.45540.qm@web28607.mail.ukl.yahoo.com>
	<4A8ACDEB.6010205@braverock.com>
Message-ID: <4A8AD0B0.7060209@prodsyse.com>

The posting guide is NOT distributed with R-SIG-Finance but is with 
r-help:  It's "www.R-project.org/posting-guide.html".  sg

Brian G. Peterson wrote:
> Alex Chan wrote:
>> I ran a AR(p)-GARCH (1,1) - skewed t using the fGARCH package. When I 
>> tried to use the sstd option in fgarch to return the cdf of the  
>> standardized residuals z and compared it with the results obtained 
>> using the skewt package, I obtained different results even though 
>> both skewed t dist are based on Fernandez and steel. Since the 
>> standardized inovations are supposed to be iid with mean o and 
>> variance 1, i used   psstd(z, mean = 0, sd = 1, nu , xi ). Anyone can 
>> explain why this might be the case?   
> Please read the posting guide.  Include sample code and data if you 
> want to maximize your chance of having someone help you figure out the 
> (possible) problem you are having.
>
> Depending on the data, the length of your data set, the degree of 
> skewness, the random seed, coding issues, and a whole bunch of other 
> factors, guessing at what the possible problem is is unlikely to be 
> very useful.
>
> Regards,
>
>   - Brian
>


-- 
Spencer Graves, PE, PhD
President and Chief Operating Officer
Structure Inspection and Monitoring, Inc.
751 Emerson Ct.
San Jos?, CA 95126
ph:  408-655-4567


From ron_michael70 at yahoo.com  Wed Aug 19 09:15:05 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Wed, 19 Aug 2009 00:15:05 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
In-Reply-To: <24985789.post@talk.nabble.com>
References: <24985789.post@talk.nabble.com>
Message-ID: <25038989.post@talk.nabble.com>


still no single reply. Should I need to design my query in better way?



RON70 wrote:
> 
> Hi all, in Lutkepohl, page 250, I found that if there are stationary
> variables in integrated system then they must be put in upper r-dimension.
> My question is, is that the fact? If I do not do it, is there any problem
> in estimation and interpretation? I have done few exercises and found that
> parameter estimation is not infected with ordering (except IRF
> estimation). Even is page 303 an example is presented wherein inflation
> rate variable is taken as 2nd variable, although it looks like a stable
> process. 
> 
> Can anyone please clarify that? If really ordering is a problem in
> presence of a stationary variable, can anyone provide me an example, with
> perhaps in R-code so that I can regenerate? 
> 
> Thanks
> 

-- 
View this message in context: http://www.nabble.com/A-question-on-VECM-tp24985789p25038989.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From hgeorgako at hotmail.com  Wed Aug 19 13:42:31 2009
From: hgeorgako at hotmail.com (Harry G)
Date: Wed, 19 Aug 2009 06:42:31 -0500
Subject: [R-SIG-Finance] Quantitative finance textbook with R examples
In-Reply-To: <mailman.5.1250676002.28372.r-sig-finance@stat.math.ethz.ch>
References: <mailman.5.1250676002.28372.r-sig-finance@stat.math.ethz.ch>
Message-ID: <BLU0-SMTP78D4D138EAE20893B6475ABAFE0@phx.gbl>

Teaching a class this fall and was wondering if there is a suitable  
graduate level text book out there with homework problems that utilize  
R.
Topics:  probability/statistics, portfolio theory, fixed income, risk.


From markleeds at verizon.net  Wed Aug 19 15:40:48 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 19 Aug 2009 08:40:48 -0500 (CDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
Message-ID: <311001457.99317.1250689248267.JavaMail.root@vms183.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090819/baf46110/attachment.html>

From frainj at tcd.ie  Wed Aug 19 16:26:19 2009
From: frainj at tcd.ie (John Frain)
Date: Wed, 19 Aug 2009 15:26:19 +0100
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
In-Reply-To: <25038989.post@talk.nabble.com>
References: <24985789.post@talk.nabble.com> <25038989.post@talk.nabble.com>
Message-ID: <cfdde1650908190726i60d9350bv734bc8397d9746b0@mail.gmail.com>

Lutkepohl is splitting up the system into r stationary components or
ecm's and n-r trends.  He uses this ordering to show that that this
can be done.  The stationary components and the ecm's only enter the
system at lag 1.  The stationary components do not and should not
enter the ecm's.  If you do have stationary variables this is testable
and may be imposed at estimation.

Best Regards

John

2009/8/19 RON70 <ron_michael70 at yahoo.com>:
>
> still no single reply. Should I need to design my query in better way?
>
>
>
> RON70 wrote:
>>
>> Hi all, in Lutkepohl, page 250, I found that if there are stationary
>> variables in integrated system then they must be put in upper r-dimension.
>> My quesltion is, is that the fact? If I do not do it, is there any problem
>> in estimation and interpretation? I have done few exercises and found that
>> parameter estimation is not infected with ordering (except IRF
>> estimation). Even is page 303 an example is presented wherein inflation
>> rate variable is taken as 2nd variable, although it looks like a stable
>> process.
>>
>> Can anyone please clarify that? If really ordering is a problem in
>> presence of a stationary variable, can anyone provide me an example, with
>> perhaps in R-code so that I can regenerate?
>>
>> Thanks
>>
>
> --
> View this message in context: http://www.nabble.com/A-question-on-VECM-tp24985789p25038989.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>



-- 
John C Frain, Ph.D.
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


From markleeds at verizon.net  Wed Aug 19 16:45:42 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 19 Aug 2009 09:45:42 -0500 (CDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
Message-ID: <269115748.107978.1250693142526.JavaMail.root@vms183.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090819/a775a5e0/attachment.html>

From markleeds at verizon.net  Wed Aug 19 16:50:48 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 19 Aug 2009 09:50:48 -0500 (CDT)
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
Message-ID: <1557159183.108656.1250693448367.JavaMail.root@vms183.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090819/cf59ff4a/attachment.html>

From bhupinder.juneja at gmail.com  Wed Aug 19 17:04:37 2009
From: bhupinder.juneja at gmail.com (Bhupinder Juneja)
Date: Wed, 19 Aug 2009 10:04:37 -0500
Subject: [R-SIG-Finance] Quantitative finance textbook with R examples
In-Reply-To: <BLU0-SMTP78D4D138EAE20893B6475ABAFE0@phx.gbl>
References: <mailman.5.1250676002.28372.r-sig-finance@stat.math.ethz.ch>
	<BLU0-SMTP78D4D138EAE20893B6475ABAFE0@phx.gbl>
Message-ID: <3ad2e32d0908190804g4be680e8l2b3d4ea6bd2025cf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090819/efc7c552/attachment.pl>

From wc90024-email at yahoo.com  Wed Aug 19 18:16:12 2009
From: wc90024-email at yahoo.com (wc90024-email at yahoo.com)
Date: Wed, 19 Aug 2009 09:16:12 -0700 (PDT)
Subject: [R-SIG-Finance] setting persistence upper limit in garchFit()
Message-ID: <12113.29593.qm@web30904.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090819/43c1d2bb/attachment.pl>

From patrick at burns-stat.com  Wed Aug 19 19:48:48 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed, 19 Aug 2009 18:48:48 +0100
Subject: [R-SIG-Finance] setting persistence upper limit in garchFit()
In-Reply-To: <12113.29593.qm@web30904.mail.mud.yahoo.com>
References: <12113.29593.qm@web30904.mail.mud.yahoo.com>
Message-ID: <4A8C3B00.6060700@burns-stat.com>

I don't know the answer to your question,
but I have a guess of what your data are
like.  The sum of the two parameters in
garch(1,1) is essentially telling you the
time it takes for the volatility from a
shock to damp down.  If there is a trend
in the volatility over the time frame of
the data, then the estimation is likely
to "think" that it hasn't seen the volatility
damp down -- hence an infinite waiting
time and a sum of the parameters more than 1.

More data can often help the problem.

Another piece of software whose existence
I'm doubtful of would be a Bayesian estimate
of the model.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of "The R Inferno" and "A Guide for the Unwilling S User")


wc90024-email at yahoo.com wrote:
> I'm using garchFit() on a volatile time series.   I'd like to set a limit such that the SUM(alpha, beta) < 1.   Is there a way to configure that by passing a parameter into garchFit()?   Or is there another way to do it?  Thanks.
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From brian at braverock.com  Wed Aug 19 20:09:43 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 19 Aug 2009 13:09:43 -0500
Subject: [R-SIG-Finance] setting persistence upper limit in garchFit()
In-Reply-To: <4A8C3B00.6060700@burns-stat.com>
References: <12113.29593.qm@web30904.mail.mud.yahoo.com>
	<4A8C3B00.6060700@burns-stat.com>
Message-ID: <4A8C3FE7.7090301@braverock.com>

Patrick Burns wrote:
> I don't know the answer to your question,
> but I have a guess of what your data are
> like.  The sum of the two parameters in
> garch(1,1) is essentially telling you the
> time it takes for the volatility from a
> shock to damp down.  If there is a trend
> in the volatility over the time frame of
> the data, then the estimation is likely
> to "think" that it hasn't seen the volatility
> damp down -- hence an infinite waiting
> time and a sum of the parameters more than 1.
>
> More data can often help the problem.
>
> Another piece of software whose existence
> I'm doubtful of would be a Bayesian estimate
> of the model.
>
http://cran.r-project.org/web/packages/bayesGARCH/index.html

perhaps?

   - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From frainj at tcd.ie  Thu Aug 20 11:56:48 2009
From: frainj at tcd.ie (John Frain)
Date: Thu, 20 Aug 2009 10:56:48 +0100
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
In-Reply-To: <1557159183.108656.1250693448367.JavaMail.root@vms183.mailsrvcs.net>
References: <1557159183.108656.1250693448367.JavaMail.root@vms183.mailsrvcs.net>
Message-ID: <cfdde1650908200256o5953411ejdb9c6078c16c5fe2@mail.gmail.com>

The Example on page 303 of Lutkepohl is an examination of
cointegration properties of an interest rate R and inflation.  If
inflation were denoted by say pi then everything would be relatively
familiar,  However inflation is denoted by Dp.  Both R and Dp (pi) are
are I(1) leading to the estimated cointegrating relationships (7.2.30)
or (7.2.31).

In the original Engle and Granger (1987) Econometrica article all the
elements of the non-stationary vector were of the same order of
integration.  The treatment of stationary variables was not explicit
but  needed to be added afterwards.  the treatment in Lutkepohl
includes this explicitly into the vecm system.

Best Regards

John

2009/8/19  <markleeds at verizon.net>:
> John: now that I think of it, you're right in that , as far as I remember,
> both variables need to
> be I(1) for there to be a possible cointegration relation so I'll have to
> check out pg 303
> closer. It doesn't make sense for R_t to be stationary and still be part of
> a cointegrating
> relation ? So, my bad and I'll go back to 303 and see what's going on there.
> Thanks for the
> clarifiication but now I'm actually more confused and don'thave the energy
> to dig through Lutkepohl at the moment.
>
>
> Mark
>
>
>
> On Aug 19, 2009, John Frain <frainj at tcd.ie> wrote:
>
> Lutkepohl is splitting up the system into r stationary components or
> ecm's and n-r trends. He uses this ordering to show that that this
> can be done. The stationary components and the ecm's only enter the
> system at lag 1. The stationary components do not and should not
> enter the ecm's. If you do have stationary variables this is testable
> and may be imposed at estimation.
>
> Best Regards
>
> John
>
> 2009/8/19 RON70 <ron_michael70 at yahoo.com>:
>>
>> still no single reply. Should I need to design my query in better way?
>>
>>
>>
>> RON70 wrote:
>>>
>>> Hi all, in Lutkepohl, page 250, I found that if there are stationary
>>> variables in integrated system then they must be put in upper
>>> r-dimension.
>>> My quesltion is, is that the fact? If I do not do it, is there any
>>> problem
>>> in estimation and interpretation? I have done few exercises and found
>>> that
>>> parameter estimation is not infected with ordering (except IRF
>>> estimation). Even is page 303 an example is presented wherein inflation
>>> rate variable is taken as 2nd variable, although it looks like a stable
>>> process.
>>>
>>> Can anyone please clarify that? If really ordering is a problem in
>>> presence of a stationary variable, can anyone provide me an example, with
>>> perhaps in R-code so that I can regenerate?
>>>
>>> Thanks
>>>
>>
>> --
>> View this message in context:
>> http://www.nabble.com/A-question-on-VECM-tp24985789p25038989.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
>
>
> --
> John C Frain, Ph.D.
> Trinity College Dublin
> Dublin 2
> Ireland
> www.tcd.ie/Economics/staff/frainj/home.htm
> mailto:frainj at tcd.ie
> mailto:frainj at gmail.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
John C Frain, Ph.D.
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


From matthieu.stigler at gmail.com  Thu Aug 20 13:25:52 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Thu, 20 Aug 2009 13:25:52 +0200
Subject: [R-SIG-Finance] [R-sig-finance] A question on VECM
In-Reply-To: <cfdde1650908200256o5953411ejdb9c6078c16c5fe2@mail.gmail.com>
References: <1557159183.108656.1250693448367.JavaMail.root@vms183.mailsrvcs.net>
	<cfdde1650908200256o5953411ejdb9c6078c16c5fe2@mail.gmail.com>
Message-ID: <111060c20908200425l95b03ecr5f1efd6404a05be0@mail.gmail.com>

I don't have neither Lutkephol nor Maddala and Kim at hand but if I
remember well there is a whole chapter (at least section) in Maddala
and Kim discussing how to deal with variables with different orders of
integration, and you may find there complementary informations
concerning I(0) variables into cointegrartion

Hope this helps

Matthieu

2009/8/20 John Frain <frainj at tcd.ie>:
> The Example on page 303 of Lutkepohl is an examination of
> cointegration properties of an interest rate R and inflation. ?If
> inflation were denoted by say pi then everything would be relatively
> familiar, ?However inflation is denoted by Dp. ?Both R and Dp (pi) are
> are I(1) leading to the estimated cointegrating relationships (7.2.30)
> or (7.2.31).
>
> In the original Engle and Granger (1987) Econometrica article all the
> elements of the non-stationary vector were of the same order of
> integration. ?The treatment of stationary variables was not explicit
> but ?needed to be added afterwards. ?the treatment in Lutkepohl
> includes this explicitly into the vecm system.
>
> Best Regards
>
> John
>
> 2009/8/19 ?<markleeds at verizon.net>:
>> John: now that I think of it, you're right in that , as far as I remember,
>> both variables need to
>> be I(1) for there to be a possible cointegration relation so I'll have to
>> check out pg 303
>> closer. It doesn't make sense for R_t to be stationary and still be part of
>> a cointegrating
>> relation ? So, my bad and I'll go back to 303 and see what's going on there.
>> Thanks for the
>> clarifiication but now I'm actually more confused and don'thave the energy
>> to dig through Lutkepohl at the moment.
>>
>>
>> Mark
>>
>>
>>
>> On Aug 19, 2009, John Frain <frainj at tcd.ie> wrote:
>>
>> Lutkepohl is splitting up the system into r stationary components or
>> ecm's and n-r trends. He uses this ordering to show that that this
>> can be done. The stationary components and the ecm's only enter the
>> system at lag 1. The stationary components do not and should not
>> enter the ecm's. If you do have stationary variables this is testable
>> and may be imposed at estimation.
>>
>> Best Regards
>>
>> John
>>
>> 2009/8/19 RON70 <ron_michael70 at yahoo.com>:
>>>
>>> still no single reply. Should I need to design my query in better way?
>>>
>>>
>>>
>>> RON70 wrote:
>>>>
>>>> Hi all, in Lutkepohl, page 250, I found that if there are stationary
>>>> variables in integrated system then they must be put in upper
>>>> r-dimension.
>>>> My quesltion is, is that the fact? If I do not do it, is there any
>>>> problem
>>>> in estimation and interpretation? I have done few exercises and found
>>>> that
>>>> parameter estimation is not infected with ordering (except IRF
>>>> estimation). Even is page 303 an example is presented wherein inflation
>>>> rate variable is taken as 2nd variable, although it looks like a stable
>>>> process.
>>>>
>>>> Can anyone please clarify that? If really ordering is a problem in
>>>> presence of a stationary variable, can anyone provide me an example, with
>>>> perhaps in R-code so that I can regenerate?
>>>>
>>>> Thanks
>>>>
>>>
>>> --
>>> View this message in context:
>>> http://www.nabble.com/A-question-on-VECM-tp24985789p25038989.html
>>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>
>>
>>
>>
>> --
>> John C Frain, Ph.D.
>> Trinity College Dublin
>> Dublin 2
>> Ireland
>> www.tcd.ie/Economics/staff/frainj/home.htm
>> mailto:frainj at tcd.ie
>> mailto:frainj at gmail.com
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> John C Frain, Ph.D.
> Trinity College Dublin
> Dublin 2
> Ireland
> www.tcd.ie/Economics/staff/frainj/home.htm
> mailto:frainj at tcd.ie
> mailto:frainj at gmail.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From sarswat at gmail.com  Thu Aug 20 13:53:32 2009
From: sarswat at gmail.com (sunil)
Date: Thu, 20 Aug 2009 17:23:32 +0530
Subject: [R-SIG-Finance]  passing fraction of seconds in xts object
Message-ID: <28fa0bac0908200453h51492dabia2da5b50db9b4563@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090820/851b2e50/attachment.pl>

From brian at braverock.com  Thu Aug 20 14:02:06 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 20 Aug 2009 07:02:06 -0500
Subject: [R-SIG-Finance] passing fraction of seconds in xts object
In-Reply-To: <28fa0bac0908200453h51492dabia2da5b50db9b4563@mail.gmail.com>
References: <28fa0bac0908200453h51492dabia2da5b50db9b4563@mail.gmail.com>
Message-ID: <4A8D3B3E.7080707@braverock.com>

?as.POSIXct
?strftime # for the format string

This topic has been covered so many times on this list that a simple 
archive search for 'xts' and 'POSIXct' should have provided enough 
pointers if the documentation did not.

your date_time should not be a column in your data but should be the 
index of your data

something like:

options(digits.secs=6)
x <- data
x.xts <- xts(x[,"price"],order.by=as.POSIXct(x[,"date_time"],"%y-%m-%d 
%H:%M:%OS" ))

Regards,

    - Brian

sunil wrote:
> Dear R User,                   I am creating a xts object for trading data.
> Initially I am passing date_time_stamp and price in xts object as follows
> price
> 12.13
> 12.44
> 12.32
> 12.54
> etc.
> date_time
> "2009-07-23 09:55:01.456"
> "2009-07-23 09:55:01.717"
> "2009-07-23 09:55:02.632"
> "2009-07-23 09:55:02.564"
> etc.
>
> data=xts(price,as.POSIXct(date_time))
>  #don't know better way if there any. If I pass the date_stamp without
> as.POSIXct it gives me an error, since date_time is character object.
>
> here data will not have milliseconds part. Any help?
> Suneel
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>   


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From kagba2006 at yahoo.com  Thu Aug 20 17:59:06 2009
From: kagba2006 at yahoo.com (FMH)
Date: Thu, 20 Aug 2009 08:59:06 -0700 (PDT)
Subject: [R-SIG-Finance] contour plot
Message-ID: <899499.40382.qm@web38304.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090820/a1c9b230/attachment.pl>

From jeff.a.ryan at gmail.com  Thu Aug 20 18:11:07 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 20 Aug 2009 11:11:07 -0500
Subject: [R-SIG-Finance] contour plot
In-Reply-To: <899499.40382.qm@web38304.mail.mud.yahoo.com>
References: <899499.40382.qm@web38304.mail.mud.yahoo.com>
Message-ID: <e8e755250908200911j53863a67q4e7e6e74d7aa8680@mail.gmail.com>

Hi Kagba,

This is the finance list, you probably want the list that says
"R-help"... and to re-read (read!?) the posting guide.

Jeff

On Thu, Aug 20, 2009 at 10:59 AM, FMH<kagba2006 at yahoo.com> wrote:
> Hi,
>
> Could someone give some ideas on plotting a contour by using geoR package, please?
>
> Thank you
>
> Kagba
>
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From frankzhen at gmail.com  Sun Aug 23 04:24:18 2009
From: frankzhen at gmail.com (Hai)
Date: Sat, 22 Aug 2009 22:24:18 -0400
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 63, Issue 14
In-Reply-To: <mailman.3.1250848802.1667.r-sig-finance@stat.math.ethz.ch>
References: <mailman.3.1250848802.1667.r-sig-finance@stat.math.ethz.ch>
Message-ID: <9D5B380D-1AF2-4BE9-B5BF-1E4B44AAE539@gmail.com>

?????

Merci !
Hai

Le Aug 21, 2009 ? 6:00 AM, r-sig-finance-request at stat.math.ethz.ch a  
?crit :

> Send R-SIG-Finance mailing list submissions to
>    r-sig-finance at stat.math.ethz.ch
>
> To subscribe or unsubscribe via the World Wide Web, visit
>    https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> or, via email, send a message with subject or body 'help' to
>    r-sig-finance-request at stat.math.ethz.ch
>
> You can reach the person managing the list at
>    r-sig-finance-owner at stat.math.ethz.ch
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-SIG-Finance digest..."
>
>
> Today's Topics:
>
>   1. Re: [R-sig-finance] A question on VECM (Matthieu Stigler)
>   2.  passing fraction of seconds in xts object (sunil)
>   3. Re: passing fraction of seconds in xts object (Brian G. Peterson)
>   4. contour plot (FMH)
>   5. Re: contour plot (Jeff Ryan)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Thu, 20 Aug 2009 13:25:52 +0200
> From: Matthieu Stigler <matthieu.stigler at gmail.com>
> Subject: Re: [R-SIG-Finance] [R-sig-finance] A question on VECM
> To: R-SIG-Finance at stat.math.ethz.ch
> Cc: ron_michael70 at yahoo.com
> Message-ID:
>    <111060c20908200425l95b03ecr5f1efd6404a05be0 at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
>
> I don't have neither Lutkephol nor Maddala and Kim at hand but if I
> remember well there is a whole chapter (at least section) in Maddala
> and Kim discussing how to deal with variables with different orders of
> integration, and you may find there complementary informations
> concerning I(0) variables into cointegrartion
>
> Hope this helps
>
> Matthieu
>
> 2009/8/20 John Frain <frainj at tcd.ie>:
>> The Example on page 303 of Lutkepohl is an examination of
>> cointegration properties of an interest rate R and inflation. ?If
>> inflation were denoted by say pi then everything would be relatively
>> familiar, ?However inflation is denoted by Dp. ?Both R and Dp (pi)  
>> are
>> are I(1) leading to the estimated cointegrating relationships  
>> (7.2.30)
>> or (7.2.31).
>>
>> In the original Engle and Granger (1987) Econometrica article all the
>> elements of the non-stationary vector were of the same order of
>> integration. ?The treatment of stationary variables was not explicit
>> but ?needed to be added afterwards. ?the treatment in Lutkepohl
>> includes this explicitly into the vecm system.
>>
>> Best Regards
>>
>> John
>>
>> 2009/8/19 ?<markleeds at verizon.net>:
>>> John: now that I think of it, you're right in that , as far as I  
>>> remember,
>>> both variables need to
>>> be I(1) for there to be a possible cointegration relation so I'll  
>>> have to
>>> check out pg 303
>>> closer. It doesn't make sense for R_t to be stationary and still  
>>> be part of
>>> a cointegrating
>>> relation ? So, my bad and I'll go back to 303 and see what's going  
>>> on there.
>>> Thanks for the
>>> clarifiication but now I'm actually more confused and don'thave  
>>> the energy
>>> to dig through Lutkepohl at the moment.
>>>
>>>
>>> Mark
>>>
>>>
>>>
>>> On Aug 19, 2009, John Frain <frainj at tcd.ie> wrote:
>>>
>>> Lutkepohl is splitting up the system into r stationary components or
>>> ecm's and n-r trends. He uses this ordering to show that that this
>>> can be done. The stationary components and the ecm's only enter the
>>> system at lag 1. The stationary components do not and should not
>>> enter the ecm's. If you do have stationary variables this is  
>>> testable
>>> and may be imposed at estimation.
>>>
>>> Best Regards
>>>
>>> John
>>>
>>> 2009/8/19 RON70 <ron_michael70 at yahoo.com>:
>>>>
>>>> still no single reply. Should I need to design my query in better  
>>>> way?
>>>>
>>>>
>>>>
>>>> RON70 wrote:
>>>>>
>>>>> Hi all, in Lutkepohl, page 250, I found that if there are  
>>>>> stationary
>>>>> variables in integrated system then they must be put in upper
>>>>> r-dimension.
>>>>> My quesltion is, is that the fact? If I do not do it, is there any
>>>>> problem
>>>>> in estimation and interpretation? I have done few exercises and  
>>>>> found
>>>>> that
>>>>> parameter estimation is not infected with ordering (except IRF
>>>>> estimation). Even is page 303 an example is presented wherein  
>>>>> inflation
>>>>> rate variable is taken as 2nd variable, although it looks like a  
>>>>> stable
>>>>> process.
>>>>>
>>>>> Can anyone please clarify that? If really ordering is a problem in
>>>>> presence of a stationary variable, can anyone provide me an  
>>>>> example, with
>>>>> perhaps in R-code so that I can regenerate?
>>>>>
>>>>> Thanks
>>>>>
>>>>
>>>> --
>>>> View this message in context:
>>>> http://www.nabble.com/A-question-on-VECM-tp24985789p25038989.html
>>>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>>
>>>
>>>
>>> --
>>> John C Frain, Ph.D.
>>> Trinity College Dublin
>>> Dublin 2
>>> Ireland
>>> www.tcd.ie/Economics/staff/frainj/home.htm
>>> mailto:frainj at tcd.ie
>>> mailto:frainj at gmail.com
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>>
>> --
>> John C Frain, Ph.D.
>> Trinity College Dublin
>> Dublin 2
>> Ireland
>> www.tcd.ie/Economics/staff/frainj/home.htm
>> mailto:frainj at tcd.ie
>> mailto:frainj at gmail.com
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> ------------------------------
>
> Message: 2
> Date: Thu, 20 Aug 2009 17:23:32 +0530
> From: sunil <sarswat at gmail.com>
> Subject: [R-SIG-Finance]  passing fraction of seconds in xts object
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID:
>    <28fa0bac0908200453h51492dabia2da5b50db9b4563 at mail.gmail.com>
> Content-Type: text/plain
>
> Dear R User,                   I am creating a xts object for  
> trading data.
> Initially I am passing date_time_stamp and price in xts object as  
> follows
> price
> 12.13
> 12.44
> 12.32
> 12.54
> etc.
> date_time
> "2009-07-23 09:55:01.456"
> "2009-07-23 09:55:01.717"
> "2009-07-23 09:55:02.632"
> "2009-07-23 09:55:02.564"
> etc.
>
> data=xts(price,as.POSIXct(date_time))
> #don't know better way if there any. If I pass the date_stamp without
> as.POSIXct it gives me an error, since date_time is character object.
>
> here data will not have milliseconds part. Any help?
> Suneel
>
>    [[alternative HTML version deleted]]
>
>
>
> ------------------------------
>
> Message: 3
> Date: Thu, 20 Aug 2009 07:02:06 -0500
> From: "Brian G. Peterson" <brian at braverock.com>
> Subject: Re: [R-SIG-Finance] passing fraction of seconds in xts object
> To: sunil <sarswat at gmail.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Message-ID: <4A8D3B3E.7080707 at braverock.com>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> ?as.POSIXct
> ?strftime # for the format string
>
> This topic has been covered so many times on this list that a simple
> archive search for 'xts' and 'POSIXct' should have provided enough
> pointers if the documentation did not.
>
> your date_time should not be a column in your data but should be the
> index of your data
>
> something like:
>
> options(digits.secs=6)
> x <- data
> x.xts <- xts(x[,"price"],order.by=as.POSIXct(x[,"date_time"],"%y-%m-%d
> %H:%M:%OS" ))
>
> Regards,
>
>    - Brian
>
> sunil wrote:
>> Dear R User,                   I am creating a xts object for  
>> trading data.
>> Initially I am passing date_time_stamp and price in xts object as  
>> follows
>> price
>> 12.13
>> 12.44
>> 12.32
>> 12.54
>> etc.
>> date_time
>> "2009-07-23 09:55:01.456"
>> "2009-07-23 09:55:01.717"
>> "2009-07-23 09:55:02.632"
>> "2009-07-23 09:55:02.564"
>> etc.
>>
>> data=xts(price,as.POSIXct(date_time))
>> #don't know better way if there any. If I pass the date_stamp without
>> as.POSIXct it gives me an error, since date_time is character object.
>>
>> here data will not have milliseconds part. Any help?
>> Suneel
>>
>>    [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
> -- 
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
>
>
> ------------------------------
>
> Message: 4
> Date: Thu, 20 Aug 2009 08:59:06 -0700 (PDT)
> From: FMH <kagba2006 at yahoo.com>
> Subject: [R-SIG-Finance] contour plot
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID: <899499.40382.qm at web38304.mail.mud.yahoo.com>
> Content-Type: text/plain
>
> Hi,
>
> Could someone give some ideas on plotting a contour by using geoR  
> package, please?
>
> Thank you
>
> Kagba
>
>
>
>    [[alternative HTML version deleted]]
>
>
>
> ------------------------------
>
> Message: 5
> Date: Thu, 20 Aug 2009 11:11:07 -0500
> From: Jeff Ryan <jeff.a.ryan at gmail.com>
> Subject: Re: [R-SIG-Finance] contour plot
> To: FMH <kagba2006 at yahoo.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Message-ID:
>    <e8e755250908200911j53863a67q4e7e6e74d7aa8680 at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
>
> Hi Kagba,
>
> This is the finance list, you probably want the list that says
> "R-help"... and to re-read (read!?) the posting guide.
>
> Jeff
>
> On Thu, Aug 20, 2009 at 10:59 AM, FMH<kagba2006 at yahoo.com> wrote:
>> Hi,
>>
>> Could someone give some ideas on plotting a contour by using geoR  
>> package, please?
>>
>> Thank you
>>
>> Kagba
>>
>>
>>
>> ? ? ? ?[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> -- 
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>
>
>
> ------------------------------
>
> _______________________________________________
> R-SIG-Finance mailing list
> R-SIG-Finance at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
> End of R-SIG-Finance Digest, Vol 63, Issue 14
> *********************************************


From lunamoonmoon at gmail.com  Sun Aug 23 17:06:59 2009
From: lunamoonmoon at gmail.com (Luna Moon)
Date: Sun, 23 Aug 2009 08:06:59 -0700
Subject: [R-SIG-Finance] help on vector auto-regressive model
Message-ID: <ad7856ef0908230806v4a48d5d9k41bb1308dfccbe9d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090823/cd308c6d/attachment.pl>

From lunamoonmoon at gmail.com  Sun Aug 23 19:10:03 2009
From: lunamoonmoon at gmail.com (Luna Moon)
Date: Sun, 23 Aug 2009 10:10:03 -0700
Subject: [R-SIG-Finance] study resources for time series?
Message-ID: <ad7856ef0908231010y133a2601h980aa7cc6835f663@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090823/9eee35b0/attachment.pl>

From shane.conway at gmail.com  Sun Aug 23 23:55:46 2009
From: shane.conway at gmail.com (Shane Conway)
Date: Sun, 23 Aug 2009 17:55:46 -0400
Subject: [R-SIG-Finance] study resources for time series?
In-Reply-To: <ad7856ef0908231010y133a2601h980aa7cc6835f663@mail.gmail.com>
References: <ad7856ef0908231010y133a2601h980aa7cc6835f663@mail.gmail.com>
Message-ID: <dd3243090908231455h216b03cera6fcbd3d10aa5acf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090823/8a7260ac/attachment.pl>

From wuertz at itp.phys.ethz.ch  Mon Aug 24 07:41:22 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon, 24 Aug 2009 07:41:22 +0200
Subject: [R-SIG-Finance] cdf of skewed t distribution using fGARCH vs
 skewt	package
In-Reply-To: <137956.45540.qm@web28607.mail.ukl.yahoo.com>
References: <137956.45540.qm@web28607.mail.ukl.yahoo.com>
Message-ID: <4A922802.7040307@itp.phys.ethz.ch>

Alex Chan wrote:
> I ran a AR(p)-GARCH (1,1) - skewed t using the fGARCH package. When I tried to use the sstd option in fgarch to return the cdf of the  standardized residuals z and compared it with the results obtained using the skewt package, I obtained different results even though both skewed t dist are based on Fernandez and steel. Since the standardized inovations are supposed to be iid with mean o and variance 1, i used   psstd(z, mean = 0, sd = 1, nu , xi ). Anyone can explain why this might be the case? 
>
> Alex 
>   
sstd means standardized skew student-t, the first s is important.

in the student-t the number of degrees of freedom is related to the 
variance, so you cannot use the usual student t.
you need a reformulated distribution for which the variance is always 1 
and a proper nu which does not
change the variance. This does the sstd.

Diethelm
>
>
>       
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From megh700004 at yahoo.com  Mon Aug 24 10:38:34 2009
From: megh700004 at yahoo.com (megh)
Date: Mon, 24 Aug 2009 01:38:34 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] help on vector auto-regressive
	model
In-Reply-To: <ad7856ef0908230806v4a48d5d9k41bb1308dfccbe9d@mail.gmail.com>
References: <ad7856ef0908230806v4a48d5d9k41bb1308dfccbe9d@mail.gmail.com>
Message-ID: <25112432.post@talk.nabble.com>


I guess same question was asked in R-forum and already answered. Before
throwing same question in different forum you better follow it up properly.


Luna Moon wrote:
> 
> Hi all,
> 
> 
> I am asking this for my friend.
> 
> 
> In VAR models, how do we test the goodness-of-fit of a VAR model?  More
> specifically in R?
> 
> 
> Moreover, are there assumptions on the joint distribution of the data in
> the
> model?
> 
> 
> Thanks a lot!
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/help-on-vector-auto-regressive-model-tp25104126p25112432.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ron_michael70 at yahoo.com  Mon Aug 24 14:15:24 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Mon, 24 Aug 2009 05:15:24 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Looking for critical values for
 Johansen's trace test
Message-ID: <25115000.post@talk.nabble.com>


Hi can anyone please help me on where to get table for critical values for
Johansen's Trace test for testing co-integration rank, for different type of
deterministic terms? I already have looked into Hamilton's book however
could not find what I want.

Your help will be highly appreciated.

Thanks
-- 
View this message in context: http://www.nabble.com/Looking-for-critical-values-for-Johansen%27s-trace-test-tp25115000p25115000.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From spencer.graves at prodsyse.com  Mon Aug 24 15:31:03 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Mon, 24 Aug 2009 06:31:03 -0700
Subject: [R-SIG-Finance] [R-sig-finance] help on vector auto-regressive
 model
In-Reply-To: <25112432.post@talk.nabble.com>
References: <ad7856ef0908230806v4a48d5d9k41bb1308dfccbe9d@mail.gmail.com>
	<25112432.post@talk.nabble.com>
Message-ID: <4A929617.9060204@prodsyse.com>

Hi, Megh: 


      What do you mean by "R-forum", and how can I search it? 


      I just searched the "Nabble R Forum" for "goodness-of-fit in a VAR 
model", and didn't see anything that looks like this.  I did see copies 
of this question asked on R-help and Rmetrics.  However, the "Nabble R 
Forum" is not a mailing list by itself on which people ask questions, as 
your comment seems to imply, but a search facility like "RSiteSearch" 
for the archives from "R help", "Rmetrics", "R devel", and "Rcom-l".  I 
also tried RSiteSearch, with the same negative results.


       What is the question and answer to which you referred? 


      Thanks,
      Spencer Graves


megh wrote:
> I guess same question was asked in R-forum and already answered. Before
> throwing same question in different forum you better follow it up properly.
>
>
> Luna Moon wrote:
>   
>> Hi all,
>>
>>
>> I am asking this for my friend.
>>
>>
>> In VAR models, how do we test the goodness-of-fit of a VAR model?  More
>> specifically in R?
>>
>>
>> Moreover, are there assumptions on the joint distribution of the data in
>> the
>> model?
>>
>>
>> Thanks a lot!
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>>     
>
>   


-- 
Spencer Graves, PE, PhD
President and Chief Operating Officer
Structure Inspection and Monitoring, Inc.
751 Emerson Ct.
San Jos?, CA 95126
ph:  408-655-4567


From megh700004 at yahoo.com  Mon Aug 24 16:06:43 2009
From: megh700004 at yahoo.com (Megh Dal)
Date: Mon, 24 Aug 2009 07:06:43 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] help on vector auto-regressive
	model
In-Reply-To: <4A929617.9060204@prodsyse.com>
Message-ID: <417550.69920.qm@web58104.mail.re3.yahoo.com>

this question was asked (and answered) on http://www.nabble.com/help-on-vector-auto-regressive-model-td25099737.html

If anyone post the same question in multiple forums then it would be always courtesy to mention that like, same question was there in so-so forums and no satisfactory answers were provided. It would also be good to mention why it is not satisfactory. When an expert answer an query he spends (I would say invest) lot of times on that and therefore after spending so much time on that, suddenly he might discover that was already answered. Therefore if it is mentioned that was already posted in some other places then he might look into that and perhaps can provide a better solution.

I do agree that ""Nabble R  Forum" is not a mailing list by itself" however I did not mean that even, rather meant to say, it was already answered in r-help (address given above).

Thanks

--- On Mon, 8/24/09, spencerg <spencer.graves at prodsyse.com> wrote:

> From: spencerg <spencer.graves at prodsyse.com>
> Subject: Re: [R-SIG-Finance] [R-sig-finance] help on vector auto-regressive model
> To: "megh" <megh700004 at yahoo.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Date: Monday, August 24, 2009, 7:01 PM
> Hi, Megh: 
> 
> 
> ? ? ? What do you mean by "R-forum", and how
> can I search it? 
> 
> 
> ? ? ? I just searched the "Nabble R Forum"
> for "goodness-of-fit in a VAR 
> model", and didn't see anything that looks like this.?
> I did see copies 
> of this question asked on R-help and Rmetrics.?
> However, the "Nabble R 
> Forum" is not a mailing list by itself on which people ask
> questions, as 
> your comment seems to imply, but a search facility like
> "RSiteSearch" 
> for the archives from "R help", "Rmetrics", "R devel", and
> "Rcom-l".? I 
> also tried RSiteSearch, with the same negative results.
> 
> 
> ? ? ???What is the question and
> answer to which you referred? 
> 
> 
> ? ? ? Thanks,
> ? ? ? Spencer Graves
> 
> 
> megh wrote:
> > I guess same question was asked in R-forum and already
> answered. Before
> > throwing same question in different forum you better
> follow it up properly.
> >
> >
> > Luna Moon wrote:
> >???
> >> Hi all,
> >>
> >>
> >> I am asking this for my friend.
> >>
> >>
> >> In VAR models, how do we test the goodness-of-fit
> of a VAR model?? More
> >> specifically in R?
> >>
> >>
> >> Moreover, are there assumptions on the joint
> distribution of the data in
> >> the
> >> model?
> >>
> >>
> >> Thanks a lot!
> >>
> >> ??? [[alternative HTML version
> deleted]]
> >>
> >> _______________________________________________
> >> R-SIG-Finance at stat.math.ethz.ch
> mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >> -- Subscriber-posting only.
> >> -- If you want to post, subscribe first.
> >>
> >>
> >>? ???
> >
> >???
> 
> 
> -- 
> Spencer Graves, PE, PhD
> President and Chief Operating Officer
> Structure Inspection and Monitoring, Inc.
> 751 Emerson Ct.
> San Jos?, CA 95126
> ph:? 408-655-4567
> 
> 





From singularitaet at gmx.net  Mon Aug 24 16:21:26 2009
From: singularitaet at gmx.net (Stefan Grosse)
Date: Mon, 24 Aug 2009 16:21:26 +0200
Subject: [R-SIG-Finance] [R-sig-finance] Looking for critical values for
 Johansen's trace test
In-Reply-To: <25115000.post@talk.nabble.com>
References: <25115000.post@talk.nabble.com>
Message-ID: <20090824162126.7f4013ee@gmx.net>

On Mon, 24 Aug 2009 05:15:24 -0700 (PDT) RON70
<ron_michael70 at yahoo.com> wrote:

R> Hi can anyone please help me on where to get table for critical
R> values for Johansen's Trace test for testing co-integration rank,
R> for different type of deterministic terms? I already have looked
R> into Hamilton's book however could not find what I want.

Did you have a look at the ca.jo function of the urca package? You can
have a look at the code with simply:
ca.jo
you will easily find the critical values with a little bit of
programming knowledge...

hth
Stefan


From matthieu.stigler at gmail.com  Mon Aug 24 16:53:01 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Mon, 24 Aug 2009 16:53:01 +0200
Subject: [R-SIG-Finance] [R-sig-finance] help on vector auto-regressive
	model
In-Reply-To: <417550.69920.qm@web58104.mail.re3.yahoo.com>
References: <4A929617.9060204@prodsyse.com>
	<417550.69920.qm@web58104.mail.re3.yahoo.com>
Message-ID: <111060c20908240753j7175946cg925f0a3364f2f1e@mail.gmail.com>

Right it's good to mention if the question was asked on antoher list,
to avoid cross-posting.

I'm not convinced by the answer there, neither by the question itself:
I'm indeed not sure goodness of fit is a relevant notion for time
series data.

To the answer: ACF/PACF and normality tests just tell you if you the
assumption needed for inference seem to hold or not. In analogy to the
usual linear regression case, this is not the point whe nassesing
goodness of fit I believe.

To the question: for time series model where you add lags, indicators
such R^2 go to 1 due to the presence of the lagged variables, so are
not indicative... You may still look at individual R^2 and make
comparisons which variable is well explained in the model and which is
not (for that rather use FEVD maybe)... But overall R^2 does not exist
to my knowledge (nor makes sense in my opinion).

Well this is rather a personal view...

Hope this helps

Mat

2009/8/24 Megh Dal <megh700004 at yahoo.com>:
> this question was asked (and answered) on http://www.nabble.com/help-on-vector-auto-regressive-model-td25099737.html
>
> If anyone post the same question in multiple forums then it would be always courtesy to mention that like, same question was there in so-so forums and no satisfactory answers were provided. It would also be good to mention why it is not satisfactory. When an expert answer an query he spends (I would say invest) lot of times on that and therefore after spending so much time on that, suddenly he might discover that was already answered. Therefore if it is mentioned that was already posted in some other places then he might look into that and perhaps can provide a better solution.
>
> I do agree that ""Nabble R ?Forum" is not a mailing list by itself" however I did not mean that even, rather meant to say, it was already answered in r-help (address given above).
>
> Thanks
>
> --- On Mon, 8/24/09, spencerg <spencer.graves at prodsyse.com> wrote:
>
>> From: spencerg <spencer.graves at prodsyse.com>
>> Subject: Re: [R-SIG-Finance] [R-sig-finance] help on vector auto-regressive model
>> To: "megh" <megh700004 at yahoo.com>
>> Cc: r-sig-finance at stat.math.ethz.ch
>> Date: Monday, August 24, 2009, 7:01 PM
>> Hi, Megh:
>>
>>
>> ? ? ? What do you mean by "R-forum", and how
>> can I search it?
>>
>>
>> ? ? ? I just searched the "Nabble R Forum"
>> for "goodness-of-fit in a VAR
>> model", and didn't see anything that looks like this.
>> I did see copies
>> of this question asked on R-help and Rmetrics.
>> However, the "Nabble R
>> Forum" is not a mailing list by itself on which people ask
>> questions, as
>> your comment seems to imply, but a search facility like
>> "RSiteSearch"
>> for the archives from "R help", "Rmetrics", "R devel", and
>> "Rcom-l".? I
>> also tried RSiteSearch, with the same negative results.
>>
>>
>> ? ? ???What is the question and
>> answer to which you referred?
>>
>>
>> ? ? ? Thanks,
>> ? ? ? Spencer Graves
>>
>>
>> megh wrote:
>> > I guess same question was asked in R-forum and already
>> answered. Before
>> > throwing same question in different forum you better
>> follow it up properly.
>> >
>> >
>> > Luna Moon wrote:
>> >
>> >> Hi all,
>> >>
>> >>
>> >> I am asking this for my friend.
>> >>
>> >>
>> >> In VAR models, how do we test the goodness-of-fit
>> of a VAR model?? More
>> >> specifically in R?
>> >>
>> >>
>> >> Moreover, are there assumptions on the joint
>> distribution of the data in
>> >> the
>> >> model?
>> >>
>> >>
>> >> Thanks a lot!
>> >>
>> >> ??? [[alternative HTML version
>> deleted]]
>> >>
>> >> _______________________________________________
>> >> R-SIG-Finance at stat.math.ethz.ch
>> mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> >> -- Subscriber-posting only.
>> >> -- If you want to post, subscribe first.
>> >>
>> >>
>> >>
>> >
>> >
>>
>>
>> --
>> Spencer Graves, PE, PhD
>> President and Chief Operating Officer
>> Structure Inspection and Monitoring, Inc.
>> 751 Emerson Ct.
>> San Jos?, CA 95126
>> ph:? 408-655-4567
>>
>>
>
>
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From bogaso.christofer at gmail.com  Mon Aug 24 17:03:39 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Mon, 24 Aug 2009 08:03:39 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] help on vector auto-regressive
	model
In-Reply-To: <111060c20908240753j7175946cg925f0a3364f2f1e@mail.gmail.com>
References: <ad7856ef0908230806v4a48d5d9k41bb1308dfccbe9d@mail.gmail.com>
	<25112432.post@talk.nabble.com> <4A929617.9060204@prodsyse.com>
	<417550.69920.qm@web58104.mail.re3.yahoo.com>
	<111060c20908240753j7175946cg925f0a3364f2f1e@mail.gmail.com>
Message-ID: <25117653.post@talk.nabble.com>


By R-square, if you meant to say, how much variance is explained by the
explanatory variables then, I would like to suggest to see Forecast variance
decomposition.



matifou wrote:
> 
> Right it's good to mention if the question was asked on antoher list,
> to avoid cross-posting.
> 
> I'm not convinced by the answer there, neither by the question itself:
> I'm indeed not sure goodness of fit is a relevant notion for time
> series data.
> 
> To the answer: ACF/PACF and normality tests just tell you if you the
> assumption needed for inference seem to hold or not. In analogy to the
> usual linear regression case, this is not the point whe nassesing
> goodness of fit I believe.
> 
> To the question: for time series model where you add lags, indicators
> such R^2 go to 1 due to the presence of the lagged variables, so are
> not indicative... You may still look at individual R^2 and make
> comparisons which variable is well explained in the model and which is
> not (for that rather use FEVD maybe)... But overall R^2 does not exist
> to my knowledge (nor makes sense in my opinion).
> 
> Well this is rather a personal view...
> 
> Hope this helps
> 
> Mat
> 
> 2009/8/24 Megh Dal <megh700004 at yahoo.com>:
>> this question was asked (and answered) on
>> http://www.nabble.com/help-on-vector-auto-regressive-model-td25099737.html
>>
>> If anyone post the same question in multiple forums then it would be
>> always courtesy to mention that like, same question was there in so-so
>> forums and no satisfactory answers were provided. It would also be good
>> to mention why it is not satisfactory. When an expert answer an query he
>> spends (I would say invest) lot of times on that and therefore after
>> spending so much time on that, suddenly he might discover that was
>> already answered. Therefore if it is mentioned that was already posted in
>> some other places then he might look into that and perhaps can provide a
>> better solution.
>>
>> I do agree that ""Nabble R ?Forum" is not a mailing list by itself"
>> however I did not mean that even, rather meant to say, it was already
>> answered in r-help (address given above).
>>
>> Thanks
>>
>> --- On Mon, 8/24/09, spencerg <spencer.graves at prodsyse.com> wrote:
>>
>>> From: spencerg <spencer.graves at prodsyse.com>
>>> Subject: Re: [R-SIG-Finance] [R-sig-finance] help on vector
>>> auto-regressive model
>>> To: "megh" <megh700004 at yahoo.com>
>>> Cc: r-sig-finance at stat.math.ethz.ch
>>> Date: Monday, August 24, 2009, 7:01 PM
>>> Hi, Megh:
>>>
>>>
>>> ? ? ? What do you mean by "R-forum", and how
>>> can I search it?
>>>
>>>
>>> ? ? ? I just searched the "Nabble R Forum"
>>> for "goodness-of-fit in a VAR
>>> model", and didn't see anything that looks like this.
>>> I did see copies
>>> of this question asked on R-help and Rmetrics.
>>> However, the "Nabble R
>>> Forum" is not a mailing list by itself on which people ask
>>> questions, as
>>> your comment seems to imply, but a search facility like
>>> "RSiteSearch"
>>> for the archives from "R help", "Rmetrics", "R devel", and
>>> "Rcom-l".? I
>>> also tried RSiteSearch, with the same negative results.
>>>
>>>
>>> ? ? ???What is the question and
>>> answer to which you referred?
>>>
>>>
>>> ? ? ? Thanks,
>>> ? ? ? Spencer Graves
>>>
>>>
>>> megh wrote:
>>> > I guess same question was asked in R-forum and already
>>> answered. Before
>>> > throwing same question in different forum you better
>>> follow it up properly.
>>> >
>>> >
>>> > Luna Moon wrote:
>>> >
>>> >> Hi all,
>>> >>
>>> >>
>>> >> I am asking this for my friend.
>>> >>
>>> >>
>>> >> In VAR models, how do we test the goodness-of-fit
>>> of a VAR model?? More
>>> >> specifically in R?
>>> >>
>>> >>
>>> >> Moreover, are there assumptions on the joint
>>> distribution of the data in
>>> >> the
>>> >> model?
>>> >>
>>> >>
>>> >> Thanks a lot!
>>> >>
>>> >> ??? [[alternative HTML version
>>> deleted]]
>>> >>
>>> >> _______________________________________________
>>> >> R-SIG-Finance at stat.math.ethz.ch
>>> mailing list
>>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> >> -- Subscriber-posting only.
>>> >> -- If you want to post, subscribe first.
>>> >>
>>> >>
>>> >>
>>> >
>>> >
>>>
>>>
>>> --
>>> Spencer Graves, PE, PhD
>>> President and Chief Operating Officer
>>> Structure Inspection and Monitoring, Inc.
>>> 751 Emerson Ct.
>>> San Jos?, CA 95126
>>> ph:? 408-655-4567
>>>
>>>
>>
>>
>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/help-on-vector-auto-regressive-model-tp25104126p25117653.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From guy.yollin at rotellacapital.com  Mon Aug 24 17:13:50 2009
From: guy.yollin at rotellacapital.com (Guy Yollin)
Date: Mon, 24 Aug 2009 10:13:50 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Looking for critical values for
 Johansen's trace test
In-Reply-To: <25115000.post@talk.nabble.com>
References: <25115000.post@talk.nabble.com>
Message-ID: <E4259A82356E7F46B4F911FB27B0D725183B488AAC@AUSP01VMBX02.collaborationhost.net>

Ron,

The following pdf has tables of critical values from Osterwald-Lenum (1992) for the cases of 1) restricted constant 2) unrestricted constant 3) restricted trend 4) unrestricted constant and trend:

http://www2.warwick.ac.uk/fac/soc/economics/staff/faculty/jeremysmith/manual/statisticaltables.pdf

Best,

-- G


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of RON70
Sent: Monday, August 24, 2009 5:15 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] [R-sig-finance] Looking for critical values for Johansen's trace test


Hi can anyone please help me on where to get table for critical values for
Johansen's Trace test for testing co-integration rank, for different type of
deterministic terms? I already have looked into Hamilton's book however
could not find what I want.

Your help will be highly appreciated.

Thanks
-- 
View this message in context: http://www.nabble.com/Looking-for-critical-values-for-Johansen%27s-trace-test-tp25115000p25115000.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From spencer.graves at prodsyse.com  Mon Aug 24 17:28:52 2009
From: spencer.graves at prodsyse.com (spencerg)
Date: Mon, 24 Aug 2009 08:28:52 -0700
Subject: [R-SIG-Finance] [R-sig-finance] help on vector auto-regressive
 model
In-Reply-To: <417550.69920.qm@web58104.mail.re3.yahoo.com>
References: <417550.69920.qm@web58104.mail.re3.yahoo.com>
Message-ID: <4A92B1B4.5060703@prodsyse.com>

Hi, Megh: 


      Thanks.  I found that post but failed to see how to view the 
reply.  With your confirmation that a reply was there, I was now able to 
find it -- and learned something about how to use Nabble. 


      If that doesn't answer Luna Moon's question, it needs to be 
rewritten as you say, acknowledging the reply from the earlier respondent. 


      Best Wishes,
      Spencer


Megh Dal wrote:
> this question was asked (and answered) on http://www.nabble.com/help-on-vector-auto-regressive-model-td25099737.html
>
> If anyone post the same question in multiple forums then it would be always courtesy to mention that like, same question was there in so-so forums and no satisfactory answers were provided. It would also be good to mention why it is not satisfactory. When an expert answer an query he spends (I would say invest) lot of times on that and therefore after spending so much time on that, suddenly he might discover that was already answered. Therefore if it is mentioned that was already posted in some other places then he might look into that and perhaps can provide a better solution.
>
> I do agree that ""Nabble R  Forum" is not a mailing list by itself" however I did not mean that even, rather meant to say, it was already answered in r-help (address given above).
>
> Thanks
>
> --- On Mon, 8/24/09, spencerg <spencer.graves at prodsyse.com> wrote:
>
>   
>> From: spencerg <spencer.graves at prodsyse.com>
>> Subject: Re: [R-SIG-Finance] [R-sig-finance] help on vector auto-regressive model
>> To: "megh" <megh700004 at yahoo.com>
>> Cc: r-sig-finance at stat.math.ethz.ch
>> Date: Monday, August 24, 2009, 7:01 PM
>> Hi, Megh: 
>>
>>
>>       What do you mean by "R-forum", and how
>> can I search it? 
>>
>>
>>       I just searched the "Nabble R Forum"
>> for "goodness-of-fit in a VAR 
>> model", and didn't see anything that looks like this. 
>> I did see copies 
>> of this question asked on R-help and Rmetrics. 
>> However, the "Nabble R 
>> Forum" is not a mailing list by itself on which people ask
>> questions, as 
>> your comment seems to imply, but a search facility like
>> "RSiteSearch" 
>> for the archives from "R help", "Rmetrics", "R devel", and
>> "Rcom-l".  I 
>> also tried RSiteSearch, with the same negative results.
>>
>>
>>        What is the question and
>> answer to which you referred? 
>>
>>
>>       Thanks,
>>       Spencer Graves
>>
>>
>> megh wrote:
>>     
>>> I guess same question was asked in R-forum and already
>>>       
>> answered. Before
>>     
>>> throwing same question in different forum you better
>>>       
>> follow it up properly.
>>     
>>> Luna Moon wrote:
>>>    
>>>       
>>>> Hi all,
>>>>
>>>>
>>>> I am asking this for my friend.
>>>>
>>>>
>>>> In VAR models, how do we test the goodness-of-fit
>>>>         
>> of a VAR model?  More
>>     
>>>> specifically in R?
>>>>
>>>>
>>>> Moreover, are there assumptions on the joint
>>>>         
>> distribution of the data in
>>     
>>>> the
>>>> model?
>>>>
>>>>
>>>> Thanks a lot!
>>>>
>>>>     [[alternative HTML version
>>>>         
>> deleted]]
>>     
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch
>>>>         
>> mailing list
>>     
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>>>      
>>>>         
>>>    
>>>       
>> -- 
>> Spencer Graves, PE, PhD
>> President and Chief Operating Officer
>> Structure Inspection and Monitoring, Inc.
>> 751 Emerson Ct.
>> San Jos?, CA 95126
>> ph:  408-655-4567
>>
>>
>>     
>
>
>       
>
>   


-- 
Spencer Graves, PE, PhD
President and Chief Operating Officer
Structure Inspection and Monitoring, Inc.
751 Emerson Ct.
San Jos?, CA 95126
ph:  408-655-4567


From ron_michael70 at yahoo.com  Mon Aug 24 18:46:50 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Mon, 24 Aug 2009 09:46:50 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Looking for critical values for
 Johansen's trace test
In-Reply-To: <E4259A82356E7F46B4F911FB27B0D725183B488AAC@AUSP01VMBX02.collaborationhost.net>
References: <25115000.post@talk.nabble.com>
	<E4259A82356E7F46B4F911FB27B0D725183B488AAC@AUSP01VMBX02.collaborationhost.net>
Message-ID: <25119450.post@talk.nabble.com>


Thank you so much Guy, for this informative document. Here I actually want to
replicate the results given in Lutkepohl. Therefore looking for the same
document from which he got the critical values. When I compare those value
with that pdf, I find some differences. For example, let say constant
deterministic term. Here in lutkepohl, 10% critical value given as 7.50 with
(p-r)=2. However I could not find that value in that pdf file.

Lutkepohl refer the table as "Johansen (1995, tables 15.2, 15.3, 15.4)". Are
you aware of that? Can you provide me any link there in net?

Thanks


Guy Yollin-2 wrote:
> 
> Ron,
> 
> The following pdf has tables of critical values from Osterwald-Lenum
> (1992) for the cases of 1) restricted constant 2) unrestricted constant 3)
> restricted trend 4) unrestricted constant and trend:
> 
> http://www2.warwick.ac.uk/fac/soc/economics/staff/faculty/jeremysmith/manual/statisticaltables.pdf
> 
> Best,
> 
> -- G
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of RON70
> Sent: Monday, August 24, 2009 5:15 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] [R-sig-finance] Looking for critical values for
> Johansen's trace test
> 
> 
> Hi can anyone please help me on where to get table for critical values for
> Johansen's Trace test for testing co-integration rank, for different type
> of
> deterministic terms? I already have looked into Hamilton's book however
> could not find what I want.
> 
> Your help will be highly appreciated.
> 
> Thanks
> -- 
> View this message in context:
> http://www.nabble.com/Looking-for-critical-values-for-Johansen%27s-trace-test-tp25115000p25115000.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Looking-for-critical-values-for-Johansen%27s-trace-test-tp25115000p25119450.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From markleeds at verizon.net  Mon Aug 24 19:05:05 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 24 Aug 2009 12:05:05 -0500 (CDT)
Subject: [R-SIG-Finance] [R-sig-finance] Looking for critical values for
 Johansen's trace test
Message-ID: <1164884009.270247.1251133505199.JavaMail.root@vms229.mailsrvcs.net>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090824/559a6ee3/attachment.html>

From vfulco1 at gmail.com  Wed Aug 26 21:05:05 2009
From: vfulco1 at gmail.com (Vince Fulco)
Date: Wed, 26 Aug 2009 15:05:05 -0400
Subject: [R-SIG-Finance] IBrokers-- server version question...
Message-ID: <34f2770f0908261205j3cc6531cr3a2f5825e48380eb@mail.gmail.com>

Installing from the latest SVN, package mates to API Ver. 9.62 which
is latest production release.  Getting 'Server_Version' error on
previously well running code. Would appreciate any insights and tips
on quick fixes for the future rather than hounding list & author.
TIA, V.

#
Loading required package: bigmemory
Loading required package: methods
Error in structure(list(s, clientId = clientId, port = port,
server.version = SERVER_VERSION,  :
  object 'SERVER_VERSION' not found
Calls: twsConnect -> structure
Execution halted
#

-- 
Vince Fulco, CFA, CAIA
612.424.5477 (universal)
vfulco1 at gmail.com

 A posse ad esse non valet consequentia

?the possibility does not necessarily lead to materialization?


From jeff.a.ryan at gmail.com  Wed Aug 26 21:16:44 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 26 Aug 2009 14:16:44 -0500
Subject: [R-SIG-Finance] IBrokers-- server version question...
In-Reply-To: <34f2770f0908261205j3cc6531cr3a2f5825e48380eb@mail.gmail.com>
References: <34f2770f0908261205j3cc6531cr3a2f5825e48380eb@mail.gmail.com>
Message-ID: <e8e755250908261216q1c98da42ub5795bc4e2d9d471@mail.gmail.com>

Hi Vince,

Make sure that the actual version of the TWS running is the most
current on from IB. The way to ensure that is to use the java web
start link from the IB homepage.

Jeff


On Wed, Aug 26, 2009 at 2:05 PM, Vince Fulco<vfulco1 at gmail.com> wrote:
> Installing from the latest SVN, package mates to API Ver. 9.62 which
> is latest production release. ?Getting 'Server_Version' error on
> previously well running code. Would appreciate any insights and tips
> on quick fixes for the future rather than hounding list & author.
> TIA, V.
>
> #
> Loading required package: bigmemory
> Loading required package: methods
> Error in structure(list(s, clientId = clientId, port = port,
> server.version = SERVER_VERSION, ?:
> ?object 'SERVER_VERSION' not found
> Calls: twsConnect -> structure
> Execution halted
> #
>
> --
> Vince Fulco, CFA, CAIA
> 612.424.5477 (universal)
> vfulco1 at gmail.com
>
> ?A posse ad esse non valet consequentia
>
> ?the possibility does not necessarily lead to materialization?
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From breman.mark at gmail.com  Thu Aug 27 08:55:14 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Thu, 27 Aug 2009 08:55:14 +0200
Subject: [R-SIG-Finance] IBrokers getting quotes
Message-ID: <5e6a2e670908262355rf1dea77g10e41a4a12b4966f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090827/8f3f3cb2/attachment.pl>

From Cristian.Gonzalez at sgfc.ch  Thu Aug 27 15:53:13 2009
From: Cristian.Gonzalez at sgfc.ch (Cristian Gonzalez)
Date: Thu, 27 Aug 2009 15:53:13 +0200
Subject: [R-SIG-Finance] Forecasting GARCH
Message-ID: <6F143AD637C8874C8BB255302BC32A317D6E74@SGFC-BIZ.SGFC-DOM.sgfc.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090827/ed5ff5be/attachment.pl>

From jeff.a.ryan at gmail.com  Thu Aug 27 16:29:03 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 27 Aug 2009 09:29:03 -0500
Subject: [R-SIG-Finance] IBrokers getting quotes
In-Reply-To: <5e6a2e670908262355rf1dea77g10e41a4a12b4966f@mail.gmail.com>
References: <5e6a2e670908262355rf1dea77g10e41a4a12b4966f@mail.gmail.com>
Message-ID: <e8e755250908270729w3a1b1c1lccff8d0864a6322e@mail.gmail.com>

Hi Mark,

There is functionality in IBrokers for quotes, though the quotes are
via the snapshot parameter and the official API.  This roughly
translates to they are not very timely (as far as I can tell).

Firstly, update to the newest version of IBrokers and the TWS (where
the following is tested).

reqMktData(tws, list(twsSTK("AAPL"),twsSTK("SBUX")), snapshot=TRUE)

        lastTimeStamp symbol bidSize bidPrice askPrice askSize lastPrice Volume
1 2009-08-26 14:22:18   AAPL       1   167.32   167.33       2    167.32  89233
2 2009-08-26 14:21:40   SBUX     142    19.32    19.33     210     19.33  67075
    Open   High    Low Close
1 168.94 169.55 166.76 169.4
2  19.45  19.68  19.21  19.5

As you can see, you can pass a list of symbols into the call to get a
data frame of quotes back.  The other thing that is rather obvious
from the output is that the lastTimeStamp is different for each quote.
 The official (IB) API docs would be where to look to identify exactly
what that implies.

Aside from that, the most recent version on CRAN (and R-forge) make
use of a few shortcuts to get what you want.  They also serve as
decent examples of what you can do.

?eWrapper.MktData.CSV and eWrapper.data (same help file)

This will print to the screen [file(s)] a csv representation of bid
and ask, as new prices arrive.

> reqMktData(tws, twsSTK("SBUX"), eventWrapper=eWrapper.MktData.CSV(1))
20090826 14:33:01.414505,143,19.31,,,,,
20090826 14:33:01.417832,,,19.32,211,,,
20090826 14:33:01.419186,,,,,19.31,,
20090826 14:33:01.420762,143,,,,,,
20090826 14:33:01.422060,,,,211,,,
20090826 14:33:01.423350,,,,,,1,
20090826 14:33:01.424725,,,,,,,68830
20090826 14:33:01.428188,143,19.31,,,,,
20090826 14:33:01.429453,143,,,,,,

The internal messages from the TWS are complex, so your original
approach is simply missing the actual raw data that you want.  Looking
at the above eWrapper.MktData.CSV function call for tickPrice:

> eWrapper.MktData.CSV()$tickPrice
function (curMsg, msg, timestamp, file, ...)
{
    tickType = msg[3]
    msg <- as.numeric(msg)
    id <- as.numeric(msg[2])
    file <- file[[id]]
    data <- eW$get.Data("data")
    attr(data[[id]], "index") <- as.numeric(Sys.time())
    nr.data <- NROW(data[[id]])
    if (tickType == .twsTickType$BID) {
        cat(paste(timestamp, msg[5], msg[4], "", "", "", "",
            "", sep = ","), "\n", file = file, append = TRUE)
        data[[id]][nr.data, 1:2] <- msg[5:4]
    }
    else if (tickType == .twsTickType$ASK) {
        cat(paste(timestamp, "", "", msg[4], msg[5], "", "",
            "", sep = ","), "\n", file = file, append = TRUE)
        data[[id]][nr.data, 3:4] <- msg[4:5]
    }
    else if (tickType == .twsTickType$LAST) {
        cat(paste(timestamp, "", "", "", "", msg[4], "", "",
            sep = ","), "\n", file = file, append = TRUE)
        data[[id]][nr.data, 5] <- msg[4]
    }
    eW$assign.Data("data", data)
    c(curMsg, msg)
}
<environment: 0x30ee558>

Gives you a handle on what is happening on the IB/TWS side.  tickPrice
is a top level handler for many types of 'price's.  Essentially the
3rd element in the message is the "tickType".  This then has further
branching/processing.

One item of note, if you are just looking to capture last (current)
bid-ask, the variable "data" inside the eWrapper closure will be
maintaining the state you are looking for.

> myWrapper <- eWrapper.MktData.CSV(1)  # create a closure, which can maintain state
> reqMktData(tws, twsSTK("SBUX"), eventWrapper=myWrapper)
20090826 14:36:48.641869,126,19.28,,,,,
20090826 14:36:48.643727,,,19.29,230,,,
20090826 14:36:48.645176,,,,,19.28,,
20090826 14:36:48.646720,126,,,,,,
20090826 14:36:48.647979,,,,230,,,
20090826 14:36:48.649229,,,,,,1,
20090826 14:36:48.650573,,,,,,,70163
20090826 14:36:48.659182,126,19.28,,,,,
20090826 14:36:48.660954,126,,,,,,
20090826 14:36:48.662304,,,19.29,230,,,
20090826 14:36:48.663892,,,,230,,,
^C

> myWrapper$get.Data("data")
[[1]]
                          BidSize BidPrice AskPrice AskSize  Last LastSize
2009-08-26 14:36:48.66338     126    19.28    19.29     230 19.28        1
                          Volume
2009-08-26 14:36:48.66338  70163

This can be useful in custom CALLBACK calls.

To create your own eWrapper, look at the eWrapper.MktData.CSV code, as
it should give guidance as to what is doable.

HTH
Jeff


On Thu, Aug 27, 2009 at 1:55 AM, Mark Breman<breman.mark at gmail.com> wrote:
> Hi,
> I am using the IBrokers package to get data from IB, but now I have run into
> a little bit of trouble.
>
> I have a lot of stock symbols I have to get a quote for (bid/ask price
> only).
>
> It looks like there is not a specific function in IBrokers to get quotes, so
> I thought about filtering out the bid/ask price from the reqMktData()
> function as in the highlighted output (is there a easier way?):
>> reqMktData(tws, twsSTK("SBUX"))
> <20090826 18:18:52.859000> id=1 symbol= Volume: 53727
> <20090826 18:18:52.860000> id=1 symbol= highPrice: 19.68
> <20090826 18:18:52.861000> id=1 symbol= lowPrice: 19.21
> <20090826 18:18:52.862000> id=1 symbol= shortable: 3.0
> <20090826 18:18:53.087000> id=1 symbol= lastTimestamp: 1251310542
> <20090826 18:18:53.088000> id=1 symbol= lastTimestamp: 1251310728
> <20090826 18:18:53.089000> id=1 symbol= lastPrice: 19.4
> <20090826 18:18:53.090000> id=1 symbol= lastSize: 1
> <20090826 18:18:53.091000> id=1 symbol= Volume: 54057
> <20090826 18:18:53.091000> id=1 symbol= highPrice: 19.68
> <20090826 18:18:53.093000> id=1 symbol= lowPrice: 19.21
> <20090826 18:18:53.094000> id=1 symbol= closePrice: 19.5
> <20090826 18:18:53.094000> id=1 symbol= openPrice: 19.45
> <20090826 18:18:53.098000> id=1 symbol= bidPrice: 19.39 ?bidSize: 214
> <20090826 18:18:53.099000> id=1 symbol= askPrice: 19.4 ?askSize: 156
> <20090826 18:18:53.100000> id=1 symbol= bidSize: 214
> <20090826 18:18:53.100000> id=1 symbol= askSize: 156
>
> I tried to override the tickPrice function in my eWrapper (as in the
> documentation example):
>
>> myWrapper <- eWrapper()
>> myWrapper$tickPrice <- function(msg, timestamp, file, ...) {}
>>
>> # add new tickPrice action
>> myWrapper$tickPrice <- function(msg, timestamp, file, ...) {
> cat("tickPrice",msg) }
>
> Now when I call the reqMktData I get the followng output:
>
>> reqMktData(tws, twsSTK("SBUX"), eventWrapper=myWrapper)
> <20090826 17:56:13.473000> id=1 symbol= Volume: 51409
> tickPrice 1tickPrice 1tickPrice 1<20090826 17:56:13.592000> id=1 symbol=
> Volume: 51409
> tickPrice 1tickPrice 1<20090826 17:56:14.214000> id=1 symbol= lastTimestamp:
> 1251309292
> <20090826 17:56:14.231000> id=1 symbol= lastTimestamp: 1251309383
> tickPrice 1<20090826 17:56:14.240000> id=1 symbol= lastSize: 1
> <20090826 17:56:14.254000> id=1 symbol= Volume: 51515
> tickPrice 1tickPrice 1tickPrice 1tickPrice 1<20090826 17:56:14.269000> id=1
> symbol= lastTimestamp: 1251309292
> <20090826 17:56:14.276000> id=1 symbol= lastTimestamp: 1251309383
> tickPrice 1<20090826 17:56:14.289000> id=1 symbol= lastSize: 1
> tickPrice 1tickPrice 1tickPrice 1tickPrice 1tickPrice 1<20090826
> 17:56:14.300000> id=1 symbol= bidSize: 134
> <20090826 17:56:14.310000> id=1 symbol= askSize: 202
> <20090826 17:56:14.324000> id=1 symbol= optionCallOpenInterest: 183787
> <20090826 17:56:14.331000> id=1 symbol= optionPutOpenInterest: 150993
> <20090826 17:56:14.345000> id=1 symbol= optionImpliedVol:
> 0.3800353909944552
> <20090826 17:56:14.358000> id=1 symbol= averageVolume: 133558
>
> If you look at the highlighted rows in the output, what strikes me as odd is
> that:
> 1) it looks like the tickPrice function is called again before the message
> could be printed. Is this correct, in other words are these functions
> re-entrant?
> 2) where are the bid and ask prices?
>
> I also tried overriding other functions in myWrapper like tickGeneric() and
> tickString() but they also don't give me the bid/ask price.
> I am probably doing something very wrong here but I can't figure out what it
> is.
>
> I am on windows (R: 2.8.1) and IBrokers version: IBrokers_0.2-2
>
> Any help appreciated,
>
> Regards,
>
> -Mark-
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From alexios at 4dscape.com  Thu Aug 27 17:01:12 2009
From: alexios at 4dscape.com (alexios)
Date: Thu, 27 Aug 2009 16:01:12 +0100
Subject: [R-SIG-Finance] Forecasting GARCH
In-Reply-To: <6F143AD637C8874C8BB255302BC32A317D6E74@SGFC-BIZ.SGFC-DOM.sgfc.ch>
References: <6F143AD637C8874C8BB255302BC32A317D6E74@SGFC-BIZ.SGFC-DOM.sgfc.ch>
Message-ID: <4A969FB8.3070109@4dscape.com>

A valid point...will aim to add this functionality to rgarch over the 
weekend (at the earliest) by extending the forecast method to accept a 
specification and data object instead of only a fitted object.

-Alexios Ghalanos

Cristian Gonzalez wrote:
> Dear All,
> 
> I have a question regarding the implementation in R of the paper
> "Prediction in dynamic models with time-dependent conditional variance"
> by Baillie and Bollerslev, Journal of Econometrics 52 (1992) 91-113.
> 
>  
> 
> The idea is to run GARCH in one time series and after that to use
> estimators in a new (several) more time series for prediction.
> 
>  
> 
> Using R, available packages (fGarch, rGarch, etc.) do not have this
> routine. The predict function allows the forecast only of the previous
> time series; garchpred(estimation,n.ahead=5)
> 
>  
> 
> MATLAB has this routine for a new time series using the garchpred
> function; garchpred(coef,newtimeseries,5)
> 
>  
> 
> I am working only with R and I would like to continue working without
> using other programs. Do you know how I can to do it in R?
> 
>  
> 
> Thanks in advance,
> 
> Cristian Gonzalez 
> 
>  
> 
>  
> 
>  
> 
>     
> 
>  
> 
> **********************************************************************************************
> IMPORTANT: The contents of this email and any attachments are confidential. They are intended for the 
> named recipient(s) only.
> If you have received this email in error, please notify the system manager or the sender immediately and do 
> not disclose the contents to anyone or make copies thereof.
> *** SGFC scanned this email for viruses, vandals, and malicious content. ***
> **********************************************************************************************
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
>


From seancarmody at gmail.com  Fri Aug 28 04:30:12 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Fri, 28 Aug 2009 12:30:12 +1000
Subject: [R-SIG-Finance] Date format for quantmod SQLite tables
Message-ID: <ce6bbb9d0908271930x14ce151k6fa1a6fc7c0c30c8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090828/0b349d8e/attachment.pl>

From jeff.a.ryan at gmail.com  Fri Aug 28 04:58:10 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 27 Aug 2009 21:58:10 -0500
Subject: [R-SIG-Finance] Date format for quantmod SQLite tables
In-Reply-To: <ce6bbb9d0908271930x14ce151k6fa1a6fc7c0c30c8@mail.gmail.com>
References: <ce6bbb9d0908271930x14ce151k6fa1a6fc7c0c30c8@mail.gmail.com>
Message-ID: <e8e755250908271958k2aaf787rf92363d8871820c0@mail.gmail.com>

Hi Sean,

The wrapper in quantmod is mostly a 'how you might use' function.

That said, without any modification to the getSymbols.SQLite function,
you would need the row_names to be text representations of numeric
time, either POSIXct or Date.

> as.numeric(Sys.Date())
[1] 14483
> as.numeric(Sys.time())
[1] 1251428145

As this gets converted via:

if (POSIX) {
            d <- as.numeric(fr[, 1])
            class(d) <- c("POSIXt", "POSIXct")
            fr <- xts(fr[, -1], order.by = d)
        }
        else {
            fr <- xts(fr[, -1], order.by = as.Date(as.numeric(fr[,
                1]), origin = "1970-01-01"))
        }

where POSIX=TRUE by default.

You could of course change the function to reflect your actual table
construction, and get rid of the as.numeric conversion, instead
calling as.Date or as.POSIXct directly.

HTH
Jeff


On Thu, Aug 27, 2009 at 9:30 PM, Sean Carmody<seancarmody at gmail.com> wrote:
> I am trying to use quantmod with src="SQLite". I have looked at the
> documentation for getSymbols.sqlite and feel I am getting close. For each
> symbol, I have a SQLite table with the fields
>
> row_names TEXT
> Open NUMERIC
> High NUMERIC
> Low NUMERIC
> Close NUMERIC
> Volume NUMERIC
> Adjusted NUMERIC
>
> When I use getSymbols I get a warning sayig "NAs introduced by coercion".
> The data seems to have been loaded, except for the row names (zoo index),
> which are all NA. At the moment, the contents of my row_names field are text
> strings with the format YYYY-MM-DD. Is it simply a matter of changing the
> format of these strings, or is there something else I have to do to get this
> working?
>
> Regards,
> Sean.
>
> --
> Sean Carmody
>
> The Stubborn Mule
> http://www.stubbornmule.net
> http://twitter.com/seancarmody
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From arun.kumar.saha at gmail.com  Fri Aug 28 10:18:13 2009
From: arun.kumar.saha at gmail.com (Arun.stat)
Date: Fri, 28 Aug 2009 01:18:13 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Forecasting GARCH
In-Reply-To: <6F143AD637C8874C8BB255302BC32A317D6E74@SGFC-BIZ.SGFC-DOM.sgfc.ch>
References: <6F143AD637C8874C8BB255302BC32A317D6E74@SGFC-BIZ.SGFC-DOM.sgfc.ch>
Message-ID: <25185941.post@talk.nabble.com>


In my best knowledge prediction for garch process was already discussed in
this forum. Why dont you have some search here?

Best,


Cristian Gonzalez wrote:
> 
> Dear All,
> 
> I have a question regarding the implementation in R of the paper
> "Prediction in dynamic models with time-dependent conditional variance"
> by Baillie and Bollerslev, Journal of Econometrics 52 (1992) 91-113.
> 
>  
> 
> The idea is to run GARCH in one time series and after that to use
> estimators in a new (several) more time series for prediction.
> 
>  
> 
> Using R, available packages (fGarch, rGarch, etc.) do not have this
> routine. The predict function allows the forecast only of the previous
> time series; garchpred(estimation,n.ahead=5)
> 
>  
> 
> MATLAB has this routine for a new time series using the garchpred
> function; garchpred(coef,newtimeseries,5)
> 
>  
> 
> I am working only with R and I would like to continue working without
> using other programs. Do you know how I can to do it in R?
> 
>  
> 
> Thanks in advance,
> 
> Cristian Gonzalez 
> 
>  
> 
>  
> 
>  
> 
>     
> 
>  
> 
> **********************************************************************************************
> IMPORTANT: The contents of this email and any attachments are
> confidential. They are intended for the 
> named recipient(s) only.
> If you have received this email in error, please notify the system manager
> or the sender immediately and do 
> not disclose the contents to anyone or make copies thereof.
> *** SGFC scanned this email for viruses, vandals, and malicious content.
> ***
> **********************************************************************************************
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Forecasting-GARCH-tp25172887p25185941.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From arun.kumar.saha at gmail.com  Fri Aug 28 14:01:26 2009
From: arun.kumar.saha at gmail.com (Arun.stat)
Date: Fri, 28 Aug 2009 05:01:26 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Forecasting GARCH
In-Reply-To: <25185941.post@talk.nabble.com>
References: <6F143AD637C8874C8BB255302BC32A317D6E74@SGFC-BIZ.SGFC-DOM.sgfc.ch>
	<25185941.post@talk.nabble.com>
Message-ID: <25188696.post@talk.nabble.com>


Well, please ignore my previous mail. I have misread that. I assumed you are
talking on the garch prediction. However I think that, what you pointed
there should be valid and learned there should this kind of functionality in
R as well.

Best,


Arun.stat wrote:
> 
> In my best knowledge prediction for garch process was already discussed in
> this forum. Why dont you have some search here?
> 
> Best,
> 
> 
> Cristian Gonzalez wrote:
>> 
>> Dear All,
>> 
>> I have a question regarding the implementation in R of the paper
>> "Prediction in dynamic models with time-dependent conditional variance"
>> by Baillie and Bollerslev, Journal of Econometrics 52 (1992) 91-113.
>> 
>>  
>> 
>> The idea is to run GARCH in one time series and after that to use
>> estimators in a new (several) more time series for prediction.
>> 
>>  
>> 
>> Using R, available packages (fGarch, rGarch, etc.) do not have this
>> routine. The predict function allows the forecast only of the previous
>> time series; garchpred(estimation,n.ahead=5)
>> 
>>  
>> 
>> MATLAB has this routine for a new time series using the garchpred
>> function; garchpred(coef,newtimeseries,5)
>> 
>>  
>> 
>> I am working only with R and I would like to continue working without
>> using other programs. Do you know how I can to do it in R?
>> 
>>  
>> 
>> Thanks in advance,
>> 
>> Cristian Gonzalez 
>> 
>>  
>> 
>>  
>> 
>>  
>> 
>>     
>> 
>>  
>> 
>> **********************************************************************************************
>> IMPORTANT: The contents of this email and any attachments are
>> confidential. They are intended for the 
>> named recipient(s) only.
>> If you have received this email in error, please notify the system
>> manager or the sender immediately and do 
>> not disclose the contents to anyone or make copies thereof.
>> *** SGFC scanned this email for viruses, vandals, and malicious content.
>> ***
>> **********************************************************************************************
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>> 
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/Forecasting-GARCH-tp25172887p25188696.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From brian at braverock.com  Fri Aug 28 14:53:59 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 28 Aug 2009 07:53:59 -0500
Subject: [R-SIG-Finance] Forecasting GARCH
In-Reply-To: <6F143AD637C8874C8BB255302BC32A317D6E74@SGFC-BIZ.SGFC-DOM.sgfc.ch>
References: <6F143AD637C8874C8BB255302BC32A317D6E74@SGFC-BIZ.SGFC-DOM.sgfc.ch>
Message-ID: <4A97D367.4000201@braverock.com>

Cristian Gonzalez wrote:
> I have a question regarding the implementation in R of the paper
> "Prediction in dynamic models with time-dependent conditional variance"
> by Baillie and Bollerslev, Journal of Econometrics 52 (1992) 91-113.
>
> The idea is to run GARCH in one time series and after that to use
> estimators in a new (several) more time series for prediction.
>
> Using R, available packages (fGarch, rGarch, etc.) do not have this
> routine. The predict function allows the forecast only of the previous
> time series; garchpred(estimation,n.ahead=5)
>
> MATLAB has this routine for a new time series using the garchpred
> function; garchpred(coef,newtimeseries,5)
>
> I am working only with R and I would like to continue working without
> using other programs. Do you know how I can to do it in R?
>   
Arun was correct, this has been covered before. I asked the question 
regarding fGarch, and Yohan answered. Unfortunately, I have not had time 
to complete working this out for fGarch following his excellent 
suggestion. The contents of our interchange are copied below:

   BGP> I've been continuing to examine the fGarch code, and I think
   BGP> that I can probably do most of what I want by fitting a model,
   BGP> overriding.series, and then calling .garchLLH although I've
   BGP> not yet confirmed that this is the case.


Yohan Chalabi wrote:

Hi Brian,

overriding .series is probably your best option. Note that .series and
other variables stored in the .fGArchEnv environment used to be global
variables. Moving those global variables to an environment was our best
solution to avoid problems with global variables without modifying to
much code.

library(fGarch)
fit <- garchFit(~garch(1,1), dem2gbp)
ls(all.names = TRUE, envir = fGarch:::.fGarchEnv)

you can use .getfGarchEnv and .getfGarchEnv to retrieve and
set new values in this environment. 

Note that .series is scaled by default in .garchFit(). If you override
.series$x, do not forget to change .series$scale because it will be used
in .garchLLH.

Calling .garchLLH with fGarchEnv = TRUE will update the variables
in .fGarchEnv.


As a side note, there is a handy update method for fGARCH object. You
can re-fit the model with new parameters, for example

update(fit, ~aparch(1,1))

HTH
Yohan

   BGP>
   BGP> I understand completely that I can predict by using something
   BGP> like rollapply or apply.fromstart to repeat garchFit and then
   BGP> predict.
   BGP>
   BGP> However, I think that much of the information in a garch
   BGP> model can be extracted without refitting if we simply want 
   BGP> to calculate the conditional variance without refitting the 
   BGP> model.  predict() would likely also be able to be applied 
   BGP> in this way. 
   BGP> Any confirmation on where to look in the code would be
   BGP> appreciated.  
   BGP> As always, any modified code I work up will be contributed back.


Regards,

- Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From rhelpacc at gmail.com  Mon Aug 31 03:38:44 2009
From: rhelpacc at gmail.com (R_help Help)
Date: Sun, 30 Aug 2009 21:38:44 -0400
Subject: [R-SIG-Finance] Aggregating irregular time series
Message-ID: <ad1ead5f0908301838g3a64adf1rc66df7b36c3fbdc0@mail.gmail.com>

Hi,

I have a couple of aggregation operations that I don't know how to
accomplish. Let me give an example. I have the following irregular
time series

time ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?x
10:00:00.021 ? ? ? ? ? ? ? ?20
10:00:00.224 ? ? ? ? ? ? ? ?20
10:00:01.002 ? ? ? ? ? ? ? ?19
10:00:02:948 ? ? ? ? ? ? ? ?20

1) For each entry time, I'd like to get sum of x for the next 2
seconds (excluding itself). Using the above example, the output should
be

time ? ? ? ? ? ? ? ? ? ? ? ?sumx
10:00:00.021 ? ? ? ? ? ? ? ?39
10:00:00.224 ? ? ? ? ? ? ? ?19
10:00:01.442 ? ? ? ? ? ? ? ?20
10:00:02:948 ? ? ? ? ? ? ? ? 0

2) For each i-th of x in the series, what's the first passage time to
x[i]-1. I.e. the output should be

time ? ? ? ? ? ? ? ? ? ? ? ? ? firstPassgeTime
10:00:00.021 ? ? ? ? ? ? ? ?0.981
10:00:00.224 ? ? ? ? ? ? ? ?0.778
10:00:01.442 ? ? ? ? ? ? ? ?NA
10:00:02:948 ? ? ? ? ? ? ? ?NA

Is there any shortcut function that allows me to do the above? Thank you.

adschai


From ggrothendieck at gmail.com  Mon Aug 31 05:08:11 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 30 Aug 2009 23:08:11 -0400
Subject: [R-SIG-Finance] Aggregating irregular time series
In-Reply-To: <ad1ead5f0908301838g3a64adf1rc66df7b36c3fbdc0@mail.gmail.com>
References: <ad1ead5f0908301838g3a64adf1rc66df7b36c3fbdc0@mail.gmail.com>
Message-ID: <971536df0908302008p379ee9b3v887a959c400a2d1f@mail.gmail.com>

Try this for the first question:

neighborapply <- function(z, width, FUN) {
	out <- z
	ix <- seq_along(z)
	jx <- findInterval(time(z) + width, time(z))
	out[] <- mapply(function(i, j) FUN(c(0, z[seq(i+1, length = j-i)])), ix, jx)
	out
}

# test - corrected :948 in last line

library(zoo)
library(chron)

Lines <- "Time                              x
10:00:00.021                20
10:00:00.224                20
10:00:01.002                19
10:00:02.948                20"

z <- read.zoo(textConnection(Lines), header = TRUE, FUN = times)

neighborapply(z, times("00:00:02"), sum)

# and here is an alternative neighborapply
# using loops.  The non-loop solution does
# have the disadvantage that it does as
# readily extend to other situations which
# is why we add this second solution to
# the first question.

neighborapply <- function(z, width, FUN) {
	out <- z
	tt <- time(z)
	for(i in seq_along(tt)) {
		for(j in seq_along(tt)) {
			if (tt[j] - tt[i] > width) break
		}
		if (j == length(tt) && tt[j] - tt[i] <= width) j <- j+1
		out[i] <- FUN(c(0, z[seq(i+1, length = j-i-1)]))
	}
	out
}

The second question can be answered along the lines
of the first by modifying neighborapply in the loop solution
appropriately.

On Sun, Aug 30, 2009 at 9:38 PM, R_help Help<rhelpacc at gmail.com> wrote:
> Hi,
>
> I have a couple of aggregation operations that I don't know how to
> accomplish. Let me give an example. I have the following irregular
> time series
>
> time ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?x
> 10:00:00.021 ? ? ? ? ? ? ? ?20
> 10:00:00.224 ? ? ? ? ? ? ? ?20
> 10:00:01.002 ? ? ? ? ? ? ? ?19
> 10:00:02:948 ? ? ? ? ? ? ? ?20
>
> 1) For each entry time, I'd like to get sum of x for the next 2
> seconds (excluding itself). Using the above example, the output should
> be
>
> time ? ? ? ? ? ? ? ? ? ? ? ?sumx
> 10:00:00.021 ? ? ? ? ? ? ? ?39
> 10:00:00.224 ? ? ? ? ? ? ? ?19
> 10:00:01.442 ? ? ? ? ? ? ? ?20
> 10:00:02:948 ? ? ? ? ? ? ? ? 0
>
> 2) For each i-th of x in the series, what's the first passage time to
> x[i]-1. I.e. the output should be
>
> time ? ? ? ? ? ? ? ? ? ? ? ? ? firstPassgeTime
> 10:00:00.021 ? ? ? ? ? ? ? ?0.981
> 10:00:00.224 ? ? ? ? ? ? ? ?0.778
> 10:00:01.442 ? ? ? ? ? ? ? ?NA
> 10:00:02:948 ? ? ? ? ? ? ? ?NA
>
> Is there any shortcut function that allows me to do the above? Thank you.
>
> adschai
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From multeesl at yahoo.co.uk  Mon Aug 31 13:56:47 2009
From: multeesl at yahoo.co.uk (bonjourbc9)
Date: Mon, 31 Aug 2009 04:56:47 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Correct specification for modelling
 a AR(p)-GJR GARCH(1, 1) - skewed t using fGARCH
Message-ID: <25222299.post@talk.nabble.com>


Hi folks, 
I am trying to model a fit for FTSE100 daily log returns. As a first step I
obtain the daily log returns using LN ( Pt / Pt-1) . Next suppose I define x
as the vector of log return series ; I used the fGARCH to model the AR(5) -
GJR GARCH(1,1) - skewed t . Can someone advise whether the following entry
is correct? 

>garchFit(FTSE100 ~arma(5,0) +aparch(1,1), data=x, init.rec = c("mci"),
delta = 2, skew = 1,
shape = 4, cond.dist = c("sstd"),
include.mean = TRUE, include.delta = FALSE, include.skew = NULL,
include.shape = NULL, leverage = NULL, trace = TRUE,
algorithm = c( "nlminb"),
control = list(), title = NULL, description = NULL)

I managed to obtain some results using this command. However the skewness
parameter returned from this command doesn't seems odd ( it is positive
value when my data exhibits negative skewness)

Can anyone help me with this code?Thanks.
-- 
View this message in context: http://www.nabble.com/Correct-specification-for-modelling-a-AR%28p%29-GJR-GARCH%281%2C1%29---skewed-t-using-fGARCH-tp25222299p25222299.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From nurogaleo at gmail.com  Mon Aug 31 20:48:19 2009
From: nurogaleo at gmail.com (=?ISO-8859-1?B?R2FzcGFyIE768WV6?=)
Date: Mon, 31 Aug 2009 11:48:19 -0700
Subject: [R-SIG-Finance] help on coef()
Message-ID: <9cd4bb290908311148x71f0c826vac7a87207c25ef86@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090831/21910588/attachment.pl>

From breno.neri at nyu.edu  Mon Aug 31 21:09:18 2009
From: breno.neri at nyu.edu (Breno Neri)
Date: Mon, 31 Aug 2009 15:09:18 -0400
Subject: [R-SIG-Finance] help on coef()
In-Reply-To: <9cd4bb290908311148x71f0c826vac7a87207c25ef86@mail.gmail.com>
References: <9cd4bb290908311148x71f0c826vac7a87207c25ef86@mail.gmail.com>
Message-ID: <917B5413-BB72-4512-AA3C-CA94C55B0280@nyu.edu>

Hi Gaspar,

Just type

?coef

To see the manual page about this command.

Best,
Breno

Sent from my iPhone

On Aug 31, 2009, at 14:48, Gaspar N??ez <nurogaleo at gmail.com> wrote:

> Hi
>
> i?m starting to work with R to do some regressions
>
> now, i need to extract a single estimated coeficient
> so that i can use it for further calculations
> i?ve been trying the coef() command (unsuccesfully)
>
> the 3000 pages reference manual does not give enough info,
> is there a way that one can solve these kind of problems
> without wasting so much time to find about an apparently
> simple command?
>
> regards
>
>
>
> -- 
> Gaspar
>
>    [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From HodgessE at uhd.edu  Mon Aug 31 22:15:19 2009
From: HodgessE at uhd.edu (Hodgess, Erin)
Date: Mon, 31 Aug 2009 15:15:19 -0500
Subject: [R-SIG-Finance] help on coef()
In-Reply-To: <9cd4bb290908311148x71f0c826vac7a87207c25ef86@mail.gmail.com>
References: <9cd4bb290908311148x71f0c826vac7a87207c25ef86@mail.gmail.com>
Message-ID: <70A5AC06FDB5E54482D19E1C04CDFCF30B7DA03D@BALI.uhd.campus>

Hi Gaspar:

I'm probably oversimplifying, but you could try:
> x <- 1:10
> y <- 3 + 2*x + runif(10)
> x.lm <- lm(y~x)
> summary(x.lm)

Call:
lm(formula = y ~ x)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.43425 -0.19559  0.03431  0.20469  0.40409 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  3.79881    0.20145   18.86 6.46e-08 ***
x            1.97021    0.03247   60.69 6.04e-12 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.2949 on 8 degrees of freedom
Multiple R-squared: 0.9978,     Adjusted R-squared: 0.9976 
F-statistic:  3683 on 1 and 8 DF,  p-value: 6.042e-12 

> x.lm$coef[1]
(Intercept) 
   3.798807 
>

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Gaspar N??ez
Sent: Monday, August 31, 2009 1:48 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] help on coef()

Hi

i?m starting to work with R to do some regressions

now, i need to extract a single estimated coeficient
so that i can use it for further calculations
i?ve been trying the coef() command (unsuccesfully)

the 3000 pages reference manual does not give enough info,
is there a way that one can solve these kind of problems
without wasting so much time to find about an apparently
simple command?

regards



-- 
Gaspar

	[[alternative HTML version deleted]]


From multeesl at yahoo.co.uk  Tue Sep  1 06:29:06 2009
From: multeesl at yahoo.co.uk (bonjourbc9)
Date: Mon, 31 Aug 2009 21:29:06 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Correct specification for
 modelling a AR(p)-GJR GARCH(1, 1) - skewed t using fGARCH
In-Reply-To: <25222299.post@talk.nabble.com>
References: <25222299.post@talk.nabble.com>
Message-ID: <25234164.post@talk.nabble.com>



Dear All,
while waiting for a reply I tried to tidy up my codes abit and this is what
I used to model a AR(1)-GARCH(1,1) with skewed student t distribution for
the residuals.

>fit1<-garchFit(EMEA~arma(1,0)+ garch(1,1),data=rr.emea ,cond.dist="sstd"
,trace=FALSE) 

This is what the fGARCH code returned;

Error Analysis:
        Estimate  Std. Error  t value Pr(>|t|)    
mu       0.08031     0.01902    4.223 2.41e-05 ***
ar1      0.09528     0.01835    5.194 2.06e-07 ***
omega    0.03102     0.00890    3.486  0.00049 ***
alpha1   0.10835     0.01399    7.745 9.55e-15 ***
beta1    0.87862     0.01519   57.848  < 2e-16 ***
skew     0.88764     0.02320   38.261  < 2e-16 ***
shape    7.37774     0.90894    8.117 4.44e-16 ***

My question is what is this skew parameter for ?Is it the skewness of the
residuals? or is it the skewness of the standardized residuals?? 

I tried to extract both the residuals and standardized residuals using the
following code;

>residuals(fit1 , standardize=FALSE)
>residuals(fit1,standardize=TRUE)

When I copy the residuals into excel and calculate its skewness , both
return me negative skewness of -0.5573 ( skew of standardized res) and
-0.85492 (skew of res). So what exactly is the skewness of 0.88764?? I
assume that the shape refers to the shape of the standardized errors?   
-- 
View this message in context: http://www.nabble.com/Correct-specification-for-modelling-a-AR%28p%29-GJR-GARCH%281%2C1%29---skewed-t-using-fGARCH-tp25222299p25234164.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jatinpatni at gmail.com  Tue Sep  1 12:35:28 2009
From: jatinpatni at gmail.com (jatin patni)
Date: Tue, 1 Sep 2009 16:05:28 +0530
Subject: [R-SIG-Finance] Quantmod and Tick Data
Message-ID: <fcc589cd0909010335g50b9c447y9ff05de64e168dd0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090901/5039096e/attachment.pl>

From brian at braverock.com  Tue Sep  1 13:26:16 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 01 Sep 2009 06:26:16 -0500
Subject: [R-SIG-Finance] Quantmod and Tick Data
In-Reply-To: <fcc589cd0909010335g50b9c447y9ff05de64e168dd0@mail.gmail.com>
References: <fcc589cd0909010335g50b9c447y9ff05de64e168dd0@mail.gmail.com>
Message-ID: <4A9D04D8.4040303@braverock.com>

jatin patni wrote:
> I am new to this field(Quantitative high frequency finance) and really would
> like some guidance from senior people. Please guide me to some good
> tutorials for handling dataframes and the xts package. I need to import data
> (time series, tick data) for my backtesting. The problem with tick data is:-
> 1)It has multiple symbols, so when I'm importing the data from a file into a
> dataframe, I have to again extract data into other dataframes with unique
> symbols(since my backtesting strategies will work on one symbol at a time,
> and for general charting purposes)
> 2)It does not have a date field, just the timestamp (hh:mm:ss), so I need to
> add the date field from the filename and convert it to a suitable format
> compatible with Quantmod/xts before starting to use the data for
> backtesting.
> 3)It is large, around 500MB per day(all symbols), so I need to split the
> file into 50000 rows per call into the dataframe. It may be a good idea to
> store it in a binary format of R.
>
> Since I'm a beginner with R, I'm having some stupid troubles handling
> dataframes, for eg. adding date into the columns of timestamps and
> converting it to a compatible format for quantmod/xts
>
> Please also guide me to some backtesting links(tutorials) or packages(like
> quantmod), preferably open source, for backtesting and even for implementing
> my own trading platform(for eg. Marketcetera)
>
>   
Jatin,

As you've probably guessed, most of your questions full into the 
category of "FAQ", so I'm not going to spend much time giving detailed 
answers to those, as a search of the list archives will turn up multiple 
details for read.table and read.zoo

I work with data very much like yours in that data may have multiple 
symbols in one extract from a database or similar.  I'll answer some of 
the particulars of your questions below.

1.
- use 'read.zoo' (or 'read.table' if 'read.zoo' doesn't work),
- use the 'format' argument in as.POSIXct to construct a POSIXct index 
for the zoo object
- simply hardcode the date from the filenames into your call to 
'format'  this should work quite well
- use 'split' to construct a list of zoo objects by symbol
- convert to 'xts' (now that you have unique timestamps) using 'lapply'
- name your columns for each symbol (also using 'lapply', I suspect)
- cbind if that's useful to you to get things out of the list

2.
- see above, use format= with as.POSIXct

3.
I regularly read files with 6-10 million rows into R, and I have read 
much larger files.  This should not be a problem for you.

If these pointers and some searching don't solve your problem, please 
read the posting guide, and reply to the list with a small data sample 
(a few symbols and data points for each would be sufficient) and the 
code you have tried to make it work.  Someone should be able to help you 
from that point, as I think my pointers and some archive searching 
should be sufficient.

After you are able to successfully load and manipulate your data, you 
can ask additional questions regarding backtesting in R once you've had 
a chance to review the list archives and formulate more specific 
questions with the code examples of what you're trying to do.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From daniel.cegielka at gmail.com  Tue Sep  1 13:49:33 2009
From: daniel.cegielka at gmail.com (Daniel Cegielka)
Date: Tue, 01 Sep 2009 13:49:33 +0200
Subject: [R-SIG-Finance] Quantmod and Tick Data
In-Reply-To: <fcc589cd0909010335g50b9c447y9ff05de64e168dd0@mail.gmail.com>
References: <fcc589cd0909010335g50b9c447y9ff05de64e168dd0@mail.gmail.com>
Message-ID: <4A9D0A4D.8030201@gmail.com>

jatin patni pisze:
> I am new to this field(Quantitative high frequency finance) and really would
> like some guidance from senior people. Please guide me to some good
> tutorials for handling dataframes and the xts package. I need to import data
> (time series, tick data) for my backtesting. The problem with tick data is:-
> 1)It has multiple symbols, so when I'm importing the data from a file into a
> dataframe, I have to again extract data into other dataframes with unique
> symbols(since my backtesting strategies will work on one symbol at a time,
> and for general charting purposes)
> 2)It does not have a date field, just the timestamp (hh:mm:ss), so I need to
> add the date field from the filename and convert it to a suitable format
> compatible with Quantmod/xts before starting to use the data for
> backtesting.
> 3)It is large, around 500MB per day(all symbols), so I need to split the
> file into 50000 rows per call into the dataframe. It may be a good idea to
> store it in a binary format of R.
>   

jatin patni pisze:
> I am new to this field(Quantitative high frequency finance) and really would
> like some guidance from senior people. Please guide me to some good
> tutorials for handling dataframes and the xts package. I need to import data
> (time series, tick data) for my backtesting. The problem with tick data is:-
> 1)It has multiple symbols, so when I'm importing the data from a file into a
> dataframe, I have to again extract data into other dataframes with unique
> symbols(since my backtesting strategies will work on one symbol at a time,
> and for general charting purposes)
> 2)It does not have a date field, just the timestamp (hh:mm:ss), so I need to
> add the date field from the filename and convert it to a suitable format
> compatible with Quantmod/xts before starting to use the data for
> backtesting.
> 3)It is large, around 500MB per day(all symbols), so I need to split the
> file into 50000 rows per call into the dataframe. It may be a good idea to
> store it in a binary format of R.
>   


http://www.quantmod.com/whatsnext/

    * (...)
    * More high frequency testing - possibly tick data
    * More getSymbols methods
          o (...)
          o hdf5

and hdf5:

http://cran.r-project.org/web/packages/hdf5/index.html

daniel

> Since I'm a beginner with R, I'm having some stupid troubles handling
> dataframes, for eg. adding date into the columns of timestamps and
> converting it to a compatible format for quantmod/xts
>
> Please also guide me to some backtesting links(tutorials) or packages(like
> quantmod), preferably open source, for backtesting and even for implementing
> my own trading platform(for eg. Marketcetera)
>
> Thanks
>
> Regards
> Jatin
>
>   





> Since I'm a beginner with R, I'm having some stupid troubles handling
> dataframes, for eg. adding date into the columns of timestamps and
> converting it to a compatible format for quantmod/xts
>
> Please also guide me to some backtesting links(tutorials) or packages(like
> quantmod), preferably open source, for backtesting and even for implementing
> my own trading platform(for eg. Marketcetera)
>
> Thanks
>
> Regards
> Jatin
>
>


From nicolas.chapados at gmail.com  Tue Sep  1 16:35:20 2009
From: nicolas.chapados at gmail.com (Nicolas Chapados)
Date: Tue, 1 Sep 2009 10:35:20 -0400
Subject: [R-SIG-Finance] Regression in fPortfolio? Sorting by date does not
	work...
Message-ID: <6fdf6d430909010735o87663a7rebed218d826ccb23@mail.gmail.com>

I am currently following the examples in the ebook "Portfolio
Optimization with R/RMetrics" (by W?rtz et al.) and the example on p.
16 simply does not work.  Here is what I get::

> set.seed(1953)
> charvec <- format(timeCalendar(2008, sample(12,6)))
> data <- matrix(round(rnorm(6), 3))
> t1 <- sort(timeSeries(data, charvec, units="A"))
Error in is.unsorted(x at positions, na.rm = na.rm) :
  1 argument passed to .Internal(is.unsorted) which requires 2

I'm using the latest binary R from CRAN (2.9.1) on Ubuntu 9.04, and
all RMetrics packages installed from scratch yesterday.  I just did an
update.packages() as suggested on p. xiii of the ebook.  So everything
should be fresh as on CRAN (as of August 31st, 2009).  For the record,
here is the transcript I obtain after loading library(fPortfolio)
immediately after starting R::

> library(fPortfolio)
Loading required package: MASS
Loading required package: timeDate
Loading required package: timeSeries
Loading required package: fBasics
Loading required package: fCopulae
Loading required package: sn
Loading required package: mnormt
Package 'sn', 0.4-12 (2009-03-21). Type 'help(SN)' for summary information
Loading required package: adapt
Loading required package: fAssets
Loading required package: robustbase

Attaching package: 'fAssets'


	The following object(s) are masked from package:fCopulae :

	 .mvstFit

Loading required package: quadprog
Loading required package: Rglpk
Loading required package: slam

Attaching package: 'slam'


	The following object(s) are masked from package:timeSeries :

	 colMeans,
	 colSums


	The following object(s) are masked from package:base :

	 colMeans,
	 colSums,
	 rowMeans,
	 rowSums

Using the GLPK callable library version 4.37
Loading required package: Rsymphony
Warning message:
'DESCRIPTION' file has 'Encoding' field and re-encoding is not possible



This error is somewhat bothersome, since the sorting functions are
used in many places within RMetrics...

Any help is greatly appreciated!

Many thanks,
+ Nicolas Chapados


From andrew.ellis at rmetrics.org  Tue Sep  1 17:32:59 2009
From: andrew.ellis at rmetrics.org (Andrew Ellis)
Date: Tue, 1 Sep 2009 17:32:59 +0200
Subject: [R-SIG-Finance] Regression in fPortfolio? Sorting by date does
	not work...
In-Reply-To: <6fdf6d430909010735o87663a7rebed218d826ccb23@mail.gmail.com>
References: <6fdf6d430909010735o87663a7rebed218d826ccb23@mail.gmail.com>
Message-ID: <92c0bfb90909010832p4f84738asde2d26d8fb82d2e6@mail.gmail.com>

Hi Nicholas,

we are unable to reproduce this. Could you please give me the output from

sessionInfo()

Thanks,
Andrew

PS. Btw, R 2.9.2 should be available for Ubuntu as Sep 1

On Tue, Sep 1, 2009 at 4:35 PM, Nicolas
Chapados<nicolas.chapados at gmail.com> wrote:
> I am currently following the examples in the ebook "Portfolio
> Optimization with R/RMetrics" (by W?rtz et al.) and the example on p.
> 16 simply does not work. ?Here is what I get::
>
>> set.seed(1953)
>> charvec <- format(timeCalendar(2008, sample(12,6)))
>> data <- matrix(round(rnorm(6), 3))
>> t1 <- sort(timeSeries(data, charvec, units="A"))
> Error in is.unsorted(x at positions, na.rm = na.rm) :
> ?1 argument passed to .Internal(is.unsorted) which requires 2
>
> I'm using the latest binary R from CRAN (2.9.1) on Ubuntu 9.04, and
> all RMetrics packages installed from scratch yesterday. ?I just did an
> update.packages() as suggested on p. xiii of the ebook. ?So everything
> should be fresh as on CRAN (as of August 31st, 2009). ?For the record,
> here is the transcript I obtain after loading library(fPortfolio)
> immediately after starting R::
>
>> library(fPortfolio)
> Loading required package: MASS
> Loading required package: timeDate
> Loading required package: timeSeries
> Loading required package: fBasics
> Loading required package: fCopulae
> Loading required package: sn
> Loading required package: mnormt
> Package 'sn', 0.4-12 (2009-03-21). Type 'help(SN)' for summary information
> Loading required package: adapt
> Loading required package: fAssets
> Loading required package: robustbase
>
> Attaching package: 'fAssets'
>
>
> ? ? ? ?The following object(s) are masked from package:fCopulae :
>
> ? ? ? ? .mvstFit
>
> Loading required package: quadprog
> Loading required package: Rglpk
> Loading required package: slam
>
> Attaching package: 'slam'
>
>
> ? ? ? ?The following object(s) are masked from package:timeSeries :
>
> ? ? ? ? colMeans,
> ? ? ? ? colSums
>
>
> ? ? ? ?The following object(s) are masked from package:base :
>
> ? ? ? ? colMeans,
> ? ? ? ? colSums,
> ? ? ? ? rowMeans,
> ? ? ? ? rowSums
>
> Using the GLPK callable library version 4.37
> Loading required package: Rsymphony
> Warning message:
> 'DESCRIPTION' file has 'Encoding' field and re-encoding is not possible
>
>
>
> This error is somewhat bothersome, since the sorting functions are
> used in many places within RMetrics...
>
> Any help is greatly appreciated!
>
> Many thanks,
> + Nicolas Chapados
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From nicolas.chapados at gmail.com  Tue Sep  1 18:02:18 2009
From: nicolas.chapados at gmail.com (Nicolas Chapados)
Date: Tue, 1 Sep 2009 12:02:18 -0400
Subject: [R-SIG-Finance] Regression in fPortfolio? Sorting by date does
	not work...
In-Reply-To: <92c0bfb90909010832p4f84738asde2d26d8fb82d2e6@mail.gmail.com>
References: <6fdf6d430909010735o87663a7rebed218d826ccb23@mail.gmail.com> 
	<92c0bfb90909010832p4f84738asde2d26d8fb82d2e6@mail.gmail.com>
Message-ID: <6fdf6d430909010902v5970183ejc4ba399eaca70576@mail.gmail.com>

Hi Andrew,

Thanks for your quick reply.  I found a solution to the problem: I
installed the latest versions of timeDate and timeSeries as were
present on R-Forge (NOT CRAN).  It appears that the following bit of
code was added relatively recently in "methods-is.R" (in the
timeSeries) package:

if (getRversion() < "2.8.0") {
    setMethod("is.unsorted", "timeSeries", function(x, na.rm = FALSE)
        callGeneric(x at positions, na.rm = na.rm))
} else {
    setMethod("is.unsorted", "timeSeries", function(x, na.rm = FALSE,
strictly = FALSE)
         callGeneric(x at positions, na.rm = na.rm, strictly = strictly))
}

The problem I was experiencing previously appeared to be due to the
first form of the callCaneric being made, without the strictly=
argument.

Although this would be unnecessary, here is sessionInfo() now that the
new version is installed::

> sessionInfo()
R version 2.9.1 (2009-06-26)
i486-pc-linux-gnu

locale:
C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] timeSeries_2100.84 timeDate_2100.86


The problem appears solved for me right now, but there may remain sync
issues between CRAN packages and R versions...  Thanks again for
writing so quickly.

Best regards,
+ Nicolas Chapados



On Tue, Sep 1, 2009 at 11:32 AM, Andrew Ellis<andrew.ellis at rmetrics.org> wrote:
> Hi Nicholas,
>
> we are unable to reproduce this. Could you please give me the output from
>
> sessionInfo()
>
> Thanks,
> Andrew
>
> PS. Btw, R 2.9.2 should be available for Ubuntu as Sep 1
>
> On Tue, Sep 1, 2009 at 4:35 PM, Nicolas
> Chapados<nicolas.chapados at gmail.com> wrote:
>> I am currently following the examples in the ebook "Portfolio
>> Optimization with R/RMetrics" (by W?rtz et al.) and the example on p.
>> 16 simply does not work. ?Here is what I get::
>>
>>> set.seed(1953)
>>> charvec <- format(timeCalendar(2008, sample(12,6)))
>>> data <- matrix(round(rnorm(6), 3))
>>> t1 <- sort(timeSeries(data, charvec, units="A"))
>> Error in is.unsorted(x at positions, na.rm = na.rm) :
>> ?1 argument passed to .Internal(is.unsorted) which requires 2
>>
>> I'm using the latest binary R from CRAN (2.9.1) on Ubuntu 9.04, and
>> all RMetrics packages installed from scratch yesterday. ?I just did an
>> update.packages() as suggested on p. xiii of the ebook. ?So everything
>> should be fresh as on CRAN (as of August 31st, 2009). ?For the record,
>> here is the transcript I obtain after loading library(fPortfolio)
>> immediately after starting R::
>>
>>> library(fPortfolio)
>> Loading required package: MASS
>> Loading required package: timeDate
>> Loading required package: timeSeries
>> Loading required package: fBasics
>> Loading required package: fCopulae
>> Loading required package: sn
>> Loading required package: mnormt
>> Package 'sn', 0.4-12 (2009-03-21). Type 'help(SN)' for summary information
>> Loading required package: adapt
>> Loading required package: fAssets
>> Loading required package: robustbase
>>
>> Attaching package: 'fAssets'
>>
>>
>> ? ? ? ?The following object(s) are masked from package:fCopulae :
>>
>> ? ? ? ? .mvstFit
>>
>> Loading required package: quadprog
>> Loading required package: Rglpk
>> Loading required package: slam
>>
>> Attaching package: 'slam'
>>
>>
>> ? ? ? ?The following object(s) are masked from package:timeSeries :
>>
>> ? ? ? ? colMeans,
>> ? ? ? ? colSums
>>
>>
>> ? ? ? ?The following object(s) are masked from package:base :
>>
>> ? ? ? ? colMeans,
>> ? ? ? ? colSums,
>> ? ? ? ? rowMeans,
>> ? ? ? ? rowSums
>>
>> Using the GLPK callable library version 4.37
>> Loading required package: Rsymphony
>> Warning message:
>> 'DESCRIPTION' file has 'Encoding' field and re-encoding is not possible
>>
>>
>>
>> This error is somewhat bothersome, since the sorting functions are
>> used in many places within RMetrics...
>>
>> Any help is greatly appreciated!
>>
>> Many thanks,
>> + Nicolas Chapados
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From jatinpatni at gmail.com  Tue Sep  1 18:50:39 2009
From: jatinpatni at gmail.com (jatin patni)
Date: Tue, 1 Sep 2009 22:20:39 +0530
Subject: [R-SIG-Finance] Non unique timestamp in zoo object
Message-ID: <fcc589cd0909010950p7926eblc6c9b776e619f46b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090901/4f7389c9/attachment.pl>

From brian at braverock.com  Tue Sep  1 19:13:37 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 01 Sep 2009 12:13:37 -0500
Subject: [R-SIG-Finance] Non unique timestamp in zoo object
In-Reply-To: <fcc589cd0909010950p7926eblc6c9b776e619f46b@mail.gmail.com>
References: <fcc589cd0909010950p7926eblc6c9b776e619f46b@mail.gmail.com>
Message-ID: <4A9D5641.4070400@braverock.com>

jatin patni wrote:
> I'm trying to create a zoo object from a tick data file(sample below) which
> contains timestamps only.
> Filename: "20080202.trd"
> 1|BHARTIARTL|EQ|18:15:05|600|1
> 2|BHARTIARTL|EQ|18:15:05|600|99
> 3|GLENMARK|EQ|18:15:05|238.1|5
> 4|HINDALCO|EQ|18:15:05|43.75|100
> 5|BHARTIARTL|EQ|18:15:05|600|1
> 6|BHEL|EQ|18:15:05|1100|11
> 7|HINDALCO|EQ|18:15:06|43.2|1
> 8|CHAMBLFERT|EQ|18:15:06|46|10
> 9|CHAMBLFERT|EQ|18:15:06|46|90
> 10|BAJAUTOFIN|EQ|18:15:06|80|100
>
> As you can see that timestamps are not unique even at microsecond levels.
> Inorder to create a zoo/xts object to be used with quantmod later, How do
> I:-
> 1)Do I have to merge two rows with the same timestamp, symbols, price but
> different volumes.(Do I need to do this or is there some other way possible)
> 2)Append date to the timestamp while creating the zoo/xts object
>
> I'm trying to explore the 'format' argument, and POSIX and chron but without
> much success.
>   

Your target should be POSIXct, as that is what xts uses internally, 
given that your stated goal is to use xts and quantmod.

?as.POSIXct
?format.POSIXct

These look like one second bars, not microsecond bars.  To see 
microseconds, if you have them:

options(digits.secs=6)

zoo doesn't require unique timestamps, just ordered ones.  In this case, 
your trades are ordered in the files.  You might need to append an order 
in the microseconds if you really only have second bars.  It's also not 
necessary to have unique timestamps, but you'll be happier if you do.

Regards,

   - Brian


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From jatinpatni at gmail.com  Tue Sep  1 19:18:29 2009
From: jatinpatni at gmail.com (jatin patni)
Date: Tue, 1 Sep 2009 22:48:29 +0530
Subject: [R-SIG-Finance] Non unique timestamp in zoo object
In-Reply-To: <4A9D5641.4070400@braverock.com>
References: <fcc589cd0909010950p7926eblc6c9b776e619f46b@mail.gmail.com>
	<4A9D5641.4070400@braverock.com>
Message-ID: <fcc589cd0909011018m66c2417eoe86321432a4bc7f0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090901/5507f70d/attachment.pl>

From ggrothendieck at gmail.com  Tue Sep  1 19:17:37 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 1 Sep 2009 13:17:37 -0400
Subject: [R-SIG-Finance] Non unique timestamp in zoo object
In-Reply-To: <fcc589cd0909010950p7926eblc6c9b776e619f46b@mail.gmail.com>
References: <fcc589cd0909010950p7926eblc6c9b776e619f46b@mail.gmail.com>
Message-ID: <971536df0909011017y2843ca48p4f94172241aa3edf@mail.gmail.com>

Assuming you want the last value field and sum
of the volume fields try this:

Lines <- "1|BHARTIARTL|EQ|18:15:05|600|1
2|BHARTIARTL|EQ|18:15:05|600|99
3|GLENMARK|EQ|18:15:05|238.1|5
4|HINDALCO|EQ|18:15:05|43.75|100
5|BHARTIARTL|EQ|18:15:05|600|1
6|BHEL|EQ|18:15:05|1100|11
7|HINDALCO|EQ|18:15:06|43.2|1
8|CHAMBLFERT|EQ|18:15:06|46|10
9|CHAMBLFERT|EQ|18:15:06|46|90
10|BAJAUTOFIN|EQ|18:15:06|80|100"

library(zoo)
library(chron)

tail1 <- function(x) tail(x, 1)
cls <- c("NULL", "NULL", "NULL", "character", "numeric", "numeric")
nms <- c("", "", "", "time", "value", "volume")

z <- read.zoo(textConnection(Lines), aggregate = tail1,
	FUN = times, sep = "|", colClasses = cls, col.names = nms)

# re-read using sum
z2 <- read.zoo(textConnection(Lines), aggregate = sum,
	FUN = times, sep = "|", colClasses = cls, col.names = nms)

z$volume <- z2$volume
z

See zoo faq #1:
   vignette("zoo-faq")
and see ?read.zoo


On Tue, Sep 1, 2009 at 12:50 PM, jatin patni<jatinpatni at gmail.com> wrote:
> I'm trying to create a zoo object from a tick data file(sample below) which
> contains timestamps only.
> Filename: "20080202.trd"
> 1|BHARTIARTL|EQ|18:15:05|600|1
> 2|BHARTIARTL|EQ|18:15:05|600|99
> 3|GLENMARK|EQ|18:15:05|238.1|5
> 4|HINDALCO|EQ|18:15:05|43.75|100
> 5|BHARTIARTL|EQ|18:15:05|600|1
> 6|BHEL|EQ|18:15:05|1100|11
> 7|HINDALCO|EQ|18:15:06|43.2|1
> 8|CHAMBLFERT|EQ|18:15:06|46|10
> 9|CHAMBLFERT|EQ|18:15:06|46|90
> 10|BAJAUTOFIN|EQ|18:15:06|80|100
>
> As you can see that timestamps are not unique even at microsecond levels.
> Inorder to create a zoo/xts object to be used with quantmod later, How do
> I:-
> 1)Do I have to merge two rows with the same timestamp, symbols, price but
> different volumes.(Do I need to do this or is there some other way possible)
> 2)Append date to the timestamp while creating the zoo/xts object
>
> I'm trying to explore the 'format' argument, and POSIX and chron but without
> much success.
>
> Thanks
>
> --
> Jatin Patni
> Tel: 91 9833 2180 52
> jatinpatni at gmail.com
> www.jatinpatni.co.nr
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Tue Sep  1 19:22:31 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 1 Sep 2009 13:22:31 -0400
Subject: [R-SIG-Finance] Non unique timestamp in zoo object
In-Reply-To: <4A9D5641.4070400@braverock.com>
References: <fcc589cd0909010950p7926eblc6c9b776e619f46b@mail.gmail.com> 
	<4A9D5641.4070400@braverock.com>
Message-ID: <971536df0909011022m5acdc9f6nfff5354027cf85cf@mail.gmail.com>

Note that for any calculation that involves merge.zoo (which
is most calculations since even z1 + z2 can do a
merge underneath) one does require unique times in zoo.
read.zoo can read in times that are not unique but ultimately
you will want to make them unique for most operations.

The aggregate= argument of read.zoo provides a way to
collapse duplicates and there are additional facilities in the
devel version of zoo.

As indicated in my prior post in this thread zoo FAQ #1
has further discussion.

On Tue, Sep 1, 2009 at 1:13 PM, Brian G. Peterson<brian at braverock.com> wrote:
> jatin patni wrote:
>>
>> I'm trying to create a zoo object from a tick data file(sample below)
>> which
>> contains timestamps only.
>> Filename: "20080202.trd"
>> 1|BHARTIARTL|EQ|18:15:05|600|1
>> 2|BHARTIARTL|EQ|18:15:05|600|99
>> 3|GLENMARK|EQ|18:15:05|238.1|5
>> 4|HINDALCO|EQ|18:15:05|43.75|100
>> 5|BHARTIARTL|EQ|18:15:05|600|1
>> 6|BHEL|EQ|18:15:05|1100|11
>> 7|HINDALCO|EQ|18:15:06|43.2|1
>> 8|CHAMBLFERT|EQ|18:15:06|46|10
>> 9|CHAMBLFERT|EQ|18:15:06|46|90
>> 10|BAJAUTOFIN|EQ|18:15:06|80|100
>>
>> As you can see that timestamps are not unique even at microsecond levels.
>> Inorder to create a zoo/xts object to be used with quantmod later, How do
>> I:-
>> 1)Do I have to merge two rows with the same timestamp, symbols, price but
>> different volumes.(Do I need to do this or is there some other way
>> possible)
>> 2)Append date to the timestamp while creating the zoo/xts object
>>
>> I'm trying to explore the 'format' argument, and POSIX and chron but
>> without
>> much success.
>>
>
> Your target should be POSIXct, as that is what xts uses internally, given
> that your stated goal is to use xts and quantmod.
>
> ?as.POSIXct
> ?format.POSIXct
>
> These look like one second bars, not microsecond bars. ?To see microseconds,
> if you have them:
>
> options(digits.secs=6)
>
> zoo doesn't require unique timestamps, just ordered ones. ?In this case,
> your trades are ordered in the files. ?You might need to append an order in
> the microseconds if you really only have second bars. ?It's also not
> necessary to have unique timestamps, but you'll be happier if you do.
>
> Regards,
>
> ?- Brian
>
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Tue Sep  1 19:32:54 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 1 Sep 2009 12:32:54 -0500
Subject: [R-SIG-Finance] Non unique timestamp in zoo object
In-Reply-To: <971536df0909011022m5acdc9f6nfff5354027cf85cf@mail.gmail.com>
References: <fcc589cd0909010950p7926eblc6c9b776e619f46b@mail.gmail.com>
	<4A9D5641.4070400@braverock.com>
	<971536df0909011022m5acdc9f6nfff5354027cf85cf@mail.gmail.com>
Message-ID: <e8e755250909011032j46dedb15o3f082215d704bca8@mail.gmail.com>

Jatin,

quantmod charts need non-unique values at present.  I suspect that
will always be the case, as the x-index relies on time.

xts can handle merging with non-unique timestamps, but keep in mind
that many operations would be quite ill-defined.  Merging will work,
but Ops methods (+-/* etc) will make little sense if you aren't
cognisant of what is actually being performed. diff() and lag() will
also be problematic.

Aside from that, the myriad of documentation available on zoo (great),
xts (less great, but good), and using the above and R in finance
(check the list archives... learn the list archives) is quite
impressive.

Obviously the caliber of the reply on this list is awe inspiring as
well, but asking questions should really be a last resort, and only if
they haven't been asked before.

Jeff


On Tue, Sep 1, 2009 at 12:22 PM, Gabor
Grothendieck<ggrothendieck at gmail.com> wrote:
> Note that for any calculation that involves merge.zoo (which
> is most calculations since even z1 + z2 can do a
> merge underneath) one does require unique times in zoo.
> read.zoo can read in times that are not unique but ultimately
> you will want to make them unique for most operations.
>
> The aggregate= argument of read.zoo provides a way to
> collapse duplicates and there are additional facilities in the
> devel version of zoo.
>
> As indicated in my prior post in this thread zoo FAQ #1
> has further discussion.
>
> On Tue, Sep 1, 2009 at 1:13 PM, Brian G. Peterson<brian at braverock.com> wrote:
>> jatin patni wrote:
>>>
>>> I'm trying to create a zoo object from a tick data file(sample below)
>>> which
>>> contains timestamps only.
>>> Filename: "20080202.trd"
>>> 1|BHARTIARTL|EQ|18:15:05|600|1
>>> 2|BHARTIARTL|EQ|18:15:05|600|99
>>> 3|GLENMARK|EQ|18:15:05|238.1|5
>>> 4|HINDALCO|EQ|18:15:05|43.75|100
>>> 5|BHARTIARTL|EQ|18:15:05|600|1
>>> 6|BHEL|EQ|18:15:05|1100|11
>>> 7|HINDALCO|EQ|18:15:06|43.2|1
>>> 8|CHAMBLFERT|EQ|18:15:06|46|10
>>> 9|CHAMBLFERT|EQ|18:15:06|46|90
>>> 10|BAJAUTOFIN|EQ|18:15:06|80|100
>>>
>>> As you can see that timestamps are not unique even at microsecond levels.
>>> Inorder to create a zoo/xts object to be used with quantmod later, How do
>>> I:-
>>> 1)Do I have to merge two rows with the same timestamp, symbols, price but
>>> different volumes.(Do I need to do this or is there some other way
>>> possible)
>>> 2)Append date to the timestamp while creating the zoo/xts object
>>>
>>> I'm trying to explore the 'format' argument, and POSIX and chron but
>>> without
>>> much success.
>>>
>>
>> Your target should be POSIXct, as that is what xts uses internally, given
>> that your stated goal is to use xts and quantmod.
>>
>> ?as.POSIXct
>> ?format.POSIXct
>>
>> These look like one second bars, not microsecond bars. ?To see microseconds,
>> if you have them:
>>
>> options(digits.secs=6)
>>
>> zoo doesn't require unique timestamps, just ordered ones. ?In this case,
>> your trades are ordered in the files. ?You might need to append an order in
>> the microseconds if you really only have second bars. ?It's also not
>> necessary to have unique timestamps, but you'll be happier if you do.
>>
>> Regards,
>>
>> ?- Brian
>>
>>
>> --
>> Brian G. Peterson
>> http://braverock.com/brian/
>> Ph: 773-459-4973
>> IM: bgpbraverock
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From alexios at 4dscape.com  Tue Sep  1 21:38:19 2009
From: alexios at 4dscape.com (alexios)
Date: Tue, 01 Sep 2009 20:38:19 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Correct specification for
 modelling a AR(p)-GJR GARCH(1, 1) - skewed t using fGARCH
In-Reply-To: <25234164.post@talk.nabble.com>
References: <25222299.post@talk.nabble.com> <25234164.post@talk.nabble.com>
Message-ID: <4A9D782B.70508@4dscape.com>

The skewness and shape parameters are distributional parameters of the
skew-student distribution of Fernandez and Steel. In order to get from
those distributional parameters to the sample skewness you use in excel
you need to apply a transformation relating to the theoretical moments
of the distribution (hint: have a look at the Rockinger/Jondeau page
www.hec.unil.ch/matlabcodes/econometrics.html for this).

I believe the skew and shape parameters of the sstd distribution are 
invariant under linear transformation so whether you are talking about
standardized or non-standardized residuals they are the same.

HTH
-Alexios Ghalanos

bonjourbc9 wrote:
> 
> Dear All,
> while waiting for a reply I tried to tidy up my codes abit and this is what
> I used to model a AR(1)-GARCH(1,1) with skewed student t distribution for
> the residuals.
> 
>> fit1<-garchFit(EMEA~arma(1,0)+ garch(1,1),data=rr.emea ,cond.dist="sstd"
> ,trace=FALSE) 
> 
> This is what the fGARCH code returned;
> 
> Error Analysis:
>         Estimate  Std. Error  t value Pr(>|t|)    
> mu       0.08031     0.01902    4.223 2.41e-05 ***
> ar1      0.09528     0.01835    5.194 2.06e-07 ***
> omega    0.03102     0.00890    3.486  0.00049 ***
> alpha1   0.10835     0.01399    7.745 9.55e-15 ***
> beta1    0.87862     0.01519   57.848  < 2e-16 ***
> skew     0.88764     0.02320   38.261  < 2e-16 ***
> shape    7.37774     0.90894    8.117 4.44e-16 ***
> 
> My question is what is this skew parameter for ?Is it the skewness of the
> residuals? or is it the skewness of the standardized residuals?? 
> 
> I tried to extract both the residuals and standardized residuals using the
> following code;
> 
>> residuals(fit1 , standardize=FALSE)
>> residuals(fit1,standardize=TRUE)
> 
> When I copy the residuals into excel and calculate its skewness , both
> return me negative skewness of -0.5573 ( skew of standardized res) and
> -0.85492 (skew of res). So what exactly is the skewness of 0.88764?? I
> assume that the shape refers to the shape of the standardized errors?


From chalabi at phys.ethz.ch  Wed Sep  2 12:30:24 2009
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Wed, 2 Sep 2009 12:30:24 +0200
Subject: [R-SIG-Finance] Regression in fPortfolio? Sorting by date does
 not work...
In-Reply-To: <6fdf6d430909010902v5970183ejc4ba399eaca70576@mail.gmail.com>
References: <6fdf6d430909010735o87663a7rebed218d826ccb23@mail.gmail.com>
	<92c0bfb90909010832p4f84738asde2d26d8fb82d2e6@mail.gmail.com>
	<6fdf6d430909010902v5970183ejc4ba399eaca70576@mail.gmail.com>
Message-ID: <20090902123024.7e9e540e@mimi>

>>>> "NC" == Nicolas Chapados <nicolas.chapados at gmail.com>
>>>> on Tue, 1 Sep 2009 12:02:18 -0400

   NC> Thanks for your quick reply.  I found a solution to the
   NC> problem: I
   NC> installed the latest versions of timeDate and timeSeries as were
   NC> present on R-Forge (NOT CRAN).  It appears that the following
   NC> bit of
   NC> code was added relatively recently in methods-is.R (in the
   NC> timeSeries) package:
   NC>
   NC> if (getRversion() < 2.8.0) {
   NC> setMethod(is.unsorted, timeSeries, function(x, na.rm = FALSE)
   NC> callGeneric(x at positions, na.rm = na.rm))
   NC> } else {
   NC> setMethod(is.unsorted, timeSeries, function(x, na.rm = FALSE,
   NC> strictly = FALSE)
   NC> callGeneric(x at positions, na.rm = na.rm, strictly = strictly))
   NC> }
   NC>
   NC> The problem I was experiencing previously appeared to be due
   NC> to the
   NC> first form of the callCaneric being made, without the strictly=
   NC> argument.

Hi Nicolas,

This code has been in timeSeries since the introduction of
is.unsorted,timeSeries method. It used to be in "is.R" since
version 2100.81. The current version is 2100.83. 

I do not understand why it does not work for you with the current
packages on CRAN. We are unable to reproduce your error using
different versions of R and packages from CRAN.

We would be very grateful if you could try again using the packages
from CRAN so that we can try to reproduce any potential errors.

You do not need the dev-packages from R-Forge to run the examples of
the ebook.

regards,
Yohan

-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch


From sarswat at gmail.com  Wed Sep  2 13:26:13 2009
From: sarswat at gmail.com (sunil)
Date: Wed, 2 Sep 2009 16:56:13 +0530
Subject: [R-SIG-Finance]  Index of data is missing (dim is NULL)
Message-ID: <28fa0bac0909020426l550185dbne6b862a758043a24@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090902/3cd8cb7c/attachment.pl>

From josh.m.ulrich at gmail.com  Wed Sep  2 13:38:29 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 2 Sep 2009 06:38:29 -0500
Subject: [R-SIG-Finance] Index of data is missing (dim is NULL)
In-Reply-To: <28fa0bac0909020426l550185dbne6b862a758043a24@mail.gmail.com>
References: <28fa0bac0909020426l550185dbne6b862a758043a24@mail.gmail.com>
Message-ID: <8cca69990909020438u9f5b84crc5b150354590f8f6@mail.gmail.com>

Please use this list for finance-related questions.  Just because
you're using R to solve a finance-related problem does not mean your
question is finance-related.

In this case, your "problem" is that rel_data is a *vector*.  Please
read, "An Introduction to R" to understand the differences between
vectors and matrices / data.frames (and note that subsetting a matrix
via "[" with drop=TRUE can result in a vector).

> x <- 1:10  # This is a vector
> dim(x)
NULL
> x[1,]
Error in x[1, ] : incorrect number of dimensions
> matrix(1:10,2)  # This is a matrix
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    3    5    7    9
[2,]    2    4    6    8   10
> matrix(1:10,2)[1,]  # This is a vector
[1] 1 3 5 7 9
> matrix(1:10,2)[1,,drop=FALSE]  # This is a matrix
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    3    5    7    9


HTH,
Josh
--
http://www.fosstrading.com



On Wed, Sep 2, 2009 at 6:26 AM, sunil<sarswat at gmail.com> wrote:
> Hi Users,
> ? ? ? ? ? ? I am working on F&O order-book data, and I noticed index of
> subset of data is missing? Usually when you type the variable name it shows
> you row numbers like [1,], [2,] etc., but here row number is missing. When I
> ask for dim(data) command , it shows "NULL" ? Please see below
>
>> rel_data
> ? ? ? symbols insuments_type ? ?expire date ? ? ? quantity ? ? ? ? ?price
> ?time_stamp ? ? ? ? ? ? BS ? ? order_type
> ?"JPASSOCIAT" ? ? ? "FUTSTK" ? ? "20080424" ? ? ? ? ?"750" ? ? ? "240.90"
> ?"13:55:30" ? ? ? ? ? ?"S" ? ? ? ? ? "RL"
>> dim(rel_data)
> NULL
>> rel_data[1,]
> Error in rel_data[1, ] : incorrect number of dimensions
>>
>
>


From jatinpatni at gmail.com  Wed Sep  2 18:39:02 2009
From: jatinpatni at gmail.com (jatin patni)
Date: Wed, 2 Sep 2009 22:09:02 +0530
Subject: [R-SIG-Finance] R-Help(Aggregate function): Multiple functions for
	different collumns
Message-ID: <fcc589cd0909020939v23d27f98m7e20921922200c6b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090902/fdaa2009/attachment.pl>

From josh.m.ulrich at gmail.com  Wed Sep  2 18:49:02 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 2 Sep 2009 11:49:02 -0500
Subject: [R-SIG-Finance] R-Help(Aggregate function): Multiple functions
	for different collumns
In-Reply-To: <fcc589cd0909020939v23d27f98m7e20921922200c6b@mail.gmail.com>
References: <fcc589cd0909020939v23d27f98m7e20921922200c6b@mail.gmail.com>
Message-ID: <8cca69990909020949h4a2219eav5e7c4836f0b1e6ad@mail.gmail.com>

Jatin,

Check out the plyr package.  And there's a quick example of what
you're trying to do on this blog post:
http://www.cerebralmastication.com/?p=339

HTH,
Josh
--
http://www.fosstrading.com



On Wed, Sep 2, 2009 at 11:39 AM, jatin patni<jatinpatni at gmail.com> wrote:
> I'm trying to reduce the resolution or remove duplicate time stamps for the
> same symbols in the following sample file, in a way such that the new price
> is a volume weighted average for that second(timestamp) and the new volume
> is the sum of all volumes within that second.
>
> I'm trying to use sapply and aggregate separately but having trouble with
> the function. I want that the function should do a volume weighted avg for
> the price column in the subgroup(where timestamps and symbols are same) but
> a sum of volume in that group. How do I achieve both goals simultaneously.
>
> I did a lot of research on the mailing archives and spent hours on reading
> the help/faq but couldn't figure it out.
>
> Here's my codes that is working:-
>
> tmp1.split <- sapply(split(tmp1, tmp1$symbol),
> function(x){weighted.mean(tmp1$price, tmp1$volume)})
> tmp1.ag <- aggregate(tmp1, by=list(tmp1$symbol, tmp1$time),
> function(x){weighted.mean(tmp5$price, tmp5$volume)})
>
> How do I go ahead now(I'm confused and new to R)
> How do I write functions that handle multiple columns differently.
>
>>tmp1 #my dataframe
> ? sno ? ? symbol series ? ? time ? price volume
> 1 ? ?1 BHARTIARTL ? ? EQ 18:15:05 ?600.00 ? ? ?1
> 2 ? ?2 BHARTIARTL ? ? EQ 18:15:05 ?600.00 ? ? 99
> 3 ? ?3 ? GLENMARK ? ? EQ 18:15:05 ?238.10 ? ? ?5
> 4 ? ?4 ? HINDALCO ? ? EQ 18:15:05 ? 43.75 ? ?100
> 5 ? ?5 BHARTIARTL ? ? EQ 18:15:05 ?600.00 ? ? ?1
> 6 ? ?6 ? ? ? BHEL ? ? EQ 18:15:05 1100.00 ? ? 11
> 7 ? ?7 ? HINDALCO ? ? EQ 18:15:06 ? 43.20 ? ? ?1
> 8 ? ?8 CHAMBLFERT ? ? EQ 18:15:06 ? 46.00 ? ? 10
> 9 ? ?9 CHAMBLFERT ? ? EQ 18:15:06 ? 46.00 ? ? 90
> 10 ?10 BAJAUTOFIN ? ? EQ 18:15:06 ? 80.00 ? ?100
>
> Thanks
>
> --
> Jatin Patni
> Tel: 91 9833 2180 52
> jatinpatni at gmail.com
> www.jatinpatni.co.nr
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jatinpatni at gmail.com  Wed Sep  2 18:56:08 2009
From: jatinpatni at gmail.com (jatin patni)
Date: Wed, 2 Sep 2009 22:26:08 +0530
Subject: [R-SIG-Finance] R-Help(Aggregate function): Multiple functions
	for different collumns
In-Reply-To: <8cca69990909020949h4a2219eav5e7c4836f0b1e6ad@mail.gmail.com>
References: <fcc589cd0909020939v23d27f98m7e20921922200c6b@mail.gmail.com>
	<8cca69990909020949h4a2219eav5e7c4836f0b1e6ad@mail.gmail.com>
Message-ID: <fcc589cd0909020956h596cad35k6cd7a4b7d0bd85b3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090902/b0d05f7d/attachment.pl>

From nurogaleo at gmail.com  Wed Sep  2 19:21:07 2009
From: nurogaleo at gmail.com (=?ISO-8859-1?B?R2FzcGFyIE768WV6?=)
Date: Wed, 2 Sep 2009 10:21:07 -0700
Subject: [R-SIG-Finance] another silly question
Message-ID: <9cd4bb290909021021v51956a8fud1068d856df1719c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090902/d6ab0a73/attachment.pl>

From sean.oriordain at gmail.com  Wed Sep  2 21:03:37 2009
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Wed, 2 Sep 2009 20:03:37 +0100
Subject: [R-SIG-Finance] another silly question
In-Reply-To: <9cd4bb290909021021v51956a8fud1068d856df1719c@mail.gmail.com>
References: <9cd4bb290909021021v51956a8fud1068d856df1719c@mail.gmail.com>
Message-ID: <8ed68eed0909021203h2de24c05kc7157fcdb7175d96@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090902/ec7970a6/attachment.pl>

From babel at centrum.sk  Wed Sep  2 22:45:17 2009
From: babel at centrum.sk (babel at centrum.sk)
Date: Wed, 02 Sep 2009 22:45:17 +0200
Subject: [R-SIG-Finance] Measurement of forecasting accuracy-GFESM
Message-ID: <200909022245.16025@centrum.cz>

Hi users

Does anyone of you know or use the different measurement of forecasting accuracy, such as General error forecast second moment (GFESM) or Trace mean square error criterion (TMSFE) suggested by Clements and Hendry. Are there any implantations of these criterions in R? 

for details see http://books.google.sk/books?id=aJ966QLx0WAC&pg=PA74&lpg=PA74&dq=GFESM&source=bl&ots=uDIsI6awEt&sig=0K6wN2fQp5sLvnYgY61N_SOg3Ls&hl=sk&ei=4LKeSpGeOMHDsgaZh-C2AQ&sa=X&oi=book_result&ct=result&resnum=4#v=onepage&q=GFESM&f=false



Thank you very much


From guygreen at netvigator.com  Thu Sep  3 02:07:40 2009
From: guygreen at netvigator.com (gug)
Date: Wed, 2 Sep 2009 17:07:40 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] another silly question
In-Reply-To: <8ed68eed0909021203h2de24c05kc7157fcdb7175d96@mail.gmail.com>
References: <9cd4bb290909021021v51956a8fud1068d856df1719c@mail.gmail.com>
	<8ed68eed0909021203h2de24c05kc7157fcdb7175d96@mail.gmail.com>
Message-ID: <25267771.post@talk.nabble.com>


Yes - sink().

I use it like this:

multireg=lm(.....data....)
sink("C:/folder/destination-file.txt")
print(summary(multireg))
sink()

If you're looking for a step by step guide on using R for regression, I
found the "Basic Statistics and R: An Introductory Tutorial" (linked PDF) on
this page  http://ehsan.karim.googlepages.com/
http://ehsan.karim.googlepages.com/  to be very useful.

Guy




Sean O'Riordain wrote:
> 
> Hi Gaspar,
> 
> Not really finance related... but have a look at sink()  I use it to save
> long sessions to a text file automatically
> 
> Kind regards,
> Se?n
> 
> 
> On Wed, Sep 2, 2009 at 6:21 PM, Gaspar N??ez <nurogaleo at gmail.com> wrote:
> 
>> Hi everybody
>>
>> i've been looking through tutorials ant the web
>> but have not found a way to export the results
>> of a regression, such as the summary,
>> either as a .txt file or as a table
>> so that i can read it with the OO Calc (or Excel)
>>
>> any help on this will be appreciated
>> thanks in advance
>>
>> as a matter of fact:
>> i'm thinking about writing a small tutorial
>> based on a simple regression were everything
>> can be replicated by a student
>> following easy directions
>> I've found that most of the documentation
>> comes in a very technical jargon
>> not easy to follow by non-experts
>>
>>
>>
>> --
>> Gaspar
>>
> 

-- 
View this message in context: http://www.nabble.com/another-silly-question-tp25262050p25267771.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From weihanliu2002 at yahoo.com  Thu Sep  3 03:21:05 2009
From: weihanliu2002 at yahoo.com (Wei-han Liu)
Date: Wed, 2 Sep 2009 18:21:05 -0700 (PDT)
Subject: [R-SIG-Finance] general Levy processes and simulation
Message-ID: <772572.23919.qm@web53502.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090902/2dfaf309/attachment.pl>

From weihanliu2002 at yahoo.com  Thu Sep  3 09:11:46 2009
From: weihanliu2002 at yahoo.com (Wei-han Liu)
Date: Thu, 3 Sep 2009 00:11:46 -0700 (PDT)
Subject: [R-SIG-Finance] Fw:  general Levy processes and simulation
Message-ID: <712413.2973.qm@web53504.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090903/0312470a/attachment.pl>

From jatinpatni at gmail.com  Thu Sep  3 12:50:56 2009
From: jatinpatni at gmail.com (jatin patni)
Date: Thu, 3 Sep 2009 16:20:56 +0530
Subject: [R-SIG-Finance] Sorry for my previous posts :(
Message-ID: <fcc589cd0909030350j213e6dd9v193b09cd4ba9b18e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090903/bff73b5e/attachment.pl>

From stefano.iacus at unimi.it  Thu Sep  3 19:26:12 2009
From: stefano.iacus at unimi.it (stefano iacus)
Date: Thu, 03 Sep 2009 19:26:12 +0200
Subject: [R-SIG-Finance] Fw:  general Levy processes and simulation
In-Reply-To: <712413.2973.qm@web53504.mail.re2.yahoo.com>
References: <712413.2973.qm@web53504.mail.re2.yahoo.com>
Message-ID: <762681E6-EB7F-4BE1-937C-60410BE475CA@unimi.it>

Yes, the book only covers 1-dim sde driven by Wiener process. Not what  
Wei-Han Liu is looking for.

I'm now involved in a bigger & new project (yet unreleased) which I  
presented at the (amazing) Rmetrics workshop. To see where we are  
heading you can have a look at the slides

http://www.rmetrics.org/Meielisalp2009/Presentations/Iacus.pdf

stefano


On 03/set/09, at 09:11, Wei-han Liu wrote:

> Hi again:
>
>
> Thanks but I am afraid that the book you mention does not cover the  
> specific topics I am working on.
> Iacus, Stefano M. 2008. Simulation and Inference for Stochastic  
> Differential Equations With R Examples: Springer.
>
> The question again:
> Just curious if there are some resources in R for implementing  
> general Levy processes and simulation, e.g. simulation with  
> truncated Poisson point processes or symmetric stable processes?
>
> W. H.
>
>
> ----- Forwarded Message ----
> From: "markleeds at verizon.net" <markleeds at verizon.net>
> To: weihanliu2002 at yahoo.com
> Sent: Thursday, September 3, 2009 9:58:32 AM
> Subject: Re: [R-SIG-Finance] general Levy processes and simulation
>
> Hi: you should check out stefano iacus's package and book, the names  
> of which escape me. there's probably material in there related to  
> below. the book is titled something like "simulation of a stochastic  
> differential equations using R". It's  a useR book.
>
>
>
>
>
>
>
>
>
> On Sep 2, 2009, Wei-han Liu <weihanliu2002 at yahoo.com> wrote:
> Hi there:
>>
>>
>> Just curious if there are some resources in R for implementing  
>> general Levy processes and simulation, e.g. simulation with  
>> truncated Poisson point processes or symmetric stable processes?
>>
>> Thanks in advance.
>>
>> Wei-han
>>
>>
>>
>> [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From zebedeeniinaate at gmail.com  Sat Sep  5 09:29:13 2009
From: zebedeeniinaate at gmail.com (Zebedee Nii-Naate)
Date: Sat, 5 Sep 2009 08:29:13 +0100
Subject: [R-SIG-Finance] Zelig::zelig
Message-ID: <000301ca2dfa$918de570$b4a9b050$@com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090905/55768f77/attachment.pl>

From pathak.saurav at gmail.com  Tue Sep  8 14:20:27 2009
From: pathak.saurav at gmail.com (saurav pathak)
Date: Tue, 8 Sep 2009 13:20:27 +0100
Subject: [R-SIG-Finance] Inverse Mills in clustered (multilevel)
	cross-sectional panel data
Message-ID: <e8cc3dd00909080520j638226aft5d83ad9eeda79110@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090908/9c462cbf/attachment.pl>

From joshcchien at yahoo.com  Wed Sep  9 14:17:51 2009
From: joshcchien at yahoo.com (Josh C. Chien)
Date: Wed, 9 Sep 2009 05:17:51 -0700 (PDT)
Subject: [R-SIG-Finance] EXCEL & R
Message-ID: <865280.3780.qm@web110105.mail.gq1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090909/7982886b/attachment.pl>

From rechtsteiner at bgki.net  Wed Sep  9 14:23:35 2009
From: rechtsteiner at bgki.net (Josuah Rechtsteiner)
Date: Wed, 9 Sep 2009 14:23:35 +0200
Subject: [R-SIG-Finance] EXCEL & R
In-Reply-To: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
References: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
Message-ID: <BB7EEA86-79CE-4812-AE70-6E5A08AC88C1@bgki.net>

hi,
try RExcel. There's even a book related to this package available at  
amazon.

Am 09.09.2009 um 14:17 schrieb Josh C. Chien:

> Hi R-all,
> In finance, EXCEL dominates over any products. It's a good interface  
> with finance application.
> I like R but, sometimes, I can't use it for daily work.
> Just curious, does any guy know how to integrate EXCEL and R for  
> working financial modeling ?
>
> Thanks a lot.
>
> Josh
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From edd at debian.org  Wed Sep  9 14:24:22 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 9 Sep 2009 12:24:22 +0000
Subject: [R-SIG-Finance] EXCEL & R
In-Reply-To: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
References: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
Message-ID: <20090909122422.GA5632@master.debian.org>

On Wed, Sep 09, 2009 at 05:17:51AM -0700, Josh C. Chien wrote:
> Hi R-all,
> In finance, EXCEL dominates over any products. It's a good interface with finance application.
> I like R but, sometimes, I can't use it for daily work.
> Just curious, does any guy know how to integrate EXCEL and R for working financial modeling ?

http://lmgtfy.com/?q=R+Excel

-- 
Three out of two people have difficulties with fractions.


From brian at braverock.com  Wed Sep  9 14:25:16 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 09 Sep 2009 07:25:16 -0500
Subject: [R-SIG-Finance] EXCEL & R
In-Reply-To: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
References: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
Message-ID: <4AA79EAC.9000404@braverock.com>

Josh C. Chien wrote:
> Hi R-all,
> In finance, EXCEL dominates over any products. It's a good interface with finance application.
> I like R but, sometimes, I can't use it for daily work.
> Just curious, does any guy know how to integrate EXCEL and R for working financial modeling ?
>   
Please at least TRY Google before this mailing list.  Some of us have 
work to do...

  - Brian


From nordicgnome at gmail.com  Wed Sep  9 15:22:26 2009
From: nordicgnome at gmail.com (Jan Vandermeer)
Date: Wed, 9 Sep 2009 09:22:26 -0400
Subject: [R-SIG-Finance] EXCEL & R
In-Reply-To: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
References: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
Message-ID: <e64efa440909090622o535a63aewf1a3ad0cb5d94dae@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090909/cd0848c7/attachment.pl>

From brian at braverock.com  Wed Sep  9 15:36:32 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 09 Sep 2009 08:36:32 -0500
Subject: [R-SIG-Finance] EXCEL & R
In-Reply-To: <e64efa440909090622o535a63aewf1a3ad0cb5d94dae@mail.gmail.com>
References: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
	<e64efa440909090622o535a63aewf1a3ad0cb5d94dae@mail.gmail.com>
Message-ID: <4AA7AF60.1030408@braverock.com>

Jan,

Thanks for the links.  They are both excellent documents that anyone 
starting in R should read.

For me, the biggest issues with Excel for financial data are:

1> it can't handle data with millions of rows (like for example tick data)

2> it has no native understanding, manipulation, or alignment of time 
series ('Sort' doesn't count)

3> I can't do anything more than the most simple of statistics or 
numerical calculations in it

I haven't used a spreadsheet as anything other than an occasional 
display device for several years in my work.

Regards,

   - Brian

Jan Vandermeer wrote:
> Dear Josh and all other R-SIG-Finance members;
>
> This is my first post to the group and I recognize that it is not directly related to R and finance, but as something of an Open Source advocate I am sensitive to comments like "In finance, EXCEL dominates over any products."
> While this may be true for volume of sales, marketing hype and general
> public acceptance, my understanding is that it is not the best product for statistical precision, responsiveness of the company producing it to correcting known errors, or adherence to good graphical standards.
>
> Here are references to two easily found websites which discuss limitations of EXCEL for use in general statistics and more specifically in statistical finance.
>
> Practical Stats - Statistics With Excel <
> http://www.practicalstats.com/xlsstats/excelstats.html>
>
> Burns Statistics - Spreadsheet Addiction <
> http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html>
>
> Again, as a lurker on the group I ask for your forbearance for a reply on a topic only remotely related to R and finance and would like to thank Brian, Dirk, Jeff, Diethelm and many other who take the time to make this such an interesting place to check in on a couple of times a day.
>
> Jan Vandermeer
>
> On Wed, Sep 9, 2009 at 8:17 AM, Josh C. Chien <joshcchien at yahoo.com> wrote:
>
>   
>> Hi R-all,
>> In finance, EXCEL dominates over any products. It's a good interface with
>> finance application.
>> I like R but, sometimes, I can't use it for daily work.
>> Just curious, does any guy know how to integrate EXCEL and R for working
>> financial modeling ?
>>
>> Thanks a lot.
>>
>> Josh
>>     

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From babel at centrum.sk  Wed Sep  9 16:37:10 2009
From: babel at centrum.sk (babel at centrum.sk)
Date: Wed, 09 Sep 2009 16:37:10 +0200
Subject: [R-SIG-Finance] access to real time market data
Message-ID: <200909091637.305@centrum.cz>

Hi guys
Are there a way how to access real-time market data (tick data, 1 minute data) for free in R? I know there are packages like RBloomberg or iBrokers but dont they depends on their commercial platform where you need to pay for account? Can I download real-time data,for example in quantmod, without paying for it? Thanks in advance


From brian at braverock.com  Wed Sep  9 16:42:36 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 09 Sep 2009 09:42:36 -0500
Subject: [R-SIG-Finance] access to real time market data
In-Reply-To: <200909091637.305@centrum.cz>
References: <200909091637.305@centrum.cz>
Message-ID: <4AA7BEDC.1080400@braverock.com>

babel at centrum.sk wrote:
> Hi guys
> Are there a way how to access real-time market data (tick data, 1 minute data) for free in R? I know there are packages like RBloomberg or iBrokers but dont they depends on their commercial platform where you need to pay for account? Can I download real-time data,for example in quantmod, without paying for it? Thanks in advance
>   
No.

The exchanges charge for data.  Thus the various data aggregators also 
charge for data, thus 'RBloomberg'. 

The "cheapest" source of tick data is usually your broker, which will 
typically have some data interface.  Thus 'iBrokers'.

There are several historical data vendors that have been described on 
this list where you can buy historical tick data in many granularities.  
But for real-time intraday data, you'll have to either pay for it or get 
it from your broker.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From cedrick at cedrickjohnson.com  Wed Sep  9 16:45:51 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Wed, 09 Sep 2009 10:45:51 -0400
Subject: [R-SIG-Finance] access to real time market data
In-Reply-To: <200909091637.305@centrum.cz>
References: <200909091637.305@centrum.cz>
Message-ID: <4AA7BF9F.9030509@cedrickjohnson.com>

The one service that was reasonably good for this was Opentick. They're 
no longer around, thus taking down 'opentick' package as well.

RBloomberg, yes, you need a BBG subscription. The base price jumped from 
$1800/seat/mo to $1900 last Dec. Realtime equities data is extra.

IBrokers, yes, you need to have an account... A couple of colleagues I 
know who can't quite go for the 1900/mo bloomy opened a IRA with IB and 
just pay the non-pro mkt data fee per month for US Equities. That may be 
the best way to go.

-c

babel at centrum.sk wrote:
> Hi guys
> Are there a way how to access real-time market data (tick data, 1 minute data) for free in R? I know there are packages like RBloomberg or iBrokers but dont they depends on their commercial platform where you need to pay for account? Can I download real-time data,for example in quantmod, without paying for it? Thanks in advance
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From josh.m.ulrich at gmail.com  Wed Sep  9 16:48:05 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 9 Sep 2009 09:48:05 -0500
Subject: [R-SIG-Finance] access to real time market data
In-Reply-To: <4AA7BEDC.1080400@braverock.com>
References: <200909091637.305@centrum.cz> <4AA7BEDC.1080400@braverock.com>
Message-ID: <8cca69990909090748o110efe88l8cc4593b3ef923bc@mail.gmail.com>

The poor man's intraday data can be scraped from Yahoo via repeated
calls to quantmod's getQuote() function.  Though you get what you pay
for... caveat emptor. ;-)

Best,
Josh
--
http://www.fosstrading.com



On Wed, Sep 9, 2009 at 9:42 AM, Brian G. Peterson <brian at braverock.com> wrote:
> babel at centrum.sk wrote:
>>
>> Hi guys
>> Are there a way how to access real-time market data (tick data, 1 minute
>> data) for free in R? I know there are packages like RBloomberg or iBrokers
>> but dont they depends on their commercial platform where you need to pay for
>> account? Can I download real-time data,for example in quantmod, without
>> paying for it? Thanks in advance
>>
>
> No.
>
> The exchanges charge for data. ?Thus the various data aggregators also
> charge for data, thus 'RBloomberg'.
> The "cheapest" source of tick data is usually your broker, which will
> typically have some data interface. ?Thus 'iBrokers'.
>
> There are several historical data vendors that have been described on this
> list where you can buy historical tick data in many granularities. ?But for
> real-time intraday data, you'll have to either pay for it or get it from
> your broker.
>
> Regards,
>
> ?- Brian
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From josh.m.ulrich at gmail.com  Wed Sep  9 16:51:30 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 9 Sep 2009 09:51:30 -0500
Subject: [R-SIG-Finance] access to real time market data
In-Reply-To: <8cca69990909090748o110efe88l8cc4593b3ef923bc@mail.gmail.com>
References: <200909091637.305@centrum.cz> <4AA7BEDC.1080400@braverock.com> 
	<8cca69990909090748o110efe88l8cc4593b3ef923bc@mail.gmail.com>
Message-ID: <8cca69990909090751m3285c213l6f6cb2ff39d77673@mail.gmail.com>

I should clarify, this data is intra-day but *not* real-time.  It's
delayed 15-20 minutes, depending on the exchange.  Once opentick went
down, I used this method to get some (slightly better than random)
intra-day data for code testing purposes.  I wouldn't recommend using
it for much more than that.

Best,
Josh
--
http://www.fosstrading.com



On Wed, Sep 9, 2009 at 9:48 AM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
> The poor man's intraday data can be scraped from Yahoo via repeated
> calls to quantmod's getQuote() function. ?Though you get what you pay
> for... caveat emptor. ;-)
>
> Best,
> Josh
> --
> http://www.fosstrading.com
>
>
>
> On Wed, Sep 9, 2009 at 9:42 AM, Brian G. Peterson <brian at braverock.com> wrote:
>> babel at centrum.sk wrote:
>>>
>>> Hi guys
>>> Are there a way how to access real-time market data (tick data, 1 minute
>>> data) for free in R? I know there are packages like RBloomberg or iBrokers
>>> but dont they depends on their commercial platform where you need to pay for
>>> account? Can I download real-time data,for example in quantmod, without
>>> paying for it? Thanks in advance
>>>
>>
>> No.
>>
>> The exchanges charge for data. ?Thus the various data aggregators also
>> charge for data, thus 'RBloomberg'.
>> The "cheapest" source of tick data is usually your broker, which will
>> typically have some data interface. ?Thus 'iBrokers'.
>>
>> There are several historical data vendors that have been described on this
>> list where you can buy historical tick data in many granularities. ?But for
>> real-time intraday data, you'll have to either pay for it or get it from
>> your broker.
>>
>> Regards,
>>
>> ?- Brian
>>
>> --
>> Brian G. Peterson
>> http://braverock.com/brian/
>> Ph: 773-459-4973
>> IM: bgpbraverock
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From davidr at rhotrading.com  Wed Sep  9 17:22:47 2009
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 9 Sep 2009 10:22:47 -0500
Subject: [R-SIG-Finance] access to real time market data
In-Reply-To: <200909091637.305@centrum.cz>
References: <200909091637.305@centrum.cz>
Message-ID: <F9F2A641C593D7408925574C05A7BE7703B2A03D@rhopost.rhotrading.com>

NASDAQ provides some (near) real-time data for free, but I'm not sure
how hard it would be to interface.
They also have intraday trade history with price and volume, but it's
only timestamped to the second.

-- David


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
babel at centrum.sk
Sent: Wednesday, September 09, 2009 9:37 AM
To: R-SIG-Finance
Subject: [R-SIG-Finance] access to real time market data

Hi guys
Are there a way how to access real-time market data (tick data, 1 minute
data) for free in R? I know there are packages like RBloomberg or
iBrokers but dont they depends on their commercial platform where you
need to pay for account? Can I download real-time data,for example in
quantmod, without paying for it? Thanks in advance

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


This e-mail and any materials attached hereto, including, without limitation, all content hereof and thereof (collectively, "Rho Content") are confidential and proprietary to Rho Trading Securities, LLC ("Rho") and/or its affiliates, and are protected by intellectual property laws.  Without the prior written consent of Rho, the Rho Content may not (i) be disclosed to any third party or (ii) be reproduced or otherwise used by anyone other than current employees of Rho or its affiliates, on behalf of Rho or its affiliates.

THE RHO CONTENT IS PROVIDED AS IS, WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND.  TO THE MAXIMUM EXTENT PERMISSIBLE UNDER APPLICABLE LAW, RHO HEREBY DISCLAIMS ANY AND ALL WARRANTIES, EXPRESS AND IMPLIED, RELATING TO THE RHO CONTENT, AND NEITHER RHO NOR ANY OF ITS AFFILIATES SHALL IN ANY EVENT BE LIABLE FOR ANY DAMAGES OF ANY NATURE WHATSOEVER, INCLUDING, BUT NOT LIMITED TO, DIRECT, INDIRECT, CONSEQUENTIAL, SPECIAL AND PUNITIVE DAMAGES, LOSS OF PROFITS AND TRADING LOSSES, RESULTING FROM ANY PERSON'S USE OR RELIANCE UPON, OR INABILITY TO USE, ANY RHO CONTENT, EVEN IF RHO IS ADVISED OF THE POSSIBILITY OF SUCH DAMAGES OR IF SUCH DAMAGES WERE FORESEEABLE.


From guygreen at netvigator.com  Wed Sep  9 17:39:01 2009
From: guygreen at netvigator.com (gug)
Date: Wed, 9 Sep 2009 08:39:01 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] access to real time market data
In-Reply-To: <200909091637.305@centrum.cz>
References: <200909091637.305@centrum.cz>
Message-ID: <25367644.post@talk.nabble.com>


I'm not sure if you want real-time data or historical intraday.

If the former, you can get real-time US data from Yahoo by signing up for a
premium account.  Not free, but $10.95/month or $13.95/month, so nothing
like the professional price.  

See  http://billing.finance.yahoo.com/realtime_quotes/signup
http://billing.finance.yahoo.com/realtime_quotes/signup , including a free
trial period.

Guy



troger19 wrote:
> 
> Hi guys
> Are there a way how to access real-time market data (tick data, 1 minute
> data) for free in R? I know there are packages like RBloomberg or iBrokers
> but dont they depends on their commercial platform where you need to pay
> for account? Can I download real-time data,for example in quantmod,
> without paying for it? Thanks in advance
> 
> 

-- 
View this message in context: http://www.nabble.com/access-to-real-time-market-data-tp25366371p25367644.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From gchappi at gmail.com  Thu Sep 10 13:55:48 2009
From: gchappi at gmail.com (Hans-Peter Suter)
Date: Thu, 10 Sep 2009 13:55:48 +0200
Subject: [R-SIG-Finance] EXCEL & R
In-Reply-To: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
References: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
Message-ID: <47fce0650909100455u77ed9633y4de568164862317b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090910/8024578f/attachment.pl>

From ggrothendieck at gmail.com  Thu Sep 10 14:21:32 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 10 Sep 2009 08:21:32 -0400
Subject: [R-SIG-Finance] EXCEL & R
In-Reply-To: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
References: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
Message-ID: <971536df0909100521x6be06ff4jbc8228c3674bfeac@mail.gmail.com>

See:
http://wiki.r-project.org/rwiki/doku.php?id=tips:data-io:ms_windows

On Wed, Sep 9, 2009 at 8:17 AM, Josh C. Chien <joshcchien at yahoo.com> wrote:
> Hi R-all,
> In finance, EXCEL dominates over any products. It's a good interface with finance application.
> I like R but, sometimes, I can't use it for daily work.
> Just curious, does any guy know how to integrate EXCEL and R for working financial modeling ?
>
> Thanks a lot.
>
> Josh
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From matteo at naufraghi.net  Thu Sep 10 19:58:05 2009
From: matteo at naufraghi.net (Matteo Bertini)
Date: Thu, 10 Sep 2009 19:58:05 +0200
Subject: [R-SIG-Finance] fit arima long period alternatives
Message-ID: <4AA93E2D.7010506@naufraghi.net>

I'd like to fit a SARIMA model on a timeseries but the period I'd like to
use is too big (7 day in 15min samples = 672 >> 350 maximum lag allowed) 
for the algorithm used in R.

Some suggested alternatives?

Thanks,
Matteo Bertini


From binabina at bellsouth.net  Thu Sep 10 20:24:46 2009
From: binabina at bellsouth.net (zubin)
Date: Thu, 10 Sep 2009 14:24:46 -0400
Subject: [R-SIG-Finance] Atlanta GA
Message-ID: <4AA9446E.8080702@bellsouth.net>

Hello R Finance,

Anybody in Atlanta interested in getting together to discuss R and 
building out trading systems using R?  Drop me a note if you are 
interested.


From andrenth at gmail.com  Thu Sep 10 22:26:28 2009
From: andrenth at gmail.com (Andre Nathan)
Date: Thu, 10 Sep 2009 17:26:28 -0300
Subject: [R-SIG-Finance] Newbie quantmod periodicity question
Message-ID: <53e651e90909101326h1b69745bs7ea06eecfd8a871a@mail.gmail.com>

Hello

I've just started using quantmod, and I have a question about how to
deal with intraday data. I'm using getSymbols() to get the data like
this:

  getSymbols("GOOG", src = 'yahoo')

I'm trying to plot an hourly barChart (for example, last 5 hours), but
I can't seem to get it to work, and all plots only have information up
to the last day. I believe this is because periodicity(GOOG) is daily,
but I can't find a way to convert this to hourly data. How can I do
that?

Should I get the 15-minute data from yahoo manually instead of using getSymbols?

Thanks,
Andre


From brian at braverock.com  Thu Sep 10 22:31:10 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 10 Sep 2009 15:31:10 -0500
Subject: [R-SIG-Finance] Newbie quantmod periodicity question
In-Reply-To: <53e651e90909101326h1b69745bs7ea06eecfd8a871a@mail.gmail.com>
References: <53e651e90909101326h1b69745bs7ea06eecfd8a871a@mail.gmail.com>
Message-ID: <4AA9620E.3060908@braverock.com>

Reinstall xts from R-Forge

*|install.packages("xts",repos="http://r-forge.r-project.org")|*

There was a bug in periodicity() that has now been fixed in the R-forge 
version of xts.

Cheers,

    - Brian

Andre Nathan wrote:
> Hello
>
> I've just started using quantmod, and I have a question about how to
> deal with intraday data. I'm using getSymbols() to get the data like
> this:
>
>   getSymbols("GOOG", src = 'yahoo')
>
> I'm trying to plot an hourly barChart (for example, last 5 hours), but
> I can't seem to get it to work, and all plots only have information up
> to the last day. I believe this is because periodicity(GOOG) is daily,
> but I can't find a way to convert this to hourly data. How can I do
> that?
>
> Should I get the 15-minute data from yahoo manually instead of using getSymbols?
>
> Thanks,
> Andre
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>   


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From brian at braverock.com  Thu Sep 10 23:24:19 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 10 Sep 2009 16:24:19 -0500
Subject: [R-SIG-Finance] Newbie quantmod periodicity question
In-Reply-To: <4AA9620E.3060908@braverock.com>
References: <53e651e90909101326h1b69745bs7ea06eecfd8a871a@mail.gmail.com>
	<4AA9620E.3060908@braverock.com>
Message-ID: <4AA96E83.50803@braverock.com>

Everything I said below is correct, however, I suspect that you may not 
have considered that there is no way to go from daily data to hourly 
data....

You'll need to get higher-frequency data, see other posts on this list 
about using getQuote to get higher frequency data from Yahoo.

     - Brian

Brian G. Peterson wrote:
> Reinstall xts from R-Forge
>
> install.packages("xts",repos="http://r-forge.r-project.org")
>
> There was a bug in periodicity() that has now been fixed in the 
> R-forge version of xts.
>
> Cheers,
>
>    - Brian
>
> Andre Nathan wrote:
>> Hello
>>
>> I've just started using quantmod, and I have a question about how to
>> deal with intraday data. I'm using getSymbols() to get the data like
>> this:
>>
>>   getSymbols("GOOG", src = 'yahoo')
>>
>> I'm trying to plot an hourly barChart (for example, last 5 hours), but
>> I can't seem to get it to work, and all plots only have information up
>> to the last day. I believe this is because periodicity(GOOG) is daily,
>> but I can't find a way to convert this to hourly data. How can I do
>> that?
>>
>> Should I get the 15-minute data from yahoo manually instead of using 
>> getSymbols?
>>
>> Thanks,
>> Andre
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>   
>
>


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From andrenth at gmail.com  Fri Sep 11 00:13:14 2009
From: andrenth at gmail.com (Andre Nathan)
Date: Thu, 10 Sep 2009 19:13:14 -0300
Subject: [R-SIG-Finance] Newbie quantmod periodicity question
In-Reply-To: <4AA96E83.50803@braverock.com>
References: <53e651e90909101326h1b69745bs7ea06eecfd8a871a@mail.gmail.com>
	<4AA9620E.3060908@braverock.com> <4AA96E83.50803@braverock.com>
Message-ID: <53e651e90909101513s6b66def3gf85fd795fbc32058@mail.gmail.com>

Oh... I thought the data was actually higher frequency and daily was
just a quantmod default...

Thanks Brian.

Best,
Andre

On Thu, Sep 10, 2009 at 6:24 PM, Brian G. Peterson <brian at braverock.com> wrote:
> Everything I said below is correct, however, I suspect that you may not have
> considered that there is no way to go from daily data to hourly data....
>
> You'll need to get higher-frequency data, see other posts on this list about
> using getQuote to get higher frequency data from Yahoo.
>
> ? ?- Brian
>
> Brian G. Peterson wrote:
>>
>> Reinstall xts from R-Forge
>>
>> install.packages("xts",repos="http://r-forge.r-project.org")
>>
>> There was a bug in periodicity() that has now been fixed in the R-forge
>> version of xts.
>>
>> Cheers,
>>
>> ? - Brian
>>
>> Andre Nathan wrote:
>>>
>>> Hello
>>>
>>> I've just started using quantmod, and I have a question about how to
>>> deal with intraday data. I'm using getSymbols() to get the data like
>>> this:
>>>
>>> ?getSymbols("GOOG", src = 'yahoo')
>>>
>>> I'm trying to plot an hourly barChart (for example, last 5 hours), but
>>> I can't seem to get it to work, and all plots only have information up
>>> to the last day. I believe this is because periodicity(GOOG) is daily,
>>> but I can't find a way to convert this to hourly data. How can I do
>>> that?
>>>
>>> Should I get the 15-minute data from yahoo manually instead of using
>>> getSymbols?
>>>
>>> Thanks,
>>> Andre
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
>


From jon.wanxian at gmail.com  Fri Sep 11 13:59:13 2009
From: jon.wanxian at gmail.com (Jonathan Ling)
Date: Fri, 11 Sep 2009 04:59:13 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Newbie Question: Portfolio
 Optimization with MV, LPM and CVaR constraints
Message-ID: <25398920.post@talk.nabble.com>


Hi there,

I'd start of by apologizing if the following has been answered in previous
discussions or materials, so far my searches haven't been resourceful.

I have a project on portfolio optimization to track performances of
portfolios created under different constraints (mean-variance, LPM, VaR and
CVaR). I've used monthly data (for 12 years) for a couple of shares, and I
stumbled on fPortfolio which I've been trying out for the last 2 weeks.

I'm having trouble figuring out a couple of things.

1. Is it possible for fPortfolio to optimize a portfolio constrained by the
VaR?

2. Would it be possible to use the first 10 years of my data to optimize the
portfolios and track performances at 6 months, 1 year and 2 years after
that?

3. I'm having some trouble with the LPM method of optimization. It's
spitting out the error below, but only on monthly data, it works fine with
weekly data.

Here's my codes:

dataset <-read.table("Portfolio1.csv",header=T,sep=",")        
data <- timeSeries(dataset)
data <- returns(data, type = "continuous")

mvspec = portfolioSpec()

lpmspec = portfolioSpec()
setEstimator(lpmspec) <- "lpmEstimator"
setType(lpmspec) <- "QLPM"
lpmspec at model$param$a <- 2
lpmspec at model$param$tau <- "colMeans"

cvarspec = portfolioSpec()
setType(cvarspec) = "CVaR"
setAlpha(cvarspec) = 0.05
setSolver(cvarspec) = "solveRsymphony"

tangencyPortfolio(data, mvspec)

tangencyPortfolio(data, cvarspec)

tangencyPortfolio(data, lpmspec)

which is spitting out the following error:

Execution stopped:
  The maximum ratio portfolio could not be computed.
Possible Reason:
  Your portfolio constraints may be too restrictive.
Status Information:
  status=2 from solver solveRquadprog.
Error: 
  returned from Rmetrics
In addition: There were 15 warnings (use warnings() to see them)

However, it works on weekly data..?

dataset2 <-read.table("weeklydata.csv",header=T,sep=",")
weeklydata <- timeSeries(dataset2)

tangencyPortfolio(weeklydata, lpmspec)


Data
Monthly:  http://www.nabble.com/file/p25398920/Portfolio1.csv Portfolio1.csv 
Weekly:  http://www.nabble.com/file/p25398920/weeklydata.csv weeklydata.csv 


If anyone is able to point me in the right direction, I'd be extremely
grateful! Thanks~
-- 
View this message in context: http://www.nabble.com/Newbie-Question%3A-Portfolio-Optimization-with-MV%2C-LPM-and-CVaR-constraints-tp25398920p25398920.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From brian at braverock.com  Fri Sep 11 14:38:40 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 11 Sep 2009 07:38:40 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Newbie Question: Portfolio
 Optimization with MV, LPM and CVaR constraints
In-Reply-To: <25398920.post@talk.nabble.com>
References: <25398920.post@talk.nabble.com>
Message-ID: <4AAA44D0.5030508@braverock.com>

Jonathan Ling wrote:
> Hi there,
>
> I'd start of by apologizing if the following has been answered in previous discussions or materials, so far my searches haven't been resourceful.
>
> I have a project on portfolio optimization to track performances of
> portfolios created under different constraints (mean-variance, LPM, VaR and CVaR). I've used monthly data (for 12 years) for a couple of shares, and I stumbled on fPortfolio which I've been trying out for the last 2 weeks.
>
> I'm having trouble figuring out a couple of things.
>
> 1. Is it possible for fPortfolio to optimize a portfolio constrained by the VaR?
>
>   
No. 

VaR optimization does not have an analytical solution, as VaR is not a 
coherent measure that can be transformed into a linear programming or 
other similar method.  I've previously used a combination of grid search 
and library("Rdonlp2") to do VaR optimization.  fPortfolio development 
code has some support for these methods, but it isn't complete or 
released, to the best of my knowledge.

With only 2 instruments, you can easily do grid search and solve the VaR 
optimization problem via brute force. (with 200 this would obviously be 
computationally infeasible)
> 2. Would it be possible to use the first 10 years of my data to optimize the
> portfolios and track performances at 6 months, 1 year and 2 years after
> that?
>
>   
Yes, in general, you'd set your weights and then float.

Please be aware that the code in fPortfolio used to assume continuous 
rebalancing to maintain your weights.  You'll need to assess whether 
this is still the case.  If the fPortfolio code still uses continuous 
rebalancing, to allow your weights to "float" after the initial 
allocation, the calculation is a little more complicated.  We have some 
code that we haven't quite finished documenting and testing for release 
yet that does this, and I'd be happy to test it on your data and 
allocation weights and share it with you.

> 3. I'm having some trouble with the LPM method of optimization. It's
> spitting out the error below, but only on monthly data, it works fine with
> weekly data.
>   
I've never done LPM optimization, so perhaps another fPortfolio user or 
Diethelm or Yohan can help you out with that part of your query.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From j_cuisinier at hotmail.com  Fri Sep 11 14:40:05 2009
From: j_cuisinier at hotmail.com (julien cuisinier)
Date: Fri, 11 Sep 2009 14:40:05 +0200
Subject: [R-SIG-Finance] [R-sig-finance] Newbie Question: Portfolio
 Optimization with MV, LPM and CVaR constraints
In-Reply-To: <25398920.post@talk.nabble.com>
References: <25398920.post@talk.nabble.com>
Message-ID: <COL102-W22624862CE751A958DE2D08FE70@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090911/c97f6f4c/attachment.pl>

From binabina at bellsouth.net  Sun Sep 13 16:09:06 2009
From: binabina at bellsouth.net (zubin)
Date: Sun, 13 Sep 2009 10:09:06 -0400
Subject: [R-SIG-Finance] multivariate zoo and looping
Message-ID: <4AACFD02.2060102@bellsouth.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090913/b3dd7b59/attachment.pl>

From ggrothendieck at gmail.com  Sun Sep 13 16:53:49 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 13 Sep 2009 10:53:49 -0400
Subject: [R-SIG-Finance] multivariate zoo and looping
In-Reply-To: <4AACFD02.2060102@bellsouth.net>
References: <4AACFD02.2060102@bellsouth.net>
Message-ID: <971536df0909130753r29cc9f8bic5e28c5b963cdb69@mail.gmail.com>

Please try to provide reproducible code. That means that
one can simply copy it from the post and paste it into their
browser to rerun it and it has no dependencies. See last
line to every message to r-help.

If the question is how to get a zoo object from a data frame
then try this:

# create DF, a test data frame

Lines <- "ticker,time,open,bid
LQD,2009-09-10 10:39:00,104.99,104.96
LQD,2009-09-10 10:39:00,104.99,104.96
LQD,2009-09-10 10:40:00,104.99,104.96
LQD,2009-09-10 10:41:00,104.99,104.96
LQD,2009-09-10 10:42:00,104.99,104.98
LQD,2009-09-10 10:43:00,104.99,104.96"

DF <- read.csv(textConnection(Lines),  header = TRUE, as.is = TRUE)
DF$time <- as.POSIXct(DF$time)

# aggregate duplicate times
DFag <- aggregate(DF[3:4], DF["time"], mean)

#  convert to zoo
library(zoo)
z <- zoo(DFag[-1], DFag$time)


On Sun, Sep 13, 2009 at 10:09 AM, zubin <binabina at bellsouth.net> wrote:
> Hello, I am working on some data input routines and learning zoo.
>
> I have a SQL database with stock price data by ticker, data looks like
> below. Tickers stacked in the db.
>
> LQD
> ? ?ticker ? ? ? ? ? ? ? ?time ? open ? ?bid ? ?ask volume
> currentsystemtime
> 1 ? ? ?LQD 2009-09-10 10:39:00 104.99 104.96 104.99 199253 2009-09-10
> 07:48:28
> 2 ? ? ?LQD 2009-09-10 10:39:00 104.99 104.96 104.99 199446 2009-09-10
> 07:49:04
> 3 ? ? ?LQD 2009-09-10 10:40:00 104.99 104.96 ? ?105 200184 2009-09-10
> 07:50:04
> 4 ? ? ?LQD 2009-09-10 10:41:00 104.99 104.96 104.99 200684 2009-09-10
> 07:51:04
> 5 ? ? ?LQD 2009-09-10 10:42:00 104.99 104.98 104.99 201124 2009-09-10
> 07:52:04
> 6 ? ? ?LQD 2009-09-10 10:43:00 104.99 104.96 104.97 201874 2009-09-10
> 07:53:04
>
> My objective is to create a multivariate ZOO object so i can perform
> modeling tasks. ? I am successful at doing this manually, now automating
> with loops.
> I can do this manually, create a POSIXct from currentsystemtime and
> generate a zoo object.
>
>
> Question 1) ?From this data set above i create a zoo object:
>
> d$time_system <- as.character(LQD$currentsystemtime)
> d$time_system <- as.POSIXct(d$time_system, "%Y-%m-%d %H:%M:%S", tz="")
> dzoo <- zoo(d$ask, d$time_system)
>
> If i wanted to create a zoo object with not only the ask prices but
> multiple columns, I cant figure out how this is done? ?Do i need to
> create a set of zoo objects and merge?
>
>
> Okay moving on now to the looping:
>
>
> Step1) Fetch tickernames only from dataset
>
> tickernames<-dbGetQuery(conn,"SELECT distinct ticker from zstockdata");
>
> works fine get: ?> tickernames
> sample tickers, there are 23
> 1 ? ? ?EEM
> 2 ? ? ?FAS
> 3 ? ? ?GLD
> 4 ? ? ?HYG
> 5 ? ? ?IWM
> 6 ? ? ?LQD
> 7 ? ? QQQQ
>
> ?> tickernames
> its a factor
>
> 'data.frame': ? ?23 obs. of ?1 variable:
> $ ticker: Factor w/ 23 levels "^IRX","^TNX",..: 5 6 7 8 9 10 11 12 13 14
> ...
>
> 2) Now i loop thru each ticker, create a data frame object in R.
>
> for(i in 1:nrow(tickernames))
> {
> ? ? ? ?sqlstring <- paste("select * from zstockdata where TICKER
> =","'",tickernames[i,1],"'",sep="")
> ? ? ? ?tname <- as.character(tickernames[i,1])
> ??A ? ? assign(tname, dbGetQuery(conn,sqlstring))
>
> #perform some conversions and create a zoo object
>
> ??B ? ?tname$ticker <- as.character(tname$ticker) ???????
>
> }
>
> a) Questions from here: ? ??A, Works well, generates a data frame object
> for each query by ticker. ?But this may be inefficient, i was thinking
> of using an array to store each dataframe. ? I tried this:
>
> mvtime[[i]] <-dbGetQuery(conn,sqlstring)
>
> double brackets, when i run this i get object "mvtime" not found
>
> How do i initialize an array to store multiple data frames. ?A data
> frame array object? ?is there such a thing. ?That would be optimal i
> think vs an object for each ticker.
>
>
> b) ??B, ?Now i need to reference each data frame object, do some data
> cleanups, conversion, etc..
>
> (example: one of many conversions: ?convert ticker from factor to a
> character)
>
> tname$ticker <- as.character(tname$ticker)
> Error in tname$ticker : $ operator is invalid for atomic vectors
> ?> tname
> [1] "EEM"
>
> However, this works:
> EEM$ticker <- as.character(EEM$ticker)
>
> So i don't understand R well enough, the tname is not being rendered
> into "EEM"?
>
> The end game is a cleansed multivariate zoo object with all the tickers
> and all the columns of data in one object.
>
> -zubin
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From megh700004 at yahoo.com  Sun Sep 13 21:09:38 2009
From: megh700004 at yahoo.com (megh)
Date: Sun, 13 Sep 2009 12:09:38 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Surface plot of multivariate
	time series
In-Reply-To: <e8e755250906021034h7ae40157pf9b9b4189f58137b@mail.gmail.com>
References: <23827284.post@talk.nabble.com> <23836769.post@talk.nabble.com>
	<dd3243090906021031g65fff62h88c22e0e7ccea851@mail.gmail.com>
	<e8e755250906021034h7ae40157pf9b9b4189f58137b@mail.gmail.com>
Message-ID: <25426141.post@talk.nabble.com>


Hi, I wanted to get a surface plot of following data (please see the
attachment as well)

06-01-2003	106.7332	113.665	116.5693	117.0284	117.1408	118.1717	119.9737
121.4082	121.3087	118.6427
07-01-2003	106.2834	111.4758	113.8203	114.421	114.6729	115.4357	116.7971
118.4125	119.943	121.0723
08-01-2003	108.942	111.7029	113.1008	113.7205	114.1928	115.141	116.7391
118.3573	119.3991	119.3241
09-01-2003	110.4675	113.2026	114.6338	115.3628	116.0853	117.2171	118.7373
120.2737	121.4736	122.0005
10-01-2003	109.4487	112.5962	114.3684	115.3881	116.4354	117.6378	118.8489
120.1786	121.7057	123.4923
14-01-2003	108.6345	111.6306	113.17	113.9849	114.8858	116.099	117.5972
119.3401	121.2892	123.405
15-01-2003	109.3081	111.9351	113.4392	114.4832	115.7795	117.3387	118.9702
120.5164	121.7997	122.6339
16-01-2003	108.8797	111.8276	113.1078	113.7528	114.8229	116.1417	117.5111
118.9683	120.4773	121.9874
17-01-2003	108.8164	110.8753	111.9389	112.6866	113.7571	114.7225	115.7346
117.4464	120.3864	125.1369
20-01-2003	111.0869	111.7453	111.7856	112.0879	113.195	114.1069	115.0907
116.9625	120.339	125.9243
21-01-2003	109.3528	111.118	111.6719	112.2673	113.6269	114.5619	115.5616
117.7047	121.847	129.0104
22-01-2003	109.4875	112.4725	113.4062	113.837	114.8819	116.1567	117.4217
118.5088	119.1914	119.2376
23-01-2003	109.7985	112.5286	113.235	113.7125	115.0282	116.3633	117.5139
118.4218	118.9089	118.7916
24-01-2003	109.5703	112.1044	112.4529	112.8908	114.393	115.489	116.6935
118.848	122.5627	128.5533
27-01-2003	102.9	109.7752	110.6848	111.302	113.4287	114.4952	115.6115
118.0155	122.6145	130.5326
28-01-2003	113.6774	111.2344	111.4797	113.292	115.138	116.0558	117.5501
119.2964	119.9029	118.4434
29-01-2003	113.2998	112.0985	112.754	114.5099	116.3476	117.6464	119.1492
120.7744	122.0346	122.6313
30-01-2003	113.7105	112.5954	113.3105	115.1203	117.0652	118.5727	120.1488
121.7213	122.9854	123.76
31-01-2003	110.3978	111.2982	112.8093	114.6619	116.3874	117.8985	119.8401
121.6789	122.5285	121.7121
03-02-2003	110.8484	113.2196	115.2325	117.0499	118.7443	120.7044	123.3587
125.2696	124.7541	120.4981
04-02-2003	111.206	113.6112	115.4636	117.0565	118.7946	120.8104	122.8482
124.4681	125.2035	124.6097
05-02-2003	115.5131	115.3867	116.7003	118.755	120.6245	122.07	123.5158
125.0743	126.8618	129.0437
06-02-2003	114.1132	115.3722	116.997	118.8211	120.6517	122.3541	123.9498
125.76	128.1162	131.3473
07-02-2003	112.9725	113.7632	115.2758	117.1311	118.794	120.3272	122.0943
123.9581	125.8084	127.5801
10-02-2003	110.7507	112.9335	114.5976	116.0416	117.6323	119.3914	121.2422
123.2691	125.5359	128.0989
12-02-2003	114.9253	116.8515	118.5538	120.1583	121.8057	123.5481	125.4166
127.4646	129.7449	132.3131
13-02-2003	116.2652	118.1099	119.845	121.762	124.2354	126.0825	126.9981
128.43	131.4719	137.256
14-02-2003	121.5586	122.3633	123.9336	125.8792	127.7922	129.0673	129.818
130.9707	133.2704	137.4908
17-02-2003	122.1714	123.4488	125.1908	126.9153	128.1863	129.1394	130.0936
131.4163	133.4777	136.6784
18-02-2003	119.9327	121.3249	123.4461	125.3816	126.4514	127.5428	128.7956
129.7536	130.1897	129.9015
19-02-2003	119.9151	120.9785	123.2676	125.6381	127.1563	128.093	128.6919
129.0927	129.4891	130.0813
20-02-2003	121.8981	123.3143	125.216	127.2039	128.8592	129.8329	130.4769
131.4318	133.2197	136.3925
21-02-2003	126.3064	127.444	129.2906	131.2926	132.9207	133.8624	134.5098
135.4903	137.3237	140.5623
24-02-2003	123.8	126.5513	128.8436	130.6382	131.9878	133.0892	134.1179
135.2494	136.6713	138.5806
25-02-2003	120.8692	123.5053	125.6099	127.2776	128.5475	129.5115	130.7823
132.2426	133.2272	133.2312
26-02-2003	119.3011	120.3103	121.8103	123.544	125.1926	126.4681	127.474
128.3516	129.2104	130.1765
27-02-2003	118.2955	120.5642	122.3673	123.8429	125.1513	126.4479	127.7845
128.9628	129.6794	129.667
28-02-2003	116.5928	119.5929	121.701	123.1976	124.5193	125.9342	126.9174
127.7501	129.0824	131.4297
03-03-2003	119.8868	121.4168	123.1731	124.906	126.236	127.0385	127.8073
128.6333	129.5156	130.5212
04-03-2003	118.764	119.6763	121.4047	123.3589	124.6016	124.9803	125.8121
126.938	127.957	128.6696
05-03-2003	125.1824	124.1091	125.2075	127.309	128.7892	129.2025	130.0101
130.9591	131.6002	131.7049
06-03-2003	124.0596	125.5667	127.6494	129.7132	130.8301	131.0106	131.4698
132.1812	133.0396	134.0884
07-03-2003	128.4414	130.7267	132.7911	134.265	134.5203	133.8361	133.4276
133.5263	134.3459	136.2228
10-03-2003	129.0602	132.5002	134.8355	135.9293	135.4292	134.2905	133.9001
134.1221	134.9415	136.478
11-03-2003	129.8975	133.2684	135.1841	135.8085	135.2414	134.263	133.7314
133.6907	134.272	135.678
12-03-2003	128.7272	130.9118	132.3165	132.9118	132.6072	131.8662	131.3247
131.2672	132.0263	133.9792
13-03-2003	129.6239	131.9765	133.4767	134.0539	133.5456	132.774	132.5958
133.0634	134.3645	136.7611
14-03-2003	132.6888	133.8677	134.7423	135.0664	134.5172	133.7007	133.2976
133.4486	134.3983	136.4424
17-03-2003	129.3289	129.9013	130.6636	131.4042	131.9068	131.9803	131.7663
131.6678	132.0203	133.1536
18-03-2003	127.1546	129.1919	130.3414	130.7845	130.7077	130.023	129.3162
129.7046	132.179	137.8575
19-03-2003	126.3198	127.4783	128.6864	129.3002	128.7598	127.7872	127.2468
127.8534	130.403	135.8299
20-03-2003	126.7134	128.8205	130.503	130.91	129.5355	128.6986	128.9049
129.3515	129.7014	129.6646
24-03-2003	127.8683	128.9257	130.2388	130.6455	129.7943	129.6254	129.931
129.8741	128.9851	126.8371
25-03-2003	129.1	130.2476	130.9012	130.9531	130.6151	130.7049	130.7913
130.1774	128.3661	124.9209
26-03-2003	125.6242	126.9605	127.6759	127.9296	127.8159	127.4472	127.5628
127.9229	127.3789	125.0737
27-03-2003	124.7791	125.8135	126.5636	126.991	126.9025	126.192	126.0366
126.4924	126.641	125.8787
28-03-2003	122.3302	126.0583	127.9815	128.5992	128.3795	127.8916	128.3327
129.1298	128.5795	125.4272
31-03-2003	122.9495	125.0476	126.0806	126.3641	126.2762	126.1326	126.0025
126.059	126.5255	127.6066
01-04-2003	121.6614	124.0167	125.1119	125.3331	125.1521	124.9378	124.741
124.738	125.1534	126.19
02-04-2003	125.0237	123.559	123.4063	123.9853	124.6415	124.8972	124.7938
124.655	124.8553	125.7382
03-04-2003	126.3097	126.0699	125.733	125.4118	125.3025	125.3965	125.3452
125.2865	125.4503	125.9926
04-04-2003	125.7782	127.0048	127.1993	126.7665	126.2338	125.8187	125.2984
124.9427	125.0865	125.9987
07-04-2003	129.3579	128.426	127.4094	126.4248	125.6679	125.0329	124.2881
123.8381	124.0886	125.3858
08-04-2003	126.2659	124.0576	123.1628	122.9543	122.7591	122.1958	121.3761
120.9294	121.4654	123.5695
09-04-2003	126.3295	123.0105	122.019	122.1099	121.8008	120.9466	120.2971
119.9777	120.1519	121.0496
10-04-2003	124.3917	122.337	121.527	121.2929	120.9075	120.0693	119.0389
118.5341	119.225	121.7831
11-04-2003	123.9958	122.8901	122.4071	122.1863	121.8305	121.2028	120.3991
119.6126	119.0208	118.7904
14-04-2003	125.304	122.2488	120.9091	120.3245	119.4888	118.5318	117.8937
117.7785	118.4408	120.169
15-04-2003	124.8533	121.7238	120.7396	120.6309	120.1175	119.1021	118.1599
118.0649	119.5515	123.4267
16-04-2003	123.9053	122.5052	122.1846	121.9726	120.8658	119.9848	119.7841
119.4567	118.5273	116.6094
17-04-2003	123.8844	123.4679	122.8284	122.0502	121.219	120.6142	120.2503
119.8874	119.3611	118.5235
18-04-2003	123.6323	123.7948	122.0972	120.1219	119.2715	118.8316	118.4449
118.3768	118.6977	119.4523
21-04-2003	122.3508	121.7169	120.978	120.2084	119.4818	118.8801	118.5117
118.4959	118.9496	119.9958
22-04-2003	117.9191	119.016	118.9652	118.2582	117.4338	117.0787	116.921
116.3783	115.034	112.5141
23-04-2003	118.5	119.7072	119.6464	118.9248	118.1209	117.6129	117.2297
116.6479	115.6375	113.9854
24-04-2003	118.0479	118.582	117.8957	116.5096	115.0604	114.1895	113.3827
112.5227	112.3112	113.2537
25-04-2003	115.3071	118.5252	118.3827	116.2328	113.9355	113.212	111.8608
109.7278	109.4651	112.9879
28-04-2003	122.1383	116.8937	113.6989	111.7677	110.2333	108.4609	107.1537
106.8094	107.67	110.121
30-04-2003	119.0417	117.4247	115.4031	113.3166	111.4878	110.2125	109.6868
109.5567	109.2572	108.3298
01-05-2003	121.1255	119.4505	117.2919	114.9878	112.8511	111.1767	110.2052
109.7325	109.4179	109.0029
02-05-2003	123.9351	118.9644	114.7846	111.4688	109.2048	107.9599	107.1667
106.7754	106.8961	107.5548
06-05-2003	116.866	113.4291	109.653	106.2401	103.9816	103.0981	103.0826
103.5084	103.96	104.0048
07-05-2003	116.0361	111.157	107.3176	104.5393	102.9419	102.3901	102.433
102.6595	102.6603	102.023
08-05-2003	116.0654	112.1026	108.9119	106.6342	105.4982	105.428	105.9504
106.4038	106.1187	104.4613
09-05-2003	113.4319	113.3651	111.3783	108.8158	107.293	106.8521	106.5365
106.5337	106.9903	107.9479
12-05-2003	116.0398	115.5277	112.5694	109.1824	107.6339	107.2938	106.8246
106.7944	107.5312	109.2402
13-05-2003	120.072	119.0612	115.5547	111.7628	110.0286	109.734	109.5997
109.7888	110.2447	110.8126
14-05-2003	123.7596	121.8108	117.7045	113.6668	111.9524	111.785	111.6971
111.2282	109.7502	106.6283
15-05-2003	126.519	125.5295	120.6593	115.0791	111.7319	110.4199	109.9832
109.5777	108.3265	105.3911
16-05-2003	123.7121	123.7787	118.2689	111.8345	108.6122	107.7476	107.5982
107.446	106.3817	103.5025
19-05-2003	129.5423	126.7081	120.6022	114.6696	111.5721	110.6218	110.6399
110.6262	109.5502	106.4368
20-05-2003	131.5287	128.358	123.7836	119.3856	116.4432	115.1857	114.7981
114.0028	111.7437	107.1061
21-05-2003	132.9794	129.0908	125.4157	122.1759	119.6083	118.0631	116.8621
114.8441	111.1378	105.0621
22-05-2003	134.482	129.4246	125.2607	121.9485	119.3591	116.9676	114.6423
112.4364	110.3236	108.2737
23-05-2003	137.8192	132.5841	127.8065	124.0915	121.5589	118.9617	116.3412
114.215	112.8307	112.4191
26-05-2003	150	136.3944	129.6574	126.0651	123.1792	120.1585	117.5713
116.1498	116.443	119.0616
27-05-2003	140.6262	134.6105	130.005	126.2856	122.7857	119.0079	116.2345
114.9039	114.5984	115.1993
28-05-2003	133.0564	129.166	125.7507	122.5701	119.1193	115.1612	112.5972
111.7337	111.7082	112.0993
29-05-2003	133.3556	131.6351	128.6401	124.931	120.8293	116.8296	114.6457
114.0274	113.6584	112.6745
30-05-2003	136.614	135.7701	132.8454	128.7274	124.0985	119.772	117.3432
116.6145	116.6199	116.775


As Jeff explained I used the same code given at
http://www.quantmod.com/examples/chartSeries3d/
However what I got some funny figure. Can you please guide me how to get
such surface plot here? Is there any direct package? I also used
"tframePlus" package however got error that :

Error : object 'tfSet' not found whilst loading namespace 'tframePlus'
Error: package/namespace load failed for 'tframePlus'


Any solution please? http://www.nabble.com/file/p25426141/dat.xls dat.xls 



Jeff Ryan wrote:
> 
> Shane beat me to it w.r.t the link...
> 
> That aside, I posted this a while back as an example for doing what
> you want.  quantmod will be getting this natively at some point, but
> for now the code can serve as an example:
> 
> http://www.quantmod.com/examples/chartSeries3d/
> 
> HTH
> Jeff
> 
> On Tue, Jun 2, 2009 at 12:31 PM, Shane Conway <shane.conway at gmail.com>
> wrote:
>> It's actually tframePlus:
>> http://cran.r-project.org/web/packages/tframePlus/.
>>
>> On Tue, Jun 2, 2009 at 1:25 PM, megh <megh700004 at yahoo.com> wrote:
>>>
>>> It is saying that, this package is not available :
>>>
>>>> install.packages("tramePlus")
>>> Warning in install.packages("tramePlus") :
>>> ?argument 'lib' is missing: using
>>> 'C:\Users\Arrun's\Documents/R/win-library/2.9'
>>> --- Please select a CRAN mirror for use in this session ---
>>> Warning message:
>>> In getDependencies(pkgs, dependencies, available, lib) :
>>> ?package ?tramePlus? is not available
>>>
>>> Any better idea please?
>>>
>>>
>>>
>>>
>>> megh wrote:
>>>>
>>>> Hi, I have following multivariate time series :
>>>> start = as.Date("01/01/05", format="%m/%d/%y")
>>>> end = as.Date("12/31/05", format="%m/%d/%y")
>>>> dates = seq(start, end, by=1)
>>>>
>>>> dat = matrix(rnorm(length(dates)*4), length(dates))
>>>> dat1 = t(apply(dat, 1, function(x) x+t(c(100, 110, 120, 130))))
>>>>
>>>> library(zoo)
>>>> dat2 = zoo(dat1, dates); head(dat2)
>>>>
>>>> Now I want to get "surface plot" wherein x-axis is time, y-axis
>>>> represents
>>>> 1,2,3,4, and z-axis is for values of "dat2". Is there any R drawing
>>>> device
>>>> to do that?
>>>>
>>>> Regards,
>>>>
>>>
>>> --
>>> View this message in context:
>>> http://www.nabble.com/Surface-plot-of-multivariate-time-series-tp23827284p23836769.html
>>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> 
> 
> -- 
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
> 
> ia: insight algorithmics
> www.insightalgo.com
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Surface-plot-of-multivariate-time-series-tp23827284p25426141.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From aleks.clark at gmail.com  Mon Sep 14 12:53:51 2009
From: aleks.clark at gmail.com (Aleks Clark)
Date: Mon, 14 Sep 2009 05:53:51 -0500
Subject: [R-SIG-Finance] column transposition with xts
Message-ID: <5aebc8960909140353w76d7ef68ya8a994e000f46a26@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090914/e409e3c3/attachment.pl>

From nelson.ana at gmail.com  Mon Sep 14 13:43:33 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Mon, 14 Sep 2009 12:43:33 +0100
Subject: [R-SIG-Finance] EXCEL & R
In-Reply-To: <971536df0909100521x6be06ff4jbc8228c3674bfeac@mail.gmail.com>
References: <865280.3780.qm@web110105.mail.gq1.yahoo.com>
	<971536df0909100521x6be06ff4jbc8228c3674bfeac@mail.gmail.com>
Message-ID: <a7d6d2740909140443j7b4135e5n7b2acd631ccfd5a1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090914/99a0b2b9/attachment.pl>

From megh700004 at yahoo.com  Mon Sep 14 14:11:31 2009
From: megh700004 at yahoo.com (megh)
Date: Mon, 14 Sep 2009 05:11:31 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Multivariate TS plot with RGL
Message-ID: <25434691.post@talk.nabble.com>


Hi,

I have following surface plot of a multivariate time series

library(zoo); library(rgl); library(xts)
mat <- matrix(nrow=250, ncol=10)
for (i in 1:250) mat[i,] <- rnorm(1, 100, 10) + seq(10:19)
Z <- zooreg(mat, start=as.Date("2000-01-01"))

time.axis <- axTicksByTime(Z); col=c("yellow","red")
yred <- colorRampPalette(col); col1 =
rep(rep(yred(NCOL(Z)),each=1),each=(NROW(Z)-1))
Z1 <- coredata(Z)

persp3d(z=Z1,x=(1:NROW(Z1)), y=(1:NCOL(Z1)),aspect=c(1, 1, 0.5), col = col1)


Now I want to show the dates along the x-axis. Can anyone please help me how
to do that? I am specially interested to do it in RGL device.

Thanks
-- 
View this message in context: http://www.nabble.com/Multivariate-TS-plot-with-RGL-tp25434691p25434691.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From markknecht at gmail.com  Mon Sep 14 22:11:40 2009
From: markknecht at gmail.com (Mark Knecht)
Date: Mon, 14 Sep 2009 13:11:40 -0700
Subject: [R-SIG-Finance] (TTR) chaikinVolatility problems
Message-ID: <5bdc1c8b0909141311hcda53fap935ff0a93d02921a@mail.gmail.com>

Hi,
   I'm wondering if there are any know limitations with the TTR
chaikinVolatility function I should be aware of. I've got a small
application I've been writing in R that uses it. I debugged the app
using weekly data for the SPX, DJI, Nasdaq and Russell going back as
far as TradeStation would give me data. Everything worked great.
However when I switched to SPX daily data the indicator fails for the
first 1/3 to 1/2 of the data. (Roughly 12400 days) Looking into the
data output of the indicator I see this:

 [4465]  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
 [4483]  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
 [4501]  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
0.00   Inf   Inf   Inf   Inf   Inf   Inf   Inf
 [4519]   Inf   Inf   Inf   Inf   Inf   Inf  3.83  1.75  0.78  0.46
0.35  0.20  0.15  0.12  0.12  0.12  0.09  0.03
 [4537]  0.01  0.05  0.03  0.02  0.00 -0.04 -0.03 -0.06 -0.07 -0.09
-0.13 -0.12 -0.07 -0.02 -0.02 -0.01 -0.02  0.01
 [4555] -0.01 -0.01 -0.01 -0.01  0.04  0.08  0.11  0.09  0.04  0.06
0.06  0.10  0.08  0.07  0.05  0.07  0.04 -0.03

   After the indicator starts working it continues to work, but it's
all 0.00 back to the beginning except for these few Inf's in the
middle.

   When I broke the code out to try to debug it before I wrote this
email I got somewhat different results with NaN's:

> head(TradeSystem, n=40)
         Date Time  Open  High   Low Close  Vol OI FastAvg SlowAvg
MyDate ChVol
1  01/04/1960 1300 59.91 59.91 59.91 59.91 3988  0       0       0
1960-01-04    NA
2  01/05/1960 1300 60.39 60.39 60.39 60.39 3712  0       0       0
1960-01-05    NA
3  01/06/1960 1300 60.13 60.13 60.13 60.13 3730  0       0       0
1960-01-06    NA
4  01/07/1960 1300 59.69 59.69 59.69 59.69 3311  0       0       0
1960-01-07    NA
5  01/08/1960 1300 59.50 59.50 59.50 59.50 3287  0       0       0
1960-01-08    NA
6  01/11/1960 1300 58.77 58.77 58.77 58.77 3466  0       0       0
1960-01-11    NA
7  01/12/1960 1300 58.41 58.41 58.41 58.41 3764  0       0       0
1960-01-12    NA
8  01/13/1960 1300 58.08 58.08 58.08 58.08 3468  0       0       0
1960-01-13    NA
9  01/14/1960 1300 58.40 58.40 58.40 58.40 3560  0       0       0
1960-01-14    NA
10 01/15/1960 1300 58.38 58.38 58.38 58.38 3422  0       0       0
1960-01-15    NA
11 01/18/1960 1300 57.89 57.89 57.89 57.89 3019  0       0       0
1960-01-18    NA
12 01/19/1960 1300 57.27 57.27 57.27 57.27 3096  0       0       0
1960-01-19    NA
13 01/20/1960 1300 57.07 57.07 57.07 57.07 2717  0       0       0
1960-01-20    NA
14 01/21/1960 1300 57.21 57.21 57.21 57.21 2697  0       0       0
1960-01-21    NA
15 01/22/1960 1300 57.38 57.38 57.38 57.38 2690  0       0       0
1960-01-22    NA
16 01/25/1960 1300 56.78 56.78 56.78 56.78 2793  0       0       0
1960-01-25    NA
17 01/26/1960 1300 56.86 56.86 56.86 56.86 3062  0       0       0
1960-01-26    NA
18 01/27/1960 1300 56.72 56.72 56.72 56.72 2463  0       0       0
1960-01-27    NA
19 01/28/1960 1300 56.13 56.13 56.13 56.13 2627  0       0       0
1960-01-28    NA
20 01/29/1960 1300 55.61 55.61 55.61 55.61 3061  0       0       0
1960-01-29    NA
21 02/01/1960 1300 55.96 55.96 55.96 55.96 2825  0       0       0
1960-02-01    NA
22 02/02/1960 1300 56.82 56.82 56.82 56.82 3077  0       0       0
1960-02-02    NA
23 02/03/1960 1300 56.32 56.32 56.32 56.32 3022  0       0       0
1960-02-03    NA
24 02/04/1960 1300 56.27 56.27 56.27 56.27 2617  0       0       0
1960-02-04    NA
25 02/05/1960 1300 55.98 55.98 55.98 55.98 2535  0       0       0
1960-02-05    NA
26 02/08/1960 1300 55.32 55.32 55.32 55.32 3348  0       0       0
1960-02-08   NaN
27 02/09/1960 1300 55.84 55.84 55.84 55.84 2859  0       0       0
1960-02-09   NaN
28 02/10/1960 1300 55.49 55.49 55.49 55.49 2441  0       0       0
1960-02-10   NaN
29 02/11/1960 1300 55.18 55.18 55.18 55.18 2606  0       0       0
1960-02-11   NaN
30 02/12/1960 1300 55.46 55.46 55.46 55.46 2229  0       0       0
1960-02-12   NaN
31 02/15/1960 1300 55.17 55.17 55.17 55.17 2772  0       0       0
1960-02-15   NaN
32 02/16/1960 1300 54.73 54.73 54.73 54.73 3278  0       0       0
1960-02-16   NaN
33 02/17/1960 1300 55.03 55.03 55.03 55.03 4208  0       0       0
1960-02-17   NaN
34 02/18/1960 1300 55.80 55.80 55.80 55.80 3798  0       0       0
1960-02-18   NaN
35 02/19/1960 1300 56.24 56.24 56.24 56.24 3234  0       0       0
1960-02-19   NaN
36 02/23/1960 1300 55.94 55.94 55.94 55.94 2960  0       0       0
1960-02-23   NaN
37 02/24/1960 1300 55.74 55.74 55.74 55.74 2744  0       0       0
1960-02-24   NaN
38 02/25/1960 1300 55.93 55.93 55.93 55.93 3601  0       0       0
1960-02-25   NaN
39 02/26/1960 1300 56.16 56.16 56.16 56.16 3384  0       0       0
1960-02-26   NaN
40 02/29/1960 1300 56.12 56.12 56.12 56.12 2994  0       0       0
1960-02-29   NaN
>

   The data came from TradeStation. It looks OK to me visually. The
code stub I'm using follows. I could possibly post the data somewhere
if necessary.

Thanks,
Mark


require(chron)
require(ggplot2)
require(quantmod)
require(reshape)
require(TTR)

AbsLookback = 13

TradeSystem = read.csv("C:\\MiningData\\SPX_Daily.txt",header=TRUE)
#TradeSystem = read.csv("C:\\MiningData\\SPX_Weekly.txt",header=TRUE)
#TradeSystem = read.csv("C:\\MiningData\\INDU_Weekly.txt",header=TRUE)
#TradeSystem = read.csv("C:\\MiningData\\NDX_Weekly.txt",header=TRUE)
#TradeSystem = read.csv("C:\\MiningData\\RUT_Weekly.txt",header=TRUE)

TradeSystem$MyDate = as.Date(TradeSystem$Date, "%m/%d/%Y")

CVol = data.frame(chaikinVolatility(TradeSystem[,c("High","Low")],
n=AbsLookback))
names(CVol)[1]="ChVol"

TradeSystem$ChVol = round(CVol$ChVol, 2)


From josh.m.ulrich at gmail.com  Mon Sep 14 22:25:32 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Mon, 14 Sep 2009 15:25:32 -0500
Subject: [R-SIG-Finance] (TTR) chaikinVolatility problems
In-Reply-To: <5bdc1c8b0909141311hcda53fap935ff0a93d02921a@mail.gmail.com>
References: <5bdc1c8b0909141311hcda53fap935ff0a93d02921a@mail.gmail.com>
Message-ID: <8cca69990909141325h7fbfd081wacd79468bb2bfac2@mail.gmail.com>

Hi Mark,

The limitation is in the way the indicator is constructed.  It
measures the rate of change of the trading range.  It's failing
because, at least for the observations you've shown, the trading range
is *zero*.  Please report back if you still have issues if you apply
the indicator over values with non-zero differences.

HTH,
Josh
--
http://www.fosstrading.com



On Mon, Sep 14, 2009 at 3:11 PM, Mark Knecht <markknecht at gmail.com> wrote:
> Hi,
> ? I'm wondering if there are any know limitations with the TTR
> chaikinVolatility function I should be aware of. I've got a small
> application I've been writing in R that uses it. I debugged the app
> using weekly data for the SPX, DJI, Nasdaq and Russell going back as
> far as TradeStation would give me data. Everything worked great.
> However when I switched to SPX daily data the indicator fails for the
> first 1/3 to 1/2 of the data. (Roughly 12400 days) Looking into the
> data output of the indicator I see this:
>
> ?[4465] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
> 0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
> ?[4483] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
> 0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
> ?[4501] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
> 0.00 ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf
> ?[4519] ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf ?3.83 ?1.75 ?0.78 ?0.46
> 0.35 ?0.20 ?0.15 ?0.12 ?0.12 ?0.12 ?0.09 ?0.03
> ?[4537] ?0.01 ?0.05 ?0.03 ?0.02 ?0.00 -0.04 -0.03 -0.06 -0.07 -0.09
> -0.13 -0.12 -0.07 -0.02 -0.02 -0.01 -0.02 ?0.01
> ?[4555] -0.01 -0.01 -0.01 -0.01 ?0.04 ?0.08 ?0.11 ?0.09 ?0.04 ?0.06
> 0.06 ?0.10 ?0.08 ?0.07 ?0.05 ?0.07 ?0.04 -0.03
>
> ? After the indicator starts working it continues to work, but it's
> all 0.00 back to the beginning except for these few Inf's in the
> middle.
>
> ? When I broke the code out to try to debug it before I wrote this
> email I got somewhat different results with NaN's:
>
>> head(TradeSystem, n=40)
> ? ? ? ? Date Time ?Open ?High ? Low Close ?Vol OI FastAvg SlowAvg
> MyDate ChVol
> 1 ?01/04/1960 1300 59.91 59.91 59.91 59.91 3988 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-04 ? ?NA
> 2 ?01/05/1960 1300 60.39 60.39 60.39 60.39 3712 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-05 ? ?NA
> 3 ?01/06/1960 1300 60.13 60.13 60.13 60.13 3730 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-06 ? ?NA
> 4 ?01/07/1960 1300 59.69 59.69 59.69 59.69 3311 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-07 ? ?NA
> 5 ?01/08/1960 1300 59.50 59.50 59.50 59.50 3287 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-08 ? ?NA
> 6 ?01/11/1960 1300 58.77 58.77 58.77 58.77 3466 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-11 ? ?NA
> 7 ?01/12/1960 1300 58.41 58.41 58.41 58.41 3764 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-12 ? ?NA
> 8 ?01/13/1960 1300 58.08 58.08 58.08 58.08 3468 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-13 ? ?NA
> 9 ?01/14/1960 1300 58.40 58.40 58.40 58.40 3560 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-14 ? ?NA
> 10 01/15/1960 1300 58.38 58.38 58.38 58.38 3422 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-15 ? ?NA
> 11 01/18/1960 1300 57.89 57.89 57.89 57.89 3019 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-18 ? ?NA
> 12 01/19/1960 1300 57.27 57.27 57.27 57.27 3096 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-19 ? ?NA
> 13 01/20/1960 1300 57.07 57.07 57.07 57.07 2717 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-20 ? ?NA
> 14 01/21/1960 1300 57.21 57.21 57.21 57.21 2697 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-21 ? ?NA
> 15 01/22/1960 1300 57.38 57.38 57.38 57.38 2690 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-22 ? ?NA
> 16 01/25/1960 1300 56.78 56.78 56.78 56.78 2793 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-25 ? ?NA
> 17 01/26/1960 1300 56.86 56.86 56.86 56.86 3062 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-26 ? ?NA
> 18 01/27/1960 1300 56.72 56.72 56.72 56.72 2463 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-27 ? ?NA
> 19 01/28/1960 1300 56.13 56.13 56.13 56.13 2627 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-28 ? ?NA
> 20 01/29/1960 1300 55.61 55.61 55.61 55.61 3061 ?0 ? ? ? 0 ? ? ? 0
> 1960-01-29 ? ?NA
> 21 02/01/1960 1300 55.96 55.96 55.96 55.96 2825 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-01 ? ?NA
> 22 02/02/1960 1300 56.82 56.82 56.82 56.82 3077 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-02 ? ?NA
> 23 02/03/1960 1300 56.32 56.32 56.32 56.32 3022 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-03 ? ?NA
> 24 02/04/1960 1300 56.27 56.27 56.27 56.27 2617 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-04 ? ?NA
> 25 02/05/1960 1300 55.98 55.98 55.98 55.98 2535 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-05 ? ?NA
> 26 02/08/1960 1300 55.32 55.32 55.32 55.32 3348 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-08 ? NaN
> 27 02/09/1960 1300 55.84 55.84 55.84 55.84 2859 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-09 ? NaN
> 28 02/10/1960 1300 55.49 55.49 55.49 55.49 2441 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-10 ? NaN
> 29 02/11/1960 1300 55.18 55.18 55.18 55.18 2606 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-11 ? NaN
> 30 02/12/1960 1300 55.46 55.46 55.46 55.46 2229 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-12 ? NaN
> 31 02/15/1960 1300 55.17 55.17 55.17 55.17 2772 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-15 ? NaN
> 32 02/16/1960 1300 54.73 54.73 54.73 54.73 3278 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-16 ? NaN
> 33 02/17/1960 1300 55.03 55.03 55.03 55.03 4208 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-17 ? NaN
> 34 02/18/1960 1300 55.80 55.80 55.80 55.80 3798 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-18 ? NaN
> 35 02/19/1960 1300 56.24 56.24 56.24 56.24 3234 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-19 ? NaN
> 36 02/23/1960 1300 55.94 55.94 55.94 55.94 2960 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-23 ? NaN
> 37 02/24/1960 1300 55.74 55.74 55.74 55.74 2744 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-24 ? NaN
> 38 02/25/1960 1300 55.93 55.93 55.93 55.93 3601 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-25 ? NaN
> 39 02/26/1960 1300 56.16 56.16 56.16 56.16 3384 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-26 ? NaN
> 40 02/29/1960 1300 56.12 56.12 56.12 56.12 2994 ?0 ? ? ? 0 ? ? ? 0
> 1960-02-29 ? NaN
>>
>
> ? The data came from TradeStation. It looks OK to me visually. The
> code stub I'm using follows. I could possibly post the data somewhere
> if necessary.
>
> Thanks,
> Mark
>
>
> require(chron)
> require(ggplot2)
> require(quantmod)
> require(reshape)
> require(TTR)
>
> AbsLookback = 13
>
> TradeSystem = read.csv("C:\\MiningData\\SPX_Daily.txt",header=TRUE)
> #TradeSystem = read.csv("C:\\MiningData\\SPX_Weekly.txt",header=TRUE)
> #TradeSystem = read.csv("C:\\MiningData\\INDU_Weekly.txt",header=TRUE)
> #TradeSystem = read.csv("C:\\MiningData\\NDX_Weekly.txt",header=TRUE)
> #TradeSystem = read.csv("C:\\MiningData\\RUT_Weekly.txt",header=TRUE)
>
> TradeSystem$MyDate = as.Date(TradeSystem$Date, "%m/%d/%Y")
>
> CVol = data.frame(chaikinVolatility(TradeSystem[,c("High","Low")],
> n=AbsLookback))
> names(CVol)[1]="ChVol"
>
> TradeSystem$ChVol = round(CVol$ChVol, 2)
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From markknecht at gmail.com  Mon Sep 14 22:44:52 2009
From: markknecht at gmail.com (Mark Knecht)
Date: Mon, 14 Sep 2009 13:44:52 -0700
Subject: [R-SIG-Finance] (TTR) chaikinVolatility problems
In-Reply-To: <8cca69990909141325h7fbfd081wacd79468bb2bfac2@mail.gmail.com>
References: <5bdc1c8b0909141311hcda53fap935ff0a93d02921a@mail.gmail.com>
	<8cca69990909141325h7fbfd081wacd79468bb2bfac2@mail.gmail.com>
Message-ID: <5bdc1c8b0909141344y7c5b1f6fke210cd95cb83e75c@mail.gmail.com>

Josh,
   I don't know why I didn't see that. I'll have to investigate why
the TS weekly data has OHLC values but the Daily does not.

   Thanks for pointing that out and sorry for wasting bandwidth.

Cheers,
Mark

Weekly:

"Date","Time","Open","High","Low","Close","Vol","OI"
01/08/1960,1300,59.91,60.39,59.50,59.50,18028,0
01/15/1960,1300,58.77,58.77,58.08,58.38,17680,0
01/22/1960,1300,57.89,57.89,57.07,57.38,14219,0
01/29/1960,1300,56.78,56.86,55.61,55.61,14006,0
02/05/1960,1300,55.96,56.82,55.96,55.98,14076,0
02/12/1960,1300,55.32,55.84,55.18,55.46,13483,0

Daily for same time period:

"Date","Time","Open","High","Low","Close","Vol","OI","FastAvg","SlowAvg"
01/04/1960,1300,59.91,59.91,59.91,59.91,3988,0,0.00,0.00
01/05/1960,1300,60.39,60.39,60.39,60.39,3712,0,0.00,0.00
01/06/1960,1300,60.13,60.13,60.13,60.13,3730,0,0.00,0.00
01/07/1960,1300,59.69,59.69,59.69,59.69,3311,0,0.00,0.00
01/08/1960,1300,59.50,59.50,59.50,59.50,3287,0,0.00,0.00
01/11/1960,1300,58.77,58.77,58.77,58.77,3466,0,0.00,0.00
01/12/1960,1300,58.41,58.41,58.41,58.41,3764,0,0.00,0.00
01/13/1960,1300,58.08,58.08,58.08,58.08,3468,0,0.00,0.00
01/14/1960,1300,58.40,58.40,58.40,58.40,3560,0,0.00,0.00
01/15/1960,1300,58.38,58.38,58.38,58.38,3422,0,0.00,0.00
01/18/1960,1300,57.89,57.89,57.89,57.89,3019,0,0.00,0.00
01/19/1960,1300,57.27,57.27,57.27,57.27,3096,0,0.00,0.00
01/20/1960,1300,57.07,57.07,57.07,57.07,2717,0,0.00,0.00
01/21/1960,1300,57.21,57.21,57.21,57.21,2697,0,0.00,0.00
01/22/1960,1300,57.38,57.38,57.38,57.38,2690,0,0.00,0.00
01/25/1960,1300,56.78,56.78,56.78,56.78,2793,0,0.00,0.00
01/26/1960,1300,56.86,56.86,56.86,56.86,3062,0,0.00,0.00
01/27/1960,1300,56.72,56.72,56.72,56.72,2463,0,0.00,0.00
01/28/1960,1300,56.13,56.13,56.13,56.13,2627,0,0.00,0.00
01/29/1960,1300,55.61,55.61,55.61,55.61,3061,0,0.00,0.00
02/01/1960,1300,55.96,55.96,55.96,55.96,2825,0,0.00,0.00
02/02/1960,1300,56.82,56.82,56.82,56.82,3077,0,0.00,0.00
02/03/1960,1300,56.32,56.32,56.32,56.32,3022,0,0.00,0.00
02/04/1960,1300,56.27,56.27,56.27,56.27,2617,0,0.00,0.00
02/05/1960,1300,55.98,55.98,55.98,55.98,2535,0,0.00,0.00
02/08/1960,1300,55.32,55.32,55.32,55.32,3348,0,0.00,0.00
02/09/1960,1300,55.84,55.84,55.84,55.84,2859,0,0.00,0.00
02/10/1960,1300,55.49,55.49,55.49,55.49,2441,0,0.00,0.00
02/11/1960,1300,55.18,55.18,55.18,55.18,2606,0,0.00,0.00
02/12/1960,1300,55.46,55.46,55.46,55.46,2229,0,0.00,0.00

On Mon, Sep 14, 2009 at 1:25 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
> Hi Mark,
>
> The limitation is in the way the indicator is constructed. ?It
> measures the rate of change of the trading range. ?It's failing
> because, at least for the observations you've shown, the trading range
> is *zero*. ?Please report back if you still have issues if you apply
> the indicator over values with non-zero differences.
>
> HTH,
> Josh
> --
> http://www.fosstrading.com
>
>
>
> On Mon, Sep 14, 2009 at 3:11 PM, Mark Knecht <markknecht at gmail.com> wrote:
>> Hi,
>> ? I'm wondering if there are any know limitations with the TTR
>> chaikinVolatility function I should be aware of. I've got a small
>> application I've been writing in R that uses it. I debugged the app
>> using weekly data for the SPX, DJI, Nasdaq and Russell going back as
>> far as TradeStation would give me data. Everything worked great.
>> However when I switched to SPX daily data the indicator fails for the
>> first 1/3 to 1/2 of the data. (Roughly 12400 days) Looking into the
>> data output of the indicator I see this:
>>
>> ?[4465] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>> 0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>> ?[4483] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>> 0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>> ?[4501] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>> 0.00 ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf
>> ?[4519] ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf ?3.83 ?1.75 ?0.78 ?0.46
>> 0.35 ?0.20 ?0.15 ?0.12 ?0.12 ?0.12 ?0.09 ?0.03
>> ?[4537] ?0.01 ?0.05 ?0.03 ?0.02 ?0.00 -0.04 -0.03 -0.06 -0.07 -0.09
>> -0.13 -0.12 -0.07 -0.02 -0.02 -0.01 -0.02 ?0.01
>> ?[4555] -0.01 -0.01 -0.01 -0.01 ?0.04 ?0.08 ?0.11 ?0.09 ?0.04 ?0.06
>> 0.06 ?0.10 ?0.08 ?0.07 ?0.05 ?0.07 ?0.04 -0.03
>>
>> ? After the indicator starts working it continues to work, but it's
>> all 0.00 back to the beginning except for these few Inf's in the
>> middle.
>>
>> ? When I broke the code out to try to debug it before I wrote this
>> email I got somewhat different results with NaN's:
>>
>>> head(TradeSystem, n=40)
>> ? ? ? ? Date Time ?Open ?High ? Low Close ?Vol OI FastAvg SlowAvg
>> MyDate ChVol
>> 1 ?01/04/1960 1300 59.91 59.91 59.91 59.91 3988 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-04 ? ?NA
>> 2 ?01/05/1960 1300 60.39 60.39 60.39 60.39 3712 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-05 ? ?NA
>> 3 ?01/06/1960 1300 60.13 60.13 60.13 60.13 3730 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-06 ? ?NA
>> 4 ?01/07/1960 1300 59.69 59.69 59.69 59.69 3311 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-07 ? ?NA
>> 5 ?01/08/1960 1300 59.50 59.50 59.50 59.50 3287 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-08 ? ?NA
>> 6 ?01/11/1960 1300 58.77 58.77 58.77 58.77 3466 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-11 ? ?NA
>> 7 ?01/12/1960 1300 58.41 58.41 58.41 58.41 3764 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-12 ? ?NA
>> 8 ?01/13/1960 1300 58.08 58.08 58.08 58.08 3468 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-13 ? ?NA
>> 9 ?01/14/1960 1300 58.40 58.40 58.40 58.40 3560 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-14 ? ?NA
>> 10 01/15/1960 1300 58.38 58.38 58.38 58.38 3422 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-15 ? ?NA
>> 11 01/18/1960 1300 57.89 57.89 57.89 57.89 3019 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-18 ? ?NA
>> 12 01/19/1960 1300 57.27 57.27 57.27 57.27 3096 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-19 ? ?NA
>> 13 01/20/1960 1300 57.07 57.07 57.07 57.07 2717 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-20 ? ?NA
>> 14 01/21/1960 1300 57.21 57.21 57.21 57.21 2697 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-21 ? ?NA
>> 15 01/22/1960 1300 57.38 57.38 57.38 57.38 2690 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-22 ? ?NA
>> 16 01/25/1960 1300 56.78 56.78 56.78 56.78 2793 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-25 ? ?NA
>> 17 01/26/1960 1300 56.86 56.86 56.86 56.86 3062 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-26 ? ?NA
>> 18 01/27/1960 1300 56.72 56.72 56.72 56.72 2463 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-27 ? ?NA
>> 19 01/28/1960 1300 56.13 56.13 56.13 56.13 2627 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-28 ? ?NA
>> 20 01/29/1960 1300 55.61 55.61 55.61 55.61 3061 ?0 ? ? ? 0 ? ? ? 0
>> 1960-01-29 ? ?NA
>> 21 02/01/1960 1300 55.96 55.96 55.96 55.96 2825 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-01 ? ?NA
>> 22 02/02/1960 1300 56.82 56.82 56.82 56.82 3077 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-02 ? ?NA
>> 23 02/03/1960 1300 56.32 56.32 56.32 56.32 3022 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-03 ? ?NA
>> 24 02/04/1960 1300 56.27 56.27 56.27 56.27 2617 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-04 ? ?NA
>> 25 02/05/1960 1300 55.98 55.98 55.98 55.98 2535 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-05 ? ?NA
>> 26 02/08/1960 1300 55.32 55.32 55.32 55.32 3348 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-08 ? NaN
>> 27 02/09/1960 1300 55.84 55.84 55.84 55.84 2859 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-09 ? NaN
>> 28 02/10/1960 1300 55.49 55.49 55.49 55.49 2441 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-10 ? NaN
>> 29 02/11/1960 1300 55.18 55.18 55.18 55.18 2606 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-11 ? NaN
>> 30 02/12/1960 1300 55.46 55.46 55.46 55.46 2229 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-12 ? NaN
>> 31 02/15/1960 1300 55.17 55.17 55.17 55.17 2772 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-15 ? NaN
>> 32 02/16/1960 1300 54.73 54.73 54.73 54.73 3278 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-16 ? NaN
>> 33 02/17/1960 1300 55.03 55.03 55.03 55.03 4208 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-17 ? NaN
>> 34 02/18/1960 1300 55.80 55.80 55.80 55.80 3798 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-18 ? NaN
>> 35 02/19/1960 1300 56.24 56.24 56.24 56.24 3234 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-19 ? NaN
>> 36 02/23/1960 1300 55.94 55.94 55.94 55.94 2960 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-23 ? NaN
>> 37 02/24/1960 1300 55.74 55.74 55.74 55.74 2744 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-24 ? NaN
>> 38 02/25/1960 1300 55.93 55.93 55.93 55.93 3601 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-25 ? NaN
>> 39 02/26/1960 1300 56.16 56.16 56.16 56.16 3384 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-26 ? NaN
>> 40 02/29/1960 1300 56.12 56.12 56.12 56.12 2994 ?0 ? ? ? 0 ? ? ? 0
>> 1960-02-29 ? NaN
>>>
>>
>> ? The data came from TradeStation. It looks OK to me visually. The
>> code stub I'm using follows. I could possibly post the data somewhere
>> if necessary.
>>
>> Thanks,
>> Mark
>>
>>
>> require(chron)
>> require(ggplot2)
>> require(quantmod)
>> require(reshape)
>> require(TTR)
>>
>> AbsLookback = 13
>>
>> TradeSystem = read.csv("C:\\MiningData\\SPX_Daily.txt",header=TRUE)
>> #TradeSystem = read.csv("C:\\MiningData\\SPX_Weekly.txt",header=TRUE)
>> #TradeSystem = read.csv("C:\\MiningData\\INDU_Weekly.txt",header=TRUE)
>> #TradeSystem = read.csv("C:\\MiningData\\NDX_Weekly.txt",header=TRUE)
>> #TradeSystem = read.csv("C:\\MiningData\\RUT_Weekly.txt",header=TRUE)
>>
>> TradeSystem$MyDate = as.Date(TradeSystem$Date, "%m/%d/%Y")
>>
>> CVol = data.frame(chaikinVolatility(TradeSystem[,c("High","Low")],
>> n=AbsLookback))
>> names(CVol)[1]="ChVol"
>>
>> TradeSystem$ChVol = round(CVol$ChVol, 2)
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From josh.m.ulrich at gmail.com  Mon Sep 14 22:50:52 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Mon, 14 Sep 2009 15:50:52 -0500
Subject: [R-SIG-Finance] (TTR) chaikinVolatility problems
In-Reply-To: <5bdc1c8b0909141344y7c5b1f6fke210cd95cb83e75c@mail.gmail.com>
References: <5bdc1c8b0909141311hcda53fap935ff0a93d02921a@mail.gmail.com> 
	<8cca69990909141325h7fbfd081wacd79468bb2bfac2@mail.gmail.com> 
	<5bdc1c8b0909141344y7c5b1f6fke210cd95cb83e75c@mail.gmail.com>
Message-ID: <8cca69990909141350u46b51726p3abca9674fe7b806@mail.gmail.com>

Mark,

It's because the daily data is used to create the weekly data.  You'll
notice that the weekly high / low is the daily maximum / minimum
within each week.  You can use to.weekly (from xts) to replicate TS's
weekly OHLC data from their daily data.

I've generally found that HL data is sparse before 1980-ish, so don't
look for it too hard (and let me know if you find it). ;-)

Best,
Josh
--
http://www.fosstrading.com



On Mon, Sep 14, 2009 at 3:44 PM, Mark Knecht <markknecht at gmail.com> wrote:
> Josh,
> ? I don't know why I didn't see that. I'll have to investigate why
> the TS weekly data has OHLC values but the Daily does not.
>
> ? Thanks for pointing that out and sorry for wasting bandwidth.
>
> Cheers,
> Mark
>
> Weekly:
>
> "Date","Time","Open","High","Low","Close","Vol","OI"
> 01/08/1960,1300,59.91,60.39,59.50,59.50,18028,0
> 01/15/1960,1300,58.77,58.77,58.08,58.38,17680,0
> 01/22/1960,1300,57.89,57.89,57.07,57.38,14219,0
> 01/29/1960,1300,56.78,56.86,55.61,55.61,14006,0
> 02/05/1960,1300,55.96,56.82,55.96,55.98,14076,0
> 02/12/1960,1300,55.32,55.84,55.18,55.46,13483,0
>
> Daily for same time period:
>
> "Date","Time","Open","High","Low","Close","Vol","OI","FastAvg","SlowAvg"
> 01/04/1960,1300,59.91,59.91,59.91,59.91,3988,0,0.00,0.00
> 01/05/1960,1300,60.39,60.39,60.39,60.39,3712,0,0.00,0.00
> 01/06/1960,1300,60.13,60.13,60.13,60.13,3730,0,0.00,0.00
> 01/07/1960,1300,59.69,59.69,59.69,59.69,3311,0,0.00,0.00
> 01/08/1960,1300,59.50,59.50,59.50,59.50,3287,0,0.00,0.00
> 01/11/1960,1300,58.77,58.77,58.77,58.77,3466,0,0.00,0.00
> 01/12/1960,1300,58.41,58.41,58.41,58.41,3764,0,0.00,0.00
> 01/13/1960,1300,58.08,58.08,58.08,58.08,3468,0,0.00,0.00
> 01/14/1960,1300,58.40,58.40,58.40,58.40,3560,0,0.00,0.00
> 01/15/1960,1300,58.38,58.38,58.38,58.38,3422,0,0.00,0.00
> 01/18/1960,1300,57.89,57.89,57.89,57.89,3019,0,0.00,0.00
> 01/19/1960,1300,57.27,57.27,57.27,57.27,3096,0,0.00,0.00
> 01/20/1960,1300,57.07,57.07,57.07,57.07,2717,0,0.00,0.00
> 01/21/1960,1300,57.21,57.21,57.21,57.21,2697,0,0.00,0.00
> 01/22/1960,1300,57.38,57.38,57.38,57.38,2690,0,0.00,0.00
> 01/25/1960,1300,56.78,56.78,56.78,56.78,2793,0,0.00,0.00
> 01/26/1960,1300,56.86,56.86,56.86,56.86,3062,0,0.00,0.00
> 01/27/1960,1300,56.72,56.72,56.72,56.72,2463,0,0.00,0.00
> 01/28/1960,1300,56.13,56.13,56.13,56.13,2627,0,0.00,0.00
> 01/29/1960,1300,55.61,55.61,55.61,55.61,3061,0,0.00,0.00
> 02/01/1960,1300,55.96,55.96,55.96,55.96,2825,0,0.00,0.00
> 02/02/1960,1300,56.82,56.82,56.82,56.82,3077,0,0.00,0.00
> 02/03/1960,1300,56.32,56.32,56.32,56.32,3022,0,0.00,0.00
> 02/04/1960,1300,56.27,56.27,56.27,56.27,2617,0,0.00,0.00
> 02/05/1960,1300,55.98,55.98,55.98,55.98,2535,0,0.00,0.00
> 02/08/1960,1300,55.32,55.32,55.32,55.32,3348,0,0.00,0.00
> 02/09/1960,1300,55.84,55.84,55.84,55.84,2859,0,0.00,0.00
> 02/10/1960,1300,55.49,55.49,55.49,55.49,2441,0,0.00,0.00
> 02/11/1960,1300,55.18,55.18,55.18,55.18,2606,0,0.00,0.00
> 02/12/1960,1300,55.46,55.46,55.46,55.46,2229,0,0.00,0.00
>
> On Mon, Sep 14, 2009 at 1:25 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>> Hi Mark,
>>
>> The limitation is in the way the indicator is constructed. ?It
>> measures the rate of change of the trading range. ?It's failing
>> because, at least for the observations you've shown, the trading range
>> is *zero*. ?Please report back if you still have issues if you apply
>> the indicator over values with non-zero differences.
>>
>> HTH,
>> Josh
>> --
>> http://www.fosstrading.com
>>
>>
>>
>> On Mon, Sep 14, 2009 at 3:11 PM, Mark Knecht <markknecht at gmail.com> wrote:
>>> Hi,
>>> ? I'm wondering if there are any know limitations with the TTR
>>> chaikinVolatility function I should be aware of. I've got a small
>>> application I've been writing in R that uses it. I debugged the app
>>> using weekly data for the SPX, DJI, Nasdaq and Russell going back as
>>> far as TradeStation would give me data. Everything worked great.
>>> However when I switched to SPX daily data the indicator fails for the
>>> first 1/3 to 1/2 of the data. (Roughly 12400 days) Looking into the
>>> data output of the indicator I see this:
>>>
>>> ?[4465] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>> 0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>> ?[4483] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>> 0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>> ?[4501] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>> 0.00 ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf
>>> ?[4519] ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf ?3.83 ?1.75 ?0.78 ?0.46
>>> 0.35 ?0.20 ?0.15 ?0.12 ?0.12 ?0.12 ?0.09 ?0.03
>>> ?[4537] ?0.01 ?0.05 ?0.03 ?0.02 ?0.00 -0.04 -0.03 -0.06 -0.07 -0.09
>>> -0.13 -0.12 -0.07 -0.02 -0.02 -0.01 -0.02 ?0.01
>>> ?[4555] -0.01 -0.01 -0.01 -0.01 ?0.04 ?0.08 ?0.11 ?0.09 ?0.04 ?0.06
>>> 0.06 ?0.10 ?0.08 ?0.07 ?0.05 ?0.07 ?0.04 -0.03
>>>
>>> ? After the indicator starts working it continues to work, but it's
>>> all 0.00 back to the beginning except for these few Inf's in the
>>> middle.
>>>
>>> ? When I broke the code out to try to debug it before I wrote this
>>> email I got somewhat different results with NaN's:
>>>
>>>> head(TradeSystem, n=40)
>>> ? ? ? ? Date Time ?Open ?High ? Low Close ?Vol OI FastAvg SlowAvg
>>> MyDate ChVol
>>> 1 ?01/04/1960 1300 59.91 59.91 59.91 59.91 3988 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-04 ? ?NA
>>> 2 ?01/05/1960 1300 60.39 60.39 60.39 60.39 3712 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-05 ? ?NA
>>> 3 ?01/06/1960 1300 60.13 60.13 60.13 60.13 3730 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-06 ? ?NA
>>> 4 ?01/07/1960 1300 59.69 59.69 59.69 59.69 3311 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-07 ? ?NA
>>> 5 ?01/08/1960 1300 59.50 59.50 59.50 59.50 3287 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-08 ? ?NA
>>> 6 ?01/11/1960 1300 58.77 58.77 58.77 58.77 3466 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-11 ? ?NA
>>> 7 ?01/12/1960 1300 58.41 58.41 58.41 58.41 3764 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-12 ? ?NA
>>> 8 ?01/13/1960 1300 58.08 58.08 58.08 58.08 3468 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-13 ? ?NA
>>> 9 ?01/14/1960 1300 58.40 58.40 58.40 58.40 3560 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-14 ? ?NA
>>> 10 01/15/1960 1300 58.38 58.38 58.38 58.38 3422 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-15 ? ?NA
>>> 11 01/18/1960 1300 57.89 57.89 57.89 57.89 3019 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-18 ? ?NA
>>> 12 01/19/1960 1300 57.27 57.27 57.27 57.27 3096 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-19 ? ?NA
>>> 13 01/20/1960 1300 57.07 57.07 57.07 57.07 2717 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-20 ? ?NA
>>> 14 01/21/1960 1300 57.21 57.21 57.21 57.21 2697 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-21 ? ?NA
>>> 15 01/22/1960 1300 57.38 57.38 57.38 57.38 2690 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-22 ? ?NA
>>> 16 01/25/1960 1300 56.78 56.78 56.78 56.78 2793 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-25 ? ?NA
>>> 17 01/26/1960 1300 56.86 56.86 56.86 56.86 3062 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-26 ? ?NA
>>> 18 01/27/1960 1300 56.72 56.72 56.72 56.72 2463 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-27 ? ?NA
>>> 19 01/28/1960 1300 56.13 56.13 56.13 56.13 2627 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-28 ? ?NA
>>> 20 01/29/1960 1300 55.61 55.61 55.61 55.61 3061 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-01-29 ? ?NA
>>> 21 02/01/1960 1300 55.96 55.96 55.96 55.96 2825 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-01 ? ?NA
>>> 22 02/02/1960 1300 56.82 56.82 56.82 56.82 3077 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-02 ? ?NA
>>> 23 02/03/1960 1300 56.32 56.32 56.32 56.32 3022 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-03 ? ?NA
>>> 24 02/04/1960 1300 56.27 56.27 56.27 56.27 2617 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-04 ? ?NA
>>> 25 02/05/1960 1300 55.98 55.98 55.98 55.98 2535 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-05 ? ?NA
>>> 26 02/08/1960 1300 55.32 55.32 55.32 55.32 3348 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-08 ? NaN
>>> 27 02/09/1960 1300 55.84 55.84 55.84 55.84 2859 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-09 ? NaN
>>> 28 02/10/1960 1300 55.49 55.49 55.49 55.49 2441 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-10 ? NaN
>>> 29 02/11/1960 1300 55.18 55.18 55.18 55.18 2606 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-11 ? NaN
>>> 30 02/12/1960 1300 55.46 55.46 55.46 55.46 2229 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-12 ? NaN
>>> 31 02/15/1960 1300 55.17 55.17 55.17 55.17 2772 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-15 ? NaN
>>> 32 02/16/1960 1300 54.73 54.73 54.73 54.73 3278 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-16 ? NaN
>>> 33 02/17/1960 1300 55.03 55.03 55.03 55.03 4208 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-17 ? NaN
>>> 34 02/18/1960 1300 55.80 55.80 55.80 55.80 3798 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-18 ? NaN
>>> 35 02/19/1960 1300 56.24 56.24 56.24 56.24 3234 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-19 ? NaN
>>> 36 02/23/1960 1300 55.94 55.94 55.94 55.94 2960 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-23 ? NaN
>>> 37 02/24/1960 1300 55.74 55.74 55.74 55.74 2744 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-24 ? NaN
>>> 38 02/25/1960 1300 55.93 55.93 55.93 55.93 3601 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-25 ? NaN
>>> 39 02/26/1960 1300 56.16 56.16 56.16 56.16 3384 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-26 ? NaN
>>> 40 02/29/1960 1300 56.12 56.12 56.12 56.12 2994 ?0 ? ? ? 0 ? ? ? 0
>>> 1960-02-29 ? NaN
>>>>
>>>
>>> ? The data came from TradeStation. It looks OK to me visually. The
>>> code stub I'm using follows. I could possibly post the data somewhere
>>> if necessary.
>>>
>>> Thanks,
>>> Mark
>>>
>>>
>>> require(chron)
>>> require(ggplot2)
>>> require(quantmod)
>>> require(reshape)
>>> require(TTR)
>>>
>>> AbsLookback = 13
>>>
>>> TradeSystem = read.csv("C:\\MiningData\\SPX_Daily.txt",header=TRUE)
>>> #TradeSystem = read.csv("C:\\MiningData\\SPX_Weekly.txt",header=TRUE)
>>> #TradeSystem = read.csv("C:\\MiningData\\INDU_Weekly.txt",header=TRUE)
>>> #TradeSystem = read.csv("C:\\MiningData\\NDX_Weekly.txt",header=TRUE)
>>> #TradeSystem = read.csv("C:\\MiningData\\RUT_Weekly.txt",header=TRUE)
>>>
>>> TradeSystem$MyDate = as.Date(TradeSystem$Date, "%m/%d/%Y")
>>>
>>> CVol = data.frame(chaikinVolatility(TradeSystem[,c("High","Low")],
>>> n=AbsLookback))
>>> names(CVol)[1]="ChVol"
>>>
>>> TradeSystem$ChVol = round(CVol$ChVol, 2)
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>


From markknecht at gmail.com  Mon Sep 14 22:59:36 2009
From: markknecht at gmail.com (Mark Knecht)
Date: Mon, 14 Sep 2009 13:59:36 -0700
Subject: [R-SIG-Finance] (TTR) chaikinVolatility problems
In-Reply-To: <8cca69990909141350u46b51726p3abca9674fe7b806@mail.gmail.com>
References: <5bdc1c8b0909141311hcda53fap935ff0a93d02921a@mail.gmail.com>
	<8cca69990909141325h7fbfd081wacd79468bb2bfac2@mail.gmail.com>
	<5bdc1c8b0909141344y7c5b1f6fke210cd95cb83e75c@mail.gmail.com>
	<8cca69990909141350u46b51726p3abca9674fe7b806@mail.gmail.com>
Message-ID: <5bdc1c8b0909141359p249a6168p161476e610593e21@mail.gmail.com>

That's great input Josh. Thanks.

I'm now wondering if there's a simple function to call that will tell
me the date range that is valid to use for a function's that require
high and low data? I suppose I could construct something that look for
Hi-Lo==0, but maybe it's already in one of these packages somewhere?

Thanks,
Mark

On Mon, Sep 14, 2009 at 1:50 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
> Mark,
>
> It's because the daily data is used to create the weekly data. ?You'll
> notice that the weekly high / low is the daily maximum / minimum
> within each week. ?You can use to.weekly (from xts) to replicate TS's
> weekly OHLC data from their daily data.
>
> I've generally found that HL data is sparse before 1980-ish, so don't
> look for it too hard (and let me know if you find it). ;-)
>
> Best,
> Josh
> --
> http://www.fosstrading.com
>
>
>
> On Mon, Sep 14, 2009 at 3:44 PM, Mark Knecht <markknecht at gmail.com> wrote:
>> Josh,
>> ? I don't know why I didn't see that. I'll have to investigate why
>> the TS weekly data has OHLC values but the Daily does not.
>>
>> ? Thanks for pointing that out and sorry for wasting bandwidth.
>>
>> Cheers,
>> Mark
>>
>> Weekly:
>>
>> "Date","Time","Open","High","Low","Close","Vol","OI"
>> 01/08/1960,1300,59.91,60.39,59.50,59.50,18028,0
>> 01/15/1960,1300,58.77,58.77,58.08,58.38,17680,0
>> 01/22/1960,1300,57.89,57.89,57.07,57.38,14219,0
>> 01/29/1960,1300,56.78,56.86,55.61,55.61,14006,0
>> 02/05/1960,1300,55.96,56.82,55.96,55.98,14076,0
>> 02/12/1960,1300,55.32,55.84,55.18,55.46,13483,0
>>
>> Daily for same time period:
>>
>> "Date","Time","Open","High","Low","Close","Vol","OI","FastAvg","SlowAvg"
>> 01/04/1960,1300,59.91,59.91,59.91,59.91,3988,0,0.00,0.00
>> 01/05/1960,1300,60.39,60.39,60.39,60.39,3712,0,0.00,0.00
>> 01/06/1960,1300,60.13,60.13,60.13,60.13,3730,0,0.00,0.00
>> 01/07/1960,1300,59.69,59.69,59.69,59.69,3311,0,0.00,0.00
>> 01/08/1960,1300,59.50,59.50,59.50,59.50,3287,0,0.00,0.00
>> 01/11/1960,1300,58.77,58.77,58.77,58.77,3466,0,0.00,0.00
>> 01/12/1960,1300,58.41,58.41,58.41,58.41,3764,0,0.00,0.00
>> 01/13/1960,1300,58.08,58.08,58.08,58.08,3468,0,0.00,0.00
>> 01/14/1960,1300,58.40,58.40,58.40,58.40,3560,0,0.00,0.00
>> 01/15/1960,1300,58.38,58.38,58.38,58.38,3422,0,0.00,0.00
>> 01/18/1960,1300,57.89,57.89,57.89,57.89,3019,0,0.00,0.00
>> 01/19/1960,1300,57.27,57.27,57.27,57.27,3096,0,0.00,0.00
>> 01/20/1960,1300,57.07,57.07,57.07,57.07,2717,0,0.00,0.00
>> 01/21/1960,1300,57.21,57.21,57.21,57.21,2697,0,0.00,0.00
>> 01/22/1960,1300,57.38,57.38,57.38,57.38,2690,0,0.00,0.00
>> 01/25/1960,1300,56.78,56.78,56.78,56.78,2793,0,0.00,0.00
>> 01/26/1960,1300,56.86,56.86,56.86,56.86,3062,0,0.00,0.00
>> 01/27/1960,1300,56.72,56.72,56.72,56.72,2463,0,0.00,0.00
>> 01/28/1960,1300,56.13,56.13,56.13,56.13,2627,0,0.00,0.00
>> 01/29/1960,1300,55.61,55.61,55.61,55.61,3061,0,0.00,0.00
>> 02/01/1960,1300,55.96,55.96,55.96,55.96,2825,0,0.00,0.00
>> 02/02/1960,1300,56.82,56.82,56.82,56.82,3077,0,0.00,0.00
>> 02/03/1960,1300,56.32,56.32,56.32,56.32,3022,0,0.00,0.00
>> 02/04/1960,1300,56.27,56.27,56.27,56.27,2617,0,0.00,0.00
>> 02/05/1960,1300,55.98,55.98,55.98,55.98,2535,0,0.00,0.00
>> 02/08/1960,1300,55.32,55.32,55.32,55.32,3348,0,0.00,0.00
>> 02/09/1960,1300,55.84,55.84,55.84,55.84,2859,0,0.00,0.00
>> 02/10/1960,1300,55.49,55.49,55.49,55.49,2441,0,0.00,0.00
>> 02/11/1960,1300,55.18,55.18,55.18,55.18,2606,0,0.00,0.00
>> 02/12/1960,1300,55.46,55.46,55.46,55.46,2229,0,0.00,0.00
>>
>> On Mon, Sep 14, 2009 at 1:25 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>>> Hi Mark,
>>>
>>> The limitation is in the way the indicator is constructed. ?It
>>> measures the rate of change of the trading range. ?It's failing
>>> because, at least for the observations you've shown, the trading range
>>> is *zero*. ?Please report back if you still have issues if you apply
>>> the indicator over values with non-zero differences.
>>>
>>> HTH,
>>> Josh
>>> --
>>> http://www.fosstrading.com
>>>
>>>
>>>
>>> On Mon, Sep 14, 2009 at 3:11 PM, Mark Knecht <markknecht at gmail.com> wrote:
>>>> Hi,
>>>> ? I'm wondering if there are any know limitations with the TTR
>>>> chaikinVolatility function I should be aware of. I've got a small
>>>> application I've been writing in R that uses it. I debugged the app
>>>> using weekly data for the SPX, DJI, Nasdaq and Russell going back as
>>>> far as TradeStation would give me data. Everything worked great.
>>>> However when I switched to SPX daily data the indicator fails for the
>>>> first 1/3 to 1/2 of the data. (Roughly 12400 days) Looking into the
>>>> data output of the indicator I see this:
>>>>
>>>> ?[4465] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>>> 0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>>> ?[4483] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>>> 0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>>> ?[4501] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>>> 0.00 ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf
>>>> ?[4519] ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf ?3.83 ?1.75 ?0.78 ?0.46
>>>> 0.35 ?0.20 ?0.15 ?0.12 ?0.12 ?0.12 ?0.09 ?0.03
>>>> ?[4537] ?0.01 ?0.05 ?0.03 ?0.02 ?0.00 -0.04 -0.03 -0.06 -0.07 -0.09
>>>> -0.13 -0.12 -0.07 -0.02 -0.02 -0.01 -0.02 ?0.01
>>>> ?[4555] -0.01 -0.01 -0.01 -0.01 ?0.04 ?0.08 ?0.11 ?0.09 ?0.04 ?0.06
>>>> 0.06 ?0.10 ?0.08 ?0.07 ?0.05 ?0.07 ?0.04 -0.03
>>>>
>>>> ? After the indicator starts working it continues to work, but it's
>>>> all 0.00 back to the beginning except for these few Inf's in the
>>>> middle.
>>>>
>>>> ? When I broke the code out to try to debug it before I wrote this
>>>> email I got somewhat different results with NaN's:
>>>>
>>>>> head(TradeSystem, n=40)
>>>> ? ? ? ? Date Time ?Open ?High ? Low Close ?Vol OI FastAvg SlowAvg
>>>> MyDate ChVol
>>>> 1 ?01/04/1960 1300 59.91 59.91 59.91 59.91 3988 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-04 ? ?NA
>>>> 2 ?01/05/1960 1300 60.39 60.39 60.39 60.39 3712 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-05 ? ?NA
>>>> 3 ?01/06/1960 1300 60.13 60.13 60.13 60.13 3730 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-06 ? ?NA
>>>> 4 ?01/07/1960 1300 59.69 59.69 59.69 59.69 3311 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-07 ? ?NA
>>>> 5 ?01/08/1960 1300 59.50 59.50 59.50 59.50 3287 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-08 ? ?NA
>>>> 6 ?01/11/1960 1300 58.77 58.77 58.77 58.77 3466 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-11 ? ?NA
>>>> 7 ?01/12/1960 1300 58.41 58.41 58.41 58.41 3764 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-12 ? ?NA
>>>> 8 ?01/13/1960 1300 58.08 58.08 58.08 58.08 3468 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-13 ? ?NA
>>>> 9 ?01/14/1960 1300 58.40 58.40 58.40 58.40 3560 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-14 ? ?NA
>>>> 10 01/15/1960 1300 58.38 58.38 58.38 58.38 3422 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-15 ? ?NA
>>>> 11 01/18/1960 1300 57.89 57.89 57.89 57.89 3019 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-18 ? ?NA
>>>> 12 01/19/1960 1300 57.27 57.27 57.27 57.27 3096 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-19 ? ?NA
>>>> 13 01/20/1960 1300 57.07 57.07 57.07 57.07 2717 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-20 ? ?NA
>>>> 14 01/21/1960 1300 57.21 57.21 57.21 57.21 2697 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-21 ? ?NA
>>>> 15 01/22/1960 1300 57.38 57.38 57.38 57.38 2690 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-22 ? ?NA
>>>> 16 01/25/1960 1300 56.78 56.78 56.78 56.78 2793 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-25 ? ?NA
>>>> 17 01/26/1960 1300 56.86 56.86 56.86 56.86 3062 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-26 ? ?NA
>>>> 18 01/27/1960 1300 56.72 56.72 56.72 56.72 2463 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-27 ? ?NA
>>>> 19 01/28/1960 1300 56.13 56.13 56.13 56.13 2627 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-28 ? ?NA
>>>> 20 01/29/1960 1300 55.61 55.61 55.61 55.61 3061 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-01-29 ? ?NA
>>>> 21 02/01/1960 1300 55.96 55.96 55.96 55.96 2825 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-01 ? ?NA
>>>> 22 02/02/1960 1300 56.82 56.82 56.82 56.82 3077 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-02 ? ?NA
>>>> 23 02/03/1960 1300 56.32 56.32 56.32 56.32 3022 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-03 ? ?NA
>>>> 24 02/04/1960 1300 56.27 56.27 56.27 56.27 2617 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-04 ? ?NA
>>>> 25 02/05/1960 1300 55.98 55.98 55.98 55.98 2535 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-05 ? ?NA
>>>> 26 02/08/1960 1300 55.32 55.32 55.32 55.32 3348 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-08 ? NaN
>>>> 27 02/09/1960 1300 55.84 55.84 55.84 55.84 2859 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-09 ? NaN
>>>> 28 02/10/1960 1300 55.49 55.49 55.49 55.49 2441 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-10 ? NaN
>>>> 29 02/11/1960 1300 55.18 55.18 55.18 55.18 2606 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-11 ? NaN
>>>> 30 02/12/1960 1300 55.46 55.46 55.46 55.46 2229 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-12 ? NaN
>>>> 31 02/15/1960 1300 55.17 55.17 55.17 55.17 2772 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-15 ? NaN
>>>> 32 02/16/1960 1300 54.73 54.73 54.73 54.73 3278 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-16 ? NaN
>>>> 33 02/17/1960 1300 55.03 55.03 55.03 55.03 4208 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-17 ? NaN
>>>> 34 02/18/1960 1300 55.80 55.80 55.80 55.80 3798 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-18 ? NaN
>>>> 35 02/19/1960 1300 56.24 56.24 56.24 56.24 3234 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-19 ? NaN
>>>> 36 02/23/1960 1300 55.94 55.94 55.94 55.94 2960 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-23 ? NaN
>>>> 37 02/24/1960 1300 55.74 55.74 55.74 55.74 2744 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-24 ? NaN
>>>> 38 02/25/1960 1300 55.93 55.93 55.93 55.93 3601 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-25 ? NaN
>>>> 39 02/26/1960 1300 56.16 56.16 56.16 56.16 3384 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-26 ? NaN
>>>> 40 02/29/1960 1300 56.12 56.12 56.12 56.12 2994 ?0 ? ? ? 0 ? ? ? 0
>>>> 1960-02-29 ? NaN
>>>>>
>>>>
>>>> ? The data came from TradeStation. It looks OK to me visually. The
>>>> code stub I'm using follows. I could possibly post the data somewhere
>>>> if necessary.
>>>>
>>>> Thanks,
>>>> Mark
>>>>
>>>>
>>>> require(chron)
>>>> require(ggplot2)
>>>> require(quantmod)
>>>> require(reshape)
>>>> require(TTR)
>>>>
>>>> AbsLookback = 13
>>>>
>>>> TradeSystem = read.csv("C:\\MiningData\\SPX_Daily.txt",header=TRUE)
>>>> #TradeSystem = read.csv("C:\\MiningData\\SPX_Weekly.txt",header=TRUE)
>>>> #TradeSystem = read.csv("C:\\MiningData\\INDU_Weekly.txt",header=TRUE)
>>>> #TradeSystem = read.csv("C:\\MiningData\\NDX_Weekly.txt",header=TRUE)
>>>> #TradeSystem = read.csv("C:\\MiningData\\RUT_Weekly.txt",header=TRUE)
>>>>
>>>> TradeSystem$MyDate = as.Date(TradeSystem$Date, "%m/%d/%Y")
>>>>
>>>> CVol = data.frame(chaikinVolatility(TradeSystem[,c("High","Low")],
>>>> n=AbsLookback))
>>>> names(CVol)[1]="ChVol"
>>>>
>>>> TradeSystem$ChVol = round(CVol$ChVol, 2)
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>
>>
>


From josh.m.ulrich at gmail.com  Mon Sep 14 23:13:05 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Mon, 14 Sep 2009 16:13:05 -0500
Subject: [R-SIG-Finance] (TTR) chaikinVolatility problems
In-Reply-To: <5bdc1c8b0909141359p249a6168p161476e610593e21@mail.gmail.com>
References: <5bdc1c8b0909141311hcda53fap935ff0a93d02921a@mail.gmail.com> 
	<8cca69990909141325h7fbfd081wacd79468bb2bfac2@mail.gmail.com> 
	<5bdc1c8b0909141344y7c5b1f6fke210cd95cb83e75c@mail.gmail.com> 
	<8cca69990909141350u46b51726p3abca9674fe7b806@mail.gmail.com> 
	<5bdc1c8b0909141359p249a6168p161476e610593e21@mail.gmail.com>
Message-ID: <8cca69990909141413l6d8cdfa9m81c2e9aa1fcdd83a@mail.gmail.com>

I don't know of such a function, but something like this should work for you:

> hldiff <- Hi(GSPC)-Lo(GSPC)
> hldiff[hldiff>0][1]
           GSPC.High
1962-01-02      1.25

Best,
Josh
--
http://www.fosstrading.com



On Mon, Sep 14, 2009 at 3:59 PM, Mark Knecht <markknecht at gmail.com> wrote:
> That's great input Josh. Thanks.
>
> I'm now wondering if there's a simple function to call that will tell
> me the date range that is valid to use for a function's that require
> high and low data? I suppose I could construct something that look for
> Hi-Lo==0, but maybe it's already in one of these packages somewhere?
>
> Thanks,
> Mark
>
> On Mon, Sep 14, 2009 at 1:50 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>> Mark,
>>
>> It's because the daily data is used to create the weekly data. ?You'll
>> notice that the weekly high / low is the daily maximum / minimum
>> within each week. ?You can use to.weekly (from xts) to replicate TS's
>> weekly OHLC data from their daily data.
>>
>> I've generally found that HL data is sparse before 1980-ish, so don't
>> look for it too hard (and let me know if you find it). ;-)
>>
>> Best,
>> Josh
>> --
>> http://www.fosstrading.com
>>
>>
>>
>> On Mon, Sep 14, 2009 at 3:44 PM, Mark Knecht <markknecht at gmail.com> wrote:
>>> Josh,
>>> ? I don't know why I didn't see that. I'll have to investigate why
>>> the TS weekly data has OHLC values but the Daily does not.
>>>
>>> ? Thanks for pointing that out and sorry for wasting bandwidth.
>>>
>>> Cheers,
>>> Mark
>>>
>>> Weekly:
>>>
>>> "Date","Time","Open","High","Low","Close","Vol","OI"
>>> 01/08/1960,1300,59.91,60.39,59.50,59.50,18028,0
>>> 01/15/1960,1300,58.77,58.77,58.08,58.38,17680,0
>>> 01/22/1960,1300,57.89,57.89,57.07,57.38,14219,0
>>> 01/29/1960,1300,56.78,56.86,55.61,55.61,14006,0
>>> 02/05/1960,1300,55.96,56.82,55.96,55.98,14076,0
>>> 02/12/1960,1300,55.32,55.84,55.18,55.46,13483,0
>>>
>>> Daily for same time period:
>>>
>>> "Date","Time","Open","High","Low","Close","Vol","OI","FastAvg","SlowAvg"
>>> 01/04/1960,1300,59.91,59.91,59.91,59.91,3988,0,0.00,0.00
>>> 01/05/1960,1300,60.39,60.39,60.39,60.39,3712,0,0.00,0.00
>>> 01/06/1960,1300,60.13,60.13,60.13,60.13,3730,0,0.00,0.00
>>> 01/07/1960,1300,59.69,59.69,59.69,59.69,3311,0,0.00,0.00
>>> 01/08/1960,1300,59.50,59.50,59.50,59.50,3287,0,0.00,0.00
>>> 01/11/1960,1300,58.77,58.77,58.77,58.77,3466,0,0.00,0.00
>>> 01/12/1960,1300,58.41,58.41,58.41,58.41,3764,0,0.00,0.00
>>> 01/13/1960,1300,58.08,58.08,58.08,58.08,3468,0,0.00,0.00
>>> 01/14/1960,1300,58.40,58.40,58.40,58.40,3560,0,0.00,0.00
>>> 01/15/1960,1300,58.38,58.38,58.38,58.38,3422,0,0.00,0.00
>>> 01/18/1960,1300,57.89,57.89,57.89,57.89,3019,0,0.00,0.00
>>> 01/19/1960,1300,57.27,57.27,57.27,57.27,3096,0,0.00,0.00
>>> 01/20/1960,1300,57.07,57.07,57.07,57.07,2717,0,0.00,0.00
>>> 01/21/1960,1300,57.21,57.21,57.21,57.21,2697,0,0.00,0.00
>>> 01/22/1960,1300,57.38,57.38,57.38,57.38,2690,0,0.00,0.00
>>> 01/25/1960,1300,56.78,56.78,56.78,56.78,2793,0,0.00,0.00
>>> 01/26/1960,1300,56.86,56.86,56.86,56.86,3062,0,0.00,0.00
>>> 01/27/1960,1300,56.72,56.72,56.72,56.72,2463,0,0.00,0.00
>>> 01/28/1960,1300,56.13,56.13,56.13,56.13,2627,0,0.00,0.00
>>> 01/29/1960,1300,55.61,55.61,55.61,55.61,3061,0,0.00,0.00
>>> 02/01/1960,1300,55.96,55.96,55.96,55.96,2825,0,0.00,0.00
>>> 02/02/1960,1300,56.82,56.82,56.82,56.82,3077,0,0.00,0.00
>>> 02/03/1960,1300,56.32,56.32,56.32,56.32,3022,0,0.00,0.00
>>> 02/04/1960,1300,56.27,56.27,56.27,56.27,2617,0,0.00,0.00
>>> 02/05/1960,1300,55.98,55.98,55.98,55.98,2535,0,0.00,0.00
>>> 02/08/1960,1300,55.32,55.32,55.32,55.32,3348,0,0.00,0.00
>>> 02/09/1960,1300,55.84,55.84,55.84,55.84,2859,0,0.00,0.00
>>> 02/10/1960,1300,55.49,55.49,55.49,55.49,2441,0,0.00,0.00
>>> 02/11/1960,1300,55.18,55.18,55.18,55.18,2606,0,0.00,0.00
>>> 02/12/1960,1300,55.46,55.46,55.46,55.46,2229,0,0.00,0.00
>>>
>>> On Mon, Sep 14, 2009 at 1:25 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>>>> Hi Mark,
>>>>
>>>> The limitation is in the way the indicator is constructed. ?It
>>>> measures the rate of change of the trading range. ?It's failing
>>>> because, at least for the observations you've shown, the trading range
>>>> is *zero*. ?Please report back if you still have issues if you apply
>>>> the indicator over values with non-zero differences.
>>>>
>>>> HTH,
>>>> Josh
>>>> --
>>>> http://www.fosstrading.com
>>>>
>>>>
>>>>
>>>> On Mon, Sep 14, 2009 at 3:11 PM, Mark Knecht <markknecht at gmail.com> wrote:
>>>>> Hi,
>>>>> ? I'm wondering if there are any know limitations with the TTR
>>>>> chaikinVolatility function I should be aware of. I've got a small
>>>>> application I've been writing in R that uses it. I debugged the app
>>>>> using weekly data for the SPX, DJI, Nasdaq and Russell going back as
>>>>> far as TradeStation would give me data. Everything worked great.
>>>>> However when I switched to SPX daily data the indicator fails for the
>>>>> first 1/3 to 1/2 of the data. (Roughly 12400 days) Looking into the
>>>>> data output of the indicator I see this:
>>>>>
>>>>> ?[4465] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>>>> 0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>>>> ?[4483] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>>>> 0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>>>> ?[4501] ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00 ?0.00
>>>>> 0.00 ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf
>>>>> ?[4519] ? Inf ? Inf ? Inf ? Inf ? Inf ? Inf ?3.83 ?1.75 ?0.78 ?0.46
>>>>> 0.35 ?0.20 ?0.15 ?0.12 ?0.12 ?0.12 ?0.09 ?0.03
>>>>> ?[4537] ?0.01 ?0.05 ?0.03 ?0.02 ?0.00 -0.04 -0.03 -0.06 -0.07 -0.09
>>>>> -0.13 -0.12 -0.07 -0.02 -0.02 -0.01 -0.02 ?0.01
>>>>> ?[4555] -0.01 -0.01 -0.01 -0.01 ?0.04 ?0.08 ?0.11 ?0.09 ?0.04 ?0.06
>>>>> 0.06 ?0.10 ?0.08 ?0.07 ?0.05 ?0.07 ?0.04 -0.03
>>>>>
>>>>> ? After the indicator starts working it continues to work, but it's
>>>>> all 0.00 back to the beginning except for these few Inf's in the
>>>>> middle.
>>>>>
>>>>> ? When I broke the code out to try to debug it before I wrote this
>>>>> email I got somewhat different results with NaN's:
>>>>>
>>>>>> head(TradeSystem, n=40)
>>>>> ? ? ? ? Date Time ?Open ?High ? Low Close ?Vol OI FastAvg SlowAvg
>>>>> MyDate ChVol
>>>>> 1 ?01/04/1960 1300 59.91 59.91 59.91 59.91 3988 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-04 ? ?NA
>>>>> 2 ?01/05/1960 1300 60.39 60.39 60.39 60.39 3712 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-05 ? ?NA
>>>>> 3 ?01/06/1960 1300 60.13 60.13 60.13 60.13 3730 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-06 ? ?NA
>>>>> 4 ?01/07/1960 1300 59.69 59.69 59.69 59.69 3311 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-07 ? ?NA
>>>>> 5 ?01/08/1960 1300 59.50 59.50 59.50 59.50 3287 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-08 ? ?NA
>>>>> 6 ?01/11/1960 1300 58.77 58.77 58.77 58.77 3466 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-11 ? ?NA
>>>>> 7 ?01/12/1960 1300 58.41 58.41 58.41 58.41 3764 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-12 ? ?NA
>>>>> 8 ?01/13/1960 1300 58.08 58.08 58.08 58.08 3468 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-13 ? ?NA
>>>>> 9 ?01/14/1960 1300 58.40 58.40 58.40 58.40 3560 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-14 ? ?NA
>>>>> 10 01/15/1960 1300 58.38 58.38 58.38 58.38 3422 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-15 ? ?NA
>>>>> 11 01/18/1960 1300 57.89 57.89 57.89 57.89 3019 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-18 ? ?NA
>>>>> 12 01/19/1960 1300 57.27 57.27 57.27 57.27 3096 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-19 ? ?NA
>>>>> 13 01/20/1960 1300 57.07 57.07 57.07 57.07 2717 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-20 ? ?NA
>>>>> 14 01/21/1960 1300 57.21 57.21 57.21 57.21 2697 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-21 ? ?NA
>>>>> 15 01/22/1960 1300 57.38 57.38 57.38 57.38 2690 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-22 ? ?NA
>>>>> 16 01/25/1960 1300 56.78 56.78 56.78 56.78 2793 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-25 ? ?NA
>>>>> 17 01/26/1960 1300 56.86 56.86 56.86 56.86 3062 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-26 ? ?NA
>>>>> 18 01/27/1960 1300 56.72 56.72 56.72 56.72 2463 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-27 ? ?NA
>>>>> 19 01/28/1960 1300 56.13 56.13 56.13 56.13 2627 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-28 ? ?NA
>>>>> 20 01/29/1960 1300 55.61 55.61 55.61 55.61 3061 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-01-29 ? ?NA
>>>>> 21 02/01/1960 1300 55.96 55.96 55.96 55.96 2825 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-01 ? ?NA
>>>>> 22 02/02/1960 1300 56.82 56.82 56.82 56.82 3077 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-02 ? ?NA
>>>>> 23 02/03/1960 1300 56.32 56.32 56.32 56.32 3022 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-03 ? ?NA
>>>>> 24 02/04/1960 1300 56.27 56.27 56.27 56.27 2617 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-04 ? ?NA
>>>>> 25 02/05/1960 1300 55.98 55.98 55.98 55.98 2535 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-05 ? ?NA
>>>>> 26 02/08/1960 1300 55.32 55.32 55.32 55.32 3348 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-08 ? NaN
>>>>> 27 02/09/1960 1300 55.84 55.84 55.84 55.84 2859 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-09 ? NaN
>>>>> 28 02/10/1960 1300 55.49 55.49 55.49 55.49 2441 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-10 ? NaN
>>>>> 29 02/11/1960 1300 55.18 55.18 55.18 55.18 2606 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-11 ? NaN
>>>>> 30 02/12/1960 1300 55.46 55.46 55.46 55.46 2229 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-12 ? NaN
>>>>> 31 02/15/1960 1300 55.17 55.17 55.17 55.17 2772 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-15 ? NaN
>>>>> 32 02/16/1960 1300 54.73 54.73 54.73 54.73 3278 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-16 ? NaN
>>>>> 33 02/17/1960 1300 55.03 55.03 55.03 55.03 4208 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-17 ? NaN
>>>>> 34 02/18/1960 1300 55.80 55.80 55.80 55.80 3798 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-18 ? NaN
>>>>> 35 02/19/1960 1300 56.24 56.24 56.24 56.24 3234 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-19 ? NaN
>>>>> 36 02/23/1960 1300 55.94 55.94 55.94 55.94 2960 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-23 ? NaN
>>>>> 37 02/24/1960 1300 55.74 55.74 55.74 55.74 2744 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-24 ? NaN
>>>>> 38 02/25/1960 1300 55.93 55.93 55.93 55.93 3601 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-25 ? NaN
>>>>> 39 02/26/1960 1300 56.16 56.16 56.16 56.16 3384 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-26 ? NaN
>>>>> 40 02/29/1960 1300 56.12 56.12 56.12 56.12 2994 ?0 ? ? ? 0 ? ? ? 0
>>>>> 1960-02-29 ? NaN
>>>>>>
>>>>>
>>>>> ? The data came from TradeStation. It looks OK to me visually. The
>>>>> code stub I'm using follows. I could possibly post the data somewhere
>>>>> if necessary.
>>>>>
>>>>> Thanks,
>>>>> Mark
>>>>>
>>>>>
>>>>> require(chron)
>>>>> require(ggplot2)
>>>>> require(quantmod)
>>>>> require(reshape)
>>>>> require(TTR)
>>>>>
>>>>> AbsLookback = 13
>>>>>
>>>>> TradeSystem = read.csv("C:\\MiningData\\SPX_Daily.txt",header=TRUE)
>>>>> #TradeSystem = read.csv("C:\\MiningData\\SPX_Weekly.txt",header=TRUE)
>>>>> #TradeSystem = read.csv("C:\\MiningData\\INDU_Weekly.txt",header=TRUE)
>>>>> #TradeSystem = read.csv("C:\\MiningData\\NDX_Weekly.txt",header=TRUE)
>>>>> #TradeSystem = read.csv("C:\\MiningData\\RUT_Weekly.txt",header=TRUE)
>>>>>
>>>>> TradeSystem$MyDate = as.Date(TradeSystem$Date, "%m/%d/%Y")
>>>>>
>>>>> CVol = data.frame(chaikinVolatility(TradeSystem[,c("High","Low")],
>>>>> n=AbsLookback))
>>>>> names(CVol)[1]="ChVol"
>>>>>
>>>>> TradeSystem$ChVol = round(CVol$ChVol, 2)
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>>
>>>
>>
>


From josh.m.ulrich at gmail.com  Tue Sep 15 04:52:19 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Mon, 14 Sep 2009 21:52:19 -0500
Subject: [R-SIG-Finance] column transposition with xts
In-Reply-To: <5aebc8960909140353w76d7ef68ya8a994e000f46a26@mail.gmail.com>
References: <5aebc8960909140353w76d7ef68ya8a994e000f46a26@mail.gmail.com>
Message-ID: <8cca69990909141952t719c562en31414043dc01e5a8@mail.gmail.com>

Aleks,

This isn't the most elegant solution, especially with large data sets,
but it is *a* solution.  The code below yields a 24-column xts object
(20 lags of close prices, plus the original OHLC series).

library(xts)
library(quantmod)

data(sample_matrix)
x <- as.xts(sample_matrix[1:50,])

res <- x
for(i in 1:20) { res <- cbind(res,lag(Cl(x),i)) }

HTH,
Josh
--
http://www.fosstrading.com



On Mon, Sep 14, 2009 at 5:53 AM, Aleks Clark <aleks.clark at gmail.com> wrote:
> I'm working with SVMs and have recently migrated to using TTR and thus xts,
> but I've run into the following problem:
>
> For each vector in an SVMs training set, I need to have a given amount of
> historical data available as features for the vector. A simple example would
> be to have the past 20 close prices. So I'd take a matrix like this:
> ? ?O H L C
> 1
> 2
> 3
> 4
> 5
> ...
> N
>
> and output something like this to give to my SVM:
>
> O, H, L, C, C-1, C-2, C-3, etc
> 1
> 2
> 3
> 4
> 5
>
> obviously at the top of the matrix, the historical Close cells would be
> empty until there was enough data. In the pre-xts days, I accomplished this
> using sapply() and t(), a probably hackish solution, but all I was able to
> come up with. However, I am now running into problems with indices using
> this technique, and was wondering if there was a better way to perform this
> transformation, which really only amounts to a simple copy of data. Possible
> solutions that occur to me include some sort of "index shift" and then a
> merge, but I've been unable to get anything to work.
>
>
> --
> Aleks Clark
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From pgilbert at bank-banque-canada.ca  Tue Sep 15 17:01:47 2009
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 15 Sep 2009 11:01:47 -0400
Subject: [R-SIG-Finance] EXCEL & R
In-Reply-To: <a7d6d2740909140443j7b4135e5n7b2acd631ccfd5a1@mail.gmail.com>
References: <865280.3780.qm@web110105.mail.gq1.yahoo.com>	<971536df0909100521x6be06ff4jbc8228c3674bfeac@mail.gmail.com>
	<a7d6d2740909140443j7b4135e5n7b2acd631ccfd5a1@mail.gmail.com>
Message-ID: <4AAFAC5B.8040700@bank-banque-canada.ca>

If you are going the other way (R -> Excel)  WriteXLS on CRAN seems to 
be a pretty good, platform independent option. It is direct, in the 
sense that you do not need to fiddle with intermediate files, ODBC, 
etc.  You do need some relatively up-to-date perl modules.

Paul

Ana Nelson wrote:
> An alternative, platform-independent approach to this issue is to use a
> library such as XLRD to parse an Excel binary file directly and then write
> this data to a delimited text file for subsequent import into R. This has
> the advantage of not requiring (a) Excel or (b) Windows. You'd need to know
> a bit of Python, or check out whether your scripting language of choice has
> a similar library, many do.
>
>
>
> On Thu, Sep 10, 2009 at 1:21 PM, Gabor Grothendieck <ggrothendieck at gmail.com
>   
>> wrote:
>>     
>
>   
>> See:
>> http://wiki.r-project.org/rwiki/doku.php?id=tips:data-io:ms_windows
>>
>> On Wed, Sep 9, 2009 at 8:17 AM, Josh C. Chien <joshcchien at yahoo.com>
>> wrote:
>>     
>>> Hi R-all,
>>> In finance, EXCEL dominates over any products. It's a good interface with
>>>       
>> finance application.
>>     
>>> I like R but, sometimes, I can't use it for daily work.
>>> Just curious, does any guy know how to integrate EXCEL and R for working
>>>       
>> financial modeling ?
>>     
>>> Thanks a lot.
>>>
>>> Josh
>>>        [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>       
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>     
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>   
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential in...{{dropped:26}}


From binabina at bellsouth.net  Tue Sep 15 21:03:22 2009
From: binabina at bellsouth.net (zubin)
Date: Tue, 15 Sep 2009 15:03:22 -0400
Subject: [R-SIG-Finance] yahoo quote what= commands
In-Reply-To: <4AAFE4D8.2080205@dowlaty.com>
References: <4AACFD02.2060102@bellsouth.net> <4AAFE4D8.2080205@dowlaty.com>
Message-ID: <4AAFE4FA.5020108@bellsouth.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090915/c3f87643/attachment.pl>

From binabina at bellsouth.net  Tue Sep 15 21:08:42 2009
From: binabina at bellsouth.net (zubin)
Date: Tue, 15 Sep 2009 15:08:42 -0400
Subject: [R-SIG-Finance] yahoo quote what= commands
In-Reply-To: <4AAFE4FA.5020108@bellsouth.net>
References: <4AACFD02.2060102@bellsouth.net> <4AAFE4D8.2080205@dowlaty.com>
	<4AAFE4FA.5020108@bellsouth.net>
Message-ID: <4AAFE63A.4080006@bellsouth.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090915/e214658b/attachment.pl>

From binabina at bellsouth.net  Tue Sep 15 21:34:25 2009
From: binabina at bellsouth.net (zubin)
Date: Tue, 15 Sep 2009 15:34:25 -0400
Subject: [R-SIG-Finance] yahoo quote what= commands
In-Reply-To: <4AAFE63A.4080006@bellsouth.net>
References: <4AACFD02.2060102@bellsouth.net> <4AAFE4D8.2080205@dowlaty.com>
	<4AAFE4FA.5020108@bellsouth.net> <4AAFE63A.4080006@bellsouth.net>
Message-ID: <4AAFEC41.5050006@bellsouth.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090915/2094dd0b/attachment.pl>

From aleks.clark at gmail.com  Wed Sep 16 12:56:54 2009
From: aleks.clark at gmail.com (Aleks Clark)
Date: Wed, 16 Sep 2009 05:56:54 -0500
Subject: [R-SIG-Finance] column transposition with xts
In-Reply-To: <8cca69990909141952t719c562en31414043dc01e5a8@mail.gmail.com>
References: <5aebc8960909140353w76d7ef68ya8a994e000f46a26@mail.gmail.com>
	<8cca69990909141952t719c562en31414043dc01e5a8@mail.gmail.com>
Message-ID: <5aebc8960909160356k34608d35k8d4dcbed77108909@mail.gmail.com>

Josh,

was hoping to avoid a for loop, but I guess this is as good as it gets
:) luckily I should only have to run this once.

thanks,

Aleks

On Mon, Sep 14, 2009 at 9:52 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>
> Aleks,
>
> This isn't the most elegant solution, especially with large data sets,
> but it is *a* solution. ?The code below yields a 24-column xts object
> (20 lags of close prices, plus the original OHLC series).
>
> library(xts)
> library(quantmod)
>
> data(sample_matrix)
> x <- as.xts(sample_matrix[1:50,])
>
> res <- x
> for(i in 1:20) { res <- cbind(res,lag(Cl(x),i)) }
>
> HTH,
> Josh
> --
> http://www.fosstrading.com
>
>
>
> On Mon, Sep 14, 2009 at 5:53 AM, Aleks Clark <aleks.clark at gmail.com> wrote:
> > I'm working with SVMs and have recently migrated to using TTR and thus xts,
> > but I've run into the following problem:
> >
> > For each vector in an SVMs training set, I need to have a given amount of
> > historical data available as features for the vector. A simple example would
> > be to have the past 20 close prices. So I'd take a matrix like this:
> > ? ?O H L C
> > 1
> > 2
> > 3
> > 4
> > 5
> > ...
> > N
> >
> > and output something like this to give to my SVM:
> >
> > O, H, L, C, C-1, C-2, C-3, etc
> > 1
> > 2
> > 3
> > 4
> > 5
> >
> > obviously at the top of the matrix, the historical Close cells would be
> > empty until there was enough data. In the pre-xts days, I accomplished this
> > using sapply() and t(), a probably hackish solution, but all I was able to
> > come up with. However, I am now running into problems with indices using
> > this technique, and was wondering if there was a better way to perform this
> > transformation, which really only amounts to a simple copy of data. Possible
> > solutions that occur to me include some sort of "index shift" and then a
> > merge, but I've been unable to get anything to work.
> >
> >
> > --
> > Aleks Clark
> >
> > ? ? ? ?[[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >



--
Aleks Clark


From daniel.cegielka at gmail.com  Wed Sep 16 15:21:44 2009
From: daniel.cegielka at gmail.com (Daniel Cegielka)
Date: Wed, 16 Sep 2009 15:21:44 +0200
Subject: [R-SIG-Finance] hdf5, quantmod, xts... and dates
Message-ID: <4AB0E668.4080503@gmail.com>

Hi

I try use hdf5 with quantmod and xts and I have some problems with dates.

Some ideas how can I restore the original date (or 'origin' parametr)?

regards
daniel cegielka


my session:

> library(hdf5)
> getSymbols('GOOG',from='2009-09-01')
[1] "GOOG"
> GOOG
           GOOG.Open GOOG.High GOOG.Low GOOG.Close GOOG.Volume GOOG.Adjusted
2009-09-01    459.68    466.82   454.42     455.76     2594900        455.76
2009-09-02    455.82    458.33   452.59     453.01     1804800        453.01
2009-09-03    455.82    458.25   455.00     457.52     1646200        457.52
2009-09-04    457.57    462.60   455.78     461.30     1499200        461.30
2009-09-08    464.29    466.99   455.84     458.62     2656700        458.62
2009-09-09    459.06    466.27   458.80     463.97     2195400        463.97
2009-09-10    466.65    470.94   462.00     470.94     2534600        470.94
2009-09-11    470.40    473.30   467.63     472.14     1902900        472.14
2009-09-14    470.51    476.80   470.05     475.12     1975700        475.12
2009-09-15    475.08    478.91   472.71     477.54     2398100        477.54
> hdf5save('test.hdf','GOOG')
Setting buffer size in plist
in vector_io: permuting
About to write
About to write
in vector_io: tidying
> ls()
[1] "GOOG"
> rm(GOOG)
> ls()
character(0)
> hdf5load('test.hdf')
NULL
> ls()
[1] "GOOG"
> GOOG
        [,1]   [,2]   [,3]   [,4]    [,5]   [,6]
 [1,] 459.68 466.82 454.42 455.76 2594900 455.76
 [2,] 455.82 458.33 452.59 453.01 1804800 453.01
 [3,] 455.82 458.25 455.00 457.52 1646200 457.52
 [4,] 457.57 462.60 455.78 461.30 1499200 461.30
 [5,] 464.29 466.99 455.84 458.62 2656700 458.62
 [6,] 459.06 466.27 458.80 463.97 2195400 463.97
 [7,] 466.65 470.94 462.00 470.94 2534600 470.94
 [8,] 470.40 473.30 467.63 472.14 1902900 472.14
 [9,] 470.51 476.80 470.05 475.12 1975700 475.12
[10,] 475.08 478.91 472.71 477.54 2398100 477.54
attr(,"index")
 [1] 1251763200 1251849600 1251936000 1252022400 1252368000 1252454400
 [7] 1252540800 1252627200 1252886400 1252972800
attr(,".indexCLASS")
[1] "Date"
attr(,".indexTZ")
[1] "GMT"
attr(,"src")
[1] "yahoo"
attr(,"updated")
[1] 1253104705


> xts(GOOG[,],as.Date(GOOG[,1]))
             [,1]   [,2]   [,3]   [,4]    [,5]   [,6]
1971-04-01 455.82 458.33 452.59 453.01 1804800 453.01
1971-04-01 455.82 458.25 455.00 457.52 1646200 457.52
1971-04-03 457.57 462.60 455.78 461.30 1499200 461.30
1971-04-05 459.06 466.27 458.80 463.97 2195400 463.97
1971-04-05 459.68 466.82 454.42 455.76 2594900 455.76
1971-04-10 464.29 466.99 455.84 458.62 2656700 458.62
1971-04-12 466.65 470.94 462.00 470.94 2534600 470.94
1971-04-16 470.40 473.30 467.63 472.14 1902900 472.14
1971-04-16 470.51 476.80 470.05 475.12 1975700 475.12
1971-04-21 475.08 478.91 472.71 477.54 2398100 477.54


> xts(GOOG[,],as.POSIXct(GOOG[,1],origin='1970-01-01'))
                      [,1]   [,2]   [,3]   [,4]    [,5]   [,6]
1970-01-01 00:07:35 455.82 458.33 452.59 453.01 1804800 453.01
1970-01-01 00:07:35 455.82 458.25 455.00 457.52 1646200 457.52
1970-01-01 00:07:37 457.57 462.60 455.78 461.30 1499200 461.30
1970-01-01 00:07:39 459.06 466.27 458.80 463.97 2195400 463.97
1970-01-01 00:07:39 459.68 466.82 454.42 455.76 2594900 455.76
1970-01-01 00:07:44 464.29 466.99 455.84 458.62 2656700 458.62
1970-01-01 00:07:46 466.65 470.94 462.00 470.94 2534600 470.94
1970-01-01 00:07:50 470.40 473.30 467.63 472.14 1902900 472.14
1970-01-01 00:07:50 470.51 476.80 470.05 475.12 1975700 475.12
1970-01-01 00:07:55 475.08 478.91 472.71 477.54 2398100 477.54


> xts(GOOG[,],as.Date(GOOG[,1],origin='1970-01-01'))
             [,1]   [,2]   [,3]   [,4]    [,5]   [,6]
1971-04-01 455.82 458.33 452.59 453.01 1804800 453.01
1971-04-01 455.82 458.25 455.00 457.52 1646200 457.52
1971-04-03 457.57 462.60 455.78 461.30 1499200 461.30
1971-04-05 459.06 466.27 458.80 463.97 2195400 463.97
1971-04-05 459.68 466.82 454.42 455.76 2594900 455.76
1971-04-10 464.29 466.99 455.84 458.62 2656700 458.62
1971-04-12 466.65 470.94 462.00 470.94 2534600 470.94
1971-04-16 470.40 473.30 467.63 472.14 1902900 472.14
1971-04-16 470.51 476.80 470.05 475.12 1975700 475.12
1971-04-21 475.08 478.91 472.71 477.54 2398100 477.54


From jeff.a.ryan at gmail.com  Wed Sep 16 16:10:44 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 16 Sep 2009 09:10:44 -0500
Subject: [R-SIG-Finance] hdf5, quantmod, xts... and dates
In-Reply-To: <4AB0E668.4080503@gmail.com>
References: <4AB0E668.4080503@gmail.com>
Message-ID: <e8e755250909160710u263f711bt2275b89d57853f95@mail.gmail.com>

The problem is GOOG[,1] isn't the time index, that is stored as the
'index' attribute in the object:


> character(0)
>> hdf5load('test.hdf')
> NULL
>> ls()
> [1] "GOOG"
>> GOOG
> ? ? ? ?[,1] ? [,2] ? [,3] ? [,4] ? ?[,5] ? [,6]
> ?[1,] 459.68 466.82 454.42 455.76 2594900 455.76
> ?[2,] 455.82 458.33 452.59 453.01 1804800 453.01
> ?[3,] 455.82 458.25 455.00 457.52 1646200 457.52
> ?[4,] 457.57 462.60 455.78 461.30 1499200 461.30
> ?[5,] 464.29 466.99 455.84 458.62 2656700 458.62
> ?[6,] 459.06 466.27 458.80 463.97 2195400 463.97
> ?[7,] 466.65 470.94 462.00 470.94 2534600 470.94
> ?[8,] 470.40 473.30 467.63 472.14 1902900 472.14
> ?[9,] 470.51 476.80 470.05 475.12 1975700 475.12
> [10,] 475.08 478.91 472.71 477.54 2398100 477.54
> attr(,"index")
> ?[1] 1251763200 1251849600 1251936000 1252022400 1252368000 1252454400
> ?[7] 1252540800 1252627200 1252886400 1252972800
> attr(,".indexCLASS")
> [1] "Date"
> attr(,".indexTZ")
> [1] "GMT"
> attr(,"src")
> [1] "yahoo"
> attr(,"updated")
> [1] 1253104705
>

Something like:

goog <- .xts(GOOG, attr(GOOG,"index"))

should work.  Note the *dot* preceding the *xts* call. This is the
internal version that uses raw
POSIXct timestamps for efficiency purposes.

This would also work:

goog <- xts(GOOG, as.POSIXct(attr(GOOG,"index"),origin="1970-01-01"))


HTH
Jeff
>
>> xts(GOOG[,],as.Date(GOOG[,1]))
> ? ? ? ? ? ? [,1] ? [,2] ? [,3] ? [,4] ? ?[,5] ? [,6]
> 1971-04-01 455.82 458.33 452.59 453.01 1804800 453.01
> 1971-04-01 455.82 458.25 455.00 457.52 1646200 457.52
> 1971-04-03 457.57 462.60 455.78 461.30 1499200 461.30
> 1971-04-05 459.06 466.27 458.80 463.97 2195400 463.97
> 1971-04-05 459.68 466.82 454.42 455.76 2594900 455.76
> 1971-04-10 464.29 466.99 455.84 458.62 2656700 458.62
> 1971-04-12 466.65 470.94 462.00 470.94 2534600 470.94
> 1971-04-16 470.40 473.30 467.63 472.14 1902900 472.14
> 1971-04-16 470.51 476.80 470.05 475.12 1975700 475.12
> 1971-04-21 475.08 478.91 472.71 477.54 2398100 477.54
>
>
>> xts(GOOG[,],as.POSIXct(GOOG[,1],origin='1970-01-01'))
> ? ? ? ? ? ? ? ? ? ? ?[,1] ? [,2] ? [,3] ? [,4] ? ?[,5] ? [,6]
> 1970-01-01 00:07:35 455.82 458.33 452.59 453.01 1804800 453.01
> 1970-01-01 00:07:35 455.82 458.25 455.00 457.52 1646200 457.52
> 1970-01-01 00:07:37 457.57 462.60 455.78 461.30 1499200 461.30
> 1970-01-01 00:07:39 459.06 466.27 458.80 463.97 2195400 463.97
> 1970-01-01 00:07:39 459.68 466.82 454.42 455.76 2594900 455.76
> 1970-01-01 00:07:44 464.29 466.99 455.84 458.62 2656700 458.62
> 1970-01-01 00:07:46 466.65 470.94 462.00 470.94 2534600 470.94
> 1970-01-01 00:07:50 470.40 473.30 467.63 472.14 1902900 472.14
> 1970-01-01 00:07:50 470.51 476.80 470.05 475.12 1975700 475.12
> 1970-01-01 00:07:55 475.08 478.91 472.71 477.54 2398100 477.54
>
>
>> xts(GOOG[,],as.Date(GOOG[,1],origin='1970-01-01'))
> ? ? ? ? ? ? [,1] ? [,2] ? [,3] ? [,4] ? ?[,5] ? [,6]
> 1971-04-01 455.82 458.33 452.59 453.01 1804800 453.01
> 1971-04-01 455.82 458.25 455.00 457.52 1646200 457.52
> 1971-04-03 457.57 462.60 455.78 461.30 1499200 461.30
> 1971-04-05 459.06 466.27 458.80 463.97 2195400 463.97
> 1971-04-05 459.68 466.82 454.42 455.76 2594900 455.76
> 1971-04-10 464.29 466.99 455.84 458.62 2656700 458.62
> 1971-04-12 466.65 470.94 462.00 470.94 2534600 470.94
> 1971-04-16 470.40 473.30 467.63 472.14 1902900 472.14
> 1971-04-16 470.51 476.80 470.05 475.12 1975700 475.12
> 1971-04-21 475.08 478.91 472.71 477.54 2398100 477.54
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From ggrothendieck at gmail.com  Wed Sep 16 16:14:58 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 16 Sep 2009 10:14:58 -0400
Subject: [R-SIG-Finance] column transposition with xts
In-Reply-To: <5aebc8960909140353w76d7ef68ya8a994e000f46a26@mail.gmail.com>
References: <5aebc8960909140353w76d7ef68ya8a994e000f46a26@mail.gmail.com>
Message-ID: <971536df0909160714x657bca21h7bb4b7b5e7ffe146@mail.gmail.com>

zoo's lag function supports vector lags so try this
where x is your xts object:

z <- as.zoo(Cl(x))
zz <- lag(z, -seq(0, 20))

zz is a zoo object and you can use as.xts(zz) to convert it back to
xts if needed.

Note that lag.zoo follows the R convention of having negative lag mean
move the lagged series forward in time where lag.xts uses the opposite
convention.


On Mon, Sep 14, 2009 at 6:53 AM, Aleks Clark <aleks.clark at gmail.com> wrote:
> I'm working with SVMs and have recently migrated to using TTR and thus xts,
> but I've run into the following problem:
>
> For each vector in an SVMs training set, I need to have a given amount of
> historical data available as features for the vector. A simple example would
> be to have the past 20 close prices. So I'd take a matrix like this:
> ? ?O H L C
> 1
> 2
> 3
> 4
> 5
> ...
> N
>
> and output something like this to give to my SVM:
>
> O, H, L, C, C-1, C-2, C-3, etc
> 1
> 2
> 3
> 4
> 5
>
> obviously at the top of the matrix, the historical Close cells would be
> empty until there was enough data. In the pre-xts days, I accomplished this
> using sapply() and t(), a probably hackish solution, but all I was able to
> come up with. However, I am now running into problems with indices using
> this technique, and was wondering if there was a better way to perform this
> transformation, which really only amounts to a simple copy of data. Possible
> solutions that occur to me include some sort of "index shift" and then a
> merge, but I've been unable to get anything to work.
>
>
> --
> Aleks Clark
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Wed Sep 16 16:15:27 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 16 Sep 2009 09:15:27 -0500
Subject: [R-SIG-Finance] column transposition with xts
In-Reply-To: <5aebc8960909160356k34608d35k8d4dcbed77108909@mail.gmail.com>
References: <5aebc8960909140353w76d7ef68ya8a994e000f46a26@mail.gmail.com>
	<8cca69990909141952t719c562en31414043dc01e5a8@mail.gmail.com>
	<5aebc8960909160356k34608d35k8d4dcbed77108909@mail.gmail.com>
Message-ID: <e8e755250909160715y273586cfx32836abaecddb500@mail.gmail.com>

This probably won't be much better, but it is cleaner code:

*note: Lag is in quantmod*

> tail(Lag(Cl(x),1:20))
              Lag.1    Lag.2    Lag.3    Lag.4    Lag.5    Lag.6    Lag.7
2007-02-15 51.04699 50.90106 50.96653 50.91160 50.69562 50.67686 50.81383
2007-02-16 51.05185 51.04699 50.90106 50.96653 50.91160 50.69562 50.67686
2007-02-17 51.02164 51.05185 51.04699 50.90106 50.96653 50.91160 50.69562
2007-02-18 51.13653 51.02164 51.05185 51.04699 50.90106 50.96653 50.91160
2007-02-19 51.15151 51.13653 51.02164 51.05185 51.04699 50.90106 50.96653
2007-02-20 51.17899 51.15151 51.13653 51.02164 51.05185 51.04699 50.90106
              Lag.8    Lag.9   Lag.10   Lag.11   Lag.12   Lag.13   Lag.14
2007-02-15 50.60611 50.49865 50.69783 50.55509 50.43109 50.36928 50.35784
2007-02-16 50.81383 50.60611 50.49865 50.69783 50.55509 50.43109 50.36928
2007-02-17 50.67686 50.81383 50.60611 50.49865 50.69783 50.55509 50.43109
2007-02-18 50.69562 50.67686 50.81383 50.60611 50.49865 50.69783 50.55509
2007-02-19 50.91160 50.69562 50.67686 50.81383 50.60611 50.49865 50.69783
2007-02-20 50.96653 50.91160 50.69562 50.67686 50.81383 50.60611 50.49865
             Lag.15   Lag.16   Lag.17   Lag.18   Lag.19   Lag.20
2007-02-15 50.22578 50.02180 49.91875 49.88096 50.01091 50.07024
2007-02-16 50.35784 50.22578 50.02180 49.91875 49.88096 50.01091
2007-02-17 50.36928 50.35784 50.22578 50.02180 49.91875 49.88096
2007-02-18 50.43109 50.36928 50.35784 50.22578 50.02180 49.91875
2007-02-19 50.55509 50.43109 50.36928 50.35784 50.22578 50.02180
2007-02-20 50.69783 50.55509 50.43109 50.36928 50.35784 50.22578

HTH
Jeff

On Wed, Sep 16, 2009 at 5:56 AM, Aleks Clark <aleks.clark at gmail.com> wrote:
> Josh,
>
> was hoping to avoid a for loop, but I guess this is as good as it gets
> :) luckily I should only have to run this once.
>
> thanks,
>
> Aleks
>
> On Mon, Sep 14, 2009 at 9:52 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>>
>> Aleks,
>>
>> This isn't the most elegant solution, especially with large data sets,
>> but it is *a* solution. ?The code below yields a 24-column xts object
>> (20 lags of close prices, plus the original OHLC series).
>>
>> library(xts)
>> library(quantmod)
>>
>> data(sample_matrix)
>> x <- as.xts(sample_matrix[1:50,])
>>
>> res <- x
>> for(i in 1:20) { res <- cbind(res,lag(Cl(x),i)) }
>>
>> HTH,
>> Josh
>> --
>> http://www.fosstrading.com
>>
>>
>>
>> On Mon, Sep 14, 2009 at 5:53 AM, Aleks Clark <aleks.clark at gmail.com> wrote:
>> > I'm working with SVMs and have recently migrated to using TTR and thus xts,
>> > but I've run into the following problem:
>> >
>> > For each vector in an SVMs training set, I need to have a given amount of
>> > historical data available as features for the vector. A simple example would
>> > be to have the past 20 close prices. So I'd take a matrix like this:
>> > ? ?O H L C
>> > 1
>> > 2
>> > 3
>> > 4
>> > 5
>> > ...
>> > N
>> >
>> > and output something like this to give to my SVM:
>> >
>> > O, H, L, C, C-1, C-2, C-3, etc
>> > 1
>> > 2
>> > 3
>> > 4
>> > 5
>> >
>> > obviously at the top of the matrix, the historical Close cells would be
>> > empty until there was enough data. In the pre-xts days, I accomplished this
>> > using sapply() and t(), a probably hackish solution, but all I was able to
>> > come up with. However, I am now running into problems with indices using
>> > this technique, and was wondering if there was a better way to perform this
>> > transformation, which really only amounts to a simple copy of data. Possible
>> > solutions that occur to me include some sort of "index shift" and then a
>> > merge, but I've been unable to get anything to work.
>> >
>> >
>> > --
>> > Aleks Clark
>> >
>> > ? ? ? ?[[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>
>
>
> --
> Aleks Clark
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From daniel.cegielka at gmail.com  Wed Sep 16 16:26:25 2009
From: daniel.cegielka at gmail.com (Daniel Cegielka)
Date: Wed, 16 Sep 2009 16:26:25 +0200
Subject: [R-SIG-Finance] hdf5, quantmod, xts... and dates
In-Reply-To: <e8e755250909160710u263f711bt2275b89d57853f95@mail.gmail.com>
References: <4AB0E668.4080503@gmail.com>
	<e8e755250909160710u263f711bt2275b89d57853f95@mail.gmail.com>
Message-ID: <4AB0F591.50305@gmail.com>

Jeff Ryan pisze:
> The problem is GOOG[,1] isn't the time index, that is stored as the
> 'index' attribute in the object:
>
>
>   
>> character(0)
>>     
>>> hdf5load('test.hdf')
>>>       
>> NULL
>>     
>>> ls()
>>>       
>> [1] "GOOG"
>>     
>>> GOOG
>>>       
>>        [,1]   [,2]   [,3]   [,4]    [,5]   [,6]
>>  [1,] 459.68 466.82 454.42 455.76 2594900 455.76
>>  [2,] 455.82 458.33 452.59 453.01 1804800 453.01
>>  [3,] 455.82 458.25 455.00 457.52 1646200 457.52
>>  [4,] 457.57 462.60 455.78 461.30 1499200 461.30
>>  [5,] 464.29 466.99 455.84 458.62 2656700 458.62
>>  [6,] 459.06 466.27 458.80 463.97 2195400 463.97
>>  [7,] 466.65 470.94 462.00 470.94 2534600 470.94
>>  [8,] 470.40 473.30 467.63 472.14 1902900 472.14
>>  [9,] 470.51 476.80 470.05 475.12 1975700 475.12
>> [10,] 475.08 478.91 472.71 477.54 2398100 477.54
>> attr(,"index")
>>  [1] 1251763200 1251849600 1251936000 1252022400 1252368000 1252454400
>>  [7] 1252540800 1252627200 1252886400 1252972800
>> attr(,".indexCLASS")
>> [1] "Date"
>> attr(,".indexTZ")
>> [1] "GMT"
>> attr(,"src")
>> [1] "yahoo"
>> attr(,"updated")
>> [1] 1253104705
>>
>>     
>
> Something like:
>
> goog <- .xts(GOOG, attr(GOOG,"index"))
>
> should work.  Note the *dot* preceding the *xts* call. This is the
> internal version that uses raw
> POSIXct timestamps for efficiency purposes.
>
> This would also work:
>
> goog <- xts(GOOG, as.POSIXct(attr(GOOG,"index"),origin="1970-01-01"))
>
>
> HTH
> Jeff
THX Jeff.. It works ;)


From gps at asu.edu  Wed Sep 16 18:24:59 2009
From: gps at asu.edu (Geoffrey Smith)
Date: Wed, 16 Sep 2009 09:24:59 -0700
Subject: [R-SIG-Finance] how to winsorize data
Message-ID: <4b5c27440909160924u29af72e9r1af44224907c4d03@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090916/7630311c/attachment.pl>

From breno.neri at nyu.edu  Wed Sep 16 18:33:13 2009
From: breno.neri at nyu.edu (Breno Neri)
Date: Wed, 16 Sep 2009 12:33:13 -0400
Subject: [R-SIG-Finance] how to winsorize data
In-Reply-To: <4b5c27440909160924u29af72e9r1af44224907c4d03@mail.gmail.com>
References: <4b5c27440909160924u29af72e9r1af44224907c4d03@mail.gmail.com>
Message-ID: <5922f2b50909160933v5421a313q87d3a4d077fe1f2d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090916/8aec4737/attachment.pl>

From gps at asu.edu  Wed Sep 16 18:53:55 2009
From: gps at asu.edu (Geoffrey Smith)
Date: Wed, 16 Sep 2009 09:53:55 -0700
Subject: [R-SIG-Finance] how to winsorize data
In-Reply-To: <5922f2b50909160933v5421a313q87d3a4d077fe1f2d@mail.gmail.com>
References: <4b5c27440909160924u29af72e9r1af44224907c4d03@mail.gmail.com>
	<5922f2b50909160933v5421a313q87d3a4d077fe1f2d@mail.gmail.com>
Message-ID: <4b5c27440909160953o2d716cf8je255e51bc76f5884@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090916/a678c846/attachment.pl>

From david at revolution-computing.com  Wed Sep 16 19:04:25 2009
From: david at revolution-computing.com (David M Smith)
Date: Wed, 16 Sep 2009 10:04:25 -0700
Subject: [R-SIG-Finance] how to winsorize data
In-Reply-To: <5922f2b50909160933v5421a313q87d3a4d077fe1f2d@mail.gmail.com>
References: <4b5c27440909160924u29af72e9r1af44224907c4d03@mail.gmail.com>
	<5922f2b50909160933v5421a313q87d3a4d077fe1f2d@mail.gmail.com>
Message-ID: <475a3c8f0909161004ia1750ffvf916c3f5108c15b5@mail.gmail.com>

On Wed, Sep 16, 2009 at 9:33 AM, Breno Neri <breno.neri at nyu.edu> wrote:
> x <- x[ x>quantile(x, .05) & x<quantile(x, .95) ]

That will delete the extreme values from x, but if I understand the
process of winsorization correctly, the extreme values should be
*replaced* by the corresponding quantiles, no?

winsorize <- function(x, q=0.05) {
 extrema <- quantile(x, c(q, 1-q))	
 x[x<extrema[1]] <- extrema[1]
 x[x>extrema[2]] <- extrema[2]
 x
}

> summary(winsorize(rnorm(100),0.05))
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
-1.55200 -0.54590 -0.03203 -0.01133  0.54230  1.46300

> On Wed, Sep 16, 2009 at 12:24 PM, Geoffrey Smith <gps at asu.edu> wrote:
>
> > Hello, is there any kind of function that can winsorize a vector of numeric
> > data? ?I realize that the function mean(x, trim=...) will calculate
> > winsorized means, but I would like to winsorize the actual data. ?That is,
> > I
> > would like to actually change the extreme values to the 95% and 5%
> > percentile values. ?Thank you.
> >
> > --
> > Geoffrey Smith
> > Visiting Assistant Professor
> > Department of Finance
> > W. P. Carey School of Business
> > Arizona State University
> >

--
David M Smith <david at revolution-computing.com>
Director of Community, REvolution Computing www.revolution-computing.com
Tel: +1 (206) 577-4778 x3203 (San Francisco, USA)

Check out our upcoming events schedule at www.revolution-computing.com/events


From breno.neri at nyu.edu  Wed Sep 16 19:14:02 2009
From: breno.neri at nyu.edu (Breno Neri)
Date: Wed, 16 Sep 2009 13:14:02 -0400
Subject: [R-SIG-Finance] how to winsorize data
In-Reply-To: <475a3c8f0909161004ia1750ffvf916c3f5108c15b5@mail.gmail.com>
References: <4b5c27440909160924u29af72e9r1af44224907c4d03@mail.gmail.com> 
	<5922f2b50909160933v5421a313q87d3a4d077fe1f2d@mail.gmail.com> 
	<475a3c8f0909161004ia1750ffvf916c3f5108c15b5@mail.gmail.com>
Message-ID: <5922f2b50909161014k5a0b167aw3a14ac7be535bf1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090916/7eca7398/attachment.pl>

From cwrward at gmail.com  Wed Sep 16 19:15:07 2009
From: cwrward at gmail.com (Charles Ward)
Date: Wed, 16 Sep 2009 18:15:07 +0100
Subject: [R-SIG-Finance] how to winsorize data
In-Reply-To: <4b5c27440909160953o2d716cf8je255e51bc76f5884@mail.gmail.com>
References: <4b5c27440909160924u29af72e9r1af44224907c4d03@mail.gmail.com>
	<5922f2b50909160933v5421a313q87d3a4d077fe1f2d@mail.gmail.com>
	<4b5c27440909160953o2d716cf8je255e51bc76f5884@mail.gmail.com>
Message-ID: <bd9aa36b0909161015s18d66fepe66375295c62f12e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090916/011fb583/attachment.pl>

From ggrothendieck at gmail.com  Wed Sep 16 19:23:39 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 16 Sep 2009 13:23:39 -0400
Subject: [R-SIG-Finance] how to winsorize data
In-Reply-To: <4b5c27440909160924u29af72e9r1af44224907c4d03@mail.gmail.com>
References: <4b5c27440909160924u29af72e9r1af44224907c4d03@mail.gmail.com>
Message-ID: <971536df0909161023t1c9479f4p22c9fe140ff3b056@mail.gmail.com>

Try this:

pmax(pmin(x, quantile(x, .95)), quantile(x, .05))

On Wed, Sep 16, 2009 at 12:24 PM, Geoffrey Smith <gps at asu.edu> wrote:
> Hello, is there any kind of function that can winsorize a vector of numeric
> data? ?I realize that the function mean(x, trim=...) will calculate
> winsorized means, but I would like to winsorize the actual data. ?That is, I
> would like to actually change the extreme values to the 95% and 5%
> percentile values. ?Thank you.
>
> --
> Geoffrey Smith
> Visiting Assistant Professor
> Department of Finance
> W. P. Carey School of Business
> Arizona State University
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From shane.conway at gmail.com  Wed Sep 16 19:33:37 2009
From: shane.conway at gmail.com (Shane Conway)
Date: Wed, 16 Sep 2009 13:33:37 -0400
Subject: [R-SIG-Finance] how to winsorize data
In-Reply-To: <4b5c27440909160924u29af72e9r1af44224907c4d03@mail.gmail.com>
References: <4b5c27440909160924u29af72e9r1af44224907c4d03@mail.gmail.com>
Message-ID: <dd3243090909161033ia54a4a9lffe8512db6c47f6b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090916/ecb70b55/attachment.pl>

From aleks.clark at gmail.com  Thu Sep 17 11:56:04 2009
From: aleks.clark at gmail.com (Aleks Clark)
Date: Thu, 17 Sep 2009 04:56:04 -0500
Subject: [R-SIG-Finance] using caret to select financial models
Message-ID: <5aebc8960909170256t8b8c429y56e867c9398a8c13@mail.gmail.com>

Hey folks,

I've been playing around with models for financial predictions, and
stumbled across the caret package. From the descriptions, it seems to
be a fairly powerful tool, but the sheer power seems to have made the
docs a bit...daunting...to say the least. Are there any step-by-step
examples out there that show how to use caret to find the most
appropriate model and features for that model? The additional pdfs
that are on the cran page seem to cover these things, but they do so
in a frustratingly (to me at least) terse manner.

Thanks,


-- 
Aleks Clark


From jorge.nieves at moorecap.com  Thu Sep 17 16:00:15 2009
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Thu, 17 Sep 2009 10:00:15 -0400
Subject: [R-SIG-Finance] Short and longs positions - Portfolio optimization
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF02B3C94F@NYC-XCH3.win.moorecap.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090917/1a9731f2/attachment.pl>

From tzygmund at googlemail.com  Thu Sep 17 17:31:46 2009
From: tzygmund at googlemail.com (tzygmund mcfarlane)
Date: Thu, 17 Sep 2009 16:31:46 +0100
Subject: [R-SIG-Finance] stableFit
Message-ID: <9c0c14910909170831x538edbd4v392cb469a61f5a26@mail.gmail.com>

A quick question about stableFit() in the fBasics package. Is it
possible to constrain the gamma and delta parameters and only estimate
the alpha and beta parameters? I tried:

##################
set.seed(1953)
r = rstable(n = 1000, alpha = 1.9, beta = 0.3)
stableFit(r, gamma=1, delta=0, type=c("q", "mle"), doplot=TRUE, trace=TRUE)
##################

but that seems to estimate the gamma and delta as well.

Thanks


From ltorgo at dcc.fc.up.pt  Fri Sep 18 16:30:24 2009
From: ltorgo at dcc.fc.up.pt (Luis Torgo)
Date: Fri, 18 Sep 2009 15:30:24 +0100
Subject: [R-SIG-Finance] Problem with getSymbols.oanda in quantmod
Message-ID: <4AB39980.3070901@dcc.fc.up.pt>

Dear all,

I think there is a small problem with the getSymbols.oanda() function in 
package quantmod.
I'm using R version 2.9.0 (2009-04-17), and quantmod version 0.3-11.

Here is a trace of the problem:
 > 
setSymbolLookup(GSPC=list(name='^GSPC',src='yahoo'),USDEUR=list(name='USD/EUR',src='oanda',from=as.Date('2009-01-01')))

 > getSymbols(c('GSPC','USDEUR'))
[1] "GSPC"   "USDEUR"
Warning message:
In getSymbols.oanda(Symbols = "USDEUR", env = <environment>, verbose = 
FALSE,  :
  incorrectly specified currency pair USD/EUR

The problem seems to be easy to solve. I think it resorts to changing 
the line in function getSymbols.oanda():
currency.pair <- strsplit(toupper(Symbols[[i]]), "/")[[1]]
into
currency.pair <- strsplit(toupper(Symbols.name), "/")[[1]]

Luis Torgo

-- 
Luis Torgo
   FC/LIAAD - INESC Porto, LA    Phone : (+351) 22 339 20 93
   University of Porto           Fax   : (+351) 22 339 20 99
   R. de Ceuta, 118, 6o          email : ltorgo at liaad.up.pt
   4050-190 PORTO - PORTUGAL     WWW   : http://www.liaad.up.pt/~ltorgo


From jeff.a.ryan at gmail.com  Fri Sep 18 16:57:43 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 18 Sep 2009 09:57:43 -0500
Subject: [R-SIG-Finance] Problem with getSymbols.oanda in quantmod
In-Reply-To: <4AB39980.3070901@dcc.fc.up.pt>
References: <4AB39980.3070901@dcc.fc.up.pt>
Message-ID: <e8e755250909180757r71d41795x6be775c21b94d21c@mail.gmail.com>

Hi Luis,

Thanks for the patch, it is now updated on R-forge.

Best,
Jeff

On Fri, Sep 18, 2009 at 9:30 AM, Luis Torgo <ltorgo at dcc.fc.up.pt> wrote:
> Dear all,
>
> I think there is a small problem with the getSymbols.oanda() function in
> package quantmod.
> I'm using R version 2.9.0 (2009-04-17), and quantmod version 0.3-11.
>
> Here is a trace of the problem:
>>
>> setSymbolLookup(GSPC=list(name='^GSPC',src='yahoo'),USDEUR=list(name='USD/EUR',src='oanda',from=as.Date('2009-01-01')))
>
>> getSymbols(c('GSPC','USDEUR'))
> [1] "GSPC" ? "USDEUR"
> Warning message:
> In getSymbols.oanda(Symbols = "USDEUR", env = <environment>, verbose =
> FALSE, ?:
> ?incorrectly specified currency pair USD/EUR
>
> The problem seems to be easy to solve. I think it resorts to changing the
> line in function getSymbols.oanda():
> currency.pair <- strsplit(toupper(Symbols[[i]]), "/")[[1]]
> into
> currency.pair <- strsplit(toupper(Symbols.name), "/")[[1]]
>
> Luis Torgo
>
> --
> Luis Torgo
> ?FC/LIAAD - INESC Porto, LA ? ?Phone : (+351) 22 339 20 93
> ?University of Porto ? ? ? ? ? Fax ? : (+351) 22 339 20 99
> ?R. de Ceuta, 118, 6o ? ? ? ? ?email : ltorgo at liaad.up.pt
> ?4050-190 PORTO - PORTUGAL ? ? WWW ? : http://www.liaad.up.pt/~ltorgo
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From nicolas.chapados at gmail.com  Sat Sep 19 23:03:15 2009
From: nicolas.chapados at gmail.com (Nicolas Chapados)
Date: Sat, 19 Sep 2009 17:03:15 -0400
Subject: [R-SIG-Finance] Bug in fOptions::GBSOption boundary condition?
Message-ID: <6fdf6d430909191403n3b5fabb5n2ce02b477c9aaa6c@mail.gmail.com>

Hello all,

I am using the fOptions module in RMetrics to price simple
Black-Scholes options.  There seems to be an oversight in boundary
conditions when the underlying falls exactly on a strike price at
maturity, yielding a NaN price for the option (both calls and puts are
affected) ::

> library(fOptions)
> GBSOption("c", 485, 485, 0, 0, 0, 0.23)@price
[1] NaN

whereas slight departures from the strike are OK ::

> GBSOption("c", 484.999, 485, 0, 0, 0, 0.23)@price
[1] 0
> GBSOption("c", 485.001, 485, 0, 0, 0, 0.23)@price
[1] 0.001

I'm using fOptions_290.75 (as returned by sessionInfo()).  Has this
been corrected in the development branch?

Many thanks for any help!
+ Nicolas Chapados


From breman.mark at gmail.com  Tue Sep 22 09:23:04 2009
From: breman.mark at gmail.com (Mark Breman)
Date: Tue, 22 Sep 2009 09:23:04 +0200
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
In-Reply-To: <d718c8210907231913m49ce69fnf44289155ec9a81d@mail.gmail.com>
References: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>
	<4A5F2A38.9060100@braverock.com>
	<d718c8210907200157m1b9c9698me96a6af142d85e87@mail.gmail.com>
	<4A646F7E.3090406@braverock.com>
	<d718c8210907231913m49ce69fnf44289155ec9a81d@mail.gmail.com>
Message-ID: <5e6a2e670909220023k5db38724v3354b9149b452e98@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090922/c1d43586/attachment.pl>

From anspassov at googlemail.com  Tue Sep 22 14:53:30 2009
From: anspassov at googlemail.com (Angel Spassov)
Date: Tue, 22 Sep 2009 14:53:30 +0200
Subject: [R-SIG-Finance] Markov Switching
Message-ID: <4ab8c8ca.02135e0a.4ffe.6493@mx.google.com>

DeaR list,

I am looking for a descent implementation of
a Markov Switching Vector Autoregressive Model.

Until now I found the following packages:

1) MSBVAR: It seems that this package estimates
Markov-Switching VAR-models only from a Bayesian
point of view? Correct me if I am wrong. 
I also need it from a frequentist point of view.

Assumed, this package is the single option, 
I would greatly appreciate some basic 
code of how to estimate a single model with it. 
For example, how can I model the following 
bivariate time series: 

set.seed(1234)
myts <- as.ts(data.frame(a=rnorm(100),b=rnorm(100) + 2))

If I got the man pages correctly, I have to use both 
"msbvar" and "gibbs.msbvar". Mr. Brandt is 
pointing the user to a non-existing vignette 
(see ?msbvar, "Note" section) and no example are 
provided in the man pages.
I think MSBVAR is a challenge for a new user.

2) fMarkovSwitching: This package is not compatible
with the latest version of R and is seemingly
suitable only for univariate models.
Nevertheless, at least I succeeded to estimate 
my model and to interpret the results with this package. 

Any other suggestions?

Angel.


From windspeedo99 at gmail.com  Tue Sep 22 15:02:50 2009
From: windspeedo99 at gmail.com (Wind)
Date: Tue, 22 Sep 2009 21:02:50 +0800
Subject: [R-SIG-Finance] LPPL model for bubble burst forcasting
In-Reply-To: <5e6a2e670909220023k5db38724v3354b9149b452e98@mail.gmail.com>
References: <d718c8210907160538r7bb2229fq41676c09e1fcfbcf@mail.gmail.com>
	<4A5F2A38.9060100@braverock.com>
	<d718c8210907200157m1b9c9698me96a6af142d85e87@mail.gmail.com>
	<4A646F7E.3090406@braverock.com>
	<d718c8210907231913m49ce69fnf44289155ec9a81d@mail.gmail.com>
	<5e6a2e670909220023k5db38724v3354b9149b452e98@mail.gmail.com>
Message-ID: <d718c8210909220602x590f774dm8269949333c41596@mail.gmail.com>

No furthure progress.  In July, I just calculate the model with genoud in
the rgenoud package, with the same codes for LPPL function I have  shared in
the list.   Today I just found the codes and run it against the S&P 500.   I
run genoud ten times for about 20 minutes.     Though the parameters may be
not within the range that LPPL papers defined, the curve fitting is OK.    I
received your email while studying the graph.    So the graph is attached.
 It says that S&P would begin correction after Sept.   Just for fun.
  Actually the LPPL should be applied with rolling window, and parameters
meeting some creterion.   It would be very time consuming for genoud.

wind

On Tue, Sep 22, 2009 at 3:23 PM, Mark Breman <breman.mark at gmail.com> wrote:

> Hello Wind,
> I was wondering if you made any progress on the R implementation of the
> model, and if you could/would be willing to share what you got so far?
>
> Regards,
>
> -Mark-
>
> 2009/7/24 Wind <windspeedo99 at gmail.com>
>
>> Since I am not good at coding or statistics, genetic algo has been used
>> instead of tabu search.    Package rgenoud is great and idiot proof.
>>
>> Attached is the latest analysis on SSEC index, the equity market index of
>> mainland China.  genoud() has been run for 10 times, generating 10 fits.
>> The residuals are all stationary according to ADF test from package urca.
>> But the time window setting is subjective and maybe some minor problems on
>> parameter conditions such as omega and C.  So,  just for fun.
>>
>> Thanks for all the encouragement and help from the list.    And thanks for
>> the detailed instructions on statistics issues by one of the author on LPPL,
>> Dr. Lin.
>>
>> wind
>>
>> On Mon, Jul 20, 2009 at 9:22 PM, Brian G. Peterson <brian at braverock.com>wrote:
>>
>>> Glad to see you're making progress on this problem.
>>>
>>> This paper
>>> http://www.diegm.uniud.it/satt/papers/DiSc06b.pdf
>>>
>>> implements a tabu search in R, though they didn't publish their code.
>>>  You might want to contact them for the implementation and permission to
>>> share their tabu search algorithm/code with the R community.
>>>
>>> They also reference an R package called RACE by this gentleman:
>>> http://iridia.ulb.ac.be/~mbiro/
>>> that they use for evaluating the solution.
>>>
>>> More generally, from my limited understanding, tabu search is an
>>> extension and refinement of simulated annealing approaches easily
>>> implemented in R.  In brief the approach is to take your best 'n' solutions
>>> from a random space search, and then search "near" those solutions.
>>>  Simulated annealing and its close cousins have a lot of benefits in
>>> finance, where a single true optima from a closed form problem is not likely
>>> to be available.  I personally have rarely found 'optim' to be usable for my
>>> problem space, and have had to use other solvers for practical problems in
>>> finance.
>>>
>>> Regards,
>>>
>>>  - Brian
>>>
>>>
>>> Wind wrote:
>>>
>>>> Some progress.   The LPPL curve could be plotted with the following
>>>> codes.
>>>> The problem now is how to get the best fit parameters.
>>>> Some researchers  use python or matlab for LPPL calibrating.     It
>>>> seems that some of them prefer tabu search for optimums locating.   It
>>>> seems that there's still no general function for tabu search in R.
>>>> At the end of codes, I give the possible parameter combinations to be
>>>> searched in, maybe there are other functions for optimum searching in
>>>> R.
>>>> Any suggestion would be appreciated.
>>>>
>>>>
>>>> ## Financial Bubbles, Real Estate bubbles, Derivative Bubbles, and the
>>>> Financial and Economic Crisis
>>>> ## http://arxiv.org/abs/0905.0220
>>>> ## Fig. 23 S&P500 index (in logarithmic scale)in  Page 39
>>>>
>>>>
>>>> library(quantmod)
>>>>
>>>> LPPL1<-function(p,dtc=20,alpha=0.35,omega=0.1,phi=1)
>>>> {
>>>>        #function in page 26 of http://arxiv.org/abs/0905.0220
>>>>        #the basic form of LPPL
>>>>        dtc=abs(floor(dtc))
>>>>        tc<-length(p)+dtc
>>>>        dt<-abs(tc-(1:length(p)))
>>>>
>>>>        x1<-dt^alpha
>>>>        x2<-(dt^alpha)*cos(omega*log(dt)+phi)
>>>>
>>>>        f<-lm(log(p) ~ x1+x2)
>>>>
>>>>
>>>>  f$para<-list(recno=dim(f$model)[1],dtc=dtc,alpha=alpha,omega=omega,phi=phi,sigma=summary(f)$sigma)
>>>>        return(f)
>>>> }
>>>>
>>>> opt.lppl<-function(x)
>>>> {
>>>>        #derived function for optim
>>>>
>>>>  return(LPPL1(p,dtc=x[1],alpha=x[2],omega=x[3],phi=x[4])$para$sigma)
>>>> }
>>>>
>>>> LPPL1.x<-function(lpplf,pt=100)
>>>> {
>>>>        #x axis for predicting
>>>>        dt<-abs((lpplf$recno+lpplf$dtc)-(1:(lpplf$recno+lpplf$dtc+pt)))
>>>>        dt[dt==0]<-0.5
>>>>
>>>>        x1<-dt^lpplf$alpha
>>>>        x2<-(dt^lpplf$alpha)*cos(lpplf$omega*log(dt)+lpplf$phi)
>>>>        return(list(x1=x1,x2=x2))
>>>>
>>>> }
>>>>
>>>> #get the SP500 index
>>>>
>>>> pr<-getSymbols("^GSPC",auto.assign=FALSE,from="2003-10-1",to="2007-05-16")[,4]
>>>> p<-as.numeric(pr)
>>>>
>>>>
>>>> plot(p,type="l",log="y",xlim=c(0,length(p)+100),ylim=c(min(p),max(p)*1.2))
>>>> abline(v=length(p),col="green")
>>>>
>>>> #something like the Fig. 23 in  Page 39 of
>>>> http://arxiv.org/abs/0905.0220
>>>> #but obviously the result is not calibrated well
>>>> #using optim like this can not calibrate the LPPL model
>>>> opts<-sapply(seq(1,50,10),function(x){
>>>>                        o<-optim(c(x,0.6,20,1),opt.lppl)
>>>>
>>>>  f3<-LPPL1(p,dtc=o$par[1],alpha=o$par[2],omega=o$par[3],phi=o$par[4])
>>>>                        xp<-LPPL1.x(f3$para,200)
>>>>                        f3p<-predict(f3,data.frame(x1=xp$x1,x2=xp$x2))
>>>>                        lines(exp(f3p),col="blue")
>>>>                        lines(exp(fitted(f3)),col="red")
>>>>                        f3$para
>>>>                })
>>>>
>>>>
>>>>
>>>> ##crash point after dtc days
>>>> dtc<-seq(1,100,1)
>>>>
>>>> ##appropraite range of the parametes of LPPL
>>>> ##according to Dr. W.X. Zhou's new book which is in Chinese
>>>> ##the increments of the sequences are added according to my own judement
>>>> alpha<-seq(0.01,1.2,0.1)
>>>> omega<-seq(0,40,1)
>>>> phi<-seq(0,7,0.1)
>>>>
>>>> ##millions possible combinations
>>>> #complete test would be difficult
>>>> para<-expand.grid(dtc=dtc,alpha=alpha,omega=omega,phi=phi)
>>>> dim(para)
>>>>
>>>> system.time(sigs<-apply(para[1:100,],1,function(x){LPPL1(p,dtc=x[1],alpha=x[2],omega=x[3],phi=x[4])$para$sigma}))
>>>>
>>>> ##methods for minimum sigma searching within the parameter combinations
>>>> ##not implemented yet
>>>>
>>>>
>>>> wind
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> On Thu, Jul 16, 2009 at 9:25 PM, Brian G. Peterson<brian at braverock.com>
>>>> wrote:
>>>>
>>>>
>>>>> So first, using your real name and ideally your professional identity,
>>>>> ask
>>>>> for the python code.  Better yet, get an academic buddy to do it.
>>>>> Usually
>>>>> getting access to the code isn't too tough.  Mention things like
>>>>> "repeatable
>>>>> research" and "collaboration" in your email.  Two of the authors
>>>>> publish
>>>>> their email addresses in one of the papers you reference, so contacting
>>>>> them
>>>>> should be easy.
>>>>>
>>>>> Next port the python code to R.
>>>>>
>>>>> If you can't do that, then replicate the model in R "from scratch".  A
>>>>> trivial scan of the paper in question lends several techniques that are
>>>>> well
>>>>> covered in R: AR, GARCH, power laws, linear regression, stochastic
>>>>> discount
>>>>> factor, Ornstein-Uhlenbeck, etc.
>>>>> There are volumes of information available on these topics from within
>>>>> R, in
>>>>> numerous books, and in the archives of this mailing list and r-help.
>>>>>
>>>>> You're going to have to do your replication in pieces, probably
>>>>> starting
>>>>> with their implementation of the log periodic power law (LPPL), for
>>>>> which I
>>>>> do not believe there is an existing direct analogue in R though all the
>>>>> component parts necessary to replicate it should be readily available.
>>>>>
>>>>> As you work on each step of the replication, share your code with this
>>>>> list
>>>>> and the problems you are having with a particular step.  Ask specific,
>>>>> directed questions with code to back them up.  Someone will likely help
>>>>> you
>>>>> solve the specific problem.
>>>>>
>>>>> In R generally, it is not necessary that you be able to *do* the math
>>>>> (think
>>>>> pencil and paper), but if you plan to replicate published work, it will
>>>>> be
>>>>> necessary to *understand* at least some of how the math works, and to
>>>>> be
>>>>> able to pick out the names of techniques that you can search for an
>>>>> utilize.
>>>>>
>>>>> Basically, I'm recommending that you (specifically) and others (more
>>>>> generally) should share the process of replicating a technique like
>>>>> this, as
>>>>> well as the final product, to give all the rest of us who are likely to
>>>>> be
>>>>> helping "you" get all this done. quid pro quo.
>>>>>
>>>>> Cheers,
>>>>>
>>>>>  - Brian
>>>>>
>>>>>
>>>>> --
>>>>> Brian G. Peterson
>>>>> http://braverock.com/brian/
>>>>> Ph: 773-459-4973
>>>>> IM: bgpbraverock
>>>>>
>>>>>
>>>>>
>>>>> Wind wrote:
>>>>>
>>>>>
>>>>>> Prof. Sornette has spent years forcasting bubble burst with
>>>>>> "log-periodic power law".    The latest paper  gives "a
>>>>>> self-consistent model for explosive financial bubbles, which combines
>>>>>> a mean-reverting volatility process and a stochastic conditional
>>>>>> return which reflects nonlinear positive feedbacks and continuous
>>>>>> updates of the investors' beliefs and sentiments."
>>>>>>
>>>>>> And his  latest  predicting is the burst of Chinese equity bubble at
>>>>>> the end of July.     http://arxiv.org/abs/0907.1827
>>>>>>
>>>>>> While waiting to see the result, I wonder whether it is possible to
>>>>>> replicate the forcast with R.  The model is in the page 10 of the "A
>>>>>> Consistent Model of `Explosive' Financial Bubbles With Mean-Reversing
>>>>>> Residuals",  http://arxiv.org/abs/0905.0128  .   The output chart is
>>>>>> in the page 3 of "The Chinese Equity Bubble: Ready to Burst",
>>>>>> http://arxiv.org/abs/0907.1827 .   I guess the authors of the latter
>>>>>> paper use the same model as described in the first paper.
>>>>>>
>>>>>> Because statistics is still challenging for me though I could use R
>>>>>> for  basic data manipulations,  I wonder which package or function
>>>>>> would be necessary to implement the model in the paper.  The model
>>>>>> seems more complicated than the models in the R tutorials for me.
>>>>>> By the way, the author of the paper used Python and the codes are
>>>>>> private.
>>>>>>
>>>>>> Any suggestion would be highly appreciated.
>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>
>>>
>>> --
>>> Brian G. Peterson
>>> http://braverock.com/brian/
>>> Ph: 773-459-4973
>>> IM: bgpbraverock
>>>
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090922/eff00f6c/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: SP500 20090921.png
Type: image/png
Size: 10672 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090922/eff00f6c/attachment.png>

From patrick.t.brandt at gmail.com  Wed Sep 23 00:32:37 2009
From: patrick.t.brandt at gmail.com (Patrick Brandt)
Date: Tue, 22 Sep 2009 17:32:37 -0500
Subject: [R-SIG-Finance] Markov Switching
In-Reply-To: <4ab8c8ca.02135e0a.4ffe.6493@mx.google.com>
References: <4ab8c8ca.02135e0a.4ffe.6493@mx.google.com>
Message-ID: <232e3f240909221532x7c2c1816q202e96c7fa0cf9d2@mail.gmail.com>

If you look at the package list on CRAN you will see a package that
meets you needs for a frequentist MS VAR: MSVAR.  It is listed right
after (my) MSBVAR in the alphabetical listing of packages.

The MSBVAR vignette is in progress right now.  The real challenge here
is coding a general implementation of Bayesian MS BVARs that is useful
to the largest possible audience.

My intent is that the MSBVAR package is for BAYESIAN models.  At
present, the only frequentist model is a reduced form VAR.  And that
is only there for comparison purposes...

If you need general VAR models, check out the vars and urca packages.

PTB

-- 
Patrick Brandt
Assistant Professor
Political Science
School of Economic, Political and Policy Sciences
University of Texas at Dallas
Personal site: http://www.utdallas.edu/~pbrandt
MSBVAR site: http://yule.utdallas.edu



On Tue, Sep 22, 2009 at 7:53 AM, Angel Spassov <anspassov at googlemail.com> wrote:
> DeaR list,
>
> I am looking for a descent implementation of
> a Markov Switching Vector Autoregressive Model.
>
> Until now I found the following packages:
>
> 1) MSBVAR: It seems that this package estimates
> Markov-Switching VAR-models only from a Bayesian
> point of view? Correct me if I am wrong.
> I also need it from a frequentist point of view.
>
> Assumed, this package is the single option,
> I would greatly appreciate some basic
> code of how to estimate a single model with it.
> For example, how can I model the following
> bivariate time series:
>
> set.seed(1234)
> myts <- as.ts(data.frame(a=rnorm(100),b=rnorm(100) + 2))
>
> If I got the man pages correctly, I have to use both
> "msbvar" and "gibbs.msbvar". Mr. Brandt is
> pointing the user to a non-existing vignette
> (see ?msbvar, "Note" section) and no example are
> provided in the man pages.
> I think MSBVAR is a challenge for a new user.
>
> 2) fMarkovSwitching: This package is not compatible
> with the latest version of R and is seemingly
> suitable only for univariate models.
> Nevertheless, at least I succeeded to estimate
> my model and to interpret the results with this package.
>
> Any other suggestions?
>
> Angel.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From brenda.quismorio at gmail.com  Wed Sep 23 02:49:29 2009
From: brenda.quismorio at gmail.com (Brenda Quismorio)
Date: Tue, 22 Sep 2009 17:49:29 -0700
Subject: [R-SIG-Finance] Help to calculate tail dependence and tail risks
Message-ID: <b3288fe10909221749m6386c6c3if190d980162cbe8e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090922/3eeab0fb/attachment.pl>

From markknecht at gmail.com  Wed Sep 23 03:24:54 2009
From: markknecht at gmail.com (Mark Knecht)
Date: Tue, 22 Sep 2009 18:24:54 -0700
Subject: [R-SIG-Finance] Help to calculate tail dependence and tail risks
In-Reply-To: <b3288fe10909221749m6386c6c3if190d980162cbe8e@mail.gmail.com>
References: <b3288fe10909221749m6386c6c3if190d980162cbe8e@mail.gmail.com>
Message-ID: <5bdc1c8b0909221824p1a25eec5gfd4977ca655f6cff@mail.gmail.com>

Does your code have a line like

require(fCopulae)

?

- Mark

On Tue, Sep 22, 2009 at 5:49 PM, Brenda Quismorio
<brenda.quismorio at gmail.com> wrote:
> Dear R users,
>
> May I ask help on very basic steps? It's from someone who is really
> not-technical. Hoping for your patience and kindness.
>
> I need to run the four Elliptical copulae dependency functions (of fCopulae
> package), the GPD modelling functions and VaR( of fExtremes).
>
> I've read through the available manuals and managed to install and load
> these packages. I get the message that the functions of these packages are
> still not found.
>
> Also, I have managed to read my bi-variate data in xls.
>
> Could someone please tell me how to proceed?
>
> Many thanks in advance.
>
> Brenda
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From nicolas.chapados at gmail.com  Wed Sep 23 17:43:14 2009
From: nicolas.chapados at gmail.com (Nicolas Chapados)
Date: Wed, 23 Sep 2009 11:43:14 -0400
Subject: [R-SIG-Finance] timeSeries:: bizarre rbind behavior with colnames
Message-ID: <6fdf6d430909230843p674da615uc7e886855d211985@mail.gmail.com>

Dear list,

Looking at the code for timeSeries::rbind, it appears that it insists
on concatenating the names of the two series it's trying to bind, even
if they match exactly, thereby creating a longer series name.  This is
in sharp contrast to, e.g. rbind.data.frame.  This creates very
badly-behaved column names (i.e. horrendously-long) in the case where
one attempts to construct a timeSeries incrementally.

For example:

> rbind(timeSeries(1.0, timeDate("1996-01-01"), units="Level"), timeSeries(2.0, timeDate("2009-01-01"), units="Level"))
GMT
           Level_Level
1996-01-01           1
2009-01-01           2


Can someone explain the design rationale for choosing to concatenate
the column names in such a manner?  In the short term, I resolved to
incrementally constructing a matrix, and at the very end building a
timeSeries, but I would really prefer to keep timeSeries all along...

Many thanks for any help!
+ Nicolas Chapados


From jeff.a.ryan at gmail.com  Wed Sep 23 17:59:07 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 23 Sep 2009 10:59:07 -0500
Subject: [R-SIG-Finance] timeSeries:: bizarre rbind behavior with
	colnames
In-Reply-To: <6fdf6d430909230843p674da615uc7e886855d211985@mail.gmail.com>
References: <6fdf6d430909230843p674da615uc7e886855d211985@mail.gmail.com>
Message-ID: <e8e755250909230859u387d1790l7775fe6c11e378de@mail.gmail.com>

Nicolas,

I don't think this is a list question per se, as you should really
contact the maintainer.

While I'll agree the design is odd, if you're insisting on using
timeSeries, and you find a problem, it would be more useful to all if
you proposed a patch.

Complaints/demands for explanations/etc aren't productive, and
effectively show no consideration for the time and effort that
_contributors_ make to R.

If you're looking for a better time-series solution than a matrix as
an interim representation, I would suggest xts.  Or zoo...

Jeff

On Wed, Sep 23, 2009 at 10:43 AM, Nicolas Chapados
<nicolas.chapados at gmail.com> wrote:
> Dear list,
>
> Looking at the code for timeSeries::rbind, it appears that it insists
> on concatenating the names of the two series it's trying to bind, even
> if they match exactly, thereby creating a longer series name. ?This is
> in sharp contrast to, e.g. rbind.data.frame. ?This creates very
> badly-behaved column names (i.e. horrendously-long) in the case where
> one attempts to construct a timeSeries incrementally.
>
> For example:
>
>> rbind(timeSeries(1.0, timeDate("1996-01-01"), units="Level"), timeSeries(2.0, timeDate("2009-01-01"), units="Level"))
> GMT
> ? ? ? ? ? Level_Level
> 1996-01-01 ? ? ? ? ? 1
> 2009-01-01 ? ? ? ? ? 2
>
>
> Can someone explain the design rationale for choosing to concatenate
> the column names in such a manner? ?In the short term, I resolved to
> incrementally constructing a matrix, and at the very end building a
> timeSeries, but I would really prefer to keep timeSeries all along...
>
> Many thanks for any help!
> + Nicolas Chapados
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From karlavhv142 at hotmail.com  Wed Sep 23 21:02:15 2009
From: karlavhv142 at hotmail.com (karla hernandez villafuerte)
Date: Wed, 23 Sep 2009 13:02:15 -0600
Subject: [R-SIG-Finance] (no subject)
Message-ID: <COL124-W464B9009C36113E00C70B7E5DB0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090923/8fa40ddf/attachment.pl>

From karlavhv142 at hotmail.com  Wed Sep 23 21:12:28 2009
From: karlavhv142 at hotmail.com (karla hernandez villafuerte)
Date: Wed, 23 Sep 2009 13:12:28 -0600
Subject: [R-SIG-Finance] loglik() in urca function
Message-ID: <COL124-W45F61F1D713B632D0BA4FE5DB0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090923/15a6837a/attachment.pl>

From cedrick at cedrickjohnson.com  Wed Sep 23 23:42:01 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Wed, 23 Sep 2009 17:42:01 -0400
Subject: [R-SIG-Finance] Possible enhancement to volatility in TTR
Message-ID: <4ABA9629.5020703@cedrickjohnson.com>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090923/c3a6596c/attachment.html>

From Bernhard_Pfaff at fra.invesco.com  Thu Sep 24 11:15:56 2009
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Thu, 24 Sep 2009 10:15:56 +0100
Subject: [R-SIG-Finance] loglik() in urca function
In-Reply-To: <COL124-W45F61F1D713B632D0BA4FE5DB0@phx.gbl>
References: <COL124-W45F61F1D713B632D0BA4FE5DB0@phx.gbl>
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C31E136B1C@GBHENXMB02.corp.amvescap.net>

Dear Karla,

see help("ur.za-class") for the details of a returned object from ur.za, in particular "testreg" and in your case A at testreg. This is an S3 summary.lm object. Unfortunately, a logLik-method is only available for lm-type objects. However, you can determine an "optimal" lag length by employing ur.df() for the whole sample or a sub-sample, by using either "AIC" or "BIC" as information criteria. 

hth,
Bernhard 

>
>
>
>Dear Group:
> 
>I want to get the loglik of the regresion associated to the 
>estimation of the unit root test Zivot-Andrews of the package 
>urca. I am new using R, then I simply tried the next sequence:
> 
>> A<-ur.za(var,model="intercept",lag=2)
>> logLik(A)
> 
>but the result is an error, and I think it is because A (the 
>result of the function) is not class S4.
> 
>> class(A)
>[1] "ur.za"
>attr(,"package")
>[1] "urca"
>
>I want to use the logLik to compare different number of lags, 
>in order to know the optimal one.
> 
>I will be really grateful if someone can tell me how to 
>calculate the logLik or the optimal number of lags.
> 
>Thank you very much!,
> 
>Karla
> 
>
>
>
>?Cu?l de estas 16 personalidades es la tuya? ?Descubre qui?n 
>eres realmente! 		 	   		  
>_________________________________________________________________
>[[elided Hotmail spam]]
>
>	[[alternative HTML version deleted]]
>
>
*****************************************************************
Confidentiality Note: The information contained in this ...{{dropped:10}}


From comtech.usa at gmail.com  Thu Sep 24 19:20:30 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 24 Sep 2009 10:20:30 -0700
Subject: [R-SIG-Finance] Does anybody know how to connect to KDB from within
	R?
Message-ID: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>

Please give me some pointers... Thanks a lot!


From brian at braverock.com  Thu Sep 24 19:39:20 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 24 Sep 2009 12:39:20 -0500
Subject: [R-SIG-Finance] Does anybody know how to connect to KDB from
 within	R?
In-Reply-To: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>
References: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>
Message-ID: <4ABBAEC8.6050305@braverock.com>

Michael wrote:
> Does anybody know how to connect to KDB from within R?
> Please give me some pointers... Thanks a lot!
>   
Michael,

KX distributes R glue code with kdb. 

http://kx.com/q/interfaces/r/

Talk to them if you need support on using it, your firm presumably paid 
enough for the license...

Also, you may wish to read Dirk's post and related posts on the list 
archives here regarding POSIXct compatibility:

http://dirk.eddelbuettel.com/blog/2009/02/03/#kdbplus_datetime_patch


Regards,

    - Brian


From brian at braverock.com  Thu Sep 24 20:00:58 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 24 Sep 2009 13:00:58 -0500
Subject: [R-SIG-Finance] computing term structures of bonds
In-Reply-To: <F159E21276C80B4E86F0C0DF7017F75901C6FE8F@ISB-MAIL1.isb.local>
References: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>
	<4ABBAEC8.6050305@braverock.com>
	<F159E21276C80B4E86F0C0DF7017F75901C6FE8F@ISB-MAIL1.isb.local>
Message-ID: <4ABBB3DA.6040200@braverock.com>

Silika Prohl wrote:
> Dear all,
> I need to compute yield corporate bond yields. I have coupon, 
> market price and maturity. Could anybody give an advice how
> to compute this. I found the package termstr in R.
> Is this one which I need?
> Best,
> Sven
>   
Please don't reply to an unrelated message with a completely different 
topic.  It messes up threaded readers.

'termstrc' is a fine package, and so is 'sde'.  Use the one appropriate 
to your problem.

Regards,

   - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From bogaso.christofer at gmail.com  Thu Sep 24 21:18:07 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Thu, 24 Sep 2009 12:18:07 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Problem on ploting of a zoo object
Message-ID: <25579310.post@talk.nabble.com>


Hi,

Let say I have following codes

dat <- rnorm(100)
date1 <- 35886
dat2 <- zooreg(dat, as.Date(date1, origin="1900-01-01"), frequency=12)
index(dat2) <- format(index(dat2), "%m/%y")

However when I try to plot "dat2", I got following error :

> plot(dat2)
Error in plot.window(...) : need finite 'xlim' values
In addition: Warning messages:
1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
2: In min(x) : no non-missing arguments to min; returning Inf
3: In max(x) : no non-missing arguments to max; returning -Inf

Can anyone please tell me how to get the plot?

Thanks,

-- 
View this message in context: http://www.nabble.com/Problem-on-ploting-of-a-zoo-object-tp25579310p25579310.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Thu Sep 24 21:25:51 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 24 Sep 2009 15:25:51 -0400
Subject: [R-SIG-Finance] [R-sig-finance] Problem on ploting of a zoo
	object
In-Reply-To: <25579310.post@talk.nabble.com>
References: <25579310.post@talk.nabble.com>
Message-ID: <971536df0909241225g2d914b8ekf33c65d9fd6b44bc@mail.gmail.com>

> set.seed(123)
> dat <- rnorm(100)
> date1 <- 35886
> dat2 <- zooreg(dat, as.Date(date1, origin="1900-01-01"), frequency=12)
> index(dat2) <- format(index(dat2), "%m/%y")
Warning message:
In diff(as.numeric(value)) : NAs introduced by coercion

In the last statement you've tried to assign the same time to every observation.

> dat2 <- zooreg(dat, as.Date(date1, origin="1900-01-01"), frequency=12)
> format(index(dat2), "%m/%y")
  [1] "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98"
 [10] "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98"
 [19] "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98"
 [28] "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98"
 [37] "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98"
 [46] "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98"
 [55] "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98"
 [64] "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98"
 [73] "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98"
 [82] "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98"
 [91] "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98" "04/98"
[100] "04/98"

On Thu, Sep 24, 2009 at 3:18 PM, Bogaso <bogaso.christofer at gmail.com> wrote:
>
> Hi,
>
> Let say I have following codes
>
> dat <- rnorm(100)
> date1 <- 35886
> dat2 <- zooreg(dat, as.Date(date1, origin="1900-01-01"), frequency=12)
> index(dat2) <- format(index(dat2), "%m/%y")
>
> However when I try to plot "dat2", I got following error :
>
>> plot(dat2)
> Error in plot.window(...) : need finite 'xlim' values
> In addition: Warning messages:
> 1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
> 2: In min(x) : no non-missing arguments to min; returning Inf
> 3: In max(x) : no non-missing arguments to max; returning -Inf
>
> Can anyone please tell me how to get the plot?
>
> Thanks,
>
> --
> View this message in context: http://www.nabble.com/Problem-on-ploting-of-a-zoo-object-tp25579310p25579310.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From daniel.cegielka at gmail.com  Thu Sep 24 23:54:04 2009
From: daniel.cegielka at gmail.com (Daniel Cegielka)
Date: Thu, 24 Sep 2009 23:54:04 +0200
Subject: [R-SIG-Finance] Does anybody know how to connect to KDB from
 within	R?
In-Reply-To: <4ABBAEC8.6050305@braverock.com>
References: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>
	<4ABBAEC8.6050305@braverock.com>
Message-ID: <4ABBEA7C.1000308@gmail.com>

Brian G. Peterson pisze:
> Also, you may wish to read Dirk's post and related posts on the list
> archives here regarding POSIXct compatibility:
>
> http://dirk.eddelbuettel.com/blog/2009/02/03/#kdbplus_datetime_patch

http://groups.google.com/group/personal-kdbplus/browse_thread/thread/f3271f5cf534c67f

This patch is build in kdbplus.c (line: 470-489):

https://code.kx.com/trac/browser/kx/kdb%2B/interfaces/r/kdbplus.c


> Re: [R-SIG-Finance] Does anybody know how to connect to KDB from
within    R?

yes:

run: ~/q/l32/q -p 5000

R session:

# dyn.load(file.path("/home/kdeplus/.../q","l64"))
# con <-
.Call("kx_r_open_connection",list(host="localhost",as.integer(port=5000),user=NULL))
# execute <- function(con,query) {
  .Call("kx_r_execute",as.integer(con),query)
 }
# execute(con,"s:([] name:`A`B`C; var:10 20 30)")
# execute(con, "select from s")


best,
daniel cegielka


From comtech.usa at gmail.com  Fri Sep 25 00:14:36 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 24 Sep 2009 15:14:36 -0700
Subject: [R-SIG-Finance] Does anybody know how to connect to KDB from
	within R?
In-Reply-To: <4ABBEA7C.1000308@gmail.com>
References: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>
	<4ABBAEC8.6050305@braverock.com> <4ABBEA7C.1000308@gmail.com>
Message-ID: <b1f16d9d0909241514y37b771b9y4b60103a3b45bace@mail.gmail.com>

Cool!

I've seen the following files in that folder:

Index of /q/interfaces/r/w32

	Name	Last modified	Size	Description
	Parent Directory	 	 -
	R.dll	12-Feb-2009 18:58	2.7M
	c.o	12-Feb-2009 18:58	 23K
	k.h	12-Feb-2009 18:58	3.3K
	kdbplus.R	19-Feb-2009 03:21	420
	kdbplus.dll	12-Feb-2009 18:58	 66K
	kdbplus.o	11-Feb-2009 08:30	 14K


How do I install these files to my R-system?

Thanks!



On Thu, Sep 24, 2009 at 2:54 PM, Daniel Cegielka
<daniel.cegielka at gmail.com> wrote:
> Brian G. Peterson pisze:
>> Also, you may wish to read Dirk's post and related posts on the list
>> archives here regarding POSIXct compatibility:
>>
>> http://dirk.eddelbuettel.com/blog/2009/02/03/#kdbplus_datetime_patch
>
> http://groups.google.com/group/personal-kdbplus/browse_thread/thread/f3271f5cf534c67f
>
> This patch is build in kdbplus.c (line: 470-489):
>
> https://code.kx.com/trac/browser/kx/kdb%2B/interfaces/r/kdbplus.c
>
>
>> Re: [R-SIG-Finance] Does anybody know how to connect to KDB from
> within ? ?R?
>
> yes:
>
> run: ~/q/l32/q -p 5000
>
> R session:
>
> # dyn.load(file.path("/home/kdeplus/.../q","l64"))
> # con <-
> .Call("kx_r_open_connection",list(host="localhost",as.integer(port=5000),user=NULL))
> # execute <- function(con,query) {
> ?.Call("kx_r_execute",as.integer(con),query)
> ?}
> # execute(con,"s:([] name:`A`B`C; var:10 20 30)")
> # execute(con, "select from s")
>
>
> best,
> daniel cegielka
>


From daniel.cegielka at gmail.com  Fri Sep 25 00:25:29 2009
From: daniel.cegielka at gmail.com (Daniel Cegielka)
Date: Fri, 25 Sep 2009 00:25:29 +0200
Subject: [R-SIG-Finance] Does anybody know how to connect to KDB from
 within R?
In-Reply-To: <b1f16d9d0909241514y37b771b9y4b60103a3b45bace@mail.gmail.com>
References: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>	
	<4ABBAEC8.6050305@braverock.com> <4ABBEA7C.1000308@gmail.com>
	<b1f16d9d0909241514y37b771b9y4b60103a3b45bace@mail.gmail.com>
Message-ID: <4ABBF1D9.2090209@gmail.com>

Michael pisze:
> Cool!
>
> I've seen the following files in that folder:
>
> Index of /q/interfaces/r/w32
>
> 	Name	Last modified	Size	Description
> 	Parent Directory	 	 -
> 	R.dll	12-Feb-2009 18:58	2.7M
> 	c.o	12-Feb-2009 18:58	 23K
> 	k.h	12-Feb-2009 18:58	3.3K
> 	kdbplus.R	19-Feb-2009 03:21	420
> 	kdbplus.dll	12-Feb-2009 18:58	 66K
> 	kdbplus.o	11-Feb-2009 08:30	 14K
>
>
> How do I install these files to my R-system?
>
> Thanks!
>   

I have linux. You can keep this files in any place.. and simply paste
localisation in this line

dyn.load("c:/my_files/q/interfaces/r/w32/kdbplus.dll")


edit kdbplus.R

or from R:
> source(kdbplus.R)


daniel


>
>
> On Thu, Sep 24, 2009 at 2:54 PM, Daniel Cegielka
> <daniel.cegielka at gmail.com> wrote:
>   
>> Brian G. Peterson pisze:
>>     
>>> Also, you may wish to read Dirk's post and related posts on the list
>>> archives here regarding POSIXct compatibility:
>>>
>>> http://dirk.eddelbuettel.com/blog/2009/02/03/#kdbplus_datetime_patch
>>>       
>> http://groups.google.com/group/personal-kdbplus/browse_thread/thread/f3271f5cf534c67f
>>
>> This patch is build in kdbplus.c (line: 470-489):
>>
>> https://code.kx.com/trac/browser/kx/kdb%2B/interfaces/r/kdbplus.c
>>
>>
>>     
>>> Re: [R-SIG-Finance] Does anybody know how to connect to KDB from
>>>       
>> within    R?
>>
>> yes:
>>
>> run: ~/q/l32/q -p 5000
>>
>> R session:
>>
>> # dyn.load(file.path("/home/kdeplus/.../q","l64"))
>> # con <-
>> .Call("kx_r_open_connection",list(host="localhost",as.integer(port=5000),user=NULL))
>> # execute <- function(con,query) {
>>  .Call("kx_r_execute",as.integer(con),query)
>>  }
>> # execute(con,"s:([] name:`A`B`C; var:10 20 30)")
>> # execute(con, "select from s")
>>
>>
>> best,
>> daniel cegielka
>>
>>


From comtech.usa at gmail.com  Fri Sep 25 00:54:10 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 24 Sep 2009 15:54:10 -0700
Subject: [R-SIG-Finance] Does anybody know how to connect to KDB from
	within R?
In-Reply-To: <4ABBF1D9.2090209@gmail.com>
References: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>
	<4ABBAEC8.6050305@braverock.com> <4ABBEA7C.1000308@gmail.com>
	<b1f16d9d0909241514y37b771b9y4b60103a3b45bace@mail.gmail.com>
	<4ABBF1D9.2090209@gmail.com>
Message-ID: <b1f16d9d0909241554l6d68e8cfke7db7659920f2334@mail.gmail.com>

Very cool.

I did the source thing. I am using Windows XP.

Are there any sample database/code/project that I could try if this
R-KDB connection works or not?

I would love to explore a bit to see how I could achieve synergy by
combining the power of R and KDB/Q together.

Any pointers?

Thanks a lot!

On Thu, Sep 24, 2009 at 3:25 PM, Daniel Cegielka
<daniel.cegielka at gmail.com> wrote:
> Michael pisze:
>> Cool!
>>
>> I've seen the following files in that folder:
>>
>> Index of /q/interfaces/r/w32
>>
>> ? ? ? Name ? ?Last modified ? Size ? ?Description
>> ? ? ? Parent Directory ? ? ? ? ? ? ? ? -
>> ? ? ? R.dll ? 12-Feb-2009 18:58 ? ? ? 2.7M
>> ? ? ? c.o ? ? 12-Feb-2009 18:58 ? ? ? ?23K
>> ? ? ? k.h ? ? 12-Feb-2009 18:58 ? ? ? 3.3K
>> ? ? ? kdbplus.R ? ? ? 19-Feb-2009 03:21 ? ? ? 420
>> ? ? ? kdbplus.dll ? ? 12-Feb-2009 18:58 ? ? ? ?66K
>> ? ? ? kdbplus.o ? ? ? 11-Feb-2009 08:30 ? ? ? ?14K
>>
>>
>> How do I install these files to my R-system?
>>
>> Thanks!
>>
>
> I have linux. You can keep this files in any place.. and simply paste
> localisation in this line
>
> dyn.load("c:/my_files/q/interfaces/r/w32/kdbplus.dll")
>
>
> edit kdbplus.R
>
> or from R:
>> source(kdbplus.R)
>
>
> daniel
>
>
>>
>>
>> On Thu, Sep 24, 2009 at 2:54 PM, Daniel Cegielka
>> <daniel.cegielka at gmail.com> wrote:
>>
>>> Brian G. Peterson pisze:
>>>
>>>> Also, you may wish to read Dirk's post and related posts on the list
>>>> archives here regarding POSIXct compatibility:
>>>>
>>>> http://dirk.eddelbuettel.com/blog/2009/02/03/#kdbplus_datetime_patch
>>>>
>>> http://groups.google.com/group/personal-kdbplus/browse_thread/thread/f3271f5cf534c67f
>>>
>>> This patch is build in kdbplus.c (line: 470-489):
>>>
>>> https://code.kx.com/trac/browser/kx/kdb%2B/interfaces/r/kdbplus.c
>>>
>>>
>>>
>>>> Re: [R-SIG-Finance] Does anybody know how to connect to KDB from
>>>>
>>> within ? ?R?
>>>
>>> yes:
>>>
>>> run: ~/q/l32/q -p 5000
>>>
>>> R session:
>>>
>>> # dyn.load(file.path("/home/kdeplus/.../q","l64"))
>>> # con <-
>>> .Call("kx_r_open_connection",list(host="localhost",as.integer(port=5000),user=NULL))
>>> # execute <- function(con,query) {
>>> ?.Call("kx_r_execute",as.integer(con),query)
>>> ?}
>>> # execute(con,"s:([] name:`A`B`C; var:10 20 30)")
>>> # execute(con, "select from s")
>>>
>>>
>>> best,
>>> daniel cegielka
>>>
>>>
>
>


From daniel.cegielka at gmail.com  Fri Sep 25 01:03:25 2009
From: daniel.cegielka at gmail.com (Daniel Cegielka)
Date: Fri, 25 Sep 2009 01:03:25 +0200
Subject: [R-SIG-Finance] Does anybody know how to connect to KDB from
 within R?
In-Reply-To: <b1f16d9d0909241554l6d68e8cfke7db7659920f2334@mail.gmail.com>
References: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>	
	<4ABBAEC8.6050305@braverock.com> <4ABBEA7C.1000308@gmail.com>	
	<b1f16d9d0909241514y37b771b9y4b60103a3b45bace@mail.gmail.com>	
	<4ABBF1D9.2090209@gmail.com>
	<b1f16d9d0909241554l6d68e8cfke7db7659920f2334@mail.gmail.com>
Message-ID: <4ABBFABD.9060901@gmail.com>

Michael pisze:
> Very cool.
>
> I did the source thing. I am using Windows XP.
>
> Are there any sample database/code/project that I could try if this
> R-KDB connection works or not?
>
> I would love to explore a bit to see how I could achieve synergy by
> combining the power of R and KDB/Q together.
>
> Any pointers?
>
> Thanks a lot!
>   

https://code.kx.com/trac/browser/kx/kdb%2B


trade:([]time:`time$();sym:`symbol$();price:`float$();size:`int$())

`trade insert(09:30:00.000;`a;10.75;100)
`trade insert(09:31:00.000;`a;10.76;100)
`trade insert(09:32:00.000;`a;10.77;100)
`trade insert(09:33:00.000;`a;10.77;100)

q)select from trade
time         sym price size
---------------------------
09:30:00.000 a   10.75 100
09:31:00.000 a   10.76 100
09:32:00.000 a   10.77 100
09:33:00.000 a   10.77 100

q)select from trade where price>10.75
time         sym price size
---------------------------
09:31:00.000 a   10.76 100
09:32:00.000 a   10.77 100
09:33:00.000 a   10.77 100

q)select sum size by sym from trade
sym| size
---| ----
a  | 400


or in web browser

http://127.0.0.1:5000/
http://127.0.0.1:5000/trade
http://127.0.0.1:5000/?select from trade where price > 10.75

best,
daniel


From comtech.usa at gmail.com  Fri Sep 25 02:35:53 2009
From: comtech.usa at gmail.com (Michael)
Date: Thu, 24 Sep 2009 17:35:53 -0700
Subject: [R-SIG-Finance] Does anybody know how to connect to KDB from
	within R?
In-Reply-To: <4ABBFABD.9060901@gmail.com>
References: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>
	<4ABBAEC8.6050305@braverock.com> <4ABBEA7C.1000308@gmail.com>
	<b1f16d9d0909241514y37b771b9y4b60103a3b45bace@mail.gmail.com>
	<4ABBF1D9.2090209@gmail.com>
	<b1f16d9d0909241554l6d68e8cfke7db7659920f2334@mail.gmail.com>
	<4ABBFABD.9060901@gmail.com>
Message-ID: <b1f16d9d0909241735o336a68earf789740e902ce2c5@mail.gmail.com>

It doesn't seem to work on my computer:

http://127.0.0.1:5000/

I am using that trial version on my personal laptop: the one that
auto-exits every 2 hours...

What might be the problem?

On Thu, Sep 24, 2009 at 4:03 PM, Daniel Cegielka
<daniel.cegielka at gmail.com> wrote:
> Michael pisze:
>> Very cool.
>>
>> I did the source thing. I am using Windows XP.
>>
>> Are there any sample database/code/project that I could try if this
>> R-KDB connection works or not?
>>
>> I would love to explore a bit to see how I could achieve synergy by
>> combining the power of R and KDB/Q together.
>>
>> Any pointers?
>>
>> Thanks a lot!
>>
>
> https://code.kx.com/trac/browser/kx/kdb%2B
>
>
> trade:([]time:`time$();sym:`symbol$();price:`float$();size:`int$())
>
> `trade insert(09:30:00.000;`a;10.75;100)
> `trade insert(09:31:00.000;`a;10.76;100)
> `trade insert(09:32:00.000;`a;10.77;100)
> `trade insert(09:33:00.000;`a;10.77;100)
>
> q)select from trade
> time ? ? ? ? sym price size
> ---------------------------
> 09:30:00.000 a ? 10.75 100
> 09:31:00.000 a ? 10.76 100
> 09:32:00.000 a ? 10.77 100
> 09:33:00.000 a ? 10.77 100
>
> q)select from trade where price>10.75
> time ? ? ? ? sym price size
> ---------------------------
> 09:31:00.000 a ? 10.76 100
> 09:32:00.000 a ? 10.77 100
> 09:33:00.000 a ? 10.77 100
>
> q)select sum size by sym from trade
> sym| size
> ---| ----
> a ?| 400
>
>
> or in web browser
>
> http://127.0.0.1:5000/
> http://127.0.0.1:5000/trade
> http://127.0.0.1:5000/?select from trade where price > 10.75
>
> best,
> daniel
>
>
>
>
>
>
>


From daniel.cegielka at gmail.com  Fri Sep 25 09:10:58 2009
From: daniel.cegielka at gmail.com (Daniel Cegielka)
Date: Fri, 25 Sep 2009 09:10:58 +0200
Subject: [R-SIG-Finance] Does anybody know how to connect to KDB from
 within R?
In-Reply-To: <b1f16d9d0909241735o336a68earf789740e902ce2c5@mail.gmail.com>
References: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>	
	<4ABBAEC8.6050305@braverock.com> <4ABBEA7C.1000308@gmail.com>	
	<b1f16d9d0909241514y37b771b9y4b60103a3b45bace@mail.gmail.com>	
	<4ABBF1D9.2090209@gmail.com>	
	<b1f16d9d0909241554l6d68e8cfke7db7659920f2334@mail.gmail.com>	
	<4ABBFABD.9060901@gmail.com>
	<b1f16d9d0909241735o336a68earf789740e902ce2c5@mail.gmail.com>
Message-ID: <4ABC6D02.1070205@gmail.com>

Michael pisze:
> It doesn't seem to work on my computer:
>
> http://127.0.0.1:5000/
>
> I am using that trial version on my personal laptop: the one that
> auto-exits every 2 hours...
>
> What might be the problem?
To connect R into kdb you must set port:

start q with "-p port" parametr:
q -p 5000

or from q console:

\q 5000


and set this port in R session:

con <- .Call("kx_r_open_connection",list(host="localhost",as.integer(port=5000),user=NULL))


and you can connect R (or web browser) to kdb.


best,
daniel


From comtech.usa at gmail.com  Fri Sep 25 15:20:05 2009
From: comtech.usa at gmail.com (Michael)
Date: Fri, 25 Sep 2009 06:20:05 -0700
Subject: [R-SIG-Finance] Does anybody know how to connect to KDB from
	within R?
In-Reply-To: <4ABC6D02.1070205@gmail.com>
References: <b1f16d9d0909241020x31f8b8blb7e8f0fa65884be0@mail.gmail.com>
	<4ABBAEC8.6050305@braverock.com> <4ABBEA7C.1000308@gmail.com>
	<b1f16d9d0909241514y37b771b9y4b60103a3b45bace@mail.gmail.com>
	<4ABBF1D9.2090209@gmail.com>
	<b1f16d9d0909241554l6d68e8cfke7db7659920f2334@mail.gmail.com>
	<4ABBFABD.9060901@gmail.com>
	<b1f16d9d0909241735o336a68earf789740e902ce2c5@mail.gmail.com>
	<4ABC6D02.1070205@gmail.com>
Message-ID: <b1f16d9d0909250620wd8aa32fp4f5e5a48feb6b06d@mail.gmail.com>

Ok. cool. But are they any real-world example to show the synergy from
combining the power of R and KDB?

I couldn't find any...

On Fri, Sep 25, 2009 at 12:10 AM, Daniel Cegielka
<daniel.cegielka at gmail.com> wrote:
> Michael pisze:
>> It doesn't seem to work on my computer:
>>
>> http://127.0.0.1:5000/
>>
>> I am using that trial version on my personal laptop: the one that
>> auto-exits every 2 hours...
>>
>> What might be the problem?
> To connect R into kdb you must set port:
>
> start q with "-p port" parametr:
> q -p 5000
>
> or from q console:
>
> \q 5000
>
>
> and set this port in R session:
>
> con <- .Call("kx_r_open_connection",list(host="localhost",as.integer(port=5000),user=NULL))
>
>
> and you can connect R (or web browser) to kdb.
>
>
> best,
> daniel
>


From comtech.usa at gmail.com  Fri Sep 25 15:22:14 2009
From: comtech.usa at gmail.com (Michael)
Date: Fri, 25 Sep 2009 06:22:14 -0700
Subject: [R-SIG-Finance] [R] Does anybody know how to connect to SAS
	from within R?
In-Reply-To: <870747.38889.qm@web65403.mail.ac4.yahoo.com>
References: <b1f16d9d0909241736r16d78068gddcc6490c1a97c07@mail.gmail.com>
	<870747.38889.qm@web65403.mail.ac4.yahoo.com>
Message-ID: <b1f16d9d0909250622u13f9b49cu123a0f5314db2dd3@mail.gmail.com>

Yeah, I also would like to know what synergy can I get from combining
the power of R and SAS...

Maybe there are something that's particularly strong in R and
someother that's particularly strong in SAS?

Thanks!

On Thu, Sep 24, 2009 at 10:26 PM, Indrajit Sengupta
<indra_calisto at yahoo.com> wrote:
> Here's a good website on using R & SAS. I am not sure if this site or the book mentioned talks about connecting to SAS from R, but nevertheless its worth going through. I have used both - and really can't see much benefit other than transferring datasets.
>
> Regards,
> Indrajit
>
>
>
> ----- Original Message ----
> From: Michael <comtech.usa at gmail.com>
> To: r-help <R-help at stat.math.ethz.ch>
> Sent: Friday, September 25, 2009 6:06:58 AM
> Subject: [R] Does anybody know how to connect to SAS from within R?
>
> And what might be the benefit doing that?
>
> Thanks a lot!
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>


From windspeedo99 at gmail.com  Fri Sep 25 16:49:11 2009
From: windspeedo99 at gmail.com (Wind)
Date: Fri, 25 Sep 2009 22:49:11 +0800
Subject: [R-SIG-Finance] write.xts() and read.xts()
Message-ID: <d718c8210909250749j5cb6982fqd015ef412737c055@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090925/6676c492/attachment.pl>

From ggrothendieck at gmail.com  Fri Sep 25 16:54:04 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 25 Sep 2009 10:54:04 -0400
Subject: [R-SIG-Finance] write.xts() and read.xts()
In-Reply-To: <d718c8210909250749j5cb6982fqd015ef412737c055@mail.gmail.com>
References: <d718c8210909250749j5cb6982fqd015ef412737c055@mail.gmail.com>
Message-ID: <971536df0909250754o7ca23a8eodbeb4e22c8b0c80b@mail.gmail.com>

Try:

x <- as.xts(read.zoo(...))
write.zoo(as.zoo(x), ...)

On Fri, Sep 25, 2009 at 10:49 AM, Wind <windspeedo99 at gmail.com> wrote:
> There are write.zoo() and read.zoo() in the zoo package. ? But there are no
> write.xts() or read.xts() in the xts package.Since xts is so powerful
> manipulating historic prices, I wonder how do you save those xts. ? Write
> your own function like write.zoo for xts? ? Or just use write.zoo and
> read.zoo and convert to xts again? ?It seems that the attributes of xts
> ?could not be maintained in the zoo-like way.
> Thanks.
>
> Wind
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From brian at braverock.com  Fri Sep 25 16:58:09 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 25 Sep 2009 09:58:09 -0500
Subject: [R-SIG-Finance] write.xts() and read.xts()
In-Reply-To: <d718c8210909250749j5cb6982fqd015ef412737c055@mail.gmail.com>
References: <d718c8210909250749j5cb6982fqd015ef412737c055@mail.gmail.com>
Message-ID: <4ABCDA81.4000908@braverock.com>

Wind wrote:
> There are write.zoo() and read.zoo() in the zoo package.   But there are no
> write.xts() or read.xts() in the xts package.Since xts is so powerful
> manipulating historic prices, I wonder how do you save those xts.   Write
> your own function like write.zoo for xts?   Or just use write.zoo and
> read.zoo and convert to xts again?  It seems that the attributes of xts
>  could not be maintained in the zoo-like way
I typically use read.zoo to read CSV or similar data into R.  This is 
then trivially converted to xts.

Once in R, I typically save my data of all types as binary .Rdata files 
using "save".  I only write data to CSV files using write.csv, 
write.table, or write.zoo when I need to export it for use by others.  
The binary formats are much faster to load and use than constantly 
converting back and forth from text.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From jeff.a.ryan at gmail.com  Fri Sep 25 16:59:55 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 25 Sep 2009 09:59:55 -0500
Subject: [R-SIG-Finance] write.xts() and read.xts()
In-Reply-To: <971536df0909250754o7ca23a8eodbeb4e22c8b0c80b@mail.gmail.com>
References: <d718c8210909250749j5cb6982fqd015ef412737c055@mail.gmail.com>
	<971536df0909250754o7ca23a8eodbeb4e22c8b0c80b@mail.gmail.com>
Message-ID: <e8e755250909250759t314423d6kf029ce3fb7679d89@mail.gmail.com>

The one issue with text based storage is that attributes are difficult
to handle.  As attributes on xts objects (any objects) can be any
legal R object, it requires some form of serialization to record as
text.

This is doable with dput.  Aside from Gabor's suggestion, the only way
to save everything in an xts object is to save it in binary form via
save/.saveRDS.  This should be portable and fast, though not obviously
in text/human-readable form.

That said, I'll add write.xts and read.xts at some point in the near future.

Thanks,
Jeff

On Fri, Sep 25, 2009 at 9:54 AM, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> Try:
>
> x <- as.xts(read.zoo(...))
> write.zoo(as.zoo(x), ...)
>
> On Fri, Sep 25, 2009 at 10:49 AM, Wind <windspeedo99 at gmail.com> wrote:
>> There are write.zoo() and read.zoo() in the zoo package. ? But there are no
>> write.xts() or read.xts() in the xts package.Since xts is so powerful
>> manipulating historic prices, I wonder how do you save those xts. ? Write
>> your own function like write.zoo for xts? ? Or just use write.zoo and
>> read.zoo and convert to xts again? ?It seems that the attributes of xts
>> ?could not be maintained in the zoo-like way.
>> Thanks.
>>
>> Wind
>>
>> ? ? ? ?[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From windspeedo99 at gmail.com  Fri Sep 25 17:33:04 2009
From: windspeedo99 at gmail.com (Wind)
Date: Fri, 25 Sep 2009 23:33:04 +0800
Subject: [R-SIG-Finance] write.xts() and read.xts()
In-Reply-To: <e8e755250909250759t314423d6kf029ce3fb7679d89@mail.gmail.com>
References: <d718c8210909250749j5cb6982fqd015ef412737c055@mail.gmail.com>
	<971536df0909250754o7ca23a8eodbeb4e22c8b0c80b@mail.gmail.com>
	<e8e755250909250759t314423d6kf029ce3fb7679d89@mail.gmail.com>
Message-ID: <d718c8210909250833h4272028aq3e920c99f030f8c2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090925/59423334/attachment.pl>

From jeff.a.ryan at gmail.com  Fri Sep 25 17:41:35 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 25 Sep 2009 10:41:35 -0500
Subject: [R-SIG-Finance] write.xts() and read.xts()
In-Reply-To: <d718c8210909250833h4272028aq3e920c99f030f8c2@mail.gmail.com>
References: <d718c8210909250749j5cb6982fqd015ef412737c055@mail.gmail.com>
	<971536df0909250754o7ca23a8eodbeb4e22c8b0c80b@mail.gmail.com>
	<e8e755250909250759t314423d6kf029ce3fb7679d89@mail.gmail.com>
	<d718c8210909250833h4272028aq3e920c99f030f8c2@mail.gmail.com>
Message-ID: <e8e755250909250841l38ac4c52v4cb541a9e092e9fd@mail.gmail.com>

xts is always under development, and one major component is the
integration with on-disk persistent storage.

This will allow for many xts operations to be directly available to
disk-based data. That said, it is a solid 6 months away from
production code.

I'll of course keep the list posted to any developments that might be
of interest.  The svn logs on R-forge are also a good way to see what
is happening with the package.

If anyone would like to alpha/beta test the on-disk xts code, please
send me an email off list.

Thanks,
Jeff

On Fri, Sep 25, 2009 at 10:33 AM, Wind <windspeedo99 at gmail.com> wrote:
> So it seems that I should use different method for different scale of data:
> hundreds of millions of rows, ?kdb+ , column based database;
> equal to or less than millions of rows, save and xts, binary file;
> even less rows, xts and save.zoo, text file.
> If write.xts could write data as binary files as well as text files, it
> would be fantastic.
> Thanks for all.
> Wind
> On Fri, Sep 25, 2009 at 10:59 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>>
>> The one issue with text based storage is that attributes are difficult
>> to handle. ?As attributes on xts objects (any objects) can be any
>> legal R object, it requires some form of serialization to record as
>> text.
>>
>> This is doable with dput. ?Aside from Gabor's suggestion, the only way
>> to save everything in an xts object is to save it in binary form via
>> save/.saveRDS. ?This should be portable and fast, though not obviously
>> in text/human-readable form.
>>
>> That said, I'll add write.xts and read.xts at some point in the near
>> future.
>>
>> Thanks,
>> Jeff
>>
>> On Fri, Sep 25, 2009 at 9:54 AM, Gabor Grothendieck
>> <ggrothendieck at gmail.com> wrote:
>> > Try:
>> >
>> > x <- as.xts(read.zoo(...))
>> > write.zoo(as.zoo(x), ...)
>> >
>> > On Fri, Sep 25, 2009 at 10:49 AM, Wind <windspeedo99 at gmail.com> wrote:
>> >> There are write.zoo() and read.zoo() in the zoo package. ? But there
>> >> are no
>> >> write.xts() or read.xts() in the xts package.Since xts is so powerful
>> >> manipulating historic prices, I wonder how do you save those xts.
>> >> Write
>> >> your own function like write.zoo for xts? ? Or just use write.zoo and
>> >> read.zoo and convert to xts again? ?It seems that the attributes of xts
>> >> ?could not be maintained in the zoo-like way.
>> >> Thanks.
>> >>
>> >> Wind
>> >>
>> >> ? ? ? ?[[alternative HTML version deleted]]
>> >>
>> >> _______________________________________________
>> >> R-SIG-Finance at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> >> -- Subscriber-posting only.
>> >> -- If you want to post, subscribe first.
>> >>
>> >
>> > _______________________________________________
>> > R-SIG-Finance at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> > -- Subscriber-posting only.
>> > -- If you want to post, subscribe first.
>> >
>>
>>
>>
>> --
>> Jeffrey Ryan
>> jeffrey.ryan at insightalgo.com
>>
>> ia: insight algorithmics
>> www.insightalgo.com
>
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From wuertz at itp.phys.ethz.ch  Sun Sep 27 15:24:04 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 27 Sep 2009 15:24:04 +0200
Subject: [R-SIG-Finance] timeSeries:: bizarre rbind behavior
	with	colnames
In-Reply-To: <e8e755250909230859u387d1790l7775fe6c11e378de@mail.gmail.com>
References: <6fdf6d430909230843p674da615uc7e886855d211985@mail.gmail.com>
	<e8e755250909230859u387d1790l7775fe6c11e378de@mail.gmail.com>
Message-ID: <4ABF6774.8090201@itp.phys.ethz.ch>

Jeff Ryan wrote:
> Nicolas,
>
> I don't think this is a list question per se, as you should really
> contact the maintainer.
>
> While I'll agree the design is odd, if you're insisting on using
> timeSeries, and you find a problem, it would be more useful to all if
> you proposed a patch.
>
> Complaints/demands for explanations/etc aren't productive, and
> effectively show no consideration for the time and effort that
> _contributors_ make to R.
>   

*****************
> If you're looking for a better time-series solution than a matrix as
> an interim representation, I would suggest xts.  Or zoo...
>   

Dear Jeff,

I don't like to comment this sentence (and also not the previous ones 
before).

Documentation is written to be read!  The users can
have a look into the free FAQ eBook:

"A Discussion of Time Series Objects for R in Finance"

Download it for free from

--- http://www.rmetrics.org/ebook.htm ---

There you will find also information on the usage of c,
cbind, rbind, and merge, and how these functions work
and how they should be used.

In this eBook many frequently asked questions are answered
on an objective level. Everybody is invited to contribute to
this eBook and to improve its content. Suggestions are
welcome.


Diethelm


**********************
> Jeff
>
> On Wed, Sep 23, 2009 at 10:43 AM, Nicolas Chapados
> <nicolas.chapados at gmail.com> wrote:
>   
>> Dear list,
>>
>> Looking at the code for timeSeries::rbind, it appears that it insists
>> on concatenating the names of the two series it's trying to bind, even
>> if they match exactly, thereby creating a longer series name.  This is
>> in sharp contrast to, e.g. rbind.data.frame.  This creates very
>> badly-behaved column names (i.e. horrendously-long) in the case where
>> one attempts to construct a timeSeries incrementally.
>>
>> For example:
>>
>>     
>>> rbind(timeSeries(1.0, timeDate("1996-01-01"), units="Level"), timeSeries(2.0, timeDate("2009-01-01"), units="Level"))
>>>       
>> GMT
>>           Level_Level
>> 1996-01-01           1
>> 2009-01-01           2
>>
>>
>> Can someone explain the design rationale for choosing to concatenate
>> the column names in such a manner?  In the short term, I resolved to
>> incrementally constructing a matrix, and at the very end building a
>> timeSeries, but I would really prefer to keep timeSeries all along...
>>
>> Many thanks for any help!
>> + Nicolas Chapados
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>     
>
>
>
>


From wuertz at itp.phys.ethz.ch  Sun Sep 27 15:40:36 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 27 Sep 2009 15:40:36 +0200
Subject: [R-SIG-Finance] timeSeries:: bizarre rbind behavior with
	colnames
In-Reply-To: <6fdf6d430909230843p674da615uc7e886855d211985@mail.gmail.com>
References: <6fdf6d430909230843p674da615uc7e886855d211985@mail.gmail.com>
Message-ID: <4ABF6B54.7010303@itp.phys.ethz.ch>

Nicolas Chapados wrote:
> Dear list,
>
> Looking at the code for timeSeries::rbind, it appears that it insists
> on concatenating the names of the two series it's trying to bind, even
> if they match exactly, thereby creating a longer series name.  This is
> in sharp contrast to, e.g. rbind.data.frame.  This creates very
> badly-behaved column names (i.e. horrendously-long) in the case where
> one attempts to construct a timeSeries incrementally.
>   
NO, you don't have understood properly the functions c, cbind, rbind, 
and merge for timeSeries objects and data.frames !!!
> For example:
>
>   
>> rbind(timeSeries(1.0, timeDate("1996-01-01"), units="Level"), timeSeries(2.0, timeDate("2009-01-01"), units="Level"))
>>     
> GMT
>            Level_Level
> 1996-01-01           1
> 2009-01-01           2
>   
That is exactly what you should get back, when you bind two univariate 
time series row by row. You bind two series with the same name and this 
is expressed by the column name Level_Level.

I think what you like to do is to merge the two time series and not to 
bind rowwise, or?

merge(timeSeries(1.0, timeDate("1996-01-01"), units="Level"), timeSeries(2.0, timeDate("2009-01-01"), units="Level"))

then you get:

GMT
           Level
1996-01-01     1
2009-01-01     2

what you possibly would have been expected! merge assumes that the source of the data is really exactly the same.

Look to all the other examples presented in the ebook concernd with the concatenation of time series:

"A Discussion of Time Series Objects for R in Finance"

Download it for free from

--- http://www.rmetrics.org/ebook.htm ---

to learn more about the usage of c, cbind, rbind, and merge.

Diethelm






>
> Can someone explain the design rationale for choosing to concatenate
> the column names in such a manner?  In the short term, I resolved to
> incrementally constructing a matrix, and at the very end building a
> timeSeries, but I would really prefer to keep timeSeries all along...
>
> Many thanks for any help!
> + Nicolas Chapados
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From ltorgo at inescporto.pt  Sun Sep 27 23:35:26 2009
From: ltorgo at inescporto.pt (Luis Torgo)
Date: Sun, 27 Sep 2009 22:35:26 +0100
Subject: [R-SIG-Finance] Problem in getSymbols.mysql()
Message-ID: <4ABFDA9E.2040109@dcc.fc.up.pt>

Dear All,

I've noted a small problem in function getSymbols.mysql().
My setup:
   Package    Version
"quantmod"   "0.3-11"

The problem has to do with the supposed flexibility in specifying the 
field names of the table on the MYSQL side. This flexibility is  to be 
given by the parameter "db.fields". However, the code in the function is 
then hardwired to a specific field name ("date"), in the following part:
...
query <- paste("SELECT ", paste(db.fields, collapse = ","),
            " FROM ", Symbols[[i]], " ORDER BY date")
...

This leads to errors when you do not have a table field named "date".

So, either the help page of the function is changed to mention that it 
is necessary to always have a "date" field and the "db.fields" parameter 
only refers to the "other" fields, or I think it is necessary to have a 
separate parameter for specifying which is the time tag field in the 
table and then do something like:
...
query <- paste("SELECT ", paste(c(time.field,db.fields), collapse = ","),
            " FROM ", Symbols[[i]], " ORDER BY ",time.field)
...

I think yet another possibility is to eliminate the ORDER BY clause, 
which maybe it is not that necessary, but this may be inadequate for 
some uses...

Thank you,
Luis Torgo

-- 
Luis Torgo
   FC/LIAAD - INESC Porto, LA    Phone : (+351) 22 339 20 93
   University of Porto           Fax   : (+351) 22 339 20 99
   R. de Ceuta, 118, 6o          email : ltorgo at liaad.up.pt
   4050-190 PORTO - PORTUGAL     WWW   : http://www.liaad.up.pt/~ltorgo


From sprohl at na.uni-tuebingen.de  Mon Sep 28 08:32:17 2009
From: sprohl at na.uni-tuebingen.de (sprohl at na.uni-tuebingen.de)
Date: Mon, 28 Sep 2009 08:32:17 +0200 (CEST)
Subject: [R-SIG-Finance] termstrc: bond yields
Message-ID: <2276.130.60.242.127.1254119537.squirrel@nastclv4.mathematik.uni-tuebingen.de>

Hello,
by using termstrc package to compute bond yields using example considered
in "Zero-coupon yield curve estimation with the Package termstrc", I run
into following problems:

data(corpbonds)

group <- c("AAA", "AA+", "AA")

cashflows<-create_cashflows_matrix(group, include_price = FALSE)
Fehler in as.vector(x, mode) : ung?ltiges Argument 'mode'
Zus?tzlich: Warning message:
$ operator is deprecated for atomic vectors, returning NULL in:
group$CASHFLOWS

cashflows<-create_maturities_matrix(group, include_price = FALSE)
Fehler in as.vector(x, mode) : ung?ltiges Argument 'mode'
Zus?tzlich: Warning message:
$ operator is deprecated for atomic vectors, returning NULL in:
group$CASHFLOWS

# compute bond yields
bond_yields(cashflows, m, searchint = c(-1, 1), tol = 1e-10)
Fehler in bond_yields(cashflows, m, searchint = c(-1, 1), tol = 1e-10) :
        unbenutzte(s) Argument(e) (searchint = c(-1, 1))

Does anybody know how to solve this problem?
Best regards,
Sven


From josef.hayden at wiwi.uni-regensburg.de  Mon Sep 28 11:05:29 2009
From: josef.hayden at wiwi.uni-regensburg.de (Josef Hayden)
Date: Mon, 28 Sep 2009 11:05:29 +0200
Subject: [R-SIG-Finance] termstrc: bond yields
Message-ID: <4AC07C59.8000402@wiwi.uni-regensburg.de>


Hello

The function "create_cashflows_matrix" requires the cashflows of one 
group element (rating class, country etc.).
Try:

library(termstrc)
data(corpbonds)
create_cashflows_matrix(corpbonds[[1]])



For calculations for the complete group use lapply or mapply:
group <- c("AAA", "AA+", "AA")

lapply(corpbonds[group], create_cashflows_matrix)

The same holds for the "create_maturities_matrix" function.

Regards,
Josef

>
>> *Von: *sprohl at na.uni-tuebingen.de <mailto:sprohl at na.uni-tuebingen.de>
>> *Datum: *28 September 2009 08:32:17 GMT+02:00
>> *An: *r-sig-finance at stat.math.ethz.ch 
>> <mailto:r-sig-finance at stat.math.ethz.ch>
>> *Betreff: **[R-SIG-Finance] termstrc: bond yields*
>>
>> Hello,
>> by using termstrc package to compute bond yields using example 
>> considered
>> in "Zero-coupon yield curve estimation with the Package termstrc", I run
>> into following problems:
>>
>> data(corpbonds)
>>
>> group <- c("AAA", "AA+", "AA")
>>
>> cashflows<-create_cashflows_matrix(group, include_price = FALSE)
>> Fehler in as.vector(x, mode) : ung?ltiges Argument 'mode'
>> Zus?tzlich: Warning message:
>> $ operator is deprecated for atomic vectors, returning NULL in:
>> group$CASHFLOWS
>>
>> cashflows<-create_maturities_matrix(group, include_price = FALSE)
>> Fehler in as.vector(x, mode) : ung?ltiges Argument 'mode'
>> Zus?tzlich: Warning message:
>> $ operator is deprecated for atomic vectors, returning NULL in:
>> group$CASHFLOWS
>>
>> # compute bond yields
>> bond_yields(cashflows, m, searchint = c(-1, 1), tol = 1e-10)
>> Fehler in bond_yields(cashflows, m, searchint = c(-1, 1), tol = 1e-10) :
>>        unbenutzte(s) Argument(e) (searchint = c(-1, 1))
>>
>> Does anybody know how to solve this problem?
>> Best regards,
>> Sven
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch 
>> <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From cedrick at cedrickjohnson.com  Mon Sep 28 16:34:03 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Mon, 28 Sep 2009 10:34:03 -0400
Subject: [R-SIG-Finance] Possible enhancement to volatility in TTR
In-Reply-To: <4ABA9629.5020703@cedrickjohnson.com>
References: <4ABA9629.5020703@cedrickjohnson.com>
Message-ID: <4AC0C95B.7030604@cedrickjohnson.com>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090928/079e9696/attachment.html>

From jessevel at andrew.cmu.edu  Mon Sep 28 17:09:10 2009
From: jessevel at andrew.cmu.edu (Jesse Velez)
Date: Mon, 28 Sep 2009 11:09:10 -0400
Subject: [R-SIG-Finance] Static Portfolio Optimization
Message-ID: <84b08d7f0909280809y73a65acep8b383108f3b33af7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090928/c0ad8d2f/attachment.pl>

From brian at braverock.com  Mon Sep 28 17:13:08 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 28 Sep 2009 10:13:08 -0500
Subject: [R-SIG-Finance] Static Portfolio Optimization
In-Reply-To: <84b08d7f0909280809y73a65acep8b383108f3b33af7@mail.gmail.com>
References: <84b08d7f0909280809y73a65acep8b383108f3b33af7@mail.gmail.com>
Message-ID: <4AC0D284.2050904@braverock.com>

Jesse Velez wrote:
> Is there any function or example in R or Rmetrics of static portfolio
> optimization, where I have a vector of expected returns for N assets and a
> expected covariance matrix of said N assets all at a fixed time (say
> generated from a MFM risk and return model).
>
> fPortfolio, Portfolio, portfolio.optim appear to all require time series of
> returns to generate the expected return and historical covariance matrix for
> use in creating weights.
>
> Ideally, I hope to find an example that allows easily allows Long/Short
> weights to make the portfolio  market neutral (i.e. Summation of Weights
> =0).
>   
All the implementations of Markowitz style mean/variance optmization use 
quadprog in R.

Plenty of information on the list archives from before all these 
packages existed about using quadprog for optimization.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From wuertz at itp.phys.ethz.ch  Mon Sep 28 17:33:20 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon, 28 Sep 2009 17:33:20 +0200
Subject: [R-SIG-Finance] discussion of time series objects in R/Rmetrics
Message-ID: <4AC0D740.5020605@itp.phys.ethz.ch>

Dear R/Rmetrics Users,

We like to announce an ebook "Discussion of Time Series Objects for R in 
Finance"
which may be of interest  (not only for financial engineers or quants).

Download it for free from

http://www.rmetrics.org/ebook.htm

Please feel free to send us your comments, suggestions, improvements or 
any other
material which you consider of interest for the next edition of the ebook.

Thanks
Diethelm Wuertz


From markknecht at gmail.com  Mon Sep 28 18:31:42 2009
From: markknecht at gmail.com (Mark Knecht)
Date: Mon, 28 Sep 2009 09:31:42 -0700
Subject: [R-SIG-Finance] discussion of time series objects in R/Rmetrics
In-Reply-To: <4AC0D740.5020605@itp.phys.ethz.ch>
References: <4AC0D740.5020605@itp.phys.ethz.ch>
Message-ID: <5bdc1c8b0909280931l2acf6638rfd1653126d241c85@mail.gmail.com>

Thank you for making this available. Clearly it will take some time to
go through but as a relative newbie I'm always looking for good
information well presented which this seems to be.

Cheers,
Mark

On Mon, Sep 28, 2009 at 8:33 AM, Diethelm Wuertz
<wuertz at itp.phys.ethz.ch> wrote:
> Dear R/Rmetrics Users,
>
> We like to announce an ebook "Discussion of Time Series Objects for R in
> Finance"
> which may be of interest ?(not only for financial engineers or quants).
>
> Download it for free from
>
> http://www.rmetrics.org/ebook.htm
>
> Please feel free to send us your comments, suggestions, improvements or any
> other
> material which you consider of interest for the next edition of the ebook.
>
> Thanks
> Diethelm Wuertz
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From josh.m.ulrich at gmail.com  Mon Sep 28 19:08:38 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Mon, 28 Sep 2009 12:08:38 -0500
Subject: [R-SIG-Finance] Possible enhancement to volatility in TTR
In-Reply-To: <4AC0C95B.7030604@cedrickjohnson.com>
References: <4ABA9629.5020703@cedrickjohnson.com>
	<4AC0C95B.7030604@cedrickjohnson.com>
Message-ID: <8cca69990909281008k22c7a1aeg79e4e124c62488bd@mail.gmail.com>

Hi Cedrick,

Sorry for the slow response.  How about this quick change to
TTR::volatility() and the code below?  I agree that OHLC should be
allowed to be univariate in the case of calc="close".  I'm hesitant to
add the other logic though, since the same results can be achieved
with an external call to apply() -- see below.

volatility3 <-
function (OHLC, n = 10, N = 260, calc = "close", ...)
{
<snip>
    if (calc == "close") {
        # Add univariate case for calc="close"
        if (NCOL(OHLC) == 1) {
            r <- ROC(OHLC, 1, ...)
        } else {
            r <- ROC(OHLC[, 4], 1, ...)
        }
        rBar <- runSum(r, n - 1)/(n - 1)
        s <- sqrt(N/(n - 2) * runSum((r - rBar)^2, n - 1))
    }
<snip>
    reclass(s, OHLC)
}

Then you can get pretty volatilities via a command like:
> require(xts)
> data(sample_matrix)
> # Pretty volatility (note "v" will always be a matrix, even if OHLC is xts)
> v <- round(apply(sample_matrix, 2, volatility3)*100,0)
> tail(v)
           Open High Low Close
2007-06-25    6    5   5     6
2007-06-26    6    6   6     6
2007-06-27    6    6   6     6
2007-06-28    6    5   6     5
2007-06-29    5    5   6     5
2007-06-30    5    5   6     5
> tail( volatility3(sample_matrix)*100 )
               [,1]
2007-06-25 5.656552
2007-06-26 5.729933
2007-06-27 5.732217
2007-06-28 4.771530
2007-06-29 4.764216
2007-06-30 4.818589

HTH,
Josh
--
http://www.fosstrading.com



On Mon, Sep 28, 2009 at 9:34 AM, Cedrick Johnson
<cedrick at cedrickjohnson.com> wrote:
> As promised, here's my attempt at incorporating it into the original
> volatility() function (for the sake of testing, I've renamed the new
> function volatility2(..). Thanks to Murali for the suggestion to use
> apply.rolling, I've added in a few new variables in the function definition.
> So far it seems to work well here.
>
> volatility2 <- function (OHLC, n = 10, N = 260, calc = "close",
> roundValue=FALSE, roundNum=2, normalize=FALSE, nonOHLC=FALSE, ...)
> {
> ??? if (nonOHLC == FALSE) {
>
> ??? ??? OHLC <- try.xts(OHLC, error = as.matrix)
> ??? ??? calc <- match.arg(calc, c("close", "garman.klass", "parkinson",
> ??? ??????? "rogers.satchell"))
> ??? ??? if (calc == "close") {
> ??? ??? ?r <- ROC(OHLC[, 4], 1, ...)
> ??? ??????? rBar <- runSum(r, n - 1)/(n - 1)
> ?????? ??? ?s <- sqrt(N/(n - 2) * runSum((r - rBar)^2, n - 1))
> ??? ??? }
> ??? ??? if (calc == "garman.klass") {
> ??? ??????? s <- sqrt(N/n * runSum(0.5 * log(OHLC[, 2]/OHLC[, 3])^2 -
> ??? ??????????? (2 * log(2) - 1) * log(OHLC[, 4]/OHLC[, 1])^2, n))
> ??? ??? }
> ??? ??? if (calc == "parkinson") {
> ??? ??????? s <- sqrt(N/(4 * n * log(2)) * runSum(log(OHLC[, 2]/OHLC[,
> ??? ??????????? 3])^2, n))
> ??? ??? }
> ??? ??? if (calc == "rogers.satchell") {
> ??? ??????? s <- sqrt(N/n * runSum(log(OHLC[, 2]/OHLC[, 4]) * log(OHLC[,
> ??? ??????????? 2]/OHLC[, 1]) + log(OHLC[, 3]/OHLC[, 4]) * log(OHLC[,
> ??? ??????????? 3]/OHLC[, 1]), n))
> ??? ??? }
> ??? } else {
> ??? #still using the OHLC object, since this is indeed a matrix
> ??? # diff(log(OHLC)) provides the same output as ROC(x,1) so we should be
> goog. Thanks to Murali Menon
> ??? # for the suggested use of apply.rolling
> ??? s <- apply.rolling(R = diff(log(OHLC)), width=n, FUN="sd", na.pad=FALSE)
> * sqrt(N)
> ??? }
>
> ??? if (roundValue == TRUE) {
> ??? s <- round(s, roundNum)
> ??? }
> ??? if (normalize == TRUE) {
> ??? s <- s * 100
> ??? }
>
> ??? reclass(s, OHLC)
> }
>
>
> Cedrick Johnson wrote:
>
> Howdy-
>
> I came up with some code that has served me well, perhaps if anyone runs
> into the problem of needing to calculate historical volatility(close) on a
> non-OHLC series (or a bunch of "univariate" series stored in a matrix):
>
> ?????????? US2Y US5Y US10Y US30Y UKG5 UKG10 USSP2 USSP5 USSP10 USSP30
> 2009-01-02 0.88 1.72? 2.46? 2.83 2.67? 3.36? 1.50? 2.17?? 2.61?? 2.78
> 2009-01-05 0.78 1.67? 2.49? 3.00 2.75? 3.47? 1.60? 2.31?? 2.82?? 3.03
> 2009-01-06 0.80 1.68? 2.51? 3.04 2.79? 3.57? 1.55? 2.34?? 2.88?? 3.18
> 2009-01-07 0.82 1.66? 2.52? 3.05 2.80? 3.60? 1.44? 2.16?? 2.69?? 2.99
> ....
> blahblah..
>
> I adapted the code to use 'close' from Josh Ulrich's volatility function
> (since all of these are closing prices), and here's my result:
>
> volatility.matrix <- function(Matrix, n = 10, N = 252, pretty=TRUE, ...) {
> ??? volm <- Matrix
> ??? for(i in 1:ncol(Matrix)) {
> ??? ??? r <- ROC(Matrix[,i], 1,...)
> ??? ??? rBar <- runSum(r, n -1)/(n - 1)
> ??? ??? s <- sqrt(N/(n - 2) * runSum((r - rBar)^2, n - 1))
> ??? ??? #because i like to make it pretty
> ??? ??? volm[,i] <- round(s,2) * 100
> ??? }
> ??? volm <- na.omit(volm)
> ??? reclass(volm, Matrix)
> }
>
>
> The Result:
>> volatility.matrix(Yields, n=30)
> ?????????? US2Y US5Y US10Y US30Y UKG5 UKG10 USSP2 USSP5 USSP10 USSP30
> 2009-03-24? 116? 110??? 73??? 45?? 54??? 57??? 67??? 65???? 60???? 51
> 2009-03-25? 115? 112??? 75??? 46?? 51??? 55??? 67??? 65???? 60???? 51
> 2009-03-26? 114? 110??? 74??? 44?? 50??? 55??? 65??? 64???? 60???? 50
> 2009-03-27? 114? 110??? 74??? 44?? 50??? 55??? 67??? 64???? 60???? 51
> 2009-03-30? 111? 104??? 68??? 39?? 52??? 55??? 67??? 63???? 58???? 51
> 2009-03-31? 107? 100??? 68??? 39?? 51??? 55??? 67??? 63???? 58???? 50
> 2009-04-01? 107? 100??? 67??? 38?? 51??? 54??? 67??? 62???? 57???? 49
> 2009-04-02? 107? 100??? 67??? 37?? 58??? 57??? 65??? 61???? 56???? 48
> ......
>
> What I get is a nice matrix of volatilities in a pretty format (rounded,
> etc.)
>
> I'll give it a shot tonight trying to implement it into the current
> volatility function, however I'm sure the code that I have here could be
> done better.
>
> Regards,
> c
>
>
> ________________________________
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From etheber at gmx.de  Mon Sep 28 20:29:25 2009
From: etheber at gmx.de (Thomas Etheber)
Date: Mon, 28 Sep 2009 20:29:25 +0200
Subject: [R-SIG-Finance] Static Portfolio Optimization
In-Reply-To: <4AC0D284.2050904@braverock.com>
References: <84b08d7f0909280809y73a65acep8b383108f3b33af7@mail.gmail.com>
	<4AC0D284.2050904@braverock.com>
Message-ID: <4AC10085.4000105@gmx.de>

Brian G. Peterson wrote:
> Jesse Velez wrote:
>> Is there any function or example in R or Rmetrics of static portfolio
>> optimization, where I have a vector of expected returns for N assets
>> and a
>> expected covariance matrix of said N assets all at a fixed time (say
>> generated from a MFM risk and return model).
>>
>> fPortfolio, Portfolio, portfolio.optim appear to all require time
>> series of
>> returns to generate the expected return and historical covariance
>> matrix for
>> use in creating weights.
>>
>> Ideally, I hope to find an example that allows easily allows Long/Short
>> weights to make the portfolio  market neutral (i.e. Summation of Weights
>> =0).
>>   
> All the implementations of Markowitz style mean/variance optmization
> use quadprog in R.
>
> Plenty of information on the list archives from before all these
> packages existed about using quadprog for optimization.
>
> Regards,
>
>  - Brian
>

Hi there,

I also had the problem with fixed parameter inputs some time ago.
Implementing methods to perform this tasks would certainly be a nice
improvement of the library (as would be some help/error messages if the
covariance matrix is not positive semidefinite).  
Although Brian's comment is helpful as usual, using basic quadprog
sounds like reinventing the wheel, but might nevertheless be needed to
solve your second task of a market-neutral portfolio.

In order to use prespecified estimates as inputs I helped myself with
overwriting some of the methods. It's not a nice solution, but it worked
for me. You will find the methods attached below.
I didn't check the code again, but I think it should work. Please note,
some other methods of Rmetrics and fPortfolio might rely on the
timeseries objects and might not work properly.

Hth
Thomas

>>>
require(MBESS)
require(fPortfolio)
rm(list=ls())
spec <- portfolioSpec()
constraints <- NULL

portfolioData <- function (data, spec = portfolioSpec())
{  
    ans = NULL
    if(class(data) == "timeSeries") {
       data = sort(data)
       nAssets = dim(data)[2]
       statistics = portfolioStatistics(data, spec)
       tailRisk = spec at model$tailRisk
       ans <- new("fPFOLIODATA", data = list(series = data, nAssets =
nAssets),
           statistics = statistics, tailRisk = tailRisk)
    }
    if(class(data) == "list") {
      statistics = list(mu = data$mu, Sigma = data$Sigma )
      attr(statistics, "estimator") = spec at model$estimator
      ans <- new("fPFOLIODATA", data = list( nAssets = length(data$mu)
), statistics = statistics, tailRisk = list() )
    }
    ans
}


####################################################################################

.efficientConstrainedMVPortfolio <- function (data, spec, constraints)
{
    if (!inherits(data, "fPFOLIODATA"))
        data = portfolioData(data, spec)
    mu = getMu(data)
    Sigma = getSigma(data)
    nAssets = getNumberOfAssets(data)
    targetAlpha = getTargetAlpha(spec)
    solver = getSolver(spec)
    stopifnot(solver == "quadprog" | solver == "Rdonlp2")
    if (solver == "quadprog") {
        portfolio = solveRQuadprog(data, spec, constraints)
    }
    else if (solver == "Rdonlp2") {
        portfolio = solveRDonlp2(data, spec, constraints)
    }
    weights = portfolio$weights
    attr(weights, "status") <- portfolio$status
    names(weights) = names(mu)
    targetReturn = matrix(as.numeric(mu %*% weights), nrow = 1)
    colnames(targetReturn) <- getEstimator(spec)[1]
    covTargetRisk = sqrt(as.numeric(weights %*% Sigma %*% weights))
#   x = getSeries(data)@Data %*% weights
#   VaR = quantile(x, targetAlpha, type = 1)
#   CVaR = VaR - 0.5 * mean(((VaR - x) + abs(VaR - x)))/targetAlpha
#   targetRisk = matrix(c(covTargetRisk, CVaR, VaR), nrow = 1)
#   colnames(targetRisk) <- c("cov", paste(c("CVaR.", "VaR."),
#   targetAlpha * 100, "%", sep = ""))
    targetRisk = matrix(c(covTargetRisk), nrow = 1)
    ## is needed to use the plotting functions....
    targetRisk = matrix(c(covTargetRisk, covTargetRisk ), nrow = 1)
    colnames(targetRisk) <- c( "cov", "dummy" )
    new("fPORTFOLIO", call = match.call(), data = list(data = data),
        spec = list(spec = spec), constraints = as.character(constraints),
        portfolio = list(weights = weights, targetReturn = targetReturn,
            targetRisk = targetRisk, targetAlpha = targetAlpha,
            status = portfolio$status), title = paste("Constrained MV
Portfolio - Solver:",
            solver), description = .description())
}

####################################################################################

.minvarianceConstrainedMVPortfolio <- function (data, spec, constraints)
{
    if (!inherits(data, "fPFOLIODATA"))
        data = portfolioData(data, spec)
    mu = getMu(data)
    Sigma = getSigma(data)
    nAssets = getNumberOfAssets(data)
    targetAlpha = getTargetAlpha(spec)
    .minVariancePortfolioFun = function(x, data, spec, constraints) {
        spec at portfolio$targetReturn = x
        ans = .efficientConstrainedMVPortfolio(data = data, spec = spec,
            constraints = constraints)
        f = getTargetRisk(ans)[1]
        attr(f, "targetReturn") <- getTargetReturn(ans)
        attr(f, "targetRisk") <- getTargetRisk(ans)[1]
        attr(f, "weights") <- getWeights(ans)
        f
    }
    minVar = optimize(.minVariancePortfolioFun, interval = range(mu),
        data = data, spec = spec, constraints = constraints,
        tol = .Machine$double.eps^0.5)
    weights = attr(minVar$objective, "weights")
    names(weights) = names(mu)
    targetReturn = spec at portfolio$targetReturn =
as.numeric(attr(minVar$objective,
        "targetReturn"))
    targetReturn = matrix(targetReturn, nrow = 1)
    colnames(targetReturn) <- spec at model$estimator[1]
    covTargetRisk = as.numeric(attr(minVar$objective, "targetRisk"))
    # x = getSeries(data)@Data %*% weights
    # VaR = quantile(x, targetAlpha, type = 1)
    # CVaR = VaR - 0.5 * mean(((VaR - x) + abs(VaR - x)))/targetAlpha
    #targetRisk = matrix(c(covTargetRisk, CVaR, VaR), nrow = 1)
    #colnames(targetRisk) <- c("cov", paste(c("CVaR.", "VaR."),
    targetRisk = matrix(c(covTargetRisk), nrow = 1)
    ## is needed to use the plotting functions....
    targetRisk = matrix(c(covTargetRisk, covTargetRisk ), nrow = 1)
    colnames(targetRisk) <- c( "cov", "dummy" )
    new("fPORTFOLIO", call = match.call(), data = list(data = data),
        spec = list(spec = spec), constraints = as.character(constraints),
        portfolio = list(weights = weights, targetReturn = targetReturn,
            targetRisk = targetRisk, targetAlpha = targetAlpha,
            status = 0), title = "Minimum Variance Portfolio",
        description = .description())
}

show.fPORTFOLIO <- function (object)
{
    cat("\nTitle:\n ")
    cat(getTitle(object), "\n")
    cat("\nCall:\n ")
    print.default(getCall(object))
    cat("\nPortfolio Weight(s):\n")
    weights = round(getWeights(object), digits = 4)
    if (length(weights) == 1) {
        cat(" ", weights, "\n")
    }
    else {
        print.table(weights)
    }
    cat("\nRiskBudget(s):\n")
    riskBudgets = round(getCovRiskBudgets(object), digits = 4)
    if (length(riskBudgets) == 1) {
        cat(" ", riskBudgets, "\n")
    }
    else {
        print.table(riskBudgets)
    }
    if (FALSE) {
        if (!is.na(getTailRiskBudgets(object))) {
            cat("\nRiskBudget(s):\n")
            riskBudgets = round(getTailRiskBudgets(object), digits = 4)
            if (length(riskBudgets) == 1) {
                cat(" ", riskBudgets, "\n")
            }
            else {
                print.table(riskBudgets)
            }
        }
    }
    targetReturn = object at portfolio$targetReturn
    targetRisk = object at portfolio$targetRisk
    spec = getSpec(object)
    cat("\nTarget Risk(s) and Return(s):\n")
    if (is.null(dim(targetReturn))) {
        targetReturn = matrix(targetReturn, nrow = 1)
        colnames(targetReturn) = getEstimator(spec)[1]
    }
    if (is.null(dim(targetRisk))) {
        targetRisk = matrix(targetRisk, nrow = length(targetRisk) )
        colnames(targetRisk) = getEstimator(spec)[2]
    }
    target = cbind(targetReturn, targetRisk)
    colnames(target) = c(colnames(targetReturn), colnames(targetRisk) )
    if (nrow(target) == 1) {
        print(target[1, ])
    }
    else {
        print(target)
    }
    cat("\nDescription:\n ")
    cat(getDescription(object), "\n")
    invisible(object)
}

setMethod("show", "fPORTFOLIO", show.fPORTFOLIO)

####################################################################################

.portfolioConstrainedMVFrontier <- function (data, spec, constraints)
{
    if (!inherits(data, "fPFOLIODATA"))
        data = portfolioData(data, spec)
    mu = getMu(data)
    Sigma = getSigma(data)
    nAssets = getNumberOfAssets(data)
    targetAlpha = getTargetAlpha(spec)
    nFrontierPoints = getNFrontierPoints(spec)
    targetReturn = targetRisk = targetWeights = error = NULL
    Spec = spec
    solver = spec at solver$solver
    Spec at portfolio$weights = rep(1/nAssets, nAssets)
    k = 0
    solverType = spec at solver$solver
    status = NULL
    for (nTargetReturn in seq(min(mu), max(mu), length = nFrontierPoints)) {
        k = k + 1
        setTargetReturn(Spec) <- nTargetReturn
        nextPortfolio = .efficientConstrainedMVPortfolio(data = data,
            spec = Spec, constraints = constraints)
        Spec at portfolio$weights = nextPortfolio at portfolio$weights
        targetReturn = rbind(targetReturn,
nextPortfolio at portfolio$targetReturn)
        targetRisk = rbind(targetRisk, nextPortfolio at portfolio$targetRisk)
        nextWeights = nextPortfolio at portfolio$weights
        names(nextWeights) = names(mu)
        status = c(status, nextPortfolio at portfolio$status)
        targetWeights = rbind(targetWeights, t(nextWeights))
    }
    Index = (1:length(status))[status == 0]
    weights = targetWeights
    colnames(weights) = names(mu)
    weights = weights[Index, ]
    DIM = dim(targetReturn)[2]
    targetReturn = targetReturn[Index, ]
    targetReturn = matrix(targetReturn, ncol = DIM)
    colnames(targetReturn) = getEstimator(spec)[1]
    targetRisk = targetRisk[Index, ]
    new("fPORTFOLIO", call = match.call(), data = list(data = data),
        spec = list(spec = spec), constraints = as.character(constraints),
        portfolio = list(weights = weights, targetReturn = targetReturn,
            targetRisk = targetRisk, targetAlpha = targetAlpha,
            status = status), title = "Constrained MV Frontier",
        description = .description())
}

####################################################################################

# You should be able to specify the data in this form:
mu <- c( 0.1, 0.08, 0.065)
sigma <- c( 0.18, 0.12, 0.09 )

correlationMatrix <- rbind( c( 1, 0.8, 0.9 ),
                                              c( 0.8, 1, 0.75),
                                              c( 0.9, 0.75, 1) )

covarianceMatrix <- cor2cov(correlationMatrix, sigma )


data = list( mu = mu, Sigma = covarianceMatrix )

# And then do the optimisation
frontier <- portfolioFrontier(data, spec = spec, constraints )


From luke.gower at cba.com.au  Tue Sep 29 02:12:08 2009
From: luke.gower at cba.com.au (Gower, Luke)
Date: Tue, 29 Sep 2009 10:12:08 +1000
Subject: [R-SIG-Finance] index tracking
Message-ID: <A3B7C29F9599CC4BBF25F9830FA0138D1BFCFDD832@VAUNSW137.au.cbainet.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090929/738ebc4d/attachment.pl>

From Zeno.Adams at ebs.edu  Tue Sep 29 08:59:02 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Tue, 29 Sep 2009 08:59:02 +0200
Subject: [R-SIG-Finance] index tracking
In-Reply-To: <A3B7C29F9599CC4BBF25F9830FA0138D1BFCFDD832@VAUNSW137.au.cbainet.com>
References: <A3B7C29F9599CC4BBF25F9830FA0138D1BFCFDD832@VAUNSW137.au.cbainet.com>
Message-ID: <9064522880125945B98983BBAECBA1CC985604@exchsrv001.ebs.local>



>>I'm after suggestions for the design of simple index-tracking models.
I >>have access to good clean data, and I am trying to cook up
portfolios >>which replicate indices with a minimum number of long
positions when >>rebalancing is done daily.


I think a nice way to replicate an index using only a few long positions
is to use equities that are cointegrated with the index. An example is
given in Carol Alexander's (2001) book: "Market Models. A Guide to
Financial Data Analysis" Ch.12. This can be done in R using Paff's urca
package. You may also want to look at his book "Analysis of Integrated
and Cointegrated Time Series with R". If done correctly, you don't even
have to rebalance on a daily basis.

Zeno


EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Praesident; Prof. Dr. Rolf Tilmes, Dekan; Sabine Fuchs, CMO; Aufsichtsrat: Dr. Hellmut K. Albrecht, Vorsitzender

From patrick at burns-stat.com  Tue Sep 29 12:44:30 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 29 Sep 2009 11:44:30 +0100
Subject: [R-SIG-Finance] index tracking
In-Reply-To: <9064522880125945B98983BBAECBA1CC985604@exchsrv001.ebs.local>
References: <A3B7C29F9599CC4BBF25F9830FA0138D1BFCFDD832@VAUNSW137.au.cbainet.com>
	<9064522880125945B98983BBAECBA1CC985604@exchsrv001.ebs.local>
Message-ID: <4AC1E50E.4030406@burns-stat.com>

Forgive my skepticism, but I doubt
that any stocks are really cointegrated
with an index.  The cointegration
approach might work, but I think you'd
be finding highly correlated stocks.

The traditional way (as far as I know)
is to do an optimization that minimizes
tracking error subject to the number-of-
names constraint that you want.

I do agree that daily rebalancing is
almost surely overkill.  Monthly rebalancing
might even be as well.  It is going to
depend on the tracking error you can
tolerate, and how important not breaking
the tracking error bound is to you.



Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of "The R Inferno" and "A Guide for the Unwilling S User")

Adams, Zeno wrote:
> 
>>> I'm after suggestions for the design of simple index-tracking models.
> I >>have access to good clean data, and I am trying to cook up
> portfolios >>which replicate indices with a minimum number of long
> positions when >>rebalancing is done daily.
> 
> 
> I think a nice way to replicate an index using only a few long positions
> is to use equities that are cointegrated with the index. An example is
> given in Carol Alexander's (2001) book: "Market Models. A Guide to
> Financial Data Analysis" Ch.12. This can be done in R using Paff's urca
> package. You may also want to look at his book "Analysis of Integrated
> and Cointegrated Time Series with R". If done correctly, you don't even
> have to rebalance on a daily basis.
> 
> Zeno
> 
> 
> EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Praesident; Prof. Dr. Rolf Tilmes, Dekan; Sabine Fuchs, CMO; Aufsichtsrat: Dr. Hellmut K. Albrecht, Vorsitzender
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
>


From jeff.a.ryan at gmail.com  Tue Sep 29 16:46:35 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 29 Sep 2009 09:46:35 -0500
Subject: [R-SIG-Finance] timeSeries:: bizarre rbind behavior with
	colnames
In-Reply-To: <4ABF6774.8090201@itp.phys.ethz.ch>
References: <6fdf6d430909230843p674da615uc7e886855d211985@mail.gmail.com>
	<e8e755250909230859u387d1790l7775fe6c11e378de@mail.gmail.com>
	<4ABF6774.8090201@itp.phys.ethz.ch>
Message-ID: <e8e755250909290746i2eac0221s5965fea5a814c5ba@mail.gmail.com>

Hi Diethelm,

My comment was to use xts or zoo IN PLACE of "MATRIX", solely to
simplify the rbind process as the user and myself understood it.
Simply a safer solution than allowing a matrix rbind (not time aware).

timeSeries -> xts/zoo -> timeSeries is a better alternative than:

timeSeries -> matrix -> timeSeries

Aside from that, I think the expectation that rbind would work as it
does on matrix object, from which timeSeries is derived if I am
correct, isn't outside the realm of reason.

> m <- matrix(1:3)
> colnames(m) <- "Level"
> rbind(m,m)
     Level
[1,]     1
[2,]     2
[3,]     3
[4,]     1
[5,]     2
[6,]     3

Of course I wholeheartedly agree that documentation is important, and
should always be consulted before posted questions to the list and/or
maintainer.  That said:


?merge.timeSeries

Bind two timeSeries objects

Description:

     Binds two 'timeSeries' objects either by column or row.

Value:

     returns a S4 object of class 'timedate'.

Examples:

     ## data -
        x = as.timeSeries(data(msft.dat))[1:12, ]

     ## cbind -
        x1 <- cbind(x[, "Open"], returns(x[, "Open"]))
        colnames(x1) <- c("Open", "Return")

     ## rbind -
        rbind(x[1:3, "Open"], x[10:12, "Open"])

Isn't incredibly informative.

Best,
Jeff


On Sun, Sep 27, 2009 at 8:24 AM, Diethelm Wuertz
<wuertz at itp.phys.ethz.ch> wrote:
> Jeff Ryan wrote:
>>
>> Nicolas,
>>
>> I don't think this is a list question per se, as you should really
>> contact the maintainer.
>>
>> While I'll agree the design is odd, if you're insisting on using
>> timeSeries, and you find a problem, it would be more useful to all if
>> you proposed a patch.
>>
>> Complaints/demands for explanations/etc aren't productive, and
>> effectively show no consideration for the time and effort that
>> _contributors_ make to R.
>>
>
> *****************
>>
>> If you're looking for a better time-series solution than a matrix as
>> an interim representation, I would suggest xts. ?Or zoo...
>>
>
> Dear Jeff,
>
> I don't like to comment this sentence (and also not the previous ones
> before).
>
> Documentation is written to be read! ?The users can
> have a look into the free FAQ eBook:
>
> "A Discussion of Time Series Objects for R in Finance"
>
> Download it for free from
>
> --- http://www.rmetrics.org/ebook.htm ---
>
> There you will find also information on the usage of c,
> cbind, rbind, and merge, and how these functions work
> and how they should be used.
>
> In this eBook many frequently asked questions are answered
> on an objective level. Everybody is invited to contribute to
> this eBook and to improve its content. Suggestions are
> welcome.
>
>
> Diethelm
>
>
> **********************
>>
>> Jeff
>>
>> On Wed, Sep 23, 2009 at 10:43 AM, Nicolas Chapados
>> <nicolas.chapados at gmail.com> wrote:
>>
>>>
>>> Dear list,
>>>
>>> Looking at the code for timeSeries::rbind, it appears that it insists
>>> on concatenating the names of the two series it's trying to bind, even
>>> if they match exactly, thereby creating a longer series name. ?This is
>>> in sharp contrast to, e.g. rbind.data.frame. ?This creates very
>>> badly-behaved column names (i.e. horrendously-long) in the case where
>>> one attempts to construct a timeSeries incrementally.
>>>
>>> For example:
>>>
>>>
>>>>
>>>> rbind(timeSeries(1.0, timeDate("1996-01-01"), units="Level"),
>>>> timeSeries(2.0, timeDate("2009-01-01"), units="Level"))
>>>>
>>>
>>> GMT
>>> ? ? ? ? ?Level_Level
>>> 1996-01-01 ? ? ? ? ? 1
>>> 2009-01-01 ? ? ? ? ? 2
>>>
>>>
>>> Can someone explain the design rationale for choosing to concatenate
>>> the column names in such a manner? ?In the short term, I resolved to
>>> incrementally constructing a matrix, and at the very end building a
>>> timeSeries, but I would really prefer to keep timeSeries all along...
>>>
>>> Many thanks for any help!
>>> + Nicolas Chapados
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>
>>
>>
>>
>>
>
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From cedrick at cedrickjohnson.com  Wed Sep 30 00:12:27 2009
From: cedrick at cedrickjohnson.com (Cedrick Johnson)
Date: Tue, 29 Sep 2009 18:12:27 -0400
Subject: [R-SIG-Finance] Possible enhancement to volatility in TTR
In-Reply-To: <8cca69990909281008k22c7a1aeg79e4e124c62488bd@mail.gmail.com>
References: <4ABA9629.5020703@cedrickjohnson.com>
	<4AC0C95B.7030604@cedrickjohnson.com>
	<8cca69990909281008k22c7a1aeg79e4e124c62488bd@mail.gmail.com>
Message-ID: <4AC2864B.40203@cedrickjohnson.com>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090929/8f158719/attachment.html>

From josh.m.ulrich at gmail.com  Wed Sep 30 03:36:21 2009
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Tue, 29 Sep 2009 20:36:21 -0500
Subject: [R-SIG-Finance] Possible enhancement to volatility in TTR
In-Reply-To: <4AC2864B.40203@cedrickjohnson.com>
References: <4ABA9629.5020703@cedrickjohnson.com>
	<4AC0C95B.7030604@cedrickjohnson.com> 
	<8cca69990909281008k22c7a1aeg79e4e124c62488bd@mail.gmail.com> 
	<4AC2864B.40203@cedrickjohnson.com>
Message-ID: <8cca69990909291836ocdfabf3l2987186362cdc0ad@mail.gmail.com>

Changes have been made on r-forge.

Best,
Josh
--
http://www.fosstrading.com



On Tue, Sep 29, 2009 at 5:12 PM, Cedrick Johnson
<cedrick at cedrickjohnson.com> wrote:
> Hi Josh-
>
> No worries! The approach you outlined below works out well and I think would
> be a great addition to TTR.
>
> The exercise has definitely taught me a few new things in R that will
> greatly clean up my scripts! Thanks everyone!
>
> -cedrick
>
> Joshua Ulrich wrote:
>
> Hi Cedrick,
>
> Sorry for the slow response.  How about this quick change to
> TTR::volatility() and the code below?  I agree that OHLC should be
> allowed to be univariate in the case of calc="close".  I'm hesitant to
> add the other logic though, since the same results can be achieved
> with an external call to apply() -- see below.
>
> volatility3 <-
> function (OHLC, n = 10, N = 260, calc = "close", ...)
> {
> <snip>
>     if (calc == "close") {
>         # Add univariate case for calc="close"
>         if (NCOL(OHLC) == 1) {
>             r <- ROC(OHLC, 1, ...)
>         } else {
>             r <- ROC(OHLC[, 4], 1, ...)
>         }
>         rBar <- runSum(r, n - 1)/(n - 1)
>         s <- sqrt(N/(n - 2) * runSum((r - rBar)^2, n - 1))
>     }
> <snip>
>     reclass(s, OHLC)
> }
>
> Then you can get pretty volatilities via a command like:
>
>
> require(xts)
> data(sample_matrix)
> # Pretty volatility (note "v" will always be a matrix, even if OHLC is xts)
> v <- round(apply(sample_matrix, 2, volatility3)*100,0)
> tail(v)
>
>
>            Open High Low Close
> 2007-06-25    6    5   5     6
> 2007-06-26    6    6   6     6
> 2007-06-27    6    6   6     6
> 2007-06-28    6    5   6     5
> 2007-06-29    5    5   6     5
> 2007-06-30    5    5   6     5
>
>
> tail( volatility3(sample_matrix)*100 )
>
>
>                [,1]
> 2007-06-25 5.656552
> 2007-06-26 5.729933
> 2007-06-27 5.732217
> 2007-06-28 4.771530
> 2007-06-29 4.764216
> 2007-06-30 4.818589
>
> HTH,
> Josh
> --
> http://www.fosstrading.com
>
>
>
> On Mon, Sep 28, 2009 at 9:34 AM, Cedrick Johnson
> <cedrick at cedrickjohnson.com> wrote:
>
>
> As promised, here's my attempt at incorporating it into the original
> volatility() function (for the sake of testing, I've renamed the new
> function volatility2(..). Thanks to Murali for the suggestion to use
> apply.rolling, I've added in a few new variables in the function definition.
> So far it seems to work well here.
>
> volatility2 <- function (OHLC, n = 10, N = 260, calc = "close",
> roundValue=FALSE, roundNum=2, normalize=FALSE, nonOHLC=FALSE, ...)
> {
> ??? if (nonOHLC == FALSE) {
>
> ??? ??? OHLC <- try.xts(OHLC, error = as.matrix)
> ??? ??? calc <- match.arg(calc, c("close", "garman.klass", "parkinson",
> ??? ??????? "rogers.satchell"))
> ??? ??? if (calc == "close") {
> ??? ??? ?r <- ROC(OHLC[, 4], 1, ...)
> ??? ??????? rBar <- runSum(r, n - 1)/(n - 1)
> ?????? ??? ?s <- sqrt(N/(n - 2) * runSum((r - rBar)^2, n - 1))
> ??? ??? }
> ??? ??? if (calc == "garman.klass") {
> ??? ??????? s <- sqrt(N/n * runSum(0.5 * log(OHLC[, 2]/OHLC[, 3])^2 -
> ??? ??????????? (2 * log(2) - 1) * log(OHLC[, 4]/OHLC[, 1])^2, n))
> ??? ??? }
> ??? ??? if (calc == "parkinson") {
> ??? ??????? s <- sqrt(N/(4 * n * log(2)) * runSum(log(OHLC[, 2]/OHLC[,
> ??? ??????????? 3])^2, n))
> ??? ??? }
> ??? ??? if (calc == "rogers.satchell") {
> ??? ??????? s <- sqrt(N/n * runSum(log(OHLC[, 2]/OHLC[, 4]) * log(OHLC[,
> ??? ??????????? 2]/OHLC[, 1]) + log(OHLC[, 3]/OHLC[, 4]) * log(OHLC[,
> ??? ??????????? 3]/OHLC[, 1]), n))
> ??? ??? }
> ??? } else {
> ??? #still using the OHLC object, since this is indeed a matrix
> ??? # diff(log(OHLC)) provides the same output as ROC(x,1) so we should be
> goog. Thanks to Murali Menon
> ??? # for the suggested use of apply.rolling
> ??? s <- apply.rolling(R = diff(log(OHLC)), width=n, FUN="sd", na.pad=FALSE)
> * sqrt(N)
> ??? }
>
> ??? if (roundValue == TRUE) {
> ??? s <- round(s, roundNum)
> ??? }
> ??? if (normalize == TRUE) {
> ??? s <- s * 100
> ??? }
>
> ??? reclass(s, OHLC)
> }
>
>
> Cedrick Johnson wrote:
>
> Howdy-
>
> I came up with some code that has served me well, perhaps if anyone runs
> into the problem of needing to calculate historical volatility(close) on a
> non-OHLC series (or a bunch of "univariate" series stored in a matrix):
>
> ?????????? US2Y US5Y US10Y US30Y UKG5 UKG10 USSP2 USSP5 USSP10 USSP30
> 2009-01-02 0.88 1.72? 2.46? 2.83 2.67? 3.36? 1.50? 2.17?? 2.61?? 2.78
> 2009-01-05 0.78 1.67? 2.49? 3.00 2.75? 3.47? 1.60? 2.31?? 2.82?? 3.03
> 2009-01-06 0.80 1.68? 2.51? 3.04 2.79? 3.57? 1.55? 2.34?? 2.88?? 3.18
> 2009-01-07 0.82 1.66? 2.52? 3.05 2.80? 3.60? 1.44? 2.16?? 2.69?? 2.99
> ....
> blahblah..
>
> I adapted the code to use 'close' from Josh Ulrich's volatility function
> (since all of these are closing prices), and here's my result:
>
> volatility.matrix <- function(Matrix, n = 10, N = 252, pretty=TRUE, ...) {
> ??? volm <- Matrix
> ??? for(i in 1:ncol(Matrix)) {
> ??? ??? r <- ROC(Matrix[,i], 1,...)
> ??? ??? rBar <- runSum(r, n -1)/(n - 1)
> ??? ??? s <- sqrt(N/(n - 2) * runSum((r - rBar)^2, n - 1))
> ??? ??? #because i like to make it pretty
> ??? ??? volm[,i] <- round(s,2) * 100
> ??? }
> ??? volm <- na.omit(volm)
> ??? reclass(volm, Matrix)
> }
>
>
> The Result:
>
>
> volatility.matrix(Yields, n=30)
>
>
> ?????????? US2Y US5Y US10Y US30Y UKG5 UKG10 USSP2 USSP5 USSP10 USSP30
> 2009-03-24? 116? 110??? 73??? 45?? 54??? 57??? 67??? 65???? 60???? 51
> 2009-03-25? 115? 112??? 75??? 46?? 51??? 55??? 67??? 65???? 60???? 51
> 2009-03-26? 114? 110??? 74??? 44?? 50??? 55??? 65??? 64???? 60???? 50
> 2009-03-27? 114? 110??? 74??? 44?? 50??? 55??? 67??? 64???? 60???? 51
> 2009-03-30? 111? 104??? 68??? 39?? 52??? 55??? 67??? 63???? 58???? 51
> 2009-03-31? 107? 100??? 68??? 39?? 51??? 55??? 67??? 63???? 58???? 50
> 2009-04-01? 107? 100??? 67??? 38?? 51??? 54??? 67??? 62???? 57???? 49
> 2009-04-02? 107? 100??? 67??? 37?? 58??? 57??? 65??? 61???? 56???? 48
> ......
>
> What I get is a nice matrix of volatilities in a pretty format (rounded,
> etc.)
>
> I'll give it a shot tonight trying to implement it into the current
> volatility function, however I'm sure the code that I have here could be
> done better.
>
> Regards,
> c
>
>
> ________________________________
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
>


From aleks.clark at gmail.com  Wed Sep 30 16:24:56 2009
From: aleks.clark at gmail.com (Aleks Clark)
Date: Wed, 30 Sep 2009 09:24:56 -0500
Subject: [R-SIG-Finance] what should I be reading?
Message-ID: <5aebc8960909300724t3356ff86l9c898073d7e48cff@mail.gmail.com>

Hey List,

I'm pretty new at this whole computational finance thing, and am
looking for some reading recommendations. I am currently working on
automated forex trading using SVMs, but looking over the Finance and
Econometrics tasks in CRAN, I realize I don't understand 70% of the
/summaries/ of what those packages do.

the question is:

What kind of reading would be suitable for someone starting out with
economic models? I realize there's not going to be much out there
specifically aimed at forex, but I'd think there'd be some R-focused
material dealing with modeling of other financial markets. I've got a
pretty good grasp of how SVMs behave and work, and a somewhat lesser
understanding of neural nets, but the caret package has a profusion of
other modelling techniques that beckon to me! And I don't know much
about any of them, or if any of them will actually suit my purposes,
so any reading recommendations would be appreciated :)

Thanks,

-- 
Aleks Clark


From ggrothendieck at gmail.com  Wed Sep 30 17:27:09 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 30 Sep 2009 11:27:09 -0400
Subject: [R-SIG-Finance] what should I be reading?
In-Reply-To: <5aebc8960909300724t3356ff86l9c898073d7e48cff@mail.gmail.com>
References: <5aebc8960909300724t3356ff86l9c898073d7e48cff@mail.gmail.com>
Message-ID: <971536df0909300827j669bb9e6t6342bd7bd2bddde1@mail.gmail.com>

You might want to start with the infrastructure packages.

There are about 30 packages that depend on or can use zoo or its
finance-oriented extension, xts, so you might want to read up on zoo
and then xts since it will give you a window into a much larger set of
packages.  There are three vignettes (pdf documents) in zoo which
should get you going quickly.  Also reading R News 4/1 for info on
date/times is useful.  After that you can review the help files and
the many examples in the help files.

On Wed, Sep 30, 2009 at 10:24 AM, Aleks Clark <aleks.clark at gmail.com> wrote:
> Hey List,
>
> I'm pretty new at this whole computational finance thing, and am
> looking for some reading recommendations. I am currently working on
> automated forex trading using SVMs, but looking over the Finance and
> Econometrics tasks in CRAN, I realize I don't understand 70% of the
> /summaries/ of what those packages do.
>
> the question is:
>
> What kind of reading would be suitable for someone starting out with
> economic models? I realize there's not going to be much out there
> specifically aimed at forex, but I'd think there'd be some R-focused
> material dealing with modeling of other financial markets. I've got a
> pretty good grasp of how SVMs behave and work, and a somewhat lesser
> understanding of neural nets, but the caret package has a profusion of
> other modelling techniques that beckon to me! And I don't know much
> about any of them, or if any of them will actually suit my purposes,
> so any reading recommendations would be appreciated :)
>
> Thanks,
>
> --
> Aleks Clark
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From inei12936 at gmail.com  Wed Sep 30 20:01:59 2009
From: inei12936 at gmail.com (Inei1234)
Date: Wed, 30 Sep 2009 11:01:59 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] limiting number of investments in
	the portfolio
Message-ID: <25685139.post@talk.nabble.com>


Not sure if it is possible -- say I have N possible investments but I only
allow to have M equal weight  (M<N) in my target portfolio. How do I specify
this constraint ? Doesn't look like MinW or smth like that will do the job.
-- 
View this message in context: http://www.nabble.com/limiting-number-of-investments-in-the-portfolio-tp25685139p25685139.html
Sent from the Rmetrics mailing list archive at Nabble.com.


