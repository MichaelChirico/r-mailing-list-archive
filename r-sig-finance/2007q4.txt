From ravis at ambaresearch.com  Mon Oct  1 09:09:36 2007
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Mon, 1 Oct 2007 12:39:36 +0530
Subject: [R-SIG-Finance] RBloomberg equity options prices
References: <mailman.1.1191060001.16777.r-sig-finance@stat.math.ethz.ch>
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3028FF0C8@BAN-MAILSRV03.Amba.com>

Hi,

I need to download option prices from Bloomberg. However I don't have
ticker for the options. What I have is Sec_WKN and the security name.
For example 00GXZ7PY6400 and PUT DAX 12/2007 STRIKE 6400.
Can I construct the ticker from the above available info and then use it
download option prices using R Bloomberg.

Thank you,
Regards,

Ravi Shankar S

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
r-sig-finance-request at stat.math.ethz.ch
Sent: Saturday, September 29, 2007 3:30 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: R-SIG-Finance Digest, Vol 40, Issue 23

Send R-SIG-Finance mailing list submissions to
	r-sig-finance at stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
or, via email, send a message with subject or body 'help' to
	r-sig-finance-request at stat.math.ethz.ch

You can reach the person managing the list at
	r-sig-finance-owner at stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-SIG-Finance digest..."


Today's Topics:

   1. Re: RBloomberg equity options prices (davidr at rhotrading.com)


----------------------------------------------------------------------

Message: 1
Date: Fri, 28 Sep 2007 08:36:06 -0500
From: <davidr at rhotrading.com>
Subject: Re: [R-SIG-Finance] RBloomberg equity options prices
To: "Moshe Olshansky" <m_olshansky at yahoo.com>,
	<r-sig-finance at stat.math.ethz.ch>
Message-ID:
	<F9F2A641C593D7408925574C05A7BE774E7C16 at rhopost.rhotrading.com>
Content-Type: text/plain;	charset="us-ascii"

I would suggest reading the Bloomberg API help.
You can start at the Bloomberg professional (terminal) and type
WAPI<go>.
You can munge around there, or download WAPI Lite so you can look at the
docs offline. Much of the time, starting in Excel with the wizards will
get you to the point where you know the tickers and fields you want.
Then you can move over to programming instead of using Excel and
formulas. One other function in Bloomberg I use quite often is FPRP<go>
once you have an instrument active. This tells you everything that is
available for that ticker and what its current value is, as well as the
API filed names.

Have fun!

David L. Reiner
Rho Trading Securities, LLC


-----Original Message-----
From: Moshe Olshansky [mailto:m_olshansky at yahoo.com] 
Sent: Thursday, September 27, 2007 7:33 PM
To: David Reiner <davidr at rhotrading.com>;
r-sig-finance at stat.math.ethz.ch
Subject: RE: [R-SIG-Finance] RBloomberg equity options prices

Thank you - this really works!

I was unable to guess that for the option data one
still must add the word "Equity" at the end.

Is there any place where such information can be
found?

Moshe.

--- davidr at rhotrading.com wrote:

> Yes and yes. See ?blpGetData.
> You just need the option ticker, e.g., "IBM 10 P105
> Equity" is the Oct
> 105 put for IBM.
> 
> David L. Reiner
> Rho Trading Securities, LLC
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On
> Behalf Of Moshe
> Olshansky
> Sent: Tuesday, September 25, 2007 11:34 PM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] RBloomberg equity options
> prices
> 
> Hello,
> 
> Is it possible to get historical option prices using
> RBloomberg? Is it possible to get intraday data?
> 
> Thank you!
> 
> Moshe Olshansky.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 



------------------------------

_______________________________________________
R-SIG-Finance mailing list
R-SIG-Finance at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


End of R-SIG-Finance Digest, Vol 40, Issue 23


From robert at sanctumfi.com  Mon Oct  1 13:54:53 2007
From: robert at sanctumfi.com (Robert Sams)
Date: Mon, 1 Oct 2007 12:54:53 +0100
Subject: [R-SIG-Finance] RBloomberg equity options prices
References: <mailman.1.1191060001.16777.r-sig-finance@stat.math.ethz.ch>
	<SANCTUMFISERVERNJJw000031e4@sanctumfi.com>
Message-ID: <SANCTUMFISERVERafzg000032b2@sanctumfi.com>

Hi Ravi,

blpGetData(conn, "DAX 12/07 P6400 Index", "PX_LAST",
start=chron("8/15/7"))
         PX_LAST
08/15/07   113.5
08/16/07   153.0
08/17/07   135.4
08/20/07   123.3
08/21/07   116.4
08/22/07    94.5
08/23/07    88.9
08/24/07    91.7
08/27/07    89.7
08/28/07   103.4
08/29/07   103.5
08/30/07    85.2
08/31/07    70.7
09/03/07    64.8
09/04/07    54.2
09/05/07    69.2
09/06/07    66.5
09/07/07    94.8
09/10/07   105.1
09/11/07    88.8
09/12/07    82.7
09/13/07    69.7
09/14/07    75.3
09/17/07    79.7
09/18/07    62.3
09/19/07    35.9
09/20/07    34.7
09/21/07    26.9
09/24/07    23.2
09/25/07    24.6
09/26/07    21.9
09/27/07    19.0
09/28/07    19.2
10/01/07    21.0
>  

Of course this is not a RBloomberg question per se, but rather a
question about the Bloomberg API itself; you pay Bloomberg to answer
these questions ;)

Robert

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Ravi S.
Shankar
Sent: 01 October 2007 08:10
To: r-sig-finance at stat.math.ethz.ch
Cc: Shubha Vishwanath Karanth
Subject: [R-SIG-Finance] RBloomberg equity options prices

Hi,

I need to download option prices from Bloomberg. However I don't have
ticker for the options. What I have is Sec_WKN and the security name.
For example 00GXZ7PY6400 and PUT DAX 12/2007 STRIKE 6400.
Can I construct the ticker from the above available info and then use it
download option prices using R Bloomberg.

Thank you,
Regards,

Ravi Shankar S

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
r-sig-finance-request at stat.math.ethz.ch
Sent: Saturday, September 29, 2007 3:30 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: R-SIG-Finance Digest, Vol 40, Issue 23

Send R-SIG-Finance mailing list submissions to
	r-sig-finance at stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
or, via email, send a message with subject or body 'help' to
	r-sig-finance-request at stat.math.ethz.ch

You can reach the person managing the list at
	r-sig-finance-owner at stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific than
"Re: Contents of R-SIG-Finance digest..."


Today's Topics:

   1. Re: RBloomberg equity options prices (davidr at rhotrading.com)


----------------------------------------------------------------------

Message: 1
Date: Fri, 28 Sep 2007 08:36:06 -0500
From: <davidr at rhotrading.com>
Subject: Re: [R-SIG-Finance] RBloomberg equity options prices
To: "Moshe Olshansky" <m_olshansky at yahoo.com>,
	<r-sig-finance at stat.math.ethz.ch>
X-Message-ID:
	<F9F2A641C593D7408925574C05A7BE774E7C16 at rhopost.rhotrading.com>
Content-Type: text/plain;	charset="us-ascii"

I would suggest reading the Bloomberg API help.
You can start at the Bloomberg professional (terminal) and type
WAPI<go>.
You can munge around there, or download WAPI Lite so you can look at the
docs offline. Much of the time, starting in Excel with the wizards will
get you to the point where you know the tickers and fields you want.
Then you can move over to programming instead of using Excel and
formulas. One other function in Bloomberg I use quite often is FPRP<go>
once you have an instrument active. This tells you everything that is
available for that ticker and what its current value is, as well as the
API filed names.

Have fun!

David L. Reiner
Rho Trading Securities, LLC


-----Original Message-----
From: Moshe Olshansky [mailto:m_olshansky at yahoo.com]
Sent: Thursday, September 27, 2007 7:33 PM
To: David Reiner <davidr at rhotrading.com>;
r-sig-finance at stat.math.ethz.ch
Subject: RE: [R-SIG-Finance] RBloomberg equity options prices

Thank you - this really works!

I was unable to guess that for the option data one still must add the
word "Equity" at the end.

Is there any place where such information can be found?

Moshe.

--- davidr at rhotrading.com wrote:

> Yes and yes. See ?blpGetData.
> You just need the option ticker, e.g., "IBM 10 P105 Equity" is the Oct
> 105 put for IBM.
> 
> David L. Reiner
> Rho Trading Securities, LLC
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Moshe 
> Olshansky
> Sent: Tuesday, September 25, 2007 11:34 PM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] RBloomberg equity options prices
> 
> Hello,
> 
> Is it possible to get historical option prices using RBloomberg? Is it

> possible to get intraday data?
> 
> Thank you!
> 
> Moshe Olshansky.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 



------------------------------

_______________________________________________
R-SIG-Finance mailing list
R-SIG-Finance at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


End of R-SIG-Finance Digest, Vol 40, Issue 23

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From calder at phz.com  Mon Oct  1 14:51:30 2007
From: calder at phz.com (Matt Calder)
Date: Mon, 01 Oct 2007 08:51:30 -0400
Subject: [R-SIG-Finance] Fwd: smart updates and rolling windows
In-Reply-To: <ea7d6a710709291914q3bba623cx41e4827646d5469@mail.gmail.com>
References: <ea7d6a710709291008y60013d7bv7f7ce7abb8fb0c19@mail.gmail.com>
	<ea7d6a710709291914q3bba623cx41e4827646d5469@mail.gmail.com>
Message-ID: <1191243090.21370.173.camel@calder-pc.phz.com>

Brad and List,
	I have implemented quite a number of statistical methods in a 'moving'
way. Most of these implementations rely on common sense transformations,
and a few strategic approximations. What I would really like, and I am
sure Brad would second, is a good reference for these sorts of
calculations. 
	For example, the updating of matrix factorizations is fairly well
researched and documented, but it is not always evident when such
approaches can be applied in a particular situation. Is there a good
collection of real-time algorithms as applied to higher level problems?
Any suggested references would be appreciated. 
	By way of payment, I offer this small calculation that I find useful to
have in my R (Splus actually) toolbox. It generalizes a number of moving
operations, and often provides sufficient speedup over a raw for-loop to
remove the need for further optimization. I won't give the
implementation, just the idea, implementation is a few lines of C. The
function is a moving matrix multiply. The signature is:

mov.mat.mul <- function(X, A)

and the operation is

Y <- mov.mat.mul(x, A)

If x is a length n vector, and A is a m x w matrix then, Y is a n x m
matrix where:

=> Y[i,] = A %*% x[(i-w+1):i]

With the right choice of A one can do moving regressions of all sorts.
Also, doing:

	apply(mov.mat.mul(diag(win), x), 1, my.non.linear.f)

is a nice way to do arbitrary moving functions and get sufficiently good
performance to assess whether further improvement is warranted.  	
	To summarize, I would like to hear about good references for these
sorts of calculations. My own small contribution is to suggest that
moving matrix multiplication is a fundamental operation that is useful
for moving computations.

	Matt





On Sat, 2007-09-29 at 19:14 -0700, Bradford Cross wrote:
> Greetings R'ers!
> 
> I have been looking for mathematics libraries for event stream processing /
> time series simulation.  Mathematics libraries for event stream processing
> require two key features; 1) "smart updates" (functions use optimal update
> algorithms, f.ex. once mean is calculated for an event stream, the
> subsequent calls to the function are computed using previous values of mean
> rather than by brute force re-calculation), 2)  "rolling calculations"
> (functions take a lag parameter for sample size, f.ex. mean of last 100
> events.)
> 
> I found a couple simple summary statistics implemented like this in the zoo
> package.  I have also found implementations for smart updates in some other
> languages (apache commons math, and BOOST accumulators) but these only
> supports accumulated calculations, not rolling calculations.
> 
> I have built libraries for this before, and I am currently working on a new
> version - but before I reinvent the wheel I am trying to find some folks in
> the community with similar interests to collaborate with.
> 
> My personal use for this is financial time series analysis, so I am
> interested in  implementing these high-performance algorithms for classical
> statistics, robust statistics, regression models, etc.
> 
> Best!
> 
> /brad
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.


From edd at debian.org  Mon Oct  1 15:47:53 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 1 Oct 2007 08:47:53 -0500
Subject: [R-SIG-Finance] [OT] Spreadsheet bashing paper from arxiv.org
Message-ID: <18176.64137.526168.770432@ron.nulle.part>


Preaching to the choir here, but for the record:
	http://www.arxiv.org/ftp/arxiv/papers/0709/0709.4063.pdf

Cheers, Dirk

-- 
Three out of two people have difficulties with fractions.


From edd at debian.org  Mon Oct  1 15:55:15 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 1 Oct 2007 08:55:15 -0500
Subject: [R-SIG-Finance] [OT] Spreadsheet bashing paper from arxiv.org
Message-ID: <18176.64579.82807.478793@ron.nulle.part>


Preaching to the choir here, but for the record:
	http://www.arxiv.org/ftp/arxiv/papers/0709/0709.4063.pdf

Cheers, Dirk

-- 
Three out of two people have difficulties with fractions.


From ngottlieb at marinercapital.com  Mon Oct  1 16:02:33 2007
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Mon, 1 Oct 2007 10:02:33 -0400
Subject: [R-SIG-Finance] Fwd: smart updates and rolling windows
In-Reply-To: <ea7d6a710709291914q3bba623cx41e4827646d5469@mail.gmail.com>
References: <ea7d6a710709291008y60013d7bv7f7ce7abb8fb0c19@mail.gmail.com>
	<ea7d6a710709291914q3bba623cx41e4827646d5469@mail.gmail.com>
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D880151E33D@500MAIL.goldbox.com>

Brad:

Not sure if this is relevant to your interest however to get some new
insights
Into complex event streaming take a look at Streambase.

http://www.streambase.com

Interesting product might give you new insights.

Neil 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Bradford
Cross
Sent: Saturday, September 29, 2007 10:14 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Fwd: smart updates and rolling windows

Greetings R'ers!

I have been looking for mathematics libraries for event stream
processing / time series simulation.  Mathematics libraries for event
stream processing require two key features; 1) "smart updates"
(functions use optimal update algorithms, f.ex. once mean is calculated
for an event stream, the subsequent calls to the function are computed
using previous values of mean rather than by brute force
re-calculation), 2)  "rolling calculations"
(functions take a lag parameter for sample size, f.ex. mean of last 100
events.)

I found a couple simple summary statistics implemented like this in the
zoo package.  I have also found implementations for smart updates in
some other languages (apache commons math, and BOOST accumulators) but
these only supports accumulated calculations, not rolling calculations.

I have built libraries for this before, and I am currently working on a
new version - but before I reinvent the wheel I am trying to find some
folks in the community with similar interests to collaborate with.

My personal use for this is financial time series analysis, so I am
interested in  implementing these high-performance algorithms for
classical statistics, robust statistics, regression models, etc.

Best!

/brad

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.
--------------------------------------------------------



This information is being sent at the recipient's request or...{{dropped}}


From jctoll at gmail.com  Mon Oct  1 23:06:26 2007
From: jctoll at gmail.com (James)
Date: Mon, 1 Oct 2007 15:06:26 -0600
Subject: [R-SIG-Finance] For loop & CRRBinomial
Message-ID: <281EDCE6-9F0A-4DAC-8771-BA9C63648C74@gmail.com>

Hi,

I've been creating P&L plots of basic option positions.  In order to  
do so, I need to calculate the option prices associated with the  
given range of prices for the underlying.  When using Rmetrics and  
the black scholes model, I can pass in a vector of underlying prices  
and get back a vector of option prices.  That doesn't seem to work  
with the CRRBinomial model.  So I've been using a for loop to fill my  
vector with option prices:

pricesToday<- NULL
for(price in priceRange) {
      pricesToday<-c(pricesToday, CRRBinomialTreeOption 
(TypeFlag="ca",S=price, X=20, Time=1/12, r=.05, b=.05, sigma=.4,  
n=100)@price)
}


It works, but it just seems "cludgey" to me.  Is there a better, more  
"r" way of doing this?  Thanks.

James


From rory.winston at gmail.com  Tue Oct  2 12:41:57 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Tue, 2 Oct 2007 11:41:57 +0100
Subject: [R-SIG-Finance] Fwd: smart updates and rolling windows
Message-ID: <3f446aa30710020341s229d069em90da650ee2fa9868@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071002/be82cd26/attachment.pl 

From barth at tac-financial.com  Tue Oct  2 12:56:22 2007
From: barth at tac-financial.com (Sylvain BARTHELEMY)
Date: Tue, 2 Oct 2007 12:56:22 +0200
Subject: [R-SIG-Finance] Fwd: smart updates and rolling windows
In-Reply-To: <3f446aa30710020341s229d069em90da650ee2fa9868@mail.gmail.com>
References: <3f446aa30710020341s229d069em90da650ee2fa9868@mail.gmail.com>
Message-ID: <005101c804e2$dfa93ef0$9efbbcd0$@com>

Hi Rory,

I think that for a stream of continuous updates, and especially if you use
very high frequency data and would like to do intensive calculation, a
better choice than R would be C or C++.

Regards.

---
Sylvain Barth?l?my
Research Director, TAC
www.tac-financial.com | www.sylbarth.com


-----Message d'origine-----
De?: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de Rory Winston
Envoy??: mardi 2 octobre 2007 12:42
??: r-sig-finance at stat.math.ethz.ch
Objet?: [R-SIG-Finance] Fwd: smart updates and rolling windows

Hi Matt

Thats a great question - I was thinking about that issue this morning. Does
anyone know of a good algorithmic reference (in code or print) for
"windowed" or update-style algos, where an operation is performed on a
rolling window of data, that may be either (a) too large to fit into memory,
so it must be 'chunked', or (b) a stream of continuous data updates?

Cheers
Rory

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From ryan.sheftel at malbecpartners.com  Tue Oct  2 16:58:47 2007
From: ryan.sheftel at malbecpartners.com (Ryan Sheftel)
Date: Tue, 2 Oct 2007 10:58:47 -0400
Subject: [R-SIG-Finance] Fwd: smart updates and rolling windows
In-Reply-To: <005101c804e2$dfa93ef0$9efbbcd0$@com>
Message-ID: <OF6A556CC6.A2107D80-ON85257368.0051FAF8-85257368.0052582D@fftw.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071002/3c184699/attachment.pl 

From edd at debian.org  Wed Oct  3 19:13:09 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 3 Oct 2007 12:13:09 -0500
Subject: [R-SIG-Finance] Fwd: smart updates and rolling windows
In-Reply-To: <OF6A556CC6.A2107D80-ON85257368.0051FAF8-85257368.0052582D@fftw.com>
References: <005101c804e2$dfa93ef0$9efbbcd0$@com>
	<OF6A556CC6.A2107D80-ON85257368.0051FAF8-85257368.0052582D@fftw.com>
Message-ID: <18179.52645.444807.81312@ron.nulle.part>


On 2 October 2007 at 10:58, Ryan Sheftel wrote:
| These are commonly refered to as "event-engines" and now come call 
| themselves, Complex Event Engines:
| 
| The biggest providers are:
| 
| http://www.progress.com/index.ssp
| http://www.coral8.com/                          [this one has a free trial 
| version on the web]
| http://www.streambase.com/
| http://www.aleri.com/

Any comments from past or current users of these?  

Dirk

-- 
Three out of two people have difficulties with fractions.


From patrick at burns-stat.com  Sat Oct  6 18:49:41 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Sat, 06 Oct 2007 17:49:41 +0100
Subject: [R-SIG-Finance] Cramer vs. Pseudo-Cramer, etc.
Message-ID: <4707BCA5.3040203@burns-stat.com>

Newly on the Burns Statistics website is:

Cramer vs. Pseudo-Cramer
http://www.burns-stat.com/pages/Working/cramer_vs_pseudocramer.pdf

A recent Barron's article examined the efficacy of stock
recommendations on the television show Mad Money.
Statistical analyses of stock recommendations are scrutinized
here in detail, and a powerful analysis using random portfolios
is suggested. Differences between simple returns and log
returns are discussed, as is the usefulness of the statistical
bootstrap. The cost to individuals of trading stocks can easily
overwhelm even quite good recommendations.

-------------------------------------------------------------

Patrick Burns will be speaking at a couple of upcoming events
in London:

October 24: One description is that it is an advertisement for
R and Monte Carlo techniques in risk management.

November 13-15: Quant Invest conference.

Links for these events can be found on the Finance page:
http://www.burns-stat.com/pages/finance.html

--------------------------------------------------------------

http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
has been been updated to include a link to a discussion of a
humorous (to some) numerical bug in Excel 2007.



Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com


From alexander.f.moreno at gmail.com  Sun Oct  7 11:24:24 2007
From: alexander.f.moreno at gmail.com (Alexander Moreno)
Date: Sun, 7 Oct 2007 04:24:24 -0500
Subject: [R-SIG-Finance] copula-based higher moment optimization
Message-ID: <3303a4570710070224i4dcdb258yc1700470fec4e67b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071007/9dca379f/attachment.pl 

From brian at braverock.com  Sun Oct  7 14:26:13 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Sun, 07 Oct 2007 07:26:13 -0500
Subject: [R-SIG-Finance] copula-based higher moment optimization
In-Reply-To: <3303a4570710070224i4dcdb258yc1700470fec4e67b@mail.gmail.com>
References: <3303a4570710070224i4dcdb258yc1700470fec4e67b@mail.gmail.com>
Message-ID: <4708D065.8040003@braverock.com>

Alexander Moreno wrote:
> Is there any very quick and dirty way to do a copula-based optimization
> incorporating skewness using R packages, i.e. something that's close to
> being automated?  Any help would be appreciated.

Is there a particular paper that you're trying to emulate?

Here's the simple answer: there is no "automated" portfolio optimizer 
for higher moments in R.

Here's the complicated answer: there are many copula functions in R that 
could be used for a better estimate of the moments of your distribution, 
and those moments could be fed into an optimization routine.  There are 
also many non-copula methods in R for fitting non-normal distributions 
for a good estimate of the higher moments of the distribution (or, more 
appropriately, for a good estimate of the risk metrics at a particular 
quantile or confidence level).

Probably the "simplest" (most-automated) copula-based method for 
portfolio optimization in R that I've seen was done by some of Prof. 
Wuertz's graduate students, who were working on fitting a copula to the 
tail of the distribution to get a better estimate of Expected Shortfall, 
and then using that to build ES-optimal portfolios.  I assume that Prof. 
Wuertz's  team work will end up in RMetrics eventually, but I don't 
believe it's made it into the released code just yet: check the 
examples, tests, and documentation for the fCopula package to find out. 
You can also use a Cornish Fisher expansion to calculate a four-moment 
VaR (see package PerformanceAnalytics) and use that as the downside risk 
metric in your optimization.

Regards,

    - Brian


From konrad.banachewicz at gmail.com  Sun Oct  7 14:34:41 2007
From: konrad.banachewicz at gmail.com (Konrad Banachewicz)
Date: Sun, 7 Oct 2007 14:34:41 +0200
Subject: [R-SIG-Finance] copula-based higher moment optimization
Message-ID: <204e4c50710070534q7293c632x121111cf7f95e314@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071007/76e9d14e/attachment.pl 

From markus at insightfromdata.com  Sun Oct  7 17:01:36 2007
From: markus at insightfromdata.com (Markus Loecher)
Date: Sun, 7 Oct 2007 11:01:36 -0400
Subject: [R-SIG-Finance] Kalman filter, SSPIR, usage
Message-ID: <98B78C3C-79DB-4267-B63A-D1E42BD6D6B1@insightfromdata.com>

To the  R time series wizards,
could you help me with an error message that I do not know how to  
interpret.

I just want to try out the kalman filter implemented in the package  
SSPIR on a random walk model:

 > tmp <- ts(cumsum(rnorm(200)), start = c(2007,2), frequency=12)
 > plot(tmp)
 > m1 <- SS(tmp,m0=matrix(1),C0=matrix(2))
 > kfilter(m1);

The error I get is:
	"Error in matrix(NA, ss$n, ss$p) : non-numeric matrix extent"

What am I doing incorrectly ? It is OK to set phi to null, is it not ?

Thanks!!

Markus


From sf at metrak.com  Mon Oct  8 13:23:21 2007
From: sf at metrak.com (paul sorenson)
Date: Mon, 08 Oct 2007 21:23:21 +1000
Subject: [R-SIG-Finance] returns convention
Message-ID: <470A1329.2040102@metrak.com>

Is it usually assumed that references to "returns" are calculated as 
diff(log(prices))?  Compared with say the simple fractional change from 
one price to the next?

For example, in PerformanceAnalytics I notice that the default value of 
CalculateReturns is diff(log(prices)).

I guess it probably doesn't matter much either way for small changes, I 
just wanted to know if there was some common convention when I see an R 
function that expects a returns vector.

cheers


From jeff.a.ryan at gmail.com  Mon Oct  8 16:28:40 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 8 Oct 2007 09:28:40 -0500
Subject: [R-SIG-Finance] returns convention
In-Reply-To: <470A1329.2040102@metrak.com>
References: <470A1329.2040102@metrak.com>
Message-ID: <e8e755250710080728g5553f448s302f8e42b89b6b68@mail.gmail.com>

It seems to be common within academic literature - econometric and
finance, though I'd not go so far as to rely on one definition across
a range of applications.  Probably should/could be, but unlikely to
ever happen.  Even my quantmod package isn't entirely consistent - as
some functions are log diff and others simple percent changes -
depending on use. Though I *think* it is documented where each is
used.

And as far as I can tell, my trading accounts don't change by log
differences : )

Jeff

On 10/8/07, paul sorenson <sf at metrak.com> wrote:
> Is it usually assumed that references to "returns" are calculated as
> diff(log(prices))?  Compared with say the simple fractional change from
> one price to the next?
>
> For example, in PerformanceAnalytics I notice that the default value of
> CalculateReturns is diff(log(prices)).
>
> I guess it probably doesn't matter much either way for small changes, I
> just wanted to know if there was some common convention when I see an R
> function that expects a returns vector.
>
> cheers
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ezivot at u.washington.edu  Mon Oct  8 21:35:39 2007
From: ezivot at u.washington.edu (Eric Zivot)
Date: Mon, 8 Oct 2007 12:35:39 -0700
Subject: [R-SIG-Finance] returns convention
In-Reply-To: <e8e755250710080728g5553f448s302f8e42b89b6b68@mail.gmail.com>
Message-ID: <200710081938.l98Jc0Va018300@smtp.washington.edu>

Log returns, or continuously compounded returns, are typically used when
doing statistical analysis of returns because of the nice additivity
properties of multiperiod returns and the fact that they are defined between
-00 and +00. However, you have to be a bit careful when dealing with
portfolios since the log portfolio return is not a weighted average of the
log returns on the individual stocks. I actually have not seen a good
discussion of the implication of using log returns for portfolio analysis.
Does someone have a good reference? 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Jeff Ryan
Sent: Monday, October 08, 2007 7:29 AM
To: paul sorenson
Cc: r-finance
Subject: Re: [R-SIG-Finance] returns convention

It seems to be common within academic literature - econometric and finance,
though I'd not go so far as to rely on one definition across a range of
applications.  Probably should/could be, but unlikely to ever happen.  Even
my quantmod package isn't entirely consistent - as some functions are log
diff and others simple percent changes - depending on use. Though I *think*
it is documented where each is used.

And as far as I can tell, my trading accounts don't change by log
differences : )

Jeff

On 10/8/07, paul sorenson <sf at metrak.com> wrote:
> Is it usually assumed that references to "returns" are calculated as 
> diff(log(prices))?  Compared with say the simple fractional change 
> from one price to the next?
>
> For example, in PerformanceAnalytics I notice that the default value 
> of CalculateReturns is diff(log(prices)).
>
> I guess it probably doesn't matter much either way for small changes, 
> I just wanted to know if there was some common convention when I see 
> an R function that expects a returns vector.
>
> cheers
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From peter at braverock.com  Mon Oct  8 22:20:40 2007
From: peter at braverock.com (Peter Carl)
Date: Mon, 8 Oct 2007 15:20:40 -0500
Subject: [R-SIG-Finance] returns convention
In-Reply-To: <470A1329.2040102@metrak.com>
References: <470A1329.2040102@metrak.com>
Message-ID: <200710081520.40773.peter@braverock.com>

On Monday 08 October 2007 6:23:21 am paul sorenson wrote:
> Is it usually assumed that references to "returns" are calculated as
> diff(log(prices))?  Compared with say the simple fractional change from
> one price to the next?
>
> For example, in PerformanceAnalytics I notice that the default value of
> CalculateReturns is diff(log(prices)).
>
> I guess it probably doesn't matter much either way for small changes, I
> just wanted to know if there was some common convention when I see an R
> function that expects a returns vector.
>
> cheers
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.

Despite the default you point out, most of the functions in  
PerformanceAnalytics assume simple returns.  Perhaps an underlying question  
is "which is the 'correct' way to calculate returns?"  but the answer for  
what to is "neither", which is somewhat unsatisfying (to point to Jeff's 
earlier comment).  The "better" method depends what you are trying to 
accomplish.  Most of PerformanceAnalytics' functions are about performance 
description, so we have erred to the simple which is slightly more 
conservative.

Before we get to 1.0, I hope to have methods included for handling both  
explicitly where needed.

As Eric already pointed out, continuously compounded returns have an advantage 
over simple returns in that they are additive through time and they include 
the effects of compounding.   The drawback to log returns is that they are 
not additive in a portfolio.  

The simple return on a portfolio of assets is a weighted average of the 
simple returns on the individual assets.  Average simple returns can be 
thought of as a reasonable one-step-ahead forecast; average compound returns 
are a better representation of the expected return over a longer period.  

But what really matters is that you are consistent, so that the comparisons 
you make are imprecise in the same direction and the resulting relative 
ranking is correct.

I can't speak for the assumptions of other packages or functions expecting 
returns, but I would note that there seem to be relatively few.  If I recall 
correctly, many functions assume prices and make the conversion.

pcc

-- 
Peter Carl
145 Scottswood Rd
Riverside, IL 60546
708 447 6465


From sf at metrak.com  Mon Oct  8 23:44:48 2007
From: sf at metrak.com (paul sorenson)
Date: Tue, 09 Oct 2007 07:44:48 +1000
Subject: [R-SIG-Finance] returns convention
In-Reply-To: <200710081520.40773.peter@braverock.com>
References: <470A1329.2040102@metrak.com>
	<200710081520.40773.peter@braverock.com>
Message-ID: <470AA4D0.9040702@metrak.com>

I appreciate all the responses.  It kind of confirms what I was thinking 
but I didn't want to die wondering.

  o Log returns have some neat properties for calculations.
  o The difference between log returns and simple returns for small 
changes isn't that big.
  o It sounds like log and simple returns unintentionally get used 
interchangeably from time to time.
  o I need to be careful when calculating portfolios.
  o Using functions out of some of these great packages has some traps 
for young players.

One simple case where it would be nice to know is where I am comparing 
my calculations with someone elses.  Eg comparing annualized returns or 
Sharpe ratios with numbers published on the web.

Peter Carl wrote:
> On Monday 08 October 2007 6:23:21 am paul sorenson wrote:
>> Is it usually assumed that references to "returns" are calculated as
>> diff(log(prices))?  Compared with say the simple fractional change from
>> one price to the next?
>>
>> For example, in PerformanceAnalytics I notice that the default value of
>> CalculateReturns is diff(log(prices)).
>>
>> I guess it probably doesn't matter much either way for small changes, I
>> just wanted to know if there was some common convention when I see an R
>> function that expects a returns vector.
>>
>> cheers
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
> 
> Despite the default you point out, most of the functions in  
> PerformanceAnalytics assume simple returns.  Perhaps an underlying question  
> is "which is the 'correct' way to calculate returns?"  but the answer for  
> what to is "neither", which is somewhat unsatisfying (to point to Jeff's 
> earlier comment).  The "better" method depends what you are trying to 
> accomplish.  Most of PerformanceAnalytics' functions are about performance 
> description, so we have erred to the simple which is slightly more 
> conservative.
> 
> Before we get to 1.0, I hope to have methods included for handling both  
> explicitly where needed.
> 
> As Eric already pointed out, continuously compounded returns have an advantage 
> over simple returns in that they are additive through time and they include 
> the effects of compounding.   The drawback to log returns is that they are 
> not additive in a portfolio.  
> 
> The simple return on a portfolio of assets is a weighted average of the 
> simple returns on the individual assets.  Average simple returns can be 
> thought of as a reasonable one-step-ahead forecast; average compound returns 
> are a better representation of the expected return over a longer period.  
> 
> But what really matters is that you are consistent, so that the comparisons 
> you make are imprecise in the same direction and the resulting relative 
> ranking is correct.
> 
> I can't speak for the assumptions of other packages or functions expecting 
> returns, but I would note that there seem to be relatively few.  If I recall 
> correctly, many functions assume prices and make the conversion.
> 
> pcc
>


From bbands at gmail.com  Tue Oct  9 00:25:43 2007
From: bbands at gmail.com (BBands)
Date: Mon, 8 Oct 2007 15:25:43 -0700
Subject: [R-SIG-Finance] Burns on Cramer
Message-ID: <6e8360ad0710081525k1e8cacf3q9162c793e602b7f8@mail.gmail.com>

I read this paper,
http://www.burns-stat.com/pages/Working/cramer_vs_pseudocramer.pdf,
and found it to be interesting, however, the idea of judging
real-world results against artificially constructed portfolios leaves
me cold. The only reasonable way of judging performance is against
stated goals. Goals tend to be specified in terms of returns (relative
or absolute),  variability (volatility, draw down and so forth) or a
combination (Sharpe, Sortino...) and the only reasonable question is
to what extent the goals are met. Judgment against a basket of random
portfolios, however cleverly constructed, is simply not defined in
relation to the efforts of the manager.

(In this particular case, one might well ask what Cramer's goals are.
They would seem to be to have fun and gather fame. Since it seems that
he is eminently successful on both counts one is forced to acknowledge
that he is doing a good job.)

As for the challenge to chartists in the paper's conclusion, they too
should be measured individually against their goals, not random
portfolios. Why this emphasis on goals? Because this goals are what
investors pay for. They may use past performance as a gage to
ascertain whether the goal is obtainable, but by and large investors
pay for specified goals and retain or fire managers on whether those
goals are met. Other assessment alternatives matter little, even if
they are 'better', unless investors agree and contract for them. This
is after all a contractual relationship and results need to be
evaluated in terms of expectations.

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From m_olshansky at yahoo.com  Tue Oct  9 03:23:48 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Mon, 8 Oct 2007 18:23:48 -0700 (PDT)
Subject: [R-SIG-Finance] American basket options
Message-ID: <500839.93381.qm@web32207.mail.mud.yahoo.com>

Hello,

Is there any R code which allows to calculate the
price of an American basket option (option on a price
of a portfolio)? 
If yes, are there any references to how accurate these
calculations are? 
If no, can anybody recommend a relatively easy to use
software doing this?

Are there any non Monte Carlo methods to compute (even
roughly) the price on an American basket put option on
a portfolio of 10 dividend paying stocks with 6 months
maturity?

Thank you in advance,

Moshe.


From kriskumar at earthlink.net  Tue Oct  9 04:04:20 2007
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Mon, 08 Oct 2007 22:04:20 -0400
Subject: [R-SIG-Finance] American basket options
In-Reply-To: <500839.93381.qm@web32207.mail.mud.yahoo.com>
References: <500839.93381.qm@web32207.mail.mud.yahoo.com>
Message-ID: <470AE1A4.1020308@earthlink.net>

I am just curious as to if this is being traded in some market ?.

This is probably not very helpful but I don't think a European style 
basket is there in the existing packages. European style baskets are 
themselves tricky if you want to get the basket smile right etc. 
American style baskets will be messy.

Cheers
Krishna




Moshe Olshansky wrote:
> Hello,
>
> Is there any R code which allows to calculate the
> price of an American basket option (option on a price
> of a portfolio)? 
> If yes, are there any references to how accurate these
> calculations are? 
> If no, can anybody recommend a relatively easy to use
> software doing this?
>
> Are there any non Monte Carlo methods to compute (even
> roughly) the price on an American basket put option on
> a portfolio of 10 dividend paying stocks with 6 months
> maturity?
>
> Thank you in advance,
>
> Moshe.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
>


From m_olshansky at yahoo.com  Tue Oct  9 04:16:19 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Mon, 8 Oct 2007 19:16:19 -0700 (PDT)
Subject: [R-SIG-Finance] American basket options
In-Reply-To: <470AE1A4.1020308@earthlink.net>
Message-ID: <703607.93093.qm@web32211.mail.mud.yahoo.com>

This is an OTC traded option.

For a European option one can estimate the covariance
matrix and then use Monte Carlo (taking into account
the dividends for each stock). This is pretty
straightforward (well, there may be many ways to
estimate the covariance matrix but let's use the
simplest one).

Regards,

Moshe.

--- Krishna Kumar <kriskumar at earthlink.net> wrote:

> I am just curious as to if this is being traded in
> some market ?.
> 
> This is probably not very helpful but I don't think
> a European style 
> basket is there in the existing packages. European
> style baskets are 
> themselves tricky if you want to get the basket
> smile right etc. 
> American style baskets will be messy.
> 
> Cheers
> Krishna
> 
> 
> 
> 
> Moshe Olshansky wrote:
> > Hello,
> >
> > Is there any R code which allows to calculate the
> > price of an American basket option (option on a
> price
> > of a portfolio)? 
> > If yes, are there any references to how accurate
> these
> > calculations are? 
> > If no, can anybody recommend a relatively easy to
> use
> > software doing this?
> >
> > Are there any non Monte Carlo methods to compute
> (even
> > roughly) the price on an American basket put
> option on
> > a portfolio of 10 dividend paying stocks with 6
> months
> > maturity?
> >
> > Thank you in advance,
> >
> > Moshe.
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only. 
> > -- If you want to post, subscribe first.
> >
> >   
> 
>


From wojciech.slusarski at gmail.com  Tue Oct  9 11:59:39 2007
From: wojciech.slusarski at gmail.com (Wojciech Slusarski)
Date: Tue, 9 Oct 2007 11:59:39 +0200
Subject: [R-SIG-Finance] American basket options
In-Reply-To: <703607.93093.qm@web32211.mail.mud.yahoo.com>
References: <470AE1A4.1020308@earthlink.net>
	<703607.93093.qm@web32211.mail.mud.yahoo.com>
Message-ID: <5e64e5be0710090259y13b50523n7607990f55e8201@mail.gmail.com>

There is an algorithm called OLS Monte Carlo, or Longstaff-Schwarz
algorithm for valuation of american/bermudan options using MC method,
though it can be a bit tricky to programm that for a portfolio of 10
securities and be a bit unstable, though worth of trying. If the
dividends are not high, it should not differ much from a european
option priced using Monte Carlo. If dividends are high, then the price
should be slightly higher.

Regards,
Wojciech ?lusarski


2007/10/9, Moshe Olshansky <m_olshansky at yahoo.com>:
> This is an OTC traded option.
>
> For a European option one can estimate the covariance
> matrix and then use Monte Carlo (taking into account
> the dividends for each stock). This is pretty
> straightforward (well, there may be many ways to
> estimate the covariance matrix but let's use the
> simplest one).
>
> Regards,
>
> Moshe.
>
> --- Krishna Kumar <kriskumar at earthlink.net> wrote:
>
> > I am just curious as to if this is being traded in
> > some market ?.
> >
> > This is probably not very helpful but I don't think
> > a European style
> > basket is there in the existing packages. European
> > style baskets are
> > themselves tricky if you want to get the basket
> > smile right etc.
> > American style baskets will be messy.
> >
> > Cheers
> > Krishna
> >
> >
> >
> >
> > Moshe Olshansky wrote:
> > > Hello,
> > >
> > > Is there any R code which allows to calculate the
> > > price of an American basket option (option on a
> > price
> > > of a portfolio)?
> > > If yes, are there any references to how accurate
> > these
> > > calculations are?
> > > If no, can anybody recommend a relatively easy to
> > use
> > > software doing this?
> > >
> > > Are there any non Monte Carlo methods to compute
> > (even
> > > roughly) the price on an American basket put
> > option on
> > > a portfolio of 10 dividend paying stocks with 6
> > months
> > > maturity?
> > >
> > > Thank you in advance,
> > >
> > > Moshe.
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > >
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> > >
> >
> >
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

From dsmith at viciscapital.com  Tue Oct  9 13:05:07 2007
From: dsmith at viciscapital.com (Dale Smith)
Date: Tue, 9 Oct 2007 07:05:07 -0400
Subject: [R-SIG-Finance] American basket options
In-Reply-To: <5e64e5be0710090259y13b50523n7607990f55e8201@mail.gmail.com>
References: <470AE1A4.1020308@earthlink.net><703607.93093.qm@web32211.mail.mud.yahoo.com>
	<5e64e5be0710090259y13b50523n7607990f55e8201@mail.gmail.com>
Message-ID: <0E4F0C7EEAAB274F8DC6B1543949F05B0113E275@vicsrv4.viciscapital.com>

For baskets of stocks larger than three or so, Monte Carlo methods outperform the best finite difference code. As mentioned below, there are Monte Carlo algorithms for the American case.

Dale Smith, Ph.D.
Vicis Capital, LLC

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Wojciech Slusarski
Sent: Tuesday, October 09, 2007 6:00 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] American basket options

There is an algorithm called OLS Monte Carlo, or Longstaff-Schwarz
algorithm for valuation of american/bermudan options using MC method,
though it can be a bit tricky to programm that for a portfolio of 10
securities and be a bit unstable, though worth of trying. If the
dividends are not high, it should not differ much from a european
option priced using Monte Carlo. If dividends are high, then the price
should be slightly higher.

Regards,
Wojciech ?lusarski


2007/10/9, Moshe Olshansky <m_olshansky at yahoo.com>:
> This is an OTC traded option.
>
> For a European option one can estimate the covariance
> matrix and then use Monte Carlo (taking into account
> the dividends for each stock). This is pretty
> straightforward (well, there may be many ways to
> estimate the covariance matrix but let's use the
> simplest one).
>
> Regards,
>
> Moshe.
>
> --- Krishna Kumar <kriskumar at earthlink.net> wrote:
>
> > I am just curious as to if this is being traded in
> > some market ?.
> >
> > This is probably not very helpful but I don't think
> > a European style
> > basket is there in the existing packages. European
> > style baskets are
> > themselves tricky if you want to get the basket
> > smile right etc.
> > American style baskets will be messy.
> >
> > Cheers
> > Krishna
> >
> >
> >
> >
> > Moshe Olshansky wrote:
> > > Hello,
> > >
> > > Is there any R code which allows to calculate the
> > > price of an American basket option (option on a
> > price
> > > of a portfolio)?
> > > If yes, are there any references to how accurate
> > these
> > > calculations are?
> > > If no, can anybody recommend a relatively easy to
> > use
> > > software doing this?
> > >
> > > Are there any non Monte Carlo methods to compute
> > (even
> > > roughly) the price on an American basket put
> > option on
> > > a portfolio of 10 dividend paying stocks with 6
> > months
> > > maturity?
> > >
> > > Thank you in advance,
> > >
> > > Moshe.
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > >
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> > >
> >
> >
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

All e-mail sent to or from this address will be received or otherwise recorded by Vicis Capital, LLC and is subject to archival, monitoring and/or review, by and/or disclosure to, someone other than the recipient.  This message is intended only for the use of the person(s) ("intended recipient") to whom it is addressed.  It may contain information that is privileged and confidential.  If you are not the intended recipient, please contact the sender as soon as possible and delete the message without reading it or making a copy.  Any dissemination, distribution, copying, or other use of this message or any of its content by any person other than the intended recipient is strictly prohibited.  Vicis Capital, LLC only transacts business in states where it is properly registered or notice filed, or excluded or exempted from registration or notice filing requirements.


From patrick at burns-stat.com  Tue Oct  9 13:30:14 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 09 Oct 2007 12:30:14 +0100
Subject: [R-SIG-Finance] Burns on Cramer
In-Reply-To: <6e8360ad0710081525k1e8cacf3q9162c793e602b7f8@mail.gmail.com>
References: <6e8360ad0710081525k1e8cacf3q9162c793e602b7f8@mail.gmail.com>
Message-ID: <470B6646.5060706@burns-stat.com>

John,

We definitely agree on one thing: Cramer's goals appear
to be having fun and getting fame.  I have no way of knowing
if he is having fun, but he has certainly succeeded with fame.
(Disclosure: Two of my top goals are having fun and not getting
famous.  At present I'm doing well with both these.)

I think goals are of secondary rather than primary importance.
Reading my horoscope in the newspaper may satisfy my goal
of knowing how to run my life, but I think a more important
question is whether that is the action of a rational person or not.

What I am advocating is SCIENCE: What can we know? And
then let's set about learning that.

What we can learn with random portfolios is whether or not a
fund or a market commentator exhibits skill over some period of
time.  (There are other things to learn with random portfolios as
well, by the way.)

Once we have information on skill (which as far as I know is
uniquely available through random portfolios), then we can address
the issue of goals.  We can get a sense of how much skill was needed
to meet the goal over the past period -- maybe a lot, maybe even
negative skill would have done.  More importantly we get a hint of
how we might best set about to achieve our goals in the future.

If we focus on goals rather than skill, we have very little basis on
which to carry past performance into the future.

If you wanted to pick a poker player to back, would you merely
look at the player's winnings or would you try to evaluate if he or
she outperformed given the hands that were dealt?

In terms of what is contracted for, I have previously suggested that a
good choice is to contract for exhibited skill as determined by random
portfolios.  This rewards fund managers that do exhibit skill, and allows
the investor a mechanism that comes closer to paying for what they get.

Pat

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com


BBands wrote:

>I read this paper,
>http://www.burns-stat.com/pages/Working/cramer_vs_pseudocramer.pdf,
>and found it to be interesting, however, the idea of judging
>real-world results against artificially constructed portfolios leaves
>me cold. The only reasonable way of judging performance is against
>stated goals. Goals tend to be specified in terms of returns (relative
>or absolute),  variability (volatility, draw down and so forth) or a
>combination (Sharpe, Sortino...) and the only reasonable question is
>to what extent the goals are met. Judgment against a basket of random
>portfolios, however cleverly constructed, is simply not defined in
>relation to the efforts of the manager.
>
>(In this particular case, one might well ask what Cramer's goals are.
>They would seem to be to have fun and gather fame. Since it seems that
>he is eminently successful on both counts one is forced to acknowledge
>that he is doing a good job.)
>
>As for the challenge to chartists in the paper's conclusion, they too
>should be measured individually against their goals, not random
>portfolios. Why this emphasis on goals? Because this goals are what
>investors pay for. They may use past performance as a gage to
>ascertain whether the goal is obtainable, but by and large investors
>pay for specified goals and retain or fire managers on whether those
>goals are met. Other assessment alternatives matter little, even if
>they are 'better', unless investors agree and contract for them. This
>is after all a contractual relationship and results need to be
>evaluated in terms of expectations.
>
>    jab
>  
>


From david.jessop at ubs.com  Tue Oct  9 13:40:06 2007
From: david.jessop at ubs.com (david.jessop at ubs.com)
Date: Tue, 9 Oct 2007 12:40:06 +0100
Subject: [R-SIG-Finance] returns convention
In-Reply-To: <mailman.1.1191924001.1317.r-sig-finance@stat.math.ethz.ch>
References: <mailman.1.1191924001.1317.r-sig-finance@stat.math.ethz.ch>
Message-ID: <25492B412B325B4FB1FED95013D3E5CE03EF8AD2@NLDNC105PEX1.ubsw.net>

The answer of "it depends" is right.  An example of when simple returns
(i.e. price (t) / price (t-1)) are often used is in calculating tracking
error.  A related question is in calculating relative returns - should
you do the ratio of returns or the difference?

Regards,

David Jessop


Date: Mon, 08 Oct 2007 21:23:21 +1000
From: paul sorenson <sf at metrak.com>
Subject: [R-SIG-Finance] returns convention
To: r-finance <r-sig-finance at stat.math.ethz.ch>
Message-ID: <470A1329.2040102 at metrak.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Is it usually assumed that references to "returns" are calculated as
diff(log(prices))?  Compared with say the simple fractional change from
one price to the next?

For example, in PerformanceAnalytics I notice that the default value of
CalculateReturns is diff(log(prices)).

I guess it probably doesn't matter much either way for small changes, I
just wanted to know if there was some common convention when I see an R
function that expects a returns vector.

cheers




This communication is issued by UBS AG or an affiliate ("UBS") to
institutional investors only. It is for informational purposes and is
not guaranteed as to accuracy, nor is it a complete statement of the
financial products or markets referred to. Opinions expressed are
subject to change without notice and may differ or be contrary to the
opinions or recommendations expressed by other business areas or
groups of UBS as a result of using different assumptions and criteria.
UBS may maintain long or short positions in the financial instruments
referred to and may transact in them as principal or agent. Unless
stated specifically otherwise, this is not a recommendation, offer or
solicitation to buy or sell and any prices or quotations contained
herein are indicative only. UBS may provide investment banking and
other services to, and/or its officers/employees may serve as
directors of, the companies referred to in this material. To the
extent permitted by law, UBS does not accept any liability arising
from the use of this communication. Additional information or UBS
Investment Research is available upon request.

Copyright 2006 UBS.  All rights reserved.  Intended for recipient only
and not for further distribution without the consent of UBS.


UBS Limited is a company registered in England & Wales under company
number 2035362, whose registered office is at 1 Finsbury Avenue,
London, EC2M 2PP, United Kingdom.

UBS AG (London Branch) is registered as a branch of a foreign company
under number BR004507, whose registered office is at
1 Finsbury Avenue, London, EC2M 2PP, United Kingdom.

UBS Clearing and Execution Services Limited is a company registered
in England & Wales under company number 03123037, whose registered
office is at 1 Finsbury Avenue, London, EC2M 2PP, United Kingdom.


From bbands at gmail.com  Tue Oct  9 16:26:28 2007
From: bbands at gmail.com (BBands)
Date: Tue, 9 Oct 2007 07:26:28 -0700
Subject: [R-SIG-Finance] Burns on Cramer
In-Reply-To: <470B6646.5060706@burns-stat.com>
References: <6e8360ad0710081525k1e8cacf3q9162c793e602b7f8@mail.gmail.com>
	<470B6646.5060706@burns-stat.com>
Message-ID: <6e8360ad0710090726q31c96094uacd620dfbecd730c@mail.gmail.com>

On 10/9/07, Patrick Burns <patrick at burns-stat.com> wrote:

> I think goals are of secondary rather than primary importance.
> Reading my horoscope in the newspaper may satisfy my goal
> of knowing how to run my life, but I think a more important
> question is whether that is the action of a rational person or not.

Ah, so we come to a divide. I suspect that in general we'll find
practitioners on one side and analysts on the other. In any case, I
got a lot out of a day spent thinking about it. Thanks for your
papers, always interesting.

As for your poker analogy, since the game most often comes down to a
few, or less, key hands, I'd guess that overall results have better
information than how the player performs on an average hand.

By-the-by, we've been using bootstrap distributions to evaluate the
probability of success for certain trades for a long time. Although we
are happy with this approach, the issue to me is getting enough
different kinds of environments in to the bootstrap sample. To wit, a
sample composed of daily US stock market data from 2002 and 2003,
yields rather different results than a sample composed of data from
2005 and 2006. We are currently using trailing two years, but there
must be a better method of sampling.

All the best,

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From patrick at burns-stat.com  Tue Oct  9 17:19:37 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 09 Oct 2007 16:19:37 +0100
Subject: [R-SIG-Finance] Burns on Cramer
In-Reply-To: <6e8360ad0710090726q31c96094uacd620dfbecd730c@mail.gmail.com>
References: <6e8360ad0710081525k1e8cacf3q9162c793e602b7f8@mail.gmail.com>	<470B6646.5060706@burns-stat.com>
	<6e8360ad0710090726q31c96094uacd620dfbecd730c@mail.gmail.com>
Message-ID: <470B9C09.3@burns-stat.com>

BBands wrote:

>On 10/9/07, Patrick Burns <patrick at burns-stat.com> wrote:
>
>  
>
>>I think goals are of secondary rather than primary importance.
>>Reading my horoscope in the newspaper may satisfy my goal
>>of knowing how to run my life, but I think a more important
>>question is whether that is the action of a rational person or not.
>>    
>>
>
>Ah, so we come to a divide. I suspect that in general we'll find
>practitioners on one side and analysts on the other. In any case, I
>got a lot out of a day spent thinking about it. Thanks for your
>papers, always interesting.
>
>As for your poker analogy, since the game most often comes down to a
>few, or less, key hands, I'd guess that overall results have better
>information than how the player performs on an average hand.
>  
>

I'm certainly no expert in poker so you may have it right for
poker, but I would rather entrust my pension to a fund manager
that has shown consistent positive skill rather than one showing
erattic skill.

>By-the-by, we've been using bootstrap distributions to evaluate the
>probability of success for certain trades for a long time. Although we
>are happy with this approach, the issue to me is getting enough
>different kinds of environments in to the bootstrap sample. To wit, a
>sample composed of daily US stock market data from 2002 and 2003,
>yields rather different results than a sample composed of data from
>2005 and 2006. We are currently using trailing two years, but there
>must be a better method of sampling.
>  
>

The market changes over time.  You might note that I emphasize
the time period when speaking of exhibiting skill.  It seems to me
that there is no alternative but to do the testing over several periods.
I would, of course, be keen to hear other solutions.

Pat

>All the best,
>
>    jab
>  
>


From ngottlieb at marinercapital.com  Tue Oct  9 19:20:49 2007
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Tue, 9 Oct 2007 13:20:49 -0400
Subject: [R-SIG-Finance] Black -Litterman Model
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D880151E35D@500MAIL.goldbox.com>

Does anyone know if the Black Litterman Model for Optimization has been
implemented in R?

If in S would help also, and if in S how mushc would be involved in
converting into an R package?

Thanks,
Neil
--------------------------------------------------------



This information is being sent at the recipient's reques...{{dropped:16}}


From brian at braverock.com  Tue Oct  9 19:41:19 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 09 Oct 2007 12:41:19 -0500
Subject: [R-SIG-Finance] Black -Litterman Model
In-Reply-To: <0946E293C7C22A45A0E33BA14FAA8D880151E35D@500MAIL.goldbox.com>
References: <0946E293C7C22A45A0E33BA14FAA8D880151E35D@500MAIL.goldbox.com>
Message-ID: <470BBD3F.4090106@braverock.com>

ngottlieb at marinercapital.com wrote:
> Does anyone know if the Black Litterman Model for Optimization has been
> implemented in R?
> 
> If in S would help also, and if in S how mushc would be involved in
> converting into an R package?

Black Litterman option pricing is certainly implemented in R.

However, I *think* you are talking about the extended Black Litterman 
models often applied to CAPM portfolio optimization.  I am not aware of 
anyone publishing code for an extended Black Litterman portfolio 
optimization model in R.  I've looked at it a couple of times, but never 
implemented it.

In the future, it would help if you would cite a reference that you are 
using as the basis for your question (web page, book, paper, etc.).  It 
would help list members to evaluate your request.

Regards,

   - Brian


From ngottlieb at marinercapital.com  Tue Oct  9 19:58:19 2007
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Tue, 9 Oct 2007 13:58:19 -0400
Subject: [R-SIG-Finance] Black -Litterman Model
In-Reply-To: <470BBD3F.4090106@braverock.com>
References: <0946E293C7C22A45A0E33BA14FAA8D880151E35D@500MAIL.goldbox.com>
	<470BBD3F.4090106@braverock.com>
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D880151E362@500MAIL.goldbox.com>

Thanks Brian for timely response.

Precisely... regarding CAPM and Optimization even though CAPM is a
little dated.
Good point about citing a reference, something I usually always do
when responding to questions from others.

Here is two citations that is a decent start regarding BL Model:
http://www.mccombs.utexas.edu/faculty/keith.brown/ChileMaterial/Idzorek%
20WP

http://www.fmpm.ch/files/2001_01_Drobetz.PDF


The more recent approach is Arbitrage Pricing Theory (APT):
CITATIONS:
Ross and Roll Paper:
http://www.cfapubs.org/doi/pdf/10.2469/faj.v51.n1.1868?cookieSet=1

William Goetzman Comments on APT:
http://viking.som.yale.edu/will/finman540/classnotes/class6.html

One more on APT
http://www.ny.frb.org/research/economists/wang/APT-Huberman-Wang.pdf


Regards,
Neil



 

-----Original Message-----
From: Brian G. Peterson [mailto:brian at braverock.com] 
Sent: Tuesday, October 09, 2007 1:41 PM
To: Gottlieb, Neil; r-sig-Finance
Subject: Re: [R-SIG-Finance] Black -Litterman Model

ngottlieb at marinercapital.com wrote:
> Does anyone know if the Black Litterman Model for Optimization has 
> been implemented in R?
> 
> If in S would help also, and if in S how mushc would be involved in 
> converting into an R package?

Black Litterman option pricing is certainly implemented in R.

However, I *think* you are talking about the extended Black Litterman
models often applied to CAPM portfolio optimization.  I am not aware of
anyone publishing code for an extended Black Litterman portfolio
optimization model in R.  I've looked at it a couple of times, but never
implemented it.

In the future, it would help if you would cite a reference that you are
using as the basis for your question (web page, book, paper, etc.).  It
would help list members to evaluate your request.

Regards,

   - Brian
--------------------------------------------------------



This information is being sent at the recipient's reques...{{dropped:16}}


From m_olshansky at yahoo.com  Wed Oct 10 04:27:59 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Tue, 9 Oct 2007 19:27:59 -0700 (PDT)
Subject: [R-SIG-Finance] American basket options
In-Reply-To: <5e64e5be0710090259y13b50523n7607990f55e8201@mail.gmail.com>
Message-ID: <965238.3160.qm@web32202.mail.mud.yahoo.com>

Thank you!

I heard about that method. Now I will check it more
carefully. 
Is is the only Monte Carlo based method?
Any idea about it's accuracy?

As to programming, I think that there is a version of
it in QuantLib (the C++ version). Has anybody used it?

I will share my experiences with the list.

Regards,

Moshe.

--- Wojciech Slusarski <wojciech.slusarski at gmail.com>
wrote:

> There is an algorithm called OLS Monte Carlo, or
> Longstaff-Schwarz
> algorithm for valuation of american/bermudan options
> using MC method,
> though it can be a bit tricky to programm that for a
> portfolio of 10
> securities and be a bit unstable, though worth of
> trying. If the
> dividends are not high, it should not differ much
> from a european
> option priced using Monte Carlo. If dividends are
> high, then the price
> should be slightly higher.
> 
> Regards,
> Wojciech ?lusarski
> 
> 
> 2007/10/9, Moshe Olshansky <m_olshansky at yahoo.com>:
> > This is an OTC traded option.
> >
> > For a European option one can estimate the
> covariance
> > matrix and then use Monte Carlo (taking into
> account
> > the dividends for each stock). This is pretty
> > straightforward (well, there may be many ways to
> > estimate the covariance matrix but let's use the
> > simplest one).
> >
> > Regards,
> >
> > Moshe.
> >
> > --- Krishna Kumar <kriskumar at earthlink.net> wrote:
> >
> > > I am just curious as to if this is being traded
> in
> > > some market ?.
> > >
> > > This is probably not very helpful but I don't
> think
> > > a European style
> > > basket is there in the existing packages.
> European
> > > style baskets are
> > > themselves tricky if you want to get the basket
> > > smile right etc.
> > > American style baskets will be messy.
> > >
> > > Cheers
> > > Krishna
> > >
> > >
> > >
> > >
> > > Moshe Olshansky wrote:
> > > > Hello,
> > > >
> > > > Is there any R code which allows to calculate
> the
> > > > price of an American basket option (option on
> a
> > > price
> > > > of a portfolio)?
> > > > If yes, are there any references to how
> accurate
> > > these
> > > > calculations are?
> > > > If no, can anybody recommend a relatively easy
> to
> > > use
> > > > software doing this?
> > > >
> > > > Are there any non Monte Carlo methods to
> compute
> > > (even
> > > > roughly) the price on an American basket put
> > > option on
> > > > a portfolio of 10 dividend paying stocks with
> 6
> > > months
> > > > maturity?
> > > >
> > > > Thank you in advance,
> > > >
> > > > Moshe.
> > > >
> > > >
> _______________________________________________
> > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > >
> > >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > -- Subscriber-posting only.
> > > > -- If you want to post, subscribe first.
> > > >
> > > >
> > >
> > >
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From wojciech.slusarski at gmail.com  Wed Oct 10 08:58:30 2007
From: wojciech.slusarski at gmail.com (Wojciech Slusarski)
Date: Wed, 10 Oct 2007 08:58:30 +0200
Subject: [R-SIG-Finance] American basket options
In-Reply-To: <965238.3160.qm@web32202.mail.mud.yahoo.com>
References: <5e64e5be0710090259y13b50523n7607990f55e8201@mail.gmail.com>
	<965238.3160.qm@web32202.mail.mud.yahoo.com>
Message-ID: <5e64e5be0710092358q24ea251r93d40c96fdb4a49e@mail.gmail.com>

I used it for american-asian options (also called hawaiian) and it was
quite stable. though there are two dimensions only of the problem -
spot price and average strike being path dependant. A friend of mine
used that for pricing bermudan swaptions, where you have to model the
whole interest rate term structure which results in higher
dimensionality and said that sometimes it gives strange results and
sometimes spurious. In the original L-S algorithm you conduct
regression only on paths on which the option is in-the-money. He
extended that, by runing regression on all paths. The main problem is
the selection of proper polynomials for the regression. Instead of
that, he was splitting the space for equally sized small pieces and
was fitting linear model. It was providing fine results, though was a
bit time consuming.

Best regards,
Wojciech

2007/10/10, Moshe Olshansky <m_olshansky at yahoo.com>:
> Thank you!
>
> I heard about that method. Now I will check it more
> carefully.
> Is is the only Monte Carlo based method?
> Any idea about it's accuracy?
>
> As to programming, I think that there is a version of
> it in QuantLib (the C++ version). Has anybody used it?
>
> I will share my experiences with the list.
>
> Regards,
>
> Moshe.
>
> --- Wojciech Slusarski <wojciech.slusarski at gmail.com>
> wrote:
>
> > There is an algorithm called OLS Monte Carlo, or
> > Longstaff-Schwarz
> > algorithm for valuation of american/bermudan options
> > using MC method,
> > though it can be a bit tricky to programm that for a
> > portfolio of 10
> > securities and be a bit unstable, though worth of
> > trying. If the
> > dividends are not high, it should not differ much
> > from a european
> > option priced using Monte Carlo. If dividends are
> > high, then the price
> > should be slightly higher.
> >
> > Regards,
> > Wojciech ?lusarski
> >
> >
> > 2007/10/9, Moshe Olshansky <m_olshansky at yahoo.com>:
> > > This is an OTC traded option.
> > >
> > > For a European option one can estimate the
> > covariance
> > > matrix and then use Monte Carlo (taking into
> > account
> > > the dividends for each stock). This is pretty
> > > straightforward (well, there may be many ways to
> > > estimate the covariance matrix but let's use the
> > > simplest one).
> > >
> > > Regards,
> > >
> > > Moshe.
> > >
> > > --- Krishna Kumar <kriskumar at earthlink.net> wrote:
> > >
> > > > I am just curious as to if this is being traded
> > in
> > > > some market ?.
> > > >
> > > > This is probably not very helpful but I don't
> > think
> > > > a European style
> > > > basket is there in the existing packages.
> > European
> > > > style baskets are
> > > > themselves tricky if you want to get the basket
> > > > smile right etc.
> > > > American style baskets will be messy.
> > > >
> > > > Cheers
> > > > Krishna
> > > >
> > > >
> > > >
> > > >
> > > > Moshe Olshansky wrote:
> > > > > Hello,
> > > > >
> > > > > Is there any R code which allows to calculate
> > the
> > > > > price of an American basket option (option on
> > a
> > > > price
> > > > > of a portfolio)?
> > > > > If yes, are there any references to how
> > accurate
> > > > these
> > > > > calculations are?
> > > > > If no, can anybody recommend a relatively easy
> > to
> > > > use
> > > > > software doing this?
> > > > >
> > > > > Are there any non Monte Carlo methods to
> > compute
> > > > (even
> > > > > roughly) the price on an American basket put
> > > > option on
> > > > > a portfolio of 10 dividend paying stocks with
> > 6
> > > > months
> > > > > maturity?
> > > > >
> > > > > Thank you in advance,
> > > > >
> > > > > Moshe.
> > > > >
> > > > >
> > _______________________________________________
> > > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > >
> > > >
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > -- Subscriber-posting only.
> > > > > -- If you want to post, subscribe first.
> > > > >
> > > > >
> > > >
> > > >
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > >
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
>


From DAvery at marketingleverage.com  Wed Oct 10 15:43:21 2007
From: DAvery at marketingleverage.com (Dan Avery)
Date: Wed, 10 Oct 2007 09:43:21 -0400
Subject: [R-SIG-Finance] Statistical Analysis of Financial Data in S-Plus
Message-ID: <E390E12F18C406459525A2C25BA9FC6707594C@SBS-MLI.MLI.local>

Hi all,
I have just started using R for investigating stock returns. I bought
the Carmona book about a year ago and have finally got comfortable
enough with R to begin going through the book. I've searched on the
R-Help mailing list and the achieves of this list but don't see anything
on using the Evanesce library in R.  A few questions:

1) is it possible to use this library in R - if so how?
2) are there other R packages that provide the same functions as
Evanesce? 
3) are there other books that are better suited to R and learning this
type of analysis?

Thanks in advance.

Dan Avery


From ngottlieb at marinercapital.com  Wed Oct 10 16:32:45 2007
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Wed, 10 Oct 2007 10:32:45 -0400
Subject: [R-SIG-Finance] Statistical Analysis of Financial Data in S-Plus
In-Reply-To: <E390E12F18C406459525A2C25BA9FC6707594C@SBS-MLI.MLI.local>
References: <E390E12F18C406459525A2C25BA9FC6707594C@SBS-MLI.MLI.local>
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D880151E36E@500MAIL.goldbox.com>

Dan:

I glanced at the referenced book you mentioned, seems to cover, based on
Table of contents,
Appropriate material.

Also on Amazon found this book which has some good stuff too
specifically
Referencing specific modeling such as Barra.
http://www.amazon.com/Modeling-Financial-Time-S-PLUS%C2%AE-Zivot/dp/0387
279652/ref=pd_bxgy_b_img_b/104-4888076-6520702


There really is no one book that fits it all, so I tend to build a
reference library of books.

Of course the primary book that is important to use S is Brian Ripley
and Venables book,
"Modern Applied Statistics with S-Plus", commonly referred to as MASS.

Another good book is Ruey S. Tsay's book, " Analysis of Financial Time
Series", also available
On Amazon.

Last it would behoove you to obtain a book such as Prof William Sharpe's
book on Investments.
http://www.amazon.com/Investments-6th-William-Sharpe/dp/0130101303/ref=s
r_1_4/104-4888076-6520702?ie=UTF8&s=books&qid=1192026000&sr=1-4

He is one of the best writers in this area of finance and makes the
subject matter approachable.

Good Luck! :)
Neil
 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Dan Avery
Sent: Wednesday, October 10, 2007 9:43 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Statistical Analysis of Financial Data in
S-Plus

Hi all,
I have just started using R for investigating stock returns. I bought
the Carmona book about a year ago and have finally got comfortable
enough with R to begin going through the book. I've searched on the
R-Help mailing list and the achieves of this list but don't see anything
on using the Evanesce library in R.  A few questions:

1) is it possible to use this library in R - if so how?
2) are there other R packages that provide the same functions as
Evanesce? 
3) are there other books that are better suited to R and learning this
type of analysis?

Thanks in advance.

Dan Avery

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.
--------------------------------------------------------



This information is being sent at the recipient's reques...{{dropped:16}}


From m_olshansky at yahoo.com  Thu Oct 11 01:51:29 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 10 Oct 2007 16:51:29 -0700 (PDT)
Subject: [R-SIG-Finance] American basket options
In-Reply-To: <5e64e5be0710092358q24ea251r93d40c96fdb4a49e@mail.gmail.com>
Message-ID: <109130.64786.qm@web32207.mail.mud.yahoo.com>

Dear Wojciech,

Thank you very much for your note!

As I understand, the polynomial regression is used in
Longstaff-Schwarz method to predict the expected
return from keeping the option between stages k and
k+1 (which allows one to decide whether to exercise it
at stage k), so any other type of prediction can be
used (like regression trees, neural networks, etc.). 
Has anyone tried to run a "mini Monte Carlo" from
stage k to k+1 (for every k from 1 to N) in order to
reduce the variance of expected returns if continued
to stage k+1?
In my case I am not concerned about the speed (even a
few hours for one run will be OK - but certainly not a
few millenniums!).

Regards,

Moshe.

--- Wojciech Slusarski <wojciech.slusarski at gmail.com>
wrote:

> I used it for american-asian options (also called
> hawaiian) and it was
> quite stable. though there are two dimensions only
> of the problem -
> spot price and average strike being path dependant.
> A friend of mine
> used that for pricing bermudan swaptions, where you
> have to model the
> whole interest rate term structure which results in
> higher
> dimensionality and said that sometimes it gives
> strange results and
> sometimes spurious. In the original L-S algorithm
> you conduct
> regression only on paths on which the option is
> in-the-money. He
> extended that, by runing regression on all paths.
> The main problem is
> the selection of proper polynomials for the
> regression. Instead of
> that, he was splitting the space for equally sized
> small pieces and
> was fitting linear model. It was providing fine
> results, though was a
> bit time consuming.
> 
> Best regards,
> Wojciech
> 
> 2007/10/10, Moshe Olshansky <m_olshansky at yahoo.com>:
> > Thank you!
> >
> > I heard about that method. Now I will check it
> more
> > carefully.
> > Is is the only Monte Carlo based method?
> > Any idea about it's accuracy?
> >
> > As to programming, I think that there is a version
> of
> > it in QuantLib (the C++ version). Has anybody used
> it?
> >
> > I will share my experiences with the list.
> >
> > Regards,
> >
> > Moshe.
> >
> > --- Wojciech Slusarski
> <wojciech.slusarski at gmail.com>
> > wrote:
> >
> > > There is an algorithm called OLS Monte Carlo, or
> > > Longstaff-Schwarz
> > > algorithm for valuation of american/bermudan
> options
> > > using MC method,
> > > though it can be a bit tricky to programm that
> for a
> > > portfolio of 10
> > > securities and be a bit unstable, though worth
> of
> > > trying. If the
> > > dividends are not high, it should not differ
> much
> > > from a european
> > > option priced using Monte Carlo. If dividends
> are
> > > high, then the price
> > > should be slightly higher.
> > >
> > > Regards,
> > > Wojciech ?lusarski
> > >
> > >
> > > 2007/10/9, Moshe Olshansky
> <m_olshansky at yahoo.com>:
> > > > This is an OTC traded option.
> > > >
> > > > For a European option one can estimate the
> > > covariance
> > > > matrix and then use Monte Carlo (taking into
> > > account
> > > > the dividends for each stock). This is pretty
> > > > straightforward (well, there may be many ways
> to
> > > > estimate the covariance matrix but let's use
> the
> > > > simplest one).
> > > >
> > > > Regards,
> > > >
> > > > Moshe.
> > > >
> > > > --- Krishna Kumar <kriskumar at earthlink.net>
> wrote:
> > > >
> > > > > I am just curious as to if this is being
> traded
> > > in
> > > > > some market ?.
> > > > >
> > > > > This is probably not very helpful but I
> don't
> > > think
> > > > > a European style
> > > > > basket is there in the existing packages.
> > > European
> > > > > style baskets are
> > > > > themselves tricky if you want to get the
> basket
> > > > > smile right etc.
> > > > > American style baskets will be messy.
> > > > >
> > > > > Cheers
> > > > > Krishna
> > > > >
> > > > >
> > > > >
> > > > >
> > > > > Moshe Olshansky wrote:
> > > > > > Hello,
> > > > > >
> > > > > > Is there any R code which allows to
> calculate
> > > the
> > > > > > price of an American basket option (option
> on
> > > a
> > > > > price
> > > > > > of a portfolio)?
> > > > > > If yes, are there any references to how
> > > accurate
> > > > > these
> > > > > > calculations are?
> > > > > > If no, can anybody recommend a relatively
> easy
> > > to
> > > > > use
> > > > > > software doing this?
> > > > > >
> > > > > > Are there any non Monte Carlo methods to
> > > compute
> > > > > (even
> > > > > > roughly) the price on an American basket
> put
> > > > > option on
> > > > > > a portfolio of 10 dividend paying stocks
> with
> > > 6
> > > > > months
> > > > > > maturity?
> > > > > >
> > > > > > Thank you in advance,
> > > > > >
> > > > > > Moshe.
> > > > > >
> > > > > >
> > > _______________________________________________
> > > > > > R-SIG-Finance at stat.math.ethz.ch mailing
> list
> > > > > >
> > > > >
> > >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > > -- Subscriber-posting only.
> > > > > > -- If you want to post, subscribe first.
> > > > > >
> > > > > >
> > > > >
> > > > >
> > > >
> > > >
> _______________________________________________
> > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > >
> > >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > -- Subscriber-posting only.
> > > > -- If you want to post, subscribe first.
> > > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> >
> >
>


From nocman43202 at yahoo.com  Thu Oct 11 15:25:58 2007
From: nocman43202 at yahoo.com (Matt Slezak)
Date: Thu, 11 Oct 2007 06:25:58 -0700 (PDT)
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 41,
	Issue 8: American Basket Options
In-Reply-To: <mailman.0.1192010401.16272.r-sig-finance@stat.math.ethz.ch>
Message-ID: <69331.51065.qm@web51508.mail.re2.yahoo.com>

American Basket Options

Here is an efficient method I used to value European
options on a basket of securities.  Maybe you can
piece together R code for the American basket option. 
I have done something similar in Excel using
QuantLibXL.  First calculate the correlation matrix
for the assets in the portfolio.  Next, do a Cholesky
decomposition on this matrix.  Generate 1000 random
numbers for each asset in the portfolio. Transform
these into correlated random numbers by multiplying
each by its factors from the Cholesky matrix.  Next
generate quasi-random sequences (Halton or other,
range 0 to 1) and pull these numbers from the
cumalative normal distribution.  Next you create
Geometric Brownian Motions for each asset using the
respective correlated random number, the asset's
volatility, and the asset's drift.  Run the 1000
simulations for each asset over the time horizon, then
calculate the net present value (NPV) of the payoffs
for the whole portfolio (which is the maximum of the
postive NPV or 0, since the long option cannot have a
negative return).  The average payoff of the portfolio
is the value of the basket option.  

If someone can add how one determines whether the
option is exercised in each stage of the Monte Carlo
simulation for American exercise it would be
appreciated - I used this method for European options.

Hope this is helpful -Matt Slezak
--- r-sig-finance-request at stat.math.ethz.ch wrote:

> Send R-SIG-Finance mailing list submissions to
> 	r-sig-finance at stat.math.ethz.ch
> 
> To subscribe or unsubscribe via the World Wide Web,
> visit
> 	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> or, via email, send a message with subject or body
> 'help' to
> 	r-sig-finance-request at stat.math.ethz.ch
> 
> You can reach the person managing the list at
> 	r-sig-finance-owner at stat.math.ethz.ch
> 
> When replying, please edit your Subject line so it
> is more specific
> than "Re: Contents of R-SIG-Finance digest..."
> > Today's Topics:
> 
>    1. Re: American basket options (Dale Smith)
>    2. Re: Burns on Cramer (Patrick Burns)
>    3. Re: returns convention (david.jessop at ubs.com)
>    4. Re: Burns on Cramer (BBands)
>    5. Re: Burns on Cramer (Patrick Burns)
>    6. Black -Litterman Model
> (ngottlieb at marinercapital.com)
>    7. Re: Black -Litterman Model (Brian G. Peterson)
>    8. Re: Black -Litterman Model
> (ngottlieb at marinercapital.com)
>    9. Re: American basket options (Moshe Olshansky)
>   10. Re: American basket options (Wojciech
> Slusarski)
> > From: "Dale Smith" <dsmith at viciscapital.com>
> To: "Wojciech Slusarski"
> <wojciech.slusarski at gmail.com>,
> 	<r-sig-finance at stat.math.ethz.ch>
> Date: Tue, 9 Oct 2007 07:05:07 -0400
> Subject: Re: [R-SIG-Finance] American basket options
> 
> For baskets of stocks larger than three or so, Monte
> Carlo methods outperform the best finite difference
> code. As mentioned below, there are Monte Carlo
> algorithms for the American case.
> 
> Dale Smith, Ph.D.
> Vicis Capital, LLC
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On
> Behalf Of Wojciech Slusarski
> Sent: Tuesday, October 09, 2007 6:00 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] American basket options
> 
> There is an algorithm called OLS Monte Carlo, or
> Longstaff-Schwarz
> algorithm for valuation of american/bermudan options
> using MC method,
> though it can be a bit tricky to programm that for a
> portfolio of 10
> securities and be a bit unstable, though worth of
> trying. If the
> dividends are not high, it should not differ much
> from a european
> option priced using Monte Carlo. If dividends are
> high, then the price
> should be slightly higher.
> 
> Regards,
> Wojciech ?lusarski
> 
> 
> 2007/10/9, Moshe Olshansky <m_olshansky at yahoo.com>:
> > This is an OTC traded option.
> >
> > For a European option one can estimate the
> covariance
> > matrix and then use Monte Carlo (taking into
> account
> > the dividends for each stock). This is pretty
> > straightforward (well, there may be many ways to
> > estimate the covariance matrix but let's use the
> > simplest one).
> >
> > Regards,
> >
> > Moshe.
> >
> > --- Krishna Kumar <kriskumar at earthlink.net> wrote:
> >
> > > I am just curious as to if this is being traded
> in
> > > some market ?.
> > >
> > > This is probably not very helpful but I don't
> think
> > > a European style
> > > basket is there in the existing packages.
> European
> > > style baskets are
> > > themselves tricky if you want to get the basket
> > > smile right etc.
> > > American style baskets will be messy.
> > >
> > > Cheers
> > > Krishna
> > >
> > >
> > >
> > >
> > > Moshe Olshansky wrote:
> > > > Hello,
> > > >
> > > > Is there any R code which allows to calculate
> the
> > > > price of an American basket option (option on
> a
> > > price
> > > > of a portfolio)?
> > > > If yes, are there any references to how
> accurate
> > > these
> > > > calculations are?
> > > > If no, can anybody recommend a relatively easy
> to
> > > use
> > > > software doing this?
> > > >
> > > > Are there any non Monte Carlo methods to
> compute
> > > (even
> > > > roughly) the price on an American basket put
> > > option on
> > > > a portfolio of 10 dividend paying stocks with
> 6
> > > months
> > > > maturity?
> > > >
> > > > Thank you in advance,
> > > >
> > > > Moshe.
> > > >
> > > >
> _______________________________________________
> > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > >
> > >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > -- Subscriber-posting only.
> > > > -- If you want to post, subscribe first.
> > > >
> > > >
> > >
> > >
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> All e-mail sent to or from this address will be
> received or otherwise recorded by Vicis Capital, LLC
> and is subject to archival, monitoring and/or
> review, by and/or disclosure to, someone other than
> the recipient.  This message is intended only for
> the use of the person(s) ("intended recipient") to
> whom it is addressed.  It may contain information
> that is privileged and confidential.  If you are not
> the intended recipient, please contact the sender as
> soon as possible and delete the message without
> reading it or making a copy.  Any dissemination,
> distribution, copying, or other use of this message
> or any of its content by any person other than the
> intended recipient is strictly prohibited.  Vicis
> Capital, LLC only transacts business in states where
> it is properly registered or notice filed, or
> excluded or exempted from registration or notice
> filing requirements.
> 
> 
> > From: Patrick Burns <patrick at burns-stat.com>
> CC: R-sig-finance <r-sig-finance at stat.math.ethz.ch>,
> 	The New Markets List <marketslist at googlegroups.com>
> To: BBands <bbands at gmail.com>
> Date: Tue, 09 Oct 2007 12:30:14 +0100
> Subject: Re: [R-SIG-Finance] Burns on Cramer
> 
> John,
> 
> We definitely agree on one thing: Cramer's goals
> appear
> to be having fun and getting fame.  I have no way of
> knowing
> if he is having fun, but he has certainly succeeded
> with fame.
> (Disclosure: Two of my top goals are having fun and
> not getting
> famous.  At present I'm doing well with both these.)
> 
> I think goals are of secondary rather than primary
> importance.
> Reading my horoscope in the newspaper may satisfy my
> goal
> of knowing how to run my life, but I think a more
> important
> question is whether that is the action of a rational
> person or not.
> 
> What I am advocating is SCIENCE: What can we know?
> And
> then let's set about learning that.
> 
> What we can learn with random portfolios is whether
> or not a
> fund or a market commentator exhibits skill over
> some period of
> time.  (There are other things to learn with random
> portfolios as
> well, by the way.)
> 
> Once we have information on skill (which as far as I
> know is
> uniquely available through random portfolios), then
> we can address
> the issue of goals.  We can get a sense of how much
> skill was needed
> to meet the goal over the past period -- maybe a
> lot, maybe even
> negative skill would have done.  More importantly we
> get a hint of
> how we might best set about to achieve our goals in
> the future.
> 
> If we focus on goals rather than skill, we have very
> little basis on
> which to carry past performance into the future.
> 
> If you wanted to pick a poker player to back, would
> you merely
> look at the player's winnings or would you try to
> evaluate if he or
> she outperformed given the hands that were dealt?
> 
> In terms of what is contracted for, I have
> previously suggested that a
> good choice is to contract for exhibited skill as
> determined by random
> portfolios.  This rewards fund managers that do
> exhibit skill, and allows
> the investor a mechanism that comes closer to paying
> for what they get.
> 
> Pat
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> 
> 
> BBands wrote:
> 
> >I read this paper,
>
>http://www.burns-stat.com/pages/Working/cramer_vs_pseudocramer.pdf,
> >and found it to be interesting, however, the idea
> of judging
> >real-world results against artificially constructed
> portfolios leaves
> >me cold. The only reasonable way of judging
> performance is against
> >stated goals. Goals tend to be specified in terms
> of returns (relative
> >or absolute),  variability (volatility, draw down
> and so forth) or a
> >combination (Sharpe, Sortino...) and the only
> reasonable question is
> >to what extent the goals are met. Judgment against
> a basket of random
> >portfolios, however cleverly constructed, is simply
> not defined in
> >relation to the efforts of the manager.
> >
> >(In this particular case, one might well ask what
> Cramer's goals are.
> >They would seem to be to have fun and gather fame.
> Since it seems that
> >he is eminently successful on both counts one is
> forced to acknowledge
> >that he is doing a good job.)
> >
> >As for the challenge to chartists in the paper's
> conclusion, they too
> >should be measured individually against their
> goals, not random
> >portfolios. Why this emphasis on goals? Because
> this goals are what
> >investors pay for. They may use past performance as
> a gage to
> >ascertain whether the goal is obtainable, but by
> and large investors
> >pay for specified goals and retain or fire managers
> on whether those
> >goals are met. Other assessment alternatives matter
> little, even if
> >they are 'better', unless investors agree and
> contract for them. This
> >is after all a contractual relationship and results
> need to be
> >evaluated in terms of expectations.
> >
> >    jab
> >  
> >
> 
> 
> > From: <david.jessop at ubs.com>
> To: <r-sig-finance at stat.math.ethz.ch>
> Date: Tue, 9 Oct 2007 12:40:06 +0100
> Subject: Re: [R-SIG-Finance] returns convention
> 
> The answer of "it depends" is right.  An example of
> when simple returns
> (i.e. price (t) / price (t-1)) are often used is in
> calculating tracking
> error.  A related question is in calculating
> relative returns - should
> you do the ratio of returns or the difference?
> 
> Regards,
> 
> David Jessop
> 
> 
> Date: Mon, 08 Oct 2007 21:23:21 +1000
> From: paul sorenson <sf at metrak.com>
> Subject: [R-SIG-Finance] returns convention
> To: r-finance <r-sig-finance at stat.math.ethz.ch>
> Message-ID: <470A1329.2040102 at metrak.com>
> Content-Type: text/plain; charset=ISO-8859-1;
> format=flowed
> 
> Is it usually assumed that references to "returns"
> are calculated as
> diff(log(prices))?  Compared with say the simple
> fractional change from
> one price to the next?
> 
> For example, in PerformanceAnalytics I notice that
> the default value of
> CalculateReturns is diff(log(prices)).
> 
> I guess it probably doesn't matter much either way
> for small changes, I
> just wanted to know if there was some common
> convention when I see an R
> function that expects a returns vector.
> 
> cheers
> 
> 
> 
> 
> This communication is issued by UBS AG or an
> affiliate ("UBS") to
> institutional investors only. It is for
> informational purposes and is
> not guaranteed as to accuracy, nor is it a complete
> statement of the
> financial products or markets referred to. Opinions
> expressed are
> subject to change without notice and may differ or
> be contrary to the
> opinions or recommendations expressed by other
> business areas or
> groups of UBS as a result of using different
> assumptions and criteria.
> UBS may maintain long or short positions in the
> financial instruments
> referred to and may transact in them as principal or
> agent. Unless
> stated specifically otherwise, this is not a
> recommendation, offer or
> solicitation to buy or sell and any prices or
> quotations contained
> herein are indicative only. UBS may provide
> investment banking and
> other services to, and/or its officers/employees may
> serve as
> directors of, the companies referred to in this
> material. To the
> extent permitted by law, UBS does not accept any
> liability arising
> from the use of this communication. Additional
> information or UBS
> Investment Research is available upon request.
> 
> Copyright 2006 UBS.  All rights reserved.  Intended
> for recipient only
> and not for further distribution without the consent
> of UBS.
> 
> 
> UBS Limited is a company registered in England &
> Wales under company
> number 2035362, whose registered office is at 1
> Finsbury Avenue,
> London, EC2M 2PP, United Kingdom.
> 
> UBS AG (London Branch) is registered as a branch of
> a foreign company
> under number BR004507, whose registered office is at
> 1 Finsbury Avenue, London, EC2M 2PP, United Kingdom.
> 
> UBS Clearing and Execution Services Limited is a
> company registered
> in England & Wales under company number 03123037,
> whose registered
> office is at 1 Finsbury Avenue, London, EC2M 2PP,
> United Kingdom.
> 
> 
> > From: BBands <bbands at gmail.com>
> To: R-sig-finance <r-sig-finance at stat.math.ethz.ch>
> Date: Tue, 9 Oct 2007 07:26:28 -0700
> Subject: Re: [R-SIG-Finance] Burns on Cramer
> 
> On 10/9/07, Patrick Burns <patrick at burns-stat.com>
> wrote:
> 
> > I think goals are of secondary rather than primary
> importance.
> > Reading my horoscope in the newspaper may satisfy
> my goal
> > of knowing how to run my life, but I think a more
> important
> > question is whether that is the action of a
> rational person or not.
> 
> Ah, so we come to a divide. I suspect that in
> general we'll find
> practitioners on one side and analysts on the other.
> In any case, I
> got a lot out of a day spent thinking about it.
> Thanks for your
> papers, always interesting.
> 
> As for your poker analogy, since the game most often
> comes down to a
> few, or less, key hands, I'd guess that overall
> results have better
> information than how the player performs on an
> average hand.
> 
> By-the-by, we've been using bootstrap distributions
> to evaluate the
> probability of success for certain trades for a long
> time. Although we
> are happy with this approach, the issue to me is
> getting enough
> different kinds of environments in to the bootstrap
> sample. To wit, a
> sample composed of daily US stock market data from
> 2002 and 2003,
> yields rather different results than a sample
> composed of data from
> 2005 and 2006. We are currently using trailing two
> years, but there
> must be a better method of sampling.
> 
> All the best,
> 
>     jab
> -- 
> John Bollinger, CFA, CMT
> www.BollingerBands.com
> 
> If you advance far enough, you arrive at the
> beginning.
> 
> 
> > From: Patrick Burns <patrick at burns-stat.com>
> CC: R-sig-finance <r-sig-finance at stat.math.ethz.ch>
> To: BBands <bbands at gmail.com>
> Date: Tue, 09 Oct 2007 16:19:37 +0100
> Subject: Re: [R-SIG-Finance] Burns on Cramer
> 
> BBands wrote:
> 
> >On 10/9/07, Patrick Burns <patrick at burns-stat.com>
> wrote:
> >
> >  
> >
> >>I think goals are of secondary rather than primary
> importance.
> >>Reading my horoscope in the newspaper may satisfy
> my goal
> >>of knowing how to run my life, but I think a more
> important
> >>question is whether that is the action of a
> rational person or not.
> >>    
> >>
> >
> >Ah, so we come to a divide. I suspect that in
> general we'll find
> >practitioners on one side and analysts on the
> other. In any case, I
> >got a lot out of a day spent thinking about it.
> Thanks for your
> >papers, always interesting.
> >
> >As for your poker analogy, since the game most
> often comes down to a
> >few, or less, key hands, I'd guess that overall
> results have better
> >information than how the player performs on an
> average hand.
> >  
> >
> 
> I'm certainly no expert in poker so you may have it
> right for
> poker, but I would rather entrust my pension to a
> fund manager
> that has shown consistent positive skill rather than
> one showing
> erattic skill.
> 
> >By-the-by, we've been using bootstrap distributions
> to evaluate the
> >probability of success for certain trades for a
> long time. Although we
> >are happy with this approach, the issue to me is
> getting enough
> >different kinds of environments in to the bootstrap
> sample. To wit, a
> >sample composed of daily US stock market data from
> 2002 and 2003,
> >yields rather different results than a sample
> composed of data from
> >2005 and 2006. We are currently using trailing two
> years, but there
> >must be a better method of sampling.
> >  
> >
> 
> The market changes over time.  You might note that I
> emphasize
> the time period when speaking of exhibiting skill. 
> It seems to me
> that there is no alternative but to do the testing
> over several periods.
> I would, of course, be keen to hear other solutions.
> 
> Pat
> 
> >All the best,
> >
> >    jab
> >  
> >
> 
> 
> > From: <ngottlieb at marinercapital.com>
> To: <r-sig-finance at stat.math.ethz.ch>
> Date: Tue, 9 Oct 2007 13:20:49 -0400
> Subject: [R-SIG-Finance] Black -Litterman Model
> 
> Does anyone know if the Black Litterman Model for
> Optimization has been
> implemented in R?
> 
> If in S would help also, and if in S how mushc would
> be involved in
> converting into an R package?
> 
> Thanks,
> Neil
>
--------------------------------------------------------
> 
> 
> 
> This information is being sent at the recipient's
> reques...{{dropped:16}}
> 
> 
> > From: "Brian G. Peterson" <brian at braverock.com>
> To: ngottlieb at marinercapital.com,
> 	r-sig-Finance <r-sig-finance at stat.math.ethz.ch>
> Date: Tue, 09 Oct 2007 12:41:19 -0500
> Subject: Re: [R-SIG-Finance] Black -Litterman Model
> 
> ngottlieb at marinercapital.com wrote:
> > Does anyone know if the Black Litterman Model for
> Optimization has been
> > implemented in R?
> > 
> > If in S would help also, and if in S how mushc
> would be involved in
> > converting into an R package?
> 
> Black Litterman option pricing is certainly
> implemented in R.
> 
> However, I *think* you are talking about the
> extended Black Litterman 
> models often applied to CAPM portfolio optimization.
>  I am not aware of 
> anyone publishing code for an extended Black
> Litterman portfolio 
> optimization model in R.  I've looked at it a couple
> of times, but never 
> implemented it.
> 
> In the future, it would help if you would cite a
> reference that you are 
> using as the basis for your question (web page,
> book, paper, etc.).  It 
> would help list members to evaluate your request.
> 
> Regards,
> 
>    - Brian
> 
> 
> > From: <ngottlieb at marinercapital.com>
> To: <brian at braverock.com>,
> <r-sig-finance at stat.math.ethz.ch>
> Date: Tue, 9 Oct 2007 13:58:19 -0400
> Subject: Re: [R-SIG-Finance] Black -Litterman Model
> 
> Thanks Brian for timely response.
> 
> Precisely... regarding CAPM and Optimization even
> though CAPM is a
> little dated.
> Good point about citing a reference, something I
> usually always do
> when responding to questions from others.
> 
> Here is two citations that is a decent start
> regarding BL Model:
>
http://www.mccombs.utexas.edu/faculty/keith.brown/ChileMaterial/Idzorek%
> 20WP
> 
> http://www.fmpm.ch/files/2001_01_Drobetz.PDF
> 
> 
> The more recent approach is Arbitrage Pricing Theory
> (APT):
> CITATIONS:
> Ross and Roll Paper:
>
http://www.cfapubs.org/doi/pdf/10.2469/faj.v51.n1.1868?cookieSet=1
> 
> William Goetzman Comments on APT:
>
http://viking.som.yale.edu/will/finman540/classnotes/class6.html
> 
> One more on APT
>
http://www.ny.frb.org/research/economists/wang/APT-Huberman-Wang.pdf
> 
> 
> Regards,
> Neil
> 
> 
> 
>  
> 
> -----Original Message-----
> From: Brian G. Peterson [mailto:brian at braverock.com]
> 
> Sent: Tuesday, October 09, 2007 1:41 PM
> To: Gottlieb, Neil; r-sig-Finance
> Subject: Re: [R-SIG-Finance] Black -Litterman Model
> 
> ngottlieb at marinercapital.com wrote:
> > Does anyone know if the Black Litterman Model for
> Optimization has 
> > been implemented in R?
> > 
> > If in S would help also, and if in S how mushc
> would be involved in 
> > converting into an R package?
> 
> Black Litterman option pricing is certainly
> implemented in R.
> 
> However, I *think* you are talking about the
> extended Black Litterman
> models often applied to CAPM portfolio optimization.
>  I am not aware of
> anyone publishing code for an extended Black
> Litterman portfolio
> optimization model in R.  I've looked at it a couple
> of times, but never
> implemented it.
> 
> In the future, it would help if you would cite a
> reference that you are
> using as the basis for your question (web page,
> book, paper, etc.).  It
> would help list members to evaluate your request.
> 
> Regards,
> 
>    - Brian
>
--------------------------------------------------------
> 
> 
> 
> This information is being sent at the recipient's
> reques...{{dropped:16}}
> 
> 

> To: Wojciech Slusarski
> <wojciech.slusarski at gmail.com>,
> 	r-sig-finance at stat.math.ethz.ch
> Date: Tue, 9 Oct 2007 19:27:59 -0700 (PDT)
> Subject: Re: [R-SIG-Finance] American basket options
> 
> Thank you!
> 
> I heard about that method. Now I will check it more
> carefully. 
> Is is the only Monte Carlo based method?
> Any idea about it's accuracy?
> 
> As to programming, I think that there is a version
> of
> it in QuantLib (the C++ version). Has anybody used
> it?
> 
> I will share my experiences with the list.
> 
> Regards,
> 
> Moshe.
> 
> --- Wojciech Slusarski
> <wojciech.slusarski at gmail.com>
> wrote:
> 
> > There is an algorithm called OLS Monte Carlo, or
> > Longstaff-Schwarz
> > algorithm for valuation of american/bermudan
> options
> > using MC method,
> > though it can be a bit tricky to programm that for
> a
> > portfolio of 10
> > securities and be a bit unstable, though worth of
> > trying. If the
> > dividends are not high, it should not differ much
> > from a european
> > option priced using Monte Carlo. If dividends are
> > high, then the price
> > should be slightly higher.
> > 
> > Regards,
> > Wojciech ?lusarski
> > 
> > 
> > 2007/10/9, Moshe Olshansky

> > > This is an OTC traded option.
> > >
> > > For a European option one can estimate the
> > covariance
> > > matrix and then use Monte Carlo (taking into
> > account
> > > the dividends for each stock). This is pretty
> > > straightforward (well, there may be many ways to
> > > estimate the covariance matrix but let's use the
> > > simplest one).
> > >
> > > Regards,
> > >
> > > Moshe.
> > >
> > > --- Krishna Kumar <kriskumar at earthlink.net>
> wrote:
> > >
> > > > I am just curious as to if this is being
> traded
> > in
> > > > some market ?.
> > > >
> > > > This is probably not very helpful but I don't
> > think
> > > > a European style
> > > > basket is there in the existing packages.
> > European
> > > > style baskets are
> > > > themselves tricky if you want to get the
> basket
> > > > smile right etc.
> > > > American style baskets will be messy.
> > > >
> > > > Cheers
> > > > Krishna
> > > >
> > > >
> > > >
> > > >
> > > > Moshe Olshansky wrote:
> > > > > Hello,
> > > > >
> > > > > Is there any R code which allows to
> calculate
> > the
> > > > > price of an American basket option (option
> on
> > a
> > > > price
> > > > > of a portfolio)?
> > > > > If yes, are there any references to how
> > accurate
> > > > these
> > > > > calculations are?
> > > > > If no, can anybody recommend a relatively
> easy
> > to
> > > > use
> > > > > software doing this?
> > > > >
> > > > > Are there any non Monte Carlo methods to
> > compute
> > > > (even
> > > > > roughly) the price on an American basket put
> > > > option on
> > > > > a portfolio of 10 dividend paying stocks
> with
> > 6
> > > > months
> > > > > maturity?
> > > > >
> > > > > Thank you in advance,
> > > > >
> > > > > Moshe.
> > > > >
> > > > >
> > _______________________________________________
> > > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > >
> > > >
> >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > -- Subscriber-posting only.
> > > > > -- If you want to post, subscribe first.
> > > > >
> > > > >
> > > >
> > > >
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > >
> >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only. 
> > -- If you want to post, subscribe first.
> >
> 
> 
> > From: "Wojciech Slusarski"
> <wojciech.slusarski at gmail.com>
> CC: r-sig-finance at stat.math.ethz.ch

> Date: Wed, 10 Oct 2007 08:58:30 +0200
> Subject: Re: [R-SIG-Finance] American basket options
> 
> I used it for american-asian options (also called
> hawaiian) and it was
> quite stable. though there are two dimensions only
> of the problem -
> spot price and average strike being path dependant.
> A friend of mine
> used that for pricing bermudan swaptions, where you
> have to model the
> whole interest rate term structure which results in
> higher
> dimensionality and said that sometimes it gives
> strange results and
> sometimes spurious. In the original L-S algorithm
> you conduct
> regression only on paths on which the option is
> in-the-money. He
> extended that, by runing regression on all paths.
> The main problem is
> the selection of proper polynomials for the
> regression. Instead of
> that, he was splitting the space for equally sized
> small pieces and
> was fitting linear model. It was providing fine
> results, though was a
> bit time consuming.
> 
> Best regards,
> Wojciech
> 

> > Thank you!
> >
> > I heard about that method. Now I will check it
> more
> > carefully.
> > Is is the only Monte Carlo based method?
> > Any idea about it's accuracy?
> >
> > As to programming, I think that there is a version
> of
> > it in QuantLib (the C++ version). Has anybody used
> it?
> >
> > I will share my experiences with the list.
> >
> > Regards,
> >
> > Moshe.
> >
> > --- Wojciech Slusarski
> <wojciech.slusarski at gmail.com>
> > wrote:
> >
> > > There is an algorithm called OLS Monte Carlo, or
> > > Longstaff-Schwarz
> > > algorithm for valuation of american/bermudan
> options
> > > using MC method,
> > > though it can be a bit tricky to programm that
> for a
> > > portfolio of 10
> > > securities and be a bit unstable, though worth
> of
> > > trying. If the
> > > dividends are not high, it should not differ
> much
> > > from a european
> > > option priced using Monte Carlo. If dividends
> are
> > > high, then the price
> > > should be slightly higher.
> > >
> > > Regards,
> > > Wojciech ?lusarski
> > >
> > >
> > > 2007/10/9, Moshe Olshansky

> > > > This is an OTC traded option.
> > > >
> > > > For a European option one can estimate the
> > > covariance
> > > > matrix and then use Monte Carlo (taking into
> > > account
> > > > the dividends for each stock). This is pretty
> > > > straightforward (well, there may be many ways
> to
> > > > estimate the covariance matrix but let's use
> the
> > > > simplest one).
> > > >
> > > > Regards,
> > > >
> > > > Moshe.
> > > >
> > > > --- Krishna Kumar <kriskumar at earthlink.net>
> wrote:
> > > >
> > > > > I am just curious as to if this is being
> traded
> > > in
> > > > > some market ?.
> > > > >
> > > > > This is probably not very helpful but I
> don't
> > > think
> > > > > a European style
> > > > > basket is there in the existing packages.
> > > European
> > > > > style baskets are
> > > > > themselves tricky if you want to get the
> basket
> > > > > smile right etc.
> > > > > American style baskets will be messy.
> > > > >
> > > > > Cheers
> > > > > Krishna
> > > > >
> > > > >
> > > > >
> > > > >
> > > > > Moshe Olshansky wrote:
> > > > > > Hello,
> > > > > >
> > > > > > Is there any R code which allows to
> calculate
> > > the
> > > > > > price of an American basket option (option
> on
> > > a
> > > > > price
> > > > > > of a portfolio)?
> > > > > > If yes, are there any references to how
> > > accurate
> > > > > these
> > > > > > calculations are?
> > > > > > If no, can anybody recommend a relatively
> easy
> > > to
> > > > > use
> > > > > > software doing this?
> > > > > >
> > > > > > Are there any non Monte Carlo methods to
> > > compute
> > > > > (even
> > > > > > roughly) the price on an American basket
> put
> > > > > option on
> > > > > > a portfolio of 10 dividend paying stocks
> with
> > > 6
> > > > > months
> > > > > > maturity?
> > > > > >
> > > > > > Thank you in advance,
> > > > > >
> > > > > > Moshe.
> > > > > >
> > > > > >
> > > _______________________________________________
> > > > > > R-SIG-Finance at stat.math.ethz.ch mailing
> list
> > > > > >
> > > > >
> > >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > > > -- Subscriber-posting only.
> > > > > > -- If you want to post, subscribe first.
> > > > > >
> > > > > >
> > > > >
> > > > >
> > > >
> > > >
> _______________________________________________
> > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > >
> > >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > -- Subscriber-posting only.
> > > > -- If you want to post, subscribe first.
> > > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> >
> >
> 
> 
> > _______________________________________________
> R-SIG-Finance mailing list
> R-SIG-Finance at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From david.jessop at ubs.com  Thu Oct 11 19:51:02 2007
From: david.jessop at ubs.com (david.jessop at ubs.com)
Date: Thu, 11 Oct 2007 18:51:02 +0100
Subject: [R-SIG-Finance] London Quant Group
In-Reply-To: <46B5FBA1DDE8DE4E9B0A29FF20E1E6DD508A7566@NLDNC105PEX1.ubsw.net>
References: <mailman.1.1191924001.1317.r-sig-finance@stat.math.ethz.ch>
	<46B5FBA1DDE8DE4E9B0A29FF20E1E6DD508A7566@NLDNC105PEX1.ubsw.net>
Message-ID: <25492B412B325B4FB1FED95013D3E5CE03F8084A@NLDNC105PEX1.ubsw.net>

www.LQG.org.uk
 
We are very pleased to announce the formation of the London Quant Group
- an individual membership, not-for-profit organisation, whose sole
purpose is to provide a continuing forum for the presentation and
discussion of practical quantitative investment ideas.

It has long been our belief that the growing London-based quantitative
community would benefit from the existence of an active forum for the
presentation and exchange of ideas, such as those that exist in other
cities such as Chicago and Boston. It is our intention that the LQ will
fulfil this role through evening seminars, bi-annual conferences plus
discussion forums for members.

Inaugural Meeting :   6.30pm, Thursday 18th October
BlackRock, 33 King William Street, London, EC4R 9AS

The Quant Crisis: the nature and causes of recent extreme volatility in 
quantitative equity strategies

During July and August this year many quantitatively managed equity
portfolios exhibited extreme volatility - initially sever weakness
followed by a sharp rebound. This effect was global - occurring at first
in the US, but followed rapidly in the rest of the world. It is clear
that the interaction of model correlation, leverage, short selling, and
rapid variation in price of risk played a major role in the evolution of
returns over this volatile period. At the inaugural meeting of the
London Quant Group practitioners involved in the management of
quantitative equity portfolios will recount some of their experiences
during this episode, and present analysis of the causes and consequences
of these events.

Speakers

Ed Fishwick (BlackRock)       Eoin Murray (Old Mutual Asset Management)
Sebastian Ceria (Axioma)
Jason MacQueen (Alpha Strategies)      Stephen Satchell (Trinity
College, Cambridge)

(NB: BlackRock is at the North end of London Bridge, next to Momument.
It will be necessary to pre-register for this event.)

This communication is issued by UBS AG or an affiliate ("UBS") to
institutional investors only. It is for informational purposes and is
not guaranteed as to accuracy, nor is it a complete statement of the
financial products or markets referred to. Opinions expressed are
subject to change without notice and may differ or be contrary to the
opinions or recommendations expressed by other business areas or
groups of UBS as a result of using different assumptions and criteria.
UBS may maintain long or short positions in the financial instruments
referred to and may transact in them as principal or agent. Unless
stated specifically otherwise, this is not a recommendation, offer or
solicitation to buy or sell and any prices or quotations contained
herein are indicative only. UBS may provide investment banking and
other services to, and/or its officers/employees may serve as
directors of, the companies referred to in this material. To the
extent permitted by law, UBS does not accept any liability arising
from the use of this communication. Additional information or UBS
Investment Research is available upon request.

Copyright 2006 UBS.  All rights reserved.  Intended for recipient only
and not for further distribution without the consent of UBS.


UBS Limited is a company registered in England & Wales under company
number 2035362, whose registered office is at 1 Finsbury Avenue,
London, EC2M 2PP, United Kingdom.

UBS AG (London Branch) is registered as a branch of a foreign company
under number BR004507, whose registered office is at
1 Finsbury Avenue, London, EC2M 2PP, United Kingdom.

UBS Clearing and Execution Services Limited is a company registered
in England & Wales under company number 03123037, whose registered
office is at 1 Finsbury Avenue, London, EC2M 2PP, United Kingdom.


From m_olshansky at yahoo.com  Fri Oct 12 01:46:46 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Thu, 11 Oct 2007 16:46:46 -0700 (PDT)
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 41,
	Issue 8: American Basket Options
In-Reply-To: <69331.51065.qm@web51508.mail.re2.yahoo.com>
Message-ID: <727355.60480.qm@web32207.mail.mud.yahoo.com>

Hi Matt,

Thank you for your note.

In R things are much easier - one can use mvrnorm
function from MASS package to generate a sample from a
multivariate normal distribution with a given
covariance matrix. I did this to price European basket
options.
The main problem in using Monte Carlo for American
options is not knowing when to exercise prior to
expiration date.

Regards,

Moshe.

P.S. as to covariance matrix, does it make sense to
use a longer history to estimate the correlation
matrix and a shorter one to estimate individual
(marginal) variances?

--- Matt Slezak <nocman43202 at yahoo.com> wrote:

> American Basket Options
> 
> Here is an efficient method I used to value European
> options on a basket of securities.  Maybe you can
> piece together R code for the American basket
> option. 
> I have done something similar in Excel using
> QuantLibXL.  First calculate the correlation matrix
> for the assets in the portfolio.  Next, do a
> Cholesky
> decomposition on this matrix.  Generate 1000 random
> numbers for each asset in the portfolio. Transform
> these into correlated random numbers by multiplying
> each by its factors from the Cholesky matrix.  Next
> generate quasi-random sequences (Halton or other,
> range 0 to 1) and pull these numbers from the
> cumalative normal distribution.  Next you create
> Geometric Brownian Motions for each asset using the
> respective correlated random number, the asset's
> volatility, and the asset's drift.  Run the 1000
> simulations for each asset over the time horizon,
> then
> calculate the net present value (NPV) of the payoffs
> for the whole portfolio (which is the maximum of the
> postive NPV or 0, since the long option cannot have
> a
> negative return).  The average payoff of the
> portfolio
> is the value of the basket option.  
> 
> If someone can add how one determines whether the
> option is exercised in each stage of the Monte Carlo
> simulation for American exercise it would be
> appreciated - I used this method for European
> options.
> 
> Hope this is helpful -Matt Slezak
> --- r-sig-finance-request at stat.math.ethz.ch wrote:
> 
> > Send R-SIG-Finance mailing list submissions to
> > 	r-sig-finance at stat.math.ethz.ch
> > 
> > To subscribe or unsubscribe via the World Wide
> Web,
> > visit
> > 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > or, via email, send a message with subject or body
> > 'help' to
> > 	r-sig-finance-request at stat.math.ethz.ch
> > 
> > You can reach the person managing the list at
> > 	r-sig-finance-owner at stat.math.ethz.ch
> > 
> > When replying, please edit your Subject line so it
> > is more specific
> > than "Re: Contents of R-SIG-Finance digest..."
> > > Today's Topics:
> > 
> >    1. Re: American basket options (Dale Smith)
> >    2. Re: Burns on Cramer (Patrick Burns)
> >    3. Re: returns convention
> (david.jessop at ubs.com)
> >    4. Re: Burns on Cramer (BBands)
> >    5. Re: Burns on Cramer (Patrick Burns)
> >    6. Black -Litterman Model
> > (ngottlieb at marinercapital.com)
> >    7. Re: Black -Litterman Model (Brian G.
> Peterson)
> >    8. Re: Black -Litterman Model
> > (ngottlieb at marinercapital.com)
> >    9. Re: American basket options (Moshe
> Olshansky)
> >   10. Re: American basket options (Wojciech
> > Slusarski)
> > > From: "Dale Smith" <dsmith at viciscapital.com>
> > To: "Wojciech Slusarski"
> > <wojciech.slusarski at gmail.com>,
> > 	<r-sig-finance at stat.math.ethz.ch>
> > Date: Tue, 9 Oct 2007 07:05:07 -0400
> > Subject: Re: [R-SIG-Finance] American basket
> options
> > 
> > For baskets of stocks larger than three or so,
> Monte
> > Carlo methods outperform the best finite
> difference
> > code. As mentioned below, there are Monte Carlo
> > algorithms for the American case.
> > 
> > Dale Smith, Ph.D.
> > Vicis Capital, LLC
> > 
> > -----Original Message-----
> > From: r-sig-finance-bounces at stat.math.ethz.ch
> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch]
> On
> > Behalf Of Wojciech Slusarski
> > Sent: Tuesday, October 09, 2007 6:00 AM
> > To: r-sig-finance at stat.math.ethz.ch
> > Subject: Re: [R-SIG-Finance] American basket
> options
> > 
> > There is an algorithm called OLS Monte Carlo, or
> > Longstaff-Schwarz
> > algorithm for valuation of american/bermudan
> options
> > using MC method,
> > though it can be a bit tricky to programm that for
> a
> > portfolio of 10
> > securities and be a bit unstable, though worth of
> > trying. If the
> > dividends are not high, it should not differ much
> > from a european
> > option priced using Monte Carlo. If dividends are
> > high, then the price
> > should be slightly higher.
> > 
> > Regards,
> > Wojciech ?lusarski
> > 
> > 
> > 2007/10/9, Moshe Olshansky
> <m_olshansky at yahoo.com>:
> > > This is an OTC traded option.
> > >
> > > For a European option one can estimate the
> > covariance
> > > matrix and then use Monte Carlo (taking into
> > account
> > > the dividends for each stock). This is pretty
> > > straightforward (well, there may be many ways to
> > > estimate the covariance matrix but let's use the
> > > simplest one).
> > >
> > > Regards,
> > >
> > > Moshe.
> > >
> > > --- Krishna Kumar <kriskumar at earthlink.net>
> wrote:
> > >
> > > > I am just curious as to if this is being
> traded
> > in
> > > > some market ?.
> > > >
> > > > This is probably not very helpful but I don't
> > think
> > > > a European style
> > > > basket is there in the existing packages.
> > European
> > > > style baskets are
> > > > themselves tricky if you want to get the
> basket
> > > > smile right etc.
> > > > American style baskets will be messy.
> > > >
> > > > Cheers
> > > > Krishna
> > > >
> > > >
> > > >
> > > >
> > > > Moshe Olshansky wrote:
> > > > > Hello,
> > > > >
> > > > > Is there any R code which allows to
> calculate
> > the
> > > > > price of an American basket option (option
> on
> > a
> > > > price
> > > > > of a portfolio)?
> > > > > If yes, are there any references to how
> > accurate
> > > > these
> > > > > calculations are?
> > > > > If no, can anybody recommend a relatively
> easy
> > to
> > > > use
> > > > > software doing this?
> > > > >
> > > > > Are there any non Monte Carlo methods to
> > compute
> > > > (even
> > > > > roughly) the price on an American basket put
> > > > option on
> > > > > a portfolio of 10 dividend paying stocks
> with
> > 6
> > > > months
> > > > > maturity?
> > > > >
> > > > > Thank you in advance,
> > > > >
> > > > > Moshe.
> > > > >
> 
=== message truncated ===


From stefan.schiman at gmx.at  Sat Oct 13 12:13:44 2007
From: stefan.schiman at gmx.at (Stefan Schiman)
Date: Sat, 13 Oct 2007 12:13:44 +0200
Subject: [R-SIG-Finance] GARCH(1,1)-M
Message-ID: <47109A58.3090405@gmx.at>

Dear colleagues,

How can I estimate a GARCH(1,1)-M-model (i.e. a Garch-in-mean-model) 
with the usual garch-command?

Thanks!


From icos.atropa at gmail.com  Sat Oct 13 13:23:13 2007
From: icos.atropa at gmail.com (icosa atropa)
Date: Sat, 13 Oct 2007 05:23:13 -0600
Subject: [R-SIG-Finance] Timezone magic with zoo, POSIXct and strptime?
Message-ID: <681d07c20710130423g42d24fb2nfba42a6de0270173@mail.gmail.com>

Just when I thought I was getting the hang of timezones...

I'm trying to remove the Daylight Savings timeshift from data that I'm
turning into zoo objects, using a POSIXct index.  My first try, as
expected, gives duplicate entries at timezone boundaries:

abq.z.full = zoo(abq[,4], order.by=as.POSIXct(strptime(paste(abq[,1],
abq[,2]), format='%Y%m%d %H%M%S')))
Warning message:
In zoo(abq[, 4], order.by = as.POSIXct(strptime(paste(abq[, 1],  :
  some methods for ?zoo? are not unique

and here are some of the duplicates:
abq.z.full[duplicated(index(abq.z.full)),]
2002-10-27 01:00:00 2002-10-27 01:15:00 2002-10-27 01:30:00 2002-10-27 01:45:00
                200                 200                 200                 200
2003-10-26 01:00:00 2003-10-26 01:15:00 2003-10-26 01:30:00 2003-10-26 01:45:00
                129                 129                 129                 129

After a lot of experimenting, I find that breaking the data into
subsets by timezone and then rbinding them works correctly.  Yet I
never inform strptime or as.POSIXct of the timezone.  Particularly,
the zoo() call is identical to above, just on different subsets.

> tmp.df= abq[abq[,3]=='MDT',];
abq.z.mdt = zoo(tmp.df[,4],
order.by=as.POSIXct(strptime(paste(tmp.df[,1], tmp.df[,2]),
format='%Y%m%d %H%M%S')))
> tmp.df= abq[abq[,3]=='MST',];
abq.z.mst = zoo(tmp.df[,4],
order.by=as.POSIXct(strptime(paste(tmp.df[,1], tmp.df[,2]),
format='%Y%m%d %H%M%S')))
> abq.z = rbind(abq.z.mst, abq.z.mdt)

I'm mystified as to how the 2 separate zoo(order.by=as.POSIXct...)
operations works but one doesn't, without my ever having to tell
strptime or as.POSIXct the timezone.  Is there more going on beneath
the surface that I'm missing?

thanks,
christian
-- 
Far better an approximate answer to the right question, which is often
vague, than the exact answer to the wrong question, which can always
be made precise -- j.w. tukey

From ggrothendieck at gmail.com  Sat Oct 13 13:59:23 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 13 Oct 2007 07:59:23 -0400
Subject: [R-SIG-Finance] Timezone magic with zoo, POSIXct and strptime?
In-Reply-To: <681d07c20710130423g42d24fb2nfba42a6de0270173@mail.gmail.com>
References: <681d07c20710130423g42d24fb2nfba42a6de0270173@mail.gmail.com>
Message-ID: <971536df0710130459q3f2fe944y473998a7dea53b12@mail.gmail.com>

The zoo function and most functions in the zoo package have
no understanding of date time classes and only require that
such classes have ordered elements and support certain methods
as defined in ?zoo so zoo is irrelevant for understanding dates, times
and timezones.

If you use chron you won't have to fight with time zones in the first place.
See R News 4/1.

Also please read the last line of every message to r-help.

On 10/13/07, icosa atropa <icos.atropa at gmail.com> wrote:
> Just when I thought I was getting the hang of timezones...
>
> I'm trying to remove the Daylight Savings timeshift from data that I'm
> turning into zoo objects, using a POSIXct index.  My first try, as
> expected, gives duplicate entries at timezone boundaries:
>
> abq.z.full = zoo(abq[,4], order.by=as.POSIXct(strptime(paste(abq[,1],
> abq[,2]), format='%Y%m%d %H%M%S')))
> Warning message:
> In zoo(abq[, 4], order.by = as.POSIXct(strptime(paste(abq[, 1],  :
>  some methods for ?zoo? are not unique
>
> and here are some of the duplicates:
> abq.z.full[duplicated(index(abq.z.full)),]
> 2002-10-27 01:00:00 2002-10-27 01:15:00 2002-10-27 01:30:00 2002-10-27 01:45:00
>                200                 200                 200                 200
> 2003-10-26 01:00:00 2003-10-26 01:15:00 2003-10-26 01:30:00 2003-10-26 01:45:00
>                129                 129                 129                 129
>
> After a lot of experimenting, I find that breaking the data into
> subsets by timezone and then rbinding them works correctly.  Yet I
> never inform strptime or as.POSIXct of the timezone.  Particularly,
> the zoo() call is identical to above, just on different subsets.
>
> > tmp.df= abq[abq[,3]=='MDT',];
> abq.z.mdt = zoo(tmp.df[,4],
> order.by=as.POSIXct(strptime(paste(tmp.df[,1], tmp.df[,2]),
> format='%Y%m%d %H%M%S')))
> > tmp.df= abq[abq[,3]=='MST',];
> abq.z.mst = zoo(tmp.df[,4],
> order.by=as.POSIXct(strptime(paste(tmp.df[,1], tmp.df[,2]),
> format='%Y%m%d %H%M%S')))
> > abq.z = rbind(abq.z.mst, abq.z.mdt)
>
> I'm mystified as to how the 2 separate zoo(order.by=as.POSIXct...)
> operations works but one doesn't, without my ever having to tell
> strptime or as.POSIXct the timezone.  Is there more going on beneath
> the surface that I'm missing?
>
> thanks,
> christian
> --
> Far better an approximate answer to the right question, which is often
> vague, than the exact answer to the wrong question, which can always
> be made precise -- j.w. tukey
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From icos.atropa at gmail.com  Sat Oct 13 15:33:11 2007
From: icos.atropa at gmail.com (icosa atropa)
Date: Sat, 13 Oct 2007 07:33:11 -0600
Subject: [R-SIG-Finance] Timezone magic with zoo, POSIXct and strptime?
In-Reply-To: <971536df0710130459q3f2fe944y473998a7dea53b12@mail.gmail.com>
References: <681d07c20710130423g42d24fb2nfba42a6de0270173@mail.gmail.com>
	<971536df0710130459q3f2fe944y473998a7dea53b12@mail.gmail.com>
Message-ID: <681d07c20710130633nff02888ya61ef3225895b226@mail.gmail.com>

Thanks.  Nice article - summarizes well.  I had no idea how POSIXct
was so pitfall-ridden.

"Also please read the last line of every message to r-help"
"... and provide commented, minimal, self-contained, reproducible code."

I'm sorry, my bad.

On 10/13/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> The zoo function and most functions in the zoo package have
> no understanding of date time classes and only require that
> such classes have ordered elements and support certain methods
> as defined in ?zoo so zoo is irrelevant for understanding dates, times
> and timezones.
>
> If you use chron you won't have to fight with time zones in the first place.
> See R News 4/1.
>
> Also please read the last line of every message to r-help.
>
> On 10/13/07, icosa atropa <icos.atropa at gmail.com> wrote:
> > Just when I thought I was getting the hang of timezones...
> >
> > I'm trying to remove the Daylight Savings timeshift from data that I'm
> > turning into zoo objects, using a POSIXct index.  My first try, as
> > expected, gives duplicate entries at timezone boundaries:
> >
> > abq.z.full = zoo(abq[,4], order.by=as.POSIXct(strptime(paste(abq[,1],
> > abq[,2]), format='%Y%m%d %H%M%S')))
> > Warning message:
> > In zoo(abq[, 4], order.by = as.POSIXct(strptime(paste(abq[, 1],  :
> >  some methods for ?zoo? are not unique
> >
> > and here are some of the duplicates:
> > abq.z.full[duplicated(index(abq.z.full)),]
> > 2002-10-27 01:00:00 2002-10-27 01:15:00 2002-10-27 01:30:00 2002-10-27 01:45:00
> >                200                 200                 200                 200
> > 2003-10-26 01:00:00 2003-10-26 01:15:00 2003-10-26 01:30:00 2003-10-26 01:45:00
> >                129                 129                 129                 129
> >
> > After a lot of experimenting, I find that breaking the data into
> > subsets by timezone and then rbinding them works correctly.  Yet I
> > never inform strptime or as.POSIXct of the timezone.  Particularly,
> > the zoo() call is identical to above, just on different subsets.
> >
> > > tmp.df= abq[abq[,3]=='MDT',];
> > abq.z.mdt = zoo(tmp.df[,4],
> > order.by=as.POSIXct(strptime(paste(tmp.df[,1], tmp.df[,2]),
> > format='%Y%m%d %H%M%S')))
> > > tmp.df= abq[abq[,3]=='MST',];
> > abq.z.mst = zoo(tmp.df[,4],
> > order.by=as.POSIXct(strptime(paste(tmp.df[,1], tmp.df[,2]),
> > format='%Y%m%d %H%M%S')))
> > > abq.z = rbind(abq.z.mst, abq.z.mdt)
> >
> > I'm mystified as to how the 2 separate zoo(order.by=as.POSIXct...)
> > operations works but one doesn't, without my ever having to tell
> > strptime or as.POSIXct the timezone.  Is there more going on beneath
> > the surface that I'm missing?
> >
> > thanks,
> > christian
> > --
> > Far better an approximate answer to the right question, which is often
> > vague, than the exact answer to the wrong question, which can always
> > be made precise -- j.w. tukey
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
>


-- 
Far better an approximate answer to the right question, which is often
vague, than the exact answer to the wrong question, which can always
be made precise -- j.w. tukey

From alexander.f.moreno at gmail.com  Sun Oct 14 22:00:15 2007
From: alexander.f.moreno at gmail.com (Alexander Moreno)
Date: Sun, 14 Oct 2007 15:00:15 -0500
Subject: [R-SIG-Finance] oanda and yahoo get.hist.quote
Message-ID: <4712754F.6000203@gmail.com>

Hi,

I have S&P index data (^gspc) that I got from get.hist.quote from yahoo, 
and fx data from the same function from Oanda.  The Oanda data has 
7-days, and the S&P data has 5.  Anyone know how to get them to match up 
for the same time period?

Best,
Alex


From edd at debian.org  Mon Oct 15 00:05:10 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 14 Oct 2007 17:05:10 -0500
Subject: [R-SIG-Finance] oanda and yahoo get.hist.quote
In-Reply-To: <4712754F.6000203@gmail.com>
References: <4712754F.6000203@gmail.com>
Message-ID: <18194.37526.227043.235039@ron.nulle.part>


On 14 October 2007 at 15:00, Alexander Moreno wrote:
| Hi,
| 
| I have S&P index data (^gspc) that I got from get.hist.quote from yahoo, 
| and fx data from the same function from Oanda.  The Oanda data has 
| 7-days, and the S&P data has 5.  Anyone know how to get them to match up 
| for the same time period?

Start with 

	> install.packages("zoo")
	> library(help=zoo)

Zoo can download these for now and match them automagically.

Dirk

-- 
Three out of two people have difficulties with fractions.


From ggrothendieck at gmail.com  Mon Oct 15 00:53:11 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 14 Oct 2007 18:53:11 -0400
Subject: [R-SIG-Finance] oanda and yahoo get.hist.quote
In-Reply-To: <18194.37526.227043.235039@ron.nulle.part>
References: <4712754F.6000203@gmail.com>
	<18194.37526.227043.235039@ron.nulle.part>
Message-ID: <971536df0710141553u2f0ad747lb54fca0aef458cb8@mail.gmail.com>

On 10/14/07, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> On 14 October 2007 at 15:00, Alexander Moreno wrote:
> | Hi,
> |
> | I have S&P index data (^gspc) that I got from get.hist.quote from yahoo,
> | and fx data from the same function from Oanda.  The Oanda data has
> | 7-days, and the S&P data has 5.  Anyone know how to get them to match up
> | for the same time period?
>
> Start with
>
>        > install.packages("zoo")
>        > library(help=zoo)
>
> Zoo can download these for now and match them automagically.

If you want info on zoo I would start with the vignettes: vignette("zoo")
and vignette("zoo-quickref"). Also note that get.hist.quote, used for
downloading such data, is in tseries, not in zoo.  zoo can merge such
series: ?merge.zoo


From jeff.a.ryan at gmail.com  Mon Oct 15 06:31:56 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sun, 14 Oct 2007 23:31:56 -0500
Subject: [R-SIG-Finance] oanda and yahoo get.hist.quote
In-Reply-To: <4712754F.6000203@gmail.com>
References: <4712754F.6000203@gmail.com>
Message-ID: <e8e755250710142131i755075adx82d49067128dd711@mail.gmail.com>

If you need 7 day data - skip this.  If you want data to match
weekdays - available from the St. Louis Fed's FRED system are Daily
Exchange Rates that have dates that match standard trading days in the
US - rates as of noon if I remember correctly...

Using quantmod:

>library(quantmod)
>getSymbols("^GSPC",src="yahoo",from="1999-01-04")
>getSymbols("DEXUSEU",src="FRED")
>showSymbols()
GSPC DEXUSEU
"yahoo" "FRED"

>all.data <- cbind(GSPC,DEXUSEU)  # through the magic of zoo
>all.data
           GSPC.Open GSPC.High GSPC.Low GSPC.Close GSPC.Volume GSPC.Adjusted
1999-01-04   1229.23   1248.81  1219.10    1228.10   877000000       1228.10
1999-01-05   1228.10   1246.11  1228.10    1244.78   775000000       1244.78
1999-01-06   1244.78   1272.50  1244.78    1272.34   986900000       1272.34
1999-01-07   1272.34   1272.34  1257.68    1269.73   863000000       1269.73
1999-01-08   1269.73   1278.24  1261.82    1275.09   937800000       1275.09
1999-01-11   1275.09   1276.22  1253.34    1263.88   818000000       1263.88
           DEXUSEU
1999-01-04  1.1812
1999-01-05  1.1760
1999-01-06  1.1636
1999-01-07  1.1672
1999-01-08  1.1554
1999-01-11  1.1534

....

The downside is that, at present, the FX data is *not* fetched with a
'from' or 'to' argument - so you get the full record from FRED.  The
default for yahoo data with getSymbols is only from 2007 onward - so I
conveniently fetched matching time periods for both data.

If you want to get rid of non-matching dates na.omit(all.data) works well.

Anyway, my 2 cents.

quantmod is on CRAN and at http://www.quantmod.com

There are many other data methods implemented for getSymbols as well
(database and local files), with many more planned (including oanda,
economagic, and some others...) in the coming weeks.

St.Louis Fed is at: http://research.stlouisfed.org/fred2/

Jeff Ryan

On 10/14/07, Alexander Moreno <alexander.f.moreno at gmail.com> wrote:
> Hi,
>
> I have S&P index data (^gspc) that I got from get.hist.quote from yahoo,
> and fx data from the same function from Oanda.  The Oanda data has
> 7-days, and the S&P data has 5.  Anyone know how to get them to match up
> for the same time period?
>
> Best,
> Alex
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From Bernd.Stampfl at Sparinvest.com  Mon Oct 15 10:50:12 2007
From: Bernd.Stampfl at Sparinvest.com (Bernd Stampfl)
Date: Mon, 15 Oct 2007 01:50:12 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] oanda and yahoo get.hist.quote
In-Reply-To: <e8e755250710142131i755075adx82d49067128dd711@mail.gmail.com>
References: <4712754F.6000203@gmail.com>
	<e8e755250710142131i755075adx82d49067128dd711@mail.gmail.com>
Message-ID: <13208854.post@talk.nabble.com>



Hi Jeff,
just tried to install the quandmod package. But unfortunately I got the
error message:

package 'quantmod' successfully unpacked and MD5 sums checked
Warning: unable to move temporary installation
'C:\Data\R\R-2.5.1\library\file14291417\quantmod' to
'C:\Data\R\R-2.5.1\library\quantmod'
updating HTML package descriptions

And nothing happened. Any idea why this happens?

Cheers, Bernd



Jeff Ryan wrote:
> 
> If you need 7 day data - skip this.  If you want data to match
> weekdays - available from the St. Louis Fed's FRED system are Daily
> Exchange Rates that have dates that match standard trading days in the
> US - rates as of noon if I remember correctly...
> 
> Using quantmod:
> 
>>library(quantmod)
>>getSymbols("^GSPC",src="yahoo",from="1999-01-04")
>>getSymbols("DEXUSEU",src="FRED")
>>showSymbols()
> GSPC DEXUSEU
> "yahoo" "FRED"
> 
>>all.data <- cbind(GSPC,DEXUSEU)  # through the magic of zoo
>>all.data
>            GSPC.Open GSPC.High GSPC.Low GSPC.Close GSPC.Volume
> GSPC.Adjusted
> 1999-01-04   1229.23   1248.81  1219.10    1228.10   877000000      
> 1228.10
> 1999-01-05   1228.10   1246.11  1228.10    1244.78   775000000      
> 1244.78
> 1999-01-06   1244.78   1272.50  1244.78    1272.34   986900000      
> 1272.34
> 1999-01-07   1272.34   1272.34  1257.68    1269.73   863000000      
> 1269.73
> 1999-01-08   1269.73   1278.24  1261.82    1275.09   937800000      
> 1275.09
> 1999-01-11   1275.09   1276.22  1253.34    1263.88   818000000      
> 1263.88
>            DEXUSEU
> 1999-01-04  1.1812
> 1999-01-05  1.1760
> 1999-01-06  1.1636
> 1999-01-07  1.1672
> 1999-01-08  1.1554
> 1999-01-11  1.1534
> 
> ....
> 
> The downside is that, at present, the FX data is *not* fetched with a
> 'from' or 'to' argument - so you get the full record from FRED.  The
> default for yahoo data with getSymbols is only from 2007 onward - so I
> conveniently fetched matching time periods for both data.
> 
> If you want to get rid of non-matching dates na.omit(all.data) works well.
> 
> Anyway, my 2 cents.
> 
> quantmod is on CRAN and at http://www.quantmod.com
> 
> There are many other data methods implemented for getSymbols as well
> (database and local files), with many more planned (including oanda,
> economagic, and some others...) in the coming weeks.
> 
> St.Louis Fed is at: http://research.stlouisfed.org/fred2/
> 
> Jeff Ryan
> 
> On 10/14/07, Alexander Moreno <alexander.f.moreno at gmail.com> wrote:
>> Hi,
>>
>> I have S&P index data (^gspc) that I got from get.hist.quote from yahoo,
>> and fx data from the same function from Oanda.  The Oanda data has
>> 7-days, and the S&P data has 5.  Anyone know how to get them to match up
>> for the same time period?
>>
>> Best,
>> Alex
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/oanda-and-yahoo-get.hist.quote-tf4623110.html#a13208854
Sent from the Rmetrics mailing list archive at Nabble.com.


From Bernd.Stampfl at Sparinvest.com  Mon Oct 15 16:54:01 2007
From: Bernd.Stampfl at Sparinvest.com (Bernd Stampfl)
Date: Mon, 15 Oct 2007 07:54:01 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] oanda and yahoo get.hist.quote
In-Reply-To: <13208854.post@talk.nabble.com>
References: <4712754F.6000203@gmail.com>
	<e8e755250710142131i755075adx82d49067128dd711@mail.gmail.com>
	<13208854.post@talk.nabble.com>
Message-ID: <13214328.post@talk.nabble.com>


Ok, tried to install the bnlearn package.

Got the message: 
package 'bnlearn' successfully unpacked and MD5 sums checked
updating HTML package descriptions

Seems to be a bit different. But anyways. My host-software seems to be a bit
weird.

Bernd




Bernd Stampfl wrote:
> 
> 
> Hi Jeff,
> just tried to install the quandmod package. But unfortunately I got the
> error message:
> 
> package 'quantmod' successfully unpacked and MD5 sums checked
> Warning: unable to move temporary installation
> 'C:\Data\R\R-2.5.1\library\file14291417\quantmod' to
> 'C:\Data\R\R-2.5.1\library\quantmod'
> updating HTML package descriptions
> 
> And nothing happened. Any idea why this happens?
> 
> Cheers, Bernd
> 
> 
> 
> Jeff Ryan wrote:
>> 
>> If you need 7 day data - skip this.  If you want data to match
>> weekdays - available from the St. Louis Fed's FRED system are Daily
>> Exchange Rates that have dates that match standard trading days in the
>> US - rates as of noon if I remember correctly...
>> 
>> Using quantmod:
>> 
>>>library(quantmod)
>>>getSymbols("^GSPC",src="yahoo",from="1999-01-04")
>>>getSymbols("DEXUSEU",src="FRED")
>>>showSymbols()
>> GSPC DEXUSEU
>> "yahoo" "FRED"
>> 
>>>all.data <- cbind(GSPC,DEXUSEU)  # through the magic of zoo
>>>all.data
>>            GSPC.Open GSPC.High GSPC.Low GSPC.Close GSPC.Volume
>> GSPC.Adjusted
>> 1999-01-04   1229.23   1248.81  1219.10    1228.10   877000000      
>> 1228.10
>> 1999-01-05   1228.10   1246.11  1228.10    1244.78   775000000      
>> 1244.78
>> 1999-01-06   1244.78   1272.50  1244.78    1272.34   986900000      
>> 1272.34
>> 1999-01-07   1272.34   1272.34  1257.68    1269.73   863000000      
>> 1269.73
>> 1999-01-08   1269.73   1278.24  1261.82    1275.09   937800000      
>> 1275.09
>> 1999-01-11   1275.09   1276.22  1253.34    1263.88   818000000      
>> 1263.88
>>            DEXUSEU
>> 1999-01-04  1.1812
>> 1999-01-05  1.1760
>> 1999-01-06  1.1636
>> 1999-01-07  1.1672
>> 1999-01-08  1.1554
>> 1999-01-11  1.1534
>> 
>> ....
>> 
>> The downside is that, at present, the FX data is *not* fetched with a
>> 'from' or 'to' argument - so you get the full record from FRED.  The
>> default for yahoo data with getSymbols is only from 2007 onward - so I
>> conveniently fetched matching time periods for both data.
>> 
>> If you want to get rid of non-matching dates na.omit(all.data) works
>> well.
>> 
>> Anyway, my 2 cents.
>> 
>> quantmod is on CRAN and at http://www.quantmod.com
>> 
>> There are many other data methods implemented for getSymbols as well
>> (database and local files), with many more planned (including oanda,
>> economagic, and some others...) in the coming weeks.
>> 
>> St.Louis Fed is at: http://research.stlouisfed.org/fred2/
>> 
>> Jeff Ryan
>> 
>> On 10/14/07, Alexander Moreno <alexander.f.moreno at gmail.com> wrote:
>>> Hi,
>>>
>>> I have S&P index data (^gspc) that I got from get.hist.quote from yahoo,
>>> and fx data from the same function from Oanda.  The Oanda data has
>>> 7-days, and the S&P data has 5.  Anyone know how to get them to match up
>>> for the same time period?
>>>
>>> Best,
>>> Alex
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>> 
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only. 
>> -- If you want to post, subscribe first.
>> 
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/oanda-and-yahoo-get.hist.quote-tf4623110.html#a13214328
Sent from the Rmetrics mailing list archive at Nabble.com.


From markleeds at verizon.net  Fri Oct 19 00:18:47 2007
From: markleeds at verizon.net (Mark Leeds)
Date: Thu, 18 Oct 2007 18:18:47 -0400
Subject: [R-SIG-Finance] bloomberg question
Message-ID: <000701c811d4$dae0e590$2f01a8c0@coresystem>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071018/89ca03f1/attachment.pl 

From edd at debian.org  Fri Oct 19 04:01:55 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 18 Oct 2007 21:01:55 -0500
Subject: [R-SIG-Finance] bloomberg question
In-Reply-To: <000701c811d4$dae0e590$2f01a8c0@coresystem>
References: <000701c811d4$dae0e590$2f01a8c0@coresystem>
Message-ID: <18200.4115.87914.926065@ron.nulle.part>


On 18 October 2007 at 18:18, Mark Leeds wrote:
| I don't currently have access to Bloomberg but, assuming I did, I was
| wondering if anyone knows if it is possible to use RBloomberg to pull back
| historical ( say the last year or so )

Someone correct me if this has changed, but it used to be that you got
'endless' daily data, but only around sixty days of intraday data from
Bloomberg.  I can't quite recall if the tickdata was already normalized to
second bars or not...  

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From nning_an at hotmail.com  Fri Oct 19 10:57:49 2007
From: nning_an at hotmail.com (ANNing-ning)
Date: Fri, 19 Oct 2007 16:57:49 +0800
Subject: [R-SIG-Finance] R can't response me,
	and maybe dead when calling garchFit
Message-ID: <BAY138-W10741BC98EA52929DFE12FFF9F0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071019/608f37a5/attachment.pl 

From hkahra at gmail.com  Fri Oct 19 11:12:44 2007
From: hkahra at gmail.com (Hannu Kahra)
Date: Fri, 19 Oct 2007 12:12:44 +0300
Subject: [R-SIG-Finance] R can't response me,
	and maybe dead when calling garchFit
In-Reply-To: <BAY138-W10741BC98EA52929DFE12FFF9F0@phx.gbl>
References: <BAY138-W10741BC98EA52929DFE12FFF9F0@phx.gbl>
Message-ID: <3d35a2ca0710190212m33cea9a6p5a05353bc46719cf@mail.gmail.com>

Instead of library(fSeires), try library(fSeries).

For your information, garchFit is now in the fGarch package, if you have
R-2.6.0.

Hannu

On 10/19/07, ANNing-ning <nning_an at hotmail.com> wrote:
>
>
> for example , when i run the following codes, R will be dead and lost
> responsion
>
> library(fSeires)
> n = 1000ans = rep(0 ,n)for(i in 1:1000) {  x = rnorm(1000)  mod =
> garchFit(~garch(1,1), data = x)  ans[i] = predict(mod, n.ahead =1)[3]  }
>
> but at sometimes, if i restart my computer, and then run these codes, it
> will be ok.
>
> I guess the problem maybe due to memory limitation, but after I run
>
> memory.limit(size = 1024*3)
>
> that problem is still on
>
> could someone  help me ?
>
> Thanks very much!
>
>
> ANN Ning-ning
>
> 2007.10.19
> _________________________________________________________________
> Windows Live Spaces ????????
>
>         [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071019/bf1242ed/attachment.html 

From davidr at rhotrading.com  Fri Oct 19 15:27:26 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Fri, 19 Oct 2007 08:27:26 -0500
Subject: [R-SIG-Finance] bloomberg question
In-Reply-To: <18200.4115.87914.926065@ron.nulle.part>
References: <000701c811d4$dae0e590$2f01a8c0@coresystem>
	<18200.4115.87914.926065@ron.nulle.part>
Message-ID: <F9F2A641C593D7408925574C05A7BE77550E17@rhopost.rhotrading.com>

Right, Dick.
You can get as much daily data as they have, but intraday data goes back
only 50 business days or so. You can get bar data down to one minute,
or tick data and apply your own bar algorithms.

David L. Reiner
Rho Trading Securities, LLC


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Dirk
Eddelbuettel
Sent: Thursday, October 18, 2007 9:02 PM
To: Mark Leeds
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] bloomberg question


On 18 October 2007 at 18:18, Mark Leeds wrote:
| I don't currently have access to Bloomberg but, assuming I did, I was
| wondering if anyone knows if it is possible to use RBloomberg to pull
back
| historical ( say the last year or so )

Someone correct me if this has changed, but it used to be that you got
'endless' daily data, but only around sixty days of intraday data from
Bloomberg.  I can't quite recall if the tickdata was already normalized
to
second bars or not...  

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Fri Oct 19 16:16:09 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 19 Oct 2007 09:16:09 -0500
Subject: [R-SIG-Finance] Financial charting now in quantmod v0.2-5
Message-ID: <e8e755250710190716l68e11f76pecc3645b428cd02e@mail.gmail.com>

R Finance Users:

I'm happy to announce the newest version of quantmod - version 0.2-5 -
is now available from www.quantmod.com and posting to CRAN mirrors as
you read this.

Important and (possibly) Interesting Changes:

? New charting tool(s):

A facility to produce very pretty (IMO) financial charts from OHLC
data. Candlesticks, Barcharts, and Linecharts are all implemented with
a variety of options to satisfy most charting preferences.
A few examples can be found at:

http://www.quantmod.com/examples and http://www.quantmod.com/gallery

Some issues of usability remain, but overall the functions works as expected.

There is work being done on adding technical analysis tools from TTR
to the charts as well, with a  screenshot of the progress/direction on
the website's examples page.


? Major website changes. New examples for using quantmod for data
management, as well as the aforementioned screenshots of the charting
abilities currently defined.

? An RSS feed.  Not much there, but if you're interested it will offer
updated info about quantmod as it becomes available.

? New getSymbols methods - data can now be fetched within the
getSymbols function from yahoo, google, St.Louis Fed's FRED, Oanda,
local MySQL db, csv, and RData files. One command covers them all.

? Misc bug fixes and function additions.

? A 'what's next' section to www.quantmod.com/whatsnext for those
interested in the future plans.

Thanks for your time,
Jeff Ryan


From Nigel.Walker at studentmail.newcastle.edu.au  Sun Oct 21 09:38:14 2007
From: Nigel.Walker at studentmail.newcastle.edu.au (Nigel.Walker at studentmail.newcastle.edu.au)
Date: Sun, 21 Oct 2007 17:38:14 +1000
Subject: [R-SIG-Finance] MAR-ARCH
Message-ID: <f6a4c1cf9b8dc.471b8e86@studentmail.newcastle.edu.au>

Hi list,
I am currently examining mixture time series models. I was wondering if anyone would have or know where to obtain an estimation code for the MAR-ARCH or MGARCH models? The MGARCH is a mixture of autoregressive components with generalized autoregressive conditional heteroscedasticy. 
I would prefer the code in R but any program will do.

Any help would be very much appreciated,

Nigel


From jeff.a.ryan at gmail.com  Sun Oct 21 14:50:07 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sun, 21 Oct 2007 07:50:07 -0500
Subject: [R-SIG-Finance] MAR-ARCH
In-Reply-To: <f6a4c1cf9b8dc.471b8e86@studentmail.newcastle.edu.au>
References: <f6a4c1cf9b8dc.471b8e86@studentmail.newcastle.edu.au>
Message-ID: <e8e755250710210550u13723618x240e054ffe9888f6@mail.gmail.com>

Nigel,

Something like:

http://www.vsthost.com/RStuff.html

Not too sure how updated/functional it is, but thought I'd post in
case you didn't google for it yourself.

Jeff


On 10/21/07, Nigel.Walker at studentmail.newcastle.edu.au
<Nigel.Walker at studentmail.newcastle.edu.au> wrote:
> Hi list,
> I am currently examining mixture time series models. I was wondering if anyone would have or know where to obtain an estimation code for the MAR-ARCH or MGARCH models? The MGARCH is a mixture of autoregressive components with generalized autoregressive conditional heteroscedasticy.
> I would prefer the code in R but any program will do.
>
> Any help would be very much appreciated,
>
> Nigel
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ajayshah at mayin.org  Sun Oct 21 16:36:51 2007
From: ajayshah at mayin.org (Ajay Shah)
Date: Sun, 21 Oct 2007 20:06:51 +0530
Subject: [R-SIG-Finance] MAR-ARCH
In-Reply-To: <e8e755250710210550u13723618x240e054ffe9888f6@mail.gmail.com>
References: <f6a4c1cf9b8dc.471b8e86@studentmail.newcastle.edu.au>
	<e8e755250710210550u13723618x240e054ffe9888f6@mail.gmail.com>
Message-ID: <20071021143651.GN207@lubyanka.local>

> http://www.vsthost.com/RStuff.html

The mgarchBEKK package looks quite exciting, but it seems to have
stopped working beyond R 2.4.

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From edd at debian.org  Sun Oct 21 17:27:34 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 21 Oct 2007 10:27:34 -0500
Subject: [R-SIG-Finance] MAR-ARCH
In-Reply-To: <20071021143651.GN207@lubyanka.local>
References: <f6a4c1cf9b8dc.471b8e86@studentmail.newcastle.edu.au>
	<e8e755250710210550u13723618x240e054ffe9888f6@mail.gmail.com>
	<20071021143651.GN207@lubyanka.local>
Message-ID: <18203.28646.653811.37160@ron.nulle.part>


On 21 October 2007 at 20:06, Ajay Shah wrote:
| > http://www.vsthost.com/RStuff.html
| 
| The mgarchBEKK package looks quite exciting, but it seems to have
| stopped working beyond R 2.4.

Yes, and moreover, mgarchBEKK is multivariate Garch, ie a specification of
2nd moments only. As I understood the original poster, he is looking for
'mixture AR in the mean' plus Garch in 2nd moments, and mgarchBEKK doesn't
help there.

Dirk

-- 
Three out of two people have difficulties with fractions.


From swtzang at gmail.com  Mon Oct 22 02:57:57 2007
From: swtzang at gmail.com (ShyhWeir Tzang)
Date: Mon, 22 Oct 2007 08:57:57 +0800
Subject: [R-SIG-Finance] historical option prices
Message-ID: <c17037a10710211757p1baaffddo113f4aa6c3ff4914@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071022/16b8afef/attachment.pl 

From jeff.a.ryan at gmail.com  Tue Oct 23 04:51:24 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 22 Oct 2007 21:51:24 -0500
Subject: [R-SIG-Finance] MAR-ARCH
In-Reply-To: <20071021152900.GP207@lubyanka.local>
References: <f6a4c1cf9b8dc.471b8e86@studentmail.newcastle.edu.au>
	<e8e755250710210550u13723618x240e054ffe9888f6@mail.gmail.com>
	<20071021143651.GN207@lubyanka.local>
	<e8e755250710210747u1ac0876fvd7c31960a8a971d6@mail.gmail.com>
	<20071021152900.GP207@lubyanka.local>
Message-ID: <e8e755250710221951q2ad5850fvb36f3f1923bd46be@mail.gmail.com>

While Dirk is correct in that the mgarchBEKK is _NOT_ MAR-GARCH, it
_IS_ an mgarch implementation... (which is worth something)

And I did get it to install and run with two fixes.  I have NO idea if
the results it produces are worth anything (and it chews up resources,
so I won't be spending any time to figure out).

But for those interested - windows users will need to adapt the
following (or switch to unix/linux/mac)...actually it *may* only work
on *nix boxes.


1. Download the most recent version from

http://www.vsthost.com/vstDocs/mgarchBEKK/release/mgarchBEKK_0.07-8.tar.gz

2. untar/zip
tar xzf mgarchBEKK_0.07-8.tar.gz

3. Fix _at_least_:
mvBEKK.est
line 475 has a trailing comma in the list of returned values after
residuals. - get rid of

mvBEKK.sim
line 252 has a trailing comma in the list of returned values AS WELL.
- just delete it

4. tarball back up for R CMD
tar czf mgarchBEKK_0.07-8.tar.gz mgarchBEKK

5. R CMD INSTALL mgarchBEKK_0.07-8.tar.gz


and happy mgarchBEKKing...

Jeff

My guess is it was breaking because previous R versions *may* have
been more lenient with the extra comma.  Too lazy to further
investigate though : )

On 10/21/07, Ajay Shah <ajayshah at mayin.org> wrote:
> On Sun, Oct 21, 2007 at 09:47:58AM -0500, Jeff Ryan wrote:
> > That's unfortunate.  Did you try and it fails - or did you read that
> > elsewhere.  Do you know why?  Can't be that hard to fix I'd assume.
>
> I've tried many times to reinstall and it does not work. I've tried
> asking on this mailing list. I've tried sending mail to one author.
>
> It must be something small. It worked till 2.4 and then stopped working.
>
> --
> Ajay Shah                                      http://www.mayin.org/ajayshah
> ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
> <*(:-? - wizard who doesn't know the answer.
>


From ajayshah at mayin.org  Tue Oct 23 06:18:31 2007
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue, 23 Oct 2007 09:48:31 +0530
Subject: [R-SIG-Finance] MAR-ARCH
In-Reply-To: <e8e755250710221951q2ad5850fvb36f3f1923bd46be@mail.gmail.com>
References: <f6a4c1cf9b8dc.471b8e86@studentmail.newcastle.edu.au>
	<e8e755250710210550u13723618x240e054ffe9888f6@mail.gmail.com>
	<20071021143651.GN207@lubyanka.local>
	<e8e755250710210747u1ac0876fvd7c31960a8a971d6@mail.gmail.com>
	<20071021152900.GP207@lubyanka.local>
	<e8e755250710221951q2ad5850fvb36f3f1923bd46be@mail.gmail.com>
Message-ID: <20071023041831.GW12969@lubyanka.local>

> 1. Download the most recent version from
> 
> http://www.vsthost.com/vstDocs/mgarchBEKK/release/mgarchBEKK_0.07-8.tar.gz
> 
> 2. untar/zip
> tar xzf mgarchBEKK_0.07-8.tar.gz
> 
> 3. Fix _at_least_:
> mvBEKK.est
> line 475 has a trailing comma in the list of returned values after
> residuals. - get rid of
> 
> mvBEKK.sim
> line 252 has a trailing comma in the list of returned values AS WELL.
> - just delete it
> 
> 4. tarball back up for R CMD
> tar czf mgarchBEKK_0.07-8.tar.gz mgarchBEKK
> 
> 5. R CMD INSTALL mgarchBEKK_0.07-8.tar.gz

Jeff,

Thanks a lot! The mistakes you have identified (line 475 and line 252)
are obvious syntax errors. It's a wonder that R tolerated them
earlier.

As is well known with MV Garch models, there are difficulties with
convergence even with the example supplied with mvBEKK.est().

What will be great is if someone with access to other MVGARCH codes
will compare and contrast what mgarchBEKK reports for some standard
problems. E.g. imagine a recent 1000-day time-series for USD/EUR,
USD/JPY and USD/GBP. All three series are easily accessible from the
US Fed website. It would be great to have reference estimates for what
results come out for this problem from a few different codes with
different starting values and algorithms. Unfortunately, I don't have
any other MVGARCH codes so I'm not able to do this experimentation.

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From patrick at burns-stat.com  Tue Oct 23 10:50:49 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 23 Oct 2007 09:50:49 +0100
Subject: [R-SIG-Finance] MAR-ARCH
In-Reply-To: <20071023041831.GW12969@lubyanka.local>
References: <f6a4c1cf9b8dc.471b8e86@studentmail.newcastle.edu.au>	<e8e755250710210550u13723618x240e054ffe9888f6@mail.gmail.com>	<20071021143651.GN207@lubyanka.local>	<e8e755250710210747u1ac0876fvd7c31960a8a971d6@mail.gmail.com>	<20071021152900.GP207@lubyanka.local>	<e8e755250710221951q2ad5850fvb36f3f1923bd46be@mail.gmail.com>
	<20071023041831.GW12969@lubyanka.local>
Message-ID: <471DB5E9.8000909@burns-stat.com>

Comparing estimators is a good idea.  But a good comparison
is more complex than stated.

What matters with garch is prediction, not estimation.  So we
want to test how good the predictions are.  For univariate we
can just compare the realized variance to the predicted variance
for a number of time periods.  It is best if this includes periods in
different volatility regimes.

For multivariate the comparison of realized to predicted is more
complex.  A couple of choices are to look at the maximum absolute
value of eigenvalues of the difference.  Another is to look at the
minimum variance portfolio.

If you do such testing, it becomes apparent that 1000 daily observations
is about as small as you want to go.  2000 observations is better, and even
more is probably good.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Ajay Shah wrote:

>>1. Download the most recent version from
>>
>>http://www.vsthost.com/vstDocs/mgarchBEKK/release/mgarchBEKK_0.07-8.tar.gz
>>
>>2. untar/zip
>>tar xzf mgarchBEKK_0.07-8.tar.gz
>>
>>3. Fix _at_least_:
>>mvBEKK.est
>>line 475 has a trailing comma in the list of returned values after
>>residuals. - get rid of
>>
>>mvBEKK.sim
>>line 252 has a trailing comma in the list of returned values AS WELL.
>>- just delete it
>>
>>4. tarball back up for R CMD
>>tar czf mgarchBEKK_0.07-8.tar.gz mgarchBEKK
>>
>>5. R CMD INSTALL mgarchBEKK_0.07-8.tar.gz
>>    
>>
>
>Jeff,
>
>Thanks a lot! The mistakes you have identified (line 475 and line 252)
>are obvious syntax errors. It's a wonder that R tolerated them
>earlier.
>
>As is well known with MV Garch models, there are difficulties with
>convergence even with the example supplied with mvBEKK.est().
>
>What will be great is if someone with access to other MVGARCH codes
>will compare and contrast what mgarchBEKK reports for some standard
>problems. E.g. imagine a recent 1000-day time-series for USD/EUR,
>USD/JPY and USD/GBP. All three series are easily accessible from the
>US Fed website. It would be great to have reference estimates for what
>results come out for this problem from a few different codes with
>different starting values and algorithms. Unfortunately, I don't have
>any other MVGARCH codes so I'm not able to do this experimentation.
>
>  
>


From stefan.albrecht at apep.com  Tue Oct 23 11:38:39 2007
From: stefan.albrecht at apep.com (Albrecht, Dr. Stefan (APEP))
Date: Tue, 23 Oct 2007 11:38:39 +0200
Subject: [R-SIG-Finance]  Getmansky et al. Smoothing Index
Message-ID: <B3E803F92F909741B050C9FA4DDEDE75F359A3@naimucog.allianzde.rootdom.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071023/2a0e0d70/attachment.pl 

From ajayshah at mayin.org  Tue Oct 23 16:17:49 2007
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue, 23 Oct 2007 19:47:49 +0530
Subject: [R-SIG-Finance] MAR-ARCH
In-Reply-To: <471DB5E9.8000909@burns-stat.com>
References: <f6a4c1cf9b8dc.471b8e86@studentmail.newcastle.edu.au>
	<e8e755250710210550u13723618x240e054ffe9888f6@mail.gmail.com>
	<20071021143651.GN207@lubyanka.local>
	<e8e755250710210747u1ac0876fvd7c31960a8a971d6@mail.gmail.com>
	<20071021152900.GP207@lubyanka.local>
	<e8e755250710221951q2ad5850fvb36f3f1923bd46be@mail.gmail.com>
	<20071023041831.GW12969@lubyanka.local>
	<471DB5E9.8000909@burns-stat.com>
Message-ID: <20071023141749.GI12969@lubyanka.local>

On Tue, Oct 23, 2007 at 09:50:49AM +0100, Patrick Burns wrote:
> Comparing estimators is a good idea.  But a good comparison
> is more complex than stated.

I'm sorry I was not clear. For starters, I was only after software
testing. Does this code replicate the numerical values obtained for
standard datasets with standard codes?

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From edd at debian.org  Tue Oct 23 16:46:56 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 23 Oct 2007 09:46:56 -0500
Subject: [R-SIG-Finance] MAR-ARCH
In-Reply-To: <20071023141749.GI12969@lubyanka.local>
References: <f6a4c1cf9b8dc.471b8e86@studentmail.newcastle.edu.au>
	<e8e755250710210550u13723618x240e054ffe9888f6@mail.gmail.com>
	<20071021143651.GN207@lubyanka.local>
	<e8e755250710210747u1ac0876fvd7c31960a8a971d6@mail.gmail.com>
	<20071021152900.GP207@lubyanka.local>
	<e8e755250710221951q2ad5850fvb36f3f1923bd46be@mail.gmail.com>
	<20071023041831.GW12969@lubyanka.local>
	<471DB5E9.8000909@burns-stat.com>
	<20071023141749.GI12969@lubyanka.local>
Message-ID: <18206.2400.154219.886385@ron.nulle.part>


Hi Ajay,

On 23 October 2007 at 19:47, Ajay Shah wrote:
| On Tue, Oct 23, 2007 at 09:50:49AM +0100, Patrick Burns wrote:
| > Comparing estimators is a good idea.  But a good comparison
| > is more complex than stated.
| 
| I'm sorry I was not clear. For starters, I was only after software
| testing. Does this code replicate the numerical values obtained for
| standard datasets with standard codes?

Well are there standard datasets and results for volatility estimation?

It's been a (longish) while since I looked closely at this, but isn't
volatility still an unobservable?  Short of a Monte Carlo study with metrics
such as the ones suggested by Pat, what do you suggest one looks at?
Realized vol?  Implied vol?  "Traded" vol from variance or vol contracts?

I'm sure there are good answers to be had for this, so let's hear them :)

Dirk

-- 
Three out of two people have difficulties with fractions.


From patrick at burns-stat.com  Tue Oct 23 17:48:47 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 23 Oct 2007 16:48:47 +0100
Subject: [R-SIG-Finance] MAR-ARCH
In-Reply-To: <18206.2400.154219.886385@ron.nulle.part>
References: <f6a4c1cf9b8dc.471b8e86@studentmail.newcastle.edu.au>	<e8e755250710210550u13723618x240e054ffe9888f6@mail.gmail.com>	<20071021143651.GN207@lubyanka.local>	<e8e755250710210747u1ac0876fvd7c31960a8a971d6@mail.gmail.com>	<20071021152900.GP207@lubyanka.local>	<e8e755250710221951q2ad5850fvb36f3f1923bd46be@mail.gmail.com>	<20071023041831.GW12969@lubyanka.local>	<471DB5E9.8000909@burns-stat.com>	<20071023141749.GI12969@lubyanka.local>
	<18206.2400.154219.886385@ron.nulle.part>
Message-ID: <471E17DF.506@burns-stat.com>

Ajay is talking about something different than what you (Dirk)
are and I was talking about.

The latter topic is: how good does my model capture the
financial phenomenon in which I'm interested.

Ajay's topic is: I have a model and at least two implementations
of that model.  I don't care how good it is for interpreting reality,
but what I do care about is if I can say anything about the quality
of the implementations.

Garch is a particularly fertile ground for the second question.  It
is probably somewhat of an exaggeration, but there may be problems
for which you get a unique answer from each implementation you try.
Even for univariate garch(1,1) assuming Gaussian errors.

As for standards on this, Bruce McCullough wrote a paper on garch
implementations, and Dietmar Maringer and Peter Winker wrote a
paper on the difficulty of getting the optimal estimate.

Pat


Dirk Eddelbuettel wrote:

>Hi Ajay,
>
>On 23 October 2007 at 19:47, Ajay Shah wrote:
>| On Tue, Oct 23, 2007 at 09:50:49AM +0100, Patrick Burns wrote:
>| > Comparing estimators is a good idea.  But a good comparison
>| > is more complex than stated.
>| 
>| I'm sorry I was not clear. For starters, I was only after software
>| testing. Does this code replicate the numerical values obtained for
>| standard datasets with standard codes?
>
>Well are there standard datasets and results for volatility estimation?
>
>It's been a (longish) while since I looked closely at this, but isn't
>volatility still an unobservable?  Short of a Monte Carlo study with metrics
>such as the ones suggested by Pat, what do you suggest one looks at?
>Realized vol?  Implied vol?  "Traded" vol from variance or vol contracts?
>
>I'm sure there are good answers to be had for this, so let's hear them :)
>
>Dirk
>
>  
>


From finbref.2006 at gmail.com  Tue Oct 23 20:54:43 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Tue, 23 Oct 2007 20:54:43 +0200
Subject: [R-SIG-Finance] historical option prices
In-Reply-To: <c17037a10710211757p1baaffddo113f4aa6c3ff4914@mail.gmail.com>
References: <c17037a10710211757p1baaffddo113f4aa6c3ff4914@mail.gmail.com>
Message-ID: <d0f55a670710231154qcf0c5e8n5afef8f8a7805624@mail.gmail.com>

go to yahoo and download the data. there.
or check out Martin's page at http://www.fam.tuwien.ac.at/~mkeller
where you perhaps find a suitable R scipt
Thomas


From Nigel.Walker at studentmail.newcastle.edu.au  Sat Oct 27 14:54:58 2007
From: Nigel.Walker at studentmail.newcastle.edu.au (Nigel.Walker at studentmail.newcastle.edu.au)
Date: Sat, 27 Oct 2007 22:54:58 +1000
Subject: [R-SIG-Finance] MAR-GARCH
Message-ID: <f6a493369d137.4723c1c2@studentmail.newcastle.edu.au>

Hi list,
I am looking at ways to estimate a MAR-GARCH model. What i would like to do is maximise the likelihood function  to obtain the parameters however i am not sure how to do this. I was thinking of using optim but am not sure how to. The problem is i have ht, et, and tau that depend on the parameters i wish to obtain. These ht's, et's and tau's are within the liklihood function. 

The code below is an example of what i need to calculate in order to obtain the likelihood function. The likelihood function is also given. Does anyone know how i would go about maximising the likelihood to obtain the estimates? 
note that n is the length of the series and the parameters(phis, beta's and gamma's) are first set to initials. 
Any help would be very welcome

ht1<-c()
ht1[1]<-1
ht2<-c()
ht2[1]<-1
et1<-c()
et1[1]<-1
et1<-c()
et2[1]<-1
denominator1<-c()
denominator1[1]<-1
denominator2<-c()
denominator2[1]<-1
top1<-c()
top1[1]<-1
top2<-c()
top2[1]<-1
for (L in 2:n){
ht1[L]=beta10+beta11*(et1[L-1])^2+gamma11*ht1[L-1];
et1[L]<- rnorm(1,0,sqrt(ht1[L]));
#yt[L]=phi10+phi11*yt[L-1]+et1[L-1]
ht2[L]=beta20+beta21*(et2[L-1])^2+gamma21*ht2[L-1];
et2[L] <- rnorm(1,0,sqrt(ht2[L]))
#yt[L]=phi20+phi21*yt[L-1]+et2[L-1]
denominator1[L]<-alpha1*(1/sqrt(ht1[L]))*dnorm(et1[L]/sqrt(ht1[L]))
denominator2[L]<- alpha2*(1/sqrt(ht2[L]))*dnorm(et2[L]/sqrt(ht2[L]))
top1[L]<-alpha1*(1/sqrt(ht1[L]))*dnorm(et1[L]/sqrt(ht1[L]))
top2[L]<-alpha2*(1/sqrt(ht2[L]))*dnorm(et2[L]/sqrt(ht2[L]))
}

# Now to get up tau
denominator<-denominator1+denominator2
tau1<-top1/denominator
tau2<-top2/denominator
lik.p1<-tau1*log(alpha1)+tau2*log(alpha2)
lik.p2<- (tau1*log(ht1))/2+(tau2*log(ht2))/2
lik.p3<- (tau1*et1^2)/(2*ht1)+(tau2*et2^2)/(2*ht2)
likeli<- lik.p1-lik.p2-lik.p3
likelihood<- sum(likeli[3:n])


From patrick at burns-stat.com  Sat Oct 27 17:21:19 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Sat, 27 Oct 2007 16:21:19 +0100
Subject: [R-SIG-Finance] MAR-GARCH
In-Reply-To: <f6a493369d137.4723c1c2@studentmail.newcastle.edu.au>
References: <f6a493369d137.4723c1c2@studentmail.newcastle.edu.au>
Message-ID: <4723576F.2060609@burns-stat.com>

DO NOT grow objects as you show:

ht1 <- c()
ht1[1] <- 1
...

Instead create the vector to be the final length and
subscript into it:

ht1 <- numeric(n)
ht1[1] <- 1
...


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Nigel.Walker at studentmail.newcastle.edu.au wrote:

>Hi list,
>I am looking at ways to estimate a MAR-GARCH model. What i would like to do is maximise the likelihood function  to obtain the parameters however i am not sure how to do this. I was thinking of using optim but am not sure how to. The problem is i have ht, et, and tau that depend on the parameters i wish to obtain. These ht's, et's and tau's are within the liklihood function. 
>
>The code below is an example of what i need to calculate in order to obtain the likelihood function. The likelihood function is also given. Does anyone know how i would go about maximising the likelihood to obtain the estimates? 
>note that n is the length of the series and the parameters(phis, beta's and gamma's) are first set to initials. 
>Any help would be very welcome
>
>ht1<-c()
>ht1[1]<-1
>ht2<-c()
>ht2[1]<-1
>et1<-c()
>et1[1]<-1
>et1<-c()
>et2[1]<-1
>denominator1<-c()
>denominator1[1]<-1
>denominator2<-c()
>denominator2[1]<-1
>top1<-c()
>top1[1]<-1
>top2<-c()
>top2[1]<-1
>for (L in 2:n){
>ht1[L]=beta10+beta11*(et1[L-1])^2+gamma11*ht1[L-1];
>et1[L]<- rnorm(1,0,sqrt(ht1[L]));
>#yt[L]=phi10+phi11*yt[L-1]+et1[L-1]
>ht2[L]=beta20+beta21*(et2[L-1])^2+gamma21*ht2[L-1];
>et2[L] <- rnorm(1,0,sqrt(ht2[L]))
>#yt[L]=phi20+phi21*yt[L-1]+et2[L-1]
>denominator1[L]<-alpha1*(1/sqrt(ht1[L]))*dnorm(et1[L]/sqrt(ht1[L]))
>denominator2[L]<- alpha2*(1/sqrt(ht2[L]))*dnorm(et2[L]/sqrt(ht2[L]))
>top1[L]<-alpha1*(1/sqrt(ht1[L]))*dnorm(et1[L]/sqrt(ht1[L]))
>top2[L]<-alpha2*(1/sqrt(ht2[L]))*dnorm(et2[L]/sqrt(ht2[L]))
>}
>
># Now to get up tau
>denominator<-denominator1+denominator2
>tau1<-top1/denominator
>tau2<-top2/denominator
>lik.p1<-tau1*log(alpha1)+tau2*log(alpha2)
>lik.p2<- (tau1*log(ht1))/2+(tau2*log(ht2))/2
>lik.p3<- (tau1*et1^2)/(2*ht1)+(tau2*et2^2)/(2*ht2)
>likeli<- lik.p1-lik.p2-lik.p3
>likelihood<- sum(likeli[3:n])
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From Swaroop.Yalla at MorganStanley.com  Thu Nov  1 20:55:50 2007
From: Swaroop.Yalla at MorganStanley.com (Yalla, Swaroop (FID))
Date: Thu, 1 Nov 2007 15:55:50 -0400
Subject: [R-SIG-Finance] ARIMA model with seasonality
Message-ID: <507725E31752A74CB00728FA6C17D99C0795789D@NYWEXMB29.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071101/ab91263c/attachment.pl 

From jeff.a.ryan at gmail.com  Thu Nov  1 21:38:23 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 1 Nov 2007 15:38:23 -0500
Subject: [R-SIG-Finance] ARIMA model with seasonality
In-Reply-To: <507725E31752A74CB00728FA6C17D99C0795789D@NYWEXMB29.msad.ms.com>
References: <507725E31752A74CB00728FA6C17D99C0795789D@NYWEXMB29.msad.ms.com>
Message-ID: <e8e755250711011338s1810b2f7v4c93f69741a9113c@mail.gmail.com>

A good start... at least for TS in general.

http://www.stat.pitt.edu/stoffer/tsa2/index.html

and more from the magic of google (actually other people hard work)

http://www.google.com/search?source=ig&hl=en&rlz=&q=time+series+R&btnG=Google+Search

Jeff



On 11/1/07, Yalla, Swaroop (FID) <Swaroop.Yalla at morganstanley.com> wrote:
> Hello:
>
> I have a series of monthly data which alos has seasonality, and I was
> trying to model it as a seasonal ARIMA model. I wasnt sure how to use
> arima function in R to model a seasonal time series (in fact my
> knowledge of ARMA model is quite rudimentary). Can someone help me in
> this regard, or can point me to some example code or other R resources?
> Another question: how far the forecasts from such a model make sense??
>
> thanks
> Swaroop
> --------------------------------------------------------
>
> This is not an offer (or solicitation of an offer) to bu...{{dropped:24}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From nbryant at optonline.net  Thu Nov  1 21:58:14 2007
From: nbryant at optonline.net (Nathan Bryant)
Date: Thu, 01 Nov 2007 16:58:14 -0400
Subject: [R-SIG-Finance] ARIMA model with seasonality
In-Reply-To: <e8e755250711011338s1810b2f7v4c93f69741a9113c@mail.gmail.com>
References: <507725E31752A74CB00728FA6C17D99C0795789D@NYWEXMB29.msad.ms.com>
	<e8e755250711011338s1810b2f7v4c93f69741a9113c@mail.gmail.com>
Message-ID: <472A3DE6.8050306@optonline.net>


Before you start, it's best to understand seasonal random walk, seasonal 
random trend, and seasonal adjustment (for example, with dummy variables.)

 From my bookmarks-- (this guy rnau is a gold mine)
http://www.duke.edu/~rnau/411outbd.htm
http://www.duke.edu/~rnau/411seas.htm
http://www.duke.edu/~rnau/411seart.htm
http://www.duke.edu/~rnau/seasarim.htm


Jeff Ryan wrote:
> A good start... at least for TS in general.
>
> http://www.stat.pitt.edu/stoffer/tsa2/index.html
>
> and more from the magic of google (actually other people hard work)
>
> http://www.google.com/search?source=ig&hl=en&rlz=&q=time+series+R&btnG=Google+Search
>
> Jeff
>
>
>
> On 11/1/07, Yalla, Swaroop (FID) <Swaroop.Yalla at morganstanley.com> wrote:
>   
>> Hello:
>>
>> I have a series of monthly data which alos has seasonality, and I was
>> trying to model it as a seasonal ARIMA model. I wasnt sure how to use
>> arima function in R to model a seasonal time series (in fact my
>> knowledge of ARMA model is quite rudimentary). Can someone help me in
>> this regard, or can point me to some example code or other R resources?
>> Another question: how far the forecasts from such a model make sense??
>>
>> thanks
>> Swaroop
>> --------------------------------------------------------
>>
>> This is not an offer (or solicitation of an offer) to bu...{{dropped:24}}
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>     
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
>


From panyuml at gmail.com  Sun Nov  4 08:57:02 2007
From: panyuml at gmail.com (Weimin Mailing List)
Date: Sun, 4 Nov 2007 02:57:02 -0500
Subject: [R-SIG-Finance] Could not find function "inbvisible" in OLS(Urgent!)
Message-ID: <f2e99fc90711040057icb61348r82368dae0c1f043e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071104/2c84a125/attachment.pl 

From pgilbert at bank-banque-canada.ca  Tue Nov  6 18:23:41 2007
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 06 Nov 2007 12:23:41 -0500
Subject: [R-SIG-Finance] time series database packages
Message-ID: <4730A31D.4010605@bank-banque-canada.ca>

I have put a group of packages on CRAN for time series databases. The 
current versions should be considered beta, and I would appreciate 
feedback from this SIG before announcing them more broadly. (Thanks to 
Gabor Grothendieck for comments on an alpha version.)

TSdbi defines a common API which the other packages use.  TSMySQL and 
TSSQLite provide methods for MySQL and SQLite, and require  RMySQL and 
RSQLite respectively. TSpadi uses an RPC based protocol for a 
client/server connection where the server could use any database, but 
the working implementation is with Fame. (This last package is mainly 
for me to support legacy applications, but also helps test the 
generality of the interface.)

I believe it should be straight forward to implement any SQL database 
having a DBI based package, and also not difficult to implement on top 
of RODBC, though I have not tried that yet. It should also be possible 
to interface to the R fame package directly, which could provide writing 
to the database and some other features not supported by TSpadi. (If 
anyone is interested in working on any of these, please contact me for 
additional hints.)

The SQL implementations define tables necessary to put in place the back 
end database, but this might benefit from examination by someone that 
understands SQL table optimization better than I do. The current 
implementation supports annual, quarterly, monthly, semiannual, weekly, 
daily, business day, minutely, irregular data with a date, and irregular 
  data with a date and time. This may be constrained by the back end 
(e.g. Fame does not support all these types.)  My own work tends to be 
with the first three, so others have not been tested as extensively.  It 
should be relatively easy to implement other types of time series data 
in the SQL back ends (suggestions and examples?).

Series documentation is supported in a meta table, which also contains a 
lookup mechanism to determine which table has the data for a given 
series identifier. (Multilingual documentation support is not 
implemented, but should not be too difficult.)

The design also (optionally) supports vintages and panels of data (e.g.
series with the same identifier but a different release date or 
country). This feature is actively under development.

The intention is that the R time series representation can optionally be 
specified, but currently only the default is working (ts were possible 
and zoo elsewhere).

Vignette examples are provided in each of the packages. (The vignettes 
are similar, but the most complete at the moment is the TSMySQL one.)

Some possible extensions include:

- a mechanism for handling aliases for series names.

- an RODBC database plug in

- an R Postgresql database plug in

- a direct fame database plug in (Fame through TSpadi is read only)

- optionally different time series representations.

- multilingual documentation

- mechanism for signaling series updates to users

It is unlikely that I will do many of these things myself, but if anyone 
is interested in working on them I would be happy to provide some guidance.

Paul Gilbert
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential information, and the Bank of
Canada does not waive any related rights. Any distribution, use, or copying of this
email or the information it contains by other than the intended recipient is
unauthorized. If you received this email in error please delete it immediately from
your system and notify the sender promptly by email that you have done so. 

------------------------------------------------------------------------------------

Le pr?sent courriel peut contenir de l'information privil?gi?e ou confidentielle.
La Banque du Canada ne renonce pas aux droits qui s'y rapportent. Toute diffusion,
utilisation ou copie de ce courriel ou des renseignements qu'il contient par une
personne autre que le ou les destinataires d?sign?s est interdite. Si vous recevez
ce courriel par erreur, veuillez le supprimer imm?diatement et envoyer sans d?lai ?
l'exp?diteur un message ?lectronique pour l'aviser que vous avez ?limin? de votre
ordinateur toute copie du courriel re?u.

From rory.winston at gmail.com  Wed Nov  7 19:31:16 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Wed, 7 Nov 2007 18:31:16 +0000
Subject: [R-SIG-Finance] Interpolating/comparing two irregular time/price
	sequences?
Message-ID: <3f446aa30711071031j37936e36i933be63c90f9ce4c@mail.gmail.com>

Hi all

I have two data frames, that both look like the following:

> head(series1)
     timestamp     mid   spread
1 1.194438e+12 2.10011 0.000260
2 1.194438e+12 2.10010 0.000290
...

These two time sequences are sampled on price ticks, so the interval
between ticks is stochastic and irregular. The time sequences are also
of different lengths, i.e. one may have 8 hours worth of data, the
other may have 4. My issue is that I want to compare these two series
for similarity - they should be producing almost exactly the same
data, although potentially at slightly different timestamps (hence the
sampling irregularity). I can subset the data so that they span
roughly the same time intervals, but the number of ticks in each
series will be different. Basically what I am trying to achieve is
some sort of constant interpolation based on a time index - so that if
series A starts at 08:01, contains 10,000 ticks, and ends at 16:05,
and series B starts at 08:00, contains 7,000 ticks, and ends at 16:06,
I would like to be able to index from series A into series B at say,
each timestamp in A. Using a simple example, for the following series
A and B:

A:
time   tick
16:01 2.05
16:02 2.06


B:
time tick
16:00 2.04
16:02 2.06

I would like to be able to index from A into B at each tick from A, so
I would get an output series that was the value of B at each time A
ticked:

C
time tick
16:01 2.04 <--- constant interpolation from value of B @ 16:00
16:02 2.06

Has anyone done anything like this before? I'm looking at the zoo
package to see if it can help me, but I havent quite figured out how
to do this kind of thing yet. Is this even a good way to checking
whether series B is very similar to series A at the discrete tick
intervals? Any better methods?(I guess another way might be to align
the two subsetted series exactly and just take differences).

Thanks
Rory


From Swaroop.Yalla at MorganStanley.com  Wed Nov  7 19:47:49 2007
From: Swaroop.Yalla at MorganStanley.com (Yalla, Swaroop (FID))
Date: Wed, 7 Nov 2007 13:47:49 -0500
Subject: [R-SIG-Finance] ARIMA question
Message-ID: <507725E31752A74CB00728FA6C17D99C07A0E63E@NYWEXMB29.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071107/4e0c7e99/attachment.pl 

From Achim.Zeileis at wu-wien.ac.at  Wed Nov  7 19:59:54 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 7 Nov 2007 19:59:54 +0100 (CET)
Subject: [R-SIG-Finance] Interpolating/comparing two irregular
 time/price sequences?
In-Reply-To: <3f446aa30711071031j37936e36i933be63c90f9ce4c@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0711071946000.5620-100000@disco.wu-wien.ac.at>

On Wed, 7 Nov 2007, Rory Winston wrote:

<snip>

> Has anyone done anything like this before? I'm looking at the zoo
> package to see if it can help me, but I havent quite figured out how
> to do this kind of thing yet.

<snip>

You can merge the two series and generate NAs for the missing time points
and then use a na.*() method to eliminate the NAs, e.g., by linear
interpolation or by carrying observations forward or backward.

A simple example is attached below. See Section 2.8 of
  vignette("zoo", package = "zoo")
for more details on NA methods.

  ## two time series
  x <- zoo(c(2.01, 2.54, 2.37),
           as.POSIXct("2007-11-07 18:47 GMT") + c(0, 125, 334))
  y <- zoo(c(2, 2.5, 2.35),
           as.POSIXct("2007-11-07 18:47 GMT") + c(2, 120, 339))

  ## the merged version
  xy <- merge(x, y)

  ## interpolate by linear approximation
  xy <- na.approx(xy, rule = 2)

  ## extract interpolated version of y at index of x
  y2 <- window(xy[,2], index(x))

hth,
Z


From jeff.a.ryan at gmail.com  Wed Nov  7 20:39:36 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 7 Nov 2007 13:39:36 -0600
Subject: [R-SIG-Finance] ARIMA question
In-Reply-To: <507725E31752A74CB00728FA6C17D99C07A0E63E@NYWEXMB29.msad.ms.com>
References: <507725E31752A74CB00728FA6C17D99C07A0E63E@NYWEXMB29.msad.ms.com>
Message-ID: <e8e755250711071139p7ae10f5ch9ff90fc77c63cb0b@mail.gmail.com>

I think it is as simple as backing out from the residuals:

# an MA2 model
x <- arima.sim(list(ma=2),n=100)

#Fitted as such...
x.model <- arima(x,c(0,0,2))

# add the residuals to the original data
x.insample.fit <- x-residuals(x.model)

# and you can even see them:
plot(x)
par(new=TRUE)
lines(x.insample.fit,col=3,lty=2)

Jeff


On Nov 7, 2007 12:47 PM, Yalla, Swaroop (FID)
<Swaroop.Yalla at morganstanley.com> wrote:
> Hi:
>
> I have another ARIMA question for R. I was finally able to use ARIMA
> modeling on my data. Now to forecast out of sample, we can use
> predict(fit, n.ahead = 10) type of command and thats fine- but how can I
> see the fit in-sample. I mean is there a easy way to just compare the
> actual data with the fitted model in-sample?
>
> thanks for all the help..
> Swaroop
> --------------------------------------------------------
>
> This is not an offer (or solicitation of an offer) to bu...{{dropped:24}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Wed Nov  7 20:47:01 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 7 Nov 2007 13:47:01 -0600
Subject: [R-SIG-Finance] ARIMA question
In-Reply-To: <e8e755250711071139p7ae10f5ch9ff90fc77c63cb0b@mail.gmail.com>
References: <507725E31752A74CB00728FA6C17D99C07A0E63E@NYWEXMB29.msad.ms.com>
	<e8e755250711071139p7ae10f5ch9ff90fc77c63cb0b@mail.gmail.com>
Message-ID: <e8e755250711071147k4bb8c8b6gd9f7ce52d8c27bf8@mail.gmail.com>

The third comment should say subtract the residuals from the data,
though the code is correct.

Jeff

On Nov 7, 2007 1:39 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> I think it is as simple as backing out from the residuals:
>
> # an MA2 model
> x <- arima.sim(list(ma=2),n=100)
>
> #Fitted as such...
> x.model <- arima(x,c(0,0,2))
>
> # SUBTRACT the residuals FROM the original data
> x.insample.fit <- x-residuals(x.model)
>
> # and you can even see them:
> plot(x)
> par(new=TRUE)
> lines(x.insample.fit,col=3,lty=2)
>
> Jeff
>
>
> On Nov 7, 2007 12:47 PM, Yalla, Swaroop (FID)
>
> <Swaroop.Yalla at morganstanley.com> wrote:
> > Hi:
> >
> > I have another ARIMA question for R. I was finally able to use ARIMA
> > modeling on my data. Now to forecast out of sample, we can use
> > predict(fit, n.ahead = 10) type of command and thats fine- but how can I
> > see the fit in-sample. I mean is there a easy way to just compare the
> > actual data with the fitted model in-sample?
> >
> > thanks for all the help..
> > Swaroop
> > --------------------------------------------------------
> >
> > This is not an offer (or solicitation of an offer) to bu...{{dropped:24}}
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From nbryant at optonline.net  Wed Nov  7 22:07:34 2007
From: nbryant at optonline.net (Nathan Bryant)
Date: Wed, 07 Nov 2007 16:07:34 -0500
Subject: [R-SIG-Finance] ARIMA question
In-Reply-To: <e8e755250711071139p7ae10f5ch9ff90fc77c63cb0b@mail.gmail.com>
References: <507725E31752A74CB00728FA6C17D99C07A0E63E@NYWEXMB29.msad.ms.com>
	<e8e755250711071139p7ae10f5ch9ff90fc77c63cb0b@mail.gmail.com>
Message-ID: <47322916.5010409@optonline.net>


There is a method called "fitted()" that applies to most model classes 
including Arima, which does the same thing.

Jeff Ryan wrote:
> I think it is as simple as backing out from the residuals:
>
> # an MA2 model
> x <- arima.sim(list(ma=2),n=100)
>
> #Fitted as such...
> x.model <- arima(x,c(0,0,2))
>
> # add the residuals to the original data
> x.insample.fit <- x-residuals(x.model)
>
> # and you can even see them:
> plot(x)
> par(new=TRUE)
> lines(x.insample.fit,col=3,lty=2)
>
> Jeff
>
>
> On Nov 7, 2007 12:47 PM, Yalla, Swaroop (FID)
> <Swaroop.Yalla at morganstanley.com> wrote:
>   
>> Hi:
>>
>> I have another ARIMA question for R. I was finally able to use ARIMA
>> modeling on my data. Now to forecast out of sample, we can use
>> predict(fit, n.ahead = 10) type of command and thats fine- but how can I
>> see the fit in-sample. I mean is there a easy way to just compare the
>> actual data with the fitted model in-sample?
>>
>> thanks for all the help..
>> Swaroop
>> --------------------------------------------------------
>>
>> This is not an offer (or solicitation of an offer) to bu...{{dropped:24}}
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>     
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
>


From jeff.a.ryan at gmail.com  Wed Nov  7 22:17:28 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 7 Nov 2007 15:17:28 -0600
Subject: [R-SIG-Finance] ARIMA question
In-Reply-To: <47322916.5010409@optonline.net>
References: <507725E31752A74CB00728FA6C17D99C07A0E63E@NYWEXMB29.msad.ms.com>
	<e8e755250711071139p7ae10f5ch9ff90fc77c63cb0b@mail.gmail.com>
	<47322916.5010409@optonline.net>
Message-ID: <e8e755250711071317k6b0dcb7vf03431a2e89d9ced@mail.gmail.com>

Do you have a particular version?  Mine does not have a method like that...

> x.model

Call:
arima(x = x, order = c(0, 0, 2))

Coefficients:
         ma1      ma2  intercept
      0.3812  -0.1138     0.0936
s.e.  0.0956   0.0862     0.2631

sigma^2 estimated as 4.318:  log likelihood = -215.15,  aic = 438.31
> class(x.model)
[1] "Arima"
> methods(fitted)
[1] fitted.default*         fitted.isoreg*          fitted.nls*
[4] fitted.quantmod*        fitted.smooth.spline*   fitted.values.quantmod*

   Non-visible functions are asterisked
> fitted(x.model)
NULL
>


On Nov 7, 2007 3:07 PM, Nathan Bryant <nbryant at optonline.net> wrote:
>
> There is a method called "fitted()" that applies to most model classes
> including Arima, which does the same thing.
>
>
> Jeff Ryan wrote:
> > I think it is as simple as backing out from the residuals:
> >
> > # an MA2 model
> > x <- arima.sim(list(ma=2),n=100)
> >
> > #Fitted as such...
> > x.model <- arima(x,c(0,0,2))
> >
> > # add the residuals to the original data
> > x.insample.fit <- x-residuals(x.model)
> >
> > # and you can even see them:
> > plot(x)
> > par(new=TRUE)
> > lines(x.insample.fit,col=3,lty=2)
> >
> > Jeff
> >
> >
> > On Nov 7, 2007 12:47 PM, Yalla, Swaroop (FID)
> > <Swaroop.Yalla at morganstanley.com> wrote:
> >
> >> Hi:
> >>
> >> I have another ARIMA question for R. I was finally able to use ARIMA
> >> modeling on my data. Now to forecast out of sample, we can use
> >> predict(fit, n.ahead = 10) type of command and thats fine- but how can I
> >> see the fit in-sample. I mean is there a easy way to just compare the
> >> actual data with the fitted model in-sample?
> >>
> >> thanks for all the help..
> >> Swaroop
> >> --------------------------------------------------------
> >>
> >> This is not an offer (or solicitation of an offer) to bu...{{dropped:24}}
> >>
> >> _______________________________________________
> >> R-SIG-Finance at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >> -- Subscriber-posting only.
> >> -- If you want to post, subscribe first.
> >>
> >>
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
> >
>
>


From jeff.a.ryan at gmail.com  Wed Nov  7 22:21:11 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 7 Nov 2007 15:21:11 -0600
Subject: [R-SIG-Finance] ARIMA question
In-Reply-To: <e8e755250711071317k6b0dcb7vf03431a2e89d9ced@mail.gmail.com>
References: <507725E31752A74CB00728FA6C17D99C07A0E63E@NYWEXMB29.msad.ms.com>
	<e8e755250711071139p7ae10f5ch9ff90fc77c63cb0b@mail.gmail.com>
	<47322916.5010409@optonline.net>
	<e8e755250711071317k6b0dcb7vf03431a2e89d9ced@mail.gmail.com>
Message-ID: <e8e755250711071321j117a3eb5ue2c3852d3c5d8cde@mail.gmail.com>

Actually there is one for the tseries class "arma" NOT "Arima" from stats.

> methods(fitted)
[1] fitted.arma*            fitted.default*         fitted.garch*
[4] fitted.isoreg*          fitted.nls*             fitted.quantmod*
[7] fitted.smooth.spline*   fitted.values.quantmod*

   Non-visible functions are asterisked

> search()
 [1] ".GlobalEnv"        "package:tseries"   "package:quadprog"
 [4] "package:quantmod"  "package:Defaults"  "package:zoo"
 [7] "package:stats"     "package:graphics"  "package:grDevices"
[10] "package:utils"     "package:datasets"  "package:methods"
[13] "Autoloads"         "package:base"
>

Jeff

On Nov 7, 2007 3:17 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Do you have a particular version?  Mine does not have a method like that...
>
> > x.model
>
> Call:
> arima(x = x, order = c(0, 0, 2))
>
> Coefficients:
>          ma1      ma2  intercept
>       0.3812  -0.1138     0.0936
> s.e.  0.0956   0.0862     0.2631
>
> sigma^2 estimated as 4.318:  log likelihood = -215.15,  aic = 438.31
> > class(x.model)
> [1] "Arima"
> > methods(fitted)
> [1] fitted.default*         fitted.isoreg*          fitted.nls*
> [4] fitted.quantmod*        fitted.smooth.spline*   fitted.values.quantmod*
>
>    Non-visible functions are asterisked
> > fitted(x.model)
> NULL
>
> >
>
>
> On Nov 7, 2007 3:07 PM, Nathan Bryant <nbryant at optonline.net> wrote:
> >
> > There is a method called "fitted()" that applies to most model classes
> > including Arima, which does the same thing.
> >
> >
> > Jeff Ryan wrote:
> > > I think it is as simple as backing out from the residuals:
> > >
> > > # an MA2 model
> > > x <- arima.sim(list(ma=2),n=100)
> > >
> > > #Fitted as such...
> > > x.model <- arima(x,c(0,0,2))
> > >
> > > # add the residuals to the original data
> > > x.insample.fit <- x-residuals(x.model)
> > >
> > > # and you can even see them:
> > > plot(x)
> > > par(new=TRUE)
> > > lines(x.insample.fit,col=3,lty=2)
> > >
> > > Jeff
> > >
> > >
> > > On Nov 7, 2007 12:47 PM, Yalla, Swaroop (FID)
> > > <Swaroop.Yalla at morganstanley.com> wrote:
> > >
> > >> Hi:
> > >>
> > >> I have another ARIMA question for R. I was finally able to use ARIMA
> > >> modeling on my data. Now to forecast out of sample, we can use
> > >> predict(fit, n.ahead = 10) type of command and thats fine- but how can I
> > >> see the fit in-sample. I mean is there a easy way to just compare the
> > >> actual data with the fitted model in-sample?
> > >>
> > >> thanks for all the help..
> > >> Swaroop
> > >> --------------------------------------------------------
> > >>
> > >> This is not an offer (or solicitation of an offer) to bu...{{dropped:24}}
> > >>
> > >> _______________________________________________
> > >> R-SIG-Finance at stat.math.ethz.ch mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > >> -- Subscriber-posting only.
> > >> -- If you want to post, subscribe first.
> > >>
> > >>
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> > >
> >
> >
>


From Swaroop.Yalla at MorganStanley.com  Wed Nov  7 22:22:50 2007
From: Swaroop.Yalla at MorganStanley.com (Yalla, Swaroop (FID))
Date: Wed, 7 Nov 2007 16:22:50 -0500
Subject: [R-SIG-Finance] ARIMA question
In-Reply-To: <e8e755250711071321j117a3eb5ue2c3852d3c5d8cde@mail.gmail.com>
References: <507725E31752A74CB00728FA6C17D99C07A0E63E@NYWEXMB29.msad.ms.com>
	<e8e755250711071139p7ae10f5ch9ff90fc77c63cb0b@mail.gmail.com>
	<47322916.5010409@optonline.net>
	<e8e755250711071317k6b0dcb7vf03431a2e89d9ced@mail.gmail.com>
	<e8e755250711071321j117a3eb5ue2c3852d3c5d8cde@mail.gmail.com>
Message-ID: <507725E31752A74CB00728FA6C17D99C07AC3B56@NYWEXMB29.msad.ms.com>

I am running into yet another problem - my model is a seasonal ARIMA -
and arima.sim ONLY applies to non-seasonal ARIMA models.

Swaroop 

-----Original Message-----
From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com] 
Sent: Wednesday, November 07, 2007 4:21 PM
To: Nathan Bryant
Cc: Yalla, Swaroop (FID); r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] ARIMA question

Actually there is one for the tseries class "arma" NOT "Arima" from
stats.

> methods(fitted)
[1] fitted.arma*            fitted.default*         fitted.garch*
[4] fitted.isoreg*          fitted.nls*             fitted.quantmod*
[7] fitted.smooth.spline*   fitted.values.quantmod*

   Non-visible functions are asterisked

> search()
 [1] ".GlobalEnv"        "package:tseries"   "package:quadprog"
 [4] "package:quantmod"  "package:Defaults"  "package:zoo"
 [7] "package:stats"     "package:graphics"  "package:grDevices"
[10] "package:utils"     "package:datasets"  "package:methods"
[13] "Autoloads"         "package:base"
>

Jeff

On Nov 7, 2007 3:17 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Do you have a particular version?  Mine does not have a method like
that...
>
> > x.model
>
> Call:
> arima(x = x, order = c(0, 0, 2))
>
> Coefficients:
>          ma1      ma2  intercept
>       0.3812  -0.1138     0.0936
> s.e.  0.0956   0.0862     0.2631
>
> sigma^2 estimated as 4.318:  log likelihood = -215.15,  aic = 438.31
> > class(x.model)
> [1] "Arima"
> > methods(fitted)
> [1] fitted.default*         fitted.isoreg*          fitted.nls*
> [4] fitted.quantmod*        fitted.smooth.spline*
fitted.values.quantmod*
>
>    Non-visible functions are asterisked
> > fitted(x.model)
> NULL
>
> >
>
>
> On Nov 7, 2007 3:07 PM, Nathan Bryant <nbryant at optonline.net> wrote:
> >
> > There is a method called "fitted()" that applies to most model 
> > classes including Arima, which does the same thing.
> >
> >
> > Jeff Ryan wrote:
> > > I think it is as simple as backing out from the residuals:
> > >
> > > # an MA2 model
> > > x <- arima.sim(list(ma=2),n=100)
> > >
> > > #Fitted as such...
> > > x.model <- arima(x,c(0,0,2))
> > >
> > > # add the residuals to the original data x.insample.fit <- 
> > > x-residuals(x.model)
> > >
> > > # and you can even see them:
> > > plot(x)
> > > par(new=TRUE)
> > > lines(x.insample.fit,col=3,lty=2)
> > >
> > > Jeff
> > >
> > >
> > > On Nov 7, 2007 12:47 PM, Yalla, Swaroop (FID) 
> > > <Swaroop.Yalla at morganstanley.com> wrote:
> > >
> > >> Hi:
> > >>
> > >> I have another ARIMA question for R. I was finally able to use 
> > >> ARIMA modeling on my data. Now to forecast out of sample, we can 
> > >> use predict(fit, n.ahead = 10) type of command and thats fine- 
> > >> but how can I see the fit in-sample. I mean is there a easy way 
> > >> to just compare the actual data with the fitted model in-sample?
> > >>
> > >> thanks for all the help..
> > >> Swaroop
> > >> --------------------------------------------------------
> > >>
> > >> This is not an offer (or solicitation of an offer) to 
> > >> bu...{{dropped:24}}
> > >>
> > >> _______________________________________________
> > >> R-SIG-Finance at stat.math.ethz.ch mailing list 
> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > >> -- Subscriber-posting only.
> > >> -- If you want to post, subscribe first.
> > >>
> > >>
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> > >
> >
> >
>
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/sell the securities/instruments mentioned or an official confirmation.  Morgan Stanley may deal as principal in or own or act as market maker for securities/instruments mentioned or may advise the issuers.  This is not research and is not from MS Research but it may refer to a research analyst/research report.  Unless indicated, these views are the author's and may differ from those of Morgan Stanley research or others in the Firm.  We do not represent this is accurate or complete and we may not update this.  Past performance is not indicative of future returns.  For additional information, research reports and important disclosures, contact me or see https://secure.ms.com/servlet/cls.  You should not use e-mail to request, authorize or effect the purchase or sale of any security or instrument, to send transfer instructions, or to effect any other transactions.  We cannot guarantee that any such requests received via e-mail will be processed in a timely manner.  This communication is solely for the addressee(s) and may contain confidential information.  We do not waive confidentiality by mistransmission.  Contact me if you do not wish to receive these communications.  In the UK, this communication is directed in the UK to those persons who are market counterparties or intermediate customers (as defined in the UK Financial Services Authority's rules).


From jeff.a.ryan at gmail.com  Wed Nov  7 22:30:19 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 7 Nov 2007 15:30:19 -0600
Subject: [R-SIG-Finance] ARIMA question
In-Reply-To: <507725E31752A74CB00728FA6C17D99C07AC3B56@NYWEXMB29.msad.ms.com>
References: <507725E31752A74CB00728FA6C17D99C07A0E63E@NYWEXMB29.msad.ms.com>
	<e8e755250711071139p7ae10f5ch9ff90fc77c63cb0b@mail.gmail.com>
	<47322916.5010409@optonline.net>
	<e8e755250711071317k6b0dcb7vf03431a2e89d9ced@mail.gmail.com>
	<e8e755250711071321j117a3eb5ue2c3852d3c5d8cde@mail.gmail.com>
	<507725E31752A74CB00728FA6C17D99C07AC3B56@NYWEXMB29.msad.ms.com>
Message-ID: <e8e755250711071330s3edd8fc7hd08f5d11758f9bb5@mail.gmail.com>

The problem is...?  The arima.sim was only for demonstration purposes
- it will work on any arima object.  Or am I missing your problem.

If you just want seasonal data there are series to be found out there
to 'practice' on...


On Nov 7, 2007 3:22 PM, Yalla, Swaroop (FID)
<Swaroop.Yalla at morganstanley.com> wrote:
> I am running into yet another problem - my model is a seasonal ARIMA -
> and arima.sim ONLY applies to non-seasonal ARIMA models.
>
> Swaroop
>
> -----Original Message-----
> From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com]
> Sent: Wednesday, November 07, 2007 4:21 PM
> To: Nathan Bryant
> Cc: Yalla, Swaroop (FID); r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] ARIMA question
>
>
> Actually there is one for the tseries class "arma" NOT "Arima" from
> stats.
>
> > methods(fitted)
> [1] fitted.arma*            fitted.default*         fitted.garch*
> [4] fitted.isoreg*          fitted.nls*             fitted.quantmod*
> [7] fitted.smooth.spline*   fitted.values.quantmod*
>
>    Non-visible functions are asterisked
>
> > search()
>  [1] ".GlobalEnv"        "package:tseries"   "package:quadprog"
>  [4] "package:quantmod"  "package:Defaults"  "package:zoo"
>  [7] "package:stats"     "package:graphics"  "package:grDevices"
> [10] "package:utils"     "package:datasets"  "package:methods"
> [13] "Autoloads"         "package:base"
> >
>
> Jeff
>
> On Nov 7, 2007 3:17 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> > Do you have a particular version?  Mine does not have a method like
> that...
> >
> > > x.model
> >
> > Call:
> > arima(x = x, order = c(0, 0, 2))
> >
> > Coefficients:
> >          ma1      ma2  intercept
> >       0.3812  -0.1138     0.0936
> > s.e.  0.0956   0.0862     0.2631
> >
> > sigma^2 estimated as 4.318:  log likelihood = -215.15,  aic = 438.31
> > > class(x.model)
> > [1] "Arima"
> > > methods(fitted)
> > [1] fitted.default*         fitted.isoreg*          fitted.nls*
> > [4] fitted.quantmod*        fitted.smooth.spline*
> fitted.values.quantmod*
> >
> >    Non-visible functions are asterisked
> > > fitted(x.model)
> > NULL
> >
> > >
> >
> >
> > On Nov 7, 2007 3:07 PM, Nathan Bryant <nbryant at optonline.net> wrote:
> > >
> > > There is a method called "fitted()" that applies to most model
> > > classes including Arima, which does the same thing.
> > >
> > >
> > > Jeff Ryan wrote:
> > > > I think it is as simple as backing out from the residuals:
> > > >
> > > > # an MA2 model
> > > > x <- arima.sim(list(ma=2),n=100)
> > > >
> > > > #Fitted as such...
> > > > x.model <- arima(x,c(0,0,2))
> > > >
> > > > # add the residuals to the original data x.insample.fit <-
> > > > x-residuals(x.model)
> > > >
> > > > # and you can even see them:
> > > > plot(x)
> > > > par(new=TRUE)
> > > > lines(x.insample.fit,col=3,lty=2)
> > > >
> > > > Jeff
> > > >
> > > >
> > > > On Nov 7, 2007 12:47 PM, Yalla, Swaroop (FID)
> > > > <Swaroop.Yalla at morganstanley.com> wrote:
> > > >
> > > >> Hi:
> > > >>
> > > >> I have another ARIMA question for R. I was finally able to use
> > > >> ARIMA modeling on my data. Now to forecast out of sample, we can
> > > >> use predict(fit, n.ahead = 10) type of command and thats fine-
> > > >> but how can I see the fit in-sample. I mean is there a easy way
> > > >> to just compare the actual data with the fitted model in-sample?
> > > >>
> > > >> thanks for all the help..
> > > >> Swaroop
> > > >> --------------------------------------------------------
> > > >>
> > > >> This is not an offer (or solicitation of an offer) to
> > > >> bu...{{dropped:24}}
> > > >>
> > > >> _______________________________________________
> > > >> R-SIG-Finance at stat.math.ethz.ch mailing list
> > > >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > >> -- Subscriber-posting only.
> > > >> -- If you want to post, subscribe first.
> > > >>
> > > >>
> > > >
> > > > _______________________________________________
> > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > -- Subscriber-posting only.
> > > > -- If you want to post, subscribe first.
> > > >
> > > >
> > >
> > >
> >
> --------------------------------------------------------
>
>
> This is not an offer (or solicitation of an offer) to buy/sell the securities/instruments mentioned or an official confirmation.  Morgan Stanley may deal as principal in or own or act as market maker for securities/instruments mentioned or may advise the issuers.  This is not research and is not from MS Research but it may refer to a research analyst/research report.  Unless indicated, these views are the author's and may differ from those of Morgan Stanley research or others in the Firm.  We do not represent this is accurate or complete and we may not update this.  Past performance is not indicative of future returns.  For additional information, research reports and important disclosures, contact me or see https://secure.ms.com/servlet/cls.  You should not use e-mail to request, authorize or effect the purchase or sale of any security or instrument, to send transfer instructions, or to effect any other transactions.  We cannot guarantee that any such requests received via e-mail will be processed in a timely manner.  This communication is solely for the addressee(s) and may contain confidential information.  We do not waive confidentiality by mistransmission.  Contact me if you do not wish to receive these communications.  In the UK, this communication is directed in the UK to those persons who are market counterparties or intermediate customers (as defined in the UK Financial Services Authority's rules).
>


From Swaroop.Yalla at MorganStanley.com  Wed Nov  7 22:36:59 2007
From: Swaroop.Yalla at MorganStanley.com (Yalla, Swaroop (FID))
Date: Wed, 7 Nov 2007 16:36:59 -0500
Subject: [R-SIG-Finance] ARIMA question
In-Reply-To: <e8e755250711071330s3edd8fc7hd08f5d11758f9bb5@mail.gmail.com>
References: <507725E31752A74CB00728FA6C17D99C07A0E63E@NYWEXMB29.msad.ms.com>
	<e8e755250711071139p7ae10f5ch9ff90fc77c63cb0b@mail.gmail.com>
	<47322916.5010409@optonline.net>
	<e8e755250711071317k6b0dcb7vf03431a2e89d9ced@mail.gmail.com>
	<e8e755250711071321j117a3eb5ue2c3852d3c5d8cde@mail.gmail.com>
	<507725E31752A74CB00728FA6C17D99C07AC3B56@NYWEXMB29.msad.ms.com>
	<e8e755250711071330s3edd8fc7hd08f5d11758f9bb5@mail.gmail.com>
Message-ID: <507725E31752A74CB00728FA6C17D99C07AC3B78@NYWEXMB29.msad.ms.com>

No I mean I get an error when I use arima.sim on my fitted model.
 
It works on models which don't have seasonality like
ARIMA(p,d,q)

But not on models of the type
ARIMA(p,d,q)X(P,D,Q)_S



-----Original Message-----
From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com] 
Sent: Wednesday, November 07, 2007 4:30 PM
To: Yalla, Swaroop (FID)
Cc: Nathan Bryant; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] ARIMA question

The problem is...?  The arima.sim was only for demonstration purposes
- it will work on any arima object.  Or am I missing your problem.

If you just want seasonal data there are series to be found out there to
'practice' on...


On Nov 7, 2007 3:22 PM, Yalla, Swaroop (FID)
<Swaroop.Yalla at morganstanley.com> wrote:
> I am running into yet another problem - my model is a seasonal ARIMA -

> and arima.sim ONLY applies to non-seasonal ARIMA models.
>
> Swaroop
>
> -----Original Message-----
> From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com]
> Sent: Wednesday, November 07, 2007 4:21 PM
> To: Nathan Bryant
> Cc: Yalla, Swaroop (FID); r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] ARIMA question
>
>
> Actually there is one for the tseries class "arma" NOT "Arima" from 
> stats.
>
> > methods(fitted)
> [1] fitted.arma*            fitted.default*         fitted.garch*
> [4] fitted.isoreg*          fitted.nls*             fitted.quantmod*
> [7] fitted.smooth.spline*   fitted.values.quantmod*
>
>    Non-visible functions are asterisked
>
> > search()
>  [1] ".GlobalEnv"        "package:tseries"   "package:quadprog"
>  [4] "package:quantmod"  "package:Defaults"  "package:zoo"
>  [7] "package:stats"     "package:graphics"  "package:grDevices"
> [10] "package:utils"     "package:datasets"  "package:methods"
> [13] "Autoloads"         "package:base"
> >
>
> Jeff
>
> On Nov 7, 2007 3:17 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> > Do you have a particular version?  Mine does not have a method like
> that...
> >
> > > x.model
> >
> > Call:
> > arima(x = x, order = c(0, 0, 2))
> >
> > Coefficients:
> >          ma1      ma2  intercept
> >       0.3812  -0.1138     0.0936
> > s.e.  0.0956   0.0862     0.2631
> >
> > sigma^2 estimated as 4.318:  log likelihood = -215.15,  aic = 438.31
> > > class(x.model)
> > [1] "Arima"
> > > methods(fitted)
> > [1] fitted.default*         fitted.isoreg*          fitted.nls*
> > [4] fitted.quantmod*        fitted.smooth.spline*
> fitted.values.quantmod*
> >
> >    Non-visible functions are asterisked
> > > fitted(x.model)
> > NULL
> >
> > >
> >
> >
> > On Nov 7, 2007 3:07 PM, Nathan Bryant <nbryant at optonline.net> wrote:
> > >
> > > There is a method called "fitted()" that applies to most model 
> > > classes including Arima, which does the same thing.
> > >
> > >
> > > Jeff Ryan wrote:
> > > > I think it is as simple as backing out from the residuals:
> > > >
> > > > # an MA2 model
> > > > x <- arima.sim(list(ma=2),n=100)
> > > >
> > > > #Fitted as such...
> > > > x.model <- arima(x,c(0,0,2))
> > > >
> > > > # add the residuals to the original data x.insample.fit <-
> > > > x-residuals(x.model)
> > > >
> > > > # and you can even see them:
> > > > plot(x)
> > > > par(new=TRUE)
> > > > lines(x.insample.fit,col=3,lty=2)
> > > >
> > > > Jeff
> > > >
> > > >
> > > > On Nov 7, 2007 12:47 PM, Yalla, Swaroop (FID) 
> > > > <Swaroop.Yalla at morganstanley.com> wrote:
> > > >
> > > >> Hi:
> > > >>
> > > >> I have another ARIMA question for R. I was finally able to use 
> > > >> ARIMA modeling on my data. Now to forecast out of sample, we 
> > > >> can use predict(fit, n.ahead = 10) type of command and thats 
> > > >> fine- but how can I see the fit in-sample. I mean is there a 
> > > >> easy way to just compare the actual data with the fitted model
in-sample?
> > > >>
> > > >> thanks for all the help..
> > > >> Swaroop
> > > >> --------------------------------------------------------
> > > >>
> > > >> This is not an offer (or solicitation of an offer) to 
> > > >> bu...{{dropped:24}}
> > > >>
> > > >> _______________________________________________
> > > >> R-SIG-Finance at stat.math.ethz.ch mailing list 
> > > >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > >> -- Subscriber-posting only.
> > > >> -- If you want to post, subscribe first.
> > > >>
> > > >>
> > > >
> > > > _______________________________________________
> > > > R-SIG-Finance at stat.math.ethz.ch mailing list 
> > > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > -- Subscriber-posting only.
> > > > -- If you want to post, subscribe first.
> > > >
> > > >
> > >
> > >
> >
> --------------------------------------------------------
>
>
> This is not an offer (or solicitation of an offer) to buy/sell the
securities/instruments mentioned or an official confirmation.  Morgan
Stanley may deal as principal in or own or act as market maker for
securities/instruments mentioned or may advise the issuers.  This is not
research and is not from MS Research but it may refer to a research
analyst/research report.  Unless indicated, these views are the author's
and may differ from those of Morgan Stanley research or others in the
Firm.  We do not represent this is accurate or complete and we may not
update this.  Past performance is not indicative of future returns.  For
additional information, research reports and important disclosures,
contact me or see https://secure.ms.com/servlet/cls.  You should not use
e-mail to request, authorize or effect the purchase or sale of any
security or instrument, to send transfer instructions, or to effect any
other transactions.  We cannot guarantee that any such requests received
via e-mail will be processed in a timely manner.  This communication is
solely for the addressee(s) and may contain confidential information.
We do not waive confidentiality by mistransmission.  Contact me if you
do not wish to receive these communications.  In the UK, this
communication is directed in the UK to those persons who are market
counterparties or intermediate customers (as defined in the UK Financial
Services Authority's rules).
>
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/sell the securities/instruments mentioned or an official confirmation.  Morgan Stanley may deal as principal in or own or act as market maker for securities/instruments mentioned or may advise the issuers.  This is not research and is not from MS Research but it may refer to a research analyst/research report.  Unless indicated, these views are the author's and may differ from those of Morgan Stanley research or others in the Firm.  We do not represent this is accurate or complete and we may not update this.  Past performance is not indicative of future returns.  For additional information, research reports and important disclosures, contact me or see https://secure.ms.com/servlet/cls.  You should not use e-mail to request, authorize or effect the purchase or sale of any security or instrument, to send transfer instructions, or to effect any other transactions.  We cannot guarantee that any such requests received via e-mail will be processed in a timely manner.  This communication is solely for the addressee(s) and may contain confidential information.  We do not waive confidentiality by mistransmission.  Contact me if you do not wish to receive these communications.  In the UK, this communication is directed in the UK to those persons who are market counterparties or intermediate customers (as defined in the UK Financial Services Authority's rules).


From nning_an at hotmail.com  Thu Nov  8 09:33:02 2007
From: nning_an at hotmail.com (ANNing-ning)
Date: Thu, 8 Nov 2007 16:33:02 +0800
Subject: [R-SIG-Finance] How to estimate an EGARCH model in R?
Message-ID: <BAY138-W41E2008E7B6BE8508FC1A9FF8B0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071108/972d59ad/attachment.pl 

From guillaume.nicoulaud at halbis.com  Thu Nov  8 10:20:40 2007
From: guillaume.nicoulaud at halbis.com (guillaume.nicoulaud at halbis.com)
Date: Thu, 8 Nov 2007 10:20:40 +0100
Subject: [R-SIG-Finance] =?iso-8859-15?q?R=E9f=2E_=3A_Re=3A__Interpolating?=
 =?iso-8859-15?q?/comparing_two_irregular_time/price_sequences=3F?=
Message-ID: <OF3A473ACD.C13B1BC4-ONC125738D.0032CBCC-C125738D.003354A1@hsbc.fr>

Rory,
Here is a generic version using the same idea:

data.frame(tm = Sys.time() - 1:10, dt = runif(10)) -> A
data.frame(tm = Sys.time() - cumsum( sample(1:3, 10, replace = TRUE) ),
      dt = runif(10)) -> B
by = "tm"

f = function(A, B, by) {
      # merge the 2 data.frames
      merge(A, B, by = by, all = TRUE) -> a
      # replace NAs with last observation (no linear approx here)
      colnames(a)[ colnames(a) != by ] -> ii
      lapply(ii, function(i) {
            # I found this on the web... and I'm still trying to understand how it works
            a[, i][ c(NA, which(!is.na(a[, i])))[cumsum(!is.na(a[, i])) + 1] ]
      } ) -> b
      # build an output data.frame from this
      do.call(data.frame, c(list(a[, by]), b) ) -> res
      colnames(res) <- c(by, ii)
      return(res)
}





                                                                                                                                                                            
                                                                                                                                                                            
                                                        Pour :   Rory Winston <rory.winston at gmail.com>                                                                      
                                                        cc :     r-sig-finance at stat.math.ethz.ch                                                                            
                                                        Objet :  Re: [R-SIG-Finance] Interpolating/comparing two irregular time/price sequences?                            
             Achim Zeileis                                                                                                                                                  
             <Achim.Zeileis at wu-wien.ac.at>                                                                                                                                  
             Envoy? par :                                                                                                                                                   
             r-sig-finance-bounces at stat.math.ethz.                                                                                                                          
             ch                                                                                                                                                             
                                                                                                                                                                            
                                                                                                                                                                            
             07/11/2007 19:59                                                                                                                                               
                                                                                                                                                                            
                                                                                                                                                                            




On Wed, 7 Nov 2007, Rory Winston wrote:

<snip>

> Has anyone done anything like this before? I'm looking at the zoo
> package to see if it can help me, but I havent quite figured out how
> to do this kind of thing yet.

<snip>

You can merge the two series and generate NAs for the missing time points
and then use a na.*() method to eliminate the NAs, e.g., by linear
interpolation or by carrying observations forward or backward.

A simple example is attached below. See Section 2.8 of
  vignette("zoo", package = "zoo")
for more details on NA methods.

  ## two time series
  x <- zoo(c(2.01, 2.54, 2.37),
           as.POSIXct("2007-11-07 18:47 GMT") + c(0, 125, 334))
  y <- zoo(c(2, 2.5, 2.35),
           as.POSIXct("2007-11-07 18:47 GMT") + c(2, 120, 339))

  ## the merged version
  xy <- merge(x, y)

  ## interpolate by linear approximation
  xy <- na.approx(xy, rule = 2)

  ## extract interpolated version of y at index of x
  y2 <- window(xy[,2], index(x))

hth,
Z

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.




Les informations contenues dans ce message sont confidentielles et peuvent constituer des informations privilegiees. Si vous n etes pas le destinataire de ce message, il vous est interdit de le copier, de le faire suivre, de le divulguer ou d en utiliser tout ou partie. Si vous avez recu ce message par erreur, merci de le supprimer de votre systeme, ainsi que toutes ses copies, et d en avertir immediatement l expediteur par message de retour.
Il est impossible de garantir que les communications par messagerie electronique arrivent en temps utile, sont securisees ou denuees de toute erreur ou virus. En consequence, l expediteur n accepte aucune responsabilite du fait des erreurs ou omissions qui pourraient en resulter.
--- ----------------------------------------------------- ---
The information contained in this e-mail is confidential...{{dropped:9}}


From a.trapletti at swissonline.ch  Thu Nov  8 12:37:20 2007
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Thu, 08 Nov 2007 12:37:20 +0100
Subject: [R-SIG-Finance] Interpolating/comparing two irregular
	time/price sequences?
Message-ID: <4732F4F0.7010902@swissonline.ch>

An HTML attachment was scrubbed...
URL: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071108/fc7e5b47/attachment.html 

From markus at insightfromdata.com  Thu Nov  8 14:29:02 2007
From: markus at insightfromdata.com (Markus Loecher)
Date: Thu, 8 Nov 2007 08:29:02 -0500
Subject: [R-SIG-Finance] yearweek creator in zoo
In-Reply-To: <mailman.1.1194519601.30831.r-sig-finance@stat.math.ethz.ch>
References: <mailman.1.1194519601.30831.r-sig-finance@stat.math.ethz.ch>
Message-ID: <E0112CAA-6CD4-441F-AD36-01A5EDC79279@insightfromdata.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071108/9461f842/attachment.pl 

From ggrothendieck at gmail.com  Thu Nov  8 14:58:04 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Nov 2007 08:58:04 -0500
Subject: [R-SIG-Finance] yearweek creator in zoo
In-Reply-To: <E0112CAA-6CD4-441F-AD36-01A5EDC79279@insightfromdata.com>
References: <mailman.1.1194519601.30831.r-sig-finance@stat.math.ethz.ch>
	<E0112CAA-6CD4-441F-AD36-01A5EDC79279@insightfromdata.com>
Message-ID: <971536df0711080558u42722299m4999f5ad86994a17@mail.gmail.com>

Try this:

library(zoo)
methods(class = "yearmon")  # shows which methods are defined
zoo:::Ops.yearmon  # shows source to Ops method

etc.

or even better see zoo home page:
 http://R-Forge.R-project.org/projects/zoo/
or CRAN:
  http://cran.r-project.org/src/contrib/Descriptions/zoo.html

and just download the source and examine yearmon.R or
yearqtr.R.

Note that in certain cases you don't really have to do anything
depending on what methods you need.  For example, if you
decide that "yyyyww" is ok then this works out-of-the-box:

z1 <- zoo(1:3, c("200701", "200702", "200703"))
z2 <- zoo(11:12, c("200703", "200704"))
merge(z1, z2)

since character strings of that form are already totally ordered,
character strings already have the minimal needed methods
defined and zoo does not need to know what the actual meanings
you have attached to the strings are.   Numbers of the form
200701, 200702, etc. would also work out of the box for the same
reason.

On Nov 8, 2007 8:29 AM, Markus Loecher <markus at insightfromdata.com> wrote:
> Dear fellow zoo-package users,
> I am quite intrigued by the ability to define my own custom index
> class in zoo. The example given in the zoo vignette is the (already
> built-in) yearmon class defined as:
>
>        yearmon <- function(x) structure(floor(12*x + .0001)/12, class =
> "yearmon")
>
> As my understanding of and practice with custom structures in R is
> rather weak, I am not entirely sure how to define my own weekly index,
> in particular, how would I add methods for human readable character
> representations and Ops and MATCH methods ?
> Any example would be greatly appreciated !
>
> Thanks in advance,
>
> Markus
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ezivot at u.washington.edu  Thu Nov  8 22:33:30 2007
From: ezivot at u.washington.edu (Eric Zivot)
Date: Thu, 8 Nov 2007 13:33:30 -0800
Subject: [R-SIG-Finance] Interpolating/comparing two irregulartime/price
	sequences?
In-Reply-To: <4732F4F0.7010902@swissonline.ch>
Message-ID: <200711082133.lA8LXUMw015388@smtp.washington.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071108/743bf3a0/attachment.pl 

From rory.winston at gmail.com  Fri Nov  9 00:51:26 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Thu, 8 Nov 2007 23:51:26 +0000
Subject: [R-SIG-Finance] Interpolating/comparing two irregulartime/price
	sequences?
In-Reply-To: <200711082133.lA8LXUMw015388@smtp.washington.edu>
References: <4732F4F0.7010902@swissonline.ch>
	<200711082133.lA8LXUMw015388@smtp.washington.edu>
Message-ID: <3f446aa30711081551g51d089bat32d6a2e199db7618@mail.gmail.com>

Thanks everyone for their extremely helpful comments on this issue.
Eric, that is a very interesting point you have raised. Did Peter
publish a paper on this topic? If so, do you happen to know the
title? I feel intuitively that the previous tick method should be more
reliable than interpolation for high-frequency data, although it would
be nice to see some research on this topic confirming this to be the
case.

Thanks
Rory


On Nov 8, 2007 9:33 PM, Eric Zivot <ezivot at u.washington.edu> wrote:
>
>
> Just a few quick comments on this issue
>
> The Olsen group book, Introduction to High Frequency Finance, discusses
> various interpolation schemes to align multiple irregularly spaced data. For
> realized variance modeling Peter Hansen at Stanford showed that one should
> use the "previous tick" method for aligning data to a common time clock and
> not an linear interpolation around neighboring ticks. The latter method
> leads to degenerate results as you sample more frequently since the
> quadratic variation of a line is zero.
>
> The type of alignment discussed below is handled in the timeSeries class in
> S-PLUS using the align() function. Diethelm Wuertz implemented a subset of
> this class in R and I think the align() function is there too.
>
>
>  ________________________________
>  From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Adrian
> Trapletti
> Sent: Thursday, November 08, 2007 3:37 AM
> To: rory.winston at gmail.com
> Cc: R-Finance
> Subject: Re: [R-SIG-Finance] Interpolating/comparing two irregulartime/price
> sequences?
>
>
>
> Rory,
>
> There is no best method for synchronizing high frequency data. It depends on
> the application. One of the pioneers for high frequency financial data
> modelling was http://www.olsen.ch . In the 90ies they published some
> articles where they used interpolation schemes to model irregularly spaced
> high frequency data with standard discrete time series methods. You can find
> some articles on their website. Currently, there is a lot of work on the
> topic realized variance/volatility, and when it comes to multivariate
> applications, you may find some methods there
> http://www.google.ch/search?hl=en&q=%22realized+covariance%22&btnG=Search&meta=
>
> Best regards
> Adrian
>
>
>
> Message: 1
> Date: Wed, 7 Nov 2007 18:31:16 +0000
> From: "Rory Winston" <rory.winston at gmail.com>
> Subject: [R-SIG-Finance] Interpolating/comparing two irregular
>  time/price sequences?
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID:
>  <3f446aa30711071031j37936e36i933be63c90f9ce4c at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
>
> Hi all
>
> I have two data frames, that both look like the following:
>
>
>
> > head(series1)
>
>  timestamp mid spread
> 1 1.194438e+12 2.10011 0.000260
> 2 1.194438e+12 2.10010 0.000290
> ...
>
> These two time sequences are sampled on price ticks, so the interval
> between ticks is stochastic and irregular. The time sequences are also
> of different lengths, i.e. one may have 8 hours worth of data, the
> other may have 4. My issue is that I want to compare these two series
> for similarity - they should be producing almost exactly the same
> data, although potentially at slightly different timestamps (hence the
> sampling irregularity). I can subset the data so that they span
> roughly the same time intervals, but the number of ticks in each
> series will be different. Basically what I am trying to achieve is
> some sort of constant interpolation based on a time index - so that if
> series A starts at 08:01, contains 10,000 ticks, and ends at 16:05,
> and series B starts at 08:00, contains 7,000 ticks, and ends at 16:06,
> I would like to be able to index from series A into series B at say,
> each timestamp in A. Using a simple example, for the following series
> A and B:
>
> A:
> time tick
> 16:01 2.05
> 16:02 2.06
>
>
> B:
> time tick
> 16:00 2.04
> 16:02 2.06
>
> I would like to be able to index from A into B at each tick from A, so
> I would get an output series that was the value of B at each time A
> ticked:
>
> C
> time tick
> 16:01 2.04 <--- constant interpolation from value of B @ 16:00
> 16:02 2.06
>
> Has anyone done anything like this before? I'm looking at the zoo
> package to see if it can help me, but I havent quite figured out how
> to do this kind of thing yet. Is this even a good way to checking
> whether series B is very similar to series A at the discrete tick
> intervals? Any better methods?(I guess another way might be to align
> the two subsetted series exactly and just take differences).
>
> Thanks
> Rory
>
>
>
>
>
> --
> Adrian Trapletti
> Wildsbergstrasse 31
> 8610 Uster
> Switzerland
>
> Phone : +41 (0) 44 9945630
> Mobile : +41 (0) 76 3705631
>
> Email : a.trapletti at swissonline.ch
>
>


From ggrothendieck at gmail.com  Fri Nov  9 00:55:24 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Nov 2007 18:55:24 -0500
Subject: [R-SIG-Finance] Interpolating/comparing two irregulartime/price
	sequences?
In-Reply-To: <200711082133.lA8LXUMw015388@smtp.washington.edu>
References: <4732F4F0.7010902@swissonline.ch>
	<200711082133.lA8LXUMw015388@smtp.washington.edu>
Message-ID: <971536df0711081555q4fe10c8fo212f763005c3f8c5@mail.gmail.com>

You can also get the previous tick by using

xy <- na.locf(xy)

in place of the na.approx line in Achim's code earlier in this thread.

On Nov 8, 2007 4:33 PM, Eric Zivot <ezivot at u.washington.edu> wrote:
> Just a few quick comments on this issue
>
> The Olsen group book, Introduction to High Frequency Finance, discusses
> various interpolation schemes to align multiple irregularly spaced data. For
> realized variance modeling Peter Hansen at Stanford showed that one should
> use the "previous tick" method for aligning data to a common time clock and
> not an linear interpolation around neighboring ticks. The latter method
> leads to degenerate results as you sample more frequently since the
> quadratic variation of a line is zero.
>
> The type of alignment discussed below is handled in the timeSeries class in
> S-PLUS using the align() function. Diethelm Wuertz implemented a subset of
> this class in R and I think the align() function is there too.
>
>
>  _____
>
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Adrian
> Trapletti
> Sent: Thursday, November 08, 2007 3:37 AM
> To: rory.winston at gmail.com
> Cc: R-Finance
> Subject: Re: [R-SIG-Finance] Interpolating/comparing two irregulartime/price
> sequences?
>
>
> Rory,
>
> There is no best method for synchronizing high frequency data. It depends on
> the application. One of the pioneers for high frequency financial data
> modelling was http://www.olsen.ch . In the 90ies they published some
> articles where they used interpolation schemes to model irregularly spaced
> high frequency data with standard discrete time series methods. You can find
> some articles on their website. Currently, there is a lot of work on the
> topic realized variance/volatility, and when it comes to multivariate
> applications, you may find some methods there
> http://www.google.ch/search?hl=en
> <http://www.google.ch/search?hl=en&q=%22realized+covariance%22&btnG=Search&m
> eta=> &q=%22realized+covariance%22&btnG=Search&meta=
>
> Best regards
> Adrian
>
>
>
>
> Message: 1
>
> Date: Wed, 7 Nov 2007 18:31:16 +0000
>
> From: "Rory Winston"  <mailto:rory.winston at gmail.com>
> <rory.winston at gmail.com>
>
> Subject: [R-SIG-Finance] Interpolating/comparing two irregular
>
>        time/price      sequences?
>
> To: r-sig-finance at stat.math.ethz.ch
>
> Message-ID:
>
>
> <mailto:3f446aa30711071031j37936e36i933be63c90f9ce4c at mail.gmail.com>
>
> <3f446aa30711071031j37936e36i933be63c90f9ce4c at mail.gmail.com>
>
> Content-Type: text/plain; charset=ISO-8859-1
>
>
>
> Hi all
>
>
>
> I have two data frames, that both look like the following:
>
>
>
>
>
> > head(series1)
>
>
>
>     timestamp     mid   spread
>
> 1 1.194438e+12 2.10011 0.000260
>
> 2 1.194438e+12 2.10010 0.000290
>
> ...
>
>
>
> These two time sequences are sampled on price ticks, so the interval
>
> between ticks is stochastic and irregular. The time sequences are also
>
> of different lengths, i.e. one may have 8 hours worth of data, the
>
> other may have 4. My issue is that I want to compare these two series
>
> for similarity - they should be producing almost exactly the same
>
> data, although potentially at slightly different timestamps (hence the
>
> sampling irregularity). I can subset the data so that they span
>
> roughly the same time intervals, but the number of ticks in each
>
> series will be different. Basically what I am trying to achieve is
>
> some sort of constant interpolation based on a time index - so that if
>
> series A starts at 08:01, contains 10,000 ticks, and ends at 16:05,
>
> and series B starts at 08:00, contains 7,000 ticks, and ends at 16:06,
>
> I would like to be able to index from series A into series B at say,
>
> each timestamp in A. Using a simple example, for the following series
>
> A and B:
>
>
>
> A:
>
> time   tick
>
> 16:01 2.05
>
> 16:02 2.06
>
>
>
>
>
> B:
>
> time tick
>
> 16:00 2.04
>
> 16:02 2.06
>
>
>
> I would like to be able to index from A into B at each tick from A, so
>
> I would get an output series that was the value of B at each time A
>
> ticked:
>
>
>
> C
>
> time tick
>
> 16:01 2.04 <--- constant interpolation from value of B @ 16:00
>
> 16:02 2.06
>
>
>
> Has anyone done anything like this before? I'm looking at the zoo
>
> package to see if it can help me, but I havent quite figured out how
>
> to do this kind of thing yet. Is this even a good way to checking
>
> whether series B is very similar to series A at the discrete tick
>
> intervals? Any better methods?(I guess another way might be to align
>
> the two subsetted series exactly and just take differences).
>
>
>
> Thanks
>
> Rory
>
>
>
>
>
>
>
>
>
>
> --
>
> Adrian Trapletti
>
> Wildsbergstrasse 31
>
> 8610 Uster
>
> Switzerland
>
>
>
> Phone :   +41 (0) 44 9945630
>
> Mobile :  +41 (0) 76 3705631
>
>
>
> Email :   a.trapletti at swissonline.ch
>
>        [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From nbryant at optonline.net  Fri Nov  9 01:06:11 2007
From: nbryant at optonline.net (Nathan Bryant)
Date: Thu, 08 Nov 2007 19:06:11 -0500
Subject: [R-SIG-Finance] ARIMA question
In-Reply-To: <e8e755250711071317k6b0dcb7vf03431a2e89d9ced@mail.gmail.com>
References: <507725E31752A74CB00728FA6C17D99C07A0E63E@NYWEXMB29.msad.ms.com>
	<e8e755250711071139p7ae10f5ch9ff90fc77c63cb0b@mail.gmail.com>
	<47322916.5010409@optonline.net>
	<e8e755250711071317k6b0dcb7vf03431a2e89d9ced@mail.gmail.com>
Message-ID: <4733A473.8020702@optonline.net>


Ok, fitted.Arima is in library(forecast)

Jeff Ryan wrote:
> Do you have a particular version?  Mine does not have a method like that...
>
>   
>> x.model
>>     
>
> Call:
> arima(x = x, order = c(0, 0, 2))
>
> Coefficients:
>          ma1      ma2  intercept
>       0.3812  -0.1138     0.0936
> s.e.  0.0956   0.0862     0.2631
>
> sigma^2 estimated as 4.318:  log likelihood = -215.15,  aic = 438.31
>   
>> class(x.model)
>>     
> [1] "Arima"
>   
>> methods(fitted)
>>     
> [1] fitted.default*         fitted.isoreg*          fitted.nls*
> [4] fitted.quantmod*        fitted.smooth.spline*   fitted.values.quantmod*
>
>    Non-visible functions are asterisked
>   
>> fitted(x.model)
>>     
> NULL
>   
>
>
> On Nov 7, 2007 3:07 PM, Nathan Bryant <nbryant at optonline.net> wrote:
>   
>> There is a method called "fitted()" that applies to most model classes
>> including Arima, which does the same thing.
>>
>>
>> Jeff Ryan wrote:
>>     
>>> I think it is as simple as backing out from the residuals:
>>>
>>> # an MA2 model
>>> x <- arima.sim(list(ma=2),n=100)
>>>
>>> #Fitted as such...
>>> x.model <- arima(x,c(0,0,2))
>>>
>>> # add the residuals to the original data
>>> x.insample.fit <- x-residuals(x.model)
>>>
>>> # and you can even see them:
>>> plot(x)
>>> par(new=TRUE)
>>> lines(x.insample.fit,col=3,lty=2)
>>>
>>> Jeff
>>>
>>>
>>> On Nov 7, 2007 12:47 PM, Yalla, Swaroop (FID)
>>> <Swaroop.Yalla at morganstanley.com> wrote:
>>>
>>>       
>>>> Hi:
>>>>
>>>> I have another ARIMA question for R. I was finally able to use ARIMA
>>>> modeling on my data. Now to forecast out of sample, we can use
>>>> predict(fit, n.ahead = 10) type of command and thats fine- but how can I
>>>> see the fit in-sample. I mean is there a easy way to just compare the
>>>> actual data with the fitted model in-sample?
>>>>
>>>> thanks for all the help..
>>>> Swaroop
>>>> --------------------------------------------------------
>>>>
>>>> This is not an offer (or solicitation of an offer) to bu...{{dropped:24}}
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>>>>
>>>>         
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>
>>>       
>>     
>
>


From nning_an at hotmail.com  Fri Nov  9 10:06:06 2007
From: nning_an at hotmail.com (ANNing-ning)
Date: Fri, 9 Nov 2007 17:06:06 +0800
Subject: [R-SIG-Finance] garchOxFit question
Message-ID: <BAY138-W18E0FCD3D631E1C6713098FF840@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071109/fa859e5d/attachment.pl 

From a.trapletti at swissonline.ch  Fri Nov  9 11:09:50 2007
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Fri, 09 Nov 2007 11:09:50 +0100
Subject: [R-SIG-Finance] Interpolating/comparing two irregulartime/price
 sequences?
In-Reply-To: <3f446aa30711081551g51d089bat32d6a2e199db7618@mail.gmail.com>
References: <4732F4F0.7010902@swissonline.ch>	
	<200711082133.lA8LXUMw015388@smtp.washington.edu>
	<3f446aa30711081551g51d089bat32d6a2e199db7618@mail.gmail.com>
Message-ID: <473431EE.8040802@swissonline.ch>

It really depends on the application what methods are appropriate. 
Possibilities include interpolation schemes (linear, previous tick, 
other), modelling in a framework that allows missing values (e.g. state 
space and Kalman filter), model prices/changes and time increments (e.g. 
Rob Engle published some work in this area).

For exmple, when important macro announcements are released, liquid 
instruments are traded immediately and the prices adjust very quickly 
(within less than a 1/10th of a second) to the new information. For less 
liquid instruments there is maybe for a longer time (several seconds up 
to several minutes) no trade. However, that does not mean that the price 
for the less liquid instrument did not update (you cannot trade anymore 
on the last observed price). It just means that there is no observation. 
Previous tick interpolation would lead to wrong conclusions (spurious 
lead/lag) in this example.

There is a lot of research in high-frequency finance in the hedge fund 
and investment bank industry (e.g. algorithmic trading, automatic market 
making). However, due to the nature of the business most is proprietary 
research.

Best regards
Adrian

Rory Winston wrote:

>Thanks everyone for their extremely helpful comments on this issue.
>Eric, that is a very interesting point you have raised. Did Peter
>publish a paper on this topic? If so, do you happen to know the
>title? I feel intuitively that the previous tick method should be more
>reliable than interpolation for high-frequency data, although it would
>be nice to see some research on this topic confirming this to be the
>case.
>
>Thanks
>Rory
>
>
>On Nov 8, 2007 9:33 PM, Eric Zivot <ezivot at u.washington.edu> wrote:
>  
>
>>Just a few quick comments on this issue
>>
>>The Olsen group book, Introduction to High Frequency Finance, discusses
>>various interpolation schemes to align multiple irregularly spaced data. For
>>realized variance modeling Peter Hansen at Stanford showed that one should
>>use the "previous tick" method for aligning data to a common time clock and
>>not an linear interpolation around neighboring ticks. The latter method
>>leads to degenerate results as you sample more frequently since the
>>quadratic variation of a line is zero.
>>
>>The type of alignment discussed below is handled in the timeSeries class in
>>S-PLUS using the align() function. Diethelm Wuertz implemented a subset of
>>this class in R and I think the align() function is there too.
>>
>>
>> ________________________________
>> From: r-sig-finance-bounces at stat.math.ethz.ch
>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Adrian
>>Trapletti
>>Sent: Thursday, November 08, 2007 3:37 AM
>>To: rory.winston at gmail.com
>>Cc: R-Finance
>>Subject: Re: [R-SIG-Finance] Interpolating/comparing two irregulartime/price
>>sequences?
>>
>>
>>
>>Rory,
>>
>>There is no best method for synchronizing high frequency data. It depends on
>>the application. One of the pioneers for high frequency financial data
>>modelling was http://www.olsen.ch . In the 90ies they published some
>>articles where they used interpolation schemes to model irregularly spaced
>>high frequency data with standard discrete time series methods. You can find
>>some articles on their website. Currently, there is a lot of work on the
>>topic realized variance/volatility, and when it comes to multivariate
>>applications, you may find some methods there
>>http://www.google.ch/search?hl=en&q=%22realized+covariance%22&btnG=Search&meta=
>>
>>Best regards
>>Adrian
>>
>>
>>
>>Message: 1
>>Date: Wed, 7 Nov 2007 18:31:16 +0000
>>From: "Rory Winston" <rory.winston at gmail.com>
>>Subject: [R-SIG-Finance] Interpolating/comparing two irregular
>> time/price sequences?
>>To: r-sig-finance at stat.math.ethz.ch
>>Message-ID:
>> <3f446aa30711071031j37936e36i933be63c90f9ce4c at mail.gmail.com>
>>Content-Type: text/plain; charset=ISO-8859-1
>>
>>Hi all
>>
>>I have two data frames, that both look like the following:
>>
>>
>>
>>    
>>
>>>head(series1)
>>>      
>>>
>> timestamp mid spread
>>1 1.194438e+12 2.10011 0.000260
>>2 1.194438e+12 2.10010 0.000290
>>...
>>
>>These two time sequences are sampled on price ticks, so the interval
>>between ticks is stochastic and irregular. The time sequences are also
>>of different lengths, i.e. one may have 8 hours worth of data, the
>>other may have 4. My issue is that I want to compare these two series
>>for similarity - they should be producing almost exactly the same
>>data, although potentially at slightly different timestamps (hence the
>>sampling irregularity). I can subset the data so that they span
>>roughly the same time intervals, but the number of ticks in each
>>series will be different. Basically what I am trying to achieve is
>>some sort of constant interpolation based on a time index - so that if
>>series A starts at 08:01, contains 10,000 ticks, and ends at 16:05,
>>and series B starts at 08:00, contains 7,000 ticks, and ends at 16:06,
>>I would like to be able to index from series A into series B at say,
>>each timestamp in A. Using a simple example, for the following series
>>A and B:
>>
>>A:
>>time tick
>>16:01 2.05
>>16:02 2.06
>>
>>
>>B:
>>time tick
>>16:00 2.04
>>16:02 2.06
>>
>>I would like to be able to index from A into B at each tick from A, so
>>I would get an output series that was the value of B at each time A
>>ticked:
>>
>>C
>>time tick
>>16:01 2.04 <--- constant interpolation from value of B @ 16:00
>>16:02 2.06
>>
>>Has anyone done anything like this before? I'm looking at the zoo
>>package to see if it can help me, but I havent quite figured out how
>>to do this kind of thing yet. Is this even a good way to checking
>>whether series B is very similar to series A at the discrete tick
>>intervals? Any better methods?(I guess another way might be to align
>>the two subsetted series exactly and just take differences).
>>
>>Thanks
>>Rory
>>
>>
>>
>>
>>
>>--
>>Adrian Trapletti
>>Wildsbergstrasse 31
>>8610 Uster
>>Switzerland
>>
>>Phone : +41 (0) 44 9945630
>>Mobile : +41 (0) 76 3705631
>>
>>Email : a.trapletti at swissonline.ch
>>
>>
>>    
>>
>
>  
>

-- 
Adrian Trapletti
Wildsbergstrasse 31
8610 Uster
Switzerland

Phone :   +41 (0) 44 9945630
Mobile :  +41 (0) 76 3705631

Email :   a.trapletti at swissonline.ch


From spencer.graves at pdf.com  Sat Nov 10 19:23:53 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Nov 2007 10:23:53 -0800
Subject: [R-SIG-Finance] nonstandard date time format?
Message-ID: <4735F739.6010304@pdf.com>

Hi, All: 

      What functions exist for processing dates and times beyond, e.g., 
strptime?  I can process the following examples using substr, paste, 
strptime, etc., but I wonder if more elegant ways exist: 

      * "    90110134228     18800    105.3750    105.5000    105.3750" 
records 4 numbers collected at 34228 seconds (9:30:28 AM) into 1 
November 1990. 

      * " 40100    103.0900" is interpreted as a quantity of 103.9 on 4 
January 2000. 

      * "1947 10    254.4" is 254.4 for October 1947. 

      * "1948 01 01   3.4" is 3.4 for 1 Jan. 1948 

      This is part of a soon-to-be released "FinTS" package companion to 
Ruey Tsay (2005) Analysis of Financial Time Series (Wiley).  Since this 
is a teaching tool, I'd like to use the best approach available.  
Otherwise, I'd just code something and move on to the next task. 

      Thanks,
      Spencer Graves    
p.s.  Anyone interested in this can obtain the current pre-release 
version via 
install.packages("FinTS",repos="http://r-forge.r-project.org").


From ggrothendieck at gmail.com  Sat Nov 10 22:38:14 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 10 Nov 2007 16:38:14 -0500
Subject: [R-SIG-Finance] nonstandard date time format?
In-Reply-To: <4735F739.6010304@pdf.com>
References: <4735F739.6010304@pdf.com>
Message-ID: <971536df0711101338u7dddc2bdya022958d1e1105c7@mail.gmail.com>

The first will require at least a minimum of processing but
the others can be done directly to create variables of chron class
c("dates", "times"), zoo class "yearmon" and class "Date"
respectively.  From those its easy to convert to other
classes:

> library(chron)
> chron(" 40100    103.0900", format = "dmy")
[1] 040100

> library(zoo)
> as.yearmon("1947 10    254.4", "%Y %m")
[1] "Oct 1947"

> as.Date("1948 01 01   3.4", "%Y %m %d")
[1] "1948-01-01"

There is an article on dates in R News 4/1 with a helpful
table at the end.  zoo comes with two vignettes.


On Nov 10, 2007 1:23 PM, Spencer Graves <spencer.graves at pdf.com> wrote:
> Hi, All:
>
>      What functions exist for processing dates and times beyond, e.g.,
> strptime?  I can process the following examples using substr, paste,
> strptime, etc., but I wonder if more elegant ways exist:
>
>      * "    90110134228     18800    105.3750    105.5000    105.3750"
> records 4 numbers collected at 34228 seconds (9:30:28 AM) into 1
> November 1990.
>
>      * " 40100    103.0900" is interpreted as a quantity of 103.9 on 4
> January 2000.
>
>      * "1947 10    254.4" is 254.4 for October 1947.
>
>      * "1948 01 01   3.4" is 3.4 for 1 Jan. 1948
>
>      This is part of a soon-to-be released "FinTS" package companion to
> Ruey Tsay (2005) Analysis of Financial Time Series (Wiley).  Since this
> is a teaching tool, I'd like to use the best approach available.
> Otherwise, I'd just code something and move on to the next task.
>
>      Thanks,
>      Spencer Graves
> p.s.  Anyone interested in this can obtain the current pre-release
> version via
> install.packages("FinTS",repos="http://r-forge.r-project.org").
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From spencer.graves at pdf.com  Sun Nov 11 00:04:41 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Nov 2007 15:04:41 -0800
Subject: [R-SIG-Finance] nonstandard date time format?
In-Reply-To: <971536df0711101338u7dddc2bdya022958d1e1105c7@mail.gmail.com>
References: <4735F739.6010304@pdf.com>
	<971536df0711101338u7dddc2bdya022958d1e1105c7@mail.gmail.com>
Message-ID: <47363909.1050807@pdf.com>

Hi, Gabor: 

      Thanks very much.  I've seen the R News and zoo vignettes before 
but forgot.  Thanks again for the reminder

      Spencer

Gabor Grothendieck wrote:
> The first will require at least a minimum of processing but
> the others can be done directly to create variables of chron class
> c("dates", "times"), zoo class "yearmon" and class "Date"
> respectively.  From those its easy to convert to other
> classes:
>
>   
>> library(chron)
>> chron(" 40100    103.0900", format = "dmy")
>>     
> [1] 040100
>
>   
>> library(zoo)
>> as.yearmon("1947 10    254.4", "%Y %m")
>>     
> [1] "Oct 1947"
>
>   
>> as.Date("1948 01 01   3.4", "%Y %m %d")
>>     
> [1] "1948-01-01"
>
> There is an article on dates in R News 4/1 with a helpful
> table at the end.  zoo comes with two vignettes.
>
>
> On Nov 10, 2007 1:23 PM, Spencer Graves <spencer.graves at pdf.com> wrote:
>   
>> Hi, All:
>>
>>      What functions exist for processing dates and times beyond, e.g.,
>> strptime?  I can process the following examples using substr, paste,
>> strptime, etc., but I wonder if more elegant ways exist:
>>
>>      * "    90110134228     18800    105.3750    105.5000    105.3750"
>> records 4 numbers collected at 34228 seconds (9:30:28 AM) into 1
>> November 1990.
>>
>>      * " 40100    103.0900" is interpreted as a quantity of 103.9 on 4
>> January 2000.
>>
>>      * "1947 10    254.4" is 254.4 for October 1947.
>>
>>      * "1948 01 01   3.4" is 3.4 for 1 Jan. 1948
>>
>>      This is part of a soon-to-be released "FinTS" package companion to
>> Ruey Tsay (2005) Analysis of Financial Time Series (Wiley).  Since this
>> is a teaching tool, I'd like to use the best approach available.
>> Otherwise, I'd just code something and move on to the next task.
>>
>>      Thanks,
>>      Spencer Graves
>> p.s.  Anyone interested in this can obtain the current pre-release
>> version via
>> install.packages("FinTS",repos="http://r-forge.r-project.org").
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>


From panyuml at gmail.com  Sun Nov 11 02:30:34 2007
From: panyuml at gmail.com (Weimin Mailing List)
Date: Sat, 10 Nov 2007 20:30:34 -0500
Subject: [R-SIG-Finance] Where is the Lagrange Multiplier test for ARCH
	effect in Rmetrics?
Message-ID: <f2e99fc90711101730s6bb12316q142f83a0a0391d4a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071110/8264e627/attachment.pl 

From panyuml at gmail.com  Sun Nov 11 03:31:32 2007
From: panyuml at gmail.com (Weimin Mailing List)
Date: Sat, 10 Nov 2007 21:31:32 -0500
Subject: [R-SIG-Finance] Shapiro-Wilk test yield different p-values in R and
	S-Plus
Message-ID: <f2e99fc90711101831r5c5d77x144e771fca35c46a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071110/446daf4e/attachment.pl 

From panyuml at gmail.com  Sun Nov 11 03:34:11 2007
From: panyuml at gmail.com (Weimin Mailing List)
Date: Sat, 10 Nov 2007 21:34:11 -0500
Subject: [R-SIG-Finance] Shapiro-Wilk test yield different p-values in R
	and S-Plus
In-Reply-To: <f2e99fc90711101831r5c5d77x144e771fca35c46a@mail.gmail.com>
References: <f2e99fc90711101831r5c5d77x144e771fca35c46a@mail.gmail.com>
Message-ID: <f2e99fc90711101834u6b6a9381nc19b5feba8154b94@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071110/1ac4fbcf/attachment.pl 

From swtzang at gmail.com  Mon Nov 12 00:40:50 2007
From: swtzang at gmail.com (ShyhWeir Tzang)
Date: Mon, 12 Nov 2007 07:40:50 +0800
Subject: [R-SIG-Finance] P values in coefficients from garch fitting
Message-ID: <c17037a10711111540w4b41e11frceca6095a551a048@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071112/70ed19e1/attachment.pl 

From ezivot at u.washington.edu  Mon Nov 12 02:37:40 2007
From: ezivot at u.washington.edu (Eric Zivot)
Date: Sun, 11 Nov 2007 17:37:40 -0800
Subject: [R-SIG-Finance] P values in coefficients from garch fitting
In-Reply-To: <c17037a10711111540w4b41e11frceca6095a551a048@mail.gmail.com>
References: <c17037a10711111540w4b41e11frceca6095a551a048@mail.gmail.com>
Message-ID: <000a01c824cc$9f318490$02000002@zivotd800>

In S-PLUS the default p-values used by summary.mgarch() are based on the
Student's t distribution. Here is the relevant code where the p-value is
computed

pv <- 2 * (1 - pt(abs(tv), xdf))

and pt() is the function for computing the cdf of the Student's t
distribution with xdf degrees of freedom. 

What is important is not the p-value distribution but the formula for the
asymptotic variance of the garch estimates. In splus the default is to use
the inverse of the numerical Hessian of the garch likelihood. Typically this
is the Gaussian distribution. However, S-plus has the option of computing
the Quasi maximum likelihood asymptotic variance based on the so-called
sandwich formula. 

In my experience garch estimates vary considerably across software packages
(see the review articles by Chris Brooks for more details). This is mainly
due to different optimizers, different convergence parameters etc. As a
result, one would not expect the p-values (or estimates) to match exactly
across different software. 


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of ShyhWeir Tzang
Sent: Sunday, November 11, 2007 3:41 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] P values in coefficients from garch fitting

Dear all:

I have seen different p values of the estimated coefficients using R and
S-plus after running the garch-fitting procedures. Why? Eviews denote that
the p values are z values. What are the distribution used in R and S-Plus?
Thanks for  your help.

ShyhWeir Tzang

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From ianseow at gmail.com  Wed Nov 14 04:22:19 2007
From: ianseow at gmail.com (Ian Seow)
Date: Wed, 14 Nov 2007 11:22:19 +0800
Subject: [R-SIG-Finance] disaggregating weekly to daily series
Message-ID: <1865010711131922u67d72374x4a72600070b3ea5a@mail.gmail.com>

Hi guys, was wondering if there was any elegant function to transform
a weekly financial timeseries into a daily series. The tricky part
about the daily series is that it is irregular - it doesn't include
trading holidays and weekends. Both date vectors weekly and daily are
given.

For instance:

Weekly series:-

1999-02-19    128.72
1999-02-26    129.15
1999-03-05    131.76
... etc ...

To this

Daily series:-

1999-02-15    128.72
1999-02-16    128.72
1999-02-17    128.72
1999-02-18    128.72
1999-02-19    128.72
1999-02-22    129.15
1999-02-23    129.15
1999-02-24    129.15
... etc ...

Thanks.

Rgds
Ian Seow


From adrian_d at eskimo.com  Wed Nov 14 04:40:41 2007
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Tue, 13 Nov 2007 19:40:41 -0800 (PST)
Subject: [R-SIG-Finance] disaggregating weekly to daily series
In-Reply-To: <1865010711131922u67d72374x4a72600070b3ea5a@mail.gmail.com>
References: <1865010711131922u67d72374x4a72600070b3ea5a@mail.gmail.com>
Message-ID: <Pine.SUN.4.58.0711131936140.18263@eskimo.com>


You need a data.frame with the correspondece between your daily dates and
weekly dates.

week        day
1999-02-19  1999-02-15
1999-02-19  1999-02-16
... etc ...

than do: merge(correspondence.df, weekly.df, by="week", all.y=TRUE)

Adrian

On Wed, 14 Nov 2007, Ian Seow wrote:

> Hi guys, was wondering if there was any elegant function to transform
> a weekly financial timeseries into a daily series. The tricky part
> about the daily series is that it is irregular - it doesn't include
> trading holidays and weekends. Both date vectors weekly and daily are
> given.
>
> For instance:
>
> Weekly series:-
>
> 1999-02-19    128.72
> 1999-02-26    129.15
> 1999-03-05    131.76
> ... etc ...
>
> To this
>
> Daily series:-
>
> 1999-02-15    128.72
> 1999-02-16    128.72
> 1999-02-17    128.72
> 1999-02-18    128.72
> 1999-02-19    128.72
> 1999-02-22    129.15
> 1999-02-23    129.15
> 1999-02-24    129.15
> ... etc ...
>
> Thanks.
>
> Rgds
> Ian Seow
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Wed Nov 14 04:45:32 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Nov 2007 22:45:32 -0500
Subject: [R-SIG-Finance] disaggregating weekly to daily series
In-Reply-To: <1865010711131922u67d72374x4a72600070b3ea5a@mail.gmail.com>
References: <1865010711131922u67d72374x4a72600070b3ea5a@mail.gmail.com>
Message-ID: <971536df0711131945s641e447ep2aabfc62ffc80cef@mail.gmail.com>

The zoo package can do that in one line but first read the weekly data
into a zoo object, w, and the dates into a one-column data frame d.

library(zoo)

Lines <- "1999-02-19    128.72
1999-02-26    129.15
1999-03-05    131.76
"
w <- read.zoo(textConnection(Lines))

Lines <- "1999-02-15
1999-02-16
1999-02-17
1999-02-18
1999-02-19
1999-02-22
1999-02-23
1999-02-24
"
d <- read.table(textConnection(Lines), colClasses = "Date")


# Now create a 0-width zoo object from the dates, d, and merge it with the
# weekly zoo object.  Then use na.locf to fill in NAs with Last Observation
# Carried Forward.

na.locf(merge(w, zoo(, d[[1]])))



On Nov 13, 2007 10:22 PM, Ian Seow <ianseow at gmail.com> wrote:
> Hi guys, was wondering if there was any elegant function to transform
> a weekly financial timeseries into a daily series. The tricky part
> about the daily series is that it is irregular - it doesn't include
> trading holidays and weekends. Both date vectors weekly and daily are
> given.
>
> For instance:
>
> Weekly series:-
>
> 1999-02-19    128.72
> 1999-02-26    129.15
> 1999-03-05    131.76
> ... etc ...
>
> To this
>
> Daily series:-
>
> 1999-02-15    128.72
> 1999-02-16    128.72
> 1999-02-17    128.72
> 1999-02-18    128.72
> 1999-02-19    128.72
> 1999-02-22    129.15
> 1999-02-23    129.15
> 1999-02-24    129.15
> ... etc ...
>
> Thanks.
>
> Rgds
> Ian Seow
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From frainj at tcd.ie  Thu Nov 15 16:45:53 2007
From: frainj at tcd.ie (John Frain)
Date: Thu, 15 Nov 2007 15:45:53 +0000
Subject: [R-SIG-Finance] rounding in FGarch procedure
Message-ID: <cfdde1650711150745j5644c304j6a5b11f6db4074ee@mail.gmail.com>

I have used garchFit to estimate several arma(p,q) garch(1,1) models .
 To check that I had understood the instructions I extracted

original series (x)
residuals (resid)
fit (xhat)
estimated variance (sigmasq)

and the coefficient estimates
omega
alpha1
beta1

I then tried to verify that

sigmasq = omega + alpha1 * (resid(-1))^2 +beta1 * sigmasq(-1)

In general I find a small error which looks to me to be too large and
in the to be due to rounding.  They also have the same sign throughout
the sample. (4967 daily observations).  A sample of the knd of results
that I am getting is below. Am I missing something or has anyone else
the same problem

Best Regards

John



> temp=cbind(fit4.ngarch at data$x,fit4.ngarch at residuals,fit4.ngarch at fitted,
+         fit4.ngarch at h.t,fit4.ngarch at sigma.t)
> colnames(temp) = c("x","resid","xhat","sigmasq","sigma")
> temp[1:10,]
               x      resid        xhat   sigmasq     sigma
 [1,] -3.2854337  0.0000000 -3.28543369 0.9483540 0.9738347
 [2,] -2.6706190 -1.5040106 -1.16660833 0.8644764 0.9297722
 [3,]  0.3662351  1.0271960 -0.66096087 0.9928236 0.9964053
 [4,] -1.8030495 -1.6765275 -0.12652202 0.9975101 0.9987543
 [5,]  1.1893279  1.5210782 -0.33175026 1.1582024 1.0761981
 [6,] -0.1981119 -0.2543172  0.05620530 1.2545135 1.1200507
 [7,]  0.6880319  0.7539935 -0.06596168 1.1382125 1.0668704
 [8,] -0.6580117 -0.6948362  0.03682448 1.0813582 1.0398837
 [9,] -3.8777101 -3.7420864 -0.13562371 1.0239515 1.0119049
[10,] -1.3002568 -0.6696918 -0.63056507 2.1796475 1.4763629
> fit4.ngarch at fit$coef
         mu         ar1         ma1       omega      alpha1       beta1
-0.04867703  0.34026902 -0.19710303  0.03442607  0.08919412  0.87542879
> omega = fit4.ngarch at fit$coef[["omega"]]
> alpha1 = fit4.ngarch at fit$coef[["alpha1"]]
> beta1 = fit4.ngarch at fit$coef[["beta1"]]
> nobs = length(temp[,1])
> error = omega + alpha1 * (temp[1:(nobs-1),2])^2 +beta1 * temp[1:(nobs-1),4]  - temp[2:nobs,4]
> error[1:10]
 [1] 0.0001660433 0.0001513575 0.0001738293 0.0001746498 0.0002027847
0.0002196475 0.0001992848 0.0001893304 0.0001792793 0.0003816252
>



-- 
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


From Markus.Gesmann at libero.uk.com  Thu Nov 15 17:21:56 2007
From: Markus.Gesmann at libero.uk.com (Markus Gesmann)
Date: Thu, 15 Nov 2007 16:21:56 +0000
Subject: [R-SIG-Finance] fExtremes labels
Message-ID: <5B78F43018FC3D488411E5F4903362E311A5975DFC@OBG-EXC-01.OBG>

Dear All,

Maybe someone can help me to understand the labels of the Excess Distribution plot '.gpd1Plot' of the package fExtremes, see the top left plot in the example:

library(fExtermes)
example(gpdFit)

It appears to me, that the x- and y-label are mixed up.

Kind regards,

Markus



This message is intended for the personal and confidential use for the designated recipient(s) named above.  If you are not the intended recipient of this message you are hereby notified that any review, dissemination,  distribution or copying of this message is strictly prohibited. This communication is for information purposes only and should not be regarded as an offer to sell or as a solicitation of an offer to buy any financial product, an official confirmation of any transaction or as an official statement of Libero Ventures Ltd.  Email transmissions cannot be guaranteed to be secure or error-free. Therefore we do not represent that this information is complete or accurate and it should not be relied upon as such.  All information is subject to change without notice.


From alexander.f.moreno at gmail.com  Wed Nov 21 05:38:41 2007
From: alexander.f.moreno at gmail.com (Alexander Moreno)
Date: Tue, 20 Nov 2007 22:38:41 -0600
Subject: [R-SIG-Finance] calendar to trading days
Message-ID: <4743B651.3010605@gmail.com>

Hi,

Is there some way to convert Oanda's get.hist.quote to trading days 
instead of calendar days using some simple technique?

Thanks,
Alex


From jeff.a.ryan at gmail.com  Wed Nov 21 16:12:33 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 21 Nov 2007 09:12:33 -0600
Subject: [R-SIG-Finance] calendar to trading days
In-Reply-To: <4743B651.3010605@gmail.com>
References: <4743B651.3010605@gmail.com>
Message-ID: <e8e755250711210712l1ab19aaand3923cb48d8a88a0@mail.gmail.com>

Hi Alex,

I think 'trading days' may be open to interpretation with respect to
FX - sort of up to you : )

As a start with quantmod ( http://www.quantmod.com and on CRAN ) you can try:

# get the rates in a zoo object indexed by Date as an object JPYUSD in
your environment
getSymbols("JPYUSD",src='oanda')

# now just the weekdays:
USDJPY[!weekdays(JPYUSD) %in% c('Sunday','Saturday')]

# and if you always take off monday as well ; )
USDJPY[!weekdays(JPYUSD) %in% c('Monday','Sunday','Saturday')]

You could further subset that by by a list of trading holidays...
One of probably an infinite solution set.

Another option from quantmod is sourcing the data from FRED, which is
the noon spot rate
per US weekday:

getSymbols("DEXJPUS",src="FRED")

Info on FRED FX is here: http://research.stlouisfed.org/fred2/categories/94

Without quantmod you won't have the quantmod weekdays.zoo function so
you'll have to spend a bit more
time typing to get what you want.

Jeff


On Nov 20, 2007 10:38 PM, Alexander Moreno <alexander.f.moreno at gmail.com> wrote:
> Hi,
>
> Is there some way to convert Oanda's get.hist.quote to trading days
> instead of calendar days using some simple technique?
>
> Thanks,
> Alex
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Wed Nov 21 16:19:40 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 21 Nov 2007 09:19:40 -0600
Subject: [R-SIG-Finance] calendar to trading days
In-Reply-To: <e8e755250711210712l1ab19aaand3923cb48d8a88a0@mail.gmail.com>
References: <4743B651.3010605@gmail.com>
	<e8e755250711210712l1ab19aaand3923cb48d8a88a0@mail.gmail.com>
Message-ID: <e8e755250711210719r5a7c52ecg116313ac796e5ed9@mail.gmail.com>

One typo ...

getSymbols("JPY/USD", src='oanda') #forgot the forward slash



On Nov 21, 2007 9:12 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Hi Alex,
>
> I think 'trading days' may be open to interpretation with respect to
> FX - sort of up to you : )
>
> As a start with quantmod ( http://www.quantmod.com and on CRAN ) you can try:
>
> # get the rates in a zoo object indexed by Date as an object JPYUSD in
> your environment
> getSymbols("JPYUSD",src='oanda')
>
> # now just the weekdays:
> USDJPY[!weekdays(JPYUSD) %in% c('Sunday','Saturday')]
>
> # and if you always take off monday as well ; )
> USDJPY[!weekdays(JPYUSD) %in% c('Monday','Sunday','Saturday')]
>
> You could further subset that by by a list of trading holidays...
> One of probably an infinite solution set.
>
> Another option from quantmod is sourcing the data from FRED, which is
> the noon spot rate
> per US weekday:
>
> getSymbols("DEXJPUS",src="FRED")
>
> Info on FRED FX is here: http://research.stlouisfed.org/fred2/categories/94
>
> Without quantmod you won't have the quantmod weekdays.zoo function so
> you'll have to spend a bit more
> time typing to get what you want.
>
> Jeff
>
>
>
> On Nov 20, 2007 10:38 PM, Alexander Moreno <alexander.f.moreno at gmail.com> wrote:
> > Hi,
> >
> > Is there some way to convert Oanda's get.hist.quote to trading days
> > instead of calendar days using some simple technique?
> >
> > Thanks,
> > Alex
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From jeff.a.ryan at gmail.com  Wed Nov 21 16:24:27 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 21 Nov 2007 09:24:27 -0600
Subject: [R-SIG-Finance] calendar to trading days
In-Reply-To: <e8e755250711210719r5a7c52ecg116313ac796e5ed9@mail.gmail.com>
References: <4743B651.3010605@gmail.com>
	<e8e755250711210712l1ab19aaand3923cb48d8a88a0@mail.gmail.com>
	<e8e755250711210719r5a7c52ecg116313ac796e5ed9@mail.gmail.com>
Message-ID: <e8e755250711210724i7d21052byab7743b7289d838b@mail.gmail.com>

And another typo - I will just take the rest of the week off myself : )

Here is the complete code without the comments:

getSymbols("JPY/USD", src="oanda")

JPYUSD[!weekdays(JPYUSD) %in% c("Saturday","Sunday")]

JPYUSD[!weekdays(JPYUSD) %in% c("Saturday","Sunday","Monday")]


Have a good week.


On Nov 21, 2007 9:19 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> One typo ...
>
> getSymbols("JPY/USD", src='oanda') #forgot the forward slash
>
>
>
>
> On Nov 21, 2007 9:12 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> > Hi Alex,
> >
> > I think 'trading days' may be open to interpretation with respect to
> > FX - sort of up to you : )
> >
> > As a start with quantmod ( http://www.quantmod.com and on CRAN ) you can try:
> >
> > # get the rates in a zoo object indexed by Date as an object JPYUSD in
> > your environment
> > getSymbols("JPYUSD",src='oanda')
> >
> > # now just the weekdays:
> > USDJPY[!weekdays(JPYUSD) %in% c('Sunday','Saturday')]
> >
> > # and if you always take off monday as well ; )
> > USDJPY[!weekdays(JPYUSD) %in% c('Monday','Sunday','Saturday')]
> >
> > You could further subset that by by a list of trading holidays...
> > One of probably an infinite solution set.
> >
> > Another option from quantmod is sourcing the data from FRED, which is
> > the noon spot rate
> > per US weekday:
> >
> > getSymbols("DEXJPUS",src="FRED")
> >
> > Info on FRED FX is here: http://research.stlouisfed.org/fred2/categories/94
> >
> > Without quantmod you won't have the quantmod weekdays.zoo function so
> > you'll have to spend a bit more
> > time typing to get what you want.
> >
> > Jeff
> >
> >
> >
> > On Nov 20, 2007 10:38 PM, Alexander Moreno <alexander.f.moreno at gmail.com> wrote:
> > > Hi,
> > >
> > > Is there some way to convert Oanda's get.hist.quote to trading days
> > > instead of calendar days using some simple technique?
> > >
> > > Thanks,
> > > Alex
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> >
>


From edd at debian.org  Wed Nov 21 17:10:17 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 21 Nov 2007 10:10:17 -0600
Subject: [R-SIG-Finance] calendar to trading days
In-Reply-To: <e8e755250711210712l1ab19aaand3923cb48d8a88a0@mail.gmail.com>
References: <4743B651.3010605@gmail.com>
	<e8e755250711210712l1ab19aaand3923cb48d8a88a0@mail.gmail.com>
Message-ID: <18244.22633.616191.712453@ron.nulle.part>


On 21 November 2007 at 09:12, Jeff Ryan wrote:
| I think 'trading days' may be open to interpretation with respect to
| FX - sort of up to you : )


The fCalendar package from the Rmetrics bundle may be of help as it contains
holiday calendars for many countries.  

But as Jeff says, for an OTC market it is somewhat open ... but if you 'just'
want to map foreign returns from a given exchange you can always use the
corresponding holiday information.

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From volchik2000 at list.ru  Sun Dec  2 18:36:20 2007
From: volchik2000 at list.ru (Yuri Volchik)
Date: Sun, 2 Dec 2007 17:36:20 -0000
Subject: [R-SIG-Finance] R memory management
Message-ID: <000001c83509$da27fa50$8e77eef0$@ru>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071202/29431a9d/attachment.pl 

From brian at braverock.com  Mon Dec  3 15:21:12 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 03 Dec 2007 08:21:12 -0600
Subject: [R-SIG-Finance] R memory management
In-Reply-To: <000001c83509$da27fa50$8e77eef0$@ru>
References: <000001c83509$da27fa50$8e77eef0$@ru>
Message-ID: <475410D8.7090406@braverock.com>

Yuri Volchik wrote:
> I'm using R to collect data for a number of exchanges through a socket
> connection and constantly running into memory problems even though task I
> believe is not that memory consuming. I guess there is a miscommunication
> between R and WinXP about freeing up memory.

<snip code example and gc stats>

> But the end result is in Task Manager:
> 
> RGui.exe  Mem Usage 470,472K  VM Size 541,988K
> 
> Even though R reports 
> 
> Garbage collection 16808 = 15754+802+252 (level 0) ... 
> 
> 6.1 Mbytes of cons cells used (7%)
> 
> 1.8 Mbytes of vectors used (1%)
> 
> Has anybody encountered this problem and how you guys deal with it?  It
> seems like a memory leak to me, as tasks are not memory demandind, the
> biggest amount of data in a single file is about 40MB.

Yuri,

This probably belongs on the R users list.  Most of the R core 
developers are on that list.  Even though you are working with exchange 
data, your question isn't really finance related, and you'll probably 
get  more assistance on r-users.

Regards,

     - Brian


From alexander.f.moreno at gmail.com  Mon Dec  3 23:17:31 2007
From: alexander.f.moreno at gmail.com (Alexander Moreno)
Date: Mon, 3 Dec 2007 16:17:31 -0600
Subject: [R-SIG-Finance] random walk: variable drift
Message-ID: <3303a4570712031417t60cbcfebo8605b1f33a28a985@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071203/92a958be/attachment.pl 

From ecjbosu at aol.com  Tue Dec  4 15:44:57 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Tue, 4 Dec 2007 14:44:57 +0000 (UTC)
Subject: [R-SIG-Finance] rulesFinCenter for America's
Message-ID: <loom.20071204T144206-904@post.gmane.org>

Rmetrics,

The DST rules for the US changed in 2007 to the 2nd Sunday in March for starting
DST and the first Sunday in November for ending DST.  A good link to see this
for Chicago is
http://www.timeanddate.com/worldclock/timezone.html?n=64&syear=2000

I would be glad to help rmetrics update this, just let me know with a reply. 

Happy Holidays!

Joe


From swtzang at gmail.com  Wed Dec  5 13:21:53 2007
From: swtzang at gmail.com (ShyhWeir Tzang)
Date: Wed, 5 Dec 2007 20:21:53 +0800
Subject: [R-SIG-Finance] GARCH estimation
Message-ID: <c17037a10712050421n50e3c9f3i14d1ef016e932d7e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071205/11ca8d38/attachment.pl 

From patrick at burns-stat.com  Wed Dec  5 18:03:43 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed, 05 Dec 2007 17:03:43 +0000
Subject: [R-SIG-Finance] GARCH estimation
In-Reply-To: <c17037a10712050421n50e3c9f3i14d1ef016e932d7e@mail.gmail.com>
References: <c17037a10712050421n50e3c9f3i14d1ef016e932d7e@mail.gmail.com>
Message-ID: <4756D9EF.9060504@burns-stat.com>

You need to do something in the event that ht
becomes non-positive.  For example, something
like:

if(ht <= 0) ht <- 1e-10

What number you use will depend on what the
reasonable range of values is for the particular
problem.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

ShyhWeir Tzang wrote:

>Dear all:
>
>I have a garch equation different from the traditional one:
>
> y(t)=b1+b2*x(t) + sqrt(h(t))*z(t), where z(t)~N(0,1)  h(t)=b3+b4*h(t-1)+b5*(
>z(t-1)-b6*sqrt(h(t-1) )^2
>
>I tried to use optim to estimate the six parameters:
>
>garch <- function(b,x,y){
>         b1<-b[1]
>         b2<-b[2]
>         b3<-b[3]
>         b4<-b[4]
>         b5<-b[5]
>         b6<-b[6]
>         N<-length(y)
>         lkhd<-0
>         ht<-var(y)
>         for (i in 1:N){
>                zt<-y[i]-b1-b2*x[i]
>                ht<-b3+b4*(ht)^2+b5*(zt/sqrt(ht)-b6*sqrt(ht))^2
>                lkhd<-lkhd +log(ht)+zt^2/ht
>                }
>        return(lkhd)
>}
>optim(c(0.01,0.2,0.2,0.2,0.2,0.2),method="BFGS",fn=garch,
>    x=data.x,y=data.y)
>
>However, I got the following warnings:
>In log(ht) : create NaNs
>In sqrt(ht) : create NaNs
>........................
>
>Can anyone help me interpreting these warnings? Are there any other better
>way to estimate the parameters? Thank you very much.
>
>ShyhWeir
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From josh at gghc.com  Wed Dec  5 18:16:05 2007
From: josh at gghc.com (Joshua Reich)
Date: Wed, 5 Dec 2007 12:16:05 -0500
Subject: [R-SIG-Finance] GARCH estimation
In-Reply-To: <4756D9EF.9060504@burns-stat.com>
References: <c17037a10712050421n50e3c9f3i14d1ef016e932d7e@mail.gmail.com>
	<4756D9EF.9060504@burns-stat.com>
Message-ID: <C20EA84D76C94F4E999DC041E81C0D11058BCF2F@exchange2k3.ny.gghc.com>

Why not use the L-BFGS-B method and supply a lower bound argument?

Josh

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Patrick
Burns
Sent: Wednesday, December 05, 2007 12:04 PM
To: ShyhWeir Tzang
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] GARCH estimation

You need to do something in the event that ht
becomes non-positive.  For example, something
like:

if(ht <= 0) ht <- 1e-10

What number you use will depend on what the
reasonable range of values is for the particular
problem.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

ShyhWeir Tzang wrote:

>Dear all:
>
>I have a garch equation different from the traditional one:
>
> y(t)=b1+b2*x(t) + sqrt(h(t))*z(t), where z(t)~N(0,1)
h(t)=b3+b4*h(t-1)+b5*(
>z(t-1)-b6*sqrt(h(t-1) )^2
>
>I tried to use optim to estimate the six parameters:
>
>garch <- function(b,x,y){
>         b1<-b[1]
>         b2<-b[2]
>         b3<-b[3]
>         b4<-b[4]
>         b5<-b[5]
>         b6<-b[6]
>         N<-length(y)
>         lkhd<-0
>         ht<-var(y)
>         for (i in 1:N){
>                zt<-y[i]-b1-b2*x[i]
>                ht<-b3+b4*(ht)^2+b5*(zt/sqrt(ht)-b6*sqrt(ht))^2
>                lkhd<-lkhd +log(ht)+zt^2/ht
>                }
>        return(lkhd)
>}
>optim(c(0.01,0.2,0.2,0.2,0.2,0.2),method="BFGS",fn=garch,
>    x=data.x,y=data.y)
>
>However, I got the following warnings:
>In log(ht) : create NaNs
>In sqrt(ht) : create NaNs
>........................
>
>Can anyone help me interpreting these warnings? Are there any other
better
>way to estimate the parameters? Thank you very much.
>
>ShyhWeir
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From patrick at burns-stat.com  Wed Dec  5 18:41:54 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed, 05 Dec 2007 17:41:54 +0000
Subject: [R-SIG-Finance] GARCH estimation
In-Reply-To: <C20EA84D76C94F4E999DC041E81C0D11058BCF2F@exchange2k3.ny.gghc.com>
References: <c17037a10712050421n50e3c9f3i14d1ef016e932d7e@mail.gmail.com>
	<4756D9EF.9060504@burns-stat.com>
	<C20EA84D76C94F4E999DC041E81C0D11058BCF2F@exchange2k3.ny.gghc.com>
Message-ID: <4756E2E2.2050006@burns-stat.com>

Putting box constraints on the parameters is probably
a good idea, but if I read the objective correctly, I don't
think that will guarantee 'ht' will always be positive.

Pat

Joshua Reich wrote:

>Why not use the L-BFGS-B method and supply a lower bound argument?
>
>Josh
>
>-----Original Message-----
>From: r-sig-finance-bounces at stat.math.ethz.ch
>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Patrick
>Burns
>Sent: Wednesday, December 05, 2007 12:04 PM
>To: ShyhWeir Tzang
>Cc: r-sig-finance at stat.math.ethz.ch
>Subject: Re: [R-SIG-Finance] GARCH estimation
>
>You need to do something in the event that ht
>becomes non-positive.  For example, something
>like:
>
>if(ht <= 0) ht <- 1e-10
>
>What number you use will depend on what the
>reasonable range of values is for the particular
>problem.
>
>Patrick Burns
>patrick at burns-stat.com
>+44 (0)20 8525 0696
>http://www.burns-stat.com
>(home of S Poetry and "A Guide for the Unwilling S User")
>
>ShyhWeir Tzang wrote:
>
>  
>
>>Dear all:
>>
>>I have a garch equation different from the traditional one:
>>
>>y(t)=b1+b2*x(t) + sqrt(h(t))*z(t), where z(t)~N(0,1)
>>    
>>
>h(t)=b3+b4*h(t-1)+b5*(
>  
>
>>z(t-1)-b6*sqrt(h(t-1) )^2
>>
>>I tried to use optim to estimate the six parameters:
>>
>>garch <- function(b,x,y){
>>        b1<-b[1]
>>        b2<-b[2]
>>        b3<-b[3]
>>        b4<-b[4]
>>        b5<-b[5]
>>        b6<-b[6]
>>        N<-length(y)
>>        lkhd<-0
>>        ht<-var(y)
>>        for (i in 1:N){
>>               zt<-y[i]-b1-b2*x[i]
>>               ht<-b3+b4*(ht)^2+b5*(zt/sqrt(ht)-b6*sqrt(ht))^2
>>               lkhd<-lkhd +log(ht)+zt^2/ht
>>               }
>>       return(lkhd)
>>}
>>optim(c(0.01,0.2,0.2,0.2,0.2,0.2),method="BFGS",fn=garch,
>>   x=data.x,y=data.y)
>>
>>However, I got the following warnings:
>>In log(ht) : create NaNs
>>In sqrt(ht) : create NaNs
>>........................
>>
>>Can anyone help me interpreting these warnings? Are there any other
>>    
>>
>better
>  
>
>>way to estimate the parameters? Thank you very much.
>>
>>ShyhWeir
>>
>>	[[alternative HTML version deleted]]
>>
>>_______________________________________________
>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>-- Subscriber-posting only. 
>>-- If you want to post, subscribe first.
>>
>>
>> 
>>
>>    
>>
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From chalabi at phys.ethz.ch  Fri Dec  7 12:07:06 2007
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Fri, 7 Dec 2007 03:07:06 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] rulesFinCenter for America's
In-Reply-To: <loom.20071204T144206-904@post.gmane.org>
References: <loom.20071204T144206-904@post.gmane.org>
Message-ID: <14209894.post@talk.nabble.com>


We are aware of this problem and it will be fixed in the next release.

regards,
Y.


Joe W. Byers-2 wrote:
> 
> Rmetrics,
> 
> The DST rules for the US changed in 2007 to the 2nd Sunday in March for
> starting
> DST and the first Sunday in November for ending DST.  A good link to see
> this
> for Chicago is
> http://www.timeanddate.com/worldclock/timezone.html?n=64&syear=2000
> 
> I would be glad to help rmetrics update this, just let me know with a
> reply. 
> 
> Happy Holidays!
> 
> Joe
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/rulesFinCenter-for-America%27s-tf4943616.html#a14209894
Sent from the Rmetrics mailing list archive at Nabble.com.


From ecjbosu at aol.com  Sat Dec  8 06:26:29 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Fri, 07 Dec 2007 23:26:29 -0600
Subject: [R-SIG-Finance] holidayNYSE missing some
In-Reply-To: <719928.15491.qm@web50703.mail.re2.yahoo.com>
References: <719928.15491.qm@web50703.mail.re2.yahoo.com>
Message-ID: <475A2B05.80906@aol.com>


John Putz wrote:
> The correct behavior is to shift the holiday to Friday (from Sat) or to Monday (from Sun).  I'm not actually using this for NYSE holidays but for power industry holidays and made a version to handle those changes as well as some other idiosyncracies.  Thanks for the suggestion though.
> 
> Message: 5
> Date: Thu, 6 Sep 2007 16:51:49 -0400
> From: "Charles Naylor" <Charles.Naylor at nikkoam.com>
> Subject: Re: [R-SIG-Finance] holidayNYSE missing some
> To: <r-sig-finance at stat.math.ethz.ch>
> Message-ID:
>     <A4678959B3D65D449266DE8D3825E0821F7F47 at nycmsg501.nikkoam.com>
> Content-Type: text/plain;    charset="us-ascii"
> 
> This is deliberate behavior.  If you check the code, the third-to-last
> line is as follows:
>     ans = ans[!(as.POSIXlt(ans at Data)$wday == 0 |
> as.POSIXlt(ans at Data)$wday== 6)]
> 
> 
> You could make an alternate version of holidayNYSE that omits this line,
> if you like.
> 
> -CN
> 
> Charles Naylor
> Assistant Vice President
> Global Dynamic Asset Allocation Group
> Nikko Alternative Asset Management, Inc.
> 535 Madison Avenue, Suite 2500
> New York, NY 10022
> T: 212.610.6158
> F: 212.610.6148
> E: charles.naylor at nikkoam.com
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of John Putz
> Sent: Thursday, September 06, 2007 4:44 PM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] holidayNYSE missing some
> 
> Hello,
> 
> I'm not sure if this is the correct list to email this too, but it
> appears that at least in R 2.5.0 that the holidayNYSE function in
> fCalendar does not include holidays that occur on Saturday.  E.g.
> holidayNYSE(2004) does not list Christmas.
> 
> Thanks, John.
>     [[alternative HTML version deleted]]
> 
>  
> 
> email: johnputz3655 at yahoo.com
> home: 206-632-6522
> cell: 206-910-5229
>   [[alternative HTML version deleted]]
> 
John,

I wrote this function for the NERC holidays you might have some interest 
in trying.  I also made some suggestions in a previous post in this list 
about this holidayNYSE problem, but did not hear from anyone on the 
feasibility of the suggestions.  This is also a problem with other US 
holidays, not just NYSE.  I would like to correct this for all the 
holiday functions but I do not want to have to overload the fCalendar 
functions every time I load fCalendar.

here is my holidayNERC()

#Method name: holidayNERC
#Written by: Joe W. Byers
#Creation Date:
#Modification Date:           Modifier:
#Inputs:  vector of Years
#Returns: holidays dates
#Example:

#*******************************************************************************
#Required Libraries

#*******************************************************************************
#Input and Temporary variables

#Holidays for the North American Energy Reliability Council (data from 
http://www.nerc.com/~oc/offpeaks.html):
#    * Saturdays
#    * Sundays
#    * New Year's Day, January 1st (possibly moved to Monday if actually 
on Sunday)
#    * Memorial Day, last Monday in May
#    * Independence Day, July 4th (moved to Monday if Sunday)
#    * Labor Day, first Monday in September
#    * Thanksgiving Day, fourth Thursday in November
#    * Christmas, December 25th (moved to Monday if Sunday)

holidayNERC<-function (year = 
currentYear,West=F,FinCenter='America/NewYork')
{
     holidays = NULL
     for (y in year) {
       holidays = c(holidays, as.character(USNewYearsDay(y)))
       holidays = c(holidays, as.character(USIndependenceDay(y)))
       holidays = c(holidays, as.character(USThanksgivingDay(y)))
       holidays = c(holidays, as.character(USChristmasDay(y)))
       holidays = c(holidays, as.character(USLaborDay(y)))
       holidays = c(holidays, as.character(USMemorialDay(y)))
     }
     holidays = sort(holidays)
     ans = timeDate(holidays)
     ans = ans + as.integer(as.POSIXlt(ans at Data)$wday == 0) *
         24 * 3600
     posix = as.POSIXlt(ans at Data)
     y = posix$year + 1900
     m = posix$mon + 1
     lastday = as.POSIXlt((timeCalendar(y = y + (m + 1)%/%13,
         m = m + 1 - (m + 1)%/%13 * 12, d = 1) - 24 * 3600)@Data)$mday
     ExceptOnLastFriday = timeDate(as.character(.last.of.nday(year = y,
         month = m, lastday = lastday, nday = 5)))
     ans = ans - as.integer(ans >= timeDate("1959-07-03") & 
as.POSIXlt(ans at Data)$wday ==
         0 & ans != ExceptOnLastFriday) * 24 * 3600
     if (West==F) {
       ans = ans[!(as.POSIXlt(ans at Data)$wday == 0 | 
as.POSIXlt(ans at Data)$wday ==
         6)]
     } else {
       ans = ans[!as.POSIXlt(ans at Data)$wday == 0]
       ans = ans[!(as.POSIXlt(ans at Data)$wday == 6 & 
!as.POSIXlt(ans at Data)$mon ==
         6)]
     }
     ans at FinCenter = FinCenter
     ans
}


From ecjbosu at aol.com  Sat Dec  8 06:32:15 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Fri, 07 Dec 2007 23:32:15 -0600
Subject: [R-SIG-Finance] [R-sig-finance] rulesFinCenter for America's
In-Reply-To: <14209894.post@talk.nabble.com>
References: <loom.20071204T144206-904@post.gmane.org>
	<14209894.post@talk.nabble.com>
Message-ID: <475A2C5F.2090300@aol.com>

Yohan Chalabi wrote:
> We are aware of this problem and it will be fixed in the next release.
> 
> regards,
> Y.
> 
> 
> Joe W. Byers-2 wrote:
>> Rmetrics,
>>
>> The DST rules for the US changed in 2007 to the 2nd Sunday in March for
>> starting
>> DST and the first Sunday in November for ending DST.  A good link to see
>> this
>> for Chicago is
>> http://www.timeanddate.com/worldclock/timezone.html?n=64&syear=2000
>>
>> I would be glad to help rmetrics update this, just let me know with a
>> reply. 
>>
>> Happy Holidays!
>>
>> Joe
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only. 
>> -- If you want to post, subscribe first.
>>
>>
> 

Very good and Thank you.  I also offer my assistance if you would like 
it to test some of rmetrics or help fixing some of the minor bugs that 
are found.  Has an consideration be given to changing the DST calendars 
to be more like the holidays where the DST is calculated using rules for 
regions rather than the hard coded date vectors?  The current method is 
simple and efficient for code but looks difficult to maintain.  The 
rules that can be derived from the website above might allow easier 
maintenance in the future.

Take care.
Joe


Joe


From jefe_goode at yahoo.com  Sun Dec  9 20:31:27 2007
From: jefe_goode at yahoo.com (jefe goode)
Date: Sun, 9 Dec 2007 11:31:27 -0800 (PST)
Subject: [R-SIG-Finance] Yahoo data
Message-ID: <265941.15090.qm@web61020.mail.yahoo.com>

Hi all

I've just checked Yahoo, and discovered that data for
the Nikkei 225 companies is not available.

Yes data for each of the S&P500 companies (+FTSE etc) 
is available but not for the Nikkei (or Hang Seng etc)

Does anyone know where we can get (free) data for
Nikkei companies?

Thanks

Jefe


From josh.m.ulrich at gmail.com  Sun Dec  9 21:37:17 2007
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Sun, 9 Dec 2007 14:37:17 -0600
Subject: [R-SIG-Finance] Yahoo data
In-Reply-To: <265941.15090.qm@web61020.mail.yahoo.com>
References: <265941.15090.qm@web61020.mail.yahoo.com>
Message-ID: <8cca69990712091237m2153620fsd3f63eb4d1826cfa@mail.gmail.com>

Hi Jefe,

Yahoo Finance help says you can get international data from their
respective websites:
http://help.yahoo.com/l/us/yahoo/finance/quotes/quote-02.html

You can find Yahoo Japan's (translated) historical data from on the
Tokyo stock exchange here:
http://translate.google.com/translate?u=http%3A%2F%2Ftable.yahoo.co.jp%2Ft%3Fs%3D2802.t%26a%3D9%26b%3D8%26c%3D2007%26d%3D12%26e%3D10%26f%3D2007%26g%3Dd%26q%3Dt%26y%3D0%26z%3D2802.t%26x%3D.csv&langpair=ja%7Cen&hl=en&ie=UTF8

I'm not sure if there's a way to easily download the data though.

Best,
Josh

On Dec 9, 2007 1:31 PM, jefe goode <jefe_goode at yahoo.com> wrote:
> Hi all
>
> I've just checked Yahoo, and discovered that data for
> the Nikkei 225 companies is not available.
>
> Yes data for each of the S&P500 companies (+FTSE etc)
> is available but not for the Nikkei (or Hang Seng etc)
>
> Does anyone know where we can get (free) data for
> Nikkei companies?
>
> Thanks
>
> Jefe
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

______________________________
http://quantemplation.blogspot.com/


From rory.winston at gmail.com  Mon Dec 10 11:57:20 2007
From: rory.winston at gmail.com (Rory Winston)
Date: Mon, 10 Dec 2007 10:57:20 +0000
Subject: [R-SIG-Finance] Extracting Interval-Based Data From Zoo Series
Message-ID: <3f446aa30712100257s5fc5e1f8v3a1510e6a0d59699@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071210/c7767d70/attachment.pl 

From edd at debian.org  Mon Dec 10 13:11:49 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 10 Dec 2007 06:11:49 -0600
Subject: [R-SIG-Finance] Extracting Interval-Based Data From Zoo Series
In-Reply-To: <3f446aa30712100257s5fc5e1f8v3a1510e6a0d59699@mail.gmail.com>
References: <3f446aa30712100257s5fc5e1f8v3a1510e6a0d59699@mail.gmail.com>
Message-ID: <18269.11525.753097.728162@ron.nulle.part>


On 10 December 2007 at 10:57, Rory Winston wrote:
| 1. I can see that as.POSIXct() can deal with millisecond resolution, but zoo
| doesnt seem to use the millis portion of the timestamp as an index variable.

It can, but you have to enable the printing:

> zz <- zoo(matrix(1:6, ncol=2), order.by=Sys.time() + seq(0,2)*1e-4)
> zz

2007-12-10 06:06:46 1 4
2007-12-10 06:06:46 2 5
2007-12-10 06:06:46 3 6
> options("digits.secs"=6)
> zz

2007-12-10 06:06:46.898058 1 4
2007-12-10 06:06:46.898158 2 5
2007-12-10 06:06:46.898258 3 6


That is a major bonus for high-frequency data -- POSIXt has a native
resolution to a microsecond.

| 2. What I would like to do with the data, is extract and aggregate
| sub-series. So for the tick data above, if I wanted to check for diurnal
[...]
| Is it possible to do this in a neater way? I tried using rollapply() to get
| a daily rolling window, but wasnt successful.

Because it is a 'native' format, you get to do all possible zoo tricks.

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From ggrothendieck at gmail.com  Mon Dec 10 16:49:33 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Dec 2007 10:49:33 -0500
Subject: [R-SIG-Finance] Extracting Interval-Based Data From Zoo Series
In-Reply-To: <3f446aa30712100257s5fc5e1f8v3a1510e6a0d59699@mail.gmail.com>
References: <3f446aa30712100257s5fc5e1f8v3a1510e6a0d59699@mail.gmail.com>
Message-ID: <971536df0712100749t3ca637efmcdeaa7cb05298ea1@mail.gmail.com>

On Dec 10, 2007 5:57 AM, Rory Winston <rory.winston at gmail.com> wrote:
> Hi all
>
> I have a few series with high-frequency tick data. The series is recorded at
> millisecond resolution, and a data file can span anything from a few hours
> to a calendar month.
>
> Here is an example of one file:
>
> 2007-10-23 00:00:36.139|1.419|1.4192
> 2007-10-23 00:00:42.629|1.4191|0
> 2007-10-23 00:01:03.109|1.419|0
> 2007-10-23 00:01:08.109|0|1.4191
>
> I load the file in as follows:
>
> eurusd <- read.zoo(file="data/eurusd_EBS_11_07.dat",
>    sep="|", FUN=as.POSIXct,
>    col.names=c("time","bid","ask"))
>
> # Replace zero ticks with NA, and thus locf
> coredata(eurusd)[,"bid"][coredata(eurusd)[,"bid"]==0] = NA
> coredata(eurusd)[,"ask"][coredata(eurusd)[,"ask"]==0] = NA
> eurusd <- na.locf(eurusd)
>
> I have a couple of questions:
>
> 1. I can see that as.POSIXct() can deal with millisecond resolution, but zoo
> doesnt seem to use the millis portion of the timestamp as an index variable.
> I tried to use a function as follows:
>
> convertDate <- function(x) {
>    # 2007-11-23 14:48:43.140
>    as.POSIXct(strptime(x, "%Y-%m-%d %H:%M:%OS"))
> }
>
> and then used this as the FUN argument in read.zoo(), but it still complains
> that index values must be unique. Is it possible to use a high-resolution
> index (milliseconds or an even finer recorded granularity)?
>
> 2. What I would like to do with the data, is extract and aggregate
> sub-series. So for the tick data above, if I wanted to check for diurnal
> seasonality, I would like to be able to extract as a daily series, the
> interarrival times of the ticks. I can  do this in a primitive way by:
>
> # Extract data for 12/11/07
> ts <- eurusd[as.Date(time(eurusd))=="2007-11-12"]
>
> plot(time(ts)[2:length(time(ts))], diff(time(ts)), type='h',
>  main="EBS EUR/USD Tick Interarrival Times\n12-11-2007",
>  xlab="Time", ylab="Interrarival Time (sec)")
>
>
> Is it possible to do this in a neater way? I tried using rollapply() to get
> a daily rolling window, but wasnt successful.
>

Try creating a zoo object of times and then plot its diff with plot.zoo:

tt <- time(eurusd)
ttz <- zoo(as.vector(tt), tt)
plot(diff(ttz))


From icos.atropa at gmail.com  Tue Dec 11 05:14:21 2007
From: icos.atropa at gmail.com (icosa atropa)
Date: Mon, 10 Dec 2007 21:14:21 -0700
Subject: [R-SIG-Finance] Extracting Interval-Based Data From Zoo Series
	(Rory Winston)
Message-ID: <681d07c20712102014u745e2105i5e50b6a5ebdf9bf8@mail.gmail.com>

I've had similar issues with zoo and POSIXct index uniqueness.  The
existing documentation requires very careful review.

To quote Gabor,

"
The zoo function and most functions in the zoo package have
no understanding of date time classes and only require that
such classes have ordered elements and support certain methods
as defined in ?zoo so zoo is irrelevant for understanding dates, times
and timezones.
[...]
Also please read the last line of every message to r-help [which asks
that code be self-enclosed]
"

>From ?strptime,
"
*     Specific to R is '%OSn', which for output gives the seconds to '0
*     <= n <= 6' decimal places (and if '%OS' is not followed by a
      digit, it uses the setting of 'getOption("digits.secs")', or if
*     that is unset, 'n = 3'). Further, for 'strptime' '%OS' will input
      seconds including fractional seconds.  Note that '%S' ignore
      fractional parts on output.
"

Try explicitly using %OS3 and make sure your operating system supports
this by making and examining such a POSIXct vector using duplicated()
and unique() outside of the read.zoo() call to see what's _actually_
happening.

best,
christian

> # Replace zero ticks with NA, and thus locf
> coredata(eurusd)[,"bid"][coredata(eurusd)[,"bid"]==0] = NA
> coredata(eurusd)[,"ask"][coredata(eurusd)[,"ask"]==0] = NA
> eurusd <- na.locf(eurusd)
>
> I have a couple of questions:
>
> 1. I can see that as.POSIXct() can deal with millisecond resolution, but zoo
> doesnt seem to use the millis portion of the timestamp as an index variable.
> I tried to use a function as follows:
>
> convertDate <- function(x) {
>     # 2007-11-23 14:48:43.140
>     as.POSIXct(strptime(x, "%Y-%m-%d %H:%M:%OS"))
> }
>
> and then used this as the FUN argument in read.zoo(), but it still complains
> that index values must be unique. Is it possible to use a high-resolution
> index (milliseconds or an even finer recorded granularity)?

-- 
Far better an approximate answer to the right question, which is often
vague, than the exact answer to the wrong question, which can always
be made precise -- j.w. tukey


From chalabi at phys.ethz.ch  Tue Dec 11 18:48:59 2007
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Tue, 11 Dec 2007 09:48:59 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] rulesFinCenter for America's
In-Reply-To: <475A2C5F.2090300@aol.com>
References: <loom.20071204T144206-904@post.gmane.org>
	<14209894.post@talk.nabble.com> <475A2C5F.2090300@aol.com>
Message-ID: <14279164.post@talk.nabble.com>


Hi Joe,

thanks for your offer. Help is always welcome! The best way to contribute is
to get the latest dev branch of Rmetrics at
https://svn.r-project.org/Rmetrics/trunk/ and test the packages you are used
to work with.

To come back to your remark, there are no real rules for DST because they
can be changed by temporal authorities. the best way is to keep updated with
one of the latest DST database. Actually, we extract data from the zoneinfo
database [http://en.wikipedia.org/wiki/Zoneinfo] and generate DST functions.
So, the function are not hard-coded but auto-generated.

take care,
Y.


Joe W. Byers-2 wrote:
> 
> Yohan Chalabi wrote:
>> We are aware of this problem and it will be fixed in the next release.
>> 
>> regards,
>> Y.
>> 
>> 
>> Joe W. Byers-2 wrote:
>>> Rmetrics,
>>>
>>> The DST rules for the US changed in 2007 to the 2nd Sunday in March for
>>> starting
>>> DST and the first Sunday in November for ending DST.  A good link to see
>>> this
>>> for Chicago is
>>> http://www.timeanddate.com/worldclock/timezone.html?n=64&syear=2000
>>>
>>> I would be glad to help rmetrics update this, just let me know with a
>>> reply. 
>>>
>>> Happy Holidays!
>>>
>>> Joe
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only. 
>>> -- If you want to post, subscribe first.
>>>
>>>
>> 
> 
> Very good and Thank you.  I also offer my assistance if you would like 
> it to test some of rmetrics or help fixing some of the minor bugs that 
> are found.  Has an consideration be given to changing the DST calendars 
> to be more like the holidays where the DST is calculated using rules for 
> regions rather than the hard coded date vectors?  The current method is 
> simple and efficient for code but looks difficult to maintain.  The 
> rules that can be derived from the website above might allow easier 
> maintenance in the future.
> 
> Take care.
> Joe
> 
> 
> Joe
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/rulesFinCenter-for-America%27s-tp14152260p14279164.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From edd at debian.org  Tue Dec 11 20:09:08 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 11 Dec 2007 13:09:08 -0600
Subject: [R-SIG-Finance] Extracting Interval-Based Data From Zoo
	Series	(Rory Winston)
In-Reply-To: <681d07c20712102014u745e2105i5e50b6a5ebdf9bf8@mail.gmail.com>
References: <681d07c20712102014u745e2105i5e50b6a5ebdf9bf8@mail.gmail.com>
Message-ID: <18270.57428.425874.604113@ron.nulle.part>


On 10 December 2007 at 21:14, icosa atropa wrote:
| I've had similar issues with zoo and POSIXct index uniqueness.  The
| existing documentation requires very careful review.
| 
| To quote Gabor,
| 
| "
| The zoo function and most functions in the zoo package have
| no understanding of date time classes and only require that
| such classes have ordered elements and support certain methods
| as defined in ?zoo so zoo is irrelevant for understanding dates, times
| and timezones.
| [...]
| Also please read the last line of every message to r-help [which asks
| that code be self-enclosed]
| "
| 
| >From ?strptime,
| "
| *     Specific to R is '%OSn', which for output gives the seconds to '0
| *     <= n <= 6' decimal places (and if '%OS' is not followed by a
|       digit, it uses the setting of 'getOption("digits.secs")', or if
| *     that is unset, 'n = 3'). Further, for 'strptime' '%OS' will input
|       seconds including fractional seconds.  Note that '%S' ignore
|       fractional parts on output.
| "
| 
| Try explicitly using %OS3 and make sure your operating system supports
| this by making and examining such a POSIXct vector using duplicated()
| and unique() outside of the read.zoo() call to see what's _actually_
| happening.


Zoo does tolerate identical time stamps for some operations [ which is why
you get a warning but not an error upon construction with identical time
stamps ], but it prefers non-zero differences between timestamps.  

I pad my high-resolution times with an eps=1.0-6 to make them distinct, if
need be.  

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From feanor0 at hotmail.com  Wed Dec 12 11:22:47 2007
From: feanor0 at hotmail.com (Murali Menon)
Date: Wed, 12 Dec 2007 10:22:47 +0000
Subject: [R-SIG-Finance] Riskmetrics volatility and correlation estimation
Message-ID: <BLU105-W12DAE56ADE374CE47041E9EE650@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071212/46adcc68/attachment.pl 

From brian at braverock.com  Wed Dec 12 13:05:07 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 12 Dec 2007 06:05:07 -0600
Subject: [R-SIG-Finance] Riskmetrics volatility and correlation
	estimation
In-Reply-To: <BLU105-W12DAE56ADE374CE47041E9EE650@phx.gbl>
References: <BLU105-W12DAE56ADE374CE47041E9EE650@phx.gbl>
Message-ID: <475FCE73.3080303@braverock.com>

Murali Menon wrote:
> Are there functions available to compute the Riskmetrics (1996) 
> volatilities and correlations for financial time-series? 
 > I refer to the exponentially weighted moving average vols
 > and exponentially smoothed correlations (with lambda = 0.94).
 > I looked in the VarModelling part of fPortfolio, but this
 > stuff doesn't seem to be there?

Not that I know of, but they shouldn't be too hard to construct.

So, if you want help constructing them from this group:

1> post the link to the RiskMetrics algorithms

2> do a little research on Google and the list archives

There have been several examples posted of exponentially weighted moving 
averages in R which should go a long way toward solving the volatility 
question above, for example.

R contains many different smoothing algorithms, as you can see with 
help.search("smooth") or help.search("smoothing")

3> suggest an approach, try an approach, post your failures

and I'm sure someone here can probably help you out.

Another interesting (to me anyway) question is "Why do you care?"  What 
in the literature leads you to want to try these techniques?  Are you 
just trying to replicate a set of RiskMetrics algorithms in R?  Have you 
looked at other research on smoothing and rolling windows?  I ask this 
trailing set of questions because often when I go looking in the 
literature, I find that there are several newer techniques which have 
been shown to work better than the older methods.  Sometimes these newer 
techniques are already implemented in R, other times I have to do it 
(but at least I'm then implementing a more modern approach).

Regards,

   - Brian


From feanor0 at hotmail.com  Wed Dec 12 13:50:15 2007
From: feanor0 at hotmail.com (Murali Menon)
Date: Wed, 12 Dec 2007 12:50:15 +0000
Subject: [R-SIG-Finance] Riskmetrics volatility and correlation
 estimation
In-Reply-To: <475FCE73.3080303@braverock.com>
References: <BLU105-W12DAE56ADE374CE47041E9EE650@phx.gbl>
	<475FCE73.3080303@braverock.com>
Message-ID: <BLU105-W219796907D05B8F7F15D86EE650@phx.gbl>



Brian,

Thanks for the suggestions and tips. Indeed, after nabbling away in the archives, I found a Zivot statement to the effect that EWMA (function available in fMultivar) on squared returns is equivalent to RiskMetrics vol. No equivalent methodology as far as I can see for correlations, so I thought I'd post a query before attempting to implement it myself.

As for RiskMetrics, I've seen the following paper that discusses why RiskMetrics seems to work reasonably well in practise even if it is mis-specified for the data (algorithms given herein):

http://www.colbud.hu/pdf/Kondor/riskm.pdf

The idea is that for the short-horizon, 95%-confidence level sort of VaR computation, even non-normal distributions have quantiles close to the normal one, so RiskMetrics works okay; but will fail to do so for longer horizons and/or 99%-ile queries.

Engle's DCC paper: http://pages.stern.nyu.edu/~rengle/dccfinal.pdf shows that RiskMetrics methodology outperforms the rolling technique in estimating and forecasting correlation, even if not always so well as dynamic conditional correlations (which I don't think is available in R either).

Thanks,

Murali

> Date: Wed, 12 Dec 2007 06:05:07 -0600
> From: brian at braverock.com
> To: feanor0 at hotmail.com; r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Riskmetrics volatility and correlation estimation
>
> Murali Menon wrote:
>> Are there functions available to compute the Riskmetrics (1996)
>> volatilities and correlations for financial time-series?
>> I refer to the exponentially weighted moving average vols
>> and exponentially smoothed correlations (with lambda = 0.94).
>> I looked in the VarModelling part of fPortfolio, but this
>> stuff doesn't seem to be there?
>
> Not that I know of, but they shouldn't be too hard to construct.
>
> So, if you want help constructing them from this group:
>
> 1> post the link to the RiskMetrics algorithms
>
> 2> do a little research on Google and the list archives
>
> There have been several examples posted of exponentially weighted moving
> averages in R which should go a long way toward solving the volatility
> question above, for example.
>
> R contains many different smoothing algorithms, as you can see with
> help.search("smooth") or help.search("smoothing")
>
> 3> suggest an approach, try an approach, post your failures
>
> and I'm sure someone here can probably help you out.
>
> Another interesting (to me anyway) question is "Why do you care?" What
> in the literature leads you to want to try these techniques? Are you
> just trying to replicate a set of RiskMetrics algorithms in R? Have you
> looked at other research on smoothing and rolling windows? I ask this
> trailing set of questions because often when I go looking in the
> literature, I find that there are several newer techniques which have
> been shown to work better than the older methods. Sometimes these newer
> techniques are already implemented in R, other times I have to do it
> (but at least I'm then implementing a more modern approach).
>
> Regards,
>
> - Brian

_________________________________________________________________
[[replacing trailing spam]]


From patrick at burns-stat.com  Wed Dec 12 19:01:16 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed, 12 Dec 2007 18:01:16 +0000
Subject: [R-SIG-Finance] Riskmetrics volatility and correlation
	estimation
In-Reply-To: <BLU105-W219796907D05B8F7F15D86EE650@phx.gbl>
References: <BLU105-W12DAE56ADE374CE47041E9EE650@phx.gbl>	<475FCE73.3080303@braverock.com>
	<BLU105-W219796907D05B8F7F15D86EE650@phx.gbl>
Message-ID: <476021EC.4040103@burns-stat.com>

The operation is essentially the same whether univariate
or multivariate -- just do an outer product of the returns
at each time.  For the univariate case this reduces to the
squared return.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Murali Menon wrote:

>Brian,
>
>Thanks for the suggestions and tips. Indeed, after nabbling away in the archives, I found a Zivot statement to the effect that EWMA (function available in fMultivar) on squared returns is equivalent to RiskMetrics vol. No equivalent methodology as far as I can see for correlations, so I thought I'd post a query before attempting to implement it myself.
>
>As for RiskMetrics, I've seen the following paper that discusses why RiskMetrics seems to work reasonably well in practise even if it is mis-specified for the data (algorithms given herein):
>
>http://www.colbud.hu/pdf/Kondor/riskm.pdf
>
>The idea is that for the short-horizon, 95%-confidence level sort of VaR computation, even non-normal distributions have quantiles close to the normal one, so RiskMetrics works okay; but will fail to do so for longer horizons and/or 99%-ile queries.
>
>Engle's DCC paper: http://pages.stern.nyu.edu/~rengle/dccfinal.pdf shows that RiskMetrics methodology outperforms the rolling technique in estimating and forecasting correlation, even if not always so well as dynamic conditional correlations (which I don't think is available in R either).
>
>Thanks,
>
>Murali
>
>  
>
>>Date: Wed, 12 Dec 2007 06:05:07 -0600
>>From: brian at braverock.com
>>To: feanor0 at hotmail.com; r-sig-finance at stat.math.ethz.ch
>>Subject: Re: [R-SIG-Finance] Riskmetrics volatility and correlation estimation
>>
>>Murali Menon wrote:
>>    
>>
>>>Are there functions available to compute the Riskmetrics (1996)
>>>volatilities and correlations for financial time-series?
>>>I refer to the exponentially weighted moving average vols
>>>and exponentially smoothed correlations (with lambda = 0.94).
>>>I looked in the VarModelling part of fPortfolio, but this
>>>stuff doesn't seem to be there?
>>>      
>>>
>>Not that I know of, but they shouldn't be too hard to construct.
>>
>>So, if you want help constructing them from this group:
>>
>>1> post the link to the RiskMetrics algorithms
>>
>>2> do a little research on Google and the list archives
>>
>>There have been several examples posted of exponentially weighted moving
>>averages in R which should go a long way toward solving the volatility
>>question above, for example.
>>
>>R contains many different smoothing algorithms, as you can see with
>>help.search("smooth") or help.search("smoothing")
>>
>>3> suggest an approach, try an approach, post your failures
>>
>>and I'm sure someone here can probably help you out.
>>
>>Another interesting (to me anyway) question is "Why do you care?" What
>>in the literature leads you to want to try these techniques? Are you
>>just trying to replicate a set of RiskMetrics algorithms in R? Have you
>>looked at other research on smoothing and rolling windows? I ask this
>>trailing set of questions because often when I go looking in the
>>literature, I find that there are several newer techniques which have
>>been shown to work better than the older methods. Sometimes these newer
>>techniques are already implemented in R, other times I have to do it
>>(but at least I'm then implementing a more modern approach).
>>
>>Regards,
>>
>>- Brian
>>    
>>
>
>_________________________________________________________________
>[[replacing trailing spam]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From d.louvet at gmail.com  Wed Dec 12 20:09:21 2007
From: d.louvet at gmail.com (Damien LOUVET)
Date: Wed, 12 Dec 2007 20:09:21 +0100
Subject: [R-SIG-Finance] Header intact
Message-ID: <cbdef58e0712121109k26f1d437ic68f3fd47b7bc9eb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071212/97d6d898/attachment.pl 

From ahala2000 at yahoo.com  Wed Dec 12 22:54:48 2007
From: ahala2000 at yahoo.com (elton wang)
Date: Wed, 12 Dec 2007 13:54:48 -0800 (PST)
Subject: [R-SIG-Finance] Riskmetrics volatility and correlation
	estimation
In-Reply-To: <475FCE73.3080303@braverock.com>
Message-ID: <940469.6352.qm@web31407.mail.mud.yahoo.com>

cut it short,
yon can check emaTA() in fTrading. then it is
straightforward.


--- "Brian G. Peterson" <brian at braverock.com> wrote:

> Murali Menon wrote:
> > Are there functions available to compute the
> Riskmetrics (1996) 
> > volatilities and correlations for financial
> time-series? 
>  > I refer to the exponentially weighted moving
> average vols
>  > and exponentially smoothed correlations (with
> lambda = 0.94).
>  > I looked in the VarModelling part of fPortfolio,
> but this
>  > stuff doesn't seem to be there?
> 
> Not that I know of, but they shouldn't be too hard
> to construct.
> 
> So, if you want help constructing them from this
> group:
> 
> 1> post the link to the RiskMetrics algorithms
> 
> 2> do a little research on Google and the list
> archives
> 
> There have been several examples posted of
> exponentially weighted moving 
> averages in R which should go a long way toward
> solving the volatility 
> question above, for example.
> 
> R contains many different smoothing algorithms, as
> you can see with 
> help.search("smooth") or help.search("smoothing")
> 
> 3> suggest an approach, try an approach, post your
> failures
> 
> and I'm sure someone here can probably help you out.
> 
> Another interesting (to me anyway) question is "Why
> do you care?"  What 
> in the literature leads you to want to try these
> techniques?  Are you 
> just trying to replicate a set of RiskMetrics
> algorithms in R?  Have you 
> looked at other research on smoothing and rolling
> windows?  I ask this 
> trailing set of questions because often when I go
> looking in the 
> literature, I find that there are several newer
> techniques which have 
> been shown to work better than the older methods. 
> Sometimes these newer 
> techniques are already implemented in R, other times
> I have to do it 
> (but at least I'm then implementing a more modern
> approach).
> 
> Regards,
> 
>    - Brian
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 



      ____________________________________________________________________________________
Looking for last minute shopping deals?


From s_achath at rediffmail.com  Fri Dec 14 09:49:49 2007
From: s_achath at rediffmail.com (Sudhakar Achath)
Date: 14 Dec 2007 08:49:49 -0000
Subject: [R-SIG-Finance] Kernel Regression
Message-ID: <20071214084949.26873.qmail@f5mail11.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071214/c093ce4f/attachment.pl 

From jeff.a.ryan at gmail.com  Fri Dec 14 16:09:18 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 14 Dec 2007 09:09:18 -0600
Subject: [R-SIG-Finance] Kernel Regression
In-Reply-To: <20071214084949.26873.qmail@f5mail11.rediffmail.com>
References: <20071214084949.26873.qmail@f5mail11.rediffmail.com>
Message-ID: <e8e755250712140709y5030f1a4t6f13808fc941067c@mail.gmail.com>

No personal experience, but you could try:

Package np - http://cran.r-project.org/src/contrib/Descriptions/np.html

?npreg

Jeff

On 14 Dec 2007 08:49:49 -0000, Sudhakar Achath <s_achath at rediffmail.com> wrote:
> Dear all:
>
> I am not able to find a function/package that can do
> kernel regression (gaussian) with p explanatory
> variables, using financial time series data.
> Can anyone help me on this, would much appreciate
> your response.
>
> Cheers!
>
> sud achath
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From swtzang at gmail.com  Mon Dec 17 14:34:14 2007
From: swtzang at gmail.com (ShyhWeir Tzang)
Date: Mon, 17 Dec 2007 21:34:14 +0800
Subject: [R-SIG-Finance] garch with additional regressors
Message-ID: <c17037a10712170534v53a9664bvbc074079fb5a3f04@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071217/09eb6624/attachment.pl 

From spencer.graves at pdf.com  Wed Dec 19 04:52:54 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 18 Dec 2007 19:52:54 -0800
Subject: [R-SIG-Finance] Analysis of Financial Time Series
Message-ID: <47689596.9060503@pdf.com>

Hello, All: 

      I'm currently developing an R companion to Ruey Tsay (2005) 
Analysis of Financial Time Series, 2nd ed. (Wiley), and I'd be pleased 
to have help. 

      A preliminary version "FinTS 0.1-17" is now available on CRAN.  A 
slightly newer version can be installed from R-Forge via 
'install.packages("FinTS",repos="http://r-forge.r-project.org")'.  The 
source code is available via "svn checkout 
svn://svn.r-forge.r-project.org/svnroot/fints". 

      The current versions contain all the data sets used in the text as 
documented R objects plus script files to generate nearly all the 
analyses in chapters 1 and 2.  It is therefore a great help, I believe, 
for anyone reading this book. 

      I could use volunteers to help me complete the package.  So far, 
I've found R functions to reproduce nearly all the examples, figures and 
tables in the book.  However, I don't use them routinely, and it is 
taking me considerable time to find what is available, to decide which 
of the available functions seem most appropriate for each application, 
and figure out how to use it so I get results reasonably close to those 
in the book.  For example, chapter 3 discusses "Conditional 
Heteroscedastic Models", including ARCH, GARCH, EGARCH, CHARMA, random 
coefficient autoregressive models, and stochastic volatility models.  
Other chapters discuss nonlinear time series, high frequency and 
continuous time models, extreme values, multivariate time series, Kalman 
filtering, and MCMC.  I can follow the math, but I have not used many of 
these models myself. 

      If you are interested in helping with this project, please let me 
know. 

      Best Wishes,
      Spencer Graves


From brian at braverock.com  Wed Dec 19 12:39:36 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 19 Dec 2007 05:39:36 -0600
Subject: [R-SIG-Finance] Analysis of Financial Time Series
In-Reply-To: <47689596.9060503@pdf.com>
References: <47689596.9060503@pdf.com>
Message-ID: <476902F8.9060409@braverock.com>

Spencer Graves wrote:
>       I'm currently developing an R companion to Ruey Tsay (2005) 
> Analysis of Financial Time Series, 2nd ed. (Wiley), and I'd be pleased 
> to have help. 

Spencer, I am very pleased to hear that you are making progress in this, 
as it is important work that many students, professionals, and 
researchers will benefit from.

I'm wondering if you've made a table of the examples or chapters and the 
specific topics or techniques employed in each.  I think that there are 
many people on this list who have expertise in specific techniques, as 
you suggest below.  I think that categorizing the examples by goal or 
technique could help enlist aid from this group, as individuals could 
choose a small number of examples that use a technique that they are are 
either already expert in in R, or a technique which they wish to learn 
in greater detail.

I'm traveling right now, and my copy of Tsay is at home, but I would be 
happy to help construct such a cross-reference table if it doesn't 
already exist.

Regards,

    - Brian

>       A preliminary version "FinTS 0.1-17" is now available on CRAN.  A 
> slightly newer version can be installed from R-Forge via 
> 'install.packages("FinTS",repos="http://r-forge.r-project.org")'.  The 
> source code is available via "svn checkout 
> svn://svn.r-forge.r-project.org/svnroot/fints". 
> 
>       The current versions contain all the data sets used in the text as 
> documented R objects plus script files to generate nearly all the 
> analyses in chapters 1 and 2.  It is therefore a great help, I believe, 
> for anyone reading this book. 
> 
>       I could use volunteers to help me complete the package.  So far, 
> I've found R functions to reproduce nearly all the examples, figures and 
> tables in the book.  However, I don't use them routinely, and it is 
> taking me considerable time to find what is available, to decide which 
> of the available functions seem most appropriate for each application, 
> and figure out how to use it so I get results reasonably close to those 
> in the book.  For example, chapter 3 discusses "Conditional 
> Heteroscedastic Models", including ARCH, GARCH, EGARCH, CHARMA, random 
> coefficient autoregressive models, and stochastic volatility models.  
> Other chapters discuss nonlinear time series, high frequency and 
> continuous time models, extreme values, multivariate time series, Kalman 
> filtering, and MCMC.  I can follow the math, but I have not used many of 
> these models myself. 
> 
>       If you are interested in helping with this project, please let me 
> know.


From spencer.graves at pdf.com  Fri Dec 21 16:48:50 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 21 Dec 2007 07:48:50 -0800
Subject: [R-SIG-Finance] Analysis of Financial Time Series
In-Reply-To: <476902F8.9060409@braverock.com>
References: <47689596.9060503@pdf.com> <476902F8.9060409@braverock.com>
Message-ID: <476BE062.8040703@pdf.com>

Dear Brian: 

      What do you think of "http://fints.r-forge.r-project.org" and the 
attached "FinTS Project Work Plan.xls"? 

      I think I should next do the following: 

      1.  Fix the obvious display problems with this web site. 

      2.  What do you think about posting questions to R Wiki, then 
asking package maintainers (and later R-SIG Finance) to help resolve 
questions?  That should stimulate a discussion whose resolution should 
be reasonably well documented and accessible to others.  I plan to try 
that with an ARCH question from Tsay's chapter 3. 

      3.  Upload a new version of FinTS to CRAN. 

      4.  When that's available, issue a progress report to 
R-SIG-Finance, inviting responses via R Wiki. 

      Best Wishes,
      Spencer Graves     

Brian G. Peterson wrote:
> Spencer Graves wrote:
>>       I'm currently developing an R companion to Ruey Tsay (2005) 
>> Analysis of Financial Time Series, 2nd ed. (Wiley), and I'd be 
>> pleased to have help. 
>
> Spencer, I am very pleased to hear that you are making progress in 
> this, as it is important work that many students, professionals, and 
> researchers will benefit from.
>
> I'm wondering if you've made a table of the examples or chapters and 
> the specific topics or techniques employed in each.  I think that 
> there are many people on this list who have expertise in specific 
> techniques, as you suggest below.  I think that categorizing the 
> examples by goal or technique could help enlist aid from this group, 
> as individuals could choose a small number of examples that use a 
> technique that they are are either already expert in in R, or a 
> technique which they wish to learn in greater detail.
>
> I'm traveling right now, and my copy of Tsay is at home, but I would 
> be happy to help construct such a cross-reference table if it doesn't 
> already exist.
>
> Regards,
>
>    - Brian
>
>>       A preliminary version "FinTS 0.1-17" is now available on CRAN.  
>> A slightly newer version can be installed from R-Forge via 
>> 'install.packages("FinTS",repos="http://r-forge.r-project.org")'.  
>> The source code is available via "svn checkout 
>> svn://svn.r-forge.r-project.org/svnroot/fints".
>>       The current versions contain all the data sets used in the text 
>> as documented R objects plus script files to generate nearly all the 
>> analyses in chapters 1 and 2.  It is therefore a great help, I 
>> believe, for anyone reading this book.
>>       I could use volunteers to help me complete the package.  So 
>> far, I've found R functions to reproduce nearly all the examples, 
>> figures and tables in the book.  However, I don't use them routinely, 
>> and it is taking me considerable time to find what is available, to 
>> decide which of the available functions seem most appropriate for 
>> each application, and figure out how to use it so I get results 
>> reasonably close to those in the book.  For example, chapter 3 
>> discusses "Conditional Heteroscedastic Models", including ARCH, 
>> GARCH, EGARCH, CHARMA, random coefficient autoregressive models, and 
>> stochastic volatility models.  Other chapters discuss nonlinear time 
>> series, high frequency and continuous time models, extreme values, 
>> multivariate time series, Kalman filtering, and MCMC.  I can follow 
>> the math, but I have not used many of these models myself.
>>       If you are interested in helping with this project, please let 
>> me know. 
>
>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: FinTS Project Work Plan .xls
Type: application/vnd.ms-excel
Size: 13824 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071221/6bcc2702/attachment.xls 

From finbref.2006 at gmail.com  Sat Dec 22 17:42:43 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Sat, 22 Dec 2007 17:42:43 +0100
Subject: [R-SIG-Finance] RBloomberg error
Message-ID: <d0f55a670712220842t17af669ar34eb271c95486f3e@mail.gmail.com>

I installed RBloomberg
(http://cran.r-project.org/src/contrib/Descriptions/RBloomberg.html)
sucessfully but when I want to run it, I get the follwong error:

>  blpConnect()
Fehler in getClass(class(object)) : "BlpCOMConnect" is not a defined class

I use it at home with my bloomberg anywhere account installed on my
machine, Windows Vista and R 2.5.1.
Do I have to install anything else then Bloomberg Professional? Of
course I am logged in while I try to access BBG via the R package.

Thanks for any help/hints
Thomas


From jamaj69 at gmail.com  Mon Dec 24 01:10:24 2007
From: jamaj69 at gmail.com (=?ISO-8859-1?Q?Jos=E9_Augusto_M._de_Andrade_Junior?=)
Date: Sun, 23 Dec 2007 21:10:24 -0300
Subject: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations
Message-ID: <f7ecaf330712231610x46dbe4earc2019553065d7bed@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071223/e639f4fe/attachment.pl 

From patrick at burns-stat.com  Mon Dec 24 11:43:55 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon, 24 Dec 2007 10:43:55 +0000
Subject: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations
In-Reply-To: <f7ecaf330712231610x46dbe4earc2019553065d7bed@mail.gmail.com>
References: <f7ecaf330712231610x46dbe4earc2019553065d7bed@mail.gmail.com>
Message-ID: <476F8D6B.3080903@burns-stat.com>

Given the model parameters and the starting volatility state,
the procedure (which you can use a 'for' loop to do) is:

* select the next random innovation.

* multiply by the volatility at that time point to get the simulated
return for that period.

* use the return to get the next period's variance using the garch
equation.

So there are two series that are being produced: the return
series and the variance series.


I'm not exactly objecting, but I hope you realize that garch models
variances while stable distributions (except the Gaussian) have infinite
variance.  Hence a garch model with a stable distribution is at least
a bit nonsensical.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Jos? Augusto M. de Andrade Junior wrote:

>Hi,
>
>Could someone give an example on how to simulate paths (forecast) of a Garch
>process with Levy stable innovations (by using rstable random deviates, for
>example)?
>
>Thanks in advance.
>
>Jos? Augusto M de Andrade Jr
>
>	[[alternative HTML version deleted]]
>
>  
>
>------------------------------------------------------------------------
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>


From jamaj69 at gmail.com  Mon Dec 24 20:26:43 2007
From: jamaj69 at gmail.com (=?ISO-8859-1?Q?Jos=E9_Augusto_M._de_Andrade_Junior?=)
Date: Mon, 24 Dec 2007 16:26:43 -0300
Subject: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations
In-Reply-To: <476F8D6B.3080903@burns-stat.com>
References: <f7ecaf330712231610x46dbe4earc2019553065d7bed@mail.gmail.com>
	<476F8D6B.3080903@burns-stat.com>
Message-ID: <f7ecaf330712241126g36bed7f0g41d5db1fa190352b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071224/4773863c/attachment.pl 

From ezivot at u.washington.edu  Mon Dec 24 20:48:23 2007
From: ezivot at u.washington.edu (Eric Zivot)
Date: Mon, 24 Dec 2007 11:48:23 -0800
Subject: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations
In-Reply-To: <f7ecaf330712241126g36bed7f0g41d5db1fa190352b@mail.gmail.com>
References: <f7ecaf330712231610x46dbe4earc2019553065d7bed@mail.gmail.com><476F8D6B.3080903@burns-stat.com>
	<f7ecaf330712241126g36bed7f0g41d5db1fa190352b@mail.gmail.com>
Message-ID: <002a01c84665$f2f1c030$02000002@zivotd800>

You are wrong
For example, a stable Gaussian GARCH(1,1) model is strictly stationary and
admits an unconditional variance that is constant (e.g. the 2nd moment of
the unconditional distribution exists and is finite). However, the
conditional variance (var(r(t)|I(t-1)) is time varying. 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Jos? Augusto
M. de Andrade Junior
Sent: Monday, December 24, 2007 11:27 AM
To: Patrick Burns
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations

Hi Patrick,

Thanks for the explanation.

I want to discuss the infinite variance of stable distributions (except
normal). I understand that infinite variance means only that this
distributions does not have a constant variance, that the integral does not
converge to a finite constant value.

When someone uses GARCH to model the variance he is indeed recogning the
same fact: the varince is not constant and should not converge, as with
stable distributions also occur.

Am i wrong?

2007/12/24, Patrick Burns <patrick at burns-stat.com>:
>
> Given the model parameters and the starting volatility state,
> the procedure (which you can use a 'for' loop to do) is:
>
> * select the next random innovation.
>
> * multiply by the volatility at that time point to get the simulated
> return for that period.
>
> * use the return to get the next period's variance using the garch
> equation.
>
> So there are two series that are being produced: the return
> series and the variance series.
>
>
> I'm not exactly objecting, but I hope you realize that garch models
> variances while stable distributions (except the Gaussian) have infinite
> variance.  Hence a garch model with a stable distribution is at least
> a bit nonsensical.
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Josi Augusto M. de Andrade Junior wrote:
>
> >Hi,
> >
> >Could someone give an example on how to simulate paths (forecast) of a
> Garch
> >process with Levy stable innovations (by using rstable random deviates,
> for
> >example)?
> >
> >Thanks in advance.
> >
> >Josi Augusto M de Andrade Jr
> >
> >       [[alternative HTML version deleted]]
> >
> >
> >
> >------------------------------------------------------------------------
> >
> >_______________________________________________
> >R-SIG-Finance at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >-- Subscriber-posting only.
> >-- If you want to post, subscribe first.
> >
>

	[[alternative HTML version deleted]]


From patrick at burns-stat.com  Mon Dec 24 20:49:18 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon, 24 Dec 2007 19:49:18 +0000
Subject: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations
In-Reply-To: <f7ecaf330712241126g36bed7f0g41d5db1fa190352b@mail.gmail.com>
References: <f7ecaf330712231610x46dbe4earc2019553065d7bed@mail.gmail.com>	
	<476F8D6B.3080903@burns-stat.com>
	<f7ecaf330712241126g36bed7f0g41d5db1fa190352b@mail.gmail.com>
Message-ID: <47700D3E.604@burns-stat.com>

Yes, you are wrong.  Stable distributions DO have
a constant variance: infinity.

Pat

Jos? Augusto M. de Andrade Junior wrote:

> Hi Patrick,
>  
> Thanks for the explanation.
>
> I want to discuss the infinite variance of stable distributions 
> (except normal). I understand that infinite variance means only that 
> this distributions does not have a constant variance, that the 
> integral does not converge to a finite constant value. 
>  
> When someone uses GARCH to model the variance he is indeed recogning 
> the same fact: the varince is not constant and should not converge, as 
> with stable distributions also occur.
>  
> Am i wrong?
>  
> 2007/12/24, Patrick Burns <patrick at burns-stat.com 
> <mailto:patrick at burns-stat.com>>:
>
>     Given the model parameters and the starting volatility state,
>     the procedure (which you can use a 'for' loop to do) is:
>
>     * select the next random innovation.
>
>     * multiply by the volatility at that time point to get the simulated
>     return for that period.
>
>     * use the return to get the next period's variance using the garch
>     equation.
>
>     So there are two series that are being produced: the return
>     series and the variance series.
>
>
>     I'm not exactly objecting, but I hope you realize that garch models
>     variances while stable distributions (except the Gaussian) have
>     infinite
>     variance.  Hence a garch model with a stable distribution is at least
>     a bit nonsensical.
>
>     Patrick Burns
>     patrick at burns-stat.com <mailto:patrick at burns-stat.com>
>     +44 (0)20 8525 0696
>     http://www.burns-stat.com
>     (home of S Poetry and "A Guide for the Unwilling S User")
>
>     Jos? Augusto M. de Andrade Junior wrote:
>
>     >Hi,
>     >
>     >Could someone give an example on how to simulate paths (forecast)
>     of a Garch
>     >process with Levy stable innovations (by using rstable random
>     deviates, for
>     >example)?
>     >
>     >Thanks in advance.
>     >
>     >Jos? Augusto M de Andrade Jr
>     >
>     >       [[alternative HTML version deleted]]
>     >
>     >
>     >
>     >------------------------------------------------------------------------
>     >
>     >_______________________________________________
>     > R-SIG-Finance at stat.math.ethz.ch
>     <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
>     >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>     >-- Subscriber-posting only.
>     >-- If you want to post, subscribe first.
>     >
>
>


From finbref.2006 at gmail.com  Mon Dec 24 22:02:24 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Mon, 24 Dec 2007 22:02:24 +0100
Subject: [R-SIG-Finance] RBloomberg error
In-Reply-To: <f2e3401f0712240913yec8f4d8o764bb1d72d6340a4@mail.gmail.com>
References: <d0f55a670712220842t17af669ar34eb271c95486f3e@mail.gmail.com>
	<f2e3401f0712240913yec8f4d8o764bb1d72d6340a4@mail.gmail.com>
Message-ID: <d0f55a670712241302s277ba935sf3f9cee14abae68f@mail.gmail.com>

Dear Paul,

thanks for your answer.

> Can you download Bloomberg data into Excel using the Bloomberg Add-in?

No, because I do not have MS Office, but just OpenOffice.org (for
which no BBG plugin exists up to my knowledge).

> If not, I'd guess you have some more bloomberg software to install,
> but I don't know what and I don't know how.  I'd start by asking for
> help to get the Excel Add-In working using the chat feature with
> Bloomberg customer service.

I keep asking HelpHelp for advice, but no success yet :(

> If you can download Bloomberg data into Excel, shutdown R, start the
> DDE Server by clicking on the lightening bolt icon in the Bloomberg
> section of your start menu, then restart R.  I've never used Vista, so
> can't offer any other specific suggestions.  Also, you might try
> upgrading R to 2.6.1.
>
> Good luck

Thank you,
Thomas


From spencer.graves at pdf.com  Mon Dec 24 22:05:32 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 24 Dec 2007 13:05:32 -0800
Subject: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations
In-Reply-To: <47700D3E.604@burns-stat.com>
References: <f7ecaf330712231610x46dbe4earc2019553065d7bed@mail.gmail.com>		<476F8D6B.3080903@burns-stat.com>	<f7ecaf330712241126g36bed7f0g41d5db1fa190352b@mail.gmail.com>
	<47700D3E.604@burns-stat.com>
Message-ID: <47701F1C.3040309@pdf.com>

Hi, Patrick, et al.: 


IS NORMAL STABLE? 

      I'm confused:  According to Wikipedia, a normal distribution is a 
stable distribution with parameters alpha = 2 and beta = 0 
(http://en.wikipedia.org/wiki/L%C3%A9vy_skew_alpha-stable_distribution).  
However, I get large discrepancies between 'pstable{fBasics}' and pnorm: 

 > library(fBasics)
 > x <- seq(-2, 2)
 > pstable(x, 2, 0)-pnorm(x
+ )
[1]  0.05589947  0.08109481  0.00000000 -0.08109481 -0.05589947
attr(,"control")
   dist alpha beta gamma delta pm
 stable     2    0     1     0  0

      What am I doing wrong? 


ASYMPTOTICS 

      What about the maximum likelihood estimates of garch parameters?  
Don't they follow the standard asymptotic normal distribution with mean 
and variance of the approximating normal distribution = the true but 
unknown parameters and the inverse of the information  matrix (Fisher or 
observed, take your pick)? 

      My favorite example for this is logistic regression, where no 
moments exist for the MLEs, because the MLEs are Infinite for some 
possible outcomes.  However, the standard normal approximation still 
works great.  Moreover, the probability of observing Infinite MLEs at a 
rate proportional to 2^(-N), if my memory is correct. 


DISTRIBUTION OF RESIDUALS

      What can be said about the distribution of the whitened 
residuals?  If N gets large faster than the number of parameters 
estimated, won't the distribution of the whitened residuals converge to 
the actual parent distribution, more or less whatever it is? 

      Best Wishes,
      Spencer

Patrick Burns wrote:
> Yes, you are wrong.  Stable distributions DO have
> a constant variance: infinity.
>
> Pat
>
> Jos? Augusto M. de Andrade Junior wrote:
>
>   
>> Hi Patrick,
>>  
>> Thanks for the explanation.
>>
>> I want to discuss the infinite variance of stable distributions 
>> (except normal). I understand that infinite variance means only that 
>> this distributions does not have a constant variance, that the 
>> integral does not converge to a finite constant value. 
>>  
>> When someone uses GARCH to model the variance he is indeed recogning 
>> the same fact: the varince is not constant and should not converge, as 
>> with stable distributions also occur.
>>  
>> Am i wrong?
>>  
>> 2007/12/24, Patrick Burns <patrick at burns-stat.com 
>> <mailto:patrick at burns-stat.com>>:
>>
>>     Given the model parameters and the starting volatility state,
>>     the procedure (which you can use a 'for' loop to do) is:
>>
>>     * select the next random innovation.
>>
>>     * multiply by the volatility at that time point to get the simulated
>>     return for that period.
>>
>>     * use the return to get the next period's variance using the garch
>>     equation.
>>
>>     So there are two series that are being produced: the return
>>     series and the variance series.
>>
>>
>>     I'm not exactly objecting, but I hope you realize that garch models
>>     variances while stable distributions (except the Gaussian) have
>>     infinite
>>     variance.  Hence a garch model with a stable distribution is at least
>>     a bit nonsensical.
>>
>>     Patrick Burns
>>     patrick at burns-stat.com <mailto:patrick at burns-stat.com>
>>     +44 (0)20 8525 0696
>>     http://www.burns-stat.com
>>     (home of S Poetry and "A Guide for the Unwilling S User")
>>
>>     Jos? Augusto M. de Andrade Junior wrote:
>>
>>     >Hi,
>>     >
>>     >Could someone give an example on how to simulate paths (forecast)
>>     of a Garch
>>     >process with Levy stable innovations (by using rstable random
>>     deviates, for
>>     >example)?
>>     >
>>     >Thanks in advance.
>>     >
>>     >Jos? Augusto M de Andrade Jr
>>     >
>>     >       [[alternative HTML version deleted]]
>>     >
>>     >
>>     >
>>     >------------------------------------------------------------------------
>>     >
>>     >_______________________________________________
>>     > R-SIG-Finance at stat.math.ethz.ch
>>     <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
>>     >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>     >-- Subscriber-posting only.
>>     >-- If you want to post, subscribe first.
>>     >
>>
>>
>>     
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From young.stat at gmail.com  Mon Dec 24 22:30:30 2007
From: young.stat at gmail.com (YOUNG CHO)
Date: Mon, 24 Dec 2007 13:30:30 -0800
Subject: [R-SIG-Finance] multivariate garch
Message-ID: <811D829B-4711-4A2E-B64A-A4F5180925C8@gmail.com>

Does R have a package for multivariate garch models, BEKK or VECH ?  
Some functions like mgarch in splus, is what I am searching for.

Thanks so much for help in advance.

-Young


From patrick at burns-stat.com  Tue Dec 25 11:13:13 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 25 Dec 2007 10:13:13 +0000
Subject: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations
In-Reply-To: <47701F1C.3040309@pdf.com>
References: <f7ecaf330712231610x46dbe4earc2019553065d7bed@mail.gmail.com>		<476F8D6B.3080903@burns-stat.com>	<f7ecaf330712241126g36bed7f0g41d5db1fa190352b@mail.gmail.com>
	<47700D3E.604@burns-stat.com> <47701F1C.3040309@pdf.com>
Message-ID: <4770D7B9.1050509@burns-stat.com>

Spencer,

Perhaps the scaling of the stable is different than for the
normal.  You should be able to figure out the standard
deviation for the normal that matches one point, and then
see if other points match.

As for asymptotics of residuals and so on, I doubt you
will get to the right answer if the data were generated by a
process (stable innovations) that do not conform to finite
likelihoods.  But this is probably better answered by someone
who cares about asymptotics.

Pat


Spencer Graves wrote:

> Hi, Patrick, et al.:
>
> IS NORMAL STABLE?
>      I'm confused:  According to Wikipedia, a normal distribution is a 
> stable distribution with parameters alpha = 2 and beta = 0 
> (http://en.wikipedia.org/wiki/L%C3%A9vy_skew_alpha-stable_distribution).  
> However, I get large discrepancies between 'pstable{fBasics}' and pnorm:
> > library(fBasics)
> > x <- seq(-2, 2)
> > pstable(x, 2, 0)-pnorm(x
> + )
> [1]  0.05589947  0.08109481  0.00000000 -0.08109481 -0.05589947
> attr(,"control")
>   dist alpha beta gamma delta pm
> stable     2    0     1     0  0
>
>      What am I doing wrong?
>
> ASYMPTOTICS
>      What about the maximum likelihood estimates of garch parameters?  
> Don't they follow the standard asymptotic normal distribution with 
> mean and variance of the approximating normal distribution = the true 
> but unknown parameters and the inverse of the information  matrix 
> (Fisher or observed, take your pick)?
>      My favorite example for this is logistic regression, where no 
> moments exist for the MLEs, because the MLEs are Infinite for some 
> possible outcomes.  However, the standard normal approximation still 
> works great.  Moreover, the probability of observing Infinite MLEs at 
> a rate proportional to 2^(-N), if my memory is correct.
>
> DISTRIBUTION OF RESIDUALS
>
>      What can be said about the distribution of the whitened 
> residuals?  If N gets large faster than the number of parameters 
> estimated, won't the distribution of the whitened residuals converge 
> to the actual parent distribution, more or less whatever it is?
>      Best Wishes,
>      Spencer
>
> Patrick Burns wrote:
>
>> Yes, you are wrong.  Stable distributions DO have
>> a constant variance: infinity.
>>
>> Pat
>>
>> Jos? Augusto M. de Andrade Junior wrote:
>>
>>  
>>
>>> Hi Patrick,
>>>  
>>> Thanks for the explanation.
>>>
>>> I want to discuss the infinite variance of stable distributions 
>>> (except normal). I understand that infinite variance means only that 
>>> this distributions does not have a constant variance, that the 
>>> integral does not converge to a finite constant value.  
>>> When someone uses GARCH to model the variance he is indeed recogning 
>>> the same fact: the varince is not constant and should not converge, 
>>> as with stable distributions also occur.
>>>  
>>> Am i wrong?
>>>  
>>> 2007/12/24, Patrick Burns <patrick at burns-stat.com 
>>> <mailto:patrick at burns-stat.com>>:
>>>
>>>     Given the model parameters and the starting volatility state,
>>>     the procedure (which you can use a 'for' loop to do) is:
>>>
>>>     * select the next random innovation.
>>>
>>>     * multiply by the volatility at that time point to get the 
>>> simulated
>>>     return for that period.
>>>
>>>     * use the return to get the next period's variance using the garch
>>>     equation.
>>>
>>>     So there are two series that are being produced: the return
>>>     series and the variance series.
>>>
>>>
>>>     I'm not exactly objecting, but I hope you realize that garch models
>>>     variances while stable distributions (except the Gaussian) have
>>>     infinite
>>>     variance.  Hence a garch model with a stable distribution is at 
>>> least
>>>     a bit nonsensical.
>>>
>>>     Patrick Burns
>>>     patrick at burns-stat.com <mailto:patrick at burns-stat.com>
>>>     +44 (0)20 8525 0696
>>>     http://www.burns-stat.com
>>>     (home of S Poetry and "A Guide for the Unwilling S User")
>>>
>>>     Jos? Augusto M. de Andrade Junior wrote:
>>>
>>>     >Hi,
>>>     >
>>>     >Could someone give an example on how to simulate paths (forecast)
>>>     of a Garch
>>>     >process with Levy stable innovations (by using rstable random
>>>     deviates, for
>>>     >example)?
>>>     >
>>>     >Thanks in advance.
>>>     >
>>>     >Jos? Augusto M de Andrade Jr
>>>     >
>>>     >       [[alternative HTML version deleted]]
>>>     >
>>>     >
>>>     >
>>>     
>>> >------------------------------------------------------------------------ 
>>>
>>>     >
>>>     >_______________________________________________
>>>     > R-SIG-Finance at stat.math.ethz.ch
>>>     <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
>>>     >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>     >-- Subscriber-posting only.
>>>     >-- If you want to post, subscribe first.
>>>     >
>>>
>>>
>>>     
>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only. -- If you want to post, subscribe first.
>>   
>
>
>


From brian at braverock.com  Tue Dec 25 13:59:51 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 25 Dec 2007 06:59:51 -0600
Subject: [R-SIG-Finance] multivariate garch
In-Reply-To: <811D829B-4711-4A2E-B64A-A4F5180925C8@gmail.com>
References: <811D829B-4711-4A2E-B64A-A4F5180925C8@gmail.com>
Message-ID: <4770FEC7.2010609@braverock.com>

YOUNG CHO wrote:
> Does R have a package for multivariate garch models, BEKK or VECH ?  
> Some functions like mgarch in splus, is what I am searching for.
> 
> Thanks so much for help in advance.

Have you tried the 'mgarch' or 'mgarchBEKK' packages?

Regards,

    - Brian


From brian at braverock.com  Tue Dec 25 14:43:16 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 25 Dec 2007 07:43:16 -0600
Subject: [R-SIG-Finance] multivariate garch
In-Reply-To: <4770FEC7.2010609@braverock.com>
References: <811D829B-4711-4A2E-B64A-A4F5180925C8@gmail.com>
	<4770FEC7.2010609@braverock.com>
Message-ID: <477108F4.8090504@braverock.com>

Brian G. Peterson wrote:
> YOUNG CHO wrote:
>> Does R have a package for multivariate garch models, BEKK or VECH ?  
>> Some functions like mgarch in splus, is what I am searching for.
>>
>> Thanks so much for help in advance.
> 
> Have you tried the 'mgarch' or 'mgarchBEKK' packages?

Sorry, not on CRAN, so I suppose a link would help:

http://www.vsthost.com/pages/RStuff.php

Regards,

    - Brian


From jeff.a.ryan at gmail.com  Tue Dec 25 17:10:39 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 25 Dec 2007 10:10:39 -0600
Subject: [R-SIG-Finance] multivariate garch
In-Reply-To: <811D829B-4711-4A2E-B64A-A4F5180925C8@gmail.com>
References: <811D829B-4711-4A2E-B64A-A4F5180925C8@gmail.com>
Message-ID: <e8e755250712250810k47233716y62d787d8c18092ab@mail.gmail.com>

Topic was visited during October (and before...)

The current version (if 2006 is current) of the software in the link
that Brian provided is broken.  To get it working required a little
bit of source editing.  Conveniently I posted the fix back then as
well.  I have the edited source which I can also post on quantmod.com
or email for those interested (assuming that is within the license -
of which I have not checked).

Either way, the link provides the fix.

https://stat.ethz.ch/pipermail/r-sig-finance/2007q4/001833.html

Enjoy.

Jeff

On Dec 24, 2007 3:30 PM, YOUNG CHO <young.stat at gmail.com> wrote:
> Does R have a package for multivariate garch models, BEKK or VECH ?
> Some functions like mgarch in splus, is what I am searching for.
>
> Thanks so much for help in advance.
>
> -Young
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From brian at braverock.com  Tue Dec 25 20:56:31 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 25 Dec 2007 13:56:31 -0600
Subject: [R-SIG-Finance] multivariate garch
In-Reply-To: <e8e755250712250810k47233716y62d787d8c18092ab@mail.gmail.com>
References: <811D829B-4711-4A2E-B64A-A4F5180925C8@gmail.com>
	<e8e755250712250810k47233716y62d787d8c18092ab@mail.gmail.com>
Message-ID: <4771606F.4070006@braverock.com>

Jeff Ryan wrote:
> Topic was visited during October (and before...)
> 
> The current version (if 2006 is current) of the software in the link
> that Brian provided is broken.  To get it working required a little
> bit of source editing.  Conveniently I posted the fix back then as
> well.  I have the edited source which I can also post on quantmod.com
> or email for those interested (assuming that is within the license -
> of which I have not checked).
> 
> Either way, the link provides the fix.
> 
> https://stat.ethz.ch/pipermail/r-sig-finance/2007q4/001833.html

License: GPL

so you may distribute your changes freely, including here.

Regards,

   - Brian


From cggreen at u.washington.edu  Tue Dec 25 21:16:55 2007
From: cggreen at u.washington.edu (Christopher G. Green (L))
Date: Tue, 25 Dec 2007 12:16:55 -0800
Subject: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations
In-Reply-To: <47701F1C.3040309@pdf.com>
References: <f7ecaf330712231610x46dbe4earc2019553065d7bed@mail.gmail.com>		<476F8D6B.3080903@burns-stat.com>	<f7ecaf330712241126g36bed7f0g41d5db1fa190352b@mail.gmail.com><47700D3E.604@burns-stat.com>
	<47701F1C.3040309@pdf.com>
Message-ID: <000301c84733$196988b0$9602a8c0@cgglappie>

The alpha-stable distribution for \alpha = 2, "scale" parameter \gamma = 1
and location parameter \delta = 0 is a normal distribution with mean 0 and
variance 2:

> x <- seq(-2, 2)
> pstable(x, 2, 0)-pnorm(x, 0, sqrt(2))
[1] 0 0 0 0 0
attr(,"control")
   dist alpha beta gamma delta pm
 stable     2    0     1     0  0



cg
________________________________

Christopher G. Green (cggreen AT stat.washington.edu) 
Doctoral Candidate
Department of Statistics, Box 354322, Seattle, WA, 98195-4322, U.S.A.
http://www.stat.washington.edu/cggreen/




> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch 
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of 
> Spencer Graves
> Sent: Monday, December 24, 2007 1:06 PM
> To: Patrick Burns
> Cc: r-sig-finance at stat.math.ethz.ch; "Jos? Augusto M. de 
> Andrade Junior"
> Subject: Re: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations
> 
> Hi, Patrick, et al.: 
> 
> 
> IS NORMAL STABLE? 
> 
>       I'm confused:  According to Wikipedia, a normal 
> distribution is a stable distribution with parameters alpha = 
> 2 and beta = 0 
> (http://en.wikipedia.org/wiki/L%C3%A9vy_skew_alpha-stable_dist
> ribution).  
> However, I get large discrepancies between 'pstable{fBasics}' 
> and pnorm: 
> 
>  > library(fBasics)
>  > x <- seq(-2, 2)
>  > pstable(x, 2, 0)-pnorm(x
> + )
> [1]  0.05589947  0.08109481  0.00000000 -0.08109481 -0.05589947
> attr(,"control")
>    dist alpha beta gamma delta pm
>  stable     2    0     1     0  0
> 
>       What am I doing wrong? 
> 
> 
> ASYMPTOTICS 
> 
>       What about the maximum likelihood estimates of garch 
> parameters?  
> Don't they follow the standard asymptotic normal distribution 
> with mean and variance of the approximating normal 
> distribution = the true but unknown parameters and the 
> inverse of the information  matrix (Fisher or observed, take 
> your pick)? 
> 
>       My favorite example for this is logistic regression, 
> where no moments exist for the MLEs, because the MLEs are 
> Infinite for some possible outcomes.  However, the standard 
> normal approximation still works great.  Moreover, the 
> probability of observing Infinite MLEs at a rate proportional 
> to 2^(-N), if my memory is correct. 
> 
> 
> DISTRIBUTION OF RESIDUALS
> 
>       What can be said about the distribution of the whitened 
> residuals?  If N gets large faster than the number of 
> parameters estimated, won't the distribution of the whitened 
> residuals converge to the actual parent distribution, more or 
> less whatever it is? 
> 
>       Best Wishes,
>       Spencer
> 
> Patrick Burns wrote:
> > Yes, you are wrong.  Stable distributions DO have a 
> constant variance: 
> > infinity.
> >
> > Pat
> >
> > Jos? Augusto M. de Andrade Junior wrote:
> >
> >   
> >> Hi Patrick,
> >>  
> >> Thanks for the explanation.
> >>
> >> I want to discuss the infinite variance of stable distributions 
> >> (except normal). I understand that infinite variance means 
> only that 
> >> this distributions does not have a constant variance, that the 
> >> integral does not converge to a finite constant value.
> >>  
> >> When someone uses GARCH to model the variance he is indeed 
> recogning 
> >> the same fact: the varince is not constant and should not 
> converge, 
> >> as with stable distributions also occur.
> >>  
> >> Am i wrong?
> >>  
> >> 2007/12/24, Patrick Burns <patrick at burns-stat.com
> >> <mailto:patrick at burns-stat.com>>:
> >>
> >>     Given the model parameters and the starting volatility state,
> >>     the procedure (which you can use a 'for' loop to do) is:
> >>
> >>     * select the next random innovation.
> >>
> >>     * multiply by the volatility at that time point to get 
> the simulated
> >>     return for that period.
> >>
> >>     * use the return to get the next period's variance 
> using the garch
> >>     equation.
> >>
> >>     So there are two series that are being produced: the return
> >>     series and the variance series.
> >>
> >>
> >>     I'm not exactly objecting, but I hope you realize that 
> garch models
> >>     variances while stable distributions (except the Gaussian) have
> >>     infinite
> >>     variance.  Hence a garch model with a stable 
> distribution is at least
> >>     a bit nonsensical.
> >>
> >>     Patrick Burns
> >>     patrick at burns-stat.com <mailto:patrick at burns-stat.com>
> >>     +44 (0)20 8525 0696
> >>     http://www.burns-stat.com
> >>     (home of S Poetry and "A Guide for the Unwilling S User")
> >>
> >>     Jos? Augusto M. de Andrade Junior wrote:
> >>
> >>     >Hi,
> >>     >
> >>     >Could someone give an example on how to simulate 
> paths (forecast)
> >>     of a Garch
> >>     >process with Levy stable innovations (by using rstable random
> >>     deviates, for
> >>     >example)?
> >>     >
> >>     >Thanks in advance.
> >>     >
> >>     >Jos? Augusto M de Andrade Jr
> >>     >
> >>     >       [[alternative HTML version deleted]]
> >>     >
> >>     >
> >>     >
> >>     
> >-------------------------------------------------------------
> -----------
> >>     >
> >>     >_______________________________________________
> >>     > R-SIG-Finance at stat.math.ethz.ch
> >>     <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
> >>     >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >>     >-- Subscriber-posting only.
> >>     >-- If you want to post, subscribe first.
> >>     >
> >>
> >>
> >>     
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only. 
> > -- If you want to post, subscribe first.
> >
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 


From spencer.graves at pdf.com  Tue Dec 25 21:50:56 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 25 Dec 2007 12:50:56 -0800
Subject: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations
In-Reply-To: <000301c84733$196988b0$9602a8c0@cgglappie>
References: <f7ecaf330712231610x46dbe4earc2019553065d7bed@mail.gmail.com>		<476F8D6B.3080903@burns-stat.com>	<f7ecaf330712241126g36bed7f0g41d5db1fa190352b@mail.gmail.com><47700D3E.604@burns-stat.com>
	<47701F1C.3040309@pdf.com>
	<000301c84733$196988b0$9602a8c0@cgglappie>
Message-ID: <47716D30.4000407@pdf.com>

Hi, Christopher: 

      Thanks very much.  Now that I know the answer, I see it should 
have been obvious from the web site I referenced. 

      Thanks again. 

      Happy holidays to all -- and a very big "THANK YOU" to all who 
have contributed to my education and others through this listserve. 

      Best Wishes,
      Spencer Graves

Christopher G. Green (L) wrote:
> The alpha-stable distribution for \alpha = 2, "scale" parameter \gamma = 1
> and location parameter \delta = 0 is a normal distribution with mean 0 and
> variance 2:
>
>   
>> x <- seq(-2, 2)
>> pstable(x, 2, 0)-pnorm(x, 0, sqrt(2))
>>     
> [1] 0 0 0 0 0
> attr(,"control")
>    dist alpha beta gamma delta pm
>  stable     2    0     1     0  0
>
>
>
> cg
> ________________________________
>
> Christopher G. Green (cggreen AT stat.washington.edu) 
> Doctoral Candidate
> Department of Statistics, Box 354322, Seattle, WA, 98195-4322, U.S.A.
> http://www.stat.washington.edu/cggreen/
>
>
>  
>
>   
>> -----Original Message-----
>> From: r-sig-finance-bounces at stat.math.ethz.ch 
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of 
>> Spencer Graves
>> Sent: Monday, December 24, 2007 1:06 PM
>> To: Patrick Burns
>> Cc: r-sig-finance at stat.math.ethz.ch; "Jos? Augusto M. de 
>> Andrade Junior"
>> Subject: Re: [R-SIG-Finance] Non-gaussian (L-stable) Garch innovations
>>
>> Hi, Patrick, et al.: 
>>
>>
>> IS NORMAL STABLE? 
>>
>>       I'm confused:  According to Wikipedia, a normal 
>> distribution is a stable distribution with parameters alpha = 
>> 2 and beta = 0 
>> (http://en.wikipedia.org/wiki/L%C3%A9vy_skew_alpha-stable_dist
>> ribution).  
>> However, I get large discrepancies between 'pstable{fBasics}' 
>> and pnorm: 
>>
>>  > library(fBasics)
>>  > x <- seq(-2, 2)
>>  > pstable(x, 2, 0)-pnorm(x
>> + )
>> [1]  0.05589947  0.08109481  0.00000000 -0.08109481 -0.05589947
>> attr(,"control")
>>    dist alpha beta gamma delta pm
>>  stable     2    0     1     0  0
>>
>>       What am I doing wrong? 
>>
>>
>> ASYMPTOTICS 
>>
>>       What about the maximum likelihood estimates of garch 
>> parameters?  
>> Don't they follow the standard asymptotic normal distribution 
>> with mean and variance of the approximating normal 
>> distribution = the true but unknown parameters and the 
>> inverse of the information  matrix (Fisher or observed, take 
>> your pick)? 
>>
>>       My favorite example for this is logistic regression, 
>> where no moments exist for the MLEs, because the MLEs are 
>> Infinite for some possible outcomes.  However, the standard 
>> normal approximation still works great.  Moreover, the 
>> probability of observing Infinite MLEs at a rate proportional 
>> to 2^(-N), if my memory is correct. 
>>
>>
>> DISTRIBUTION OF RESIDUALS
>>
>>       What can be said about the distribution of the whitened 
>> residuals?  If N gets large faster than the number of 
>> parameters estimated, won't the distribution of the whitened 
>> residuals converge to the actual parent distribution, more or 
>> less whatever it is? 
>>
>>       Best Wishes,
>>       Spencer
>>
>> Patrick Burns wrote:
>>     
>>> Yes, you are wrong.  Stable distributions DO have a 
>>>       
>> constant variance: 
>>     
>>> infinity.
>>>
>>> Pat
>>>
>>> Jos? Augusto M. de Andrade Junior wrote:
>>>
>>>   
>>>       
>>>> Hi Patrick,
>>>>  
>>>> Thanks for the explanation.
>>>>
>>>> I want to discuss the infinite variance of stable distributions 
>>>> (except normal). I understand that infinite variance means 
>>>>         
>> only that 
>>     
>>>> this distributions does not have a constant variance, that the 
>>>> integral does not converge to a finite constant value.
>>>>  
>>>> When someone uses GARCH to model the variance he is indeed 
>>>>         
>> recogning 
>>     
>>>> the same fact: the varince is not constant and should not 
>>>>         
>> converge, 
>>     
>>>> as with stable distributions also occur.
>>>>  
>>>> Am i wrong?
>>>>  
>>>> 2007/12/24, Patrick Burns <patrick at burns-stat.com
>>>> <mailto:patrick at burns-stat.com>>:
>>>>
>>>>     Given the model parameters and the starting volatility state,
>>>>     the procedure (which you can use a 'for' loop to do) is:
>>>>
>>>>     * select the next random innovation.
>>>>
>>>>     * multiply by the volatility at that time point to get 
>>>>         
>> the simulated
>>     
>>>>     return for that period.
>>>>
>>>>     * use the return to get the next period's variance 
>>>>         
>> using the garch
>>     
>>>>     equation.
>>>>
>>>>     So there are two series that are being produced: the return
>>>>     series and the variance series.
>>>>
>>>>
>>>>     I'm not exactly objecting, but I hope you realize that 
>>>>         
>> garch models
>>     
>>>>     variances while stable distributions (except the Gaussian) have
>>>>     infinite
>>>>     variance.  Hence a garch model with a stable 
>>>>         
>> distribution is at least
>>     
>>>>     a bit nonsensical.
>>>>
>>>>     Patrick Burns
>>>>     patrick at burns-stat.com <mailto:patrick at burns-stat.com>
>>>>     +44 (0)20 8525 0696
>>>>     http://www.burns-stat.com
>>>>     (home of S Poetry and "A Guide for the Unwilling S User")
>>>>
>>>>     Jos? Augusto M. de Andrade Junior wrote:
>>>>
>>>>     >Hi,
>>>>     >
>>>>     >Could someone give an example on how to simulate 
>>>>         
>> paths (forecast)
>>     
>>>>     of a Garch
>>>>     >process with Levy stable innovations (by using rstable random
>>>>     deviates, for
>>>>     >example)?
>>>>     >
>>>>     >Thanks in advance.
>>>>     >
>>>>     >Jos? Augusto M de Andrade Jr
>>>>     >
>>>>     >       [[alternative HTML version deleted]]
>>>>     >
>>>>     >
>>>>     >
>>>>     
>>>>         
>>> -------------------------------------------------------------
>>>       
>> -----------
>>     
>>>>     >
>>>>     >_______________________________________________
>>>>     > R-SIG-Finance at stat.math.ethz.ch
>>>>     <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
>>>>     >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>     >-- Subscriber-posting only.
>>>>     >-- If you want to post, subscribe first.
>>>>     >
>>>>
>>>>
>>>>     
>>>>         
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only. 
>>> -- If you want to post, subscribe first.
>>>
>>>       
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only. 
>> -- If you want to post, subscribe first.
>>
>>     
>
>


From jeff.a.ryan at gmail.com  Wed Dec 26 06:15:45 2007
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 25 Dec 2007 23:15:45 -0600
Subject: [R-SIG-Finance] multivariate garch
In-Reply-To: <4771606F.4070006@braverock.com>
References: <811D829B-4711-4A2E-B64A-A4F5180925C8@gmail.com>
	<e8e755250712250810k47233716y62d787d8c18092ab@mail.gmail.com>
	<4771606F.4070006@braverock.com>
Message-ID: <e8e755250712252115y3fba688fiaf3166fc029d3641@mail.gmail.com>

Now playing at:

http://www.quantmod.com/download/mgarchBEKK/

Jeff

On Dec 25, 2007 1:56 PM, Brian G. Peterson <brian at braverock.com> wrote:
> Jeff Ryan wrote:
> > Topic was visited during October (and before...)
> >
> > The current version (if 2006 is current) of the software in the link
> > that Brian provided is broken.  To get it working required a little
> > bit of source editing.  Conveniently I posted the fix back then as
> > well.  I have the edited source which I can also post on quantmod.com
> > or email for those interested (assuming that is within the license -
> > of which I have not checked).
> >
> > Either way, the link provides the fix.
> >
> > https://stat.ethz.ch/pipermail/r-sig-finance/2007q4/001833.html
>
> License: GPL
>
> so you may distribute your changes freely, including here.
>
> Regards,
>
>    - Brian
>


From volchik2000 at list.ru  Wed Dec 26 12:01:47 2007
From: volchik2000 at list.ru (Yuri Volchik)
Date: Wed, 26 Dec 2007 11:01:47 -0000
Subject: [R-SIG-Finance] Adding milliseconds to zoo object
In-Reply-To: <mailman.1.1198580401.18896.r-sig-finance@stat.math.ethz.ch>
References: <mailman.1.1198580401.18896.r-sig-finance@stat.math.ethz.ch>
Message-ID: <000a01c847ae$b725d290$257177b0$@ru>

I've read already some discussions concerning milliseconds as an index in
zoo index and was wondering if there is a simple solution to differentiate
ticks which come with 1 second timestamp resolution by adding index for rows
with the same timestamp.
One of the solutions would be just to create an index with length of
nrows(x) and consider them as milli(micro)seconds, so my index will look
like:
2007-12-24 06:14:00.154  22.00  
2007-12-24 06:14:00.155  22.00  
2007-12-24 06:15:00.156  22.00  
2007-12-24 06:15:00.157  22.00  

So with solution order of the observations(ticks) is preserved, though the
solution is not very nice.
Is there a way to create index in the following way?
2007-12-24 06:14:00.154  22.00  
2007-12-24 06:14:00.155  22.00  
2007-12-24 06:15:00.001  22.00  
2007-12-24 06:15:00.002  22.00  

So this index will be in effect number(position) of the tick in the second.

Thanks


From brian at braverock.com  Wed Dec 26 15:59:52 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 26 Dec 2007 08:59:52 -0600
Subject: [R-SIG-Finance] Adding milliseconds to zoo object
In-Reply-To: <000a01c847ae$b725d290$257177b0$@ru>
References: <mailman.1.1198580401.18896.r-sig-finance@stat.math.ethz.ch>
	<000a01c847ae$b725d290$257177b0$@ru>
Message-ID: <47726C68.3000104@braverock.com>

Yuri Volchik wrote:
> I've read already some discussions concerning milliseconds as an index in
> zoo index and was wondering if there is a simple solution to differentiate
> ticks which come with 1 second timestamp resolution by adding index for rows
> with the same timestamp.
> One of the solutions would be just to create an index with length of
> nrows(x) and consider them as milli(micro)seconds, so my index will look
> like:
> 2007-12-24 06:14:00.154  22.00  
> 2007-12-24 06:14:00.155  22.00  
> 2007-12-24 06:15:00.156  22.00  
> 2007-12-24 06:15:00.157  22.00  
> 
> So with solution order of the observations(ticks) is preserved, though the
> solution is not very nice.
> Is there a way to create index in the following way?
> 2007-12-24 06:14:00.154  22.00  
> 2007-12-24 06:14:00.155  22.00  
> 2007-12-24 06:15:00.001  22.00  
> 2007-12-24 06:15:00.002  22.00  
> 
> So this index will be in effect number(position) of the tick in the second.

There was an extensive discussion on this list of millisecond support in 
zoo a few weeks ago.  Have you reviewed the suggestions posted in that 
thread, including by Gabor (one of the authors of zoo), Dirk, and other 
experienced users?

Regards,

    - Brian


From ggrothendieck at gmail.com  Wed Dec 26 16:09:31 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Dec 2007 10:09:31 -0500
Subject: [R-SIG-Finance] Adding milliseconds to zoo object
In-Reply-To: <000a01c847ae$b725d290$257177b0$@ru>
References: <mailman.1.1198580401.18896.r-sig-finance@stat.math.ethz.ch>
	<000a01c847ae$b725d290$257177b0$@ru>
Message-ID: <971536df0712260709r13200a05p777780e12e7d641d@mail.gmail.com>

The time class considerations for zoo are that whatever class you
use must support the methods indicated in ?zoo and the index on
any zoo time series must have unique elements.  If they are not
unique you can jitter them, add small amounts to make
them unique or aggregate them so that multiple non-unique points
are eliminated and replaced with a single point.

If your question is whether you could use ordered pairs as a time
class try the R "complex" class or create your own class.

> library(zoo)
> z1 <- zoo(11:15, complex(real = c(1, 1, 2, 2, 3), imag = c(2, 1, 1, 2, 5)))
> merge(z1, lag(z1))
     z1 lag(z1)
1+1i 12      11
1+2i 11      13
2+1i 13      14
2+2i 14      15
3+5i 15      NA

On Dec 26, 2007 6:01 AM, Yuri Volchik <volchik2000 at list.ru> wrote:
> I've read already some discussions concerning milliseconds as an index in
> zoo index and was wondering if there is a simple solution to differentiate
> ticks which come with 1 second timestamp resolution by adding index for rows
> with the same timestamp.
> One of the solutions would be just to create an index with length of
> nrows(x) and consider them as milli(micro)seconds, so my index will look
> like:
> 2007-12-24 06:14:00.154  22.00
> 2007-12-24 06:14:00.155  22.00
> 2007-12-24 06:15:00.156  22.00
> 2007-12-24 06:15:00.157  22.00
>
> So with solution order of the observations(ticks) is preserved, though the
> solution is not very nice.
> Is there a way to create index in the following way?
> 2007-12-24 06:14:00.154  22.00
> 2007-12-24 06:14:00.155  22.00
> 2007-12-24 06:15:00.001  22.00
> 2007-12-24 06:15:00.002  22.00
>
> So this index will be in effect number(position) of the tick in the second.
>
> Thanks
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From finbref.2006 at gmail.com  Wed Dec 26 16:13:19 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Wed, 26 Dec 2007 16:13:19 +0100
Subject: [R-SIG-Finance] RBloomberg error
In-Reply-To: <d0f55a670712241302s277ba935sf3f9cee14abae68f@mail.gmail.com>
References: <d0f55a670712220842t17af669ar34eb271c95486f3e@mail.gmail.com>
	<f2e3401f0712240913yec8f4d8o764bb1d72d6340a4@mail.gmail.com>
	<d0f55a670712241302s277ba935sf3f9cee14abae68f@mail.gmail.com>
Message-ID: <d0f55a670712260713p711c002dud06db5577e0e6c68@mail.gmail.com>

The problem is resolved. :)
I was bothering the help desk until wefound out together that I should
start the bbcomm first.
Thomas


From volchik2000 at list.ru  Wed Dec 26 17:11:53 2007
From: volchik2000 at list.ru (Yuri Volchik)
Date: Wed, 26 Dec 2007 16:11:53 -0000
Subject: [R-SIG-Finance] Adding milliseconds to zoo object
In-Reply-To: <971536df0712260709r13200a05p777780e12e7d641d@mail.gmail.com>
References: <mailman.1.1198580401.18896.r-sig-finance@stat.math.ethz.ch>	
	<000a01c847ae$b725d290$257177b0$@ru>
	<971536df0712260709r13200a05p777780e12e7d641d@mail.gmail.com>
Message-ID: <000c01c847da$08278730$18769590$@ru>

Thank you for replies Gabor & Brian,

Yes I did read the previous threads concerning similar problem and I saw
Dirk's post:

>> I pad my high-resolution times with an eps=1.0-6 to make them distinct,
if
>> need be 

So I presume the question is better formulated as how to pad high-resolution
times with an eps to make them distinct given that we have original index
with non-unique values, but order in the file reflect their relative
position inside this time interval.
 
2007-12-24 06:14:00  22.00
2007-12-24 06:14:00  22.00
2007-12-24 06:15:00  22.00
2007-12-24 06:15:00  22.00

My current idea is to convert timestamps to factors and run tapply on them,
but probably there is a better solution.


From ggrothendieck at gmail.com  Wed Dec 26 17:40:18 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Dec 2007 11:40:18 -0500
Subject: [R-SIG-Finance] Adding milliseconds to zoo object
In-Reply-To: <000c01c847da$08278730$18769590$@ru>
References: <mailman.1.1198580401.18896.r-sig-finance@stat.math.ethz.ch>
	<000a01c847ae$b725d290$257177b0$@ru>
	<971536df0712260709r13200a05p777780e12e7d641d@mail.gmail.com>
	<000c01c847da$08278730$18769590$@ru>
Message-ID: <971536df0712260840o151c3328v4f84653e303d8a1@mail.gmail.com>

Even though non-unique times form illegal zoo objects, both
zoo() and read.zoo() will allow you to create such illegal zoo objects
with a warning, rather than an error, in order to give you a chance to
fix them up into legal ones.  You can still extract and replace the times
in case you want to apply jitter or fixed increments and aggregate.zoo
will still work in case you want to summarize all points with the same
time into a single point. Here is an example of adding increments via
interpolation with numeric times:

> library(zoo)
> z <- zoo(11:15, c(1, 1, 2, 2, 5))
Warning message:
In zoo(11:15, c(1, 1, 2, 2, 5)) :
  some methods for "zoo" objects do not work if the index entries in
'order.by' are not unique
> z
 1  1  2  2  5
11 12 13 14 15
> time(z) <- na.approx(ifelse(duplicated(time(z)), NA, time(z)))
> z
  1 1.5   2 3.5   5
 11  12  13  14  15


On Dec 26, 2007 11:11 AM, Yuri Volchik <volchik2000 at list.ru> wrote:
> Thank you for replies Gabor & Brian,
>
> Yes I did read the previous threads concerning similar problem and I saw
> Dirk's post:
>
> >> I pad my high-resolution times with an eps=1.0-6 to make them distinct,
> if
> >> need be
>
> So I presume the question is better formulated as how to pad high-resolution
> times with an eps to make them distinct given that we have original index
> with non-unique values, but order in the file reflect their relative
> position inside this time interval.
>
> 2007-12-24 06:14:00  22.00
> 2007-12-24 06:14:00  22.00
> 2007-12-24 06:15:00  22.00
> 2007-12-24 06:15:00  22.00
>
> My current idea is to convert timestamps to factors and run tapply on them,
> but probably there is a better solution.
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Wed Dec 26 20:17:58 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Dec 2007 14:17:58 -0500
Subject: [R-SIG-Finance] Adding milliseconds to zoo object
In-Reply-To: <971536df0712260840o151c3328v4f84653e303d8a1@mail.gmail.com>
References: <mailman.1.1198580401.18896.r-sig-finance@stat.math.ethz.ch>
	<000a01c847ae$b725d290$257177b0$@ru>
	<971536df0712260709r13200a05p777780e12e7d641d@mail.gmail.com>
	<000c01c847da$08278730$18769590$@ru>
	<971536df0712260840o151c3328v4f84653e303d8a1@mail.gmail.com>
Message-ID: <971536df0712261117x4d9e2040me1484196cbfbe5ec@mail.gmail.com>

On Dec 26, 2007 11:40 AM, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Even though non-unique times form illegal zoo objects, both
> zoo() and read.zoo() will allow you to create such illegal zoo objects
> with a warning, rather than an error, in order to give you a chance to
> fix them up into legal ones.  You can still extract and replace the times
> in case you want to apply jitter or fixed increments and aggregate.zoo
> will still work in case you want to summarize all points with the same
> time into a single point. Here is an example of adding increments via
> interpolation with numeric times:
>
> > library(zoo)
> > z <- zoo(11:15, c(1, 1, 2, 2, 5))
> Warning message:
> In zoo(11:15, c(1, 1, 2, 2, 5)) :
>  some methods for "zoo" objects do not work if the index entries in
> 'order.by' are not unique
> > z
>  1  1  2  2  5
> 11 12 13 14 15
> > time(z) <- na.approx(ifelse(duplicated(time(z)), NA, time(z)))
> > z
>  1 1.5   2 3.5   5
>  11  12  13  14  15

Just one follow up.  The code above will only work if the last
point in the series is at a unique time point.

Here is a solution that also deletes trailing duplicates so
it should be ok even in that case:

> z <- zoo(11:15, c(1, 1, 2, 2, 2))
Warning message:
In zoo(11:15, c(1, 1, 2, 2, 2)) :
  some methods for "zoo" objects do not work if the index entries in
'order.by' are not unique
> time(z) <- na.approx(ifelse(duplicated(time(z)), NA, time(z)), na.rm = FALSE)
> z <- z[!is.na(time(z))]
> z
  1 1.5   2
 11  12  13


>
>
>
> On Dec 26, 2007 11:11 AM, Yuri Volchik <volchik2000 at list.ru> wrote:
> > Thank you for replies Gabor & Brian,
> >
> > Yes I did read the previous threads concerning similar problem and I saw
> > Dirk's post:
> >
> > >> I pad my high-resolution times with an eps=1.0-6 to make them distinct,
> > if
> > >> need be
> >
> > So I presume the question is better formulated as how to pad high-resolution
> > times with an eps to make them distinct given that we have original index
> > with non-unique values, but order in the file reflect their relative
> > position inside this time interval.
> >
> > 2007-12-24 06:14:00  22.00
> > 2007-12-24 06:14:00  22.00
> > 2007-12-24 06:15:00  22.00
> > 2007-12-24 06:15:00  22.00
> >
> > My current idea is to convert timestamps to factors and run tapply on them,
> > but probably there is a better solution.
> >
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From volchik2000 at list.ru  Wed Dec 26 20:21:16 2007
From: volchik2000 at list.ru (Yuri Volchik)
Date: Wed, 26 Dec 2007 19:21:16 -0000
Subject: [R-SIG-Finance] Adding milliseconds to zoo object
In-Reply-To: <971536df0712260840o151c3328v4f84653e303d8a1@mail.gmail.com>
References: <mailman.1.1198580401.18896.r-sig-finance@stat.math.ethz.ch>	
	<000a01c847ae$b725d290$257177b0$@ru>	
	<971536df0712260709r13200a05p777780e12e7d641d@mail.gmail.com>	
	<000c01c847da$08278730$18769590$@ru>
	<971536df0712260840o151c3328v4f84653e303d8a1@mail.gmail.com>
Message-ID: <000d01c847f4$7d008ec0$7701ac40$@ru>

Thanks, very interesting solution, works like magic. The only (very minor)
inconvenience are NA's at the end but they can be removed or extrapolated.

Thanks once again.


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Wednesday, December 26, 2007 4:40 PM
To: Yuri Volchik
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Adding milliseconds to zoo object

Even though non-unique times form illegal zoo objects, both
zoo() and read.zoo() will allow you to create such illegal zoo objects
with a warning, rather than an error, in order to give you a chance to
fix them up into legal ones.  You can still extract and replace the times
in case you want to apply jitter or fixed increments and aggregate.zoo
will still work in case you want to summarize all points with the same
time into a single point. Here is an example of adding increments via
interpolation with numeric times:

> library(zoo)
> z <- zoo(11:15, c(1, 1, 2, 2, 5))
Warning message:
In zoo(11:15, c(1, 1, 2, 2, 5)) :
  some methods for "zoo" objects do not work if the index entries in
'order.by' are not unique
> z
 1  1  2  2  5
11 12 13 14 15
> time(z) <- na.approx(ifelse(duplicated(time(z)), NA, time(z)))
> z
  1 1.5   2 3.5   5
 11  12  13  14  15


From markus at insightfromdata.com  Wed Dec 26 21:50:44 2007
From: markus at insightfromdata.com (Markus Loecher)
Date: Wed, 26 Dec 2007 15:50:44 -0500
Subject: [R-SIG-Finance] get.hist.quote stalls
Message-ID: <C39828D4.43A%markus@insightfromdata.com>

Dear all,
I am trying to download a few thousand financial timeseries from
yahoo.finance using get.hist.quote().
It worked great initially, but for some reason, my loop seems to always get
stuck after ~1500 requests. R becomes completely unresponsive and I need to
restart it (MAC OS). As these are daily aggregates for just a year, this
should not be a memory/data size problem.
Does Yaho protect itself against such automated scripts ?
If so, why does my get.hist.quote function not simply time out after a few
minutes ?

Any help would be immensely useful,

Thanks,
Markus


From patrick at burns-stat.com  Wed Dec 26 23:21:34 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed, 26 Dec 2007 22:21:34 +0000
Subject: [R-SIG-Finance] get.hist.quote stalls
In-Reply-To: <C39828D4.43A%markus@insightfromdata.com>
References: <C39828D4.43A%markus@insightfromdata.com>
Message-ID: <4772D3EE.9090509@burns-stat.com>

It could be a memory issue if you are growing
objects instead of creating an object the final
size and then subscripting into it with the assignment.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Markus Loecher wrote:

>Dear all,
>I am trying to download a few thousand financial timeseries from
>yahoo.finance using get.hist.quote().
>It worked great initially, but for some reason, my loop seems to always get
>stuck after ~1500 requests. R becomes completely unresponsive and I need to
>restart it (MAC OS). As these are daily aggregates for just a year, this
>should not be a memory/data size problem.
>Does Yaho protect itself against such automated scripts ?
>If so, why does my get.hist.quote function not simply time out after a few
>minutes ?
>
>Any help would be immensely useful,
>
>Thanks,
>Markus
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From WKeneipp at stormproducts.com  Thu Dec 27 01:57:48 2007
From: WKeneipp at stormproducts.com (Walt Keneipp)
Date: Wed, 26 Dec 2007 18:57:48 -0600
Subject: [R-SIG-Finance] Non-financial "seasonality"
In-Reply-To: <000c01c847da$08278730$18769590$@ru>
References: <mailman.1.1198580401.18896.r-sig-finance@stat.math.ethz.ch>	<000a01c847ae$b725d290$257177b0$@ru><971536df0712260709r13200a05p777780e12e7d641d@mail.gmail.com>
	<000c01c847da$08278730$18769590$@ru>
Message-ID: <A7380169D8B74E48845894A481017AD301F364E8@wdgmxsms01.stormproducts.com>

We have a dataset with frequency from 40 to 1200 MHz in .01 MHz
increments, and corresponding amplitude in db.  There are spikes in
amplitude near 60 Hz and its harmonics caused by the AC line.  The
spikes are not necessarily AT 60 Hz, but vary between 59 and 61.  Is
there a good way to filter out these spikes?  If not, would smoothing be
the way to go?

Walt Keneipp
wkeneipp at stormproducts.com


From volchik2000 at list.ru  Thu Dec 27 12:42:18 2007
From: volchik2000 at list.ru (=?koi8-r?Q?=D2=CF=D7=C5=CE_=E1=CB=D8=C1=D4=CF=C9?=)
Date: Thu, 27 Dec 2007 14:42:18 +0300
Subject: [R-SIG-Finance] Interaction with graphs in R
Message-ID: <E1J7r7y-0007Tl-00.volchik2000-list-ru@f69.mail.ru>

Hi to all,

thanks a lot for you replies concerning milliseconds in zoo objects.

I was wondering if the following was possible with R:

Say i have a lognormal distribution:
 x<-exp(rnorm(1000))
 plot(density(x))

so i have a PDF graph and then i can pull some random points on the line using mouse (the way 'curve' works in  Microsoft Paint for example) in effect to create a custom distribution (bimodal, multimodal, etc), basically whatever shape i want, and then being able to extract resulting distribution for my calculation?

I looked at iPlot but it's not useful for this purpose i presume.

Any direction would be appreciated.

Thanks


From edd at debian.org  Thu Dec 27 14:29:28 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 27 Dec 2007 07:29:28 -0600
Subject: [R-SIG-Finance] Brief reminder that R-SIG-Finance is about Finance,
	and for subscribers
Message-ID: <18291.43192.976639.37242@ron.nulle.part>


A brief reminder from your list maintainer that 

a) The charter of the list is 'special interest group for R in finance', so
   please continue to send _finance_-related questions here.
 
   The signal-to-noise ration is currently pleasantly high, and I hope that we
   all can do our share in keeping it there.  This requires focus.

   Consequently, non-finance-related questions, be they about plotting devices or
   general computing or R issues, belong to r-help and not here.  Do NOT post
   non-finance matters here, however urgent you feel about them.

b) The list is subscriber-only and will only allow posts from subscribers. As
   the so-called 'list owner', I get an ungodly amount of spam bounces as our
   gratious hosts at ETH Zuerich seem to pass literally everything through.

   Consequently, if you accidentally post from a non-list address, do NOT expect 
   the post to be 'moderated in'.  Bounces are 'by default' taken to be spam
   and can not be expected to be read. They might, but I would not pin too
   much hope on it, and therefore recommend re-posting from a known address.

Thanks -- that's all.  

Best wishes for 2008, and now back to our regular programming.

Dirk
   

-- 
Three out of two people have difficulties with fractions.


From volchik2000 at list.ru  Thu Dec 27 15:36:13 2007
From: volchik2000 at list.ru (=?koi8-r?Q?=D2=CF=D7=C5=CE_=E1=CB=D8=C1=D4=CF=C9?=)
Date: Thu, 27 Dec 2007 17:36:13 +0300
Subject: [R-SIG-Finance] Interaction with graphs in R
In-Reply-To: <mailman.1.1198753201.4706.r-sig-finance@stat.math.ethz.ch>
References: <mailman.1.1198753201.4706.r-sig-finance@stat.math.ethz.ch>
Message-ID: <E1J7tqH-000H9m-00.volchik2000-list-ru@f122.mail.ru>

Just to clarify (Re: Dirk's post) my post was about visually modifying distribution to value options based on non-standard distribution, i.e. custom, ready made ones.
I think a lot of people dealing with finance have similar problems be it time series representation, storage in the DB, etc, which seem to be not directly linked to finance but nevertheless have to dealt by anybody who is doing financial studies.


From mmiklovic at yahoo.com  Thu Dec 27 15:41:04 2007
From: mmiklovic at yahoo.com (michal miklovic)
Date: Thu, 27 Dec 2007 06:41:04 -0800 (PST)
Subject: [R-SIG-Finance] fixing arma coefficients in garch modelling
Message-ID: <232277.45848.qm@web50103.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071227/8d844eef/attachment.pl 

From mmiklovic at yahoo.com  Thu Dec 27 19:21:11 2007
From: mmiklovic at yahoo.com (michal miklovic)
Date: Thu, 27 Dec 2007 10:21:11 -0800 (PST)
Subject: [R-SIG-Finance] ljung-box tests in arma and garch models
Message-ID: <292006.49737.qm@web50110.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071227/1743241f/attachment.pl 

From markleeds at verizon.net  Thu Dec 27 19:44:29 2007
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 27 Dec 2007 12:44:29 -0600 (CST)
Subject: [R-SIG-Finance] ljung-box tests in arma and garch models
Message-ID: <15681540.1194111198781069266.JavaMail.root@vms063.mailsrvcs.net>

>From: michal miklovic <mmiklovic at yahoo.com>
>Date: 2007/12/27 Thu PM 12:21:11 CST
>To: r-sig-finance at stat.math.ethz.ch
>Subject: [R-SIG-Finance] ljung-box tests in arma and garch models

I'm replying privately because I don't want
to get abused by the geniuses on this list
in the case that I'm totally wrong but I
think you'd have to look at the derivation of the
Q statistic to know what the right df is
and i'm sure it's derived in the original
paper.

I think Box-Leung wrote a paper on
the derivation of the statistic in
the early 70's but I forget the journal.
Possibly biometrika but i can't recall.
Just google Box-Ljung and it will probably
shoot up. You're best bet
is to get the original paper.

But, here's my unofficial 2 cents  that you can
take with a grain of salt. I used
to know this stuff but it's blurry
so that's why I say take it with a grain of salt.

Conceptually, the df used should be the  number of
observations that go into the estimate of
whatever the Q statistic is trying to estimate.
Generally, I don''t the number of parameters estimates estimation
during parameter estimation should come into
play as far as what df are used in looking
up the p-value for Q. The Box test
just used all of the observation that
went into Q. Then, I think
Ljeung came along and figured out
that for small samples, you could
correct the df to get better convergence
to whatever asympototic assumption is
being made in the derivation. I foreget
what correction he/she made but it's
in any decent time series book.

Clearly, the first p+q values in the series
go unestimated but the residuals considered at
for the calculation of Q should start at
whatever the non-NA residual of the series  is ?
When the lag is on the horizontal axis
in acf plot, that denotes the number
of lags between two values in the series
and what the acf estimate was for that lag distance.
So, there's no need to not start at lag 1.
1 just represents the correlation between
the values that were were one lag apart.

yes, in the calculation of estimates and
residuals, whatever number of data points
have to be skipped but this has nothing
to do with lag(p+q) in the acf plot or
the calculation of Q().

I may not be understanding your question 
and hopefully someone else will respond with
their take on it.






















> Hi,
>
>I would like to ask/clarify how should degrees of freedom (and p-values) for the Ljung-Box Q-statistics in arma and garch models be computed. The reason for the question is that I have encountered two different approaches. Let us say we have an arma(p,q) garch(m,n) model. The two approaches are as follows:
>
>1) In R and fArma and fGarch packages, the arma and garch orders are disregarded in the computation of degrees of freedom for the Ljung-Box (LB) Q-statistics. In other words, regardless of p, q, m and n, the LB Q-statistic computed from the first x autocorrelations of (squared) standardised residuals has x degrees of freedom. Given the statistic and degrees of freedom, the corresponding p-value is computed.
>
>2) In EViews, TSP and other statistical software, the LB Q-statistic computed from the first x autocorrelations of standardised residuals has (x - (p+q)) degrees of freedom. Degrees of freedom and p-values are not computed for the first (p+q) LB Q-statistics. A similar method is applied to squared standardised residuals: the LB Q-statistic computed from the first x autocorrelations
>of squared standardised residuals has (x - (m+n)) degrees of freedom.
>Degrees of freedom and p-values are not computed for the first (m+n) LB
>Q-statistics.
>
>I think the second approach is better because the first (p+q) orders in standardised residuals and the first (m+n) orders in squared standardised residuals should not exhibit any pattern and higher orders should be checked for any remaining arma and garch structures. Am I right or wrong?
>
>Thanks for answers and suggestions.
>
>Best regards
>
>Michal Miklovic
>
>
>
>
>
>      ____________________________________________________________________________________
>Be a better friend, newshound, and 
>
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.


From spencer.graves at pdf.com  Fri Dec 28 03:35:01 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 27 Dec 2007 18:35:01 -0800
Subject: [R-SIG-Finance] ljung-box tests in arma and garch models
In-Reply-To: <292006.49737.qm@web50110.mail.re2.yahoo.com>
References: <292006.49737.qm@web50110.mail.re2.yahoo.com>
Message-ID: <477460D5.3050303@pdf.com>

Dear Michal:

      The best way to check something like this is to do a simulation,
tailored to your application.  If you do such, I'd like to hear the
results.

      Absent that, my gut reaction is to agree with you.  The chi-square
distribution with k degrees of freedom is defined as distribution of the
sum of squares of k independent N(0, 1) variates
(http://en.wikipedia.org/wiki/Chi-square_distribution).  In 1900, Karl
Pearson published "On the criterion that a given system of deviations
from the probable in the case of a correlated system of variables is
such that it can be reasonably supposed to have arisen from random
sampling", Philosophical magazine, t.50
(http://fr.wikipedia.org/wiki/Karl_Pearson).  In this test, Pearson
assumed that the sums of squares of k N(0, 1) variates, independent or
not, would follow a chi-square(k).  R. A. Fisher determined that the
number of degrees of freedom should be reduced by the number of
parameters estimated
(http://www.mrs.umn.edu/~sungurea/introstat/history/w98/RAFisher.html).
This led to a feud that continued after Pearson died.

      The "Box-Pierce" and "Ljung-Box" tests are both available in
'Box.test{stats}' and discussed in Tsay (2005) Analysis of "financial
Time Series (Wiley, p. 27), which includes a comment that, "Simulation
studies suggest that the choice of" the number of lags included in the
Ljung-Box statistic should be roughly log(number of observations) for
"better power performance."

       Based on this, the "FinTS" package includes a function "ARIMA"
that calls "arima", computes Box.test on the residuals and adjusts the
number of degrees of freedom to match the examples in Tsay (2005).  I
haven't looked at this in depth, but it would seem to conform with
Eviews, etc., and not with fArma, etc., as you mentioned.

      I haven't done a substantive literature search on this, but if
anyone has evidence bearing on this issue beyond the original Ljung-Box
paper, I'd like to know.

      Hope this helps.
      Spencer Graves

michal miklovic wrote:
>  Hi,
>
> I would like to ask/clarify how should degrees of freedom (and p-values) for the Ljung-Box Q-statistics in arma and garch models be computed. The reason for the question is that I have encountered two different approaches. Let us say we have an arma(p,q) garch(m,n) model. The two approaches are as follows:
>
> 1) In R and fArma and fGarch packages, the arma and garch orders are disregarded in the computation of degrees of freedom for the Ljung-Box (LB) Q-statistics. In other words, regardless of p, q, m and n, the LB Q-statistic computed from the first x autocorrelations of (squared) standardised residuals has x degrees of freedom. Given the statistic and degrees of freedom, the corresponding p-value is computed.
>
> 2) In EViews, TSP and other statistical software, the LB Q-statistic computed from the first x autocorrelations of standardised residuals has (x - (p+q)) degrees of freedom. Degrees of freedom and p-values are not computed for the first (p+q) LB Q-statistics. A similar method is applied to squared standardised residuals: the LB Q-statistic computed from the first x autocorrelations
> of squared standardised residuals has (x - (m+n)) degrees of freedom.
> Degrees of freedom and p-values are not computed for the first (m+n) LB
> Q-statistics.
>
> I think the second approach is better because the first (p+q) orders in standardised residuals and the first (m+n) orders in squared standardised residuals should not exhibit any pattern and higher orders should be checked for any remaining arma and garch structures. Am I right or wrong?
>
> Thanks for answers and suggestions.
>
> Best regards
>
> Michal Miklovic
>
>
>
>
>
>       ____________________________________________________________________________________
> Be a better friend, newshound, and 
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From icos.atropa at gmail.com  Fri Dec 28 09:28:40 2007
From: icos.atropa at gmail.com (icosa atropa)
Date: Fri, 28 Dec 2007 01:28:40 -0700
Subject: [R-SIG-Finance] R.e. Non-financial "seasonality"
Message-ID: <681d07c20712280028j2c23f86am77b9e20c307f117e@mail.gmail.com>

Is there any useful information in the 59-61 Hz range?  If not,
excising everything from that frequency range using a bandstop filter
should give minimal distortion to the rest of the spectrum.

Another question relevant to smoothing is: how much power is in the
noise spikes relevant to the signal of interest?

best,
xian

> spikes are not necessarily AT 60 Hz, but vary between 59 and 61.  Is
> there a good way to filter out these spikes?
> Walt Keneipp
> wkeneipp at stormproducts.com

-- 
Far better an approximate answer to the right question, which is often
vague, than the exact answer to the wrong question, which can always
be made precise -- j.w. tukey


From patrick at burns-stat.com  Fri Dec 28 11:21:33 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Fri, 28 Dec 2007 10:21:33 +0000
Subject: [R-SIG-Finance] ljung-box tests in arma and garch models
In-Reply-To: <477460D5.3050303@pdf.com>
References: <292006.49737.qm@web50110.mail.re2.yahoo.com>
	<477460D5.3050303@pdf.com>
Message-ID: <4774CE2D.8050700@burns-stat.com>

I heartily agree with Spencer that a simulation is the
way to answer the question.  However, my intuition is
the opposite of Spencer's regarding what the answer
will be.

The Burns Statistics working paper on Ljung-Box tests
makes it clear that using rank tests for testing the garch
adequacy will be much more important than messing with
the degrees of freedom.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Spencer Graves wrote:

>Dear Michal:
>
>      The best way to check something like this is to do a simulation,
>tailored to your application.  If you do such, I'd like to hear the
>results.
>
>      Absent that, my gut reaction is to agree with you.  The chi-square
>distribution with k degrees of freedom is defined as distribution of the
>sum of squares of k independent N(0, 1) variates
>(http://en.wikipedia.org/wiki/Chi-square_distribution).  In 1900, Karl
>Pearson published "On the criterion that a given system of deviations
>from the probable in the case of a correlated system of variables is
>such that it can be reasonably supposed to have arisen from random
>sampling", Philosophical magazine, t.50
>(http://fr.wikipedia.org/wiki/Karl_Pearson).  In this test, Pearson
>assumed that the sums of squares of k N(0, 1) variates, independent or
>not, would follow a chi-square(k).  R. A. Fisher determined that the
>number of degrees of freedom should be reduced by the number of
>parameters estimated
>(http://www.mrs.umn.edu/~sungurea/introstat/history/w98/RAFisher.html).
>This led to a feud that continued after Pearson died.
>
>      The "Box-Pierce" and "Ljung-Box" tests are both available in
>'Box.test{stats}' and discussed in Tsay (2005) Analysis of "financial
>Time Series (Wiley, p. 27), which includes a comment that, "Simulation
>studies suggest that the choice of" the number of lags included in the
>Ljung-Box statistic should be roughly log(number of observations) for
>"better power performance."
>
>       Based on this, the "FinTS" package includes a function "ARIMA"
>that calls "arima", computes Box.test on the residuals and adjusts the
>number of degrees of freedom to match the examples in Tsay (2005).  I
>haven't looked at this in depth, but it would seem to conform with
>Eviews, etc., and not with fArma, etc., as you mentioned.
>
>      I haven't done a substantive literature search on this, but if
>anyone has evidence bearing on this issue beyond the original Ljung-Box
>paper, I'd like to know.
>
>      Hope this helps.
>      Spencer Graves
>
>michal miklovic wrote:
>  
>
>> Hi,
>>
>>I would like to ask/clarify how should degrees of freedom (and p-values) for the Ljung-Box Q-statistics in arma and garch models be computed. The reason for the question is that I have encountered two different approaches. Let us say we have an arma(p,q) garch(m,n) model. The two approaches are as follows:
>>
>>1) In R and fArma and fGarch packages, the arma and garch orders are disregarded in the computation of degrees of freedom for the Ljung-Box (LB) Q-statistics. In other words, regardless of p, q, m and n, the LB Q-statistic computed from the first x autocorrelations of (squared) standardised residuals has x degrees of freedom. Given the statistic and degrees of freedom, the corresponding p-value is computed.
>>
>>2) In EViews, TSP and other statistical software, the LB Q-statistic computed from the first x autocorrelations of standardised residuals has (x - (p+q)) degrees of freedom. Degrees of freedom and p-values are not computed for the first (p+q) LB Q-statistics. A similar method is applied to squared standardised residuals: the LB Q-statistic computed from the first x autocorrelations
>>of squared standardised residuals has (x - (m+n)) degrees of freedom.
>>Degrees of freedom and p-values are not computed for the first (m+n) LB
>>Q-statistics.
>>
>>I think the second approach is better because the first (p+q) orders in standardised residuals and the first (m+n) orders in squared standardised residuals should not exhibit any pattern and higher orders should be checked for any remaining arma and garch structures. Am I right or wrong?
>>
>>Thanks for answers and suggestions.
>>
>>Best regards
>>
>>Michal Miklovic
>>
>>
>>
>>
>>
>>      ____________________________________________________________________________________
>>Be a better friend, newshound, and 
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>_______________________________________________
>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>-- Subscriber-posting only. 
>>-- If you want to post, subscribe first.
>>
>>    
>>
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From WKeneipp at stormproducts.com  Fri Dec 28 15:32:55 2007
From: WKeneipp at stormproducts.com (Walt Keneipp)
Date: Fri, 28 Dec 2007 08:32:55 -0600
Subject: [R-SIG-Finance] Non-financial "seasonality"
In-Reply-To: <370AE4BE62A7674C8299149A951D1D580C1B7FF1@EXNJMB61.nam.nsroot.net>
References: <370AE4BE62A7674C8299149A951D1D580C1B7FF1@EXNJMB61.nam.nsroot.net>
Message-ID: <A7380169D8B74E48845894A481017AD301F364FF@wdgmxsms01.stormproducts.com>

Thanks to everyone who replied - my apologies to posting to the
incorrect R group.

Walt


-----Original Message-----
From: Panov, Evgeny [mailto:evgeny.panov at citi.com] 
Sent: Friday, December 28, 2007 7:54 AM
To: Walt Keneipp; r-sig-finance at stat.math.ethz.ch
Subject: RE: [R-SIG-Finance] Non-financial "seasonality"

Here is a little function that filters out the base frequency and
harmonics.
It should be very performant even for data sets of large sizes.

trim.harmonics <- function(time.series, base.frequency, n.modes = 1)
{
	len <- length(time.series)
	harmonic.space.basis <- matrix(ncol = 2 * n.modes, nrow = len)
	for(mode.n in 1:n.modes)
	{
		harmonic.space.basis[, 2 * mode.n - 1] <- sin(0.5 / pi *
base.frequency * mode.n * c(1:len))
		harmonic.space.basis[, 2 * mode.n] <- cos(0.5 / pi *
base.frequency * mode.n * c(1:len))
	}
	lsfit(harmonic.space.basis, time.series)$residuals
}

...and here is how you can test that it works:

trim.harmonics.test <- function()
{
	# creating the time series which has a harmonic with frequency
0.1 per period
	test.time.series <- 1:1000 + 4000 * sin(0.1 * 0.5 / pi *
(1:1000))

	# filtering out the harmonics of frequencies 0.1, 0.2 and 0.3
	filtered.time.series <- trim.harmonics(test.time.series,
base.frequency = 0.1, n.modes = 3)
	
	# plotting the original time series in black
	plot(x = test.time.series, type = "l")

	# plotting the filtered time series in blue
	lines(x = filtered.time.series, type = "l", col = 6)
}

At least this works in Splus.

Regards,
Gene

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of Walt
Keneipp
Sent: Wednesday, December 26, 2007 7:58 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Non-financial "seasonality"


We have a dataset with frequency from 40 to 1200 MHz in .01 MHz
increments, and corresponding amplitude in db.  There are spikes in
amplitude near 60 Hz and its harmonics caused by the AC line.  The
spikes are not necessarily AT 60 Hz, but vary between 59 and 61.  Is
there a good way to filter out these spikes?  If not, would smoothing be
the way to go?

Walt Keneipp
wkeneipp at stormproducts.com

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From mmiklovic at yahoo.com  Sun Dec 30 15:16:59 2007
From: mmiklovic at yahoo.com (michal miklovic)
Date: Sun, 30 Dec 2007 06:16:59 -0800 (PST)
Subject: [R-SIG-Finance] ljung-box tests in arma and garch models
Message-ID: <297830.15280.qm@web50107.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071230/7c15c383/attachment.pl 

From spencer.graves at pdf.com  Sun Dec 30 17:01:40 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 30 Dec 2007 08:01:40 -0800
Subject: [R-SIG-Finance] ljung-box tests in arma and garch models
In-Reply-To: <297830.15280.qm@web50107.mail.re2.yahoo.com>
References: <297830.15280.qm@web50107.mail.re2.yahoo.com>
Message-ID: <4777C0E4.4080805@pdf.com>

Hi, Michal and Patrick: 


PATRICK: 

      In your 2002 paper on the "Robustness of the Ljung-Box Test and 
its Rank Equivalent" 
(http://www.burns-stat.com/pages/Working/ljungbox.pdf), do you consider 
using m-g degrees of freedom, where  m = number of lags and g = number 
of parameters estimated (ignoring an intercept)?  I didn't read every 
word, but I only saw you using 'm' degrees of freedom, and I did not 
notice a comment on this issue. 

      Your Exhibit 3 (p. 7) presents a histogram of the "Distribution of 
the 50-lag Ljung-Box p-vallue under the Gaussian distribution with 100 
observations".  It looks to me like a Beta(a, b) distribution, with a < 
b < 1 but with both a and b fairly close to 1.  The excess of p-values 
in the lower tail suggests to me that the real degrees of freedom for a 
reference chi-square should in this case be slightly greater than 50.  
Your Exhibit 10 shows a comparable histogram for the "Distribution of 
the Ljung-Box 15 lag p-value for the square of a t with 4 degrees of 
freedom with 10,000 observations."  This looks to me like a Beta(a, b) 
distribution with b < a < 1 but with many fewer p-values near 0 than 
near 1.  This in turn suggests to me that the degrees of freedom of the 
reference chi-square test would be less than 15 in this case.  Apart 
from this question, your power curves, Exhibits 14-22 provide rather 
persuasive support for your recommended use of the rank equivalent to 
the traditional Ljung-Box. 


MICHAL: 

      Thanks very much for your further comments on this.  The standard 
asymptotic theory would support Enders' and Tsay's usage of m-g degrees 
of freedom, with m = number of lags and g = number of parameters 
estimated, apart from an intercept -- PROVIDED the parameters were 
estimated using to minimize the Ljung-Box statistic.  However, the 
parameters are typically estimated to maximize a likelihood.  The effect 
of this would likely be to understate the p-value, which we generally 
want to avoid. 

      However, we never want to use these statistics infinite sample 
sizes and degrees of freedom.  Therefore, the asymptotic theory is only 
a guideline, preferably with some adjustment for finite sample sizes and 
degrees of freedom.  Therefore, it is wise to evaluate the adequacy of 
the asymptotics with appropriate simulations.  These may have been 
done;  I have not researched the literature on this, apart from Burns 
(2002).  If anyone knows of other relevant simulations, I'd like to hear 
about them.

      By the way, Tsay's second edition (2005, p. 44) includes a similar 
comment:  "For an AR(p) model, the Ljung-Box statistic Q(m) follows 
asymptotically a chi-square distribution with m-g degrees of freedom, 
where g denotes the number of AR coefficients used in the model."  This 
is similar to but different from your quote from the first edition. 


      Best Wishes,
      Spencer Graves

michal miklovic wrote:
> Hi,
>
> First, I would like to thank Patrick and Spencer for their comments 
> and suggestions.
>
> Second, I did a literature search on the computation of degrees of 
> freedom for the Ljung-Box Q-statistic when testing residuals from an 
> arma model. I do not mean an optimum number of lags for the ACF or the 
> LB Q-statistic but I tried to find an answer to the question: how do I 
> determine degrees of freedom for a given LB Q-statistic from an 
> arma(p,q) model?
> Enders states the following in Applied Econometric Time Series (2nd 
> edition, 2004, Wiley & Sons) on pp. 68 - 69: "The Box-Pierce and 
> Ljung-Box Q-statistics also serve as a check to see if the residuals 
> from an estimated arma(p,q) model behave as a white noise process. 
> However, when the s correlations from an estimated arma(p,q) model are 
> formed, the degrees of freedom are reduced by the number of estimated 
> coefficients. Hence, using the residuals of an arma(p,q) model, Q has 
> a chi-squared [distribution] with s - p - q degrees of freedom."
> Tsay states the following in Analysis of Financial Time Series (1st 
> edition, 2002, Wiley & Sons) on p. 52: "The Ljung-Box statistics of 
> the residuals can be used to check the adequacy of a fitted model. If 
> the model is correctly specified, then Q(m) follows asymptotically a 
> chi-squared distribution with m - g degrees of freedom, where g 
> denotes the number of parameters used in the model."
>
> The two above quotations are in line with mine and Spencer's opinions. 
> Considering what the books say, I would suggest that the computation 
> of the degrees of freedom and, consequently, p-values could be altered 
> in the next release of fArma and fGarch.
>
> I did not find any exact formulations concerning the computation of 
> degrees of freedom for the LB Q-statistics when testing squared 
> standardised residuals from an estimated garch model.
>
> Best regards
>
> Michal Miklovic
>
>
>
> ----- Original Message ----
> From: Patrick Burns <patrick at burns-stat.com>
> To: Spencer Graves <spencer.graves at pdf.com>
> Cc: michal miklovic <mmiklovic at yahoo.com>; r-sig-finance at stat.math.ethz.ch
> Sent: Friday, December 28, 2007 11:21:33 AM
> Subject: Re: [R-SIG-Finance] ljung-box tests in arma and garch models
>
> I heartily agree with Spencer that a simulation is the
> way to answer the question.  However, my intuition is
> the opposite of Spencer's regarding what the answer
> will be.
>
> The Burns Statistics working paper on Ljung-Box tests
> makes it clear that using rank tests for testing the garch
> adequacy will be much more important than messing with
> the degrees of freedom.
>
>
> Patrick Burns
> patrick at burns-stat.com <mailto:patrick at burns-stat.com>
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Spencer Graves wrote:
>
> >Dear Michal:
> >
> >      The best way to check something like this is to do a simulation,
> >tailored to your application.  If you do such, I'd like to hear the
> >results.
> >
> >      Absent that, my gut reaction is to agree with you.  The chi-square
> >distribution with k degrees of freedom is defined as distribution of the
> >sum of squares of k independent N(0, 1) variates
> >(http://en.wikipedia.org/wiki/Chi-square_distribution).  In 1900, Karl
> >Pearson published "On the criterion that a given system of deviations
> >from the probable in the case of a correlated system of variables is
> >such that it can be reasonably supposed to have arisen from random
> >sampling", Philosophical magazine, t.50
> >(http://fr.wikipedia.org/wiki/Karl_Pearson).  In this test, Pearson
> >assumed that the sums of squares of k N(0, 1) variates, independent or
> >not, would follow a chi-square(k).  R. A. Fisher determined that the
> >number of degrees of freedom should be reduced by the number of
> >parameters estimated
> >(http://www.mrs.umn.edu/~sungurea/introstat/history/w98/RAFisher.html 
> <http://www.mrs.umn.edu/%7Esungurea/introstat/history/w98/RAFisher.html>).
> >This led to a feud that continued after Pearson died.
> >
> >      The "Box-Pierce" and "Ljung-Box" tests are both available in
> >'Box.test{stats}' and discussed in Tsay (2005) Analysis of "financial
> >Time Series (Wiley, p. 27), which includes a comment that, "Simulation
> >studies suggest that the choice of" the number of lags included in the
> >Ljung-Box statistic should be roughly log(number of observations) for
> >"better power performance."
> >
> >      Based on this, the "FinTS" package includes a function "ARIMA"
> >that calls "arima", computes Box.test on the residuals and adjusts the
> >number of degrees of freedom to match the examples in Tsay (2005).  I
> >haven't looked at this in depth, but it would seem to conform with
> >Eviews, etc., and not with fArma, etc., as you mentioned.
> >
> >      I haven't done a substantive literature search on this, but if
> >anyone has evidence bearing on this issue beyond the original Ljung-Box
> >paper, I'd like to know.
> >
> >      Hope this helps.
> >      Spencer Graves
> >
> >michal miklovic wrote:
> > 
> >
> >> Hi,
> >>
> >>I would like to ask/clarify how should degrees of freedom (and 
> p-values) for the Ljung-Box Q-statistics in arma and garch models be 
> computed. The reason for the question is that I have encountered two 
> different approaches. Let us say we have an arma(p,q) garch(m,n) 
> model. The two approaches are as follows:
> >>
> >>1) In R and fArma and fGarch packages, the arma and garch orders are 
> disregarded in the computation of degrees of freedom for the Ljung-Box 
> (LB) Q-statistics. In other words, regardless of p, q, m and n, the LB 
> Q-statistic computed from the first x autocorrelations of (squared) 
> standardised residuals has x degrees of freedom. Given the statistic 
> and degrees of freedom, the corresponding p-value is computed.
> >>
> >>2) In EViews, TSP and other statistical software, the LB Q-statistic 
> computed from the first x autocorrelations of standardised residuals 
> has (x - (p+q)) degrees of freedom. Degrees of freedom and p-values 
> are not computed for the first (p+q) LB Q-statistics. A similar method 
> is applied to squared standardised residuals: the LB Q-statistic 
> computed from the first x autocorrelations
> >>of squared standardised residuals has (x - (m+n)) degrees of freedom.
> >>Degrees of freedom and p-values are not computed for the first (m+n) LB
> >>Q-statistics.
> >>
> >>I think the second approach is better because the first (p+q) orders 
> in standardised residuals and the first (m+n) orders in squared 
> standardised residuals should not exhibit any pattern and higher 
> orders should be checked for any remaining arma and garch structures. 
> Am I right or wrong?
> >>
> >>Thanks for answers and suggestions.
> >>
> >>Best regards
> >>
> >>Michal Miklovic
> >>
> >>
> >>
> >>
> >>
> >>      
> ____________________________________________________________________________________
> >>Be a better friend, newshound, and
> >>
> >>
> >>    [[alternative HTML version deleted]]
> >>
> >>_______________________________________________
> >>R-SIG-Finance at stat.math.ethz.ch 
> <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >>-- Subscriber-posting only.
> >>-- If you want to post, subscribe first.
> >>
> >>   
> >>
> >
> >_______________________________________________
> >R-SIG-Finance at stat.math.ethz.ch 
> <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >-- Subscriber-posting only.
> >-- If you want to post, subscribe first.
> >
> >
> > 
> >
>
>
> ------------------------------------------------------------------------
> Be a better friend, newshound, and know-it-all with Yahoo! Mobile. Try 
> it now. 
> <http://us.rd.yahoo.com/evt=51733/*http://mobile.yahoo.com/;_ylt=Ahu06i62sR8HDtDypao8Wcj9tAcJ%20>


From frainj at tcd.ie  Sun Dec 30 19:54:23 2007
From: frainj at tcd.ie (John Frain)
Date: Sun, 30 Dec 2007 18:54:23 +0000
Subject: [R-SIG-Finance] ljung-box tests in arma and garch models
In-Reply-To: <297830.15280.qm@web50107.mail.re2.yahoo.com>
References: <297830.15280.qm@web50107.mail.re2.yahoo.com>
Message-ID: <cfdde1650712301054k39240ef1y42b597aa4e60df20@mail.gmail.com>

For a proof that the appropriate degrees of freedom is s-p-q see
Brockwell and Davis (1990), Time Series: Theory and Methods, 2nd
Edition, Springer, page 310.

John Frain

On 30/12/2007, michal miklovic <mmiklovic at yahoo.com> wrote:
> Hi,
>
> First, I would like to thank Patrick and Spencer for their comments and suggestions.
>
> Second,
> I did a literature search on the computation of degrees of freedom for
> the Ljung-Box Q-statistic when testing residuals from an arma model. I
> do not mean an optimum number of lags for the ACF or the LB Q-statistic
> but I tried to find an answer to the question: how do I determine
> degrees of freedom for a given LB Q-statistic from an arma(p,q) model?
> Enders
> states the following in Applied Econometric Time Series (2nd edition,
> 2004, Wiley & Sons) on pp. 68 - 69: "The Box-Pierce and Ljung-Box
> Q-statistics also serve as a check to see if the residuals from an
> estimated arma(p,q) model behave as a white noise process. However,
> when the s correlations from an estimated arma(p,q) model are formed,
> the degrees of freedom are reduced by the number of estimated
> coefficients. Hence, using the residuals of an arma(p,q) model, Q has a
> chi-squared [distribution] with s - p - q degrees of freedom."
> Tsay
> states the following in Analysis of Financial Time Series (1st edition,
> 2002, Wiley & Sons) on p. 52: "The Ljung-Box statistics of the
> residuals can be used to check the adequacy of a fitted model. If the
> model is correctly specified, then Q(m) follows asymptotically a
> chi-squared distribution with m - g degrees of freedom, where g denotes
> the number of parameters used in the model."
>
> The two above
> quotations are in line with mine and Spencer's opinions. Considering
> what the books say, I would suggest that the computation of the degrees
> of freedom and, consequently, p-values could be altered in the next
> release of fArma and fGarch.
>
> I did not find any exact
> formulations concerning the computation of degrees of freedom for the
> LB Q-statistics when testing squared standardised residuals from an
> estimated garch model.
>
> Best regards
>
> Michal Miklovic
>
>
>
> ----- Original Message ----
> From: Patrick Burns <patrick at burns-stat.com>
> To: Spencer Graves <spencer.graves at pdf.com>
> Cc: michal miklovic <mmiklovic at yahoo.com>; r-sig-finance at stat.math.ethz.ch
> Sent: Friday, December 28, 2007 11:21:33 AM
> Subject: Re: [R-SIG-Finance] ljung-box tests in arma and garch models
>
>
> I heartily agree with Spencer that a simulation is the
> way to answer the question.  However, my intuition is
> the opposite of Spencer's regarding what the answer
> will be.
>
> The Burns Statistics working paper on Ljung-Box tests
> makes it clear that using rank tests for testing the garch
> adequacy will be much more important than messing with
> the degrees of freedom.
>
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Spencer Graves wrote:
>
> >Dear Michal:
> >
> >      The best way to check something like this is to do a simulation,
> >tailored to your application.  If you do such, I'd like to hear the
> >results.
> >
> >      Absent that, my gut reaction is to agree with you.  The
>  chi-square
> >distribution with k degrees of freedom is defined as distribution of
>  the
> >sum of squares of k independent N(0, 1) variates
> >(http://en.wikipedia.org/wiki/Chi-square_distribution).  In 1900, Karl
> >Pearson published "On the criterion that a given system of deviations
> >from the probable in the case of a correlated system of variables is
> >such that it can be reasonably supposed to have arisen from random
> >sampling", Philosophical magazine, t.50
> >(http://fr.wikipedia.org/wiki/Karl_Pearson).  In this test, Pearson
> >assumed that the sums of squares of k N(0, 1) variates, independent or
> >not, would follow a chi-square(k).  R. A. Fisher determined that the
> >number of degrees of freedom should be reduced by the number of
> >parameters estimated
> >(http://www.mrs.umn.edu/~sungurea/introstat/history/w98/RAFisher.html).
> >This led to a feud that continued after Pearson died.
> >
> >      The "Box-Pierce" and "Ljung-Box" tests are both available in
> >'Box.test{stats}' and discussed in Tsay (2005) Analysis of "financial
> >Time Series (Wiley, p. 27), which includes a comment that, "Simulation
> >studies suggest that the choice of" the number of lags included in the
> >Ljung-Box statistic should be roughly log(number of observations) for
> >"better power performance."
> >
> >       Based on this, the "FinTS" package includes a function "ARIMA"
> >that calls "arima", computes Box.test on the residuals and adjusts the
> >number of degrees of freedom to match the examples in Tsay (2005).  I
> >haven't looked at this in depth, but it would seem to conform with
> >Eviews, etc., and not with fArma, etc., as you mentioned.
> >
> >      I haven't done a substantive literature search on this, but if
> >anyone has evidence bearing on this issue beyond the original
>  Ljung-Box
> >paper, I'd like to know.
> >
> >      Hope this helps.
> >      Spencer Graves
> >
> >michal miklovic wrote:
> >
> >
> >> Hi,
> >>
> >>I would like to ask/clarify how should degrees of freedom (and
>  p-values) for the Ljung-Box Q-statistics in arma and garch models be
>  computed. The reason for the question is that I have encountered two different
>  approaches. Let us say we have an arma(p,q) garch(m,n) model. The two
>  approaches are as follows:
> >>
> >>1) In R and fArma and fGarch packages, the arma and garch orders are
>  disregarded in the computation of degrees of freedom for the Ljung-Box
>  (LB) Q-statistics. In other words, regardless of p, q, m and n, the LB
>  Q-statistic computed from the first x autocorrelations of (squared)
>  standardised residuals has x degrees of freedom. Given the statistic and
>  degrees of freedom, the corresponding p-value is computed.
> >>
> >>2) In EViews, TSP and other statistical software, the LB Q-statistic
>  computed from the first x autocorrelations of standardised residuals
>  has (x - (p+q)) degrees of freedom. Degrees of freedom and p-values are
>  not computed for the first (p+q) LB Q-statistics. A similar method is
>  applied to squared standardised residuals: the LB Q-statistic computed
>  from the first x autocorrelations
> >>of squared standardised residuals has (x - (m+n)) degrees of freedom.
> >>Degrees of freedom and p-values are not computed for the first (m+n)
>  LB
> >>Q-statistics.
> >>
> >>I think the second approach is better because the first (p+q) orders
>  in standardised residuals and the first (m+n) orders in squared
>  standardised residuals should not exhibit any pattern and higher orders should
>  be checked for any remaining arma and garch structures. Am I right or
>  wrong?
> >>
> >>Thanks for answers and suggestions.
> >>
> >>Best regards
> >>
> >>Michal Miklovic
> >>
> >>
> >>
> >>
> >>
> >>
>   ____________________________________________________________________________________
> >>Be a better friend, newshound, and
> >>
> >>
> >>    [[alternative HTML version deleted]]
> >>
> >>_______________________________________________
> >>R-SIG-Finance at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >>-- Subscriber-posting only.
> >>-- If you want to post, subscribe first.
> >>
> >>
> >>
> >
> >_______________________________________________
> >R-SIG-Finance at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >-- Subscriber-posting only.
> >-- If you want to post, subscribe first.
> >
> >
> >
> >
>
>
>
>
>
>
>       ____________________________________________________________________________________
> Never miss a thing.  Make Yahoo your home page.
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


-- 
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


From patrick at burns-stat.com  Mon Dec 31 11:46:43 2007
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon, 31 Dec 2007 10:46:43 +0000
Subject: [R-SIG-Finance] ljung-box tests in arma and garch models
In-Reply-To: <4777C0E4.4080805@pdf.com>
References: <297830.15280.qm@web50107.mail.re2.yahoo.com>
	<4777C0E4.4080805@pdf.com>
Message-ID: <4778C893.2090805@burns-stat.com>

I thought I'd start off with some background for those who
don't know what we are talking about.

The Ljung-Box test in this context is used to see if the model
that is fit has captured all of the signal.  So in hypothesis testing
terms, we have things backwards -- we are satisfied when we
see large p-values rather than wanting to see small p-values.

The working paper referred to below shows that the Ljung-Box
test is fantastically robust to the data being non-Gaussian.  However,
there is a practical setting in which it is not robust enough.  That is
when testing if a garch model captures all of the variation in variance
by squaring the residuals (which will themselves be long-tailed in
practice).

One symptom is seeing p-values for the Ljung-Box test that are very
close to 1, such as .998.  (This is essentially saying that the model has
overfit the data, but overfitting a couple thousand observations with a
handful of parameters is unlikely.)

A good remedy is to use the ranks of the squared residuals rather than
the actual squared residuals in the Ljung-Box test.

This thread is really about the degrees of freedom with which to use to
get the p-value from the test statistic.  In the big picture I regard 
this as
rather unimportant -- it doesn't matter much if the p-value is 3.3% or
3.4%.  However, I do believe in doing things as well as possible.

The asymptotics seem to be saying to use 'm - g' degrees of freedom rather
than 'm'.  Asymptotics are nice but the real question is what happens in
a finite sample with a long-tailed distribution.

Spencer, no I didn't look at degrees of freedom when I was doing the
simulations for the paper.

Pat

Spencer Graves wrote:

> Hi, Michal and Patrick:
>
> PATRICK:
>      In your 2002 paper on the "Robustness of the Ljung-Box Test and 
> its Rank Equivalent" 
> (http://www.burns-stat.com/pages/Working/ljungbox.pdf), do you 
> consider using m-g degrees of freedom, where  m = number of lags and g 
> = number of parameters estimated (ignoring an intercept)?  I didn't 
> read every word, but I only saw you using 'm' degrees of freedom, and 
> I did not notice a comment on this issue.
>      Your Exhibit 3 (p. 7) presents a histogram of the "Distribution 
> of the 50-lag Ljung-Box p-vallue under the Gaussian distribution with 
> 100 observations".  It looks to me like a Beta(a, b) distribution, 
> with a < b < 1 but with both a and b fairly close to 1.  The excess of 
> p-values in the lower tail suggests to me that the real degrees of 
> freedom for a reference chi-square should in this case be slightly 
> greater than 50.  Your Exhibit 10 shows a comparable histogram for the 
> "Distribution of the Ljung-Box 15 lag p-value for the square of a t 
> with 4 degrees of freedom with 10,000 observations."  This looks to me 
> like a Beta(a, b) distribution with b < a < 1 but with many fewer 
> p-values near 0 than near 1.  This in turn suggests to me that the 
> degrees of freedom of the reference chi-square test would be less than 
> 15 in this case.  Apart from this question, your power curves, 
> Exhibits 14-22 provide rather persuasive support for your recommended 
> use of the rank equivalent to the traditional Ljung-Box.
>
> MICHAL:
>      Thanks very much for your further comments on this.  The standard 
> asymptotic theory would support Enders' and Tsay's usage of m-g 
> degrees of freedom, with m = number of lags and g = number of 
> parameters estimated, apart from an intercept -- PROVIDED the 
> parameters were estimated using to minimize the Ljung-Box statistic.  
> However, the parameters are typically estimated to maximize a 
> likelihood.  The effect of this would likely be to understate the 
> p-value, which we generally want to avoid.
>      However, we never want to use these statistics infinite sample 
> sizes and degrees of freedom.  Therefore, the asymptotic theory is 
> only a guideline, preferably with some adjustment for finite sample 
> sizes and degrees of freedom.  Therefore, it is wise to evaluate the 
> adequacy of the asymptotics with appropriate simulations.  These may 
> have been done;  I have not researched the literature on this, apart 
> from Burns (2002).  If anyone knows of other relevant simulations, I'd 
> like to hear about them.
>
>      By the way, Tsay's second edition (2005, p. 44) includes a 
> similar comment:  "For an AR(p) model, the Ljung-Box statistic Q(m) 
> follows asymptotically a chi-square distribution with m-g degrees of 
> freedom, where g denotes the number of AR coefficients used in the 
> model."  This is similar to but different from your quote from the 
> first edition.
>
>      Best Wishes,
>      Spencer Graves
>
> michal miklovic wrote:
>
>> Hi,
>>
>> First, I would like to thank Patrick and Spencer for their comments 
>> and suggestions.
>>
>> Second, I did a literature search on the computation of degrees of 
>> freedom for the Ljung-Box Q-statistic when testing residuals from an 
>> arma model. I do not mean an optimum number of lags for the ACF or 
>> the LB Q-statistic but I tried to find an answer to the question: how 
>> do I determine degrees of freedom for a given LB Q-statistic from an 
>> arma(p,q) model?
>> Enders states the following in Applied Econometric Time Series (2nd 
>> edition, 2004, Wiley & Sons) on pp. 68 - 69: "The Box-Pierce and 
>> Ljung-Box Q-statistics also serve as a check to see if the residuals 
>> from an estimated arma(p,q) model behave as a white noise process. 
>> However, when the s correlations from an estimated arma(p,q) model 
>> are formed, the degrees of freedom are reduced by the number of 
>> estimated coefficients. Hence, using the residuals of an arma(p,q) 
>> model, Q has a chi-squared [distribution] with s - p - q degrees of 
>> freedom."
>> Tsay states the following in Analysis of Financial Time Series (1st 
>> edition, 2002, Wiley & Sons) on p. 52: "The Ljung-Box statistics of 
>> the residuals can be used to check the adequacy of a fitted model. If 
>> the model is correctly specified, then Q(m) follows asymptotically a 
>> chi-squared distribution with m - g degrees of freedom, where g 
>> denotes the number of parameters used in the model."
>>
>> The two above quotations are in line with mine and Spencer's 
>> opinions. Considering what the books say, I would suggest that the 
>> computation of the degrees of freedom and, consequently, p-values 
>> could be altered in the next release of fArma and fGarch.
>>
>> I did not find any exact formulations concerning the computation of 
>> degrees of freedom for the LB Q-statistics when testing squared 
>> standardised residuals from an estimated garch model.
>>
>> Best regards
>>
>> Michal Miklovic
>>
>>
>>
>> ----- Original Message ----
>> From: Patrick Burns <patrick at burns-stat.com>
>> To: Spencer Graves <spencer.graves at pdf.com>
>> Cc: michal miklovic <mmiklovic at yahoo.com>; 
>> r-sig-finance at stat.math.ethz.ch
>> Sent: Friday, December 28, 2007 11:21:33 AM
>> Subject: Re: [R-SIG-Finance] ljung-box tests in arma and garch models
>>
>> I heartily agree with Spencer that a simulation is the
>> way to answer the question.  However, my intuition is
>> the opposite of Spencer's regarding what the answer
>> will be.
>>
>> The Burns Statistics working paper on Ljung-Box tests
>> makes it clear that using rank tests for testing the garch
>> adequacy will be much more important than messing with
>> the degrees of freedom.
>>
>>
>> Patrick Burns
>> patrick at burns-stat.com <mailto:patrick at burns-stat.com>
>> +44 (0)20 8525 0696
>> http://www.burns-stat.com
>> (home of S Poetry and "A Guide for the Unwilling S User")
>>
>> Spencer Graves wrote:
>>
>> >Dear Michal:
>> >
>> >      The best way to check something like this is to do a simulation,
>> >tailored to your application.  If you do such, I'd like to hear the
>> >results.
>> >
>> >      Absent that, my gut reaction is to agree with you.  The 
>> chi-square
>> >distribution with k degrees of freedom is defined as distribution of 
>> the
>> >sum of squares of k independent N(0, 1) variates
>> >(http://en.wikipedia.org/wiki/Chi-square_distribution).  In 1900, Karl
>> >Pearson published "On the criterion that a given system of deviations
>> >from the probable in the case of a correlated system of variables is
>> >such that it can be reasonably supposed to have arisen from random
>> >sampling", Philosophical magazine, t.50
>> >(http://fr.wikipedia.org/wiki/Karl_Pearson).  In this test, Pearson
>> >assumed that the sums of squares of k N(0, 1) variates, independent or
>> >not, would follow a chi-square(k).  R. A. Fisher determined that the
>> >number of degrees of freedom should be reduced by the number of
>> >parameters estimated
>> >(http://www.mrs.umn.edu/~sungurea/introstat/history/w98/RAFisher.html 
>> <http://www.mrs.umn.edu/%7Esungurea/introstat/history/w98/RAFisher.html>). 
>>
>> >This led to a feud that continued after Pearson died.
>> >
>> >      The "Box-Pierce" and "Ljung-Box" tests are both available in
>> >'Box.test{stats}' and discussed in Tsay (2005) Analysis of "financial
>> >Time Series (Wiley, p. 27), which includes a comment that, "Simulation
>> >studies suggest that the choice of" the number of lags included in the
>> >Ljung-Box statistic should be roughly log(number of observations) for
>> >"better power performance."
>> >
>> >      Based on this, the "FinTS" package includes a function "ARIMA"
>> >that calls "arima", computes Box.test on the residuals and adjusts the
>> >number of degrees of freedom to match the examples in Tsay (2005).  I
>> >haven't looked at this in depth, but it would seem to conform with
>> >Eviews, etc., and not with fArma, etc., as you mentioned.
>> >
>> >      I haven't done a substantive literature search on this, but if
>> >anyone has evidence bearing on this issue beyond the original Ljung-Box
>> >paper, I'd like to know.
>> >
>> >      Hope this helps.
>> >      Spencer Graves
>> >
>> >michal miklovic wrote:
>> > >
>> >> Hi,
>> >>
>> >>I would like to ask/clarify how should degrees of freedom (and 
>> p-values) for the Ljung-Box Q-statistics in arma and garch models be 
>> computed. The reason for the question is that I have encountered two 
>> different approaches. Let us say we have an arma(p,q) garch(m,n) 
>> model. The two approaches are as follows:
>> >>
>> >>1) In R and fArma and fGarch packages, the arma and garch orders 
>> are disregarded in the computation of degrees of freedom for the 
>> Ljung-Box (LB) Q-statistics. In other words, regardless of p, q, m 
>> and n, the LB Q-statistic computed from the first x autocorrelations 
>> of (squared) standardised residuals has x degrees of freedom. Given 
>> the statistic and degrees of freedom, the corresponding p-value is 
>> computed.
>> >>
>> >>2) In EViews, TSP and other statistical software, the LB 
>> Q-statistic computed from the first x autocorrelations of 
>> standardised residuals has (x - (p+q)) degrees of freedom. Degrees of 
>> freedom and p-values are not computed for the first (p+q) LB 
>> Q-statistics. A similar method is applied to squared standardised 
>> residuals: the LB Q-statistic computed from the first x autocorrelations
>> >>of squared standardised residuals has (x - (m+n)) degrees of freedom.
>> >>Degrees of freedom and p-values are not computed for the first 
>> (m+n) LB
>> >>Q-statistics.
>> >>
>> >>I think the second approach is better because the first (p+q) 
>> orders in standardised residuals and the first (m+n) orders in 
>> squared standardised residuals should not exhibit any pattern and 
>> higher orders should be checked for any remaining arma and garch 
>> structures. Am I right or wrong?
>> >>
>> >>Thanks for answers and suggestions.
>> >>
>> >>Best regards
>> >>
>> >>Michal Miklovic
>> >>
>> >>
>> >>
>> >>
>> >>
>> >>      
>> ____________________________________________________________________________________ 
>>
>> >>Be a better friend, newshound, and
>> >>
>> >>
>> >>    [[alternative HTML version deleted]]
>> >>
>> >>_______________________________________________
>> >>R-SIG-Finance at stat.math.ethz.ch 
>> <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
>> >>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> >>-- Subscriber-posting only.
>> >>-- If you want to post, subscribe first.
>> >>
>> >>   >>
>> >
>> >_______________________________________________
>> >R-SIG-Finance at stat.math.ethz.ch 
>> <mailto:R-SIG-Finance at stat.math.ethz.ch> mailing list
>> >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> >-- Subscriber-posting only.
>> >-- If you want to post, subscribe first.
>> >
>> >
>> > >
>>
>>
>> ------------------------------------------------------------------------
>> Be a better friend, newshound, and know-it-all with Yahoo! Mobile. 
>> Try it now. 
>> <http://us.rd.yahoo.com/evt=51733/*http://mobile.yahoo.com/;_ylt=Ahu06i62sR8HDtDypao8Wcj9tAcJ%20> 
>>
>
>
>


From brian at braverock.com  Mon Dec 31 12:59:29 2007
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 31 Dec 2007 05:59:29 -0600
Subject: [R-SIG-Finance] PerformanceAnalytics version 0.9.6 released to CRAN
Message-ID: <4778D9A1.4060806@braverock.com>

We are pleased to announce the availability on CRAN of 
PerformanceAnalytics version 0.9.6.

This is a feature and bugfix release.

http://cran.r-project.org/src/contrib/Descriptions/PerformanceAnalytics.html

PerformanceAnalytics is a library of econometric functions for 
performance and risk analysis. This library aims to aid practitioners 
and researchers in utilizing the latest research in analysis of 
non-normal return streams.

Package: PerformanceAnalytics
Type: Package
Title: Econometric tools for performance and risk analysis.
Version: 0.9.6
Date: 2007-12-29
License: GPL
URL: 
http://cran.r-project.org/src/contrib/Descriptions/PerformanceAnalytics.html
URL: http://braverock.com/R/

New Functions:
     chart.ECDF
         Creates an empirical cumulative distribution function (ECDF)
         overlaid with a cumulative distribution function (CDF)
         Inspired by:
         Ruppert, David. 2004.
         Statistics and Finance, an Introduction.
         Ch. 2 Fig. 2.5

     chart.ACF
     chart.ACFplus
         Inspired by (and partially ported from) the website:
         http://www.stat.pitt.edu/stoffer/tsa2/Rcode/acf2.R
         "here's an R function that will plot the ACF and PACF of a time
         series at the same time on the SAME SCALE, and it leaves out
         the zero lag in the ACF [and uses the number of observations
         as the default]"
         That description made a lot of sense, so it's implemented here
         for both the ACF alone and the ACF with the PACF.

     chart.Regression
         Uses a scatterplot to display the relationship of returns
         to a market benchmark.  Fits a linear model and overlays the
         resulting model.  Also overlays a Loess line for comparison.

     Return.read
         Wrapper of 'read.zoo' with some defaults for different
         date formats and frequencies.

     Return.Geltner
         Calculate Geltner liquidity-risk-adjusted return series.
         David Geltner developed a method to remove estimating/liquidity
         bias in real estate index returns.  It has since been applied
         to other return series that show autocorrelation or
         illiquidity effects. The theory is that by correcting for
         autocorrelation, you are uncovering a "true" return from series
         of observed returns that contain illiquidity or manual pricing
         effects.

     SmoothingIndex
         Proposed by Getmansky et al to provide a normalized measure of
         liquidity risk.  The index will produces a number from zero to
         one.  A low number indicates low liquidity risk.  A number
         trending towards one indicates a higher liquidity risk.

     table.Autocorrelation
         Produces data table of autocorrelation coefficients rho and
         corresponding Q(6)-statistic for each column in return series.

     table.CalendarReturns
         Returns a table of returns formatted with years in rows, months
         in columns, and a total column in the last column.
         For additional columns, annual returns will be appended.


Significantly Changed Functions:
     chart.Boxplot
         Added the ability to more completely control the visual display.
         Added the ability to render a Tufte-style compact boxplot.

     chart.Histogram
         Improved visual display for print-quality graphics
         Added fits for extra distributions (stable,cauchy,skew-T)
         Added more control over risk lines
         Added event lines

     chart.QQPlot
         Replaced most internals with port of John Fox's
         qq.plot from 'car'
         Now fits arbitrary distributions
         Allows use of error bands

     We have made changes throughout the package to allow the
     risk-free rate to contain a vector of changing rates corresponding
     with the return series being examined.

     In addition, we have made more extensive use of the features of the
     'zoo' package in this release of PerformanceAnalytics, and removed
     a few external dependencies where those dependencies were minor and
     easily replicated or ported to this package.  We expect both of
     these trends to continue in later releases.  Hopefully, we have
     properly credited the original authors and functions both in our
     code and in the manual pages.

Deprecated Functions:
     rollingCorrelation
     rollingFunction
         These functions have been replaced in our code by the use of
         zoo's 'rollapply' function, and are no longer needed as
         separate custom functions.

New Vignettes:
     We have added as vignettes the presentations we gave on
     PerformanceAnalytics at the R/RMetrics Conference in Mielesalp
     in July 2007 and at UseR! 2007 in Ames, Iowa.

Other:
     This version of PerformanceAnalytics contains many, many minor
     improvements and changes.  We added aver 1500 lines of code
     and comments, and over 1000 lines of documentation.

We have benefited greatly from feedback and comments from the users of
PerformanceAnalytics and from R-SIG-Finance.  Please continue to send 
your questions, comments, and complaints.

Full details available in the ChangeLog or in the CVS logs in all .R 
files in the source package.

Regards,

     - Brian


From rosenthal at galton.uchicago.edu  Mon Dec 31 16:39:17 2007
From: rosenthal at galton.uchicago.edu (Dale Rosenthal)
Date: Mon, 31 Dec 2007 09:39:17 -0600 (CST)
Subject: [R-SIG-Finance] ljung-box tests in arma and garch models
Message-ID: <20071231093917.AUQ38687@m4500-03.uchicago.edu>


I think Patrick is technically correct; but, the point may be
moot for many analyses.  Here's why:

The Box and Ljung-Box statistics are asymptotically
chi-squared.  For iid normal rhos (autocorrelation
coefficients), that is exact.

However, if the rhos are correlated (not unusual) or few in
number (for tests of smaller models), large-scale asymptotics
may offer poor approximations.

For correlated rhos (say corr(rho_j, rho_k) is about 0.2),
large-scale approximations will probably be fine for tests of
3 or more lags.  But higher correlations will decrease the
accuracy of small-model tests.

I discuss similar small-sample approximations in a working
paper: (Data Delays, Index Deletions, Prepayments, and
Defaults,
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1032671). 
Keep in mind that the chi-squared is a special case of the gamma.

Dale

>Message: 3
>Date: Sun, 30 Dec 2007 18:54:23 +0000
>From: "John Frain" <frainj at tcd.ie>
>Subject: Re: [R-SIG-Finance] ljung-box tests in arma and
garch models
>
>For a proof that the appropriate degrees of freedom is
>s-p-q see Brockwell and Davis (1990), Time Series: Theory
>and Methods, 2nd Edition, Springer, page 310.
>
>John Frain
>
>[...]


From lloyd.lubet at gmail.com  Tue Oct  2 00:57:45 2007
From: lloyd.lubet at gmail.com (Lloyd Lubet)
Date: Mon, 01 Oct 2007 22:57:45 -0000
Subject: [R-SIG-Finance] prediction based on the results of fracdiff
Message-ID: <000601c8047e$7489fdd0$0200a8c0@your4dacd0ea75>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071001/553914a7/attachment.pl 

From stefan.albrecht at apep.com  Fri Oct 19 12:58:56 2007
From: stefan.albrecht at apep.com (Albrecht, Dr. Stefan (APEP))
Date: Fri, 19 Oct 2007 10:58:56 -0000
Subject: [R-SIG-Finance]  Getmansky et al. Smoothing Index
Message-ID: <B3E803F92F909741B050C9FA4DDEDE75EEFBE9@naimucog.allianzde.rootdom.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071019/4b86385c/attachment.pl 

From i2000 at go2.pl  Fri Nov  2 17:15:08 2007
From: i2000 at go2.pl (i2000 at go2.pl)
Date: Fri, 02 Nov 2007 16:15:08 -0000
Subject: [R-SIG-Finance] fPortfolio problems and question
Message-ID: <472B4CFC.6040509@go2.pl>

Hello
I have problems with functions calculating Portfolio

I created a sample data:
An object of class ?timeSeries?
Slot "Data":
                   V1          V2
1970-01-01 -0.2765348 2.514018406
1970-01-02 -2.0271939 0.007694128
1970-01-03 -0.5272612 0.620621476
1970-01-04  1.3828997 0.875798234

Slot "positions":
[1] "1970-01-01" "1970-01-02" "1970-01-03" "1970-01-04"
attr(,"control")
FinCenter
    "GMT"

Slot "format":
[1] "%Y-%m-%d"

Slot "FinCenter":
[1] "GMT"

Slot "units":
[1] "V1" "V2"

Slot "recordIDs":
NULL data frame with 0 rows

Slot "title":
[1] "Time Series Object"

Slot "documentation":
[1] "Fri Nov  2 15:37:08 2007"

but
e.g
feasiblePortfolio ,cmlPortfolio tangencyPortfolio
returns
Title:
 Error in cat(getTitle(object), "\n") : could not find function "getTitle"



minvariancePortfolio returns the minimum variance portfolio,
efficientPortfolio returns
Error: is.numeric(targetReturn) is not TRUE

2. In rPortfolio manual is described MarkovitzPortfolio function which 
seems to be absent in fPortfolio. I guess it was replaced by other 
functions?

sincerely
Grzegorz Rogowski


From ryker at wanadoo.fr  Mon Nov 19 15:01:27 2007
From: ryker at wanadoo.fr (Ryker)
Date: Mon, 19 Nov 2007 14:01:27 -0000
Subject: [R-SIG-Finance] [R-sig-finance] timeDate vector
Message-ID: <13826501.post@talk.nabble.com>


Hi,
I would like to create a vector of timeDate and access or change its
elements but when I do that I get some error messages... 

Example:
test <- rep(Sys.timeDate(),9)
test[1] <- Sys.timeDate()

And I get:
Error in test[1] <- Sys.timeDate() : object of type 'S4' is not subsettable

Is there something wrong here? Or is it just not possible to use timeDate
that way?

Thanks,
Laurent


From mark_difford at yahoo.co.uk  Fri Dec 14 11:33:01 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Fri, 14 Dec 2007 10:33:01 -0000
Subject: [R-SIG-Finance] [R-sig-finance] Kernel Regression
In-Reply-To: <20071214084949.26873.qmail@f5mail11.rediffmail.com>
References: <20071214084949.26873.qmail@f5mail11.rediffmail.com>
Message-ID: <14333269.post@talk.nabble.com>


Hi Sudhakar,

The np package is well worth a look, and can handle mixed data types; not
sure if it has specific options for time series.

Regards,
Mark


Sudhakar Achath wrote:
> 
> Dear all:
> 
> I am not able to find a function/package that can do
> kernel regression (gaussian) with p explanatory
> variables, using financial time series data.
> Can anyone help me on this, would much appreciate
> your response.
> 
> Cheers!
> 
> sud achath 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Kernel-Regression-tp14332156p14333269.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From yuri.volchik at gmail.com  Sun Dec  2 13:15:27 2007
From: yuri.volchik at gmail.com (Yuri Volchik)
Date: Sun, 02 Dec 2007 12:15:27 -0000
Subject: [R-SIG-Finance] R memory management
Message-ID: <4752a1c7.01a8420a.7b7d.fffffcf5@mx.google.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071202/abc5ac8f/attachment.pl 

From eric.thibault at gazdefrance.com  Thu Dec 13 13:24:46 2007
From: eric.thibault at gazdefrance.com (Eric THIBAULT)
Date: Thu, 13 Dec 2007 12:24:46 -0000
Subject: [R-SIG-Finance] FGarch Information
Message-ID: <OF4319CAF3.C7E407D1-ONC12573B0.0043D46C-C12573B0.004424EF@notes.edfgdf.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071213/f253e503/attachment.pl 

From mnevill at backcheck.net  Thu Dec 27 17:55:06 2007
From: mnevill at backcheck.net (Max Nevill)
Date: Thu, 27 Dec 2007 16:55:06 -0000
Subject: [R-SIG-Finance] books?
Message-ID: <BFF7EAF6841D7A4389FFB72C795A5896527F59@chkmsx01p.backchk.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20071227/d0c81e24/attachment.pl 

From evgeny.panov at citi.com  Fri Dec 28 14:54:52 2007
From: evgeny.panov at citi.com (Panov, Evgeny )
Date: Fri, 28 Dec 2007 13:54:52 -0000
Subject: [R-SIG-Finance] Non-financial "seasonality"
Message-ID: <370AE4BE62A7674C8299149A951D1D580C1B7FF1@EXNJMB61.nam.nsroot.net>

Here is a little function that filters out the base frequency and harmonics.
It should be very performant even for data sets of large sizes.

trim.harmonics <- function(time.series, base.frequency, n.modes = 1)
{
	len <- length(time.series)
	harmonic.space.basis <- matrix(ncol = 2 * n.modes, nrow = len)
	for(mode.n in 1:n.modes)
	{
		harmonic.space.basis[, 2 * mode.n - 1] <- sin(0.5 / pi * base.frequency * mode.n * c(1:len))
		harmonic.space.basis[, 2 * mode.n] <- cos(0.5 / pi * base.frequency * mode.n * c(1:len))
	}
	lsfit(harmonic.space.basis, time.series)$residuals
}

...and here is how you can test that it works:

trim.harmonics.test <- function()
{
	# creating the time series which has a harmonic with frequency 0.1 per period
	test.time.series <- 1:1000 + 4000 * sin(0.1 * 0.5 / pi * (1:1000))

	# filtering out the harmonics of frequencies 0.1, 0.2 and 0.3
	filtered.time.series <- trim.harmonics(test.time.series, base.frequency = 0.1, n.modes = 3)
	
	# plotting the original time series in black
	plot(x = test.time.series, type = "l")

	# plotting the filtered time series in blue
	lines(x = filtered.time.series, type = "l", col = 6)
}

At least this works in Splus.

Regards,
Gene

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of Walt
Keneipp
Sent: Wednesday, December 26, 2007 7:58 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Non-financial "seasonality"


We have a dataset with frequency from 40 to 1200 MHz in .01 MHz
increments, and corresponding amplitude in db.  There are spikes in
amplitude near 60 Hz and its harmonics caused by the AC line.  The
spikes are not necessarily AT 60 Hz, but vary between 59 and 61.  Is
there a good way to filter out these spikes?  If not, would smoothing be
the way to go?

Walt Keneipp
wkeneipp at stormproducts.com

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


