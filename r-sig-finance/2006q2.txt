From IChatterjee at axarosenberg.com  Wed Apr  5 20:16:19 2006
From: IChatterjee at axarosenberg.com (Indrajit Chatterjee)
Date: Wed, 5 Apr 2006 11:16:19 -0700
Subject: [R-sig-finance] Saving a matrix in binary format
Message-ID: <8C0417103005E641BFEC0A128BC8EB2513F4C4@AREXCH3.axaros.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060405/f13a4ea5/attachment.pl 

From edd at debian.org  Wed Apr  5 22:12:50 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 5 Apr 2006 15:12:50 -0500
Subject: [R-sig-finance] Saving a matrix in binary format
In-Reply-To: <8C0417103005E641BFEC0A128BC8EB2513F4C4@AREXCH3.axaros.com>
References: <8C0417103005E641BFEC0A128BC8EB2513F4C4@AREXCH3.axaros.com>
Message-ID: <17460.9410.782052.501992@basebud.nulle.part>


On 5 April 2006 at 11:16, Indrajit Chatterjee wrote:
| Hi - is it possible to save and load a matrix in binary form using R?
| 
|  
| 
| For example save.matrix() - and load.matrix() , but where the saves and
| loads are in binary format for speed and compactness. 

See  help(save)   -- it is even better than what you ask for as it works for
every R data type, allows for compression and is portable across portable
thanks to the underlying xdr binary representation.  It is the same format R
uses when it saves an interactive session.

Hope this helps, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From jessen at econinfo.de  Thu Apr  6 13:41:50 2006
From: jessen at econinfo.de (Owe Jessen)
Date: Thu, 06 Apr 2006 13:41:50 +0200
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
Message-ID: <4434FE7E.2030509@econinfo.de>

Thanks to the help from this list, I'm on my way learning how to handle R.

There are two things that bugger me:
1. Using yahooImport I can easily get financial data. But how do I
handle the data from there on?
For example, I wanted to calculate the beta of DaimlerChrysler vs. the
DAX. So I imported both datasets. How do I have to manipulate the
objects so I can have a linear regression of the returns of DCX to the DAX?

2. Using different data-sources, one usually gets timeseries of
differing length. Whats the most economical way to make shure that one
feeds the appropiate pieces to lm, plot and so on?

Thanks in advance,
Owe

-- 
Owe Jessen
Diplom-Volkswirt
Hanssenstra?e 17
24106 Kiel

jessen at econinfo.de
http://www.econinfo.de

Festnetz 0431-89096
Mobil 0160-97780537
FAX 0941-599289096

-------------- next part --------------
A non-text attachment was scrubbed...
Name: jessen.vcf
Type: text/x-vcard
Size: 366 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060406/86fa6f09/attachment.vcf 

From steve.moffitt at mail.stuart.iit.edu  Wed Apr  5 22:01:02 2006
From: steve.moffitt at mail.stuart.iit.edu (Steven D. Moffitt)
Date: Wed,  5 Apr 2006 15:01:02 -0500
Subject: [R-sig-finance] Saving a matrix in binary format
Message-ID: <200604051501.AA312017166@stuart.iit.edu>

Dear Mr. Chatterjee,

I'm not aware of a function that does this generically, and I doubt there exists one in the standard relase of R for the following reason - different processors have different numerical formats that would require a specialized format for general purpose floating-point storage.  For example, Intel processors have a little endian architecture whereas Sparcs have a big endian architecture.  Different processors represent doubles differently, so that a read of a bits forming a double from one processor won't be correct for another.  at the present time, processors are transitioning from 32-bit to 64-bit, making it necessary to distinguish 64-bit doubles from 80-bit doubles from 128-bit 'long doubles.'  

In short, while it is possible to write a C/C++ program that would accomplish what you want, it wouldn't be portable.  Even an upgrade from an Intel x86 processor to an Itanium processor could give problems!

Steve Moffitt

Director of Research
WH Trading, LLC



---------- Original Message ----------------------------------
From: "Indrajit Chatterjee" <IChatterjee at axarosenberg.com>
Date:  Wed, 5 Apr 2006 11:16:19 -0700

>Hi - is it possible to save and load a matrix in binary form using R?
>
> 
>
>For example save.matrix() - and load.matrix() , but where the saves and
>loads are in binary format for speed and compactness. 
>
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From whit at twinfieldscapital.com  Thu Apr  6 15:21:51 2006
From: whit at twinfieldscapital.com (Whit Armstrong)
Date: Thu, 6 Apr 2006 09:21:51 -0400
Subject: [R-sig-finance] Saving a matrix in binary format
Message-ID: <726FC6DD09DE1046AF81B499D70C3BCE5232DA@twinfields02.CORP.TWINFIELDSCAPITAL.COM>

Have a look at .saveRDS.  I use it extensively.


     .saveRDS(object, file = "", ascii = FALSE, version = NULL,
              compress = FALSE, refhook = NULL)
     .readRDS(file, refhook = NULL)



-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Steven D.
Moffitt
Sent: Wednesday, April 05, 2006 4:01 PM
To: r-sig-finance at stat.math.ethz.ch; Indrajit Chatterjee
Subject: Re: [R-sig-finance] Saving a matrix in binary format

Dear Mr. Chatterjee,

I'm not aware of a function that does this generically, and I doubt
there exists one in the standard relase of R for the following reason -
different processors have different numerical formats that would require
a specialized format for general purpose floating-point storage.  For
example, Intel processors have a little endian architecture whereas
Sparcs have a big endian architecture.  Different processors represent
doubles differently, so that a read of a bits forming a double from one
processor won't be correct for another.  at the present time, processors
are transitioning from 32-bit to 64-bit, making it necessary to
distinguish 64-bit doubles from 80-bit doubles from 128-bit 'long
doubles.'  

In short, while it is possible to write a C/C++ program that would
accomplish what you want, it wouldn't be portable.  Even an upgrade from
an Intel x86 processor to an Itanium processor could give problems!

Steve Moffitt

Director of Research
WH Trading, LLC



---------- Original Message ----------------------------------
From: "Indrajit Chatterjee" <IChatterjee at axarosenberg.com>
Date:  Wed, 5 Apr 2006 11:16:19 -0700

>Hi - is it possible to save and load a matrix in binary form using R?
>
> 
>
>For example save.matrix() - and load.matrix() , but where the saves and

>loads are in binary format for speed and compactness.
>
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list 
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>

_______________________________________________
R-sig-finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From jgalt70 at yahoo.com  Thu Apr  6 15:59:26 2006
From: jgalt70 at yahoo.com (Andrew West)
Date: Thu, 6 Apr 2006 06:59:26 -0700 (PDT)
Subject: [R-sig-finance] using yahoo and other data to calculate CAPM and FF
	betas
Message-ID: <20060406135926.811.qmail@web60325.mail.yahoo.com>

Thanks to some help from others on the list last year
I was able to improve a function for calculating CAPM
beta coefficients and Fama-French calculations, using
data gathered from the internet.

I have begun working on a more challenging calculation
of Fama-French betas, using a panel data set. After a
couple of days, I now have a roughly working function
allowing one to give a list of stocks (should be
within same industry) to estimate coefficients using
mixed-effects models and compare this to least-squares
modelling. The problem right now is that the function
is not very smart, and I haven't been able to figure
out how to prevent the function from crashing when one
of the companies in the list has a late start or
missing data. Aligning multiple time series into a
panel data dataframe is tough for non-programmers like
me! 

The first function is getrffBeta, requiring a number
of packages. It's activated by typing something like:
getrffBeta("GE", 60) 
[this indicates you want to analyse GE, using a
rolling 60 month window.]

The second, rougher function is getpanelBeta. It's
activated by typing something like:
getpanelbeta(c("BNI","CSX","NSC","UNP"),"2000-01-01")
[the list of stocks and the starting date of your
analysis]
I hope this proves to be of use to someone, and would
welcome any feedback and/or improvements regarding
this code. 

Because YahooMail destroys the code formatting, I'm
attaching the functions as 2 text (r) files.

Regards,
Andrew West


__________________________________________________


-------------- next part --------------
A non-text attachment was scrubbed...
Name: getrffBeta.r
Type: application/octet-stream
Size: 4864 bytes
Desc: pat753074949
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060406/3ffc5687/attachment.obj 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: getpanelbeta.r
Type: application/octet-stream
Size: 2014 bytes
Desc: pat628913167
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060406/3ffc5687/attachment-0001.obj 

From ggrothendieck at gmail.com  Thu Apr  6 19:35:06 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 6 Apr 2006 13:35:06 -0400
Subject: [R-sig-finance] using yahoo and other data to calculate CAPM
	and FF betas
In-Reply-To: <20060406135926.811.qmail@web60325.mail.yahoo.com>
References: <20060406135926.811.qmail@web60325.mail.yahoo.com>
Message-ID: <971536df0604061035i21820680mc6a0ad6c3f47fd83@mail.gmail.com>

On 4/6/06, Andrew West <jgalt70 at yahoo.com> wrote:
> I haven't been able to figure
> out how to prevent the function from crashing when one
> of the companies in the list has a late start or
> missing data. Aligning multiple time series into a
> panel data dataframe is tough for non-programmers like
> me!

If t1 and t2 are two ts class time series or two zoo series
then cbind(t1, t2) will create a multivariate series (2 columns)
In the case of zoo, merge(t1, t2) will also work.

na.omit(cbind(t1, t2)) or na.omit(merge(t1,t2))
will eliminate rows that have any NAs in the case of zoo series.


From wuertz at itp.phys.ethz.ch  Thu Apr  6 13:10:32 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 06 Apr 2006 13:10:32 +0200
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <4434FE7E.2030509@econinfo.de>
References: <4434FE7E.2030509@econinfo.de>
Message-ID: <4434F728.5080103@itp.phys.ethz.ch>

Owe Jessen wrote:

>Thanks to the help from this list, I'm on my way learning how to handle R.
>
>There are two things that bugger me:
>1. Using yahooImport I can easily get financial data. But how do I
>handle the data from there on?
>For example, I wanted to calculate the beta of DaimlerChrysler vs. the
>DAX. So I imported both datasets. How do I have to manipulate the
>objects so I can have a linear regression of the returns of DCX to the DAX?
>
>2. Using different data-sources, one usually gets timeseries of
>differing length. Whats the most economical way to make shure that one
>feeds the appropiate pieces to lm, plot and so on?
>
>Thanks in advance,
>Owe
>
>  
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  
>
# DCX.DE
# ^GDAXI

require(fMultivar)
myFinCenter = "GMT"

# Download -> Data Slot -> As Time Series -> Close
# GET Closing Prices: 2005-06-01 until now - see help(yahooImport)
query = "s=^GDAXI&a=5&b=1&c=2005&d=0&q=31&f=2009&z=^GDAXI&x=.csv"
DAX.CLOSE = as.timeSeries(yahooImport(query)@data)[,"Close"]
query = "s=DCX.DE&a=5&b=1&c=2005&d=0&q=31&f=2009&z=DCX.DE&x=.csv"
DCX.CLOSE = as.timeSeries(yahooImport(query)@data)[,"Close"]

# Align to daily dates: --- So both time series will have afterwards the 
same time stamps
# Consult help(timeSeries)
DAX.ALIGNED = alignDailySeries(DAX.CLOSE, method = "interp")
DCX.ALIGNED = alignDailySeries(DCX.CLOSE, method = "interp")

# Cut Common Piece from each:  --- help(timeDate)
START = modify(c(start(DAX.ALIGNED), start(DCX.ALIGNED)), "sort")[2]
END = modify(c(end(DAX.ALIGNED), end(DCX.ALIGNED)), "sort")[1]
DAX.CUTTED = cutSeries(DAX.ALIGNED, from = START, to = END)
DCX.CUTTED = cutSeries(DCX.ALIGNED, from = START, to = END)

# Merge the two Series:
DCXDAX = mergeSeries(DCX.CUTTED, DAX.CUTTED at Data, units = c("DCX", "DAX"))

# Compute Return Series:
DCXDAX.RET = as.data.frame(returnSeries(DCXDAX))

# Compute Beta: --- linear Modelling:
c(Beta = lm(formula = DCX ~ DAX, data = DCXDAX.RET)$coef[2])

Beta.DAX
1.320997

Is that what you wanted ? Read my Paper about timeDate and timeSeries 
objects under Rmetrics!

Regards Diethelm


From kriskumar at earthlink.net  Fri Apr  7 05:27:06 2006
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Thu, 06 Apr 2006 23:27:06 -0400
Subject: [R-sig-finance] using yahoo and other data to calculate CAPM
 and FF betas
In-Reply-To: <971536df0604061035i21820680mc6a0ad6c3f47fd83@mail.gmail.com>
References: <20060406135926.811.qmail@web60325.mail.yahoo.com>
	<971536df0604061035i21820680mc6a0ad6c3f47fd83@mail.gmail.com>
Message-ID: <4435DC0A.3030806@earthlink.net>

Gabor Grothendieck wrote:

>On 4/6/06, Andrew West <jgalt70 at yahoo.com> wrote:
>  
>
>>I haven't been able to figure
>>out how to prevent the function from crashing when one
>>of the companies in the list has a late start or
>>missing data. Aligning multiple time series into a
>>panel data dataframe is tough for non-programmers like
>>me!
>>    
>>
>
>If t1 and t2 are two ts class time series or two zoo series
>then cbind(t1, t2) will create a multivariate series (2 columns)
>In the case of zoo, merge(t1, t2) will also work.
>
>na.omit(cbind(t1, t2)) or na.omit(merge(t1,t2))
>will eliminate rows that have any NAs in the case of zoo series.
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>

To add to Gabor's suggestion you could do the following to get an 
approximated series..
so if mydata is a vector with "NA" 's then doing

 >mydata<-approx(mydata,xout=seq(along=mydata))$y

this would approximate the series and then you can do a ts.union

Also there was a very interesting paper that showed that the Fama-French 
effect was not really a anamoly when you estimate using
 Robust regression instead of OLS. I can't remember the reference but it 
was Doug Martin and someone else from UW ...
R has some nice facilities with rrcov to do the robust regressions!!

Best,
Krishna


From edd at debian.org  Fri Apr  7 05:49:50 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 6 Apr 2006 22:49:50 -0500
Subject: [R-sig-finance] using yahoo and other data to calculate CAPM
 and FF betas
In-Reply-To: <4435DC0A.3030806@earthlink.net>
References: <20060406135926.811.qmail@web60325.mail.yahoo.com>
	<971536df0604061035i21820680mc6a0ad6c3f47fd83@mail.gmail.com>
	<4435DC0A.3030806@earthlink.net>
Message-ID: <17461.57694.317170.242358@basebud.nulle.part>


On 6 April 2006 at 23:27, Krishna Kumar wrote:
| To add to Gabor's suggestion you could do the following to get an 
| approximated series..
| so if mydata is a vector with "NA" 's then doing
| 
|  >mydata<-approx(mydata,xout=seq(along=mydata))$y

Personally, I'd be careful about interpolating / imputing.  

The zoo class has fine features such as na.locf() and merge() which do most
common operations.  Owe will probably learn a lot just from studying the
documents supplied with the zoo package, and the R News articles.

| Also there was a very interesting paper that showed that the Fama-French
| effect was not really a anamoly when you estimate using Robust regression
| instead of OLS. I can't remember the reference but it

IIRC it was mentioned in the Scherer/Martin book on 'Modern Portofolio
Optimization'. Google'ing for 'doug martin fama french robust' leads to a few
pages at Insightful and UW.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From ggrothendieck at gmail.com  Fri Apr  7 05:53:50 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 6 Apr 2006 23:53:50 -0400
Subject: [R-sig-finance] using yahoo and other data to calculate CAPM
	and FF betas
In-Reply-To: <4435DC0A.3030806@earthlink.net>
References: <20060406135926.811.qmail@web60325.mail.yahoo.com>
	<971536df0604061035i21820680mc6a0ad6c3f47fd83@mail.gmail.com>
	<4435DC0A.3030806@earthlink.net>
Message-ID: <971536df0604062053l3140e028s4cf965d33c528e2@mail.gmail.com>

I mentioned omitting the missing values via na.omit and the poster
below mentioned using linear approximation.  Note that
the zoo package actually has 4 missing value routines:

na.omit - omit missing values
na.approx - replace missing values with linear approximations
na.locf - replace missing values with the last occurrernce carried forward
na.contiguous - remove all but a contiguous stretch of non-missing values

> library(zoo)
> z <- zoo(c(1,NA,3,NA,5))
> na.omit(z)
1 3 5
1 3 5
> na.locf(z)
1 2 3 4 5
1 1 3 3 5
> na.approx(z)
1 2 3 4 5
1 2 3 4 5
> na.contiguous(z)
3
3


On 4/6/06, Krishna Kumar <kriskumar at earthlink.net> wrote:
> Gabor Grothendieck wrote:
>
> >On 4/6/06, Andrew West <jgalt70 at yahoo.com> wrote:
> >
> >
> >>I haven't been able to figure
> >>out how to prevent the function from crashing when one
> >>of the companies in the list has a late start or
> >>missing data. Aligning multiple time series into a
> >>panel data dataframe is tough for non-programmers like
> >>me!
> >>
> >>
> >
> >If t1 and t2 are two ts class time series or two zoo series
> >then cbind(t1, t2) will create a multivariate series (2 columns)
> >In the case of zoo, merge(t1, t2) will also work.
> >
> >na.omit(cbind(t1, t2)) or na.omit(merge(t1,t2))
> >will eliminate rows that have any NAs in the case of zoo series.
> >
> >_______________________________________________
> >R-sig-finance at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
> >
> >
>
> To add to Gabor's suggestion you could do the following to get an
> approximated series..
> so if mydata is a vector with "NA" 's then doing
>
>  >mydata<-approx(mydata,xout=seq(along=mydata))$y
>
> this would approximate the series and then you can do a ts.union
>
> Also there was a very interesting paper that showed that the Fama-French
> effect was not really a anamoly when you estimate using
>  Robust regression instead of OLS. I can't remember the reference but it
> was Doug Martin and someone else from UW ...
> R has some nice facilities with rrcov to do the robust regressions!!
>
> Best,
> Krishna
>
>
>
>


From ajayshah at mayin.org  Fri Apr  7 07:03:13 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Fri, 7 Apr 2006 10:33:13 +0530
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <4434F728.5080103@itp.phys.ethz.ch>
References: <4434FE7E.2030509@econinfo.de> <4434F728.5080103@itp.phys.ethz.ch>
Message-ID: <20060407050313.GR281@lubyanka.local>

> require(fMultivar)
> myFinCenter = "GMT"
> 
> # Download -> Data Slot -> As Time Series -> Close
> # GET Closing Prices: 2005-06-01 until now - see help(yahooImport)
> query = "s=^GDAXI&a=5&b=1&c=2005&d=0&q=31&f=2009&z=^GDAXI&x=.csv"
> DAX.CLOSE = as.timeSeries(yahooImport(query)@data)[,"Close"]
> query = "s=DCX.DE&a=5&b=1&c=2005&d=0&q=31&f=2009&z=DCX.DE&x=.csv"
> DCX.CLOSE = as.timeSeries(yahooImport(query)@data)[,"Close"]
> 
> # Align to daily dates: --- So both time series will have afterwards the 
> same time stamps
> # Consult help(timeSeries)
> DAX.ALIGNED = alignDailySeries(DAX.CLOSE, method = "interp")
> DCX.ALIGNED = alignDailySeries(DCX.CLOSE, method = "interp")
> 
> # Cut Common Piece from each:  --- help(timeDate)
> START = modify(c(start(DAX.ALIGNED), start(DCX.ALIGNED)), "sort")[2]
> END = modify(c(end(DAX.ALIGNED), end(DCX.ALIGNED)), "sort")[1]
> DAX.CUTTED = cutSeries(DAX.ALIGNED, from = START, to = END)
> DCX.CUTTED = cutSeries(DCX.ALIGNED, from = START, to = END)
> 
> # Merge the two Series:
> DCXDAX = mergeSeries(DCX.CUTTED, DAX.CUTTED at Data, units = c("DCX", "DAX"))
> 
> # Compute Return Series:
> DCXDAX.RET = as.data.frame(returnSeries(DCXDAX))
> 
> # Compute Beta: --- linear Modelling:
> c(Beta = lm(formula = DCX ~ DAX, data = DCXDAX.RET)$coef[2])
> 
> Beta.DAX
> 1.320997

This looks like hard work! It's easier using R + zoo:

       http://www.mayin.org/ajayshah/KB/R/html/o6.html
       http://www.mayin.org/ajayshah/KB/R/html/o5.html

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From wuertz at itp.phys.ethz.ch  Fri Apr  7 11:08:07 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Fri, 07 Apr 2006 11:08:07 +0200
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <20060407050313.GR281@lubyanka.local>
References: <4434FE7E.2030509@econinfo.de> <4434F728.5080103@itp.phys.ethz.ch>
	<20060407050313.GR281@lubyanka.local>
Message-ID: <44362BF7.4020101@itp.phys.ethz.ch>

Ajay Narottam Shah wrote:

>>require(fMultivar)
>>myFinCenter = "GMT"
>>
>># Download -> Data Slot -> As Time Series -> Close
>># GET Closing Prices: 2005-06-01 until now - see help(yahooImport)
>>query = "s=^GDAXI&a=5&b=1&c=2005&d=0&q=31&f=2009&z=^GDAXI&x=.csv"
>>DAX.CLOSE = as.timeSeries(yahooImport(query)@data)[,"Close"]
>>query = "s=DCX.DE&a=5&b=1&c=2005&d=0&q=31&f=2009&z=DCX.DE&x=.csv"
>>DCX.CLOSE = as.timeSeries(yahooImport(query)@data)[,"Close"]
>>
>># Align to daily dates: --- So both time series will have afterwards the 
>>same time stamps
>># Consult help(timeSeries)
>>DAX.ALIGNED = alignDailySeries(DAX.CLOSE, method = "interp")
>>DCX.ALIGNED = alignDailySeries(DCX.CLOSE, method = "interp")
>>
>># Cut Common Piece from each:  --- help(timeDate)
>>START = modify(c(start(DAX.ALIGNED), start(DCX.ALIGNED)), "sort")[2]
>>END = modify(c(end(DAX.ALIGNED), end(DCX.ALIGNED)), "sort")[1]
>>DAX.CUTTED = cutSeries(DAX.ALIGNED, from = START, to = END)
>>DCX.CUTTED = cutSeries(DCX.ALIGNED, from = START, to = END)
>>
>># Merge the two Series:
>>DCXDAX = mergeSeries(DCX.CUTTED, DAX.CUTTED at Data, units = c("DCX", "DAX"))
>>
>># Compute Return Series:
>>DCXDAX.RET = as.data.frame(returnSeries(DCXDAX))
>>
>># Compute Beta: --- linear Modelling:
>>c(Beta = lm(formula = DCX ~ DAX, data = DCXDAX.RET)$coef[2])
>>
>>Beta.DAX
>>1.320997
>>    
>>
>
>This looks like hard work! It's easier using R + zoo:
>  
>
Come On!!!!
If it is really so easy in R+zoo, be fair enough and show us how we can 
do it in 10 or less lines!
I'm waiting for it ...

Thanks Diethelm

>       http://www.mayin.org/ajayshah/KB/R/html/o6.html
>       http://www.mayin.org/ajayshah/KB/R/html/o5.html
>
>  
>


From wuertz at itp.phys.ethz.ch  Fri Apr  7 11:58:21 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Fri, 07 Apr 2006 11:58:21 +0200
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <20060407050313.GR281@lubyanka.local>
References: <4434FE7E.2030509@econinfo.de> <4434F728.5080103@itp.phys.ethz.ch>
	<20060407050313.GR281@lubyanka.local>
Message-ID: <443637BD.3010506@itp.phys.ethz.ch>

Why you should definitely use Rmetrics to solve Owe Jessens problem:

# Be sure you have the latest Version of Rmetrics ...65 from the CRAN Server

# Download -> Data Slot -> As Time Series -> Close
# GET Closing Prices: 2005-06-01 until now
require(fCalendar)
myFinCenter = "GMT"
query = "s=^GDAXI&a=5&b=1&c=2005&d=0&q=31&f=2009&z=^GDAXI&x=.csv"
DAX.CLOSE = as.timeSeries(yahooImport(query)@data)[,"Close"]
query = "s=DCX.DE&a=5&b=1&c=2005&d=0&q=31&f=2009&z=DCX.DE&x=.csv"
DCX.CLOSE = as.timeSeries(yahooImport(query)@data)[,"Close"]

# Assume we have the Data, then
DAX = alignDailySeries(DAX.CLOSE)
DCX = alignDailySeries(DCX.CLOSE)
DAX = cutSeries(DAX, start(DCX), end(DCX))
DCX = cutSeries(DCX, start(DCX), end(DCX))
DCA = returnSeries(mergeSeries(DCX, DAX at Data, c("DCX", "DAX")))
c(Beta = lm(DCX ~ DAX, as.data.frame(DCA))$coef[2])

Beta.DAX
1.348041

When you have downloaded the data,
you can do it with default settings in 6 lines !!!!

Note you have several alternatives to align the data, and to
compute the returns. Check the arguments of the functions
alignDailySeries() and returnSeries().

Diethelm




Ajay Narottam Shah wrote:

>>require(fMultivar)
>>myFinCenter = "GMT"
>>
>># Download -> Data Slot -> As Time Series -> Close
>># GET Closing Prices: 2005-06-01 until now - see help(yahooImport)
>>query = "s=^GDAXI&a=5&b=1&c=2005&d=0&q=31&f=2009&z=^GDAXI&x=.csv"
>>DAX.CLOSE = as.timeSeries(yahooImport(query)@data)[,"Close"]
>>query = "s=DCX.DE&a=5&b=1&c=2005&d=0&q=31&f=2009&z=DCX.DE&x=.csv"
>>DCX.CLOSE = as.timeSeries(yahooImport(query)@data)[,"Close"]
>>
>># Align to daily dates: --- So both time series will have afterwards the 
>>same time stamps
>># Consult help(timeSeries)
>>DAX.ALIGNED = alignDailySeries(DAX.CLOSE, method = "interp")
>>DCX.ALIGNED = alignDailySeries(DCX.CLOSE, method = "interp")
>>
>># Cut Common Piece from each:  --- help(timeDate)
>>START = modify(c(start(DAX.ALIGNED), start(DCX.ALIGNED)), "sort")[2]
>>END = modify(c(end(DAX.ALIGNED), end(DCX.ALIGNED)), "sort")[1]
>>DAX.CUTTED = cutSeries(DAX.ALIGNED, from = START, to = END)
>>DCX.CUTTED = cutSeries(DCX.ALIGNED, from = START, to = END)
>>
>># Merge the two Series:
>>DCXDAX = mergeSeries(DCX.CUTTED, DAX.CUTTED at Data, units = c("DCX", "DAX"))
>>
>># Compute Return Series:
>>DCXDAX.RET = as.data.frame(returnSeries(DCXDAX))
>>
>># Compute Beta: --- linear Modelling:
>>c(Beta = lm(formula = DCX ~ DAX, data = DCXDAX.RET)$coef[2])
>>
>>Beta.DAX
>>1.320997
>>    
>>
>
>This looks like hard work! It's easier using R + zoo:
>
>       http://www.mayin.org/ajayshah/KB/R/html/o6.html
>       http://www.mayin.org/ajayshah/KB/R/html/o5.html
>
>  
>


From maechler at stat.math.ethz.ch  Fri Apr  7 12:30:28 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 7 Apr 2006 12:30:28 +0200
Subject: [R-sig-finance] using yahoo and other data to calculate CAPM
 and FF betas
In-Reply-To: <4435DC0A.3030806@earthlink.net>
References: <20060406135926.811.qmail@web60325.mail.yahoo.com>
	<971536df0604061035i21820680mc6a0ad6c3f47fd83@mail.gmail.com>
	<4435DC0A.3030806@earthlink.net>
Message-ID: <17462.16196.412298.871781@stat.math.ethz.ch>

>>>>> "Krishna" == Krishna Kumar <kriskumar at earthlink.net>
>>>>>     on Thu, 06 Apr 2006 23:27:06 -0400 writes:

  .........

    Krishna> Also there was a very interesting paper that showed
    Krishna> that the Fama-French effect was not really a
    Krishna> anamoly when you estimate using Robust regression
    Krishna> instead of OLS. I can't remember the reference but
    Krishna> it was Doug Martin and someone else from UW ...  R
    Krishna> has some nice facilities with rrcov to do the
    Krishna> robust regressions!!

Apropos  "Robust regression":

- Note that 'rrcov' (by Valentin Todorov) has recently been merged
  into the new package "robustbase" -- and the latest rrcov
  version will be merged again.
  The goal of "robustbase" is to provide ``basic robust
  statistics'' to R -- additionally to what's already in 'stats'
  and 'MASS' and trying to be closer to the "state-of-the-art".
  The latest version of robustbase, 0.1-5 has "hit" CRAN yesterday
  and should become available more generally shortly.

  Additionally to the fast ltsReg() {from 'rrcov' originally}, 
  "robustbase" now also contains  lmrob(), implementing a
  "fast MM" estimator (based on fast-S) from Matias
  Salibian-Barreras and Victor Yohai.

- Further note that there's also a `young' R-SIG-robust mailing
  list with quite a few "robustniks" subscribed -- some of who
  do not read other R-lists AFAIK.

Martin Maechler, ETH Zurich


From edd at debian.org  Fri Apr  7 14:20:36 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 7 Apr 2006 07:20:36 -0500
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <443637BD.3010506@itp.phys.ethz.ch>
References: <4434FE7E.2030509@econinfo.de> <4434F728.5080103@itp.phys.ethz.ch>
	<20060407050313.GR281@lubyanka.local>
	<443637BD.3010506@itp.phys.ethz.ch>
Message-ID: <17462.22804.605680.43102@basebud.nulle.part>


On 7 April 2006 at 11:58, Diethelm Wuertz wrote:
| Why you should definitely use Rmetrics to solve Owe Jessens problem:
[...]
| When you have downloaded the data,
| you can do it with default settings in 6 lines !!!!
| 
| Note you have several alternatives to align the data, and to
| compute the returns. Check the arguments of the functions
| alignDailySeries() and returnSeries().

Ajay has point as zoo can really do this compactly:


stopifnot(require(tseries), quiet=TRUE) 	# also loads zoo these days
simpleBeta <- function(ysymb="F", xsymb="^GSPC", startdate="2005-01-01") {
  Y <- get.hist.quote(instrument=ysymb, start=startdate, quote="Close")
  X <- get.hist.quote(instrument=xsymb, start=startdate, quote="Close")
  rets <- diff(log(merge(Y,X,all=FALSE)))
  fit <- lm(Close.Y ~ Close.X, data=rets)
  return(coef(fit)[2])
}

One command each for data retrieval [ and is there a way to not get the
verbose output from downloading ? ], the return transformation and merge
really can be as compact as one compact, fit, and then coefficient
extraction. 

Obviously, this even fits in one call without really saving anything:

reallyShortSimpleBeta <- function(ysymb="F", xsymb="^GSPC",
                                  startdate="2005-01-01") {
  return( coef( lm(Close.Y ~ Close.X,
                   data = diff( log (  merge(
                     Y=get.hist.quote(instrument=ysymb,
                                    start=startdate, quote="Close"),
                     X=get.hist.quote(instrument=xsymb,
                                    start=startdate, quote="Close"),
                     all=FALSE)))))[2] )
}

Now, what is the deal with the Frankfurt data at data Yahoo? 
For regressions of Daimler or Deutsche Bank, ie
  > simpleBeta("DCX.DE", "^GDAXI")
  > simpleBeta("DBK.DE", "^GDAXI")
I get only 200 datapoints whereas US data (with the same start data request
of Jan 1, 2005) gets me the full 317 points...

Anyway, hope this helps.

Cheers, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From Achim.Zeileis at wu-wien.ac.at  Fri Apr  7 14:29:49 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 7 Apr 2006 14:29:49 +0200
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <44362BF7.4020101@itp.phys.ethz.ch>
References: <4434FE7E.2030509@econinfo.de> <4434F728.5080103@itp.phys.ethz.ch>
	<20060407050313.GR281@lubyanka.local>
	<44362BF7.4020101@itp.phys.ethz.ch>
Message-ID: <20060407142949.45001447.Achim.Zeileis@wu-wien.ac.at>

On Fri, 07 Apr 2006 11:08:07 +0200 Diethelm Wuertz wrote:

> >This looks like hard work! It's easier using R + zoo:
> >
> Come On!!!!

Could we please *not* have discussions like this? This is not a
competition (at least IMHO).

I think we're all aware that Diethelm has a great collection of
financial functionality in the Rmetrics packages and I hope that most
users will agree that zoo has time series functionality with more
flexibility concerning the choice of date/time class and the more
standard interface. So it's the question how to get these work
together and not against each other.

> If it is really so easy in R+zoo, be fair enough and show us how we
> can do it in 10 or less lines!
> I'm waiting for it ...

Except for the alignment part which I've omitted for simplicity, you can
do:

## get data from yahoo
library("tseries")
dax <- get.hist.quote("^GDAXI", start = "2005-06-01")[, "Close"]
dcx <- get.hist.quote("DCX.DE", start = "2005-06-01")[, "Close"]

## merge -> returns -> lm
dcxdax <- merge(dcx, dax, all = FALSE)
ddret <- diff(log(dcxdax))
lm(dcx ~ dax, data = as.data.frame(ddret))

Best,
Z

> Thanks Diethelm
> 
> >       http://www.mayin.org/ajayshah/KB/R/html/o6.html
> >       http://www.mayin.org/ajayshah/KB/R/html/o5.html
> >
> >  
> >
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From ggrothendieck at gmail.com  Fri Apr  7 14:47:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Apr 2006 08:47:35 -0400
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <17462.22804.605680.43102@basebud.nulle.part>
References: <4434FE7E.2030509@econinfo.de> <4434F728.5080103@itp.phys.ethz.ch>
	<20060407050313.GR281@lubyanka.local>
	<443637BD.3010506@itp.phys.ethz.ch>
	<17462.22804.605680.43102@basebud.nulle.part>
Message-ID: <971536df0604070547n45ca1e1eq6e8bf3b8a7cfdcbf@mail.gmail.com>

On 4/7/06, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> On 7 April 2006 at 11:58, Diethelm Wuertz wrote:
> | Why you should definitely use Rmetrics to solve Owe Jessens problem:
> [...]
> | When you have downloaded the data,
> | you can do it with default settings in 6 lines !!!!
> |
> | Note you have several alternatives to align the data, and to
> | compute the returns. Check the arguments of the functions
> | alignDailySeries() and returnSeries().
>
> Ajay has point as zoo can really do this compactly:
>
>
> stopifnot(require(tseries), quiet=TRUE)         # also loads zoo these days
> simpleBeta <- function(ysymb="F", xsymb="^GSPC", startdate="2005-01-01") {
>  Y <- get.hist.quote(instrument=ysymb, start=startdate, quote="Close")
>  X <- get.hist.quote(instrument=xsymb, start=startdate, quote="Close")
>  rets <- diff(log(merge(Y,X,all=FALSE)))
>  fit <- lm(Close.Y ~ Close.X, data=rets)
>  return(coef(fit)[2])
> }
>
> and is there a way to not get the
> verbose output from downloading ?

The tseries function, get.hist.quote, uses download.file and that's where
its coming from.  If you want you can redefine download.file with quiet = TRUE.
The following creates a wrapper around get.hist.quote that first
defines a download.file wrapper with quiet = TRUE and then
places a copy of get.hist.quote in a proto object whose parent is
the environment within the redefined get.hist.quote so that the
redefined download.file is found.

library(tseries)
library(proto)

get.hist.quote <- function(...) {
	download.file <- function(url, destfile, method, quiet = TRUE,
	mode = "w", cacheOK = TRUE)
	   utils::download.file(url, destfile = destfile, method = method,
		quiet = quiet, mode = mode, cacheOK = cacheOK)
	with(proto(get.hist.quote = tseries::get.hist.quote),
		get.hist.quote(...))
}

ibm <- get.hist.quote("ibm")

The above does not display messages on the R console
although it still does have a popup that briefly appears.

I also tried using capture.output but this seems not to have
any effect on download.file.


From Achim.Zeileis at wu-wien.ac.at  Fri Apr  7 15:18:13 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 7 Apr 2006 15:18:13 +0200
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <971536df0604070547n45ca1e1eq6e8bf3b8a7cfdcbf@mail.gmail.com>
References: <4434FE7E.2030509@econinfo.de> <4434F728.5080103@itp.phys.ethz.ch>
	<20060407050313.GR281@lubyanka.local>
	<443637BD.3010506@itp.phys.ethz.ch>
	<17462.22804.605680.43102@basebud.nulle.part>
	<971536df0604070547n45ca1e1eq6e8bf3b8a7cfdcbf@mail.gmail.com>
Message-ID: <20060407151813.30f89f8f.Achim.Zeileis@wu-wien.ac.at>

Just that I understand. Is this basically a request to add a `quiet =
TRUE' option to get.hist.quote()?
Z

On Fri, 7 Apr 2006 08:47:35 -0400 Gabor Grothendieck wrote:

> On 4/7/06, Dirk Eddelbuettel <edd at debian.org> wrote:
> >
> > On 7 April 2006 at 11:58, Diethelm Wuertz wrote:
> > | Why you should definitely use Rmetrics to solve Owe Jessens
> > | problem:
> > [...]
> > | When you have downloaded the data,
> > | you can do it with default settings in 6 lines !!!!
> > |
> > | Note you have several alternatives to align the data, and to
> > | compute the returns. Check the arguments of the functions
> > | alignDailySeries() and returnSeries().
> >
> > Ajay has point as zoo can really do this compactly:
> >
> >
> > stopifnot(require(tseries), quiet=TRUE)         # also loads zoo
> > these days simpleBeta <- function(ysymb="F", xsymb="^GSPC",
> > startdate="2005-01-01") { Y <- get.hist.quote(instrument=ysymb,
> > start=startdate, quote="Close") X <- get.hist.quote
> > (instrument=xsymb, start=startdate, quote="Close") rets <- diff(log
> > (merge(Y,X,all=FALSE))) fit <- lm(Close.Y ~ Close.X, data=rets)
> >  return(coef(fit)[2])
> > }
> >
> > and is there a way to not get the
> > verbose output from downloading ?
> 
> The tseries function, get.hist.quote, uses download.file and that's
> where its coming from.  If you want you can redefine download.file
> with quiet = TRUE. The following creates a wrapper around
> get.hist.quote that first defines a download.file wrapper with quiet
> = TRUE and then places a copy of get.hist.quote in a proto object
> whose parent is the environment within the redefined get.hist.quote
> so that the redefined download.file is found.
> 
> library(tseries)
> library(proto)
> 
> get.hist.quote <- function(...) {
> 	download.file <- function(url, destfile, method, quiet = TRUE,
> 	mode = "w", cacheOK = TRUE)
> 	   utils::download.file(url, destfile = destfile, method =
> method, quiet = quiet, mode = mode, cacheOK = cacheOK)
> 	with(proto(get.hist.quote = tseries::get.hist.quote),
> 		get.hist.quote(...))
> }
> 
> ibm <- get.hist.quote("ibm")
> 
> The above does not display messages on the R console
> although it still does have a popup that briefly appears.
> 
> I also tried using capture.output but this seems not to have
> any effect on download.file.
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From edd at debian.org  Fri Apr  7 15:54:45 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 7 Apr 2006 08:54:45 -0500
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <971536df0604070547n45ca1e1eq6e8bf3b8a7cfdcbf@mail.gmail.com>
References: <4434FE7E.2030509@econinfo.de> <4434F728.5080103@itp.phys.ethz.ch>
	<20060407050313.GR281@lubyanka.local>
	<443637BD.3010506@itp.phys.ethz.ch>
	<17462.22804.605680.43102@basebud.nulle.part>
	<971536df0604070547n45ca1e1eq6e8bf3b8a7cfdcbf@mail.gmail.com>
Message-ID: <17462.28453.11552.906120@basebud.nulle.part>


Gabor, Achim, 

On 7 April 2006 at 08:47, Gabor Grothendieck wrote:
| The tseries function, get.hist.quote, uses download.file and that's where
| its coming from.  If you want you can redefine download.file with quiet =
| TRUE.

Right. I think I did that once.

| The following creates a wrapper around get.hist.quote that first
| defines a download.file wrapper with quiet = TRUE and then
| places a copy of get.hist.quote in a proto object whose parent is
| the environment within the redefined get.hist.quote so that the
| redefined download.file is found.

Very nice. I hope I get a chance to study this, and the proto package, at
some point.

| The above does not display messages on the R console
| although it still does have a popup that briefly appears.

Only on some OSs but not on the one I tend to use :)


On 7 April 2006 at 15:18, Achim Zeileis wrote:
| Just that I understand. Is this basically a request to add a `quiet =
| TRUE' option to get.hist.quote()?

Yes please!

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From wuertz at itp.phys.ethz.ch  Fri Apr  7 16:41:35 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Fri, 07 Apr 2006 16:41:35 +0200
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <20060407142949.45001447.Achim.Zeileis@wu-wien.ac.at>
References: <4434FE7E.2030509@econinfo.de>	<4434F728.5080103@itp.phys.ethz.ch>	<20060407050313.GR281@lubyanka.local>	<44362BF7.4020101@itp.phys.ethz.ch>
	<20060407142949.45001447.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <44367A1F.1000807@itp.phys.ethz.ch>

Achim Zeileis wrote:

>On Fri, 07 Apr 2006 11:08:07 +0200 Diethelm Wuertz wrote:
>
>  
>
>>Come On!!!!
>>    
>>
>
>Could we please *not* have discussions like this? This is not a
>competition (at least IMHO).
>  
>
Just one remark to this: When I was a student I hated it when a teacher 
said,
"it is (very) easy to show that", or "it is (much) easier to do" and it 
was never
shown or done ...
When this is the case, why one  don't show it or do it? The student will 
be happy.
Then I think *Come on!!!*  is neither a rude nor an unfair reaction nor the
signal for a competetion, it is just a reminder that it would be fine to 
show the
easy way to the student. He will be thankful.

Anyway, if somebody felt be attacked by the selection of my words, I
apologize - but not for the content behind my words.

Nevertheless, my heart was really happy about the many positive reactions
and code solutions submitted which wouldn't be happened otherwise.

DW
please also read my other remarks further down ...

>Except for the alignment part which I've omitted for simplicity, you can
>do:
>
>## get data from yahoo
>library("tseries")
>dax <- get.hist.quote("^GDAXI", start = "2005-06-01")[, "Close"]
>dcx <- get.hist.quote("DCX.DE", start = "2005-06-01")[, "Close"]
>
>## merge -> returns -> lm
>dcxdax <- merge(dcx, dax, all = FALSE)
>ddret <- diff(log(dcxdax))
>lm(dcx ~ dax, data = as.data.frame(ddret))
>
>Best,
>Z
>
>  
>
Thanks for this clear part of code.
DW

Two other comments:

To have a  great flexibility yahooImport()  requests the download query. 
This may be not
very user friendly, but it gives a great flexibility.

Compare:
          dax1 = 
as.timeSeries(yahooImport(query="s=^GDAXI&a=0&b=1&c=2003&g=w&x=.csv")@data 
) [,"Close"]
          dax2 = get.hist.quote (instrument = "^GDAXI", 
start="2003-01-01", quote = "Close", compression = "w")

Like get.hist.quote() one can make it more user friendly very easily: [I 
show it]


getYahoo =
function(symbol = "^GDAXI", start = "2003-01-01", quote = "Close",  
aggregation = "w", class = c("timeSeries", "zoo"))
{
    # Extend it ...
    year = substring(start, 1, 4)
    month = as.character(as.integer(substring(start, 6,7))-1)
    day = substring(start, 9, 10)
    query = paste("s=", symbol, "&a=", month, "&b=", day, "&c=",
        year, "&g=", aggregation, "&x=.csv", sep = "")    
    X = yahooImport(query="s=^GDAXI&a=0&b=1&c=2003&g=w&x=.csv")@data
    ans = as.timeSeries(X)[, quote]
    if (class[1] == "zoo")  ans = as.zoo(ans)
    ans
}

also use:
as.zoo.timeSeries = function(x, ...)  zoo(seriesData(x), 
as.character(seriesPositions(x))) }


Then you can use any code proposal ...

The download results either in a timeSeries object or optionally in a 
zoo Object         
 
Convert zoo to timeSeries ...

as.timeSeries.zoo = function(x, ...) {
    # make as.timeSeries() generic in Rmetrics ... (Still to Improve it)
    timeSeries(as.matrix(x), attr(x, "index"), myFincenter = "GMT")
}

Why Rmetrics uses mergeSeries, diffSeries, logSeries instead of the
generic functions merge, diff, log, etc. is for compatibility reasons 
with SPlus/Finmetrics.
With the next version of Rmetrics I will add the generic functions for 
the Rmetrics
timeDate and timeSeries objects.


regards Diethelm

>>Thanks Diethelm
>>
>>    
>>
>>>      http://www.mayin.org/ajayshah/KB/R/html/o6.html
>>>      http://www.mayin.org/ajayshah/KB/R/html/o5.html
>>>
>>> 
>>>
>>>      
>>>
>>_______________________________________________
>>R-sig-finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
>>    
>>
>
>  
>


From Achim.Zeileis at wu-wien.ac.at  Fri Apr  7 17:40:43 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 7 Apr 2006 17:40:43 +0200
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <44367A1F.1000807@itp.phys.ethz.ch>
References: <4434FE7E.2030509@econinfo.de> <4434F728.5080103@itp.phys.ethz.ch>
	<20060407050313.GR281@lubyanka.local>
	<44362BF7.4020101@itp.phys.ethz.ch>
	<20060407142949.45001447.Achim.Zeileis@wu-wien.ac.at>
	<44367A1F.1000807@itp.phys.ethz.ch>
Message-ID: <20060407174043.15db198c.Achim.Zeileis@wu-wien.ac.at>

On Fri, 07 Apr 2006 16:41:35 +0200 Diethelm Wuertz wrote:

> Just one remark to this: When I was a student I hated it when a
> teacher said, "it is (very) easy to show that", or "it is (much)
> easier to do" and it was never shown or done ...

Ajay posted a link which did basically the same thing albeit not with
those two instruments.

> When this is the case, why one  don't show it or do it? The student
> will be happy.
> Then I think *Come on!!!*  is neither a rude nor an unfair reaction
> nor the signal for a competetion,

My impression was that the combination of implying that the posted link
was not proof enough and that it needs to be done in less than 10 lines
conveyed an atmosphere of two rival approaches. Also Ajay's first reply
was a surely a bit sloppy and could have been interpreted to be
somewhat tendential.

Hence, I tried to stop such discussions from the very start. Sorry if I
was very direct in this...

Thanks also for the other remarks. I think we'll also have time
to discuss all of this at useR! in Vienna.

Best,
Z


From bbands at gmail.com  Fri Apr  7 17:59:20 2006
From: bbands at gmail.com (BBands)
Date: Fri, 7 Apr 2006 08:59:20 -0700
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <20060407142949.45001447.Achim.Zeileis@wu-wien.ac.at>
References: <4434FE7E.2030509@econinfo.de> <4434F728.5080103@itp.phys.ethz.ch>
	<20060407050313.GR281@lubyanka.local>
	<44362BF7.4020101@itp.phys.ethz.ch>
	<20060407142949.45001447.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <6e8360ad0604070859t7dd69f6bv8146e597719d659a@mail.gmail.com>

On 4/7/06, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> Could we please *not* have discussions like this? This is not a
> competition (at least IMHO).

Discussions like this are extraordinarily valuable. The competition of
ideas in the market place is the mechanism of progress.

     jab
--
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From Achim.Zeileis at wu-wien.ac.at  Fri Apr  7 18:14:46 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 7 Apr 2006 18:14:46 +0200
Subject: [R-sig-finance] Using data from yahooImport (fBasics)
In-Reply-To: <6e8360ad0604070859t7dd69f6bv8146e597719d659a@mail.gmail.com>
References: <4434FE7E.2030509@econinfo.de> <4434F728.5080103@itp.phys.ethz.ch>
	<20060407050313.GR281@lubyanka.local>
	<44362BF7.4020101@itp.phys.ethz.ch>
	<20060407142949.45001447.Achim.Zeileis@wu-wien.ac.at>
	<6e8360ad0604070859t7dd69f6bv8146e597719d659a@mail.gmail.com>
Message-ID: <20060407181446.5645699a.Achim.Zeileis@wu-wien.ac.at>

On Fri, 7 Apr 2006 08:59:20 -0700 BBands wrote:

> On 4/7/06, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> > Could we please *not* have discussions like this? This is not a
> > competition (at least IMHO).
> 
> Discussions like this are extraordinarily valuable. The competition of
> ideas in the market place is the mechanism of progress.

Of course. I've had off-list discussions with almost all people
involved in this thread (sometimes just one-to-one, sometimes in a
larger group) and my understanding is that essentially everyone is
d'accord that we've got to find a way to get the best of n worlds
(where is at least 2).
And if you want to team up in an effort, it is IMO easier to create an
atmosphere of doing things together rather than against each other.

Furthermore, it will be much more relaxed to discuss this over a beer
in a Viennese pub during useR! :-)

Best,
Z

>      jab
> --
> John Bollinger, CFA, CMT
> www.BollingerBands.com
> 
> If you advance far enough, you arrive at the beginning.
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From kriskumar at earthlink.net  Sat Apr  8 06:00:18 2006
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Sat, 08 Apr 2006 00:00:18 -0400
Subject: [R-sig-finance] using yahoo and other data to calculate CAPM and FF
 betas]
Message-ID: <44373552.2040106@earthlink.net>

>- Further note that there's also a `young' R-SIG-robust mailing
>  list with quite a few "robustniks" subscribed -- some of who
>  do not read other R-lists AFAIK.
>
>Martin Maechler, ETH Zurich
>
>  
>
Thanks Martin for the pointer and the new mailing list...it is 
surprising that robust methods aren't more popular in finance.

Dirk wrote:

>>Optimization'. Google'ing for 'doug martin fama french robust' leads to a few
>>pages at Insightful and UW.

I found the following through Google's  Cache  http://tinyurl.com/l3c7y 
But I think the paper appeared in FAJ or something simillar.


Best,
Krishna


From neileastep at gmail.com  Sat Apr  8 14:05:20 2006
From: neileastep at gmail.com (Neil Eastep)
Date: Sat, 8 Apr 2006 21:05:20 +0900
Subject: [R-sig-finance] option prices
Message-ID: <ce4d86730604080505r41a8c050p8619595cb8bfbeec@mail.gmail.com>

Is there a way to download options prices (or better yet, implied
volatility) using Rmetrics or another R package?  Neil.


From patrick.henaff at kermatheano.com  Mon Apr 10 01:56:13 2006
From: patrick.henaff at kermatheano.com (=?iso-8859-1?Q?P._H=E9naff?=)
Date: Sun, 9 Apr 2006 18:56:13 -0500
Subject: [R-sig-finance] Option prices
References: <mailman.11.1144576804.9610.r-sig-finance@stat.math.ethz.ch>
Message-ID: <000e01c65c31$303e73f0$4501a8c0@DDG0MG21>

The NYMEX publishes daily settlement prices for listed options on WTI, 
NatGas, Heating Oil, Unleaded Gasoline, plus quotes on various spread 
options. See this link for option quotes. There is a similar file for 
futures prices.

http://www.newyorkmerc.com/futures/inno.txt

Hope this helps,

Patrick Henaff


From jgalt70 at yahoo.com  Mon Apr 10 16:49:01 2006
From: jgalt70 at yahoo.com (Andrew West)
Date: Mon, 10 Apr 2006 07:49:01 -0700 (PDT)
Subject: [R-sig-finance] using yahoo and other data to calculate CAPM
	and FF betas
In-Reply-To: <4435DC0A.3030806@earthlink.net>
Message-ID: <20060410144901.73558.qmail@web60319.mail.yahoo.com>

Incidentally, one of the functions I attached,
getrffBeta, uses the robust package WLE to calculate
3-factor coefficients. Even if one believes the return
differentials accruing to value-growth and
largecap-smallcap spreads are an anomaly erased
through robust statistics, that does not mean that one
could not include those factors to get a better
calculation of true market beta, does it? You could
take the 3-factor regression output, assign zero
premiums to size and value factors, and use the market
beta that has already controlled for size and value
effects, right?

For most of the companies I've looked at, the AIC on 3
factor models are a lot higher than CAPM models. You
can see it for yourself graphically by running the
getrffBeta function on various stocks. For example,
using CAPM, some of the dramatic swings in the beta of
low-tech stocks (falling dramatically during the
internet surge) may have been more due to value-growth
return trends than a true change in market beta. The 3
factor model thus seems to result in more stable
market beta estimates than the CAPM model.

Regards,
Andrew
--- Krishna Kumar <kriskumar at earthlink.net> wrote:

> Gabor Grothendieck wrote:
> 
> >On 4/6/06, Andrew West <jgalt70 at yahoo.com> wrote:
> >  
> >
> >>I haven't been able to figure
> >>out how to prevent the function from crashing when
> one
> >>of the companies in the list has a late start or
> >>missing data. Aligning multiple time series into a
> >>panel data dataframe is tough for non-programmers
> like
> >>me!
> >>    
> >>
> >
> >If t1 and t2 are two ts class time series or two
> zoo series
> >then cbind(t1, t2) will create a multivariate
> series (2 columns)
> >In the case of zoo, merge(t1, t2) will also work.
> >
> >na.omit(cbind(t1, t2)) or na.omit(merge(t1,t2))
> >will eliminate rows that have any NAs in the case
> of zoo series.
> >
> >_______________________________________________
> >R-sig-finance at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
> >  
> >
> 
> To add to Gabor's suggestion you could do the
> following to get an 
> approximated series..
> so if mydata is a vector with "NA" 's then doing
> 
>  >mydata<-approx(mydata,xout=seq(along=mydata))$y
> 
> this would approximate the series and then you can
> do a ts.union
> 
> Also there was a very interesting paper that showed
> that the Fama-French 
> effect was not really a anamoly when you estimate
> using
>  Robust regression instead of OLS. I can't remember
> the reference but it 
> was Doug Martin and someone else from UW ...
> R has some nice facilities with rrcov to do the
> robust regressions!!
> 
> Best,
> Krishna
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From ggrothendieck at gmail.com  Mon Apr 10 17:12:16 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Apr 2006 11:12:16 -0400
Subject: [R-sig-finance] using yahoo and other data to calculate CAPM
	and FF betas
In-Reply-To: <971536df0604062053l3140e028s4cf965d33c528e2@mail.gmail.com>
References: <20060406135926.811.qmail@web60325.mail.yahoo.com>
	<971536df0604061035i21820680mc6a0ad6c3f47fd83@mail.gmail.com>
	<4435DC0A.3030806@earthlink.net>
	<971536df0604062053l3140e028s4cf965d33c528e2@mail.gmail.com>
Message-ID: <971536df0604100812j69aa0a34hb617a90c8a3ec31e@mail.gmail.com>

ince I wrote the post below a new version of zoo came
out.  One addition is the na.trim function which adds a fifth NA
handling routine, na.trim, to the prior four illustrated in my last
post.  It trims leading and/or trailing NAs but leaves others
as is:

> library(zoo)
> zz <- zoo(c(NA,1,NA,3,NA,5,NA))
> na.trim(zz)
 2  3  4  5  6
 1 NA  3 NA  5

On 4/6/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> I mentioned omitting the missing values via na.omit and the poster
> below mentioned using linear approximation.  Note that
> the zoo package actually has 4 missing value routines:
>
> na.omit - omit missing values
> na.approx - replace missing values with linear approximations
> na.locf - replace missing values with the last occurrernce carried forward
> na.contiguous - remove all but a contiguous stretch of non-missing values
>
> > library(zoo)
> > z <- zoo(c(1,NA,3,NA,5))
> > na.omit(z)
> 1 3 5
> 1 3 5
> > na.locf(z)
> 1 2 3 4 5
> 1 1 3 3 5
> > na.approx(z)
> 1 2 3 4 5
> 1 2 3 4 5
> > na.contiguous(z)
> 3
> 3
>
>
> On 4/6/06, Krishna Kumar <kriskumar at earthlink.net> wrote:
> > Gabor Grothendieck wrote:
> >
> > >On 4/6/06, Andrew West <jgalt70 at yahoo.com> wrote:
> > >
> > >
> > >>I haven't been able to figure
> > >>out how to prevent the function from crashing when one
> > >>of the companies in the list has a late start or
> > >>missing data. Aligning multiple time series into a
> > >>panel data dataframe is tough for non-programmers like
> > >>me!
> > >>
> > >>
> > >
> > >If t1 and t2 are two ts class time series or two zoo series
> > >then cbind(t1, t2) will create a multivariate series (2 columns)
> > >In the case of zoo, merge(t1, t2) will also work.
> > >
> > >na.omit(cbind(t1, t2)) or na.omit(merge(t1,t2))
> > >will eliminate rows that have any NAs in the case of zoo series.
> > >
> > >_______________________________________________
> > >R-sig-finance at stat.math.ethz.ch mailing list
> > >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > >
> > >
> > >
> >
> > To add to Gabor's suggestion you could do the following to get an
> > approximated series..
> > so if mydata is a vector with "NA" 's then doing
> >
> >  >mydata<-approx(mydata,xout=seq(along=mydata))$y
> >
> > this would approximate the series and then you can do a ts.union
> >
> > Also there was a very interesting paper that showed that the Fama-French
> > effect was not really a anamoly when you estimate using
> >  Robust regression instead of OLS. I can't remember the reference but it
> > was Doug Martin and someone else from UW ...
> > R has some nice facilities with rrcov to do the robust regressions!!
> >
> > Best,
> > Krishna
> >
> >
> >
> >
>


From mvyver at gmail.com  Thu Apr 13 09:23:54 2006
From: mvyver at gmail.com (Mark Van De Vyver)
Date: Thu, 13 Apr 2006 17:23:54 +1000
Subject: [R-sig-finance] cond.dist conflict between garchSim and garchSpec
Message-ID: <389c43e40604130023w50b86b40p488267a190270526@mail.gmail.com>

Hi,
Thanks for all the great effort put into Rmetrics.

I apprciate the Rmetrics package is a work-in-progress, so the
following observation may be redundant.
Following the Journal of Statistical Software article "Parameter
estimation of ARMA...."
I notived that garchSim does not accept results of garchSpec as input.
 I get the follwoing error:

"Error in h^deltainv * z : non-numeric argument to binary operator"

It turns out that in garchSpec cond.dist="dnorm" is OK, but garchSim
requires/uses cond.dist="rnorm" and this generates the error message
above.
It would be nice if both functions accepted both or just one, but it's
not a show stopper.

Hope that helps.  Thanks again.

Regards
Mark

--
Mark Van De Vyver, PhD
--------------------------------------------------
My research is available from my SSRN Author page:
http://ssrn.com/author=36577
--------------------------------------------------
Finance Discipline
School of Business
The University of Sydney
Sydney NSW 2006
Australia

Telephone: +61 2 9351-6452
Fax: +61 2 9351-6461


From montezumasrevenge at gmail.com  Sun Apr 16 14:11:02 2006
From: montezumasrevenge at gmail.com (Monty B. )
Date: Sun, 16 Apr 2006 14:11:02 +0200
Subject: [R-sig-finance] fSeries_221.10065 and garchFit+sqp makes R lock up
Message-ID: <ae75f2d40604160511k5a409dd5s4dc2a212d6136bf5@mail.gmail.com>

Dear all,

I am fitting garch models to a sliding window of observations of the
USD/NOK exchange rate. I've been provided with the Ox/G at RCH package,
but I am not entirely happy with it's scriptability, so I thought I
would give fSeries a go. The package seems to work well for some
series, but for others, it locks up R.

This code:

library(fSeries)
y <- read.table("fGARCH_crash.csv")
fg <- garchFit(formula.mean =~ arma(0,0), formula.var =~ garch(1,1),
                 cond.dist = "dnorm", y, trace=T, title="USD vs NOK")

and the file:

http://us.f13.yahoofs.com/bc/44422dee_a419/bc/My+Documents/fGARCH_crash.csv?bfcOjQEBfGO1k9on

makes R crash giving no output when the default settings are used.
Changing the algorithm to "nlminb" seems to provide estimates. BUT, I
am a bit skeptical about changing defaults when I do not know what the
difference between sqp and nlminb is.

Any suggestions? Should I use the non-default optimization? Can anyone
refer me to literature on what the difference is? Will the parameter
estimates be of worse quality?

BTW: I am using R for windows 2.2.1. I have tested both the standard
2.2.1 and the patched 2.2.1 versions with this code.

Thanks for any input,

cheers,

Monty


From montezumasrevenge at gmail.com  Sun Apr 16 14:39:19 2006
From: montezumasrevenge at gmail.com (Monty B. )
Date: Sun, 16 Apr 2006 14:39:19 +0200
Subject: [R-sig-finance] fSeries_221.10065 and garchFit+sqp makes R lock
	up
In-Reply-To: <ae75f2d40604160511k5a409dd5s4dc2a212d6136bf5@mail.gmail.com>
References: <ae75f2d40604160511k5a409dd5s4dc2a212d6136bf5@mail.gmail.com>
Message-ID: <ae75f2d40604160539o6c0443ald00ab5d452ed201b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060416/72f9b43a/attachment.pl 

From wuertz at itp.phys.ethz.ch  Sun Apr 16 15:46:04 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 16 Apr 2006 15:46:04 +0200
Subject: [R-sig-finance] fSeries_221.10065 and garchFit+sqp makes R lock
 up
In-Reply-To: <ae75f2d40604160539o6c0443ald00ab5d452ed201b@mail.gmail.com>
References: <ae75f2d40604160511k5a409dd5s4dc2a212d6136bf5@mail.gmail.com>
	<ae75f2d40604160539o6c0443ald00ab5d452ed201b@mail.gmail.com>
Message-ID: <44424A9C.4010305@itp.phys.ethz.ch>

GARCH-Modelling is not easy, and indeed for your dataset the default
"Sequential Quadratic Programming" solver doesn't converge. I observed
this also for some other time series. There is already an updated 
version on
the server, https://svn.r-project.org/Rmetrics/trunk/fSeries/ which uses 
improved
control parameter settings as default values. With this version there exist
no convergence problems. What can you do? Download the updated
version from the repository, or just use the alternative optimization 
"nlminb"
until the next version of "Rmetrics" becomes published.

regards Diethelm Wuertz


garchFit() # Update - Default Settings
           Estimate  Std. Error  t value   Pr(>|t|)   
mu       -0.016772    0.020792    -0.807     0.4199   
omega     0.008898    0.004055     2.194     0.0282 * 
alpha1    0.047233    0.011134     4.242   2.21e-05 ***
beta1     0.936329    0.014828    63.146    < 2e-16 ***  
Log Likelihood:
 1045.871    normalized:  1.045871
 

garchFit(algorithm = "nlminb") # Current Version
          Estimate  Std. Error    t value   Pr(>|t|)   
mu       -0.016772    0.020793    -0.807     0.4199   
omega     0.008898    0.004055     2.194     0.0282 * 
alpha1    0.047233    0.011134     4.242   2.21e-05 ***
beta1     0.936329    0.014828    63.145    < 2e-16 ***
Log Likelihood:
 1045.871    normalized:  1.045871

 
garchOxFit()
Coefficient(s):
              Value   Std.Error    t.value
Cst(M)   -0.0166990   0.0207920   -0.80315
Cst(V)    0.0089064   0.0040545    2.19670
ARCH(1)   0.0472270   0.0111270    4.24430
GARCH(1)  0.9362900   0.0148290   63.13900




Monty B. wrote:

>Thanks to Sean and Diethelm for pointing out that the link was not working.
>
>The data can be found here:
>
>http://host-a.net/getfile.php?usern=upppload&file=fGARCH_crash.csv
>(click in the yellow box to receive file)
>
>Sorry about the quirky download site. It was the best I could do right now..
>
>
>Many thanks,
>
>Monty
>
>
>On 4/16/06, Monty B. <montezumasrevenge at gmail.com> wrote:
>  
>
>>Dear all,
>>
>>I am fitting garch models to a sliding window of observations of the
>>USD/NOK exchange rate. I've been provided with the Ox/G at RCH package,
>>but I am not entirely happy with it's scriptability, so I thought I
>>would give fSeries a go. The package seems to work well for some
>>series, but for others, it locks up R.
>>
>>This code:
>>
>>library(fSeries)
>>y <- read.table("fGARCH_crash.csv")
>>fg <- garchFit(formula.mean =~ arma(0,0), formula.var =~ garch(1,1),
>>                cond.dist = "dnorm", y, trace=T, title="USD vs NOK")
>>
>>and the file:
>>
>>
>>http://us.f13.yahoofs.com/bc/44422dee_a419/bc/My+Documents/fGARCH_crash.csv?bfcOjQEBfGO1k9on
>>
>>makes R crash giving no output when the default settings are used.
>>Changing the algorithm to "nlminb" seems to provide estimates. BUT, I
>>am a bit skeptical about changing defaults when I do not know what the
>>difference between sqp and nlminb is.
>>
>>Any suggestions? Should I use the non-default optimization? Can anyone
>>refer me to literature on what the difference is? Will the parameter
>>estimates be of worse quality?
>>
>>BTW: I am using R for windows 2.2.1. I have tested both the standard
>>2.2.1 and the patched 2.2.1 versions with this code.
>>
>>Thanks for any input,
>>
>>cheers,
>>
>>Monty
>>
>>    
>>
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From montezumasrevenge at gmail.com  Sun Apr 16 18:15:35 2006
From: montezumasrevenge at gmail.com (Monty B. )
Date: Sun, 16 Apr 2006 18:15:35 +0200
Subject: [R-sig-finance] fSeries_221.10065 and garchFit+sqp makes R lock
	up
In-Reply-To: <44424A9C.4010305@itp.phys.ethz.ch>
References: <ae75f2d40604160511k5a409dd5s4dc2a212d6136bf5@mail.gmail.com>
	<ae75f2d40604160539o6c0443ald00ab5d452ed201b@mail.gmail.com>
	<44424A9C.4010305@itp.phys.ethz.ch>
Message-ID: <ae75f2d40604160915q7c4c42b0r1a0d112a7ee4b0fe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060416/6e796353/attachment.pl 

From icos.atropa at gmail.com  Tue Apr 18 07:48:31 2006
From: icos.atropa at gmail.com (icosa atropa)
Date: Mon, 17 Apr 2006 23:48:31 -0600
Subject: [R-sig-finance] Trouble with zoo + POSIXct/POSIXlt
Message-ID: <681d07c20604172248i5805d9dfoe7f96a6f6306c0d5@mail.gmail.com>

After successfully using the 'its' package, I've been trying out 'zoo'
for some of its more advanced functions.  I can coerce my its objects
into zoo objects without problem.
Yet I'm having trouble understanding the read.zoo function's errors.
It seems that my index class, POSIXct, is the problem.

I have csv files like this:
7/23/2003 4:05:51 PM,9.35,98.027
7/23/2003 4:20:51 PM,9.32,97.954

-----------------------
#First try:
>my.format = "%m/%d/%Y %I:%M:%S %p"
>x=read.zoo(myfile, sep=',', skip=30, format=my.format)
> z
             V2      V3
2003-07-23 9.35  98.027
2003-07-23 9.32  97.954
> summary(z)
Error in "row.names<-.data.frame"(`*tmp*`, value = NULL) :
        invalid 'row.names' length
#format= doesn't seem to work here, error means index not #unique?
#
#Second try,
#This works:
>x=read.table(myfile, sep=',' , skip=30,\
x[,1]=as.POSIXct(strptime(x[,1], format=my.format,\ tz='UTC+6'))
> y=zoo(x[,2:3], order.by=x[,1])
#
#but this doesn't:
#
>x=read.zoo(my.file, sep=',',skip=30,\
FUN=as.POSIXct(strptime(as.character(x), format=my.format,
tz='UTC+6')) )
Error in read.zoo(files[1], sep = ",", skip = 30, FUN =
as.POSIXct(strptime(as.character(x),  :
        couldn't find function "FUN"
---------
Any idea what the last error message means?
TIA,
christian


From Achim.Zeileis at wu-wien.ac.at  Tue Apr 18 13:04:53 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 18 Apr 2006 13:04:53 +0200 (CEST)
Subject: [R-sig-finance] Trouble with zoo + POSIXct/POSIXlt
In-Reply-To: <681d07c20604172248i5805d9dfoe7f96a6f6306c0d5@mail.gmail.com>
References: <681d07c20604172248i5805d9dfoe7f96a6f6306c0d5@mail.gmail.com>
Message-ID: <Pine.LNX.4.58.0604181254171.20929@thorin.ci.tuwien.ac.at>

On Mon, 17 Apr 2006, icosa atropa wrote:

> After successfully using the 'its' package, I've been trying out 'zoo'
> for some of its more advanced functions.  I can coerce my its objects
> into zoo objects without problem.
> Yet I'm having trouble understanding the read.zoo function's errors.
> It seems that my index class, POSIXct, is the problem.

POSIXct is fine, but you not used the arguments of read.zoo appropriately,
check ?read.zoo again and also see below.

> I have csv files like this:
> 7/23/2003 4:05:51 PM,9.35,98.027
> 7/23/2003 4:20:51 PM,9.32,97.954
>
> -----------------------
> #First try:
> >my.format = "%m/%d/%Y %I:%M:%S %p"
> >x=read.zoo(myfile, sep=',', skip=30, format=my.format)

The format argument does not do what you think it does. It is passed to
as.Date(), hence "Date" is used for the indexes, resulting in non-unique
indexes (because both rows are from the same day).

> #Second try,
> #This works:
> >x=read.table(myfile, sep=',' , skip=30,\
> x[,1]=as.POSIXct(strptime(x[,1], format=my.format,\ tz='UTC+6'))

That function is what you need:
  myfun <- function(x)
    as.POSIXct(strptime(x, format = "%m/%d/%Y %I:%M:%S %p", tz = "UTC+6"))

and then you can do

  z <- read.zoo(myfile, FUN = myfun, sep = ",", skip = 30)

the function above is then used for transforming the first column.

> > y=zoo(x[,2:3], order.by=x[,1])
> #
> #but this doesn't:
> #
> >x=read.zoo(my.file, sep=',',skip=30,\
> FUN=as.POSIXct(strptime(as.character(x), format=my.format,
> tz='UTC+6')) )
> Error in read.zoo(files[1], sep = ",", skip = 30, FUN =
> as.POSIXct(strptime(as.character(x),  :
>         couldn't find function "FUN"
> ---------
> Any idea what the last error message means?

You have not passed a function to FUN, but a function call.

Best,
Z


From ggrothendieck at gmail.com  Tue Apr 18 13:39:27 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 18 Apr 2006 07:39:27 -0400
Subject: [R-sig-finance] Trouble with zoo + POSIXct/POSIXlt
In-Reply-To: <681d07c20604172248i5805d9dfoe7f96a6f6306c0d5@mail.gmail.com>
References: <681d07c20604172248i5805d9dfoe7f96a6f6306c0d5@mail.gmail.com>
Message-ID: <971536df0604180439v7bbc592dw15a78349a2b620b7@mail.gmail.com>

Don't know what z is but if its supposed to be x then, yes,
you have created an invalid zoo object since the index entries
must be unique.  The second try does not make sense since
x[,1]= is not a valid argument to read.zoo and the third one
is erroneous as it specifies an expression whereas it should
specify a function.

I think you want:

read.zoo(myfile, sep = ',' , FUN = function(x) as.POSIXct(strptime(x,
my.format)))

or read it in using the its package and then use xx <- as.zoo(x) where x is
the its object.

On 4/18/06, icosa atropa <icos.atropa at gmail.com> wrote:
> After successfully using the 'its' package, I've been trying out 'zoo'
> for some of its more advanced functions.  I can coerce my its objects
> into zoo objects without problem.
> Yet I'm having trouble understanding the read.zoo function's errors.
> It seems that my index class, POSIXct, is the problem.
>
> I have csv files like this:
> 7/23/2003 4:05:51 PM,9.35,98.027
> 7/23/2003 4:20:51 PM,9.32,97.954
>
> -----------------------
> #First try:
> >my.format = "%m/%d/%Y %I:%M:%S %p"
> >x=read.zoo(myfile, sep=',', skip=30, format=my.format)
> > z
>             V2      V3
> 2003-07-23 9.35  98.027
> 2003-07-23 9.32  97.954
> > summary(z)
> Error in "row.names<-.data.frame"(`*tmp*`, value = NULL) :
>        invalid 'row.names' length
> #format= doesn't seem to work here, error means index not #unique?
> #
> #Second try,
> #This works:
> >x=read.table(myfile, sep=',' , skip=30,\
> x[,1]=as.POSIXct(strptime(x[,1], format=my.format,\ tz='UTC+6'))
> > y=zoo(x[,2:3], order.by=x[,1])
> #
> #but this doesn't:
> #
> >x=read.zoo(my.file, sep=',',skip=30,\
> FUN=as.POSIXct(strptime(as.character(x), format=my.format,
> tz='UTC+6')) )
> Error in read.zoo(files[1], sep = ",", skip = 30, FUN =
> as.POSIXct(strptime(as.character(x),  :
>        couldn't find function "FUN"
> ---------
> Any idea what the last error message means?
> TIA,
> christian
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From lorenzo.isella at gmail.com  Mon Apr 24 00:10:41 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Mon, 24 Apr 2006 00:10:41 +0200
Subject: [R-sig-finance] Distribution Fitting
Message-ID: <a18f85580604231510n4cefcae4h9686d233073bf199@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060424/5ee8e8a2/attachment.pl 

From edd at debian.org  Mon Apr 24 00:40:43 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 23 Apr 2006 17:40:43 -0500
Subject: [R-sig-finance] Distribution Fitting
In-Reply-To: <a18f85580604231510n4cefcae4h9686d233073bf199@mail.gmail.com>
References: <a18f85580604231510n4cefcae4h9686d233073bf199@mail.gmail.com>
Message-ID: <17484.619.752885.435199@basebud.nulle.part>


On 24 April 2006 at 00:10, Lorenzo Isella wrote:
|  Dear All,
| I am experiencing some problems in fitting a trimodal distribution (which
| should be thought as a sum of three Gaussian distributions) using the nls
| function for nonlinear fittings.
| As a test, just consider the very simple code:
| 
| rm(list=ls());
| mydata<-rnorm(10000,0,4);
| mydens<-density(mydata,kernel="gaussian");
| y1<-mydens$y;
| x1<-mydens$x;
| myfit<-nls(y1~A*exp(-x1^2/sig));
| 
| 
| which I use to get the empirical density (as I would from real experimental
| data) and test it against a Gaussian ansatz.
| Well, either R always crashes for a segmentation fault and I have to restart
| it manually or I get this output:
| 
| Error in match.call(definition, call, expand.dots) :
| '.Primitiv???i???d???????????????????????????????????????...' is not a function
| 
| Am I missing the obvious or is there some bug in my R build?

Are you aware of fitdistr() in MASS?

> library(MASS)
> mydata<-rnorm(10000,0,4)
> fitdistr(mydata, "normal")
        mean                  sd            
  -0.0055755191185632    3.9956609772436904 
 ( 0.0399566097724369) ( 0.0282535897233148)
> 

fitdistr() fits a bunch of distribution. I can't recall whether I used this,
or one of the mixed distribution packages from CRAN when I was doing some
work on mixtures for fat tails.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From kriskumar at earthlink.net  Mon Apr 24 05:50:21 2006
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Sun, 23 Apr 2006 23:50:21 -0400
Subject: [R-sig-finance] Distribution Fitting
In-Reply-To: <17484.619.752885.435199@basebud.nulle.part>
References: <a18f85580604231510n4cefcae4h9686d233073bf199@mail.gmail.com>
	<17484.619.752885.435199@basebud.nulle.part>
Message-ID: <444C4AFD.3080306@earthlink.net>

mixdist is what you want it works very well and gets you nice gaussian 
mixtures.

The issue is with calibration of these mixtures and I would like to be 
shown otherwise..or just wrong!

But if you have three distribution there could be two different 
combination of distribution parameters that can fit the data equally well.
This becomes a problem of how to divvy up say the variance between two 
of the gaussians..and how good an optimizer you have..

Krishna




Dirk Eddelbuettel wrote:

>On 24 April 2006 at 00:10, Lorenzo Isella wrote:
>|  Dear All,
>| I am experiencing some problems in fitting a trimodal distribution (which
>| should be thought as a sum of three Gaussian distributions) using the nls
>| function for nonlinear fittings.
>| As a test, just consider the very simple code:
>| 
>| rm(list=ls());
>| mydata<-rnorm(10000,0,4);
>| mydens<-density(mydata,kernel="gaussian");
>| y1<-mydens$y;
>| x1<-mydens$x;
>| myfit<-nls(y1~A*exp(-x1^2/sig));
>| 
>| 
>| which I use to get the empirical density (as I would from real experimental
>| data) and test it against a Gaussian ansatz.
>| Well, either R always crashes for a segmentation fault and I have to restart
>| it manually or I get this output:
>| 
>| Error in match.call(definition, call, expand.dots) :
>| '.Primitiv???i???d???????????????????????????????????????...' is not a function
>| 
>| Am I missing the obvious or is there some bug in my R build?
>
>Are you aware of fitdistr() in MASS?
>
>  
>
>>library(MASS)
>>mydata<-rnorm(10000,0,4)
>>fitdistr(mydata, "normal")
>>    
>>
>        mean                  sd            
>  -0.0055755191185632    3.9956609772436904 
> ( 0.0399566097724369) ( 0.0282535897233148)
>  
>
>
>fitdistr() fits a bunch of distribution. I can't recall whether I used this,
>or one of the mixed distribution packages from CRAN when I was doing some
>work on mixtures for fat tails.
>
>Dirk
>
>  
>


From maechler at stat.math.ethz.ch  Mon Apr 24 15:08:04 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 24 Apr 2006 15:08:04 +0200
Subject: [R-sig-finance] Distribution Fitting
In-Reply-To: <a18f85580604231510n4cefcae4h9686d233073bf199@mail.gmail.com>
References: <a18f85580604231510n4cefcae4h9686d233073bf199@mail.gmail.com>
Message-ID: <17484.52660.190645.384993@stat.math.ethz.ch>

>>>>> "Lorenzo" == Lorenzo Isella <lorenzo.isella at gmail.com>
>>>>>     on Mon, 24 Apr 2006 00:10:41 +0200 writes:

    Lorenzo> Dear All,
    Lorenzo> I am experiencing some problems in fitting a trimodal distribution (which
    Lorenzo> should be thought as a sum of three Gaussian distributions) using the nls
    Lorenzo> function for nonlinear fittings.
    Lorenzo> As a test, just consider the very simple code:

    Lorenzo> rm(list=ls());
    Lorenzo> mydata<-rnorm(10000,0,4);
    Lorenzo> mydens<-density(mydata,kernel="gaussian");
    Lorenzo> y1<-mydens$y;
    Lorenzo> x1<-mydens$x;
    Lorenzo> myfit<-nls(y1~A*exp(-x1^2/sig));

(it's slightly inefficient and completely unnecessary to append
 an empty statement to every line -- which you 
 do incidentally if you add superfluous ';' at end of lines )


    Lorenzo> which I use to get the empirical density (as I would from real experimental
    Lorenzo> data) and test it against a Gaussian ansatz.
    Lorenzo> Well, either R always crashes for a segmentation fault and I have to restart
    Lorenzo> it manually or I get this output:

    Lorenzo> Error in match.call(definition, call, expand.dots) :
    Lorenzo> '.Primitiv???i???d???????????????????????????????????????...' is not a function

I cannot confirm any segmentation fault (and if you really get
them, indeed your R *installation* is buggy (or outdated)), but
indeed, you've revealed a buglet in nls() - which is still
present in R 2.3.0 which has been released a few hours ago.
Thank you for reporting it as a reproducible example!

I'm going to try to fix the bug.

    Lorenzo> Am I missing the obvious or is there some bug in my R build?

As others have said (implicitly): you're missing the facts

-  that it's rather bad idea to fit "data" to a density estimate.
-  if your density has a closed from, you should rather use
   maximum likelihood, typically most easily  via MASS::fitdistr

Martin


From lorenzo.isella at gmail.com  Mon Apr 24 17:19:33 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Mon, 24 Apr 2006 17:19:33 +0200
Subject: [R-sig-finance] Distribution Fitting
In-Reply-To: <17484.52660.190645.384993@stat.math.ethz.ch>
References: <a18f85580604231510n4cefcae4h9686d233073bf199@mail.gmail.com>
	<17484.52660.190645.384993@stat.math.ethz.ch>
Message-ID: <1145891973.11958.26.camel@quez.DomName>

On Mon, 2006-04-24 at 15:08 +0200, Martin Maechler wrote:
> >>>>> "Lorenzo" == Lorenzo Isella <lorenzo.isella at gmail.com>
> >>>>>     on Mon, 24 Apr 2006 00:10:41 +0200 writes:
> 
>     Lorenzo> Dear All,
>     Lorenzo> I am experiencing some problems in fitting a trimodal distribution (which
>     Lorenzo> should be thought as a sum of three Gaussian distributions) using the nls
>     Lorenzo> function for nonlinear fittings.
>     Lorenzo> As a test, just consider the very simple code:
> 
>     Lorenzo> rm(list=ls());
>     Lorenzo> mydata<-rnorm(10000,0,4);
>     Lorenzo> mydens<-density(mydata,kernel="gaussian");
>     Lorenzo> y1<-mydens$y;
>     Lorenzo> x1<-mydens$x;
>     Lorenzo> myfit<-nls(y1~A*exp(-x1^2/sig));
> 
> (it's slightly inefficient and completely unnecessary to append
>  an empty statement to every line -- which you 
>  do incidentally if you add superfluous ';' at end of lines )
> 
> 
>     Lorenzo> which I use to get the empirical density (as I would from real experimental
>     Lorenzo> data) and test it against a Gaussian ansatz.
>     Lorenzo> Well, either R always crashes for a segmentation fault and I have to restart
>     Lorenzo> it manually or I get this output:
> 
>     Lorenzo> Error in match.call(definition, call, expand.dots) :
>     Lorenzo> '.Primitiv?i?d?????????????...' is not a function
> 
> I cannot confirm any segmentation fault (and if you really get
> them, indeed your R *installation* is buggy (or outdated)), but
> indeed, you've revealed a buglet in nls() - which is still
> present in R 2.3.0 which has been released a few hours ago.
> Thank you for reporting it as a reproducible example!
> 
> I'm going to try to fix the bug.
> 
>     Lorenzo> Am I missing the obvious or is there some bug in my R build?
> 
> As others have said (implicitly): you're missing the facts
> 
> -  that it's rather bad idea to fit "data" to a density estimate.
> -  if your density has a closed from, you should rather use
>    maximum likelihood, typically most easily  via MASS::fitdistr
> 
> Martin

Thanks everybody for their advice.
Granted that I had better use fitdistr for my particular problem, how do
I feed a particular density function of my own choice into it?
E.g., consider the case of Gaussian-distributed data and suppose there
is no "normal" argument for the fitdistr().

mysample<-rnorm(10000,2,4)

You want to model the data as drawn from the probability distribution
A*exp(-(x-mu)^2/sig), where x is an experimental data and have A, mu and
sigma as parameters to fit.
I read the help(fitdistr) page, but there is not an example with a
user-provided probability density and I did not get very far with my
attempts.
Best regards and thanks in advance for your help

Lorenzo


From lorenzo.isella at gmail.com  Tue Apr 25 00:27:49 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Tue, 25 Apr 2006 00:27:49 +0200
Subject: [R-sig-finance] Distribution Fitting and fitdistr()
In-Reply-To: <000801c667ac$fcc89f00$0ac0a8c0@MightyMini>
References: <000801c667ac$fcc89f00$0ac0a8c0@MightyMini>
Message-ID: <1145917669.11958.56.camel@quez.DomName>

Dear All,
I think I understood how to use the fitdistr() function to fit random
draws to a certain distribution.
I first tested it on a unimodal Gaussian distribution:

rm(list=ls())
library(MASS)
set.seed(123)
mydata<-rnorm(10000,2.0,4.0)
gauden<-function(x,mu,sig)
{
1/(sqrt(2*pi)*sig)*exp(-1/(2*sig^2)*(x-mu)^2)
}
myfit<-fitdistr(mydata,gauden,start=list(mu=5.0,sig=5.0),upper=c(20,20))
myfit2<-fitdistr(mydata,"normal")

seeing the (expected) agreement between myfit and myfit2.

Then I tested it on a set of draws from a mixture made up of three
Gaussian distributions.
This time I used 9 fitting parameters:
- 3 weights N1,N2,N3 for the components of the Gaussian mixture
- 3 standard deviations sig1,sig2,sig3
- 3 means mu1,mu2,mu3

Here is the code I used:

rm(list=ls()) 
library(MASS)
set.seed(123)
NN<-10000
three<-3 #number of gaussian sequences I want to generate
mysample<-matrix(nrow=NN,ncol=three)
sdvec<-seq(1:three)
sdvec[1]<-4
sdvec[2]<-3
sdvec[3]<-3.5
meanvec<-seq(1:three)
meanvec[1]<-20
meanvec[2]<-50
meanvec[3]<-70

for (i in 1:three)
{
 mysample[ ,i] <-rnorm(NN,meanvec[i],sdvec[i])
}
dim(mysample)<-c(3*NN,1)
plot(density(mysample,kern="gaussian"),lwd=2,col=300)

mydistr<-function(x,mu1,sig1,N1,mu2,sig2,N2,mu3,sig3,N3)
{
N1/(sqrt(2*pi)*sig1)*exp(-1/(2*sig1^2)*(x-mu1)^2)+
N2/(sqrt(2*pi)*sig2)*exp(-1/(2*sig2^2)*(x-mu2)^2)+
N3/(sqrt(2*pi)*sig3)*exp(-1/(2*sig3^2)*(x-mu3)^2)
}
myfit<-fitdistr(mysample,mydistr,start=list(sig1=2,sig2=2,sig3=2,
N1=0.3,N2=0.3,N3=0.4,mu1=25,mu2=55,mu3=76))

But this time the fitdistr() is not able to carry out the optimization
and the warnings do not tell (me at least) that much.
Am I mistaking something?  Furthermore, how can I be sure that the
optimization will lead to weights such that N1+N2+N3=1?
I have not been able to try the mixdist package or any other tool, but
any suggestion about how to fit this threemodal distribution (which
should not be rocket science for someone more experienced than myself)
is welcome.
Best Regards

Lorenzo


From kriskumar at earthlink.net  Tue Apr 25 05:09:09 2006
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Mon, 24 Apr 2006 23:09:09 -0400
Subject: [R-sig-finance] Distribution Fitting and fitdistr()
In-Reply-To: <1145917669.11958.56.camel@quez.DomName>
References: <000801c667ac$fcc89f00$0ac0a8c0@MightyMini>
	<1145917669.11958.56.camel@quez.DomName>
Message-ID: <444D92D5.7030206@earthlink.net>

Lorenzo Isella wrote:

>Dear All,
>I think I understood how to use the fitdistr() function to fit random
>draws to a certain distribution.
>I first tested it on a unimodal Gaussian distribution:
>
>  
>

This only works for distributions that are specified below
 >>>
Distributions '"beta"', '"cauchy"', '"chi-squared"',
          '"exponential"', '"f"', '"gamma"', '"geometric"',
          '"log-normal"', '"lognormal"', '"logistic"', '"negative
          binomial"', '"normal"', '"Poisson"', '"t"' and '"weibull"'
          are recognised, case being ignored.
 >>>>>>>>

>Then I tested it on a set of draws from a mixture made up of three
>Gaussian distributions.
>This time I used 9 fitting parameters:
>- 3 weights N1,N2,N3 for the components of the Gaussian mixture
>- 3 standard deviations sig1,sig2,sig3
>- 3 means mu1,mu2,mu3
>
>  
>

The package nor1Mix has density , distribution and rng for mixtures but 
no fitting so here is a  fitting function..

mydistr<-function(x,mu,sig,wt)
{
sum(wt*dnorm(x,mu,sig))
}

mydistrmle<-function(x, y = x) {

#mu goes from x1:3 sig goes x 4:6 and wt goes x 7:9
        f = -sum(log(mydistr(y, c(x[1],x[2],x[3]),     
c(x[4],x[5],x[6]),     c(x[7],x[8],x[9])  )))
        cat("\n Function value:  ", -f)
        cat("\n  Estimated parameters:       ", 
c(x[1],x[2],x[3]),c(x[4],x[5],x[6]),c(x[7],x[8],x[9]), "\n")

        f
    }
   


#generate 1000 samples from norMix  mixture MW.nm2
x<-rnorMix(1000,MW.nm2)
hist(rnorMix(1000,MW.nm2))
#  Give it a starting values
mu<-c(-0.2,0.4,0.9)
sigma<-c(0.2, 0.2, 0.6)
wt<-c(0.3,0.3,0.2)
r<-optim(c(mu,sigma,wt),mydistrmle,control=list(maxit=50000))
print(matrix(r$par,3,3))
print(MW.nm2)

You will have to add additional constraints on the "wt" etc.  ( see 
?constrOptim)

Further I don't think there is a guaranteed unique decomposition of 
variance between the three gaussians. Just try different starting values
and also is there a reason to stop at 3 gaussians ?..

Krishna


From lorenzo.isella at gmail.com  Tue Apr 25 12:21:02 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Tue, 25 Apr 2006 12:21:02 +0200
Subject: [R-sig-finance] Distribution Fitting and fitdistr()
In-Reply-To: <444D92D5.7030206@earthlink.net>
References: <000801c667ac$fcc89f00$0ac0a8c0@MightyMini>
	<1145917669.11958.56.camel@quez.DomName>
	<444D92D5.7030206@earthlink.net>
Message-ID: <1145960462.4997.8.camel@quez.DomName>

On Mon, 2006-04-24 at 23:09 -0400, Krishna Kumar wrote:
> Lorenzo Isella wrote:
> 
> >Dear All,
> >I think I understood how to use the fitdistr() function to fit random
> >draws to a certain distribution.
> >I first tested it on a unimodal Gaussian distribution:
> >
> >  
> >
> 
> This only works for distributions that are specified below
>  >>>
> Distributions '"beta"', '"cauchy"', '"chi-squared"',
>           '"exponential"', '"f"', '"gamma"', '"geometric"',
>           '"log-normal"', '"lognormal"', '"logistic"', '"negative
>           binomial"', '"normal"', '"Poisson"', '"t"' and '"weibull"'
>           are recognised, case being ignored.
>  >>>>>>>>
> 
> >Then I tested it on a set of draws from a mixture made up of three
> >Gaussian distributions.
> >This time I used 9 fitting parameters:
> >- 3 weights N1,N2,N3 for the components of the Gaussian mixture
> >- 3 standard deviations sig1,sig2,sig3
> >- 3 means mu1,mu2,mu3
> >
> >  
> >
> 
> The package nor1Mix has density , distribution and rng for mixtures but 
> no fitting so here is a  fitting function..
> 
> mydistr<-function(x,mu,sig,wt)
> {
> sum(wt*dnorm(x,mu,sig))
> }
> 
> mydistrmle<-function(x, y = x) {
> 
> #mu goes from x1:3 sig goes x 4:6 and wt goes x 7:9
>         f = -sum(log(mydistr(y, c(x[1],x[2],x[3]),     
> c(x[4],x[5],x[6]),     c(x[7],x[8],x[9])  )))
>         cat("\n Function value:  ", -f)
>         cat("\n  Estimated parameters:       ", 
> c(x[1],x[2],x[3]),c(x[4],x[5],x[6]),c(x[7],x[8],x[9]), "\n")
> 
>         f
>     }
>    
> 
> 
> #generate 1000 samples from norMix  mixture MW.nm2
> x<-rnorMix(1000,MW.nm2)
> hist(rnorMix(1000,MW.nm2))
> #  Give it a starting values
> mu<-c(-0.2,0.4,0.9)
> sigma<-c(0.2, 0.2, 0.6)
> wt<-c(0.3,0.3,0.2)
> r<-optim(c(mu,sigma,wt),mydistrmle,control=list(maxit=50000))
> print(matrix(r$par,3,3))
> print(MW.nm2)
> 
> You will have to add additional constraints on the "wt" etc.  ( see 
> ?constrOptim)
> 
> Further I don't think there is a guaranteed unique decomposition of 
> variance between the three gaussians. Just try different starting values
> and also is there a reason to stop at 3 gaussians ?..
> 
> Krishna

Thanks. There is not a particular reason to stop at three Gaussians
apart from the fact that I know that the data I will soon be given show
a 3 modal distribution.
Apart from implementing some extra constrains, are these sort of
problems numerically stable? I get plenty of warnings when I run your
code, although it executes.
Are those warnings anything to worry about or do they simply mean that
in some iterations the optimizer rejects the solution (though they seem
to deal with dnorm())?
Cheers

Lorenzo


From L.Isella at myrealbox.com  Sun Apr 23 22:02:28 2006
From: L.Isella at myrealbox.com (L.Isella)
Date: Sun, 23 Apr 2006 20:02:28 +0000
Subject: [R-sig-finance] Distribution Fitting
Message-ID: <1145822548.c819659cL.Isella@myrealbox.com>

Dear All,
I am experiencing some problems in fitting a trimodal distribution (which should be thought as a sum of three Gaussian distributions) using the nls function for nonlinear fittings.
As a test, just consider the very simple code:

rm(list=ls());
mydata<-rnorm(10000,0,4);
mydens<-density(mydata,kernel="gaussian");
y1<-mydens$y;
x1<-mydens$x;
myfit<-nls(y1~A*exp(-x1^2/sig));


which I use to get the empirical density (as I would from real experimental data) and test it against a Gaussian ansatz.
Well, either R always crashes for a segmentation fault and I have to restart it manually or I get this output:

Error in match.call(definition, call, expand.dots) :
        '.Primitiv?i?d?????????????...' is not a function

Am I missing the obvious or is there some bug in my R build?
Many thanks

Lorenzo


From L.Isella at myrealbox.com  Sun Apr 23 22:08:25 2006
From: L.Isella at myrealbox.com (L.Isella)
Date: Sun, 23 Apr 2006 20:08:25 +0000
Subject: [R-sig-finance] Distribution Fitting
Message-ID: <1145822905.c810c0bcL.Isella@myrealbox.com>

Dear All,
I am experiencing some problems in fitting a trimodal distribution (which should be thought as a sum of three Gaussian distributions) using the nls function for nonlinear fittings.
As a test, just consider the very simple code:

rm(list=ls());
mydata<-rnorm(10000,0,4);
mydens<-density(mydata,kernel="gaussian");
y1<-mydens$y;
x1<-mydens$x;
myfit<-nls(y1~A*exp(-x1^2/sig));


which I use to get the empirical density (as I would from real experimental data) and test it against a Gaussian ansatz.
Well, either R always crashes for a segmentation fault and I have to restart it manually or I get this output:

Error in match.call(definition, call, expand.dots) :
        '.Primitiv?i?d?????????????...' is not a function

Am I missing the obvious or is there some bug in my R build?
Many thanks

Lorenzo


From L.Isella at myrealbox.com  Sun Apr 23 22:17:23 2006
From: L.Isella at myrealbox.com (L.Isella)
Date: Sun, 23 Apr 2006 20:17:23 +0000
Subject: [R-sig-finance] Distribution Fitting
Message-ID: <1145823443.c819659cL.Isella@myrealbox.com>

Dear All,
I am experiencing some problems in fitting a trimodal distribution (which should be thought as a sum of three Gaussian distributions) using the nls function for nonlinear fittings.
As a test, just consider the very simple code:

rm(list=ls());
mydata<-rnorm(10000,0,4);
mydens<-density(mydata,kernel="gaussian");
y1<-mydens$y;
x1<-mydens$x;
myfit<-nls(y1~A*exp(-x1^2/sig));


which I use to get the empirical density (as I would from real experimental data) and test it against a Gaussian ansatz.
Well, either R always crashes for a segmentation fault and I have to restart it manually or I get this output:

Error in match.call(definition, call, expand.dots) :
        '.Primitiv?i?d?????????????...' is not a function

Am I missing the obvious or is there some bug in my R build?
Many thanks

Lorenzo


From L.Isella at myrealbox.com  Sun Apr 23 23:11:12 2006
From: L.Isella at myrealbox.com (L.Isella)
Date: Sun, 23 Apr 2006 21:11:12 +0000
Subject: [R-sig-finance] Distribution Fitting
Message-ID: <1145826672.c800765cL.Isella@myrealbox.com>

Dear All,
I am experiencing some problems in fitting a trimodal distribution (which should be thought as a sum of three Gaussian distributions) using the nls function for nonlinear fittings.
As a test, just consider the very simple code:

rm(list=ls());
mydata<-rnorm(10000,0,4);
mydens<-density(mydata,kernel="gaussian");
y1<-mydens$y;
x1<-mydens$x;
myfit<-nls(y1~A*exp(-x1^2/sig));


which I use to get the empirical density (as I would from real experimental data) and test it against a Gaussian ansatz.
Well, either R always crashes for a segmentation fault and I have to restart it manually or I get this output:

Error in match.call(definition, call, expand.dots) :
'.Primitiv?i?d?????????????...' is not a function

Am I missing the obvious or is there some bug in my R build?
Many thanks


From lorenzo.isella at gmail.com  Thu Apr 27 12:34:52 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Thu, 27 Apr 2006 12:34:52 +0200
Subject: [R-sig-finance] R-sig-finance Digest, Vol 23, Issue 14
In-Reply-To: <mailman.13.1146132005.12672.r-sig-finance@stat.math.ethz.ch>
References: <mailman.13.1146132005.12672.r-sig-finance@stat.math.ethz.ch>
Message-ID: <a2b3004b0604270334i5f3b4309p98d92c3bc47ab9b1@mail.gmail.com>

Apologies for these emails.
I had some troubles with my account a few days ago and I thought I was
not able to send anything to the mailing list.

Lorenzo

On 4/27/06, r-sig-finance-request at stat.math.ethz.ch
<r-sig-finance-request at stat.math.ethz.ch> wrote:
> Send R-sig-finance mailing list submissions to
>         r-sig-finance at stat.math.ethz.ch
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> or, via email, send a message with subject or body 'help' to
>         r-sig-finance-request at stat.math.ethz.ch
>
> You can reach the person managing the list at
>         r-sig-finance-owner at stat.math.ethz.ch
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-sig-finance digest..."
>
>
> Today's Topics:
>
>    1. Distribution Fitting (L.Isella)
>    2. Distribution Fitting (L.Isella)
>    3. Distribution Fitting (L.Isella)
>    4. Distribution Fitting (L.Isella)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Sun, 23 Apr 2006 20:02:28 +0000
> From: "L.Isella" <L.Isella at myrealbox.com>
> Subject: [R-sig-finance] Distribution Fitting
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID: <1145822548.c819659cL.Isella at myrealbox.com>
> Content-Type: text/plain; charset="UTF-8"
>
> Dear All,
> I am experiencing some problems in fitting a trimodal distribution (which should be thought as a sum of three Gaussian distributions) using the nls function for nonlinear fittings.
> As a test, just consider the very simple code:
>
> rm(list=ls());
> mydata<-rnorm(10000,0,4);
> mydens<-density(mydata,kernel="gaussian");
> y1<-mydens$y;
> x1<-mydens$x;
> myfit<-nls(y1~A*exp(-x1^2/sig));
>
>
> which I use to get the empirical density (as I would from real experimental data) and test it against a Gaussian ansatz.
> Well, either R always crashes for a segmentation fault and I have to restart it manually or I get this output:
>
> Error in match.call(definition, call, expand.dots) :
>         '.Primitiv?i?d?????????????...' is not a function
>
> Am I missing the obvious or is there some bug in my R build?
> Many thanks
>
> Lorenzo
>
>
>
> ------------------------------
>
> Message: 2
> Date: Sun, 23 Apr 2006 20:08:25 +0000
> From: "L.Isella" <L.Isella at myrealbox.com>
> Subject: [R-sig-finance] Distribution Fitting
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID: <1145822905.c810c0bcL.Isella at myrealbox.com>
> Content-Type: text/plain; charset="UTF-8"
>
> Dear All,
> I am experiencing some problems in fitting a trimodal distribution (which should be thought as a sum of three Gaussian distributions) using the nls function for nonlinear fittings.
> As a test, just consider the very simple code:
>
> rm(list=ls());
> mydata<-rnorm(10000,0,4);
> mydens<-density(mydata,kernel="gaussian");
> y1<-mydens$y;
> x1<-mydens$x;
> myfit<-nls(y1~A*exp(-x1^2/sig));
>
>
> which I use to get the empirical density (as I would from real experimental data) and test it against a Gaussian ansatz.
> Well, either R always crashes for a segmentation fault and I have to restart it manually or I get this output:
>
> Error in match.call(definition, call, expand.dots) :
>         '.Primitiv?i?d?????????????...' is not a function
>
> Am I missing the obvious or is there some bug in my R build?
> Many thanks
>
> Lorenzo
>
>
>
> ------------------------------
>
> Message: 3
> Date: Sun, 23 Apr 2006 20:17:23 +0000
> From: "L.Isella" <L.Isella at myrealbox.com>
> Subject: [R-sig-finance] Distribution Fitting
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID: <1145823443.c819659cL.Isella at myrealbox.com>
> Content-Type: text/plain; charset="UTF-8"
>
> Dear All,
> I am experiencing some problems in fitting a trimodal distribution (which should be thought as a sum of three Gaussian distributions) using the nls function for nonlinear fittings.
> As a test, just consider the very simple code:
>
> rm(list=ls());
> mydata<-rnorm(10000,0,4);
> mydens<-density(mydata,kernel="gaussian");
> y1<-mydens$y;
> x1<-mydens$x;
> myfit<-nls(y1~A*exp(-x1^2/sig));
>
>
> which I use to get the empirical density (as I would from real experimental data) and test it against a Gaussian ansatz.
> Well, either R always crashes for a segmentation fault and I have to restart it manually or I get this output:
>
> Error in match.call(definition, call, expand.dots) :
>         '.Primitiv?i?d?????????????...' is not a function
>
> Am I missing the obvious or is there some bug in my R build?
> Many thanks
>
> Lorenzo
>
>
>
> ------------------------------
>
> Message: 4
> Date: Sun, 23 Apr 2006 21:11:12 +0000
> From: "L.Isella" <L.Isella at myrealbox.com>
> Subject: [R-sig-finance] Distribution Fitting
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID: <1145826672.c800765cL.Isella at myrealbox.com>
> Content-Type: text/plain; charset="UTF-8"
>
> Dear All,
> I am experiencing some problems in fitting a trimodal distribution (which should be thought as a sum of three Gaussian distributions) using the nls function for nonlinear fittings.
> As a test, just consider the very simple code:
>
> rm(list=ls());
> mydata<-rnorm(10000,0,4);
> mydens<-density(mydata,kernel="gaussian");
> y1<-mydens$y;
> x1<-mydens$x;
> myfit<-nls(y1~A*exp(-x1^2/sig));
>
>
> which I use to get the empirical density (as I would from real experimental data) and test it against a Gaussian ansatz.
> Well, either R always crashes for a segmentation fault and I have to restart it manually or I get this output:
>
> Error in match.call(definition, call, expand.dots) :
> '.Primitiv?i?d?????????????...' is not a function
>
> Am I missing the obvious or is there some bug in my R build?
> Many thanks
>
>
>
> ------------------------------
>
> _______________________________________________
> R-sig-finance mailing list
> R-sig-finance at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
> End of R-sig-finance Digest, Vol 23, Issue 14
> *********************************************
>


From bbands at gmail.com  Sat Apr 29 00:48:11 2006
From: bbands at gmail.com (BBands)
Date: Fri, 28 Apr 2006 15:48:11 -0700
Subject: [R-sig-finance] negative weights
Message-ID: <6e8360ad0604281548i51cac408n8fc002a52f551f3d@mail.gmail.com>

lsfit does not allow negative weights. Is there a similar function that does?

     jab
--
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From edd at debian.org  Sat Apr 29 01:29:52 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 28 Apr 2006 18:29:52 -0500
Subject: [R-sig-finance] negative weights
In-Reply-To: <6e8360ad0604281548i51cac408n8fc002a52f551f3d@mail.gmail.com>
References: <6e8360ad0604281548i51cac408n8fc002a52f551f3d@mail.gmail.com>
Message-ID: <17490.42352.159830.195550@basebud.nulle.part>


Hi John,

On 28 April 2006 at 15:48, BBands wrote:
| lsfit does not allow negative weights. Is there a similar function that does?

When OLS is generalized to GLS (also called WLS), weights are typically
thought to account for varying degrees of uncertainty reflected in varying
sizes of residuals.  But you are still minimizing a sum of squares in which
each observation contributes at least some marginal bits of observations.

So negative weights don't really fit that framework. That said, from a purely
numerical as opposed to statistical point of view you can probably minimize a
suitable expression with nls() or optim().  But you'd be 'on your own out
there'.

Hope this helps,  Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From bbands at gmail.com  Sat Apr 29 02:44:31 2006
From: bbands at gmail.com (BBands)
Date: Fri, 28 Apr 2006 17:44:31 -0700
Subject: [R-sig-finance] negative weights
In-Reply-To: <17490.42352.159830.195550@basebud.nulle.part>
References: <6e8360ad0604281548i51cac408n8fc002a52f551f3d@mail.gmail.com>
	<17490.42352.159830.195550@basebud.nulle.part>
Message-ID: <6e8360ad0604281744w2f39db31ya4aea4edac0f545d@mail.gmail.com>

On 4/28/06, Dirk Eddelbuettel <edd at debian.org> wrote:
> So negative weights don't really fit that framework. That said, from a purely
> numerical as opposed to statistical point of view you can probably minimize a
> suitable expression with nls() or optim().  But you'd be 'on your own out
> there'.

Hi Dirk,

I was looking for an all-in sort of solution, but preprocessing the
data will get me where I need to go, so no traipsing around in the
'out there' for me. Perhaps I don't have the necessary statistical
sophistication, but negative weights for linear models seem like a
perfectly reasonable solution to the problem of different forecasting
abilities at different horizons.

     jab
--
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From bbands at gmail.com  Sat Apr 29 16:03:28 2006
From: bbands at gmail.com (BBands)
Date: Sat, 29 Apr 2006 07:03:28 -0700
Subject: [R-sig-finance] Fwd:  negative weights
In-Reply-To: <6e8360ad0604290702l44ddccccg45db0a685a904843@mail.gmail.com>
References: <6e8360ad0604281548i51cac408n8fc002a52f551f3d@mail.gmail.com>
	<17490.42352.159830.195550@basebud.nulle.part>
	<6e8360ad0604281744w2f39db31ya4aea4edac0f545d@mail.gmail.com>
	<17490.51436.864937.834276@basebud.nulle.part>
	<6e8360ad0604290702l44ddccccg45db0a685a904843@mail.gmail.com>
Message-ID: <6e8360ad0604290703n139fd586oa570407c3d69f58@mail.gmail.com>

On 4/28/06, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> Hm, you didn't mention forecasting. I am not even sure where weights would
> enter there...

On 4/29/06, Patrick Burns <patrick at burns-stat.com> wrote:

> I'm not sure what you are aiming at.  I would think
> that a negative weight would mean that the bigger
> the residual for that observation, the better.

I build these models to forecast future returns, but maybe I am
barking up the wrong tree on this one. Let's use a very widely
accepted meme to see:

Suppose you buy into the Columbine thesis that mean reversion prevails
in the short term while momentum prevails in the long term. Let's look
at the simplest model that can capture that thesis, a
two-period-return model where a is the long-term return and b is the
short-term return. In order for this model to work you would need
weights of something like 1 and -1 for a and b respectively. Now
expand the model to a reasonable number of returns and a larger number
of securities and a regression using a shaped set of weights including
negative weights starts to look like an attractive idea. Of course I
can preprocess the data and then feed it to the model...

Any ideas?

    jab
--
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From ggrothendieck at gmail.com  Sat Apr 29 16:14:56 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 29 Apr 2006 10:14:56 -0400
Subject: [R-sig-finance] Fwd: negative weights
In-Reply-To: <6e8360ad0604290703n139fd586oa570407c3d69f58@mail.gmail.com>
References: <6e8360ad0604281548i51cac408n8fc002a52f551f3d@mail.gmail.com>
	<17490.42352.159830.195550@basebud.nulle.part>
	<6e8360ad0604281744w2f39db31ya4aea4edac0f545d@mail.gmail.com>
	<17490.51436.864937.834276@basebud.nulle.part>
	<6e8360ad0604290702l44ddccccg45db0a685a904843@mail.gmail.com>
	<6e8360ad0604290703n139fd586oa570407c3d69f58@mail.gmail.com>
Message-ID: <971536df0604290714s2d092c67qbac76f817fb28abc@mail.gmail.com>

On 4/29/06, BBands <bbands at gmail.com> wrote:
> On 4/28/06, Dirk Eddelbuettel <edd at debian.org> wrote:
> >
> > Hm, you didn't mention forecasting. I am not even sure where weights would
> > enter there...
>
> On 4/29/06, Patrick Burns <patrick at burns-stat.com> wrote:
>
> > I'm not sure what you are aiming at.  I would think
> > that a negative weight would mean that the bigger
> > the residual for that observation, the better.
>
> I build these models to forecast future returns, but maybe I am
> barking up the wrong tree on this one. Let's use a very widely
> accepted meme to see:
>
> Suppose you buy into the Columbine thesis that mean reversion prevails
> in the short term while momentum prevails in the long term. Let's look
> at the simplest model that can capture that thesis, a
> two-period-return model where a is the long-term return and b is the
> short-term return. In order for this model to work you would need
> weights of something like 1 and -1 for a and b respectively. Now
> expand the model to a reasonable number of returns and a larger number
> of securities and a regression using a shaped set of weights including
> negative weights starts to look like an attractive idea. Of course I
> can preprocess the data and then feed it to the model...
>
> Any ideas?

I think you will need to specify your model more concretely to get more
than passing comments.  At any rate, note that if the weights can be
negative then the sum of squares to be optimized is no longer a convex function
of the coefficients so we really don't have a conventional least squares
model and uniqueness and existence have possibly different answers.


From bbands at gmail.com  Sun Apr 30 19:56:40 2006
From: bbands at gmail.com (BBands)
Date: Sun, 30 Apr 2006 10:56:40 -0700
Subject: [R-sig-finance] Fwd: negative weights
In-Reply-To: <002e01c66c5e$c56e65b0$0ac0a8c0@MightyMini>
References: <mailman.11.1146391204.29640.r-sig-finance@stat.math.ethz.ch>
	<002e01c66c5e$c56e65b0$0ac0a8c0@MightyMini>
Message-ID: <6e8360ad0604301056r7ff24711s9f9bd72e665f1de4@mail.gmail.com>

On 4/30/06, Dan Rie <drie at portfoliointelligence.com> wrote:

> but in most cases does not change the expected
> value of the coefficient estimates themselves.

That was an aha for me, though I should have known it... I calculated
a couple of regressions by hand with varying weight schemes to verify
and I get it now. (Actually I saw this early on, but assumed it was a
mistake in my usage of R.) For my purposes I must apply the weights to
the dependent returns prior to doing an unweighted regression. An
initial pass on purpose-built test data produced intuitively correct
results.

Thanks to all,

    jab
--
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From tolga.uzuner at credit-suisse.com  Tue May  2 14:03:31 2006
From: tolga.uzuner at credit-suisse.com (Uzuner, Tolga)
Date: Tue, 2 May 2006 13:03:31 +0100
Subject: [R-sig-finance] Accessing the Option Price in CashorNothingOption
Message-ID: <D8B41C349763B14BB57F4E04339322DB0BF6D83E@elon12p32001.csfp.co.uk>

> Dear R-sig-finance members,
> 
> I am trying to use the CashorNothingOption , and would like to access the Option Price directly.
> 
> When I use CashorNothingOption, I am being given back the following:
> 
> > CashOrNothingOption("p",100,30,50,5,.05,0,.2)
> 
> Title:
> Cash Or Nothing Option 
> 
> Call:
> CashOrNothingOption(TypeFlag = "p", S = 100, X = 30, K = 50, 
>     Time = 5, r = 0.05, b = 0, sigma = 0.2)
> 
> Parameters:
>          Value:
> TypeFlag p     
> S        100   
> X        30    
> K        50    
> Time     5     
> r        0.05  
> b        0     
> sigma    0.2   
> 
> Option Price:
> 0.2641278 
> 
> Description:
> Tue May 02 12:46:07 2006 
> 
> How can I access the Option Price by assigning it's value to a variable so that I can multiply it by something else ?
> 
> Assuming the formula returned a list, I tried a number of things, none of which worked:
> 
> > CashOrNothingOption("p",100,30,50,5,.05,0,.2)$par
> NULL
> > CashOrNothingOption("p",100,30,50,5,.05,0,.2)$price
> NULL
> > CashOrNothingOption("p",100,30,50,5,.05,0,.2)$OptionPrice
> NULL
> > 
> 
> The documentation just says:
> 
> ...
> Value:
> 
>      The option price, a numeric value.
> ....
> 
> which is not very revealing, neither does it seem to be true !
> 
> Many thanks in advance,
> Tolga
> 

==============================================================================
Please access the attached hyperlink for an important electr...{{dropped}}


From wojciech.slusarski at gmail.com  Tue May  2 16:00:35 2006
From: wojciech.slusarski at gmail.com (Wojciech Slusarski)
Date: Tue, 2 May 2006 16:00:35 +0200
Subject: [R-sig-finance] Accessing the Option Price in
	CashorNothingOption
In-Reply-To: <D8B41C349763B14BB57F4E04339322DB0BF6D83E@elon12p32001.csfp.co.uk>
References: <D8B41C349763B14BB57F4E04339322DB0BF6D83E@elon12p32001.csfp.co.uk>
Message-ID: <5e64e5be0605020700r60421738pb7d21088ea8894d4@mail.gmail.com>

Hi,

Try the one below:

CashOrNothingOption("p",100,30,50,5,.05,0,.2)@price

should work.

Best regards,
Wojtek

2006/5/2, Uzuner, Tolga <tolga.uzuner at credit-suisse.com>:
> > Dear R-sig-finance members,
> >
> > I am trying to use the CashorNothingOption , and would like to access the Option Price directly.
> >
> > When I use CashorNothingOption, I am being given back the following:
> >
> > > CashOrNothingOption("p",100,30,50,5,.05,0,.2)
> >
> > Title:
> > Cash Or Nothing Option
> >
> > Call:
> > CashOrNothingOption(TypeFlag = "p", S = 100, X = 30, K = 50,
> >     Time = 5, r = 0.05, b = 0, sigma = 0.2)
> >
> > Parameters:
> >          Value:
> > TypeFlag p
> > S        100
> > X        30
> > K        50
> > Time     5
> > r        0.05
> > b        0
> > sigma    0.2
> >
> > Option Price:
> > 0.2641278
> >
> > Description:
> > Tue May 02 12:46:07 2006
> >
> > How can I access the Option Price by assigning it's value to a variable so that I can multiply it by something else ?
> >
> > Assuming the formula returned a list, I tried a number of things, none of which worked:
> >
> > > CashOrNothingOption("p",100,30,50,5,.05,0,.2)$par
> > NULL
> > > CashOrNothingOption("p",100,30,50,5,.05,0,.2)$price
> > NULL
> > > CashOrNothingOption("p",100,30,50,5,.05,0,.2)$OptionPrice
> > NULL
> > >
> >
> > The documentation just says:
> >
> > ...
> > Value:
> >
> >      The option price, a numeric value.
> > ....
> >
> > which is not very revealing, neither does it seem to be true !
> >
> > Many thanks in advance,
> > Tolga
> >
>
> ==============================================================================
> Please access the attached hyperlink for an important electr...{{dropped}}
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From wuertz at itp.phys.ethz.ch  Thu May  4 08:05:53 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 04 May 2006 08:05:53 +0200
Subject: [R-sig-finance] Accessing the Option Price
	in	CashorNothingOption
In-Reply-To: <5e64e5be0605020700r60421738pb7d21088ea8894d4@mail.gmail.com>
References: <D8B41C349763B14BB57F4E04339322DB0BF6D83E@elon12p32001.csfp.co.uk>
	<5e64e5be0605020700r60421738pb7d21088ea8894d4@mail.gmail.com>
Message-ID: <445999C1.7000209@itp.phys.ethz.ch>


CashOrNothingOption() returns as all other options an S4
object.

CashOrNothingOption("p",100,30,50,5,.05,0,.2)@parameters

returns a list of all parameters

CashOrNothingOption("p",100,30,50,5,.05,0,.2)@parameters$sigma

returns a specific parameter, here sigma

The price has its own slot

CashOrNothingOption("p",100,30,50,5,.05,0,.2)@price

returns the price of the option.


DW





Wojciech Slusarski wrote:

>Hi,
>
>Try the one below:
>
>CashOrNothingOption("p",100,30,50,5,.05,0,.2)@price
>
>should work.
>
>Best regards,
>Wojtek
>
>2006/5/2, Uzuner, Tolga <tolga.uzuner at credit-suisse.com>:
>  
>
>>>Dear R-sig-finance members,
>>>
>>>I am trying to use the CashorNothingOption , and would like to access the Option Price directly.
>>>
>>>When I use CashorNothingOption, I am being given back the following:
>>>
>>>      
>>>
>>>>CashOrNothingOption("p",100,30,50,5,.05,0,.2)
>>>>        
>>>>
>>>Title:
>>>Cash Or Nothing Option
>>>
>>>Call:
>>>CashOrNothingOption(TypeFlag = "p", S = 100, X = 30, K = 50,
>>>    Time = 5, r = 0.05, b = 0, sigma = 0.2)
>>>
>>>Parameters:
>>>         Value:
>>>TypeFlag p
>>>S        100
>>>X        30
>>>K        50
>>>Time     5
>>>r        0.05
>>>b        0
>>>sigma    0.2
>>>
>>>Option Price:
>>>0.2641278
>>>
>>>Description:
>>>Tue May 02 12:46:07 2006
>>>
>>>How can I access the Option Price by assigning it's value to a variable so that I can multiply it by something else ?
>>>
>>>Assuming the formula returned a list, I tried a number of things, none of which worked:
>>>
>>>      
>>>
>>>>CashOrNothingOption("p",100,30,50,5,.05,0,.2)$par
>>>>        
>>>>
>>>NULL
>>>      
>>>
>>>>CashOrNothingOption("p",100,30,50,5,.05,0,.2)$price
>>>>        
>>>>
>>>NULL
>>>      
>>>
>>>>CashOrNothingOption("p",100,30,50,5,.05,0,.2)$OptionPrice
>>>>        
>>>>
>>>NULL
>>>      
>>>
>>>The documentation just says:
>>>
>>>...
>>>Value:
>>>
>>>     The option price, a numeric value.
>>>....
>>>
>>>which is not very revealing, neither does it seem to be true !
>>>
>>>Many thanks in advance,
>>>Tolga
>>>
>>>      
>>>
>>==============================================================================
>>Please access the attached hyperlink for an important electr...{{dropped}}
>>
>>_______________________________________________
>>R-sig-finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
>>    
>>
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From spencer.graves at pdf.com  Fri May  5 16:57:43 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 05 May 2006 07:57:43 -0700
Subject: [R-sig-finance] negative weights
In-Reply-To: <6e8360ad0604281744w2f39db31ya4aea4edac0f545d@mail.gmail.com>
References: <6e8360ad0604281548i51cac408n8fc002a52f551f3d@mail.gmail.com>	<17490.42352.159830.195550@basebud.nulle.part>
	<6e8360ad0604281744w2f39db31ya4aea4edac0f545d@mail.gmail.com>
Message-ID: <445B67E7.3090306@pdf.com>

	  Weights in 'nls' and in forecasting are two very different things. 
Weights in functions like 'nls', 'lm', 'lme', and often also 'optim' are 
typically justified from a maximum likelihood argument.  In that case, 
the weights are (exactly or metaphorically, depending on context) 
inversely proportional to the variances of the observations.  Negative 
weights in that context implies imaginary standard deviations;  I'll let 
you extrapolate from there.

	  Weights in forecasting, however, commonly occur when modeling, for 
example, the output of a reactor:  If the reactor delivers less than its 
standard output on one cycle, it will often do the opposite on the next. 
  This is common with straight "moving average" models in the standard 
time series literature, e.g., the famous Box and Jenkins (or Box, 
Jenkins and Reinsel now) book "Time Series Analysis, Forecasting and 
Control".  Any good book on "arima" / "Box Jenkins" modeling should 
discuss this.  You can get started on this with the time series chapter 
in the Venables and Ripley book, "Modern Applied Statistics with S".

	  hope this helps,
	  spencer graves

BBands wrote:
> On 4/28/06, Dirk Eddelbuettel <edd at debian.org> wrote:
>> So negative weights don't really fit that framework. That said, from a purely
>> numerical as opposed to statistical point of view you can probably minimize a
>> suitable expression with nls() or optim().  But you'd be 'on your own out
>> there'.
> 
> Hi Dirk,
> 
> I was looking for an all-in sort of solution, but preprocessing the
> data will get me where I need to go, so no traipsing around in the
> 'out there' for me. Perhaps I don't have the necessary statistical
> sophistication, but negative weights for linear models seem like a
> perfectly reasonable solution to the problem of different forecasting
> abilities at different horizons.
> 
>      jab
> --
> John Bollinger, CFA, CMT
> www.BollingerBands.com
> 
> If you advance far enough, you arrive at the beginning.
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From ajayshah at mayin.org  Tue May  9 11:11:31 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Tue, 9 May 2006 14:41:31 +0530
Subject: [R-sig-finance] Behaviour of get.hist.quote when presented with bad
	inputs
Message-ID: <20060509091131.GG191@lubyanka.local>

With R 2.3, when I say get.hist.quote("rubbish") he gets extremely unhappy:

> library(tseries)
Loading required package: quadprog
Loading required package: zoo
> x <- get.hist.quote("rubbish")
trying URL
'http://chart.yahoo.com/table.csv?s=rubbish&a=0&b=02&c=1991&d=4&e=08&f=2006&g=d&q=q&y=0&z=rubbish&x=.csv'

 *** caught segfault ***
address 0x5f4d4550, cause 'memory not mapped'

Traceback:
 1: download.file(url, destfile, method = method)
 2: get.hist.quote("rubbish")

Possible actions:
1: abort (with core dump)
2: normal R exit
3: exit R without saving workspace
4: exit R saving workspace
Selection: c

I believe that before R 2.3, it was not so bad - there was no segfault
when a bad input was presented. I feel that it's enough to return NULL
when presented with "rubbish", but segfaulting is kinda extreme.

I tried to say
  > try(x <- get.hist.quote("rubbish"))
but that also segfaults.

How can I do better?

My machine is an ibook running an uptodate copy of OS X "tiger".
$ uname -a
Darwin lubyanka.local 8.6.0 Darwin Kernel Version 8.6.0: Tue Mar  7 16:58:48 PST 2006; root:xnu-792.6.70.obj~1/RELEASE_PPC Power Macintosh powerpc

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From edd at debian.org  Tue May  9 12:20:28 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 9 May 2006 05:20:28 -0500
Subject: [R-sig-finance] Behaviour of get.hist.quote when presented with
	bad inputs
In-Reply-To: <20060509091131.GG191@lubyanka.local>
References: <20060509091131.GG191@lubyanka.local>
Message-ID: <17504.27884.9086.448404@basebud.nulle.part>


Hi Ajay,

On 9 May 2006 at 14:41, Ajay Narottam Shah wrote:
| With R 2.3, when I say get.hist.quote("rubbish") he gets extremely unhappy:
| 
| > library(tseries)
| Loading required package: quadprog
| Loading required package: zoo
| > x <- get.hist.quote("rubbish")
| trying URL
| 'http://chart.yahoo.com/table.csv?s=rubbish&a=0&b=02&c=1991&d=4&e=08&f=2006&g=d&q=q&y=0&z=rubbish&x=.csv'
| 
|  *** caught segfault ***
| address 0x5f4d4550, cause 'memory not mapped'
| 
| Traceback:
|  1: download.file(url, destfile, method = method)
|  2: get.hist.quote("rubbish")
| 
| Possible actions:
| 1: abort (with core dump)
| 2: normal R exit
| 3: exit R without saving workspace
| 4: exit R saving workspace
| Selection: c
| 
| I believe that before R 2.3, it was not so bad - there was no segfault
| when a bad input was presented. I feel that it's enough to return NULL
| when presented with "rubbish", but segfaulting is kinda extreme.
| 
| I tried to say
|   > try(x <- get.hist.quote("rubbish"))
| but that also segfaults.
| 
| How can I do better?
| 
| My machine is an ibook running an uptodate copy of OS X "tiger".
| $ uname -a
| Darwin lubyanka.local 8.6.0 Darwin Kernel Version 8.6.0: Tue Mar  7 16:58:48 PST 2006; root:xnu-792.6.70.obj~1/RELEASE_PPC Power Macintosh powerpc

Looks like it is a Mac issue as it works fine here. I'm CCing Simon.

One other thing you could try is to set the option() argument for the
downloader to use the external wget command instead of the built-in solution.
That may prevent the segfault.

Hth, Dirk

edd at basebud:~> R

R : Copyright 2006, The R Foundation for Statistical Computing
Version 2.3.0 (2006-04-24)
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(tseries)
Loading required package: quadprog
Loading required package: zoo
Warning message:
use of NULL environment is deprecated
>  x <- get.hist.quote("rubbish")
trying URL 'http://chart.yahoo.com/table.csv?s=rubbish&a=0&b=02&c=1991&d=4&e=08&f=2006&g=d&q=q&y=0&z=rubbish&x=.csv'
Error in download.file(url, destfile, method = method) :
        cannot open URL 'http://chart.yahoo.com/table.csv?s=rubbish&a=0&b=02&c=1991&d=4&e=08&f=2006&g=d&q=q&y=0&z=rubbish&x=.csv'
In addition: Warning message:
cannot open: HTTP status was '404 Not Found'
>



| 
| -- 
| Ajay Shah                                      http://www.mayin.org/ajayshah  
| ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
| <*(:-? - wizard who doesn't know the answer.
| 
| _______________________________________________
| R-sig-finance at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From simon.urbanek at r-project.org  Tue May  9 15:37:09 2006
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 9 May 2006 09:37:09 -0400
Subject: [R-sig-finance] Behaviour of get.hist.quote when presented with
	bad inputs
In-Reply-To: <17504.27884.9086.448404@basebud.nulle.part>
References: <20060509091131.GG191@lubyanka.local>
	<17504.27884.9086.448404@basebud.nulle.part>
Message-ID: <F9042EC2-8EDE-4DD3-8F96-C867DF60D7E0@r-project.org>


On May 9, 2006, at 6:20 AM, Dirk Eddelbuettel wrote:

>
> Hi Ajay,
>
> On 9 May 2006 at 14:41, Ajay Narottam Shah wrote:
> | With R 2.3, when I say get.hist.quote("rubbish") he gets  
> extremely unhappy:
> |
> | > library(tseries)
> | Loading required package: quadprog
> | Loading required package: zoo
> | > x <- get.hist.quote("rubbish")
> | trying URL
> | 'http://chart.yahoo.com/table.csv? 
> s=rubbish&a=0&b=02&c=1991&d=4&e=08&f=2006&g=d&q=q&y=0&z=rubbish&x=.csv 
> '
> |
> |  *** caught segfault ***
> | address 0x5f4d4550, cause 'memory not mapped'
> |

This is a (hard to trace) bug in the internal http request code in R  
2.3.0 that strangely enough appears only on OS X. If the URL doesn't  
exist, R blows (basically it blows when it wants to print an error  
message). You may want to switch to another download method for now  
as Dirk suggested, and hopefully we'll fix this soon.

Cheers,
Simon


From ajayshah at mayin.org  Tue May  9 16:34:02 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Tue, 9 May 2006 20:04:02 +0530
Subject: [R-sig-finance] Behaviour of get.hist.quote when presented with
	bad inputs
In-Reply-To: <F9042EC2-8EDE-4DD3-8F96-C867DF60D7E0@r-project.org>
References: <20060509091131.GG191@lubyanka.local>
	<17504.27884.9086.448404@basebud.nulle.part>
	<F9042EC2-8EDE-4DD3-8F96-C867DF60D7E0@r-project.org>
Message-ID: <20060509143402.GE290@lubyanka.local>

> >| With R 2.3, when I say get.hist.quote("rubbish") he gets  
> >extremely unhappy:
> >|
> >| > library(tseries)
> >| Loading required package: quadprog
> >| Loading required package: zoo
> >| > x <- get.hist.quote("rubbish")
> >| trying URL
> >| 'http://chart.yahoo.com/table.csv? 
> >s=rubbish&a=0&b=02&c=1991&d=4&e=08&f=2006&g=d&q=q&y=0&z=rubbish&x=.csv 
> >'
> >|  *** caught segfault ***
> >| address 0x5f4d4550, cause 'memory not mapped'
> 
> This is a (hard to trace) bug in the internal http request code in R  
> 2.3.0 that strangely enough appears only on OS X. If the URL doesn't  
> exist, R blows (basically it blows when it wants to print an error  
> message). You may want to switch to another download method for now  
> as Dirk suggested, and hopefully we'll fix this soon.

I tried to use options() to set the download method to wget, but the
problem didn't go away.

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From simon.urbanek at r-project.org  Tue May  9 16:47:08 2006
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 9 May 2006 10:47:08 -0400
Subject: [R-sig-finance] Behaviour of get.hist.quote when presented with
	bad inputs
In-Reply-To: <20060509143402.GE290@lubyanka.local>
References: <20060509091131.GG191@lubyanka.local>
	<17504.27884.9086.448404@basebud.nulle.part>
	<F9042EC2-8EDE-4DD3-8F96-C867DF60D7E0@r-project.org>
	<20060509143402.GE290@lubyanka.local>
Message-ID: <7FAA15A1-EA1F-456C-8397-DDAB82AF205B@r-project.org>


On May 9, 2006, at 10:34 AM, Ajay Narottam Shah wrote:

>>> | With R 2.3, when I say get.hist.quote("rubbish") he gets
>>> extremely unhappy:
>>> |
>>> | > library(tseries)
>>> | Loading required package: quadprog
>>> | Loading required package: zoo
>>> | > x <- get.hist.quote("rubbish")
>>> | trying URL
>>> | 'http://chart.yahoo.com/table.csv?
>>> s=rubbish&a=0&b=02&c=1991&d=4&e=08&f=2006&g=d&q=q&y=0&z=rubbish&x=.c 
>>> sv
>>> '
>>> |  *** caught segfault ***
>>> | address 0x5f4d4550, cause 'memory not mapped'
>>
>> This is a (hard to trace) bug in the internal http request code in R
>> 2.3.0 that strangely enough appears only on OS X. If the URL doesn't
>> exist, R blows (basically it blows when it wants to print an error
>> message). You may want to switch to another download method for now
>> as Dirk suggested, and hopefully we'll fix this soon.
>
> I tried to use options() to set the download method to wget, but  
> the problem didn't go away.
>

get.hist.quote stubbornly refuses to use options, so you have to use  
get.hist.quote("foo",method="wget")

Cheers,
Simon


From Achim.Zeileis at R-project.org  Tue May  9 16:57:11 2006
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Tue, 9 May 2006 16:57:11 +0200
Subject: [R-sig-finance] Behaviour of get.hist.quote when presented with
 bad inputs
In-Reply-To: <7FAA15A1-EA1F-456C-8397-DDAB82AF205B@r-project.org>
References: <20060509091131.GG191@lubyanka.local>
	<17504.27884.9086.448404@basebud.nulle.part>
	<F9042EC2-8EDE-4DD3-8F96-C867DF60D7E0@r-project.org>
	<20060509143402.GE290@lubyanka.local>
	<7FAA15A1-EA1F-456C-8397-DDAB82AF205B@r-project.org>
Message-ID: <20060509165711.a77699b6.Achim.Zeileis@R-project.org>

On Tue, 9 May 2006 10:47:08 -0400 Simon Urbanek wrote:

> 
> On May 9, 2006, at 10:34 AM, Ajay Narottam Shah wrote:
> 
> >>> | With R 2.3, when I say get.hist.quote("rubbish") he gets
> >>> extremely unhappy:
> >>> |
> >>> | > library(tseries)
> >>> | Loading required package: quadprog
> >>> | Loading required package: zoo
> >>> | > x <- get.hist.quote("rubbish")
> >>> | trying URL
> >>> | 'http://chart.yahoo.com/table.csv?
> >>> s=rubbish&a=0&b=02&c=1991&d=4&e=08&f=2006&g=d&q=q&y=0&z=rubbish&x=.c 
> >>> sv
> >>> '
> >>> |  *** caught segfault ***
> >>> | address 0x5f4d4550, cause 'memory not mapped'
> >>
> >> This is a (hard to trace) bug in the internal http request code in
> >> R 2.3.0 that strangely enough appears only on OS X. If the URL
> >> doesn't exist, R blows (basically it blows when it wants to print
> >> an error message). You may want to switch to another download
> >> method for now as Dirk suggested, and hopefully we'll fix this
> >> soon.
> >
> > I tried to use options() to set the download method to wget, but  
> > the problem didn't go away.
> >
> 
> get.hist.quote stubbornly refuses to use options,

...because `method = method' is explicitely passed to download.file()
which only triggers getOptions if(missing(method)).

I've just added a workaround to the tseries devel version so that
getOption () is used.
Z


From ajayshah at mayin.org  Tue May  9 17:15:06 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Tue, 9 May 2006 20:45:06 +0530
Subject: [R-sig-finance] Behaviour of get.hist.quote when presented with
	bad inputs
In-Reply-To: <7FAA15A1-EA1F-456C-8397-DDAB82AF205B@r-project.org>
References: <20060509091131.GG191@lubyanka.local>
	<17504.27884.9086.448404@basebud.nulle.part>
	<F9042EC2-8EDE-4DD3-8F96-C867DF60D7E0@r-project.org>
	<20060509143402.GE290@lubyanka.local>
	<7FAA15A1-EA1F-456C-8397-DDAB82AF205B@r-project.org>
Message-ID: <20060509151506.GH290@lubyanka.local>

> get.hist.quote stubbornly refuses to use options, so you have to use  
> get.hist.quote("foo",method="wget")

Thanks! It worked. Just a small suggestion - a table.csv* file is left
lying around; ideally it should be blown away (or it should be created
in /tmp).

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From joe-byers at utulsa.edu  Mon May 15 06:16:46 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Sun, 14 May 2006 23:16:46 -0500
Subject: [R-sig-finance] Exogenous Variable in armafit
Message-ID: <446800AE.2050707@utulsa.edu>

I am trying to estimate a simple AR process but need to include trend 
components.  The documentation says exogenous variables can be included 
using the ... argument in the formula description section.  I am new to 
R and do not understand what this means.  Can anyone help?  A snippet of 
code would be great.

Thank you

-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 104 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060514/4e84703f/attachment.vcf 

From edd at debian.org  Tue May 16 03:16:31 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 15 May 2006 20:16:31 -0500
Subject: [R-sig-finance] Exogenous Variable in armafit
In-Reply-To: <446800AE.2050707@utulsa.edu>
References: <446800AE.2050707@utulsa.edu>
Message-ID: <17513.10223.449136.970354@basebud.nulle.part>


On 14 May 2006 at 23:16, Joe Byers wrote:
| I am trying to estimate a simple AR process but need to include trend 
| components.  The documentation says exogenous variables can be included 
| using the ... argument in the formula description section.  I am new to 
| R and do not understand what this means.  Can anyone help?  A snippet of 
| code would be great.

I've use arima() with its xreg argument in the past.  Consider this snippet
from the documentation of the arima() function:

    arima(LakeHuron, order = c(2, 0, 0), xreg = time(LakeHuron) - 1920)

It estimates a regression model with a time trend modeled as the offset
relative to 1920, and an AR(2) error component.

Hope this helps.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From joe-byers at utulsa.edu  Wed May 17 07:02:14 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Wed, 17 May 2006 00:02:14 -0500
Subject: [R-sig-finance] Using summary function from and armafit with R2HTML
In-Reply-To: <17513.10223.449136.970354@basebud.nulle.part>
References: <446800AE.2050707@utulsa.edu>
	<17513.10223.449136.970354@basebud.nulle.part>
Message-ID: <446AAE56.1080501@utulsa.edu>

Has anyone used the summary function for an armafit object with R2HTML?

For example
result<-armaFit(data~arma(2,0), include.mean);
summary(result,doplot=false); # produces a full output of the results 
including coef stats, AIC, LL, and residual stats.
HTML(result); # produces on the model information and coef estimates.

I need the summary functions results printed to my HTML file.  I use the 
HTMLInitFile and need the insert graph capabilities.  Using HTMLStart 
does not provide the results I want.  Mainly I do not want a dump of my 
code as well.

Thank you
Joe

-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 104 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060517/08afe62a/attachment.vcf 

From spencer.graves at pdf.com  Thu May 18 03:47:18 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 17 May 2006 18:47:18 -0700
Subject: [R-sig-finance] Using summary function from and armafit with
 R2HTML
In-Reply-To: <446AAE56.1080501@utulsa.edu>
References: <446800AE.2050707@utulsa.edu>	<17513.10223.449136.970354@basebud.nulle.part>
	<446AAE56.1080501@utulsa.edu>
Message-ID: <446BD226.40706@pdf.com>

	  Have you tried the following:

htmp(summary(result,doplot=false))

	  If this fails, please provide a simple, self-contained (replicatable) 
example, as suggested in the posting guide! 
"www.R-project.org/posting-guide.html".  Please include the package 
containing the "HTML" function.  I have replied to many posts regarding 
functions I've not previously used.  If you supply a simple, 
replicatable example, someone like me can try different things.  This 
will increase your chances that the reply will actually contain working 
code.

	  hope this help,
	  Spencer Graves

Joe Byers wrote:
> Has anyone used the summary function for an armafit object with R2HTML?
> 
> For example
> result<-armaFit(data~arma(2,0), include.mean);
> summary(result,doplot=false); # produces a full output of the results 
> including coef stats, AIC, LL, and residual stats.
> HTML(result); # produces on the model information and coef estimates.
> 
> I need the summary functions results printed to my HTML file.  I use the 
> HTMLInitFile and need the insert graph capabilities.  Using HTMLStart 
> does not provide the results I want.  Mainly I do not want a dump of my 
> code as well.
> 
> Thank you
> Joe
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From lorenzo.isella at gmail.com  Thu May 18 15:45:30 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Thu, 18 May 2006 15:45:30 +0200
Subject: [R-sig-finance] Fitting Distribution Mixs with Incomplete Data
Message-ID: <a2b3004b0605180645y346a2d35n67dc42461c6c4e26@mail.gmail.com>

Dear All,
Suppose you are given some data to fit to a mixture of Gaussian
distributions (a weighted sum of three distributions).
You can do that using e.g. the mix package
(http://www.math.mcmaster.ca/peter/mix/mix.html)
which gave me very good results on some data I generated artificially.
See e.g. the code:

# R code used to fit a sum of three normal distributions
rm(list=ls())
library(mixdist)
set.seed(123)
NN<-10000
three<-3 #number of gaussian sequences I want to generate
mysample<-matrix(nrow=NN,ncol=three)
sdvec<-seq(1:three)
sdvec[1]<-4
sdvec[2]<-3
sdvec[3]<-3.5
meanvec<-seq(1:three)
meanvec[1]<-20
meanvec[2]<-50
meanvec[3]<-70

for (i in 1:three)
{
 mysample[ ,i] <-rnorm(NN,meanvec[i],sdvec[i])
}
dim(mysample)<-c(3*NN,1)
plot(density(mysample,kern="gaussian"),lwd=2,col=300)
## The commented lines are parts of the 1st version of this script

#x<-matrix(ncol=2,nrow=three*NN)
#x[ ,1]<-mysample[]
#x[ ,2]<-1
y<-as.data.frame(mysample)
mixgroup(y)
z<-mixgroup(y,breaks=100)
plot(z)
#ini<-read.table("gaupar.TXT")
ini<-mixparam(mu = c(24, 45, 73), sigma = c(5, 4, 4),pi=c(0.4,0.3,0.3))
mymix<-mix(z,ini,"norm")
summary(mymix)
plot(mymix)
print("So far so good")


However, you can be in a situation in which you cannot sample the
distribution that easily (e.g. you do not have stock quotations
below/above a certain price) and then the same tool does not work that
well. Consider for instance the same case as above but removing all
the data below 15 and above 65:

# R code used to fit a sum of three normal distributions
rm(list=ls())
library(mixdist)
set.seed(123)
NN<-10000
three<-3 #number of gaussian sequences I want to generate
mysample<-matrix(nrow=NN,ncol=three)
sdvec<-seq(1:three)
sdvec[1]<-4
sdvec[2]<-3
sdvec[3]<-3.5
meanvec<-seq(1:three)
meanvec[1]<-20
meanvec[2]<-50
meanvec[3]<-70

for (i in 1:three)
{
 mysample[ ,i] <-rnorm(NN,meanvec[i],sdvec[i])
}
dim(mysample)<-c(3*NN,1)
##  now I am going to remove some data and see if I can still
meaningfully optimize the
## distribution
N3=3*NN
mycount=0
for (i in 1:N3)
{if (mysample[i]>22 & mysample[i]<67)
{
mycount=mycount+1
if (mycount ==1 )
{
mysample2<-cbind(mysample[i])
}
else
{
mysample2<-cbind(mysample2,mysample[i])
}

}

}

plot(density(mysample,kern="gaussian"),lwd=2,col=300)
## The commented lines are parts of the 1st version of this script

#x<-matrix(ncol=2,nrow=three*NN)
#x[ ,1]<-mysample[]
#x[ ,2]<-1
# y<-as.data.frame(mysample)
mysample3<-mysample2[1:15000]
y<-as.data.frame(mysample3)
mixgroup(y)
z<-mixgroup(y,breaks=100)
plot(z)
#ini<-read.table("gaupar.TXT")
ini<-mixparam(mu = c(24, 45, 73), sigma = c(5, 4, 4),pi=c(0.4,0.3,0.3))
mymix<-mix(z,ini,"norm")
summary(mymix)
plot(mymix)
print("So far so good")


Then the mixdist completely misses the means of the 2 external modes.
I am sure that, apart from the combination of three Gaussians, this
situation cannot be "new" at all.
I am thinking about some iteration technique (e.g. you fit the data as
well as you can with mixdist to start with, then according to your
fitted parameters you generate the "missing"  results, then you re-fit
the whole new set of data (real ones and those freshly generated) and
so on hoping to get some convergent results).
But it seems a bit cumbersome (furthermore, how many of them should I
generate?) plus I am sure that solving this problem from scratch is
re-inventing the wheel.
Sorry for the long email.
Kind Regards

Lorenzo Isella


From nop27392 at mail.telepac.pt  Thu May 18 11:31:44 2006
From: nop27392 at mail.telepac.pt (nop27392 at mail.telepac.pt)
Date: Thu, 18 May 2006 10:31:44 +0100
Subject: [R-sig-Finance] zoo object -> ts object
Message-ID: <1147944704.5avrhzbrp4w@w3.webmail.telepac.pt>

Dear all,

I obtained data from the net using

get.hist.quote(instrument = "^DJI", start = "1995-01-01", end = "2004-12-31",
                       quote = "Close")

as suggested n this discussion. The result is a zoo object. How to transform it
in time series object in order to perform log-return etc?

Thanks in advance
Vitor Teixeira


From Achim.Zeileis at wu-wien.ac.at  Thu May 18 18:38:27 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 18 May 2006 18:38:27 +0200
Subject: [R-sig-Finance] zoo object -> ts object
In-Reply-To: <1147944704.5avrhzbrp4w@w3.webmail.telepac.pt>
References: <1147944704.5avrhzbrp4w@w3.webmail.telepac.pt>
Message-ID: <20060518183827.ee79ac8d.Achim.Zeileis@wu-wien.ac.at>

On Thu, 18 May 2006 10:31:44 +0100 nop27392 at mail.telepac.pt wrote:

> Dear all,
> 
> I obtained data from the net using
> 
> get.hist.quote(instrument = "^DJI", start = "1995-01-01", end =
> "2004-12-31", quote = "Close")
> 
> as suggested n this discussion. The result is a zoo object. How to
> transform it in time series object in order to perform log-return etc?

If z is your "zoo" object:
z <- get.hist.quote(instrument = "^DJI", start = "1995-01-01",
  end = "2004-12-31 ", quote = "Close")
then you can do
  plot(diff(log(z)))

So why convert to another class?
See vignette("zoo") for more information on "zoo" and its relations to
other time series classes.

hth,
Z


> Thanks in advance
> Vitor Teixeira
> 
> _______________________________________________
> R-sig-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From ggrothendieck at gmail.com  Fri May 19 00:13:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 18 May 2006 18:13:41 -0400
Subject: [R-sig-Finance] zoo object -> ts object
In-Reply-To: <1147944704.5avrhzbrp4w@w3.webmail.telepac.pt>
References: <1147944704.5avrhzbrp4w@w3.webmail.telepac.pt>
Message-ID: <971536df0605181513i6e30b0dcsb656e951ed73760c@mail.gmail.com>

On 5/18/06, nop27392 at mail.telepac.pt <nop27392 at mail.telepac.pt> wrote:
> Dear all,
>
> I obtained data from the net using
>
> get.hist.quote(instrument = "^DJI", start = "1995-01-01", end = "2004-12-31",
>                       quote = "Close")
>
> as suggested n this discussion. The result is a zoo object. How to transform it
> in time series object in order to perform log-return etc?
>

In addition to what Achim already mentioned

- note the retclass= argument on get.hist.quote
- note that in addition to the vignette Achim mentioned there is a
second vignette
  vignette("zoo-quickref")
that specifically has examples of log returns.
- Ajay Shah's web site has some useful examples:
  http://www.mayin.org/ajayshah/KB/R/index.html


From p.kostov at Queens-Belfast.AC.UK  Fri May 19 10:34:30 2006
From: p.kostov at Queens-Belfast.AC.UK (p.kostov at Queens-Belfast.AC.UK)
Date: 19 May 2006 08:34:30 +0000
Subject: [R-sig-Finance] [R-sig-finance] Fitting Distribution Mixs with
	Incomplete Data
In-Reply-To: <a2b3004b0605180645y346a2d35n67dc42461c6c4e26@mail.gmail.com>
References: <a2b3004b0605180645y346a2d35n67dc42461c6c4e26@mail.gmail.com>
Message-ID: <Prayer.1.0.12.0605190834300.13804@amos>

Why don't you try to use the mmlcr package, where you can specify cnorm 
(censored normal) with the appropriate min and max (known) censoring points.

Hope this helps
Philip

On May 18 2006, Lorenzo Isella wrote:

> Dear All,
> Suppose you are given some data to fit to a mixture of Gaussian
> distributions (a weighted sum of three distributions).
> You can do that using e.g. the mix package
> (http://www.math.mcmaster.ca/peter/mix/mix.html)
> which gave me very good results on some data I generated artificially.
> See e.g. the code:
> 
> # R code used to fit a sum of three normal distributions
> rm(list=ls())
> library(mixdist)
> set.seed(123)
> NN<-10000
> three<-3 #number of gaussian sequences I want to generate
> mysample<-matrix(nrow=NN,ncol=three)
> sdvec<-seq(1:three)
> sdvec[1]<-4
> sdvec[2]<-3
> sdvec[3]<-3.5
> meanvec<-seq(1:three)
> meanvec[1]<-20
> meanvec[2]<-50
> meanvec[3]<-70
> 
> for (i in 1:three)
> {
>  mysample[ ,i] <-rnorm(NN,meanvec[i],sdvec[i])
> }
> dim(mysample)<-c(3*NN,1)
> plot(density(mysample,kern="gaussian"),lwd=2,col=300)
> ## The commented lines are parts of the 1st version of this script
> 
> #x<-matrix(ncol=2,nrow=three*NN)
> #x[ ,1]<-mysample[]
> #x[ ,2]<-1
> y<-as.data.frame(mysample)
> mixgroup(y)
> z<-mixgroup(y,breaks=100)
> plot(z)
> #ini<-read.table("gaupar.TXT")
> ini<-mixparam(mu = c(24, 45, 73), sigma = c(5, 4, 4),pi=c(0.4,0.3,0.3))
> mymix<-mix(z,ini,"norm")
> summary(mymix)
> plot(mymix)
> print("So far so good")
> 
> 
> However, you can be in a situation in which you cannot sample the
> distribution that easily (e.g. you do not have stock quotations
> below/above a certain price) and then the same tool does not work that
> well. Consider for instance the same case as above but removing all
> the data below 15 and above 65:
> 
> # R code used to fit a sum of three normal distributions
> rm(list=ls())
> library(mixdist)
> set.seed(123)
> NN<-10000
> three<-3 #number of gaussian sequences I want to generate
> mysample<-matrix(nrow=NN,ncol=three)
> sdvec<-seq(1:three)
> sdvec[1]<-4
> sdvec[2]<-3
> sdvec[3]<-3.5
> meanvec<-seq(1:three)
> meanvec[1]<-20
> meanvec[2]<-50
> meanvec[3]<-70
> 
> for (i in 1:three)
> {
>  mysample[ ,i] <-rnorm(NN,meanvec[i],sdvec[i])
> }
> dim(mysample)<-c(3*NN,1)
> ##  now I am going to remove some data and see if I can still
> meaningfully optimize the
> ## distribution
> N3=3*NN
> mycount=0
> for (i in 1:N3)
> {if (mysample[i]>22 & mysample[i]<67)
> {
> mycount=mycount+1
> if (mycount ==1 )
> {
> mysample2<-cbind(mysample[i])
> }
> else
> {
> mysample2<-cbind(mysample2,mysample[i])
> }
> 
> }
> 
> }
> 
> plot(density(mysample,kern="gaussian"),lwd=2,col=300)
> ## The commented lines are parts of the 1st version of this script
> 
> #x<-matrix(ncol=2,nrow=three*NN)
> #x[ ,1]<-mysample[]
> #x[ ,2]<-1
> # y<-as.data.frame(mysample)
> mysample3<-mysample2[1:15000]
> y<-as.data.frame(mysample3)
> mixgroup(y)
> z<-mixgroup(y,breaks=100)
> plot(z)
> #ini<-read.table("gaupar.TXT")
> ini<-mixparam(mu = c(24, 45, 73), sigma = c(5, 4, 4),pi=c(0.4,0.3,0.3))
> mymix<-mix(z,ini,"norm")
> summary(mymix)
> plot(mymix)
> print("So far so good")
> 
> 
> Then the mixdist completely misses the means of the 2 external modes.
> I am sure that, apart from the combination of three Gaussians, this
> situation cannot be "new" at all.
> I am thinking about some iteration technique (e.g. you fit the data as
> well as you can with mixdist to start with, then according to your
> fitted parameters you generate the "missing"  results, then you re-fit
> the whole new set of data (real ones and those freshly generated) and
> so on hoping to get some convergent results).
> But it seems a bit cumbersome (furthermore, how many of them should I
> generate?) plus I am sure that solving this problem from scratch is
> re-inventing the wheel.
> Sorry for the long email.
> Kind Regards
> 
> Lorenzo Isella
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From lorenzo.isella at gmail.com  Sun May 21 02:10:43 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Sun, 21 May 2006 02:10:43 +0200
Subject: [R-sig-Finance] Levenberg-Marquardt
Message-ID: <1148170243.4895.8.camel@quez.DomName>

Dear All,
In doing some fittings I tried several R packages and, for my specific
problem, I get good results with the minpack.lm library, which
implements the Levenberg-Marquardt (LM) algorithm for nonlinear
least-square optimization.
However, the nls.lm package does not allow the user to set any constrain
(and I would need mainly some linear constrains) on the fitting
parameters.
Do you know about any package in R implementing constrains in LM?
Unfortunately, nls did not work very well with my set of data.
Cheers

Lorenzo


From edd at debian.org  Sun May 21 03:00:15 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 20 May 2006 20:00:15 -0500
Subject: [R-sig-Finance] Levenberg-Marquardt
In-Reply-To: <1148170243.4895.8.camel@quez.DomName>
References: <1148170243.4895.8.camel@quez.DomName>
Message-ID: <17519.48031.381109.618421@basebud.nulle.part>


On 21 May 2006 at 02:10, Lorenzo Isella wrote:
| Dear All,
| In doing some fittings I tried several R packages and, for my specific
| problem, I get good results with the minpack.lm library, which
| implements the Levenberg-Marquardt (LM) algorithm for nonlinear
| least-square optimization.
| However, the nls.lm package does not allow the user to set any constrain
| (and I would need mainly some linear constrains) on the fitting
| parameters.
| Do you know about any package in R implementing constrains in LM?
| Unfortunately, nls did not work very well with my set of data.

I continue to be surprised by your apparent lack of success when it comes to
finding the already supplied documentation. Try

	> ?optim
	> help.search("optimization")

optim() may well be your friend here as it implements several algorithms
including box-constrained optimization.

Also, and now with my listmaster head on, allow me to remind you that this
list is meant to finance-related matters.  Neither this post, nor your
preceding, and repeated, posts about multi-modal density estimation are
concerned with finance per se.  

Therefore, I would suggest that you send those questions to r-help instead.

Thanks, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From ma.yuchao at gmail.com  Sun May 21 03:39:10 2006
From: ma.yuchao at gmail.com (ma yuchao)
Date: Sun, 21 May 2006 09:39:10 +0800
Subject: [R-sig-Finance] [R]how to estimate adding-regression GARCH Model
Message-ID: <ab4114ba0605201839m2ac05c5epfd3718b0838430b7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060521/801d4018/attachment.pl 

From wuertz at itp.phys.ethz.ch  Mon May 22 13:40:57 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon, 22 May 2006 13:40:57 +0200
Subject: [R-sig-Finance] [R]how to estimate adding-regression GARCH Model
In-Reply-To: <ab4114ba0605201839m2ac05c5epfd3718b0838430b7@mail.gmail.com>
References: <ab4114ba0605201839m2ac05c5epfd3718b0838430b7@mail.gmail.com>
Message-ID: <4471A349.8010300@itp.phys.ethz.ch>

The supported models are restircted to those listed in the help page.
For the family of GARCH models these are in its moste general form

    ARMA(m,n)-APARCH(p,q).

I have not considered to add regression.

Diethelm Wuertz


   



ma yuchao wrote:

> Hello, R people:
>
>     I have a question in using fSeries package--the funciton garchFit and
>garchOxFit
>  if adding a regression to the  mean formula, how to estimate the model in
>R? using garchFit or garchOxFit?
>   For example, Observations is {x,y}_t,there may be some relation between x
>and y.
>   the model is
>    y_t=gamma0 + *gamma1*x_t*+psi*e_{t-1}+e_t       the gamma1*x_t is
>regression.
>    e_t=sqrt(h_t)*N(0,1)
>    h_t=alpha0+alpha1*e_t^2+beta*h_{t_1}~~~~~~~GARCH(1,1).
>    I  didn't know how to estimate the model using function garchFit or
>garchOxFit or other functions?    because the argument in
>garchFit/garchOxFit is formular.mean=~arma(1,1).
>
>   Do you have some instrucitons?
>   thank you very much for you help.
>
>Best wishes
>
>Ma Yuchao
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From ma.yuchao at gmail.com  Mon May 22 15:56:17 2006
From: ma.yuchao at gmail.com (ma yuchao)
Date: Mon, 22 May 2006 21:56:17 +0800
Subject: [R-sig-Finance] [R]how to estimate adding-regression GARCH Model
In-Reply-To: <ab4114ba0605220654r4537c266v7d130ccad0b255c5@mail.gmail.com>
References: <ab4114ba0605201839m2ac05c5epfd3718b0838430b7@mail.gmail.com>
	<4471A349.8010300@itp.phys.ethz.ch>
	<ab4114ba0605220654r4537c266v7d130ccad0b255c5@mail.gmail.com>
Message-ID: <ab4114ba0605220656s26e3a8e1n3f8d815fb1708d85@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060522/5ae59fe7/attachment.pl 

From jladekarl at worldbank.org  Mon May 22 16:23:25 2006
From: jladekarl at worldbank.org (jladekarl at worldbank.org)
Date: Mon, 22 May 2006 10:23:25 -0400
Subject: [R-sig-Finance] Rolling correlations with zoo object
Message-ID: <OF6B1C4695.AB33D324-ON85257176.004D477A-85257176.004F0C66@worldbank.org>

I'm a novice user of R so, please, excuse me for asking a basic question I hope
there is a basic answer to:

I have tried to created a correlation matrix using a "zoo object".  I don't have
any issues using the cor("zoo data") function, but when I try to expand the
process by introducing rolling correlations I run into problems. I have created
a workaround using "rapply" specifying the exact variables to use, rather than
the general zoo object, but it's not very practical solution for large matrixes.
Any hints?

Jeppe

##Connect to Bloomberg through session "conn"
conn <- blpConnect ()

##download currency data for the last 3 years into a zoo object called "data"
data <- blpGetData(conn, c("EUR Curncy", "CHF Curncy"), "PX Last",
start=as.chron(Sys.time() - 86400 * (3*365)))

##End Bloomberg Session "conn"
blpDisconnect(conn)

#plot running correlation estimated by last 30 observations rolling by "1"
EUR.chf.cor <- rapply(data[,c("EUR CURNCY", "CHF CURNCY")], 30,
function(x) cor(x[,1], x[,2]), by = 1, by.column = FALSE)
plot(EUR.chf.cor)


From ggrothendieck at gmail.com  Mon May 22 17:24:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 22 May 2006 11:24:23 -0400
Subject: [R-sig-Finance] Rolling correlations with zoo object
In-Reply-To: <OF6B1C4695.AB33D324-ON85257176.004D477A-85257176.004F0C66@worldbank.org>
References: <OF6B1C4695.AB33D324-ON85257176.004D477A-85257176.004F0C66@worldbank.org>
Message-ID: <971536df0605220824h11a8cb00gb41c653c00cd85e2@mail.gmail.com>

rollmean is much faster than rapply so use the fact that we
can calculate correlations in terms of means, means of squares
and means of cross products.   Doing this appears to be 4.6x
faster for this example:

> library(zoo)
> set.seed(1)
> n <- 10000
> x <- zoo(rnorm(n))
> y <- zoo(rnorm(n))
>
> x. <- rollmean(x, 30)
> y. <- rollmean(y, 30)
>
> xx. <- rollmean(x*x, 30)
> yy. <- rollmean(y*y, 30)
>
> xy. <- rollmean(x*y, 30)
>
> system.time(out <- (xy. - x. * y.) / sqrt((xx. - x.^2) * (yy. - y.^2)))
[1] 0.94 0.12 1.45   NA   NA
>
> system.time(out.rapply <- rapply(cbind(x,y), 30, cor, by.column = FALSE)[,3])
[1]  4.35  0.34 16.21    NA    NA
>
> all.equal(out, out.rapply)
[1] TRUE
>
> R.version.string # XP
[1] "R version 2.2.1, 2005-12-20"


On 5/22/06, jladekarl at worldbank.org <jladekarl at worldbank.org> wrote:
> I'm a novice user of R so, please, excuse me for asking a basic question I hope
> there is a basic answer to:
>
> I have tried to created a correlation matrix using a "zoo object".  I don't have
> any issues using the cor("zoo data") function, but when I try to expand the
> process by introducing rolling correlations I run into problems. I have created
> a workaround using "rapply" specifying the exact variables to use, rather than
> the general zoo object, but it's not very practical solution for large matrixes.
> Any hints?
>
> Jeppe
>
> ##Connect to Bloomberg through session "conn"
> conn <- blpConnect ()
>
> ##download currency data for the last 3 years into a zoo object called "data"
> data <- blpGetData(conn, c("EUR Curncy", "CHF Curncy"), "PX Last",
> start=as.chron(Sys.time() - 86400 * (3*365)))
>
> ##End Bloomberg Session "conn"
> blpDisconnect(conn)
>
> #plot running correlation estimated by last 30 observations rolling by "1"
> EUR.chf.cor <- rapply(data[,c("EUR CURNCY", "CHF CURNCY")], 30,
> function(x) cor(x[,1], x[,2]), by = 1, by.column = FALSE)
> plot(EUR.chf.cor)
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From Achim.Zeileis at wu-wien.ac.at  Mon May 22 16:55:43 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 22 May 2006 16:55:43 +0200
Subject: [R-sig-Finance] Rolling correlations with zoo object
In-Reply-To: <OF6B1C4695.AB33D324-ON85257176.004D477A-85257176.004F0C66@worldbank.org>
References: <OF6B1C4695.AB33D324-ON85257176.004D477A-85257176.004F0C66@worldbank.org>
Message-ID: <20060522165543.e65668cb.Achim.Zeileis@wu-wien.ac.at>

On Mon, 22 May 2006 10:23:25 -0400 jladekarl at worldbank.org wrote:

> I'm a novice user of R so, please, excuse me for asking a basic
> question I hope there is a basic answer to:
> 
> I have tried to created a correlation matrix using a "zoo object".  I
> don't have any issues using the cor("zoo data") function, but when I
> try to expand the process by introducing rolling correlations I run
> into problems. I have created a workaround using "rapply" specifying
> the exact variables to use, rather than the general zoo object, but
> it's not very practical solution for large matrixes. Any hints?

Mmmmh, I'm not sure what exactly you would like to compute...but I
guess you want to have the lower triangle of the correlation matrix,
i.e., all pairwise correlations, right? If this is what you want, you
could set up a function for that, e.g.:

  lcor <- function(x, ...)
  {
    rval <- cor(x, ...)
    rval <- rval[lower.tri(rval)]
    if(!is.null(colnames(x))) {
      nam <- outer(colnames(x), colnames(x),
        function(a, b) paste(a, b, sep = "."))
      names(rval) <- nam[lower.tri(nam)]
    }
    return(rval)
  }

which returns the lower triangular of cor(x) with name formatting if
available.

> ##Connect to Bloomberg through session "conn"
> conn <- blpConnect ()
[...]

Thanks for providing this example. Note however, that Bloomberg is not
freely available which renders the example irreproducible for some of
us.

A simpler example which might do what you want:
  ## generate data
  z <- zoo(matrix(rnorm(120), ncol = 3))
  colnames(z) <- c("EUR", "CHF", "USD")  
  ## full sample estimates
  cor(z)
  ## lower triangle only
  lcor(z)
  ## rolling estimates
  rapply(z, 30, lcor, by.column = FALSE)

hth,
Z


From ggrothendieck at gmail.com  Mon May 22 17:44:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 22 May 2006 11:44:43 -0400
Subject: [R-sig-Finance] Rolling correlations with zoo object
In-Reply-To: <971536df0605220824h11a8cb00gb41c653c00cd85e2@mail.gmail.com>
References: <OF6B1C4695.AB33D324-ON85257176.004D477A-85257176.004F0C66@worldbank.org>
	<971536df0605220824h11a8cb00gb41c653c00cd85e2@mail.gmail.com>
Message-ID: <971536df0605220844k5a296a9by2c11fffe9f2ceab4@mail.gmail.com>

Sorry.  I did not enclose everything in system.time so the timing
was incomplete.  Here it is again.  Doing it properly like this
gives a 2.7x speedup.


set.seed(1)
n <- 10000
x <- zoo(rnorm(n))
y <- zoo(rnorm(n))

system.time({
	x. <- rollmean(x, 30)
	y. <- rollmean(y, 30)

	xx. <- rollmean(x*x, 30)
	yy. <- rollmean(y*y, 30)

	xy. <- rollmean(x*y, 30)

	corr <- (xy. - x. * y.) / sqrt((xx. - x.^2) * (yy. - y.^2))
})

system.time(corr.rapply <- rapply(cbind(x,y), 30, cor, by.column = FALSE)[,3])

all.equal(corr, corr.rapply)




On 5/22/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> rollmean is much faster than rapply so use the fact that we
> can calculate correlations in terms of means, means of squares
> and means of cross products.   Doing this appears to be 4.6x
> faster for this example:
>
> > library(zoo)
> > set.seed(1)
> > n <- 10000
> > x <- zoo(rnorm(n))
> > y <- zoo(rnorm(n))
> >
> > x. <- rollmean(x, 30)
> > y. <- rollmean(y, 30)
> >
> > xx. <- rollmean(x*x, 30)
> > yy. <- rollmean(y*y, 30)
> >
> > xy. <- rollmean(x*y, 30)
> >
> > system.time(out <- (xy. - x. * y.) / sqrt((xx. - x.^2) * (yy. - y.^2)))
> [1] 0.94 0.12 1.45   NA   NA
> >
> > system.time(out.rapply <- rapply(cbind(x,y), 30, cor, by.column = FALSE)[,3])
> [1]  4.35  0.34 16.21    NA    NA
> >
> > all.equal(out, out.rapply)
> [1] TRUE
> >
> > R.version.string # XP
> [1] "R version 2.2.1, 2005-12-20"
>
>
> On 5/22/06, jladekarl at worldbank.org <jladekarl at worldbank.org> wrote:
> > I'm a novice user of R so, please, excuse me for asking a basic question I hope
> > there is a basic answer to:
> >
> > I have tried to created a correlation matrix using a "zoo object".  I don't have
> > any issues using the cor("zoo data") function, but when I try to expand the
> > process by introducing rolling correlations I run into problems. I have created
> > a workaround using "rapply" specifying the exact variables to use, rather than
> > the general zoo object, but it's not very practical solution for large matrixes.
> > Any hints?
> >
> > Jeppe
> >
> > ##Connect to Bloomberg through session "conn"
> > conn <- blpConnect ()
> >
> > ##download currency data for the last 3 years into a zoo object called "data"
> > data <- blpGetData(conn, c("EUR Curncy", "CHF Curncy"), "PX Last",
> > start=as.chron(Sys.time() - 86400 * (3*365)))
> >
> > ##End Bloomberg Session "conn"
> > blpDisconnect(conn)
> >
> > #plot running correlation estimated by last 30 observations rolling by "1"
> > EUR.chf.cor <- rapply(data[,c("EUR CURNCY", "CHF CURNCY")], 30,
> > function(x) cor(x[,1], x[,2]), by = 1, by.column = FALSE)
> > plot(EUR.chf.cor)
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
>


From joe-byers at utulsa.edu  Tue May 23 05:29:08 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Mon, 22 May 2006 22:29:08 -0500
Subject: [R-sig-Finance] [R-sig-finance] Using summary function from and
 armafit with R2HTML
In-Reply-To: <446BD226.40706@pdf.com>
References: <446800AE.2050707@utulsa.edu>	<17513.10223.449136.970354@basebud.nulle.part>
	<446AAE56.1080501@utulsa.edu> <446BD226.40706@pdf.com>
Message-ID: <44728184.2060400@utulsa.edu>

Spencer,

Thank you for you suggestion, but that did not work.  It is in way the 
summary function uses cat to produce the pretty output.  R2HTML does not 
see to be able to reroute the report.  I create my own 
summary.fARMA.html function to produce the report.

I appreciate the help and hope the code below is useful for other 
people.  If Rmetrics would like to include this code please be my 
guest.  I just would like a citation so I can put it in my vita. :-)

Joe

The function is
______________________________________________________________________________
##Created by Joe W. Byers, The University of Tulsa and FinancialSEAL
##This code is available under current GPL Licenses of R and Rmetrics

summary.fARMA.HTML<-function (object, doplot = FALSE, ...)
{
    ans <- NULL
    digits <- max(5, getOption("digits") - 4)
    x <- object
    object <- x at fit
    ans$call <- object$call
    ans$tsmodel <- object$tstitle
    ans$residuals <- as.vector(na.omit(object$residuals))
    if (length(ans$residuals) == 0) {
        ans$var <- 0
    }
    if (length(ans$residuals) > 0) {
        ans$var <- var(ans$residuals)
    }
    ans$sigma2 <- object$sigma2
    tval <- object$coef/object$se.coef
    prob <- 2 * (1 - pnorm(abs(tval)))
    ans$coefmat <- cbind(format(object$coef,digits=digits), 
format(object$se.coef,digits=digits),
        format(tval,digits=digits), prob=format.pval(prob,digits=digits))
    dimnames(ans$coefmat) <- list(names(object$coef), c(" Estimate",
        " Std. Error", " t value", "Pr(>|t|)"))
    row.names(ans$coefmat)<-toupper(row.names(ans$coefmat))
    if (object$tsmodel == "ar") {
        ans$aic <- (object$n.used * (1 + log(2 * pi)) + object$n.used *
            log(ans$var) + 2 * length(object$coef))
    }
    if (object$tsmodel == "arma") {
        ans$aic <- (object$n.used * (1 + log(2 * pi)) + object$n.used *
            log(ans$var) + 2 * length(object$coef))
        ans$css <- object$css
    }
    if (object$tsmodel == "arima") {
        ans$aic <- object$aic
        ans$loglik <- object$loglik
    }
    if (object$tsmodel == "fracdiff") {
        doplot <- FALSE
    }
    HTML("Title: ")
    HTML(x at title)
    HTML("Call: ")
    HTML(object$call)
    HTML(c("Model: ", object$tstitle))#, "", sep = "")
    HTML("Coefficient(s):")
    digits <- max(5, getOption("digits") - 4)
    t1<-data.frame(object$coef)#copy to dataframe
    t1<-data.frame(t(t1)) #traspose for reporting
    names(t1)<-toupper(names(t1))
    row.names(t1)<-" " # rename row name
    HTML(t1,digits=digits)
    #HTML(print.default(format(object$coef, digits = digits), print.gap 
= 2, quote = FALSE))
    digits <- max(5, getOption("digits") - 4)
    if (length(object$residuals) > 2) {
        HTML("Residuals:")
        rq <- as.data.frame(t(structure(quantile(ans$residuals), names = 
c("Min",
            "1Q", "Median", "3Q", "Max"))))
        row.names(rq)<-' '
        HTML(rq,digits=digits)
        HTML("Moments: ")
        skewness <- sum((ans$residuals - 
mean(ans$residuals))^3/sqrt(var(ans$residuals))^3)/length(ans$residuals)
        kurtosis <- sum((ans$residuals - 
mean(ans$residuals))^4/var(ans$residuals)^2)/length(ans$residuals) -
            3
        stats <- as.data.frame(t(structure(c(skewness, kurtosis), names 
= c("Skewness",
            "Kurtosis"))))
        row.names(stats)<-" "
        HTML(stats,digits=digits)
    }
    HTML("Coefficient(s):")
    signif.stars <- getOption("show.signif.stars")
    #HTML(printCoefmat(ans$coefmat, digits=digits, signif.stars = 
signif.stars,    ...))
    HTML(ans$coefmat, digits=digits, signif.stars = signif.stars)
    if (x at fit$tsmodel == "ar") {
        t1<-data.frame(c(format(object$sigma2, digits = 
digits),format(round(object$aic, digits))),
            row.names=c("sigma^2 estimated as:       ","AIC 
Criterion:              "))
        names(t1)<-" "
        HTML(t1)
    }
    if (x at fit$tsmodel == "arma") {
        t1<-data.frame(c(format(object$sigma2, digits = 
digits),format(round(object$css, digits = digits))),
            row.names=c("sigma^2 estimated as:       ", "Conditional 
Sum-of-Squares: "))
            names(t1)<-" "
        HTML(t1)
    }
    if (x at fit$tsmodel == "arima") {
        cm <- object$call$method
        if (is.null(cm) || cm != "CSS") {
            t1<-data.frame(c(format(object$sigma2, digits = digits), 
format(round(object$loglik, digits)),
                format(round(object$aic, digits))),
                row.names=c("sigma^2 estimated as:       ", "log 
likelihood:       ",
                "AIC Criterion:        ",))
            names(t1)<-" "
            HTML(t1)
        }
        else {
            t1<-data.frame(c(format(object$sigma2, digits = digits), 
format(round(object$loglik, digits))),
                row.names=c("sigma^2 estimated as:       ", "log 
likelihood:       "))
            names(t1)<-" "
            HTML(t1)
        }
    }
    if (doplot)
        plot.fARMA(x, ...)
    HTML(c("Description: ",x at description))
    invisible()
}
_____________________________________________________________________________________

Spencer Graves wrote:
>       Have you tried the following:
>
> htmp(summary(result,doplot=false))
>
>       If this fails, please provide a simple, self-contained 
> (replicatable) example, as suggested in the posting guide! 
> "www.R-project.org/posting-guide.html".  Please include the package 
> containing the "HTML" function.  I have replied to many posts 
> regarding functions I've not previously used.  If you supply a simple, 
> replicatable example, someone like me can try different things.  This 
> will increase your chances that the reply will actually contain 
> working code.
>
>       hope this help,
>       Spencer Graves
>
> Joe Byers wrote:
>> Has anyone used the summary function for an armafit object with R2HTML?
>>
>> For example
>> result<-armaFit(data~arma(2,0), include.mean);
>> summary(result,doplot=false); # produces a full output of the results 
>> including coef stats, AIC, LL, and residual stats.
>> HTML(result); # produces on the model information and coef estimates.
>>
>> I need the summary functions results printed to my HTML file.  I use 
>> the HTMLInitFile and need the insert graph capabilities.  Using 
>> HTMLStart does not provide the results I want.  Mainly I do not want 
>> a dump of my code as well.
>>
>> Thank you
>> Joe
>>
>> _______________________________________________
>> R-sig-finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 295 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060522/8a9961f8/attachment.vcf 

From david.fiorino at abnamro.com  Wed May 24 18:04:14 2006
From: david.fiorino at abnamro.com (david.fiorino at abnamro.com)
Date: Wed, 24 May 2006 11:04:14 -0500
Subject: [R-sig-Finance] IMM Dates
Message-ID: <OF316C45D7.70E24CA1-ON86257178.0057F966-86257178.00584784@abnamro.com>


Before I reinvent the wheel, does anyone know of functionality that
calculates the next IMM settlement date given today's date?

Thanks - David
---------------------------------------------------------------------------
This message (including any attachments) is confidential and...{{dropped}}


From Robert at sanctumfi.com  Wed May 24 18:30:33 2006
From: Robert at sanctumfi.com (Robert Sams)
Date: Wed, 24 May 2006 17:30:33 +0100
Subject: [R-sig-Finance] IMM Dates
Message-ID: <E585EABA11227445B918BFB74C1A4D36545DEA@sanctum01.sanctumfi.com>

Hi David, 

This will give you the IMM date (3rd wednesday of the month) for a given integer month and year. A little hacking on it will yield exactly what you want. 

Cheers,
Robert

dcIMMDate <- function(year, month, ...){
  dcTuple <- function(x){
    lt <- as.POSIXlt(x)
    year <- lt$year + 1900
    month <- lt$mon + 1
    mday <- lt$mday
    wday <- lt$wday
    if(length(x)!=1){
      return(matrix(c(year,month,mday,wday),ncol=4))
    }
    else{
      return(c(year,month,mday,wday))
    }
  }
  dcDatePart <- function(x, part){ 
    parts <- c("year", "month", "mday", "wday")
    if(!class(x)[1] %in% c("Date", "chron", "POSIXt")){
      return(x)
    }
    if(is.character(part)){
      part <- which(parts == part)
    }
    if(length(x) == 1){
      return(dcTuple(x)[part])
    }
    else{
      return(dcTuple(x)[, part])
    }
  }
  dcNthDayOfWeek <- function(x, dow, n){
    m <- dcDatePart(x, "month")
    y <- dcDatePart(x, "year")
    dow1 <- dcDatePart(as.Date(ISOdate(y, m, 1)), "wday")
    return((n -  1) * 7 + (dow - dow1) %% 7 + x)
  }
  x <- as.Date(ISOdate(year, month, 1))
  x <- dcNthDayOfWeek(x, n = 3, dow = 3)
}


> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of
> david.fiorino at abnamro.com
> Sent: Wednesday, May 24, 2006 5:04 PM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-sig-Finance] IMM Dates
> 
> 
> 
> Before I reinvent the wheel, does anyone know of functionality that
> calculates the next IMM settlement date given today's date?
> 
> Thanks - David
> --------------------------------------------------------------
> -------------
> This message (including any attachments) is confidential\ ...{{dropped}}


From ggrothendieck at gmail.com  Wed May 24 22:17:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 24 May 2006 16:17:54 -0400
Subject: [R-sig-Finance] IMM Dates
In-Reply-To: <OF316C45D7.70E24CA1-ON86257178.0057F966-86257178.00584784@abnamro.com>
References: <OF316C45D7.70E24CA1-ON86257178.0057F966-86257178.00584784@abnamro.com>
Message-ID: <971536df0605241317i69b16e72qf517a3eb5f3e5f20@mail.gmail.com>

If x is a "Date" object then the first line in the body
of thirdwed calculates the "Date" class date of the
first of x's month.

Now, suppose n = 1 if d is on Wed, n = 2 if d is on Tue,
..., n = 6 if d is on Thu.   The correspondence between
the usual encoding (0=Sun, ..., 6=Sat) and this encoding
is given in the second line and in the third line we
calculate the date of the third Wed.

Following that we have nexthirdwed which calls
thirdwed adding 30 days to the result if the
result is before x (to get us the third wed in
the following month):

thirdwed <- function(x) {
   d <- x - as.POSIXlt(x)$mday + 1
   n <- (3-as.POSIXlt(d)$wday) %% 7 + 1
   d + 14 + n - 1
}

nextthirdwed <- function(x) {
  y <- thirdwed(x)
  thirdwed(y + 30 * (y < x))
}

nextthirdwed(as.Date(c("2006-04-05", "2006-04-19", "2006-04-30")))


On 5/24/06, david.fiorino at abnamro.com <david.fiorino at abnamro.com> wrote:
>
> Before I reinvent the wheel, does anyone know of functionality that
> calculates the next IMM settlement date given today's date?
>
> Thanks - David
> ---------------------------------------------------------------------------
> This message (including any attachments) is confidential and...{{dropped}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From anass.mouhsine at sgcib.com  Wed May 31 14:48:03 2006
From: anass.mouhsine at sgcib.com (anass.mouhsine at sgcib.com)
Date: Wed, 31 May 2006 14:48:03 +0200
Subject: [R-sig-Finance] garchFit and NAs
Message-ID: <OFE8630A34.BFEEBC30-ONC125717F.00457B61-C125717F.0046517C@fr.world.socgen>

Hi everybody,

When trying to fit an ARMA(0,2)-APARCH(1,1) model to a timeseries, it
results in the following warning message:

NaNs produced in: sqrt(diag(fit$cvar))

the print function gives the following result

_______________________________________________________________________
Title:
 GARCH Modelling

Call:
 garchFit(formula.mean = ~arma(0, 2), formula.var = ~aparch(1,
    1), series = seriemul, cond.dist = "dsged")

Mean and Variance Equation:
 ~arma(0, 2) + ~aparch(1, 1)

Conditional Distribution:
 dsged

Coefficient(s):
       mu        ma1        ma2      omega     alpha1     gamma1
-0.478628   0.356290   0.054928   1.472516   0.944778   0.418131
    beta1      delta       skew      shape
 0.306527   1.912457   0.541992   1.304578

Error Analysis:
        Estimate  Std. Error  t value Pr(>|t|)
mu     -0.478628    0.066091   -7.242 4.42e-13 ***
ma1     0.356290    0.096042    3.710 0.000207 ***
ma2     0.054928    0.055839    0.984 0.325272
omega   1.472516    0.223275    6.595 4.25e-11 ***
alpha1  0.944778    0.213975    4.415 1.01e-05 ***
gamma1  0.418131    0.094418    4.428 9.49e-06 ***
beta1   0.306527          NA       NA       NA
delta   1.912457          NA       NA       NA
skew    0.541992    0.004978  108.879  < 2e-16 ***
shape   1.304578          NA       NA       NA
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Log Likelihood:
 764.6944    normalized:  2.044638
_______________________________________________________________________

My question is:
--> How are these NaNs produced?
--> How can we read and interpret the NAs in the result?

Thank you in advance for your help,

Anass

*************************************************************************
This message and any attachments (the "message") are confide...{{dropped}}


From pattonjava at yahoo.com  Thu Jun  1 00:29:26 2006
From: pattonjava at yahoo.com (Jonathan Patton)
Date: Wed, 31 May 2006 15:29:26 -0700 (PDT)
Subject: [R-sig-Finance] Testing technical indicators
Message-ID: <20060531222927.74132.qmail@web38013.mail.mud.yahoo.com>

I am a programmer and have downloaded and installed R
and the fExtremes package. I am trying to test various
ideas that I have about the stock market. I've looked
through the function reference inside rMetrics and
there looks to be a lot of useful functions there. I'm
having trouble though finding examples similar to what
I would like to do.

Initially, I would like to test a 5 and 10 day moving
average crossover system to see if there is any
correlation between the 5 day moving average crossing
the 10 day moving average. I'm just looking for some
general direction to get started. I would like to know
if this type of analysis is possible to begin with and
then go from there. R looks great and very powerful. 
I've written a small program in vb to do some of this
but was getting stuck when it came to backtesting.


From patrick at burns-stat.com  Thu Jun  1 10:41:41 2006
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 01 Jun 2006 09:41:41 +0100
Subject: [R-sig-Finance] Testing technical indicators
In-Reply-To: <20060531222927.74132.qmail@web38013.mail.mud.yahoo.com>
References: <20060531222927.74132.qmail@web38013.mail.mud.yahoo.com>
Message-ID: <447EA845.1070609@burns-stat.com>

The working paper "Random portfolios for evaluating
trading strategies" on the Burns Statistics website may
be of interest to you.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Jonathan Patton wrote:

>I am a programmer and have downloaded and installed R
>and the fExtremes package. I am trying to test various
>ideas that I have about the stock market. I've looked
>through the function reference inside rMetrics and
>there looks to be a lot of useful functions there. I'm
>having trouble though finding examples similar to what
>I would like to do.
>
>Initially, I would like to test a 5 and 10 day moving
>average crossover system to see if there is any
>correlation between the 5 day moving average crossing
>the 10 day moving average. I'm just looking for some
>general direction to get started. I would like to know
>if this type of analysis is possible to begin with and
>then go from there. R looks great and very powerful. 
>I've written a small program in vb to do some of this
>but was getting stuck when it came to backtesting.
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
>  
>


From dave at kanecap.com  Thu Jun  1 14:43:14 2006
From: dave at kanecap.com (David Kane)
Date: Thu, 1 Jun 2006 08:43:14 -0400
Subject: [R-sig-Finance] Testing technical indicators
In-Reply-To: <20060531222927.74132.qmail@web38013.mail.mud.yahoo.com>
References: <20060531222927.74132.qmail@web38013.mail.mud.yahoo.com>
Message-ID: <17534.57570.697279.100075@gargle.gargle.HOWL>

In terms of testing and managing equity portfolios, you may find our
"portfolio" package to be of use. There is an introductory article in
the latest issue of R News as well.

Dave Kane

Jonathan Patton writes:
 > I am a programmer and have downloaded and installed R
 > and the fExtremes package. I am trying to test various
 > ideas that I have about the stock market. I've looked
 > through the function reference inside rMetrics and
 > there looks to be a lot of useful functions there. I'm
 > having trouble though finding examples similar to what
 > I would like to do.
 > 
 > Initially, I would like to test a 5 and 10 day moving
 > average crossover system to see if there is any
 > correlation between the 5 day moving average crossing
 > the 10 day moving average. I'm just looking for some
 > general direction to get started. I would like to know
 > if this type of analysis is possible to begin with and
 > then go from there. R looks great and very powerful. 
 > I've written a small program in vb to do some of this
 > but was getting stuck when it came to backtesting.
 > 
 > _______________________________________________
 > R-SIG-Finance at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From bbands at gmail.com  Thu Jun  1 15:50:20 2006
From: bbands at gmail.com (BBands)
Date: Thu, 1 Jun 2006 06:50:20 -0700
Subject: [R-sig-Finance] Fwd:  Testing technical indicators
In-Reply-To: <6e8360ad0606010648p78e4f32brb44f72c2ee1390e9@mail.gmail.com>
References: <20060531222927.74132.qmail@web38013.mail.mud.yahoo.com>
	<6e8360ad0606010648p78e4f32brb44f72c2ee1390e9@mail.gmail.com>
Message-ID: <6e8360ad0606010650x22959159l6f91e4397ba5647b@mail.gmail.com>

On 5/31/06, Jonathan Patton <pattonjava at yahoo.com> wrote:
> I am a programmer and have downloaded and installed R
> and the fExtremes package. I am trying to test various
> ideas that I have about the stock market. I've looked
> through the function reference inside rMetrics and
> there looks to be a lot of useful functions there. I'm
> having trouble though finding examples similar to what
> I would like to do.
>
> Initially, I would like to test a 5 and 10 day moving
> average crossover system to see if there is any
> correlation between the 5 day moving average crossing
> the 10 day moving average. I'm just looking for some
> general direction to get started. I would like to know
> if this type of analysis is possible to begin with and
> then go from there. R looks great and very powerful.
> I've written a small program in vb to do some of this
> but was getting stuck when it came to backtesting.

The solution we use is to create the indicators and systems in Python,
pass the results of the test(s) to R for evaluation via Rpy. This has
proved to be a very robust approach that combines the strengths of
each package into a superior solution. We use gnuplot for 3D system
parameter graphics via gnuplot-py, but these days I gather that can be
done nicely in R as well.

     jab
--
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From jeff.a.ryan at gmail.com  Thu Jun  1 18:20:36 2006
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 1 Jun 2006 16:20:36 +0000 GMT
Subject: [R-sig-Finance] Fwd:  Testing technical indicators
In-Reply-To: <6e8360ad0606010650x22959159l6f91e4397ba5647b@mail.gmail.com>
References: <20060531222927.74132.qmail@web38013.mail.mud.yahoo.com><6e8360ad0606010648p78e4f32brb44f72c2ee1390e9@mail.gmail.com>
	<6e8360ad0606010650x22959159l6f91e4397ba5647b@mail.gmail.com>
Message-ID: <143925829-1149178985-cardhu_blackberry.rim.net-342319862-@bwe041-cell00.bisx.prod.on.blackberry>

I too have been looking for a more elegant solution than my current perl/R approach.

Jab - my current (yet uncoded) thinking has been to migrate everything except the raw data manipulation to R. I'd be interested to hear why you use python for some of your systems and/or test(s).

The only thing keeping me back is from a signal generation standpoint - seems to be easier to leave that outside of R.

Jeff
  

-----Original Message-----
From: BBands <bbands at gmail.com>
Date: Thu, 1 Jun 2006 06:50:20 
To:R-sig-finance <r-sig-finance at stat.math.ethz.ch>
Subject: [R-sig-Finance] Fwd:  Testing technical indicators

On 5/31/06, Jonathan Patton <pattonjava at yahoo.com> wrote:
> I am a programmer and have downloaded and installed R
> and the fExtremes package. I am trying to test various
> ideas that I have about the stock market. I've looked
> through the function reference inside rMetrics and
> there looks to be a lot of useful functions there. I'm
> having trouble though finding examples similar to what
> I would like to do.
>
> Initially, I would like to test a 5 and 10 day moving
> average crossover system to see if there is any
> correlation between the 5 day moving average crossing
> the 10 day moving average. I'm just looking for some
> general direction to get started. I would like to know
> if this type of analysis is possible to begin with and
> then go from there. R looks great and very powerful.
> I've written a small program in vb to do some of this
> but was getting stuck when it came to backtesting.

The solution we use is to create the indicators and systems in Python,
pass the results of the test(s) to R for evaluation via Rpy. This has
proved to be a very robust approach that combines the strengths of
each package into a superior solution. We use gnuplot for 3D system
parameter graphics via gnuplot-py, but these days I gather that can be
done nicely in R as well.

     jab
--
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From bbands at gmail.com  Thu Jun  1 18:46:26 2006
From: bbands at gmail.com (BBands)
Date: Thu, 1 Jun 2006 09:46:26 -0700
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <143925829-1149178985-cardhu_blackberry.rim.net-342319862-@bwe041-cell00.bisx.prod.on.blackberry>
References: <20060531222927.74132.qmail@web38013.mail.mud.yahoo.com>
	<6e8360ad0606010648p78e4f32brb44f72c2ee1390e9@mail.gmail.com>
	<6e8360ad0606010650x22959159l6f91e4397ba5647b@mail.gmail.com>
	<143925829-1149178985-cardhu_blackberry.rim.net-342319862-@bwe041-cell00.bisx.prod.on.blackberry>
Message-ID: <6e8360ad0606010946u103d11fck959c429acf089eb6@mail.gmail.com>

On 6/1/06, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> I too have been looking for a more elegant solution than my current perl/R approach.
>
> Jab - my current (yet uncoded) thinking has been to migrate everything except the raw data manipulation to R. I'd be interested to hear why you use python for some of your systems and/or test(s).
>
> The only thing keeping me back is from a signal generation standpoint - seems to be easier to leave that outside of R.

Several factors lie behind that decision, but the most important is
that it is the easiest/fastest way for me to do things. I agree that
everything could be done in R, but it would take a huge amount of
effort for _me_ to get there. I did start on an R package for all the
important technical indicators and some classic systems, but my
progress was too slow to allow me to do anything important in the time
I had available. For example I ran into problems involving the details
of S3 and S4 classes which seemed very hard to solve and the package
its, which I relied on, was orphaned. As I already had these things
coded, debugged and working in Python, I just didn't have the
time/will to recode everything. The bottom line is I can code systems
very quickly in Python and relying on R for analytics rather than
coding the stats myself makes perfect sense to me.

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From bbands at gmail.com  Thu Jun  1 21:08:40 2006
From: bbands at gmail.com (BBands)
Date: Thu, 1 Jun 2006 12:08:40 -0700
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <17535.11543.203291.278671@gargle.gargle.HOWL>
References: <20060531222927.74132.qmail@web38013.mail.mud.yahoo.com>
	<6e8360ad0606010648p78e4f32brb44f72c2ee1390e9@mail.gmail.com>
	<6e8360ad0606010650x22959159l6f91e4397ba5647b@mail.gmail.com>
	<143925829-1149178985-cardhu_blackberry.rim.net-342319862-@bwe041-cell00.bisx.prod.on.blackberry>
	<6e8360ad0606010946u103d11fck959c429acf089eb6@mail.gmail.com>
	<17535.11543.203291.278671@gargle.gargle.HOWL>
Message-ID: <6e8360ad0606011208t3d71532fm84dfe2c8840e2fe6@mail.gmail.com>

On 6/1/06, David Kane <dave at kanecap.com> wrote:
> Have you considered open sourcing your Python code?

Yes, I had an open source project called Crusher, which Dirk worked on
as well. We got a good ways, but Dirk wanted to maintain his own
copyright on the portions of the core code he had contributed. I had
seen what chaos that could lead to when contributing to gnuplot and
felt that there should be a single master copyright on the core, so we
agreed to disagree. He freely contributed his code and I made one last
Crusher release under the GPL. After that I took the idea in house
where it is in use until this day. Of course I have learned a great
deal since then and would, and do, do things differently today, but
the core of Crusher is still quite viable. The last release was 0.0.4.

> If you did, I expect someone would translate it into R.

Are you volunteering?

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From walmir-rodrigues at uol.com.br  Thu Jun  1 21:36:26 2006
From: walmir-rodrigues at uol.com.br (walmir-rodrigues)
Date: Thu,  1 Jun 2006 16:36:26 -0300
Subject: [R-sig-Finance] Mixture
Message-ID: <J0754Q$0A55CA5B986F43DD1ECE812BED4894C5@uol.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060601/0a3f6aca/attachment.pl 

From pattonjava at yahoo.com  Thu Jun  1 23:22:44 2006
From: pattonjava at yahoo.com (Jonathan Patton)
Date: Thu, 1 Jun 2006 14:22:44 -0700 (PDT)
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <6e8360ad0606011208t3d71532fm84dfe2c8840e2fe6@mail.gmail.com>
Message-ID: <20060601212244.81417.qmail@web38006.mail.mud.yahoo.com>

I just wanted to quickly thank everyone for their
input. I've had an interest in the financial markets
for some time now but have not explored any
statistical analysis.  Your comments have been very
helpful. 

Is there a place where the last GPL version of Crusher
could be downloaded? I would be very interested in
that. 

Also, even though I have been a programmer for about
10 years now, I have done very little statistical
analysis. Could anyone recommend a good book to start
with on financial statistical analysis?

Thanks, 

Jonathan

--- BBands <bbands at gmail.com> wrote:

> On 6/1/06, David Kane <dave at kanecap.com> wrote:
> > Have you considered open sourcing your Python
> code?
> 
> Yes, I had an open source project called Crusher,
> which Dirk worked on
> as well. We got a good ways, but Dirk wanted to
> maintain his own
> copyright on the portions of the core code he had
> contributed. I had
> seen what chaos that could lead to when contributing
> to gnuplot and
> felt that there should be a single master copyright
> on the core, so we
> agreed to disagree. He freely contributed his code
> and I made one last
> Crusher release under the GPL. After that I took the
> idea in house
> where it is in use until this day. Of course I have
> learned a great
> deal since then and would, and do, do things
> differently today, but
> the core of Crusher is still quite viable. The last
> release was 0.0.4.
> 
> > If you did, I expect someone would translate it
> into R.
> 
> Are you volunteering?
> 
>      jab
> -- 
> John Bollinger, CFA, CMT
> www.BollingerBands.com
> 
> If you advance far enough, you arrive at the
> beginning.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From hkahra at gmail.com  Fri Jun  2 08:50:25 2006
From: hkahra at gmail.com (Hannu Kahra)
Date: Fri, 2 Jun 2006 09:50:25 +0300
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <20060601212244.81417.qmail@web38006.mail.mud.yahoo.com>
References: <6e8360ad0606011208t3d71532fm84dfe2c8840e2fe6@mail.gmail.com>
	<20060601212244.81417.qmail@web38006.mail.mud.yahoo.com>
Message-ID: <3d35a2ca0606012350m5d5e85f3rf823293e2cf0c907@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060602/27b10507/attachment.pl 

From bbands at gmail.com  Fri Jun  2 16:26:04 2006
From: bbands at gmail.com (BBands)
Date: Fri, 2 Jun 2006 07:26:04 -0700
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <002301c6863f$dc3091b0$0ac0a8c0@MightyMini>
References: <002301c6863f$dc3091b0$0ac0a8c0@MightyMini>
Message-ID: <6e8360ad0606020726h770dde0fy588baf014b5603d3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060602/445c4410/attachment.pl 

From whit at twinfieldscapital.com  Fri Jun  2 17:39:33 2006
From: whit at twinfieldscapital.com (Whit Armstrong)
Date: Fri, 2 Jun 2006 11:39:33 -0400
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <6e8360ad0606020726h770dde0fy588baf014b5603d3@mail.gmail.com>
Message-ID: <726FC6DD09DE1046AF81B499D70C3BCE2C8FAD@twinfields02.CORP.TWINFIELDSCAPITAL.COM>

I too have been down the backtesting road in R.  I would have
participated more in the discussion, but the CFA exam is this Saturday
(level 3 this time).  John, I can see you have been down that road too,
so you know how much time is involved in preparation.

> For me it goes like this. I use R as a calculation engine 
> rather than a programming environment. So any time I have a 
> calc that R seems like a natural resource for, I use R. That 
> include regressions, t tests, Chi Square, etc... I've found 
> no need to go to C or C++ as with a decent computer and a 
> modicum of memory R and Python are fast enough for my needs.
 
We have a package which allows users to write rules in R which are
evaluated by C++ routines via callbacks to R via eval.  It's messy messy
messy.  This package worked fine on daily data.  However, when we
started testing with 10 minute bars or 1 minute bars it became
impossibly slow.

I have never used Python, but I am curious.  Do you find that it works
well with higher frequency data.  Do you use it with intraday data at
all?

Thanks,
Whit

Here is an example script for a simple rsi divergence system.  The big
advantage of writing the rules in R is that you can make calls to
browser in your scripts which I find helps a lot in debugging.

library(tslib)
library(pl)

var.grid <-
expand.grid(rnk.win=seq(10,20,5),rsi.win=seq(14,50,20),p.target=seq(0.5,
2.5,0.5),p.stop=seq(0.5,2.5,0.5))
#var.grid <-
expand.grid(rnk.win=seq(10,40,5),rsi.win=14,p.target=1.0,p.stop=1.0)

rsi.system <- function(mkt) {

    # for position size
    notional <- 100*10^6
    risk.pct <- 0.001
    risk <- notional*risk.pct
    mkt.atr <- moving.avg(true.range(ps(mkt)),60)
    trade.size <- round(risk/(mkt.atr*com.factor(mkt)),0)
    #trade.size <- 100

    mkt.ps <- ps(mkt)
    mkt.ds.close <- ds(mkt)[,"close"]

    rsi.buff=2
    mkt.rsi <- rsi(mkt.ps,rsi.win)
    mkt.rsi.rnk <- rnk(mkt.rsi,rnk.win)

    new.high <- rnk(mkt.ps[,"high"],rnk.win)<2
    new.low <- rnk(mkt.ps[,"low"],rnk.win)>(rnk.win-1)

    # new high & not an rsi new high
    sell.sig <- new.high & mkt.rsi.rnk > (1 + rsi.buff)

    buy.sig <- new.low & mkt.rsi.rnk < (rnk.win - 1 - rsi.buff)

    buy.entry <- function() {
        #if(is.na(pl.pos()) || is.na(pl.value(sys.buy)))
        #    browser()

        if(pl.pos() <= 0 && pl.value(buy.sig)) {
            pl.new.trade(trade.size)
 
set.target(pl.value(mkt.ds.close)+pl.value(mkt.atr)*p.target)
            set.stop(pl.value(mkt.ds.close)-pl.value(mkt.atr)*p.stop)
        }
    }
   sell.entry <- function() {
        #if(is.na(pl.pos()) || is.na(pl.value(sys.buy)))
        #   browser()

        if(pl.pos() >= 0 && pl.value(sell.sig)) {
            pl.new.trade(-trade.size)
 
set.target(pl.value(mkt.ds.close)-pl.value(mkt.atr)*p.target)
            set.stop(pl.value(mkt.ds.close)+pl.value(mkt.atr)*p.stop)
        }
    }

    buy.rule <- list(entry=buy.entry)
    sell.rule <- list(entry=sell.entry)

    ans <- list(buy.rule=buy.rule,sell.rule=sell.rule)
    class(ans) <- "pl.system"
    ans
}

mkts <- scan("/home/whit/.std.mkt.list",what="")[-c(1:9)]

mkt.list <- lapply(mkts,lim.com)

sys.reports <- do.system(mkt.list,rsi.system,100,grid=var.grid)


From edd at debian.org  Fri Jun  2 18:22:06 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 2 Jun 2006 11:22:06 -0500
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <6e8360ad0606020726h770dde0fy588baf014b5603d3@mail.gmail.com>
References: <002301c6863f$dc3091b0$0ac0a8c0@MightyMini>
	<6e8360ad0606020726h770dde0fy588baf014b5603d3@mail.gmail.com>
Message-ID: <17536.26030.502037.580416@basebud.nulle.part>


On 2 June 2006 at 07:26, BBands wrote:
| I'll have a look for a copy--maybe Dirk still has one, but Crusher was not
| ready for prime time. At version 0.0.4 it had not been seriously debugged
| and I am sure that there were calc errors that had not been fixed yet. I had
| planned for basic usability at 0.1.0, but we didn't get that far. If I can't
| find it, I'll post some Python TA indicator code.

But Crusher comes from the pre-Rmetrics days. I'd suspect that you find as
much if not more code in the TA examples in Rmetrics.  Most of what I
contributed to Crusher was the Bollingerbands plotting function (now also on
the excellent R Graph gallery) I wrote to convince John to drop the fugly
gnuplot :)
 
| For me it goes like this. I use R as a calculation engine rather than a
| programming environment. So any time I have a calc that R seems like a
| natural resource for, I use R. That include regressions, t tests, Chi
| Square, etc... I've found no need to go to C or C++ as with a decent
| computer and a modicum of memory R and Python are fast enough for my needs.

I think Gabor once noted on one of the lists that he found that he, over
time, converged to R for the other tasks previously done outside R.  That's
true for me as well. My very first exposure to S-Plus involved piping to it
from Perl which talked to DBs etc pp.  These days I rarely have to go outside
of R --- other than to compiled code for performance reasons, and even then I
try to glue that code back to R in order to control its rich environment.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From bbands at gmail.com  Fri Jun  2 19:07:27 2006
From: bbands at gmail.com (BBands)
Date: Fri, 2 Jun 2006 10:07:27 -0700
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <17536.26030.502037.580416@basebud.nulle.part>
References: <002301c6863f$dc3091b0$0ac0a8c0@MightyMini>
	<6e8360ad0606020726h770dde0fy588baf014b5603d3@mail.gmail.com>
	<17536.26030.502037.580416@basebud.nulle.part>
Message-ID: <6e8360ad0606021007n74a4682xc549a54aa6818dd5@mail.gmail.com>

On 6/2/06, Dirk Eddelbuettel <edd at debian.org> wrote:
> I wrote to convince John to drop the fugly gnuplot :)

Not sure what 'fugly' means, but I am sure that I love gnuplot. ;-)

      jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From bbands at gmail.com  Fri Jun  2 19:57:33 2006
From: bbands at gmail.com (BBands)
Date: Fri, 2 Jun 2006 10:57:33 -0700
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <17536.26030.502037.580416@basebud.nulle.part>
References: <002301c6863f$dc3091b0$0ac0a8c0@MightyMini>
	<6e8360ad0606020726h770dde0fy588baf014b5603d3@mail.gmail.com>
	<17536.26030.502037.580416@basebud.nulle.part>
Message-ID: <6e8360ad0606021057i42a6afccwc38859c5bb88e2ec@mail.gmail.com>

On 6/2/06, Dirk Eddelbuettel <edd at debian.org> wrote:
> But Crusher comes from the pre-Rmetrics days. I'd suspect that you find as
> much if not more code in the TA examples in Rmetrics.

Not true. Rmetrics is many things, but a TA package it is not, nor do
I think it was intended to be one. There are several TA indicators
included in fMultivar, so perhaps it is a beginning, but if it is a
beginning, it has a long ways to go before anything serious could be
done with it TA-wise. Perhaps I have missed something?

(Nothing in the above may be construed in any way as a criticism of
Mr. Wuertz's work.)

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From jeff.a.ryan at gmail.com  Fri Jun  2 21:18:35 2006
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 2 Jun 2006 14:18:35 -0500
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <6e8360ad0606021057i42a6afccwc38859c5bb88e2ec@mail.gmail.com>
References: <002301c6863f$dc3091b0$0ac0a8c0@MightyMini>
	<6e8360ad0606020726h770dde0fy588baf014b5603d3@mail.gmail.com>
	<17536.26030.502037.580416@basebud.nulle.part>
	<6e8360ad0606021057i42a6afccwc38859c5bb88e2ec@mail.gmail.com>
Message-ID: <e8e755250606021218l3f347205pf831c720d2d360c0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060602/ec7ee650/attachment.pl 

From bbands at gmail.com  Fri Jun  2 21:55:28 2006
From: bbands at gmail.com (BBands)
Date: Fri, 2 Jun 2006 12:55:28 -0700
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <e8e755250606021218l3f347205pf831c720d2d360c0@mail.gmail.com>
References: <002301c6863f$dc3091b0$0ac0a8c0@MightyMini>
	<6e8360ad0606020726h770dde0fy588baf014b5603d3@mail.gmail.com>
	<17536.26030.502037.580416@basebud.nulle.part>
	<6e8360ad0606021057i42a6afccwc38859c5bb88e2ec@mail.gmail.com>
	<e8e755250606021218l3f347205pf831c720d2d360c0@mail.gmail.com>
Message-ID: <6e8360ad0606021255x1b7d78dsea9266aa233dcfda@mail.gmail.com>

On 6/2/06, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> What would it take to build a set of wrappers in R to the ta-lib library
> referenced by John earlier ( http://sourceforge.net/projects/ta-lib )?
>
> That project seems quite complete and mature (although I have only had a
> chance to read through the online docs) - and seems easily callable from R
> (as it is all available as C/C++ from what I can tell)
>
> I haven't personally developed anything beyond what I use myself, but I
> wouldn't mind trying to give something back to R in the way of a
> contribution or two.
>
> Any comments/advice about the time/difficulty of doing the above?

What an interesting idea! Please keep me posted if you undertake this project.

       jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From ggrothendieck at gmail.com  Fri Jun  2 22:18:17 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Jun 2006 16:18:17 -0400
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <e8e755250606021218l3f347205pf831c720d2d360c0@mail.gmail.com>
References: <002301c6863f$dc3091b0$0ac0a8c0@MightyMini>
	<6e8360ad0606020726h770dde0fy588baf014b5603d3@mail.gmail.com>
	<17536.26030.502037.580416@basebud.nulle.part>
	<6e8360ad0606021057i42a6afccwc38859c5bb88e2ec@mail.gmail.com>
	<e8e755250606021218l3f347205pf831c720d2d360c0@mail.gmail.com>
Message-ID: <971536df0606021318n272357fesf829dfb12e16e2f2@mail.gmail.com>

There is a tutorial on interfacing to C here:

http://genetics.agrsci.dk/~sorenh/misc/Rdocs/Load-C-from-R.pdf

Info on creating packages is in the R Writing Extensions manual
and some people have written tutorials found via googling:
creating R package

Also see:
?package.skeleton

and one can download the source of some packages and look at them.

On 6/2/06, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> For those who have developed packages before:
>
> What would it take to build a set of wrappers in R to the ta-lib library
> referenced by John earlier ( http://sourceforge.net/projects/ta-lib )?
>
> That project seems quite complete and mature (although I have only had a
> chance to read through the online docs) - and seems easily callable from R
> (as it is all available as C/C++ from what I can tell)
>
> I haven't personally developed anything beyond what I use myself, but I
> wouldn't mind trying to give something back to R in the way of a
> contribution or two.
>
> Any comments/advice about the time/difficulty of doing the above?
>
> Jeff
>
> On 6/2/06, BBands <bbands at gmail.com> wrote:
> >
> > On 6/2/06, Dirk Eddelbuettel <edd at debian.org> wrote:
> > > But Crusher comes from the pre-Rmetrics days. I'd suspect that you find
> > as
> > > much if not more code in the TA examples in Rmetrics.
> >
> > Not true. Rmetrics is many things, but a TA package it is not, nor do
> > I think it was intended to be one. There are several TA indicators
> > included in fMultivar, so perhaps it is a beginning, but if it is a
> > beginning, it has a long ways to go before anything serious could be
> > done with it TA-wise. Perhaps I have missed something?
> >
> > (Nothing in the above may be construed in any way as a criticism of
> > Mr. Wuertz's work.)
> >
> >     jab
> > --
> > John Bollinger, CFA, CMT
> > www.BollingerBands.com
> >
> > If you advance far enough, you arrive at the beginning.
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From jeff.a.ryan at gmail.com  Sat Jun  3 00:05:25 2006
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 2 Jun 2006 22:05:25 +0000 GMT
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <971536df0606021318n272357fesf829dfb12e16e2f2@mail.gmail.com>
References: <002301c6863f$dc3091b0$0ac0a8c0@MightyMini>
	<6e8360ad0606020726h770dde0fy588baf014b5603d3@mail.gmail.com>
	<17536.26030.502037.580416@basebud.nulle.part>
	<6e8360ad0606021057i42a6afccwc38859c5bb88e2ec@mail.gmail.com>
	<e8e755250606021218l3f347205pf831c720d2d360c0@mail.gmail.com>
	<971536df0606021318n272357fesf829dfb12e16e2f2@mail.gmail.com>
Message-ID: <605461980-1149286076-cardhu_blackberry.rim.net-1927551232-@bwe047-cell00.bisx.prod.on.blackberry>

Thanks for the links Gabor. I'm less concerned about the packaging and interfacing external code (having done both before), than the prospect of calling code _I_ didn't write (kind of "green" in that department) and the overall scale (documentation etc.)

I'll get in contact with the lead developer to see what help I could look forward to - or if much is needed. I read on their forum a post that seemed to indicate his disappointment with the library not being picked for other oss - specifically mentioning R... - so he may be inclined to assist.

Does anyone else think this to be a worthwhile effort? It seems to me the big challenge is just in its scale - as the code has been sorted out over the 5 years it has been around.

Jeff
  

-----Original Message-----
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
Date: Fri, 2 Jun 2006 16:18:17 
To:"Jeff Ryan" <jeff.a.ryan at gmail.com>
Cc:R-sig-finance <r-sig-finance at stat.math.ethz.ch>
Subject: Re: [R-sig-Finance] Fwd: Testing technical indicators

There is a tutorial on interfacing to C here:

http://genetics.agrsci.dk/~sorenh/misc/Rdocs/Load-C-from-R.pdf

Info on creating packages is in the R Writing Extensions manual
and some people have written tutorials found via googling:
creating R package

Also see:
?package.skeleton

and one can download the source of some packages and look at them.

On 6/2/06, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> For those who have developed packages before:
>
> What would it take to build a set of wrappers in R to the ta-lib library
> referenced by John earlier ( http://sourceforge.net/projects/ta-lib )?
>
> That project seems quite complete and mature (although I have only had a
> chance to read through the online docs) - and seems easily callable from R
> (as it is all available as C/C++ from what I can tell)
>
> I haven't personally developed anything beyond what I use myself, but I
> wouldn't mind trying to give something back to R in the way of a
> contribution or two.
>
> Any comments/advice about the time/difficulty of doing the above?
>
> Jeff
>
> On 6/2/06, BBands <bbands at gmail.com> wrote:
> >
> > On 6/2/06, Dirk Eddelbuettel <edd at debian.org> wrote:
> > > But Crusher comes from the pre-Rmetrics days. I'd suspect that you find
> > as
> > > much if not more code in the TA examples in Rmetrics.
> >
> > Not true. Rmetrics is many things, but a TA package it is not, nor do
> > I think it was intended to be one. There are several TA indicators
> > included in fMultivar, so perhaps it is a beginning, but if it is a
> > beginning, it has a long ways to go before anything serious could be
> > done with it TA-wise. Perhaps I have missed something?
> >
> > (Nothing in the above may be construed in any way as a criticism of
> > Mr. Wuertz's work.)
> >
> >     jab
> > --
> > John Bollinger, CFA, CMT
> > www.BollingerBands.com
> >
> > If you advance far enough, you arrive at the beginning.
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From helprhelp at gmail.com  Sat Jun  3 05:44:40 2006
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 2 Jun 2006 22:44:40 -0500
Subject: [R-sig-Finance] time series clustering
Message-ID: <cdf817830606022044s766247d9o5da6e1442daffecd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060602/b0c6f129/attachment.pl 

From neileastep at gmail.com  Sat Jun  3 16:07:11 2006
From: neileastep at gmail.com (Neil Eastep)
Date: Sat, 3 Jun 2006 23:07:11 +0900
Subject: [R-sig-Finance] Automatic Download On All Symbols
Message-ID: <ce4d86730606030707v47e337cbye2c9d685f0008244@mail.gmail.com>

Dumb question here --

If I want to evaluate and compare a statistic on the values of all
available US stocks -- how do I go about downloading the data
automatically?

I wonder if I could use have a list of stock symbols - and pass a
variable to yahooImport that iterates through the list of symbols . .
. does this make any sense?

I'd ideally like to pull in one year of daily (EOD) data on all stock
symbols that are optionable.

Any ideas?


From bbands at gmail.com  Sat Jun  3 16:53:13 2006
From: bbands at gmail.com (BBands)
Date: Sat, 3 Jun 2006 07:53:13 -0700
Subject: [R-sig-Finance] Automatic Download On All Symbols
In-Reply-To: <ce4d86730606030707v47e337cbye2c9d685f0008244@mail.gmail.com>
References: <ce4d86730606030707v47e337cbye2c9d685f0008244@mail.gmail.com>
Message-ID: <6e8360ad0606030753i35290d6ey504dca0202cd4553@mail.gmail.com>

On 6/3/06, Neil Eastep <neileastep at gmail.com> wrote:
> Dumb question here --
>
> If I want to evaluate and compare a statistic on the values of all
> available US stocks -- how do I go about downloading the data
> automatically?
>
> I wonder if I could use have a list of stock symbols - and pass a
> variable to yahooImport that iterates through the list of symbols . .
> . does this make any sense?
>
> I'd ideally like to pull in one year of daily (EOD) data on all stock
> symbols that are optionable.

The R part is easy or there are any number of open-source data
retrievers floating around. What you'll need is a list of optionable
stocks. That you can get from Zacks. (This is also a great way to get
snapshots of other types of data.)

http://www.zacks.com/research/screening/custom/

Under Select Category select miscellaneous and set optionable = yes
and click to screen. The result should be 3249 companies. Get the
report, cut and paste to a plain text file and you are there. Now just
loop though the list with your favorite R data-retrieval function.
It'll take some time, but if you do it once a week as a nighttime
batch job it shouldn't be too bad.

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From michaelbcohen at gmail.com  Sun Jun  4 06:42:00 2006
From: michaelbcohen at gmail.com (Michael Cohen)
Date: Sun, 4 Jun 2006 14:42:00 +1000
Subject: [R-sig-Finance] Problem using merge with a loop and get.hist.quote
Message-ID: <ea4c8e5b0606032142m272ecbb9pc07d6a72e861a513@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060604/a6376dc9/attachment.pl 

From ggrothendieck at gmail.com  Sun Jun  4 09:00:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 4 Jun 2006 03:00:22 -0400
Subject: [R-sig-Finance] Problem using merge with a loop and
	get.hist.quote
In-Reply-To: <ea4c8e5b0606032142m272ecbb9pc07d6a72e861a513@mail.gmail.com>
References: <ea4c8e5b0606032142m272ecbb9pc07d6a72e861a513@mail.gmail.com>
Message-ID: <971536df0606040000k60fdaab1rc2d6c197724345d6@mail.gmail.com>

zoo has a function called index and by using a variable called
index the code gets confused.

Use another name.

On 6/4/06, Michael Cohen <michaelbcohen at gmail.com> wrote:
> I have a problem trying to use a loop structure to download prices from
> Yahoo.
> When I try the following code segment:
>
> library(tseries)
> # Initialize price vector
> prices <- get.hist.quote("^dji", start = "2006-01-01", compress = "m")[,
> "Close"]
> # Now download individual securities
> indices <- c("OEX", "IXIC")
> for (index in indices)
>  {
>    symbol = paste("^", index, sep="")
>    p <-  get.hist.quote(symbol, start = "2006-01-01", compress = "m")[,
> "Close"]
>    prices <- merge(prices, p, all = FALSE)
>  }
>
> I get the error message:
> "Error in get(x, envir, mode, inherits) : variable "OEX" of mode "function"
> was not found".
>
> The prices for the first item in the index series appears to be downlaoded
> correctly, but the "merge" fails?
>
> I have a lot of equities to process, so doing it by hand would be very
> unpleasant.
> Any help wold be appreciated.
> (As you might have guessed, I am not a programmer!)
>
> Michael.
>
> --
> Michael B Cohen
> Southern Cross University
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From spencer.graves at pdf.com  Sun Jun  4 19:21:18 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Jun 2006 10:21:18 -0700
Subject: [R-sig-Finance] garchFit and NAs
In-Reply-To: <OFE8630A34.BFEEBC30-ONC125717F.00457B61-C125717F.0046517C@fr.world.socgen>
References: <OFE8630A34.BFEEBC30-ONC125717F.00457B61-C125717F.0046517C@fr.world.socgen>
Message-ID: <4483168E.80208@pdf.com>

	  You don't provide a simple, self-contained example, so I can't be 
certain of what you are seeing.  However, I tried a "garchFit" example 
in "~/fSeries/demo/xmpDWChapter34.R", as suggested in the "garchFit" 
help file, and it seemed to return sensible answers.  Then I modified it 
with "formula.mean = ~arma(0, 2)" and 'cond.dist="dsged"' as in your 
example.  That completed in just under 13 minutes, if my memory is 
correct.  When I tried the same example without the 'cond.dist="dsged"', 
it seemed to run forever without making progress.  Over an hour later, I 
had such trouble killing it that I would up rebooting!

	  Since the example that ran took almost 13 minutes, I'm not eager to 
rerun it.  However, as I recall, the example from xmpDWChapter34.R 
didn't have the problem you identified, but the "~arma(0, 2)" 
modification did.  To explore this further, I used "str" to see the 
structure of the garchFit output.  In that, I found something called 
"hessian".  If I'm not mistaken, I believe the square roots of the 
diagonals of the inverse of "hessian" should be the approximate standard 
errors;  if not, the difference should be something like a factor of the 
standard deviation of the residuals.  You can check this for an example 
that works.  For the "~arma(0, 2)" modification, I computed the 
eigenvalues of the "hessian", and found that it was NOT positive 
definite, as it should be:  Instead it had some positive and some 
negative eigenvalues.  I'm guessing that this means that the formally 
computed approximate covariance matrix might have negative numbers on 
its diagonal.  If so, this would correspond to imaginary standard errors 
and would generate the NAs you reported.

	  I'm not completely certain how to interpret this in the context of 
your application, but I'm guessing that the algorithm is getting stuck 
in a saddle-point and is not actually maximizing the likelihood.  If you 
wanted to pursue this further, you could list the "garchFit" function to 
see what it does.  It might help to walk through it line-by-line, e.g., 
using "debug" (see, e.g., 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/68215.html").  This 
will likely consist of a call to an optimizer.  You could find that, 
then experiment with evaluating the objective function over a moderately 
course grid, using e.g., expand.grid to generate a set of points at 
which to evaluate the grid, then evaluating the objective function at 
each such point in a loop, then studying the results, e.g., using 
'lattice' graphics.

	  I realize this won't solve your problem, but I think you are working 
in an area that has not yet been extensively studied.  Nevertheless, I 
hope this helps.

	  Spencer Graves

anass.mouhsine at sgcib.com wrote:
> Hi everybody,
> 
> When trying to fit an ARMA(0,2)-APARCH(1,1) model to a timeseries, it
> results in the following warning message:
> 
> NaNs produced in: sqrt(diag(fit$cvar))
> 
> the print function gives the following result
> 
> _______________________________________________________________________
> Title:
>  GARCH Modelling
> 
> Call:
>  garchFit(formula.mean = ~arma(0, 2), formula.var = ~aparch(1,
>     1), series = seriemul, cond.dist = "dsged")
> 
> Mean and Variance Equation:
>  ~arma(0, 2) + ~aparch(1, 1)
> 
> Conditional Distribution:
>  dsged
> 
> Coefficient(s):
>        mu        ma1        ma2      omega     alpha1     gamma1
> -0.478628   0.356290   0.054928   1.472516   0.944778   0.418131
>     beta1      delta       skew      shape
>  0.306527   1.912457   0.541992   1.304578
> 
> Error Analysis:
>         Estimate  Std. Error  t value Pr(>|t|)
> mu     -0.478628    0.066091   -7.242 4.42e-13 ***
> ma1     0.356290    0.096042    3.710 0.000207 ***
> ma2     0.054928    0.055839    0.984 0.325272
> omega   1.472516    0.223275    6.595 4.25e-11 ***
> alpha1  0.944778    0.213975    4.415 1.01e-05 ***
> gamma1  0.418131    0.094418    4.428 9.49e-06 ***
> beta1   0.306527          NA       NA       NA
> delta   1.912457          NA       NA       NA
> skew    0.541992    0.004978  108.879  < 2e-16 ***
> shape   1.304578          NA       NA       NA
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Log Likelihood:
>  764.6944    normalized:  2.044638
> _______________________________________________________________________
> 
> My question is:
> --> How are these NaNs produced?
> --> How can we read and interpret the NAs in the result?
> 
> Thank you in advance for your help,
> 
> Anass
> 
> *************************************************************************
> This message and any attachments (the "message") are confide...{{dropped}}
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From spencer.graves at pdf.com  Mon Jun  5 00:30:53 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 04 Jun 2006 15:30:53 -0700
Subject: [R-sig-Finance] Mixture
In-Reply-To: <J0754Q$0A55CA5B986F43DD1ECE812BED4894C5@uol.com.br>
References: <J0754Q$0A55CA5B986F43DD1ECE812BED4894C5@uol.com.br>
Message-ID: <44835F1D.9000608@pdf.com>

	  What problem are you trying to solve?  Mixing Gaussian and 
hypergeometric distributions seems to me like mixing apples and volts: 
The Gaussian or normal distribution is continuous, while the 
hypergeometric is discrete.  Poisson and binomial are both discrete, but 
the first represents counts of events assumed to be independent with no 
theoretical upper limit, while the binomial is the number of "successes" 
out of a fixed number of trials.  This is more like comparing a rain 
storm with a bottle of water.

	  If you tell us more about the problem you are trying to solve 
(preferably following the posting guide, 
"www.R-project.org/posting-guide.html"), you might get more useful 
replies.

	  I know this doesn't answer your question, but I hope it helps.
	  Spencer Graves
p.s.  I just got 45 hits to 'RSiteSearch("mixtures of distributions")'. 
  I don't know if any of this can help you.

walmir-rodrigues wrote:
> How can I do a mixture of distribution in R? For exemple an Gaussian and Hipergeometric? or Poison and Binomial
> 
> Walmir
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From kriskumar at earthlink.net  Mon Jun  5 02:57:54 2006
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Sun, 04 Jun 2006 20:57:54 -0400
Subject: [R-sig-Finance] Mixture
In-Reply-To: <44835F1D.9000608@pdf.com>
References: <J0754Q$0A55CA5B986F43DD1ECE812BED4894C5@uol.com.br>
	<44835F1D.9000608@pdf.com>
Message-ID: <44838192.4010108@earthlink.net>

This looks like a parameter mix distribution.  Si a binomial distr with 
parameters (N,p)  where the parameter N
now is **not** fixed but varies as poisson with intensity say lambda. 
Then we have a new distribution which is poisson with intensity (lambda*p) 
[so mixture =a conditional distribution given the parameter and the 
distribution of the parameter itself ]

I vaguely think this may be possible with the distrEx package.

Best,
Krishna


Spencer Graves wrote:

>	  What problem are you trying to solve?  Mixing Gaussian and 
>hypergeometric distributions seems to me like mixing apples and volts: 
>The Gaussian or normal distribution is continuous, while the 
>hypergeometric is discrete.  Poisson and binomial are both discrete, but 
>the first represents counts of events assumed to be independent with no 
>theoretical upper limit, while the binomial is the number of "successes" 
>out of a fixed number of trials.  This is more like comparing a rain 
>storm with a bottle of water.
>
>	 
>


From spencer.graves at pdf.com  Tue Jun  6 02:51:21 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 05 Jun 2006 17:51:21 -0700
Subject: [R-sig-Finance] [R] time series clustering
In-Reply-To: <cdf817830606022044s766247d9o5da6e1442daffecd@mail.gmail.com>
References: <cdf817830606022044s766247d9o5da6e1442daffecd@mail.gmail.com>
Message-ID: <4484D189.6040303@pdf.com>

	  I know of no software for time series clustering in R.  Google 
produced some interesting hits for "time series clustering".  If you 
find an algorithm you like, the author might have software. 
Alternatively, the algorithm might be a modification of something 
already available in R.  If that's the case, you wouldn't need to start 
from scratch to program something since R is open source.

	  hope this helps.
	  Spencer Graves

Weiwei Shi wrote:
> Dear Listers:
> 
> I happened to have a problem requiring time-series clustering since the
> clusters will change with time (too old data need to be removed from data
> while new data comes in). I am wondering if there is some paper or reference
> on this topic and there is some kind of implementation in R?
> 
> Thanks,
> 
> Weiwei
>


From montezumasrevenge at gmail.com  Tue Jun  6 13:40:26 2006
From: montezumasrevenge at gmail.com (Monty B. )
Date: Tue, 6 Jun 2006 13:40:26 +0200
Subject: [R-sig-Finance] tests for superior predictive ability?
Message-ID: <ae75f2d40606060440r6f6bf77ak9fc6586424785bad@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060606/73c3f3fc/attachment.pl 

From gyollin at insightful.com  Tue Jun  6 17:37:22 2006
From: gyollin at insightful.com (Guy Yollin)
Date: Tue, 6 Jun 2006 08:37:22 -0700
Subject: [R-sig-Finance] tests for superior predictive ability?
Message-ID: <F90D76D3C7CC6A41B6E2E6F88CA8252E2C0FA8@se2kexch01.insightful.com>

Monty,

I'd also be interested in either of the diagnostics you mentioned.

In the meantime, perhaps the following would be of help to you.  It's an
implementation of a forecast directional accuracy test described in Tsay,
2002, section 4.4.2.1

# NAME:        DMtest.s
# DESCRIPTION: directional measure test
# REFERENCE:   Tsay, Analysis of Financial Time Series , 2002
# 
# args:  actual & forecast should be vectors of 1 and -1 indicating
#        positive and negative forecasts, respectively
DM.test <- function(actual,forecast)
{
  m.tab <-
matrix(NA,nrow=3,ncol=3,dimnames=list(as.character(1:3),as.character(1:3)))

  m.tab[1,1] <- sum(actual>=0 & forecast>=0)
  m.tab[1,2] <- sum(actual>=0 & forecast<0)
  m.tab[1,3] <- sum(actual>=0)
  m.tab[2,1] <- sum(actual<0 & forecast>=0)
  m.tab[2,2] <- sum(actual<0 & forecast<0)
  m.tab[2,3] <- sum(actual<0)
  m.tab[3,1] <- sum(forecast>=0)
  m.tab[3,2] <- sum(forecast<0)
  m.tab[3,3] <- numRows(actual)
  stat <- 0
  for( k in 1:2 )
  {
    for( l in 1:2 )
    {
      stat <- stat +
(m.tab[k,l]-m.tab[k,3]*m.tab[3,l]/m.tab[3,3])^2/(m.tab[k,3]*m.tab[3,l]/m.tab[
3,3])
    }
  }
  pc <- (m.tab[1,1]+m.tab[2,2])/m.tab[3,3]
  stat <- round(stat,digits=3)
  p.value <- round(1-pchisq(stat,df=1),digits=3)
  return(list(percent.correct=pc,statistic=stat,p.value=p.value))
}

Best,

-- Guy




-----Original Message-----
From: Monty B. [mailto:montezumasrevenge at gmail.com] 
Sent: Tuesday, June 06, 2006 4:40 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-sig-Finance] tests for superior predictive ability?

Dear list,

Does anyone know of an implementation of either
- reality check (RC) of White 2000 (Econometrica)
- superior predictive ability (SPA) of Hansen 2005  (J of Bus and Ec
Statistics)

in R or S-plus?

Cheers,

Monty

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From Achim.Zeileis at wu-wien.ac.at  Tue Jun  6 22:32:44 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 6 Jun 2006 22:32:44 +0200
Subject: [R-sig-Finance] Problem using merge with a loop and
 get.hist.quote
In-Reply-To: <971536df0606040000k60fdaab1rc2d6c197724345d6@mail.gmail.com>
References: <ea4c8e5b0606032142m272ecbb9pc07d6a72e861a513@mail.gmail.com>
	<971536df0606040000k60fdaab1rc2d6c197724345d6@mail.gmail.com>
Message-ID: <20060606223244.64e065ea.Achim.Zeileis@wu-wien.ac.at>

On Sun, 4 Jun 2006 03:00:22 -0400 Gabor Grothendieck wrote:

> zoo has a function called index and by using a variable called
> index the code gets confused.
>
> Use another name.

Another solution to the problem is to give zoo a NAMESPACE. As we wanted
to do this for a long time, Gabor and I have revised the package
accordingly and with zoo 1.1-0 (which will become available in the next
days) this problem will not persist anymore.

Best,
Z

 
> On 6/4/06, Michael Cohen <michaelbcohen at gmail.com> wrote:
> > I have a problem trying to use a loop structure to download prices
> > from Yahoo.
> > When I try the following code segment:
> >
> > library(tseries)
> > # Initialize price vector
> > prices <- get.hist.quote("^dji", start = "2006-01-01", compress =
> > "m")[, "Close"]
> > # Now download individual securities
> > indices <- c("OEX", "IXIC")
> > for (index in indices)
> >  {
> >    symbol = paste("^", index, sep="")
> >    p <-  get.hist.quote(symbol, start = "2006-01-01", compress =
> > "m")[, "Close"]
> >    prices <- merge(prices, p, all = FALSE)
> >  }
> >
> > I get the error message:
> > "Error in get(x, envir, mode, inherits) : variable "OEX" of mode
> > "function" was not found".
> >
> > The prices for the first item in the index series appears to be
> > downlaoded correctly, but the "merge" fails?
> >
> > I have a lot of equities to process, so doing it by hand would be
> > very unpleasant.
> > Any help wold be appreciated.
> > (As you might have guessed, I am not a programmer!)
> >
> > Michael.
> >
> > --
> > Michael B Cohen
> > Southern Cross University
> >
> >        [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From Achim.Zeileis at wu-wien.ac.at  Tue Jun  6 23:47:10 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 6 Jun 2006 23:47:10 +0200
Subject: [R-sig-Finance] new version of zoo
Message-ID: <20060606234710.a44afa25.Achim.Zeileis@wu-wien.ac.at>

Dear finance useRs,

as you have probably seen, I've just posted an announcement of the new
version 1.1-0 of the zoo package. As it is of increased interest for
this list, I would like to elaborate a bit on the two changes in the
new version:

  - The package now has a NAMESPACE, most S3 methods are not exported
    explicitely anymore, i.e., they should not be called directly
    anymore. The benefit is that some confusion can be avoided as in
    the post of Michael Cohen, where an `index' object in the global
    environment shadowed the index() function of zoo. 
    If you need some methods/functions to be exported explicitely for
    your application or package, please let us know and we'll try to
    resolve this.

  - read.zoo() does not try to produce regular series by default
    anymore. This could lead to undesired behaviour if you read in
    a daily financial series with "Date" index which was typically
    coerced to a regular "zooreg" series with deltat = 1 day.
    If this series contains observations on trading days only, the
    following problem could occur: If you compute returns as
      diff(log(x))
    then an irregular series will compare Mondays and Fridays whereas
    a regular series will try to compare Mondays and Sundays and hence
    typically lead to much more missing values. 
    The first example in the "zoo-quickref" vignette did the latter,
    although the former would have been more appropriate. This has now
    been fixed.

Feedback and suggestions are - as always - welcome.

Best wishes,
Z


From jgalt70 at yahoo.com  Wed Jun  7 05:42:15 2006
From: jgalt70 at yahoo.com (Andrew West)
Date: Tue, 6 Jun 2006 20:42:15 -0700 (PDT)
Subject: [R-sig-Finance] get.hist.quote question
Message-ID: <20060607034215.47815.qmail@web60315.mail.yahoo.com>

I've just started using get.hist.quote to download
returns on non-US stocks, and noticed that the period
downloaded is typically shorter than that shown as
available in the yahoo finance website.

e.g.
get.hist.quote("vow.f")
says 
"time series starts 2005-08-17"

yet on the yahoo.finance website I can manually click
on historical prices back to 2000. I get similar
truncated results on other international markets.

I've tried setting the start farther back, that
doesn't help. Any tips on how to get more history?
I'm using windows, R2.21 and 2.30

Regards,
Andrew West


From r-project at michael-stegh.com  Wed Jun  7 10:19:05 2006
From: r-project at michael-stegh.com (Michael Stegh)
Date: Wed, 07 Jun 2006 10:19:05 +0200
Subject: [R-sig-Finance] get.hist.quote question
In-Reply-To: <20060607034215.47815.qmail@web60315.mail.yahoo.com>
Message-ID: <4486A819.31364.288191@r-project.michael-stegh.com>

Dear Andrew,

after I read your mail, I tried downloading the quotes manually from the German Yahoo 
Finance webpage into the .csv format. The resulting file is truncated, too. I have tried this for 
several major German stocks and the files only inlcude the prices back to August 2005 
(although the historical prices back to 2000 are available). 

In contrast the historical prices of the German Stock Market Index DAX are available till 1990 
and the downloaded .csv file contains all of them.

So it rather seems to be a problem of Yahoo and not of R.

Regards,

Michael Stegh


From martin.becker at mx.uni-saarland.de  Wed Jun  7 10:45:29 2006
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Wed, 07 Jun 2006 10:45:29 +0200
Subject: [R-sig-Finance] get.hist.quote question
In-Reply-To: <20060607034215.47815.qmail@web60315.mail.yahoo.com>
References: <20060607034215.47815.qmail@web60315.mail.yahoo.com>
Message-ID: <44869229.6080505@mx.uni-saarland.de>

Hi,

I noticed this behavior of get.hist.quote (or rather finance.yahoo.com) 
a while ago and wrote some "wrapper" for get.hist.quote (which also 
makes get.hist.quote more quiet, which may be implemented in 
get.hist.quote itself meanwhile).
Yahoo seems to return only a small amount of data for foreign time 
series, so the wrapper shifts the time window and merges the parts of 
the time series. The implementation is quick and dirty, but maybe it is 
useful anyhow. I think it will only work for retclass="zoo", but 
modification for other return classes should be possible.

Regards,

  Martin Becker

--- Start Code ---

GetQuote <- function(instrument="vow.f",start,end,quiet=TRUE,...) {
  HV<-NULL;
  if (require(tseries)) {
     if (missing(start))
        start <- "1991-01-02"
     if (missing(end))
        end <- format(Sys.Date() - 1, "%Y-%m-%d")
     if (quiet) {
      zz <- file(".GetQuoteLog", open="wt")
      sink(zz,type="message");
      sink(zz);
    }
    numtries=0;notworked=TRUE;
    while((numtries<5)&&(notworked)) {
      
tmpdata<-try(get.hist.quote(instrument=instrument,start=start,end=end,...));
      notworked<-inherits(tmpdata,"try-error");
      numtries<-numtries+1;
    }
    if (!notworked) {
      HV<-tmpdata;
      oldstart<-end;
      while (start(HV)>as.Date(as.Date(start)+5)) {
        numtries=0;notworked=TRUE;
        while((numtries<5)&&(notworked)) {
          
tmpdata<-try(get.hist.quote(instrument=instrument,start=start,end=start(HV)-1),...);
          notworked<-inherits(tmpdata,"try-error");
          numtries<-numtries+1;
        }
        if (!notworked) HV<-c(HV,tmpdata);
        newstart<-start(HV);
        if (newstart<oldstart) oldstart<-newstart else break;
      }
    }
    if (quiet) {
      sink(type="message");
      sink();
      close(zz);
    }
    return(HV);
  } else stop("Package tseries not installed!");
}

--- End Code ---


Andrew West wrote:

>I've just started using get.hist.quote to download
>returns on non-US stocks, and noticed that the period
>downloaded is typically shorter than that shown as
>available in the yahoo finance website.
>
>e.g.
>get.hist.quote("vow.f")
>says 
>"time series starts 2005-08-17"
>
>yet on the yahoo.finance website I can manually click
>on historical prices back to 2000. I get similar
>truncated results on other international markets.
>
>I've tried setting the start farther back, that
>doesn't help. Any tips on how to get more history?
>I'm using windows, R2.21 and 2.30
>
>Regards,
>Andrew West
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  
>


From Achim.Zeileis at wu-wien.ac.at  Wed Jun  7 11:06:06 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 7 Jun 2006 11:06:06 +0200
Subject: [R-sig-Finance] get.hist.quote question
In-Reply-To: <44869229.6080505@mx.uni-saarland.de>
References: <20060607034215.47815.qmail@web60315.mail.yahoo.com>
	<44869229.6080505@mx.uni-saarland.de>
Message-ID: <20060607110606.0a32b27a.Achim.Zeileis@wu-wien.ac.at>

On Wed, 07 Jun 2006 10:45:29 +0200 Martin Becker wrote:

> Hi,
> 
> I noticed this behavior of get.hist.quote (or rather
> finance.yahoo.com) a while ago and wrote some "wrapper" for
> get.hist.quote (which also makes get.hist.quote more quiet, which may
> be implemented in get.hist.quote itself meanwhile).

yes, it is.
Z

> Yahoo seems to return only a small amount of data for foreign time 
> series, so the wrapper shifts the time window and merges the parts of 
> the time series. The implementation is quick and dirty, but maybe it
> is useful anyhow. I think it will only work for retclass="zoo", but 
> modification for other return classes should be possible.
> 
> Regards,
> 
>   Martin Becker
> 
> --- Start Code ---
> 
> GetQuote <- function(instrument="vow.f",start,end,quiet=TRUE,...) {
>   HV<-NULL;
>   if (require(tseries)) {
>      if (missing(start))
>         start <- "1991-01-02"
>      if (missing(end))
>         end <- format(Sys.Date() - 1, "%Y-%m-%d")
>      if (quiet) {
>       zz <- file(".GetQuoteLog", open="wt")
>       sink(zz,type="message");
>       sink(zz);
>     }
>     numtries=0;notworked=TRUE;
>     while((numtries<5)&&(notworked)) {
>       
> tmpdata<-try(get.hist.quote
> (instrument=instrument,start=start,end=end,...)); notworked<-inherits
> (tmpdata,"try-error"); numtries<-numtries+1;
>     }
>     if (!notworked) {
>       HV<-tmpdata;
>       oldstart<-end;
>       while (start(HV)>as.Date(as.Date(start)+5)) {
>         numtries=0;notworked=TRUE;
>         while((numtries<5)&&(notworked)) {
>           
> tmpdata<-try(get.hist.quote
> (instrument=instrument,start=start,end=start(HV)-1),...);
> notworked<-inherits(tmpdata,"try-error"); numtries<-numtries+1;
>         }
>         if (!notworked) HV<-c(HV,tmpdata);
>         newstart<-start(HV);
>         if (newstart<oldstart) oldstart<-newstart else break;
>       }
>     }
>     if (quiet) {
>       sink(type="message");
>       sink();
>       close(zz);
>     }
>     return(HV);
>   } else stop("Package tseries not installed!");
> }
> 
> --- End Code ---
> 
> 
> Andrew West wrote:
> 
> >I've just started using get.hist.quote to download
> >returns on non-US stocks, and noticed that the period
> >downloaded is typically shorter than that shown as
> >available in the yahoo finance website.
> >
> >e.g.
> >get.hist.quote("vow.f")
> >says 
> >"time series starts 2005-08-17"
> >
> >yet on the yahoo.finance website I can manually click
> >on historical prices back to 2000. I get similar
> >truncated results on other international markets.
> >
> >I've tried setting the start farther back, that
> >doesn't help. Any tips on how to get more history?
> >I'm using windows, R2.21 and 2.30
> >
> >Regards,
> >Andrew West
> >
> >_______________________________________________
> >R-SIG-Finance at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >  
> >
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From Kyle.W.Campbell at williams.edu  Wed Jun  7 15:24:59 2006
From: Kyle.W.Campbell at williams.edu (Kyle Campbell)
Date: Wed, 07 Jun 2006 09:24:59 -0400
Subject: [R-sig-Finance] Map of the Market
Message-ID: <20dacad29280b5f7d8e222012d32074d@williams.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060607/bd49b5ce/attachment.pl 

From bbands at gmail.com  Wed Jun  7 16:28:08 2006
From: bbands at gmail.com (BBands)
Date: Wed, 7 Jun 2006 07:28:08 -0700
Subject: [R-sig-Finance] Map of the Market
In-Reply-To: <20dacad29280b5f7d8e222012d32074d@williams.edu>
References: <20dacad29280b5f7d8e222012d32074d@williams.edu>
Message-ID: <6e8360ad0606070728w7b754709m1b4ad0b1491bf0cc@mail.gmail.com>

On 6/7/06, Kyle Campbell <Kyle.W.Campbell at williams.edu> wrote:
> Hello all,
>
> I am trying to design an R function to display the Map of Market
> graphic for a given portfolio of stocks (something like this, though
> not so interactive: http://www.smartmoney.com/marketmap/ ).  Does
> anyone know a good algorithm I could use for positioning the different
> rectangles once I know the relative area that each one should occupy?
> Any help on this, or any other suggestions for this project would be
> much appreciated.

This is a tiling problem a very interesting area. (It is actually a
nested tiling problem.) Long a source of interesting mathematical
diversions, tiling was examined on several occasions by Martin
Gardiner in his Mathematical Recreations column in Scientific
American. I couldn't find much in R at present, but here is a link to
a site that should get you started.

http://www.ics.uci.edu/~eppstein/junkyard/tiling.html

Implementation in R ought to be pretty straight forward.

Have fun,

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From pcarl at gsbalum.uchicago.edu  Wed Jun  7 16:55:25 2006
From: pcarl at gsbalum.uchicago.edu (Peter Carl)
Date: Wed, 7 Jun 2006 09:55:25 -0500
Subject: [R-sig-Finance] Map of the Market
In-Reply-To: <6e8360ad0606070728w7b754709m1b4ad0b1491bf0cc@mail.gmail.com>
References: <20dacad29280b5f7d8e222012d32074d@williams.edu>
	<6e8360ad0606070728w7b754709m1b4ad0b1491bf0cc@mail.gmail.com>
Message-ID: <200606070955.25923.pcarl@gsbalum.uchicago.edu>

Another place to look, with an open-source java implementation, is:

http://www.cs.umd.edu/hcil/treemap/

This has proven to be very useful for portfolio visualization.

pcc

On Wednesday 07 June 2006 09:28, BBands wrote:
> On 6/7/06, Kyle Campbell <Kyle.W.Campbell at williams.edu> wrote:
> > Hello all,
> >
> > I am trying to design an R function to display the Map of Market
> > graphic for a given portfolio of stocks (something like this,
> > though not so interactive: http://www.smartmoney.com/marketmap/ ). 
> > Does anyone know a good algorithm I could use for positioning the
> > different rectangles once I know the relative area that each one
> > should occupy? Any help on this, or any other suggestions for this
> > project would be much appreciated.
>
> This is a tiling problem a very interesting area. (It is actually a
> nested tiling problem.) Long a source of interesting mathematical
> diversions, tiling was examined on several occasions by Martin
> Gardiner in his Mathematical Recreations column in Scientific
> American. I couldn't find much in R at present, but here is a link to
> a site that should get you started.
>
> http://www.ics.uci.edu/~eppstein/junkyard/tiling.html
>
> Implementation in R ought to be pretty straight forward.
>
> Have fun,
>
>      jab

-- 
Peter Carl
145 Scottswood Rd
Riverside, IL 60546


From brugolsky at telemetry-investments.com  Wed Jun  7 16:56:35 2006
From: brugolsky at telemetry-investments.com (Bill Rugolsky Jr.)
Date: Wed, 7 Jun 2006 10:56:35 -0400
Subject: [R-sig-Finance] Map of the Market
In-Reply-To: <20dacad29280b5f7d8e222012d32074d@williams.edu>
References: <20dacad29280b5f7d8e222012d32074d@williams.edu>
Message-ID: <20060607145635.GE4668@ti64.telemetry-investments.com>

The visualization is called a treemap:

http://www.cs.umd.edu/hcil/treemap-history/index.shtml

KCachegrind uses a very nice Qt-based treemap widget to display program
profile information:

http://kcachegrind.sourceforge.net/cgi-bin/show.cgi/KcacheGrindTreeMap
http://kcachegrind.sourceforge.net/cgi-bin/show.cgi/KcacheGrindShot

There are various other implementations of treemap widgets floating
around for use in Perl, Java, etc.

Regards,

	Bill Rugolsky


From Achim.Zeileis at wu-wien.ac.at  Wed Jun  7 17:35:42 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 7 Jun 2006 17:35:42 +0200
Subject: [R-sig-Finance] Map of the Market
In-Reply-To: <20060607145635.GE4668@ti64.telemetry-investments.com>
References: <20dacad29280b5f7d8e222012d32074d@williams.edu>
	<20060607145635.GE4668@ti64.telemetry-investments.com>
Message-ID: <20060607173542.520a9d49.Achim.Zeileis@wu-wien.ac.at>

On Wed, 7 Jun 2006 10:56:35 -0400 Bill Rugolsky Jr. wrote:

> The visualization is called a treemap:

Treemaps are also closely related to mosaic displays. In package "vcd"
there is a very flexible implementation of mosaics provided by mosaic()
(based on the workhorse function strucplot()). We've been playing
around with providing a simple treemap implementation based on
strucplot(), but did not get around to put it into a user-friendly
interface. However, it is not so difficult to produce simple
treemaps with mosaic() by setting up a table with the splitting
variables and corresponding areas.

Best,
Z
 
> http://www.cs.umd.edu/hcil/treemap-history/index.shtml
> 
> KCachegrind uses a very nice Qt-based treemap widget to display
> program profile information:
> 
> http://kcachegrind.sourceforge.net/cgi-bin/show.cgi/KcacheGrindTreeMap
> http://kcachegrind.sourceforge.net/cgi-bin/show.cgi/KcacheGrindShot
> 
> There are various other implementations of treemap widgets floating
> around for use in Perl, Java, etc.
> 
> Regards,
> 
> 	Bill Rugolsky
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From David.Brahm at geodecapital.com  Wed Jun  7 18:05:10 2006
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Wed, 7 Jun 2006 12:05:10 -0400
Subject: [R-sig-Finance] Map of the Market
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D806867D64@MSGBOSCLF2WIN.DMN1.FMR.COM>

Kyle Campbell <Kyle.W.Campbell at williams.edu> wrote:
> I am trying to design an R function to display the Map of Market

Here is the essence of a "squarified treemap":

1) First, order the rectangle sizes (areas) from largest to smallest.

2) If the plotting space available is higher than it is wide, we will
   build blocks starting from the bottom, otherwise from the left.
   For concreteness, suppose we are building a block from the left.

3) A block consists of the next "n" available rectangles.  "n" is
   chosen so that the block is as square as possible.

4) Again assuming (w >= h), we fix the width of the block, then fill
   it with rectangles from the bottom up.

5) What we do to create a rectangle depends on a "hook" function.  It
   may be as simple as a call to "rect", or may involve another level
   of treemapping (e.g. of industries within a sector).

6) Repeat from step (2) until all rectangles are created.

### Code begins ###

chop <- function(x) rev( rev(x)[-1] )

simple.hook <- function(z, xl, yl, xu, yu) {
  rect(xl, yl, xu, yu, lwd=3, border="blue")
  text((xl+xu)/2, (yl+yu)/2, z$one, cex=2.5, col="green")
}

squarified.treemap <- function(z, x=0, y=0, w=1, h=1, hook) {
  cz <- cumsum(z$size) / sum(z$size)
  n <- which.min(abs(log(max(w/h, h/w) * sum(z$size) * cz^2/z$size)))
  more <- n < length(z$size)
  a <- c(0, cz[1:n]) / cz[n]
  if (h > w) {
    hook(z[1:n, ], x+w*chop(a), rep(y,n), x+w*a[-1], rep(y+h*cz[n],n))
    if (more) Recall(z[-(1:n), ], x, y+h*cz[n], w, h*(1-cz[n]), hook)
  } else {
    hook(z[1:n, ], rep(x,n), y+h*chop(a), rep(x+w*cz[n],n), y+h*a[-1])
    if (more) Recall(z[-(1:n), ], x+w*cz[n], y, w*(1-cz[n]), h, hook)
  }    
}

z <- data.frame(size=c(5,8,3,12,2,6,7), one=LETTERS[1:7])
z <- z[order(-z$size), ]
plot(0:1, 0:1, type="n", axes=FALSE, xlab="", ylab="")
squarified.treemap(z, hook=simple.hook)

### Code ends ###

-- David Brahm (brahm at alum.mit.edu)


From spencer.graves at pdf.com  Thu Jun  8 05:52:12 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 07 Jun 2006 20:52:12 -0700
Subject: [R-sig-Finance] tests for superior predictive ability?
In-Reply-To: <ae75f2d40606060440r6f6bf77ak9fc6586424785bad@mail.gmail.com>
References: <ae75f2d40606060440r6f6bf77ak9fc6586424785bad@mail.gmail.com>
Message-ID: <44879EEC.2060405@pdf.com>

	  RSiteSearch("white reality check") produced one irrelevant hit, and 
RSiteSearch("superior predictive ability") returned 0 hits.  If you have 
not received replies to the contrary from anyone else, I would assume 
that they are not part of any contributed package.

	  What do the authors say about the availability of software to perform 
those tests?  I'm not familiar with those articles, but I'd be surprised 
if either were very difficult to program in R.  If the authors offered 
free distribution of compiled code of some kind, it shouldn't be too 
difficult to link it to R.  They should be compiled with a compiler 
compatible with R, and you need to know the internal names [case 
sensitive], etc.  I don't do much of that, but others do, so if you try 
that and can't get it to work, you might find help on this listserve.

	  This doesn't solve your problem, but I hope it helps.
	  Spencer Graves

Monty B. wrote:
> Dear list,
> 
> Does anyone know of an implementation of either
> - reality check (RC) of White 2000 (Econometrica)
> - superior predictive ability (SPA) of Hansen 2005  (J of Bus and Ec
> Statistics)
> 
> in R or S-plus?
> 
> Cheers,
> 
> Monty
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From Kyle.W.Campbell at williams.edu  Sat Jun 10 07:02:09 2006
From: Kyle.W.Campbell at williams.edu (Kyle Campbell)
Date: Sat, 10 Jun 2006 01:02:09 -0400
Subject: [R-sig-Finance] map of the market
Message-ID: <8de76f8997008890f73e448b1cdbf6b3@williams.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060610/9404a59e/attachment.pl 

From Achim.Zeileis at wu-wien.ac.at  Mon Jun 12 11:11:38 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 12 Jun 2006 11:11:38 +0200
Subject: [R-sig-Finance] map of the market
In-Reply-To: <8de76f8997008890f73e448b1cdbf6b3@williams.edu>
References: <8de76f8997008890f73e448b1cdbf6b3@williams.edu>
Message-ID: <20060612111138.7d1e5633.Achim.Zeileis@wu-wien.ac.at>

Kyle:

> I've put together an R function for drawing a basic map of the
> market. It's designed to take data from a portfolio object, but the
> variables can also be set manually to any appropriate data vectors.
> Credit goes to David Brahm for writing the code underlying the
> treemap algorithm. Also thanks to John Bollinger, Bill Rugolsky,
> Peter Carl, Vincent, and Achim Zelleis for valuable background on
> treemaps, and to David Kane for giving me this project.  The code is
> below, followed by a short example.

Interesting, looks very nice. I'll try to have a closer look after
useR!. One quick remark though: perceptually better palettes can be
chosen based on HCL colors. See ?diverge_hcl in "vcd" for some
examples.

Best wishes,
Z


From wuertz at itp.phys.ethz.ch  Mon Jun 12 19:26:37 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon, 12 Jun 2006 19:26:37 +0200
Subject: [R-sig-Finance] [R] TsayData
In-Reply-To: <20060529123219.29857.qmail@web7601.mail.in.yahoo.com>
References: <20060529123219.29857.qmail@web7601.mail.in.yahoo.com>
Message-ID: <448DA3CD.3060708@itp.phys.ethz.ch>

Unfortunately, the Tsay data sets got lost in the  fSeries package.

They are downloadable from the authors web site at

http://www.gsb.uchicago.edu/fac/ruey.tsay/teaching/fts/

You can also use the follwing (untested) function to download
the data sets from the web site mentioned above.

Diethelm Wuertz

tsaySeries =
function(query = "d-ibmln", file = "tempfile",
source = "http://www.gsb.uchicago.edu/fac/ruey.tsay/teaching/fts/",
save = FALSE, sepCSV = ";", try = TRUE, ...)
{   # A function implemented by Diethelm Wuertz

    # Description:
    #   Downloads *.dat data sets from R Tsay's textbook
   
    # Value:
    #   A data frame.
       
    # Examples:
    #   tsaySeries(query = "d-ibmln")   1-column
    #   tsaySeries(query = "d-vwew")    3-column

    # FUNCTION:
    
    # Download:
    if (try) {
        # Try for Internet Connection:
        z = try(tsaySeries(query = query, file = file, source = source,
            save = save, try = FALSE))
        if (class(z) == "try-error" || class(z) == "Error") {
            return("No Internet Access")
        } else {
            return(z)
        }
    } else {
        # File name:
        queryFile = paste(query, ".dat", sep = "")
       
        # For S-Plus Compatibility:
        if (class(version) != "Sversion") {
            # R:
            method = NULL
        } else {
            # SPlus
            method = "wget"
        }
   
        # Download and temporarily store:
        download.file(url = paste(source, queryFile, sep = ""),
            destfile = file, method = method)
       
        # Scan the file and transform into data frame:
        ans = read.table(file, ...)
       
        # Save download ?
        if (save) {
            write.table(z, file, quote = FALSE, col.names = FALSE,
                sep = sepCSV)
        } else {
            unlink(file)
        }
       
        # Return Value:
        return(ans)
    }
   
    # Return Value:
    invisible()
}





SUMANTA BASAK wrote:

>Hi,
>
>I'm trying to work with TsayData in fSeries package.
>How can i fetch any time series data of this package.
>Please advice.
>
>Thanks,
>Sumanta Basak.
>
>
>Send instant messages to your online friends http://in.messenger.yahoo.com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


From volchik2000 at list.ru  Tue Jun 13 13:35:36 2006
From: volchik2000 at list.ru (Yuri Volchik)
Date: Tue, 13 Jun 2006 12:35:36 +0100
Subject: [R-sig-Finance] How to get values from dataframe's column
	conditional on other column
In-Reply-To: <mailman.13.1150106404.10250.r-sig-finance@stat.math.ethz.ch>
References: <mailman.13.1150106404.10250.r-sig-finance@stat.math.ethz.ch>
Message-ID: <222915329.20060613123536@list.ru>

Hi to all members of this list,

I'm quite a novice to R and was wondering if there is a more elegant
way to solve a following problem:
Suppose we have a dataframe
  X     Y
.12   TRUE
-.24  TRUE
..     ...
.34   FALSE

i.e. two (or more columns) with data and we want to get vector of X
values conditional on Y values (say only X's when Y=TRUE).
Of course it is possible to do it by looping through the whole
dataframe, i was wondering if there is a a more elegant solution to
this in R?

-- 
Best regards,
 Yuri                            mailto:volchik2000 at list.ru


From edd at debian.org  Tue Jun 13 13:52:26 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 13 Jun 2006 06:52:26 -0500
Subject: [R-sig-Finance] How to get values from dataframe's column
	conditional on other column
In-Reply-To: <222915329.20060613123536@list.ru>
References: <mailman.13.1150106404.10250.r-sig-finance@stat.math.ethz.ch>
	<222915329.20060613123536@list.ru>
Message-ID: <20060613115226.GA30963@eddelbuettel.com>

On Tue, Jun 13, 2006 at 12:35:36PM +0100, Yuri Volchik wrote:
> Hi to all members of this list,
> 
> I'm quite a novice to R and was wondering if there is a more elegant
> way to solve a following problem:
> Suppose we have a dataframe
>   X     Y
> .12   TRUE
> -.24  TRUE
> ..     ...
> .34   FALSE
> 
> i.e. two (or more columns) with data and we want to get vector of X
> values conditional on Y values (say only X's when Y=TRUE).
> Of course it is possible to do it by looping through the whole
> dataframe, i was wondering if there is a a more elegant solution to
> this in R?

You would do yourself a huge favour if you read the 'An Introduction to R'
manual distributed with R, or, for that matter, any of the other contributed  
introductory manuals on the R/CRAN site.

The answer, by the way, is 

    DF[ DF[,"Y"]==TRUE, "X" ]

ie you create a boolean condution for the rows (and == 'TRUE' could be
omitted if Y is boolean), and then select "X".  There are still issues that
may confuse you about data.frames collapsing to a single vector, please go
and read the manual. Really.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From ggrothendieck at gmail.com  Tue Jun 13 14:02:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Jun 2006 08:02:14 -0400
Subject: [R-sig-Finance] How to get values from dataframe's column
	conditional on other column
In-Reply-To: <222915329.20060613123536@list.ru>
References: <mailman.13.1150106404.10250.r-sig-finance@stat.math.ethz.ch>
	<222915329.20060613123536@list.ru>
Message-ID: <971536df0606130502x730dbc26i49c3ea3889cecdf7@mail.gmail.com>

If DF is your data.frame

with(DF, X[Y])

or

DF$X[DF$Y]



On 6/13/06, Yuri Volchik <volchik2000 at list.ru> wrote:
> Hi to all members of this list,
>
> I'm quite a novice to R and was wondering if there is a more elegant
> way to solve a following problem:
> Suppose we have a dataframe
>  X     Y
> .12   TRUE
> -.24  TRUE
> ..     ...
> .34   FALSE
>
> i.e. two (or more columns) with data and we want to get vector of X
> values conditional on Y values (say only X's when Y=TRUE).
> Of course it is possible to do it by looping through the whole
> dataframe, i was wondering if there is a a more elegant solution to
> this in R?
>
> --
> Best regards,
>  Yuri                            mailto:volchik2000 at list.ru
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From ggrothendieck at gmail.com  Tue Jun 13 14:04:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Jun 2006 08:04:28 -0400
Subject: [R-sig-Finance] How to get values from dataframe's column
	conditional on other column
In-Reply-To: <971536df0606130502x730dbc26i49c3ea3889cecdf7@mail.gmail.com>
References: <mailman.13.1150106404.10250.r-sig-finance@stat.math.ethz.ch>
	<222915329.20060613123536@list.ru>
	<971536df0606130502x730dbc26i49c3ea3889cecdf7@mail.gmail.com>
Message-ID: <971536df0606130504q248dd458waf25db702fe04ec9@mail.gmail.com>

Sorry, that should be:

with(DF, X[Y,])

or

DF[DF$Y, "X"]

On 6/13/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> If DF is your data.frame
>
> with(DF, X[Y])
>
> or
>
> DF$X[DF$Y]
>
>
>
> On 6/13/06, Yuri Volchik <volchik2000 at list.ru> wrote:
> > Hi to all members of this list,
> >
> > I'm quite a novice to R and was wondering if there is a more elegant
> > way to solve a following problem:
> > Suppose we have a dataframe
> >  X     Y
> > .12   TRUE
> > -.24  TRUE
> > ..     ...
> > .34   FALSE
> >
> > i.e. two (or more columns) with data and we want to get vector of X
> > values conditional on Y values (say only X's when Y=TRUE).
> > Of course it is possible to do it by looping through the whole
> > dataframe, i was wondering if there is a a more elegant solution to
> > this in R?
> >
> > --
> > Best regards,
> >  Yuri                            mailto:volchik2000 at list.ru
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
>


From ggrothendieck at gmail.com  Tue Jun 13 14:17:24 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Jun 2006 08:17:24 -0400
Subject: [R-sig-Finance] How to get values from dataframe's column
	conditional on other column
In-Reply-To: <971536df0606130504q248dd458waf25db702fe04ec9@mail.gmail.com>
References: <mailman.13.1150106404.10250.r-sig-finance@stat.math.ethz.ch>
	<222915329.20060613123536@list.ru>
	<971536df0606130502x730dbc26i49c3ea3889cecdf7@mail.gmail.com>
	<971536df0606130504q248dd458waf25db702fe04ec9@mail.gmail.com>
Message-ID: <971536df0606130517m4af057bare1a10b35c303dde2@mail.gmail.com>

Aaarg.  Here it is again.  Hopefully I got it right this time.  Three
possibilities are:

> # test data
> DF <- data.frame(X = 1:4, Y = c(TRUE, TRUE, FALSE, FALSE))

> with(DF, X[Y]) # 1
[1] 1 2
> DF$X[DF$Y]  # 2
[1] 1 2
> DF[DF$Y, "X"] # 3
[1] 1 2

On 6/13/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Sorry, that should be:
>
> with(DF, X[Y,])
>
> or
>
> DF[DF$Y, "X"]
>
> On 6/13/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > If DF is your data.frame
> >
> > with(DF, X[Y])
> >
> > or
> >
> > DF$X[DF$Y]
> >
> >
> >
> > On 6/13/06, Yuri Volchik <volchik2000 at list.ru> wrote:
> > > Hi to all members of this list,
> > >
> > > I'm quite a novice to R and was wondering if there is a more elegant
> > > way to solve a following problem:
> > > Suppose we have a dataframe
> > >  X     Y
> > > .12   TRUE
> > > -.24  TRUE
> > > ..     ...
> > > .34   FALSE
> > >
> > > i.e. two (or more columns) with data and we want to get vector of X
> > > values conditional on Y values (say only X's when Y=TRUE).
> > > Of course it is possible to do it by looping through the whole
> > > dataframe, i was wondering if there is a a more elegant solution to
> > > this in R?
> > >
> > > --
> > > Best regards,
> > >  Yuri                            mailto:volchik2000 at list.ru
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > >
> >
>


From stefan.albrecht at allianz.com  Tue Jun 13 14:50:17 2006
From: stefan.albrecht at allianz.com (stefan.albrecht at allianz.com)
Date: Tue, 13 Jun 2006 14:50:17 +0200
Subject: [R-sig-Finance] =?iso-8859-1?q?Stefan_Albrecht/HV/Finanzen/Allian?=
 =?iso-8859-1?q?z-Sach_ist_au=DFer_Haus=2E_=3A_R-SIG-Finance_Digest=2C_Vol?=
 =?iso-8859-1?q?_25=2C_Issue_11?=
Message-ID: <OFB4F8BD8A.6258A8D5-ONC125718C.00468549@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060613/f673d85d/attachment.pl 

From dave at kanecap.com  Tue Jun 13 15:20:34 2006
From: dave at kanecap.com (David Kane)
Date: Tue, 13 Jun 2006 09:20:34 -0400
Subject: [R-sig-Finance] portfolio package update and RFC on package names
Message-ID: <17550.48034.441501.19780@gargle.gargle.HOWL>

Hi,

1) Version 0.2-1 of the portfolio package is available on CRAN. An
introductory article is available in the latest issue of R News. We
are thankful to many people for thoughtful comments, especially Dan
Rie,  Dirk Eddelbuettel, and Patrick Burns. As we noted in our initial
announcement:

https://stat.ethz.ch/pipermail/r-packages/2006/000171.html

we hope to recruit other developpers to this project. At this point,
our main concern is with the design of the class. Informed commentary
and suggestions (and contributions!) would be welcome.

2) We will be making signficant additions to the portfolio package in
the coming months, including extensive trading facilities and
improved graphics, including a Map of the Market plot.

3) We are working on two other related packages and would like to
request comments on their names. The first package, which we have
tentatively named "backtest", will provide --- you guessed it! --- a
basic backtest capability for financial applications. This will *not*
use the portfolio package. It will instead provide simple results like
decile spreads, Sharpe ratios, turnover and other measures of
historical performance for a quantitative trading signal.

RFC: Is "backtest" a good name for such a package?

4) The other package will *require* the portfolio package and will
provide a *much* more serious environment for backtesting. Some people
in finance distinguish between "backtests" --- meaning simple
approaches to seeing if a given signal would work without worrying too
much about trading rules, investibility and the like --- and
"simulations" --- meaning testing environments in which you do things
as much as possible as you would in the "real world.". 

An example of the difference between "backtests" and "simulations"
concerns funds under management. A typical "backtest" --- looking at
decile spreads for a price-to-book signal on a monthly basis for the
last decade --- does not explicitly worry about the amount of money
under management. It is implicitly assumed that you could trade into
the desired positions whether you are managing ten million or ten
billion dollars. A "simulation" would take position size seriously. It
might take days/weeks more to invest the ten billion than the ten
million, time during which you do not get the benefit of holding those
stocks. Our simulation package will take these and related issues
seriously.

RFC: "simulation" does not seem like a good name for this package
since the terms overlaps too much with other R communities. What would
be a good name?

Thanks,

Dave Kane


From rfin.20.phftt at xoxy.net  Tue Jun 13 20:34:40 2006
From: rfin.20.phftt at xoxy.net (Rob Steele)
Date: Tue, 13 Jun 2006 14:34:40 -0400
Subject: [R-sig-Finance] How to get values from dataframe's column
 conditional on other column
In-Reply-To: <222915329.20060613123536@list.ru>
References: <mailman.13.1150106404.10250.r-sig-finance@stat.math.ethz.ch>
	<222915329.20060613123536@list.ru>
Message-ID: <e6n0g0$42r$1@sea.gmane.org>

There's more than one way to do it.  My favorite:

df <- data.frame(X = c(0.12, -0.24, 0.34),
                  Y = c(TRUE, TRUE, FALSE))

subset(df, Y, X)


From michaelbcohen at gmail.com  Sat Jun 17 03:49:49 2006
From: michaelbcohen at gmail.com (Michael Cohen)
Date: Sat, 17 Jun 2006 11:49:49 +1000
Subject: [R-sig-Finance] Moving zoo objects to a db and back again
Message-ID: <ea4c8e5b0606161849g7373f90cve455b3916172906d@mail.gmail.com>

I need to store some larger zoo objects on a local database.
I realize that this is a vast subject, but I suspect that I am not
alone in needing to use a database for storing equity prices, volumes,
etc. I have not been able to find much help on designing the db
tables, and pointers to the literature are more than welcome.

The following code works, but I would like to make it more robust or elegant.

"prices" is a df with the prices of shares.
# str(prices)
#   num [1:1872, 1:25] 9184 9311 9545 9538 9643 ...
#   - attr(*, "dimnames")=List of 2
#    ..$ : NULL
#    ..$ : chr [1:25] "AdjClose" "A" "AA" "AAC" ...
#   - attr(*, "index")=Class 'Date'  num [1:1872] 10595 10596 10597
10598 10599 ..

# extract the date from the zoo object:
Date <- index(prices)
# extract the data from the zoo object:
y <- coredata(prices)
# combine into a df . Note: I can not write a zoo object directly to
the db - correct?
z <- as.data.frame(cbind(Date,y))

# Write to db:
dbWriteTable(con, "Prices", z)

# Read from db:
a <- dbReadTable(con, "Prices")
# extract the date
index.date <- as.Date(a[[1]])
# form the zoo object
b <- zoo(a[-1], index.date)

Many thanks,

Michael Cohen
Southern Cross University


From ggrothendieck at gmail.com  Sat Jun 17 16:28:56 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 17 Jun 2006 10:28:56 -0400
Subject: [R-sig-Finance] Moving zoo objects to a db and back again
In-Reply-To: <ea4c8e5b0606161849g7373f90cve455b3916172906d@mail.gmail.com>
References: <ea4c8e5b0606161849g7373f90cve455b3916172906d@mail.gmail.com>
Message-ID: <971536df0606170728s6fafbc1w8e1518fa38b3c6d6@mail.gmail.com>

Yes.  Convert the zoo object to/from a data.frame:


library(zoo)
library(RSQLite)

# z is zoo object
z <- zoo(cbind(x = 1:10, y = 11:20), as.Date("2006-01-01") + 0:9)

# set up data base
m <- dbDriver("SQLite")
con <- dbConnect(m, dbname = "base.dbms")

# create data frame from zoo object and write to data base
z.df <- cbind(idx = index(z), as.data.frame(z))
dbWriteTable(con, "z", z.df, overwrite = TRUE)

# read data frame from data base and conver to zoo
z2.df <- dbReadTable(con, "z")
z2 <- zoo(data.matrix(z2.df[,-1]), as.Date(z2.df[,1]))

all.equal(coredata(z), coredata(z2)) # TRUE
all.equal(time(z), time(z2)) # TRUE


On 6/16/06, Michael Cohen <michaelbcohen at gmail.com> wrote:
> I need to store some larger zoo objects on a local database.
> I realize that this is a vast subject, but I suspect that I am not
> alone in needing to use a database for storing equity prices, volumes,
> etc. I have not been able to find much help on designing the db
> tables, and pointers to the literature are more than welcome.
>
> The following code works, but I would like to make it more robust or elegant.
>
> "prices" is a df with the prices of shares.
> # str(prices)
> #   num [1:1872, 1:25] 9184 9311 9545 9538 9643 ...
> #   - attr(*, "dimnames")=List of 2
> #    ..$ : NULL
> #    ..$ : chr [1:25] "AdjClose" "A" "AA" "AAC" ...
> #   - attr(*, "index")=Class 'Date'  num [1:1872] 10595 10596 10597
> 10598 10599 ..
>
> # extract the date from the zoo object:
> Date <- index(prices)
> # extract the data from the zoo object:
> y <- coredata(prices)
> # combine into a df . Note: I can not write a zoo object directly to
> the db - correct?
> z <- as.data.frame(cbind(Date,y))
>
> # Write to db:
> dbWriteTable(con, "Prices", z)
>
> # Read from db:
> a <- dbReadTable(con, "Prices")
> # extract the date
> index.date <- as.Date(a[[1]])
> # form the zoo object
> b <- zoo(a[-1], index.date)
>
> Many thanks,
>
> Michael Cohen
> Southern Cross University
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From ajayshah at mayin.org  Sat Jun 17 06:55:30 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sat, 17 Jun 2006 10:25:30 +0530
Subject: [R-sig-Finance] Moving zoo objects to a db and back again
In-Reply-To: <ea4c8e5b0606161849g7373f90cve455b3916172906d@mail.gmail.com>
References: <ea4c8e5b0606161849g7373f90cve455b3916172906d@mail.gmail.com>
Message-ID: <20060617045530.GK17581@lubyanka.local>

On Sat, Jun 17, 2006 at 11:49:49AM +1000, Michael Cohen wrote:
> I need to store some larger zoo objects on a local database.

One possibility could be like this. Use save() to make a binary file
out of R. Feed that into the database as a blob. :-) On the reverse
trip, extract the blob from the database and do load(). This is
inefficient in some ways and efficient in others.

      -ans

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From maechler at stat.math.ethz.ch  Mon Jun 19 14:01:12 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 19 Jun 2006 14:01:12 +0200
Subject: [R-sig-Finance] portfolio package update and RFC on package
	names
In-Reply-To: <17550.48034.441501.19780@gargle.gargle.HOWL>
References: <17550.48034.441501.19780@gargle.gargle.HOWL>
Message-ID: <17558.37384.118390.242906@stat.math.ethz.ch>

>>>>> "DavK" == David Kane <dave at kanecap.com>
>>>>>     on Tue, 13 Jun 2006 09:20:34 -0400 writes:

	  ....................
	  ....................


    DavK> RFC: Is "backtest" a good name for such a package?

not bad in any case;
though I think it is also a bit on the "too generic" side,
I think it would be fine to keep the name.

    DavK> 4) The other package will *require* the portfolio
    DavK> package and will provide a *much* more serious
    DavK> environment for backtesting. Some people in finance
    DavK> distinguish between "backtests" --- meaning simple
    DavK> approaches to seeing if a given signal would work
    DavK> without worrying too much about trading rules,
    DavK> investibility and the like --- and "simulations" ---
    DavK> meaning testing environments in which you do things as
    DavK> much as possible as you would in the "real world.".

    DavK> An example of the difference between "backtests" and
    DavK> "simulations" concerns funds under management. A
    DavK> typical "backtest" --- looking at decile spreads for a
    DavK> price-to-book signal on a monthly basis for the last
    DavK> decade --- does not explicitly worry about the amount
    DavK> of money under management. It is implicitly assumed
    DavK> that you could trade into the desired positions
    DavK> whether you are managing ten million or ten billion
    DavK> dollars. A "simulation" would take position size
    DavK> seriously. It might take days/weeks more to invest the
    DavK> ten billion than the ten million, time during which
    DavK> you do not get the benefit of holding those
    DavK> stocks. Our simulation package will take these and
    DavK> related issues seriously.

    DavK> RFC: "simulation" does not seem like a good name for
    DavK> this package since the terms overlaps too much with
    DavK> other R communities. 

yes indeed.

    DavK> What would be a good name?

"stockSimulation"  or
"stocksim"

since it simulates things that happen at a "stock exchange",
right? (but not just "stocks", of course)

or then something like

  exchangeSimulation
  xchgSimulation
  xSimul

or

  portfolioSim
  PFsimulation
  PFSimul
  ...

?

    DavK> Thanks,

    DavK> Dave Kane


From wuertz at itp.phys.ethz.ch  Wed Jun 21 10:57:40 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Wed, 21 Jun 2006 10:57:40 +0200
Subject: [R-sig-Finance] R-help: Searching for a PostDoc Candidate
Message-ID: <44990A04.1070402@itp.phys.ethz.ch>

PostDoc Position

Econophysics:
  Computational Finance and Financial Engineering

Institute for Theoretical Physics
Swiss Federal Institute of Technology, Zurich

The Institute for Theoretical Physics of the Swiss
Federal Institute of Technology in Zurich, ETHZ,
invites applications for a postdoctoral position
starting in autumn 2006. We expect from the
candidates a PhD degree in Theoretical Physics
or in a related field (Mathematics, Statistics,
Computer Science) with background in financial
mathematics, computational finance, and/or
financial engineering.

We are looking for a young f/m person who has
beside a computer science background experience
in using statistical software for data analysis
and statistical modeling, such as R and/or S-PLUS.
An understanding of capital markets, valuation
of derivatives, portfolio construction and
optimization, financial risk management, and
Monte-Carlo simulations is of great advantage.

Collaboration will be in a small team concerned

* with the open source project Rmetrics,
* with an R Communication and Presentation Server
  project for e-distant learning and content
  management, and
* with the development of new algorithms to model
  volatility and risk, and to implement formulas
  to valuate futures, swaps, options and other
  financial derivatives.

Self-motivated candidates whose research interests
are mainly application and solution oriented are
encouraged to apply. Applicants are requested to
send curriculum vita, graduate transcripts, and
two letters of recommendation to the following
address:
 
PD Dr. Diethelm W?rtz
Institute for Theoretical Physics
ETH - Swiss Federal Institute of Technology
Hoenggerberg HPZ E2
Schafmattstr. 32
CH-8093 ZURICH

wuertz at phys.ethz.ch
www.rmetrics.org
www.itp.phys.ethz.ch*
**
* <http://www.itp.phys.ethz.ch/>


From vivek.satsangi at gmail.com  Wed Jun 21 13:37:30 2006
From: vivek.satsangi at gmail.com (Vivek Satsangi)
Date: Wed, 21 Jun 2006 07:37:30 -0400
Subject: [R-sig-Finance] R-SIG-Finance Digest, Vol 25, Issue 15
In-Reply-To: <mailman.13.1150797605.3451.r-sig-finance@stat.math.ethz.ch>
References: <mailman.13.1150797605.3451.r-sig-finance@stat.math.ethz.ch>
Message-ID: <bcb171920606210437r1f0a399bu952742234df14402@mail.gmail.com>

Hello Mr. Kane,
1. I would call the backtest package factorsBacktest, since the
"factors" are well understood by many people. Alternately "alphaTest"
-- but that's a lot less informative as a name. I agree with Herr
Doktor Maechler that "backtest" by itself seems too generic.

2. Based on the literature, portfolioSimulation  would seem to be most
informative -- e.g. the work that John Brush did (
http://www.columbinecap.com/about/ ) seems to suggest that this is
"portfolio Simulation" we are talking about. Please consider NOT
abbreviating the word "portfolio", though "portfolioSim" would work
fine as a name and be informative as well.

Vivek
On 6/20/06, r-sig-finance-request at stat.math.ethz.ch
<r-sig-finance-request at stat.math.ethz.ch> wrote:
>
> Today's Topics:
>
>    1. Re: portfolio package update and RFC on package   names
>       (Martin Maechler)
>
-- 
-- Vivek Satsangi
Rochester, NY USA


From oyvfos at yahoo.no  Wed Jun 21 15:09:17 2006
From: oyvfos at yahoo.no (yvind Foshaug)
Date: Wed, 21 Jun 2006 06:09:17 -0700 (PDT)
Subject: [R-sig-Finance] fSeries prob
Message-ID: <20060621130918.81124.qmail@web25505.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060621/21030343/attachment.pl 

From dave at kanecap.com  Wed Jun 21 15:39:23 2006
From: dave at kanecap.com (David Kane)
Date: Wed, 21 Jun 2006 09:39:23 -0400
Subject: [R-sig-Finance] Confidence intervals for spread returns
Message-ID: <17561.19467.957034.487545@gargle.gargle.HOWL>

We are creating an R package for simple backtests. One part will
involve creating decile (or whatever) portfolios and then looking at
the spread return between the top and bottom decile. So, for example,
the top decile might return 10% and the bottom decile 2%, yielding an
8% spread return if one were to go long the top decile and short the
bottom.

Question: How might one calculate a reasonable confidence interval
around this 8% spread return?

The obvious intution is that more securities in each decile should
lead to more narrow confidence interval. For example, if there are 100
securities in each decile, then the 8% result is fairly accurate. If
there are only 2 securities per decile, then the 8% could easily be
very wrong.

One hack might be to argue the spread is sort of a weighted mean
calculation in which the weights are 1 for the long decile and -1 for
the short decile. If there are N securities total, there would be N/10
in each decile or 2*N/10 in the bottom/top together. If sd(r) is the
standard deviation of the returns of these securities (just those in
the extreme deciles), the standard error would be:

SE = sd(r) / sqrt(N/5)

This would suggest that a reasonable confidence interval around 8%
might be +/- 2 times SE. Does that make sense?

Thanks,

Dave Kane


From ggrothendieck at gmail.com  Wed Jun 21 15:50:42 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 21 Jun 2006 09:50:42 -0400
Subject: [R-sig-Finance] Confidence intervals for spread returns
In-Reply-To: <17561.19467.957034.487545@gargle.gargle.HOWL>
References: <17561.19467.957034.487545@gargle.gargle.HOWL>
Message-ID: <971536df0606210650p5254c999yf2d13a08e6e6dae9@mail.gmail.com>

You might want to provide for the user to choose normal theory or
bootstrap confidence intervals in the design.  The actual implementation
could be left to a later release if you already have your hands full.

On 6/21/06, David Kane <dave at kanecap.com> wrote:
> We are creating an R package for simple backtests. One part will
> involve creating decile (or whatever) portfolios and then looking at
> the spread return between the top and bottom decile. So, for example,
> the top decile might return 10% and the bottom decile 2%, yielding an
> 8% spread return if one were to go long the top decile and short the
> bottom.
>
> Question: How might one calculate a reasonable confidence interval
> around this 8% spread return?
>
> The obvious intution is that more securities in each decile should
> lead to more narrow confidence interval. For example, if there are 100
> securities in each decile, then the 8% result is fairly accurate. If
> there are only 2 securities per decile, then the 8% could easily be
> very wrong.
>
> One hack might be to argue the spread is sort of a weighted mean
> calculation in which the weights are 1 for the long decile and -1 for
> the short decile. If there are N securities total, there would be N/10
> in each decile or 2*N/10 in the bottom/top together. If sd(r) is the
> standard deviation of the returns of these securities (just those in
> the extreme deciles), the standard error would be:
>
> SE = sd(r) / sqrt(N/5)
>
> This would suggest that a reasonable confidence interval around 8%
> might be +/- 2 times SE. Does that make sense?
>
> Thanks,
>
> Dave Kane
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From Jens.Wildermuth at gmx.de  Thu Jun 22 11:03:56 2006
From: Jens.Wildermuth at gmx.de (Jens Wildermuth)
Date: Thu, 22 Jun 2006 11:03:56 +0200
Subject: [R-sig-Finance] MC simulation
Message-ID: <20060622090356.104080@gmx.net>

Hi, 

I am pretty new to R. However, I checked the introduction to R paper and did not find a solution for my problem. I use a mean reversion process to simulate US RMBS spreads for different rating classes. My problem is that for each MC run I would like to generate a vector and attach these vectors with each other to generate a matrix, which contains all the simulation results. Right now I just see the results of every simulation run because I plot them. I tried something using the print(x) (as.matrix() and so forth) command but I didn't work out properly. I would very much appreciate any help on that problem. Thanks in advance. 

Cheers 
Jens


#Set up variables
AAA<-data$AAA
AA<-data$AA
A<-data$A
BBB<-data$BBB

logAAA<-log(AAA)
logAA<-log(AA)
logA<-log(A)
logBBB<-log(BBB)

diffAAA<-logAAA[1:100]-logAAA[2:101]
diffAA<-logAA[1:100]-logAA[2:101]
diffA<-logA[1:100]-logA[2:101]
diffBBB<-logBBB[1:100]-logBBB[2:101]

#Estimate coefficients by OLS
regAAA<-lm(diffAAA ~ logAAA[2:101])

#Parameters for O-U process
n?<--log(1+regAAA$coefficients[2])
averagex<--(regAAA$coefficients[1]/regAAA$coefficients[2])
resierror<-sqrt(var(regAAA$residuals)) #check this 
sigma<-resierror*sqrt((2*log(1+regAAA$coefficients[2]))/((1+regAAA$coefficients[2])^2-1))


lAAA<-length(diffAAA)
x<-logAAA[lAAA]
t<-c(1:60)
delta<-1
mcsim<-40

par(mfrow=c(4,1))

#MC for AAA
for ( k in 1:mcsim ){

    rn<-rnorm(length(t), mean=0,sd=1)

    for ( i in 1:length(t) ){
        x[t+1]<-x[t]*exp(-n?*delta*t)+averagex*(1-exp(-n?*delta*t))+sigma*sqrt((1-exp(-2*n?*delta*t))/(2*n?))*rn[t]
        }
    vect<-c(logAAA,x[2:length(x)])
    plot.ts(exp(vect), plot.type = c("single"),col="red",type="l")
    lines(AAA)
}

-- 


Der GMX SmartSurfer hilft bis zu 70% Ihrer Onlinekosten zu sparen!
Ideal f?r Modem und ISDN: http://www.gmx.net/de/go/smartsurfer


From amberti at inwind.it  Thu Jun 22 12:16:28 2006
From: amberti at inwind.it (Daniele Amberti)
Date: Thu, 22 Jun 2006 12:16:28 +0200
Subject: [R-sig-Finance] MC simulations from R-SIG-Finance Digest, Vol 25,
	Issue 17
In-Reply-To: <mailman.13.1150970405.13199.r-sig-finance@stat.math.ethz.ch>
Message-ID: <20060622101623.8BF5BA8CBD@smtp2.libero.it>

    vect<-c(logAAA,x[2:length(x)])
    plot.ts(exp(vect), plot.type = c("single"),col="red",type="l")
    lines(AAA)

istead of plot you should use cbind()
for istance:

vect <- c()
results <- cbind(results, vect)

so results will have a column for each calculated vect...

Regards
Daniele Amberti


Message: 5
Date: Thu, 22 Jun 2006 11:03:56 +0200
From: "Jens Wildermuth" <Jens.Wildermuth at gmx.de>
Subject: [R-sig-Finance] MC simulation
To: r-sig-finance at stat.math.ethz.ch
Message-ID: <20060622090356.104080 at gmx.net>
Content-Type: text/plain; charset="iso-8859-1"

Hi, 

I am pretty new to R. However, I checked the introduction to R paper and did
not find a solution for my problem. I use a mean reversion process to
simulate US RMBS spreads for different rating classes. My problem is that
for each MC run I would like to generate a vector and attach these vectors
with each other to generate a matrix, which contains all the simulation
results. Right now I just see the results of every simulation run because I
plot them. I tried something using the print(x) (as.matrix() and so forth)
command but I didn't work out properly. I would very much appreciate any
help on that problem. Thanks in advance. 

Cheers 
Jens


#Set up variables
AAA<-data$AAA
AA<-data$AA
A<-data$A
BBB<-data$BBB

logAAA<-log(AAA)
logAA<-log(AA)
logA<-log(A)
logBBB<-log(BBB)

diffAAA<-logAAA[1:100]-logAAA[2:101]
diffAA<-logAA[1:100]-logAA[2:101]
diffA<-logA[1:100]-logA[2:101]
diffBBB<-logBBB[1:100]-logBBB[2:101]

#Estimate coefficients by OLS
regAAA<-lm(diffAAA ~ logAAA[2:101])

#Parameters for O-U process
n?<--log(1+regAAA$coefficients[2])
averagex<--(regAAA$coefficients[1]/regAAA$coefficients[2])
resierror<-sqrt(var(regAAA$residuals)) #check this 
sigma<-resierror*sqrt((2*log(1+regAAA$coefficients[2]))/((1+regAAA$coefficie
nts[2])^2-1))


lAAA<-length(diffAAA)
x<-logAAA[lAAA]
t<-c(1:60)
delta<-1
mcsim<-40

par(mfrow=c(4,1))

#MC for AAA
for ( k in 1:mcsim ){

    rn<-rnorm(length(t), mean=0,sd=1)

    for ( i in 1:length(t) ){
 
x[t+1]<-x[t]*exp(-n?*delta*t)+averagex*(1-exp(-n?*delta*t))+sigma*sqrt((1-ex
p(-2*n?*delta*t))/(2*n?))*rn[t]
        }
    vect<-c(logAAA,x[2:length(x)])
    plot.ts(exp(vect), plot.type = c("single"),col="red",type="l")
    lines(AAA)
}

-- 


Der GMX SmartSurfer hilft bis zu 70% Ihrer Onlinekosten zu sparen!
Ideal f?r Modem und ISDN: http://www.gmx.net/de/go/smartsurfer



------------------------------

_______________________________________________
R-SIG-Finance mailing list
R-SIG-Finance at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


End of R-SIG-Finance Digest, Vol 25, Issue 17


From volchik2000 at list.ru  Thu Jun 22 15:15:34 2006
From: volchik2000 at list.ru (Yuri Volchik)
Date: Thu, 22 Jun 2006 14:15:34 +0100
Subject: [R-sig-Finance]  MC simulation
In-Reply-To: <mailman.13.1150970405.13199.r-sig-finance@stat.math.ethz.ch>
References: <mailman.13.1150970405.13199.r-sig-finance@stat.math.ethz.ch>
Message-ID: <1123109360.20060622141534@list.ru>

Jens,

you might try to look at the "merge" function.
http://spider.stat.umn.edu/R/library/base/html/merge.html



> I am pretty new to R. However, I checked the introduction to R
> paper and did not find a solution for my problem. I use a mean
> reversion process to simulate US RMBS spreads for different rating
> classes. My problem is that for each MC run I would like to generate
> a vector and attach these vectors with each other to generate a
> matrix, which contains all the simulation results. Right now I just
> see the results of every simulation run because I plot them. I tried
> something using the print(x) (as.matrix() and so forth) command but
> I didn't work out properly. I would very much appreciate any help on
> that problem. Thanks in advance. 

> Cheers 
> Jens


> #Set up variables
> AAA<-data$AAA
> AA<-data$AA
> A<-data$A
> BBB<-data$BBB

> logAAA<-log(AAA)
> logAA<-log(AA)
> logA<-log(A)
> logBBB<-log(BBB)

> diffAAA<-logAAA[1:100]-logAAA[2:101]
> diffAA<-logAA[1:100]-logAA[2:101]
> diffA<-logA[1:100]-logA[2:101]
> diffBBB<-logBBB[1:100]-logBBB[2:101]

> #Estimate coefficients by OLS
> regAAA<-lm(diffAAA ~ logAAA[2:101])

> #Parameters for O-U process
> n?<--log(1+regAAA$coefficients[2])
> averagex<--(regAAA$coefficients[1]/regAAA$coefficients[2])
> resierror<-sqrt(var(regAAA$residuals)) #check this 
> sigma<-resierror*sqrt((2*log(1+regAAA$coefficients[2]))/((1+regAAA$coefficients[2])^2-1))


> lAAA<-length(diffAAA)
> x<-logAAA[lAAA]
> t<-c(1:60)
> delta<-1
> mcsim<-40

> par(mfrow=c(4,1))

> #MC for AAA
> for ( k in 1:mcsim ){

>     rn<-rnorm(length(t), mean=0,sd=1)

>     for ( i in 1:length(t) ){
>        
> x[t+1]<-x[t]*exp(-n?*delta*t)+averagex*(1-exp(-n?*delta*t))+sigma*sqrt((1-exp(-2*n?*delta*t))/(2*n?))*rn[t]
>         }
>     vect<-c(logAAA,x[2:length(x)])
>     plot.ts(exp(vect), plot.type = c("single"),col="red",type="l")
>     lines(AAA)
> }




-- 
Best regards,
 Yuri                            mailto:volchik2000 at list.ru


From joe-byers at utulsa.edu  Fri Jun 23 18:05:38 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Fri, 23 Jun 2006 11:05:38 -0500
Subject: [R-sig-Finance] MC simulations from R-SIG-Finance Digest, Vol 25,
 Issue, 17 (Daniele Amberti)
Message-ID: <449C1152.2010602@utulsa.edu>

Jens,

I am new to R as you are. I noticed in your code you brute force the lag 
of your data series. 

diffAA<-logAA[1:100]-logAA[2:101]
diffA<-logA[1:100]-logA[2:101]
diffBBB<-logBBB[1:100]-logBBB[2:101]

You might look at the zoo package.  It has some really good functions for time series as well.  For example the lag operator lag.zoo.

My example converts my price data.frame column(variable) gas$price to a zoo object, lags one day,converts it to a vector to get rid of some of the zoo class components, and assigns it to PriceLag1 in my data.frame.  I do some statements looking at daily prices changes +, -, and NA.  The NA is because some time series functions do not handle missing values.  Also, my code is for a technical analysis project I am working on.  In this example I loose one degree of freedom at the beginning of my data series, hence the k=-1.  

#lagging Gas Prices pair today with yesterday hence k=-1 below
gas$PriceLag1<-as.vector(lag.zoo(as.zoo(gas$Price),k=-1,na.pad=TRUE)); # use zoo so can pad on the run
gas$PriceD[which(gas$Price>gas$PriceLag1)]<-1;  # Price_t>Price_t-1 1
gas$PriceD[which(gas$Price<gas$PriceLag1)]<- -1;# Price_t<Price_t-1 -1
gas$PriceD[which(gas$Price==gas$PriceLag1)]<- 0;# Price_t=Price_t-1 0
gas$PriceD[which(gas$PriceLag1==NA)]<- 0;# Handle the lost degrees of freedom

You might try
diffAAA<-logAAA-as.vector(lag.zoo(as.zoo(logAAA),k=-1, na.pad=TRUE));# diff today with the lag one day back

Using the zoo functions alleviates hard coding dimensions, inserting dimension tests, and adds dynamic reusable code functionality.  There also may be more efficient ways to use the zoo package that I have here, but it worked for me.

Good Luck
Joe



-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060623/6efe5c73/attachment.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 104 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060623/6efe5c73/attachment.vcf 

From spencer.graves at pdf.com  Sat Jun 24 06:01:48 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 23 Jun 2006 21:01:48 -0700
Subject: [R-sig-Finance] fSeries prob
In-Reply-To: <20060621130918.81124.qmail@web25505.mail.ukl.yahoo.com>
References: <20060621130918.81124.qmail@web25505.mail.ukl.yahoo.com>
Message-ID: <449CB92C.7020601@pdf.com>

Dear ?yvind Foshaug and Diethelm Wuertz:

?YVIND:

	  Thanks for reporting this.  Could you please try the revised function 
below?  I changed three lines over the previous version

	  In case you are interested, I will outline here what I did to resolve 
this issue.  First, I looked on the 'armaFit' help page for an example. 
  Then I modified it in a way that successfully reproduced the error you 
reported:

xmpSeries("\nStart: Simulate an ARMA(2, 1) process > ")
x = armaSim(model = list(ar = c(0.5, -0.5), ma = 0.1), n = 1000)
Continue = xmpSeries("Press any key > ")
# Estimate the parameters:
fit = armaFit(x ~ arma(2, 1))
predict(fit, doplot=FALSE)

Error in predict.fARMA(fit, doplot = FALSE) :
	object "npred" not found

	  If you list 'predict', you will see it primarily consists of a call 
to 'UseMethod'.  The help page for 'UseMethod' says, "see also: 
methods".  When I tried 'methods("predict")', I got a character vector 
of length 23, all beginning "predict.".  This list included in 
particular, "predict.fARMA".  I listed that, made a local copy, found 
where 'npred' is designed and used, and made two minor changes in 
'predict.fARMA'.  After sourcing that, I got a different error, and I 
found an apparent patch for that.

	  Thanks again for reporting this problem.

DIETHELM:

	  Below please find a modification to your 'predict.fARMA' from fSeries 
221.10065.  I made three changes marked by comments.  Please let me know 
how you would like me to handle issues like this.  Until I hear from you 
I will send them to you like this -- unless I find a caret "^", which 
might be eaten by certain email software.

	  You might consider adding a 'predict(fit, doplot=FALSE)' to the help 
page example section.

	  Best Wishes,
	  Spencer Graves
##################################
sessionInfo()
Version 2.3.1 (2006-06-01)
i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
     fSeries   fCalendar     fBasics
"221.10065" "221.10065" "221.10065"

	  Best Wishes,
####################################
predict.fARMA <-
function (object, n.ahead = 10, n.back = 50, conf = c(80, 95),
     doplot = TRUE, doprint = TRUE, ...)
{
     object = object at fit
     class(object) = object$class
     TS = "stats"
     if (object$tsmodel == "fracdiff") {
         warning("Prediction for FRACDIFF not yet implemented")
         return(NA)
     }
     options(warn = -1)
     pred = .predictTS(object = object, n.ahead = n.ahead, se.fit = TRUE)
     nint = length(conf)
     upper = lower = matrix(NA, ncol = nint, nrow = length(pred$pred))
     for (i in 1:nint) {
         qq = qnorm(0.5 * (1 + conf[i]/100))
         lower[, i] = pred$pred - qq * pred$se
         upper[, i] = pred$pred + qq * pred$se
     }
     colnames(lower) = colnames(upper) = paste(conf, "%", sep = "")
     shadecols = switch(1 + (length(conf) > 1), 7, length(conf):1)
     shadepalette = heat.colors(length(conf))
     col = 1
     data = as.ts(object$x)
     freq = frequency(data)
     start = start(data)
     n = length(data)
     pred.mean = pred$pred
     npred = length(pred.mean)
#   The preceeding 2 lines (pred.mean and npred)
#   were moved from after if(doplot) to before.
     if (doplot) {
         ylim = range(c(data[(n - n.back + 1):n], pred.mean),
             na.rm = TRUE)
         ylim = range(ylim, lower, upper, na.rm = TRUE)
         ylab = paste("Series: ", object$series)
         plot(ts(c(data[(n - n.back + 1):n], pred.mean[1], rep(NA,
             npred - 1)), end = tsp(data)[2] + npred/freq, f = freq),
             ylim = ylim, ylab = ylab)
         title(main = paste(object$tstitle))
     }
     xx = tsp(data)[2] + (1:npred)/freq
     idx = rev(order(conf))
     if (nint > 1)
         palette(shadepalette)
#   "if(doplot)" was added before 'for(i in 1:nint)'
     if(doplot)for (i in 1:nint) {
         polygon(c(xx, rev(xx)), c(lower[, idx[i]], rev(upper[,
             idx[i]])), col = shadecols[i], border = FALSE)
     }
     palette("default")
     lines(ts(pred.mean, start = tsp(data)[2] + 1/freq, f = freq),
         lty = 1, col = 4)
     nconf = length(conf)
     out = pred.mean
     upper = as.matrix(upper)
     lower = as.matrix(lower)
     names = "Forecast"
     for (i in nconf:1) {
         out = cbind(out, lower[, i])
         names = c(names, paste("Low", conf[i]))
     }
     out = cbind(out, pred.mean)
     names = c(names, "Forecast")
     for (i in 1:nconf) {
         out = cbind(out, upper[, i])
         names = c(names, paste("High", conf[i]))
     }
     out = round(out, digits = 4)[, 2:(2 * nconf + 2)]
     colnames(out) = names[2:(2 * nconf + 2)]
     if (doprint)
         print(out)
     options(warn = 0)
     result = list(pred = pred$pred, se = pred$se)
     invisible(result)
}
####################################
?yvind Foshaug wrote:
> Hi,
>   I seem to get an error when I apply predict with the 
option doplot=FALSE in function armaFit. It says npred not found.
>    
>   Thanks,
>   Oyvind Foshaug
> 
>  		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From kriskumar at earthlink.net  Sun Jun 25 20:26:27 2006
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Sun, 25 Jun 2006 14:26:27 -0400
Subject: [R-sig-Finance] Confidence intervals for spread returns
In-Reply-To: <17561.19467.957034.487545@gargle.gargle.HOWL>
References: <17561.19467.957034.487545@gargle.gargle.HOWL>
Message-ID: <449ED553.5020308@earthlink.net>

are the portfolio weights found  using simulation. i.e. You simulate the 
underlying asset returns and then optimize
on the simulated paths?. Bootstrapping seems like a natural way to get 
the conf.intervals.

Krishna



 David Kane wrote:

>We are creating an R package for simple backtests. One part will
>involve creating decile (or whatever) portfolios and then looking at
>the spread return between the top and bottom decile. So, for example,
>the top decile might return 10% and the bottom decile 2%, yielding an
>8% spread return if one were to go long the top decile and short the
>bottom.
>
>Question: How might one calculate a reasonable confidence interval
>around this 8% spread return?
>
>The obvious intution is that more securities in each decile should
>lead to more narrow confidence interval. For example, if there are 100
>securities in each decile, then the 8% result is fairly accurate. If
>there are only 2 securities per decile, then the 8% could easily be
>very wrong.
>
>One hack might be to argue the spread is sort of a weighted mean
>calculation in which the weights are 1 for the long decile and -1 for
>the short decile. If there are N securities total, there would be N/10
>in each decile or 2*N/10 in the bottom/top together. If sd(r) is the
>standard deviation of the returns of these securities (just those in
>the extreme deciles), the standard error would be:
>
>SE = sd(r) / sqrt(N/5)
>
>This would suggest that a reasonable confidence interval around 8%
>might be +/- 2 times SE. Does that make sense?
>
>Thanks,
>
>Dave Kane
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From patrick at burns-stat.com  Sun Jun 25 20:39:14 2006
From: patrick at burns-stat.com (Patrick Burns)
Date: Sun, 25 Jun 2006 19:39:14 +0100
Subject: [R-sig-Finance] MC simulations from R-SIG-Finance Digest, Vol 25,
 Issue 17
In-Reply-To: <20060622101623.8BF5BA8CBD@smtp2.libero.it>
References: <20060622101623.8BF5BA8CBD@smtp2.libero.it>
Message-ID: <449ED852.3070906@burns-stat.com>

Actually 'cbind' is not a very good choice.  Operations
like:

results <- cbind(results, vect)

and

vect <- c(vect, newval)

eat up a lot of memory.  Instead you want to do something
like:

ans <- array(NA, c(length.result, trials))
for(i in 1:trials) {
    this.result <- ...
    ans[,i] <- this.result
}

S Poetry has some explanation of the fragmentation of
memory that the "bad" operations create.  S Poetry is also
one place, among many, to find information on the commands
to write code such as a simulation like this.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


Daniele Amberti wrote:

>    vect<-c(logAAA,x[2:length(x)])
>    plot.ts(exp(vect), plot.type = c("single"),col="red",type="l")
>    lines(AAA)
>
>istead of plot you should use cbind()
>for istance:
>
>vect <- c()
>results <- cbind(results, vect)
>
>so results will have a column for each calculated vect...
>
>Regards
>Daniele Amberti
>
>
>Message: 5
>Date: Thu, 22 Jun 2006 11:03:56 +0200
>From: "Jens Wildermuth" <Jens.Wildermuth at gmx.de>
>Subject: [R-sig-Finance] MC simulation
>To: r-sig-finance at stat.math.ethz.ch
>Message-ID: <20060622090356.104080 at gmx.net>
>Content-Type: text/plain; charset="iso-8859-1"
>
>Hi, 
>
>I am pretty new to R. However, I checked the introduction to R paper and did
>not find a solution for my problem. I use a mean reversion process to
>simulate US RMBS spreads for different rating classes. My problem is that
>for each MC run I would like to generate a vector and attach these vectors
>with each other to generate a matrix, which contains all the simulation
>results. Right now I just see the results of every simulation run because I
>plot them. I tried something using the print(x) (as.matrix() and so forth)
>command but I didn't work out properly. I would very much appreciate any
>help on that problem. Thanks in advance. 
>
>Cheers 
>Jens
>
>
>#Set up variables
>AAA<-data$AAA
>AA<-data$AA
>A<-data$A
>BBB<-data$BBB
>
>logAAA<-log(AAA)
>logAA<-log(AA)
>logA<-log(A)
>logBBB<-log(BBB)
>
>diffAAA<-logAAA[1:100]-logAAA[2:101]
>diffAA<-logAA[1:100]-logAA[2:101]
>diffA<-logA[1:100]-logA[2:101]
>diffBBB<-logBBB[1:100]-logBBB[2:101]
>
>#Estimate coefficients by OLS
>regAAA<-lm(diffAAA ~ logAAA[2:101])
>
>#Parameters for O-U process
>n?<--log(1+regAAA$coefficients[2])
>averagex<--(regAAA$coefficients[1]/regAAA$coefficients[2])
>resierror<-sqrt(var(regAAA$residuals)) #check this 
>sigma<-resierror*sqrt((2*log(1+regAAA$coefficients[2]))/((1+regAAA$coefficie
>nts[2])^2-1))
>
>
>lAAA<-length(diffAAA)
>x<-logAAA[lAAA]
>t<-c(1:60)
>delta<-1
>mcsim<-40
>
>par(mfrow=c(4,1))
>
>#MC for AAA
>for ( k in 1:mcsim ){
>
>    rn<-rnorm(length(t), mean=0,sd=1)
>
>    for ( i in 1:length(t) ){
> 
>x[t+1]<-x[t]*exp(-n?*delta*t)+averagex*(1-exp(-n?*delta*t))+sigma*sqrt((1-ex
>p(-2*n?*delta*t))/(2*n?))*rn[t]
>        }
>    vect<-c(logAAA,x[2:length(x)])
>    plot.ts(exp(vect), plot.type = c("single"),col="red",type="l")
>    lines(AAA)
>}
>
>  
>


From vivek.satsangi at gmail.com  Mon Jun 26 02:21:15 2006
From: vivek.satsangi at gmail.com (Vivek Satsangi)
Date: Sun, 25 Jun 2006 20:21:15 -0400
Subject: [R-sig-Finance] Confidence intervals for spread returns
Message-ID: <bcb171920606251721td2edd76pe80c55e2c047448c@mail.gmail.com>

Hello Mr. Kane,

My own )very limited) experience suggests that there are two large
sources of error here that are not adequately considered if one were
to come up with confidence intervals. On a purely statistical basis,
the confidence interval can be calculated by using boot stapping on
the realized returns, either by just taking the quarterly returns
(being careful not to count annual returns but considering them each
quarter -- an error I have often seen) or by somehow combining/
randomizing the realized returns of each observed decile over time.
However, I think the key to the answer is in considering how the
results will be used.

1. Suppose one were to do boot strapping, then the confidence interval
would suffer greatly from potential survival, reporting, backfill,
etc. bias because these often tend to be concentrated in the extreme
deciles. And estimation error of the confidence interval would itself
be quite large because even in US markets we will quickly run out of
data. Now sensible people obviously control for these things, but once
the results are comitted to a summary statistic like "the 95%
confidence interval for the hedged return is 5% to 10%", then all that
the bosses of people like me will remember will be that summary, just
like all that anyone remembers of the BHB asset allocation study is
that "93.6% of the variation" statistic. The streakiness
(autocorrelation) of the results, the  fact that the hedged return
itself is not normally distributed, any trend in the hedged return (is
the factor being arbed away?), the potential error in the confidence
interval itself, etc. is all forgotten.

2. The influence of transaction costs, the speed of execution, messed
up incentives, etc. don't appear to me to be second-order effects. In
fact, it may be an entirely defensible position that the reason most
of the factors continue to be useful alpha generators is because of
these factors -- and as such, the confidence interval will be
dependent on the transaction details (e.g. if the boundaries of the
confidence interval are highly sensitive to the transaction cost
assumption (which I suspect that they will be), then stating a
confidence interval would end up being misleading to the user of the
information.

Now, I know that you did not ask about either of the above things in
your original question. But I am trying to assert that the user of the
information may rely on the confidence interval entirely too much
given the HUGE potential for errors. Somehow, that fact should be
conveyed.

I just got done reading the Edward Tufte books again, and I think it
might be a better goal for your (I should say "our", since I have been
meaning to get involved since recovering from my illness) project
would be create a dasboad view of the performance of the portfolio
simulation, rather than reducing it to a single number or tuple.
People will still tend to compare the summary statistics, but atleast
we would not be guilty of reporting the statitic in a way that hides
"the truth".

With warm regards,

Vivek Satsangi

p.s. While we are on the topic of ways to conduct the search for
alpha, does anyone have recommendations on book like the Tufte books
on how to use statistical graphics for pattern recognition (rather
than to convey information or already identified patterns, which is
what the Tufte books cover nicely). For example, I am hoping that the
books people suggest would answer the question, "what would be a good
way to construct, and things to consider while designing, graphics
that help us rapidly identify useful factors vs. lousy ones?"
(wouldn't *everyone* like to know the answer to that :-) ).

David Kane wote:
> Question: How might one calculate a reasonable confidence interval
> around this 8% spread return?


From oyvfos at yahoo.no  Mon Jun 26 14:14:58 2006
From: oyvfos at yahoo.no (yvind Foshaug)
Date: Mon, 26 Jun 2006 05:14:58 -0700 (PDT)
Subject: [R-sig-Finance] fSeries prob
In-Reply-To: <449CB92C.7020601@pdf.com>
Message-ID: <20060626121458.19733.qmail@web25510.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060626/817d03bf/attachment.pl 

From dave at kanecap.com  Wed Jun 28 17:59:40 2006
From: dave at kanecap.com (David Kane)
Date: Wed, 28 Jun 2006 11:59:40 -0400
Subject: [R-sig-Finance] Confidence intervals for spread returns
In-Reply-To: <bcb171920606251721td2edd76pe80c55e2c047448c@mail.gmail.com>
References: <bcb171920606251721td2edd76pe80c55e2c047448c@mail.gmail.com>
Message-ID: <17570.42860.640003.232551@gargle.gargle.HOWL>

Thanks for these comments. I agree with most of what you say.

Vivek Satsangi writes:
 > Now, I know that you did not ask about either of the above things in
 > your original question. But I am trying to assert that the user of the
 > information may rely on the confidence interval entirely too much
 > given the HUGE potential for errors. Somehow, that fact should be
 > conveyed.

I agree. But a simple R package can only prevent stupid users from
hurting themselves to a certain degree. Is the best answer to report
nothing? To report infinite confidence intervals?

I agree that there is no clear correct answer. But what answer would
you like to see in the package?

 > I just got done reading the Edward Tufte books again, and I think it
 > might be a better goal for your (I should say "our", since I have been
 > meaning to get involved since recovering from my illness) project
 > would be create a dasboad view of the performance of the portfolio
 > simulation, rather than reducing it to a single number or tuple.
 > People will still tend to compare the summary statistics, but atleast
 > we would not be guilty of reporting the statitic in a way that hides
 > "the truth".

That sounds interesting. If you have the time, please take a look at
the code. We are eager for all suggestions/contributions.

Dave Kane


From dave at kanecap.com  Wed Jun 28 22:46:25 2006
From: dave at kanecap.com (David Kane)
Date: Wed, 28 Jun 2006 16:46:25 -0400
Subject: [R-sig-Finance] Confidence intervals for spread returns
In-Reply-To: <449ED553.5020308@earthlink.net>
References: <17561.19467.957034.487545@gargle.gargle.HOWL>
	<449ED553.5020308@earthlink.net>
Message-ID: <17570.60065.735399.585091@gargle.gargle.HOWL>

Krishna Kumar writes:
 > are the portfolio weights found  using simulation. i.e. You simulate the 
 > underlying asset returns and then optimize
 > on the simulated paths?. Bootstrapping seems like a natural way to get 
 > the conf.intervals.

The backtest package, in its current form, is way simpler than
this. All we are doing (for now) is forming quintile (or decile or
whatever) portfolios and reporting the spread returns. We also allow
the user to stratify these by, for example, sector. The reliability of
the spread returns then depends, obviously, on the number of
observations in each sector. The confidence interval is just supposed
to provide a rough guide which takes this effect into account.

A bootstrap approach seems useful and productive, but we will probably
not be able to integrate it in the first release.

Dave


From wilfred.daye at dbzco.com  Fri Jun 30 20:43:39 2006
From: wilfred.daye at dbzco.com (Daye, Wilfred)
Date: Fri, 30 Jun 2006 14:43:39 -0400
Subject: [R-sig-Finance] Data management question
Message-ID: <6E41ACBA39282B47BD1E231869C1130C032BC9FD@ex3.nyc.dbzco.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060630/33e42a4f/attachment.pl 

