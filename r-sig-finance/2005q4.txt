From fparlamis at gmail.com  Wed Oct  5 02:46:20 2005
From: fparlamis at gmail.com (Parlamis Franklin)
Date: Tue, 4 Oct 2005 14:46:20 -1000
Subject: [R-sig-finance] (i) bug report in listFinCenter and (ii) question
Message-ID: <16E2A01F-5281-4042-976F-D8961BF558AD@mac.com>

     Hello all.

     I am using the Mac OS X version of R (Aqua GUI 1.12, R Framework  
2.1.1).

    (i) The function 'listFinCenter' in the fCalendar package does  
not work.  Having checked the code of that function it appears the  
problem is in the passing of the '*' regular expression pattern to  
the R grep function.  R complains that this is not a valid regular  
expression.  I am not sure whether this bug is the result of more  
liberal regular expression matching on the Windows platform (on which  
fCalendar has been exclusively tested) than on UNIX, or whether it  
results from the fact that R, as of version 2.1.0, has switched its  
standard for regular expression matching to glibc 2.3.3 from grep 2.4.2.

     In any event, I have found that replacing the pattern '*' with  
the pattern '.*' will do the trick (although I must admit the entire  
grep call seems a bit superfluous to me--perhaps the function was  
originally intended to do more than it currently does).

    (ii) As we all know by now, fCalendar needs the TZ environment  
variable to be set to 'GMT'.  However, as installed on my Mac, a  
Sys.getenv() call returned no environment variable named TZ.  My  
solution was to use Sys.putenv() in my .Rprofile file to initialize  
and set the TZ variable to 'GMT'.  This stopped the program from  
complaining, but I am a bit concerned that the solution will not be a  
complete one.  My system clock is still set to my local time zone and  
daylight savings rules, so that other programs in other environments  
on my system know the time to be different than R.  Does anyone  
foresee a problem with this workaround?  Is there a better way?

     Thanks in advance.

     Franklin Parlamis


From fparlamis at gmail.com  Wed Oct  5 02:55:30 2005
From: fparlamis at gmail.com (Franklin Parlamis)
Date: Tue, 4 Oct 2005 14:55:30 -1000
Subject: [R-sig-finance] (i) bug report in listFinCenter and (ii) question
Message-ID: <D4F10697-3C80-4FA5-92D3-3CD6A9D116BE@gmail.com>

     Hello all.

     I am using the Mac OS X version of R (Aqua GUI 1.12, R Framework  
2.1.1).

    (i) The function 'listFinCenter' in the fCalendar package does  
not work.  Having checked the code of that function it appears the  
problem is in the passing of the '*' regular expression pattern to  
the R grep function.  R complains that this is not a valid regular  
expression.  I am not sure whether this bug is the result of more  
liberal regular expression matching on the Windows platform (on which  
fCalendar has been exclusively tested) than on UNIX, or whether it  
results from the fact that R, as of version 2.1.0, has switched its  
standard for regular expression matching to glibc 2.3.3 from grep 2.4.2.

     In any event, I have found that replacing the pattern '*' with  
the pattern '.*' will do the trick (although I must admit the entire  
grep call seems a bit superfluous to me--perhaps the function was  
originally intended to do more than it currently does).

    (ii) As we all know by now, fCalendar needs the TZ environment  
variable to be set to 'GMT'.  However, as installed on my Mac, a  
Sys.getenv() call returned no environment variable named TZ.  My  
solution was to use Sys.putenv() in my .Rprofile file to initialize  
and set the TZ variable to 'GMT'.  This stopped the program from  
complaining, but I am a bit concerned that the solution will not be a  
complete one.  My system clock is still set to my local time zone and  
daylight savings rules, so that other programs in other environments  
on my system know the time to be different than R.  Does anyone  
foresee a problem with this workaround?  Is there a better way?

     Thanks in advance.

     Franklin Parlamis


From edd at debian.org  Wed Oct  5 03:01:13 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 4 Oct 2005 20:01:13 -0500
Subject: [R-sig-finance] (i) bug report in listFinCenter and (ii)
	question
In-Reply-To: <16E2A01F-5281-4042-976F-D8961BF558AD@mac.com>
References: <16E2A01F-5281-4042-976F-D8961BF558AD@mac.com>
Message-ID: <17219.9689.161400.217156@basebud.nulle.part>


Hi,

On 4 October 2005 at 14:46, Parlamis Franklin wrote:
|      Hello all.
| 
|      I am using the Mac OS X version of R (Aqua GUI 1.12, R Framework  
| 2.1.1).
| 
|     (i) The function 'listFinCenter' in the fCalendar package does  
| not work.  Having checked the code of that function it appears the  
| problem is in the passing of the '*' regular expression pattern to  
| the R grep function.  R complains that this is not a valid regular  
| expression.  I am not sure whether this bug is the result of more  
| liberal regular expression matching on the Windows platform (on which  
| fCalendar has been exclusively tested) than on UNIX, or whether it  
| results from the fact that R, as of version 2.1.0, has switched its  
| standard for regular expression matching to glibc 2.3.3 from grep 2.4.2.
| 
|      In any event, I have found that replacing the pattern '*' with  
| the pattern '.*' will do the trick (although I must admit the entire  
| grep call seems a bit superfluous to me--perhaps the function was  
| originally intended to do more than it currently does).

I think Diethelm listens in here, so he may pick this up. If all else fails
you can always send bug reports to the package author/maintainer :)

|     (ii) As we all know by now, fCalendar needs the TZ environment  
| variable to be set to 'GMT'.  However, as installed on my Mac, a  
| Sys.getenv() call returned no environment variable named TZ.  My  
| solution was to use Sys.putenv() in my .Rprofile file to initialize  
| and set the TZ variable to 'GMT'.  This stopped the program from  
| complaining, but I am a bit concerned that the solution will not be a  
| complete one.  My system clock is still set to my local time zone and  
| daylight savings rules, so that other programs in other environments  
| on my system know the time to be different than R.  Does anyone  
| foresee a problem with this workaround?  Is there a better way?

I think that is a fair solution as .Rprofile should only affect R, and
processes spawned by R, if any.  So it should _not_ spill over to other
apps.  

We have the same issue on Debian where timezone behaviour is controlled by
the file /etc/timezone, and TZ is unset by default. I also wish the code in
fCalendar (and similarly in its and other packages using timezone info) was
smarter amd set a default as fallback.

Best regards, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'


From fparlamis at mac.com  Wed Oct  5 03:11:02 2005
From: fparlamis at mac.com (Parlamis Franklin)
Date: Tue, 4 Oct 2005 15:11:02 -1000
Subject: [R-sig-finance] (i) bug report in listFinCenter and (ii)
	question
In-Reply-To: <17219.9689.161400.217156@basebud.nulle.part>
References: <16E2A01F-5281-4042-976F-D8961BF558AD@mac.com>
	<17219.9689.161400.217156@basebud.nulle.part>
Message-ID: <9A09209B-F021-43F2-AA2B-BF49C4A19495@mac.com>

Thanks, Dirk.  I tried to send the bug report to Diethelm, but his  
email address bounced, and he does not list an alternate on the  
Rmetrics web site (only his phone number).

On Oct 4, 2005, at 3:01 PM, Dirk Eddelbuettel wrote:

>
> Hi,
>
> On 4 October 2005 at 14:46, Parlamis Franklin wrote:
> |      Hello all.
> |
> |      I am using the Mac OS X version of R (Aqua GUI 1.12, R  
> Framework
> | 2.1.1).
> |
> |     (i) The function 'listFinCenter' in the fCalendar package does
> | not work.  Having checked the code of that function it appears the
> | problem is in the passing of the '*' regular expression pattern to
> | the R grep function.  R complains that this is not a valid regular
> | expression.  I am not sure whether this bug is the result of more
> | liberal regular expression matching on the Windows platform (on  
> which
> | fCalendar has been exclusively tested) than on UNIX, or whether it
> | results from the fact that R, as of version 2.1.0, has switched its
> | standard for regular expression matching to glibc 2.3.3 from grep  
> 2.4.2.
> |
> |      In any event, I have found that replacing the pattern '*' with
> | the pattern '.*' will do the trick (although I must admit the entire
> | grep call seems a bit superfluous to me--perhaps the function was
> | originally intended to do more than it currently does).
>
> I think Diethelm listens in here, so he may pick this up. If all  
> else fails
> you can always send bug reports to the package author/maintainer :)
>
> |     (ii) As we all know by now, fCalendar needs the TZ environment
> | variable to be set to 'GMT'.  However, as installed on my Mac, a
> | Sys.getenv() call returned no environment variable named TZ.  My
> | solution was to use Sys.putenv() in my .Rprofile file to initialize
> | and set the TZ variable to 'GMT'.  This stopped the program from
> | complaining, but I am a bit concerned that the solution will not  
> be a
> | complete one.  My system clock is still set to my local time zone  
> and
> | daylight savings rules, so that other programs in other environments
> | on my system know the time to be different than R.  Does anyone
> | foresee a problem with this workaround?  Is there a better way?
>
> I think that is a fair solution as .Rprofile should only affect R, and
> processes spawned by R, if any.  So it should _not_ spill over to  
> other
> apps.
>
> We have the same issue on Debian where timezone behaviour is  
> controlled by
> the file /etc/timezone, and TZ is unset by default. I also wish the  
> code in
> fCalendar (and similarly in its and other packages using timezone  
> info) was
> smarter amd set a default as fallback.
>
> Best regards, Dirk
>
> -- 
> Statistics: The (futile) attempt to offer certainty about uncertainty.
>          -- Roger Koenker, 'Dictionary of Received Ideas of  
> Statistics'
>


From atp at piskorski.com  Thu Oct  6 03:16:28 2005
From: atp at piskorski.com (Andrew Piskorski)
Date: Wed, 5 Oct 2005 21:16:28 -0400
Subject: [R-sig-finance] (i) bug report in listFinCenter and (ii)
	question
In-Reply-To: <16E2A01F-5281-4042-976F-D8961BF558AD@mac.com>
References: <16E2A01F-5281-4042-976F-D8961BF558AD@mac.com>
Message-ID: <20051006011628.GA24065@tehun.pair.com>

On Tue, Oct 04, 2005 at 02:46:20PM -1000, Parlamis Franklin wrote:

>    (i) The function 'listFinCenter' in the fCalendar package does  
> not work.  Having checked the code of that function it appears the  
> problem is in the passing of the '*' regular expression pattern to  
> the R grep function.  R complains that this is not a valid regular  
> expression.  I am not sure whether this bug is the result of more  

>     In any event, I have found that replacing the pattern '*' with  
> the pattern '.*' will do the trick (although I must admit the entire  

Well, that's quite correct, '*' is typically NOT a valid regular
expression at all.  That probably varies by regexp library (and I'm
not sure what the spec'd POSIX behavior is).  But in any regexp
library where '*' IS a valid regexp, it's going to match the literal
character '*' - AKA, it is equivalent to the regular expression '\*'
or '[*]'.

So which did the author intend, greping for '[*]', or for '.*'?  The
results will be very different.  Either way, using plain '*' is a bad
idea.

-- 
Andrew Piskorski <atp at piskorski.com>
http://www.piskorski.com/


From silmarelli at yahoo.co.uk  Sun Oct  9 15:49:08 2005
From: silmarelli at yahoo.co.uk (Silvia Marelli)
Date: Sun, 9 Oct 2005 14:49:08 +0100 (BST)
Subject: [R-sig-finance] Monte Carlo and Portfolio Optimization
Message-ID: <20051009134908.19208.qmail@web25905.mail.ukl.yahoo.com>

Hi,
I am trying to build some realistic efficient
portfolios using some mean/variance techniques
(Markowitz, CAPM etc...).
I normally end up with an unrealistic concentration of
the wealth in a too limited number of assets.
I heard about Monte Carlo techniques to account for
the unaccuracy of the information available.
What would be a good starting point?
I am not experienced, so I need to keep it as simple
as possible.
Should I simply optimize many ptfs, by sampling the
return of each asset from a distribution which I
assume to be a Gaussian centered on the expected
return of the asset?
Is it possible to introduce some "noise" also in the
covariance matrix?
Then how should I "average out" the results?
I am not very familiar with these techniques, so if
anyone can suggest some online resources, I would be
very grateful.
Regards

Silvia


From patrick at burns-stat.com  Sun Oct  9 18:32:40 2005
From: patrick at burns-stat.com (Patrick Burns)
Date: Sun, 09 Oct 2005 17:32:40 +0100
Subject: [R-sig-finance] Monte Carlo and Portfolio Optimization
In-Reply-To: <20051009134908.19208.qmail@web25905.mail.ukl.yahoo.com>
References: <20051009134908.19208.qmail@web25905.mail.ukl.yahoo.com>
Message-ID: <43494628.9090805@burns-stat.com>

This seems like two different issues to me.

Putting a limit on the maximum weight of the assets should
alleviate the concentration that you are worried about.

The POP User's Manual (on http://www.burns-stat.com)
has a very brief section on resampling with a couple of
references, but for on-line resources I suspect that googling
for 'resampled efficient frontier' would be a good move.
The POP manual is probably more useful as background
for optimization in general than for information on resampling.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Silvia Marelli wrote:

>Hi,
>I am trying to build some realistic efficient
>portfolios using some mean/variance techniques
>(Markowitz, CAPM etc...).
>I normally end up with an unrealistic concentration of
>the wealth in a too limited number of assets.
>I heard about Monte Carlo techniques to account for
>the unaccuracy of the information available.
>What would be a good starting point?
>I am not experienced, so I need to keep it as simple
>as possible.
>Should I simply optimize many ptfs, by sampling the
>return of each asset from a distribution which I
>assume to be a Gaussian centered on the expected
>return of the asset?
>Is it possible to introduce some "noise" also in the
>covariance matrix?
>Then how should I "average out" the results?
>I am not very familiar with these techniques, so if
>anyone can suggest some online resources, I would be
>very grateful.
>Regards
>
>Silvia
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
>  
>


From spencer.graves at pdf.com  Sun Oct  9 23:26:03 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 09 Oct 2005 14:26:03 -0700
Subject: [R-sig-finance] Portfolio performance metrics/ratios
In-Reply-To: <ff91746c0509270423719b396f@mail.gmail.com>
References: <ff91746c0509270423719b396f@mail.gmail.com>
Message-ID: <43498AEB.2050606@pdf.com>

	  Have you considerred RMetrics (www.rmetrics.org)?  If yes, and if you 
would still like more help from this list, I suggest you review the 
Posting Guide for r-help (www.R-project.org/posting-guide.html) and 
submit a more specific question, preferably with some code that 
illustrates what you are trying to do and helps describe where you would 
like further assistance.

	  spencer graves

Sankalp Upadhyay wrote:
> Hi,
> 
> I am looking for a package that can calculate portfolio performance
> metrics and ratios from a time series of returns or asset values. Is
> there such a package? one that can give comparison ratios with
> benchmarks also?
> The closest seems to be tseries that has maxdrawdown, sharpe and
> sterling ratios. I have started writing own code based on that but
> there are a lot of ratios and performance analysis metrics.
> Am I missing some package that can help?
> 
> Thanks,
> 
> Sankalp
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915


From kb2qzv at poczta.wp.pl  Mon Oct 10 11:58:52 2005
From: kb2qzv at poczta.wp.pl (Benedict P. Barszcz)
Date: Mon, 10 Oct 2005 11:58:52 +0200
Subject: [R-sig-finance] candlestick charts and fMultivar functions?
Message-ID: <200510101158.53082.kb2qzv@poczta.wp.pl>

Hi, I'd need to have light shed on the question I posted on the r-help. 
Apparently no one there noticed it. Can you have a look at it, please?

https://stat.ethz.ch/pipermail/r-help/2005-October/079011.html

-- 
Benedict

Cyfrowy klucz publiczny / Digital public key 
http://agrypa1.atspace.com/klucze/kb2qzv_wp.pl-public.asc


From david.jessop at ubs.com  Mon Oct 10 12:40:10 2005
From: david.jessop at ubs.com (david.jessop@ubs.com)
Date: Mon, 10 Oct 2005 11:40:10 +0100
Subject: [R-sig-finance] Monte Carlo and Portfolio Optimisation
Message-ID: <D86B416F551CBA489FDED1567D11805108BAA9@NLDNC105PEX1.ubsw.net>

Hi,

As Patrick Burns said, you have various problems here.  

Resampling is a technique popularised by Richard Michaud and is described quite well in hid book which from memory is called Efficient Portfolio Management (or something like that).  The basic idea is that you assume the return / covariance structure you have is the real structure and then generate lots of samples using this as the distribution, calculate the efficient frontiers and then average them. 

There are numerous problems with this, and many are rehearsed in Bernd Scherer's book (again from memory Portfolio Construction and RiskBudgetting). These include the fact that your covariance matrix isn't the true one - it is a sample itself; log returns aren't normally distributed; and that there will be an upward bias to the weights of stocks with a high standard deviation of returns assuming you have the constraint that the weights in the portfolio will be positive.

One way round some of these problems was implemented (I don't know who did it first) by Robert Rice (www.occamsrazor.com I think - or google his name and I'm sure you'll find it), which is if you have the original time series used to build the covariance matrix then just use bootstrapping on this to generate your various covariance matrices.  

However, all these methods try and fix the problem by averaging everything, yet there is a relatively old academic paper (which as I'm not at home I can't find the reference) which shows that it is errors in the returns that explain virtually all the problems the original author was getting - so "messing around" with the covariance matrix probably doesn't add much. 

This leads to the ideas in the Black-Litterman model, where you condidion your return forecasts on the covariance matrix, explicitly admitting that your forecasts are distributions, not single points.

I hope that helps,

David Jessop
--------------------------
David Jessop
Head of European Quantitative & Derivative Research
UBS Investment Research

+44 20 7567 9882

This communication is issued by UBS AG or an affiliate ("UBS...{{dropped}}


From hkahra at gmail.com  Mon Oct 10 13:36:59 2005
From: hkahra at gmail.com (Hannu Kahra)
Date: Mon, 10 Oct 2005 12:36:59 +0100
Subject: [R-sig-finance] Monte Carlo and Portfolio Optimization
In-Reply-To: <20051009134908.19208.qmail@web25905.mail.ukl.yahoo.com>
References: <20051009134908.19208.qmail@web25905.mail.ukl.yahoo.com>
Message-ID: <3d35a2ca0510100436j4b4454f4vd2c49769e673b1ea@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20051010/544df57a/attachment.pl

From con.keating at financedevelopmentcentre.com  Mon Oct 10 14:28:29 2005
From: con.keating at financedevelopmentcentre.com (con.keating@financedevelopmentcentre.com)
Date: Mon, 10 Oct 2005 13:28:29 +0100
Subject: [R-sig-finance] Monte Carlo and Portfolio Optimization
In-Reply-To: <3d35a2ca0510100436j4b4454f4vd2c49769e673b1ea@mail.gmail.com>
References: <20051009134908.19208.qmail@web25905.mail.ukl.yahoo.com>
	<3d35a2ca0510100436j4b4454f4vd2c49769e673b1ea@mail.gmail.com>
Message-ID: <20051010132829.h3x7tjdqezk08cwo@webmail.financedevelopmentcentre.com>

Silvia

 From memory, The Chopra and Ziemba (1993) results were that errors in 
means were
approximately 8 x as important as errors in standard deviation which 
were 4 x as
important as errors in the correlation matrix.

There ia also a criticism of the Michaud resampling method by Cam 
Harvey at Duke
where he shows that it is strictly sub-optimal.

Given anyway that most returns are pretty far from Normal in the multivariate
setting if not the univariate, I wonder how accurate anyone can hope to 
be in a
mean-variance framework-I was recently working with an equity index problem
where correlations were of the order of 0.75 (quarterly data) at which levels
concentration should be expected.

Hannu's suggestions seem worth investigating.

Con Keating

Quoting Hannu Kahra <hkahra at gmail.com>:

> Silvia,
> David Jessop refers in a separate mail to Berndt Scher's book "Portfolio
> Construction and Risk Budgeting"
> http://www.amazon.com/exec/obidos/tg/detail/-/1904339301/qid=1128942250/sr=8-1/ref=sr_8_xs_ap_i1_xgl14/103-5833626-9777409?v=glance&s=books&n=507846
> .
> His latetes book is with Douglas Martin: " Introduction to Modern Portfolio
> Optimization with NuOPT, S-PLUS and S+Bayes"
> http://www.amazon.com/exec/obidos/tg/detail/-/0387210164/qid=1128942250/sr=8-2/ref=sr_8_xs_ap_i2_xgl14/103-5833626-9777409?v=glance&s=books&n=507846
>
> and I remember that in this book he is against using portfolio resampling
> with the Gaussian assumption.
> I recall (I do not have the references with me) that Ziemba and Kallberg
> have found that errors in the means are 10-20 times more serious (for the
> portfolio) than errors in the covariances. As a consequence the resulting
> portfolios tend to be very consentrating such that a lot of assets have zero
> weights.
> An old trick is to apply the so-called Bayes-Stein or James Stein
> estimators that shrink the sample means towards some fixed value, e.g. the
> grand mean or the return on the minumum variance portfolio. The latter is
> the approach suggested by Jorion. The Black-Litterman "model" applies
> shrinkage towards the CAPM equilibrium returns, since it applies the
> inversion of the Markowitz problem.
> Instead of using standard Markowitz optimization, benchmark related
> optimization (explained in Scherer's Portfolio Construction book) may be
> usefull.
> I hope these help.
> Hannu Kahra
> Turku Business School, Finland
> and CeRP, Italy
>
>
> On 10/9/05, Silvia Marelli <silmarelli at yahoo.co.uk> wrote:
>>
>> Hi,
>> I am trying to build some realistic efficient
>> portfolios using some mean/variance techniques
>> (Markowitz, CAPM etc...).
>> I normally end up with an unrealistic concentration of
>> the wealth in a too limited number of assets.
>> I heard about Monte Carlo techniques to account for
>> the unaccuracy of the information available.
>> What would be a good starting point?
>> I am not experienced, so I need to keep it as simple
>> as possible.
>> Should I simply optimize many ptfs, by sampling the
>> return of each asset from a distribution which I
>> assume to be a Gaussian centered on the expected
>> return of the asset?
>> Is it possible to introduce some "noise" also in the
>> covariance matrix?
>> Then how should I "average out" the results?
>> I am not very familiar with these techniques, so if
>> anyone can suggest some online resources, I would be
>> very grateful.
>> Regards
>>
>> Silvia
>>
>> _______________________________________________
>> R-sig-finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From kb2qzv at poczta.wp.pl  Mon Oct 10 14:56:41 2005
From: kb2qzv at poczta.wp.pl (Benedict P. Barszcz)
Date: Mon, 10 Oct 2005 14:56:41 +0200
Subject: [R-sig-finance] ohlcPlot
Message-ID: <200510101456.41932.kb2qzv@poczta.wp.pl>

I am looking for examples on how to use ohlcPlot functions.
I dont understand what parameters (in its simpleast from) should be provided 
to it.
This is the synopsis it uses:

ohlcPlot(x, xlim = NULL, ylim = NULL, xlab = "Time", ylab, col = par("col"),
             bg = par("bg"), axes = TRUE, frame.plot = axes, ann = par("ann"),
             main = NULL, date = c("calendar", "julian"), format = "%Y-%m-%d",
         origin = "1899-12-30", ...)

Where does it say about the OHLC values? 
-- 
Benedict

Cyfrowy klucz publiczny / Digital public key 
http://agrypa1.atspace.com/klucze/kb2qzv_wp.pl-public.asc


From sankalp.upadhyay at gmail.com  Mon Oct 10 23:28:13 2005
From: sankalp.upadhyay at gmail.com (Sankalp Upadhyay)
Date: Tue, 11 Oct 2005 02:58:13 +0530
Subject: [R-sig-finance] Portfolio performance metrics/ratios
In-Reply-To: <43498AEB.2050606@pdf.com>
References: <ff91746c0509270423719b396f@mail.gmail.com>
	<43498AEB.2050606@pdf.com>
Message-ID: <ff91746c0510101428l672f8618x85d32805f3aaa15b@mail.gmail.com>

As I had written, these are portfolio performance metrics - those that
an investor or evaluator of a fund would calculate given the return
series. So in that sense there is not much statistics in it and
functions are standard and straight forward - this is the reason I
thought that some package must contain them,

Anyways, it is done now. e.g.

sortino <- function(returns, MAR = 0) {
	if (length (returns) <= 1) NA
	else mean (returns - MAR) / downDeviation(returns, MAR)
}

trackingError <- function (portfolio, benchmark) {
	if (length (portfolio) != length (benchmark)) NA
	else if (length (portfolio) <= 1) NA
	else sd (portfolio - benchmark)
}

informationRatio <- function (portfolio, benchmark) {
	if (length (portfolio) != length (benchmark)) NA
	else if (length (portfolio) <= 1) NA
	else mean (portfolio - benchmark) / trackingError (portfolio, benchmark)
}

In case anyone needs these let me know - there are a lot of them. I
can clean them up and send it,
Thanks for the help,

Sankalp

On 10/10/05, Spencer Graves <spencer.graves at pdf.com> wrote:
>           Have you considerred RMetrics (www.rmetrics.org)?  If yes, and if you
> would still like more help from this list, I suggest you review the
> Posting Guide for r-help (www.R-project.org/posting-guide.html) and
> submit a more specific question, preferably with some code that
> illustrates what you are trying to do and helps describe where you would
> like further assistance.
>
>           spencer graves
>
> Sankalp Upadhyay wrote:
> > Hi,
> >
> > I am looking for a package that can calculate portfolio performance
> > metrics and ratios from a time series of returns or asset values. Is
> > there such a package? one that can give comparison ratios with
> > benchmarks also?
> > The closest seems to be tseries that has maxdrawdown, sharpe and
> > sterling ratios. I have started writing own code based on that but
> > there are a lot of ratios and performance analysis metrics.
> > Am I missing some package that can help?
> >
> > Thanks,
> >
> > Sankalp
> >
> > _______________________________________________
> > R-sig-finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
> --
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
>
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>


--
--
Sankalp Upadhyay
Investment Analyst
Roulac Global Funds


From hkahra at gmail.com  Tue Oct 11 08:54:14 2005
From: hkahra at gmail.com (Hannu Kahra)
Date: Tue, 11 Oct 2005 07:54:14 +0100
Subject: [R-sig-finance] Monte Carlo and Portfolio Optimization
In-Reply-To: <20051009134908.19208.qmail@web25905.mail.ukl.yahoo.com>
References: <20051009134908.19208.qmail@web25905.mail.ukl.yahoo.com>
Message-ID: <3d35a2ca0510102354s6052ddb8wdbad799c9db4ead6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20051011/5e12afa0/attachment.ksh

From L.Isella at myrealbox.com  Tue Oct 11 12:53:06 2005
From: L.Isella at myrealbox.com (L.Isella)
Date: Tue, 11 Oct 2005 10:53:06 +0000
Subject: [R-sig-finance] MarkowitzPortfolio Package
Message-ID: <1129027986.c816015cL.Isella@myrealbox.com>

Dear All,
I am also interested in the problem of portfolio optimization.
Leaving aside for a while the theoretical discussions, I am practising with the MarkowitzPortfolio package in Rmetrics.
The portfolioMarkowitz routine has a bug still present in the version I installed on a Windows machine today.
I was suggested how to fix it some months ago by changing a line in the code (on a Linux machine at the time).
Can I use the same approach on Windows machine?
Then, the montecarloMarkowitz can be used to generate a number of random portfolios.
But how are they actually generated? Can they be of any use to work out the resampled efficient frontier?
Finally, how can one access the data used to generate them? I cannot find where and how they are saved.
Thanks in advance

Lorenzo


From fparlamis at mac.com  Wed Oct 12 20:52:19 2005
From: fparlamis at mac.com (Parlamis Franklin)
Date: Wed, 12 Oct 2005 08:52:19 -1000
Subject: [R-sig-finance] problem when both fCalendar and chron packages are
	loaded
Message-ID: <07D7568D-79CF-480A-93CA-9658AB0A3E0A@mac.com>

When both the fCalendar and chron packages are loaded, the function  
easter(2005) returns the following object of class sdate:

[1] 'myFinCenter'
[2] [2015-03-28]

If the chron package is unloaded, the result becomes correct.  this  
must be related to the following message which appears when attaching  
chron:

     "The following object(s) are masked from package:fCalendar :

      day.of.week leap.year month.day.year"

It may also be related to sometimes differing origins for the julian  
calendar functions (I have alternately seen 1960-01-01 and 1970-01-01  
as the origin).

I cc'd the chron package maintainer.  I am hoping Diethelm, the  
fCalendar package manager, picks up this list as well.

Franklin Parlamis


From ggrothendieck at gmail.com  Wed Oct 12 21:54:35 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Oct 2005 15:54:35 -0400
Subject: [R-sig-finance] problem when both fCalendar and chron packages
	are loaded
In-Reply-To: <07D7568D-79CF-480A-93CA-9658AB0A3E0A@mac.com>
References: <07D7568D-79CF-480A-93CA-9658AB0A3E0A@mac.com>
Message-ID: <971536df0510121254t27e689b8x677a364124a1bcdd@mail.gmail.com>

If you don't need the shift= argument in easter then a workaround
would be:

my.easter <- function(year) structure(.easter.sunday(year), "sdate")

which does not depend on any masked functions.


On 10/12/05, Parlamis Franklin <fparlamis at mac.com> wrote:
> When both the fCalendar and chron packages are loaded, the function
> easter(2005) returns the following object of class sdate:
>
> [1] 'myFinCenter'
> [2] [2015-03-28]
>
> If the chron package is unloaded, the result becomes correct.  this
> must be related to the following message which appears when attaching
> chron:
>
>     "The following object(s) are masked from package:fCalendar :
>
>      day.of.week leap.year month.day.year"
>
> It may also be related to sometimes differing origins for the julian
> calendar functions (I have alternately seen 1960-01-01 and 1970-01-01
> as the origin).
>
> I cc'd the chron package maintainer.  I am hoping Diethelm, the
> fCalendar package manager, picks up this list as well.
>
> Franklin Parlamis
\


From spencer.graves at pdf.com  Thu Oct 13 00:52:27 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 12 Oct 2005 15:52:27 -0700
Subject: [R-sig-finance] problem when both fCalendar and chron packages
 are	loaded
In-Reply-To: <07D7568D-79CF-480A-93CA-9658AB0A3E0A@mac.com>
References: <07D7568D-79CF-480A-93CA-9658AB0A3E0A@mac.com>
Message-ID: <434D93AB.4060504@pdf.com>

I've seen the same problem under (if I remember correctly) R 2.1.1 
patched, Windows XP.

	  spencer graves

Parlamis Franklin wrote:

> When both the fCalendar and chron packages are loaded, the function  
> easter(2005) returns the following object of class sdate:
> 
> [1] 'myFinCenter'
> [2] [2015-03-28]
> 
> If the chron package is unloaded, the result becomes correct.  this  
> must be related to the following message which appears when attaching  
> chron:
> 
>      "The following object(s) are masked from package:fCalendar :
> 
>       day.of.week leap.year month.day.year"
> 
> It may also be related to sometimes differing origins for the julian  
> calendar functions (I have alternately seen 1960-01-01 and 1970-01-01  
> as the origin).
> 
> I cc'd the chron package maintainer.  I am hoping Diethelm, the  
> fCalendar package manager, picks up this list as well.
> 
> Franklin Parlamis
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915


From wuertz at itp.phys.ethz.ch  Thu Oct 13 17:49:00 2005
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 13 Oct 2005 15:49:00 +0000
Subject: [R-sig-finance] Status Rmetrics - New Version
Message-ID: <434E81EC.3050401@itp.phys.ethz.ch>

    Problems to be solved with Rmetrics for R-2.2.0
    
    I'm aware of the following problems which will be very
    likely be patched for the Rmetrics Version for R 2.2.0
    
    1.  Changing the compiler from g77 to gfortran
        under Linux created some severe problems and errors.
        STATUS: already solved, fixed with next Rmetrics Version
        
    2.  Does the windows version also compile under gfortran?
        STATUS: yes - I created my own mingw gfortran suite
        
    3.  fCalendar and chron create conflicts and deliver
        wrong results when both are loaded at the same time.
        STATUS: not yet solved - maybe fixed with next version
        
    4.  Markowitz approach from fPortfolio has some bugs ...
        STATUS They will be fixed with the next version of Rmetrics
        
    5.  listFinCenter got a problem with version 2.1.
        STATUS: already solved, fixed with next Rmetrics Version
        
    6.  timeDate got a problem under Windows with version 2.1.
        STATUS: already solved, fixed with next Rmetrics Version
        
    *** The next updated Rmetrics version for R 2.2.0 will become ***
    *** available as soon as possbile, likely within October.     ***
    
    Did you know?
    
        Rmetrics comes with 1399 S functions and 95945 lines of R-code!
        RUnit testing is under progress to improve software quality.
    
    I apologize for any inconveniances caused by bugs and
    software updates.
    
    Diethelm Wuertz


From spencer.graves at pdf.com  Thu Oct 13 18:49:09 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 13 Oct 2005 09:49:09 -0700
Subject: [R-sig-finance] Status Rmetrics - New Version
In-Reply-To: <434E81EC.3050401@itp.phys.ethz.ch>
References: <434E81EC.3050401@itp.phys.ethz.ch>
Message-ID: <434E9005.6060204@pdf.com>

Hi, Diethelm:

	  Thank you for all your hard work in assembling this package and 
making it available to the R community.

	  Best Wishes,
	  Spencer Graves

Diethelm Wuertz wrote:

>     Problems to be solved with Rmetrics for R-2.2.0
>     
>     I'm aware of the following problems which will be very
>     likely be patched for the Rmetrics Version for R 2.2.0
>     
>     1.  Changing the compiler from g77 to gfortran
>         under Linux created some severe problems and errors.
>         STATUS: already solved, fixed with next Rmetrics Version
>         
>     2.  Does the windows version also compile under gfortran?
>         STATUS: yes - I created my own mingw gfortran suite
>         
>     3.  fCalendar and chron create conflicts and deliver
>         wrong results when both are loaded at the same time.
>         STATUS: not yet solved - maybe fixed with next version
>         
>     4.  Markowitz approach from fPortfolio has some bugs ...
>         STATUS They will be fixed with the next version of Rmetrics
>         
>     5.  listFinCenter got a problem with version 2.1.
>         STATUS: already solved, fixed with next Rmetrics Version
>         
>     6.  timeDate got a problem under Windows with version 2.1.
>         STATUS: already solved, fixed with next Rmetrics Version
>         
>     *** The next updated Rmetrics version for R 2.2.0 will become ***
>     *** available as soon as possbile, likely within October.     ***
>     
>     Did you know?
>     
>         Rmetrics comes with 1399 S functions and 95945 lines of R-code!
>         RUnit testing is under progress to improve software quality.
>     
>     I apologize for any inconveniances caused by bugs and
>     software updates.
>     
>     Diethelm Wuertz
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915


From edd at debian.org  Thu Oct 13 20:02:03 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 13 Oct 2005 13:02:03 -0500
Subject: [R-sig-finance] Status Rmetrics - New Version
In-Reply-To: <434E81EC.3050401@itp.phys.ethz.ch>
References: <434E81EC.3050401@itp.phys.ethz.ch>
Message-ID: <17230.41243.372057.681189@basebud.nulle.part>


Hi Diethelm,

Thanks for the status update -- much appreciated.

On 13 October 2005 at 15:49, Diethelm Wuertz wrote:
|     Problems to be solved with Rmetrics for R-2.2.0
|     
|     I'm aware of the following problems which will be very
|     likely be patched for the Rmetrics Version for R 2.2.0
|     
|     1.  Changing the compiler from g77 to gfortran
|         under Linux created some severe problems and errors.
|         STATUS: already solved, fixed with next Rmetrics Version

As far as I know (which may not mean much as I don;t follow compiler issues
all that closely) that GCC upstream does not yet recommend to use gfortran. 
I think Brian Ripley stated about the same on one of the R lists.  

As another data point, Debian switched its default compiler to gcc 4.0.* for
every language but Fortran where g77 from gcc 3.4.* is still used (and 3.4
and 4.0 are said to produce interchangeable object code).  

So it may be too soon to start fixing wholesale for gfortran. Just my $0.02.

The rest looks good -- looking forward to the next Rmetrics release!

Regards, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'


From john.muller at bankofamerica.com  Mon Oct 17 16:12:37 2005
From: john.muller at bankofamerica.com (Muller, John)
Date: Mon, 17 Oct 2005 10:12:37 -0400
Subject: [R-sig-finance] question about Asian option pricing arguments
Message-ID: <8D67B54C47F0A041B8AC53427E66AD6F029510C4@ex2k.bankofamerica.com>

Dear Prof. Wuertz,

I am trying to use the fOptions package to check 
the results of my own Asian option pricing code.

In particular I want to check the method
	 TurnbullWakemanAsianApproxOption

I am a bit confused about the input parameters.

What is the difference between the two asset prices
	 S and SA
and between the 2 times
	Time and time

I am guessing S is the initial price and SA is the initial average.

Thank you for your time and thank greatly for these wonderful packages.

best regards,

- john muller

-------------------------------------------------
John H. Muller
mailto:john.muller at bankofamerica.com
404.607.5943


From Kurt.Hornik at wu-wien.ac.at  Tue Oct 18 12:37:00 2005
From: Kurt.Hornik at wu-wien.ac.at (Kurt Hornik)
Date: Tue, 18 Oct 2005 12:37:00 +0200
Subject: [R-sig-finance] problem when both fCalendar and chron packages
	are loaded
In-Reply-To: <971536df0510121254t27e689b8x677a364124a1bcdd@mail.gmail.com>
References: <07D7568D-79CF-480A-93CA-9658AB0A3E0A@mac.com>
	<971536df0510121254t27e689b8x677a364124a1bcdd@mail.gmail.com>
Message-ID: <17236.53324.205519.131353@mithrandir.hornik.net>

>>>>> Gabor Grothendieck writes:

> If you don't need the shift= argument in easter then a workaround
> would be:

> my.easter <- function(year) structure(.easter.sunday(year), "sdate")

> which does not depend on any masked functions.


> On 10/12/05, Parlamis Franklin <fparlamis at mac.com> wrote:
>> When both the fCalendar and chron packages are loaded, the function
>> easter(2005) returns the following object of class sdate:
>> 
>> [1] 'myFinCenter'
>> [2] [2015-03-28]
>> 
>> If the chron package is unloaded, the result becomes correct.  this
>> must be related to the following message which appears when attaching
>> chron:
>> 
>> "The following object(s) are masked from package:fCalendar :
>> 
>> day.of.week leap.year month.day.year"
>> 
>> It may also be related to sometimes differing origins for the julian
>> calendar functions (I have alternately seen 1960-01-01 and 1970-01-01
>> as the origin).
>> 
>> I cc'd the chron package maintainer.  I am hoping Diethelm, the
>> fCalendar package manager, picks up this list as well.
>> 
>> Franklin Parlamis

Why is that a problem with chron?

Package chron has its functions month.day.year() etc, and fCalendar has
different added ones, but in case chron is loaded on top picks up the
ones from chron.  Seems like fCalendar needs a name space, and it would
not hurt providing one for chron either ...

-k


From sumantab at ambaresearch.com  Tue Oct 18 14:13:44 2005
From: sumantab at ambaresearch.com (Sumanta Basak)
Date: Tue, 18 Oct 2005 17:43:44 +0530
Subject: [R-sig-finance] HELP ON FIGARCH
Message-ID: <14850601FF012647A90A5DB31F96DB371D17E5@INBLRDC01.BANG.irpvl.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20051018/18cd6a98/attachment.pl

From sumantab at ambaresearch.com  Wed Oct 19 12:13:18 2005
From: sumantab at ambaresearch.com (Sumanta Basak)
Date: Wed, 19 Oct 2005 15:43:18 +0530
Subject: [R-sig-finance] FIGARCH
Message-ID: <14850601FF012647A90A5DB31F96DB371D1940@INBLRDC01.BANG.irpvl.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20051019/ae67d045/attachment.pl

From fparlamis at mac.com  Wed Oct 19 21:02:18 2005
From: fparlamis at mac.com (Parlamis Franklin)
Date: Wed, 19 Oct 2005 09:02:18 -1000
Subject: [R-sig-finance] fCalendar
Message-ID: <19959159-0C61-4150-A61B-6D1B5743EF25@mac.com>

hello again:

     another little error in fCalendar.  i sincerely hope this list  
is the place to post these.

     in 2000, Japanese Coming of Age Day was changed from Jan 15 to  
the second monday of January.  accordingly the current function  
JPComingOfAgeDay gives incorrect results.

franklin


From fparlamis at mac.com  Thu Oct 20 00:10:39 2005
From: fparlamis at mac.com (Parlamis Franklin)
Date: Wed, 19 Oct 2005 12:10:39 -1000
Subject: [R-sig-finance] more fCalendar
Message-ID: <132FB39D-A207-4E0E-92AD-49117EADCB86@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20051019/c79a6ab4/attachment.pl

From helprhelp at gmail.com  Fri Oct 21 23:38:55 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 21 Oct 2005 16:38:55 -0500
Subject: [R-sig-finance] get.hist.quote from tseries package
Message-ID: <cdf817830510211438r376df759gbfbd8b148bdbdc05@mail.gmail.com>

Dear Listers:
I am wondering if someone has ever used get.hist.quote to get data
from www. How is the quality of the data, is there any record on
splitting and etc?

I am also wondering if it is possible to get intra-day tick data somehow.

Any sharing of info will be appreciated.

Thanks,

--
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From bbands at gmail.com  Sat Oct 22 17:37:41 2005
From: bbands at gmail.com (BBands)
Date: Sat, 22 Oct 2005 08:37:41 -0700
Subject: [R-sig-finance] get.hist.quote from tseries package
In-Reply-To: <cdf817830510211438r376df759gbfbd8b148bdbdc05@mail.gmail.com>
References: <cdf817830510211438r376df759gbfbd8b148bdbdc05@mail.gmail.com>
Message-ID: <6e8360ad0510220837l5ebfcdcdw3a1d742cf7cef56a@mail.gmail.com>

On 10/21/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> Dear Listers:
> I am wondering if someone has ever used get.hist.quote to get data
> from www.

Yes.

> How is the quality of the data, is there any record on
> splitting and etc?

The quality of the data is quite good, but not perfect. You get what to pay for.

The data is not split adjusted.

> I am also wondering if it is possible to get intra-day tick data somehow.

Not that I am aware of.

    jab
--
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From jgalt70 at yahoo.com  Mon Oct 24 17:40:37 2005
From: jgalt70 at yahoo.com (Andrew West)
Date: Mon, 24 Oct 2005 08:40:37 -0700 (PDT)
Subject: [R-sig-finance] get.hist.quote from tseries package
In-Reply-To: <cdf817830510211438r376df759gbfbd8b148bdbdc05@mail.gmail.com>
Message-ID: <20051024154037.10905.qmail@web60323.mail.yahoo.com>

Yes, someone has used get.hist.quote. It's the same
data you get if you download data from yahoo finance,
as far as I can tell. If you use the adjusted close,
you can get split & dividend adjusted prices, eg:
,quote =c("Ad")
I haven't found problems with adjusted close pricing,
in my limited use of it to calculate long-term
Fama-French three-factor models, it seemed clean.
Check out the R-sig-finance archive and you'll see
discusions and uses of get.hist.quote.
There is no free source of intraday tick data that I
know of.



--- Weiwei Shi <helprhelp at gmail.com> wrote:

> Dear Listers:
> I am wondering if someone has ever used
> get.hist.quote to get data
> from www. How is the quality of the data, is there
> any record on
> splitting and etc?
> 
> I am also wondering if it is possible to get
> intra-day tick data somehow.
> 
> Any sharing of info will be appreciated.
> 
> Thanks,
> 
> --
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From jaromir.baxa at centrum.cz  Sun Oct 30 14:34:59 2005
From: jaromir.baxa at centrum.cz (Jaromir Baxa)
Date: Sun, 30 Oct 2005 14:34:59 +0100
Subject: [R-sig-finance] ARIMA Error
Message-ID: <200510301434.31950@centrum.cz>

Hi, 
my name is Jaromir Baxa and I have problem with ARIMA modelling using fSeries package. This is what I did (same thing as last week with another data) and what R responded me:
> res <- read.table("E:/BusinessCyclesTheory/SAPres.txt")
> res <- edit(res)
> library(fSeries)
Loading required package: fBasics

Rmetrics, (C) 1999-2005, Diethelm Wuertz, GPL
fBasics: Markets, Basic Statistics, Date and Time
Loading required package: fCalendar

Rmetrics, (C) 1999-2005, Diethelm Wuertz, GPL
fCalendar: Markets, Basic Statistics, Date and Time

Rmetrics, (C) 1999-2005, Diethelm Wuertz, GPL
fSeries: The Dynamical Process Behind Financial Markets

> armaFit(formula = res ~ arima(1, 0, 1))
Error in fit$x - fit$residuals : non-numeric argument to binary operator
In addition: Warning message:
Incompatible methods ("Ops.data.frame", "Ops.ts") for "-" 

"res" is vector of residuals (1 column, 119 observations) obtained from Henderson moving average.
Does anybody know what this error does mean and how can I avoid it (by transforming data??)?
Thanks for helping me. JB.


From edd at debian.org  Sun Oct 30 14:54:59 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 30 Oct 2005 07:54:59 -0600
Subject: [R-sig-finance] ARIMA Error
In-Reply-To: <200510301434.31950@centrum.cz>
References: <200510301434.31950@centrum.cz>
Message-ID: <17252.53427.854166.434747@basebud.nulle.part>


Jaromir,

On 30 October 2005 at 14:34, Jaromir Baxa wrote:
| Hi, 
| my name is Jaromir Baxa and I have problem with ARIMA modelling using fSeries package. This is what I did (same thing as last week with another data) and what R responded me:
| > res <- read.table("E:/BusinessCyclesTheory/SAPres.txt")
| > res <- edit(res)
| > library(fSeries)
[...]
| > armaFit(formula = res ~ arima(1, 0, 1))
| Error in fit$x - fit$residuals : non-numeric argument to binary operator
| In addition: Warning message:
| Incompatible methods ("Ops.data.frame", "Ops.ts") for "-" 
| 
| "res" is vector of residuals (1 column, 119 observations) obtained from Henderson moving average.
| Does anybody know what this error does mean and how can I avoid it (by
| transforming data??)?

I would try to narrow it down by using the ARIMA command directly --- this is
what fSeries calls anyway, and there may just be a bug in the wrapping which
you'd avoid. See help(arima).

Hope this helps, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'


From edd at debian.org  Mon Oct 31 04:12:03 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 30 Oct 2005 21:12:03 -0600
Subject: [R-sig-finance] New version of RQuantLib
Message-ID: <17253.35715.443094.418346@basebud.nulle.part>


A new version of RQuantLib has been uploaded to CRAN a few days ago (and
should be available at http://cran.r-project.org and all mirrors). Debian
binaries are also available.

Among the new features in version 0.2.0, all of which are due to excellent
contributions by Dominick Samperi, are
- new functions for Fixed Income support:
   * DiscountCurve for discount / forward / zero curve construction
   * BermudanSwaption provding several QuantLib pricers for Swaptions
- new internal R / C++ interface class RCpp
- Windows binary packages on CRAN 

Windows users can now install RQuantLib via the menu just like any other
package.  Debian users can still say 'apt-get install r-cran-rquantlib'. All
others can compile manually (which still requires Boost and QuantLib
headers).

We also have an experimental OpenGL demo of option analytics visualisation
via the rgl package.  However, there are some concerns over stability,
especially on platforms with ATI graphics card.  If you want to try it, do
 
> library(RQuantLib)
> source(url("http://basebud.nulle.part/~edd/code/rquantlib/OptionSurfaces.R"))

and let us know how it worked (or not) and what your platform is. We still
would like to release this in an upcoming revision.  A more fail-safe way,
using animated gifs of the demo's output, is available at

     http://dirk.eddelbuettel.com/code/rquantlib-rgl.html

Special thanks to Dominick Sapieri for the new code and all his testing and
building help, and to Uwe Ligges for help with the Windows binaries.

With best regards, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'


From edd at debian.org  Mon Oct 31 04:29:30 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 30 Oct 2005 21:29:30 -0600
Subject: [R-sig-finance] [Quantlib-users] New version of RQuantLib
In-Reply-To: <17253.35715.443094.418346@basebud.nulle.part>
References: <17253.35715.443094.418346@basebud.nulle.part>
Message-ID: <17253.36762.423575.817244@basebud.nulle.part>


On 30 October 2005 at 21:12, Dirk Eddelbuettel wrote:
| > library(RQuantLib)
| > source(url("http://basebud.nulle.part/~edd/code/rquantlib/OptionSurfaces.R"))

Ooops -- that's the URL from inside my home network, the real code/URL would be

> library(RQuantLib)
> source(url("http://dirk.eddelbuettel.com/code/rquantlib/OptionSurfaces.R"))

My sincere apologies for any inconveniences,  Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'


From martin.becker at mx.uni-saarland.de  Fri Nov  4 17:13:43 2005
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Fri, 04 Nov 2005 17:13:43 +0100
Subject: [R-sig-finance] bug report: garchOxFit() in fSeries (Ver. 220.10063)
Message-ID: <436B88B7.6040307@mx.uni-saarland.de>

Hi,

line number 55 of function garchOxFit() in fSeries (Version 220.10063, 
maybe in earlier versions, too) reads

   write(x, file = "OxSeries.csv", ncolumns = 1, append = TRUE)  ,

but I think it should be

   write(x = series, file = "OxSeries.csv", ncolumns = 1, append = TRUE)   .

I hope this mailing list is suitable for this bug report.

Thanks,

  Martin Becker


From patrick at burns-stat.com  Sat Nov  5 16:14:02 2005
From: patrick at burns-stat.com (Patrick Burns)
Date: Sat, 05 Nov 2005 15:14:02 +0000
Subject: [R-sig-finance] [R] Stochastic Volatility
In-Reply-To: <NGECIFANPOJAGABBAEAPGEHGFEAA.pcampbell@econ.bbk.ac.uk>
References: <NGECIFANPOJAGABBAEAPGEHGFEAA.pcampbell@econ.bbk.ac.uk>
Message-ID: <436CCC3A.2010609@burns-stat.com>

This seems much more appropriate for R-sig-finance than
for R-help.

I'm curious why you think garch models are less satisfactory
than stochastic volatility models.  I'm not aware of any literature
that shows one dominating the other, and not even very much
that compares the two.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Phineas Campbell wrote:

>Has anybody implemented or tried to implement a stochastic volatility model
>using the Kalman filter following a series of papers by Harvey, Ruiz and
>Shepard?
>
>This is a sophisticated approach for estimating an important class of
>models, so I am surprised that no implementation exists, is this because
>there are unforeseeable problems?
>
>In a related but off topic question, it has been a while since I looked at
>the non homoskedastic time series literature but back then you couldn't pick
>up a journal without reading another stochastic volatility paper, does
>anybody have any ideas why the literature has drifted back toward less
>satisfactory GARCH and EGARCH models?
>
>This question is somewhat moot as if I choose to pursue this I will
>implement a model myself.
>
>
>Phineas Campbell
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


From spencer.graves at pdf.com  Sat Nov  5 20:59:02 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 05 Nov 2005 11:59:02 -0800
Subject: [R-sig-finance] more fCalendar
In-Reply-To: <132FB39D-A207-4E0E-92AD-49117EADCB86@mac.com>
References: <132FB39D-A207-4E0E-92AD-49117EADCB86@mac.com>
Message-ID: <436D0F06.2050303@pdf.com>

	  If you have not received a reply to this and your other similar post, 
I suggest you send a private email to the Maintainer:    Diethelm Wuertz 
<wuertz at itp.phys.ethz.ch>.

	  I would think he would appreciate your contribution.

	  Good Luck,
	  spencer graves

Parlamis Franklin wrote:

> It's me again, with Japanese calendar minutiae I'm sure you've all  
> been dying to brush up on.
> 
> the fCalendar functions don't include the Japanese Vernal Equinox  
> holiday.  this is perhaps because there is no easy way to calculate  
> it.  at any rate, here's a function i wrote to fill the gap.
> 
> =====
> 
> JPVernalEquinox <- function(year) {
> 
>      ##  Origin and End Date data from http://aa.usno.navy.mil/data/ 
> docs/EarthSeasons.html
>      ##  The function Vernal.Equinox delivers correct values at the  
> endpoints of the above data.
>      ##  There may be minor variances (+/- a few minutes) in the  
> intermediate values,
>      ##  because the function linearly approximates a phenomenon that  
> is apparently
>      ##  nonlinear in recorded time.
> 
>      Equinox.Origin <- timeCalendar(1992, 3, 20, 8, 48, 0,  
> FinCenter="GMT")
>      Data.EndDate <- timeCalendar(2020,3,20,3,49,0,FinCenter="GMT")
>      Total.Seconds <- as.numeric(Data.EndDate-Equinox.Origin)*24*60*60
>      Mean.Annual.Seconds <- Total.Seconds/(atoms(Data.EndDate)$Y-atoms 
> (Equinox.Origin)$Y)
>      Vernal.Equinox <- function(year) Equinox.Origin+unclass((year- 
> atoms(Equinox.Origin)$Y)*Mean.Annual.Seconds)
>      JPVernal.Equinox <- function(year) timeDate(as.character 
> (Vernal.Equinox(year)), FinCenter="Tokyo")
> 
>      ## Nota bene:  JP Vernal Equinox is celebrated when the equinox  
> occurs in the Japanese time zone
>      ## (see, e.g., 2006, where GMT Vernal Equinox is on 20 March,  
> but Japanese Equinox holiday is 21 March)
> 
>      as.Date(as.character(JPVernal.Equinox(year)))}
> 
> =====
> 
> in contrast to the other holiday functions in fCalendar, the above  
> function returns an object of class "Date" rather than "sdate"  
> because use of sdates appears to be deprecated following the  
> introduction of the Date class, and also because the sdate function  
> appears to have a bug.
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915


From ezivot at u.washington.edu  Sat Nov  5 21:13:50 2005
From: ezivot at u.washington.edu (Eric Zivot)
Date: Sat, 5 Nov 2005 12:13:50 -0800 (PST)
Subject: [R-sig-finance] [R] Stochastic Volatility
In-Reply-To: <436CCC3A.2010609@burns-stat.com>
Message-ID: <Pine.LNX.4.43.0511051213500.29025@hymn09.u.washington.edu>

Estimating a SV model using the kalman filter a la Harvey, Ruiz and Shephard is straightforward and fast. In my book Modeling Financial Time Series with Splus, 2nd Edition, I give an example of doing this using the state space modeling tools (i.e. ssfpack) available in S+FinMetrics. The main problem with the kalman filter approach is that the resulting estimates are only best linear estimators. The state space representation of the SV model is non-Gaussian and the Kalman filter is only optimal for linear Gaussian models. Neil Shepard wrote a very nice survey article comparing SV and GARCH models about 5 years ago that was published in a Chapman & Hall book. I believe you can still download this paper from his webpage.

In writing the 2nd Edition of my book, I spent a lot of time with various forms of SV models (discrete time and continuous time). The main problem is that efficient estimation via maximum likelihood takes a lot of programming effort, and the techniques have to be tailored to specific models. Since volatility is latent, it has to be integrated out of the likelihood in discrete time models. In continuous-time models the transition density for the observables is only known in closed form for very simple models. The simulated MLE approach of Koopman and his co-authors is reasonably straightforward for simple models and provides much better estimates than the Kalman filter (in terms of MSE). I have an illustration of this approach in my paper (joint with Siem Jan Koopman and Jiahui Wang) "State Space Modeling in Macroeconomics and Finance Using SsfPack in S+FinMetrics", which can be downloaded 
from my webpage. However, for more complicated models and multivariate models the approach is not so straightforward.

The most promising approach for estimating SV models appears to be simulation-based. The Bayesian approach uses MCMC or particle filters, and can be very accurate. However, each model requires a different MCMC algorithm. I have had good success with Gallant and Tauchen's efficient method of moments (EMM) technique. The main advantage of this method is that one does not need to tailor the method for a specific problem: all it requires is that you write a simulator for your proposed SV model. Jiahui Wang and I, with help from George Tauchen, implemented EMM in S+FinMetrics 2.0 and the 2nd Edition of Modeling Financial Time Series describes in detail how to use EMM for estimation of general discrete time and continuous time SV models (as well as other models).

ez
****************************************************************
*  Eric Zivot                  			               *
*  Associate Professor         phone:  206-543-6715            *
*  Department of Economics     fax:    206-685-7477            *
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington                                    *
*  Seattle, WA 98195-3330                                      *
*                                                              *
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Sat, 5 Nov 2005, Patrick Burns wrote:

> This seems much more appropriate for R-sig-finance than
> for R-help.
>
> I'm curious why you think garch models are less satisfactory
> than stochastic volatility models.  I'm not aware of any literature
> that shows one dominating the other, and not even very much
> that compares the two.
>
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Phineas Campbell wrote:
>
>> Has anybody implemented or tried to implement a stochastic volatility model
>> using the Kalman filter following a series of papers by Harvey, Ruiz and
>> Shepard?
>>
>> This is a sophisticated approach for estimating an important class of
>> models, so I am surprised that no implementation exists, is this because
>> there are unforeseeable problems?
>>
>> In a related but off topic question, it has been a while since I looked at
>> the non homoskedastic time series literature but back then you couldn't pick
>> up a journal without reading another stochastic volatility paper, does
>> anybody have any ideas why the literature has drifted back toward less
>> satisfactory GARCH and EGARCH models?
>>
>> This question is somewhat moot as if I choose to pursue this I will
>> implement a model myself.
>>
>>
>> Phineas Campbell
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>
>>
>>
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From phleum at chello.se  Sun Nov  6 19:57:58 2005
From: phleum at chello.se (Carl)
Date: Sun, 6 Nov 2005 19:57:58 +0100
Subject: [R-sig-finance] Stochastic volatility
In-Reply-To: <mailman.13.1131274801.25819.r-sig-finance@stat.math.ethz.ch>
References: <mailman.13.1131274801.25819.r-sig-finance@stat.math.ethz.ch>
Message-ID: <200511061957.58547.phleum@chello.se>

The performance of discrete time (Garch) vs continous time SV models with 
stochastic diffusions and/or jumps in returns and/or variances have been 
compared and tested for option pricing purposes (search the net for 
references; P. Christoffersen and K. Jacobs, "Which Volatility Models for 
Option Valuation?", for example). 

The main result seems to be that the different models do not compete but 
complement each other. In some cases one can prove that a continuos model is 
the continuos limit of a Garch model (this is valid for the Heston vs the 
Heston-Nandi Garch model). One advantage of Garch models are that they 
provide the conditional volatility, which is unknown in the continuous SV 
models. 

A major obstacle in option pricing is the problem of calibrating the 
parameters to the current skew (ie the theoretical option prices should agree 
with the market prices for different strikes). Therefore, fast option pricing 
models are very important for both pricing and the calculation of greeks in a 
trading environment. The "speed" criterion favours closed-form, 
semiclosed-form (ie fourier transform, FFT) or approximate models which can 
price large number of options w.r.t different strikes. The most known models 
that fit this description are Heston's SV model and the Heston-Nandi Garch 
model. Today, incredibly many alternative models exist that add features and 
flavours to the basic structure of the above-mentioned models.

SV models with jumps seem to better adapted to options with shorter time to 
expiry because of steeper skews whereas longer dated options (with their 
flatter volatilities) are better treated with stochastic diffusion models 
(such as the Heston or Heston-Nandi Garch model).

I have not yet seen the application of the Kalman filter (and the particle 
filter) to option pricing. This sounds like a very interesting approach! If 
anyone knows about such work, please give me a link!

Carl

On Sunday 06 November 2005 12:00, r-sig-finance-request at stat.math.ethz.ch 
wrote:
> Send R-sig-finance mailing list submissions to
> 	r-sig-finance at stat.math.ethz.ch
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> or, via email, send a message with subject or body 'help' to
> 	r-sig-finance-request at stat.math.ethz.ch
>
> You can reach the person managing the list at
> 	r-sig-finance-owner at stat.math.ethz.ch
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-sig-finance digest..."
>
>
> Today's Topics:
>
>    1. Re: [R] Stochastic Volatility (Patrick Burns)
>    2. Re: more fCalendar (Spencer Graves)
>    3. Re: [R] Stochastic Volatility (Eric Zivot)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Sat, 05 Nov 2005 15:14:02 +0000
> From: Patrick Burns <patrick at burns-stat.com>
> Subject: Re: [R-sig-finance] [R] Stochastic Volatility
> To: Phineas Campbell <pcampbell at econ.bbk.ac.uk>
> Cc: "r-sig-finance at stat.math.ethz.ch"
> 	<r-sig-finance at stat.math.ethz.ch>
> Message-ID: <436CCC3A.2010609 at burns-stat.com>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> This seems much more appropriate for R-sig-finance than
> for R-help.
>
> I'm curious why you think garch models are less satisfactory
> than stochastic volatility models.  I'm not aware of any literature
> that shows one dominating the other, and not even very much
> that compares the two.
>
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Phineas Campbell wrote:
> >Has anybody implemented or tried to implement a stochastic volatility
> > model using the Kalman filter following a series of papers by Harvey,
> > Ruiz and Shepard?
> >
> >This is a sophisticated approach for estimating an important class of
> >models, so I am surprised that no implementation exists, is this because
> >there are unforeseeable problems?
> >
> >In a related but off topic question, it has been a while since I looked at
> >the non homoskedastic time series literature but back then you couldn't
> > pick up a journal without reading another stochastic volatility paper,
> > does anybody have any ideas why the literature has drifted back toward
> > less satisfactory GARCH and EGARCH models?
> >
> >This question is somewhat moot as if I choose to pursue this I will
> >implement a model myself.
> >
> >
> >Phineas Campbell
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ------------------------------
>
> Message: 2
> Date: Sat, 05 Nov 2005 11:59:02 -0800
> From: Spencer Graves <spencer.graves at pdf.com>
> Subject: Re: [R-sig-finance] more fCalendar
> To: Parlamis Franklin <fparlamis at mac.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Message-ID: <436D0F06.2050303 at pdf.com>
> Content-Type: text/plain; charset=us-ascii; format=flowed
>
> 	  If you have not received a reply to this and your other similar post,
> I suggest you send a private email to the Maintainer:    Diethelm Wuertz
> <wuertz at itp.phys.ethz.ch>.
>
> 	  I would think he would appreciate your contribution.
>
> 	  Good Luck,
> 	  spencer graves
>
> Parlamis Franklin wrote:
> > It's me again, with Japanese calendar minutiae I'm sure you've all
> > been dying to brush up on.
> >
> > the fCalendar functions don't include the Japanese Vernal Equinox
> > holiday.  this is perhaps because there is no easy way to calculate
> > it.  at any rate, here's a function i wrote to fill the gap.
> >
> > =====
> >
> > JPVernalEquinox <- function(year) {
> >
> >      ##  Origin and End Date data from http://aa.usno.navy.mil/data/
> > docs/EarthSeasons.html
> >      ##  The function Vernal.Equinox delivers correct values at the
> > endpoints of the above data.
> >      ##  There may be minor variances (+/- a few minutes) in the
> > intermediate values,
> >      ##  because the function linearly approximates a phenomenon that
> > is apparently
> >      ##  nonlinear in recorded time.
> >
> >      Equinox.Origin <- timeCalendar(1992, 3, 20, 8, 48, 0,
> > FinCenter="GMT")
> >      Data.EndDate <- timeCalendar(2020,3,20,3,49,0,FinCenter="GMT")
> >      Total.Seconds <- as.numeric(Data.EndDate-Equinox.Origin)*24*60*60
> >      Mean.Annual.Seconds <- Total.Seconds/(atoms(Data.EndDate)$Y-atoms
> > (Equinox.Origin)$Y)
> >      Vernal.Equinox <- function(year) Equinox.Origin+unclass((year-
> > atoms(Equinox.Origin)$Y)*Mean.Annual.Seconds)
> >      JPVernal.Equinox <- function(year) timeDate(as.character
> > (Vernal.Equinox(year)), FinCenter="Tokyo")
> >
> >      ## Nota bene:  JP Vernal Equinox is celebrated when the equinox
> > occurs in the Japanese time zone
> >      ## (see, e.g., 2006, where GMT Vernal Equinox is on 20 March,
> > but Japanese Equinox holiday is 21 March)
> >
> >      as.Date(as.character(JPVernal.Equinox(year)))}
> >
> > =====
> >
> > in contrast to the other holiday functions in fCalendar, the above
> > function returns an object of class "Date" rather than "sdate"
> > because use of sdates appears to be deprecated following the
> > introduction of the Date class, and also because the sdate function
> > appears to have a bug.
> > 	[[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From Gregor.gawron at rmf.ch  Mon Nov  7 09:15:33 2005
From: Gregor.gawron at rmf.ch (Gregor.gawron@rmf.ch)
Date: Mon, 7 Nov 2005 09:15:33 +0100
Subject: [R-sig-finance] Stochastic volatility
Message-ID: <66665938324C834DB5FC60A6F1C42694948A7D@michexmb01.maninvestments.ad.man.com>

Carl,
There is a presentation given by van der Merwe, de Freitas, Doucet and
Wan about the Unscented Particle Filter with an example to option
pricing - maybe it helps
http://cslu.cse.ogi.edu/publications/ps/UPF_CSLU_talk.pdf

Best,
Gregor

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Carl
Sent: Sonntag, 6. November 2005 19:58
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-sig-finance] Stochastic volatility


The performance of discrete time (Garch) vs continous time SV models
with 
stochastic diffusions and/or jumps in returns and/or variances have been

compared and tested for option pricing purposes (search the net for 
references; P. Christoffersen and K. Jacobs, "Which Volatility Models
for 
Option Valuation?", for example). 

The main result seems to be that the different models do not compete but

complement each other. In some cases one can prove that a continuos
model is 
the continuos limit of a Garch model (this is valid for the Heston vs
the 
Heston-Nandi Garch model). One advantage of Garch models are that they 
provide the conditional volatility, which is unknown in the continuous
SV 
models. 

A major obstacle in option pricing is the problem of calibrating the 
parameters to the current skew (ie the theoretical option prices should
agree 
with the market prices for different strikes). Therefore, fast option
pricing 
models are very important for both pricing and the calculation of greeks
in a 
trading environment. The "speed" criterion favours closed-form, 
semiclosed-form (ie fourier transform, FFT) or approximate models which
can 
price large number of options w.r.t different strikes. The most known
models 
that fit this description are Heston's SV model and the Heston-Nandi
Garch 
model. Today, incredibly many alternative models exist that add features
and 
flavours to the basic structure of the above-mentioned models.

SV models with jumps seem to better adapted to options with shorter time
to 
expiry because of steeper skews whereas longer dated options (with their

flatter volatilities) are better treated with stochastic diffusion
models 
(such as the Heston or Heston-Nandi Garch model).

I have not yet seen the application of the Kalman filter (and the
particle 
filter) to option pricing. This sounds like a very interesting approach!
If 
anyone knows about such work, please give me a link!

Carl

On Sunday 06 November 2005 12:00,
r-sig-finance-request at stat.math.ethz.ch 
wrote:
> Send R-sig-finance mailing list submissions to
> 	r-sig-finance at stat.math.ethz.ch
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> or, via email, send a message with subject or body 'help' to
> 	r-sig-finance-request at stat.math.ethz.ch
>
> You can reach the person managing the list at
> 	r-sig-finance-owner at stat.math.ethz.ch
>
> When replying, please edit your Subject line so it is more specific 
> than "Re: Contents of R-sig-finance digest..."
>
>
> Today's Topics:
>
>    1. Re: [R] Stochastic Volatility (Patrick Burns)
>    2. Re: more fCalendar (Spencer Graves)
>    3. Re: [R] Stochastic Volatility (Eric Zivot)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Sat, 05 Nov 2005 15:14:02 +0000
> From: Patrick Burns <patrick at burns-stat.com>
> Subject: Re: [R-sig-finance] [R] Stochastic Volatility
> To: Phineas Campbell <pcampbell at econ.bbk.ac.uk>
> Cc: "r-sig-finance at stat.math.ethz.ch"
> 	<r-sig-finance at stat.math.ethz.ch>
> Message-ID: <436CCC3A.2010609 at burns-stat.com>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> This seems much more appropriate for R-sig-finance than
> for R-help.
>
> I'm curious why you think garch models are less satisfactory than 
> stochastic volatility models.  I'm not aware of any literature that 
> shows one dominating the other, and not even very much that compares 
> the two.
>
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Phineas Campbell wrote:
> >Has anybody implemented or tried to implement a stochastic volatility

> >model using the Kalman filter following a series of papers by Harvey,

> >Ruiz and Shepard?
> >
> >This is a sophisticated approach for estimating an important class of

> >models, so I am surprised that no implementation exists, is this 
> >because there are unforeseeable problems?
> >
> >In a related but off topic question, it has been a while since I 
> >looked at the non homoskedastic time series literature but back then 
> >you couldn't  pick up a journal without reading another stochastic 
> >volatility paper,  does anybody have any ideas why the literature has

> >drifted back toward  less satisfactory GARCH and EGARCH models?
> >
> >This question is somewhat moot as if I choose to pursue this I will 
> >implement a model myself.
> >
> >
> >Phineas Campbell
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list 
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!  
> >http://www.R-project.org/posting-guide.html
>
> ------------------------------
>
> Message: 2
> Date: Sat, 05 Nov 2005 11:59:02 -0800
> From: Spencer Graves <spencer.graves at pdf.com>
> Subject: Re: [R-sig-finance] more fCalendar
> To: Parlamis Franklin <fparlamis at mac.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Message-ID: <436D0F06.2050303 at pdf.com>
> Content-Type: text/plain; charset=us-ascii; format=flowed
>
> 	  If you have not received a reply to this and your other
similar post,
> I suggest you send a private email to the Maintainer:    Diethelm
Wuertz
> <wuertz at itp.phys.ethz.ch>.
>
> 	  I would think he would appreciate your contribution.
>
> 	  Good Luck,
> 	  spencer graves
>
> Parlamis Franklin wrote:
> > It's me again, with Japanese calendar minutiae I'm sure you've all 
> > been dying to brush up on.
> >
> > the fCalendar functions don't include the Japanese Vernal Equinox 
> > holiday.  this is perhaps because there is no easy way to calculate 
> > it.  at any rate, here's a function i wrote to fill the gap.
> >
> > =====
> >
> > JPVernalEquinox <- function(year) {
> >
> >      ##  Origin and End Date data from http://aa.usno.navy.mil/data/

> > docs/EarthSeasons.html
> >      ##  The function Vernal.Equinox delivers correct values at the 
> > endpoints of the above data.
> >      ##  There may be minor variances (+/- a few minutes) in the 
> > intermediate values,
> >      ##  because the function linearly approximates a phenomenon 
> > that is apparently
> >      ##  nonlinear in recorded time.
> >
> >      Equinox.Origin <- timeCalendar(1992, 3, 20, 8, 48, 0,
> > FinCenter="GMT")
> >      Data.EndDate <- timeCalendar(2020,3,20,3,49,0,FinCenter="GMT")
> >      Total.Seconds <-
as.numeric(Data.EndDate-Equinox.Origin)*24*60*60
> >      Mean.Annual.Seconds <- 
> > Total.Seconds/(atoms(Data.EndDate)$Y-atoms
> > (Equinox.Origin)$Y)
> >      Vernal.Equinox <- function(year) Equinox.Origin+unclass((year-
> > atoms(Equinox.Origin)$Y)*Mean.Annual.Seconds)
> >      JPVernal.Equinox <- function(year) timeDate(as.character
> > (Vernal.Equinox(year)), FinCenter="Tokyo")
> >
> >      ## Nota bene:  JP Vernal Equinox is celebrated when the equinox

> > occurs in the Japanese time zone
> >      ## (see, e.g., 2006, where GMT Vernal Equinox is on 20 March, 
> > but Japanese Equinox holiday is 21 March)
> >
> >      as.Date(as.character(JPVernal.Equinox(year)))}
> >
> > =====
> >
> > in contrast to the other holiday functions in fCalendar, the above 
> > function returns an object of class "Date" rather than "sdate" 
> > because use of sdates appears to be deprecated following the 
> > introduction of the Date class, and also because the sdate function 
> > appears to have a bug.
> > 	[[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-finance at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance

_______________________________________________
R-sig-finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


Any information in this communication is confidential and ma...{{dropped}}


From jmbucci at stat.ucla.edu  Fri Nov 11 19:20:09 2005
From: jmbucci at stat.ucla.edu (jmbucci@stat.ucla.edu)
Date: Fri, 11 Nov 2005 10:20:09 -0800 (PST)
Subject: [R-sig-finance] foptions package
Message-ID: <50786.71.106.144.208.1131733209.squirrel@71.106.144.208>

Hello,

I had a quick question regarding the CRR Binomial program in R:

Does this program apply the backwards numerical procedure for valuing
American call options if I choose the "ca" as typeflag, i.e. at each node
of constructing the option tree backwards, the algorithm chooses between
the maximum of the discounted call values of the subsequent up and down
moves and the intrinsic value?

Thank you,

John Bucci


From edd at debian.org  Sat Nov 12 06:18:38 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 11 Nov 2005 23:18:38 -0600
Subject: [R-sig-finance] foptions package
In-Reply-To: <50786.71.106.144.208.1131733209.squirrel@71.106.144.208>
References: <50786.71.106.144.208.1131733209.squirrel@71.106.144.208>
Message-ID: <17269.31534.330688.343520@basebud.nulle.part>


John,

On 11 November 2005 at 10:20, jmbucci at stat.ucla.edu wrote:
| Hello,
| 
| I had a quick question regarding the CRR Binomial program in R:
| 
| Does this program apply the backwards numerical procedure for valuing
| American call options if I choose the "ca" as typeflag, i.e. at each node
| of constructing the option tree backwards, the algorithm chooses between
| the maximum of the discounted call values of the subsequent up and down
| moves and the intrinsic value?

Unless you hear from Diethelm, your best bet is probably do go digging in the
source code itself.  

Regards, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'


From ajayshah at mayin.org  Sat Nov 12 14:05:06 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sat, 12 Nov 2005 18:35:06 +0530
Subject: [R-sig-finance] Is oanda.com data trustworthy?
Message-ID: <20051112130506.GT11451@lubyanka.local>

The EUR/USD currency market is very liquid and the data should be very
sharp.

But I see big differences between data on the US Federal reserve
website and data on http://www.oanda.com (accessed using
tseries::get.hist.quote()).

Here are some examples:

               US Fed         oanda
31-Oct-05     0.833681       0.8293
1-Nov-05      0.833472       0.829 
2-Nov-05      0.828706       0.8325
3-Nov-05      0.835352       0.8286
4-Nov-05      0.845451       0.8374

When expressed as 100*log(p2/p1), the returns look like this:

           US Fed       oanda.com

2005-11-01 -0.02507268 -0.03618163
2005-11-02 -0.57346603  0.42130667
2005-11-03  0.79877448 -0.46956922
2005-11-04  1.20170199  1.05643239

These differences seem huge to me! E.g. on 3 November, the US Fed says
that returns were +0.798% and oanda.com says it's -0.469%.

Here's the exact get.hist.quote incantation:

> get.hist.quote("USD/EUR", provider="oanda", start="2005-10-31", end="2005-11-04")
trying URL 'http://www.oanda.com/convert/fxhistory?lang=en&date1=10%2F31%2F2005&date=11%2F04%2F2005&date_fmt=us&exch=USD&exch2=&expr=EUR&expr2=&margin_fixed=0&&SUBMIT=Get+Table&format=ASCII&redirected=1'
Content type 'text/html' length unknown
opened URL
.......... ...
downloaded 13Kb

2005-10-31 2005-11-01 2005-11-02 2005-11-03 2005-11-04 
    0.8293     0.8290     0.8325     0.8286     0.8374 

What should one do? :-(

-- 
Ajay Shah
ajayshah at mayin.org
http://www.mayin.org/ajayshah


From edd at debian.org  Sat Nov 12 15:16:52 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 12 Nov 2005 08:16:52 -0600
Subject: [R-sig-finance] Is oanda.com data trustworthy?
In-Reply-To: <20051112130506.GT11451@lubyanka.local>
References: <20051112130506.GT11451@lubyanka.local>
Message-ID: <17269.63828.308431.588080@basebud.nulle.part>


On 12 November 2005 at 18:35, Ajay Narottam Shah wrote:
| When expressed as 100*log(p2/p1), the returns look like this:
| 
|            US Fed       oanda.com
| 
| 2005-11-01 -0.02507268 -0.03618163
| 2005-11-02 -0.57346603  0.42130667
| 2005-11-03  0.79877448 -0.46956922
| 2005-11-04  1.20170199  1.05643239
| 
| These differences seem huge to me! E.g. on 3 November, the US Fed says
| that returns were +0.798% and oanda.com says it's -0.469%.

But as you know, FX markets operate 24hrs, and are broker-based implying that
there will be a) different quotes throughout the day and no "final" one and
b) different sources may well disagree about quotes at one point [ though
arguably not by the margin you show there ]. Oanda may report London fixings
whereas the US Fed probably calls up US banks and end-of-day US time.

| What should one do? :-(

Not sure -- I am not that initimately familiar with FX markets. Go with one
convention and stick with it. Try to find an 'official' one.  When I was
still working in Toronto, the official end-of-day quotes were those from the
Bank of Canada.

As for the question above, maybe time to test some Granger causality between
Oanda and the FED sources. :-)

Hoep this helps, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'


From kriskumar at earthlink.net  Sat Nov 12 15:45:54 2005
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Sat, 12 Nov 2005 09:45:54 -0500
Subject: [R-sig-finance] foptions package
In-Reply-To: <50786.71.106.144.208.1131733209.squirrel@71.106.144.208>
References: <50786.71.106.144.208.1131733209.squirrel@71.106.144.208>
Message-ID: <43760022.1020505@earthlink.net>

I am attaching a little function I wrote which does the LR/CRR/JR trees 
it also does the Binomial with Blackscholes
2-point richardson extrapolation with a BS adjustment at the penultimate 
step (Broadie-DeTempleton) particularly good for getting smooth greeks.

Hope this helps,

Best,
Kris

ps: It is ugly and not well documented is there a R standard for naming 
variables etc?



jmbucci at stat.ucla.edu wrote:

>Hello,
>
>I had a quick question regarding the CRR Binomial program in R:
>
>Does this program apply the backwards numerical procedure for valuing
>American call options if I choose the "ca" as typeflag, i.e. at each node
>of constructing the option tree backwards, the algorithm chooses between
>the maximum of the discounted call values of the subsequent up and down
>moves and the intrinsic value?
>
>Thank you,
>
>John Bucci
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: bintree.r
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20051112/9fc34c40/bintree.pl

From ajayshah at mayin.org  Sat Nov 12 15:51:31 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sat, 12 Nov 2005 20:21:31 +0530
Subject: [R-sig-finance] Is oanda.com data trustworthy?
In-Reply-To: <17269.63828.308431.588080@basebud.nulle.part>
References: <20051112130506.GT11451@lubyanka.local>
	<17269.63828.308431.588080@basebud.nulle.part>
Message-ID: <20051112145131.GV11451@lubyanka.local>

> But as you know, FX markets operate 24hrs, and are broker-based implying that
> there will be a) different quotes throughout the day and no "final" one and
> b) different sources may well disagree about quotes at one point [ though
> arguably not by the margin you show there ]. Oanda may report London fixings
> whereas the US Fed probably calls up US banks and end-of-day US time.

I was aware of the 24hour issue. I thought it may lead to some clear
leads and lags. Read on.

> As for the question above, maybe time to test some Granger causality between
> Oanda and the FED sources. :-)

> usfed <- mydb("currency", "EUR/USD", firstdate="2002-01-01",
                lastdate="2004-12-31")
> oanda <- window(read.zoo("EUR", skip=1), start=as.Date("2002-01-01"),
                end=as.Date("2004-12-31"))
> prices <- cbind(usfed, oanda, all=FALSE)
> plot.zoo(prices, plot.type="single", col=c("blue", "yellow"), lwd=2)

# Visually looks okay.

> returns <- prices2returns(prices)
> 
> summary(returns)
     Index                usfed              oanda         
 Min.   :2002-01-03   Min.   :-1.82563   Min.   :-1.80702  
 1st Qu.:2002-09-26   1st Qu.:-0.46976   1st Qu.:-0.42765  
 Median :2003-07-01   Median :-0.04195   Median :-0.05355  
 Mean   :2003-07-02   Mean   :-0.03983   Mean   :-0.05427  
 3rd Qu.:2004-04-06   3rd Qu.: 0.31539   3rd Qu.: 0.30200  
 Max.   :2004-12-31   Max.   : 2.11408   Max.   : 1.98718  

# Looks plausible.

> returns <- as.data.frame(returns)
> 
> plot(returns$oanda, returns$usfed)

It looks like a zero correlation!!!!!!!!!!!!

> summary(lm(usfed ~ oanda, returns))

Call:
lm(formula = usfed ~ oanda, data = returns)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.78081 -0.42691  0.01766  0.36543  2.10784 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.03637    0.02563  -1.419    0.156
oanda        0.06373    0.04205   1.516    0.130

Residual standard error: 0.6191 on 586 degrees of freedom
Multiple R-Squared: 0.003904,	Adjusted R-squared: 0.002204 
F-statistic: 2.297 on 1 and 586 DF,  p-value: 0.1302 

> cor(returns)
           usfed      oanda
usfed 1.00000000 0.06248328
oanda 0.06248328 1.00000000

This is truly scary - a returns correlation of 0.06 between two
measures of returns!

> library(lmtest)
> grangertest(returns$usfed ~ returns$oanda, order=5)
Granger causality test

Model 1: returns$usfed ~ Lags(returns$usfed, 1:5) + Lags(returns$oanda, 1:5)
Model 2: returns$usfed ~ Lags(returns$usfed, 1:5)
  Res.Df  Df      F Pr(>F)
1    572                  
2    577   5 1.2864 0.2681
> grangertest(returns$oanda ~ returns$usfed, order=5)
Granger causality test

Model 1: returns$oanda ~ Lags(returns$oanda, 1:5) + Lags(returns$usfed, 1:5)
Model 2: returns$oanda ~ Lags(returns$oanda, 1:5)
  Res.Df  Df      F    Pr(>F)    
1    572                         
2    577   5 59.686 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

So there IS something going on! Could be a time of day story?

Here are a few days of returns, to help fix the intuition:

> tail(returns, 10)
                 usfed       oanda
2004-12-16  1.06300992 -0.76106917
2004-12-17 -0.06020826  1.18581098
2004-12-21  0.14203074 -0.42746525
2004-12-22 -0.09716418  0.12040941
2004-12-23 -0.75145123 -0.12040941
2004-12-24 -0.33297193 -0.91447652
2004-12-28  0.02928946 -0.58364604
2004-12-29  0.38988948  0.05443658
2004-12-30 -0.40446502  0.04080800
2004-12-31  0.62592084 -0.29964610

So there does seem to be a pattern where information for 2004-12-16
with the US Fed (1.06%) seems to show up on the next day for oanda
(1.18%).

If I lag the US Fed data by 1, the correlation goes up to 0.57. I
think this is a useful discovery :-) but .57 still sux!

-- 
Ajay Shah
ajayshah at mayin.org
http://www.mayin.org/ajayshah


From kriskumar at earthlink.net  Sat Nov 12 16:04:35 2005
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Sat, 12 Nov 2005 10:04:35 -0500
Subject: [R-sig-finance] foptions package
In-Reply-To: <43760022.1020505@earthlink.net>
References: <50786.71.106.144.208.1131733209.squirrel@71.106.144.208>
	<43760022.1020505@earthlink.net>
Message-ID: <43760483.9010601@earthlink.net>

I forgot to mention a few things
Function description
(i) optionvalue.euro  - this does CRR/LR/JR trees
(ii) greek.binomial  - this is based on Pelseer/Vorst  ("The Binomial 
Model and the Greeks." /Journal of Derivatives/, Spring 1994, 45-49 )
(iii) optionvalue.amer  - american option pricing using CRR/LR/JR
(iv) optionvalue.bbs  - Broadie-Detempleton adjustment to the tree (2 pt 
richardson extrapolation + BS at penultimate)
*See American Option Valuation: New Bounds, Approximations, and a 
Comparison of Existing Methods* 
(http://rfs.oupjournals.org/cgi/reprint/9/4/1211)

My favorite among these is the LR tree the original paper is here 
(http://www.wiwi.uni-bonn.de/sfb/papers/1995/b/bonnsfb309.pdf)



Krishna Kumar wrote:

> I am attaching a little function I wrote which does the LR/CRR/JR 
> trees it also does the Binomial with Blackscholes
> 2-point richardson extrapolation with a BS adjustment at the 
> penultimate step (Broadie-DeTempleton) particularly good for getting 
> smooth greeks.
>
> Hope this helps,
>
> Best,
> Kris
>
> ps: It is ugly and not well documented is there a R standard for 
> naming variables etc?


From kriskumar at earthlink.net  Sat Nov 12 16:15:59 2005
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Sat, 12 Nov 2005 10:15:59 -0500
Subject: [R-sig-finance] Is oanda.com data trustworthy?
In-Reply-To: <20051112145131.GV11451@lubyanka.local>
References: <20051112130506.GT11451@lubyanka.local>	<17269.63828.308431.588080@basebud.nulle.part>
	<20051112145131.GV11451@lubyanka.local>
Message-ID: <4376072F.1020606@earthlink.net>

Does Oanda provide an indication of how the closing price in a market 
that never closes is computed ?
I think it would be best to use quotes/trades at the most liquid time of 
the day (for EUR/USD around a.m. EST) if available.




Ajay Narottam Shah wrote:

>>But as you know, FX markets operate 24hrs, and are broker-based implying that
>>there will be a) different quotes throughout the day and no "final" one and
>>b) different sources may well disagree about quotes at one point [ though
>>arguably not by the margin you show there ]. Oanda may report London fixings
>>whereas the US Fed probably calls up US banks and end-of-day US time.
>>    
>>
>
>I was aware of the 24hour issue. I thought it may lead to some clear
>leads and lags. Read on.
>
>  
>
>>As for the question above, maybe time to test some Granger causality between
>>Oanda and the FED sources. :-)
>>    
>>


From pcampbell at econ.bbk.ac.uk  Sat Nov 12 17:19:18 2005
From: pcampbell at econ.bbk.ac.uk (Phineas Campbell)
Date: Sat, 12 Nov 2005 16:19:18 -0000
Subject: [R-sig-finance] Is oanda.com data trustworthy?
In-Reply-To: <4376072F.1020606@earthlink.net>
Message-ID: <NGECIFANPOJAGABBAEAPKENBFEAA.pcampbell@econ.bbk.ac.uk>

There must be an official fixing for the options market to price from.  Also
at any given time there will buy and sell prices.

Given that 31% of Gloabl FX trading occurs in London I would use Bank of
England data:

http://213.225.136.206/mfsd/iadb/FromShowColumns.asp?Travel=NIxAZxI1x&FromCa
tegoryList=Yes&NewMeaningId=REURX&CategId=6&HighlightCatValueDisplay=Exchang
e%20rate%20(spot)%20-%20Euro%20into%20US%20dollar

which is nice because it comes with notes:

http://213.225.136.206/mfsd/iadb/notesiadb/Spot_rates.htm


HTH Phineas


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of Krishna
Kumar
Sent: Saturday, November 12, 2005 3:16 PM
To: Ajay Narottam Shah
Cc: R-sig-finance
Subject: Re: [R-sig-finance] Is oanda.com data trustworthy?


Does Oanda provide an indication of how the closing price in a market
that never closes is computed ?
I think it would be best to use quotes/trades at the most liquid time of
the day (for EUR/USD around a.m. EST) if available.




Ajay Narottam Shah wrote:

>>But as you know, FX markets operate 24hrs, and are broker-based implying
that
>>there will be a) different quotes throughout the day and no "final" one
and
>>b) different sources may well disagree about quotes at one point [ though
>>arguably not by the margin you show there ]. Oanda may report London
fixings
>>whereas the US Fed probably calls up US banks and end-of-day US time.
>>
>>
>
>I was aware of the 24hour issue. I thought it may lead to some clear
>leads and lags. Read on.
>
>
>
>>As for the question above, maybe time to test some Granger causality
between
>>Oanda and the FED sources. :-)
>>
>>

_______________________________________________
R-sig-finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From kriskumar at earthlink.net  Sun Nov 13 00:08:37 2005
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Sat, 12 Nov 2005 18:08:37 -0500
Subject: [R-sig-finance] Stochastic volatility
In-Reply-To: <200511061957.58547.phleum@chello.se>
References: <mailman.13.1131274801.25819.r-sig-finance@stat.math.ethz.ch>
	<200511061957.58547.phleum@chello.se>
Message-ID: <437675F5.7000201@earthlink.net>


>I have not yet seen the application of the Kalman filter (and the particle 
>filter) to option pricing. This sounds like a very interesting approach! If 
>anyone knows about such work, please give me a link!
>
>  
>
Another paper that might be of interest is this by Alireza (reza) et.al
http://www.wilmott.com/pdfs/050511_javaheri.pdf
the kalman filter is a prefered approach for computing time-varying 
betas in some banks :-)


From ajayshah at mayin.org  Sun Nov 13 00:21:18 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sun, 13 Nov 2005 04:51:18 +0530
Subject: [R-sig-finance] Is oanda.com data trustworthy?
In-Reply-To: <NGECIFANPOJAGABBAEAPKENBFEAA.pcampbell@econ.bbk.ac.uk>
References: <4376072F.1020606@earthlink.net>
	<NGECIFANPOJAGABBAEAPKENBFEAA.pcampbell@econ.bbk.ac.uk>
Message-ID: <20051112232118.GI11451@lubyanka.local>

On Sat, Nov 12, 2005 at 04:19:18PM -0000, Phineas Campbell wrote:
> There must be an official fixing for the options market to price from.  Also
> at any given time there will buy and sell prices.
> 
> Given that 31% of Gloabl FX trading occurs in London I would use Bank of
> England data:
> 
> http://213.225.136.206/mfsd/iadb/FromShowColumns.asp?Travel=NIxAZxI1x&FromCategoryList=Yes&NewMeaningId=REURX&CategId=6&HighlightCatValueDisplay=Exchange%20rate%20(spot)%20-%20Euro%20into%20US%20dollar

Thanks. I will investigate. At first blush, the US Federal Reserve
dissemination of data seems a lot nicer because they have well defined
files for each exchange rate of interest. So one can just setup
automation using wget or curl to a fixed URL.

The Bank of England setup involves interaction, which generally comes
at the cost of automation. Or are the URLs such as

http://213.225.136.206/mfsd/iadb/fromshowcolumns.asp?Travel=NIxSTxTIxSUx&FromSeries=1&ToSeries=50&DAT=RNG&FD=1&FM=Jan&FY=1963&TD=12&TM=Nov&TY=2005&csv.x=19&csv.y=22&CSVF=TT&FN=N&C=EC3

stable and trustworthy? I wasn't able to figure out a mapping from
standard currency codes to the notation they are using, such as "EC3"
for the Australian dollar.

Has someone deciphered the above URL, so that we can write something
like a Yahoo::Quote for it?

-- 
Ajay Shah
ajayshah at mayin.org
http://www.mayin.org/ajayshah


From vincent at 7d4.com  Sun Nov 13 19:00:24 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Sun, 13 Nov 2005 19:00:24 +0100
Subject: [R-sig-finance] Is oanda.com data trustworthy?
In-Reply-To: <20051112130506.GT11451@lubyanka.local>
References: <20051112130506.GT11451@lubyanka.local>
Message-ID: <43777F38.2020001@7d4.com>

Ajay Narottam Shah a ?crit :

> The EUR/USD currency market is very liquid and the data should be very
> sharp.
> But I see big differences between data on the US Federal reserve
> website ...

Could you please indicate the adress of this website ?
Thanks
Vincent


From ajayshah at mayin.org  Sun Nov 13 19:14:11 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sun, 13 Nov 2005 23:44:11 +0530
Subject: [R-sig-finance] Is oanda.com data trustworthy?
In-Reply-To: <43777F38.2020001@7d4.com>
References: <20051112130506.GT11451@lubyanka.local> <43777F38.2020001@7d4.com>
Message-ID: <20051113181411.GS11451@lubyanka.local>

On Sun, Nov 13, 2005 at 07:00:24PM +0100, vincent at 7d4.com wrote:
> Ajay Narottam Shah a ?crit :
> 
> > The EUR/USD currency market is very liquid and the data should be very
> > sharp.
> > But I see big differences between data on the US Federal reserve
> > website ...
> 
> Could you please indicate the adress of this website ?

http://www.oanda.com/cgi/crossrate/crossrate.shtml
for codes.

http://www.federalreserve.gov/releases/H10/hist/
for data.

http://in.finance.yahoo.com/m3
for typical cross-rates.

-- 
Ajay Shah
ajayshah at mayin.org
http://www.mayin.org/ajayshah


From vincent at 7d4.com  Sun Nov 13 19:22:55 2005
From: vincent at 7d4.com (vincent@7d4.com)
Date: Sun, 13 Nov 2005 19:22:55 +0100
Subject: [R-sig-finance] Is oanda.com data trustworthy?
In-Reply-To: <20051113181411.GS11451@lubyanka.local>
References: <20051112130506.GT11451@lubyanka.local> <43777F38.2020001@7d4.com>
	<20051113181411.GS11451@lubyanka.local>
Message-ID: <4377847F.7010308@7d4.com>

Ajay Narottam Shah a ?crit :

> http://www.oanda.com/cgi/crossrate/crossrate.shtml
> for codes.
> 
> http://www.federalreserve.gov/releases/H10/hist/
> for data.
> 
> http://in.finance.yahoo.com/m3
> for typical cross-rates.
> 

Thank you.


From manojsw at gmail.com  Mon Nov 14 04:58:26 2005
From: manojsw at gmail.com (Manoj)
Date: Mon, 14 Nov 2005 12:58:26 +0900
Subject: [R-sig-finance] Stochastic volatility
In-Reply-To: <437675F5.7000201@earthlink.net>
References: <mailman.13.1131274801.25819.r-sig-finance@stat.math.ethz.ch>
	<200511061957.58547.phleum@chello.se> <437675F5.7000201@earthlink.net>
Message-ID: <829e6c8a0511131958k358ae9bej7f9567aa39983f17@mail.gmail.com>

has anyone tried to implement any of the filters mentioned in this
paper using R?


On 11/13/05, Krishna Kumar <kriskumar at earthlink.net> wrote:
>
> >I have not yet seen the application of the Kalman filter (and the particle
> >filter) to option pricing. This sounds like a very interesting approach! If
> >anyone knows about such work, please give me a link!
> >
> >
> >
> Another paper that might be of interest is this by Alireza (reza) et.al
> http://www.wilmott.com/pdfs/050511_javaheri.pdf
> the kalman filter is a prefered approach for computing time-varying
> betas in some banks :-)
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From a.trapletti at swissonline.ch  Mon Nov 14 12:24:15 2005
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Mon, 14 Nov 2005 12:24:15 +0100
Subject: [R-sig-finance] Is oanda.com data trustworthy?
Message-ID: <437873DF.6020908@swissonline.ch>

>
>
>Message: 1
>Date: Sat, 12 Nov 2005 18:35:06 +0530
>From: Ajay Narottam Shah <ajayshah at mayin.org>
>Subject: [R-sig-finance] Is oanda.com data trustworthy?
>To: R-sig-finance <R-sig-finance at stat.math.ethz.ch>
>Message-ID: <20051112130506.GT11451 at lubyanka.local>
>Content-Type: text/plain; charset=us-ascii
>
>The EUR/USD currency market is very liquid and the data should be very
>sharp.
>
>But I see big differences between data on the US Federal reserve
>website and data on http://www.oanda.com (accessed using
>tseries::get.hist.quote()).
>
>Here are some examples:
>
>               US Fed         oanda
>31-Oct-05     0.833681       0.8293
>1-Nov-05      0.833472       0.829 
>2-Nov-05      0.828706       0.8325
>3-Nov-05      0.835352       0.8286
>4-Nov-05      0.845451       0.8374
>
>When expressed as 100*log(p2/p1), the returns look like this:
>
>           US Fed       oanda.com
>
>2005-11-01 -0.02507268 -0.03618163
>2005-11-02 -0.57346603  0.42130667
>2005-11-03  0.79877448 -0.46956922
>2005-11-04  1.20170199  1.05643239
>
>These differences seem huge to me! E.g. on 3 November, the US Fed says
>that returns were +0.798% and oanda.com says it's -0.469%.
>
>Here's the exact get.hist.quote incantation:
>
>  
>
>>> get.hist.quote("USD/EUR", provider="oanda", start="2005-10-31", end="2005-11-04")
>>    
>>
>trying URL 'http://www.oanda.com/convert/fxhistory?lang=en&date1=10%2F31%2F2005&date=11%2F04%2F2005&date_fmt=us&exch=USD&exch2=&expr=EUR&expr2=&margin_fixed=0&&SUBMIT=Get+Table&format=ASCII&redirected=1'
>Content type 'text/html' length unknown
>opened URL
>.......... ...
>downloaded 13Kb
>
>2005-10-31 2005-11-01 2005-11-02 2005-11-03 2005-11-04 
>    0.8293     0.8290     0.8325     0.8286     0.8374 
>
>What should one do? 
>
> -- Ajay Shah ajayshah at mayin.org http://www.mayin.org/ajayshah
>
The prices are very different if you do not sample synchronously or if you take average prices as Oanda does (Go to http://www.oanda.com/convert/fxhistory get a rate and read the text at the bottom). For example the daily low of EUR/USD on 11/04 was around 1.1802 and the daily high around 1.1996 (194 basis points difference!).Hence both numbers - Oanda and Fed - you report above are "correct". Further if you look at correlations of increments between different sources and use the fact that there is almost no autocorrelation on each series it is not a surprise that there is low correlation between the returns of the different sources.

Best regards
Adrian


From davidr at rhotrading.com  Mon Nov 14 16:42:22 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Mon, 14 Nov 2005 09:42:22 -0600
Subject: [R-sig-finance] Is oanda.com data trustworthy?
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A7CEB27@rhosvr02.rhotrading.com>

Yes, OandA data are trustworthy. They are very careful in capturing all
the data they can (that's a lot!) and processing it very carefully. But
you have to read carefully what they are telling you about the numbers
you are seeing! :-) (Read An Introduction to High-Frequency Finance by
Michel Dacorogna, et al., to get an idea.)

The big things to look for in treating OTC markets are timing, source,
and price type. OandA, the US Fed, and the Bank of England (or any other
source you have) give data for different times, different sources, and
different "averages". Many researchers try to focus on a single-point
source (say Reuters or Bloomberg feed) so the data are timestamped
consistently, pick a particular time (say London 10 am or NY 2 pm), and
quite often use just bid prices (or both bid and ask.) Any averaging
across time or prices (like mid) changes the statistics, but if you know
what you are dealing with, you can probably take it into account, at
least partially, and it may make for smoother data series.

and just to rag on it again, EUR/USD is the number of US dollars per
Euro, the usual way of quoting this pair, currently around 1.1700 .


David L. Reiner
Rho Trading
 

> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-
> bounces at stat.math.ethz.ch] On Behalf Of Ajay Narottam Shah
> Sent: Saturday, November 12, 2005 7:05 AM
> To: R-sig-finance
> Subject: [R-sig-finance] Is oanda.com data trustworthy?
> 
> The EUR/USD currency market is very liquid and the data should be very
> sharp.
> 
> But I see big differences between data on the US Federal reserve
> website and data on http://www.oanda.com (accessed using
> tseries::get.hist.quote()).
> 
> Here are some examples:
> 
>                US Fed         oanda
> 31-Oct-05     0.833681       0.8293
> 1-Nov-05      0.833472       0.829
> 2-Nov-05      0.828706       0.8325
> 3-Nov-05      0.835352       0.8286
> 4-Nov-05      0.845451       0.8374
> 
> When expressed as 100*log(p2/p1), the returns look like this:
> 
>            US Fed       oanda.com
> 
> 2005-11-01 -0.02507268 -0.03618163
> 2005-11-02 -0.57346603  0.42130667
> 2005-11-03  0.79877448 -0.46956922
> 2005-11-04  1.20170199  1.05643239
> 
> These differences seem huge to me! E.g. on 3 November, the US Fed says
> that returns were +0.798% and oanda.com says it's -0.469%.
> 
> Here's the exact get.hist.quote incantation:
> 
> > get.hist.quote("USD/EUR", provider="oanda", start="2005-10-31",
> end="2005-11-04")
> trying URL
>
'http://www.oanda.com/convert/fxhistory?lang=en&date1=10%2F31%2F2005&dat
e=
>
11%2F04%2F2005&date_fmt=us&exch=USD&exch2=&expr=EUR&expr2=&margin_fixed=
0&
> &SUBMIT=Get+Table&format=ASCII&redirected=1'
> Content type 'text/html' length unknown
> opened URL
> .......... ...
> downloaded 13Kb
> 
> 2005-10-31 2005-11-01 2005-11-02 2005-11-03 2005-11-04
>     0.8293     0.8290     0.8325     0.8286     0.8374
> 
> What should one do? :-(
> 
> --
> Ajay Shah
> ajayshah at mayin.org
> http://www.mayin.org/ajayshah
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From jmbucci at stat.ucla.edu  Mon Nov 14 23:09:28 2005
From: jmbucci at stat.ucla.edu (jmbucci@stat.ucla.edu)
Date: Mon, 14 Nov 2005 14:09:28 -0800 (PST)
Subject: [R-sig-finance] R source code
Message-ID: <1852.128.97.244.15.1132006168.squirrel@128.97.244.15>

Hello,

Is there a way to view source code directly in R for Finance added packages?

Thank you.


From L.Isella at myrealbox.com  Wed Nov 16 10:17:44 2005
From: L.Isella at myrealbox.com (L.Isella)
Date: Wed, 16 Nov 2005 09:17:44 +0000
Subject: [R-sig-finance] Sorting
Message-ID: <1132132664.c7d26e9cL.Isella@myrealbox.com>

Dear All,
I have a long array (say 2xN, with N of the order of 10^5 at least) made up of couples of numerical values.
I would like to sort these N couples in increasing order of the value of the numbers in the lower row.
It is very easy to use sort() to take care of the sorting of the lower row, but then I also have to sort the upper row so that the values of each couple still match.
I did it using a double loop, but for large N this is very slow.
This is really a bottleneck in my code...
Any suggestions?
Regards

Lorenzo


From sean.oriordain at gmail.com  Wed Nov 16 10:32:28 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Wed, 16 Nov 2005 09:32:28 +0000
Subject: [R-sig-finance] Sorting
In-Reply-To: <1132132664.c7d26e9cL.Isella@myrealbox.com>
References: <1132132664.c7d26e9cL.Isella@myrealbox.com>
Message-ID: <8ed68eed0511160132n5818ea93v@mail.gmail.com>

Good Morning Lorenzo,

I had a look at ?sort and it said "For ordering along more than one
variable, e.g., for sorting data frames, see 'order'." So I had a look
at ?order... and popped down to the examples and it talks about
ordering a dataframe - not the clearest of examples, but hey :-) 
Sometimes the obvious is the hardest to see :-)

cheers!
Sean


On 16/11/05, L.Isella <L.Isella at myrealbox.com> wrote:
> Dear All,
> I have a long array (say 2xN, with N of the order of 10^5 at least) made up of couples of numerical values.
> I would like to sort these N couples in increasing order of the value of the numbers in the lower row.
> It is very easy to use sort() to take care of the sorting of the lower row, but then I also have to sort the upper row so that the values of each couple still match.
> I did it using a double loop, but for large N this is very slow.
> This is really a bottleneck in my code...
> Any suggestions?
> Regards
>
> Lorenzo
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From L.Isella at myrealbox.com  Wed Nov 16 16:20:21 2005
From: L.Isella at myrealbox.com (L.Isella)
Date: Wed, 16 Nov 2005 15:20:21 +0000
Subject: [R-sig-finance] Random Numbers
Message-ID: <1132154421.c7ed257cL.Isella@myrealbox.com>

Dear All,
I would like to be able to generate long sequences of random numbers which I need to work out the resampled efficient frontier of a portfolio.
People normally take random draws from some multivariate (say Gaussian, to keep it simple) distribution to simulate the returns of the various assets and generate many random portfolios.
I accomplish that working with rnorm().
However, this way, in the limit of very long sequences, the simulated asset returns have the chosen mean and std, but each sequence is totally uncorrelated from the other.
Is there (in R) a way to produce sequences of Gaussian distributed random numbers with a chosen correlation? Regardless whether this is the proper way of calculating the resampled efficient frontier, I wonder it this has been implemented somewhere for R.
Regards

Lorenzo


From David.King at schroders.com  Wed Nov 16 16:28:38 2005
From: David.King at schroders.com (King, David)
Date: Wed, 16 Nov 2005 15:28:38 -0000
Subject: [R-sig-finance] Random Numbers
Message-ID: <77FB5D6344964D4B9D2FCE45D50B561B8DBED8@LON0820.london.schroders.com>


Lorenzo,

You could try utilising rmvnorm in package mvtnorm.

David


-----Original Message-----
From: L.Isella [mailto:L.Isella at myrealbox.com] 
Sent: 16 November 2005 15:21
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-sig-finance] Random Numbers


* Please Note : This message was received from the Internet *
_____________________________________________________________
Dear All,
I would like to be able to generate long sequences of random numbers
which I need to work out the resampled efficient frontier of a
portfolio. People normally take random draws from some multivariate (say
Gaussian, to keep it simple) distribution to simulate the returns of the
various assets and generate many random portfolios. I accomplish that
working with rnorm(). However, this way, in the limit of very long
sequences, the simulated asset returns have the chosen mean and std, but
each sequence is totally uncorrelated from the other. Is there (in R) a
way to produce sequences of Gaussian distributed random numbers with a
chosen correlation? Regardless whether this is the proper way of
calculating the resampled efficient frontier, I wonder it this has been
implemented somewhere for R. Regards

Lorenzo

_______________________________________________
R-sig-finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


__________________________________________________________________


This message might contain confidential information. If it has been sent to you in error please do not forward it or copy it or act upon its contents, but report it to postmaster at schroders.com

Schroders has the right lawfully to record, monitor and inspect messages between its employees and any third party. Your messages shall be subject to such lawful supervision as Schroders deems to be necessary in order to protect its information, its interests and its reputation.

Schroders prohibits and takes steps to prevent its information systems from being used to view, store or forward offensive or discriminatory material. If this message contains such material please report it to abuse at schroders.com

Schroders does not normally accept or offer business instructions via email unless prior agreements are in place. Any action that you might take upon this message might be at your own risk.



Schroder Investment Management Limited
31 Gresham Street
London EC2V 7QA

Authorised and regulated by the Financial Services Authority. Schroder Investment Management Limited is entered on the FSA register under the following register number: 119348

Registered Office
31 Gresham Street
London EC2V 7QA

Registered number 1893220
VAT registration number 243 8687 30


From patrick at burns-stat.com  Wed Nov 16 16:33:04 2005
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed, 16 Nov 2005 15:33:04 +0000
Subject: [R-sig-finance] Random Numbers
In-Reply-To: <1132154421.c7ed257cL.Isella@myrealbox.com>
References: <1132154421.c7ed257cL.Isella@myrealbox.com>
Message-ID: <437B5130.30207@burns-stat.com>

It's not precisely clear to me what you wish to accomplish.
It is possible that random portfolios may be able to answer
whatever question you have.  See the Burns Statistics website.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com


L.Isella wrote:

>Dear All,
>I would like to be able to generate long sequences of random numbers which I need to work out the resampled efficient frontier of a portfolio.
>People normally take random draws from some multivariate (say Gaussian, to keep it simple) distribution to simulate the returns of the various assets and generate many random portfolios.
>I accomplish that working with rnorm().
>However, this way, in the limit of very long sequences, the simulated asset returns have the chosen mean and std, but each sequence is totally uncorrelated from the other.
>Is there (in R) a way to produce sequences of Gaussian distributed random numbers with a chosen correlation? Regardless whether this is the proper way of calculating the resampled efficient frontier, I wonder it this has been implemented somewhere for R.
>Regards
>
>Lorenzo
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
>
>  
>


From rainer.boehme at inf.tu-dresden.de  Wed Nov 16 11:29:01 2005
From: rainer.boehme at inf.tu-dresden.de (Rainer =?iso-8859-1?Q?B=F6hme?=)
Date: Wed, 16 Nov 2005 10:29:01 -0000 (GMT)
Subject: [R-sig-finance] Sorting
In-Reply-To: <8ed68eed0511160132n5818ea93v@mail.gmail.com>
References: <1132132664.c7d26e9cL.Isella@myrealbox.com>
	<8ed68eed0511160132n5818ea93v@mail.gmail.com>
Message-ID: <51356.195.128.2.67.1132136941.squirrel@mail.inf.tu-dresden.de>

Hi Lorenzo,

# given this 2 x N matrix

N <- 10^5
A <- matrix(rnorm(2*N),2)

# use the following statement to sort the column vectors by their elements
in the i-th row

i <- 1
A <- A[,order(A[i,])]

Hope this helps,
Rainer

> Good Morning Lorenzo,
>
> I had a look at ?sort and it said "For ordering along more than one
> variable, e.g., for sorting data frames, see 'order'." So I had a look
> at ?order... and popped down to the examples and it talks about
> ordering a dataframe - not the clearest of examples, but hey :-)
> Sometimes the obvious is the hardest to see :-)
>
> cheers!
> Sean
>
>
> On 16/11/05, L.Isella <L.Isella at myrealbox.com> wrote:
>> Dear All,
>> I have a long array (say 2xN, with N of the order of 10^5 at least) made
>> up of couples of numerical values.
>> I would like to sort these N couples in increasing order of the value of
>> the numbers in the lower row.
>> It is very easy to use sort() to take care of the sorting of the lower
>> row, but then I also have to sort the upper row so that the values of
>> each couple still match.
>> I did it using a double loop, but for large N this is very slow.
>> This is really a bottleneck in my code...
>> Any suggestions?
>> Regards
>>
>> Lorenzo
>>
>> _______________________________________________
>> R-sig-finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From TobiasBr at Taquanta.com  Thu Nov 17 08:56:56 2005
From: TobiasBr at Taquanta.com (Brandt, T. (Tobias))
Date: Thu, 17 Nov 2005 09:56:56 +0200
Subject: [R-sig-finance] Sorting
Message-ID: <A77412E534FCD248A93A81F37CC75B7A033F8C1E@waxbill.africa.nedcor.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20051117/7a4befb4/attachment.pl

From maechler at stat.math.ethz.ch  Thu Nov 17 11:37:03 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 17 Nov 2005 11:37:03 +0100
Subject: [R-sig-finance] Sorting
In-Reply-To: <A77412E534FCD248A93A81F37CC75B7A033F8C1E@waxbill.africa.nedcor.net>
References: <A77412E534FCD248A93A81F37CC75B7A033F8C1E@waxbill.africa.nedcor.net>
Message-ID: <17276.23887.730903.678165@stat.math.ethz.ch>

>>>>> "ToBra" == Brandt, T (Tobias) <TobiasBr at taquanta.com>
>>>>>     on Thu, 17 Nov 2005 09:56:56 +0200 writes:

    ToBra> Hi
    ToBra> I'd like to add that in my experience using "order" would be the preferred
    ToBra> method as "sort" can lead to unexpected results as I hope the following
    ToBra> example will show.

    >> a <- matrix(4:1, 2,2, byrow=TRUE)
    >> colnames(a) <- c('b', 'a')
    >> a
    ToBra>      b a
    ToBra> [1,] 4 3
    ToBra> [2,] 2 1
    >> sort(a[2,])
    ToBra> a b 
    ToBra> 1 2 
    >> # works as expected 

yes, since it's sorting  the *vector*  a[2,] ,
and for vectors it's clear what do to with names.

    >> # whereas the following can lead the unwary user astray

    >> sort(a[2,,drop=FALSE])
    ToBra>      b a
    ToBra> [1,] 1 2
    >> 

    ToBra> This led to some errors in my code which took me a
    ToBra> while to track down so I'd just like to put it out
    ToBra> there as a caveat.

good;  better would have been to go to the R-help (or maybe
R-devel) mailing list and ask / comment about it in public
(but read on)

    ToBra> Not having looked at the source code of "sort" in any
    ToBra> detail and going purely on input->black box->output
    ToBra> based experience it seems to me that "sort" only
    ToBra> sorts the associated "names" attribute and not the
    ToBra> "colnames".


Several weeks ago, we (R Core development team) have seen this
and changed it for "R-devel" (which will become R-2.3.0 in April'06).

The new behavior is to drop the dimnames when sorting matrices,
since in general it doesn't make any sense to keep them;
only in your case of a matrix with just one row (or just one
column) it would make sense.

If this needs more discussion,  *PLEASE*  move it to the 
R-devel mailing list.

Regards,
Martin Maechler, ETH Zurich



    ToBra> Now the "drop=FALSE" part in the above example might seem somewhat
    ToBra> artificial but when dealing with timeseries this occurs quite often.  For
    ToBra> example, carrying on with the above example

    >> library(zoo)
    >> t <- zoo(a, c(2001,2002))
    >> t
    ToBra> b a
    ToBra> 2001 4 3
    ToBra> 2002 2 1
    >> t2 <- window(t, 2002)
    >> t2
    ToBra> b a
    ToBra> 2002 2 1
    >> sort(t2)
    ToBra> b a
    ToBra> 2002 1 2
    >> sort(as.vector(t2))
    ToBra> [1] 1 2
    >> sort(as.matrix(t2))
    ToBra> b a
    ToBra> [1,] 1 2
    >> sort(as.matrix(t2)[1,])
    ToBra> a b 
    ToBra> 1 2 
    >> sort(t2[1,])
    ToBra> b a
    ToBra> 2002 1 2
    >> # none of which really gives the desired result since there is always some
    ToBra> loss of information or erroneous
    >> # information, ie. loss of dimension names or incorrectly labelled
    ToBra> dimensions.
    >> # Using order on the other hand is transparent and preserves all the
    ToBra> information.
    >> t2[,order(t2)]
    ToBra> a b
    ToBra> 2002 1 2
    >> 

    ToBra> Regards

    ToBra> Tobias 


    >> -----Original Message-----
    >> From: r-sig-finance-bounces at stat.math.ethz.ch 
    >> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of 
    >> Rainer Bhme
    >> Sent: 16 November 2005 12:29 PM
    >> To: seanpor at acm.org
    >> Cc: r-sig-finance at stat.math.ethz.ch; L.Isella
    >> Subject: Re: [R-sig-finance] Sorting
    >> 
    >> Hi Lorenzo,
    >> 
    >> # given this 2 x N matrix
    >> 
    >> N <- 10^5
    >> A <- matrix(rnorm(2*N),2)
    >> 
    >> # use the following statement to sort the column vectors by 
    >> their elements in the i-th row
    >> 
    >> i <- 1
    >> A <- A[,order(A[i,])]
    >> 
    >> Hope this helps,
    >> Rainer
    >> 
    >>> Good Morning Lorenzo,
    >>> 
    >>> I had a look at ?sort and it said "For ordering along more than one 
    >>> variable, e.g., for sorting data frames, see 'order'." So I 
    >> had a look 
    >>> at ?order... and popped down to the examples and it talks about 
    >>> ordering a dataframe - not the clearest of examples, but hey :-) 
    >>> Sometimes the obvious is the hardest to see :-)
    >>> 
    >>> cheers!
    >>> Sean
    >>> 
    >>> 
    >>> On 16/11/05, L.Isella <L.Isella at myrealbox.com> wrote:
    >>>> Dear All,
    >>>> I have a long array (say 2xN, with N of the order of 10^5 at least) 
    >>>> made up of couples of numerical values.
    >>>> I would like to sort these N couples in increasing order of 
    >> the value 
    >>>> of the numbers in the lower row.
    >>>> It is very easy to use sort() to take care of the sorting of the 
    >>>> lower row, but then I also have to sort the upper row so that the 
    >>>> values of each couple still match.
    >>>> I did it using a double loop, but for large N this is very slow.
    >>>> This is really a bottleneck in my code...
    >>>> Any suggestions?
    >>>> Regards
    >>>> 
    >>>> Lorenzo


From jeff at kanecap.com  Fri Nov 18 15:49:03 2005
From: jeff at kanecap.com (Jeff Enos)
Date: Fri, 18 Nov 2005 09:49:03 -0500
Subject: [R-sig-finance] "portfolio" package for equity portfolio analysis
Message-ID: <17277.59871.146582.546582@gargle.gargle.HOWL>

We would like to announce the availability of the "portfolio" package
in R for analysing equity portfolios. It is available at CRAN and will
propagate out to various mirrors in due course. To take a look, you
can:

> install.packages("portfolio")
...
> vignette("portfolio")

and play around. Those who would just like to check out an
introduction can simply look at:

http://www.kanecap.com/R/portfolio/portfolio.pdf

This is the first, very rough version of a package that we hope to
build and extend over the coming months. Our ambitions for this
project are not small. Hundreds of professionals around the world use
R for portfolio analysis but, right now, we all use our own individual
tools for doing so. That seems a shame.

Of course, one reason for the lack of cooperation is that finance is
largely a zero sum game. Every dollar that we make is a dollar that
someone else loses. But, at least when it comes to analysis tools
(rather than alpha generation). this conflict does not seem
insurmountable. We will see.

So, consider this e-mail a call for comment and cooperation. Are there
others out there who would be interested in using this package? Are
there others willing to contribute, if only in terms of bug fixes,
test cases, documentation and the like? Please let us know and, in
fact, let the whole list know.

By releasing a rough version of the package, we are especially
interested in recruiting other developers to the project. None of our
design decisions are set in stone. Serious offers to contribute will
earn a significant say in future development.

Dave Kane
Jeff Enos


From kriskumar at earthlink.net  Fri Nov 18 17:05:35 2005
From: kriskumar at earthlink.net (Kris)
Date: Fri, 18 Nov 2005 08:05:35 -0800 (GMT-08:00)
Subject: [R-sig-finance] "portfolio" package for equity portfolio
 analysis
Message-ID: <30840636.1132329935707.JavaMail.root@elwamui-darkeyed.atl.sa.earthlink.net>

I have been playing around with the almgren and chriss paper on optimal portfolios given a sort
and no information on expected returns could someone confirm if this makes sense?

#This function compute the optimal portfolio given a sort
#and covariance information given no information on 
#expected return information
#see: http://www.math.toronto.edu/almgren/papers/sort.pdf

#computes the centroid of the portfolio
#
#See Almgren and Chris  Pg 16.
centroid<-function(i,n)
{
# n - number of assets in this sort
#predefined const for single complete sort
aConst<-0.4424
bConst<-0.1185
betaConst<-0.21
alpha <- aConst - bConst * (n ^ (-betaConst))
centroid <- qnorm((n + 1 - i - alpha) / (n - 2 * alpha + 1))
return(centroid)
}
#Weights from covariance information
port.wt<-function(centroid,Sigma)
{
SigInv<-solve(Sigma)
port.wt<-centroid %*% SigInv
}

### To use this function compute the centroid first 
# then call port.wt


From kriskumar at earthlink.net  Fri Nov 18 17:10:17 2005
From: kriskumar at earthlink.net (Kris)
Date: Fri, 18 Nov 2005 08:10:17 -0800 (GMT-08:00)
Subject: [R-sig-finance] Random Numbers
Message-ID: <5823559.1132330217681.JavaMail.root@elwamui-darkeyed.atl.sa.earthlink.net>

 I dont quite follow what you mean? People do resampled eff frontier with bootstrapping/bootstrapping+jackknife but this is done on the correlation/covarianceestimation process.
If all you need is correlated rng take a look at V&R's MASS package rmvnorm in particular. alternatively you can use rnorm with chol to get the correlated RNG.

L.Isella wrote:

>Dear All,
>I would like to be able to generate long sequences of random numbers which I need to work out the resampled efficient frontier of a portfolio.
>People normally take random draws from some multivariate (say Gaussian, to keep it simple) distribution to simulate the returns of the various assets and generate many random portfolios.
>I accomplish that working with rnorm().
>However, this way, in the limit of very long sequences, the simulated asset returns have the chosen mean and std, but each sequence is totally uncorrelated from the other.
>Is there (in R) a way to produce sequences of Gaussian distributed random numbers with a chosen correlation? Regardless whether this is the proper way of calculating the resampled efficient frontier, I wonder it this has been implemented somewhere for R.
>Regards
>
>Lorenzo
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
>
>  
>

_______________________________________________
R-sig-finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From L.Isella at myrealbox.com  Fri Nov 18 21:09:54 2005
From: L.Isella at myrealbox.com (L.Isella)
Date: Fri, 18 Nov 2005 20:09:54 +0000
Subject: [R-sig-finance] Random Numbers
Message-ID: <1132344594.c8061ffcL.Isella@myrealbox.com>

On 11/18/05, Kris <kriskumar at earthlink.net> wrote:
>  I dont quite follow what you mean? People do resampled eff frontier with bootstrapping/bootstrapping+jackknife but this is done on the correlation/covarianceestimation process.
> If all you need is correlated rng take a look at V&R's MASS package rmvnorm in particular. alternatively you can use rnorm with chol to get the correlated RNG.

Well, I mean the idea of resampled efficiency as expressed by Michaud in his book: you assume that the returns of the stocks in your ptfs are normally distributed (which is a reasonable approximation for the stocks I deal with).
You come up with some guesses about the "true" expected rtns and the "true" covariance matrix of these assets. 
In other words you assume that your historical data are the sample of multivariate normal distribution with certain correlations.
Then you take random draws from this distribution and simulate several (actually plenty) sets of returns.
For each simulated set of returns, this provides you with some average returns and correlations and you optimize a ptf on the basis of these data.
Oversimplyfing, you repeat this procedure many times, obtain some average ptf weights along the simulated efficient frontier and you use these weights to generate the resampled efficient frontier by means of the "true" covariance matrix and "true" expected rtns.
At least this is how I understood it. Anyone understood it differently?
Cheers

Lorenzo


From patrick at burns-stat.com  Fri Nov 18 21:42:49 2005
From: patrick at burns-stat.com (Patrick Burns)
Date: Fri, 18 Nov 2005 20:42:49 +0000
Subject: [R-sig-finance] Random Numbers
In-Reply-To: <1132344594.c8061ffcL.Isella@myrealbox.com>
References: <1132344594.c8061ffcL.Isella@myrealbox.com>
Message-ID: <437E3CC9.8050701@burns-stat.com>

I think there are several problems with the resampled
efficient frontier, here is one: The procedure as I understand
it is to bootstrap the mean of the historical returns.  What
should be bootstrapped is the alpha generation process.
One hopes that there are few fund managers who use the
historical mean as their expected return.  Bootstrapping the
actual alpha generation process is likely to be non-trivial.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

L.Isella wrote:

>On 11/18/05, Kris <kriskumar at earthlink.net> wrote:
>  
>
>> I dont quite follow what you mean? People do resampled eff frontier with bootstrapping/bootstrapping+jackknife but this is done on the correlation/covarianceestimation process.
>>If all you need is correlated rng take a look at V&R's MASS package rmvnorm in particular. alternatively you can use rnorm with chol to get the correlated RNG.
>>    
>>
>
>Well, I mean the idea of resampled efficiency as expressed by Michaud in his book: you assume that the returns of the stocks in your ptfs are normally distributed (which is a reasonable approximation for the stocks I deal with).
>You come up with some guesses about the "true" expected rtns and the "true" covariance matrix of these assets. 
>In other words you assume that your historical data are the sample of multivariate normal distribution with certain correlations.
>Then you take random draws from this distribution and simulate several (actually plenty) sets of returns.
>For each simulated set of returns, this provides you with some average returns and correlations and you optimize a ptf on the basis of these data.
>Oversimplyfing, you repeat this procedure many times, obtain some average ptf weights along the simulated efficient frontier and you use these weights to generate the resampled efficient frontier by means of the "true" covariance matrix and "true" expected rtns.
>At least this is how I understood it. Anyone understood it differently?
>Cheers
>
>Lorenzo
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
>
>  
>


From L.Isella at myrealbox.com  Fri Nov 18 23:45:49 2005
From: L.Isella at myrealbox.com (L.Isella)
Date: Fri, 18 Nov 2005 22:45:49 +0000
Subject: [R-sig-finance] Random Numbers
Message-ID: <1132353949.c818a39cL.Isella@myrealbox.com>

On 11/18/05, Patrick Burns <patrick at burns-stat.com> wrote:
> I think there are several problems with the resampled
> efficient frontier, here is one: The procedure as I understand
> it is to bootstrap the mean of the historical returns.  What
> should be bootstrapped is the alpha generation process.
> One hopes that there are few fund managers who use the
> historical mean as their expected return.  Bootstrapping the
> actual alpha generation process is likely to be non-trivial.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")

Hi,
Well, the way I understood the efficient resample thing is above all a cure for the instability of Markowitz optimization leading to non-diversified ptfs.
The problem of the correlations and expected rtns is quite a separate matter, but whatever solution  I "invent" for these problems, then I can Monte Carlo simulate the series of returns accordingly.

Lorenzo


From kriskumar at earthlink.net  Sat Nov 19 01:49:42 2005
From: kriskumar at earthlink.net (Kris)
Date: Fri, 18 Nov 2005 16:49:42 -0800 (GMT-08:00)
Subject: [R-sig-finance] Random Numbers
Message-ID: <7123618.1132361382292.JavaMail.root@elwamui-ovcar.atl.sa.earthlink.net>

Ok there are several things going on here:
i)  Michaud's resampling algorithm
ii) What i described which is doing some sort of bias reduction in your historical covariance estimation piece by resampling and if you think there is serial correlation information then do block bootstrapping
iii) What Patrick describes which is to do the resampling on the alpha. 

I don't if this is related to the original question, but what is the prefered method that is used to detect
 for a medium/long term investor that things have changed enough to rebalance?.



-----Original Message-----
From: Patrick Burns <patrick at burns-stat.com>
Sent: Nov 18, 2005 12:42 PM
To: "L.Isella" <L.Isella at myrealbox.com>
Cc: kriskumar at earthlink.net, r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-sig-finance] Random Numbers

I think there are several problems with the resampled
efficient frontier, here is one: The procedure as I understand
it is to bootstrap the mean of the historical returns.  What
should be bootstrapped is the alpha generation process.
One hopes that there are few fund managers who use the
historical mean as their expected return.  Bootstrapping the
actual alpha generation process is likely to be non-trivial.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

L.Isella wrote:

>On 11/18/05, Kris <kriskumar at earthlink.net> wrote:
>  
>
>> I dont quite follow what you mean? People do resampled eff frontier with bootstrapping/bootstrapping+jackknife but this is done on the correlation/covarianceestimation process.
>>If all you need is correlated rng take a look at V&R's MASS package rmvnorm in particular. alternatively you can use rnorm with chol to get the correlated RNG.
>>    
>>
>
>Well, I mean the idea of resampled efficiency as expressed by Michaud in his book: you assume that the returns of the stocks in your ptfs are normally distributed (which is a reasonable approximation for the stocks I deal with).
>You come up with some guesses about the "true" expected rtns and the "true" covariance matrix of these assets. 
>In other words you assume that your historical data are the sample of multivariate normal distribution with certain correlations.
>Then you take random draws from this distribution and simulate several (actually plenty) sets of returns.
>For each simulated set of returns, this provides you with some average returns and correlations and you optimize a ptf on the basis of these data.
>Oversimplyfing, you repeat this procedure many times, obtain some average ptf weights along the simulated efficient frontier and you use these weights to generate the resampled efficient frontier by means of the "true" covariance matrix and "true" expected rtns.
>At least this is how I understood it. Anyone understood it differently?
>Cheers
>
>Lorenzo
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
>
>  
>


From con.keating at financedevelopmentcentre.com  Sat Nov 19 10:37:10 2005
From: con.keating at financedevelopmentcentre.com (con.keating@financedevelopmentcentre.com)
Date: Sat, 19 Nov 2005 09:37:10 +0000
Subject: [R-sig-finance] Random Numbers
In-Reply-To: <1132353949.c818a39cL.Isella@myrealbox.com>
References: <1132353949.c818a39cL.Isella@myrealbox.com>
Message-ID: <20051119093710.i8t3wawhwkk44kc0@webmail.financedevelopmentcentre.com>

Hi Lorenzo

You should look at Campbell Harvey's (Duke University) website for a 
critique of
Michaud's resampled frontier - he makes the point rather well that it is not /
cannot be optimal.

Con





Quoting "L.Isella" <L.Isella at myrealbox.com>:

> On 11/18/05, Patrick Burns <patrick at burns-stat.com> wrote:
>> I think there are several problems with the resampled
>> efficient frontier, here is one: The procedure as I understand
>> it is to bootstrap the mean of the historical returns.  What
>> should be bootstrapped is the alpha generation process.
>> One hopes that there are few fund managers who use the
>> historical mean as their expected return.  Bootstrapping the
>> actual alpha generation process is likely to be non-trivial.
>>
>> Patrick Burns
>> patrick at burns-stat.com
>> +44 (0)20 8525 0696
>> http://www.burns-stat.com
>> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Hi,
> Well, the way I understood the efficient resample thing is above all 
> a cure for the instability of Markowitz optimization leading to 
> non-diversified ptfs.
> The problem of the correlations and expected rtns is quite a separate 
> matter, but whatever solution  I "invent" for these problems, then I 
> can Monte Carlo simulate the series of returns accordingly.
>
> Lorenzo
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From patrick at burns-stat.com  Sat Nov 19 11:26:04 2005
From: patrick at burns-stat.com (Patrick Burns)
Date: Sat, 19 Nov 2005 10:26:04 +0000
Subject: [R-sig-finance] Noise in portfolio optimization (was: Random
 Numbers)
In-Reply-To: <7123618.1132361382292.JavaMail.root@elwamui-ovcar.atl.sa.earthlink.net>
References: <7123618.1132361382292.JavaMail.root@elwamui-ovcar.atl.sa.earthlink.net>
Message-ID: <437EFDBC.4060401@burns-stat.com>

In the optimizations we are talking about, there is noise
in the expected returns and noise in the variance matrix.

Unless you are using a sample estimate of the variance
rather than something more stable like a factor model,
the error in the variance matrix will be minimal compared
to the error in the expected returns.  Hence a reasonable
approach to error in the variance matrix is not to worry
about it.

I think the proper answer of how to deal with noise in the
expected returns is to increase the trading cost based on
how noisy the expected return is for each asset.

First, note that 'portfolio optimization' is really a misnomer.
We really are (or should be) optimizing the trade.

We are also in a classic James-Stein shrinkage setting in
which we care about the overall outcome, not the individual
pieces.  If in reality the actual best trade is MSFT=-143,
IBM=78, and so on, we don't get any extra benefit for selling
exactly 143 of MSFT.  We benefit from the trade as a whole
being good. 

Given that we have noise, then theory tells us to shrink towards
something.  The question is, shrink towards what?  I think that
the answer has to be to shrink towards where we are, that is,
towards less trading.  The way to accomplish this is to increase
the trading cost based on the amount of noise in the expected
return.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Kris wrote:

>Ok there are several things going on here:
>i)  Michaud's resampling algorithm
>ii) What i described which is doing some sort of bias reduction in your historical covariance estimation piece by resampling and if you think there is serial correlation information then do block bootstrapping
>iii) What Patrick describes which is to do the resampling on the alpha. 
>
>I don't if this is related to the original question, but what is the prefered method that is used to detect
> for a medium/long term investor that things have changed enough to rebalance?.
>
>
>
>-----Original Message-----
>From: Patrick Burns <patrick at burns-stat.com>
>Sent: Nov 18, 2005 12:42 PM
>To: "L.Isella" <L.Isella at myrealbox.com>
>Cc: kriskumar at earthlink.net, r-sig-finance at stat.math.ethz.ch
>Subject: Re: [R-sig-finance] Random Numbers
>
>I think there are several problems with the resampled
>efficient frontier, here is one: The procedure as I understand
>it is to bootstrap the mean of the historical returns.  What
>should be bootstrapped is the alpha generation process.
>One hopes that there are few fund managers who use the
>historical mean as their expected return.  Bootstrapping the
>actual alpha generation process is likely to be non-trivial.
>
>Patrick Burns
>patrick at burns-stat.com
>+44 (0)20 8525 0696
>http://www.burns-stat.com
>(home of S Poetry and "A Guide for the Unwilling S User")
>
>L.Isella wrote:
>
>  
>
>>On 11/18/05, Kris <kriskumar at earthlink.net> wrote:
>> 
>>
>>    
>>
>>>I dont quite follow what you mean? People do resampled eff frontier with bootstrapping/bootstrapping+jackknife but this is done on the correlation/covarianceestimation process.
>>>If all you need is correlated rng take a look at V&R's MASS package rmvnorm in particular. alternatively you can use rnorm with chol to get the correlated RNG.
>>>   
>>>
>>>      
>>>
>>Well, I mean the idea of resampled efficiency as expressed by Michaud in his book: you assume that the returns of the stocks in your ptfs are normally distributed (which is a reasonable approximation for the stocks I deal with).
>>You come up with some guesses about the "true" expected rtns and the "true" covariance matrix of these assets. 
>>In other words you assume that your historical data are the sample of multivariate normal distribution with certain correlations.
>>Then you take random draws from this distribution and simulate several (actually plenty) sets of returns.
>>For each simulated set of returns, this provides you with some average returns and correlations and you optimize a ptf on the basis of these data.
>>Oversimplyfing, you repeat this procedure many times, obtain some average ptf weights along the simulated efficient frontier and you use these weights to generate the resampled efficient frontier by means of the "true" covariance matrix and "true" expected rtns.
>>At least this is how I understood it. Anyone understood it differently?
>>Cheers
>>
>>Lorenzo
>>
>>_______________________________________________
>>R-sig-finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
>>
>>
>> 
>>
>>    
>>
>
>
>
>
>
>  
>


From spencer.graves at pdf.com  Mon Nov 21 06:46:49 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 20 Nov 2005 21:46:49 -0800
Subject: [R-sig-finance] R source code
In-Reply-To: <1852.128.97.244.15.1132006168.squirrel@128.97.244.15>
References: <1852.128.97.244.15.1132006168.squirrel@128.97.244.15>
Message-ID: <43815F49.1030402@pdf.com>

	  The source for all of R is available (or it's in violation of the GNU 
license).  What would you like to see?  To see the R code for any 
function, type its name [without parentheses] at a command prompt.  The 
source for compiled code is also available.  Judicious use of 
"RSiteSearch" might provide more information if this is not enough.  Or 
send another question to this list.  (However, please read the posting 
guide! "www.R-project.org/posting-guide.html" first;  I believe that 
people who follow the posting guide generally receive quicker, more 
informative replies than those who don't -- and any find answers in the 
process of preparing a question.)

	  hope this helps.
	  spencer graves

jmbucci at stat.ucla.edu wrote:

> Hello,
> 
> Is there a way to view source code directly in R for Finance added packages?
> 
> Thank you.
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915


From Rich at mango-solutions.com  Wed Nov 23 13:15:06 2005
From: Rich at mango-solutions.com (Rich@mango-solutions.com)
Date: Wed, 23 Nov 2005 12:15:06 -0000
Subject: [R-sig-finance] JOB: S-PLUS/R Finance Consultant,
	Mango Solutions (UK)
Message-ID: <200511231215.jANCF2Iq030621@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20051123/e1505523/attachment.pl

From Rich at mango-solutions.com  Wed Nov 23 15:13:39 2005
From: Rich at mango-solutions.com (Rich@mango-solutions.com)
Date: Wed, 23 Nov 2005 14:13:39 -0000
Subject: [R-sig-finance] JOB: S-PLUS/R Finance Consultant,
	Mango Solutions (UK)
In-Reply-To: <804E2D20C6625A48A2E58992E627F5CB03E217@ppc-ex01.ppcmetrics.com>
Message-ID: <200511231413.jANEDgNM005535@hypatia.math.ethz.ch>

Hi,

Many apologies if I have caused offence.  We are trying to recruit an R
consultant with finance sector knowledge.  The R-Finance list seemed a good
place to advertise for people with this skillset.  

Perhaps the alternative is to set up a section for job postings on the main
R-Project.org site, or only send such emails to the main R-Help list.  As
you say, this is up for the discussion.  In the meantime, let me apologise
again for any offence caused.

Kind regards,
Rich.

mangosolutions
S & R Training and Consulting Services

-----Original Message-----
From: Schumacher Franz [mailto:franz.schumacher at ppcmetrics.ch] 
Sent: 23 November 2005 13:57
To: Rich at mango-solutions.com; r-sig-finance at stat.math.ethz.ch
Subject: AW: [R-sig-finance] JOB: S-PLUS/R Finance Consultant,Mango
Solutions (UK)

Rich, 

Is this the right forum for this? In my opinion the answer is a clear "No" -
it should remain a forum for technical problems/questions. But this is of
course up for discussion. 

Dr. Franz Schumacher, Z?rich

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]Im Auftrag von
Rich at mango-solutions.com
Gesendet: Mittwoch, 23. November 2005 13:15
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-sig-finance] JOB: S-PLUS/R Finance Consultant,Mango
Solutions (UK)


 Mango Solutions, providers of S-PLUS and R consulting, development and
Training Services, are looking for 2 consultants to join their UK-based
technical team.  We are looking for highly motivated individuals to work in
a customer-focused environment.

This is a unique opportunity to develop within a dynamic company which has
been expanding rapidly and profitably since it's inception in 2002.  Mango
Solutions have an excellent reputation for quality work, and customers
include major finance, pharmaceutical, marketing and consumer companies.
For further information, please send your CV to careers at mango-solutions.com.

 
S-PLUS/R Finance Consultant (Reading, UK)

The role

*	To support the development of the R language in the financial
sector.
*	To provide ad-hoc statistical software consulting services to
finance customers.
*	To create and deliver tailored statistical software courses to
finance clients worldwide.
*	To advise financial customers on statistical and software issues.
*	To participate in the design and implementation of bespoke
statistical and reporting applications.

 

Qualifications

*	Phd, Msc or Bsc in statistics, mathematics, physics or computer
sciences.
*	Knowledge of statistical languages: S-PLUS, R, or Matlab.
*	Experience of programming and analysis in a financial company.
*	Excellent communication skills.
*	Experience of other finance softwares is an advantage (eg. eViews,
FactSet, RATS). 
*	Experience of general software systems is an advantage (eg. Java,
Oracle, C, XML, HTML).

 

Offering

*	Opportunity to extend your technical knowledge within the finance
sector.
*	Exposure to other areas of statistical software consulting by
working with customers from pharmaceutcal, CRM, consumer and environmental
sectors.
*	Flexible working environment and working hours.
*	Competitive salary.

 

 


	[[alternative HTML version deleted]]

_______________________________________________
R-sig-finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From dave at kanecap.com  Wed Nov 23 15:30:44 2005
From: dave at kanecap.com (David Kane)
Date: Wed, 23 Nov 2005 09:30:44 -0500
Subject: [R-sig-finance] JOB: S-PLUS/R Finance Consultant,
	Mango Solutions (UK)
In-Reply-To: <200511231413.jANEDgNM005535@hypatia.math.ethz.ch>
References: <804E2D20C6625A48A2E58992E627F5CB03E217@ppc-ex01.ppcmetrics.com>
	<200511231413.jANEDgNM005535@hypatia.math.ethz.ch>
Message-ID: <17284.32020.801394.620291@gargle.gargle.HOWL>

 > Is this the right forum for this? In my opinion the answer is a clear "No" -
 > it should remain a forum for technical problems/questions. But this is of
 > course up for discussion. 

I think that this list is a perfect forum for job postings relating to
R and finance.

Not that I am looking for a job or anything . . .

;-)

Dave Kane


From jmbucci at stat.ucla.edu  Wed Nov 23 16:44:57 2005
From: jmbucci at stat.ucla.edu (jmbucci@stat.ucla.edu)
Date: Wed, 23 Nov 2005 07:44:57 -0800 (PST)
Subject: [R-sig-finance] JOB: S-PLUS/R Finance Consultant,
 Mango Solutions (UK)
In-Reply-To: <17284.32020.801394.620291@gargle.gargle.HOWL>
References: <804E2D20C6625A48A2E58992E627F5CB03E217@ppc-ex01.ppcmetrics.com>
	<200511231413.jANEDgNM005535@hypatia.math.ethz.ch>
	<17284.32020.801394.620291@gargle.gargle.HOWL>
Message-ID: <50790.71.106.144.208.1132760697.squirrel@71.106.144.208>

As a grad student, it is nice to see the opportunities out there,
especially R computing in the Finance practice.




>  > Is this the right forum for this? In my opinion the answer is a clear
> "No" -
>  > it should remain a forum for technical problems/questions. But this is
> of
>  > course up for discussion.
>
> I think that this list is a perfect forum for job postings relating to
> R and finance.
>
> Not that I am looking for a job or anything . . .
>
> ;-)
>
> Dave Kane
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>


From sean.oriordain at gmail.com  Wed Nov 23 16:54:57 2005
From: sean.oriordain at gmail.com (Sean O'Riordain)
Date: Wed, 23 Nov 2005 15:54:57 +0000
Subject: [R-sig-finance] JOB: S-PLUS/R Finance Consultant,
	Mango Solutions (UK)
In-Reply-To: <50790.71.106.144.208.1132760697.squirrel@71.106.144.208>
References: <804E2D20C6625A48A2E58992E627F5CB03E217@ppc-ex01.ppcmetrics.com>
	<200511231413.jANEDgNM005535@hypatia.math.ethz.ch>
	<17284.32020.801394.620291@gargle.gargle.HOWL>
	<50790.71.106.144.208.1132760697.squirrel@71.106.144.208>
Message-ID: <8ed68eed0511230754n3abc70e0h@mail.gmail.com>

I don't think there are *that* many R job opportunities in finance
arising every month - if the number of opportunities posted to the
list stays low relative to the rest of the more technical posts, then
I don't have a problem with it at all.

If the number of R-Job emails arising each month starts to rise
significantly, then perhaps a separate r-jobs at stat.math.ethz.ch email
list might be considered specificially for this purpose ?

cheers,
Sean


On 23/11/05, jmbucci at stat.ucla.edu <jmbucci at stat.ucla.edu> wrote:
> As a grad student, it is nice to see the opportunities out there,
> especially R computing in the Finance practice.
>
>
>
>
> >  > Is this the right forum for this? In my opinion the answer is a clear
> > "No" -
> >  > it should remain a forum for technical problems/questions. But this is
> > of
> >  > course up for discussion.
> >
> > I think that this list is a perfect forum for job postings relating to
> > R and finance.
> >
> > Not that I am looking for a job or anything . . .
> >
> > ;-)
> >
> > Dave Kane
> >
> > _______________________________________________
> > R-sig-finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
> >
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From pcampbell at econ.bbk.ac.uk  Wed Nov 23 17:41:31 2005
From: pcampbell at econ.bbk.ac.uk (Phineas Campbell)
Date: Wed, 23 Nov 2005 16:41:31 -0000
Subject: [R-sig-finance] JOB: S-PLUS/R Finance Consultant,
	Mango Solutions (UK)
In-Reply-To: <200511231215.jANCF2Iq030621@hypatia.math.ethz.ch>
Message-ID: <NGECIFANPOJAGABBAEAPCEGAFFAA.pcampbell@econ.bbk.ac.uk>

I'm perfectly happy with relevant job postings on the mailiong list,
although I'm not responsible for maintaining the servers so Martin Maechler,
who does an excellent job, should have the final say.

We all have an interest in increasing R's user base, especially in the
commercial sector.

Phineas

ps I haven't completely discounted applying for this position


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of Rich
Sent: Wednesday, November 23, 2005 12:15 PM
To: r-sig-finance
Subject: [R-sig-finance] JOB: S-PLUS/R Finance Consultant, Mango
Solutions (UK)




Mango Solutions, providers of S-PLUS and R consulting, development and
Training Services, are looking for 2 consultants to join their UK-based
technical team.  We are looking for highly motivated individuals to work in
a customer-focused environment.

This is a unique opportunity to develop within a dynamic company which has
been expanding rapidly and profitably since it's inception in 2002.  Mango
Solutions have an excellent reputation for quality work, and customers
include major finance, pharmaceutical, marketing and consumer companies.
For further information, please send your CV to careers at mango-solutions.com.





S-PLUS/R Finance Consultant (Reading, UK)



The role

*	To support the development of the R language in the financial
sector.
*	To provide ad-hoc statistical software consulting services to
finance customers.
*	To create and deliver tailored statistical software courses to
finance clients worldwide.
*	To advise financial customers on statistical and software issues.
*	To participate in the design and implementation of bespoke
statistical and reporting applications.



Qualifications

*	Phd, Msc or Bsc in statistics, mathematics, physics or computer
sciences.
*	Knowledge of statistical languages: S-PLUS, R, or Matlab.
*	Experience of programming and analysis in a financial company.
*	Excellent communication skills.
*	Experience of other finance softwares is an advantage (eg. eViews,
FactSet, RATS).
*	Experience of general software systems is an advantage (eg. Java,
Oracle, C, XML, HTML).



Offering

*	Opportunity to extend your technical knowledge within the finance
sector.
*	Exposure to other areas of statistical software consulting by
working with customers from pharmaceutcal, CRM, consumer and environmental
sectors.
*	Flexible working environment and working hours.
*	Competitive salary.






	[[alternative HTML version deleted]]

_______________________________________________
R-sig-finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From spencer.graves at pdf.com  Wed Nov 23 17:47:23 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 23 Nov 2005 08:47:23 -0800
Subject: [R-sig-finance] JOB: S-PLUS/R Finance Consultant,
 Mango Solutions (UK)
In-Reply-To: <8ed68eed0511230754n3abc70e0h@mail.gmail.com>
References: <804E2D20C6625A48A2E58992E627F5CB03E217@ppc-ex01.ppcmetrics.com>	<200511231413.jANEDgNM005535@hypatia.math.ethz.ch>	<17284.32020.801394.620291@gargle.gargle.HOWL>	<50790.71.106.144.208.1132760697.squirrel@71.106.144.208>
	<8ed68eed0511230754n3abc70e0h@mail.gmail.com>
Message-ID: <43849D1B.8040407@pdf.com>

	  I concur with Sean:  It's a fact of life that none of us really know 
if our jobs will be there tomorrow.  Even traditionally secure jobs in 
government can (at least in the US, and I believe elsewhere) be 
terminated in relatively short order by a change in legislation.  Even 
without this uncertainty, it's good to know where there is demand for 
the skills of people like those who subscribe to this list.  Of course, 
if it gets too frequent, then a separate list for that might be 
appropriate.  However, we are not even close to that, in my opinion.

	  Spencer Graves

Sean O'Riordain wrote:

> I don't think there are *that* many R job opportunities in finance
> arising every month - if the number of opportunities posted to the
> list stays low relative to the rest of the more technical posts, then
> I don't have a problem with it at all.
> 
> If the number of R-Job emails arising each month starts to rise
> significantly, then perhaps a separate r-jobs at stat.math.ethz.ch email
> list might be considered specificially for this purpose ?
> 
> cheers,
> Sean
> 
> 
> On 23/11/05, jmbucci at stat.ucla.edu <jmbucci at stat.ucla.edu> wrote:
> 
>>As a grad student, it is nice to see the opportunities out there,
>>especially R computing in the Finance practice.
>>
>>
>>
>>
>>
>>> > Is this the right forum for this? In my opinion the answer is a clear
>>>"No" -
>>> > it should remain a forum for technical problems/questions. But this is
>>>of
>>> > course up for discussion.
>>>
>>>I think that this list is a perfect forum for job postings relating to
>>>R and finance.
>>>
>>>Not that I am looking for a job or anything . . .
>>>
>>>;-)
>>>
>>>Dave Kane
>>>
>>>_______________________________________________
>>>R-sig-finance at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>
>>>
>>
>>_______________________________________________
>>R-sig-finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
> 
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915


From maechler at stat.math.ethz.ch  Thu Nov 24 15:30:39 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 24 Nov 2005 15:30:39 +0100
Subject: [R-sig-finance] JOB: S-PLUS/R Finance Consultant...
In-Reply-To: <43849D1B.8040407@pdf.com>
References: <804E2D20C6625A48A2E58992E627F5CB03E217@ppc-ex01.ppcmetrics.com>
	<200511231413.jANEDgNM005535@hypatia.math.ethz.ch>
	<17284.32020.801394.620291@gargle.gargle.HOWL>
	<50790.71.106.144.208.1132760697.squirrel@71.106.144.208>
	<8ed68eed0511230754n3abc70e0h@mail.gmail.com>
	<43849D1B.8040407@pdf.com>
Message-ID: <17285.52879.115516.738501@stat.math.ethz.ch>

Since people have asked for it, let me have "my say" as list maintainer:

As long as such posting are really rare, we have accepted them,
as "of wider interest to R users", on the R lists

However,

1) we would like really short e-mails with a link to a website
   for more details.

2) We request that the advertizers are informed about the
   posting guide, and hence

 2a) I'd rather be frowning to see "text + HTML" add postings.

 2b) Crossposting {sending things to more than one R list specifically},
     seems impolite.

Martin


From ajayshah at mayin.org  Fri Nov 25 02:33:40 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Fri, 25 Nov 2005 07:03:40 +0530
Subject: [R-sig-finance] Baxter-King filtering?
Message-ID: <20051125013340.GA1037@lubyanka.local>

A few weeks ago, Gabor showed this code for Hodrick Prescott
filtering:

# See http://econ.ohio-state.edu/hwkim/hpfilter.pdf
# Maravall & del Rio (2001) recommend
#      Quarterly data         lambda=1600
#      Monthly                1e5<lambda<1.4e5
#      Annual                 6 < lambda < 14
hodrickprescott <- function(y, lambda) {
  eye <- diag(length(y))
  F <- crossprod(diff(eye, d=2))
  tau <- solve(lambda * F + eye, y)
  list(trend=tau, cycle=y-tau)
}

The literature seems to think that Baxter-King works better. The idea
is not hard: it's just a band pass filter at `business cycle
frequencies'. So the steps would be to do a fourier transform, zap out
power at all but the interesting frequencies, and then do an inverse
transform. The implementation is a bit harder because of short data
series and (I think) edge effects. I'm not fluent with spectral
analysis and am unable to hack this up. Has someone already done this?

I noticed that it's supported by `grocer' which runs under Scilab, so
GPL code is probably out there already.

-- 
Ajay Shah
ajayshah at mayin.org
http://www.mayin.org/ajayshah


From atp at piskorski.com  Fri Nov 25 12:55:01 2005
From: atp at piskorski.com (Andrew Piskorski)
Date: Fri, 25 Nov 2005 06:55:01 -0500
Subject: [R-sig-finance] Baxter-King filtering?
In-Reply-To: <20051125013340.GA1037@lubyanka.local>
References: <20051125013340.GA1037@lubyanka.local>
Message-ID: <20051125115501.GA3521@tehun.pair.com>

On Fri, Nov 25, 2005 at 07:03:40AM +0530, Ajay Narottam Shah wrote:
> A few weeks ago, Gabor showed this code for Hodrick Prescott
> filtering:

> The literature seems to think that Baxter-King works better. The idea

This paper:

  "A Frequency Selective Filter for Short-Length Time Series"
  by Alessandra Iacobucci, May 2004
  http://www.ofce.sciences-po.fr/pdf/dtravail/WP2004-05.pdf

compares the Hodrick-Prescott, Baxter-King, and Christiano-Fitsgerald
filters to his Hamming-windowed filter, and says that the
Hamming-windowed is best.

-- 
Andrew Piskorski <atp at piskorski.com>
http://www.piskorski.com/


From franz.schumacher at ppcmetrics.ch  Wed Nov 23 14:56:54 2005
From: franz.schumacher at ppcmetrics.ch (Schumacher Franz)
Date: Wed, 23 Nov 2005 14:56:54 +0100
Subject: [R-sig-finance] JOB: S-PLUS/R Finance Consultant,
	Mango Solutions (UK)
Message-ID: <804E2D20C6625A48A2E58992E627F5CB03E217@ppc-ex01.ppcmetrics.com>

Rich, 

Is this the right forum for this? In my opinion the answer is a clear "No" - it should remain a forum for technical problems/questions. But this is of course up for discussion. 

Dr. Franz Schumacher, Z?rich

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]Im Auftrag von
Rich at mango-solutions.com
Gesendet: Mittwoch, 23. November 2005 13:15
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-sig-finance] JOB: S-PLUS/R Finance Consultant,Mango
Solutions (UK)


 

Mango Solutions, providers of S-PLUS and R consulting, development and
Training Services, are looking for 2 consultants to join their UK-based
technical team.  We are looking for highly motivated individuals to work in
a customer-focused environment.

This is a unique opportunity to develop within a dynamic company which has
been expanding rapidly and profitably since it's inception in 2002.  Mango
Solutions have an excellent reputation for quality work, and customers
include major finance, pharmaceutical, marketing and consumer companies.
For further information, please send your CV to careers at mango-solutions.com.

 

 

S-PLUS/R Finance Consultant (Reading, UK)

 

The role

*	To support the development of the R language in the financial
sector.
*	To provide ad-hoc statistical software consulting services to
finance customers.
*	To create and deliver tailored statistical software courses to
finance clients worldwide.
*	To advise financial customers on statistical and software issues.
*	To participate in the design and implementation of bespoke
statistical and reporting applications.

 

Qualifications

*	Phd, Msc or Bsc in statistics, mathematics, physics or computer
sciences.
*	Knowledge of statistical languages: S-PLUS, R, or Matlab.
*	Experience of programming and analysis in a financial company.
*	Excellent communication skills.
*	Experience of other finance softwares is an advantage (eg. eViews,
FactSet, RATS). 
*	Experience of general software systems is an advantage (eg. Java,
Oracle, C, XML, HTML).

 

Offering

*	Opportunity to extend your technical knowledge within the finance
sector.
*	Exposure to other areas of statistical software consulting by
working with customers from pharmaceutcal, CRM, consumer and environmental
sectors.
*	Flexible working environment and working hours.
*	Competitive salary.

 

 


	[[alternative HTML version deleted]]

_______________________________________________
R-sig-finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From rfin.20.phftt at xoxy.net  Sat Nov 26 06:11:27 2005
From: rfin.20.phftt at xoxy.net (Rob Steele)
Date: Sat, 26 Nov 2005 00:11:27 -0500
Subject: [R-sig-finance] Backtest trading strategies
In-Reply-To: <BAY104-F28892D8506C00BF4C33984AF0C0@phx.gbl>
References: <6e8360ad050523101870f2ecd4@mail.gmail.com>
	<BAY104-F28892D8506C00BF4C33984AF0C0@phx.gbl>
Message-ID: <4387EE7F.20401@xoxy.net>

Neuro LeSuperH?ros wrote:
> Hello,
> 
> I understand the utility of MySQL for data storage.  But why is Python 
> essential?  What does it do that R can't do for system 
> creation/calculation?
> 
> Thanks


Python is great for parsing data from wherever you get it and populating 
databases.  MySQL is ideal for the write-once-read-thereafter scenario 
that research implies.  You can use R for the initial data marshaling 
if you'd rather not learn another language but Python seems like a 
better fit for that sort of thing.  It's a scripting language that 
integrates more naturally into its host environment.  For analysis and 
visualization however, R absolutely rules.


From ggrothendieck at gmail.com  Sat Nov 26 06:37:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 26 Nov 2005 00:37:15 -0500
Subject: [R-sig-finance] Backtest trading strategies
In-Reply-To: <4387EE7F.20401@xoxy.net>
References: <6e8360ad050523101870f2ecd4@mail.gmail.com>
	<BAY104-F28892D8506C00BF4C33984AF0C0@phx.gbl>
	<4387EE7F.20401@xoxy.net>
Message-ID: <971536df0511252137l3e177312t9ad62df6c3e3891e@mail.gmail.com>

On 11/26/05, Rob Steele <rfin.20.phftt at xoxy.net> wrote:
> Neuro LeSuperH?ros wrote:
> > Hello,
> >
> > I understand the utility of MySQL for data storage.  But why is Python
> > essential?  What does it do that R can't do for system
> > creation/calculation?
> >
> > Thanks
>
>
> Python is great for parsing data from wherever you get it and populating
> databases.  MySQL is ideal for the write-once-read-thereafter scenario
> that research implies.  You can use R for the initial data marshaling
> if you'd rather not learn another language but Python seems like a
> better fit for that sort of thing.  It's a scripting language that
> integrates more naturally into its host environment.  For analysis and
> visualization however, R absolutely rules.

I don't use MySQL so won't comment on that part but for parsing
data I have found R to have everything I need.  I used to use perl
but now use R exclusively.    R's string manipulation includes
regular expressions and the vector processing often simplifies
string manipulation by eliminating loops over lines or vectors
of strings.

To me its much easier to maintain code if its all in one language and
moving to R has enabled me to replace a bunch of perl, batch files
and other statistical software with R which really helps clean it
all up.  (Actually I still have some Windows batch files, see
http://cran.r-project.org/contrib/extra/batchfiles/, but they are only
for generic configuration utilities and nothing specific to any application.)


From rfin.20.phftt at xoxy.net  Sat Nov 26 08:29:52 2005
From: rfin.20.phftt at xoxy.net (Rob Steele)
Date: Sat, 26 Nov 2005 02:29:52 -0500
Subject: [R-sig-finance] Backtest trading strategies
In-Reply-To: <971536df0511252137l3e177312t9ad62df6c3e3891e@mail.gmail.com>
References: <6e8360ad050523101870f2ecd4@mail.gmail.com>	<BAY104-F28892D8506C00BF4C33984AF0C0@phx.gbl>	<4387EE7F.20401@xoxy.net>
	<971536df0511252137l3e177312t9ad62df6c3e3891e@mail.gmail.com>
Message-ID: <43880EF0.3020005@xoxy.net>

Gabor Grothendieck wrote:
> On 11/26/05, Rob Steele <rfin.20.phftt at xoxy.net> wrote:
> 
>>Neuro LeSuperH?ros wrote:
>>
>>>Hello,
>>>
>>>I understand the utility of MySQL for data storage.  But why is Python
>>>essential?  What does it do that R can't do for system
>>>creation/calculation?
>>>
>>>Thanks
>>
>>
>>Python is great for parsing data from wherever you get it and populating
>>databases.  MySQL is ideal for the write-once-read-thereafter scenario
>>that research implies.  You can use R for the initial data marshaling
>>if you'd rather not learn another language but Python seems like a
>>better fit for that sort of thing.  It's a scripting language that
>>integrates more naturally into its host environment.  For analysis and
>>visualization however, R absolutely rules.
> 
> 
> I don't use MySQL so won't comment on that part but for parsing
> data I have found R to have everything I need.  I used to use perl
> but now use R exclusively.    R's string manipulation includes
> regular expressions and the vector processing often simplifies
> string manipulation by eliminating loops over lines or vectors
> of strings.
> 
> To me its much easier to maintain code if its all in one language and
> moving to R has enabled me to replace a bunch of perl, batch files
> and other statistical software with R which really helps clean it
> all up.  (Actually I still have some Windows batch files, see
> http://cran.r-project.org/contrib/extra/batchfiles/, but they are only
> for generic configuration utilities and nothing specific to any application.)
> 

Yeah, well, Perl.  :)

I take your point though.  Maybe someday R will play more nicely with 
shells and the command line so there will be no reason not to use it 
even for little jobs.

It's not totally bad now.  You can do stuff like:

#! /bin/sh

x=5

tmp=`R --vanilla --quiet << EOF
options(echo = FALSE)
y = $x * 1024 ^ 2
cat('\n', as.character(y), '\n')
q('no')
EOF`

y=`echo $tmp | awk '{print $NF}'`

echo $y


But you see what I mean.


From sourceforge at metrak.com  Sat Nov 26 10:00:14 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Sat, 26 Nov 2005 20:00:14 +1100
Subject: [R-sig-finance] Backtest trading strategies
In-Reply-To: <971536df0511252137l3e177312t9ad62df6c3e3891e@mail.gmail.com>
References: <6e8360ad050523101870f2ecd4@mail.gmail.com>	<BAY104-F28892D8506C00BF4C33984AF0C0@phx.gbl>	<4387EE7F.20401@xoxy.net>
	<971536df0511252137l3e177312t9ad62df6c3e3891e@mail.gmail.com>
Message-ID: <4388241E.3020908@metrak.com>

Gabor Grothendieck wrote:
> On 11/26/05, Rob Steele <rfin.20.phftt at xoxy.net> wrote:
> 
>>Neuro LeSuperH?ros wrote:
>>
>>>Hello,
>>>
>>>I understand the utility of MySQL for data storage.  But why is Python
>>>essential?  What does it do that R can't do for system
>>>creation/calculation?
>>>
>>>Thanks
>>
>>
>>Python is great for parsing data from wherever you get it and populating
>>databases.  MySQL is ideal for the write-once-read-thereafter scenario
>>that research implies.  You can use R for the initial data marshaling
>>if you'd rather not learn another language but Python seems like a
>>better fit for that sort of thing.  It's a scripting language that
>>integrates more naturally into its host environment.  For analysis and
>>visualization however, R absolutely rules.
> 
> 
> I don't use MySQL so won't comment on that part but for parsing
> data I have found R to have everything I need.  I used to use perl
> but now use R exclusively.    R's string manipulation includes
> regular expressions and the vector processing often simplifies
> string manipulation by eliminating loops over lines or vectors
> of strings.
> 
> To me its much easier to maintain code if its all in one language and
> moving to R has enabled me to replace a bunch of perl, batch files
> and other statistical software with R which really helps clean it
> all up.  (Actually I still have some Windows batch files, see
> http://cran.r-project.org/contrib/extra/batchfiles/, but they are only
> for generic configuration utilities and nothing specific to any application.)

Each to their own I guess.  I happen to be much more familiar with 
Python than R and often use it to grab data in various formats which R 
won't read.  I wouldn't dream of using an MSDOS batch file.  As I learn 
more about R, I tend to do more in it but I couldn't imagine myself 
parsing dodgy HTML, for example, with it.


From ggrothendieck at gmail.com  Sat Nov 26 14:13:35 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 26 Nov 2005 08:13:35 -0500
Subject: [R-sig-finance] Backtest trading strategies
In-Reply-To: <4388241E.3020908@metrak.com>
References: <6e8360ad050523101870f2ecd4@mail.gmail.com>
	<BAY104-F28892D8506C00BF4C33984AF0C0@phx.gbl>
	<4387EE7F.20401@xoxy.net>
	<971536df0511252137l3e177312t9ad62df6c3e3891e@mail.gmail.com>
	<4388241E.3020908@metrak.com>
Message-ID: <971536df0511260513j4395cfe7q166050f2340ef84d@mail.gmail.com>

On 11/26/05, paul sorenson <sourceforge at metrak.com> wrote:
> Gabor Grothendieck wrote:
> > On 11/26/05, Rob Steele <rfin.20.phftt at xoxy.net> wrote:
> >
> >>Neuro LeSuperH?ros wrote:
> >>
> >>>Hello,
> >>>
> >>>I understand the utility of MySQL for data storage.  But why is Python
> >>>essential?  What does it do that R can't do for system
> >>>creation/calculation?
> >>>
> >>>Thanks
> >>
> >>
> >>Python is great for parsing data from wherever you get it and populating
> >>databases.  MySQL is ideal for the write-once-read-thereafter scenario
> >>that research implies.  You can use R for the initial data marshaling
> >>if you'd rather not learn another language but Python seems like a
> >>better fit for that sort of thing.  It's a scripting language that
> >>integrates more naturally into its host environment.  For analysis and
> >>visualization however, R absolutely rules.
> >
> >
> > I don't use MySQL so won't comment on that part but for parsing
> > data I have found R to have everything I need.  I used to use perl
> > but now use R exclusively.    R's string manipulation includes
> > regular expressions and the vector processing often simplifies
> > string manipulation by eliminating loops over lines or vectors
> > of strings.
> >
> > To me its much easier to maintain code if its all in one language and
> > moving to R has enabled me to replace a bunch of perl, batch files
> > and other statistical software with R which really helps clean it
> > all up.  (Actually I still have some Windows batch files, see
> > http://cran.r-project.org/contrib/extra/batchfiles/, but they are only
> > for generic configuration utilities and nothing specific to any application.)
>
> Each to their own I guess.  I happen to be much more familiar with
> Python than R and often use it to grab data in various formats which R
> won't read.  I wouldn't dream of using an MSDOS batch file.  As I learn
> more about R, I tend to do more in it but I couldn't imagine myself
> parsing dodgy HTML, for example, with it.

Actually I use R for parsing HTML and for parsing XML too.  I do
agree by Rob that it would be nice if R worked better with shells
and also wish I could write small self contained executables in R
like one can in tcl and Python.


From atp at piskorski.com  Sat Nov 26 16:16:12 2005
From: atp at piskorski.com (Andrew Piskorski)
Date: Sat, 26 Nov 2005 10:16:12 -0500
Subject: [R-sig-finance] MySQL and other RDBMSs
In-Reply-To: <4387EE7F.20401@xoxy.net>
References: <4387EE7F.20401@xoxy.net>
Message-ID: <20051126151612.GA44704@tehun.pair.com>

On Sat, Nov 26, 2005 at 12:11:27AM -0500, Rob Steele wrote:

> MySQL is ideal for the write-once-read-thereafter scenario 
> that research implies.

True (that's what MySQL was originally designed for), except that it's
SQL dialect is pretty limited, and the database has various
misfeatures, e.g.:

  http://sql-info.de/mysql/gotchas.html

The limited SQL support might or might not matter to you, depending on
just what sort of data you're loading in and how complicated your data
model and queries are, but it's certainly something you should keep in
mind when picking a RDBMS.  Maybe that's been improved in MySQL 5.0
(which was just released a month or so ago), I don't know.

If I didn't want to use PostgreSQL or Oracle, I'd be tempted to try a
lightweight embedded database like SQLite or Metakit:

  http://www.sqlite.org/
  http://www.equi4.com/metakit.html

PostgreSQL's SQL tends to be more powerful than MySQL's, but even
PostgreSQL doesn't yet support the (very useful) SQL:2003 windowing
functions (aka, the "OLAP" features T611 and T612 in the spec).
Oracle and DB2 both do.

  http://www.wintercorp.com/rwintercolumns/SQL_99snewolapfunctions.html
  http://www.ncb.ernet.in/education/modules/dbms/SQL99/OLAP-99-154r2.pdf
  http://www.wiscorp.com/sql/SQL2003Features.pdf
  http://troels.arvin.dk/db/rdbms/#select-limit-offset
  http://www.postgresql.org/docs/8.0/interactive/features.html
  http://en.wikipedia.org/wiki/SQL
  http://www.sigmod.org/sigmod/record/issues/0403/E.JimAndrew-standard.pdf
  http://www.oracle.com/oramag/oracle/01-jul/o41industry.html

Of course, with PostgreSQL you can also run R (or a whole bunch of
other languages) inside the RDBMS if you want:

  http://www.joeconway.com/plr/
  http://gborg.postgresql.org/project/plr/projdisplay.php
  http://archives.postgresql.org/pgsql-hackers/2003-02/msg00142.php
  http://www.omegahat.org/RSPostgres/

-- 
Andrew Piskorski <atp at piskorski.com>
http://www.piskorski.com/


From sourceforge at metrak.com  Sun Nov 27 08:22:56 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Sun, 27 Nov 2005 18:22:56 +1100
Subject: [R-sig-finance] bayesian signal classifier
Message-ID: <43895ED0.1070201@metrak.com>

The issue I am curious about is how to classify various signals (eg 
price, volume, MACD, etc) into to buy, sell, or hold?

Assuming I could "tokenize" various attributes of signals (value, 1st, 
2nd and 3rd derivatives, crossing, etc), would it be feasible to take 
these as inputs to a (trained) classifier which then outputs some number 
between 0 and 1 representing buy, hold, sell?  The analogy I am thinking 
of is a Bayesian spam classifier.

My background is in engineering and I have only basic statistics 
knowledge.  I have been using R for a couple of years now mostly for 
graphic output.  I have a reasonable grasp of the language but I'm not 
strong on the underlying theory of the statistical functions.

R has a number of packages which deal with Bayesian statistics but I 
don't have the knowledge to join the dots from there to a classifier.

Any pointers would be most welcome.

cheers


From kriskumar at earthlink.net  Mon Nov 28 02:21:14 2005
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Sun, 27 Nov 2005 20:21:14 -0500
Subject: [R-sig-finance] bayesian signal classifier
In-Reply-To: <43895ED0.1070201@metrak.com>
References: <43895ED0.1070201@metrak.com>
Message-ID: <438A5B8A.4030405@earthlink.net>

Assume you have two decisions to make buy or sell. Then if you assume 
the classifications errors have the same cost (this  can be
modified suitably for cost) then a naive bayes classifier that minimizes 
the error rate is simply

prior * likelihood

So assuming that you have a prior computed in-sample from the timeseries 
you compute for each date the posterior decision buy or sell based
on the returns+ the signals (i.e. the feature vector).

Date            Returns        signal 1           signal 2            ...
---------------------------------------------------------
01/01/1990                   
02/01/1990  return2       buy               sell         ..........
03/01/1990  return3       buy               buy         ..............   
.
.


I am sure there are folks on this list who can throw more light on this 
R has a lot of classification related machinery that can be put to use
that will do better than this Bayes classifier. You may want to refer to 
these two books.

http://www.amazon.com/exec/obidos/ASIN/0122698517/kriskumar-20/104-8074544-7680720
http://www.amazon.com/exec/obidos/ASIN/0471056693/kriskumar-20/104-8074544-7680720

And finally the V&R yellow book describes some of the classifiers that 
are available in MASS.


Kris



paul sorenson wrote:

>The issue I am curious about is how to classify various signals (eg 
>price, volume, MACD, etc) into to buy, sell, or hold?
>
>Assuming I could "tokenize" various attributes of signals (value, 1st, 
>2nd and 3rd derivatives, crossing, etc), would it be feasible to take 
>these as inputs to a (trained) classifier which then outputs some number 
>between 0 and 1 representing buy, hold, sell?  The analogy I am thinking 
>of is a Bayesian spam classifier.
>
>My background is in engineering and I have only basic statistics 
>knowledge.  I have been using R for a couple of years now mostly for 
>graphic output.  I have a reasonable grasp of the language but I'm not 
>strong on the underlying theory of the statistical functions.
>
>R has a number of packages which deal with Bayesian statistics but I 
>don't have the knowledge to join the dots from there to a classifier.
>
>Any pointers would be most welcome.
>
>cheers
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From steve.miller at jhu.edu  Mon Nov 28 15:27:21 2005
From: steve.miller at jhu.edu (Steve Miller)
Date: Mon, 28 Nov 2005 08:27:21 -0600
Subject: [R-sig-finance] Backtest trading strategies
In-Reply-To: <971536df0511260513j4395cfe7q166050f2340ef84d@mail.gmail.com>
Message-ID: <200511281427.jASERkPI003572@hypatia.math.ethz.ch>

My company delivers broad business intelligence solutions on foundations of
data warehouses and marts that grow to hundreds of gigabytes. It is
therefore critical that we optimize data storage and the ETL -- extract,
transform, and load -- processes for very large data. Our minimal open
source analytics solution consists of: MySql/Postgres (or Oracle, et. al. if
standard) repositories that house the sourcing data, assuring one version of
the truth; Perl/Python for "munging" potentially very large, complicated,
and messy input files, consolidating data across disparate sources,
performing array and date calculations on millions of records,
inserting/updating the databases, reporting, and sourcing data sets for
input to analytics; and R for graphics, statistics, and analytical
calculations. While R is a very powerful and rich language, we do not see
its strength in parsing ugly and error-prone data files, nor do we find its
interpreted speed adequate for very large data management. Once the data
stores are built and validated, we turn them over to users of R and other BI
tools such as Pentaho. We are very encouraged by both the acceptance of this
approach and R that we're starting to experience in the commercial world.

Steve Miller 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Gabor
Grothendieck
Sent: Saturday, November 26, 2005 7:14 AM
To: paul sorenson
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-sig-finance] Backtest trading strategies

On 11/26/05, paul sorenson <sourceforge at metrak.com> wrote:
> Gabor Grothendieck wrote:
> > On 11/26/05, Rob Steele <rfin.20.phftt at xoxy.net> wrote:
> >
> >>Neuro LeSuperH?ros wrote:
> >>
> >>>Hello,
> >>>
> >>>I understand the utility of MySQL for data storage.  But why is Python
> >>>essential?  What does it do that R can't do for system
> >>>creation/calculation?
> >>>
> >>>Thanks
> >>
> >>
> >>Python is great for parsing data from wherever you get it and populating
> >>databases.  MySQL is ideal for the write-once-read-thereafter scenario
> >>that research implies.  You can use R for the initial data marshaling
> >>if you'd rather not learn another language but Python seems like a
> >>better fit for that sort of thing.  It's a scripting language that
> >>integrates more naturally into its host environment.  For analysis and
> >>visualization however, R absolutely rules.
> >
> >
> > I don't use MySQL so won't comment on that part but for parsing
> > data I have found R to have everything I need.  I used to use perl
> > but now use R exclusively.    R's string manipulation includes
> > regular expressions and the vector processing often simplifies
> > string manipulation by eliminating loops over lines or vectors
> > of strings.
> >
> > To me its much easier to maintain code if its all in one language and
> > moving to R has enabled me to replace a bunch of perl, batch files
> > and other statistical software with R which really helps clean it
> > all up.  (Actually I still have some Windows batch files, see
> > http://cran.r-project.org/contrib/extra/batchfiles/, but they are only
> > for generic configuration utilities and nothing specific to any
application.)
>
> Each to their own I guess.  I happen to be much more familiar with
> Python than R and often use it to grab data in various formats which R
> won't read.  I wouldn't dream of using an MSDOS batch file.  As I learn
> more about R, I tend to do more in it but I couldn't imagine myself
> parsing dodgy HTML, for example, with it.

Actually I use R for parsing HTML and for parsing XML too.  I do
agree by Rob that it would be nice if R worked better with shells
and also wish I could write small self contained executables in R
like one can in tcl and Python.

_______________________________________________
R-sig-finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From pvirketis at hbk.com  Mon Nov 28 16:46:55 2005
From: pvirketis at hbk.com (Pijus Virketis)
Date: Mon, 28 Nov 2005 10:46:55 -0500
Subject: [R-sig-finance] Backtest trading strategies
Message-ID: <4DDAC3B7F3996A4291CBF8B0CDED8A7C010D570D@NYCMAIL1.hbk.com>

Much as I love R, I had occasion to contemplate its limitations vis-?-vis Python this weekend, when I had to scrape brutally malformed HTML and actually found it to be fairly painless thanks to BeautifulSoup (http://www.crummy.com/software/BeautifulSoup/). Reading through the neat code (a mere 1000 lines with comments), it seemed pretty clear that the R equivalent would be much more cumbersome (just the OO aspect would be hard to replicate, not to mention the libraries BeautifulSoup can count on). But thanks to Rpy (http://rpy.sourceforge.net/), one can have the best of all worlds! Use Python for general programming tasks and take advantage of existing libraries, use R for data analysis and visualisation, and have everything on the same page for maintainability and clarity.

Cheers, 

Pijus 

> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch 
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of 
> Gabor Grothendieck
> Sent: Saturday, November 26, 2005 8:14 AM
> To: paul sorenson
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-sig-finance] Backtest trading strategies
> 
> On 11/26/05, paul sorenson <sourceforge at metrak.com> wrote:
> > Gabor Grothendieck wrote:
> > > On 11/26/05, Rob Steele <rfin.20.phftt at xoxy.net> wrote:
> > >
> > >>Neuro LeSuperH?ros wrote:
> > >>
> > >>>Hello,
> > >>>
> > >>>I understand the utility of MySQL for data storage.  But why is 
> > >>>Python essential?  What does it do that R can't do for system 
> > >>>creation/calculation?
> > >>>
> > >>>Thanks
> > >>
> > >>
> > >>Python is great for parsing data from wherever you get it and 
> > >>populating databases.  MySQL is ideal for the 
> > >>write-once-read-thereafter scenario that research 
> implies.  You can 
> > >>use R for the initial data marshaling if you'd rather not learn 
> > >>another language but Python seems like a better fit for 
> that sort of 
> > >>thing.  It's a scripting language that integrates more naturally 
> > >>into its host environment.  For analysis and 
> visualization however, R absolutely rules.
> > >
> > >
> > > I don't use MySQL so won't comment on that part but for 
> parsing data 
> > > I have found R to have everything I need.  I used to use perl
> > > but now use R exclusively.    R's string manipulation includes
> > > regular expressions and the vector processing often simplifies 
> > > string manipulation by eliminating loops over lines or vectors of 
> > > strings.
> > >
> > > To me its much easier to maintain code if its all in one language 
> > > and moving to R has enabled me to replace a bunch of perl, batch 
> > > files and other statistical software with R which really 
> helps clean 
> > > it all up.  (Actually I still have some Windows batch files, see 
> > > http://cran.r-project.org/contrib/extra/batchfiles/, but they are 
> > > only for generic configuration utilities and nothing 
> specific to any 
> > > application.)
> >
> > Each to their own I guess.  I happen to be much more familiar with 
> > Python than R and often use it to grab data in various 
> formats which R 
> > won't read.  I wouldn't dream of using an MSDOS batch file.  As I 
> > learn more about R, I tend to do more in it but I couldn't imagine 
> > myself parsing dodgy HTML, for example, with it.
> 
> Actually I use R for parsing HTML and for parsing XML too.  I 
> do agree by Rob that it would be nice if R worked better with 
> shells and also wish I could write small self contained 
> executables in R like one can in tcl and Python.
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> 
>


From ggrothendieck at gmail.com  Mon Nov 28 17:47:34 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 28 Nov 2005 11:47:34 -0500
Subject: [R-sig-finance] Backtest trading strategies
In-Reply-To: <4DDAC3B7F3996A4291CBF8B0CDED8A7C010D570D@NYCMAIL1.hbk.com>
References: <4DDAC3B7F3996A4291CBF8B0CDED8A7C010D570D@NYCMAIL1.hbk.com>
Message-ID: <971536df0511280847o1e02b8e0i66e783694ec96028@mail.gmail.com>

On 11/28/05, Pijus Virketis <pvirketis at hbk.com> wrote:
> Much as I love R, I had occasion to contemplate its limitations vis-?-vis Python this weekend, when I had to scrape brutally malformed HTML and actually found it to be fairly painless thanks to BeautifulSoup (http://www.crummy.com/software/BeautifulSoup/). Reading through the neat code (a mere 1000 lines with comments), it seemed pretty clear that the R equivalent would be much more cumbersome (just the OO aspect would be hard to replicate,

I have not reviewed Beautiful Soup but regarding the comments on OO
there are four OO models available in R (S3, S4, the proto package and
the oo.R package) so I doubt there is much that can't be readily done in R
in the way of OO.

> not to mention the libraries BeautifulSoup can count on). But thanks to Rpy (http://rpy.sourceforge.net/), one can have the best of all worlds! Use Python for general programming tasks and take advantage of existing libraries, use R for data analysis and visualisation, and have everything on the same page for maintainability and clarity.

>From a maintainability viewpoint having everything in one language would
surely be preferable.

>
> > -----Original Message-----
> > From: r-sig-finance-bounces at stat.math.ethz.ch
> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
> > Gabor Grothendieck
> > Sent: Saturday, November 26, 2005 8:14 AM
> > To: paul sorenson
> > Cc: r-sig-finance at stat.math.ethz.ch
> > Subject: Re: [R-sig-finance] Backtest trading strategies
> >
> > On 11/26/05, paul sorenson <sourceforge at metrak.com> wrote:
> > > Gabor Grothendieck wrote:
> > > > On 11/26/05, Rob Steele <rfin.20.phftt at xoxy.net> wrote:
> > > >
> > > >>Neuro LeSuperH?ros wrote:
> > > >>
> > > >>>Hello,
> > > >>>
> > > >>>I understand the utility of MySQL for data storage.  But why is
> > > >>>Python essential?  What does it do that R can't do for system
> > > >>>creation/calculation?
> > > >>>
> > > >>>Thanks
> > > >>
> > > >>
> > > >>Python is great for parsing data from wherever you get it and
> > > >>populating databases.  MySQL is ideal for the
> > > >>write-once-read-thereafter scenario that research
> > implies.  You can
> > > >>use R for the initial data marshaling if you'd rather not learn
> > > >>another language but Python seems like a better fit for
> > that sort of
> > > >>thing.  It's a scripting language that integrates more naturally
> > > >>into its host environment.  For analysis and
> > visualization however, R absolutely rules.
> > > >
> > > >
> > > > I don't use MySQL so won't comment on that part but for
> > parsing data
> > > > I have found R to have everything I need.  I used to use perl
> > > > but now use R exclusively.    R's string manipulation includes
> > > > regular expressions and the vector processing often simplifies
> > > > string manipulation by eliminating loops over lines or vectors of
> > > > strings.
> > > >
> > > > To me its much easier to maintain code if its all in one language
> > > > and moving to R has enabled me to replace a bunch of perl, batch
> > > > files and other statistical software with R which really
> > helps clean
> > > > it all up.  (Actually I still have some Windows batch files, see
> > > > http://cran.r-project.org/contrib/extra/batchfiles/, but they are
> > > > only for generic configuration utilities and nothing
> > specific to any
> > > > application.)
> > >
> > > Each to their own I guess.  I happen to be much more familiar with
> > > Python than R and often use it to grab data in various
> > formats which R
> > > won't read.  I wouldn't dream of using an MSDOS batch file.  As I
> > > learn more about R, I tend to do more in it but I couldn't imagine
> > > myself parsing dodgy HTML, for example, with it.
> >
> > Actually I use R for parsing HTML and for parsing XML too.  I
> > do agree by Rob that it would be nice if R worked better with
> > shells and also wish I could write small self contained
> > executables in R like one can in tcl and Python.
> >
> > _______________________________________________
> > R-sig-finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
> >
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From gyollin at insightful.com  Mon Nov 28 19:24:19 2005
From: gyollin at insightful.com (Guy Yollin)
Date: Mon, 28 Nov 2005 10:24:19 -0800
Subject: [R-sig-finance]  bayesian signal classifier
Message-ID: <F90D76D3C7CC6A41B6E2E6F88CA8252E1CBFFB@se2kexch01.insightful.com>

Paul,

I did some research in this area back in grad school and tested a variety of
classical technical analysis indicators and their ability to forecast
in-the-market or out-of-the-market periods based on classification.  Its an
interesting topic.

Implementing a basic classification system is quite straight-forward.  I
would suggest looking at the following sections of V&R (4th edition):
9.1-9.3 Classification Trees
7.2 GLM with binomial data
8.10 Neural Networks

If you're interested in our research report, drop me an email and I'll be
happy to forward it to you.

-- Guy



Guy Yollin
Senior Financial Engineer
Insightful Corporation
www.insightful.com
gyollin at insightful.com


paul sorenson wrote:

>The issue I am curious about is how to classify various signals (eg 
>price, volume, MACD, etc) into to buy, sell, or hold?
>
>Assuming I could "tokenize" various attributes of signals (value, 1st, 
>2nd and 3rd derivatives, crossing, etc), would it be feasible to take 
>these as inputs to a (trained) classifier which then outputs some number 
>between 0 and 1 representing buy, hold, sell?  The analogy I am thinking 
>of is a Bayesian spam classifier.
>
>My background is in engineering and I have only basic statistics 
>knowledge.  I have been using R for a couple of years now mostly for 
>graphic output.  I have a reasonable grasp of the language but I'm not 
>strong on the underlying theory of the statistical functions.
>
>R has a number of packages which deal with Bayesian statistics but I 
>don't have the knowledge to join the dots from there to a classifier.
>
>Any pointers would be most welcome.
>
>cheers
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From sourceforge at metrak.com  Mon Nov 28 22:18:47 2005
From: sourceforge at metrak.com (paul sorenson)
Date: Tue, 29 Nov 2005 08:18:47 +1100
Subject: [R-sig-finance] bayesian signal classifier
In-Reply-To: <F90D76D3C7CC6A41B6E2E6F88CA8252E1CBFFB@se2kexch01.insightful.com>
References: <F90D76D3C7CC6A41B6E2E6F88CA8252E1CBFFB@se2kexch01.insightful.com>
Message-ID: <438B7437.6000509@metrak.com>

Guy,

I would be interested in the paper thanks.  Unfortunately my level of 
expertise is not high in these matters.

I may have just misunderstood yours and Krishna's response, the kind of 
paradigm I am thinking is:

	- User selects signals he/she wants to monitor.

	- When the user makes a buy/sell decision, the classifier then looks at 
the parameters of those signals and classifies the conditions for that 
decision.

	- The user continues to train the classifier in this way, analogously 
to training a spam filter.

	- The classifier then can start emitting buy/sell signals based on the 
training.  Ie it is personalized to that users previous choices.

I only mentioned Bayesian methods because the most effective spam 
filtering I have used is apparently based on that method 
(http://spambayes.org).

cheers

Guy Yollin wrote:
> Paul,
> 
> I did some research in this area back in grad school and tested a variety of
> classical technical analysis indicators and their ability to forecast
> in-the-market or out-of-the-market periods based on classification.  Its an
> interesting topic.
> 
> Implementing a basic classification system is quite straight-forward.  I
> would suggest looking at the following sections of V&R (4th edition):
> 9.1-9.3 Classification Trees
> 7.2 GLM with binomial data
> 8.10 Neural Networks
> 
> If you're interested in our research report, drop me an email and I'll be
> happy to forward it to you.
> 
> -- Guy
> 
> 
> 
> Guy Yollin
> Senior Financial Engineer
> Insightful Corporation
> www.insightful.com
> gyollin at insightful.com
> 
> 
> paul sorenson wrote:
> 
> 
>>The issue I am curious about is how to classify various signals (eg 
>>price, volume, MACD, etc) into to buy, sell, or hold?
>>
>>Assuming I could "tokenize" various attributes of signals (value, 1st, 
>>2nd and 3rd derivatives, crossing, etc), would it be feasible to take 
>>these as inputs to a (trained) classifier which then outputs some number 
>>between 0 and 1 representing buy, hold, sell?  The analogy I am thinking 
>>of is a Bayesian spam classifier.
>>
>>My background is in engineering and I have only basic statistics 
>>knowledge.  I have been using R for a couple of years now mostly for 
>>graphic output.  I have a reasonable grasp of the language but I'm not 
>>strong on the underlying theory of the statistical functions.
>>
>>R has a number of packages which deal with Bayesian statistics but I 
>>don't have the knowledge to join the dots from there to a classifier.
>>
>>Any pointers would be most welcome.
>>
>>cheers
>>
>>_______________________________________________
>>R-sig-finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
> 
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From kriskumar at earthlink.net  Tue Nov 29 06:07:52 2005
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Tue, 29 Nov 2005 00:07:52 -0500
Subject: [R-sig-finance] bayesian signal classifier
In-Reply-To: <438B7437.6000509@metrak.com>
References: <F90D76D3C7CC6A41B6E2E6F88CA8252E1CBFFB@se2kexch01.insightful.com>
	<438B7437.6000509@metrak.com>
Message-ID: <438BE228.1010009@earthlink.net>

Here is a simple example of how this could be done. This is using the 
mclust package.
(for more on mclust see here 
http://staff.washington.edu/fraley/mclust/tr415R.pdf)

I have attached a test file there are 11 columns the first 10 columns 
are various features(signals)
and the last column -11  is  the best thing to do based on 20/20 
hindsight...
 i.e. buy or sell indicator

 > require("mclust")
 > findata<-read.table("findata.txt",header=F)

# Create a matrix out of the data

> finMatrix <- as.matrix(findata[,1:10])

# identify column 11 as the classes and do the classifcation with default values.

> finClass <- findata[,11]
> finMclust <- Mclust(finMatrix,maxG=2)
> plot(finMclust,finMatrix)

Where we trained on the entire data set and finMclust$classification gives
the decision made by the classifier.


Now if you want to train on a subset say all the odd rows 
[or one can alternatively cross validate with ?sample or even bootstrap a training data set.]

>odd <- seq(from=1, to=nrow(findata), by=2)
>even <- seq(from=2, to=nrow(findata), by=2)
>round(cv1EMtrain(data = findata[odd,-11], labels = findata[odd,11]),3)

This will show that the VVI model would be selected based on the training data(all the odd rows)

> vviModd <- mstepVVI(data=findata[odd,-11], z=unmap(findata[odd,11]))
> vviZ <- do.call("estepVVI", c(vviModd, list(data=findata[,-11])))$z
> classError(map(vviZ[odd,]), findata[odd,11])

How do we do on the test data?

 >classError(map(vviZ[even,]), findata[even,11])

===  0.04081633

Hmmm. so a classification error of 4% on the test data....whew!  (maybe 
i am missing something..)

Later,
Krishna






paul sorenson wrote:

>Guy,
>
>I would be interested in the paper thanks.  Unfortunately my level of 
>expertise is not high in these matters.
>
>I may have just misunderstood yours and Krishna's response, the kind of 
>paradigm I am thinking is:
>
>	- User selects signals he/she wants to monitor.
>
>	- When the user makes a buy/sell decision, the classifier then looks at 
>the parameters of those signals and classifies the conditions for that 
>decision.
>
>	- The user continues to train the classifier in this way, analogously 
>to training a spam filter.
>
>	- The classifier then can start emitting buy/sell signals based on the 
>training.  Ie it is personalized to that users previous choices.
>
>I only mentioned Bayesian methods because the most effective spam 
>filtering I have used is apparently based on that method 
>(http://spambayes.org).
>
>cheers
>  
>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: findata.txt
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20051129/7d5944ba/findata.txt

From tariq.khan at gmail.com  Thu Dec  1 13:12:24 2005
From: tariq.khan at gmail.com (=?ISO-8859-1?Q?=A8Tariq_Khan?=)
Date: Thu, 1 Dec 2005 12:12:24 +0000
Subject: [R-sig-finance] Kalman Smoothing - time-variant parameters (sspir)
Message-ID: <2310043c0512010412j5298187cv4599b4acc056abfc@mail.gmail.com>

Dear R-brains,

I'm rather new to state-space models and would benefit from the extra
confidence in using the excellent package sspir.

In a one-factor model, If I am trying to do a simple regression where
I assume the intercept is constant and the 'Beta' is changing, how do
I do that? How do i Initialize the filter (i.e. what is appropriate to
set m0, and C0 for the example below)?

The model I want is: y = alpha + beta + err1; beta_(t+1) = beta_t + err2

I thought of the following:
library(mvtnorm) # (1)
library(sspir)
# Let's get some data so we can all try this at home
dfrm <- data.frame(
                   y =
c(0.02,0.04,-0.03,0.02,0,0.01,0.04,0.03,-0.01,0.04,-0.01,0.05,0.04,
                          
0.03,0.01,-0.01,-0.01,-0.03,0.02,-0.04,-0.05,-0.02,-0.04,0,0.02,0,
                         
-0.01,-0.01,0.01,0.09,0.03,0.03,0.05,0.04,-0.01,0.05,0.03,0.01,
                          0.04,0.01,-0.01,-0.02,-0.01,-0.01,
0.06,0.03,0.02,0.03,0.03,0.04,
                          0.03,0.04,-0.02,-0.03,0.04,0.03,0.05,0.02,0.03,-0.1),
                   x = c(-0.03,-0.01,0.07,-0.03,-0.07,0.05,0.02,-0.05,-0.04,
                           -0.02,-0.19,0.07,0.09,0.01,0.01,0,0.05,0,-0.02,-0.09,
                           -0.12,-0.01,-0.13,0.04,0.04,-0.07,-0.05,-0.03,
                           -0.01,0.11,0.06,0.03,0.06,0.06,-0.01,0.07,0.01,
                           
0,0.07,0.04,-0.02,0,-0.03,0.04,-0.04,-0.01,0.03,0.02,0.05,0.04,
                            0.05,0.03,0,-0.04,0.05,0.05,0.06,0.02,0.04,-0.06)
)
ss <- ssm(y ~ tvar(x), time = 1:nrow(dfrm), family=gaussian(link="identity"),
               data=dfrm)
smooth.params <- smoother(kfilter(ss$ss))$m

(1) I read in http://ww.math.aau.dk/~mbn/Teaching/MarkovE05/Lecture3.pdf
that this is requred as there is a bug in sspir.

To what should I set ss$ss$m0 and ss$ss$C0? (I did notice that
smoother() replaces these, but it still matters what I initialize it
to in the first place)

Many thanks!

Tariq Khan


From fparlamis at mac.com  Fri Dec  2 06:52:34 2005
From: fparlamis at mac.com (Parlamis Franklin)
Date: Thu, 1 Dec 2005 19:52:34 -1000
Subject: [R-sig-finance] rMetrics bug
Message-ID: <C619AB04-7A0F-473C-9F03-AE2A8FBFC701@mac.com>

In method rev.timeDate

 > rev(timeSequence(len=20))
[1] "Honolulu"
[1] [2004-01-09] [2004-01-08] [2004-01-07] [2004-01-06] [2004-01-05]  
[2004-01-04] [2004-01-03] [2004-01-02]
[9] [2004-01-01]



Note the following:

 > rev(timeSequence(len=20))@Dim
[1] 20

 > rev(timeSequence(len=20))[15]
[1] "Honolulu"
[1] [NA]

 > rev(timeSequence(len=5))
[1] "Honolulu"
[1] [NA]         [NA]         [NA]         [NA]         [2004-01-05]  
[2004-01-04] [2004-01-03] [2004-01-02]
[9] [2004-01-01]



Class "POSIXlt" is a named list of exactly nine vectors.  hmmmmmm . . .


From wuertz at itp.phys.ethz.ch  Fri Dec  2 09:52:37 2005
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Fri, 02 Dec 2005 08:52:37 +0000
Subject: [R-sig-finance] rMetrics bug
In-Reply-To: <C619AB04-7A0F-473C-9F03-AE2A8FBFC701@mac.com>
References: <C619AB04-7A0F-473C-9F03-AE2A8FBFC701@mac.com>
Message-ID: <43900B55.1080002@itp.phys.ethz.ch>



# FIX:

rev.timeDate =
function (x)
{
    if (Sys.getenv("TZ")[[1]] != "GMT")
        warning("Set timezone to GMT!")
    if (!inherits(x, "timeDate"))
        stop("Wrong class type")
    # Replace:
    # x at Data = rev(x at Data)
    # with
    x at Data = x at Data[length(x at Data[[1]]):1]
    x
}

# Try:

rev(timeSequence(len=20))

[1] "Zurich"
 [1] [2004-01-20] [2004-01-19] [2004-01-18] [2004-01-17] [2004-01-16]
 [6] [2004-01-15] [2004-01-14] [2004-01-13] [2004-01-12] [2004-01-11]
[11] [2004-01-10] [2004-01-09] [2004-01-08] [2004-01-07] [2004-01-06]
[16] [2004-01-05] [2004-01-04] [2004-01-03] [2004-01-02] [2004-01-01]


Best regards DW



Parlamis Franklin wrote:

>In method rev.timeDate
>
> > rev(timeSequence(len=20))
>[1] "Honolulu"
>[1] [2004-01-09] [2004-01-08] [2004-01-07] [2004-01-06] [2004-01-05]  
>[2004-01-04] [2004-01-03] [2004-01-02]
>[9] [2004-01-01]
>
>
>
>Note the following:
>
> > rev(timeSequence(len=20))@Dim
>[1] 20
>
> > rev(timeSequence(len=20))[15]
>[1] "Honolulu"
>[1] [NA]
>
> > rev(timeSequence(len=5))
>[1] "Honolulu"
>[1] [NA]         [NA]         [NA]         [NA]         [2004-01-05]  
>[2004-01-04] [2004-01-03] [2004-01-02]
>[9] [2004-01-01]
>
>
>
>Class "POSIXlt" is a named list of exactly nine vectors.  hmmmmmm . . .
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From Jordi.Molins at drkw.com  Fri Dec  2 14:15:13 2005
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Fri, 2 Dec 2005 14:15:13 +0100
Subject: [R-sig-finance] R and exchanges
Message-ID: <C5A76BA0CA4D734CA725124C4D6397AC03219493@ibfftce502.de.ad.drkw.net>

I sent a question to the general Help. I received an answer from Patrick
Burns, and he suggested me to post it here. The question is how to use batch
files that call R in an effective way. More:

What I need is to interact with other applications(a connection to
exchanges, like Eurex, Liffe, CBOT and eventually to something like ESB in
FX). So, if I get a price from the exchanges, I want that R makes a
calculation. But to get R connected to the exchanges is complex, so what I
have thought is to have an external program that connects to the exchanges,
then gets the prices and saves them to a txt file, then R is opened, the txt
file is read, the calculations are performed, and the results are saved into
another txt file. Then, another application reads the new txt file and sends
a message to the exchanges.

This is what I want to do.

Does anybody have some ideas about it?

Thank you!

Jordi



--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}


From edd at debian.org  Fri Dec  2 14:33:49 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 2 Dec 2005 07:33:49 -0600
Subject: [R-sig-finance] R and exchanges
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC03219493@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC03219493@ibfftce502.de.ad.drkw.net>
Message-ID: <17296.19773.479697.424779@basebud.nulle.part>


On 2 December 2005 at 14:15, Molins, Jordi wrote:
| I sent a question to the general Help. I received an answer from Patrick
| Burns, and he suggested me to post it here. The question is how to use batch

Umm, why? Nothing finance-specific here.

| files that call R in an effective way. More:
| 
| What I need is to interact with other applications(a connection to
| exchanges, like Eurex, Liffe, CBOT and eventually to something like ESB in
| FX). So, if I get a price from the exchanges, I want that R makes a
| calculation. But to get R connected to the exchanges is complex, so what I
| have thought is to have an external program that connects to the exchanges,
| then gets the prices and saves them to a txt file, then R is opened, the txt
| file is read, the calculations are performed, and the results are saved into
| another txt file. Then, another application reads the new txt file and sends
| a message to the exchanges.
| 
| This is what I want to do.
| 
| Does anybody have some ideas about it?

Sure. Control R from another app. Python is popular, and there is RPy to
call/control R from Python. There is also, as I recall, a Perl package to do
something similar.

Or you just write everything in R as R has basic tcp/ip networking built in.

Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'


From ggrothendieck at gmail.com  Fri Dec  2 14:46:06 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Dec 2005 08:46:06 -0500
Subject: [R-sig-finance] R and exchanges
In-Reply-To: <C5A76BA0CA4D734CA725124C4D6397AC03219493@ibfftce502.de.ad.drkw.net>
References: <C5A76BA0CA4D734CA725124C4D6397AC03219493@ibfftce502.de.ad.drkw.net>
Message-ID: <971536df0512020546v2b34de54udcb6a2728b65a286@mail.gmail.com>

See ?socketConnection or if they supply a COM object see the
RDCOMClient or rcom packages.  In that case there is no need for
anything beyond R.

On 12/2/05, Molins, Jordi <Jordi.Molins at drkw.com> wrote:
> I sent a question to the general Help. I received an answer from Patrick
> Burns, and he suggested me to post it here. The question is how to use batch
> files that call R in an effective way. More:
>
> What I need is to interact with other applications(a connection to
> exchanges, like Eurex, Liffe, CBOT and eventually to something like ESB in
> FX). So, if I get a price from the exchanges, I want that R makes a
> calculation. But to get R connected to the exchanges is complex, so what I
> have thought is to have an external program that connects to the exchanges,
> then gets the prices and saves them to a txt file, then R is opened, the txt
> file is read, the calculations are performed, and the results are saved into
> another txt file. Then, another application reads the new txt file and sends
> a message to the exchanges.
>
> This is what I want to do.
>
> Does anybody have some ideas about it?
>
> Thank you!
>
> Jordi
>
>
>
> --------------------------------------------------------------------------------
> The information contained herein is confidential and is inte...{{dropped}}
>
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From Jordi.Molins at drkw.com  Fri Dec  2 15:09:55 2005
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Fri, 2 Dec 2005 15:09:55 +0100
Subject: [R-sig-finance] R and exchanges
Message-ID: <C5A76BA0CA4D734CA725124C4D6397AC03219498@ibfftce502.de.ad.drkw.net>

Of course, I have access to several databases. The data from the exchange is
a single price, so it does not seem necessary to me to save this price into
a database.

Thank you to all replies I have got. If anybody has something to add, please
feel free!

Jordi


> -----Original Message-----
> From: bogdan romocea [mailto:br44114 at gmail.com]
> Sent: 02 December 2005 14:59
> To: Jordi.Molins at drkw.com
> Cc: r-sig-finance-bounces at stat.math.ethz.ch
> Subject: RE: [R-sig-finance] R and exchanges
> 
> 
> IMHO, that may be a (very) poor design, depending on how frequently
> you want to connect, how much data you get etc. A much more robust
> solution is to have a DBMS between the data connections and R. R can
> interact with MySQL, PostgreSQL, Oracle, SQLlite, and in fact
> depending on the calculations perhaps you don't really need R and can
> do everything in Python, Perl etc.
> 
> 
> > -----Original Message-----
> > From: r-sig-finance-bounces at stat.math.ethz.ch
> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
> > Molins, Jordi
> > Sent: Friday, December 02, 2005 8:15 AM
> > To: 'r-sig-finance at stat.math.ethz.ch'
> > Cc: Jordi Molins
> > Subject: [R-sig-finance] R and exchanges
> >
> >
> > I sent a question to the general Help. I received an answer
> > from Patrick
> > Burns, and he suggested me to post it here. The question is
> > how to use batch
> > files that call R in an effective way. More:
> >
> > What I need is to interact with other applications(a connection to
> > exchanges, like Eurex, Liffe, CBOT and eventually to
> > something like ESB in
> > FX). So, if I get a price from the exchanges, I want that R makes a
> > calculation. But to get R connected to the exchanges is
> > complex, so what I
> > have thought is to have an external program that connects to
> > the exchanges,
> > then gets the prices and saves them to a txt file, then R is
> > opened, the txt
> > file is read, the calculations are performed, and the results
> > are saved into
> > another txt file. Then, another application reads the new txt
> > file and sends
> > a message to the exchanges.
> >
> > This is what I want to do.
> >
> > Does anybody have some ideas about it?
> >
> > Thank you!
> >
> > Jordi
> >
> >
> >
> > --------------------------------------------------------------
> > ------------------
> > The information contained herein is confidential and is
> > inte...{{dropped}}
> >
> > _______________________________________________
> > R-sig-finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}


From svenier at aueb.gr  Sun Nov 27 01:03:10 2005
From: svenier at aueb.gr (svenier@aueb.gr)
Date: Sun, 27 Nov 2005 02:03:10 +0200
Subject: [R-sig-finance] coherency
Message-ID: <6.0.2.0.1.20051127020237.0036ea38@pop.aueb.gr>

hello!
My name is Stefanos, from Athens.
I'm a new user of R and I'm studying multivariate time series. I can't find 
in the help menu how to calculate the cross spectrum and the coherency of 2 
Time Series. Would you like to help me?
Thanks


From eric at ehlarson.com  Sat Nov 26 17:05:27 2005
From: eric at ehlarson.com (eric)
Date: Sat, 26 Nov 2005 11:05:27 -0500
Subject: [R-sig-finance] MySQL and other RDBMSs
In-Reply-To: <20051126151612.GA44704@tehun.pair.com>
References: <4387EE7F.20401@xoxy.net> <20051126151612.GA44704@tehun.pair.com>
Message-ID: <438887C7.7080406@ehlarson.com>

Andrew Piskorski wrote:
> On Sat, Nov 26, 2005 at 12:11:27AM -0500, Rob Steele wrote:
>
>   
>> MySQL is ideal for the write-once-read-thereafter scenario 
>> that research implies
>>     

>> True (that's what MySQL was originally designed for), except that it's
>> SQL dialect is pretty limited, and the database has various
>> misfeatures, e.g.:
>>
>>   http://sql-info.de/mysql/gotchas.html
>>
>> The limited SQL support might or might not matter to you, depending on
>> just what sort of data you're loading in and how complicated your data
>> model and queries are, but it's certainly something you should keep in
>> mind when picking a RDBMS.  Maybe that's been improved in MySQL 5.0
>> (which was just released a month or so ago), I don't know.
>>
>> If I didn't want to use PostgreSQL or Oracle, I'd be tempted to try a
>> lightweight embedded database like SQLite or Metakit:
>>
>>     

One  of the advantages of PostgreSQL is that it is relatively easy to 
port a PostgreSQL database to Oracle if needed.  Moving MySQL to Oracle 
is a much bigger challenge.


From aas.claus.dethlefsen at nja.dk  Thu Dec  1 17:32:01 2005
From: aas.claus.dethlefsen at nja.dk (Claus Dethlefsen / Aalborg Sygehus)
Date: Thu, 1 Dec 2005 17:32:01 +0100
Subject: [R-sig-finance] Kalman Smoothing - time-variant parameters
	(sspir)
References: <2310043c0512010412j5298187cv4599b4acc056abfc@mail.gmail.com>
Message-ID: <878184D41285C04B92FE9DBAD4E86671583C39@SFEXC00002.amtg.NJA.local>

Dear Tariq Khan
 
The initial conditions m0 and C0 can be specified according to your needs. If you are a Bayesian (as in West&Harrison 1997), you will use m0 and C0 to express your prior information. If you use a vague prior, you will give a high weight to your observations in the beginning, and the influence of the prior will die out fast. 
 
The values of m0 and C0 could also stem from several time-series and express a random effect of the level of the individual series.
 
Finally, you may estimate m0 and C0 using maximum likelihood estimation. This is not done in sspir (but the log-likelihood value is provided from a run of the filter).
 
One crude way of specifying m0 and C0 would be to use the estimates from a static model, i.e.
 
ss$ss$m0[1:2,] <- coef(lm(y~x,data=dfrm))
ss$ss$C0[1:2,1:2] <- summary(lm(y~x,data=dfrm))$cov.unscaled
smooth.params3 <- kfs(ss)$m
ts.plot(t(smooth.params3))

Note that the 'kfs' function is a shortcut for using smoother(kfilter()).
 
Note also, that your variance parameters are both set to unity. Again, you may discuss how to set these either by previous knowledge or by maximum likelihood estimation. It is set using
 
ss$ss$phi[1]   <- 2 # observational variance
ss$ss$phi[2]   <- .5# variance of the beta-parameter
 
Hope this helps,
 
Claus
 
________________________________
Claus Dethlefsen, Msc, PhD
Statistiker ved Kardiovaskul?rt Forskningscenter

 
Forskningens Hus
Aalborg Sygehus 
Sdr. Skovvej 15
9000 Aalborg

Tlf:   9932 6863
email: aas.claus.dethlefsen at nja.dk <mailto:aas.claus.dethlefsen at nja.dk> 

________________________________

Fra: ?Tariq Khan [mailto:tariq.khan at gmail.com]
Sendt: to 01-12-2005 13:12
Til: R-help at stat.math.ethz.ch; R-sig-finance at stat.math.ethz.ch
Cc: Claus Dethlefsen / Aalborg Sygehus
Emne: Kalman Smoothing - time-variant parameters (sspir)



Dear R-brains,

I'm rather new to state-space models and would benefit from the extra
confidence in using the excellent package sspir.

In a one-factor model, If I am trying to do a simple regression where
I assume the intercept is constant and the 'Beta' is changing, how do
I do that? How do i Initialize the filter (i.e. what is appropriate to
set m0, and C0 for the example below)?

The model I want is: y = alpha + beta + err1; beta_(t+1) = beta_t + err2

I thought of the following:
library(mvtnorm) # (1)
library(sspir)
# Let's get some data so we can all try this at home
dfrm <- data.frame(
                   y =
c(0.02,0.04,-0.03,0.02,0,0.01,0.04,0.03,-0.01,0.04,-0.01,0.05,0.04,
                         
0.03,0.01,-0.01,-0.01,-0.03,0.02,-0.04,-0.05,-0.02,-0.04,0,0.02,0,
                        
-0.01,-0.01,0.01,0.09,0.03,0.03,0.05,0.04,-0.01,0.05,0.03,0.01,
                          0.04,0.01,-0.01,-0.02,-0.01,-0.01,
0.06,0.03,0.02,0.03,0.03,0.04,
                          0.03,0.04,-0.02,-0.03,0.04,0.03,0.05,0.02,0.03,-0.1),
                   x = c(-0.03,-0.01,0.07,-0.03,-0.07,0.05,0.02,-0.05,-0.04,
                           -0.02,-0.19,0.07,0.09,0.01,0.01,0,0.05,0,-0.02,-0.09,
                           -0.12,-0.01,-0.13,0.04,0.04,-0.07,-0.05,-0.03,
                           -0.01,0.11,0.06,0.03,0.06,0.06,-0.01,0.07,0.01,
                          
0,0.07,0.04,-0.02,0,-0.03,0.04,-0.04,-0.01,0.03,0.02,0.05,0.04,
                            0.05,0.03,0,-0.04,0.05,0.05,0.06,0.02,0.04,-0.06)
)
ss <- ssm(y ~ tvar(x), time = 1:nrow(dfrm), family=gaussian(link="identity"),
               data=dfrm)
smooth.params <- smoother(kfilter(ss$ss))$m

(1) I read in http://ww.math.aau.dk/~mbn/Teaching/MarkovE05/Lecture3.pdf
that this is requred as there is a bug in sspir.

To what should I set ss$ss$m0 and ss$ss$C0? (I did notice that
smoother() replaces these, but it still matters what I initialize it
to in the first place)

Many thanks!

Tariq Khan


From L.Isella at myrealbox.com  Fri Dec  2 14:58:10 2005
From: L.Isella at myrealbox.com (L.Isella)
Date: Fri, 02 Dec 2005 13:58:10 +0000
Subject: [R-sig-finance] R and C++
Message-ID: <1133531890.c814a61cL.Isella@myrealbox.com>

Dear All,
I am learning C++ for financial applications and I am toying around with the "basic" routines provided by the financial recipes site http://finance.bi.no/~bernt/gcc_prog/

It would be useful for me to be able to run some of them under R (e.g. I perform some statistic analysis on some stock data with R and I pass some variable to the C++ routine and get back a result I can use in R).
I know this is possible, but I wonder whether it is simple to implement (I do not have outstanding expertise in R and I am an absolute beginner in C++).
Best Regards

Lorenzo Isella


From kriskumar at earthlink.net  Fri Dec  2 22:42:40 2005
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Fri, 02 Dec 2005 16:42:40 -0500
Subject: [R-sig-finance] R and C++
In-Reply-To: <1133531890.c814a61cL.Isella@myrealbox.com>
References: <1133531890.c814a61cL.Isella@myrealbox.com>
Message-ID: <4390BFD0.3000707@earthlink.net>

Have you looked at Dirk's  RQuantlib source code that will give you a 
good idea on how to wrap c++  functions.
And ofcourse the R-ext manual is useful.

Best,
Kris



L.Isella wrote:

>Dear All,
>I am learning C++ for financial applications and I am toying around with the "basic" routines provided by the financial recipes site http://finance.bi.no/~bernt/gcc_prog/
>
>It would be useful for me to be able to run some of them under R (e.g. I perform some statistic analysis on some stock data with R and I pass some variable to the C++ routine and get back a result I can use in R).
>I know this is possible, but I wonder whether it is simple to implement (I do not have outstanding expertise in R and I am an absolute beginner in C++).
>Best Regards
>
>Lorenzo Isella
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From spencer.graves at pdf.com  Fri Dec  2 23:51:01 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 02 Dec 2005 14:51:01 -0800
Subject: [R-sig-finance] coherency
In-Reply-To: <6.0.2.0.1.20051127020237.0036ea38@pop.aueb.gr>
References: <6.0.2.0.1.20051127020237.0036ea38@pop.aueb.gr>
Message-ID: <4390CFD5.9060700@pdf.com>

	  I just got 15 hits from 'RSiteSearch("cross spectrum")' and 12 from 
'RSiteSearch("coherency")'.  I doubt if all are relevant to your 
question, but the first in each list seems relevant.

	  If you would like further support from this list, I suggest you first 
review the "r-help" posting guide! 
"www.R-project.org/posting-guide.html".  I believe that people who 
follow that guide tend to get more useful replies quicker.

	  hope this helps,
	  Spencer Graves

svenier at aueb.gr wrote:

> hello!
> My name is Stefanos, from Athens.
> I'm a new user of R and I'm studying multivariate time series. I can't find 
> in the help menu how to calculate the cross spectrum and the coherency of 2 
> Time Series. Would you like to help me?
> Thanks
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915


From a.trapletti at swissonline.ch  Mon Dec  5 14:20:32 2005
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Mon, 05 Dec 2005 14:20:32 +0100
Subject: [R-sig-finance] R and exchanges
Message-ID: <43943EA0.9000509@swissonline.ch>

>
>
>I sent a question to the general Help. I received an answer from Patrick
>Burns, and he suggested me to post it here. The question is how to use batch
>files that call R in an effective way. More:
>
>What I need is to interact with other applications(a connection to
>exchanges, like Eurex, Liffe, CBOT and eventually to something like ESB in
>FX). So, if I get a price from the exchanges, I want that R makes a
>calculation. But to get R connected to the exchanges is complex, so what I
>have thought is to have an external program that connects to the exchanges,
>then gets the prices and saves them to a txt file, then R is opened, the txt
>file is read, the calculations are performed, and the results are saved into
>another txt file. Then, another application reads the new txt file and sends
>a message to the exchanges.
>
>This is what I want to do.
>
>Does anybody have some ideas about it?
>
>Thank you!
>
>Jordi
>
>
>  
>
The companies I (used to) work for typically use(d) the following setup: 
Data receiver/collector gets the data over some proprietary API from the 
data provider (Reuters, Bloomberg,...), attaches filter information 
(e.g., a number between 0 and 1 representing the "credibility" of the 
data; even todays electronic markets produce strange information from 
time to time) and stores it in a database (e.g., mysql, binary 
files,...). Applications like trading systems connect to the data base, 
get the needed information (historical and realtime; for realtime 
distribution of information it might useful if the DB notifies the 
applications if new data arrives), compute something, and react somehow, 
e.g., send a trading order through another API (FIX or proprietary) to 
the broker/exchange for execution. The frameworks I have seen so far 
used mainly C, C++ and Java, but it would be no problem to use, e.g, R 
for the computation engine and use its DB interfaces or Java interfaces etc.

Best
Adrian


From dkf at specere.com  Tue Dec  6 04:31:19 2005
From: dkf at specere.com (dkf@specere.com)
Date: Tue, 6 Dec 2005 10:31:19 +0700 (ICT)
Subject: [R-sig-finance] NIG Option Pricing
Message-ID: <1878.58.10.34.88.1133839879.squirrel@mail.specere.com>


Hello,

I have been using the NIG-functions in the fBasics package
trying to determine some european call prices using a NIG
distribution, but seemed to have hit a wall.

Basically, the process (as I know it) is to determine the
parameter for the Esscher transform, which is then used in
adjusting the density function for integrating in a BS=style
fashion.

A test case with parameters is included.  The results do not
appear correct as the classic "W" shape of "call(NIG) - call(BS)"
is not observed.  further, if you compare the different call
prices vs. stock/strike, as well as the implicit value - the
results are rather odd, as the call(NIG) value is often less
than the intrinsic value.

There are some debugging routines defined in the following
R-script.

Any help is greatly appreciated.

Regards,

--Dan

test case and parameters
------------------------


library(fBasics)   # for nig distribution

alpha <- 17.32268
beta <- 1.453270
delta <- 0.01144148
mu <- -0.0004846318

mean <- 0.0004780999
sd <- sqrt(delta*(alpha^2)/(sqrt((alpha^2)-(beta^2))^3))

rf <- 0.04

#  determine the parameter, theta, for the riskneutral process
#  under the esscher transform

esscher <- function(x) {
  result <- mu + delta*(sqrt(alpha^2 - (beta + x)^2) - sqrt(alpha^2 -
(beta + x + 1)^2)) - rf
  return(result)
}

lowerlimit <- -alpha - beta
upperlimit <- alpha - beta - 1
xmin <- uniroot(esscher, c(lowerlimit, upperlimit), tol = 1.0e-6)

theta <- xmin$root

# visual test of distributions
#

showpdfs <- function() {
  #
  #  global: alpha, beta, delta, mu, theta, rf
  windows()

  span.min = qnig(0.001, alpha, beta, delta, mu)
  span.max = qnig(0.999, alpha, beta, delta, mu)
  span <- seq(span.min, span.max, length=200)
  yfit <- dnig(span, alpha = alpha, beta = beta, delta = delta, mu = mu)
  yfit2 <- dnig(span, alpha = alpha, beta = beta + theta, delta = delta,
mu = 0)
  yfitnorm <- dnorm(span, mean=mean, sd=sd)

  ylim <- log(c(min(yfit), max(yfit)))  # only bounding on fitted
distribution
  xlim <- c(span[1], span[length(span)])
  plot(span, log(yfit), ylim=ylim, xlim=xlim, type="l", lty=2, col="blue",
ylab="", xlab="logreturns")
  par(new=TRUE)
  plot(span, log(yfit2), ylim=ylim, xlim=xlim, type="l", lty=3, col="red",
ylab="", xlab="logreturns")
  par(new=TRUE)
  plot(span, log(yfitnorm), ylim=ylim, xlim=xlim, type="l", lty=4,
col="green", ylab="", xlab="logreturns")
  legend(x="topleft", lty=c(2, 3, 4), col=c("blue", "red", "green"),
c("objective", "riskneutral", "normal"))
}

# define european call under NIG

nigpdf <- function(x) {
  #
  #  global: alpha, beta, delta, mu, theta, rf
  dnig(x, alpha=alpha, beta=beta, delta=delta, mu=mu)
}

callnig <- function(stock, strike, maturity) {
  #
  #  global: alpha, beta, delta, mu, theta, rf
  if (maturity <= 0) {
    return(max(stock - strike, 0))
  }
  iblower <- log(strike/stock)
  ibupper <- Inf

  # adjust distribution estimates
  muold <- mu
  mu <- 0
  delta <- delta*maturity
  beta <- beta + theta
  intg2 <- integrate(nigpdf, iblower, ibupper)$value
  beta <- beta + 1
  intg1 <- integrate(nigpdf, iblower, ibupper)$value
  result <- stock*intg1 - strike*exp(-rf*maturity)*intg2

  # unwind adjustments
  beta <- beta - theta - 1
  delta <- delta/maturity
  mu <- muold

  return(result)
}

# define european call under B&S

callbs <- function(stock, strike, maturity) {
  #
  # global:  sd
  if (maturity <= 0) {
    return(max(stock - strike, 0))
  }
  d1 <- (log(stock/strike) + ((rf +
(0.5*(sd^2)))*maturity))/(sd*sqrt(maturity))
  d2 <- d1 - sd*sqrt(maturity)
  result <- stock*pnorm(d1) - strike*exp(-rf*maturity)*pnorm(d2)

  return(result)
}

# define function for evaluating calls
#

pfmt5 <- function(x) {
  t1 <- sprintf("%5g", x)
  return(as.numeric(t1))
}

# plot call vs underlying

plotcall <- function(from=27.5, to=32.5, strike=30, maturity=1) {
  npoints <- 200
  sp <- seq(from=from, to=to, length=npoints)
  cbs <- double(npoints)
  cnig <- double(npoints)
  ip <- double(npoints)

  windows()  # open new window for plotting

  for (i in 1:npoints) {
    cnig[i] <- callnig(sp[i], strike, maturity)
    cbs[i] <- callbs(sp[i], strike, maturity)
    ip[i] <- max(sp[i] - strike, 0)   # intrinsic value of call
  }
  plot(sp/strike, ip, ylim=c(0,4), ylab="", xlab="",
    type="l", lty=2, col="blue")
  par(new=TRUE)
  plot(sp/strike, cbs, ylim=c(0,4), ylab="", xlab="",
    type="l", lty=3, col="red")
  par(new=TRUE)
  plot(sp/strike, cnig, ylim=c(0,4), ylab="call price", xlab="(stock
price)/strike",
    type="l", lty=4, col="green")
  legend(x="bottomright", lty=c(2, 3, 4), col=c("blue", "red", "green"),
c("implicit value", "black-scholes", "nig"))
  y <- 3.7
  yinc <- 0.15
  text(0.92, y, adj=0, paste("NIG Parameters:")); y <- y - yinc
  text(0.93, y, adj=0, substitute(alpha==v, list(v=pfmt5(alpha)))); y <- y
- yinc
  text(0.93, y, adj=0, substitute(beta==v, list(v=pfmt5(beta)))); y <- y -
yinc
  text(0.93, y, adj=0, substitute(delta==v, list(v=pfmt5(delta)))); y <- y
- yinc
  text(0.93, y, adj=0, substitute(mu==v, list(v=pfmt5(mu)))); y <- y - yinc
  text(0.93, y, adj=0, substitute(theta==v, list(v=pfmt5(theta)))); y <- y
- yinc
  y <- y - yinc
  text(0.92, y, adj=0, paste("Normal Parameters:")); y <- y - yinc
  text(0.93, y, adj=0, substitute(mu==v, list(v=pfmt5(mean)))); y <- y - yinc
  text(0.93, y, adj=0, substitute(sigma==v, list(v=pfmt5(sd)))); y <- y -
yinc

  title("call(NIG), call(BS), implicit value vs. stock/strike")
}

# plot the difference of the two prices

plotdiff <- function(from=27.5, to=32.5, strike=30, maturity=1) {
  npoints <- 200
  sp <- seq(from=from, to=to, length=npoints)
  cbs <- double(npoints)
  cnig <- double(npoints)
  ip <- double(npoints)

  windows()  # open new window for plotting

  for (i in 1:npoints) {
    cnig[i] <- callnig(sp[i], strike, maturity)
    cbs[i] <- callbs(sp[i], strike, maturity)
  }
  plot((sp/strike), (cnig - cbs), type="l", col="red")
  y <- -0.2
  yinc <- 0.025
  text(0.92, y, adj=0, paste("NIG Parameters:")); y <- y - yinc
  text(0.93, y, adj=0, substitute(alpha==v, list(v=pfmt5(alpha)))); y <- y
- yinc
  text(0.93, y, adj=0, substitute(beta==v, list(v=pfmt5(beta)))); y <- y -
yinc
  text(0.93, y, adj=0, substitute(delta==v, list(v=pfmt5(delta)))); y <- y
- yinc
  text(0.93, y, adj=0, substitute(mu==v, list(v=pfmt5(mu)))); y <- y - yinc
  text(0.93, y, adj=0, substitute(theta==v, list(v=pfmt5(theta)))); y <- y
- yinc
  y <- y - yinc
  text(0.92, y, adj=0, paste("Normal Parameters:")); y <- y - yinc
  text(0.93, y, adj=0, substitute(mu==v, list(v=pfmt5(mean)))); y <- y - yinc
  text(0.93, y, adj=0, substitute(sigma==v, list(v=pfmt5(sd)))); y <- y -
yinc

  title("call(NIG) - call(BS)")
}

plotsurfacediff <- function() {

  windows()

  sp <- seq(from=15, to=45, length=100)
  call <- matrix(nrow=100, ncol=25)
  mat <- seq(from=.2, to=10, length=25)

  strike <- 30

  for (idx in 1:25) {
    for (i in 1:100) {
      # print(paste("idx = ", idx))
      # print(paste("i = ", i))
      call[i, idx] <- callnig(sp[i], strike, mat[idx]) - callbs(sp[i],
strike, mat[idx])
      # call[i, idx] <- callbs0(sp[i], strike, mat[idx], 0.3, rf, rf)
      # call[i, idx] <- callbs(sp[i], strike, mat[idx])
      # call[i, idx] <- callbs2(sp[i], strike, mat[idx])
      # call[i, idx] <- callnig(sp[i], strike, mat[idx])
    }
  }

  persp(sp, mat, call, theta=30, phi=30, expand=0.8, col="lightblue",
ticktype = "detailed",
           xlab = "stock price", ylab = "maturity", zlab = "call price")
}

#  these just do the default.

plotcall()

plotdiff()

showpdfs()

plotsurfacediff()


From patrick at burns-stat.com  Tue Dec  6 23:20:14 2005
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 06 Dec 2005 22:20:14 +0000
Subject: [R-sig-finance] Version 3 of POP Portfolio Construction Suite
	released
Message-ID: <43960E9E.4030502@burns-stat.com>

Version 3 of POP Portfolio Construction Suite is now available.

Changes include:
Now available for Linux as well as Windows.

Faster optimization, especially when a large number of assets are traded.

More constraints, most of these are mainly useful for generating random
portfolios.

As always, the User's Manual can be educational no matter what software
you want to use.

More details on the Burns Statistics website.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


From kriskumar at earthlink.net  Wed Dec  7 03:13:36 2005
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Tue, 06 Dec 2005 21:13:36 -0500
Subject: [R-sig-finance] NIG Option Pricing
In-Reply-To: <1878.58.10.34.88.1133839879.squirrel@mail.specere.com>
References: <1878.58.10.34.88.1133839879.squirrel@mail.specere.com>
Message-ID: <43964550.8000500@earthlink.net>

I am not sure of a few things you are doing

The BS volatility in your function below is what you compute as "sd"  
below I don't think that makes sense. You'd have to
imply the bs volatility from NiG call prices to compare NiG to B.S

So you imply b.s. volatility and then price with that implied to get the 
B.S price. Also the reason your implicit values are odd is
because "max"  by default does the maximum across the list/array/vector 
object. What you probably want to do is to use "pmax" instead.

Finally you would find AVT and wim schoutens posting on wilmott helpful.
http://www.wilmott.com/messageview.cfm?catid=8&threadid=14313

Cheers,
Kris







dkf at specere.com wrote:

>Hello,
>
>I have been using the NIG-functions in the fBasics package
>trying to determine some european call prices using a NIG
>distribution, but seemed to have hit a wall.
>
>Basically, the process (as I know it) is to determine the
>parameter for the Esscher transform, which is then used in
>adjusting the density function for integrating in a BS=style
>fashion.
>
>A test case with parameters is included.  The results do not
>appear correct as the classic "W" shape of "call(NIG) - call(BS)"
>is not observed.  further, if you compare the different call
>prices vs. stock/strike, as well as the implicit value - the
>results are rather odd, as the call(NIG) value is often less
>than the intrinsic value.
>
>There are some debugging routines defined in the following
>R-script.
>
>Any help is greatly appreciated.
>
>Regards,
>
>--Dan
>
>test case and parameters
>------------------------
>
>
>library(fBasics)   # for nig distribution
>
>alpha <- 17.32268
>beta <- 1.453270
>delta <- 0.01144148
>mu <- -0.0004846318
>
>mean <- 0.0004780999
>sd <- sqrt(delta*(alpha^2)/(sqrt((alpha^2)-(beta^2))^3))
>
>rf <- 0.04
>
>#  determine the parameter, theta, for the riskneutral process
>#  under the esscher transform
>
>esscher <- function(x) {
>  result <- mu + delta*(sqrt(alpha^2 - (beta + x)^2) - sqrt(alpha^2 -
>(beta + x + 1)^2)) - rf
>  return(result)
>}
>
>lowerlimit <- -alpha - beta
>upperlimit <- alpha - beta - 1
>xmin <- uniroot(esscher, c(lowerlimit, upperlimit), tol = 1.0e-6)
>
>theta <- xmin$root
>
># visual test of distributions
>#
>
>showpdfs <- function() {
>  #
>  #  global: alpha, beta, delta, mu, theta, rf
>  windows()
>
>  span.min = qnig(0.001, alpha, beta, delta, mu)
>  span.max = qnig(0.999, alpha, beta, delta, mu)
>  span <- seq(span.min, span.max, length=200)
>  yfit <- dnig(span, alpha = alpha, beta = beta, delta = delta, mu = mu)
>  yfit2 <- dnig(span, alpha = alpha, beta = beta + theta, delta = delta,
>mu = 0)
>  yfitnorm <- dnorm(span, mean=mean, sd=sd)
>
>  ylim <- log(c(min(yfit), max(yfit)))  # only bounding on fitted
>distribution
>  xlim <- c(span[1], span[length(span)])
>  plot(span, log(yfit), ylim=ylim, xlim=xlim, type="l", lty=2, col="blue",
>ylab="", xlab="logreturns")
>  par(new=TRUE)
>  plot(span, log(yfit2), ylim=ylim, xlim=xlim, type="l", lty=3, col="red",
>ylab="", xlab="logreturns")
>  par(new=TRUE)
>  plot(span, log(yfitnorm), ylim=ylim, xlim=xlim, type="l", lty=4,
>col="green", ylab="", xlab="logreturns")
>  legend(x="topleft", lty=c(2, 3, 4), col=c("blue", "red", "green"),
>c("objective", "riskneutral", "normal"))
>}
>
># define european call under NIG
>
>nigpdf <- function(x) {
>  #
>  #  global: alpha, beta, delta, mu, theta, rf
>  dnig(x, alpha=alpha, beta=beta, delta=delta, mu=mu)
>}
>
>callnig <- function(stock, strike, maturity) {
>  #
>  #  global: alpha, beta, delta, mu, theta, rf
>  if (maturity <= 0) {
>    return(max(stock - strike, 0))
>  }
>  iblower <- log(strike/stock)
>  ibupper <- Inf
>
>  # adjust distribution estimates
>  muold <- mu
>  mu <- 0
>  delta <- delta*maturity
>  beta <- beta + theta
>  intg2 <- integrate(nigpdf, iblower, ibupper)$value
>  beta <- beta + 1
>  intg1 <- integrate(nigpdf, iblower, ibupper)$value
>  result <- stock*intg1 - strike*exp(-rf*maturity)*intg2
>
>  # unwind adjustments
>  beta <- beta - theta - 1
>  delta <- delta/maturity
>  mu <- muold
>
>  return(result)
>}
>
># define european call under B&S
>
>callbs <- function(stock, strike, maturity) {
>  #
>  # global:  sd
>  if (maturity <= 0) {
>    return(max(stock - strike, 0))
>  }
>  d1 <- (log(stock/strike) + ((rf +
>(0.5*(sd^2)))*maturity))/(sd*sqrt(maturity))
>  d2 <- d1 - sd*sqrt(maturity)
>  result <- stock*pnorm(d1) - strike*exp(-rf*maturity)*pnorm(d2)
>
>  return(result)
>}
>
># define function for evaluating calls
>#
>
>pfmt5 <- function(x) {
>  t1 <- sprintf("%5g", x)
>  return(as.numeric(t1))
>}
>
># plot call vs underlying
>
>plotcall <- function(from=27.5, to=32.5, strike=30, maturity=1) {
>  npoints <- 200
>  sp <- seq(from=from, to=to, length=npoints)
>  cbs <- double(npoints)
>  cnig <- double(npoints)
>  ip <- double(npoints)
>
>  windows()  # open new window for plotting
>
>  for (i in 1:npoints) {
>    cnig[i] <- callnig(sp[i], strike, maturity)
>    cbs[i] <- callbs(sp[i], strike, maturity)
>    ip[i] <- max(sp[i] - strike, 0)   # intrinsic value of call
>  }
>  plot(sp/strike, ip, ylim=c(0,4), ylab="", xlab="",
>    type="l", lty=2, col="blue")
>  par(new=TRUE)
>  plot(sp/strike, cbs, ylim=c(0,4), ylab="", xlab="",
>    type="l", lty=3, col="red")
>  par(new=TRUE)
>  plot(sp/strike, cnig, ylim=c(0,4), ylab="call price", xlab="(stock
>price)/strike",
>    type="l", lty=4, col="green")
>  legend(x="bottomright", lty=c(2, 3, 4), col=c("blue", "red", "green"),
>c("implicit value", "black-scholes", "nig"))
>  y <- 3.7
>  yinc <- 0.15
>  text(0.92, y, adj=0, paste("NIG Parameters:")); y <- y - yinc
>  text(0.93, y, adj=0, substitute(alpha==v, list(v=pfmt5(alpha)))); y <- y
>- yinc
>  text(0.93, y, adj=0, substitute(beta==v, list(v=pfmt5(beta)))); y <- y -
>yinc
>  text(0.93, y, adj=0, substitute(delta==v, list(v=pfmt5(delta)))); y <- y
>- yinc
>  text(0.93, y, adj=0, substitute(mu==v, list(v=pfmt5(mu)))); y <- y - yinc
>  text(0.93, y, adj=0, substitute(theta==v, list(v=pfmt5(theta)))); y <- y
>- yinc
>  y <- y - yinc
>  text(0.92, y, adj=0, paste("Normal Parameters:")); y <- y - yinc
>  text(0.93, y, adj=0, substitute(mu==v, list(v=pfmt5(mean)))); y <- y - yinc
>  text(0.93, y, adj=0, substitute(sigma==v, list(v=pfmt5(sd)))); y <- y -
>yinc
>
>  title("call(NIG), call(BS), implicit value vs. stock/strike")
>}
>
># plot the difference of the two prices
>
>plotdiff <- function(from=27.5, to=32.5, strike=30, maturity=1) {
>  npoints <- 200
>  sp <- seq(from=from, to=to, length=npoints)
>  cbs <- double(npoints)
>  cnig <- double(npoints)
>  ip <- double(npoints)
>
>  windows()  # open new window for plotting
>
>  for (i in 1:npoints) {
>    cnig[i] <- callnig(sp[i], strike, maturity)
>    cbs[i] <- callbs(sp[i], strike, maturity)
>  }
>  plot((sp/strike), (cnig - cbs), type="l", col="red")
>  y <- -0.2
>  yinc <- 0.025
>  text(0.92, y, adj=0, paste("NIG Parameters:")); y <- y - yinc
>  text(0.93, y, adj=0, substitute(alpha==v, list(v=pfmt5(alpha)))); y <- y
>- yinc
>  text(0.93, y, adj=0, substitute(beta==v, list(v=pfmt5(beta)))); y <- y -
>yinc
>  text(0.93, y, adj=0, substitute(delta==v, list(v=pfmt5(delta)))); y <- y
>- yinc
>  text(0.93, y, adj=0, substitute(mu==v, list(v=pfmt5(mu)))); y <- y - yinc
>  text(0.93, y, adj=0, substitute(theta==v, list(v=pfmt5(theta)))); y <- y
>- yinc
>  y <- y - yinc
>  text(0.92, y, adj=0, paste("Normal Parameters:")); y <- y - yinc
>  text(0.93, y, adj=0, substitute(mu==v, list(v=pfmt5(mean)))); y <- y - yinc
>  text(0.93, y, adj=0, substitute(sigma==v, list(v=pfmt5(sd)))); y <- y -
>yinc
>
>  title("call(NIG) - call(BS)")
>}
>
>plotsurfacediff <- function() {
>
>  windows()
>
>  sp <- seq(from=15, to=45, length=100)
>  call <- matrix(nrow=100, ncol=25)
>  mat <- seq(from=.2, to=10, length=25)
>
>  strike <- 30
>
>  for (idx in 1:25) {
>    for (i in 1:100) {
>      # print(paste("idx = ", idx))
>      # print(paste("i = ", i))
>      call[i, idx] <- callnig(sp[i], strike, mat[idx]) - callbs(sp[i],
>strike, mat[idx])
>      # call[i, idx] <- callbs0(sp[i], strike, mat[idx], 0.3, rf, rf)
>      # call[i, idx] <- callbs(sp[i], strike, mat[idx])
>      # call[i, idx] <- callbs2(sp[i], strike, mat[idx])
>      # call[i, idx] <- callnig(sp[i], strike, mat[idx])
>    }
>  }
>
>  persp(sp, mat, call, theta=30, phi=30, expand=0.8, col="lightblue",
>ticktype = "detailed",
>           xlab = "stock price", ylab = "maturity", zlab = "call price")
>}
>
>#  these just do the default.
>
>plotcall()
>
>plotdiff()
>
>showpdfs()
>
>plotsurfacediff()
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From dkf at specere.com  Tue Dec  6 18:23:34 2005
From: dkf at specere.com (dkf@specere.com)
Date: Wed, 7 Dec 2005 00:23:34 +0700 (ICT)
Subject: [R-sig-finance] NIG Option Pricing
In-Reply-To: <43964550.8000500@earthlink.net>
References: <1878.58.10.34.88.1133839879.squirrel@mail.specere.com>
	<43964550.8000500@earthlink.net>
Message-ID: <1925.58.10.34.88.1133889814.squirrel@mail.specere.com>

Hello,

> I am not sure of a few things you are doing
>

As it turns out - neither was I.  I was able to narrow the problem
down and identify my programing error.  I apologize for the noise
on the list.

Anyway, to those that might be interested (and to prevent anyone
else wasting their time on this).  Basically, my mistake was to
assume that the default arguments to the function "nigpdf" were
evaluated at runtime. In fact, they are evaluated at the time
of the function defnition. Hence both integrate calls were
returning identical values.  The fix is, change the calls to
integrate from:

intg2 <- integrate(nigpdf, iblower, ibupper)$value
....
intg2 <- integrate(nigpdf, iblower, ibupper)$value

To:

intg2 <- integrate(nigpdf, iblower, ibupper,
  alpha=alpha, beta=beta, delta=delta, mu=0)$value
....
intg2 <- integrate(nigpdf, iblower, ibupper,
  alpha=alpha, beta=beta, delta=delta, mu=0)$value

In the callnig() function.

Regards,

--Dan


> The BS volatility in your function below is what you compute as "sd"
> below I don't think that makes sense. You'd have to
> imply the bs volatility from NiG call prices to compare NiG to B.S
>
> So you imply b.s. volatility and then price with that implied to get the
> B.S price. Also the reason your implicit values are odd is
> because "max"  by default does the maximum across the list/array/vector
> object. What you probably want to do is to use "pmax" instead.
>
> Finally you would find AVT and wim schoutens posting on wilmott helpful.
> http://www.wilmott.com/messageview.cfm?catid=8&threadid=14313
>
> Cheers,
> Kris
>
>


From maechler at stat.math.ethz.ch  Wed Dec  7 12:09:48 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 7 Dec 2005 12:09:48 +0100
Subject: [R-sig-finance] NIG Option Pricing
In-Reply-To: <1925.58.10.34.88.1133889814.squirrel@mail.specere.com>
References: <1878.58.10.34.88.1133839879.squirrel@mail.specere.com>
	<43964550.8000500@earthlink.net>
	<1925.58.10.34.88.1133889814.squirrel@mail.specere.com>
Message-ID: <17302.49916.661338.114070@stat.math.ethz.ch>

>>>>> "dkf" == dkf  <dkf at specere.com>
>>>>>     on Wed, 7 Dec 2005 00:23:34 +0700 (ICT) writes:

    dkf> Hello,
    >> I am not sure of a few things you are doing

    dkf> As it turns out - neither was I.  I was able to narrow
    dkf> the problem down and identify my programing error.  I
    dkf> apologize for the noise on the list.

    dkf> Anyway, to those that might be interested (and to
    dkf> prevent anyone else wasting their time on this).
    dkf> Basically, my mistake was to assume that the default
    dkf> arguments to the function "nigpdf" were evaluated at
    dkf> runtime. In fact, they are evaluated at the time of the
    dkf> function defnition. 

not really --  since this is not true for any R function!

I think your problem is that you are working with global
variables a lot, something which is *very strongly* discouraged....
In your case, I think you could pass a list(alpha=.., beta=..)
to the corresponding functions.

Regards,
Martin


From Ricardo.Zambrano at corpbanca.cl  Wed Dec  7 15:50:50 2005
From: Ricardo.Zambrano at corpbanca.cl (Ricardo Zambrano Aguilera)
Date: Wed, 7 Dec 2005 11:50:50 -0300
Subject: [R-sig-finance] RV: please ...i need help...
Message-ID: <800F3C4ADCFCA643A919EF7C140A045A064FF519@LLANQUIHUE.corpgroup.cl>



> -----Mensaje original-----
> De:	Ricardo Zambrano Aguilera 
> Enviado el:	lunes, 05 de diciembre de 2005 17:06
> Para:	'r-sig-finance at stat.math.ethz.ch'
> Asunto:	please ...i need help...
> 
> hello everyone my name is ricardo and i leave in santiago ,Chile i?m 23 years old, and i?m close to graduated from engineering with mention in stats
> my problem follows me every time.
> to graduated i must to do a paper work based in modelling the change type, between different coins. specially dollar - peso ,trought GARCH models, besides i need to create to my bank (the place where i?m doing my thesis), a system to calculate the Var (value at risk), but i have problems to calculate the var in a portfolio standar (just like a regular client of the bank) .   i don?t know, call me a oldie fashion , but i like to work in R, actually  i?m doing my job with the package tseries...
> BUT i did hear, that the splus 7.0 has a super module called finmetrics...   but i understand that R has a similar module the r-sig-finance it?s true this????????
> if is true  i would like to have it , can you help me??
> por favor , le habla un studiante de un pais tercer mundista, en donde el banco no puede comprar la licencia de splus
> BYE


From PSchwarz at gcrinsight.com  Wed Dec  7 18:07:12 2005
From: PSchwarz at gcrinsight.com (Schwarz,Paul)
Date: Wed, 7 Dec 2005 09:07:12 -0800
Subject: [R-sig-finance] query string tokens for Yahoo finance site
Message-ID: <89C7DEAADB79564DB278694EB2417CCE6995D3@CHOCOLATE.GCRInsight.com>

 
I recognize that this question is more Yahoo-related not directly
R-related, but I'm trying to better understand some of the details of
the yahooImport() function. Specifically, I'm looking for the
definitions and documentation for the query string tokens such as "&g="
etc.  Many of the tokens are readily understandable, e.g., &a= and &g=
based on their usage and context. However, I've come across other tokens
such as "&q=" and "&y=" that are less obvious. I'm curious to know where
these tokens are defined and documented? So far I've not been able to
find any documentation online at Yahoo or elsewhere which describes the
details of these query strings. So, I'd appreciate any pointers on where
to look.

Thanks.

Paul Schwarz


From gyollin at insightful.com  Wed Dec  7 18:32:12 2005
From: gyollin at insightful.com (Guy Yollin)
Date: Wed, 7 Dec 2005 09:32:12 -0800
Subject: [R-sig-finance] query string tokens for Yahoo finance site
Message-ID: <F90D76D3C7CC6A41B6E2E6F88CA8252E1CBC1B@se2kexch01.insightful.com>

Perhaps this website may be of help:

http://www.gummy-stuff.org/Yahoo-data.htm

-- Guy


Guy Yollin
Senior Financial Engineer
Insightful Corporation
www.insightful.com
gyollin at insightful.com


-----Original Message-----
From: Schwarz,Paul [mailto:PSchwarz at gcrinsight.com]
Sent: Wednesday, December 07, 2005 9:07 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-sig-finance] query string tokens for Yahoo finance site


 
I recognize that this question is more Yahoo-related not directly
R-related, but I'm trying to better understand some of the details of
the yahooImport() function. Specifically, I'm looking for the
definitions and documentation for the query string tokens such as "&g="
etc.  Many of the tokens are readily understandable, e.g., &a= and &g=
based on their usage and context. However, I've come across other tokens
such as "&q=" and "&y=" that are less obvious. I'm curious to know where
these tokens are defined and documented? So far I've not been able to
find any documentation online at Yahoo or elsewhere which describes the
details of these query strings. So, I'd appreciate any pointers on where
to look.

Thanks.

Paul Schwarz

_______________________________________________
R-sig-finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From spencer.graves at pdf.com  Sat Dec 10 17:56:14 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 10 Dec 2005 08:56:14 -0800
Subject: [R-sig-finance] RV: please ...i need help...
In-Reply-To: <800F3C4ADCFCA643A919EF7C140A045A064FF519@LLANQUIHUE.corpgroup.cl>
References: <800F3C4ADCFCA643A919EF7C140A045A064FF519@LLANQUIHUE.corpgroup.cl>
Message-ID: <439B08AE.3020501@pdf.com>

	  Just now, RSiteSearch("GARCH") produced for me 258 hits.  The first 
was a function 'garch' in the 'tseries' package.  At least 9 of the 
first 10 might be useful for you.  Similarly, 'help.search("GARCH")' 
identified capabilities already installed in local copy of R in packages 
  base, cluster, fCalendar, fOptions, fSeries, lattice, MASS, methods, 
stats, tcktk, tseries, and utils.  Some of these may not help you, but 
if you haven't already, I believe you will want to study fCalendar, 
fOptions, and fSeries and Rmetrics (www.rmetrics.org) of which these are 
part.  Rmetrics looks like an open source alternative to Finmetrics. 
I'm told that Finmetrics contains things that Rmetrics does not. 
However, Rmetrics looks like it will get much of what you want. 
Similarly, RSiteSearch("value at risk") produced for me 195 hits, the 
first of which looked like it would interest you.

	  This should help you take the next few steps in your project.  If you 
later would like more help from this listserve, I encourage you to 
PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html".  I believe it can (a) help you 
find answers that might not come to you without it, and (b) increase 
your chances of receiving a quicker, more useful reply.

	  Buena suerte,
	  spencer graves
-- 
Ricardo Zambrano Aguilera wrote:

> 
>>-----Mensaje original-----
>>De:	Ricardo Zambrano Aguilera 
>>Enviado el:	lunes, 05 de diciembre de 2005 17:06
>>Para:	'r-sig-finance at stat.math.ethz.ch'
>>Asunto:	please ...i need help...
>>
>>hello everyone my name is ricardo and i leave in santiago ,Chile i?m 23 years old, 
and i?m close to graduated from engineering with mention in stats
>>my problem follows me every time.
>>to graduated i must to do a paper work based in modelling the change type, 
between different coins. specially dollar - peso ,trought GARCH models, 
besides
i need to create to my bank (the place where i?m doing my thesis), a 
system to
calculate the Var (value at risk), but i have problems to calculate the 
var in
a portfolio standar (just like a regular client of the bank) .   i don?t 
know,
call me a oldie fashion , but i like to work in R, actually  i?m doing 
my job
with the package tseries...
>>BUT i did hear, that the splus 7.0 has a super module called finmetrics...  
  but i understand that R has a similar module the r-sig-finance it?s 
true this????????
>>if is true  i would like to have it , can you help me??
>>por favor , le habla un studiante de un pais tercer mundista, en donde el banco no puede comprar la licencia de splus
>>BYE
> 
> 
> _______________________________________________
> R-sig-finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915


From wuertz at itp.phys.ethz.ch  Tue Dec 13 14:18:16 2005
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Tue, 13 Dec 2005 13:18:16 +0000
Subject: [R-sig-finance] Constrained Log-Likelihood with SQP Solver
Message-ID: <439ECA18.40107@itp.phys.ethz.ch>




Dear R-Users,

I'm searching for somebody who can support me or even likes to 
collaborate with
me in setting up an R-package for "constrained maximim log-likelihood" 
parameter
estimation.

For example fitting the parameters of a MA(1)-APARCH(1,1) model for a 
time series
of 17'000 points (e.g. the famous Ding-Granger-Engle mode) takes about 
10 minutes
with the existing optimization algorithms available under R.

Modern state of the art algorithms, like SQP algorithms as implemented 
in Gauss,
Matlab, Ox, take about a few seconds. I tested this finding with a free 
constrained
SQP solver written in FORTRAN under R and found these results confirmed. I
got the results in a few seconds instead of a few minutes!

Now I'm looking for a collegue who has the experience in implementing 
FORTRAN
Optimization Code in R, calling the objective function and optionally 
gradient and
hessian from R functions. I have already inspected a lot of Fortran, C, 
and R sources
from the base package, but I didn't succeed so far with a reasonable effort.


Many thanks in advance
Diethelm Wuertz


From L.Isella at myrealbox.com  Tue Dec 13 16:41:38 2005
From: L.Isella at myrealbox.com (L.Isella)
Date: Tue, 13 Dec 2005 15:41:38 +0000
Subject: [R-sig-finance] Optimization of Non-Quadratic Functions
Message-ID: <1134488498.c7e0aafcL.Isella@myrealbox.com>

Dear All,
I am trying to implement some portfolio optimization techinique going beyond mean/variance optimization (e.g. including higher order moments in the quantity to maximize or using non-quadratic utilities). Furthermore, I am also interested in applying nonlinear constrains to my portfolio weights.
For all these tasks, I need something different from the solve.QP routine I have been using happily so far.
Is there any specifically designed routine for this kind of problem or should I revert to some general-purpose optimization package?
Any suggestion is welcome.

Best Regards

Lorenzo


From atp at piskorski.com  Wed Dec 14 14:43:06 2005
From: atp at piskorski.com (Andrew Piskorski)
Date: Wed, 14 Dec 2005 08:43:06 -0500
Subject: [R-sig-finance] Constrained Log-Likelihood with SQP Solver
In-Reply-To: <439ECA18.40107@itp.phys.ethz.ch>
References: <439ECA18.40107@itp.phys.ethz.ch>
Message-ID: <20051214134305.GA73479@tehun.pair.com>

On Tue, Dec 13, 2005 at 01:18:16PM +0000, Diethelm Wuertz wrote:

> Modern state of the art algorithms, like SQP algorithms as
> implemented in Gauss, Matlab, Ox, take about a few seconds.

I'm curious, to these other languages implement these SQP algorithms
natively, or are they too calling underlying Fortran or C libraries?
(I strongly suspect the latter.)

-- 
Andrew Piskorski <atp at piskorski.com>
http://www.piskorski.com/


From vivek.satsangi at gmail.com  Thu Dec 15 00:29:35 2005
From: vivek.satsangi at gmail.com (Vivek Satsangi)
Date: Wed, 14 Dec 2005 18:29:35 -0500
Subject: [R-sig-finance] R-sig-finance Digest, Vol 19, Issue 6
In-Reply-To: <mailman.17.1134558002.5724.r-sig-finance@stat.math.ethz.ch>
References: <mailman.17.1134558002.5724.r-sig-finance@stat.math.ethz.ch>
Message-ID: <bcb171920512141529s298f2edds2139ad3bd13b0a28@mail.gmail.com>

Hello Lorenzo,
I am assuming that you are already familiar with the offerings of the
book, "Modern Portfolio Optimization with NuOPT, S-PLUS and S+BAYES"
[Springer 2005] at

http://www.springer.com/sgw/cda/frontpage/0,,4-40109-22-34952077-0,00.htm

If not...
I have not read the whole book (it is above my current level, but I am
building up to it slowly). However, I understand that this book offers
various other optimizations. It also comes with a 150 day license for
the tools it uses, so even if your budget does not allow for SPLUS,
you can try the stuff they are doing in there. The only down side is
that they do not permit you to translate the packages into R.
(Although I suppose there is nothing to stop anyone from
reimplementing the algorithms in R, just not using their code -- I am
no legal/ethics expert).

Hope this helps. If you already knew all this, I am sorry to have
taken up your time.

Vivek
>
> ------------------------------
>
> Message: 2
> Date: Tue, 13 Dec 2005 15:41:38 +0000
> From: "L.Isella" <L.Isella at myrealbox.com>
> Subject: [R-sig-finance] Optimization of Non-Quadratic Functions
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID: <1134488498.c7e0aafcL.Isella at myrealbox.com>
> Content-Type: text/plain; charset="UTF-8"
>
> Dear All,
> I am trying to implement some portfolio optimization techinique going beyond mean/variance optimization (e.g. including higher order moments in the quantity to maximize or using non-quadratic utilities). Furthermore, I am also interested in applying nonlinear constrains to my portfolio weights.
> For all these tasks, I need something different from the solve.QP routine I have been using happily so far.
> Is there any specifically designed routine for this kind of problem or should I revert to some general-purpose optimization package?
> Any suggestion is welcome.
>
> Best Regards
>
> Lorenzo
>
>
>
> ------------------------------
>
> _______________________________________________
> R-sig-finance mailing list
> R-sig-finance at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
> End of R-sig-finance Digest, Vol 19, Issue 6
> ********************************************
>


--
-- Vivek Satsangi
Student, Rochester, NY USA


From bruche at cemfi.es  Fri Dec 16 09:45:34 2005
From: bruche at cemfi.es (Max)
Date: Fri, 16 Dec 2005 08:45:34 +0000 (UTC)
Subject: [R-sig-finance] Constrained Log-Likelihood with SQP Solver
References: <439ECA18.40107@itp.phys.ethz.ch>
	<20051214134305.GA73479@tehun.pair.com>
Message-ID: <loom.20051216T093738-612@post.gmane.org>

Andrew Piskorski <atp <at> piskorski.com> writes:

> 
> On Tue, Dec 13, 2005 at 01:18:16PM +0000, Diethelm Wuertz wrote:
> 
> > Modern state of the art algorithms, like SQP algorithms as
> > implemented in Gauss, Matlab, Ox, take about a few seconds.
> 
> I'm curious, to these other languages implement these SQP algorithms
> natively, or are they too calling underlying Fortran or C libraries?
> (I strongly suspect the latter.)
> 
I don't know about Gauss and Matlab, but Ox has a native SQP algorithm (in the
file src/solveqp.ox).

I would be interested to hear why you suspect that they would implement this
algorithm in Fortran or C, btw - wouldn't it be easier to implement/ integrate
with the various languages at a higher level?

Regards,

Max

PS: Note that Jurgen Doornik holds the copyright to the file mentioned (and the
rest of Ox, of course), and expressly disallows translating it into other
computing languages in the copyright notice at the beginning of the file.


From atp at piskorski.com  Fri Dec 16 14:38:02 2005
From: atp at piskorski.com (Andrew Piskorski)
Date: Fri, 16 Dec 2005 08:38:02 -0500
Subject: [R-sig-finance] Constrained Log-Likelihood with SQP Solver
In-Reply-To: <loom.20051216T093738-612@post.gmane.org>
References: <loom.20051216T093738-612@post.gmane.org>
Message-ID: <20051216133759.GA49679@tehun.pair.com>

On Fri, Dec 16, 2005 at 08:45:34AM +0000, Max wrote:

> I would be interested to hear why you suspect that they would
> implement this algorithm in Fortran or C, btw - wouldn't it be
> easier to implement/ integrate with the various languages at a
> higher level?

For the same reason that AFAIK nearly all real-world numerical
libraries are written in either C or Fortran - for speed.  (I suppose
there are probably some such kicking around in compiled Lisp or other
atypical languages, but they seem few and far between.)

C is also often chosen for libraries simply because any other language
you'd care about integrates easily with C (by necessity), but
integration with other languages doesn't even enter into it here.  The
original poster seemed to be talking about 2 or even 3 orders of
magnitude difference in performance, and being quite unhappy with the
slow runtimes he's seeing now.  That's more than enough speedup to
justify the added hassle and pain of writing C code.

Of course, I don't know whether R functions he was using were well
implemented or not; I was asssuming they are.  But a good C version is
likely to be much faster than R anyway.

C (and to a lesser extent Fortran, for numerial stuff) are an
omnipresent fact of programming life, especially for lower level
libraries.  You don't have to like it, but that's the way it is.

I know next to nothing about Ox.  If it is in fact as high level and
nice to use as R, but much faster, then that's interesting.  Of course
Ox (like K, another reputedly high performance vector-oriented
language), is closed source, which makes me less interested in
investigating it.  I do kind of wonder how fast A+ is, though.

  http://www.doornik.com/products.html#Ox
  http://www.aplusdev.org/

-- 
Andrew Piskorski <atp at piskorski.com>
http://www.piskorski.com/


From bruche at cemfi.es  Fri Dec 16 15:21:07 2005
From: bruche at cemfi.es (Max)
Date: Fri, 16 Dec 2005 14:21:07 +0000 (UTC)
Subject: [R-sig-finance] Constrained Log-Likelihood with SQP Solver
References: <loom.20051216T093738-612@post.gmane.org>
	<20051216133759.GA49679@tehun.pair.com>
Message-ID: <loom.20051216T144936-433@post.gmane.org>

Andrew Piskorski <atp <at> piskorski.com> writes:
> For the same reason that AFAIK nearly all real-world numerical
> libraries are written in either C or Fortran - for speed.

For the stuff that I write, the bottleneck is usually the function evaluation/
gradient evaluation, which I do of course implement in C if necessary. (In one
of my applications, a function evaluation took 5+ mins, with optimised C code...
and I need to run the maximisation on 30 different series.)

In this sort of case, it does matter what kind of algorithm you use (because
that determines the number of function evaluations), but it doesn't really
matter that much what language the algorithm is written in, as the number of
steps are typically only a couple of hundred, so that things like slow looping
in interpreted languages don't really play a role.

The problem that Diethelm mentioned sounded vaguely similar (I guess for each
evaluation of the likelihood function, you need to multiply some pretty large
vectors and matrices).

I guess there must be other types of applications, though, where it is necessary
to run the optimisation algorithm itself in a low-level language. I was just
wondering whether you had anything in particular in mind.

Ox is fairly high level, although more similar to C/C++ (I guess R is more
similar to Lisp in some ways). It is closed source, though, which I do agree is
a very important drawback. Actually, that's the reason I started using R.

I hadn't heard of A+, it sounds interesting, thanks.


From abunn at whrc.org  Sun Dec 18 18:39:35 2005
From: abunn at whrc.org (Andy Bunn)
Date: Sun, 18 Dec 2005 12:39:35 -0500
Subject: [R-sig-finance] Butterworth low-pass  filter
Message-ID: <NEBBIPHDAMMOKDKPOFFIMEAODOAA.abunn@whrc.org>

Hi r-sig: I tried this on r-help first a few days ago with no hits, so sorry
for xposting....


-----Original Message-----
From: Andy Bunn [mailto:abunn at whrc.org]
Sent: Friday, December 16, 2005 11:32 AM
To: R-Help
Subject: Butterworth low-pass filter


Has anybody implemented code to extract coefficients for a Butterworth
low-pass filter? I know Matlab has it implemented in the signal toolbox. I
want to make use of a 10 point Butterworth low-pass filter for smoothing.

In Matlab the code would look like this:
% Determine the filter coefficients
[b,a]=butter(10,0.1)
% Apply the filter to the input
outdata = filter (b,a,indata);

The archives contain similar questions but if somebody responded, they did
it off-list.

Thanks in advance,
Andy


From ajayshah at mayin.org  Sat Dec 24 16:28:38 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sat, 24 Dec 2005 20:58:38 +0530
Subject: [R-sig-finance] Puzzled in arch estimation
Message-ID: <20051224152838.GA23574@lubyanka.local>

I have attached a time-series r. I find that I am able to use the
garch() function in the tseries package to estimate a GARCH(1,1)
model. But it fails to get convergence for the GARCH(1,0) -- i.e.,
ARCH(1) -- model. I seem to think that the latter is actually a simple
likelihood function and it's much easier to get convergence for it.

There's a mysterious statement towards the end of the time-series
chapter of the MASS book which says that the garch() function requires
the series to be mean 0 (a statement which I didn't find in the
documentation of tseries). So I also did this one more way: To first
estimate an AR(1) model, and focus on the residuals. Once again, I get
the same pathology: GARCH(1,1) works but ARCH(1) does not.

I fed the same series to stata and it estimates AR(1)-ARCH(1) easily,
so this doesn't seem to be a pathological case of bad
data. Difficulties in convergence can, of course, always come about in
realworld data. I wondered if I'm missing something fundamental.

Here's some code which illustrates my problem --

library(tseries)
# take the definition of `r' ahead and then say

m.ar1 <- arima(r, order=c(1,0,0))
m.garch11 <- garch(m.ar1$residual, order=c(1,1)) # Seems to be okay
# BUT
m.arch1 <- garch(m.ar1$residual, order=c(1,0))
# breaks.

Alternatively, equally, one can do:

m.garch11 <- garch(r, order=c(1,1)) # Seems to be okay
# BUT
m.arch1 <- garch(r, order=c(1,0))
# breaks.

    -ans.

r <- c(0, 0.579142, 1.03736, -0.208672, -0.765883, 0.281692,
-0.397706, 0.328364, -0.125584, -0.193477, -0.0610791, -0.990004,
-0.71914, -0.829374, -2.08766, 0.756307, -0.637729, 0.134032, 1.63858,
-0.627174, 0.169323, -1.11391, -1.08862, -1.46264, -1.14154,
-0.446539, -3.38369, -1.61522, 1.17325, -1.00522, -0.578756,
-0.677731, -1.83069, -1.39811, -1.74083, 2.51195, -1.13877, -2.06056,
0.169783, 1.98911, 1.26197, -1.07611, 0.325627, 1.42208, 2.21524,
-1.28735, -1.10025, 0.235853, 1.10524, -0.555269, -0.104693,
-0.088162, 0.123343, 0.204409, 1.67633, 0.0161745, 0.5192, 1.87783,
-0.864954, -0.660769, -0.0564514, -0.844007, -0.492193, -0.705106,
-0.557051, -1.71258, -0.0664624, -0.614921, 1.09121, 0.214559,
-0.438513, -0.32201, -1.32279, -0.970354, -0.571795, 0.0379108,
-1.36068, -1.72062, -0.461166, 1.93922, 1.87047, 0.218172, 2.45879,
5.45718, 2.66156, -1.85931, 0.679358, 5.04796, 5.24155, -1.41465,
0.864773, 2.36954, -0.548473, -2.23819, -0.166255, -2.10307, 2.44766,
0.383161, 0.570884, -0.705994, -0.0057834, -3.45401, 0.69873,
-0.063715, 0.40676, -0.44536, -0.989944, -1.36045, 0.0988081, 1.61739,
-0.10033, -0.940164, -2.0354, -1.57128, 1.93312, -0.985401, -0.222106,
0.329964, 1.76344, 2.34919, -1.21275, 0.959598, 0.848386, 1.01093,
-0.249911, 2.07488, -0.297048, 0.0197712, -0.398818, 0.681907,
1.37138, 1.16532, 3.48685, 2.29011, 0.893023, -2.21592, -1.0093,
4.39966, -2.14394, 0.250645, -0.469434, -0.358823, 2.08695, -0.512225,
-1.11659, -2.85231, 0.46674, 1.40263, -1.6371, -1.09721, 0.874134,
1.58375, 0.94534, 1.12451, -1.16305, -2.86807, -0.833724, 0.568613,
-0.382739, 0.740059, -1.86052, -0.18797, 3.14841, -1.28442, 0.636933,
1.72372, -0.291713, 0.67605, 1.48735, 0.319893, 0.695452, 0.14971,
1.80418, 0.388463, 2.33975, -0.897786, -0.288935, -1.1847, 0.124064,
-1.03658, -0.923538, -1.67069, -0.705635, 0.636449, -0.444643,
-1.16541, -0.814853, -1.83491, 0.738011, 0.458506, 0.953355, -1.34203,
-0.196747, 0.990823, -0.739901, 0.369263, -0.910496, 2.24238,
0.955673, 0.0527268, 0.570173, -3.02393, -2.10133, -0.633178,
-1.08236, -1.65647, -1.86058, 2.73012, -0.612611, -0.356596,
-0.062947, 0.311442, 1.25523, -0.895673, -0.12806, -0.554569,
-0.752702, -0.582504, -1.60638, -2.68886, 0.172043, 2.21104, 0.297798,
1.04623, -0.363052, 0.557391, 1.21563, -0.119828, 0.305617, 0.769626,
-1.32803, 0.30639, 1.07178, -0.519918, -1.78013, -1.26352, -0.984705,
-0.208016, -0.869041, -1.89919, 0.924764, 2.3627, -1.69055,
-0.0746883, -0.841788, 0.00722487, 0.245336, -1.24108, -1.26723,
-0.345814, -0.81799, -1.21966, -2.27452, -3.38817, -0.0114423,
0.0937883, 2.67338, 1.14764, -1.85919, 0.639173, -0.558492,
-0.0784384, 0.327911, 2.65603, 3.44349, -1.96603, 0.447127, -0.113197,
-1.02026, -1.86312, -0.28745, -1.84221, -0.966311, -1.71615, 0.955527,
1.10826, -0.484987, 1.23165, 0.623006, -0.834164, 0.551664, 1.08322,
-0.718242, -2.36401, -0.77759, -2.26618, 0.226714, 0.377107, -3.13461,
-0.436373, 0.585825, -1.2202, -1.32789, -2.66419, 1.77069, 0.740183,
0.0618475, -0.681191, 4.1304, -1.18008, -0.318427, 2.70967, 0.408714,
1.12908, 0.0209183, 2.72803, -0.269478, 0.232157, 3.0907, -1.40264,
4.40068, -1.33935, 0.494971, -0.414102, 0.107724, 1.90888, 1.04031,
2.45243, 0.0356952, 0.873102, 5.17076, -3.15939, -0.760762, -2.50171,
-1.70327, 2.73174, -0.121642, 2.33087, 1.24574, -0.451202, -1.70538,
-2.7731, -2.54589, 1.05954, 0.437318, 0.543972, 1.13535, 0.270374,
1.14478, 0.827591, 0.114819, 0.725794, 1.0495, -1.91509, 1.10306,
-0.868513, -1.15811, -2.74004, 0.139835, 0.284245, 2.43691, 5.94418,
4.5823, 0.429739, 0.862927, 1.38657, -0.499968, -4.40244, -2.50635,
2.08831, -0.719394, 0.336984, 0.233351, -0.772132, -1.16236, 0.261053,
0.623762, -0.28777, 0.0802815, -0.132263, -8.84046, 0.190874, 3.25022,
0.83961, -0.959419, -1.01412, 3.89917, -2.15347, 0.499568, -1.00663,
1.15454, 1.98046, 2.42401, -0.0754575, 1.05593, -0.360108, 0.205935,
-0.374743, -0.748997, 2.09618, -1.00985, -2.22284, 0.276995,
-0.382263, 0.919733, -0.0664326, -1.22742, -0.912367, 1.02283,
0.321089, 0.324831, 0.228659, 0.00475817, -0.845742, -0.461717,
-0.338018, -0.660004, -0.116925, 0.126662, 1.08465, 1.21585, 1.32809,
0.822934, 1.11138, 0.0782563, -0.354945, -0.750907, 0.598401, 2.15042,
1.44702, 1.38239, -0.582526, 0.802297, 1.82725, 0, -0.1251, -0.402241,
0.138588, 0.569655, 0.540751, 0.124046, 1.92635, 0.67284, 1.85695,
-0.66042, 0.0987248, -0.527662, -0.951024, 3.14631, 0.210042,
-1.46736, -2.82809, -0.312184, -0.0295814, -0.135341, -1.39791,
0.705659, 2.3127, -0.0624831, -1.24949, -0.28732, 0.729361, 0.636465,
1.95096, 0.888388, 1.45, 1.80306, 1.54323, -1.81852, -1.00531,
0.111319, -0.789856, -0.308859, -1.35088, -2.17762, -1.74217, 1.47129,
-1.40356, -1.7977, -1.40148, 0.648943, 1.43528, -2.93111, -2.64789,
-0.027153, 1.1207, 1.55875, 0.215683, 1.05844, 0.780069, 0.112175,
-0.432134, -0.0216558, -0.996896, -0.162007, -0.0745238, -0.523228,
-2.21533, -0.456261, -1.76317, 0.335841, 1.11442, 0.949288, 0.426497,
0.85646, -0.168926, -0.813049, 1.76508, 2.52396, -0.568844, -1.66028,
1.09687, 1.54393, 0.805579, 0.879127, -1.1214, -0.435006, -1.56798,
-1.31104, -1.40426, -8.19628, 6.95738, -1.98433, -0.115247, 0.570326,
-0.478118, -0.37397, -0.570569, -0.831509, 0.696509, -2.67677,
-2.18641, 0.249103, -1.48915, -2.19767, 0.736262, 1.05953, -2.46615,
2.95712, 0.606645, 2.89805, -2.20224, 0.846062, -0.276371, 0.0873574,
-1.85567, -0.540127, 0.248127, -2.48401, -1.24238, 0.179898,
-0.876837, 1.5116, 2.2703, 1.0717, -0.341103, 1.83509, 2.21184,
0.322857, -0.0616421, 0.690114, 1.63988, -1.91819, 1.95525, 0.16662,
0.461383, 0.500495, -1.15166, 0.30071, -2.30368, -2.31445, -3.72083,
0.330844, -0.15526, -0.795093, 0.820151, 1.93053, -0.571796,
0.0938805, -2.00032, -0.560848, -2.41025, -1.93916, -0.365825,
2.32056, 1.35566, -0.13833, -0.751342, 0.838461, 0.158669, -0.158669,
-1.2785, -0.499325, -0.23492, 1.85382, 1.74448, -0.287799, 0.875962,
-0.391724, 1.59245, 0.444731, 2.14139, 0.936674, 1.41941, 3.46963,
-3.21071, 2.6766, -0.827325, -0.175536, 1.65528, -1.23083, -0.281229,
-0.509143, 1.19469, -0.657852, 1.83391, 0.221805, 0.343054, 1.29831,
-0.289539, 2.30189, -1.53312, 0.956018, -1.36855, -0.70927, 2.92919,
0.632722, 1.3772, 1.15652, 0.0589499, 1.81869, -1.43216, -2.80223,
3.96121, -0.769763, 1.07602, -0.235815, 0.46283, -0.160921, -2.39888,
-0.704617, -0.808336, -1.51875, 0.374255, 0.71415, 2.20098, 0.248604,
-1.32598, -0.934026, -0.107673, -2.44718, -3.42747, 2.21905, -0.76725,
0.87, 0.173987, 1.7278, -1.33632, 0.0355129, -0.320071, -1.66117,
-1.22986, -1.42167, -1.1596, -0.888097, -3.29448, 1.39277, -0.669613,
-3.31631, -2.53823, 2.69413, -3.28434, -0.161102, 1.01424, -5.75916,
-0.733486, 7.04203, -2.10599, -4.59051, -3.68605, -0.523979, 3.68855,
2.56989, -0.840296, 2.64234, -0.788051, 0.0636977, -2.40057, -2.12593,
1.84834, 0.798028, 2.08157, 1.77414, 1.96792, -1.62513, 0.310607,
0.417796, 3.30588, -0.309227, -2.39112, -1.21476, 0.0207125, -3.29486,
-1.54223, -2.28101, 1.97078, 2.221, -0.561364, -0.128755, -2.76495,
0.0220726, 0.137844, -0.519281, -1.13631, -1.55821, -1.40383,
-0.741345, 1.40274, -0.938758, -1.11135, -2.62019, 0.700287, 3.56218,
-1.53702, 0.507216, 1.39749, 0.634524, -3.20189, 0.352403, -2.16324,
0.9481, 0.467747, 2.3411, 2.34956, 1.43815, 0.404758, 0.419658,
-0.852199, 2.28556, 0.0325857, -0.686539, -0.482298, -0.932807,
-0.439048, -0.0501407, 1.16345, 2.93636, 0.0160449, -0.756908,
-2.12392, -0.380512, -7.35853, 0.592946, 3.31426, -3.05447, 0.135529,
-1.57876, -2.30563, 3.22861, -0.940931, 0.459661, -0.987588, -2.75026,
0.357959, 0.705979, 3.3565, -0.43276, -0.925131, -0.192065, -0.409368,
-0.586867, -0.0971345, -0.279789, 0.999978, 1.39527, 2.7971, 0.467304,
0.0172657, 0.333219, -0.51754, -1.38165, 0.542165, 1.73486, -1.14359,
0.0462267, -0.940282, -1.15547, -0.686678, -2.07666, -1.46621,
-0.197129, 0.847359, -0.134606, 0.549519, -0.50055, 1.37355, 2.00784,
0.924542, 0.415315, 0.912293, 0.645759, -0.738351, -2.17139, 0.766242,
-2.53314, 0.594436, 2.50663, 0.105036, 0.16317, 0.279102, 2.15408,
-0.821688, 3.08046, -1.75994, 0.738011, 0.782739, 1.04156, 2.29382,
2.80961, 3.63583, -0.450485, -2.16926, -0.337549, -0.673298, -2.16519,
1.12815, -2.22588, 4.5585, -1.05497, -0.76042, 2.23839, -1.68193,
1.15548, -2.73314, -0.956536, 2.2511, -1.34244, -0.362474, -2.13739,
0.896136, 3.68368, 0.0625163, 1.05684, -1.63137, 0.454677, 1.05294,
0.453025, 0.292345, -2.32635, -0.441293, 0.838887, -0.350428,
-1.38224, 4.17226, 3.45535, 3.4874, -1.01766, 1.26454, 1.66, 0.581275,
0.0463564, -1.80042, 1.22407, 0.121122, -1.95578, 0.709625, 1.30666,
-1.09475, -0.240198, 0.249606, 0.931676, -0.950493, -0.746096,
-1.30241, 1.27396, 2.19923, -1.36355, -5.01828, 1.25763, 0.666588,
-1.0432, -2.67676, 0.482025, -0.396497, -2.6137, 7.5394, -7.70989,
2.74898, 0.150807, 0.125499, -1.51648, 0.0356479, -5.29562, 1.29612,
2.41884, 1.19294, -0.764518, 3.80016, 1.1046, 2.65198, 1.51198,
2.90225, 1.5147, 0.458037, 2.83575, 1.49131, -1.16087, 0.713655,
1.7177, -1.13756, -1.82039, 0.500556, 0.744012, -2.15193, -3.9566,
-0.915812, 4.5902, -0.753516, 0.120056, 1.0169, 1.60583, 2.04806,
-1.34494, -0.0515952, -1.79653, -1.3223, -0.569549, 2.43769, 2.42645,
-0.102063, 2.28406, 0.993139, -1.75709, -0.998134, -0.36467, 1.28315,
-0.0755192, -0.302648, -0.379603, 1.23057, 2.66891, 0.890154,
0.197187, 0.0281379, 0.796626, 4.77643, 0.462683, 1.0314, -1.31559,
0.495847, 1.8962, -2.61997, 1.9173, 0.673028, -0.464314, -1.25887,
-1.23674, 0.779486, 0.423313, -1.1799, -1.58095, -0.353446, 3.16709,
-0.233989, -0.492407, 0.828123, -1.36495, 0.42285, -0.441939,
-1.35701, 1.38755, 1.048, 2.95141, -1.35509, 0.170858, 1.64891,
0.941021, -0.798755, 2.2242, 1.06384, 0.359143, -0.747904, -0.0921104,
-2.50143, -0.0654355, 1.10303, -0.501182, 1.16792, -0.185953,
-0.416069, -1.48827, 0.302341, -1.72424, 0.6271, -0.136152, -0.535363,
2.61976, 0.829612, 1.01415, -1.62402, -0.74766, 2.58742, -0.155565,
-0.703053, -2.22672, 0.425386, 1.05023, 5.3848, 0.644288, 0.320594,
-0.0101083, 0.835533, 0.593039, -3.42292, -0.724491, 2.30013,
-1.65818, -1.6022, 1.35764, -1.64121, -0.83435, -0.208794, -3.14516,
-3.13785, -4.27351, 4.78148, -0.436321, 0.781019, 2.0546, 0.369402,
0.116754, 1.29698, 0.0359881, -1.16173, -0.79306, -0.363911,
-0.405905, 0.883496, -0.176076, 0.979146, 1.4223, 0.977323, -0.644532,
-1.07391, -0.615765, 0.911432, 1.43697, 0.647366, 1.73729, 0.15928,
-0.450795, 0.61673, -0.543771, -0.840502, -2.08083, 0.225028, 1.29738,
-0.0281789, 0.221694, -0.0527398, 4.13367, 0.447805, -1.99169,
-1.86094, 0.702783, 2.34031, 0.277327, 7.27706, 2.87865, -2.65281,
1.35684, -0.26618, 1.21064, -3.77215, 3.2718, -0.209476, 0.0832267,
-0.689477, -0.304509, 1.73687, -2.08602, 1.21056, -0.432874, -1.70004,
1.09709, -0.299719, -3.36407, 0.213199, 2.45431, 0.62149, 0.11571,
2.27736, 1.58519, 1.60485, 1.26735, 2.58436, -0.657051, -2.43409,
0.500931, 1.79548, -1.40469, 2.05694, -0.82748, -2.48306, 2.07685,
-1.25203, 0.704926, -4.01256, 3.43909, -0.94743, -2.41917, 1.94355,
0.840403, -2.1609, -1.21356, -2.67791, -2.65865, 0.406043, 3.3293,
-3.63928, -0.359113, 2.09785, -2.30363, 1.03429, -0.421389, 0.360844,
-0.662011, -0.56311, -1.36781, 0.411335, -7.20226, 0.457603, 1.26751,
6.92613, 3.52386, 0.719661, -1.98617, -4.76089, -5.07164, -2.01172,
-0.698646, 0.758707, -1.97249, -2.07837, 5.48509, -1.34597, -0.733149,
-5.33703, -1.31347, 4.78471, 2.98636, -4.11545, 0.984116, -1.1234,
-4.394, -1.6813, 1.2742, 0.583249, 0.366623, -1.40115, -1.98336,
-2.79906, -0.69993, 0.934852, 0.946233, 2.19589, 2.76077, 2.54541,
2.61271, -2.3046, 2.94005, 1.09885, 1.21359, 0.603066, 2.30141,
0.242251, -1.8435, -0.522049, 0.181286, 0.67691, 2.19338, 1.44153,
0.555585, -2.14276, 0.884125, -1.08431, -1.4057, 0.182384, 1.07377,
1.50897, -1.41037, 1.60451, 1.06768, 0.971249, -0.607985, 0.0527287,
-0.52523, 0.58781, 0.969895, -0.703548, -0.847533, -2.01385, -1.12481,
-1.88353, -0.811193, -1.91042, -5.85803, 2.33637, -2.29843, 1.49082,
-0.318132, -0.0712504, -0.451179, 0.379878, -0.802824, -0.204561,
-0.589501, 2.62024, -0.0483173, -1.26828, -1.30746, 0.544007, 2.51385,
-0.746425, 1.2336, 0.894336, 0.795948, 0.350529, 0.0649117, -0.411821,
-0.828764, -0.157075, 0.60139, 1.31046, 2.38507, 0.035014, 0.49588,
0.306077, 0.712855, 0.433527, 0.772918, -0.786652, -0.747905,
-1.96338, -4.53615, -2.79652, 1.9475, -0.97653, -4.88484, 2.03993,
-1.9255, 1.9255, -2.04388, 0.413704, 0.810609, 1.22492, -1.01064,
0.0194571, -1.38701, -2.26244, -3.03606, 0.361274, -2.476, -0.110535,
-1.49135, -1.92242, 2.61944, 0.500397, -2.42245, 0.740293, 2.6924,
0.202515, -0.642708, -0.984726, 0.478654, 2.36366, 1.98304, 1.39449,
-0.145027, 0.522719, 1.59539, 0.248349, -2.42291, -2.57409, 2.83996,
0.337363, -1.11278, 0.22274, 0.129366, -0.210305, -1.02957, -0.520845,
0.753731, 2.23568, 1.5719, -0.630544, 0.268467, 0.632777, -0.0470256,
0.706965, 1.07619, 0.303724, 0.856209, 1.39466, 0.0900394, 1.55901,
-0.366172, -2.76131, 0.3802, -0.540316, -1.1705, -1.3877, -2.81038,
-2.44511, 1.33592, 1.66721, 1.1622, -0.734757, 1.38556, 1.51775,
1.26209, 1.48775, -1.36547, 0.183143, -1.87389, -0.537447, 0.494713,
0, 0.488411, 0.37438, 0.618317, 1.75713, 1.412, 0.532703, 0.790111,
0.303357, -2.06854, 2.76678, -0.581524, -0.919134, 1.43903, 0.271597,
0.324945, -1.18207, 1.78583, 0.728263, -0.249297, -0.787575, 0.154424,
1.66193, -2.52689, 0.249444, -0.0686255, -0.998574, -1.10085,
-2.59027, -0.611507, -1.29222, 4.22058, 0.490876, -3.88129, -2.7079,
1.48718, 0.181934, -2.99127, -4.63246, -6.30954, 5.99602, 1.90356,
-1.958, -0.575571, -1.3361, 3.04054, -1.63284, -2.23523, 0.0172206,
1.38936, 2.38691, -0.924506, -4.00344, -0.883529, 0.974935, -1.10242,
0.259198, -0.992094, -2.26773, -3.34156, -4.00685, 1.9039, 2.12169,
3.35453, 3.65279, -0.0393279, 0.501363, -0.300516, 0.786396, -1.0091,
-3.7821, 2.15139, 1.05639, -1.34117, 0.710451, 0.806438, 0.852222,
0.0261074, -0.374859, -0.389421, 0.0263008, 0.393684, 0.509483,
2.04642, -0.183154, -0.28605, -0.115506, 0.937294, 0.232957,
-0.589797, 1.54557, 0.439028, -1.7593, -0.822874, -1.71424, -1.83282,
-1.03443, 0.00896338, -0.300712, 1.27294, 0.398636, -0.349829,
0.163996, -1.44979, -2.27231, -0.872562, 1.68744, 0.08659, -0.218898,
-0.691759, -1.91684, 2.73635, -0.0456059, -0.191772, 1.26256,
-0.647457, -2.85201, -0.173079, 0.168405, -0.435628, -0.527159,
1.17756, 1.07623, 1.99627, 0.446762, -0.442239, -0.221855, -1.01593,
-0.555595, -0.758, -0.65632, 0.177305, -0.781565, -1.02003, -0.161513,
0.9228, 1.06828, -0.908246, 0.267713, 0.803517, 0.0604693, -0.293385,
-0.38316, 0.24782, 0.0466897, -0.763774, 1.16906, 0.320268, -0.907764,
-0.51103, 0.464256, 0.261658, -0.21956, 0.317505, -0.177305,
-0.299331, -0.309627, -0.982113, -0.542394, 0.0143113, -0.305752,
-0.855322, -0.086902, -0.174031, -0.972392, -4.10907, -1.07478,
-5.49996, -5.29714, 3.15409, 1.32423, -1.47987, -5.08952, 1.72353,
-0.884169, 1.41781, 1.84844, 2.6445, -0.411196, -1.15487, 1.32503,
0.323067, -1.39277, 1.18482, 2.98449, 1.53545, 0.574324, 0.311883,
0.811521, 1.5326, -1.45026, 0.47211, -0.025601, 1.70597, -0.201572,
-0.810377, -0.586542, -1.47899, 0.909567, 2.24843, 0.361519,
-0.658741, 1.08885, -1.4477, 1.02761, 0.634447, 0.67992, -0.545555,
1.0291, 1.9401, 3.08507, -1.69476, 0.583895, 0.575799, -0.32525,
2.01914, -0.421951, -0.48909, -0.341449, -0.164123, 1.14788, 2.46089,
0.532733, 0.166461, 0.264865, -0.453842, -0.229953, -0.806748,
-0.99699, -0.511486, -2.01122, 0.117772, -1.05546, -0.223879, -1.3684,
-1.38739, 1.34387, 2.41309, -0.354719, 0.515112, 1.0783, 2.20904,
0.359688, 0.882339, -0.641752, -0.417992, -0.882594, 1.93333, -1.4202,
-0.352492, 1.71861, -1.45756, -0.164797, 0.13735, -0.316188,
-0.377064, -0.480282, -0.813409, 0.0279981, -0.392689, 0.742006,
0.579497, -0.440111, -0.24638, 3.55262, -0.238358, 1.1906, 0.691707,
-0.181332, 0.49457, 1.30412, 0.861496, 1.10598, -1.19654, -1.12373,
0.339751, 1.18012, 0.167457, 2.03417, -0.0168166, -4.04559, 3.09932,
-0.0551935, 0.0976293, -0.501894, 1.72895, -0.453649, -1.68121,
-1.50113, 0.572049, 0.207209, 0.88443, -0.0384772, -1.47755, 0.298993,
-0.991399, -0.503801, -1.36195, 0.0267094, 0.550403, 0.828746,
-0.175755, -1.19004, 1.97415, -0.345303, -0.588444, -0.755988,
1.04186, 0.446957, 0.253264, -1.08304, -1.36715, 0.565993, 0.346036,
-2.57494, 0.349294, 0.167409, 0.415051, -1.47855, 0.282886, -2.13675,
0.954285, 0.808159, 0.333296, 0.363984, 0.8817, 0.619308, 0.890795,
-0.998226, 0.290691, -0.407205, -0.656802, -1.36329, -0.196936,
-1.5058, -2.36879, -0.372404, -1.79055, 3.84525, -0.403813, -2.33244,
0.331755, -0.916199, -0.325093, 1.05872, 0.541929, 1.78238, 0.0140941,
-1.47176, 1.99185, 2.50596, -0.388155, -0.914676, 0.262849, 0.294307,
-1.28939, -1.12731, 0.670435, -0.70337, -0.065901, -0.609283,
-1.06688, 0.415722, 0.878302, 1.04856, -0.0842302, 0.173063,
0.0607349, 0.303122, 0.765363, -0.161861, -0.799261, -1.419, 0.156039,
-0.973301, -1.15647, -0.328741, 0.843846, -0.519932, -2.3342,
0.973507, -1.76218, -0.249302, -2.84063, -0.190217, -1.13855,
-0.182334, -0.125222, -0.308505, 0.886346, 0.35235, 0.253132,
-1.61759, 0.877061, 0.817895, 0.637239, -0.657863, 0.985179,
0.0612526, 0.883972, -0.0101163, -0.278601, 0.959167, 0.36609,
-1.12256, -0.202696, 0.157125, 2.33762, 0.286547, -1.23103, 0.582656,
0.163727, -1.33748, 0.33605, 0.0300391, 0.27993, -0.968081, -0.632033,
0.923946, -1.14229, -0.77055, -0.66302, 0.0721687, -0.423445,
0.397676, -0.0154643, 0.673057, -1.37144, -0.828842, -0.735529, 0,
0.688408, 0.631674, -0.631674, 0.386787, 1.30606, 0.14407, 0.118188,
-0.0308182, -0.16967, -0.443528, -0.502631, -0.536502, -1.09756,
-1.56461, -1.02432, 1.52724, 0.0906836, 1.44512, 0.00525527, 1.11313,
0.0207857, -0.166407, -0.391104, -0.303506, 0.606094, 0.291288,
0.956302, 1.88055, 0.654189, 0.475369, 0.711322, 1.12377, 0.591298,
0.964926, -0.488573, 1.78782, 0.0428602, 1.67611, -1.21533, -1.77876,
0.917239, 2.25462, -1.04772, 0.475888, 0.567158, 0.675442, 0.850597,
-0.716054, -0.48334, 0.436966, -0.181063, 0.306222, -0.306222,
0.832953, 0.899171, 0.328288, -0.588949, 0.141847, 0.606297,
-0.647458, -0.31613, -0.482993, -0.235441, 0.695487, 0.731699,
-1.57057, -0.60353, 0.483115, 0.559164, 0.30828, -0.170127, -0.938583,
0.143902, 0.462792, -1.11432, -1.39639, -1.7577, 0.820601, -0.863978,
-0.25099, 0.69831, 1.28271, -0.0473911, -0.704027, 1.53485, -0.575175,
-0.845169, -0.0572355, -0.339315, -0.812329, 2.12022, 0.103896,
0.470899, 0.122071, 0.0516008, 0.37448, -1.37369, -0.560518, 0.313897,
0.987557, -0.428791, -1.16366, -0.565325, -0.91219, -1.38162,
-1.02778, 0.776751, -1.27466, -0.204862, -0.667455, 1.0917, 2.10441,
0.515616, 0.638393, -2.26723, -0.256765, 0.251833, -1.10586,
-0.209654, -2.2641, 0.621658, 1.52244, 0.970857, 0.769956, 1.42061,
-1.30759, -1.30999, -4.33712, -1.29709, 0.147291, 0.78017, -1.89018,
0.688599, -0.39139, -0.990582, -0.482859, -0.582526, 0.561011,
0.300784, 0.187532, 0.453976, 0.753839, 0.679934, -0.178759,
-0.909241, -0.393743, -0.197455, 0.872253, 0.838458, 0.805444,
1.37098, -0.675382, 0.51597, -0.366065, -0.49191, 0.481579, 1.49207,
-0.571634, 1.41796, 1.18392, 0.418036, 0.82594, -0.44427, 1.02378,
1.36182, 1.0906, 0.543247, -1.36851, 0.605218, 0.687222, 0.465007,
-0.417458, 2.8262, 0.442662, 0.532282, 0.702301, -1.00939, -0.354097,
1.94349, 0.8727, 0.820737, 0.761167, -0.304656, 0.273791, 0.952409,
-0.543123, 0.184291, 0.467975, -0.424146, 1.84949, -0.060241,
0.844357, -0.999429, 0.764412, -1.44352, -1.04713, -2.14566, -0.59326,
0.884108, 1.80656, 2.02422, 0.553187, 0.47356, 0.699823, 0.240623,
0.831379, 0.654346, -1.60385, -1.13777, 2.18759, 2.1244, 0.830793,
0.153996, 0.979195, 0.0681458, 2.66113, -0.289164, 0.756309, 1.04701,
0.780985, -3.10219, 3.63845, 1.66263, 0.055942, 1.14918, 1.41997,
0.688059, -1.90184, 0.977296, 1.85491, 1.34602, -0.729362, 0.177519,
-0.45508, -2.23773, -3.17275, 2.13613, -1.21133, -2.96926, 1.50889,
-1.46667, 1.92321, 3.24813, -1.08822, 2.16833, 0.932943, 1.2176,
0.264275, 1.98254, 2.02179, -0.0710239, 0.0507365, 1.57684, 1.38836,
1.54082, -1.69194, 1.0596, 1.20931, 0.879963, -1.71911, -2.3745,
-0.826506, -1.59555, 2.39219, 1.05021, -2.43756, -0.239295, 1.12074,
1.22046, 2.54183, 2.89802, 1.0589, -0.591727, 0.189362, -1.25772,
0.153771, 0.416191, 0.165369, -1.49826, -1.8848, 0.793387, 1.08825,
-0.985919, -1.53304, -1.19496, 1.20145, 0.207482, 1.59037, 1.87565,
1.05179, 2.59112, 0.0512643, 0.72094, 0.280958, -1.7706, 0.0273386,
1.78205, 0.657203, 0.502618, 0.206228, 1.46372, 0.710945, -0.172936,
1.30972, 1.2703, 0.594222, -0.495876, 1.58265, 1.55527, 1.99408,
-0.0426974, 0.34639, 1.71418, 1.75211, 0.458852, -1.45815, -0.517765,
2.66662, 0.170031, -1.34271, 0.920911, 0.940259, -1.9203, -2.27832,
1.80923, -2.19933, -3.69341, -3.00988, 4.25984, 3.04641, -2.20827,
-1.05216, -1.85315, -2.27743, 2.96301, -0.976102, 1.6025, 2.53355,
0.00265855, 0.569953, -0.328321, 1.48993, -0.00261291, 0.341711,
-0.190275, -3.08124, -0.304504, -2.42852, 0.724611, -1.91517,
-1.18225, 1.93495)

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From Michael_Parzen at bus.emory.edu  Sun Dec 25 12:07:05 2005
From: Michael_Parzen at bus.emory.edu (Michael Parzen)
Date: Sun, 25 Dec 2005 06:07:05 -0500
Subject: [R-sig-finance] R-sig-finance Digest, Vol 19, Issue 11
Message-ID: <fc.000f464a07adb7813b9aca003b9754f9.7adb782@bus.emory.edu>

I will not be reading email  between Dec 24th and January 1st.


From Michael_Parzen at bus.emory.edu  Mon Dec 26 12:12:34 2005
From: Michael_Parzen at bus.emory.edu (Michael Parzen)
Date: Mon, 26 Dec 2005 06:12:34 -0500
Subject: [R-sig-finance] R-sig-finance Digest, Vol 19, Issue 12
Message-ID: <fc.000f464a07ae95b73b9aca0061806120.7ae95b8@bus.emory.edu>

I will not be reading email  between Dec 24th and January 1st.


From Michael_Parzen at bus.emory.edu  Tue Dec 27 12:17:25 2005
From: Michael_Parzen at bus.emory.edu (Michael Parzen)
Date: Tue, 27 Dec 2005 06:17:25 -0500
Subject: [R-sig-finance] R-sig-finance Digest, Vol 19, Issue 13
Message-ID: <fc.000f464a07af90c63b9aca004ba7146d.7af90c7@bus.emory.edu>

I will not be reading email  between Dec 24th and January 1st.


From spencer.graves at pdf.com  Fri Dec 30 04:19:16 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 29 Dec 2005 19:19:16 -0800
Subject: [R-sig-finance] Puzzled in arch estimation
In-Reply-To: <20051224152838.GA23574@lubyanka.local>
References: <20051224152838.GA23574@lubyanka.local>
Message-ID: <43B4A734.5080104@pdf.com>

	  Have you received a reply to this post?  I haven't seen one.

	  I ran the script you included with your post.  For the garch(1,1) 
model, I got an answer with a message "FALSE CONVERGENCE".  For the 
arch(1,0) model, I got a message, "Warning: singular information" with 
parameter estimates that raise questions in my mind about what the 
algorithm did.  You say the first one "Seems to be okay",  but the 
second one "breaks."  What do you mean by "breaks"?

	  I also did 'RSiteSearch("garch")'.  This revealed that the fSeries 
package also has a garch modeling function.  Have you looked at that? 
I'm sorry, but I've done very little with garch, and I don't have the 
time now to study it more deeply.

	  If you'd like more help from this listserve, PLEASE do read the 
posting guide! "www.R-project.org/posting-guide.html".  Anecdotal 
evidence suggests that posts more consistent with this guide tend to get 
more useful replies quicker.

	  hope this helps.
	

 > sessionInfo()
R version 2.2.0, 2005-10-06, i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
  tseries      zoo quadprog
"0.10-0"  "1.0-3"  "1.4-7"
 >

Ajay Narottam Shah wrote:

> I have attached a time-series r. I find that I am able to use the
> garch() function in the tseries package to estimate a GARCH(1,1)
> model. But it fails to get convergence for the GARCH(1,0) -- i.e.,
> ARCH(1) -- model. I seem to think that the latter is actually a simple
> likelihood function and it's much easier to get convergence for it.
> 
> There's a mysterious statement towards the end of the time-series
> chapter of the MASS book which says that the garch() function requires
> the series to be mean 0 (a statement which I didn't find in the
> documentation of tseries). So I also did this one more way: To first
> estimate an AR(1) model, and focus on the residuals. Once again, I get
> the same pathology: GARCH(1,1) works but ARCH(1) does not.
> 
> I fed the same series to stata and it estimates AR(1)-ARCH(1) easily,
> so this doesn't seem to be a pathological case of bad
> data. Difficulties in convergence can, of course, always come about in
> realworld data. I wondered if I'm missing something fundamental.
> 
> Here's some code which illustrates my problem --
> 
> library(tseries)
> # take the definition of `r' ahead and then say
> 
> m.ar1 <- arima(r, order=c(1,0,0))
> m.garch11 <- garch(m.ar1$residual, order=c(1,1)) # Seems to be okay
> # BUT
> m.arch1 <- garch(m.ar1$residual, order=c(1,0))
> # breaks.
> 
> Alternatively, equally, one can do:
> 
> m.garch11 <- garch(r, order=c(1,1)) # Seems to be okay
> # BUT
> m.arch1 <- garch(r, order=c(1,0))
> # breaks.
> 
>     -ans.

> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915


From wojciech.slusarski at gmail.com  Fri Dec 30 11:26:16 2005
From: wojciech.slusarski at gmail.com (Wojciech Slusarski)
Date: Fri, 30 Dec 2005 11:26:16 +0100
Subject: [R-sig-finance] Puzzled in arch estimation
In-Reply-To: <43B4A734.5080104@pdf.com>
References: <20051224152838.GA23574@lubyanka.local> <43B4A734.5080104@pdf.com>
Message-ID: <5e64e5be0512300226ie9b8e2el@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20051230/dd9e155b/attachment.pl

