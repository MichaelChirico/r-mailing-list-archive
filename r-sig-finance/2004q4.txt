From James.Callahan at CityofOrlando.net  Fri Oct  1 16:20:59 2004
From: James.Callahan at CityofOrlando.net (James.Callahan@CityofOrlando.net)
Date: Fri Oct  1 16:21:08 2004
Subject: [R-sig-finance] How and when to USE ts and its objects?
Message-ID: <OFCEE74C8C.F6163DBF-ON85256F20.00437047-85256F20.004EBB37@ci.orlando.fl.us>

Creating specialized objects for use within a single R package was for me 
the easy part
(the previous question on this list). My questions are:

1. How portable is the object to other R packages? 
Which packages will accept the object as is? 
Will the object work in LM or other linear model packages?

2. If another R package won't accept the object as is, is there a function 
available to
coerce the object and/or the objects values to the appropriate type?

Is there documentation that deals with object compatibility within R?

As far as I can tell, Factors are accepted in almost every package in R, 
but times series objects
(ts or its) are restricted to a few specialized packages.

3. Can the time series objects be stored in and retrieved from a 
relational database (perhaps via RODBC)?

Before I can recommend R to my coworkers, I will have to build either a 
compelling demo or a GUI,
that performs useful work. Before I can even begin to DESIGN the demo or 
GUI I need to know
which parts of R are compatible. If I build a GUI that creates time series 
objects, will it crash and
burn if I try to incorporate parts of John Fox's RCommander GUI?
http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/

If different parts of a GUI require different objects, can I make the 
transition transparent to the user of the GUI?

The upcoming RMetrics modules particularly fBonds and fPorfolios look 
useful:
http://www.itp.phys.ethz.ch/econophysics/R/pdf/DocFactsheet.pdf

Does RMetrics use ITS objects or does it have its own time series objects 
(timeDate / timeSeries Classes)?

It might be neat to combine an RMetrics backend with EPRI's new RPad GUI 
fontend:
http://tolstoy.newcastle.edu.au/R/packages/04/0051.html
http://www.rpad.org/Rpad/

Look at the RPad screenshot and imagine Yield Curves and Efficiency 
Frontiers.

Even without RMetrics, Patrick Burns (Burns Statistics) seems to be making 
some progress in this area.
http://www.burns-stat.com/

The City I work for has a variety portfolios ranging from long term 
defined benefit pension plans 
to short term cash funds each with its own investment policy and benchmark 
portfolios.

Do our investment policies achieve their intended purposes? Do they 
overconstrain the portfolio?
Do they provide a false sense of confidence in meeting risk management 
objectives?

These are not just theoretical concerns, as a Budget Analyst, I am 
painfully aware of the fact
that when our defined benefit pension funds fall short of their 
actuarially defined investment targets 
-- the City's General Fund has to make up the difference.  It could be 
worse,  absent vigilance, 
we could have an LA-style (Orange County, CA) derivative meltdown.

Orange County (November 1994): Orange County, California has an investment 
pool 
that supports various pension liabilities. The pool lost USD 1700 MM from 
structured notes 
and leveraged repo positions.
http://www.riskglossary.com/articles/risk_management.htm

A quick check of Yahoo shows that the Dow Jones Industrial Average is 
almost exactly where it was 5 years ago.
http://finance.yahoo.com/q/bc?s=^DJI&t=5y
Interest rates are near zero and oil is nearly $50 a barrel.

So my real concern is, in a tight budget environment, can the City of 
Orlando use open source software 
to assist its development and monitoring of investment policies? or would 
the City get bogged down in 
issues of times series object compatibility?

In my own area, Budget, assuming I can get the data out of our financial 
system into a convenient relational data base, 
and into R via RODBC -- what sort of time series objects should I build 
for time series and econometric modeling? 
-- or should I just use GRETL? or MS Excel? or RGnumeric?
http://gretl.sourceforge.net/
http://www.omegahat.org/RGnumeric/

Jim Callahan, MBA
Budget Analyst
Management, Budget & Accounting
City of Orlando
(407) 246-3039 office
(407) 234-3744 cell phone
	[[alternative HTML version deleted]]

From ggrothendieck at myway.com  Fri Oct  1 19:16:26 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri Oct  1 19:16:34 2004
Subject: [R-sig-finance] How and when to USE ts and its objects?
Message-ID: <20041001171626.4523512D4D@mprdmxin.myway.com>



ts is for regularly spaced time series and works best with
monthly or coarser time series since there is no explicit
date support for finer granularity in ts.  If such date
support is not important then ts is fine for finer
granularity series as well.  ts is defined in the stats
package which comes out-of-the-box with R so its normally
always available and is therefore the most likely to be
supported.

If you need irregularly spaced time series such as those
that are defined for weekdays but not holidays or weekends then
you can choose from a number of options:

1. The irts class in the tseries package changes the
frequency of the time scale to simulate a irregular time
series with a regular one in such a way that its reversable.

2. The its class in the its package uses S4 constructs to
define a time series object with a POSIXct time scale.

3. The zoo class has _not_ hard coded in the datetime class
so it can use just about any datetime class (e.g. Date,
chron, POSIXct).  In most cases zoo implements methods which
simply extend base generics to achieve interface
compatibility with ts and the base.  This also means that in
most cases the user does not have to learn a whole new set
of commands making it easier to use. A major new version of
zoo is in development so it is being actively maintained and
enhanced.  The current version of zoo on CRAN will import ts
and irts objects (as well as matrices and vectors) and the
upcoming version will also import its objects.  Its also
easy to extract the data and datetime.   

I personally use ts for my regular time series and zoo for
my irregular time series and have contributed to the
development of the latter.

Hope that helps.

---

Creating specialized objects for use within a single R package was for me
the easy part
(the previous question on this list). My questions are:

1. How portable is the object to other R packages?
Which packages will accept the object as is?
Will the object work in LM or other linear model packages?

2. If another R package won't accept the object as is, is there a function
available to
coerce the object and/or the objects values to the appropriate type?

Is there documentation that deals with object compatibility within R?

As far as I can tell, Factors are accepted in almost every package in R,
but times series objects
(ts or its) are restricted to a few specialized packages.

3. Can the time series objects be stored in and retrieved from a
relational database (perhaps via RODBC)?

Before I can recommend R to my coworkers, I will have to build either a
compelling demo or a GUI,
that performs useful work. Before I can even begin to DESIGN the demo or
GUI I need to know
which parts of R are compatible. If I build a GUI that creates time series
objects, will it crash and
burn if I try to incorporate parts of John Fox's RCommander GUI?
http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/

If different parts of a GUI require different objects, can I make the
transition transparent to the user of the GUI?

The upcoming RMetrics modules particularly fBonds and fPorfolios look
useful:
http://www.itp.phys.ethz.ch/econophysics/R/pdf/DocFactsheet.pdf

Does RMetrics use ITS objects or does it have its own time series objects
(timeDate / timeSeries Classes)?

It might be neat to combine an RMetrics backend with EPRI's new RPad GUI
fontend:
http://tolstoy.newcastle.edu.au/R/packages/04/0051.html
http://www.rpad.org/Rpad/

Look at the RPad screenshot and imagine Yield Curves and Efficiency
Frontiers.

Even without RMetrics, Patrick Burns (Burns Statistics) seems to be making
some progress in this area.
http://www.burns-stat.com/

The City I work for has a variety portfolios ranging from long term
defined benefit pension plans
to short term cash funds each with its own investment policy and benchmark
portfolios.

Do our investment policies achieve their intended purposes? Do they
overconstrain the portfolio?
Do they provide a false sense of confidence in meeting risk management
objectives?

These are not just theoretical concerns, as a Budget Analyst, I am
painfully aware of the fact
that when our defined benefit pension funds fall short of their
actuarially defined investment targets
-- the City's General Fund has to make up the difference. It could be
worse, absent vigilance,
we could have an LA-style (Orange County, CA) derivative meltdown.

Orange County (November 1994): Orange County, California has an investment
pool
that supports various pension liabilities. The pool lost USD 1700 MM from
structured notes
and leveraged repo positions.
http://www.riskglossary.com/articles/risk_management.htm

A quick check of Yahoo shows that the Dow Jones Industrial Average is
almost exactly where it was 5 years ago.
http://finance.yahoo.com/q/bc?s=^DJI&t=5y
Interest rates are near zero and oil is nearly $50 a barrel.

So my real concern is, in a tight budget environment, can the City of
Orlando use open source software
to assist its development and monitoring of investment policies? or would
the City get bogged down in
issues of times series object compatibility?

In my own area, Budget, assuming I can get the data out of our financial
system into a convenient relational data base,
and into R via RODBC -- what sort of time series objects should I build
for time series and econometric modeling?
-- or should I just use GRETL? or MS Excel? or RGnumeric?
http://gretl.sourceforge.net/
http://www.omegahat.org/RGnumeric/

Jim Callahan, MBA
Budget Analyst
Management, Budget & Accounting
City of Orlando
(407) 246-3039 office
(407) 234-3744 cell phone
     [[alternative HTML version deleted]]

From ajayshah at mayin.org  Mon Oct  4 18:13:31 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue Oct  5 06:44:52 2004
Subject: [R-sig-finance] How and when to USE ts and its objects?
In-Reply-To: <OFCEE74C8C.F6163DBF-ON85256F20.00437047-85256F20.004EBB37@ci.orlando.fl.us>
References: <OFCEE74C8C.F6163DBF-ON85256F20.00437047-85256F20.004EBB37@ci.orlando.fl.us>
Message-ID: <20041004161331.GS23573@igidr.ac.in>

> 1. How portable is the object to other R packages? 
> Which packages will accept the object as is? 
> Will the object work in LM or other linear model packages?

As an example, "ITS objects" are basically matrices, so it isn't that
foreign. One can say x <- core(itsobject) and get back a raw numerical
matrices.

If d.prices is an ITS object containing daily stock prices, I say:
  d.returns <- data.frame(core(100*diff(log(d.prices))))

to get a "data frame" containing daily returns. After that, I'm home
free i.t.o. doing things like feeding d.returns to lm() and so on.

> Does RMetrics use ITS objects or does it have its own time series objects 
> (timeDate / timeSeries Classes)?

I'm curious about this too.

> The City I work for has a variety portfolios ranging from long term 
> defined benefit pension plans 
> to short term cash funds each with its own investment policy and benchmark 
> portfolios.
> 
> Do our investment policies achieve their intended purposes? Do they 
> overconstrain the portfolio?
> Do they provide a false sense of confidence in meeting risk management 
> objectives?
> 
> These are not just theoretical concerns, as a Budget Analyst, I am 
> painfully aware of the fact
> that when our defined benefit pension funds fall short of their 
> actuarially defined investment targets 
> -- the City's General Fund has to make up the difference.  It could be 
> worse,  absent vigilance, 
> we could have an LA-style (Orange County, CA) derivative meltdown.

> So my real concern is, in a tight budget environment, can the City of 
> Orlando use open source software 
> to assist its development and monitoring of investment policies? or would 
> the City get bogged down in 
> issues of times series object compatibility?

All I can say is that a lot of people are successfully using R to
analyse such questions. I have used R to help in policy thinking in a
context slightly bigger than Orlando :-)

In the last few days, I have been porting some of my personal code
which prices pension guarantees from C to R. I don't feel hesitant in
putting R into mission critical applications. It is solid, elegant,
well thought out code. Sometimes I think that R is to statistics
programs what Unix is to operating systems: elegant and brilliant.

I wouldn't worry so much about TS object compatibility. E.g. speaking
for me, I have "standardised" on ITS. Everything that I'm doing is
ITS. This has worked well. ITS has not choked on functionality for me
thus far. Yes, it bothers me that ITS is an orphan but so far I've not
been stuck for want of functionality. And, it isn't that hard to get
data in and out of ITS objects, so it isn't asif you're making a big
investment and incurring much by way of sunk costs. If you should
choose to switch to something else in the future, it won't be hard.

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From ajayshah at mayin.org  Mon Oct  4 18:06:04 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue Oct  5 06:51:52 2004
Subject: [R-sig-finance] How and when to USE ts and its objects?
In-Reply-To: <20041001171626.4523512D4D@mprdmxin.myway.com>
References: <20041001171626.4523512D4D@mprdmxin.myway.com>
Message-ID: <20041004160604.GR23573@igidr.ac.in>

> ts is for regularly spaced time series and works best with
> monthly or coarser time series since there is no explicit
> date support for finer granularity in ts.  If such date
> support is not important then ts is fine for finer
> granularity series as well.  ts is defined in the stats
> package which comes out-of-the-box with R so its normally
> always available and is therefore the most likely to be
> supported.

Yes. I only use ts for monthly or slower data.

> If you need irregularly spaced time series such as those
> that are defined for weekdays but not holidays or weekends then
> you can choose from a number of options:
> 
> 1. The irts class in the tseries package changes the
> frequency of the time scale to simulate a irregular time
> series with a regular one in such a way that its reversable.
> 
> 2. The its class in the its package uses S4 constructs to
> define a time series object with a POSIXct time scale.

ITS is not being actively developed right now. It seems to have
fabulous functionality but it would be great if it was not orphaned.

> 3. The zoo class has _not_ hard coded in the datetime class
> so it can use just about any datetime class (e.g. Date,
> chron, POSIXct).  In most cases zoo implements methods which
> simply extend base generics to achieve interface
> compatibility with ts and the base.  This also means that in
> most cases the user does not have to learn a whole new set
> of commands making it easier to use. A major new version of
> zoo is in development so it is being actively maintained and
> enhanced.  The current version of zoo on CRAN will import ts
> and irts objects (as well as matrices and vectors) and the
> upcoming version will also import its objects.  Its also
> easy to extract the data and datetime.   
> 
> I personally use ts for my regular time series and zoo for
> my irregular time series and have contributed to the
> development of the latter.

Gabor, I'm sure you know full well the full power of ITS. Is zoo
headed to match that, and then to do better? I, for one, will be happy
to pitch in and help by using zoo, contributing suggestions and
perhaps occasionally a little code, bug reports, etc.

If ITS is 'the existing standard' then wouldn't it be nice if zoo
would import and export ITS objects, so as to ease interoperability?

Curious: I was struck by the coolness of union() and intersect() in
ITS. Does zoo do these already?

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From ggrothendieck at myway.com  Tue Oct  5 07:20:20 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue Oct  5 07:20:24 2004
Subject: [R-sig-finance] How and when to USE ts and its objects?
Message-ID: <20041005052020.46B953960@mprdmxin.myway.com>



From: Ajay Shah <ajayshah@mayin.org>
> > Does RMetrics use ITS objects or does it have its own time series objects
> > (timeDate / timeSeries Classes)?

> I'm curious about this too.

I have just been looking at the fBasics rmetrics package 
(which I had neglected to mention in my list of irregular
time series packages earlier on this thread).

The timeSeries class of rmetrics uses a more sophisticated
time zone processing scheme on top of POSIXt.   One must
set their computer (Windows) or process (UNIX) to "GMT"
in order for this work.  (Somebody please correct me if I 
am wrong about computer/process granularity on various OSes.)
The timeSeries class is similar to the S class of the same
name.

The timeDate class of fBasics/rmetrics provides essentially the
same time and Date functionality but on a class of its
own independent of timeSeries.  timeSeries does not actually store
times and dates using the timeDate class.

From ggrothendieck at myway.com  Tue Oct  5 07:50:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue Oct  5 07:50:56 2004
Subject: [R-sig-finance] How and when to USE ts and its objects?
Message-ID: <20041005055043.D26DE39A8@mprdmxin.myway.com>


From: Ajay Shah <ajayshah@mayin.org>

Gabor, I'm sure you know full well the full power of ITS. Is zoo
headed to match that, and then to do better? I, for one, will be happy
to pitch in and help by using zoo, contributing suggestions and
perhaps occasionally a little code, bug reports, etc.

If ITS is 'the existing standard' then wouldn't it be nice if zoo
would import and export ITS objects, so as to ease interoperability?

Curious: I was struck by the coolness of union() and intersect() in
ITS. Does zoo do these already?

---

We are not attempting competing with its, in fact, I offerred 
some code to its but the unreleased version of zoo 
which Achim and I have running
is approximately comparable in functionality to its.  It can
import its objects and we also have an its export too.  It
will also be able to import and export fBasics/rmetrics timeSeries
objects.

The version on CRAN can already import ts and irts objects.
That version, i.e. on CRAN, already has the union and intersect
functionality.  They are combined into a 
single routine, merge, which is a generalization of union
and intersect.  It has an argument all which is TRUE for
union and FALSE for intersect but can be a logical vector
which generalizes these two special cases to create more
possibilities for combination.

One of the key goals of zoo is to be compatible with ts and
the base of R so you do not have to learn a whole new set of
commands making it, hopefully, easy to use.  In fact, most
functions are simply methods which extend core generics.

Achim Zeileis started the project and I joined in and I was really
hoping others would do so so too so I am really pleased you
are interested.  I will contact you offline.

From wuertz at itp.phys.ethz.ch  Wed Oct 13 01:59:12 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Wed Oct 13 01:59:20 2004
Subject: [R-sig-finance] New Built Rmetrics 200.10058
Message-ID: <416C6FD0.2000205@itp.phys.ethz.ch>

Dear R and Rmetrics Users,

It is a pleasure for me to announce the new built for Rmetrics Version
200.10058, updated for R Version 2.0. The source files and Windows binary
packages can be downloaded from the Rmetrics Server 
http://www.rmetrics.org .

The new built has also been submitted to the CRAN server 
http://cran.r-project.org/.

Many new functions and examples have been added and the documentation has
been updated and enhanced. You can download and install the fBasics, 
fSeries,
fExtremes and fOptions packages in the usual way.


Diethelm Wuertz
info@rmetrics.org
www.rmetrics.org

From wuertz at itp.phys.ethz.ch  Wed Oct 13 02:00:36 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Wed Oct 13 02:00:43 2004
Subject: [R-sig-finance] New Built Rmetrics 200.10058
Message-ID: <416C7024.8020805@itp.phys.ethz.ch>

Dear R and Rmetrics Users,

It is a pleasure for me to announce the new built for Rmetrics Version
200.10058, updated for R Version 2.0. The source files and Windows binary
packages can be downloaded from the Rmetrics Server 
http://www.rmetrics.org .

The new built has also been submitted to the CRAN server 
http://cran.r-project.org/.

Many new functions and examples have been added and the documentation has
been updated and enhanced. You can download and install the fBasics, 
fSeries,
fExtremes and fOptions packages in the usual way.


Diethelm Wuertz
info@rmetrics.org
www.rmetrics.org

From edd at debian.org  Wed Oct 13 06:15:34 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed Oct 13 06:15:40 2004
Subject: [R-sig-finance] New Built Rmetrics 200.10058
In-Reply-To: <416C7024.8020805@itp.phys.ethz.ch>
References: <416C7024.8020805@itp.phys.ethz.ch>
Message-ID: <20041013041534.GA7346@sonny.eddelbuettel.com>

On Wed, Oct 13, 2004 at 12:00:36AM +0000, Diethelm Wuertz wrote:
> Dear R and Rmetrics Users,
> 
> It is a pleasure for me to announce the new built for Rmetrics Version
> 200.10058, updated for R Version 2.0. The source files and Windows binary
> packages can be downloaded from the Rmetrics Server 
> http://www.rmetrics.org .
> 
> The new built has also been submitted to the CRAN server 
> http://cran.r-project.org/.
> 
> Many new functions and examples have been added and the documentation has
> been updated and enhanced. You can download and install the fBasics, 
> fSeries,
> fExtremes and fOptions packages in the usual way.

These new packages are now also on the main Debian server and will hit
mirrors over the next day or two.

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From William.Alpert at barrons.com  Fri Oct 15 17:05:49 2004
From: William.Alpert at barrons.com (Alpert, William)
Date: Fri Oct 15 17:05:55 2004
Subject: [R-sig-finance] WinRmetrics for 2.0?
Message-ID: <1CDEA031FEA76E4F9A41822D63A2D2BC1AA6A1@SBKE2KMB04.win.dowjones.net>

Should any of you feel pity for those of us still on training wheels,
could you tell me if there's going to be a WinRmetrics package for R 2.0
?  I rather enjoyed its dropdown menus, and I'm not yet skilled enough
to create my own.

 

Bill Alpert

Sr. Editor

Barron's

212.416.2742

william.alpert@barrons.com

 


	[[alternative HTML version deleted]]

From davidr at rhotrading.com  Mon Oct 18 22:47:33 2004
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Mon Oct 18 22:47:43 2004
Subject: [R-sig-finance] Is there an R-analog of S-Plus's
	import.dta.bloomberg?
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A32748F@rhosvr02.rhotrading.com>

Has anyone considered implementing an R version of S-Plus's
import.data.bloomberg?

(Or perhaps someone has?)

I realize that Bloomberg is expensive so few will have access to it, but
being able to get Bloomberg data directly into R would be very useful
(as it has been in S-Plus.)

 

 

 

David L. Reiner

 

Rho Trading

440 S. LaSalle St -- Suite 620

Chicago  IL  60605

 

312-362-4963 (voice)

312-362-4941 (fax)

 

 


	[[alternative HTML version deleted]]

From edd at debian.org  Mon Oct 18 23:04:20 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon Oct 18 23:04:30 2004
Subject: [R-sig-finance] Is there an R-analog of S-Plus's
	import.dta.bloomberg?
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A32748F@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A32748F@rhosvr02.rhotrading.com>
Message-ID: <20041018210420.GA8114@sonny.eddelbuettel.com>

On Mon, Oct 18, 2004 at 03:47:33PM -0500, davidr@rhotrading.com wrote:
> Has anyone considered implementing an R version of S-Plus's
> import.data.bloomberg?
> 
> (Or perhaps someone has?)
> 
> I realize that Bloomberg is expensive so few will have access to it, but
> being able to get Bloomberg data directly into R would be very useful
> (as it has been in S-Plus.)

Sure is. I wrote a package for it against the (rather well documented)
Bloomberg C API -- try WAPI <Go> and look for the 'C SDK' under downloads.
http://dirk.eddelbuettel.com/presentations.html has slides I showed at the
Use R! 2004 conference about this and a related connector package (for Lim).
I cannot release these as the work was done on company time. 

>From what I can tell, most people use Excel as a go-between and use the
Beier and Neuwirth tools to connect Excel to R (they have a list in Vienna,
at http://mailman.csd.univie.ac.at/pipermail/rcom-l/ where some of the
regulars contribute as well).

Seeing your address, and given I work across the street from you: would you
consider a 'Chicago Area S Users Group' feasible?  A friend and I once toyed
with the idea, but didn't end up doing anything about it. Email me off-list
if you'd be up for it.

Regards, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From enrique.bengoechea at credit-suisse.com  Tue Oct 19 12:45:24 2004
From: enrique.bengoechea at credit-suisse.com (Enrique Bengoechea)
Date: Tue Oct 19 12:45:55 2004
Subject: [R-sig-finance] Re: Is there an R-analog of S-Plus's
	import.dta.bloomberg
Message-ID: <OF286C90AB.798A6B9F-ONC1256F32.0039901D@credit-suisse.com>





Hi,

> being able to get Bloomberg data directly into R would be very useful

I am doing this every day. To my knowledge, you can do this in 2 ways:

      1) The hard way would be to use the Bloomberg C API  (as Dirk Eddelbuettel has done, but he cannot release his work)
      2)The easy way would be to use the Bloomberg ActiveX API, as I have done myself. This option is easier because you only have to write R code, and, of course, import some R COM client library (I use RDCOMClient).

You have some example code to import into R Bloomberg time series and fields on a couple of messages I sent to this list some months ago, just search for "Bloomberg" on the archives.

If my company allows (which I should verify) eventually I could clean up my code and release a package. For that it would be useful to know the API of the equivalent S-Plus package and match it. Do you know how to get it?

Regards,

Enrique

From kriskumar at earthlink.net  Tue Oct 19 14:14:43 2004
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Tue Oct 19 14:14:58 2004
Subject: [R-sig-finance] Is there an R-analog of
	S-Plus's	import.dta.bloomberg?
In-Reply-To: <20041018210420.GA8114@sonny.eddelbuettel.com>
References: <12AE52872B5C5348BE5CF47C707FF53A32748F@rhosvr02.rhotrading.com>
	<20041018210420.GA8114@sonny.eddelbuettel.com>
Message-ID: <41750533.6070007@earthlink.net>

Dirk Eddelbuettel wrote:

>Seeing your address, and given I work across the street from you: would you
>consider a 'Chicago Area S Users Group' feasible?  A friend and I once toyed
>with the idea, but didn't end up doing anything about it. Email me off-list
>if you'd be up for it.
>
>Regards, Dirk
>
>  
>
Ha!
Anyone up for a NYC Area S Users Group ?.  p.m. me off the list.

Best,

From Scott.Waichler at pnl.gov  Tue Oct 19 18:02:31 2004
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Tue Oct 19 18:04:31 2004
Subject: [R-sig-finance] Getting stock prices and annualized returns for
	common indexes
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7AB76542@pnlmse35.pnl.gov>


Is there a way to obtain stock and mutual fund prices (previous day's
close would be fine) 
from R?  Can bond prices also be obtained?  How about annualized returns
for previous calendar
year and last 3 and 5 years for major indexes such as S&P 500, 
Wilshire 5000, MSCI EAFE, etc.?  I am a non-professional
who manages some family portfolios and I am looking for better ways to
access information
and compare investment results in annual reports I write using Sweave.

Thanks,
Scott Waichler
scott.waichler@pnl.gov

From patrick at burns-stat.com  Tue Oct 19 21:05:34 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue Oct 19 21:05:55 2004
Subject: [R-sig-finance] Getting stock prices and annualized returns for
	common indexes
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7AB76542@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7AB76542@pnlmse35.pnl.gov>
Message-ID: <4175657E.1030604@burns-stat.com>

You can get data from Yahoo via the "get.hist.quote" function in
the tseries package.  Indices have strange symbols in Yahooland.

S&P 500:     ^GSPC
Nasdaq Comp:   ^IXIC
Nasdaq 100:      ^NDX

US 30yr Treasury:   ^TYX
US 10yr Treasury:   ^TNX
US   5yr Treasury:   ^FVX


Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Waichler, Scott R wrote:

>Is there a way to obtain stock and mutual fund prices (previous day's
>close would be fine) 
>from R?  Can bond prices also be obtained?  How about annualized returns
>for previous calendar
>year and last 3 and 5 years for major indexes such as S&P 500, 
>Wilshire 5000, MSCI EAFE, etc.?  I am a non-professional
>who manages some family portfolios and I am looking for better ways to
>access information
>and compare investment results in annual reports I write using Sweave.
>
>Thanks,
>Scott Waichler
>scott.waichler@pnl.gov
>
>_______________________________________________
>R-sig-finance@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>

From paul.medica at hp.com  Tue Oct 19 21:57:44 2004
From: paul.medica at hp.com (Medica, Paul A)
Date: Tue Oct 19 21:57:58 2004
Subject: [R-sig-finance] Getting stock prices and annualized returns
	forcommon indexes
Message-ID: <8422B8BDDBCF7045A59138E8E6A1457BDB3AEA@cacexc01.americas.cpqcorp.net>

I would urge anyone who is downloading price history from yahoo to paste
into any analysis package (R, S-plus, Matlab or otherwise) to run the
dates through a sequential calendar recognition script as there have
been incidents where dates or the associated values are missing.  This
is a real issue when attempting to correlate multivariate series,
especially VaR calculations.

Paul A. Medica
Reliability Engineering
Hewlett-Packard Co.
o: 360-212-2766
paul.medica@hp.com


-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Patrick
Burns
Sent: Tuesday, October 19, 2004 12:06 PM
To: Waichler, Scott R
Cc: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] Getting stock prices and annualized returns
forcommon indexes


You can get data from Yahoo via the "get.hist.quote" function in the
tseries package.  Indices have strange symbols in Yahooland.

S&P 500:     ^GSPC
Nasdaq Comp:   ^IXIC
Nasdaq 100:      ^NDX

US 30yr Treasury:   ^TYX
US 10yr Treasury:   ^TNX
US   5yr Treasury:   ^FVX


Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Waichler, Scott R wrote:

>Is there a way to obtain stock and mutual fund prices (previous day's 
>close would be fine) from R?  Can bond prices also be obtained?  How 
>about annualized returns for previous calendar
>year and last 3 and 5 years for major indexes such as S&P 500, 
>Wilshire 5000, MSCI EAFE, etc.?  I am a non-professional
>who manages some family portfolios and I am looking for better ways to
>access information
>and compare investment results in annual reports I write using Sweave.
>
>Thanks,
>Scott Waichler
>scott.waichler@pnl.gov
>
>_______________________________________________
>R-sig-finance@stat.math.ethz.ch mailing list 
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance

From patrick at burns-stat.com  Tue Oct 19 23:09:53 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue Oct 19 23:10:17 2004
Subject: [R-sig-finance] Getting stock prices and annualized
	returns	forcommon indexes
In-Reply-To: <8422B8BDDBCF7045A59138E8E6A1457BDB3AEA@cacexc01.americas.cpqcorp.net>
References: <8422B8BDDBCF7045A59138E8E6A1457BDB3AEA@cacexc01.americas.cpqcorp.net>
Message-ID: <417582A1.4070504@burns-stat.com>

On that topic, I should have pointed out that the stock prices in
Yahoo are subject to containing missed splits.  (I just got done
fixing some.)  That makes graphing the prices a valuable exercise.

Medica, Paul A wrote:

>I would urge anyone who is downloading price history from yahoo to paste
>into any analysis package (R, S-plus, Matlab or otherwise) to run the
>dates through a sequential calendar recognition script as there have
>been incidents where dates or the associated values are missing.  This
>is a real issue when attempting to correlate multivariate series,
>especially VaR calculations.
>
>Paul A. Medica
>Reliability Engineering
>Hewlett-Packard Co.
>o: 360-212-2766
>paul.medica@hp.com
>
>
>-----Original Message-----
>From: r-sig-finance-bounces@stat.math.ethz.ch
>[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Patrick
>Burns
>Sent: Tuesday, October 19, 2004 12:06 PM
>To: Waichler, Scott R
>Cc: r-sig-finance@stat.math.ethz.ch
>Subject: Re: [R-sig-finance] Getting stock prices and annualized returns
>forcommon indexes
>
>
>You can get data from Yahoo via the "get.hist.quote" function in the
>tseries package.  Indices have strange symbols in Yahooland.
>
>S&P 500:     ^GSPC
>Nasdaq Comp:   ^IXIC
>Nasdaq 100:      ^NDX
>
>US 30yr Treasury:   ^TYX
>US 10yr Treasury:   ^TNX
>US   5yr Treasury:   ^FVX
>
>
>Patrick Burns
>
>Burns Statistics
>patrick@burns-stat.com
>+44 (0)20 8525 0696
>http://www.burns-stat.com
>(home of S Poetry and "A Guide for the Unwilling S User")
>
>Waichler, Scott R wrote:
>
>  
>
>>Is there a way to obtain stock and mutual fund prices (previous day's 
>>close would be fine) from R?  Can bond prices also be obtained?  How 
>>about annualized returns for previous calendar
>>year and last 3 and 5 years for major indexes such as S&P 500, 
>>Wilshire 5000, MSCI EAFE, etc.?  I am a non-professional
>>who manages some family portfolios and I am looking for better ways to
>>access information
>>and compare investment results in annual reports I write using Sweave.
>>
>>Thanks,
>>Scott Waichler
>>scott.waichler@pnl.gov
>>
>>_______________________________________________
>>R-sig-finance@stat.math.ethz.ch mailing list 
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
>> 
>>
>>    
>>
>
>_______________________________________________
>R-sig-finance@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>_______________________________________________
>R-sig-finance@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>

From schouwla at yahoo.com  Wed Oct 20 02:54:02 2004
From: schouwla at yahoo.com (Lars Schouw)
Date: Wed Oct 20 02:54:06 2004
Subject: [R-sig-finance] fBonds from rmetrics
Message-ID: <20041020005403.30298.qmail@web50301.mail.yahoo.com>

Hi

Is it possible to download a beta version of fBonds
for testing?

Regards
Lars Schouw
Tokyo

From atp at piskorski.com  Wed Oct 20 15:24:40 2004
From: atp at piskorski.com (Andrew Piskorski)
Date: Wed Oct 20 15:24:46 2004
Subject: [R-sig-finance] Is there an R-analog of S-Plus's
	import.dta.bloomberg?
In-Reply-To: <20041018210420.GA8114@sonny.eddelbuettel.com>
References: <20041018210420.GA8114@sonny.eddelbuettel.com>
Message-ID: <20041020132440.GA28821@piskorski.com>

On Mon, Oct 18, 2004 at 04:04:20PM -0500, Dirk Eddelbuettel wrote:
> On Mon, Oct 18, 2004 at 03:47:33PM -0500, davidr@rhotrading.com wrote:

> > I realize that Bloomberg is expensive so few will have access to it, but
> > being able to get Bloomberg data directly into R would be very useful
> > (as it has been in S-Plus.)
> 
> Sure is. I wrote a package for it against the (rather well documented)
> Bloomberg C API -- try WAPI <Go> and look for the 'C SDK' under downloads.

The Bloomberg C API is reasonably well documented, but it is also
quite low level, so it NEEDS to be.  For anyone going that route, my
advice is to pay careful attention to the dozen or so example C
programs Bloomberg provides, as they are ugly, but CORRECT.

Whenever you encounter subtle correctness or performance bugs in your
code, look again at Bloomberg's examples.  Often you will see that
they are doing something in one particular way which happens to work
well, even though the API docs don't seem to require that you do it
exactly that way.  Basically, the Bloomberg C API docs are helpful but
do not fully specify how you need to use it.  The C examples do.

-- 
Andrew Piskorski <atp@piskorski.com>
http://www.piskorski.com/

From edd at debian.org  Wed Oct 20 16:10:02 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed Oct 20 16:10:15 2004
Subject: [R-sig-finance] Is there an R-analog of S-Plus's
	import.dta.bloomberg?
In-Reply-To: <20041020132440.GA28821@piskorski.com>
References: <20041018210420.GA8114@sonny.eddelbuettel.com>
	<20041020132440.GA28821@piskorski.com>
Message-ID: <20041020141002.GA27313@sonny.eddelbuettel.com>

On Wed, Oct 20, 2004 at 09:24:40AM -0400, Andrew Piskorski wrote:
> On Mon, Oct 18, 2004 at 04:04:20PM -0500, Dirk Eddelbuettel wrote:
> > On Mon, Oct 18, 2004 at 03:47:33PM -0500, davidr@rhotrading.com wrote:
> 
> > > I realize that Bloomberg is expensive so few will have access to it, but
> > > being able to get Bloomberg data directly into R would be very useful
> > > (as it has been in S-Plus.)
> > 
> > Sure is. I wrote a package for it against the (rather well documented)
> > Bloomberg C API -- try WAPI <Go> and look for the 'C SDK' under downloads.
> 
> The Bloomberg C API is reasonably well documented, but it is also
> quite low level, so it NEEDS to be.  For anyone going that route, my
> advice is to pay careful attention to the dozen or so example C
> programs Bloomberg provides, as they are ugly, but CORRECT.
> 
> Whenever you encounter subtle correctness or performance bugs in your
> code, look again at Bloomberg's examples.  Often you will see that
> they are doing something in one particular way which happens to work
> well, even though the API docs don't seem to require that you do it
> exactly that way.  Basically, the Bloomberg C API docs are helpful but
> do not fully specify how you need to use it.  The C examples do.

Excellent points and very true -- my presentation didn't stress that as much
as it could have. The informal algorithm was

- for a given access style, pick the corresponding example C file
- prune the code, and refactor some recurrent code into a utility library
- try it in repeated calls from a simple main()
- wrap the R-style interface around it and disable main()

As Andrew says, their examples work, though they are not the most elegant
code I've ever seen, and there is a pretty obscure call pattern from one
function down to another and another ... for a rather large total. But it
*works*, and given a Bloomberg terminal, one can alter these.

I still haven't found a good way to initiate a conversation with my boss(es)
about why "we should give something away for free". I do of course have my
views on that, but need something more convincing. I'd love to hear, on or
off list, from anybody with a template they could share.

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx

From davidr at rhotrading.com  Wed Oct 20 17:06:49 2004
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed Oct 20 17:06:54 2004
Subject: Sharing privately funded development work (was RE: [R-sig-finance] Is
	there an R-analog of S-Plus'simport.dta.bloomberg?)
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A32751E@rhosvr02.rhotrading.com>

Other than altruism, which not so many big companies believe in minus
tax or PR advantages, the main argument I've seen work is that the
company gets future development for free from the grateful community. In
this case, however, I imagine that all people contributing to extending
a bloomberg package would themselves be in the financial industry, so
the motivation to share would be smallish, open source licensing
requirements to do so aside.
Of course, many companies participate in standards setting and similar
activities, which do not (always) directly benefit them, but do
indirectly, through ease of communication with other companies like
themselves and ease of hiring people who can step in and get to work
quickly. 
Of course, Bloomberg itself wouldn't want to encourage this development
either since they are very protective of their data and analytics.
I'll not hold my breath for your company to give permission.

Thanks for the good discussion on this topic!
David

-----Original Message-----
From: Dirk Eddelbuettel [mailto:edd@debian.org] 
Sent: Wednesday, October 20, 2004 9:10 AM
To: Andrew Piskorski
Cc: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] Is there an R-analog of
S-Plus'simport.dta.bloomberg?

...
I still haven't found a good way to initiate a conversation with my
boss(es)
about why "we should give something away for free". I do of course have
my
views on that, but need something more convincing. I'd love to hear, on
or
off list, from anybody with a template they could share.

Dirk

-- 
Those are my principles, and if you don't like them... well, I have
others.
                                                -- Groucho Marx

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance

From davidr at rhotrading.com  Thu Oct 21 20:57:31 2004
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Thu Oct 21 20:57:43 2004
Subject: [R-sig-finance] import.data.bloomberg interface
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A327579@rhosvr02.rhotrading.com>

This much is on the web:

import.data.bloomberg(topic, security, fields, flag,type, start.date, end.date, periodicity, chron.order, pathNameToServer)

This is not the greatest design ever (well, neither is the Bloomberg API, etc.) It is also incomplete, using just the DDE server rather than the API, so historical intraday data is not available (a shortcoming I would hope someone could improve for R.)

There are also examples such as:
--------------------------------------------------------------
Summary of Replies on how to get splits from splus / bloomberg
from [Clark Sims]
[Bookmark?Link][Original]
To: 
s-news@wubios.wustl.edu, Matt.Kurbat@kmv.com
Subject: 
Summary of Replies on how to get splits from splus / bloomberg
From: 
"Clark Sims" <clarksimssplus@my-deja.com>
Date: 
Thu, 5 Jul 2001 13:40:12 -0700

The following command gets splits for IBM

temp.data <-  import.data.bloomberg("Bulk", "IBM Equity", "Eqy_Dvd_Hist_Splits")

column 1:  declared date
column 2:  ex date
column 3:  record date
column 4:  pay date
column 5:  split ratio

Thanks To Steve Wisdom

HTH,

Clark
--------------------------------------------------------------
Maybe they'll sue me for publicizing this, but surely it's out on the net somewhere:

DESCRIPTION

Import data from a Bloomberg database into an S-PLUS data sheet.

USAGE

import.data.bloomberg(topic, security, fields, flag, type,
     start.date, end.date, periodicity, chron.order, pathNameToServer)

REQUIRED ARGUMENTS

topic 	character string specifying a Bloomberg data category, one of "Historical", "Market", or "Bulk".
	security	character string containing a comma-separated sequence of Bloomberg security IDs, or a character vector of such IDs. For example,  "IBM US Equity, F US Equity" or c("IBM US Equity", "F US Equity").
	fields	a character string containing a comma-separated sequence of field mnemonics from the Bloomberg data dictionary, or a character vector of such mnemonics. For example, "Px High, Px Low" or c("Opt chain", "eqy dvd hist")
. 

OPTIONAL ARGUMENTS

flag	a character string containing any special flags that apply to your query. See the Bloomberg documentation for more details.
	type 	a character string describing the type of security identifier that can be specified in the DDE link, such as "AIBD, VALOREN". The default value is "".
	start.date	number giving the start date, or the character string "Now". The Bloomberg date format is yyyymmdd.   For example: 19980101, 19960520. 

stop.date 	The end date (see start.date for format).
	periodicity	character string giving the frequency of the requested data. Can be one of: "Day", "Week", "Month", "Quarter", or "Year".
	chron.order	character string specifying the order in which to sort the data, either "Chronological" or "Reverse".
	pathNameToServer	a character string containing the path to BLP.EXE. This should not need to be set unless you have trouble accessing the server, in which case you can specify an explicit path.

VALUE

a data sheet containing the requested data. 

SEE ALSO

import.data, import.data.mim, import.data.fame.

EXAMPLES

IBM.DS<-import.data.bloomberg("Historical", c("IBM Equity", "F Equity"),

      c("Px Last","Px high"), "","",19990501, "Now",

      "Day","Chronological","BLP.EXE")

IBM.DS.M<-import.data.bloomberg("Market", c("IBM Equity", "F Equity"),

      c("Px Last", "Px high", "Px Low"), "","","Now", "Now", "Day",

      "Chronological","BLP.EXE")

David L. Reiner
?
Rho Trading
440 S. LaSalle St -- Suite 620
Chicago? IL? 60605
?
312-362-4963 (voice)
312-362-4941 (fax)
?

From Brandon.J.Whitcher at gsk.com  Tue Nov  2 09:52:18 2004
From: Brandon.J.Whitcher at gsk.com (Brandon.J.Whitcher@gsk.com)
Date: Tue Nov  2 09:53:22 2004
Subject: [R-sig-finance] updated package waveslim 1.4
Message-ID: <OFB6434839.51D96622-ON80256F40.002FFA02-80256F40.0030CC99@sb.com>

waveslim 1.4 has recently been uploaded to CRAN and is fully compatible 
with Rv2.0.  The package includes:

Basic wavelet routines for time series (1D), image (2D) and array (3D) 
analysis. The code provided here is based on wavelet methodology developed 
in Percival and Walden (2000) and Gencay, Selcuk and Whitcher (2001).  All 
figures in chapters 4-7 of GSW (2001) are reproducible using this package 
and R code available at 
http://www.cgd.ucar.edu/stats/staff/whitcher/book/GSW_figures.R.
Gencay, R., Selcuk, F. and Whitcher, B. (2001).  An Introduction to 
Wavelets and Other Filtering Methods for Finance and Economics.  Academic 
Press, San Diego.
Percival, D.B., and Walden, A.T. (2000).  Wavelet Methods for Time Series 
Analysis.  Cambridge University Press, Cambridge.

Besides ensuring usability with the most recent version of R, two 
additional "flavors" of wavelet methodology have been added to the 
package: (1) Hilbert wavelet pairs and (2) the dual-tree complex wavelet 
transform [only 1D and 2D ported from Matlab code by Selesnick].  The 
dual-tree CWT code has not been optimized for R (only a "direct" port from 
Matlab) so reasonable speed-ups are anticipated in the future. 
Comments/bug reports are welcome.

References for (1) include: 

Selesnick, I.W. (2001). Hilbert transform pairs of wavelet bases. IEEE 
Signal Processing Letters 8 (6), 170–173.
Selesnick, I.W. (2002). The design of approximate Hilbert transform pairs 
of wavelet bases. IEEE Transactions on Signal Processing 50 (5), 
1144–1152.
Whitcher, B. and P.F. Craigmile (2004). Multivariate Spectral Analysis 
Using Hilbert Wavelet Pairs, International Journal of Wavelets, 
Multiresolution and Information Processing, 2 (4), in press.

References for (2) include: 

Kingsbury, N. (1999). Image processing with complex wavelets. Proceedings 
of the Royal Society of London, Series A, 357, 2543–2560.
Kingsbury, N. (2001). Complex wavelets for shift invariant analysis and 
filtering of signals. Applied and Computational Harmonic Analysis, 10 (3), 
234–253.

=======

Brandon


Brandon Whitcher, PhD
Translational Medicine & Technology
GlaxoSmithKline
Greenford Road
Greenford UB6 0HE, United Kingdom
Tel: +44 (0)20 8966 4511
Fax: +44 (0)20 8966 2757
Mob: +44 (0)7717 800 375


	[[alternative HTML version deleted]]

From vfulco at optonline.net  Wed Nov  3 19:00:27 2004
From: vfulco at optonline.net (Vincent C. Fulco)
Date: Wed Nov  3 19:00:40 2004
Subject: [R-sig-finance] RMetrics functions...
Message-ID: <41891CBB.2070007@optonline.net>

Dear R-finance list-

Can anyone tell me if they are using Rmetrics economagicImport and 
yahooImport functions on a regular basis?  I've attempted to use them 
and followed the instructions religiously and I keep receiving an error 
message "No Internet Access".

Thanks for your time.


Vince Fulco

From paul at woodgasllc.com  Wed Nov  3 19:35:59 2004
From: paul at woodgasllc.com (Paul DeBruicker)
Date: Wed Nov  3 19:43:14 2004
Subject: [R-sig-finance] RMetrics functions...
In-Reply-To: <41891CBB.2070007@optonline.net>
References: <41891CBB.2070007@optonline.net>
Message-ID: <34C01FCA-2DC7-11D9-8C75-0003931A8C1A@woodgasllc.com>

Set the "try" flag to FALSE in the import function

Historical.data<-yahooImport(query=query, try=FALSE);


It will prevent the function from checking for internet access and see 
if it works without confirmation.  Setting "try" to FALSE solves that 
problem on my Mac.



On Nov 3, 2004, at 1:00 PM, Vincent C. Fulco wrote:

> Dear R-finance list-
>
> Can anyone tell me if they are using Rmetrics economagicImport and 
> yahooImport functions on a regular basis?  I've attempted to use them 
> and followed the instructions religiously and I keep receiving an 
> error message "No Internet Access".
>
> Thanks for your time.
>
>
> Vince Fulco
>
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>

From ru68y7s at myrealbox.com  Thu Nov  4 19:03:20 2004
From: ru68y7s at myrealbox.com (s viswanath)
Date: Thu Nov  4 19:03:33 2004
Subject: [R-sig-finance] question on GBS
Message-ID: <1099591400.8c6bf57cru68y7s@myrealbox.com>

hello r experts,

i am tryingto back out implied volatility from some options and am struggling to get an understanding of 
tol(erence) and how to set it as shown below.

 GBSVolatility(price, TypeFlag, S, X, Time, r, b, tol, maxiter)

Could someone please provide and example of what tolerence means and how to set it?

fyi below is the explanation for this in rhelp
maxiter, tol: [GBSVolatility*] - 
the maximum number of iterations and the tolerance to          compute the root of the GBS volatility equation, see          'uniroot'. 

Best,

Sri
-----Original Message-----
From: r-sig-finance-request@stat.math.ethz.ch
To: r-sig-finance@stat.math.ethz.ch
Date: Thu, 4 Nov 2004 12:29:30 +0100
Subject: R-sig-finance Digest, Vol 6, Issue 2

Send R-sig-finance mailing list submissions to
	r-sig-finance@stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
or, via email, send a message with subject or body 'help' to
	r-sig-finance-request@stat.math.ethz.ch

You can reach the person managing the list at
	r-sig-finance-owner@stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-sig-finance digest..."


Today's Topics:

   1. RMetrics functions... (Vincent C. Fulco)
   2. Re: RMetrics functions... (Paul DeBruicker)


----------------------------------------------------------------------

Message: 1
Date: Wed, 03 Nov 2004 13:00:27 -0500
From: "Vincent C. Fulco" <vfulco@optonline.net>
Subject: [R-sig-finance] RMetrics functions...
To: r-sig-finance@stat.math.ethz.ch
Message-ID: <41891CBB.2070007@optonline.net>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Dear R-finance list-

Can anyone tell me if they are using Rmetrics economagicImport and 
yahooImport functions on a regular basis?  I've attempted to use them 
and followed the instructions religiously and I keep receiving an error 
message "No Internet Access".

Thanks for your time.


Vince Fulco



------------------------------

Message: 2
Date: Wed, 3 Nov 2004 13:35:59 -0500
From: Paul DeBruicker <paul@woodgasllc.com>
Subject: Re: [R-sig-finance] RMetrics functions...
To: "Vincent C. Fulco" <vfulco@optonline.net>
Cc: r-sig-finance@stat.math.ethz.ch
Message-ID: <34C01FCA-2DC7-11D9-8C75-0003931A8C1A@woodgasllc.com>
Content-Type: text/plain; charset=US-ASCII; format=flowed

Set the "try" flag to FALSE in the import function

Historical.data<-yahooImport(query=query, try=FALSE);


It will prevent the function from checking for internet access and see 
if it works without confirmation.  Setting "try" to FALSE solves that 
problem on my Mac.



On Nov 3, 2004, at 1:00 PM, Vincent C. Fulco wrote:

> Dear R-finance list-
>
> Can anyone tell me if they are using Rmetrics economagicImport and 
> yahooImport functions on a regular basis?  I've attempted to use them 
> and followed the instructions religiously and I keep receiving an 
> error message "No Internet Access".
>
> Thanks for your time.
>
>
> Vince Fulco
>
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>



------------------------------

_______________________________________________
R-sig-finance mailing list
R-sig-finance@stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


End of R-sig-finance Digest, Vol 6, Issue 2
*******************************************



--
This message was sent with an unlicensed evaluation version of
Novell NetMail. Please see http://www.netmail.com/ for details.

From vograno at evafunds.com  Thu Nov  4 20:14:17 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu Nov  4 20:14:16 2004
Subject: [R-sig-finance] hdf5
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5750E45@phost015.EVAFUNDS.intermedia.net>

Hi,

I wonder if anyone here on the list has an experience of doing research
in an environment where historical market data is stored in hdf5 or
netCdf formats. What are the pros and the cons comparing to RDBMS or
ASCII?


Thanks,
Vadim

From edd at debian.org  Thu Nov  4 20:16:27 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu Nov  4 20:16:50 2004
Subject: [R-sig-finance] question on GBS
In-Reply-To: <1099591400.8c6bf57cru68y7s@myrealbox.com>
References: <1099591400.8c6bf57cru68y7s@myrealbox.com>
Message-ID: <20041104191627.GA1174@sonny.eddelbuettel.com>

On Thu, Nov 04, 2004 at 10:03:20AM -0800, s viswanath wrote:
> hello r experts,
> 
> i am tryingto back out implied volatility from some options and am struggling to get an understanding of 
> tol(erence) and how to set it as shown below.
> 
>  GBSVolatility(price, TypeFlag, S, X, Time, r, b, tol, maxiter)
> 
> Could someone please provide and example of what tolerence means and how to set it?

Implied vol is calculated by one-dimensional numerical optimisation, and
such methods need a convergence criterion -- tol and maxiter are just that.

What you may have overlooked is that these do not need to be set as default
valies are defined in the function definition:

> args(GBSVolatility)
function (price, TypeFlag = c("c", "p"), S, X, Time, r, b, tol = .Machine$double.eps, 
    maxiter = 10000) 
NULL

So you only need to supply the other arguments.  Here is the implied vol. on
a current valuation of IBM @ 90 in Dec:

> GBSVolatility(3.4, "c", 92.20, 90, 44/365, 0.01977, 0.01977-0.0134)
[1] 0.1646709

which happens to be pretty close to where Bloomberg has it: 16.488%

Hope this helps, and *please* remove all the irrelevant quoted material
before you post.

Best, Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm

From ckjmaner at carolina.rr.com  Sat Nov  6 00:39:58 2004
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Sat Nov  6 00:40:55 2004
Subject: [R-sig-finance] Reading Foreign Exchange XML Data
In-Reply-To: <200411051136.iA5BaQWs003257@hypatia.math.ethz.ch>
Message-ID: <200411052340.iA5Nehkd025352@ms-smtp-03-eri0.southeast.rr.com>

 
Hi all.  Does anyone know of an R package which reads and dynamically
creates an R data object, (e.g., dataframe, ts, its object), from the
foreign exchange data posted on the Fed Reserve Bank of New York website,
http://www.ny.frb.org/xml/fx.html?  Currently, I'm parsing it using a
sed/vim script to create a CSV file then importing it into R using read.csv.
I've a hunch there's an easier way but am yet to come across one.  Any help
would be appreciated.


Thanks,
Charles

From edd at debian.org  Sat Nov  6 01:18:42 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat Nov  6 01:19:02 2004
Subject: [R-sig-finance] Reading Foreign Exchange XML Data
In-Reply-To: <200411052340.iA5Nehkd025352@ms-smtp-03-eri0.southeast.rr.com>
References: <200411051136.iA5BaQWs003257@hypatia.math.ethz.ch>
	<200411052340.iA5Nehkd025352@ms-smtp-03-eri0.southeast.rr.com>
Message-ID: <20041106001842.GA15488@sonny.eddelbuettel.com>

Charles,

On Fri, Nov 05, 2004 at 06:39:58PM -0500, Charles and Kimberly Maner wrote:
>  
> Hi all.  Does anyone know of an R package which reads and dynamically
> creates an R data object, (e.g., dataframe, ts, its object), from the
> foreign exchange data posted on the Fed Reserve Bank of New York website,
> http://www.ny.frb.org/xml/fx.html?  Currently, I'm parsing it using a
> sed/vim script to create a CSV file then importing it into R using read.csv.
> I've a hunch there's an easier way but am yet to come across one.  Any help
> would be appreciated.

If it were xml, you could try the xml packages from CRAN. If it isn't, then
I'm afraid I do not know of de-html-ifier in R. Perl has some things at
CPAN, but I have no idea how robust these scrapers are.

Hth, Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm

From edd at debian.org  Sat Nov  6 01:19:46 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat Nov  6 01:19:49 2004
Subject: [R-sig-finance] hdf5
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A5750E45@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A5750E45@phost015.EVAFUNDS.intermedia.net>
Message-ID: <20041106001946.GB15488@sonny.eddelbuettel.com>

Vadim,

On Thu, Nov 04, 2004 at 11:14:17AM -0800, Vadim Ogranovich wrote:
> Hi,
> 
> I wonder if anyone here on the list has an experience of doing research
> in an environment where historical market data is stored in hdf5 or
> netCdf formats. What are the pros and the cons comparing to RDBMS or
> ASCII?

Great question, and I have wondered about that too given that there are so
many scientific apps interfacing hdf5/cdf.  I don;t know of anything -- If
you find something, let us know!

Best regards, Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm

From kriskumar at earthlink.net  Sat Nov  6 01:22:50 2004
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Sat Nov  6 01:23:01 2004
Subject: [R-sig-finance] Reading Foreign Exchange XML Data
In-Reply-To: <20041106001842.GA15488@sonny.eddelbuettel.com>
References: <200411051136.iA5BaQWs003257@hypatia.math.ethz.ch>	<200411052340.iA5Nehkd025352@ms-smtp-03-eri0.southeast.rr.com>
	<20041106001842.GA15488@sonny.eddelbuettel.com>
Message-ID: <418C195A.3070203@earthlink.net>

Dirk Eddelbuettel wrote:

>Charles,
>
>On Fri, Nov 05, 2004 at 06:39:58PM -0500, Charles and Kimberly Maner wrote:
>  
>
>> 
>>Hi all.  Does anyone know of an R package which reads and dynamically
>>creates an R data object, (e.g., dataframe, ts, its object), from the
>>foreign exchange data posted on the Fed Reserve Bank of New York website,
>>http://www.ny.frb.org/xml/fx.html?  Currently, I'm parsing it using a
>>sed/vim script to create a CSV file then importing it into R using read.csv.
>>I've a hunch there's an easier way but am yet to come across one.  Any help
>>would be appreciated.
>>    
>>
>
>If it were xml, you could try the xml packages from CRAN. If it isn't, then
>I'm afraid I do not know of de-html-ifier in R. Perl has some things at
>CPAN, but I have no idea how robust these scrapers are.
>
>Hth, Dirk
>
>  
>
Have you looked at the omegahat RSXML package? don't know if that is the 
same as what Dirk
is suggesting.

From ggrothendieck at myway.com  Sat Nov  6 02:39:40 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat Nov  6 02:39:54 2004
Subject: [R-sig-finance] Reading Foreign Exchange XML Data
Message-ID: <20041106013940.6D0CA3979@mprdmxin.myway.com>



I don't know specifically about that data but, in general,
if you have a sufficiently recent version of Excel and 
don't mind a semi-manual procedure you can transfer
XML to Excel and then to R without programming by:

- read data into Excel (sufficiently recent versions of
  Excel read XML directly.  Just use File | Open)
- select rectangular area of cells to transfer to R
- copy to clipboard (ctrl-c)
- read.table("clipboard") or suitable variation in R


Date:   Fri, 5 Nov 2004 18:39:58 -0500 
From:   Charles and Kimberly Maner <ckjmaner@carolina.rr.com>
To:   <r-sig-finance@stat.math.ethz.ch> 
Subject:   [R-sig-finance] Reading Foreign Exchange XML Data 

 

Hi all. Does anyone know of an R package which reads and dynamically
creates an R data object, (e.g., dataframe, ts, its object), from the
foreign exchange data posted on the Fed Reserve Bank of New York website,
http://www.ny.frb.org/xml/fx.html? Currently, I'm parsing it using a
sed/vim script to create a CSV file then importing it into R using read.csv.
I've a hunch there's an easier way but am yet to come across one. Any help
would be appreciated.


Thanks,
Charles

From arshia22 at yahoo.com  Tue Nov 16 09:14:52 2004
From: arshia22 at yahoo.com (ebashi)
Date: Tue Nov 16 09:15:02 2004
Subject: [R-sig-finance] ts-plotting?
Message-ID: <20041116081452.94506.qmail@web81002.mail.yahoo.com>

Dear R users,
I have just started R and already fascinated by its power. As this is my first email, I want to thank you all in advance and appreciate your efforts for helping others.
 
I have a problem to plot Time in x-axis and as I was looking in email archives, I found out that other people also have had some problems with it before. To make it short and simple, I use the same examples in fBasics package, I imported IBM data as follow,   
 
> ## yahooImport -
> xmpBasics("\nNext: IBM Shares from Yahoo > ")
> # [test 19/20 century change 01-12-1999 -- 31-01-2000]
> IBM = yahooImport(
+ query = "s=IBM&a=11&b=1&c=1999&d=0&q=31&f=2000&z=IBM&x=.csv",
+ try = TRUE)
 
> if (!is.null(IBM)) print(IBM[1:6, ])
           Open   High    Low  Close   Volume Adj. Close*
19991201 102.56 104.44 102.25 103.42  5340000      100.15
19991202 103.44 106.31 103.37 105.27  6220800      101.94
19991203 109.81 112.87 107.94 111.87 14680500      108.33
19991206 113.00 116.50 112.50 116.00  9933000      112.33
19991207 117.00 119.19 115.44 116.62 11329500      112.93
19991208 116.25 121.00 115.50 118.28  8145300      114.54

 
the dates in the first column don't have header and I don't know what type of data they are. when I plot them as follow,
>tsPlot(IBM$Close)
 
the x-axis has the row number ( I want to have Time in the x-axis)
 
but when I use the example for the tsPlot from the fBasics which is as follows,
> tsPlot(IBM$Open)
> ## tsPlot -
> xmpBasics("\nStart: European Stock Markets > ")
> # Show multiple plot:
> par(mfrow = c(1, 1), cex = 0.7)
> data(EuStockMarkets)
> tsPlot(EuStockMarkets, gpars = list(col = c(2:4, 6)), labels = FALSE)
> title(main = "European Stock Markets")
      (this is how EuStockMarkets look like)
> if (!is.null(IBM)) print(EuStockMarkets[1:6, ])
          DAX    SMI    CAC   FTSE
 [1,] 1628.75 1678.1 1772.8 2443.6
 [2,] 1613.63 1688.5 1750.5 2460.2
 [3,] 1606.51 1678.6 1718.0 2448.2
 [4,] 1621.04 1684.1 1708.1 2470.4
 [5,] 1618.16 1686.6 1723.1 2484.7
 [6,] 1610.61 1671.6 1714.3 2466.8
 
on the x-axis I get actual time values,
I will appreciate it if someone help me regarding that and also clarify it for me what is the difference between these two type of data.
 
Sincerely Yours,
 
Arshia



	[[alternative HTML version deleted]]

From Bernhard.Pfaff at drkw.com  Tue Nov 16 10:33:02 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Tue Nov 16 10:33:31 2004
Subject: [R-sig-finance] ts-plotting?
Message-ID: <29E0BC0C716A584582941615CF9FFB0902585D1E@ibfftce107.de.ad.drkw.net>

Dear Arshia,

in those cases (and not only in those cases!) str() is your friend which
will tell you that the series in EuStockMarkets are of type ts, whereas the
data on IBM stock is not.

library(fBasics)
IBM = yahooImport(query =
"s=IBM&a=11&b=1&c=1999&d=0&q=31&f=2000&z=IBM&x=.csv",
try = TRUE)
str(IBM) 
#
data(EuStockMarkets)
str(EuStockMarkets)

Now, having said this, you can convert IBM to an ts object (but have a look
at the contributed packages 'its' and 'zoo' too, in order to get acquainted
how to deal with irregular series). However, in order to get time labels
beneath your x-axes it would suffice to save the 'rownames(IBM)' into a
nicely date format first, and then use pretty() such that your labels are
not utterly cluttered in your graph and plot them. This approach has been
outlined many time on R-Help, try a Google search through the archives.

Hopefully, these pointers have helped you by figuring out what is going on
and why, as well as to solve the problem by yourself.

Cheers,
Bernhard  

> Dear R users,
> I have just started R and already fascinated by its power. As 
> this is my first email, I want to thank you all in advance 
> and appreciate your efforts for helping others.
>  
> I have a problem to plot Time in x-axis and as I was looking 
> in email archives, I found out that other people also have 
> had some problems with it before. To make it short and 
> simple, I use the same examples in fBasics package, I 
> imported IBM data as follow,   
>  
> > ## yahooImport -
> > xmpBasics("\nNext: IBM Shares from Yahoo > ")
> > # [test 19/20 century change 01-12-1999 -- 31-01-2000]
> > IBM = yahooImport(
> + query = "s=IBM&a=11&b=1&c=1999&d=0&q=31&f=2000&z=IBM&x=.csv",
> + try = TRUE)
>  
> > if (!is.null(IBM)) print(IBM[1:6, ])
>            Open   High    Low  Close   Volume Adj. Close*
> 19991201 102.56 104.44 102.25 103.42  5340000      100.15
> 19991202 103.44 106.31 103.37 105.27  6220800      101.94
> 19991203 109.81 112.87 107.94 111.87 14680500      108.33
> 19991206 113.00 116.50 112.50 116.00  9933000      112.33
> 19991207 117.00 119.19 115.44 116.62 11329500      112.93
> 19991208 116.25 121.00 115.50 118.28  8145300      114.54
> 
>  
> the dates in the first column don't have header and I don't 
> know what type of data they are. when I plot them as follow,
> >tsPlot(IBM$Close)
>  
> the x-axis has the row number ( I want to have Time in the x-axis)
>  
> but when I use the example for the tsPlot from the fBasics 
> which is as follows,
> > tsPlot(IBM$Open)
> > ## tsPlot -
> > xmpBasics("\nStart: European Stock Markets > ")
> > # Show multiple plot:
> > par(mfrow = c(1, 1), cex = 0.7)
> > data(EuStockMarkets)
> > tsPlot(EuStockMarkets, gpars = list(col = c(2:4, 6)), 
> labels = FALSE)
> > title(main = "European Stock Markets")
>       (this is how EuStockMarkets look like)
> > if (!is.null(IBM)) print(EuStockMarkets[1:6, ])
>           DAX    SMI    CAC   FTSE
>  [1,] 1628.75 1678.1 1772.8 2443.6
>  [2,] 1613.63 1688.5 1750.5 2460.2
>  [3,] 1606.51 1678.6 1718.0 2448.2
>  [4,] 1621.04 1684.1 1708.1 2470.4
>  [5,] 1618.16 1686.6 1723.1 2484.7
>  [6,] 1610.61 1671.6 1714.3 2466.8
>  
> on the x-axis I get actual time values,
> I will appreciate it if someone help me regarding that and 
> also clarify it for me what is the difference between these 
> two type of data.
>  
> Sincerely Yours,
>  
> Arshia
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From p.kostov at Queens-Belfast.AC.UK  Tue Nov 16 10:41:18 2004
From: p.kostov at Queens-Belfast.AC.UK (p.kostov@Queens-Belfast.AC.UK)
Date: Tue Nov 16 12:25:26 2004
Subject: [R-sig-finance] ts-plotting?
Message-ID: <E1CTzpu-0000Go-Fa@amos>

The reason is simple: IBM in your example is a data frame, while 
EuStockMarkets is a time series object. (use str(XXX) to check this). To 
get the date you may wish to transform IBM to ts (use e.g. as.ts())



On Nov 16 2004, ebashi wrote:

> Dear R users, I have just started R and already fascinated by its power. 
> As this is my first email, I want to thank you all in advance and 
> appreciate your efforts for helping others.
>  
> I have a problem to plot Time in x-axis and as I was looking in email 
> archives, I found out that other people also have had some problems with 
> it before. To make it short and simple, I use the same examples in 
> fBasics package, I imported IBM data as follow,
>  
> > ## yahooImport -
> > xmpBasics("\nNext: IBM Shares from Yahoo > ")
> > # [test 19/20 century change 01-12-1999 -- 31-01-2000]
> > IBM = yahooImport(
> + query = "s=IBM&a=11&b=1&c=1999&d=0&q=31&f=2000&z=IBM&x=.csv",
> + try = TRUE)
>  
> > if (!is.null(IBM)) print(IBM[1:6, ])
>            Open   High    Low  Close   Volume Adj. Close*
> 19991201 102.56 104.44 102.25 103.42  5340000      100.15
> 19991202 103.44 106.31 103.37 105.27  6220800      101.94
> 19991203 109.81 112.87 107.94 111.87 14680500      108.33
> 19991206 113.00 116.50 112.50 116.00  9933000      112.33
> 19991207 117.00 119.19 115.44 116.62 11329500      112.93
> 19991208 116.25 121.00 115.50 118.28  8145300      114.54
> 
>  
> the dates in the first column don't have header and I don't know what 
> type of data they are. when I plot them as follow,
> >tsPlot(IBM$Close)
>  
> the x-axis has the row number ( I want to have Time in the x-axis)
>  
> but when I use the example for the tsPlot from the fBasics which is as 
> follows,
> > tsPlot(IBM$Open)
> > ## tsPlot -
> > xmpBasics("\nStart: European Stock Markets > ")
> > # Show multiple plot:
> > par(mfrow = c(1, 1), cex = 0.7)
> > data(EuStockMarkets)
> > tsPlot(EuStockMarkets, gpars = list(col = c(2:4, 6)), labels = FALSE)
> > title(main = "European Stock Markets")
>       (this is how EuStockMarkets look like)
> > if (!is.null(IBM)) print(EuStockMarkets[1:6, ])
>           DAX    SMI    CAC   FTSE
>  [1,] 1628.75 1678.1 1772.8 2443.6
>  [2,] 1613.63 1688.5 1750.5 2460.2
>  [3,] 1606.51 1678.6 1718.0 2448.2
>  [4,] 1621.04 1684.1 1708.1 2470.4
>  [5,] 1618.16 1686.6 1723.1 2484.7
>  [6,] 1610.61 1671.6 1714.3 2466.8
>  
> on the x-axis I get actual time values, I will appreciate it if someone 
> help me regarding that and also clarify it for me what is the difference 
> between these two type of data.
>  
> Sincerely Yours,
>  
> Arshia
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>

From gcreamer at cs.columbia.edu  Sun Nov 21 00:08:25 2004
From: gcreamer at cs.columbia.edu (German G. Creamer)
Date: Sun Nov 21 00:09:26 2004
Subject: [R-sig-finance] Question about Exponential Weighted Moving Average
	(EWMA) in rmetrics
Message-ID: <Pine.GSO.4.58.0411201759010.16076@play.cs.columbia.edu>

Hi all,

I



I am trying to use the macd function. It is used with cdoTA which calls
macdTA and this calls emaTA. However arguments of emaTA do not match those
requested at macdTA.
I wonder if there is another function of emaTA or EWMA that can be used
with macd. A similar problem I found with the file funSeries.R.

Thanks

German Creamer

These are the function definitions at fSeries:

cdoTA <-
function (x, lag1, lag2, lag3)
{
    macdTA(x, lag1, lag2) - cdsTA(x, lag1, lag2, lag3)
}

> macdTA
function (x, lag1, lag2)
{
    emaTA(x, lag1) - emaTA(x, lag2)
}

Do not correspond to:

> emaTA
function (x, lambda, startup = 0)
{
    if (lambda >= 1)
        lambda = 2/(lambda + 1)
    if (startup == 0)
        startup = floor(2/lambda)
    if (lambda == 0) {
        xema = rep(mean(x), length(x))
    }
    if (lambda > 0) {
        xlam = x * lambda
        xlam[1] = mean(x[1:startup])
        xema = filter(xlam, filter = (1 - lambda), method = "rec")
    }
    xema
}

From gcreamer at cs.columbia.edu  Sun Nov 21 06:55:54 2004
From: gcreamer at cs.columbia.edu (German G. Creamer)
Date: Sun Nov 21 06:56:30 2004
Subject: [R-sig-finance] Question about Exponential Weighted Moving Average
 (EWMA) in rmetrics. 
Message-ID: <Pine.GSO.4.58.0411201850340.16076@play.cs.columbia.edu>

Please disregard the previous message. I realized that in the emaTA
equation,
a lambda greater than one is used as a lag of n periods to calculate the
decay parameter. A lambda less than one is used directly as the decay
parameter.

So, the functions are consistent.

Thanks anyway,

German

From arshia22 at yahoo.com  Tue Nov 23 21:54:40 2004
From: arshia22 at yahoo.com (ebashi)
Date: Tue Nov 23 21:54:48 2004
Subject: [R-sig-finance] How to extract data?
Message-ID: <20041123205440.37889.qmail@web81002.mail.yahoo.com>

I appreciate if anyone can help me,
I have a table as follow,
> rate
          DATE VALUE
1   1997-01-10  5.30
2   1997-01-17  5.30
3   1997-01-24  5.28
4   1997-01-31  5.30
5   1997-02-07  5.29
6   1997-02-14  5.26
7   1997-02-21  5.24
8   1997-02-28  5.26
9   1997-03-07  5.30
10  1997-03-14  5.30
.    ......     ...
.    ......     ...
.    ......     ...
I want to extract the DATE(s) on which the VALUE has
already dropped twice and the DATE(s) that VALUE has
already increased for three times,( ignore where
VALUE(i+1)-VALUE(i)=0),I try to use diff() function,
however that works only for one increase or decrease.

Sincerely,

Sean

From james.holtman at convergys.com  Tue Nov 23 22:27:45 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Tue Nov 23 22:43:24 2004
Subject: [R-sig-finance] Re: [R] How to extract data?
Message-ID: <OFBBCB2ECC.F8077073-ON85256F55.00755F8B@nd.convergys.com>





By 'ignore', can we delete those from the list of data?  I would then
assume that if you have a sequence of +0+0+ that you would want the last
"+" for the increase of three.

If that is the case, then do a 'diff' and delete the entries that are 0.
Then create a new 'diff' and then use 'rle' to see what the length of the
sequences are:

> x <- c(1,2,2,3,3,4,3,3,2,2,2,1)
> x
 [1] 1 2 2 3 3 4 3 3 2 2 2 1
> x.d <- diff(x)
> x.d
 [1]  1  0  1  0  1 -1  0 -1  0  0 -1
> x.new <- x[c(x.d,1) != 0]
> x.new
[1] 1 2 3 4 3 2 1
> x.d1 <- diff(x.new)
> x.d1
[1]  1  1  1 -1 -1 -1
> rle(x.d1)
Run Length Encoding
  lengths: int [1:2] 3 3
  values : num [1:2] 1 -1
>

you can check the results of 'rle' to determine where the changes are.
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman@convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      ebashi                                                                                                               
                      <arshia22@yahoo.com>         To:       r-sig-finance@stat.math.ethz.ch, r-help@stat.math.ethz.ch                     
                      Sent by:                     cc:                                                                                     
                      r-help-bounces@stat.m        Subject:  [R] How to extract data?                                                      
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      11/23/2004 15:54                                                                                                     
                                                                                                                                           
                                                                                                                                           




I appreciate if anyone can help me,
I have a table as follow,
> rate
          DATE VALUE
1   1997-01-10  5.30
2   1997-01-17  5.30
3   1997-01-24  5.28
4   1997-01-31  5.30
5   1997-02-07  5.29
6   1997-02-14  5.26
7   1997-02-21  5.24
8   1997-02-28  5.26
9   1997-03-07  5.30
10  1997-03-14  5.30
.    ......     ...
.    ......     ...
.    ......     ...
I want to extract the DATE(s) on which the VALUE has
already dropped twice and the DATE(s) that VALUE has
already increased for three times,( ignore where
VALUE(i+1)-VALUE(i)=0),I try to use diff() function,
however that works only for one increase or decrease.

Sincerely,

Sean

______________________________________________
R-help@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

From arshia22 at yahoo.com  Sun Nov 28 12:20:22 2004
From: arshia22 at yahoo.com (ebashi)
Date: Sun Nov 28 12:20:26 2004
Subject: [R-sig-finance] syntax for a loop
Message-ID: <20041128112022.63070.qmail@web81002.mail.yahoo.com>

I'll appreciate if some one can help me with the
following loop. This is the logic of the loop,
if we have the following data;
> x.df
    x.dif
 .    .
 .    .
102  0.00
103  0.42
104  0.08
105  0.00
106  0.00
107  0.00
108 -0.16
109 -0.34
110  0.00
111 -0.17
112 -0.33
113  0.00
114  0.00
115  0.00
116  0.33
117  0.17
118  0.00 
 .    .
 .    .
I'm trying to find i's where 
  for (i in 2:length(x.dif))
  if (x.dif[i-1]<=0 and x.dif[i]>0 and x.dif[i+2]>0)
  it would return i+2 to me,
How can I turn this to a right format as a loop.(I
can't figure out the syntax)

Cheers,

Sean

From steve.moffitt at mail.stuart.iit.edu  Mon Nov 29 00:45:24 2004
From: steve.moffitt at mail.stuart.iit.edu (Steven D. Moffitt)
Date: Mon Nov 29 01:40:19 2004
Subject: [R-sig-finance] syntax for a loop
Message-ID: <200411281745.AA1419116724@stuart.iit.edu>

Create lagged vectors:

# corresponds to x.dif[i-1]
lag.minus1 <- c(NA,x.dif[-length(x.dif)])

# corresponds to x.dif[i+2]
lag.plus2 <- c(x.dif[-c(1,2)],NA,NA)

Then create a true/false vector:

selected.rows <- lag.minus1 <= 0 && x.dif > 0 && lag.plus2 > 0

Those indices you want will then have TRUE values.
To see row positions in x.dif, use the command

(1:length(x.dif))[selected.rows]


---------- Original Message ----------------------------------
From: ebashi <arshia22@yahoo.com>
Date:  Sun, 28 Nov 2004 03:20:22 -0800 (PST)

>I'll appreciate if some one can help me with the
>following loop. This is the logic of the loop,
>if we have the following data;
>> x.df
>    x.dif
> .    .
> .    .
>102  0.00
>103  0.42
>104  0.08
>105  0.00
>106  0.00
>107  0.00
>108 -0.16
>109 -0.34
>110  0.00
>111 -0.17
>112 -0.33
>113  0.00
>114  0.00
>115  0.00
>116  0.33
>117  0.17
>118  0.00 
> .    .
> .    .
>I'm trying to find i's where 
>  for (i in 2:length(x.dif))
>  if (x.dif[i-1]<=0 and x.dif[i]>0 and x.dif[i+2]>0)
>  it would return i+2 to me,
>How can I turn this to a right format as a loop.(I
>can't figure out the syntax)
>
>Cheers,
>
>Sean
>
>_______________________________________________
>R-sig-finance@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>

From abu3ammar at gmail.com  Mon Nov 29 19:54:46 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Mon Nov 29 19:55:24 2004
Subject: [R-sig-finance] R vs. S-PLUS
Message-ID: <b1d3150404112910545500a922@mail.gmail.com>

I am at a cross-road where I have to make a decision of going with R
or S-PLUS. I am mainly interested in S+'s finmetrics library or R's
Rmetrics. Can you please help me out by stating, out of experience,
the pros and cons of each?

From abu3ammar at gmail.com  Mon Nov 29 19:54:46 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Tue Nov 30 01:27:56 2004
Subject: [R-sig-finance] R vs. S-PLUS
Message-ID: <b1d3150404112910545500a922@mail.gmail.com>

I am at a cross-road where I have to make a decision of going with R
or S-PLUS. I am mainly interested in S+'s finmetrics library or R's
Rmetrics. Can you please help me out by stating, out of experience,
the pros and cons of each?

From dave at kanecap.com  Tue Nov 30 14:47:12 2004
From: dave at kanecap.com (David Kane)
Date: Tue Nov 30 14:47:02 2004
Subject: [R-sig-finance] R vs. S-PLUS
In-Reply-To: <b1d3150404112910545500a922@mail.gmail.com>
References: <b1d3150404112910545500a922@mail.gmail.com>
Message-ID: <16812.31200.895189.685279@gargle.gargle.HOWL>

Yasser El-Zein writes:
 > I am at a cross-road where I have to make a decision of going with R
 > or S-PLUS. I am mainly interested in S+'s finmetrics library or R's
 > Rmetrics. Can you please help me out by stating, out of experience,
 > the pros and cons of each?

Alas, I have never used finmetrics and have only touched on Rmetrics,
but I have used R in a production environment in the financial world
for 3+ years. There is no doubt in my mind that, for serious
statistical/computational work in finance, R is the best choice.

In terms of specific pros/cons, I would highlight the trajectory of R
over the last few years compared to the (much lower) trajectory of
S+. If R improves half as much (relative to S+) in the next 3 years as
it has over the last 3, the comparison between the two won't even be
worth discussing.

If you don't go with R now, you will someday.

Just my 2 cents,

David Kane

From MySubs at 3wplace.com  Tue Nov 30 16:50:11 2004
From: MySubs at 3wplace.com (My Newletters Etc.)
Date: Tue Nov 30 16:50:28 2004
Subject: [R-sig-finance] R vs. S-PLUS
In-Reply-To: <16812.31200.895189.685279@gargle.gargle.HOWL>
References: <b1d3150404112910545500a922@mail.gmail.com>
	<16812.31200.895189.685279@gargle.gargle.HOWL>
Message-ID: <opsiablxvki7oa02@104.dallas-20rh16rt.tx.dial-access.att.net>

On Tue, 30 Nov 2004 08:47:12 -0500, David Kane <dave@kanecap.com> wrote:

> Yasser El-Zein writes:
>  > I am at a cross-road where I have to make a decision of going with R
>  > or S-PLUS. I am mainly interested in S+'s finmetrics library or R's
>  > Rmetrics. Can you please help me out by stating, out of experience,
>  > the pros and cons of each?
>
> Alas, I have never used finmetrics and have only touched on Rmetrics,
> but I have used R in a production environment in the financial world
> for 3+ years. There is no doubt in my mind that, for serious
> statistical/computational work in finance, R is the best choice.
>
> In terms of specific pros/cons, I would highlight the trajectory of R
> over the last few years compared to the (much lower) trajectory of
> S+. If R improves half as much (relative to S+) in the next 3 years as
> it has over the last 3, the comparison between the two won't even be
> worth discussing.
>
> If you don't go with R now, you will someday.
>
> Just my 2 cents,
>
> David Kane
>

Hi David,

I'm wondering if you might be willing to share some specifics about
your use of R.  For example, what types of analysis have you done
with R?  Have you used pre-defined "packages" or have you "rolled
your own?"  Do you have any pointers about how to most effectively
approach learning R?  Hopefully these questions are sufficient
to give you a feel for the direction of my inquiry.

I'd be appreciative of any comments, suggestions, or insights
you are willing to share.

Thanks.

/JF

From dave at kanecap.com  Tue Nov 30 17:06:01 2004
From: dave at kanecap.com (David Kane)
Date: Tue Nov 30 17:05:48 2004
Subject: [R-sig-finance] R vs. S-PLUS
In-Reply-To: <opsiablxvki7oa02@104.dallas-20rh16rt.tx.dial-access.att.net>
References: <b1d3150404112910545500a922@mail.gmail.com>
	<16812.31200.895189.685279@gargle.gargle.HOWL>
	<opsiablxvki7oa02@104.dallas-20rh16rt.tx.dial-access.att.net>
Message-ID: <16812.39529.463769.246947@gargle.gargle.HOWL>

My Newletters Etc. writes:
 > I'm wondering if you might be willing to share some specifics about
 > your use of R.  

Of course. I am a real R evangalist.

 > For example, what types of analysis have you done
 > with R?  

I work in quantitive global equity modeling. How much is a share of
IBM worth? For me, the power in R is not so much that the statistical
tools are fancier than what one find in SAS or Stata or whatever ---
although this is often the case --- but that the programming language
is richer and the production tools (especially packages and test
cases) are so easy to use.

 > Have you used pre-defined "packages" or have you "rolled
 > your own?"  

I use all sorts of R packages but, for the actual financial analysis
parts, have had to role my own. It is on my to-do list to more fully
explore things like Rmetrics. I am unaware of any packages devoted to
the sort of stuff that I need to do regularly. As an example, I am
today calculating growth rates of various sorts. Which company has the
fastest growing sales? The answer to this question depends on all
sorts of sticky points that reasonable people can disagree about.

 > Do you have any pointers about how to most effectively
 > approach learning R?  Hopefully these questions are sufficient
 > to give you a feel for the direction of my inquiry.

The standard R documentation is a fine place to begin. I would start
by reading An Introduction to R cover to cover (while doing the
exercises) and then, depending on your level of computer experience,
going on to Writing R Extensions.

I encourage you to use R, especially in academics. To the extent that
you believe, as I do, that research should be public and replication
easy (see: http://gking.harvard.edu/replrepl/replrepl.html), R
provides the perfect tool.


-- 
David Kane
Kane Capital Management

From gcable at insightful.com  Tue Nov 30 18:58:13 2004
From: gcable at insightful.com (Gary Cable)
Date: Tue Nov 30 18:58:19 2004
Subject: [R-sig-finance] R vs. S-PLUS
Message-ID: <0AED0D29DB783641A47A3BD7C4FE1476A976F8@se2kexch02.insightful.com>

Hello all, 

I recently joined Insightful Corporation as Product Manager for Financial
Solutions. I continue to hear a theme of R versus S and I find it somewhat
curious. R is based on S and has gained a following based on a number of
contributions from academia and industry experts.
 
I prefer to view R as one evolutionary path that S has taken. It has the same
positives that any other GNU tool that I have seen in my professional
life--it is free and open for everyone's use and contribution. This is both
it's strength and it's weakness. It is a strength because people can use it
as a sandbox for their ideas; it can also be a weakness because it (IMO) does
not represent a commercial tool that provides me with stability, product
support, and professional services. 

R is primarily an academic solution. Diethelm Wuertz states, "A major aim is
to bring financial algorithms and concepts together under a common software
platform and to make it public available mainly for teaching financial
engineering and computational finance." and  "Rmetrics is embedded in R, both
building an environment which creates especially for students and researchers
in the third world a first class system for applications in statistics and
finance." 

I don't want to detract from the success of the R language, but if I were
going to build solutions in a highly regulated industry that expects
documentation, official support and standardization -- I would be inclined
toward a commercial solution rather than a freeware solution. This decision
comes from twenty years of experience of application development. 

I recently got this information from the RMETRICS web site.

"No warranty for this free software." (GNU License) 
RMETRICS website states "Use "RMETRICS" at your own risk! 
For commercial and business applications we recommend to use S-PLUS from
www.Insightful.Com"
Multiple packages may or may not be supported or standardized 
Commercial deployment options may be limited (GNU License)

As a product manager for Insightful, I can offer the following:

Insightful owns the IP (the S Language) and is committed to its ongoing
improvement.
Insightful provides product support for multiple platforms (Windows, UNIX,
Linux) for both desktop and server solutions.
Insightful is committed to providing world class solutions to finance, and
intends that finance will represent a major part of its market share.
S+FinMetrics continues to receive contributions from industry leaders in
financial statistics and econometrics.
Insightful staff (25% of whom are PhD's) are committed to the support of our
customers in finance.

And finally, I am personally committed to provide the financial tools and
solutions that you (our customers) need and expect.

Please feel free to contact me directly if you'd like to discuss R and S or
have any questions about Insightful products/services. 

gcable@insightful.com
800-569-0123 ext. 460

Gary Cable

Senior Manager of Finance
Insightful Financial Products and Services 
gcable@insightful.com



	[[alternative HTML version deleted]]

From dave at kanecap.com  Tue Nov 30 19:36:09 2004
From: dave at kanecap.com (David Kane)
Date: Tue Nov 30 19:35:54 2004
Subject: [R-sig-finance] R vs. S-PLUS
In-Reply-To: <0AED0D29DB783641A47A3BD7C4FE1476A976F8@se2kexch02.insightful.com>
References: <0AED0D29DB783641A47A3BD7C4FE1476A976F8@se2kexch02.insightful.com>
Message-ID: <16812.48537.651455.509202@gargle.gargle.HOWL>

Gary Cable writes:
 > I recently joined Insightful Corporation as Product Manager for Financial
 > Solutions. I continue to hear a theme of R versus S and I find it somewhat
 > curious. 

The theme that I hear most often is R versus S+, both being
implementations of the S language.

 > it can also be a weakness because it (IMO) does not represent a
 > commercial tool that provides me with stability, product support,
 > and professional services.

During 2001, I used both R and S+ simultaneously for some relatively
serious work. I found R superior then in terms of stability and
product support. Others opinions may differ.

 > R is primarily an academic solution. 

This is your opinion. Billions of dollars in real assets around the
globe are run using R every day. Maybe those managers are all idiots,
but I doubt it.

 > Insightful owns the IP (the S Language) and is committed to its ongoing
 > improvement.

Does Insightful own the S language? I do not believe that this is true.

Dave Kane

PS. My purpose here is not to start a flame war. I can think of
reasons why someone in finance might prefer S+ to R --- easy data
retrieval from Factset and Bloomberg comes to mind. But there was no
way that I could let a claim about R being "primarily an academic
solution" go unchallenged.

From gcable at insightful.com  Tue Nov 30 19:55:25 2004
From: gcable at insightful.com (Gary Cable)
Date: Tue Nov 30 19:55:29 2004
Subject: [R-sig-finance] R vs. S-PLUS
Message-ID: <0AED0D29DB783641A47A3BD7C4FE1476A976FD@se2kexch02.insightful.com>

Hi David,

Don't want to start a "flame war" either... 

Just a couple of points to clear the air and provide an update to your
information since 2001...

Insightful bought the rights to S in January 2004.

The comments regarding R as being primarily academic are quotes from the
RMETRICS web site. When I worked for UBS, we used freeware, spreadsheets and
S-PLUS (which was then still owned by Lucent). While we had large sums
exposed to the use of these tools, I am only stating my preference for
commercially supported products. I understand that a number of groups have
been using R for commercial purposes; I am simply saying that I believe that
Insightful may offer a better alternative in the long run.

There have been and will continue to be many improvements to S-PLUS and
Insightful products, support and documentation since 2001 -- including
several books and numerous contributions by Eric Zivot, Doug Martin,
Alexander McNeil, Rene Carmona, Mark Salmon... and the list goes on.

My interest is primarily in promoting what I think is a fine set of products
from a committed organization, and it is not my interest to detract from R,
but to point out what I believe are compelling reasons to use S-PLUS,
S+FinMetrics, S+NUOPT and the rest of our products for commercial solutions.

Regards,

Gary Cable

 

-----Original Message-----
From: David Kane [mailto:dave@kanecap.com] 
Sent: Tuesday, November 30, 2004 10:36 AM
To: Gary Cable
Cc: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] R vs. S-PLUS

Gary Cable writes:
 > I recently joined Insightful Corporation as Product Manager for Financial
> Solutions. I continue to hear a theme of R versus S and I find it somewhat
> curious. 

The theme that I hear most often is R versus S+, both being implementations
of the S language.

 > it can also be a weakness because it (IMO) does not represent a  >
commercial tool that provides me with stability, product support,  > and
professional services.

During 2001, I used both R and S+ simultaneously for some relatively serious
work. I found R superior then in terms of stability and product support.
Others opinions may differ.

 > R is primarily an academic solution. 

This is your opinion. Billions of dollars in real assets around the globe are
run using R every day. Maybe those managers are all idiots, but I doubt it.

 > Insightful owns the IP (the S Language) and is committed to its ongoing  >
improvement.

Does Insightful own the S language? I do not believe that this is true.

Dave Kane

PS. My purpose here is not to start a flame war. I can think of reasons why
someone in finance might prefer S+ to R --- easy data retrieval from Factset
and Bloomberg comes to mind. But there was no way that I could let a claim
about R being "primarily an academic solution" go unchallenged.

From dave at kanecap.com  Tue Nov 30 20:22:09 2004
From: dave at kanecap.com (David Kane)
Date: Tue Nov 30 20:21:59 2004
Subject: [R-sig-finance] R vs. S-PLUS
In-Reply-To: <0AED0D29DB783641A47A3BD7C4FE1476A976FD@se2kexch02.insightful.com>
References: <0AED0D29DB783641A47A3BD7C4FE1476A976FD@se2kexch02.insightful.com>
Message-ID: <16812.51297.217894.997310@gargle.gargle.HOWL>

Gary Cable writes:
 > Insightful bought the rights to S in January 2004.

Thanks for taking the time to clarify this point. For those who are
interested, a news release is available here.

http://www.insightful.com/news_events/release.asp?RID=180

To be precise, Insightful "announced today that on January 19, 2004,
the company acquired the copyrights to the software code underlying
the "S" language from Lucent Technologies for $2 million."

Although I am not an intellectual property lawyer, I'll note that
"rights to S" is not the same thing as the "copyrights to the software
code underlying the "S" language." If Insightful actually had the
"rights to S," they would be able to able to charge me a fee for using
R, a dialect of S. Fortunately, this not the case.

Dave Kane

From arshia22 at yahoo.com  Tue Nov 30 20:55:27 2004
From: arshia22 at yahoo.com (ebashi)
Date: Tue Nov 30 20:55:29 2004
Subject: [R-sig-finance] xy_plot
Message-ID: <20041130195527.24048.qmail@web81003.mail.yahoo.com>



> R users; 
> Does anyone have a recommendation for a faster
> plotting function rather than plot(x,y)?
> when I use plot(x,y) for a large number of
> variables,
> it take couple of moments to plot the graph?
> 
> Sincerely,
> 
> Sean
>

From vograno at yahoo.com  Tue Nov 30 21:03:39 2004
From: vograno at yahoo.com (Vadim Ogranovich)
Date: Tue Nov 30 21:26:46 2004
Subject: [R-sig-finance] R vs. S-PLUS
In-Reply-To: <0AED0D29DB783641A47A3BD7C4FE1476A976F8@se2kexch02.insightful.com>
Message-ID: <200411302003.iAUK3RoD027154@hypatia.math.ethz.ch>

 

> -----Original Message-----
> From: r-sig-finance-bounces@stat.math.ethz.ch 
> [mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of 
> Gary Cable
> Sent: Tuesday, November 30, 2004 9:58 AM
> To: r-sig-finance@stat.math.ethz.ch
> Subject: [R-sig-finance] R vs. S-PLUS
> 
> Hello all, 
> 
> I recently joined Insightful Corporation as Product Manager 
> for Financial Solutions. I continue to hear a theme of R 
> versus S and I find it somewhat curious. R is based on S and 
> has gained a following based on a number of contributions 
> from academia and industry experts.
>  
> I prefer to view R as one evolutionary path that S has taken. 
> It has the same positives that any other GNU tool that I have 
> seen in my professional life--it is free and open for 
> everyone's use and contribution. This is both it's strength 
> and it's weakness. It is a strength because people can use it 
> as a sandbox for their ideas; it can also be a weakness 
> because it (IMO) does not represent a commercial tool that 
> provides me with stability, product support, and professional 
> services. 

Hi Gary,


Welcome to the exciting world of S.

I guess you are really new to Insightful. My more than a decade experience
is different from what you outlined above. I used to use S-Plus until about
year 2000 in a company with a decent IT budget. Most of the so called
support was coming from the S-news list. I did contact Insightful for
support on issues I could not resolve on the S-news list. In both cases the
answer was "it is too deep in the language, we can not fix it". One of those
issues was related to the introduction of the new S classes in S-Plus (they
were basically unusable at that time). A similar introduction in R has
recently gone very smoothly.
In 2000 or 2001 (R-1.3) I figured R to be a superior system to S-Plus and
had switched, some of my fellow colleagues followed the suit. I've found R
to be very stable as far as our production requirements are concerned. It
has a very predictable release schedule with a procedure for features
deprecation and removal so you are never caught by a surprise.

The support that comes from R-help list is more than enough and is 24*7*365.
The only real drawback is that the language of the replies is sometimes too
"mentoring" so sensitive people can get upset (this is where Insightful does
have an edge). Bugs get fixed almost instantly.

I am sure you know, but just in case, the "no warranty and no support"
clauses in GPL are merely to protect the software contributor against
lawsuits in some litigious environments, they are not representative of the
actual support you are going to have with the software.

Please let me know if there is anything else to product support that I
didn't address.


R is not perfect, with it I too have run into a couple of issues (related to
the speed of file reading and database connectivity) which I was not able to
resolve on R-help / R-devel list to my satisfaction. In both cases I looked
at the underlying C code, figured out the problem and wrote a couple of
packages to work around them. I had the very same problems with S-Plus too
and the only option I had back then was to use Perl for IO intensive jobs.

Now about the professional services. You can get R training, you can hire an
R consultant to write a specialized package for you, etc. I was surprised to
learn how much of a supply of such services is out there.

Sorry if you already knew all of this. I just wanted to give you some info
that you might find useful in your new role of the product manager.

Regards,
Vadim

From patrick at burns-stat.com  Tue Nov 30 21:31:23 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue Nov 30 21:32:39 2004
Subject: [R-sig-finance] R vs. S-PLUS
In-Reply-To: <0AED0D29DB783641A47A3BD7C4FE1476A976F8@se2kexch02.insightful.com>
References: <0AED0D29DB783641A47A3BD7C4FE1476A976F8@se2kexch02.insightful.com>
Message-ID: <41ACD89B.8040403@burns-stat.com>



Gary Cable wrote:

>
>I don't want to detract from the success of the R language, but if I were
>going to build solutions in a highly regulated industry that expects
>documentation, official support and standardization -- I would be inclined
>toward a commercial solution rather than a freeware solution. This decision
>comes from twenty years of experience of application development. 
>  
>

The usual argument for preferring commercial software is that the commercial
software will be better written, tested, documented and supported.  The 
expectation
is that the free software will have many bugs, and the commercial 
software will
have few bugs.

Perhaps my experience is unusual, but in using S-PLUS for Windows for 4 
months
I found 2 serious bugs, 2 very annoying bugs and a few minor bugs.  In 
2.5 years
of intense use of R I have found 4 or 5 minor bugs (almost all of which 
have now
been fixed).

I don't think playing the "commercial" card for S-PLUS is a good 
strategy until
the point that it has at least as good of quality control as R.

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

>
>  
>

From gcable at insightful.com  Tue Nov 30 21:41:00 2004
From: gcable at insightful.com (Gary Cable)
Date: Tue Nov 30 21:41:05 2004
Subject: [R-sig-finance] R vs. S-PLUS
Message-ID: <0AED0D29DB783641A47A3BD7C4FE1476A97700@se2kexch02.insightful.com>

Vadim,

Thanks for your response.

For the most part I understand your experiences, and have to say that since
my initial involvement with S and S-PLUS started when I worked at Bell Labs
in the early 90's and later with Mathsoft while I was at UBS Warburg in the
late 90's, I experienced the issues you have. I can only say that much of
this had to do with Mathsoft/Insightful's inability to do anything with the
base language because of ownership issues. Now that we own the S Language, we
can have a significant impact on the product line.

I also have to say that from the late 90's on through very recently, the
issues of support and commitment to the finance vertical were not as clearly
defined as they are today. We have had significant changes in management that
I believe provide us with the right mix of people and vision. I hope that I
bring my experience (not unlike yours) to bear on constant improvement and
innovation of Insightful products.

I would not have joined Insightful unless I truly believed in its people and
product potential, and I hope to work with all our customers to provide the
best products and services.

I do not want to underestimate the value of R, but rather promote the
positives of Insightful.

Regards,

Gary

-----Original Message-----
From: Vadim Ogranovich [mailto:vograno@yahoo.com] 
Sent: Tuesday, November 30, 2004 12:04 PM
To: Gary Cable; r-sig-finance@stat.math.ethz.ch
Subject: RE: [R-sig-finance] R vs. S-PLUS

 

> -----Original Message-----
> From: r-sig-finance-bounces@stat.math.ethz.ch
> [mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Gary 
> Cable
> Sent: Tuesday, November 30, 2004 9:58 AM
> To: r-sig-finance@stat.math.ethz.ch
> Subject: [R-sig-finance] R vs. S-PLUS
> 
> Hello all,
> 
> I recently joined Insightful Corporation as Product Manager for 
> Financial Solutions. I continue to hear a theme of R versus S and I 
> find it somewhat curious. R is based on S and has gained a following 
> based on a number of contributions from academia and industry experts.
>  
> I prefer to view R as one evolutionary path that S has taken. 
> It has the same positives that any other GNU tool that I have seen in 
> my professional life--it is free and open for everyone's use and 
> contribution. This is both it's strength and it's weakness. It is a 
> strength because people can use it as a sandbox for their ideas; it 
> can also be a weakness because it (IMO) does not represent a 
> commercial tool that provides me with stability, product support, and 
> professional services.

Hi Gary,


Welcome to the exciting world of S.

I guess you are really new to Insightful. My more than a decade experience is
different from what you outlined above. I used to use S-Plus until about year
2000 in a company with a decent IT budget. Most of the so called support was
coming from the S-news list. I did contact Insightful for support on issues I
could not resolve on the S-news list. In both cases the answer was "it is too
deep in the language, we can not fix it". One of those issues was related to
the introduction of the new S classes in S-Plus (they were basically unusable
at that time). A similar introduction in R has recently gone very smoothly.
In 2000 or 2001 (R-1.3) I figured R to be a superior system to S-Plus and had
switched, some of my fellow colleagues followed the suit. I've found R to be
very stable as far as our production requirements are concerned. It has a
very predictable release schedule with a procedure for features deprecation
and removal so you are never caught by a surprise.

The support that comes from R-help list is more than enough and is 24*7*365.
The only real drawback is that the language of the replies is sometimes too
"mentoring" so sensitive people can get upset (this is where Insightful does
have an edge). Bugs get fixed almost instantly.

I am sure you know, but just in case, the "no warranty and no support"
clauses in GPL are merely to protect the software contributor against
lawsuits in some litigious environments, they are not representative of the
actual support you are going to have with the software.

Please let me know if there is anything else to product support that I didn't
address.


R is not perfect, with it I too have run into a couple of issues (related to
the speed of file reading and database connectivity) which I was not able to
resolve on R-help / R-devel list to my satisfaction. In both cases I looked
at the underlying C code, figured out the problem and wrote a couple of
packages to work around them. I had the very same problems with S-Plus too
and the only option I had back then was to use Perl for IO intensive jobs.

Now about the professional services. You can get R training, you can hire an
R consultant to write a specialized package for you, etc. I was surprised to
learn how much of a supply of such services is out there.

Sorry if you already knew all of this. I just wanted to give you some info
that you might find useful in your new role of the product manager.

Regards,
Vadim

From arshia22 at yahoo.com  Tue Nov 30 21:43:48 2004
From: arshia22 at yahoo.com (ebashi)
Date: Tue Nov 30 21:43:51 2004
Subject: [R-sig-finance] a simple plot problem
Message-ID: <20041130204348.36359.qmail@web81003.mail.yahoo.com>

R users;

This is the case, I
want to plot for example Dow Jones Ind Average versus
Time (Time on X-axis) , I use Plot(t,djia) (t is for
time and djia for dow jones), and then I use lines(t,
djia) to make it a line type of graph. however it
takes   
a long time to graph this line because I think it
plots it point by point, on the other hand if I use
tsPlot from fBasics package, on the x-axis I don't get
time value. ( I know that tsPlot is for time series
objects, but I dont know how to convert my data.frame
data to a time series data type in a way that I get my
DATE column to be substitute in the x axis)

Thanks for your help,

Sean

From gcable at insightful.com  Tue Nov 30 21:45:34 2004
From: gcable at insightful.com (Gary Cable)
Date: Tue Nov 30 21:45:39 2004
Subject: [R-sig-finance] R vs. S-PLUS
Message-ID: <0AED0D29DB783641A47A3BD7C4FE1476A97702@se2kexch02.insightful.com>

Patrick,

Thanks for your response.

Please send me any outstanding issues you have and I will relate whether
these have been addressed or not. In any event, I can assure you that I will
do my best to meet your expectations.

Gary 

-----Original Message-----
From: Patrick Burns [mailto:patrick@burns-stat.com] 
Sent: Tuesday, November 30, 2004 12:31 PM
To: Gary Cable
Cc: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] R vs. S-PLUS



Gary Cable wrote:

>
>I don't want to detract from the success of the R language, but if I 
>were going to build solutions in a highly regulated industry that 
>expects documentation, official support and standardization -- I would 
>be inclined toward a commercial solution rather than a freeware 
>solution. This decision comes from twenty years of experience of application
development.
>  
>

The usual argument for preferring commercial software is that the commercial
software will be better written, tested, documented and supported.  The
expectation is that the free software will have many bugs, and the commercial
software will have few bugs.

Perhaps my experience is unusual, but in using S-PLUS for Windows for 4
months I found 2 serious bugs, 2 very annoying bugs and a few minor bugs.  In
2.5 years
of intense use of R I have found 4 or 5 minor bugs (almost all of which have
now been fixed).

I don't think playing the "commercial" card for S-PLUS is a good strategy
until the point that it has at least as good of quality control as R.

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

>
>  
>

From paul.medica at hp.com  Tue Nov 30 21:59:46 2004
From: paul.medica at hp.com (Medica, Paul A)
Date: Tue Nov 30 22:00:45 2004
Subject: [R-sig-finance] R vs. S-PLUS
Message-ID: <8422B8BDDBCF7045A59138E8E6A1457BDB3B90@cacexc01.americas.cpqcorp.net>

Enough with the Tennis match.
	Gary, why don't you send e-mire a student intro copy of S-Plus
with the 1 month expiration control date imbedded and the Fin-metrics
toolkit.   And e-mire,  why don't you download a copy of the R complier
at the Ill Univ. website and run some of the shared R code.   Make your
own decision based on your specific needs.

Paul A. Medica
Product Reliability Engineering
Personal Image & Printing Group
Hewlett-Packard Co.
o: 360-212-2766
paul.medica@hp.com


-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Patrick
Burns
Sent: Tuesday, November 30, 2004 12:31 PM
To: Gary Cable
Cc: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] R vs. S-PLUS




Gary Cable wrote:

>
>I don't want to detract from the success of the R language, but if I 
>were going to build solutions in a highly regulated industry that 
>expects documentation, official support and standardization -- I would 
>be inclined toward a commercial solution rather than a freeware 
>solution. This decision comes from twenty years of experience of 
>application development.
>  
>

The usual argument for preferring commercial software is that the
commercial software will be better written, tested, documented and
supported.  The 
expectation
is that the free software will have many bugs, and the commercial 
software will
have few bugs.

Perhaps my experience is unusual, but in using S-PLUS for Windows for 4 
months
I found 2 serious bugs, 2 very annoying bugs and a few minor bugs.  In 
2.5 years
of intense use of R I have found 4 or 5 minor bugs (almost all of which 
have now
been fixed).

I don't think playing the "commercial" card for S-PLUS is a good 
strategy until
the point that it has at least as good of quality control as R.

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

>
>  
>

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance

From gcable at insightful.com  Tue Nov 30 22:23:07 2004
From: gcable at insightful.com (Gary Cable)
Date: Tue Nov 30 22:23:12 2004
Subject: [R-sig-finance] R vs. S-PLUS
Message-ID: <0AED0D29DB783641A47A3BD7C4FE1476A97704@se2kexch02.insightful.com>

Paul has a good idea... And I would be happy to have Carol Wedekind contact
anyone else who may be interested regarding an evaluation copy of our
software. 

Gary Cable

-----Original Message-----
From: Medica, Paul A [mailto:paul.medica@hp.com] 
Sent: Tuesday, November 30, 2004 1:00 PM
To: Patrick Burns; Gary Cable
Cc: r-sig-finance@stat.math.ethz.ch
Subject: RE: [R-sig-finance] R vs. S-PLUS

Enough with the Tennis match.
	Gary, why don't you send e-mire a student intro copy of S-Plus with
the 1 month expiration control date imbedded and the Fin-metrics
toolkit.   And e-mire,  why don't you download a copy of the R complier
at the Ill Univ. website and run some of the shared R code.   Make your
own decision based on your specific needs.

Paul A. Medica
Product Reliability Engineering
Personal Image & Printing Group
Hewlett-Packard Co.
o: 360-212-2766
paul.medica@hp.com


-----Original Message-----
From: r-sig-finance-bounces@stat.math.ethz.ch
[mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Patrick Burns
Sent: Tuesday, November 30, 2004 12:31 PM
To: Gary Cable
Cc: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] R vs. S-PLUS




Gary Cable wrote:

>
>I don't want to detract from the success of the R language, but if I 
>were going to build solutions in a highly regulated industry that 
>expects documentation, official support and standardization -- I would 
>be inclined toward a commercial solution rather than a freeware 
>solution. This decision comes from twenty years of experience of 
>application development.
>  
>

The usual argument for preferring commercial software is that the commercial
software will be better written, tested, documented and supported.  The
expectation is that the free software will have many bugs, and the commercial
software will have few bugs.

Perhaps my experience is unusual, but in using S-PLUS for Windows for 4
months I found 2 serious bugs, 2 very annoying bugs and a few minor bugs.  In
2.5 years
of intense use of R I have found 4 or 5 minor bugs (almost all of which have
now been fixed).

I don't think playing the "commercial" card for S-PLUS is a good strategy
until the point that it has at least as good of quality control as R.

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

>
>  
>

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance

From ajayshah at mayin.org  Wed Dec  1 06:51:16 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Wed Dec  1 06:51:16 2004
Subject: [R-sig-finance] R vs. S-PLUS
In-Reply-To: <0AED0D29DB783641A47A3BD7C4FE1476A97700@se2kexch02.insightful.com>
References: <0AED0D29DB783641A47A3BD7C4FE1476A97700@se2kexch02.insightful.com>
Message-ID: <20041201055116.GD744@igidr.ac.in>

I am personally an R user, but I believe that one of the reasons which
made me more comfortable with R was that under certain states of
nature, I could drop into a commercial version.

So far, I have generated no revenues for the firm :-) but I do believe
that's possible (under certain states of nature).

I think there's a powerful duality between free software and
commercial software in this kind of situation. I'm reminded of the
time - long ago - when a bunch of guys close to the Free Software
Foundation created a company named Cygnus which gave commercial
support for GNU related software. The company got bought out by Red
Hat later <http://news.com.com/2100-1001-232971.html?legacy=cnet> but
it does serve to illustrate the principle.

Similarly I see a nice duality between linux and solaris - firms can
delicately calibrate how much they want to use free software, and how
much they want commercial software. I think both have a role. I think
both sides benefit from working together.

  Example: in finance, date and time is of great importance, and right
    now, it looks like the two are diverging - R is going the its/zoo
    way, and S/finmetrics have done their own thing. It will be of
    benefit to both sides if a common free software library does
    datetime.

  Example: The Eric Zivot book is great - but it should have pitched
    higher, at becoming something like MASS. That, of course, requires
    a much richer set of public domain libraries, not proprietary
    libraries.

I have thus far had 0 interactions with S. I am actively using R and
love it. I am completely unpersuaded when arguments are made that S is
technically superior. But I do recognise a potential role for having
commercial support for R. Peering into the crystal ball, I expect that
the creative and intellectual part is best done in the free software
community, and the company will work best saying they are "a
commercial supported R", instead of saying they have proprietary
code. They can perhaps be like a stable release, and R can be
risk-taking.

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From gcable at insightful.com  Wed Dec  1 14:09:28 2004
From: gcable at insightful.com (Gary Cable)
Date: Wed Dec  1 14:09:36 2004
Subject: [R-sig-finance] R vs. S-PLUS
Message-ID: <0AED0D29DB783641A47A3BD7C4FE1476A97710@se2kexch02.insightful.com>

Sounds like the story of Insightful as well because of its roots in S... I
agree that we should work closely together when it comes to thought
leadership. My job is to make Insightful a successful commercial player in
financial statistics and more. I work closely with Eric and I'm sure he will
be interested in your ideas. 

Gary Cable

Senior Manager of Finance
Insightful Financial Products and Services
gcable@insightful.com 
Insightful Corporation
1700 Westlake Avenue North
Suite 500
Seattle, WA 98109-3044
USA



-----Original Message-----
From: Ajay Shah [mailto:ajayshah@mayin.org] 
Sent: Tuesday, November 30, 2004 9:51 PM
To: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] R vs. S-PLUS

I am personally an R user, but I believe that one of the reasons which made
me more comfortable with R was that under certain states of nature, I could
drop into a commercial version.

So far, I have generated no revenues for the firm :-) but I do believe that's
possible (under certain states of nature).

I think there's a powerful duality between free software and commercial
software in this kind of situation. I'm reminded of the time - long ago -
when a bunch of guys close to the Free Software Foundation created a company
named Cygnus which gave commercial support for GNU related software. The
company got bought out by Red Hat later
<http://news.com.com/2100-1001-232971.html?legacy=cnet> but it does serve to
illustrate the principle.

Similarly I see a nice duality between linux and solaris - firms can
delicately calibrate how much they want to use free software, and how much
they want commercial software. I think both have a role. I think both sides
benefit from working together.

  Example: in finance, date and time is of great importance, and right
    now, it looks like the two are diverging - R is going the its/zoo
    way, and S/finmetrics have done their own thing. It will be of
    benefit to both sides if a common free software library does
    datetime.

  Example: The Eric Zivot book is great - but it should have pitched
    higher, at becoming something like MASS. That, of course, requires
    a much richer set of public domain libraries, not proprietary
    libraries.

I have thus far had 0 interactions with S. I am actively using R and love it.
I am completely unpersuaded when arguments are made that S is technically
superior. But I do recognise a potential role for having commercial support
for R. Peering into the crystal ball, I expect that the creative and
intellectual part is best done in the free software community, and the
company will work best saying they are "a commercial supported R", instead of
saying they have proprietary code. They can perhaps be like a stable release,
and R can be risk-taking.

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance

From Bernhard.Pfaff at drkw.com  Thu Dec  2 17:22:49 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Thu Dec  2 17:22:55 2004
Subject: [R-sig-finance] Fractional Integration: Dolado & Gonzalo & Mayoral
 (2002), A Frac
 tional Dickey-Fuller Test for Unit Roots, Econometrica
Message-ID: <29E0BC0C716A584582941615CF9FFB0902585D89@ibfftce107.de.ad.drkw.net>

Dear list member,

has anybody implemented the above test or other functions with respect to
fractional integration and is willing to make his/her code public? If not, I
will implement the necessary functionalities in `urca'. 

Incidentally, a new version of urca has been uploaded to CRAN (0.6-9,
currently tarball only). The main new features are an `add-in' to `Rcmdr'
shipped in the package's /inst directory and the introduction of \concept in
*.Rd. For further information consult the ChangeLog.

Best Regards,
Bernhard 

ps: I am aware of the packages `fracdiff' as well as `fseries'.


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From Robert at sanctumfi.com  Thu Dec  2 19:09:55 2004
From: Robert at sanctumfi.com (Robert Sams)
Date: Thu Dec  2 19:00:49 2004
Subject: [R-sig-finance] status of the its package
Message-ID: <E585EABA11227445B918BFB74C1A4D36015977@sanctum01.sanctumfi.com>

is the its package still maintained and, if so, who maintains it? the latest (v1.0.3) documentation states that this is giles heywood at commertzbank but mail sent to giles.heywood@commerzbankib.com bounces.

also, i welcome *brief* commentary on experience using its or zoo; i'd like to get a rough idea of who uses these packages and for what purposes. thanks.

cheers,
robert




Robert Sams

SANCTUM FI LLP
Charles House
18B Charles Street
Mayfair
London W1J 5DU
Tel: +44 (0) 207 667 6360
Dir: +44 (0) 207 667 6363
fax: +44 (0) 207 667 6460
email: robert@sanctumfi.com

Authorised and Regulated by the FSA. 

Sending encrypted mail:

See http://pgp.mit.edu (search string 'sanctumfi') for updates.

-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: GnuPG v1.0.6 (GNU/Linux)
Comment: For info see http://www.gnupg.org

mQGiBD/xA1kRBAC2RUt8WyHDjXDoO1eu2Mli83cuEV37FicaBs/Wj5ry1QIz1drj
ubP25QQvu0lYOOnP7iS48bcOVP77uQYqLbsvzQ4fChFLCg9O3D4ourolZEK69ooJ
74r87PPV/LEnURL4T8E7QCDrRmylJ1iCffgJ9JWkAN4qUG+6fFuOyiqxDwCgiMkM
oLaYHjl3unc7Anx59xAlm+MD/R1EdXM9uewCj2kS3SdN+akklg6QVy+gTl3+HGzc
MC6ZcLsk1uIklkvCfoYDN7oeC3uVGik7QYkE3n02vfAMGjW7AqsQEoblzi3QscoX
Kzitd04NcWeDYIXRThCCydIJ64DdpF293ewJf2fRykmBdAbV0oaqL+zgdrRqFxYJ
m2d9A/9m3DoHwXBXPyqueX8naY5hmmeG+mihkdI4H4MBmaPJZW9DnZxor6P2Nm/X
muDSf7aZ1t9J0t75oEY/SyjoCYhWlMJS+wnOUq3u5XRNRyo9oI1qQaF5zw8ZmIzW
x7nfQMlXpXpETGwbNxZuUoucbN1cDrZBymHxlI5AiBCZ/fya3rQzUm9iZXJ0IFNh
bXMgKFNhbmN0dW0gRkkgTExQKSA8cm9iZXJ0QHNhbmN0dW1maS5jb20+iF0EExEC
AB0FAj/xA1kFCQHhM4AFCwcKAwQDFQMCAxYCAQIXgAAKCRD3WGga9bCIIPV6AJ9t
cDQkN8jW8CxzbU5K2O3dxaMlKwCeNOYQsAgW7S1qAlM345QpDXxbjgC5Ag0EP/ED
bRAIANoxq2NjQFdrUMSlQEaitR3pFTmOC7n2rICBAbU/hxlVs1PFSyh6Tr00vzFM
py6n4uBCOzrz3b5u5YbRaQzs8ipkqnSzoDD6GKfMEEWYQvZ76kkShWt5zDkLQ2X4
V4X+xJ0iuFT+9cK7VuJ102pjsAwltDUGPKsSwWqs55tzBN8BwAxqNMxRtNYbOTAB
Dpnm1BsiZ1TLqHIr4a1t2ZEuqhKV0HEP9VugP9XQz9u1f5QZrriNW/foxBwLuXS7
g+945IGXZq/qxHzgQjJQhC3jIaHUrchrQxy6snoQxgAnuO2/g9SLI8BBsvpVZ+Ac
mpkhPtT4pGujwsK/oRFDloAb7BsAAwYIAMlQrB2GPn1ZNFIf9zN+euTv2jWx5Hv4
ZEhqeBTqq00KCT3NSrnOHBTX4x6F4L+ofRzl2L5zIi685wWTpLgqQI7hzKvAxerJ
xe1qpz5GfGX976uaqxEfzwQZqcZB2iihhjeOUTxalSWdkX73yNtRmLLikTr0U3E0
v0dB1laMqldYub4X+GeH7tAeQGqYfS6Y+BdNDWfIcwADM0ggLIbNsw+IsjdQNOpq
5R4p1E2o5kfvafIFLpMOZACKKdTBkfiAqOZq8ezDpNHwLrRG4RvQ1K80pGGqaikI
XFbJIthvimA5w4MjvenuIn367zj+bz5eFE7YeQ0KG7NAdg2DkxD4W9CITAQYEQIA
DAUCP/EDbQUJAeEzgAAKCRD3WGga9bCIIL8RAJ4o9zXtkqK5RMKXxJTmZejtDjTC
kwCdFevBc9z4ermWaKb9BsDU7OYdgM8=
=6Y3T
-----END PGP PUBLIC KEY BLOCK-----

From Whit.Armstrong at tudor.com  Thu Dec  2 19:06:43 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Thu Dec  2 19:07:03 2004
Subject: [R-sig-finance] status of the its package
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF042B3BB5@tudor.com>

I'm the current maintainer of its.  I took over from Giles about a month
ago.  I should have a new release in the next few days.

I'm sure the zoo team will get back to you in a few minutes with a more
verbose answer regarding the differences between the two packages, but
briefly zoo allows flexibility in the dates one uses for the time series
while its forces POSIXct dates to be used.  Zoo is a great package and I
strongly encourage you to give both its and zoo a try.  There is no reason
not to use both since objects can be easily converted back and forth.

Regards,
Whit Armstrong

From hoonkim380 at yahoo.com  Thu Dec  2 20:02:02 2004
From: hoonkim380 at yahoo.com (Hoon Kim)
Date: Thu Dec  2 20:02:12 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
Message-ID: <20041202190202.2978.qmail@web52301.mail.yahoo.com>

Dear David

I appreciate sharing your experience using R in
finance.  I am also doing quant equity research and my
main tool is SAS.  I do not have much experience with
R at this stage and I am curious what is your opinion
on R vs. SAS.  My perception is SAS is much better in
handling large data sets, which is usually the case in
quant equity modeling.  Historical backtesting data
can be easily over several hundread megabites.  In my
opinion, other than the capability of handling large
data set in SAS, I think R is a more flexible
programming language.

Could you kindly share your thoughts with me on this
issue?  Do you have any problem in handling large data
sets in R?  How do you deal with large data set in R? 
Do you have recommendation on handling those large
data in R?

Thanks again for sharing your thoughts.

Best,

Hoon Kim


------------------------------

Message: 3
Date: Tue, 30 Nov 2004 11:06:01 -0500
From: David Kane <dave@kanecap.com>
Subject: Re: [R-sig-finance] R vs. S-PLUS
To: "My Newletters Etc." <MySubs@3wplace.com>
Cc: r-sig-finance@stat.math.ethz.ch
Message-ID:
<16812.39529.463769.246947@gargle.gargle.HOWL>
Content-Type: text/plain; charset=us-ascii

My Newletters Etc. writes:
 > I'm wondering if you might be willing to share some
specifics about
 > your use of R.  

Of course. I am a real R evangalist.

 > For example, what types of analysis have you done
 > with R?  

I work in quantitive global equity modeling. How much
is a share of
IBM worth? For me, the power in R is not so much that
the statistical
tools are fancier than what one find in SAS or Stata
or whatever ---
although this is often the case --- but that the
programming language
is richer and the production tools (especially
packages and test
cases) are so easy to use.

 > Have you used pre-defined "packages" or have you
"rolled
 > your own?"  

I use all sorts of R packages but, for the actual
financial analysis
parts, have had to role my own. It is on my to-do list
to more fully
explore things like Rmetrics. I am unaware of any
packages devoted to
the sort of stuff that I need to do regularly. As an
example, I am
today calculating growth rates of various sorts. Which
company has the
fastest growing sales? The answer to this question
depends on all
sorts of sticky points that reasonable people can
disagree about.

 > Do you have any pointers about how to most
effectively
 > approach learning R?  Hopefully these questions are
sufficient
 > to give you a feel for the direction of my inquiry.

The standard R documentation is a fine place to begin.
I would start
by reading An Introduction to R cover to cover (while
doing the
exercises) and then, depending on your level of
computer experience,
going on to Writing R Extensions.

I encourage you to use R, especially in academics. To
the extent that
you believe, as I do, that research should be public
and replication
easy (see:
http://gking.harvard.edu/replrepl/replrepl.html), R
provides the perfect tool.


-- 
David Kane
Kane Capital Management

From pvirketis at hbk.com  Thu Dec  2 20:49:48 2004
From: pvirketis at hbk.com (Pijus Virketis)
Date: Thu Dec  2 20:49:53 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
Message-ID: <4DDAC3B7F3996A4291CBF8B0CDED8A7C024F46@NYCMAIL1.hbk.com>

Hoon, 

Your questions were addressed to David, but I hope he won't mind if I
interject. "How to handle a large dataset" is practically a FAQ on the
R-help list: search through the archives at
http://maths.newcastle.edu.au/~rking/R/. To summarise, first it must be
noted that thoughtful use of R (scan(), avoid silently copying data in
memory, etc.) helps handle very large datasets reasonably quickly; the
limit is basically the available amount of RAM. If you need to work on
more data than that, the best practice is to put it in a database, and
to access it in segments. The R gurus also usually encourage the
research to think whether all that data is truly necessary, or if a
subset of it could be used to draw the same conclusions without too much
trouble. 

Cheers, 

Pijus

> -----Original Message-----
> From: r-sig-finance-bounces@stat.math.ethz.ch 
> [mailto:r-sig-finance-bounces@stat.math.ethz.ch] On Behalf Of Hoon Kim
> Sent: Thursday, December 02, 2004 2:02 PM
> To: r-sig-finance@stat.math.ethz.ch
> Subject: Re: [R-sig-finance] R vs. S-PLUS vs. SAS
> 
> Dear David
> 
> I appreciate sharing your experience using R in
> finance.  I am also doing quant equity research and my
> main tool is SAS.  I do not have much experience with
> R at this stage and I am curious what is your opinion
> on R vs. SAS.  My perception is SAS is much better in
> handling large data sets, which is usually the case in
> quant equity modeling.  Historical backtesting data
> can be easily over several hundread megabites.  In my
> opinion, other than the capability of handling large
> data set in SAS, I think R is a more flexible
> programming language.
> 
> Could you kindly share your thoughts with me on this
> issue?  Do you have any problem in handling large data
> sets in R?  How do you deal with large data set in R? 
> Do you have recommendation on handling those large
> data in R?
> 
> Thanks again for sharing your thoughts.
> 
> Best,
> 
> Hoon Kim
> 
> 
> ------------------------------
> 
> Message: 3
> Date: Tue, 30 Nov 2004 11:06:01 -0500
> From: David Kane <dave@kanecap.com>
> Subject: Re: [R-sig-finance] R vs. S-PLUS
> To: "My Newletters Etc." <MySubs@3wplace.com>
> Cc: r-sig-finance@stat.math.ethz.ch
> Message-ID:
> <16812.39529.463769.246947@gargle.gargle.HOWL>
> Content-Type: text/plain; charset=us-ascii
> 
> My Newletters Etc. writes:
>  > I'm wondering if you might be willing to share some
> specifics about
>  > your use of R.  
> 
> Of course. I am a real R evangalist.
> 
>  > For example, what types of analysis have you done
>  > with R?  
> 
> I work in quantitive global equity modeling. How much
> is a share of
> IBM worth? For me, the power in R is not so much that
> the statistical
> tools are fancier than what one find in SAS or Stata
> or whatever ---
> although this is often the case --- but that the
> programming language
> is richer and the production tools (especially
> packages and test
> cases) are so easy to use.
> 
>  > Have you used pre-defined "packages" or have you
> "rolled
>  > your own?"  
> 
> I use all sorts of R packages but, for the actual
> financial analysis
> parts, have had to role my own. It is on my to-do list
> to more fully
> explore things like Rmetrics. I am unaware of any
> packages devoted to
> the sort of stuff that I need to do regularly. As an
> example, I am
> today calculating growth rates of various sorts. Which
> company has the
> fastest growing sales? The answer to this question
> depends on all
> sorts of sticky points that reasonable people can
> disagree about.
> 
>  > Do you have any pointers about how to most
> effectively
>  > approach learning R?  Hopefully these questions are
> sufficient
>  > to give you a feel for the direction of my inquiry.
> 
> The standard R documentation is a fine place to begin.
> I would start
> by reading An Introduction to R cover to cover (while
> doing the
> exercises) and then, depending on your level of
> computer experience,
> going on to Writing R Extensions.
> 
> I encourage you to use R, especially in academics. To
> the extent that
> you believe, as I do, that research should be public
> and replication
> easy (see:
> http://gking.harvard.edu/replrepl/replrepl.html), R
> provides the perfect tool.
> 
> 
> -- 
> David Kane
> Kane Capital Management
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> 
>

From Robert at sanctumfi.com  Thu Dec  2 21:29:20 2004
From: Robert at sanctumfi.com (Robert Sams)
Date: Thu Dec  2 21:20:13 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
Message-ID: <E585EABA11227445B918BFB74C1A4D3601597A@sanctum01.sanctumfi.com>

hi hoon,

a case study, perhaps relevant to your query: i use r on a debian machine within an emacs buffer, often working with financial time series between 20-40mb, usually pulled from a postgresql cluster via the package Rdbi. never had a problem due to the size of the data. 

ditto pijus' suggestion that you search the help archives... there's allot there concerning r and large datasets. also look at the manual "R Data Import/Export". since you're dealing with financial time series, also look at packages its and zoo.

never use sas, so can't offer any comments on the relative merits of the two packages.

cheers,
robert


Robert Sams

SANCTUM FI LLP
Charles House
18B Charles Street
Mayfair
London W1J 5DU
Tel: +44 (0) 207 667 6360
Dir: +44 (0) 207 667 6363
fax: +44 (0) 207 667 6460
email: robert@sanctumfi.com

Authorised and Regulated by the FSA. 

Sending encrypted mail:

See http://pgp.mit.edu (search string 'sanctumfi') for updates.

-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: GnuPG v1.0.6 (GNU/Linux)
Comment: For info see http://www.gnupg.org

mQGiBD/xA1kRBAC2RUt8WyHDjXDoO1eu2Mli83cuEV37FicaBs/Wj5ry1QIz1drj
ubP25QQvu0lYOOnP7iS48bcOVP77uQYqLbsvzQ4fChFLCg9O3D4ourolZEK69ooJ
74r87PPV/LEnURL4T8E7QCDrRmylJ1iCffgJ9JWkAN4qUG+6fFuOyiqxDwCgiMkM
oLaYHjl3unc7Anx59xAlm+MD/R1EdXM9uewCj2kS3SdN+akklg6QVy+gTl3+HGzc
MC6ZcLsk1uIklkvCfoYDN7oeC3uVGik7QYkE3n02vfAMGjW7AqsQEoblzi3QscoX
Kzitd04NcWeDYIXRThCCydIJ64DdpF293ewJf2fRykmBdAbV0oaqL+zgdrRqFxYJ
m2d9A/9m3DoHwXBXPyqueX8naY5hmmeG+mihkdI4H4MBmaPJZW9DnZxor6P2Nm/X
muDSf7aZ1t9J0t75oEY/SyjoCYhWlMJS+wnOUq3u5XRNRyo9oI1qQaF5zw8ZmIzW
x7nfQMlXpXpETGwbNxZuUoucbN1cDrZBymHxlI5AiBCZ/fya3rQzUm9iZXJ0IFNh
bXMgKFNhbmN0dW0gRkkgTExQKSA8cm9iZXJ0QHNhbmN0dW1maS5jb20+iF0EExEC
AB0FAj/xA1kFCQHhM4AFCwcKAwQDFQMCAxYCAQIXgAAKCRD3WGga9bCIIPV6AJ9t
cDQkN8jW8CxzbU5K2O3dxaMlKwCeNOYQsAgW7S1qAlM345QpDXxbjgC5Ag0EP/ED
bRAIANoxq2NjQFdrUMSlQEaitR3pFTmOC7n2rICBAbU/hxlVs1PFSyh6Tr00vzFM
py6n4uBCOzrz3b5u5YbRaQzs8ipkqnSzoDD6GKfMEEWYQvZ76kkShWt5zDkLQ2X4
V4X+xJ0iuFT+9cK7VuJ102pjsAwltDUGPKsSwWqs55tzBN8BwAxqNMxRtNYbOTAB
Dpnm1BsiZ1TLqHIr4a1t2ZEuqhKV0HEP9VugP9XQz9u1f5QZrriNW/foxBwLuXS7
g+945IGXZq/qxHzgQjJQhC3jIaHUrchrQxy6snoQxgAnuO2/g9SLI8BBsvpVZ+Ac
mpkhPtT4pGujwsK/oRFDloAb7BsAAwYIAMlQrB2GPn1ZNFIf9zN+euTv2jWx5Hv4
ZEhqeBTqq00KCT3NSrnOHBTX4x6F4L+ofRzl2L5zIi685wWTpLgqQI7hzKvAxerJ
xe1qpz5GfGX976uaqxEfzwQZqcZB2iihhjeOUTxalSWdkX73yNtRmLLikTr0U3E0
v0dB1laMqldYub4X+GeH7tAeQGqYfS6Y+BdNDWfIcwADM0ggLIbNsw+IsjdQNOpq
5R4p1E2o5kfvafIFLpMOZACKKdTBkfiAqOZq8ezDpNHwLrRG4RvQ1K80pGGqaikI
XFbJIthvimA5w4MjvenuIn367zj+bz5eFE7YeQ0KG7NAdg2DkxD4W9CITAQYEQIA
DAUCP/EDbQUJAeEzgAAKCRD3WGga9bCIIL8RAJ4o9zXtkqK5RMKXxJTmZejtDjTC
kwCdFevBc9z4ermWaKb9BsDU7OYdgM8=
=6Y3T
-----END PGP PUBLIC KEY BLOCK-----


> -----Original Message-----
> From: Hoon Kim [mailto:hoonkim380@yahoo.com]
> Sent: Thursday, December 02, 2004 7:02 PM
> To: r-sig-finance@stat.math.ethz.ch
> Subject: Re: [R-sig-finance] R vs. S-PLUS vs. SAS
> 
> 
> Dear David
> 
> I appreciate sharing your experience using R in
> finance.  I am also doing quant equity research and my
> main tool is SAS.  I do not have much experience with
> R at this stage and I am curious what is your opinion
> on R vs. SAS.  My perception is SAS is much better in
> handling large data sets, which is usually the case in
> quant equity modeling.  Historical backtesting data
> can be easily over several hundread megabites.  In my
> opinion, other than the capability of handling large
> data set in SAS, I think R is a more flexible
> programming language.
> 
> Could you kindly share your thoughts with me on this
> issue?  Do you have any problem in handling large data
> sets in R?  How do you deal with large data set in R? 
> Do you have recommendation on handling those large
> data in R?
> 
> Thanks again for sharing your thoughts.
> 
> Best,
> 
> Hoon Kim
> 
> 
> ------------------------------
> 
> Message: 3
> Date: Tue, 30 Nov 2004 11:06:01 -0500
> From: David Kane <dave@kanecap.com>
> Subject: Re: [R-sig-finance] R vs. S-PLUS
> To: "My Newletters Etc." <MySubs@3wplace.com>
> Cc: r-sig-finance@stat.math.ethz.ch
> Message-ID:
> <16812.39529.463769.246947@gargle.gargle.HOWL>
> Content-Type: text/plain; charset=us-ascii
> 
> My Newletters Etc. writes:
>  > I'm wondering if you might be willing to share some
> specifics about
>  > your use of R.  
> 
> Of course. I am a real R evangalist.
> 
>  > For example, what types of analysis have you done
>  > with R?  
> 
> I work in quantitive global equity modeling. How much
> is a share of
> IBM worth? For me, the power in R is not so much that
> the statistical
> tools are fancier than what one find in SAS or Stata
> or whatever ---
> although this is often the case --- but that the
> programming language
> is richer and the production tools (especially
> packages and test
> cases) are so easy to use.
> 
>  > Have you used pre-defined "packages" or have you
> "rolled
>  > your own?"  
> 
> I use all sorts of R packages but, for the actual
> financial analysis
> parts, have had to role my own. It is on my to-do list
> to more fully
> explore things like Rmetrics. I am unaware of any
> packages devoted to
> the sort of stuff that I need to do regularly. As an
> example, I am
> today calculating growth rates of various sorts. Which
> company has the
> fastest growing sales? The answer to this question
> depends on all
> sorts of sticky points that reasonable people can
> disagree about.
> 
>  > Do you have any pointers about how to most
> effectively
>  > approach learning R?  Hopefully these questions are
> sufficient
>  > to give you a feel for the direction of my inquiry.
> 
> The standard R documentation is a fine place to begin.
> I would start
> by reading An Introduction to R cover to cover (while
> doing the
> exercises) and then, depending on your level of
> computer experience,
> going on to Writing R Extensions.
> 
> I encourage you to use R, especially in academics. To
> the extent that
> you believe, as I do, that research should be public
> and replication
> easy (see:
> http://gking.harvard.edu/replrepl/replrepl.html), R
> provides the perfect tool.
> 
> 
> -- 
> David Kane
> Kane Capital Management
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>

From patrick at burns-stat.com  Thu Dec  2 21:41:38 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu Dec  2 21:42:55 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
In-Reply-To: <20041202190202.2978.qmail@web52301.mail.yahoo.com>
References: <20041202190202.2978.qmail@web52301.mail.yahoo.com>
Message-ID: <41AF7E02.2050707@burns-stat.com>

There was just a thread on R-help about R versus SAS.  Your
summary that SAS' main advantage is better handling of large
datasets is similar to the comments in that thread.  (There was
some sentiment that SAS was better with mixed effects models,
but that is unlikely to come into play much in finance.)

In the old days with S-PLUS, the rule of thumb was that you
needed 10 times as much memory as your dataset.  By that
standard you could handle a 200 MB dataset if you have 2GB
of RAM.  R (and current versions of S-PLUS) are more frugal
than S-PLUS was back then.  The 10 times rule was pretty
much a worst case -- if you do simple things, then you are
unlikely to use as much memory.

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Hoon Kim wrote:

>Dear David
>
>I appreciate sharing your experience using R in
>finance.  I am also doing quant equity research and my
>main tool is SAS.  I do not have much experience with
>R at this stage and I am curious what is your opinion
>on R vs. SAS.  My perception is SAS is much better in
>handling large data sets, which is usually the case in
>quant equity modeling.  Historical backtesting data
>can be easily over several hundread megabites.  In my
>opinion, other than the capability of handling large
>data set in SAS, I think R is a more flexible
>programming language.
>
>Could you kindly share your thoughts with me on this
>issue?  Do you have any problem in handling large data
>sets in R?  How do you deal with large data set in R? 
>Do you have recommendation on handling those large
>data in R?
>
>Thanks again for sharing your thoughts.
>
>Best,
>
>Hoon Kim
>
>
>------------------------------
>
>Message: 3
>Date: Tue, 30 Nov 2004 11:06:01 -0500
>From: David Kane <dave@kanecap.com>
>Subject: Re: [R-sig-finance] R vs. S-PLUS
>To: "My Newletters Etc." <MySubs@3wplace.com>
>Cc: r-sig-finance@stat.math.ethz.ch
>Message-ID:
><16812.39529.463769.246947@gargle.gargle.HOWL>
>Content-Type: text/plain; charset=us-ascii
>
>My Newletters Etc. writes:
> > I'm wondering if you might be willing to share some
>specifics about
> > your use of R.  
>
>Of course. I am a real R evangalist.
>
> > For example, what types of analysis have you done
> > with R?  
>
>I work in quantitive global equity modeling. How much
>is a share of
>IBM worth? For me, the power in R is not so much that
>the statistical
>tools are fancier than what one find in SAS or Stata
>or whatever ---
>although this is often the case --- but that the
>programming language
>is richer and the production tools (especially
>packages and test
>cases) are so easy to use.
>
> > Have you used pre-defined "packages" or have you
>"rolled
> > your own?"  
>
>I use all sorts of R packages but, for the actual
>financial analysis
>parts, have had to role my own. It is on my to-do list
>to more fully
>explore things like Rmetrics. I am unaware of any
>packages devoted to
>the sort of stuff that I need to do regularly. As an
>example, I am
>today calculating growth rates of various sorts. Which
>company has the
>fastest growing sales? The answer to this question
>depends on all
>sorts of sticky points that reasonable people can
>disagree about.
>
> > Do you have any pointers about how to most
>effectively
> > approach learning R?  Hopefully these questions are
>sufficient
> > to give you a feel for the direction of my inquiry.
>
>The standard R documentation is a fine place to begin.
>I would start
>by reading An Introduction to R cover to cover (while
>doing the
>exercises) and then, depending on your level of
>computer experience,
>going on to Writing R Extensions.
>
>I encourage you to use R, especially in academics. To
>the extent that
>you believe, as I do, that research should be public
>and replication
>easy (see:
>http://gking.harvard.edu/replrepl/replrepl.html), R
>provides the perfect tool.
>
>
>  
>

From Daltonmota at aol.com  Fri Dec  3 00:11:33 2004
From: Daltonmota at aol.com (Daltonmota@aol.com)
Date: Fri Dec  3 00:11:44 2004
Subject: [R-sig-finance] RE: R vs. S-PLUS
Message-ID: <49331ECE.7F970ED7.0ACFBA63@aol.com>

I have used both, and they very close to each other, being the main difference the already stated commercial support.

My personal preference is for R, since i belive the open architeture offers much more contribution possibilities among user community. 

For the very very risk activities where i cannot commit any error, i also use mat lab software, in place of R, or to check the main results outputed by R.

summary of my opinion :
Academic or personal use : R
Usual Professional activity : R
Very Risky or very complex Professional activity : R and/or S-Plus/Mat Lab to validate

From ggrothendieck at myway.com  Fri Dec  3 05:14:58 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri Dec  3 05:15:18 2004
Subject: [R-sig-finance] status of the its package
Message-ID: <20041203041458.E3CF5398E@mprdmxin.myway.com>



From:   Robert Sams <Robert@sanctumfi.com>

>also, i welcome *brief* commentary on experience using its or zoo; >i'd like to get a rough idea of who uses these packages and for >what purposes. thanks.

Of the irregular series its is the most mature.  fBasics
roughly follows S-Plus finmetrics and has a wide range of
financial routines in the associated rmetrics project.  It
has a sophisticated time zone system that sits on top of
POSIXct although it does require that the user set the
process or computer to GMT which may or may not be a problem
for you.  zoo is the only one that supports datetime classes
other than POSIXct (useful if you want to use Date or chron)
and is intended to be easy to use by following ts and R 
particularly closely.  I do not know eough about  the irts class 
in package tseries to provide useful characterization 
but it is another possibility.

Also the commercial S-Plus finmetrics package is available
although it works with S-Plus, not R.

irts and zoo use S3 which is simpler and faster while
its and fBasics use S4 which is more sophisticated.
All of the above seem to be under active development.
zoo has multiple concurrent developers (including moi).

From ajayshah at mayin.org  Fri Dec  3 08:00:47 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Fri Dec  3 08:00:45 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
In-Reply-To: <41AF7E02.2050707@burns-stat.com>
References: <20041202190202.2978.qmail@web52301.mail.yahoo.com>
	<41AF7E02.2050707@burns-stat.com>
Message-ID: <20041203070047.GO1586@igidr.ac.in>

On Thu, Dec 02, 2004 at 08:41:38PM +0000, Patrick Burns wrote:
> There was just a thread on R-help about R versus SAS.  Your
> summary that SAS' main advantage is better handling of large
> datasets is similar to the comments in that thread.  (There was
> some sentiment that SAS was better with mixed effects models,
> but that is unlikely to come into play much in finance.)
> 
> In the old days with S-PLUS, the rule of thumb was that you
> needed 10 times as much memory as your dataset.  By that
> standard you could handle a 200 MB dataset if you have 2GB
> of RAM.  R (and current versions of S-PLUS) are more frugal
> than S-PLUS was back then.  The 10 times rule was pretty
> much a worst case -- if you do simple things, then you are
> unlikely to use as much memory.

Hmm, so it'd be interesting to apply current prices of RAM and current
prices of R, to work out the break-even point at which it's better to
buy SAS! :-) Without making any calculations, I can't see how SAS can
compete with the price of 4G of RAM.

You do need 64 bit CPUs, though.

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From john.muller at bankofamerica.com  Fri Dec  3 16:42:44 2004
From: john.muller at bankofamerica.com (Muller, John)
Date: Fri Dec  3 16:42:50 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
Message-ID: <8D67B54C47F0A041B8AC53427E66AD6FD79AD6@ex2k.bankofamerica.com>


I have a question related to the discussion of R versus S-Plus versus
SAS
regarding handling large data sets.

I too have often heard that SAS is better with large data sets
but never sure whether "better" meant that the others had hard
limits on data set size or simply that SAS is faster.

Any insight into this from the group?

For example, does SAS use special algorithms (e.g. regression,
clustering,
tree construction) that do not require all the data in RAM
OR does SAS do very clever sampling OR ...?

I think SAS's Enterprise Miner does sampling by default.

Thanks for any insight you can offer.

- john muller
-------------------------------------------------
John H. Muller
mailto:john.muller@bankofamerica.com
404.607.5943

From patrick at burns-stat.com  Fri Dec  3 19:37:15 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Fri Dec  3 19:38:48 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
In-Reply-To: <8D67B54C47F0A041B8AC53427E66AD6FD79AD6@ex2k.bankofamerica.com>
References: <8D67B54C47F0A041B8AC53427E66AD6FD79AD6@ex2k.bankofamerica.com>
Message-ID: <41B0B25B.20406@burns-stat.com>

There may be some differences between SAS procedures, but
at least generally SAS does not require the whole data to be in
RAM.  Regression will take the data row by row and do an update
for the answer.

For many things SAS is remarkably fast as well.   Some of their
routines are decades old, when machines were very much slower
so it paid to put a lot of effort into efficient algorithms and code.

If SAS does exactly what you want, you'll get an answer quickly.
If you want something slightly different than what SAS does, you
are probably stuck.  That is one of the major differences between
SAS and R.

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Muller, John wrote:

>I have a question related to the discussion of R versus S-Plus versus
>SAS
>regarding handling large data sets.
>
>I too have often heard that SAS is better with large data sets
>but never sure whether "better" meant that the others had hard
>limits on data set size or simply that SAS is faster.
>
>Any insight into this from the group?
>
>For example, does SAS use special algorithms (e.g. regression,
>clustering,
>tree construction) that do not require all the data in RAM
>OR does SAS do very clever sampling OR ...?
>
>I think SAS's Enterprise Miner does sampling by default.
>
>Thanks for any insight you can offer.
>
>- john muller
>-------------------------------------------------
>John H. Muller
>mailto:john.muller@bankofamerica.com
>404.607.5943
>
>_______________________________________________
>R-sig-finance@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
>  
>

From Brandon.J.Whitcher at gsk.com  Fri Dec  3 20:46:05 2004
From: Brandon.J.Whitcher at gsk.com (Brandon.J.Whitcher@gsk.com)
Date: Fri Dec  3 20:47:13 2004
Subject: [R-sig-finance] Re: Fractional Integration: Dolado & Gonzalo &
 Mayoral (2002), 
 A Frac tional Dickey-Fuller Test for Unit Roots, Econometrica
In-Reply-To: <200412031246.iB3CiskM028492@hypatia.math.ethz.ch>
Message-ID: <OF988EC2B4.8F3C4102-ON80256F5F.006BF7EB-80256F5F.006CA8FF@sb.com>

> has anybody implemented the above test or other functions with respect 
to
> fractional integration and is willing to make his/her code public? If 
not, I
> will implement the necessary functionalities in `urca'.
> 
> ps: I am aware of the packages `fracdiff' as well as `fseries'.

A limited number of functions that deal with fractional difference 
processes (or long-memory processes) are available in the "waveslim" 
package.  Specifically, (1) ML-based parameter estimation for univariate 
LM processes using the discrete wavelet transform (DWT), (2) locating 
variance change points in LM processes, (3) ML-based parameter estimation 
for univariate seasonal LM processes, (4) examples using time series in 
finance and economics.

cheers...

Brandon


Brandon Whitcher, PhD
Translational Medicine & Technology
GlaxoSmithKline
Greenford Road
Greenford UB6 0HE, United Kingdom
	[[alternative HTML version deleted]]

From atp at piskorski.com  Sat Dec  4 13:15:40 2004
From: atp at piskorski.com (Andrew Piskorski)
Date: Sat Dec  4 13:15:51 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
In-Reply-To: <41B0B25B.20406@burns-stat.com>
References: <41B0B25B.20406@burns-stat.com>
Message-ID: <20041204121540.GA25064@piskorski.com>

On Fri, Dec 03, 2004 at 06:37:15PM +0000, Patrick Burns wrote:

> There may be some differences between SAS procedures, but
> at least generally SAS does not require the whole data to be in
> RAM.  Regression will take the data row by row and do an update
> for the answer.

Someone might want to ask Joe Conway about his experience and thoughts
integrating R as a procedural language inside PostgreSQL, to create
PL/R:

  http://www.joeconway.com/plr/
  http://gborg.postgresql.org/project/plr/projdisplay.php

(Hm, for good measure, I have Cc'd him on this email.)  Obviously, an
RDBMS like PostgreSQL is expert at dealing with data that doesn't fit
into RAM.  I've no idea whether PL/R does anything special to take
advantage of that, or how feasible it would be to do so.

Does anyone here know much about what makes R dependent on all data
being in RAM, or of links to same?  Is it just some centralized
low-level bits, or do broad swaths of code and algorithms all depend
on the in-RAM assumption?

How do SAS and other such systems avoid that?  Do they do this better
or much more more transparently than what an R user would do now
manually?  Where by "manually", I mean, query some fits-in-RAM amount
data out of an RDBMS (or other such on-disk store), analyze it, delete
the data to free up RAM, and repeat.

Could one say, tie a light-weight high-performance RDBMS library, like
SQLite, into R, and have R use it profitably to scale nicely on data
that does not fit in RAM?  In what way, if any, would this offer a
substantial advantage over current manual R-plus-RDBMS practice?

-- 
Andrew Piskorski <atp@piskorski.com>
http://www.piskorski.com/

From edd at debian.org  Sat Dec  4 17:31:53 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat Dec  4 17:32:01 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
In-Reply-To: <20041204121540.GA25064@piskorski.com>
References: <41B0B25B.20406@burns-stat.com>
	<20041204121540.GA25064@piskorski.com>
Message-ID: <20041204163153.GA12978@sonny.eddelbuettel.com>

On Sat, Dec 04, 2004 at 07:15:40AM -0500, Andrew Piskorski wrote:
> On Fri, Dec 03, 2004 at 06:37:15PM +0000, Patrick Burns wrote:
> 
> > There may be some differences between SAS procedures, but
> > at least generally SAS does not require the whole data to be in
> > RAM.  Regression will take the data row by row and do an update
> > for the answer.
> 
> Someone might want to ask Joe Conway about his experience and thoughts
> integrating R as a procedural language inside PostgreSQL, to create
> PL/R:
> 
>   http://www.joeconway.com/plr/
>   http://gborg.postgresql.org/project/plr/projdisplay.php
> 
> (Hm, for good measure, I have Cc'd him on this email.)  Obviously, an

Very good point, but you didn't CC Joe. Done now. Hi Joe :)

> RDBMS like PostgreSQL is expert at dealing with data that doesn't fit
> into RAM.  I've no idea whether PL/R does anything special to take
> advantage of that, or how feasible it would be to do so.
> 
> Does anyone here know much about what makes R dependent on all data
> being in RAM, or of links to same?  Is it just some centralized
> low-level bits, or do broad swaths of code and algorithms all depend
> on the in-RAM assumption?

Discount my $0.02 severely enough as I don't really know what I am rambling
about, but here it goes anyway as talk is so cheap:

S implementations are from a 'workstation' design era. Data objects are in
Ram.  As Pat mentioned in this thread, they used to be way less efficient
than it is now. R made huge leaps. I haven

Our friendly listmembers from Insightful way want to complement me here with
factual data :)

> How do SAS and other such systems avoid that?  Do they do this better

SAS reflects its mainframe-age design, i.e. pass (efficiently) over huge
amounts of data that could never have been held in memory anyway.

The interactive/exploratory/graphical nature of S versus the
batch/non-interactive/non-graphical nature of SAS follows from relative
cleanly from that basic design premise.

> or much more more transparently than what an R user would do now
> manually?  Where by "manually", I mean, query some fits-in-RAM amount
> data out of an RDBMS (or other such on-disk store), analyze it, delete
> the data to free up RAM, and repeat.
>
> Could one say, tie a light-weight high-performance RDBMS library, like
> SQLite, into R, and have R use it profitably to scale nicely on data
> that does not fit in RAM?  In what way, if any, would this offer a
> substantial advantage over current manual R-plus-RDBMS practice?

Fei Chen, a doctoral student of Brian Ripley, gave a truly impressive
presentation at DSC 2003 about out-of-memory work with R. I bugged Brian
repeatedly about writeups on this, but apparently there are none. Fei now is
a professional data miner on truly gigantic data sets ...

It can be done, but it requires surgery on the engine.  For someone really
committed, it may be worth digging up Fei Chen's dissertation.  Might even
be a market niche for Insightful to explore. 

Dirk


-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004

From Achim.Zeileis at wu-wien.ac.at  Sat Dec  4 19:30:16 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat Dec  4 19:30:36 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
In-Reply-To: <20041204163153.GA12978@sonny.eddelbuettel.com>
References: <41B0B25B.20406@burns-stat.com>
	<20041204121540.GA25064@piskorski.com>
	<20041204163153.GA12978@sonny.eddelbuettel.com>
Message-ID: <Pine.LNX.4.58.0412041922560.14632@thorin.ci.tuwien.ac.at>

<snip>

> Fei Chen, a doctoral student of Brian Ripley, gave a truly impressive
> presentation at DSC 2003 about out-of-memory work with R. I bugged Brian
> repeatedly about writeups on this, but apparently there are none. Fei now is

For those of you who want to get an impression, there is at least a DSC
2003 Proceedings paper at
  http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Proceedings/
although I admit that it's probably not what Dirk had in mind when asking
for a writeup :-)
Z

From mail at joeconway.com  Sat Dec  4 21:39:04 2004
From: mail at joeconway.com (Joe Conway)
Date: Sat Dec  4 21:39:16 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
In-Reply-To: <20041204121540.GA25064@piskorski.com>
References: <41B0B25B.20406@burns-stat.com>
	<20041204121540.GA25064@piskorski.com>
Message-ID: <41B22068.4010807@joeconway.com>

Andrew Piskorski wrote:
> Someone might want to ask Joe Conway about his experience and thoughts
> integrating R as a procedural language inside PostgreSQL, to create
> PL/R:
> 
>   http://www.joeconway.com/plr/
>   http://gborg.postgresql.org/project/plr/projdisplay.php
> 
> (Hm, for good measure, I have Cc'd him on this email.)  Obviously, an
> RDBMS like PostgreSQL is expert at dealing with data that doesn't fit
> into RAM.  I've no idea whether PL/R does anything special to take
> advantage of that, or how feasible it would be to do so.

There is nothing particularly special in PL/R per se, but there are some 
advantages of the RDBMS-embedded R-interpreter approach of PL/R. 
Although not written about PL/R, much of Duncan Temple Lang's whitepaper 
related to usage scenarios for his REmbeddedPostgres package applies. It 
can be found here:

   http://www.omegahat.org/RSPostgres/Scenarios.pdf

Additionally, since PL/R supports direct in-process SQL queries from R, 
large data sets can be processed in groups of records (e.g. by using 
"DECLARE ... CURSOR FOR SELECT ..." and "FETCH 1000 FROM ..." in a loop) 
as long as the analysis being performed can support it.

I got the impression somewhere that the DBI packages support a kind of 
virtual data.frame interface to supported databases -- i.e. the rows are 
retrieved in groups as they are accessed, in some sort of FIFO manner. 
Is that true? Perhaps at some point that notion could be added to PL/R 
as well (no promises on timing, howver -- my day-job is keeping me 
pretty busy these days).

Joe

From mail at joeconway.com  Sat Dec  4 21:40:20 2004
From: mail at joeconway.com (Joe Conway)
Date: Sat Dec  4 21:40:23 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
In-Reply-To: <20041204163153.GA12978@sonny.eddelbuettel.com>
References: <41B0B25B.20406@burns-stat.com>
	<20041204121540.GA25064@piskorski.com>
	<20041204163153.GA12978@sonny.eddelbuettel.com>
Message-ID: <41B220B4.5080101@joeconway.com>

Dirk Eddelbuettel wrote:
> On Sat, Dec 04, 2004 at 07:15:40AM -0500, Andrew Piskorski wrote:
>>(Hm, for good measure, I have Cc'd him on this email.)  Obviously, an
> 
> Very good point, but you didn't CC Joe. Done now. Hi Joe :)

No worries, I'm already subscribed ;-)

Joe

From atp at piskorski.com  Sun Dec  5 18:31:19 2004
From: atp at piskorski.com (Andrew Piskorski)
Date: Sun Dec  5 18:31:28 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
In-Reply-To: <20041204163153.GA12978@sonny.eddelbuettel.com>
References: <41B0B25B.20406@burns-stat.com>
	<20041204121540.GA25064@piskorski.com>
	<20041204163153.GA12978@sonny.eddelbuettel.com>
Message-ID: <20041205173119.GA97509@piskorski.com>

On Sat, Dec 04, 2004 at 10:31:53AM -0600, Dirk Eddelbuettel wrote:

> It can be done, but it requires surgery on the engine.  For someone really
> committed, it may be worth digging up Fei Chen's dissertation.  Might even

Here's some more info about that, although the links to Chen's actual
thesis are broken:

https://stat.ethz.ch/pipermail/r-devel/2004-March/029204.html
https://stat.ethz.ch/pipermail/r-devel/2004-March/029211.html
http://www.stats.ox.ac.uk/people/former.htm

-- 
Andrew Piskorski <atp@piskorski.com>
http://www.piskorski.com/

From Bernhard.Pfaff at drkw.com  Mon Dec  6 08:51:45 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Mon Dec  6 08:52:00 2004
Subject: [R-sig-finance] Re: Fractional Integration: Dolado & Gonzalo &
	Mayoral (2002), A Frac tional Dickey-Fuller Test for Unit Roots, Econo
	metrica
Message-ID: <29E0BC0C716A584582941615CF9FFB0902585D8E@ibfftce107.de.ad.drkw.net>

> A limited number of functions that deal with fractional difference 
> processes (or long-memory processes) are available in the "waveslim" 

Hello Brandon,

thks for pointing me to `waveslim'; I was not aware of this package. I will
have a look at it, and see if I can utilise part of its functionalities.

Best Regards,
Bernhard

Dr. Bernhard Pfaff
Global Debt Research - Index and Quantitative Strategy
Dresdner Kleinwort Wasserstein
Phone:	+49 (0)69 713 12273 
Mobile: 	na 
Fax:	+49 (0)69 713 19816

<http://www.drkwresearch.com>
Bloomberg: DRKW<GO>



--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From badegeeter at zonnet.nl  Mon Dec  6 14:41:12 2004
From: badegeeter at zonnet.nl (badegeeter@zonnet.nl)
Date: Mon Dec  6 14:39:32 2004
Subject: [R-sig-finance] fSeries - adfTest / unitrootTest
Message-ID: <002501c4db99$621acfa0$b932a8c0@saenoptions.nl>

Hello,

I'm sorry if I'm asking a very basic question, I am not so familiar with
R..At the moment I have a script which uses ADF test from the tseries
package. Now I am trying to adjust my script to use the fSeries package.
So, I would like to use the function adfTest and/or unitrootTest.
As an input for these functions I have a vector 'x' with observations. When
trying the tests on 'x' I get the following message:

'Error in validObject(.Object) : Invalid "fURTEST" object: Invalid object
for slot "data.name" in class "fURTEST": got class "NULL", should be or
extend class "character"'

What am I doing wrong ?

greetings,

Bastiaan

From abu3ammar at gmail.com  Fri Dec 10 18:35:54 2004
From: abu3ammar at gmail.com (Omar Lakkis)
Date: Fri Dec 10 18:36:45 2004
Subject: [R-sig-finance] covariance
Message-ID: <b1d3150404121009353b876e8f@mail.gmail.com>

Is there an R function that is equivalent to S-PLUS's EWCE.cov() --
Exponentially Weighted Covariance Estimate?

From Robert.McGehee at geodecapital.com  Fri Dec 10 18:50:37 2004
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Fri Dec 10 18:51:03 2004
Subject: [R-sig-finance] covariance
Message-ID: <67DCA285A2D7754280D3B8E88EB5480208CA1149@MSGBOSCLB2WIN.DMN1.FMR.COM>

I believe the equivalent is to use the cov.wt() function, which returns
a weighted covariance matrix for any arbitrary weight vector. Just
choose your weight vector from an exponential (decay) function.

Robert

-----Original Message-----
From: Omar Lakkis [mailto:abu3ammar@gmail.com] 
Sent: Friday, December 10, 2004 12:36 PM
To: r-sig-finance@stat.math.ethz.ch
Subject: [R-sig-finance] covariance


Is there an R function that is equivalent to S-PLUS's EWCE.cov() --
Exponentially Weighted Covariance Estimate?

_______________________________________________
R-sig-finance@stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance

From abu3ammar at gmail.com  Fri Dec 10 20:43:30 2004
From: abu3ammar at gmail.com (Omar Lakkis)
Date: Fri Dec 10 20:43:33 2004
Subject: [R-sig-finance] portfolio.optim, where is the source code
Message-ID: <b1d315040412101143784159f6@mail.gmail.com>

How do I see the code (actual implemetation) of the function
portfolio.optim() in tseries?

When I type the function's name on teh command line I get:
> portfolio.optim  
function (x, ...) 
UseMethod("portfolio.optim")

I was unable to find the source in my installation. 

/usr/local/lib/R/site-library$ find tseries/ | grep op
tseries/help/portfolio.optim
tseries/html/portfolio.optim.html
tseries/latex/portfolio.optim.tex
tseries/R-ex/portfolio.optim.R

From ggrothendieck at myway.com  Fri Dec 10 22:53:39 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri Dec 10 22:53:53 2004
Subject: [R-sig-finance] portfolio.optim, where is the source code
Message-ID: <20041210215339.7EB2B39B1@mprdmxin.myway.com>


> 
From:   Omar Lakkis <abu3ammar@gmail.com>
>  
> How do I see the code (actual implemetation) of the function
> portfolio.optim() in tseries?
> 
> When I type the function's name on teh command line I get:
> > portfolio.optim 
> function (x, ...) 
> UseMethod("portfolio.optim")

The UseMethod call has the effect of calling
portfolio.optim.foo for the appropriate class foo.
To find the possible portfolio.optim.foo's issue
this R command:

	methods(portfolio.optim)

> I was unable to find the source in my installation. 
> 
> /usr/local/lib/R/site-library$ find tseries/ | grep op
> tseries/help/portfolio.optim
> tseries/html/portfolio.optim.html
> tseries/latex/portfolio.optim.tex
> tseries/R-ex/portfolio.optim.R

Assuming you downloaded the source package, and not just
the binary package then the R files are all in tseries/R and 
grepping through that we find that finance.R contains it.

From tmulholland at bigpond.com  Sun Dec 12 11:14:23 2004
From: tmulholland at bigpond.com (Tom Mulholland)
Date: Sun Dec 12 11:14:30 2004
Subject: [R-sig-finance] dates and times on Windows for fMetrics
Message-ID: <41BC19FF.2020800@bigpond.com>

First things first

R       "R version 2.0.1, 2004-11-15"
OS.type "windows"
GUI     "Rgui"

I thought that I had the time and date stuff nearly under control. I 
don't get the ubiquitous "GMT" warning although I'm not sure that the 
way I have done it is correct. I use a batch file to invoke R

set TZ=GMT
rgui.exe

I have a dataset that I use called tempdata

 > str(tempdata)
`data.frame':   300 obs. of  7 variables:
  $ date     : chr  "2003/10/09" "2003/10/10" "2003/10/13" "2003/10/14" ...
  $ Open     : num  1.27 1.25 1.27 1.29 1.27 1.28 1.32 1.35 1.35 1.34 ...
  $ High     : num  1.28 1.28 1.29 1.29 1.29 1.31 1.35 1.37 1.37 1.34 ...
  $ Low      : num  1.25 1.25 1.27 1.27 1.27 1.28 1.31 1.32 1.33 1.32 ...
  $ Close    : num  1.25 1.27 1.28 1.27 1.28 1.31 1.35 1.35 1.34 1.33 ...
  $ Volume   : int  152810 111338 243843 180211 159147 386021 270289 
690343 574630 314740 ...
  $ dateposix:`POSIXct', format: chr  "2003-10-09" "2003-10-10" 
"2003-10-13" "2003-10-14" ...

I use the POSIXct in my own home-made plots. In playing with Fmetrics I 
naturaly wanted to create a time series

This works
ts = timeSeries(tempdata[,2:6], charvec = tempdata[,1],format = 
"%Y/%m/%d",FinCenter = "Australia/Sydney")

Although if I set myFinCenter to "Australia/Perth" it fails. (See below 
for structure)

while
ts = timeSeries(tempdata[,2:6], charvec = tempdata[,1],format = 
"%Y/%m/%d",FinCenter = "Australia/Perth") fails with

Error in if (timeTest == 0) iso.format = "%Y-%m-%d" :
         missing value where TRUE/FALSE needed

Ive looked at the function but I'm missing something.

Any ideas would be much appreciated


 > str(ts)
Formal class 'timeSeries' [package "fBasics"] with 7 slots
   ..@ Data         : num [1:300, 1:5] 1.27 1.25 1.27 1.29 1.27 1.28 
1.32 1.35 1.35 1.34 ...
   .. ..- attr(*, "dimnames")=List of 2
   .. .. ..$ : chr [1:300] "2003-10-09 10:00:00" "2003-10-10 10:00:00" 
"2003-10-13 10:00:00" "2003-10-14 10:00:00" ...
   .. .. ..$ : chr [1:5] "TS.1" "TS.2" "TS.3" "TS.4" ...
   ..@ positions    : chr [1:300] "2003-10-09 10:00:00" "2003-10-10 
10:00:00" "2003-10-13 10:00:00" "2003-10-14 10:00:00" ...
   ..@ format       : chr "%Y-%m-%d %H:%M:%S"
   ..@ FinCenter    : chr "Australia/Sydney"
   ..@ units        : chr [1:5] "TS.1" "TS.2" "TS.3" "TS.4" ...
   ..@ title        : chr "Time Series Object"
   ..@ documentation: chr "Created at Australia/Sydney 2004-12-12 19:52:35"

From wuertz at itp.phys.ethz.ch  Mon Dec 13 00:23:20 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon Dec 13 00:23:24 2004
Subject: [R-sig-finance] dates and times on Windows for fMetrics
In-Reply-To: <41BC19FF.2020800@bigpond.com>
References: <41BC19FF.2020800@bigpond.com>
Message-ID: <41BCD2E8.1030109@itp.phys.ethz.ch>



# Here is the solution:

require(fBasics)

# Be sure that R is running in time zone GMT.
# Set your Windows environment variable to "GMT"
# Your PC Windows clock can still run in any other time zone!
# My clock is now running in Zurich in Europe.


Date    = c("2003-10-09", "2003-10-10", "2003-10-13", "2003-10-14")
 Open   = c(1.27, 1.25, 1.27, 1.29)
 High   = c(1.28, 1.28, 1.29, 1.29)
 Low    = c(1.25, 1.25, 1.27, 1.27)
 Close  = c(1.25, 1.27, 1.28, 1.27)
 Volume = c(152810, 111338, 243843, 180211)
Data    = data.frame(Open, High, Low, Close, Volume)


# In which time zone are your data recorded?
zone = "Australia/Sydney"
# At what local time have your data been recorded?
# Say, 16:00:00 local time "Australia/Sydney" when the exchange closes?
Time = "16:00:00"
# At which Financial Center you like to use your Data?
FinCenter = "Australia/Sydney"


# Make a timeSeries Object:
sydney.ts = timeSeries(
   data = Data,
   charvec = paste(Date, Time),
   units = c("Open", "High", "Low", "Close", "Volume"),
   zone = "Australia/Sydney",
   FinCenter = "Australia/Sydney")

sydney.ts
# You should Get:
#                     Open High  Low Close Volume
# 2003-10-09 16:00:00 1.27 1.28 1.25  1.25 152810
# 2003-10-10 16:00:00 1.25 1.28 1.25  1.27 111338
# 2003-10-13 16:00:00 1.27 1.29 1.27  1.28 243843
# 2003-10-14 16:00:00 1.29 1.29 1.27  1.27 180211


# Now, you are living at your "FinCenter" in Adelaide,
# but the data were recorded in the time "zone" of Sydney:

adelaide.ts = timeSeries(
   data = Data,
   charvec = paste(Date, Time),
   units = c("Open", "High", "Low", "Close", "Volume"),
   zone = "Australia/Sydney",
   FinCenter = "Australia/Adelaide")
adelaide.ts

# Or, you are living in Melbourne:

melbourne.ts = timeSeries(
   data = Data,
   charvec = paste(Date, Time),
   units = c("Open", "High", "Low", "Close", "Volume"),
   zone = "Australia/Sydney",
   FinCenter = "Australia/Melbourne") 
melbourne.ts


# Why does it fail for Perth?
# Have a look on the fail of the DST rules for Sydney:
tail(Sydney())
# You get:
#                  Sydney offSet
# 123 2028-03-25 16:00:00  36000
# 124 2028-10-28 16:00:00  39600
# 125 2029-03-24 16:00:00  36000
# 126 2029-10-27 16:00:00  39600
# 127 2030-03-30 16:00:00  36000
# 128 2030-10-26 16:00:00  39600

# Now for Perth:
tail(Perth())
# You get:
#                  Perth offSet
# 8  1974-10-26 18:00:00  32400
# 9  1975-03-01 18:00:00  28800
# 10 1983-10-29 18:00:00  32400
# 11 1984-03-03 18:00:00  28800
# 12 1991-11-16 18:00:00  32400
# 13 1992-02-29 18:00:00  28800

# OOPS ...
# The DST rules are missing after 1992.
# A quick repair:
# Let's assume that the DST dates are the same as for Sydney:
# and the offset 2 hours (120 Minutes) earlier:

rm(Perth)
PERTH <<- Perth
Perth = function() {
    Perth = paste(substring(as.character(Sydney()[52:128,1]), 1, 10), 
"18:00:00")
    offSet = Sydney()[52:128,2] - 2*60*60
    rbind(PERTH(), data.frame(Perth, offSet))
}

# Try the complete Perth():
Perth()


perth.ts = timeSeries(
   data = Data,
   charvec = paste(Date, Time),
   units = c("Open", "High", "Low", "Close", "Volume"),
   zone = "Australia/Sydney",
   FinCenter = "Australia/Perth") 
perth.ts

# You get:
#                     Open High  Low Close Volume
# 2003-10-09 14:00:00 1.27 1.28 1.25  1.25 152810
# 2003-10-10 14:00:00 1.25 1.28 1.25  1.27 111338
# 2003-10-13 14:00:00 1.27 1.29 1.27  1.28 243843
# 2003-10-14 14:00:00 1.29 1.29 1.27  1.27 180211

 

# Is that right, there are 2 hours difference from Perth to Sydney?



# Note there are some other FinCenters which are not up to date.
# The list will be checked and updated with the next version of Rmetrics.


# Regards
# Diethelm Wuertz




Tom Mulholland wrote:

> First things first
>
> R       "R version 2.0.1, 2004-11-15"
> OS.type "windows"
> GUI     "Rgui"
>
> I thought that I had the time and date stuff nearly under control. I 
> don't get the ubiquitous "GMT" warning although I'm not sure that the 
> way I have done it is correct. I use a batch file to invoke R
>
> set TZ=GMT
> rgui.exe
>
> I have a dataset that I use called tempdata
>
> > str(tempdata)
> `data.frame':   300 obs. of  7 variables:
>  $ date     : chr  "2003/10/09" "2003/10/10" "2003/10/13" "2003/10/14" 
> ...
>  $ Open     : num  1.27 1.25 1.27 1.29 1.27 1.28 1.32 1.35 1.35 1.34 ...
>  $ High     : num  1.28 1.28 1.29 1.29 1.29 1.31 1.35 1.37 1.37 1.34 ...
>  $ Low      : num  1.25 1.25 1.27 1.27 1.27 1.28 1.31 1.32 1.33 1.32 ...
>  $ Close    : num  1.25 1.27 1.28 1.27 1.28 1.31 1.35 1.35 1.34 1.33 ...
>  $ Volume   : int  152810 111338 243843 180211 159147 386021 270289 
> 690343 574630 314740 ...
>  $ dateposix:`POSIXct', format: chr  "2003-10-09" "2003-10-10" 
> "2003-10-13" "2003-10-14" ...
>
> I use the POSIXct in my own home-made plots. In playing with Fmetrics 
> I naturaly wanted to create a time series
>
> This works
> ts = timeSeries(tempdata[,2:6], charvec = tempdata[,1],format = 
> "%Y/%m/%d",FinCenter = "Australia/Sydney")
>
> Although if I set myFinCenter to "Australia/Perth" it fails. (See 
> below for structure)
>
> while
> ts = timeSeries(tempdata[,2:6], charvec = tempdata[,1],format = 
> "%Y/%m/%d",FinCenter = "Australia/Perth") fails with
>
> Error in if (timeTest == 0) iso.format = "%Y-%m-%d" :
>         missing value where TRUE/FALSE needed
>
> Ive looked at the function but I'm missing something.
>
> Any ideas would be much appreciated
>
>
> > str(ts)
> Formal class 'timeSeries' [package "fBasics"] with 7 slots
>   ..@ Data         : num [1:300, 1:5] 1.27 1.25 1.27 1.29 1.27 1.28 
> 1.32 1.35 1.35 1.34 ...
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : chr [1:300] "2003-10-09 10:00:00" "2003-10-10 10:00:00" 
> "2003-10-13 10:00:00" "2003-10-14 10:00:00" ...
>   .. .. ..$ : chr [1:5] "TS.1" "TS.2" "TS.3" "TS.4" ...
>   ..@ positions    : chr [1:300] "2003-10-09 10:00:00" "2003-10-10 
> 10:00:00" "2003-10-13 10:00:00" "2003-10-14 10:00:00" ...
>   ..@ format       : chr "%Y-%m-%d %H:%M:%S"
>   ..@ FinCenter    : chr "Australia/Sydney"
>   ..@ units        : chr [1:5] "TS.1" "TS.2" "TS.3" "TS.4" ...
>   ..@ title        : chr "Time Series Object"
>   ..@ documentation: chr "Created at Australia/Sydney 2004-12-12 
> 19:52:35"
>
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>

From wuertz at itp.phys.ethz.ch  Mon Dec 13 01:07:14 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon Dec 13 01:07:21 2004
Subject: [R-sig-finance] fSeries - adfTest / unitrootTest
In-Reply-To: <002501c4db99$621acfa0$b932a8c0@saenoptions.nl>
References: <002501c4db99$621acfa0$b932a8c0@saenoptions.nl>
Message-ID: <41BCDD32.8060306@itp.phys.ethz.ch>



# Here comes the solution:

require(fSeries)

# A time series which contains no unit-root:
x = rnorm(1000)
x.ts = as.ts(x) 

# A time series which contains a unit-root:
y = diffinv(x)
y.ts = diffinv(x.ts)

# Use: tsadfTest() in Rmetrics this is Adrian Trapletti's implementation:

tsadfTest(x)
# or:
tsadfTest(y)


tsadfTest(x.ts)
# or:
tsadfTest(y.ts)


# Rmetrics Functions:


# First, adfTest() considers in contrast to tsadfTest() all three
# types "nc", "c" and "ct". This is the main difference!

# Second, unitrootTest() does the same as adfTest() but uses the
# statistic as implemented by McKinnon (Response Surface Approach).

# Unfortunately, the list of the Return Value of both functions assigns
# a non-defined variable to the slot data.name:
#
# new("fURTEST", call = CALL, data = as.data.frame(x),
#      data.name = test$DNAME, test = test, title = as.character(title),
#      description = as.character(description))
#
# should read:
#
# new("fURTEST", call = CALL, data = as.data.frame(x),
#      data.name = test$data.name, test = test, title = 
as.character(title),
#      description = as.character(description))


# here are the correct versions:


adfTest =
function (x, type = c("nc", "c", "ct"), lags = 1)
{
    if (ncol(as.matrix(x)) > 1)
        stop("x is not a vector or univariate time series")
    if (any(is.na(x)))
        stop("NAs in x")
    if (lags < 0)
        stop("lags negative")
    doprint = FALSE
    CALL = match.call()
    DNAME = deparse(substitute(x))
    type = type[1]
    x.name = deparse(substitute(x))
    lags = lags + 1
    y = diff(x)
    n = length(y)
    z = embed(y, lags)
    y.diff = z[, 1]
    y.lag.1 = x[lags:n]
    tt = lags:n
    if (lags > 1) {
        y.diff.lag = z[, 2:lags]
        if (type == "nc") {
            res = lm(y.diff ~ y.lag.1 - 1 + y.diff.lag)
        }
        if (type == "c") {
            res = lm(y.diff ~ y.lag.1 + 1 + y.diff.lag)
        }
        if (type == "ct") {
            res = lm(y.diff ~ y.lag.1 + 1 + tt + y.diff.lag)
        }
    }
    else {
        if (type == "nc") {
            res = lm(y.diff ~ y.lag.1 - 1)
        }
        if (type == "c") {
            res = lm(y.diff ~ y.lag.1 + 1)
        }
        if (type == "ct") {
            res = lm(y.diff ~ y.lag.1 + 1 + tt)
        }
    }
    res.sum = summary(res)
    if (doprint)
        print(res.sum)
    if (type == "nc")
        coefNum = 1
    else coefNum = 2
    STAT = res.sum$coefficients[coefNum, 1]/res.sum$coefficients[coefNum,
        2]
    if (type == "nc")
        table = cbind(c(-2.66, -2.26, -1.95, -1.6, +0.92, +1.33,
            +1.7, +2.16), c(-2.62, -2.25, -1.95, -1.61, +0.91,
            +1.31, +1.66, +2.08), c(-2.6, -2.24, -1.95, -1.61,
            +0.9, +1.29, +1.64, +2.03), c(-2.58, -2.23, -1.95,
            -1.62, +0.89, +1.29, +1.63, +2.01), c(-2.58, -2.23,
            -1.95, -1.62, +0.89, +1.28, +1.62, +2), c(-2.58,
            -2.23, -1.95, -1.62, +0.89, +1.28, +1.62, +2))
    if (type == "c")
        table = cbind(c(-3.75, -3.33, -3, -2.63, -0.37, +0, +0.34,
            +0.72), c(-3.58, -3.22, -2.93, -2.6, -0.4, -0.03,
            +0.29, +0.66), c(-3.51, -3.17, -2.89, -2.58, -0.42,
            -0.05, +0.26, +0.63), c(-3.46, -3.14, -2.88, -2.57,
            -0.42, -0.06, +0.24, +0.62), c(-3.44, -3.13, -2.87,
            -2.57, -0.43, -0.07, +0.24, +0.61), c(-3.43, -3.12,
            -2.86, -2.57, -0.44, -0.07, +0.23, +0.6))
    if (type == "ct")
        table = cbind(c(-4.38, -3.95, -3.6, -3.24, -1.14, -0.8,
            -0.5, -0.15), c(-4.15, -3.8, -3.5, -3.18, -1.19,
            -0.87, -0.58, -0.24), c(-4.04, -3.73, -3.45, -3.15,
            -1.22, -0.9, -0.62, -0.28), c(-3.99, -3.69, -3.43,
            -3.13, -1.23, -0.92, -0.64, -0.31), c(-3.98, -3.68,
            -3.42, -3.13, -1.24, -0.93, -0.65, -0.32), c(-3.96,
            -3.66, -3.41, -3.12, -1.25, -0.94, -0.66, -0.33))
    table = t(table)
    tablen = dim(table)[2]
    tableT = c(25, 50, 100, 250, 500, 1e+05)
    tablep = c(0.01, 0.025, 0.05, 0.1, 0.9, 0.95, 0.975, 0.99)
    tableipl = numeric(tablen)
    for (i in (1:tablen)) tableipl[i] = approx(tableT, table[,
        i], n, rule = 2)$y
    PVAL = approx(tableipl, tablep, STAT, rule = 2)$y
    if (is.na(approx(tableipl, tablep, STAT, rule = 1)$y)) {
        if (PVAL == min(tablep)) {
            warning("p-value smaller than printed p-value")
        }
        else {
            warning("p-value greater than printed p-value")
        }
    }
    PARAMETER = lags - 1
    names(PARAMETER) = "Lag order"
    METHOD = "Augmented Dickey-Fuller Test"
    names(STAT) = "Dickey-Fuller"
    test = list(statistic = STAT, parameter = PARAMETER, p.value = PVAL,
        method = METHOD, data.name = DNAME)
    class(test) = c("list", "htest")
    title = test$method
    description = date()
    # BUG FIXED IMN THE FOLLOWING LINE:
    new("fURTEST", call = CALL, data = as.data.frame(x), data.name = 
test$data.name,
        test = test, title = as.character(title), description = 
as.character(description))
}



unitrootTest =
function (x, trend = c("nc", "c", "ct"), statistic = c("t", "n"),
    method = "adf", lags = 1)
{
    if (class(x) == "timeSeries")
        x = x@Data
    CALL = match.call()
    test = .unitrootADF(x = x, trend = trend[1], statistic = statistic[1],
        lags = lags)
    class(test) = c("list", "htest")
    title = test$method
    description = date()
    # BUG FIXED IMN THE FOLLOWING LINE:
    new("fURTEST", call = CALL, data = as.data.frame(x), data.name = 
test$data.name,
        test = test, title = as.character(title), description = 
as.character(description))
}


# Now try:

adfTest(x)
unitrootTest(x)

# The Bug will be fixed in the next Version of Rmetrics.


# I also highly recommend the package urca written by Bernhard Pfaff.

# Best Regards
# Diethelm Wuertz



badegeeter@zonnet.nl wrote:

>Hello,
>
>I'm sorry if I'm asking a very basic question, I am not so familiar with
>R..At the moment I have a script which uses ADF test from the tseries
>package. Now I am trying to adjust my script to use the fSeries package.
>So, I would like to use the function adfTest and/or unitrootTest.
>As an input for these functions I have a vector 'x' with observations. When
>trying the tests on 'x' I get the following message:
>
>'Error in validObject(.Object) : Invalid "fURTEST" object: Invalid object
>for slot "data.name" in class "fURTEST": got class "NULL", should be or
>extend class "character"'
>
>What am I doing wrong ?
>
>greetings,
>
>Bastiaan
>
>_______________________________________________
>R-sig-finance@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>

From wuertz at itp.phys.ethz.ch  Mon Dec 13 01:25:24 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon Dec 13 01:25:39 2004
Subject: [R-sig-finance] dates and times on Windows for fMetrics
In-Reply-To: <41BCDB05.40803@bigpond.com>
References: <41BC19FF.2020800@bigpond.com> <41BCD286.8080207@itp.phys.ethz.ch>
	<41BCDB05.40803@bigpond.com>
Message-ID: <41BCE174.7040208@itp.phys.ethz.ch>

Thanks for this information.

I will take care about DST in Australia for the next Rmetrics release.

Thanks Diethelm




Tom Mulholland wrote:

>
>
> Diethelm Wuertz wrote:
>
> ...
>
>> # OOPS ...
>> # The DST rules are missing after 1992.
>
> ...
>
> We don't have daylight saving anymore,(the running joke here is that 
> it fades the curtains too quickly) and it's been that way for at least 
> a decade. So I don't think there are any missing rules.
>
> I'm just on my way out to work so I'll fully digest the message once I 
> get home.
>
> Thanks.
>
> Tom
>

From adz at ubp.ch  Mon Dec 13 14:20:00 2004
From: adz at ubp.ch (DERUAZ Alexandre)
Date: Mon Dec 13 14:20:04 2004
Subject: [R-sig-finance] Multivariate GARCH
Message-ID: <230D850A3926DD4C84ACDEB46D81679008DE7E@srvmailgva101.corp.ubp.ch>

Hello everybody.
 
I found a thread on multivariate GARCH in archives, asking if something
was being developped.
Any news since then ?
 
Is there any code available for some bivariate GARCH model fitting ?
 
Many thanks
 
Alexandre

	[[alternative HTML version deleted]]

From patrick at burns-stat.com  Mon Dec 13 19:13:35 2004
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon Dec 13 19:18:38 2004
Subject: [R-sig-finance] Multivariate GARCH
In-Reply-To: <230D850A3926DD4C84ACDEB46D81679008DE7E@srvmailgva101.corp.ubp.ch>
References: <230D850A3926DD4C84ACDEB46D81679008DE7E@srvmailgva101.corp.ubp.ch>
Message-ID: <41BDDBCF.40307@burns-stat.com>

Below I will outline a method of getting multivariate GARCH estimates
by using only univariate GARCH estimates.  I actually did it (years ago)
not for lack of a multivariate GARCH estimator, but to get estimates for
large problems (that is, a large number of assets) in a reasonable amount
of time.  For being ad hoc, it performs remarkably well.

Here is the recipe.  Assume there are n observations (dates) for each of
the p assets.

Step 1)  Perform a univariate GARCH estimation on each asset.

Step 2)  Form the standardized residuals of all of the assets.  This is 
an n by p
matrix where each value theoretically has mean 0 and variance 1.

Step 3)  Perform a principal component rotation on the standardized 
residuals.

Step 4)  Perform a univariate GARCH estimate on each of the principal
components.

Step 5)  At each point in time we have a variance for each of the principal
components.  If we cross our fingers real hard, we can assume that there is
no correlation between the principal components at each of the times.  (On
average throughout the sample period, this is true, but it is very 
doubtful that
it is always true.)

With our assumption the variance matrix for the principal components at a
point in time is diagonal.  Rotate this diagonal matrix back into asset
co-ordinates.

Step 6)  The end result of step 5 is conceptually the correlation matrix 
of the
assets at the point in time.  In actuality the diagonals will not all be 
1.  Perform
the transformation of a variance matrix into a correlation matrix on the 
result
of step 5.  (This may or may not undo some of the damage from the assumption
of constant zero correlation of the principal components.)

Step 7)  Scale the correlation matrix created in step 6 by the variances 
estimated
in step 1 to arrive at the estimate of the variance matrix at a point in 
time.


Predictions are straightforward -- just predict the principal component 
GARCH
models, do the transformation into assets, then predict the asset GARCH 
models
and put them together.

Patrick Burns

Burns Statistics
patrick@burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

DERUAZ Alexandre wrote:

>Hello everybody.
> 
>I found a thread on multivariate GARCH in archives, asking if something
>was being developped.
>Any news since then ?
> 
>Is there any code available for some bivariate GARCH model fitting ?
> 
>Many thanks
> 
>Alexandre
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-finance@stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
>  
>

From abu3ammar at gmail.com  Mon Dec 13 19:27:06 2004
From: abu3ammar at gmail.com (Omar Lakkis)
Date: Mon Dec 13 19:27:19 2004
Subject: [R-sig-finance] timeSeries to its error
Message-ID: <b1d3150404121310275cbaec42@mail.gmail.com>

I have a timeSeries that I want to convert to its. When I use as.its()
I get the error:

Error in checkSlotAssignment(object, name, value) : 
        Assignment of an object of class "NULL" is not valid for slot
"units" in an object of class "timeSeries"; is(value, "character") is
not TRUE

When I constructed the timeSeries object I did not use the units param
in the constructor because I do not know what it does.

From ajayshah at mayin.org  Tue Dec 14 06:28:45 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue Dec 14 06:28:37 2004
Subject: [R-sig-finance] Multivariate GARCH
In-Reply-To: <230D850A3926DD4C84ACDEB46D81679008DE7E@srvmailgva101.corp.ubp.ch>
References: <230D850A3926DD4C84ACDEB46D81679008DE7E@srvmailgva101.corp.ubp.ch>
Message-ID: <20041214052845.GQ728@igidr.ac.in>

On Mon, Dec 13, 2004 at 02:20:00PM +0100, DERUAZ Alexandre wrote:
> Hello everybody.
>  
> I found a thread on multivariate GARCH in archives, asking if something
> was being developped.
> Any news since then ?
>  
> Is there any code available for some bivariate GARCH model fitting ?

At http://www.mayin.org/ajayshah/KB/R/R_for_economists.html I have a
pointer to a half-there effort. At some point I had looked at the docs
and they were great.

-- 
Ajay Shah                                                   Consultant
ajayshah@mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi

From abu3ammar at gmail.com  Tue Dec 14 19:20:29 2004
From: abu3ammar at gmail.com (Omar Lakkis)
Date: Tue Dec 14 19:20:36 2004
Subject: [R-sig-finance] EWMA.cov
Message-ID: <b1d31504041214102039dbea29@mail.gmail.com>

EWMA.cov: exponential weighted covariance estimate
Is there an R implementation for this splus fucntion?

From spratap at assembla.com  Mon Dec 20 18:49:27 2004
From: spratap at assembla.com (Seshasayanan Pratap)
Date: Mon Dec 20 18:49:50 2004
Subject: [R-sig-finance] Interest in commercial support for R, R-metrics,
	and related packages
Message-ID: <200412201749.iBKHnjP22459@assembla.apolloservers.com>

I am a partner at Assembla, a software services group that helps companies
make use of open source techniques and software. We have been asked by a
company in the financial services sector to provide support for their use of
R, Rmetrics and possibly SciViews. 

I am trying to locate other companies in the financial services industry
that might be interested in commercial support for R, Rmetrics and/or other
R packages used for financial analysis. I would like to understand what type
of services - such as technical support, custom development, versions tuned
for financial analysis, user conferences/workshops, etc - would be
interesting to users of R. 

Thank you. 

Sesha Pratap 

 


	[[alternative HTML version deleted]]

From Achim.Zeileis at wu-wien.ac.at  Thu Dec 23 14:33:38 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu Dec 23 14:33:43 2004
Subject: [R-sig-finance] More info: zoo 0.9-1
Message-ID: <Pine.LNX.4.58.0412231339200.23821@thorin.ci.tuwien.ac.at>

Dear finance useRs,

you will probably have seen my post to R-packages that a new and much
improved version of the zoo package for indexed totally ordered
observations (such as irregular time series) is available from CRAN.

As there has been some discussion on this list about the topic of
irregular time series, I would like to add a few comments:
The focus of zoo is 1) to extend standard generic functions and 2) be
independent of a particular class for the time/index information. For
example, it allows the usage of the "Date" class which might be somewhat
more convenient for daily observations than "POSIXct".

But the purpose of zoo is not to replace other classes like "its",
"timeSeries" or "irts": zoo allows for conversion between these classes
and also aims to be a basic building block when constructing more
specialized solutions for irregular time series.

Another new feature of zoo which fills a gap in R is that it allows for
regression based on zoo series. In particular, it is also possible to fit
dynamic linear regression models (where functions like diff() or lag() are
used in the formula specification). Gabor and I have been struggling a bit
with the design of this feature and the current solution is probably not
the best one possible. Therefore, we would be happy about feedback and
suggestions about this feature in particular but also the package in
general.
So if you don't know yet how to spend the long winter nights between Xmas
and New Year's Eve, you might want to enjoy yourselves by trying out the
new zoo package ;-))

In this spirit: seasonal greatings,
Z

From edd at debian.org  Thu Dec 23 15:58:32 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu Dec 23 15:58:46 2004
Subject: [R-sig-finance] its and TZ
Message-ID: <20041223145832.GA5178@sonny.eddelbuettel.com>

My thanks to Whit for adopting its and releasing the 1.0.4 release.  After
upgrading, I noticed that I now seem to be falling into the TZ pit: intraday
data read from a csv file "shows" the right time stamps when displayed.

However, when plotted, the axis is shifted back to UTC which is now imposed
in a few places in the code.  I tried to overcome this by explicitly setting
a tz argument in readcsvIts, but no luck.

Anybody else bitten by this or similar intraday code with its 1.0.4?

Dirk

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004

From ggrothendieck at myway.com  Thu Dec 23 20:01:42 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu Dec 23 20:02:04 2004
Subject: [R-sig-finance] its and TZ
Message-ID: <20041223190142.6E6EB395E@mprdmxin.myway.com>



Dirk Eddelbuettel <edd@debian.org>
 
> My thanks to Whit for adopting its and releasing the 1.0.4 release. After
> upgrading, I noticed that I now seem to be falling into the TZ pit: intraday
> data read from a csv file "shows" the right time stamps when displayed.
> 
> However, when plotted, the axis is shifted back to UTC which is now imposed
> in a few places in the code. I tried to overcome this by explicitly setting
> a tz argument in readcsvIts, but no luck.
> 
> Anybody else bitten by this or similar intraday code with its 1.0.4?

One workaround, depending on your situation, might be to set 
your computer or process to GMT.

From edd at debian.org  Thu Dec 23 20:17:36 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu Dec 23 20:17:43 2004
Subject: [R-sig-finance] its and TZ
In-Reply-To: <20041223190142.6E6EB395E@mprdmxin.myway.com>
References: <20041223190142.6E6EB395E@mprdmxin.myway.com>
Message-ID: <20041223191736.GA6856@sonny.eddelbuettel.com>

On Thu, Dec 23, 2004 at 02:01:42PM -0500, Gabor Grothendieck wrote:
> 
> 
> Dirk Eddelbuettel <edd@debian.org>
>  
> > My thanks to Whit for adopting its and releasing the 1.0.4 release. After
> > upgrading, I noticed that I now seem to be falling into the TZ pit: intraday
> > data read from a csv file "shows" the right time stamps when displayed.
> > 
> > However, when plotted, the axis is shifted back to UTC which is now imposed
> > in a few places in the code. I tried to overcome this by explicitly setting
> > a tz argument in readcsvIts, but no luck.
> > 
> > Anybody else bitten by this or similar intraday code with its 1.0.4?
> 
> One workaround, depending on your situation, might be to set 
> your computer or process to GMT.

Confirmed -- adding

  TZ="GMT"
  
in ~/.Renviron does the trick. I'll keep an eye out to see if this has other
side effects.

Dirk

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004

From ggrothendieck at myway.com  Thu Dec 23 20:31:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu Dec 23 20:31:15 2004
Subject: [R-sig-finance] More info: zoo 0.9-1
Message-ID: <20041223193102.837A5398A@mprdmxin.myway.com>



> Another new feature of zoo which fills a gap in R is that it allows for
> regression based on zoo series. In particular, it is also possible to fit
> dynamic linear regression models (where functions like diff() or lag() are
> used in the formula specification). Gabor and I have been struggling a bit
> with the design of this feature and the current solution is probably not
> the best one possible. Therefore, we would be happy about feedback and
> suggestions about this feature in particular but also the package in
> general.

I just wanted to add some color to this.  Three versions of
the dynamic regression code were developed.  

1. The first involved modifying the R lm function to accept zoo
series based on some code of Whit Armstrong.  (Whit 
is the maintainer of the `its' package.)  This was never 
released. 

2. The second version (which is in the current zoo release) is
completely different and involves no modification to 
any standard R code.  Unlike the first version which was specific 
to lm, this second version is completely general and applies not
only to lm but any regression or other formula-based
function that is written in the same way as lm (e.g. glm,
xtabs, rq from quantreg, svm from e1071, etc.).  This second
version works not only with zoo but also with ts.  The
main disadvantage is that one must surround the
formula argument with an I to tell it to intercept the
formula, e.g.  lm( I( diff(y) ~ lag(x1)+x2 ) ).   

2a. A variant of this second version is to provide the second
version together with convenience wrappers so that
dynlm(...whatever...) might be used in place of
lm(I(...whatevfer...)).  This eliminates the I though it
introduces a plethora of new function names while the rest
of zoo introduces very few new function names since zoo
mostly extends base generics and therefore just extends the
use of existing names to the zoo context.

3. The third version (which we have running but at this time
has not been released) is equally general to the second version 
and has the added advantage that the I() is not required.
The disadvantage of version three is that it requires 
replacing one of the standard R functions with an upwardly
compatible version that also handles this context.

From cj5815 at yahoo.com  Tue Dec 28 15:36:56 2004
From: cj5815 at yahoo.com (Joe Cerniglia)
Date: Tue Dec 28 19:10:12 2004
Subject: [R-sig-finance] bid-ask bouse
Message-ID: <20041228143656.9946.qmail@web50207.mail.yahoo.com>


I am trying to test a strategy on small cap stocks in
R.  I am concerned that the bid-ask bounce is
contributed to the excess return generated by the
strategy.  How can I adjust the calculation of the
returns on my portfolio to account for the bid-ask
spread?

Joe

From dave at kanecap.com  Tue Dec 28 20:34:03 2004
From: dave at kanecap.com (David Kane)
Date: Tue Dec 28 20:33:57 2004
Subject: [R-sig-finance] bid-ask bouse
In-Reply-To: <20041228143656.9946.qmail@web50207.mail.yahoo.com>
References: <20041228143656.9946.qmail@web50207.mail.yahoo.com>
Message-ID: <16849.46379.161253.806415@gargle.gargle.HOWL>

I am not sure if this is the appropriate list for your question, but,
since I don't know a better list, here are my thoughts.

1) In order to provide a decent answer, we need to know *much* more
about the strategy. Start by telling us the typical holding period,
the universe of stocks included and the dates for the historical data.

2) Replace "return" in your code with some measure of return adjusted
for bid/ask spread. If you are using daily returns and a daily holding
period (which probably wouldn't be such a great idea, but ignore that
for now), you could just subtract the spread from the return for
puchases while adding it to returns for shorts. Again, that has
nothing to do with R, but seems reasonable enough.

3) More sophisticated answers would involve measuring returns from
offer to bid for buys and bid to offer for shorts. Again, it would be
helpful to know precisely what sort of data you are working with.

Best of luck. Although your question has nothing to do with R, you are
wise to be using R for applied finance. There is no better tool.

Dave Kane


Joe Cerniglia writes:
 > 
 > I am trying to test a strategy on small cap stocks in
 > R.  I am concerned that the bid-ask bounce is
 > contributed to the excess return generated by the
 > strategy.  How can I adjust the calculation of the
 > returns on my portfolio to account for the bid-ask
 > spread?
 > 
 > Joe
 > 
 > _______________________________________________
 > R-sig-finance@stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-sig-finance

From arshia22 at yahoo.com  Tue Dec 28 22:19:08 2004
From: arshia22 at yahoo.com (ebashi)
Date: Tue Dec 28 22:19:21 2004
Subject: [R-sig-finance] R&PHP
Message-ID: <20041228211908.38961.qmail@web81001.mail.yahoo.com>

Dear friends;

Does anyone have any idea how to connect PHP & R and
using Mysql as DB.

Sincerely,
Sean

From Jordi.Molins at drkw.com  Wed Dec 29 13:08:57 2004
From: Jordi.Molins at drkw.com (Molins, Jordi)
Date: Wed Dec 29 13:09:06 2004
Subject: [R-sig-finance] Re: bid-ask bouse
Message-ID: <AA0BBC8742AFFF4583B91782E958CB660FD2BF@ibfftce121.de.ad.drkw.net>

Like David Kane, I do not know if this is the place to discuss these issues
(although I would find interesting to discuss about them here; why don?t we
do it? R is a wonderful package for finance, and for sure lots of us have
done very sofisticated things that could be helpful for everybody - if there
are no license issues). 

But here is my thinking: probably it is extremely trivial, but why not using
bid prices when your strategy tells you to sell, and using ask prices when
your strategy tells you to buy?

For example, if when your strategy tells you to buy (t=0) the prices are
99-101, and in t=1 your strategy tells to to sell (with prices 100-102),
your P&L is:

(100-101) = -1

Instead, if you used mid prices (100 and 101 resp), your profit would be
(101-100) = +1

So, in this way you take into account the bid ask spread.

But maybe you were referring to something more complex than that.

Jordi



--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}

From a.trapletti at bluewin.ch  Wed Dec 29 17:16:05 2004
From: a.trapletti at bluewin.ch (Adrian Trapletti)
Date: Wed Dec 29 17:16:13 2004
Subject: [R-sig-finance] bid-ask bouse
In-Reply-To: <200412291140.iBTBcnek014626@hypatia.math.ethz.ch>
References: <200412291140.iBTBcnek014626@hypatia.math.ethz.ch>
Message-ID: <41D2D845.9070705@bluewin.ch>

An HTML attachment was scrubbed...
URL: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20041229/fe909b09/attachment.html
From Adam.Klein at aqrcapital.com  Wed Dec 29 19:14:16 2004
From: Adam.Klein at aqrcapital.com (Adam Klein)
Date: Wed Dec 29 19:39:47 2004
Subject: [R-sig-finance] fBasics / timeSeries() - memory issue?
Message-ID: <057B58A540EA34419BEC867B25D9052A395D90@GRN-GEN-EXCH-01.aqrcapital.com>

Hi all, 

I'm new to R and using R v2.0.0 and fBasics 200.10058, and trying to construct a timeSeries from a data frame (it's a currency total return index).  I'm wondering if there are memory-related issues because I can only load 7567 dates into memory across 51 countries; but when I try to load 7568 periods I get an error:

> len
[1] 7567
> myTS <- timeSeries(recs[1:len,2:51], recs[1:len,1], units=names(recs)[2:51])		# THIS WORKS
> len<-len+1
> len
[1] 7568
> myTS <- timeSeries(recs[1:len,2:51], recs[1:len,1], units=names(recs)[2:51])		# THIS DOESN'T WORK
Error in if (timeTest == 0) iso.format = "%Y-%m-%d" : 
        missing value where TRUE/FALSE needed

Does anyone think this might be a memory issue?  How could I check?

The recs data frame looks like this ...

> str(recs)
`data.frame':   8867 obs. of  51 variables:
 $ TIME: chr  "1970-12-31 16:00:00" "1971-01-01 16:00:00" "1971-01-04 16:00:00" "1971-01-05 16:00:00" ...
 $ AR  : num  NA NA NA NA NA NA NA NA NA NA ...
 $ AU  : num  NA NA NA NA NA NA NA NA NA NA ...
 $ BD  : num  NA NA NA NA NA NA NA NA NA NA ...
 $ BG  : num  NA NA NA NA NA NA NA NA NA NA ...
 $ BR  : num  NA NA NA NA NA NA NA NA NA NA ...
	...

Thanks!

--Adam
 
 
 
 
 
Disclaimer: This e-mail may contain confidential and/or privileged information.  If you are not the intended recipient or have received this e-mail in error, please notify the sender immediately and destroy/delete this e-mail.  You are hereby notified that any unauthorized copying, disclosure or distribution of the material in this e-mail is strictly prohibited.
 
This communication is for informational purposes only.  It is not intended as an offer or solicitation for the purchase or sale of any financial instrument or as an official confirmation of any transaction.  All information contained in this communication is not warranted as to completeness or accuracy and is subject to change without notice.  Any comments or statements made in this communication do not necessarily reflect those of AQR Capital Management, LLC and its affiliates.

From edd at debian.org  Wed Dec 29 19:53:28 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed Dec 29 19:53:42 2004
Subject: [R-sig-finance] fBasics / timeSeries() - memory issue?
In-Reply-To: <057B58A540EA34419BEC867B25D9052A395D90@GRN-GEN-EXCH-01.aqrcapital.com>
References: <057B58A540EA34419BEC867B25D9052A395D90@GRN-GEN-EXCH-01.aqrcapital.com>
Message-ID: <20041229185328.GA15656@sonny.eddelbuettel.com>

On Wed, Dec 29, 2004 at 01:14:16PM -0500, Adam Klein wrote:
> Hi all, 
> 
> I'm new to R and using R v2.0.0 and fBasics 200.10058, and trying to construct a timeSeries from a data frame (it's a currency total return index).  I'm wondering if there are memory-related issues because I can only load 7567 dates into memory across 51 countries; but when I try to load 7568 periods I get an error:
> 
> > len
> [1] 7567
> > myTS <- timeSeries(recs[1:len,2:51], recs[1:len,1], units=names(recs)[2:51])		# THIS WORKS
> > len<-len+1
> > len
> [1] 7568
> > myTS <- timeSeries(recs[1:len,2:51], recs[1:len,1], units=names(recs)[2:51])		# THIS DOESN'T WORK
> Error in if (timeTest == 0) iso.format = "%Y-%m-%d" : 
>         missing value where TRUE/FALSE needed
> 
> Does anyone think this might be a memory issue?  How could I check?

In my experience this is almost always due to an irregular data field. I'd
check that line in the input, i.e.

recs[7568,1:51]

as I suspect that recs[7568,1] may simply have a format different from
%Y-%m-%d.

Hth, Dirk

 
> The recs data frame looks like this ...
> 
> > str(recs)
> `data.frame':   8867 obs. of  51 variables:
>  $ TIME: chr  "1970-12-31 16:00:00" "1971-01-01 16:00:00" "1971-01-04 16:00:00" "1971-01-05 16:00:00" ...
>  $ AR  : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ AU  : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ BD  : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ BG  : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ BR  : num  NA NA NA NA NA NA NA NA NA NA ...
> 	...
> 
> Thanks!
> 
> --Adam
>  
>  
>  
>  
>  
> Disclaimer: This e-mail may contain confidential and/or privileged information.  If you are not the intended recipient or have received this e-mail in error, please notify the sender immediately and destroy/delete this e-mail.  You are hereby notified that any unauthorized copying, disclosure or distribution of the material in this e-mail is strictly prohibited.
>  
> This communication is for informational purposes only.  It is not intended as an offer or solicitation for the purchase or sale of any financial instrument or as an official confirmation of any transaction.  All information contained in this communication is not warranted as to completeness or accuracy and is subject to change without notice.  Any comments or statements made in this communication do not necessarily reflect those of AQR Capital Management, LLC and its affiliates.
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004

From Adam.Klein at aqrcapital.com  Wed Dec 29 19:58:33 2004
From: Adam.Klein at aqrcapital.com (Adam Klein)
Date: Wed Dec 29 19:58:37 2004
Subject: [R-sig-finance] fBasics / timeSeries() - memory issue?
Message-ID: <057B58A540EA34419BEC867B25D9052A348E23@GRN-GEN-EXCH-01.aqrcapital.com>

You are right: the date for row 7568 is NA-NA-NA due to a bug in my date format transformation function ...

thanks!

-----Original Message-----
From: Dirk Eddelbuettel [mailto:edd@debian.org]
Sent: Wednesday, December 29, 2004 1:53 PM
To: Adam Klein
Cc: r-sig-finance@stat.math.ethz.ch
Subject: Re: [R-sig-finance] fBasics / timeSeries() - memory issue?


On Wed, Dec 29, 2004 at 01:14:16PM -0500, Adam Klein wrote:
> Hi all, 
> 
> I'm new to R and using R v2.0.0 and fBasics 200.10058, and trying to construct a timeSeries from a data frame (it's a currency total return index).  I'm wondering if there are memory-related issues because I can only load 7567 dates into memory across 51 countries; but when I try to load 7568 periods I get an error:
> 
> > len
> [1] 7567
> > myTS <- timeSeries(recs[1:len,2:51], recs[1:len,1], units=names(recs)[2:51])		# THIS WORKS
> > len<-len+1
> > len
> [1] 7568
> > myTS <- timeSeries(recs[1:len,2:51], recs[1:len,1], units=names(recs)[2:51])		# THIS DOESN'T WORK
> Error in if (timeTest == 0) iso.format = "%Y-%m-%d" : 
>         missing value where TRUE/FALSE needed
> 
> Does anyone think this might be a memory issue?  How could I check?

In my experience this is almost always due to an irregular data field. I'd
check that line in the input, i.e.

recs[7568,1:51]

as I suspect that recs[7568,1] may simply have a format different from
%Y-%m-%d.

Hth, Dirk

 
> The recs data frame looks like this ...
> 
> > str(recs)
> `data.frame':   8867 obs. of  51 variables:
>  $ TIME: chr  "1970-12-31 16:00:00" "1971-01-01 16:00:00" "1971-01-04 16:00:00" "1971-01-05 16:00:00" ...
>  $ AR  : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ AU  : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ BD  : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ BG  : num  NA NA NA NA NA NA NA NA NA NA ...
>  $ BR  : num  NA NA NA NA NA NA NA NA NA NA ...
> 	...
> 
> Thanks!
> 
> --Adam
>  
>  
>  
>  
>  
> Disclaimer: This e-mail may contain confidential and/or privileged information.  If you are not the intended recipient or have received this e-mail in error, please notify the sender immediately and destroy/delete this e-mail.  You are hereby notified that any unauthorized copying, disclosure or distribution of the material in this e-mail is strictly prohibited.
>  
> This communication is for informational purposes only.  It is not intended as an offer or solicitation for the purchase or sale of any financial instrument or as an official confirmation of any transaction.  All information contained in this communication is not warranted as to completeness or accuracy and is subject to change without notice.  Any comments or statements made in this communication do not necessarily reflect those of AQR Capital Management, LLC and its affiliates.
> 
> _______________________________________________
> R-sig-finance@stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004

From dave at kanecap.com  Wed Dec 29 20:51:33 2004
From: dave at kanecap.com (David Kane)
Date: Wed Dec 29 20:51:34 2004
Subject: [R-sig-finance] R vs. S-PLUS vs. SAS
In-Reply-To: <20041202190202.2978.qmail@web52301.mail.yahoo.com>
References: <20041202190202.2978.qmail@web52301.mail.yahoo.com>
Message-ID: <16851.2757.357056.804704@gargle.gargle.HOWL>

Apologies for taking so long to answer your question. I did equity
modeling with SAS from 1997 to 2001; indeed, for a brief shining
moment, my colleagues and I could plausibly claim to be the best SAS
programmers in Boston. Those were the days! I have been working with R
ever since. To my mind, there is no plausible reason for using SAS
instead of R for this task (other than concern with transition costs).

This is not to say that some smart people don't continue to use SAS
instead of R. Indeed, many folks at places like Numeric Investors, AQR
and BGI use SAS for equity modelling today. When it is reasonable,
this decision is based on concerns about the cost of converting
hundreds of pages of legacy code and/or the (lack of) desire among the
senior folks for learning new tools. When it is unreasonable, it is
based on a lack of understanding as to why SAS is such an unsuitable
tool for serious people.

Here are some highlights as to why you should use R instead of SAS
(and/or most other options) for equity modelling.

1) The memory issue is overwhelming a red herring, as many other
commentators have pointed out. This is especially true in equity
modelling where, unless you are working with daily data, memory
concerns should be the least of your worries. And, even in the case of
daily data, you would be much better off investing some time in
learning a relational database like PostgreSQL then in becoming
skilled at arcane SAS commands.

2) R encourages the good software development practices, especially
the use of test cases and documentation. There is no SAS equivalent to
R CMD check. This is extremely important. Quantitative finance is
largely an exercise in software development so ensuring that your code
does what you think it does, both today and in the future, is job 1.

3) R's graphics are superb. SAS's are pathetic.

4) Via Sweave, xtable and friends, R makes it very easy to practice
literate programming. 

5) R's tools for statistical analysis are superior to SAS's.

I have never heard anyone (knowledgable or otherwise) claim that, in
the absence of transition costs, SAS is better than R for equity
modeling. If you come across any such claim, I would be happy to
refute it.

I hope that this is helpful.

Dave


Hoon Kim writes:
 > Dear David
 > 
 > I appreciate sharing your experience using R in
 > finance.  I am also doing quant equity research and my
 > main tool is SAS.  I do not have much experience with
 > R at this stage and I am curious what is your opinion
 > on R vs. SAS.  My perception is SAS is much better in
 > handling large data sets, which is usually the case in
 > quant equity modeling.  Historical backtesting data
 > can be easily over several hundread megabites.  In my
 > opinion, other than the capability of handling large
 > data set in SAS, I think R is a more flexible
 > programming language.
 > 
 > Could you kindly share your thoughts with me on this
 > issue?  Do you have any problem in handling large data
 > sets in R?  How do you deal with large data set in R? 
 > Do you have recommendation on handling those large
 > data in R?
 > 
 > Thanks again for sharing your thoughts.
 > 
 > Best,
 > 
 > Hoon Kim

From adrian.trapletti at swiss-systematic.com  Thu Dec 30 18:53:14 2004
From: adrian.trapletti at swiss-systematic.com (Adrian Trapletti)
Date: Thu Dec 30 19:21:28 2004
Subject: [Fwd: Re: Re: [R-sig-finance] bid-ask bouse]
Message-ID: <41D4408A.3020605@swiss-systematic.com>

This time not as html
Adrian

-------- Original Message --------
Subject: 	Re: Re: [R-sig-finance] bid-ask bouse
Date: 	Wed, 29 Dec 2004 17:16:05 +0100
From: 	Adrian Trapletti <a.trapletti@bluewin.ch>
To: 	Joe Cerniglia <cj5815@yahoo.com>
CC: 	r-sig-finance@stat.math.ethz.ch
References: 	<200412291140.iBTBcnek014626@hypatia.math.ethz.ch>




>Message: 2
>Date: Tue, 28 Dec 2004 14:34:03 -0500
>From: David Kane <dave@kanecap.com>
>Subject: Re: [R-sig-finance] bid-ask bouse
>To: Joe Cerniglia <cj5815@yahoo.com>
>Cc: r-sig-finance@stat.math.ethz.ch
>Message-ID: <16849.46379.161253.806415@gargle.gargle.HOWL>
>Content-Type: text/plain; charset=us-ascii
>
>I am not sure if this is the appropriate list for your question, but,
>since I don't know a better list, here are my thoughts.
>
>1) In order to provide a decent answer, we need to know *much* more
>about the strategy. Start by telling us the typical holding period,
>the universe of stocks included and the dates for the historical data.
>
>2) Replace "return" in your code with some measure of return adjusted
>for bid/ask spread. If you are using daily returns and a daily holding
>period (which probably wouldn't be such a great idea, but ignore that
>for now), you could just subtract the spread from the return for
>puchases while adding it to returns for shorts. Again, that has
>nothing to do with R, but seems reasonable enough.
>
>3) More sophisticated answers would involve measuring returns from
>offer to bid for buys and bid to offer for shorts. Again, it would be
>helpful to know precisely what sort of data you are working with.
>
>Best of luck. Although your question has nothing to do with R, you are
>wise to be using R for applied finance. There is no better tool.
>
>Dave Kane
>
>  
>
Some more comments based on my experience:

    * Accurate estimation of fill prices from observed prices is maybe
      the most difficult task when simulating trading systems.
    * Slippage (>= 0) is a function of market momentum, number of
      shares, time of the day, market depth, and more specific
      properties such as liquidity of the considered instrument.
    * Some of the above measures may not be observed, in particular no
      history is available, e.g., for market depth.
    * Fill price = signal price +/- slippage (+ for buy orders, - for
      sell orders)
    * Signal price = bid/ask price (bid for sell orders, ask for buy
      orders) at the time an order is generated by the system or trader
    * To get an accurate estimate of the slippage function, a history of
      real trades is necessary.
    * In cases no bid/ask history is available, the slippage function
      may be based on last traded prices.
    * Published bid/ask prices may only be advertising quotes and nobody
      might be willing to trade on those prices.
    * Some markets close trading of shares when limit down is reached.

For some instruments it is possible to come up with a relatively simple 
and accurate slippage function, but this really depends...

Best
Adrian

>Joe Cerniglia writes:
> > 
> > I am trying to test a strategy on small cap stocks in
> > R.  I am concerned that the bid-ask bounce is
> > contributed to the excess return generated by the
> > strategy.  How can I adjust the calculation of the
> > returns on my portfolio to account for the bid-ask
> > spread?
> > 
> > Joe
> > 
> > _______________________________________________
> > R-sig-finance@stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
>
>  
>



-- 
Dr. Adrian Trapletti
Swiss-Systematic Asset Management AG
Milit?rstrasse 76
8004 Z?rich
Switzerland

Phone :   +41 (0) 43 2433904
Fax :     +41 (0) 43 2433907
Mobile :  +41 (0) 76 3705631

Email :   adrian.trapletti@swiss-systematic.com
WWW :     www.swiss-systematic.com


This e-mail message (including any attachments) is confidential and may be privileged. It is intended solely for the use of the individual or entity named on this message. It is provided for informational purposes only and does not constitute an offer or invitation to subscribe for or purchase any services or products. Any form of disclosure, copying, modification or distribution is unauthorised. If you are not the intended recipient, you are requested to please notify the sender immediately and delete the message (including any attachments) from your computer system network. 

E-mail transmission cannot be guaranteed to be secure or error-free as information could be modified. We therefore do not accept responsibility or liability as to the completeness or accuracy of the information contained in this message or any attachments.

From phleum at chello.se  Fri Dec 31 11:12:09 2004
From: phleum at chello.se (Carl)
Date: Fri Dec 31 11:11:25 2004
Subject: [R-sig-finance] Rmetrics =?windows-1252?q?=96_xmpEBMotion?=
Message-ID: <41D525F9.9040707@chello.se>

Dear friends,

Where can I find the xmpEBMotion, xmpGARCH, and xmpMoFiTS Rmetrics 
example collections?

What is the status of fBonds and fPortfolio packages?

Carl

