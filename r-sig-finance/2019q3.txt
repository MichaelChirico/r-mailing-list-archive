From jte||er|@@rproject @end|ng |rom gm@||@com  Wed Jul 10 18:56:45 2019
From: jte||er|@@rproject @end|ng |rom gm@||@com (Juan Telleria Ruiz de Aguirre)
Date: Wed, 10 Jul 2019 18:56:45 +0200
Subject: [R-SIG-Finance] R for Finance: Resources (Books, Papers)
Message-ID: <CAJXDcw07OKLcckgYyedNeAgKoH5--6dBVOSe3_CeJMzgTAmpwA@mail.gmail.com>

Dear R Community,

As regards R for Financial applications:

?Could anyone recommend some good resources for getting started in such
field? Books, Papers, etc.

I have identified the following R Resources:

https://www.crcpress.com/R-Programming-and-Its-Applications-in-Financial-Mathematics/Ohsaki-Ruppert-Felsot-Yoshikawa/p/book/9781498766098

https://www.springer.com/gp/book/9789462390690

https://www.crcpress.com/Reproducible-Finance-with-R-Code-Flows-and-Shiny-Apps-for-Portfolio-Analysis/Jr/p/book/9781138484030

https://www.rpackages.io/view/Finance

The objective is to be able to identify, from a Data Science Viewpoint,
what are we able to provide to corporate financial stakeholders.

Thank you.

Kind regards,
Juan Telleria

	[[alternative HTML version deleted]]


From hpr@mo@4 @end|ng |rom gm@||@com  Wed Jul 10 19:05:47 2019
From: hpr@mo@4 @end|ng |rom gm@||@com (Henrique Ramos)
Date: Wed, 10 Jul 2019 14:05:47 -0300
Subject: [R-SIG-Finance] R for Finance: Resources (Books, Papers)
In-Reply-To: <CAJXDcw07OKLcckgYyedNeAgKoH5--6dBVOSe3_CeJMzgTAmpwA@mail.gmail.com>
References: <CAJXDcw07OKLcckgYyedNeAgKoH5--6dBVOSe3_CeJMzgTAmpwA@mail.gmail.com>
Message-ID: <CABHS13eg_zbiUniNnMcnx=HgDJO=NdF51MVtSLeEGaoTxjVz9w@mail.gmail.com>

Processing and Analyzing Financial Data with R

https://www.amazon.com/dp/B071DTSCPS (kindle)

https://www.msperlin.com/pafdR/index.html (online)

Em qua, 10 de jul de 2019 ?s 13:57, Juan Telleria Ruiz de Aguirre <
jtelleria.rproject at gmail.com> escreveu:

> Dear R Community,
>
> As regards R for Financial applications:
>
> ?Could anyone recommend some good resources for getting started in such
> field? Books, Papers, etc.
>
> I have identified the following R Resources:
>
>
> https://www.crcpress.com/R-Programming-and-Its-Applications-in-Financial-Mathematics/Ohsaki-Ruppert-Felsot-Yoshikawa/p/book/9781498766098
>
> https://www.springer.com/gp/book/9789462390690
>
>
> https://www.crcpress.com/Reproducible-Finance-with-R-Code-Flows-and-Shiny-Apps-for-Portfolio-Analysis/Jr/p/book/9781138484030
>
> https://www.rpackages.io/view/Finance
>
> The objective is to be able to identify, from a Data Science Viewpoint,
> what are we able to provide to corporate financial stakeholders.
>
> Thank you.
>
> Kind regards,
> Juan Telleria
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. If you want to post, subscribe first.
> -- Also note that this is not the r-help list where general R questions
> should go.
>


-- 
--
Henrique P. Ramos

	[[alternative HTML version deleted]]


From @tobi@skr@mer m@iii@g oii posteo@@et  Wed Jul 10 22:26:16 2019
From: @tobi@skr@mer m@iii@g oii posteo@@et (@tobi@skr@mer m@iii@g oii posteo@@et)
Date: Wed, 10 Jul 2019 22:26:16 +0200
Subject: [R-SIG-Finance] R for Finance: Resources (Books, Papers)
In-Reply-To: <CABHS13eg_zbiUniNnMcnx=HgDJO=NdF51MVtSLeEGaoTxjVz9w@mail.gmail.com>
References: <CAJXDcw07OKLcckgYyedNeAgKoH5--6dBVOSe3_CeJMzgTAmpwA@mail.gmail.com>
 <CABHS13eg_zbiUniNnMcnx=HgDJO=NdF51MVtSLeEGaoTxjVz9w@mail.gmail.com>
Message-ID: <0A546DCF-48FB-4257-9AD0-2888EE3F7528@posteo.net>

You could also check out books and resources mentioned in these threads:
- [R-SIG-Finance] Books on R & Finance from 2017-10-25
- [R-SIG-Finance] R finance resources to start learning from 2017-10-07

https://www.r-bloggers.com/how-to-search-the-r-sig-finance-archives/

Toby

> On 10. Jul 2019, at 19:05, Henrique Ramos <hpramos4 at gmail.com> wrote:
> 
> Processing and Analyzing Financial Data with R
> 
> https://www.amazon.com/dp/B071DTSCPS (kindle)
> 
> https://www.msperlin.com/pafdR/index.html (online)
> 
> Em qua, 10 de jul de 2019 ?s 13:57, Juan Telleria Ruiz de Aguirre <
> jtelleria.rproject at gmail.com> escreveu:
> 
>> Dear R Community,
>> 
>> As regards R for Financial applications:
>> 
>> ?Could anyone recommend some good resources for getting started in such
>> field? Books, Papers, etc.
>> 
>> I have identified the following R Resources:
>> 
>> 
>> https://www.crcpress.com/R-Programming-and-Its-Applications-in-Financial-Mathematics/Ohsaki-Ruppert-Felsot-Yoshikawa/p/book/9781498766098
>> 
>> https://www.springer.com/gp/book/9789462390690
>> 
>> 
>> https://www.crcpress.com/Reproducible-Finance-with-R-Code-Flows-and-Shiny-Apps-for-Portfolio-Analysis/Jr/p/book/9781138484030
>> 
>> https://www.rpackages.io/view/Finance
>> 
>> The objective is to be able to identify, from a Data Science Viewpoint,
>> what are we able to provide to corporate financial stakeholders.
>> 
>> Thank you.
>> 
>> Kind regards,
>> Juan Telleria
>> 
>>        [[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-SIG-Finance at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only. If you want to post, subscribe first.
>> -- Also note that this is not the r-help list where general R questions
>> should go.
> 
> 
> -- 
> --
> Henrique P. Ramos
> 
>    [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. If you want to post, subscribe first.
> -- Also note that this is not the r-help list where general R questions should go.

	[[alternative HTML version deleted]]


From c|1 @end|ng |rom |n|om@n|@k@ch  Wed Jul 10 22:36:01 2019
From: c|1 @end|ng |rom |n|om@n|@k@ch (Christian Lear)
Date: Wed, 10 Jul 2019 22:36:01 +0200
Subject: [R-SIG-Finance] R for Finance: Resources (Books, Papers)
In-Reply-To: <CAJXDcw07OKLcckgYyedNeAgKoH5--6dBVOSe3_CeJMzgTAmpwA@mail.gmail.com>
References: <CAJXDcw07OKLcckgYyedNeAgKoH5--6dBVOSe3_CeJMzgTAmpwA@mail.gmail.com>
Message-ID: <33EB2A2F-E2DC-43D8-8005-12446CD193DE@infomaniak.ch>

I think your answer may be here.  Check out

https://www.apress.com/gp/book/9781484234945

This guy is very good.

> On 10 Jul 2019, at 18:56, Juan Telleria Ruiz de Aguirre <jtelleria.rproject at gmail.com> wrote:
> 
> Dear R Community,
> 
> As regards R for Financial applications:
> 
> ?Could anyone recommend some good resources for getting started in such
> field? Books, Papers, etc.
> 
> I have identified the following R Resources:
> 
> https://www.crcpress.com/R-Programming-and-Its-Applications-in-Financial-Mathematics/Ohsaki-Ruppert-Felsot-Yoshikawa/p/book/9781498766098
> 
> https://www.springer.com/gp/book/9789462390690
> 
> https://www.crcpress.com/Reproducible-Finance-with-R-Code-Flows-and-Shiny-Apps-for-Portfolio-Analysis/Jr/p/book/9781138484030
> 
> https://www.rpackages.io/view/Finance
> 
> The objective is to be able to identify, from a Data Science Viewpoint,
> what are we able to provide to corporate financial stakeholders.
> 
> Thank you.
> 
> Kind regards,
> Juan Telleria
> 
>    [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. If you want to post, subscribe first.
> -- Also note that this is not the r-help list where general R questions should go.
> 

	[[alternative HTML version deleted]]


From jte||er|@@rproject @end|ng |rom gm@||@com  Sun Jul 14 14:03:10 2019
From: jte||er|@@rproject @end|ng |rom gm@||@com (Juan Telleria Ruiz de Aguirre)
Date: Sun, 14 Jul 2019 14:03:10 +0200
Subject: [R-SIG-Finance] R for Finance: Resources (Books, Papers)
In-Reply-To: <33EB2A2F-E2DC-43D8-8005-12446CD193DE@infomaniak.ch>
References: <CAJXDcw07OKLcckgYyedNeAgKoH5--6dBVOSe3_CeJMzgTAmpwA@mail.gmail.com>
 <33EB2A2F-E2DC-43D8-8005-12446CD193DE@infomaniak.ch>
Message-ID: <CAJXDcw33E=VhPH5NxkTh-nRJc6f1SMNCttf6WV0FFSP=Y727sg@mail.gmail.com>

Thank you!

El mi?rcoles, 10 de julio de 2019, Christian Lear <cl1 at infomaniak.ch>
escribi?:

> I think your answer may be here.  Check out
>
> https://www.apress.com/gp/book/9781484234945
>
> This guy is very good.
>
> On 10 Jul 2019, at 18:56, Juan Telleria Ruiz de Aguirre <
> jtelleria.rproject at gmail.com> wrote:
>
> Dear R Community,
>
> As regards R for Financial applications:
>
> ?Could anyone recommend some good resources for getting started in such
> field? Books, Papers, etc.
>
> I have identified the following R Resources:
>
> https://www.crcpress.com/R-Programming-and-Its-Applications-in-Financial-
> Mathematics/Ohsaki-Ruppert-Felsot-Yoshikawa/p/book/9781498766098
>
> https://www.springer.com/gp/book/9789462390690
>
> https://www.crcpress.com/Reproducible-Finance-with-R-
> Code-Flows-and-Shiny-Apps-for-Portfolio-Analysis/Jr/p/book/9781138484030
>
> https://www.rpackages.io/view/Finance
>
> The objective is to be able to identify, from a Data Science Viewpoint,
> what are we able to provide to corporate financial stakeholders.
>
> Thank you.
>
> Kind regards,
> Juan Telleria
>
>    [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. If you want to post, subscribe first.
> -- Also note that this is not the r-help list where general R questions
> should go.
>
>

	[[alternative HTML version deleted]]


From @@m@hhh1 @end|ng |rom gm@||@com  Wed Jul 24 00:06:34 2019
From: @@m@hhh1 @end|ng |rom gm@||@com (Sam H)
Date: Tue, 23 Jul 2019 18:06:34 -0400
Subject: [R-SIG-Finance] Partitioning approach towards portfolio
 construction - ensemble models/weights based on parameter sets
In-Reply-To: <CAEtT+h1Ch5UHKJ1VnbGDcEfSnNwhDr9DJufrNhByXqXVm=8imw@mail.gmail.com>
References: <CAEtT+h1Ch5UHKJ1VnbGDcEfSnNwhDr9DJufrNhByXqXVm=8imw@mail.gmail.com>
Message-ID: <CAEtT+h1DS2YNzQvHv-ov3tgB2qTYtyzd3Tkf=60BOihPSTRF8w@mail.gmail.com>

Hi,

Is there some (example) code available somewhere (can be highly
experimental) that would enable conducting this kind of analysis (portfolio
construction) (possibly wrapping PortfolioAnalytics):
    - https://blog.thinknewfound.com/2019/07/ensemble-multi-asset-momentum/
    -
https://docs.wixstatic.com/ugd/7c4c63_b3f66bbea0f648e19e535b1da004aeba.pdf
    -
https://docs.wixstatic.com/ugd/7c4c63_735bc38a987340cc8db85691a41dbfe4.pdf

So to be able to create average/ensemble weights based on a set of
parameters (like rebalance date, look back periods for momentum and
whatever the parameters are). Something like quantstrat has
with apply.paramset, add.distribution, add.distribution.constraint, ...

Original message was not delivered due to attachments, I guess.
-- 
Best regards,
Sam

	[[alternative HTML version deleted]]


From ||y@@k|pn|@ @end|ng |rom gm@||@com  Wed Jul 24 00:10:49 2019
From: ||y@@k|pn|@ @end|ng |rom gm@||@com (Ilya Kipnis)
Date: Tue, 23 Jul 2019 18:10:49 -0400
Subject: [R-SIG-Finance] Partitioning approach towards portfolio
 construction - ensemble models/weights based on parameter sets
In-Reply-To: <CAEtT+h1DS2YNzQvHv-ov3tgB2qTYtyzd3Tkf=60BOihPSTRF8w@mail.gmail.com>
References: <CAEtT+h1Ch5UHKJ1VnbGDcEfSnNwhDr9DJufrNhByXqXVm=8imw@mail.gmail.com>
 <CAEtT+h1DS2YNzQvHv-ov3tgB2qTYtyzd3Tkf=60BOihPSTRF8w@mail.gmail.com>
Message-ID: <CA+oJuEEsvxXT3FMjcUbqQ3DECtEHi6TFXBit2YnMn+nukX=3vw@mail.gmail.com>

In my latest post, for tactical asset allocation rebalancing strategies
such as described on the Newfound blog post, I create a method to allow the
user to set a lag on the endpoints so as to allow different trading days
after the month.

https://quantstrattrader.wordpress.com/2019/02/27/kda-robustness-results/

On Tue, Jul 23, 2019 at 6:06 PM Sam H <sam.hhh1 at gmail.com> wrote:

> Hi,
>
> Is there some (example) code available somewhere (can be highly
> experimental) that would enable conducting this kind of analysis (portfolio
> construction) (possibly wrapping PortfolioAnalytics):
>     -
> https://blog.thinknewfound.com/2019/07/ensemble-multi-asset-momentum/
>     -
> https://docs.wixstatic.com/ugd/7c4c63_b3f66bbea0f648e19e535b1da004aeba.pdf
>     -
> https://docs.wixstatic.com/ugd/7c4c63_735bc38a987340cc8db85691a41dbfe4.pdf
>
> So to be able to create average/ensemble weights based on a set of
> parameters (like rebalance date, look back periods for momentum and
> whatever the parameters are). Something like quantstrat has
> with apply.paramset, add.distribution, add.distribution.constraint, ...
>
> Original message was not delivered due to attachments, I guess.
> --
> Best regards,
> Sam
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. If you want to post, subscribe first.
> -- Also note that this is not the r-help list where general R questions
> should go.
>

	[[alternative HTML version deleted]]


From e@ @end|ng |rom enr|co@chum@nn@net  Wed Jul 24 08:03:36 2019
From: e@ @end|ng |rom enr|co@chum@nn@net (Enrico Schumann)
Date: Wed, 24 Jul 2019 08:03:36 +0200
Subject: [R-SIG-Finance] Partitioning approach towards portfolio
 construction - ensemble models/weights based on parameter sets
In-Reply-To: <CAEtT+h1DS2YNzQvHv-ov3tgB2qTYtyzd3Tkf=60BOihPSTRF8w@mail.gmail.com>
 (Sam H.'s message of "Tue, 23 Jul 2019 18:06:34 -0400")
References: <CAEtT+h1Ch5UHKJ1VnbGDcEfSnNwhDr9DJufrNhByXqXVm=8imw@mail.gmail.com>
 <CAEtT+h1DS2YNzQvHv-ov3tgB2qTYtyzd3Tkf=60BOihPSTRF8w@mail.gmail.com>
Message-ID: <874l3c2clj.fsf@enricoschumann.net>

>>>>> "Sam" == Sam H <sam.hhh1 at gmail.com> writes:

    Sam> Hi,
    Sam> Is there some (example) code available somewhere (can be highly
    Sam> experimental) that would enable conducting this kind of analysis (portfolio
    Sam> construction) (possibly wrapping PortfolioAnalytics):
    Sam>     - https://blog.thinknewfound.com/2019/07/ensemble-multi-asset-momentum/
    Sam>     -
    Sam> https://docs.wixstatic.com/ugd/7c4c63_b3f66bbea0f648e19e535b1da004aeba.pdf
    Sam>     -
    Sam> https://docs.wixstatic.com/ugd/7c4c63_735bc38a987340cc8db85691a41dbfe4.pdf

    Sam> So to be able to create average/ensemble weights based on a set of
    Sam> parameters (like rebalance date, look back periods for momentum and
    Sam> whatever the parameters are). Something like quantstrat has
    Sam> with apply.paramset, add.distribution, add.distribution.constraint, ...

    Sam> Original message was not delivered due to attachments, I guess.
    Sam> -- 
    Sam> Best regards,
    Sam> Sam

Perhaps the examples in https://ssrn.com/abstract=3374195 are of
interest (though they do not use PortfolioAnalytics).

kind regards
     Enrico

-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From rodr|go@b@d|||@ @end|ng |rom gm@||@com  Wed Jul 31 17:36:46 2019
From: rodr|go@b@d|||@ @end|ng |rom gm@||@com (Rodrigo Badilla)
Date: Wed, 31 Jul 2019 11:36:46 -0400
Subject: [R-SIG-Finance] Replicating TVECM plot
Message-ID: <6869d5a2c71d1f53a2eb4802efd45939@desktop-3nkp7dq>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20190731/04236c42/attachment.html>

From er|cjberger @end|ng |rom gm@||@com  Thu Aug  1 17:17:53 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Thu, 1 Aug 2019 18:17:53 +0300
Subject: [R-SIG-Finance] Replicating TVECM plot
In-Reply-To: <6869d5a2c71d1f53a2eb4802efd45939@desktop-3nkp7dq>
References: <6869d5a2c71d1f53a2eb4802efd45939@desktop-3nkp7dq>
Message-ID: <CAGgJW77nesP6J-2P9hCmhvw7s3GK9ay_GLtdzJmaYhWeuvkFeg@mail.gmail.com>

Hi Rodrigo,
I have no experience with this particular package but I took a quick look.
a) I was able to find the information in the tvec object for the final
Residual Sum of Squares (i.e. for the "optimal" choice).
    More precisely the individual residuals are available in an Nx2 matrix,
call it M, and by calculating t(M) %*% M you get a 2x2 matrix
    and the sum of its diagonals is the Residual Sum of Squares at the
final stage in the search.
b) it looks to me like the tvec object does *not* contain the information
in the plot (I could certainly be wrong).
    This would not be that surprising as there is no obvious necessity for
reporting what are essentially "intermediate" results on the way to the
final answer.

If you really want to reproduce the plot, a couple of options you might
consider:
i. go into the source code and find where the plot is generated and  modify
the code to print out the data that you want
ii. contact the author(s) of the package directly with your question. Maybe
the data is in the returned object. Or maybe they can make an enhancement
to include it in the returned object.

Best,
Eric


On Wed, Jul 31, 2019 at 6:36 PM Rodrigo Badilla <rodrigo.badilla at gmail.com>
wrote:

> Hi all,
>
> I am working with a TVECM model using library tsDyn.
>
> I am trying to get data (Residual Sum of squares)  from a TVECM model, the
> result give me a plot that I would like replicate with ggplot. I had check
> all results but none coincide with plot data, example:
>
> library(tsDyn)
> data(zeroyld)
>
> #runing this command you get the plot that I need replicate:
>
>
> tvec <- TVECM(zeroyld, nthresh=2,lag=1, ngridBeta=20, ngridTh=30,
> plot=TRUE,trim=0.05, common="All")
>
>
> #I did check all this results but none help me to replicate the graph
> names(tvec)
>
>
> [1] "coefficients"   "residuals"      "model"          "coeffmat"
>  [5] "nobs_regimes"   "k"              "t"              "T"
>  [9] "nparB"          "fitted.values"  "lag"            "include"
> [13] "model.specific"
>
> Do you now how get Residual Sum of squares to replicate this graph?
>
> Any comments I will grateful
> Regards
> Rodrigo
>
> _______________________________________________
> R-SIG-Finance at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. If you want to post, subscribe first.
> -- Also note that this is not the r-help list where general R questions
> should go.
>

	[[alternative HTML version deleted]]


From jo@hu@kn|pe @end|ng |rom gm@||@com  Sun Aug  4 21:56:57 2019
From: jo@hu@kn|pe @end|ng |rom gm@||@com (Joshua Knipe)
Date: Sun, 4 Aug 2019 21:56:57 +0200
Subject: [R-SIG-Finance] [PortfolioAnalytics] optimize.portfolio.rebalancing
 with changing stock universe
Message-ID: <CAK79wcwjsedAJDXpXFbtiwG7QorK9SgX2k7oru=BDtHNriuDug@mail.gmail.com>

I am using the PortfolioAnalytics package to run a multi-period
optimisation on a stock universe of just over 100 stocks but I'm running
into some issues. I would like to exclude certain stocks at each
rebalancing date but have not been able to get it working thus far.


<a href="
http://r.789695.n4.nabble.com/optimize-portfolio-rebalancing-with-changing-dynamic-stock-universe-PortfolioAnalytics-td4753227.html
">
http://r.789695.n4.nabble.com/optimize-portfolio-rebalancing-with-changing-dynamic-stock-universe-PortfolioAnalytics-td4753227.html
</a>

This thread has a very similar issue and Ross Bennett (the creator of this
awesome package) mentions writing ones own rebalancing loop in order to
achieve an evolving universe of stocks. This sounds like the solution but I
am not sure how to implement that unfortunately.


I have calculated a measure of illiquidity for each stock (the Amihud's
ILLIQ value specifically) and have these in a separate file which I am
reading into R. The measures are weekly in order to match the data I am
using for the returns of the stocks. I have calculated a vector of the
median value of this measure for each week i.e. 160 medians for 160 weeks.


What I would like to do is exclude all the stocks which have a Amihud value
above this median from the rebalancing for that period i.e. make sure they
have 0 weighting if they are not liquid enough.

I tried it initially with a penalty function which would just sum the
weights * Amihud values and then scale that value, but this has the
undesired effect of also penalising stocks below but close to the median
value.


My end goal is to get the returns of certain portfolio strategies (e.g.
mean-var, min-var, equal weight etc) on this data set and then compare how
the returns change when these "illiquid stocks" are excluded at each
rebalancing period.


Any advice would be seriously appreciated! :)


<b>Data</b>


The Amihud values are in the following format (which is the same format as
the price data):


Date.               Stock 1.             Stock 2.                Stock 3.
          Stock 4.           etc

2009-06-05.    1.057348e-06.    5.263602e-07   2.054617e-11.   6.470526e-11

2009-06-12.    1.057346e-06.    5.123452e-07   2.311231e-11.   6.381738e-11

2009-06-19.    1.034941e-06.    5.219238e-07   2.192929e-11.   6.238939e-11


<b>Code</b>

An extract of the code I have thus far:


sjPrices = as.data.frame(read_excel("ALSI_Cleaned_Weekly_Prices.xlsx",
sheet = 1, col_names = T,

                                            na = "", skip = 0))

weeklyAmihud = as.data.frame(read_excel("weeklyAmihud.xlsx", sheet = 1,
col_names = T,

                                              na = "", skip = 0))


#create vector of medians (uses function from matrixStats package)

mid <- rowMedians(as.matrix(weeklyAmihud[, -1]))


dates = as_date(sjPrices[,1])


# remove the first column (of dates)

sjPrices <- sjPrices[, -1]

weeklyAmihud <- weeklyAmihud[, -1]


#create xts objects

sjPrices <- xts(sjPrices, order.by=dates)

weeklyAmihud <- xts(weeklyAmihud, order.by=dates)


sjReturns <- ROC(sjPrices)


#remove first row (0 returns)

sjReturns <- sjReturns[-1,]


#this function does penalise the very illiquid stocks but has other
undesired side-effects

AMIHUD <- function(sjReturns, weights, wAmihud){

  return(sum(weights*wAmihud)*100)

}


portf <- portfolio.spec(colnames(sjReturns))


portf <- add.constraint(portf,type = "weight_sum", min_sum = 0.99, max_sum
= 1.01)

portf <- add.constraint(portf, type="long_only")


portf <- add.objective(portf, type = "risk", name = "StdDev")

portf <- add.objective(portf, type="risk", name="AMIHUD",

                       arguments=list(wAmihud=weeklyAmihud))


opt_rebal <- optimize.portfolio.rebalancing(sjReturns,

                                            portf,

                                            optimize_method="DEoptim",

                                            rebalance_on="quarters",

                                            training_period=52,

                                            rolling_window=52,

                                            trace=TRUE, traceDE=5,
search_size=2000)

	[[alternative HTML version deleted]]


From br|@n @end|ng |rom br@verock@com  Sun Aug  4 23:09:08 2019
From: br|@n @end|ng |rom br@verock@com (Brian G. Peterson)
Date: Sun, 04 Aug 2019 16:09:08 -0500
Subject: [R-SIG-Finance] 
 [PortfolioAnalytics] optimize.portfolio.rebalancing
 with changing stock universe
In-Reply-To: <CAK79wcwjsedAJDXpXFbtiwG7QorK9SgX2k7oru=BDtHNriuDug@mail.gmail.com>
References: <CAK79wcwjsedAJDXpXFbtiwG7QorK9SgX2k7oru=BDtHNriuDug@mail.gmail.com>
Message-ID: <5c42da04867951ba43b3efd83f9f2cbf4b3262dc.camel@braverock.com>

Joshua,

The problem of assets becoming available or unavailable through time
has been one we've been aware of for a long time.

Ross' solution of building a custom loop certainly works, as you can
call optimize.portfolio in your own loop and change either the 'assets'
slot in the portfolio specification or the constraints matrix,
constraining some assets to 0 position.

We've been discussing this as one open package TODO with our 2019 GSoC
Student, Shawn Feng of the University of Illinois, and are hoping that
we may have a solution in the package code this month for a more
general solution.

We have identified a couple of possible general solutions.  All revolve
around making some aspect of the portfolios specification into a time
series.  Either the 'assets' slot in the portfolio spec could specify
valid assets as a time series rather than the named vector that it is
right now, the 'box_constraint' could take a time series and e.g. take
NA as a constrain for unavailable assets, or the 'position_limit'
constraint could be specified as a time series.

My suspicion is that either changing the assets slot over time or
allowing NA's in the box constraints are the most feasible solution,
but we don't have any code to back that hypothesis up yet.

As you point out indirectly, different analysts will have different
goals for how to include or exclude specific assets at a particular
point in time, so I think we will need to just support a time series
methodology for defining the universe and then lett the user define
their universe using their own criteria (e.g. your Amihud criteria).

Unfortunately, we don't have working general code to solve this feature
request just yet.  Until then (hopefully soon), I'm afraid you'll need
to adjust things manually and call 'optimize.portfolio' multiple times,
on dates of your choosing.

As for your desire to check multiple different benchmark portfolios,
see help and examples for 'combine.portfolios' and
'combine.optimizations' to create an output object of type  'opt.list'.
Most of the summary, print, plot, etc functions should handle these
combined optimizations and give you reasonable output already. 

Regards,

Brian


On Sun, 2019-08-04 at 21:56 +0200, Joshua Knipe wrote:
> I am using the PortfolioAnalytics package to run a multi-period
> optimisation on a stock universe of just over 100 stocks but I'm
> running
> into some issues. I would like to exclude certain stocks at each
> rebalancing date but have not been able to get it working thus far.
> 
> 
> <a href="
> http://r.789695.n4.nabble.com/optimize-portfolio-rebalancing-with-changing-dynamic-stock-universe-PortfolioAnalytics-td4753227.html
> ">
> http://r.789695.n4.nabble.com/optimize-portfolio-rebalancing-with-changing-dynamic-stock-universe-PortfolioAnalytics-td4753227.html
> </a>
> 
> This thread has a very similar issue and Ross Bennett (the creator of
> this awesome package) mentions writing ones own rebalancing loop in
> order to achieve an evolving universe of stocks. This sounds like the
> solution but I am not sure how to implement that unfortunately.
> 
> 
> I have calculated a measure of illiquidity for each stock (the
> Amihud'sILLIQ value specifically) and have these in a separate file
> which I am reading into R. The measures are weekly in order to match
> the data I am using for the returns of the stocks. I have calculated
> a vector of the median value of this measure for each week i.e. 160
> medians for 160 weeks.
> 
> 
> What I would like to do is exclude all the stocks which have a Amihud
> value above this median from the rebalancing for that period i.e.
> make sure they have 0 weighting if they are not liquid enough.
> 
> I tried it initially with a penalty function which would just sum the
> weights * Amihud values and then scale that value, but this has the
> undesired effect of also penalising stocks below but close to the
> median
> value.
> 
> 
> My end goal is to get the returns of certain portfolio strategies
> (e.g. mean-var, min-var, equal weight etc) on this data set and then
> compare how the returns change when these "illiquid stocks" are
> excluded at each rebalancing period.
> 
> 
> Any advice would be seriously appreciated! :)
> 
> 
> <b>Data</b>
> 
> 
> The Amihud values are in the following format (which is the same
> format as
> the price data):
> 
> 
> Date.               Stock 1.             Stock
> 2.                Stock 3.
>           Stock 4.           etc
> 
> 2009-06-05.    1.057348e-06.    5.263602e-07   2.054617e-
> 11.   6.470526e-11
> 
> 2009-06-12.    1.057346e-06.    5.123452e-07   2.311231e-
> 11.   6.381738e-11
> 
> 2009-06-19.    1.034941e-06.    5.219238e-07   2.192929e-
> 11.   6.238939e-11
> 
> 
> <b>Code</b>
> 
> An extract of the code I have thus far:
> 
> 
> sjPrices =
> as.data.frame(read_excel("ALSI_Cleaned_Weekly_Prices.xlsx",
> sheet = 1, col_names = T,
> 
>                                             na = "", skip = 0))
> 
> weeklyAmihud = as.data.frame(read_excel("weeklyAmihud.xlsx", sheet =
> 1,
> col_names = T,
> 
>                                               na = "", skip = 0))
> 
> 
> #create vector of medians (uses function from matrixStats package)
> 
> mid <- rowMedians(as.matrix(weeklyAmihud[, -1]))
> 
> 
> dates = as_date(sjPrices[,1])
> 
> 
> # remove the first column (of dates)
> 
> sjPrices <- sjPrices[, -1]
> 
> weeklyAmihud <- weeklyAmihud[, -1]
> 
> 
> #create xts objects
> 
> sjPrices <- xts(sjPrices, order.by=dates)
> 
> weeklyAmihud <- xts(weeklyAmihud, order.by=dates)
> 
> 
> sjReturns <- ROC(sjPrices)
> 
> 
> #remove first row (0 returns)
> 
> sjReturns <- sjReturns[-1,]
> 
> 
> #this function does penalise the very illiquid stocks but has other
> undesired side-effects
> 
> AMIHUD <- function(sjReturns, weights, wAmihud){
> 
>   return(sum(weights*wAmihud)*100)
> 
> }
> 
> 
> portf <- portfolio.spec(colnames(sjReturns))
> 
> 
> portf <- add.constraint(portf,type = "weight_sum", min_sum = 0.99,
> max_sum
> = 1.01)
> 
> portf <- add.constraint(portf, type="long_only")
> 
> 
> portf <- add.objective(portf, type = "risk", name = "StdDev")
> 
> portf <- add.objective(portf, type="risk", name="AMIHUD",
> 
>                        arguments=list(wAmihud=weeklyAmihud))
> 
> 
> opt_rebal <- optimize.portfolio.rebalancing(sjReturns,
> 
>                                             portf,
> 
>                                             optimize_method="DEoptim"
> ,
> 
>                                             rebalance_on="quarters",
> 
>                                             training_period=52,
> 
>                                             rolling_window=52,
> 
>                                             trace=TRUE, traceDE=5,
> search_size=2000)
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. If you want to post, subscribe first.
> -- Also note that this is not the r-help list where general R
> questions should go.
-- 
Brian G. Peterson
ph: +1.773.459.4973
im: bgpbraverock


From ||y@@k|pn|@ @end|ng |rom gm@||@com  Mon Aug  5 00:09:32 2019
From: ||y@@k|pn|@ @end|ng |rom gm@||@com (Ilya Kipnis)
Date: Sun, 4 Aug 2019 18:09:32 -0400
Subject: [R-SIG-Finance] 
 [PortfolioAnalytics] optimize.portfolio.rebalancing
 with changing stock universe
In-Reply-To: <5c42da04867951ba43b3efd83f9f2cbf4b3262dc.camel@braverock.com>
References: <CAK79wcwjsedAJDXpXFbtiwG7QorK9SgX2k7oru=BDtHNriuDug@mail.gmail.com>
 <5c42da04867951ba43b3efd83f9f2cbf4b3262dc.camel@braverock.com>
Message-ID: <CA+oJuEFSdU7FjKoPmjfnOKV6mbo-UWNq_skNNgBWDRj9SwPXQg@mail.gmail.com>

This seems a bit kludge-y, but can't you include an investment limit
constraint, that as an asset becomes available, that there's a constraint
vector which will allow its maximum absolute weight to be greater than zero?

On Sun, Aug 4, 2019 at 5:09 PM Brian G. Peterson <brian at braverock.com>
wrote:

> Joshua,
>
> The problem of assets becoming available or unavailable through time
> has been one we've been aware of for a long time.
>
> Ross' solution of building a custom loop certainly works, as you can
> call optimize.portfolio in your own loop and change either the 'assets'
> slot in the portfolio specification or the constraints matrix,
> constraining some assets to 0 position.
>
> We've been discussing this as one open package TODO with our 2019 GSoC
> Student, Shawn Feng of the University of Illinois, and are hoping that
> we may have a solution in the package code this month for a more
> general solution.
>
> We have identified a couple of possible general solutions.  All revolve
> around making some aspect of the portfolios specification into a time
> series.  Either the 'assets' slot in the portfolio spec could specify
> valid assets as a time series rather than the named vector that it is
> right now, the 'box_constraint' could take a time series and e.g. take
> NA as a constrain for unavailable assets, or the 'position_limit'
> constraint could be specified as a time series.
>
> My suspicion is that either changing the assets slot over time or
> allowing NA's in the box constraints are the most feasible solution,
> but we don't have any code to back that hypothesis up yet.
>
> As you point out indirectly, different analysts will have different
> goals for how to include or exclude specific assets at a particular
> point in time, so I think we will need to just support a time series
> methodology for defining the universe and then lett the user define
> their universe using their own criteria (e.g. your Amihud criteria).
>
> Unfortunately, we don't have working general code to solve this feature
> request just yet.  Until then (hopefully soon), I'm afraid you'll need
> to adjust things manually and call 'optimize.portfolio' multiple times,
> on dates of your choosing.
>
> As for your desire to check multiple different benchmark portfolios,
> see help and examples for 'combine.portfolios' and
> 'combine.optimizations' to create an output object of type  'opt.list'.
> Most of the summary, print, plot, etc functions should handle these
> combined optimizations and give you reasonable output already.
>
> Regards,
>
> Brian
>
>
> On Sun, 2019-08-04 at 21:56 +0200, Joshua Knipe wrote:
> > I am using the PortfolioAnalytics package to run a multi-period
> > optimisation on a stock universe of just over 100 stocks but I'm
> > running
> > into some issues. I would like to exclude certain stocks at each
> > rebalancing date but have not been able to get it working thus far.
> >
> >
> > <a href="
> >
> http://r.789695.n4.nabble.com/optimize-portfolio-rebalancing-with-changing-dynamic-stock-universe-PortfolioAnalytics-td4753227.html
> > ">
> >
> http://r.789695.n4.nabble.com/optimize-portfolio-rebalancing-with-changing-dynamic-stock-universe-PortfolioAnalytics-td4753227.html
> > </a>
> >
> > This thread has a very similar issue and Ross Bennett (the creator of
> > this awesome package) mentions writing ones own rebalancing loop in
> > order to achieve an evolving universe of stocks. This sounds like the
> > solution but I am not sure how to implement that unfortunately.
> >
> >
> > I have calculated a measure of illiquidity for each stock (the
> > Amihud'sILLIQ value specifically) and have these in a separate file
> > which I am reading into R. The measures are weekly in order to match
> > the data I am using for the returns of the stocks. I have calculated
> > a vector of the median value of this measure for each week i.e. 160
> > medians for 160 weeks.
> >
> >
> > What I would like to do is exclude all the stocks which have a Amihud
> > value above this median from the rebalancing for that period i.e.
> > make sure they have 0 weighting if they are not liquid enough.
> >
> > I tried it initially with a penalty function which would just sum the
> > weights * Amihud values and then scale that value, but this has the
> > undesired effect of also penalising stocks below but close to the
> > median
> > value.
> >
> >
> > My end goal is to get the returns of certain portfolio strategies
> > (e.g. mean-var, min-var, equal weight etc) on this data set and then
> > compare how the returns change when these "illiquid stocks" are
> > excluded at each rebalancing period.
> >
> >
> > Any advice would be seriously appreciated! :)
> >
> >
> > <b>Data</b>
> >
> >
> > The Amihud values are in the following format (which is the same
> > format as
> > the price data):
> >
> >
> > Date.               Stock 1.             Stock
> > 2.                Stock 3.
> >           Stock 4.           etc
> >
> > 2009-06-05.    1.057348e-06.    5.263602e-07   2.054617e-
> > 11.   6.470526e-11
> >
> > 2009-06-12.    1.057346e-06.    5.123452e-07   2.311231e-
> > 11.   6.381738e-11
> >
> > 2009-06-19.    1.034941e-06.    5.219238e-07   2.192929e-
> > 11.   6.238939e-11
> >
> >
> > <b>Code</b>
> >
> > An extract of the code I have thus far:
> >
> >
> > sjPrices =
> > as.data.frame(read_excel("ALSI_Cleaned_Weekly_Prices.xlsx",
> > sheet = 1, col_names = T,
> >
> >                                             na = "", skip = 0))
> >
> > weeklyAmihud = as.data.frame(read_excel("weeklyAmihud.xlsx", sheet =
> > 1,
> > col_names = T,
> >
> >                                               na = "", skip = 0))
> >
> >
> > #create vector of medians (uses function from matrixStats package)
> >
> > mid <- rowMedians(as.matrix(weeklyAmihud[, -1]))
> >
> >
> > dates = as_date(sjPrices[,1])
> >
> >
> > # remove the first column (of dates)
> >
> > sjPrices <- sjPrices[, -1]
> >
> > weeklyAmihud <- weeklyAmihud[, -1]
> >
> >
> > #create xts objects
> >
> > sjPrices <- xts(sjPrices, order.by=dates)
> >
> > weeklyAmihud <- xts(weeklyAmihud, order.by=dates)
> >
> >
> > sjReturns <- ROC(sjPrices)
> >
> >
> > #remove first row (0 returns)
> >
> > sjReturns <- sjReturns[-1,]
> >
> >
> > #this function does penalise the very illiquid stocks but has other
> > undesired side-effects
> >
> > AMIHUD <- function(sjReturns, weights, wAmihud){
> >
> >   return(sum(weights*wAmihud)*100)
> >
> > }
> >
> >
> > portf <- portfolio.spec(colnames(sjReturns))
> >
> >
> > portf <- add.constraint(portf,type = "weight_sum", min_sum = 0.99,
> > max_sum
> > = 1.01)
> >
> > portf <- add.constraint(portf, type="long_only")
> >
> >
> > portf <- add.objective(portf, type = "risk", name = "StdDev")
> >
> > portf <- add.objective(portf, type="risk", name="AMIHUD",
> >
> >                        arguments=list(wAmihud=weeklyAmihud))
> >
> >
> > opt_rebal <- optimize.portfolio.rebalancing(sjReturns,
> >
> >                                             portf,
> >
> >                                             optimize_method="DEoptim"
> > ,
> >
> >                                             rebalance_on="quarters",
> >
> >                                             training_period=52,
> >
> >                                             rolling_window=52,
> >
> >                                             trace=TRUE, traceDE=5,
> > search_size=2000)
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only. If you want to post, subscribe first.
> > -- Also note that this is not the r-help list where general R
> > questions should go.
> --
> Brian G. Peterson
> ph: +1.773.459.4973
> im: bgpbraverock
>
> _______________________________________________
> R-SIG-Finance at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. If you want to post, subscribe first.
> -- Also note that this is not the r-help list where general R questions
> should go.
>

	[[alternative HTML version deleted]]


From @tu|486 @end|ng |rom gm@||@com  Mon Aug  5 01:47:01 2019
From: @tu|486 @end|ng |rom gm@||@com (Atul Agarawal)
Date: Mon, 5 Aug 2019 05:17:01 +0530
Subject: [R-SIG-Finance] Mothly Returns of Mutual funds
Message-ID: <CAKysKFHkQrXY_E9yqCjV6rgBUOy-eaX78+aO0pknPYetfodiFA@mail.gmail.com>

Hi Everyone,

I am New to R. I would like to create Mutual fund Portfolio. Can someone
help me to understand from where I can download Monthly returns of Indian
MF.

Also pls share the process document for Portfolio Optimisation and which R
packages are required to achieve the above goal.

Regards,
Atul Agarawal

	[[alternative HTML version deleted]]


From br|@n @end|ng |rom br@verock@com  Mon Aug  5 02:21:53 2019
From: br|@n @end|ng |rom br@verock@com (Brian G. Peterson)
Date: Sun, 04 Aug 2019 19:21:53 -0500
Subject: [R-SIG-Finance] Mothly Returns of Mutual funds
In-Reply-To: <CAKysKFHkQrXY_E9yqCjV6rgBUOy-eaX78+aO0pknPYetfodiFA@mail.gmail.com>
References: <CAKysKFHkQrXY_E9yqCjV6rgBUOy-eaX78+aO0pknPYetfodiFA@mail.gmail.com>
Message-ID: <2a5092a9ce73034d7cae68ad20d50cb810ef9bf1.camel@braverock.com>

On Mon, 2019-08-05 at 05:17 +0530, Atul Agarawal wrote:
> I am New to R. I would like to create Mutual fund Portfolio. Can
> someone help me to understand from where I can download Monthly
> returns of Indian MF.

In the United States and most other places, I would consult with your
broker, or a service that sells that data.

> Also pls share the process document for Portfolio Optimisation and
> which R packages are required to achieve the above goal.

PortfolioAnalytics can do the optimization.  There is lots of
documentation in the package and elsewhere online.

-- 
Brian G. Peterson
ph: +1.773.459.4973
im: bgpbraverock


From jo@hu@kn|pe @end|ng |rom gm@||@com  Tue Aug  6 10:03:33 2019
From: jo@hu@kn|pe @end|ng |rom gm@||@com (Joshua Knipe)
Date: Tue, 6 Aug 2019 10:03:33 +0200
Subject: [R-SIG-Finance] 
 [PortfolioAnalytics] optimize.portfolio.rebalancing
 with changing stock universe
In-Reply-To: <5c42da04867951ba43b3efd83f9f2cbf4b3262dc.camel@braverock.com>
References: <CAK79wcwjsedAJDXpXFbtiwG7QorK9SgX2k7oru=BDtHNriuDug@mail.gmail.com>
 <5c42da04867951ba43b3efd83f9f2cbf4b3262dc.camel@braverock.com>
Message-ID: <CAK79wcxn2rF+XbJ5+ngHcJLFxkRyp1M24aaNq1FO9cKEkE8UCA@mail.gmail.com>

Hi Brian,

Thank you for your response - I was a bit unclear as to what exactly the
rebalancing loop entailed but that helped clear things up. Thank you also
for all the work you've done in developing both PerformanceAnalytics and
PortfolioAnalytics! Both have been essential in the research project I'm
working on at the moment.

I was also not aware of the 'combine.portfolios' and
'combine.optimizations' functions but those look like they are just what I
need!

I have included the rebalancing loop section of my code below if anyone
else needs it before you guys find the time to implement it in the package.

Kind regards
Josh

*Code:*

numWeeks = floor(as.duration(startDate %m+% weeks(52) %--%
last(dates))/dweeks(13))

for (i in 1:numWeeks){
  portf <- add.constraint(portf, type="group",

    #this is checking which stocks have Amihud values above the median
Amihud value at each rebalancing date
                          groups = list(which(weeklyAmihud[paste0(startDate
%m+% weeks(52+13*i))]
                                >as.numeric(mid[paste0(startDate %m+%
weeks(52+13*i))])) ),

    #this is setting their max weight to be 0
                          group_min=c(0),
                          group_max=c(0),
                          group_labels=c("Illiquid"),
                          group_pos=c(1),
                          indexnum=3)

  #will have to increase search_size when running on full dataset
  portfolio.optimized <- optimize.portfolio(R = sjReturns[paste0(startDate
%m+% weeks(13*i),
                                                  "::",
                                                   startDate %m+%
weeks(52+13*i))],
                                            portfolio = portf,
                                            optimize_method = "random",
                                            search_size = 500)
  #get the weights at each rebalacing period and add to dataframe
  extra <-extractWeights(portfolio.optimized)
  rebal_weights <- rbind(rebal_weights,extra)

  #set rowname to equal rebalancing date
  rownames(rebal_weights)[i+1]<-paste0(startDate %m+% weeks(52+13*i))
   }

#change first rowname to first rebalancing date
rownames(rebal_weights)[1]<- paste0(startDate %m+% weeks(52))
dates2 = as_date(rownames(rebal_weights))

rebal_weights <- xts(rebal_weights, order.by=dates2)
rebal_returns <- Return.portfolio(sjReturns, weights=rebal_weights)


On Sun, Aug 4, 2019 at 11:09 PM Brian G. Peterson <brian at braverock.com>
wrote:

> Joshua,
>
> The problem of assets becoming available or unavailable through time
> has been one we've been aware of for a long time.
>
> Ross' solution of building a custom loop certainly works, as you can
> call optimize.portfolio in your own loop and change either the 'assets'
> slot in the portfolio specification or the constraints matrix,
> constraining some assets to 0 position.
>
> We've been discussing this as one open package TODO with our 2019 GSoC
> Student, Shawn Feng of the University of Illinois, and are hoping that
> we may have a solution in the package code this month for a more
> general solution.
>
> We have identified a couple of possible general solutions.  All revolve
> around making some aspect of the portfolios specification into a time
> series.  Either the 'assets' slot in the portfolio spec could specify
> valid assets as a time series rather than the named vector that it is
> right now, the 'box_constraint' could take a time series and e.g. take
> NA as a constrain for unavailable assets, or the 'position_limit'
> constraint could be specified as a time series.
>
> My suspicion is that either changing the assets slot over time or
> allowing NA's in the box constraints are the most feasible solution,
> but we don't have any code to back that hypothesis up yet.
>
> As you point out indirectly, different analysts will have different
> goals for how to include or exclude specific assets at a particular
> point in time, so I think we will need to just support a time series
> methodology for defining the universe and then lett the user define
> their universe using their own criteria (e.g. your Amihud criteria).
>
> Unfortunately, we don't have working general code to solve this feature
> request just yet.  Until then (hopefully soon), I'm afraid you'll need
> to adjust things manually and call 'optimize.portfolio' multiple times,
> on dates of your choosing.
>
> As for your desire to check multiple different benchmark portfolios,
> see help and examples for 'combine.portfolios' and
> 'combine.optimizations' to create an output object of type  'opt.list'.
> Most of the summary, print, plot, etc functions should handle these
> combined optimizations and give you reasonable output already.
>
> Regards,
>
> Brian
>
>
> On Sun, 2019-08-04 at 21:56 +0200, Joshua Knipe wrote:
> > I am using the PortfolioAnalytics package to run a multi-period
> > optimisation on a stock universe of just over 100 stocks but I'm
> > running
> > into some issues. I would like to exclude certain stocks at each
> > rebalancing date but have not been able to get it working thus far.
> >
> >
> > <a href="
> >
> http://r.789695.n4.nabble.com/optimize-portfolio-rebalancing-with-changing-dynamic-stock-universe-PortfolioAnalytics-td4753227.html
> > ">
> >
> http://r.789695.n4.nabble.com/optimize-portfolio-rebalancing-with-changing-dynamic-stock-universe-PortfolioAnalytics-td4753227.html
> > </a>
> >
> > This thread has a very similar issue and Ross Bennett (the creator of
> > this awesome package) mentions writing ones own rebalancing loop in
> > order to achieve an evolving universe of stocks. This sounds like the
> > solution but I am not sure how to implement that unfortunately.
> >
> >
> > I have calculated a measure of illiquidity for each stock (the
> > Amihud'sILLIQ value specifically) and have these in a separate file
> > which I am reading into R. The measures are weekly in order to match
> > the data I am using for the returns of the stocks. I have calculated
> > a vector of the median value of this measure for each week i.e. 160
> > medians for 160 weeks.
> >
> >
> > What I would like to do is exclude all the stocks which have a Amihud
> > value above this median from the rebalancing for that period i.e.
> > make sure they have 0 weighting if they are not liquid enough.
> >
> > I tried it initially with a penalty function which would just sum the
> > weights * Amihud values and then scale that value, but this has the
> > undesired effect of also penalising stocks below but close to the
> > median
> > value.
> >
> >
> > My end goal is to get the returns of certain portfolio strategies
> > (e.g. mean-var, min-var, equal weight etc) on this data set and then
> > compare how the returns change when these "illiquid stocks" are
> > excluded at each rebalancing period.
> >
> >
> > Any advice would be seriously appreciated! :)
> >
> >
> > <b>Data</b>
> >
> >
> > The Amihud values are in the following format (which is the same
> > format as
> > the price data):
> >
> >
> > Date.               Stock 1.             Stock
> > 2.                Stock 3.
> >           Stock 4.           etc
> >
> > 2009-06-05.    1.057348e-06.    5.263602e-07   2.054617e-
> > 11.   6.470526e-11
> >
> > 2009-06-12.    1.057346e-06.    5.123452e-07   2.311231e-
> > 11.   6.381738e-11
> >
> > 2009-06-19.    1.034941e-06.    5.219238e-07   2.192929e-
> > 11.   6.238939e-11
> >
> >
> > <b>Code</b>
> >
> > An extract of the code I have thus far:
> >
> >
> > sjPrices =
> > as.data.frame(read_excel("ALSI_Cleaned_Weekly_Prices.xlsx",
> > sheet = 1, col_names = T,
> >
> >                                             na = "", skip = 0))
> >
> > weeklyAmihud = as.data.frame(read_excel("weeklyAmihud.xlsx", sheet =
> > 1,
> > col_names = T,
> >
> >                                               na = "", skip = 0))
> >
> >
> > #create vector of medians (uses function from matrixStats package)
> >
> > mid <- rowMedians(as.matrix(weeklyAmihud[, -1]))
> >
> >
> > dates = as_date(sjPrices[,1])
> >
> >
> > # remove the first column (of dates)
> >
> > sjPrices <- sjPrices[, -1]
> >
> > weeklyAmihud <- weeklyAmihud[, -1]
> >
> >
> > #create xts objects
> >
> > sjPrices <- xts(sjPrices, order.by=dates)
> >
> > weeklyAmihud <- xts(weeklyAmihud, order.by=dates)
> >
> >
> > sjReturns <- ROC(sjPrices)
> >
> >
> > #remove first row (0 returns)
> >
> > sjReturns <- sjReturns[-1,]
> >
> >
> > #this function does penalise the very illiquid stocks but has other
> > undesired side-effects
> >
> > AMIHUD <- function(sjReturns, weights, wAmihud){
> >
> >   return(sum(weights*wAmihud)*100)
> >
> > }
> >
> >
> > portf <- portfolio.spec(colnames(sjReturns))
> >
> >
> > portf <- add.constraint(portf,type = "weight_sum", min_sum = 0.99,
> > max_sum
> > = 1.01)
> >
> > portf <- add.constraint(portf, type="long_only")
> >
> >
> > portf <- add.objective(portf, type = "risk", name = "StdDev")
> >
> > portf <- add.objective(portf, type="risk", name="AMIHUD",
> >
> >                        arguments=list(wAmihud=weeklyAmihud))
> >
> >
> > opt_rebal <- optimize.portfolio.rebalancing(sjReturns,
> >
> >                                             portf,
> >
> >                                             optimize_method="DEoptim"
> > ,
> >
> >                                             rebalance_on="quarters",
> >
> >                                             training_period=52,
> >
> >                                             rolling_window=52,
> >
> >                                             trace=TRUE, traceDE=5,
> > search_size=2000)
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only. If you want to post, subscribe first.
> > -- Also note that this is not the r-help list where general R
> > questions should go.
> --
> Brian G. Peterson
> ph: +1.773.459.4973
> im: bgpbraverock
>
>

	[[alternative HTML version deleted]]


From tom|err|93 @end|ng |rom gm@||@com  Wed Aug  7 16:47:36 2019
From: tom|err|93 @end|ng |rom gm@||@com (Tommaso Ferrari)
Date: Wed, 7 Aug 2019 16:47:36 +0200
Subject: [R-SIG-Finance] Understanding fixed.pars of rmgarch
Message-ID: <CAKPkyQShQ+qS0rG7azSfdtu+to9UA_EDRQeDWM4dEJYGY_8EJA@mail.gmail.com>

Dear all,
I'm using *rugarch* and *rmgarch *to implement a DCC model over 30
different assets.

While using *ugarchspec *for defining input parameters of univariate GARCH
using a t-Student distribution, I have noticed that there is a parameter
called *fixed.pars*. Using the example given in GitHub, this parameter is
set as following:

fixed.pars = list(shape = 5)

in case of a t-Student distribution. If I change this setting from 5 to 3
(for example) results are very different. Can someone tell me the
importante and the usage of this parameter?

Thanks to all

	[[alternative HTML version deleted]]


From eth@n@b@@m|th @end|ng |rom gm@||@com  Sat Aug 10 18:06:50 2019
From: eth@n@b@@m|th @end|ng |rom gm@||@com (Ethan Smith)
Date: Sat, 10 Aug 2019 10:06:50 -0600
Subject: [R-SIG-Finance] systematic trading w/ quantstrat
Message-ID: <5d4eeb9a.1c69fb81.b2566.ab29@mx.google.com>

Hi All,

I have largely developed my own homegrown? system for trading which is loosely composed of some indicator functions and what I call a scanner. This was great for learning, but has become unwieldly so I?m looking to move to something more structured

I?ve been looking at quantstrat? and it seems to fit closely to hat I need which can roughly be defined by:

Strategy: a named systematic methodology for producing trading revenue composed of:
? Universe criteria: criteria defining for which instruments this strategy is intended to work. (eg: All US equities, SPY constituents, US small caps ex OTC, etc.)
? Trading Rules: Conditions for entering and Exiting trades, based on
o Signals:? conditions defined by the state of one or more 
? Security Indicators: named, quantified time series values for each security in the universe
? Market Indicators: named, quantified time series values for market conditions
o Rebalancing Events: time based events at which positions are adjusted based on portfolio risk

I have gone through most of the quantstrat demos and overview articles I could find, but still have a few questions:
1. Where/how does one implement universe rules? Most of the examples just start with a small list of symbols and I?m not sure how to extrapolate that. I can define liquidity rules with signals/indicators, but other universe criteria don?t seem to have a natural home. I also have a 3rd party time series universe feed that I?d like to feed into this, as well as back-test some ETF constituent models that change over time


	[[alternative HTML version deleted]]


From ||y@@k|pn|@ @end|ng |rom gm@||@com  Sat Aug 10 18:28:46 2019
From: ||y@@k|pn|@ @end|ng |rom gm@||@com (Ilya Kipnis)
Date: Sat, 10 Aug 2019 12:28:46 -0400
Subject: [R-SIG-Finance] systematic trading w/ quantstrat
In-Reply-To: <5d4eeb9a.1c69fb81.b2566.ab29@mx.google.com>
References: <5d4eeb9a.1c69fb81.b2566.ab29@mx.google.com>
Message-ID: <CA+oJuEHJ==roopVRVPLMfT+OQcyc2CmFtoYDE7ZwihUf1-3jPQ@mail.gmail.com>

Quantstrat isn't particularly designed for universal rules. What you should
do, in this case, is to codify the universal rules beforehand (EG rank all
your assets by some sort of fundamental value rank, like top 10% of E/P
ratios as an example), then cbind that data to your OHLC data before
beginning your quantstrat analysis, and simply refer to those columns with
your add.signal code and so on.

On Sat, Aug 10, 2019 at 12:07 PM Ethan Smith <ethan.b.smith at gmail.com>
wrote:

> Hi All,
>
> I have largely developed my own homegrown  system for trading which is
> loosely composed of some indicator functions and what I call a scanner.
> This was great for learning, but has become unwieldly so I?m looking to
> move to something more structured
>
> I?ve been looking at quantstrat  and it seems to fit closely to hat I need
> which can roughly be defined by:
>
> Strategy: a named systematic methodology for producing trading revenue
> composed of:
> ? Universe criteria: criteria defining for which instruments this strategy
> is intended to work. (eg: All US equities, SPY constituents, US small caps
> ex OTC, etc.)
> ? Trading Rules: Conditions for entering and Exiting trades, based on
> o Signals:  conditions defined by the state of one or more
> ? Security Indicators: named, quantified time series values for each
> security in the universe
> ? Market Indicators: named, quantified time series values for market
> conditions
> o Rebalancing Events: time based events at which positions are adjusted
> based on portfolio risk
>
> I have gone through most of the quantstrat demos and overview articles I
> could find, but still have a few questions:
> 1. Where/how does one implement universe rules? Most of the examples just
> start with a small list of symbols and I?m not sure how to extrapolate
> that. I can define liquidity rules with signals/indicators, but other
> universe criteria don?t seem to have a natural home. I also have a 3rd
> party time series universe feed that I?d like to feed into this, as well as
> back-test some ETF constituent models that change over time
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. If you want to post, subscribe first.
> -- Also note that this is not the r-help list where general R questions
> should go.
>

	[[alternative HTML version deleted]]


From tom|err|93 @end|ng |rom gm@||@com  Thu Aug 22 15:32:45 2019
From: tom|err|93 @end|ng |rom gm@||@com (Tommaso Ferrari)
Date: Thu, 22 Aug 2019 15:32:45 +0200
Subject: [R-SIG-Finance] DCC model estimation with t-Student distribution
 rmgarch
Message-ID: <CAKPkyQTuk2p2chB=xmx6he0i2psbgWrBzdm4SugYUdLsXgASRw@mail.gmail.com>

Dear all,
I was analyzing and implementing the DCC model (Dynamic Conditional
Correlation) for the one-day forecast calculation of the
variance-covariance matrix of a system consisting of, approximately,
30 stocks. For each title I consider a historical series of logarithmic
daily returns of 250 samples.
In particular, I was interested in the simulation of this model using the
t-Student distribution.
In this regard I was using the "rugarch" and "rmgarch" packages.
According to the examples found in the literature, I run my analysis in the
following way:

1) specification of the univariate garch model for each stock passing the
number of degrees of freedom of the t-Student distribution as input
(parameter mshape)

2) multifitting of the univariate garch models

3) specification of DCC model with a multivariate t-Student distribution
(degrees of freedom are not passed as input, in this case)

4) fitting of the DCC model

5) variance-covariance matrix forecasting

I give an example of the code I'm running (qxts is the time series data,
mshape is the number of degrees of freedom):

# GARCH(1,1) specification
garch11.spec = ugarchspec(mean.model = list(armaOrder = c(0, 0)),
variance.model = list(garchOrder = c(1, 1), model = "sGARCH"),
distribution.model = "std", fixed.pars = list(shape = mshape))
# replicate Garch(1,1) spec for both time series
uspec = multispec(replicate(ncol(qxts), garch11.spec))
# Fit Garch models: hybrid -> in case of non convergence, all solvers are
used
multf = multifit(uspec, qxts, solver = "hybrid", fit.control = list(scale =
1))
# Dcc model spec
dcc.garch11.spec = dccspec(uspec = uspec, dccOrder = c(1, 1), distribution
= "mvt", model = "DCC")
# Fitting parameters of DCC
dcc.fit = dccfit(dcc.garch11.spec, qxts, fit = multf, fit.control =
list(scale = TRUE))
# dcc.fit = dccfit(dcc.garch11.spec, qxts, fit = multf, fit.control =
list(scale = 1))
# Forecast
dcc.fcst = dccforecast(dcc.fit, n.ahead = 1)
varmat = rcov(dcc.fcst)[[dt_cov]][,, 1]

However, using a number of degrees of freedom of 2.5, I get the following
error:

Error in solve.default(A) : system is computationally singular: reciprocal
condition number = 1.19994e-18

If, for example, I change the degrees of freedom from 2.5 to 2.6, the error
no longer appears.
I would like to know if there is a way to allow the calculation to be
performed even using a number of degrees of freedom equals to 2.5, as I
cannot find any reference in the literature that addresses this problem.

I also tried the following idea.
I don't pass the number of degrees of freedom as input, but I make sure
that, for each title, the number of degrees of freedom is calculated
internally by the function multifit.
Obviously, I get different degrees of freedom depending on the stock
considered.
In this case, however, passing the calculated parameters of the multifit to
the dccfunction, leads to the following error:
'data' must be of a vector type, was 'NULL'
even if the data sample I pass as input contains no null value.

I would like to know if these problems are due solely to the numerical
values of the data passed as input or if I am conceptually wrong in the
implementation of the method.

Thanks to all

	[[alternative HTML version deleted]]


From tonytonov @end|ng |rom gm@||@com  Tue Sep  3 15:25:35 2019
From: tonytonov @end|ng |rom gm@||@com (Anton Antonov)
Date: Tue, 3 Sep 2019 15:25:35 +0200
Subject: [R-SIG-Finance] Endorsement for arxiv q-fin
Message-ID: <015001d5625b$122813a0$36783ae0$@gmail.com>

Dear list,

 

I am looking for a submission endorsement on arxiv.org (q-fin, market microstructure). The paper is titled ?CME Iceberg Order Detection and Prediction? and is a joint work with a colleague of mine. We present an algorithm that operates on CME order flow, detecting hidden liquidity. Next, a predictive model is built, accounting for partially executed and cancelled orders. All reported quantities and estimates were built using R.

Let me know if you can help us with the endorsement (or if you?re interested in the topic), I?ll reach out and share the article and the endorsement URL. Your help (as well as comments or recommendations) is greatly appreciated.

 

Best regards,

Anton Antonov, PhD

Lead Quantitative Analyst at dxFeed Solutions

 


	[[alternative HTML version deleted]]


From vmorozov2006 @end|ng |rom gm@||@com  Fri Sep  6 04:51:46 2019
From: vmorozov2006 @end|ng |rom gm@||@com (Vladimir Morozov)
Date: Fri, 6 Sep 2019 11:51:46 +0900
Subject: [R-SIG-Finance] how to grow XTS series in R dynamically ? And
 Quickly!
Message-ID: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>

Dear xts experts

I use R and XTS to store dynamically growing price time-series for currency
pair rates (e.g. time series of EUR/USD, EUR/JPY, etc growing with each new
incoming market price - say, one per second).

I want to speed up my R function that generates and rbinds a few new rows
to multiple Global XTS objects. This function us called often (sometimes
once per second) and takes a lot of time.

Specifically, I have a function add.newRate.EUR(POSIXct Time, float Rate)
This function performs some time-consuming manipulations, then modifies
(global objects) XTS series1 through XTS seriesN. Specifically, I rbind
some new rows to each of series1 through seriesN in global environment.
(specific example - a new Rate for EUR/USD comes into the function. This
function needs to rbind new point to EUR.USD xts object, but it also needs
to calculate new value for cross-currency pairs, like EUR/JPY, EUR/GPB,
EUR/CHF, EUR/CAD, EUR/AUD, EUR/NZD, and rbind new point to those XTS series
as well)

it all happens quite slowly in R. How do I accelerate it?

To avoid copying XTS series between the function and the global
environment, the function attempts to modify the seriesX in the global
environment, and not return XTS on function return.
The structure of my project is such that my other tasks in R need to access
these XTS series1 through seriesN in Global Environment.
Still, this whole set-up is rather slow.

Do you think using Rcpp to move cross-currency rate calculations to C++
could help?
E.g. I want to write add.newRate.EUR.RCPP(int Time, float Rate) in C++. I
think it will perform internal manipulations of time T and value X a little
faster.

Second idea for speed up:  doing rbind in C++ using package RcppXts.
I plan to use the following function in RcppXts package:
    function("xtsRbind",
             &xtsRbind,
             List::create(Named("x"), Named("y"), Named("dup")),
             "Combine two xts objects row-wise");
Then I want to use Rcpp function assign( name, x ) to assign the created
object back to GlobalEnvironment.

Do you think the above way is a good way to grow dynamical XTS series inside
Rcpp?
Do you think using Rcpp and RcppXts will provide significant acceleration
compared to pure R ?

Thank you
Vlad

	[[alternative HTML version deleted]]


From vmorozov2006 @end|ng |rom gm@||@com  Fri Sep  6 06:05:18 2019
From: vmorozov2006 @end|ng |rom gm@||@com (Vladimir Morozov)
Date: Fri, 6 Sep 2019 13:05:18 +0900
Subject: [R-SIG-Finance] how to grow XTS series in R dynamically ? And
 Quickly!
In-Reply-To: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>
References: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>
Message-ID: <CAHqROV5O2xL0cAqOmnxnGSSs2=mjsbHyOHPittuPxwZPc4n0YA@mail.gmail.com>

did some googling on my question
general wisdom on the internet seems to be - it's SLOW to grow R lists in a
loop. Must Pre-allocate First.

Question: is there a feature in XTS that allows to pre-allocate space for
growing XTS series?
thanks again!

On Fri, Sep 6, 2019 at 11:51 AM Vladimir Morozov <vmorozov2006 at gmail.com>
wrote:

> Dear xts experts
>
> I use R and XTS to store dynamically growing price time-series for
> currency pair rates (e.g. time series of EUR/USD, EUR/JPY, etc growing with
> each new incoming market price - say, one per second).
>
> I want to speed up my R function that generates and rbinds a few new rows
> to multiple Global XTS objects. This function us called often (sometimes
> once per second) and takes a lot of time.
>
> Specifically, I have a function add.newRate.EUR(POSIXct Time, float Rate)
> This function performs some time-consuming manipulations, then modifies
> (global objects) XTS series1 through XTS seriesN. Specifically, I rbind
> some new rows to each of series1 through seriesN in global environment.
> (specific example - a new Rate for EUR/USD comes into the function. This
> function needs to rbind new point to EUR.USD xts object, but it also needs
> to calculate new value for cross-currency pairs, like EUR/JPY, EUR/GPB,
> EUR/CHF, EUR/CAD, EUR/AUD, EUR/NZD, and rbind new point to those XTS series
> as well)
>
> it all happens quite slowly in R. How do I accelerate it?
>
> To avoid copying XTS series between the function and the global
> environment, the function attempts to modify the seriesX in the global
> environment, and not return XTS on function return.
> The structure of my project is such that my other tasks in R need to
> access these XTS series1 through seriesN in Global Environment.
> Still, this whole set-up is rather slow.
>
> Do you think using Rcpp to move cross-currency rate calculations to C++
> could help?
> E.g. I want to write add.newRate.EUR.RCPP(int Time, float Rate) in C++. I
> think it will perform internal manipulations of time T and value X a little
> faster.
>
> Second idea for speed up:  doing rbind in C++ using package RcppXts.
> I plan to use the following function in RcppXts package:
>     function("xtsRbind",
>              &xtsRbind,
>              List::create(Named("x"), Named("y"), Named("dup")),
>              "Combine two xts objects row-wise");
> Then I want to use Rcpp function assign( name, x ) to assign the created
> object back to GlobalEnvironment.
>
> Do you think the above way is a good way to grow dynamical XTS series inside
> Rcpp?
> Do you think using Rcpp and RcppXts will provide significant acceleration
> compared to pure R ?
>
> Thank you
> Vlad
>
>
>

	[[alternative HTML version deleted]]


From m@rk|eed@2 @end|ng |rom gm@||@com  Fri Sep  6 14:32:58 2019
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Fri, 6 Sep 2019 08:32:58 -0400
Subject: [R-SIG-Finance] how to grow XTS series in R dynamically ? And
 Quickly!
In-Reply-To: <CAHqROV5O2xL0cAqOmnxnGSSs2=mjsbHyOHPittuPxwZPc4n0YA@mail.gmail.com>
References: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>
 <CAHqROV5O2xL0cAqOmnxnGSSs2=mjsbHyOHPittuPxwZPc4n0YA@mail.gmail.com>
Message-ID: <CAHz+bWYSuRA6uLCfoDuidfeY4c-A2CAY6FnwtO=VXisC3f53KA@mail.gmail.com>

Hi Vladimir: I can tell that  you tried to explain your problem clearly but
you'd get more
replies if you actually showed the R code that you wrote. Minimal,
reproducible examples
get a lot more responses.  Good luck.
                                                                      Mark

On Fri, Sep 6, 2019 at 12:05 AM Vladimir Morozov <vmorozov2006 at gmail.com>
wrote:

> did some googling on my question
> general wisdom on the internet seems to be - it's SLOW to grow R lists in a
> loop. Must Pre-allocate First.
>
> Question: is there a feature in XTS that allows to pre-allocate space for
> growing XTS series?
> thanks again!
>
> On Fri, Sep 6, 2019 at 11:51 AM Vladimir Morozov <vmorozov2006 at gmail.com>
> wrote:
>
> > Dear xts experts
> >
> > I use R and XTS to store dynamically growing price time-series for
> > currency pair rates (e.g. time series of EUR/USD, EUR/JPY, etc growing
> with
> > each new incoming market price - say, one per second).
> >
> > I want to speed up my R function that generates and rbinds a few new rows
> > to multiple Global XTS objects. This function us called often (sometimes
> > once per second) and takes a lot of time.
> >
> > Specifically, I have a function add.newRate.EUR(POSIXct Time, float Rate)
> > This function performs some time-consuming manipulations, then modifies
> > (global objects) XTS series1 through XTS seriesN. Specifically, I rbind
> > some new rows to each of series1 through seriesN in global environment.
> > (specific example - a new Rate for EUR/USD comes into the function. This
> > function needs to rbind new point to EUR.USD xts object, but it also
> needs
> > to calculate new value for cross-currency pairs, like EUR/JPY, EUR/GPB,
> > EUR/CHF, EUR/CAD, EUR/AUD, EUR/NZD, and rbind new point to those XTS
> series
> > as well)
> >
> > it all happens quite slowly in R. How do I accelerate it?
> >
> > To avoid copying XTS series between the function and the global
> > environment, the function attempts to modify the seriesX in the global
> > environment, and not return XTS on function return.
> > The structure of my project is such that my other tasks in R need to
> > access these XTS series1 through seriesN in Global Environment.
> > Still, this whole set-up is rather slow.
> >
> > Do you think using Rcpp to move cross-currency rate calculations to C++
> > could help?
> > E.g. I want to write add.newRate.EUR.RCPP(int Time, float Rate) in C++. I
> > think it will perform internal manipulations of time T and value X a
> little
> > faster.
> >
> > Second idea for speed up:  doing rbind in C++ using package RcppXts.
> > I plan to use the following function in RcppXts package:
> >     function("xtsRbind",
> >              &xtsRbind,
> >              List::create(Named("x"), Named("y"), Named("dup")),
> >              "Combine two xts objects row-wise");
> > Then I want to use Rcpp function assign( name, x ) to assign the created
> > object back to GlobalEnvironment.
> >
> > Do you think the above way is a good way to grow dynamical XTS series
> inside
> > Rcpp?
> > Do you think using Rcpp and RcppXts will provide significant acceleration
> > compared to pure R ?
> >
> > Thank you
> > Vlad
> >
> >
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. If you want to post, subscribe first.
> -- Also note that this is not the r-help list where general R questions
> should go.
>

	[[alternative HTML version deleted]]


From d@n|e|@ceg|e|k@ @end|ng |rom gm@||@com  Fri Sep  6 16:10:27 2019
From: d@n|e|@ceg|e|k@ @end|ng |rom gm@||@com (=?utf-8?Q?Daniel_Cegie=C5=82ka?=)
Date: Fri, 6 Sep 2019 16:10:27 +0200
Subject: [R-SIG-Finance] how to grow XTS series in R dynamically ? And
 Quickly!
In-Reply-To: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>
References: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>
Message-ID: <5C0F4F0F-6CC5-4A48-A741-56E7C2535BA4@gmail.com>



> Wiadomo?? napisana przez Vladimir Morozov <vmorozov2006 at gmail.com> w dniu 06.09.2019, o godz. 04:51:
> 
> Dear xts experts
> 
> I use R and XTS to store dynamically growing price time-series for currency
> pair rates (e.g. time series of EUR/USD, EUR/JPY, etc growing with each new
> incoming market price - say, one per second).
> 
> 

(?)

> it all happens quite slowly in R. How do I accelerate it?


1) forgotten xts acceleration:

https://github.com/joshuaulrich/xts/blob/master/src/rbind.c#L540 <https://github.com/joshuaulrich/xts/blob/master/src/rbind.c#L540>

to activate it add the -DRBIND_APPEND flag to Makevars (linux/unix/macos) or Makevars.win (Windows), and recompile xts sources.



2) preallocation

preallocate_matrix <- function(n)
{
    x <- matrix()
    length(x) <- 4 * n      # bid, ask, bid_size, ask_size
    dim(x) <- c(n, 4)       # see: ?dim
    return(x)
}

> x <- preallocate_matrix(5)
> x
     [,1] [,2] [,3] [,4]
[1,]   NA   NA   NA   NA
[2,]   NA   NA   NA   NA
[3,]   NA   NA   NA   NA
[4,]   NA   NA   NA   NA
[5,]   NA   NA   NA   NA

Now you need xts index, eg:

i <- Sys.time() + 1:5
> i
[1] "2019-09-06 15:47:48 CEST" "2019-09-06 15:47:49 CEST" "2019-09-06 15:47:50 CEST" "2019-09-06 15:47:51 CEST"
[5] "2019-09-06 15:47:52 CEST?


Our xts object:

> x <- .xts(x, index = i)
                    [,1] [,2] [,3] [,4]
2019-09-06 15:47:48   NA   NA   NA   NA
2019-09-06 15:47:49   NA   NA   NA   NA
2019-09-06 15:47:50   NA   NA   NA   NA
2019-09-06 15:47:51   NA   NA   NA   NA
2019-09-06 15:47:52   NA   NA   NA   NA


And now you can use this xts object without memory copying:


> x[1,] <- c(1, 2, 3, 4)
> x
                    [,1] [,2] [,3] [,4]
2019-09-06 15:47:48    1    2    3    4
2019-09-06 15:47:49   NA   NA   NA   NA
2019-09-06 15:47:50   NA   NA   NA   NA
2019-09-06 15:47:51   NA   NA   NA   NA
2019-09-06 15:47:52   NA   NA   NA   NA

> x[2,] <- c(11, 22, 33, 44)
> x
                    [,1] [,2] [,3] [,4]
2019-09-06 15:47:48    1    2    3    4
2019-09-06 15:47:49   11   22   33   44
2019-09-06 15:47:50   NA   NA   NA   NA
2019-09-06 15:47:51   NA   NA   NA   NA
2019-09-06 15:47:52   NA   NA   NA   NA


etc.

That's what you meant.

Best,
Daniel


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20190906/115ffec8/attachment.html>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: Message signed with OpenPGP
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20190906/115ffec8/attachment.sig>

From d@n|e|@ceg|e|k@ @end|ng |rom gm@||@com  Fri Sep  6 17:08:39 2019
From: d@n|e|@ceg|e|k@ @end|ng |rom gm@||@com (=?utf-8?Q?Daniel_Cegie=C5=82ka?=)
Date: Fri, 6 Sep 2019 17:08:39 +0200
Subject: [R-SIG-Finance] how to grow XTS series in R dynamically ? And
 Quickly!
In-Reply-To: <5C0F4F0F-6CC5-4A48-A741-56E7C2535BA4@gmail.com>
References: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>
 <5C0F4F0F-6CC5-4A48-A741-56E7C2535BA4@gmail.com>
Message-ID: <A02F1950-21D1-4D9E-AF4E-218361CB37B0@gmail.com>



> Wiadomo?? napisana przez Daniel Cegie?ka <daniel.cegielka at gmail.com> w dniu 06.09.2019, o godz. 16:10:
> 

> 
> 2) preallocation
> 
> preallocate_matrix <- function(n)
> {
>     x <- matrix()
>     length(x) <- 4 * n      # bid, ask, bid_size, ask_size
>     dim(x) <- c(n, 4)       # see: ?dim
>     return(x)
> }
> 
> > x <- preallocate_matrix(5)
> > x
>      [,1] [,2] [,3] [,4]
> [1,]   NA   NA   NA   NA
> [2,]   NA   NA   NA   NA
> [3,]   NA   NA   NA   NA
> [4,]   NA   NA   NA   NA
> [5,]   NA   NA   NA   NA

?matrix

Usage
matrix(data = NA, nrow = 1, ncol = 1, byrow = FALSE,
       dimnames = NULL)

so we don't even need preallocate_matrix() function

> x <- .xts(matrix(nrow = 5, ncol = 4), index = Sys.time() + 1:5)
> x
                    [,1] [,2] [,3] [,4]
2019-09-06 17:07:27   NA   NA   NA   NA
2019-09-06 17:07:28   NA   NA   NA   NA
2019-09-06 17:07:29   NA   NA   NA   NA
2019-09-06 17:07:30   NA   NA   NA   NA
2019-09-06 17:07:31   NA   NA   NA   NA




-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: Message signed with OpenPGP
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20190906/4ebbe7ce/attachment.sig>

From vmorozov2006 @end|ng |rom gm@||@com  Fri Sep  6 20:04:51 2019
From: vmorozov2006 @end|ng |rom gm@||@com (Vladimir Morozov)
Date: Sat, 7 Sep 2019 03:04:51 +0900
Subject: [R-SIG-Finance] how to grow XTS series in R dynamically ? And
 Quickly!
In-Reply-To: <A02F1950-21D1-4D9E-AF4E-218361CB37B0@gmail.com>
References: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>
 <5C0F4F0F-6CC5-4A48-A741-56E7C2535BA4@gmail.com>
 <A02F1950-21D1-4D9E-AF4E-218361CB37B0@gmail.com>
Message-ID: <CAHqROV6+A-XW=O0e_UoSDcKSUH6kD-A_zfc5TgzupvyXgDmq4w@mail.gmail.com>

Hi Daniel
Thanks a lot.
Those are very helpful ideas.

rbind_append --> it still has to allocate memory for the resulting
series... so if memory allocation was the main reason for slow performance,
maybe rbind_append doesn't change much? what do you think?

preallocating regular-interval time-series is a good idea.
however financial data are irregularly spaced (sometimes there may not be
any price updates for a few secs, even more).
even if we postulate that prices are allowed to change no more than once
per second, there's a lot of uses for the frequency of price updates, not
only the values of the prices (the simplest assumption is the poisson
arrival process for the updates, but there are many fancier, more powerful
models...)
so, pre-allocating a regularly spaced 1-sec interval xts series dumbs down
many things!

i wish i could pre-allocate the vector for the values and maybe indices,
but then do the assignment of the sort:
(say, in C++ i would have a method)
    price.set_next_point(time, value);

thanks!

On Sat, Sep 7, 2019 at 12:08 AM Daniel Cegie?ka <daniel.cegielka at gmail.com>
wrote:

>
>
> > Wiadomo?? napisana przez Daniel Cegie?ka <daniel.cegielka at gmail.com> w
> dniu 06.09.2019, o godz. 16:10:
> >
>
> >
> > 2) preallocation
> >
> > preallocate_matrix <- function(n)
> > {
> >     x <- matrix()
> >     length(x) <- 4 * n      # bid, ask, bid_size, ask_size
> >     dim(x) <- c(n, 4)       # see: ?dim
> >     return(x)
> > }
> >
> > > x <- preallocate_matrix(5)
> > > x
> >      [,1] [,2] [,3] [,4]
> > [1,]   NA   NA   NA   NA
> > [2,]   NA   NA   NA   NA
> > [3,]   NA   NA   NA   NA
> > [4,]   NA   NA   NA   NA
> > [5,]   NA   NA   NA   NA
>
> ?matrix
>
> Usage
> matrix(data = NA, nrow = 1, ncol = 1, byrow = FALSE,
>        dimnames = NULL)
>
> so we don't even need preallocate_matrix() function
>
> > x <- .xts(matrix(nrow = 5, ncol = 4), index = Sys.time() + 1:5)
> > x
>                     [,1] [,2] [,3] [,4]
> 2019-09-06 17:07:27   NA   NA   NA   NA
> 2019-09-06 17:07:28   NA   NA   NA   NA
> 2019-09-06 17:07:29   NA   NA   NA   NA
> 2019-09-06 17:07:30   NA   NA   NA   NA
> 2019-09-06 17:07:31   NA   NA   NA   NA
>
>
>
>

	[[alternative HTML version deleted]]


From ||y@@k|pn|@ @end|ng |rom gm@||@com  Fri Sep  6 20:47:39 2019
From: ||y@@k|pn|@ @end|ng |rom gm@||@com (Ilya Kipnis)
Date: Fri, 6 Sep 2019 14:47:39 -0400
Subject: [R-SIG-Finance] how to grow XTS series in R dynamically ? And
 Quickly!
In-Reply-To: <CAHqROV6+A-XW=O0e_UoSDcKSUH6kD-A_zfc5TgzupvyXgDmq4w@mail.gmail.com>
References: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>
 <5C0F4F0F-6CC5-4A48-A741-56E7C2535BA4@gmail.com>
 <A02F1950-21D1-4D9E-AF4E-218361CB37B0@gmail.com>
 <CAHqROV6+A-XW=O0e_UoSDcKSUH6kD-A_zfc5TgzupvyXgDmq4w@mail.gmail.com>
Message-ID: <CA+oJuEFDMBNE4wcKx2LJrTt6ZLvRUPBOK+ACQsA=5VS+sO9q2g@mail.gmail.com>

Vladimir,

Question--do you have to do operations on the newly updated xts each time
you append it? For instance, when I backtest strategies and I don't know
how long the output would be, I start off by allocating an empty list,
keep updating the list, and rbind it at the end.

EG:

results <- list()
for(i in 1:n){
  #output <- do something
  results[[i]] <-output
}
results <- do.call(rbind, results)

This might help, as this doesn't keep copying the larger list over and over
again, which I remember doing before I learned about this method.

Hope this helps.

-Ilya

On Fri, Sep 6, 2019 at 2:05 PM Vladimir Morozov <vmorozov2006 at gmail.com>
wrote:

> Hi Daniel
> Thanks a lot.
> Those are very helpful ideas.
>
> rbind_append --> it still has to allocate memory for the resulting
> series... so if memory allocation was the main reason for slow performance,
> maybe rbind_append doesn't change much? what do you think?
>
> preallocating regular-interval time-series is a good idea.
> however financial data are irregularly spaced (sometimes there may not be
> any price updates for a few secs, even more).
> even if we postulate that prices are allowed to change no more than once
> per second, there's a lot of uses for the frequency of price updates, not
> only the values of the prices (the simplest assumption is the poisson
> arrival process for the updates, but there are many fancier, more powerful
> models...)
> so, pre-allocating a regularly spaced 1-sec interval xts series dumbs down
> many things!
>
> i wish i could pre-allocate the vector for the values and maybe indices,
> but then do the assignment of the sort:
> (say, in C++ i would have a method)
>     price.set_next_point(time, value);
>
> thanks!
>
> On Sat, Sep 7, 2019 at 12:08 AM Daniel Cegie?ka <daniel.cegielka at gmail.com
> >
> wrote:
>
> >
> >
> > > Wiadomo?? napisana przez Daniel Cegie?ka <daniel.cegielka at gmail.com> w
> > dniu 06.09.2019, o godz. 16:10:
> > >
> >
> > >
> > > 2) preallocation
> > >
> > > preallocate_matrix <- function(n)
> > > {
> > >     x <- matrix()
> > >     length(x) <- 4 * n      # bid, ask, bid_size, ask_size
> > >     dim(x) <- c(n, 4)       # see: ?dim
> > >     return(x)
> > > }
> > >
> > > > x <- preallocate_matrix(5)
> > > > x
> > >      [,1] [,2] [,3] [,4]
> > > [1,]   NA   NA   NA   NA
> > > [2,]   NA   NA   NA   NA
> > > [3,]   NA   NA   NA   NA
> > > [4,]   NA   NA   NA   NA
> > > [5,]   NA   NA   NA   NA
> >
> > ?matrix
> >
> > Usage
> > matrix(data = NA, nrow = 1, ncol = 1, byrow = FALSE,
> >        dimnames = NULL)
> >
> > so we don't even need preallocate_matrix() function
> >
> > > x <- .xts(matrix(nrow = 5, ncol = 4), index = Sys.time() + 1:5)
> > > x
> >                     [,1] [,2] [,3] [,4]
> > 2019-09-06 17:07:27   NA   NA   NA   NA
> > 2019-09-06 17:07:28   NA   NA   NA   NA
> > 2019-09-06 17:07:29   NA   NA   NA   NA
> > 2019-09-06 17:07:30   NA   NA   NA   NA
> > 2019-09-06 17:07:31   NA   NA   NA   NA
> >
> >
> >
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. If you want to post, subscribe first.
> -- Also note that this is not the r-help list where general R questions
> should go.
>

	[[alternative HTML version deleted]]


From d@n|e|@ceg|e|k@ @end|ng |rom gm@||@com  Fri Sep  6 21:07:47 2019
From: d@n|e|@ceg|e|k@ @end|ng |rom gm@||@com (=?utf-8?Q?Daniel_Cegie=C5=82ka?=)
Date: Fri, 6 Sep 2019 21:07:47 +0200
Subject: [R-SIG-Finance] how to grow XTS series in R dynamically ? And
 Quickly!
In-Reply-To: <CAHqROV6+A-XW=O0e_UoSDcKSUH6kD-A_zfc5TgzupvyXgDmq4w@mail.gmail.com>
References: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>
 <5C0F4F0F-6CC5-4A48-A741-56E7C2535BA4@gmail.com>
 <A02F1950-21D1-4D9E-AF4E-218361CB37B0@gmail.com>
 <CAHqROV6+A-XW=O0e_UoSDcKSUH6kD-A_zfc5TgzupvyXgDmq4w@mail.gmail.com>
Message-ID: <40BBF89D-B672-4A06-8F5B-C0FE8FFC455D@gmail.com>



> Wiadomo?? napisana przez Vladimir Morozov <vmorozov2006 at gmail.com> w dniu 06.09.2019, o godz. 20:04:
> 
> Hi Daniel
> Thanks a lot.
> Those are very helpful ideas.
> 
> rbind_append --> it still has to allocate memory for the resulting series... so if memory allocation was the main reason for slow performance, maybe rbind_append doesn't change much? what do you think?
> 
> preallocating regular-interval time-series is a good idea.
> however financial data are irregularly spaced (sometimes there may not be any price updates for a few secs, even more).
> even if we postulate that prices are allowed to change no more than once per second, there's a lot of uses for the frequency of price updates, not only the values of the prices (the simplest assumption is the poisson arrival process for the updates, but there are many fancier, more powerful models...)
> so, pre-allocating a regularly spaced 1-sec interval xts series dumbs down many things!
> 

let's start with what exactly do you want to do? Do you want to collect market data and save it to disk? Or maybe you want to have a real-time strategy? These are two different problems and require distinct solutions.

1) market data storage - why do you want to use R here? Isn't it better to dump the memory using mmap syscall and then import it into the database or R?

2) real-time market strategy in R - in this case your lookback is limited. So if you add new data point, you can also discard/drop the oldest. In this way, your memory usage will remain at the same low level. If this solution suits you, then you can write a fast function in C here that would operate on the xts object.

There is no such thing as matrix in R - this is a multidimensional vector. Let's say we have classic OHLC data for xts object:

O H L C
O H L C
O H L C
O H L C
O H L C


In the memory of the data looks like one long vector.

x: OOOOOHHHHHLLLLLCCCCC

You can be clever here and use memcpy():

memcpy(&xp + 1, &xp, (nrows(x) - 1)) * sizeof(double));  // or int - use: switch((TYPEOF(x))

memcpy(&index_p + 1, &index_p, (nrows(x) - 1)) * sizeof(double)); // or int for Date() type

This will move the memory so that the oldest value will be overwritten:

    1 2 3 4 5   1 2 3 4 5   1 2 3 4 5   1 2 3 4 5
x: OOOOH   HHHHL      LLLLC      CCCC N

Then you can add a new index and value.

You will have preallocated memory at all times and you will use memory copy as little as possible. And the most important: you'll be operating on the xts object all time, so your code in R will be very fast :)

It is advanced solutions - you need to understand not only how R's internals works, but also have a good C skills. If you want to use R for real-time trading, it's worth learn these things.

Daniel





> i wish i could pre-allocate the vector for the values and maybe indices, but then do the assignment of the sort:
> (say, in C++ i would have a method)
>     price.set_next_point(time, value);
> 
> thanks!
> 
> On Sat, Sep 7, 2019 at 12:08 AM Daniel Cegie?ka <daniel.cegielka at gmail.com <mailto:daniel.cegielka at gmail.com>> wrote:
> 
> 
> > Wiadomo?? napisana przez Daniel Cegie?ka <daniel.cegielka at gmail.com <mailto:daniel.cegielka at gmail.com>> w dniu 06.09.2019, o godz. 16:10:
> >
> 
> >
> > 2) preallocation
> >
> > preallocate_matrix <- function(n)
> > {
> >     x <- matrix()
> >     length(x) <- 4 * n      # bid, ask, bid_size, ask_size
> >     dim(x) <- c(n, 4)       # see: ?dim
> >     return(x)
> > }
> >
> > > x <- preallocate_matrix(5)
> > > x
> >      [,1] [,2] [,3] [,4]
> > [1,]   NA   NA   NA   NA
> > [2,]   NA   NA   NA   NA
> > [3,]   NA   NA   NA   NA
> > [4,]   NA   NA   NA   NA
> > [5,]   NA   NA   NA   NA
> 
> ?matrix
> 
> Usage
> matrix(data = NA, nrow = 1, ncol = 1, byrow = FALSE,
>        dimnames = NULL)
> 
> so we don't even need preallocate_matrix() function
> 
> > x <- .xts(matrix(nrow = 5, ncol = 4), index = Sys.time() + 1:5)
> > x
>                     [,1] [,2] [,3] [,4]
> 2019-09-06 17:07:27   NA   NA   NA   NA
> 2019-09-06 17:07:28   NA   NA   NA   NA
> 2019-09-06 17:07:29   NA   NA   NA   NA
> 2019-09-06 17:07:30   NA   NA   NA   NA
> 2019-09-06 17:07:31   NA   NA   NA   NA
> 
> 
> 


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20190906/c8bcda47/attachment.html>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: Message signed with OpenPGP
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20190906/c8bcda47/attachment.sig>

From vmorozov2006 @end|ng |rom gm@||@com  Sat Sep  7 08:14:07 2019
From: vmorozov2006 @end|ng |rom gm@||@com (Vladimir Morozov)
Date: Sat, 7 Sep 2019 15:14:07 +0900
Subject: [R-SIG-Finance] how to grow XTS series in R dynamically ? And
 Quickly!
In-Reply-To: <40BBF89D-B672-4A06-8F5B-C0FE8FFC455D@gmail.com>
References: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>
 <5C0F4F0F-6CC5-4A48-A741-56E7C2535BA4@gmail.com>
 <A02F1950-21D1-4D9E-AF4E-218361CB37B0@gmail.com>
 <CAHqROV6+A-XW=O0e_UoSDcKSUH6kD-A_zfc5TgzupvyXgDmq4w@mail.gmail.com>
 <40BBF89D-B672-4A06-8F5B-C0FE8FFC455D@gmail.com>
Message-ID: <CAHqROV5XjmCP4W6qzoSCs-gh16620uMY9rJzNVBjCpG17tPLPw@mail.gmail.com>

Daniel
It's a brilliant idea
I'll try it out
Yes, it's quite advanced

On Sat, Sep 7, 2019, 4:07 AM Daniel Cegie?ka <daniel.cegielka at gmail.com>
wrote:

>
>
> Wiadomo?? napisana przez Vladimir Morozov <vmorozov2006 at gmail.com> w dniu
> 06.09.2019, o godz. 20:04:
>
> Hi Daniel
> Thanks a lot.
> Those are very helpful ideas.
>
> rbind_append --> it still has to allocate memory for the resulting
> series... so if memory allocation was the main reason for slow performance,
> maybe rbind_append doesn't change much? what do you think?
>
> preallocating regular-interval time-series is a good idea.
> however financial data are irregularly spaced (sometimes there may not be
> any price updates for a few secs, even more).
> even if we postulate that prices are allowed to change no more than once
> per second, there's a lot of uses for the frequency of price updates, not
> only the values of the prices (the simplest assumption is the poisson
> arrival process for the updates, but there are many fancier, more powerful
> models...)
> so, pre-allocating a regularly spaced 1-sec interval xts series dumbs down
> many things!
>
>
> let's start with what exactly do you want to do? Do you want to collect
> market data and save it to disk? Or maybe you want to have a real-time
> strategy? These are two different problems and require distinct solutions.
>
> 1) market data storage - why do you want to use R here? Isn't it better to
> dump the memory using mmap syscall and then import it into the database or
> R?
>
> 2) real-time market strategy in R - in this case your lookback is limited.
> So if you add new data point, you can also discard/drop the oldest. In this
> way, your memory usage will remain at the same low level. If this solution
> suits you, then you can write a fast function in C here that would operate
> on the xts object.
>
> There is no such thing as matrix in R - this is a multidimensional vector.
> Let's say we have classic OHLC data for xts object:
>
> O H L C
> O H L C
> O H L C
> O H L C
> O H L C
>
>
> In the memory of the data looks like one long vector.
>
> x: OOOOOHHHHHLLLLLCCCCC
>
> You can be clever here and use memcpy():
>
> memcpy(&xp + 1, &xp, (nrows(x) - 1)) * sizeof(double));  // or int - use:
> switch((TYPEOF(x))
>
> memcpy(&index_p + 1, &index_p, (nrows(x) - 1)) * sizeof(double)); // or
> int for Date() type
>
> This will move the memory so that the oldest value will be overwritten:
>
>     1 2 3 4 5   1 2 3 4 5   1 2 3 4 5   1 2 3 4 5
> x: OOOOH   HHHHL      LLLLC      CCCC N
>
> Then you can add a new index and value.
>
> You will have preallocated memory at all times and you will use memory
> copy as little as possible. And the most important: you'll be operating on
> the xts object all time, so your code in R will be very fast :)
>
> It is advanced solutions - you need to understand not only how R's
> internals works, but also have a good C skills. If you want to use R for
> real-time trading, it's worth learn these things.
>
> Daniel
>
>
>
>
>
> i wish i could pre-allocate the vector for the values and maybe indices,
> but then do the assignment of the sort:
> (say, in C++ i would have a method)
>     price.set_next_point(time, value);
>
> thanks!
>
> On Sat, Sep 7, 2019 at 12:08 AM Daniel Cegie?ka <daniel.cegielka at gmail.com>
> wrote:
>
>>
>>
>> > Wiadomo?? napisana przez Daniel Cegie?ka <daniel.cegielka at gmail.com> w
>> dniu 06.09.2019, o godz. 16:10:
>> >
>>
>> >
>> > 2) preallocation
>> >
>> > preallocate_matrix <- function(n)
>> > {
>> >     x <- matrix()
>> >     length(x) <- 4 * n      # bid, ask, bid_size, ask_size
>> >     dim(x) <- c(n, 4)       # see: ?dim
>> >     return(x)
>> > }
>> >
>> > > x <- preallocate_matrix(5)
>> > > x
>> >      [,1] [,2] [,3] [,4]
>> > [1,]   NA   NA   NA   NA
>> > [2,]   NA   NA   NA   NA
>> > [3,]   NA   NA   NA   NA
>> > [4,]   NA   NA   NA   NA
>> > [5,]   NA   NA   NA   NA
>>
>> ?matrix
>>
>> Usage
>> matrix(data = NA, nrow = 1, ncol = 1, byrow = FALSE,
>>        dimnames = NULL)
>>
>> so we don't even need preallocate_matrix() function
>>
>> > x <- .xts(matrix(nrow = 5, ncol = 4), index = Sys.time() + 1:5)
>> > x
>>                     [,1] [,2] [,3] [,4]
>> 2019-09-06 17:07:27   NA   NA   NA   NA
>> 2019-09-06 17:07:28   NA   NA   NA   NA
>> 2019-09-06 17:07:29   NA   NA   NA   NA
>> 2019-09-06 17:07:30   NA   NA   NA   NA
>> 2019-09-06 17:07:31   NA   NA   NA   NA
>>
>>
>>
>>
>

	[[alternative HTML version deleted]]


From vmorozov2006 @end|ng |rom gm@||@com  Sat Sep  7 08:15:05 2019
From: vmorozov2006 @end|ng |rom gm@||@com (Vladimir Morozov)
Date: Sat, 7 Sep 2019 15:15:05 +0900
Subject: [R-SIG-Finance] how to grow XTS series in R dynamically ? And
 Quickly!
In-Reply-To: <CA+oJuEFDMBNE4wcKx2LJrTt6ZLvRUPBOK+ACQsA=5VS+sO9q2g@mail.gmail.com>
References: <CAHqROV5Wv-_x_ZEUr1oK78-jQ9qa36u5TPfcWifFKr+=CozTTg@mail.gmail.com>
 <5C0F4F0F-6CC5-4A48-A741-56E7C2535BA4@gmail.com>
 <A02F1950-21D1-4D9E-AF4E-218361CB37B0@gmail.com>
 <CAHqROV6+A-XW=O0e_UoSDcKSUH6kD-A_zfc5TgzupvyXgDmq4w@mail.gmail.com>
 <CA+oJuEFDMBNE4wcKx2LJrTt6ZLvRUPBOK+ACQsA=5VS+sO9q2g@mail.gmail.com>
Message-ID: <CAHqROV6NWKPxcvLKbHAEhsjAV4o1o2fFwKeFAWLN968bWtXyVA@mail.gmail.com>

Thanks, Ilya

On Sat, Sep 7, 2019, 3:47 AM Ilya Kipnis <ilya.kipnis at gmail.com> wrote:

> Vladimir,
>
> Question--do you have to do operations on the newly updated xts each time
> you append it? For instance, when I backtest strategies and I don't know
> how long the output would be, I start off by allocating an empty list,
> keep updating the list, and rbind it at the end.
>
> EG:
>
> results <- list()
> for(i in 1:n){
>   #output <- do something
>   results[[i]] <-output
> }
> results <- do.call(rbind, results)
>
> This might help, as this doesn't keep copying the larger list over and
> over again, which I remember doing before I learned about this method.
>
> Hope this helps.
>
> -Ilya
>
> On Fri, Sep 6, 2019 at 2:05 PM Vladimir Morozov <vmorozov2006 at gmail.com>
> wrote:
>
>> Hi Daniel
>> Thanks a lot.
>> Those are very helpful ideas.
>>
>> rbind_append --> it still has to allocate memory for the resulting
>> series... so if memory allocation was the main reason for slow
>> performance,
>> maybe rbind_append doesn't change much? what do you think?
>>
>> preallocating regular-interval time-series is a good idea.
>> however financial data are irregularly spaced (sometimes there may not be
>> any price updates for a few secs, even more).
>> even if we postulate that prices are allowed to change no more than once
>> per second, there's a lot of uses for the frequency of price updates, not
>> only the values of the prices (the simplest assumption is the poisson
>> arrival process for the updates, but there are many fancier, more powerful
>> models...)
>> so, pre-allocating a regularly spaced 1-sec interval xts series dumbs down
>> many things!
>>
>> i wish i could pre-allocate the vector for the values and maybe indices,
>> but then do the assignment of the sort:
>> (say, in C++ i would have a method)
>>     price.set_next_point(time, value);
>>
>> thanks!
>>
>> On Sat, Sep 7, 2019 at 12:08 AM Daniel Cegie?ka <
>> daniel.cegielka at gmail.com>
>> wrote:
>>
>> >
>> >
>> > > Wiadomo?? napisana przez Daniel Cegie?ka <daniel.cegielka at gmail.com>
>> w
>> > dniu 06.09.2019, o godz. 16:10:
>> > >
>> >
>> > >
>> > > 2) preallocation
>> > >
>> > > preallocate_matrix <- function(n)
>> > > {
>> > >     x <- matrix()
>> > >     length(x) <- 4 * n      # bid, ask, bid_size, ask_size
>> > >     dim(x) <- c(n, 4)       # see: ?dim
>> > >     return(x)
>> > > }
>> > >
>> > > > x <- preallocate_matrix(5)
>> > > > x
>> > >      [,1] [,2] [,3] [,4]
>> > > [1,]   NA   NA   NA   NA
>> > > [2,]   NA   NA   NA   NA
>> > > [3,]   NA   NA   NA   NA
>> > > [4,]   NA   NA   NA   NA
>> > > [5,]   NA   NA   NA   NA
>> >
>> > ?matrix
>> >
>> > Usage
>> > matrix(data = NA, nrow = 1, ncol = 1, byrow = FALSE,
>> >        dimnames = NULL)
>> >
>> > so we don't even need preallocate_matrix() function
>> >
>> > > x <- .xts(matrix(nrow = 5, ncol = 4), index = Sys.time() + 1:5)
>> > > x
>> >                     [,1] [,2] [,3] [,4]
>> > 2019-09-06 17:07:27   NA   NA   NA   NA
>> > 2019-09-06 17:07:28   NA   NA   NA   NA
>> > 2019-09-06 17:07:29   NA   NA   NA   NA
>> > 2019-09-06 17:07:30   NA   NA   NA   NA
>> > 2019-09-06 17:07:31   NA   NA   NA   NA
>> >
>> >
>> >
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only. If you want to post, subscribe first.
>> -- Also note that this is not the r-help list where general R questions
>> should go.
>>
>

	[[alternative HTML version deleted]]


From ne||@|@wton2 @end|ng |rom m@||@dcu@|e  Thu Sep 12 01:54:53 2019
From: ne||@|@wton2 @end|ng |rom m@||@dcu@|e (Neil Patrick Lawton)
Date: Thu, 12 Sep 2019 00:54:53 +0100
Subject: [R-SIG-Finance] External Regressors in GARCH Equation when using
 Twinkle
Message-ID: <CALZLRahR8mOy4_hEuaSR4ALbfF05B3zgRh3KV5dc1dM0vsAp=Q@mail.gmail.com>

Hello all,

I am having trouble with the inclusion of external regressors in the GARCH
equation, when using the twinkle package kindly provided by Alexios
Ghalanos.
I believe I am including the data correctly to starspec. An xts object,
which is pre-lagged and has the same index as the data included within
starfit optimization.

Below, I have included an example of my code, with some generated data.

library(xts)
library(twinkle)

xts1 <- xts(x=rnorm(100), order.by=Sys.Date()-1:100)

xts2 <- xts(x=rnorm(100), order.by=index(xts1))



spec = starspec(mean.model = list(states = 2,

                                   include.intercept = c(1,1),

                                   arOrder = c(1,1),

                                   maOrder = c(2, 2),

                                   matype="state",

                                   statevar = 'y',

                                   ylags = 3,

                                  statear = TRUE),

                                  variance.model=list(dynamic=TRUE,

                                   model="sGARCH",submodel = NULL,

                                   garchOrder=c(1,1),

                                  vreg=xts1,

                                  variance.targeting = FALSE),

                                  distribution.model = "norm")


ctrl=list(maxit=1000, alpha=0.2, beta=0.1, gamma=0.7, reltol=1e-12,

           trace=0,method="BFGS",n.restarts=2,rseed=1)


mod = starfit(spec, data = xts2,

              solver.control = ctrl,

               solver = "msoptim")



I get the following error when trying to run, which only occurs when the
'vreg' argument is included in the variance model:


Error in matrix(vexdata, ncol = modelinc[39]) :

  object 'vexdata' not found



I get errors with other solvers also, whether I am including the
solver.control list or not.

In the rugarch package, the external regressors should be included as a
matrix, not an xts object. However, when I include data as a matrix here, I
get: starspec-->error: vreg must be an xts object.


Is there a solution or work around to this problem?


Kind regards,

Neil

-- 
__

S?anadh R?omhphoist/_

Email?Disclaimer__
**

T? an r?omhphost seo agus 
aon chomhad a sheoltar leis faoi r?n agus is lena ?s?id ag an seola? agus 
sin amh?in ?.?Is f?idir tuilleadh a l?amh anseo. 
<https://www4.dcu.ie/iss/seanadh-riomhphoist.shtml>? 
<https://www4.dcu.ie/iss/seanadh-riomhphoist.shtml>*
_

This e-mail and any 
files transmitted with it are confidential and are intended solely for use 
by the addressee.?Read more here. 
<https://www4.dcu.ie/iss/email-disclaimer.shtml>?_
*_

	[[alternative HTML version deleted]]


From @|ex|o@ @end|ng |rom 4d@c@pe@com  Thu Sep 12 06:23:12 2019
From: @|ex|o@ @end|ng |rom 4d@c@pe@com (alexios galanos)
Date: Wed, 11 Sep 2019 21:23:12 -0700
Subject: [R-SIG-Finance] 
 External Regressors in GARCH Equation when using Twinkle
In-Reply-To: <CALZLRahR8mOy4_hEuaSR4ALbfF05B3zgRh3KV5dc1dM0vsAp=Q@mail.gmail.com>
References: <CALZLRahR8mOy4_hEuaSR4ALbfF05B3zgRh3KV5dc1dM0vsAp=Q@mail.gmail.com>
Message-ID: <eb73bc65-f77d-318d-bf40-b6a14a7bdd1d@4dscape.com>

Hi,

That's my bad for not testing this. Fixed and on bitbucket now.

Thanks for the bug report.

Alexios

On 9/11/19 4:54 PM, Neil Patrick Lawton wrote:
> Hello all,
> 
> I am having trouble with the inclusion of external regressors in the GARCH
> equation, when using the twinkle package kindly provided by Alexios
> Ghalanos.
> I believe I am including the data correctly to starspec. An xts object,
> which is pre-lagged and has the same index as the data included within
> starfit optimization.
> 
> Below, I have included an example of my code, with some generated data.
> 
> library(xts)
> library(twinkle)
> 
> xts1 <- xts(x=rnorm(100), order.by=Sys.Date()-1:100)
> 
> xts2 <- xts(x=rnorm(100), order.by=index(xts1))
> 
> 
> 
> spec = starspec(mean.model = list(states = 2,
> 
>                                     include.intercept = c(1,1),
> 
>                                     arOrder = c(1,1),
> 
>                                     maOrder = c(2, 2),
> 
>                                     matype="state",
> 
>                                     statevar = 'y',
> 
>                                     ylags = 3,
> 
>                                    statear = TRUE),
> 
>                                    variance.model=list(dynamic=TRUE,
> 
>                                     model="sGARCH",submodel = NULL,
> 
>                                     garchOrder=c(1,1),
> 
>                                    vreg=xts1,
> 
>                                    variance.targeting = FALSE),
> 
>                                    distribution.model = "norm")
> 
> 
> ctrl=list(maxit=1000, alpha=0.2, beta=0.1, gamma=0.7, reltol=1e-12,
> 
>             trace=0,method="BFGS",n.restarts=2,rseed=1)
> 
> 
> mod = starfit(spec, data = xts2,
> 
>                solver.control = ctrl,
> 
>                 solver = "msoptim")
> 
> 
> 
> I get the following error when trying to run, which only occurs when the
> 'vreg' argument is included in the variance model:
> 
> 
> Error in matrix(vexdata, ncol = modelinc[39]) :
> 
>    object 'vexdata' not found
> 
> 
> 
> I get errors with other solvers also, whether I am including the
> solver.control list or not.
> 
> In the rugarch package, the external regressors should be included as a
> matrix, not an xts object. However, when I include data as a matrix here, I
> get: starspec-->error: vreg must be an xts object.
> 
> 
> Is there a solution or work around to this problem?
> 
> 
> Kind regards,
> 
> Neil
>


From e||ot@t@bet @end|ng |rom meteoprotect@com  Tue Sep 17 10:21:09 2019
From: e||ot@t@bet @end|ng |rom meteoprotect@com (Eliot Tabet)
Date: Tue, 17 Sep 2019 10:21:09 +0200
Subject: [R-SIG-Finance] Manually calculating and backtesting VaR and CVaR
 from DCC-GARCH
Message-ID: <CAGo0mnYB5AQveT7LCzSa9OOT5GjyagrWLO4LNC5923Qr8YfHqQ@mail.gmail.com>

I estimated a GARCH fit to the log returns of three series (CAC 40, a french
real estate index and french T10 bond yield series) using `rugarch`. I then
manually calculated and backtested the VaR and CVaR measures. I also fitted
a DCC-GARCH(1,1) to the log returns of the 3 series using `rmgarch` and now
I would like to backtest the VaR and CVaR measures in a similar way as I did
for the univariate GARCH cases.

We'll need to specify the following functions for the CVaR before we
proceed:

    #This function calculates the CVaR at a certain position gdist list
    cvar <- function(p=0.05, s = "CAC", dist_params = gdist_var, pos = l, v
= df, dist = "jsu"){

      ES <- abs((integrate(qdist, lower = 0, upper = p, distribution = dist,
mu = gdist_var[[s]][, 'Mu'][pos], sigma = gdist_var[[s]][, 'Sigma'][pos],
                           shape = gdist_var[[s]][, 'Shape'][pos], skew =
gdist_var[[s]][, 'Skew'][pos])$value)/p * v[nrow(v),s])

      return(ES)
    }

    #This function calculates the CVaR given the arguments
    cvar_df <- function(p=0.01, dist = "jsu", mu = Mu, sigma = Sigma, shape
= Shape, skew = Skew){

      ES <- (integrate(qdist, lower = 0, upper = p, distribution = dist, mu
= mu, sigma = sigma, shape = shape, skew = skew)$value)/p

      return(ES)
    }

    #This function is a vectorized form of the above
    vcvar_df <- Vectorize(cvar_df)

The data can be found on dropbox under the following links (one for french
real estate index data and the other for french bonds) the CAC 40 data is
downloadable with `quantmod`:

https://www.dropbox.com/s/vy8sl88fs5opmi3/IEIF%20SIIC%20FRANCE_quote_chart.csv?dl=0

https://www.dropbox.com/s/xljxk5izy6pt1ds/entre_obligations.csv?dl=0

The commented code is the following:

    #loading libraries and data:

    require(tidyquant)
    require(reshape2)
    require(astsa)
    require(GGally)
    require(forecast)
    source("functions.R", local = T)

    #
#
https://www.banque-france.fr/statistiques/taux-et-cours/les-indices-obligataires

    obli_10 <- read.csv("entre_obligations.csv", sep = ";", na.strings =
"-", stringsAsFactors = F) %>%
      rename(Date = 1) %>%
      mutate(Date = dmy(Date)) %>%
      mutate_at(vars(-Date), funs(gsub("\\,", ".", .))) %>%
      mutate_at(vars(-Date), funs(as.numeric)) %>%
      dplyr::select(c(1,2))

    # #https://live.euronext.com/en/product/indices/QS0010980447-XPAR/quotes
indices nu de:
    #
#
https://www.ieif.fr/wp-content/plugins/aa-indices/datas/histo/index.php?IndiceNu=SIICNu&IndiceNet=SIICNet&IndiceBrut=SIICBrut&Indice=Euronext%20IEIF%20SIIC%20France
    reit <- read.csv("IEIF SIIC FRANCE_quote_chart.csv", stringsAsFactors =
F) %>%
      dplyr::select(1,2) %>%
      rename(Date = 1) %>%
      mutate(Date = substr(Date, 1, 10)) %>%
      mutate(Date = ymd(Date))

    cac <- as.data.frame(Ad(getSymbols("^FCHI", src = "yahoo", adjust = T,
auto.assign = FALSE)))

    cac <- cac %>%
      mutate(Date = rownames(.)) %>%
      mutate(Date = ymd(Date)) %>%
      dplyr::select(Date, everything())

    #Calculate the log returns

    lr_df <- as.data.frame(sapply(df[2:ncol(df)], function(x) diff(log(x))))

    lr_df <-cbind(df$Date[2:nrow(df)], lr_df) %>%
      dplyr::rename(Date = !!names(.)[1])

    #Specification of GARCH models

    cac_egarch_spec <- ugarchspec(mean.model = list(armaOrder = c(3, 3),
include.mean = T, archm = F, archpow = 1),
                                  variance.model = list(model = "eGARCH",
garchOrder = c(2, 1)),
                                  distribution.model="jsu")

    reit_egarch_spec <- ugarchspec(mean.model = list(armaOrder = c(3, 1),
include.mean = T, archm = F, archpow = 1),
                                   variance.model = list(model = "eGARCH",
garchOrder = c(2, 1)),
                                   distribution.model="nig")

    obli_apgarch_spec <- ugarchspec(mean.model = list(armaOrder = c(2, 1),
include.mean = T, archm = F, archpow = 1),
                                     variance.model = list(model = "apARCH",
garchOrder = c(1, 1)),
                                     distribution.model="jsu")

    #Get VaR and CVaR

    cac_roll <- ugarchroll(cac_egarch_spec, lr_df[,2],n.start = 750,
refit.every = 50, refit.window = "moving",
                                       solver = "hybrid", calculate.VaR =
TRUE, VaR.alpha = c(0.01, 0.025, 0.05), keep.coef = T,
                                       fit.control = list(scale = 1))

    reit_roll <- ugarchroll(reit_egarch_spec, lr_df[,3],n.start = 750,
refit.every = 50, refit.window = "moving",
                           solver = "hybrid", calculate.VaR = TRUE,
VaR.alpha = c(0.01, 0.025, 0.05), keep.coef = T,
                           fit.control = list(scale = 1))

    obli_roll <- ugarchroll(obli_apgarch_spec, lr_df[,4],n.start = 750,
refit.every = 50, refit.window = "moving",
                            solver = "hybrid", calculate.VaR = TRUE,
VaR.alpha = c(0.01, 0.025, 0.05), keep.coef = T,
                            fit.control = list(scale = 1))

    gdist_var <- list()

    gdist_var[["CAC"]] <- as.data.frame(cac_roll, which = 'density')
    gdist_var[["REIT"]] <- as.data.frame(reit_roll, which = 'density')
    gdist_var[["OBLI_10"]] <- as.data.frame(obli_roll, which = 'density')

    #VaR and CVaR calculations
    p <- c(0.05, 0.025, 0.01)
    l <- nrow(gdist_var[["CAC"]])

    for(j in p){
      for(i in 1:3){
        print(paste("VaR", names(gdist_var)[i], 1-j))
        print(abs(qdist(dg[[i]], p=j, mu=gdist_var[[i]]$Mu[l],
sigma=gdist_var[[i]]$Sigma[l], skew=gdist_var[[i]]$Skew[l],
shape=gdist_var[[i]]$Shape[l]))*df[nrow(df),i+1])
      }
    }

    for(j in p){
      for(i in 1:3){
        print(paste("CVaR", names(gdist_var)[i], 1-j))
        print(cvar(p = j, s = names(gdist_var)[i], dist_params = gdist_var,
pos = l, v = df, dist = dg[[i]]))
      }
    }

    #VaR plots for cac only but will be done for others the same way
    var_cac <- gdist_var$CAC
    var_cac <- cbind.data.frame(tail(lr_df[,c("Date","CAC")],2438), var_cac)
%>%
      dplyr::select(-`Shape(GIG)`, -Realized) %>%
      dplyr::mutate(VaR_99 = qdist("jsu", p = 0.01, mu = Mu, sigma = Sigma,
skew = Skew, shape = Shape)) %>%
      dplyr::select(-Mu, -Sigma, -Skew, -Shape)
    var_cac <- melt(var_cac, id.vars = "Date")
    ggplot(data = var_cac, aes(x = Date, value)) + geom_line(aes(colour =
variable)) +
      ggtitle("Series with 1% 1D VaR Limit") +
      theme(plot.title = element_text(hjust = 0.5))

    #VaR backtesting reports using report function
    report(cac_roll, type = "VaR", VaR.alpha = 0.05, conf.level = 0.95)
    report(reit_roll, type = "VaR", VaR.alpha = 0.05, conf.level = 0.95)
    report(obli_roll, type = "VaR", VaR.alpha = 0.05, conf.level = 0.95)

    #CVaR plots for CAC only but will be done for others
    cvar_cac <- gdist_var$CAC
    cvar_cac <- cbind.data.frame(tail(lr_df[,c("Date","CAC")],2438),
cvar_cac) %>%
      dplyr::select(-`Shape(GIG)`, -Realized) %>%
      dplyr::mutate(CVaR_99 = vcvar_df(p = 0.01, dist = "jsu", mu = Mu,
sigma = Sigma, shape = Shape, skew = Skew)) %>%
      dplyr::select(-Mu, -Sigma, -Skew, -Shape)

    mcvar_cac <- melt(cvar_cac, id.vars = "Date")
    ggplot(data = mcvar_cac, aes(x = Date, value)) + geom_line(aes(colour =
variable)) +
      ggtitle("Series with 1% 1D CVaR Limit") +
      theme(plot.title = element_text(hjust = 0.5))

    #Bactesting CVaRby calculating nuber of times CVaR crossed
    cvar_cac <- gdist_var$CAC
    cvar_cac <- cbind.data.frame(tail(lr_df[,c("Date","CAC")],2438),
cvar_cac) %>%
      dplyr::select(-`Shape(GIG)`, -Realized) %>%
      dplyr::mutate(CVaR_99 = vcvar_df(p = 0.01, dist = "jsu", mu = Mu,
sigma = Sigma, shape = Shape, skew = Skew)) %>%
      dplyr::mutate(CVaR_975 = vcvar_df(p = 0.025, dist = "jsu", mu = Mu,
sigma = Sigma, shape = Shape, skew = Skew)) %>%
      dplyr::mutate(CVaR_95 = vcvar_df(p = 0.05, dist = "jsu", mu = Mu,
sigma = Sigma, shape = Shape, skew = Skew)) %>%
      mutate(depasse_99 = case_when(CVaR_99 >= .[[2]] ~ 1, TRUE ~ 0)) %>%
      mutate(depasse_975 = case_when(CVaR_975 >= .[[2]] ~ 1, TRUE ~ 0)) %>%
      mutate(depasse_95 = case_when(CVaR_95 >= .[[2]] ~ 1, TRUE ~ 0)) %>%
      mutate(sum_99 = sum(depasse_99)) %>%
      mutate(sum_975 = sum(depasse_975)) %>%
      mutate(sum_95 = sum(depasse_95))

    #DCC GARCH of GARCH models above:
    require(rmgarch)

    dcc_garch <- multispec(c(cac_egarch_spec, reit_egarch_spec,
obli_apgarch_spec))
    dcc_multfit <- multifit(dcc_garch, lr_df[,2:ncol(lr_df)]) #fitting many
univariate models
    dcc_spec <- dccspec(uspec = dcc_garch, dccOrder = c(1,1), distribution =
"mvnorm")
    dcc_fit <- dccfit(dcc_spec, lr_df[,2:ncol(lr_df)], fit.control =
list(eval.se = TRUE), fit = dcc_multfit) #fit = dcc_multfit not really
necessary but more robust
    dcc_roll <- dccroll(dcc_spec, lr_df[,2:4],n.start = 750, refit.every =
50, refit.window = "moving",
                           solver = "solnp", calculate.VaR = TRUE, VaR.alpha
= c(0.01, 0.025, 0.05), keep.coef = T,
                           fit.control = list(scale = 1))

Now I want to do the backtesting and plotting steps for both 1Day VaR and
1Day CVaR measures. Ideally I would also conduct the Kupiec and
Christoffersen test just like in the function `report` of the package
`rugarch`. I am realy stumped as I tried to find an answer online but
couldn't.

-- 
Eliot TABET | Structurer/Quantitative Analyst



Les informations contenues dans ce message sont confidentielles.
Si vous receviez ce message par erreur, merci de pr?venir l'exp?diteur
et de le supprimer ainsi que ses pi?ces jointes.

N'imprimez ce mail que si c'est n?cessaire

	[[alternative HTML version deleted]]


From d|egoperon|1971 @end|ng |rom gm@||@com  Thu Sep 19 11:13:57 2019
From: d|egoperon|1971 @end|ng |rom gm@||@com (diego peroni)
Date: Thu, 19 Sep 2019 11:13:57 +0200
Subject: [R-SIG-Finance] breatdh indicators
Message-ID: <493727B1-D99C-4198-932E-67350D1691C0@gmail.com>

Hi all,
I?m looking for breatdh indicators data (daily): NYSI, NASI, BPSPX, BPCOMPQ, BPFINA, SPXA50r, SPXADP
Anyone knows where can I download them (using R) for free?
Thanks in advance for your help.
Diego

From p@nk@j@b| @end|ng |rom y@hoo@com  Tue Sep 24 11:06:20 2019
From: p@nk@j@b| @end|ng |rom y@hoo@com (Pankaj K Agarwal)
Date: Tue, 24 Sep 2019 09:06:20 +0000 (UTC)
Subject: [R-SIG-Finance] Resources for AI/ML in Risk Management
References: <1802055414.9447663.1569315980893.ref@mail.yahoo.com>
Message-ID: <1802055414.9447663.1569315980893@mail.yahoo.com>

Dear allI would be grateful if someone can suggest R-based resources/books on use of AI/ML in Financial Risk Management.?
Regards,Pankaj K Agarwal
+91-98397-11444http://in.linkedin.com/in/pankajkagarwal/
	[[alternative HTML version deleted]]


From br|@n @end|ng |rom br@verock@com  Tue Sep 24 11:46:59 2019
From: br|@n @end|ng |rom br@verock@com (Brian G. Peterson)
Date: Tue, 24 Sep 2019 04:46:59 -0500
Subject: [R-SIG-Finance] Resources for AI/ML in Risk Management
In-Reply-To: <1802055414.9447663.1569315980893@mail.yahoo.com>
References: <1802055414.9447663.1569315980893.ref@mail.yahoo.com>
 <1802055414.9447663.1569315980893@mail.yahoo.com>
Message-ID: <296f496a0a2789575cb5430197e088201082361e.camel@braverock.com>

On Tue, 2019-09-24 at 09:06 +0000, Pankaj K Agarwal via R-SIG-Finance
wrote:
> Dear allI would be grateful if someone can suggest R-based
> resources/books on use of AI/ML in Financial Risk Management. 
> Regards,Pankaj K Agarwal

You need to be a little more clear about what it is that you intend to
do.

What types of models are you using today?  What do you hope to gain by
using more complicated models?  What challenges are the models you are
currently using encountering?

Both "Financial Risk Management" and "AI/ML" are vast topics.  A simple
intersection doesn't give anyone here any guidance on how to help you. 

Regards,

Brian

-- 
Brian G. Peterson
ph: +1.773.459.4973
im: bgpbraverock


