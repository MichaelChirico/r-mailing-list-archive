From manojsw at gmail.com  Mon Jul  3 07:02:27 2006
From: manojsw at gmail.com (Manoj)
Date: Mon, 3 Jul 2006 14:02:27 +0900
Subject: [R-sig-Finance] Backtesting speed
Message-ID: <829e6c8a0607022202r8e6c165v527847860ed9e368@mail.gmail.com>

Hello All,
      Generally I do most of my back-test (basically running
correlation/regression analysis etc on a set of historical data) in R.

      Siince I haven't done any similar analysis on any other
platform/language, I wonder If any of you out there has any experience
in conduting computation intensive back-tests in other langauges and
done some sort of statistics on the time taken to perform the
back-test in R as oppose to other langauges.

     My hunch is that processing time shouldn't be *very* different
between R & other languages but then it's just a hunch. Can anybody
share/comment on there experience? Sorry for the open-ended nature of
the question.

Cheers

Manoj


From a.trapletti at swissonline.ch  Mon Jul  3 13:27:34 2006
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Mon, 03 Jul 2006 13:27:34 +0200
Subject: [R-sig-Finance] Backtesting speed
Message-ID: <44A8FF26.8060504@swissonline.ch>

The companies I (used to) work for use intraday data, sometimes 
tick-by-tick data, to backtest trading strategies. In that case  speed 
is a very important issue and interpreted language environments like R 
are slow in comparison to compiled language environments like C/C++. 
Generic environment for quant trading might look like the following: 
time critical computations like backtesting and realtime computing in 
C/C++, client/server, GUI etc. in Java, and statistical analysis and 
diagnostics, prototyping etc. in Matlab or R or similar.

Regards
Adrian

>Message: 1
>Date: Mon, 3 Jul 2006 14:02:27 +0900
>From: Manoj <manojsw at gmail.com>
>Subject: [R-sig-Finance] Backtesting speed
>To: "r-sig-finance at stat.math.ethz.ch"
>	<r-sig-finance at stat.math.ethz.ch>
>Message-ID:
>	<829e6c8a0607022202r8e6c165v527847860ed9e368 at mail.gmail.com>
>Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
>Hello All,
>      Generally I do most of my back-test (basically running
>correlation/regression analysis etc on a set of historical data) in R.
>
>      Siince I haven't done any similar analysis on any other
>platform/language, I wonder If any of you out there has any experience
>in conduting computation intensive back-tests in other langauges and
>done some sort of statistics on the time taken to perform the
>back-test in R as oppose to other langauges.
>
>     My hunch is that processing time shouldn't be *very* different
>between R & other languages but then it's just a hunch. Can anybody
>share/comment on there experience? Sorry for the open-ended nature of
>the question.
>
>Cheers
>
>Manoj
>  
>


From bbands at gmail.com  Mon Jul  3 18:23:21 2006
From: bbands at gmail.com (BBands)
Date: Mon, 3 Jul 2006 09:23:21 -0700
Subject: [R-sig-Finance] Backtesting speed
In-Reply-To: <829e6c8a0607022202r8e6c165v527847860ed9e368@mail.gmail.com>
References: <829e6c8a0607022202r8e6c165v527847860ed9e368@mail.gmail.com>
Message-ID: <6e8360ad0607030923yb04d317g63b30896f3a191a5@mail.gmail.com>

On 7/2/06, Manoj <manojsw at gmail.com> wrote:
> Hello All,
>       Generally I do most of my back-test (basically running
> correlation/regression analysis etc on a set of historical data) in R.
>
>       Siince I haven't done any similar analysis on any other
> platform/language, I wonder If any of you out there has any experience
> in conduting computation intensive back-tests in other langauges and
> done some sort of statistics on the time taken to perform the
> back-test in R as oppose to other langauges.
>
>      My hunch is that processing time shouldn't be *very* different
> between R & other languages but then it's just a hunch. Can anybody
> share/comment on there experience? Sorry for the open-ended nature of
> the question.
>
> Cheers
>
> Manoj

In my experience scripted languages can be quite fast. The bulk of the
time is usually consumed by a relatively small part of the code.
Profiling to see where the problems are, optimisation of the time
heavy sections and perhaps off loading the still irksome portions to a
C or C++ library can help.

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From ggrothendieck at gmail.com  Tue Jul  4 15:43:44 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 4 Jul 2006 09:43:44 -0400
Subject: [R-sig-Finance] Data management question
In-Reply-To: <6E41ACBA39282B47BD1E231869C1130C032BC9FD@ex3.nyc.dbzco.com>
References: <6E41ACBA39282B47BD1E231869C1130C032BC9FD@ex3.nyc.dbzco.com>
Message-ID: <971536df0607040643j18d5da1cvf69c97c1a491eaf0@mail.gmail.com>

This will create a zoo object.  Replace textConnection(Lines), which
I have used here to keep it self contained, with your file name, e.g.
"myfile.dat"


# test data
Lines <- "Date    ticker  price
06/1/06 A       1
06/2/06 A       2
06/1/06 B       1
06/2/06 B       2
06/1/06 C       1
06/2/06 C       2
"

library(zoo)
z <- read.zoo(textConnection(Lines), header = TRUE, format = "%m/%d/%y")
do.call(merge, split(z[,2], z[,1]))


On 6/30/06, Daye, Wilfred <wilfred.daye at dbzco.com> wrote:
> I have a text file data that looks like this
>
> Date    ticker  price
> 06/1/06 A       x1
> 06/2/06 A       x2
> ;;      ;;      ;;
> 06/1/06 B       y1
> 06/2/06 B       y2
> ;;      ;;      ;;
> 06/1/06 C       z1
> 06/2/06 C       z2
> ;;      ;;      ;;
>
> So on
>
> I want to arrange tickers as column fields and match the time series
> price by date.
> To be
>
> Date    A       B       C
> 06/1/06 x1      y1      z1
>
>
> The time series per ticker does not necessarily equal to those of others
>
> Thx in advance
>
>
>
>
> This e-mail message is intended only for the named recipient(s) above. It may contain confidential information. If you are not the intended recipient, you are hereby notified that any use, dissemination, distribution or copying of this e-mail and any attachment(s) is strictly prohibited. D.B. Zwirn & Co., L.P. reserves the right to archive and monitor all e-mail communications through its networks.  If you have received this e-mail in error, please immediately notify the sender by replying to this e-mail and delete the message and any attachment(s) from your system. Thank you.
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From cgb at datanalytics.com  Tue Jul  4 16:27:41 2006
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Tue, 04 Jul 2006 16:27:41 +0200
Subject: [R-sig-Finance] Data management question
In-Reply-To: <6E41ACBA39282B47BD1E231869C1130C032BC9FD@ex3.nyc.dbzco.com>
References: <6E41ACBA39282B47BD1E231869C1130C032BC9FD@ex3.nyc.dbzco.com>
Message-ID: <20060704162741.u1zwuvlcrscgwg48@webmail.datanalytics.com>

Try ?reshape

Carlos J. Gil Bellosta
http://www.datanalytics.com


Quoting "Daye, Wilfred" <wilfred.daye at dbzco.com>:

> I have a text file data that looks like this
>
> Date	ticker	price
> 06/1/06	A	x1
> 06/2/06	A	x2
> ;;	;;	;;
> 06/1/06	B	y1
> 06/2/06	B	y2
> ;;	;;	;;
> 06/1/06	C	z1
> 06/2/06	C	z2
> ;;	;;	;;
>
> So on
>
> I want to arrange tickers as column fields and match the time series
> price by date.
> To be
>
> Date	A	B	C
> 06/1/06	x1	y1	z1
>
>
> The time series per ticker does not necessarily equal to those of others
>
> Thx in advance


From ivan.kalafatic at gmail.com  Thu Jul  6 13:21:42 2006
From: ivan.kalafatic at gmail.com (Ivan Kalafatic)
Date: Thu, 6 Jul 2006 12:21:42 +0100
Subject: [R-sig-Finance] Problem with garchFit function in fSeries
In-Reply-To: <6dbf89a50607060419k2a0892bdtd2264694e78a2278@mail.gmail.com>
References: <6dbf89a50607060419k2a0892bdtd2264694e78a2278@mail.gmail.com>
Message-ID: <6dbf89a50607060421r5eb6a3c9m964b8f84b12f974e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060706/ea7f3eb1/attachment.pl 

From joe-byers at utulsa.edu  Thu Jul  6 18:05:52 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Thu, 06 Jul 2006 11:05:52 -0500
Subject: [R-sig-Finance] Rmetric and sciviews
Message-ID: <44AD34E0.1070106@utulsa.edu>

I am using sciviews as my gui for R.  Sciviews does not support mdi mode 
with R, so when you run any summary(armaobjectresults) functions, only 
the last box ljung plot is shown.  The history is also empty in the 
graph window.

My work around was to modify summary.arma to use r2html functions 
instead of cat functions.  See a previous post.

Has anyone else experienced this? And, do they have a fix for this problem?

Thank you
Joe

-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 104 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060706/5bc78630/attachment.vcf 

From joe-byers at utulsa.edu  Thu Jul  6 18:14:18 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Thu, 06 Jul 2006 11:14:18 -0500
Subject: [R-sig-Finance] arma model when exogenouse variables used in
	ARMA(p=5, q=(1-6, 19))
Message-ID: <44AD36DA.7030807@utulsa.edu>

I am fitting a ARMA(p=5,q=(1-6,19)) with exnogenouse variables 
(xreg=exovars), and masking (fixed=fixedparms) the MA terms 7-18 to get 
the model to run.  My code is
    fixed=c(rep(NA,5),rep(NA,6),rep(0,12),NA,NA);#5 ar terms, 19 ma 
terms fixed 7-18 lag ma term, intercept


    fixed<-c(fixed,NA); #add the lin.trend term
    ts.results.2.ma<- 
armaFit(datats~arma(5,19),xreg=cbind(Lin.Trend=d$factor.Lin.Trend.),
      include.mean=TRUE,optim.control=list(maxit=500),fixed=fixed);
    summary.fARMA.HTML(ts.results.2.ma,title="AR(5) with Intercept and 
Linear Trend");

Is there an easier way to specify this model maybe like
armaFit(datats~arma(5,(1:6,19))

I tried this as
    ts.results.2.ma<- 
armaFit(datats~arma(5,(1:6,19)),xreg=cbind(Lin.Trend=d$factor.Lin.Trend.),
      include.mean=TRUE,optim.control=list(maxit=500));

And I get a syntax error.

Thank you
Joe W. Byers

-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 104 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060706/4821dc77/attachment.vcf 

From joe-byers at utulsa.edu  Thu Jul  6 18:38:02 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Thu, 06 Jul 2006 11:38:02 -0500
Subject: [R-sig-Finance] arma model results when exogenouse variables used
	in ARMA(p=5, q=(1-6, 19))
Message-ID: <44AD3C6A.9000204@utulsa.edu>

All,

I posted a message earlier about fitting an ARMA(p=5,q=(1-6,19)) with 
exnogenouse variables (xreg=exovars), and masking (fixed=fixedparms) the 
MA terms 7-18 to get the model to run. I am reposting some of the 
message to help with understanding the summary() function problem

My code is
    fixed=c(rep(NA,5),rep(NA,6),rep(0,12),NA,NA);#5 ar terms, 19 ma
terms fixed 7-18 lag ma term, intercept


    fixed<-c(fixed,NA); #add the lin.trend term
           
 ts.results.2.ma<-armaFit(datats~arma(5,19),xreg=cbind(Lin.Trend=d$factor.Lin.Trend.),
      include.mean=TRUE,optim.control=list(maxit=500),fixed=fixed);
    summary.fARMA.HTML(ts.results.2.ma,title="AR(5) MA(1:6,19) with 
Intercept and Linear Trend");# this function is a modification of 
summary.fARMA to work with r2HTML replacing the cat() functions.

The problem I have is that the @fit$coef(26) and @fit$se.coef(14) are of 
different lengths causing the t-stats calculation is summary to issue 
warnings.
Warning messages:
1: longer object length
        is not a multiple of shorter object length in: 
object$coef/object$se.coef
2: number of rows of result
        is not a multiple of vector length (arg 2) in: cbind(1, 
format(object$coef, digits = digits), format(object$se.coef,  
The tval and prob are not correct.

The summary() code is
    tval <- object$coef/object$se.coef
    prob <- 2 * (1 - pnorm(abs(tval)))
    ans$coefmat <- cbind(format(object$coef,digits=digits), 
format(object$se.coef,digits=digits),
        format(tval,digits=digits), prob=format.pval(prob,digits=digits))
    dimnames(ans$coefmat) <- list(names(object$coef), c(" Estimate",
        " Std. Error", " t value", "Pr(>|t|)"))
    row.names(ans$coefmat)<-toupper(row.names(ans$coefmat))

I can modify tval as
tval<-subset(object$coef,object$mask)/object$se.coef
    prob <- 2 * (1 - pnorm(abs(tval)))

The subset functions removes all FALSE or masked MA terms from the coef 
vector.  It will return a vector of length 14.  Now I have to expand 
tval, prob and se.coef out to match the length of coef to get the the 
results printed correctly.

Can anyone help me with this? It would probably be a good thing to 
include in future versions of rMetrics as well.

Thank you
Joe W. Byers

-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 104 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060706/a4c15bae/attachment.vcf 

From fcitta at inwind.it  Thu Jul  6 16:10:36 2006
From: fcitta at inwind.it (Citta Francesco)
Date: Thu, 6 Jul 2006 16:10:36 +0200
Subject: [R-sig-Finance] multivariate GARCH
Message-ID: <000b01c6a106$16cdce60$4f5a1997@y6c3m0>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060706/c2eafe5a/attachment.pl 

From hkahra at gmail.com  Fri Jul  7 08:51:53 2006
From: hkahra at gmail.com (Hannu Kahra)
Date: Fri, 7 Jul 2006 09:51:53 +0300
Subject: [R-sig-Finance] multivariate GARCH
In-Reply-To: <000b01c6a106$16cdce60$4f5a1997@y6c3m0>
References: <000b01c6a106$16cdce60$4f5a1997@y6c3m0>
Message-ID: <3d35a2ca0607062351v3cd2c709lac13eec52f9622c2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060707/960f3f18/attachment.pl 

From a.trapletti at swissonline.ch  Fri Jul  7 10:22:38 2006
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Fri, 07 Jul 2006 10:22:38 +0200
Subject: [R-sig-Finance] Backtesting Speed
In-Reply-To: <030B041DE2D0A34F8C7283D96877A44FE861A0@hou0mbx01.kochind.com>
References: <030B041DE2D0A34F8C7283D96877A44FE861A0@hou0mbx01.kochind.com>
Message-ID: <44AE19CE.3030309@swissonline.ch>

Wojciechowski, William wrote:

>Hi,
>
>What type of algorithms/computations would you
>program in C/C++?
>  
>
The only general comment I can make about your question is: You have to 
find out where speed really matters for your application. As already 
mentioned here on the list, typically a very small portion of the code 
uses almost all cpu and/or I/O time. And improving this code or porting 
it to a faster environment helps. However, sometimes it is better to 
implement the whole application in a faster and maybe more primitive 
environment as switching between high-level and low-level environments 
is too expensive (like calling C/C++ code from R). And sometimes the 
"cheapest" way is just to improve your hardware, e.g. if I/O behaviour 
is really important, then a faster hardware solution is maybe far 
cheaper than having a software engineer redesign your whole application.

Best regards
Adrian

>Thanks,
>
>William C. Wojciechowski
>Koch Quantitative Trading
>20 East Greenway Plaza
>Houston, TX 77046
>713-544-5093
>william.wojciechowski at kochind.com
>
>-----
>
>The companies I (used to) work for use intraday data, sometimes
>tick-by-tick data, to backtest trading strategies. In that case speed is
>a very important issue and interpreted language environments like R are
>slow in comparison to compiled language environments like C/C++. 
>
>Generic environment for quant trading might look like the following: 
>
>time critical computations like backtesting and realtime computing in
>C/C++, client/server, GUI etc. in Java, and statistical analysis and
>diagnostics, prototyping etc. in Matlab or R or similar.
>
>Regards
>
>Adrian
>
>
>
>  
>


From William.Wojciechowski at kochind.com  Thu Jul  6 16:01:19 2006
From: William.Wojciechowski at kochind.com (Wojciechowski, William)
Date: Thu, 6 Jul 2006 09:01:19 -0500
Subject: [R-sig-Finance] Backtesting Speed
Message-ID: <030B041DE2D0A34F8C7283D96877A44FE861A0@hou0mbx01.kochind.com>

Hi,

What type of algorithms/computations would you
program in C/C++?

Thanks,

William C. Wojciechowski
Koch Quantitative Trading
20 East Greenway Plaza
Houston, TX 77046
713-544-5093
william.wojciechowski at kochind.com

-----

The companies I (used to) work for use intraday data, sometimes
tick-by-tick data, to backtest trading strategies. In that case speed is
a very important issue and interpreted language environments like R are
slow in comparison to compiled language environments like C/C++. 

Generic environment for quant trading might look like the following: 

time critical computations like backtesting and realtime computing in
C/C++, client/server, GUI etc. in Java, and statistical analysis and
diagnostics, prototyping etc. in Matlab or R or similar.

Regards

Adrian


From William.Wojciechowski at kochind.com  Thu Jul  6 16:50:26 2006
From: William.Wojciechowski at kochind.com (Wojciechowski, William)
Date: Thu, 6 Jul 2006 09:50:26 -0500
Subject: [R-sig-Finance] Backtesting Speed
Message-ID: <030B041DE2D0A34F8C7283D96877A44FE861A4@hou0mbx01.kochind.com>

Hi,

What type of algorithms/computations did you implement in C/C++?

Thanks,

William C. Wojciechowski

Koch Quantitative Trading
20 East Greenway Plaza
Houston, TX 77046
713-544-5093
william.wojciechowski at kochind.com

-----

The companies I (used to) work for use intraday data, sometimes
tick-by-tick data, to backtest trading strategies. In that case speed is
a very important issue and interpreted language environments like R are
slow in comparison to compiled language environments like C/C++. 

Generic environment for quant trading might look like the following: 

time critical computations like backtesting and realtime computing in
C/C++, client/server, GUI etc. in Java, and statistical analysis and
diagnostics, prototyping etc. in Matlab or R or similar.

Regards

Adrian


From uofiowa at gmail.com  Fri Jul  7 17:58:00 2006
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 7 Jul 2006 11:58:00 -0400
Subject: [R-sig-Finance] fMultivar rollMax question
In-Reply-To: <3f87cc6d0607051106x15fd945eo158c2a27aa267ee8@mail.gmail.com>
References: <3f87cc6d0607051106x15fd945eo158c2a27aa267ee8@mail.gmail.com>
Message-ID: <3f87cc6d0607070858xbfa37bdhf5fd6192f6a93c83@mail.gmail.com>

I am using fMultivar under R 2.2.1 on a Debian linux box. Could
someone, please, explain to me why there are two trailing NAs in the
last statement in the code beow?


> library(fMultivar)
> x <- 1:20
> rollMax(x, n=3)
 [1]  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
> rollMax(x, n=2)
 [1]  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
> rollMax(x, n=1)
 [1]  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 NA NA
>


From spencer.graves at pdf.com  Sat Jul  8 04:35:54 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 07 Jul 2006 19:35:54 -0700
Subject: [R-sig-Finance] fMultivar rollMax question
In-Reply-To: <3f87cc6d0607070858xbfa37bdhf5fd6192f6a93c83@mail.gmail.com>
References: <3f87cc6d0607051106x15fd945eo158c2a27aa267ee8@mail.gmail.com>
	<3f87cc6d0607070858xbfa37bdhf5fd6192f6a93c83@mail.gmail.com>
Message-ID: <44AF1A0A.7060806@pdf.com>

	  This is known in some circles as an "infelicity" in the code.  A 
corrected version of rollFun, called by rollMax, appears below.

	  In brief, the problem occurs because the author of rollFun (Diethelm 
Wuertz, I believe) did not envision n<2, and rollFun contains "for(i in 
2:n)".  My proposed solution is simply to insert a new statement as the 
first line of "rollFun":

   if(n<=1)return(FUN(x))

	  I'm copying Prof. Wuertz on this email, so he can take appropriate 
action.  Until that change works its way into code on your computer via, 
e.g., "update.packages()", you can copy the revised function into any 
script that uses rollMax, rollMin, rollMean, rollVar, or rollFun.

	  Hope this helps.
	  Spencer Graves
####################
rollFun <-
function (x, n, trim = TRUE, na.rm = FALSE, FUN, ...)
{
   if(n<=1)return(FUN(x))
   #
     x.orig = x
     if (is.timeSeries(x))
         TS = TRUE
     else TS = FALSE
     if (TS) {
         positions = x.orig at positions
         x = x.orig at Data[, 1]
     }
     else {
         x = as.vector(x.orig)
         names(x) = NULL
     }
     if (na.rm) {
         if (TS)
             positions = positions[!is.na(x)]
         x = as.vector(na.omit(x))
     }
     start = 1
     end = length(x) - n + 1
     m = x[start:end]
     for (i in 2:n) {
         start = start + 1
         end = end + 1
         m = cbind(m, x[start:end])
     }
     ans = apply(m, MARGIN = 1, FUN = FUN, ...)
     if (!trim)
         ans = c(rep(NA, (n - 1)), ans)
     if (trim & TS)
         positions = positions[-(1:(n - 1))]
     if (TS) {
         ans = timeSeries(as.matrix(ans), positions, units = x.orig at units,
             FinCenter = x.orig at FinCenter)
     }
     ans
}

####################
Omar Lakkis wrote:
> I am using fMultivar under R 2.2.1 on a Debian linux box. Could
> someone, please, explain to me why there are two trailing NAs in the
> last statement in the code beow?
> 
> 
>> library(fMultivar)
>> x <- 1:20
>> rollMax(x, n=3)
>  [1]  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
>> rollMax(x, n=2)
>  [1]  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
>> rollMax(x, n=1)
>  [1]  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 NA NA
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From g-farnsworth at kellogg.northwestern.edu  Sat Jul  8 20:13:36 2006
From: g-farnsworth at kellogg.northwestern.edu (Grant Farnsworth)
Date: Sat, 8 Jul 2006 13:13:36 -0500
Subject: [R-sig-Finance] multivariate GARCH
In-Reply-To: <000b01c6a106$16cdce60$4f5a1997@y6c3m0>
References: <000b01c6a106$16cdce60$4f5a1997@y6c3m0>
Message-ID: <20060708181336.GA8129@dhcp-129-105-194-62.kellogg.northwestern.edu>

I believe the forthcoming mgarch package, which was discussed at UseR
2006, will (at least eventually) include DCC.  You can get the early
version of the BEKK model at

http://www.vsthost.com/vstDocs/projects/R/mgarchBEKK/

>From what I understand, the authors intend to post mgarch on CRAN fairly
soon.

Grant
________________________________________________________________________
Grant Verdell Farnsworth 
Ph.D. Candidate, Finance 
Kellogg School of Management

On Jul 06 at 04:10PM, Citta Francesco wrote:
> Dear Listers: I am interested to use the Dynamic Conditional
> Correlation model of Engle. I am wondering if there is some paper or
> reference on if this topic and there is some kind of implementation in
> R.  And how can I use multivariate GARCH in R?  Tanks, Francesco Citta


From spencer.graves at pdf.com  Mon Jul 10 06:15:33 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 09 Jul 2006 21:15:33 -0700
Subject: [R-sig-Finance] Rmetric and sciviews
In-Reply-To: <44AD34E0.1070106@utulsa.edu>
References: <44AD34E0.1070106@utulsa.edu>
Message-ID: <44B1D465.50307@pdf.com>

	  To a similar post roughly 20 months ago, Mark Schwartz replied, "The 
standard way to pause between plots is to set 'par(ask = TRUE)' before 
your first plot, which will then prompt you to continue after each new 
plot. See ?par for more information." 
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/42330.html)

	  Hope this helps.
	  Spencer Graves

Joe Byers wrote:
> I am using sciviews as my gui for R.  Sciviews does not support mdi mode 
> with R, so when you run any summary(armaobjectresults) functions, only 
> the last box ljung plot is shown.  The history is also empty in the 
> graph window.
> 
> My work around was to modify summary.arma to use r2html functions 
> instead of cat functions.  See a previous post.
> 
> Has anyone else experienced this? And, do they have a fix for this problem?
> 
> Thank you
> Joe
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From joe-byers at utulsa.edu  Mon Jul 10 16:16:53 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Mon, 10 Jul 2006 09:16:53 -0500
Subject: [R-sig-Finance] Rmetric and sciviews
In-Reply-To: <44B1D465.50307@pdf.com>
References: <44AD34E0.1070106@utulsa.edu> <44B1D465.50307@pdf.com>
Message-ID: <44B26155.2090305@utulsa.edu>

Spencer,

Thank you very much.  My proficiency in R is getting better but I still 
can't figure some things out.  Much like SAS, which I have over 15 years 
of experience.  There is always some little option or trick buried deep 
in documentation or mailing lists.

Again,
Thank you
Joe


Spencer Graves wrote:
>       To a similar post roughly 20 months ago, Mark Schwartz replied, 
> "The standard way to pause between plots is to set 'par(ask = TRUE)' 
> before your first plot, which will then prompt you to continue after 
> each new plot. See ?par for more information." 
> (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/42330.html)
>
>       Hope this helps.
>       Spencer Graves
>
> Joe Byers wrote:
>> I am using sciviews as my gui for R.  Sciviews does not support mdi 
>> mode with R, so when you run any summary(armaobjectresults) 
>> functions, only the last box ljung plot is shown.  The history is 
>> also empty in the graph window.
>>
>> My work around was to modify summary.arma to use r2html functions 
>> instead of cat functions.  See a previous post.
>>
>> Has anyone else experienced this? And, do they have a fix for this 
>> problem?
>>
>> Thank you
>> Joe
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 104 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060710/655f99e1/attachment.vcf 

From joe-byers at utulsa.edu  Mon Jul 10 22:31:47 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Mon, 10 Jul 2006 15:31:47 -0500
Subject: [R-sig-Finance] arma model results when exogenouse variables
 used	in, ARMA(p=5, q=(1-6, 19)) (Joe Byers)
In-Reply-To: <mailman.13.1152266405.22923.r-sig-finance@stat.math.ethz.ch>
References: <mailman.13.1152266405.22923.r-sig-finance@stat.math.ethz.ch>
Message-ID: <44B2B933.70307@utulsa.edu>

All,

I have developed a modification for the summary.fARMA() to handle the 
different vector sizes.  I am not sure if this is the best or most 
efficient coding and would appreciate your comments.  The original code 
is commented out and my new code follows as
    # the following lines replace to handle exongeous variables and 
masking of paramters
#    tval <- object$coef/object$se.coef
#    prob <- 2 * (1 - pnorm(abs(tval)))
#    ans$coefmat <- cbind(format(object$coef,digits=digits), 
format(object$se.coef,digits=digits),
#        format(tval,digits=digits), prob=format.pval(prob,digits=digits))
#    rownames(ans$coefmat)<-ans$coefmat$Row.names
#    ans$coefmat<-ans$coefmat[2:length(ans$coefmat)]
#    dimnames(ans$coefmat) <- list(names(object$coef), c(" Estimate",
#        " Std. Error", " t value", "Pr(>|t|)"))
#    row.names(ans$coefmat)<-toupper(row.names(ans$coefmat))
    tval<-subset(object$coef,object$mask)/object$se.coef
    prob <- 2 * (1 - pnorm(abs(tval)))
    ans$coefmat<-merge(format(object$coef,digits=digits),
      
cbind(format(object$se.coef,digits=digits),format(tval,digits=digits),format.pval(prob,digits=digits)),
        by.x="row.names",by.y="row.names",sort=FALSE,all.x=TRUE,all.y=FALSE)
    rownames(ans$coefmat)<-toupper(as.vector(ans$coefmat$Row.names)) 
#make the row indexes the parameter names
    ans$coefmat<-ans$coefmat[2:length(ans$coefmat)] # drop first column 
that merge create of row.names
       names(ans$coefmat) <- c(" Estimate"," Std. Error", " t value", 
"Pr(>|t|)")

This code worked in my modified summary.fARMA() that outputs to html 
files.  The only problem is that the merge creates a data.frame of the 
intersected rows of the two data sets and then adds the outer 
(non-intersecting) rows from object$coef data.frame.  I can not figure 
out how to keep the order based on object$coef.  The full method is 
include below for reference.

Thank you
Joe W. Byers
Professor of Finance
The University of Tulsa


>    4. arma model results when exogenouse variables used	in
>       ARMA(p=5, q=(1-6, 19)) (Joe Byers)
> ------------------------------
>
> Message: 4
> Date: Thu, 06 Jul 2006 11:38:02 -0500
> From: Joe Byers <joe-byers at utulsa.edu>
> Subject: [R-sig-Finance] arma model results when exogenouse variables
> 	used	in ARMA(p=5, q=(1-6, 19))
> To: r-sig-finance at stat.math.ethz.ch
> Message-ID: <44AD3C6A.9000204 at utulsa.edu>
> Content-Type: text/plain; charset="iso-8859-1"
>
> All,
>
> I posted a message earlier about fitting an ARMA(p=5,q=(1-6,19)) with 
> exnogenouse variables (xreg=exovars), and masking (fixed=fixedparms) the 
> MA terms 7-18 to get the model to run. I am reposting some of the 
> message to help with understanding the summary() function problem
>
> My code is
>     fixed=c(rep(NA,5),rep(NA,6),rep(0,12),NA,NA);#5 ar terms, 19 ma
> terms fixed 7-18 lag ma term, intercept
>
>
>     fixed<-c(fixed,NA); #add the lin.trend term
>            
>  ts.results.2.ma<-armaFit(datats~arma(5,19),xreg=cbind(Lin.Trend=d$factor.Lin.Trend.),
>       include.mean=TRUE,optim.control=list(maxit=500),fixed=fixed);
>     summary.fARMA.HTML(ts.results.2.ma,title="AR(5) MA(1:6,19) with 
> Intercept and Linear Trend");# this function is a modification of 
> summary.fARMA to work with r2HTML replacing the cat() functions.
>
> The problem I have is that the @fit$coef(26) and @fit$se.coef(14) are of 
> different lengths causing the t-stats calculation is summary to issue 
> warnings.
> Warning messages:
> 1: longer object length
>         is not a multiple of shorter object length in: 
> object$coef/object$se.coef
> 2: number of rows of result
>         is not a multiple of vector length (arg 2) in: cbind(1, 
> format(object$coef, digits = digits), format(object$se.coef,  
> The tval and prob are not correct.
>
> The summary() code is
>     tval <- object$coef/object$se.coef
>     prob <- 2 * (1 - pnorm(abs(tval)))
>     ans$coefmat <- cbind(format(object$coef,digits=digits), 
> format(object$se.coef,digits=digits),
>         format(tval,digits=digits), prob=format.pval(prob,digits=digits))
>     dimnames(ans$coefmat) <- list(names(object$coef), c(" Estimate",
>         " Std. Error", " t value", "Pr(>|t|)"))
>     row.names(ans$coefmat)<-toupper(row.names(ans$coefmat))
>
> I can modify tval as
> tval<-subset(object$coef,object$mask)/object$se.coef
>     prob <- 2 * (1 - pnorm(abs(tval)))
>
> The subset functions removes all FALSE or masked MA terms from the coef 
> vector.  It will return a vector of length 14.  Now I have to expand 
> tval, prob and se.coef out to match the length of coef to get the the 
> results printed correctly.
>
> Can anyone help me with this? It would probably be a good thing to 
> include in future versions of rMetrics as well.
>
> Thank you
> Joe W. Byers
>
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: joe-byers.vcf
> Type: text/x-vcard
> Size: 104 bytes
> Desc: not available
> Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060706/a4c15bae/attachment-0001.vcf 
>
>   
>
summary.fARMA.html()
##Created by Joe W. Byers, The University of Tulsa
##This code is available under current GPL Licenses of R and Rmetrics
##This is modification of Original code from summary.fARMA from Rmetrics
## at http://www.itp.phys.ethz.ch/econophysics/R/

summary.fARMA.HTML<-function (object, doplot = FALSE, ...)
{
    ans <- NULL
    digits <- max(5, getOption("digits") - 4)
    x <- object
    object <- x at fit
    ans$call <- object$call
    ans$tsmodel <- object$tstitle
    ans$residuals <- as.vector(na.omit(object$residuals))
    if (length(ans$residuals) == 0) {
        ans$var <- 0
    }
    if (length(ans$residuals) > 0) {
        ans$var <- var(ans$residuals)
    }
    ans$sigma2 <- object$sigma2
    # the following lines replace to handle exongeous variables and 
masking of paramters
#    tval <- object$coef/object$se.coef
#    prob <- 2 * (1 - pnorm(abs(tval)))
#    ans$coefmat <- cbind(format(object$coef,digits=digits), 
format(object$se.coef,digits=digits),
#        format(tval,digits=digits), prob=format.pval(prob,digits=digits))
#    rownames(ans$coefmat)<-ans$coefmat$Row.names
#    ans$coefmat<-ans$coefmat[2:length(ans$coefmat)]
#    dimnames(ans$coefmat) <- list(names(object$coef), c(" Estimate",
#        " Std. Error", " t value", "Pr(>|t|)"))
#    row.names(ans$coefmat)<-toupper(row.names(ans$coefmat))
    tval<-subset(object$coef,object$mask)/object$se.coef
    prob <- 2 * (1 - pnorm(abs(tval)))
    ans$coefmat<-merge(format(object$coef,digits=digits),
      
cbind(format(object$se.coef,digits=digits),format(tval,digits=digits),format.pval(prob,digits=digits)),
        by.x="row.names",by.y="row.names",sort=FALSE,all.x=TRUE,all.y=FALSE)
    rownames(ans$coefmat)<-toupper(as.vector(ans$coefmat$Row.names))
    ans$coefmat<-ans$coefmat[2:length(ans$coefmat)]
       names(ans$coefmat) <- c(" Estimate"," Std. Error", " t value", 
"Pr(>|t|)")
    if (object$tsmodel == "ar") {
        ans$aic <- (object$n.used * (1 + log(2 * pi)) + object$n.used *
            log(ans$var) + 2 * length(object$coef))
    }
    if (object$tsmodel == "arma") {
        ans$aic <- (object$n.used * (1 + log(2 * pi)) + object$n.used *
            log(ans$var) + 2 * length(object$coef))
        ans$css <- object$css
    }
    if (object$tsmodel == "arima") {
        ans$aic <- object$aic
        ans$loglik <- object$loglik
    }
    if (object$tsmodel == "fracdiff") {
        doplot <- FALSE
    }
    HTML("Title: ")
    HTML(x at title)
    HTML("Call: ")
    HTML(object$call)
    HTML(c("Model: ", object$tstitle))#, "", sep = "")
    HTML("Coefficient(s):")
    digits <- max(5, getOption("digits") - 4)
    t1<-data.frame(object$coef)#copy to dataframe
    t1<-data.frame(t(t1)) #traspose for reporting
    names(t1)<-toupper(names(t1))
    row.names(t1)<-" " # rename row name
    HTML(t1,digits=digits)
    #HTML(print.default(format(object$coef, digits = digits), print.gap 
= 2, quote = FALSE))
    digits <- max(5, getOption("digits") - 4)
    if (length(object$residuals) > 2) {
        HTML("Residuals:")
        rq <- as.data.frame(t(structure(quantile(ans$residuals), names = 
c("Min",
            "1Q", "Median", "3Q", "Max"))))
        row.names(rq)<-' '
        HTML(rq,digits=digits)
        HTML("Moments: ")
        skewness <- sum((ans$residuals - 
mean(ans$residuals))^3/sqrt(var(ans$residuals))^3)/length(ans$residuals)
        kurtosis <- sum((ans$residuals - 
mean(ans$residuals))^4/var(ans$residuals)^2)/length(ans$residuals) -
            3
        stats <- as.data.frame(t(structure(c(skewness, kurtosis), names 
= c("Skewness",
            "Kurtosis"))))
        row.names(stats)<-" "
        HTML(stats,digits=digits)
    }
    HTML("Coefficient(s):")
    signif.stars <- getOption("show.signif.stars")
    #HTML(printCoefmat(ans$coefmat, digits=digits, signif.stars = 
signif.stars,    ...))
    HTML(ans$coefmat, digits=digits, signif.stars = signif.stars)
    if (x at fit$tsmodel == "ar") {
        t1<-data.frame(c(format(object$sigma2, digits = 
digits),format(round(object$aic, digits))),
            row.names=c("sigma^2 estimated as:       ","AIC 
Criterion:              "))
        names(t1)<-" "
        HTML(t1)
    }
    if (x at fit$tsmodel == "arma") {
        t1<-data.frame(c(format(object$sigma2, digits = 
digits),format(round(object$css, digits = digits))),
            row.names=c("sigma^2 estimated as:       ", "Conditional 
Sum-of-Squares: "))
            names(t1)<-" "
        HTML(t1)
    }
    if (x at fit$tsmodel == "arima") {
        cm <- object$call$method
        if (is.null(cm) || cm != "CSS") {
            t1<-data.frame(c(format(object$sigma2, digits = digits), 
format(round(object$loglik, digits)),
                format(round(object$aic, digits))),
                row.names=c("sigma^2 estimated as:       ", "log 
likelihood:       ",
                "AIC Criterion:        ",))
            names(t1)<-" "
            HTML(t1)
        }
        else {
            t1<-data.frame(c(format(object$sigma2, digits = digits), 
format(round(object$loglik, digits))),
                row.names=c("sigma^2 estimated as:       ", "log 
likelihood:       "))
            names(t1)<-" "
            HTML(t1)
        }
    }
    if (doplot)
        plot.fARMA(x, ...)
    HTML(c("Description: ",x at description))
    invisible()
}



-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 295 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060710/c622510c/attachment.vcf 

From hkahra at gmail.com  Wed Jul 12 14:12:11 2006
From: hkahra at gmail.com (Hannu Kahra)
Date: Wed, 12 Jul 2006 15:12:11 +0300
Subject: [R-sig-Finance] frontierMarkowitz in fPortfolio
Message-ID: <3d35a2ca0607120512x73328b54hd15651f15d8002e4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060712/0c863e10/attachment.pl 

From ggrothendieck at gmail.com  Wed Jul 12 14:43:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 08:43:54 -0400
Subject: [R-sig-Finance] frontierMarkowitz in fPortfolio
In-Reply-To: <3d35a2ca0607120512x73328b54hd15651f15d8002e4@mail.gmail.com>
References: <3d35a2ca0607120512x73328b54hd15651f15d8002e4@mail.gmail.com>
Message-ID: <971536df0607120543g569d4ba2j3def803def0422ba@mail.gmail.com>

Its in fExtremes.

On 7/12/06, Hannu Kahra <hkahra at gmail.com> wrote:
> Hi,
>
> I have been trying to plot the efficient frontier using frontierMarkowitz in
> fPortfolio:
>
> front <- frontierMarkowitz(returns, targetReturn=mean(returns),Rf=0.00403
> ,doplot=T,length=300,r.range=NULL,s.range=NULL,which="all")
>
> Using summary(front) I get the numerical values of the efficient frontier,
> but I also get the error message
> Error in plot.fPFOLIO(object, ...) : could not find function
> "interactivePlot"
> I cannot get the plots.
>
> It seems that somethins small but important must be missing. Any ideas?
>
> Thanks in advance,
> Hannu
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From a_malagoli at hotmail.com  Tue Jul 11 15:46:29 2006
From: a_malagoli at hotmail.com (Andrea Malagoli)
Date: Tue, 11 Jul 2006 09:46:29 -0400
Subject: [R-sig-Finance]  Fwd: Testing technical indicators
Message-ID: <BAY114-F336C6196B5E40018EFEF57FD680@phx.gbl>

An HTML attachment was scrubbed...
URL: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060711/2094a81e/attachment.html 

From jeff.a.ryan at gmail.com  Thu Jul 13 16:07:32 2006
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 13 Jul 2006 14:07:32 +0000 GMT
Subject: [R-sig-Finance] Fwd: Testing technical indicators
In-Reply-To: <BAY114-F336C6196B5E40018EFEF57FD680@phx.gbl>
References: <BAY114-F336C6196B5E40018EFEF57FD680@phx.gbl>
Message-ID: <637285609-1152799663-cardhu_blackberry.rim.net-409275247-@bwe032-cell00.bisx.prod.on.blackberry>

Hello Andrea,

While I can't promise that my C skills are (or will ever be) capable of putting together a well constructed/portable package - I'd be more than happy to give it a shot. I started to look at the source for the TA project, but got side-tracked on another effort. 

Timetable and my skills aside - if you're willing to send me what you've done thus far - that might be enough for me to figure out the rest. I primarily run R on a Solaris x86 box, as well as Mac OSX - so if I can get it running in these - it could be on its way to being portable.

Have you had any contact with the developer(s) at the TA-LIB project? That has been on "to do" list for a while... The lead seemed to be interested in R...

Jeff
  

-----Original Message-----
From: "Andrea Malagoli" <a_malagoli at hotmail.com>
Date: Tue, 11 Jul 2006 09:46:29 
To:r-sig-finance at stat.math.ethz.ch
Subject: [R-sig-Finance]  Fwd: Testing technical indicators

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From joe-byers at utulsa.edu  Thu Jul 13 22:56:54 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Thu, 13 Jul 2006 15:56:54 -0500
Subject: [R-sig-Finance] arma model results when exogenouse variables
 used	in, ARMA(p=5, q=(1-6, 19)) (Joe Byers)
In-Reply-To: <44B686B3.5090105@pdf.com>
References: <mailman.13.1152266405.22923.r-sig-finance@stat.math.ethz.ch>
	<44B2B933.70307@utulsa.edu> <44B66631.3050003@pdf.com>
	<44B68271.6030205@utulsa.edu> <44B686B3.5090105@pdf.com>
Message-ID: <44B6B396.60906@utulsa.edu>

All I have a simply script that replicates by error.  I have include a 
summary.fARMA.new function because the digits variables needed 
modification and printCoefmat did not print correctly.  The code is
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Example to replicate ARMA model with exogenous variables error in the 
fARMA.summary functions
#set up enviroment and libraries
library(date); 
library(car);library(tseries);library(Hmisc);library(fBasics);
library(R2HTML);#library(gdata);
library(partsm);library(tseries);library(pear);
library(fBasics);library(fSeries); library(cba);
summary.fARMA.new<-function (object, doplot = TRUE, ...)
{
    digits <- max(5, getOption("digits") - 4)      #added by joe w. byers
    ans = NULL
    x = object
    object = x at fit
    ans$call = object$call
    ans$tsmodel = object$tstitle
    ans$residuals = as.vector(na.omit(object$residuals))
    if (length(ans$residuals) == 0) {
        ans$var = 0
    }
    if (length(ans$residuals) > 0) {
        ans$var = var(ans$residuals)
    }
    ans$sigma2 = object$sigma2
    # the following lines replace to handle exongeous variables and 
masking of paramters
#    tval <- object$coef/object$se.coef
#    prob <- 2 * (1 - pnorm(abs(tval)))
#    ans$coefmat <- cbind(format(object$coef,digits=digits), 
format(object$se.coef,digits=digits),
#        format(tval,digits=digits), prob=format.pval(prob,digits=digits))
#    rownames(ans$coefmat)<-ans$coefmat$Row.names
#    ans$coefmat<-ans$coefmat[2:length(ans$coefmat)]
#    dimnames(ans$coefmat) <- list(names(object$coef), c(" Estimate",
#        " Std. Error", " t value", "Pr(>|t|)"))
#    row.names(ans$coefmat)<-toupper(row.names(ans$coefmat))
    tval<-subset(object$coef,object$mask)/object$se.coef
    prob <- 2 * (1 - pnorm(abs(tval)))
    ans$coefmat<-merge(format(object$coef,digits=digits),
      
cbind(format(object$se.coef,digits=digits),format(tval,digits=digits),format.pval(prob,digits=digits)),
        by.x="row.names",by.y="row.names",sort=FALSE,all.x=TRUE,all.y=FALSE)
    rownames(ans$coefmat)<-toupper(as.vector(ans$coefmat$Row.names)) 
#make the row indexes the parameter names
    ans$coefmat<-ans$coefmat[2:length(ans$coefmat)] # drop first column 
that merge create of row.names
       names(ans$coefmat) <- c(" Estimate"," Std. Error", " t value", 
"Pr(>|t|)")
    if (object$tsmodel == "ar") {
        ans$aic = (object$n.used * (1 + log(2 * pi)) + object$n.used *
            log(ans$var) + 2 * length(object$coef))
    }
    if (object$tsmodel == "arma") {
        ans$aic = (object$n.used * (1 + log(2 * pi)) + object$n.used *
            log(ans$var) + 2 * length(object$coef))
        ans$css = object$css
    }
    if (object$tsmodel == "arima") {
        ans$aic = object$aic
        ans$loglik = object$loglik
    }
    if (object$tsmodel == "fracdiff") {
        doplot = FALSE
    }
    cat("\nTitle:\n ")
    cat(x at title, "\n")
    cat("\nCall:\n ")
    cat(paste(deparse(object$call), sep = "\n", collapse = "\n"),
        "\n", sep = "")
    cat("\nModel:\n ", object$tstitle, "\n", sep = "")
    cat("\nCoefficient(s):\n")
    digits = max(5, getOption("digits") - 4) # changed 4 to 5 by joe w. 
byers
    print.default(format(object$coef, digits = digits), print.gap = 2,
        quote = FALSE)
    digits = max(5, getOption("digits") - 4) # changed 4 to 5 by joe w. 
byers
    if (length(object$residuals) > 2) {
        cat("\nResiduals:\n")
        rq = structure(quantile(ans$residuals), names = c("Min",
            "1Q", "Median", "3Q", "Max"))
        print(rq, digits = digits)
        cat("\nMoments: \n")
        skewness = sum((ans$residuals - 
mean(ans$residuals))^3/sqrt(var(ans$residuals))^3)/length(ans$residuals)
        kurtosis = sum((ans$residuals - 
mean(ans$residuals))^4/var(ans$residuals)^2)/length(ans$residuals) -
            3
        stats = structure(c(skewness, kurtosis), names = c("Skewness",
            "Kurtosis"))
        print(stats, digits = digits)
    }
    cat("\nCoefficient(s):\n")
    signif.stars = getOption("show.signif.stars")
    print(ans$coefmat, digits = digits, signif.stars = signif.stars,
        ...) #changes printCoefmat to print by joe w. byers
    cat("\n")
    if (x at fit$tsmodel == "ar") {
        cat("sigma^2 estimated as:       ", format(object$var,
            digits = digits), "\n")
        cat("AIC Criterion:              ", format(round(object$aic,
            2)), "\n")
    }
    if (x at fit$tsmodel == "arma") {
        cat("sigma^2 estimated as:       ", format(object$sigma2,
            digits = digits), "\n")
        cat("Conditional Sum-of-Squares: ", format(round(object$css,
            digits = 2)), "\n")
    }
    if (x at fit$tsmodel == "arima") {
        cm = object$call$method
        if (is.null(cm) || cm != "CSS")
            cat("sigma^2 estimated as: ", format(object$sigma2,
                digits = digits), "\nlog likelihood:       ",
                format(round(object$loglik, 2)), "\nAIC 
Criterion:        ",
                format(round(object$aic, 2)), "\n", sep = "")
        else cat("sigma^2 estimated as: ", format(object$sigma2,
            digits = digits), "\npart log likelihood:  ", 
format(round(object$loglik,
            2)), "\n", sep = "")
    }
    if (doplot)
        plot.fARMA(x, ...)
    cat("\nDescription:\n ")
    cat(x at description, "\n\n")
    invisible()
}
# generate white noise vector
set.seed(1)           # so you can reproduce the results
v = rnorm(1000,1,1)    # v contains 1000 iid N(1,1) variates
e=rnorm(1000,0,1)
x = cumsum(v)         # x is a random walk with drift = 1
plot.ts(x)            # have a look if you don't believe me
#generate ARMA(1,(1,3)) process with trend
#ar(1)=.5 MA(1)=.05 MA(3)=-.25 trend=.025 mean=1
trend=.025;ma1=.05; ma3=-.25; ar1=.5;
len<-length(v);
data<-ar1*v[3:(len-1)]+ma1*e[3:(len-1)]+ma3*e[1:(len-3)]
plot.ts(data)

# estimate ARMA model using fARMA()
fixed=c(NA,NA,0,NA,NA);#1 ar terms, 3 ma terms, intercept
ts.results<-armaFit(data~arma(1,3),include.mean=TRUE,optim.control=list(maxit=500),fixed=fixed);

#set the graphics to pause since I am using sciViews
par(ask=TRUE);
#produce results with error using summary.fARMA()
summary(ts.results)

#produce results without error using modified summary.fARMA.new()
summary.fARMA.new(ts.results)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Spencer Graves wrote:
> Hi, Joe:
>
>       Not a problem.  However, if you had 26 coefficients, you had 
> more than a simple ARMA(1,1).  With only 14 computed standard errors 
> for 26 coefficients, I gather you fixed a dozen coefficients.  Thus, 
> if I had had time to experiment, I could have tried an ARMA(1,1), 
> fixing one coefficient.
>
>       Also, how does an arma(5, 19) produce 26 coefficients?  5 AR + 
> 19 MA = 24, + 1 intercept = 25.  Does this model include the standard 
> deviation of the whitened residuals?
>
>       If you'll permit another question:  An arma(5, 19) sounds like 
> overfitting to me.  What kinds of plots have you made?  Have you 
> plotted the data vs. time?  Have you made normal probability plots of 
> the the data and of whitened residuals from a simple model?  Have you 
> tried acf and pacf of the absolute values of whitened residuals?
>
>       Best Wishes,
>       Spencer Graves
>
> Joe Byers wrote:
>> Spencer,
>>
>> Thank you for your reply.  I knew I should include an example, but 
>> using my data would not be a simple one to provide.  I hope to 
>> produce a short one today since I have the afternoon.  A simple 
>> simulated data of an ARMA(1,1) with a trend and exogenous component.  
>> Hopefully my thesis advisee will not take up too much of my afternoon.
>>
>> Please bear with me.
>>
>> Thank you
>> Joe W. Byers
>>
>> PS Dirk,  How was your trip to Europe?
>>
>> Spencer Graves wrote:
>>> Hi, Diethelm, Martin, Dirk, Joe:
>>>
>>> DIETHELM, MARTIN, DIRK:
>>>
>>>       Joe Byers identified a problem and an apparent fix for 
>>> summary.fARMA;  see below.  Under certain circumstances, 
>>> summary.fARMA apparently computed 14 standard errors for 26 
>>> coefficients.  I didn't see a simple, self-contained / replicatable 
>>> example in his post, and I haven't found the time to try to create 
>>> one.  However, his suggested code changes look plausible to me.
>>>
>>>       How do you think we should react to this and similar reports?
>>>
>>>
>>> JOE:
>>>
>>>       Thanks for the suggested fix.  For future reference, you might 
>>> increase the chances for a quick, helpful reply by including a 
>>> simple, self-contained / replicatable example?  It also helps to 
>>> mention the name of the package you are using.  I have replied to 
>>> many time series questions, partly as a means of familiarizing 
>>> myself with the time series capabilities available in R.  Without 
>>> such an example, I often try to generate one, modifying an example 
>>> from a help page or using made-up numbers;  if random numbers are 
>>> involved, one should always include 'set.seed'.  For the 'armaFit 
>>> problem you reported, I have not found the time to do so.  Also, I 
>>> don't know why others have not responded to your question, but your 
>>> "Subject" line may not have caught their eye.  For other suggestions 
>>> on how to increase your chances of a quicker resolution of an issue, 
>>> see the posting guide! "www.R-project.org/posting-guide.html".
>>>
>>>       Best Wishes,
>>>       Spencer Graves
>>>
>>> Joe Byers wrote:
>>>> All,
>>>>
>>>> I have developed a modification for the summary.fARMA() to handle 
>>>> the different vector sizes.  I am not sure if this is the best or 
>>>> most efficient coding and would appreciate your comments.  The 
>>>> original code is commented out and my new code follows as
>>>>    # the following lines replace to handle exongeous variables and 
>>>> masking of paramters
>>>> #    tval <- object$coef/object$se.coef
>>>> #    prob <- 2 * (1 - pnorm(abs(tval)))
>>>> #    ans$coefmat <- cbind(format(object$coef,digits=digits), 
>>>> format(object$se.coef,digits=digits),
>>>> #        format(tval,digits=digits), 
>>>> prob=format.pval(prob,digits=digits))
>>>> #    rownames(ans$coefmat)<-ans$coefmat$Row.names
>>>> #    ans$coefmat<-ans$coefmat[2:length(ans$coefmat)]
>>>> #    dimnames(ans$coefmat) <- list(names(object$coef), c(" Estimate",
>>>> #        " Std. Error", " t value", "Pr(>|t|)"))
>>>> #    row.names(ans$coefmat)<-toupper(row.names(ans$coefmat))
>>>>    tval<-subset(object$coef,object$mask)/object$se.coef
>>>>    prob <- 2 * (1 - pnorm(abs(tval)))
>>>>    ans$coefmat<-merge(format(object$coef,digits=digits),
>>>>      
>>>> cbind(format(object$se.coef,digits=digits),format(tval,digits=digits),format.pval(prob,digits=digits)), 
>>>>
>>>>        
>>>> by.x="row.names",by.y="row.names",sort=FALSE,all.x=TRUE,all.y=FALSE)
>>>>    rownames(ans$coefmat)<-toupper(as.vector(ans$coefmat$Row.names)) 
>>>> #make the row indexes the parameter names
>>>>    ans$coefmat<-ans$coefmat[2:length(ans$coefmat)] # drop first 
>>>> column that merge create of row.names
>>>>       names(ans$coefmat) <- c(" Estimate"," Std. Error", " t 
>>>> value", "Pr(>|t|)")
>>>>
>>>> This code worked in my modified summary.fARMA() that outputs to 
>>>> html files.  The only problem is that the merge creates a 
>>>> data.frame of the intersected rows of the two data sets and then 
>>>> adds the outer (non-intersecting) rows from object$coef 
>>>> data.frame.  I can not figure out how to keep the order based on 
>>>> object$coef.  The full method is include below for reference.
>>>>
>>>> Thank you
>>>> Joe W. Byers
>>>> Professor of Finance
>>>> The University of Tulsa
>>>>
>>>>
>>>>>    4. arma model results when exogenouse variables used    in
>>>>>       ARMA(p=5, q=(1-6, 19)) (Joe Byers)
>>>>> ------------------------------
>>>>>
>>>>> Message: 4
>>>>> Date: Thu, 06 Jul 2006 11:38:02 -0500
>>>>> From: Joe Byers <joe-byers at utulsa.edu>
>>>>> Subject: [R-sig-Finance] arma model results when exogenouse variables
>>>>>     used    in ARMA(p=5, q=(1-6, 19))
>>>>> To: r-sig-finance at stat.math.ethz.ch
>>>>> Message-ID: <44AD3C6A.9000204 at utulsa.edu>
>>>>> Content-Type: text/plain; charset="iso-8859-1"
>>>>>
>>>>> All,
>>>>>
>>>>> I posted a message earlier about fitting an ARMA(p=5,q=(1-6,19)) 
>>>>> with exnogenouse variables (xreg=exovars), and masking 
>>>>> (fixed=fixedparms) the MA terms 7-18 to get the model to run. I am 
>>>>> reposting some of the message to help with understanding the 
>>>>> summary() function problem
>>>>>
>>>>> My code is
>>>>>     fixed=c(rep(NA,5),rep(NA,6),rep(0,12),NA,NA);#5 ar terms, 19 ma
>>>>> terms fixed 7-18 lag ma term, intercept
>>>>>
>>>>>
>>>>>     fixed<-c(fixed,NA); #add the lin.trend term
>>>>>            
>>>>>  ts.results.2.ma<-armaFit(datats~arma(5,19),xreg=cbind(Lin.Trend=d$factor.Lin.Trend.), 
>>>>>
>>>>>       include.mean=TRUE,optim.control=list(maxit=500),fixed=fixed);
>>>>>     summary.fARMA.HTML(ts.results.2.ma,title="AR(5) MA(1:6,19) 
>>>>> with Intercept and Linear Trend");# this function is a 
>>>>> modification of summary.fARMA to work with r2HTML replacing the 
>>>>> cat() functions.
>>>>>
>>>>> The problem I have is that the @fit$coef(26) and @fit$se.coef(14) 
>>>>> are of different lengths causing the t-stats calculation is 
>>>>> summary to issue warnings.
>>>>> Warning messages:
>>>>> 1: longer object length
>>>>>         is not a multiple of shorter object length in: 
>>>>> object$coef/object$se.coef
>>>>> 2: number of rows of result
>>>>>         is not a multiple of vector length (arg 2) in: cbind(1, 
>>>>> format(object$coef, digits = digits), format(object$se.coef,  The 
>>>>> tval and prob are not correct.
>>>>>
>>>>> The summary() code is
>>>>>     tval <- object$coef/object$se.coef
>>>>>     prob <- 2 * (1 - pnorm(abs(tval)))
>>>>>     ans$coefmat <- cbind(format(object$coef,digits=digits), 
>>>>> format(object$se.coef,digits=digits),
>>>>>         format(tval,digits=digits), 
>>>>> prob=format.pval(prob,digits=digits))
>>>>>     dimnames(ans$coefmat) <- list(names(object$coef), c(" Estimate",
>>>>>         " Std. Error", " t value", "Pr(>|t|)"))
>>>>>     row.names(ans$coefmat)<-toupper(row.names(ans$coefmat))
>>>>>
>>>>> I can modify tval as
>>>>> tval<-subset(object$coef,object$mask)/object$se.coef
>>>>>     prob <- 2 * (1 - pnorm(abs(tval)))
>>>>>
>>>>> The subset functions removes all FALSE or masked MA terms from the 
>>>>> coef vector.  It will return a vector of length 14.  Now I have to 
>>>>> expand tval, prob and se.coef out to match the length of coef to 
>>>>> get the the results printed correctly.
>>>>>
>>>>> Can anyone help me with this? It would probably be a good thing to 
>>>>> include in future versions of rMetrics as well.
>>>>>
>>>>> Thank you
>>>>> Joe W. Byers
>>>>>
>>>>> -------------- next part --------------
>>>>> A non-text attachment was scrubbed...
>>>>> Name: joe-byers.vcf
>>>>> Type: text/x-vcard
>>>>> Size: 104 bytes
>>>>> Desc: not available
>>>>> Url : 
>>>>> https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060706/a4c15bae/attachment-0001.vcf 
>>>>>
>>>>>  
>>>> summary.fARMA.html()
>>>> ##Created by Joe W. Byers, The University of Tulsa
>>>> ##This code is available under current GPL Licenses of R and Rmetrics
>>>> ##This is modification of Original code from summary.fARMA from 
>>>> Rmetrics
>>>> ## at http://www.itp.phys.ethz.ch/econophysics/R/
>>>>
>>>> summary.fARMA.HTML<-function (object, doplot = FALSE, ...)
>>>> {
>>>>    ans <- NULL
>>>>    digits <- max(5, getOption("digits") - 4)
>>>>    x <- object
>>>>    object <- x at fit
>>>>    ans$call <- object$call
>>>>    ans$tsmodel <- object$tstitle
>>>>    ans$residuals <- as.vector(na.omit(object$residuals))
>>>>    if (length(ans$residuals) == 0) {
>>>>        ans$var <- 0
>>>>    }
>>>>    if (length(ans$residuals) > 0) {
>>>>        ans$var <- var(ans$residuals)
>>>>    }
>>>>    ans$sigma2 <- object$sigma2
>>>>    # the following lines replace to handle exongeous variables and 
>>>> masking of paramters
>>>> #    tval <- object$coef/object$se.coef
>>>> #    prob <- 2 * (1 - pnorm(abs(tval)))
>>>> #    ans$coefmat <- cbind(format(object$coef,digits=digits), 
>>>> format(object$se.coef,digits=digits),
>>>> #        format(tval,digits=digits), 
>>>> prob=format.pval(prob,digits=digits))
>>>> #    rownames(ans$coefmat)<-ans$coefmat$Row.names
>>>> #    ans$coefmat<-ans$coefmat[2:length(ans$coefmat)]
>>>> #    dimnames(ans$coefmat) <- list(names(object$coef), c(" Estimate",
>>>> #        " Std. Error", " t value", "Pr(>|t|)"))
>>>> #    row.names(ans$coefmat)<-toupper(row.names(ans$coefmat))
>>>>    tval<-subset(object$coef,object$mask)/object$se.coef
>>>>    prob <- 2 * (1 - pnorm(abs(tval)))
>>>>    ans$coefmat<-merge(format(object$coef,digits=digits),
>>>>      
>>>> cbind(format(object$se.coef,digits=digits),format(tval,digits=digits),format.pval(prob,digits=digits)), 
>>>>
>>>>        
>>>> by.x="row.names",by.y="row.names",sort=FALSE,all.x=TRUE,all.y=FALSE)
>>>>    rownames(ans$coefmat)<-toupper(as.vector(ans$coefmat$Row.names))
>>>>    ans$coefmat<-ans$coefmat[2:length(ans$coefmat)]
>>>>       names(ans$coefmat) <- c(" Estimate"," Std. Error", " t 
>>>> value", "Pr(>|t|)")
>>>>    if (object$tsmodel == "ar") {
>>>>        ans$aic <- (object$n.used * (1 + log(2 * pi)) + object$n.used *
>>>>            log(ans$var) + 2 * length(object$coef))
>>>>    }
>>>>    if (object$tsmodel == "arma") {
>>>>        ans$aic <- (object$n.used * (1 + log(2 * pi)) + object$n.used *
>>>>            log(ans$var) + 2 * length(object$coef))
>>>>        ans$css <- object$css
>>>>    }
>>>>    if (object$tsmodel == "arima") {
>>>>        ans$aic <- object$aic
>>>>        ans$loglik <- object$loglik
>>>>    }
>>>>    if (object$tsmodel == "fracdiff") {
>>>>        doplot <- FALSE
>>>>    }
>>>>    HTML("Title: ")
>>>>    HTML(x at title)
>>>>    HTML("Call: ")
>>>>    HTML(object$call)
>>>>    HTML(c("Model: ", object$tstitle))#, "", sep = "")
>>>>    HTML("Coefficient(s):")
>>>>    digits <- max(5, getOption("digits") - 4)
>>>>    t1<-data.frame(object$coef)#copy to dataframe
>>>>    t1<-data.frame(t(t1)) #traspose for reporting
>>>>    names(t1)<-toupper(names(t1))
>>>>    row.names(t1)<-" " # rename row name
>>>>    HTML(t1,digits=digits)
>>>>    #HTML(print.default(format(object$coef, digits = digits), 
>>>> print.gap = 2, quote = FALSE))
>>>>    digits <- max(5, getOption("digits") - 4)
>>>>    if (length(object$residuals) > 2) {
>>>>        HTML("Residuals:")
>>>>        rq <- as.data.frame(t(structure(quantile(ans$residuals), 
>>>> names = c("Min",
>>>>            "1Q", "Median", "3Q", "Max"))))
>>>>        row.names(rq)<-' '
>>>>        HTML(rq,digits=digits)
>>>>        HTML("Moments: ")
>>>>        skewness <- sum((ans$residuals - 
>>>> mean(ans$residuals))^3/sqrt(var(ans$residuals))^3)/length(ans$residuals) 
>>>>
>>>>        kurtosis <- sum((ans$residuals - 
>>>> mean(ans$residuals))^4/var(ans$residuals)^2)/length(ans$residuals) -
>>>>            3
>>>>        stats <- as.data.frame(t(structure(c(skewness, kurtosis), 
>>>> names = c("Skewness",
>>>>            "Kurtosis"))))
>>>>        row.names(stats)<-" "
>>>>        HTML(stats,digits=digits)
>>>>    }
>>>>    HTML("Coefficient(s):")
>>>>    signif.stars <- getOption("show.signif.stars")
>>>>    #HTML(printCoefmat(ans$coefmat, digits=digits, signif.stars = 
>>>> signif.stars,    ...))
>>>>    HTML(ans$coefmat, digits=digits, signif.stars = signif.stars)
>>>>    if (x at fit$tsmodel == "ar") {
>>>>        t1<-data.frame(c(format(object$sigma2, digits = 
>>>> digits),format(round(object$aic, digits))),
>>>>            row.names=c("sigma^2 estimated as:       ","AIC 
>>>> Criterion:              "))
>>>>        names(t1)<-" "
>>>>        HTML(t1)
>>>>    }
>>>>    if (x at fit$tsmodel == "arma") {
>>>>        t1<-data.frame(c(format(object$sigma2, digits = 
>>>> digits),format(round(object$css, digits = digits))),
>>>>            row.names=c("sigma^2 estimated as:       ", "Conditional 
>>>> Sum-of-Squares: "))
>>>>            names(t1)<-" "
>>>>        HTML(t1)
>>>>    }
>>>>    if (x at fit$tsmodel == "arima") {
>>>>        cm <- object$call$method
>>>>        if (is.null(cm) || cm != "CSS") {
>>>>            t1<-data.frame(c(format(object$sigma2, digits = digits), 
>>>> format(round(object$loglik, digits)),
>>>>                format(round(object$aic, digits))),
>>>>                row.names=c("sigma^2 estimated as:       ", "log 
>>>> likelihood:       ",
>>>>                "AIC Criterion:        ",))
>>>>            names(t1)<-" "
>>>>            HTML(t1)
>>>>        }
>>>>        else {
>>>>            t1<-data.frame(c(format(object$sigma2, digits = digits), 
>>>> format(round(object$loglik, digits))),
>>>>                row.names=c("sigma^2 estimated as:       ", "log 
>>>> likelihood:       "))
>>>>            names(t1)<-" "
>>>>            HTML(t1)
>>>>        }
>>>>    }
>>>>    if (doplot)
>>>>        plot.fARMA(x, ...)
>>>>    HTML(c("Description: ",x at description))
>>>>    invisible()
>>>> }
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 295 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060713/705147a8/attachment.vcf 

From kevinramoutar at yahoo.co.uk  Fri Jul 14 00:09:20 2006
From: kevinramoutar at yahoo.co.uk (Kevin Ramoutar)
Date: Thu, 13 Jul 2006 22:09:20 +0000 (GMT)
Subject: [R-sig-Finance] R-SIG-Finance Digest, Vol 26, Issue 9
In-Reply-To: <mailman.13.1152784804.5134.r-sig-finance@stat.math.ethz.ch>
Message-ID: <20060713220920.27057.qmail@web25505.mail.ukl.yahoo.com>

Hannu,
 
I hope I am replying using the proper protocol. Forgive me if I am not.
 
I also ran into the same problem your are having. After getting the Error in plot.fPFOLIO(object, ...) : could not find function
"interactivePlot"
 message.
 
I listed the objects in my workspace and noticed the following:
 
ls()
[1] "adjdata"     "gretldata"   "myportfolio" "plot.1"      "plot.2"     
[6] "plot.3"      "plot.4"      "plot.5"      "plot.6"
 
The objects "plot.1"      "plot.2"     
[6] "plot.3"      "plot.4"      "plot.5"      "plot.6" contain code for the plots.
 
If you use the following code you should be able to get the plots.
 
plot.1(myportfolio)
plot.2(myportfolio) etc.
 
where myportfolio contains the results of the frontierMarkowitz function.
 
Hope this helps.
 
Regards
Kevin

----- Original Message ----
From: r-sig-finance-request at stat.math.ethz.ch
To: r-sig-finance at stat.math.ethz.ch
Sent: Thursday, 13 July, 2006 6:00:04 AM
Subject: R-SIG-Finance Digest, Vol 26, Issue 9


Send R-SIG-Finance mailing list submissions to
    r-sig-finance at stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
    https://stat.ethz.ch/mailman/listinfo/r-sig-finance
or, via email, send a message with subject or body 'help' to
    r-sig-finance-request at stat.math.ethz.ch

You can reach the person managing the list at
    r-sig-finance-owner at stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-SIG-Finance digest..."


Today's Topics:

   1. frontierMarkowitz in fPortfolio (Hannu Kahra)
   2. Re: frontierMarkowitz in fPortfolio (Gabor Grothendieck)


----------------------------------------------------------------------

Message: 1
Date: Wed, 12 Jul 2006 15:12:11 +0300
From: "Hannu Kahra" <hkahra at gmail.com>
Subject: [R-sig-Finance] frontierMarkowitz in fPortfolio
To: R-SIG-Finance at stat.math.ethz.ch
Message-ID:
    <3d35a2ca0607120512x73328b54hd15651f15d8002e4 at mail.gmail.com>
Content-Type: text/plain

Hi,

I have been trying to plot the efficient frontier using frontierMarkowitz in
fPortfolio:

front <- frontierMarkowitz(returns, targetReturn=mean(returns),Rf=0.00403
,doplot=T,length=300,r.range=NULL,s.range=NULL,which="all")

Using summary(front) I get the numerical values of the efficient frontier,
but I also get the error message
Error in plot.fPFOLIO(object, ...) : could not find function
"interactivePlot"
I cannot get the plots.

It seems that somethins small but important must be missing. Any ideas?

Thanks in advance,
Hannu

    [[alternative HTML version deleted]]



------------------------------

Message: 2
Date: Wed, 12 Jul 2006 08:43:54 -0400
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
Subject: Re: [R-sig-Finance] frontierMarkowitz in fPortfolio
To: "Hannu Kahra" <hkahra at gmail.com>
Cc: R-SIG-Finance at stat.math.ethz.ch
Message-ID:
    <971536df0607120543g569d4ba2j3def803def0422ba at mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Its in fExtremes.

On 7/12/06, Hannu Kahra <hkahra at gmail.com> wrote:
> Hi,
>
> I have been trying to plot the efficient frontier using frontierMarkowitz in
> fPortfolio:
>
> front <- frontierMarkowitz(returns, targetReturn=mean(returns),Rf=0.00403
> ,doplot=T,length=300,r.range=NULL,s.range=NULL,which="all")
>
> Using summary(front) I get the numerical values of the efficient frontier,
> but I also get the error message
> Error in plot.fPFOLIO(object, ...) : could not find function
> "interactivePlot"
> I cannot get the plots.
>
> It seems that somethins small but important must be missing. Any ideas?
>
> Thanks in advance,
> Hannu
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>



------------------------------

_______________________________________________
R-SIG-Finance mailing list
R-SIG-Finance at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


End of R-SIG-Finance Digest, Vol 26, Issue 9


From ivan.kalafatic at gmail.com  Fri Jul 14 17:26:23 2006
From: ivan.kalafatic at gmail.com (Ivan Kalafatic)
Date: Fri, 14 Jul 2006 16:26:23 +0100
Subject: [R-sig-Finance] Help for updating package
Message-ID: <6dbf89a50607140826g4afe4996lf5e5e500af7412ec@mail.gmail.com>

I have a problem with garchFit fuction in fSeries package. I found the
following reply on one of the R list:
"GARCH-Modelling is not easy, and indeed for your dataset the default
"Sequential Quadratic Programming" solver doesn't converge. I observed
this also for some other time series. There is already an updated
version on the server,
https://svn.r-project.org/Rmetrics/trunk/fSeries/
which uses improved control parameter settings as default values. With
this version there exist no convergence problems"

How to update my version of fSeries with the provided link?

Thank you.


From icos.atropa at gmail.com  Sat Jul 15 23:16:53 2006
From: icos.atropa at gmail.com (icosa atropa)
Date: Sat, 15 Jul 2006 15:16:53 -0600
Subject: [R-sig-Finance] Problems subsetting zoo objects
Message-ID: <681d07c20607151416g5a238840r457401cd7ddb058d@mail.gmail.com>

Hello,

After 6 months of using both the its and the zoo packages, I've grown
fond of the zoo package, but I still struggle with subsetting
zoo objects.

I'm trying to remove outliers from my records by setting them to NA; I
can do some dataframe-type subsetting with "its" objects, but I'm
still confused about zoo's rules.

###An example -------------------
require(zoo); require(its)
my.data = data.frame(depth=c(5, 6, 30), temp=c(20, 21, 100))
my.index = as.POSIXct(c("1999-02-10", "1999-02-11", "1999-02-12"))
my.zoo = zoo(my.data, order.by=my.index)
my.its = as.its(my.zoo)

#Questions
#1 -   Not supposed to work?
my.zoo$depth  #NULL
my.its$zoo   #NULL
dynlm(depth ~ temp, data=my.zoo)  #works
my.data$depth  # works

#2 - I expected the following line to return values
my.zoo[my.zoo[,1] > 10]    # returns column names but no values
my.zoo[my.zoo[,1] > 10]  = NA   # Still, this _does_ work
my.its[my.its[,1] > 10 ]               #  And this returns values

my.zoo = zoo(my.data, order.by=my.index)  # reset my.zoo

#3 - I try to isolate my confusion:
my.zoo[c(T,F,F),1]  == 5 # True
my.zoo[c(T,F,F),2] == 20 # True
my.zoo[c(T,F,F),1:2]  == 5 # True.  Why not ==c(5,20)?
#i.e.
my.zoo[c(T,F,F),1:2] == my.zoo[c(T,F,F),c(T,F)]   #True
my.zoo[c(T,F,F),1:2] == my.zoo[c(T,F,F),c(F,T)]  #False
my.zoo[c(F,F,T), c(T,F)] ==  my.zoo[c(F,F,T), c(T,T)]  #True

#End of Example---------------------------------

I'd greatly appreciate any insights or suggests.
Thanks in advance,

Christian Gunning
Masters Student
University of New Mexico
Dept of Water Resources


From ggrothendieck at gmail.com  Sun Jul 16 01:27:20 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 15 Jul 2006 19:27:20 -0400
Subject: [R-sig-Finance] Problems subsetting zoo objects
In-Reply-To: <681d07c20607151416g5a238840r457401cd7ddb058d@mail.gmail.com>
References: <681d07c20607151416g5a238840r457401cd7ddb058d@mail.gmail.com>
Message-ID: <971536df0607151627y66e0e183s184a59bafae3b27d@mail.gmail.com>

$ is not valid.  zoo is modelled on ts which also
does not support $.

In the case of logical subscripting its a bug.  (Thanks.)
It will be fixed.  In the interim do it this way:

	my.zoo[which(my.zoo[,1] > 10),]
	my.zoo[which(my.zoo[,1] > 10),] <- NA


On 7/15/06, icosa atropa <icos.atropa at gmail.com> wrote:
> Hello,
>
> After 6 months of using both the its and the zoo packages, I've grown
> fond of the zoo package, but I still struggle with subsetting
> zoo objects.
>
> I'm trying to remove outliers from my records by setting them to NA; I
> can do some dataframe-type subsetting with "its" objects, but I'm
> still confused about zoo's rules.
>
> ###An example -------------------
> require(zoo); require(its)
> my.data = data.frame(depth=c(5, 6, 30), temp=c(20, 21, 100))
> my.index = as.POSIXct(c("1999-02-10", "1999-02-11", "1999-02-12"))
> my.zoo = zoo(my.data, order.by=my.index)
> my.its = as.its(my.zoo)
>
> #Questions
> #1 -   Not supposed to work?
> my.zoo$depth  #NULL
> my.its$zoo   #NULL
> dynlm(depth ~ temp, data=my.zoo)  #works
> my.data$depth  # works
>
> #2 - I expected the following line to return values
> my.zoo[my.zoo[,1] > 10]    # returns column names but no values
> my.zoo[my.zoo[,1] > 10]  = NA   # Still, this _does_ work
> my.its[my.its[,1] > 10 ]               #  And this returns values
>
> my.zoo = zoo(my.data, order.by=my.index)  # reset my.zoo
>
> #3 - I try to isolate my confusion:
> my.zoo[c(T,F,F),1]  == 5 # True
> my.zoo[c(T,F,F),2] == 20 # True
> my.zoo[c(T,F,F),1:2]  == 5 # True.  Why not ==c(5,20)?
> #i.e.
> my.zoo[c(T,F,F),1:2] == my.zoo[c(T,F,F),c(T,F)]   #True
> my.zoo[c(T,F,F),1:2] == my.zoo[c(T,F,F),c(F,T)]  #False
> my.zoo[c(F,F,T), c(T,F)] ==  my.zoo[c(F,F,T), c(T,T)]  #True
>
> #End of Example---------------------------------
>
> I'd greatly appreciate any insights or suggests.
> Thanks in advance,
>
> Christian Gunning
> Masters Student
> University of New Mexico
> Dept of Water Resources
>


From John.Kerpel at infores.com  Fri Jul 14 19:07:59 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Fri, 14 Jul 2006 12:07:59 -0500
Subject: [R-sig-Finance] [R] Help for updating package
Message-ID: <44A8B25381923D4F93B74B2676A50F6D02E0055C@MAIL1.infores.com>

Ivan:

I'm guessing you've got the latest version of fSeries if you downloaded
it recently from CRAN.

I've noticed the convergence problems too.  I changed to
algorithm="lbfgsb" and haven't had a problem since.  (I'm running
Windows XP and R 2.3.1)

John

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ivan Kalafatic
Sent: Friday, July 14, 2006 10:26 AM
To: r-help at stat.math.ethz.ch; r-sig-finance at stat.math.ethz.ch
Subject: [R] Help for updating package

I have a problem with garchFit fuction in fSeries package. I found the
following reply on one of the R list:
"GARCH-Modelling is not easy, and indeed for your dataset the default
"Sequential Quadratic Programming" solver doesn't converge. I observed
this also for some other time series. There is already an updated
version on the server,
https://svn.r-project.org/Rmetrics/trunk/fSeries/
which uses improved control parameter settings as default values. With
this version there exist no convergence problems"

How to update my version of fSeries with the provided link?

Thank you.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From bbands at gmail.com  Tue Jul 18 18:04:33 2006
From: bbands at gmail.com (BBands)
Date: Tue, 18 Jul 2006 09:04:33 -0700
Subject: [R-sig-Finance] Backtesting speed
In-Reply-To: <829e6c8a0607022202r8e6c165v527847860ed9e368@mail.gmail.com>
References: <829e6c8a0607022202r8e6c165v527847860ed9e368@mail.gmail.com>
Message-ID: <6e8360ad0607180904o3aa673dfk6982b06e1f5b4f20@mail.gmail.com>

On 7/2/06, Manoj <manojsw at gmail.com> wrote:
>      My hunch is that processing time shouldn't be *very* different
> between R & other languages but then it's just a hunch. Can anybody
> share/comment on there experience? Sorry for the open-ended nature of
> the question.

There is a interesting thread on slashdot this AM that addresses this question:

http://it.slashdot.org/it/06/07/18/0146216.shtml

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From roger.bos at gmail.com  Wed Jul 19 14:06:07 2006
From: roger.bos at gmail.com (roger bos)
Date: Wed, 19 Jul 2006 08:06:07 -0400
Subject: [R-sig-Finance] Backtesting speed
In-Reply-To: <6e8360ad0607180904o3aa673dfk6982b06e1f5b4f20@mail.gmail.com>
References: <829e6c8a0607022202r8e6c165v527847860ed9e368@mail.gmail.com>
	<6e8360ad0607180904o3aa673dfk6982b06e1f5b4f20@mail.gmail.com>
Message-ID: <1db726800607190506r48d88852xaf31027d2445ef9d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060719/7506773b/attachment.pl 

From edd at debian.org  Thu Jul 20 04:05:40 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 19 Jul 2006 21:05:40 -0500
Subject: [R-sig-Finance] Backtesting speed
In-Reply-To: <1db726800607190506r48d88852xaf31027d2445ef9d@mail.gmail.com>
References: <829e6c8a0607022202r8e6c165v527847860ed9e368@mail.gmail.com>
	<6e8360ad0607180904o3aa673dfk6982b06e1f5b4f20@mail.gmail.com>
	<1db726800607190506r48d88852xaf31027d2445ef9d@mail.gmail.com>
Message-ID: <17598.58612.368663.920368@basebud.nulle.part>


Just to play contrarian here, let me mention that on the quantlib-user list
two issues recently surfaced:

 - replacing float/double calculations with (scaled) integers for greater 
   speed

 - using dedicated hardware (field programmable gate arrays) with their
   specialised language dialects / compilers for speed increases of up to
   100x (relative to a baseline of C++, not R)

And then there are the folks who want to do linear algebra in their graphics
cards as those things have better bus speed and floating point performance
than the usual motherboard and fpu ...

So to sum it up, there is always someone trying to be faster yet.  But then
most of us use R for the speed and power of prototyping and 'r & d', right?

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From whit.armstrong at hcmny.com  Thu Jul 20 17:16:56 2006
From: whit.armstrong at hcmny.com (Armstrong, Whit)
Date: Thu, 20 Jul 2006 11:16:56 -0400
Subject: [R-sig-Finance] help on portfolio optimization
Message-ID: <E58BE6136618CF4C964F6EC7773AE5699FA44B@ex4.nyc.hcmny.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060720/113fd280/attachment.pl 

From atp at piskorski.com  Thu Jul 20 19:05:27 2006
From: atp at piskorski.com (Andrew Piskorski)
Date: Thu, 20 Jul 2006 13:05:27 -0400
Subject: [R-sig-Finance] Backtesting speed
In-Reply-To: <1db726800607190506r48d88852xaf31027d2445ef9d@mail.gmail.com>
References: <1db726800607190506r48d88852xaf31027d2445ef9d@mail.gmail.com>
Message-ID: <20060720170527.GA6441@tehun.pair.com>

On Wed, Jul 19, 2006 at 08:06:07AM -0400, roger bos wrote:

> When I profile my R code, the vast majority of the time is usually
> in read.table and write.table, so I figure there is not much I can
> do to improve my code.

No, that's likely an unwarranted assumption.  If I remember right, R
read.table() can be GROTESQUELY inefficient in some cases.  So, you
might just have one super slow thing obscuring the fact that you also
have lots of other moderately slow things, all of which could be
dramatically (and perhaps usefully) sped up.

-- 
Andrew Piskorski <atp at piskorski.com>
http://www.piskorski.com/


From frednovo at pipeline.com  Thu Jul 20 20:57:35 2006
From: frednovo at pipeline.com (Frederick Novomestky)
Date: Thu, 20 Jul 2006 14:57:35 -0400
Subject: [R-sig-Finance] help on portfolio optimization
In-Reply-To: <E58BE6136618CF4C964F6EC7773AE5699FA44B@ex4.nyc.hcmny.com>
References: <E58BE6136618CF4C964F6EC7773AE5699FA44B@ex4.nyc.hcmny.com>
Message-ID: <6.2.1.2.2.20060720145530.02199e88@pop.pipeline.com>

One of the best books on the subject of Portfolio Construction is by Bernd 
Scherer,  Portfolio Construction and Risk Budgeting, Second Edition, Risk 
Books, ISBN 1-904339-30-1.  It is slanted toward SPLUS but I have ported 
some of the models to R.  I also used this book in my Portfolio Theory and 
Applications course at Polytechnic University, Brooklyn, NY.

Best regards,

Fred Novomestky

At 11:16 AM 7/20/2006, Armstrong, Whit wrote:
>Can anyone suggest a good book on optimization as it pertains to
>portfolio construction?
>
>I'm trying to migrate a portfolio construction process that is currently
>being done in excel over to R.
>
>I'm having problems reworking the variance covariance matrix of assets,
>the forecasted alphas and the constraints into the arguments needed by
>the solve.QP function: Dmat, dvec, Amat, bvec, and meq.
>
>I've looked at portfolio.optim in the tseries package, but I have a
>different objective function:   maximize utility where: utility =
>expected return - risk aversion * volatility.
>
>Thanks,
>Whit
>
>
>
>
>
>This e-mail message is intended only for the named recipient(s) above. It 
>may contain confidential information. If you are not the intended 
>recipient you are hereby notified that any dissemination, distribution or 
>copying of this e-mail and any attachment(s) is strictly prohibited. If 
>you have received this e-mail in error, please immediately notify the 
>sender by replying to this e-mail and delete the message and any 
>attachment(s) from your system. Thank you.
>
>
>         [[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance

Frederick Novomestky, Ph.D.
Novomestky Associates
41 Eastover Drive
East Northport, NY 11731-4330
Vox: 1.631.368.0701
Fax: 1.631.368.1696

Confidentiality Notice: This electronic mail transmission, i...{{dropped}}


From ajayshah at mayin.org  Fri Jul 21 06:26:41 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Fri, 21 Jul 2006 09:56:41 +0530
Subject: [R-sig-Finance] Supplied example breaks with mvBEKK.sim()
Message-ID: <20060721042641.GA22435@lubyanka.local>

I'm using `mgarchBEKK' version: 0.07-7.

> ?mvBEKK.sim
>      sim = mvBEKK.sim(series.count = 3, T = 2500)
Error in mvBEKK.sim(series.count = 3, T = 2500) : 
        could not find function "count.triangular"

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From eric at ehlarson.com  Thu Jul 20 00:06:30 2006
From: eric at ehlarson.com (eric larson)
Date: Wed, 19 Jul 2006 18:06:30 -0400
Subject: [R-sig-Finance] Backtesting speed
In-Reply-To: <1db726800607190506r48d88852xaf31027d2445ef9d@mail.gmail.com>
References: <829e6c8a0607022202r8e6c165v527847860ed9e368@mail.gmail.com>
	<6e8360ad0607180904o3aa673dfk6982b06e1f5b4f20@mail.gmail.com>
	<1db726800607190506r48d88852xaf31027d2445ef9d@mail.gmail.com>
Message-ID: <44BEACE6.4080607@ehlarson.com>

roger bos wrote:
> Yeah reading the link above, I would summarize it as this: If someone is
> good at/and likes C/C++, you will never be able to convince them that an
> interpretted language is as good.  Most proponents of interpretted languages
> just figure the processor speed and memory improvements will allow them to
> carry on without using compilers.  When I profile my R code, the vast
> majority of the time is usually in read.table and write.table, so I figure
> there is not much I can do to improve my code.  While using Perl & C & R
> together could bring some speed imporovement, there is also a downside to
> learning and maintaining code in different langues and putting all the
> pieces together.
>
> But then again, I work with monthly data, so its not really a concern of
> mine.  Most hedge funds that work with tick data use Perl to process the
> data and then maybe R to analyze it.  Basically, the volume is too great to
> do in R. Of course linking to a database to a nice plus in R, I don't know
> if Perl can do that.
>   

Actually Perl has excellent database connectivity. The DBA's I work with
tend to
use Perl more than anything else to write DB maintenance and data
transformation
tools because the combination of Perl's db connectivity and text
manipulation
capabilities is very hard to beat.

As far as interpreted vs. compiled code the gap is narrowing every day.
C doesn't map
directly to machine instructions as in the past as CPUs become more
sophisticated,
and languages like Java are using techniques like run-time optimization
that are not
available to statically compiled languages.


From bshroyer at rehbeinsolutions.com  Wed Jul 19 22:06:32 2006
From: bshroyer at rehbeinsolutions.com (Bret Shroyer)
Date: Wed, 19 Jul 2006 15:06:32 -0500
Subject: [R-sig-Finance] Convert tick data to OHLC
Message-ID: <70D2860F3C4C804CB236A9347FBA3C7611AF3A@grchq01.GRC.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060719/c991ec17/attachment.pl 

From edd at debian.org  Fri Jul 21 13:13:27 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 21 Jul 2006 06:13:27 -0500
Subject: [R-sig-Finance] Reminder about subscription before posting
Message-ID: <17600.46807.20449.22782@basebud.nulle.part>


As I had to hand-approve several posts from non-subscribed addresses, a quick
reminder: R-sig-finance uses 'subscriber-only posting'.

So if your mailer sends out a From: address that is different from the one
you are subscribed with, your post will not go through. You may need to
either subscribe from that address, or alter the From field in your mailer.

Hope this helps,  Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From kriskumar at earthlink.net  Sun Jul 23 22:22:16 2006
From: kriskumar at earthlink.net (kriskumar at earthlink.net)
Date: Sun, 23 Jul 2006 20:22:16 +0000 GMT
Subject: [R-sig-Finance] Convert tick data to OHLC
In-Reply-To: <70D2860F3C4C804CB236A9347FBA3C7611AF3A@grchq01.GRC.local>
References: <70D2860F3C4C804CB236A9347FBA3C7611AF3A@grchq01.GRC.local>
Message-ID: <1616866973-1153686182-cardhu_blackberry.rim.net-255710337-@bwe026-cell00.bisx.prod.on.blackberry>

There are atleast two ways to this use Wolfgang Breyman's HFfinance library.  This needs the timeSeries class and I am not sure if it can be done without further tinkering.  I think the fSeries class timeSeries is compatible ? Or simillar to the splus class. Or use Eric Zivot's splus hfanalysis code in R after some munging perhaps!


Sent from my BlackBerry? wireless handheld  

-----Original Message-----
From: "Bret Shroyer" <bshroyer at rehbeinsolutions.com>
Date: Wed, 19 Jul 2006 15:06:32 
To:<r-sig-finance at stat.math.ethz.ch>
Subject: [R-sig-Finance] Convert tick data to OHLC

I have tick data that I'd like to convert to OHLC data at various
arbitrary timeframes - 1 minute, 5 minutes, one hour, etc.  I've
searched "R site search" and looked into RMetrics and the closest I
found was the not-yet-released fTickdata package.
 
I don't think it would be challenging to write this routine myself, but
I have to believe I'm not the first to attempt this, and the solution is
out there somewhere.  
 
Can anyone help point me in the right direction, or give me some better
search words?
 
Thanks,
 
Bret Shroyer

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From spencer.graves at pdf.com  Mon Jul 24 00:32:36 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 24 Jul 2006 06:32:36 +0800
Subject: [R-sig-Finance] Convert tick data to OHLC
In-Reply-To: <70D2860F3C4C804CB236A9347FBA3C7611AF3A@grchq01.GRC.local>
References: <70D2860F3C4C804CB236A9347FBA3C7611AF3A@grchq01.GRC.local>
Message-ID: <44C3F904.5040409@pdf.com>

	  The only "open, high, low, close" functions I found were 
plotOHLC{tseries} and ohlcPlot{fMultivar}.  I too would be surprised if 
you were the first to attempt this.  However, if anyone has contributed 
any such thing to CRAN, it is sufficiently well hidden that neither of 
us could find it.

	  If I had a need for such right now, I likely would write my own 
function.

	  Sorry I couldn't be more helpful.
	  Spencer Graves

Bret Shroyer wrote:
> I have tick data that I'd like to convert to OHLC data at various
> arbitrary timeframes - 1 minute, 5 minutes, one hour, etc.  I've
> searched "R site search" and looked into RMetrics and the closest I
> found was the not-yet-released fTickdata package.
>  
> I don't think it would be challenging to write this routine myself, but
> I have to believe I'm not the first to attempt this, and the solution is
> out there somewhere.  
>  
> Can anyone help point me in the right direction, or give me some better
> search words?
>  
> Thanks,
>  
> Bret Shroyer
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From br44114 at gmail.com  Mon Jul 24 16:19:03 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Mon, 24 Jul 2006 10:19:03 -0400
Subject: [R-sig-Finance] Convert tick data to OHLC
Message-ID: <8d5a36350607240719p3634aea3kddeeaf1ecd253bdf@mail.gmail.com>

I guess it's faster and perhaps easier to write the code from scratch
than to look around for an existing function. For example:
ohlc <- function(ttime,tprice,fmt)
{
ttime.int <- format(ttime,fmt)
data.frame(o = tapply(tprice,ttime.int,function(x) {head(x,1)}),
  h = tapply(tprice,ttime.int,max),
  l = tapply(tprice,ttime.int,min),
  c = tapply(tprice,ttime.int,function(x) {tail(x,1)}),
  time = ttime[tapply(1:length(ttime),ttime.int,function(x) {head(x,1)})])
}
ohlc.1min <- ohlc(Sys.time() + 1:10000,runif(10000),"%Y%m%d %H%M")
ohlc.1hour <- ohlc(Sys.time() + 1:10000,runif(10000),"%Y%m%d %H")



> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
> Spencer Graves
> Sent: Sunday, July 23, 2006 6:33 PM
> To: Bret Shroyer
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-sig-Finance] Convert tick data to OHLC
>
>   The only "open, high, low, close" functions I found were
> plotOHLC{tseries} and ohlcPlot{fMultivar}.  I too would be
> surprised if
> you were the first to attempt this.  However, if anyone has
> contributed
> any such thing to CRAN, it is sufficiently well hidden that
> neither of
> us could find it.
>
>   If I had a need for such right now, I likely would
> write my own
> function.
>
>   Sorry I couldn't be more helpful.
>   Spencer Graves
>
> Bret Shroyer wrote:
> > I have tick data that I'd like to convert to OHLC data at various
> > arbitrary timeframes - 1 minute, 5 minutes, one hour, etc.  I've
> > searched "R site search" and looked into RMetrics and the closest I
> > found was the not-yet-released fTickdata package.
> >
> > I don't think it would be challenging to write this routine
> myself, but
> > I have to believe I'm not the first to attempt this, and
> the solution is
> > out there somewhere.
> >
> > Can anyone help point me in the right direction, or give me
> some better
> > search words?
> >
> > Thanks,
> >
> > Bret Shroyer
> >
> > [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From tom_harte at yahoo.com  Mon Jul 24 18:06:10 2006
From: tom_harte at yahoo.com (Thomas Harte)
Date: Mon, 24 Jul 2006 09:06:10 -0700 (PDT)
Subject: [R-sig-Finance] S-PLUS (binary code?) to R ...
Message-ID: <20060724160610.5582.qmail@web30201.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060724/1fb3f9b3/attachment.pl 

From edd at debian.org  Tue Jul 25 04:23:49 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 24 Jul 2006 21:23:49 -0500
Subject: [R-sig-Finance] S-PLUS (binary code?) to R ...
In-Reply-To: <20060724160610.5582.qmail@web30201.mail.mud.yahoo.com>
References: <20060724160610.5582.qmail@web30201.mail.mud.yahoo.com>
Message-ID: <17605.32949.436858.896837@basebud.nulle.part>


On 24 July 2006 at 09:06, Thomas Harte wrote:
| as a quick follow-on, but on a different subject, how does one convert S-PLUS code in the format that Wolfgang Breymann's  HFfinance 
| package is stored (looks like "S V4" format which is in binary...don't ask me i have no experience with S-PLUS, only R) to R? e.g. 
| 
| tharte at quantzone:~/downloads/hfFinance-1.0-beta4/.Data$ od -c pCondSum | head -3
| 0000000  \0   S       V   4   -   M 003  \t  \0  \0  \0   f   u   n   c
| 0000020   t   i   o   n  \0  \0  \0  \0   - 001  \0  \0 005  \0  \0  \0
| 0000040   3  \0  \0  \0   m   i   s   s   i   n   g  \0   m   i   s   s
 
Hm, looks like you will need to write a parser for that.
 
| according to 
| 
|  http://tolstoy.newcastle.edu.au/R/help/00b/1903.html
| 
| at some point Brian Ripley had a package called Rstreams, but this seems to have disappeared from CRAN.
| 
| any ideas?

AFAICT Rstreams refers to the stream conncections that are part of R
itself. Way back when these may have been an additional package.

Can you export in ascii from Breymann's library and then re-import? If so, it
would presumably be easier to use and debug and a lot faster to write.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From bennfine at yahoo.com  Tue Jul 25 17:16:13 2006
From: bennfine at yahoo.com (Benn Fine)
Date: Tue, 25 Jul 2006 08:16:13 -0700 (PDT)
Subject: [R-sig-Finance] calling rmvnorm from C ?
Message-ID: <20060725151613.96439.qmail@web61320.mail.yahoo.com>

Writing an asset allocation simulation in C callable
from R and want to be able to generate random vectors
from a multivariate normal distribution.

Can I call the rmvnorm() function from within my C
program ? I realize I could just use BLAS and do the
Cholesky factorization and then the matrix multiply,
but I am already too worried about using C from within
R to add additional code.

Any code fragments showing how to call rmvnorm() from
a C program called from R would be most appreciated.

Thanks!

Benn


From smiller73 at jhu.edu  Fri Jul 21 15:32:21 2006
From: smiller73 at jhu.edu (Steve Miller)
Date: Fri, 21 Jul 2006 08:32:21 -0500
Subject: [R-sig-Finance] Backtesting speed
In-Reply-To: <44BEACE6.4080607@ehlarson.com>
Message-ID: <200607211335.k6LDZ2ua021217@hypatia.math.ethz.ch>

Python is probably even a better agile language than Perl at this point. Its
database connectivity is nonesuch, and the RPy interface provides the
capabilities to combine both R and Python functionality in a single program.
For more information, check out "Poor Man's BI", an article I wrote for the
June 2006 DM Review Extended Edition. http://www.dmreview.com/ee/


Steve Miller

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of eric larson
Sent: Wednesday, July 19, 2006 5:07 PM
To: R-sig-finance
Subject: Re: [R-sig-Finance] Backtesting speed

roger bos wrote:
> Yeah reading the link above, I would summarize it as this: If someone is
> good at/and likes C/C++, you will never be able to convince them that an
> interpretted language is as good.  Most proponents of interpretted
languages
> just figure the processor speed and memory improvements will allow them to
> carry on without using compilers.  When I profile my R code, the vast
> majority of the time is usually in read.table and write.table, so I figure
> there is not much I can do to improve my code.  While using Perl & C & R
> together could bring some speed imporovement, there is also a downside to
> learning and maintaining code in different langues and putting all the
> pieces together.
>
> But then again, I work with monthly data, so its not really a concern of
> mine.  Most hedge funds that work with tick data use Perl to process the
> data and then maybe R to analyze it.  Basically, the volume is too great
to
> do in R. Of course linking to a database to a nice plus in R, I don't know
> if Perl can do that.
>   

Actually Perl has excellent database connectivity. The DBA's I work with
tend to
use Perl more than anything else to write DB maintenance and data
transformation
tools because the combination of Perl's db connectivity and text
manipulation
capabilities is very hard to beat.

As far as interpreted vs. compiled code the gap is narrowing every day.
C doesn't map
directly to machine instructions as in the past as CPUs become more
sophisticated,
and languages like Java are using techniques like run-time optimization
that are not
available to statically compiled languages.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From brian at braverock.com  Wed Jul 26 23:50:25 2006
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 26 Jul 2006 16:50:25 -0500
Subject: [R-sig-Finance] Modified Cornish-Fisher VaR
Message-ID: <200607261650.26365.brian@braverock.com>

Has anyone done any work on Modified Cornish-Fisher VaR calculations in R?

The limitations of VaR in modeling of risk for non-normal distributions 
have been known for quite some time, and this approach seems to hold some 
value over other approaches already implemented in RMetrics like 
Conditional VaR.

I'm trying to replicate the calculation as laid out in: 
Favre, Laurent, and Jose-Antonio Galeano. ?Mean-Modified
Value at Risk Optimization with Hedge Funds.? The Journal
of Alternative Investments, 5 (2002), pp. 21-25.

and presented in a different form in an earlier paper:
Fallon, William. "Calculating Value-at-Risk" 
Working Paper, Wharton, 1996.

Both of these papers rely on calculating traditional VaR (as is done with 
fPortfolio.VaR() from the RMetrics package) and then using a 
Cornish-Fisher Expansion for skew (both papers) or skew and kurtosis 
(Favre 2002) (as is done using the fBasics.skewness() and 
fBasics.kurtosis() functions from RMetrics)

I'm wondering if anyone has already replicated this work in R, and could 
provide a pointer, or alternately if some of the more experienced people 
on this list could render an opinion on which of the Cornish-Fisher 
functions in the core R stats package might be appropriate for this kind 
of analysis.

I would like to implement and share a function for Modified Cornish-Fisher 
VaR, so any assistance would be greatly appreciated.

Regards,

   - Brian


From wuertz at itp.phys.ethz.ch  Thu Jul 27 15:35:48 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 27 Jul 2006 15:35:48 +0200
Subject: [R-sig-Finance] Modified Cornish-Fisher VaR
In-Reply-To: <200607261650.26365.brian@braverock.com>
References: <200607261650.26365.brian@braverock.com>
Message-ID: <44C8C134.1080303@itp.phys.ethz.ch>



Brian G. Peterson wrote:

>Has anyone done any work on Modified Cornish-Fisher VaR calculations in R?
>  
>
----------------- YES ----------------------

You can download R-functions and Description PDF from

http://www.itp.phys.ethz.ch/econophysics/favre/

WARNING - UNTESTED and not part of official Rmetrics !!!!!

Diethelm Wuertz

>The limitations of VaR in modeling of risk for non-normal distributions 
>have been known for quite some time, and this approach seems to hold some 
>value over other approaches already implemented in RMetrics like 
>Conditional VaR.
>
>I'm trying to replicate the calculation as laid out in: 
>Favre, Laurent, and Jose-Antonio Galeano. ?Mean-Modified
>Value at Risk Optimization with Hedge Funds.? The Journal
>of Alternative Investments, 5 (2002), pp. 21-25.
>
>and presented in a different form in an earlier paper:
>Fallon, William. "Calculating Value-at-Risk" 
>Working Paper, Wharton, 1996.
>
>Both of these papers rely on calculating traditional VaR (as is done with 
>fPortfolio.VaR() from the RMetrics package) and then using a 
>Cornish-Fisher Expansion for skew (both papers) or skew and kurtosis 
>(Favre 2002) (as is done using the fBasics.skewness() and 
>fBasics.kurtosis() functions from RMetrics)
>
>I'm wondering if anyone has already replicated this work in R, and could 
>provide a pointer, or alternately if some of the more experienced people 
>on this list could render an opinion on which of the Cornish-Fisher 
>functions in the core R stats package might be appropriate for this kind 
>of analysis.
>
>I would like to implement and share a function for Modified Cornish-Fisher 
>VaR, so any assistance would be greatly appreciated.
>
>Regards,
>
>   - Brian
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  
>


From joe-byers at utulsa.edu  Thu Jul 27 15:40:34 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Thu, 27 Jul 2006 08:40:34 -0500
Subject: [R-sig-Finance] Has anyone done any work on Modified
 Cornish-Fisher VaR calculations in R?
Message-ID: <44C8C252.1010907@utulsa.edu>

Brian,

John Hull's text Options, Futures and other derivatives has a nice 
appendix for Cornish Fisher on pagers 370-371.  This example only 
requires linear algebra to approximate the moments.    The appendix does 
not define zq but it is up*sigmap * alphahat where alphahat is the 
require percentile of a standard normal distribution (1%) in the 
appendix's example.

Good Luck
Joe W. Byers
Professor of Finance
The University of Tulsa


> The limitations of VaR in modeling of risk for non-normal distributions 
> have been known for quite some time, and this approach seems to hold some 
> value over other approaches already implemented in RMetrics like 
> Conditional VaR.
>
> I'm trying to replicate the calculation as laid out in: 
> Favre, Laurent, and Jose-Antonio Galeano. ?Mean-Modified
> Value at Risk Optimization with Hedge Funds.? The Journal
> of Alternative Investments, 5 (2002), pp. 21-25.
>
> and presented in a different form in an earlier paper:
> Fallon, William. "Calculating Value-at-Risk" 
> Working Paper, Wharton, 1996.
>
> Both of these papers rely on calculating traditional VaR (as is done with 
> fPortfolio.VaR() from the RMetrics package) and then using a 
> Cornish-Fisher Expansion for skew (both papers) or skew and kurtosis 
> (Favre 2002) (as is done using the fBasics.skewness() and 
> fBasics.kurtosis() functions from RMetrics)
>
> I'm wondering if anyone has already replicated this work in R, and could 
> provide a pointer, or alternately if some of the more experienced people 
> on this list could render an opinion on which of the Cornish-Fisher 
> functions in the core R stats package might be appropriate for this kind 
> of analysis.
>
> I would like to implement and share a function for Modified Cornish-Fisher 
> VaR, so any assistance would be greatly appreciated.
>
> Regards,
>
>    - Brian
>
>
>
> ------------------------------
>
>   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060727/cf86843c/attachment.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 104 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060727/cf86843c/attachment.vcf 

From edward.m at psu.ac.th  Wed Aug  2 03:31:14 2006
From: edward.m at psu.ac.th (Edward)
Date: Wed, 2 Aug 2006 08:31:14 +0700
Subject: [R-sig-Finance] [R-sig-finance] Multivariate GARCH with only
	univariate estimation
Message-ID: <000901c6b5d3$57e61060$9b061dac@edward>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060802/d87f63a0/attachment.pl 

From hkahra at gmail.com  Thu Aug  3 15:22:20 2006
From: hkahra at gmail.com (Hannu Kahra)
Date: Thu, 3 Aug 2006 16:22:20 +0300
Subject: [R-SIG-Finance] [R-sig-Finance] [R-sig-finance] Multivariate
	GARCH with only univariate estimation
In-Reply-To: <000901c6b5d3$57e61060$9b061dac@edward>
References: <000901c6b5d3$57e61060$9b061dac@edward>
Message-ID: <3d35a2ca0608030622o186647bakede2789a270d3c60@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060803/a7a3745e/attachment.pl 

From brian at braverock.com  Thu Aug  3 23:08:20 2006
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 3 Aug 2006 16:08:20 -0500
Subject: [R-SIG-Finance] [R-sig-Finance] [R-sig-finance] Multivariate
	GARCH with only univariate estimation
In-Reply-To: <000901c6b5d3$57e61060$9b061dac@edward>
References: <000901c6b5d3$57e61060$9b061dac@edward>
Message-ID: <200608031608.21145.brian@braverock.com>

On Tuesday 01 August 2006 20:31, Edward wrote:
> Have you or anyone you know implemented a computer algorithm to perform
> the PC-Garch technique? I'm interested in doing this myself using, for
> example, R software, but I'd like to know if it has already been done
> yet or not.

There are a number of GARCH functions available in the fSeries package 
that is part of RMetrics (installable from CRAN).  We've used them with 
some success here.  You may be able to use them or adapt them to your 
needs.

Regards,

  - Brian


From oyvfos at yahoo.no  Fri Aug  4 10:49:43 2006
From: oyvfos at yahoo.no (yvind Foshaug)
Date: Fri, 4 Aug 2006 01:49:43 -0700 (PDT)
Subject: [R-SIG-Finance] plotting zoo objects
Message-ID: <20060804084943.24863.qmail@web25511.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060804/aaf75c78/attachment.pl 

From ggrothendieck at gmail.com  Fri Aug  4 12:07:42 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 4 Aug 2006 06:07:42 -0400
Subject: [R-SIG-Finance] plotting zoo objects
In-Reply-To: <20060804084943.24863.qmail@web25511.mail.ukl.yahoo.com>
References: <20060804084943.24863.qmail@web25511.mail.ukl.yahoo.com>
Message-ID: <971536df0608040307m59ddb267o58183db3bb4af797@mail.gmail.com>

Interpreting the problem as  given a 1d zoo object z, plot its
data against 1, 2, 3, ... rather than the times
and then use some custom labelling of the axis:

# test data -- z has 5 values, a break of 2, 5 values, etc.

z <- zoo(c(1:5, 8:12, 15:19), c(1:5, 8:12, 15:19))
plot(coredata(z), xaxt = "n", type = "l")
idx <- seq(1, length(z), 5)
axis(1, idx, time(z)[idx])  # custom label axis with every 5th value

The above does not actually use plot.zoo since we are plotting
the data of zoo rather than a zoo object.  If for some reason you
need to use plot.zoo change it (only the plot statement has changed)
to the following which defines a new zoo object with sequence
numbers as times:

z <- zoo(c(1:5, 8:12, 15:19), c(1:5, 8:12, 15:19))
plot(zoo(coredata(z)), xaxt = "n", type = "l")
idx <- seq(1, length(z), 5)
axis(1, idx, time(z)[idx])


On 8/4/06, ?yvind Foshaug <oyvfos at yahoo.no> wrote:
> Hi all,
>  I would like to plot a zoo object, removing non-trading days, and adjust the tick resolution and the ticklabels. Is that possible?
>  Oyvind
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From neileastep at gmail.com  Fri Aug  4 14:56:16 2006
From: neileastep at gmail.com (Neil Eastep)
Date: Fri, 4 Aug 2006 21:56:16 +0900
Subject: [R-SIG-Finance] tradestation & R
Message-ID: <ce4d86730608040556t740d9ba8p4a267d57d6cf5fd4@mail.gmail.com>

I am contemplating buying Tradestation, and I wonder if and how this
can integrate with R . . . . Any insights would be appreciated.
Thanks, Neil.


From neileastep at gmail.com  Fri Aug  4 17:10:11 2006
From: neileastep at gmail.com (Neil Eastep)
Date: Sat, 5 Aug 2006 00:10:11 +0900
Subject: [R-SIG-Finance] tradestation & R
In-Reply-To: <ce4d86730608040556t740d9ba8p4a267d57d6cf5fd4@mail.gmail.com>
References: <ce4d86730608040556t740d9ba8p4a267d57d6cf5fd4@mail.gmail.com>
Message-ID: <ce4d86730608040810v6efbe0e9m4356115e3fc150eb@mail.gmail.com>

In aswer to my own question, I think my easiest route is to read and use
"Handling R objects in C", and create a dll that can (now that I
notice) be accessed from tradestation.  Neil.

On 8/4/06, Neil Eastep <neileastep at gmail.com> wrote:
> I am contemplating buying Tradestation, and I wonder if and how this
> can integrate with R . . . . Any insights would be appreciated.
> Thanks, Neil.
>


From edd at debian.org  Fri Aug  4 17:32:56 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 4 Aug 2006 10:32:56 -0500
Subject: [R-SIG-Finance] tradestation & R
In-Reply-To: <ce4d86730608040810v6efbe0e9m4356115e3fc150eb@mail.gmail.com>
References: <ce4d86730608040556t740d9ba8p4a267d57d6cf5fd4@mail.gmail.com>
	<ce4d86730608040810v6efbe0e9m4356115e3fc150eb@mail.gmail.com>
Message-ID: <17619.26792.382139.720930@basebud.nulle.part>


On 5 August 2006 at 00:10, Neil Eastep wrote:
| In aswer to my own question, I think my easiest route is to read and use
| "Handling R objects in C", and create a dll that can (now that I
| notice) be accessed from tradestation.  Neil.

As Tradestation imposes Windows, also consider the two (D)Com servers.

Otherwise, I have had luck with C/C++ interfaces to R but I haven't tried the
embedding route you are heading to.  For C++, I can *really* recommend
Dominick's RcppTemplate. 

Another option is Simon's Rserve -- that avoids all linking issues as
everything is done over tcp/ip, either on the same box or across the network.

Keep us (or maybe r-devel) posted ...

Good luck, Dirk

| 
| On 8/4/06, Neil Eastep <neileastep at gmail.com> wrote:
| > I am contemplating buying Tradestation, and I wonder if and how this
| > can integrate with R . . . . Any insights would be appreciated.
| > Thanks, Neil.
| >
| 
| _______________________________________________
| R-SIG-Finance at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-sig-finance

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From gyollin at insightful.com  Fri Aug  4 17:36:30 2006
From: gyollin at insightful.com (Guy Yollin)
Date: Fri, 4 Aug 2006 08:36:30 -0700
Subject: [R-SIG-Finance] tradestation & R
In-Reply-To: <ce4d86730608040556t740d9ba8p4a267d57d6cf5fd4@mail.gmail.com>
Message-ID: <96C6A984FE81AF4492020E9281CE67A8103F83@sewinexch00.insightful.com>

Hi Niel,

I've used both Tradestation and R for a while now and tried to connect them
up a few years back.

The connection that I used was via the DCOM module for R and Tradestation's
ELkit DLL interface.

Basically, your tradestation indicator or strategy can call a dll that you
create; the dll then also uses the COM connector interface to talk with R.

I did a few working examples that passed TS data through the interface to R,
R created some type of forecasting model e.g. loess regression and returned
results through the dll and back to TS where this indicator was the basis for
a trading signal.

It work but was terminally slow and I was not compelled at that time to try
to speed it up.

Perhaps there are better alternatives today.

I still have tradestation but now consider there greatest value to be their
on-demand historic database of intraday data for virtually all stocks and
futures (if you subscribe to the real-time data feed for the associated
exchange).

I've also used interactivebrokers and they support a number of interfaces for
real-time data and order execution; plus, there is quite a nice little
collection of 3rd party contributed software that exploits these interfaces
in various ways.

If I was trying to build a platform for trading system backtesting and
automated order entry with core modeling done in R, I'm not sure that I'd
hitch R to tradestation.

I'd probably look to connect up with interactivebrokers for the order
execution and find a clean way to get updated historic intraday data; of
course if you don't need intraday data you can just use yahoo.

If you can't find a source of continually updated intraday data then
tradestation may be the answer; the on-demand access to years of intraday
data for any symbol you want is quite nice.

Hope this helps.

-- Guy


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Neil Eastep
Sent: Friday, August 04, 2006 5:56 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] tradestation & R

I am contemplating buying Tradestation, and I wonder if and how this
can integrate with R . . . . Any insights would be appreciated.
Thanks, Neil.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From brian at braverock.com  Fri Aug  4 19:12:27 2006
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 4 Aug 2006 12:12:27 -0500
Subject: [R-SIG-Finance] [R-sig-Finance] Modified Cornish-Fisher VaR
In-Reply-To: <44C8C134.1080303@itp.phys.ethz.ch>
References: <200607261650.26365.brian@braverock.com>
	<44C8C134.1080303@itp.phys.ethz.ch>
Message-ID: <200608041212.28139.brian@braverock.com>

On Thursday 27 July 2006 08:35, Diethelm Wuertz wrote:
> Brian G. Peterson wrote:
> >Has anyone done any work on Modified Cornish-Fisher VaR calculations
> > in R?
>
> ----------------- YES ----------------------
>
> You can download R-functions and Description PDF from
>
> http://www.itp.phys.ethz.ch/econophysics/favre/
>
> WARNING - UNTESTED and not part of official Rmetrics !!!!!
>
> Diethelm Wuertz

Thank you to everyone who responded to this thread.  The file provided by 
Prof. Wuertz was an excellent starting point. I've corrected a small 
calculation error, and added some type checking and other bits to the 
function.  I've attached the modified .R file here which implements a 
Modified Cornish-Fisher VaR calculation.  Interested persons can now 
consider this updated function as tested and working. I'll post again as 
we get the co-moment calculations for co-kurtosis and co-skewness 
working. 

Regards,

    - Brian

> Brian G. Peterson wrote:
> >The limitations of VaR in modeling of risk for non-normal
> > distributions have been known for quite some time, and this approach
> > seems to hold some value over other approaches already implemented in
> > RMetrics like Conditional VaR.
> >
> >I'm trying to replicate the calculation as laid out in:
> >Favre, Laurent, and Jose-Antonio Galeano. ?Mean-Modified
> >Value at Risk Optimization with Hedge Funds.? The Journal
> >of Alternative Investments, 5 (2002), pp. 21-25.
> >
> >and presented in a different form in an earlier paper:
> >Fallon, William. "Calculating Value-at-Risk"
> >Working Paper, Wharton, 1996.
> >
> >Both of these papers rely on calculating traditional VaR (as is done
> > with fPortfolio.VaR() from the RMetrics package) and then using a
> >Cornish-Fisher Expansion for skew (both papers) or skew and kurtosis
> >(Favre 2002) (as is done using the fBasics.skewness() and
> >fBasics.kurtosis() functions from RMetrics)
> >
> >I'm wondering if anyone has already replicated this work in R, and
> > could provide a pointer, or alternately if some of the more
> > experienced people on this list could render an opinion on which of
> > the Cornish-Fisher functions in the core R stats package might be
> > appropriate for this kind of analysis.
> >
> >I would like to implement and share a function for Modified
> > Cornish-Fisher VaR, so any assistance would be greatly appreciated.
> >
> >Regards,
> >
> >   - Brian
> >
> >_______________________________________________
> >R-SIG-Finance at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-------------- next part --------------
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Library General Public
# License as published by the Free Software Foundation; either
# version 2 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU Library General
# Public License along with this library; if not, write to the
# Free Foundation, Inc., 59 Temple Place, Suite 330, Boston,
# MA  02111-1307  USA

# Copyright 1999-2003 Diethelm Wuertz for this R-port


################################################################################
# FUNCTION:
# annualizedMean
# annualizedVolatility
# annualizedSkewness
# annualizedKurtosis
# maxDrawdown
# timeUnderWater
# maxMonthlyLoss
# modifiedVaR
# monthlySharpeRatio
# skewnesskurtosisPrice
################################################################################


################################################################################
# The following functions are implemented from Extreme Metrics
# These are the risk measures implemented in the hedge fund
#   software from www.AlternativeSoft.com
#   See, "ExtremeMetrics Software", Help Document, Alternative Software,
#   March 2003, 4 pages.


# All returns are assumed to be on a monthly scale!
# The argument r is the monthly return time series !


annualizedMean =
function(r)
{   # A function implemented by Diethelm Wuertz

    # Description:

    # FUNCTION:

    # Mean of Monthly Returns
    result = (1 + mean(r))^12 - 1

    # Return Value:
    result
}


# ------------------------------------------------------------------------------


annualizedVolatility =
function(r)
{   # A function implemented by Diethelm Wuertz

    # Description:

    # FUNCTION:

    # Standard deviation of Monthly Returns:
    result = sqrt(var(r))

    # Return Value:
    result
}

std =
function(r)
{   # A function implemented by Diethelm Wuertz
    # NOTE: std function is listed in the doc for fBasics, but not implemented
    # Standard deviation of Monthly Returns:
    result = sqrt(var(r))

    # Return Value:
    result
}

# ------------------------------------------------------------------------------


annualizedSkewness =
function(r)
{   # A function implemented by Diethelm Wuertz

    # Description:

    # FUNCTION:

    # Skewness of Monthly Returns:
    #   [Skewness is part of the fBasics library]
    result = skewness(r)

    # Return Value:
    result
}


# ------------------------------------------------------------------------------


annualizedKurtosis =
function(r)
{   # A function implemented by Diethelm Wuertz

    # Description:

    # FUNCTION:

    # Excess Kurtosis of Monthly Returns:
    #    [Excess Kurtosis is part of the fBasics library]
    result = kurtosis(r)

    # Return Value:
    result
}


# ------------------------------------------------------------------------------


maxDrawdown =
function(r)
{   # A function implemented by Diethelm Wuertz

    # Description:

    # FUNCTION:

    # Maximum Drawdon of the Return Series
    result = NA

    # Return Value:
    result
}


# ------------------------------------------------------------------------------


timeUnderWater =
function(r)
{   # A function implemented by Diethelm Wuertz

    # Description:

    # FUNCTION:

    # Maximum time under water:
    NA

    # Return Value:
    result

}


# ------------------------------------------------------------------------------


maxMonthlyLoss =
function(r)
{   # A function implemented by Diethelm Wuertz

    # Description:

    # FUNCTION:

    result = min(r)

    # Return Value:
    result
}


# ------------------------------------------------------------------------------


modifiedVaR =
function(r, modified = TRUE, p=0.99, column=1)
{   # A function implemented by Diethelm Wuertz,
    # function completed/debugged by Brian G. Peterson

    # Description:

    # The limitations of mean Value-at-Risk are well covered in the literature.
    # Laurent Favre and Jose-Antonio Galeano published a paper in the
    # Fall 2002, volume 5 of the Journal of Alternative Investment,
    # "Mean ?modified Value-at-Risk optimization With Hedge Funds",
    # that proposed a modified VaR calculation that takes the higher moments
    # of non-normal distributions (skewness, kurtosis) into account, and
    # collapses to standard mean-VaR if the return stream follows a
    # standard distribution.
    # This measure is now widely cited and used in the literature,
    # and is usually referred to as "Modified VaR" or "Modified Cornish-Fisher VaR"

    # Diethelm Wuertz's original function was called monthlyVaR, but did not
    # contain the required modifications to get to a monthly number.  I have converted
    # it to modifiedVaR, and made the assumption of p=0.99, with an option for p=0.95 and
    # a collapse to normal mean VaR.

    # FUNCTION:

    # NOTE: see the data type conditionals in 'cov' and replicate here
    if (class(r) == "matrix") {
        r = r[, column]
        warning("Column ", column, colnames(r)[,column], " of matrix used")
    }
    if (class(r) == "data.frame") {
        r = r[, column]
        warning("Column ", column, colnames(r)[,column], " of data.frame used")
    }
    if (class(r) == "timeSeries") {
        r = r at Data[, column]
        warning("Column ", column, colnames(r)[,column], " of timeSeries used")
    }
    if (!is.numeric(r)) stop("The selected column is not numeric")
    r = as.vector(r)

    if ( p == 0.95 ) {
        zc = -1.96 #95% probability
    }
    if ( p == 0.99 ) {
        zc = -2.33 #99% probability
    }
    #} else {
    #    #some function here to compute zc with arbitrary p
    #}

    if (modified) {
        s = colSkewness(r) #use regular skewness and kurtosis fn if data.frame is converted to matrix?
        k = colKurtosis(r) #to compute excess kurtosis
        Zcf = zc + (((zc^2-1)*s)/6) + (((zc^3-3*zc)*k)/24) + (((2*zc^3)-(5*zc)*s^2)/36)
        result = mean(r) - (Zcf * sqrt(var(r)))
    } else {
        # should probably add risk-free-rate skew here?
        result = mean(r) - (zc * sqrt(var(r)))
    }

    # Return Value:
    result
}

monthlyVaR =
function(r, modified = FALSE)
{
    # stub function to preserve the original syntax in the port of ExtremeMetrics
    result = modifiedVaR(r, modified)
    # Return Value:
    result
}

# ------------------------------------------------------------------------------


monthlySharpeRatio =
function(r, modified=FALSE)
{   # A function implemented by Diethelm Wuertz

    # Description:

    # FUNCTION:

    if (modified) {
        result = mean(r)/monthlyVaR(r, modified = TRUE)
    } else {
        result = mean(r)/sqrt(var(r))}

    # Return Value:
    result
}


# ------------------------------------------------------------------------------


skewnesskurtosisPrice =
function(r)
{   # A function implemented by Diethelm Wuertz

    # Description:

    # FUNCTION:

    result = mean(r) *
        ( monthlyVaR(r, modified = TRUE) / monthlyVaR(r, modified = FALSE) - 1   )

    # Return Value:
    result
}


################################################################################

From ajayshah at mayin.org  Sun Aug  6 17:03:11 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sun, 6 Aug 2006 20:33:11 +0530
Subject: [R-SIG-Finance] Elementary zoo question
Message-ID: <20060806150311.GC253@lubyanka.local>

I want to print out the cells in a zoo vector which satisfy a
boolean. Here are a pair of lines that establishes the object `x':

> library(zoo)
> x <- structure(c(3.06998257357605, 3.33772140570104, 3.3633043045167, 
  3.41858136637240, 3.3658893681899, 3.70288557892392, 3.70802276353851, 
  3.47820398631861, 3.39745971102681, 3.27788314930746, 3.21152179993291, 
  3.19076418031979, 3.15816979006477, 3.22623617599131, 3.24522228272378, 
  3.24310489894978, 3.15909156737315, 3.18912820274379, 3.06356055461099, 
  3.01357540944708, 2.96377166092153, 2.91085513859483, 2.74785431983229, 
  2.58340878444823, 2.47441643715188, 2.27267478028934, 2.26052797003791, 
  1.97871265843542, 2.08038946495422, 2.07798508066967, 2.08428258775158, 
  2.13334354449931, 2.14911548113283, 2.14179508745853, 2.17909394235188, 
  2.07815119482578, 2.08190247495454, 2.09346304733713, 2.09247814243038, 
  1.88816169712064, 1.89600381030329), index = structure(c(13292, 
  13293, 13294, 13297, 13298, 13299, 13300, 13301, 13304, 13305, 
  13306, 13307, 13308, 13311, 13312, 13313, 13314, 13315, 13318, 
  13319, 13320, 13321, 13322, 13324, 13325, 13326, 13327, 13328, 
  13329, 13332, 13333, 13334, 13335, 13336, 13339, 13340, 13341, 
  13342, 13343, 13346, 13347), class = "Date"), class = "zoo")

The obvious syntax, inspired by ordinary R vectors, doesn't work:

> x[x>3]
Data:
numeric(0)

Index:
character(0)

What gives? I'm reduced to:

> for (i in 1:length(x)) {
   if (x[i] > 3) {
     print(x[i])
   }
 }

Similarly, how would we print out rows from a zoo matrix based on a
boolean condition that is applied one timepoint at a time?

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From ggrothendieck at gmail.com  Sun Aug  6 21:35:51 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 6 Aug 2006 15:35:51 -0400
Subject: [R-SIG-Finance] Elementary zoo question
In-Reply-To: <20060806150311.GC253@lubyanka.local>
References: <20060806150311.GC253@lubyanka.local>
Message-ID: <971536df0608061235x327e4c61w705956f94b4477da@mail.gmail.com>

I think what you wanted to do was to subscript with a variable
of class "logical" variable but instead you subscripted with a
variable of class "zoo" :

class(x > 3) # "zoo"

Try:

x[which(x>3]]

On 8/6/06, Ajay Narottam Shah <ajayshah at mayin.org> wrote:
> I want to print out the cells in a zoo vector which satisfy a
> boolean. Here are a pair of lines that establishes the object `x':
>
> > library(zoo)
> > x <- structure(c(3.06998257357605, 3.33772140570104, 3.3633043045167,
>  3.41858136637240, 3.3658893681899, 3.70288557892392, 3.70802276353851,
>  3.47820398631861, 3.39745971102681, 3.27788314930746, 3.21152179993291,
>  3.19076418031979, 3.15816979006477, 3.22623617599131, 3.24522228272378,
>  3.24310489894978, 3.15909156737315, 3.18912820274379, 3.06356055461099,
>  3.01357540944708, 2.96377166092153, 2.91085513859483, 2.74785431983229,
>  2.58340878444823, 2.47441643715188, 2.27267478028934, 2.26052797003791,
>  1.97871265843542, 2.08038946495422, 2.07798508066967, 2.08428258775158,
>  2.13334354449931, 2.14911548113283, 2.14179508745853, 2.17909394235188,
>  2.07815119482578, 2.08190247495454, 2.09346304733713, 2.09247814243038,
>  1.88816169712064, 1.89600381030329), index = structure(c(13292,
>  13293, 13294, 13297, 13298, 13299, 13300, 13301, 13304, 13305,
>  13306, 13307, 13308, 13311, 13312, 13313, 13314, 13315, 13318,
>  13319, 13320, 13321, 13322, 13324, 13325, 13326, 13327, 13328,
>  13329, 13332, 13333, 13334, 13335, 13336, 13339, 13340, 13341,
>  13342, 13343, 13346, 13347), class = "Date"), class = "zoo")
>
> The obvious syntax, inspired by ordinary R vectors, doesn't work:
>
> > x[x>3]
> Data:
> numeric(0)
>
> Index:
> character(0)
>
> What gives? I'm reduced to:
>
> > for (i in 1:length(x)) {
>   if (x[i] > 3) {
>     print(x[i])
>   }
>  }
>
> Similarly, how would we print out rows from a zoo matrix based on a
> boolean condition that is applied one timepoint at a time?
>
> --
> Ajay Shah                                      http://www.mayin.org/ajayshah
> ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
> <*(:-? - wizard who doesn't know the answer.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From ggrothendieck at gmail.com  Sun Aug  6 21:47:38 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 6 Aug 2006 15:47:38 -0400
Subject: [R-SIG-Finance] Elementary zoo question
In-Reply-To: <971536df0608061235x327e4c61w705956f94b4477da@mail.gmail.com>
References: <20060806150311.GC253@lubyanka.local>
	<971536df0608061235x327e4c61w705956f94b4477da@mail.gmail.com>
Message-ID: <971536df0608061247q2abaa566r7d74f2f24f687458@mail.gmail.com>

Sorry, there was a typo.  Here it is again:

x[which(x>3)]


On 8/6/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> I think what you wanted to do was to subscript with a variable
> of class "logical" variable but instead you subscripted with a
> variable of class "zoo" :
>
> class(x > 3) # "zoo"
>
> Try:
>
> x[which(x>3]]
>
> On 8/6/06, Ajay Narottam Shah <ajayshah at mayin.org> wrote:
> > I want to print out the cells in a zoo vector which satisfy a
> > boolean. Here are a pair of lines that establishes the object `x':
> >
> > > library(zoo)
> > > x <- structure(c(3.06998257357605, 3.33772140570104, 3.3633043045167,
> >  3.41858136637240, 3.3658893681899, 3.70288557892392, 3.70802276353851,
> >  3.47820398631861, 3.39745971102681, 3.27788314930746, 3.21152179993291,
> >  3.19076418031979, 3.15816979006477, 3.22623617599131, 3.24522228272378,
> >  3.24310489894978, 3.15909156737315, 3.18912820274379, 3.06356055461099,
> >  3.01357540944708, 2.96377166092153, 2.91085513859483, 2.74785431983229,
> >  2.58340878444823, 2.47441643715188, 2.27267478028934, 2.26052797003791,
> >  1.97871265843542, 2.08038946495422, 2.07798508066967, 2.08428258775158,
> >  2.13334354449931, 2.14911548113283, 2.14179508745853, 2.17909394235188,
> >  2.07815119482578, 2.08190247495454, 2.09346304733713, 2.09247814243038,
> >  1.88816169712064, 1.89600381030329), index = structure(c(13292,
> >  13293, 13294, 13297, 13298, 13299, 13300, 13301, 13304, 13305,
> >  13306, 13307, 13308, 13311, 13312, 13313, 13314, 13315, 13318,
> >  13319, 13320, 13321, 13322, 13324, 13325, 13326, 13327, 13328,
> >  13329, 13332, 13333, 13334, 13335, 13336, 13339, 13340, 13341,
> >  13342, 13343, 13346, 13347), class = "Date"), class = "zoo")
> >
> > The obvious syntax, inspired by ordinary R vectors, doesn't work:
> >
> > > x[x>3]
> > Data:
> > numeric(0)
> >
> > Index:
> > character(0)
> >
> > What gives? I'm reduced to:
> >
> > > for (i in 1:length(x)) {
> >   if (x[i] > 3) {
> >     print(x[i])
> >   }
> >  }
> >
> > Similarly, how would we print out rows from a zoo matrix based on a
> > boolean condition that is applied one timepoint at a time?
> >
> > --
> > Ajay Shah                                      http://www.mayin.org/ajayshah
> > ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
> > <*(:-? - wizard who doesn't know the answer.
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
>


From brian at braverock.com  Thu Aug 10 14:05:43 2006
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 10 Aug 2006 07:05:43 -0500
Subject: [R-SIG-Finance] coskewness, cokurtosis,
	and higher beta co-moments of the return distribution (implemented)
Message-ID: <200608100705.43884.brian@braverock.com>

As I discussed in my posts on Modified Cornish-Fisher VaR, many papers in 
the literature use higher moments of the return vector of an instrument 
to aid in the analysis of the returns for non-normal distributions.  The 
R 'stats' package contains functions for mean, variance, and covariance. 
RMetrics by Diethelm Wuertz contains functions for skewness and kurtosis, 
but does not contain any of the higher co-moments of the return 
distribution.

I've attached a .R file that contains functions for coskewness, 
cokurtosis, betacoskewness (or systematic skewness), betacokurtosis (or 
systematic kurtosis), and betacovariance (or systematic beta).

The use of the higher co-moments of the return distribution function may 
aid practitioners in analyzing the extreme downside risks and 
diversification potential of non-normally distributed assets.

My earlier posts on Modified Cornish-Fisher VaR may be found here:
    http://article.gmane.org/gmane.comp.lang.r.r-metrics/855

The current and all future versions of the attached file may be found 
here:
http://braverock.com/brian/R/extra_moments.R

I hope you find it useful.  Any comments, criticisms, or suggestions for 
improvement are gladly accepted.

Regards,

   - Brian

-------------- next part --------------
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either
# version 2 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Library General Public License for more details.
#
# You should have received a copy of the GNU General
# Public License along with this library; if not,
# go here: http://www.gnu.org/licenses/gpl.html
# or write to the
# Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston,
# MA  02111-1307  USA

# Copyright 2006 Brian G. Peterson for this R-port


################################################################################
# FUNCTIONS:
# ThirdMoment
# FourthMoment
# modifiedVaR
# CoSkewness
# CoKurtosis
# BetaCoVariance
# BetaCoV (wrapper for BetaCoVariance)
# SystematicBeta (wrapper for BetaCoVariance)
# BetaCoSkewness
# BetaCoS (wrapper for BetaCoSkewness)
# SystematicSkewness (wrapper for BetaCoSkewness)
# BetaCoKurtosis
# BetaCoK (wrapper for BetaCoKurtosis)
# SystematicKurtosis (wrapper for BetaCoKurtosis)
# modifiedVaR
#
################################################################################


################################################################################
# The following functions are intended to replicate calculations for
# taking higher moments of hedge fund returns into account in analyzing
# particular investments.  Most of the formulae are taken from various EDHEC
# research papers.

# All returns are assumed to be on a monthly scale!
# The argument r is the monthly return time series !

# @todo add modifiers for Risk-free rate, presumed to be zero in these functions

# @todo add ability to account for multidimensionality of
#       systematic beta, systematic skewness, and systematic kurtosis

# ------------------------------------------------------------------------------
std =
function(r)
{   # A function implemented by Diethelm Wuertz
    # NOTE: std function is listed in the doc for fBasics, but not implemented
    # Standard deviation of Monthly Returns:
    result = sqrt(var(r))

    # Return Value:
    result
}

library("fBasics")

# ------------------------------------------------------------------------------
ThirdMoment=
function(Ri,na.rm=FALSE)
{ # @author Brian G. Peterson

    # Description:
    # The third mathematical moment of the return function.
    # Favre and Renaldo use this as separate from skewness in developing a
    # four-moment CAPM model
    #
    # as defined in:
    # Favre, L. and Renaldo, A., October 2003
    # How to Price Hedge Funds: From Two- to Four-Moment CAPM
    # UBS and Edhec Business School

    # Setup

    Ri = as.vector(Ri)

    if(na.rm) {
        Ri <- Ri[!is.na(Ri)]
    }

    # FUNCTION:

    S = (mean((Ri-mean(Ri)^3)))^(1/3)

    result = S

    # Return Value:
    result
}

# ------------------------------------------------------------------------------
FourthMoment=
function(Ri,na.rm=FALSE)
{ # @author Brian G. Peterson

    # Description:
    # The fourth mathematical moment of the return function.
    # Favre and Renaldo use this as separate from kurtosis in developing a
    # four-moment CAPM model
    #
    # as defined in:
    # Favre, L. and Renaldo, A., October 2003
    # How to Price Hedge Funds: From Two- to Four-Moment CAPM
    # UBS and Edhec Business School

    # Setup

    Ri = as.vector(Ri)

    if(na.rm) {
        Ri <- Ri[!is.na(Ri)]
    }

    # FUNCTION:

    K = (mean((Ri-mean(Ri)^4)))^(1/4)

    result = K
    # Return Value:
    result
}

# ------------------------------------------------------------------------------
CoSkewness =
function(Ri, Ra, na.rm=FALSE)
{ # @author Brian G. Peterson

    # Description:
    # CoSkewness is the product of the third higher moments of two assets,
    # as defined in
    # Martellini L. and Ziemann V., 2005,
    # Marginal Impacts on Portfolio Distributions,
    # Working Paper, Edhec Risk and Asset Management Research Centre
    # and in:
    # Martellini L., Vaissie M., Ziemann V., October 2005,
    # Investing in Hedge Funds:
    #   Adding Value through Active Style Allocation Decisions
    # Edhec Risk and Asset Management Research Centre

    # Ri = return vector of initial portfolio
    # Ra = return vector of asset being considered for addition to portfolio

    # Setup

    Ri = as.vector(Ri)
    Ra = as.vector(Ra)

    if(na.rm) {
        Ri <- Ri[!is.na(Ri)]
        Ra <- Ra[!is.na(Ra)]
    }

    # FUNCTION:

    # CoSkewness of two assets
    CoS = mean((Ri - mean(Ri))*((Ra-mean(Ra))^2))

    result = CoS

    # Return Value:
    result
}

# ------------------------------------------------------------------------------
CoKurtosis =
function(Ri, Ra, na.rm=FALSE)
{ # @author Brian G. Peterson

    # Description:
    # CoKurtosis is the product of the fourth higher moments of two assets,
    # as defined in
    # Martellini L. and Ziemann V., 2005,
    # Marginal Impacts on Portfolio Distributions,
    # Working Paper, Edhec Risk and Asset Management Research Centre
    # and in:
    # Martellini L., Vaissie M., Ziemann V., October 2005,
    # Investing in Hedge Funds:
    #   Adding Value through Active Style Allocation Decisions
    # Edhec Risk and Asset Management Research Centre

    # Ri = return vector of initial portfolio
    # Ra = return vector of asset being considered for addition to portfolio

    # Setup

    Ri = as.vector(Ri)
    Ra = as.vector(Ra)

    if(na.rm) {
        Ri <- Ri[!is.na(Ri)]
        Ra <- Ra[!is.na(Ra)]
    }

    # FUNCTION:

    # CoKurtosis of two assets
    CoK = mean((Ri - mean(Ri))*((Ra-mean(Ra))^3))

    result = CoK

    # Return Value:
    result
}

# ------------------------------------------------------------------------------
BetaCoVariance =
function(Ri, Ra, na.rm=FALSE)
{ # @author Brian G. Peterson

    # Description:
    # Beta covariance is the beta of an asset to the variance and covariance
    # of an initial portfolio.  Used to determine diversification potential.
    # also called "systematic beta" by several papers
    #
    # as defined in
    # Favre, L. and Renaldo, A., October 2003
    # How to Price Hedge Funds: From Two- to Four-Moment CAPM
    # UBS and Edhec Business School
    # Equation [5] p. 10

    # Ri = return vector of initial portfolio
    # Ra = return vector of asset being considered for addition to portfolio

    # Setup

    Ri = as.vector(Ri)
    Ra = as.vector(Ra)

    if(na.rm) {
        Ri <- Ri[!is.na(Ri)]
        Ra <- Ra[!is.na(Ra)]
    }

    # FUNCTION:

    # CovarianceBeta of two assets
    covB = cov(Ra,Ri)/var(Ri)

    result = covB

    # Return Value:
    result
}

BetaCoV =
function(Ri, Ra, na.rm=FALSE)
{
    # wrapper function with a shorter name
    result = BetaCoVariance(Ri, Ra, na.rm)
    # Return Value:
    result
}

SystematicBeta =
function(Ri, Ra, na.rm=FALSE)
{
    # wrapper function with a shorter name
    result = BetaCoVariance(Ri, Ra, na.rm)
    # Return Value:
    result
}

# ------------------------------------------------------------------------------
BetaCoSkewness =
function(Ri, Ra, na.rm=FALSE)
{ # @author Brian G. Peterson

    # Description:
    # Beta CoSkewness is the beta of an asset to the skewness
    # of an initial portfolio.  Used to determine diversification potential.
    # also called "systematic skewness" or "systematic co-skewness"
    # by several papers.
    # as defined in
    # Favre, L. and Renaldo, A., October 2003
    # How to Price Hedge Funds: From Two- to Four-Moment CAPM
    # UBS and Edhec Business School
    # Equation [5] p. 10

    # Ri = return vector of initial portfolio
    # Ra = return vector of asset being considered for addition to portfolio

    # Setup

    Ri = as.vector(Ri)
    Ra = as.vector(Ra)

    if(na.rm) {
        Ri <- Ri[!is.na(Ri)]
        Ra <- Ra[!is.na(Ra)]
    }

    # FUNCTION:

    # systematic skewness of two assets
    skB = CoSkewness(Ra,Ri)/(mean(Ri-mean(Ri))^3)

    result = skB

    # Return Value:
    result
}

BetaCoS =
function(Ri, Ra, na.rm=FALSE)
{
    # wrapper function with a shorter name
    result = BetaCoSkewness(Ri, Ra, na.rm)
    # Return Value:
    result
}

SystematicSkewness =
function(Ri, Ra, na.rm=FALSE)
{
    # wrapper function with a shorter name
    result = BetaCoSkewness(Ri, Ra, na.rm)
    # Return Value:
    result
}

# ------------------------------------------------------------------------------
BetaCoKurtosis =
function( Ri, Ra, na.rm=FALSE, method=c("moment", "excess", "fisher") )
{ # @author Brian G. Peterson

    # Description:
    # Beta CoKurtosis is the beta of an asset to the kurtosis
    # of an initial portfolio.  Used to determine diversification potential.
    # Also called "systematic kurtosis" or "systematic cokurtosis" by several papers.
    #
    # as defined in
    # Martellini L., Vaissie M., Ziemann V., October 2005,
    # Investing in Hedge Funds:
    #   Adding Value through Active Style Allocation Decisions
    # Edhec Risk and Asset Management Research Centre


    # Ri = return vector of initial portfolio
    # Ra = return vector of asset being considered for addition to portfolio

    # Setup

    Ri = as.vector(Ri)
    Ra = as.vector(Ra)

    if(na.rm) {
        Ri <- Ri[!is.na(Ri)]
        Ra <- Ra[!is.na(Ra)]
    }

    # FUNCTION:

    # Beta CoKurtosis of two assets
    ktB = CoKurtosis(Ra,Ri)/kurtosis(Ri, na.rm ,method ) #method = c("excess", "moment", "fisher")

    result = ktB

    # Return Value:
    result
}

BetaCoK =
function( Ri, Ra, na.rm=FALSE, method=c("moment", "excess", "fisher") )
{
    # wrapper function with a shorter name
    result = BetaCoKurtosis(Ri, Ra, na.rm, method)
    # Return Value:
    result
}

SystematicKurtosis =
function( Ri, Ra, na.rm=FALSE, method=c("moment", "excess", "fisher") )
{
    # wrapper function with a shorter name
    result = BetaCoKurtosis(Ri, Ra, na.rm, method)
    # Return Value:
    result
}

# ------------------------------------------------------------------------------
modifiedVaR =
function(r, modified = TRUE, p=0.99, column=1)
{   # @author Diethelm Wuertz (original prototype fn)
    # @author Brian G. Peterson (completed/debugged fn)

    # Description:

    # The limitations of mean Value-at-Risk are well covered in the literature.
    # Laurent Favre and Jose-Antonio Galeano published a paper in the
    # Fall 2002, volume 5 of the Journal of Alternative Investment,
    # "Mean-Modified Value-at-Risk optimization With Hedge Funds",
    # that proposed a modified VaR calculation that takes the higher moments
    # of non-normal distributions (skewness, kurtosis) into account, and
    # collapses to standard mean-VaR if the return stream follows a
    # standard distribution.
    # This measure is now widely cited and used in the literature,
    # and is usually referred to as "Modified VaR" or "Modified Cornish-Fisher VaR"

    # Diethelm Wuertz's original function was called monthlyVaR, but did not
    # contain the required modifications to get to a monthly or an annualized number.
    # I have converted it to modifiedVaR, and made the assumption of p=0.99, with an option for p=0.95 and
    # a collapse to normal mean VaR.

    # FUNCTION:

    # NOTE: see the data type conditionals in 'cov' and replicate here
    if (class(r) == "matrix") {
        r = r[, column]
        warning("Column ", column, colnames(r)[,column], " of matrix used")
    }
    if (class(r) == "data.frame") {
        r = r[, column]
        warning("Column ", column, colnames(r)[,column], " of data.frame used")
    }
    if (class(r) == "timeSeries") {
        r = r at Data[, column]
        warning("Column ", column, colnames(r)[,column], " of timeSeries used")
    }
    if (!is.numeric(r)) stop("The selected column is not numeric")
    r = as.vector(r)

    if ( p == 0.95 ) {
        zc = -1.96 #95% probability
    }
    if ( p == 0.99 ) {
        zc = -2.33 #99% probability
    }
    #} else {
    #    #some function here to compute zc with arbitrary p
    #}

    if (modified) {
        s = colSkewness(r) #use regular skewness and kurtosis fn if data.frame is converted to matrix?
        k = colKurtosis(r) #to compute excess kurtosis
        Zcf = zc + (((zc^2-1)*s)/6) + (((zc^3-3*zc)*k)/24) + (((2*zc^3)-(5*zc)*s^2)/36)
        result = mean(r) - (Zcf * sqrt(var(r)))
    } else {
        # should probably add risk-free-rate skew here?
        result = mean(r) - (zc * sqrt(var(r)))
    }

    # Return Value:
    result
}

################################################################################

From john.janmaat at acadiau.ca  Wed Aug  9 03:12:45 2006
From: john.janmaat at acadiau.ca (John Janmaat)
Date: Tue,  8 Aug 2006 22:12:45 -0300
Subject: [R-SIG-Finance] NLS and IV
Message-ID: <1155085965.44d9368d5cefe@webmail.acadiau.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060808/3d838506/attachment.pl 

From Xiaochen.Sun at brunel.ac.uk  Fri Aug 11 18:06:18 2006
From: Xiaochen.Sun at brunel.ac.uk (Xiaochen Sun)
Date: Fri, 11 Aug 2006 17:06:18 +0100
Subject: [R-SIG-Finance] Workshop on Portfolio Optimisation & Stochastic
	Programming
Message-ID: <4053699F3E4AD04BA536A5F6BB81A2CFF91F05@UXEXMBU116.academic.windsor>

(Apologies for any cross-listings!)

Business Applications of Optimisation, Stochastic Programming & Portfolio Planning:

?Introduction to Optimisation and its Applications: Linear & Integer Programming - Embedded DSS using SCRIPTING and COM Objects,
16 - 17 October, CARISMA, Brunel University, West London

?Decision Making under Uncertainty: Stochastic Programming, 18 - 19 October, CARISMA, Brunel University, West London

?Financial Planning Using Integer Quadratic Programming,
20 October, CARISMA, Brunel University, West London

Dear Colleague,

We are pleased to announce the above workshops, which are organised by CARISMA, Brunel University, OptiRisk Systems and UNICOM Seminars.

The workshop series is specially designed to provide insight into the discipline of optimisation for a wide range of individuals such as OR professionals, quantitative analysts, risk analysts, DSS application developers, consultants, and academic researchers.

The courses will take you through all the steps of an optimisation project using powerful optimisation tools such as AMPL Modelling System, CPLEX, FortMP, FortSP and SPInE. They are most comprehensive and cover the latest developments in the field, with plenty of hands-on examples, which help you develop stochastic programming applications for your sector, be it financial planning, portfolio selection, supply chain, or energy systems planning.

Guest Presentation:
?Scenario Generation ? Hidden Markov Model
Enza Messina, University of Milan, Italy
?Sloving Integer Stochastic Programming
Suvrajeet Sen, University of Arizona, USA

For further details please go to www.unicom.co.uk/optimise, either download brochure or email mapgxcs at brunel.ac.uk for a PDF filer.

We look forward to welcoming you to the workshops; please also make your colleagues aware of it. Thank you.

Best regards
CARISMA

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Michael(Xiaochen) Sun
CARISMA, www.carisma.brunel.ac.uk 
Centre for the Analysis of Risk and Optimisation Modelling Application;
School of Computing, Information Systems and Mathematics 
Brunel University 
Middlesex 
Uxbridge, UB8 3PH 
United Kingdom 
* xiaochen.sun at brunel.ac.uk 
http://people.brunel.ac.uk/~mapgxcs 
http://mam3xs.blogspot.com/
((+44) (0)1895 265625
((+44) (0)7841873292
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


From tom.boonen.maiden at gmail.com  Wed Aug 16 21:26:13 2006
From: tom.boonen.maiden at gmail.com (Tom Boonen)
Date: Wed, 16 Aug 2006 15:26:13 -0400
Subject: [R-SIG-Finance] how to fit arma(1,1)?
Message-ID: <cc088e260608161226j1bfd271bl1d8176f8e5bafe4c@mail.gmail.com>

Dear List,

I am new to TS-Modeling in R. I would like to fit an ARMA(1,1) model
for a balanced panel, running Y on a full set of unit and year dummies
using an arma(1,1) for the disturbance:

y_it=unit.dummies+yeardummies+e_it

where: e_it=d*e_it-1+u_it+q*u_it-1

How can I fit this model in R? arma() does not seem to take covariates
(or I don't understand how to specify the function so that it would).
Thank you very much.

Best, Tom


From Achim.Zeileis at wu-wien.ac.at  Wed Aug 16 23:30:24 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 16 Aug 2006 23:30:24 +0200
Subject: [R-SIG-Finance] how to fit arma(1,1)?
In-Reply-To: <cc088e260608161226j1bfd271bl1d8176f8e5bafe4c@mail.gmail.com>
References: <cc088e260608161226j1bfd271bl1d8176f8e5bafe4c@mail.gmail.com>
Message-ID: <20060816233024.da76d839.Achim.Zeileis@wu-wien.ac.at>

On Wed, 16 Aug 2006 15:26:13 -0400 Tom Boonen wrote:

> Dear List,
> 
> I am new to TS-Modeling in R. I would like to fit an ARMA(1,1) model
> for a balanced panel, running Y on a full set of unit and year dummies
> using an arma(1,1) for the disturbance:
> 
> y_it=unit.dummies+yeardummies+e_it
> 
> where: e_it=d*e_it-1+u_it+q*u_it-1
> 
> How can I fit this model in R?

Look at arima() from stats (note that it's ar*i*ma) which takes an
`xreg' argument through which you can pass additional regressors.
There's a worked example on ?arima and also on ?UKDriverDeaths.

hth,
Z

> arma() does not seem to take covariates
> (or I don't understand how to specify the function so that it would).
> Thank you very much.
> 
> Best, Tom
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From muckjail at yahoo.com  Thu Aug 17 16:52:22 2006
From: muckjail at yahoo.com (michael mathews)
Date: Thu, 17 Aug 2006 07:52:22 -0700 (PDT)
Subject: [R-SIG-Finance] Problem with garch (tseries)
Message-ID: <20060817145222.44724.qmail@web38902.mail.mud.yahoo.com>

Hi folks,
I have been playing with garch models to model the volatility in
physical natural prices. 
Here is the issue I have a dataset of 801 daily returns (attached).
If I run

garchall<-garch(hsc)

 ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 


> summary(garchall)

Call:
garch(x = hsc)

Model:
GARCH(1,1)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.3424 -0.5734  0.0000  0.6037  4.0501 

Coefficient(s):
    Estimate  Std. Error  t value Pr(>|t|)    
a0 2.507e-05   9.200e-06    2.726  0.00642 ** 
a1 1.218e-01   2.085e-02    5.840 5.21e-09 ***
b1 8.759e-01   1.937e-02   45.212  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Diagnostic Tests:
        Jarque Bera Test

data:  Residuals 
X-squared = 62.7291, df = 2, p-value = 2.387e-14


        Box-Ljung test

data:  Squared.Residuals 
X-squared = 0.0384, df = 1, p-value = 0.8447

Now if we run the same model on a subset say the last 351 days we get
> garch351<-garch(tail(hsc,351))

 ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 


> summary(garch351)

Call:
garch(x = tail(hsc, 351))

Model:
GARCH(1,1)

Residuals:
      Min        1Q    Median        3Q       Max 
-4.171521 -0.424628  0.008727  0.532158  3.962116 

Coefficient(s):
    Estimate  Std. Error  t value Pr(>|t|)    
a0 2.511e-05   1.589e-05    1.580 0.114167    
a1 1.043e-01   2.950e-02    3.536 0.000406 ***
b1 8.957e-01   2.567e-02   34.896  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Diagnostic Tests:
        Jarque Bera Test

data:  Residuals 
X-squared = 76.3704, df = 2, p-value < 2.2e-16


        Box-Ljung test

data:  Squared.Residuals 
X-squared = 1.2806, df = 1, p-value = 0.2578

still ok. Now finally we get t the point of this email lets look at 352
days of data:

garch352<-garch(tail(hsc,352))

 ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 


Warning message:
NaNs produced in: sqrt(pred$e) 
> summary(garch352)

Call:
garch(x = tail(hsc, 352))

Model:
GARCH(1,1)

Residuals:
     Min       1Q   Median       3Q      Max 
-4.16377 -0.58155  0.01454  0.70886 12.41242 

Coefficient(s):
    Estimate  Std. Error  t value Pr(>|t|)    
a0 2.428e-05   1.556e-05    1.561 0.118632    
a1 1.043e-01   2.947e-02    3.540 0.000400 ***
b1 8.962e-01   2.556e-02   35.058  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Diagnostic Tests:
        Jarque Bera Test

data:  Residuals 
X-squared = 10993.57, df = 2, p-value < 2.2e-16


        Box-Ljung test

data:  Squared.Residuals 
X-squared = 0.1831, df = 1, p-value = 0.6687

whats up? Any Ideas. 
I have also tried using garchFit from the fSeries package but it locks
up completely left it running last night and it was still spinning this
morning when I got back to the office.

thanks in advance

michael


From muckjail at yahoo.com  Thu Aug 17 16:56:24 2006
From: muckjail at yahoo.com (michael mathews)
Date: Thu, 17 Aug 2006 07:56:24 -0700 (PDT)
Subject: [R-SIG-Finance] Problem with garch (tseries) forgot to attach the
	data
Message-ID: <20060817145624.72438.qmail@web38912.mail.mud.yahoo.com>

oops forgot to attach the data!

thanks again
michael

__________________________________________________


-------------- next part --------------
A non-text attachment was scrubbed...
Name: returns
Type: application/octet-stream
Size: 6512 bytes
Desc: 3697619705-returns
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060817/4ef95e54/attachment.obj 

From pcarl at gsbalum.uchicago.edu  Thu Aug 17 20:16:21 2006
From: pcarl at gsbalum.uchicago.edu (Peter Carl)
Date: Thu, 17 Aug 2006 13:16:21 -0500
Subject: [R-SIG-Finance] Data and label skew in plot.timeSeries? (fCalendar)
Message-ID: <200608171316.22005.pcarl@gsbalum.uchicago.edu>

I've noticed that there seems to be some skew between the x-axis labels 
and the data points when using plot.timeSeries (in fCalendar).

> benchmark.ts
           S&P500TR
2002-12-31  -0.0588
2003-01-31  -0.0262
2003-02-28  -0.0150
2003-03-31   0.0097
2003-04-30   0.0824
2003-05-30   0.0527
2003-06-30   0.0128
2003-07-31   0.0176
2003-08-29   0.0195
2003-09-30  -0.0106
2003-10-31   0.0566
2003-11-28   0.0088
2003-12-31   0.0524
2004-01-30   0.0184
2004-02-27   0.0139
2004-03-31  -0.0151
2004-04-30  -0.0157
2004-05-31   0.0137
2004-06-30   0.0194
2004-07-30  -0.0331
2004-08-31   0.0040
2004-09-30   0.0108
2004-10-29   0.0153
2004-11-30   0.0405
2004-12-31   0.0340
2005-01-31  -0.0244
2005-02-28   0.0210
2005-03-31  -0.0177
2005-04-29  -0.0190
2005-05-31   0.0318
2005-06-30   0.0014
2005-07-29   0.0372
2005-08-31  -0.0091
2005-09-30   0.0081
2005-10-31  -0.0167
2005-11-30   0.0378
2005-12-30   0.0003
2006-01-31   0.0265
> plot.timeSeries(benchmark.ts,format="%m/%y")
> plot.timeSeries(benchmark.ts,format="%Y-%m-%d")

Note that the x-axis labels are dated the following day (the first day 
of the next month) rather than the dates that were passed in with the 
timeSeries object.  So when viewing the x-axis as "%m/%y", the dates 
and the data do not appear to be lined up correctly.

Am I missing something about how to use the function or is this a 
genuine bug?
-- 
Peter Carl
145 Scottswood Rd
Riverside, IL 60546
708.447.6465


From ggrothendieck at gmail.com  Thu Aug 17 20:35:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 17 Aug 2006 14:35:08 -0400
Subject: [R-SIG-Finance] Data and label skew in plot.timeSeries?
	(fCalendar)
In-Reply-To: <200608171316.22005.pcarl@gsbalum.uchicago.edu>
References: <200608171316.22005.pcarl@gsbalum.uchicago.edu>
Message-ID: <971536df0608171135m559f23ddx5c492766306db4e9@mail.gmail.com>

I have not examined this in detail but its likely a time zone problem.
Try setting your time zone to "GMT"

setenv(TZ = "GMT")

at the start of your session and see if you still get the problem.  Also read
the help desk article in R News 4/1 on dates and times.

On 8/17/06, Peter Carl <pcarl at gsbalum.uchicago.edu> wrote:
> I've noticed that there seems to be some skew between the x-axis labels
> and the data points when using plot.timeSeries (in fCalendar).
>
> > benchmark.ts
>           S&P500TR
> 2002-12-31  -0.0588
> 2003-01-31  -0.0262
> 2003-02-28  -0.0150
> 2003-03-31   0.0097
> 2003-04-30   0.0824
> 2003-05-30   0.0527
> 2003-06-30   0.0128
> 2003-07-31   0.0176
> 2003-08-29   0.0195
> 2003-09-30  -0.0106
> 2003-10-31   0.0566
> 2003-11-28   0.0088
> 2003-12-31   0.0524
> 2004-01-30   0.0184
> 2004-02-27   0.0139
> 2004-03-31  -0.0151
> 2004-04-30  -0.0157
> 2004-05-31   0.0137
> 2004-06-30   0.0194
> 2004-07-30  -0.0331
> 2004-08-31   0.0040
> 2004-09-30   0.0108
> 2004-10-29   0.0153
> 2004-11-30   0.0405
> 2004-12-31   0.0340
> 2005-01-31  -0.0244
> 2005-02-28   0.0210
> 2005-03-31  -0.0177
> 2005-04-29  -0.0190
> 2005-05-31   0.0318
> 2005-06-30   0.0014
> 2005-07-29   0.0372
> 2005-08-31  -0.0091
> 2005-09-30   0.0081
> 2005-10-31  -0.0167
> 2005-11-30   0.0378
> 2005-12-30   0.0003
> 2006-01-31   0.0265
> > plot.timeSeries(benchmark.ts,format="%m/%y")
> > plot.timeSeries(benchmark.ts,format="%Y-%m-%d")
>
> Note that the x-axis labels are dated the following day (the first day
> of the next month) rather than the dates that were passed in with the
> timeSeries object.  So when viewing the x-axis as "%m/%y", the dates
> and the data do not appear to be lined up correctly.
>
> Am I missing something about how to use the function or is this a
> genuine bug?
> --
> Peter Carl
> 145 Scottswood Rd
> Riverside, IL 60546
> 708.447.6465
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From ggrothendieck at gmail.com  Thu Aug 17 22:25:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 17 Aug 2006 16:25:52 -0400
Subject: [R-SIG-Finance] Data and label skew in plot.timeSeries?
	(fCalendar)
In-Reply-To: <971536df0608171135m559f23ddx5c492766306db4e9@mail.gmail.com>
References: <200608171316.22005.pcarl@gsbalum.uchicago.edu>
	<971536df0608171135m559f23ddx5c492766306db4e9@mail.gmail.com>
Message-ID: <971536df0608171325h2f40b312o12ca0f87a23962c8@mail.gmail.com>

That should be:

Sys.putenv(TZ="GMT")

On 8/17/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> I have not examined this in detail but its likely a time zone problem.
> Try setting your time zone to "GMT"
>
> setenv(TZ = "GMT")
>
> at the start of your session and see if you still get the problem.  Also read
> the help desk article in R News 4/1 on dates and times.
>
> On 8/17/06, Peter Carl <pcarl at gsbalum.uchicago.edu> wrote:
> > I've noticed that there seems to be some skew between the x-axis labels
> > and the data points when using plot.timeSeries (in fCalendar).
> >
> > > benchmark.ts
> >           S&P500TR
> > 2002-12-31  -0.0588
> > 2003-01-31  -0.0262
> > 2003-02-28  -0.0150
> > 2003-03-31   0.0097
> > 2003-04-30   0.0824
> > 2003-05-30   0.0527
> > 2003-06-30   0.0128
> > 2003-07-31   0.0176
> > 2003-08-29   0.0195
> > 2003-09-30  -0.0106
> > 2003-10-31   0.0566
> > 2003-11-28   0.0088
> > 2003-12-31   0.0524
> > 2004-01-30   0.0184
> > 2004-02-27   0.0139
> > 2004-03-31  -0.0151
> > 2004-04-30  -0.0157
> > 2004-05-31   0.0137
> > 2004-06-30   0.0194
> > 2004-07-30  -0.0331
> > 2004-08-31   0.0040
> > 2004-09-30   0.0108
> > 2004-10-29   0.0153
> > 2004-11-30   0.0405
> > 2004-12-31   0.0340
> > 2005-01-31  -0.0244
> > 2005-02-28   0.0210
> > 2005-03-31  -0.0177
> > 2005-04-29  -0.0190
> > 2005-05-31   0.0318
> > 2005-06-30   0.0014
> > 2005-07-29   0.0372
> > 2005-08-31  -0.0091
> > 2005-09-30   0.0081
> > 2005-10-31  -0.0167
> > 2005-11-30   0.0378
> > 2005-12-30   0.0003
> > 2006-01-31   0.0265
> > > plot.timeSeries(benchmark.ts,format="%m/%y")
> > > plot.timeSeries(benchmark.ts,format="%Y-%m-%d")
> >
> > Note that the x-axis labels are dated the following day (the first day
> > of the next month) rather than the dates that were passed in with the
> > timeSeries object.  So when viewing the x-axis as "%m/%y", the dates
> > and the data do not appear to be lined up correctly.
> >
> > Am I missing something about how to use the function or is this a
> > genuine bug?
> > --
> > Peter Carl
> > 145 Scottswood Rd
> > Riverside, IL 60546
> > 708.447.6465
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
>


From a.trapletti at swissonline.ch  Fri Aug 18 13:37:34 2006
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Fri, 18 Aug 2006 13:37:34 +0200
Subject: [R-SIG-Finance] Problem with garch (tseries)
Message-ID: <44E5A67E.3060104@swissonline.ch>

The model does not fit the data: Sloppy formulated, your estimated 
models are nearly IGARCH (garchall and garch351), and the garch352 is an 
IGARCH. Compute the unconditional variance given by 
coef(model)[1]/(1-coef(model)[2]-coef(model)[3]) for each of them and 
you will see that the last model does not imply a reasonable 
unconditional variance (also the first two models do imply unconditional 
variances which are "far away" from the observed variance).

The acf of absolute values of your data does not decay exponentially 
fast as with a GARCH process. It looks more like "long memory" or  
"structural changes".

Best regards
Adrian


>Message: 1
>Date: Thu, 17 Aug 2006 07:52:22 -0700 (PDT)
>From: michael mathews <muckjail at yahoo.com>
>Subject: [R-SIG-Finance] Problem with garch (tseries)
>To: r-sig-finance at stat.math.ethz.ch
>Message-ID: <20060817145222.44724.qmail at web38902.mail.mud.yahoo.com>
>Content-Type: text/plain; charset=iso-8859-1
>
>Hi folks,
>I have been playing with garch models to model the volatility in
>physical natural prices. 
>Here is the issue I have a dataset of 801 daily returns (attached).
>If I run
>
>garchall<-garch(hsc)
>
> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>
>
>  
>
>>> summary(garchall)
>>    
>>
>
>Call:
>garch(x = hsc)
>
>Model:
>GARCH(1,1)
>
>Residuals:
>    Min      1Q  Median      3Q     Max 
>-4.3424 -0.5734  0.0000  0.6037  4.0501 
>
>Coefficient(s):
>    Estimate  Std. Error  t value Pr(>|t|)    
>a0 2.507e-05   9.200e-06    2.726  0.00642 ** 
>a1 1.218e-01   2.085e-02    5.840 5.21e-09 ***
>b1 8.759e-01   1.937e-02   45.212  < 2e-16 ***
>---
>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>
>Diagnostic Tests:
>        Jarque Bera Test
>
>data:  Residuals 
>X-squared = 62.7291, df = 2, p-value = 2.387e-14
>
>
>        Box-Ljung test
>
>data:  Squared.Residuals 
>X-squared = 0.0384, df = 1, p-value = 0.8447
>
>Now if we run the same model on a subset say the last 351 days we get
>  
>
>>> garch351<-garch(tail(hsc,351))
>>    
>>
>
> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>
>
>  
>
>>> summary(garch351)
>>    
>>
>
>Call:
>garch(x = tail(hsc, 351))
>
>Model:
>GARCH(1,1)
>
>Residuals:
>      Min        1Q    Median        3Q       Max 
>-4.171521 -0.424628  0.008727  0.532158  3.962116 
>
>Coefficient(s):
>    Estimate  Std. Error  t value Pr(>|t|)    
>a0 2.511e-05   1.589e-05    1.580 0.114167    
>a1 1.043e-01   2.950e-02    3.536 0.000406 ***
>b1 8.957e-01   2.567e-02   34.896  < 2e-16 ***
>---
>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>
>Diagnostic Tests:
>        Jarque Bera Test
>
>data:  Residuals 
>X-squared = 76.3704, df = 2, p-value < 2.2e-16
>
>
>        Box-Ljung test
>
>data:  Squared.Residuals 
>X-squared = 1.2806, df = 1, p-value = 0.2578
>
>still ok. Now finally we get t the point of this email lets look at 352
>days of data:
>
>garch352<-garch(tail(hsc,352))
>
> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>
>
>Warning message:
>NaNs produced in: sqrt(pred$e) 
>  
>
>>> summary(garch352)
>>    
>>
>
>Call:
>garch(x = tail(hsc, 352))
>
>Model:
>GARCH(1,1)
>
>Residuals:
>     Min       1Q   Median       3Q      Max 
>-4.16377 -0.58155  0.01454  0.70886 12.41242 
>
>Coefficient(s):
>    Estimate  Std. Error  t value Pr(>|t|)    
>a0 2.428e-05   1.556e-05    1.561 0.118632    
>a1 1.043e-01   2.947e-02    3.540 0.000400 ***
>b1 8.962e-01   2.556e-02   35.058  < 2e-16 ***
>---
>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>
>Diagnostic Tests:
>        Jarque Bera Test
>
>data:  Residuals 
>X-squared = 10993.57, df = 2, p-value < 2.2e-16
>
>
>        Box-Ljung test
>
>data:  Squared.Residuals 
>X-squared = 0.1831, df = 1, p-value = 0.6687
>
>whats up? Any Ideas. 
>I have also tried using garchFit from the fSeries package but it locks
>up completely left it running last night and it was still spinning this
>morning when I got back to the office.
>
>thanks in advance
>
>michael
>
>
>
>  
>


From Joe-Byers at utulsa.edu  Fri Aug 18 16:18:59 2006
From: Joe-Byers at utulsa.edu (Joe W. Byers)
Date: Fri, 18 Aug 2006 09:18:59 -0500
Subject: [R-SIG-Finance] Problem with garch (tseries)
In-Reply-To: <20060817145222.44724.qmail@web38902.mail.mud.yahoo.com>
References: <20060817145222.44724.qmail@web38902.mail.mud.yahoo.com>
Message-ID: <44E5CC53.3070706@utulsa.edu>

Micheal,

Physical natural daily prices are a problem with time series models 
especially if the physical asset is storable.  Storable assets means 
that the prices are not an easy single model time series process but a 
multi-variate model ie Supply and Demand.  Another problem is the 
returns of these prices are not normally distributed especially over 
short times series. Other things to think about are how liquid are these 
asset prices, the breath and depth of the markets, are is there a well 
developed forward market?

These do not mean that your model is not going to work, it may just not 
be robust next time you estimate it.  I would suggest running some 
outlier tests for spikes in the prices, looking at volatility clustering 
around these outliers.  Also consider using a GED GARCH model that is 
normal distribution under restrictions on the parameters and a negative 
exponential on others.  My collegues and I and U Tulsa have found this 
works well for some Power prices and Weather Temps.  I am working on it 
for other prices series as well.

Good Luck
Joe

michael mathews wrote:
> Hi folks,
> I have been playing with garch models to model the volatility in
> physical natural prices. 
> Here is the issue I have a dataset of 801 daily returns (attached).
> If I run
> 
> garchall<-garch(hsc)
> 
>  ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
> 
> 
>> summary(garchall)
> 
> Call:
> garch(x = hsc)
> 
> Model:
> GARCH(1,1)
> 
> Residuals:
>     Min      1Q  Median      3Q     Max 
> -4.3424 -0.5734  0.0000  0.6037  4.0501 
> 
> Coefficient(s):
>     Estimate  Std. Error  t value Pr(>|t|)    
> a0 2.507e-05   9.200e-06    2.726  0.00642 ** 
> a1 1.218e-01   2.085e-02    5.840 5.21e-09 ***
> b1 8.759e-01   1.937e-02   45.212  < 2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Diagnostic Tests:
>         Jarque Bera Test
> 
> data:  Residuals 
> X-squared = 62.7291, df = 2, p-value = 2.387e-14
> 
> 
>         Box-Ljung test
> 
> data:  Squared.Residuals 
> X-squared = 0.0384, df = 1, p-value = 0.8447
> 
> Now if we run the same model on a subset say the last 351 days we get
>> garch351<-garch(tail(hsc,351))
> 
>  ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
> 
> 
>> summary(garch351)
> 
> Call:
> garch(x = tail(hsc, 351))
> 
> Model:
> GARCH(1,1)
> 
> Residuals:
>       Min        1Q    Median        3Q       Max 
> -4.171521 -0.424628  0.008727  0.532158  3.962116 
> 
> Coefficient(s):
>     Estimate  Std. Error  t value Pr(>|t|)    
> a0 2.511e-05   1.589e-05    1.580 0.114167    
> a1 1.043e-01   2.950e-02    3.536 0.000406 ***
> b1 8.957e-01   2.567e-02   34.896  < 2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Diagnostic Tests:
>         Jarque Bera Test
> 
> data:  Residuals 
> X-squared = 76.3704, df = 2, p-value < 2.2e-16
> 
> 
>         Box-Ljung test
> 
> data:  Squared.Residuals 
> X-squared = 1.2806, df = 1, p-value = 0.2578
> 
> still ok. Now finally we get t the point of this email lets look at 352
> days of data:
> 
> garch352<-garch(tail(hsc,352))
> 
>  ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
> 
> 
> Warning message:
> NaNs produced in: sqrt(pred$e) 
>> summary(garch352)
> 
> Call:
> garch(x = tail(hsc, 352))
> 
> Model:
> GARCH(1,1)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max 
> -4.16377 -0.58155  0.01454  0.70886 12.41242 
> 
> Coefficient(s):
>     Estimate  Std. Error  t value Pr(>|t|)    
> a0 2.428e-05   1.556e-05    1.561 0.118632    
> a1 1.043e-01   2.947e-02    3.540 0.000400 ***
> b1 8.962e-01   2.556e-02   35.058  < 2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Diagnostic Tests:
>         Jarque Bera Test
> 
> data:  Residuals 
> X-squared = 10993.57, df = 2, p-value < 2.2e-16
> 
> 
>         Box-Ljung test
> 
> data:  Squared.Residuals 
> X-squared = 0.1831, df = 1, p-value = 0.6687
> 
> whats up? Any Ideas. 
> I have also tried using garchFit from the fSeries package but it locks
> up completely left it running last night and it was still spinning this
> morning when I got back to the office.
> 
> thanks in advance
> 
> michael
>


From patrick at burns-stat.com  Fri Aug 18 17:22:39 2006
From: patrick at burns-stat.com (Patrick Burns)
Date: Fri, 18 Aug 2006 16:22:39 +0100
Subject: [R-SIG-Finance] Problem with garch (tseries)
In-Reply-To: <44E5CC53.3070706@utulsa.edu>
References: <20060817145222.44724.qmail@web38902.mail.mud.yahoo.com>
	<44E5CC53.3070706@utulsa.edu>
Message-ID: <44E5DB3F.1090105@burns-stat.com>

These are good points.  But probably a key aspect
is that garch thinks 800 data points is a small sample,
and 400 points (at least for daily data) is likely to yield
parameter estimates that are exceptionally variable.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Joe W. Byers wrote:

>Micheal,
>
>Physical natural daily prices are a problem with time series models 
>especially if the physical asset is storable.  Storable assets means 
>that the prices are not an easy single model time series process but a 
>multi-variate model ie Supply and Demand.  Another problem is the 
>returns of these prices are not normally distributed especially over 
>short times series. Other things to think about are how liquid are these 
>asset prices, the breath and depth of the markets, are is there a well 
>developed forward market?
>
>These do not mean that your model is not going to work, it may just not 
>be robust next time you estimate it.  I would suggest running some 
>outlier tests for spikes in the prices, looking at volatility clustering 
>around these outliers.  Also consider using a GED GARCH model that is 
>normal distribution under restrictions on the parameters and a negative 
>exponential on others.  My collegues and I and U Tulsa have found this 
>works well for some Power prices and Weather Temps.  I am working on it 
>for other prices series as well.
>
>Good Luck
>Joe
>
>michael mathews wrote:
>  
>
>>Hi folks,
>>I have been playing with garch models to model the volatility in
>>physical natural prices. 
>>Here is the issue I have a dataset of 801 daily returns (attached).
>>If I run
>>
>>garchall<-garch(hsc)
>>
>> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>>
>>
>>    
>>
>>>summary(garchall)
>>>      
>>>
>>Call:
>>garch(x = hsc)
>>
>>Model:
>>GARCH(1,1)
>>
>>Residuals:
>>    Min      1Q  Median      3Q     Max 
>>-4.3424 -0.5734  0.0000  0.6037  4.0501 
>>
>>Coefficient(s):
>>    Estimate  Std. Error  t value Pr(>|t|)    
>>a0 2.507e-05   9.200e-06    2.726  0.00642 ** 
>>a1 1.218e-01   2.085e-02    5.840 5.21e-09 ***
>>b1 8.759e-01   1.937e-02   45.212  < 2e-16 ***
>>---
>>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>>
>>Diagnostic Tests:
>>        Jarque Bera Test
>>
>>data:  Residuals 
>>X-squared = 62.7291, df = 2, p-value = 2.387e-14
>>
>>
>>        Box-Ljung test
>>
>>data:  Squared.Residuals 
>>X-squared = 0.0384, df = 1, p-value = 0.8447
>>
>>Now if we run the same model on a subset say the last 351 days we get
>>    
>>
>>>garch351<-garch(tail(hsc,351))
>>>      
>>>
>> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>>
>>
>>    
>>
>>>summary(garch351)
>>>      
>>>
>>Call:
>>garch(x = tail(hsc, 351))
>>
>>Model:
>>GARCH(1,1)
>>
>>Residuals:
>>      Min        1Q    Median        3Q       Max 
>>-4.171521 -0.424628  0.008727  0.532158  3.962116 
>>
>>Coefficient(s):
>>    Estimate  Std. Error  t value Pr(>|t|)    
>>a0 2.511e-05   1.589e-05    1.580 0.114167    
>>a1 1.043e-01   2.950e-02    3.536 0.000406 ***
>>b1 8.957e-01   2.567e-02   34.896  < 2e-16 ***
>>---
>>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>>
>>Diagnostic Tests:
>>        Jarque Bera Test
>>
>>data:  Residuals 
>>X-squared = 76.3704, df = 2, p-value < 2.2e-16
>>
>>
>>        Box-Ljung test
>>
>>data:  Squared.Residuals 
>>X-squared = 1.2806, df = 1, p-value = 0.2578
>>
>>still ok. Now finally we get t the point of this email lets look at 352
>>days of data:
>>
>>garch352<-garch(tail(hsc,352))
>>
>> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>>
>>
>>Warning message:
>>NaNs produced in: sqrt(pred$e) 
>>    
>>
>>>summary(garch352)
>>>      
>>>
>>Call:
>>garch(x = tail(hsc, 352))
>>
>>Model:
>>GARCH(1,1)
>>
>>Residuals:
>>     Min       1Q   Median       3Q      Max 
>>-4.16377 -0.58155  0.01454  0.70886 12.41242 
>>
>>Coefficient(s):
>>    Estimate  Std. Error  t value Pr(>|t|)    
>>a0 2.428e-05   1.556e-05    1.561 0.118632    
>>a1 1.043e-01   2.947e-02    3.540 0.000400 ***
>>b1 8.962e-01   2.556e-02   35.058  < 2e-16 ***
>>---
>>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>>
>>Diagnostic Tests:
>>        Jarque Bera Test
>>
>>data:  Residuals 
>>X-squared = 10993.57, df = 2, p-value < 2.2e-16
>>
>>
>>        Box-Ljung test
>>
>>data:  Squared.Residuals 
>>X-squared = 0.1831, df = 1, p-value = 0.6687
>>
>>whats up? Any Ideas. 
>>I have also tried using garchFit from the fSeries package but it locks
>>up completely left it running last night and it was still spinning this
>>morning when I got back to the office.
>>
>>thanks in advance
>>
>>michael
>>
>>    
>>
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
>  
>


From Ricardo.Zambrano at corpbanca.cl  Fri Aug 18 18:03:00 2006
From: Ricardo.Zambrano at corpbanca.cl (Ricardo Zambrano Aguilera)
Date: Fri, 18 Aug 2006 12:03:00 -0400
Subject: [R-SIG-Finance] Problem with garch (tseries)
Message-ID: <800F3C4ADCFCA643A919EF7C140A045A0DB9899D@LLANQUIHUE.corpgroup.cl>

	greetings

	i have a doubt with the meaning of the jarque bera test and ljung box test, all because of this...
	1) why if i did simulate a garch(1,1) process with cond.dist="rnorm", then the jarque bera reject the null hyp??
	2)what it say me the box-ljung test??

	> creatingdata<-garchSim(model = list(omega = 1.0e-6, alpha = 0.1, beta = 0.8), n = 2000, 
	+     n.start = 100, presample = NULL, cond.dist = "rnorm", rseed = NULL)
	> adjuste<-garch(creatingdata,order=c(1,1))

	 ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 


	> summary(adjuste)

	Call:
	garch(x = creatingdata, order = c(1, 1))

	Model:
	GARCH(1,1)

	Residuals:
	     Min       1Q   Median       3Q      Max 
	-3.75511 -0.65994  0.01976  0.70273  3.12522 

	Coefficient(s):
	    Estimate  Std. Error  t value Pr(>|t|)    
	a0 8.245e-07   2.977e-07    2.770  0.00561 ** 
	a1 8.083e-02   1.847e-02    4.376 1.21e-05 ***
	b1 8.371e-01   4.198e-02   19.940  < 2e-16 ***
	---
	Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

	Diagnostic Tests:
	        Jarque Bera Test

	data:  Residuals 
	X-squared = 2.6605, df = 2, p-value = 0.2644


	        Box-Ljung test

	data:  Squared.Residuals 
	X-squared = 0.083, df = 1, p-value = 0.7732

	> 


> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From Ricardo.Zambrano at corpbanca.cl  Fri Aug 18 19:07:59 2006
From: Ricardo.Zambrano at corpbanca.cl (Ricardo Zambrano Aguilera)
Date: Fri, 18 Aug 2006 13:07:59 -0400
Subject: [R-SIG-Finance] Problem with garch (tseries)
Message-ID: <800F3C4ADCFCA643A919EF7C140A045A0DBAF554@LLANQUIHUE.corpgroup.cl>


Se?ores
Can i do some forescast of log returns ???
i?m doing  an ARMA-GARCH process  step by step 
rt=ut+et
 
why am i do this?? because just like this i can to remove the no significative parameters and come back to estimate it?s that right??, (because it  exist  the great fSeries package, and there?s no reason for no use it???)



now can i predict return of this way....???


predict(arma)+ predict(garch) of any sentence of the times series packages????


thanks a lot 

pd: by the way ... can i use the iid.test package to seek the behavior of the returns????  

Ricardo Zambrano Aguilera
Chile


From muckjail at yahoo.com  Sat Aug 19 02:50:22 2006
From: muckjail at yahoo.com (michael mathews)
Date: Fri, 18 Aug 2006 17:50:22 -0700 (PDT)
Subject: [R-SIG-Finance] Problem with garch (tseries)
In-Reply-To: <44E5DB3F.1090105@burns-stat.com>
Message-ID: <20060819005022.79785.qmail@web38912.mail.mud.yahoo.com>

As I mentioned to Joe off list I am dealing with natural Gas prices at
houston ship channel. While I could assemble a longer price series it
seems that nat gas has entered into a new price/volatility regime in
the last two years which I think makes using older data somewhat
problematic.

If garch does not like "small" samples what would be a robust way to
estimate the volatility?

thanks for your input

michael

--- Patrick Burns <patrick at burns-stat.com> wrote:

> These are good points.  But probably a key aspect
> is that garch thinks 800 data points is a small sample,
> and 400 points (at least for daily data) is likely to yield
> parameter estimates that are exceptionally variable.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Joe W. Byers wrote:
> 
> >Micheal,
> >
> >Physical natural daily prices are a problem with time series models 
> >especially if the physical asset is storable.  Storable assets means
> 
> >that the prices are not an easy single model time series process but
> a 
> >multi-variate model ie Supply and Demand.  Another problem is the 
> >returns of these prices are not normally distributed especially over
> 
> >short times series. Other things to think about are how liquid are
> these 
> >asset prices, the breath and depth of the markets, are is there a
> well 
> >developed forward market?
> >
> >These do not mean that your model is not going to work, it may just
> not 
> >be robust next time you estimate it.  I would suggest running some 
> >outlier tests for spikes in the prices, looking at volatility
> clustering 
> >around these outliers.  Also consider using a GED GARCH model that
> is 
> >normal distribution under restrictions on the parameters and a
> negative 
> >exponential on others.  My collegues and I and U Tulsa have found
> this 
> >works well for some Power prices and Weather Temps.  I am working on
> it 
> >for other prices series as well.
> >
> >Good Luck
> >Joe
> >
> >michael mathews wrote:
> >  
> >
> >>Hi folks,
> >>I have been playing with garch models to model the volatility in
> >>physical natural prices. 
> >>Here is the issue I have a dataset of 801 daily returns (attached).
> >>If I run
> >>
> >>garchall<-garch(hsc)
> >>
> >> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
> >>
> >>
> >>    
> >>
> >>>summary(garchall)
> >>>      
> >>>
> >>Call:
> >>garch(x = hsc)
> >>
> >>Model:
> >>GARCH(1,1)
> >>
> >>Residuals:
> >>    Min      1Q  Median      3Q     Max 
> >>-4.3424 -0.5734  0.0000  0.6037  4.0501 
> >>
> >>Coefficient(s):
> >>    Estimate  Std. Error  t value Pr(>|t|)    
> >>a0 2.507e-05   9.200e-06    2.726  0.00642 ** 
> >>a1 1.218e-01   2.085e-02    5.840 5.21e-09 ***
> >>b1 8.759e-01   1.937e-02   45.212  < 2e-16 ***
> >>---
> >>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> >>
> >>Diagnostic Tests:
> >>        Jarque Bera Test
> >>
> >>data:  Residuals 
> >>X-squared = 62.7291, df = 2, p-value = 2.387e-14
> >>
> >>
> >>        Box-Ljung test
> >>
> >>data:  Squared.Residuals 
> >>X-squared = 0.0384, df = 1, p-value = 0.8447
> >>
> >>Now if we run the same model on a subset say the last 351 days we
> get
> >>    
> >>
> >>>garch351<-garch(tail(hsc,351))
> >>>      
> >>>
> >> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
> >>
> >>
> >>    
> >>
> >>>summary(garch351)
> >>>      
> >>>
> >>Call:
> >>garch(x = tail(hsc, 351))
> >>
> >>Model:
> >>GARCH(1,1)
> >>
> >>Residuals:
> >>      Min        1Q    Median        3Q       Max 
> >>-4.171521 -0.424628  0.008727  0.532158  3.962116 
> >>
> >>Coefficient(s):
> >>    Estimate  Std. Error  t value Pr(>|t|)    
> >>a0 2.511e-05   1.589e-05    1.580 0.114167    
> >>a1 1.043e-01   2.950e-02    3.536 0.000406 ***
> >>b1 8.957e-01   2.567e-02   34.896  < 2e-16 ***
> >>---
> >>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> >>
> >>Diagnostic Tests:
> >>        Jarque Bera Test
> >>
> >>data:  Residuals 
> >>X-squared = 76.3704, df = 2, p-value < 2.2e-16
> >>
> >>
> >>        Box-Ljung test
> >>
> >>data:  Squared.Residuals 
> >>X-squared = 1.2806, df = 1, p-value = 0.2578
> >>
> >>still ok. Now finally we get t the point of this email lets look at
> 352
> >>days of data:
> >>
> >>garch352<-garch(tail(hsc,352))
> >>
> >> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
> >>
> >>
> >>Warning message:
> >>NaNs produced in: sqrt(pred$e) 
> >>    
> >>
> >>>summary(garch352)
> >>>      
> >>>
> >>Call:
> >>garch(x = tail(hsc, 352))
> >>
> >>Model:
> >>GARCH(1,1)
> >>
> >>Residuals:
> >>     Min       1Q   Median       3Q      Max 
> >>-4.16377 -0.58155  0.01454  0.70886 12.41242 
> >>
> >>Coefficient(s):
> >>    Estimate  Std. Error  t value Pr(>|t|)    
> >>a0 2.428e-05   1.556e-05    1.561 0.118632    
> >>a1 1.043e-01   2.947e-02    3.540 0.000400 ***
> >>b1 8.962e-01   2.556e-02   35.058  < 2e-16 ***
> >>---
> >>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> >>
> >>Diagnostic Tests:
> >>        Jarque Bera Test
> >>
> >>data:  Residuals 
> >>X-squared = 10993.57, df = 2, p-value < 2.2e-16
> >>
> >>
> >>        Box-Ljung test
> >>
> >>data:  Squared.Residuals 
> >>X-squared = 0.1831, df = 1, p-value = 0.6687
> >>
> >>whats up? Any Ideas. 
> >>I have also tried using garchFit from the fSeries package but it
> locks
> >>up completely left it running last night and it was still spinning
> this
> >>morning when I got back to the office.
> >>
> >>thanks in advance
> >>
> >>michael
> >>
> >>    
> >>
> >
> 
=== message truncated ===


From patrick at burns-stat.com  Sat Aug 19 11:10:34 2006
From: patrick at burns-stat.com (Patrick Burns)
Date: Sat, 19 Aug 2006 10:10:34 +0100
Subject: [R-SIG-Finance] Problem with garch (tseries)
In-Reply-To: <20060819005022.79785.qmail@web38912.mail.mud.yahoo.com>
References: <20060819005022.79785.qmail@web38912.mail.mud.yahoo.com>
Message-ID: <44E6D58A.7020209@burns-stat.com>

Two ideas, no guarantees.

The working paper "The quality of Value at Risk via
univariate GARCH" on the Burns Statistics website
indicates that giving more weight to recent observations
in the estimation can be a good thing.

I don't have experience with it, but it seems to me that
Bayesian estimates could be useful.  We have a reasonable
idea of what are silly estimates and what are not so silly,
so an informative prior makes sense.

If you have implied volatilities at different expiries, then you
should be able to get a sense of the half-life.  For the garch(1,1)
model this is a function of alpha plus beta.  So that would be
something to put a prior on.  But the components model
(described in the paper already referred to) is likely to give
better predictions, and its half-life only depends on one
parameter.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

michael mathews wrote:

>As I mentioned to Joe off list I am dealing with natural Gas prices at
>houston ship channel. While I could assemble a longer price series it
>seems that nat gas has entered into a new price/volatility regime in
>the last two years which I think makes using older data somewhat
>problematic.
>
>If garch does not like "small" samples what would be a robust way to
>estimate the volatility?
>
>thanks for your input
>
>michael
>
>--- Patrick Burns <patrick at burns-stat.com> wrote:
>
>  
>
>>These are good points.  But probably a key aspect
>>is that garch thinks 800 data points is a small sample,
>>and 400 points (at least for daily data) is likely to yield
>>parameter estimates that are exceptionally variable.
>>
>>Patrick Burns
>>patrick at burns-stat.com
>>+44 (0)20 8525 0696
>>http://www.burns-stat.com
>>(home of S Poetry and "A Guide for the Unwilling S User")
>>
>>Joe W. Byers wrote:
>>
>>    
>>
>>>Micheal,
>>>
>>>Physical natural daily prices are a problem with time series models 
>>>especially if the physical asset is storable.  Storable assets means
>>>      
>>>
>>>that the prices are not an easy single model time series process but
>>>      
>>>
>>a 
>>    
>>
>>>multi-variate model ie Supply and Demand.  Another problem is the 
>>>returns of these prices are not normally distributed especially over
>>>      
>>>
>>>short times series. Other things to think about are how liquid are
>>>      
>>>
>>these 
>>    
>>
>>>asset prices, the breath and depth of the markets, are is there a
>>>      
>>>
>>well 
>>    
>>
>>>developed forward market?
>>>
>>>These do not mean that your model is not going to work, it may just
>>>      
>>>
>>not 
>>    
>>
>>>be robust next time you estimate it.  I would suggest running some 
>>>outlier tests for spikes in the prices, looking at volatility
>>>      
>>>
>>clustering 
>>    
>>
>>>around these outliers.  Also consider using a GED GARCH model that
>>>      
>>>
>>is 
>>    
>>
>>>normal distribution under restrictions on the parameters and a
>>>      
>>>
>>negative 
>>    
>>
>>>exponential on others.  My collegues and I and U Tulsa have found
>>>      
>>>
>>this 
>>    
>>
>>>works well for some Power prices and Weather Temps.  I am working on
>>>      
>>>
>>it 
>>    
>>
>>>for other prices series as well.
>>>
>>>Good Luck
>>>Joe
>>>
>>>michael mathews wrote:
>>> 
>>>
>>>      
>>>
>>>>Hi folks,
>>>>I have been playing with garch models to model the volatility in
>>>>physical natural prices. 
>>>>Here is the issue I have a dataset of 801 daily returns (attached).
>>>>If I run
>>>>
>>>>garchall<-garch(hsc)
>>>>
>>>>***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>>>>
>>>>
>>>>   
>>>>
>>>>        
>>>>
>>>>>summary(garchall)
>>>>>     
>>>>>
>>>>>          
>>>>>
>>>>Call:
>>>>garch(x = hsc)
>>>>
>>>>Model:
>>>>GARCH(1,1)
>>>>
>>>>Residuals:
>>>>   Min      1Q  Median      3Q     Max 
>>>>-4.3424 -0.5734  0.0000  0.6037  4.0501 
>>>>
>>>>Coefficient(s):
>>>>   Estimate  Std. Error  t value Pr(>|t|)    
>>>>a0 2.507e-05   9.200e-06    2.726  0.00642 ** 
>>>>a1 1.218e-01   2.085e-02    5.840 5.21e-09 ***
>>>>b1 8.759e-01   1.937e-02   45.212  < 2e-16 ***
>>>>---
>>>>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>>>>
>>>>Diagnostic Tests:
>>>>       Jarque Bera Test
>>>>
>>>>data:  Residuals 
>>>>X-squared = 62.7291, df = 2, p-value = 2.387e-14
>>>>
>>>>
>>>>       Box-Ljung test
>>>>
>>>>data:  Squared.Residuals 
>>>>X-squared = 0.0384, df = 1, p-value = 0.8447
>>>>
>>>>Now if we run the same model on a subset say the last 351 days we
>>>>        
>>>>
>>get
>>    
>>
>>>>   
>>>>
>>>>        
>>>>
>>>>>garch351<-garch(tail(hsc,351))
>>>>>     
>>>>>
>>>>>          
>>>>>
>>>>***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>>>>
>>>>
>>>>   
>>>>
>>>>        
>>>>
>>>>>summary(garch351)
>>>>>     
>>>>>
>>>>>          
>>>>>
>>>>Call:
>>>>garch(x = tail(hsc, 351))
>>>>
>>>>Model:
>>>>GARCH(1,1)
>>>>
>>>>Residuals:
>>>>     Min        1Q    Median        3Q       Max 
>>>>-4.171521 -0.424628  0.008727  0.532158  3.962116 
>>>>
>>>>Coefficient(s):
>>>>   Estimate  Std. Error  t value Pr(>|t|)    
>>>>a0 2.511e-05   1.589e-05    1.580 0.114167    
>>>>a1 1.043e-01   2.950e-02    3.536 0.000406 ***
>>>>b1 8.957e-01   2.567e-02   34.896  < 2e-16 ***
>>>>---
>>>>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>>>>
>>>>Diagnostic Tests:
>>>>       Jarque Bera Test
>>>>
>>>>data:  Residuals 
>>>>X-squared = 76.3704, df = 2, p-value < 2.2e-16
>>>>
>>>>
>>>>       Box-Ljung test
>>>>
>>>>data:  Squared.Residuals 
>>>>X-squared = 1.2806, df = 1, p-value = 0.2578
>>>>
>>>>still ok. Now finally we get t the point of this email lets look at
>>>>        
>>>>
>>352
>>    
>>
>>>>days of data:
>>>>
>>>>garch352<-garch(tail(hsc,352))
>>>>
>>>>***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>>>>
>>>>
>>>>Warning message:
>>>>NaNs produced in: sqrt(pred$e) 
>>>>   
>>>>
>>>>        
>>>>
>>>>>summary(garch352)
>>>>>     
>>>>>
>>>>>          
>>>>>
>>>>Call:
>>>>garch(x = tail(hsc, 352))
>>>>
>>>>Model:
>>>>GARCH(1,1)
>>>>
>>>>Residuals:
>>>>    Min       1Q   Median       3Q      Max 
>>>>-4.16377 -0.58155  0.01454  0.70886 12.41242 
>>>>
>>>>Coefficient(s):
>>>>   Estimate  Std. Error  t value Pr(>|t|)    
>>>>a0 2.428e-05   1.556e-05    1.561 0.118632    
>>>>a1 1.043e-01   2.947e-02    3.540 0.000400 ***
>>>>b1 8.962e-01   2.556e-02   35.058  < 2e-16 ***
>>>>---
>>>>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>>>>
>>>>Diagnostic Tests:
>>>>       Jarque Bera Test
>>>>
>>>>data:  Residuals 
>>>>X-squared = 10993.57, df = 2, p-value < 2.2e-16
>>>>
>>>>
>>>>       Box-Ljung test
>>>>
>>>>data:  Squared.Residuals 
>>>>X-squared = 0.1831, df = 1, p-value = 0.6687
>>>>
>>>>whats up? Any Ideas. 
>>>>I have also tried using garchFit from the fSeries package but it
>>>>        
>>>>
>>locks
>>    
>>
>>>>up completely left it running last night and it was still spinning
>>>>        
>>>>
>>this
>>    
>>
>>>>morning when I got back to the office.
>>>>
>>>>thanks in advance
>>>>
>>>>michael
>>>>
>>>>   
>>>>
>>>>        
>>>>
>=== message truncated ===
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>--------------------------------------------------------
>
>If you are not an intended recipient of this e-mail, please notify the sender, delete it and do not read, act upon, print, disclose, copy, retain or redistribute it. Click here for important additional terms relating to this e-mail.     http://www.ml.com/email_terms/
>--------------------------------------------------------
>
>
>  
>


From a.trapletti at swissonline.ch  Sat Aug 19 14:20:39 2006
From: a.trapletti at swissonline.ch (Adrian Trapletti)
Date: Sat, 19 Aug 2006 14:20:39 +0200
Subject: [R-SIG-Finance] Problem with garch (tseries)
Message-ID: <44E70217.5080906@swissonline.ch>

Volatility of financial time series often exhibits a very persistent 
behaviour which is not well represented by plain GARCH. Many models have 
been developped to address this issue among others IGARCH and long 
memory GARCH. My personal point of view is that this almost unit root or 
long memory behaviour is due to the fact that many structural breaks 
(exogenous shocks) of different size occur randomly from time to time. 
Appropriately simulated time series of such structural break models 
cannot be distinguished from long memory GARCH for example.

To give you some input to your question: A very robust solution is to 
use simple running averages or medians (or other robust measures of 
location) of recent realized volatility. There is a lot of recent 
research about estimating realized volatility, one very simple measure 
is to use the range (high minus low of the period) appropriately scaled 
to represent standard deviations. According to my experience forecasts 
based on these simple models are often better than forecasts based on 
complex statistical time series models, in particular if the loss 
function is not a statistical one, but an economical one.

Best regards
Adrian

>Message: 6
>Date: Fri, 18 Aug 2006 17:50:22 -0700 (PDT)
>From: michael mathews <muckjail at yahoo.com>
>Subject: Re: [R-SIG-Finance] Problem with garch (tseries)
>To: Patrick Burns <patrick at burns-stat.com>
>Cc: r-sig-finance at stat.math.ethz.ch
>Message-ID: <20060819005022.79785.qmail at web38912.mail.mud.yahoo.com>
>Content-Type: text/plain; charset=iso-8859-1
>
>As I mentioned to Joe off list I am dealing with natural Gas prices at
>houston ship channel. While I could assemble a longer price series it
>seems that nat gas has entered into a new price/volatility regime in
>the last two years which I think makes using older data somewhat
>problematic.
>
>If garch does not like "small" samples what would be a robust way to
>estimate the volatility?
>
>thanks for your input
>
>michael
>


From ezivot at u.washington.edu  Sun Aug 20 00:17:51 2006
From: ezivot at u.washington.edu (Eric Zivot)
Date: Sat, 19 Aug 2006 15:17:51 -0700
Subject: [R-SIG-Finance] Problem with garch (tseries)
In-Reply-To: <20060819005022.79785.qmail@web38912.mail.mud.yahoo.com>
Message-ID: <000a01c6c3dd$50643110$6401a8c0@zivotd800>

The riskmetrics approach (simple EWMA on r(t)^2 with lamda = 0.94) is a
stable adaptive volatility estimator. It's also a simple restricted IGARCH
model. No estimation is required. Carol Alexander has a nice discussion of 
Robustness issues in the estimation of garch models in chapters 3-5 of her
book Market Models.

Also, simple markov switching models in variance sometimes work as a good
alternative to garch models when volatility goes through abrupt regimes. For
returns, the basic hidden markov model works reasonably well and there is an
R package for estimating HMMs.

ez
-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of michael
mathews
Sent: Friday, August 18, 2006 4:50 PM
To: Patrick Burns
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Problem with garch (tseries)

As I mentioned to Joe off list I am dealing with natural Gas prices at
houston ship channel. While I could assemble a longer price series it
seems that nat gas has entered into a new price/volatility regime in
the last two years which I think makes using older data somewhat
problematic.

If garch does not like "small" samples what would be a robust way to
estimate the volatility?

thanks for your input

michael

--- Patrick Burns <patrick at burns-stat.com> wrote:

> These are good points.  But probably a key aspect
> is that garch thinks 800 data points is a small sample,
> and 400 points (at least for daily data) is likely to yield
> parameter estimates that are exceptionally variable.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Joe W. Byers wrote:
> 
> >Micheal,
> >
> >Physical natural daily prices are a problem with time series models 
> >especially if the physical asset is storable.  Storable assets means
> 
> >that the prices are not an easy single model time series process but
> a 
> >multi-variate model ie Supply and Demand.  Another problem is the 
> >returns of these prices are not normally distributed especially over
> 
> >short times series. Other things to think about are how liquid are
> these 
> >asset prices, the breath and depth of the markets, are is there a
> well 
> >developed forward market?
> >
> >These do not mean that your model is not going to work, it may just
> not 
> >be robust next time you estimate it.  I would suggest running some 
> >outlier tests for spikes in the prices, looking at volatility
> clustering 
> >around these outliers.  Also consider using a GED GARCH model that
> is 
> >normal distribution under restrictions on the parameters and a
> negative 
> >exponential on others.  My collegues and I and U Tulsa have found
> this 
> >works well for some Power prices and Weather Temps.  I am working on
> it 
> >for other prices series as well.
> >
> >Good Luck
> >Joe
> >
> >michael mathews wrote:
> >  
> >
> >>Hi folks,
> >>I have been playing with garch models to model the volatility in
> >>physical natural prices. 
> >>Here is the issue I have a dataset of 801 daily returns (attached).
> >>If I run
> >>
> >>garchall<-garch(hsc)
> >>
> >> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
> >>
> >>
> >>    
> >>
> >>>summary(garchall)
> >>>      
> >>>
> >>Call:
> >>garch(x = hsc)
> >>
> >>Model:
> >>GARCH(1,1)
> >>
> >>Residuals:
> >>    Min      1Q  Median      3Q     Max 
> >>-4.3424 -0.5734  0.0000  0.6037  4.0501 
> >>
> >>Coefficient(s):
> >>    Estimate  Std. Error  t value Pr(>|t|)    
> >>a0 2.507e-05   9.200e-06    2.726  0.00642 ** 
> >>a1 1.218e-01   2.085e-02    5.840 5.21e-09 ***
> >>b1 8.759e-01   1.937e-02   45.212  < 2e-16 ***
> >>---
> >>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> >>
> >>Diagnostic Tests:
> >>        Jarque Bera Test
> >>
> >>data:  Residuals 
> >>X-squared = 62.7291, df = 2, p-value = 2.387e-14
> >>
> >>
> >>        Box-Ljung test
> >>
> >>data:  Squared.Residuals 
> >>X-squared = 0.0384, df = 1, p-value = 0.8447
> >>
> >>Now if we run the same model on a subset say the last 351 days we
> get
> >>    
> >>
> >>>garch351<-garch(tail(hsc,351))
> >>>      
> >>>
> >> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
> >>
> >>
> >>    
> >>
> >>>summary(garch351)
> >>>      
> >>>
> >>Call:
> >>garch(x = tail(hsc, 351))
> >>
> >>Model:
> >>GARCH(1,1)
> >>
> >>Residuals:
> >>      Min        1Q    Median        3Q       Max 
> >>-4.171521 -0.424628  0.008727  0.532158  3.962116 
> >>
> >>Coefficient(s):
> >>    Estimate  Std. Error  t value Pr(>|t|)    
> >>a0 2.511e-05   1.589e-05    1.580 0.114167    
> >>a1 1.043e-01   2.950e-02    3.536 0.000406 ***
> >>b1 8.957e-01   2.567e-02   34.896  < 2e-16 ***
> >>---
> >>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> >>
> >>Diagnostic Tests:
> >>        Jarque Bera Test
> >>
> >>data:  Residuals 
> >>X-squared = 76.3704, df = 2, p-value < 2.2e-16
> >>
> >>
> >>        Box-Ljung test
> >>
> >>data:  Squared.Residuals 
> >>X-squared = 1.2806, df = 1, p-value = 0.2578
> >>
> >>still ok. Now finally we get t the point of this email lets look at
> 352
> >>days of data:
> >>
> >>garch352<-garch(tail(hsc,352))
> >>
> >> ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
> >>
> >>
> >>Warning message:
> >>NaNs produced in: sqrt(pred$e) 
> >>    
> >>
> >>>summary(garch352)
> >>>      
> >>>
> >>Call:
> >>garch(x = tail(hsc, 352))
> >>
> >>Model:
> >>GARCH(1,1)
> >>
> >>Residuals:
> >>     Min       1Q   Median       3Q      Max 
> >>-4.16377 -0.58155  0.01454  0.70886 12.41242 
> >>
> >>Coefficient(s):
> >>    Estimate  Std. Error  t value Pr(>|t|)    
> >>a0 2.428e-05   1.556e-05    1.561 0.118632    
> >>a1 1.043e-01   2.947e-02    3.540 0.000400 ***
> >>b1 8.962e-01   2.556e-02   35.058  < 2e-16 ***
> >>---
> >>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> >>
> >>Diagnostic Tests:
> >>        Jarque Bera Test
> >>
> >>data:  Residuals 
> >>X-squared = 10993.57, df = 2, p-value < 2.2e-16
> >>
> >>
> >>        Box-Ljung test
> >>
> >>data:  Squared.Residuals 
> >>X-squared = 0.1831, df = 1, p-value = 0.6687
> >>
> >>whats up? Any Ideas. 
> >>I have also tried using garchFit from the fSeries package but it
> locks
> >>up completely left it running last night and it was still spinning
> this
> >>morning when I got back to the office.
> >>
> >>thanks in advance
> >>
> >>michael
> >>
> >>    
> >>
> >
> 
=== message truncated ===

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From Ricardo.Zambrano at corpbanca.cl  Mon Aug 21 20:19:02 2006
From: Ricardo.Zambrano at corpbanca.cl (Ricardo Zambrano Aguilera)
Date: Mon, 21 Aug 2006 14:19:02 -0400
Subject: [R-SIG-Finance] Problem with garch (tseries)
Message-ID: <800F3C4ADCFCA643A919EF7C140A045A0DC6BD9B@LLANQUIHUE.corpgroup.cl>

Greetings
i have this time series, (currency japanese log returns)
the data?s iid and it don?t have any serial correlation
but it does not fit to a regular garch ...
what could it be?
best regards

RZA
 <<JPY.txt>> 
pd:weekens removed

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: JPY.txt
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060821/98166406/attachment.txt 

From markleeds2 at yahoo.com  Mon Aug 21 23:06:16 2006
From: markleeds2 at yahoo.com (Mark Leeds)
Date: Mon, 21 Aug 2006 14:06:16 -0700 (PDT)
Subject: [R-SIG-Finance] Problem with garch (tseries)
In-Reply-To: <800F3C4ADCFCA643A919EF7C140A045A0DC6BD9B@LLANQUIHUE.corpgroup.cl>
Message-ID: <20060821210616.60352.qmail@web56203.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060821/ebbc1b27/attachment.pl 

From hkahra at gmail.com  Tue Aug 22 00:16:11 2006
From: hkahra at gmail.com (Hannu Kahra)
Date: Tue, 22 Aug 2006 01:16:11 +0300
Subject: [R-SIG-Finance] Problem with garch (tseries)
In-Reply-To: <800F3C4ADCFCA643A919EF7C140A045A0DC6BD9B@LLANQUIHUE.corpgroup.cl>
References: <800F3C4ADCFCA643A919EF7C140A045A0DC6BD9B@LLANQUIHUE.corpgroup.cl>
Message-ID: <3d35a2ca0608211516o169cf3amfbb43aa65dc0b96c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060822/68438070/attachment.pl 

From patrick at burns-stat.com  Tue Aug 22 11:14:18 2006
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 22 Aug 2006 10:14:18 +0100
Subject: [R-SIG-Finance] Problem with garch (tseries)
In-Reply-To: <3d35a2ca0608211516o169cf3amfbb43aa65dc0b96c@mail.gmail.com>
References: <800F3C4ADCFCA643A919EF7C140A045A0DC6BD9B@LLANQUIHUE.corpgroup.cl>
	<3d35a2ca0608211516o169cf3amfbb43aa65dc0b96c@mail.gmail.com>
Message-ID: <44EACAEA.30202@burns-stat.com>

The Ljung-Box test is susceptible to outliers in
this context.  I haven't looked at this data at all,
but I suspect that a rank Ljung-Box test here
would give a dramatically different result.  More
details are in the Ljung-Box working paper on
the Burns Statistics website.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Hannu Kahra wrote:

>Ricardo,
>
>have you tested for the ARCH effects?
>
>  
>
>>r <- scan("jpy.txt")
>>    
>>
>Read 1459 items
>  
>
>>a <- r-mean(r)
>>rsq <- a^2
>># Testing for ARCH effects
>>Box.test(rsq, lag=5, type = "Ljung-Box")
>>    
>>
>
>        Box-Ljung test
>
>data:  rsq
>X-squared = 2.8022, df = 5, p-value = 0.7305
>
>  
>
>># Look at the acf and pacf of squared returns
>>acf(rsq)
>>pacf(rsq)
>>    
>>
>
>It is no wonder that you are unable to fit a regular GARCH model.
>
>Regards,
>Hannu
>
>On 8/21/06, Ricardo Zambrano Aguilera <Ricardo.Zambrano at corpbanca.cl> wrote:
>  
>
>>Greetings
>>i have this time series, (currency japanese log returns)
>>the data?s iid and it don?t have any serial correlation
>>but it does not fit to a regular garch ...
>>what could it be?
>>best regards
>>
>>RZA
>><<JPY.txt>>
>>pd:weekens removed
>>
>>
>>
>>_______________________________________________
>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
>>
>>
>>
>>    
>>
>
>	[[alternative HTML version deleted]]
>
>  
>
>------------------------------------------------------------------------
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  
>


From Xiaochen.Sun at brunel.ac.uk  Tue Aug 22 12:17:13 2006
From: Xiaochen.Sun at brunel.ac.uk (Xiaochen Sun)
Date: Tue, 22 Aug 2006 11:17:13 +0100
Subject: [R-SIG-Finance] marginal model with AR-t-GARCH model
Message-ID: <4053699F3E4AD04BA536A5F6BB81A2CFF91F2A@UXEXMBU116.academic.windsor>

Dear list,
 
I am now dealing with the time series data from electricity market, regarding the marginal modelling I wonder how to fit data with AR-t-GARCH model in R or S-plus.
 
Many thanks.
 
With regards
Mc


From Ricardo.Zambrano at corpbanca.cl  Tue Aug 22 16:50:02 2006
From: Ricardo.Zambrano at corpbanca.cl (Ricardo Zambrano Aguilera)
Date: Tue, 22 Aug 2006 10:50:02 -0400
Subject: [R-SIG-Finance] Problem with garch (tseries)
Message-ID: <800F3C4ADCFCA643A919EF7C140A045A0DCF7EC8@LLANQUIHUE.corpgroup.cl>

	Thanks a Lot!!!
	 (i should know with the acf and pack of the squared series!!!)
	what i supose to do with that data to modelling the volatility????
	this is a great list


From swestermair at gmx.net  Tue Aug 22 19:10:07 2006
From: swestermair at gmx.net (Stefan Westermair)
Date: Tue, 22 Aug 2006 19:10:07 +0200
Subject: [R-SIG-Finance] Problems using garchFit() in package fseries
Message-ID: <20060822171007.173740@gmx.net>

Dear list members, 

how can i make the garchFit() function accept variables in the formula.mean or formula.var expressions?
I was trying to run something like the following (shorter but produceses the same error message) example:

> my_Garch <- function(data,p,q) {
+ fit <- garchFit(series=data, formula.var= ~garch(p,q))
+ return(fit)
+ }

##simulate testdata  and run the function

> testdata <- garchSim(n=1000)@.Data
> testfit <- my_Garch(testdata,1,2)

##produced the following error message

Fehler in if (p + q == 0) stop("Misspecified GARCH Model: Both Orders are zero!") : 
        Fehlender Wert, wo TRUE/FALSE n?tig ist
Zus?tzlich: Warning message:
NAs durch Umwandlung erzeugt 

It seems that .garchInitSeries reads the parameter from the formula.var expression and the then checks for positivity. In my case not "1" and "2" but "p" and "q" were read (?).

Did anybody ever try to use variables in the garchFit() function?  I am intending to fit GARCH models for several parameter constellations and then select the best fit according to some information criterion.

Thanks a lot in advance,

Stefan

-- 
Stefan Westermair
Sch?linstra?e 13/1
89073 Ulm
swestermair at gmx.net


From Joe-Byers at utulsa.edu  Wed Aug 23 03:53:01 2006
From: Joe-Byers at utulsa.edu (Joe W. Byers)
Date: Tue, 22 Aug 2006 20:53:01 -0500
Subject: [R-SIG-Finance] marginal model with AR-t-GARCH model
In-Reply-To: <4053699F3E4AD04BA536A5F6BB81A2CFF91F2A@UXEXMBU116.academic.windsor>
References: <4053699F3E4AD04BA536A5F6BB81A2CFF91F2A@UXEXMBU116.academic.windsor>
Message-ID: <44EBB4FD.8080601@utulsa.edu>

Xiaochen Sun wrote:
> Dear list,
>  
> I am now dealing with the time series data from electricity market, regarding the marginal modelling I wonder how to fit data with AR-t-GARCH model in R or S-plus.
>  
> Many thanks.
>  
> With regards
> Mc
> 
MC

I think you will have to define your own LL function and call optim to 
fit the AR t Garch model.  I'm not sure it is in fseries.  I am working 
on a GED distribution Garch myself for commodities.  The documentation 
on rmetrics website for Garch modelling has a example of the LL function 
and the call to optim to fit the data as well as the code to calc SE, 
Tstats, and Pvalues of the parameter estimates.

Are you fitting daily, hourly or other data? and what power market?

Good luck
Joe


From hkahra at gmail.com  Wed Aug 23 11:30:17 2006
From: hkahra at gmail.com (Hannu Kahra)
Date: Wed, 23 Aug 2006 12:30:17 +0300
Subject: [R-SIG-Finance] Problem with garch (tseries)
In-Reply-To: <20060817145222.44724.qmail@web38902.mail.mud.yahoo.com>
References: <20060817145222.44724.qmail@web38902.mail.mud.yahoo.com>
Message-ID: <3d35a2ca0608230230t4262e890qda77816a47680c3c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060823/2bbc269f/attachment.pl 

From brian at braverock.com  Wed Aug 23 15:03:18 2006
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 23 Aug 2006 08:03:18 -0500
Subject: [R-SIG-Finance] mean-(scalar) portfolio optimization
Message-ID: <200608230803.19012.brian@braverock.com>

The R function solve.QP is used by several authors to solve classic 
Markowitz mean-variance optimization using solve.QP and a covariance 
matrix.

Many other classes of portfolio optimization solve for the weighting 
vector w using a scalar measure of risk, such as VaR, Sortino, Omega, 
etc. 

Basically, this class of problems could be expressed as:

let w' be the desired portfolio weights
let R be a set of returns for various instruments

solve for a weighting vector w such that risk is minimized

w' = min(risk(R))

solve for a weighting vector w such that return is maximized over risk 
budget y

w'=max(mean(R)) such that risk(R)<.05

and other similar formulations.

solve.QP does not appear to be appropriate for these kinds of 
optimization.  The functions 'optim' and 'optimize' seem to return scalar 
values, solving only for a single minima or maxima, and not for the 
vector (although I may be misunderstanding them).

Does anyone have any pointers on how you might go about solving these 
kinds of optimization problems in R?  I apologize if this is a simple 
problem that I haven't been able to find a reference for online. I will 
happily post the optimizer code once it's working.

Thank you,

  - Brian


From patrick at burns-stat.com  Wed Aug 23 19:01:11 2006
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed, 23 Aug 2006 18:01:11 +0100
Subject: [R-SIG-Finance] mean-(scalar) portfolio optimization
In-Reply-To: <200608230803.19012.brian@braverock.com>
References: <200608230803.19012.brian@braverock.com>
Message-ID: <44EC89D7.6020109@burns-stat.com>

Brian,

You are misunderstanding 'optim' -- it optimizes
a function over one argument but that argument can be
a vector.

However the utilities that you mention are hard to
optimize.  See 'A Data-driven optimization heuristic
for downside risk minimization' by Gilli et al.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Brian G. Peterson wrote:

>The R function solve.QP is used by several authors to solve classic 
>Markowitz mean-variance optimization using solve.QP and a covariance 
>matrix.
>
>Many other classes of portfolio optimization solve for the weighting 
>vector w using a scalar measure of risk, such as VaR, Sortino, Omega, 
>etc. 
>
>Basically, this class of problems could be expressed as:
>
>let w' be the desired portfolio weights
>let R be a set of returns for various instruments
>
>solve for a weighting vector w such that risk is minimized
>
>w' = min(risk(R))
>
>solve for a weighting vector w such that return is maximized over risk 
>budget y
>
>w'=max(mean(R)) such that risk(R)<.05
>
>and other similar formulations.
>
>solve.QP does not appear to be appropriate for these kinds of 
>optimization.  The functions 'optim' and 'optimize' seem to return scalar 
>values, solving only for a single minima or maxima, and not for the 
>vector (although I may be misunderstanding them).
>
>Does anyone have any pointers on how you might go about solving these 
>kinds of optimization problems in R?  I apologize if this is a simple 
>problem that I haven't been able to find a reference for online. I will 
>happily post the optimizer code once it's working.
>
>Thank you,
>
>  - Brian
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>
>  
>


From brian at braverock.com  Thu Aug 24 00:52:11 2006
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 23 Aug 2006 17:52:11 -0500
Subject: [R-SIG-Finance] mean-(scalar) portfolio optimization
In-Reply-To: <44EC89D7.6020109@burns-stat.com>
References: <200608230803.19012.brian@braverock.com>
	<44EC89D7.6020109@burns-stat.com>
Message-ID: <200608231752.11733.brian@braverock.com>

> Brian G. Peterson wrote:
> >The R function solve.QP is used by several authors to solve classic
> >Markowitz mean-variance optimization using solve.QP and a covariance
> >matrix.
> >
> >Many other classes of portfolio optimization solve for the weighting
> >vector w using a scalar measure of risk, such as VaR, Sortino, Omega,
> >etc.
> >
> >Basically, this class of problems could be expressed as:
> >
> >let w' be the desired portfolio weights
> >let R be a set of returns for various instruments
> >
> >solve for a weighting vector w such that risk is minimized
> >
> >w' = min(risk(R))
> >
> >solve for a weighting vector w such that return is maximized over risk
> >budget y
> >
> >w'=max(mean(R)) such that risk(R)<.05
> >
> >and other similar formulations.
> >
> >solve.QP does not appear to be appropriate for these kinds of
> >optimization.  The functions 'optim' and 'optimize' seem to return
> > scalar values, solving only for a single minima or maxima, and not
> > for the vector (although I may be misunderstanding them).
> >
> >Does anyone have any pointers on how you might go about solving these
> >kinds of optimization problems in R?  I apologize if this is a simple
> >problem that I haven't been able to find a reference for online. I
> > will happily post the optimizer code once it's working.

On Wednesday 23 August 2006 12:01, Patrick Burns wrote:
> You are misunderstanding 'optim' -- it optimizes
> a function over one argument but that argument can be
> a vector.

Patrick, 

Thanks for correcting me on 'optim'.  I'll take a closer look.  I am aware 
of the problem of finding *a* minima, but not necessarily the best 
minima, in a series that could have more than one.

> However the utilities that you mention are hard to
> optimize.  See 'A Data-driven optimization heuristic
> for downside risk minimization' by Gilli et al.

This paper basically advocates a constrained brute-force estimation.

I'd like to avoid that if possible, for reasons of computational 
complexity.  That's clearly not the only approach being advocated in the 
current literature.


This one:
http://citeseer.ist.psu.edu/lemus99portfolio.html 
Portfolio Optimization w/ Quantile-based Risk Measures
Gerardo Jose Lemus Rodriguez, MIT, 1999

does a pretty good comparison of quantile-based, gradient methods, and 
non-gradient methods, and looks like it has some good prototypes that 
could be implemented.  From this paper, it looks like a non-parametric 
gradient estimator should give acceptable results with minimal 
computational effort.

This paper:
http://www.edhec-risk.com/site_edhecrisk/public/features/RISKReview.2005-12-19.1651
Investing in Hedge Funds: Adding Value through Active Style Allocation 
Decisions
Martellini, Vaissi?, and Ziemann 2005

uses w' = min(VaR(95%)) with constraints on weight 
to good effect to establish strategic weighting, but does not provide the 
math for solving directly for the weighting vector, only expected return 
under a a four-moment CAPM model, or a four-moment Taylor expansion that 
could be transformed and solved for w'.

This paper:
http://www.banque-france.fr/gb/publications/ner/1-108.htm
Optimal Portfolio Allocation Under Higher Moments

solves for a differentiable series of nonlinear equations into a four 
moment CAPM model.  This is another relatively intensive approach that 
I'd like to avoid.

I could reference other papers, but I think that these are representative.  
I was hoping to spark some discussion of optimization around scalar 
measures of risk such as VaR, Omega, or Expected Shortfall.  

I'm hoping that others on this list have done something similar and would 
be willing to point me more directly towards implementation in R, as 
estimation and optimization function in R are still pretty foreign to me.

Regards,

  - Brian


From pvirketis at hbk.com  Thu Aug 24 00:52:52 2006
From: pvirketis at hbk.com (Pijus Virketis)
Date: Wed, 23 Aug 2006 18:52:52 -0400
Subject: [R-SIG-Finance] anyone have RDCOMClient?
Message-ID: <4DDAC3B7F3996A4291CBF8B0CDED8A7C023B59D7@NYCMAIL1.hbk.com>

Hi,

The Omegahat site looks to be down (one could add "again" after a glance
at the r-help history); does anyone have their RDCOMClient package, on
which RBloomberg depends? I looked for mirrors, but the one's I've found
are either also down, or mirror only a fraction of Omegahat's packages
...

Thank you,

Pijus


From ggrothendieck at gmail.com  Thu Aug 24 01:05:19 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 23 Aug 2006 19:05:19 -0400
Subject: [R-SIG-Finance] anyone have RDCOMClient?
In-Reply-To: <4DDAC3B7F3996A4291CBF8B0CDED8A7C023B59D7@NYCMAIL1.hbk.com>
References: <4DDAC3B7F3996A4291CBF8B0CDED8A7C023B59D7@NYCMAIL1.hbk.com>
Message-ID: <971536df0608231605o521b32b7s28e30b3d51f16be8@mail.gmail.com>

Try rumaging through:
http://sunsite.univie.ac.at/rcom/download/

On 8/23/06, Pijus Virketis <pvirketis at hbk.com> wrote:
> Hi,
>
> The Omegahat site looks to be down (one could add "again" after a glance
> at the r-help history); does anyone have their RDCOMClient package, on
> which RBloomberg depends? I looked for mirrors, but the one's I've found
> are either also down, or mirror only a fraction of Omegahat's packages
> ...
>
> Thank you,
>
> Pijus
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From patrick at burns-stat.com  Thu Aug 24 12:38:32 2006
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 24 Aug 2006 11:38:32 +0100
Subject: [R-SIG-Finance] mean-(scalar) portfolio optimization
In-Reply-To: <200608231752.11733.brian@braverock.com>
References: <200608230803.19012.brian@braverock.com>
	<44EC89D7.6020109@burns-stat.com>
	<200608231752.11733.brian@braverock.com>
Message-ID: <44ED81A8.8030804@burns-stat.com>

If you are serious about portfolio optimization, then
you need to confront integer constraints such as a
maximum number of assets to trade and a maximum
number of assets in the portfolio.  The "nice" optimization
algorithms are going to fail in this case.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Brian G. Peterson wrote:

>>Brian G. Peterson wrote:
>>    
>>
>>>The R function solve.QP is used by several authors to solve classic
>>>Markowitz mean-variance optimization using solve.QP and a covariance
>>>matrix.
>>>
>>>Many other classes of portfolio optimization solve for the weighting
>>>vector w using a scalar measure of risk, such as VaR, Sortino, Omega,
>>>etc.
>>>
>>>Basically, this class of problems could be expressed as:
>>>
>>>let w' be the desired portfolio weights
>>>let R be a set of returns for various instruments
>>>
>>>solve for a weighting vector w such that risk is minimized
>>>
>>>w' = min(risk(R))
>>>
>>>solve for a weighting vector w such that return is maximized over risk
>>>budget y
>>>
>>>w'=max(mean(R)) such that risk(R)<.05
>>>
>>>and other similar formulations.
>>>
>>>solve.QP does not appear to be appropriate for these kinds of
>>>optimization.  The functions 'optim' and 'optimize' seem to return
>>>scalar values, solving only for a single minima or maxima, and not
>>>for the vector (although I may be misunderstanding them).
>>>
>>>Does anyone have any pointers on how you might go about solving these
>>>kinds of optimization problems in R?  I apologize if this is a simple
>>>problem that I haven't been able to find a reference for online. I
>>>will happily post the optimizer code once it's working.
>>>      
>>>
>
>On Wednesday 23 August 2006 12:01, Patrick Burns wrote:
>  
>
>>You are misunderstanding 'optim' -- it optimizes
>>a function over one argument but that argument can be
>>a vector.
>>    
>>
>
>Patrick, 
>
>Thanks for correcting me on 'optim'.  I'll take a closer look.  I am aware 
>of the problem of finding *a* minima, but not necessarily the best 
>minima, in a series that could have more than one.
>
>  
>
>>However the utilities that you mention are hard to
>>optimize.  See 'A Data-driven optimization heuristic
>>for downside risk minimization' by Gilli et al.
>>    
>>
>
>This paper basically advocates a constrained brute-force estimation.
>
>I'd like to avoid that if possible, for reasons of computational 
>complexity.  That's clearly not the only approach being advocated in the 
>current literature.
>
>
>This one:
>http://citeseer.ist.psu.edu/lemus99portfolio.html 
>Portfolio Optimization w/ Quantile-based Risk Measures
>Gerardo Jose Lemus Rodriguez, MIT, 1999
>
>does a pretty good comparison of quantile-based, gradient methods, and 
>non-gradient methods, and looks like it has some good prototypes that 
>could be implemented.  From this paper, it looks like a non-parametric 
>gradient estimator should give acceptable results with minimal 
>computational effort.
>
>This paper:
>http://www.edhec-risk.com/site_edhecrisk/public/features/RISKReview.2005-12-19.1651
>Investing in Hedge Funds: Adding Value through Active Style Allocation 
>Decisions
>Martellini, Vaissi?, and Ziemann 2005
>
>uses w' = min(VaR(95%)) with constraints on weight 
>to good effect to establish strategic weighting, but does not provide the 
>math for solving directly for the weighting vector, only expected return 
>under a a four-moment CAPM model, or a four-moment Taylor expansion that 
>could be transformed and solved for w'.
>
>This paper:
>http://www.banque-france.fr/gb/publications/ner/1-108.htm
>Optimal Portfolio Allocation Under Higher Moments
>
>solves for a differentiable series of nonlinear equations into a four 
>moment CAPM model.  This is another relatively intensive approach that 
>I'd like to avoid.
>
>I could reference other papers, but I think that these are representative.  
>I was hoping to spark some discussion of optimization around scalar 
>measures of risk such as VaR, Omega, or Expected Shortfall.  
>
>I'm hoping that others on this list have done something similar and would 
>be willing to point me more directly towards implementation in R, as 
>estimation and optimization function in R are still pretty foreign to me.
>
>Regards,
>
>  - Brian
>
>
>  
>


From dave at kanecap.com  Thu Aug 24 15:06:18 2006
From: dave at kanecap.com (David Kane)
Date: Thu, 24 Aug 2006 09:06:18 -0400
Subject: [R-SIG-Finance] mean-(scalar) portfolio optimization
In-Reply-To: <44ED81A8.8030804@burns-stat.com>
References: <200608230803.19012.brian@braverock.com>
	<44EC89D7.6020109@burns-stat.com>
	<200608231752.11733.brian@braverock.com>
	<44ED81A8.8030804@burns-stat.com>
Message-ID: <17645.42058.8640.85745@gargle.gargle.HOWL>

Patrick Burns writes:
 > If you are serious about portfolio optimization, then
 > you need to confront integer constraints such as a
 > maximum number of assets to trade and a maximum
 > number of assets in the portfolio.  

Well, I guess it depends on what you mean by "serious". I like to
think of myself as someone who is very serious about creating optimal
portfolios, but something like integer contraints has never been an
issue, anywhere that I have worked. I have never heard of an actual
applied example with an institutionally-sized portfolio of equities in
which integer constraints made a meaningful difference to the answer.

But, if there is some example of such a case, a case in which you get
a very different answer using more sophisticated approaches, I would
be interested in reading about it.


Dave

-- 
David Kane
Kane Capital Management
646-644-3626


From muckjail at yahoo.com  Thu Aug 24 15:49:39 2006
From: muckjail at yahoo.com (michael mathews)
Date: Thu, 24 Aug 2006 06:49:39 -0700 (PDT)
Subject: [R-SIG-Finance] Problem with garch (tseries)
In-Reply-To: <3d35a2ca0608230230t4262e890qda77816a47680c3c@mail.gmail.com>
Message-ID: <20060824134939.60138.qmail@web38907.mail.mud.yahoo.com>

Hi Folks,
I would like to thank everyone for their input. As seems too often to
be the case the part of the analysis I think will be simple is not.
thanks again
michael 

--- Hannu Kahra <hkahra at gmail.com> wrote:

> Michael,
> 
> it may well be the case that specifying the conditional variance as a
> Smooth
> Transition GARCH (STGARCH) model can provide a good description for
> the
> conditional variance. Have a look at the paper by Lundbergh and
> Ter?svirta:
> http://swopec.hhs.se/hastef/abs/hastef0291.htm
> 
> Regards,
> Hannu
> 
> On 8/17/06, michael mathews <muckjail at yahoo.com> wrote:
> >
> > Hi folks,
> > I have been playing with garch models to model the volatility in
> > physical natural prices.
> > Here is the issue I have a dataset of 801 daily returns (attached).
> > If I run
> >
> > garchall<-garch(hsc)
> >
> > ***** ESTIMATION WITH ANALYTICAL GRADIENT *****
> >
> >
> > > summary(garchall)
> >
> > Call:
> > garch(x = hsc)
> >
> > Model:
> > GARCH(1,1)
> >
> > Residuals:
> >     Min      1Q  Median      3Q     Max
> > -4.3424 -0.5734  0.0000  0.6037  4.0501
> >
> > Coefficient(s):
> >     Estimate  Std. Error  t value Pr(>|t|)
> > a0 2.507e-05   9.200e-06    2.726  0.00642 **
> > a1 1.218e-01   2.085e-02    5.840 5.21e-09 ***
> > b1 8.759e-01   1.937e-02   45.212  < 2e-16 ***
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> > Diagnostic Tests:
> >         Jarque Bera Test
> >
> > data:  Residuals
> > X-squared = 62.7291, df = 2, p-value = 2.387e-14
> >
> >
> >         Box-Ljung test
> >
> > data:  Squared.Residuals
> > X-squared = 0.0384, df = 1, p-value = 0.8447
> >
> > Now if we run the same model on a subset say the last 351 days we
> get
> > > garch351<-garch(tail(hsc,351))
> >
> > ***** ESTIMATION WITH ANALYTICAL GRADIENT *****
> >
> >
> > > summary(garch351)
> >
> > Call:
> > garch(x = tail(hsc, 351))
> >
> > Model:
> > GARCH(1,1)
> >
> > Residuals:
> >       Min        1Q    Median        3Q       Max
> > -4.171521 -0.424628  0.008727  0.532158  3.962116
> >
> > Coefficient(s):
> >     Estimate  Std. Error  t value Pr(>|t|)
> > a0 2.511e-05   1.589e-05    1.580 0.114167
> > a1 1.043e-01   2.950e-02    3.536 0.000406 ***
> > b1 8.957e-01   2.567e-02   34.896  < 2e-16 ***
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> > Diagnostic Tests:
> >         Jarque Bera Test
> >
> > data:  Residuals
> > X-squared = 76.3704, df = 2, p-value < 2.2e-16
> >
> >
> >         Box-Ljung test
> >
> > data:  Squared.Residuals
> > X-squared = 1.2806, df = 1, p-value = 0.2578
> >
> > still ok. Now finally we get t the point of this email lets look at
> 352
> > days of data:
> >
> > garch352<-garch(tail(hsc,352))
> >
> > ***** ESTIMATION WITH ANALYTICAL GRADIENT *****
> >
> >
> > Warning message:
> > NaNs produced in: sqrt(pred$e)
> > > summary(garch352)
> >
> > Call:
> > garch(x = tail(hsc, 352))
> >
> > Model:
> > GARCH(1,1)
> >
> > Residuals:
> >      Min       1Q   Median       3Q      Max
> > -4.16377 -0.58155  0.01454  0.70886 12.41242
> >
> > Coefficient(s):
> >     Estimate  Std. Error  t value Pr(>|t|)
> > a0 2.428e-05   1.556e-05    1.561 0.118632
> > a1 1.043e-01   2.947e-02    3.540 0.000400 ***
> > b1 8.962e-01   2.556e-02   35.058  < 2e-16 ***
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> > Diagnostic Tests:
> >         Jarque Bera Test
> >
> > data:  Residuals
> > X-squared = 10993.57, df = 2, p-value < 2.2e-16
> >
> >
> >         Box-Ljung test
> >
> > data:  Squared.Residuals
> > X-squared = 0.1831, df = 1, p-value = 0.6687
> >
> > whats up? Any Ideas.
> > I have also tried using garchFit from the fSeries package but it
> locks
> > up completely left it running last night and it was still spinning
> this
> > morning when I got back to the office.
> >
> > thanks in advance
> >
> > michael
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >
>


From brian at braverock.com  Thu Aug 24 16:01:04 2006
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 24 Aug 2006 09:01:04 -0500
Subject: [R-SIG-Finance] mean-(scalar) portfolio optimization
In-Reply-To: <44ED81A8.8030804@burns-stat.com>
References: <200608230803.19012.brian@braverock.com>
	<200608231752.11733.brian@braverock.com>
	<44ED81A8.8030804@burns-stat.com>
Message-ID: <200608240901.05219.brian@braverock.com>

On Thursday 24 August 2006 05:38, Patrick Burns wrote:
> If you are serious about portfolio optimization, then
> you need to confront integer constraints such as a
> maximum number of assets to trade and a maximum
> number of assets in the portfolio.  The "nice" optimization
> algorithms are going to fail in this case.

I understand that completely unconstrained optimization approaches are 
dangerous because of local minima and other problems.  All the papers 
that I've referenced use some constraints, at least on the weighting 
vector, and often on number of instruments.   

I'm working in a pretty contrained universe already. I'll never have 
several thousand instruments in a portfolio, for example. The application 
that I'm looking for is for a strategic asset allocation (for example in 
a portfolio of funds, or in a sector or industry portfolio), rebalanced 
relatively infrequently, so some of the constraints that you mention 
would have been implied by my universe.  I use different approaches for 
frequent trading or tactical allocation.

I've consolidated the references below in GNU format, as I think it will 
make it easier for others to find and discuss these useful papers.

So, back to the goals of my original post, how best to solve the 
constrained mean-(scalar) portfolio optimization problem in R...  

In general, they all fall into a class of problems that may be simply 
stated: 

  let w' be the desired portfolio weights
  let R be a set of returns for various instruments

 solve for a weighting vector w such that risk is minimized

  w' = min(risk(R))

 solve for a weighting vector w such that return is maximized over
 risk budget rb

 w'=max(mean(R)) such that risk(R) < rb
 
and other similar formulations. The details on all these a[proaches are of 
course more complicated, and what we're trying to get... 

It does not look like simple quadratic methods are appropriate for a 
solution using a scalar measure of risk.  So, solve.QP is out.  If anyone 
disagrees with this, I'd love to talk about it.

'optim' may be appropriate, if we have a good way of estimating the 
gradient, and if it can allow for reasonable constraints.  Rodriguez(5) 
discusses methods of estimating the gradient, and 'constrOptim' may be 
able to constrain the space appropriately, I don't have any opinion on 
whether this is a workable approach, community input solicited.

Gilli(1) recommends a constrained brute force approach to estimating.  
Since I am not completely familiar with this paper, having not really 
evaluated brute force methods, I'm not sure what the best approach in R 
would be. I think that one could use 'nlm' or 'constrOptim' to model an 
approach similar to the one Gilli proposes, but input from Patrick or 
anyone else who has tried to implement Gilli would be appreciated.  

Martellini(4) and Rockinger(2) solve a four-moment CAPM using a 
differentiable series of linear equations.  So, I believe that a 
'simplex' or 'glm' or 'gls' approach could be used to go down that path.

Ma(3) approaches the problem from a Quantile Regression angle.  I think 
that this approach is quite promising, and could be solved for in R using 
'quantreg'.

I believe that all of these approaches have theoretical merit.  Like any 
approach, they all have drawbacks and limitations.  

I'd like to avoid spending a bunch of trial and error time implementing 
each of these methods in turn.  Some pointers specifically on 
implementation approaches in R would be vastly appreciated. Thoughts from 
anyone else who has implemented portfolio optimization in R would be very 
valuable. I'll continue to share what I come up with.

Regards,

   - Brian
 
Ref:
(1)
Gilli, M., et. al., "A Data-Driven Optimization Heuristic for Downside 
Risk Minimization" . Swiss Finance Institute Research Paper No. 06-2 
Available at SSRN: http://ssrn.com/abstract=910233

(2)
Jondeau, E. and Rockinger, M.," Optimal Portfolio Allocation Under Higher 
Moments" Bank of France, 2004 Available at:
http://www.banque-france.fr/gb/publications/ner/1-108.htm

(3)
Ma, L. and Pohlman, L. "Return Forecasts and Optimal Portfolio 
Construction: A Quantile Regression Approach" Available at:
http://www.fma.org/Chicago/Papers/equityQR2.pdf

(4)
Martellini, Vaissi?, and Ziemann "Investing in Hedge Funds: Adding Value 
through Active Style Allocation Decisions" EDHEC 2005. Available at:
http://www.edhec-risk.com/site_edhecrisk/public/features/RISKReview.2005-12-19.1651 

(5)
Rodriguez, G. ,"Portfolio Optimization w/ Quantile-based Risk Measures", 
MIT, 1999. Available at http://citeseer.ist.psu.edu/lemus99portfolio.html

Earlier thread:
> Brian G. Peterson wrote:
> >>Brian G. Peterson wrote:
> >>>The R function solve.QP is used by several authors to solve classic
> >>>Markowitz mean-variance optimization using solve.QP and a covariance
> >>>matrix.
> >>>
> >>>Many other classes of portfolio optimization solve for the weighting
> >>>vector w using a scalar measure of risk, such as VaR, Sortino,
> >>> Omega, etc.
> >>>
> >>>Basically, this class of problems could be expressed as:
> >>>
> >>>let w' be the desired portfolio weights
> >>>let R be a set of returns for various instruments
> >>>
> >>>solve for a weighting vector w such that risk is minimized
> >>>
> >>>w' = min(risk(R))
> >>>
> >>>solve for a weighting vector w such that return is maximized over
> >>> risk budget y
> >>>
> >>>w'=max(mean(R)) such that risk(R)<.05
> >>>
> >>>and other similar formulations.
> >>>
> >>>solve.QP does not appear to be appropriate for these kinds of
> >>>optimization.  The functions 'optim' and 'optimize' seem to return
> >>>scalar values, solving only for a single minima or maxima, and not
> >>>for the vector (although I may be misunderstanding them).
> >>>
> >>>Does anyone have any pointers on how you might go about solving
> >>> these kinds of optimization problems in R?  I apologize if this is
> >>> a simple problem that I haven't been able to find a reference for
> >>> online. I will happily post the optimizer code once it's working.
> >
> >On Wednesday 23 August 2006 12:01, Patrick Burns wrote:
> >>You are misunderstanding 'optim' -- it optimizes
> >>a function over one argument but that argument can be
> >>a vector.
> >
> >Patrick,
> >
> >Thanks for correcting me on 'optim'.  I'll take a closer look.  I am
> > aware of the problem of finding *a* minima, but not necessarily the
> > best minima, in a series that could have more than one.
> >
> >>However the utilities that you mention are hard to
> >>optimize.  See 'A Data-driven optimization heuristic
> >>for downside risk minimization' by Gilli et al.
> >
> >This paper basically advocates a constrained brute-force estimation.
> >
> >I'd like to avoid that if possible, for reasons of computational
> >complexity.  That's clearly not the only approach being advocated in
> > the current literature.
> >
> >
> >This one:
> >http://citeseer.ist.psu.edu/lemus99portfolio.html
> >Portfolio Optimization w/ Quantile-based Risk Measures
> >Gerardo Jose Lemus Rodriguez, MIT, 1999
> >
> >does a pretty good comparison of quantile-based, gradient methods, and
> >non-gradient methods, and looks like it has some good prototypes that
> >could be implemented.  From this paper, it looks like a non-parametric
> >gradient estimator should give acceptable results with minimal
> >computational effort.
> >
> >This paper:
> >http://www.edhec-risk.com/site_edhecrisk/public/features/RISKReview.20
> >05-12-19.1651 Investing in Hedge Funds: Adding Value through Active
> > Style Allocation Decisions
> >Martellini, Vaissi?, and Ziemann 2005
> >
> >uses w' = min(VaR(95%)) with constraints on weight
> >to good effect to establish strategic weighting, but does not provide
> > the math for solving directly for the weighting vector, only expected
> > return under a a four-moment CAPM model, or a four-moment Taylor
> > expansion that could be transformed and solved for w'.
> >
> >This paper:
> >http://www.banque-france.fr/gb/publications/ner/1-108.htm
> >Optimal Portfolio Allocation Under Higher Moments
> >
> >solves for a differentiable series of nonlinear equations into a four
> >moment CAPM model.  This is another relatively intensive approach that
> >I'd like to avoid.
> >
> >I could reference other papers, but I think that these are
> > representative. I was hoping to spark some discussion of optimization
> > around scalar measures of risk such as VaR, Omega, or Expected
> > Shortfall.
> >
> >I'm hoping that others on this list have done something similar and
> > would be willing to point me more directly towards implementation in
> > R, as estimation and optimization function in R are still pretty
> > foreign to me.
> >
> >Regards,
> >
> >  - Brian


From hkahra at gmail.com  Thu Aug 24 20:45:08 2006
From: hkahra at gmail.com (Hannu Kahra)
Date: Thu, 24 Aug 2006 21:45:08 +0300
Subject: [R-SIG-Finance] mean-(scalar) portfolio optimization
In-Reply-To: <200608240901.05219.brian@braverock.com>
References: <200608230803.19012.brian@braverock.com>
	<200608231752.11733.brian@braverock.com>
	<44ED81A8.8030804@burns-stat.com>
	<200608240901.05219.brian@braverock.com>
Message-ID: <3d35a2ca0608241145kf85a88akb2872da40e2f9033@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060824/e91688b1/attachment.pl 

From wuertz at itp.phys.ethz.ch  Thu Aug 24 23:20:36 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 24 Aug 2006 23:20:36 +0200
Subject: [R-SIG-Finance] Problems using garchFit() in package fseries
In-Reply-To: <20060822171007.173740@gmx.net>
References: <20060822171007.173740@gmx.net>
Message-ID: <44EE1824.1070700@itp.phys.ethz.ch>

Stefan Westermair wrote:

>Dear list members, 
>
>how can i make the garchFit() function accept variables in the formula.mean or formula.var expressions?
>I was trying to run something like the following (shorter but produceses the same error message) example:
>
>  
>
>>my_Garch <- function(data,p,q) {
>>    
>>
>+ fit <- garchFit(series=data, formula.var= ~garch(p,q))
>+ return(fit)
>+ }
>  
>

Use:

require(fSeries)
  
   
    myGarch = function(data, p, q)
    {
        form = as.formula(paste("~garch(", p, ",", q, ")"))
        garchFit(formula.var = form, series = data)
    }
   
    myGarch(data = garchSim(n = 1000, rseed = 4711), p = 1, q = 1)


DW

>##simulate testdata  and run the function
>
>  
>
>>testdata <- garchSim(n=1000)@.Data
>>testfit <- my_Garch(testdata,1,2)
>>    
>>
>
>##produced the following error message
>
>Fehler in if (p + q == 0) stop("Misspecified GARCH Model: Both Orders are zero!") : 
>        Fehlender Wert, wo TRUE/FALSE n?tig ist
>Zus?tzlich: Warning message:
>NAs durch Umwandlung erzeugt 
>
>It seems that .garchInitSeries reads the parameter from the formula.var expression and the then checks for positivity. In my case not "1" and "2" but "p" and "q" were read (?).
>
>Did anybody ever try to use variables in the garchFit() function?  I am intending to fit GARCH models for several parameter constellations and then select the best fit according to some information criterion.
>
>Thanks a lot in advance,
>
>Stefan
>
>  
>


From wuertz at itp.phys.ethz.ch  Thu Aug 24 23:28:43 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 24 Aug 2006 23:28:43 +0200
Subject: [R-SIG-Finance] marginal model with AR-t-GARCH model
In-Reply-To: <44EBB4FD.8080601@utulsa.edu>
References: <4053699F3E4AD04BA536A5F6BB81A2CFF91F2A@UXEXMBU116.academic.windsor>
	<44EBB4FD.8080601@utulsa.edu>
Message-ID: <44EE1A0B.3040603@itp.phys.ethz.ch>

Joe W. Byers wrote:

Use:

require(fSeries)

garchFit(formula.mean = ~arma(1, 0), formula.var = ~garch(1, 1), series 
= x, cond.dist ="dstd")

for an AR(1)-t-GARCH(1,1) model

Important: You have to use the standardized t-distribution dstd(), the 
usual dt() gives wrong results!

DW

>Xiaochen Sun wrote:
>  
>
>>Dear list,
>> 
>>I am now dealing with the time series data from electricity market, regarding the marginal modelling I wonder how to fit data with AR-t-GARCH model in R or S-plus.
>> 
>>Many thanks.
>> 
>>With regards
>>Mc
>>
>>    
>>
>MC
>
>I think you will have to define your own LL function and call optim to 
>fit the AR t Garch model.  I'm not sure it is in fseries.  I am working 
>on a GED distribution Garch myself for commodities.  The documentation 
>on rmetrics website for Garch modelling has a example of the LL function 
>and the call to optim to fit the data as well as the code to calc SE, 
>Tstats, and Pvalues of the parameter estimates.
>
>Are you fitting daily, hourly or other data? and what power market?
>
>Good luck
>Joe
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From wuertz at itp.phys.ethz.ch  Thu Aug 24 23:53:44 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 24 Aug 2006 23:53:44 +0200
Subject: [R-SIG-Finance] Problem with garch (tseries)
In-Reply-To: <800F3C4ADCFCA643A919EF7C140A045A0DC6BD9B@LLANQUIHUE.corpgroup.cl>
References: <800F3C4ADCFCA643A919EF7C140A045A0DC6BD9B@LLANQUIHUE.corpgroup.cl>
Message-ID: <44EE1FE8.3060701@itp.phys.ethz.ch>

Try:

    > require(fSeries)

    > x = scan("jpy.txt")
    > sd(x)
    [1] 0.005828643
   
    # The time series is badly scaled, use function scaling or just
    # multiply with 100!
   
    > x = 100 * x
    > sd(x)
    [1] 0.5828643


    > garchFit(series = x)
   
   
    Coefficient(s):
          mu         omega        alpha1         beta1 
    -8.88965e-05   1.06363e-02   1.94304e-02   9.48852e-01 
   
    Error Analysis:
             Estimate  Std. Error  t value Pr(>|t|)   
    mu     -0.0000889   0.0077757   -0.011   0.9909   
    omega   0.0106363   0.0052342    2.032   0.0421 * 
    alpha1  0.0194304   0.0079660    2.439   0.0147 * 
    beta1   0.9488518   0.0199498   47.562   <2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
   
    Log Likelihood:
     1275.933    normalized:  0.8745259


Best regards Diethelm Wuertz


Ricardo Zambrano Aguilera wrote:

>Greetings
>i have this time series, (currency japanese log returns)
>the data?s iid and it don?t have any serial correlation
>but it does not fit to a regular garch ...
>what could it be?
>best regards
>
>RZA
> <<JPY.txt>> 
>pd:weekens removed
>
>  
>
>------------------------------------------------------------------------
>
>
>8.74E-05
>-6.93E-03
>1.73E-02
>8.45E-03
>-3.27E-03
>6.27E-03
>-3.86E-03
>1.18E-02
>6.18E-03
>3.79E-03
>-1.07E-02
>9.81E-03
>-6.24E-03
>-8.16E-03
>-6.59E-03
>2.75E-03
>8.53E-03
>-8.02E-03
>3.33E-03
>-3.59E-03
>-7.74E-03
>6.11E-03
>-8.44E-03
>1.38E-03
>-8.50E-03
>8.71E-05
>1.25E-02
>4.81E-03
>6.15E-03
>-7.66E-04
>-5.04E-03
>-4.38E-03
>-6.99E-03
>1.90E-03
>2.68E-03
>-3.11E-03
>7.66E-03
>4.37E-03
>-1.09E-02
>5.77E-03
>-1.98E-03
>9.67E-03
>2.56E-04
>1.36E-02
>1.43E-03
>-3.70E-03
>9.56E-03
>-1.17E-03
>-1.76E-03
>8.66E-03
>-7.33E-03
>1.11E-02
>1.08E-02
>5.62E-03
>-1.71E-03
>-5.14E-03
>1.06E-02
>2.43E-04
>-6.17E-03
>-1.55E-03
>-2.61E-03
>1.06E-03
>1.05E-02
>2.13E-02
>3.71E-03
>-1.01E-02
>-1.59E-03
>-1.05E-02
>-2.02E-03
>1.10E-02
>-5.45E-03
>2.73E-03
>-9.74E-03
>4.92E-03
>1.21E-03
>-7.91E-03
>-1.07E-02
>-9.14E-03
>1.31E-02
>-1.06E-02
>8.21E-03
>-1.15E-03
>8.64E-03
>6.64E-03
>-4.12E-03
>-1.22E-02
>-4.52E-03
>-5.77E-04
>-8.24E-05
>-1.81E-03
>1.32E-03
>8.05E-03
>2.53E-03
>-5.71E-04
>6.83E-03
>-2.43E-04
>9.73E-04
>-7.07E-03
>7.48E-03
>-6.83E-03
>1.95E-03
>-2.25E-02
>-2.75E-03
>6.90E-03
>3.48E-03
>-6.71E-03
>7.48E-04
>-9.43E-03
>1.68E-04
>3.35E-04
>8.02E-03
>-7.49E-04
>0.00E+00
>6.55E-03
>7.99E-03
>-1.81E-03
>3.53E-03
>-6.41E-03
>1.30E-02
>2.76E-03
>-3.41E-03
>7.95E-03
>6.20E-03
>-8.03E-04
>-5.56E-03
>-1.62E-04
>4.43E-03
>3.53E-03
>-1.04E-03
>-4.26E-03
>2.25E-03
>4.02E-04
>9.83E-03
>1.51E-03
>-4.30E-03
>7.18E-04
>-8.89E-03
>-3.38E-03
>8.44E-03
>4.15E-03
>-3.99E-03
>-8.60E-03
>-4.53E-03
>-2.84E-03
>1.15E-02
>-2.50E-03
>-4.52E-03
>1.86E-03
>-1.86E-03
>1.29E-02
>-1.12E-03
>-2.96E-03
>-7.49E-03
>-3.23E-04
>1.94E-03
>-2.91E-03
>5.66E-04
>-1.38E-02
>8.20E-04
>3.93E-03
>-6.06E-03
>-1.80E-02
>6.67E-03
>-1.16E-03
>4.15E-03
>-9.07E-03
>6.17E-03
>-4.33E-03
>1.33E-03
>-1.00E-03
>1.25E-03
>5.83E-04
>-6.10E-03
>-5.12E-03
>-4.21E-04
>4.96E-03
>1.12E-02
>1.08E-03
>-6.64E-03
>8.63E-03
>-1.35E-02
>7.53E-04
>-5.03E-03
>-1.32E-02
>4.00E-03
>-4.76E-03
>2.64E-03
>-1.02E-02
>1.72E-03
>8.20E-03
>7.65E-04
>7.65E-04
>1.70E-02
>-1.50E-03
>5.51E-03
>4.81E-03
>-1.91E-03
>-9.96E-04
>4.98E-04
>-5.24E-03
>1.92E-03
>2.16E-03
>8.69E-03
>-2.47E-03
>-2.23E-03
>3.30E-03
>4.95E-04
>-1.73E-03
>8.25E-04
>1.02E-02
>1.06E-03
>1.55E-03
>8.96E-04
>-1.38E-03
>-5.97E-03
>9.01E-04
>2.78E-03
>-4.26E-03
>-1.40E-03
>-5.75E-04
>-3.46E-03
>-1.73E-03
>-7.63E-03
>1.75E-03
>1.74E-03
>8.59E-03
>-8.23E-05
>6.89E-03
>4.32E-03
>1.63E-03
>-4.48E-03
>5.21E-03
>5.58E-03
>2.50E-03
>-1.29E-03
>-8.06E-04
>-7.29E-03
>6.48E-03
>-3.07E-03
>5.41E-03
>2.42E-04
>-3.22E-04
>3.70E-03
>7.44E-03
>3.82E-03
>0.00E+00
>0.00E+00
>4.76E-04
>9.63E-03
>1.96E-03
>2.43E-03
>2.66E-03
>3.66E-03
>7.13E-03
>2.85E-03
>7.13E-03
>7.64E-05
>6.85E-03
>-5.17E-03
>4.19E-03
>-7.60E-05
>3.64E-03
>-2.50E-03
>-5.78E-03
>3.81E-04
>1.34E-02
>2.18E-03
>-5.72E-03
>-1.36E-03
>-1.21E-03
>-6.92E-03
>7.07E-03
>3.55E-03
>2.26E-04
>1.13E-03
>8.63E-03
>5.22E-03
>8.91E-04
>-1.71E-03
>-7.61E-03
>1.50E-04
>-4.28E-03
>1.34E-02
>-1.20E-02
>-5.43E-03
>1.19E-02
>-1.79E-03
>4.49E-04
>7.15E-03
>-9.92E-03
>-5.56E-03
>4.74E-03
>-8.67E-03
>3.70E-03
>7.54E-04
>6.46E-03
>1.20E-03
>3.51E-03
>-3.73E-04
>-2.16E-03
>6.11E-03
>-2.97E-03
>-6.87E-03
>7.50E-04
>-9.33E-03
>1.51E-04
>-1.19E-02
>-2.41E-02
>8.51E-03
>-2.18E-03
>5.21E-03
>3.33E-03
>-2.40E-03
>-1.24E-03
>1.79E-02
>5.69E-03
>-5.16E-03
>5.09E-03
>6.11E-03
>3.61E-03
>-2.40E-03
>-3.01E-03
>9.04E-04
>-2.26E-04
>5.63E-03
>-2.02E-03
>-3.53E-03
>-3.09E-03
>-5.68E-03
>7.60E-04
>-6.09E-03
>-1.30E-03
>5.95E-03
>3.57E-03
>1.59E-03
>-7.37E-03
>-3.82E-03
>-5.22E-03
>2.61E-03
>-2.69E-03
>1.61E-03
>-4.70E-03
>-7.36E-03
>-6.24E-03
>7.82E-04
>4.52E-03
>-8.83E-03
>3.92E-03
>-7.06E-03
>1.42E-03
>6.89E-03
>6.15E-03
>-4.12E-03
>-5.86E-03
>1.64E-03
>5.46E-03
>-6.40E-03
>3.60E-03
>-1.69E-02
>-5.25E-03
>-9.06E-03
>-8.05E-05
>7.46E-03
>-3.12E-03
>7.21E-04
>-1.92E-03
>-1.77E-03
>-8.07E-03
>7.03E-03
>-5.17E-03
>4.52E-03
>1.77E-03
>-1.85E-03
>1.61E-03
>3.29E-03
>4.64E-03
>3.03E-03
>-6.38E-03
>-6.18E-03
>1.21E-03
>4.83E-04
>-4.03E-03
>-2.51E-03
>-1.98E-02
>4.94E-03
>-3.13E-03
>-1.06E-02
>-3.42E-03
>-1.42E-03
>2.17E-03
>1.00E-03
>-5.84E-04
>2.50E-03
>2.91E-03
>-1.67E-02
>-2.79E-03
>-3.73E-03
>-4.69E-03
>-1.97E-03
>-4.54E-03
>-4.65E-03
>4.39E-03
>1.55E-03
>-5.34E-03
>2.84E-03
>1.14E-02
>-7.77E-03
>-1.37E-03
>1.98E-02
>6.96E-03
>5.00E-03
>-3.17E-03
>-5.35E-03
>-3.02E-03
>7.04E-03
>8.98E-03
>-4.98E-03
>6.80E-03
>-6.88E-03
>-9.95E-03
>-5.06E-03
>-8.31E-03
>0.00E+00
>2.21E-03
>8.72E-03
>1.01E-03
>-2.44E-03
>1.15E-02
>-1.92E-03
>7.52E-04
>-1.28E-02
>5.57E-03
>-5.06E-03
>1.52E-03
>-2.79E-03
>-1.00E-02
>9.11E-03
>2.45E-03
>1.94E-03
>3.12E-03
>7.79E-03
>3.91E-03
>-1.41E-03
>1.29E-02
>4.51E-03
>-1.72E-03
>-4.43E-03
>-1.48E-03
>1.70E-02
>4.04E-03
>-4.77E-03
>-3.57E-03
>-6.61E-03
>3.76E-03
>-5.81E-03
>6.71E-03
>2.04E-03
>-2.20E-03
>5.53E-03
>8.56E-03
>-4.83E-04
>-8.16E-03
>2.84E-03
>3.39E-03
>2.17E-03
>2.89E-03
>-1.36E-03
>4.89E-03
>3.11E-03
>-4.79E-03
>1.92E-03
>-4.40E-03
>-1.28E-03
>-8.84E-04
>-6.46E-03
>-3.73E-03
>-1.14E-03
>-3.67E-03
>-2.53E-03
>1.64E-04
>-2.62E-03
>-1.23E-03
>-4.69E-03
>-1.09E-02
>-1.42E-03
>-5.85E-04
>4.01E-03
>2.91E-03
>4.98E-04
>4.80E-03
>1.00E-02
>3.59E-03
>-8.97E-04
>1.55E-03
>-5.47E-03
>-4.43E-03
>5.90E-03
>-2.45E-04
>2.04E-03
>1.55E-02
>-8.04E-04
>1.69E-03
>2.89E-03
>-1.14E-02
>-1.30E-03
>3.40E-03
>-1.05E-03
>-7.63E-03
>-1.81E-02
>7.28E-03
>-7.42E-04
>-2.23E-03
>-3.48E-03
>-1.83E-03
>6.64E-04
>-1.41E-03
>0.00E+00
>-2.75E-03
>-6.67E-04
>-1.14E-02
>2.28E-03
>0.00E+00
>1.08E-02
>-3.75E-03
>-3.52E-03
>1.06E-02
>-1.12E-02
>1.93E-03
>-4.19E-04
>-2.77E-03
>-7.08E-03
>-4.23E-04
>-2.46E-03
>1.70E-04
>1.70E-03
>1.10E-03
>1.02E-03
>-1.35E-03
>-2.54E-03
>6.26E-03
>1.77E-03
>-3.46E-03
>5.06E-03
>7.62E-03
>3.50E-03
>-5.25E-03
>3.34E-03
>-2.17E-03
>3.92E-03
>8.61E-03
>-1.90E-03
>3.05E-03
>-7.36E-03
>-7.47E-04
>-1.16E-03
>-1.03E-02
>-3.11E-03
>-3.29E-03
>3.04E-03
>-7.27E-03
>-4.17E-03
>-1.54E-03
>4.94E-03
>2.97E-03
>-3.99E-03
>1.70E-04
>-3.15E-03
>1.02E-03
>-2.47E-03
>-2.31E-03
>2.56E-03
>2.05E-03
>9.92E-03
>-1.94E-03
>1.60E-03
>3.03E-03
>1.30E-02
>-9.14E-04
>1.13E-02
>-7.84E-03
>-5.56E-03
>-1.08E-03
>2.50E-04
>-6.67E-04
>-1.51E-02
>-1.44E-03
>8.78E-03
>3.44E-03
>5.26E-03
>-2.25E-03
>1.25E-03
>1.75E-03
>-3.50E-03
>7.33E-03
>-1.83E-03
>-8.31E-05
>-6.00E-03
>4.18E-04
>1.00E-03
>6.16E-03
>-4.66E-03
>3.41E-03
>-2.91E-03
>1.67E-03
>2.08E-03
>-5.83E-03
>-7.38E-03
>-2.36E-03
>3.96E-03
>-4.46E-03
>-8.22E-03
>-9.66E-03
>5.65E-03
>2.56E-03
>-3.33E-03
>-1.97E-03
>-4.38E-03
>2.32E-03
>-3.95E-03
>9.18E-03
>-4.28E-03
>8.20E-03
>-3.41E-03
>-2.56E-03
>-2.57E-04
>3.33E-03
>1.11E-02
>-4.73E-03
>1.08E-02
>-5.97E-03
>5.30E-03
>-2.60E-03
>-1.07E-02
>8.55E-03
>-4.05E-03
>-3.31E-03
>0.00E+00
>-8.49E-04
>-2.30E-03
>1.87E-03
>4.24E-03
>-2.63E-03
>4.66E-03
>-2.53E-04
>-5.00E-03
>8.49E-04
>1.53E-03
>1.12E-02
>2.01E-03
>1.59E-03
>-3.09E-03
>-1.09E-02
>4.23E-04
>-3.39E-04
>3.39E-04
>-3.39E-04
>-2.97E-03
>-9.34E-04
>2.04E-03
>-1.53E-03
>1.78E-03
>1.27E-03
>5.99E-03
>-2.44E-03
>-5.06E-04
>5.81E-03
>-3.11E-03
>1.01E-03
>-1.09E-03
>5.54E-03
>4.18E-03
>2.83E-03
>2.33E-03
>-3.57E-03
>1.66E-03
>-3.91E-03
>3.58E-03
>-1.10E-02
>1.01E-03
>-4.04E-03
>1.18E-03
>3.45E-03
>0.00E+00
>4.20E-04
>2.01E-03
>-9.00E-03
>-2.28E-03
>-2.46E-03
>-2.29E-03
>-1.87E-03
>2.56E-04
>5.11E-04
>-7.67E-04
>-3.42E-03
>-3.08E-03
>8.58E-05
>-5.76E-03
>7.05E-03
>9.42E-04
>8.56E-05
>8.56E-05
>2.05E-03
>1.71E-04
>1.54E-03
>1.62E-03
>-1.10E-02
>-1.38E-03
>-6.83E-03
>-1.09E-02
>-1.57E-02
>7.13E-04
>-5.00E-03
>4.47E-03
>-4.47E-03
>-8.54E-03
>6.39E-03
>-7.38E-03
>3.61E-04
>1.99E-03
>5.41E-04
>-9.60E-03
>-3.01E-03
>-4.76E-03
>-3.86E-03
>2.48E-03
>-5.51E-04
>6.14E-03
>3.83E-03
>-5.29E-03
>9.10E-03
>-8.10E-03
>-4.58E-03
>4.76E-03
>-2.38E-03
>-6.61E-03
>-2.31E-03
>3.69E-04
>3.87E-03
>1.10E-02
>9.24E-03
>-1.27E-02
>3.01E-03
>3.18E-03
>-7.74E-03
>-5.13E-03
>-1.10E-03
>1.38E-03
>-6.91E-03
>2.22E-03
>5.25E-03
>-8.95E-03
>1.26E-02
>-2.66E-03
>-2.39E-03
>5.32E-03
>4.57E-04
>-2.93E-03
>9.17E-05
>4.94E-03
>-2.01E-03
>-7.71E-03
>-2.58E-03
>-7.39E-04
>-4.91E-03
>-2.60E-03
>-1.30E-03
>1.10E-02
>-3.88E-03
>-2.32E-03
>-2.14E-03
>-4.65E-04
>-9.31E-04
>2.14E-03
>1.76E-03
>-2.32E-03
>-1.30E-03
>-6.52E-04
>-9.32E-04
>1.86E-03
>-4.01E-03
>-5.61E-04
>2.33E-03
>2.14E-03
>-3.64E-03
>-8.07E-03
>-1.88E-04
>-9.42E-05
>1.88E-04
>5.63E-03
>-1.41E-03
>-4.04E-03
>-3.77E-04
>1.69E-03
>2.63E-03
>5.80E-03
>-1.77E-03
>-1.50E-03
>-8.08E-03
>6.67E-03
>-3.38E-03
>-7.93E-03
>6.42E-03
>-2.64E-03
>-2.27E-03
>-9.46E-04
>-1.14E-03
>-5.69E-04
>3.79E-03
>-3.22E-03
>9.48E-04
>2.84E-04
>-1.42E-03
>-9.49E-04
>8.54E-04
>9.48E-05
>1.89E-03
>1.02E-02
>2.43E-03
>1.88E-02
>-6.07E-03
>-1.85E-03
>7.46E-03
>5.31E-03
>-4.21E-03
>-1.10E-03
>1.01E-02
>-5.45E-04
>9.77E-03
>8.25E-03
>-7.62E-03
>1.17E-03
>-3.69E-03
>-1.17E-03
>9.03E-05
>-4.34E-03
>-1.41E-02
>-4.52E-03
>-1.36E-02
>-1.41E-03
>1.97E-03
>-1.12E-03
>-5.07E-03
>-3.77E-04
>-1.60E-03
>-4.44E-03
>2.08E-03
>-1.47E-02
>-5.19E-03
>7.88E-03
>4.11E-03
>7.69E-03
>-4.93E-03
>1.01E-02
>1.41E-03
>-9.91E-03
>1.12E-02
>1.92E-02
>-3.50E-03
>-4.16E-03
>5.64E-03
>3.22E-03
>5.59E-03
>9.14E-05
>-3.94E-03
>-3.40E-03
>7.52E-03
>6.20E-03
>-1.91E-03
>5.72E-03
>-1.72E-03
>-8.83E-03
>-6.15E-03
>9.80E-03
>2.21E-02
>1.30E-02
>-4.67E-03
>-1.33E-03
>1.33E-02
>-2.36E-03
>8.75E-04
>-4.37E-04
>-1.03E-02
>-3.19E-03
>-3.55E-03
>5.06E-03
>-1.08E-02
>1.88E-03
>-9.61E-03
>-5.88E-03
>-6.37E-03
>1.02E-02
>-6.62E-03
>8.34E-03
>3.33E-03
>-1.48E-02
>8.21E-04
>7.09E-03
>-9.92E-03
>6.56E-03
>8.86E-03
>-1.39E-02
>5.19E-03
>-3.91E-03
>-7.23E-03
>6.43E-04
>3.21E-03
>-7.16E-03
>-1.26E-02
>4.01E-03
>3.43E-03
>2.68E-03
>4.98E-03
>-3.41E-03
>-7.38E-04
>6.99E-03
>3.84E-03
>-7.98E-03
>3.49E-03
>-7.55E-03
>6.47E-04
>3.78E-03
>4.59E-03
>5.66E-03
>-9.89E-03
>-4.06E-03
>3.78E-03
>1.03E-02
>-9.11E-05
>3.64E-03
>-2.09E-03
>8.78E-03
>6.47E-03
>3.04E-03
>-5.73E-03
>-5.67E-03
>-1.45E-03
>5.14E-03
>6.19E-03
>-1.21E-02
>1.27E-03
>6.31E-03
>-3.60E-03
>-1.80E-04
>-1.81E-03
>-1.90E-03
>-3.90E-03
>-6.65E-03
>1.83E-04
>-2.11E-03
>6.39E-03
>-1.73E-03
>5.00E-03
>-4.46E-03
>-9.12E-05
>1.82E-03
>-6.48E-03
>3.20E-03
>-2.74E-04
>9.00E-03
>-2.99E-03
>-7.29E-03
>0.00E+00
>3.01E-03
>-9.12E-05
>3.28E-03
>-3.55E-03
>3.55E-03
>-4.01E-03
>2.10E-03
>7.28E-04
>-2.28E-03
>8.63E-03
>1.72E-03
>-1.45E-03
>5.95E-03
>8.08E-04
>-4.41E-03
>-7.60E-03
>4.44E-03
>3.97E-03
>1.53E-03
>1.80E-03
>-9.88E-04
>-1.55E-02
>-1.55E-03
>3.01E-03
>5.47E-04
>-7.29E-04
>-3.11E-03
>-6.40E-04
>-8.09E-03
>-1.29E-03
>-7.05E-03
>-2.05E-03
>-4.77E-03
>-1.87E-04
>-2.72E-03
>-1.41E-03
>-4.53E-03
>6.12E-03
>-3.95E-03
>1.79E-03
>-1.79E-03
>-4.35E-03
>-9.48E-04
>1.71E-03
>1.38E-02
>-4.77E-03
>-9.90E-03
>-2.85E-03
>7.60E-04
>-1.25E-02
>1.34E-03
>-1.04E-02
>4.85E-04
>1.65E-03
>-5.05E-03
>-3.31E-03
>1.27E-03
>2.63E-03
>2.04E-03
>-4.77E-03
>6.32E-03
>-1.12E-02
>1.07E-02
>-2.04E-03
>9.76E-03
>6.61E-03
>5.24E-03
>-3.71E-03
>6.66E-03
>-1.23E-02
>5.26E-03
>-5.74E-03
>-7.68E-04
>2.40E-03
>-1.15E-03
>-6.35E-03
>9.65E-04
>-5.90E-03
>-9.70E-05
>7.73E-03
>-7.64E-03
>-4.28E-03
>1.46E-03
>1.73E-02
>-4.02E-03
>8.22E-03
>-2.38E-03
>-3.92E-03
>-9.92E-03
>-8.55E-03
>-5.86E-04
>-3.23E-03
>7.84E-04
>1.37E-03
>4.88E-03
>6.40E-03
>-7.18E-03
>-6.82E-04
>1.40E-02
>-9.66E-03
>-3.50E-03
>5.53E-03
>4.06E-03
>-1.93E-04
>-8.68E-04
>8.46E-03
>-3.64E-03
>7.18E-03
>9.59E-03
>-2.08E-03
>2.08E-03
>-1.32E-03
>-5.98E-03
>-6.30E-03
>9.53E-03
>1.14E-03
>1.04E-03
>-7.58E-04
>-1.46E-02
>7.66E-03
>5.23E-03
>-1.52E-03
>-5.72E-03
>-2.30E-03
>3.44E-03
>5.14E-03
>-4.86E-03
>3.91E-03
>-4.86E-03
>-7.00E-03
>1.63E-03
>-1.15E-03
>8.04E-03
>-2.67E-03
>-3.74E-03
>3.55E-03
>2.01E-03
>4.19E-03
>4.08E-03
>3.21E-03
>3.49E-03
>5.64E-04
>7.40E-03
>3.63E-03
>-1.86E-04
>-3.73E-03
>3.82E-03
>6.95E-03
>-1.29E-03
>4.70E-03
>-4.60E-04
>-3.60E-03
>-3.42E-03
>-1.02E-03
>-3.81E-03
>7.70E-03
>-3.61E-03
>-2.69E-03
>-7.19E-03
>1.40E-03
>7.48E-04
>-9.02E-03
>-3.12E-03
>3.31E-03
>-1.42E-03
>2.36E-03
>-1.30E-02
>2.76E-03
>3.81E-04
>-4.96E-03
>-9.57E-04
>5.73E-03
>5.41E-03
>-6.63E-04
>1.80E-03
>9.69E-03
>5.04E-03
>-5.61E-03
>7.56E-03
>-5.13E-03
>6.15E-03
>4.82E-03
>-4.73E-03
>-9.29E-05
>6.50E-04
>2.41E-03
>-4.63E-04
>7.41E-04
>5.26E-03
>1.75E-03
>-4.33E-03
>-5.56E-03
>-6.99E-03
>-3.09E-03
>6.36E-03
>1.30E-03
>1.15E-02
>8.25E-03
>-9.13E-04
>-2.56E-03
>-2.57E-03
>-3.13E-03
>6.34E-03
>-1.00E-02
>6.45E-03
>4.59E-04
>1.10E-03
>1.92E-03
>7.11E-03
>3.63E-03
>4.43E-03
>7.72E-03
>-1.25E-03
>8.06E-04
>4.02E-03
>-8.92E-04
>1.34E-03
>-3.75E-03
>-9.07E-03
>9.61E-03
>4.37E-03
>-1.87E-03
>-1.52E-03
>6.32E-03
>1.86E-03
>-2.37E-02
>9.66E-03
>1.44E-03
>9.28E-03
>-6.22E-04
>-3.03E-03
>3.38E-03
>-2.31E-03
>-6.88E-03
>-3.59E-03
>1.53E-03
>6.18E-03
>2.23E-03
>-2.50E-03
>-1.11E-02
>-8.71E-03
>-2.92E-03
>-1.28E-03
>2.56E-03
>3.74E-03
>5.44E-03
>-6.34E-04
>-6.08E-03
>8.19E-04
>3.09E-03
>-1.63E-03
>1.36E-03
>3.99E-03
>5.59E-03
>-5.86E-03
>-7.08E-03
>0.00E+00
>-6.03E-03
>4.75E-03
>3.82E-03
>3.81E-03
>-7.63E-03
>6.09E-03
>2.71E-03
>-2.44E-03
>2.35E-03
>6.40E-03
>1.70E-03
>3.76E-03
>-6.00E-03
>3.77E-03
>7.05E-03
>-2.85E-03
>9.85E-03
>-1.59E-03
>-8.84E-04
>4.41E-03
>5.45E-03
>9.63E-04
>-2.02E-03
>-6.25E-03
>4.58E-03
>2.54E-03
>4.02E-03
>-1.66E-03
>1.14E-03
>-3.76E-03
>7.42E-03
>6.76E-03
>-2.34E-03
>-1.13E-03
>5.19E-03
>-3.98E-03
>-3.56E-03
>7.10E-03
>-3.63E-03
>1.99E-03
>6.29E-03
>2.92E-03
>1.20E-03
>2.99E-03
>9.42E-03
>-5.59E-03
>-3.32E-03
>1.96E-03
>5.85E-03
>-1.61E-03
>6.50E-03
>9.26E-04
>2.44E-03
>-3.02E-03
>2.27E-03
>-5.04E-04
>-2.52E-03
>0.00E+00
>1.60E-03
>5.87E-03
>-6.79E-03
>7.13E-03
>1.17E-03
>6.90E-03
>-4.15E-04
>1.91E-03
>5.79E-04
>1.24E-03
>-5.97E-03
>2.82E-03
>-6.07E-03
>2.50E-04
>-2.17E-02
>-1.01E-02
>-4.57E-03
>4.05E-03
>9.43E-03
>1.28E-03
>-6.41E-03
>-3.35E-03
>5.16E-04
>9.58E-03
>4.08E-03
>-5.94E-04
>-8.49E-04
>1.10E-03
>-1.46E-02
>-6.03E-04
>-1.47E-03
>-1.29E-02
>3.49E-04
>-1.31E-03
>-1.58E-03
>1.93E-03
>-1.66E-03
>6.81E-03
>4.51E-03
>-2.08E-03
>1.39E-03
>-9.54E-04
>-7.57E-03
>2.27E-03
>1.02E-02
>5.08E-03
>7.61E-03
>2.38E-03
>-3.49E-03
>7.06E-03
>4.48E-03
>3.20E-03
>8.40E-04
>-9.20E-03
>4.82E-03
>2.70E-03
>-8.28E-03
>-7.64E-04
>-3.15E-03
>4.08E-03
>-2.55E-03
>4.07E-03
>1.69E-03
>3.80E-03
>-1.43E-03
>-1.19E-02
>-2.05E-03
>-6.69E-03
>-3.02E-03
>2.85E-03
>-1.90E-03
>4.31E-03
>1.03E-02
>2.38E-03
>0.00E+00
>2.97E-03
>6.74E-03
>-2.36E-03
>-9.56E-03
>-1.87E-03
>-4.78E-03
>-7.73E-03
>4.05E-03
>7.53E-03
>-2.13E-03
>7.07E-03
>-3.31E-03
>-6.15E-03
>1.03E-02
>-1.10E-03
>-4.25E-03
>3.91E-03
>-8.49E-04
>-1.19E-03
>-1.02E-03
>2.81E-03
>4.66E-03
>4.23E-04
>-5.92E-04
>2.45E-03
>-8.43E-05
>1.10E-03
>-7.44E-03
>-5.79E-03
>1.19E-03
>2.30E-03
>-8.20E-03
>-1.67E-02
>1.48E-03
>-7.84E-04
>-5.42E-03
>-2.46E-03
>-4.14E-03
>-8.82E-05
>2.03E-03
>6.16E-04
>-9.46E-03
>-8.47E-03
>-5.30E-03
>-4.97E-03
>2.08E-03
>-6.34E-03
>4.26E-03
>-6.63E-03
>1.11E-02
>-1.44E-03
>7.73E-03
>-1.16E-03
>3.94E-03
>8.45E-03
>-1.03E-02
>7.75E-03
>-1.69E-03
>-2.67E-03
>4.36E-03
>5.32E-04
>-8.91E-03
>4.73E-03
>9.58E-03
>2.20E-03
>5.88E-03
>-2.37E-03
>3.68E-03
>8.96E-03
>-3.12E-03
>-2.70E-03
>3.83E-03
>3.03E-03
>-5.03E-03
>-5.22E-04
>1.06E-02
>3.87E-03
>-2.58E-03
>1.72E-04
>1.63E-03
>-1.12E-02
>-6.62E-03
>2.27E-03
>1.22E-03
>7.89E-03
>-5.37E-03
>-9.34E-03
>2.54E-03
>-7.00E-04
>1.11E-02
>-1.13E-03
>6.31E-03
>9.60E-03
>6.82E-04
>-3.67E-03
>1.28E-03
>-7.46E-03
>4.38E-03
>4.79E-03
>-8.31E-03
>-4.22E-03
>-9.81E-03
>1.74E-04
>-8.72E-04
>8.72E-04
>2.96E-03
>-6.11E-03
>  
>
>------------------------------------------------------------------------
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  
>


From joe-byers at utulsa.edu  Mon Aug 28 17:40:59 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Mon, 28 Aug 2006 10:40:59 -0500
Subject: [R-SIG-Finance] Risk management research simulation questions
Message-ID: <ecv2pa$i3a$1@sea.gmane.org>

Rmetrics group,

I am working on a project to determine the errors associated with 
structural assumptions underlying a companies Value at Risk calculation. 
  Normal VAR calculations using a covariance matrix for the portfolio 
assume constant mean or zero mean if the returns are mean adjusted. 
This project calls for creating 4-5 hypothetical assets, 1 constant mean 
and variance, 1 seasonal mean and constant variance, 1 constant mean and 
seasonal variance, 1 time varying mean (AR or Garch in mean), 1 time 
varying variance (GARCH type).  I want to provide the hypothetical 
parameters for these assets and simulate returns.  I can simulate each 
of these assets as independent but really need correlated errors.

These returns will be used to calculate a benchmark risk metrics type 
VAR and then progess through correcting the VAR calculations for each 
case of asses type.

Anyone that is interested, I would appreciate suggestions.  I am also 
favoring co-authorship for this help.

Thank you
Joe


From joe-byers at utulsa.edu  Mon Aug 28 18:23:11 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Mon, 28 Aug 2006 11:23:11 -0500
Subject: [R-SIG-Finance] Risk management research simulation questions
In-Reply-To: <ecv2pa$i3a$1@sea.gmane.org>
References: <ecv2pa$i3a$1@sea.gmane.org>
Message-ID: <ecv58e$rmu$1@sea.gmane.org>

I should have been a little more specific.  I need help simulating the 
correlated assets.
thanx


Joe Byers wrote:
> Rmetrics group,
> 
> I am working on a project to determine the errors associated with 
> structural assumptions underlying a companies Value at Risk calculation. 
>   Normal VAR calculations using a covariance matrix for the portfolio 
> assume constant mean or zero mean if the returns are mean adjusted. 
> This project calls for creating 4-5 hypothetical assets, 1 constant mean 
> and variance, 1 seasonal mean and constant variance, 1 constant mean and 
> seasonal variance, 1 time varying mean (AR or Garch in mean), 1 time 
> varying variance (GARCH type).  I want to provide the hypothetical 
> parameters for these assets and simulate returns.  I can simulate each 
> of these assets as independent but really need correlated errors.
> 
> These returns will be used to calculate a benchmark risk metrics type 
> VAR and then progess through correcting the VAR calculations for each 
> case of asses type.
> 
> Anyone that is interested, I would appreciate suggestions.  I am also 
> favoring co-authorship for this help.
> 
> Thank you
> Joe
>


From brian at braverock.com  Mon Aug 28 19:04:52 2006
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 28 Aug 2006 12:04:52 -0500
Subject: [R-SIG-Finance] Risk management research simulation questions
In-Reply-To: <ecv2pa$i3a$1@sea.gmane.org>
References: <ecv2pa$i3a$1@sea.gmane.org>
Message-ID: <200608281204.52922.brian@braverock.com>

On Monday 28 August 2006 10:40, Joe Byers wrote:
> Rmetrics group,
>
> I am working on a project to determine the errors associated with
> structural assumptions underlying a companies Value at Risk
> calculation. Normal VAR calculations using a covariance matrix for the
> portfolio assume constant mean or zero mean if the returns are mean
> adjusted. This project calls for creating 4-5 hypothetical assets, 1
> constant mean and variance, 1 seasonal mean and constant variance, 1
> constant mean and seasonal variance, 1 time varying mean (AR or Garch
> in mean), 1 time varying variance (GARCH type).  I want to provide the
> hypothetical parameters for these assets and simulate returns.  I can
> simulate each of these assets as independent but really need correlated
> errors.
>
> These returns will be used to calculate a benchmark risk metrics type
> VAR and then progess through correcting the VAR calculations for each
> case of asses type.
>
> Anyone that is interested, I would appreciate suggestions.  I am also
> favoring co-authorship for this help.

I've had very good success using Modified Cornish-Fisher VaR to handle the 
non-normality of the distribution, occasionally with a weighted average 
of since-inception VaR and rolling period VaR. 

Why wouldn't you choose existing (real) assets with the characteristics 
that you want to use in your simulated portfolios?

If you want to simulate assets, there are several simulation functions in 
RMetrics and in other R packages, and I'd suggest that you start there.  
However, I don't find that these end up looking much like the 
distributions of real assets in practice, so I don't tend to use them 
very often.

Regards,

  - Brian


From joe-byers at utulsa.edu  Mon Aug 28 19:37:36 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Mon, 28 Aug 2006 12:37:36 -0500
Subject: [R-SIG-Finance] Risk management research simulation questions
In-Reply-To: <200608281204.52922.brian@braverock.com>
References: <ecv2pa$i3a$1@sea.gmane.org>
	<200608281204.52922.brian@braverock.com>
Message-ID: <44F329E0.90805@utulsa.edu>

I want to simulate hypothetical assets so I can control all aspects of 
the tests, from parameters to correlations across assets.  I can 
construct correlations based on minimum variance hedge ratios that will 
allow me to create hedge portfolios with higher weights on some assets 
than others.  This way I can also look at hedging aspects within the VAR 
calculation and the problems with violating the models assumptions.

I have used garchsim and armasim, but as I understand their 
implementation, I am simulating the independent process, not a 
correlated process.

Including the modified cornish VAR is a really good idea as a benchmark 
case as well.

thanks for that suggestion, if nothing else you are entitled to a 
footnote for it.

Thanx
Joe


Brian G. Peterson wrote:
> On Monday 28 August 2006 10:40, Joe Byers wrote:
>   
>> Rmetrics group,
>>
>> I am working on a project to determine the errors associated with
>> structural assumptions underlying a companies Value at Risk
>> calculation. Normal VAR calculations using a covariance matrix for the
>> portfolio assume constant mean or zero mean if the returns are mean
>> adjusted. This project calls for creating 4-5 hypothetical assets, 1
>> constant mean and variance, 1 seasonal mean and constant variance, 1
>> constant mean and seasonal variance, 1 time varying mean (AR or Garch
>> in mean), 1 time varying variance (GARCH type).  I want to provide the
>> hypothetical parameters for these assets and simulate returns.  I can
>> simulate each of these assets as independent but really need correlated
>> errors.
>>
>> These returns will be used to calculate a benchmark risk metrics type
>> VAR and then progess through correcting the VAR calculations for each
>> case of asses type.
>>
>> Anyone that is interested, I would appreciate suggestions.  I am also
>> favoring co-authorship for this help.
>>     
>
> I've had very good success using Modified Cornish-Fisher VaR to handle the 
> non-normality of the distribution, occasionally with a weighted average 
> of since-inception VaR and rolling period VaR. 
>
> Why wouldn't you choose existing (real) assets with the characteristics 
> that you want to use in your simulated portfolios?
>
> If you want to simulate assets, there are several simulation functions in 
> RMetrics and in other R packages, and I'd suggest that you start there.  
> However, I don't find that these end up looking much like the 
> distributions of real assets in practice, so I don't tend to use them 
> very often.
>
> Regards,
>
>   - Brian
>   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060828/800933a3/attachment.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: joe-byers.vcf
Type: text/x-vcard
Size: 295 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060828/800933a3/attachment.vcf 

From Ricardo.Zambrano at corpbanca.cl  Tue Aug 29 19:50:32 2006
From: Ricardo.Zambrano at corpbanca.cl (Ricardo Zambrano Aguilera)
Date: Tue, 29 Aug 2006 13:50:32 -0400
Subject: [R-SIG-Finance] HSAUR
Message-ID: <800F3C4ADCFCA643A919EF7C140A045A0E0FE914@LLANQUIHUE.corpgroup.cl>

Dear List
this last weekend  i?m was cheking the package "HSAUR":- A Handbook of Statistical Analyses Using R with their examples and data base.
it was an excellent contribution to see the the abilities of the graphical behavior of this wonderful program.
 Best regards
 Ricardo


From kriskumar at earthlink.net  Wed Aug 30 02:50:02 2006
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Tue, 29 Aug 2006 20:50:02 -0400
Subject: [R-SIG-Finance] Risk management research simulation questions
In-Reply-To: <ecv58e$rmu$1@sea.gmane.org>
References: <ecv2pa$i3a$1@sea.gmane.org> <ecv58e$rmu$1@sea.gmane.org>
Message-ID: <44F4E0BA.10609@earthlink.net>

Hello Joe, Here is a simple piece of code that simulates correlated 
assets and computes both historical and MC VaR.

 Now for the seasonal mean and covariance a very inelegant way is to 
simply timestep the simulation and then generate random returns using 
the seasonal covariance(mean) I am not sure about using Garch and if 
indeed using pearson correlation one can generate Garch processes that 
have the required correlation structure. Essentially the marginal and 
joint distributions are unlikely to be gaussian and so ordinary 
correlation mayn't be a good measure of the dependence.
However I suggest a more practical alternative assuming marginal 
distribution one can easily use copula functions to generate very 
realistic scenarios.
This works very well and I like this approach better.

Indeed there appears to be a copula Garch model, there is a very nice 
copula library in R and univariate and multivariate Garch can be done
so one can attempt to do what these authors indicate 
(http://www.faculty.ucr.edu/~taelee/paper/LeeLong.pdf  and 
http://www.fame.ch/library/EN/RP69.pdf)
I haven't tried this yet.

Hope this helps,

Best,
Krishna



# the below code assumes 1 year VaR at the 95% level you'd have to scale 
volatility/returns if you want some other horizon!.

require(VaR)
data(DJIA)
require(MASS)
nsim<-1000
ticker<-c("INTC","IBM","GE")
mydata<-as.matrix(subset(DJIA,select=ticker))
retdata<-diff(log(mydata))
cov.mat<-cov(retdata)
wts<-c(0.2,0.4,0.4) # define your portfolio weights.
hist.vol<- sqrt(wts %*% cov.mat %*% wts)
ret.mean<-apply(retdata,2,mean)
hist.mean<-sum(ret.mean,wts)
#Historical VaR estimate
hist.var <- qnorm(0.05,mean=hist.vol,sd=hist.vol)
cat("historical VaR at the 95% level \n ", format(hist.var,digits=2))
sim.ret<-mvrnorm(nsim,mu=ret.mean,Sigma=cov.mat)
sim.wtret<-t(wts * t(sim.ret))
x11()
hist(sim.wtret)
#read the VaR as the quantile of the loss distribution
mc.var<-quantile(sim.wtret,0.05)
cat("mc VaR at the 95% level \n ", format(mc.var,digits=2))




Joe Byers wrote:

>I should have been a little more specific.  I need help simulating the 
>correlated assets.
>thanx
>
>
>Joe Byers wrote:
>  
>
>>Rmetrics group,
>>
>>I am working on a project to determine the errors associated with 
>>structural assumptions underlying a companies Value at Risk calculation. 
>>  Normal VAR calculations using a covariance matrix for the portfolio 
>>assume constant mean or zero mean if the returns are mean adjusted. 
>>This project calls for creating 4-5 hypothetical assets, 1 constant mean 
>>and variance, 1 seasonal mean and constant variance, 1 constant mean and 
>>seasonal variance, 1 time varying mean (AR or Garch in mean), 1 time 
>>varying variance (GARCH type).  I want to provide the hypothetical 
>>parameters for these assets and simulate returns.  I can simulate each 
>>of these assets as independent but really need correlated errors.
>>
>>These returns will be used to calculate a benchmark risk metrics type 
>>VAR and then progess through correcting the VAR calculations for each 
>>case of asses type.
>>
>>Anyone that is interested, I would appreciate suggestions.  I am also 
>>favoring co-authorship for this help.
>>
>>Thank you
>>Joe
>>
>>    
>>
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From kriskumar at earthlink.net  Wed Aug 30 02:53:22 2006
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Tue, 29 Aug 2006 20:53:22 -0400
Subject: [R-SIG-Finance] Risk management research simulation questions
In-Reply-To: <44F4E0BA.10609@earthlink.net>
References: <ecv2pa$i3a$1@sea.gmane.org> <ecv58e$rmu$1@sea.gmane.org>
	<44F4E0BA.10609@earthlink.net>
Message-ID: <44F4E182.7040806@earthlink.net>

A small typo
 hist.var <- qnorm(0.05,mean=hist.vol,sd=hist.vol)
should read
hist.var <- qnorm(0.05,mean=hist.mean,sd=hist.vol)

Krishna Kumar wrote:

> Hello Joe, Here is a simple piece of code that simulates correlated 
> assets and computes both historical and MC VaR.
>
> Now for the seasonal mean and covariance a very inelegant way is to 
> simply timestep the simulation and then generate random returns using 
> the seasonal covariance(mean) I am not sure about using Garch and if 
> indeed using pearson correlation one can generate Garch processes that 
> have the required correlation structure. Essentially the marginal and 
> joint distributions are unlikely to be gaussian and so ordinary 
> correlation mayn't be a good measure of the dependence.
> However I suggest a more practical alternative assuming marginal 
> distribution one can easily use copula functions to generate very 
> realistic scenarios.
> This works very well and I like this approach better.
>
> Indeed there appears to be a copula Garch model, there is a very nice 
> copula library in R and univariate and multivariate Garch can be done
> so one can attempt to do what these authors indicate 
> (http://www.faculty.ucr.edu/~taelee/paper/LeeLong.pdf  and 
> http://www.fame.ch/library/EN/RP69.pdf)
> I haven't tried this yet.
>
> Hope this helps,
>
> Best,
> Krishna
>
>
>
> # the below code assumes 1 year VaR at the 95% level you'd have to 
> scale volatility/returns if you want some other horizon!.
>
> require(VaR)
> data(DJIA)
> require(MASS)
> nsim<-1000
> ticker<-c("INTC","IBM","GE")
> mydata<-as.matrix(subset(DJIA,select=ticker))
> retdata<-diff(log(mydata))
> cov.mat<-cov(retdata)
> wts<-c(0.2,0.4,0.4) # define your portfolio weights.
> hist.vol<- sqrt(wts %*% cov.mat %*% wts)
> ret.mean<-apply(retdata,2,mean)
> hist.mean<-sum(ret.mean,wts)
> #Historical VaR estimate
> hist.var <- qnorm(0.05,mean=hist.vol,sd=hist.vol)
> cat("historical VaR at the 95% level \n ", format(hist.var,digits=2))
> sim.ret<-mvrnorm(nsim,mu=ret.mean,Sigma=cov.mat)
> sim.wtret<-t(wts * t(sim.ret))
> x11()
> hist(sim.wtret)
> #read the VaR as the quantile of the loss distribution
> mc.var<-quantile(sim.wtret,0.05)
> cat("mc VaR at the 95% level \n ", format(mc.var,digits=2))
>
>
>
>
> Joe Byers wrote:
>
>> I should have been a little more specific.  I need help simulating 
>> the correlated assets.
>> thanx
>>
>>
>> Joe Byers wrote:
>>  
>>
>>> Rmetrics group,
>>>
>>> I am working on a project to determine the errors associated with 
>>> structural assumptions underlying a companies Value at Risk 
>>> calculation.  Normal VAR calculations using a covariance matrix for 
>>> the portfolio assume constant mean or zero mean if the returns are 
>>> mean adjusted. This project calls for creating 4-5 hypothetical 
>>> assets, 1 constant mean and variance, 1 seasonal mean and constant 
>>> variance, 1 constant mean and seasonal variance, 1 time varying mean 
>>> (AR or Garch in mean), 1 time varying variance (GARCH type).  I want 
>>> to provide the hypothetical parameters for these assets and simulate 
>>> returns.  I can simulate each of these assets as independent but 
>>> really need correlated errors.
>>>
>>> These returns will be used to calculate a benchmark risk metrics 
>>> type VAR and then progess through correcting the VAR calculations 
>>> for each case of asses type.
>>>
>>> Anyone that is interested, I would appreciate suggestions.  I am 
>>> also favoring co-authorship for this help.
>>>
>>> Thank you
>>> Joe
>>>
>>>   
>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
>>  
>>
>
>


From brian at braverock.com  Wed Aug 30 04:52:59 2006
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 29 Aug 2006 21:52:59 -0500
Subject: [R-SIG-Finance] Risk management research simulation questions
In-Reply-To: <44F4E0BA.10609@earthlink.net>
References: <ecv2pa$i3a$1@sea.gmane.org> <ecv58e$rmu$1@sea.gmane.org>
	<44F4E0BA.10609@earthlink.net>
Message-ID: <200608292153.00115.brian@braverock.com>

Krishna, 

Thank you for the very useful simulation code.  I would like to correct an 
oversimplification on your estimate of VaR.

On Tuesday 29 August 2006 19:50, Krishna Kumar wrote:
> #read the VaR as the quantile of the loss distribution
> mc.var<-quantile(sim.wtret,0.05)
> cat("mc VaR at the 95% level \n ", format(mc.var,digits=2))

This isn't always correct for non-normal distributions.  
See my earlier posts on Modified Cornish-Fisher VaR here:
? ? http://article.gmane.org/gmane.comp.lang.r.r-metrics/855

A function to calculate modified Cornish-Fisher VaR for all distributions 
and a variety of other R functions for dealing with higher moments of the 
return distribution may be found here:

http://braverock.com/brian/R/extra_moments.R

I suspect that you might be able to use quantile if you understood enough 
about the distribution to know which estimating technique to use, but it 
will be more accurate to calculate the moments directly, using the 
modifiedVaR function in the file I link to above.

Regards,

  - Brian


From gyadav at ccilindia.co.in  Thu Aug 31 14:13:53 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Thu, 31 Aug 2006 17:43:53 +0530
Subject: [R-SIG-Finance] Risk management research simulation questions
In-Reply-To: <44F4E182.7040806@earthlink.net>
Message-ID: <OFC96960F1.897587EA-ON652571DB.0042F191-652571AE.0043570C@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060831/dbeb191a/attachment.pl 

From gyadav at ccilindia.co.in  Thu Aug 31 14:23:11 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Thu, 31 Aug 2006 17:53:11 +0530
Subject: [R-SIG-Finance] Risk management research simulation questions
Message-ID: <OF0333565C.322F0A86-ON652571DB.0043CE86-652571AE.0044315D@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060831/9fe46ba6/attachment.pl 

From prossi79.r at gmail.com  Thu Aug 31 17:36:57 2006
From: prossi79.r at gmail.com (Paolo Rossi)
Date: Thu, 31 Aug 2006 17:36:57 +0200
Subject: [R-SIG-Finance] R GUI crashes with RBloomberg package
Message-ID: <9e8f1a130608310836g363b9aa1w5d18e8da940b6b4d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060831/5bf44a0f/attachment.pl 

From davidr at rhotrading.com  Thu Aug 31 17:50:11 2006
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Thu, 31 Aug 2006 10:50:11 -0500
Subject: [R-SIG-Finance] R GUI crashes with RBloomberg package
Message-ID: <F9F2A641C593D7408925574C05A7BE770D1E42@rhopost.rhotrading.com>

You _do_ have Bloomberg installed on your computer?
It should work then - does for me.

David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605
312-362-4963
-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Paolo
Rossi
Sent: Thursday, August 31, 2006 10:37 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] R GUI crashes with RBloomberg package

Hi all,
I'm trying to use the RBloomberg package with R 2.3.1 installed on my
computer.

I load R Console, then type "library(RBloomberg)" and the application
correctly loads all the required libraries, in the order RDCOMClient,
zoo
and chron. I get a message saying "Contents of bbfields have been stored
in
.bbfields in the currenct workspace". It seems that all is working fine
:-)

The problem is that when I type "blpConnect()", R crashes and I have to
quit
the program with the classical Windows XP general error window.

What should I do in order to have the package working?

Thank you very much

Paolo Rossi
prossi79.r at gmail.com

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From edd at debian.org  Thu Aug 31 18:16:16 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 31 Aug 2006 11:16:16 -0500
Subject: [R-SIG-Finance] UKSIP - Quantitative investment professionals
	meetings
Message-ID: <17655.2896.541826.251785@basebud.nulle.part>


John Marsland, currently struggling with his email software, asked me to
forward this.  Please direct all questions directly to John at John dot
Marsland at Schroders dot com    -- Dirk, wearing his listmaster head


From: Marsland, John 
Sent: 31 August 2006 13:20
To: 'r-sig-finance at stat.math.ethz.ch'
Subject: UKSIP - Quantitative investment professionals meetings


I noticed a few R-finance list attendees last time, so I though it might be of interest...

UKSIP's Quantitative investment professionals specialist interest group has organised two professional development events at which members will be able to debate more specialist quantitative issues and also take advantage of the opportunity to network with fellow "quant" experts. There is a small charge of ?25 for UKSIP members / ?35 for non-members

Portfolio analysis with random portfolios
Tuesday 5 September 2006, 5.30pm 
Speakers:
Patrick Burns - Burns Statistics
Frances Cowell IMC - Morley Fund Management

Random portfolios sample from the set of all portfolios of a given universe that satisfy some set of constraints. Uses of random portfolios include performance measurement, evaluating proposed strategies and deciding how tight constraints should be. They provide much more certainty for these tasks than currently used methods allow. Patrick Burns will argue that the practice of fund management could potentially be transformed as the use of random portfolios spreads. Frances Cowell will consider some practical applications of random portfolio simulation for institutional investors.

Click here for more information and to register http://www.uksip.org/calendar/event_details.cfm?iEventID=134

Risk management, optimisation and option pricing: stable non-Gaussian 
Tuesday 7 November 2006, 5.30pm
Speakers:
Boryana Racheva-Iotova, FinAnalytica
Zari Rachev, University of Karlsruhe and University of California, Santa Barbara

In this talk the speakers will introduce:
- An accurate approach to risk management, based on tail risk and stable processes for asset returns
- A stable non-Gaussian approach to portfolio optimisation and ALM, yielding increased risk adjusted returns
- A stable option pricing, capturing clustering of the volatility

Click here for more information and to register http://www.uksip.org/calendar/event_details.cfm?iEventID=135

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From kriskumar at earthlink.net  Thu Aug 31 18:39:38 2006
From: kriskumar at earthlink.net (kriskumar at earthlink.net)
Date: Thu, 31 Aug 2006 16:39:38 +0000
Subject: [R-SIG-Finance] Risk management research simulation questions
In-Reply-To: <OFC96960F1.897587EA-ON652571DB.0042F191-652571AE.0043570C@ccilindia.co.in>
References: <44F4E182.7040806@earthlink.net>
	<OFC96960F1.897587EA-ON652571DB.0042F191-652571AE.0043570C@ccilindia.co.in>
Message-ID: <2078517811-1157042383-cardhu_blackberry.rim.net-2061866941-@bwe017-cell00.bisx.prod.on.blackberry>

Yep that is correct.
hist.mean <- ret.mean %*% wts
Hope this helps

Sent from my BlackBerry? wireless handheld  

-----Original Message-----
From: gyadav at ccilindia.co.in
Date: Thu, 31 Aug 2006 17:43:53 
Cc:r-sig-finance at stat.math.ethz.ch, r-sig-finance-bounces at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Risk management research simulation questions


hi Krishna

I feel that there is one more typo error in your code. Please clarify me 
if in case i am wrong....

hist.mean <- sum(ret.mean,wts)

should be multiplication of weights as mean = mu1*wt1 + mu2*wt2 + mu3*wt3

thanks






   Sayonara With Smile & With Warm Regards :-)

  G a u r a v   Y a d a v
  Senior Executive Officer,
  Economic Research & Surveillance Department,
  Clearing Corporation Of India Limited.

  Address: 5th, 6th, 7th Floor, Trade Wing 'C',  Kamala City, S.B. Marg, 
Mumbai - 400 013
  Telephone(Office): - +91 022 6663 9398 ,  Mobile(Personal) (0)9821286118
  Email(Office) :- gyadav at ccilindia.co.in ,  Email(Personal) :- 
emailtogauravyadav at gmail.com


============================================================================================
DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From victor_gushchin at yahoo.co.uk  Fri Aug 25 11:19:37 2006
From: victor_gushchin at yahoo.co.uk (Victor Gushchin)
Date: Fri, 25 Aug 2006 09:19:37 +0000 (GMT)
Subject: [R-SIG-Finance] ca.po Pz test question
Message-ID: <20060825091937.29190.qmail@web25105.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060825/548875ee/attachment.pl 

From Bernhard_Pfaff at fra.invesco.com  Fri Sep  1 14:41:48 2006
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Fri, 1 Sep 2006 13:41:48 +0100
Subject: [R-SIG-Finance] ca.po Pz test question
In-Reply-To: <20060825091937.29190.qmail@web25105.mail.ukl.yahoo.com>
Message-ID: <E4A9111DA23BA048B9A46686BF727CF461BD20@DEFRAXMB01.corp.amvescap.net>

Hello Victor,

let me answer your questions in turn:

>Hello,
>I have a few questions about Phillips& Ouliaris Unit Root Test 
>of type Pz 
>1) I noticed that critical values given in the article 
>"Asymptotic properties of residual..." by Phillips and 
>Ouliaris are different from those given by R. More presicely - 
>they are swaped with each other. Could explain why?

No, probably you should contact R.More, to clarify the issue.
Incidentally, would it be possible to provide a source for 'R.More';
i.e., to which article, monographie are you referring? 

>2) that question is quite stupid. Can you explain what 
>coefficients for test Pz mean?
>for Pu it is straight-forward -      Var1~ Var2*k1 + intercept 
>+u. and u is tested for having a unit root.
>for Pz there it looks like
>Coefficients:
>            Estimate Std. Error t value Pr(>|t|)    
>(Intercept) 0.010013   0.002409   4.156 3.40e-05 ***
>zrV1        0.971824   0.006092 159.536  < 2e-16 ***
>zrV2        0.108994   0.047737   2.283   0.0225 *  
> 
>is it an equation for first differences? like if zrV1 is the 
>first differ for V1 and zrV2 is the first diff for zrV2 then
>k1*zrV1+k2*zrV2+intercept~0   ???
> 

Type 'ca.po' at the prompt and you will see function. Here, you will
find pretty at the beginning of the function's body:
.
.
.
zl <- z[2:nobs, ]
zr <- z[1:(nobs - 1), ] 
.
.
.
and further below:
.
.
.
else if (type == "Pz") {
   test.reg <- summary(lm(zl ~ zr))
.
.
.
hence, the levels of the endogenous variables are regressed on their
first lag. See ?lm for how lm behaves if a matrix/data.frame is on the
left hand side.


>3) and the last question - why for Pz there is an option to 
>use demean="trend" and there is no such an option for Pu?
> 

Well, to be precise: if you call type = "Pz" and demean = "trend" an
error is returned; and here you caught me. The trend 'trd' is not set
properly, you can fix this easily by inserting the line end marked as:
## inserting fix:


    else if (demean == "trend") {
        ari3 <- 3
        model <- "with constant and linear trend"
        trd <- 1:nobs
        res <- residuals(lm(zl ~ zr + trd))
        if (type == "Pu") {
            trd <- 1:(nobs+1) ## inserting fix
            resu <- residuals(lm(z[, 1] ~ z[, -1] + trd))
            test.reg <- summary(lm(z[, 1] ~ z[, -1] + trd))
        }
        else if (type == "Pz") {
            test.reg <- summary(lm(zl ~ zr + trd))
        }
    }


I will send an corrected version of 'urca' to CRAN; thanks for pointing
this out.

Best,
Bernhard

>Kind regards,
>Victor
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From deliverable at gmail.com  Thu Sep  7 09:20:29 2006
From: deliverable at gmail.com (A Curious Mind)
Date: Thu, 7 Sep 2006 00:20:29 -0700
Subject: [R-SIG-Finance] Examples from Rmetrics User Guides
Message-ID: <7c737f300609070020h5b77ecf8m15dbf0701ab122ff@mail.gmail.com>

Is there a download with the examples -- xmp...() and the datasets
from the user guides, f*.pdf, or are they a part of Rmetrics?

Cheers,
Alexy


From deliverable at gmail.com  Thu Sep  7 09:44:30 2006
From: deliverable at gmail.com (A Curious Mind)
Date: Thu, 7 Sep 2006 00:44:30 -0700
Subject: [R-SIG-Finance] Eric Zivot's HF library in R
Message-ID: <7c737f300609070044u7384dd37j9fe76488ab8288dd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060907/74be34eb/attachment.pl 

From Ricardo.Zambrano at corpbanca.cl  Thu Sep  7 19:06:29 2006
From: Ricardo.Zambrano at corpbanca.cl (Ricardo Zambrano Aguilera)
Date: Thu, 7 Sep 2006 13:06:29 -0400
Subject: [R-SIG-Finance] predict returns with the fSeries package
Message-ID: <800F3C4ADCFCA643A919EF7C140A045A0E6540CF@LLANQUIHUE.corpgroup.cl>

Dear List
 how i can predict returns with new data??
 > ajuste1

Title:
 GARCH Modelling 

Call:
 garchFit(formula.mean = ~arma(2, 0), formula.var = ~garch(1, 
    1), series = r_peso, cond.dist = "dnorm") 

Mean and Variance Equation:
 ~arma(2, 0) + ~garch(1, 1) 

Conditional Distribution:
 dnorm 

Coefficient(s):
          mu           ar1           ar2         omega        alpha1         beta1  
-1.25313e-04   6.10406e-02  -7.06526e-02   2.11754e-06   9.47503e-02   8.45540e-01  

Error Analysis:
         Estimate  Std. Error  t value Pr(>|t|)    
mu     -1.253e-04   1.466e-04   -0.855   0.3925    
ar1     6.104e-02   2.765e-02    2.208   0.0272 *  
ar2    -7.065e-02   2.785e-02   -2.537   0.0112 *  
omega   2.118e-06   1.114e-06    1.902   0.0572 .  
alpha1  9.475e-02   2.411e-02    3.929 8.51e-05 ***
beta1   8.455e-01   5.195e-02   16.277  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Log Likelihood:
 -5473.802    normalized:  -3.751749 

Description:
 Thu Sep 07 11:50:50 2006 

#######
################## then....############################
> predict(ajuste1)
    meanForecast   meanError standardDeviation
1   0.0001657992 0.005873028       0.004574582
2  -0.0002516247 0.005883959       0.004668490
3  -0.0001535909 0.005897073       0.004755100
4  -0.0001181148 0.005897279       0.004835123
5  -0.0001228757 0.005897331       0.004909178
6  -0.0001256727 0.005897333       0.004977807
7  -0.0001255071 0.005897334       0.005041485
8  -0.0001252994 0.005897334       0.005100636
9  -0.0001252984 0.005897334       0.005155636
10 -0.0001253130 0.005897334       0.005206823



######### now if a put newdata it?s the same??, how i can know the returns for the next week if i put the returns of the last week????############ 

> predict(ajuste1,newdata=returns[1451:1460,1])
    meanForecast   meanError standardDeviation
1   0.0001657992 0.005873028       0.004574582
2  -0.0002516247 0.005883959       0.004668490
3  -0.0001535909 0.005897073       0.004755100
4  -0.0001181148 0.005897279       0.004835123
5  -0.0001228757 0.005897331       0.004909178
6  -0.0001256727 0.005897333       0.004977807
7  -0.0001255071 0.005897334       0.005041485
8  -0.0001252994 0.005897334       0.005100636
9  -0.0001252984 0.005897334       0.005155636
10 -0.0001253130 0.005897334       0.005206823

My best regards Ricardo Z.


From Joe-Byers at utulsa.edu  Fri Sep  8 18:18:13 2006
From: Joe-Byers at utulsa.edu (Joe W. Byers)
Date: Fri, 08 Sep 2006 11:18:13 -0500
Subject: [R-SIG-Finance] Risk management research simulation questions
In-Reply-To: <44F329E0.90805@utulsa.edu>
References: <ecv2pa$i3a$1@sea.gmane.org>	<200608281204.52922.brian@braverock.com>
	<44F329E0.90805@utulsa.edu>
Message-ID: <eds547$8cb$1@sea.gmane.org>

I want to thank everyone that posted replies to this question.  You have 
provided insight and helped me clarify how to proceed with this project, 
along with some additional ideas to incorporate, like the modified 
cornish VAR.

When I get this project to a complete working paper, I will gladly 
provide it to you.

Thank you
Joe


Joe Byers wrote:
> I want to simulate hypothetical assets so I can control all aspects of 
> the tests, from parameters to correlations across assets.  I can 
> construct correlations based on minimum variance hedge ratios that will 
> allow me to create hedge portfolios with higher weights on some assets 
> than others.  This way I can also look at hedging aspects within the VAR 
> calculation and the problems with violating the models assumptions.
> 
> I have used garchsim and armasim, but as I understand their 
> implementation, I am simulating the independent process, not a 
> correlated process.
> 
> Including the modified cornish VAR is a really good idea as a benchmark 
> case as well.
> 
> thanks for that suggestion, if nothing else you are entitled to a 
> footnote for it.
> 
> Thanx
> Joe
> 
> 
> Brian G. Peterson wrote:
>> On Monday 28 August 2006 10:40, Joe Byers wrote:
>>   
>>> Rmetrics group,
>>>
>>> I am working on a project to determine the errors associated with
>>> structural assumptions underlying a companies Value at Risk
>>> calculation. Normal VAR calculations using a covariance matrix for the
>>> portfolio assume constant mean or zero mean if the returns are mean
>>> adjusted. This project calls for creating 4-5 hypothetical assets, 1
>>> constant mean and variance, 1 seasonal mean and constant variance, 1
>>> constant mean and seasonal variance, 1 time varying mean (AR or Garch
>>> in mean), 1 time varying variance (GARCH type).  I want to provide the
>>> hypothetical parameters for these assets and simulate returns.  I can
>>> simulate each of these assets as independent but really need correlated
>>> errors.
>>>
>>> These returns will be used to calculate a benchmark risk metrics type
>>> VAR and then progess through correcting the VAR calculations for each
>>> case of asses type.
>>>
>>> Anyone that is interested, I would appreciate suggestions.  I am also
>>> favoring co-authorship for this help.
>>>     
>>
>> I've had very good success using Modified Cornish-Fisher VaR to handle the 
>> non-normality of the distribution, occasionally with a weighted average 
>> of since-inception VaR and rolling period VaR. 
>>
>> Why wouldn't you choose existing (real) assets with the characteristics 
>> that you want to use in your simulated portfolios?
>>
>> If you want to simulate assets, there are several simulation functions in 
>> RMetrics and in other R packages, and I'd suggest that you start there.  
>> However, I don't find that these end up looking much like the 
>> distributions of real assets in practice, so I don't tend to use them 
>> very often.
>>
>> Regards,
>>
>>   - Brian
>>


From wuertz at itp.phys.ethz.ch  Sat Sep  9 01:34:15 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sat, 09 Sep 2006 01:34:15 +0200
Subject: [R-SIG-Finance] predict returns with the fSeries package
In-Reply-To: <800F3C4ADCFCA643A919EF7C140A045A0E6540CF@LLANQUIHUE.corpgroup.cl>
References: <800F3C4ADCFCA643A919EF7C140A045A0E6540CF@LLANQUIHUE.corpgroup.cl>
Message-ID: <4501FDF7.9020006@itp.phys.ethz.ch>

Ricardo Zambrano Aguilera wrote:


Ther is no new data! It's like in the Arima case of R!

You model and fit the time series up to the end, and then you start
with your forecast at position n+1. since there can't is be no newdata,
you must get always the same result, as you did.

I hope this helps
Diethelm Wuertz


>Dear List
> how i can predict returns with new data??
> > ajuste1
>
>Title:
> GARCH Modelling 
>
>Call:
> garchFit(formula.mean = ~arma(2, 0), formula.var = ~garch(1, 
>    1), series = r_peso, cond.dist = "dnorm") 
>
>Mean and Variance Equation:
> ~arma(2, 0) + ~garch(1, 1) 
>
>Conditional Distribution:
> dnorm 
>
>Coefficient(s):
>          mu           ar1           ar2         omega        alpha1         beta1  
>-1.25313e-04   6.10406e-02  -7.06526e-02   2.11754e-06   9.47503e-02   8.45540e-01  
>
>Error Analysis:
>         Estimate  Std. Error  t value Pr(>|t|)    
>mu     -1.253e-04   1.466e-04   -0.855   0.3925    
>ar1     6.104e-02   2.765e-02    2.208   0.0272 *  
>ar2    -7.065e-02   2.785e-02   -2.537   0.0112 *  
>omega   2.118e-06   1.114e-06    1.902   0.0572 .  
>alpha1  9.475e-02   2.411e-02    3.929 8.51e-05 ***
>beta1   8.455e-01   5.195e-02   16.277  < 2e-16 ***
>---
>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>
>Log Likelihood:
> -5473.802    normalized:  -3.751749 
>
>Description:
> Thu Sep 07 11:50:50 2006 
>
>#######
>################## then....############################
>  
>
>>predict(ajuste1)
>>    
>>
>    meanForecast   meanError standardDeviation
>1   0.0001657992 0.005873028       0.004574582
>2  -0.0002516247 0.005883959       0.004668490
>3  -0.0001535909 0.005897073       0.004755100
>4  -0.0001181148 0.005897279       0.004835123
>5  -0.0001228757 0.005897331       0.004909178
>6  -0.0001256727 0.005897333       0.004977807
>7  -0.0001255071 0.005897334       0.005041485
>8  -0.0001252994 0.005897334       0.005100636
>9  -0.0001252984 0.005897334       0.005155636
>10 -0.0001253130 0.005897334       0.005206823
>
>
>
>######### now if a put newdata it?s the same??, how i can know the returns for the next week if i put the returns of the last week????############ 
>
>  
>
>>predict(ajuste1,newdata=returns[1451:1460,1])
>>    
>>
>    meanForecast   meanError standardDeviation
>1   0.0001657992 0.005873028       0.004574582
>2  -0.0002516247 0.005883959       0.004668490
>3  -0.0001535909 0.005897073       0.004755100
>4  -0.0001181148 0.005897279       0.004835123
>5  -0.0001228757 0.005897331       0.004909178
>6  -0.0001256727 0.005897333       0.004977807
>7  -0.0001255071 0.005897334       0.005041485
>8  -0.0001252994 0.005897334       0.005100636
>9  -0.0001252984 0.005897334       0.005155636
>10 -0.0001253130 0.005897334       0.005206823
>
>My best regards Ricardo Z.
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From deliverable at gmail.com  Sun Sep 10 23:35:30 2006
From: deliverable at gmail.com (Alexy Khrabrov)
Date: Sun, 10 Sep 2006 14:35:30 -0700
Subject: [R-SIG-Finance] Beginning R metrics
Message-ID: <7c737f300609101435g15104e76j20eb0415d588960a@mail.gmail.com>

Trying to follow the examples in User Guides, I've found a bit more
step-by-step instruction is needed.  Apparently, xmp* functions load
with fBasics package.   However, trying to load them with xmpfBasics()
menu shows the menu which is not self-explanatory -- a mapping to the
User Guide is desirable.

Generally, a concise list of actions would be very helpful -- starting
with "install *these* packages from CRAN" -- exactly which ones and
how -- then, a HOWTO on following the Guides.  A way to list/check the
list of examples would help.

Cheers,
Alexy


From edd at debian.org  Mon Sep 11 01:02:20 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 10 Sep 2006 18:02:20 -0500
Subject: [R-SIG-Finance] Beginning R metrics
In-Reply-To: <7c737f300609101435g15104e76j20eb0415d588960a@mail.gmail.com>
References: <7c737f300609101435g15104e76j20eb0415d588960a@mail.gmail.com>
Message-ID: <17668.39292.59028.755693@basebud.nulle.part>


On 10 September 2006 at 14:35, Alexy Khrabrov wrote:
| Trying to follow the examples in User Guides, I've found a bit more
| step-by-step instruction is needed.  Apparently, xmp* functions load
| with fBasics package.   However, trying to load them with xmpfBasics()
| menu shows the menu which is not self-explanatory -- a mapping to the
| User Guide is desirable.
| 
| Generally, a concise list of actions would be very helpful -- starting
| with "install *these* packages from CRAN" -- exactly which ones and
| how -- then, a HOWTO on following the Guides.  A way to list/check the
| list of examples would help.

Well, R itself comes with six manuals; to learn R you could consider the
introductory manual, FAQ, etc.  As for Rmetrics, it also comes with a rather
extensive and unparallel set of manuals for each module.

And consider that almost every function is documented with examples, so try 
	> help("xmpFoo")
	> example("xmpFoo")
for whichever function xmpFoo you are interested.

Lastly, all of R is written by volunteer contributors. So with all due
respect, if you consider something is missing, maybe a good way to fix it
would be to draft such a helpful document and to place on a website somewhere
with requests for comments.

Regards,  Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From deliverable at gmail.com  Mon Sep 11 01:49:18 2006
From: deliverable at gmail.com (Alexy Khrabrov)
Date: Sun, 10 Sep 2006 16:49:18 -0700
Subject: [R-SIG-Finance] Beginning R metrics
In-Reply-To: <17668.39292.59028.755693@basebud.nulle.part>
References: <7c737f300609101435g15104e76j20eb0415d588960a@mail.gmail.com>
	<17668.39292.59028.755693@basebud.nulle.part>
Message-ID: <7c737f300609101649n9adf628r22ba8b3c539e5c99@mail.gmail.com>

In fact, I was asking for a way to sync the examples in the User
Guides with the packages themselves  -- what is the intended way to
get the examples for fBasics shown in the fBasics.pdf in the sequence
from the text?  I.e., in fBasics.pdf, on page 32, there's sample code
-- I wonder how one would load/see it, if it's available?

Cheers,
Alexy


From edd at debian.org  Mon Sep 11 03:05:26 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 10 Sep 2006 20:05:26 -0500
Subject: [R-SIG-Finance] Beginning R metrics
In-Reply-To: <7c737f300609101649n9adf628r22ba8b3c539e5c99@mail.gmail.com>
References: <7c737f300609101435g15104e76j20eb0415d588960a@mail.gmail.com>
	<17668.39292.59028.755693@basebud.nulle.part>
	<7c737f300609101649n9adf628r22ba8b3c539e5c99@mail.gmail.com>
Message-ID: <17668.46678.283180.225161@basebud.nulle.part>


On 10 September 2006 at 16:49, Alexy Khrabrov wrote:
| In fact, I was asking for a way to sync the examples in the User
| Guides with the packages themselves  -- what is the intended way to
| get the examples for fBasics shown in the fBasics.pdf in the sequence
| from the text?  I.e., in fBasics.pdf, on page 32, there's sample code
| -- I wonder how one would load/see it, if it's available?

For the particular case, I think we have to defer to Rmetrics author on
that. Looking at the sources of fBasics here, I see that it has a demo/
directory so that you have the demo() commands as well -- try those.

For the general case, one would have to invent a way to link documentation
and examples in a generic fashion.  

Now, it turns out that R actually solved that for the particular case of
vignette() documentation.  For a given vignette foo, e.g. what you'd see by
issueing
	> vignette("foo")
where my favourite example is probably the vignette of the zoo package, you
also get code included in the vignette via
	> edit(vignette("foo"))

In other words, for my zoo example it is 
	> library(zoo)			## load the library
	> vignette("zoo")		## display the pdf document
	> edit(vignette("zoo"))		## extracts and displays the code

Now, for your particular case of fBasics et al, you are out of luck.  Dr
Wuertz started the Rmetrics project way before R matured to the point that it
could support all of Rmetrics; luckily we are now at a point where R "can do"
Rmetrics.  

However, Rmetrics' documentation precedes the vignette-style documentation
mechanism that was added to R in the last few years.  So I am afraid you're
back to doing the extraction the manual way. Sorry!

Hope this help,  Dirk 

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From prossi79.r at gmail.com  Mon Sep 11 10:42:38 2006
From: prossi79.r at gmail.com (Paolo Rossi)
Date: Mon, 11 Sep 2006 10:42:38 +0200
Subject: [R-SIG-Finance] Passing dates from Excel to R using RExcel functions
Message-ID: <9e8f1a130609110142hc59c8a9w55d5d24763f36837@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060911/8d93c9c8/attachment.pl 

From brian at braverock.com  Mon Sep 11 13:58:51 2006
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 11 Sep 2006 06:58:51 -0500
Subject: [R-SIG-Finance] Passing dates from Excel to R using RExcel
	functions
In-Reply-To: <9e8f1a130609110142hc59c8a9w55d5d24763f36837@mail.gmail.com>
References: <9e8f1a130609110142hc59c8a9w55d5d24763f36837@mail.gmail.com>
Message-ID: <200609110658.51813.brian@braverock.com>

On Monday 11 September 2006 03:42, Paolo Rossi wrote:
> I have the following problem using RExcel. When I use the VBA function
> "putarray" to put a vector of dates from Excel to R, I don't see the
> dates in R but the serial number converted to text. I don't know, then,
> how to trasform them in dates again...
>
> could anyone help me?

Excel denominates dates in days since 1 Jan 1970.  You need to convert 
those to a unix timestamp, and then convert them to a Date class in R.

I've never specifically had this problem directly in R, but I do have some 
PHP code that you should be able to adapt to R to solve your problem.

if ($passed_date<2958465) { //this is an MS Excel date
    //MS Excel denominates in days since 1 Jan 1970
    $ts = ($passed_date - 25568) * 86400;
    $formatted_date  = date ( 'Y-m-d' , $ts);
}


Hope this helps,

   - Brian


From ggrothendieck at gmail.com  Mon Sep 11 15:31:05 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 11 Sep 2006 09:31:05 -0400
Subject: [R-SIG-Finance] Passing dates from Excel to R using RExcel
	functions
In-Reply-To: <9e8f1a130609110142hc59c8a9w55d5d24763f36837@mail.gmail.com>
References: <9e8f1a130609110142hc59c8a9w55d5d24763f36837@mail.gmail.com>
Message-ID: <971536df0609110631r3e3f78f0ud1fc45d296434046@mail.gmail.com>

There is some discussion of Excel dates in the R Help article
in R News 4/1.

On 9/11/06, Paolo Rossi <prossi79.r at gmail.com> wrote:
> Good morning all,
>
> I have the following problem using RExcel. When I use the VBA function
> "putarray" to put a vector of dates from Excel to R, I don't see the dates
> in R but the serial number converted to text. I don't know, then, how to
> trasform them in dates again...
>
> could anyone help me?
>
> thank you very much
>
> Paolo Rossi
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From nyamada at millburncorp.com  Mon Sep 11 19:00:42 2006
From: nyamada at millburncorp.com (Norman Yamada)
Date: Mon, 11 Sep 2006 13:00:42 -0400
Subject: [R-SIG-Finance] New York-based hedge fund seeks senior application
	developer with R experience
Message-ID: <37FAAFB6-6E2D-4FCC-979E-D8171B45D94A@millburncorp.com>

Apologies if this isn't the right place to post this.

Wanted: Experienced senior application developer with substantial  
experience with Java and R to take over development and maintenance  
of a real-time commodity and currency trading system for hedge fund.   
A strong background in UNIX required. Strong knowledge of SQL needed  
(i.e., experience writing stored procedures and triggers in  
PostgreSQL, Oracle or Informix). Knowledge of financial markets a  
plus. Knowledge of the RMetrics library and background in  
quantitative finance helpful, but not absolutely required. A creative  
mind is necessary. Salary highly competitive with excellent benefits.  
Please email resume and contact information to nyamada AT  
millburncorp DOT com. Again, my apologies if this isn't the right  
place to post; I'll gladly crosspost this in what ever mailing list  
you recommend.

Norman Yamada


From kriskumar at earthlink.net  Thu Sep 14 02:40:09 2006
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Wed, 13 Sep 2006 20:40:09 -0400
Subject: [R-SIG-Finance] Passing dates from Excel to R using
	RExcel	functions
In-Reply-To: <971536df0609110631r3e3f78f0ud1fc45d296434046@mail.gmail.com>
References: <9e8f1a130609110142hc59c8a9w55d5d24763f36837@mail.gmail.com>
	<971536df0609110631r3e3f78f0ud1fc45d296434046@mail.gmail.com>
Message-ID: <4508A4E9.8060202@earthlink.net>

I have had no problems passing date in as dataframes back and forth and 
is now my prefered method..  No need to even do as.Date or know
all the Posixct kungfu although it is worth knowing all that..
[Instead of  rput I use rputdataframe and this works like a charm with 
zoo or the new Diethelm's timeSeries class .]


Best,
Krishna

Gabor Grothendieck wrote:

>There is some discussion of Excel dates in the R Help article
>in R News 4/1.
>
>On 9/11/06, Paolo Rossi <prossi79.r at gmail.com> wrote:
>  
>
>>Good morning all,
>>
>>I have the following problem using RExcel. When I use the VBA function
>>"putarray" to put a vector of dates from Excel to R, I don't see the dates
>>in R but the serial number converted to text. I don't know, then, how to
>>trasform them in dates again...
>>
>>could anyone help me?
>>
>>thank you very much
>>
>>Paolo Rossi
>>
>>       [[alternative HTML version deleted]]
>>
>>_______________________________________________
>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>
>>    
>>
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From stefano.iacus at unimi.it  Thu Sep 14 12:28:15 2006
From: stefano.iacus at unimi.it (stefano iacus)
Date: Thu, 14 Sep 2006 19:28:15 +0900
Subject: [R-SIG-Finance] bessel functions
Message-ID: <33505FF1-C142-4A0A-98FE-B71376CE8307@unimi.it>

Hi,

does anyone know about a reliable  algorithm to evaluate the  
exponentially rescaled bessel function of first find, any order, real  
argument?

what I need is the equivalent of R function

besselI(x, nu, expon.scaled = TRUE)

for large real x (say 5000) and large nu (say 10 to 30)

The R implementation does not satisfy my needs because, for example,


 > besselI(5539.947191,11,exp=TRUE)
[1] NaN

but if you ask Mathematica you get this number: 0.00530180603357

To be more specific I need this to evaluate the likelihood of the CIR  
model. I know there are many different estimators for the CIR but I  
need to evaluate the likelihood.

Googling around did not help me.

thanks in advance.

stefano


From mel at altk.com  Thu Sep 14 13:01:13 2006
From: mel at altk.com (mel)
Date: Thu, 14 Sep 2006 13:01:13 +0200
Subject: [R-SIG-Finance] bessel functions
In-Reply-To: <33505FF1-C142-4A0A-98FE-B71376CE8307@unimi.it>
References: <33505FF1-C142-4A0A-98FE-B71376CE8307@unimi.it>
Message-ID: <45093679.1080402@altk.com>

stefano iacus a ?crit :

> does anyone know about a reliable  algorithm to evaluate the  
> exponentially rescaled bessel function of first find, any order, real  
> argument?

try http://library.lanl.gov/numerical/bookcpdf.html
hih


From maechler at stat.math.ethz.ch  Fri Sep 15 17:00:13 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Sep 2006 17:00:13 +0200
Subject: [R-SIG-Finance] bessel functions
In-Reply-To: <33505FF1-C142-4A0A-98FE-B71376CE8307@unimi.it>
References: <33505FF1-C142-4A0A-98FE-B71376CE8307@unimi.it>
Message-ID: <17674.49149.252234.281982@stat.math.ethz.ch>

Hi Stefano,  et al.

I have been too busy to get this earlier,

>>>>> "Stefano" == stefano iacus <stefano.iacus at unimi.it>
>>>>>     on Thu, 14 Sep 2006 19:28:15 +0900 writes:

    Stefano> Hi, does anyone know about a reliable algorithm to
    Stefano> evaluate the exponentially rescaled bessel function
    Stefano> of first find, any order, real argument?

    Stefano> what I need is the equivalent of R function

    Stefano> besselI(x, nu, expon.scaled = TRUE)

    Stefano> for large real x (say 5000) and large nu (say 10 to
    Stefano> 30)

    Stefano> The R implementation does not satisfy my needs
    Stefano> because, for example,


    >> besselI(5539.947191,11,exp=TRUE)
    Stefano> [1] NaN

    Stefano> but if you ask Mathematica you get this number:
    Stefano> 0.00530180603357

    Stefano> To be more specific I need this to evaluate the
    Stefano> likelihood of the CIR model. I know there are many
    Stefano> different estimators for the CIR but I need to
    Stefano> evaluate the likelihood.

    Stefano> Googling around did not help me.

But  RSiteSearch("bessel")  does immediately help

The answer is *not* to use numerical recipes 
(they have ``source'' but have heavily restricted copyrights;
 and also very often, their code is far suboptimal) as 'mel'
indicated, but  for "special math functions" like these,
rather look at the GSL = GNU Scientific Library.

It has all kinds of Bessel functions and uses newer versions of
the algorithm than R, since
--- as we have the 'gsl' package on CRAN - thanks to Robin Hankin ---
you get the desired result from

 > library(gsl)
 > bessel_In_scaled(x= 5539.947191, n= 11)
 [1] 0.005301806


Martin Maechler


From stefano.iacus at unimi.it  Fri Sep 15 17:27:42 2006
From: stefano.iacus at unimi.it (stefano iacus)
Date: Sat, 16 Sep 2006 00:27:42 +0900
Subject: [R-SIG-Finance] bessel functions
In-Reply-To: <17674.49149.252234.281982@stat.math.ethz.ch>
References: <33505FF1-C142-4A0A-98FE-B71376CE8307@unimi.it>
	<17674.49149.252234.281982@stat.math.ethz.ch>
Message-ID: <9A5746B3-1BA9-4749-B836-7ED0BD6A58E3@unimi.it>


On 16/set/06, at 00:00, Martin Maechler wrote:

> Hi Stefano,  et al.
>
> I have been too busy to get this earlier,
>
>>>>>> "Stefano" == stefano iacus <stefano.iacus at unimi.it>
>>>>>>     on Thu, 14 Sep 2006 19:28:15 +0900 writes:
>
>     Stefano> Hi, does anyone know about a reliable algorithm to
>     Stefano> evaluate the exponentially rescaled bessel function
>     Stefano> of first find, any order, real argument?
>
>     Stefano> what I need is the equivalent of R function
>
>     Stefano> besselI(x, nu, expon.scaled = TRUE)
>
>     Stefano> for large real x (say 5000) and large nu (say 10 to
>     Stefano> 30)
>
>     Stefano> The R implementation does not satisfy my needs
>     Stefano> because, for example,
>
>
>>> besselI(5539.947191,11,exp=TRUE)
>     Stefano> [1] NaN
>
>     Stefano> but if you ask Mathematica you get this number:
>     Stefano> 0.00530180603357
>
>     Stefano> To be more specific I need this to evaluate the
>     Stefano> likelihood of the CIR model. I know there are many
>     Stefano> different estimators for the CIR but I need to
>     Stefano> evaluate the likelihood.
>
>     Stefano> Googling around did not help me.
>
> But  RSiteSearch("bessel")  does immediately help
>
> The answer is *not* to use numerical recipes
> (they have ``source'' but have heavily restricted copyrights;
>  and also very often, their code is far suboptimal) as 'mel'
> indicated, but  for "special math functions" like these,
> rather look at the GSL = GNU Scientific Library.

I agree, numerical recipies is not the solution. Thanks for pointing  
me to gsl !!!

stefano

>
> It has all kinds of Bessel functions and uses newer versions of
> the algorithm than R, since
> --- as we have the 'gsl' package on CRAN - thanks to Robin Hankin ---
> you get the desired result from
>
>> library(gsl)
>> bessel_In_scaled(x= 5539.947191, n= 11)
>  [1] 0.005301806
>
>
> Martin Maechler


From spencer.graves at pdf.com  Fri Sep 15 20:03:25 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 15 Sep 2006 20:03:25 +0200
Subject: [R-SIG-Finance] Examples from Rmetrics User Guides
In-Reply-To: <7c737f300609070020h5b77ecf8m15dbf0701ab122ff@mail.gmail.com>
References: <7c737f300609070020h5b77ecf8m15dbf0701ab122ff@mail.gmail.com>
Message-ID: <450AEAED.7080909@pdf.com>

      Have you looked at the contents of, for example, 
~R\library\fBasics?  If you've looked there and can't find what you 
want, could you please describe more explicitly what you tried and what 
you want?  Which example(s) most interest you?  To which user guide 
f*.pdf are you referring?  Where did you get it?  I have some experience 
with finding things crudely like that, but I don't want to spend a whole 
lot more time on this without some further confidence that you might 
actually find the result helpful. 

      Spencer Graves

A Curious Mind wrote:
> Is there a download with the examples -- xmp...() and the datasets
> from the user guides, f*.pdf, or are they a part of Rmetrics?
>
> Cheers,
> Alexy
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From gyadav at ccilindia.co.in  Sat Sep 16 06:00:50 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Sat, 16 Sep 2006 09:30:50 +0530
Subject: [R-SIG-Finance] regarding chaos
In-Reply-To: <33505FF1-C142-4A0A-98FE-B71376CE8307@unimi.it>
Message-ID: <OF94065FF0.23F0F354-ON652571EB.0015CCA9-652571B3.001629B3@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20060916/dcb57435/attachment.pl 

From wuertz at itp.phys.ethz.ch  Sun Sep 17 02:37:08 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 17 Sep 2006 02:37:08 +0200
Subject: [R-SIG-Finance] bessel functions
In-Reply-To: <17674.49149.252234.281982@stat.math.ethz.ch>
References: <33505FF1-C142-4A0A-98FE-B71376CE8307@unimi.it>
	<17674.49149.252234.281982@stat.math.ethz.ch>
Message-ID: <450C98B4.508@itp.phys.ethz.ch>

You can just  use for nu > 10 and x > 1000

x = 1000
nu = 10
mu = 4*nu^2

    A1 =  1
    A2 = A1 * (mu-  1) / (1 * (8*x))
    A3 = A2 * (mu-  9) / (2 * (8*x))
    A4 = A3 * (mu- 25) / (3 * (8*x))
    A5 = A4 * (mu- 49) / (4 * (8*x))
    A6 = A5 * (mu- 81) / (5 * (8*x))
    A7 = A6 * (mu-121) / (6 * (8*x))
   
    print(1/sqrt(2*pi*x) * (A1 - A2 + A3 - A4 + A5 - A6 + A7), digits = 12)
    # [1] 0.0120015950241
    print(besselI(x, nu, TRUE), digits = 12)
    # [1] 0.0120015950241
   
 
x = 5539.947191
nu = 11
mu = 4*nu^2

    A1 =  1
    A2 = A1 * (mu-  1) / (1 * (8*x))
    A3 = A2 * (mu-  9) / (2 * (8*x))
    A4 = A3 * (mu- 25) / (3 * (8*x))
    A5 = A4 * (mu- 49) / (4 * (8*x))
    A6 = A5 * (mu- 81) / (5 * (8*x))
    A7 = A6 * (mu-121) / (6 * (8*x))
   
    print(1/sqrt(2*pi*x) * (A1 - A2 + A3 - A4 + A5 - A6 + A7), digits = 12)
    # 0.00530180603357
    # BesselI fails
    # Mathematica:
    # 0.00530180603357
    # Maple:
    # 0.005301806035

regards
Diethelm Wuertz




Martin Maechler wrote:

>Hi Stefano,  et al.
>
>I have been too busy to get this earlier,
>
>  
>
>>>>>>"Stefano" == stefano iacus <stefano.iacus at unimi.it>
>>>>>>    on Thu, 14 Sep 2006 19:28:15 +0900 writes:
>>>>>>            
>>>>>>
>
>    Stefano> Hi, does anyone know about a reliable algorithm to
>    Stefano> evaluate the exponentially rescaled bessel function
>    Stefano> of first find, any order, real argument?
>
>    Stefano> what I need is the equivalent of R function
>
>    Stefano> besselI(x, nu, expon.scaled = TRUE)
>
>    Stefano> for large real x (say 5000) and large nu (say 10 to
>    Stefano> 30)
>
>    Stefano> The R implementation does not satisfy my needs
>    Stefano> because, for example,
>
>
>    >> besselI(5539.947191,11,exp=TRUE)
>    Stefano> [1] NaN
>
>    Stefano> but if you ask Mathematica you get this number:
>    Stefano> 0.00530180603357
>
>    Stefano> To be more specific I need this to evaluate the
>    Stefano> likelihood of the CIR model. I know there are many
>    Stefano> different estimators for the CIR but I need to
>    Stefano> evaluate the likelihood.
>
>    Stefano> Googling around did not help me.
>
>But  RSiteSearch("bessel")  does immediately help
>
>The answer is *not* to use numerical recipes 
>(they have ``source'' but have heavily restricted copyrights;
> and also very often, their code is far suboptimal) as 'mel'
>indicated, but  for "special math functions" like these,
>rather look at the GSL = GNU Scientific Library.
>
>It has all kinds of Bessel functions and uses newer versions of
>the algorithm than R, since
>--- as we have the 'gsl' package on CRAN - thanks to Robin Hankin ---
>you get the desired result from
>
> > library(gsl)
> > bessel_In_scaled(x= 5539.947191, n= 11)
> [1] 0.005301806
>
>
>Martin Maechler
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From wuertz at itp.phys.ethz.ch  Sun Sep 17 10:27:13 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 17 Sep 2006 10:27:13 +0200
Subject: [R-SIG-Finance] [R] regarding chaos
In-Reply-To: <OF94065FF0.23F0F354-ON652571EB.0015CCA9-652571B3.001629B3@ccilindia.co.in>
References: <OF94065FF0.23F0F354-ON652571EB.0015CCA9-652571B3.001629B3@ccilindia.co.in>
Message-ID: <450D06E1.80408@itp.phys.ethz.ch>

gyadav at ccilindia.co.in wrote:

>hi all,
>
>I have a simple question that does power spectral analysis related to 
>capacity dimension, information dimension, lyapunov exponent, hurst 
>exponent.
>  
>
Hurst Aanalysis is implemented in fSeries from Rmetrics.
The following functions are available:

###############################################################################
# PART I: Simulation
# FUNCTIONS:            FRACTIONAL BROWNIAN MOTION:
#  fbmSim                Generates fractional Brownian motion
#                         Available Methods:
#                          Numerical approximation of the stochastic 
integral
#                          Choleki's decomposition of the covariance matrix
#                          Method of Levinson
#                          Method of Wood and Chan
#                          Wavelet synthesis
# FUNCTIONS:            FRACTIONAL GAUSSIAN NOISE:
#  fgnSim                Generates fractional Gaussian noise
#                         Available Methods:
#                          Durbin's Method
#                          Paxson's Method
#                          Beran's Method
# FUNCTIONS:            FARIMA PROCESS:
#  farimaSim             Generates FARIMA time series process
################################################################################


################################################################################
# PART II: Reimplemented functions from Beran's SPlus Scripts
# FUNCTIONS:            DESCRIPTION:
#  farimaTrueacf         Returns FARMA true autocorrelation function
#  farimaTruefft         Returns FARMA true fast Fourier transform
#  fgnTrueacf            Returns FGN true autocorrelation function
#  fgnTruefft            Returns FGN true fast Fourier transform
# FUNCTIONS:            WHITTLE ESTIMATOR:
#  whittleFit            Whittle Estimator
################################################################################


################################################################################
# PART III: Reimplemented SPlus/C functions from
#   Taqqu M.S, Teverovsky V, Willinger W.
#   Estimators for Long-Range Dependence: An Empirical Study
#   Fractals, Vol 3, No. 4, 785-788, 1995
# FUNCTIONS:          HURST EXPONENT:
#  'fHURST'            S4 Class Representation
#   print.fHURST        S3 Print Method
#   plot.fHURST         S3 Plot Method
#  aggvarFit               Aggregated variance method
#  diffvarFit              Differenced aggregated variance method
#  absvalFit               Absolute values (moments) method
#  higuchiFit              Higuchi's method
#  pengFit                 Peng's or Residuals of Regression method
#  rsFit                   R/S method
#  perFit                  Periodogram and cumulated periodogram method
#  boxperFit               Boxed (modified) peridogram method
#  whittleFit              Whittle estimator -> PART II
#  hurstSlider         Hurst Slider
################################################################################


Diethelm Wuertz

>If yes then please show me the way. I am newbie in the world of chaos.
>
>   Sayonara With Smile & With Warm Regards :-)
>
>  G a u r a v   Y a d a v
>  Senior Executive Officer,
>  Economic Research & Surveillance Department,
>  Clearing Corporation Of India Limited.
>
>  Address: 5th, 6th, 7th Floor, Trade Wing 'C',  Kamala City, S.B. Marg, 
>Mumbai - 400 013
>  Telephone(Office): - +91 022 6663 9398 ,  Mobile(Personal) (0)9821286118
>  Email(Office) :- gyadav at ccilindia.co.in ,  Email(Personal) :- 
>emailtogauravyadav at gmail.com
>
>
>============================================================================================
>DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>  
>


From frainj at tcd.ie  Thu Sep 21 13:09:03 2006
From: frainj at tcd.ie (John C. Frain)
Date: Thu, 21 Sep 2006 12:09:03 +0100
Subject: [R-SIG-Finance] problem aggregating daily series
Message-ID: <1158836943.451272cf0ae00@mymail.tcd.ie>

I am having problems converting daily returns to monthly.  The output of one of
the many various attempts that I have made is given below.  daily.csv is a csv
file with dates -in the format "$Y-$m-%d" with daily returns on 29 equities in
the columns.

I do not appear to be able to generate correct results.  In the final example
below the aggregated data for October to February are correct.  The aggregated
data for March to September exclude the last day of the month and give the
wrong answer (unless the return on that last day is zero or the last day is at
the weekend).  It is likely that the problem is caused by my not accounting for
summer time correctly.  I am probably missing something obvious and would be
delighted if someone could enlighten me.

<to = timeLastDayInMonth(as.character.timeDate(from))> as used below is one of
several formulations that give an error message with the applySeries. In
another formulations of the to dates adding or subtracting a second removed the
error.  My work around for this problem is to code the aggregation directly
using the applySeries function as a model.  Any suggestions would be welcome.

Ialso note that in example fCalender\demo\xmpZWChapter02.R in the Rmetrics
distribution Diethelm Wuertz mentions two arguements "include.from" and
"include.to" which determines if the start or end/date/time are included or not
with a default of including the start and excluding the end.  (Rmetrics
equivalent of material p. 34 of Zivot and Wang).  Including a logical
"include.to=TRUE" causes an error.  I have updated my fBasics and fCalendar
libraries recently.  Again any comments would be welcome

The output (with some material removed) of my most recent program is below.
Conversions were checked directly in R using instructions such as

sum(cutSeries(T.ts3,"1990-01-01","1990-01-31")@Data)

for a variety of series and then verified in RATS.

The output (with some material removed) of my most recent program is below.

R : Copyright 2006, The R Foundation for Statistical Computing
Version 2.3.1 (2006-06-01)
ISBN 3-900051-07-0


Loading required package: datasets
Loading required package: utils
Loading required package: grDevices
Loading required package: graphics
Loading required package: stats
Loading required package: methods
Loading required package: tcltk
Loading Tcl/Tk interface ... done
Loading required package: R2HTML
Loading required package: svMisc
Loading required package: svIO
Loading required package: svViews
During startup - Warning message:
use of NULL environment is deprecated
>
> library(fBasics)

Rmetrics, (C) 1999-2005, Diethelm Wuertz, GPL
fBasics: Markets and Basic Statistics
> library(fCalendar)

Rmetrics, (C) 1999-2005, Diethelm Wuertz, GPL
fCalendar: Time, Date and Calendar Tools
> myFinCenter="Europe/Dublin"
> #
> # read in data from csv file to timeseries object
> daily.ts=read.timeSeries(file="D:\\equities\\yahoo\\R\\daily_data\\daily.csv",
+   zone="GMT",FinCenter=myFinCenter,sep=",")
> #head(daily.ts)
> #tail(daily.ts)
> #
> #set up from and to series for aggregation
> y = c(rep(1990,12),rep(1991,12),rep(1992,12),rep(1993,12),rep(1994,12),
+ rep(1995,12),rep(1996,12),rep(1997,12),rep(1998,12),rep(1999,12),rep(2000,
12),
+ rep(2001, 12),rep(2002, 12),rep(2003, 12),rep(2004, 12),rep(2005, 12),
+   rep(2006, 9))
>   m = c(rep(1:12,16),1:9)
> from = timeCalendar(y, m,FinCenter=myFinCenter)[1:200]
> to = timeLastDayInMonth(as.character.timeDate(from))
> data.frame(from, to)[1:5,]
                 from                  to
1 1990-01-01 00:00:00 1990-01-31 00:00:00
2 1990-02-01 00:00:00 1990-02-28 00:00:00
3 1990-03-01 00:00:00 1990-03-31 00:00:00
4 1990-04-01 00:00:00 1990-04-30 01:00:00
5 1990-05-01 01:00:00 1990-05-31 01:00:00
> ndays=from at Dim
> data.frame(from,to)[(ndays-5):ndays,]
                   from                  to
195 2006-03-01 00:00:00 2006-03-31 00:00:00
196 2006-04-01 00:00:00 2006-04-30 01:00:00
197 2006-05-01 01:00:00 2006-05-31 01:00:00
198 2006-06-01 01:00:00 2006-06-30 01:00:00
199 2006-07-01 01:00:00 2006-07-31 01:00:00
200 2006-08-01 01:00:00 2006-08-31 01:00:00
>     # Aggregate to Monthly Means:
> monthly.ts=applySeries(daily.ts, from = from, to = to, FUN = colSums)
Error in if (timeTest == 0) iso.format = "%Y-%m-%d" :
        missing value where TRUE/FALSE needed
> #
>
write.csv(file="D:\\equities\\yahoo\\R\\daily_data\\monthly_normality.csv",monthly.ts at Data)
Error in inherits(x, "data.frame") : object "monthly.ts" not found
> #
> # Direct implementation apply Series
> #
> #
>     j.pos = as.POSIXct(seriesPositions(daily.ts))
>     j.from =as.POSIXct(from)
>     j.to = as.POSIXct(to)
>     y = daily.ts at Data
>     pos = seriesPositions(daily.ts)
>     rowNames = rownames(daily.ts at Data)
>     rowBind = NULL
>     for (i in 1:from at Dim) {
+         test = (j.pos >= j.from[i] & j.pos <= j.to[i])
+         cutted = as.matrix(y[test, ])
+         ans = colSums(cutted)
+ #        print(dim(ans))
+         rowBind = rbind(rowBind, ans)
+ #        print(dim(rowBind))
+     }
>     rownames(rowBind) = as.character(to)
>         units = daily.ts at units
>
> monthly.ts=timeSeries(data = rowBind, charvec = as.character(to), units =
units,
+          zone = "GMT", FinCenter = daily.ts at FinCenter,
+         title = daily.ts at title, documentation = daily.ts at documentation)
>
> write.csv(file="D:\\equities\\yahoo\\R\\agreg\\monthly.csv",monthly.ts at Data)
>
>

John C. Frain.
Economics Department
Trinity College Dublin
Dublin 2

www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie


From joe-byers at utulsa.edu  Thu Sep 21 19:50:19 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Thu, 21 Sep 2006 12:50:19 -0500
Subject: [R-SIG-Finance] Help understanding how nls parses the formula
 argument to estimate the model
Message-ID: <eeujbo$ooo$1@sea.gmane.org>

I could use some help understanding how nls parses the formula argument
to a model.frame and estimates the model.  I am trying to utilize the
functionality of the nls formula argument to modify garchFit() to handle
other variables in the mean equation besides just an arma(u,v)
specification.

My nonlinear model is
      y<-nls(t~a*sin(w*2*pi/365*id+p)+b*id+int,data=t1,
	start=list(w=.5,a=.1,p=.5,b=init.y$coef[2],int=init.y$coef[1] ),
	control=list(maxiter=1000000,minFactor=1e-18))
where t is change in daily temperatures, id is just a time trend and the
a*sin is a one year fourier series.

I have tried to debug the nls code using the following code
t1<-data.frame(t=as.vector(x),id=index(x))
data=t1;
formula <- as.formula(t ~ a *sin(w *2* pi/365 * id + p) + b * id + int);
      varNames <- all.vars(formula)
      algorithm<-'default';
      mf <- match.call(definition=nls,expand.dots=FALSE,
      call('nls',formula, data=parent.frame(),start,control = nls.control(),
      algorithm = "default", trace = FALSE,
      subset, weights, na.action, model = FALSE, lower = -Inf,
      upper = Inf));
      mWeights<-F;#missing(weights);
	start=list(w=.5,a=.1,p=.5,b=init.y$coef[2],int=init.y$coef[1] );
      pnames <- names(start);
       varNames <- varNames[is.na(match(varNames, pnames, nomatch = NA))]

	varIndex <- sapply(varNames,
		function(varName, data, respLength) {
          	length(eval(as.name(varName), data))%%respLength == 0},
          	 data, length(eval(formula[[2]], data))
          );
	mf$formula <- as.formula(paste("~", paste(varNames[varIndex],
          collapse = "+")), env = environment(formula));
	mf$start <- NULL;mf$control <- NULL;mf$algorithm <- NULL;
	mf$trace <- NULL;mf$model <- NULL;
      mf$lower <- NULL;mf$upper <- NULL;
      mf[[1]] <- as.name("model.frame");
      mf<-evalq(mf,data);
      n<-nrow(mf)
      mf<-as.list(mf);
      wts <- if (!mWeights)
          model.weights(mf)
      else rep(1, n)
      if (any(wts < 0 | is.na(wts)))
          stop("missing or negative weights not allowed")

      m <- switch(algorithm,
      		plinear = nlsModel.plinear(formula, mf, start, wts),
      		port = nlsModel(formula, mf, start, wts, upper),
      		nlsModel(formula, mf, start, wts));

I am struggling with the environment issues associated with performing
these model.frame operations with the eval functions.

thank you


From john.marsland at cantab.net  Fri Sep 22 00:22:59 2006
From: john.marsland at cantab.net (John Marsland)
Date: Thu, 21 Sep 2006 22:22:59 +0000 (UTC)
Subject: [R-SIG-Finance]
	=?utf-8?q?UKSIP_-_Quantitative_investment_profess?=
	=?utf-8?q?ionals=09meetings?=
References: <17655.2896.541826.251785@basebud.nulle.part>
Message-ID: <loom.20060922T001956-523@post.gmane.org>

I would like to draw your attention to the following two events. The first event
is applicable to both quants and non-quants who have an interest in the state of
the quantitative investment landscape - this will not be a technical
presentation. It would be of particular interest to investment consultants as
well as quantitative investment managers. The second event is more technical in
nature, where we will take a look at risk management using stable, non-normal
distributions; these techniques are applicable to equity, credit and derivative
portfolios alike.
 
We have had lively debates and good feedback from our previous events, and I am
sure that these will continue that tradition. Please feel free to distribute
this email to any colleagues who you think might have an interest in this area.
 
Both members and non-members are most welcome to attend either event. CFA
Institute members will be automatically credited with PD points for these events.
 
Please refer to the UKSIP website for full details http://www.uksip.org 
 
Regards,
 
John Marsland, CFA
Chairman of the QUIPs (Quant Investment Professionals) Special Interest Group


UKSIP

UKSIP's Quantitative investment professionals specialist interest group has
organised two professional development events at which members will be able to
debate more specialist quantitative issues and also take advantage of the
opportunity to network with fellow "quant" experts.
Trends in equity portfolio modelling

Tuesday, 3 October, 2006
12.45pm for 13.00pm prompt start
Weaver Suite, UKSIP, 90 Basinghall Street, London EC2V 5AY

Speakers:

Sergio Focardi & Caroline Jonas, Partners, The Intertek Group
The 2006 Fabozzi/Intertek Survey Trends in Equity Portfolio Management looks at
trends in the role of modelling in equity portfolio management. It is based on
interviews and responses from 38 firms in North America and Western Europe
managing a total of ?3.3 trillion in equities. Relative to the period 2004-2005,
the 2006 survey finds that 1) the amount of equity assets under quantitative
management has increased, 2) a wider range of equity strategies with
quantitative methods has been implemented and 3) the automation of the equity
portfolio management process has continued to advance. The Survey also explores
model types currently in use and user experience with the models.

Click here for more information and to register (Members ?10.00; Non-members
?15.00) http://www.uksip.org/calendar/event_details.cfm?iEventID=138

In addition, The Intertek Group are organising a one-day seminar entitled
"Improving the Performance of Momentum & Reversal Strategies with Autoregressive
Models and Dynamic Risk Estimates" in London on Friday 20 October 2006. Details
are available on their website
http://www.theintertekgroup.com/training-Momentum-0610.html. UKSIP members are
offered a 25% on the fee for this course by quoting the code "UKSIP-QUIPS".
 
Risk management, optimisation and option pricing: stable non-Gaussian

Tuesday 7 November 2006
5.30pm for 6.00pm prompt start
Weaver Suite, UKSIP, 90 Basinghall Street, London EC2V 5AY

Speakers:

Boryana Racheva-Iotova, FinAnalytica
Zari Rachev, University of Karlsruhe and University of California, Santa Barbara

In this talk the speakers will introduce:

- An accurate approach to risk management, based on tail risk and stable
processes for asset returns
- A stable non-Gaussian approach to portfolio optimisation and ALM, yielding
increased risk adjusted returns
- A stable option pricing, capturing clustering of the volatility

Click here for more information and to register (Members ?25.00; Non-members
?35.00) http://www.uksip.org/calendar/event_details.cfm?iEventID=135


From deliverable at gmail.com  Sat Sep 23 00:42:38 2006
From: deliverable at gmail.com (Alexy Khrabrov)
Date: Fri, 22 Sep 2006 15:42:38 -0700
Subject: [R-SIG-Finance] Examples from Rmetrics User Guides
In-Reply-To: <450AEAED.7080909@pdf.com>
References: <7c737f300609070020h5b77ecf8m15dbf0701ab122ff@mail.gmail.com>
	<450AEAED.7080909@pdf.com>
Message-ID: <7c737f300609221542s469c6c2bubfce6fbf144121c1@mail.gmail.com>

I refer to the code examples in fBasics.pdf inlined in text -- would
be useful to link them to a specific step one should use at R prompt
to follow the User's Guide without retyping, if they're available...

Cheers,
Alexy

On 9/15/06, Spencer Graves <spencer.graves at pdf.com> wrote:
>       Have you looked at the contents of, for example,
> ~R\library\fBasics?  If you've looked there and can't find what you
> want, could you please describe more explicitly what you tried and what
> you want?  Which example(s) most interest you?  To which user guide
> f*.pdf are you referring?  Where did you get it?  I have some experience
> with finding things crudely like that, but I don't want to spend a whole
> lot more time on this without some further confidence that you might
> actually find the result helpful.
>
>       Spencer Graves
>
> A Curious Mind wrote:
> > Is there a download with the examples -- xmp...() and the datasets
> > from the user guides, f*.pdf, or are they a part of Rmetrics?
> >
> > Cheers,
> > Alexy
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance


From frainj at tcd.ie  Sat Sep 23 12:37:10 2006
From: frainj at tcd.ie (John C. Frain)
Date: Sat, 23 Sep 2006 11:37:10 +0100
Subject: [R-SIG-Finance] Examples from Rmetrics User Guides
In-Reply-To: <7c737f300609221542s469c6c2bubfce6fbf144121c1@mail.gmail.com>
References: <7c737f300609070020h5b77ecf8m15dbf0701ab122ff@mail.gmail.com>
	<450AEAED.7080909@pdf.com>
	<7c737f300609221542s469c6c2bubfce6fbf144121c1@mail.gmail.com>
Message-ID: <1159007830.45150e56514eb@mymail.tcd.ie>

These examples are available in the subdirectory C:\Program
Files\R\R-2.3.1\library\fBasics\R-ex\ on a standard R windows install.  For
example the file 012A-TimeSeriesImport.R contains the example on page 7 of the
fBasics manual. Other Rmetrics examples are in the cooresponding subdirectories
of the library subdirectory.  I would recommend that these be copied to a work
directory before working on them.

John


John C. Frain.
Economics Department
Trinity College Dublin
Dublin 2

www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie


Quoting Alexy Khrabrov <deliverable at gmail.com>:

> I refer to the code examples in fBasics.pdf inlined in text -- would
> be useful to link them to a specific step one should use at R prompt
> to follow the User's Guide without retyping, if they're available...
>
> Cheers,
> Alexy
>
> On 9/15/06, Spencer Graves <spencer.graves at pdf.com> wrote:
> >       Have you looked at the contents of, for example,
> > ~R\library\fBasics?  If you've looked there and can't find what you
> > want, could you please describe more explicitly what you tried and what
> > you want?  Which example(s) most interest you?  To which user guide
> > f*.pdf are you referring?  Where did you get it?  I have some experience
> > with finding things crudely like that, but I don't want to spend a whole
> > lot more time on this without some further confidence that you might
> > actually find the result helpful.
> >
> >       Spencer Graves
> >
> > A Curious Mind wrote:
> > > Is there a download with the examples -- xmp...() and the datasets
> > > from the user guides, f*.pdf, or are they a part of Rmetrics?
> > >
> > > Cheers,
> > > Alexy
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>


From wuertz at itp.phys.ethz.ch  Mon Sep 25 02:34:39 2006
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon, 25 Sep 2006 02:34:39 +0200
Subject: [R-SIG-Finance] problem aggregating daily series
In-Reply-To: <1158836943.451272cf0ae00@mymail.tcd.ie>
References: <1158836943.451272cf0ae00@mymail.tcd.ie>
Message-ID: <4517241F.5020605@itp.phys.ethz.ch>

Dear John Frain,
please apologize for any inconveniances caused by a bug in the 
applySeries() Function.
It doesent appear when you worj with daily records using "GMT" as FinCenter.
Try:

 myFinCenter = "GMT"
 tD = timeSequence("2006-01-01", length.out = 365)
 Data = matrix(rep(1, times = 365), ncol = 1)
 tS = timeSeries(Data, tD)
 from = timeCalendar(2006)
 to = timeLastDayInMonth(from)
 applySeries(tS, from, to, FUN = colSums, units = "N")

Now fix the bug by overwriting the last line in applySeries()
timeSeries(data = rowBind, charvec = as.character(to), units = units,
        # format = x at format, zone = "GMT", FinCenter = x at FinCenter,
        format = x at format, zone = x at FinCenter, FinCenter = x at FinCenter,
        title = x at title, documentation = x at documentation, ...)

Here is the full function:

.applySeries =
function (x, from = NULL, to = NULL, FUN = colAvgs, units = NULL,  ...)
{
    colNames = units
    if (class(x) != "timeSeries") stop("s is not a timeSeries object")
    fun = match.fun(FUN)
    j.pos = as.POSIXct(seriesPositions(x))
    j.from = as.POSIXct(from)
    j.to = as.POSIXct(to)
    y = x at Data
    pos = seriesPositions(x)
    rowNames = rownames(x at Data)
    rowBind = NULL
    for (i in 1:from at Dim) {
        test = (j.pos >= j.from[i] & j.pos <= j.to[i])
        cutted = as.matrix(y[test, ])
        ans = fun(cutted, ...)
        rowBind = rbind(rowBind, ans)
    }
    rownames(rowBind) = as.character(to)
    if (is.null(colNames)) {
        units = x at units
    }
    else {
        units = colNames
    }
    timeSeries(data = rowBind, charvec = as.character(to), units = units,
        # format = x at format, zone = "GMT", FinCenter = x at FinCenter,
        format = x at format, zone = x at FinCenter, FinCenter = x at FinCenter,
        title = x at title, documentation = x at documentation, ...)
}


# Now try:

myFinCenter = "Dublin"
tD = timeSequence("2006-01-01", length.out = 365, FinCenter = "Dublin")
Data = matrix(rep(1, times = 365), ncol = 1)
tS = timeSeries(Data, tD, zone = "Dublin", FinCenter = "Dublin")
from = timeCalendar(2006)
to = timeLastDayInMonth(from, FinCenter = "Dublin")
.applySeries(tS, from, to, FUN = colSums, units = "N")

The philosophy behind the timeSeries class is that you have two
time reference points: The first where the data were collected (zone) 
and the
second where the data willl be used (FinCenter). To circumvent 
inconsistencies
in the future, I think it will be the best to allow to specify in any 
case zone and
FinCenter through the argument list in any of the timeSeries class 
functions


Diethelm Wuertz






John C. Frain wrote:

>I am having problems converting daily returns to monthly.  The output of one of
>the many various attempts that I have made is given below.  daily.csv is a csv
>file with dates -in the format "$Y-$m-%d" with daily returns on 29 equities in
>the columns.
>
>I do not appear to be able to generate correct results.  In the final example
>below the aggregated data for October to February are correct.  The aggregated
>data for March to September exclude the last day of the month and give the
>wrong answer (unless the return on that last day is zero or the last day is at
>the weekend).  It is likely that the problem is caused by my not accounting for
>summer time correctly.  I am probably missing something obvious and would be
>delighted if someone could enlighten me.
>
><to = timeLastDayInMonth(as.character.timeDate(from))> as used below is one of
>several formulations that give an error message with the applySeries. In
>another formulations of the to dates adding or subtracting a second removed the
>error.  My work around for this problem is to code the aggregation directly
>using the applySeries function as a model.  Any suggestions would be welcome.
>
>Ialso note that in example fCalender\demo\xmpZWChapter02.R in the Rmetrics
>distribution Diethelm Wuertz mentions two arguements "include.from" and
>"include.to" which determines if the start or end/date/time are included or not
>with a default of including the start and excluding the end.  (Rmetrics
>equivalent of material p. 34 of Zivot and Wang).  Including a logical
>"include.to=TRUE" causes an error.  I have updated my fBasics and fCalendar
>libraries recently.  Again any comments would be welcome
>
>The output (with some material removed) of my most recent program is below.
>Conversions were checked directly in R using instructions such as
>
>sum(cutSeries(T.ts3,"1990-01-01","1990-01-31")@Data)
>
>for a variety of series and then verified in RATS.
>
>The output (with some material removed) of my most recent program is below.
>
>R : Copyright 2006, The R Foundation for Statistical Computing
>Version 2.3.1 (2006-06-01)
>ISBN 3-900051-07-0
>
>
>Loading required package: datasets
>Loading required package: utils
>Loading required package: grDevices
>Loading required package: graphics
>Loading required package: stats
>Loading required package: methods
>Loading required package: tcltk
>Loading Tcl/Tk interface ... done
>Loading required package: R2HTML
>Loading required package: svMisc
>Loading required package: svIO
>Loading required package: svViews
>During startup - Warning message:
>use of NULL environment is deprecated
>  
>
>>library(fBasics)
>>    
>>
>
>Rmetrics, (C) 1999-2005, Diethelm Wuertz, GPL
>fBasics: Markets and Basic Statistics
>  
>
>>library(fCalendar)
>>    
>>
>
>Rmetrics, (C) 1999-2005, Diethelm Wuertz, GPL
>fCalendar: Time, Date and Calendar Tools
>  
>
>>myFinCenter="Europe/Dublin"
>>#
>># read in data from csv file to timeseries object
>>daily.ts=read.timeSeries(file="D:\\equities\\yahoo\\R\\daily_data\\daily.csv",
>>    
>>
>+   zone="GMT",FinCenter=myFinCenter,sep=",")
>  
>
>>#head(daily.ts)
>>#tail(daily.ts)
>>#
>>#set up from and to series for aggregation
>>y = c(rep(1990,12),rep(1991,12),rep(1992,12),rep(1993,12),rep(1994,12),
>>    
>>
>+ rep(1995,12),rep(1996,12),rep(1997,12),rep(1998,12),rep(1999,12),rep(2000,
>12),
>+ rep(2001, 12),rep(2002, 12),rep(2003, 12),rep(2004, 12),rep(2005, 12),
>+   rep(2006, 9))
>  
>
>>  m = c(rep(1:12,16),1:9)
>>from = timeCalendar(y, m,FinCenter=myFinCenter)[1:200]
>>to = timeLastDayInMonth(as.character.timeDate(from))
>>data.frame(from, to)[1:5,]
>>    
>>
>                 from                  to
>1 1990-01-01 00:00:00 1990-01-31 00:00:00
>2 1990-02-01 00:00:00 1990-02-28 00:00:00
>3 1990-03-01 00:00:00 1990-03-31 00:00:00
>4 1990-04-01 00:00:00 1990-04-30 01:00:00
>5 1990-05-01 01:00:00 1990-05-31 01:00:00
>  
>
>>ndays=from at Dim
>>data.frame(from,to)[(ndays-5):ndays,]
>>    
>>
>                   from                  to
>195 2006-03-01 00:00:00 2006-03-31 00:00:00
>196 2006-04-01 00:00:00 2006-04-30 01:00:00
>197 2006-05-01 01:00:00 2006-05-31 01:00:00
>198 2006-06-01 01:00:00 2006-06-30 01:00:00
>199 2006-07-01 01:00:00 2006-07-31 01:00:00
>200 2006-08-01 01:00:00 2006-08-31 01:00:00
>  
>
>>    # Aggregate to Monthly Means:
>>monthly.ts=applySeries(daily.ts, from = from, to = to, FUN = colSums)
>>    
>>
>Error in if (timeTest == 0) iso.format = "%Y-%m-%d" :
>        missing value where TRUE/FALSE needed
>  
>
>>#
>>
>>    
>>
>write.csv(file="D:\\equities\\yahoo\\R\\daily_data\\monthly_normality.csv",monthly.ts at Data)
>Error in inherits(x, "data.frame") : object "monthly.ts" not found
>  
>
>>#
>># Direct implementation apply Series
>>#
>>#
>>    j.pos = as.POSIXct(seriesPositions(daily.ts))
>>    j.from =as.POSIXct(from)
>>    j.to = as.POSIXct(to)
>>    y = daily.ts at Data
>>    pos = seriesPositions(daily.ts)
>>    rowNames = rownames(daily.ts at Data)
>>    rowBind = NULL
>>    for (i in 1:from at Dim) {
>>    
>>
>+         test = (j.pos >= j.from[i] & j.pos <= j.to[i])
>+         cutted = as.matrix(y[test, ])
>+         ans = colSums(cutted)
>+ #        print(dim(ans))
>+         rowBind = rbind(rowBind, ans)
>+ #        print(dim(rowBind))
>+     }
>  
>
>>    rownames(rowBind) = as.character(to)
>>        units = daily.ts at units
>>
>>monthly.ts=timeSeries(data = rowBind, charvec = as.character(to), units =
>>    
>>
>units,
>+          zone = "GMT", FinCenter = daily.ts at FinCenter,
>+         title = daily.ts at title, documentation = daily.ts at documentation)
>  
>
>>write.csv(file="D:\\equities\\yahoo\\R\\agreg\\monthly.csv",monthly.ts at Data)
>>
>>
>>    
>>
>
>John C. Frain.
>Economics Department
>Trinity College Dublin
>Dublin 2
>
>www.tcd.ie/Economics/staff/frainj/home.html
>mailto:frainj at tcd.ie
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>


From marcella.marinelli at uniroma1.it  Sun Sep 17 23:22:10 2006
From: marcella.marinelli at uniroma1.it (march)
Date: Sun, 17 Sep 2006 14:22:10 -0700 (PDT)
Subject: [R-SIG-Finance] time serie simulation
Message-ID: <6354537.post@talk.nabble.com>


Hi everybody 
I'm trying to simulate a stochastic process in R. I would like consider n
log normal time series with different initial condition and starting at
different times. The first time serie has a growth rate lower than the
second and so on. In the long run the series have the same value. Do you
have any idea at running such a process? 
Thanks 
March 

-- 
View this message in context: http://www.nabble.com/time-serie-simulation-tf2287755.html#a6354537
Sent from the Rmetrics mailing list archive at Nabble.com.


From frainj at tcd.ie  Tue Sep 26 00:08:24 2006
From: frainj at tcd.ie (John C. Frain)
Date: Mon, 25 Sep 2006 23:08:24 +0100
Subject: [R-SIG-Finance] problem aggregating daily series solved
Message-ID: <1159222104.45185358d6fe7@mymail.tcd.ie>

Diethelm

Thank you for looking into my problem.  I now understand why I had such
problems.  Apart from the problem with the applySeries function y from and to
lists should have been in the same zone as th read.timeSeries statemenmt which
created the original time series object.  Again thank you very much.

Best Regards

John


-----Original Message-----
From: Diethelm Wuertz [mailto:wuertz at itp.phys.ethz.ch]
Sent: 25 September 2006 01:35
To: John C. Frain; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] problem aggregating daily series

Dear John Frain,
please apologize for any inconveniances caused by a bug in the
applySeries() Function.
It doesent appear when you worj with daily records using "GMT" as FinCenter.
Try:

 myFinCenter = "GMT"
 tD = timeSequence("2006-01-01", length.out = 365)
 Data = matrix(rep(1, times = 365), ncol = 1)
 tS = timeSeries(Data, tD)
 from = timeCalendar(2006)
 to = timeLastDayInMonth(from)
 applySeries(tS, from, to, FUN = colSums, units = "N")

Now fix the bug by overwriting the last line in applySeries()
timeSeries(data = rowBind, charvec = as.character(to), units = units,
       # format = x at format, zone = "GMT", FinCenter = x at FinCenter,
       format = x at format, zone = x at FinCenter, FinCenter = x at FinCenter,
       title = x at title, documentation = x at documentation, ...)

Here is the full function:

.applySeries =
function (x, from = NULL, to = NULL, FUN = colAvgs, units = NULL,  ...)
{
   colNames = units
   if (class(x) != "timeSeries") stop("s is not a timeSeries object")
   fun = match.fun(FUN)
   j.pos = as.POSIXct(seriesPositions(x))
   j.from = as.POSIXct(from)
   j.to = as.POSIXct(to)
   y = x at Data
   pos = seriesPositions(x)
   rowNames = rownames(x at Data)
   rowBind = NULL
   for (i in 1:from at Dim) {
       test = (j.pos >= j.from[i] & j.pos <= j.to[i])
       cutted = as.matrix(y[test, ])
       ans = fun(cutted, ...)
       rowBind = rbind(rowBind, ans)
   }
   rownames(rowBind) = as.character(to)
   if (is.null(colNames)) {
       units = x at units
   }
   else {
       units = colNames
   }
   timeSeries(data = rowBind, charvec = as.character(to), units = units,
       # format = x at format, zone = "GMT", FinCenter = x at FinCenter,
       format = x at format, zone = x at FinCenter, FinCenter = x at FinCenter,
       title = x at title, documentation = x at documentation, ...)
}


# Now try:

myFinCenter = "Dublin"
tD = timeSequence("2006-01-01", length.out = 365, FinCenter = "Dublin")
Data = matrix(rep(1, times = 365), ncol = 1)
tS = timeSeries(Data, tD, zone = "Dublin", FinCenter = "Dublin")
from = timeCalendar(2006)
to = timeLastDayInMonth(from, FinCenter = "Dublin")
.applySeries(tS, from, to, FUN = colSums, units = "N")

The philosophy behind the timeSeries class is that you have two
time reference points: The first where the data were collected (zone)
and the
second where the data willl be used (FinCenter). To circumvent
inconsistencies
in the future, I think it will be the best to allow to specify in any
case zone and
FinCenter through the argument list in any of the timeSeries class
functions


Diethelm Wuertz
John C. Frain.
Economics Department
Trinity College Dublin
Dublin 2

www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie




----- End forwarded message -----


From stefano.iacus at unimi.it  Tue Sep 26 01:51:01 2006
From: stefano.iacus at unimi.it (stefano iacus)
Date: Tue, 26 Sep 2006 08:51:01 +0900
Subject: [R-SIG-Finance] time serie simulation
In-Reply-To: <6354537.post@talk.nabble.com>
References: <6354537.post@talk.nabble.com>
Message-ID: <1AA3382C-FF63-4F4B-B143-FB257AF52BE5@unimi.it>


On 18/set/06, at 06:22, march wrote:

>
> Hi everybody
> I'm trying to simulate a stochastic process in R. I would like  
> consider n
> log normal time series with different initial condition and  
> starting at
> different times. The first time serie has a growth rate lower than the
> second and so on. In the long run the series have the same value.
what do you mean by same value? Same stationary distribution or moments?

> Do you
> have any idea at running such a process?
do you have a model in mind or are you asking about which processes  
satisfy such behaviour?

stefano
> Thanks
> March
>
> -- 
> View this message in context: http://www.nabble.com/time-serie- 
> simulation-tf2287755.html#a6354537
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance


