From chalabi at phys.ethz.ch  Sat Jan  3 09:19:20 2009
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Sat, 3 Jan 2009 09:19:20 +0100
Subject: [R-SIG-Finance] [R-sig-finance] converting to timeSeries
In-Reply-To: <21222886.post@talk.nabble.com>
References: <21222886.post@talk.nabble.com>
Message-ID: <20090103091920.37f8bbe0@mimi>

>>>> "P" == patzoul <patzoul at free.fr>
>>>> on Tue, 30 Dec 2008 12:46:29 -0800 (PST)

   P>
   P> I am loading data from a csv file:
   P> x = read.csv(XFR00104.txt, header = FALSE)
   P>
   P> The format is like this:
   P> V1 V2 V3 V4 V5 V6 V7
   P> 1 XFR0010411884 12/31/07 1 55.64 55.32 55.62 7244
   P> 2 XFR0010411884 01/02/08 1 56.80 54.50 56.75 58724
   P> 3 XFR0010411884 01/03/08 1 57.20 56.42 56.67 156879
   P> 4 XFR0010411884 01/04/08 1 58.98 56.28 58.43 138630
   P>
   P> I would like to convert it to a timeSeries.
   P>
   P> As a first step I converted the second column into dates:
   P> x[,2] = as.Date(x[,2], format = %m/%d/%y)
   P>
   P> and then used the as.timeSeries function:
   P> y = as.timeSeries(x[2:6])
   P>
   P> but lost all the date information:
   P> V3 V4 V5 V6
   P> 1 1 55.64 55.32 55.62
   P> 2 1 56.80 54.50 56.75
   P> 3 1 57.20 56.42 56.67
   P> 4 1 58.98 56.28 58.43
   P>
   P> What do I need to do to get a proper timeSeries?
   P>
   P>

read the manual pages ...

?timeSeries

my guess would be : 

timeSeries(data = x[2:6], charvec = x[,2], format = "%m/%d/%y")


From yee at post.harvard.edu  Mon Jan  5 02:31:51 2009
From: yee at post.harvard.edu (Andrew Yee)
Date: Sun, 4 Jan 2009 20:31:51 -0500
Subject: [R-SIG-Finance] how do you use get.hist.quote() to obtain dividend
Message-ID: <5dff5a0d0901041731s686a4fccoc9d1c462b4a0f25b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090104/be0f19db/attachment.pl>

From jeff.a.ryan at gmail.com  Mon Jan  5 03:57:53 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sun, 4 Jan 2009 20:57:53 -0600
Subject: [R-SIG-Finance] how do you use get.hist.quote() to obtain
	dividend
In-Reply-To: <5dff5a0d0901041731s686a4fccoc9d1c462b4a0f25b@mail.gmail.com>
References: <5dff5a0d0901041731s686a4fccoc9d1c462b4a0f25b@mail.gmail.com>
Message-ID: <e8e755250901041857h59b4a650vf41a7ca9c69723c2@mail.gmail.com>

Try quantmod:

library(quantmod)
?getDividends

> getDividends("MSFT")
> MSFT.div
           [,1]
2003-02-19 0.08
2003-10-15 0.16
2004-08-23 0.08
2004-11-15 3.08
2005-02-15 0.08
2005-05-16 0.08
2005-08-15 0.08
2005-11-15 0.08
2006-02-15 0.09
2006-05-15 0.09
2006-08-15 0.09
2006-11-14 0.10
2007-02-13 0.10
2007-05-15 0.10
2007-08-14 0.10
2007-11-13 0.11
2008-02-19 0.11
2008-05-13 0.11
2008-08-19 0.11
2008-11-18 0.13

HTH
Jeff

On Sun, Jan 4, 2009 at 7:31 PM, Andrew Yee <yee at post.harvard.edu> wrote:
> Apologies if this is a newbie question, but I've been able to use
> get.hist.quote() to obtain quote information.
>
> However, is there a way to use this function to obtain dividend distribution
> information?  Or is there a function for doing this?
>
> Thanks,
> Andrew
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From josh.m.ulrich at gmail.com  Mon Jan  5 03:59:30 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Sun, 4 Jan 2009 20:59:30 -0600
Subject: [R-SIG-Finance] how do you use get.hist.quote() to obtain
	dividend
In-Reply-To: <5dff5a0d0901041731s686a4fccoc9d1c462b4a0f25b@mail.gmail.com>
References: <5dff5a0d0901041731s686a4fccoc9d1c462b4a0f25b@mail.gmail.com>
Message-ID: <8cca69990901041859x475667c3je88331f5fce9be35@mail.gmail.com>

Hi Andrew,

You can use TTR's functionality to get both split and dividend
information from Yahoo Finance.

x <- getYahooData("SPY", type='split', quiet=TRUE)

Best,
Josh
--
http://quantemplation.blogspot.com



On Sun, Jan 4, 2009 at 7:31 PM, Andrew Yee <yee at post.harvard.edu> wrote:
> Apologies if this is a newbie question, but I've been able to use
> get.hist.quote() to obtain quote information.
>
> However, is there a way to use this function to obtain dividend distribution
> information?  Or is there a function for doing this?
>
> Thanks,
> Andrew
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From markleeds at verizon.net  Mon Jan  5 05:26:11 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sun, 04 Jan 2009 22:26:11 -0600 (CST)
Subject: [R-SIG-Finance] calculating the high frequency return
Message-ID: <1166908187.35133631231129571787.JavaMail.javamailuser@localhost>

  Suppose I have the bid and ask data for a stock XXX, at every minute ( 
best bid and best ask ). Then, say I want to calculate the
return to

A) going long that stock at 10:10 for 10 minutes.

B) going short that stock at 10:10 for 10 minutes.

I realize that , since I have only quote data, the whole thing is 
approximate anyway because the actual prices that one transacts in
are unknown ( or in the transaction price data which I'd rather avoid 
dealing with ) but my understanding is that  the best approximation is

lgoing ong return =   (bid at 10:20 - ask at 10:10)/ask at 10:10

going short return = ( ask at 10:20 - bid at 10:10)/bid at 10:10

Since I'm taking the spread into account in the formula ( rather than 
using ( midpoint at 10:20 - midpoint at 10:10)/midpoint at 10:10 ),
  then these are not the negative of each other, as they would be if one 
used the midpoint.  but I would think that above gives a better measure 
than using the midpoint because it implicity takes into account the 
transaction cost due to the spread which actually could be different 
depending on whether one is going long or short.

Any comments or corrections are appreciated. There's also interest 
rebates when one shorts but I'm assuming they are small enough
to ignore.  it also assumes that you can close the transaction EXACTLY 
when you want to which is not particularly realistic either. Still, if 
there's something wrong with above or a better way to calculate these 
things or a known standard source that explains it, enlightenment is 
appreciated.


 
Mark


On Sun, Jan 4, 2009 at  9:59 PM, Josh Ulrich wrote:

> Hi Andrew,
>
> You can use TTR's functionality to get both split and dividend
> information from Yahoo Finance.
>
> x <- getYahooData("SPY", type='split', quiet=TRUE)
>
> Best,
> Josh
> --
> http://quantemplation.blogspot.com
>
>
>
> On Sun, Jan 4, 2009 at 7:31 PM, Andrew Yee <yee at post.harvard.edu> 
> wrote:
>> Apologies if this is a newbie question, but I've been able to use
>> get.hist.quote() to obtain quote information.
>>
>> However, is there a way to use this function to obtain dividend 
>> distribution
>> information?  Or is there a function for doing this?
>>
>> Thanks,
>> Andrew
>>
>>        [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From markleeds at verizon.net  Mon Jan  5 05:33:55 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sun, 04 Jan 2009 22:33:55 -0600 (CST)
Subject: [R-SIG-Finance] calculating the high frequency return
Message-ID: <716725516.35136951231130035875.JavaMail.javamailuser@localhost>

  sorry but in my previous message, i meant to say that the going short 
return is -1.0* what i have. my apologies.

also, lgoing ong return should have read going long return. ( it's late 
).



On Sun, Jan 4, 2009 at 11:26 PM, markleeds at verizon.net wrote:

> Suppose I have the bid and ask data for a stock XXX, at every minute ( 
> best bid and best ask ). Then, say I want to calculate the
> return to
>
> A) going long that stock at 10:10 for 10 minutes.
>
> B) going short that stock at 10:10 for 10 minutes.
>
> I realize that , since I have only quote data, the whole thing is 
> approximate anyway because the actual prices that one transacts in
> are unknown ( or in the transaction price data which I'd rather avoid 
> dealing with ) but my understanding is that  the best approximation is
>
> lgoing ong return =   (bid at 10:20 - ask at 10:10)/ask at 10:10
>
> going short return = ( ask at 10:20 - bid at 10:10)/bid at 10:10
>
> Since I'm taking the spread into account in the formula ( rather than 
> using ( midpoint at 10:20 - midpoint at 10:10)/midpoint at 10:10 ),
>  then these are not the negative of each other, as they would be if 
> one used the midpoint.  but I would think that above gives a better 
> measure than using the midpoint because it implicity takes into 
> account the transaction cost due to the spread which actually could be 
> different depending on whether one is going long or short.
>
> Any comments or corrections are appreciated. There's also interest 
> rebates when one shorts but I'm assuming they are small enough
> to ignore.  it also assumes that you can close the transaction EXACTLY 
> when you want to which is not particularly realistic either. Still, if 
> there's something wrong with above or a better way to calculate these 
> things or a known standard source that explains it, enlightenment is 
> appreciated.
>
>
>
> Mark
>
>
> On Sun, Jan 4, 2009 at  9:59 PM, Josh Ulrich wrote:
>
>> Hi Andrew,
>>
>> You can use TTR's functionality to get both split and dividend
>> information from Yahoo Finance.
>>
>> x <- getYahooData("SPY", type='split', quiet=TRUE)
>>
>> Best,
>> Josh
>> --
>> http://quantemplation.blogspot.com
>>
>>
>>
>> On Sun, Jan 4, 2009 at 7:31 PM, Andrew Yee <yee at post.harvard.edu> 
>> wrote:
>>> Apologies if this is a newbie question, but I've been able to use
>>> get.hist.quote() to obtain quote information.
>>>
>>> However, is there a way to use this function to obtain dividend 
>>> distribution
>>> information?  Or is there a function for doing this?
>>>
>>> Thanks,
>>> Andrew
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From Neil.Beddoe at mbamfunds.com  Tue Jan  6 12:27:39 2009
From: Neil.Beddoe at mbamfunds.com (Neil Beddoe)
Date: Tue, 6 Jan 2009 11:27:39 +0000
Subject: [R-SIG-Finance] Trouble using ohlcPlot
Message-ID: <D4D64717AB295048B0FD52443103D75D07501776D2@LONMAIL.mbam.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090106/f1d1885b/attachment.pl>

From zakdump at gmail.com  Tue Jan  6 13:13:09 2009
From: zakdump at gmail.com (Michael Zak)
Date: Tue, 6 Jan 2009 13:13:09 +0100
Subject: [R-SIG-Finance] Trouble using ohlcPlot
In-Reply-To: <D4D64717AB295048B0FD52443103D75D07501776D2@LONMAIL.mbam.local>
References: <D4D64717AB295048B0FD52443103D75D07501776D2@LONMAIL.mbam.local>
Message-ID: <8C413DDC-9711-423F-BEE8-FC750CF54583@gmail.com>

Hi

You should use rownames() instead of names(). Try this:

mts <- data=c(results$OpenPrice,results$HighPrice,results$LowPrice,  
results$ClosePrice)
rownames(mts) <- c("Open", "High", "Low","Close")
ohlcPlot(mts)

HTH, Michael Zak

On 06.01.2009, at 12:27, Neil Beddoe wrote:

> Hi,
>
> Sorry about the cross-posting with r-help, I've just found this list.
>
> I'm trying to create a time series that will work with ohlcPlot:
>
> mts<-ts(data=c(results$OpenPrice,results$HighPrice,results$LowPrice,  
> results$ClosePrice),c="mts",names=c("Open", "High", "Low","Close"))
>
> ohlcPlot(mts) fails with: Error in if ((!is.mts(x)) || (colnames(x) 
> [1] != "Open") || (colnames(x)[2] !=  : ...
>
> is.mts(mts) returns true but colnames(mts) returns NULL.
>
> I've trawled the archive for an example of using this function to no  
> avail and it's not apparent from the docs what the data needs to  
> look like.  Does anyone have an example of code that will work with  
> this function?
>
> Thanks
>
> Neil
>
>
> .
>
> This message is intended only for the use of the person(s) to whom  
> it is addressed. It may contain information which is privileged and  
> confidential. Accordingly any unauthorised use is strictly  
> prohibited. If you are not the intended recipient, please contact  
> the sender as soon as possible.
>
> It is not intended as an offer or solicitation for the purchase or  
> sale of any financial instrument or as an official confirmation of  
> any transaction, unless specifically agreed otherwise. All market  
> prices, data and other information are not warranted as to  
> completeness or accuracy and are subject to change without notice.  
> Any opinions or advice contained in this Internet email are subject  
> to the terms and conditions expressed in any applicable governing  
> Marble Bar Asset Management LLP's  terms and conditions of business  
> or client agreement letter. Any comments or statements made herein  
> do not necessarily reflect those of Marble Bar Asset Management LLP.
>
> Marble Bar Asset Management LLP is regulated and authorised by the  
> FSA.
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From zakdump at gmail.com  Tue Jan  6 13:37:57 2009
From: zakdump at gmail.com (Michael Zak)
Date: Tue, 6 Jan 2009 13:37:57 +0100
Subject: [R-SIG-Finance] Trouble using ohlcPlot
In-Reply-To: <D4D64717AB295048B0FD52443103D75D07501776D2@LONMAIL.mbam.local>
References: <D4D64717AB295048B0FD52443103D75D07501776D2@LONMAIL.mbam.local>
Message-ID: <D9FE6579-598E-4707-9C4A-0F2803499447@gmail.com>

Hi

You should use colnames() if they're columns instead of names(). Try  
this:

mts <- cbind(results$OpenPrice, results$HighPrice, results$LowPrice,  
results$ClosePrice)
colnames(mts) <- c("Open",  "High",  "Low", "Close")
ohlcPlot(mts)

HTH, Michael Zak

On 06.01.2009, at 12:27, Neil Beddoe wrote:

> Hi,
>
> Sorry about the cross-posting with r-help, I've just found this list.
>
> I'm trying to create a time series that will work with ohlcPlot:
>
> mts<-ts(data=c(results$OpenPrice,results$HighPrice,results$LowPrice,  
> results$ClosePrice),c="mts",names=c("Open", "High", "Low","Close"))
>
> ohlcPlot(mts) fails with: Error in if ((!is.mts(x)) || (colnames(x) 
> [1] != "Open") || (colnames(x)[2] !=  : ...
>
> is.mts(mts) returns true but colnames(mts) returns NULL.
>
> I've trawled the archive for an example of using this function to no  
> avail and it's not apparent from the docs what the data needs to  
> look like.  Does anyone have an example of code that will work with  
> this function?
>
> Thanks
>
> Neil
>
>
> .
>
> This message is intended only for the use of the person(s) to whom  
> it is addressed. It may contain information which is privileged and  
> confidential. Accordingly any unauthorised use is strictly  
> prohibited. If you are not the intended recipient, please contact  
> the sender as soon as possible.
>
> It is not intended as an offer or solicitation for the purchase or  
> sale of any financial instrument or as an official confirmation of  
> any transaction, unless specifically agreed otherwise. All market  
> prices, data and other information are not warranted as to  
> completeness or accuracy and are subject to change without notice.  
> Any opinions or advice contained in this Internet email are subject  
> to the terms and conditions expressed in any applicable governing  
> Marble Bar Asset Management LLP's  terms and conditions of business  
> or client agreement letter. Any comments or statements made herein  
> do not necessarily reflect those of Marble Bar Asset Management LLP.
>
> Marble Bar Asset Management LLP is regulated and authorised by the  
> FSA.
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Tue Jan  6 15:54:48 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 6 Jan 2009 08:54:48 -0600
Subject: [R-SIG-Finance] Trouble using ohlcPlot
In-Reply-To: <D4D64717AB295048B0FD52443103D75D07501776D2@LONMAIL.mbam.local>
References: <D4D64717AB295048B0FD52443103D75D07501776D2@LONMAIL.mbam.local>
Message-ID: <e8e755250901060654s9224b42wad59422df8908634@mail.gmail.com>

A more flexible tool for financial charts can be had in the quantmod
package on CRAN.

This should accept all common time-series formats, though 'xts',
'zoo', 'fts', and 'timeSeries' may be the most useful choices for all
other things in R at this point in time.

library(quantmod)

#using the mts with colnames set by you

chartSeries(mts)

or barChart(), candleChart(), matchChart()...

Take a look here for examples:

http://www.quantmod.com/examples/charting/

HTH
Jeff

On Tue, Jan 6, 2009 at 5:27 AM, Neil Beddoe <Neil.Beddoe at mbamfunds.com> wrote:
> Hi,
>
> Sorry about the cross-posting with r-help, I've just found this list.
>
> I'm trying to create a time series that will work with ohlcPlot:
>
> mts<-ts(data=c(results$OpenPrice,results$HighPrice,results$LowPrice, results$ClosePrice),c="mts",names=c("Open", "High", "Low","Close"))
>
> ohlcPlot(mts) fails with: Error in if ((!is.mts(x)) || (colnames(x)[1] != "Open") || (colnames(x)[2] !=  : ...
>
> is.mts(mts) returns true but colnames(mts) returns NULL.
>
> I've trawled the archive for an example of using this function to no avail and it's not apparent from the docs what the data needs to look like.  Does anyone have an example of code that will work with this function?
>
> Thanks
>
> Neil
>
>
> .
>
> This message is intended only for the use of the person(s) to whom it is addressed. It may contain information which is privileged and confidential. Accordingly any unauthorised use is strictly prohibited. If you are not the intended recipient, please contact the sender as soon as possible.
>
> It is not intended as an offer or solicitation for the purchase or sale of any financial instrument or as an official confirmation of any transaction, unless specifically agreed otherwise. All market prices, data and other information are not warranted as to completeness or accuracy and are subject to change without notice. Any opinions or advice contained in this Internet email are subject to the terms and conditions expressed in any applicable governing Marble Bar Asset Management LLP's  terms and conditions of business or client agreement letter. Any comments or statements made herein do not necessarily reflect those of Marble Bar Asset Management LLP.
>
> Marble Bar Asset Management LLP is regulated and authorised by the FSA.
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From Neil.Beddoe at mbamfunds.com  Wed Jan  7 13:47:46 2009
From: Neil.Beddoe at mbamfunds.com (Neil Beddoe)
Date: Wed, 7 Jan 2009 12:47:46 +0000
Subject: [R-SIG-Finance] Trouble using ohlcPlot
In-Reply-To: <e8e755250901060654s9224b42wad59422df8908634@mail.gmail.com>
References: <D4D64717AB295048B0FD52443103D75D07501776D2@LONMAIL.mbam.local>
	<e8e755250901060654s9224b42wad59422df8908634@mail.gmail.com>
Message-ID: <D4D64717AB295048B0FD52443103D75D07501776E5@LONMAIL.mbam.local>

Thanks a lot Jeff.  That looks perfect. I'll give it a try.

-----Original Message-----
From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com] 
Sent: 06 January 2009 14:55
To: Neil Beddoe
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Trouble using ohlcPlot

A more flexible tool for financial charts can be had in the quantmod package on CRAN.

This should accept all common time-series formats, though 'xts', 'zoo', 'fts', and 'timeSeries' may be the most useful choices for all other things in R at this point in time.

library(quantmod)

#using the mts with colnames set by you

chartSeries(mts)

or barChart(), candleChart(), matchChart()...

Take a look here for examples:

http://www.quantmod.com/examples/charting/

HTH
Jeff

On Tue, Jan 6, 2009 at 5:27 AM, Neil Beddoe <Neil.Beddoe at mbamfunds.com> wrote:
> Hi,
>
> Sorry about the cross-posting with r-help, I've just found this list.
>
> I'm trying to create a time series that will work with ohlcPlot:
>
> mts<-ts(data=c(results$OpenPrice,results$HighPrice,results$LowPrice, 
> results$ClosePrice),c="mts",names=c("Open", "High", "Low","Close"))
>
> ohlcPlot(mts) fails with: Error in if ((!is.mts(x)) || (colnames(x)[1] != "Open") || (colnames(x)[2] !=  : ...
>
> is.mts(mts) returns true but colnames(mts) returns NULL.
>
> I've trawled the archive for an example of using this function to no avail and it's not apparent from the docs what the data needs to look like.  Does anyone have an example of code that will work with this function?
>
> Thanks
>
> Neil
>
>
> .
>
> This message is intended only for the use of the person(s) to whom it is addressed. It may contain information which is privileged and confidential. Accordingly any unauthorised use is strictly prohibited. If you are not the intended recipient, please contact the sender as soon as possible.
>
> It is not intended as an offer or solicitation for the purchase or sale of any financial instrument or as an official confirmation of any transaction, unless specifically agreed otherwise. All market prices, data and other information are not warranted as to completeness or accuracy and are subject to change without notice. Any opinions or advice contained in this Internet email are subject to the terms and conditions expressed in any applicable governing Marble Bar Asset Management LLP's  terms and conditions of business or client agreement letter. Any comments or statements made herein do not necessarily reflect those of Marble Bar Asset Management LLP.
>
> Marble Bar Asset Management LLP is regulated and authorised by the FSA.
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



--
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com

.

This message is intended only for the use of the person(s) to whom it is addressed. It may contain information which is privileged and confidential. Accordingly any unauthorised use is strictly prohibited. If you are not the intended recipient, please contact the sender as soon as possible.

It is not intended as an offer or solicitation for the purchase or sale of any financial instrument or as an official confirmation of any transaction, unless specifically agreed otherwise. All market prices, data and other information are not warranted as to completeness or accuracy and are subject to change without notice. Any opinions or advice contained in this Internet email are subject to the terms and conditions expressed in any applicable governing Marble Bar Asset Management LLP's  terms and conditions of business or client agreement letter. Any comments or statements made herein do not necessarily reflect those of Marble Bar Asset Management LLP.

Marble Bar Asset Management LLP is regulated and authorised by the FSA.


From ahala2000 at yahoo.com  Thu Jan  8 06:13:11 2009
From: ahala2000 at yahoo.com (elton wang)
Date: Wed, 7 Jan 2009 21:13:11 -0800 (PST)
Subject: [R-SIG-Finance] for help: quantmod library: getSymbols("000002.SS")
In-Reply-To: <e8e755250901041857h59b4a650vf41a7ca9c69723c2@mail.gmail.com>
Message-ID: <750589.767.qm@web31403.mail.mud.yahoo.com>

Jeff and all, 
How can I use quantmod to download stocks with special characters in names?

for instance, SHA:000002 in Google or 000002.SS in Yahoo, with getSymbols("000002.SS"), the output variable name is illegal in R.

Thanks for your help.

elton


From jeff.a.ryan at gmail.com  Thu Jan  8 06:23:55 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 7 Jan 2009 23:23:55 -0600
Subject: [R-SIG-Finance] for help: quantmod library:
	getSymbols("000002.SS")
In-Reply-To: <750589.767.qm@web31403.mail.mud.yahoo.com>
References: <e8e755250901041857h59b4a650vf41a7ca9c69723c2@mail.gmail.com>
	<750589.767.qm@web31403.mail.mud.yahoo.com>
Message-ID: <e8e755250901072123o7c195e3ei71575a0326a9e093@mail.gmail.com>

Take a look here:

http://www.nabble.com/Newbie-question-on-getSymbols%28%29-in-quantmod-tt21226579.html

You can either set 'auto.assign=FALSE' in the getSymbols call, and do
the assignment to a variable of your choosing with <-

-OR-

You can use ?setSymbolLookup to names from the source (Yahoo) to R

To set auto.assign=FALSE for all future calls you can use setDefaults
from the Defaults library (included by quantmod)

setDefaults(getSymbols, auto.assign=FALSE)

and all future calls will follow that rule.

HTH,
Jeff

On Wed, Jan 7, 2009 at 11:13 PM, elton wang <ahala2000 at yahoo.com> wrote:
> Jeff and all,
> How can I use quantmod to download stocks with special characters in names?
>
> for instance, SHA:000002 in Google or 000002.SS in Yahoo, with getSymbols("000002.SS"), the output variable name is illegal in R.
>
> Thanks for your help.
>
> elton
>
>
>
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From gmain.20.phftt at xoxy.net  Thu Jan  8 18:23:09 2009
From: gmain.20.phftt at xoxy.net (Rob Steele)
Date: Thu, 08 Jan 2009 12:23:09 -0500
Subject: [R-SIG-Finance] calculating the high frequency return
In-Reply-To: <1166908187.35133631231129571787.JavaMail.javamailuser@localhost>
References: <1166908187.35133631231129571787.JavaMail.javamailuser@localhost>
Message-ID: <gk5cpu$9j$1@ger.gmane.org>

Doing what you propose makes sense but be aware that it assumes there's
enough depth at the quote to fill your order and that your order has no
lasting impact on the market.  Both assumptions are valid if your order
is small enough and the stock is liquid enough but you'd have to get
much fancier to model large orders realistically.


markleeds at verizon.net wrote:
>  Suppose I have the bid and ask data for a stock XXX, at every minute (
> best bid and best ask ). Then, say I want to calculate the
> return to
> 
> A) going long that stock at 10:10 for 10 minutes.
> 
> B) going short that stock at 10:10 for 10 minutes.
> 
> I realize that , since I have only quote data, the whole thing is
> approximate anyway because the actual prices that one transacts in
> are unknown ( or in the transaction price data which I'd rather avoid
> dealing with ) but my understanding is that  the best approximation is
> 
> lgoing ong return =   (bid at 10:20 - ask at 10:10)/ask at 10:10
> 
> going short return = ( ask at 10:20 - bid at 10:10)/bid at 10:10
> 
> Since I'm taking the spread into account in the formula ( rather than
> using ( midpoint at 10:20 - midpoint at 10:10)/midpoint at 10:10 ),
>  then these are not the negative of each other, as they would be if one
> used the midpoint.  but I would think that above gives a better measure
> than using the midpoint because it implicity takes into account the
> transaction cost due to the spread which actually could be different
> depending on whether one is going long or short.
> 
> Any comments or corrections are appreciated. There's also interest
> rebates when one shorts but I'm assuming they are small enough
> to ignore.  it also assumes that you can close the transaction EXACTLY
> when you want to which is not particularly realistic either. Still, if
> there's something wrong with above or a better way to calculate these
> things or a known standard source that explains it, enlightenment is
> appreciated.
> 
> 
> 
> Mark


From judsonm123 at yahoo.com  Fri Jan  9 17:38:54 2009
From: judsonm123 at yahoo.com (Judson m)
Date: Fri, 9 Jan 2009 08:38:54 -0800 (PST)
Subject: [R-SIG-Finance] getFX problem
Message-ID: <861589.31353.qm@web33003.mail.mud.yahoo.com>

> getFX("USD/JPY",from="2004-01-01")
Error in (grep("PRE", fr)[1]):(grep("PRE", fr)[2]) : NA/NaN argument

Has anyone else had that problem today?

1-9-09


From jeff.a.ryan at gmail.com  Fri Jan  9 18:43:28 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 9 Jan 2009 11:43:28 -0600
Subject: [R-SIG-Finance] getFX problem
In-Reply-To: <861589.31353.qm@web33003.mail.mud.yahoo.com>
References: <861589.31353.qm@web33003.mail.mud.yahoo.com>
Message-ID: <e8e755250901090943p43f929b5n6801f6a6b0c5b0fb@mail.gmail.com>

oanda (the source) seems to have changed their rules on max data:

"Sorry, couldn't create table, because you entered incorrect dates
<BR> or the time period is too big (max. 500 days)."

getFX("USD/JPY",from='2008-01-01')

works.

I'll patch the code to make the error propagate through.

Jeff

On Fri, Jan 9, 2009 at 10:38 AM, Judson m <judsonm123 at yahoo.com> wrote:
>> getFX("USD/JPY",from="2004-01-01")
> Error in (grep("PRE", fr)[1]):(grep("PRE", fr)[2]) : NA/NaN argument
>
> Has anyone else had that problem today?
>
> 1-9-09
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From ron_michael70 at yahoo.com  Fri Jan  9 19:07:19 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Fri, 9 Jan 2009 10:07:19 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from Economagic
Message-ID: <21359782.post@talk.nabble.com>


Hi, I have posted this same problem in R help but did not get any fruitful
reply. Therefore I am again posting that same thread here as well, hope this
time I will get some efficient reply.

I was trying to dw data from Economagic
[http://www.economagic.com/em-cgi/data.exe/libor/day-ussnon], using
following code :

library(fimport)
dat2 = economagicSeries("libor/day-ussnon", frequency = "daily") 
> tail(dat2)
GMT
           libor/day-ussnon
2008-12-22               NA
2008-12-23               NA
2008-12-24          0.14750
2008-12-29               NA
2008-12-30          0.13875
2008-12-31               NA

Data for 2008-12-31 is given as NA, whereas in the website it is 0?13500.
Does anyone have a better idea on how to download data from Economagic more
efficiently? 
-- 
View this message in context: http://www.nabble.com/Downloading-data-from-Economagic-tp21359782p21359782.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Fri Jan  9 19:17:21 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 9 Jan 2009 12:17:21 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from Economagic
In-Reply-To: <21359782.post@talk.nabble.com>
References: <21359782.post@talk.nabble.com>
Message-ID: <e8e755250901091017g49b4f3e9pbd471c4c5d794859@mail.gmail.com>

You could subscribe :)

Beyond that you'll probably have to resign yourself to constantly
keeping up with the HTML that hides the data in the free pages.  It
changes often, and from series to series in my experience.

A snippet of the code in question:

2008 12 22<font color=white> 85</font   >0&#183;11375
 2008 12 23<font color=white> 39</font>0&#183;11750<font color=white></font>

 2008 12 24<font color=white> 28</font  >0&#183;14750
 2008 12 29<font color=white> 99</font       >0.13750<font color=white></font>
 2008<font color=white></font> 12 30<font color=white> 11</font>0&#183;13875
 2008 12 31<font color=white> 91</font>0&#183;13500

Your could simply parse yourself, converting all the &#183; instances
to . --- and then in the spirit of OSS contribute back your efforts to
rmetrics.

Jeff

On Fri, Jan 9, 2009 at 12:07 PM, RON70 <ron_michael70 at yahoo.com> wrote:
>
> Hi, I have posted this same problem in R help but did not get any fruitful
> reply. Therefore I am again posting that same thread here as well, hope this
> time I will get some efficient reply.
>
> I was trying to dw data from Economagic
> [http://www.economagic.com/em-cgi/data.exe/libor/day-ussnon], using
> following code :
>
> library(fimport)
> dat2 = economagicSeries("libor/day-ussnon", frequency = "daily")
>> tail(dat2)
> GMT
>           libor/day-ussnon
> 2008-12-22               NA
> 2008-12-23               NA
> 2008-12-24          0.14750
> 2008-12-29               NA
> 2008-12-30          0.13875
> 2008-12-31               NA
>
> Data for 2008-12-31 is given as NA, whereas in the website it is 0?13500.
> Does anyone have a better idea on how to download data from Economagic more
> efficiently?
> --
> View this message in context: http://www.nabble.com/Downloading-data-from-Economagic-tp21359782p21359782.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From Horace.Tso at pgn.com  Fri Jan  9 19:48:16 2009
From: Horace.Tso at pgn.com (Horace Tso)
Date: Fri, 9 Jan 2009 10:48:16 -0800
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from Economagic
In-Reply-To: <21359782.post@talk.nabble.com>
References: <21359782.post@talk.nabble.com>
Message-ID: <5C3F9922B1D5FB4886B2D2045AB952F301F26AD856@IPEXMAIL.corp.dom>

Over the holidays I went to their site and grabbed a couple of tables and dropped them into OpenOffice's Calc and did the text-to-column thing and many data points showed up with an unrecognizable character in place of the decimal point. I guess that's what's causing the R function to fail. No way around it. You just have to write your own script, or like what Jeff said, subscribe! 

H

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of RON70
Sent: Friday, January 09, 2009 10:07 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from Economagic


Hi, I have posted this same problem in R help but did not get any fruitful
reply. Therefore I am again posting that same thread here as well, hope this
time I will get some efficient reply.

I was trying to dw data from Economagic
[http://www.economagic.com/em-cgi/data.exe/libor/day-ussnon], using
following code :

library(fimport)
dat2 = economagicSeries("libor/day-ussnon", frequency = "daily") 
> tail(dat2)
GMT
           libor/day-ussnon
2008-12-22               NA
2008-12-23               NA
2008-12-24          0.14750
2008-12-29               NA
2008-12-30          0.13875
2008-12-31               NA

Data for 2008-12-31 is given as NA, whereas in the website it is 0*13500.
Does anyone have a better idea on how to download data from Economagic more
efficiently? 
-- 
View this message in context: http://www.nabble.com/Downloading-data-from-Economagic-tp21359782p21359782.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

From ajayshah at mayin.org  Fri Jan  9 20:10:14 2009
From: ajayshah at mayin.org (Ajay Shah)
Date: Sat, 10 Jan 2009 00:40:14 +0530
Subject: [R-SIG-Finance] [R-sig-finance] Downloading data from Economagic
In-Reply-To: <e8e755250901091017g49b4f3e9pbd471c4c5d794859@mail.gmail.com>
References: <21359782.post@talk.nabble.com>
	<e8e755250901091017g49b4f3e9pbd471c4c5d794859@mail.gmail.com>
Message-ID: <20090109191014.GU411@lubyanka.local>

> > library(fimport)
> > dat2 = economagicSeries("libor/day-ussnon", frequency = "daily")

Seems to me that economagicSeries() is lacking in critical
functionality. The release managers of fImport should not carry such a
function. After that, it's upto users desiring this functionality to
build their own...

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From hutch at moneyware.com  Fri Jan  9 20:37:21 2009
From: hutch at moneyware.com (Bill Hutchison)
Date: Fri, 9 Jan 2009 14:37:21 -0500
Subject: [R-SIG-Finance] Economagic data downloads
Message-ID: <013001c97291$b09ea0a0$11dbe1e0$@com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090109/673fac7c/attachment.pl>

From paulteetor at yahoo.com  Fri Jan  9 22:38:20 2009
From: paulteetor at yahoo.com (Paul Teetor)
Date: Fri, 9 Jan 2009 15:38:20 -0600
Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs Dickey-Fuller
Message-ID: <540297A816C146FD8AB916967EDC317E@XI>

R SIG Finance readers:
 
I am checking a futures spread for mean reversion.  I am using the Johansen
test (ca.jo) for cointegration and the Augmented Dickey-Fuller test (ur.df)
for mean reversion.

Here is the odd part:  The Johansen test says the two futures prices are not
cointegrated, but the ADF test says the spread is, in fact, mean-reverting.
 
I am very puzzled.  The spread is a linear combination of the prices, and
the ADF test says it is mean-reverting.  But the failed Johansen test says
the prices are not cointegrated, so no linear combination of prices is
mean-reverting.  Huh??
 
I would be very grateful is someone could suggest where I went wrong, or
steer me towards some relevent reference materials.

 
Background:  I am studying the spread between TY futures (10-year US
Treasurys) and SR futures (10-year US swap rate), calculated as:
 
    sprd = ty - (1.2534 * sr)
 
where ty and sr are the time series of futures prices.  (The 1.2534 factor
is from an ordinary least squares fit.)  I execute the Johansen procedure
this way:
 
    ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")
 
The summary of the test result is:

	###################### 
	# Johansen-Procedure # 
	###################### 

	Test type: maximal eigenvalue statistic (lambda max) , without
linear trend and constant in 	cointegration 

	Eigenvalues (lambda):
	[1]  2.929702e-03  6.616599e-04 -1.001412e-17

	Values of teststatistic and critical values of test:

	         test 10pct  5pct  1pct
	r <= 1 | 2.00  7.52  9.24 12.97
	r = 0  | 8.89 13.75 15.67 20.20

	<snip>

I interpret the "r <= 1" line this way:  The test statistic for r <= 1 is
below the critical values, hence we cannot reject the null hypothesis that
the rank is less than 2.  We conclude that the two time series are not
cointegrated.

I run the ADF test this way:

	ur.df(sprd, type="drift")

(I set type="drift" because that seems to correspond to ecdet="const" for
the Johansen test.)  The summary of the ADF test is:

	###############################################
	# Augmented Dickey-Fuller Test Unit Root Test #
###############################################

	Test regression drift

	<snip>

	Value of test-statistic is: -2.9624 4.4142

	Critical values for test statistics:
		1pct  5pct 10pct
	tau2 -3.43 -2.86 -2.57
	phi1  6.43  4.59  3.78 

I interpret the test statistics as meaning we can reject the null hypothesis
of a unit root (at a confidence level of 90% or better), hence the spread is
mean-reverting.  I get similar results from the adf.test() procedure.

F.Y.I., I am running version 2.6.2 of R.
 
Paul Teetor
Elgin, IL   USA


From markleeds at verizon.net  Fri Jan  9 23:10:16 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 09 Jan 2009 16:10:16 -0600 (CST)
Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs
 Dickey-Fuller
Message-ID: <272389586.39434131231539016078.JavaMail.javamailuser@localhost>

  i think this can happen quite often but i'm not clear on how to 
resolve it. with the DF
methodology, you are specifying the response and with Johansen's you 
aren't so
that may have something to do with it. The literature talks about it but 
I don't think
there's a resolution. Bernhard's cointegration book may talk about it 
also.



On Fri, Jan 9, 2009 at  4:38 PM, Paul Teetor wrote:

> R SIG Finance readers:
>  I am checking a futures spread for mean reversion.  I am using the 
> Johansen
> test (ca.jo) for cointegration and the Augmented Dickey-Fuller test 
> (ur.df)
> for mean reversion.
>
> Here is the odd part:  The Johansen test says the two futures prices 
> are not
> cointegrated, but the ADF test says the spread is, in fact, 
> mean-reverting.
>  I am very puzzled.  The spread is a linear combination of the prices, 
> and
> the ADF test says it is mean-reverting.  But the failed Johansen test 
> says
> the prices are not cointegrated, so no linear combination of prices is
> mean-reverting.  Huh??
>  I would be very grateful is someone could suggest where I went wrong, 
> or
> steer me towards some relevent reference materials.
>
>  Background:  I am studying the spread between TY futures (10-year US
> Treasurys) and SR futures (10-year US swap rate), calculated as:
>      sprd = ty - (1.2534 * sr)
>  where ty and sr are the time series of futures prices.  (The 1.2534 
> factor
> is from an ordinary least squares fit.)  I execute the Johansen 
> procedure
> this way:
>      ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")
>  The summary of the test result is:
>
> 	###################### 	# Johansen-Procedure # 	 
> ######################
> 	Test type: maximal eigenvalue statistic (lambda max) , without
> linear trend and constant in 	cointegration
> 	Eigenvalues (lambda):
> 	[1]  2.929702e-03  6.616599e-04 -1.001412e-17
>
> 	Values of teststatistic and critical values of test:
>
> 	         test 10pct  5pct  1pct
> 	r <= 1 | 2.00  7.52  9.24 12.97
> 	r = 0  | 8.89 13.75 15.67 20.20
>
> 	<snip>
>
> I interpret the "r <= 1" line this way:  The test statistic for r <= 1 
> is
> below the critical values, hence we cannot reject the null hypothesis 
> that
> the rank is less than 2.  We conclude that the two time series are not
> cointegrated.
>
> I run the ADF test this way:
>
> 	ur.df(sprd, type="drift")
>
> (I set type="drift" because that seems to correspond to ecdet="const" 
> for
> the Johansen test.)  The summary of the ADF test is:
>
> 	###############################################
> 	# Augmented Dickey-Fuller Test Unit Root Test #
> ###############################################
>
> 	Test regression drift
>
> 	<snip>
>
> 	Value of test-statistic is: -2.9624 4.4142
>
> 	Critical values for test statistics:
> 		1pct  5pct 10pct
> 	tau2 -3.43 -2.86 -2.57
> 	phi1  6.43  4.59  3.78
> I interpret the test statistics as meaning we can reject the null 
> hypothesis
> of a unit root (at a confidence level of 90% or better), hence the 
> spread is
> mean-reverting.  I get similar results from the adf.test() procedure.
>
> F.Y.I., I am running version 2.6.2 of R.
>  Paul Teetor
> Elgin, IL   USA
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From brian at braverock.com  Fri Jan  9 23:22:46 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 09 Jan 2009 16:22:46 -0600
Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs
	Dickey-Fuller
Message-ID: <a7o7w99rdwx4nmfws75mg51v.1231540582684@email.android.com>

I'll look when I get home, but if I recall correctly, you need to check the unit root first.  Bernhard's book is definitely the best reference, and the new edition expands substantially onn the previous version.

markleeds at verizon.net wrote:

>  i think this can happen quite often but i'm not clear on how to 
>resolve it. with the DF
>methodology, you are specifying the response and with Johansen's you 
>aren't so
>that may have something to do with it. The literature talks about it but 
>I don't think
>there's a resolution. Bernhard's cointegration book may talk about it 
>also.
>
>
>
>On Fri, Jan 9, 2009 at  4:38 PM, Paul Teetor wrote:
>
>> R SIG Finance readers:
>>  I am checking a futures spread for mean reversion.  I am using the 
>> Johansen
>> test (ca.jo) for cointegration and the Augmented Dickey-Fuller test 
>> (ur.df)
>> for mean reversion.
>>
>> Here is the odd part:  The Johansen test says the two futures prices 
>> are not
>> cointegrated, but the ADF test says the spread is, in fact, 
>> mean-reverting.
>>  I am very puzzled.  The spread is a linear combination of the prices, 
>> and
>> the ADF test says it is mean-reverting.  But the failed Johansen test 
>> says
>> the prices are not cointegrated, so no linear combination of prices is
>> mean-reverting.  Huh??
>>  I would be very grateful is someone could suggest where I went wrong, 
>> or
>> steer me towards some relevent reference materials.
>>
>>  Background:  I am studying the spread between TY futures (10-year US
>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>      sprd = ty - (1.2534 * sr)
>>  where ty and sr are the time series of futures prices.  (The 1.2534 
>> factor
>> is from an ordinary least squares fit.)  I execute the Johansen 
>> procedure
>> this way:
>>      ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")
>>  The summary of the test result is:
>>
>> 	###################### 	# Johansen-Procedure # 	 
>> ######################
>> 	Test type: maximal eigenvalue statistic (lambda max) , without
>> linear trend and constant in 	cointegration
>> 	Eigenvalues (lambda):
>> 	[1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>
>> 	Values of teststatistic and critical values of test:
>>
>> 	         test 10pct  5pct  1pct
>> 	r <= 1 | 2.00  7.52  9.24 12.97
>> 	r = 0  | 8.89 13.75 15.67 20.20
>>
>> 	<snip>
>>
>> I interpret the "r <= 1" line this way:  The test statistic for r <= 1 
>> is
>> below the critical values, hence we cannot reject the null hypothesis 
>> that
>> the rank is less than 2.  We conclude that the two time series are not
>> cointegrated.
>>
>> I run the ADF test this way:
>>
>> 	ur.df(sprd, type="drift")
>>
>> (I set type="drift" because that seems to correspond to ecdet="const" 
>> for
>> the Johansen test.)  The summary of the ADF test is:
>>
>> 	###############################################
>> 	# Augmented Dickey-Fuller Test Unit Root Test #
>> ###############################################
>>
>> 	Test regression drift
>>
>> 	<snip>
>>
>> 	Value of test-statistic is: -2.9624 4.4142
>>
>> 	Critical values for test statistics:
>> 		1pct  5pct 10pct
>> 	tau2 -3.43 -2.86 -2.57
>> 	phi1  6.43  4.59  3.78
>> I interpret the test statistics as meaning we can reject the null 
>> hypothesis
>> of a unit root (at a confidence level of 90% or better), hence the 
>> spread is
>> mean-reverting.  I get similar results from the adf.test() procedure.
>>
>> F.Y.I., I am running version 2.6.2 of R.
>>  Paul Teetor
>> Elgin, IL   USA
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.

From markleeds at verizon.net  Sat Jan 10 00:04:14 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 09 Jan 2009 17:04:14 -0600 (CST)
Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs
 Dickey-Fuller
Message-ID: <1797760867.44271361231542254858.JavaMail.javamailuser@localhost>

Hi Brian:  in both instances ( ADF and Johansen ) the unit root in each 
series needs to be checked first because if there's not a unit root in 
both of them then neither test applies.  But I don't think ( or atleast 
I  don't remember. it's been a while ) that has anything to do with the 
conflicting testing results between the two approaches.

In fact, if you use DF to test for cointegration ( after you find a unit 
root in each series ), you can switch Y and X and get different answers 
just doing that. The DF results themselves can depend on what one 
defines as the response and the predictor. Johansen atleast doesn't have 
that problem but I always found DF ( I think they call it Engle-Granger 
to not confuse thre DF pretesting for the unit root with the 
cointegration test ) A LOT more indersatandable and intuitive.

Also, thanks for pointig out that Bernhard has an updated book. The 
first edition was great so
I'm sure the second one will be also.






On Fri, Jan 9, 2009 at  5:22 PM, Brian G. Peterson wrote:

> I'll look when I get home, but if I recall correctly, you need to 
> check the unit root first.  Bernhard's book is definitely the best 
> reference, and the new edition expands substantially onn the previous 
> version.
>
> markleeds at verizon.net wrote:
>
>>  i think this can happen quite often but i'm not clear on how to 
>> resolve it. with the DF
>> methodology, you are specifying the response and with Johansen's you 
>> aren't so
>> that may have something to do with it. The literature talks about it 
>> but I don't think
>> there's a resolution. Bernhard's cointegration book may talk about it 
>> also.
>>
>>
>>
>> On Fri, Jan 9, 2009 at  4:38 PM, Paul Teetor wrote:
>>
>>> R SIG Finance readers:
>>>  I am checking a futures spread for mean reversion.  I am using the 
>>> Johansen
>>> test (ca.jo) for cointegration and the Augmented Dickey-Fuller test 
>>> (ur.df)
>>> for mean reversion.
>>>
>>> Here is the odd part:  The Johansen test says the two futures prices 
>>> are not
>>> cointegrated, but the ADF test says the spread is, in fact, 
>>> mean-reverting.
>>>  I am very puzzled.  The spread is a linear combination of the 
>>> prices, and
>>> the ADF test says it is mean-reverting.  But the failed Johansen 
>>> test says
>>> the prices are not cointegrated, so no linear combination of prices 
>>> is
>>> mean-reverting.  Huh??
>>>  I would be very grateful is someone could suggest where I went 
>>> wrong, or
>>> steer me towards some relevent reference materials.
>>>
>>>  Background:  I am studying the spread between TY futures (10-year 
>>> US
>>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>>      sprd = ty - (1.2534 * sr)
>>>  where ty and sr are the time series of futures prices.  (The 1.2534 
>>> factor
>>> is from an ordinary least squares fit.)  I execute the Johansen 
>>> procedure
>>> this way:
>>>      ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")
>>>  The summary of the test result is:
>>>
>>> 	###################### 	# Johansen-Procedure # 	 
>>> ######################
>>> 	Test type: maximal eigenvalue statistic (lambda max) , without
>>> linear trend and constant in 	cointegration
>>> 	Eigenvalues (lambda):
>>> 	[1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>>
>>> 	Values of teststatistic and critical values of test:
>>>
>>> 	         test 10pct  5pct  1pct
>>> 	r <= 1 | 2.00  7.52  9.24 12.97
>>> 	r = 0  | 8.89 13.75 15.67 20.20
>>>
>>> 	<snip>
>>>
>>> I interpret the "r <= 1" line this way:  The test statistic for r <= 
>>> 1 is
>>> below the critical values, hence we cannot reject the null 
>>> hypothesis that
>>> the rank is less than 2.  We conclude that the two time series are 
>>> not
>>> cointegrated.
>>>
>>> I run the ADF test this way:
>>>
>>> 	ur.df(sprd, type="drift")
>>>
>>> (I set type="drift" because that seems to correspond to 
>>> ecdet="const" for
>>> the Johansen test.)  The summary of the ADF test is:
>>>
>>> 	###############################################
>>> 	# Augmented Dickey-Fuller Test Unit Root Test #
>>> ###############################################
>>>
>>> 	Test regression drift
>>>
>>> 	<snip>
>>>
>>> 	Value of test-statistic is: -2.9624 4.4142
>>>
>>> 	Critical values for test statistics:
>>> 		1pct  5pct 10pct
>>> 	tau2 -3.43 -2.86 -2.57
>>> 	phi1  6.43  4.59  3.78
>>> I interpret the test statistics as meaning we can reject the null 
>>> hypothesis
>>> of a unit root (at a confidence level of 90% or better), hence the 
>>> spread is
>>> mean-reverting.  I get similar results from the adf.test() 
>>> procedure.
>>>
>>> F.Y.I., I am running version 2.6.2 of R.
>>>  Paul Teetor
>>> Elgin, IL   USA
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.


From jh8080 at hotmail.com  Sat Jan 10 00:06:45 2009
From: jh8080 at hotmail.com (Jae Kim)
Date: Sat, 10 Jan 2009 10:06:45 +1100
Subject: [R-SIG-Finance] Fw: Testing for cointegration: Johansen vs
	Dickey-Fuller
Message-ID: <BAY104-DS23A50089FB3C5091B84AECDDD0@phx.gbl>

From: "Jae Kim" <jh8080 at hotmail.com>
Sent: Saturday, January 10, 2009 10:04 AM
To: "Paul Teetor" <paulteetor at yahoo.com>
Subject: Re: [R-SIG-Finance] Testing for cointegration: Johansen vs 
Dickey-Fuller

> Hi,
>
> 1. If you are using the ADF test here, you are giving the restriction that 
> the  cointegrating vector between the two is (1, -1.2534). That is, you 
> are saying that the two variables are related in the long run with the 
> cointegrating vector given. Under this restriction, you find the spread 
> stationary, so they are cointegrated with given cointegrating vector.
>
> 2. If you are using Johansen method, you are doing unrestricted estimation 
> of cointegrating vector. But if you believe that the above restriction is 
> sensible economically, the ADF result should be preferred to Johansen 
> result.
>
> 3. This is the bivariate case, so Johansen method may not be necessary. 
> try Engle-Granger 2-stage method, you might find cointegration. In 
> addition, Johansen method assumes normality, which may often be violated.
>
> hope this helps. JHK
>
>
> --------------------------------------------------
> From: "Paul Teetor" <paulteetor at yahoo.com>
> Sent: Saturday, January 10, 2009 8:38 AM
> To: <r-sig-finance at stat.math.ethz.ch>
> Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs 
> Dickey-Fuller
>
>> R SIG Finance readers:
>>
>> I am checking a futures spread for mean reversion.  I am using the 
>> Johansen
>> test (ca.jo) for cointegration and the Augmented Dickey-Fuller test 
>> (ur.df)
>> for mean reversion.
>>
>> Here is the odd part:  The Johansen test says the two futures prices are 
>> not
>> cointegrated, but the ADF test says the spread is, in fact, 
>> mean-reverting.
>>
>> I am very puzzled.  The spread is a linear combination of the prices, and
>> the ADF test says it is mean-reverting.  But the failed Johansen test 
>> says
>> the prices are not cointegrated, so no linear combination of prices is
>> mean-reverting.  Huh??
>>
>> I would be very grateful is someone could suggest where I went wrong, or
>> steer me towards some relevent reference materials.
>>
>>
>> Background:  I am studying the spread between TY futures (10-year US
>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>
>>    sprd = ty - (1.2534 * sr)
>>
>> where ty and sr are the time series of futures prices.  (The 1.2534 
>> factor
>> is from an ordinary least squares fit.)  I execute the Johansen procedure
>> this way:
>>
>>    ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")
>>
>> The summary of the test result is:
>>
>> ######################
>> # Johansen-Procedure #
>> ######################
>>
>> Test type: maximal eigenvalue statistic (lambda max) , without
>> linear trend and constant in cointegration
>>
>> Eigenvalues (lambda):
>> [1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>
>> Values of teststatistic and critical values of test:
>>
>>          test 10pct  5pct  1pct
>> r <= 1 | 2.00  7.52  9.24 12.97
>> r = 0  | 8.89 13.75 15.67 20.20
>>
>> <snip>
>>
>> I interpret the "r <= 1" line this way:  The test statistic for r <= 1 is
>> below the critical values, hence we cannot reject the null hypothesis 
>> that
>> the rank is less than 2.  We conclude that the two time series are not
>> cointegrated.
>>
>> I run the ADF test this way:
>>
>> ur.df(sprd, type="drift")
>>
>> (I set type="drift" because that seems to correspond to ecdet="const" for
>> the Johansen test.)  The summary of the ADF test is:
>>
>> ###############################################
>> # Augmented Dickey-Fuller Test Unit Root Test #
>> ###############################################
>>
>> Test regression drift
>>
>> <snip>
>>
>> Value of test-statistic is: -2.9624 4.4142
>>
>> Critical values for test statistics:
>> 1pct  5pct 10pct
>> tau2 -3.43 -2.86 -2.57
>> phi1  6.43  4.59  3.78
>>
>> I interpret the test statistics as meaning we can reject the null 
>> hypothesis
>> of a unit root (at a confidence level of 90% or better), hence the spread 
>> is
>> mean-reverting.  I get similar results from the adf.test() procedure.
>>
>> F.Y.I., I am running version 2.6.2 of R.
>>
>> Paul Teetor
>> Elgin, IL   USA
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>


From ERICKSON at ers.usda.gov  Sat Jan 10 00:11:38 2009
From: ERICKSON at ers.usda.gov (Erickson, Ken)
Date: Fri, 9 Jan 2009 18:11:38 -0500
Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs
	Dickey-Fuller
In-Reply-To: <540297A816C146FD8AB916967EDC317E@XI>
References: <540297A816C146FD8AB916967EDC317E@XI>
Message-ID: <9AFA50D47626474F873C93FFFDF3E9D2D67745@ECONEXVS01.econ.ers.usda.gov>

Hi all, I am very new to R and thus don't have any profound answers
here.
Wish I did. But I did recently purchase Bernhard Pfaff's book,
Analysis of Integrated and Cointegrated Series with R, 2nd, edition..
Springer. Isbn 978-0-387-75966-1.

The book appears to 'cover' both the Johansen cointegration concepts and
the D-F unit root tests...this might help in answering your
question...at some point, and may be a good reference...Perhaps others
in the group have used this book and might want to comment.
Best regards,
Ken Erickson

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Paul
Teetor
Sent: Friday, January 09, 2009 4:38 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs
Dickey-Fuller

R SIG Finance readers:
 
I am checking a futures spread for mean reversion.  I am using the
Johansen
test (ca.jo) for cointegration and the Augmented Dickey-Fuller test
(ur.df)
for mean reversion.

Here is the odd part:  The Johansen test says the two futures prices are
not
cointegrated, but the ADF test says the spread is, in fact,
mean-reverting.
 
I am very puzzled.  The spread is a linear combination of the prices,
and
the ADF test says it is mean-reverting.  But the failed Johansen test
says
the prices are not cointegrated, so no linear combination of prices is
mean-reverting.  Huh??
 
I would be very grateful is someone could suggest where I went wrong, or
steer me towards some relevent reference materials.

 
Background:  I am studying the spread between TY futures (10-year US
Treasurys) and SR futures (10-year US swap rate), calculated as:
 
    sprd = ty - (1.2534 * sr)
 
where ty and sr are the time series of futures prices.  (The 1.2534
factor
is from an ordinary least squares fit.)  I execute the Johansen
procedure
this way:
 
    ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")
 
The summary of the test result is:

	###################### 
	# Johansen-Procedure # 
	###################### 

	Test type: maximal eigenvalue statistic (lambda max) , without
linear trend and constant in 	cointegration 

	Eigenvalues (lambda):
	[1]  2.929702e-03  6.616599e-04 -1.001412e-17

	Values of teststatistic and critical values of test:

	         test 10pct  5pct  1pct
	r <= 1 | 2.00  7.52  9.24 12.97
	r = 0  | 8.89 13.75 15.67 20.20

	<snip>

I interpret the "r <= 1" line this way:  The test statistic for r <= 1
is
below the critical values, hence we cannot reject the null hypothesis
that
the rank is less than 2.  We conclude that the two time series are not
cointegrated.

I run the ADF test this way:

	ur.df(sprd, type="drift")

(I set type="drift" because that seems to correspond to ecdet="const"
for
the Johansen test.)  The summary of the ADF test is:

	###############################################
	# Augmented Dickey-Fuller Test Unit Root Test #
###############################################

	Test regression drift

	<snip>

	Value of test-statistic is: -2.9624 4.4142

	Critical values for test statistics:
		1pct  5pct 10pct
	tau2 -3.43 -2.86 -2.57
	phi1  6.43  4.59  3.78 

I interpret the test statistics as meaning we can reject the null
hypothesis
of a unit root (at a confidence level of 90% or better), hence the
spread is
mean-reverting.  I get similar results from the adf.test() procedure.

F.Y.I., I am running version 2.6.2 of R.

Paul Teetor
Elgin, IL   USA

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From matthieu.stigler at gmail.com  Sat Jan 10 07:53:43 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Sat, 10 Jan 2009 12:23:43 +0530
Subject: [R-SIG-Finance] Fw: Testing for cointegration: Johansen vs
	Dickey-Fuller
In-Reply-To: <BAY104-DS23A50089FB3C5091B84AECDDD0@phx.gbl>
References: <BAY104-DS23A50089FB3C5091B84AECDDD0@phx.gbl>
Message-ID: <496845F7.5070403@gmail.com>

Hi

First of all, when dealing with time series, having contradictory 
results is not the exception but almost the rule! You may have opposite 
results between tests, with a same test with different specifications 
and finally with same test, same specification but different number of lags!

Your contradictory results come maybe from the way you apply the DF 
test, as pointed out by Jae Kim. In the residual based approach (or 
"Engle and Granger", or "two steps"), one applies unit root tests on the 
residuals from the cointegrating vector. If this cointegrating vector is 
known (from theory), just use the ADF.

Nevertheless, if the cointegrating vector has to be estimated (with 
usual OLS, as you did), one has to take into account this uncertainty 
and correct the distribution (which is then "larger") what EG did with 
Monte-Carlo, and Philips and Ouliaris more theoretically. The last one 
is implemented  in package urca under ca.po.

So maybe your two favorable result come from the fact that you estimate 
the cointegrating value but then take it as known a priori. Note 
nevertheless that studying spreads one typically assumes the vector (1, 
-1).

Small point, as residuals from a regression have zero mean, I don't 
think you need to test a model with a drift.

Finally, for your data, you may be interested in testing for threshold 
cointegration, that is, cointegration and error correction occurring 
only when deviations from long-run are big enough, which is more 
realistic and theory consistent. Some functions (as for your case a test 
of no cointegration against threshold cointegration with known vector) 
are availabe in dev version of package tsDyn (see TVECM_SeoTest).

Matthieu

Jae Kim a ?crit :
> From: "Jae Kim" <jh8080 at hotmail.com>
> Sent: Saturday, January 10, 2009 10:04 AM
> To: "Paul Teetor" <paulteetor at yahoo.com>
> Subject: Re: [R-SIG-Finance] Testing for cointegration: Johansen vs 
> Dickey-Fuller
>
>> Hi,
>>
>> 1. If you are using the ADF test here, you are giving the restriction 
>> that the  cointegrating vector between the two is (1, -1.2534). That 
>> is, you are saying that the two variables are related in the long run 
>> with the cointegrating vector given. Under this restriction, you find 
>> the spread stationary, so they are cointegrated with given 
>> cointegrating vector.
>>
>> 2. If you are using Johansen method, you are doing unrestricted 
>> estimation of cointegrating vector. But if you believe that the above 
>> restriction is sensible economically, the ADF result should be 
>> preferred to Johansen result.
>>
>> 3. This is the bivariate case, so Johansen method may not be 
>> necessary. try Engle-Granger 2-stage method, you might find 
>> cointegration. In addition, Johansen method assumes normality, which 
>> may often be violated.
>>
>> hope this helps. JHK
>>
>>
>> --------------------------------------------------
>> From: "Paul Teetor" <paulteetor at yahoo.com>
>> Sent: Saturday, January 10, 2009 8:38 AM
>> To: <r-sig-finance at stat.math.ethz.ch>
>> Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs 
>> Dickey-Fuller
>>
>>> R SIG Finance readers:
>>>
>>> I am checking a futures spread for mean reversion.  I am using the 
>>> Johansen
>>> test (ca.jo) for cointegration and the Augmented Dickey-Fuller test 
>>> (ur.df)
>>> for mean reversion.
>>>
>>> Here is the odd part:  The Johansen test says the two futures prices 
>>> are not
>>> cointegrated, but the ADF test says the spread is, in fact, 
>>> mean-reverting.
>>>
>>> I am very puzzled.  The spread is a linear combination of the 
>>> prices, and
>>> the ADF test says it is mean-reverting.  But the failed Johansen 
>>> test says
>>> the prices are not cointegrated, so no linear combination of prices is
>>> mean-reverting.  Huh??
>>>
>>> I would be very grateful is someone could suggest where I went 
>>> wrong, or
>>> steer me towards some relevent reference materials.
>>>
>>>
>>> Background:  I am studying the spread between TY futures (10-year US
>>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>>
>>>    sprd = ty - (1.2534 * sr)
>>>
>>> where ty and sr are the time series of futures prices.  (The 1.2534 
>>> factor
>>> is from an ordinary least squares fit.)  I execute the Johansen 
>>> procedure
>>> this way:
>>>
>>>    ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")
>>>
>>> The summary of the test result is:
>>>
>>> ######################
>>> # Johansen-Procedure #
>>> ######################
>>>
>>> Test type: maximal eigenvalue statistic (lambda max) , without
>>> linear trend and constant in cointegration
>>>
>>> Eigenvalues (lambda):
>>> [1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>>
>>> Values of teststatistic and critical values of test:
>>>
>>>          test 10pct  5pct  1pct
>>> r <= 1 | 2.00  7.52  9.24 12.97
>>> r = 0  | 8.89 13.75 15.67 20.20
>>>
>>> <snip>
>>>
>>> I interpret the "r <= 1" line this way:  The test statistic for r <= 
>>> 1 is
>>> below the critical values, hence we cannot reject the null 
>>> hypothesis that
>>> the rank is less than 2.  We conclude that the two time series are not
>>> cointegrated.
>>>
>>> I run the ADF test this way:
>>>
>>> ur.df(sprd, type="drift")
>>>
>>> (I set type="drift" because that seems to correspond to 
>>> ecdet="const" for
>>> the Johansen test.)  The summary of the ADF test is:
>>>
>>> ###############################################
>>> # Augmented Dickey-Fuller Test Unit Root Test #
>>> ###############################################
>>>
>>> Test regression drift
>>>
>>> <snip>
>>>
>>> Value of test-statistic is: -2.9624 4.4142
>>>
>>> Critical values for test statistics:
>>> 1pct  5pct 10pct
>>> tau2 -3.43 -2.86 -2.57
>>> phi1  6.43  4.59  3.78
>>>
>>> I interpret the test statistics as meaning we can reject the null 
>>> hypothesis
>>> of a unit root (at a confidence level of 90% or better), hence the 
>>> spread is
>>> mean-reverting.  I get similar results from the adf.test() procedure.
>>>
>>> F.Y.I., I am running version 2.6.2 of R.
>>>
>>> Paul Teetor
>>> Elgin, IL   USA
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From ERICKSON at ers.usda.gov  Sat Jan 10 14:30:28 2009
From: ERICKSON at ers.usda.gov (Erickson, Ken)
Date: Sat, 10 Jan 2009 08:30:28 -0500
Subject: [R-SIG-Finance] Fw: Testing for cointegration: Johansen
	vsDickey-Fuller
References: <BAY104-DS23A50089FB3C5091B84AECDDD0@phx.gbl>
	<496845F7.5070403@gmail.com>
Message-ID: <9AFA50D47626474F873C93FFFDF3E9D2A571D0@ECONEXVS01.econ.ers.usda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090110/8ac61e62/attachment.pl>

From wuertz at itp.phys.ethz.ch  Sun Jan 11 01:53:27 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 11 Jan 2009 01:53:27 +0100
Subject: [R-SIG-Finance] Economagic data downloads
In-Reply-To: <013001c97291$b09ea0a0$11dbe1e0$@com>
References: <013001c97291$b09ea0a0$11dbe1e0$@com>
Message-ID: <49694307.5060101@itp.phys.ethz.ch>

The functions economagicSeries and economagicImport has been removed 
from the package fImport.

The functions were written many years ago and updated from time to time 
to download data sets from economagic's web site. Since we got aware 
that the download requires subscription we have removed the functions 
from the fImport package and have submitted the new version to CRAN.

Diethelm Wuertz


From ron_michael70 at yahoo.com  Sun Jan 11 08:01:10 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Sat, 10 Jan 2009 23:01:10 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] What is the order of Integration of
	following?
Message-ID: <21396660.post@talk.nabble.com>


Hi, can anyone please tell me what is the order of Integration of following
series : y[t] = 1.1*y[t-1] +epsilon
In naked eye, it looks it is of order 1, because, the coef. is close to 1.
However when I generated the series, it remains non-stationary even after 8
differencing.

y <- vector(length=1000); y[1]<-100; for (i in 2:1000) y[i] <- 1.1*y[i-1] +
rnorm(1)
plot(diff(y, differences = 8))

Also interested to know, does anyone face this kind of DGP in his practical
data analysis?
-- 
View this message in context: http://www.nabble.com/What-is-the-order-of-Integration-of-following--tp21396660p21396660.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From windspeedo at qq.com  Sun Jan 11 12:17:04 2009
From: windspeedo at qq.com (=?ISO-8859-1?B?V2luZA==?=)
Date: Sun, 11 Jan 2009 19:17:04 +0800
Subject: [R-SIG-Finance] How to add grid to plot.zoo easily
Message-ID: <tencent_00923F9876D8264E57577CE4@qq.com>

The grid() could add grids to plot easily.   But the vertical grids are not at the positions of  major ticks if we plot the time series.   
I am satisfied with the major ticks generated by plot.zoo function.    Could we just add grids to plot.zoo according to the major ticks genereated by plot.zoo?    

The following codes work well except that the vertical grids not at the position of major ticks of x axis.

>require(RODBC)
>require(zoo)
>stock1<-getSymbol.RE("SH600036")
>>head(stock1)
               Open     High      Low    Close Volume
2002-04-09 4.288226 4.439191 4.288226 4.349428 101491
2002-04-10 4.349428 4.365749 4.239264 4.324947  16651
2002-04-11 4.324947 4.357588 4.280066 4.292306   5583
2002-04-12 4.284146 4.341268 4.275985 4.312707   5208
2002-04-15 4.312707 4.324947 4.222944 4.239264   4541
2002-04-16 4.222944 4.239264 4.137261 4.145421   6107
> plot(stock1$Close)
> grid()
> 

From windspeedo at qq.com  Sun Jan 11 12:24:21 2009
From: windspeedo at qq.com (Wind2)
Date: Sun, 11 Jan 2009 03:24:21 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] How to add grid to plot.zoo
	easily
In-Reply-To: <tencent_00923F9876D8264E57577CE4@qq.com>
References: <tencent_00923F9876D8264E57577CE4@qq.com>
Message-ID: <21398130.post@talk.nabble.com>


I know the method of plot(,tck=1).   But I don't like this method because the
color of the grids is in "black" except to change the foreground color via
par() which will change the colors of other items, I guess.


Wind2 wrote:
> 
> The grid() could add grids to plot easily.   But the vertical grids are
> not at the positions of  major ticks if we plot the time series.   
> I am satisfied with the major ticks generated by plot.zoo function.   
> Could we just add grids to plot.zoo according to the major ticks
> genereated by plot.zoo?    
> 
> The following codes work well except that the vertical grids not at the
> position of major ticks of x axis.
> 
>>require(RODBC)
>>require(zoo)
>>stock1<-getSymbol.RE("SH600036")
>>>head(stock1)
>                Open     High      Low    Close Volume
> 2002-04-09 4.288226 4.439191 4.288226 4.349428 101491
> 2002-04-10 4.349428 4.365749 4.239264 4.324947  16651
> 2002-04-11 4.324947 4.357588 4.280066 4.292306   5583
> 2002-04-12 4.284146 4.341268 4.275985 4.312707   5208
> 2002-04-15 4.312707 4.324947 4.222944 4.239264   4541
> 2002-04-16 4.222944 4.239264 4.137261 4.145421   6107
>> plot(stock1$Close)
>> grid()
>> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/How-to-add-grid-to-plot.zoo-easily-tp21398072p21398130.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From wuertz at itp.phys.ethz.ch  Sun Jan 11 13:16:27 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 11 Jan 2009 13:16:27 +0100
Subject: [R-SIG-Finance] fPortfolio - Status - New Functionalities ...
In-Reply-To: <19811401A1D8174CB3EAD7F6072E9B5001D0ADD7@chsa1025.share.beluni.net>
References: <19811401A1D8174CB3EAD7F6072E9B5001D0ADD7@chsa1025.share.beluni.net>
Message-ID: <4969E31B.8080905@itp.phys.ethz.ch>

About the current status and new functionalities of Portfolio analysis 
and optimization in Rmetrics.
Hpefully, it would help to clarify some frequently asked questions on 
R-sig-finance

------------

Currently there are two packages on the *** CRAN *** server for 
portfolio optimization: "fPortfolio" which is dependent on another 
portfolio specific package, "fAssets". Please note, the portfolio 
packages rely on the "timeDate" and "timeSeries" packages.


WHAT IS POSSIBLE NOW ?

fPortfolio can currently handle the following two problems:

1. Mean-Variance portfolio optimization with a quadratic objective and 
linear constraints using the "quadprog" solver from R for quadratic 
programming problems. Note, to this class of problems do not belong the 
problems of maximizing the return for a given risk, and also not 
problems with quadratic or nonlinear constraints like portfolios with 
covariance risk budgeting, or 130/30 portfolios. These portfolios are 
more complex and require solvers which can handle quadratic and/or 
nonlinear constraints.

2. Mean-CVaR portfolios which belong to the class of scenario 
optimization problems with a linear objective function and linear 
constraints. The default solver for this kind of portfolio is the solver 
from the package "Rglpk". Again yo cannot solve more complex problems 
which add quadratic or non-linear  constraints.


WHAT IS ONGOING ?

To solve more complex portfolio models we started some time ago to write 
a package called "fPortfolioSolver". We also started to write some  
portfolio specific interface packages named  "Ripop" (interior point 
optimization),  "Rscop" (second order cone programming) among others. 
And we have slightly modified packages like "Rdonlp2",  "RlpsolveAPI" 
among others to support our needs for more complex portfolio 
optimization. We also provided a package "fPortfolioBacktest" including 
functions for rolling backtesting and partly for rolling benchmarking 
and rolling performance analysis. All these packages are under *** 
current development *** . They are therefore not yet or less documented 
and in many cases also less or even not yet fully tested.

*** Nevertheless, we share the code with you on r-forge, here is the link:
*** 
http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/pkg/?root=rmetrics
*** Evenmore, the repository also contains the most recent versions of 
all Rmetrics packages

You should not expect that you get a free portfolio software ready for 
commercial use running without any problems! Rmetrics is open source 
mainly for educational purposes and we have programmed it for *** our 
needs *** and and for *** our fun ***! We are willing to share our 
software code with others who are interested to use part of this code or 
functions for their personal investigations. Look at the packages on 
r-forge as a code reservoir, and feel free to adapt and modify it for 
your personal needs. We are open to any discussions on the software, to 
get recommendations and improvements, and to get any other kind of help. 
And even more important, we are *** dependent *** on financial support 
from you and other parties and institutions which would definitely help 
us to continue Rmetrics, to continue a recently started documentation 
project for portfolio optimization, and to speed up code development by 
financing new Student Internships, Bachelor, Masters and PhD thesis 
projects for Rmetrics. If you like Rmetrics, feel free to tell us your 
wishes and to report us your needs. Please, contact us 
(wuertz[at]phys.ethz.ch) if you like to support Rmetrics in any of  the 
mentioned directions!

We wish you a successful 2009

Diethelm Wuertz


From ggrothendieck at gmail.com  Sun Jan 11 15:20:13 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 11 Jan 2009 09:20:13 -0500
Subject: [R-SIG-Finance] How to add grid to plot.zoo easily
In-Reply-To: <tencent_00923F9876D8264E57577CE4@qq.com>
References: <tencent_00923F9876D8264E57577CE4@qq.com>
Message-ID: <971536df0901110620lfd96c45qf6c795862e1ea823@mail.gmail.com>

That's not reproducible:

> require(RODBC)
Loading required package: RODBC
> require(zoo)
> stock1<-getSymbol.RE("SH600036")
Error: could not find function "getSymbol.RE"

however, I suspect you are referring to the fact that
grid in R does not automatically align with Date and
that is independent of zoo.  The following misaligned
plot does not use zoo:

plot(Sys.Date() + 1:100, 1:100, pch = 20)
grid()

Tested in R version 2.8.1 Patched (2008-12-26 r47350)

Its not clear here whether or not you are really
using zoo but if you are then see the first example
in the answer to zoo FAQ #8 "How are axes
added to a plot created using plot.zoo?" and follow
that code by the following line which does align the grid
properly:

abline(v = m, col = grey(0.8), lty = 2)

The zoo FAQ is available via: vignette("zoo-faq")


On Sun, Jan 11, 2009 at 6:17 AM, Wind <windspeedo at qq.com> wrote:
> The grid() could add grids to plot easily.   But the vertical grids are not at the positions of  major ticks if we plot the time series.
> I am satisfied with the major ticks generated by plot.zoo function.    Could we just add grids to plot.zoo according to the major ticks genereated by plot.zoo?
>
> The following codes work well except that the vertical grids not at the position of major ticks of x axis.
>
>>require(RODBC)
>>require(zoo)
>>stock1<-getSymbol.RE("SH600036")
>>>head(stock1)
>               Open     High      Low    Close Volume
> 2002-04-09 4.288226 4.439191 4.288226 4.349428 101491
> 2002-04-10 4.349428 4.365749 4.239264 4.324947  16651
> 2002-04-11 4.324947 4.357588 4.280066 4.292306   5583
> 2002-04-12 4.284146 4.341268 4.275985 4.312707   5208
> 2002-04-15 4.312707 4.324947 4.222944 4.239264   4541
> 2002-04-16 4.222944 4.239264 4.137261 4.145421   6107
>> plot(stock1$Close)
>> grid()
>>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From windspeedo at qq.com  Sun Jan 11 15:28:51 2009
From: windspeedo at qq.com (=?ISO-8859-1?B?V2luZA==?=)
Date: Sun, 11 Jan 2009 22:28:51 +0800
Subject: [R-SIG-Finance] How to add grid to plot.zoo easily
Message-ID: <tencent_3EFDD1314B1CF0D243467559@qq.com>

Sorry for suppling codes not reproducible.
I think you have taught me enough on this issue:
1.  it is the fact that grid in R does not automatically align with Date and that is independent of zoo.
2.  there is vignette("zoo-faq"), besides vignette("zoo").
3.  FAQ #8 is very useful for me.
Thanks Gabor.

Winds 

 
------------------ Original ------------------
From:  "Gabor Grothendieck"<ggrothendieck at gmail.com>;
Date:  Sun, Jan 11, 2009 10:20 PM
To:  "Wind"<windspeedo at qq.com>; 
Cc:  "r-sig-finance"<r-sig-finance at stat.math.ethz.ch>; 
Subject:  Re: [R-SIG-Finance] How to add grid to plot.zoo easily

 
That's not reproducible:

> require(RODBC)
Loading required package: RODBC
> require(zoo)
> stock1<-getSymbol.RE("SH600036")
Error: could not find function "getSymbol.RE"

however, I suspect you are referring to the fact that
grid in R does not automatically align with Date and
that is independent of zoo.  The following misaligned
plot does not use zoo:

plot(Sys.Date() + 1:100, 1:100, pch = 20)
grid()

Tested in R version 2.8.1 Patched (2008-12-26 r47350)

Its not clear here whether or not you are really
using zoo but if you are then see the first example
in the answer to zoo FAQ #8 "How are axes
added to a plot created using plot.zoo?" and follow
that code by the following line which does align the grid
properly:

abline(v = m, col = grey(0.8), lty = 2)

The zoo FAQ is available via: vignette("zoo-faq")


On Sun, Jan 11, 2009 at 6:17 AM, Wind <windspeedo at qq.com> wrote:
> The grid() could add grids to plot easily.   But the vertical grids are not at the positions of  major ticks if we plot the time series.
> I am satisfied with the major ticks generated by plot.zoo function.    Could we just add grids to plot.zoo according to the major ticks genereated by plot.zoo?
>
> The following codes work well except that the vertical grids not at the position of major ticks of x axis.
>
>>require(RODBC)
>>require(zoo)
>>stock1<-getSymbol.RE("SH600036")
>>>head(stock1)
>               Open     High      Low    Close Volume
> 2002-04-09 4.288226 4.439191 4.288226 4.349428 101491
> 2002-04-10 4.349428 4.365749 4.239264 4.324947  16651
> 2002-04-11 4.324947 4.357588 4.280066 4.292306   5583
> 2002-04-12 4.284146 4.341268 4.275985 4.312707   5208
> 2002-04-15 4.312707 4.324947 4.222944 4.239264   4541
> 2002-04-16 4.222944 4.239264 4.137261 4.145421   6107
>> plot(stock1$Close)
>> grid()
>>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Sun Jan 11 15:43:44 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 11 Jan 2009 09:43:44 -0500
Subject: [R-SIG-Finance] How to add grid to plot.zoo easily
In-Reply-To: <tencent_3EFDD1314B1CF0D243467559@qq.com>
References: <tencent_3EFDD1314B1CF0D243467559@qq.com>
Message-ID: <971536df0901110643g347e061em60e1561ba967a33e@mail.gmail.com>

On Sun, Jan 11, 2009 at 9:28 AM, Wind <windspeedo at qq.com> wrote:
> Sorry for suppling codes not reproducible.
> I think you have taught me enough on this issue:
> 1.  it is the fact that grid in R does not automatically align with Date and that is independent of zoo.
> 2.  there is vignette("zoo-faq"), besides vignette("zoo").

There are actually three zoo vignettes in total.  library(help = zoo)
lists them at the bottom.

> 3.  FAQ #8 is very useful for me.
> Thanks Gabor.
>
> Winds
>
>
> ------------------ Original ------------------
> From:  "Gabor Grothendieck"<ggrothendieck at gmail.com>;
> Date:  Sun, Jan 11, 2009 10:20 PM
> To:  "Wind"<windspeedo at qq.com>;
> Cc:  "r-sig-finance"<r-sig-finance at stat.math.ethz.ch>;
> Subject:  Re: [R-SIG-Finance] How to add grid to plot.zoo easily
>
>
> That's not reproducible:
>
>> require(RODBC)
> Loading required package: RODBC
>> require(zoo)
>> stock1<-getSymbol.RE("SH600036")
> Error: could not find function "getSymbol.RE"
>
> however, I suspect you are referring to the fact that
> grid in R does not automatically align with Date and
> that is independent of zoo.  The following misaligned
> plot does not use zoo:
>
> plot(Sys.Date() + 1:100, 1:100, pch = 20)
> grid()
>
> Tested in R version 2.8.1 Patched (2008-12-26 r47350)
>
> Its not clear here whether or not you are really
> using zoo but if you are then see the first example
> in the answer to zoo FAQ #8 "How are axes
> added to a plot created using plot.zoo?" and follow
> that code by the following line which does align the grid
> properly:
>
> abline(v = m, col = grey(0.8), lty = 2)
>
> The zoo FAQ is available via: vignette("zoo-faq")
>
>
> On Sun, Jan 11, 2009 at 6:17 AM, Wind <windspeedo at qq.com> wrote:
>> The grid() could add grids to plot easily.   But the vertical grids are not at the positions of  major ticks if we plot the time series.
>> I am satisfied with the major ticks generated by plot.zoo function.    Could we just add grids to plot.zoo according to the major ticks genereated by plot.zoo?
>>
>> The following codes work well except that the vertical grids not at the position of major ticks of x axis.
>>
>>>require(RODBC)
>>>require(zoo)
>>>stock1<-getSymbol.RE("SH600036")
>>>>head(stock1)
>>               Open     High      Low    Close Volume
>> 2002-04-09 4.288226 4.439191 4.288226 4.349428 101491
>> 2002-04-10 4.349428 4.365749 4.239264 4.324947  16651
>> 2002-04-11 4.324947 4.357588 4.280066 4.292306   5583
>> 2002-04-12 4.284146 4.341268 4.275985 4.312707   5208
>> 2002-04-15 4.312707 4.324947 4.222944 4.239264   4541
>> 2002-04-16 4.222944 4.239264 4.137261 4.145421   6107
>>> plot(stock1$Close)
>>> grid()
>>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>


From windspeedo at qq.com  Sun Jan 11 15:51:51 2009
From: windspeedo at qq.com (=?ISO-8859-1?B?V2luZA==?=)
Date: Sun, 11 Jan 2009 22:51:51 +0800
Subject: [R-SIG-Finance] How to add grid to plot.zoo easily
Message-ID: <tencent_32AAB7C553E82D186DA0FFBD@qq.com>

quantmod and zoo are two packages I used most frequently. 
It's time for me to study zoo from ABC via these three pdf, I think.
Thanks Gabor.  
 
------------------ Original ------------------
From:  "Gabor Grothendieck"<ggrothendieck at gmail.com>;
Date:  Sun, Jan 11, 2009 10:43 PM
To:  "Wind"<windspeedo at qq.com>; 
Cc:  "r-sig-finance"<r-sig-finance at stat.math.ethz.ch>; 
Subject:  Re: [R-SIG-Finance] How to add grid to plot.zoo easily

 
On Sun, Jan 11, 2009 at 9:28 AM, Wind <windspeedo at qq.com> wrote:
> Sorry for suppling codes not reproducible.
> I think you have taught me enough on this issue:
> 1.  it is the fact that grid in R does not automatically align with Date and that is independent of zoo.
> 2.  there is vignette("zoo-faq"), besides vignette("zoo").

There are actually three zoo vignettes in total.  library(help = zoo)
lists them at the bottom.

> 3.  FAQ #8 is very useful for me.
> Thanks Gabor.
>
> Winds
>
>
> ------------------ Original ------------------
> From:  "Gabor Grothendieck"<ggrothendieck at gmail.com>;
> Date:  Sun, Jan 11, 2009 10:20 PM
> To:  "Wind"<windspeedo at qq.com>;
> Cc:  "r-sig-finance"<r-sig-finance at stat.math.ethz.ch>;
> Subject:  Re: [R-SIG-Finance] How to add grid to plot.zoo easily
>
>
> That's not reproducible:
>
>> require(RODBC)
> Loading required package: RODBC
>> require(zoo)
>> stock1<-getSymbol.RE("SH600036")
> Error: could not find function "getSymbol.RE"
>
> however, I suspect you are referring to the fact that
> grid in R does not automatically align with Date and
> that is independent of zoo.  The following misaligned
> plot does not use zoo:
>
> plot(Sys.Date() + 1:100, 1:100, pch = 20)
> grid()
>
> Tested in R version 2.8.1 Patched (2008-12-26 r47350)
>
> Its not clear here whether or not you are really
> using zoo but if you are then see the first example
> in the answer to zoo FAQ #8 "How are axes
> added to a plot created using plot.zoo?" and follow
> that code by the following line which does align the grid
> properly:
>
> abline(v = m, col = grey(0.8), lty = 2)
>
> The zoo FAQ is available via: vignette("zoo-faq")
>
>
> On Sun, Jan 11, 2009 at 6:17 AM, Wind <windspeedo at qq.com> wrote:
>> The grid() could add grids to plot easily.   But the vertical grids are not at the positions of  major ticks if we plot the time series.
>> I am satisfied with the major ticks generated by plot.zoo function.    Could we just add grids to plot.zoo according to the major ticks genereated by plot.zoo?
>>
>> The following codes work well except that the vertical grids not at the position of major ticks of x axis.
>>
>>>require(RODBC)
>>>require(zoo)
>>>stock1<-getSymbol.RE("SH600036")
>>>>head(stock1)
>>               Open     High      Low    Close Volume
>> 2002-04-09 4.288226 4.439191 4.288226 4.349428 101491
>> 2002-04-10 4.349428 4.365749 4.239264 4.324947  16651
>> 2002-04-11 4.324947 4.357588 4.280066 4.292306   5583
>> 2002-04-12 4.284146 4.341268 4.275985 4.312707   5208
>> 2002-04-15 4.312707 4.324947 4.222944 4.239264   4541
>> 2002-04-16 4.222944 4.239264 4.137261 4.145421   6107
>>> plot(stock1$Close)
>>> grid()
>>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>


From ggrothendieck at gmail.com  Sun Jan 11 16:01:01 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 11 Jan 2009 10:01:01 -0500
Subject: [R-SIG-Finance] How to add grid to plot.zoo easily
In-Reply-To: <tencent_32AAB7C553E82D186DA0FFBD@qq.com>
References: <tencent_32AAB7C553E82D186DA0FFBD@qq.com>
Message-ID: <971536df0901110701x12ad3b1bubc13559f0c53791c@mail.gmail.com>

Note that quantmod has specific charting capabilities for
OHLC series that you might be interested in:

library(quantmod)
getSymbols("IBM")
chartSeries(IBM)


On Sun, Jan 11, 2009 at 9:51 AM, Wind <windspeedo at qq.com> wrote:
> quantmod and zoo are two packages I used most frequently.
> It's time for me to study zoo from ABC via these three pdf, I think.
> Thanks Gabor.
>
> ------------------ Original ------------------
> From:  "Gabor Grothendieck"<ggrothendieck at gmail.com>;
> Date:  Sun, Jan 11, 2009 10:43 PM
> To:  "Wind"<windspeedo at qq.com>;
> Cc:  "r-sig-finance"<r-sig-finance at stat.math.ethz.ch>;
> Subject:  Re: [R-SIG-Finance] How to add grid to plot.zoo easily
>
>
> On Sun, Jan 11, 2009 at 9:28 AM, Wind <windspeedo at qq.com> wrote:
>> Sorry for suppling codes not reproducible.
>> I think you have taught me enough on this issue:
>> 1.  it is the fact that grid in R does not automatically align with Date and that is independent of zoo.
>> 2.  there is vignette("zoo-faq"), besides vignette("zoo").
>
> There are actually three zoo vignettes in total.  library(help = zoo)
> lists them at the bottom.
>
>> 3.  FAQ #8 is very useful for me.
>> Thanks Gabor.
>>
>> Winds
>>
>>
>> ------------------ Original ------------------
>> From:  "Gabor Grothendieck"<ggrothendieck at gmail.com>;
>> Date:  Sun, Jan 11, 2009 10:20 PM
>> To:  "Wind"<windspeedo at qq.com>;
>> Cc:  "r-sig-finance"<r-sig-finance at stat.math.ethz.ch>;
>> Subject:  Re: [R-SIG-Finance] How to add grid to plot.zoo easily
>>
>>
>> That's not reproducible:
>>
>>> require(RODBC)
>> Loading required package: RODBC
>>> require(zoo)
>>> stock1<-getSymbol.RE("SH600036")
>> Error: could not find function "getSymbol.RE"
>>
>> however, I suspect you are referring to the fact that
>> grid in R does not automatically align with Date and
>> that is independent of zoo.  The following misaligned
>> plot does not use zoo:
>>
>> plot(Sys.Date() + 1:100, 1:100, pch = 20)
>> grid()
>>
>> Tested in R version 2.8.1 Patched (2008-12-26 r47350)
>>
>> Its not clear here whether or not you are really
>> using zoo but if you are then see the first example
>> in the answer to zoo FAQ #8 "How are axes
>> added to a plot created using plot.zoo?" and follow
>> that code by the following line which does align the grid
>> properly:
>>
>> abline(v = m, col = grey(0.8), lty = 2)
>>
>> The zoo FAQ is available via: vignette("zoo-faq")
>>
>>
>> On Sun, Jan 11, 2009 at 6:17 AM, Wind <windspeedo at qq.com> wrote:
>>> The grid() could add grids to plot easily.   But the vertical grids are not at the positions of  major ticks if we plot the time series.
>>> I am satisfied with the major ticks generated by plot.zoo function.    Could we just add grids to plot.zoo according to the major ticks genereated by plot.zoo?
>>>
>>> The following codes work well except that the vertical grids not at the position of major ticks of x axis.
>>>
>>>>require(RODBC)
>>>>require(zoo)
>>>>stock1<-getSymbol.RE("SH600036")
>>>>>head(stock1)
>>>               Open     High      Low    Close Volume
>>> 2002-04-09 4.288226 4.439191 4.288226 4.349428 101491
>>> 2002-04-10 4.349428 4.365749 4.239264 4.324947  16651
>>> 2002-04-11 4.324947 4.357588 4.280066 4.292306   5583
>>> 2002-04-12 4.284146 4.341268 4.275985 4.312707   5208
>>> 2002-04-15 4.312707 4.324947 4.222944 4.239264   4541
>>> 2002-04-16 4.222944 4.239264 4.137261 4.145421   6107
>>>> plot(stock1$Close)
>>>> grid()
>>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>
>


From windspeedo at qq.com  Sun Jan 11 16:20:48 2009
From: windspeedo at qq.com (=?ISO-8859-1?B?V2luZA==?=)
Date: Sun, 11 Jan 2009 23:20:48 +0800
Subject: [R-SIG-Finance] How to add grid to plot.zoo easily
Message-ID: <tencent_0107741365D54A8C15FB807C@qq.com>

The amazing effect of chartSeries shocked me about 2 month ago when I used R for the first time.   Quantmod is the first package I used.    It demonstrate the power of R well to layman as me.     I always saw zoo loading when I loaded quantmod.   :)

Thanks Gabor. 

 
 
------------------ Original ------------------
From:  "Gabor Grothendieck"<ggrothendieck at gmail.com>;
Date:  Sun, Jan 11, 2009 11:01 PM
To:  "Wind"<windspeedo at qq.com>; 
Cc:  "r-sig-finance"<r-sig-finance at stat.math.ethz.ch>; 
Subject:  Re: [R-SIG-Finance] How to add grid to plot.zoo easily

 
Note that quantmod has specific charting capabilities for
OHLC series that you might be interested in:

library(quantmod)
getSymbols("IBM")
chartSeries(IBM)


On Sun, Jan 11, 2009 at 9:51 AM, Wind <windspeedo at qq.com> wrote:
> quantmod and zoo are two packages I used most frequently.
> It's time for me to study zoo from ABC via these three pdf, I think.
> Thanks Gabor.
>
> ------------------ Original ------------------
> From:  "Gabor Grothendieck"<ggrothendieck at gmail.com>;
> Date:  Sun, Jan 11, 2009 10:43 PM
> To:  "Wind"<windspeedo at qq.com>;
> Cc:  "r-sig-finance"<r-sig-finance at stat.math.ethz.ch>;
> Subject:  Re: [R-SIG-Finance] How to add grid to plot.zoo easily
>
>
> On Sun, Jan 11, 2009 at 9:28 AM, Wind <windspeedo at qq.com> wrote:
>> Sorry for suppling codes not reproducible.
>> I think you have taught me enough on this issue:
>> 1.  it is the fact that grid in R does not automatically align with Date and that is independent of zoo.
>> 2.  there is vignette("zoo-faq"), besides vignette("zoo").
>
> There are actually three zoo vignettes in total.  library(help = zoo)
> lists them at the bottom.
>
>> 3.  FAQ #8 is very useful for me.
>> Thanks Gabor.
>>
>> Winds
>>
>>
>> ------------------ Original ------------------
>> From:  "Gabor Grothendieck"<ggrothendieck at gmail.com>;
>> Date:  Sun, Jan 11, 2009 10:20 PM
>> To:  "Wind"<windspeedo at qq.com>;
>> Cc:  "r-sig-finance"<r-sig-finance at stat.math.ethz.ch>;
>> Subject:  Re: [R-SIG-Finance] How to add grid to plot.zoo easily
>>
>>
>> That's not reproducible:
>>
>>> require(RODBC)
>> Loading required package: RODBC
>>> require(zoo)
>>> stock1<-getSymbol.RE("SH600036")
>> Error: could not find function "getSymbol.RE"
>>
>> however, I suspect you are referring to the fact that
>> grid in R does not automatically align with Date and
>> that is independent of zoo.  The following misaligned
>> plot does not use zoo:
>>
>> plot(Sys.Date() + 1:100, 1:100, pch = 20)
>> grid()
>>
>> Tested in R version 2.8.1 Patched (2008-12-26 r47350)
>>
>> Its not clear here whether or not you are really
>> using zoo but if you are then see the first example
>> in the answer to zoo FAQ #8 "How are axes
>> added to a plot created using plot.zoo?" and follow
>> that code by the following line which does align the grid
>> properly:
>>
>> abline(v = m, col = grey(0.8), lty = 2)
>>
>> The zoo FAQ is available via: vignette("zoo-faq")
>>
>>
>> On Sun, Jan 11, 2009 at 6:17 AM, Wind <windspeedo at qq.com> wrote:
>>> The grid() could add grids to plot easily.   But the vertical grids are not at the positions of  major ticks if we plot the time series.
>>> I am satisfied with the major ticks generated by plot.zoo function.    Could we just add grids to plot.zoo according to the major ticks genereated by plot.zoo?
>>>
>>> The following codes work well except that the vertical grids not at the position of major ticks of x axis.
>>>
>>>>require(RODBC)
>>>>require(zoo)
>>>>stock1<-getSymbol.RE("SH600036")
>>>>>head(stock1)
>>>               Open     High      Low    Close Volume
>>> 2002-04-09 4.288226 4.439191 4.288226 4.349428 101491
>>> 2002-04-10 4.349428 4.365749 4.239264 4.324947  16651
>>> 2002-04-11 4.324947 4.357588 4.280066 4.292306   5583
>>> 2002-04-12 4.284146 4.341268 4.275985 4.312707   5208
>>> 2002-04-15 4.312707 4.324947 4.222944 4.239264   4541
>>> 2002-04-16 4.222944 4.239264 4.137261 4.145421   6107
>>>> plot(stock1$Close)
>>>> grid()
>>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>
>
>


From bogaso.christofer at gmail.com  Mon Jan 12 02:12:19 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Sun, 11 Jan 2009 17:12:19 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] A question on Unit root
Message-ID: <21406847.post@talk.nabble.com>


In my textbook, I read that for a Integrated with order 1 process, there
would be atleast one unit root. But if I take following DGP : y[t] =
1.001y[t-1] + epsilon then what I found was, this is a Integrated with order
1 process, because, data generated from this process becomes stationary
after 1 difference. However there is, strictly speaking, no unit root. The
Ch. polynomial has root as 0.999001, this is not exactly one.Or I should
consider 0.999001 as 1?

Can you give a better clarification pls?
-- 
View this message in context: http://www.nabble.com/A-question-on-Unit-root-tp21406847p21406847.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From bogaso.christofer at gmail.com  Mon Jan 12 02:25:00 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Sun, 11 Jan 2009 17:25:00 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Fw: Testing for cointegration:
 Johansen vs Dickey-Fuller
In-Reply-To: <BAY104-DS23A50089FB3C5091B84AECDDD0@phx.gbl>
References: <BAY104-DS23A50089FB3C5091B84AECDDD0@phx.gbl>
Message-ID: <21406957.post@talk.nabble.com>


I have one question. What is the point to keep constant in cointegration
euqation? I think you should consider zero intercept in cointegrating
equation.



Jae Kim-3 wrote:
> 
> From: "Jae Kim" <jh8080 at hotmail.com>
> Sent: Saturday, January 10, 2009 10:04 AM
> To: "Paul Teetor" <paulteetor at yahoo.com>
> Subject: Re: [R-SIG-Finance] Testing for cointegration: Johansen vs 
> Dickey-Fuller
> 
>> Hi,
>>
>> 1. If you are using the ADF test here, you are giving the restriction
>> that 
>> the  cointegrating vector between the two is (1, -1.2534). That is, you 
>> are saying that the two variables are related in the long run with the 
>> cointegrating vector given. Under this restriction, you find the spread 
>> stationary, so they are cointegrated with given cointegrating vector.
>>
>> 2. If you are using Johansen method, you are doing unrestricted
>> estimation 
>> of cointegrating vector. But if you believe that the above restriction is 
>> sensible economically, the ADF result should be preferred to Johansen 
>> result.
>>
>> 3. This is the bivariate case, so Johansen method may not be necessary. 
>> try Engle-Granger 2-stage method, you might find cointegration. In 
>> addition, Johansen method assumes normality, which may often be violated.
>>
>> hope this helps. JHK
>>
>>
>> --------------------------------------------------
>> From: "Paul Teetor" <paulteetor at yahoo.com>
>> Sent: Saturday, January 10, 2009 8:38 AM
>> To: <r-sig-finance at stat.math.ethz.ch>
>> Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs 
>> Dickey-Fuller
>>
>>> R SIG Finance readers:
>>>
>>> I am checking a futures spread for mean reversion.  I am using the 
>>> Johansen
>>> test (ca.jo) for cointegration and the Augmented Dickey-Fuller test 
>>> (ur.df)
>>> for mean reversion.
>>>
>>> Here is the odd part:  The Johansen test says the two futures prices are 
>>> not
>>> cointegrated, but the ADF test says the spread is, in fact, 
>>> mean-reverting.
>>>
>>> I am very puzzled.  The spread is a linear combination of the prices,
>>> and
>>> the ADF test says it is mean-reverting.  But the failed Johansen test 
>>> says
>>> the prices are not cointegrated, so no linear combination of prices is
>>> mean-reverting.  Huh??
>>>
>>> I would be very grateful is someone could suggest where I went wrong, or
>>> steer me towards some relevent reference materials.
>>>
>>>
>>> Background:  I am studying the spread between TY futures (10-year US
>>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>>
>>>    sprd = ty - (1.2534 * sr)
>>>
>>> where ty and sr are the time series of futures prices.  (The 1.2534 
>>> factor
>>> is from an ordinary least squares fit.)  I execute the Johansen
>>> procedure
>>> this way:
>>>
>>>    ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")
>>>
>>> The summary of the test result is:
>>>
>>> ######################
>>> # Johansen-Procedure #
>>> ######################
>>>
>>> Test type: maximal eigenvalue statistic (lambda max) , without
>>> linear trend and constant in cointegration
>>>
>>> Eigenvalues (lambda):
>>> [1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>>
>>> Values of teststatistic and critical values of test:
>>>
>>>          test 10pct  5pct  1pct
>>> r <= 1 | 2.00  7.52  9.24 12.97
>>> r = 0  | 8.89 13.75 15.67 20.20
>>>
>>> <snip>
>>>
>>> I interpret the "r <= 1" line this way:  The test statistic for r <= 1
>>> is
>>> below the critical values, hence we cannot reject the null hypothesis 
>>> that
>>> the rank is less than 2.  We conclude that the two time series are not
>>> cointegrated.
>>>
>>> I run the ADF test this way:
>>>
>>> ur.df(sprd, type="drift")
>>>
>>> (I set type="drift" because that seems to correspond to ecdet="const"
>>> for
>>> the Johansen test.)  The summary of the ADF test is:
>>>
>>> ###############################################
>>> # Augmented Dickey-Fuller Test Unit Root Test #
>>> ###############################################
>>>
>>> Test regression drift
>>>
>>> <snip>
>>>
>>> Value of test-statistic is: -2.9624 4.4142
>>>
>>> Critical values for test statistics:
>>> 1pct  5pct 10pct
>>> tau2 -3.43 -2.86 -2.57
>>> phi1  6.43  4.59  3.78
>>>
>>> I interpret the test statistics as meaning we can reject the null 
>>> hypothesis
>>> of a unit root (at a confidence level of 90% or better), hence the
>>> spread 
>>> is
>>> mean-reverting.  I get similar results from the adf.test() procedure.
>>>
>>> F.Y.I., I am running version 2.6.2 of R.
>>>
>>> Paul Teetor
>>> Elgin, IL   USA
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Fw%3A-Testing-for-cointegration%3A-Johansen-vs-Dickey-Fuller-tp21382220p21406957.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jh8080 at hotmail.com  Mon Jan 12 03:01:08 2009
From: jh8080 at hotmail.com (Jae Kim)
Date: Mon, 12 Jan 2009 02:01:08 +0000
Subject: [R-SIG-Finance] [R-sig-finance] Fw: Testing for cointegration:
 Johansen vs Dickey-Fuller
In-Reply-To: <21406957.post@talk.nabble.com>
References: <BAY104-DS23A50089FB3C5091B84AECDDD0@phx.gbl>
	<21406957.post@talk.nabble.com>
Message-ID: <BAY104-W56533F13152FC9047A058ECDD80@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090112/e2d50228/attachment.pl>

From bogaso.christofer at gmail.com  Mon Jan 12 05:18:04 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Sun, 11 Jan 2009 20:18:04 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Fw: Testing for cointegration:
 Johansen vs Dickey-Fuller
In-Reply-To: <21406957.post@talk.nabble.com>
References: <BAY104-DS23A50089FB3C5091B84AECDDD0@phx.gbl>
	<21406957.post@talk.nabble.com>
Message-ID: <21408233.post@talk.nabble.com>


I feel whether the form Y = a + bX will be taken or zero-intercept form will
be taken should be entirely based on economic theory, not from a regression
analysis. Because, in this case, as both series are non-stationary, it is
not legitimate to infer anything on the coef.



Bogaso wrote:
> 
> I have one question. What is the point to keep constant in cointegration
> euqation? I think you should consider zero intercept in cointegrating
> equation.
> 
> 
> 
> Jae Kim-3 wrote:
>> 
>> From: "Jae Kim" <jh8080 at hotmail.com>
>> Sent: Saturday, January 10, 2009 10:04 AM
>> To: "Paul Teetor" <paulteetor at yahoo.com>
>> Subject: Re: [R-SIG-Finance] Testing for cointegration: Johansen vs 
>> Dickey-Fuller
>> 
>>> Hi,
>>>
>>> 1. If you are using the ADF test here, you are giving the restriction
>>> that 
>>> the  cointegrating vector between the two is (1, -1.2534). That is, you 
>>> are saying that the two variables are related in the long run with the 
>>> cointegrating vector given. Under this restriction, you find the spread 
>>> stationary, so they are cointegrated with given cointegrating vector.
>>>
>>> 2. If you are using Johansen method, you are doing unrestricted
>>> estimation 
>>> of cointegrating vector. But if you believe that the above restriction
>>> is 
>>> sensible economically, the ADF result should be preferred to Johansen 
>>> result.
>>>
>>> 3. This is the bivariate case, so Johansen method may not be necessary. 
>>> try Engle-Granger 2-stage method, you might find cointegration. In 
>>> addition, Johansen method assumes normality, which may often be
>>> violated.
>>>
>>> hope this helps. JHK
>>>
>>>
>>> --------------------------------------------------
>>> From: "Paul Teetor" <paulteetor at yahoo.com>
>>> Sent: Saturday, January 10, 2009 8:38 AM
>>> To: <r-sig-finance at stat.math.ethz.ch>
>>> Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs 
>>> Dickey-Fuller
>>>
>>>> R SIG Finance readers:
>>>>
>>>> I am checking a futures spread for mean reversion.  I am using the 
>>>> Johansen
>>>> test (ca.jo) for cointegration and the Augmented Dickey-Fuller test 
>>>> (ur.df)
>>>> for mean reversion.
>>>>
>>>> Here is the odd part:  The Johansen test says the two futures prices
>>>> are 
>>>> not
>>>> cointegrated, but the ADF test says the spread is, in fact, 
>>>> mean-reverting.
>>>>
>>>> I am very puzzled.  The spread is a linear combination of the prices,
>>>> and
>>>> the ADF test says it is mean-reverting.  But the failed Johansen test 
>>>> says
>>>> the prices are not cointegrated, so no linear combination of prices is
>>>> mean-reverting.  Huh??
>>>>
>>>> I would be very grateful is someone could suggest where I went wrong,
>>>> or
>>>> steer me towards some relevent reference materials.
>>>>
>>>>
>>>> Background:  I am studying the spread between TY futures (10-year US
>>>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>>>
>>>>    sprd = ty - (1.2534 * sr)
>>>>
>>>> where ty and sr are the time series of futures prices.  (The 1.2534 
>>>> factor
>>>> is from an ordinary least squares fit.)  I execute the Johansen
>>>> procedure
>>>> this way:
>>>>
>>>>    ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")
>>>>
>>>> The summary of the test result is:
>>>>
>>>> ######################
>>>> # Johansen-Procedure #
>>>> ######################
>>>>
>>>> Test type: maximal eigenvalue statistic (lambda max) , without
>>>> linear trend and constant in cointegration
>>>>
>>>> Eigenvalues (lambda):
>>>> [1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>>>
>>>> Values of teststatistic and critical values of test:
>>>>
>>>>          test 10pct  5pct  1pct
>>>> r <= 1 | 2.00  7.52  9.24 12.97
>>>> r = 0  | 8.89 13.75 15.67 20.20
>>>>
>>>> <snip>
>>>>
>>>> I interpret the "r <= 1" line this way:  The test statistic for r <= 1
>>>> is
>>>> below the critical values, hence we cannot reject the null hypothesis 
>>>> that
>>>> the rank is less than 2.  We conclude that the two time series are not
>>>> cointegrated.
>>>>
>>>> I run the ADF test this way:
>>>>
>>>> ur.df(sprd, type="drift")
>>>>
>>>> (I set type="drift" because that seems to correspond to ecdet="const"
>>>> for
>>>> the Johansen test.)  The summary of the ADF test is:
>>>>
>>>> ###############################################
>>>> # Augmented Dickey-Fuller Test Unit Root Test #
>>>> ###############################################
>>>>
>>>> Test regression drift
>>>>
>>>> <snip>
>>>>
>>>> Value of test-statistic is: -2.9624 4.4142
>>>>
>>>> Critical values for test statistics:
>>>> 1pct  5pct 10pct
>>>> tau2 -3.43 -2.86 -2.57
>>>> phi1  6.43  4.59  3.78
>>>>
>>>> I interpret the test statistics as meaning we can reject the null 
>>>> hypothesis
>>>> of a unit root (at a confidence level of 90% or better), hence the
>>>> spread 
>>>> is
>>>> mean-reverting.  I get similar results from the adf.test() procedure.
>>>>
>>>> F.Y.I., I am running version 2.6.2 of R.
>>>>
>>>> Paul Teetor
>>>> Elgin, IL   USA
>>>>
>>>> _______________________________________________
>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>> -- Subscriber-posting only.
>>>> -- If you want to post, subscribe first.
>>>>
>> 
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>> 
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/Fw%3A-Testing-for-cointegration%3A-Johansen-vs-Dickey-Fuller-tp21382220p21408233.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From paulteetor at yahoo.com  Mon Jan 12 05:56:48 2009
From: paulteetor at yahoo.com (Paul Teetor)
Date: Sun, 11 Jan 2009 22:56:48 -0600
Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs
	Dickey-Fuller
In-Reply-To: <540297A816C146FD8AB916967EDC317E@XI>
References: <540297A816C146FD8AB916967EDC317E@XI>
Message-ID: <DECB29055B6149D78D7C15715D96C807@XI>

All:

Many, many thanks to Mark, Brian, Jae, Ken, Matthieu, and Silika for the
thoughtful comments which they posted to the list.  I am very grateful for
your insights.  I will be studying your replies and following-up on what I
learn.

Here is a summary of some major points:

- Testing for cointegration and unit roots is an art, just like building any
significant statistical model.  The cointegration and unit root tests are
not as mechanical as I thought.

- My problem is a (relatively) simple bivariate case, therefore the
Engle-Granger or Philips-Ouliaris procedures could be more appropriate.  The
Johansen procedure is probably overkill.

- The bivariate tests will likely be more consistent with the Dickey-Fuller
test results.

- I need to decide if the OLS fit is really a meaningful long-run
cointegration vector.  If so, my modelling and testing process changes.

- I need to carefully construct my model and consider issues such as long-
and short-run modelling, choice of predictor versus response, number of
lags, trend versus no-trend, etc.

- I also need to carefully check the model, especially for significance.

- I definitely need to get Bernhard Pfaff's book (2nd ed.).

- Bernhard Pfaff's vignette for the VAR package contains useful information,
too.

Again, thanks to everyone who contributed to this discussion.

Paul


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Paul Teetor
Sent: Friday, January 09, 2009 3:38 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs
Dickey-Fuller

R SIG Finance readers:
 
I am checking a futures spread for mean reversion.  I am using the Johansen
test (ca.jo) for cointegration and the Augmented Dickey-Fuller test (ur.df)
for mean reversion.

Here is the odd part:  The Johansen test says the two futures prices are not
cointegrated, but the ADF test says the spread is, in fact, mean-reverting.
 
I am very puzzled.  The spread is a linear combination of the prices, and
the ADF test says it is mean-reverting.  But the failed Johansen test says
the prices are not cointegrated, so no linear combination of prices is
mean-reverting.  Huh??
 
I would be very grateful is someone could suggest where I went wrong, or
steer me towards some relevent reference materials.

 
Background:  I am studying the spread between TY futures (10-year US
Treasurys) and SR futures (10-year US swap rate), calculated as:
 
    sprd = ty - (1.2534 * sr)
 
where ty and sr are the time series of futures prices.  (The 1.2534 factor
is from an ordinary least squares fit.)  I execute the Johansen procedure
this way:
 
    ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")
 
The summary of the test result is:

	###################### 
	# Johansen-Procedure # 
	###################### 

	Test type: maximal eigenvalue statistic (lambda max) , without
linear trend and constant in 	cointegration 

	Eigenvalues (lambda):
	[1]  2.929702e-03  6.616599e-04 -1.001412e-17

	Values of teststatistic and critical values of test:

	         test 10pct  5pct  1pct
	r <= 1 | 2.00  7.52  9.24 12.97
	r = 0  | 8.89 13.75 15.67 20.20

	<snip>

I interpret the "r <= 1" line this way:  The test statistic for r <= 1 is
below the critical values, hence we cannot reject the null hypothesis that
the rank is less than 2.  We conclude that the two time series are not
cointegrated.

I run the ADF test this way:

	ur.df(sprd, type="drift")

(I set type="drift" because that seems to correspond to ecdet="const" for
the Johansen test.)  The summary of the ADF test is:

	###############################################
	# Augmented Dickey-Fuller Test Unit Root Test #
###############################################

	Test regression drift

	<snip>

	Value of test-statistic is: -2.9624 4.4142

	Critical values for test statistics:
		1pct  5pct 10pct
	tau2 -3.43 -2.86 -2.57
	phi1  6.43  4.59  3.78 

I interpret the test statistics as meaning we can reject the null hypothesis
of a unit root (at a confidence level of 90% or better), hence the spread is
mean-reverting.  I get similar results from the adf.test() procedure.

F.Y.I., I am running version 2.6.2 of R.
 
Paul Teetor
Elgin, IL   USA

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From markleeds at verizon.net  Mon Jan 12 06:45:54 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sun, 11 Jan 2009 23:45:54 -0600 (CST)
Subject: [R-SIG-Finance] [R-sig-finance] Fw: Testing for cointegration:
 Johansen vs Dickey-Fuller
Message-ID: <1706209495.41259921231739154618.JavaMail.javamailuser@localhost>

  but why restrict A to be zero. the point is to see whether the 
regression residuals are stationary ( this determines whether the two 
series
are cointegrated.  also, did you test that they both have a unit root 
first ? ). if non zero estimate of the intercept makes the residuals  be 
more stationary from a hypothesis testing standpoint, then you may as 
well include A in the initial regresssion. I say that partally based
on intuition in that if the intercept really is zero, A should get 
estimated near zero but it's also based on guess work.

on the other hand,  testing whether  any series has a unit root ( the 
dickey fuller stuff ) is where inclusion or not of the intercept gets 
very tricky. hamilton has a very nice chapter on that. i think 17. i'm 
pretty sure though that including the intercept in the cointegrating 
regression itself  can't hurt you. maybe someone can confirm or 
unconfirm this ? it's been a while since I looked at this material.




On Sun, Jan 11, 2009 at 11:18 PM, Bogaso wrote:

> I feel whether the form Y = a + bX will be taken or zero-intercept 
> form will
> be taken should be entirely based on economic theory, not from a 
> regression
> analysis. Because, in this case, as both series are non-stationary, it 
> is
> not legitimate to infer anything on the coef.
>
>
>
> Bogaso wrote:
>>
>> I have one question. What is the point to keep constant in 
>> cointegration
>> euqation? I think you should consider zero intercept in cointegrating
>> equation.
>>
>>
>>
>> Jae Kim-3 wrote:
>>>
>>> From: "Jae Kim" <jh8080 at hotmail.com>
>>> Sent: Saturday, January 10, 2009 10:04 AM
>>> To: "Paul Teetor" <paulteetor at yahoo.com>
>>> Subject: Re: [R-SIG-Finance] Testing for cointegration: Johansen vs 
>>> Dickey-Fuller
>>>
>>>> Hi,
>>>>
>>>> 1. If you are using the ADF test here, you are giving the 
>>>> restriction
>>>> that the  cointegrating vector between the two is (1, -1.2534). 
>>>> That is, you are saying that the two variables are related in the 
>>>> long run with the cointegrating vector given. Under this 
>>>> restriction, you find the spread stationary, so they are 
>>>> cointegrated with given cointegrating vector.
>>>>
>>>> 2. If you are using Johansen method, you are doing unrestricted
>>>> estimation of cointegrating vector. But if you believe that the 
>>>> above restriction
>>>> is sensible economically, the ADF result should be preferred to 
>>>> Johansen result.
>>>>
>>>> 3. This is the bivariate case, so Johansen method may not be 
>>>> necessary. try Engle-Granger 2-stage method, you might find 
>>>> cointegration. In addition, Johansen method assumes normality, 
>>>> which may often be
>>>> violated.
>>>>
>>>> hope this helps. JHK
>>>>
>>>>
>>>> --------------------------------------------------
>>>> From: "Paul Teetor" <paulteetor at yahoo.com>
>>>> Sent: Saturday, January 10, 2009 8:38 AM
>>>> To: <r-sig-finance at stat.math.ethz.ch>
>>>> Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs 
>>>> Dickey-Fuller
>>>>
>>>>> R SIG Finance readers:
>>>>>
>>>>> I am checking a futures spread for mean reversion.  I am using the 
>>>>> Johansen
>>>>> test (ca.jo) for cointegration and the Augmented Dickey-Fuller 
>>>>> test (ur.df)
>>>>> for mean reversion.
>>>>>
>>>>> Here is the odd part:  The Johansen test says the two futures 
>>>>> prices
>>>>> are not
>>>>> cointegrated, but the ADF test says the spread is, in fact, 
>>>>> mean-reverting.
>>>>>
>>>>> I am very puzzled.  The spread is a linear combination of the 
>>>>> prices,
>>>>> and
>>>>> the ADF test says it is mean-reverting.  But the failed Johansen 
>>>>> test says
>>>>> the prices are not cointegrated, so no linear combination of 
>>>>> prices is
>>>>> mean-reverting.  Huh??
>>>>>
>>>>> I would be very grateful is someone could suggest where I went 
>>>>> wrong,
>>>>> or
>>>>> steer me towards some relevent reference materials.
>>>>>
>>>>>
>>>>> Background:  I am studying the spread between TY futures (10-year 
>>>>> US
>>>>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>>>>
>>>>>    sprd = ty - (1.2534 * sr)
>>>>>
>>>>> where ty and sr are the time series of futures prices.  (The 
>>>>> 1.2534 factor
>>>>> is from an ordinary least squares fit.)  I execute the Johansen
>>>>> procedure
>>>>> this way:
>>>>>
>>>>>    ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")
>>>>>
>>>>> The summary of the test result is:
>>>>>
>>>>> ######################
>>>>> # Johansen-Procedure #
>>>>> ######################
>>>>>
>>>>> Test type: maximal eigenvalue statistic (lambda max) , without
>>>>> linear trend and constant in cointegration
>>>>>
>>>>> Eigenvalues (lambda):
>>>>> [1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>>>>
>>>>> Values of teststatistic and critical values of test:
>>>>>
>>>>>          test 10pct  5pct  1pct
>>>>> r <= 1 | 2.00  7.52  9.24 12.97
>>>>> r = 0  | 8.89 13.75 15.67 20.20
>>>>>
>>>>> <snip>
>>>>>
>>>>> I interpret the "r <= 1" line this way:  The test statistic for r 
>>>>> <= 1
>>>>> is
>>>>> below the critical values, hence we cannot reject the null 
>>>>> hypothesis that
>>>>> the rank is less than 2.  We conclude that the two time series are 
>>>>> not
>>>>> cointegrated.
>>>>>
>>>>> I run the ADF test this way:
>>>>>
>>>>> ur.df(sprd, type="drift")
>>>>>
>>>>> (I set type="drift" because that seems to correspond to 
>>>>> ecdet="const"
>>>>> for
>>>>> the Johansen test.)  The summary of the ADF test is:
>>>>>
>>>>> ###############################################
>>>>> # Augmented Dickey-Fuller Test Unit Root Test #
>>>>> ###############################################
>>>>>
>>>>> Test regression drift
>>>>>
>>>>> <snip>
>>>>>
>>>>> Value of test-statistic is: -2.9624 4.4142
>>>>>
>>>>> Critical values for test statistics:
>>>>> 1pct  5pct 10pct
>>>>> tau2 -3.43 -2.86 -2.57
>>>>> phi1  6.43  4.59  3.78
>>>>>
>>>>> I interpret the test statistics as meaning we can reject the null 
>>>>> hypothesis
>>>>> of a unit root (at a confidence level of 90% or better), hence the
>>>>> spread is
>>>>> mean-reverting.  I get similar results from the adf.test() 
>>>>> procedure.
>>>>>
>>>>> F.Y.I., I am running version 2.6.2 of R.
>>>>>
>>>>> Paul Teetor
>>>>> Elgin, IL   USA
>>>>>
>>>>> _______________________________________________
>>>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>> -- Subscriber-posting only.
>>>>> -- If you want to post, subscribe first.
>>>>>
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>
>>
>>
>
> -- 
> View this message in context: 
> http://www.nabble.com/Fw%3A-Testing-for-cointegration%3A-Johansen-vs-Dickey-Fuller-tp21382220p21408233.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From j_cuisinier at hotmail.com  Mon Jan 12 09:26:28 2009
From: j_cuisinier at hotmail.com (julien cuisinier)
Date: Mon, 12 Jan 2009 09:26:28 +0100
Subject: [R-SIG-Finance] FW: Covariance in R - wrong?
Message-ID: <COL102-W36CDFD5E3A228A921A11C08FD80@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090112/465ce0b0/attachment.pl>

From cgb at datanalytics.com  Mon Jan 12 09:45:02 2009
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Mon, 12 Jan 2009 09:45:02 +0100
Subject: [R-SIG-Finance] FW: Covariance in R - wrong?
In-Reply-To: <COL102-W36CDFD5E3A228A921A11C08FD80@phx.gbl>
References: <COL102-W36CDFD5E3A228A921A11C08FD80@phx.gbl>
Message-ID: <1231749902.10163.16.camel@kropotkin>

From

?cov

"The denominator n - 1 is used which gives an unbiased estimator of the
(co)variance for i.i.d. observations."

As you see, it tells you what the denominator is... and why.

best regards,

Carlos J. Gil Bellosta
http://www.datanalytics.com


On Mon, 2009-01-12 at 09:26 +0100, julien cuisinier wrote:
> 
> 
> Hi,I am bit puzzled by the cov() function in RChecking a function I write, it seems that R compute the covariance between two variable as:1/(n-1) sum[(x-mean(x))*(y-mean(y))]While Excel divide the sum by n instead of (n-1)...which is the right way to do as far as I know...Has anyone already encountered this? or is it just me getting crazy?I use R 2.8.0 on Mac OS XThanks for your feedbackBest regardsJulien
> 
> Descbrelo! Qu puedes hacer con el nuevo Windows Live?
> _________________________________________________________________
> Descubre todas las formas en que puedes estar en contacto con amigos y familiares.
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From seancarmody at gmail.com  Mon Jan 12 09:57:53 2009
From: seancarmody at gmail.com (Sean Carmody)
Date: Mon, 12 Jan 2009 19:57:53 +1100
Subject: [R-SIG-Finance] FW: Covariance in R - wrong?
In-Reply-To: <COL102-W36CDFD5E3A228A921A11C08FD80@phx.gbl>
References: <COL102-W36CDFD5E3A228A921A11C08FD80@phx.gbl>
Message-ID: <ce6bbb9d0901120057t25476751q88e0fd9e2ad2c067@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090112/540081b9/attachment.pl>

From ezivot at u.washington.edu  Mon Jan 12 19:39:19 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Mon, 12 Jan 2009 10:39:19 -0800
Subject: [R-SIG-Finance] Testing for cointegration: Johansen
	vsDickey-Fuller
In-Reply-To: <a7o7w99rdwx4nmfws75mg51v.1231540582684@email.android.com>
Message-ID: <200901121839.n0CIdJGe018058@smtp.washington.edu>

 There are statistical issues associated with this problem that can help
explain what is going on. When you do the ADF procedure, you are imposing a
known cointegrating  vector and so all of the uncertainty associated with
estimating the cointegrating vector has been eliminated. When you use the
Johansen framework, you are estimating the cointegrating vector and so the
uncertainty associated with this estimation is incorporated in the test.
With the futures example, you know the cointegrating vector (if it exists)
from theory so it makes sense to impose it. The resulting test will have
more power (ability to reject the null when the alternative is true) than
the Johansen test. Both tests have no-cointegration as the null (a unit
root). So your ability to find cointegration with the ADF test can be
attributed to the fact that the ADF test has higher power than the Johansen
test in this context. 
>From a more general perspective, the arbitrage relationship between spot and
futures implies that the basis cannot have a unit root so it is essentially
irrelevant to do a unit root test. What is more important here is to
understand the dynamic behavior of the "cointegrating error". More than
likely it will probably have some nonlinear effects that may make it look
nonstationary. There is a rather big literature on threshold type effects in
these models. See, for example, some of the early papers by Martin Martens.
PS. I don't think that the 2nd edition of Bernhard's cointegration book
discusses this issue in any detail.


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
Peterson
Sent: Friday, January 09, 2009 2:23 PM
To: markleeds at verizon.net; Paul Teetor
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Testing for cointegration: Johansen
vsDickey-Fuller

I'll look when I get home, but if I recall correctly, you need to check the
unit root first.  Bernhard's book is definitely the best reference, and the
new edition expands substantially onn the previous version.

markleeds at verizon.net wrote:

>  i think this can happen quite often but i'm not clear on how to 
>resolve it. with the DF methodology, you are specifying the response 
>and with Johansen's you aren't so that may have something to do with 
>it. The literature talks about it but I don't think there's a 
>resolution. Bernhard's cointegration book may talk about it also.
>
>
>
>On Fri, Jan 9, 2009 at  4:38 PM, Paul Teetor wrote:
>
>> R SIG Finance readers:
>>  I am checking a futures spread for mean reversion.  I am using the 
>> Johansen test (ca.jo) for cointegration and the Augmented 
>> Dickey-Fuller test
>> (ur.df)
>> for mean reversion.
>>
>> Here is the odd part:  The Johansen test says the two futures prices 
>> are not cointegrated, but the ADF test says the spread is, in fact, 
>> mean-reverting.
>>  I am very puzzled.  The spread is a linear combination of the 
>> prices, and the ADF test says it is mean-reverting.  But the failed 
>> Johansen test says the prices are not cointegrated, so no linear 
>> combination of prices is mean-reverting.  Huh??
>>  I would be very grateful is someone could suggest where I went 
>> wrong, or steer me towards some relevent reference materials.
>>
>>  Background:  I am studying the spread between TY futures (10-year US
>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>      sprd = ty - (1.2534 * sr)
>>  where ty and sr are the time series of futures prices.  (The 1.2534 
>> factor is from an ordinary least squares fit.)  I execute the 
>> Johansen procedure this way:
>>      ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")  The 
>> summary of the test result is:
>>
>> 	###################### 	# Johansen-Procedure # 	 
>> ######################
>> 	Test type: maximal eigenvalue statistic (lambda max) , without
>> linear trend and constant in 	cointegration
>> 	Eigenvalues (lambda):
>> 	[1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>
>> 	Values of teststatistic and critical values of test:
>>
>> 	         test 10pct  5pct  1pct
>> 	r <= 1 | 2.00  7.52  9.24 12.97
>> 	r = 0  | 8.89 13.75 15.67 20.20
>>
>> 	<snip>
>>
>> I interpret the "r <= 1" line this way:  The test statistic for r <= 
>> 1 is below the critical values, hence we cannot reject the null 
>> hypothesis that the rank is less than 2.  We conclude that the two 
>> time series are not cointegrated.
>>
>> I run the ADF test this way:
>>
>> 	ur.df(sprd, type="drift")
>>
>> (I set type="drift" because that seems to correspond to ecdet="const" 
>> for
>> the Johansen test.)  The summary of the ADF test is:
>>
>> 	###############################################
>> 	# Augmented Dickey-Fuller Test Unit Root Test # 
>> ###############################################
>>
>> 	Test regression drift
>>
>> 	<snip>
>>
>> 	Value of test-statistic is: -2.9624 4.4142
>>
>> 	Critical values for test statistics:
>> 		1pct  5pct 10pct
>> 	tau2 -3.43 -2.86 -2.57
>> 	phi1  6.43  4.59  3.78
>> I interpret the test statistics as meaning we can reject the null 
>> hypothesis of a unit root (at a confidence level of 90% or better), 
>> hence the spread is mean-reverting.  I get similar results from the 
>> adf.test() procedure.
>>
>> F.Y.I., I am running version 2.6.2 of R.
>>  Paul Teetor
>> Elgin, IL   USA
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list 
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.


From bogaso.christofer at gmail.com  Tue Jan 13 09:01:08 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Tue, 13 Jan 2009 00:01:08 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Testing for cointegration:
 Johansen vsDickey-Fuller
In-Reply-To: <200901121839.n0CIdJGe018058@smtp.washington.edu>
References: <540297A816C146FD8AB916967EDC317E@XI>
	<a7o7w99rdwx4nmfws75mg51v.1231540582684@email.android.com>
	<200901121839.n0CIdJGe018058@smtp.washington.edu>
Message-ID: <21430991.post@talk.nabble.com>


Hi Eric, your note I feel obviously valid and truly intuitive. However can
you provide some monte carlo analysis on that? It would be easier to
visualize the whole thing.


Eric Zivot wrote:
> 
>  There are statistical issues associated with this problem that can help
> explain what is going on. When you do the ADF procedure, you are imposing
> a
> known cointegrating  vector and so all of the uncertainty associated with
> estimating the cointegrating vector has been eliminated. When you use the
> Johansen framework, you are estimating the cointegrating vector and so the
> uncertainty associated with this estimation is incorporated in the test.
> With the futures example, you know the cointegrating vector (if it exists)
> from theory so it makes sense to impose it. The resulting test will have
> more power (ability to reject the null when the alternative is true) than
> the Johansen test. Both tests have no-cointegration as the null (a unit
> root). So your ability to find cointegration with the ADF test can be
> attributed to the fact that the ADF test has higher power than the
> Johansen
> test in this context. 
>>From a more general perspective, the arbitrage relationship between spot
and
> futures implies that the basis cannot have a unit root so it is
> essentially
> irrelevant to do a unit root test. What is more important here is to
> understand the dynamic behavior of the "cointegrating error". More than
> likely it will probably have some nonlinear effects that may make it look
> nonstationary. There is a rather big literature on threshold type effects
> in
> these models. See, for example, some of the early papers by Martin
> Martens.
> PS. I don't think that the 2nd edition of Bernhard's cointegration book
> discusses this issue in any detail.
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
> Peterson
> Sent: Friday, January 09, 2009 2:23 PM
> To: markleeds at verizon.net; Paul Teetor
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Testing for cointegration: Johansen
> vsDickey-Fuller
> 
> I'll look when I get home, but if I recall correctly, you need to check
> the
> unit root first.  Bernhard's book is definitely the best reference, and
> the
> new edition expands substantially onn the previous version.
> 
> markleeds at verizon.net wrote:
> 
>>  i think this can happen quite often but i'm not clear on how to 
>>resolve it. with the DF methodology, you are specifying the response 
>>and with Johansen's you aren't so that may have something to do with 
>>it. The literature talks about it but I don't think there's a 
>>resolution. Bernhard's cointegration book may talk about it also.
>>
>>
>>
>>On Fri, Jan 9, 2009 at  4:38 PM, Paul Teetor wrote:
>>
>>> R SIG Finance readers:
>>>  I am checking a futures spread for mean reversion.  I am using the 
>>> Johansen test (ca.jo) for cointegration and the Augmented 
>>> Dickey-Fuller test
>>> (ur.df)
>>> for mean reversion.
>>>
>>> Here is the odd part:  The Johansen test says the two futures prices 
>>> are not cointegrated, but the ADF test says the spread is, in fact, 
>>> mean-reverting.
>>>  I am very puzzled.  The spread is a linear combination of the 
>>> prices, and the ADF test says it is mean-reverting.  But the failed 
>>> Johansen test says the prices are not cointegrated, so no linear 
>>> combination of prices is mean-reverting.  Huh??
>>>  I would be very grateful is someone could suggest where I went 
>>> wrong, or steer me towards some relevent reference materials.
>>>
>>>  Background:  I am studying the spread between TY futures (10-year US
>>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>>      sprd = ty - (1.2534 * sr)
>>>  where ty and sr are the time series of futures prices.  (The 1.2534 
>>> factor is from an ordinary least squares fit.)  I execute the 
>>> Johansen procedure this way:
>>>      ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")  The 
>>> summary of the test result is:
>>>
>>> 	###################### 	# Johansen-Procedure # 	 
>>> ######################
>>> 	Test type: maximal eigenvalue statistic (lambda max) , without
>>> linear trend and constant in 	cointegration
>>> 	Eigenvalues (lambda):
>>> 	[1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>>
>>> 	Values of teststatistic and critical values of test:
>>>
>>> 	         test 10pct  5pct  1pct
>>> 	r <= 1 | 2.00  7.52  9.24 12.97
>>> 	r = 0  | 8.89 13.75 15.67 20.20
>>>
>>> 	<snip>
>>>
>>> I interpret the "r <= 1" line this way:  The test statistic for r <= 
>>> 1 is below the critical values, hence we cannot reject the null 
>>> hypothesis that the rank is less than 2.  We conclude that the two 
>>> time series are not cointegrated.
>>>
>>> I run the ADF test this way:
>>>
>>> 	ur.df(sprd, type="drift")
>>>
>>> (I set type="drift" because that seems to correspond to ecdet="const" 
>>> for
>>> the Johansen test.)  The summary of the ADF test is:
>>>
>>> 	###############################################
>>> 	# Augmented Dickey-Fuller Test Unit Root Test # 
>>> ###############################################
>>>
>>> 	Test regression drift
>>>
>>> 	<snip>
>>>
>>> 	Value of test-statistic is: -2.9624 4.4142
>>>
>>> 	Critical values for test statistics:
>>> 		1pct  5pct 10pct
>>> 	tau2 -3.43 -2.86 -2.57
>>> 	phi1  6.43  4.59  3.78
>>> I interpret the test statistics as meaning we can reject the null 
>>> hypothesis of a unit root (at a confidence level of 90% or better), 
>>> hence the spread is mean-reverting.  I get similar results from the 
>>> adf.test() procedure.
>>>
>>> F.Y.I., I am running version 2.6.2 of R.
>>>  Paul Teetor
>>> Elgin, IL   USA
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>
>>_______________________________________________
>>R-SIG-Finance at stat.math.ethz.ch mailing list 
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>-- Subscriber-posting only.
>>-- If you want to post, subscribe first.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Testing-for-cointegration%3A-Johansen-vs-Dickey-Fuller-tp21381112p21430991.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From Zeno.Adams at ebs.edu  Tue Jan 13 10:46:38 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Tue, 13 Jan 2009 10:46:38 +0100
Subject: [R-SIG-Finance] Testing for cointegration:
	JohansenvsDickey-Fuller
In-Reply-To: <200901121839.n0CIdJGe018058@smtp.washington.edu>
References: <a7o7w99rdwx4nmfws75mg51v.1231540582684@email.android.com>
	<200901121839.n0CIdJGe018058@smtp.washington.edu>
Message-ID: <9064522880125945B98983BBAECBA1CC98547C@exchsrv001.ebs.local>

I would like to comment on what Eric wrote:


"When you do the ADF procedure, you are imposing a
known cointegrating  vector and so all of the uncertainty associated with
estimating the cointegrating vector has been eliminated. When you use the
Johansen framework, you are estimating the cointegrating vector and so the
uncertainty associated with this estimation is incorporated in the test."


>From what I understand the ADF procedure is the Engle-Granger approach. However, I don't think that the "uncertainty associated with
estimating the cointegrating vector" has been eliminated since as Paul wrote the sprd = ty - (1.2534 * sr) has been estimated by OLS. The spread is the residual in the Engle-Granger procedure so testing the residual (which itself is the outcome of an estimation involving uncertainty) on a unit root requires higher critical values (the MacKinnon 1991 tables). Therefore, in my opinion, Paul should compare the test statistic of his ADF test with the higher MacKinnon 1991 values in order to conclude if, from a technical point of view, the two variables are cointegrated or not.

>From the theoretical point of view I would of course agree with what Eric wrote.






-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Eric Zivot
Gesendet: Montag, 12. Januar 2009 19:39
An: 'Brian G. Peterson'; markleeds at verizon.net; 'Paul Teetor'
Cc: r-sig-finance at stat.math.ethz.ch
Betreff: Re: [R-SIG-Finance] Testing for cointegration: JohansenvsDickey-Fuller


 There are statistical issues associated with this problem that can help
explain what is going on. When you do the ADF procedure, you are imposing a
known cointegrating  vector and so all of the uncertainty associated with
estimating the cointegrating vector has been eliminated. When you use the
Johansen framework, you are estimating the cointegrating vector and so the
uncertainty associated with this estimation is incorporated in the test.
With the futures example, you know the cointegrating vector (if it exists)
from theory so it makes sense to impose it. The resulting test will have
more power (ability to reject the null when the alternative is true) than
the Johansen test. Both tests have no-cointegration as the null (a unit
root). So your ability to find cointegration with the ADF test can be
attributed to the fact that the ADF test has higher power than the Johansen
test in this context. 
>From a more general perspective, the arbitrage relationship between spot and
futures implies that the basis cannot have a unit root so it is essentially
irrelevant to do a unit root test. What is more important here is to
understand the dynamic behavior of the "cointegrating error". More than
likely it will probably have some nonlinear effects that may make it look
nonstationary. There is a rather big literature on threshold type effects in
these models. See, for example, some of the early papers by Martin Martens.
PS. I don't think that the 2nd edition of Bernhard's cointegration book
discusses this issue in any detail.


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
Peterson
Sent: Friday, January 09, 2009 2:23 PM
To: markleeds at verizon.net; Paul Teetor
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Testing for cointegration: Johansen
vsDickey-Fuller

I'll look when I get home, but if I recall correctly, you need to check the
unit root first.  Bernhard's book is definitely the best reference, and the
new edition expands substantially onn the previous version.

markleeds at verizon.net wrote:

>  i think this can happen quite often but i'm not clear on how to 
>resolve it. with the DF methodology, you are specifying the response 
>and with Johansen's you aren't so that may have something to do with 
>it. The literature talks about it but I don't think there's a 
>resolution. Bernhard's cointegration book may talk about it also.
>
>
>
>On Fri, Jan 9, 2009 at  4:38 PM, Paul Teetor wrote:
>
>> R SIG Finance readers:
>>  I am checking a futures spread for mean reversion.  I am using the 
>> Johansen test (ca.jo) for cointegration and the Augmented 
>> Dickey-Fuller test
>> (ur.df)
>> for mean reversion.
>>
>> Here is the odd part:  The Johansen test says the two futures prices 
>> are not cointegrated, but the ADF test says the spread is, in fact, 
>> mean-reverting.
>>  I am very puzzled.  The spread is a linear combination of the 
>> prices, and the ADF test says it is mean-reverting.  But the failed 
>> Johansen test says the prices are not cointegrated, so no linear 
>> combination of prices is mean-reverting.  Huh??
>>  I would be very grateful is someone could suggest where I went 
>> wrong, or steer me towards some relevent reference materials.
>>
>>  Background:  I am studying the spread between TY futures (10-year US
>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>      sprd = ty - (1.2534 * sr)
>>  where ty and sr are the time series of futures prices.  (The 1.2534 
>> factor is from an ordinary least squares fit.)  I execute the 
>> Johansen procedure this way:
>>      ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")  The 
>> summary of the test result is:
>>
>> 	###################### 	# Johansen-Procedure # 	 
>> ######################
>> 	Test type: maximal eigenvalue statistic (lambda max) , without
>> linear trend and constant in 	cointegration
>> 	Eigenvalues (lambda):
>> 	[1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>
>> 	Values of teststatistic and critical values of test:
>>
>> 	         test 10pct  5pct  1pct
>> 	r <= 1 | 2.00  7.52  9.24 12.97
>> 	r = 0  | 8.89 13.75 15.67 20.20
>>
>> 	<snip>
>>
>> I interpret the "r <= 1" line this way:  The test statistic for r <= 
>> 1 is below the critical values, hence we cannot reject the null 
>> hypothesis that the rank is less than 2.  We conclude that the two 
>> time series are not cointegrated.
>>
>> I run the ADF test this way:
>>
>> 	ur.df(sprd, type="drift")
>>
>> (I set type="drift" because that seems to correspond to ecdet="const" 
>> for
>> the Johansen test.)  The summary of the ADF test is:
>>
>> 	###############################################
>> 	# Augmented Dickey-Fuller Test Unit Root Test # 
>> ###############################################
>>
>> 	Test regression drift
>>
>> 	<snip>
>>
>> 	Value of test-statistic is: -2.9624 4.4142
>>
>> 	Critical values for test statistics:
>> 		1pct  5pct 10pct
>> 	tau2 -3.43 -2.86 -2.57
>> 	phi1  6.43  4.59  3.78
>> I interpret the test statistics as meaning we can reject the null 
>> hypothesis of a unit root (at a confidence level of 90% or better), 
>> hence the spread is mean-reverting.  I get similar results from the 
>> adf.test() procedure.
>>
>> F.Y.I., I am running version 2.6.2 of R.
>>  Paul Teetor
>> Elgin, IL   USA
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list 
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrung: Prof. Dr. Christopher Jahns, Rektor; Dr. Reimar Palte, Kanzler;  Sabine Fuchs, Prokuristin; Verwaltungsrat: Dr. Hellmut K. Albrecht, Vorsitzender
-------------- next part --------------
A non-text attachment was scrubbed...
Name: MacKinnon(1991).xls
Type: application/vnd.ms-excel
Size: 22528 bytes
Desc: MacKinnon(1991).xls
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090113/85ff5bc9/attachment.xls>

From Bernhard_Pfaff at fra.invesco.com  Tue Jan 13 11:19:31 2009
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Tue, 13 Jan 2009 10:19:31 -0000
Subject: [R-SIG-Finance] Testing for
	cointegration:JohansenvsDickey-Fuller
In-Reply-To: <9064522880125945B98983BBAECBA1CC98547C@exchsrv001.ebs.local>
References: <a7o7w99rdwx4nmfws75mg51v.1231540582684@email.android.com><200901121839.n0CIdJGe018058@smtp.washington.edu>
	<9064522880125945B98983BBAECBA1CC98547C@exchsrv001.ebs.local>
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C316EE9C3D@GBHENXMB02.corp.amvescap.net>

Dear Adam, Eric and all others involved in this thread,

Adam: many thanks for making your point and indeed the critical values returned by ur.df() apply not for the cointegration ADF-test (see ?ur.df). However, in the package fUnitRoots as well as in the latest version of urca the MacKinnon values are available. In addition, one could also employ the critical values that are reported in Engle & Yoo. Incidentally, this issue is adressed in my book.

Best,
Bernhard

>
>I would like to comment on what Eric wrote:
>
>
>"When you do the ADF procedure, you are imposing a
>known cointegrating  vector and so all of the uncertainty 
>associated with
>estimating the cointegrating vector has been eliminated. When 
>you use the
>Johansen framework, you are estimating the cointegrating 
>vector and so the
>uncertainty associated with this estimation is incorporated in 
>the test."
>
>
>>From what I understand the ADF procedure is the Engle-Granger 
>approach. However, I don't think that the "uncertainty associated with
>estimating the cointegrating vector" has been eliminated since 
>as Paul wrote the sprd = ty - (1.2534 * sr) has been estimated 
>by OLS. The spread is the residual in the Engle-Granger 
>procedure so testing the residual (which itself is the outcome 
>of an estimation involving uncertainty) on a unit root 
>requires higher critical values (the MacKinnon 1991 tables). 
>Therefore, in my opinion, Paul should compare the test 
>statistic of his ADF test with the higher MacKinnon 1991 
>values in order to conclude if, from a technical point of 
>view, the two variables are cointegrated or not.
>
>>From the theoretical point of view I would of course agree 
>with what Eric wrote.
>
>
>
>
>
>
>-----Urspr?ngliche Nachricht-----
>Von: r-sig-finance-bounces at stat.math.ethz.ch 
>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag 
>von Eric Zivot
>Gesendet: Montag, 12. Januar 2009 19:39
>An: 'Brian G. Peterson'; markleeds at verizon.net; 'Paul Teetor'
>Cc: r-sig-finance at stat.math.ethz.ch
>Betreff: Re: [R-SIG-Finance] Testing for cointegration: 
>JohansenvsDickey-Fuller
>
>
> There are statistical issues associated with this problem 
>that can help
>explain what is going on. When you do the ADF procedure, you 
>are imposing a
>known cointegrating  vector and so all of the uncertainty 
>associated with
>estimating the cointegrating vector has been eliminated. When 
>you use the
>Johansen framework, you are estimating the cointegrating 
>vector and so the
>uncertainty associated with this estimation is incorporated in 
>the test.
>With the futures example, you know the cointegrating vector 
>(if it exists)
>from theory so it makes sense to impose it. The resulting test 
>will have
>more power (ability to reject the null when the alternative is 
>true) than
>the Johansen test. Both tests have no-cointegration as the null (a unit
>root). So your ability to find cointegration with the ADF test can be
>attributed to the fact that the ADF test has higher power than 
>the Johansen
>test in this context. 
>>From a more general perspective, the arbitrage relationship 
>between spot and
>futures implies that the basis cannot have a unit root so it 
>is essentially
>irrelevant to do a unit root test. What is more important here is to
>understand the dynamic behavior of the "cointegrating error". More than
>likely it will probably have some nonlinear effects that may 
>make it look
>nonstationary. There is a rather big literature on threshold 
>type effects in
>these models. See, for example, some of the early papers by 
>Martin Martens.
>PS. I don't think that the 2nd edition of Bernhard's cointegration book
>discusses this issue in any detail.
>
>
>-----Original Message-----
>From: r-sig-finance-bounces at stat.math.ethz.ch
>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
>Peterson
>Sent: Friday, January 09, 2009 2:23 PM
>To: markleeds at verizon.net; Paul Teetor
>Cc: r-sig-finance at stat.math.ethz.ch
>Subject: Re: [R-SIG-Finance] Testing for cointegration: Johansen
>vsDickey-Fuller
>
>I'll look when I get home, but if I recall correctly, you need 
>to check the
>unit root first.  Bernhard's book is definitely the best 
>reference, and the
>new edition expands substantially onn the previous version.
>
>markleeds at verizon.net wrote:
>
>>  i think this can happen quite often but i'm not clear on how to 
>>resolve it. with the DF methodology, you are specifying the response 
>>and with Johansen's you aren't so that may have something to do with 
>>it. The literature talks about it but I don't think there's a 
>>resolution. Bernhard's cointegration book may talk about it also.
>>
>>
>>
>>On Fri, Jan 9, 2009 at  4:38 PM, Paul Teetor wrote:
>>
>>> R SIG Finance readers:
>>>  I am checking a futures spread for mean reversion.  I am using the 
>>> Johansen test (ca.jo) for cointegration and the Augmented 
>>> Dickey-Fuller test
>>> (ur.df)
>>> for mean reversion.
>>>
>>> Here is the odd part:  The Johansen test says the two 
>futures prices 
>>> are not cointegrated, but the ADF test says the spread is, in fact, 
>>> mean-reverting.
>>>  I am very puzzled.  The spread is a linear combination of the 
>>> prices, and the ADF test says it is mean-reverting.  But the failed 
>>> Johansen test says the prices are not cointegrated, so no linear 
>>> combination of prices is mean-reverting.  Huh??
>>>  I would be very grateful is someone could suggest where I went 
>>> wrong, or steer me towards some relevent reference materials.
>>>
>>>  Background:  I am studying the spread between TY futures 
>(10-year US
>>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>>      sprd = ty - (1.2534 * sr)
>>>  where ty and sr are the time series of futures prices.  
>(The 1.2534 
>>> factor is from an ordinary least squares fit.)  I execute the 
>>> Johansen procedure this way:
>>>      ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")  The 
>>> summary of the test result is:
>>>
>>> 	###################### 	# Johansen-Procedure # 	 
>>> ######################
>>> 	Test type: maximal eigenvalue statistic (lambda max) , without
>>> linear trend and constant in 	cointegration
>>> 	Eigenvalues (lambda):
>>> 	[1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>>
>>> 	Values of teststatistic and critical values of test:
>>>
>>> 	         test 10pct  5pct  1pct
>>> 	r <= 1 | 2.00  7.52  9.24 12.97
>>> 	r = 0  | 8.89 13.75 15.67 20.20
>>>
>>> 	<snip>
>>>
>>> I interpret the "r <= 1" line this way:  The test statistic 
>for r <= 
>>> 1 is below the critical values, hence we cannot reject the null 
>>> hypothesis that the rank is less than 2.  We conclude that the two 
>>> time series are not cointegrated.
>>>
>>> I run the ADF test this way:
>>>
>>> 	ur.df(sprd, type="drift")
>>>
>>> (I set type="drift" because that seems to correspond to 
>ecdet="const" 
>>> for
>>> the Johansen test.)  The summary of the ADF test is:
>>>
>>> 	###############################################
>>> 	# Augmented Dickey-Fuller Test Unit Root Test # 
>>> ###############################################
>>>
>>> 	Test regression drift
>>>
>>> 	<snip>
>>>
>>> 	Value of test-statistic is: -2.9624 4.4142
>>>
>>> 	Critical values for test statistics:
>>> 		1pct  5pct 10pct
>>> 	tau2 -3.43 -2.86 -2.57
>>> 	phi1  6.43  4.59  3.78
>>> I interpret the test statistics as meaning we can reject the null 
>>> hypothesis of a unit root (at a confidence level of 90% or better), 
>>> hence the spread is mean-reverting.  I get similar results from the 
>>> adf.test() procedure.
>>>
>>> F.Y.I., I am running version 2.6.2 of R.
>>>  Paul Teetor
>>> Elgin, IL   USA
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>
>>_______________________________________________
>>R-SIG-Finance at stat.math.ethz.ch mailing list 
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>-- Subscriber-posting only.
>>-- If you want to post, subscribe first.
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.
>
>
>EBS European Business School gemeinnuetzige GmbH - Sitz der 
>Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - 
>Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrung: Prof. Dr. 
>Christopher Jahns, Rektor; Dr. Reimar Palte, Kanzler;  Sabine 
>Fuchs, Prokuristin; Verwaltungsrat: Dr. Hellmut K. Albrecht, 
>Vorsitzender
>
*****************************************************************
Confidentiality Note: The information contained in this ...{{dropped:10}}


From brian at braverock.com  Tue Jan 13 12:39:15 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 13 Jan 2009 05:39:15 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Testing for cointegration:
 Johansen vsDickey-Fuller
In-Reply-To: <21430991.post@talk.nabble.com>
References: <540297A816C146FD8AB916967EDC317E@XI>	<a7o7w99rdwx4nmfws75mg51v.1231540582684@email.android.com>	<200901121839.n0CIdJGe018058@smtp.washington.edu>
	<21430991.post@talk.nabble.com>
Message-ID: <496C7D63.8000407@braverock.com>

Christofer Bogaso wrote:
> Hi Eric, your note I feel obviously valid and truly intuitive. However can
> you provide some monte carlo analysis on that? It would be easier to
> visualize the whole thing.
<...>

Christofer.

No offense, but if you want to visualize the difference in cointegration test 
methods using a Monte Carlo simulation, why don't you do it yourself and post 
the results to help educate the list on what you find, rather than asking 
others to spend valuable time doing it for you?

Further, I fail to see how a Monte Carlo *simulation* could create a pair of 
simulated cointegrated spot/future time series.

Regards,

    - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From ezivot at u.washington.edu  Tue Jan 13 18:40:16 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Tue, 13 Jan 2009 09:40:16 -0800 (PST)
Subject: [R-SIG-Finance] Testing for cointegration:
	JohansenvsDickey-Fuller
In-Reply-To: <9064522880125945B98983BBAECBA1CC98547C@exchsrv001.ebs.local>
Message-ID: <Pine.LNX.4.43.0901130940160.25694@hymn13.u.washington.edu>

Zeno
I should have read the original post more carefully. I didn't realize that the cointegrating vector used in the ADF procedure was first estimated by OLS. Thanks for pointing this important mistake out. If this is the case, then the procedure is the traditional Engle-Granger two-step procedure and the asymptotic distn of the "ADF test statistic" follows the distribution described by Phillips and Ouliaris (see Hamilton's Time Series Analysis book for a description of this disn or my chapter on cointegration in MFTS). Under the null of no-cointegration (unit root) this distribution incorporates the fact that the estimated cointegrating relationship is spurious and is similar to the Johansen distn. In this case, the Engle-Granger and Johansen procedures may give different results if the OLS cointegrating vector is quite a bit different than the cointegrating vector estimated by the Johansen MLE. 
The Johansen MLE can sometime give very strange results. One reason for this is that the finite sample distn of the Johansen MLE does not have any moments (as proved by Peter Phillips in a Journal of Econometrics article) and so the tails of the finite sample distn are very fat which can produce extreme values of the cointegrating vector. This is similar to the situation with the LIML estimator in the traditional simultaneous equation model (the 2SLS estimator has moments in overidentified models; the LIML estimator does not). 
Still, my original post is still relevant here. In an arbitrage situation with spot and futures you should know the cost of carry relationship and so there should be no need to estimate the cointegrating vector.


****************************************************************
*  Eric Zivot                  			               *
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Adjunct Professor of Finance                                *
*  Adjunct Professor of Statistics
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *                                                           *
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Tue, 13 Jan 2009, Adams, Zeno wrote:

> I would like to comment on what Eric wrote:
>
>
> "When you do the ADF procedure, you are imposing a
> known cointegrating  vector and so all of the uncertainty associated with
> estimating the cointegrating vector has been eliminated. When you use the
> Johansen framework, you are estimating the cointegrating vector and so the
> uncertainty associated with this estimation is incorporated in the test."
>
>
> From what I understand the ADF procedure is the Engle-Granger approach. However, I don't think that the "uncertainty associated with
> estimating the cointegrating vector" has been eliminated since as Paul wrote the sprd = ty - (1.2534 * sr) has been estimated by OLS. The spread is the residual in the Engle-Granger procedure so testing the residual (which itself is the outcome of an estimation involving uncertainty) on a unit root requires higher critical values (the MacKinnon 1991 tables). Therefore, in my opinion, Paul should compare the test statistic of his ADF test with the higher MacKinnon 1991 values in order to conclude if, from a technical point of view, the two variables are cointegrated or not.
>
> From the theoretical point of view I would of course agree with what Eric wrote.
>
>
>
>
>
>
> -----Urspr?ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Eric Zivot
> Gesendet: Montag, 12. Januar 2009 19:39
> An: 'Brian G. Peterson'; markleeds at verizon.net; 'Paul Teetor'
> Cc: r-sig-finance at stat.math.ethz.ch
> Betreff: Re: [R-SIG-Finance] Testing for cointegration: JohansenvsDickey-Fuller
>
>
> There are statistical issues associated with this problem that can help
> explain what is going on. When you do the ADF procedure, you are imposing a
> known cointegrating  vector and so all of the uncertainty associated with
> estimating the cointegrating vector has been eliminated. When you use the
> Johansen framework, you are estimating the cointegrating vector and so the
> uncertainty associated with this estimation is incorporated in the test.
> With the futures example, you know the cointegrating vector (if it exists)
> from theory so it makes sense to impose it. The resulting test will have
> more power (ability to reject the null when the alternative is true) than
> the Johansen test. Both tests have no-cointegration as the null (a unit
> root). So your ability to find cointegration with the ADF test can be
> attributed to the fact that the ADF test has higher power than the Johansen
> test in this context.
>> From a more general perspective, the arbitrage relationship between spot and
> futures implies that the basis cannot have a unit root so it is essentially
> irrelevant to do a unit root test. What is more important here is to
> understand the dynamic behavior of the "cointegrating error". More than
> likely it will probably have some nonlinear effects that may make it look
> nonstationary. There is a rather big literature on threshold type effects in
> these models. See, for example, some of the early papers by Martin Martens.
> PS. I don't think that the 2nd edition of Bernhard's cointegration book
> discusses this issue in any detail.
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
> Peterson
> Sent: Friday, January 09, 2009 2:23 PM
> To: markleeds at verizon.net; Paul Teetor
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Testing for cointegration: Johansen
> vsDickey-Fuller
>
> I'll look when I get home, but if I recall correctly, you need to check the
> unit root first.  Bernhard's book is definitely the best reference, and the
> new edition expands substantially onn the previous version.
>
> markleeds at verizon.net wrote:
>
>>  i think this can happen quite often but i'm not clear on how to
>> resolve it. with the DF methodology, you are specifying the response
>> and with Johansen's you aren't so that may have something to do with
>> it. The literature talks about it but I don't think there's a
>> resolution. Bernhard's cointegration book may talk about it also.
>>
>>
>>
>> On Fri, Jan 9, 2009 at  4:38 PM, Paul Teetor wrote:
>>
>>> R SIG Finance readers:
>>>  I am checking a futures spread for mean reversion.  I am using the
>>> Johansen test (ca.jo) for cointegration and the Augmented
>>> Dickey-Fuller test
>>> (ur.df)
>>> for mean reversion.
>>>
>>> Here is the odd part:  The Johansen test says the two futures prices
>>> are not cointegrated, but the ADF test says the spread is, in fact,
>>> mean-reverting.
>>>  I am very puzzled.  The spread is a linear combination of the
>>> prices, and the ADF test says it is mean-reverting.  But the failed
>>> Johansen test says the prices are not cointegrated, so no linear
>>> combination of prices is mean-reverting.  Huh??
>>>  I would be very grateful is someone could suggest where I went
>>> wrong, or steer me towards some relevent reference materials.
>>>
>>>  Background:  I am studying the spread between TY futures (10-year US
>>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>>      sprd = ty - (1.2534 * sr)
>>>  where ty and sr are the time series of futures prices.  (The 1.2534
>>> factor is from an ordinary least squares fit.)  I execute the
>>> Johansen procedure this way:
>>>      ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")  The
>>> summary of the test result is:
>>>
>>> 	###################### 	# Johansen-Procedure #
>>> ######################
>>> 	Test type: maximal eigenvalue statistic (lambda max) , without
>>> linear trend and constant in 	cointegration
>>> 	Eigenvalues (lambda):
>>> 	[1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>>
>>> 	Values of teststatistic and critical values of test:
>>>
>>> 	         test 10pct  5pct  1pct
>>> 	r <= 1 | 2.00  7.52  9.24 12.97
>>> 	r = 0  | 8.89 13.75 15.67 20.20
>>>
>>> 	<snip>
>>>
>>> I interpret the "r <= 1" line this way:  The test statistic for r <=
>>> 1 is below the critical values, hence we cannot reject the null
>>> hypothesis that the rank is less than 2.  We conclude that the two
>>> time series are not cointegrated.
>>>
>>> I run the ADF test this way:
>>>
>>> 	ur.df(sprd, type="drift")
>>>
>>> (I set type="drift" because that seems to correspond to ecdet="const"
>>> for
>>> the Johansen test.)  The summary of the ADF test is:
>>>
>>> 	###############################################
>>> 	# Augmented Dickey-Fuller Test Unit Root Test #
>>> ###############################################
>>>
>>> 	Test regression drift
>>>
>>> 	<snip>
>>>
>>> 	Value of test-statistic is: -2.9624 4.4142
>>>
>>> 	Critical values for test statistics:
>>> 		1pct  5pct 10pct
>>> 	tau2 -3.43 -2.86 -2.57
>>> 	phi1  6.43  4.59  3.78
>>> I interpret the test statistics as meaning we can reject the null
>>> hypothesis of a unit root (at a confidence level of 90% or better),
>>> hence the spread is mean-reverting.  I get similar results from the
>>> adf.test() procedure.
>>>
>>> F.Y.I., I am running version 2.6.2 of R.
>>>  Paul Teetor
>>> Elgin, IL   USA
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
> EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrung: Prof. Dr. Christopher Jahns, Rektor; Dr. Reimar Palte, Kanzler;  Sabine Fuchs, Prokuristin; Verwaltungsrat: Dr. Hellmut K. Albrecht, Vorsitzender


From paulteetor at yahoo.com  Tue Jan 13 22:15:41 2009
From: paulteetor at yahoo.com (Paul Teetor)
Date: Tue, 13 Jan 2009 15:15:41 -0600
Subject: [R-SIG-Finance] Testing for cointegration: Johansen vs
	Dickey-Fuller
In-Reply-To: <Pine.LNX.4.43.0901130940160.25694@hymn13.u.washington.edu>
References: <9064522880125945B98983BBAECBA1CC98547C@exchsrv001.ebs.local>
	<Pine.LNX.4.43.0901130940160.25694@hymn13.u.washington.edu>
Message-ID: <74FC68BD371542DEA51E433DC7D90FCF@XI>

All:

I want to thank Eric, Zeno, and Bernhard for continuing and expanding this
thread.  I am learning more with each post, and I am certain others are
profiting, too.  Soon we will have enough material for a small monograph!

Eric:  I want to clarify something.  The original post was testing a spread
for mean reversion.  The spread was between two futures and did not involve
any spot data (10-year Treasury Note versus 10-year Swap Rate).  These
futures have a shifting economic relationship unlike spot and futures
prices.  I cannot assume the spread is mean-reverting like a basis spread,
so the unit root test is important.

Everyone:  In case anyone wants to expore the concepts in this thread, I
have posted the original time series to my web site:

	http://quanttrader.info/public/ty.sr.csv

This is a CSV file which you can load directly into R using

	sprd <- read.csv("http://quanttrader.info/public/ty.sr.csv")

(Note to students of the futures market:  The data are not pure contract
prices.  These are synthetic series constructed through blending and
concatenation.)

Paul 

-----Original Message-----
From: Eric Zivot [mailto:ezivot at u.washington.edu] 
Sent: Tuesday, January 13, 2009 11:40 AM
To: Adams, Zeno
Cc: Brian G. Peterson; markleeds at verizon.net; Paul Teetor;
r-sig-finance at stat.math.ethz.ch
Subject: Re: AW: [R-SIG-Finance] Testing for cointegration:
JohansenvsDickey-Fuller

Zeno
I should have read the original post more carefully. I didn't realize that
the cointegrating vector used in the ADF procedure was first estimated by
OLS. Thanks for pointing this important mistake out. If this is the case,
then the procedure is the traditional Engle-Granger two-step procedure and
the asymptotic distn of the "ADF test statistic" follows the distribution
described by Phillips and Ouliaris (see Hamilton's Time Series Analysis book
for a description of this disn or my chapter on cointegration in MFTS).
Under the null of no-cointegration (unit root) this distribution
incorporates the fact that the estimated cointegrating relationship is
spurious and is similar to the Johansen distn. In this case, the
Engle-Granger and Johansen procedures may give different results if the OLS
cointegrating vector is quite a bit different than the cointegrating vector
estimated by the Johansen MLE. 
The Johansen MLE can sometime give very strange results. One reason for this
is that the finite sample distn of the Johansen MLE does not have any
moments (as proved by Peter Phillips in a Journal of Econometrics article)
and so the tails of the finite sample distn are very fat which can produce
extreme values of the cointegrating vector. This is similar to the situation
with the LIML estimator in the traditional simultaneous equation model (the
2SLS estimator has moments in overidentified models; the LIML estimator does
not). 
Still, my original post is still relevant here. In an arbitrage situation
with spot and futures you should know the cost of carry relationship and so
there should be no need to estimate the cointegrating vector.


****************************************************************
*  Eric Zivot                  			               *
*  Professor and Gary Waterman Distinguished Scholar           *
*  Department of Economics                                     *
*  Adjunct Professor of Finance                                *
*  Adjunct Professor of Statistics
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington    phone:  206-543-6715            *
*  Seattle, WA 98195-3330                                      *
*
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Tue, 13 Jan 2009, Adams, Zeno wrote:

> I would like to comment on what Eric wrote:
>
>
> "When you do the ADF procedure, you are imposing a known cointegrating 
> vector and so all of the uncertainty associated with estimating the 
> cointegrating vector has been eliminated. When you use the Johansen 
> framework, you are estimating the cointegrating vector and so the 
> uncertainty associated with this estimation is incorporated in the test."
>
>
> From what I understand the ADF procedure is the Engle-Granger 
> approach. However, I don't think that the "uncertainty associated with
estimating the cointegrating vector" has been eliminated since as Paul wrote
the sprd = ty - (1.2534 * sr) has been estimated by OLS. The spread is the
residual in the Engle-Granger procedure so testing the residual (which
itself is the outcome of an estimation involving uncertainty) on a unit root
requires higher critical values (the MacKinnon 1991 tables). Therefore, in
my opinion, Paul should compare the test statistic of his ADF test with the
higher MacKinnon 1991 values in order to conclude if, from a technical point
of view, the two variables are cointegrated or not.
>
> From the theoretical point of view I would of course agree with what Eric
wrote.
>
>
>
>
>
>
> -----Urspr?ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch 
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Eric 
> Zivot
> Gesendet: Montag, 12. Januar 2009 19:39
> An: 'Brian G. Peterson'; markleeds at verizon.net; 'Paul Teetor'
> Cc: r-sig-finance at stat.math.ethz.ch
> Betreff: Re: [R-SIG-Finance] Testing for cointegration: 
> JohansenvsDickey-Fuller
>
>
> There are statistical issues associated with this problem that can 
> help explain what is going on. When you do the ADF procedure, you are 
> imposing a known cointegrating  vector and so all of the uncertainty 
> associated with estimating the cointegrating vector has been 
> eliminated. When you use the Johansen framework, you are estimating 
> the cointegrating vector and so the uncertainty associated with this
estimation is incorporated in the test.
> With the futures example, you know the cointegrating vector (if it 
> exists) from theory so it makes sense to impose it. The resulting test 
> will have more power (ability to reject the null when the alternative 
> is true) than the Johansen test. Both tests have no-cointegration as 
> the null (a unit root). So your ability to find cointegration with the 
> ADF test can be attributed to the fact that the ADF test has higher 
> power than the Johansen test in this context.
>> From a more general perspective, the arbitrage relationship between 
>> spot and
> futures implies that the basis cannot have a unit root so it is 
> essentially irrelevant to do a unit root test. What is more important 
> here is to understand the dynamic behavior of the "cointegrating 
> error". More than likely it will probably have some nonlinear effects 
> that may make it look nonstationary. There is a rather big literature 
> on threshold type effects in these models. See, for example, some of the
early papers by Martin Martens.
> PS. I don't think that the 2nd edition of Bernhard's cointegration 
> book discusses this issue in any detail.
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Brian G.
> Peterson
> Sent: Friday, January 09, 2009 2:23 PM
> To: markleeds at verizon.net; Paul Teetor
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Testing for cointegration: Johansen 
> vsDickey-Fuller
>
> I'll look when I get home, but if I recall correctly, you need to 
> check the unit root first.  Bernhard's book is definitely the best 
> reference, and the new edition expands substantially onn the previous
version.
>
> markleeds at verizon.net wrote:
>
>>  i think this can happen quite often but i'm not clear on how to 
>> resolve it. with the DF methodology, you are specifying the response 
>> and with Johansen's you aren't so that may have something to do with 
>> it. The literature talks about it but I don't think there's a 
>> resolution. Bernhard's cointegration book may talk about it also.
>>
>>
>>
>> On Fri, Jan 9, 2009 at  4:38 PM, Paul Teetor wrote:
>>
>>> R SIG Finance readers:
>>>  I am checking a futures spread for mean reversion.  I am using the 
>>> Johansen test (ca.jo) for cointegration and the Augmented 
>>> Dickey-Fuller test
>>> (ur.df)
>>> for mean reversion.
>>>
>>> Here is the odd part:  The Johansen test says the two futures prices 
>>> are not cointegrated, but the ADF test says the spread is, in fact, 
>>> mean-reverting.
>>>  I am very puzzled.  The spread is a linear combination of the 
>>> prices, and the ADF test says it is mean-reverting.  But the failed 
>>> Johansen test says the prices are not cointegrated, so no linear 
>>> combination of prices is mean-reverting.  Huh??
>>>  I would be very grateful is someone could suggest where I went 
>>> wrong, or steer me towards some relevent reference materials.
>>>
>>>  Background:  I am studying the spread between TY futures (10-year 
>>> US
>>> Treasurys) and SR futures (10-year US swap rate), calculated as:
>>>      sprd = ty - (1.2534 * sr)
>>>  where ty and sr are the time series of futures prices.  (The 1.2534 
>>> factor is from an ordinary least squares fit.)  I execute the 
>>> Johansen procedure this way:
>>>      ca.jo(data.frame(ty, sr), type="eigen", ecdet="const")  The 
>>> summary of the test result is:
>>>
>>> 	###################### 	# Johansen-Procedure #
>>> ######################
>>> 	Test type: maximal eigenvalue statistic (lambda max) , without
>>> linear trend and constant in 	cointegration
>>> 	Eigenvalues (lambda):
>>> 	[1]  2.929702e-03  6.616599e-04 -1.001412e-17
>>>
>>> 	Values of teststatistic and critical values of test:
>>>
>>> 	         test 10pct  5pct  1pct
>>> 	r <= 1 | 2.00  7.52  9.24 12.97
>>> 	r = 0  | 8.89 13.75 15.67 20.20
>>>
>>> 	<snip>
>>>
>>> I interpret the "r <= 1" line this way:  The test statistic for r <=
>>> 1 is below the critical values, hence we cannot reject the null 
>>> hypothesis that the rank is less than 2.  We conclude that the two 
>>> time series are not cointegrated.
>>>
>>> I run the ADF test this way:
>>>
>>> 	ur.df(sprd, type="drift")
>>>
>>> (I set type="drift" because that seems to correspond to ecdet="const"
>>> for
>>> the Johansen test.)  The summary of the ADF test is:
>>>
>>> 	###############################################
>>> 	# Augmented Dickey-Fuller Test Unit Root Test # 
>>> ###############################################
>>>
>>> 	Test regression drift
>>>
>>> 	<snip>
>>>
>>> 	Value of test-statistic is: -2.9624 4.4142
>>>
>>> 	Critical values for test statistics:
>>> 		1pct  5pct 10pct
>>> 	tau2 -3.43 -2.86 -2.57
>>> 	phi1  6.43  4.59  3.78
>>> I interpret the test statistics as meaning we can reject the null 
>>> hypothesis of a unit root (at a confidence level of 90% or better), 
>>> hence the spread is mean-reverting.  I get similar results from the
>>> adf.test() procedure.
>>>
>>> F.Y.I., I am running version 2.6.2 of R.
>>>  Paul Teetor
>>> Elgin, IL   USA
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
> EBS European Business School gemeinnuetzige GmbH - Sitz der 
> Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - 
> Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrung: Prof. Dr. Christopher 
> Jahns, Rektor; Dr. Reimar Palte, Kanzler;  Sabine Fuchs, Prokuristin; 
> Verwaltungsrat: Dr. Hellmut K. Albrecht, Vorsitzender


From bastian2507hk at yahoo.co.uk  Wed Jan 14 03:28:17 2009
From: bastian2507hk at yahoo.co.uk (Bastian Offermann)
Date: Wed, 14 Jan 2009 03:28:17 +0100
Subject: [R-SIG-Finance] fPortfolio Inputs as List - Error Msg
Message-ID: <496D4DC1.4020704@yahoo.co.uk>

Hello,

I am trying to use flexible inputs for an unconstrained Markowitz 
optimization using the fPortfolio package.

The documentation says

*"a time series or a named list, containing either a series of returns 
or named
entries ?mu? and ?Sigma? being mean and covariance matrix." *

is required as inputs. However, when supplying a list containing mu and 
sigma I get the following error

*Error: class(data) == "timeSeries" is not TRUE

r.p <- 0.05
Spec <- portfolioSpec()
setType(Spec) <- "MV"
cons <- "Short"
setOptimize(Spec) <- "minRisk"
setTargetReturn(Spec) <- r.p/52
setNFrontierPoints(Spec) <- 10
setSolver(Spec) <- "solveRshortExact"

mu <- c(0.0002884615, 0.0007085510)
covar <- matrix(c(0.0002536010, 0.0001704248, 0.0001704248, 
0.0007122588), 2, 2)

Data1 <- list(mu, covar)

eff.front1 <- portfolioFrontier(data = Data1, spec = Spec, constraints = 
cons)

*Any input appreciated. Thanks in advance.

Regards

BO


From elise at predictiveanalyticsworld.com  Wed Jan 14 07:08:04 2009
From: elise at predictiveanalyticsworld.com (Elise Johnson)
Date: Tue, 13 Jan 2009 22:08:04 -0800
Subject: [R-SIG-Finance] PAW Update: Predictive analytics workshops and more
	case studies
Message-ID: <7896264a0901132208x35b60eads110be9c2561b2d8c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090113/39371159/attachment.pl>

From mdkhalidiqbal at gmail.com  Wed Jan 14 07:10:52 2009
From: mdkhalidiqbal at gmail.com (Khalid Iqbal)
Date: Wed, 14 Jan 2009 11:40:52 +0530
Subject: [R-SIG-Finance] batch processing in R for WINDOWS users
Message-ID: <ded8d49c0901132210s49cf415bn5baf7e42be22186d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090114/9a613a07/attachment.pl>

From brian at braverock.com  Wed Jan 14 13:23:27 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 14 Jan 2009 06:23:27 -0600
Subject: [R-SIG-Finance] batch processing in R for WINDOWS users
In-Reply-To: <ded8d49c0901132210s49cf415bn5baf7e42be22186d@mail.gmail.com>
References: <ded8d49c0901132210s49cf415bn5baf7e42be22186d@mail.gmail.com>
Message-ID: <496DD93F.60904@braverock.com>

Khalid Iqbal wrote:
> I would like to know how to do batch processing in R in Windows.
> I have to analyze large volume of financial data. I want to make use of
> batch processing for that. Please suggest me some methods to do that.

This question really belongs on r-help, not r-sig-finance.

That said, 'R CMD BATCH' comes to mind.  Also 'littler'.

This is a FAQ, and is extensively covered with a little work on Google.  If you 
have trouble and need additional help, please direct your queries to the 
appropriate list.

Regards,

     - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From ggrothendieck at gmail.com  Wed Jan 14 13:55:17 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 14 Jan 2009 07:55:17 -0500
Subject: [R-SIG-Finance] batch processing in R for WINDOWS users
In-Reply-To: <496DD93F.60904@braverock.com>
References: <ded8d49c0901132210s49cf415bn5baf7e42be22186d@mail.gmail.com>
	<496DD93F.60904@braverock.com>
Message-ID: <971536df0901140455r599ce4d9x6ead9f79edd1123c@mail.gmail.com>

Note that littler is not for Windows.

Rcmd BATCH or Rscript could be used.  There are also
some batch files at:

   http://batchfiles.googlecode.com

that act as front ends to Rcmd and Rscript that can be
used to avoid having to set any paths on your system.

On Wed, Jan 14, 2009 at 7:23 AM, Brian G. Peterson <brian at braverock.com> wrote:
> Khalid Iqbal wrote:
>>
>> I would like to know how to do batch processing in R in Windows.
>> I have to analyze large volume of financial data. I want to make use of
>> batch processing for that. Please suggest me some methods to do that.
>
> This question really belongs on r-help, not r-sig-finance.
>
> That said, 'R CMD BATCH' comes to mind.  Also 'littler'.
>
> This is a FAQ, and is extensively covered with a little work on Google.  If
> you have trouble and need additional help, please direct your queries to the
> appropriate list.
>
> Regards,
>
>    - Brian
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jorge.nieves at moorecap.com  Wed Jan 14 17:46:43 2009
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Wed, 14 Jan 2009 11:46:43 -0500
Subject: [R-SIG-Finance] findDrawdowns/maxDrawdown  clarification, please?
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF014EBBE5@NYC-XCH3.win.moorecap.com>

Hi,

I have a hypothetical  time series for some cumulative PNL.  I am trying
to use the findDrawdowns function from the PerformanceAnalytics package.
When I pass the series to the function, I get an error message which
meaning I do not understand. Can some one shed some light about what I
am doing wrong?  
The function seems to work when using returns instead of dollar amounts
(which is my case). I am also using the maxDrawdown <maxDrawdown.html>
function, and this last one appears to work well with either returns or
dollar amounts. I am just wondering if this is driving my error message.

On the other had, the example used in the help(findDrawdowns) is based
on returns of the edhec[,"Funds.of.Funds"] . These returns appear to be
monthly returns. I would like to understand if the idea behind the
findDrawdowns should be applied to a cumulative returns/pnl instead just
monthly (or other frequency) returns/pnl? I believe that drawdown is
defined as the difference between today's cumulative pnl and the "high
water mark" highest point cumulative pnl, but my interpretation could be
completely wrong. Can some one explain?


Thanks,

Jorge

CODE
> ser <- results[samper,"cumpnl",drop=FALSE]
> # write.csv(ser,"test.csv")
> plot(ser,type = "l",col= "red")
> toto = findDrawdowns(ser)
Error in if (thisSign == priorSign) { : 
  missing value where TRUE/FALSE needed

 <<test.csv>> 

Jorge Nieves
Phone 212.782.7083
Fax 212.642.7644

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090114/5e1fb15d/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: test.csv
Type: application/octet-stream
Size: 25630 bytes
Desc: test.csv
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090114/5e1fb15d/attachment.obj>

From brian at braverock.com  Wed Jan 14 18:34:16 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 14 Jan 2009 11:34:16 -0600 (CST)
Subject: [R-SIG-Finance] findDrawdowns/maxDrawdown  clarification,
 please?
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF014EBBE5@NYC-XCH3.win.moorecap.com>
References: <D595C0E05185614C90515F1E8A2D4CBF014EBBE5@NYC-XCH3.win.moorecap.com>
Message-ID: <45974.199.198.251.108.1231954456.squirrel@mail.braverock.com>

The drawdown calculations should work with simple return series.  The
functions internally calculate the cumulative wealth index and use that to
calculate the drawdown. If you have a price series, you should first
convert to a simple return series.

Drawdowns, by definition, are based on returns.

Regards,

  - Brian

On Wed, January 14, 2009 10:46 am, Jorge Nieves wrote:
> Hi,
>
> I have a hypothetical  time series for some cumulative PNL.  I am trying
> to use the findDrawdowns function from the PerformanceAnalytics package.
> When I pass the series to the function, I get an error message which
> meaning I do not understand. Can some one shed some light about what I
> am doing wrong?
> The function seems to work when using returns instead of dollar amounts
> (which is my case). I am also using the maxDrawdown <maxDrawdown.html>
> function, and this last one appears to work well with either returns or
> dollar amounts. I am just wondering if this is driving my error message.
>
> On the other had, the example used in the help(findDrawdowns) is based
> on returns of the edhec[,"Funds.of.Funds"] . These returns appear to be
> monthly returns. I would like to understand if the idea behind the
> findDrawdowns should be applied to a cumulative returns/pnl instead just
> monthly (or other frequency) returns/pnl? I believe that drawdown is
> defined as the difference between today's cumulative pnl and the "high
> water mark" highest point cumulative pnl, but my interpretation could be
> completely wrong. Can some one explain?
>
>
> Thanks,
>
> Jorge
>
> CODE
>> ser <- results[samper,"cumpnl",drop=FALSE]
>> # write.csv(ser,"test.csv")
>> plot(ser,type = "l",col= "red")
>> toto = findDrawdowns(ser)
> Error in if (thisSign == priorSign) { :
>   missing value where TRUE/FALSE needed
>
>  <<test.csv>>
>
> Jorge Nieves
> Phone 212.782.7083
> Fax 212.642.7644
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From jorge.nieves at moorecap.com  Wed Jan 14 22:30:22 2009
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Wed, 14 Jan 2009 16:30:22 -0500
Subject: [R-SIG-Finance] extracting a subTable
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF014EBBF7@NYC-XCH3.win.moorecap.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090114/e6ea43a6/attachment.pl>

From jorge.nieves at moorecap.com  Wed Jan 14 22:37:06 2009
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Wed, 14 Jan 2009 16:37:06 -0500
Subject: [R-SIG-Finance] extracting a subTable
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF014EBBF7@NYC-XCH3.win.moorecap.com>
References: <D595C0E05185614C90515F1E8A2D4CBF014EBBF7@NYC-XCH3.win.moorecap.com>
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF014EBBF8@NYC-XCH3.win.moorecap.com>

sorry,

The data got out of order on the email.. Here is the data in a
spreadsheet.

Jorge
 <<test.xls>> 
> _____________________________________________ 
> From: 	Jorge Nieves  
> Sent:	Wednesday, January 14, 2009 04:30 PM
> To:	r-sig-finance at stat.math.ethz.ch
> Subject:	extracting a subTable
> 
> Hi,
> 
> I have a BIG table with data organized by ID, see example on Table#1
> bellow. I would like to extract a smaller table with the same number
> of columns. The rows should include only the last ROW of each ID.
> Table# 2 is an example extracted form table #1. I tried using the
> SUBSET function and plan to loop, but I was wondering if someone out
> there has a more efficient way to accomplish this.
> 
> You recommendations are highly appreciated.
> 
> Jorge
> 	
> 
> 	Table # 1					Table # 2
> 
> 	Seq	ID	X1	X2		Seq	ID	X1
> X2		
> 	1	15	-616	796		12	15	-148
> 878		
> 	2	15	-661	-455		20	20	-515
> 1,802		
> 	3	15	-355	-1,639		27	45	-138
> 179		
> 	4	15	-1,226	-97		34	70	1,348
> 840		
> 	5	15	506	985
> 
> 	6	15	-612	76
> 
> 	7	15	947	122
> 
> 	8	15	-45	1,980
> 
> 	9	15	295	-1,827
> 
> 	10	15	1,490	-743
> 
> 	11	15	115	1,283
> 
> 	12	15	-148	878
> 
> 	13	20	359	1,297
> 
> 	14	20	-185	-430
> 
> 	15	20	481	800
> 
> 	16	20	1,567	874
> 
> 	17	20	2,365	-167
> 
> 	18	20	1,636	1,649
> 
> 	19	20	-2,741	337
> 
> 	20	20	-515	1,802
> 
> 	21	45	90	624
> 
> 	22	45	-221	345
> 
> 	23	45	-2,168	-1,734
> 
> 	24	45	1,056	-1,070
> 
> 	25	45	678	-80
> 
> 	26	45	1,747	-74
> 
> 	27	45	-138	179
> 
> 	28	70	1,152	-3,167
> 
> 	29	70	-802	-337
> 
> 	30	70	-388	263
> 
> 	31	70	812	342
> 
> 	32	70	-576	972
> 
> 	33	70	653	-737
> 
> 	34	70	1,348	840
> 
> 	
> 
> 	
> 
> Jorge Nieves
> Phone 212.782.7083
> Fax 212.642.7644
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090114/eda87907/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: test.xls
Type: application/vnd.ms-excel
Size: 24576 bytes
Desc: test.xls
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090114/eda87907/attachment.xls>

From jeff.a.ryan at gmail.com  Wed Jan 14 22:40:14 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 14 Jan 2009 15:40:14 -0600
Subject: [R-SIG-Finance] extracting a subTable
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF014EBBF7@NYC-XCH3.win.moorecap.com>
References: <D595C0E05185614C90515F1E8A2D4CBF014EBBF7@NYC-XCH3.win.moorecap.com>
Message-ID: <e8e755250901141340l46b61029q63b796f70bd2cfc@mail.gmail.com>

>
> I have a BIG table with data organized by ID, see example on Table#1
> bellow. I would like to extract a smaller table with the same number of
> columns. The rows should include only the last ROW of each ID. Table# 2
> is an example extracted form table #1. I tried using the SUBSET function
> and plan to loop, but I was wondering if someone out there has a more
> efficient way to accomplish this.
>
> You recommendations are highly appreciated.
>

My recommendation would be not to send R-help questions to R-sig-FINANCE.

:)

This list derives its value from the high signal to noise ratio it
maintains. Please feel free to send all the 'finance' related
questions you deem fit, but send the general R questions to the
appropriate list(s).

Jeff


From cgb at datanalytics.com  Thu Jan 15 00:24:22 2009
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Thu, 15 Jan 2009 00:24:22 +0100
Subject: [R-SIG-Finance] extracting a subTable
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF014EBBF7@NYC-XCH3.win.moorecap.com>
References: <D595C0E05185614C90515F1E8A2D4CBF014EBBF7@NYC-XCH3.win.moorecap.com>
Message-ID: <1231975462.6908.7.camel@kropotkin>

Hello,

Something you can do is first, find the indexes to extract as follows:

indexes.to.extract <- tapply(table.1$Seq, tablle.1$ID, max)

Then, table.2 is just

table.2 <- table.1[ indexes.to.extract, ]

You could do it in just one line. I think this is as efficient as it can
get.

Best regards,

Carlos J. Gil Bellosta
http://www.datanalytics.com


On Wed, 2009-01-14 at 16:30 -0500, Jorge Nieves wrote:
> Hi,
> 
> I have a BIG table with data organized by ID, see example on Table#1
> bellow. I would like to extract a smaller table with the same number of
> columns. The rows should include only the last ROW of each ID. Table# 2
> is an example extracted form table #1. I tried using the SUBSET function
> and plan to loop, but I was wondering if someone out there has a more
> efficient way to accomplish this.
> 
> You recommendations are highly appreciated.
> 
> Jorge
> 	
> 
> 	Table # 1					Table # 2
> 
> 	Seq	ID	X1	X2		Seq	ID	X1
> X2		
> 	1	15	-616	796		12	15	-148
> 878		
> 	2	15	-661	-455		20	20	-515
> 1,802		
> 	3	15	-355	-1,639		27	45	-138
> 179		
> 	4	15	-1,226	-97		34	70	1,348
> 840		
> 	5	15	506	985
> 
> 	6	15	-612	76
> 
> 	7	15	947	122
> 
> 	8	15	-45	1,980
> 
> 	9	15	295	-1,827
> 
> 	10	15	1,490	-743
> 
> 	11	15	115	1,283
> 
> 	12	15	-148	878
> 
> 	13	20	359	1,297
> 
> 	14	20	-185	-430
> 
> 	15	20	481	800
> 
> 	16	20	1,567	874
> 
> 	17	20	2,365	-167
> 
> 	18	20	1,636	1,649
> 
> 	19	20	-2,741	337
> 
> 	20	20	-515	1,802
> 
> 	21	45	90	624
> 
> 	22	45	-221	345
> 
> 	23	45	-2,168	-1,734
> 
> 	24	45	1,056	-1,070
> 
> 	25	45	678	-80
> 
> 	26	45	1,747	-74
> 
> 	27	45	-138	179
> 
> 	28	70	1,152	-3,167
> 
> 	29	70	-802	-337
> 
> 	30	70	-388	263
> 
> 	31	70	812	342
> 
> 	32	70	-576	972
> 
> 	33	70	653	-737
> 
> 	34	70	1,348	840
> 
> 	
> 
> 	
> 
> Jorge Nieves
> Phone 212.782.7083
> Fax 212.642.7644
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From bastian2507hk at yahoo.co.uk  Thu Jan 15 01:11:02 2009
From: bastian2507hk at yahoo.co.uk (Bastian Offermann)
Date: Thu, 15 Jan 2009 01:11:02 +0100
Subject: [R-SIG-Finance] fPortfolio Inputs as List - Error Msg
In-Reply-To: <496D4DC1.4020704@yahoo.co.uk>
References: <496D4DC1.4020704@yahoo.co.uk>
Message-ID: <496E7F16.1040006@yahoo.co.uk>

Nobody? Thanks.

Bastian Offermann schrieb:
> Hello,
>
> I am trying to use flexible inputs for an unconstrained Markowitz 
> optimization using the fPortfolio package.
>
> The documentation says
>
> *"a time series or a named list, containing either a series of returns 
> or named
> entries ?mu? and ?Sigma? being mean and covariance matrix." *
>
> is required as inputs. However, when supplying a list containing mu 
> and sigma I get the following error
>
> *Error: class(data) == "timeSeries" is not TRUE
>
> r.p <- 0.05
> Spec <- portfolioSpec()
> setType(Spec) <- "MV"
> cons <- "Short"
> setOptimize(Spec) <- "minRisk"
> setTargetReturn(Spec) <- r.p/52
> setNFrontierPoints(Spec) <- 10
> setSolver(Spec) <- "solveRshortExact"
>
> mu <- c(0.0002884615, 0.0007085510)
> covar <- matrix(c(0.0002536010, 0.0001704248, 0.0001704248, 
> 0.0007122588), 2, 2)
>
> Data1 <- list(mu, covar)
>
> eff.front1 <- portfolioFrontier(data = Data1, spec = Spec, constraints 
> = cons)
>
> *Any input appreciated. Thanks in advance.
>
> Regards
>
> BO
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From brian at braverock.com  Thu Jan 15 02:56:13 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 14 Jan 2009 19:56:13 -0600
Subject: [R-SIG-Finance] fPortfolio Inputs as List - Error Msg
In-Reply-To: <496E7F16.1040006@yahoo.co.uk>
References: <496D4DC1.4020704@yahoo.co.uk> <496E7F16.1040006@yahoo.co.uk>
Message-ID: <496E97BD.8060108@braverock.com>

Bastian Offermann wrote:
> Nobody? Thanks.

Please post both your code and the data you are using to replicate your 
problem.  If you provide data in addition to the commands you tried, per the 
list posting guidelines, it is much easier for someone else to replicate your 
problem.

Regards,

    - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From martin.becker at mx.uni-saarland.de  Thu Jan 15 09:44:43 2009
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Thu, 15 Jan 2009 09:44:43 +0100
Subject: [R-SIG-Finance] fPortfolio Inputs as List - Error Msg
In-Reply-To: <496D4DC1.4020704@yahoo.co.uk>
References: <496D4DC1.4020704@yahoo.co.uk>
Message-ID: <496EF77B.9060006@mx.uni-saarland.de>

Bastian Offermann wrote:
> Hello,
>
> I am trying to use flexible inputs for an unconstrained Markowitz 
> optimization using the fPortfolio package.
>
> The documentation says
>
> *"a time series or a named list, containing either a series of returns 
> or named
> entries ?mu? and ?Sigma? being mean and covariance matrix." *
>
Which documentation? What version of fPortfolio? I don't see this 
documentation fragment on ?portfolioFrontier for 280.74.
(Besides, Data1 [as defined below] is not a list with named entries ?mu? 
and ?Sigma?.)

Regards,

  Martin

> is required as inputs. However, when supplying a list containing mu 
> and sigma I get the following error
>
> *Error: class(data) == "timeSeries" is not TRUE
>
> r.p <- 0.05
> Spec <- portfolioSpec()
> setType(Spec) <- "MV"
> cons <- "Short"
> setOptimize(Spec) <- "minRisk"
> setTargetReturn(Spec) <- r.p/52
> setNFrontierPoints(Spec) <- 10
> setSolver(Spec) <- "solveRshortExact"
>
> mu <- c(0.0002884615, 0.0007085510)
> covar <- matrix(c(0.0002536010, 0.0001704248, 0.0001704248, 
> 0.0007122588), 2, 2)
>
> Data1 <- list(mu, covar)
>
> eff.front1 <- portfolioFrontier(data = Data1, spec = Spec, constraints 
> = cons)
>
> *Any input appreciated. Thanks in advance.
>
> Regards
>
> BO
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


-- 
Dr. Martin Becker
Statistics and Econometrics
Saarland University
Campus C3 1, Room 206
66123 Saarbruecken
Germany


From j_cuisinier at hotmail.com  Thu Jan 15 11:32:29 2009
From: j_cuisinier at hotmail.com (julien cuisinier)
Date: Thu, 15 Jan 2009 11:32:29 +0100
Subject: [R-SIG-Finance] Report production  in R?
Message-ID: <COL102-W3E0F98584BCEEF42A08E08FD70@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090115/9fc89f00/attachment.pl>

From robert at sanctumfi.com  Thu Jan 15 11:40:30 2009
From: robert at sanctumfi.com (Robert Sams)
Date: Thu, 15 Jan 2009 10:40:30 -0000
Subject: [R-SIG-Finance] Report production  in R?
References: <SANCTUMFISERVERGtXs000000af@sanctumfi.com>
Message-ID: <SANCTUMFISERVERAW8Q000000b4@sanctumfi.com>

Hi Julien,

If you do LaTeX, then Sweave and friends is your solution.

Robert

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of julien
cuisinier
Sent: 15 January 2009 10:32
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Report production in R?


Hi List,
 
 
Is anyone aware of a reporting package in R? 
e.g. if I compute many metrics on a portfolio & want to create directly
in R a nice looking report
 
 
Any idea/suggestion welcome
 
Many thanks
Julien

_________________________________________________________________
Descubre todas las formas en que puedes estar en contacto con amigos y
familiares.

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From brian at braverock.com  Thu Jan 15 12:43:02 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 15 Jan 2009 05:43:02 -0600
Subject: [R-SIG-Finance] Report production  in R?
In-Reply-To: <COL102-W3E0F98584BCEEF42A08E08FD70@phx.gbl>
References: <COL102-W3E0F98584BCEEF42A08E08FD70@phx.gbl>
Message-ID: <496F2146.6070706@braverock.com>

julien cuisinier wrote:
> Is anyone aware of a reporting package in R? 
> e.g. if I compute many metrics on a portfolio & want to create directly in R a nice looking report
>  
> Any idea/suggestion welcome

Robert has already suggested Sweave.

Note that with both graphics and text, you can also output to Postscript and 
PDF files.  With a little work, you can combine your textual data into tables 
(I usually use a data.frame) and output as a PDF.  With a little more work, you 
can combine graphics and tables on a PDF page, all from inside R.

There is also a plugin available for OpenOffice that works like Sweave in OOo.

For future reference, this question belonged on r-help, as it is not really 
finance related.  Many threads on r-help over the years have covered this topic.

Regards,

    - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From nelson.ana at gmail.com  Thu Jan 15 12:50:13 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Thu, 15 Jan 2009 11:50:13 +0000
Subject: [R-SIG-Finance] Report production in R?
In-Reply-To: <496F2146.6070706@braverock.com>
References: <COL102-W3E0F98584BCEEF42A08E08FD70@phx.gbl>
	<496F2146.6070706@braverock.com>
Message-ID: <a7d6d2740901150350l27604052hef0ebf115f81f9e8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090115/18fd9ef7/attachment.pl>

From Wayne.W.Jones at shell.com  Thu Jan 15 12:52:19 2009
From: Wayne.W.Jones at shell.com (Wayne.W.Jones at shell.com)
Date: Thu, 15 Jan 2009 12:52:19 +0100
Subject: [R-SIG-Finance] Report production in R?
In-Reply-To: <a7d6d2740901150350l27604052hef0ebf115f81f9e8@mail.gmail.com>
Message-ID: <8E3590214E4380489D2F8BF129B939F329D5B2@amsdc1-s-03371.europe.shell.com>

Pakcage R2HTML is nice for generating html output.


Wayne


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of Ana Nelson
Sent: 15 January 2009 11:50
To: r-sig-finance at stat.math.ethz.ch
Cc: julien cuisinier
Subject: Re: [R-SIG-Finance] Report production in R?


I have actually started to use a document generation system called Webby,
and a tool for integrating and syntax-highlighting text files called
Idiopidae. It's much more flexible than Sweave (and much less painful to
install). For example, you could integrate the output of an R script into a
HTML or RTF (e.g. Word) document, as well as LaTeX.

http://webby.rubyforge.org/
http://www.zedshaw.com/projects/idiopidae/

You could also just use Idiopidae on its own, it might be all you need.




2009/1/15 Brian G. Peterson <brian at braverock.com>

> julien cuisinier wrote:
>
>> Is anyone aware of a reporting package in R? e.g. if I compute many
>> metrics on a portfolio & want to create directly in R a nice looking report
>>  Any idea/suggestion welcome
>>
>
> Robert has already suggested Sweave.
>
> Note that with both graphics and text, you can also output to Postscript
> and PDF files.  With a little work, you can combine your textual data into
> tables (I usually use a data.frame) and output as a PDF.  With a little more
> work, you can combine graphics and tables on a PDF page, all from inside R.
>
> There is also a plugin available for OpenOffice that works like Sweave in
> OOo.
>
> For future reference, this question belonged on r-help, as it is not really
> finance related.  Many threads on r-help over the years have covered this
> topic.
>
> Regards,
>
>   - Brian
>
> --
> Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From landronimirc at gmail.com  Thu Jan 15 22:52:37 2009
From: landronimirc at gmail.com (Liviu Andronic)
Date: Thu, 15 Jan 2009 22:52:37 +0100
Subject: [R-SIG-Finance] Report production in R?
In-Reply-To: <SANCTUMFISERVERAW8Q000000b4@sanctumfi.com>
References: <SANCTUMFISERVERGtXs000000af@sanctumfi.com>
	<SANCTUMFISERVERAW8Q000000b4@sanctumfi.com>
Message-ID: <68b1e2610901151352q62ba455end0ced6b804404ce3@mail.gmail.com>

On 1/15/09, Robert Sams <robert at sanctumfi.com> wrote:
>  If you do LaTeX, then Sweave and friends is your solution.
>
You can also use LaTeX and Sweave through LyX, a GUI to LaTeX. Please
check the documentation of RcmdrPlugin.Export for more information.
One other option is package `relax'.
Liviu


-- 
Do you know how to read?
http://www.alienetworks.com/srtest.cfm
Do you know how to write?
http://garbl.home.comcast.net/~garbl/stylemanual/e.htm#e-mail


From poirier at jhmi.edu  Sat Jan 17 05:00:41 2009
From: poirier at jhmi.edu (John Poirier)
Date: Fri, 16 Jan 2009 23:00:41 -0500
Subject: [R-SIG-Finance] quantmod tradeModel function
Message-ID: <7810CF4E-2A3B-4FE7-B62F-704554CA11B5@jhmi.edu>

I'm starting to familiarize myself with using the quantmod package,  
coming from a limited background in differential gene expression  
analysis. I thought I would try the simplest code I could think of to  
understand how a typical workflow might work using an example from the  
quantmod manual. I am having some trouble getting this code to work. I  
think the issue might be with not understanding the requirements of  
tradeModel very well. It's not clear to me in the manual how this  
should work. I always learn best by example, if possible.

Ultimately, I want to learn to use these tools because they are also  
applicable to my basic research (we use machine learning code for  
disease classification) and also because I would like to be able to  
manage my personal portfolio using more informed technical indicators  
and evaluate different trading strategies using these models and  
performanceanalytics. Also it's fun. So if you can share code, that  
would be extremely enlightening for me.

Cheers,

John

rm(list=ls())

library(quantmod)

getSymbols('QQQQ',src='yahoo')
q.model = specifyModel(Next(OpCl(QQQQ)) ~ Lag(OpHi(QQQQ),0:3))
test<- 
buildModel 
(q.model,method='lm',training.per=c('2007-01-03','2008-01-03'))
tradeModel(test)
traceback()
sessionInfo()

Results:

 > rm(list=ls())
 >
 > library(quantmod)
 >
 > getSymbols('QQQQ',src='yahoo')
[1] "QQQQ"
 > q.model = specifyModel(Next(OpCl(QQQQ)) ~ Lag(OpHi(QQQQ),0:3))
 > test<- 
buildModel 
(q.model,method='lm',training.per=c('2007-01-03','2008-01-03'))
 > tradeModel(test)
Error in endpoints(xx, on = on.opts[[period]], ...) :
   unused argument(s) (indexAt = "endof")
 > traceback()
7: endpoints(xx, on = on.opts[[period]], ...)
6: periodReturn(x, "monthly", type = type, indexAt = "endof")
5: merge.zoo(..., all = all, fill = fill, suffixes = suffixes,  
retclass = "zoo")
4: cbind.zoo(periodReturn(x, "daily", type = type, leading),  
periodReturn(x,
        "weekly", type = type), periodReturn(x, "monthly", type = type,
        indexAt = "endof"), periodReturn(x, "quarterly", type = type,
        indexAt = "endof"), periodReturn(x, "yearly", type = type))
3: allReturns(model.cumret)
2: modelReturn(quantmodResults, trade.dates = trade.dates, leverage =  
leverage,
        ret.type = ret.type)
1: tradeModel(test)
 > sessionInfo()
R version 2.7.0 (2008-04-22)
i386-apple-darwin8.10.1

locale:
en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] nnet_7.2-44    TTR_0.14-0     quantmod_0.3-7 Defaults_1.1-1  
xts_0.6-4
[6] zoo_1.5-4

loaded via a namespace (and not attached):
[1] grid_2.7.0      lattice_0.17-15 tools_2.7.0
 >


From ron_michael70 at yahoo.com  Sun Jan 18 18:51:12 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Sun, 18 Jan 2009 09:51:12 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] VAR process
Message-ID: <21530701.post@talk.nabble.com>


Hi,

In every book on VAR [Vector auto regression] I see that, any VAR [p]
process can be expressed as a VAR [1] process. Here my question is how it
can be possible? When you change it to a VAR [1] process, the VCV matrix of
Innovations contains zero and hence it is not of full rank. Therefore it is
not a PD matrix, you cannot decompose that according cholesky decomposition
and lot more things can not be done with it because VCV matrix is singular.
Then how can that process be a VAR process?
-- 
View this message in context: http://www.nabble.com/VAR-process-tp21530701p21530701.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From sergeyg at gmail.com  Mon Jan 19 17:24:32 2009
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Mon, 19 Jan 2009 17:24:32 +0100
Subject: [R-SIG-Finance] Problem with RBloomberg retval argument
Message-ID: <7cb007bd0901190824h5f2bf869h2f21023139468cd8@mail.gmail.com>

Hello everyone,

I have two issues that I want to ask.

1)
I have problems with loading data with RBloomberg.
More precisely, I do not seem to be able to load data into a data.frame.
I get an error message:

start.date 	<- as.chron("1990-01-19")
end.date	<- as.chron("2009-01-19")

conn <- blpConnect(show.days="week", na.action="na", periodicity="daily")

> bldata <- blpGetData(conn, c("ED4 Comdty", "ED12 Comdty"), "PX_LAST", start=start.date, end=end.date, retval="data.frame")
Error in if (typ[n] == "character") { : argument is of length zero

What does that error message mean and what can I do to avoid this error message?

2)
If I load data from Bloomberg in matrix format, the date is converted
to a number. For example:

> bldata <- blpGetData(conn, "ED4 Comdty", "PX_LAST", start=start.date, end=end.date, retval="matrix")
> head(bldata)
     [DATETIME] PX_LAST
[1,]      32892   91.36
[2,]      32895   91.37
[3,]      32896   91.38
[4,]      32897   91.38
[5,]      32898   91.34
[6,]      32899   91.28

I tried to convert the number to normal date and by trial and error I
found the following:

> as.Date(32892, "1899-12-30")
[1] "1990-01-19"

Is it really true that count starts from December 30th 1899? Why?

Thank you in advance for your help!

Regards,
Sergey


From nelson.ana at gmail.com  Mon Jan 19 17:50:38 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Mon, 19 Jan 2009 16:50:38 +0000
Subject: [R-SIG-Finance] Problem with RBloomberg retval argument
In-Reply-To: <7cb007bd0901190824h5f2bf869h2f21023139468cd8@mail.gmail.com>
References: <7cb007bd0901190824h5f2bf869h2f21023139468cd8@mail.gmail.com>
Message-ID: <a7d6d2740901190850i585a75c7t4d6903d491623c52@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090119/9ae5c874/attachment.pl>

From edd at debian.org  Mon Jan 19 18:04:39 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 19 Jan 2009 11:04:39 -0600
Subject: [R-SIG-Finance] Problem with RBloomberg retval argument
In-Reply-To: <7cb007bd0901190824h5f2bf869h2f21023139468cd8@mail.gmail.com>
References: <7cb007bd0901190824h5f2bf869h2f21023139468cd8@mail.gmail.com>
Message-ID: <18804.45735.725963.533360@ron.nulle.part>


On 19 January 2009 at 17:24, Sergey Goriatchev wrote:
| Hello everyone,
| 
| I have two issues that I want to ask.
|
| 1)
| I have problems with loading data with RBloomberg.
| More precisely, I do not seem to be able to load data into a data.frame.
| I get an error message:
| 
| start.date 	<- as.chron("1990-01-19")
| end.date	<- as.chron("2009-01-19")
| 
| conn <- blpConnect(show.days="week", na.action="na", periodicity="daily")
| 
| > bldata <- blpGetData(conn, c("ED4 Comdty", "ED12 Comdty"), "PX_LAST", start=start.date, end=end.date, retval="data.frame")
| Error in if (typ[n] == "character") { : argument is of length zero
| 
| What does that error message mean and what can I do to avoid this error message?

Your problem is not really in the assignment to bldata. Your problme is
indicated by the 'argument is of length zero': you didn't get any data from
Bloomberg.  You need to fix that first.  As a first guess, try a less
'greedy' retrieval.  You are unlikely to get 20 years of daily data in one
request.
 
| 2)
| If I load data from Bloomberg in matrix format, the date is converted
| to a number. For example:
| 
| > bldata <- blpGetData(conn, "ED4 Comdty", "PX_LAST", start=start.date, end=end.date, retval="matrix")
| > head(bldata)
|      [DATETIME] PX_LAST
| [1,]      32892   91.36
| [2,]      32895   91.37
| [3,]      32896   91.38
| [4,]      32897   91.38
| [5,]      32898   91.34
| [6,]      32899   91.28
| 
| I tried to convert the number to normal date and by trial and error I
| found the following:
| 
| > as.Date(32892, "1899-12-30")
| [1] "1990-01-19"
|
| Is it really true that count starts from December 30th 1899? Why?

Different offsets for 'the first known day' between different applications
and systems, eg Excel uses something very different from Unix etc.

Try return type 'zoo' instead of matrix -- zoo is a class that knows how to
deal with dates.  You may need to run    install.packages("zoo")   first.

Hope this helps, Dirk

-- 
Three out of two people have difficulties with fractions.


From michael.sankowski at gmail.com  Mon Jan 19 18:30:52 2009
From: michael.sankowski at gmail.com (Michael Sankowski)
Date: Mon, 19 Jan 2009 11:30:52 -0600
Subject: [R-SIG-Finance] Problem with RBloomberg retval argument
In-Reply-To: <18804.45735.725963.533360@ron.nulle.part>
References: <7cb007bd0901190824h5f2bf869h2f21023139468cd8@mail.gmail.com>
	<18804.45735.725963.533360@ron.nulle.part>
Message-ID: <b34262a80901190930u7e7b1547v9503320b1c3c249d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090119/4a586487/attachment.pl>

From sergeyg at gmail.com  Tue Jan 20 09:04:20 2009
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Tue, 20 Jan 2009 09:04:20 +0100
Subject: [R-SIG-Finance] Problem with RBloomberg retval argument
In-Reply-To: <b34262a80901190930u7e7b1547v9503320b1c3c249d@mail.gmail.com>
References: <7cb007bd0901190824h5f2bf869h2f21023139468cd8@mail.gmail.com>
	<18804.45735.725963.533360@ron.nulle.part>
	<b34262a80901190930u7e7b1547v9503320b1c3c249d@mail.gmail.com>
Message-ID: <7cb007bd0901200004x2cfe61f6i89a5961dbfbf781c@mail.gmail.com>

Dear Ana, Dirk, and Michael

Thank you for your replies, I appreciate that!

Now, let me take is point by point:

1) Ana, I tried retval="data.frame" with just one ticker and that did
not work either. In this particular case I need to load 16 variables
from Bloomberg, and merging them all (given retval="data.frame" were
working) would be quite a hassle. Even though I love to mess about
with R code, this time I am trying to avoid any unneccessary work. :-)

2) Dirk, I tried the same retrieval with retval set to matrix, zoo,
and raw, and it always worked. In fact, in this particular case, like
I mentioned above, I am retrieving 16 instruments from Bloomberg, 20
years of data for each. With zoo it works without a hitch. I am quite
familiar with zoo package, well, I worked with it extensively, but
some of the characteristics of zoo make me prefer data.frame, probably
because I am not a real expert user when it comes to handling zoo
objects.

3) Michael, I have already extensively explored the Bloomberg-Excel-R
route, pulling long histories for about a hundred instruments into
different sheets in an Excel file, then running background VBA code
that prepares the sheet for R, then cleaning, converting, merging
certain variables from different sheets of Excel file in R. I played
around with zoo objects while doing that. Wrote a long piece of code
to massage the data into specific format for use with
PerformanceAnalytics (lovely package, btw, but timestamp handling was
a beatch for me, can't wait when the guys improve that package!). I am
quite proud of the code I wrote, though for experienced R users it
will most likely seem crude and verbose. That is why I am trying to
avoid any unnecessary steps and want to load data into R directly from
Bloomberg.

With respect to zoo object I have a question. When I load data from
Bloomberg into a zoo object (called bldata, for example), I can do
head(bldata) and I see first rows of the array, with timestamps and
column names. I tried to save bldata and open it on a different
machine running R, and though bldata is still a zoo object, doing
head(bldata) only brings up first few rows of the first column (like a
vector) without timestamps and colname. I tried to save bldata as a
.RData file, and I tried to save the whole workspace and then move it
to another R machine - same result. This is the problem with
timestamps, as far as I understand. If I do something like
attributes(bldata)[[2]] <- dates, where dates is a previously saved
column of the timestamps (in character format), then head(bldata)
works normal again. I wonder if there is any way to transfer bldata
between machines without loosing the formating?

Finally, thank you for your explanation of 1899 timestart. For some
reason, I always thought 01-01-1970 was the day from which count
begins in R.

Regards,
Sergey

On Mon, Jan 19, 2009 at 18:30, Michael Sankowski
<michael.sankowski at gmail.com> wrote:
> For very large data pulls with Bloomberg, I found it easier to pull into
> excel and then use read.table to get into R.   Like Dirk said, for some
> reason, Bloomberg does not like it when are greedy with data pulls.
> I pulled fundamental data across the Russell 3000 for a long time period
> into excel and then loaded into xts.  It was easier to check if the data was
> all there visually, then push into R.
> I used this code to create the xts objects from the flat files.
> f.import_data <- function(datamatrixfile) {
> #11/01/08
> jj.holder <- read.table(datamatrixfile, header = TRUE, row.names = 1, sep =
> '\t')
> xts.datamatrix <- xts(jj.holder, as.Date(rownames(jj.holder), format =
> '%m/%d/%Y'))
> return(xts.datamatrix)
> }
> HTH
>
> On Mon, Jan 19, 2009 at 11:04 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
>>
>> On 19 January 2009 at 17:24, Sergey Goriatchev wrote:
>> | Hello everyone,
>> |
>> | I have two issues that I want to ask.
>> |
>> | 1)
>> | I have problems with loading data with RBloomberg.
>> | More precisely, I do not seem to be able to load data into a data.frame.
>> | I get an error message:
>> |
>> | start.date    <- as.chron("1990-01-19")
>> | end.date      <- as.chron("2009-01-19")
>> |
>> | conn <- blpConnect(show.days="week", na.action="na",
>> periodicity="daily")
>> |
>> | > bldata <- blpGetData(conn, c("ED4 Comdty", "ED12 Comdty"), "PX_LAST",
>> start=start.date, end=end.date, retval="data.frame")
>> | Error in if (typ[n] == "character") { : argument is of length zero
>> |
>> | What does that error message mean and what can I do to avoid this error
>> message?
>>
>> Your problem is not really in the assignment to bldata. Your problme is
>> indicated by the 'argument is of length zero': you didn't get any data
>> from
>> Bloomberg.  You need to fix that first.  As a first guess, try a less
>> 'greedy' retrieval.  You are unlikely to get 20 years of daily data in one
>> request.
>>
>> | 2)
>> | If I load data from Bloomberg in matrix format, the date is converted
>> | to a number. For example:
>> |
>> | > bldata <- blpGetData(conn, "ED4 Comdty", "PX_LAST", start=start.date,
>> end=end.date, retval="matrix")
>> | > head(bldata)
>> |      [DATETIME] PX_LAST
>> | [1,]      32892   91.36
>> | [2,]      32895   91.37
>> | [3,]      32896   91.38
>> | [4,]      32897   91.38
>> | [5,]      32898   91.34
>> | [6,]      32899   91.28
>> |
>> | I tried to convert the number to normal date and by trial and error I
>> | found the following:
>> |
>> | > as.Date(32892, "1899-12-30")
>> | [1] "1990-01-19"
>> |
>> | Is it really true that count starts from December 30th 1899? Why?
>>
>> Different offsets for 'the first known day' between different applications
>> and systems, eg Excel uses something very different from Unix etc.
>>
>> Try return type 'zoo' instead of matrix -- zoo is a class that knows how
>> to
>> deal with dates.  You may need to run    install.packages("zoo")   first.
>>
>> Hope this helps, Dirk
>>
>> --
>> Three out of two people have difficulties with fractions.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
>
>
> --
> Michael Sankowski, CFA, CAIA
> 312.404.8717
> michael.sankowski at gmail.com
>



-- 
I'm not young enough to know everything. /Oscar Wilde
Experience is one thing you can't get for nothing. /Oscar Wilde
When you are finished changing, you're finished. /Benjamin Franklin
Tell me and I forget, teach me and I remember, involve me and I learn.
/Benjamin Franklin
Luck is where preparation meets opportunity. /George Patten


From nelson.ana at gmail.com  Tue Jan 20 10:13:18 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Tue, 20 Jan 2009 09:13:18 +0000
Subject: [R-SIG-Finance] Problem with RBloomberg retval argument
In-Reply-To: <7cb007bd0901200004x2cfe61f6i89a5961dbfbf781c@mail.gmail.com>
References: <7cb007bd0901190824h5f2bf869h2f21023139468cd8@mail.gmail.com>
	<18804.45735.725963.533360@ron.nulle.part>
	<b34262a80901190930u7e7b1547v9503320b1c3c249d@mail.gmail.com>
	<7cb007bd0901200004x2cfe61f6i89a5961dbfbf781c@mail.gmail.com>
Message-ID: <a7d6d2740901200113v4c180405xbfe46c8d46ce1c7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090120/7a392520/attachment.pl>

From ron_michael70 at yahoo.com  Tue Jan 20 14:44:29 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Tue, 20 Jan 2009 05:44:29 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Confusing result with AIC and ACF
Message-ID: <21563337.post@talk.nabble.com>


Hi, I have a strange problem with following dataset :

-0.075851693
-0.046125504
-0.009117161
0.025569817
0.034882743
0.073671497
0.063805297
0.062306796
0.072343820
0.058354121
-0.007635359
0.086790779
0.085487789
0.113577103
0.021293381
0.089423068
0.090485998
0.128847827
0.011859335
0.058794744
0.065909368
0.020887431
0.085387467
0.097375525
0.108981417
0.044289044
0.071428571
0.052430556
0.056307049
0.041957314

If I see the ACF then, it seems that this values are random. However next I
fit two regressions and get the AIC values :

1. regression of y (above series) on : y[-1], y[-2], y[-3], y[-4] and x[-1],
x[-2], x[-3], x[-4]

2. regression of y on y[-1], y[-2], y[-3] and x[-1], x[-2], x[-3], x[-4]

Here y[-p] is the p-th lagged value. and x is some other variable.

AIC for those regressions are : -6.673 and -6.636. This means 4-th lag of y
has some explanatory power on y. Therefore this result is coming confusing
when I compare with ACF figures [which tells y is random]

Can ppl here suggest me what inference I should make on y?

Regards,

-- 
View this message in context: http://www.nabble.com/Confusing-result-with-AIC-and-ACF-tp21563337p21563337.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From roger at bergande.ch  Tue Jan 20 18:02:34 2009
From: roger at bergande.ch (roger at bergande.ch)
Date: Tue, 20 Jan 2009 18:02:34 +0100
Subject: [R-SIG-Finance] Problem with RBloomberg retval argument
In-Reply-To: <761A199DCD7BD14189945879D96801830376EE79@NS5118.swiss.group4net.com>
References: <761A199DCD7BD14189945879D96801830376EE79@NS5118.swiss.group4net.com>
Message-ID: <20090120180234.dn3ku8x20ckkc04s@mail.bergande.ch>

Hello Sergey

There seems to be a small bug in the function which returns the data.frame.

To fix it simply save the following function in a file and source it  
after you load library(RBloomberg)

It works at least on my machine.

Regards,
Roger

#######################################################
rm(list=ls(all=TRUE))
library(RBloomberg)
source("H:/ROGB/downloads/Rsource/RBloomberg/R/as.data.frame.R")

.bbfields <- blpReadFields(path = "C:/Program Files/blp/API")

start.date 	<- as.chron("1990-01-19")
end.date	<- as.chron("2009-01-19")

conn <- blpConnect(show.days="week", na.action="na", periodicity="daily")

bldata <- blpGetData(conn, c("ED4 Comdty", "ED12 Comdty") ,"PX_LAST",  
start=start.date, end=end.date, retval="data.frame")

#########################################################

as.data.frame.BlpCOMReturn <- function(x, row.names = NULL, optional =
                                        FALSE){
   bbfields <- .bbfields
   lst <- list()
   mtx <- as.matrix.BlpCOMReturn(x)
   cols <- colnames(mtx)
   flds <- attr(x, "fields")
   secs <- attr(x, "securities")
   blds <- attr(x, "barfields")
   ndat <- attr(x, "num.of.date.cols")
   ## if date column exists, convert it to chron
   if(ndat != 0){
     dtime <- as.chron.COMDate(mtx[,1])
     mtx <- matrix(mtx[, 2:ncol(mtx)], ncol=ncol(mtx) - 1)
   }
   ## convert all other columns to appropriate datatype
   if(!is.null(blds)){
     fields <- blds
   }else{
     fields <- flds
   }
   ####################### fix ############################################
   typTmp <- dataType(fields, bbfields)
   typ <- rep(typTmp,length(secs))
   ########################################################################
   for(n in seq(1, ncol(mtx))){
     # n = 2
     vec <- mtx[,n]
     if(typ[n] == "character"){
       lst <- append(lst, list(as.character(vec)))
     }else if(typ[n] == "double"){
       lst <- append(lst, list(as.numeric(vec)))
     }else if(typ[n] == "logical"){
       lst <- append(lst, list(as.logical(vec)))
     }else if(typ[n] == "chron"){
       lst <- append(lst, list(as.chron(vec)))
     }
   }
   if(ndat != 0){
     lst <- append(list(dtime), lst)
     df <- as.data.frame.list(lst)
     colnames(df) <- cols
   }else{
     df <- as.data.frame.list(lst)
     colnames(df) <- flds
     rownames(df) <- secs
   }
   return(df)
}

###################################################





>
>
> -----Original Message-----
> From: roger at bergande.ch [mailto:roger at bergande.ch]
> Sent: Dienstag, 20. Januar 2009 16:36
> To: Bergande Roger (FI/RM)
> Subject: Fwd: [R-SIG-Finance] Problem with RBloomberg retval argument
>
>
>
> ----- Weitergeleitete Nachricht von sergeyg at gmail.com -----
>       Datum: Mon, 19 Jan 2009 17:24:32 +0100
>         Von: Sergey Goriatchev <sergeyg at gmail.com>
> Antwort an: Sergey Goriatchev <sergeyg at gmail.com>
>     Betreff: [R-SIG-Finance] Problem with RBloomberg retval argument
>          An: r-sig-finance at stat.math.ethz.ch
>
> Hello everyone,
>
> I have two issues that I want to ask.
>
> 1)
> I have problems with loading data with RBloomberg.
> More precisely, I do not seem to be able to load data into a data.frame.
> I get an error message:
>
> start.date 	<- as.chron("1990-01-19")
> end.date	<- as.chron("2009-01-19")
>
> conn <- blpConnect(show.days="week", na.action="na",
> periodicity="daily")
>
>> bldata <- blpGetData(conn, c("ED4 Comdty", "ED12 Comdty"),
>> "PX_LAST", start=start.date, end=end.date, retval="data.frame")
> Error in if (typ[n] == "character") { : argument is of length zero
>
> What does that error message mean and what can I do to avoid this
> error message?
>
> 2)
> If I load data from Bloomberg in matrix format, the date is converted
> to a number. For example:
>
>> bldata <- blpGetData(conn, "ED4 Comdty", "PX_LAST",
>> start=start.date, end=end.date, retval="matrix")
>> head(bldata)
>       [DATETIME] PX_LAST
> [1,]      32892   91.36
> [2,]      32895   91.37
> [3,]      32896   91.38
> [4,]      32897   91.38
> [5,]      32898   91.34
> [6,]      32899   91.28
>
> I tried to convert the number to normal date and by trial and error I
> found the following:
>
>> as.Date(32892, "1899-12-30")
> [1] "1990-01-19"
>
> Is it really true that count starts from December 30th 1899? Why?
>
> Thank you in advance for your help!
>
> Regards,
> Sergey
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
>
> ----- Ende der weitergeleiteten Nachricht -----
>
>
>
>
>


From vlanschot at yahoo.com  Wed Jan 21 18:08:44 2009
From: vlanschot at yahoo.com (R@Nabble)
Date: Wed, 21 Jan 2009 09:08:44 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Any suitable backtest functions?
In-Reply-To: <20814024.post@talk.nabble.com>
References: <tencent_1DB9EF5A5AD0DB5A65496439@qq.com>
	<tencent_69465C367F33C3FC0D0AC06B@qq.com>
	<4935FCB9.1080000@braverock.com> <20814024.post@talk.nabble.com>
Message-ID: <21587613.post@talk.nabble.com>


You may want to have a look at Amibroker (www.amibroker.com). It is cheap but
much more powerful and versatile than Wealth Lab, Trade Station, etc. More
importantly, it has an R plug-in, freely available for registred AB-users,
which allows you to backtest any of your R-models within AB.

PS

After learning from replies of the list, I think maybe I shoud not use
'backtest' here.
I am new to R so my future job would consist of  two phase.

Phase One:  Analysis of Signals.
Step 1.  Signal generating:   All trading systems should have signals for
trade.  Even the trading system of the great Simons, I think.    Signal
generating would be the first step, using technical indicators, fundamental
changes, news, statistical  indicators or whatever you like.   The
flexibility of R?makes it the best candidate for signal generating in
prototyping process.  But it would be hard to define some univesal functions
for signal generating.  
Step 2.  Signal analysis:   If we generate trading signals according to some
specific ideas, then, the 2nd step would be analyzing the statistical
pattern of such signals and the following yeild/risk pattern.   The
frequence of the signal, the distribution of the signals.  If the signal is
some threshold of an indicator, then the distribution of the indicator. and
so on.  And the maximum yeild/risk during the following days/months.   There
would be different yeild/risk distribution pattern in different time frames.  
The most useful output would be histgram and charts and so on since I don't
think such distributions would obey specific classical distributions in
textbook.    I wonder whether there are some packages help me in the step 2.   
Indeed, there would be difference between studies on single instruments such
as futures of euro or  index and studies on stocks in a stock market.    The
latter should consider the sorting of thousands of stocks. 

If there are some signal robust and significant in alpha generating, then
comes the phase two.
Phase Two: Backtest/Simulation.
There maybe some packages focusing in this phase.  Maybe not.  Backtest for
stock markets is different from backtest for single instrument.    

It's the step 2 of phase 1 that I am most interested in currently.  So I
think maybe I should not use 'backtest' in the topic.   

Regards,
Wind


Brian G. Peterson wrote:
> 
> Wind wrote:
>> I have checked backtest package.  it seems there is no such functions I
>> needed.
>> I will check it again though.
>> Thanks Gabor.
> 
> Can you please be specific about what features you are looking for? 
> There has been some discussion of creating a more generally applicable 
> backtest framework, so I'm curious what your requirements might be, and 
> how much effort you'd be willing to put into a shared effort.
> 
> Regards,
> 
>      - Brian
> 
> 

-- 
View this message in context: http://www.nabble.com/Any-suitable-backtest-functions--tp20789888p21587613.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From hardendl at ensignpeak.org  Wed Jan 21 19:10:47 2009
From: hardendl at ensignpeak.org (quant_PM)
Date: Wed, 21 Jan 2009 10:10:47 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Plotrix Graph - diagonal labels for
	graph
Message-ID: <21223355.post@talk.nabble.com>


Notice how the labels are not readable due to the number of 'points'.  I have
tried to 'tilt' the labels on this graph to show diagonally (more readable),
but can't seem to make it happen.  For example see the below code....

Any help would be appreciated.

Thanks,
Dave

corelations <- c(1:97)
corelation.names <- names(corelations) <- c("Alp12Mn", "AvrROE", "DivToP",
"GrowAPS", "GrowAsst", "GrowBPS", "GrowCFPS", "GrowDPS", "GrowEPS",
"GrowSPS", "HistAlp", "HistSigm", "InvVsSal", "LevGrow", "Payout5",
"PredSigm", "RecVsSal", "Ret12Mn", "Ret3Mn", "Ret1Mn", "ROE", "_CshPlow",
"_DDM", "_EarnMom", "_EstChgs", "_EstRvMd", "_Neglect", "_NrmEToP",
"_PredEToP", "_RelStMd", "_ResRev", "_SectMom", "AssetToP",
"ARM_Pref_Earnings", "AvrCFtoP", "AvrDtoP", "AvrEtoP", "ARM_Sec_Earnings",
"BondSens", "BookToP", "Capt", "CaptAdj", "CashToP", "CshFlToP", "CurrSen",
"DivCuts5", "EarnToP", "Earnvar", "Earnyld", "Growth", "HistBeta",
"IndConc", "Leveflag", "Leverag", "Leverage", "Lncap", "Momentum",
"Payoflag", "PredBeta", "Ret_11M_Momentum", "PotDilu", "Price", "ProjEgro",
"RecEPSGr", "SalesToP", "Size", "SizeNonl", "Tradactv", "TradVol", "Value",
"VarDPS", "Volatility", "Yield", "CFROI", "ADJUST", "ERC", "RC", "SPX",
"R1000", "MarketCap", "TotalRisk", "Value_AX", "truncate_ret_1mo",
"truncate_PredSigma", "Residual_Returns", "ARM_Revenue", "ARM_Rec_Comp",
"ARM_Revisions_Comp", "ARM_Global_Rank", "ARM_Score", "TEMP", "EQ_Raw",
"EQ_Region_Rank", "EQ_Acc_Comp", "EQ_CF_Comp", "EQ_Oper_Eff_Comp",
"EQ_Exc_Comp")


corelations <- c(0.223, 0.1884, -0.131, 0.1287, 0.0307, 0.2003, 0.2280,
0.1599, 0.2680, 0.2596, 0.3399, 0.0324, 0.0382, -0.173, -0.177, -0.056,
-0.063, 0.2211, 0.0674, -0.023, 0.2641, 0.2369, 0.1652, -0.023, 0.1070,
0.0791, -0.023, 0.0434, -0.002, -0.001, -0.000, -0.108, -0.288, 0.1504,
-0.127, -0.142, 0.0852, 0, -0.031, -0.320, 0.0785, 0.0465, -0.166, 0.1416,
0.0945, -0.063, 0.1461, -0.305, 0.1215, 0.0776, 0.0449, 0.0823, -0.018,
-0.261, -0.318, 0.1194, 0.3151, -0.124, 0.1037, 0.2240, -0.115, 0.1543, 0,
0.1775, -0.153, 0.1194, 0.1407, 0.1047, 0.0926, -0.403, 0.0067, -0.048,
-0.136, 0.1068, 0.0381, 0.1878, -0.035, 0.0761, 0.0784, 0, 0, 0, -0.018,
0.1602, 0.0543, 0, -0.013, 0.1439, 0, 0, -0.054, 0.7426, 0.7510, 0.1657,
0.1657, 0.4949, 1.0000)
require(plotrix)
radial.plot(corelations,
labels=corelation.names,rp.type="p",main="Correlation Radar",
radial.lim=c(-1,1),line.col="blue")


-- 
View this message in context: http://www.nabble.com/Plotrix-Graph---diagonal-labels-for-graph-tp21223355p21223355.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From vijay at MyEmailForever.com  Wed Jan 21 20:36:35 2009
From: vijay at MyEmailForever.com (Vijay Vaidyanathan)
Date: Wed, 21 Jan 2009 11:36:35 -0800
Subject: [R-SIG-Finance] [R-sig-finance] Any suitable backtest functions?
In-Reply-To: <21587613.post@talk.nabble.com>
References: <tencent_1DB9EF5A5AD0DB5A65496439@qq.com>
	<tencent_69465C367F33C3FC0D0AC06B@qq.com>
	<4935FCB9.1080000@braverock.com>
	<20814024.post@talk.nabble.com> <21587613.post@talk.nabble.com>
Message-ID: <D6C62E2A-3DEE-4766-BCB2-61AD56BEF850@MyEmailForever.com>

On Jan 21, 2009, at 9:08 AM, R at Nabble wrote:
>
> After learning from replies of the list, I think maybe I shoud not use
> 'backtest' here.

I don't know if this will help, but one approach to portfolio  
simulation is: to assemble portfolios at regular intervals in time by  
using a certain selection criterion, and then simulate the behaviour  
of that portfolio over time. Youi'll need to specify how to weight  
the portfolio and how long to revisit the weighting and reselection  
decisions etc. This is the model that is used in a number of academic  
studies.

A couple of years ago, I implemented this model, and wrote a  
portfolio simulation package in R. I called it PAST, (Portfolio  
Attribution and Simulation Toolkit) and it was designed to be  
database independent. Unfortunately, it isn't in CRAN yet, but I've  
spent the last couple of months cleaning it up and bundling it as an  
R package.

The major outstanding issue is that It still lacks decent  
documentation, which I am working on and expect to get done in the  
next month or two, after which I hope to submit it to CRAN. In the  
meanwhile, if you can't wait for a month or so, just drop me an email  
message off-list and, I'll send you a link to it on the web so you  
can play with it.

Also, if you are in the San Francisco Bay Area, I believe that Dave  
Stewart will be giving a talk at the monthly meeting of the Silicon  
Valley SI Pro User Group of the American Association of Individual  
Investors on Feb 2 (at the Saratoga Public Library, 6:30 pm). He'll  
talk about using the software, including helping you getting it  
installed and running some basic simulations. Unfortunately, I will  
be traveling and won't be at the meeting, but you'll be in good hands  
with Dave!

- Vijay
======


From peter at braverock.com  Wed Jan 21 21:54:26 2009
From: peter at braverock.com (Peter Carl)
Date: Wed, 21 Jan 2009 14:54:26 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Any suitable backtest functions?
In-Reply-To: <D6C62E2A-3DEE-4766-BCB2-61AD56BEF850@MyEmailForever.com>
References: <tencent_1DB9EF5A5AD0DB5A65496439@qq.com>
	<21587613.post@talk.nabble.com>
	<D6C62E2A-3DEE-4766-BCB2-61AD56BEF850@MyEmailForever.com>
Message-ID: <200901211454.26423.peter@braverock.com>

I'd be very interested in PAST.  I'm glad to see you're finding 
PerformanceAnalytics useful, too... Any feedback you might have on that front 
would be very welcome.

My co-author, Brian Peterson, and Jeffrey Ryan (of xts fame) have all put our 
heads together on a trade simulation package called 'blotter', which you can 
find on r-forge.r-project.org.  It takes the opposite approach to PAST - using  
instruments, transactions, P&L, and portfolios - but at some point I'd love to 
combine both approaches (weights/returns and instruments/transactions) into a 
uniform interface.  blotter is very incomplete at the moment, but we're making 
slow progress.

pcc

On Wednesday 21 January 2009 1:36:35 pm Vijay Vaidyanathan wrote:
> On Jan 21, 2009, at 9:08 AM, R at Nabble wrote:
> > After learning from replies of the list, I think maybe I shoud not use
> > 'backtest' here.
>
> I don't know if this will help, but one approach to portfolio
> simulation is: to assemble portfolios at regular intervals in time by
> using a certain selection criterion, and then simulate the behaviour
> of that portfolio over time. Youi'll need to specify how to weight
> the portfolio and how long to revisit the weighting and reselection
> decisions etc. This is the model that is used in a number of academic
> studies.
>
> A couple of years ago, I implemented this model, and wrote a
> portfolio simulation package in R. I called it PAST, (Portfolio
> Attribution and Simulation Toolkit) and it was designed to be
> database independent. Unfortunately, it isn't in CRAN yet, but I've
> spent the last couple of months cleaning it up and bundling it as an
> R package.
>
> The major outstanding issue is that It still lacks decent
> documentation, which I am working on and expect to get done in the
> next month or two, after which I hope to submit it to CRAN. In the
> meanwhile, if you can't wait for a month or so, just drop me an email
> message off-list and, I'll send you a link to it on the web so you
> can play with it.
>
> Also, if you are in the San Francisco Bay Area, I believe that Dave
> Stewart will be giving a talk at the monthly meeting of the Silicon
> Valley SI Pro User Group of the American Association of Individual
> Investors on Feb 2 (at the Saratoga Public Library, 6:30 pm). He'll
> talk about using the software, including helping you getting it
> installed and running some basic simulations. Unfortunately, I will
> be traveling and won't be at the meeting, but you'll be in good hands
> with Dave!
>
> - Vijay
> ======
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.

-- 
Peter Carl
145 Scottswood Rd
Riverside, IL 60546
312 307 6346


From comtech.usa at gmail.com  Thu Jan 22 02:17:46 2009
From: comtech.usa at gmail.com (Michael)
Date: Wed, 21 Jan 2009 17:17:46 -0800
Subject: [R-SIG-Finance] how to study the lead and lag relation of two time
	series?
Message-ID: <b1f16d9d0901211717ka4d890fgbec1424068aceaa0@mail.gmail.com>

Hi all,

Is there a way to study the lead and lag relation of two time series?

Let's say I have two time series, At and Bt. Is there a systematic way
of concluding whether it's A leading B or B leading A and by how much?

Thanks!


From matthieu.stigler at gmail.com  Thu Jan 22 05:38:26 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Thu, 22 Jan 2009 10:08:26 +0530
Subject: [R-SIG-Finance] how to study the lead and lag relation of two
 time series?
In-Reply-To: <b1f16d9d0901211717ka4d890fgbec1424068aceaa0@mail.gmail.com>
References: <b1f16d9d0901211717ka4d890fgbec1424068aceaa0@mail.gmail.com>
Message-ID: <4977F842.8010405@gmail.com>

Michael a ?crit :
> Hi all,
>
> Is there a way to study the lead and lag relation of two time series?
>
> Let's say I have two time series, At and Bt. Is there a systematic way
> of concluding whether it's A leading B or B leading A and by how much?
>
> Thanks!
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>   
You can use cross-correlation:
a<-rnorm(100)
 > b<-runif(100)
 > ccf<-ccf(a,b)
plot(ccf)
ccf$acf

Or use some VAR from package vars.

Bests Mat


From cwrward at gmail.com  Thu Jan 22 10:15:55 2009
From: cwrward at gmail.com (Charles Ward)
Date: Thu, 22 Jan 2009 09:15:55 +0000
Subject: [R-SIG-Finance] how to study the lead and lag relation of two
	time series?
In-Reply-To: <4977F842.8010405@gmail.com>
References: <b1f16d9d0901211717ka4d890fgbec1424068aceaa0@mail.gmail.com>
	<4977F842.8010405@gmail.com>
Message-ID: <bd9aa36b0901220115k7dd926d8j91551c5933ceda87@mail.gmail.com>

Google "Granger Causality"
or use Wikipedia and you will find a reference to testing for Granger
Causality using R

CW

On Thu, Jan 22, 2009 at 4:38 AM, Matthieu Stigler
<matthieu.stigler at gmail.com> wrote:
> Michael a ?crit :
>>
>> Hi all,
>>
>> Is there a way to study the lead and lag relation of two time series?
>>
>> Let's say I have two time series, At and Bt. Is there a systematic way
>> of concluding whether it's A leading B or B leading A and by how much?
>>
>> Thanks!
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
> You can use cross-correlation:
> a<-rnorm(100)
>> b<-runif(100)
>> ccf<-ccf(a,b)
> plot(ccf)
> ccf$acf
>
> Or use some VAR from package vars.
>
> Bests Mat
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From barth at tac-financial.com  Thu Jan 22 16:12:08 2009
From: barth at tac-financial.com (Sylvain Barthelemy)
Date: Thu, 22 Jan 2009 16:12:08 +0100
Subject: [R-SIG-Finance] how to study the lead and lag relation of two
	time	series?
In-Reply-To: <b1f16d9d0901211717ka4d890fgbec1424068aceaa0@mail.gmail.com>
References: <b1f16d9d0901211717ka4d890fgbec1424068aceaa0@mail.gmail.com>
Message-ID: <00dd01c97ca3$cc457fb0$64d07f10$@com>

Dear Michael,

David Ruelle wrote a very interesting paper on "Recurrence plots of
dynamical Systems" that you should read, and I remember of simples lead/lags
methods to detect random or determinist systems.

I think that you should take a look at this very interesting paper on
"Lead-lag cross-sectional structure and detection of
correlated-anticorrelated regime shifts": http://tinyurl.com/b6cw5m

Regards.

Sylvain

__________________________________________
Sylvain Barth?l?my
Research Director, TAC
Applied Economic & Financial Research
Tel: +33.(0).299.393.140 - Fax: +33.(0).299.393.189
E-mail: barth at tac-financial.com
www.tac-financial.com | www.sylbarth.com


-----Message d'origine-----
De?: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de Michael
Envoy??: jeudi 22 janvier 2009 02:18
??: r-help; r-sig-finance at stat.math.ethz.ch
Objet?: [R-SIG-Finance] how to study the lead and lag relation of two time
series?

Hi all,

Is there a way to study the lead and lag relation of two time series?

Let's say I have two time series, At and Bt. Is there a systematic way
of concluding whether it's A leading B or B leading A and by how much?

Thanks!

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG - http://www.avg.com 

21/01/2009
21:15


From enrique.bengoechea at credit-suisse.com  Fri Jan 23 11:57:07 2009
From: enrique.bengoechea at credit-suisse.com (=?iso-8859-1?Q?Bengoechea_Bartolom=E9_Enrique_=28SIES_73=29?=)
Date: Fri, 23 Jan 2009 11:57:07 +0100
Subject: [R-SIG-Finance] Bug in PerformanceAnalytics 0.9.7.1
Message-ID: <19811401A1D8174CB3EAD7F6072E9B5001E40A51@chsa1025.share.beluni.net>

Hi,
 
I've hit a bug in PerformanceAnalytics 0.9.7.1: looks like the "sortDrawdowns()" function (usually invoked from "table.Drawdowns()") assumes that there's more than one drawdown, which is not necesarily the case. 
 
Code to reproduce the bug:

> library(PerformanceAnalytics) 
> x <- zoo(rep(-1, 5), seq(Sys.Date()-5, Sys.Date(), by="days"))
> table.Drawdowns(x)
Error in if (runs$return[j] > runs$return[j + 1]) { : 
  missing value where TRUE/FALSE needed


Proposed solution: check the number of drawdowns in parameter 'runs' of function "sortDrawdowns()" before looping. Just add the following line at the beginning of 'sortDrawdowns':

> if (length(runs$return) < 2) return(runs)


Tested on the following environment:

> sessionInfo()
R version 2.8.1 (2008-12-22) 
i386-pc-mingw32 

locale:
LC_COLLATE=Spanish_Spain.1252;LC_CTYPE=Spanish_Spain.1252;LC_MONETARY=Spanish_Spain.1252;LC_NUMERIC=C;LC_TIME=Spanish_Spain.1252

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base     

other attached packages:
[1] PerformanceAnalytics_0.9.7.1 zoo_1.5-4                    rcom_2.0-4                   rscproxy_1.0-12             

loaded via a namespace (and not attached):
[1] grid_2.8.1      lattice_0.17-17 tools_2.8.1 


Best,

Enrique


From s4008654 at student.uq.edu.au  Sat Jan 24 07:46:44 2009
From: s4008654 at student.uq.edu.au (Paul Tacon)
Date: Sat, 24 Jan 2009 16:46:44 +1000
Subject: [R-SIG-Finance] ARMA-GARCH (Fixed Parameters)
Message-ID: <000001c97def$869f4390$93ddcab0$@uq.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090124/c05db234/attachment.pl>

From g1enn.roberts at btinternet.com  Sun Jan 25 02:23:54 2009
From: g1enn.roberts at btinternet.com (glenn)
Date: Sun, 25 Jan 2009 01:23:54 +0000
Subject: [R-SIG-Finance] Rquantlib discount curve
Message-ID: <C5A16FAA.F88%g1enn.roberts@btinternet.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090125/99e23b45/attachment.pl>

From edd at debian.org  Sun Jan 25 03:04:44 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 24 Jan 2009 20:04:44 -0600
Subject: [R-SIG-Finance] Rquantlib discount curve
In-Reply-To: <C5A16FAA.F88%g1enn.roberts@btinternet.com>
References: <C5A16FAA.F88%g1enn.roberts@btinternet.com>
Message-ID: <18811.51388.599230.334343@ron.nulle.part>


On 25 January 2009 at 01:23, glenn wrote:
| Been looking into the Rquantlib package and have a newbie question sorry.
| 
| Is it possible to get some flexibility over the inputs in the tsQuotes
| argument. The example gives;
| 
| tsQuotes <- list(d1w  =0.0382,
|                  d1m  =0.0372,
|                  fut1=96.2875,
|                  fut2=96.7875,
|                  fut3=96.9875,
|                  fut4=96.6875,
|                  fut5=96.4875,
|                  fut6=96.3875,
|                  fut7=96.2875,
|                  fut8=96.0875,
|                  s3y  =0.0398,
|                  s5y  =0.0443,
|                  s10y =0.05165,
|                  s15y =0.055175)
| 
| And ideally I would like to put more information in (or at least other) -
| s7y for instance. Tried looking round the files to see how the function is
| constructed with no luck.

Well, for QuantLib your best bet is probably the QuantLib documentation at
http://quantlib.org.   But that won't help you with R itself -- the object
tsQuotes is actually an R list, and it just enumerates it's component.  So
you could just add a line
    s7y = 0.04888,
if you wanted to add that before construction a curve.

Dirk

-- 
Three out of two people have difficulties with fractions.


From wssecn at uol.com.br  Mon Jan 26 03:25:19 2009
From: wssecn at uol.com.br (Washington Santos da Silva)
Date: Mon, 26 Jan 2009 00:25:19 -0200
Subject: [R-SIG-Finance] how to study the lead and lag relation of two
	time series?
In-Reply-To: <4977F842.8010405@gmail.com>
References: <b1f16d9d0901211717ka4d890fgbec1424068aceaa0@mail.gmail.com>
	<4977F842.8010405@gmail.com>
Message-ID: <497d1f0fe1e69_2325155555587eb41159@weasel3.tmail>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090126/5010fbaa/attachment.html>

From CVorlow at eurobank.gr  Mon Jan 26 12:19:15 2009
From: CVorlow at eurobank.gr (Vorlow Constantinos)
Date: Mon, 26 Jan 2009 13:19:15 +0200
Subject: [R-SIG-Finance] how to study the lead and lag relation of
 two	time series? (Washington Santos da Silva)
In-Reply-To: <mailman.3.1232967602.29463.r-sig-finance@stat.math.ethz.ch>
References: <mailman.3.1232967602.29463.r-sig-finance@stat.math.ethz.ch>
Message-ID: <BCEA70B53E1BE64290556580EA0708D68A2CBC@EH002EXC.eurobank.efg.gr>

 
You could use any packages that estimates VAR and Granger causality
tests (and derivatives)...

Try help.search("causal")...

However you need to clarify what you are up to... 

Read Granger's papers carefully. Causality and correlation do not
necessarily imply dependence...

Good luck,
 
Costas 
 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
r-sig-finance-request at stat.math.ethz.ch
Sent: Monday, January 26, 2009 1:00 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: R-SIG-Finance Digest, Vol 56, Issue 22

Send R-SIG-Finance mailing list submissions to
	r-sig-finance at stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
	https://stat.ethz.ch/mailman/listinfo/r-sig-finance
or, via email, send a message with subject or body 'help' to
	r-sig-finance-request at stat.math.ethz.ch

You can reach the person managing the list at
	r-sig-finance-owner at stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific than
"Re: Contents of R-SIG-Finance digest..."


Today's Topics:

   1. Re: how to study the lead and lag relation of two	time
      series? (Washington Santos da Silva)


----------------------------------------------------------------------

Message: 1
Date: Mon, 26 Jan 2009 00:25:19 -0200
From: Washington Santos da Silva <wssecn at uol.com.br>
Subject: Re: [R-SIG-Finance] how to study the lead and lag relation of
	two	time series?
To: r-sig-finance at stat.math.ethz.ch
Message-ID: <497d1f0fe1e69_2325155555587eb41159 at weasel3.tmail>
Content-Type: text/plain; charset="us-ascii"

An HTML attachment was scrubbed...
URL:
<https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090126/5010f
baa/attachment-0001.html>

------------------------------

_______________________________________________
R-SIG-Finance mailing list
R-SIG-Finance at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-sig-finance


End of R-SIG-Finance Digest, Vol 56, Issue 22
*********************************************

P Think before you print.

Disclaimer:
This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.


From kafka at centras.lt  Mon Jan 26 18:12:28 2009
From: kafka at centras.lt (kafkaz)
Date: Mon, 26 Jan 2009 09:12:28 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] xts feature
Message-ID: <21669362.post@talk.nabble.com>


Hi, 
I have xts object like this:
                      Open   High    Low  Close Volume    WAP hasGaps Count
2009-01-07 15:30:00 917.25 920.00 913.75 918.00 213846 917.00       0 27968
2009-01-07 16:00:00 918.25 919.75 909.50 912.00 324639 913.00       0 43038
2009-01-07 17:00:00 912.25 918.50 910.75 917.25 216257 914.75       0 28348
2009-01-07 18:00:00 917.50 917.50 910.50 911.00 151387 914.00       0 21369
2009-01-07 19:00:00 911.00 914.50 906.25 906.50 185240 910.50       0 23381
2009-01-07 20:00:00 906.50 907.75 902.25 903.00 231738 905.00       0 30068
2009-01-07 21:00:00 903.00 905.25 898.75 904.25 365659 902.00       0 42882
2009-01-07 22:00:00 904.00 905.50 903.00 905.25  79034 904.25       0  6445
2009-01-08 15:30:00 898.50 900.75 893.75 895.75 195798 897.50       0 24943
2009-01-08 16:00:00 895.50 903.75 893.50 902.00 314974 898.25       0 39624
2009-01-08 17:00:00 902.00 904.75 895.75 897.00 223484 900.00       0 31627
2009-01-08 18:00:00 897.00 902.25 896.50 898.75 122442 899.25       0 17533
2009-01-08 19:00:00 898.75 899.00 894.25 897.50 138558 896.50       0 19243
2009-01-08 20:00:00 897.50 905.50 896.50 904.00 217668 901.25       0 28658
2009-01-08 21:00:00 904.00 907.75 899.00 907.50 301768 903.50       0 36725
2009-01-08 22:00:00 907.50 907.50 905.50 907.00  52999 906.50       0  4100

Is it possible to find the average volume per hour? 
I want to see, what was the volume at 15:30, 16:00 and so on.
Should I write my own function? Can I find something useful?  

Thank you.

-- 
View this message in context: http://www.nabble.com/xts-feature-tp21669362p21669362.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Mon Jan 26 18:31:22 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 26 Jan 2009 11:31:22 -0600
Subject: [R-SIG-Finance] [R-sig-finance] xts feature
In-Reply-To: <21669362.post@talk.nabble.com>
References: <21669362.post@talk.nabble.com>
Message-ID: <e8e755250901260931w72f9df8cje9678b8b1a0a946e@mail.gmail.com>

Hi Kafkaz,

Try something like:

period.apply(Vo(x), endpoints(x, 'hours'), mean)

Useful tools from xts that are of interest:

?period.apply, ?endpoints, ?to.period

You can also use aggregate, which on xts objects will call the method
from the zoo package (automatically loaded).

HTH
Jeff

R/Finance 2009: Applied Finance with R
April 24, 25th Chicago, IL

http://www.quantmod.com/news/  http://www.RinFinance.com

-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From Murali.MENON at fortisinvestments.com  Mon Jan 26 19:15:09 2009
From: Murali.MENON at fortisinvestments.com (Murali.MENON at fortisinvestments.com)
Date: Mon, 26 Jan 2009 19:15:09 +0100
Subject: [R-SIG-Finance] [R-sig-finance] xts feature
In-Reply-To: <e8e755250901260931w72f9df8cje9678b8b1a0a946e@mail.gmail.com>
References: <21669362.post@talk.nabble.com> 
	<e8e755250901260931w72f9df8cje9678b8b1a0a946e@mail.gmail.com>
Message-ID: <CCB03D8892BBE94DA2EFFC20303A997A02783935@BE-S0340-V22.adroot.local>

 
Hi Jeff,

With that same dataset of Kafkaz's, if we wanted to compute the mean of
all the 15:30:00 Volume datapoints (there are two here), how would we do
it? The code you give below will perform the 'mean' operation on all
data points falling within each hour, but separate them day-by-day, if
that makes sense.

Would I have to do something like 

mean(x[grep("15:30:00", index(x)), "Volume"])

This works with a POSIXct index, but seems to assume that there is a
character representation for the time index?

Cheers,
Murali


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Jeff Ryan
Sent: 26 January 2009 17:31
To: kafkaz
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] [R-sig-finance] xts feature

Hi Kafkaz,

Try something like:

period.apply(Vo(x), endpoints(x, 'hours'), mean)

Useful tools from xts that are of interest:

?period.apply, ?endpoints, ?to.period

You can also use aggregate, which on xts objects will call the method
from the zoo package (automatically loaded).

HTH
Jeff

R/Finance 2009: Applied Finance with R
April 24, 25th Chicago, IL

http://www.quantmod.com/news/  http://www.RinFinance.com

--
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From jeff.a.ryan at gmail.com  Mon Jan 26 19:37:54 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 26 Jan 2009 12:37:54 -0600
Subject: [R-SIG-Finance] [R-sig-finance] xts feature
In-Reply-To: <CCB03D8892BBE94DA2EFFC20303A997A02783935@BE-S0340-V22.adroot.local>
References: <21669362.post@talk.nabble.com>
	<e8e755250901260931w72f9df8cje9678b8b1a0a946e@mail.gmail.com>
	<CCB03D8892BBE94DA2EFFC20303A997A02783935@BE-S0340-V22.adroot.local>
Message-ID: <e8e755250901261037r4b433ad8u9d1e31ade93769d1@mail.gmail.com>

Hi Murali,

Yes grep will coerce the index with as.character.  That method should
always work with xts though... and off the top of my head I can't
think of a (more) efficient way to accomplish this.

And you are correct, the period.apply would calculate the in-period
statistic (mean in this case) for each period, NOT the mean for all
observation with that time-stamp.


Jeff

R/Finance 2009: Applied Finance with R
April 24, 25th Chicago, IL

http://www.quantmod.com/news/  http://www.RinFinance.com

On Mon, Jan 26, 2009 at 12:15 PM,  <Murali.MENON at fortisinvestments.com> wrote:
>
> Hi Jeff,
>
> With that same dataset of Kafkaz's, if we wanted to compute the mean of
> all the 15:30:00 Volume datapoints (there are two here), how would we do
> it? The code you give below will perform the 'mean' operation on all
> data points falling within each hour, but separate them day-by-day, if
> that makes sense.
>
> Would I have to do something like
>
> mean(x[grep("15:30:00", index(x)), "Volume"])
>
> This works with a POSIXct index, but seems to assume that there is a
> character representation for the time index?
>
> Cheers,
> Murali
>
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Jeff Ryan
> Sent: 26 January 2009 17:31
> To: kafkaz
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] [R-sig-finance] xts feature
>
> Hi Kafkaz,
>
> Try something like:
>
> period.apply(Vo(x), endpoints(x, 'hours'), mean)
>
> Useful tools from xts that are of interest:
>
> ?period.apply, ?endpoints, ?to.period
>
> You can also use aggregate, which on xts objects will call the method
> from the zoo package (automatically loaded).
>
> HTH
> Jeff
>
> R/Finance 2009: Applied Finance with R
> April 24, 25th Chicago, IL
>
> http://www.quantmod.com/news/  http://www.RinFinance.com
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From kafka at centras.lt  Mon Jan 26 20:50:09 2009
From: kafka at centras.lt (kafkaz)
Date: Mon, 26 Jan 2009 11:50:09 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] xts feature
In-Reply-To: <CCB03D8892BBE94DA2EFFC20303A997A02783935@BE-S0340-V22.adroot.local>
References: <21669362.post@talk.nabble.com>
	<e8e755250901260931w72f9df8cje9678b8b1a0a946e@mail.gmail.com>
	<CCB03D8892BBE94DA2EFFC20303A997A02783935@BE-S0340-V22.adroot.local>
Message-ID: <21672357.post@talk.nabble.com>


Murali,
that is exactly what I was looking for.

Thank you.



murali.menon-2 wrote:
> 
>  
> Would I have to do something like 
> 
> mean(x[grep("15:30:00", index(x)), "Volume"])
> 
> This works with a POSIXct index, but seems to assume that there is a
> character representation for the time index?
> 
> Cheers,
> 

-- 
View this message in context: http://www.nabble.com/xts-feature-tp21669362p21672357.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ron_michael70 at yahoo.com  Tue Jan 27 03:40:51 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Mon, 26 Jan 2009 18:40:51 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Confusing result with AIC and
	ACF
In-Reply-To: <21563337.post@talk.nabble.com>
References: <21563337.post@talk.nabble.com>
Message-ID: <21678634.post@talk.nabble.com>


One week, still no suggestion. Is my question not understandable or
answerable?
Regards,


RON70 wrote:
> 
> Hi, I have a strange problem with following dataset :
> 
> -0.075851693
> -0.046125504
> -0.009117161
> 0.025569817
> 0.034882743
> 0.073671497
> 0.063805297
> 0.062306796
> 0.072343820
> 0.058354121
> -0.007635359
> 0.086790779
> 0.085487789
> 0.113577103
> 0.021293381
> 0.089423068
> 0.090485998
> 0.128847827
> 0.011859335
> 0.058794744
> 0.065909368
> 0.020887431
> 0.085387467
> 0.097375525
> 0.108981417
> 0.044289044
> 0.071428571
> 0.052430556
> 0.056307049
> 0.041957314
> 
> If I see the ACF then, it seems that this values are random. However next
> I fit two regressions and get the AIC values :
> 
> 1. regression of y (above series) on : y[-1], y[-2], y[-3], y[-4] and
> x[-1], x[-2], x[-3], x[-4]
> 
> 2. regression of y on y[-1], y[-2], y[-3] and x[-1], x[-2], x[-3], x[-4]
> 
> Here y[-p] is the p-th lagged value. and x is some other variable.
> 
> AIC for those regressions are : -6.673 and -6.636. This means 4-th lag of
> y has some explanatory power on y. Therefore this result is coming
> confusing when I compare with ACF figures [which tells y is random]
> 
> Can ppl here suggest me what inference I should make on y?
> 
> Regards,
> 
> 

-- 
View this message in context: http://www.nabble.com/Confusing-result-with-AIC-and-ACF-tp21563337p21678634.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ron_michael70 at yahoo.com  Tue Jan 27 03:42:10 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Mon, 26 Jan 2009 18:42:10 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] VAR process
In-Reply-To: <21530701.post@talk.nabble.com>
References: <21530701.post@talk.nabble.com>
Message-ID: <21678648.post@talk.nabble.com>


Hi,

More than one week, still no suggestion. Is my question not understandable
or answerable?
Regards,


RON70 wrote:
> 
> Hi,
> 
> In every book on VAR [Vector auto regression] I see that, any VAR [p]
> process can be expressed as a VAR [1] process. Here my question is how it
> can be possible? When you change it to a VAR [1] process, the VCV matrix
> of Innovations contains zero and hence it is not of full rank. Therefore
> it is not a PD matrix, you cannot decompose that according cholesky
> decomposition and lot more things can not be done with it because VCV
> matrix is singular. Then how can that process be a VAR process?
> 

-- 
View this message in context: http://www.nabble.com/VAR-process-tp21530701p21678648.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From markleeds at verizon.net  Tue Jan 27 06:38:06 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 26 Jan 2009 23:38:06 -0600 (CST)
Subject: [R-SIG-Finance] [R-sig-finance] VAR process
Message-ID: <1566865055.63839611233034686344.JavaMail.javamailuser@localhost>

I didn't respond earlier because I'm not clear on what the problem is 
with rewriting it as VAR(1) ? Lutkepohl text shows how this is done on 
pages 15 an 16 of his text. Except for the first row, the rest of the A 
matrix is composed of identity matrices. They y_t* below the first 
element play no role essentially because they are already known because 
they are in {t-1,t-2,t-3.... }. The only noise
term is the first element, u_t associated with the first element y_t.

I agree that ithe Cov is not of full rank when you write it that way but 
I don't know of any negative repurcussions of that. I think it's more of 
a tool that he uses  to show what the stability condition reduces to for 
a VAR(p) and nothing more than that. This same type of technique is used 
when
writing AR models in state space form.

Hopefully Eric or Bernhard or someone else can say more but I think it's 
just used for
deriving the stability condition in a easier way.




On Mon, Jan 26, 2009 at  9:42 PM, RON70 wrote:

> Hi,
>
> More than one week, still no suggestion. Is my question not 
> understandable
> or answerable?
> Regards,
>
>
> RON70 wrote:
>>
>> Hi,
>>
>> In every book on VAR [Vector auto regression] I see that, any VAR [p]
>> process can be expressed as a VAR [1] process. Here my question is 
>> how it
>> can be possible? When you change it to a VAR [1] process, the VCV 
>> matrix
>> of Innovations contains zero and hence it is not of full rank. 
>> Therefore
>> it is not a PD matrix, you cannot decompose that according cholesky
>> decomposition and lot more things can not be done with it because VCV
>> matrix is singular. Then how can that process be a VAR process?
>>
>
> -- 
> View this message in context: 
> http://www.nabble.com/VAR-process-tp21530701p21678648.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From frainj at tcd.ie  Tue Jan 27 17:27:04 2009
From: frainj at tcd.ie (John Frain)
Date: Tue, 27 Jan 2009 16:27:04 +0000
Subject: [R-SIG-Finance] [R-sig-finance] VAR process
In-Reply-To: <1566865055.63839611233034686344.JavaMail.javamailuser@localhost>
References: <1566865055.63839611233034686344.JavaMail.javamailuser@localhost>
Message-ID: <cfdde1650901270827x4cfd2e8dhc39bf5939443da23@mail.gmail.com>

I agree.  Reducing a VAR(p) to a VAR(1) in this way is simply a device
to generalise certain properties of a VAR(1) to a VAR(p) or possibly
to complete certain computations.  In a VAR(p) the covariance matrix
or the contemporaneous errors is in general non-diagonal.  The
Choleski decomposition was the original way of transforming the
contemporaneous variables so that the covariance of the disturbances
is diagonal and has nothing to do with the VAR(1) representation.   It
would be possible to work in terms of the VAR(1) representation but
this would be an unnecessary complication.  There are of course
various problems with this kind of analysis (e.g. uniqueness) and
structural VARs, relying on restrictions from economic theory are more
used today.

Best Regards

John

2009/1/27  <markleeds at verizon.net>:
> I didn't respond earlier because I'm not clear on what the problem is with
> rewriting it as VAR(1) ? Lutkepohl text shows how this is done on pages 15
> an 16 of his text. Except for the first row, the rest of the A matrix is
> composed of identity matrices. They y_t* below the first element play no
> role essentially because they are already known because they are in
> {t-1,t-2,t-3.... }. The only noise
> term is the first element, u_t associated with the first element y_t.
>
> I agree that ithe Cov is not of full rank when you write it that way but I
> don't know of any negative repurcussions of that. I think it's more of a
> tool that he uses  to show what the stability condition reduces to for a
> VAR(p) and nothing more than that. This same type of technique is used when
> writing AR models in state space form.
>
> Hopefully Eric or Bernhard or someone else can say more but I think it's
> just used for
> deriving the stability condition in a easier way.
>
>
>
>
> On Mon, Jan 26, 2009 at  9:42 PM, RON70 wrote:
>
>> Hi,
>>
>> More than one week, still no suggestion. Is my question not understandable
>> or answerable?
>> Regards,
>>
>>
>> RON70 wrote:
>>>
>>> Hi,
>>>
>>> In every book on VAR [Vector auto regression] I see that, any VAR [p]
>>> process can be expressed as a VAR [1] process. Here my question is how it
>>> can be possible? When you change it to a VAR [1] process, the VCV matrix
>>> of Innovations contains zero and hence it is not of full rank. Therefore
>>> it is not a PD matrix, you cannot decompose that according cholesky
>>> decomposition and lot more things can not be done with it because VCV
>>> matrix is singular. Then how can that process be a VAR process?
>>>
>>
>> --
>> View this message in context:
>> http://www.nabble.com/VAR-process-tp21530701p21678648.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>



-- 
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.htm
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


From bogaso.christofer at gmail.com  Tue Jan 27 18:25:39 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Tue, 27 Jan 2009 09:25:39 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Plese help me to understand this
Message-ID: <21690051.post@talk.nabble.com>


Hi all,

Can anyone please help me to understand, whether there is any difference
between "Zero rate" and "Zero Yield". I thought they are synonymous and both
are used in Zero-coupon bond context. However I got different results in my
textbook while those two are used in different places. Here is the problem :

Case 1. Suppose 5-year spot yield is 7.6 p.a.. Now assume there is a
receivable after 5 year worth $100. Then current value of this receivable is
$100*(1+7.6*5/100)^-1. Am I correct? However in my textbook, it is reported
as $100*(1+7.6/100)^-5.

Case 2. Suppose 6 month spot rate 6.39 p.a.. Now assume there is a
receivable after 6 months worth $100. Then current value of this receivable
is $100*(1+6.39/100/2)^-1. Here my understanding is matching with my
textbook.

Please forgive me as I understand, this question is too fundamental. But
still I think I am missing something important. Can anyone please help me?

Regards,
-- 
View this message in context: http://www.nabble.com/Plese-help-me-to-understand-this-tp21690051p21690051.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From bogaso.christofer at gmail.com  Tue Jan 27 18:26:49 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Tue, 27 Jan 2009 09:26:49 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Plese help me to understand this
Message-ID: <21690077.post@talk.nabble.com>


Hi all,

Can anyone please help me to understand, whether there is any difference
between "Zero rate" and "Zero Yield". I thought they are synonymous and both
are used in Zero-coupon bond context. However I got different results in my
textbook while those two are used in different places. Here is the problem :

Case 1. Suppose 5-year spot yield is 7.6 p.a.. Now assume there is a
receivable after 5 year worth $100. Then current value of this receivable is
$100*(1+7.6*5/100)^-1. Am I correct? However in my textbook, it is reported
as $100*(1+7.6/100)^-5.

Case 2. Suppose 6 month spot rate 6.39 p.a.. Now assume there is a
receivable after 6 months worth $100. Then current value of this receivable
is $100*(1+6.39/100/2)^-1. Here my understanding is matching with my
textbook.

Please forgive me as I understand, this question is too fundamental. But
still I think I am missing something important. Can anyone please help me?

Regards,
-- 
View this message in context: http://www.nabble.com/Plese-help-me-to-understand-this-tp21690077p21690077.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ezivot at u.washington.edu  Tue Jan 27 18:29:32 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Tue, 27 Jan 2009 09:29:32 -0800
Subject: [R-SIG-Finance] [R-sig-finance] VAR process
In-Reply-To: <1566865055.63839611233034686344.JavaMail.javamailuser@localhost>
Message-ID: <200901271729.n0RHTWB0002774@smtp.washington.edu>

The transformation of the VAR(p) to a VAR(1) is a mechanical device used to
help understand the dynamics of the model. The VAR(1) form is called the
companion form of the VAR(p) and is an equivalent representation. In the
VAR(1) form the model is first order markov and the eigenvalues of the
transition matrix determines the stability properties of the system. This is
a common trick used for systems of stochastic differential equations.
Lutkepohl's book on multivariate time series gives a very clear explanation
of this.


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
markleeds at verizon.net
Sent: Monday, January 26, 2009 9:38 PM
To: RON70
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] [R-sig-finance] VAR process

I didn't respond earlier because I'm not clear on what the problem is with
rewriting it as VAR(1) ? Lutkepohl text shows how this is done on pages 15
an 16 of his text. Except for the first row, the rest of the A matrix is
composed of identity matrices. They y_t* below the first element play no
role essentially because they are already known because they are in
{t-1,t-2,t-3.... }. The only noise term is the first element, u_t associated
with the first element y_t.

I agree that ithe Cov is not of full rank when you write it that way but I
don't know of any negative repurcussions of that. I think it's more of a
tool that he uses  to show what the stability condition reduces to for a
VAR(p) and nothing more than that. This same type of technique is used when
writing AR models in state space form.

Hopefully Eric or Bernhard or someone else can say more but I think it's
just used for deriving the stability condition in a easier way.




On Mon, Jan 26, 2009 at  9:42 PM, RON70 wrote:

> Hi,
>
> More than one week, still no suggestion. Is my question not 
> understandable
> or answerable?
> Regards,
>
>
> RON70 wrote:
>>
>> Hi,
>>
>> In every book on VAR [Vector auto regression] I see that, any VAR [p]
>> process can be expressed as a VAR [1] process. Here my question is 
>> how it
>> can be possible? When you change it to a VAR [1] process, the VCV 
>> matrix
>> of Innovations contains zero and hence it is not of full rank. 
>> Therefore
>> it is not a PD matrix, you cannot decompose that according cholesky
>> decomposition and lot more things can not be done with it because VCV
>> matrix is singular. Then how can that process be a VAR process?
>>
>
> -- 
> View this message in context: 
> http://www.nabble.com/VAR-process-tp21530701p21678648.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From davidr at rhotrading.com  Tue Jan 27 18:34:39 2009
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 27 Jan 2009 11:34:39 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Plese help me to understand this
In-Reply-To: <21690077.post@talk.nabble.com>
References: <21690077.post@talk.nabble.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE77022A5E28@rhopost.rhotrading.com>

The text is compounding interest but you are not.
HTH,
-- David


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Bogaso
Sent: Tuesday, January 27, 2009 11:27 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] [R-sig-finance] Plese help me to understand
this


Hi all,

Can anyone please help me to understand, whether there is any difference
between "Zero rate" and "Zero Yield". I thought they are synonymous and
both
are used in Zero-coupon bond context. However I got different results in
my
textbook while those two are used in different places. Here is the
problem :

Case 1. Suppose 5-year spot yield is 7.6 p.a.. Now assume there is a
receivable after 5 year worth $100. Then current value of this
receivable is
$100*(1+7.6*5/100)^-1. Am I correct? However in my textbook, it is
reported
as $100*(1+7.6/100)^-5.

Case 2. Suppose 6 month spot rate 6.39 p.a.. Now assume there is a
receivable after 6 months worth $100. Then current value of this
receivable
is $100*(1+6.39/100/2)^-1. Here my understanding is matching with my
textbook.

Please forgive me as I understand, this question is too fundamental. But
still I think I am missing something important. Can anyone please help
me?

Regards,
-- 
View this message in context:
http://www.nabble.com/Plese-help-me-to-understand-this-tp21690077p216900
77.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From bogaso.christofer at gmail.com  Tue Jan 27 18:56:54 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Tue, 27 Jan 2009 09:56:54 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Plese help me to understand this
In-Reply-To: <F9F2A641C593D7408925574C05A7BE77022A5E28@rhopost.rhotrading.com>
References: <21690077.post@talk.nabble.com>
	<F9F2A641C593D7408925574C05A7BE77022A5E28@rhopost.rhotrading.com>
Message-ID: <21690727.post@talk.nabble.com>


But Zero rate semblances with Zero coupon bond, where all Interests will be
paid at maturity. Then why it is compounded annually?


davidr-2 wrote:
> 
> The text is compounding interest but you are not.
> HTH,
> -- David
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Bogaso
> Sent: Tuesday, January 27, 2009 11:27 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] [R-sig-finance] Plese help me to understand
> this
> 
> 
> Hi all,
> 
> Can anyone please help me to understand, whether there is any difference
> between "Zero rate" and "Zero Yield". I thought they are synonymous and
> both
> are used in Zero-coupon bond context. However I got different results in
> my
> textbook while those two are used in different places. Here is the
> problem :
> 
> Case 1. Suppose 5-year spot yield is 7.6 p.a.. Now assume there is a
> receivable after 5 year worth $100. Then current value of this
> receivable is
> $100*(1+7.6*5/100)^-1. Am I correct? However in my textbook, it is
> reported
> as $100*(1+7.6/100)^-5.
> 
> Case 2. Suppose 6 month spot rate 6.39 p.a.. Now assume there is a
> receivable after 6 months worth $100. Then current value of this
> receivable
> is $100*(1+6.39/100/2)^-1. Here my understanding is matching with my
> textbook.
> 
> Please forgive me as I understand, this question is too fundamental. But
> still I think I am missing something important. Can anyone please help
> me?
> 
> Regards,
> -- 
> View this message in context:
> http://www.nabble.com/Plese-help-me-to-understand-this-tp21690077p216900
> 77.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Plese-help-me-to-understand-this-tp21690077p21690727.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From j_cuisinier at hotmail.com  Tue Jan 27 19:09:34 2009
From: j_cuisinier at hotmail.com (julien cuisinier)
Date: Tue, 27 Jan 2009 19:09:34 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Plese help me to understand this
In-Reply-To: <21690727.post@talk.nabble.com>
References: <21690077.post@talk.nabble.com>
	<F9F2A641C593D7408925574C05A7BE77022A5E28@rhopost.rhotrading.com> 
	<21690727.post@talk.nabble.com>
Message-ID: <COL102-W17B48DA344E86A983E85A8FCB0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090127/b541f0e1/attachment.pl>

From munkey906 at gmail.com  Wed Jan 28 03:41:56 2009
From: munkey906 at gmail.com (Theodore Van Rooy)
Date: Tue, 27 Jan 2009 19:41:56 -0700
Subject: [R-SIG-Finance] How do I load Rmetrics indicator functions now
Message-ID: <3c8226f70901271841k236caf2lf5521e685eea6d14@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090127/234f035d/attachment.pl>

From jeff.a.ryan at gmail.com  Wed Jan 28 04:01:10 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 27 Jan 2009 21:01:10 -0600
Subject: [R-SIG-Finance] How do I load Rmetrics indicator functions now
In-Reply-To: <3c8226f70901271841k236caf2lf5521e685eea6d14@mail.gmail.com>
References: <3c8226f70901271841k236caf2lf5521e685eea6d14@mail.gmail.com>
Message-ID: <e8e755250901271901q1b09488bi2b527922401252ab@mail.gmail.com>

Most of the Rmetrics ones are in fTrading if I recall correctly.

A different set (and more complete) is now available in the TTR package on CRAN.

http://cran.r-project.org/web/packages/TTR/index.html

The indicators are directly usable on charts from the quantmod
package. Additionally the backend is implemented in fortran, so they
are very fast.

Some examples are at:

http://www.quantmod.com/examples/charting

HTH
Jeff

On Tue, Jan 27, 2009 at 8:41 PM, Theodore Van Rooy <munkey906 at gmail.com> wrote:
> Recently (Remetrics 2.7, 2.8) functions like emaTA() , and any other
> function in Rmetrics was loaded with the library(Rmetrics) package.
>
> We just updated our server and got the new Rmetrics package... now that no
> longer works...
>
> In which packages are all the standard indicator functions located?...
> library(fMultivar) was my first choice... but it didn't seem to have them.
>
>
> Theo
>
>
> --
> Theodore Van Rooy
> http://greentheo.scroggles.com
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From chalabi at phys.ethz.ch  Wed Jan 28 10:59:54 2009
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Wed, 28 Jan 2009 10:59:54 +0100
Subject: [R-SIG-Finance] How do I load Rmetrics indicator functions now
In-Reply-To: <3c8226f70901271841k236caf2lf5521e685eea6d14@mail.gmail.com>
References: <3c8226f70901271841k236caf2lf5521e685eea6d14@mail.gmail.com>
Message-ID: <20090128105954.4c985517@mimi>

>>>> "TVR" == Theodore Van Rooy <munkey906 at gmail.com>
>>>> on Tue, 27 Jan 2009 19:41:56 -0700

   TVR> Recently (Remetrics 2.7, 2.8) functions like emaTA() , and
   TVR> any other
   TVR> function in Rmetrics was loaded with the library(Rmetrics)
   TVR> package.
   TVR>
   TVR> We just updated our server and got the new Rmetrics
   TVR> package... now that no
   TVR> longer works...
   TVR>
   TVR> In which packages are all the standard indicator functions
   TVR> located?...
   TVR> library(fMultivar) was my first choice... but it didn't seem
   TVR> to have them.
   TVR>
   TVR>
   TVR> Theo


The meta-package 'Rmetrics' was removed few months ago for technical
reasons. You need to load the packages individually.

emaTA() is in fTrading...

HTH
Yohan

-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch


From megh700004 at yahoo.com  Thu Jan 29 06:20:41 2009
From: megh700004 at yahoo.com (megh)
Date: Wed, 28 Jan 2009 21:20:41 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for some option
	portfolio
Message-ID: <21704929.post@talk.nabble.com>


Can people here please guide me how to calculate Componrnt VaR (sensitivity)
of an option position, for a portfolio which consists of number of stocks
and option contracts (put ot call or both). Any document, research paper
over net is highly appreciated.

Thanks
-- 
View this message in context: http://www.nabble.com/Conponent-VaR-for-some-option-portfolio-tp21704929p21704929.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From guy.yollin at rotellacapital.com  Thu Jan 29 17:55:43 2009
From: guy.yollin at rotellacapital.com (Guy Yollin)
Date: Thu, 29 Jan 2009 10:55:43 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for some
	option	portfolio
In-Reply-To: <21704929.post@talk.nabble.com>
References: <21704929.post@talk.nabble.com>
Message-ID: <E4259A82356E7F46B4F911FB27B0D72516E85E6A3B@AUSP01VMBX02.collaborationhost.net>

I've used this paper as a guide to implementing cvar decomposition in R/S-PLUS:

Yamai, Yasuhiro & Yoshiba, Toshinao, 2002.
"Comparative Analyses of Expected Shortfall and Value-at-Risk: Their Estimation Error, Decomposition, and Optimization,"
Monetary and Economic Studies, Institute for Monetary and Economic Studies, Bank of Japan, vol. 20(1), pages 87-121, January.

It's available here:

http://www.imes.boj.or.jp/english/publication/mes/2002/me20-1-4.pdf

Best,

-- Guy


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of megh
Sent: Wednesday, January 28, 2009 9:21 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for some option portfolio


Can people here please guide me how to calculate Componrnt VaR (sensitivity)
of an option position, for a portfolio which consists of number of stocks
and option contracts (put ot call or both). Any document, research paper
over net is highly appreciated.

Thanks
--
View this message in context: http://www.nabble.com/Conponent-VaR-for-some-option-portfolio-tp21704929p21704929.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From davidr at rhotrading.com  Thu Jan 29 18:45:49 2009
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Thu, 29 Jan 2009 11:45:49 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for
	someoption	portfolio
In-Reply-To: <E4259A82356E7F46B4F911FB27B0D72516E85E6A3B@AUSP01VMBX02.collaborationhost.net>
References: <21704929.post@talk.nabble.com>
	<E4259A82356E7F46B4F911FB27B0D72516E85E6A3B@AUSP01VMBX02.collaborationhost.net>
Message-ID: <F9F2A641C593D7408925574C05A7BE77022A6233@rhopost.rhotrading.com>

There's also
http://www.fea.com/resources/pdf/a_endsearchvar.pdf
Ending the Search for Component VaR
by
Mark B. Garman
Financial Engineering Associates, Inc.

Remember that for options, the normality assumption is wrong
and you have to use skew and kurtosis or simulations.

-- David


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Guy Yollin
Sent: Thursday, January 29, 2009 10:56 AM
To: megh; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] [R-sig-finance] Conponent VaR for
someoption portfolio

I've used this paper as a guide to implementing cvar decomposition in
R/S-PLUS:

Yamai, Yasuhiro & Yoshiba, Toshinao, 2002.
"Comparative Analyses of Expected Shortfall and Value-at-Risk: Their
Estimation Error, Decomposition, and Optimization,"
Monetary and Economic Studies, Institute for Monetary and Economic
Studies, Bank of Japan, vol. 20(1), pages 87-121, January.

It's available here:

http://www.imes.boj.or.jp/english/publication/mes/2002/me20-1-4.pdf

Best,

-- Guy


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of megh
Sent: Wednesday, January 28, 2009 9:21 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for some option
portfolio


Can people here please guide me how to calculate Componrnt VaR
(sensitivity)
of an option position, for a portfolio which consists of number of
stocks
and option contracts (put ot call or both). Any document, research paper
over net is highly appreciated.

Thanks
--
View this message in context:
http://www.nabble.com/Conponent-VaR-for-some-option-portfolio-tp21704929
p21704929.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From megh700004 at yahoo.com  Fri Jan 30 14:06:57 2009
From: megh700004 at yahoo.com (megh)
Date: Fri, 30 Jan 2009 05:06:57 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for some
	option	portfolio
In-Reply-To: <E4259A82356E7F46B4F911FB27B0D72516E85E6A3B@AUSP01VMBX02.collaborationhost.net>
References: <21704929.post@talk.nabble.com>
	<E4259A82356E7F46B4F911FB27B0D72516E85E6A3B@AUSP01VMBX02.collaborationhost.net>
Message-ID: <21746856.post@talk.nabble.com>


Hi  Guy , thanks so much. This document is really useful. Although I think
that will serve most of my purposes, still I found that they adopted
Simulation approach to calculate all those analytics. My question is that,
is it possible to have those under Parametric approach?

Thanks and regards,


Guy Yollin-2 wrote:
> 
> I've used this paper as a guide to implementing cvar decomposition in
> R/S-PLUS:
> 
> Yamai, Yasuhiro & Yoshiba, Toshinao, 2002.
> "Comparative Analyses of Expected Shortfall and Value-at-Risk: Their
> Estimation Error, Decomposition, and Optimization,"
> Monetary and Economic Studies, Institute for Monetary and Economic
> Studies, Bank of Japan, vol. 20(1), pages 87-121, January.
> 
> It's available here:
> 
> http://www.imes.boj.or.jp/english/publication/mes/2002/me20-1-4.pdf
> 
> Best,
> 
> -- Guy
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of megh
> Sent: Wednesday, January 28, 2009 9:21 PM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for some option
> portfolio
> 
> 
> Can people here please guide me how to calculate Componrnt VaR
> (sensitivity)
> of an option position, for a portfolio which consists of number of stocks
> and option contracts (put ot call or both). Any document, research paper
> over net is highly appreciated.
> 
> Thanks
> --
> View this message in context:
> http://www.nabble.com/Conponent-VaR-for-some-option-portfolio-tp21704929p21704929.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Conponent-VaR-for-some-option-portfolio-tp21704929p21746856.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From yana.roth at yahoo.com  Fri Jan 30 15:36:32 2009
From: yana.roth at yahoo.com (Yana Roth)
Date: Fri, 30 Jan 2009 06:36:32 -0800 (PST)
Subject: [R-SIG-Finance] Copula in R
Message-ID: <889260.76247.qm@web46206.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090130/e2941ae4/attachment.pl>

From Xiaochen.Sun at brunel.ac.uk  Fri Jan 30 15:55:56 2009
From: Xiaochen.Sun at brunel.ac.uk (Xiaochen Sun)
Date: Fri, 30 Jan 2009 14:55:56 -0000
Subject: [R-SIG-Finance] Copula in R
In-Reply-To: <889260.76247.qm@web46206.mail.sp1.yahoo.com>
Message-ID: <E386E504246A9249A9176B5BEEC13B6F018B5C4F@UXEXMBU116.academic.windsor>

Hi there,

Suppose you have produced 15 CDFs, which all lie between [0,1]. They are pseudo samples to be fitted in the copula model.

There are some packages in R allow you to estimate the parameters in copula function, for example, "copula", "QRMlib".....

Your marginals have been estimated by GPD model, which means you will adopt two-step IFM method for copula estimation.

Alternatively, you can use CML method, which use empirical distribution for the marginals. For this, the function "edf" can help you to produce all your return data into uniform variates.

Hope this helps.

Regards,
Michael



-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of Yana Roth
Sent: 2009??1??30?? 14:37
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Copula in R


Hello,
I try to reproduce copula fitting from Matlab by R. I constructed pieswise distribution: Generalised Pareto at the tails and empirical distribution estimated with Gaussian Kernel. Like this I obtain 15 CDF. However, I dont find my way to convert them to uniforms and fit copula.
?
If you could provide some help, I would be thanjful
?
Thank You
Yana


      
	[[alternative HTML version deleted]]


From alexios at 4dscape.com  Fri Jan 30 16:05:42 2009
From: alexios at 4dscape.com (alexios)
Date: Fri, 30 Jan 2009 15:05:42 +0000
Subject: [R-SIG-Finance] Copula in R
In-Reply-To: <889260.76247.qm@web46206.mail.sp1.yahoo.com>
References: <889260.76247.qm@web46206.mail.sp1.yahoo.com>
Message-ID: <49831746.3040606@4dscape.com>

The QRMlib function "fit.tcopula.rank" with method="kendall" will accept
 the uniform data from the cdf transformation.

It is more commonplace to first filter the data with a process like
garch, and then apply the fit to the standardized residuals.

There is also a package on r-forge for the semi-parametric distribution
with pareto tail which implements density, distribution, quantile and
sampling (http://r-forge.r-project.org/projects/spd/).

-Alexios

Yana Roth wrote:
> Hello,
> I try to reproduce copula fitting from Matlab by R. I constructed pieswise distribution: Generalised Pareto at the tails and empirical distribution estimated with Gaussian Kernel. Like this I obtain 15 CDF. However, I dont find my way to convert them to uniforms and fit copula.
>  
> If you could provide some help, I would be thanjful
>  
> Thank You
> Yana
> 
> 
>       
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From davidr at rhotrading.com  Fri Jan 30 16:37:02 2009
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Fri, 30 Jan 2009 09:37:02 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for
	someoption	portfolio
In-Reply-To: <21746856.post@talk.nabble.com>
References: <21704929.post@talk.nabble.com><E4259A82356E7F46B4F911FB27B0D72516E85E6A3B@AUSP01VMBX02.collaborationhost.net>
	<21746856.post@talk.nabble.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE77022A639C@rhopost.rhotrading.com>

Component VaR aside, Value-at-Risk is very difficult for options
and non-linear instruments in general. A number of years ago
RiskMetrics tried to use a delta-gamma approximation, but they
abandoned it soon since it really didn't work very well. You
may be able to find a paper or two on it somewhere under that name or
quadratic VaR, but many of them have been pulled.
(Check RiskMetrics.com and GloriaMundi.org.)
Finding the distribution of option values under an assumption of 
a distribution assumption for the underlying is hard. 
Taking volatility skew and such into account makes it harder.
It may be an interesting exercise to compute it, but using simulation
is going to be much easier and more useful in the end. Just make sure
you use a distribution for the underlying that has fat enough tails.

My 2 cents.
David L. Reiner, PhD
Head Quant
Rho Trading Securities, LLC


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of megh
Sent: Friday, January 30, 2009 7:07 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] [R-sig-finance] Conponent VaR for
someoption portfolio


Hi  Guy , thanks so much. This document is really useful. Although I
think
that will serve most of my purposes, still I found that they adopted
Simulation approach to calculate all those analytics. My question is
that,
is it possible to have those under Parametric approach?

Thanks and regards,


Guy Yollin-2 wrote:
> 
> I've used this paper as a guide to implementing cvar decomposition in
> R/S-PLUS:
> 
> Yamai, Yasuhiro & Yoshiba, Toshinao, 2002.
> "Comparative Analyses of Expected Shortfall and Value-at-Risk: Their
> Estimation Error, Decomposition, and Optimization,"
> Monetary and Economic Studies, Institute for Monetary and Economic
> Studies, Bank of Japan, vol. 20(1), pages 87-121, January.
> 
> It's available here:
> 
> http://www.imes.boj.or.jp/english/publication/mes/2002/me20-1-4.pdf
> 
> Best,
> 
> -- Guy
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of megh
> Sent: Wednesday, January 28, 2009 9:21 PM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for some option
> portfolio
> 
> 
> Can people here please guide me how to calculate Componrnt VaR
> (sensitivity)
> of an option position, for a portfolio which consists of number of
stocks
> and option contracts (put ot call or both). Any document, research
paper
> over net is highly appreciated.
> 
> Thanks
> --
> View this message in context:
>
http://www.nabble.com/Conponent-VaR-for-some-option-portfolio-tp21704929
p21704929.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context:
http://www.nabble.com/Conponent-VaR-for-some-option-portfolio-tp21704929
p21746856.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From alexios at 4dscape.com  Fri Jan 30 16:56:20 2009
From: alexios at 4dscape.com (alexios)
Date: Fri, 30 Jan 2009 15:56:20 +0000
Subject: [R-SIG-Finance] Copula in R
In-Reply-To: <684606.86535.qm@web46213.mail.sp1.yahoo.com>
References: <684606.86535.qm@web46213.mail.sp1.yahoo.com>
Message-ID: <49832324.8030706@4dscape.com>

No, first fit the standardized residuals with the distribution you want
(e.g. the spd), then apply the cdf transform on those residuals(i.e.
pspd(std.resids, fit) ) to get the pseudo-uniform
numbers, then collect into a matrix and fit using the copula.

Once you have fitted, use the random number generator of copula to get a
correlated sample which you then transform back by using the quantile
function (qspd), and then reintroduce them into the univariate garch
fit from stage 1 from which you can now simulate.

HTH.

Alexios

Yana Roth wrote:
> Yes, I filtered the residuals with GARCH. You mean, I should apply
> fit.copula.rank to standardised residuals directly?
> I thought, I should estimate CDFs before....
>  
> Thank you
> 
> --- On *Fri, 1/30/09, alexios /<alexios at 4dscape.com>/* wrote:
> 
>     From: alexios <alexios at 4dscape.com>
>     Subject: Re: [R-SIG-Finance] Copula in R
>     To: yana.roth at yahoo.com
>     Cc: r-sig-finance at stat.math.ethz.ch
>     Date: Friday, January 30, 2009, 3:05 PM
> 
>     The QRMlib function "fit.tcopula.rank" with method="kendall"
>     will accept
>      the uniform data from the cdf transformation.
> 
>     It is more commonplace to first filter the data with a process like
>     garch, and then apply the fit to the standardized residuals.
> 
>     There is also a package on r-forge for the semi-parametric distribution
>     with pareto tail which implements density, distribution, quantile and
>     sampling (http://r-forge.r-project.org/projects/spd/).
> 
>     -Alexios
> 
>     Yana Roth wrote:
>     > Hello,
>     > I try to reproduce copula fitting from Matlab by R. I constructed pieswise
>     distribution: Generalised Pareto at the tails and empirical distribution
>     estimated with Gaussian Kernel. Like this I obtain 15 CDF. However, I dont find
>     my way to convert them to uniforms and fit copula.
>     >  
>     > If you could provide some help, I would be thanjful
>     >  
>     > Thank You
>     > Yana
>     > 
>     > 
>     >       
>     > 	[[alternative HTML version deleted]]
>     > 
>     > 
>     > 
>     > ------------------------------------------------------------------------
>     > 
>     > _______________________________________________
>     > R-SIG-Finance at stat.math.ethz.ch mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>     > -- Subscriber-posting only.
>     > -- If you want to post, subscribe first.
> 
>


From dutt.debashis at gmail.com  Fri Jan 30 17:58:30 2009
From: dutt.debashis at gmail.com (Debashis Dutta)
Date: Fri, 30 Jan 2009 19:58:30 +0300
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for someoption
	portfolio
In-Reply-To: <F9F2A641C593D7408925574C05A7BE77022A639C@rhopost.rhotrading.com>
References: <21704929.post@talk.nabble.com>
	<E4259A82356E7F46B4F911FB27B0D72516E85E6A3B@AUSP01VMBX02.collaborationhost.net>
	<21746856.post@talk.nabble.com>
	<F9F2A641C593D7408925574C05A7BE77022A639C@rhopost.rhotrading.com>
Message-ID: <37673c2d0901300858h14e00282m3d6c2c07100c9241@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090130/4668e5bb/attachment.pl>

From kriskumar at earthlink.net  Sat Jan 31 00:21:59 2009
From: kriskumar at earthlink.net (kriskumar at earthlink.net)
Date: Fri, 30 Jan 2009 23:21:59 +0000
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR
	forsomeoption	portfolio
In-Reply-To: <F9F2A641C593D7408925574C05A7BE77022A6233@rhopost.rhotrading.com>
References: <21704929.post@talk.nabble.com><E4259A82356E7F46B4F911FB27B0D72516E85E6A3B@AUSP01VMBX02.collaborationhost.net><F9F2A641C593D7408925574C05A7BE77022A6233@rhopost.rhotrading.com>
Message-ID: <268200793-1233362509-cardhu_decombobulator_blackberry.rim.net-1923743662-@bxe242.bisx.prod.on.blackberry>

Typically VaR for option portfolios is computed by using a Taylor expansion to write the option value as the sum of  various greeks. In this approach one can then precompute the greeks at various spot, vol, rate and tenors.(Underlying factorsy) Once this is done  one could simulate the factors and use a table lookup to compute VaR. This is typically how it is done in large financial institutions.

One could use a simillar approach for CVaR perhaps.

Best
Krishna 


----
"When I get a little money, I buy books and if any 
      is left, I buy food and clothes."  -- Erasmus

-----Original Message-----
From: <davidr at rhotrading.com>

Date: Thu, 29 Jan 2009 11:45:49 
To: Guy Yollin<guy.yollin at rotellacapital.com>; megh<megh700004 at yahoo.com>; <r-sig-finance at stat.math.ethz.ch>
Subject: Re: [R-SIG-Finance] [R-sig-finance] Conponent VaR for
	someoption	portfolio


There's also
http://www.fea.com/resources/pdf/a_endsearchvar.pdf
Ending the Search for Component VaR
by
Mark B. Garman
Financial Engineering Associates, Inc.

Remember that for options, the normality assumption is wrong
and you have to use skew and kurtosis or simulations.

-- David


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Guy Yollin
Sent: Thursday, January 29, 2009 10:56 AM
To: megh; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] [R-sig-finance] Conponent VaR for
someoption portfolio

I've used this paper as a guide to implementing cvar decomposition in
R/S-PLUS:

Yamai, Yasuhiro & Yoshiba, Toshinao, 2002.
"Comparative Analyses of Expected Shortfall and Value-at-Risk: Their
Estimation Error, Decomposition, and Optimization,"
Monetary and Economic Studies, Institute for Monetary and Economic
Studies, Bank of Japan, vol. 20(1), pages 87-121, January.

It's available here:

http://www.imes.boj.or.jp/english/publication/mes/2002/me20-1-4.pdf

Best,

-- Guy


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of megh
Sent: Wednesday, January 28, 2009 9:21 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for some option
portfolio


Can people here please guide me how to calculate Componrnt VaR
(sensitivity)
of an option position, for a portfolio which consists of number of
stocks
and option contracts (put ot call or both). Any document, research paper
over net is highly appreciated.

Thanks
--
View this message in context:
http://www.nabble.com/Conponent-VaR-for-some-option-portfolio-tp21704929
p21704929.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

From markus.wrake at ivl.se  Sun Feb  1 00:46:07 2009
From: markus.wrake at ivl.se (=?UTF-8?Q?Markus=20Wr=C3=A5ke?=)
Date: Sun, 01 Feb 2009 00:46:07 +0100
Subject: [R-SIG-Finance] Function finding optimal lag length in ADL model
	using AIC?
Message-ID: <4984F0CF020000C700004EA5@mail.ivl.se>

Hi,
 
I have specified an ADL model with 8 exogenous variables and I want to
use the AIC (or BIC) to find the optimal lag length of each variable.
 
1. Where do I find a function that calculates the AIC for a linear
model? 
2. Is there a function that that finds one common optimal lag length for
the entire model by minimising the AIC (or BIC)?
3. Is there a function that finds the optimal lag length for each
individual variable by minimising the AIC?
 
I am new to R and apologise if the questions are trivial to some. I
have, however, search the archives and haven't been able to find the
answers and I would be very grateful for any help.
 
/Markus

-----------------------------------------------------------------------
Markus Wr?ke (prev. ?hman)
Swedish Environmental Research Institute
IVL AB
Box 210 60
SE - 100 31 Stockholm
Sweden

tel:           +46 (0)8 - 598 563 21
mobile:     +46 (0)8 - 598 563 21

fax:           +46 (0)8 - 598 563 90
www.ivl.se


From brian at braverock.com  Mon Feb  2 15:28:37 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 02 Feb 2009 08:28:37 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for some option
 portfolio
In-Reply-To: <21704929.post@talk.nabble.com>
References: <21704929.post@talk.nabble.com>
Message-ID: <49870315.6020208@braverock.com>

megh wrote:
> Can people here please guide me how to calculate Componrnt VaR (sensitivity)
> of an option position, for a portfolio which consists of number of stocks
> and option contracts (put ot call or both). Any document, research paper
> over net is highly appreciated.

Sorry to be late to this thread.  As others have noted, option portfolios are 
not normally distributed, so you need to take skewness and kurtosis into 
account at least, and depending on the complexity of your options portfolio, 
you may need to take the greeks on the options into account as well.

The R package PerformanceAnalytics includes functions for computing portfolio 
component VaR and Expected Shortfall using the Gaussian, Cornish Fisher, and 
Student-t approaches.   We've recently published papers in the Journal of Risk, 
RISK, and Computational Finance that describe these approaches.  these papers 
are referenced in PerformanceAnalytics, and I would be happy to make them 
available upon request.

Regards,

     - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From megh700004 at yahoo.com  Mon Feb  2 16:53:23 2009
From: megh700004 at yahoo.com (Megh Dal)
Date: Mon, 2 Feb 2009 07:53:23 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Conponent VaR for some option
	portfolio
In-Reply-To: <49870315.6020208@braverock.com>
Message-ID: <314662.57104.qm@web58103.mail.re3.yahoo.com>

Thanks Brian for this reference. Can I have those papers? Atleast some weblinks will also be fine.

Regards,


--- On Mon, 2/2/09, Brian G. Peterson <brian at braverock.com> wrote:

> From: Brian G. Peterson <brian at braverock.com>
> Subject: Re: [R-SIG-Finance] [R-sig-finance] Conponent VaR for some option portfolio
> To: "megh" <megh700004 at yahoo.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Date: Monday, February 2, 2009, 7:58 PM
> megh wrote:
> > Can people here please guide me how to calculate
> Componrnt VaR (sensitivity)
> > of an option position, for a portfolio which consists
> of number of stocks
> > and option contracts (put ot call or both). Any
> document, research paper
> > over net is highly appreciated.
> 
> Sorry to be late to this thread.  As others have noted,
> option portfolios are not normally distributed, so you need
> to take skewness and kurtosis into account at least, and
> depending on the complexity of your options portfolio, you
> may need to take the greeks on the options into account as
> well.
> 
> The R package PerformanceAnalytics includes functions for
> computing portfolio component VaR and Expected Shortfall
> using the Gaussian, Cornish Fisher, and Student-t
> approaches.   We've recently published papers in the
> Journal of Risk, RISK, and Computational Finance that
> describe these approaches.  these papers are referenced in
> PerformanceAnalytics, and I would be happy to make them
> available upon request.
> 
> Regards,
> 
>     - Brian
> 
> -- Brian G. Peterson
> http://braverock.com/brian/
> Ph: 773-459-4973
> IM: bgpbraverock


From Jose at erini.ac.uk  Mon Feb  2 23:17:24 2009
From: Jose at erini.ac.uk (Jose Iparraguirre D'Elia)
Date: Mon, 2 Feb 2009 22:17:24 -0000
Subject: [R-SIG-Finance] Panel Data Unit Root tests
Message-ID: <C9328F0EEDC3BC439FDABD12060E91090E2D4B@erini1.ERINI.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090202/502616ef/attachment.pl>

From Zeno.Adams at ebs.edu  Tue Feb  3 11:44:09 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Tue, 3 Feb 2009 11:44:09 +0100
Subject: [R-SIG-Finance] Panel Data Unit Root tests
In-Reply-To: <C9328F0EEDC3BC439FDABD12060E91090E2D4B@erini1.ERINI.local>
References: <C9328F0EEDC3BC439FDABD12060E91090E2D4B@erini1.ERINI.local>
Message-ID: <9064522880125945B98983BBAECBA1CC9854C6@exchsrv001.ebs.local>




Note that the Levin-Lin and the IPS test belong to the group of first generation panel unit root tests which do not control for cross-sectional correlation.

A very simple, yet powerful second generation test has been proposed by Hanck(2008), www.statistik.tu-dortmund.de/fileadmin/user_upload/Lehrstuehle/MSind/SFB_475/2008 . As it builds upon comparing the p-values of single time series unit root tests it is also easy to implement with only a few lines of code.


-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Jose Iparraguirre D'Elia
Gesendet: Montag, 2. Februar 2009 23:17
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] Panel Data Unit Root tests


I could not find a package to run panel data unit root tests in R (there's a STATA module, though - PANELUNIT- and routines for Sarno and Taylor's MADF test, Levin-Lin-Chu's test, Im-Pesaran-Shin's Test, etc, to do this).

Hence, I am toying with the idea of having a go at writing up one for R, but would like to know whether anyone is already working on this.

Jos? 

Jos? Luis Iparraguirre
Senior Research Economist
Economic Research Institute of Northern Ireland
2 -14 East Bridge Street
Belfast BT1 3NQ 
Northern Ireland
United Kingdom

	[[alternative HTML version deleted]]

EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Rektor/CEO; Dr. Reimar Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Verwaltungsrat: Dr. Hellmut K. Albrecht, Vorsitzender

From VOSSK at kochind.com  Tue Feb  3 15:44:08 2009
From: VOSSK at kochind.com (Voss, Kent)
Date: Tue, 3 Feb 2009 07:44:08 -0700
Subject: [R-SIG-Finance] Rbloomberg problem "Seems like this is not a
 Bloomberg Workstation"
Message-ID: <13CB99597D0BAA43B07ABFCE29E45F6B024692A5@phx0mbx01.kochind.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090203/6dc59ef3/attachment.pl>

From Zeno.Adams at ebs.edu  Tue Feb  3 16:01:02 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Tue, 3 Feb 2009 16:01:02 +0100
Subject: [R-SIG-Finance] Panel Data Unit Root tests
In-Reply-To: <C9328F0EEDC3BC439FDABD12060E91090E2D4B@erini1.ERINI.local>
References: <C9328F0EEDC3BC439FDABD12060E91090E2D4B@erini1.ERINI.local>
Message-ID: <9064522880125945B98983BBAECBA1CC9854CB@exchsrv001.ebs.local>



Sorry, the link does not work. Here is the paper.


EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Rektor/CEO; Dr. Reimar Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Verwaltungsrat: Dr. Hellmut K. Albrecht, Vorsitzender
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Hanck(2008).pdf
Type: application/octet-stream
Size: 249850 bytes
Desc: Hanck(2008).pdf
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090203/ceac4a8b/attachment.obj>

From albertosantini at gmail.com  Wed Feb  4 12:14:07 2009
From: albertosantini at gmail.com (Alberto Santini)
Date: Wed, 4 Feb 2009 03:14:07 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] An extensive set of scaling laws...
Message-ID: <21828514.post@talk.nabble.com>


A new paper by Olsen World. :)

http://www.olsen.ch/publications/working-papers/

An extensive set of scaling laws and the FX coastline
J. B. Glattfelder, A. Dupuis and R. B. Olsen
2008

http://www.olsen.ch/fileadmin/Publications/Working_Papers/sl-1.pdf

Regards,
Alberto Santini
-- 
View this message in context: http://www.nabble.com/An-extensive-set-of-scaling-laws...-tp21828514p21828514.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From marco.zanella at inbox.com  Wed Feb  4 12:35:41 2009
From: marco.zanella at inbox.com (Zanella Marco)
Date: Wed, 4 Feb 2009 03:35:41 -0800
Subject: [R-SIG-Finance] RNG from skewed Normal distribution
Message-ID: <490CA218A8A.0000066Bmarco.zanella@inbox.com>

Dear Sirs,
I'm in trouble with a random number generation from a skewed Normal distribution. In detail I have to generate numbers from a Normal distribution with a certain skewness and kurtosis.

It's an apparently simple problem but actually I have no idea how to solve it. Have you any suggestions?

Regards,

MZ

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!
Check it out at http://www.inbox.com/marineaquarium


From markus.wrake at ivl.se  Wed Feb  4 13:06:57 2009
From: markus.wrake at ivl.se (=?UTF-8?Q?Markus=20Wr=C3=A5ke?=)
Date: Wed, 04 Feb 2009 13:06:57 +0100
Subject: [R-SIG-Finance] How should I use NeweyWest and vcovHAC in coeftest
	function?
Message-ID: <498992F1.D06E.00C7.0@ivl.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090204/dddef21f/attachment.pl>

From megh700004 at yahoo.com  Wed Feb  4 14:28:16 2009
From: megh700004 at yahoo.com (megh)
Date: Wed, 4 Feb 2009 05:28:16 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] RNG from skewed Normal
	distribution
In-Reply-To: <490CA218A8A.0000066Bmarco.zanella@inbox.com>
References: <490CA218A8A.0000066Bmarco.zanella@inbox.com>
Message-ID: <21830457.post@talk.nabble.com>


look at "sn" library in CRAN



Zanella Marco wrote:
> 
> Dear Sirs,
> I'm in trouble with a random number generation from a skewed Normal
> distribution. In detail I have to generate numbers from a Normal
> distribution with a certain skewness and kurtosis.
> 
> It's an apparently simple problem but actually I have no idea how to solve
> it. Have you any suggestions?
> 
> Regards,
> 
> MZ
> 
> ____________________________________________________________
> FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on
> your desktop!
> Check it out at http://www.inbox.com/marineaquarium
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/RNG-from-skewed-Normal-distribution-tp21828792p21830457.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ecjbosu at aol.com  Wed Feb  4 18:54:44 2009
From: ecjbosu at aol.com (Joe W. Byers)
Date: Wed, 4 Feb 2009 17:54:44 +0000 (UTC)
Subject: [R-SIG-Finance] Yield Curve
Message-ID: <loom.20090204T174623-221@post.gmane.org>

This may be outside this discussion group but you finance junkies might 
like this.

The term structure Rate curve is really interesting lately.  The one year 
libor swap and one year money market differ by about 70 basis points.  
The data is below.  I am searching for a reason for this difference. 
The forward rates derived from this curve take a big dip one year out.  

1 WEEK	0.351%
1 MONTH	0.445%
2 MONTH	0.949%
3 MONTH	1.234%
4 MONTH	1.460%
5 MONTH	1.600%
6 MONTH	1.776%
7 MONTH	1.827%
8 MONTH	1.881%
9 MONTH	1.935%
10 MONTH	1.981%
11 MONTH	2.033%
12 MONTH	2.084%
USD IR Swap 1Y	1.316%
USD IR Swap 2Y	1.600%
USD IR Swap 3Y	1.955%
USD IR Swap 4Y	2.244%
USD IR Swap 5Y	2.457%
USD IR Swap 6Y	2.615%
USD IR Swap 7Y	2.757%
USD IR Swap 8Y	2.866%
USD IR Swap 9Y	2.951%
USD IR Swap 10Y	3.031%
USD IR Swap 12Y	3.162%
USD IR Swap 15Y	3.299%
USD IR Swap 20Y	3.340%
USD IR Swap 25Y	3.329%
USD IR Swap 30Y	3.328%


From micha.keijzers at gmail.com  Wed Feb  4 19:02:23 2009
From: micha.keijzers at gmail.com (Micha Keijzers)
Date: Wed, 4 Feb 2009 19:02:23 +0100
Subject: [R-SIG-Finance] Yield Curve
In-Reply-To: <loom.20090204T174623-221@post.gmane.org>
References: <loom.20090204T174623-221@post.gmane.org>
Message-ID: <13eac1860902041002t1a87a674mb07afd4a8498e784@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090204/611910c4/attachment.pl>

From robert at sanctumfi.com  Wed Feb  4 19:42:04 2009
From: robert at sanctumfi.com (Robert Sams)
Date: Wed, 4 Feb 2009 18:42:04 -0000
Subject: [R-SIG-Finance] Yield Curve
References: <loom.20090204T174623-221@post.gmane.org>
	<SANCTUMFISERVERNc3J0000259b@sanctumfi.com>
Message-ID: <SANCTUMFISERVER8HFP000025a7@sanctumfi.com>

Hi Joe, 

It's due to the spread between libor and fed funds, which blew out in
August 2007 and has been very volatile since. This is of course due to
the total breakdown of the banking system and the inter-bank lending
market, which is what libor represents.  In theory, all of these money
market tenors represent expectations of the average overnight rate that
is targeted by the central bank (in USD the Fed Funds Effective) + some
credit/liquidity risk spread. Prior to the banking blow-up this spread
traded in a tight 10-20bp range but has been as high has 360ish in
October last year and is currently about 98bps for the 3m tenor. The
spread has a term-structure: 1m is about 23bp, 6m is about 150 and the
12m is over 170 (12m libor is meaningless btw, as there is no longer ANY
interbank lending done for 12m terms these days). Given the volatility
of these things we speculate on their direction and trade them in the
rate derivatives market. In the jargon it's called the "FRA/OIS basis
swap" (a forward rate agreement (libor) vs a forward-dated overnight
index swap (fed funds).

And now you can see why why that 12m libor is 2.08 whilst the 1y swap is
1.32. The 1y USD swap is a 1y 30/360 fixed rate vs four 3m libor
fixings, so the swap rate really embodies the 3m FRA/OIS spread even
though it's a 1y maturity; the 12M libor is also a 1y maturty but of
course trades at much wider spread over expected fed funds.. on the
basis of the numbers above, an additional 72bp to be precise.

There is btw a general point to be made here about forward rates. Actual
forward rates (like those traded in the FRA market or implied in the FX
forwards, etc) are not equal to the implied forward off the spot curve.
For example, a 3x6 FRA is NOT just some algebra on the 3m and 6m spot
rates because the basis spread implied in the 6m tenor is different from
the 3m tenor, but the 3x6 fra represents the expected 3m basis in
3-month's time, which is not the same thing. This has always been the
case but in the old regime didn't matter because the error involved in
ignoring it was tiny. Now it's big and matters allot. 

Robert


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Micha
Keijzers
Sent: 04 February 2009 18:02
To: Joe W. Byers
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Yield Curve

Joe,

Money costs money: swap is simply exchanging cash flows, while LIBOR
contains the cost of borrowing/lending money. LIBOR has costs for
liquidity, while swap doesn't. And apparently 1Y liquidity is priced at
around 70bps.

Regards,
Micha


2009/2/4 Joe W. Byers <ecjbosu at aol.com>

> This may be outside this discussion group but you finance junkies 
> might like this.
>
> The term structure Rate curve is really interesting lately.  The one 
> year libor swap and one year money market differ by about 70 basis
points.
> The data is below.  I am searching for a reason for this difference.
> The forward rates derived from this curve take a big dip one year out.
>
> 1 WEEK  0.351%
> 1 MONTH 0.445%
> 2 MONTH 0.949%
> 3 MONTH 1.234%
> 4 MONTH 1.460%
> 5 MONTH 1.600%
> 6 MONTH 1.776%
> 7 MONTH 1.827%
> 8 MONTH 1.881%
> 9 MONTH 1.935%
> 10 MONTH        1.981%
> 11 MONTH        2.033%
> 12 MONTH        2.084%
> USD IR Swap 1Y  1.316%
> USD IR Swap 2Y  1.600%
> USD IR Swap 3Y  1.955%
> USD IR Swap 4Y  2.244%
> USD IR Swap 5Y  2.457%
> USD IR Swap 6Y  2.615%
> USD IR Swap 7Y  2.757%
> USD IR Swap 8Y  2.866%
> USD IR Swap 9Y  2.951%
> USD IR Swap 10Y 3.031%
> USD IR Swap 12Y 3.162%
> USD IR Swap 15Y 3.299%
> USD IR Swap 20Y 3.340%
> USD IR Swap 25Y 3.329%
> USD IR Swap 30Y 3.328%
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From albertosantini at gmail.com  Wed Feb  4 20:06:19 2009
From: albertosantini at gmail.com (Alberto Santini)
Date: Wed, 4 Feb 2009 11:06:19 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] An extensive set of scaling
	laws...
In-Reply-To: <21828514.post@talk.nabble.com>
References: <21828514.post@talk.nabble.com>
Message-ID: <21837236.post@talk.nabble.com>




Alberto Santini wrote:
> 
> http://www.olsen.ch/fileadmin/Publications/Working_Papers/sl-1.pdf
> 

The R scripts are located at 

http://www.olsen.ch/index.php?id=255

Regards,
Alberto Santini

-- 
View this message in context: http://www.nabble.com/An-extensive-set-of-scaling-laws...-tp21828514p21837236.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From andyzhu35 at yahoo.com  Wed Feb  4 21:10:42 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Wed, 4 Feb 2009 12:10:42 -0800 (PST)
Subject: [R-SIG-Finance] Capacity of fPortfolio
In-Reply-To: <21837236.post@talk.nabble.com>
Message-ID: <641247.81069.qm@web56204.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090204/98cf71b2/attachment.pl>

From schreiber_irene at web.de  Thu Feb  5 01:05:13 2009
From: schreiber_irene at web.de (Irene Schreiber)
Date: Thu, 5 Feb 2009 01:05:13 +0100
Subject: [R-SIG-Finance] package ccgarch - dcc.estimation
Message-ID: <001b01c98725$6c6ad450$45407cf0$@de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090205/2a96e7a6/attachment.pl>

From sue at xlsolutions-corp.com  Fri Feb  6 02:16:09 2009
From: sue at xlsolutions-corp.com (Sue Turner)
Date: Thu, 05 Feb 2009 18:16:09 -0700
Subject: [R-SIG-Finance] Call for Beta Testers: R+ FIN (read R-PLUS FINANCE)
Message-ID: <20090205181609.aa8924c5d28ca71e2a043bb294e795eb.f645c4ed5d.wbe@email.secureserver.net>

Code-named  R+FIN, our commercially supported R group is looking for
beta testers on Windows. We'd love to get your
feedback our Professional version that has additional functionalities
for enterprise support.

The sooner we get your feedback and inputs, the faster we'll make
changes for the final release!

To participate in our beta test program, please email
jen at xlsolutions-corp.com or go online
www.xlsolutions-corp.com/contact.htm

The linux version of R+FIN is also into development with a great
graphical user interface, and we'll be calling for beta testers this
fall. If you want to beta test the linux
version, please email Jennifer McDonald, jen at xlsolutions-corp.com.

We look forward to hearing your comments and inputs on R+FIN... please
feel free to suggest a final name for our commercially supported R+FIN.

Thank you!

Jennifer McDonald
Assistant to Mary RITZ
Research and Consulting
XLSolutions Corporation
1700 7th Ave
Suite 2100
Seattle, WA 98101
jen at xlsolutions-corp.com
206 686 1578


From kriskumar at earthlink.net  Fri Feb  6 23:39:51 2009
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Fri, 06 Feb 2009 17:39:51 -0500
Subject: [R-SIG-Finance] RNG from skewed Normal distribution
In-Reply-To: <490CA218A8A.0000066Bmarco.zanella@inbox.com>
References: <490CA218A8A.0000066Bmarco.zanella@inbox.com>
Message-ID: <498CBC37.7090200@earthlink.net>

The package SuppDists and the Johnson distribution in there will do this 
try ?rJohnson

Zanella Marco wrote:
> Dear Sirs,
> I'm in trouble with a random number generation from a skewed Normal distribution. In detail I have to generate numbers from a Normal distribution with a certain skewness and kurtosis.
>
> It's an apparently simple problem but actually I have no idea how to solve it. Have you any suggestions?
>
> Regards,
>
> MZ
>
> ____________________________________________________________
> FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!
> Check it out at http://www.inbox.com/marineaquarium
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From klein82517 at yahoo.de  Sun Feb  8 15:01:51 2009
From: klein82517 at yahoo.de (Andreas Klein)
Date: Sun, 8 Feb 2009 14:01:51 +0000 (GMT)
Subject: [R-SIG-Finance] Standard Errors for VAR(p) Estimation with dse1
Message-ID: <739294.631.qm@web24207.mail.ird.yahoo.com>

Hello.

I have two times seires of returns. Both can be modelled with a VAR(2)-process. I use the package dse1 and the estimation routine estMaxLik.

When I look at the output I cannot realize any standard errors for the coeffients.

How do I obtain the standard errors for the VAR(2)-coefficients, when I use estMaxLik from the package dse1?


Regards,
Andreas.





From pdebruic at gmail.com  Mon Feb  9 16:44:19 2009
From: pdebruic at gmail.com (Paul DeBruicker)
Date: Mon, 9 Feb 2009 15:44:19 +0000
Subject: [R-SIG-Finance] Rbloomberg problem "Seems like this is not a
	Bloomberg Workstation"
In-Reply-To: <13CB99597D0BAA43B07ABFCE29E45F6B024692A5@phx0mbx01.kochind.com>
References: <13CB99597D0BAA43B07ABFCE29E45F6B024692A5@phx0mbx01.kochind.com>
Message-ID: <f2e3401f0902090744nbd417ck4f07b37a5559ede@mail.gmail.com>

Hi Kent,

I'm confused by this bit of your post:

>At first the Bloomberg installation didn't have the COM API installed.
>So I went to WAPI in Bloomberg and downloaded and installed both the
>Desktop v3.x - API and ActiveX Data Control SDK Package and the Desktop
>Pre-3.x API SDK Package (the v3.x didn't work at first, so I tried the
>older one to no avail)


Did either of the packages install?  From my reading it looks like
neither did.  I don't think you need them.  Do you have the Excel
Add-In & DDE Server installed and functioning on your Bloomberg
terminal?    If not, then you might go here:
http://about.bloomberg.com/software/info_upgr.html and download and
install them, then restart, log in to bloomberg, and try your
blpconnect() calls again.  RBloomberg, which must run on the BBG
terminal, just interfaces with the COM api provided & used by the
Excel add-in and dde server.


Good luck

Paul

On Tue, Feb 3, 2009 at 2:44 PM, Voss, Kent <VOSSK at kochind.com> wrote:
> I am trying to set up the RBloomberg connection on a new machine, and am
> running into the following error when I run blpConnect()
>
>> blpConnect()
> [1] "Error : Invalid class string\n"
> attr(,"class")
> [1] "BlpCOMConnect"
> Warning message:
> In blpConnect() :
>  Seems like this is not a Bloomberg Workstation:  Error : Invalid class
> string
>>
>
> I have set up RBloomberg many times before, and never run across this,
> so here's what debugging/troubleshooting information I have.
>
> At first the Bloomberg installation didn't have the COM API installed.
> So I went to WAPI in Bloomberg and downloaded and installed both the
> Desktop v3.x - API and ActiveX Data Control SDK Package and the Desktop
> Pre-3.x API SDK Package (the v3.x didn't work at first, so I tried the
> older one to no avail)
>
> I downloaded and installed RDCOMClient and Rbloomberg.  Here is the
> session info dump with the relevant information:
>> sessionInfo()
> R version 2.8.1 (2008-12-22)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] graphics  grDevices datasets  stats     tcltk     utils     methods
> base
>
> other attached packages:
>  [1] RBloomberg_0.1-10  chron_2.3-28       RDCOMClient_0.92-0 zoo_1.5-4
> RODBC_1.2-4        svSocket_0.9-42    svMisc_0.9-47
>  [8] TinnR_1.0.2        R2HTML_1.59        Hmisc_3.4-4
>
> loaded via a namespace (and not attached):
> [1] cluster_1.11.11 grid_2.8.1      lattice_0.17-17 tools_2.8.1
>>
>
> The problem seems to be in the COMCreate() function call inside of
> blpConnect().  When I just run the COMCreate(), I get the following:
>> COMCreate("Bloomberg.Data.1")
> Error: Invalid class string
>>
>
> I can run COMCreate("Excel.Application") successfully, so I don't think
> it's the RDCOMClient that's the issue (of course I could be wrong on
> that).  I can download Bloomberg into Excel no problem is that is
> helpful at all.  Bloomberg is installed in the default at c:\blp\API,
> but I'm not sure if there is a particular file I should be looking for
> the COMCreate.  "bbcomm.exe" is in c:\blp\api and I can fire that up and
> it works, but I'm not sure what else is needed.  When I load the
> Rbloomberg library all looks fine:
>> library(RBloomberg)
> Loading required package: RDCOMClient
> Loading required package: chron
> Contents of bbfields have been stored in .bbfields in the current
> workspace
>>
>
> At this point I've exhausted my rudimentary troubleshooting skills, and
> so am looking for any suggestions.  I have searched the archives, but
> the only hits I get on my error is the actual code for the blpConnect()
> function.  Any help would be greatly appreciated.
>
> Thank you in advance,
>
> Kent
>
> "EMF <kochind.com>" made the following annotations.
> ------------------------------------------------------------------------------
> The information in this e-mail and any attachments is co...{{dropped:15}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From pdebruic at gmail.com  Mon Feb  9 16:47:38 2009
From: pdebruic at gmail.com (Paul DeBruicker)
Date: Mon, 9 Feb 2009 15:47:38 +0000
Subject: [R-SIG-Finance] Call for Beta Testers: R+ FIN (read R-PLUS
	FINANCE)
In-Reply-To: <20090205181609.aa8924c5d28ca71e2a043bb294e795eb.f645c4ed5d.wbe@email.secureserver.net>
References: <20090205181609.aa8924c5d28ca71e2a043bb294e795eb.f645c4ed5d.wbe@email.secureserver.net>
Message-ID: <f2e3401f0902090747g3b0034d6o311b3a0f6c8cf4f9@mail.gmail.com>

Hi -


Are you offering multi-year site licenses to beta testers in return
for their testing services?  Or just cash?  I didn't see anything
mentioned in your offer.  Thanks


Paul





On Fri, Feb 6, 2009 at 1:16 AM, Sue Turner <sue at xlsolutions-corp.com> wrote:
> Code-named  R+FIN, our commercially supported R group is looking for
> beta testers on Windows. We'd love to get your
> feedback our Professional version that has additional functionalities
> for enterprise support.
>
> The sooner we get your feedback and inputs, the faster we'll make
> changes for the final release!
>
> To participate in our beta test program, please email
> jen at xlsolutions-corp.com or go online
> www.xlsolutions-corp.com/contact.htm
>
> The linux version of R+FIN is also into development with a great
> graphical user interface, and we'll be calling for beta testers this
> fall. If you want to beta test the linux
> version, please email Jennifer McDonald, jen at xlsolutions-corp.com.
>
> We look forward to hearing your comments and inputs on R+FIN... please
> feel free to suggest a final name for our commercially supported R+FIN.
>
> Thank you!
>
> Jennifer McDonald
> Assistant to Mary RITZ
> Research and Consulting
> XLSolutions Corporation
> 1700 7th Ave
> Suite 2100
> Seattle, WA 98101
> jen at xlsolutions-corp.com
> 206 686 1578
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From VOSSK at kochind.com  Mon Feb  9 18:52:00 2009
From: VOSSK at kochind.com (Voss, Kent)
Date: Mon, 9 Feb 2009 10:52:00 -0700
Subject: [R-SIG-Finance] Rbloomberg problem "Seems like this is not a
 Bloomberg Workstation"
In-Reply-To: <f2e3401f0902090744nbd417ck4f07b37a5559ede@mail.gmail.com>
References: <13CB99597D0BAA43B07ABFCE29E45F6B024692A5@phx0mbx01.kochind.com>
	<f2e3401f0902090744nbd417ck4f07b37a5559ede@mail.gmail.com>
Message-ID: <13CB99597D0BAA43B07ABFCE29E45F6B024692D9@phx0mbx01.kochind.com>

Paul,

I believe that I did have the Excel add-in and DDE server installed
already (at least I could pull Bloomberg history and real-time data in
Excel).  I thought I had to have those COM API's installed as well.
Anyway, just to be sure I went to that site you listed and there was an
update to the Excel add-in.  I installed the update and voila, it works
now.
As I can figure, it may have been one of the following: 
a) I didn't have the Excel Add-in installed properly such that I could
pull data in Excel, but not through R.
b) Installing the COM/API SDK Package overwrote something that caused it
not to work.
c) Some combination of (a) and (b)
d) Gross operator error to which I am thankful for your suggestions that
fixed it.

Many thanks again,

Kent

 

-----Original Message-----
From: Paul DeBruicker [mailto:pdebruic at gmail.com] 
Sent: Monday, February 09, 2009 8:44 AM
To: Voss, Kent
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Rbloomberg problem "Seems like this is not
a Bloomberg Workstation"

Hi Kent,

I'm confused by this bit of your post:

>At first the Bloomberg installation didn't have the COM API installed.
>So I went to WAPI in Bloomberg and downloaded and installed both the 
>Desktop v3.x - API and ActiveX Data Control SDK Package and the Desktop

>Pre-3.x API SDK Package (the v3.x didn't work at first, so I tried the 
>older one to no avail)


Did either of the packages install?  From my reading it looks like
neither did.  I don't think you need them.  Do you have the Excel Add-In
& DDE Server installed and functioning on your Bloomberg
terminal?    If not, then you might go here:
http://about.bloomberg.com/software/info_upgr.html and download and
install them, then restart, log in to bloomberg, and try your
blpconnect() calls again.  RBloomberg, which must run on the BBG
terminal, just interfaces with the COM api provided & used by the Excel
add-in and dde server.


Good luck

Paul

On Tue, Feb 3, 2009 at 2:44 PM, Voss, Kent <VOSSK at kochind.com> wrote:
> I am trying to set up the RBloomberg connection on a new machine, and 
> am running into the following error when I run blpConnect()
>
>> blpConnect()
> [1] "Error : Invalid class string\n"
> attr(,"class")
> [1] "BlpCOMConnect"
> Warning message:
> In blpConnect() :
>  Seems like this is not a Bloomberg Workstation:  Error : Invalid 
> class string
>>
>
> I have set up RBloomberg many times before, and never run across this,

> so here's what debugging/troubleshooting information I have.
>
> At first the Bloomberg installation didn't have the COM API installed.
> So I went to WAPI in Bloomberg and downloaded and installed both the 
> Desktop v3.x - API and ActiveX Data Control SDK Package and the 
> Desktop Pre-3.x API SDK Package (the v3.x didn't work at first, so I 
> tried the older one to no avail)
>
> I downloaded and installed RDCOMClient and Rbloomberg.  Here is the 
> session info dump with the relevant information:
>> sessionInfo()
> R version 2.8.1 (2008-12-22)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] graphics  grDevices datasets  stats     tcltk     utils
methods
> base
>
> other attached packages:
>  [1] RBloomberg_0.1-10  chron_2.3-28       RDCOMClient_0.92-0
zoo_1.5-4
> RODBC_1.2-4        svSocket_0.9-42    svMisc_0.9-47
>  [8] TinnR_1.0.2        R2HTML_1.59        Hmisc_3.4-4
>
> loaded via a namespace (and not attached):
> [1] cluster_1.11.11 grid_2.8.1      lattice_0.17-17 tools_2.8.1
>>
>
> The problem seems to be in the COMCreate() function call inside of 
> blpConnect().  When I just run the COMCreate(), I get the following:
>> COMCreate("Bloomberg.Data.1")
> Error: Invalid class string
>>
>
> I can run COMCreate("Excel.Application") successfully, so I don't 
> think it's the RDCOMClient that's the issue (of course I could be 
> wrong on that).  I can download Bloomberg into Excel no problem is 
> that is helpful at all.  Bloomberg is installed in the default at 
> c:\blp\API, but I'm not sure if there is a particular file I should be

> looking for the COMCreate.  "bbcomm.exe" is in c:\blp\api and I can 
> fire that up and it works, but I'm not sure what else is needed.  When

> I load the Rbloomberg library all looks fine:
>> library(RBloomberg)
> Loading required package: RDCOMClient
> Loading required package: chron
> Contents of bbfields have been stored in .bbfields in the current 
> workspace
>>
>
> At this point I've exhausted my rudimentary troubleshooting skills, 
> and so am looking for any suggestions.  I have searched the archives, 
> but the only hits I get on my error is the actual code for the 
> blpConnect() function.  Any help would be greatly appreciated.
>
> Thank you in advance,
>
> Kent
>
> "EMF <kochind.com>" made the following annotations.
> ----------------------------------------------------------------------
> -------- The information in this e-mail and any attachments is 
> co...{{dropped:15}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


"EMF <kochind.com>" made the following annotations.
------------------------------------------------------------------------------
The information in this e-mail and any attachments is co...{{dropped:14}}


From dbmeehan at frontiernet.net  Mon Feb  9 19:52:12 2009
From: dbmeehan at frontiernet.net (brian meehan)
Date: Mon, 9 Feb 2009 13:52:12 -0500
Subject: [R-SIG-Finance] adding p&l streams
In-Reply-To: <13CB99597D0BAA43B07ABFCE29E45F6B024692D9@phx0mbx01.kochind.com>
References: <13CB99597D0BAA43B07ABFCE29E45F6B024692A5@phx0mbx01.kochind.com><f2e3401f0902090744nbd417ck4f07b37a5559ede@mail.gmail.com>
	<13CB99597D0BAA43B07ABFCE29E45F6B024692D9@phx0mbx01.kochind.com>
Message-ID: <471804AED1DF4E5CA1AF10F24E36E7AA@memcsmlkmPC>

i have a group of time series of p&ls

BEDH05U05_PL <- xts(yyy[,21], order.by = index(BEDH05U05_C['::2004-03']), 
frequency = NULL )
BEDH04U04_PL <- xts(zzz[,21], order.by = index(BEDH04U04_C['::2003-03']), 
frequency = NULL )

the p&ls are for only partially overlapping times...

> tail(BEDH05U05_PL )
            [,1]
2003-03-24 83.50
2003-03-25 83.50
2003-03-26 75.25
2003-03-27 75.25
2003-03-28 82.25
2003-03-31 82.25
> tail(BEDH04U04_PL )
            [,1]
2004-03-24 56.25
2004-03-25 56.25
2004-03-26 55.50
2004-03-29 55.50
2004-03-30 54.25
2004-03-31 54.25

i can combine the time series into a larger time series using

combined_pl <- merge (BEDH05U05_PL, BEDH04U04_PL )

i am trying to then add these two columns into a cumulative p&l. i tried 
using combined_pl [is.na(combined_pl )] <- 0  to remove the NA values but 
this doesnt solve the problem since when i hit the last p&l value in 
BEDH05U05_PL the remaining values are set to 0 since they are NA. is there a 
way to set all NA values until the first numerical value to 0 and then all 
NA values after the last numerical value to the last p&l? i wasnt able to 
find a fill forward type of function.

thanks  - brian


From josh.m.ulrich at gmail.com  Mon Feb  9 20:06:08 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Mon, 9 Feb 2009 13:06:08 -0600
Subject: [R-SIG-Finance] adding p&l streams
In-Reply-To: <471804AED1DF4E5CA1AF10F24E36E7AA@memcsmlkmPC>
References: <13CB99597D0BAA43B07ABFCE29E45F6B024692A5@phx0mbx01.kochind.com>
	<f2e3401f0902090744nbd417ck4f07b37a5559ede@mail.gmail.com>
	<13CB99597D0BAA43B07ABFCE29E45F6B024692D9@phx0mbx01.kochind.com>
	<471804AED1DF4E5CA1AF10F24E36E7AA@memcsmlkmPC>
Message-ID: <8cca69990902091106j123df662g5548b32437f64f5c@mail.gmail.com>

require(zoo)
combined_pl <- na.locf(combined_pl, na.rm=FALSE)
combined_pl[is.na(combined_pl)] <- 0

HTH,
Josh
--
http://quantemplation.blogspot.com



On Mon, Feb 9, 2009 at 12:52 PM, brian meehan <dbmeehan at frontiernet.net> wrote:
> i have a group of time series of p&ls
>
> BEDH05U05_PL <- xts(yyy[,21], order.by = index(BEDH05U05_C['::2004-03']),
> frequency = NULL )
> BEDH04U04_PL <- xts(zzz[,21], order.by = index(BEDH04U04_C['::2003-03']),
> frequency = NULL )
>
> the p&ls are for only partially overlapping times...
>
>> tail(BEDH05U05_PL )
>
>           [,1]
> 2003-03-24 83.50
> 2003-03-25 83.50
> 2003-03-26 75.25
> 2003-03-27 75.25
> 2003-03-28 82.25
> 2003-03-31 82.25
>>
>> tail(BEDH04U04_PL )
>
>           [,1]
> 2004-03-24 56.25
> 2004-03-25 56.25
> 2004-03-26 55.50
> 2004-03-29 55.50
> 2004-03-30 54.25
> 2004-03-31 54.25
>
> i can combine the time series into a larger time series using
>
> combined_pl <- merge (BEDH05U05_PL, BEDH04U04_PL )
>
> i am trying to then add these two columns into a cumulative p&l. i tried
> using combined_pl [is.na(combined_pl )] <- 0  to remove the NA values but
> this doesnt solve the problem since when i hit the last p&l value in
> BEDH05U05_PL the remaining values are set to 0 since they are NA. is there a
> way to set all NA values until the first numerical value to 0 and then all
> NA values after the last numerical value to the last p&l? i wasnt able to
> find a fill forward type of function.
>
> thanks  - brian
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From irafuchs at gmail.com  Mon Feb  9 21:25:10 2009
From: irafuchs at gmail.com (Fuchs Ira)
Date: Mon, 9 Feb 2009 15:25:10 -0500
Subject: [R-SIG-Finance] stock quotes
Message-ID: <7A7E72D8-FEFC-482C-BD96-D0CC934D43F4@gmail.com>

Is there a way to get current (or even 15 minute delayed) stock quotes  
from the major exchanges? I know about get.hist.quote but is there a  
way to get current quotes?


From cedrickj at cavengerllc.com  Mon Feb  9 21:50:45 2009
From: cedrickj at cavengerllc.com (Cedrick Johnson)
Date: Mon, 9 Feb 2009 15:50:45 -0500
Subject: [R-SIG-Finance] stock quotes
In-Reply-To: <7A7E72D8-FEFC-482C-BD96-D0CC934D43F4@gmail.com>
References: <7A7E72D8-FEFC-482C-BD96-D0CC934D43F4@gmail.com>
Message-ID: <6feb0b1a0902091250l37fbfec1t8a9cf003dfc110e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090209/fb6eec94/attachment.pl>

From davidr at rhotrading.com  Mon Feb  9 22:11:58 2009
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Mon, 9 Feb 2009 15:11:58 -0600
Subject: [R-SIG-Finance] stock quotes
In-Reply-To: <7A7E72D8-FEFC-482C-BD96-D0CC934D43F4@gmail.com>
References: <7A7E72D8-FEFC-482C-BD96-D0CC934D43F4@gmail.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE77023C206D@rhopost.rhotrading.com>

You can get (sort of) real-time quotes from NASDAQ and NYSE, e.g.:

http://www.nasdaq.com/aspx/nasdaqlastsale.aspx?symbol=MSFT&selected=MSFT

http://www.nyse.com/about/listed/lcddata.html?ticker=IBM

There remains to interface these with R; not sure it would be easy,
since they sell datafeeds for $$$$,
but for a small number of stocks you could get some intra-day data this
way.

-- David


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Fuchs Ira
Sent: Monday, February 09, 2009 2:25 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] stock quotes

Is there a way to get current (or even 15 minute delayed) stock quotes  
from the major exchanges? I know about get.hist.quote but is there a  
way to get current quotes?

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From michael.sankowski at gmail.com  Mon Feb  9 22:16:36 2009
From: michael.sankowski at gmail.com (Michael Sankowski)
Date: Mon, 9 Feb 2009 15:16:36 -0600
Subject: [R-SIG-Finance] stock quotes
In-Reply-To: <F9F2A641C593D7408925574C05A7BE77023C206D@rhopost.rhotrading.com>
References: <7A7E72D8-FEFC-482C-BD96-D0CC934D43F4@gmail.com>
	<F9F2A641C593D7408925574C05A7BE77023C206D@rhopost.rhotrading.com>
Message-ID: <b34262a80902091316u2ebe517cvefb04450c2d8d057@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090209/09f16976/attachment.pl>

From klein82517 at yahoo.de  Tue Feb 10 14:25:11 2009
From: klein82517 at yahoo.de (Andreas Klein)
Date: Tue, 10 Feb 2009 13:25:11 +0000 (GMT)
Subject: [R-SIG-Finance] Spread Libor-Fed Fund Rate: ARIMA(1,1,1)
Message-ID: <895055.79884.qm@web24206.mail.ird.yahoo.com>

Hello.

I have 246 realizations of the spread of the 3 Month Libor (L3M) and the Fed Fund Rate (FFR) from January 1985 to June 2005. I took the logarithms of L3M and FFR and calculated the difference. At last I obtain my 246 observations.

With the SBC or BIC criterion I identified an ARIMA(1,1,1)-process.
The estimation gives the folllowing results:

> x <- log(L3M)-log(FFR)
> arima(x,order=c(1,1,1),method="ML",include.mean=FALSE)

Call:
arima(x = x, order = c(1, 1, 1), include.mean = FALSE, method = "ML")

Coefficients:
         ar1      ma1
      0.6890  -0.9185
s.e.  0.1374   0.0917

sigma^2 estimated as 0.002002:  log likelihood = 413.25,  aic = -820.5


Now I try the model based bootstrap to test for significance of the coefficients:

model <- arima(x,order=c(1,1,1),method="ML",include.mean=FALSE)

ar1 <- as.numeric(model$coef[1])
ma1 <- as.numeric(model$coef[2])

res <- model$res[3:246]-mean(model$res[3:246])

ar1_boot <- numeric(10000)
ma1_boot <- numeric(10000)

for (i in 1:10000) {

  res_boot <- sample(res,replace=TRUE)
  
  bootseries <- numeric(243)

  for (t in 4:246) {

    bootseries[t-3] <- (1+ar1)*ifelse(t<=4,x[t-1],bootseries[t-4])-ar1*ifelse(t<=5,x[t-2],bootseries[t-5])+res_boot[t-2]+ma1*res_boot[t-3]

  }

  model_boot <- arima(bootseries,order=c(1,1,1),method="ML",include.mean=FALSE)

  ar1_boot[i] <- as.numeric(model_boot$coef[1])
  ma1_boot[i] <- as.numeric(model_boot$coef[2])
  
}


When I try to run the code, R interrupts with the following message:
Failure in solve.default(res$hessian * n.used) : 
        Lapackroutine dgesv: System is singular



What is wrong???


Please help me!



Regards,
Andreas.






From Eduard.Pieterse at macquarie.com  Tue Feb 10 15:25:51 2009
From: Eduard.Pieterse at macquarie.com (Eduard Pieterse (Macquarie Securities))
Date: Tue, 10 Feb 2009 14:25:51 -0000
Subject: [R-SIG-Finance] Help with optimization
Message-ID: <A4B987FA3EE557449BC63149ED42D378028ADB12@ntlonexm01.pc.internal.macquarie.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090210/45807a96/attachment.pl>

From rory.winston at gmail.com  Wed Feb 11 12:41:52 2009
From: rory.winston at gmail.com (Rory Winston)
Date: Wed, 11 Feb 2009 11:41:52 +0000
Subject: [R-SIG-Finance] Help with optimization
Message-ID: <3f446aa30902110341y71e3c83fl596d27236786d39e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090211/c53753c8/attachment.pl>

From vfulco1 at gmail.com  Thu Feb 12 00:17:29 2009
From: vfulco1 at gmail.com (Vince Fulco)
Date: Wed, 11 Feb 2009 18:17:29 -0500
Subject: [R-SIG-Finance] Fwd: Question on multiple sessions...
In-Reply-To: <34f2770f0902111435n16a0bf1l8623281794918211@mail.gmail.com>
References: <34f2770f0902111435n16a0bf1l8623281794918211@mail.gmail.com>
Message-ID: <34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>

Assuming one wants to use an open session to collect real time data a
la IBrokers or similar, what would be the best way to maintain another
session for calculations?  Is RServe the answer or is there a lighter
solution for semi-auto analytical script running?  Either Windows or
Linux solutions would be appreciated.  TIA


Vince Fulco, CFA, CAIA
612.424.5477 (universal)
vfulco1 at gmail.com


From nelson.ana at gmail.com  Thu Feb 12 11:53:19 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Thu, 12 Feb 2009 10:53:19 +0000
Subject: [R-SIG-Finance] Fwd: Question on multiple sessions...
In-Reply-To: <34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>
References: <34f2770f0902111435n16a0bf1l8623281794918211@mail.gmail.com>
	<34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>
Message-ID: <a7d6d2740902120253x4a9df39flffc66c4e8655d963@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090212/c3f9bcbf/attachment.pl>

From ggrothendieck at gmail.com  Thu Feb 12 12:49:47 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 12 Feb 2009 06:49:47 -0500
Subject: [R-SIG-Finance] Fwd: Question on multiple sessions...
In-Reply-To: <34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>
References: <34f2770f0902111435n16a0bf1l8623281794918211@mail.gmail.com>
	<34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>
Message-ID: <971536df0902120349n214156edl68f480bd8ab13cee@mail.gmail.com>

One possibility is to write the real time acquisition in tcl
and then the actual data processing in R.  The tcltk package
(which is included in the Windows distribution of R but is available
from CRAN on all distributions) can handle tasks running concurrently
with R.  See the svSocket and Rpad packages as examples
of this approach.

On Wed, Feb 11, 2009 at 6:17 PM, Vince Fulco <vfulco1 at gmail.com> wrote:
> Assuming one wants to use an open session to collect real time data a
> la IBrokers or similar, what would be the best way to maintain another
> session for calculations?  Is RServe the answer or is there a lighter
> solution for semi-auto analytical script running?  Either Windows or
> Linux solutions would be appreciated.  TIA
>
>
> Vince Fulco, CFA, CAIA
> 612.424.5477 (universal)
> vfulco1 at gmail.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From vfulco1 at gmail.com  Thu Feb 12 13:45:20 2009
From: vfulco1 at gmail.com (Vince Fulco)
Date: Thu, 12 Feb 2009 07:45:20 -0500
Subject: [R-SIG-Finance] Fwd: Question on multiple sessions...
In-Reply-To: <a7d6d2740902120253x4a9df39flffc66c4e8655d963@mail.gmail.com>
References: <34f2770f0902111435n16a0bf1l8623281794918211@mail.gmail.com>
	<34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>
	<a7d6d2740902120253x4a9df39flffc66c4e8655d963@mail.gmail.com>
Message-ID: <34f2770f0902120445l2ee22a24ie34aace64b7e76df@mail.gmail.com>

Yes to both question sbut it could be as simple as taking a copy of
the real time data and calculating on that.  I've noted Gabor's answer
(thanks!) but would welcome yours as well.


Vince Fulco, CFA, CAIA
612.424.5477 (universal)
vfulco1 at gmail.com



On Thu, Feb 12, 2009 at 5:53 AM, Ana Nelson <nelson.ana at gmail.com> wrote:
> Do you just need to run 2 sessions at once? Or do you also need to be able
> to pass information from one to the other?
>


From nelson.ana at gmail.com  Thu Feb 12 14:26:21 2009
From: nelson.ana at gmail.com (Ana Nelson)
Date: Thu, 12 Feb 2009 13:26:21 +0000
Subject: [R-SIG-Finance] Fwd: Question on multiple sessions...
In-Reply-To: <34f2770f0902120445l2ee22a24ie34aace64b7e76df@mail.gmail.com>
References: <34f2770f0902111435n16a0bf1l8623281794918211@mail.gmail.com>
	<34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>
	<a7d6d2740902120253x4a9df39flffc66c4e8655d963@mail.gmail.com>
	<34f2770f0902120445l2ee22a24ie34aace64b7e76df@mail.gmail.com>
Message-ID: <a7d6d2740902120526oe1823e6u59274d1a9567bfbe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090212/3f56fe89/attachment.pl>

From weihanliu2002 at yahoo.com  Thu Feb 12 15:35:19 2009
From: weihanliu2002 at yahoo.com (Wei-han Liu)
Date: Thu, 12 Feb 2009 06:35:19 -0800 (PST)
Subject: [R-SIG-Finance] saddlepoint approximations with applications
Message-ID: <726978.5103.qm@web53509.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090212/6f403df1/attachment.pl>

From rory.winston at gmail.com  Thu Feb 12 16:17:38 2009
From: rory.winston at gmail.com (Rory Winston)
Date: Thu, 12 Feb 2009 15:17:38 +0000
Subject: [R-SIG-Finance] Question on multiple sessions...
Message-ID: <3f446aa30902120717j625ec428r34585cb25731b36@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090212/a7e1acaf/attachment.pl>

From edd at debian.org  Thu Feb 12 16:48:49 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 12 Feb 2009 09:48:49 -0600
Subject: [R-SIG-Finance] Question on multiple sessions...
In-Reply-To: <3f446aa30902120717j625ec428r34585cb25731b36@mail.gmail.com>
References: <3f446aa30902120717j625ec428r34585cb25731b36@mail.gmail.com>
Message-ID: <18836.17633.548110.255135@ron.nulle.part>


On 12 February 2009 at 15:17, Rory Winston wrote:
| This is an interesting scenario. It may be possible to use something like
| networkspaces to act as a distributed broker to contain the results of
| real-time updates and calculations, and access them concurrently from
| another session. I dont see any reason why this would not work.

A few years ago I gave a talk on 'sixteen different ways to Use R' which was
in the context of my Quantian live cdrom/dvd -- all those methods are
available on the live cdrom/dvd.  Networkspaces, Rserve, web solutions,
... are all part of it.  The slides from the talk are still on my
presentations page.

A newer entrant is the bigmemory package -- it permits one to load a
(potentially large object as e.g. a really big matrix with time, price, size,
and a numeric key for the symbol [ as it has to be of class matrix IIRC ] )
and to share that object across multiple R sessions on the same machine.  It
uses shared memory and proper locking mechanisms.

All of these methods skew to Linux.  One should not dismiss this -- the OP
wanted this in a server context, and R is almost perfectly OS-agnostic. In my
mind it makes sense to consider putting R on a real server which opens a
whole new set of possibilities (incl proper 64 bit support for oodles of
ram).  I am sure that it could all be done with Windows too in some way,
shape or form as Turing equivalence still holds. I'd simply argue that it is
easier to get done on Linux.

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From jeff.a.ryan at gmail.com  Thu Feb 12 17:04:02 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 12 Feb 2009 08:04:02 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Fwd: Question on multiple
	sessions...
In-Reply-To: <34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>
References: <34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>
Message-ID: <21979004.post@talk.nabble.com>


Within the context of IBrokers, the entire process of collecting data and
acting upon it can take place within one session.

The notion of needing this to be multi-threaded isn't quite accurate, at
least by my figuring.

I am working on a paper/tutorial to outline this whole process, but here is
what I have found to work.

R is single threaded.  The approach I initially envisioned did involve some
sort of dual R-process, where communication was via a file or sockets.  This
obviously wasn't ideal, but seemed workable.

Turns out that with sockets, *blocking* sockets in R are the only
reliable/efficient cross-platform mechanism to communicate that I have found
while keeping everything in R.  This causes some interesting problems if you
are waiting for messages from another process, and those messages are
irregular in time.  Essentially you are just shifting the issue from one
single-threaded process into two.  Same problems persist.

So, a quick primer on how IBrokers can manage this.

A request for data is sent via reqMktData.  The connection then blocks until
a message is received back from Interactive Brokers.  This is in the form of
a continuous stream of messages.  At each iteration/call of readBin [where
new messages are taken from the socket] the proper handling via callbacks
take place.

In the CRAN version I have two methods available to the user. One is where
custom handlers for each _type_ of data can be passed into the original
reqMktData call, e.g. on each new bid message that arrives, do *something*. 
The other original approach was to use a custom CALLBACK argument to manage
the entire message stream.

The CALLBACK basically handles _all_ possible messages, and with custom
action based on what is happening.

The newest version at  http://ibrokers.googlecode.com
http://ibrokers.googlecode.com  has a much better interface.  This new
version allows for a user defined actions to take place at each new message
(essentially a global level of processing), and within each different type
of message received.  By using special eWrapper closures you can enable data
to persist from one message to another, all in memory, all within one
session.

The obvious caveat here is that everything is non-interactive.  Which at
first glance seems a bit strange given most interaction with R is
interactive, but from an automated strategy perspective is just fine.

Again, I am working on some solid documentation related to IBrokers
specifically, but will also be publishing a white-paper on the general idea
of real-time processing in the near future at  http://www.insightalgo.com
http://www.insightalgo.com .

Jeff


I will be making a paper/presentation available with more details at some
point in the near future 

Vince Fulco wrote:
> 
> Assuming one wants to use an open session to collect real time data a
> la IBrokers or similar, what would be the best way to maintain another
> session for calculations?  Is RServe the answer or is there a lighter
> solution for semi-auto analytical script running?  Either Windows or
> Linux solutions would be appreciated.  TIA
> 
> 
> Vince Fulco, CFA, CAIA
> 612.424.5477 (universal)
> vfulco1 at gmail.com
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Fwd%3A-Question-on-multiple-sessions...-tp21966670p21979004.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From daniel.cegielka at gmail.com  Thu Feb 12 17:47:11 2009
From: daniel.cegielka at gmail.com (Daniel Cegielka)
Date: Thu, 12 Feb 2009 17:47:11 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Fwd: Question on multiple
	sessions...
In-Reply-To: <21979004.post@talk.nabble.com>
References: <34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>
	<21979004.post@talk.nabble.com>
Message-ID: <4994528F.7070503@gmail.com>

Jeff Ryan pisze:

> 
> R is single threaded.  The approach I initially envisioned did involve some
> sort of dual R-process, where communication was via a file or sockets.  This
> obviously wasn't ideal, but seemed workable.
> 
> Turns out that with sockets, *blocking* sockets in R are the only
> reliable/efficient cross-platform mechanism to communicate that I have found
> while keeping everything in R.  This causes some interesting problems if you
> are waiting for messages from another process, and those messages are
> irregular in time.  Essentially you are just shifting the issue from one
> single-threaded process into two.  Same problems persist.
> 

Hi Jeff ;)

Twisted (python framework) include non-blocking function...
It can be ported to R.

http://twistedmatrix.com/trac/wiki/FrequentlyAskedQuestions#HowdoIuseDeferredstomakemyblockingcodenon-blocking


daniel


From jeff.a.ryan at gmail.com  Thu Feb 12 17:54:55 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 12 Feb 2009 10:54:55 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Fwd: Question on multiple
	sessions...
In-Reply-To: <4994528F.7070503@gmail.com>
References: <34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>
	<21979004.post@talk.nabble.com> <4994528F.7070503@gmail.com>
Message-ID: <e8e755250902120854n13408106r4335878022a1f537@mail.gmail.com>

Daniel,

The issue isn't blocking vs. non-blocking, as R technically has that
as well, the problem is in reliability and performance, at least in my
experience.

readBin on *nix when non-blocking will return a character(0) (or
whatever you request) when _nothing_ is available to read.  That is
testable and logical.  On Windows (this is the point related to
cross-platform) it returns garbage.  No way to test/etc.  Additionally
you are limited by nul terminated strings in R's processing.  There
are workarounds, but they are just that.

This is all within the context of IBrokers mind you, other connections
will be different in terms of requirements.  IB uses simple
nul-terminated character messages.

You can of course write your own C code, use Java, or Python etc. But
that then has the multiprocess issue, and all its complexity/ugliness.

Given the correct logic I contend most everything is doable in R.  And
that is a very good thing.

Jeff

On Thu, Feb 12, 2009 at 10:47 AM, Daniel Cegielka
<daniel.cegielka at gmail.com> wrote:
> Jeff Ryan pisze:
>
>>
>> R is single threaded.  The approach I initially envisioned did involve some
>> sort of dual R-process, where communication was via a file or sockets.  This
>> obviously wasn't ideal, but seemed workable.
>>
>> Turns out that with sockets, *blocking* sockets in R are the only
>> reliable/efficient cross-platform mechanism to communicate that I have found
>> while keeping everything in R.  This causes some interesting problems if you
>> are waiting for messages from another process, and those messages are
>> irregular in time.  Essentially you are just shifting the issue from one
>> single-threaded process into two.  Same problems persist.
>>
>
> Hi Jeff ;)
>
> Twisted (python framework) include non-blocking function...
> It can be ported to R.
>
> http://twistedmatrix.com/trac/wiki/FrequentlyAskedQuestions#HowdoIuseDeferredstomakemyblockingcodenon-blocking
>
>
> daniel
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From daniel.cegielka at gmail.com  Thu Feb 12 18:07:03 2009
From: daniel.cegielka at gmail.com (Daniel Cegielka)
Date: Thu, 12 Feb 2009 18:07:03 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Fwd: Question on multiple
	sessions...
In-Reply-To: <e8e755250902120854n13408106r4335878022a1f537@mail.gmail.com>
References: <34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>	
	<21979004.post@talk.nabble.com> <4994528F.7070503@gmail.com>
	<e8e755250902120854n13408106r4335878022a1f537@mail.gmail.com>
Message-ID: <49945737.9020404@gmail.com>

Jeff Ryan pisze:

> 
> readBin on *nix when non-blocking will return a character(0) (or
> whatever you request) when _nothing_ is available to read.  That is
> testable and logical.  On Windows (this is the point related to
> cross-platform) it returns garbage.  No way to test/etc.  Additionally
> you are limited by nul terminated strings in R's processing.  There
> are workarounds, but they are just that.
> 
> This is all within the context of IBrokers mind you, other connections
> will be different in terms of requirements.  IB uses simple
> nul-terminated character messages.
> 


http://code.google.com/p/ibrokers/source/browse/trunk/R/processMsg.R

So if I understand this code...

daniel


From pzweden at gmail.com  Thu Feb 12 18:26:36 2009
From: pzweden at gmail.com (P vanzweden)
Date: Thu, 12 Feb 2009 18:26:36 +0100
Subject: [R-SIG-Finance] Fwd: Using dummy variables R
In-Reply-To: <931412660902120439r533f2a0cr518db132b5e8c65@mail.gmail.com>
References: <931412660902120439r533f2a0cr518db132b5e8c65@mail.gmail.com>
Message-ID: <931412660902120926v7c4ab3a1off1f996519bd8945@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090212/5ac833af/attachment.pl>

From jeff.a.ryan at gmail.com  Thu Feb 12 18:45:14 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 12 Feb 2009 11:45:14 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Fwd: Question on multiple
	sessions...
In-Reply-To: <49945737.9020404@gmail.com>
References: <34f2770f0902111517s61ee7ba9pb507c26b414d0cd0@mail.gmail.com>
	<21979004.post@talk.nabble.com> <4994528F.7070503@gmail.com>
	<e8e755250902120854n13408106r4335878022a1f537@mail.gmail.com>
	<49945737.9020404@gmail.com>
Message-ID: <e8e755250902120945v688bf498r57c67640567f326e@mail.gmail.com>

Again, in the IBrokers case:

twsCALLBACK is the main callback to the reqMktData request.

*processMsg* is the main routine to process each individual message
that comes in.

Within there are eWrapper$calls that handle the actual message
details, e.g. print to a file, do some processing, etc.

http://code.google.com/p/ibrokers/source/browse/trunk/R/eWrapper.R
http://code.google.com/p/ibrokers/source/browse/trunk/R/eWrapper.MktData.CSV.R

reqMktData => twsCALLBACK => processMsg => eWrapper$method [repeat in
while loop]

Using closures (as in the MktData.CSV version), or global variables,
you can keep track of last data or details that can be used for
real-time processing.

placeOrder is also available to _act_ on the processing as well.

Jeff


On Thu, Feb 12, 2009 at 11:07 AM, Daniel Cegielka
<daniel.cegielka at gmail.com> wrote:
> Jeff Ryan pisze:
>
>>
>> readBin on *nix when non-blocking will return a character(0) (or
>> whatever you request) when _nothing_ is available to read.  That is
>> testable and logical.  On Windows (this is the point related to
>> cross-platform) it returns garbage.  No way to test/etc.  Additionally
>> you are limited by nul terminated strings in R's processing.  There
>> are workarounds, but they are just that.
>>
>> This is all within the context of IBrokers mind you, other connections
>> will be different in terms of requirements.  IB uses simple
>> nul-terminated character messages.
>>
>
>
> http://code.google.com/p/ibrokers/source/browse/trunk/R/processMsg.R
>
> So if I understand this code...
>
> daniel
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From markleeds at verizon.net  Thu Feb 12 19:36:37 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 12 Feb 2009 12:36:37 -0600 (CST)
Subject: [R-SIG-Finance] Fwd: Using dummy variables R
Message-ID: <267257332.8682941234463797203.JavaMail.javamailuser@localhost>

hi: suppose you had just sectors so it was a one way anova : then if you 
make the sectors factors rather than dummies, then, the default 
contrasts for R are contr.sum and this means that your design matrix 
will automatically make one of your dummies the average response
and in this way get rid of the implicit multicollinearity in the design 
matrix.

john fox's text and car book get into this topic in more detail. I have 
found it complicated and, since you have a two way anova, it's even more 
complicated. but the key thing to do is change your dummies to factors 
and let R do the work of making the design matrix full rank.

Also, I would send your question to R-help because there are people over 
there ( who may not be on R-Sig-Finance )  who can say a lot about your 
problem.

 
Mark




On Thu, Feb 12, 2009 at 12:26 PM, P vanzweden wrote:

> Dear R-helpers,
>
>
>
> I have the following model: Return ~ intercept+ EU + AUS + ASIA + USA 
> +
> office +
> retail + industrial + residential.
>
> The model consist of an intercept ,region- and sector - dummies.
>
>
>
> When including all the dummies and the intercept, I will be caught in 
> a so
> called "dummy variable trap". This problem is a perfect 
> multicollinearity;
> because the regression is not solvable since the X matrix is not fully
> ranked.
>
>
>
> To estimate the model that includes all dummies variables and the 
> intercept,
> I need to imposes two restriction that the sum of the region 
> parameters is
> zero and the sum of the sector parameters is zero.
>
>
>
> How can I estimate this restricted linear regression model in  R?
>
>
> Is this possible to do this in the lm framework?
>
> Thanks in advance,
>
> Peter
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From station.ning at gmail.com  Thu Feb 12 22:55:07 2009
From: station.ning at gmail.com (ning zhang)
Date: Thu, 12 Feb 2009 21:55:07 +0000
Subject: [R-SIG-Finance] R package update problem at Company's PC
Message-ID: <9519fd9f0902121355i2fae2a17l5ca35f693b60ae57@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090212/70b2c862/attachment.pl>

From josh.m.ulrich at gmail.com  Thu Feb 12 23:09:21 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Thu, 12 Feb 2009 16:09:21 -0600
Subject: [R-SIG-Finance] R package update problem at Company's PC
In-Reply-To: <9519fd9f0902121355i2fae2a17l5ca35f693b60ae57@mail.gmail.com>
References: <9519fd9f0902121355i2fae2a17l5ca35f693b60ae57@mail.gmail.com>
Message-ID: <8cca69990902121409t5076b10vf0ebc18e89e6f780@mail.gmail.com>

Hi,

This list is for finance-specific questions.  You're more likely to
get help with this question via the R-help mailing list.

Josh
--
http://www.fosstrading.com



On Thu, Feb 12, 2009 at 3:55 PM, ning zhang <station.ning at gmail.com> wrote:
> Hi R-helpers
>
> I got problem when trying updating R package at company's PC. The error
> information shows that it failed to access the CRAN mirror via Port 80.
> I guess it is due to the firewall or permission issues. I wonder if anyone
> has met same problem. How do you solve it?  Change environment path?
>
>
> Appreciate your helps.
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From markleeds at verizon.net  Thu Feb 12 23:30:43 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 12 Feb 2009 16:30:43 -0600 (CST)
Subject: [R-SIG-Finance] dummy variables in regression
Message-ID: <256697148.10908351234477843059.JavaMail.javamailuser@localhost>

I don't remember what I said in my previous message but I was playing 
with something
else and realize that, if I said contr.sum, that was wrong with respect 
to their being a zeroed
out effect for the baseline. the contrasts that do that are 
contr.treatment and they are
the default in R. I always get this stuff backwards.


From patrick at burns-stat.com  Fri Feb 13 09:30:42 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Fri, 13 Feb 2009 08:30:42 +0000
Subject: [R-SIG-Finance] Fwd: Using dummy variables R
In-Reply-To: <931412660902120926v7c4ab3a1off1f996519bd8945@mail.gmail.com>
References: <931412660902120439r533f2a0cr518db132b5e8c65@mail.gmail.com>
	<931412660902120926v7c4ab3a1off1f996519bd8945@mail.gmail.com>
Message-ID: <49952FB2.7040802@burns-stat.com>

If you give 'lm' factors representing your
variables instead of dummy variables, then
it will take care of the identification problem
itself.  That's much easier all around.  Your
only problem -- if you care -- is understanding
the meaning of the individual coefficients.  But
I doubt that is a very big problem.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of "The R Inferno" and "A Guide for the Unwilling S User")

P vanzweden wrote:
>  Dear R-helpers,
>
>
>
> I have the following model: Return ~ intercept+ EU + AUS + ASIA + USA +
> office +
> retail + industrial + residential.
>
> The model consist of an intercept ,region- and sector - dummies.
>
>
>
> When including all the dummies and the intercept, I will be caught in a so
> called "dummy variable trap". This problem is a perfect multicollinearity;
> because the regression is not solvable since the X matrix is not fully
> ranked.
>
>
>
> To estimate the model that includes all dummies variables and the intercept,
> I need to imposes two restriction that the sum of the region parameters is
> zero and the sum of the sector parameters is zero.
>
>
>
> How can I estimate this restricted linear regression model in  R?
>
>
> Is this possible to do this in the lm framework?
>
> Thanks in advance,
>
> Peter
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
>


From andyzhu35 at yahoo.com  Fri Feb 13 11:16:16 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Fri, 13 Feb 2009 02:16:16 -0800 (PST)
Subject: [R-SIG-Finance] R package update problem at Company's PC
In-Reply-To: <9519fd9f0902121355i2fae2a17l5ca35f693b60ae57@mail.gmail.com>
Message-ID: <167694.54021.qm@web56202.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090213/25cae488/attachment.pl>

From edd at debian.org  Fri Feb 13 15:10:29 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 13 Feb 2009 08:10:29 -0600
Subject: [R-SIG-Finance] R package update problem at Company's PC
In-Reply-To: <167694.54021.qm@web56202.mail.re3.yahoo.com>
References: <9519fd9f0902121355i2fae2a17l5ca35f693b60ae57@mail.gmail.com>
	<167694.54021.qm@web56202.mail.re3.yahoo.com>
Message-ID: <18837.32597.146337.287732@ron.nulle.part>


Can we please stop this thread?  Josh was spot on when he re-directed this to
r-help.

On 13 February 2009 at 02:16, Andy Zhu wrote:
| download the source code and untar it. Then
| 
| R CMD INSTALL xxx
| 
| you may need a set of tools in installation. The tools usually are ready in Unix. In windows, you may need MinGW for those tools.

Yes sure you _could_ recompile it but part of the appeal of using Windows is
that you don't have to.  The original poster has a problem with his proxy
server, which is (IIRC) in the R FAQ and has been discussed a hundred times
at least on r-help.  Not a topic for this list, so let's just freeze it out.

Let's all make an effort to keep this list on-topic.

Cheers,  Dirk

| --- On Thu, 2/12/09, ning zhang <station.ning at gmail.com> wrote:
| From: ning zhang <station.ning at gmail.com>
| Subject: [R-SIG-Finance] R package update problem at Company's PC
| To: r-sig-finance at stat.math.ethz.ch
| Date: Thursday, February 12, 2009, 4:55 PM
| 
| Hi R-helpers
| 
| I got problem when trying updating R package at company's PC. The error
| information shows that it failed to access the CRAN mirror via Port 80.
| I guess it is due to the firewall or permission issues. I wonder if anyone
| has met same problem. How do you solve it?  Change environment path?
| 
| 
| Appreciate your helps.
| 
| 	[[alternative HTML version deleted]]
| 
| _______________________________________________
| R-SIG-Finance at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-sig-finance
| -- Subscriber-posting only.
| -- If you want to post, subscribe first.
| 
| 
| 
|       
| 	[[alternative HTML version deleted]]
| 
| _______________________________________________
| R-SIG-Finance at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-sig-finance
| -- Subscriber-posting only.
| -- If you want to post, subscribe first.

-- 
Three out of two people have difficulties with fractions.


From Achim.Zeileis at wu-wien.ac.at  Fri Feb 13 21:45:57 2009
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 13 Feb 2009 21:45:57 +0100 (CET)
Subject: [R-SIG-Finance] useR! 2009: finance/econometrics submissions
Message-ID: <Pine.LNX.4.64.0902132139350.14079@paninaro.stat-math.wu-wien.ac.at>

Dear useRs,

this is just a brief reminder that the submission deadline for this year's 
useR! conference is in two weeks:

   useR! 2009 - The R User Conference
   Location: Rennes, France
   Date: 2009-07-08 to 2009-07-10
   Submission Deadline: 2009-02-27
   URL: http://www.R-project.org/useR-2009/

It would be great if we would have many submissions again from the 
contributors to this list so that there are again exciting sessions about 
using R for finance, econometrics, and other related applications.

If you want to contribute, just submit your abstract via the conference 
Web page.

Hope to see many of you in France this summer!
Best wishes,
Z


From unixunix99 at gmail.com  Sun Feb 15 17:47:46 2009
From: unixunix99 at gmail.com (Unixunix99@gmail.com)
Date: Sun, 15 Feb 2009 11:47:46 -0500
Subject: [R-SIG-Finance] How to fit GARCH(1,
	1) with targeted unconditional variance?
Message-ID: <67a7e4210902150847w5645820ck30b65d951028f10e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090215/a24e28ed/attachment.pl>

From josh.m.ulrich at gmail.com  Mon Feb 16 01:50:50 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Sun, 15 Feb 2009 18:50:50 -0600
Subject: [R-SIG-Finance] Package Update: TTR_0.2 now on CRAN
Message-ID: <8cca69990902151650w466f36a7g17514f030fd61b90@mail.gmail.com>

Dear R-Finance UseRs,

I am happy to announce a long-overdue update to the TTR package
(version 0.2) is now on CRAN.

This update represents a major milestone, as TTR useRs are no longer
restricted to using matrix objects.  TTR 0.2 uses xts internally, so
all major time series classes are now supported.

NEW FEATURES:

- Added the zig zag indicator: ZigZag()

- Added volatility estimators/indicators: volatility(), with the
following calculations
  - Close-to-Close
  - Garman Klass
  - Parkinson
  - Rogers Satchell

- Added Money Flow Index: MFI()

- Added Donchian channel: DonchianChannel()


CHANGES:

- All functions now use xts internally, adding support for all major
time series classes.  If try.xts() fails on the input object(s), they
will be converted to a matrix and a matrix object will be returned.

- Added 'bounded' arg to stoch() and SMI(), which includes the current
period in the calculation.

- Added naCheck() and implemented it in the moving average functions.

- Moved maType argument default values from function formals to
function body for the following functions:
    ADX, ATR, CCI, DPO, EMV, KST, MACD, RSI, TRIX, BBands,
chaikinVolatility, stoch, SMI

- momentum() in CMO() no longer sets na=100

- Replaced 'na' argument in momentum() and ROC() with 'na.pad'

- Added 'multiple' argument to TDI(), allowing more user control

- getYahooData() now returns an xts object

- Added colnames to output for ADX, EMV, and CLV (for xts)

- Added unit tests using the RUnit package
  - Used checkEquals on object attributes as well as values

- Removed .First.lib function and added .onLoad with package version.


BUG FIXES:

- Corrected NaN replacement in CLV()

- Corrected williamsAD(): AD=0 if C(t)=C(t-1)

- Corrected runMedian() and runMAD().  The argument controlling which
type of median to calculate for even-numbered samples wasn't being
passed to the Fortran routine.

- aroon() calculation starts at period n+1, instead of n

- Added NA to first element of closeLag of ATR()

- Corrected BBands() and CCI() for rowMeans use on xts objects

- Made changes to Rd files to pass R CMD check on R-devel (2.9.0)


Please do contact me with any questions, concerns, bug reports, etc.

Thank you,
Josh Ulrich
--
http://quantemplation.blogspot.com
http://www.fosstrading.com/


From shimrit.sabraham at gmail.com  Mon Feb 16 12:17:06 2009
From: shimrit.sabraham at gmail.com (Shimrit Abraham)
Date: Mon, 16 Feb 2009 12:17:06 +0100
Subject: [R-SIG-Finance] efficient sandwich matrix multiplication and
	determinant
Message-ID: <36da694c0902160317m49837182sfa3823ecbd1d7951@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090216/9c3bc478/attachment.pl>

From helenar at gmx.de  Mon Feb 16 17:55:05 2009
From: helenar at gmx.de (Helena Richter)
Date: Mon, 16 Feb 2009 17:55:05 +0100
Subject: [R-SIG-Finance] odd GARCH(1,1) results
Message-ID: <49999A68.4080703@gmx.de>

Hi everybody,

I'm trying to fit a Garch(1,1) process to the DAX returns. My data 
consists of about 2300 10day-logreturns in chronologically descending 
order (see attachment). But if I use the garch function I get a very 
high alpha_1 and a quite low beta, which doesn't make that much sense. I 
think I am missing something, but have no idea what it might be. I'd 
appreciate it a lot if someone could have a look at the output I posted 
at the end of this mail. Maybe there's something an experienced user 
might see at once. I also tried the garchFit function with nearly the 
same results.
I'm very thankful for every answer. Please excuse my bad english.
Helena


 > g2005out<-garch(g2005,order=c(1,1))

***** ESTIMATION WITH ANALYTICAL GRADIENT *****


I INITIAL X(I) D(I)

1 2.214508e-03 1.000e+00
2 5.000000e-02 1.000e+00
3 5.000000e-02 1.000e+00

IT NF F RELDF PRELDF RELDX STPPAR D*STEP NPRELDF
0 1 -5.974e+03
1 6 -5.998e+03 3.91e-03 5.51e-03 3.6e-03 2.5e+08 3.6e-04 6.90e+05
2 7 -6.000e+03 3.19e-04 6.17e-04 3.1e-03 2.0e+00 3.6e-04 5.50e+02
3 8 -6.001e+03 2.76e-04 2.98e-04 3.5e-03 2.0e+00 3.6e-04 5.21e+02
4 13 -6.161e+03 2.59e-02 3.79e-02 4.8e-01 2.0e+00 9.3e-02 5.07e+02
5 21 -6.182e+03 3.39e-03 7.77e-03 1.1e-03 3.6e+00 3.2e-04 4.87e-01
6 22 -6.182e+03 9.68e-05 7.28e-05 1.1e-03 2.0e+00 3.2e-04 2.11e+00
7 23 -6.183e+03 2.15e-05 2.25e-05 1.1e-03 2.0e+00 3.2e-04 2.37e+00
8 29 -6.213e+03 4.86e-03 4.50e-03 5.7e-01 2.0e+00 1.6e-01 2.34e+00
9 31 -6.278e+03 1.03e-02 1.04e-02 4.3e-01 2.0e+00 3.2e-01 1.65e+02
10 33 -6.301e+03 3.67e-03 5.01e-03 2.9e-02 1.9e+00 3.2e-02 7.66e-02
11 36 -6.347e+03 7.26e-03 7.63e-03 1.0e-01 1.3e+00 1.3e-01 1.06e-01
12 37 -6.399e+03 8.17e-03 9.09e-03 2.1e-01 7.0e-01 2.6e-01 1.33e-02
13 39 -6.412e+03 2.04e-03 2.19e-03 2.6e-02 1.8e+00 2.6e-02 2.18e-02
14 41 -6.437e+03 3.80e-03 5.84e-03 9.4e-02 1.1e+00 1.0e-01 1.80e-02
15 43 -6.498e+03 9.44e-03 8.74e-03 2.9e-01 8.5e-02 3.2e-01 8.77e-03
16 44 -6.518e+03 3.07e-03 2.11e-03 9.0e-02 0.0e+00 8.9e-02 2.11e-03
17 45 -6.527e+03 1.38e-03 1.07e-03 7.2e-02 0.0e+00 8.3e-02 1.07e-03
18 46 -6.530e+03 4.03e-04 3.24e-04 4.9e-02 0.0e+00 7.6e-02 3.24e-04
19 47 -6.530e+03 6.83e-05 7.29e-05 1.7e-02 0.0e+00 2.4e-02 7.29e-05
20 48 -6.530e+03 5.26e-06 6.29e-06 6.2e-03 0.0e+00 1.2e-02 6.29e-06
21 49 -6.530e+03 1.53e-07 1.54e-07 9.0e-04 0.0e+00 1.7e-03 1.54e-07
22 50 -6.530e+03 1.33e-10 1.22e-10 2.3e-05 0.0e+00 3.3e-05 1.22e-10
23 51 -6.530e+03 2.08e-12 6.94e-12 3.9e-06 0.0e+00 6.0e-06 6.94e-12

***** RELATIVE FUNCTION CONVERGENCE *****

FUNCTION -6.530104e+03 RELDX 3.860e-06
FUNC. EVALS 51 GRAD. EVALS 24
PRELDF 6.944e-12 NPRELDF 6.944e-12

I FINAL X(I) D(I) G(I)

1 2.041120e-04 1.000e+00 4.584e-01
2 7.096202e-01 1.000e+00 2.579e-04
3 2.487274e-01 1.000e+00 3.097e-04

 > summary(g2005out)

Call:
garch(x = g2005, order = c(1, 1))

Model:
GARCH(1,1)

Residuals:
Min 1Q Median 3Q Max
-4.1857 -0.6978 0.3268 0.9039 4.9820

Coefficient(s):
Estimate Std. Error t value Pr(>|t|)
a0 2.041e-04 2.072e-05 9.849 <2e-16 ***
a1 7.096e-01 5.780e-02 12.277 <2e-16 ***
b1 2.487e-01 2.062e-02 12.060 <2e-16 ***
---
Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Diagnostic Tests:
Jarque Bera Test

data: Residuals
X-squared = 33.1741, df = 2, p-value = 6.257e-08


Box-Ljung test

data: Squared.Residuals
X-squared = 8.9147, df = 1, p-value = 0.002829





-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Garch.csv
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090216/b67503c4/attachment.pl>

From brian at braverock.com  Mon Feb 16 18:39:46 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 16 Feb 2009 11:39:46 -0600
Subject: [R-SIG-Finance] efficient sandwich matrix multiplication and
 determinant
In-Reply-To: <36da694c0902160317m49837182sfa3823ecbd1d7951@mail.gmail.com>
References: <36da694c0902160317m49837182sfa3823ecbd1d7951@mail.gmail.com>
Message-ID: <4999A4E2.8010505@braverock.com>

Shimrit Abraham wrote:
> I am looking for two ways to speed up my computations:
>
> 1. Is there a function that efficiently computes the 'sandwich product' of
> three matrices, say, ZPZ'
> 2. Is there a function that efficiently computes the determinant of a
> positive definite symmetric matrix?
>   

Shimrit,

You are likely to get a more complete answer on r-help than here.  This 
list is for questions which are directly related to finance.  The 
matrrix-math wizards hang out on r-help.

Regards,

    - Brian


From Zeno.Adams at ebs.edu  Mon Feb 16 22:20:24 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Mon, 16 Feb 2009 22:20:24 +0100
Subject: [R-SIG-Finance] odd GARCH(1,1) results
References: <49999A68.4080703@gmx.de>
Message-ID: <9064522880125945B98983BBAECBA1CC21CB94@exchsrv001.ebs.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090216/8d7e4e8d/attachment.pl>

From ezivot at u.washington.edu  Mon Feb 16 22:50:39 2009
From: ezivot at u.washington.edu (Eric Zivot)
Date: Mon, 16 Feb 2009 13:50:39 -0800
Subject: [R-SIG-Finance] odd GARCH(1,1) results
In-Reply-To: <9064522880125945B98983BBAECBA1CC21CB94@exchsrv001.ebs.local>
Message-ID: <200902162150.n1GLodAb020041@smtp.washington.edu>

Everything doesn't have to be a GARCH(1,1) model. Look at the
autocorrelations of the squared returns. It could be that a pure ARCH
process is most appropriate for this series. The GARCH(1,1) model gives
reasonably slow mean reversion.  

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Adams, Zeno
Sent: Monday, February 16, 2009 1:20 PM
To: Helena Richter; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] odd GARCH(1,1) results


I checked your data using different model specifications and options in
EViews. It seems that the parameters have been estimated correctly
(different error distributions, optimization algorithms or asymmetric
parameters do not change the estimated parameter values by much). I think
the line graph supports the model parameters: the data is mainly driven by
shocks (high alpha) but the shocks are not very persistent (low beta). I
know that the DAX series normally produces parameter estimates of around
0.05 for alpha and 0.9 for beta using daily data but you may have just
picked an awkward period (although 2300 is pretty long). 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of Helena Richter
Sent: Mon 2/16/2009 5:55 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] odd GARCH(1,1) results
 
Hi everybody,

I'm trying to fit a Garch(1,1) process to the DAX returns. My data consists
of about 2300 10day-logreturns in chronologically descending order (see
attachment). But if I use the garch function I get a very high alpha_1 and a
quite low beta, which doesn't make that much sense. I think I am missing
something, but have no idea what it might be. I'd appreciate it a lot if
someone could have a look at the output I posted at the end of this mail.
Maybe there's something an experienced user might see at once. I also tried
the garchFit function with nearly the same results.
I'm very thankful for every answer. Please excuse my bad english.
Helena


 > g2005out<-garch(g2005,order=c(1,1))

***** ESTIMATION WITH ANALYTICAL GRADIENT *****


I INITIAL X(I) D(I)

1 2.214508e-03 1.000e+00
2 5.000000e-02 1.000e+00
3 5.000000e-02 1.000e+00

IT NF F RELDF PRELDF RELDX STPPAR D*STEP NPRELDF 0 1 -5.974e+03
1 6 -5.998e+03 3.91e-03 5.51e-03 3.6e-03 2.5e+08 3.6e-04 6.90e+05
2 7 -6.000e+03 3.19e-04 6.17e-04 3.1e-03 2.0e+00 3.6e-04 5.50e+02
3 8 -6.001e+03 2.76e-04 2.98e-04 3.5e-03 2.0e+00 3.6e-04 5.21e+02
4 13 -6.161e+03 2.59e-02 3.79e-02 4.8e-01 2.0e+00 9.3e-02 5.07e+02
5 21 -6.182e+03 3.39e-03 7.77e-03 1.1e-03 3.6e+00 3.2e-04 4.87e-01
6 22 -6.182e+03 9.68e-05 7.28e-05 1.1e-03 2.0e+00 3.2e-04 2.11e+00
7 23 -6.183e+03 2.15e-05 2.25e-05 1.1e-03 2.0e+00 3.2e-04 2.37e+00
8 29 -6.213e+03 4.86e-03 4.50e-03 5.7e-01 2.0e+00 1.6e-01 2.34e+00
9 31 -6.278e+03 1.03e-02 1.04e-02 4.3e-01 2.0e+00 3.2e-01 1.65e+02 10 33
-6.301e+03 3.67e-03 5.01e-03 2.9e-02 1.9e+00 3.2e-02 7.66e-02
11 36 -6.347e+03 7.26e-03 7.63e-03 1.0e-01 1.3e+00 1.3e-01 1.06e-01
12 37 -6.399e+03 8.17e-03 9.09e-03 2.1e-01 7.0e-01 2.6e-01 1.33e-02
13 39 -6.412e+03 2.04e-03 2.19e-03 2.6e-02 1.8e+00 2.6e-02 2.18e-02
14 41 -6.437e+03 3.80e-03 5.84e-03 9.4e-02 1.1e+00 1.0e-01 1.80e-02
15 43 -6.498e+03 9.44e-03 8.74e-03 2.9e-01 8.5e-02 3.2e-01 8.77e-03
16 44 -6.518e+03 3.07e-03 2.11e-03 9.0e-02 0.0e+00 8.9e-02 2.11e-03
17 45 -6.527e+03 1.38e-03 1.07e-03 7.2e-02 0.0e+00 8.3e-02 1.07e-03
18 46 -6.530e+03 4.03e-04 3.24e-04 4.9e-02 0.0e+00 7.6e-02 3.24e-04
19 47 -6.530e+03 6.83e-05 7.29e-05 1.7e-02 0.0e+00 2.4e-02 7.29e-05 20 48
-6.530e+03 5.26e-06 6.29e-06 6.2e-03 0.0e+00 1.2e-02 6.29e-06
21 49 -6.530e+03 1.53e-07 1.54e-07 9.0e-04 0.0e+00 1.7e-03 1.54e-07
22 50 -6.530e+03 1.33e-10 1.22e-10 2.3e-05 0.0e+00 3.3e-05 1.22e-10
23 51 -6.530e+03 2.08e-12 6.94e-12 3.9e-06 0.0e+00 6.0e-06 6.94e-12

***** RELATIVE FUNCTION CONVERGENCE *****

FUNCTION -6.530104e+03 RELDX 3.860e-06
FUNC. EVALS 51 GRAD. EVALS 24
PRELDF 6.944e-12 NPRELDF 6.944e-12

I FINAL X(I) D(I) G(I)

1 2.041120e-04 1.000e+00 4.584e-01
2 7.096202e-01 1.000e+00 2.579e-04
3 2.487274e-01 1.000e+00 3.097e-04

 > summary(g2005out)

Call:
garch(x = g2005, order = c(1, 1))

Model:
GARCH(1,1)

Residuals:
Min 1Q Median 3Q Max
-4.1857 -0.6978 0.3268 0.9039 4.9820

Coefficient(s):
Estimate Std. Error t value Pr(>|t|)
a0 2.041e-04 2.072e-05 9.849 <2e-16 ***
a1 7.096e-01 5.780e-02 12.277 <2e-16 ***
b1 2.487e-01 2.062e-02 12.060 <2e-16 ***
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Diagnostic Tests:
Jarque Bera Test

data: Residuals
X-squared = 33.1741, df = 2, p-value = 6.257e-08


Box-Ljung test

data: Squared.Residuals
X-squared = 8.9147, df = 1, p-value = 0.002829








EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft:
Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213
Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Rektor/CEO; Dr. Reimar
Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Verwaltungsrat: Dr. Hellmut K.
Albrecht, Vorsitzender
	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From marcolondonuk at googlemail.com  Tue Feb 17 14:15:36 2009
From: marcolondonuk at googlemail.com (Marco Bianchi)
Date: Tue, 17 Feb 2009 13:15:36 +0000
Subject: [R-SIG-Finance] Bloomberg chart window does not stay on the screen
	when working with R
Message-ID: <531b284b0902170515qa3dee65w7ce3e26102964551@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090217/63e2710b/attachment.pl>

From Reena.Bansal at moorecap.com  Tue Feb 17 16:14:59 2009
From: Reena.Bansal at moorecap.com (Reena Bansal)
Date: Tue, 17 Feb 2009 10:14:59 -0500
Subject: [R-SIG-Finance] Checking fit of data against student t distribution
Message-ID: <4AAD56F399C8564C9EB6817C17618CDD0251E80C@NYC-XCH3.win.moorecap.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090217/7f7e1ebb/attachment.pl>

From cgb at datanalytics.com  Tue Feb 17 17:04:44 2009
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Tue, 17 Feb 2009 17:04:44 +0100
Subject: [R-SIG-Finance] Checking fit of data against student t
 distribution
In-Reply-To: <4AAD56F399C8564C9EB6817C17618CDD0251E80C@NYC-XCH3.win.moorecap.com>
References: <4AAD56F399C8564C9EB6817C17618CDD0251E80C@NYC-XCH3.win.moorecap.com>
Message-ID: <1234886684.13669.25.camel@kropotkin>

?ks.test

Read the help carefully and you will find the answer to your question.

Best regards,

Carlos J. Gil Bellosta
http://www.datanalytics.com

On Tue, 2009-02-17 at 10:14 -0500, Reena Bansal wrote:
> Hi All -
> 
> I want to check how well a given data (sample size = 250) fits Normality
> and Student t-distribution with given degrees of freedom, say 5. For
> normality test I use Jarque Bera. I am trying to find test for checking
> my data against Student-t distribution. Any suggestions?
> 
> Thanks,
> Reena
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From matthewcleggphd at gmail.com  Tue Feb 17 18:38:44 2009
From: matthewcleggphd at gmail.com (Matthew Clegg)
Date: Tue, 17 Feb 2009 12:38:44 -0500
Subject: [R-SIG-Finance] Checking fit of data against student t
	distribution
In-Reply-To: <4AAD56F399C8564C9EB6817C17618CDD0251E80C@NYC-XCH3.win.moorecap.com>
References: <4AAD56F399C8564C9EB6817C17618CDD0251E80C@NYC-XCH3.win.moorecap.com>
Message-ID: <e199b8e70902170938i4e1dbb44t7ade0a1fd66c94fb@mail.gmail.com>

The Kolmogorov-Smirnov test is a great approach, but
unfortunately it is not always appropriate:

> X <- rt(250, df=5)  # Your data goes here
> ks.test(X, pt, df=5)

If the parameters have been estimated from the data, then
ks.test can give inflated p-values.  With financial data, it is common
to estimate the mean and the scale parameter (and possibly the
degrees of freedom) from the data, so the Kolmogorov-Smirnov test
may not be the best choice.

One possible workaround is to use bootstrapping.  Here is
a paper that describes that approach:

Stute, Winfried, Wenceslao Gonz?les Manteiga, and Manuel Presedo Quindmil,
1992, "Bootstrap based goodness-of-fit tests," Metrika, Vol. 40, No.
1, pp. 243-256

I have found that this can be computationally expensive, but
for your case -- fitting a t-distribution to 250 data points -- it's
certainly feasible.

Another traditional approach is to use the chi-square test of
goodness of fit.  This is described in many statistics textbooks,
for example, DeGroot and Schervish, 3rd ed., pp. 536-541.
Here is some sample R code showing how you might do this:

> library(fGarch)
> X <- rstd(250, mean=0.05, sd=0.10, nu=5)  # Replace X with your data
> theta <- stdFit(X)
> ncells <- 20
> quantiles <- qstd((1:(ncells-1))/ncells, mean=theta$par[1], sd=theta$par[2], nu=theta$par[3])
> quantiles <- c(-Inf, quantiles, Inf)
> obs <- table(cut(X, quantiles))
> exp <- length(X) / ncells
> p.chisq <- 1 - pchisq(sum((obs-exp)^2/exp), df=ncells-1-3)
> p.chisq

Note that the degrees of freedom used in the call to pchisq is
reduced by the number of parameters estimated.  Also, the
number of cells used for the test is somewhat arbitrary, but 20
seems to be a common value.

I'm sure you can easily code this up as an R function, if someone
hasn't done it already.

Perhaps one of the statisticians on the list can offer further guidance.

I hope this helps!

Matthew Clegg

On Tue, Feb 17, 2009 at 10:14 AM, Reena Bansal
<Reena.Bansal at moorecap.com> wrote:
> Hi All -
>
> I want to check how well a given data (sample size = 250) fits Normality
> and Student t-distribution with given degrees of freedom, say 5. For
> normality test I use Jarque Bera. I am trying to find test for checking
> my data against Student-t distribution. Any suggestions?
>
> Thanks,
> Reena
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

-- 
Matt Clegg
matthewcleggphd at gmail.com


From ts3665 at gmail.com  Tue Feb 17 20:24:27 2009
From: ts3665 at gmail.com (Tom Smythe)
Date: Tue, 17 Feb 2009 14:24:27 -0500
Subject: [R-SIG-Finance] Generating Data for Portfolio Simulation
Message-ID: <e8460120902171124i42b7eca8j24d3066fcd9625ad@mail.gmail.com>

Hi All,

I am looking for a way to generate generate M snapshots of return data
for all securities in an N security universe.  The return for the n-th
security at the m-th snapshot should come from a simple factor model
such that
r[n,m] = alpha[n] + beta[n] * rm[m] + epsilon[n,m],
where rm[m] is the market return, and epsilon[n,m] is noise with zero
mean and given covariance (more on this below).

The issue that I am facing is that I want the market return to be the
weighted sum of the individual security returns.  That is
rm[m] = sum_n b[n] r[n,m]
where b[n] is the ratio of the n-th security's market capitalization
to the total market capitalization, and rm[m] has a given (time
series) mean and variance.

Some other constraints of the problem are that the weighted alpha is zero
sum_n b[n] alpha[n] = 0
and the weighted beta is one
sum_n b[n] beta[n] = 1.
It would be nice if the noise term epsilon[n,m] had zero column (i.e.,
cross-sectional) mean and zero row (i.e., time series) mean; but both
of these requirements cannot be met simultaneously because they force
the covariance matrix to be singular.  For simplicity, I'd like to
assume that the covariance is non-singular and diagonal, with known
constant diagonal elements.

This appears to be a circular problem.  I seem to be unable to specify
r[n,m] without rm[m], and vice versa.

Can anybody advise me how to generate such factor-model data that is
compatible with the given market return statistics and given noise
covariance matrix?  Unless I'm missing something silly, this seems to
be an under-determined problem (if M >> N), with no unique solution.
However, in my case, any non-trivial but internally consistent
solution will do.

Thanks.


From shimrit.sabraham at gmail.com  Tue Feb 17 20:52:50 2009
From: shimrit.sabraham at gmail.com (Shimrit Abraham)
Date: Tue, 17 Feb 2009 20:52:50 +0100
Subject: [R-SIG-Finance] Efficient Kalman Filter
Message-ID: <36da694c0902171152u1b62ee21o19aacf8285fe29c8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090217/ed7c090c/attachment.pl>

From rkevinburton at charter.net  Tue Feb 17 20:59:49 2009
From: rkevinburton at charter.net (rkevinburton at charter.net)
Date: Tue, 17 Feb 2009 11:59:49 -0800
Subject: [R-SIG-Finance] efficient sandwich matrix multiplication and
 determinant
In-Reply-To: <4999A4E2.8010505@braverock.com>
Message-ID: <20090217145949.MN890.900197.root@mp20>

I know chol2inv will invert a positive definite symmetric matrix and the underlying fortran code will optionally compute the determinant but I have been unable to find an 'R' hook into just the determinant part.

Kevin

---- "Brian G. Peterson" <brian at braverock.com> wrote: 
> Shimrit Abraham wrote:
> > I am looking for two ways to speed up my computations:
> >
> > 1. Is there a function that efficiently computes the 'sandwich product' of
> > three matrices, say, ZPZ'
> > 2. Is there a function that efficiently computes the determinant of a
> > positive definite symmetric matrix?
> >   
> 
> Shimrit,
> 
> You are likely to get a more complete answer on r-help than here.  This 
> list is for questions which are directly related to finance.  The 
> matrrix-math wizards hang out on r-help.
> 
> Regards,
> 
>     - Brian
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From shimrit.sabraham at gmail.com  Tue Feb 17 21:12:02 2009
From: shimrit.sabraham at gmail.com (Shimrit Abraham)
Date: Tue, 17 Feb 2009 21:12:02 +0100
Subject: [R-SIG-Finance] efficient sandwich matrix multiplication and
	determinant
In-Reply-To: <20090217145949.MN890.900197.root@mp20>
References: <4999A4E2.8010505@braverock.com>
	<20090217145949.MN890.900197.root@mp20>
Message-ID: <36da694c0902171212q21e2fb5csd5d51f26ddc1ce9c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090217/5eb73187/attachment.pl>

From B_Rowe at ml.com  Tue Feb 17 21:38:25 2009
From: B_Rowe at ml.com (Rowe, Brian Lee Yung (Portfolio Analytics))
Date: Tue, 17 Feb 2009 15:38:25 -0500
Subject: [R-SIG-Finance] Generating Data for Portfolio Simulation
In-Reply-To: <e8460120902171124i42b7eca8j24d3066fcd9625ad@mail.gmail.com>
Message-ID: <3BAD818D9407B043817CC6D89ABA14EC032B830C@MLNYC20MB051.amrs.win.ml.com>

Hi Tom,

>From your construction of the problem, you do have a circular
dependency. Your simple factor model is basically CAPM, and if you think
about it from that perspective then you should generate the market
returns independently of your factor model, since you are trying to
explain your asset returns based on the market.

My 2c,
Brian


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Tom Smythe
Sent: Tuesday, February 17, 2009 2:24 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Generating Data for Portfolio Simulation


Hi All,

I am looking for a way to generate generate M snapshots of return data
for all securities in an N security universe.  The return for the n-th
security at the m-th snapshot should come from a simple factor model
such that
r[n,m] = alpha[n] + beta[n] * rm[m] + epsilon[n,m],
where rm[m] is the market return, and epsilon[n,m] is noise with zero
mean and given covariance (more on this below).

The issue that I am facing is that I want the market return to be the
weighted sum of the individual security returns.  That is
rm[m] = sum_n b[n] r[n,m]
where b[n] is the ratio of the n-th security's market capitalization
to the total market capitalization, and rm[m] has a given (time
series) mean and variance.

Some other constraints of the problem are that the weighted alpha is
zero
sum_n b[n] alpha[n] = 0
and the weighted beta is one
sum_n b[n] beta[n] = 1.
It would be nice if the noise term epsilon[n,m] had zero column (i.e.,
cross-sectional) mean and zero row (i.e., time series) mean; but both
of these requirements cannot be met simultaneously because they force
the covariance matrix to be singular.  For simplicity, I'd like to
assume that the covariance is non-singular and diagonal, with known
constant diagonal elements.

This appears to be a circular problem.  I seem to be unable to specify
r[n,m] without rm[m], and vice versa.

Can anybody advise me how to generate such factor-model data that is
compatible with the given market return statistics and given noise
covariance matrix?  Unless I'm missing something silly, this seems to
be an under-determined problem (if M >> N), with no unique solution.
However, in my case, any non-trivial but internally consistent
solution will do.

Thanks.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

--------------------------------------------------------------------------
This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. References to "Merrill Lynch" are references to any company in the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by Bank of America Corporation. Securities and Insurance Products: * Are Not FDIC Insured * Are Not Bank Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition to Any Banking Service or Activity * Are Not Insured by Any Federal Government Agency. Attachments that are part of this E-communication may have additional important disclosures and disclaimers, which you should read. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
--------------------------------------------------------------------------


From bogaso.christofer at gmail.com  Wed Feb 18 04:04:43 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Tue, 17 Feb 2009 19:04:43 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Data-set for Hamilton Time Series
	analysis.
Message-ID: <22071259.post@talk.nabble.com>


Do anyone know whether is it possible to get original dataset, used by
Hamilton in Time series analysis in web? Currently I am studying that book,
and trying to implement the examples given there from my own. Is there any R
package where those materials are implemented?

Thanks in advance
-- 
View this message in context: http://www.nabble.com/Data-set-for-Hamilton-Time-Series-analysis.-tp22071259p22071259.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From matthieu.stigler at gmail.com  Wed Feb 18 04:34:05 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Wed, 18 Feb 2009 09:04:05 +0530
Subject: [R-SIG-Finance] [R-sig-finance] Data-set for Hamilton Time
	Series analysis.
In-Reply-To: <22071259.post@talk.nabble.com>
References: <22071259.post@talk.nabble.com>
Message-ID: <111060c20902171934x38690ea7kdd801997940deeb7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090218/75c6d460/attachment.pl>

From bogaso.christofer at gmail.com  Wed Feb 18 06:05:14 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Tue, 17 Feb 2009 21:05:14 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Data-set for Hamilton Time
	Series analysis.
In-Reply-To: <111060c20902171934x38690ea7kdd801997940deeb7@mail.gmail.com>
References: <22071259.post@talk.nabble.com>
	<111060c20902171934x38690ea7kdd801997940deeb7@mail.gmail.com>
Message-ID: <22072297.post@talk.nabble.com>


If you dont mind, can you please share his email id?


Matthieu Stigler-2 wrote:
> 
> to my knowledge it is not in R, neither on the web. I actually asked
> directly Hamilton and he sent my some few hours later the dataset I needed
> (for section 6.4, use of spec ana, can send it to you). Actually it could
> be
> a good idea to ask him for the whole dataset and include it R, (in package
> ecdat? or build a new one with only datasets of usual textbooks?, software
> gretl has many other datasets).
> 
> Mat
> 
> 2009/2/18 Bogaso <bogaso.christofer at gmail.com>
> 
>>
>> Do anyone know whether is it possible to get original dataset, used by
>> Hamilton in Time series analysis in web? Currently I am studying that
>> book,
>> and trying to implement the examples given there from my own. Is there
>> any
>> R
>> package where those materials are implemented?
>>
>> Thanks in advance
>> --
>> View this message in context:
>> http://www.nabble.com/Data-set-for-Hamilton-Time-Series-analysis.-tp22071259p22071259.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Data-set-for-Hamilton-Time-Series-analysis.-tp22071259p22072297.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Wed Feb 18 06:21:04 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 17 Feb 2009 23:21:04 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Data-set for Hamilton Time
	Series analysis.
In-Reply-To: <22072297.post@talk.nabble.com>
References: <22071259.post@talk.nabble.com>
	<111060c20902171934x38690ea7kdd801997940deeb7@mail.gmail.com>
	<22072297.post@talk.nabble.com>
Message-ID: <e8e755250902172121o54b5ff17r3f5f2a74f0d013ad@mail.gmail.com>

His website:

http://weber.ucsd.edu/~jhamilto/

Jeff
On Tue, Feb 17, 2009 at 11:05 PM, Bogaso <bogaso.christofer at gmail.com> wrote:
>
> If you dont mind, can you please share his email id?
>
>
> Matthieu Stigler-2 wrote:
>>
>> to my knowledge it is not in R, neither on the web. I actually asked
>> directly Hamilton and he sent my some few hours later the dataset I needed
>> (for section 6.4, use of spec ana, can send it to you). Actually it could
>> be
>> a good idea to ask him for the whole dataset and include it R, (in package
>> ecdat? or build a new one with only datasets of usual textbooks?, software
>> gretl has many other datasets).
>>
>> Mat
>>
>> 2009/2/18 Bogaso <bogaso.christofer at gmail.com>
>>
>>>
>>> Do anyone know whether is it possible to get original dataset, used by
>>> Hamilton in Time series analysis in web? Currently I am studying that
>>> book,
>>> and trying to implement the examples given there from my own. Is there
>>> any
>>> R
>>> package where those materials are implemented?
>>>
>>> Thanks in advance
>>> --
>>> View this message in context:
>>> http://www.nabble.com/Data-set-for-Hamilton-Time-Series-analysis.-tp22071259p22071259.html
>>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>
>>       [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
> --
> View this message in context: http://www.nabble.com/Data-set-for-Hamilton-Time-Series-analysis.-tp22071259p22072297.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From bogaso.christofer at gmail.com  Wed Feb 18 07:16:00 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Tue, 17 Feb 2009 22:16:00 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] odd GARCH(1,1) results
In-Reply-To: <49999A68.4080703@gmx.de>
References: <49999A68.4080703@gmx.de>
Message-ID: <22072829.post@talk.nabble.com>


If your data is correct, I see very high auto-correlation in first few lags.
Which is natural in returns for higher frequencies. Therefore a mere
gacch(1,1) specification may not be correct here. Have you tried with
arma-garch specification?



Helena Richter wrote:
> 
> Hi everybody,
> 
> I'm trying to fit a Garch(1,1) process to the DAX returns. My data 
> consists of about 2300 10day-logreturns in chronologically descending 
> order (see attachment). But if I use the garch function I get a very 
> high alpha_1 and a quite low beta, which doesn't make that much sense. I 
> think I am missing something, but have no idea what it might be. I'd 
> appreciate it a lot if someone could have a look at the output I posted 
> at the end of this mail. Maybe there's something an experienced user 
> might see at once. I also tried the garchFit function with nearly the 
> same results.
> I'm very thankful for every answer. Please excuse my bad english.
> Helena
> 
> 
>  > g2005out<-garch(g2005,order=c(1,1))
> 
> ***** ESTIMATION WITH ANALYTICAL GRADIENT *****
> 
> 
> I INITIAL X(I) D(I)
> 
> 1 2.214508e-03 1.000e+00
> 2 5.000000e-02 1.000e+00
> 3 5.000000e-02 1.000e+00
> 
> IT NF F RELDF PRELDF RELDX STPPAR D*STEP NPRELDF
> 0 1 -5.974e+03
> 1 6 -5.998e+03 3.91e-03 5.51e-03 3.6e-03 2.5e+08 3.6e-04 6.90e+05
> 2 7 -6.000e+03 3.19e-04 6.17e-04 3.1e-03 2.0e+00 3.6e-04 5.50e+02
> 3 8 -6.001e+03 2.76e-04 2.98e-04 3.5e-03 2.0e+00 3.6e-04 5.21e+02
> 4 13 -6.161e+03 2.59e-02 3.79e-02 4.8e-01 2.0e+00 9.3e-02 5.07e+02
> 5 21 -6.182e+03 3.39e-03 7.77e-03 1.1e-03 3.6e+00 3.2e-04 4.87e-01
> 6 22 -6.182e+03 9.68e-05 7.28e-05 1.1e-03 2.0e+00 3.2e-04 2.11e+00
> 7 23 -6.183e+03 2.15e-05 2.25e-05 1.1e-03 2.0e+00 3.2e-04 2.37e+00
> 8 29 -6.213e+03 4.86e-03 4.50e-03 5.7e-01 2.0e+00 1.6e-01 2.34e+00
> 9 31 -6.278e+03 1.03e-02 1.04e-02 4.3e-01 2.0e+00 3.2e-01 1.65e+02
> 10 33 -6.301e+03 3.67e-03 5.01e-03 2.9e-02 1.9e+00 3.2e-02 7.66e-02
> 11 36 -6.347e+03 7.26e-03 7.63e-03 1.0e-01 1.3e+00 1.3e-01 1.06e-01
> 12 37 -6.399e+03 8.17e-03 9.09e-03 2.1e-01 7.0e-01 2.6e-01 1.33e-02
> 13 39 -6.412e+03 2.04e-03 2.19e-03 2.6e-02 1.8e+00 2.6e-02 2.18e-02
> 14 41 -6.437e+03 3.80e-03 5.84e-03 9.4e-02 1.1e+00 1.0e-01 1.80e-02
> 15 43 -6.498e+03 9.44e-03 8.74e-03 2.9e-01 8.5e-02 3.2e-01 8.77e-03
> 16 44 -6.518e+03 3.07e-03 2.11e-03 9.0e-02 0.0e+00 8.9e-02 2.11e-03
> 17 45 -6.527e+03 1.38e-03 1.07e-03 7.2e-02 0.0e+00 8.3e-02 1.07e-03
> 18 46 -6.530e+03 4.03e-04 3.24e-04 4.9e-02 0.0e+00 7.6e-02 3.24e-04
> 19 47 -6.530e+03 6.83e-05 7.29e-05 1.7e-02 0.0e+00 2.4e-02 7.29e-05
> 20 48 -6.530e+03 5.26e-06 6.29e-06 6.2e-03 0.0e+00 1.2e-02 6.29e-06
> 21 49 -6.530e+03 1.53e-07 1.54e-07 9.0e-04 0.0e+00 1.7e-03 1.54e-07
> 22 50 -6.530e+03 1.33e-10 1.22e-10 2.3e-05 0.0e+00 3.3e-05 1.22e-10
> 23 51 -6.530e+03 2.08e-12 6.94e-12 3.9e-06 0.0e+00 6.0e-06 6.94e-12
> 
> ***** RELATIVE FUNCTION CONVERGENCE *****
> 
> FUNCTION -6.530104e+03 RELDX 3.860e-06
> FUNC. EVALS 51 GRAD. EVALS 24
> PRELDF 6.944e-12 NPRELDF 6.944e-12
> 
> I FINAL X(I) D(I) G(I)
> 
> 1 2.041120e-04 1.000e+00 4.584e-01
> 2 7.096202e-01 1.000e+00 2.579e-04
> 3 2.487274e-01 1.000e+00 3.097e-04
> 
>  > summary(g2005out)
> 
> Call:
> garch(x = g2005, order = c(1, 1))
> 
> Model:
> GARCH(1,1)
> 
> Residuals:
> Min 1Q Median 3Q Max
> -4.1857 -0.6978 0.3268 0.9039 4.9820
> 
> Coefficient(s):
> Estimate Std. Error t value Pr(>|t|)
> a0 2.041e-04 2.072e-05 9.849 <2e-16 ***
> a1 7.096e-01 5.780e-02 12.277 <2e-16 ***
> b1 2.487e-01 2.062e-02 12.060 <2e-16 ***
> ---
> Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Diagnostic Tests:
> Jarque Bera Test
> 
> data: Residuals
> X-squared = 33.1741, df = 2, p-value = 6.257e-08
> 
> 
> Box-Ljung test
> 
> data: Squared.Residuals
> X-squared = 8.9147, df = 1, p-value = 0.002829
> 
> 
> 
> 
> 
> 
> 0.0101469753486814
> 0.0302708525301284
> 0.0298869542573102
> 0.0250238066360943
> 0.0219854269480636
> 0.0255910978745133
> 0.0208766939739549
> 0.024472384972313
> 0.0104622603956565
> 0.0156958456499357
> 0.00856720663997033
> 0.00554233092097657
> 0.0178169931968505
> 0.0210859439733833
> 0.0237885580502165
> 0.0167733305146503
> 0.0188592789433067
> 0.0135089788757992
> 0.0240819536500512
> 0.0184436895969784
> 0.0353754332617942
> 0.0321898629583924
> 0.0217899667893017
> 0.0172398497190095
> 0.0163914150178588
> 0.0201309274983712
> 0.0338013250091364
> 0.0361931688792124
> 0.0325828765178621
> 0.0287244326640976
> 0.0253523402633412
> 0.0175501402847404
> 0.0252357634588918
> 0.0374920819481566
> 0.0326047940338621
> 0.0534817989667282
> 0.0426675454652661
> 0.0223149096583253
> 0.0274987474190061
> 0.0246657927597472
> 0.0319013714312724
> 0.0297229546082391
> 0.0222133631437097
> -0.00499102835204226
> -0.0100445943888354
> -0.0305945586081492
> -0.0295261769725028
> -0.0163888329648776
> -0.0322053226938304
> -0.0243851116782977
> -0.0344066186625488
> -0.0309734197068084
> -0.0450769173663317
> -0.037850073973392
> -0.0205237352329365
> -0.0136852813721192
> -0.0142612563819063
> -0.0133534576292516
> 0.0133184076877491
> 0.00491571150380778
> 0.0253169375424642
> 0.0341114006741854
> 0.0390611810003106
> 0.0346856037825977
> 0.0311649689770797
> 0.011488947602242
> 0.0232081050895178
> 0.0276265045794663
> 0.0129717174745311
> 0.00163794296612078
> -0.024949441008574
> -0.0292122997648142
> -0.0228978631488851
> -0.00109151628878845
> 0.00330215163389701
> 0.0302721218697545
> 0.0129328948578658
> 0.0167299192156694
> 0.0227293766139797
> 0.0362692295589419
> 0.0453879999373127
> 0.0277697471036363
> 0.0145780740041389
> 0.01022462837413
> -0.00645583966150953
> -0.0188585933198392
> -0.00171855201200866
> -0.0086114034161504
> -0.0190362245646881
> -0.0226213550134221
> -0.0315894895003865
> -0.0199640879082264
> -0.0150651109153295
> 0.00168104549306336
> 0.0212349037667019
> 0.0210582860843916
> -0.00468673900344215
> -0.0105487890336046
> -0.00999531581955057
> 0.006417914681847
> 0.0103483984580009
> 0.0124777806832815
> 0.0274689419236655
> 0.0135324946248328
> -0.000999942235425851
> -0.00201157343018796
> 0.0091077131563335
> 0.0285609496226735
> 0.0334614627324541
> 0.0356484187594591
> 0.036172870988177
> 0.0402962427842855
> 0.0368065843932807
> 0.0401169149644983
> 0.0377319093198783
> 0.0506591179118645
> 0.0640578783835291
> 0.0359634884039598
> 0.0356100389214576
> 0.0205851656791246
> 0.0205431250384663
> 0.0243379395459431
> 0.020783344757343
> 0.020753169504452
> 0.0303837374432619
> 0.00687223432568442
> -0.0212507696631987
> -0.000890083425574738
> -0.000968327517976209
> 0.00793683396186942
> 0.00271101618710815
> 0.00139862451408937
> 0.00771134071306876
> -0.00748269723711762
> -0.0165277799518513
> -0.0042873222748434
> 0.0140869315400682
> 0.0135799726433151
> 0.0091780564304972
> 0.0197273716722796
> 0.0206656643060105
> 0.010469759948881
> 0.00468289969766444
> 0.0289581363659279
> 0.0261655256520015
> 0.0313153647002045
> 0.0280371945979963
> 0.0375037106176739
> 0.0377991253641397
> 0.0202854822303422
> 0.0337556157743076
> 0.0386324704981861
> 0.0457773696540876
> 0.0479546448369638
> 0.0499382574860499
> 0.0387667933183458
> 0.0389655953582428
> 0.0336805176739553
> 0.0336557828929925
> 0.026334493441797
> 0.0114441949500574
> 0.0140673202787567
> 0.0140298845111077
> 0.00146634667119319
> 0.00899348523647398
> 0.0214793602439927
> 0.0210661242267349
> 0.0130771039611837
> 0.00409434282951619
> 0.0106449138933413
> 0.0206285657062989
> 0.0249176635548972
> 0.0203087175599312
> 0.00968747775997912
> 0.00517908356842432
> -0.0299913382321829
> -0.0522137437681652
> -0.050430070940295
> -0.0321575302108152
> -0.0345120766154353
> -0.0412038265915149
> -0.045636482538291
> -0.0468804437630209
> -0.0368889597167846
> -0.0325863756596286
> -0.0141106536863289
> 0.0121772950170371
> 0.0132913197222168
> 0.00463778383120631
> 0.0120120121286101
> 0.013055577077234
> 0.0105163843407379
> 0.0142544482805856
> 0.00965539038636364
> 0.0104264207541255
> 0.0106544037109501
> 0.007582534953233
> 0.00887418130815244
> -0.00819265919875061
> -0.00544147182930852
> -0.00388093900667831
> 0.00136385438959426
> -0.0134366092944446
> -0.0173936581129147
> -0.0302001872647736
> -0.0220196995997601
> -0.0132005014927413
> -0.0193788569264968
> 0.000928025640890851
> 0.0038564867549862
> 0.00272128382807853
> 0.00772744319924974
> 0.0149526286208086
> 0.016810587954446
> 0.0170249732705174
> 0.0145852688987958
> 0.000821233153973175
> 0.00562873824105864
> -0.00419093110832941
> -0.00822036267277076
> -0.00896480998361527
> -0.00872517476781923
> -0.00980869632757009
> -0.0110828544522088
> -0.00298405325282003
> 0.00464205460684028
> 0.0203536661116602
> 0.0167249902059099
> 0.0281197979417544
> 0.0304493363477014
> 0.0433125776858556
> 0.0293533194520085
> 0.0324589294868293
> 0.0319456728556491
> 0.0383929957557034
> 0.02936731415153
> 0.0143990951785218
> 0.0118851391883572
> 0.00685997241626194
> 0.00219755487032636
> -0.00724437226368031
> 0.00101322300165641
> 0.0012584681119624
> -0.00566655150992872
> -0.0247930849760908
> -0.0240805999268673
> -0.0188965848686644
> -0.00298455380947663
> -0.00931724845795504
> -0.0107813588247703
> -0.00558879175451497
> -0.0103777167692447
> -0.00920710761226237
> -0.000887344831574602
> 0.0168591786513701
> 0.0151216347298227
> 0.0115335166825457
> 0.00399081889557025
> 0.0178984165180372
> 0.0188125617570961
> 0.017494387522454
> 0.00526987163893765
> 0.00805068261716505
> 0.00717998537291134
> 0.00381331335223118
> 0.0182935187150967
> 0.0240929622342112
> 0.00945920826401526
> 0.00042007782737388
> 0.00419727793533992
> -0.00634004178248987
> 0.00409699381732858
> 0.00658595762214191
> 0.0252008597985653
> 0.0172746585530059
> 0.00486984712817144
> -0.00239208080603106
> 0.0182671577987833
> 0.0238421380497397
> 0.0168147565395778
> 0.0177334771699302
> 0.00898627620791174
> 0.000626087309445634
> 0.00213023632032983
> 0.00305265581228666
> 0.00263208156178418
> 0.0071256916986782
> 0.00880651092340985
> 0.0117477233532896
> 0.013428820308931
> 0.0173963674663941
> 0.0334091893860974
> 0.0351196634612955
> 0.0195351524689469
> 0.0298783238636937
> 0.0451974847857433
> 0.0423330290980611
> 0.0399396574136204
> 0.0512424161455808
> 0.0541719488608689
> 0.0321178990975415
> 0.0269142463020634
> 0.0318560435394822
> 0.0183566147733833
> 0.024590627580086
> 0.00967738029113653
> 0.00484301678872009
> -0.0118912579596504
> -0.0266265471475261
> -0.0415215196694662
> -0.0202253746150089
> -0.027404057981743
> -0.0344819305429126
> -0.0211119007757319
> -0.0297211894735586
> -0.0184037923567812
> 0.0121430865963278
> 0.0141003477171616
> 0.0214590174129034
> 0.0363563980396132
> 0.026557736599672
> 0.0346492463896824
> 0.0268559347703999
> 0.0143514745327217
> 0.0138812059439327
> 0.00172616204894809
> -0.0180109367500213
> -0.00544130081512372
> -0.0167257622589755
> -0.0201701337493256
> 0.00622602645063428
> 0.0140368018751599
> 0.0148702479814149
> 0.0258844967882779
> 0.0229118786909834
> 0.0308309779656704
> 0.0334001368970977
> 0.0319975953341715
> 0.0420444288478354
> 0.029380360111681
> 0.0090084760993894
> 0.00493005488698442
> 0.0248362833158545
> 0.0308221222649122
> 0.0301443764007555
> 0.0407412933183506
> 0.0292380721395114
> 0.0241577364616712
> 0.0212210971376596
> 0.0370805924050055
> 0.0544774250035719
> 0.0465133057391012
> 0.0294539482553744
> 0.0134445217781137
> 0.0219265947229511
> -0.00406701858245493
> -0.0280843985272508
> -0.0257595459062084
> -0.0452639229484746
> -0.0432767595028435
> -0.0659481082572031
> -0.0613802714421972
> -0.0342801247694719
> -0.0248037858141004
> -0.016730383300487
> -0.0184960362901824
> 0.0073341630546773
> -0.0139564568468299
> 0.0102970221218781
> 0.0130497699651159
> 0.0128348318273541
> 0.0109838803508913
> -0.0237824355717937
> -0.0232714480613084
> -0.0367954475933564
> -0.0329382182940499
> -0.0345013813883218
> -0.0136015395964663
> -0.0275713643785256
> -0.0469070512230988
> -0.0389713675591478
> -0.0476682466541746
> -0.0387116551113008
> -0.0416057278141162
> -0.0442415656986874
> -0.0223898975416764
> -0.0182765839974116
> -0.00368730482991154
> 0.00418886285162511
> 0.00160800731952331
> -0.000255045909646019
> 0.0123585198855283
> 0.012286694859706
> 0.0204623480743715
> 0.0301095331968522
> -0.000301448324448817
> -0.00363446989742791
> -0.0132599017422999
> -0.0227910017513599
> -0.00711869445067605
> 0.00951057946595941
> 0.0173062601847406
> 0.0291294173169169
> 0.0313648109970505
> 0.00692246533813156
> 0.0282540064146679
> 0.0273011056940766
> 0.0331207295103254
> 0.0486599147490723
> 0.0380407663781464
> 0.0333861746922965
> 0.0200512106836536
> 0.00413630000972881
> 0.0195840278347282
> 0.0435307881637761
> 0.0258572223069899
> 0.0228485074852761
> 0.023884669165522
> -0.00567082925965337
> 0.0217533684174691
> -0.0165128734633909
> -0.0181039894574897
> -0.0379658383732039
> -0.0518137118809703
> -0.0652845397783706
> -0.0467734848391144
> -0.0469792174622804
> -0.0738669626671105
> -0.0712380616192844
> -0.0863243683959267
> -0.0520115319420648
> -0.0375743350462611
> -0.00100643005419919
> -0.0174820768490909
> -0.00433726740346562
> -0.0121634731917566
> 0.00107318641812894
> 0.0131139923406817
> 0.0152778176599253
> 0.0275960583332117
> 0.0221983576547162
> 0.0113024387547474
> 0.00622626526966881
> 0.00948059659567896
> -0.00582884032169232
> 0.0065609232585537
> 0.020118063558006
> 0.0396698803694333
> 0.0496939427771939
> 0.033513886634549
> 0.048810969014196
> 0.0515381541213287
> 0.0712302601722484
> 0.0758888336979855
> 0.082169364316466
> 0.0481646736332026
> 0.0251345742613254
> -0.0103412417562568
> 0.0134272410902793
> 0.0183286223879309
> -0.0240522029923154
> -0.0241119798994927
> -0.0820533232176898
> -0.0918539388465809
> -0.105939824018467
> -0.0773144604718445
> -0.0769986829381959
> -0.0439074653651024
> -0.0701992480148465
> -0.0619814661741739
> -0.0259116990939665
> -0.0259999783569164
> 0.0122787001435858
> 0.0237987112521023
> 0.0188156061501094
> 0.0128765885216163
> -0.00187304216174823
> -0.00578913927758568
> 0.0010931896357231
> -0.00394590469511312
> -0.00963202211969791
> -0.0280086111561588
> -0.0312486158437527
> -0.0294705994564248
> -0.00740972673992791
> 0.00698667840048369
> 0.0310802426119022
> 0.0164879143027075
> 0.00940722249021498
> -0.00028002741503981
> -0.000381978034746748
> 0.00631348417597723
> -0.00678886607211478
> -0.00572939584690978
> -0.00722202065799319
> -0.0260701225825271
> -0.0306769286700066
> -0.0268604181434762
> -0.0119796825087047
> -0.0166404213829733
> -0.0129838891213947
> 0.00660425747472084
> 0.0231636897531602
> 0.0339981382898026
> 0.0326864052850039
> 0.0332179987998988
> 0.0230741219006272
> 0.0328284892189975
> 0.0174338258583891
> 0.0254471388015509
> 0.02291327701149
> 0.0257896196125616
> 0.0224562667804007
> 0.00780270696370744
> 0.0108673884836062
> 0.0284986052201535
> 0.035755252057699
> 0.0255611596812037
> 0.0332827447436808
> 0.0401831424918455
> 0.030337355826766
> 0.0240643359888403
> 0.0301044271645569
> 0.0253309998951655
> 0.0197370037537281
> 0.0111317352158881
> 0.011463385255591
> 0.0213413655639112
> 0.0147521489384983
> 0.0183255426357108
> 0.0146485568649416
> -0.00100701562670613
> -0.00727419068508759
> 0.0147802629144485
> 0.0141024363590982
> 0.0300256053040247
> 0.0299502648877483
> 0.028656442617576
> 0.0298253879598184
> 0.0184134063471393
> 0.0533211634269603
> 0.0630438119123345
> 0.0593616027594169
> 0.0382574205284818
> 0.0391365546513702
> -0.0136413656651027
> -0.00548560874333058
> -0.00947828660933971
> 0.000881679482227844
> -0.00244543660639616
> -0.0377994065273824
> -0.0260162232037148
> -0.0177508293782351
> -0.0203652021608202
> -0.0188601419449387
> 0.0379497036687741
> 0.0340142847894037
> 0.0361050438751704
> 0.0390766970566867
> 0.0631158838976889
> 0.091261873274332
> 0.0655158066964125
> 0.0630315562929304
> 0.0441575796532576
> 0.050715771802802
> 0.0388523992375882
> 0.0171645446771525
> 0.0124799838531344
> 0.0136983444590838
> -0.00603503502039807
> -0.00537560383385352
> 0.00436736978835738
> 0.0276726831544205
> 0.064700914306688
> 0.0443538173703928
> 0.028166408463651
> 0.0879072189095575
> 0.0698067959433833
> 0.0828593617951945
> 0.0626894840984529
> 0.0430901993306218
> 0.0457265657378202
> 0.0262567171228262
> -0.0163271345775315
> -0.0149714653561203
> -0.0456514953167085
> -0.0974486655321045
> -0.0671285772218334
> -0.0903549479896134
> -0.0564298069760525
> -0.0536386178182463
> -0.0698312972790422
> -0.0670979143049713
> -0.0523657119713219
> -0.0522139501149992
> -0.00807361605597209
> -0.0155620259263866
> -0.0239949141548983
> -0.000687049151038388
> -0.0154951297980617
> 0.00671565767598893
> 0.021016366649809
> 0.0153251954110971
> 0.0394157021156541
> 0.0396152405547339
> 0.0163932599891
> 0.0285333160807182
> 0.040930407684644
> 0.0177245569872134
> 0.0180807219805033
> 0.0117342592948353
> 0.011509953037652
> 0.0244680520944705
> 0.0215798779364074
> 0.0469436318718666
> 0.0630352115185743
> 0.0677561719292718
> 0.0365234745686862
> 0.0190618038232971
> 0.0294905740852519
> 0.00146451625956197
> -0.0101318339661007
> -0.00882851461814235
> -0.0136305072353975
> -0.0231432410373206
> -0.00737019958706372
> -0.0128022790145208
> 0.0213117372043506
> 0.0355872148176148
> 0.0353606833597034
> 0.0212127385127121
> 0.0461118802768027
> 0.0121439085837905
> 0.0127496842888767
> 0.00636941089398639
> 0.00909124375131416
> 0.0316009355204286
> -0.00541731935358146
> -0.00789784686758043
> -0.0138584999824583
> 0.0384830541946253
> 0.0270107404665406
> 0.0442392478120647
> 0.0729557071375012
> 0.0530574131624443
> 0.0310961880245224
> 0.00879125190894737
> 0.0379144511101029
> 0.0387503143686486
> 0.0449451801221042
> 0.000194486763936473
> -0.00159962308902211
> -0.0192849881909
> -0.0435104686709127
> -0.0135451440850519
> 0.0175029533298774
> 0.00673305240393933
> 0.0064827253034794
> 0.0242238568888808
> 0.029178255839096
> 0.0350372919158828
> 0.0659868050389957
> 0.0702431636730495
> 0.0823045197468548
> 0.0632026088421528
> 0.0605023397810518
> 0.102194892337039
> 0.0848731968734094
> 0.0887654751873121
> 0.0900416056691809
> 0.102481245675389
> 0.0591369190233781
> 0.0856315827015946
> 0.0640853428371879
> 0.0723464404902272
> -0.0021434225254437
> -0.0280441506429176
> -0.00222048582656944
> -0.012570299779459
> -0.0378084209706574
> -0.0462966493821571
> -0.00725753408538265
> -0.0611860159105827
> -0.0772563618057776
> -0.055391973552716
> 0.00103094863091361
> 0.0159627897615656
> -0.00545666282952264
> 0.000340269916376739
> -0.00565592781483244
> 0.0408559587736568
> -0.00191771871085246
> 0.0104481856948394
> 0.0351700423448907
> 0.0383146638190966
> 0.0292998270712081
> 0.0144683462340936
> 0.0407081344337855
> 0.0260641456386527
> 0.0618411188154409
> 0.0374331685299237
> 0.0696397770007706
> 0.0842400239074169
> 0.0674643722531787
> 0.0318276840663705
> 0.0885405612010042
> 0.120802905832706
> 0.0869882048826663
> 0.145565910780054
> 0.135926546598336
> 0.081155270598689
> 0.0428191861269372
> 0.0582726387918469
> 0.0487486156375795
> 0.0973532488808089
> -0.0227197417444369
> -0.0135431257604667
> -0.00994134503384236
> -0.0534090536019995
> -0.0257599781560623
> 0.0477951578529399
> 0.0931102446715628
> 0.157727764688971
> 0.134089732364357
> 0.0899977735907788
> 0.110239900890181
> 0.066398065527999
> 0.045849805920816
> 0.0328719791466589
> -0.0248306802861234
> -0.0581388052824821
> -0.065317108590058
> -0.106367743281125
> -0.0752630267291769
> -0.0989748873106328
> -0.0855589822639788
> -0.0611672174561946
> -0.0494491357441157
> -0.0913063669597308
> -0.0606123939964599
> -0.0488116640677756
> -0.0165930937223677
> -0.0482225081070968
> -0.0553686641508126
> -0.00571602970982038
> 0.0304840773755794
> -0.0220379663881857
> -0.0378437579419137
> 0.039892729398082
> -0.015755797499047
> -0.0270640184625108
> -0.0527875184796437
> -0.0512899885969625
> -0.0167451953231867
> -0.0220701936348766
> -0.0561810317829638
> -0.0594363425350005
> -0.0279881465963647
> -0.0863943317365838
> -0.0501598356494849
> -0.060367915743584
> -0.125542812785546
> -0.119262699373677
> -0.148401416970695
> -0.146410035544208
> -0.111148751651856
> -0.0774755133452325
> -0.0654967705117088
> -0.0810023886927833
> -0.0872172735510927
> -0.0579426698558164
> -0.0165383732657254
> 0.0527787270596935
> 0.0688230001013086
> 0.0564611888113941
> 0.0671747878491474
> 0.0122018166947399
> -0.00261602060486366
> 0.0366207555410498
> 0.0508091460687169
> 0.0224688878788928
> 0.0473610914820969
> -0.0439810405044236
> -0.0820471228384173
> -0.102636447002606
> -0.0801705434891892
> -0.0363347956859369
> -0.0630234228619226
> -0.0542050679061152
> -0.0213412732585695
> -0.0588479635488478
> -0.0851868205746854
> -0.094043497630076
> -0.04377955574795
> -0.0531323094434082
> -0.076086568163474
> -0.076940093712732
> -0.0458917703561578
> -0.00743444070041606
> -0.0734586161843211
> -0.0347286375044652
> -0.0244721886053829
> 0.0329886932576513
> 0.022678708262428
> 0.0490629663017024
> 0.0394886750348975
> 0.0526510516767207
> 0.0872966853242803
> 0.0240201417264441
> 0.0811572214098502
> 0.0755924615548418
> 0.0461268495733566
> -0.0263689237521623
> -0.0440402048584305
> -0.0334815809099987
> 0.00836888154990604
> 0.0112092815054123
> -0.0152656439188244
> 0.0305894434435966
> -0.0502908394732695
> -0.00741294198351916
> 0.0210233387243717
> 0.08983176756508
> 0.0600584008433198
> 0.0136963783021537
> 0.000470861131529471
> -0.00620050648192429
> 0.0341918674534477
> -0.00865204366643538
> 0.115468166954773
> 0.0567953376076795
> 0.122704901947638
> 0.149043396154704
> 0.185324187539814
> 0.207556649418037
> 0.153080765686892
> 0.120149140645554
> 0.0276953582219987
> 0.061925590401742
> 0.0288605116606405
> 0.00404811789059043
> -0.0999860619473703
> -0.131337778656047
> -0.0914581619571679
> -0.0885120305877086
> -0.12163373352702
> -0.0667410222262697
> -0.0655195072480756
> -0.137974465940891
> -0.181181519036591
> -0.141115020834621
> -0.124731698118443
> -0.190638492442819
> -0.19581480195928
> -0.162720333251736
> -0.128377433180322
> -0.109266134781039
> -0.0919555280571695
> -0.032855236550374
> -0.0838657266818906
> -0.0995021618124415
> -0.0675354913056518
> -0.0270121947696855
> -0.0971764660800556
> -0.0984065369280297
> -0.0937472447394767
> -0.152289303017965
> -0.121417316069354
> -0.103201369205686
> -0.0613210875215777
> 0.0076376183523574
> -0.00130754834241606
> 0.0255542755444241
> 0.0446364851195671
> 0.0367988100701001
> 0.0177627372165227
> 0.0599429865499806
> 0.109913062331931
> 0.0544951270437569
> 0.141097628295817
> 0.04218339255601
> 0.0163063159080536
> -0.0302407386428535
> -0.0517772658619527
> -0.0566696301063497
> 0.0495642228689598
> 0.0441199821135625
> -0.047096697636722
> 0.0149089425739706
> -0.102246133545353
> -0.096889326655348
> -0.128445974801897
> -0.100863562248443
> -0.025154412371727
> -0.0135689247397282
> -0.143387661031223
> -0.156897356509104
> -0.14278807623025
> -0.217432459435046
> -0.185165096716862
> -0.141406825726716
> -0.0377752803277668
> -0.0110146090415743
> -0.0534034415466429
> -0.109853653654219
> -0.0591619357201668
> -0.0336463291128159
> 0.0219979989773423
> 0.0389166694521166
> 0.0735773796772275
> 0.0575299490503986
> 0.00304316874670675
> -0.0510344495855566
> -0.0551485067700941
> -0.0244959324180901
> 0.0181230575842313
> -0.0482843795335517
> -0.0955840416854052
> -0.0915879226996194
> -0.106117157135981
> -0.0854976937515753
> -0.0925811222420903
> -0.0600438929976276
> -0.0423788860661278
> -0.0591842480963259
> -0.112911197803777
> -0.063239620151855
> -0.0791747230844206
> -0.0656405300214987
> -0.0779973853861287
> -0.0607907365326796
> -0.0465596379657704
> -0.0618797286782978
> -0.0747079984605013
> -0.051522946087169
> -0.0442723579338437
> -0.058223817064396
> -0.0382981026085817
> -0.0261861775130994
> -0.00280567195082636
> 0.00561468617510987
> -0.0176685840850152
> -0.0219327274000535
> 0.0227664844309823
> 0.0239533865064366
> 0.0309808449097635
> 0.0165584920340603
> 0.00616795794210929
> 0.00156189949177975
> -0.0065227726178593
> -0.0260709573892322
> -0.0175497898535331
> -0.0258240841846205
> -0.0635495583987133
> -0.0644294113122768
> -0.0790747872766775
> -0.0583537321999572
> -0.0535565608311323
> -0.0583078319112054
> -0.046078083013164
> -0.0371523479779721
> -0.0212489288959108
> -0.020185808408421
> 0.00421719636627739
> 0.00484315602927125
> 0.00455568692568344
> 0.00150791589189207
> 0.0069261879524186
> 0.00615677648895637
> -0.028774271749078
> -0.0392307182243702
> -0.0443869173147511
> -0.0155731399229188
> -0.041733797646338
> -0.0261119400359268
> -0.0198751961920759
> -0.0176793119575627
> -0.0155659346723271
> -0.0281205092944465
> -0.00531261037398992
> -0.000707512253546669
> 0.0225638568379092
> 0.0192586840110902
> 0.0215226188207727
> -0.00437041242457753
> 0.00122696198758125
> 0.0111393119012333
> 0.0149186425828396
> 0.0437587700645768
> 0.0337742045711498
> 0.0578719204051115
> 0.0461095931723377
> 0.0560138806367074
> 0.0743563060902096
> 0.0935845454432291
> 0.121666357812007
> 0.0865812805051257
> 0.100431059501455
> 0.0930587968133089
> 0.0739801134409442
> 0.0471592933112282
> 0.0130454218765932
> 0.00502650223548358
> 0.00265166732935263
> -0.0155987633766347
> -0.0188639350534687
> -0.00244817821629721
> -0.00504349167344658
> -0.0356090731352725
> -0.022873816923527
> -0.0470906286331945
> -0.0265534805714321
> -0.0234001996301024
> -0.0400763811081205
> -0.0433813077754619
> -0.064205671779208
> -0.0613804039786831
> -0.0719893703344856
> -0.0218331392273541
> -0.0169604492028283
> -0.00492598818441791
> -0.00503662324694945
> 0.0135508828955742
> 0.00443106565804708
> 0.0182266709976061
> -0.010290832236245
> -0.0110920448095324
> -0.0239561978617171
> -0.0370881509978783
> -0.0315461279636895
> -0.0376446619470477
> -0.0263171818221285
> -0.0361896373055315
> -0.0191863950439205
> -0.018435991172959
> 0.00961813844066178
> 0.0214561010802242
> 0.0522471112180122
> 0.0423958070809319
> 0.0416029591009473
> 0.0580018499508737
> 0.0659069972536629
> 0.0360913314760168
> 0.0236213081899391
> 0.0180116698586566
> 0.0498001505945868
> 0.0299689795104278
> -0.00863958111469434
> -0.0250744534971777
> -0.0208353809511023
> -0.035239366625798
> -0.0660967221341836
> -0.0542824888979125
> 0.00510264564223025
> 0.0158210536565239
> -0.016262064972572
> 0.0060532615736774
> 0.0293872634328386
> 0.0170256571455651
> 0.00206274245894388
> 0.00928702327071537
> 0.0282343494841881
> 0.03395954060964
> -0.016259234148621
> -0.0386658922631309
> -0.0144702128426405
> -0.014131617804645
> -0.00761543295498169
> 0.022506231505394
> 0.0591546529899202
> 0.0478968481307159
> 0.0258896800721027
> 0.0455199320475133
> 0.0793023943788412
> 0.0865692986251808
> 0.0994666418488504
> 0.0768231452324566
> 0.0829686378864362
> 0.0849719879450879
> 0.0337601359885403
> 0.018460329807539
> 0.0572749233906532
> 0.0100988402322157
> 0.000728866878069312
> 0.0289723211598982
> 0.0153418920619506
> 0.0134109821721059
> -0.0186208043194326
> -0.0179930397745953
> 0.0242974413969417
> 0.0413233898389391
> -0.000606313737164056
> 0.0421557992544127
> 0.0505303290571841
> 0.0272484703351453
> 0.00574145970950943
> 0.00575282485452001
> 0.0458507790612365
> 0.0722050295939609
> 0.0702369174692015
> 0.0709959010479903
> 0.120095252682208
> 0.119074733344502
> 0.109357883289072
> 0.107078655813862
> 0.169703211585234
> 0.177173588258582
> 0.0932117038061687
> 0.0257337077548664
> 0.00127912866781921
> 0.0456316187559751
> -0.0484885522506294
> -0.0569228869396799
> -0.0638684177423844
> -0.145266526587947
> -0.222431960355462
> -0.246653427791486
> -0.222317835671864
> -0.216357523524324
> -0.184805942223622
> -0.231504074969818
> -0.161525812898072
> -0.201882026796179
> -0.21692187007086
> -0.14640947595925
> -0.130014613908678
> -0.0748013054757742
> -0.0335296579965298
> -0.00153680740823251
> -0.02208025498147
> -0.00652241587084793
> -0.0379205322569308
> -0.0279635039112537
> -0.0391444215289815
> -0.00871072813871796
> -0.0085001976548669
> -0.0479810261980977
> -0.0728166871609679
> -0.0978843679414547
> -0.0983476596128852
> -0.0938377328622188
> -0.0746108637911747
> -0.0673004062938174
> -0.0598461794388584
> -0.0602033457623583
> -0.0574630621207557
> -0.0292261444262202
> 0.00567104036819742
> 0.015636583162006
> -0.00792011607746016
> -0.0049009049445978
> -0.00903084201920315
> 0.0184826597267715
> 0.00248209662059307
> -0.0105737314080129
> -0.0296438596964557
> -0.0370311488357861
> -0.03848493646979
> -0.0266680612118892
> -0.0133997454956531
> -0.0168658136464328
> -0.0286606344079332
> -0.0489450204669629
> -0.0353176539081351
> -0.0427607469536436
> -0.0217538627084448
> -0.0138077419328247
> -0.00538037722837234
> -0.00539605273960995
> -0.00551471048939178
> -0.0134991676179498
> 0.0122108888207821
> 0.0234929773708079
> 0.022424489466132
> 0.0401538597503483
> 0.0239205095357019
> -0.00991423628749123
> -0.0466956436007705
> -0.0355057141269159
> -0.0431759898839281
> -0.0404771893241979
> -0.042592166082173
> -0.0524461275539477
> -0.0525577478234132
> -0.0512614317696145
> -0.0348845830729782
> -0.0151370322774699
> 0.0116382567569956
> -0.0100464904222448
> -0.0087386466912903
> -0.00585943897627811
> -0.0151890685261425
> -0.0036767562207758
> -0.00454897881154139
> -0.0116081547382808
> -0.0100227933815017
> -0.00822151695267711
> -0.0175924127999606
> 0.00819481056181996
> 0.0247783512786881
> 0.0133528449453209
> 0.0182774726318948
> 0.0246461955831074
> 0.0261531904995475
> 0.0205705540432077
> 0.00788473190709583
> 0.0138043679567258
> -0.010580671224022
> -0.0314791669214615
> -0.0324185937424575
> -0.00555689601499725
> 0.00675737628463961
> -0.00841608596097018
> -0.00259129128290386
> 0.0116872392634305
> 0.00168103575712246
> -0.0151155017733505
> 0.00791039062242223
> 0.0539356243539231
> 0.0427576438908512
> 0.0284050190475861
> 0.0200172278777967
> 0.0271896812540937
> 0.0350131832675034
> 0.0457244623219739
> 0.0725938725594453
> 0.068376516008685
> 0.09652001843505
> 0.0665435917107528
> 0.0410733186595953
> 0.0291343051116441
> 0.0207050219031792
> 0.0227121044476142
> -0.00411237454190302
> 0.00939181285005798
> 0.027432558348989
> 0.0690727987062084
> -0.00435482762246882
> -0.0403561583629604
> 0.018124431480467
> 0.0165096050065705
> -0.00180980145835981
> 0.00403044387816429
> -0.00415422985540864
> -0.0543029202366485
> -0.112424835325541
> -0.151129374661825
> -0.114740990112013
> -0.0832389531092811
> -0.0942427647900852
> -0.0714188549903141
> -0.0388667229881601
> -0.0690338317143462
> -0.0422851031770667
> -0.0232953177926974
> 0.0210239865433532
> -0.00174252039898339
> -0.00669375635751059
> -0.0263072436344253
> -0.0403298829378303
> -0.0444959527016318
> -0.0736924990232037
> -0.0428230065070877
> -0.052827930187907
> -0.0589539655149124
> -0.067113351305128
> -0.0555815608075986
> -0.0357384507704025
> -0.0367431743626156
> -0.023796034068037
> -0.030427215719324
> -0.0169990593423321
> -0.047507202341153
> -0.0272810575843618
> -0.0279459183964639
> -0.0300395456926961
> -0.0135706908194275
> -0.0192258754471804
> -0.00438003479133552
> -0.00705554352248585
> -0.00200606099150318
> 0.0103325847079143
> 0.0210826545570358
> 0.035906223425271
> 0.0343702538589755
> 0.0311236985350645
> 0.0397666228866694
> 0.0593722695344028
> 0.0484427301062742
> 0.0432954004313194
> 0.0413168026014301
> 0.0398475748099609
> 0.0333793711961116
> 0.0331173850878756
> 0.0137786516356827
> 0.00873134170208188
> 0.0145786010182452
> -0.0012792305489744
> 0.0241985524244001
> 0.022268365190938
> 0.0207246621270095
> 0.0279618288753313
> 0.029362580958128
> -0.0296769396290424
> 0.00676242024457379
> 0.0160302251810587
> -0.0153114872310424
> -0.0451175769043942
> -0.0743029951603712
> -0.0815432756477509
> -0.0679952640613981
> -0.0572532013596389
> -0.0580521472927331
> -0.0240642213356006
> -0.00278942388262019
> -0.0282807746336531
> 0.0152032007236708
> 0.00331202032452233
> 0.0161735372225701
> 0.0127024883274138
> 0.00405378742137597
> -0.00530414972338804
> 0.0170127892680298
> -0.00615540923144255
> -0.0309421669478119
> -0.0360953456563125
> -0.0711309838125877
> -0.0535210001041493
> -0.0501995813534391
> -0.00672522311142554
> -0.027748421869301
> -0.0528864799786844
> -0.0737212715760727
> -0.0579193222465785
> -0.0766894526675424
> -0.0541868616307161
> -0.0353973390837014
> -0.0139772435031331
> -0.0157777824429231
> -0.0269932920831107
> -0.0105965044460549
> 0.0279167859042652
> 0.0378649072604033
> 0.0394125837754079
> 0.0750310912833012
> 0.0742103865378415
> 0.0684842391574641
> 0.0851299485900652
> 0.0802434830912451
> 0.0441748043238489
> 0.0387771809200444
> 0.0457476188152385
> 0.028039718703997
> 0.0192437498038737
> -0.00907140372946643
> -0.0235863300725867
> -0.0404231495395548
> -0.0511789117856443
> -0.0493680562395209
> -0.0348468966914146
> -0.0203314426693975
> -0.0552854635514075
> -0.0377490768998512
> -0.013676162077556
> -0.016023244977561
> 0.00534749554929352
> 0.0308773952769564
> 0.008566018411885
> -0.0109392395222732
> -0.00427950442097851
> -0.0291983292415337
> -0.0310861367116211
> -0.0278159105337173
> -0.0533491866400577
> -0.0608279670674363
> -0.0753524882391176
> -0.0983157743593176
> -0.0805952680135483
> -0.0638374889045015
> -0.0773015436070403
> -0.0481304474833989
> -0.0235483140061557
> -0.0252694215125385
> -0.0219895694453107
> -0.0171466107057664
> -0.00540655343685517
> 0.0195957953481896
> 0.0137680563238128
> 0.019922444022711
> 0.0336285264208166
> 0.0154011834689056
> -0.00855203737511705
> -0.0178904876019674
> -0.00178470828089068
> 0.00102924916845673
> -0.00216129105364983
> -0.00698909829438364
> 0.00083958706103256
> 0.0174484232886874
> 0.0120343305962108
> 0.0302963515900222
> 0.0336039285162712
> 0.0281171746981141
> 0.0224046523804569
> 0.0194606900107243
> 0.0269445721291379
> 0.0134857181316079
> -0.0103808327568284
> -0.0284018440374488
> -0.0297899054130249
> -0.0495825971513212
> -0.0609402572949285
> -0.0351054280467928
> -0.0359263619115718
> -0.0328774361728181
> -0.0263162151798721
> -0.00174554946706429
> 0.032874413707764
> 0.0453659797909284
> 0.0357684737606671
> 0.0445174813135039
> 0.0733530265769478
> 0.056524190919555
> 0.0644835289568424
> 0.0655900290494463
> 0.0591270343467174
> 0.0456991942031855
> 0.00140489839720164
> -0.00640152930347835
> 0.00618954579978602
> 0.0102348065683964
> -0.0146495669400466
> -0.0196794533452669
> -0.039931552171151
> -0.0338843075710093
> -0.0332456109294822
> -0.0639624985565466
> -0.0409427913566472
> -0.0307262608685391
> -0.0292319046415753
> -0.0385184470250959
> -0.0265053846641847
> -0.0268047816193848
> -0.01817138695517
> -0.0286488516593405
> -0.0422221412655267
> 0.00765136891437322
> 0.0333723669679311
> 0.0208025616421598
> 0.030731493710569
> 0.0445649897733673
> 0.0371664208650355
> 0.0648733283143491
> 0.0605063341579499
> 0.0691652863777766
> 0.0623879329077447
> 0.0126164473036533
> -0.0142225314786288
> -0.0347577347439127
> -0.0251198665919338
> -0.046596137946058
> -0.0394211883761134
> -0.0409895913183314
> -0.0496786109813059
> -0.0691747355516825
> -0.0746621566798786
> -0.0281630250580259
> -0.0226792021904676
> -0.0247698397885584
> -0.0300546309603197
> -0.0198045693913185
> 0.00521227982765038
> -0.0369029999654849
> 0.00000412058196346215
> 0.034349019170114
> 0.050780306497194
> 0.0314588124917155
> 0.0219583925236972
> 0.0487379548154692
> 0.0311685009807644
> 0.0273231889686348
> -0.0309919288346683
> -0.00735189353131799
> -0.0220273971292984
> -0.0489368434554367
> -0.0496350212398541
> -0.0394815509868298
> -0.0156813877491094
> -0.0443450019305348
> -0.0331275554947734
> -0.0519293509884478
> -0.0259496008541492
> -0.055108622961608
> -0.0636680737767869
> -0.0487512038400697
> -0.0530995109634894
> -0.0328369987737656
> -0.0618662368354806
> -0.0372014748208835
> -0.0579395575352761
> -0.0145695250946119
> 0.00800196011109342
> 0.0589598735210566
> 0.0361842018412041
> 0.0254903659944338
> -0.0054726045020821
> -0.0325228278224458
> -0.0238684249693628
> -0.0323901078867708
> -0.0130490185882783
> -0.0317952626856677
> -0.0466042209149493
> -0.0414088913141903
> 0.00071920811751566
> 0.0139679176476787
> 0.0301996334115409
> 0.0395980322110137
> 0.0367286662568805
> 0.0583375530167805
> 0.049508031839252
> 0.0497404863745157
> 0.0470565935378986
> 0.0312295186501704
> 0.0330360852081928
> -0.00757228736791593
> 0.016564299918909
> -0.00895652836165308
> 0.00911536000860901
> 0.00766076968177305
> 0.0395313062322826
> 0.0172020017054567
> 0.0303033808912559
> 0.0434339360854746
> 0.0478640275165346
> 0.111881428358737
> 0.0742873714999261
> 0.0786551907908754
> 0.0904463603569766
> 0.10319241376691
> 0.0512231821921402
> 0.0626163825665062
> 0.0334034423572495
> 0.0113455689032631
> -0.00306743059461902
> -0.0600840524149925
> -0.014975188241066
> 0.0241665887985865
> 0.0081486211769498
> -0.0119132355937793
> 0.000933789760228575
> 0.0307552131976052
> 0.0939400425088339
> 0.0867114275173302
> 0.0710698745963996
> 0.0725732110666485
> 0.0304424499924815
> -0.00031047597941539
> 0.00772999676213349
> 0.00432058458210783
> 0.0128041345511748
> -0.000210862361481363
> -0.0463932949138881
> 0.00146830248163848
> 0.0258779298356546
> 0.0566970471898971
> 0.0908433996459196
> 0.0928299794682165
> 0.0958285674481233
> 0.103321589661162
> 0.109671129342557
> 0.106385097598492
> 0.103084494403674
> 0.0598110367198523
> 0.041335406802138
> 0.0377766878598895
> 0.0376423311939035
> 0.065844576014988
> 0.049146089683019
> 0.0483293775539479
> 0.0396720488324458
> 0.0231978464812133
> 0.0259313127291418
> 0.0497592236674559
> 0.0574810092638115
> 0.0538999948680839
> 0.027032388540319
> -0.00216198111510208
> 0.0107879640041941
> -0.00228367072097455
> 0.00503739094195154
> 0.0284329808373463
> 0.0270490087338474
> 0.0132013006035453
> 0.0208548872526941
> 0.0299904843160379
> 0.0513059310585601
> 0.0542924548249766
> 0.0541290233966387
> 0.0633163826944177
> 0.058759668177873
> 0.0469579793205171
> 0.0573622056457291
> 0.0681968591160539
> 0.0552258829710594
> 0.0597423884080483
> 0.0545440359803269
> 0.0715478864771796
> 0.0497038378210574
> 0.0539711771031049
> 0.0690533783952299
> 0.0637343176235139
> 0.0483495697914656
> 0.0128396807098305
> 0.00563868242795336
> -0.0175320515892931
> -0.0114151639323378
> -0.0324092194929419
> -0.0116661961667096
> -0.00877876620777622
> -0.0120635979734586
> 0.0115786090583761
> 0.0135892514848756
> 0.0306436083320576
> 0.0456979780173811
> 0.032827746403533
> 0.0431870426841955
> 0.0223428158831412
> 0.0216321000797544
> 0.0036071278391777
> -0.0251875986589624
> -0.034407224311034
> -0.0295767308468411
> -0.0478214808025629
> -0.0536924963966229
> -0.0387956817256511
> -0.0550534121451825
> -0.0255760031468337
> -0.0304437456910389
> -0.0203489837866662
> -0.00903421433736524
> -0.00606759654633907
> 0.021822152468108
> 0.0130902535286228
> 0.0244946768095426
> 0.0100338128603792
> 0.0116634103159661
> 0.00877875989422759
> 0.0000703637249365198
> 0.0125690151034409
> 0.0184204645372624
> 0.0220488712992349
> 0.000591706299661787
> 0.0164306631895483
> 0.00206254560008572
> 0.0254351186356061
> 0.0377740333800102
> 0.0498183694497477
> 0.0730900322649316
> 0.0626167598019577
> 0.0413330977416078
> 0.0409389427775701
> 0.0410079788912754
> 0.0214697555903696
> 0.0293686269576825
> 0.0245713553667995
> 0.0227810613388445
> 0.014757015260816
> -0.0409589657097869
> -0.043682122078613
> -0.0231490829082803
> -0.058180749212076
> -0.070277452828779
> -0.0559882061674914
> -0.0724179112178849
> -0.0921665362543513
> -0.0967138485008733
> -0.106352438904875
> -0.0703822077094837
> -0.0647866163739199
> -0.0821166825214947
> -0.059987405681383
> -0.0486376013931428
> -0.0316913240705092
> -0.0219043194192949
> -0.000156439438701518
> 0.0181152436634506
> 0.0250547364986403
> 0.0422961088361788
> 0.0392043510795345
> 0.0536182201134017
> 0.0617627815774176
> 0.0511327964870536
> 0.0344767896715694
> 0.0260320775712135
> 0.0283322944256605
> 0.0162076799591078
> 0.0125265358309556
> -0.000507445802016411
> 0.00311519454646648
> 0.0165788649636606
> 0.00739540790477403
> 0.0229987724017594
> 0.0272653705571689
> 0.0482483144964213
> 0.0522362713187687
> 0.0612273231454075
> 0.079614892079699
> 0.0739168709650611
> 0.0620841749709886
> 0.0385086318404381
> 0.0374493953553757
> 0.02383716841245
> 0.0179469074388824
> 0.0131311174408541
> -0.0142577665171377
> -0.0302496240873323
> -0.0464852726804559
> -0.0360927013544968
> -0.0262785630625307
> -0.00876649075276525
> -0.0204108245819923
> -0.0317859480796069
> -0.0168290154342274
> -0.0204101482255518
> 0.0112181627878673
> -0.00619199369792913
> -0.00738531761397304
> -0.0239716380856918
> -0.0433293723054642
> -0.0501609624769156
> -0.0416210952479572
> -0.0163293002073233
> -0.0189858514564005
> -0.018563003240787
> -0.00960993500822698
> 0.0194348904578314
> 0.0106030858225463
> 0.0274941844548786
> 0.0565395782459041
> 0.0297086025847475
> 0.0450872025462802
> 0.029180144857352
> 0.0316147562936652
> 0.0281282411294734
> 0.0186383616877982
> 0.013806956028383
> 0.0291770838191394
> 0.0217363259734746
> 0.0242735676220515
> 0.060317805102058
> 0.0478266783996652
> 0.0527916859482944
> 0.0592164523218929
> 0.068112900169304
> 0.0562599024773351
> 0.0705409368299116
> 0.0420023044577606
> 0.0553699044544159
> 0.0101738473089493
> -0.0226269881754379
> -0.0369302417357992
> -0.0199498770580725
> -0.038799722614231
> -0.0477990231461685
> -0.0307550061899024
> -0.0476390067760369
> 0.0220193260408994
> 0.0123602589858067
> 0.0323737708598567
> 0.0485784325987232
> 0.0524118872253207
> 0.0691336408229493
> 0.0777385869520685
> 0.0587339764154081
> 0.0499270653764719
> 0.01942607285427
> -0.0420468010424317
> -0.0697154170637042
> -0.0470227257341761
> -0.011727508766881
> 0.00761505815984149
> -0.0349391888273791
> -0.0236491538481913
> -0.0207367839312497
> -0.0197111886183626
> 0.00470790774212368
> 0.0243431280311659
> 0.0538697025004126
> 0.0168242458216872
> -0.0368827510735768
> -0.0563511965447905
> -0.0469241865832241
> -0.0557090196270669
> -0.0520772379837632
> -0.061838678345745
> -0.0539942259526024
> -0.0517600888983463
> -0.053646459329393
> -0.0166718550915609
> 0.00894540949745497
> 0.0121763277849579
> -0.0154030786715279
> -0.0112234184789307
> 0.018305162871221
> 0.0274242327728772
> 0.0394787329335414
> 0.0367025038067987
> 0.0258956247136817
> -0.0418834346830207
> -0.0562224701335044
> -0.0717856507044386
> -0.0317854825205251
> -0.0567960476691854
> -0.0350106348605705
> -0.0392100513362182
> -0.00846570363659348
> -0.018081931742753
> -0.0200876514267731
> 0.030325793098138
> 0.0623991196418352
> 0.0853270594035215
> 0.0723314164263929
> 0.12055479257001
> 0.0943374730624208
> 0.126286951835311
> 0.0573001208546417
> 0.0701602806622471
> 0.0952942812687784
> 0.109207525964273
> 0.087655431259001
> 0.0644511487908366
> 0.0599402523634738
> 0.0264674148429949
> 0.0141067691331591
> -0.0310515927397555
> -0.0133049448250173
> -0.00603734113604771
> -0.0443050025731634
> -0.104823100611204
> -0.121353802616703
> -0.0844170238523682
> -0.0584447194831287
> -0.0537457358840582
> -0.0627263004170363
> -0.0282146202670564
> -0.00179697567966134
> -0.00149727249679974
> 0.0166804790299301
> 0.048738699150975
> 0.0988045274222837
> 0.0850203044241317
> 0.0469281628444301
> 0.0615559724214776
> 0.0512060528719422
> 0.0155233367189217
> -0.00331207125925123
> -0.0299798983006359
> -0.000520849400201929
> 0.0047397028673056
> -0.00675993926167468
> 0.0197106896877433
> 0.0392008731737378
> -0.00420964043641153
> 0.0408431401022579
> 0.0829844138555427
> 0.0771642374379233
> 0.0680414782848606
> 0.0234955813521893
> 0.0656992784290263
> 0.0397466353922986
> 0.0335913608512729
> 0.049207768559509
> 0.0911590196591109
> 0.080070196142262
> 0.110950525546965
> 0.133894512249482
> 0.101214284305352
> 0.100440139122395
> 0.0999745039971234
> 0.124777097024594
> 0.0400167471614337
> -0.0354840975596806
> -0.0686587399473948
> -0.0965789422454352
> -0.135470570282573
> -0.17608952317895
> -0.139418446149362
> -0.0959324294689482
> -0.0944573385888327
> -0.148872420983166
> -0.0996822797621049
> -0.0822238066552398
> -0.0537778262504379
> -0.0508044492403338
> -0.0377666343112742
> -0.0215219121136773
> -0.0701459731750643
> -0.109353712513496
> -0.104720463511322
> -0.0470782337508034
> -0.0300960627743637
> -0.0228997834899649
> 0.00819081314982712
> 0.0128670953611532
> -0.0527093880310906
> -0.0639501226837229
> -0.0371403838582114
> -0.0511624039759485
> -0.0613506660474488
> -0.0687907868609409
> -0.131453773187042
> -0.118604882068318
> -0.150285745909784
> -0.121170693336145
> -0.0870851362364777
> -0.0567278919745972
> -0.0321186760834058
> 0.0194288904219427
> -0.0450766387223018
> -0.0777908096899394
> -0.00534625555378733
> -0.00642984799486954
> -0.0330835782519335
> -0.0539016903613416
> -0.0752920000252378
> -0.0934443407480063
> -0.0802242354502407
> -0.111361190899406
> -0.0719861805662689
> -0.0782148590081822
> -0.0935037591500661
> -0.0814910735819452
> -0.0686949454070188
> -0.0692066674383408
> -0.0455834670207332
> -0.0356032523254427
> -0.0425766976894909
> -0.0344267991304012
> -0.02259413614009
> 0.00879708112142963
> 0.0102330008462051
> 0.016099167641728
> 0.0337375830897265
> 0.0418695045305513
> 0.0321835093501173
> 0.0316609376506638
> 0.033525984913429
> 0.0329963427270806
> 0.0174874016415151
> 0.0188989868390566
> 0.0219406275848433
> 0.0395589630446658
> 0.0414916416086927
> 0.0455651732197011
> 0.0429982062392956
> 0.032017437799607
> 0.0340057610788791
> 0.0532582520356802
> 0.0678104064420587
> 0.0345906900297282
> 0.0193051570359774
> 0.00441290530831409
> -0.00719942754585649
> -0.021750330917075
> 0.00247737177790823
> 0.0222067488668969
> 0.0168862065643007
> 0.00157324619561995
> -0.00752680097172499
> 0.018105621765735
> 0.0486402047824274
> 0.0469303820014694
> 0.0202982683584294
> 0.035925214357101
> 0.0220915770174723
> 0.0146803723685161
> 0.0184782609695955
> 0.0353455777534181
> 0.0414706512745974
> 0.0321020941451052
> 0.0221434404147187
> 0.0209365440178037
> 0.0634377191527466
> 0.0427789341821171
> 0.0566841711436273
> 0.0607354294825107
> 0.0523694293612404
> 0.0295419343773499
> 0.0052901786146529
> 0.054429475384987
> 0.0484932609129528
> 0.0512063763030921
> 0.0540417371546652
> 0.0486317062779277
> 0.0217854270342359
> -0.0124990968737376
> -0.0247121999591309
> -0.02954935701491
> -0.0236776005098237
> -0.0310948425576398
> -0.0415528959947739
> -0.0533595816290475
> -0.0685526845817749
> -0.0440402343565194
> -0.0330651925801034
> -0.0124444772020877
> 0.0171573458769561
> 0.00593525466082811
> 0.0318032338320352
> 0.00862163862954523
> 0.0280911355435123
> 0.0444476291746674
> 0.0520164250279893
> 0.0595626006246878
> 0.0450174949391093
> 0.0535921484805668
> 0.0333503155903943
> 0.0633434281517652
> 0.05865800812284
> 0.0434236397444549
> 0.0475399463382077
> 0.0465622381368595
> 0.0309845734302321
> 0.0254541665116078
> 0.0388123891602259
> 0.0405263892166495
> 0.0531598418868538
> 0.0393067907127156
> 0.0390712268091598
> 0.0587974839803665
> 0.0654897491581031
> 0.0436528862378961
> 0.039069997154978
> 0.0305847078701617
> 0.0374689123243826
> 0.0309667480525049
> 0.0416046348836725
> 0.0498069330782978
> 0.0261267598418491
> 0.0243181558364794
> 0.00890748682120465
> 0.0172945956975921
> 0.0330687407633573
> 0.0466955586347823
> 0.0450234578772556
> 0.0336367943464907
> 0.0102751805186665
> 0.0166150025989008
> 0.0203865823377133
> 0.0233707161028239
> 0.00743416421077113
> 0.0311309527512708
> 0.0152998303539716
> -0.000479905355068668
> 0.0138883977139471
> 0.026229115528762
> 0.0439763712901041
> 0.0556780479051166
> 0.0771336409699792
> 0.0593328745311444
> 0.0748943461340025
> 0.0465568426862022
> 0.050825208501963
> 0.0540269324782439
> 0.0593622950094857
> 0.0632911605361609
> 0.0468417195097956
> 0.0282924148890151
> 0.0330874763025653
> 0.0000873233620052202
> -0.0296444926605165
> -0.0250636218959374
> -0.0103273923951745
> -0.0230397457349108
> -0.0308054048941133
> -0.0241377848782506
> -0.0141262122689831
> -0.0214752059978344
> -0.0265784265290786
> 0.0275537755533899
> 0.0532513256320583
> 0.0633924278108002
> 0.0544927250783672
> 0.0811543578848325
> 0.0549225413233136
> 0.01983208870261
> 0.0216483228713911
> 0.0397764845635498
> 0.0332653128921341
> 0.00955349467047264
> 0.0224814576240898
> 0.00109721265616683
> -0.0151976633060313
> -0.0436390823047995
> -0.0206746933501693
> 0.00636647290535453
> 0.018392645228376
> 0.00269954668884689
> -0.00197094338103599
> 0.0332362066680741
> 0.0191188188255913
> 0.0499842646754116
> 0.0836120700368248
> 0.0976017392458299
> 0.0517694809051194
> 0.0555920746975632
> 0.0626825733967978
> 0.0605996096076369
> 0.0695821303945882
> 0.0568376210590562
> 0.0651956511224986
> 0.0679423837283832
> 0.0311753923152194
> 0.0205990013632923
> 0.0678626498953341
> 0.0237819434466543
> -0.00827376011117308
> 0.00549837114605166
> -0.0155480940065122
> 0.00113977241029415
> -0.00621933220878823
> -0.0355798036843642
> 0.0449111550281341
> -0.0331780446237832
> -0.0906285833853521
> -0.0393196806527974
> -0.0758674394779039
> -0.082302980503291
> -0.0473005855329637
> -0.0860319232889494
> -0.0997100915063887
> -0.100737536978816
> -0.16386207090062
> -0.0869245837795541
> -0.0334157767155726
> -0.0646797830402803
> -0.0412774637805753
> -0.0397571446428294
> -0.0626799305842094
> -0.0509299389568716
> -0.0370612712949267
> -0.000784204400508157
> 0.00824833653451254
> 0.0305050123889597
> 0.0260593029646707
> 0.0225068160255632
> 0.0579714302689207
> 0.0497842993189579
> 0.0508293723866183
> 0.0579792252334706
> 0.0660679485452765
> 0.0454542841285013
> 0.0666930292025197
> 0.0717434417250793
> 0.0569664274548257
> 0.0433579011329155
> 0.0128182822814583
> 0.000930108860487405
> 0.00512694011165713
> -0.0166472977614191
> -0.0176413367780662
> -0.0229658866775795
> -0.0373600013448568
> -0.0434967523358232
> -0.0131997937566038
> -0.000276861898020738
> 0.0135426763409453
> 0.0335429250274779
> -0.000928769815406545
> 0.00358143121995404
> -0.0437264490225357
> -0.0287707725358678
> -0.0297576170979456
> -0.0219725864872455
> -0.0612757405931867
> -0.0628549296140502
> -0.0683890342482978
> -0.0971057029043395
> -0.0622074384064605
> -0.0635604695246604
> -0.0370784581280732
> -0.0261639774672935
> -0.0314131798101662
> -0.0521493491891009
> -0.0598030218836947
> -0.0478733590161418
> -0.0329309543360827
> -0.00425397278130081
> -0.00873827549155742
> 0.00842045661286469
> 0.0179682986092862
> -0.01617610327862
> 0.0169368054609033
> 0.0448694831556985
> 0.0493422820637444
> 0.0488474994497774
> 0.0511490408552908
> 0.0613070971178213
> 0.0581587832387218
> 0.066224194521728
> 0.0825047549401932
> 0.0828249259345266
> 0.0544081964257375
> 0.0335524541228679
> 0.0624353441186675
> 0.0838278756752873
> 0.0860709715424518
> 0.0759162427686413
> 0.0856203360587158
> 0.0626923202611311
> 0.047995340384354
> 0.0655538643520662
> 0.064586445608614
> 0.0564674889944966
> 0.0399131987082044
> 0.0361768269194446
> 0.0328281311729577
> 0.0207298997575043
> 0.00547222351717391
> 0.013520368824902
> 0.0259022530869652
> 0.0323947459338113
> 0.0244755730271208
> 0.0235225709105328
> 0.0248504147439559
> 0.0205442198511446
> 0.0185148637687642
> 0.0314228496699429
> 0.043283337986608
> 0.0539331054759223
> 0.0194902696329013
> 0.0112144574465596
> -0.00254790399127038
> 0.00261286732527769
> 0.02551704192317
> 0.0258161584002965
> 0.018118609221337
> 0.0229632208113428
> 0.000296802990180524
> -0.0158579687138895
> 0.0206466152775647
> 0.0174010057690892
> 0.0217932529190365
> 0.0228096096622635
> 0.011104711717928
> 0.00769558163075594
> 0.0123419496214188
> -0.00698850238204029
> 0.0212447341722667
> 0.0408214165472124
> 0.0354427984896846
> 0.0386884079981601
> 0.0607666635298219
> 0.0612173934818914
> 0.0543086600218066
> 0.0445065350589851
> 0.0449216493464967
> 0.0660140824718742
> 0.0527145054141798
> 0.0340912045521893
> 0.0160734814537149
> 0.0249206007774259
> 0.016543686900534
> 0.0250383396857708
> 0.0101428451595096
> 0.0135936363516566
> 0.0108032983634704
> 0.00316937518021952
> 0.01041979501531
> 0.0301905369416461
> 0.0509347809933131
> 0.0154885777326465
> 0.00958699053210998
> -0.0444703207587069
> -0.0262974779486795
> -0.0228782318123192
> -0.0173104455540839
> -0.00580336713880052
> -0.00270094456043619
> -0.0162952145915992
> -0.0152566806636157
> -0.0042370383613055
> 0.00143917245112596
> 0.0230274303182238
> 0.0205536127628183
> 0.0233796270744909
> 0.000796075856556123
> -0.0327354983021603
> -0.0338095301662659
> -0.0233618238188604
> -0.0457798646989035
> -0.0146868640446473
> -0.0089143568454816
> 0.0263452805417918
> 0.0301128822893173
> 0.0220607057481591
> 0.0533789102611745
> 0.0679636429801076
> 0.0761241558595997
> 0.0585842827708416
> 0.0670291357502939
> 0.0397825368059714
> 0.0134915535049497
> 0.00963305752995634
> 0.00352192034990019
> 0.0145217885003736
> 0.00673382350230353
> 0.0142134435750074
> -0.0000471063362820002
> 0.014577692865398
> 0.029172710196748
> 0.0428857125234249
> 0.0659525568152987
> 0.0541144844040005
> 0.0678339927991772
> 0.0679521133816246
> 0.0698362962679584
> 0.0642127261835075
> 0.0614640358781647
> 0.0455633912036933
> 0.02303282217204
> 0.0226331627318598
> 0.0299007842466193
> 0.0103759853065732
> 0.0111919955896174
> 0.00798922166507657
> 0.00359072809876016
> 0.0136238447800138
> 0.0133055607980674
> 0.0218666990339431
> 0.0475377593286258
> 0.0412290216468429
> 0.0309021594028822
> 0.0505384943060241
> 0.0484990062081858
> 0.049492476339201
> 0.0339517461793937
> 
> 
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 

-- 
View this message in context: http://www.nabble.com/odd-GARCH%281%2C1%29-results-tp22041102p22072829.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From markli.xiaochuan at gmail.com  Wed Feb 18 23:08:47 2009
From: markli.xiaochuan at gmail.com (Xiaochuan Li)
Date: Wed, 18 Feb 2009 16:08:47 -0600
Subject: [R-SIG-Finance] [R] package ccgarch - dcc.estimation
Message-ID: <2d6f32eb0902181408o71256a75ve03500a4cf511450@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090218/4c1d92fa/attachment.pl>

From rsela at stern.nyu.edu  Thu Feb 19 14:39:17 2009
From: rsela at stern.nyu.edu (Rebecca Sela)
Date: Thu, 19 Feb 2009 08:39:17 -0500 (EST)
Subject: [R-SIG-Finance] DLM and matrices with 0 eigenvalues
Message-ID: <33023485.588341235050757910.JavaMail.root@calliope.stern.nyu.edu>

I am using DLM to fit a state space model.  The covariance matrix of states (W) is given by:
a 0 a 0
0 0 0 0
a 0 a 0
0 0 0 0
where a is a parameter to be estimated.  Even though the matrix is positive semidefinite, sometimes DLM gives me an error that W is not a valid variance matrix.  As far as I can tell, the reason is that one of R's computed eigenvalues is very slightly negative (something like -5E-17).  Is there a way to work around this?

Thanks!

Rebecca


From GPetris at uark.edu  Thu Feb 19 16:49:15 2009
From: GPetris at uark.edu (Giovanni Petris)
Date: Thu, 19 Feb 2009 09:49:15 -0600 (CST)
Subject: [R-SIG-Finance] DLM and matrices with 0 eigenvalues
In-Reply-To: <33023485.588341235050757910.JavaMail.root@calliope.stern.nyu.edu>
	(message from Rebecca Sela on Thu, 19 Feb 2009 08:39:17 -0500 (EST))
References: <33023485.588341235050757910.JavaMail.root@calliope.stern.nyu.edu>
Message-ID: <200902191549.n1JFnFFb029331@definetti.ddns.uark.edu>


Hi Rebecca,

Are you using package dlm? If so, what are the functions that give you
an error? Could you provide a minimal working example, as required by
the posting guide?

Also, (1) please do not post to both R-help and R-SIG-Finance; (2)
questions about contributed packages should be addressed to the
package maintainer.

Best,
Giovanni

> Date: Thu, 19 Feb 2009 08:39:17 -0500 (EST)
> From: Rebecca Sela <rsela at stern.nyu.edu>
> Sender: r-sig-finance-bounces at stat.math.ethz.ch
> Precedence: list
> 
> I am using DLM to fit a state space model.  The covariance matrix of states (W) is given by:
> a 0 a 0
> 0 0 0 0
> a 0 a 0
> 0 0 0 0
> where a is a parameter to be estimated.  Even though the matrix is positive semidefinite, sometimes DLM gives me an error that W is not a valid variance matrix.  As far as I can tell, the reason is that one of R's computed eigenvalues is very slightly negative (something like -5E-17).  Is there a way to work around this?
> 
> Thanks!
> 
> Rebecca
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 

Giovanni Petris  <GPetris at uark.edu>
Associate Professor
Department of Mathematical Sciences
University of Arkansas - Fayetteville, AR 72701
Ph: (479) 575-6324, 575-8630 (fax)
http://definetti.uark.edu/~gpetris/


From Reena.Bansal at moorecap.com  Thu Feb 19 20:41:45 2009
From: Reena.Bansal at moorecap.com (Reena Bansal)
Date: Thu, 19 Feb 2009 14:41:45 -0500
Subject: [R-SIG-Finance] Black Litterman portfolio optimization
Message-ID: <4AAD56F399C8564C9EB6817C17618CDD0251E822@NYC-XCH3.win.moorecap.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090219/37e76f24/attachment.pl>

From dutt.debashis at gmail.com  Thu Feb 19 21:16:39 2009
From: dutt.debashis at gmail.com (Debashis Dutta)
Date: Thu, 19 Feb 2009 23:16:39 +0300
Subject: [R-SIG-Finance] Black Litterman portfolio optimization
In-Reply-To: <4AAD56F399C8564C9EB6817C17618CDD0251E822@NYC-XCH3.win.moorecap.com>
References: <4AAD56F399C8564C9EB6817C17618CDD0251E822@NYC-XCH3.win.moorecap.com>
Message-ID: <37673c2d0902191216k6de6086en91b016c5ad2c2a47@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090219/9bc8ba51/attachment.pl>

From klein82517 at yahoo.de  Fri Feb 20 08:12:49 2009
From: klein82517 at yahoo.de (Andreas Klein)
Date: Fri, 20 Feb 2009 07:12:49 +0000 (GMT)
Subject: [R-SIG-Finance] TAR Models and predictive residuals (Tsay, 1989)
In-Reply-To: <4AAD56F399C8564C9EB6817C17618CDD0251E822@NYC-XCH3.win.moorecap.com>
Message-ID: <97388.59053.qm@web24208.mail.ird.yahoo.com>

Hello R users.

There is a paper from Ruey Tsay with the title: "Testing and Modelling Threshold Autoregressive Processes", published in 1989 in the Journal of the American Statistical Association (March, Vol. 84, No. 405).

Mr. Tsay describes a very interesting way of identifying and modelling threshold AR processes.

1. Is there a package in R or some routines, which implements his ideas and his methodology?

2. Is there a routine in R to calculate the predictive residuals (like defined in the paper)?


Thanks in advance.

Regards,
Andreas.





From matthieu.stigler at gmail.com  Fri Feb 20 08:27:36 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Fri, 20 Feb 2009 12:57:36 +0530
Subject: [R-SIG-Finance] TAR Models and predictive residuals (Tsay, 1989)
In-Reply-To: <97388.59053.qm@web24208.mail.ird.yahoo.com>
References: <4AAD56F399C8564C9EB6817C17618CDD0251E822@NYC-XCH3.win.moorecap.com>
	<97388.59053.qm@web24208.mail.ird.yahoo.com>
Message-ID: <111060c20902192327w77f205b8le5d9ae91bd35173d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090220/76f03535/attachment.pl>

From fgochez at mango-solutions.com  Fri Feb 20 10:19:01 2009
From: fgochez at mango-solutions.com (Francisco Gochez)
Date: Fri, 20 Feb 2009 09:19:01 -0000
Subject: [R-SIG-Finance] Black Litterman portfolio optimization
In-Reply-To: <4AAD56F399C8564C9EB6817C17618CDD0251E822@NYC-XCH3.win.moorecap.com>
References: <4AAD56F399C8564C9EB6817C17618CDD0251E822@NYC-XCH3.win.moorecap.com>
Message-ID: <3CBFCFB1FEFFA841BA83ADF2F2A9C6FA2F67F9@mango-data1.Mango.local>

Hi Reena,

As mentioned by Debashis, I am the author of this package, which is
still under development.  I have not had much time to move beyond
version 0.2.2 until now, but I plan to release another update fairly
soon.  In particular, I am planning to make it easier to use the other
Rmetrics tools for portfolio optimization together with the results
produced by the prior/posterior blending routines (expected shortfall
optimization for copula opinion pooling should be part of this).  In the
meantime, I welcome any questions or feedback you might have about the
package.

Regards,

Francisco


Mango Solutions

S & R Consulting and Training

+44 (0)1249 767 700


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Reena
Bansal
Sent: 19 February 2009 19:42
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Black Litterman portfolio optimization

Hi All -

I am looking for Black Litterman CAPM portfolio optimization
implementation in R. 

Reference
http://corporate.morningstar.com/ib/documents/MethodologyDocuments/IBBAs
sociates/BlackLitterman.pdf

I came across the beta release of BLCOP package. Has anybody used this
package and is there any other implementation of the above in R?

Thanks,
Reena

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From Jose at erini.ac.uk  Sun Feb 22 19:02:53 2009
From: Jose at erini.ac.uk (Jose Iparraguirre D'Elia)
Date: Sun, 22 Feb 2009 18:02:53 -0000
Subject: [R-SIG-Finance] Newey-West Long-run variance
Message-ID: <C9328F0EEDC3BC439FDABD12060E91090E2D4F@erini1.ERINI.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090222/cd72b041/attachment.pl>

From matthieu.stigler at gmail.com  Sun Feb 22 19:03:25 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Sun, 22 Feb 2009 23:33:25 +0530
Subject: [R-SIG-Finance] Newey-West Long-run variance
In-Reply-To: <C9328F0EEDC3BC439FDABD12060E91090E2D4F@erini1.ERINI.local>
References: <C9328F0EEDC3BC439FDABD12060E91090E2D4F@erini1.ERINI.local>
Message-ID: <49A1936D.6020008@gmail.com>

see package sandwich

Jose Iparraguirre D'Elia a ?crit :
> In S-plus + FinMetrics, there is a function (asymp.var) to compute the nonparametric Newey-West long-run variance of a time series (see example in Modelling Financial Time Series with S-PLUS by Eric Zivot and Jiahui Wang, Section 3.2.8).
> Is there a similar function in R? The NW estimator is used, for example, to obtain the variance as part of the Phillips-Perron unit root test in the tseries package, but how can that estimate be obtained? Has anyone written a code in R that does what asymp.var does in S-Plus? Regards,
> Jose
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From wuertz at itp.phys.ethz.ch  Mon Feb 23 09:05:01 2009
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon, 23 Feb 2009 09:05:01 +0100
Subject: [R-SIG-Finance] R/Rmetrics Workshop: Computational Finance and
	Financial Engineering
In-Reply-To: <49A1936D.6020008@gmail.com>
References: <C9328F0EEDC3BC439FDABD12060E91090E2D4F@erini1.ERINI.local>
	<49A1936D.6020008@gmail.com>
Message-ID: <49A258AD.4040202@itp.phys.ethz.ch>

Computational Finance and Financial Engineering
Third R/Rmetrics User and Developer Workshop 2009
Meielisalp, Lake Thune, Switzerland, June 28th to July 2nd


The great interest in the R/Rmetrics software environment and the success of the previous Rmetrics user and developer workshop on "Computational Finance and Financial Engineering" has encouraged us to organize the third Rmetrics workshop in 2009, June 28th - July 2nd, at Meielisalp, Lake Thune, Switzerland.

We cordially invite you to participate and contribute in this summer school like workshop, a workshop made by R/Rmetrics users for R/Rmetrics users.

The Workshop focuses on using R/Rmetrics as the premier open source solution for financial market analysis, valuation of financial instruments, and insurance tasks.  We provide a platform for R/Rmetrics users to discuss and exchange ideas of how R and Rmetrics can be used to perform computations, data analysis, and visualization in finance and insurance, and we give an overview of the new features of the rapidly evolving R/Rmetrics project and discuss future developments.

The program consists of presentations of new R/Rmetrics directions and developments through keynote lectures, user-contributed presentations reflecting the wide range of fields in which R and Rmetrics are used in finance and insurance to analyze and model data. We want to bring together developers, practitioners, and users from finance and insurance, and to provide a platform for common discussions and exchange of ideas.

On behalf of the organizing committee of the workshop:
Diethelm Wuertz, ETH Zurich, 
David Scott, University of Auckland, NZ

Local Organization:
Rmetrics Association, Finance Online, ETH Zurich
Yohan Chalabi, Andrew Ellis


More information can be found on the Rmetrics home page

             "www.rmetrics.org/meielisalp" 

Furthermore, we would kindly like to ask you to make the workshop public at your
institute, department, university or company.  A printable PDF poster can be
downloaded from www.rmetrics.org.  Please note that we also have a limited
number of scholarships for bachelor and master students.


From albertosantini at gmail.com  Mon Feb 23 13:19:49 2009
From: albertosantini at gmail.com (Alberto Santini)
Date: Mon, 23 Feb 2009 04:19:49 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Black Litterman portfolio
	optimization
In-Reply-To: <4AAD56F399C8564C9EB6817C17618CDD0251E822@NYC-XCH3.win.moorecap.com>
References: <4AAD56F399C8564C9EB6817C17618CDD0251E822@NYC-XCH3.win.moorecap.com>
Message-ID: <22160510.post@talk.nabble.com>




Reena Bansal wrote:
> 
> I am looking for Black Litterman CAPM portfolio optimization
> implementation in R. 
> 

http://www.blacklitterman.org/ contains matlab implementation and in-depth
paper details.

Regards,
Alberto

-- 
View this message in context: http://www.nabble.com/Black-Litterman-portfolio-optimization-tp22108227p22160510.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From edd at debian.org  Tue Feb 24 03:36:41 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 23 Feb 2009 20:36:41 -0600
Subject: [R-SIG-Finance] R/Finance 2009: Applied Finance with R --
	Registration now open
Message-ID: <18851.23865.894401.880575@ron.nulle.part>


   R/Finance 2009: Applied Finance with R
   April 24 & 25, Chicago, IL, US
  
   The first annual R/Finance conference for applied finance using R, the
   premier free software system for statistical computation and graphics,
   will be held this spring in Chicago, IL, USA on Friday April 24 and
   Saturday April 25.

   The two-day conference will cover topics as diverse as portfolio theory,
   time-series analysis, as well as advanced risk tools, high-performance
   computing, and econometrics. All will be discussed within the context of
   using R as a primary tool for financial risk management and trading.

   Assembled to talk over the two days are some of the industry's most
   recognizable authorities within the world of R and quantitative finance.

   R/Finance 2009 is organized by a leading group of R package authors and
   community contributors, and hosted by the International Center for Futures
   and Derivatives [ICFD] at the University of Illinois at Chicago.

   Conference registration is now open. Special advanced registration pricing is
   available, as well as discounted pricing for academic and student
   registrations. 

   More details and registration information can be found at the website at  

        	       http://www.RinFinance.com

   For the program committee:

        Gib Bassett, Peter Carl, Dirk Eddelbuettel, John Miller,
        Brian Peterson, Dale Rosenthal, Jeffrey Ryan



-- 
Three out of two people have difficulties with fractions.


From klein82517 at yahoo.de  Tue Feb 24 10:16:28 2009
From: klein82517 at yahoo.de (Andreas Klein)
Date: Tue, 24 Feb 2009 09:16:28 +0000 (GMT)
Subject: [R-SIG-Finance] Appropriate model to data?
Message-ID: <363002.5275.qm@web24205.mail.ird.yahoo.com>

Dear R Users.

In the attachment I have a *.txt file with two datasets:
- present_data (#246)
- future_data (#39)

I tried to model the present_data to simulate the future_data (scenario-simulation with Monte Carlo based on 100.000 cases), so the forecast will be the 99% and the 99.9% quantile of all the simulations based on the model of the present_data. My aim is to reach the peak at the end of the future_data.

The different unit root tests give hints for stationary, but some also showed a unit root. I worked with both.

- I fitted an AR(1) model.

- I fitted an ARIMA(1,1,1) model, but I couldn't verify the model and the coefficients with bootstrap. The model was unstable while resampling with replacement, which is an indicator for overdifferencing of the series in my oppinion, so the series has no unit root.

- I fitted a threshold autoregressive model with 2 regimes and a threshold variable of 0.02259983 and the order of the lower and upper regime of 1.

- I tried to model a GARCH(1,1) model, but I got partially insignificant coefficients.

In all four cases I was not able to reach the peak of the future_data with the above described simulation type.

The innovations were always non-normal.
So as innovation distribution for simulation I used a not justified normal distribution, a more justified skewed-t-distribution and the empirical distribution obtained with bootstrapping.



I would be glad if anyone could have a look at the data and give me some hints what models I can try next, which will give me more succes in my attempt to find an appropriate model for the given present_data in reaching my aim!



Thanks in advance.



Regards,
Andreas.


      
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: data.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090224/a92b3b98/attachment.txt>

From Jose at erini.ac.uk  Tue Feb 24 12:54:20 2009
From: Jose at erini.ac.uk (Jose Iparraguirre D'Elia)
Date: Tue, 24 Feb 2009 11:54:20 -0000
Subject: [R-SIG-Finance] Newey-West Long-run variance
Message-ID: <C9328F0EEDC3BC439FDABD12060E91098E1793@erini1.ERINI.local>

Thanks Matthieu. I had been poring over the sandwich library, but can't get around to estimating what I need. 
Could you or any one else in the list help me with the following? 
I am reproducing here Example 9 (Long-run variance of AR(1)) from Zivot and Wang (2001). They use the function asymp.var which unfortunately doesn't seem to be available in R (if anyone has an R code for this function, I'd be greatly appreciate it).
How can I replicate these results in R? In particular, the Newey-West estimate for the long-run variance of the series (=0.269)? 
Perhaps the answer lies within the package sandwich, but I can't get through the crust enough to find any pleasure yet...
Thanks, in advance,
Jos?


Example 9 - Long-run variance of AR(1)

Let yt be an AR(1) process created using

> set.seed(101)
> e = rnorm(100,sd=1)
> y.ar1 = 1 + arima.sim(model=list(ar=0.75),innov=e)

Here ?(1) = 1/?(1) = 1/1?? 

and

lrv(yt)= ?2/(1 ? ?)2 

For ? =0.75, ?2 =1, lrv(yt)=16 implies for T = 100 an asymptotic
standard error for ? y equal to SE(?y)=0.40.If yt ~ WN(0, 1), then the
asymptotic standard error for ?y is SE(?y)=0.10.

lrvAR(yt) may be easily computed in S-PLUS using OLS to estimate the
AR(1) parameters:

> ar1.fit = OLS(y.ar1~ar(1))
> rho.hat = coef(ar1.fit)[2]
> sig2.hat = sum(residuals(ar1.fit)^2)/ar1.fit$df.resid
> lrv.ar1 = sig2.hat/1-rho.hat)^2
> as.numeric(lrv.ar1)
[1] 13.75

Here lrvAR(yt)=13.75, and an estimate for SE(?y) is SEAR(?y)=0.371.

The S+FinMetrics function asymp.var may be used to compute the
nonparameteric Newey-West estimate lrvNW(yt). The arguments expected
by asymp.var are

> args(asymp.var)
function(x, bandwidth, window = "bartlett", na.rm = F)
where x is a ?timeSeries?, bandwidth sets the truncation lag MT in (3.20)
and window specifies the weight function. Newey and West suggest setting
the bandwidth using the sample size dependent rule
MT =4(T/100)^2/9, which is equal to 4 in the present case. 

The Newey-West long-run variance estimate is then
> lrv.nw = asymp.var(y.ar1, bandwidth=4)
> lrv.nw
[1] 7.238

and the Newey-West estimate of SE(?y) is SENW(?y)=0.269.

Reference:

Zivot, E. and Wang, J. (2002). Modelling Financial Time Series with
S-PLUS. 



-----Original Message-----
From: Matthieu Stigler [mailto:matthieu.stigler at gmail.com] 
Sent: 22 February 2009 18:03
To: Jose Iparraguirre D'Elia
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Newey-West Long-run variance

see package sandwich

Jose Iparraguirre D'Elia a ?crit :
> In S-plus + FinMetrics, there is a function (asymp.var) to compute the nonparametric Newey-West long-run variance of a time series (see example in Modelling Financial Time Series with S-PLUS by Eric Zivot and Jiahui Wang, Section 3.2.8).
> Is there a similar function in R? The NW estimator is used, for example, to obtain the variance as part of the Phillips-Perron unit root test in the tseries package, but how can that estimate be obtained? Has anyone written a code in R that does what asymp.var does in S-Plus? Regards,
> Jose
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>   


From shimrit.sabraham at gmail.com  Tue Feb 24 13:49:47 2009
From: shimrit.sabraham at gmail.com (Shimrit Abraham)
Date: Tue, 24 Feb 2009 12:49:47 +0000
Subject: [R-SIG-Finance] Tracing gradient during optimization
Message-ID: <36da694c0902240449hb62dca7md7f8ba04241f1189@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090224/f80d7ae9/attachment.pl>

From edd at debian.org  Tue Feb 24 14:10:57 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 24 Feb 2009 07:10:57 -0600
Subject: [R-SIG-Finance] Tracing gradient during optimization
In-Reply-To: <36da694c0902240449hb62dca7md7f8ba04241f1189@mail.gmail.com>
References: <36da694c0902240449hb62dca7md7f8ba04241f1189@mail.gmail.com>
Message-ID: <18851.61921.979141.463707@ron.nulle.part>


On 24 February 2009 at 12:49, Shimrit Abraham wrote:
| I am currently using the function optim() to maximize/minimize functions and
| I would like to see more output of the optimization procedure, in particular

We encourage you to send non-finance questions to the r-help list.

Dirk

-- 
Three out of two people have difficulties with fractions.


From gmain.20.phftt at xoxy.net  Tue Feb 24 16:14:16 2009
From: gmain.20.phftt at xoxy.net (Rob Steele)
Date: Tue, 24 Feb 2009 10:14:16 -0500
Subject: [R-SIG-Finance] Tracing gradient during optimization
In-Reply-To: <36da694c0902240449hb62dca7md7f8ba04241f1189@mail.gmail.com>
References: <36da694c0902240449hb62dca7md7f8ba04241f1189@mail.gmail.com>
Message-ID: <49A40EC8.7090403@xoxy.net>

Enclose your objective function in an environment that contains the
state you want to track and some way to get at it.  This lets you track
as much detail as you like.  Something like:


make.objective <- function(blah, blah, ...)
{
    log <- list()

    fn <- function(params)
    {
        log[[length(log) + 1]] <<- <state info>
        fitness <- <...>
        return(fitness)
    }

    print.log <- funciton()
    {
        ...
    }

    plot.log <- function()
    {
        ....
    }

    return(list(fn        = fn,
                print.log = print.log,
                plot.log  = plot.log))
}


objective <- make.objective(...)

o <- optim(fn = objective$fn, ...)

objective$print.log()
objective$plot.log()


By the way, it's more efficient to pre-allocate log space in a data
frame and only grow it occasionally rather than to add elements to a
list one at a time as I show here.  This works however.

Good hunting!


Shimrit Abraham wrote:
> Hi everyone,
> 
> I am currently using the function optim() to maximize/minimize functions and
> I would like to see more output of the optimization procedure, in particular
> the numerical gradient of the parameter vector during each iteration.
> The documentation of optim() describes that the trace parameter should allow
> one to trace the progress of the optimization.
> I use the following command:
> 
> optim(par = vPar,
>          fn = calcLogLik,
>          method = "BFGS",
>          control = list(trace = TRUE, fnscale = -1, maxit = 2000));
> 
> which gives very little information:
> 
> initial  value 3.056998
> final  value 2.978351
> converged
> 
> Specifying trace >1, for instance trace = 20, does not result in more
> information. Is there a way to view more details of the progress perhaps by
> using another optimizer?
> 
> Thanks,
> 
> Shimrit Abraham
> 
> 	[[alternative HTML version deleted]]
>


From yamacorp at gmail.com  Tue Feb 24 16:54:09 2009
From: yamacorp at gmail.com (Al)
Date: Tue, 24 Feb 2009 16:54:09 +0100
Subject: [R-SIG-Finance] R/Finance 2009: Applied Finance with R --
	Registration now open
In-Reply-To: <18851.23865.894401.880575@ron.nulle.part>
References: <18851.23865.894401.880575@ron.nulle.part>
Message-ID: <9e850ab70902240754i240c9c17sfd2e0042a7370520@mail.gmail.com>

i sadly will not be able to attend ; do you plan on recording videos
of the tutorials for online viewing by any chance?

Best Regards,
Alex

On 2/24/09, Dirk Eddelbuettel <edd at debian.org> wrote:
>
>    R/Finance 2009: Applied Finance with R
>    April 24 & 25, Chicago, IL, US
>
>    The first annual R/Finance conference for applied finance using R, the
>    premier free software system for statistical computation and graphics,
>    will be held this spring in Chicago, IL, USA on Friday April 24 and
>    Saturday April 25.
>
>    The two-day conference will cover topics as diverse as portfolio theory,
>    time-series analysis, as well as advanced risk tools, high-performance
>    computing, and econometrics. All will be discussed within the context of
>    using R as a primary tool for financial risk management and trading.
>
>    Assembled to talk over the two days are some of the industry's most
>    recognizable authorities within the world of R and quantitative finance.
>
>    R/Finance 2009 is organized by a leading group of R package authors and
>    community contributors, and hosted by the International Center for
> Futures
>    and Derivatives [ICFD] at the University of Illinois at Chicago.
>
>    Conference registration is now open. Special advanced registration
> pricing is
>    available, as well as discounted pricing for academic and student
>    registrations.
>
>    More details and registration information can be found at the website at
>
>         	       http://www.RinFinance.com
>
>    For the program committee:
>
>         Gib Bassett, Peter Carl, Dirk Eddelbuettel, John Miller,
>         Brian Peterson, Dale Rosenthal, Jeffrey Ryan
>
>
>
> --
> Three out of two people have difficulties with fractions.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From sue at xlsolutions-corp.com  Tue Feb 24 16:56:29 2009
From: sue at xlsolutions-corp.com (Sue Turner)
Date: Tue, 24 Feb 2009 08:56:29 -0700
Subject: [R-SIG-Finance] Course - March/April ** R / Splus ** course in New
 York City *** by XLSolutions Corp
Message-ID: <20090224085629.aa8924c5d28ca71e2a043bb294e795eb.d8197ca976.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to announce
our*** R/Splus Fundamentals and Programming Techniques and  R Advanced
Programming***courses at USA locations for March - April 2009.


********* New York City  ********** March 19-20, 2009
********* Seattle ***************** April
********* Salt Lake City ********** April
********* San Francisco *********** April

R/Splus Fundamentals and Programming Techniques
http://www.xlsolutions-corp.com/rplus.asp

Looking for   R/Splus Advanced Programming  ?

http://www.xlsolutions-corp.com/rplus.asp 

********* San Francisco  ********** April 27-28, 2009
********* Boston         ********** April 22-23, 2009
********* New York City  ********** April 20-21, 2009


Ask for group discount and reserve your seat Now - Earlybird Rates.
Payment due after the class! Email Sue Turner:  sue at xlsolutions-corp.com

Phone: 206-686-1578


Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat!

Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com


From rechtsteiner at bgki.net  Tue Feb 24 17:02:43 2009
From: rechtsteiner at bgki.net (rechtsteiner at bgki.net)
Date: Tue, 24 Feb 2009 17:02:43 +0100 (CET)
Subject: [R-SIG-Finance] mean reverting model
Message-ID: <56263.212.147.43.113.1235491363.squirrel@mail.xnos.de>

dear list members,

i'm trying to estimate the following two regressions:

(I) v_t - v_t-1 = c*(v_t-1 - v_f) + s_t

where v_t and v_t-1 are market volatility at t resp. t-1; c ist the
persistence volatility parameter, v_f the long-run fundamental volatility
and s_t are volatility surprises.
parameters to estimate are c and v_f.

(II) b_t(z_t) = m + f*(b_t-1(z_t) - m) + k*z_t
where the b_t(z_t) are estimated in a seperate regression;
parameters to be estimated are m, the mean reverting beta term, the
persistence parameter f and the sensitivities k of b towards z.

these equations are used in risk and beta anatomy in the hedge fund
industry (savona 2008).

my problem is, that i don't know how to estimate the two equations in R. i
have searched Rsitesearch, google,.. for many hours and didn't find
anything.

could you please give me a hint?

cheers,
josuah


From edd at debian.org  Tue Feb 24 17:29:51 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 24 Feb 2009 10:29:51 -0600
Subject: [R-SIG-Finance] Course - March/April ** R / Splus ** course in
 New York City *** by XLSolutions Corp
In-Reply-To: <20090224085629.aa8924c5d28ca71e2a043bb294e795eb.d8197ca976.wbe@email.secureserver.net>
References: <20090224085629.aa8924c5d28ca71e2a043bb294e795eb.d8197ca976.wbe@email.secureserver.net>
Message-ID: <18852.8319.814898.604950@ron.nulle.part>


On 24 February 2009 at 08:56, Sue Turner wrote:
| XLSolutions Corporation (www.xlsolutions-corp.com) is proud to announce
| our*** R/Splus Fundamentals and Programming Techniques and  R Advanced
| Programming***courses at USA locations for March - April 2009.

Please keep unsolicited commercial advertisements off r-sig-finance.  Readers
will have no difficulties finding them on r-help.

Dirk

-- 
Three out of two people have difficulties with fractions.


From fgochez at mango-solutions.com  Tue Feb 24 18:12:11 2009
From: fgochez at mango-solutions.com (Francisco Gochez)
Date: Tue, 24 Feb 2009 17:12:11 -0000
Subject: [R-SIG-Finance] London useR group
Message-ID: <3CBFCFB1FEFFA841BA83ADF2F2A9C6FA2F6940@mango-data1.Mango.local>

All

Mango Solutions are pleased to announce the first meeting of the London
useR Group. 

Date:                                Tuesday 31st March

Time:                                4pm to 7pm 

Venue:                              The Wall

45 Old Broad St

London

EC2N 1HU

Tel 020 7588 4845

Speakers will include

*	David Jessop            UBS 
*	Markus Gesmann       Lloyd's of London 
*	Pat Burns                Burns Statistics 
*	Rory Winston           The Research kitchen Ltd 

This is a great opportunity to meet and mingle with other R users in the
London area. Please send an email to marketing at mango-solutions.com
<mailto:marketing at mango-solutions.com>  to reserve your seat or to
request any further information. 

We will be seeking to make the meeting a regular affair and would
welcome ideas on structure and topics to be covered.

 

Kind regards

Sharon


*	mango solutions  
*	Tel   +44 (0)1249 767700 
*	Mob +44 (0)7966 062462  
*	Fax  +44 (0)1249 767707 

        data analysis that delivers


From brian at braverock.com  Tue Feb 24 18:27:14 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 24 Feb 2009 11:27:14 -0600
Subject: [R-SIG-Finance] mean reverting model
In-Reply-To: <56263.212.147.43.113.1235491363.squirrel@mail.xnos.de>
References: <56263.212.147.43.113.1235491363.squirrel@mail.xnos.de>
Message-ID: <49A42DF2.9070506@braverock.com>

rechtsteiner at bgki.net wrote:
> dear list members,
>
> i'm trying to estimate the following two regressions:
>
> (I) v_t - v_t-1 = c*(v_t-1 - v_f) + s_t
>
> where v_t and v_t-1 are market volatility at t resp. t-1; c ist the
> persistence volatility parameter, v_f the long-run fundamental volatility
> and s_t are volatility surprises.
> parameters to estimate are c and v_f.
>
> (II) b_t(z_t) = m + f*(b_t-1(z_t) - m) + k*z_t
> where the b_t(z_t) are estimated in a seperate regression;
> parameters to be estimated are m, the mean reverting beta term, the
> persistence parameter f and the sensitivities k of b towards z.
>
> these equations are used in risk and beta anatomy in the hedge fund
> industry (savona 2008).
>
> my problem is, that i don't know how to estimate the two equations in R. i
> have searched Rsitesearch, google,.. for many hours and didn't find
> anything.
>
> could you please give me a hint?
>   

Sure, there are several books on regression models in R.  Start there.

Regards,

  - Brian


From bearxu83 at gmail.com  Wed Feb 25 16:11:04 2009
From: bearxu83 at gmail.com (BearXu)
Date: Wed, 25 Feb 2009 15:11:04 +0000
Subject: [R-SIG-Finance] get data in quantmod
Message-ID: <82527b5d0902250711v25830212l19990cbbf5f8a698@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090225/f77d6ff5/attachment.pl>

From B_Rowe at ml.com  Wed Feb 25 16:30:05 2009
From: B_Rowe at ml.com (Rowe, Brian Lee Yung (Portfolio Analytics))
Date: Wed, 25 Feb 2009 10:30:05 -0500
Subject: [R-SIG-Finance] get data in quantmod
In-Reply-To: <82527b5d0902250711v25830212l19990cbbf5f8a698@mail.gmail.com>
Message-ID: <3BAD818D9407B043817CC6D89ABA14EC032B8342@MLNYC20MB051.amrs.win.ml.com>

1: From the code for getSymbols.yahoo this appears to be hardcoded. The
parameter to set the series frequency on yahoo is 'g':

http://chart.yahoo.com/table.csv?s=FXP&a=0&b=01&c=2007&d=1&e=25&f=2009&g
=m&q=q&y=0&z=FXP&x=.csv - returns monthly

http://chart.yahoo.com/table.csv?s=FXP&a=0&b=01&c=2007&d=1&e=25&f=2009&g
=d&q=q&y=0&z=FXP&x=.csv - returns daily

Here is the code in getSymbols.yahoo:
        download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=",
            from.m, "&b=", sprintf("%.2d", from.d), "&c=", from.y,
            "&d=", to.m, "&e=", sprintf("%.2d", to.d), "&f=",
            to.y, "&g=d&q=q&y=0", "&z=", Symbols.name, "&x=.csv",
            sep = ""), destfile = tmp, quiet = !verbose)

Note the "&g=d" is hardcoded (at least in my versions 0.3-6, 0.3-7)

2: This has been asked before (see attached).

HTH,
Brian

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of BearXu
Sent: Wednesday, February 25, 2009 10:11 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] get data in quantmod


I have two questions about how to get data in quantmod
1)Yahoo Finance provide us with data in daily, weekly, monthly. When we
are
using getSymbols can we choose the time spectrum?

2)I want to download some Chinese Stock Data, So I use this command:
getSymbols("600089.SS"), but when I want to use command
barChart(600089.SS),
it's wrong. Maybe the problem is the name of the object "600089.SS". So
can
I give another name to the object from the getSymbols?

Thanks

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

--------------------------------------------------------------------------
This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. References to "Merrill Lynch" are references to any company in the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by Bank of America Corporation. Securities and Insurance Products: * Are Not FDIC Insured * Are Not Bank Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition to Any Banking Service or Activity * Are Not Insured by Any Federal Government Agency. Attachments that are part of this E-communication may have additional important disclosures and disclaimers, which you should read. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
--------------------------------------------------------------------------

-------------- next part --------------
An embedded message was scrubbed...
From: "Brian Lee Yung Rowe" <brian at muxspace.com>
Subject: Re: [R-SIG-Finance] Newbie question on getSymbols() in quantmod
Date: Tue, 30 Dec 2008 22:31:39 -0500
Size: 4340
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090225/6a13787d/attachment.mht>
-------------- next part --------------
An embedded message was scrubbed...
From: "Jeff Ryan" <jeff.a.ryan at gmail.com>
Subject: Re: [R-SIG-Finance] Newbie question on getSymbols() in quantmod
Date: Wed, 31 Dec 2008 00:04:21 -0500
Size: 5616
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090225/6a13787d/attachment-0001.mht>

From jeff.a.ryan at gmail.com  Wed Feb 25 16:40:17 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 25 Feb 2009 09:40:17 -0600
Subject: [R-SIG-Finance] get data in quantmod
In-Reply-To: <3BAD818D9407B043817CC6D89ABA14EC032B8342@MLNYC20MB051.amrs.win.ml.com>
References: <82527b5d0902250711v25830212l19990cbbf5f8a698@mail.gmail.com>
	<3BAD818D9407B043817CC6D89ABA14EC032B8342@MLNYC20MB051.amrs.win.ml.com>
Message-ID: <e8e755250902250740p678b3e74j741442fabcd76042@mail.gmail.com>

To add to what Brian has said:

> SS600089 <- getSymbols("600089.SS", auto.assign=FALSE)

auto.assign is the current workaround.  Other options I will have to
work on.  Maybe a "name" arg or a "prefix" arg.

As far as periodicity.  I will add the option to getSymbols.yahoo to
allow for that to be specified.  A more flexible approach (as you
can't easily make daily data from monthly...) is to use the
*to.period* functions now in xts.

?to.monthly

> str(SS600089)
An 'xts' object from 2007-01-01 to 2009-02-24 containing:
  Data: num [1:536, 1:6] 14.9 14.9 14.9 15.2 15.5 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:6] "600089.SS.Open" "600089.SS.High" "600089.SS.Low"
"600089.SS.Close" ...
  Indexed by objects of class: [Date]
   xts Attributes:
List of 2
 $ src    : chr "yahoo"
 $ updated: POSIXct[1:1], format: "2009-02-25 09:25:48"

> periodicity(SS600089)
Daily periodicity from 2007-01-01 to 2009-02-24

> periodicity(to.weekly(SS600089))
Weekly periodicity from 2007-01-05 to 2009-02-24

> periodicity(to.monthly(SS600089))
Monthly periodicity from Jan 2007 to Feb 2009

> head(to.monthly(SS600089))
         SS600089.Open SS600089.High SS600089.Low SS600089.Close
Jan 2007         14.95         23.54        14.95          20.74
Feb 2007         20.15         21.00        19.00          20.41
Mar 2007         20.41         20.41        20.41          20.41
Apr 2007         20.41         20.41        20.41          20.41
May 2007         20.41         20.41        20.41          20.41
Jun 2007         20.41         35.89        20.41          30.75
         SS600089.Volume SS600089.Adjusted
Jan 2007       177635800             20.62
Feb 2007        10694100             20.29
Mar 2007               0             20.29
Apr 2007               0             20.29
May 2007               0             20.29
Jun 2007        58088500             30.72


HTH,
Jeff
On Wed, Feb 25, 2009 at 9:30 AM, Rowe, Brian Lee Yung (Portfolio
Analytics) <B_Rowe at ml.com> wrote:
> 1: From the code for getSymbols.yahoo this appears to be hardcoded. The
> parameter to set the series frequency on yahoo is 'g':
>
> http://chart.yahoo.com/table.csv?s=FXP&a=0&b=01&c=2007&d=1&e=25&f=2009&g
> =m&q=q&y=0&z=FXP&x=.csv - returns monthly
>
> http://chart.yahoo.com/table.csv?s=FXP&a=0&b=01&c=2007&d=1&e=25&f=2009&g
> =d&q=q&y=0&z=FXP&x=.csv - returns daily
>
> Here is the code in getSymbols.yahoo:
> ? ? ? ?download.file(paste(yahoo.URL, "s=", Symbols.name, "&a=",
> ? ? ? ? ? ?from.m, "&b=", sprintf("%.2d", from.d), "&c=", from.y,
> ? ? ? ? ? ?"&d=", to.m, "&e=", sprintf("%.2d", to.d), "&f=",
> ? ? ? ? ? ?to.y, "&g=d&q=q&y=0", "&z=", Symbols.name, "&x=.csv",
> ? ? ? ? ? ?sep = ""), destfile = tmp, quiet = !verbose)
>
> Note the "&g=d" is hardcoded (at least in my versions 0.3-6, 0.3-7)
>
> 2: This has been asked before (see attached).
>
> HTH,
> Brian
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of BearXu
> Sent: Wednesday, February 25, 2009 10:11 AM
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] get data in quantmod
>
>
> I have two questions about how to get data in quantmod
> 1)Yahoo Finance provide us with data in daily, weekly, monthly. When we
> are
> using getSymbols can we choose the time spectrum?
>
> 2)I want to download some Chinese Stock Data, So I use this command:
> getSymbols("600089.SS"), but when I want to use command
> barChart(600089.SS),
> it's wrong. Maybe the problem is the name of the object "600089.SS". So
> can
> I give another name to the object from the getSymbols?
>
> Thanks
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> --------------------------------------------------------------------------
> This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. References to "Merrill Lynch" are references to any company in the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by Bank of America Corporation. Securities and Insurance Products: * Are Not FDIC Insured * Are Not Bank Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition to Any Banking Service or Activity * Are Not Insured by Any Federal Government Agency. Attachments that are part of this E-communication may have additional important disclosures and disclaimers, which you should read. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
> --------------------------------------------------------------------------
>
>
>
> ---------- Forwarded message ----------
> From:?"Brian Lee Yung Rowe" <brian at muxspace.com>
> To:?"L.-Y. Hin" <lyhin at netvigator.com>
> Date:?Tue, 30 Dec 2008 22:31:39 -0500
> Subject:?Re: [R-SIG-Finance] Newbie question on getSymbols() in quantmod
> Try:
>
> HSBC <- getSymbols("0005.HK",src="yahoo", auto.assign=FALSE)
>
> Brian
>
>
> On Wed, 2008-12-31 at 10:58 +0800, L.-Y. Hin wrote:
>> Dear gurus,
>>
>> I am learning the quantmod library, and trying to download
>> the equity series for HSBC from yahoo.
>>
>> I checked from yahoo finance that the symbol is 0005.HK,
>> and then I used
>>
>>
>> > library(quantmod)
>> > getSymbols("0005.HK",src="yahoo")
>> [1] "0005.HK"
>> > candleChart(0005.HK)
>> Error: unexpected symbol in "candleChart(0005.HK"
>> > chartSeries(0005.HK)
>> Error: unexpected symbol in "chartSeries(0005.HK"
>> > Cl(0005.HK)
>> Error: unexpected symbol in "Cl(0005.HK"
>>
>> It seems like probably no data has been loaded by the
>> approach above.
>>
>>
>> However, the same scripts work for NYSE equities like
>> > getSymbols("A",src="yahoo")
>> [1] "A"
>> > chartSeries(A)
>> > length(Cl(A)) ? #To check whether OHLC data has been loaded
>> [1] 503
>>
>>
>> I suspect the problem is in the way I tried to load 0005.HK....
>>
>>
>> I'll be very grateful for your kind advice.
>>
>>
>> Many thanks
>>
>> Lin
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>
> ---------- Forwarded message ----------
> From:?"Jeff Ryan" <jeff.a.ryan at gmail.com>
> To:?"Brian Lee Yung Rowe" <brian at muxspace.com>
> Date:?Wed, 31 Dec 2008 00:04:21 -0500
> Subject:?Re: [R-SIG-Finance] Newbie question on getSymbols() in quantmod
> An alternate approach uses setSymbolLookup
>
> setSymbolLookup(HSBC=list(src='yahoo', name="0005.HK"))
>
> You can put this in a start up file for future R sessions. ?See ?setSymbolLookup
>
>> getSymbols("HSBC")
> [1] "HSBC"
>> str(HSBC)
> An 'xts' object from 2007-01-01 to 2008-12-29 containing:
> ?Data: num [1:510, 1:6] 142 143 144 145 144 ...
> ?- attr(*, "dimnames")=List of 2
> ?..$ : NULL
> ?..$ : chr [1:6] "0005.HK.Open" "0005.HK.High" "0005.HK.Low"
> "0005.HK.Close" ...
> ?Indexed by objects of class: [Date]
> ? xts Attributes:
> List of 2
> ?$ src ? ?: chr "yahoo"
> ?$ updated: POSIXct[1:1], format: "2008-12-31 05:02:43"
>
> HTH
> Jeff
>
> On Tue, Dec 30, 2008 at 9:31 PM, Brian Lee Yung Rowe <brian at muxspace.com> wrote:
>> Try:
>>
>> HSBC <- getSymbols("0005.HK",src="yahoo", auto.assign=FALSE)
>>
>> Brian
>>
>>
>> On Wed, 2008-12-31 at 10:58 +0800, L.-Y. Hin wrote:
>>> Dear gurus,
>>>
>>> I am learning the quantmod library, and trying to download
>>> the equity series for HSBC from yahoo.
>>>
>>> I checked from yahoo finance that the symbol is 0005.HK,
>>> and then I used
>>>
>>>
>>> > library(quantmod)
>>> > getSymbols("0005.HK",src="yahoo")
>>> [1] "0005.HK"
>>> > candleChart(0005.HK)
>>> Error: unexpected symbol in "candleChart(0005.HK"
>>> > chartSeries(0005.HK)
>>> Error: unexpected symbol in "chartSeries(0005.HK"
>>> > Cl(0005.HK)
>>> Error: unexpected symbol in "Cl(0005.HK"
>>>
>>> It seems like probably no data has been loaded by the
>>> approach above.
>>>
>>>
>>> However, the same scripts work for NYSE equities like
>>> > getSymbols("A",src="yahoo")
>>> [1] "A"
>>> > chartSeries(A)
>>> > length(Cl(A)) ? #To check whether OHLC data has been loaded
>>> [1] 503
>>>
>>>
>>> I suspect the problem is in the way I tried to load 0005.HK....
>>>
>>>
>>> I'll be very grateful for your kind advice.
>>>
>>>
>>> Many thanks
>>>
>>> Lin
>>>
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From Wang.Yu at constellation.com  Wed Feb 25 20:32:16 2009
From: Wang.Yu at constellation.com (Yu, Wang)
Date: Wed, 25 Feb 2009 14:32:16 -0500
Subject: [R-SIG-Finance] Help needed on Time Zone
Message-ID: <51A941521A51CB4D8442E480B4530DF5ADE8BCBF55@EXM-OMF-04.Ceg.Corp.Net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090225/32902f9d/attachment.pl>

From jeff.a.ryan at gmail.com  Wed Feb 25 20:42:38 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 25 Feb 2009 13:42:38 -0600
Subject: [R-SIG-Finance] Help needed on Time Zone
In-Reply-To: <51A941521A51CB4D8442E480B4530DF5ADE8BCBF55@EXM-OMF-04.Ceg.Corp.Net>
References: <51A941521A51CB4D8442E480B4530DF5ADE8BCBF55@EXM-OMF-04.Ceg.Corp.Net>
Message-ID: <e8e755250902251142l6cc04dfhdbaaaead66839af@mail.gmail.com>

Hi Wang,

The current approach in xts has been to use GMT internally.  This
really isn't ideal.  The current code on R-forge has been changed to
simply let the system variable do the work.

The caveat is that if it is not set (via the shell or Sys.setenv(TZ=)
), then you can possibly encounter some odd things.

as.POSIXct and as.POSIXlt behave differently, sometimes ignoring the
TZ you have.  A change (or 10) internal to xts has solved this issue
as well.

The current code is here: under /pkg

http://r-forge.r-project.org/scm/?group_id=118

The "R Packages" tab may not be updated yet.  But the subversion code is.

For now you can do Sys.setenv(TZ="GMT") at the beginning of the
session and have everything default to GMT.  This really isn't the
correct solution though.

I will be pushing the changes to CRAN soon.

Thanks,
Jeff

On Wed, Feb 25, 2009 at 1:32 PM, Yu, Wang <Wang.Yu at constellation.com> wrote:
> I am having problem with converting objects into xts objects and it looks to have something to do with time zone.
>
> It looks that it has something to do with my computer settings and I am wondering whether someone has seen this and solved it or the best practice is to just setting to "GMT"?
>
> Also, what is the default behavior if the Time Zone is unknown, defaulting to "GMT" already?
>
> Thanks, Wang
>
>
> Sys.getenv("TZ")
> ? ? ? TZ
> "est5edt"
>> Sys.timezone()
> [1] "EST"
>> z<- as.POSIXlt(Sys.time())
>> attributes(z)
> $names
> [1] "sec" ? "min" ? "hour" ?"mday" ?"mon" ? "year" ?"wday" ?"yday" ?"isdst"
>
> $class
> [1] "POSIXt" ?"POSIXlt"
>
> $tzone
> [1] "" ? ?"EST" "EDT"
>
> data(sample_matrix)
> sample.xts <- as.xts(sample_matrix, descr='my new xts object')
>
> Warning messages:
> 1: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt", "POSIXct"), ?:
> ?unknown timezone '_st5edt'
> 2: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt", "POSIXct"), ?:
> ?unknown timezone '_st5edt'
> 3: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt", "POSIXct"), ?:
> ?unknown timezone '_st5edt'
> 4: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt", "POSIXct"), ?:
> ?unknown timezone '_st5edt'
> 5: In structure(.Internal(as.POSIXct(x, tz)), class = c("POSIXt", "POSIXct"), ?:
> ?unknown timezone '_st5edt'
>
> class(sample.xts)
> [1] "xts" "zoo"
>> str(sample.xts)
> An 'xts' object from 2007-01-02 to 2007-06-30 containing:
> ?Data: num [1:180, 1:4] 50 50.2 50.4 50.4 50.2 ...
> ?- attr(*, "dimnames")=List of 2
> ?..$ : NULL
> ?..$ : chr [1:4] "Open" "High" "Low" "Close"
> ?Indexed by objects of class: [POSIXt,POSIXct]
> ? Original class: 'matrix'
> ?xts Attributes:
> List of 1
> ?$ descr: chr "my new xts object"
>>>> This e-mail and any attachments are confidential, may contain legal,
> professional or other privileged information, and are intended solely for the
> addressee. ?If you are not the intended recipient, do not use the information
> in this e-mail in any way, delete this e-mail and notify the sender. CEG-IP2
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From vlanschot at yahoo.com  Thu Feb 26 11:28:44 2009
From: vlanschot at yahoo.com (R@Nabble)
Date: Thu, 26 Feb 2009 02:28:44 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Black Litterman portfolio
	optimization
In-Reply-To: <3CBFCFB1FEFFA841BA83ADF2F2A9C6FA2F67F9@mango-data1.Mango.local>
References: <4AAD56F399C8564C9EB6817C17618CDD0251E822@NYC-XCH3.win.moorecap.com>
	<3CBFCFB1FEFFA841BA83ADF2F2A9C6FA2F67F9@mango-data1.Mango.local>
Message-ID: <22221441.post@talk.nabble.com>


Hi Reena,

As Francisco knows, I've been using his BLCOP package, and take its output,
posteriors for example, as input into other packages like the fPortfolio
package. Any improvement is welcome of course, but I think BLCOP works fine
already. I suggest you just start playing around with it.

PS

Francisco Gochez wrote:
> 
> Hi Reena,
> 
> As mentioned by Debashis, I am the author of this package, which is
> still under development.  I have not had much time to move beyond
> version 0.2.2 until now, but I plan to release another update fairly
> soon.  In particular, I am planning to make it easier to use the other
> Rmetrics tools for portfolio optimization together with the results
> produced by the prior/posterior blending routines (expected shortfall
> optimization for copula opinion pooling should be part of this).  In the
> meantime, I welcome any questions or feedback you might have about the
> package.
> 
> Regards,
> 
> Francisco
> 
> 
> Mango Solutions
> 
> S & R Consulting and Training
> 
> +44 (0)1249 767 700
> 
> 
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Reena
> Bansal
> Sent: 19 February 2009 19:42
> To: r-sig-finance at stat.math.ethz.ch
> Subject: [R-SIG-Finance] Black Litterman portfolio optimization
> 
> Hi All -
> 
> I am looking for Black Litterman CAPM portfolio optimization
> implementation in R. 
> 
> Reference
> http://corporate.morningstar.com/ib/documents/MethodologyDocuments/IBBAs
> sociates/BlackLitterman.pdf
> 
> I came across the beta release of BLCOP package. Has anybody used this
> package and is there any other implementation of the above in R?
> 
> Thanks,
> Reena
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Black-Litterman-portfolio-optimization-tp22108227p22221441.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From vlanschot at yahoo.com  Thu Feb 26 12:25:40 2009
From: vlanschot at yahoo.com (R@Nabble)
Date: Thu, 26 Feb 2009 03:25:40 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
Message-ID: <22222236.post@talk.nabble.com>


I'd like to be able to plot a time-series matrix (i.e. first col contains
sorted dates, rest of cols contains non-sorted data, with headings at the
top; current format is as a zoo object) as a surface/3D chart. The function
persp() is giving me errors, even if I transform data as core data, etc.

Any suggestions / other functions available?

Thx,

PS
-- 
View this message in context: http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22222236.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From Zeno.Adams at ebs.edu  Thu Feb 26 13:42:54 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Thu, 26 Feb 2009 13:42:54 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
In-Reply-To: <22222236.post@talk.nabble.com>
References: <22222236.post@talk.nabble.com>
Message-ID: <9064522880125945B98983BBAECBA1CC9854F9@exchsrv001.ebs.local>

I usually use the wireframe() function from the lattice package. I think it is more comfortable to use than persp() and has more intuitive inputs. The wireframe() function requires the value of the third variable (or function) in the (ith, jth) element of your matrix.
For example if you want to plot the likelihood function for two parameters p1 <- c(0.5, 1, 1.5, 2) and p2 <- c(-0.3, 0, 0.3, 0.6, 0.9) then the matrix mat is 4x5 and the [3,2] element of mat contains the value of the likelihood function when p1 is 1.5 and p2 is 0.

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von R at Nabble
Gesendet: Donnerstag, 26. Februar 2009 12:26
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface


I'd like to be able to plot a time-series matrix (i.e. first col contains
sorted dates, rest of cols contains non-sorted data, with headings at the
top; current format is as a zoo object) as a surface/3D chart. The function
persp() is giving me errors, even if I transform data as core data, etc.

Any suggestions / other functions available?

Thx,

PS
-- 
View this message in context: http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22222236.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Rektor/CEO; Dr. Reimar Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Verwaltungsrat: Dr. Hellmut K. Albrecht, Vorsitzender

From enricoschumann at yahoo.de  Thu Feb 26 14:57:17 2009
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Thu, 26 Feb 2009 14:57:17 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
In-Reply-To: <22222236.post@talk.nabble.com>
Message-ID: <338466.18985.bm@omp220.mail.ukl.yahoo.com>

it should be helpful to give a little code example, or at least the content
of the error messages that you received.

the following gives me a perspective plot...

require(zoo)
nC     <- 10	# columns
nO 	 <- 100	# observations
dataM  <- array(runif(nC * nO), dim=c(nO, nC))
zz 	 <- zoo(dataM, 1:nO)
persp(1:nO,1:nC,zz)


regards 
enrico
-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von R at Nabble
Gesendet: Donnerstag, 26. Februar 2009 12:26
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface


I'd like to be able to plot a time-series matrix (i.e. first col contains
sorted dates, rest of cols contains non-sorted data, with headings at the
top; current format is as a zoo object) as a surface/3D chart. The function
persp() is giving me errors, even if I transform data as core data, etc.

Any suggestions / other functions available?

Thx,

PS
--
View this message in context:
http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22222236.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG - www.avg.com


06:40:00


From vlanschot at yahoo.com  Thu Feb 26 15:20:53 2009
From: vlanschot at yahoo.com (R@Nabble)
Date: Thu, 26 Feb 2009 06:20:53 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
In-Reply-To: <338466.18985.bm@omp220.mail.ukl.yahoo.com>
References: <22222236.post@talk.nabble.com>
	<338466.18985.bm@omp220.mail.ukl.yahoo.com>
Message-ID: <22225107.post@talk.nabble.com>


Thank you Enrico. Your example helped me to solve a (embarrasingly basic)
error in my code.

Plots fine now.

Thx again.

PS

Enrico Schumann wrote:
> 
> it should be helpful to give a little code example, or at least the
> content
> of the error messages that you received.
> 
> the following gives me a perspective plot...
> 
> require(zoo)
> nC     <- 10	# columns
> nO 	 <- 100	# observations
> dataM  <- array(runif(nC * nO), dim=c(nO, nC))
> zz 	 <- zoo(dataM, 1:nO)
> persp(1:nO,1:nC,zz)
> 
> 
> regards 
> enrico
> -----Urspr?ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von R at Nabble
> Gesendet: Donnerstag, 26. Februar 2009 12:26
> An: r-sig-finance at stat.math.ethz.ch
> Betreff: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
> 
> 
> I'd like to be able to plot a time-series matrix (i.e. first col contains
> sorted dates, rest of cols contains non-sorted data, with headings at the
> top; current format is as a zoo object) as a surface/3D chart. The
> function
> persp() is giving me errors, even if I transform data as core data, etc.
> 
> Any suggestions / other functions available?
> 
> Thx,
> 
> PS
> --
> View this message in context:
> http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22222236.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> No virus found in this incoming message.
> Checked by AVG - www.avg.com
> 
> 
> 06:40:00
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22225107.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Thu Feb 26 18:13:11 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 26 Feb 2009 11:13:11 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
In-Reply-To: <22225107.post@talk.nabble.com>
References: <22222236.post@talk.nabble.com>
	<338466.18985.bm@omp220.mail.ukl.yahoo.com>
	<22225107.post@talk.nabble.com>
Message-ID: <e8e755250902260913x38dadf0xc40d84ea7e050a0b@mail.gmail.com>

PS,

I have been working on adding a chartSeries3d function to quantmod.
The code isn't in place yet, but it relies on persp, and some extra
manipulation to offer much better axis labeling.  It is designed for
xts/zoo structures in particular.  The final code will work for any
time-series.

This should be in the next release of quantmod, but I've posted the
alpha code and a .png here:

http://www.quantmod.com/example/chartSeries3d

HTH,
Jeff


R/Finance 2009: Applied Finance with R
April 24, 25 2009 Chicago, IL USA
http://www.RinFinance.com


On Thu, Feb 26, 2009 at 8:20 AM, R at Nabble <vlanschot at yahoo.com> wrote:
>
> Thank you Enrico. Your example helped me to solve a (embarrasingly basic)
> error in my code.
>
> Plots fine now.
>
> Thx again.
>
> PS
>
> Enrico Schumann wrote:
>>
>> it should be helpful to give a little code example, or at least the
>> content
>> of the error messages that you received.
>>
>> the following gives me a perspective plot...
>>
>> require(zoo)
>> nC ? ? <- 10 ?# columns
>> nO ? ? <- 100 # observations
>> dataM ?<- array(runif(nC * nO), dim=c(nO, nC))
>> zz ? ? <- zoo(dataM, 1:nO)
>> persp(1:nO,1:nC,zz)
>>
>>
>> regards
>> enrico
>> -----Urspr?ngliche Nachricht-----
>> Von: r-sig-finance-bounces at stat.math.ethz.ch
>> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von R at Nabble
>> Gesendet: Donnerstag, 26. Februar 2009 12:26
>> An: r-sig-finance at stat.math.ethz.ch
>> Betreff: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
>>
>>
>> I'd like to be able to plot a time-series matrix (i.e. first col contains
>> sorted dates, rest of cols contains non-sorted data, with headings at the
>> top; current format is as a zoo object) as a surface/3D chart. The
>> function
>> persp() is giving me errors, even if I transform data as core data, etc.
>>
>> Any suggestions / other functions available?
>>
>> Thx,
>>
>> PS
>> --
>> View this message in context:
>> http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22222236.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>> No virus found in this incoming message.
>> Checked by AVG - www.avg.com
>>
>>
>> 06:40:00
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>
> --
> View this message in context: http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22225107.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From john.hawver at gmail.com  Thu Feb 26 22:23:42 2009
From: john.hawver at gmail.com (John Hawver)
Date: Thu, 26 Feb 2009 16:23:42 -0500
Subject: [R-SIG-Finance] RDCOMClient install package problem
Message-ID: <d6f602790902261323k44d0778exb0d5ab890e3eff59@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090226/d9293875/attachment.pl>

From cedrickj at cavengerllc.com  Thu Feb 26 23:05:56 2009
From: cedrickj at cavengerllc.com (Cedrick Johnson)
Date: Thu, 26 Feb 2009 17:05:56 -0500
Subject: [R-SIG-Finance] RDCOMClient install package problem
In-Reply-To: <d6f602790902261323k44d0778exb0d5ab890e3eff59@mail.gmail.com>
References: <d6f602790902261323k44d0778exb0d5ab890e3eff59@mail.gmail.com>
Message-ID: <c2fd498c0902261405i93133b8kc9f3f1f18cdc10f7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090226/2b53fbf3/attachment.pl>

From john.hawver at gmail.com  Thu Feb 26 23:13:33 2009
From: john.hawver at gmail.com (John Hawver)
Date: Thu, 26 Feb 2009 17:13:33 -0500
Subject: [R-SIG-Finance] RDCOMClient install package problem
In-Reply-To: <c2fd498c0902261405i93133b8kc9f3f1f18cdc10f7@mail.gmail.com>
References: <d6f602790902261323k44d0778exb0d5ab890e3eff59@mail.gmail.com>
	<c2fd498c0902261405i93133b8kc9f3f1f18cdc10f7@mail.gmail.com>
Message-ID: <d6f602790902261413y683ccfdawddf52128f4f1172f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090226/85c24e4c/attachment.pl>

From pgilbert at bank-banque-canada.ca  Thu Feb 26 23:46:45 2009
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Thu, 26 Feb 2009 17:46:45 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
In-Reply-To: <e8e755250902260913x38dadf0xc40d84ea7e050a0b@mail.gmail.com>
References: <22222236.post@talk.nabble.com>	<338466.18985.bm@omp220.mail.ukl.yahoo.com>	<22225107.post@talk.nabble.com>
	<e8e755250902260913x38dadf0xc40d84ea7e050a0b@mail.gmail.com>
Message-ID: <49A71BD5.5030503@bank-banque-canada.ca>

In the tframePlus package there is a function called tfpersp() that does 
this.  I'm sure it can be improved, I've never been really happy with 
it. But you might want to compare, to be sure you are improving it.

Paul

Jeff Ryan wrote:
> PS,
> 
> I have been working on adding a chartSeries3d function to quantmod.
> The code isn't in place yet, but it relies on persp, and some extra
> manipulation to offer much better axis labeling.  It is designed for
> xts/zoo structures in particular.  The final code will work for any
> time-series.
> 
> This should be in the next release of quantmod, but I've posted the
> alpha code and a .png here:
> 
> http://www.quantmod.com/example/chartSeries3d
> 
> HTH,
> Jeff
> 
> 
> R/Finance 2009: Applied Finance with R
> April 24, 25 2009 Chicago, IL USA
> http://www.RinFinance.com
> 
> 
> On Thu, Feb 26, 2009 at 8:20 AM, R at Nabble <vlanschot at yahoo.com> wrote:
> 
>>Thank you Enrico. Your example helped me to solve a (embarrasingly basic)
>>error in my code.
>>
>>Plots fine now.
>>
>>Thx again.
>>
>>PS
>>
>>Enrico Schumann wrote:
>>
>>>it should be helpful to give a little code example, or at least the
>>>content
>>>of the error messages that you received.
>>>
>>>the following gives me a perspective plot...
>>>
>>>require(zoo)
>>>nC     <- 10  # columns
>>>nO     <- 100 # observations
>>>dataM  <- array(runif(nC * nO), dim=c(nO, nC))
>>>zz     <- zoo(dataM, 1:nO)
>>>persp(1:nO,1:nC,zz)
>>>
>>>
>>>regards
>>>enrico
>>>-----Urspr?ngliche Nachricht-----
>>>Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von R at Nabble
>>>Gesendet: Donnerstag, 26. Februar 2009 12:26
>>>An: r-sig-finance at stat.math.ethz.ch
>>>Betreff: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
>>>
>>>
>>>I'd like to be able to plot a time-series matrix (i.e. first col contains
>>>sorted dates, rest of cols contains non-sorted data, with headings at the
>>>top; current format is as a zoo object) as a surface/3D chart. The
>>>function
>>>persp() is giving me errors, even if I transform data as core data, etc.
>>>
>>>Any suggestions / other functions available?
>>>
>>>Thx,
>>>
>>>PS
>>>--
>>>View this message in context:
>>>http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22222236.html
>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>
>>>_______________________________________________
>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>-- Subscriber-posting only.
>>>-- If you want to post, subscribe first.
>>>No virus found in this incoming message.
>>>Checked by AVG - www.avg.com
>>>
>>>
>>>06:40:00
>>>
>>>_______________________________________________
>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>-- Subscriber-posting only.
>>>-- If you want to post, subscribe first.
>>>
>>>
>>
>>--
>>View this message in context: http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22225107.html
>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>>_______________________________________________
>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>-- Subscriber-posting only.
>>-- If you want to post, subscribe first.
> 
> 
> 
> 
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential in...{{dropped:26}}


From john.hawver at gmail.com  Fri Feb 27 00:08:16 2009
From: john.hawver at gmail.com (John Hawver)
Date: Thu, 26 Feb 2009 18:08:16 -0500
Subject: [R-SIG-Finance] RBloomberg Date
Message-ID: <d6f602790902261508q1524a93bi6ed504a59381b639@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090226/39005507/attachment.pl>

From pdebruic at gmail.com  Fri Feb 27 02:30:25 2009
From: pdebruic at gmail.com (Paul DeBruicker)
Date: Thu, 26 Feb 2009 20:30:25 -0500
Subject: [R-SIG-Finance] RBloomberg Date
In-Reply-To: <d6f602790902261508q1524a93bi6ed504a59381b639@mail.gmail.com>
References: <d6f602790902261508q1524a93bi6ed504a59381b639@mail.gmail.com>
Message-ID: <f2e3401f0902261730i6a6c9ffbm9fa49169f282ca9c@mail.gmail.com>

Hi John,

Try setting retval="raw" in your blpGetData call which will make it
skip the coercion steps and just give you what BBG returns to your
terminal in your t2 variable.  From looking at t2 you should be able
to figure out how to extract the date.  I'm not at a Bloomberg
terminal at the moment so its hard for me to be more specific than
that.

t2 <- blpGetData(conn, c("HAL Equity"), "DVD_EX_DT", retval="raw")


Paul




On Thu, Feb 26, 2009 at 6:08 PM, John Hawver <john.hawver at gmail.com> wrote:
> Hi,
>
> I just installed RBloomberg to setup a script that pulls dividend data from
> bbg. ?It works well, except on data that is supposed to be returned as a
> date. ?After talking to bbg, I'm reasonably sure the data IS being returned
> to R, but it is not being cast correctly. ?Anyone know how to fix or adapt
> RBloomberg?
>
> Here are examples:
>> t2 <- blpGetData(conn, c("HAL Equity"), "DVD_EX_DT")
> Warning message:
> In as.matrix.BlpCOMReturn(x) : NAs introduced by coercion
>> t2
> ? ? ? ? ? DVD_EX_DT
> HAL EQUITY ? ? ?<NA>
>> t3 <- blpGetData(conn, c("HAL Equity"), "DVD_SH_LAST")
>> t3
> ? ? ? ? ? DVD_SH_LAST
> HAL EQUITY ? ? ? ?0.09
>
> Thanks,
> John
>
>
> --
> plan the dive, dive the plan
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From sergeyg at gmail.com  Fri Feb 27 09:16:43 2009
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Fri, 27 Feb 2009 09:16:43 +0100
Subject: [R-SIG-Finance] Problem with RBloomberg (not the usual one)
Message-ID: <7cb007bd0902270016h31633e81sc1bbfd2808de7ba4@mail.gmail.com>

Hello, everyone!

I have a problem with RBloomberg and this is not the usual "no
administrator rights" problem.

I have R 2.7.2, RBloomberg 0.1-10, RDCOMclient 0.92-0

RDCOMClient, chron, zoo, stats: these packages load OK.

Then, trying to connect, I get following error message:


 conn <- blpConnect(show.days="week", na.action="previous.days",
periodicity="daily")
Warning messages:
1: In getCOMInstance(name, force = TRUE, silent = TRUE) :
  Couldn't get clsid from the string
2: In blpConnect(show.days = "week", na.action = "previous.days",
periodicity = "daily") :
  Seems like this is not a Bloomberg Workstation:  Error : Invalid class string

Anyone encountered this problem?
What is wrong and how can I solve it?

Online, I found just one instance of this problem discussed, and it
was in Chinese:

http://cos.name/bbs/read.php?tid=12821&fpage=3

Thank you for your help!

Sergey


From sergeyg at gmail.com  Fri Feb 27 09:53:30 2009
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Fri, 27 Feb 2009 09:53:30 +0100
Subject: [R-SIG-Finance] Problem with RBloomberg (not the usual one)
In-Reply-To: <7cb007bd0902270016h31633e81sc1bbfd2808de7ba4@mail.gmail.com>
References: <7cb007bd0902270016h31633e81sc1bbfd2808de7ba4@mail.gmail.com>
Message-ID: <7cb007bd0902270053h73dd8931pb7080c8ac5ac69c2@mail.gmail.com>

Hello, again, everyone

I went through the code and narrowed down the problem

in blpConnect:
COMCreate("Bloomberg.Data.1") which then calls getCOMInstance does not
work, because
getCLSID("Bloomberg.Data.1") returns
"Fehler: Invalid class string"

What is this problem???

Best,
Sergey

On Fri, Feb 27, 2009 at 09:16, Sergey Goriatchev <sergeyg at gmail.com> wrote:
> Hello, everyone!
>
> I have a problem with RBloomberg and this is not the usual "no
> administrator rights" problem.
>
> I have R 2.7.2, RBloomberg 0.1-10, RDCOMclient 0.92-0
>
> RDCOMClient, chron, zoo, stats: these packages load OK.
>
> Then, trying to connect, I get following error message:
>
>
> ?conn <- blpConnect(show.days="week", na.action="previous.days",
> periodicity="daily")
> Warning messages:
> 1: In getCOMInstance(name, force = TRUE, silent = TRUE) :
> ?Couldn't get clsid from the string
> 2: In blpConnect(show.days = "week", na.action = "previous.days",
> periodicity = "daily") :
> ?Seems like this is not a Bloomberg Workstation: ?Error : Invalid class string
>
> Anyone encountered this problem?
> What is wrong and how can I solve it?
>
> Online, I found just one instance of this problem discussed, and it
> was in Chinese:
>
> http://cos.name/bbs/read.php?tid=12821&fpage=3
>
> Thank you for your help!
>
> Sergey
>



-- 
I'm not young enough to know everything. /Oscar Wilde
Experience is one thing you can't get for nothing. /Oscar Wilde
When you are finished changing, you're finished. /Benjamin Franklin
Tell me and I forget, teach me and I remember, involve me and I learn.
/Benjamin Franklin
Luck is where preparation meets opportunity. /George Patten


From vlanschot at yahoo.com  Fri Feb 27 16:07:47 2009
From: vlanschot at yahoo.com (R@Nabble)
Date: Fri, 27 Feb 2009 07:07:47 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
In-Reply-To: <49A71BD5.5030503@bank-banque-canada.ca>
References: <22222236.post@talk.nabble.com>
	<338466.18985.bm@omp220.mail.ukl.yahoo.com>
	<22225107.post@talk.nabble.com>
	<e8e755250902260913x38dadf0xc40d84ea7e050a0b@mail.gmail.com>
	<49A71BD5.5030503@bank-banque-canada.ca>
Message-ID: <22247689.post@talk.nabble.com>


Jeff and Paul,

Thanks for the suggestions. Jeff's png seems to reflect what I need, will
investigate/apply, and report back.

PS

Paul Gilbert wrote:
> 
> In the tframePlus package there is a function called tfpersp() that does 
> this.  I'm sure it can be improved, I've never been really happy with 
> it. But you might want to compare, to be sure you are improving it.
> 
> Paul
> 
> Jeff Ryan wrote:
>> PS,
>> 
>> I have been working on adding a chartSeries3d function to quantmod.
>> The code isn't in place yet, but it relies on persp, and some extra
>> manipulation to offer much better axis labeling.  It is designed for
>> xts/zoo structures in particular.  The final code will work for any
>> time-series.
>> 
>> This should be in the next release of quantmod, but I've posted the
>> alpha code and a .png here:
>> 
>> http://www.quantmod.com/example/chartSeries3d
>> 
>> HTH,
>> Jeff
>> 
>> 
>> R/Finance 2009: Applied Finance with R
>> April 24, 25 2009 Chicago, IL USA
>> http://www.RinFinance.com
>> 
>> 
>> On Thu, Feb 26, 2009 at 8:20 AM, R at Nabble <vlanschot at yahoo.com> wrote:
>> 
>>>Thank you Enrico. Your example helped me to solve a (embarrasingly basic)
>>>error in my code.
>>>
>>>Plots fine now.
>>>
>>>Thx again.
>>>
>>>PS
>>>
>>>Enrico Schumann wrote:
>>>
>>>>it should be helpful to give a little code example, or at least the
>>>>content
>>>>of the error messages that you received.
>>>>
>>>>the following gives me a perspective plot...
>>>>
>>>>require(zoo)
>>>>nC     <- 10  # columns
>>>>nO     <- 100 # observations
>>>>dataM  <- array(runif(nC * nO), dim=c(nO, nC))
>>>>zz     <- zoo(dataM, 1:nO)
>>>>persp(1:nO,1:nC,zz)
>>>>
>>>>
>>>>regards
>>>>enrico
>>>>-----Urspr?ngliche Nachricht-----
>>>>Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von R at Nabble
>>>>Gesendet: Donnerstag, 26. Februar 2009 12:26
>>>>An: r-sig-finance at stat.math.ethz.ch
>>>>Betreff: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
>>>>
>>>>
>>>>I'd like to be able to plot a time-series matrix (i.e. first col
contains
>>>>sorted dates, rest of cols contains non-sorted data, with headings at
the
>>>>top; current format is as a zoo object) as a surface/3D chart. The
>>>>function
>>>>persp() is giving me errors, even if I transform data as core data, etc.
>>>>
>>>>Any suggestions / other functions available?
>>>>
>>>>Thx,
>>>>
>>>>PS
>>>>--
>>>>View this message in context:
>>>>http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22222236.html
>>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>
>>>>_______________________________________________
>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>-- Subscriber-posting only.
>>>>-- If you want to post, subscribe first.
>>>>No virus found in this incoming message.
>>>>Checked by AVG - www.avg.com
>>>>
>>>>
>>>>06:40:00
>>>>
>>>>_______________________________________________
>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>-- Subscriber-posting only.
>>>>-- If you want to post, subscribe first.
>>>>
>>>>
>>>
>>>--
>>>View this message in context:
http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22225107.html
>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>
>>>_______________________________________________
>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>-- Subscriber-posting only.
>>>-- If you want to post, subscribe first.
>> 
>> 
>> 
>> 
> ====================================================================================
> 
> La version fran?aise suit le texte anglais.
> 
> ------------------------------------------------------------------------------------
> 
> This email may contain privileged and/or confidential in...{{dropped:26}}
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22247689.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From yana.roth at yahoo.com  Fri Feb 27 19:47:07 2009
From: yana.roth at yahoo.com (Yana Roth)
Date: Fri, 27 Feb 2009 10:47:07 -0800 (PST)
Subject: [R-SIG-Finance] convert coordinate system to percentage
Message-ID: <422971.2526.qm@web46204.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090227/6d3f7075/attachment.pl>

From enricoschumann at yahoo.de  Sat Feb 28 09:57:33 2009
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Sat, 28 Feb 2009 09:57:33 +0100
Subject: [R-SIG-Finance] convert coordinate system to percentage
In-Reply-To: <422971.2526.qm@web46204.mail.sp1.yahoo.com>
Message-ID: <843793.49003.bm@omp219.mail.ukl.yahoo.com>

i am not exactly sure what you want (or if this is really a finance
question), but maybe the following does what you want

# create some random data
x 	<- rnorm(20) * 0.05
y 	<- rnorm(20) * 0.05

plot(x, y)
par(ask = TRUE)
plot(x, y, xaxt = "n", yaxt = "n")

wo <- axTicks(1)
axis(1, at = wo, labels = paste(format(round(wo * 100, 1), nsmall = 1), "%",
sep = ""))

wo <- axTicks(2)
axis(2, at = wo, labels = paste(format(round(wo * 100, 1), nsmall = 1), "%",
sep = ""))



regards, enrico


-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Yana Roth
Gesendet: Freitag, 27. Februar 2009 19:47
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] convert coordinate system to percentage

Hello,
I would like to change coordinate system from metric to percentage in R when
I do scatter plot.
 
 
Thank you
Yana


      
	[[alternative HTML version deleted]]


No virus found in this incoming message.
Checked by AVG - www.avg.com

02/27/09
13:27:00


From helenar at gmx.de  Sat Feb 28 12:41:32 2009
From: helenar at gmx.de (Helena Richter)
Date: Sat, 28 Feb 2009 12:41:32 +0100
Subject: [R-SIG-Finance] implement quasi-bayesian maximum likelihood
 estimation for normal mixtures
Message-ID: <49A922EC.6030508@gmx.de>

Hi,

as you can see in the topic, I am trying to fit a normal mixture 
distribution with the approach suggested by Hamilton (1991). Since I 
couldn't find any existing packages including the quasi-bayesian mle, I 
have to write my own function. Unfortunately, I have absolutely no 
experience in doing this.

If you're not familiar with the QB-MLE, I attached the formula as pdf. 
The idea is to extend the usual MLE with prior beliefs about the values 
sigma_n and sigma_b. My priors are already included in the code below. I 
intend to try a mixture of two normal distributions with same mean, and 
variances 1 and 5 as starting values.
This is what I've done so far:

 > R <-read.table("C:\\...\\rendite.txt", header=F)
 > qbmle  <- function(p, data){
    mu <- mean(data);   
(-sum(log(p[1]/p[2]*exp(-0.5*(data-mu)^2/p[2]^2)+(1-p[1])/p[3]*exp(-0.5*data^2/p[3]^2)))-2.772*log(p[2]^2)-2.772*log(p[3]^2) 
- 2.772/p[2]^2 - 13.86/p[3]^2 )}
 > start <-c(0.9, 1, 5)
 > out <- nlm(qbmle, start, data=R)

The result is: error in nlm(...): non-finite value for nlm, plus a lot 
of warnings, and the following output:

 > out
$minimum
[1] -27513.60

$estimate
[1]  3.478212e+04 -2.146767e+03 -3.806269e-02

$gradient
[1] -5.971628e-02  1.939856e-03 -2.946156e+02

$code
[1] 5

$iterations
[1] 49

So, what did I do wrong? How can I implement any non-negative 
constraints, and a restriction for p to be between 0 and 1?
I'm sorry to bother you with such a beginners question and am very 
helpful for any remarks. I don't have to use the qb-mle so if you think 
there's a better way to do the estimation tell me.

Thanks a lot,
Helena




-------------- next part --------------
A non-text attachment was scrubbed...
Name: formula.pdf
Type: application/pdf
Size: 33969 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090228/fb5785e9/attachment.pdf>

From brian at braverock.com  Sat Feb 28 14:00:08 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Sat, 28 Feb 2009 07:00:08 -0600
Subject: [R-SIG-Finance] implement quasi-bayesian maximum likelihood
 estimation for normal mixtures
In-Reply-To: <49A922EC.6030508@gmx.de>
References: <49A922EC.6030508@gmx.de>
Message-ID: <49A93558.7050607@braverock.com>

Helena,

First, this isn't actually a finance question (although your data or goals may 
be related to finance), so it might be more appropriate to r-help than here.

I'm not familiar with the quasi-Bayesian mle, but I am familiar with Bayesian 
operations in R.  I'll first recommend the three books I'm aware of:

Jim Albert "Bayesian Computation with R"

Jean-Michel Marin and Christian Robert "Bayesian Core"

David Ardia "Financial Risk Management with Bayesian Estimation of GARCH Models"

The first book above has examples that are probably the closest to what you're 
looking to do.

Once you've specified a mixture model, then in the Bayesian framework sampling 
from that posterior distribution to get an estimate for some probability is 
quite straightforward.  A simple conditioning step is to use the prior 
*observed* mean (or other observed moment) to further condition your mixture 
model (or MCMC sampled distribution or gamma distribution or GPD distribution, 
or whatever).  It seems that your approach is missing this conditioning step.

Again, I'm not familiar with the quasi-Bayesian mle, but it seems to me that 
with more modern fully Bayesian techniques available, you might get better 
results by going all the way to a full Bayesian sampling method (with the added 
bonus that there is quite a lot of R code available already).

Regards,

      - Brian

Helena Richter wrote:
> Hi,
> 
> as you can see in the topic, I am trying to fit a normal mixture 
> distribution with the approach suggested by Hamilton (1991). Since I 
> couldn't find any existing packages including the quasi-bayesian mle, I 
> have to write my own function. Unfortunately, I have absolutely no 
> experience in doing this.
> 
> If you're not familiar with the QB-MLE, I attached the formula as pdf. 
> The idea is to extend the usual MLE with prior beliefs about the values 
> sigma_n and sigma_b. My priors are already included in the code below. I 
> intend to try a mixture of two normal distributions with same mean, and 
> variances 1 and 5 as starting values.
> This is what I've done so far:
> 
>  > R <-read.table("C:\\...\\rendite.txt", header=F)
>  > qbmle  <- function(p, data){
>    mu <- mean(data);   
> (-sum(log(p[1]/p[2]*exp(-0.5*(data-mu)^2/p[2]^2)+(1-p[1])/p[3]*exp(-0.5*data^2/p[3]^2)))-2.772*log(p[2]^2)-2.772*log(p[3]^2) 
> - 2.772/p[2]^2 - 13.86/p[3]^2 )}
>  > start <-c(0.9, 1, 5)
>  > out <- nlm(qbmle, start, data=R)
> 
> The result is: error in nlm(...): non-finite value for nlm, plus a lot 
> of warnings, and the following output:
> 
>  > out
> $minimum
> [1] -27513.60
> 
> $estimate
> [1]  3.478212e+04 -2.146767e+03 -3.806269e-02
> 
> $gradient
> [1] -5.971628e-02  1.939856e-03 -2.946156e+02
> 
> $code
> [1] 5
> 
> $iterations
> [1] 49
> 
> So, what did I do wrong? How can I implement any non-negative 
> constraints, and a restriction for p to be between 0 and 1?
> I'm sorry to bother you with such a beginners question and am very 
> helpful for any remarks. I don't have to use the qb-mle so if you think 
> there's a better way to do the estimation tell me.
> 
> Thanks a lot,
> Helena


-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From bennfine at gmail.com  Sun Mar  1 02:27:39 2009
From: bennfine at gmail.com (benn fine)
Date: Sat, 28 Feb 2009 20:27:39 -0500
Subject: [R-SIG-Finance] Question about RSI command in the TTR package
Message-ID: <33b4672e0902281727h1dc70f11t285bc2bdaea1d4fd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090228/369f7c2b/attachment.pl>

From josh.m.ulrich at gmail.com  Sun Mar  1 03:30:01 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Sat, 28 Feb 2009 20:30:01 -0600
Subject: [R-SIG-Finance] Question about RSI command in the TTR package
In-Reply-To: <33b4672e0902281727h1dc70f11t285bc2bdaea1d4fd@mail.gmail.com>
References: <33b4672e0902281727h1dc70f11t285bc2bdaea1d4fd@mail.gmail.com>
Message-ID: <8cca69990902281830o7ebcabd1te6caabf2f56867b6@mail.gmail.com>

Hello,

On Sat, Feb 28, 2009 at 7:27 PM, benn fine <bennfine at gmail.com> wrote:
> How does one use the maType command inside RSI ?
>
> I want to call something like RSI(price,n=2,maType=list(SMA,n=2))
>
It's a bit redundant to specify n=2 for RSI and in maType.  That said,
it shouldn't cause an error... this is a bug.  What you want is:
RSI(price,n=2,maType=SMA)

> Also, in the RSI help file is the following code-what is it doing with the
> maUp and maDown?
> Can't figure it out either:
>
> # Case of two different 'maType's for both MAs
> ?rsiMA2 <- RSI(price, n=14, maType=list(maUp=list(EMA,ratio=1/5),
> ? ? ? ? ? ? ? ?maDown=list(WMA,wts=1:10)))
> Thanks.
>
The code in the above example is doing something highly unusual, in
order to demonstrate the flexibility of the TTR packge.  RSI compares
the average positive and negative prices changes and TTR's RSI allows
you to specify two different moving averages for positive/negative
price changes.  In the example, the positive price changes are
smoothed with an EMA, while the negative price changes are smoothed
with a weighted moving average.

>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

HTH,
Josh
--
http://quantemplation.blogspot.com


From edd at debian.org  Sun Mar  1 19:47:10 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 1 Mar 2009 12:47:10 -0600
Subject: [R-SIG-Finance] Request for comments: Finance taskview
Message-ID: <18858.55342.336825.937610@ron.nulle.part>


As you may know, the 'CRAN Task Views' [1] include a task view on Empirical
Finance [2].

I would like to solicit comments and suggestions on packages I may have
missed.  I noticed that more or more finance-related packages appear on
R-Forge too, but I cannot follow all of these to assess which ones already
worthy of regular use, and hence a worthy of mention.  The same goes for
Google Code.

So if there is a package you use and like, please drop me a line and I would
be happy to include it in the task view.

Thanks,  Dirk

Links:
 [1] http://cran.r-project.org/web/views/ 
 [2] http://cran.r-project.org/web/views/Finance.html

-- 
Three out of two people have difficulties with fractions.


From Achim.Zeileis at wu-wien.ac.at  Sun Mar  1 20:05:22 2009
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sun, 1 Mar 2009 20:05:22 +0100 (CET)
Subject: [R-SIG-Finance] Request for comments: Finance taskview
In-Reply-To: <18858.55342.336825.937610@ron.nulle.part>
References: <18858.55342.336825.937610@ron.nulle.part>
Message-ID: <Pine.LNX.4.64.0903012003020.31843@paninaro.stat-math.wu-wien.ac.at>

On Sun, 1 Mar 2009, Dirk Eddelbuettel wrote:

>
> As you may know, the 'CRAN Task Views' [1] include a task view on Empirical
> Finance [2].
>
> I would like to solicit comments and suggestions on packages I may have
> missed.  I noticed that more or more finance-related packages appear on
> R-Forge too, but I cannot follow all of these to assess which ones already
> worthy of regular use, and hence a worthy of mention.  The same goes for
> Google Code.

Good idea, Dirk. Let me join in: If any of you have comments/suggestions 
for the econometrics view:
   http://CRAN.R-project.org/view=Econometrics
let me know.

thx,
Z

> So if there is a package you use and like, please drop me a line and I would
> be happy to include it in the task view.
>
> Thanks,  Dirk
>
> Links:
> [1] http://cran.r-project.org/web/views/
> [2] http://cran.r-project.org/web/views/Finance.html
>
> -- 
> Three out of two people have difficulties with fractions.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From robert at sanctumfi.com  Mon Mar  2 11:47:45 2009
From: robert at sanctumfi.com (Robert Sams)
Date: Mon, 2 Mar 2009 10:47:45 -0000
Subject: [R-SIG-Finance] RBloomberg Date
References: <d6f602790902261508q1524a93bi6ed504a59381b639@mail.gmail.com>
	<SANCTUMFISERVER1znX00001654@sanctumfi.com>
Message-ID: <SANCTUMFISERVERv0aG000019c4@sanctumfi.com>

Hi John,

This is a bug in RBloomberg. The work-around is indeed setting retval="raw":

> t2 <- as.chron(unlist(blpGetData(conn, c("HAL Equity"), "DVD_EX_DT", retval="raw")))
> t2
[1] 02/27/09
>  

A fix (among other improvements) is on the way. 

~R


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Paul DeBruicker
Sent: 27 February 2009 01:30
To: John Hawver
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] RBloomberg Date

Hi John,

Try setting retval="raw" in your blpGetData call which will make it skip the coercion steps and just give you what BBG returns to your terminal in your t2 variable.  From looking at t2 you should be able to figure out how to extract the date.  I'm not at a Bloomberg terminal at the moment so its hard for me to be more specific than that.

t2 <- blpGetData(conn, c("HAL Equity"), "DVD_EX_DT", retval="raw")


Paul




On Thu, Feb 26, 2009 at 6:08 PM, John Hawver <john.hawver at gmail.com> wrote:
> Hi,
>
> I just installed RBloomberg to setup a script that pulls dividend data 
> from bbg. ?It works well, except on data that is supposed to be 
> returned as a date. ?After talking to bbg, I'm reasonably sure the 
> data IS being returned to R, but it is not being cast correctly. ?
> Anyone know how to fix or adapt RBloomberg?
>
> Here are examples:
>> t2 <- blpGetData(conn, c("HAL Equity"), "DVD_EX_DT")
> Warning message:
> In as.matrix.BlpCOMReturn(x) : NAs introduced by coercion
>> t2
> ? ? ? ? ? DVD_EX_DT
> HAL EQUITY ? ? ?<NA>
>> t3 <- blpGetData(conn, c("HAL Equity"), "DVD_SH_LAST")
>> t3
> ? ? ? ? ? DVD_SH_LAST
> HAL EQUITY ? ? ? ?0.09
>
> Thanks,
> John
>
>
> --
> plan the dive, dive the plan
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From yana.roth at yahoo.com  Mon Mar  2 17:12:26 2009
From: yana.roth at yahoo.com (Yana Roth)
Date: Mon, 2 Mar 2009 08:12:26 -0800 (PST)
Subject: [R-SIG-Finance] help-time series
Message-ID: <489379.2778.qm@web46206.mail.sp1.yahoo.com>

Hello,
I am working with libraries
?
timeDate)
library(timeSeries)
library(tseries)
library(RODBC)
library(PerformanceAnalytics)
library(fTrading)
?
and try to convert returns to monthly returns. I think the main problem that when it convert to time series object it doesn't prroduce the right time format , therefore when I apply 
"monthly" it doesnt read right the function.
Also when I converted data to csv it didn't help.
Anyway, I sen u the data and the code and output of error. I would b thankfull if u could help.
?data<-read.xls(file="Prices.xls")
tdata<-as.timeSeries(data) doesn't work
c(start(tdata), end(tdata))? #beginning and end of the time series (it doesnt work)
library(fCalendar)

price.ts<-timeSeries(data=data,charvec=rownames(data),
format="%Y-%m-%d",units=colnames(data))
?
produce an error
Warning message:
In timeDate(charvec = charvec, format = format, zone = zone, FinCenter = FinCenter) :
? 'charvec' entries of different number of characters are replaced by NA's

?
tsret<-returns(price.ts)
head(tsret)
output
?2.621954e-05 -1.076771e-03 -0.0030647848? 0.003368925
[2,] 2.621885e-05? 8.432440e-04 -0.0003460069 -0.007477860
[3,] 2.621816e-05 -1.864423e-03? 0.0022385116? 0.005220514
[4,] 7.865036e-05? 1.280149e-03? 0.0002193982? 0.015714158
[5,] 2.621541e-05? 8.727527e-05? 0.0014423604? 0.001540914
[6,] 2.621472e-05 -2.186654e-03? 0.0043773979 -0.009590629

applySeries(tsret, by="monthly", FUN=sum)
produce an error
Errore in midnightStandard(charvec, format) : 
? 'charvec' has non-NA entries of different number of characters
Inoltre: Warning messages:
1: In min(x) : no non-missing arguments to min; returning Inf
2: In max(x) : no non-missing arguments to max; returning -Inf
?
?
thank you



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090302/765b4952/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Prices.xls
Type: application/octet-stream
Size: 120832 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090302/765b4952/attachment.obj>

From Anil.Vijendran.wg08 at wharton.upenn.edu  Mon Mar  2 17:33:05 2009
From: Anil.Vijendran.wg08 at wharton.upenn.edu (Anil Vijendran)
Date: Mon, 2 Mar 2009 08:33:05 -0800
Subject: [R-SIG-Finance] getFX/getSymbols for FX
Message-ID: <f6ac578e0903020833i2bb20ae7h94c7dd7649bf51b5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090302/bfe2b79c/attachment.pl>

From Anil.Vijendran.wg08 at wharton.upenn.edu  Mon Mar  2 17:44:20 2009
From: Anil.Vijendran.wg08 at wharton.upenn.edu (Anil Vijendran)
Date: Mon, 2 Mar 2009 08:44:20 -0800
Subject: [R-SIG-Finance] getFX/getSymbols for FX
In-Reply-To: <f6ac578e0903020833i2bb20ae7h94c7dd7649bf51b5@mail.gmail.com>
References: <f6ac578e0903020833i2bb20ae7h94c7dd7649bf51b5@mail.gmail.com>
Message-ID: <f6ac578e0903020844y626f2fc6j98a4bbb9ed49b01f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090302/9f8bd378/attachment.pl>

From chalabi at phys.ethz.ch  Mon Mar  2 17:44:48 2009
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Mon, 2 Mar 2009 17:44:48 +0100
Subject: [R-SIG-Finance] help-time series
In-Reply-To: <489379.2778.qm@web46206.mail.sp1.yahoo.com>
References: <489379.2778.qm@web46206.mail.sp1.yahoo.com>
Message-ID: <20090302174448.4fb69105@mimi>

>>>> "YR" == Yana Roth <yana.roth at yahoo.com>
>>>> on Mon, 2 Mar 2009 08:12:26 -0800 (PST)

   YR> Hello,
   YR> I am working with libraries
   YR> ?
   YR> timeDate)
   YR> library(timeSeries)
   YR> library(tseries)
   YR> library(RODBC)
   YR> library(PerformanceAnalytics)
   YR> library(fTrading)
   YR> ?
   YR> and try to convert returns to monthly returns. I think the
   YR> main problem that when it convert to time series object it
   YR> doesn't prroduce the right time format , therefore when I apply
   YR> monthly it doesnt read right the function.
   YR> Also when I converted data to csv it didn't help.
   YR> Anyway, I sen u the data and the code and output of error. I
   YR> would b thankfull if u could help.
   YR> ?data<-read.xls(file=Prices.xls)

library(timeSeries) # have you updated your packages ??

# I prefer to load the data set from a csv files with ";" delimiter 
data <- read.csv("Prices.csv", sep = ";")

   YR> tdata<-as.timeSeries(data) doesn't work

tdata <- as.timeSeries(data) # does work ...

# or could have been done with
 
tdata <- readSeries("Prices.csv")

   YR> c(start(tdata), end(tdata))? #beginning and end of the time
   YR> series (it doesnt work)
   YR> library(fCalendar)

library(timeDate) # but already loaded by timeSeries !!!

   YR>
   YR> price.ts<-timeSeries(data=data,charvec=rownames(data),
   YR> format=%Y-%m-%d,units=colnames(data))
   YR> ?
   YR> produce an error
   YR> Warning message:
   YR> In timeDate(charvec = charvec, format = format, zone = zone,
   YR> FinCenter = FinCenter) :
   YR> ? 'charvec' entries of different number of characters are
   YR> replaced by NA's
   YR>
   YR> ?
   YR> tsret<-returns(price.ts)
   YR> head(tsret)
   YR> output
   YR> ?2.621954e-05 -1.076771e-03 -0.0030647848? 0.003368925
   YR> [2,] 2.621885e-05? 8.432440e-04 -0.0003460069 -0.007477860
   YR> [3,] 2.621816e-05 -1.864423e-03? 0.0022385116? 0.005220514
   YR> [4,] 7.865036e-05? 1.280149e-03? 0.0002193982? 0.015714158
   YR> [5,] 2.621541e-05? 8.727527e-05? 0.0014423604? 0.001540914
   YR> [6,] 2.621472e-05 -2.186654e-03? 0.0043773979 -0.009590629
   YR>
   YR> applySeries(tsret, by=monthly, FUN=sum)

# should be
applySeries(tsret, by = "monthly", FUN = colSums)


   YR> produce an error
   YR> Errore in midnightStandard(charvec, format) :
   YR> ? 'charvec' has non-NA entries of different number of
   YR> characters
   YR> Inoltre: Warning messages:
   YR> 1: In min(x) : no non-missing arguments to min; returning Inf
   YR> 2: In max(x) : no non-missing arguments to max; returning -Inf
   YR> ?
   YR> ?
   YR> thank you
   YR>
   YR>
   YR>
   YR>





-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch


From john.hawver at gmail.com  Mon Mar  2 22:49:32 2009
From: john.hawver at gmail.com (John Hawver)
Date: Mon, 2 Mar 2009 16:49:32 -0500
Subject: [R-SIG-Finance] Batch File Question
Message-ID: <d6f602790903021349rcf92604ub72e8614904ea540@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090302/cb0711c1/attachment.pl>

From josh.m.ulrich at gmail.com  Mon Mar  2 23:09:34 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Mon, 2 Mar 2009 16:09:34 -0600
Subject: [R-SIG-Finance] Batch File Question
In-Reply-To: <d6f602790903021349rcf92604ub72e8614904ea540@mail.gmail.com>
References: <d6f602790903021349rcf92604ub72e8614904ea540@mail.gmail.com>
Message-ID: <8cca69990903021409m397090e6k43378e0346556aab@mail.gmail.com>

As this is not a finance-related question, you'll probably get much
more/better help from R-help.

Best,
Josh
--
http://quantemplation.blogspot.com



On Mon, Mar 2, 2009 at 3:49 PM, John Hawver <john.hawver at gmail.com> wrote:
> Hi,
>
> I'm trying to setup an XP scheduled task that runs this batch file:
>
> @echo off
> cd C:\Program Files\R\R-2.8.1\bin
> Rscript -e "date()" -e "format(Sys.time(), \"Z:\John\Rfiles\Btest.R")"
> REM R CMD BATCH Z:\John\Rfiles\Btest.R
> pause
>
> but I get this error:
>
> 'Rscript' is not recognized as an internal or external command,
> operable program or batch file.
> Press any key to continue . . .
>
> R is installed on the C drive of the above box.
>
> Can anyone tell me what I'm doing wrong pls?
>
> Thanks,
> John
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From john.hawver at gmail.com  Mon Mar  2 23:10:51 2009
From: john.hawver at gmail.com (John Hawver)
Date: Mon, 2 Mar 2009 17:10:51 -0500
Subject: [R-SIG-Finance] Batch File Question
In-Reply-To: <8cca69990903021409m397090e6k43378e0346556aab@mail.gmail.com>
References: <d6f602790903021349rcf92604ub72e8614904ea540@mail.gmail.com>
	<8cca69990903021409m397090e6k43378e0346556aab@mail.gmail.com>
Message-ID: <d6f602790903021410g19dbba1ex9f37eaaae52afd4e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090302/38064379/attachment.pl>

From bogaso.christofer at gmail.com  Tue Mar  3 12:20:55 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Tue, 3 Mar 2009 03:20:55 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] VaR
Message-ID: <22306743.post@talk.nabble.com>


I frequently hear Value at risk i.e. VaR is not a coherent risk measure
because, sum of VaR for two individual assets may be LOWER than VaR of
portfolio consists of that two aseets i.e. VaR may not be sub-additive.
However when I calculate VaR for general assets like Equity, commodity etc,
I see that VaR is actually sub-addtive i.e. portfolio VaR is always less
than sum of individuals, which is reported as "diversification benefit". Can
anyone give me a particular example why VaR is not sub-additive?

Thanks
-- 
View this message in context: http://www.nabble.com/VaR-tp22306743p22306743.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From markleeds at verizon.net  Tue Mar  3 12:34:01 2009
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 03 Mar 2009 05:34:01 -0600 (CST)
Subject: [R-SIG-Finance] [R-sig-finance] VaR
Message-ID: <138157806.1802961236080041494.JavaMail.javamailuser@localhost>

  Hi  Christofer: I don't know if the analogy is allowed but this can 
happen with regular statistical
variance so maybe it can happen with Value at Risk also ? if you have a 
covariance matrix
of 2 assets with portfolio weights w_1 and w_2 and the 2 assets have 
positive covariance, then the resulting variance of the portfolio will 
be greater than the sum of the individual variances of the two assets 
with weights w_1 and w_2. ( w_1*v_1 + w_2*v_2 ).

now I have no idea if the result for statistical variance holds for 
Value at Risk ( i don't know the definition of Value at Risk ) but, if 
it does, then that's probably the answer. Hopefully someone else will 
tell us if the analogy is allowed ?



On Tue, Mar 3, 2009 at  6:20 AM, Bogaso wrote:

> I frequently hear Value at risk i.e. VaR is not a coherent risk 
> measure
> because, sum of VaR for two individual assets may be LOWER than VaR of
> portfolio consists of that two aseets i.e. VaR may not be 
> sub-additive.
> However when I calculate VaR for general assets like Equity, commodity 
> etc,
> I see that VaR is actually sub-addtive i.e. portfolio VaR is always 
> less
> than sum of individuals, which is reported as "diversification 
> benefit". Can
> anyone give me a particular example why VaR is not sub-additive?
>
> Thanks
> -- 
> View this message in context: 
> http://www.nabble.com/VaR-tp22306743p22306743.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.


From Matthias.Koberstein at hsbctrinkaus.de  Tue Mar  3 12:59:40 2009
From: Matthias.Koberstein at hsbctrinkaus.de (Matthias.Koberstein at hsbctrinkaus.de)
Date: Tue, 3 Mar 2009 12:59:40 +0100
Subject: [R-SIG-Finance] Antwort:  [R-sig-finance] VaR
In-Reply-To: <22306743.post@talk.nabble.com>
Message-ID: <OFDD876D03.47BD8548-ONC125756E.003EBFDA-C125756E.0041D8A7@hsbctrinkaus.de>

Hi Christofer,

I think the analogy is allowed if you assume normal distributions for the
assets.
Since then the VaR is dependent on the volatility.
The variance of two random variables (combined assets in this case) is
given by

Var(x+y)= E((x+y)^2) - E(x+y)^2

which transforms to
Var( x+y) = Var(x) + Var(y) + 2  * Covariance(x, y)

So it all depends on the covariance of x to y.
To give it a better feel this can be expressed in Correlation

Var(x+y)= Var(x) + Var(y) + 2 * Vol(x) * Vol(y) * Correlation

To better see  the effect throw some weights in w1, and w2 which combine to
one.
Then

Var( w1 x + w2 y)= Var(x) w1^2 + Var(y) w2^2 + 2 * w1 * w2 * Vol(x) * Vol
(y) * Correlation

the volatility used to estimate VaR is the square root of the variance.
So you see that if correlation is 1 VaR is not sub-additive.

Another point is if the distributions you use for the assets are not the
same,
the VaR can not even been combined easily but you have to find the combined
distributions of the assets in the portfolio (which can be quite painful).

I hope that helps. All the best

Matthias





**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential and / or
privileged information. If you are not the intended recipient or have
received this e-mail in error, please notify the sender immediately and
destroy this e-mail . Any unauthorized copying, storing, disclosure or
distribution of the contents of this e-mail is strictly forbidden.

---------------------------------------------------------------------
HSBC Trinkaus & Burkhardt AG
Sitz: D?sseldorf, K?nigsallee 21/23, 40212 D?sseldorf, Handelsregister:
Amtsgericht D?sseldorf HRB 54447
Mitglieder des Vorstands: Andreas Schmitz (Sprecher), Paul Hagen, Dr. Olaf
Huth, Carola Gr?fin v. Schmettow
Vorsitzender des Aufsichtsrats: Dr. Sieghardt Rometsch


                                                                           
             Bogaso                                                        
             <bogaso.christofe                                             
             r at gmail.com>                                               An 
             Gesendet von:               r-sig-finance at stat.math.ethz.ch   
             r-sig-finance-bou                                       Kopie 
             nces at stat.math.et                                             
             hz.ch                                                   Thema 
                                         [R-SIG-Finance] [R-sig-finance]   
             Fax-Deckblatt:              VaR                               
             HSBCTuB                                                       
             03.03.2009 12:24                                              
                                                                           
                                                                           
                                                                           
                                                                           





I frequently hear Value at risk i.e. VaR is not a coherent risk measure
because, sum of VaR for two individual assets may be LOWER than VaR of
portfolio consists of that two aseets i.e. VaR may not be sub-additive.
However when I calculate VaR for general assets like Equity, commodity etc,
I see that VaR is actually sub-addtive i.e. portfolio VaR is always less
than sum of individuals, which is reported as "diversification benefit".
Can
anyone give me a particular example why VaR is not sub-additive?

Thanks
--
View this message in context:
http://www.nabble.com/VaR-tp22306743p22306743.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From bastian2507hk at yahoo.co.uk  Tue Mar  3 13:01:33 2009
From: bastian2507hk at yahoo.co.uk (Bastian Offermann)
Date: Tue, 03 Mar 2009 13:01:33 +0100
Subject: [R-SIG-Finance] [R-sig-finance] VaR
In-Reply-To: <22306743.post@talk.nabble.com>
References: <22306743.post@talk.nabble.com>
Message-ID: <49AD1C1D.7040502@yahoo.co.uk>

A brief example is given in "Introduction to Modern Portfolio 
Optimization with NuOPT..." by Bernd Scherer (2005), page 180. That's 
most probably what you are looking for.

This might also be useful

"Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. 
Coherent measures
of risk. Mathematical Finance, pages 203-228 (1999)

Regards


Bogaso schrieb:
> I frequently hear Value at risk i.e. VaR is not a coherent risk measure
> because, sum of VaR for two individual assets may be LOWER than VaR of
> portfolio consists of that two aseets i.e. VaR may not be sub-additive.
> However when I calculate VaR for general assets like Equity, commodity etc,
> I see that VaR is actually sub-addtive i.e. portfolio VaR is always less
> than sum of individuals, which is reported as "diversification benefit". Can
> anyone give me a particular example why VaR is not sub-additive?
>
> Thanks
>


From David.King at schroders.com  Tue Mar  3 13:05:12 2009
From: David.King at schroders.com (King, David)
Date: Tue, 3 Mar 2009 12:05:12 -0000
Subject: [R-SIG-Finance] [R-sig-finance] VaR
In-Reply-To: <22306743.post@talk.nabble.com>
Message-ID: <77FB5D6344964D4B9D2FCE45D50B561B06449808@LON0820.london.schroders.com>


Christofer,

If I recall correctly there are some examples of this in the following book:

Risk Measures for the 21st Century, Giorgio Szeg? (Editor) 

Regards,

David



-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Bogaso
Sent: 03 March 2009 11:21
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] [R-sig-finance] VaR



I frequently hear Value at risk i.e. VaR is not a coherent risk measure because, sum of VaR for two individual assets may be LOWER than VaR of portfolio consists of that two aseets i.e. VaR may not be sub-additive. However when I calculate VaR for general assets like Equity, commodity etc, I see that VaR is actually sub-addtive i.e. portfolio VaR is always less than sum of individuals, which is reported as "diversification benefit". Can anyone give me a particular example why VaR is not sub-additive?

Thanks
-- 
View this message in context: http://www.nabble.com/VaR-tp22306743p22306743.html
Sent from the Rmetrics mailing list archive at Nabble.com.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
* Please Note : This message was received from the Internet * _____________________________________________________________

__________________________________________________________________


Visit Schroders Talking Point for market news and expert views http://www.schroders.com/talkingpoint

This message might contain confidential information. If it has been sent to you in error please do not forward it or copy it or act upon its contents, but report it to postmaster at schroders.com

Schroders has the right lawfully to record, monitor and inspect messages between its employees and any third party. Your messages shall be subject to such lawful supervision as Schroders deems to be necessary in order to protect its information, its interests and its reputation.

Schroders prohibits and takes steps to prevent its information systems from being used to view, store or forward offensive or discriminatory material. If this message contains such material please report it to abuse at schroders.com

Schroders does not normally accept or offer business instructions via email unless prior agreements are in place. Any action that you might take upon this message might be at your own risk.


Schroder Investment Management Limited
31 Gresham Street
London EC2V 7QA

Authorised and regulated by the Financial Services Authority. Schroder Investment Management Limited is entered on the FSA register under the following register number: 119348

Registered Office
31 Gresham Street
London EC2V 7QA

Registered number 1893220
VAT registration number 243 8687 30


From micha.keijzers at gmail.com  Tue Mar  3 13:22:09 2009
From: micha.keijzers at gmail.com (Micha Keijzers)
Date: Tue, 3 Mar 2009 13:22:09 +0100
Subject: [R-SIG-Finance] Antwort: [R-sig-finance] VaR
In-Reply-To: <OFDD876D03.47BD8548-ONC125756E.003EBFDA-C125756E.0041D8A7@hsbctrinkaus.de>
References: <22306743.post@talk.nabble.com>
	<OFDD876D03.47BD8548-ONC125756E.003EBFDA-C125756E.0041D8A7@hsbctrinkaus.de>
Message-ID: <13eac1860903030422o19d9bd88xf3767501679e2c0d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090303/d5c4f2e5/attachment.pl>

From Matthias.Koberstein at hsbctrinkaus.de  Tue Mar  3 13:26:53 2009
From: Matthias.Koberstein at hsbctrinkaus.de (Matthias.Koberstein at hsbctrinkaus.de)
Date: Tue, 3 Mar 2009 13:26:53 +0100
Subject: [R-SIG-Finance] Antwort: Re:  Antwort: [R-sig-finance] VaR
In-Reply-To: <13eac1860903030422o19d9bd88xf3767501679e2c0d@mail.gmail.com>
Message-ID: <OF0C33C561.D40621EE-ONC125756E.004434D7-C125756E.0044564C@hsbctrinkaus.de>



Hi

thats what I ment with the second paragraph. If any of the return
distributions is not normal or shifted/skewed or whatever, you usually have
a serious problem finding the quantile.

Cheers
Matthias





**** Ressourcen schonen, weniger drucken - Think before you print! ****

---------------------------------------------------------------------
Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
gestattet.

This e-mail and any attachments may contain confidential and / or
privileged information. If you are not the intended recipient or have
received this e-mail in error, please notify the sender immediately and
destroy this e-mail . Any unauthorized copying, storing, disclosure or
distribution of the contents of this e-mail is strictly forbidden.

---------------------------------------------------------------------
HSBC Trinkaus & Burkhardt AG
Sitz: D?sseldorf, K?nigsallee 21/23, 40212 D?sseldorf, Handelsregister:
Amtsgericht D?sseldorf HRB 54447
Mitglieder des Vorstands: Andreas Schmitz (Sprecher), Paul Hagen, Dr. Olaf
Huth, Carola Gr?fin v. Schmettow
Vorsitzender des Aufsichtsrats: Dr. Sieghardt Rometsch


                                                                           
             Micha Keijzers                                                
             <micha.keijzers at g                                             
             mail.com>                                                  An 
             Gesendet von:               Matthias.Koberstein at hsbctrinkaus. 
             r-sig-finance-bou           de                                
             nces at stat.math.et                                       Kopie 
             hz.ch                       r-sig-finance at stat.math.ethz.ch,  
                                         Bogaso                            
             Fax-Deckblatt:              <bogaso.christofer at gmail.com>     
             HSBCTuB                                                 Thema 
             03.03.2009 13:23            Re: [R-SIG-Finance] Antwort:      
                                         [R-sig-finance] VaR               
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Matthias and others,

Indeed, correlation possibly has something to do with it. But it's not the
whole story. VaR is a quantile of a distribution and you can draw up
examples that go wrong specifically there, regardless of correlation. I
constructed or adapted one, which must have been about three years ago I
think, based on an example which came from IIRC F?llmer's book "Stochastic
Finance" or "Quantitative Risk Management" by McNeil, Frey and Embrechts. I
would have to do some serious digging to be sure... The example was based
on
a very simple example of defaults in a loan portfolio. Explicitly showing
the quantiles in the loss distribution you could show that subadditivity
did
not hold when VaR is used as a risk measure.

Kind regards,
Micha Keijzers

2009/3/3 <Matthias.Koberstein at hsbctrinkaus.de>

> Hi Christofer,
>
> I think the analogy is allowed if you assume normal distributions for the
> assets.
> Since then the VaR is dependent on the volatility.
> The variance of two random variables (combined assets in this case) is
> given by
>
> Var(x+y)= E((x+y)^2) - E(x+y)^2
>
> which transforms to
> Var( x+y) = Var(x) + Var(y) + 2  * Covariance(x, y)
>
> So it all depends on the covariance of x to y.
> To give it a better feel this can be expressed in Correlation
>
> Var(x+y)= Var(x) + Var(y) + 2 * Vol(x) * Vol(y) * Correlation
>
> To better see  the effect throw some weights in w1, and w2 which combine
to
> one.
> Then
>
> Var( w1 x + w2 y)= Var(x) w1^2 + Var(y) w2^2 + 2 * w1 * w2 * Vol(x) * Vol
> (y) * Correlation
>
> the volatility used to estimate VaR is the square root of the variance.
> So you see that if correlation is 1 VaR is not sub-additive.
>
> Another point is if the distributions you use for the assets are not the
> same,
> the VaR can not even been combined easily but you have to find the
combined
> distributions of the assets in the portfolio (which can be quite
painful).
>
> I hope that helps. All the best
>
> Matthias
>
>
>
>
>
> **** Ressourcen schonen, weniger drucken - Think before you print! ****
>
> ---------------------------------------------------------------------
> Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
> rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
> sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
> sofort den Absender und vernichten Sie diese Mail. Das unerlaubte
Kopieren
> oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
> gestattet.
>
> This e-mail and any attachments may contain confidential and / or
> privileged information. If you are not the intended recipient or have
> received this e-mail in error, please notify the sender immediately and
> destroy this e-mail . Any unauthorized copying, storing, disclosure or
> distribution of the contents of this e-mail is strictly forbidden.
>
> ---------------------------------------------------------------------
> HSBC Trinkaus & Burkhardt AG
> Sitz: D?sseldorf, K?nigsallee 21/23, 40212 D?sseldorf, Handelsregister:
> Amtsgericht D?sseldorf HRB 54447
> Mitglieder des Vorstands: Andreas Schmitz (Sprecher), Paul Hagen, Dr.
Olaf
> Huth, Carola Gr?fin v. Schmettow
> Vorsitzender des Aufsichtsrats: Dr. Sieghardt Rometsch
>
>
>
>             Bogaso
>             <bogaso.christofe
>             r at gmail.com>                                               An
>             Gesendet von:               r-sig-finance at stat.math.ethz.ch
>             r-sig-finance-bou                                       Kopie
>             nces at stat.math.et
>             hz.ch                                                   Thema
>                                         [R-SIG-Finance] [R-sig-finance]
>             Fax-Deckblatt:              VaR
>             HSBCTuB
>             03.03.2009 12:24
>
>
>
>
>
>
>
>
>
> I frequently hear Value at risk i.e. VaR is not a coherent risk measure
> because, sum of VaR for two individual assets may be LOWER than VaR of
> portfolio consists of that two aseets i.e. VaR may not be sub-additive.
> However when I calculate VaR for general assets like Equity, commodity
etc,
> I see that VaR is actually sub-addtive i.e. portfolio VaR is always less
> than sum of individuals, which is reported as "diversification benefit".
> Can
> anyone give me a particular example why VaR is not sub-additive?
>
> Thanks
> --
> View this message in context:
> http://www.nabble.com/VaR-tp22306743p22306743.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

             [[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.

From Zeno.Adams at ebs.edu  Tue Mar  3 13:34:00 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Tue, 3 Mar 2009 13:34:00 +0100
Subject: [R-SIG-Finance] Antwort: [R-sig-finance] VaR
In-Reply-To: <13eac1860903030422o19d9bd88xf3767501679e2c0d@mail.gmail.com>
References: <22306743.post@talk.nabble.com><OFDD876D03.47BD8548-ONC125756E.003EBFDA-C125756E.0041D8A7@hsbctrinkaus.de>
	<13eac1860903030422o19d9bd88xf3767501679e2c0d@mail.gmail.com>
Message-ID: <9064522880125945B98983BBAECBA1CC985501@exchsrv001.ebs.local>

I don't know what you think about the topic but I feel that this matter of subadditivity is strongly overemphasized. Many authors argue in their papers that they will use the CVaR instead of the VaR because of the subbaditivity property (which goes back to Artzner, 1999). From my point of view the matter of getting the return distribution right, especially its variation over time, as well as the dependence structure between asset returns if the distribution is not elliptic is far more important for modeling the VaR adequately.

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Micha Keijzers
Gesendet: Dienstag, 3. M?rz 2009 13:22
An: Matthias.Koberstein at hsbctrinkaus.de
Cc: r-sig-finance at stat.math.ethz.ch; Bogaso
Betreff: Re: [R-SIG-Finance] Antwort: [R-sig-finance] VaR

Matthias and others,

Indeed, correlation possibly has something to do with it. But it's not the
whole story. VaR is a quantile of a distribution and you can draw up
examples that go wrong specifically there, regardless of correlation. I
constructed or adapted one, which must have been about three years ago I
think, based on an example which came from IIRC F?llmer's book "Stochastic
Finance" or "Quantitative Risk Management" by McNeil, Frey and Embrechts. I
would have to do some serious digging to be sure... The example was based on
a very simple example of defaults in a loan portfolio. Explicitly showing
the quantiles in the loss distribution you could show that subadditivity did
not hold when VaR is used as a risk measure.

Kind regards,
Micha Keijzers

2009/3/3 <Matthias.Koberstein at hsbctrinkaus.de>

> Hi Christofer,
>
> I think the analogy is allowed if you assume normal distributions for the
> assets.
> Since then the VaR is dependent on the volatility.
> The variance of two random variables (combined assets in this case) is
> given by
>
> Var(x+y)= E((x+y)^2) - E(x+y)^2
>
> which transforms to
> Var( x+y) = Var(x) + Var(y) + 2  * Covariance(x, y)
>
> So it all depends on the covariance of x to y.
> To give it a better feel this can be expressed in Correlation
>
> Var(x+y)= Var(x) + Var(y) + 2 * Vol(x) * Vol(y) * Correlation
>
> To better see  the effect throw some weights in w1, and w2 which combine to
> one.
> Then
>
> Var( w1 x + w2 y)= Var(x) w1^2 + Var(y) w2^2 + 2 * w1 * w2 * Vol(x) * Vol
> (y) * Correlation
>
> the volatility used to estimate VaR is the square root of the variance.
> So you see that if correlation is 1 VaR is not sub-additive.
>
> Another point is if the distributions you use for the assets are not the
> same,
> the VaR can not even been combined easily but you have to find the combined
> distributions of the assets in the portfolio (which can be quite painful).
>
> I hope that helps. All the best
>
> Matthias
>
>
>
>
>
> **** Ressourcen schonen, weniger drucken - Think before you print! ****
>
> ---------------------------------------------------------------------
> Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
> rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
> sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
> sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
> oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
> gestattet.
>
> This e-mail and any attachments may contain confidential and / or
> privileged information. If you are not the intended recipient or have
> received this e-mail in error, please notify the sender immediately and
> destroy this e-mail . Any unauthorized copying, storing, disclosure or
> distribution of the contents of this e-mail is strictly forbidden.
>
> ---------------------------------------------------------------------
> HSBC Trinkaus & Burkhardt AG
> Sitz: D?sseldorf, K?nigsallee 21/23, 40212 D?sseldorf, Handelsregister:
> Amtsgericht D?sseldorf HRB 54447
> Mitglieder des Vorstands: Andreas Schmitz (Sprecher), Paul Hagen, Dr. Olaf
> Huth, Carola Gr?fin v. Schmettow
> Vorsitzender des Aufsichtsrats: Dr. Sieghardt Rometsch
>
>
>
>             Bogaso
>             <bogaso.christofe
>             r at gmail.com>                                               An
>             Gesendet von:               r-sig-finance at stat.math.ethz.ch
>             r-sig-finance-bou                                       Kopie
>             nces at stat.math.et
>             hz.ch                                                   Thema
>                                         [R-SIG-Finance] [R-sig-finance]
>             Fax-Deckblatt:              VaR
>             HSBCTuB
>             03.03.2009 12:24
>
>
>
>
>
>
>
>
>
> I frequently hear Value at risk i.e. VaR is not a coherent risk measure
> because, sum of VaR for two individual assets may be LOWER than VaR of
> portfolio consists of that two aseets i.e. VaR may not be sub-additive.
> However when I calculate VaR for general assets like Equity, commodity etc,
> I see that VaR is actually sub-addtive i.e. portfolio VaR is always less
> than sum of individuals, which is reported as "diversification benefit".
> Can
> anyone give me a particular example why VaR is not sub-additive?
>
> Thanks
> --
> View this message in context:
> http://www.nabble.com/VaR-tp22306743p22306743.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

	[[alternative HTML version deleted]]


EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Rektor/CEO; Dr. Reimar Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Verwaltungsrat: Dr. Hellmut K. Albrecht, Vorsitzender

From Xiaochen.Sun at brunel.ac.uk  Tue Mar  3 14:01:07 2009
From: Xiaochen.Sun at brunel.ac.uk (Xiaochen Sun)
Date: Tue, 3 Mar 2009 13:01:07 -0000
Subject: [R-SIG-Finance] Antwort: [R-sig-finance] VaR
References: <22306743.post@talk.nabble.com><OFDD876D03.47BD8548-ONC125756E.003EBFDA-C125756E.0041D8A7@hsbctrinkaus.de><13eac1860903030422o19d9bd88xf3767501679e2c0d@mail.gmail.com>
	<9064522880125945B98983BBAECBA1CC985501@exchsrv001.ebs.local>
Message-ID: <E386E504246A9249A9176B5BEEC13B6F018CEC32@UXEXMBU116.academic.windsor>

I fully agreed with Adams.
For example, to apply EVT to each marginal distribution of return and to apply copula to te dependence structure.
I think this has been also addressed in the book "Introduction to Modern Portfolio Optimization with NuOPT" by Bernd Scherer.
Regards,
Michael

________________________________

From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of Adams, Zeno
Sent: Tue 03/03/2009 12:34
To: Micha Keijzers; Matthias.Koberstein at hsbctrinkaus.de
Cc: r-sig-finance at stat.math.ethz.ch; Bogaso
Subject: Re: [R-SIG-Finance] Antwort: [R-sig-finance] VaR



I don't know what you think about the topic but I feel that this matter of subadditivity is strongly overemphasized. Many authors argue in their papers that they will use the CVaR instead of the VaR because of the subbaditivity property (which goes back to Artzner, 1999). From my point of view the matter of getting the return distribution right, especially its variation over time, as well as the dependence structure between asset returns if the distribution is not elliptic is far more important for modeling the VaR adequately.

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Micha Keijzers
Gesendet: Dienstag, 3. M?rz 2009 13:22
An: Matthias.Koberstein at hsbctrinkaus.de
Cc: r-sig-finance at stat.math.ethz.ch; Bogaso
Betreff: Re: [R-SIG-Finance] Antwort: [R-sig-finance] VaR

Matthias and others,

Indeed, correlation possibly has something to do with it. But it's not the
whole story. VaR is a quantile of a distribution and you can draw up
examples that go wrong specifically there, regardless of correlation. I
constructed or adapted one, which must have been about three years ago I
think, based on an example which came from IIRC F?llmer's book "Stochastic
Finance" or "Quantitative Risk Management" by McNeil, Frey and Embrechts. I
would have to do some serious digging to be sure... The example was based on
a very simple example of defaults in a loan portfolio. Explicitly showing
the quantiles in the loss distribution you could show that subadditivity did
not hold when VaR is used as a risk measure.

Kind regards,
Micha Keijzers

2009/3/3 <Matthias.Koberstein at hsbctrinkaus.de>

> Hi Christofer,
>
> I think the analogy is allowed if you assume normal distributions for the
> assets.
> Since then the VaR is dependent on the volatility.
> The variance of two random variables (combined assets in this case) is
> given by
>
> Var(x+y)= E((x+y)^2) - E(x+y)^2
>
> which transforms to
> Var( x+y) = Var(x) + Var(y) + 2  * Covariance(x, y)
>
> So it all depends on the covariance of x to y.
> To give it a better feel this can be expressed in Correlation
>
> Var(x+y)= Var(x) + Var(y) + 2 * Vol(x) * Vol(y) * Correlation
>
> To better see  the effect throw some weights in w1, and w2 which combine to
> one.
> Then
>
> Var( w1 x + w2 y)= Var(x) w1^2 + Var(y) w2^2 + 2 * w1 * w2 * Vol(x) * Vol
> (y) * Correlation
>
> the volatility used to estimate VaR is the square root of the variance.
> So you see that if correlation is 1 VaR is not sub-additive.
>
> Another point is if the distributions you use for the assets are not the
> same,
> the VaR can not even been combined easily but you have to find the combined
> distributions of the assets in the portfolio (which can be quite painful).
>
> I hope that helps. All the best
>
> Matthias
>
>
>
>
>
> **** Ressourcen schonen, weniger drucken - Think before you print! ****
>
> ---------------------------------------------------------------------
> Diese E-Mail sowie eventuelle Anh?nge enthalten vertrauliche und / oder
> rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat
> sind oder diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte
> sofort den Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren
> oder Speichern sowie die unbefugte Weitergabe dieser E-Mail sind nicht
> gestattet.
>
> This e-mail and any attachments may contain confidential and / or
> privileged information. If you are not the intended recipient or have
> received this e-mail in error, please notify the sender immediately and
> destroy this e-mail . Any unauthorized copying, storing, disclosure or
> distribution of the contents of this e-mail is strictly forbidden.
>
> ---------------------------------------------------------------------
> HSBC Trinkaus & Burkhardt AG
> Sitz: D?sseldorf, K?nigsallee 21/23, 40212 D?sseldorf, Handelsregister:
> Amtsgericht D?sseldorf HRB 54447
> Mitglieder des Vorstands: Andreas Schmitz (Sprecher), Paul Hagen, Dr. Olaf
> Huth, Carola Gr?fin v. Schmettow
> Vorsitzender des Aufsichtsrats: Dr. Sieghardt Rometsch
>
>
>
>             Bogaso
>             <bogaso.christofe
>             r at gmail.com>                                               An
>             Gesendet von:               r-sig-finance at stat.math.ethz.ch
>             r-sig-finance-bou                                       Kopie
>             nces at stat.math.et
>             hz.ch                                                   Thema
>                                         [R-SIG-Finance] [R-sig-finance]
>             Fax-Deckblatt:              VaR
>             HSBCTuB
>             03.03.2009 12:24
>
>
>
>
>
>
>
>
>
> I frequently hear Value at risk i.e. VaR is not a coherent risk measure
> because, sum of VaR for two individual assets may be LOWER than VaR of
> portfolio consists of that two aseets i.e. VaR may not be sub-additive.
> However when I calculate VaR for general assets like Equity, commodity etc,
> I see that VaR is actually sub-addtive i.e. portfolio VaR is always less
> than sum of individuals, which is reported as "diversification benefit".
> Can
> anyone give me a particular example why VaR is not sub-additive?
>
> Thanks
> --
> View this message in context:
> http://www.nabble.com/VaR-tp22306743p22306743.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

        [[alternative HTML version deleted]]


EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Rektor/CEO; Dr. Reimar Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Verwaltungsrat: Dr. Hellmut K. Albrecht, Vorsitzender
_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From brian at braverock.com  Tue Mar  3 14:12:24 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 03 Mar 2009 07:12:24 -0600
Subject: [R-SIG-Finance] [R-sig-finance] VaR
In-Reply-To: <138157806.1802961236080041494.JavaMail.javamailuser@localhost>
References: <138157806.1802961236080041494.JavaMail.javamailuser@localhost>
Message-ID: <49AD2CB8.1010108@braverock.com>

Mark is definitely on the right track here.  Bastian has also pointed 
out the major paper that introduced coherent risk measures.

First, I need to say that there are many definitions of Value at Risk, 
using different underlying assumptions to calculate the risk.  Jorion's 
book "Value at Risk" is probably the most often cited collection of 
these definitions.

Now, that bit aside, VaR, generally speaking, is defined as the  minimum 
loss level at a certain confidence level. So, for 95% confidence, the 
minimum amount you could expect to lose.  With this definition on daily 
data, 95% VaR thus describes that the losses should be below this level 
on 19 out of 20 trading days.  The problem comes in the 20th day, or the 
day that exceeds your 95% threshold.  On that day, your "tail loss" is 
defined by the process beyond the VaR, it is outside of the VaR 
estimating process.

Many common VaR estimators for single instruments use some variation of 
a normal distribution (Gaussian) assumption.  This means, as a result, 
that for a single instrument the VaR is defined by the Gaussian 
distribution quantile with the mean and variance of the observed 
series.   In a portfolio context, the VaR using these distributional 
assumptions will be defined by the joint distribution of the moments, 
the mean of the portfolio and the covariance of the instruments.   To 
Mark's point, you can end up with a portfolio variance that is larger 
than the sum of the variance of the individual instruments.

Your original understanding of sub-additive is incorrect as well.  A 
sub-additive measure will add up precisely to the portfolio measure. 
(neither more nor less). See Artzner's papers.

Several other people responding to this post have pointed out that 
getting the distributional assumption right is the important bit.  This 
is exactly correct, as what you are trying to figure out is the risk to 
a real portfolio.  Most real instruments are do not have normally 
distributed returns, so you need to account for that non-normality.

Portfolio measures of VaR and ES  are either marginal, which shows what 
happens when one instrument is removed, or component. Component measures 
of VaR should show the contribution to VaR/ES of individual instruments 
to the risk of the whole portfolio.  Here subadditivity is required if 
you wish to rearrange your aggregation criteria (e.g. to re-examine the 
portfolio by currencies, instrument types, or styles you want the 
portfolio risk measure to stay the same, but allow you to look at the 
component risks of these various slices).  In other cases, subadditivity 
doesn't really gain you anything, for instance if you are trying to 
determine the risk of loss of an individual position to calculate 
stop-loss or exit criteria.

PerformanceAnalytics contains several different methods for calculating 
VaR and CVaR/ES, including their portfolio and component variants.  Many 
of the measures we present are indeed sub-additive, if you're looking 
for a subadditive measure.

Regards,

    - Brian

-- 
Brian G. Peterson
http://www.braverock.com/brian/
IM: bgpbraverock
Ph: +1 773-459-4973


markleeds at verizon.net wrote:
>  Hi  Christofer: I don't know if the analogy is allowed but this can 
> happen with regular statistical
> variance so maybe it can happen with Value at Risk also ? if you have 
> a covariance matrix
> of 2 assets with portfolio weights w_1 and w_2 and the 2 assets have 
> positive covariance, then the resulting variance of the portfolio will 
> be greater than the sum of the individual variances of the two assets 
> with weights w_1 and w_2. ( w_1*v_1 + w_2*v_2 ).
>
> now I have no idea if the result for statistical variance holds for 
> Value at Risk ( i don't know the definition of Value at Risk ) but, if 
> it does, then that's probably the answer. Hopefully someone else will 
> tell us if the analogy is allowed ?
>
>
>
> On Tue, Mar 3, 2009 at  6:20 AM, Bogaso wrote:
>
>> I frequently hear Value at risk i.e. VaR is not a coherent risk measure
>> because, sum of VaR for two individual assets may be LOWER than VaR of
>> portfolio consists of that two aseets i.e. VaR may not be sub-additive.
>> However when I calculate VaR for general assets like Equity, 
>> commodity etc,
>> I see that VaR is actually sub-addtive i.e. portfolio VaR is always less
>> than sum of individuals, which is reported as "diversification 
>> benefit". Can
>> anyone give me a particular example why VaR is not sub-additive?
>>
>> Thanks


From brian at braverock.com  Tue Mar  3 22:14:01 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 03 Mar 2009 15:14:01 -0600
Subject: [R-SIG-Finance] Antwort: [R-sig-finance] VaR
In-Reply-To: <9064522880125945B98983BBAECBA1CC985501@exchsrv001.ebs.local>
References: <22306743.post@talk.nabble.com><OFDD876D03.47BD8548-ONC125756E.003EBFDA-C125756E.0041D8A7@hsbctrinkaus.de>	<13eac1860903030422o19d9bd88xf3767501679e2c0d@mail.gmail.com>
	<9064522880125945B98983BBAECBA1CC985501@exchsrv001.ebs.local>
Message-ID: <49AD9D99.8000104@braverock.com>

Subadditivity is critically important in a portfolio context if you wish 
to dis-aggregate or slice the portfolio in different ways, as I 
mentioned in my previous email.  You need to understand risk 
contribution in a coherent fashion if you want to be able to rearrange a 
large portfolio into sub-portfolios and have a rational and fungible 
understanding of what each of those slices contributes to the total risk 
of your portfolio.

That said, ES/CVaR is often just an excuse for not getting the tail 
distribution correct.  By providing the "mean loss beyond the VaR" 
CVaR/ES try to smooth out the tail risk into one number.  This has its 
own risks, but is at least honest about the fact that you don't truly 
know the distribution of the returns under all circumstances.

As for the "dependence structure between asset[s]", my personal 
preference is to use more than two moments, by extending things to 
include skewness and kurtosis.  In a portfolio context, you then use all 
four moments of each asset, and all the co-moments (covariance, 
coskewness, cokurtosis).  I find this to be a more intellectually 
satisfying approach than just finding the best-fitting distribution out 
of some arbitrary list of distributions (or copulae, etc.) because the 
first four moments of the observed returns are easily understood, have 
real economic meaning, and can be communicated to most other investment 
professionals.  Contrast that with the greek-alphabet-soup of parameters 
to the arbitrary distribution of your choice that have mathematical 
meaning but not direct financial meaning.

I agree completely that subadditivity is not always important, but 
neither is it unimportant.  The trick is knowing when you need a 
subadditive measure.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


Adams, Zeno wrote:
> I don't know what you think about the topic but I feel that this matter of subadditivity is strongly overemphasized. Many authors argue in their papers that they will use the CVaR instead of the VaR because of the subbaditivity property (which goes back to Artzner, 1999). From my point of view the matter of getting the return distribution right, especially its variation over time, as well as the dependence structure between asset returns if the distribution is not elliptic is far more important for modeling the VaR adequately.
>
> -----Urspr?ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Micha Keijzers
> Gesendet: Dienstag, 3. M?rz 2009 13:22
> An: Matthias.Koberstein at hsbctrinkaus.de
> Cc: r-sig-finance at stat.math.ethz.ch; Bogaso
> Betreff: Re: [R-SIG-Finance] Antwort: [R-sig-finance] VaR
>
> Matthias and others,
>
> Indeed, correlation possibly has something to do with it. But it's not the
> whole story. VaR is a quantile of a distribution and you can draw up
> examples that go wrong specifically there, regardless of correlation. I
> constructed or adapted one, which must have been about three years ago I
> think, based on an example which came from IIRC F?llmer's book "Stochastic
> Finance" or "Quantitative Risk Management" by McNeil, Frey and Embrechts. I
> would have to do some serious digging to be sure... The example was based on
> a very simple example of defaults in a loan portfolio. Explicitly showing
> the quantiles in the loss distribution you could show that subadditivity did
> not hold when VaR is used as a risk measure.
>
> Kind regards,
> Micha Keijzers
>
> 2009/3/3 <Matthias.Koberstein at hsbctrinkaus.de>
>
>   
>> Hi Christofer,
>>
>> I think the analogy is allowed if you assume normal distributions for the
>> assets.
>> Since then the VaR is dependent on the volatility.
>> The variance of two random variables (combined assets in this case) is
>> given by
>>
>> Var(x+y)= E((x+y)^2) - E(x+y)^2
>>
>> which transforms to
>> Var( x+y) = Var(x) + Var(y) + 2  * Covariance(x, y)
>>
>> So it all depends on the covariance of x to y.
>> To give it a better feel this can be expressed in Correlation
>>
>> Var(x+y)= Var(x) + Var(y) + 2 * Vol(x) * Vol(y) * Correlation
>>
>> To better see  the effect throw some weights in w1, and w2 which combine to
>> one.
>> Then
>>
>> Var( w1 x + w2 y)= Var(x) w1^2 + Var(y) w2^2 + 2 * w1 * w2 * Vol(x) * Vol
>> (y) * Correlation
>>
>> the volatility used to estimate VaR is the square root of the variance.
>> So you see that if correlation is 1 VaR is not sub-additive.
>>
>> Another point is if the distributions you use for the assets are not the
>> same,
>> the VaR can not even been combined easily but you have to find the combined
>> distributions of the assets in the portfolio (which can be quite painful).
>>
>> I hope that helps. All the best
>>
>> Matthias
>>
>> ---------------------------------------------------------------------
>> HSBC Trinkaus & Burkhardt AG
>> Sitz: D?sseldorf, K?nigsallee 21/23, 40212 D?sseldorf, Handelsregister:
>> Amtsgericht D?sseldorf HRB 54447
>> Mitglieder des Vorstands: Andreas Schmitz (Sprecher), Paul Hagen, Dr. Olaf
>> Huth, Carola Gr?fin v. Schmettow
>> Vorsitzender des Aufsichtsrats: Dr. Sieghardt Rometsch
>>
>>
>>
>>             Bogaso
>>             <bogaso.christofe
>>             r at gmail.com>                                               An
>>             Gesendet von:               r-sig-finance at stat.math.ethz.ch
>>             r-sig-finance-bou                                       Kopie
>>             nces at stat.math.et
>>             hz.ch                                                   Thema
>>                                         [R-SIG-Finance] [R-sig-finance]
>>             Fax-Deckblatt:              VaR
>>             HSBCTuB
>>             03.03.2009 12:24
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> I frequently hear Value at risk i.e. VaR is not a coherent risk measure
>> because, sum of VaR for two individual assets may be LOWER than VaR of
>> portfolio consists of that two aseets i.e. VaR may not be sub-additive.
>> However when I calculate VaR for general assets like Equity, commodity etc,
>> I see that VaR is actually sub-addtive i.e. portfolio VaR is always less
>> than sum of individuals, which is reported as "diversification benefit".
>> Can
>> anyone give me a particular example why VaR is not sub-additive?
>>
>> Thanks
>>


From VOSSK at kochind.com  Wed Mar  4 01:06:10 2009
From: VOSSK at kochind.com (Voss, Kent)
Date: Tue, 3 Mar 2009 17:06:10 -0700
Subject: [R-SIG-Finance] Specifying an expected mu and Sigma for fPortfolio
Message-ID: <13CB99597D0BAA43B07ABFCE29E45F6B0262FB44@phx0mbx01.kochind.com>

I've read the other post on this topic that suggests specifying a custom
estimator, but I cannot seem to even get that to work as I had expected.

Below is the code to replicate my issue and illustrates what I'm trying
to do.  Essentially I am trying to override the calculation of the mu
and Sigma in the Estimator and replace it with static data.  This should
yield the same optimization results regardless of what data I send to
the estimator (since I'm overriding it), but it does not appear to work
for me that way.  Please see the code below for details.  Any help at
all would be greatly appreciated.  

Thanks,

Kent

#### OBJECTIVE:  Specify an expected return and covariance matrix for
use in the optimization

# Define the data to be used in the example
Data <- as.timeSeries(data(smallcap.ts))
Data <- Data[,1:3]

# Create an expected return and covariance matrix based on the first 12
months
# It could be anything, but just to create a static expected return and
cov matrix
# In reality the expected returns are not generated from the historical
data, and so
# feeding the estimator a paricular slice of data is not a workable
solution.
ExpRet <- apply(head(Data,12), 2, mean)
ExpCov <- cov(head(Data,12))

# Now what I would like to do is to just feed the expected return and
cov
# matrix to one of the fPorfolio functions like "portfolioFrontier"

# The only way I can think to do that based on what I've found is to
# specify a custom Estimator, that just disregards what's passed to it
# and sends back the expected return and covariance matrix.
# This seems a silly way to do it, and I'm sure there's a better way,
but
# in a pinch, I had hoped this would work.  But it doesn't appear to.

# Specify the custom estimator
myEstimator <- function(x, Spec=NULL) {

  # Just get the specified return and cov from the environment
  expectedAssetReturns <- get("ExpRet")
  assetCov <- get("ExpCov")

  # Ignore the inputted data, and send back the expected returns
  return(list(mu=expectedAssetReturns,Sigma=assetCov))
}

# Now create a portfolio Spec using the custom estimator defined above.
Spec = portfolioSpec()
setTargetReturn(Spec) = .05
setEstimator(Spec) <- "myEstimator"

# Subset out some of the data and calculate the portfolio frontier.
# I subset out a piece of the data, so that I can change the data set
input
# which should have no effect if I'm specifying the mu and cov, but
# for some reason it does, and hence my dilemma.

Data1 <- Data[20:40,]
frontier1 <- portfolioFrontier(Data1,Spec,"LongOnly")

Data2 <- Data[41:60,]
frontier2 <- portfolioFrontier(Data2,Spec,"LongOnly")

# Now I thought that frontier1 would look identical to frontier2 since
I'm overriding
# the inputs with the specified mu and Sigma in the "myEstimator", the
frontier1 and
# frontier2 look different.
op <- par(mfrow = c(2,1),mar = c(3,3,3,3),mgp=c(1.5,0.6,0),oma =
c(1,1,1,1))
frontierPlot(frontier1)
frontierPlot(frontier2)

# I'm clearly doing something wrong, and I'm sure it's some
misunderstanding on my part
# but I can't for the life of me figure it out.  Any help would be
greatly appreciated.




"EMF <kochind.com>" made the following annotations.
------------------------------------------------------------------------------
The information in this e-mail and any attachments is co...{{dropped:14}}


From christian.langkamp at basf.com  Wed Mar  4 12:30:52 2009
From: christian.langkamp at basf.com (Christian Langkamp)
Date: Wed, 4 Mar 2009 03:30:52 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] VaR
In-Reply-To: <22306743.post@talk.nabble.com>
References: <22306743.post@talk.nabble.com>
Message-ID: <22328207.post@talk.nabble.com>


Hello
I remember a simple example given for this subadditivity feature in the GARP
Magazine some time ago. I will try to reproduce it, but ask for apologies if
I mixed up two different terms.
You have a portfolio of two credit default swaps (digital options, ... main
point is something extremely unsmooth and really tail oriented)
A and B both Payout -1 with respective probabilities 0.5 % and no
correlation.
You compare then the 1 % VaR of the portfolio of A, B, and A+B. 
VaR(A, 1%) = 0 = VaR(B, 1%) whereas VaR (A+B, 1%) = 1 (in 1% of cases either
A or B defaults) which shouldn't be the case because Diversification should
reduce the risk.

Whilst this can occur in a banking context, in a corporate where all payouts
are linear (forwards) or continuous (normal options) this situation
practically cannot occur and thus this aspect is highly irrelevant. On a CDS
portfolio this is an entirely different game I think, but the extent of the
problem I am not familiar with. 

The amount of assumptions to construct a portfolio where this Subadditivity
feature produces 'wrong results' I think shows that whatever problems VaR
holds, this is not its major one, and hence should not be worried about too
much.

Please feel free to correct the above, or supply a link to the original if
ready at hand
Christian Langkamp
-- 
View this message in context: http://www.nabble.com/VaR-tp22306743p22328207.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From david.jessop at ubs.com  Wed Mar  4 12:40:54 2009
From: david.jessop at ubs.com (david.jessop at ubs.com)
Date: Wed, 4 Mar 2009 11:40:54 -0000
Subject: [R-SIG-Finance] [R-sig-finance] VaR
In-Reply-To: <mailman.3.1236164401.661.r-sig-finance@stat.math.ethz.ch>
References: <mailman.3.1236164401.661.r-sig-finance@stat.math.ethz.ch>
Message-ID: <25492B412B325B4FB1FED95013D3E5CE07EACD05@NLDNC105PEX1.ubsw.net>

Hi

A very simple (if somewhat artificial) example.  Suppose you have an
option which pays off zero 96% of the time and -10 4% of the time.  The
95% VaR is zero (obviously).  Now suppose you have another similar
option on an uncorrelated event, the VaR of this is also zero.  Now
combine them together.  There is a 92.2% probability of getting zero,
0.2% of getting -20 and 7.6% of getting -10. Hence the VaR of the
combined portfolio is -10.  

In the case of equities or anything with a reasonable distribution this
type of thing is unlikely to happen. 

Regards,

David


Message: 1
Date: Tue, 3 Mar 2009 03:20:55 -0800 (PST)
From: Bogaso <bogaso.christofer at gmail.com>
Subject: [R-SIG-Finance] [R-sig-finance] VaR
To: r-sig-finance at stat.math.ethz.ch
Message-ID: <22306743.post at talk.nabble.com>
Content-Type: text/plain; charset=us-ascii


I frequently hear Value at risk i.e. VaR is not a coherent risk measure
because, sum of VaR for two individual assets may be LOWER than VaR of
portfolio consists of that two aseets i.e. VaR may not be sub-additive.
However when I calculate VaR for general assets like Equity, commodity
etc,
I see that VaR is actually sub-addtive i.e. portfolio VaR is always less
than sum of individuals, which is reported as "diversification benefit".
Can
anyone give me a particular example why VaR is not sub-additive?

Thanks
-- 
View this message in context:
http://www.nabble.com/VaR-tp22306743p22306743.html
Sent from the Rmetrics mailing list archive at Nabble.com.




Issued by UBS AG or affiliates to professional investors...{{dropped:27}}


From enricoschumann at yahoo.de  Wed Mar  4 12:56:07 2009
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Wed, 4 Mar 2009 12:56:07 +0100
Subject: [R-SIG-Finance] [R-sig-finance] VaR
In-Reply-To: <25492B412B325B4FB1FED95013D3E5CE07EACD05@NLDNC105PEX1.ubsw.net>
Message-ID: <166362.31791.bm@omp205.mail.ukl.yahoo.com>

 
sorry, i did not follow this thread closely, so maybe i repeat something
that has already been said.

to quote david: "In the case of equities or anything with a reasonable
distribution this type of thing is unlikely to happen."  theoretically,
"reasonable" is elliptical distributions, for which var actually is a
coherent risk measure. a very readable paper describing this can be found
here http://www.math.ethz.ch/~strauman/light/risk.pdf

regards
enrico


-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von
david.jessop at ubs.com
Gesendet: Mittwoch, 4. M?rz 2009 12:41
An: r-sig-finance at stat.math.ethz.ch
Betreff: Re: [R-SIG-Finance] [R-sig-finance] VaR

Hi

A very simple (if somewhat artificial) example.  Suppose you have an option
which pays off zero 96% of the time and -10 4% of the time.  The 95% VaR is
zero (obviously).  Now suppose you have another similar option on an
uncorrelated event, the VaR of this is also zero.  Now combine them
together.  There is a 92.2% probability of getting zero, 0.2% of getting -20
and 7.6% of getting -10. Hence the VaR of the combined portfolio is -10.  

In the case of equities or anything with a reasonable distribution this type
of thing is unlikely to happen. 

Regards,

David


Message: 1
Date: Tue, 3 Mar 2009 03:20:55 -0800 (PST)
From: Bogaso <bogaso.christofer at gmail.com>
Subject: [R-SIG-Finance] [R-sig-finance] VaR
To: r-sig-finance at stat.math.ethz.ch
Message-ID: <22306743.post at talk.nabble.com>
Content-Type: text/plain; charset=us-ascii


I frequently hear Value at risk i.e. VaR is not a coherent risk measure
because, sum of VaR for two individual assets may be LOWER than VaR of
portfolio consists of that two aseets i.e. VaR may not be sub-additive.
However when I calculate VaR for general assets like Equity, commodity etc,
I see that VaR is actually sub-addtive i.e. portfolio VaR is always less
than sum of individuals, which is reported as "diversification benefit".
Can
anyone give me a particular example why VaR is not sub-additive?

Thanks
--
View this message in context:
http://www.nabble.com/VaR-tp22306743p22306743.html
Sent from the Rmetrics mailing list archive at Nabble.com.




Issued by UBS AG or affiliates to professional =\ invest...{{dropped:15}}


From vlanschot at yahoo.com  Wed Mar  4 13:56:53 2009
From: vlanschot at yahoo.com (R@Nabble)
Date: Wed, 4 Mar 2009 04:56:53 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
In-Reply-To: <22247689.post@talk.nabble.com>
References: <22222236.post@talk.nabble.com>
	<338466.18985.bm@omp220.mail.ukl.yahoo.com>
	<22225107.post@talk.nabble.com>
	<e8e755250902260913x38dadf0xc40d84ea7e050a0b@mail.gmail.com>
	<49A71BD5.5030503@bank-banque-canada.ca>
	<22247689.post@talk.nabble.com>
Message-ID: <22329485.post@talk.nabble.com>


Jeff,

I've taken the liberty to download and adjust your preliminary function code
for persp() for my purposes, i.e. simply replacing your ts-matrix with my
own doesn't work. Unfortunately (due to my limited expertise of R) I haven't
been able to get the lables for the y- and x-axis (you use trans3d, etc.)
replaced with my own "rownames" and "colnames". For now, I will simply keep
the x-axis as 1:nrows and y-axis as 1:ncols, i.e. the default persp
settings.

Request/suggestion: don't know whether it would be possible for you in your
final code to simply allow a user to feed a zoo/ts matrix as main input,
perhaps by specifying the date-format, and for your function then to
automatically create the axes labels.

Still, your preliminary code has enabled me more flexibility re inputs for
the persp function and will leave it for now as is. 

Paul,

Sorry, but I haven't had time yet to play around with the tframe package.

Thx again to both.

PS


R at Nabble wrote:
> 
> Jeff and Paul,
> 
> Thanks for the suggestions. Jeff's png seems to reflect what I need, will
> investigate/apply, and report back.
> 
> PS
> 
> Paul Gilbert wrote:
>> 
>> In the tframePlus package there is a function called tfpersp() that does 
>> this.  I'm sure it can be improved, I've never been really happy with 
>> it. But you might want to compare, to be sure you are improving it.
>> 
>> Paul
>> 
>> Jeff Ryan wrote:
>>> PS,
>>> 
>>> I have been working on adding a chartSeries3d function to quantmod.
>>> The code isn't in place yet, but it relies on persp, and some extra
>>> manipulation to offer much better axis labeling.  It is designed for
>>> xts/zoo structures in particular.  The final code will work for any
>>> time-series.
>>> 
>>> This should be in the next release of quantmod, but I've posted the
>>> alpha code and a .png here:
>>> 
>>> http://www.quantmod.com/example/chartSeries3d
>>> 
>>> HTH,
>>> Jeff
>>> 
>>> 
>>> R/Finance 2009: Applied Finance with R
>>> April 24, 25 2009 Chicago, IL USA
>>> http://www.RinFinance.com
>>> 
>>> 
>>> On Thu, Feb 26, 2009 at 8:20 AM, R at Nabble <vlanschot at yahoo.com> wrote:
>>> 
>>>>Thank you Enrico. Your example helped me to solve a (embarrasingly
basic)
>>>>error in my code.
>>>>
>>>>Plots fine now.
>>>>
>>>>Thx again.
>>>>
>>>>PS
>>>>
>>>>Enrico Schumann wrote:
>>>>
>>>>>it should be helpful to give a little code example, or at least the
>>>>>content
>>>>>of the error messages that you received.
>>>>>
>>>>>the following gives me a perspective plot...
>>>>>
>>>>>require(zoo)
>>>>>nC     <- 10  # columns
>>>>>nO     <- 100 # observations
>>>>>dataM  <- array(runif(nC * nO), dim=c(nO, nC))
>>>>>zz     <- zoo(dataM, 1:nO)
>>>>>persp(1:nO,1:nC,zz)
>>>>>
>>>>>
>>>>>regards
>>>>>enrico
>>>>>-----Urspr?ngliche Nachricht-----
>>>>>Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von
R at Nabble
>>>>>Gesendet: Donnerstag, 26. Februar 2009 12:26
>>>>>An: r-sig-finance at stat.math.ethz.ch
>>>>>Betreff: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
>>>>>
>>>>>
>>>>>I'd like to be able to plot a time-series matrix (i.e. first col
contains
>>>>>sorted dates, rest of cols contains non-sorted data, with headings at
the
>>>>>top; current format is as a zoo object) as a surface/3D chart. The
>>>>>function
>>>>>persp() is giving me errors, even if I transform data as core data,
etc.
>>>>>
>>>>>Any suggestions / other functions available?
>>>>>
>>>>>Thx,
>>>>>
>>>>>PS
>>>>>--
>>>>>View this message in context:
>>>>>http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22222236.html
>>>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>>
>>>>>_______________________________________________
>>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>-- Subscriber-posting only.
>>>>>-- If you want to post, subscribe first.
>>>>>No virus found in this incoming message.
>>>>>Checked by AVG - www.avg.com
>>>>>
>>>>>
>>>>>06:40:00
>>>>>
>>>>>_______________________________________________
>>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>-- Subscriber-posting only.
>>>>>-- If you want to post, subscribe first.
>>>>>
>>>>>
>>>>
>>>>--
>>>>View this message in context:
http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22225107.html
>>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>
>>>>_______________________________________________
>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>-- Subscriber-posting only.
>>>>-- If you want to post, subscribe first.
>>> 
>>> 
>>> 
>>> 
>> ====================================================================================
>> 
>> La version fran?aise suit le texte anglais.
>> 
>> ------------------------------------------------------------------------------------
>> 
>> This email may contain privileged and/or confidential in...{{dropped:26}}
>> 
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>> 
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22329485.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From john.b.gavin at gmail.com  Wed Mar  4 12:45:25 2009
From: john.b.gavin at gmail.com (John Gavin)
Date: Wed, 4 Mar 2009 11:45:25 +0000 (UTC)
Subject: [R-SIG-Finance] [R-sig-finance] VaR
References: <22306743.post@talk.nabble.com> <49AD1C1D.7040502@yahoo.co.uk>
Message-ID: <loom.20090304T114222-576@post.gmane.org>

Bastian Offermann <bastian2507hk <at> yahoo.co.uk> writes:

> 
> A brief example is given in "Introduction to Modern Portfolio 
> Optimization with NuOPT..." by Bernd Scherer (2005), page 180. That's 
> most probably what you are looking for.
> 
> This might also be useful
> 
> "Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. 
> Coherent measures
> of risk. Mathematical Finance, pages 203-228 (1999)
> 
> Regards
> 


Hi,

Fyi, I found 

Coherent Measures of Risk, An Exposition for the Lay Actuary
by Glenn Meyers
http://ur.ly/8Y0

to be an easy and worthwhile read on this topic.

Regards,

John.


From pgilbert at bank-banque-canada.ca  Wed Mar  4 17:43:26 2009
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 04 Mar 2009 11:43:26 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
In-Reply-To: <22329485.post@talk.nabble.com>
References: <22222236.post@talk.nabble.com>	<338466.18985.bm@omp220.mail.ukl.yahoo.com>	<22225107.post@talk.nabble.com>	<e8e755250902260913x38dadf0xc40d84ea7e050a0b@mail.gmail.com>	<49A71BD5.5030503@bank-banque-canada.ca>	<22247689.post@talk.nabble.com>
	<22329485.post@talk.nabble.com>
Message-ID: <49AEAFAE.30201@bank-banque-canada.ca>



R at Nabble wrote:
> Jeff,
> 
> I've taken the liberty to download and adjust your preliminary function code
> for persp() for my purposes, i.e. simply replacing your ts-matrix with my
> own doesn't work. Unfortunately (due to my limited expertise of R) I haven't
> been able to get the lables for the y- and x-axis (you use trans3d, etc.)
> replaced with my own "rownames" and "colnames". For now, I will simply keep
> the x-axis as 1:nrows and y-axis as 1:ncols, i.e. the default persp
> settings.
> 
> Request/suggestion: don't know whether it would be possible for you in your
> final code to simply allow a user to feed a zoo/ts matrix as main input,
> perhaps by specifying the date-format, and for your function then to
> automatically create the axes labels.
> 
> Still, your preliminary code has enabled me more flexibility re inputs for
> the persp function and will leave it for now as is. 
> 
> Paul,
> 
> Sorry, but I haven't had time yet to play around with the tframe package.

Well, it could be that I am missing something important in what you are 
trying to do, but you might want to look at it (tfpersp in tframePlus). 
It certainly allows you to pass your own labels, and most of the other 
arguments to persp. It just tries to make default guesses that work for 
time series, and provide windowing.

Paul
> 
> Thx again to both.
> 
> PS
> 
> 
> R at Nabble wrote:
> 
>>Jeff and Paul,
>>
>>Thanks for the suggestions. Jeff's png seems to reflect what I need, will
>>investigate/apply, and report back.
>>
>>PS
>>
>>Paul Gilbert wrote:
>>
>>>In the tframePlus package there is a function called tfpersp() that does 
>>>this.  I'm sure it can be improved, I've never been really happy with 
>>>it. But you might want to compare, to be sure you are improving it.
>>>
>>>Paul
>>>
>>>Jeff Ryan wrote:
>>>
>>>>PS,
>>>>
>>>>I have been working on adding a chartSeries3d function to quantmod.
>>>>The code isn't in place yet, but it relies on persp, and some extra
>>>>manipulation to offer much better axis labeling.  It is designed for
>>>>xts/zoo structures in particular.  The final code will work for any
>>>>time-series.
>>>>
>>>>This should be in the next release of quantmod, but I've posted the
>>>>alpha code and a .png here:
>>>>
>>>>http://www.quantmod.com/example/chartSeries3d
>>>>
>>>>HTH,
>>>>Jeff
>>>>
>>>>
>>>>R/Finance 2009: Applied Finance with R
>>>>April 24, 25 2009 Chicago, IL USA
>>>>http://www.RinFinance.com
>>>>
>>>>
>>>>On Thu, Feb 26, 2009 at 8:20 AM, R at Nabble <vlanschot at yahoo.com> wrote:
>>>>
>>>>
>>>>>Thank you Enrico. Your example helped me to solve a (embarrasingly
> 
> basic)
> 
>>>>>error in my code.
>>>>>
>>>>>Plots fine now.
>>>>>
>>>>>Thx again.
>>>>>
>>>>>PS
>>>>>
>>>>>Enrico Schumann wrote:
>>>>>
>>>>>
>>>>>>it should be helpful to give a little code example, or at least the
>>>>>>content
>>>>>>of the error messages that you received.
>>>>>>
>>>>>>the following gives me a perspective plot...
>>>>>>
>>>>>>require(zoo)
>>>>>>nC     <- 10  # columns
>>>>>>nO     <- 100 # observations
>>>>>>dataM  <- array(runif(nC * nO), dim=c(nO, nC))
>>>>>>zz     <- zoo(dataM, 1:nO)
>>>>>>persp(1:nO,1:nC,zz)
>>>>>>
>>>>>>
>>>>>>regards
>>>>>>enrico
>>>>>>-----Urspr?ngliche Nachricht-----
>>>>>>Von: r-sig-finance-bounces at stat.math.ethz.ch
>>>>>>[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von
> 
> R at Nabble
> 
>>>>>>Gesendet: Donnerstag, 26. Februar 2009 12:26
>>>>>>An: r-sig-finance at stat.math.ethz.ch
>>>>>>Betreff: [R-SIG-Finance] [R-sig-finance] Plot TS-matrix as a surface
>>>>>>
>>>>>>
>>>>>>I'd like to be able to plot a time-series matrix (i.e. first col
> 
> contains
> 
>>>>>>sorted dates, rest of cols contains non-sorted data, with headings at
> 
> the
> 
>>>>>>top; current format is as a zoo object) as a surface/3D chart. The
>>>>>>function
>>>>>>persp() is giving me errors, even if I transform data as core data,
> 
> etc.
> 
>>>>>>Any suggestions / other functions available?
>>>>>>
>>>>>>Thx,
>>>>>>
>>>>>>PS
>>>>>>--
>>>>>>View this message in context:
>>>>>>http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22222236.html
>>>>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>>>
>>>>>>_______________________________________________
>>>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>>-- Subscriber-posting only.
>>>>>>-- If you want to post, subscribe first.
>>>>>>No virus found in this incoming message.
>>>>>>Checked by AVG - www.avg.com
>>>>>>
>>>>>>
>>>>>>06:40:00
>>>>>>
>>>>>>_______________________________________________
>>>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>>-- Subscriber-posting only.
>>>>>>-- If you want to post, subscribe first.
>>>>>>
>>>>>>
>>>>>
>>>>>--
>>>>>View this message in context:
> 
> http://www.nabble.com/Plot-TS-matrix-as-a-surface-tp22222236p22225107.html
> 
>>>>>Sent from the Rmetrics mailing list archive at Nabble.com.
>>>>>
>>>>>_______________________________________________
>>>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>>>-- Subscriber-posting only.
>>>>>-- If you want to post, subscribe first.
>>>>
>>>>
>>>>
>>>>
>>>====================================================================================
>>>
>>>La version fran?aise suit le texte anglais.
>>>
>>>------------------------------------------------------------------------------------
>>>
>>>This email may contain privileged and/or confidential in...{{dropped:26}}
>>>
>>>_______________________________________________
>>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>>-- Subscriber-posting only.
>>>-- If you want to post, subscribe first.
>>>
>>>
>>
>>
> 
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential information, and the Bank of
Canada does not waive any related rights. Any distribution, use, or copying of this
email or the information it contains by other than the intended recipient is
unauthorized. If you received this email in error please delete it immediately from
your system and notify the sender promptly by email that you have done so. 

------------------------------------------------------------------------------------

Le pr?sent courriel peut contenir de l'information privil?gi?e ou confidentielle.
La Banque du Canada ne renonce pas aux droits qui s'y rapportent. Toute diffusion,
utilisation ou copie de ce courriel ou des renseignements qu'il contient par une
personne autre que le ou les destinataires d?sign?s est interdite. Si vous recevez
ce courriel par erreur, veuillez le supprimer imm?diatement et envoyer sans d?lai ?
l'exp?diteur un message ?lectronique pour l'aviser que vous avez ?limin? de votre
ordinateur toute copie du courriel re?u.

From BChiquoine at tiff.org  Thu Mar  5 00:25:49 2009
From: BChiquoine at tiff.org (Chiquoine, Ben)
Date: Wed, 4 Mar 2009 18:25:49 -0500
Subject: [R-SIG-Finance] Full-Scale Optimization
Message-ID: <E71E6D5B2274B341B26B6DF3D34D802879374A@vsw3exch2.tiff.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090304/e0d7bd50/attachment.pl>

From CVorlow at eurobank.gr  Thu Mar  5 16:00:07 2009
From: CVorlow at eurobank.gr (Vorlow Constantinos)
Date: Thu, 5 Mar 2009 17:00:07 +0200
Subject: [R-SIG-Finance] Locating peaks in zoo objects in one go
Message-ID: <BCEA70B53E1BE64290556580EA0708D6AA410D@EH002EXC.eurobank.efg.gr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090305/aeacd677/attachment.pl>

From josh.m.ulrich at gmail.com  Thu Mar  5 16:25:42 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Thu, 5 Mar 2009 09:25:42 -0600
Subject: [R-SIG-Finance] Locating peaks in zoo objects in one go
In-Reply-To: <BCEA70B53E1BE64290556580EA0708D6AA410D@EH002EXC.eurobank.efg.gr>
References: <BCEA70B53E1BE64290556580EA0708D6AA410D@EH002EXC.eurobank.efg.gr>
Message-ID: <8cca69990903050725x72b8a5b8qdf688097e3addd78@mail.gmail.com>

Try this:

> require(xts)
> vix <- as.xts(VIX)
> vix[vix > 70]
           Close
2008-10-17 70.33
2008-10-24 79.13
2008-10-27 80.06
2008-11-19 74.26
2008-11-20 80.86
2008-11-21 72.67
> index(vix[vix > 70])
[1] "2008-10-17" "2008-10-24" "2008-10-27"
[4] "2008-11-19" "2008-11-20" "2008-11-21"

HTH,
Josh
--
http://quantemplation.blogspot.com
http://www.fosstrading.com


On Thu, Mar 5, 2009 at 9:00 AM, Vorlow Constantinos <CVorlow at eurobank.gr> wrote:
> Dear R users,
>
> The following code will download the VIX volatility index to a variable
> called "VIX" (what else)...
>
> library(tseries)
> VIX <- get.hist.quote("^VIX", start = "1990-01-01", quote = "Close")
>
> Is there a way I could determine (in one go or iteratively), where the
> peaks (local maxima) lie in the sequence
> (say by using an arbitrary rule i.e., locate prices which are greater
> than x% the sample's average or a certain value) ???
>
> I would like to be able to capture the prices as well as their
> time-stamps....
>
> Thanks in advance &
> Best regards,
>
> Costas Vorlow
>
>
>
> P Think before you print.
>
> Disclaimer:
> This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
> EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
> EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Thu Mar  5 17:37:59 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 5 Mar 2009 11:37:59 -0500
Subject: [R-SIG-Finance] Locating peaks in zoo objects in one go
In-Reply-To: <BCEA70B53E1BE64290556580EA0708D6AA410D@EH002EXC.eurobank.efg.gr>
References: <BCEA70B53E1BE64290556580EA0708D6AA410D@EH002EXC.eurobank.efg.gr>
Message-ID: <971536df0903050837q4cafea5cm8df159bc60c3977e@mail.gmail.com>

This locates peaks over plus or minus (k+1)/2
days (k odd):

# VIX from post
library(zoo)
k <- 201
idx <- rollapply(VIX, k, function(x) which.max(x) == (k+1)/2)
plot(VIX)
abline(v = time(idx)[idx])


On Thu, Mar 5, 2009 at 10:00 AM, Vorlow Constantinos
<CVorlow at eurobank.gr> wrote:
> Dear R users,
>
> The following code will download the VIX volatility index to a variable
> called "VIX" (what else)...
>
> library(tseries)
> VIX <- get.hist.quote("^VIX", start = "1990-01-01", quote = "Close")
>
> Is there a way I could determine (in one go or iteratively), where the
> peaks (local maxima) lie in the sequence
> (say by using an arbitrary rule i.e., locate prices which are greater
> than x% the sample's average or a certain value) ???
>
> I would like to be able to capture the prices as well as their
> time-stamps....
>
> Thanks in advance &
> Best regards,
>
> Costas Vorlow
>
>
>
> P Think before you print.
>
> Disclaimer:
> This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
> EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
> EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Thu Mar  5 17:38:41 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 5 Mar 2009 11:38:41 -0500
Subject: [R-SIG-Finance] Locating peaks in zoo objects in one go
In-Reply-To: <BCEA70B53E1BE64290556580EA0708D6AA410D@EH002EXC.eurobank.efg.gr>
References: <AcmdoxKvwTg7tfggT2+YYZpvx752ng==>
	<BCEA70B53E1BE64290556580EA0708D6AA410D@EH002EXC.eurobank.efg.gr>
Message-ID: <971536df0903050838r1cc1d436sb10b9151c6eac25a@mail.gmail.com>

This locates peaks over plus or minus (k+1)/2
days (k odd):

# VIX from post
library(zoo)
k <- 201
idx <- rollapply(VIX, k, function(x) which.max(x) == (k+1)/2)
plot(VIX)
abline(v = time(idx)[idx])


On Thu, Mar 5, 2009 at 10:00 AM, Vorlow Constantinos
<CVorlow at eurobank.gr> wrote:
> Dear R users,
>
> The following code will download the VIX volatility index to a variable
> called "VIX" (what else)...
>
> library(tseries)
> VIX <- get.hist.quote("^VIX", start = "1990-01-01", quote = "Close")
>
> Is there a way I could determine (in one go or iteratively), where the
> peaks (local maxima) lie in the sequence
> (say by using an arbitrary rule i.e., locate prices which are greater
> than x% the sample's average or a certain value) ???
>
> I would like to be able to capture the prices as well as their
> time-stamps....
>
> Thanks in advance &
> Best regards,
>
> Costas Vorlow
>
>
>
> P Think before you print.
>
> Disclaimer:
> This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
> EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
> EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From john.hawver at gmail.com  Thu Mar  5 22:39:46 2009
From: john.hawver at gmail.com (John Hawver)
Date: Thu, 5 Mar 2009 16:39:46 -0500
Subject: [R-SIG-Finance] R to ROOT binary data structures
Message-ID: <d6f602790903051339l732896cat2f8f1919c33164f7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090305/2aec2083/attachment.pl>

From bogaso.christofer at gmail.com  Fri Mar  6 08:45:04 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Thu, 5 Mar 2009 23:45:04 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Commodity swap?
Message-ID: <22367589.post@talk.nabble.com>


Hi, currently I am working on commodity swap. Can anyone provide me some good
references on Risk management for commodity sawp i.e. how to calculate Value
at Risk for a portfolio having atleast one position in commodity swap,
decomposition of risk etc under various methodologies like parametric,
simulation etc? Any good book and/or online references will be highly
appreciated. Is there any implementation on R itself?

Also, if possible, information on where to get data on commodity swap over
net.

Thanks and regards,

-- 
View this message in context: http://www.nabble.com/Commodity-swap--tp22367589p22367589.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From john.b.gavin at gmail.com  Fri Mar  6 13:50:12 2009
From: john.b.gavin at gmail.com (John Gavin)
Date: Fri, 6 Mar 2009 12:50:12 +0000 (UTC)
Subject: [R-SIG-Finance] R to ROOT binary data structures
References: <d6f602790903051339l732896cat2f8f1919c33164f7@mail.gmail.com>
Message-ID: <loom.20090306T124824-372@post.gmane.org>

John Hawver <john.hawver <at> gmail.com> writes:

> Are there any packages that allow R to reach into a ROOT binary data
> structure and extract financial time series data?

Try

https://plone4.fnal.gov:4430/P0/phystat/packages/0703001

for linux only.
If anyone knows of a binary (windows) version of this package 
please let me know.

Regards,

John.


From weihanliu2002 at yahoo.com  Fri Mar  6 16:38:39 2009
From: weihanliu2002 at yahoo.com (Wei-han Liu)
Date: Fri, 6 Mar 2009 07:38:39 -0800 (PST)
Subject: [R-SIG-Finance] multivariate integration and partial differentiation
Message-ID: <540823.4507.qm@web53501.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090306/00b3de9f/attachment.pl>

From CVorlow at eurobank.gr  Fri Mar  6 17:16:36 2009
From: CVorlow at eurobank.gr (Vorlow Constantinos)
Date: Fri, 6 Mar 2009 18:16:36 +0200
Subject: [R-SIG-Finance] Locating peaks in zoo objects in one go
In-Reply-To: <971536df0903050838r1cc1d436sb10b9151c6eac25a@mail.gmail.com>
References: <AcmdoxKvwTg7tfggT2+YYZpvx752ng==>
	<BCEA70B53E1BE64290556580EA0708D6AA410D@EH002EXC.eurobank.efg.gr>
	<971536df0903050838r1cc1d436sb10b9151c6eac25a@mail.gmail.com>
Message-ID: <BCEA70B53E1BE64290556580EA0708D6AA448E@EH002EXC.eurobank.efg.gr>

 

Dear Gabor,

Thanks for your prompt answer. One question though:

It seems to be missing the VIX peaks at the beginning and the very end of the history of the sequence.

What am I missing? I am trying to understand it from the help pages but...
 
Costas Vorlow 
 

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Thursday, March 05, 2009 6:39 PM
To: Vorlow Constantinos; costas at vorlow.org
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Locating peaks in zoo objects in one go

This locates peaks over plus or minus (k+1)/2 days (k odd):

# VIX from post
library(zoo)
k <- 201
idx <- rollapply(VIX, k, function(x) which.max(x) == (k+1)/2)
plot(VIX)
abline(v = time(idx)[idx])


On Thu, Mar 5, 2009 at 10:00 AM, Vorlow Constantinos <CVorlow at eurobank.gr> wrote:
> Dear R users,
>
> The following code will download the VIX volatility index to a 
> variable called "VIX" (what else)...
>
> library(tseries)
> VIX <- get.hist.quote("^VIX", start = "1990-01-01", quote = "Close")
>
> Is there a way I could determine (in one go or iteratively), where the 
> peaks (local maxima) lie in the sequence (say by using an arbitrary 
> rule i.e., locate prices which are greater than x% the sample's 
> average or a certain value) ???
>
> I would like to be able to capture the prices as well as their 
> time-stamps....
>
> Thanks in advance &
> Best regards,
>
> Costas Vorlow
>
>
>
> P Think before you print.
>
> Disclaimer:
> This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
> EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
> EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Fri Mar  6 17:37:08 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 6 Mar 2009 11:37:08 -0500
Subject: [R-SIG-Finance] Locating peaks in zoo objects in one go
In-Reply-To: <BCEA70B53E1BE64290556580EA0708D6AA448E@EH002EXC.eurobank.efg.gr>
References: <BCEA70B53E1BE64290556580EA0708D6AA410D@EH002EXC.eurobank.efg.gr>
	<971536df0903050838r1cc1d436sb10b9151c6eac25a@mail.gmail.com>
	<BCEA70B53E1BE64290556580EA0708D6AA448E@EH002EXC.eurobank.efg.gr>
Message-ID: <971536df0903060837sacf5d4dj14ece9731776ba18@mail.gmail.com>

It will only locate peaks in the middle of a stretch of k values so near
the ends you don't have (k+1)/2 values on either side.

2009/3/6 Vorlow Constantinos <CVorlow at eurobank.gr>:
>
>
> Dear Gabor,
>
> Thanks for your prompt answer. One question though:
>
> It seems to be missing the VIX peaks at the beginning and the very end of the history of the sequence.
>
> What am I missing? I am trying to understand it from the help pages but...
>
> Costas Vorlow
>
>
> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Thursday, March 05, 2009 6:39 PM
> To: Vorlow Constantinos; costas at vorlow.org
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Locating peaks in zoo objects in one go
>
> This locates peaks over plus or minus (k+1)/2 days (k odd):
>
> # VIX from post
> library(zoo)
> k <- 201
> idx <- rollapply(VIX, k, function(x) which.max(x) == (k+1)/2)
> plot(VIX)
> abline(v = time(idx)[idx])
>
>
> On Thu, Mar 5, 2009 at 10:00 AM, Vorlow Constantinos <CVorlow at eurobank.gr> wrote:
>> Dear R users,
>>
>> The following code will download the VIX volatility index to a
>> variable called "VIX" (what else)...
>>
>> library(tseries)
>> VIX <- get.hist.quote("^VIX", start = "1990-01-01", quote = "Close")
>>
>> Is there a way I could determine (in one go or iteratively), where the
>> peaks (local maxima) lie in the sequence (say by using an arbitrary
>> rule i.e., locate prices which are greater than x% the sample's
>> average or a certain value) ???
>>
>> I would like to be able to capture the prices as well as their
>> time-stamps....
>>
>> Thanks in advance &
>> Best regards,
>>
>> Costas Vorlow
>>
>>
>>
>> P Think before you print.
>>
>> Disclaimer:
>> This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
>> EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
>> EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.
>>
>>
>> ? ? ? ?[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>


From john.b.gavin at gmail.com  Sat Mar  7 13:08:27 2009
From: john.b.gavin at gmail.com (John Gavin)
Date: Sat, 7 Mar 2009 12:08:27 +0000 (UTC)
Subject: [R-SIG-Finance] Locating peaks in zoo objects in one go
References: <BCEA70B53E1BE64290556580EA0708D6AA410D@EH002EXC.eurobank.efg.gr>
Message-ID: <loom.20090307T120335-766@post.gmane.org>

Vorlow Constantinos <CVorlow <at> eurobank.gr> writes:

> Is there a way I could determine (in one go or iteratively), where the
> peaks (local maxima) lie in the sequence 
> (say by using an arbitrary rule i.e., locate prices which are greater
> than x% the sample's average or a certain value) ???

Perhaps this peaks function will give you some ideas
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/33097.html
and another one
http://ur.ly/8Ii

Regards,

John.


From rusers.sh at gmail.com  Sun Mar  8 02:17:02 2009
From: rusers.sh at gmail.com (zhijie zhang)
Date: Sun, 8 Mar 2009 09:17:02 +0800
Subject: [R-SIG-Finance] How to solve it? Y is not only related with the
	same year's X, but 	also the previsous few years' X and Y
Message-ID: <a835c81e0903071717m48bb77c2ufc4a7aa5b68b1865@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090308/8c0d3d7b/attachment.pl>

From ggrothendieck at gmail.com  Sun Mar  8 02:58:48 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 7 Mar 2009 20:58:48 -0500
Subject: [R-SIG-Finance] How to solve it? Y is not only related with the
	same year's X, but also the previsous few years' X and Y
In-Reply-To: <a835c81e0903071717m48bb77c2ufc4a7aa5b68b1865@mail.gmail.com>
References: <a835c81e0903071717m48bb77c2ufc4a7aa5b68b1865@mail.gmail.com>
Message-ID: <971536df0903071758q254d4dd5had9e964e09d03bd6@mail.gmail.com>

See VAR in the vars package.

On Sat, Mar 7, 2009 at 8:17 PM, zhijie zhang <rusers.sh at gmail.com> wrote:
> Dear all,
> ?Does R have function or method to finish the following task.
> Say i have a dataset. The response variable is Y, and the indepedent
> variables are X1, X2, X3, and YEAR. See an example.
> Y ? ? ? ? ? ? ?X1 ? ? ? ? ? ?X2 ? ? ? ? ? ?X3 ? ? ? ? ?X4 ? ? ? ? ? ?YEAR
> 13.4 ? ? ? ? 2.8 ? ? ? ? ? 3.5 ? ? ? ? ? ? ?2.5 ? ? ?1.8 ? ? ? ? ?1990
> 10.5 ? ? ? ?1.8 ? ? ? ? ? ?2.4 ? ? ? ? ? ? ?2 ? ? ? ? ?3 ? ? ? ? ? 1991
> ? ? ? ? ? ?.... ? ? ? ? ?.....
> ?In ecology, Y may be not only related with X1, X2, X3 in the same year as
> Y, but also may be related with X1, X2, X3 and Y in the previous one , two
> or more years as Y. But which year has the closest relationship is not know
> and this may be one of the analysis aim.
> Take Year=1995 as an example,
> ?Y_1995 may be related with
> ? ? ?X1_1995 , X2_1995 , X3_1995,
> ? ? ?X1_1994 , X2_1994 , X3_1994, ?Y_1994, ? (previous one year)
> ? ? X1_1993 , X2_1993 , X3_1993, Y_1993 ? ? ?(previous two years)
> ? ? ? ? ? ? ? ?... ? ? ? ? ... ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?(previous
> more years)
> ?So the possible model seems like Y_1995= (X1_1995 + X2_1995 + X3_1995)
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?+(X1_1994 +
> X2_1994 + X3_1994 + Y_1994)
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?+(X1_1993 +
> X2_1993 + X3_1993 + Y_1993)
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?+...
> ?Pay attention to the situation that Y may be not only related with the
> same year's X, ?but also the previsous few years' X and Y. I had checked out
> the methods of time series (TS), but i did not find the right method. I
> think this should be related with TS, but i am not sure if it cannot solve
> it. TS may only considers the autocorrelation of Y in different time, but
> not Xs. Besides Y, i also have several independent variables that have
> effects on Y in the next few years .
> ? Anybody knows whether R have functions or methods to finish the above
> task.
> ?I hope i have explained it clearly. Any suggestions or help are greatly
> appreciated.
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From vfulco1 at gmail.com  Sun Mar  8 05:41:30 2009
From: vfulco1 at gmail.com (Vince Fulco)
Date: Sat, 7 Mar 2009 23:41:30 -0500
Subject: [R-SIG-Finance] zoo library and unique error...
Message-ID: <34f2770f0903072041x6b8ccaf0h38a059de9239abf6@mail.gmail.com>

I'm puzzled by a recurring warning re: "some methods for 'zoo' objects
do not work if the index entries in ?order.by? are not unique."

The data is machine generated in specific, unique, ordered increments
and I've tried to manipulate this zoo object as well as running the
index(foo) through a  make.unique function but the warning continues
to surface. It then causes calculations in the script further down to
balk.  Prior Q&A re: this error indicate 1) the index or the 2) file
using read.zoo, must have a make.unique func() applied to it. But this
application doesn't alter the warnings or provides mean values which I
don't want.

rm(list=ls());
#

require(zoo)
require(quantmod)

es.raw<- read.table('C://IBrtdata/ESdat030609.dat', as.is=T, sep=',')
names(es.raw)<- c('dtetme', 'O', 'H', 'L', 'C', 'VOL', 'WAP', 'GAPS', 'CNT')

es.raw2<- strptime(es.raw[,1], '%Y-%m-%d %H:%M:%S')
es.raw2<- unique(es.raw2)

es.raw3<- zoo(es.raw[,-c(1)],es.raw2)
#
Warning message:
In zoo(es.raw[, -c(1)], es.raw2) :
  some methods for ?zoo? objects do not work if the index entries in
?order.by? are not unique
-- 

TIA,

Vince Fulco, CFA, CAIA
612.424.5477 (universal)
vfulco1 at gmail.com


From elise at predictionimpact.com  Sun Mar  8 08:10:10 2009
From: elise at predictionimpact.com (Elise Johnson)
Date: Sat, 7 Mar 2009 23:10:10 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Predictive Analytics Seminar: San
 Jose, NYC, Toronto, more
Message-ID: <22395500.post@talk.nabble.com>


Hi all, 

I wanted to let you know about our training seminar on predictive analytics
- coming April, May, Oct, and Nov in San Jose, NYC, Stockholm, Toronto and
other cities.  This is intensive training for marketers, managers and
business professionals to make actionable sense of customer data by
predicting buying behavior, churn, etc.  Past attendees provided rave
reviews.

Here's more info:
----------------------

Event: Predictive Analytics for Business, Marketing and Web
Dates: April 2-3, May 3-4, May 27-28, Oct 14-15, Oct 18-19, and Nov 11-12,
2009
Locations: Toronto (April), San Jose (May), NYC (May), Stockholm (Oct), DC
(Oct), San Francisco (Nov)

A two-day intensive seminar brought to you by Prediction Impact, Inc.

93% rate this program Excellent or Very Good.

**The official training program of Predictive Analytics World**
**Offered in conjunction with eMetrics events**

(Also see our Online Training: Predictive Analytics Applied - immediate
access at any time
www.predictionimpact.com/predictive-analytics-online-training.html)

-----------------------------------------------------------------------------------------------
ABOUT THIS SEMINAR:
Business metrics do a great job summarizing the past. But if you want to
predict how customers will respond in the future, there is one place to
turn--predictive analytics. By learning from your abundant historical data,
predictive analytics provides the marketer something beyond standard
business reports and sales forecasts: actionable predictions for each
customer. These predictions encompass all channels, both online and off,
foreseeing which customers will buy, click, respond, convert or cancel. If
you predict it, you own it.

The customer predictions generated by predictive analytics deliver more
relevant content to each customer, improving response rates, click rates,
buying behavior, retention and overall profit. For online applications such
as e-marketing and customer care recommendations, predictive analytics acts
in real-time, dynamically selecting the ad, web content or cross-sell
product each visitor is most likely to click on or respond to, according to
that visitor's profile. This is AB selection, rather than just AB testing.

Predictive Analytics for Business, Marketing and Web is a concentrated
training program that includes interactive breakout sessions and a brief
hands-on exercise. In two days we cover:

? The techniques, tips and pointers you need in order to run a successful
predictive analytics and data mining initiative
? How to strategically position and tactically deploy predictive analytics
and data mining at your company
? How to bridge the prevalent gap between technical understanding and
practical use
? How a predictive model works, how it's created and how much revenue it
generates
? Several detailed case studies that demonstrate predictive analytics in
action and make the concepts concrete

? NEW TOPIC: Five Ways to Lower Costs with Predictive Analytics

No background in statistics or modeling is required. The only specific
knowledge assumed for this training program is moderate experience with
Microsoft Excel or equivalent.

For more information, visit
www.predictionimpact.com/predictive-analytics-training.html, or e-mail us at
training at predictionimpact.com.  You may also call (415) 683-1146.

Cross-Registration Special: Attendees earn $250 off the Predictive Analytics
World Conference

SNEAK PREVIEW VIDEO:
www.predictionimpact.com/predictive-analytics-times.html

$100 off early registration, 3 weeks ahead

-- 
View this message in context: http://www.nabble.com/Predictive-Analytics-Seminar%3A-San-Jose%2C-NYC%2C-Toronto%2C-more-tp22395500p22395500.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Sun Mar  8 11:23:07 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 8 Mar 2009 05:23:07 -0500
Subject: [R-SIG-Finance] zoo library and unique error...
In-Reply-To: <34f2770f0903072041x6b8ccaf0h38a059de9239abf6@mail.gmail.com>
References: <34f2770f0903072041x6b8ccaf0h38a059de9239abf6@mail.gmail.com>
Message-ID: <971536df0903080323j236344baxcffd575bfb9402f2@mail.gmail.com>

That won't make the times unique -- it will only ensure that
of the es2.raw rows with identical times that no two have the same
value in all other columns.

See zoo faq #1:

library(zoo)
vignette("zoo-faq")

On Sat, Mar 7, 2009 at 11:41 PM, Vince Fulco <vfulco1 at gmail.com> wrote:
> I'm puzzled by a recurring warning re: "some methods for 'zoo' objects
> do not work if the index entries in ?order.by? are not unique."
>
> The data is machine generated in specific, unique, ordered increments
> and I've tried to manipulate this zoo object as well as running the
> index(foo) through a ?make.unique function but the warning continues
> to surface. It then causes calculations in the script further down to
> balk. ?Prior Q&A re: this error indicate 1) the index or the 2) file
> using read.zoo, must have a make.unique func() applied to it. But this
> application doesn't alter the warnings or provides mean values which I
> don't want.
>
> rm(list=ls());
> #
>
> require(zoo)
> require(quantmod)
>
> es.raw<- read.table('C://IBrtdata/ESdat030609.dat', as.is=T, sep=',')
> names(es.raw)<- c('dtetme', 'O', 'H', 'L', 'C', 'VOL', 'WAP', 'GAPS', 'CNT')
>
> es.raw2<- strptime(es.raw[,1], '%Y-%m-%d %H:%M:%S')
> es.raw2<- unique(es.raw2)
>
> es.raw3<- zoo(es.raw[,-c(1)],es.raw2)
> #
> Warning message:
> In zoo(es.raw[, -c(1)], es.raw2) :
> ?some methods for ?zoo? objects do not work if the index entries in
> ?order.by? are not unique
> --
>
> TIA,
>
> Vince Fulco, CFA, CAIA
> 612.424.5477 (universal)
> vfulco1 at gmail.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From rechtsteiner at bgki.net  Mon Mar  9 19:47:40 2009
From: rechtsteiner at bgki.net (Josuah Rechtsteiner)
Date: Mon, 9 Mar 2009 19:47:40 +0100
Subject: [R-SIG-Finance] regression problem
Message-ID: <26BA4E97-112F-461D-9728-ADE02FC14B25@bgki.net>

dear useRs,

i'm working with a mean reverting model of the following specification:

y = mu + beta(x - mu) + errorterm, where mu is a constant

currently I estimate just y = x (with lm()) to get beta and then  
calculate mu = estimated intercept / (1-beta).

but I'd like to estimate mu and beta together in one regression-step  
and also get the test-statistics (including parameter variance) for mu  
as well as for beta in the summary of the regression.

could you please help me?

thanks very much in advance!

josuah


From kriskumar at earthlink.net  Tue Mar 10 00:35:11 2009
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Mon, 09 Mar 2009 19:35:11 -0400
Subject: [R-SIG-Finance] multivariate integration and partial
	differentiation
In-Reply-To: <540823.4507.qm@web53501.mail.re2.yahoo.com>
References: <540823.4507.qm@web53501.mail.re2.yahoo.com>
Message-ID: <49B5A7AF.6090608@earthlink.net>

Wei-han Liu wrote:
> Hi R Users:
>
> Could somebody share some tips on implementing multivariate integration and partial differentiation in R? 
> For example, for a trivariate joint distribution (cumulative density function) of F(x,y,z), how to differentiate with respect to x and get the bivariate distribution (probability density function) of f(y,z). Or integrate f(x,y,z) with respect to x to get bivariate distribution of (y,z).
> Your sharing is appreciated.
> Wei-han Liu
>
>
>       
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>   
Hi there are several multivariate integration possibilities in R besides 
Pseudo-MC/Quasi-MC, R package adapt that does adaptive quadrature
based on fortran code from Alan Genz. There is also a package that does 
sparse grids (what is also goes as Smolyak Construction) but I can't 
seem to locate it quickly.

In order to compute the cumulative distribution have a look at package 
mvtnorm, also it is possible to interface to mulnor directly from R 
using Rcpp or simillar.

Best
Krishna


From bogaso.christofer at gmail.com  Tue Mar 10 10:10:05 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Tue, 10 Mar 2009 02:10:05 -0700 (PDT)
Subject: [R-SIG-Finance] re[R-sig-finance] gression problem
In-Reply-To: <26BA4E97-112F-461D-9728-ADE02FC14B25@bgki.net>
References: <26BA4E97-112F-461D-9728-ADE02FC14B25@bgki.net>
Message-ID: <22430366.post@talk.nabble.com>


You might try MLE, construct the liklihood function and then optimize it by
experimenting different choices of parameters. I have doubt how LS
estimation procedure can be employed here as parameters are nonlinear in
nature


rechtsteiner wrote:
> 
> dear useRs,
> 
> i'm working with a mean reverting model of the following specification:
> 
> y = mu + beta(x - mu) + errorterm, where mu is a constant
> 
> currently I estimate just y = x (with lm()) to get beta and then  
> calculate mu = estimated intercept / (1-beta).
> 
> but I'd like to estimate mu and beta together in one regression-step  
> and also get the test-statistics (including parameter variance) for mu  
> as well as for beta in the summary of the regression.
> 
> could you please help me?
> 
> thanks very much in advance!
> 
> josuah
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/regression-problem-tp22419670p22430366.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From irafuchs at gmail.com  Tue Mar 10 14:34:36 2009
From: irafuchs at gmail.com (Fuchs Ira)
Date: Tue, 10 Mar 2009 09:34:36 -0400
Subject: [R-SIG-Finance] Quantmod: getFinancials error
In-Reply-To: <e8e755250902100713k6631f55dqd5d8893bbe43308@mail.gmail.com>
References: <7A7E72D8-FEFC-482C-BD96-D0CC934D43F4@gmail.com>
	<e8e755250902100713k6631f55dqd5d8893bbe43308@mail.gmail.com>
Message-ID: <C8C8E9B1-A772-4FB0-B147-9EE84E1D869C@gmail.com>

Jeff,

I tried using getFinancials and got:

 > getFin('AAPL')
Error in colnamesISCF[[2]] : subscript out of bounds

Perhaps Google changed something?

Thanks,
Ira


From jeff.a.ryan at gmail.com  Tue Mar 10 16:00:02 2009
From: jeff.a.ryan at gmail.com (J Ryan)
Date: Tue, 10 Mar 2009 10:00:02 -0500
Subject: [R-SIG-Finance] Quantmod: getFinancials error
In-Reply-To: <C8C8E9B1-A772-4FB0-B147-9EE84E1D869C@gmail.com>
References: <7A7E72D8-FEFC-482C-BD96-D0CC934D43F4@gmail.com>
	<e8e755250902100713k6631f55dqd5d8893bbe43308@mail.gmail.com>
	<C8C8E9B1-A772-4FB0-B147-9EE84E1D869C@gmail.com>
Message-ID: <50162D4F-81D4-45D3-8E8E-BBA077384C4E@gmail.com>

Hi Ira

Google made a simple formatting change recently. The new version can  
be found on the R-forge repository or quantmod.

I should have a CRAN release soon, pending some possible changes  
happening on R-devel with respect to S3/S4 method dispatch in 2.9.0

I'll also make the most recent tarball available on quantmod.com

Thanks for the report.

Jeff

http://www.RinFinance.com
R/Finance 2009: Applied Finance with R
April 24, 25 Chicago, IL, USA

Jeffrey A. Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com

On Mar 10, 2009, at 8:34 AM, Fuchs Ira <irafuchs at gmail.com> wrote:

> Jeff,
>
> I tried using getFinancials and got:
>
> > getFin('AAPL')
> Error in colnamesISCF[[2]] : subscript out of bounds
>
> Perhaps Google changed something?
>
> Thanks,
> Ira


From john.seppanen99 at gmail.com  Tue Mar 10 21:37:56 2009
From: john.seppanen99 at gmail.com (=?ISO-8859-1?Q?John_Sepp=E4nen?=)
Date: Tue, 10 Mar 2009 22:37:56 +0200
Subject: [R-SIG-Finance] "To post to this list, send your email to"
Message-ID: <f5dfae7c0903101337x48f15195uf17780306f49ae4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090310/9ce73228/attachment.pl>

From jdenhamer60 at hotmail.com  Wed Mar 11 15:55:44 2009
From: jdenhamer60 at hotmail.com (Jasper den Hamer)
Date: Wed, 11 Mar 2009 15:55:44 +0100
Subject: [R-SIG-Finance] Question about fit.st
Message-ID: <BAY141-W1653598110DE357F907DE7BF9E0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090311/3c2468c4/attachment.pl>

From bogaso.christofer at gmail.com  Thu Mar 12 15:02:03 2009
From: bogaso.christofer at gmail.com (Bogaso)
Date: Thu, 12 Mar 2009 07:02:03 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Commodity swap?
In-Reply-To: <22367589.post@talk.nabble.com>
References: <22367589.post@talk.nabble.com>
Message-ID: <22477027.post@talk.nabble.com>


Any suggestion/reference please?



Bogaso wrote:
> 
> Hi, currently I am working on commodity swap. Can anyone provide me some
> good references on Risk management for commodity sawp i.e. how to
> calculate Value at Risk for a portfolio having atleast one position in
> commodity swap, decomposition of risk etc under various methodologies like
> parametric, simulation etc? Any good book and/or online references will be
> highly appreciated. Is there any implementation on R itself?
> 
> Also, if possible, information on where to get data on commodity swap over
> net.
> 
> Thanks and regards,
> 
> 

-- 
View this message in context: http://www.nabble.com/Commodity-swap--tp22367589p22477027.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From rbali at ufmg.br  Thu Mar 12 15:50:13 2009
From: rbali at ufmg.br (Robert Iquiapaza)
Date: Thu, 12 Mar 2009 11:50:13 -0300
Subject: [R-SIG-Finance] [R-sig-finance] Commodity swap?
In-Reply-To: <22477027.post@talk.nabble.com>
References: <22367589.post@talk.nabble.com> <22477027.post@talk.nabble.com>
Message-ID: <EDC8FC938727462DABDB7DC53E8CBC16@HPR>

Maybe this book is useful

Measuring market risk with value at risk
by Pietro Penza, Vipul K. Bansal

Good luck

--------------------------------------------------
From: "Bogaso" <bogaso.christofer at gmail.com>
Sent: Thursday, March 12, 2009 11:02 AM
To: <r-sig-finance at stat.math.ethz.ch>
Subject: Re: [R-SIG-Finance] [R-sig-finance] Commodity swap?

>
> Any suggestion/reference please?
>
>
>
> Bogaso wrote:
>>
>> Hi, currently I am working on commodity swap. Can anyone provide me some
>> good references on Risk management for commodity sawp i.e. how to
>> calculate Value at Risk for a portfolio having atleast one position in
>> commodity swap, decomposition of risk etc under various methodologies 
>> like
>> parametric, simulation etc? Any good book and/or online references will 
>> be
>> highly appreciated. Is there any implementation on R itself?
>>
>> Also, if possible, information on where to get data on commodity swap 
>> over
>> net.
>>
>> Thanks and regards,
>>


From briankim19 at yahoo.com  Thu Mar 12 21:06:10 2009
From: briankim19 at yahoo.com (B Kim)
Date: Thu, 12 Mar 2009 13:06:10 -0700 (PDT)
Subject: [R-SIG-Finance] quantmod custom layouts
Message-ID: <929092.21330.qm@web62504.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090312/4c49f4cc/attachment.pl>

From jeff.a.ryan at gmail.com  Thu Mar 12 21:18:46 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 12 Mar 2009 15:18:46 -0500
Subject: [R-SIG-Finance] quantmod custom layouts
In-Reply-To: <929092.21330.qm@web62504.mail.re1.yahoo.com>
References: <929092.21330.qm@web62504.mail.re1.yahoo.com>
Message-ID: <e8e755250903121318n18f1a1c2y8fdf9e9e2cbadc6e@mail.gmail.com>

Hi Brian,

I need to make this easier, and document it better, but this will get
you started. There are slides from my presentations at Rmetrics 2008
and Columbia on the quantmod website.

(From a previous post on the same topic)

Before calling any chartSeries function (including barChart), use the
R 'layout' function. see ?layout
Each window is technically a plot region, so count the TA _windows_ when
you do this.

getSymbols("SPY;DIA;QQQQ;IWM")
layout(matrix(1:8, nrow=4), height=c(4,2.5,4,2.5))

Then set layout=NULL in each call.
chartSeries(IWM, layout=NULL)
chartSeries(SPY, layout=NULL)
chartSeries(DIA, layout=NULL)
chartSeries(QQQQ, layout=NULL)

You need to have a constant 'theme', or funny things will happen.


Other arrangements are possible, though require you to really get your
head around 'layout'.  I will one day be adding in a chartLayout tool
that will make this easier, but for now it is at least doable.

layout(matrix(c(1,1,1,2,3:6), nrow=4), height=c(4,2.5,4,2.5))
chartSeries(QQQQ, layout=NULL)
chartSeries(QQQQ, layout=NULL)
chartSeries(QQQQ, layout=NULL)

You won't be able to use this interactively once the chart is drawn
--- i.e. calling addTA to dynamically add TA additions.  The layout
needs to be calculated before any of the drawing takes place.  This is
what happens internally when you call addTA/chartSeries,, which is why
setting layout=NULL places the burden on you the user.

HTH,
Jeff

R/Finance 2009: Applied Finance with R
April 24, 25 Chicago, IL USA
http://www.RinFinance.com

[original here:]
http://www.nabble.com/layout--options-of--barChart-in-quantmod-tt20481570.html#a20481570

On Thu, Mar 12, 2009 at 3:06 PM, B Kim <briankim19 at yahoo.com> wrote:
> Hi,
>
> I just started using quantmod and R. ?Can someone tell me how to use the custom layouts? ?For example, I just want to have 2 stock charts with indicators on the same page.
>
> thanks,
> Brian
>
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From briankim19 at yahoo.com  Thu Mar 12 21:40:10 2009
From: briankim19 at yahoo.com (B Kim)
Date: Thu, 12 Mar 2009 13:40:10 -0700 (PDT)
Subject: [R-SIG-Finance] quantmod custom layouts
References: <929092.21330.qm@web62504.mail.re1.yahoo.com>
	<e8e755250903121318n18f1a1c2y8fdf9e9e2cbadc6e@mail.gmail.com>
Message-ID: <67481.84692.qm@web62501.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090312/c3caa972/attachment.pl>

From jeff.a.ryan at gmail.com  Thu Mar 12 21:46:59 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 12 Mar 2009 15:46:59 -0500
Subject: [R-SIG-Finance] quantmod custom layouts
In-Reply-To: <67481.84692.qm@web62501.mail.re1.yahoo.com>
References: <929092.21330.qm@web62504.mail.re1.yahoo.com>
	<e8e755250903121318n18f1a1c2y8fdf9e9e2cbadc6e@mail.gmail.com>
	<67481.84692.qm@web62501.mail.re1.yahoo.com>
Message-ID: <e8e755250903121346w3002855ag1280645616d1aec5@mail.gmail.com>

Hi Brian,

There is a new function in quantmod to save the chart (though not in
the CRAN version yet).  It doesn't do much more than:

dev.copy2pdf(file="AAPL.pdf")

Which will do what you want.  The interactive nature of the
chartSeries logic makes using it on other devices not quite as smooth
as one would like/expect.  But it can be done. (see ?dev.copy)

HTH,
Jeff

On Thu, Mar 12, 2009 at 3:40 PM, B Kim <briankim19 at yahoo.com> wrote:
> Fantastic, thats exactly what I was looking for!
>
> Also, how can I change the colors of the technical indicator lines?? I use
> the "white" theme to save on ink when printing, but the dashed line in the
> addSMI call is very hard to see.
>
> ??? chartSeries(AAPL, layout=NULL, up.col='white', dn.col = 'blue',
> theme="white", TA="addBBands();addSMI();")
> ??? addLines(h=-25, on = 2, col = "red")
> ??? addLines(h=25,? on = 2, col = "red")
>
> Last question, whats the best way to add overbought and oversold lines?
> When I use the above code and write to a pdf, I get pages in the pdf file.
> The first page contains the chart and indicators, the 2nd page contains the
> same but has a line added at -25, and the 3rd page contains lines at -25 and
> 25.
>
> thanks again,
> Brian
>
>
> ________________________________
> From: Jeff Ryan <jeff.a.ryan at gmail.com>
> To: B Kim <brian.a.kim at gmail.com>
> Cc: r-sig-finance at stat.math.ethz.ch
> Sent: Thursday, March 12, 2009 4:18:46 PM
> Subject: Re: [R-SIG-Finance] quantmod custom layouts
>
> Hi Brian,
>
> I need to make this easier, and document it better, but this will get
> you started. There are slides from my presentations at Rmetrics 2008
> and Columbia on the quantmod website.
>
> (From a previous post on the same topic)
>
> Before calling any chartSeries function (including barChart), use the
> R 'layout' function. see ?layout
> Each window is technically a plot region, so count the TA _windows_ when
> you do this.
>
> getSymbols("SPY;DIA;QQQQ;IWM")
> layout(matrix(1:8, nrow=4), height=c(4,2.5,4,2.5))
>
> Then set layout=NULL in each call.
> chartSeries(IWM, layout=NULL)
> chartSeries(SPY, layout=NULL)
> chartSeries(DIA, layout=NULL)
> chartSeries(QQQQ, layout=NULL)
>
> You need to have a constant 'theme', or funny things will happen.
>
>
> Other arrangements are possible, though require you to really get your
> head around 'layout'.? I will one day be adding in a chartLayout tool
> that will make this easier, but for now it is at least doable.
>
> layout(matrix(c(1,1,1,2,3:6), nrow=4), height=c(4,2.5,4,2.5))
> chartSeries(QQQQ, layout=NULL)
> chartSeries(QQQQ, layout=NULL)
> chartSeries(QQQQ, layout=NULL)
>
> You won't be able to use this interactively once the chart is drawn
> --- i.e. calling addTA to dynamically add TA additions.? The layout
> needs to be calculated before any of the drawing takes place.? This is
> what happens internally when you call addTA/chartSeries,, which is why
> setting layout=NULL places the burden on you the user.
>
> HTH,
> Jeff
>
> R/Finance 2009: Applied Finance with R
> April 24, 25 Chicago, IL USA
> http://www.RinFinance.com
>
> [original here:]
> http://www.nabble.com/layout--options-of--barChart-in-quantmod-tt20481570.html#a20481570
>
> On Thu, Mar 12, 2009 at 3:06 PM, B Kim <briankim19 at yahoo.com> wrote:
>> Hi,
>>
>> I just started using quantmod and R. ?Can someone tell me how to use the
>> custom layouts? ?For example, I just want to have 2 stock charts with
>> indicators on the same page.
>>
>> thanks,
>> Brian
>>
>>
>>
>> ? ? ? ?[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From VOSSK at kochind.com  Fri Mar 13 00:11:46 2009
From: VOSSK at kochind.com (Voss, Kent)
Date: Thu, 12 Mar 2009 16:11:46 -0700
Subject: [R-SIG-Finance] Specifying an expected mu and Sigma for
 fPortfolio
In-Reply-To: <13CB99597D0BAA43B07ABFCE29E45F6B0262FB44@phx0mbx01.kochind.com>
References: <13CB99597D0BAA43B07ABFCE29E45F6B0262FB44@phx0mbx01.kochind.com>
Message-ID: <13CB99597D0BAA43B07ABFCE29E45F6B0262FB9C@phx0mbx01.kochind.com>

Any suggestions as to how to do this?  I've tried to be specific and
provide a reproducible example and any help would be greatly
appreciated.

Thanks,

Kent

-----Original Message-----
From: Voss, Kent 
Sent: Tuesday, March 03, 2009 5:06 PM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Specifying an expected mu and Sigma for
fPortfolio

I've read the other post on this topic that suggests specifying a custom
estimator, but I cannot seem to even get that to work as I had expected.

Below is the code to replicate my issue and illustrates what I'm trying
to do.  Essentially I am trying to override the calculation of the mu
and Sigma in the Estimator and replace it with static data.  This should
yield the same optimization results regardless of what data I send to
the estimator (since I'm overriding it), but it does not appear to work
for me that way.  Please see the code below for details.  Any help at
all would be greatly appreciated.  

Thanks,

Kent

#### OBJECTIVE:  Specify an expected return and covariance matrix for
use in the optimization

# Define the data to be used in the example Data <-
as.timeSeries(data(smallcap.ts)) Data <- Data[,1:3]

# Create an expected return and covariance matrix based on the first 12
months # It could be anything, but just to create a static expected
return and cov matrix # In reality the expected returns are not
generated from the historical data, and so # feeding the estimator a
paricular slice of data is not a workable solution.
ExpRet <- apply(head(Data,12), 2, mean)
ExpCov <- cov(head(Data,12))

# Now what I would like to do is to just feed the expected return and
cov # matrix to one of the fPorfolio functions like "portfolioFrontier"

# The only way I can think to do that based on what I've found is to #
specify a custom Estimator, that just disregards what's passed to it #
and sends back the expected return and covariance matrix.
# This seems a silly way to do it, and I'm sure there's a better way,
but # in a pinch, I had hoped this would work.  But it doesn't appear
to.

# Specify the custom estimator
myEstimator <- function(x, Spec=NULL) {

  # Just get the specified return and cov from the environment
  expectedAssetReturns <- get("ExpRet")
  assetCov <- get("ExpCov")

  # Ignore the inputted data, and send back the expected returns
  return(list(mu=expectedAssetReturns,Sigma=assetCov))
}

# Now create a portfolio Spec using the custom estimator defined above.
Spec = portfolioSpec()
setTargetReturn(Spec) = .05
setEstimator(Spec) <- "myEstimator"

# Subset out some of the data and calculate the portfolio frontier.
# I subset out a piece of the data, so that I can change the data set
input # which should have no effect if I'm specifying the mu and cov,
but # for some reason it does, and hence my dilemma.

Data1 <- Data[20:40,]
frontier1 <- portfolioFrontier(Data1,Spec,"LongOnly")

Data2 <- Data[41:60,]
frontier2 <- portfolioFrontier(Data2,Spec,"LongOnly")

# Now I thought that frontier1 would look identical to frontier2 since
I'm overriding # the inputs with the specified mu and Sigma in the
"myEstimator", the
frontier1 and
# frontier2 look different.
op <- par(mfrow = c(2,1),mar = c(3,3,3,3),mgp=c(1.5,0.6,0),oma =
c(1,1,1,1))
frontierPlot(frontier1)
frontierPlot(frontier2)

# I'm clearly doing something wrong, and I'm sure it's some
misunderstanding on my part # but I can't for the life of me figure it
out.  Any help would be greatly appreciated.




"EMF <kochind.com>" made the following annotations.
------------------------------------------------------------------------
------
The information in this e-mail and any attachments is\ c...{{dropped:22}}


From sergeyg at gmail.com  Fri Mar 13 08:44:16 2009
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Fri, 13 Mar 2009 08:44:16 +0100
Subject: [R-SIG-Finance] RBloomberg: loading Futures Tickers: how?
Message-ID: <7cb007bd0903130044icc0e583kb1a7b15ca5d1e4f8@mail.gmail.com>

Hello, everyone

I've been using RBloomberg extensively, but I've only been downloading
price information. I've been using zoo objects for that purpose.
Now, I need to load futures tickers (mnemonic is:
"FUT_CUR_GEN_TICKER"). As it is not numeric, I cannot use zoo,
unfortunately. Also, I decided to separate prices from futures tickers
into two blpGetData runs.
I have discovered that specifying "matrix" or "data.frame" does not
work for loading tickers either. I get an error message: "In
as.matrix.BlpCOMReturn(BLP): NAs are introduced by coersion".
Only retval="raw" works, but then I get a huge list, which I do not
know how to turn into a matrix properly.
Anyone has any suggestions regarding this issue of getting non-numeric
data from Bloomberg?
What I need in the end is to do analysis on both futures price data
and on corresponding tickers data (for futures rolling).

Best,
Sergey

-- 
I'm not young enough to know everything. /Oscar Wilde
Experience is one thing you can't get for nothing. /Oscar Wilde
When you are finished changing, you're finished. /Benjamin Franklin
Tell me and I forget, teach me and I remember, involve me and I learn.
/Benjamin Franklin
Luck is where preparation meets opportunity. /George Patten


From robert at sanctumfi.com  Fri Mar 13 10:46:58 2009
From: robert at sanctumfi.com (Robert Sams)
Date: Fri, 13 Mar 2009 09:46:58 -0000
Subject: [R-SIG-Finance] RBloomberg: loading Futures Tickers: how?
References: <SANCTUMFISERVERaUMT0000155e@sanctumfi.com>
Message-ID: <SANCTUMFISERVERvvsG0000158e@sanctumfi.com>

Hi Sergey,

Is this what you need?

> unlist(blpGetData(conn, c("ED1 COMDTY","GC1 COMDTY","TY1 COMDTY","SP1
COMDTY"),"FUT_CUR_GEN_TICKER", retval="raw"))
[1] "EDH9" "GCJ9" "TYH9" "SPH9"

The use of retval="raw" is a work-around as this is a bug. The bug is
fixed in the beta 0.2 version on r-forge
http://r-forge.r-project.org/projects/rbloomberg/

blp(c("ED1 COMDTY","GC1 COMDTY","TY1 COMDTY","SP1 COMDTY"),
"FUT_CUR_GEN_TICKER")
           FUT_CUR_GEN_TICKER
ED1 COMDTY               EDH9
GC1 COMDTY               GCJ9
TY1 COMDTY               TYH9
SP1 COMDTY               SPH9
 
Cheers, 
Robert

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Sergey
Goriatchev
Sent: 13 March 2009 07:44
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] RBloomberg: loading Futures Tickers: how?

Hello, everyone

I've been using RBloomberg extensively, but I've only been downloading
price information. I've been using zoo objects for that purpose.
Now, I need to load futures tickers (mnemonic is:
"FUT_CUR_GEN_TICKER"). As it is not numeric, I cannot use zoo,
unfortunately. Also, I decided to separate prices from futures tickers
into two blpGetData runs.
I have discovered that specifying "matrix" or "data.frame" does not work
for loading tickers either. I get an error message: "In
as.matrix.BlpCOMReturn(BLP): NAs are introduced by coersion".
Only retval="raw" works, but then I get a huge list, which I do not know
how to turn into a matrix properly.
Anyone has any suggestions regarding this issue of getting non-numeric
data from Bloomberg?
What I need in the end is to do analysis on both futures price data and
on corresponding tickers data (for futures rolling).

Best,
Sergey

--
I'm not young enough to know everything. /Oscar Wilde Experience is one
thing you can't get for nothing. /Oscar Wilde When you are finished
changing, you're finished. /Benjamin Franklin Tell me and I forget,
teach me and I remember, involve me and I learn.
/Benjamin Franklin
Luck is where preparation meets opportunity. /George Patten

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From VOSSK at kochind.com  Fri Mar 13 15:32:57 2009
From: VOSSK at kochind.com (Voss, Kent)
Date: Fri, 13 Mar 2009 07:32:57 -0700
Subject: [R-SIG-Finance] Problem with RBloomberg (not the usual one)
In-Reply-To: <7cb007bd0902270053h73dd8931pb7080c8ac5ac69c2@mail.gmail.com>
References: <7cb007bd0902270016h31633e81sc1bbfd2808de7ba4@mail.gmail.com>
	<7cb007bd0902270053h73dd8931pb7080c8ac5ac69c2@mail.gmail.com>
Message-ID: <13CB99597D0BAA43B07ABFCE29E45F6B0262FBA2@phx0mbx01.kochind.com>

Sergey,

Sorry I missed your post earlier.  I had this same problem and I reinstalled the DDE Server and Excel Add-In, rebooted and it fixed it.  I thought they were already installed as I could pull down data in Excel from Bloomberg, but reinstalling them fixed my R problem.  You can find the downloads at http://about.bloomberg.com/contact_softwaresupport_upgr.html

Good luck.

Kent
 

-----Original Message-----
From: Sergey Goriatchev [mailto:sergeyg at gmail.com] 
Sent: Friday, February 27, 2009 1:54 AM
To: r-sig-finance at stat.math.ethz.ch; r-help at r-project.org
Subject: Re: [R-SIG-Finance] Problem with RBloomberg (not the usual one)

Hello, again, everyone

I went through the code and narrowed down the problem

in blpConnect:
COMCreate("Bloomberg.Data.1") which then calls getCOMInstance does not work, because
getCLSID("Bloomberg.Data.1") returns
"Fehler: Invalid class string"

What is this problem???

Best,
Sergey

On Fri, Feb 27, 2009 at 09:16, Sergey Goriatchev <sergeyg at gmail.com> wrote:
> Hello, everyone!
>
> I have a problem with RBloomberg and this is not the usual "no 
> administrator rights" problem.
>
> I have R 2.7.2, RBloomberg 0.1-10, RDCOMclient 0.92-0
>
> RDCOMClient, chron, zoo, stats: these packages load OK.
>
> Then, trying to connect, I get following error message:
>
>
> ?conn <- blpConnect(show.days="week", na.action="previous.days",
> periodicity="daily")
> Warning messages:
> 1: In getCOMInstance(name, force = TRUE, silent = TRUE) :
> ?Couldn't get clsid from the string
> 2: In blpConnect(show.days = "week", na.action = "previous.days", 
> periodicity = "daily") :
> ?Seems like this is not a Bloomberg Workstation: ?Error : Invalid 
> class string
>
> Anyone encountered this problem?
> What is wrong and how can I solve it?
>
> Online, I found just one instance of this problem discussed, and it 
> was in Chinese:
>
> http://cos.name/bbs/read.php?tid=12821&fpage=3
>
> Thank you for your help!
>
> Sergey
>



--
I'm not young enough to know everything. /Oscar Wilde Experience is one thing you can't get for nothing. /Oscar Wilde When you are finished changing, you're finished. /Benjamin Franklin Tell me and I forget, teach me and I remember, involve me and I learn.
/Benjamin Franklin
Luck is where preparation meets opportunity. /George Patten



"EMF <kochind.com>" made the following annotations.
------------------------------------------------------------------------------
The information in this e-mail and any attachments is co...{{dropped:14}}


From sergeyg at gmail.com  Fri Mar 13 16:40:49 2009
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Fri, 13 Mar 2009 16:40:49 +0100
Subject: [R-SIG-Finance] Problem with RBloomberg (not the usual one)
In-Reply-To: <13CB99597D0BAA43B07ABFCE29E45F6B0262FBA2@phx0mbx01.kochind.com>
References: <7cb007bd0902270016h31633e81sc1bbfd2808de7ba4@mail.gmail.com>
	<7cb007bd0902270053h73dd8931pb7080c8ac5ac69c2@mail.gmail.com>
	<13CB99597D0BAA43B07ABFCE29E45F6B0262FBA2@phx0mbx01.kochind.com>
Message-ID: <7cb007bd0903130840v79427dbcxdd83772dd0e84799@mail.gmail.com>

Hi, Kent

Thank you for that!
I'll pass the info to our systems administrator, he is the only one
who is allowed to do things like that.

Have a nice weekend!

Best,
Serge

On Fri, Mar 13, 2009 at 15:32, Voss, Kent <VOSSK at kochind.com> wrote:
> Sergey,
>
> Sorry I missed your post earlier. ?I had this same problem and I reinstalled the DDE Server and Excel Add-In, rebooted and it fixed it. ?I thought they were already installed as I could pull down data in Excel from Bloomberg, but reinstalling them fixed my R problem. ?You can find the downloads at http://about.bloomberg.com/contact_softwaresupport_upgr.html
>
> Good luck.
>
> Kent
>
>
> -----Original Message-----
> From: Sergey Goriatchev [mailto:sergeyg at gmail.com]
> Sent: Friday, February 27, 2009 1:54 AM
> To: r-sig-finance at stat.math.ethz.ch; r-help at r-project.org
> Subject: Re: [R-SIG-Finance] Problem with RBloomberg (not the usual one)
>
> Hello, again, everyone
>
> I went through the code and narrowed down the problem
>
> in blpConnect:
> COMCreate("Bloomberg.Data.1") which then calls getCOMInstance does not work, because
> getCLSID("Bloomberg.Data.1") returns
> "Fehler: Invalid class string"
>
> What is this problem???
>
> Best,
> Sergey
>
> On Fri, Feb 27, 2009 at 09:16, Sergey Goriatchev <sergeyg at gmail.com> wrote:
>> Hello, everyone!
>>
>> I have a problem with RBloomberg and this is not the usual "no
>> administrator rights" problem.
>>
>> I have R 2.7.2, RBloomberg 0.1-10, RDCOMclient 0.92-0
>>
>> RDCOMClient, chron, zoo, stats: these packages load OK.
>>
>> Then, trying to connect, I get following error message:
>>
>>
>> ?conn <- blpConnect(show.days="week", na.action="previous.days",
>> periodicity="daily")
>> Warning messages:
>> 1: In getCOMInstance(name, force = TRUE, silent = TRUE) :
>> ?Couldn't get clsid from the string
>> 2: In blpConnect(show.days = "week", na.action = "previous.days",
>> periodicity = "daily") :
>> ?Seems like this is not a Bloomberg Workstation: ?Error : Invalid
>> class string
>>
>> Anyone encountered this problem?
>> What is wrong and how can I solve it?
>>
>> Online, I found just one instance of this problem discussed, and it
>> was in Chinese:
>>
>> http://cos.name/bbs/read.php?tid=12821&fpage=3
>>
>> Thank you for your help!
>>
>> Sergey
>>
>
>
>
> --
> I'm not young enough to know everything. /Oscar Wilde Experience is one thing you can't get for nothing. /Oscar Wilde When you are finished changing, you're finished. /Benjamin Franklin Tell me and I forget, teach me and I remember, involve me and I learn.
> /Benjamin Franklin
> Luck is where preparation meets opportunity. /George Patten
>
>
>
> "EMF <kochind.com>" made the following annotations.
> ------------------------------------------------------------------------------
> The information in this e-mail and any attachments is confidential and intended solely for the attention and use of the named addressee(s). It must not be disclosed to any person without proper authority. If you are not the intended recipient, or a person responsible for delivering it to the intended recipient, you are not authorized to and must not disclose, copy, distribute, or retain this message or any part of it.
>
> ==============================================================================
>
>



-- 
I'm not young enough to know everything. /Oscar Wilde
Experience is one thing you can't get for nothing. /Oscar Wilde
When you are finished changing, you're finished. /Benjamin Franklin
Tell me and I forget, teach me and I remember, involve me and I learn.
/Benjamin Franklin
Luck is where preparation meets opportunity. /George Patten


From tom at limepepper.co.uk  Mon Mar 16 18:54:07 2009
From: tom at limepepper.co.uk (Tom H)
Date: Mon, 16 Mar 2009 17:54:07 +0000
Subject: [R-SIG-Finance] quantmod package using "convert.time.series"
	function
Message-ID: <1237226047.1110.2.camel@localhost.localdomain>

Hi,

I was having a look at the quantmod function for getSymbols.MySQL, and
it uses a "convert.time.series()" function. I had a search for a
reference on it, but I can't find anything. What am I doing wrong?

> ?convert.time.series
No documentation for 'convert.time.series' in specified packages and
libraries:
you could try '??convert.time.series'
> ??convert.time.series

and nothing there as well, but the function executes, so something must
understand what it is...;

			fr <- convert.time.series(fr = fr, return.class = return.class)

Any ideas?

Cheers,

T


From jeff.a.ryan at gmail.com  Mon Mar 16 19:04:55 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 16 Mar 2009 13:04:55 -0500
Subject: [R-SIG-Finance] quantmod package using "convert.time.series"
	function
In-Reply-To: <1237226047.1110.2.camel@localhost.localdomain>
References: <1237226047.1110.2.camel@localhost.localdomain>
Message-ID: <e8e755250903161104w19f2814dp6fd9a49ba55bf940@mail.gmail.com>

Hi Tom,

The function is internal to quantmod, as such undocumented and ugly :)

> quantmod:::convert.time.series
function (fr, return.class)
{
    if ("quantmod.OHLC" %in% return.class) {
        class(fr) <- c("quantmod.OHLC", "zoo")
        return(fr)
    }
    else if ("xts" %in% return.class) {
        return(fr)
    }
    if ("zoo" %in% return.class) {
        return(as.zoo(fr))
    }
    else if ("ts" %in% return.class) {
        fr <- as.ts(fr)
        return(fr)
    }
    else if ("data.frame" %in% return.class) {
        fr <- as.data.frame(fr)
        return(fr)
    }
    else if ("matrix" %in% return.class) {
        fr <- as.data.frame(fr)
        return(fr)
    }
    else if ("its" %in% return.class) {
        if ("package:its" %in% search() || suppressMessages(require("its",
            quietly = TRUE))) {
            fr.dates <- as.POSIXct(as.character(index(fr)))
            fr <- its::its(coredata(fr), fr.dates)
            return(fr)
        }
        else {
            warning(paste("'its' from package 'its' could not be loaded:",
                " 'xts' class returned"))
        }
    }
    else if ("timeSeries" %in% return.class) {
        if ("package:fSeries" %in% search() ||
suppressMessages(require("fSeries",
            quietly = TRUE))) {
            fr <- timeSeries(coredata(fr), charvec = as.character(index(fr)))
            return(fr)
        }
        else {
            warning(paste("'timeSeries' from package 'fSeries' could
not be loaded:",
                " 'xts' class returned"))
        }
    }
}
<environment: namespace:quantmod>

Much better conversion (though slightly different form) can be found in xts

?try.xts, reclass, and Reclass.

HTH,
Jeff

R/Finance 2009: Applied Finance with R
April 24 and 25th 2009, Chicago, IL, USA
http://www.RinFinance.com




On Mon, Mar 16, 2009 at 12:54 PM, Tom H <tom at limepepper.co.uk> wrote:
> Hi,
>
> I was having a look at the quantmod function for getSymbols.MySQL, and
> it uses a "convert.time.series()" function. I had a search for a
> reference on it, but I can't find anything. What am I doing wrong?
>
>> ?convert.time.series
> No documentation for 'convert.time.series' in specified packages and
> libraries:
> you could try '??convert.time.series'
>> ??convert.time.series
>
> and nothing there as well, but the function executes, so something must
> understand what it is...;
>
> ? ? ? ? ? ? ? ? ? ? ? ?fr <- convert.time.series(fr = fr, return.class = return.class)
>
> Any ideas?
>
> Cheers,
>
> T
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From tom at limepepper.co.uk  Mon Mar 16 19:37:35 2009
From: tom at limepepper.co.uk (Tom H)
Date: Mon, 16 Mar 2009 18:37:35 +0000
Subject: [R-SIG-Finance] quantmod package using "convert.time.series"
 function
In-Reply-To: <e8e755250903161104w19f2814dp6fd9a49ba55bf940@mail.gmail.com>
References: <1237226047.1110.2.camel@localhost.localdomain>
	<e8e755250903161104w19f2814dp6fd9a49ba55bf940@mail.gmail.com>
Message-ID: <1237228655.1110.10.camel@localhost.localdomain>

On Mon, 2009-03-16 at 13:04 -0500, Jeff Ryan wrote:
> Hi Tom,
> 
> The function is internal to quantmod, as such undocumented and ugly :)

Ah right, I am still new to R.

So its suggests a quick question, which I am sure further reading of the
documentation will resolve, however I am hoping to short-cut reading all
of that pdf today ;-)

Question: I am making a quick change to the getSymbols.MySQL to support
my database layout of choice, which is a single table with a symbol
field.
Is there any way to temporarily add my getSymbols.MySQL <- function() to
the the quantmod package/namespace? It seems I can reference the
internal function directly with quantmod:::convert.time.series() but I
would prefer some sort of namespace("quantmod") to add my function to
quantmod while I am doing some messing about on it?

Thanks,

T


From jeff.a.ryan at gmail.com  Mon Mar 16 19:45:18 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 16 Mar 2009 13:45:18 -0500
Subject: [R-SIG-Finance] quantmod package using "convert.time.series"
	function
In-Reply-To: <1237228655.1110.10.camel@localhost.localdomain>
References: <1237226047.1110.2.camel@localhost.localdomain>
	<e8e755250903161104w19f2814dp6fd9a49ba55bf940@mail.gmail.com>
	<1237228655.1110.10.camel@localhost.localdomain>
Message-ID: <e8e755250903161145h66096af8l2bc6707850f89558@mail.gmail.com>

Hi Tom,

Unexported means you've got to go the ::: route.  Primary reason is
that it was really written to simplify the code, and unexported meant
one less thing to document.

You could simply hard-code the conversion to whatever you like, and
forgo that function.

The other option is to each session use fixInNamespace and fix on the
fly.  This is probably quite unreliable though, not to mention messy.

You could also change the source before installing.

In all honestly the interface should include a way to modify for your
own db schema, but that isn't there yet...

HTH,
Jeff

On Mon, Mar 16, 2009 at 1:37 PM, Tom H <tom at limepepper.co.uk> wrote:
> On Mon, 2009-03-16 at 13:04 -0500, Jeff Ryan wrote:
>> Hi Tom,
>>
>> The function is internal to quantmod, as such undocumented and ugly :)
>
> Ah right, I am still new to R.
>
> So its suggests a quick question, which I am sure further reading of the
> documentation will resolve, however I am hoping to short-cut reading all
> of that pdf today ;-)
>
> Question: I am making a quick change to the getSymbols.MySQL to support
> my database layout of choice, which is a single table with a symbol
> field.
> Is there any way to temporarily add my getSymbols.MySQL <- function() to
> the the quantmod package/namespace? It seems I can reference the
> internal function directly with quantmod:::convert.time.series() but I
> would prefer some sort of namespace("quantmod") to add my function to
> quantmod while I am doing some messing about on it?
>
> Thanks,
>
> T
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From yana.roth at yahoo.com  Tue Mar 17 09:30:53 2009
From: yana.roth at yahoo.com (Yana Roth)
Date: Tue, 17 Mar 2009 01:30:53 -0700 (PDT)
Subject: [R-SIG-Finance] help
Message-ID: <124558.26460.qm@web46213.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090317/8dd7264b/attachment.pl>

From ron_michael70 at yahoo.com  Tue Mar 17 11:27:38 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Tue, 17 Mar 2009 03:27:38 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Garch problem
Message-ID: <22556251.post@talk.nabble.com>


I have following dataset as monthly percentage return for a stock :

0.173741362
-0.062237174
0.02690583
0.04628821
0.056761269
0.018167457
-0.003103181
0.024902724
0.035687168
0.004398827
-0.054014599
-0.141975309
0.008093525
-0.035682426
0.034227567
-0.072450805
-0.037608486
0.026052104
-0.055664062
-0.024819028
-0.004241782
-0.052183174
-0.004494382
0.081828442
-0.015127804
0.091101695
-0.027184466
-0.032934132
-0.012383901
-0.009404389
0.033755274
-0.004081633
-0.014344262
0.033264033
0.03722334
0.094083414
0.019503546
0.010434783
-0.003442341
-0.121761658
0.041297935
0.050991501
-0.056603774
-0.018095238
0.022308438
-0.059772296
0.042381433
0.012584705
0.003824092
-0.024761905
0.047851562
-0.018173346
0.034646417
-0.033027523
-0.007590133
-0.004780115
0.008645533
-0.016190476
-0.016456922
0.049212598
0.003752345
-0.008411215
0.100848256
0.042808219
0.063218391
-0.016602317
0.056144484
0.054275093
0.028208745
0.020576132
-0.092741935
-0.061481481
-0.08445146
-0.045689655
0.023486902
-0.064430715
0.037735849
0.001818182
-0.025408348
-0.049348231
0.065621939
-0.054227941
-0.040816327
-0.038500507
0.062170706
-0.074404762
0.046087889
0.079918033
-0.046489564
0.080597015
-0.092081031
0.059837728
0.041148325
0.098345588
0.007531381
-0.039036545
0.021607606
0.044839255
-0.07611336
-0.043821209
0.028414299
0.041889483
-0.026518392
0.013181019
0.01300954
0.010273973
-0.022033898
-0.066291161
-0.011600928
-0.020187793
0.00431241
-0.014312977
-0.039690223
-0.023185484
-0.036119711
-0.042826552
-0.035234899
-0.016231884
-0.063052445
-0.031446541
0.045454545
-0.021118012
0.007614213
0.023929471
0.036900369
-0.024911032
-0.0486618
0.051150895
-0.057177616
0.010322581
-0.029374202
0.044736842
0.042821159
-0.073671498
0.070404172
-0.004872107
-0.048959608
0.009009009
0.00127551
0.01910828
0.09
-0.055045872
0.024271845
0.146919431
-0.013429752
0.064921466
0.025565388
0.091083413
-0.024604569
-0.06036036
0.064237776
-0.106306306
0.050403226
-0.065259117
0.137577002
-0.040613718
0.100658514
0.064957265
0.040930979
0.070932922
-0.010079194
-0.055272727
-0.010777521
-0.042801556
0.028455285
0.079841897
0.039531479
0.088028169
0.110032362
0.113119534
0.207962284
0.022983521
0.121237813
0.200378072
0.155905512
-0.122615804
0.060559006
0.010248902
-0.034782609
0.267267267
0.036729858
-0.033142857
-0.177304965
0
-0.064655172
0.153917051
-0.016240682
-0.093369418
0.058208955
-0.122708039
-0.019292605
-0.073770492
-0.081415929
-0.086705202
0.061181435
0.089463221
-0.153284672
-0.038793103
-0.100896861
-0.036408978
-0.01552795
-0.047844374
-0.072335726
-0.330357143
0.075555556
-0.001652893
-0.092301325

Now I fit a GARCH (1,1) model on that :

> garch(Delt(dat)[-1], c(1,1))

 ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 


     I     INITIAL X(I)        D(I)

     1     4.331103e-03     1.000e+00
     2     5.000000e-02     1.000e+00
     3     5.000000e-02     1.000e+00

    IT   NF      F         RELDF    PRELDF    RELDX   STPPAR   D*STEP  
NPRELDF
     0    1 -4.507e+02
     1    6 -4.508e+02  2.00e-04  3.20e-04  1.5e-03  6.3e+06  1.5e-04 
1.01e+03
     2    7 -4.508e+02  1.57e-05  1.69e-05  1.4e-03  2.0e+00  1.5e-04 
3.19e-01
     3   13 -4.521e+02  2.85e-03  4.72e-03  5.6e-01  2.0e+00  1.3e-01 
3.16e-01
     4   16 -4.602e+02  1.76e-02  4.41e-03  8.1e-01  6.7e-01  5.1e-01 
1.99e-02
     5   23 -4.607e+02  1.13e-03  2.77e-03  1.6e-04  7.4e+00  1.8e-04 
8.48e+00
     6   24 -4.607e+02  4.81e-05  4.37e-05  1.6e-04  2.0e+00  1.8e-04 
1.77e+01
     7   30 -4.638e+02  6.60e-03  8.81e-03  9.8e-02  2.0e+00  1.2e-01 
1.84e+01
     8   31 -4.645e+02  1.52e-03  7.73e-03  8.2e-02  1.3e+00  1.2e-01 
1.39e-02
     9   33 -4.688e+02  9.18e-03  6.28e-03  6.8e-02  0.0e+00  1.2e-01 
6.94e-03
    10   35 -4.693e+02  9.32e-04  9.33e-04  8.9e-03  1.9e+00  1.8e-02 
2.86e-02
    11   37 -4.699e+02  1.34e-03  1.59e-03  1.6e-02  1.8e+00  3.5e-02 
5.99e-02
    12   38 -4.704e+02  1.05e-03  1.43e-03  1.6e-02  1.6e+00  3.5e-02 
9.10e-03
    13   40 -4.705e+02  1.84e-04  2.85e-04  5.3e-03  1.2e+00  1.3e-02 
7.52e-04
    14   42 -4.705e+02  3.71e-05  5.18e-05  2.4e-03  8.1e-01  5.0e-03 
7.09e-05
    15   44 -4.705e+02  8.51e-07  3.04e-06  4.9e-04  8.2e-01  9.5e-04 
5.29e-06
    16   57 -4.705e+02 -7.73e-15  1.09e-15  5.0e-15  4.4e+06  9.1e-15 
2.87e-07

 ***** FALSE CONVERGENCE *****

 FUNCTION    -4.704848e+02   RELDX        4.961e-15
 FUNC. EVALS      57         GRAD. EVALS      16
 PRELDF       1.088e-15      NPRELDF      2.867e-07

     I      FINAL X(I)        D(I)          G(I)

     1    2.824235e-05     1.000e+00     5.619e+01
     2    8.649332e-02     1.000e+00    -5.899e-01
     3    9.175397e-01     1.000e+00    -6.866e-01


Call:
garch(x = Delt(dat)[-1], order = c(1, 1))

Coefficient(s):
       a0         a1         b1  
2.824e-05  8.649e-02  9.175e-01  

Warning message:
In sqrt(pred$e) : NaNs produced

What we see that sum of alpha and beta coef is more than 1. Therefore
probably I choose a wrong model on my dataset. Can anyone please guide me
how to modify that model?

Regards,
-- 
View this message in context: http://www.nabble.com/Garch-problem-tp22556251p22556251.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From brian at braverock.com  Tue Mar 17 13:25:30 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 17 Mar 2009 07:25:30 -0500
Subject: [R-SIG-Finance] help (regarding block bootstrap)
In-Reply-To: <124558.26460.qm@web46213.mail.sp1.yahoo.com>
References: <124558.26460.qm@web46213.mail.sp1.yahoo.com>
Message-ID: <49BF96BA.9090509@braverock.com>

Yana Roth wrote:
> Hello,
> I am trying to do block reasampling to rearrange my data and not succeed to do random permutation and assugnement.
> I would like to divide original time series to subsamples and then to rearange this subsamples randomly.
>  
> Function tsboot works only if I need to check statistic, I am interested in just rearranging the data while keeping its structure.
>  
> The problem is defined as follows.
> 1. I define llentgh of block , b.
> 2.Divide an original time series by b and receive k=n/b subsamles.
> 3. I need to generate random vector of integers from 1 to k
> 4 Let Z*(j) be for j=1....k be the j th row of a matrix with num of rows equal to number of blocks and number of columns equal to number of simulations.
> 5. Assigne to each Z*(j) the blocks according to generated random vector(each column of matrix is a different order of permutations)
For future reference, please provide reproducible code as per the posting guidelines.  It makes it easier for others to help you.  Also, please use a desciptive subject, as we all get a quite a lot of mail.

Your procedure appears incorrect. 

Your steps 3-5 look like a homework assignment, so I'm going to ignore those and focus on the block bootstrap, which has some applicability to other members of this list in financial time series analysis.

I suspect that you simply misunderstood the "statistic" parameter of tsboot().  I expect that you do indeed intend to use the bootstrapped data to calculate one or more statstics, this is what the statistic parameter is for.

Block bootstrapping works by randomly sampling blocks of length l from your original series.  The tsboot function also applies one or more statistics to the bootstrapped data, and uses the multiple samples to calculate the bias and standard error for those statistics, providing you with a sensitivity analysis for those statistics on your data.

Using the data series "acme" included with R, you would do something like:

library(boot)
library(PerformanceAnalytics)
data(acme)

#calculate the sensitivity of standard deviation on the data:
tsboot(tseries=acme[,2],statistic=sd,R=1000,l=12,sim="fixed",endcorr=FALSE,n.sim=1000)
# use blocks of length 12 (one year) to 
# create 1000 bootstrapped time series
# each of length 1000 observations
 
#Returns:
#Bootstrap Statistics :
#      original       bias    std. error
#t1* 0.05362889 0.0001614213 0.001925484

# calculate sensitivity of VaR:
tsboot(tseries=acme[,2],statistic=VaR.CornishFisher,R=1000,sim="fixed",l=12,endcorr=FALSE,n.sim=1000)

#Returns:
#Bootstrap Statistics :
#    original      bias    std. error
#t1* 0.227064 0.009412978 0.007284343

Normally, this is what you want.  The random bootstrapped series itself is not useful to you, except to calculate a statistic or statistics of interest, and understand their sensitivity.  If you want the bootstrapped series returned, you can modify the code of the tsboot function to do what you want.  

If you want to apply your steps 3-5 to the bootstrapped data, see the documentation of tsboot() for an example of defining a function to use as the statistic parameter.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From meenusahi at gmail.com  Tue Mar 17 14:30:06 2009
From: meenusahi at gmail.com (Meenu Sahi)
Date: Tue, 17 Mar 2009 14:30:06 +0100
Subject: [R-SIG-Finance] How to input a matrix from an excel file /text
	file/database
Message-ID: <b18988540903170630s19ebda85u565b6675dae8a6bf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090317/2239a234/attachment.pl>

From matthieu.stigler at gmail.com  Tue Mar 17 14:35:27 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Tue, 17 Mar 2009 19:05:27 +0530
Subject: [R-SIG-Finance] help (regarding block bootstrap)
In-Reply-To: <49BF96BA.9090509@braverock.com>
References: <124558.26460.qm@web46213.mail.sp1.yahoo.com>
	<49BF96BA.9090509@braverock.com>
Message-ID: <49BFA71F.10100@gmail.com>

Brian G. Peterson a ?crit :
> Yana Roth wrote:
>> Hello,
>> I am trying to do block reasampling to rearrange my data and not 
>> succeed to do random permutation and assugnement.
>> I would like to divide original time series to subsamples and then to 
>> rearange this subsamples randomly.
>>  
>> Function tsboot works only if I need to check statistic, I am 
>> interested in just rearranging the data while keeping its structure.
>>  
>> The problem is defined as follows.
>> 1. I define llentgh of block , b.
>> 2.Divide an original time series by b and receive k=n/b subsamles.
>> 3. I need to generate random vector of integers from 1 to k
>> 4 Let Z*(j) be for j=1....k be the j th row of a matrix with num of 
>> rows equal to number of blocks and number of columns equal to number 
>> of simulations.
>> 5. Assigne to each Z*(j) the blocks according to generated random 
>> vector(each column of matrix is a different order of permutations)
> For future reference, please provide reproducible code as per the 
> posting guidelines.  It makes it easier for others to help you.  Also, 
> please use a desciptive subject, as we all get a quite a lot of mail.
>
> Your procedure appears incorrect.
> Your steps 3-5 look like a homework assignment, so I'm going to ignore 
> those and focus on the block bootstrap, which has some applicability 
> to other members of this list in financial time series analysis.
>


Thanks Brian for these examples!

Actually even if it is homework I would be really interested in the 
answer ;-) this is a question I always wanted to find out, maybe is it 
the right time to ask? I looked in source code of tsboot() but got lost

Does anyone has an idea about how to generate block resampling with 
function sample()? And with overlapping and non-overlapping blocks? That 
is, (example just taken from Maddala and Li 1998, bootstraping 
cointegrating relationships in journal of econometrics 80,2 also in 
their book unit roots, coint and struc change page 328) you pick blocks:

if series is {3, 6, 7, 2, 1, 5}
-non-overlapping:  {(3,6,7), (2,1,5)}
-overlapping:  {(3,6,7), (6,7,2), (7,2, l), (2, 1,5)}

and then sample those blocks with replacement. I don't have a clear idea 
about how do to that on R... Thanks!
 
a<-1:100
boot1<-sample(a, replace=TRUE) #length 1

> I suspect that you simply misunderstood the "statistic" parameter of 
> tsboot().  I expect that you do indeed intend to use the bootstrapped 
> data to calculate one or more statstics, this is what the statistic 
> parameter is for.
>
> Block bootstrapping works by randomly sampling blocks of length l from 
> your original series.  The tsboot function also applies one or more 
> statistics to the bootstrapped data, and uses the multiple samples to 
> calculate the bias and standard error for those statistics, providing 
> you with a sensitivity analysis for those statistics on your data.
>
> Using the data series "acme" included with R, you would do something 
> like:
>
> library(boot)
> library(PerformanceAnalytics)
> data(acme)
>
> #calculate the sensitivity of standard deviation on the data:
> tsboot(tseries=acme[,2],statistic=sd,R=1000,l=12,sim="fixed",endcorr=FALSE,n.sim=1000) 
>
> # use blocks of length 12 (one year) to # create 1000 bootstrapped 
> time series
> # each of length 1000 observations
>
> #Returns:
> #Bootstrap Statistics :
> #      original       bias    std. error
> #t1* 0.05362889 0.0001614213 0.001925484
>
> # calculate sensitivity of VaR:
> tsboot(tseries=acme[,2],statistic=VaR.CornishFisher,R=1000,sim="fixed",l=12,endcorr=FALSE,n.sim=1000) 
>
>
> #Returns:
> #Bootstrap Statistics :
> #    original      bias    std. error
> #t1* 0.227064 0.009412978 0.007284343
>
> Normally, this is what you want.  The random bootstrapped series 
> itself is not useful to you, except to calculate a statistic or 
> statistics of interest, and understand their sensitivity.  If you want 
> the bootstrapped series returned, you can modify the code of the 
> tsboot function to do what you want. 
> If you want to apply your steps 3-5 to the bootstrapped data, see the 
> documentation of tsboot() for an example of defining a function to use 
> as the statistic parameter.
>
> Regards,
>
>  - Brian


From Zeno.Adams at ebs.edu  Tue Mar 17 14:39:18 2009
From: Zeno.Adams at ebs.edu (Adams, Zeno)
Date: Tue, 17 Mar 2009 14:39:18 +0100
Subject: [R-SIG-Finance] Unit Root Tests: Empirical Results vs Theory
Message-ID: <9064522880125945B98983BBAECBA1CC21CBC0@exchsrv001.ebs.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090317/0a1a19a8/attachment.pl>

From Xiaochen.Sun at brunel.ac.uk  Tue Mar 17 14:40:14 2009
From: Xiaochen.Sun at brunel.ac.uk (Xiaochen Sun)
Date: Tue, 17 Mar 2009 13:40:14 -0000
Subject: [R-SIG-Finance] How to input a matrix from an excel file
	/textfile/database
References: <b18988540903170630s19ebda85u565b6675dae8a6bf@mail.gmail.com>
Message-ID: <E386E504246A9249A9176B5BEEC13B6F018CEC40@UXEXMBU116.academic.windsor>

http://cran.r-project.org/doc/manuals/R-data.html
 
 

________________________________

From: r-sig-finance-bounces at stat.math.ethz.ch on behalf of Meenu Sahi
Sent: Tue 17/03/2009 13:30
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] How to input a matrix from an excel file /textfile/database



Dear Members

I am a beginner at R. I am trying to input data from an excel file or text
file or from a database. Where can i look for help on these topics.

Thanks
Meenu

        [[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From ggrothendieck at gmail.com  Tue Mar 17 14:41:49 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 17 Mar 2009 09:41:49 -0400
Subject: [R-SIG-Finance] How to input a matrix from an excel file /text
	file/database
In-Reply-To: <b18988540903170630s19ebda85u565b6675dae8a6bf@mail.gmail.com>
References: <b18988540903170630s19ebda85u565b6675dae8a6bf@mail.gmail.com>
Message-ID: <971536df0903170641j2a0adae0rc64a4720982f987f@mail.gmail.com>

For Excel see:
http://wiki.r-project.org/rwiki/doku.php?id=tips:data-io:ms_windows&s=excel

ROBDC, RSQLite, RMySQL, etc. packages can be used for databases.


On Tue, Mar 17, 2009 at 9:30 AM, Meenu Sahi <meenusahi at gmail.com> wrote:
> Dear Members
>
> I am a beginner at R. I am trying to input data from an excel file or text
> file or from a database. Where can i look for help on these topics.
>
> Thanks
> Meenu
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From brian at braverock.com  Tue Mar 17 16:17:04 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 17 Mar 2009 10:17:04 -0500
Subject: [R-SIG-Finance] How to input a matrix from an excel file /text
 file/database
In-Reply-To: <b18988540903170630s19ebda85u565b6675dae8a6bf@mail.gmail.com>
References: <b18988540903170630s19ebda85u565b6675dae8a6bf@mail.gmail.com>
Message-ID: <49BFBEF0.1090707@braverock.com>

Meenu Sahi wrote:
> Dear Members
>
> I am a beginner at R. I am trying to input data from an excel file or text
> file or from a database. Where can i look for help on these topics.
>   
Try the archives of the r-help list, or the copious R documentation.  
This list is for finance-related posts only, and the members appreciate 
the high signal to noise ratio.

Regards,

   - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From Horace.Tso at pgn.com  Tue Mar 17 17:47:13 2009
From: Horace.Tso at pgn.com (Horace Tso)
Date: Tue, 17 Mar 2009 09:47:13 -0700
Subject: [R-SIG-Finance] ibrokers issue
Message-ID: <5C3F9922B1D5FB4886B2D2045AB952F302BC0ED85E@IPEXMAIL.corp.dom>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090317/372f7a38/attachment.pl>

From patrick at burns-stat.com  Tue Mar 17 18:03:38 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Tue, 17 Mar 2009 17:03:38 +0000
Subject: [R-SIG-Finance] [R-sig-finance] Garch problem
In-Reply-To: <22556251.post@talk.nabble.com>
References: <22556251.post@talk.nabble.com>
Message-ID: <49BFD7EA.8080205@burns-stat.com>

The fit is essentially saying that the half-life
of a shock is infinite.  This generally occurs
when the in-sample volatility has a general
trend.  One solution is more data.  There are
doubtless other paths as well.

RON70 wrote:
> I have following dataset as monthly percentage return for a stock :
>
> 0.173741362
> -0.062237174
>
>   

[ ... ]
> -0.001652893
> -0.092301325
>
> Now I fit a GARCH (1,1) model on that :
>
>   
>> garch(Delt(dat)[-1], c(1,1))
>>     
>
>  ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>
>
>      I     INITIAL X(I)        D(I)
>
>      1     4.331103e-03     1.000e+00
>      2     5.000000e-02     1.000e+00
>      3     5.000000e-02     1.000e+00
>
>     IT   NF      F         RELDF    PRELDF    RELDX   STPPAR   D*STEP  
> NPRELDF
>      0    1 -4.507e+02
>      1    6 -4.508e+02  2.00e-04  3.20e-04  1.5e-03  6.3e+06  1.5e-04 
> 1.01e+03
>      2    7 -4.508e+02  1.57e-05  1.69e-05  1.4e-03  2.0e+00  1.5e-04 
> 3.19e-01
>      3   13 -4.521e+02  2.85e-03  4.72e-03  5.6e-01  2.0e+00  1.3e-01 
> 3.16e-01
>      4   16 -4.602e+02  1.76e-02  4.41e-03  8.1e-01  6.7e-01  5.1e-01 
> 1.99e-02
>      5   23 -4.607e+02  1.13e-03  2.77e-03  1.6e-04  7.4e+00  1.8e-04 
> 8.48e+00
>      6   24 -4.607e+02  4.81e-05  4.37e-05  1.6e-04  2.0e+00  1.8e-04 
> 1.77e+01
>      7   30 -4.638e+02  6.60e-03  8.81e-03  9.8e-02  2.0e+00  1.2e-01 
> 1.84e+01
>      8   31 -4.645e+02  1.52e-03  7.73e-03  8.2e-02  1.3e+00  1.2e-01 
> 1.39e-02
>      9   33 -4.688e+02  9.18e-03  6.28e-03  6.8e-02  0.0e+00  1.2e-01 
> 6.94e-03
>     10   35 -4.693e+02  9.32e-04  9.33e-04  8.9e-03  1.9e+00  1.8e-02 
> 2.86e-02
>     11   37 -4.699e+02  1.34e-03  1.59e-03  1.6e-02  1.8e+00  3.5e-02 
> 5.99e-02
>     12   38 -4.704e+02  1.05e-03  1.43e-03  1.6e-02  1.6e+00  3.5e-02 
> 9.10e-03
>     13   40 -4.705e+02  1.84e-04  2.85e-04  5.3e-03  1.2e+00  1.3e-02 
> 7.52e-04
>     14   42 -4.705e+02  3.71e-05  5.18e-05  2.4e-03  8.1e-01  5.0e-03 
> 7.09e-05
>     15   44 -4.705e+02  8.51e-07  3.04e-06  4.9e-04  8.2e-01  9.5e-04 
> 5.29e-06
>     16   57 -4.705e+02 -7.73e-15  1.09e-15  5.0e-15  4.4e+06  9.1e-15 
> 2.87e-07
>
>  ***** FALSE CONVERGENCE *****
>
>  FUNCTION    -4.704848e+02   RELDX        4.961e-15
>  FUNC. EVALS      57         GRAD. EVALS      16
>  PRELDF       1.088e-15      NPRELDF      2.867e-07
>
>      I      FINAL X(I)        D(I)          G(I)
>
>      1    2.824235e-05     1.000e+00     5.619e+01
>      2    8.649332e-02     1.000e+00    -5.899e-01
>      3    9.175397e-01     1.000e+00    -6.866e-01
>
>
> Call:
> garch(x = Delt(dat)[-1], order = c(1, 1))
>
> Coefficient(s):
>        a0         a1         b1  
> 2.824e-05  8.649e-02  9.175e-01  
>
> Warning message:
> In sqrt(pred$e) : NaNs produced
>
> What we see that sum of alpha and beta coef is more than 1. Therefore
> probably I choose a wrong model on my dataset. Can anyone please guide me
> how to modify that model?
>
> Regards,
>


From jeff.a.ryan at gmail.com  Tue Mar 17 19:54:38 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 17 Mar 2009 13:54:38 -0500
Subject: [R-SIG-Finance] ibrokers issue
In-Reply-To: <5C3F9922B1D5FB4886B2D2045AB952F302BC0ED85E@IPEXMAIL.corp.dom>
References: <AcmnIAWwRj2Br9z4QHe94sgsFODM1A==>
	<5C3F9922B1D5FB4886B2D2045AB952F302BC0ED85E@IPEXMAIL.corp.dom>
Message-ID: <e8e755250903171154k32c04d3aufce48a41a2969d53@mail.gmail.com>

Hi Horace,

They did.  I posted the fix to the googlecode repository, as there are
additional changes being made before the next CRAN version.

You can grab from googlecode, or fix this file in your distribution.

http://code.google.com/p/ibrokers/source/browse/trunk/R/twsConnect.R

I should have the CRAN version and new docs sometime near the
beginning of next month.

Jeff

R/Finance 2009: Applied Finance with R
April 24, 25 2009, Chicago, IL, USA
http://www.RinFinance.com

EARLY REGISTRATION ENDS MARCH 31st!

On Tue, Mar 17, 2009 at 11:47 AM, Horace Tso <Horace.Tso at pgn.com> wrote:
> Hi folks/Jeff,
>
> Did?anyone?have problem connecting to Ibrokers? Started about couple days
> ago,?twsConnect?fails and?complains
>
> Error in structure(list(s, clientId = clientId, port = port, server.version
> = SERVER_VERSION,? :
> ? object "SERVER_VERSION" not found
> Looks like?ib?had a server upgrade or something....
>
> TIA,
>
> Horace



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From klein82517 at yahoo.de  Tue Mar 17 20:24:39 2009
From: klein82517 at yahoo.de (Andreas Klein)
Date: Tue, 17 Mar 2009 19:24:39 +0000 (GMT)
Subject: [R-SIG-Finance] help (regarding block bootstrap)
In-Reply-To: <49BFA71F.10100@gmail.com>
Message-ID: <371947.97764.qm@web24206.mail.ird.yahoo.com>


Hello.

Some time ago I write a seminar work about regressing the oil price on the CDAX. There, I used a nonparamtetric-block-bootstrap approach by hand, because I needed to resample pairs of blocks. I worked with sample(). I think there is some need of further optimization, but the code should give the idea of block-sampling:

The example:
if series is       {3, 6, 7, 2, 1, 5}
- non-overlapping: {(3,6,7), (2,1,5)}
- overlapping:     {(3,6,7), (6,7,2), (7,2,l), (2,1,5)}



For the overlapping case you have 4 blocks with length 3. Spoken in time indices the blocks have the following structure:
1:3
2:4
3:5
4:6

So, one has to resample the starting time indices 1:4 and add 2 to each time index to grab the data right:

x <- c(3,6,7,2,1,5)

x_sample  <- numeric(4*3) #4 blocks, each of length 3
mean_boot <- numeric(10000)

for (i in 1:10000)

{

for (j in 0:3)

{
  
  idx <- sample(1:4,1,replace=TRUE) #the starting index

  x_sample[(3*j+1):(3*j+3)] <- x[(idx):(idx+2)]
  
}

mean_boot[i] <- mean(x_sample)

}


Next, the non-overlapping example with 2 blocks. Here we have the following time structure:
1:3
4:6

So one has to resample 1 and 4 and add 2 to grab the data. If the series is longer, one would recognize, that the first time index can be described by the row: 3*t+1, so one only has to draw with replacement from (1:2) or equivalently (0:1):

x <- c(3,6,7,2,1,5)

x_sample  <- numeric(2*3) #2 blocks of length 3
mean_boot <- numeric(10000)

for (i in 1:10000)

{

for (j in 0:1)

{
  
  idx <- sample(0:1,1,replace=TRUE)

  idx <- 3*idx+1 #the starting index

  x_sample[(3*j+1):(3*j+3)] <- x[(idx):(idx+2)]
  
}

mean_boot[i] <- mean(x_sample)

}



And when you ask yourself what block length would be the right one, well Politis and White (2004) have the answer: http://econ.ucsd.edu/~mbacci/white/pub_files/hwcv-093.pdf
Also pay attention to the corrections of the algorithm: http://www.economics.ox.ac.uk/members/andrew.patton/SBblockCORRECTION_jan08.pdf

But the most important thing, pay attention to the R implementation:
http://www.math.ucsd.edu/~politis/SOFT/PPW/ppw.R


Hope it works.... it is some time ago since I played around with it, but maybe it is some food for though :)


Matthias.


--- Matthieu Stigler <matthieu.stigler at gmail.com> schrieb am Di, 17.3.2009:

> Von: Matthieu Stigler <matthieu.stigler at gmail.com>
> Betreff: Re: [R-SIG-Finance] help (regarding block bootstrap)
> An: r-sig-finance at stat.math.ethz.ch
> CC: "Yana Roth" <yana.roth at yahoo.com>
> Datum: Dienstag, 17. M?rz 2009, 14:35
> Brian G. Peterson a ?crit :
> > Yana Roth wrote:
> >> Hello,
> >> I am trying to do block reasampling to rearrange
> my data and not succeed to do random permutation and
> assugnement.
> >> I would like to divide original time series to
> subsamples and then to rearange this subsamples randomly.
> >>  Function tsboot works only if I need to check
> statistic, I am interested in just rearranging the data
> while keeping its structure.
> >>  The problem is defined as follows.
> >> 1. I define llentgh of block , b.
> >> 2.Divide an original time series by b and receive
> k=n/b subsamles.
> >> 3. I need to generate random vector of integers
> from 1 to k
> >> 4 Let Z*(j) be for j=1....k be the j th row of a
> matrix with num of rows equal to number of blocks and number
> of columns equal to number of simulations.
> >> 5. Assigne to each Z*(j) the blocks according to
> generated random vector(each column of matrix is a different
> order of permutations)
> > For future reference, please provide reproducible code
> as per the posting guidelines.  It makes it easier for
> others to help you.  Also, please use a desciptive subject,
> as we all get a quite a lot of mail.
> > 
> > Your procedure appears incorrect.
> > Your steps 3-5 look like a homework assignment, so
> I'm going to ignore those and focus on the block
> bootstrap, which has some applicability to other members of
> this list in financial time series analysis.
> > 
> 
> 
> Thanks Brian for these examples!
> 
> Actually even if it is homework I would be really
> interested in the answer ;-) this is a question I always
> wanted to find out, maybe is it the right time to ask? I
> looked in source code of tsboot() but got lost
> 
> Does anyone has an idea about how to generate block
> resampling with function sample()? And with overlapping and
> non-overlapping blocks? That is, (example just taken from
> Maddala and Li 1998, bootstraping cointegrating
> relationships in journal of econometrics 80,2 also in their
> book unit roots, coint and struc change page 328) you pick
> blocks:
> 
> if series is {3, 6, 7, 2, 1, 5}
> -non-overlapping:  {(3,6,7), (2,1,5)}
> -overlapping:  {(3,6,7), (6,7,2), (7,2, l), (2, 1,5)}
> 
> and then sample those blocks with replacement. I don't
> have a clear idea about how do to that on R... Thanks!
> 
> a<-1:100
> boot1<-sample(a, replace=TRUE) #length 1
> 
> > I suspect that you simply misunderstood the
> "statistic" parameter of tsboot().  I expect that
> you do indeed intend to use the bootstrapped data to
> calculate one or more statstics, this is what the statistic
> parameter is for.
> > 
> > Block bootstrapping works by randomly sampling blocks
> of length l from your original series.  The tsboot function
> also applies one or more statistics to the bootstrapped
> data, and uses the multiple samples to calculate the bias
> and standard error for those statistics, providing you with
> a sensitivity analysis for those statistics on your data.
> > 
> > Using the data series "acme" included with
> R, you would do something like:
> > 
> > library(boot)
> > library(PerformanceAnalytics)
> > data(acme)
> > 
> > #calculate the sensitivity of standard deviation on
> the data:
> >
> tsboot(tseries=acme[,2],statistic=sd,R=1000,l=12,sim="fixed",endcorr=FALSE,n.sim=1000)
> 
> > # use blocks of length 12 (one year) to # create 1000
> bootstrapped time series
> > # each of length 1000 observations
> > 
> > #Returns:
> > #Bootstrap Statistics :
> > #      original       bias    std. error
> > #t1* 0.05362889 0.0001614213 0.001925484
> > 
> > # calculate sensitivity of VaR:
> >
> tsboot(tseries=acme[,2],statistic=VaR.CornishFisher,R=1000,sim="fixed",l=12,endcorr=FALSE,n.sim=1000)
> 
> > 
> > #Returns:
> > #Bootstrap Statistics :
> > #    original      bias    std. error
> > #t1* 0.227064 0.009412978 0.007284343
> > 
> > Normally, this is what you want.  The random
> bootstrapped series itself is not useful to you, except to
> calculate a statistic or statistics of interest, and
> understand their sensitivity.  If you want the bootstrapped
> series returned, you can modify the code of the tsboot
> function to do what you want. If you want to apply your
> steps 3-5 to the bootstrapped data, see the documentation of
> tsboot() for an example of defining a function to use as the
> statistic parameter.
> > 
> > Regards,
> > 
> >  - Brian
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.





From matthieu.stigler at gmail.com  Tue Mar 17 20:25:55 2009
From: matthieu.stigler at gmail.com (Matthieu Stigler)
Date: Wed, 18 Mar 2009 00:55:55 +0530
Subject: [R-SIG-Finance] Unit Root Tests: Empirical Results vs Theory
In-Reply-To: <9064522880125945B98983BBAECBA1CC21CBC0@exchsrv001.ebs.local>
References: <9064522880125945B98983BBAECBA1CC21CBC0@exchsrv001.ebs.local>
Message-ID: <49BFF943.6040007@gmail.com>

Adams, Zeno a ?crit :
> I am currently analyzing a data set using panel cointegration. Like with any other cointegration method the variables have to be non-stationary. For my specific case (German commercial real estate market) I find the variables "total stock of office space" and "vacancy rates" to be non-stationary using standard unit-root tests. 
> >From theoretic considerations however it is clear that the vacancy rate has to be between 0 and 1 and also that the office space is unlikely to wander off to any arbitrary values.
>
> how would you deal with such a situation?
>
> Do you think it is possible to have "technical non-stationarity" in the sense that shocks in the past have a permanent impact on the series but that the variable exhibits a random walk behavior only within certain bounds? If that would be the case then it would be necessary to treat the vacancy rate as a non-stationary variables since ignoring this would lead to spurious regression.
>   
The interesting concept that a variable is non-stationary within  bounds 
and stationary outside (and so globally stationary) can be tested, using 
non-linear unit root tests. There are two of such tests (unit root vs 
stationary threshold autoregressive process) in the dev version of 
package tsDyn: BBCTest() and KapShinTest() 
(http://code.google.com/p/tsdyn/wiki/ThresholdCointegration)

They may be nevertheless not so useful as if you find that the variable 
is indeed globally stationary, you may nevertheless not know what do do 
with it!!

It is not clear and few discussed in the literature whether the fact 
that the variable is globally stationary does not lead to spurious 
regression. Note also that the concept of integrated (I(0) and I(1)) 
time series is not well defined in the case of non-linear time series 
(see Gonzalo and Pittarakis threshold effect in cointegrating 
relationships 2006).

Mat
> Thank you for your contributions
>
>
> EBS European Business School gemeinnuetzige GmbH - Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 19951 - Umsatzsteuer-ID DE 113891213 Geschaeftsfuehrer: Prof. Dr. Christopher Jahns,  Rektor/CEO; Dr. Reimar Palte,  Kanzler/CFO;  Sabine Fuchs, CMO; Verwaltungsrat: Dr. Hellmut K. Albrecht, Vorsitzender
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From andyzhu35 at yahoo.com  Wed Mar 18 01:54:07 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Tue, 17 Mar 2009 17:54:07 -0700 (PDT)
Subject: [R-SIG-Finance] quantmod: corp action
Message-ID: <55762.5115.qm@web56205.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090317/969e4fbf/attachment.pl>

From rmeier55 at web.de  Wed Mar 18 02:56:54 2009
From: rmeier55 at web.de (Robert Meier)
Date: Wed, 18 Mar 2009 02:56:54 +0100
Subject: [R-SIG-Finance] Removing the seasonality of a time series with FFT
Message-ID: <540279665@web.de>

I'm trying to remove the seasonality from a time series with FFT. For example, I'd like to remove the yearly seasonality from a time series and estimate the parameters (amplitude, frequency, phase).

I'm having problems with the estimation of the correct phase parameters. Could somebody help me out? Thanks in advance.

P.S. For testing, for example the sunspot series could be used:
test=read.table("http://www.stat.pitt.edu/stoffer/tsa2/data/sunspots.dat")
_______________________________________________________________________
Aufgepasst: Sind Ihre Daten beim Online-Banking auch optimal gesch?tzt?
Jetzt absichern: https://homebanking.web.de/?mc=mail at footer.hb


From josh.m.ulrich at gmail.com  Wed Mar 18 05:30:16 2009
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Tue, 17 Mar 2009 23:30:16 -0500
Subject: [R-SIG-Finance] quantmod: corp action
In-Reply-To: <55762.5115.qm@web56205.mail.re3.yahoo.com>
References: <55762.5115.qm@web56205.mail.re3.yahoo.com>
Message-ID: <8cca69990903172130s2bee67dcg4d7a852b3532979f@mail.gmail.com>

Hi Andy,

getDividends() returns dividends (adjusted for splits) from Yahoo.

The TTR fuction getYahooData() allows you to pull prices, splits and
dividends from Yahoo.

# Pull prices (adjusted for splits and dividends),
# volume (adjusted for splits), splits, and dividends
# (adjusted for splits).
x <- getYahooData("GE")

# Pull only splits and dividends
x <- getYahooData("GE",type="split")

Best,
Josh
--
http://quantemplation.blogspot.com



On Tue, Mar 17, 2009 at 7:54 PM, Andy Zhu <andyzhu35 at yahoo.com> wrote:
> Hi,
>
> It seems that quantmod doesn't provide functions to get series for stock split which in original Yahoo series has this data.
>
> second, does getDividends give stock dividends?
>
> If not, how do you extract stock dividends and split from the public sources?
>
> Thanks.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ron_michael70 at yahoo.com  Wed Mar 18 06:36:12 2009
From: ron_michael70 at yahoo.com (RON70)
Date: Tue, 17 Mar 2009 22:36:12 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Garch problem
In-Reply-To: <49BFD7EA.8080205@burns-stat.com>
References: <22556251.post@talk.nabble.com> <49BFD7EA.8080205@burns-stat.com>
Message-ID: <22573080.post@talk.nabble.com>


Dear Patrick, thank you so much for this reply. You said one solution is to
increase the data point. However at this point I can not get more. Therefore
if you please tell more about "doubtless other paths" I will be truly
grateful.

Regards,


Patrick Burns-2 wrote:
> 
> The fit is essentially saying that the half-life
> of a shock is infinite.  This generally occurs
> when the in-sample volatility has a general
> trend.  One solution is more data.  There are
> doubtless other paths as well.
> 
> RON70 wrote:
>> I have following dataset as monthly percentage return for a stock :
>>
>> 0.173741362
>> -0.062237174
>>
>>   
> 
> [ ... ]
>> -0.001652893
>> -0.092301325
>>
>> Now I fit a GARCH (1,1) model on that :
>>
>>   
>>> garch(Delt(dat)[-1], c(1,1))
>>>     
>>
>>  ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>>
>>
>>      I     INITIAL X(I)        D(I)
>>
>>      1     4.331103e-03     1.000e+00
>>      2     5.000000e-02     1.000e+00
>>      3     5.000000e-02     1.000e+00
>>
>>     IT   NF      F         RELDF    PRELDF    RELDX   STPPAR   D*STEP  
>> NPRELDF
>>      0    1 -4.507e+02
>>      1    6 -4.508e+02  2.00e-04  3.20e-04  1.5e-03  6.3e+06  1.5e-04 
>> 1.01e+03
>>      2    7 -4.508e+02  1.57e-05  1.69e-05  1.4e-03  2.0e+00  1.5e-04 
>> 3.19e-01
>>      3   13 -4.521e+02  2.85e-03  4.72e-03  5.6e-01  2.0e+00  1.3e-01 
>> 3.16e-01
>>      4   16 -4.602e+02  1.76e-02  4.41e-03  8.1e-01  6.7e-01  5.1e-01 
>> 1.99e-02
>>      5   23 -4.607e+02  1.13e-03  2.77e-03  1.6e-04  7.4e+00  1.8e-04 
>> 8.48e+00
>>      6   24 -4.607e+02  4.81e-05  4.37e-05  1.6e-04  2.0e+00  1.8e-04 
>> 1.77e+01
>>      7   30 -4.638e+02  6.60e-03  8.81e-03  9.8e-02  2.0e+00  1.2e-01 
>> 1.84e+01
>>      8   31 -4.645e+02  1.52e-03  7.73e-03  8.2e-02  1.3e+00  1.2e-01 
>> 1.39e-02
>>      9   33 -4.688e+02  9.18e-03  6.28e-03  6.8e-02  0.0e+00  1.2e-01 
>> 6.94e-03
>>     10   35 -4.693e+02  9.32e-04  9.33e-04  8.9e-03  1.9e+00  1.8e-02 
>> 2.86e-02
>>     11   37 -4.699e+02  1.34e-03  1.59e-03  1.6e-02  1.8e+00  3.5e-02 
>> 5.99e-02
>>     12   38 -4.704e+02  1.05e-03  1.43e-03  1.6e-02  1.6e+00  3.5e-02 
>> 9.10e-03
>>     13   40 -4.705e+02  1.84e-04  2.85e-04  5.3e-03  1.2e+00  1.3e-02 
>> 7.52e-04
>>     14   42 -4.705e+02  3.71e-05  5.18e-05  2.4e-03  8.1e-01  5.0e-03 
>> 7.09e-05
>>     15   44 -4.705e+02  8.51e-07  3.04e-06  4.9e-04  8.2e-01  9.5e-04 
>> 5.29e-06
>>     16   57 -4.705e+02 -7.73e-15  1.09e-15  5.0e-15  4.4e+06  9.1e-15 
>> 2.87e-07
>>
>>  ***** FALSE CONVERGENCE *****
>>
>>  FUNCTION    -4.704848e+02   RELDX        4.961e-15
>>  FUNC. EVALS      57         GRAD. EVALS      16
>>  PRELDF       1.088e-15      NPRELDF      2.867e-07
>>
>>      I      FINAL X(I)        D(I)          G(I)
>>
>>      1    2.824235e-05     1.000e+00     5.619e+01
>>      2    8.649332e-02     1.000e+00    -5.899e-01
>>      3    9.175397e-01     1.000e+00    -6.866e-01
>>
>>
>> Call:
>> garch(x = Delt(dat)[-1], order = c(1, 1))
>>
>> Coefficient(s):
>>        a0         a1         b1  
>> 2.824e-05  8.649e-02  9.175e-01  
>>
>> Warning message:
>> In sqrt(pred$e) : NaNs produced
>>
>> What we see that sum of alpha and beta coef is more than 1. Therefore
>> probably I choose a wrong model on my dataset. Can anyone please guide me
>> how to modify that model?
>>
>> Regards,
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Garch-problem-tp22556251p22573080.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From patrick at burns-stat.com  Wed Mar 18 10:19:20 2009
From: patrick at burns-stat.com (Patrick Burns)
Date: Wed, 18 Mar 2009 09:19:20 +0000
Subject: [R-SIG-Finance] [R-sig-finance] Garch problem
In-Reply-To: <22573080.post@talk.nabble.com>
References: <22556251.post@talk.nabble.com> <49BFD7EA.8080205@burns-stat.com>
	<22573080.post@talk.nabble.com>
Message-ID: <49C0BC98.4040906@burns-stat.com>

I was hoping to leave the "Doubtless" [1] as
an exercise for the reader -- mainly as I'm not
at all well versed in what is available in R for
garch these days.

One idea would be to try a components model
(may not be available).

Another idea would be to try a Bayesian estimate
(may not be available).

A method that certainly is available is to pick a
"reasonable" set of parameters (no estimation).

The course of action may well depend on the use
to which the model is to be put.

[1] Stephen Crane "The Wayfarer"

Pat


RON70 wrote:
> Dear Patrick, thank you so much for this reply. You said one solution is to
> increase the data point. However at this point I can not get more. Therefore
> if you please tell more about "doubtless other paths" I will be truly
> grateful.
>
> Regards,
>
>
> Patrick Burns-2 wrote:
>   
>> The fit is essentially saying that the half-life
>> of a shock is infinite.  This generally occurs
>> when the in-sample volatility has a general
>> trend.  One solution is more data.  There are
>> doubtless other paths as well.
>>
>> RON70 wrote:
>>     
>>> I have following dataset as monthly percentage return for a stock :
>>>
>>> 0.173741362
>>> -0.062237174
>>>
>>>   
>>>       
>> [ ... ]
>>     
>>> -0.001652893
>>> -0.092301325
>>>
>>> Now I fit a GARCH (1,1) model on that :
>>>
>>>   
>>>       
>>>> garch(Delt(dat)[-1], c(1,1))
>>>>     
>>>>         
>>>  ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 
>>>
>>>
>>>      I     INITIAL X(I)        D(I)
>>>
>>>      1     4.331103e-03     1.000e+00
>>>      2     5.000000e-02     1.000e+00
>>>      3     5.000000e-02     1.000e+00
>>>
>>>     IT   NF      F         RELDF    PRELDF    RELDX   STPPAR   D*STEP  
>>> NPRELDF
>>>      0    1 -4.507e+02
>>>      1    6 -4.508e+02  2.00e-04  3.20e-04  1.5e-03  6.3e+06  1.5e-04 
>>> 1.01e+03
>>>      2    7 -4.508e+02  1.57e-05  1.69e-05  1.4e-03  2.0e+00  1.5e-04 
>>> 3.19e-01
>>>      3   13 -4.521e+02  2.85e-03  4.72e-03  5.6e-01  2.0e+00  1.3e-01 
>>> 3.16e-01
>>>      4   16 -4.602e+02  1.76e-02  4.41e-03  8.1e-01  6.7e-01  5.1e-01 
>>> 1.99e-02
>>>      5   23 -4.607e+02  1.13e-03  2.77e-03  1.6e-04  7.4e+00  1.8e-04 
>>> 8.48e+00
>>>      6   24 -4.607e+02  4.81e-05  4.37e-05  1.6e-04  2.0e+00  1.8e-04 
>>> 1.77e+01
>>>      7   30 -4.638e+02  6.60e-03  8.81e-03  9.8e-02  2.0e+00  1.2e-01 
>>> 1.84e+01
>>>      8   31 -4.645e+02  1.52e-03  7.73e-03  8.2e-02  1.3e+00  1.2e-01 
>>> 1.39e-02
>>>      9   33 -4.688e+02  9.18e-03  6.28e-03  6.8e-02  0.0e+00  1.2e-01 
>>> 6.94e-03
>>>     10   35 -4.693e+02  9.32e-04  9.33e-04  8.9e-03  1.9e+00  1.8e-02 
>>> 2.86e-02
>>>     11   37 -4.699e+02  1.34e-03  1.59e-03  1.6e-02  1.8e+00  3.5e-02 
>>> 5.99e-02
>>>     12   38 -4.704e+02  1.05e-03  1.43e-03  1.6e-02  1.6e+00  3.5e-02 
>>> 9.10e-03
>>>     13   40 -4.705e+02  1.84e-04  2.85e-04  5.3e-03  1.2e+00  1.3e-02 
>>> 7.52e-04
>>>     14   42 -4.705e+02  3.71e-05  5.18e-05  2.4e-03  8.1e-01  5.0e-03 
>>> 7.09e-05
>>>     15   44 -4.705e+02  8.51e-07  3.04e-06  4.9e-04  8.2e-01  9.5e-04 
>>> 5.29e-06
>>>     16   57 -4.705e+02 -7.73e-15  1.09e-15  5.0e-15  4.4e+06  9.1e-15 
>>> 2.87e-07
>>>
>>>  ***** FALSE CONVERGENCE *****
>>>
>>>  FUNCTION    -4.704848e+02   RELDX        4.961e-15
>>>  FUNC. EVALS      57         GRAD. EVALS      16
>>>  PRELDF       1.088e-15      NPRELDF      2.867e-07
>>>
>>>      I      FINAL X(I)        D(I)          G(I)
>>>
>>>      1    2.824235e-05     1.000e+00     5.619e+01
>>>      2    8.649332e-02     1.000e+00    -5.899e-01
>>>      3    9.175397e-01     1.000e+00    -6.866e-01
>>>
>>>
>>> Call:
>>> garch(x = Delt(dat)[-1], order = c(1, 1))
>>>
>>> Coefficient(s):
>>>        a0         a1         b1  
>>> 2.824e-05  8.649e-02  9.175e-01  
>>>
>>> Warning message:
>>> In sqrt(pred$e) : NaNs produced
>>>
>>> What we see that sum of alpha and beta coef is more than 1. Therefore
>>> probably I choose a wrong model on my dataset. Can anyone please guide me
>>> how to modify that model?
>>>
>>> Regards,
>>>
>>>       
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>>     
>
>


From alexios at 4dscape.com  Wed Mar 18 10:42:26 2009
From: alexios at 4dscape.com (alexios)
Date: Wed, 18 Mar 2009 09:42:26 +0000
Subject: [R-SIG-Finance] [R-sig-finance] Garch problem
In-Reply-To: <49C0BC98.4040906@burns-stat.com>
References: <22556251.post@talk.nabble.com>
	<49BFD7EA.8080205@burns-stat.com>	<22573080.post@talk.nabble.com>
	<49C0BC98.4040906@burns-stat.com>
Message-ID: <49C0C202.7060206@4dscape.com>

You could try to use a different distribution. For example, and using 
the rgarch package from r-forge:

spec=ugarchspec(variance.model=list(model="sGARCH", 
garchOrder=c(1,1)),mean.model=list(armaOrder=c(0,0),
include.mean=TRUE),distribution.model="std")

fit=ugarchfit(data, spec, solver="nlminb", control=list(trace=1))

This seems to converge, though with such little data I would be cautious 
in making predictions/inferences from the model.

-Alexios

Patrick Burns wrote:
> I was hoping to leave the "Doubtless" [1] as
> an exercise for the reader -- mainly as I'm not
> at all well versed in what is available in R for
> garch these days.
> 
> One idea would be to try a components model
> (may not be available).
> 
> Another idea would be to try a Bayesian estimate
> (may not be available).
> 
> A method that certainly is available is to pick a
> "reasonable" set of parameters (no estimation).
> 
> The course of action may well depend on the use
> to which the model is to be put.
> 
> [1] Stephen Crane "The Wayfarer"
> 
> Pat
> 
> 
> RON70 wrote:
>> Dear Patrick, thank you so much for this reply. You said one solution 
>> is to
>> increase the data point. However at this point I can not get more. 
>> Therefore
>> if you please tell more about "doubtless other paths" I will be truly
>> grateful.
>>
>> Regards,
>>
>>
>> Patrick Burns-2 wrote:
>>  
>>> The fit is essentially saying that the half-life
>>> of a shock is infinite.  This generally occurs
>>> when the in-sample volatility has a general
>>> trend.  One solution is more data.  There are
>>> doubtless other paths as well.
>>>
>>> RON70 wrote:
>>>    
>>>> I have following dataset as monthly percentage return for a stock :
>>>>
>>>> 0.173741362
>>>> -0.062237174
>>>>
>>>>         
>>> [ ... ]
>>>    
>>>> -0.001652893
>>>> -0.092301325
>>>>
>>>> Now I fit a GARCH (1,1) model on that :
>>>>
>>>>        
>>>>> garch(Delt(dat)[-1], c(1,1))
>>>>>             
>>>>  ***** ESTIMATION WITH ANALYTICAL GRADIENT *****
>>>>
>>>>      I     INITIAL X(I)        D(I)
>>>>
>>>>      1     4.331103e-03     1.000e+00
>>>>      2     5.000000e-02     1.000e+00
>>>>      3     5.000000e-02     1.000e+00
>>>>
>>>>     IT   NF      F         RELDF    PRELDF    RELDX   STPPAR   
>>>> D*STEP  NPRELDF
>>>>      0    1 -4.507e+02
>>>>      1    6 -4.508e+02  2.00e-04  3.20e-04  1.5e-03  6.3e+06  
>>>> 1.5e-04 1.01e+03
>>>>      2    7 -4.508e+02  1.57e-05  1.69e-05  1.4e-03  2.0e+00  
>>>> 1.5e-04 3.19e-01
>>>>      3   13 -4.521e+02  2.85e-03  4.72e-03  5.6e-01  2.0e+00  
>>>> 1.3e-01 3.16e-01
>>>>      4   16 -4.602e+02  1.76e-02  4.41e-03  8.1e-01  6.7e-01  
>>>> 5.1e-01 1.99e-02
>>>>      5   23 -4.607e+02  1.13e-03  2.77e-03  1.6e-04  7.4e+00  
>>>> 1.8e-04 8.48e+00
>>>>      6   24 -4.607e+02  4.81e-05  4.37e-05  1.6e-04  2.0e+00  
>>>> 1.8e-04 1.77e+01
>>>>      7   30 -4.638e+02  6.60e-03  8.81e-03  9.8e-02  2.0e+00  
>>>> 1.2e-01 1.84e+01
>>>>      8   31 -4.645e+02  1.52e-03  7.73e-03  8.2e-02  1.3e+00  
>>>> 1.2e-01 1.39e-02
>>>>      9   33 -4.688e+02  9.18e-03  6.28e-03  6.8e-02  0.0e+00  
>>>> 1.2e-01 6.94e-03
>>>>     10   35 -4.693e+02  9.32e-04  9.33e-04  8.9e-03  1.9e+00  
>>>> 1.8e-02 2.86e-02
>>>>     11   37 -4.699e+02  1.34e-03  1.59e-03  1.6e-02  1.8e+00  
>>>> 3.5e-02 5.99e-02
>>>>     12   38 -4.704e+02  1.05e-03  1.43e-03  1.6e-02  1.6e+00  
>>>> 3.5e-02 9.10e-03
>>>>     13   40 -4.705e+02  1.84e-04  2.85e-04  5.3e-03  1.2e+00  
>>>> 1.3e-02 7.52e-04
>>>>     14   42 -4.705e+02  3.71e-05  5.18e-05  2.4e-03  8.1e-01  
>>>> 5.0e-03 7.09e-05
>>>>     15   44 -4.705e+02  8.51e-07  3.04e-06  4.9e-04  8.2e-01  
>>>> 9.5e-04 5.29e-06
>>>>     16   57 -4.705e+02 -7.73e-15  1.09e-15  5.0e-15  4.4e+06  
>>>> 9.1e-15 2.87e-07
>>>>
>>>>  ***** FALSE CONVERGENCE *****
>>>>
>>>>  FUNCTION    -4.704848e+02   RELDX        4.961e-15
>>>>  FUNC. EVALS      57         GRAD. EVALS      16
>>>>  PRELDF       1.088e-15      NPRELDF      2.867e-07
>>>>
>>>>      I      FINAL X(I)        D(I)          G(I)
>>>>
>>>>      1    2.824235e-05     1.000e+00     5.619e+01
>>>>      2    8.649332e-02     1.000e+00    -5.899e-01
>>>>      3    9.175397e-01     1.000e+00    -6.866e-01
>>>>
>>>>
>>>> Call:
>>>> garch(x = Delt(dat)[-1], order = c(1, 1))
>>>>
>>>> Coefficient(s):
>>>>        a0         a1         b1  2.824e-05  8.649e-02  9.175e-01 
>>>> Warning message:
>>>> In sqrt(pred$e) : NaNs produced
>>>>
>>>> What we see that sum of alpha and beta coef is more than 1. Therefore
>>>> probably I choose a wrong model on my dataset. Can anyone please 
>>>> guide me
>>>> how to modify that model?
>>>>
>>>> Regards,
>>>>
>>>>       
>>> _______________________________________________
>>> R-SIG-Finance at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>> -- Subscriber-posting only.
>>> -- If you want to post, subscribe first.
>>>
>>>
>>>     
>>
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 
>


From josh at globalherald.net  Wed Mar 18 15:58:20 2009
From: josh at globalherald.net (Joshua Kramer)
Date: Wed, 18 Mar 2009 10:58:20 -0400 (EDT)
Subject: [R-SIG-Finance] New Site using R for Financial Models
Message-ID: <alpine.LFD.2.00.0903181049270.22355@home-av-server.home-av>


Hello,

I apologize ahead of time if this is not the correct venue for this.  I 
would like to introduce a site I have created for financial managers to 
assist their clients in managing relationships with their banks.  The site 
uses R to run financial models on bank data that is publicly available in 
the U.S.

The site is currently in beta.  It might return an error... I am using 
rPy, and for some reason, in long-running rPy processes the Unix socket 
between Python and R gets stale and throws an exception.

Please tell me what you think:

http://www.bankhealthmonitor.com

Cheers,
-Joshua Kramer

-- 

-----
http://www.globalherald.net/jb01
GlobalHerald.NET, the Smarter Social Network! (tm)


From chd850 at gmail.com  Wed Mar 18 21:28:04 2009
From: chd850 at gmail.com (CHD850)
Date: Wed, 18 Mar 2009 13:28:04 -0700 (PDT)
Subject: [R-SIG-Finance] Re[R-sig-finance] lative Date Question
In-Reply-To: <e8e755250808280705g60ec8fe2wac7c06197eab9a6a@mail.gmail.com>
References: <F2F0B4F4D798D1499DC8D3AFE36E1D4472FC4D@mbzajhb02.za.corp.investec.com>
	<e8e755250808280705g60ec8fe2wac7c06197eab9a6a@mail.gmail.com>
Message-ID: <22587804.post@talk.nabble.com>


Hi Jeff, 

Did you notice the difference in Feb 2000, as in bold? Should we expect to
see that from using lastof() ? In the function the year was divided by 400.
I wonder if that has something to do with this. 

Thanks,
Stanley


Jeff Ryan wrote:
> 
> using xts:
> 
>> library(xts)
> 
>> timeBasedSeq('2000-02/2001-01',"Date")-1
>  [1] "2000-01-31" "2000-02-29" "2000-03-31" "2000-04-30" "2000-05-31"
>  [6] "2000-06-30" "2000-07-31" "2000-08-31" "2000-09-30" "2000-10-31"
> [11] "2000-11-30" "2000-12-31"
> 
>> Sys.setenv(TZ='GMT')   # watch for TZ issues...
>> as.Date(lastof(2000,1:12))
>  [1] "2000-01-31" "2000-02-28" "2000-03-31" "2000-04-30" "2000-05-31"
>  [6] "2000-06-30" "2000-07-31" "2000-08-31" "2000-09-30" "2000-10-31"
> [11] "2000-11-30" "2000-12-31"
> 
> 
> 
> HTH,
> Jeff
> 
-- 
View this message in context: http://www.nabble.com/Relative-Date-Question-tp19196979p22587804.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Wed Mar 18 22:27:03 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 18 Mar 2009 16:27:03 -0500
Subject: [R-SIG-Finance] Re[R-sig-finance] lative Date Question
In-Reply-To: <22587804.post@talk.nabble.com>
References: <F2F0B4F4D798D1499DC8D3AFE36E1D4472FC4D@mbzajhb02.za.corp.investec.com>
	<e8e755250808280705g60ec8fe2wac7c06197eab9a6a@mail.gmail.com>
	<22587804.post@talk.nabble.com>
Message-ID: <e8e755250903181427n501f1a57g25f91adff2873221@mail.gmail.com>

Sandy,

Somewhere in the quest to vectorize I must have left off some of the
leap year logic.

Thanks for catching this.

I will add to the next xts, and once I pass R CMD check will get up to CRAN.

For now:

lastof <-
function (year = 1970, month = 12, day = 31, hour = 23, min = 59, sec
= 59, tz = "")
{
    mon.lengths <- c(31, 28, 31, 30, 31, 30, 31, 31, 30, 31,
        30, 31)
    if (missing(day)) {
        day <- ifelse(month %in% 2, ifelse(((year%%4 %in% 0 &
            !year%%100 %in% 0) | (year%%400 %in% 0)), 29, 28),
            mon.lengths[month])
    }
    if (length(c(year, month, day, hour, min, sec)) == 7 && c(year,
        month, day, hour, min, sec) == c(1969, 12, 31, 23, 59,
        59) && Sys.getenv("TZ") %in% c("", "GMT", "UTC"))
        sec <- 58.9
    ISOdatetime(year, month, day, hour, min, sec, tz)
}

> lastof(2000,1:12)
 [1] "2000-01-31 23:59:59 GMT" "2000-02-29 23:59:59 GMT"
 [3] "2000-03-31 23:59:59 GMT" "2000-04-30 23:59:59 GMT"
 [5] "2000-05-31 23:59:59 GMT" "2000-06-30 23:59:59 GMT"
 [7] "2000-07-31 23:59:59 GMT" "2000-08-31 23:59:59 GMT"
 [9] "2000-09-30 23:59:59 GMT" "2000-10-31 23:59:59 GMT"
[11] "2000-11-30 23:59:59 GMT" "2000-12-31 23:59:59 GMT"
> lastof(2004,1:12)
 [1] "2004-01-31 23:59:59 GMT" "2004-02-29 23:59:59 GMT"
 [3] "2004-03-31 23:59:59 GMT" "2004-04-30 23:59:59 GMT"
 [5] "2004-05-31 23:59:59 GMT" "2004-06-30 23:59:59 GMT"
 [7] "2004-07-31 23:59:59 GMT" "2004-08-31 23:59:59 GMT"
 [9] "2004-09-30 23:59:59 GMT" "2004-10-31 23:59:59 GMT"
[11] "2004-11-30 23:59:59 GMT" "2004-12-31 23:59:59 GMT"
> lastof(2003,1:12)
 [1] "2003-01-31 23:59:59 GMT" "2003-02-28 23:59:59 GMT"
 [3] "2003-03-31 23:59:59 GMT" "2003-04-30 23:59:59 GMT"
 [5] "2003-05-31 23:59:59 GMT" "2003-06-30 23:59:59 GMT"
 [7] "2003-07-31 23:59:59 GMT" "2003-08-31 23:59:59 GMT"
 [9] "2003-09-30 23:59:59 GMT" "2003-10-31 23:59:59 GMT"
[11] "2003-11-30 23:59:59 GMT" "2003-12-31 23:59:59 GMT"
> lastof(1900,1:12)
 [1] "1900-01-31 23:59:59 GMT" "1900-02-28 23:59:59 GMT"
 [3] "1900-03-31 23:59:59 GMT" "1900-04-30 23:59:59 GMT"
 [5] "1900-05-31 23:59:59 GMT" "1900-06-30 23:59:59 GMT"
 [7] "1900-07-31 23:59:59 GMT" "1900-08-31 23:59:59 GMT"
 [9] "1900-09-30 23:59:59 GMT" "1900-10-31 23:59:59 GMT"
[11] "1900-11-30 23:59:59 GMT" "1900-12-31 23:59:59 GMT"


R/Finance 2009
April 24 & 25, 2009 Chicago, IL, USA
http://www.RinFinance.com

EARLY REGISTRATION ENDS MARCH 31




-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From jorge.nieves at moorecap.com  Thu Mar 19 18:53:26 2009
From: jorge.nieves at moorecap.com (Jorge Nieves)
Date: Thu, 19 Mar 2009 13:53:26 -0400
Subject: [R-SIG-Finance] R2HTML
Message-ID: <D595C0E05185614C90515F1E8A2D4CBF02B3BFAB@NYC-XCH3.win.moorecap.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090319/d9f36297/attachment.pl>

From Reena.Bansal at moorecap.com  Thu Mar 19 20:34:36 2009
From: Reena.Bansal at moorecap.com (Reena Bansal)
Date: Thu, 19 Mar 2009 15:34:36 -0400
Subject: [R-SIG-Finance] Trimmed L Moments
Message-ID: <4AAD56F399C8564C9EB6817C17618CDD0251E8C8@NYC-XCH3.win.moorecap.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090319/316459a9/attachment.pl>

From jeff.a.ryan at gmail.com  Thu Mar 19 21:03:45 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 19 Mar 2009 15:03:45 -0500
Subject: [R-SIG-Finance] R2HTML
In-Reply-To: <D595C0E05185614C90515F1E8A2D4CBF02B3BFAB@NYC-XCH3.win.moorecap.com>
References: <D595C0E05185614C90515F1E8A2D4CBF02B3BFAB@NYC-XCH3.win.moorecap.com>
Message-ID: <e8e755250903191303l4d2ca952tcd2f4fc49cc0daf@mail.gmail.com>

Hi Jorge,

You really need to have this redirected to R-help.  Or possibly even a
CSS/web mailing list.

Thanks,
Jeff



On Thu, Mar 19, 2009 at 12:53 PM, Jorge Nieves
<jorge.nieves at moorecap.com> wrote:
> I am trying to apply CSS commands to finish formatting a data frame in
> HTML. I am half way. I would like to format the first column (row
> names). I would like to make the fonts darker and use a background
> color.
>
> I also would like to add different background color to columns with
> names "var1" and "var2".
>
> Does anyone have any ideas?
>
>
>
>
> Hre is the R2HTML code:
>
> ? ? ?mytable = as.data.frame(x$tables$t1)
> ? ? ?HTMLCSS(file = file, CSSfile = CSSfile)
> ? ? ? ?HTML(mytable, file = file,
> ? ? ? ? ? ? ? ? ? # align="center",
> ? ? ? ? ? ? ? ? ? ?#innerBorder=2,
> ? ? ? ? ? ? ? ? ? ?#digits =2,
> ? ? ? ? ? ? ? ? ? # Border = 1,
> ? ? ? ? ? ? ? ? ? ?row.names =TRUE,
> ? ? ? ? ? ? ? ? ? ?#classfirstline = "firstline",
> ? ? ? ? ? ? ? ? ? ?#classfirstcolumn = "firstcolumn",
> ? ? ? ? ? ? ? ? ? ?#BackGroundColor="#BBBBEE")
>
>
>
>
> Here is the CSS code so far
>
> TH { text-align: center;
> ? ? ?color:"white";
> ? ? font-weight: bold;
> ? ? background: yellow;
> ? ? vertical-align: baseline;
> ? ? background: "black";
>
> }
>
>
>
> .centered-table {
> ? margin-left: auto;
> ? margin-right: auto;
> ? text-align: center;
> ? background: #FFFFFF;
> }
>
>
>
> TD.cellinside {
> ? ? ? ?text-align: center;
> ? ? ? ?padding: 5 10;
> ? ? ? ?background: "#BBBBEE";
> ? ? ? ?text-align=right
> }
>
> Jorge Nieves
>
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From andyzhu35 at yahoo.com  Thu Mar 19 22:09:31 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Thu, 19 Mar 2009 14:09:31 -0700 (PDT)
Subject: [R-SIG-Finance] quantmod: getFinancials error
Message-ID: <506316.46994.qm@web56207.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090319/64f3f177/attachment.pl>

From jeff.a.ryan at gmail.com  Thu Mar 19 22:23:30 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 19 Mar 2009 16:23:30 -0500
Subject: [R-SIG-Finance] quantmod: getFinancials error
In-Reply-To: <506316.46994.qm@web56207.mail.re3.yahoo.com>
References: <506316.46994.qm@web56207.mail.re3.yahoo.com>
Message-ID: <e8e755250903191423s111726bcu604f55c96d27d704@mail.gmail.com>

Hi Andy,

getFin screen-scrapes google.  It is pretty reliable, but not perfect.

The problem here is that there is no quarterly data on the site for
ANAT, so the function just fails.

see: http://www.google.com/finance?fstype=ci&q=NASDAQ:ANAT

I'll see if I can figure out a solution, but it may be more of a
one-off hack than really a solution.

See what you can come up with to solve this, and please share your successes!

Jeff

On Thu, Mar 19, 2009 at 4:09 PM, Andy Zhu <andyzhu35 at yahoo.com> wrote:
> ANAT is a valid ticker in google finance.
>
> However, the following code generates error in quantmod: getFinancials
>
>> stock='ANAT';
>> getFinancials(Symbol=stock, src='google');
> Error in getFinancials(Symbol = stock, src = "google") :
> ? subscript out of bounds
>
> what is wrong here?
>
> Thanks in advance.
>
>
>
>
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From gunnar.hoyer at uni-oldenburg.de  Thu Mar 19 23:12:15 2009
From: gunnar.hoyer at uni-oldenburg.de (Gunnar Hoyer)
Date: Thu, 19 Mar 2009 23:12:15 +0100
Subject: [R-SIG-Finance] Phase spectrum
Message-ID: <49C2C33F.4020607@uni-oldenburg.de>

Hello,

I have trouble to interpret the phase spectrum correctly.

Here is the code (with different data) I used to get the phase spectrum:

mfdeaths.spc <- spec.pgram(ts.union(mdeaths, fdeaths), spans = c(3,3))
plot(mfdeaths.spc, plot.type = "phase")

I would like to know whether positive or negative values indicate that 
the time series mdeaths leads or lags.

Thanks in advance
Gunnar Hoyer


From andyzhu35 at yahoo.com  Fri Mar 20 00:34:37 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Thu, 19 Mar 2009 16:34:37 -0700 (PDT)
Subject: [R-SIG-Finance] quantmod: getFinancials error
In-Reply-To: <e8e755250903191423s111726bcu604f55c96d27d704@mail.gmail.com>
Message-ID: <380557.69696.qm@web56208.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090319/5dfe9c7d/attachment.pl>

From Andreas.Johansson at mbamfunds.com  Fri Mar 20 16:36:53 2009
From: Andreas.Johansson at mbamfunds.com (Andreas Johansson)
Date: Fri, 20 Mar 2009 15:36:53 +0000
Subject: [R-SIG-Finance] Quantmod - chartSeries
Message-ID: <D4D64717AB295048B0FD52443103D75D08986D3B01@LONMAIL.mbam.local>

Hi,

am using chartSeries to create a set of charts that I output to pdf in a loop. Works fine but have encountered a problem. If I want to loop over a call like

for (i=1:10){
                Data=getSymbols(ID[i])
                chartSeries(Data,TA='addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888' )
}

I cannot do it as chartSeires complains that object Data does not exist

does not complain if I do
for (i=1:10){
                Data=getSymbols(ID[i])
                chartSeries(Data)
                addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888' )
}

Is there a way of making the call to chart series in one go and referencing to Data in the addTA function?

If I run Data=getSymbols(ID[i]) and  then  chartSeries(Data,TA='addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888' ) in the console it works fine.

regards

Andreas

Andreas Johansson
Quantitative Analyst


[cid:image001.jpg at 01C9A970.18997030]
Marble Bar Asset Management LLP
11-12 St James Square
London, SW1Y 4LB
Direct +44 (0) 20 3023 8141
Fax +44 (0) 20 3023 8065
Mobile +44 (0) 7747 725992
Andreas.Johansson at mbamfunds.com


.

This message is intended only for the use of the person(s) to whom it is addressed. It may contain information which is privileged and confidential. Accordingly any unauthorised use is strictly prohibited. If you are not the intended recipient, please contact the sender as soon as possible.

It is not intended as an offer or solicitation for the purchase or sale of any financial instrument or as an official confirmation of any transaction, unless specifically agreed otherwise. All market prices, data and other information are not warranted as to completeness or accuracy and are subject to change without notice. Any opinions or advice contained in this Internet email are subject to the terms and conditions expressed in any applicable governing Marble Bar Asset Management LLP's  terms and conditions of business or client agreement letter. Any comments or statements made herein do not necessarily reflect those of Marble Bar Asset Management LLP.

Marble Bar Asset Management LLP is regulated and authorised by the FSA.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090320/667f46df/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 8581 bytes
Desc: image001.jpg
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090320/667f46df/attachment.jpg>

From jeff.a.ryan at gmail.com  Fri Mar 20 17:08:33 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 20 Mar 2009 11:08:33 -0500
Subject: [R-SIG-Finance] Quantmod - chartSeries
In-Reply-To: <D4D64717AB295048B0FD52443103D75D08986D3B01@LONMAIL.mbam.local>
References: <D4D64717AB295048B0FD52443103D75D08986D3B01@LONMAIL.mbam.local>
Message-ID: <e8e755250903200908w3ba3136akf46b1c23d7f3f238@mail.gmail.com>

Andreas,

Reproducible code would be of great help, as would actual output and a
traceback() call, but without that here is my guess.

The code that draws TA objects onto the chart does a bit of testing to
'see' where the call comes from.  Inside of the chartSeries call, the
code gets evaluated but not drawn until later.  Outside (from the
command line) it simply updates critical parts of the chart in memory,
then redraws the entire thing. The nesting inside is the culprit ---
and to change this is quite difficult.

One possible work-around may be to not wrap in a pdf() call, but
instead use dev.copy2pdf after the chart & TA has been drawn at each
iteration of the loop.


HTH,
Jeff

R/Finance 2009: Applied Finance with R
April 24 and 25th, 2009 Chicago, IL USA
http://www.RinFinance.com

EARLY REGISTRATION ENDS MARCH 31st!
On Fri, Mar 20, 2009 at 10:36 AM, Andreas Johansson
<Andreas.Johansson at mbamfunds.com> wrote:
> Hi,
>
>
>
> am using chartSeries to create a set of charts that I output to pdf in a
> loop. Works fine but have encountered a problem. If I want to loop over a
> call like
>
>
>
> for (i=1:10){
>
> ??????????????? Data=getSymbols(ID[i])
>
>
> chartSeries(Data,TA='addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888'
> )
>
> }
>
>
>
> I cannot do it as chartSeires complains that object Data does not exist
>
>
>
> does not complain if I do
>
> for (i=1:10){
>
> ??????????????? Data=getSymbols(ID[i])
>
> ??????????????? chartSeries(Data)
>
>
> addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888' )
>
> }
>
>
>
> Is there a way of making the call to chart series in one go and referencing
> to Data in the addTA function?
>
>
>
> If I run Data=getSymbols(ID[i]) and? then
> ?chartSeries(Data,TA='addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888'
> ) in the console it works fine.
>
>
>
> regards
>
>
>
> Andreas
>
>
>
> Andreas Johansson
> Quantitative Analyst
>
>
>
> Marble Bar Asset Management LLP
> 11-12 St James Square
> London, SW1Y 4LB
>
> Direct +44 (0) 20 3023 8141
> Fax +44 (0) 20 3023 8065
> Mobile +44 (0) 7747 725992
> Andreas.Johansson at mbamfunds.com
>
>
>
> .
>
> This message is intended only for the use of the person(s) to whom it is
> addressed. It may contain information which is privileged and confidential.
> Accordingly any unauthorised use is strictly prohibited. If you are not the
> intended recipient, please contact the sender as soon as possible.
>
> It is not intended as an offer or solicitation for the purchase or sale of
> any financial instrument or as an official confirmation of any transaction,
> unless specifically agreed otherwise. All market prices, data and other
> information are not warranted as to completeness or accuracy and are subject
> to change without notice. Any opinions or advice contained in this Internet
> email are subject to the terms and conditions expressed in any applicable
> governing Marble Bar Asset Management LLP's terms and conditions of business
> or client agreement letter. Any comments or statements made herein do not
> necessarily reflect those of Marble Bar Asset Management LLP.
>
> Marble Bar Asset Management LLP is regulated and authorised by the FSA.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From jeff.a.ryan at gmail.com  Fri Mar 20 18:56:23 2009
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 20 Mar 2009 12:56:23 -0500
Subject: [R-SIG-Finance] Quantmod - chartSeries
In-Reply-To: <e8e755250903200908w3ba3136akf46b1c23d7f3f238@mail.gmail.com>
References: <D4D64717AB295048B0FD52443103D75D08986D3B01@LONMAIL.mbam.local>
	<e8e755250903200908w3ba3136akf46b1c23d7f3f238@mail.gmail.com>
Message-ID: <e8e755250903201056r4cea78a0u2d7268853c99868c@mail.gmail.com>

Hi Andreas, <cc'ing back the list>

Thanks for the minimal code.  The lookup is tough.  Environments when
embedded in this whole process are very tough to handle correctly.

One possible way to fix this is by using *newTA* to create a function
call that is just like the base functions in quantmod.

Another (easier) way is to break it apart in the loop.

> LoopChart
function(ID){
 for (i in 1:length(ID)){
   Data=getSymbols(ID[i],auto.assign=F)
   print(class(Data))
   print(dimnames(Data))
   chartSeries(Data,TA=NULL)
   plot(addTA(Cl(Data)<3,border=NA,col='#888888',on=-1))  #need plot()
   # dev.copy2pdf(file=ID[i])
 }
}

A second (cooler!) approach is with newTA:

# define the logic
myfun <- function(x) { Cl(x) < 3 }
# create a 'new' TA function
myTA <- newTA(myfun,border=NA,col="#888888",on=-1)

LoopChart <-
function(ID){
 for (i in 1:length(ID)){
   Data=getSymbols(ID[i],auto.assign=F)
   print(class(Data))
   print(dimnames(Data))
   chartSeries(Data,TA='myTA()')
   # dev.copy2pdf(file=ID[i])
 }
}

LoopChart("C")

HTH,
Jeff

******* ORIGINAL MSG ********
Thanks for your quick response,

Will try to rework as you suggested. Appreciate your comment reg
runable code in my example. Cannot fully replicate my function call
because I am using a version of getSymbols that we have linked to our
internal database. Created a similar example that should catch the
same error.

Did a bit more digging and it looks like chartSeries looks in the
global environment and not the frame of the function call when it
searches for the "Data" object. Everything works if I put a object
called Data in the global environment before i.e. run
Data=getSymbols(ID[i],auto.assign=F) in the console before making the
LoopChart("C") function call. Function is below with function call and
traceback. Thanks again /Andreas

LoopChart("C")

LoopChart<-function(ID){
 for (i in 1:length(ID)){
   Data=getSymbols(ID[i],auto.assign=F)
   print(class(Data))
   print(dimnames(Data))
   chartSeries(Data,TA="addTA(Cl(Data)<3,border=NA,col='#888888',on=-1)" )
 }
}

> LoopChart("C")
[1] "xts" "zoo"
[[1]]
NULL

[[2]]
[1] "C.Open"     "C.High"     "C.Low"      "C.Close"    "C.Volume"
"C.Adjusted"

Error in inherits(x, "data.frame") : object "Data" not found
> traceback()
11: inherits(x, "data.frame")
10: is.data.frame(x)
9: colnames(x)
8: grep("Close", colnames(x))
7: has.Cl(x)
6: Cl(Data)
5: addTA(Cl(Data) < 3, border = NA, col = "#888888", on = -1)
4: eval(expr, envir, enclos)
3: eval(parse(text = TA[[ta]]), env = thisEnv)
2: chartSeries(Data, TA = "addTA(Cl(Data)<3,border=NA,col='#888888',on=-1)")
1: LoopChart("C")

On Fri, Mar 20, 2009 at 11:08 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Andreas,
>
> Reproducible code would be of great help, as would actual output and a
> traceback() call, but without that here is my guess.
>
> The code that draws TA objects onto the chart does a bit of testing to
> 'see' where the call comes from. ?Inside of the chartSeries call, the
> code gets evaluated but not drawn until later. ?Outside (from the
> command line) it simply updates critical parts of the chart in memory,
> then redraws the entire thing. The nesting inside is the culprit ---
> and to change this is quite difficult.
>
> One possible work-around may be to not wrap in a pdf() call, but
> instead use dev.copy2pdf after the chart & TA has been drawn at each
> iteration of the loop.
>
>
> HTH,
> Jeff
>
> R/Finance 2009: Applied Finance with R
> April 24 and 25th, 2009 Chicago, IL USA
> http://www.RinFinance.com
>
> EARLY REGISTRATION ENDS MARCH 31st!
> On Fri, Mar 20, 2009 at 10:36 AM, Andreas Johansson
> <Andreas.Johansson at mbamfunds.com> wrote:
>> Hi,
>>
>>
>>
>> am using chartSeries to create a set of charts that I output to pdf in a
>> loop. Works fine but have encountered a problem. If I want to loop over a
>> call like
>>
>>
>>
>> for (i=1:10){
>>
>> ??????????????? Data=getSymbols(ID[i])
>>
>>
>> chartSeries(Data,TA='addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888'
>> )
>>
>> }
>>
>>
>>
>> I cannot do it as chartSeires complains that object Data does not exist
>>
>>
>>
>> does not complain if I do
>>
>> for (i=1:10){
>>
>> ??????????????? Data=getSymbols(ID[i])
>>
>> ??????????????? chartSeries(Data)
>>
>>
>> addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888' )
>>
>> }
>>
>>
>>
>> Is there a way of making the call to chart series in one go and referencing
>> to Data in the addTA function?
>>
>>
>>
>> If I run Data=getSymbols(ID[i]) and? then
>> ?chartSeries(Data,TA='addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888'
>> ) in the console it works fine.
>>
>>
>>
>> regards
>>
>>
>>
>> Andreas
>>
>>
>>
>> Andreas Johansson
>> Quantitative Analyst
>>
>>
>>
>> Marble Bar Asset Management LLP
>> 11-12 St James Square
>> London, SW1Y 4LB
>>
>> Direct +44 (0) 20 3023 8141
>> Fax +44 (0) 20 3023 8065
>> Mobile +44 (0) 7747 725992
>> Andreas.Johansson at mbamfunds.com
>>
>>
>>
>> .
>>
>> This message is intended only for the use of the person(s) to whom it is
>> addressed. It may contain information which is privileged and confidential.
>> Accordingly any unauthorised use is strictly prohibited. If you are not the
>> intended recipient, please contact the sender as soon as possible.
>>
>> It is not intended as an offer or solicitation for the purchase or sale of
>> any financial instrument or as an official confirmation of any transaction,
>> unless specifically agreed otherwise. All market prices, data and other
>> information are not warranted as to completeness or accuracy and are subject
>> to change without notice. Any opinions or advice contained in this Internet
>> email are subject to the terms and conditions expressed in any applicable
>> governing Marble Bar Asset Management LLP's terms and conditions of business
>> or client agreement letter. Any comments or statements made herein do not
>> necessarily reflect those of Marble Bar Asset Management LLP.
>>
>> Marble Bar Asset Management LLP is regulated and authorised by the FSA.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com


From Andreas.Johansson at mbamfunds.com  Mon Mar 23 11:41:27 2009
From: Andreas.Johansson at mbamfunds.com (Andreas Johansson)
Date: Mon, 23 Mar 2009 10:41:27 +0000
Subject: [R-SIG-Finance] Quantmod - chartSeries
In-Reply-To: <e8e755250903201056r4cea78a0u2d7268853c99868c@mail.gmail.com>
References: <D4D64717AB295048B0FD52443103D75D08986D3B01@LONMAIL.mbam.local>
	<e8e755250903200908w3ba3136akf46b1c23d7f3f238@mail.gmail.com>
	<e8e755250903201056r4cea78a0u2d7268853c99868c@mail.gmail.com>
Message-ID: <D4D64717AB295048B0FD52443103D75D08986D3B08@LONMAIL.mbam.local>

Hi Jeff,

that is nice and clean solution. Found a quick fix as well. By temporarily assigning all objects I needed in the chartSeries call to .GlobalEnv everything works. A bit messy since you need to clean up .GlobalEnv. 

thanks

Andreas  

-----Original Message-----
From: Jeff Ryan [mailto:jeff.a.ryan at gmail.com] 
Sent: 20 March 2009 17:56
To: Andreas Johansson
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Quantmod - chartSeries

Hi Andreas, <cc'ing back the list>

Thanks for the minimal code.  The lookup is tough.  Environments when
embedded in this whole process are very tough to handle correctly.

One possible way to fix this is by using *newTA* to create a function
call that is just like the base functions in quantmod.

Another (easier) way is to break it apart in the loop.

> LoopChart
function(ID){
 for (i in 1:length(ID)){
   Data=getSymbols(ID[i],auto.assign=F)
   print(class(Data))
   print(dimnames(Data))
   chartSeries(Data,TA=NULL)
   plot(addTA(Cl(Data)<3,border=NA,col='#888888',on=-1))  #need plot()
   # dev.copy2pdf(file=ID[i])
 }
}

A second (cooler!) approach is with newTA:

# define the logic
myfun <- function(x) { Cl(x) < 3 }
# create a 'new' TA function
myTA <- newTA(myfun,border=NA,col="#888888",on=-1)

LoopChart <-
function(ID){
 for (i in 1:length(ID)){
   Data=getSymbols(ID[i],auto.assign=F)
   print(class(Data))
   print(dimnames(Data))
   chartSeries(Data,TA='myTA()')
   # dev.copy2pdf(file=ID[i])
 }
}

LoopChart("C")

HTH,
Jeff

******* ORIGINAL MSG ********
Thanks for your quick response,

Will try to rework as you suggested. Appreciate your comment reg
runable code in my example. Cannot fully replicate my function call
because I am using a version of getSymbols that we have linked to our
internal database. Created a similar example that should catch the
same error.

Did a bit more digging and it looks like chartSeries looks in the
global environment and not the frame of the function call when it
searches for the "Data" object. Everything works if I put a object
called Data in the global environment before i.e. run
Data=getSymbols(ID[i],auto.assign=F) in the console before making the
LoopChart("C") function call. Function is below with function call and
traceback. Thanks again /Andreas

LoopChart("C")

LoopChart<-function(ID){
 for (i in 1:length(ID)){
   Data=getSymbols(ID[i],auto.assign=F)
   print(class(Data))
   print(dimnames(Data))
   chartSeries(Data,TA="addTA(Cl(Data)<3,border=NA,col='#888888',on=-1)" )
 }
}

> LoopChart("C")
[1] "xts" "zoo"
[[1]]
NULL

[[2]]
[1] "C.Open"     "C.High"     "C.Low"      "C.Close"    "C.Volume"
"C.Adjusted"

Error in inherits(x, "data.frame") : object "Data" not found
> traceback()
11: inherits(x, "data.frame")
10: is.data.frame(x)
9: colnames(x)
8: grep("Close", colnames(x))
7: has.Cl(x)
6: Cl(Data)
5: addTA(Cl(Data) < 3, border = NA, col = "#888888", on = -1)
4: eval(expr, envir, enclos)
3: eval(parse(text = TA[[ta]]), env = thisEnv)
2: chartSeries(Data, TA = "addTA(Cl(Data)<3,border=NA,col='#888888',on=-1)")
1: LoopChart("C")

On Fri, Mar 20, 2009 at 11:08 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Andreas,
>
> Reproducible code would be of great help, as would actual output and a
> traceback() call, but without that here is my guess.
>
> The code that draws TA objects onto the chart does a bit of testing to
> 'see' where the call comes from. ?Inside of the chartSeries call, the
> code gets evaluated but not drawn until later. ?Outside (from the
> command line) it simply updates critical parts of the chart in memory,
> then redraws the entire thing. The nesting inside is the culprit ---
> and to change this is quite difficult.
>
> One possible work-around may be to not wrap in a pdf() call, but
> instead use dev.copy2pdf after the chart & TA has been drawn at each
> iteration of the loop.
>
>
> HTH,
> Jeff
>
> R/Finance 2009: Applied Finance with R
> April 24 and 25th, 2009 Chicago, IL USA
> http://www.RinFinance.com
>
> EARLY REGISTRATION ENDS MARCH 31st!
> On Fri, Mar 20, 2009 at 10:36 AM, Andreas Johansson
> <Andreas.Johansson at mbamfunds.com> wrote:
>> Hi,
>>
>>
>>
>> am using chartSeries to create a set of charts that I output to pdf in a
>> loop. Works fine but have encountered a problem. If I want to loop over a
>> call like
>>
>>
>>
>> for (i=1:10){
>>
>> ??????????????? Data=getSymbols(ID[i])
>>
>>
>> chartSeries(Data,TA='addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888'
>> )
>>
>> }
>>
>>
>>
>> I cannot do it as chartSeires complains that object Data does not exist
>>
>>
>>
>> does not complain if I do
>>
>> for (i=1:10){
>>
>> ??????????????? Data=getSymbols(ID[i])
>>
>> ??????????????? chartSeries(Data)
>>
>>
>> addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888' )
>>
>> }
>>
>>
>>
>> Is there a way of making the call to chart series in one go and referencing
>> to Data in the addTA function?
>>
>>
>>
>> If I run Data=getSymbols(ID[i]) and? then
>> ?chartSeries(Data,TA='addTA(as.Date(index(Data))<as.Date("2009-02-02))',border=NA,col='#888888'
>> ) in the console it works fine.
>>
>>
>>
>> regards
>>
>>
>>
>> Andreas
>>
>>
>>
>> Andreas Johansson
>> Quantitative Analyst
>>
>>
>>
>> Marble Bar Asset Management LLP
>> 11-12 St James Square
>> London, SW1Y 4LB
>>
>> Direct +44 (0) 20 3023 8141
>> Fax +44 (0) 20 3023 8065
>> Mobile +44 (0) 7747 725992
>> Andreas.Johansson at mbamfunds.com
>>
>>
>>
>> .
>>
>> This message is intended only for the use of the person(s) to whom it is
>> addressed. It may contain information which is privileged and confidential.
>> Accordingly any unauthorised use is strictly prohibited. If you are not the
>> intended recipient, please contact the sender as soon as possible.
>>
>> It is not intended as an offer or solicitation for the purchase or sale of
>> any financial instrument or as an official confirmation of any transaction,
>> unless specifically agreed otherwise. All market prices, data and other
>> information are not warranted as to completeness or accuracy and are subject
>> to change without notice. Any opinions or advice contained in this Internet
>> email are subject to the terms and conditions expressed in any applicable
>> governing Marble Bar Asset Management LLP's terms and conditions of business
>> or client agreement letter. Any comments or statements made herein do not
>> necessarily reflect those of Marble Bar Asset Management LLP.
>>
>> Marble Bar Asset Management LLP is regulated and authorised by the FSA.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>
>
>
> --
> Jeffrey Ryan
> jeffrey.ryan at insightalgo.com
>
> ia: insight algorithmics
> www.insightalgo.com
>



-- 
Jeffrey Ryan
jeffrey.ryan at insightalgo.com

ia: insight algorithmics
www.insightalgo.com

.

This message is intended only for the use of the person(s) to whom it is addressed. It may contain information which is privileged and confidential. Accordingly any unauthorised use is strictly prohibited. If you are not the intended recipient, please contact the sender as soon as possible.

It is not intended as an offer or solicitation for the purchase or sale of any financial instrument or as an official confirmation of any transaction, unless specifically agreed otherwise. All market prices, data and other information are not warranted as to completeness or accuracy and are subject to change without notice. Any opinions or advice contained in this Internet email are subject to the terms and conditions expressed in any applicable governing Marble Bar Asset Management LLP's  terms and conditions of business or client agreement letter. Any comments or statements made herein do not necessarily reflect those of Marble Bar Asset Management LLP.

Marble Bar Asset Management LLP is regulated and authorised by the FSA.


From aliona.mn at gmail.com  Wed Mar 25 20:45:16 2009
From: aliona.mn at gmail.com (PitaBread)
Date: Wed, 25 Mar 2009 12:45:16 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Shaded regions as an indicator
	in quantmod
In-Reply-To: <etPan.492ed69b.331e07dc.1148@localhost>
References: <etPan.492ed69b.331e07dc.1148@localhost>
Message-ID: <22709315.post@talk.nabble.com>


Hello.
I have got a related question.

When I generate Trades signals of either -1s or 1s, and do 
   > plot(addTA(Trades > 0, border = NA, col = 'green', on = -1))
although it plots correctly, it seems to be plotting starting the next day,
and ending between the last and previous-to-last day. 
As a result, if i add: 
   > plot(addTA(Trades < 0, border = NA, col = 'red', on = -1))
it should really have plotted all the region (since i don't have any 0's in
Trades). Instead, it has spaces, not filled by either 'red' or 'green'...
Another result of that is that if i have a one-day Trade, i.e. 
   > Trades[23:27]
       [1] 1 1 -1 1 1
instead of a red rectangle i get a gray line when on=-1 and absolutely
nothing when on=-2. FYI: my x-range is 2 months, so it is not a zoom
problem.

http://www.nabble.com/file/p22709315/DUG-1.jpg 

Aliona


Brian Lee Yung Rowe wrote:
> 
> 
> Hi, 
> 
> I want to produce charts using quantmod similar to the ones available from
> FRED. Specifically, FRED has this handy feature of rendering the
> NBER-defined recession periods under the displayed time series. 
> 
> It seems like calling newTA with the appropriate function handle could
> produce the output necessary to define the shading bars, but I'm not sure
> if any of the existing charting types can generate the correct look and
> feel. Anybody have suggestions? 
> 
> Thanks, 
> Brian
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
> 

-- 
View this message in context: http://www.nabble.com/Shaded-regions-as-an-indicator-in-quantmod-tp20722874p22709315.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From HodgessE at uhd.edu  Wed Mar 25 21:54:26 2009
From: HodgessE at uhd.edu (Hodgess, Erin)
Date: Wed, 25 Mar 2009 15:54:26 -0500
Subject: [R-SIG-Finance] commodity prices
Message-ID: <70A5AC06FDB5E54482D19E1C04CDFCF3084DC2C2@BALI.uhd.campus>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090325/197fbee8/attachment.pl>

From Bernd.Stampfl at Sparinvest.com  Thu Mar 26 10:12:37 2009
From: Bernd.Stampfl at Sparinvest.com (Stampfl Bernd 0969 SPI)
Date: Thu, 26 Mar 2009 10:12:37 +0100
Subject: [R-SIG-Finance] commodity prices
In-Reply-To: <70A5AC06FDB5E54482D19E1C04CDFCF3084DC2C2@BALI.uhd.campus>
References: <70A5AC06FDB5E54482D19E1C04CDFCF3084DC2C2@BALI.uhd.campus>
Message-ID: <EF39FEEAA8A0A04A892894BFB5ED780C4C15BA4512@M0164.s-mxs.net>

try IMF: http://imf.org/external/np/res/commod/index.asp

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Hodgess, Erin
Gesendet: 25 March 2009 21:54
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] commodity prices [bayes][heur]

Dear R Finance People:

 

I posted this to the regular list but thought that I would try here as
well.

 

Does anyone know of a website for historical commodity prices, please?

 

Thanks,

Erin

 

 

Erin M. Hodgess, Ph.D.

Associate Professor

Department of Computer and Mathematical Sciences

University of Houston - Downtown

mailto: hodgesse at uhd.edu

 

 


	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From edd at debian.org  Thu Mar 26 15:07:46 2009
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 26 Mar 2009 09:07:46 -0500
Subject: [R-SIG-Finance] R/Finance 2009: Applied Finance with R --
	Registration discount window closing
Message-ID: <18891.35890.541104.855601@ron.nulle.part>


    Quick reminder that early-bird pricing ends April 1, 2009. Conference
    details, including program details and registration information are at
    	http://www.RinFinance.com
    Please feel free to send any questions about the conference to 
        committee at rinfinance.com


    R/Finance 2009: Applied Finance with R
    April 24 & 25, Chicago, IL, US

    The first annual R/Finance conference for applied finance using R, the
    premier free software system for statistical computation and graphics,
    will be held this spring in Chicago, IL, USA on Friday April 24 and
    Saturday April 25.

    The two-day conference will cover topics as diverse as portfolio theory,
    time-series analysis, as well as advanced risk tools, high-performance
    computing, and econometrics. All will be discussed within the context of
    using R as a primary tool for financial risk management and trading.

    Assembled to talk over the two days are some of the industry's most
    recognizable authorities within the world of R and quantitative finance.

    R/Finance 2009 is organized by a leading group of R package authors and
    community contributors, and hosted by the International Center for Futures
    and Derivatives [ICFD] at the University of Illinois at Chicago.

    Conference registration is now open. Special advanced registration
    pricing is available, as well as discounted pricing for academic 
    and student registrations.

    EARLY-BIRD PRICING ENDS APRIL 1, 2009

    More details and registration information can be found at the web site at

               http://www.RinFinance.com

    For the program committee:

         Gib Bassett, Peter Carl, Dirk Eddelbuettel, John Miller,
         Brian Peterson, Dale Rosenthal, Jeffrey Ryan

    Sponsors:
         International Center for Futures and Derivatives at UIC
         University of Illinois at Chicago
         REvolution Computing
         Microsoft Windows HPC
         ia: insight algorithmics

-- 
Three out of two people have difficulties with fractions.


From B_Rowe at ml.com  Thu Mar 26 19:47:33 2009
From: B_Rowe at ml.com (Rowe, Brian Lee Yung (Portfolio Analytics))
Date: Thu, 26 Mar 2009 14:47:33 -0400
Subject: [R-SIG-Finance] Elegant bootstrapping with zoo
Message-ID: <3BAD818D9407B043817CC6D89ABA14EC032B83EC@MLNYC20MB051.amrs.win.ml.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090326/6e4e0d3e/attachment.pl>

From enricoschumann at yahoo.de  Thu Mar 26 20:33:47 2009
From: enricoschumann at yahoo.de (Enrico Schumann)
Date: Thu, 26 Mar 2009 20:33:47 +0100
Subject: [R-SIG-Finance] Elegant bootstrapping with zoo
In-Reply-To: <3BAD818D9407B043817CC6D89ABA14EC032B83EC@MLNYC20MB051.amrs.win.ml.com>
Message-ID: <34791.36086.bm@omp209.mail.ukl.yahoo.com>

hi brian,

one line of code, seems elegant to me ;) i guess sp500.subset has several
columns? i am not sure whether `sample' can be made to work row-wise on a
matrix, but you could write your own function.

require(zoo)

## create some artifical returns data
z <- rnorm(200) * 0.05
dim(z) <- c(100, 2)
z <- zoo(z, 1:100)

## function (x = matrix with time series in columns, nR = number of
replications)
bsRows <- function(x, nR){
	z  <- as.matrix(x)
	bs <- sample(seq(1,nrow(z)), nR, replace = TRUE)
	z  <- z[bs,]
	z  <- zoo(z, index(x)[bs])
	return(z)
}

## try
bsRows(z, 10)

(you may get warnings for repeated dates)
regards,
enrico

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Rowe, Brian
Lee Yung (Portfolio Analytics)
Gesendet: Donnerstag, 26. M?rz 2009 19:48
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] Elegant bootstrapping with zoo

Hello,

I want to bootstrap from a population in a zoo object. For example, I have a
set of returns for the S&P and want to bootstrap from this set over time. I
can do this with the below code, and I wanted to know if there is a more
elegant approach?

> h <- sp500.subset[sample(index(sp500.subset), 100, TRUE),]

I was hoping that a cleaner call would work, but apparently the length is
being used in the call to sample:

> h <- sample(sp500.subset, 100, TRUE)
Error in `[.zoo`(x, .Internal(sample(length(x), size, replace, prob))) :
  subscript out of bounds

Any thoughts or should I stick with the above approach?

Thanks,
Brian


--------------------------------------------------------------------------
This message w/attachments (message) may be privileged, confidential or
proprietary, and if you are not an intended recipient, please notify the
sender, do not use or share it and delete it. Unless specifically indicated,
this message is not an offer to sell or a solicitation of any investment
products or other financial product or service, an official confirmation of
any transaction, or an official statement of Merrill Lynch. Subject to
applicable law, Merrill Lynch may monitor, review and retain
e-communications (EC) traveling through its networks/systems. The laws of
the country of each sender/recipient may impact the handling of EC, and EC
may be archived, supervised and produced in countries other than the country
in which you are located. This message cannot be guaranteed to be secure or
error-free. References to "Merrill Lynch" are references to any company in
the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by
[[elided Yahoo spam]]
 rities and Insurance Products: * Are Not FDIC Insured * Are Not Bank
Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition
to Any Banking Service or Activity * Are Not Insured by Any Federal
Government Agency. Attachments that are part of this E-communication may
have additional important disclosures and disclaimers, which you should
read. This message is subject to terms available at the following link:
http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch
you consent to the foregoing.
--------------------------------------------------------------------------


	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG - www.avg.com

03/25/09
18:54:00


From B_Rowe at ml.com  Thu Mar 26 20:44:33 2009
From: B_Rowe at ml.com (Rowe, Brian Lee Yung (Portfolio Analytics))
Date: Thu, 26 Mar 2009 15:44:33 -0400
Subject: [R-SIG-Finance] Elegant bootstrapping with zoo
In-Reply-To: <34791.36086.bm@omp209.mail.ukl.yahoo.com>
Message-ID: <3BAD818D9407B043817CC6D89ABA14EC032B83EF@MLNYC20MB051.amrs.win.ml.com>

I guess a better way to phrase the question is: can sample() can handle 2-dimensional data? At this point, I will switch to r-help since it is not specific to finance.


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Enrico Schumann
Sent: Thursday, March 26, 2009 3:34 PM
To: Rowe, Brian Lee Yung (Portfolio Analytics)
Cc: r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Elegant bootstrapping with zoo


hi brian,

one line of code, seems elegant to me ;) i guess sp500.subset has several
columns? i am not sure whether `sample' can be made to work row-wise on a
matrix, but you could write your own function.

require(zoo)

## create some artifical returns data
z <- rnorm(200) * 0.05
dim(z) <- c(100, 2)
z <- zoo(z, 1:100)

## function (x = matrix with time series in columns, nR = number of
replications)
bsRows <- function(x, nR){
	z  <- as.matrix(x)
	bs <- sample(seq(1,nrow(z)), nR, replace = TRUE)
	z  <- z[bs,]
	z  <- zoo(z, index(x)[bs])
	return(z)
}

## try
bsRows(z, 10)

(you may get warnings for repeated dates)
regards,
enrico

-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Rowe, Brian
Lee Yung (Portfolio Analytics)
Gesendet: Donnerstag, 26. M?rz 2009 19:48
An: r-sig-finance at stat.math.ethz.ch
Betreff: [R-SIG-Finance] Elegant bootstrapping with zoo

Hello,

I want to bootstrap from a population in a zoo object. For example, I have a
set of returns for the S&P and want to bootstrap from this set over time. I
can do this with the below code, and I wanted to know if there is a more
elegant approach?

> h <- sp500.subset[sample(index(sp500.subset), 100, TRUE),]

I was hoping that a cleaner call would work, but apparently the length is
being used in the call to sample:

> h <- sample(sp500.subset, 100, TRUE)
Error in `[.zoo`(x, .Internal(sample(length(x), size, replace, prob))) :
  subscript out of bounds

Any thoughts or should I stick with the above approach?

Thanks,
Brian


--------------------------------------------------------------------------
This message w/attachments (message) may be privileged, confidential or
proprietary, and if you are not an intended recipient, please notify the
sender, do not use or share it and delete it. Unless specifically indicated,
this message is not an offer to sell or a solicitation of any investment
products or other financial product or service, an official confirmation of
any transaction, or an official statement of Merrill Lynch. Subject to
applicable law, Merrill Lynch may monitor, review and retain
e-communications (EC) traveling through its networks/systems. The laws of
the country of each sender/recipient may impact the handling of EC, and EC
may be archived, supervised and produced in countries other than the country
in which you are located. This message cannot be guaranteed to be secure or
error-free. References to "Merrill Lynch" are references to any company in
the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by
[[elided Yahoo spam]]
 rities and Insurance Products: * Are Not FDIC Insured * Are Not Bank
Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition
to Any Banking Service or Activity * Are Not Insured by Any Federal
Government Agency. Attachments that are part of this E-communication may
have additional important disclosures and disclaimers, which you should
read. This message is subject to terms available at the following link:
http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch
you consent to the foregoing.
--------------------------------------------------------------------------


	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.
No virus found in this incoming message.
Checked by AVG - www.avg.com

03/25/09
18:54:00

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.


From ggrothendieck at gmail.com  Thu Mar 26 20:50:53 2009
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 Mar 2009 15:50:53 -0400
Subject: [R-SIG-Finance] Elegant bootstrapping with zoo
In-Reply-To: <3BAD818D9407B043817CC6D89ABA14EC032B83EC@MLNYC20MB051.amrs.win.ml.com>
References: <3BAD818D9407B043817CC6D89ABA14EC032B83EC@MLNYC20MB051.amrs.win.ml.com>
Message-ID: <971536df0903261250h7663119fw44955529aaa12a4a@mail.gmail.com>

A zoo object is a time series and should have unique times
so sampling with replacement  requires a non-zoo object.

On Thu, Mar 26, 2009 at 2:47 PM, Rowe, Brian Lee Yung (Portfolio
Analytics) <B_Rowe at ml.com> wrote:
> Hello,
>
> I want to bootstrap from a population in a zoo object. For example, I
> have a set of returns for the S&P and want to bootstrap from this set
> over time. I can do this with the below code, and I wanted to know if
> there is a more elegant approach?
>
>> h <- sp500.subset[sample(index(sp500.subset), 100, TRUE),]
>
> I was hoping that a cleaner call would work, but apparently the length
> is being used in the call to sample:
>
>> h <- sample(sp500.subset, 100, TRUE)
> Error in `[.zoo`(x, .Internal(sample(length(x), size, replace, prob))) :
> ?subscript out of bounds
>
> Any thoughts or should I stick with the above approach?
>
> Thanks,
> Brian
>
>
> --------------------------------------------------------------------------
> This message w/attachments (message) may be privileged, confidential or proprietary, and if you are not an intended recipient, please notify the sender, do not use or share it and delete it. Unless specifically indicated, this message is not an offer to sell or a solicitation of any investment products or other financial product or service, an official confirmation of any transaction, or an official statement of Merrill Lynch. Subject to applicable law, Merrill Lynch may monitor, review and retain e-communications (EC) traveling through its networks/systems. The laws of the country of each sender/recipient may impact the handling of EC, and EC may be archived, supervised and produced in countries other than the country in which you are located. This message cannot be guaranteed to be secure or error-free. References to "Merrill Lynch" are references to any company in the Merrill Lynch & Co., Inc. group of companies, which are wholly-owned by Bank of America Corporation. Secu!
> ?rities and Insurance Products: * Are Not FDIC Insured * Are Not Bank Guaranteed * May Lose Value * Are Not a Bank Deposit * Are Not a Condition to Any Banking Service or Activity * Are Not Insured by Any Federal Government Agency. Attachments that are part of this E-communication may have additional important disclosures and disclaimers, which you should read. This message is subject to terms available at the following link: http://www.ml.com/e-communications_terms/. By messaging with Merrill Lynch you consent to the foregoing.
> --------------------------------------------------------------------------
>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From bastian2507hk at yahoo.co.uk  Fri Mar 27 01:06:47 2009
From: bastian2507hk at yahoo.co.uk (Bastian Offermann)
Date: Fri, 27 Mar 2009 01:06:47 +0100
Subject: [R-SIG-Finance] Black Litterman question
Message-ID: <49CC1897.2070607@yahoo.co.uk>

Hi all,

my question is aimed at the Black-Litterman experts. I am constructing a 
portfolio consisting of various equity indices and express 1 absolute 
view and 1 relative view on another 2 assets.

Additionally, I have to express confidence levels put into each view. 
The higher the level the larger the deviation from its equilibrium 
weight. Thats my question: Does the tilt from equilibrium necessarily 
reach its maximum when my confidence is 100% ?? Or may it reach its 
largest tilt at a lower confidence level? Does not sound intuitive, but 
my calculations just show that (maximum tilt at 99% confidence to be 
specific).

Thanks for your input.

Regards


From brian at braverock.com  Fri Mar 27 02:01:43 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 26 Mar 2009 20:01:43 -0500
Subject: [R-SIG-Finance] Black Litterman question
In-Reply-To: <49CC1897.2070607@yahoo.co.uk>
References: <49CC1897.2070607@yahoo.co.uk>
Message-ID: <49CC2577.7030407@braverock.com>

Bastian Offermann wrote:
> my question is aimed at the Black-Litterman experts. I am constructing a 
> portfolio consisting of various equity indices and express 1 absolute 
> view and 1 relative view on another 2 assets.
> 
> Additionally, I have to express confidence levels put into each view. 
> The higher the level the larger the deviation from its equilibrium 
> weight. Thats my question: Does the tilt from equilibrium necessarily 
> reach its maximum when my confidence is 100% ?? Or may it reach its 
> largest tilt at a lower confidence level? Does not sound intuitive, but 
> my calculations just show that (maximum tilt at 99% confidence to be 
> specific).

Depending on your constraints, you may reach the maximum tilt much sooner than 
100% confidence.

Regards,

    - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From j_cuisinier at hotmail.com  Fri Mar 27 13:01:42 2009
From: j_cuisinier at hotmail.com (julien cuisinier)
Date: Fri, 27 Mar 2009 13:01:42 +0100
Subject: [R-SIG-Finance] Black Litterman question
In-Reply-To: <49CC1897.2070607@yahoo.co.uk>
References: <49CC1897.2070607@yahoo.co.uk>
Message-ID: <COL102-W378D5C64F3BE1554F843958F8E0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090327/c4c7071f/attachment.pl>

From andyzhu35 at yahoo.com  Mon Mar 30 01:29:58 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Sun, 29 Mar 2009 16:29:58 -0700 (PDT)
Subject: [R-SIG-Finance] help: market capitalization
Message-ID: <381278.1383.qm@web56204.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090329/018d4724/attachment.pl>

From guygreen at netvigator.com  Mon Mar 30 03:11:38 2009
From: guygreen at netvigator.com (gug)
Date: Sun, 29 Mar 2009 18:11:38 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] commodity prices
Message-ID: <22719668.post@talk.nabble.com>


"Resending" this reply as it has been stalled (not yet accepted by the
mailing list) for several days:


Not a perfect solution: as far as I can tell there isn't one.  You'll
probably know of these already, but for what they're worth:

* Economagic.com for a wide variety of historic time series.

* The NBER Macrohistory Database
http://www.nber.org/databases/macrohistory/contents/ including Chapter 4 -
price indices from 1800's to 1960's in some cases.

* For monthly gold, the Bank of England:
http://www.bankofengland.co.uk/mfsd/iadb/CategoryIndex.asp?Travel=NIxAZx&CategId=allcats&CategName=Combined%20A%20to%20Z

* In theory, annual gold prices since 1257 (though its not working for me
right now): http://www.measuringworth.org/gold/

* For metals, with charts and some daily data back to 1996:
http://www.kitco.com/gold.londonfix.html

* And for commodity price charts, but not the underlying data:
http://futures.tradingcharts.com/menu.html



Dear R Finance People:

I posted this to the regular list but thought that I would try here as
well.

Does anyone know of a website for historical commodity prices, please?

Thanks,

Erin

Erin M. Hodgess, Ph.D.
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgesse at uhd.edu
-- 
View this message in context: http://www.nabble.com/commodity-prices-tp22710933p22719668.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From guygreen at netvigator.com  Mon Mar 30 03:42:01 2009
From: guygreen at netvigator.com (gug)
Date: Sun, 29 Mar 2009 18:42:01 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] help: market capitalization
In-Reply-To: <381278.1383.qm@web56204.mail.re3.yahoo.com>
References: <381278.1383.qm@web56204.mail.re3.yahoo.com>
Message-ID: <22775055.post@talk.nabble.com>


If you mean a website with free data, Yahoo Finance gives the ability to
download a surprisingly large number of data items for US (and I think
Canadian) stocks.  See this for a listing of data items:
http://www.gummy-stuff.org/Yahoo-data.htm.

This works for lists of stocks though I think there are limits to how many
you can get at one time - from memory something like 25.

Guy


Andy Zhu wrote:
> 
> Hi, is there any package containing function to download market cap data:
> such as outstanding share and floating?
> 
> 

-- 
View this message in context: http://www.nabble.com/help%3A-market-capitalization-tp22774150p22775055.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From andyzhu35 at yahoo.com  Mon Mar 30 11:11:56 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Mon, 30 Mar 2009 02:11:56 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] help: market capitalization
In-Reply-To: <22775055.post@talk.nabble.com>
Message-ID: <974071.77515.qm@web56204.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090330/86c0a5e0/attachment.pl>

From j_cuisinier at hotmail.com  Mon Mar 30 11:44:49 2009
From: j_cuisinier at hotmail.com (julien cuisinier)
Date: Mon, 30 Mar 2009 11:44:49 +0200
Subject: [R-SIG-Finance] [R-sig-finance] help: market capitalization
In-Reply-To: <974071.77515.qm@web56204.mail.re3.yahoo.com>
References: <22775055.post@talk.nabble.com>
	<974071.77515.qm@web56204.mail.re3.yahoo.com>
Message-ID: <COL102-W735F8E585D37EC25493DDA8F8D0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090330/6d3ecb13/attachment.pl>

From araki.aito at gmail.com  Mon Mar 30 22:11:20 2009
From: araki.aito at gmail.com (aito araki)
Date: Mon, 30 Mar 2009 22:11:20 +0200
Subject: [R-SIG-Finance] Multi-asset portfolio skewness&kurtosis formulae
Message-ID: <645f81140903301311y1daed8c8i1b830ab02c73805d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090330/cba58645/attachment.pl>

From brian at braverock.com  Tue Mar 31 00:46:18 2009
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 30 Mar 2009 17:46:18 -0500
Subject: [R-SIG-Finance] Multi-asset portfolio skewness&kurtosis formulae
In-Reply-To: <645f81140903301311y1daed8c8i1b830ab02c73805d@mail.gmail.com>
References: <645f81140903301311y1daed8c8i1b830ab02c73805d@mail.gmail.com>
Message-ID: <49D14BBA.7060507@braverock.com>

aito araki wrote:
> I do face some difficulties trying to set up a mean-modified-VaR
> optimization in excel, using the SOLVER function. I use a parametric
> approach in calculating the MVaR and the cornish-fisher expansion to account
> for skewness and leptokurtosis. So far i just could not find any general
> formula to calculate the  s&k on a portfolio level only the well known
> formulae for individual assets. Does anyone know how i can calculate the s&k
> for the portfolio that enter my MVaR measure?
>
> I would very much appreciate some advice from anyone familiar with this..
>
> Thanks a lot!
>   
This is a list for R in finance, not Excel in finance.  As such, I am 
happy to report that the functions you need are all available in the R 
package PerformanceAnalytics.

You can find all the formulae and proofs in our paper:

/Estimation and Decomposition of Downside Risk for Portfolios with 
Non-normal Returns/. Kris Boudt and Brian Peterson and Christophe Croux. 
Journal of Risk. Winter 2008 11(2) **p. 79-103.


Regards,

   - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From andyzhu35 at yahoo.com  Tue Mar 31 05:47:29 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Mon, 30 Mar 2009 20:47:29 -0700 (PDT)
Subject: [R-SIG-Finance] fImport: yahooKeystats error
Message-ID: <304398.20308.qm@web56203.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090330/a9f0afa4/attachment.pl>

From araki.aito at gmail.com  Tue Mar 31 09:00:21 2009
From: araki.aito at gmail.com (aito araki)
Date: Tue, 31 Mar 2009 09:00:21 +0200
Subject: [R-SIG-Finance] Multi-asset portfolio skewness&kurtosis formulae
In-Reply-To: <49D14BBA.7060507@braverock.com>
References: <645f81140903301311y1daed8c8i1b830ab02c73805d@mail.gmail.com>
	<49D14BBA.7060507@braverock.com>
Message-ID: <645f81140903310000u36f134c2p2694ee89c51f2bcf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090331/8f1bf52d/attachment.pl>

From chalabi at phys.ethz.ch  Tue Mar 31 09:43:47 2009
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Tue, 31 Mar 2009 09:43:47 +0200
Subject: [R-SIG-Finance] fImport: yahooKeystats error
In-Reply-To: <304398.20308.qm@web56203.mail.re3.yahoo.com>
References: <304398.20308.qm@web56203.mail.re3.yahoo.com>
Message-ID: <20090331094347.73e81a18@mimi>

>>>> "AZ" == Andy Zhu <andyzhu35 at yahoo.com>
>>>> on Mon, 30 Mar 2009 20:47:29 -0700 (PDT)

   AZ> How to catch error in this function? My following code seems
   AZ> not working with this fImport:
   AZ> library(fImport);
   AZ> ticker = 'ABI';
   AZ> if (inherits(try({ks=yahooKeystats(ticker)}, silent = F),
   AZ> try-error)) {
   AZ> print('no data');
   AZ> }

Hi Andy,

I would set the argument 'try' to FALSE  in 'yahooKeystats'.
This gives with your code  :

library(fImport);
ticker = 'ABI';
if (inherits(try({ks=yahooKeystats(ticker, try=FALSE)}, silent = F), "try-error")) {
print('no data');
}

But it would be better to cast the error you mentioned inside 
'yahooKeystats'. I will have a look at it when I have some spare time.

regards,
Yohan

-- 
PhD student
Swiss Federal Institute of Technology
Zurich

www.ethz.ch


From tohm.kantikovit at bnymellon.com  Tue Mar 31 16:00:58 2009
From: tohm.kantikovit at bnymellon.com (tohm.kantikovit at bnymellon.com)
Date: Tue, 31 Mar 2009 10:00:58 -0400
Subject: [R-SIG-Finance] AUTO: Tohm Kantikovit will be out of the office for
 the day of 03/31/2009. (returning 04/01/2009)
Message-ID: <OF2B610AC6.E8088B9F-ON8525758A.004CFE65-8525758A.004CFE67@bankofny.com>


I am out of the office until 04/01/2009.

For deal inqueries regarding MJX, please contact Raul Burgos
(212-815-3259); for those regarding PLID, SABL, ARES please contact Paul
Motusesky (212-298-1914); for all other matters please contact Jeriel
Zuniga (212-815-2443). Thank you.


Note: This is an automated response to your message  "R-SIG-Finance
Digest, Vol 58, Issue 24" sent on 3/31/2009 6:00:02 AM.

This is the only notification you will receive while this person is
away.


The information contained in this e-mail may be confiden...{{dropped:13}}


From bearxu83 at gmail.com  Tue Mar 31 16:52:01 2009
From: bearxu83 at gmail.com (BearXu)
Date: Tue, 31 Mar 2009 15:52:01 +0100
Subject: [R-SIG-Finance] Question about the ARMA model
Message-ID: <82527b5d0903310752o32ef555ej6ceb24c5ac53f323@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090331/f331e907/attachment.pl>

From andyzhu35 at yahoo.com  Tue Mar 31 18:28:23 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Tue, 31 Mar 2009 09:28:23 -0700 (PDT)
Subject: [R-SIG-Finance] fImport: yahooKeystats error
Message-ID: <680670.72773.qm@web56201.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090331/9be8caaa/attachment.pl>

From andyzhu35 at yahoo.com  Tue Mar 31 18:38:01 2009
From: andyzhu35 at yahoo.com (andyzhu35 at yahoo.com)
Date: Tue, 31 Mar 2009 09:38:01 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] help: market capitalization
Message-ID: <212102.15357.qm@web56207.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090331/f7bd768e/attachment.pl>

From luethid at gmail.com  Tue Mar 31 19:54:13 2009
From: luethid at gmail.com (=?ISO-8859-1?Q?David_L=FCthi?=)
Date: Tue, 31 Mar 2009 19:54:13 +0200
Subject: [R-SIG-Finance] Multi-asset portfolio skewness&kurtosis formulae
In-Reply-To: <645f81140903301311y1daed8c8i1b830ab02c73805d@mail.gmail.com>
References: <645f81140903301311y1daed8c8i1b830ab02c73805d@mail.gmail.com>
Message-ID: <49D258C5.90704@gmail.com>

Dear Aito

If you want to tackle your problem in a fully parametric way you might
also consider to use the package 'ghyp'.

This package provides code to fit a multivariate generalized
hyperbolic distribution (or one of its many special cases) which
allows for skewness and excess-kurtosis to your return series and
subsequently use 'portfolio.optimize' to optimize the portfolio with
respect to different risk-measures as VaR, Conditional VaR, and variance.

Best regards,
David L?thi


aito araki wrote:
> Hi everyone,
> 
> I do face some difficulties trying to set up a mean-modified-VaR
> optimization in excel, using the SOLVER function. I use a parametric
> approach in calculating the MVaR and the cornish-fisher expansion to account
> for skewness and leptokurtosis. So far i just could not find any general
> formula to calculate the  s&k on a portfolio level only the well known
> formulae for individual assets. Does anyone know how i can calculate the s&k
> for the portfolio that enter my MVaR measure?
> 
> I would very much appreciate some advice from anyone familiar with this..
> 
> Thanks a lot!
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From andyzhu35 at yahoo.com  Tue Mar 31 23:43:21 2009
From: andyzhu35 at yahoo.com (Andy Zhu)
Date: Tue, 31 Mar 2009 14:43:21 -0700 (PDT)
Subject: [R-SIG-Finance] fImport: yahooKeystats error - workaround
Message-ID: <810441.7772.qm@web56207.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20090331/1a45eba6/attachment.pl>

