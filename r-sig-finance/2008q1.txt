From konceptualno50 at gmail.com  Wed Jan  2 22:37:09 2008
From: konceptualno50 at gmail.com (Pika Novak)
Date: Wed, 2 Jan 2008 22:37:09 +0100
Subject: [R-SIG-Finance] Gumbel model & Maximum Likelihood
Message-ID: <8eb857fd0801021337j13f6ddf4m864f22b4774f98c4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080102/d9fd303b/attachment.pl 

From brian at braverock.com  Wed Jan  2 22:43:34 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 02 Jan 2008 15:43:34 -0600
Subject: [R-SIG-Finance] Gumbel model & Maximum Likelihood
In-Reply-To: <8eb857fd0801021337j13f6ddf4m864f22b4774f98c4@mail.gmail.com>
References: <8eb857fd0801021337j13f6ddf4m864f22b4774f98c4@mail.gmail.com>
Message-ID: <477C0586.4040800@braverock.com>

Pika Novak wrote:
> The parameter ?? involved in the Gumbel model must be estimated via Maximum
> Likelihood,
> starting from the following initial value:
> ????/(1 ??? ????)= 0, 104;
> Can anybody help?

Your HTML text was deleted by the list, and the characters in your email 
are illegible.  Could you try again inplain text, or point us at a 
reference (preferably both) so that we can try to help you.

Regards,

   - Brian


From ecjbosu at aol.com  Thu Jan  3 06:23:07 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Wed, 02 Jan 2008 23:23:07 -0600
Subject: [R-SIG-Finance] PerformanceAnalytics version 0.9.6 released to
	CRAN
In-Reply-To: <4778D9A1.4060806@braverock.com>
References: <4778D9A1.4060806@braverock.com>
Message-ID: <477C713B.3030301@aol.com>

Brian G. Peterson wrote:
> We are pleased to announce the availability on CRAN of 
> PerformanceAnalytics version 0.9.6.
> 
> This is a feature and bugfix release.
> 
> http://cran.r-project.org/src/contrib/Descriptions/PerformanceAnalytics.html
> 
> PerformanceAnalytics is a library of econometric functions for 
> performance and risk analysis. This library aims to aid practitioners 
> and researchers in utilizing the latest research in analysis of 
> non-normal return streams.
> 
> Package: PerformanceAnalytics
> Type: Package
> Title: Econometric tools for performance and risk analysis.
> Version: 0.9.6
> Date: 2007-12-29
> License: GPL
> URL: 
> http://cran.r-project.org/src/contrib/Descriptions/PerformanceAnalytics.html
> URL: http://braverock.com/R/
> 
> New Functions:
>      chart.ECDF
>          Creates an empirical cumulative distribution function (ECDF)
>          overlaid with a cumulative distribution function (CDF)
>          Inspired by:
>          Ruppert, David. 2004.
>          Statistics and Finance, an Introduction.
>          Ch. 2 Fig. 2.5
> 
>      chart.ACF
>      chart.ACFplus
>          Inspired by (and partially ported from) the website:
>          http://www.stat.pitt.edu/stoffer/tsa2/Rcode/acf2.R
>          "here's an R function that will plot the ACF and PACF of a time
>          series at the same time on the SAME SCALE, and it leaves out
>          the zero lag in the ACF [and uses the number of observations
>          as the default]"
>          That description made a lot of sense, so it's implemented here
>          for both the ACF alone and the ACF with the PACF.
> 
>      chart.Regression
>          Uses a scatterplot to display the relationship of returns
>          to a market benchmark.  Fits a linear model and overlays the
>          resulting model.  Also overlays a Loess line for comparison.
> 
>      Return.read
>          Wrapper of 'read.zoo' with some defaults for different
>          date formats and frequencies.
> 
>      Return.Geltner
>          Calculate Geltner liquidity-risk-adjusted return series.
>          David Geltner developed a method to remove estimating/liquidity
>          bias in real estate index returns.  It has since been applied
>          to other return series that show autocorrelation or
>          illiquidity effects. The theory is that by correcting for
>          autocorrelation, you are uncovering a "true" return from series
>          of observed returns that contain illiquidity or manual pricing
>          effects.
> 
>      SmoothingIndex
>          Proposed by Getmansky et al to provide a normalized measure of
>          liquidity risk.  The index will produces a number from zero to
>          one.  A low number indicates low liquidity risk.  A number
>          trending towards one indicates a higher liquidity risk.
> 
>      table.Autocorrelation
>          Produces data table of autocorrelation coefficients rho and
>          corresponding Q(6)-statistic for each column in return series.
> 
>      table.CalendarReturns
>          Returns a table of returns formatted with years in rows, months
>          in columns, and a total column in the last column.
>          For additional columns, annual returns will be appended.
> 
> 
> Significantly Changed Functions:
>      chart.Boxplot
>          Added the ability to more completely control the visual display.
>          Added the ability to render a Tufte-style compact boxplot.
> 
>      chart.Histogram
>          Improved visual display for print-quality graphics
>          Added fits for extra distributions (stable,cauchy,skew-T)
>          Added more control over risk lines
>          Added event lines
> 
>      chart.QQPlot
>          Replaced most internals with port of John Fox's
>          qq.plot from 'car'
>          Now fits arbitrary distributions
>          Allows use of error bands
> 
>      We have made changes throughout the package to allow the
>      risk-free rate to contain a vector of changing rates corresponding
>      with the return series being examined.
> 
>      In addition, we have made more extensive use of the features of the
>      'zoo' package in this release of PerformanceAnalytics, and removed
>      a few external dependencies where those dependencies were minor and
>      easily replicated or ported to this package.  We expect both of
>      these trends to continue in later releases.  Hopefully, we have
>      properly credited the original authors and functions both in our
>      code and in the manual pages.
> 
> Deprecated Functions:
>      rollingCorrelation
>      rollingFunction
>          These functions have been replaced in our code by the use of
>          zoo's 'rollapply' function, and are no longer needed as
>          separate custom functions.
> 
> New Vignettes:
>      We have added as vignettes the presentations we gave on
>      PerformanceAnalytics at the R/RMetrics Conference in Mielesalp
>      in July 2007 and at UseR! 2007 in Ames, Iowa.
> 
> Other:
>      This version of PerformanceAnalytics contains many, many minor
>      improvements and changes.  We added aver 1500 lines of code
>      and comments, and over 1000 lines of documentation.
> 
> We have benefited greatly from feedback and comments from the users of
> PerformanceAnalytics and from R-SIG-Finance.  Please continue to send 
> your questions, comments, and complaints.
> 
> Full details available in the ChangeLog or in the CVS logs in all .R 
> files in the source package.
> 
> Regards,
> 
>      - Brian
> 
Brian,

Great looking package!  I was installing it on my linux server using my 
VNC connection and a terminal window as su root, so I was only using the 
command windows, when I encountered an error with the install.  The 
package requires tclk which will not load unless the display is set 
correctly on the linux terminal shell.  I had to physically log on to 
the linux machine to get it to install.  I am not sure if there is a 
workaround or if you can change the package to only load the tclk 
library as needed, not depend on it.  This is not a major issue for me 
but being able to remotely connect and run a superuser shell is 
convenient.  This will impact anyone that has a cron job that 
automatically updates the R packages on machines.

Happy New Year
Joe


From ecjbosu at aol.com  Thu Jan  3 06:23:07 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Wed, 02 Jan 2008 23:23:07 -0600
Subject: [R-SIG-Finance] PerformanceAnalytics version 0.9.6 released to
	CRAN
In-Reply-To: <4778D9A1.4060806@braverock.com>
References: <4778D9A1.4060806@braverock.com>
Message-ID: <477C713B.3030301@aol.com>

Brian G. Peterson wrote:
> We are pleased to announce the availability on CRAN of 
> PerformanceAnalytics version 0.9.6.
> 
> This is a feature and bugfix release.
> 
> http://cran.r-project.org/src/contrib/Descriptions/PerformanceAnalytics.html
> 
> PerformanceAnalytics is a library of econometric functions for 
> performance and risk analysis. This library aims to aid practitioners 
> and researchers in utilizing the latest research in analysis of 
> non-normal return streams.
> 
> Package: PerformanceAnalytics
> Type: Package
> Title: Econometric tools for performance and risk analysis.
> Version: 0.9.6
> Date: 2007-12-29
> License: GPL
> URL: 
> http://cran.r-project.org/src/contrib/Descriptions/PerformanceAnalytics.html
> URL: http://braverock.com/R/
> 
> New Functions:
>      chart.ECDF
>          Creates an empirical cumulative distribution function (ECDF)
>          overlaid with a cumulative distribution function (CDF)
>          Inspired by:
>          Ruppert, David. 2004.
>          Statistics and Finance, an Introduction.
>          Ch. 2 Fig. 2.5
> 
>      chart.ACF
>      chart.ACFplus
>          Inspired by (and partially ported from) the website:
>          http://www.stat.pitt.edu/stoffer/tsa2/Rcode/acf2.R
>          "here's an R function that will plot the ACF and PACF of a time
>          series at the same time on the SAME SCALE, and it leaves out
>          the zero lag in the ACF [and uses the number of observations
>          as the default]"
>          That description made a lot of sense, so it's implemented here
>          for both the ACF alone and the ACF with the PACF.
> 
>      chart.Regression
>          Uses a scatterplot to display the relationship of returns
>          to a market benchmark.  Fits a linear model and overlays the
>          resulting model.  Also overlays a Loess line for comparison.
> 
>      Return.read
>          Wrapper of 'read.zoo' with some defaults for different
>          date formats and frequencies.
> 
>      Return.Geltner
>          Calculate Geltner liquidity-risk-adjusted return series.
>          David Geltner developed a method to remove estimating/liquidity
>          bias in real estate index returns.  It has since been applied
>          to other return series that show autocorrelation or
>          illiquidity effects. The theory is that by correcting for
>          autocorrelation, you are uncovering a "true" return from series
>          of observed returns that contain illiquidity or manual pricing
>          effects.
> 
>      SmoothingIndex
>          Proposed by Getmansky et al to provide a normalized measure of
>          liquidity risk.  The index will produces a number from zero to
>          one.  A low number indicates low liquidity risk.  A number
>          trending towards one indicates a higher liquidity risk.
> 
>      table.Autocorrelation
>          Produces data table of autocorrelation coefficients rho and
>          corresponding Q(6)-statistic for each column in return series.
> 
>      table.CalendarReturns
>          Returns a table of returns formatted with years in rows, months
>          in columns, and a total column in the last column.
>          For additional columns, annual returns will be appended.
> 
> 
> Significantly Changed Functions:
>      chart.Boxplot
>          Added the ability to more completely control the visual display.
>          Added the ability to render a Tufte-style compact boxplot.
> 
>      chart.Histogram
>          Improved visual display for print-quality graphics
>          Added fits for extra distributions (stable,cauchy,skew-T)
>          Added more control over risk lines
>          Added event lines
> 
>      chart.QQPlot
>          Replaced most internals with port of John Fox's
>          qq.plot from 'car'
>          Now fits arbitrary distributions
>          Allows use of error bands
> 
>      We have made changes throughout the package to allow the
>      risk-free rate to contain a vector of changing rates corresponding
>      with the return series being examined.
> 
>      In addition, we have made more extensive use of the features of the
>      'zoo' package in this release of PerformanceAnalytics, and removed
>      a few external dependencies where those dependencies were minor and
>      easily replicated or ported to this package.  We expect both of
>      these trends to continue in later releases.  Hopefully, we have
>      properly credited the original authors and functions both in our
>      code and in the manual pages.
> 
> Deprecated Functions:
>      rollingCorrelation
>      rollingFunction
>          These functions have been replaced in our code by the use of
>          zoo's 'rollapply' function, and are no longer needed as
>          separate custom functions.
> 
> New Vignettes:
>      We have added as vignettes the presentations we gave on
>      PerformanceAnalytics at the R/RMetrics Conference in Mielesalp
>      in July 2007 and at UseR! 2007 in Ames, Iowa.
> 
> Other:
>      This version of PerformanceAnalytics contains many, many minor
>      improvements and changes.  We added aver 1500 lines of code
>      and comments, and over 1000 lines of documentation.
> 
> We have benefited greatly from feedback and comments from the users of
> PerformanceAnalytics and from R-SIG-Finance.  Please continue to send 
> your questions, comments, and complaints.
> 
> Full details available in the ChangeLog or in the CVS logs in all .R 
> files in the source package.
> 
> Regards,
> 
>      - Brian
> 
Brian,

Great looking package!  I was installing it on my linux server using my 
VNC connection and a terminal window as su root, so I was only using the 
command windows, when I encountered an error with the install.  The 
package requires tclk which will not load unless the display is set 
correctly on the linux terminal shell.  I had to physically log on to 
the linux machine to get it to install.  I am not sure if there is a 
workaround or if you can change the package to only load the tclk 
library as needed, not depend on it.  This is not a major issue for me 
but being able to remotely connect and run a superuser shell is 
convenient.  This will impact anyone that has a cron job that 
automatically updates the R packages on machines.

Happy New Year
Joe


From edd at debian.org  Thu Jan  3 12:12:42 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 3 Jan 2008 05:12:42 -0600
Subject: [R-SIG-Finance] PerformanceAnalytics version 0.9.6 released
	to	CRAN
In-Reply-To: <477C713B.3030301@aol.com>
References: <4778D9A1.4060806@braverock.com>
	<477C713B.3030301@aol.com>
Message-ID: <18300.49962.24787.337166@ron.nulle.part>


On 2 January 2008 at 23:23, Joe W. Byers wrote:
| Great looking package!  I was installing it on my linux server using my 
| VNC connection and a terminal window as su root, so I was only using the 
| command windows, when I encountered an error with the install.  The 
| package requires tclk which will not load unless the display is set 

This is due to one of the Rmetrics packages, I have forgotten which, that is
loading tcltk.

| correctly on the linux terminal shell.  I had to physically log on to 
| the linux machine to get it to install.  I am not sure if there is a 
| workaround or if you can change the package to only load the tclk 
| library as needed, not depend on it.  This is not a major issue for me 

One way around is to modify the package's DESCRIPTION file and to set
LazyLoad to 'no' --- you can do that with a Perl one-liner after expanding
the tarball.  That's basically what I used to do for the Debian packages as
they are 'assembled' in a chroot without $DISPLAY.

The more elegant way around we use now is to use xvfb-run as suggested by
Charles Plessy: it invokes a virtual X11 session using xvfb. One Debian, this
means also installing the "xvfb, xbase-clients, xfonts-base" packages and to
then call

	 xvfb-run R CMD INSTALL -l $(InstalLDir) --clean $(Target)

instead of just 

	R CMD INSTALL -l $(InstalLDir) --clean $(Target)

You should be able to work that out with your sysadmin if need be. 

Hth, Dirk

| but being able to remotely connect and run a superuser shell is 
| convenient.  This will impact anyone that has a cron job that 
| automatically updates the R packages on machines.



-- 
Three out of two people have difficulties with fractions.


From ecjbosu at aol.com  Thu Jan  3 14:24:12 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Thu, 03 Jan 2008 07:24:12 -0600
Subject: [R-SIG-Finance] PerformanceAnalytics version 0.9.6 released to
 CRAN
In-Reply-To: <18300.49962.24787.337166@ron.nulle.part>
References: <4778D9A1.4060806@braverock.com>	<477C713B.3030301@aol.com>
	<18300.49962.24787.337166@ron.nulle.part>
Message-ID: <477CE1FC.6090607@aol.com>

Dirk Eddelbuettel wrote:
> On 2 January 2008 at 23:23, Joe W. Byers wrote:
> | Great looking package!  I was installing it on my linux server using my 
> | VNC connection and a terminal window as su root, so I was only using the 
> | command windows, when I encountered an error with the install.  The 
> | package requires tclk which will not load unless the display is set 
>
> This is due to one of the Rmetrics packages, I have forgotten which, that is
> loading tcltk.
>
> | correctly on the linux terminal shell.  I had to physically log on to 
> | the linux machine to get it to install.  I am not sure if there is a 
> | workaround or if you can change the package to only load the tclk 
> | library as needed, not depend on it.  This is not a major issue for me 
>
> One way around is to modify the package's DESCRIPTION file and to set
> LazyLoad to 'no' --- you can do that with a Perl one-liner after expanding
> the tarball.  That's basically what I used to do for the Debian packages as
> they are 'assembled' in a chroot without $DISPLAY.
>
> The more elegant way around we use now is to use xvfb-run as suggested by
> Charles Plessy: it invokes a virtual X11 session using xvfb. One Debian, this
> means also installing the "xvfb, xbase-clients, xfonts-base" packages and to
> then call
>
>      xvfb-run R CMD INSTALL -l $(InstalLDir) --clean $(Target)
>
> instead of just 
>
>     R CMD INSTALL -l $(InstalLDir) --clean $(Target)
>
> You should be able to work that out with your sysadmin if need be. 
>   
This is on my server at my home, so I am the sysadmin :)
> Hth, Dirk
>
> | but being able to remotely connect and run a superuser shell is 
> | convenient.  This will impact anyone that has a cron job that 
> | automatically updates the R packages on machines.
>
>
>
>   
Dirk,  I appreciate the help and Happy New year

Thanx Joe


From ecjbosu at aol.com  Thu Jan  3 17:29:42 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Thu, 3 Jan 2008 16:29:42 +0000 (UTC)
Subject: [R-SIG-Finance] fCopula and mnormt
Message-ID: <loom.20080103T162842-816@post.gmane.org>

>
>

I have found several technical inconsistencies with covariance matrices and
correlation matrices parameters in the functions and documentations.

for example rcopulae.gauss
> print(rcopula.gauss)
function (n, Sigma = equicorr(d, rho), d = 2, rho = 0.7) 
{
    d <- dim(Sigma)[1]
    if (sum(diag(Sigma)) != d) 
        stop("Sigma should be correlation matrix")
    mnorm <- rmnorm(n, Sigma = Sigma)
    matrix(pnorm(mnorm), ncol = d)
}

has sigma as a parameter but wants a correlation matrix.  rmnorm in mnormt is
the same:
> print(rmnorm)
function (n, Sigma = equicorr(d, rho), mu = rep(0, d), d = 2, 
    rho = 0.7) 
{
    d <- dim(Sigma)[1]
    A <- t(chol(Sigma))
    X <- matrix(rnorm(n * d), nrow = n, ncol = d)
    mu.matrix <- matrix(mu, nrow = n, ncol = d, byrow = TRUE)
    return(t(A %*% t(X)) + mu.matrix)
}

This inconsistency is really confusing and has caused me stress in determining
if I am actually providing the correct parameter to the function.

rmvnorm in mvtnorm clearly requires a covariance parameter when the default
method is used.
function (n, mean = rep(0, nrow(sigma)), sigma = diag(length(mean)), 
    method = c("svd", "chol")) 
{
    if (nrow(sigma) != ncol(sigma)) {
        stop("sigma must be a square matrix")
    }
    if (length(mean) != nrow(sigma)) {
        stop("mean and sigma have non-conforming size")
    }
    method <- match.arg(method)
    if (method == "svd") {
        ev <- eigen(sigma, sym = TRUE)$values
        if (!all(ev >= -sqrt(.Machine$double.eps) * abs(ev[1]))) {
            warning("sigma is numerically not positive definite")
        }
        sigsvd <- svd(sigma)
        retval <- t(sigsvd$v %*% (t(sigsvd$u) * sqrt(sigsvd$d)))
    }
    if (method == "chol") {
        retval <- chol(sigma, pivot = T)
        o <- order(attr(retval, "pivot"))
        retval <- retval[, o]
    }
    retval <- matrix(rnorm(n * ncol(sigma)), nrow = n) %*% retval
    retval <- sweep(retval, 2, mean, "+")
    retval
}
<environment: namespace:mvtnorm>

This function is much cleaner, easier to understand thus less confusing because
it differentiates between two generating methods.  I only wanted to point this
because package:mnormt is a dependency of many of the Rmetrics packages.  I also
may be really off base with this comment, but I was taught to use clear and
consistent parameter names as much as possible.

If this is determined a fix that needs to be done, I would love to assist with
any modifications of the code or documents.

Thank you
Joe


From MichelBeck at sbcglobal.Net  Wed Jan  2 18:27:22 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Wed, 2 Jan 2008 17:27:22 +0000 (UTC)
Subject: [R-SIG-Finance] R - Error Question for chart.Correlation
	(PerformanceAnalytics)
Message-ID: <loom.20080102T171756-520@post.gmane.org>

Hi!

I am a new user of this user group. Apologies if this is question has an obvious
answer.


While I ran the below function from PerformanceAnalytics (edhec is a matrix
provided with the library), this is the error I get

chart.Correlation(edhec[,1],edhec[,3])
Error in pairs.default(x, y, gap = 0, lower.panel = panel.smooth, upper.panel =
panel.cor,  : 
        only one column in the argument to 'pairs'


When I run the below, things work

chart.Correlation(edhec[,1:3],edhec[,1:3])

Any idea?

MAB


From Achim.Zeileis at wu-wien.ac.at  Fri Jan  4 18:29:25 2008
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 4 Jan 2008 18:29:25 +0100 (CET)
Subject: [R-SIG-Finance] Workshop: Computational and Financial Econometrics
Message-ID: <Pine.LNX.4.44.0801041825210.4234-100000@disco.wu-wien.ac.at>

Dear useRs,

CSDA and ERCIM organize jointly a workshop on

  Computational and Financial Econometrics
  June 19-21, 2008,
  Neuchatel, Switzerland
  http://www.dcs.bbk.ac.uk/cfe08/

which follows up on last year's successful meeting in Geneva, see
http://www.csdassn.org/europe/cfe07/.

In this workshop, Christian Kleiber and myself are organizing a session on

  Computational Econometrics and Finance in R

which should also reflect the activities on this mailing list and I would
be happy if many of you would consider attending the workshop or (even
better) giving a presentation. If you are interested in presenting in this
session, please let me know in an informal mail. The official call for
papers along with further details on the organization can be found at the
above URL.

Best wishes,
Z


From daniel at dunncapital.com  Fri Jan  4 23:07:34 2008
From: daniel at dunncapital.com (Daniel Dunn)
Date: Fri, 4 Jan 2008 17:07:34 -0500
Subject: [R-SIG-Finance] Matlab <--> R
Message-ID: <005601c84f1e$358a5310$a112060a@DANO>

DUNN Capital Management is looking to establish a contracting relationship
with a coder with finance expertise who is proficient in both R and Matlab,
to convert libraries of code from Matlab to R and/or vice versa.  If this is
of potential interest to you, please e-mail to me a description of the
services you can provide, as well as any other supporting materials you care
to send.  Thank you.

Daniel Dunn, M.D. Ph.D.
daniel at dunncapital.com


From swtzang at gmail.com  Mon Jan  7 02:21:57 2008
From: swtzang at gmail.com (ShyhWeir Tzang)
Date: Mon, 7 Jan 2008 09:21:57 +0800
Subject: [R-SIG-Finance] extract certain date in a month
Message-ID: <c17037a10801061721y523837b2tb18063033a4d1ac6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080107/36a7f87b/attachment.pl 

From jeff.a.ryan at gmail.com  Mon Jan  7 04:28:39 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sun, 6 Jan 2008 21:28:39 -0600
Subject: [R-SIG-Finance] extract certain date in a month
In-Reply-To: <c17037a10801061721y523837b2tb18063033a4d1ac6@mail.gmail.com>
References: <c17037a10801061721y523837b2tb18063033a4d1ac6@mail.gmail.com>
Message-ID: <e8e755250801061928h3ec05b66w5bcffb7b04780559@mail.gmail.com>

ShyhWeir:

Dates <- as.POSIXct('2007-01-01') + (0:365)
third.wednesdays <- which(format(Dates, "%d") > 14 & format(Dates, "%d") < 22 &
               format(Dates, "%u") == 3)
Dates[third.wednesdays]

The last format() is the actual weekday, Monday is 1...Wed is 3

You can get the last Friday (options/futures expiry) all wrapped up in
a function in
the package quantmod ( www.quantmod.com and on CRAN)

?options.expiry

Which is where I got the code to modify.

Jeff

On Jan 6, 2008 7:21 PM, ShyhWeir Tzang <swtzang at gmail.com> wrote:
> Dear all:
>
> How can I extract the date of the third Wednesday in each month? Is there
> any available function to do it?
> Thanks for help.
>
> ShyhWeir
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Mon Jan  7 04:32:57 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sun, 6 Jan 2008 21:32:57 -0600
Subject: [R-SIG-Finance] extract certain date in a month
In-Reply-To: <e8e755250801061928h3ec05b66w5bcffb7b04780559@mail.gmail.com>
References: <c17037a10801061721y523837b2tb18063033a4d1ac6@mail.gmail.com>
	<e8e755250801061928h3ec05b66w5bcffb7b04780559@mail.gmail.com>
Message-ID: <e8e755250801061932p4737d223n649214801427d0d2@mail.gmail.com>

> Dates <- as.POSIXct('2007-01-01') + (0:365)

*should read*

Dates <- as.Date('2007-01-01') + (0:365)

-OR-

Dates <- as.POSIXct('2007-01-01') + (0:365) * 86400

Sorry.
Jeff

On Jan 6, 2008 9:28 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> ShyhWeir:
>
> Dates <- as.POSIXct('2007-01-01') + (0:365)
> third.wednesdays <- which(format(Dates, "%d") > 14 & format(Dates, "%d") < 22 &
>                format(Dates, "%u") == 3)
> Dates[third.wednesdays]
>
> The last format() is the actual weekday, Monday is 1...Wed is 3
>
> You can get the last Friday (options/futures expiry) all wrapped up in
> a function in
> the package quantmod ( www.quantmod.com and on CRAN)
>
> ?options.expiry
>
> Which is where I got the code to modify.
>
> Jeff
>
>
> On Jan 6, 2008 7:21 PM, ShyhWeir Tzang <swtzang at gmail.com> wrote:
> > Dear all:
> >
> > How can I extract the date of the third Wednesday in each month? Is there
> > any available function to do it?
> > Thanks for help.
> >
> > ShyhWeir
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From ianseow at gmail.com  Mon Jan  7 12:12:56 2008
From: ianseow at gmail.com (Ian Seow)
Date: Mon, 7 Jan 2008 19:12:56 +0800
Subject: [R-SIG-Finance] Ops on zoo objects
Message-ID: <1865010801070312td943fd7r767d7c49c7f8b24b@mail.gmail.com>

Hi guys, currently the zoo package extends the group generic functions
Ops to perform operations only for the **intersection** of the indexes
of the objects. Was just wondering if anyone has came across an
elegant/simple solution for operations on the union of the indexes?
I'm trying to look for a clean way of adding the signals of my
different trading models which return zoo time-series with indexes
which are not necessary the same. Unfortunately, the only way I can
think of is to merge the zoo objects, obtain the union of the indexes,
prepend / append each zoo object with zeros, and then add them. This
looks really ugly especially when there are many objects to be added
:(

Brief example:

> model1.output

               EURUSD   USDJPY
2007-12-27 -19.470868 32.79096
2007-12-28 -17.159300 32.79096
2008-01-02 -17.162815 32.79096
2008-01-03  -5.701861 10.93032
2008-01-04  -5.701861 10.93032

> model2.output

               EURUSD   USDJPY
2007-12-27 -20.470868 11.79096
2007-12-28 -20.159300 11.79096
2008-01-02 -20.162815 11.79096

model1.output + model2.output would return a zoo object starting from
2007-12-27 to 2008-01-02. :(
Any tips on this would be greatly appreciated!
Thanks!

Best regards
Ian Seow
GIC


From ggrothendieck at gmail.com  Mon Jan  7 15:09:58 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 7 Jan 2008 09:09:58 -0500
Subject: [R-SIG-Finance] Ops on zoo objects
In-Reply-To: <1865010801070312td943fd7r767d7c49c7f8b24b@mail.gmail.com>
References: <1865010801070312td943fd7r767d7c49c7f8b24b@mail.gmail.com>
Message-ID: <971536df0801070609s5d57b391l6ad3ccc55eca401e@mail.gmail.com>

On Jan 7, 2008 6:12 AM, Ian Seow <ianseow at gmail.com> wrote:
> Hi guys, currently the zoo package extends the group generic functions
> Ops to perform operations only for the **intersection** of the indexes
> of the objects. Was just wondering if anyone has came across an
> elegant/simple solution for operations on the union of the indexes?
> I'm trying to look for a clean way of adding the signals of my
> different trading models which return zoo time-series with indexes
> which are not necessary the same. Unfortunately, the only way I can
> think of is to merge the zoo objects, obtain the union of the indexes,
> prepend / append each zoo object with zeros, and then add them. This
> looks really ugly especially when there are many objects to be added
> :(
>
> Brief example:
>
> > model1.output
>
>               EURUSD   USDJPY
> 2007-12-27 -19.470868 32.79096
> 2007-12-28 -17.159300 32.79096
> 2008-01-02 -17.162815 32.79096
> 2008-01-03  -5.701861 10.93032
> 2008-01-04  -5.701861 10.93032
>
> > model2.output
>
>               EURUSD   USDJPY
> 2007-12-27 -20.470868 11.79096
> 2007-12-28 -20.159300 11.79096
> 2008-01-02 -20.162815 11.79096
>
> model1.output + model2.output would return a zoo object starting from
> 2007-12-27 to 2008-01-02. :(
> Any tips on this would be greatly appreciated!
> Thanks!

It would be enough to define a subclass of zoo with its own
constructor function and
an as function for conversions:

> Ops.zooall <- function (e1, e2)
+ {
+     e <- if (missing(e2)) {
+         NextMethod(.Generic)
+     }
+     else if (any(nchar(.Method) == 0)) {
+         NextMethod(.Generic)
+     }
+     else {
+ merge(e1, e2, all = TRUE, fill = 0, retclass = NULL)
+         NextMethod(.Generic)
+     }
+     if (is.null(attr(e, "index")))
+ zooall(e, index(e1), attr(e1, "frequency"))
+     else
+ e
+ }
>
> zooall <- function(x = NULL, order = index(x), frequency = NULL) {
+ structure(zoo(x = x, order = order, frequency = frequency),
+ class = c("zooall", "zoo"))
+ }
>
> as.zooall <- function(x, ...) UseMethod("as.zooall")
> as.zooall.zoo <- function(x, ...) structure(x, class = c("zooall", "zoo"))
>
> # now using your zoo data:
>
> model1.output <-
+ structure(c(-19.470868, -17.1593, -17.162815, -5.701861, -5.701861,
+ 32.79096, 32.79096, 32.79096, 10.93032, 10.93032), .Dim = c(5L,
+ 2L), .Dimnames = list(NULL, c("V2", "V3")), index = structure(c(13874,
+ 13875, 13880, 13881, 13882), class = "Date"), class = "zoo")
>
> model2.output <-
+ structure(c(-20.470868, -20.1593, -20.162815, 11.79096, 11.79096,
+ 11.79096), .Dim = c(3L, 2L), .Dimnames = list(NULL, c("V2", "V3"
+ )), index = structure(c(13874, 13875, 13880), class = "Date"), class = "zoo")
>
> model1.output.all <- as.zooall(model1.output)
> model2.output.all <- as.zooall(model2.output)
> model1.output.all + model2.output.all

2007-12-27 -39.941736 44.58192
2007-12-28 -37.318600 44.58192
2008-01-02 -37.325630 44.58192
2008-01-03  -5.701861 10.93032
2008-01-04  -5.701861 10.93032
>
> model1.output + model2.output
                  V2       V3
2007-12-27 -39.94174 44.58192
2007-12-28 -37.31860 44.58192
2008-01-02 -37.32563 44.58192


From ianseow at gmail.com  Mon Jan  7 16:40:15 2008
From: ianseow at gmail.com (Ian Seow)
Date: Mon, 7 Jan 2008 23:40:15 +0800
Subject: [R-SIG-Finance] Ops on zoo objects
In-Reply-To: <971536df0801070609s5d57b391l6ad3ccc55eca401e@mail.gmail.com>
References: <1865010801070312td943fd7r767d7c49c7f8b24b@mail.gmail.com>
	<971536df0801070609s5d57b391l6ad3ccc55eca401e@mail.gmail.com>
Message-ID: <1865010801070740s1e5fcb95mda852280f4e50368@mail.gmail.com>

Amazing!! Thank you very much Gabor for your excellent suggestion...
certainly saved me lots of hair-pulling :) Btw, are there any
resources where I can read up about OOP with R? Esp. the syntax used
here in this solution... it's pretty new to me... so far most of the R
code I've encountered are mainly to do with statistical analysis,
haven't really come across examples on encapsulation, inheritance,
polymorphism etc etc with R. Thanks again for the great help!

Best regards
Ian Seow
GIC


On 1/7/08, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On Jan 7, 2008 6:12 AM, Ian Seow <ianseow at gmail.com> wrote:
> > Hi guys, currently the zoo package extends the group generic functions
> > Ops to perform operations only for the **intersection** of the indexes
> > of the objects. Was just wondering if anyone has came across an
> > elegant/simple solution for operations on the union of the indexes?
> > I'm trying to look for a clean way of adding the signals of my
> > different trading models which return zoo time-series with indexes
> > which are not necessary the same. Unfortunately, the only way I can
> > think of is to merge the zoo objects, obtain the union of the indexes,
> > prepend / append each zoo object with zeros, and then add them. This
> > looks really ugly especially when there are many objects to be added
> > :(
> >
> > Brief example:
> >
> > > model1.output
> >
> >               EURUSD   USDJPY
> > 2007-12-27 -19.470868 32.79096
> > 2007-12-28 -17.159300 32.79096
> > 2008-01-02 -17.162815 32.79096
> > 2008-01-03  -5.701861 10.93032
> > 2008-01-04  -5.701861 10.93032
> >
> > > model2.output
> >
> >               EURUSD   USDJPY
> > 2007-12-27 -20.470868 11.79096
> > 2007-12-28 -20.159300 11.79096
> > 2008-01-02 -20.162815 11.79096
> >
> > model1.output + model2.output would return a zoo object starting from
> > 2007-12-27 to 2008-01-02. :(
> > Any tips on this would be greatly appreciated!
> > Thanks!
>
> It would be enough to define a subclass of zoo with its own
> constructor function and
> an as function for conversions:
>
> > Ops.zooall <- function (e1, e2)
> + {
> +     e <- if (missing(e2)) {
> +         NextMethod(.Generic)
> +     }
> +     else if (any(nchar(.Method) == 0)) {
> +         NextMethod(.Generic)
> +     }
> +     else {
> + merge(e1, e2, all = TRUE, fill = 0, retclass = NULL)
> +         NextMethod(.Generic)
> +     }
> +     if (is.null(attr(e, "index")))
> + zooall(e, index(e1), attr(e1, "frequency"))
> +     else
> + e
> + }
> >
> > zooall <- function(x = NULL, order = index(x), frequency = NULL) {
> + structure(zoo(x = x, order = order, frequency = frequency),
> + class = c("zooall", "zoo"))
> + }
> >
> > as.zooall <- function(x, ...) UseMethod("as.zooall")
> > as.zooall.zoo <- function(x, ...) structure(x, class = c("zooall", "zoo"))
> >
> > # now using your zoo data:
> >
> > model1.output <-
> + structure(c(-19.470868, -17.1593, -17.162815, -5.701861, -5.701861,
> + 32.79096, 32.79096, 32.79096, 10.93032, 10.93032), .Dim = c(5L,
> + 2L), .Dimnames = list(NULL, c("V2", "V3")), index = structure(c(13874,
> + 13875, 13880, 13881, 13882), class = "Date"), class = "zoo")
> >
> > model2.output <-
> + structure(c(-20.470868, -20.1593, -20.162815, 11.79096, 11.79096,
> + 11.79096), .Dim = c(3L, 2L), .Dimnames = list(NULL, c("V2", "V3"
> + )), index = structure(c(13874, 13875, 13880), class = "Date"), class = "zoo")
> >
> > model1.output.all <- as.zooall(model1.output)
> > model2.output.all <- as.zooall(model2.output)
> > model1.output.all + model2.output.all
>
> 2007-12-27 -39.941736 44.58192
> 2007-12-28 -37.318600 44.58192
> 2008-01-02 -37.325630 44.58192
> 2008-01-03  -5.701861 10.93032
> 2008-01-04  -5.701861 10.93032
> >
> > model1.output + model2.output
>                  V2       V3
> 2007-12-27 -39.94174 44.58192
> 2007-12-28 -37.31860 44.58192
> 2008-01-02 -37.32563 44.58192
>


From ggrothendieck at gmail.com  Mon Jan  7 16:53:25 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 7 Jan 2008 10:53:25 -0500
Subject: [R-SIG-Finance] Ops on zoo objects
In-Reply-To: <1865010801070740s1e5fcb95mda852280f4e50368@mail.gmail.com>
References: <1865010801070312td943fd7r767d7c49c7f8b24b@mail.gmail.com>
	<971536df0801070609s5d57b391l6ad3ccc55eca401e@mail.gmail.com>
	<1865010801070740s1e5fcb95mda852280f4e50368@mail.gmail.com>
Message-ID: <971536df0801070753j3b147929y674f0d616a430cbb@mail.gmail.com>

On Jan 7, 2008 10:40 AM, Ian Seow <ianseow at gmail.com> wrote:
> Amazing!! Thank you very much Gabor for your excellent suggestion...
> certainly saved me lots of hair-pulling :) Btw, are there any
> resources where I can read up about OOP with R? Esp. the syntax used
> here in this solution... it's pretty new to me... so far most of the R
> code I've encountered are mainly to do with statistical analysis,
> haven't really come across examples on encapsulation, inheritance,
> polymorphism etc etc with R. Thanks again for the great help!
>

MASS has a brief discussion of S3 which should be sufficient.
Beyond that read the source of zoo or other S3 package.


From ecjbosu at aol.com  Mon Jan  7 17:34:49 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Mon, 7 Jan 2008 16:34:49 +0000 (UTC)
Subject: [R-SIG-Finance] rcopula.gauss problem with
	sum(diag(correlationmatrix))
Message-ID: <loom.20080107T162425-659@post.gmane.org>

>
>

All,

I keep getting rcopula.gauss to error out due to machine/R significant digits. 
The test in the methods sum(diag(sigma))==d fails because the one of my diag
elements is not quite 1.  I had to execute a ceiling function on the diag
elements of the correlation matrix to get the method not to fail.  

Is this a problem for anyone else?

My example is
              
              
              require(VGAM)
              require(QRMlib)
              require(gamlss)
              require(fCopulae)
              require(fGarch)
              require(copula)
              require(date)
              require(rgl)
              
              df<-7.5
              sig1<-.5
              var2<-2.5
              seed<-1234
              set.seed(seed);
              #base on sas code from web site
              #generate t distributed data
              tdata<-data.frame(date=seq(as.numeric(as.date('06/01/2001')),
                as.numeric(as.date('11/01/2002')),1))
              tdata$tdata<-.05*tdata$date+rTF(length(tdata$date),mu=5000,
              sigma=sig1,nu=df)
              
              #estimate model data~a * date + constant
                  fam<-TF
                  y1<-gamlss(tdata~date+1,data=tdata,family=fam,na.rm=T,
                    nu.start=16.58,sigma.formula=~1
                    ,nu.formula=~1)
                  y1<-gamlss(tdata~date+1,data=tdata,family=
                  TF(mu.link = identity,sigma.link = log, nu.link = log))
                  #plot(y1,na.rm=T)
                  
               
               #generate normal garch distributed data
               set.seed(12345)
                ndata<-data.frame(date=seq(as.numeric(as.date('06/01/2001')),
                  as.numeric(as.date('11/01/2002')),1))
                ndata$rnd<-rnorm(length(ndata$date))
                ndata$v<-NaN
                ndata$e<-NaN
                ndata$v[1]<-.0001+.25*var2^2 +.75* var2
                ndata$e[1]<-sqrt(ndata$v[1])*ndata$rnd[1]
                for (i in 2:length(ndata$date) ) {
                  ndata$v[i]<- 0.0001 + 0.2 * ndata$e[(i - 1)]^2 + 0.75 * 
                    ndata$v[(i - 1)]
                  ndata$e[i]<-sqrt(ndata$v[i])+ndata$rnd[i]
                }
                gs<-garchSim(model = list( mu=25,omega = .0001, alpha = .2, 
                beta = .75), n = length(ndata$date), 
                  n.start = 100, presample = NULL, cond.dist = c("rnorm"), 
                  rseed = 12345)
              
                # add constant 25
                ndata$ndata<-as.numeric(gs); #25+ndata$e
                # estimate garch 1,1 model of normally distributed data
                yn<-garchFit(ndata~garch(1,1),data=ndata,trace=F)
               
               #y = rcauchy(519, loc=-4, scale=1)
               # generate cauchy data
               set.seed(seed);
               cdata<-data.frame(date=seq(as.numeric(as.date('06/01/2001')),
                as.numeric(as.date('11/01/2002')),1))
               cauch<-unlist(-4+tan(runif(length(cdata$date))-.5)*pi)
               set.seed(seed);
               cdata$cdata<-rcauchy(length(cdata$date),loc=-4,scale=1)
               #estimate cauchy model
               yc<-vglm(cdata ~ 1 , data=cdata,cauchy1(lloc="identity"), 
               trace=TRUE, crit="c")
               
               
               
               #create transformed uniform residuals for all estimated 
               #series' residuals
               resids<-data.frame(date=tdata$date,tresids=y1$residuals,
                nresids=yn at residuals,cresids=yc at residuals)
               resids$tresids<-pnorm(pTF(resids$tresids/
                exp(y1$sigma.coefficient),y1$nu.coefficients))
               resids$cresids<-pnorm(pcauchy(resids$cresids,yc at coefficients[1]))
               resids$nresids<-pnorm(resids$nresids,mean(resids$nresids),
                sd=sd(resids$nresids))
               yall<-merge(tdata,merge(cdata,ndata,by.x="date",by.y="date",
                all=T),by.x="date",by.y="date",all=T)
               rnds<-data.frame(rmvnorm(2000,sigma=cov(resids[-1]),
                mean=mean(resids[-1])))
               names(rnds)<-c('T',"Normal",'Cauchy')
              #fit a normal copula to the uniform residuals 
               cop.fit<-fit.norm(resids[,2:4]) 
              #simulate the normal copula
               cop.sim1<-rmnorm(2000, Sigma=cop.fit$Sigma, mu=cop.fit$mu)
               
 ## to get rcopula.gauss to not fail
# diag(cop.fit$cor)=ceil(diag(cop.fit$cor))
 #uncomment the above line to get rcopula.gauss to work
 cop.sim<-rcopula.gauss(2000, Sigma=cop.fit$cor)
 
           ####################
           sims<-data.frame(cop.sim)
           names(sims)<- c('T',"Normal",'Cauchy')
           # for rmnorm
           #sims$T<-qTF(pnorm(sims$T),mu=y1$mu.coefficients[1],
           # sigma=exp(y1$sigma.coefficients), nu=y1$nu.coefficients)
           #sims$Normal<-qnorm(pnorm(sims$Normal),mean(resids$nresids),
           #sd=sd(resids$nresids))
           #sims$Cauchy<-qcauchy(pnorm(sims$Cauchy),location=yc at coefficients[1])
           #sims$T<-seq(as.numeric(as.date('11/01/2002'))+1,
           #as.numeric(as.date('11/01/2002'))+2000,1) *
           #  y1$mu.coefficients[2] + sims$T
           # for rcopula.gauss
           sims$T<-qTF(sims$T,mu=y1$mu.coefficients[1],
            sigma=exp(y1$sigma.coefficients), nu=y1$nu.coefficients)
           sims$Normal<-qnorm(sims$Normal,mean(resids$nresids),
            sd=sd(resids$nresids))
           sims$Cauchy<-qcauchy(sims$Cauchy,location=yc at coefficients[1])
           sims$T<-seq(as.numeric(as.date('11/01/2002'))+1,
            as.numeric(as.date('11/01/2002'))+2000,1) *
             y1$mu.coefficients[2] + sims$T
          
          f2<-kde2d(sims$T,sims$Cauchy,lims=c(range(sims$T),c(-150,150)))
          open3d()
          bg3d("white")
          material3d(col="black")
           persp3d(x=f2$x,y=f2$y,z=f2$z, aspect=c(1, 1, 0.5), col = "lightblue",
            xlab='T',ylab='Cauchy')


From Achim.Zeileis at wu-wien.ac.at  Mon Jan  7 17:45:41 2008
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 7 Jan 2008 17:45:41 +0100 (CET)
Subject: [R-SIG-Finance] Ops on zoo objects
In-Reply-To: <971536df0801070609s5d57b391l6ad3ccc55eca401e@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0801071741380.5863-100000@disco.wu-wien.ac.at>

On Mon, 7 Jan 2008, Gabor Grothendieck wrote:

> On Jan 7, 2008 6:12 AM, Ian Seow <ianseow at gmail.com> wrote:
> > Hi guys, currently the zoo package extends the group generic functions
> > Ops to perform operations only for the **intersection** of the indexes
> > of the objects. Was just wondering if anyone has came across an
> > elegant/simple solution for operations on the union of the indexes?
> > I'm trying to look for a clean way of adding the signals of my
> > different trading models which return zoo time-series with indexes
> > which are not necessary the same. Unfortunately, the only way I can
> > think of is to merge the zoo objects, obtain the union of the indexes,
> > prepend / append each zoo object with zeros, and then add them. This
> > looks really ugly especially when there are many objects to be added
> > :(
> >
> > Brief example:
> >
> > > model1.output
> >
> >               EURUSD   USDJPY
> > 2007-12-27 -19.470868 32.79096
> > 2007-12-28 -17.159300 32.79096
> > 2008-01-02 -17.162815 32.79096
> > 2008-01-03  -5.701861 10.93032
> > 2008-01-04  -5.701861 10.93032
> >
> > > model2.output
> >
> >               EURUSD   USDJPY
> > 2007-12-27 -20.470868 11.79096
> > 2007-12-28 -20.159300 11.79096
> > 2008-01-02 -20.162815 11.79096
> >
> > model1.output + model2.output would return a zoo object starting from
> > 2007-12-27 to 2008-01-02. :(
> > Any tips on this would be greatly appreciated!
> > Thanks!

In addition to Gabor's more general solution, you could also take a
somewhat simpler (but less general) solution and simply extend both series
to the same time grid:
  merge(model1.output, model2.output)
and then compute with the results from that.

In this particular case, it would be sufficient to just extend
model2.ouput, e.g., via
  model2.output <- merge(zoo(,time(model1.output)), model2.output)

hth,
Z

> It would be enough to define a subclass of zoo with its own
> constructor function and
> an as function for conversions:
>
> > Ops.zooall <- function (e1, e2)
> + {
> +     e <- if (missing(e2)) {
> +         NextMethod(.Generic)
> +     }
> +     else if (any(nchar(.Method) == 0)) {
> +         NextMethod(.Generic)
> +     }
> +     else {
> + merge(e1, e2, all = TRUE, fill = 0, retclass = NULL)
> +         NextMethod(.Generic)
> +     }
> +     if (is.null(attr(e, "index")))
> + zooall(e, index(e1), attr(e1, "frequency"))
> +     else
> + e
> + }
> >
> > zooall <- function(x = NULL, order = index(x), frequency = NULL) {
> + structure(zoo(x = x, order = order, frequency = frequency),
> + class = c("zooall", "zoo"))
> + }
> >
> > as.zooall <- function(x, ...) UseMethod("as.zooall")
> > as.zooall.zoo <- function(x, ...) structure(x, class = c("zooall", "zoo"))
> >
> > # now using your zoo data:
> >
> > model1.output <-
> + structure(c(-19.470868, -17.1593, -17.162815, -5.701861, -5.701861,
> + 32.79096, 32.79096, 32.79096, 10.93032, 10.93032), .Dim = c(5L,
> + 2L), .Dimnames = list(NULL, c("V2", "V3")), index = structure(c(13874,
> + 13875, 13880, 13881, 13882), class = "Date"), class = "zoo")
> >
> > model2.output <-
> + structure(c(-20.470868, -20.1593, -20.162815, 11.79096, 11.79096,
> + 11.79096), .Dim = c(3L, 2L), .Dimnames = list(NULL, c("V2", "V3"
> + )), index = structure(c(13874, 13875, 13880), class = "Date"), class = "zoo")
> >
> > model1.output.all <- as.zooall(model1.output)
> > model2.output.all <- as.zooall(model2.output)
> > model1.output.all + model2.output.all
>
> 2007-12-27 -39.941736 44.58192
> 2007-12-28 -37.318600 44.58192
> 2008-01-02 -37.325630 44.58192
> 2008-01-03  -5.701861 10.93032
> 2008-01-04  -5.701861 10.93032
> >
> > model1.output + model2.output
>                   V2       V3
> 2007-12-27 -39.94174 44.58192
> 2007-12-28 -37.31860 44.58192
> 2008-01-02 -37.32563 44.58192
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From guy.yollin at rotellacapital.com  Tue Jan  8 17:08:06 2008
From: guy.yollin at rotellacapital.com (Guy Yollin)
Date: Tue, 8 Jan 2008 11:08:06 -0500
Subject: [R-SIG-Finance] R - Error Question for
	chart.Correlation(PerformanceAnalytics)
In-Reply-To: <loom.20080102T171756-520@post.gmane.org>
References: <loom.20080102T171756-520@post.gmane.org>
Message-ID: <E634AF2410E42246A35865D8C0C784D90FC85E@MI8NYCMAIL09.Mi8.com>

Hi Michel,

I defer to the original authors of this very useful package, but I
believe it is simply a coding error.

In the last few lines of the chart.Correlation function there are a
couple of calls to the pairs function:

	pairs(x, y, gap = 0, lower.panel = ...

I believe x and y should be bound together in the call as follows:

	pairs(cbind(x,y), gap = 0, lower.panel = ...

Essentially, that was what you were doing with your 1:3.

Also, it appears that a couple of the datasets (managers and edhec) have
what should really be the rownames as the first column of data in the
dataframe.  I did the following to get some of the examples to work:

rn <- edhec[,1]
edhec <- edhec[,-1]
rownames(edhec) <- rn

rn <- managers[,1]
managers <- managers[,-1]
rownames(mananagers) <- rn

Hope this is helpful.

Best,

-- G


-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of MAB
Sent: Wednesday, January 02, 2008 9:27 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] R - Error Question for
chart.Correlation(PerformanceAnalytics)

Hi!

I am a new user of this user group. Apologies if this is question has an
obvious
answer.


While I ran the below function from PerformanceAnalytics (edhec is a
matrix
provided with the library), this is the error I get

chart.Correlation(edhec[,1],edhec[,3])
Error in pairs.default(x, y, gap = 0, lower.panel = panel.smooth,
upper.panel =
panel.cor,  : 
        only one column in the argument to 'pairs'


When I run the below, things work

chart.Correlation(edhec[,1:3],edhec[,1:3])

Any idea?

MAB

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From peter at braverock.com  Tue Jan  8 18:39:40 2008
From: peter at braverock.com (Peter Carl)
Date: Tue, 8 Jan 2008 11:39:40 -0600
Subject: [R-SIG-Finance] R - Error Question for
	chart.Correlation(PerformanceAnalytics)
In-Reply-To: <E634AF2410E42246A35865D8C0C784D90FC85E@MI8NYCMAIL09.Mi8.com>
References: <loom.20080102T171756-520@post.gmane.org>
	<E634AF2410E42246A35865D8C0C784D90FC85E@MI8NYCMAIL09.Mi8.com>
Message-ID: <200801081139.40783.peter@braverock.com>

On Tuesday 08 January 2008 10:08:06 am Guy Yollin wrote:
> Also, it appears that a couple of the datasets (managers and edhec) have
> what should really be the rownames as the first column of data in the
> dataframe.

Can you clarify this?  How are you loading the data?  The examples, which are 
all run (tested) as part of the submission to CRAN, simply load the data 
using: 

> data(managers)
> head(managers)
            HAM1 HAM2    HAM3    HAM4 HAM5 HAM6 EDHEC.LS.EQ SP500.TR US.10Y.TR
Jan 1996  0.0100   NA  0.0359  0.0208   NA   NA          NA   0.0340   0.00380
Feb 1996  0.0215   NA  0.0295  0.0231   NA   NA          NA   0.0093  -0.03532
Mar 1996  0.0226   NA  0.0253 -0.0053   NA   NA          NA   0.0096  -0.01057
Apr 1996  0.0008   NA  0.0478  0.0200   NA   NA          NA   0.0147  -0.01739
May 1996  0.0158   NA  0.0337  0.0122   NA   NA          NA   0.0258  -0.00543
Jun 1996 -0.0086   NA -0.0293 -0.0089   NA   NA          NA   0.0038   0.01507
         US.3m.TR
Jan 1996  0.00456
Feb 1996  0.00398
Mar 1996  0.00371
Apr 1996  0.00428
May 1996  0.00443
Jun 1996  0.00412

If you use read.csv, you'll want to set row.names=1 so to capture the 
appropriate label.

Also, these data objects are now zoo objects and degrade nicely with as.matrix 
and other similar functions.

pcc
-- 
Peter Carl
145 Scottswood Rd
Riverside, IL 60546
708 447 6465


From brian at braverock.com  Tue Jan  8 18:40:30 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 08 Jan 2008 11:40:30 -0600
Subject: [R-SIG-Finance] R - Error Question
	for	chart.Correlation(PerformanceAnalytics)
In-Reply-To: <E634AF2410E42246A35865D8C0C784D90FC85E@MI8NYCMAIL09.Mi8.com>
References: <loom.20080102T171756-520@post.gmane.org>
	<E634AF2410E42246A35865D8C0C784D90FC85E@MI8NYCMAIL09.Mi8.com>
Message-ID: <4783B58E.9010009@braverock.com>

Guy,

Thanks for your constructive comments on chart.Correlation and the data 
sets included with PerformanceAnalytics.

Guy Yollin wrote:
<... snip chart.Correlation bits ...>
I'll defer to Peter to comment on (fixing) the coding in 
chart.Correlation, as I am admittedly a novice on graphics in R (I focus 
on the analytical functions in PerformanceAnalytics).

I wanted to address this point from Guy:
> Also, it appears that a couple of the datasets (managers and edhec) have
> what should really be the rownames as the first column of data in the
> dataframe.  I did the following to get some of the examples to work:

I'm a little confused by this, as

rownames(edhec)
or
rownames(managers)

returns the rownames correctly:
 > data(edhec)
 > rownames(edhec)
[1] "1997-01-31" "1997-02-28" "1997-03-31" "1997-04-30" "1997-05-31"
[6] "1997-06-30" "1997-07-31" "1997-08-31" "1997-09-30" "1997-10-31"
<...>

So, can you give an example of something that fails for you, which R 
version you're using, etc.?

Thanks,

    - Brian


From guy.yollin at rotellacapital.com  Tue Jan  8 20:04:01 2008
From: guy.yollin at rotellacapital.com (Guy Yollin)
Date: Tue, 8 Jan 2008 14:04:01 -0500
Subject: [R-SIG-Finance] R - Error Question for
	chart.Correlation(PerformanceAnalytics)
In-Reply-To: <200801081139.40783.peter@braverock.com>
References: <loom.20080102T171756-520@post.gmane.org>
	<E634AF2410E42246A35865D8C0C784D90FC85E@MI8NYCMAIL09.Mi8.com>
	<200801081139.40783.peter@braverock.com>
Message-ID: <E634AF2410E42246A35865D8C0C784D90FC860@MI8NYCMAIL09.Mi8.com>

Interesting, I wasn't calling data(managers), I was just calling
library(PerformanceAnalytics) and then accessing the dataframe:

> head(managers)
   X.Y..m..d    HAM1 HAM2    HAM3    HAM4 HAM5 HAM6 EDHEC.LS.EQ SP500.TR
1 1996-01-31  0.0074   NA  0.0349  0.0222   NA   NA          NA   0.0340
2 1996-02-29  0.0193   NA  0.0351  0.0195   NA   NA          NA   0.0093
3 1996-03-31  0.0155   NA  0.0258 -0.0098   NA   NA          NA   0.0096
4 1996-04-30 -0.0091   NA  0.0449  0.0236   NA   NA          NA   0.0147
5 1996-05-31  0.0076   NA  0.0353  0.0028   NA   NA          NA   0.0258
6 1996-06-30 -0.0039   NA -0.0303 -0.0019   NA   NA          NA   0.0038
  US.10Y.TR US.3m.TR
1   0.00380  0.00456
2  -0.03532  0.00398
3  -0.01057  0.00371
4  -0.01739  0.00428
5  -0.00543  0.00443
6   0.01507  0.00412

I see that calling data(managers) first supplies the dataframe with the
expected format; thanks for the clarification.  I don't know enough
about building packages to know if this is the expected behavior or not.

Best,

-- G

-----Original Message-----
From: Peter Carl [mailto:peter at braverock.com] 
Sent: Tuesday, January 08, 2008 9:40 AM
To: r-sig-finance at stat.math.ethz.ch
Cc: Guy Yollin; MAB
Subject: Re: [R-SIG-Finance] R - Error Question for
chart.Correlation(PerformanceAnalytics)

On Tuesday 08 January 2008 10:08:06 am Guy Yollin wrote:
> Also, it appears that a couple of the datasets (managers and edhec)
have
> what should really be the rownames as the first column of data in the
> dataframe.

Can you clarify this?  How are you loading the data?  The examples,
which are 
all run (tested) as part of the submission to CRAN, simply load the data

using: 

> data(managers)
> head(managers)
            HAM1 HAM2    HAM3    HAM4 HAM5 HAM6 EDHEC.LS.EQ SP500.TR
US.10Y.TR
Jan 1996  0.0100   NA  0.0359  0.0208   NA   NA          NA   0.0340
0.00380
Feb 1996  0.0215   NA  0.0295  0.0231   NA   NA          NA   0.0093
-0.03532
Mar 1996  0.0226   NA  0.0253 -0.0053   NA   NA          NA   0.0096
-0.01057
Apr 1996  0.0008   NA  0.0478  0.0200   NA   NA          NA   0.0147
-0.01739
May 1996  0.0158   NA  0.0337  0.0122   NA   NA          NA   0.0258
-0.00543
Jun 1996 -0.0086   NA -0.0293 -0.0089   NA   NA          NA   0.0038
0.01507
         US.3m.TR
Jan 1996  0.00456
Feb 1996  0.00398
Mar 1996  0.00371
Apr 1996  0.00428
May 1996  0.00443
Jun 1996  0.00412

If you use read.csv, you'll want to set row.names=1 so to capture the 
appropriate label.

Also, these data objects are now zoo objects and degrade nicely with
as.matrix 
and other similar functions.

pcc
-- 
Peter Carl
145 Scottswood Rd
Riverside, IL 60546
708 447 6465


From peter at braverock.com  Tue Jan  8 20:53:27 2008
From: peter at braverock.com (Peter Carl)
Date: Tue, 8 Jan 2008 13:53:27 -0600
Subject: [R-SIG-Finance] R - Error Question for
	chart.Correlation(PerformanceAnalytics)
In-Reply-To: <E634AF2410E42246A35865D8C0C784D90FC860@MI8NYCMAIL09.Mi8.com>
References: <loom.20080102T171756-520@post.gmane.org>
	<200801081139.40783.peter@braverock.com>
	<E634AF2410E42246A35865D8C0C784D90FC860@MI8NYCMAIL09.Mi8.com>
Message-ID: <200801081353.27211.peter@braverock.com>

On Tuesday 08 January 2008 1:04:01 pm Guy Yollin wrote:
> ?I don't know enough
> about building packages to know if this is the expected behavior or not.

Well, it isn't and it shouldn't work that way (although I've verified that, in 
fact, it does work as you say).  

The issue seems to be some namespace confusion.  fEcofin, which is 
a "required" package for fBasics or some of the other RMetrics packages 
(which we in turn require), has recently added an older version of our data 
objects to its list.  Spencer has recently pointed out the namespace 
collision and I'm sure we'll work with Diethelm to figure out a way around it 
in the next version.

> head(fEcofin::managers)
   X.Y..m..d    HAM1 HAM2    HAM3    HAM4 HAM5 HAM6 EDHEC.LS.EQ SP500.TR
1 1996-01-31  0.0074   NA  0.0349  0.0222   NA   NA          NA   0.0340
2 1996-02-29  0.0193   NA  0.0351  0.0195   NA   NA          NA   0.0093
3 1996-03-31  0.0155   NA  0.0258 -0.0098   NA   NA          NA   0.0096
4 1996-04-30 -0.0091   NA  0.0449  0.0236   NA   NA          NA   0.0147
5 1996-05-31  0.0076   NA  0.0353  0.0028   NA   NA          NA   0.0258
6 1996-06-30 -0.0039   NA -0.0303 -0.0019   NA   NA          NA   0.0038
  US.10Y.TR US.3m.TR
1   0.00380  0.00456
2  -0.03532  0.00398
3  -0.01057  0.00371
4  -0.01739  0.00428
5  -0.00543  0.00443
6   0.01507  0.00412

At this point, the best way to make sure you're working with the correct data 
is to load it explicitly after loading PerformanceAnalytics.  (RMetrics loads 
in all of the data when the package loads.)

> library(PerformanceAnalytics)
> data(managers)
> head(managers)
            HAM1 HAM2    HAM3    HAM4 HAM5 HAM6 EDHEC.LS.EQ SP500.TR US.10Y.TR
Jan 1996  0.0100   NA  0.0359  0.0208   NA   NA          NA   0.0340   0.00380
Feb 1996  0.0215   NA  0.0295  0.0231   NA   NA          NA   0.0093  -0.03532
Mar 1996  0.0226   NA  0.0253 -0.0053   NA   NA          NA   0.0096  -0.01057
Apr 1996  0.0008   NA  0.0478  0.0200   NA   NA          NA   0.0147  -0.01739
May 1996  0.0158   NA  0.0337  0.0122   NA   NA          NA   0.0258  -0.00543
Jun 1996 -0.0086   NA -0.0293 -0.0089   NA   NA          NA   0.0038   0.01507
         US.3m.TR
Jan 1996  0.00456
Feb 1996  0.00398
Mar 1996  0.00371
Apr 1996  0.00428
May 1996  0.00443
Jun 1996  0.00412

Thanks very much for pointing out the issue, and I apologize for the 
confusion.

pcc
-- 
Peter Carl
145 Scottswood Rd
Riverside, IL 60546
708 447 6465


From chalabi at phys.ethz.ch  Wed Jan  9 10:49:15 2008
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Wed, 09 Jan 2008 10:49:15 +0100
Subject: [R-SIG-Finance] PerformanceAnalytics version 0.9.6 released to
 CRAN
In-Reply-To: <477CE1FC.6090607@aol.com>
References: <4778D9A1.4060806@braverock.com>	<477C713B.3030301@aol.com>	<18300.49962.24787.337166@ron.nulle.part>
	<477CE1FC.6090607@aol.com>
Message-ID: <4784989B.5010505@phys.ethz.ch>

Joe W. Byers wrote:
> Dirk Eddelbuettel wrote:
>   
>> On 2 January 2008 at 23:23, Joe W. Byers wrote:
>> | Great looking package!  I was installing it on my linux server using my 
>> | VNC connection and a terminal window as su root, so I was only using the 
>> | command windows, when I encountered an error with the install.  The 
>> | package requires tclk which will not load unless the display is set 
>>
>> This is due to one of the Rmetrics packages, I have forgotten which, that is
>> loading tcltk.
>>
>> | correctly on the linux terminal shell.  I had to physically log on to 
>> | the linux machine to get it to install.  I am not sure if there is a 
>> | workaround or if you can change the package to only load the tclk 
>> | library as needed, not depend on it.  This is not a major issue for me 
>>
>> One way around is to modify the package's DESCRIPTION file and to set
>> LazyLoad to 'no' --- you can do that with a Perl one-liner after expanding
>> the tarball.  That's basically what I used to do for the Debian packages as
>> they are 'assembled' in a chroot without $DISPLAY.
>>
>> The more elegant way around we use now is to use xvfb-run as suggested by
>> Charles Plessy: it invokes a virtual X11 session using xvfb. One Debian, this
>> means also installing the "xvfb, xbase-clients, xfonts-base" packages and to
>> then call
>>
>>      xvfb-run R CMD INSTALL -l $(InstalLDir) --clean $(Target)
>>
>> instead of just 
>>
>>     R CMD INSTALL -l $(InstalLDir) --clean $(Target)
>>
>> You should be able to work that out with your sysadmin if need be. 
>>   
>>     
> This is on my server at my home, so I am the sysadmin :)
>   
>> Hth, Dirk
>>
>> | but being able to remotely connect and run a superuser shell is 
>> | convenient.  This will impact anyone that has a cron job that 
>> | automatically updates the R packages on machines.
>>
>>
>>
>>   
>>     
> Dirk,  I appreciate the help and Happy New year
>
> Thanx Joe
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>   
Hi Joe,

tcltk is loaded by fUtilities package in Rmetrics. I changed it and 
tcltk is now a suggested package and is no more required. So, there 
should be no problem to install future versions of Rmetrics on server 
without X11 or xvfb support.

regards,
Yohan


From markus at insightfromdata.com  Wed Jan  9 17:15:42 2008
From: markus at insightfromdata.com (Markus Loecher)
Date: Wed, 09 Jan 2008 11:15:42 -0500
Subject: [R-SIG-Finance] merge zoo
Message-ID: <C3AA5D5E.4F2%markus@insightfromdata.com>

Dear zoo users,
I have used merge() succesfully to combine zoo objects uncountable times but
am running into a new issue.
When I call 
    merge(x1,x2)
Where x1 and x2 are two zoo objects I see the following error message:
    Error in z[match0(index(a), indexes), ] <- a[match0(indexes, index(a))]
: 
    number of items to replace is not a multiple of replacement length

I am quite stuck, any help would be greatly appreciated !

Thanks,
Markus


From ggrothendieck at gmail.com  Wed Jan  9 17:30:22 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 9 Jan 2008 11:30:22 -0500
Subject: [R-SIG-Finance] merge zoo
In-Reply-To: <C3AA5D5E.4F2%markus@insightfromdata.com>
References: <C3AA5D5E.4F2%markus@insightfromdata.com>
Message-ID: <971536df0801090830v5f18cf54m769611166669a0ed@mail.gmail.com>

This is not enough to go on.  You need to provide a minimal
reproducible example.


On Jan 9, 2008 11:15 AM, Markus Loecher <markus at insightfromdata.com> wrote:
> Dear zoo users,
> I have used merge() succesfully to combine zoo objects uncountable times but
> am running into a new issue.
> When I call
>    merge(x1,x2)
> Where x1 and x2 are two zoo objects I see the following error message:
>    Error in z[match0(index(a), indexes), ] <- a[match0(indexes, index(a))]
> :
>    number of items to replace is not a multiple of replacement length
>
> I am quite stuck, any help would be greatly appreciated !
>
> Thanks,
> Markus
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Wed Jan  9 18:53:57 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 9 Jan 2008 12:53:57 -0500
Subject: [R-SIG-Finance] merge zoo
In-Reply-To: <971536df0801090830v5f18cf54m769611166669a0ed@mail.gmail.com>
References: <C3AA5D5E.4F2%markus@insightfromdata.com>
	<971536df0801090830v5f18cf54m769611166669a0ed@mail.gmail.com>
Message-ID: <971536df0801090953i596161ecq6802bbdd756169c1@mail.gmail.com>

Just to follow up the user's data had illegal duplicates and zoo does issue
a warning in that case.

On Jan 9, 2008 11:30 AM, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> This is not enough to go on.  You need to provide a minimal
> reproducible example.
>
>
>
> On Jan 9, 2008 11:15 AM, Markus Loecher <markus at insightfromdata.com> wrote:
> > Dear zoo users,
> > I have used merge() succesfully to combine zoo objects uncountable times but
> > am running into a new issue.
> > When I call
> >    merge(x1,x2)
> > Where x1 and x2 are two zoo objects I see the following error message:
> >    Error in z[match0(index(a), indexes), ] <- a[match0(indexes, index(a))]
> > :
> >    number of items to replace is not a multiple of replacement length
> >
> > I am quite stuck, any help would be greatly appreciated !
> >
> > Thanks,
> > Markus
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From feanor0 at hotmail.com  Thu Jan 10 12:14:09 2008
From: feanor0 at hotmail.com (Murali Menon)
Date: Thu, 10 Jan 2008 11:14:09 +0000
Subject: [R-SIG-Finance] business day to monthly or quarterly aggregation.
Message-ID: <BLU105-W24B37DF8CC66EC11704441EE4A0@phx.gbl>


Folks,

I have a matrix of portfolio weights whose rownames are daily dates (weekends elided).
I'd like to create a new matrix of portfolio weights such that the weight on the first
day of the month (quarter) is copied to the rest of the month (quarter). The idea is 
that I rebalance my portfolio only on the first of the month (quarter). Is there a 
neat way to do this?

The reason I'm thinking of copying weights till the end of each period is that then,
in order to compute rebalancing costs, I just need to do a abs(diff) on the weights matrix and multiply by the transaction cost.

I guess if I can just convert the vector of dates into a vector of repeated first-of-period
dates, I can use this vector to index into the portfolio weights matrix. But I can't
think how I could achieve this vector conversion either.

Any suggestions?

Thanks,
Murali


ourdates <- c("1997-01-29", "1997-01-30", "1997-01-31", "1997-02-03", "1997-02-04", 
"1997-02-05", "1997-02-06", "1997-02-07", "1997-02-10", "1997-02-11", 
"1997-02-12", "1997-02-13", "1997-02-14", "1997-02-17", "1997-02-18", 
"1997-02-19", "1997-02-20", "1997-02-21", "1997-02-24", "1997-02-25", 
"1997-02-26", "1997-02-27", "1997-02-28", "1997-03-03", "1997-03-04", 
"1997-03-05", "1997-03-06", "1997-03-07", "1997-03-10", "1997-03-11", 
"1997-03-12", "1997-03-13", "1997-03-14", "1997-03-17", "1997-03-18", 
"1997-03-19", "1997-03-20", "1997-03-21", "1997-03-24", "1997-03-25", 
"1997-03-26", "1997-03-27", "1997-03-28", "1997-03-31", "1997-04-01", 
"1997-04-02", "1997-04-03", "1997-04-04", "1997-04-07", "1997-04-08", 
"1997-04-09", "1997-04-10", "1997-04-11", "1997-04-14", "1997-04-15", 
"1997-04-16", "1997-04-17", "1997-04-18", "1997-04-21", "1997-04-22", 
"1997-04-23", "1997-04-24", "1997-04-25", "1997-04-28", "1997-04-29", 
"1997-04-30", "1997-05-01", "1997-05-02", "1997-05-05", "1997-05-06", 
"1997-05-07", "1997-05-08", "1997-05-09", "1997-05-12", "1997-05-13", 
"1997-05-14", "1997-05-15", "1997-05-16", "1997-05-19", "1997-05-20", 
"1997-05-21", "1997-05-22", "1997-05-23", "1997-05-26", "1997-05-27", 
"1997-05-28", "1997-05-29", "1997-05-30", "1997-06-02", "1997-06-03", 
"1997-06-04", "1997-06-05", "1997-06-06", "1997-06-09", "1997-06-10", 
"1997-06-11", "1997-06-12", "1997-06-13", "1997-06-16", "1997-06-17"
)

set.seed(123)

pWt <- matrix(rnorm(1000), ncol = 10)

somehow create new vector newdates: ("1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02")

pWt[newdates, ] will give me the repeated monthly weights that I want.

And similarly for repeated quarterly weights.
_________________________________________________________________
Share life as it happens with the new Windows Live.

08

From brian at braverock.com  Thu Jan 10 13:05:40 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 10 Jan 2008 06:05:40 -0600
Subject: [R-SIG-Finance] business day to monthly or quarterly
	aggregation.
In-Reply-To: <BLU105-W24B37DF8CC66EC11704441EE4A0@phx.gbl>
References: <BLU105-W24B37DF8CC66EC11704441EE4A0@phx.gbl>
Message-ID: <47860A14.3060903@braverock.com>

Murali Menon wrote:
> Folks,
> 
> I have a matrix of portfolio weights whose rownames are daily dates (weekends elided).
> I'd like to create a new matrix of portfolio weights such that the weight on the first
> day of the month (quarter) is copied to the rest of the month (quarter). The idea is 
> that I rebalance my portfolio only on the first of the month (quarter). Is there a 
> neat way to do this?

The 'aggregate' function function from zoo comes immediately to mind, 
but seems like overkill for what you're talking about, as no real 
operation needs to be performed on the data series.

The quantmod package has functions that may be more on-point, for 
converting daily price time series to weekly, monthly, or quarterly 
series.  This is much closer to what you describe, as conversion of a 
price series in this manner involves taking the observation at the 
beginning(end) of the period.  The functions are to.weekly, to.monthly, 
and to.quarterly.

Help may be accessed via

install.packages("quantmod")
library("quantmod")
?to.period

Regards,

     - Brian


From robert at sanctumfi.com  Thu Jan 10 14:06:41 2008
From: robert at sanctumfi.com (Robert Sams)
Date: Thu, 10 Jan 2008 13:06:41 -0000
Subject: [R-SIG-Finance] business day to monthly or quarterly
	aggregation.
References: <SANCTUMFISERVERSnRM000000c2@sanctumfi.com>
Message-ID: <SANCTUMFISERVERsokL000000fb@sanctumfi.com>

Hi Murali,

Does this help?

dateparts <- function (x) {
+     list(mday = as.POSIXlt(x)$mday, month = as.POSIXlt(x)$mon + 
+         1, year = as.POSIXlt(x)$year + 1900, wday =
as.POSIXlt(x)$wday)
+ }
> 
> ourdates <- c("1997-01-29", "1997-01-30", "1997-01-31", "1997-02-03",
"1997-02-04", "1997-02-05", "1997-02-06", "1997-02-07", "1997-02-10", 
+               "1997-02-11", "1997-02-12", "1997-02-13", "1997-02-14",
"1997-02-17", "1997-02-18", "1997-02-19", "1997-02-20", "1997-02-21", 
+               "1997-02-24", "1997-02-25", "1997-02-26", "1997-02-27",
"1997-02-28", "1997-03-03", "1997-03-04", "1997-03-05", "1997-03-06", 
+               "1997-03-07", "1997-03-10", "1997-03-11", "1997-03-12",
"1997-03-13", "1997-03-14", "1997-03-17", "1997-03-18", "1997-03-19", 
+               "1997-03-20", "1997-03-21", "1997-03-24", "1997-03-25",
"1997-03-26", "1997-03-27", "1997-03-28", "1997-03-31", "1997-04-01", 
+               "1997-04-02", "1997-04-03", "1997-04-04", "1997-04-07",
"1997-04-08", "1997-04-09", "1997-04-10", "1997-04-11", "1997-04-14", 
+               "1997-04-15", "1997-04-16", "1997-04-17", "1997-04-18",
"1997-04-21", "1997-04-22", "1997-04-23", "1997-04-24", "1997-04-25", 
+               "1997-04-28", "1997-04-29", "1997-04-30", "1997-05-01",
"1997-05-02", "1997-05-05", "1997-05-06", "1997-05-07", "1997-05-08", 
+               "1997-05-09", "1997-05-12", "1997-05-13", "1997-05-14",
"1997-05-15", "1997-05-16", "1997-05-19", "1997-05-20", "1997-05-21", 
+               "1997-05-22", "1997-05-23", "1997-05-26", "1997-05-27",
"1997-05-28", "1997-05-29", "1997-05-30", "1997-06-02", "1997-06-03",  
+               "1997-06-04", "1997-06-05", "1997-06-06", "1997-06-09",
"1997-06-10", "1997-06-11", "1997-06-12", "1997-06-13", "1997-06-16",  
+               "1997-06-17")
> 
> n <- c(1, which(diff(dateparts(ourdates)$month) != 0) + 1,
length(ourdates))
> newdates <- ourdates
> for(i in 1:(length(n) - 1)){
+  newdates[n[i]:n[i+1]] <- ourdates[n[i]]
+ }
> ## vector of start of months..
> newdates
  [1] "1997-01-29" "1997-01-29" "1997-01-29" "1997-02-03" "1997-02-03"
"1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03"
"1997-02-03"
 [12] "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03"
"1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03"
"1997-02-03"
 [23] "1997-02-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"
"1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"
"1997-03-03"
 [34] "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"
"1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"
"1997-03-03"
 [45] "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"
"1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"
"1997-04-01"
 [56] "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"
"1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"
"1997-04-01"
 [67] "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"
"1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"
"1997-05-01"
 [78] "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"
"1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"
"1997-05-01"
 [89] "1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02"
"1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02"
"1997-06-02"
[100] "1997-06-02"
> 
> 

Robert Sams 

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Murali
Menon
Sent: 10 January 2008 11:14
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] business day to monthly or quarterly
aggregation.


Folks,

I have a matrix of portfolio weights whose rownames are daily dates
(weekends elided).
I'd like to create a new matrix of portfolio weights such that the
weight on the first day of the month (quarter) is copied to the rest of
the month (quarter). The idea is that I rebalance my portfolio only on
the first of the month (quarter). Is there a neat way to do this?

The reason I'm thinking of copying weights till the end of each period
is that then, in order to compute rebalancing costs, I just need to do a
abs(diff) on the weights matrix and multiply by the transaction cost.

I guess if I can just convert the vector of dates into a vector of
repeated first-of-period dates, I can use this vector to index into the
portfolio weights matrix. But I can't think how I could achieve this
vector conversion either.

Any suggestions?

Thanks,
Murali


ourdates <- c("1997-01-29", "1997-01-30", "1997-01-31", "1997-02-03",
"1997-02-04", "1997-02-05", "1997-02-06", "1997-02-07", "1997-02-10",
"1997-02-11", "1997-02-12", "1997-02-13", "1997-02-14", "1997-02-17",
"1997-02-18", "1997-02-19", "1997-02-20", "1997-02-21", "1997-02-24",
"1997-02-25", "1997-02-26", "1997-02-27", "1997-02-28", "1997-03-03",
"1997-03-04", "1997-03-05", "1997-03-06", "1997-03-07", "1997-03-10",
"1997-03-11", "1997-03-12", "1997-03-13", "1997-03-14", "1997-03-17",
"1997-03-18", "1997-03-19", "1997-03-20", "1997-03-21", "1997-03-24",
"1997-03-25", "1997-03-26", "1997-03-27", "1997-03-28", "1997-03-31",
"1997-04-01", "1997-04-02", "1997-04-03", "1997-04-04", "1997-04-07",
"1997-04-08", "1997-04-09", "1997-04-10", "1997-04-11", "1997-04-14",
"1997-04-15", "1997-04-16", "1997-04-17", "1997-04-18", "1997-04-21",
"1997-04-22", "1997-04-23", "1997-04-24", "1997-04-25", "1997-04-28",
"1997-04-29", "1997-04-30", "1997-05-01", "1997-05-02", "1997-05-05",
"1997-05-06", "1997-05-07", "1997-05-08", "1997-05-09", "1997-05-12",
"1997-05-13", "1997-05-14", "1997-05-15", "1997-05-16", "1997-05-19",
"1997-05-20", "1997-05-21", "1997-05-22", "1997-05-23", "1997-05-26",
"1997-05-27", "1997-05-28", "1997-05-29", "1997-05-30", "1997-06-02",
"1997-06-03", "1997-06-04", "1997-06-05", "1997-06-06", "1997-06-09",
"1997-06-10", "1997-06-11", "1997-06-12", "1997-06-13", "1997-06-16",
"1997-06-17"
)

set.seed(123)

pWt <- matrix(rnorm(1000), ncol = 10)

somehow create new vector newdates: ("1997-02-03", "1997-02-03",
"1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03",
"1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03",
"1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03",
"1997-02-03", "1997-02-03", "1997-02-03", "1997-03-03", "1997-03-03",
"1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03",
"1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03",
"1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03",
"1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-04-01",
"1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",
"1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",
"1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",
"1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",
"1997-04-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",
"1997-05-01", !
 "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",
"1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",
"1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",
"1997-05-01", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02",
"1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02",
"1997-06-02", "1997-06-02", "1997-06-02")

pWt[newdates, ] will give me the repeated monthly weights that I want.

And similarly for repeated quarterly weights.
_________________________________________________________________
Share life as it happens with the new Windows Live.

08
_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From feanor0 at hotmail.com  Thu Jan 10 16:02:00 2008
From: feanor0 at hotmail.com (Murali Menon)
Date: Thu, 10 Jan 2008 15:02:00 +0000
Subject: [R-SIG-Finance] business day to monthly or quarterly
 aggregation.
In-Reply-To: <SANCTUMFISERVERsokL000000fb@sanctumfi.com>
References: <SANCTUMFISERVERSnRM000000c2@sanctumfi.com>
	<SANCTUMFISERVERsokL000000fb@sanctumfi.com>
Message-ID: <BLU105-W9961A47266A945386B9D5EE4A0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080110/0a777e2a/attachment.pl 

From ggrothendieck at gmail.com  Thu Jan 10 16:23:03 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 10 Jan 2008 10:23:03 -0500
Subject: [R-SIG-Finance] business day to monthly or quarterly
	aggregation.
In-Reply-To: <BLU105-W24B37DF8CC66EC11704441EE4A0@phx.gbl>
References: <BLU105-W24B37DF8CC66EC11704441EE4A0@phx.gbl>
Message-ID: <971536df0801100723s10614492r7488d7dab2ccf25c@mail.gmail.com>

Using zoo, convert the dates to Date class.
Then convert them to yearmon class so every
date in a month will be labelled the same.  Replace
duplicates with NAs and use locf (last observation
carried forward to fill them back in).
library(zoo)

dt <- as.Date(ourdates)
ym <- as.yearmon(dt)
isdup <- duplicated(ym)
dt[isdup] <- NA
na.locf(dt)

To do the above with quarterly data use as.yearqtr instead of as.yearmon.

In your example the first 4 points are missing from the output.  If
that was intentional
then you could retain just those months with at least 20 points, say,
by replacing last
line with:

na.locf(dt)[ ave(seq_along(dt), ym, FUN = length) >= 20 ]

Another thing to consider is it would be easy to pick out just the first date
in each month using !isdup that we calculated above if you wanted to apply
that to the values being weighted rather than repeating the weights.

For more on zoo read the two zoo vignettes:

library(zoo)
vignette("zoo")
vignette("zoo-quickref")

On Jan 10, 2008 6:14 AM, Murali Menon <feanor0 at hotmail.com> wrote:
>
> Folks,
>
> I have a matrix of portfolio weights whose rownames are daily dates (weekends elided).
> I'd like to create a new matrix of portfolio weights such that the weight on the first
> day of the month (quarter) is copied to the rest of the month (quarter). The idea is
> that I rebalance my portfolio only on the first of the month (quarter). Is there a
> neat way to do this?
>
> The reason I'm thinking of copying weights till the end of each period is that then,
> in order to compute rebalancing costs, I just need to do a abs(diff) on the weights matrix and multiply by the transaction cost.
>
> I guess if I can just convert the vector of dates into a vector of repeated first-of-period
> dates, I can use this vector to index into the portfolio weights matrix. But I can't
> think how I could achieve this vector conversion either.
>
> Any suggestions?
>
> Thanks,
> Murali
>
>
> ourdates <- c("1997-01-29", "1997-01-30", "1997-01-31", "1997-02-03", "1997-02-04",
> "1997-02-05", "1997-02-06", "1997-02-07", "1997-02-10", "1997-02-11",
> "1997-02-12", "1997-02-13", "1997-02-14", "1997-02-17", "1997-02-18",
> "1997-02-19", "1997-02-20", "1997-02-21", "1997-02-24", "1997-02-25",
> "1997-02-26", "1997-02-27", "1997-02-28", "1997-03-03", "1997-03-04",
> "1997-03-05", "1997-03-06", "1997-03-07", "1997-03-10", "1997-03-11",
> "1997-03-12", "1997-03-13", "1997-03-14", "1997-03-17", "1997-03-18",
> "1997-03-19", "1997-03-20", "1997-03-21", "1997-03-24", "1997-03-25",
> "1997-03-26", "1997-03-27", "1997-03-28", "1997-03-31", "1997-04-01",
> "1997-04-02", "1997-04-03", "1997-04-04", "1997-04-07", "1997-04-08",
> "1997-04-09", "1997-04-10", "1997-04-11", "1997-04-14", "1997-04-15",
> "1997-04-16", "1997-04-17", "1997-04-18", "1997-04-21", "1997-04-22",
> "1997-04-23", "1997-04-24", "1997-04-25", "1997-04-28", "1997-04-29",
> "1997-04-30", "1997-05-01", "1997-05-02", "1997-05-05", "1997-05-06",
> "1997-05-07", "1997-05-08", "1997-05-09", "1997-05-12", "1997-05-13",
> "1997-05-14", "1997-05-15", "1997-05-16", "1997-05-19", "1997-05-20",
> "1997-05-21", "1997-05-22", "1997-05-23", "1997-05-26", "1997-05-27",
> "1997-05-28", "1997-05-29", "1997-05-30", "1997-06-02", "1997-06-03",
> "1997-06-04", "1997-06-05", "1997-06-06", "1997-06-09", "1997-06-10",
> "1997-06-11", "1997-06-12", "1997-06-13", "1997-06-16", "1997-06-17"
> )
>
> set.seed(123)
>
> pWt <- matrix(rnorm(1000), ncol = 10)
>
> somehow create new vector newdates: ("1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", !
>  "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02")
>
> pWt[newdates, ] will give me the repeated monthly weights that I want.
>
> And similarly for repeated quarterly weights.
> _________________________________________________________________
> Share life as it happens with the new Windows Live.
>
> 08
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From patrick at burns-stat.com  Thu Jan 10 16:31:34 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Thu, 10 Jan 2008 15:31:34 +0000
Subject: [R-SIG-Finance] business day to monthly or quarterly
	aggregation.
In-Reply-To: <BLU105-W9961A47266A945386B9D5EE4A0@phx.gbl>
References: <SANCTUMFISERVERSnRM000000c2@sanctumfi.com>	<SANCTUMFISERVERsokL000000fb@sanctumfi.com>
	<BLU105-W9961A47266A945386B9D5EE4A0@phx.gbl>
Message-ID: <47863A56.7000402@burns-stat.com>

You seem to have an answer for the computing question,
but weights do not remain constant -- they change as the
prices change.  It is positions, not weights, that are constants.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Murali Menon wrote:

>Hi,
> 
>Thanks, Robert, that works nicely. I'm trying to adapt it for quarterly.
> 
>Brian, I was not aware of the quantmod stuff: thanks for the pointer.
> 
>Cheers,
>Murali> From: robert at sanctumfi.com> To: feanor0 at hotmail.com; r-sig-finance at stat.math.ethz.ch> Date: Thu, 10 Jan 2008 13:06:41 +0000> Subject: RE: [R-SIG-Finance] business day to monthly or quarterly aggregation.> > Hi Murali,> > Does this help?> > dateparts <- function (x) {> + list(mday = as.POSIXlt(x)$mday, month = as.POSIXlt(x)$mon + > + 1, year = as.POSIXlt(x)$year + 1900, wday => as.POSIXlt(x)$wday)> + }> > > > ourdates <- c("1997-01-29", "1997-01-30", "1997-01-31", "1997-02-03",> "1997-02-04", "1997-02-05", "1997-02-06", "1997-02-07", "1997-02-10", > + "1997-02-11", "1997-02-12", "1997-02-13", "1997-02-14",> "1997-02-17", "1997-02-18", "1997-02-19", "1997-02-20", "1997-02-21", > + "1997-02-24", "1997-02-25", "1997-02-26", "1997-02-27",> "1997-02-28", "1997-03-03", "1997-03-04", "1997-03-05", "1997-03-06", > + "1997-03-07", "1997-03-10", "1997-03-11", "1997-03-12",> "1997-03-13", "1997-03-14", "1997-03-17", "1997-03-18", "1997-03-19", > + "1997-03-20", "1997-03-21", "19!
> 97-03-24", "1997-03-25",> "1997-03-26", "1997-03-27", "1997-03-28", "1997-03-31", "1997-04-01", > + "1997-04-02", "1997-04-03", "1997-04-04", "1997-04-07",> "1997-04-08", "1997-04-09", "1997-04-10", "1997-04-11", "1997-04-14", > + "1997-04-15", "1997-04-16", "1997-04-17", "1997-04-18",> "1997-04-21", "1997-04-22", "1997-04-23", "1997-04-24", "1997-04-25", > + "1997-04-28", "1997-04-29", "1997-04-30", "1997-05-01",> "1997-05-02", "1997-05-05", "1997-05-06", "1997-05-07", "1997-05-08", > + "1997-05-09", "1997-05-12", "1997-05-13", "1997-05-14",> "1997-05-15", "1997-05-16", "1997-05-19", "1997-05-20", "1997-05-21", > + "1997-05-22", "1997-05-23", "1997-05-26", "1997-05-27",> "1997-05-28", "1997-05-29", "1997-05-30", "1997-06-02", "1997-06-03", > + "1997-06-04", "1997-06-05", "1997-06-06", "1997-06-09",> "1997-06-10", "1997-06-11", "1997-06-12", "1997-06-13", "1997-06-16", > + "1997-06-17")> > > > n <- c(1, which(diff(dateparts(ourdates)$month) != 0) + 1,> length(ourdates))> > !
> newdates <- ourdates> > for(i in 1:(length(n) - 1)){> + newdates[n[i]:
>n[i+1]] <- ourdates[n[i]]> + }> > ## vector of start of months..> > newdates> [1] "1997-01-29" "1997-01-29" "1997-01-29" "1997-02-03" "1997-02-03"> "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03"> "1997-02-03"> [12] "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03"> "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03"> "1997-02-03"> [23] "1997-02-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"> "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"> "1997-03-03"> [34] "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"> "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"> "1997-03-03"> [45] "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"> "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"> "1997-04-01"> [56] "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"> "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"> "1997-04-01"> [67] "!
> 1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"> "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"> "1997-05-01"> [78] "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"> "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"> "1997-05-01"> [89] "1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02"> "1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02"> "1997-06-02"> [100] "1997-06-02"> > > > > > Robert Sams > > -----Original Message-----> From: r-sig-finance-bounces at stat.math.ethz.ch> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Murali> Menon> Sent: 10 January 2008 11:14> To: r-sig-finance at stat.math.ethz.ch> Subject: [R-SIG-Finance] business day to monthly or quarterly> aggregation.> > > Folks,> > I have a matrix of portfolio weights whose rownames are daily dates> (weekends elided).> I'd like to create a new matrix of portfolio weights such that the> weight on the first day of th!
> e month (quarter) is copied to the rest of> the month (quarter). The i
>dea is that I rebalance my portfolio only on> the first of the month (quarter). Is there a neat way to do this?> > The reason I'm thinking of copying weights till the end of each period> is that then, in order to compute rebalancing costs, I just need to do a> abs(diff) on the weights matrix and multiply by the transaction cost.> > I guess if I can just convert the vector of dates into a vector of> repeated first-of-period dates, I can use this vector to index into the> portfolio weights matrix. But I can't think how I could achieve this> vector conversion either.> > Any suggestions?> > Thanks,> Murali> > > ourdates <- c("1997-01-29", "1997-01-30", "1997-01-31", "1997-02-03",> "1997-02-04", "1997-02-05", "1997-02-06", "1997-02-07", "1997-02-10",> "1997-02-11", "1997-02-12", "1997-02-13", "1997-02-14", "1997-02-17",> "1997-02-18", "1997-02-19", "1997-02-20", "1997-02-21", "1997-02-24",> "1997-02-25", "1997-02-26", "1997-02-27", "1997-02-28", "1997-03-03",> "1997-03-04", "1997!
> -03-05", "1997-03-06", "1997-03-07", "1997-03-10",> "1997-03-11", "1997-03-12", "1997-03-13", "1997-03-14", "1997-03-17",> "1997-03-18", "1997-03-19", "1997-03-20", "1997-03-21", "1997-03-24",> "1997-03-25", "1997-03-26", "1997-03-27", "1997-03-28", "1997-03-31",> "1997-04-01", "1997-04-02", "1997-04-03", "1997-04-04", "1997-04-07",> "1997-04-08", "1997-04-09", "1997-04-10", "1997-04-11", "1997-04-14",> "1997-04-15", "1997-04-16", "1997-04-17", "1997-04-18", "1997-04-21",> "1997-04-22", "1997-04-23", "1997-04-24", "1997-04-25", "1997-04-28",> "1997-04-29", "1997-04-30", "1997-05-01", "1997-05-02", "1997-05-05",> "1997-05-06", "1997-05-07", "1997-05-08", "1997-05-09", "1997-05-12",> "1997-05-13", "1997-05-14", "1997-05-15", "1997-05-16", "1997-05-19",> "1997-05-20", "1997-05-21", "1997-05-22", "1997-05-23", "1997-05-26",> "1997-05-27", "1997-05-28", "1997-05-29", "1997-05-30", "1997-06-02",> "1997-06-03", "1997-06-04", "1997-06-05", "1997-06-06", "1997-06-09",> "1997-06-10",!
>  "1997-06-11", "1997-06-12", "1997-06-13", "1997-06-16",> "1997-06-17"
>  
>
>>)> > set.seed(123)> > pWt <- matrix(rnorm(1000), ncol = 10)> > somehow create new vector newdates: ("1997-02-03", "1997-02-03",> "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03",> "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03",> "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03",> "1997-02-03", "1997-02-03", "1997-02-03", "1997-03-03", "1997-03-03",> "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03",> "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03",> "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03",> "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-04-01",> "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",> "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",> "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",> "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",> "1997-!
>>    
>>
> 04-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",> "1997-05-01", !> "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",> "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",> "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",> "1997-05-01", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02",> "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02",> "1997-06-02", "1997-06-02", "1997-06-02")> > pWt[newdates, ] will give me the repeated monthly weights that I want.> > And similarly for repeated quarterly weights.> _________________________________________________________________> Share life as it happens with the new Windows Live.> > 08> _______________________________________________> R-SIG-Finance at stat.math.ethz.ch mailing list> https://stat.ethz.ch/mailman/listinfo/r-sig-finance> -- Subscriber-posting only. > -- If you want to post, subscribe first.
>_________________________________________________________________
>
>
>GLM_CPC_VideoChat_distantfamily_012008
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From jeff.a.ryan at gmail.com  Thu Jan 10 17:04:11 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 10 Jan 2008 10:04:11 -0600
Subject: [R-SIG-Finance] business day to monthly or quarterly
	aggregation.
In-Reply-To: <BLU105-W9961A47266A945386B9D5EE4A0@phx.gbl>
References: <SANCTUMFISERVERSnRM000000c2@sanctumfi.com>
	<SANCTUMFISERVERsokL000000fb@sanctumfi.com>
	<BLU105-W9961A47266A945386B9D5EE4A0@phx.gbl>
Message-ID: <e8e755250801100804n29ce4c06ucead0fe07e54e502@mail.gmail.com>

A quick two functions to get the quarters (and more), though this
doesn't address your full issue (or the issue Patrick brings up):

This is from the new xts package we have put up on CRAN and on R-forge
( http://r-forge.r-project.org/projects/xts/ ).  the as.xts is what
you'll need xts for, though I am sure you can make a non-generic
solution instead.

`endpoints` <-
function(x,on='months') {
  x <- as.xts(x)
  if(on=='quarters') {
    xi <- (as.POSIXlt(index(x))$mon%/%3) + 1
    c(0,which(diff(xi) != 0),NROW(x))
  } else {
    on.opts <- list(secs='%S',seconds='%S',mins='%M',minutes='%M',
                    hours='%H',days='%j',
                    weeks='%W',months='%m',years='%y')
    c(0,which(diff(as.numeric(format(index(x),on.opts[[on]]))) != 0),NROW(x))
  }
}

`startof` <-
function(x,by='months') {
  ep <- endpoints(x,on=by)
  (ep+1)[-length(ep)]
}

A quick intro to xts:

An extension of zoo to enforce time-based indexing, while allowing for
arbitrary attributes to be added on to an object.  The idea is to
allow for quick time based subsetting, as well as clean conversion
among the different R data classes, both back and forth, while
maintaining all the available object information.  It basically makes
time-series life easy in R by standardizing time-series behavior to
'zoo-like' behavior.  And all the greatness of zoo is preserved!

The SVN code has some very handy sub-setting methods in the works:

library(quantmod); library(its)
getSymbols("QQQQ",src='yahoo',return.class='its')

as.xts(QQQQ)['2007']  # all of 07
as.xts(QQQQ)['2007-01']  # just January
as.xts(QQQQ)['2007-03::2007-03-20']  # March 07 - beginning of month
to the 20th (even if the 20th doesn't exist)

reclass(as.xts(QQQQ))  # though contrived returns the [now xts] QQQQ
object back to its original class (whatever that may have been - ts,
its, timeSeries, matrix, data.frame, zoo  - exactly as found).


The functions are not yet [exported] in the xts CRAN release - though
in the SVN on r-forge.

Jeff

On Jan 10, 2008 9:02 AM, Murali Menon <feanor0 at hotmail.com> wrote:
>
> Hi,
>
> Thanks, Robert, that works nicely. I'm trying to adapt it for quarterly.
>
> Brian, I was not aware of the quantmod stuff: thanks for the pointer.
>
> Cheers,
> Murali> From: robert at sanctumfi.com> To: feanor0 at hotmail.com; r-sig-finance at stat.math.ethz.ch> Date: Thu, 10 Jan 2008 13:06:41 +0000> Subject: RE: [R-SIG-Finance] business day to monthly or quarterly aggregation.> > Hi Murali,> > Does this help?> > dateparts <- function (x) {> + list(mday = as.POSIXlt(x)$mday, month = as.POSIXlt(x)$mon + > + 1, year = as.POSIXlt(x)$year + 1900, wday => as.POSIXlt(x)$wday)> + }> > > > ourdates <- c("1997-01-29", "1997-01-30", "1997-01-31", "1997-02-03",> "1997-02-04", "1997-02-05", "1997-02-06", "1997-02-07", "1997-02-10", > + "1997-02-11", "1997-02-12", "1997-02-13", "1997-02-14",> "1997-02-17", "1997-02-18", "1997-02-19", "1997-02-20", "1997-02-21", > + "1997-02-24", "1997-02-25", "1997-02-26", "1997-02-27",> "1997-02-28", "1997-03-03", "1997-03-04", "1997-03-05", "1997-03-06", > + "1997-03-07", "1997-03-10", "1997-03-11", "1997-03-12",> "1997-03-13", "1997-03-14", "1997-03-17", "1997-03-18", "1997-03-19", > + "1997-03-20", "1997-03-21", "19!
>  97-03-24", "1997-03-25",> "1997-03-26", "1997-03-27", "1997-03-28", "1997-03-31", "1997-04-01", > + "1997-04-02", "1997-04-03", "1997-04-04", "1997-04-07",> "1997-04-08", "1997-04-09", "1997-04-10", "1997-04-11", "1997-04-14", > + "1997-04-15", "1997-04-16", "1997-04-17", "1997-04-18",> "1997-04-21", "1997-04-22", "1997-04-23", "1997-04-24", "1997-04-25", > + "1997-04-28", "1997-04-29", "1997-04-30", "1997-05-01",> "1997-05-02", "1997-05-05", "1997-05-06", "1997-05-07", "1997-05-08", > + "1997-05-09", "1997-05-12", "1997-05-13", "1997-05-14",> "1997-05-15", "1997-05-16", "1997-05-19", "1997-05-20", "1997-05-21", > + "1997-05-22", "1997-05-23", "1997-05-26", "1997-05-27",> "1997-05-28", "1997-05-29", "1997-05-30", "1997-06-02", "1997-06-03", > + "1997-06-04", "1997-06-05", "1997-06-06", "1997-06-09",> "1997-06-10", "1997-06-11", "1997-06-12", "1997-06-13", "1997-06-16", > + "1997-06-17")> > > > n <- c(1, which(diff(dateparts(ourdates)$month) != 0) + 1,> length(ourdates))> > !
>  newdates <- ourdates> > for(i in 1:(length(n) - 1)){> + newdates[n[i]:
> n[i+1]] <- ourdates[n[i]]> + }> > ## vector of start of months..> > newdates> [1] "1997-01-29" "1997-01-29" "1997-01-29" "1997-02-03" "1997-02-03"> "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03"> "1997-02-03"> [12] "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03"> "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03" "1997-02-03"> "1997-02-03"> [23] "1997-02-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"> "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"> "1997-03-03"> [34] "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"> "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03" "1997-03-03"> "1997-03-03"> [45] "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"> "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"> "1997-04-01"> [56] "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"> "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01" "1997-04-01"> "1997-04-01"> [67] "!
>  1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"> "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"> "1997-05-01"> [78] "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"> "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01" "1997-05-01"> "1997-05-01"> [89] "1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02"> "1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02" "1997-06-02"> "1997-06-02"> [100] "1997-06-02"> > > > > > Robert Sams > > -----Original Message-----> From: r-sig-finance-bounces at stat.math.ethz.ch> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Murali> Menon> Sent: 10 January 2008 11:14> To: r-sig-finance at stat.math.ethz.ch> Subject: [R-SIG-Finance] business day to monthly or quarterly> aggregation.> > > Folks,> > I have a matrix of portfolio weights whose rownames are daily dates> (weekends elided).> I'd like to create a new matrix of portfolio weights such that the> weight on the first day of th!
>  e month (quarter) is copied to the rest of> the month (quarter). The i
> dea is that I rebalance my portfolio only on> the first of the month (quarter). Is there a neat way to do this?> > The reason I'm thinking of copying weights till the end of each period> is that then, in order to compute rebalancing costs, I just need to do a> abs(diff) on the weights matrix and multiply by the transaction cost.> > I guess if I can just convert the vector of dates into a vector of> repeated first-of-period dates, I can use this vector to index into the> portfolio weights matrix. But I can't think how I could achieve this> vector conversion either.> > Any suggestions?> > Thanks,> Murali> > > ourdates <- c("1997-01-29", "1997-01-30", "1997-01-31", "1997-02-03",> "1997-02-04", "1997-02-05", "1997-02-06", "1997-02-07", "1997-02-10",> "1997-02-11", "1997-02-12", "1997-02-13", "1997-02-14", "1997-02-17",> "1997-02-18", "1997-02-19", "1997-02-20", "1997-02-21", "1997-02-24",> "1997-02-25", "1997-02-26", "1997-02-27", "1997-02-28", "1997-03-03",> "1997-03-04", "1997!
>  -03-05", "1997-03-06", "1997-03-07", "1997-03-10",> "1997-03-11", "1997-03-12", "1997-03-13", "1997-03-14", "1997-03-17",> "1997-03-18", "1997-03-19", "1997-03-20", "1997-03-21", "1997-03-24",> "1997-03-25", "1997-03-26", "1997-03-27", "1997-03-28", "1997-03-31",> "1997-04-01", "1997-04-02", "1997-04-03", "1997-04-04", "1997-04-07",> "1997-04-08", "1997-04-09", "1997-04-10", "1997-04-11", "1997-04-14",> "1997-04-15", "1997-04-16", "1997-04-17", "1997-04-18", "1997-04-21",> "1997-04-22", "1997-04-23", "1997-04-24", "1997-04-25", "1997-04-28",> "1997-04-29", "1997-04-30", "1997-05-01", "1997-05-02", "1997-05-05",> "1997-05-06", "1997-05-07", "1997-05-08", "1997-05-09", "1997-05-12",> "1997-05-13", "1997-05-14", "1997-05-15", "1997-05-16", "1997-05-19",> "1997-05-20", "1997-05-21", "1997-05-22", "1997-05-23", "1997-05-26",> "1997-05-27", "1997-05-28", "1997-05-29", "1997-05-30", "1997-06-02",> "1997-06-03", "1997-06-04", "1997-06-05", "1997-06-06", "1997-06-09",> "1997-06-10",!
>   "1997-06-11", "1997-06-12", "1997-06-13", "1997-06-16",> "1997-06-17"
> > )> > set.seed(123)> > pWt <- matrix(rnorm(1000), ncol = 10)> > somehow create new vector newdates: ("1997-02-03", "1997-02-03",> "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03",> "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03",> "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03", "1997-02-03",> "1997-02-03", "1997-02-03", "1997-02-03", "1997-03-03", "1997-03-03",> "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03",> "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03",> "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03",> "1997-03-03", "1997-03-03", "1997-03-03", "1997-03-03", "1997-04-01",> "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",> "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",> "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",> "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01", "1997-04-01",> "1997-!
>  04-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",> "1997-05-01", !> "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",> "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",> "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01", "1997-05-01",> "1997-05-01", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02",> "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02", "1997-06-02",> "1997-06-02", "1997-06-02", "1997-06-02")> > pWt[newdates, ] will give me the repeated monthly weights that I want.> > And similarly for repeated quarterly weights.> _________________________________________________________________> Share life as it happens with the new Windows Live.> > 08> _______________________________________________> R-SIG-Finance at stat.math.ethz.ch mailing list> https://stat.ethz.ch/mailman/listinfo/r-sig-finance> -- Subscriber-posting only. > -- If you want to post, subscribe first.
> _________________________________________________________________
>
>
> GLM_CPC_VideoChat_distantfamily_012008
>         [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From feanor0 at hotmail.com  Thu Jan 10 18:21:46 2008
From: feanor0 at hotmail.com (Murali Menon)
Date: Thu, 10 Jan 2008 17:21:46 +0000
Subject: [R-SIG-Finance] business day to monthly or quarterly
 aggregation.
In-Reply-To: <47863A56.7000402@burns-stat.com>
References: <SANCTUMFISERVERSnRM000000c2@sanctumfi.com>
	<SANCTUMFISERVERsokL000000fb@sanctumfi.com>
	<BLU105-W9961A47266A945386B9D5EE4A0@phx.gbl>
	<47863A56.7000402@burns-stat.com>
Message-ID: <BLU105-W568607EBF571A8C42C85C9EE4A0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080110/a90409a6/attachment.pl 

From rfrancois at mango-solutions.com  Fri Jan 11 16:49:50 2008
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Fri, 11 Jan 2008 15:49:50 +0000
Subject: [R-SIG-Finance] R for finance public training course in london,
	27-29th Feb 2008
Message-ID: <4787901E.3040300@mango-solutions.com>

Mango Solutions are pleased to announce the above course in London as
part of our schedule for Q1 2008.
 
-----------------------------------------------------------------------
     R for financial services  -  27th to 29th of February 2008
-----------------------------------------------------------------------
 
* Who Should Attend ?
 
The R for finance course provides a practical introduction to the R
environment within financial services to enable quantitative analysts
to quickly become more productive with R.
 
* Course Goals
 
- To allow attendees to understand the technology behind the R package
- Improve attendees programming style and confidence
- To understand R's models related to the finance sector
- To enable users to access a wide range of available functionality
- To enable attendees to program in R within their own environment
 
* Course Outline
 
1. Introduction to the R language
2. The R Environment
3. R data objects
4. Using R functions
5. Important R functions
6. R Graphics
7. Basic R Statistical and Mathematical Functions used in the Finance 
industry
8. Time Series Modeling
9. Overview of additional R finance packages
 
The cost of this course is ?1200 for commercial attendees and ?800 for
academic attendees.
 
Should you want to book a place on this course or have any questions
please contact training at mango-solutions.com
 
Should your organization have more than 3 possible attendees why not
talk to us about hosting a customized and focused course delivered at
your premises?
 
--
Mango Solutions
Tel +44 (0)1249 467 467
Fax +44 (0)1249 467 468
data analysis that delivers


From spencer.graves at pdf.com  Sat Jan 12 04:40:54 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Jan 2008 19:40:54 -0800
Subject: [R-SIG-Finance] FinTS_0.2-5 available & GARCH questions
Message-ID: <478836C6.1090909@pdf.com>

Hi, All:

      FinTS version 0.2-5 is now available on CRAN.  This version
includes script files to produce nearly all the examples, figures and
tables in the first two chapters of Tsay (2005) Analysis of Financial
Time Series, 2nd ed. (Wiley), plus portions of chapters 3 and 11.


NEW FUNCTION

      It also includes "AutocorTest", which is a wrapper for "Box.test"
that supports two modifications:

            (1) Computing the p-value for the Ljung-Box test using
degrees of freedom different from the number of lags, as recommended  by
many sources for the residuals for model fits.

            (2) Computing the Ljung-Box test using ranks, as recommended
by  Patrick Burns (2002) 'Robustness of the Ljung-Box Test and its Rank
Equivalent', <www.burns-stat.com/pages/Working/ljungbox.pdf>, and
references cited therein.


OPEN QUESTIONS:

      I wonder if anyone can help me with the following two issues that
I hope are simple and self-contained given the FinTS package:


LAGRANGE MULTIPLIER TEST FOR CONDITIONAL HETEROSCEDASTICITY

      The S-Plus Finmetrics package includes a function "archTest" to
compute the Langrange multiplier test for conditional heteroscedasticity
described by Engle (1982) Autoregressive Conditional Heteroscedasticity
with estimates of the variance of United Kingdom inflations,
Econometrica, 50:  987-1007.  Tsay (pp. 101-102) discusses this test and
applies it to the monthly log stock returns of Intel corporation from
1973 to 2003.  The simple returns are available in FinTS as
"m.intc7303".  Thus, if "archTest" were available then the following
should produce the answers in the book:

library(FinTS)
data(m.intc7303)
# S-Plus Finmetrics function:
#archTest(log(1+as.numeric(m.intc7303)), lag=12)

#      Tsay reports that this function returned a test statistic of
43.5041, which should follow approximately a ch-square with 12 degrees
of freedom.


PREDICT for garchFit in fGarch

      The 'gardhFit' function seems to work fine, but I'd like a
'predict' function.  The following is a self-contained example discussed
on pp. 118-120 of Tsay:

library(FinTS)
data(sp500)
library(fGarch)
# p. 118
spFit00.11 <- garchFit(sp500~garch(1,1), data=sp500)

#Coefficient(s):
#                       mu        omega       alpha1        beta1
#garchFit{fGarch}:  7.44972e-03  8.06162e-05  1.21975e-01  8.54361e-01
#garch{Finmetrics}: 7.6e-3       8.6e-5       1.216e-1     8.511e-1

# p. 120
# Figure 3.1.  Volatility forecasts
# Horizon       1       2       3       4       5      Inf
# Return     0.0076  0.0076  0.0076  0.0076  0.0076  0.0076
# Volatility 0.0536  0.0537  0.0537  0.0538  0.0538  0.0560


      I hope some of you find the 'AutocorTest' function useful, and I
hope someone is able to help me with 'archTest' and
predict(garchFit(...)).
      Thanks,
      Spencer Graves


From swtzang at gmail.com  Sun Jan 13 03:23:54 2008
From: swtzang at gmail.com (ShyhWeir Tzang)
Date: Sun, 13 Jan 2008 10:23:54 +0800
Subject: [R-SIG-Finance] merge with two time series
Message-ID: <c17037a10801121823q2a60c87gb6a57870eff41d99@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080113/86a29630/attachment.pl 

From ggrothendieck at gmail.com  Sun Jan 13 04:06:16 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 12 Jan 2008 22:06:16 -0500
Subject: [R-SIG-Finance] merge with two time series
In-Reply-To: <c17037a10801121823q2a60c87gb6a57870eff41d99@mail.gmail.com>
References: <c17037a10801121823q2a60c87gb6a57870eff41d99@mail.gmail.com>
Message-ID: <971536df0801121906of190158x7b9b33eee9647507@mail.gmail.com>

These are not time series in the strict sense since they lack unique times
but you can do this with data frames:

> Lines1 <- " x1            x2
+ 2003-01-02      12           200
+ 2003-01-02      13           300
+ 2003-01-02      14           400
+ 2003-01-03      12           150
+ 2003-01-03      13           170
+ 2003-01-03      14           190
+ "
>
> Lines2 <- " y1
+ 2003-01-02       1000
+ 2003-01-03       1500
+ "
>
> DF1 <- read.table(textConnection(Lines1), skip = 1, col.names = c("Date", "x1", "x2"), colClasses = c("Date", "numeric", "numeric"))
> DF2 <- read.table(textConnection(Lines2), skip = 1, col.names = c("Date", "y"), colClasses = c("Date", "numeric"))
> merge(DF1, DF2)
        Date x1  x2    y
1 2003-01-02 12 200 1000
2 2003-01-02 13 300 1000
3 2003-01-02 14 400 1000
4 2003-01-03 12 150 1500
5 2003-01-03 13 170 1500
6 2003-01-03 14 190 1500

On Jan 12, 2008 9:23 PM, ShyhWeir Tzang <swtzang at gmail.com> wrote:
> Dear all:
>
> I have two time series x and y. x is the time series data with several
> data columns:
>                         x1            x2
> 2003-01-02      12           200
> 2003-01-02      13           300
> 2003-01-02      14           400
> 2003-01-03      12           150
> 2003-01-03      13           170
> 2003-01-03      14           190
> ....................
>
> y is  the time series data with uniqe date stamp
>                           y1
> 2003-01-02       1000
> 2003-01-03       1500
> .............
>
> I would like to have the following with merge or other commands available:
>                          x1            x2             y1
> 2003-01-02      12           200            1000
> 2003-01-02      13           300            1000
> 2003-01-02      14           400            1000
> 2003-01-03      12           150            1500
>  2003-01-03      13           170            1500
>  2003-01-03      14           190            1500
> ......................
>


From MichelBeck at sbcglobal.Net  Mon Jan 14 16:10:52 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Mon, 14 Jan 2008 15:10:52 +0000 (UTC)
Subject: [R-SIG-Finance]
	=?utf-8?q?R_-_Error_Question_for=09chart=2ECorrel?=
	=?utf-8?q?ation=28PerformanceAnalytics=29?=
References: <loom.20080102T171756-520@post.gmane.org>
	<E634AF2410E42246A35865D8C0C784D90FC85E@MI8NYCMAIL09.Mi8.com>
Message-ID: <loom.20080114T150804-333@post.gmane.org>


Guy Yollin <guy.yollin <at> rotellacapital.com> writes:

> 
> Hi Michel,
> 

Hi!

Many thanks to all who helped me with this problem

Michel


From MichelBeck at sbcglobal.Net  Mon Jan 14 16:13:52 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Mon, 14 Jan 2008 15:13:52 +0000 (UTC)
Subject: [R-SIG-Finance] Financial Basket Options
Message-ID: <loom.20080114T151149-759@post.gmane.org>

Hi!

I am looking for R code to price a multiple asset basket option.
So far I only found a 2 asset pricer (in the R-metrics packafe fExoticOption).
Where could I find some code for at least 4 assets?

Thank you,

Michel Beck


From wuertz at itp.phys.ethz.ch  Tue Jan 15 19:06:14 2008
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Tue, 15 Jan 2008 19:06:14 +0100
Subject: [R-SIG-Finance] extract certain date in a month
In-Reply-To: <c17037a10801061721y523837b2tb18063033a4d1ac6@mail.gmail.com>
References: <c17037a10801061721y523837b2tb18063033a4d1ac6@mail.gmail.com>
Message-ID: <478CF616.2000200@itp.phys.ethz.ch>

ShyhWeir Tzang wrote:
> Dear all:
>
> How can I extract the date of the third Wednesday in each month? Is there
> any available function to do it?
> Thanks for help.
>
> ShyhWeir
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
>   
The Rmetrics package has special dates functions

# FUNCTION:                 FIRST AND LAST DAY IN PERIOD:
#  timeLastDayInMonth        Computes the last day in a given month and year
#  timeFirstDayInMonth       Computes the first day in a given month and 
year
#  timeLastDayInQuarter      Computes the last day in a given quarter 
and year
#  timeFirstDayInQuarter     Computes the first day in a given quarter 
and year
# FUNCTION:                 DAYS BEFORE AND AFTER:
#  timeNdayOnOrAfter         Computes date in month that is a n-day ON 
OR AFTER 
#  timeNdayOnOrBefore        Computes date in month that is a n-day ON 
OR BEFORE 
# FUNCTION:                 THE N'TH DAY IN:
#  timeNthNdayInMonth        Computes n-th ocurrance of a n-day in 
year/month
#  timeLastNdayInMonth       Computes the last n-day in year/month


timeLastNdayInMonth, calculates e.g. the last Friday in April, ....
timeNthNdayInMonth, calculates e.g. second Monday in June


reagrds
Diethelm Wuertz


From m_olshansky at yahoo.com  Wed Jan 16 06:35:02 2008
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Tue, 15 Jan 2008 21:35:02 -0800 (PST)
Subject: [R-SIG-Finance] Financial Basket Options
In-Reply-To: <loom.20080114T151149-759@post.gmane.org>
Message-ID: <367338.236.qm@web32206.mail.mud.yahoo.com>

Hi Michel,

I wrote an R code implementing Longstaff-Schwartz
algorithm for pricing American put Basket option (on
the portfolio value). This code can be easily changed
to value call option (I intend to allow for "any"
payoff function in the future).
I can send you this code. It is in a very preliminary
state, so even though I did some basic testing I can
not guarantee it to be correct.

Regards,

Moshe.

--- MAB <MichelBeck at sbcglobal.Net> wrote:

> Hi!
> 
> I am looking for R code to price a multiple asset
> basket option.
> So far I only found a 2 asset pricer (in the
> R-metrics packafe fExoticOption).
> Where could I find some code for at least 4 assets?
> 
> Thank you,
> 
> Michel Beck
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From btyner at gmail.com  Thu Jan  3 01:30:05 2008
From: btyner at gmail.com (Benjamin Tyner)
Date: Wed, 2 Jan 2008 19:30:05 -0500
Subject: [R-SIG-Finance] fPortfolio: add arbitrary portfolio to frontierPlot
Message-ID: <5f88b2c50801021630u787dd324jef6507bf37b47779@mail.gmail.com>

Hi,

I'm wondering what's the recommended way to specify an arbitrary
portfolio and add it to a frontierPlot?

Thanks
Ben


From johnputz3655 at yahoo.com  Fri Jan 18 19:09:27 2008
From: johnputz3655 at yahoo.com (John Putz)
Date: Fri, 18 Jan 2008 10:09:27 -0800 (PST)
Subject: [R-SIG-Finance] exponential moving avg appears to use future data
Message-ID: <863138.5904.qm@web50702.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080118/901bbb0f/attachment.pl 

From swtzang at gmail.com  Sat Jan 19 00:14:59 2008
From: swtzang at gmail.com (ShyhWeir Tzang)
Date: Sat, 19 Jan 2008 07:14:59 +0800
Subject: [R-SIG-Finance] file import
Message-ID: <c17037a10801181514t7915db36w1ebecd264a22c08d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080119/6c42e744/attachment.pl 

From ggrothendieck at gmail.com  Sat Jan 19 02:11:38 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 18 Jan 2008 20:11:38 -0500
Subject: [R-SIG-Finance] file import
In-Reply-To: <c17037a10801181514t7915db36w1ebecd264a22c08d@mail.gmail.com>
References: <c17037a10801181514t7915db36w1ebecd264a22c08d@mail.gmail.com>
Message-ID: <971536df0801181711y6c7ec712w7aa6ed919bcba740@mail.gmail.com>

On Jan 18, 2008 6:14 PM, ShyhWeir Tzang <swtzang at gmail.com> wrote:
> Dear all:
>
> My data file to be imported has a trailing comment at the end line. However,
> when I use read.table to read the file, the error message shows "...line 6
> did not have three elements....". How can I import the data file without
> having to delete the comment line each time?
>
> header1  header2  header3
>    100        200        300
>     400       500        600
>     700       800        900
> <blank>
> comments
>
> Thanks very much for any possible help.
>

Read it once with count.fields and then a second
time with read.table setting nrows.  The following
assumes that there is always a blank line following
the data.  You may need to generalize this if that's
not always so.  (If you can control the imput format
then an even easier thing to do is to ensure that
the comments are prefaced by the comment.char,
an argument to read.table)

Lines <- " header1  header2  header3
   100        200        300
    400       500        600
    700       800        900

comments
"
k <- count.fields(textConnection(Lines), blank.lines.skip = FALSE)
read.table(textConnection(Lines), header = TRUE, nrows = which(k==0)[1]-2)


From bbands at gmail.com  Sat Jan 19 16:18:31 2008
From: bbands at gmail.com (BBands)
Date: Sat, 19 Jan 2008 07:18:31 -0800
Subject: [R-SIG-Finance] exponential moving avg appears to use future
	data
In-Reply-To: <863138.5904.qm@web50702.mail.re2.yahoo.com>
References: <863138.5904.qm@web50702.mail.re2.yahoo.com>
Message-ID: <6e8360ad0801190718s47a621ccr25e7cf2cbd476578@mail.gmail.com>

On Jan 18, 2008 10:09 AM, John Putz <johnputz3655 at yahoo.com> wrote:
> Hello,
>
> As usual, I'm not sure if this is the correct list to ask this, but it appears that the exponential moving average functions (emaTA and EWMA) use future information to calculate the moving average for periods prior to the kick-in of the weightings.  E.g. at one point it sets:
>
> ylam[1] = mean(y[1:startup])
>
> which, i am not quite able to trace through the rest of the function calls, but seems to be having the effect of starting the moving average calculation based on the average of future prices.  Am I correct in this?  If so, is there a reason why this might work that way?

What you are seeing in a method of finding a seed value for the ema.
There are many methods, each with its own pros and cons. In any case,
you need to respect an run-in period after which the seeding method is
largely moot.

The article here is not a bad intro:
http://en.wikipedia.org/wiki/Weighted_moving_average#Exponential_moving_average

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From josh.m.ulrich at gmail.com  Sun Jan 20 16:19:08 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Sun, 20 Jan 2008 09:19:08 -0600
Subject: [R-SIG-Finance] exponential moving avg appears to use future
	data
In-Reply-To: <6e8360ad0801190718s47a621ccr25e7cf2cbd476578@mail.gmail.com>
References: <863138.5904.qm@web50702.mail.re2.yahoo.com>
	<6e8360ad0801190718s47a621ccr25e7cf2cbd476578@mail.gmail.com>
Message-ID: <8cca69990801200719i6dbeb961q6fe7a68e31f30b5b@mail.gmail.com>

On Jan 19, 2008 9:18 AM, BBands <bbands at gmail.com> wrote:
> On Jan 18, 2008 10:09 AM, John Putz <johnputz3655 at yahoo.com> wrote:
> > Hello,
> >
> > As usual, I'm not sure if this is the correct list to ask this, but it appears that the exponential moving average functions (emaTA and EWMA) use future information to calculate the moving average for periods prior to the kick-in of the weightings.  E.g. at one point it sets:
> >
> > ylam[1] = mean(y[1:startup])
> >
> > which, i am not quite able to trace through the rest of the function calls, but seems to be having the effect of starting the moving average calculation based on the average of future prices.  Am I correct in this?  If so, is there a reason why this might work that way?
>
> What you are seeing in a method of finding a seed value for the ema.
> There are many methods, each with its own pros and cons. In any case,
> you need to respect an run-in period after which the seeding method is
> largely moot.
>
> The article here is not a bad intro:
> http://en.wikipedia.org/wiki/Weighted_moving_average#Exponential_moving_average
>
>     jab
> --
> John Bollinger, CFA, CMT
> www.BollingerBands.com
>
> If you advance far enough, you arrive at the beginning.
>

I second John's response.  The EMA function in package TTR uses a
seeding value of
ema[n] <- mean(x[1:n])
and then begins using the exponential smoothing.  You can then compare
the results of emaTA with EMA and see that the results converge after
the run-in period.

TTR contains another 30+ TA functions, many of which are chartable via
the quantmod package.  It is being updated with new functions and
Fortran implementations of existing functions.  TTR's development
source code is here:
https://r-forge.r-project.org/projects/ttr/

HTH,
Josh

PS Sorry if this is my second reply.  I replied on Friday, but it
didn't seem to make it to the list.
--
http://quantemplation.blogspot.com


From ecjbosu at aol.com  Sun Jan 20 16:55:40 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Sun, 20 Jan 2008 09:55:40 -0600
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
	object(s) to utilize.
Message-ID: <fmvquf$9hm$1@ger.gmane.org>

I am soliciting your opinions on the different time series objects as to 
which you prefer, or which you do not prefer.  I have been using 
dataframes and date columns to handle my timeseries processes for awhile 
now, and recently began experimenting with different timeseries 
packages, mainly: zoo, ts, tseries, Rmetric's timeSequences, and 
rseries.  There are two reasons for my change here
1.  Utilize the object oriented components of these packages,
2.  Standardization for handling the nuances of my time series data.


Both of these are important, but the second one is specific to my 
industry: Energy.  I work with assets where the transfer of physical 
molecules occur over time (ratable) under constraints, not immediately 
as with fixed income and equities.  I have assets that settle yearly, 
qrtly, monthly, weekly, balance of the week, balance of the month, 
daily, hourly, and as low as 5 minute intervals.  I have assets where 
the underlying is spread over different regions or zones, and the 
underlying is the asset, an average, an index, a lookback, and many 
more.  All of these require careful manipulation of the timeseries with 
great care for calendars to maintain integrity of the system where the 
energy asset is located.

I could continue babbling on about my issues but will leave it here with 
this introduction.  I would appreciate any and all comments.  Also, if 
this topic should be on the r-users mailing list, I do apologize for my 
indiscretion. I thought this would be better since the underlying data 
is financial/business in nature.

Thank you and have a wonderful day.
Joe


From ecjbosu at aol.com  Sun Jan 20 17:07:52 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Sun, 20 Jan 2008 10:07:52 -0600
Subject: [R-SIG-Finance] books?
In-Reply-To: <BFF7EAF6841D7A4389FFB72C795A5896527F59@chkmsx01p.backchk.local>
References: <BFF7EAF6841D7A4389FFB72C795A5896527F59@chkmsx01p.backchk.local>
Message-ID: <fmvrlc$bpa$1@ger.gmane.org>

Max Nevill wrote:
>  
> 
> Hi Everyone!
> 
> I do some forecasts for my job and recently have started to realize that
> I may be able to make better forecasts if I look at multivariate
> analysis rather than fit several univariate models. Unfortunately in
> undergrad I only took one course in univarate analysis.
> 
> I was wondering what some good books are on the subject of multivariate
> time series analysis? Especially any that use R over other languages.
> 
> Thanks,
> 
> -Max
> 
> _________________________________________
> 
> 
> 	[[alternative HTML version deleted]]
> 
My base references for time series are
Statistical Analysis of Financial Data in S-Plus by Carmona
Time Series Analysis by Hamilton
and
Applied Econometric Time Series by Enders

I also use several econometric texts
Judge, etal's series
	Theory and Practice of Econometrics,
	Introduction to the ditto, and
	Learning and Practicing Econometrics
Greene's Econometric Analysis.

A side note, my wife and I are building a bibliography of our personal 
libraries of books and papers on www.financialseal.com under analytics 
and research.   It is in the infancy stages, but we are trying to update 
it regularly.  These and other financial and statistical references will 
be there.  The connection is slow, so patience is required.

Joe


From edd at debian.org  Sun Jan 20 17:22:56 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 20 Jan 2008 10:22:56 -0600
Subject: [R-SIG-Finance] Solicitation of opinions on which
	Timeseries	object(s) to utilize.
In-Reply-To: <fmvquf$9hm$1@ger.gmane.org>
References: <fmvquf$9hm$1@ger.gmane.org>
Message-ID: <18323.30048.977818.40873@ron.nulle.part>


On 20 January 2008 at 09:55, Joe W. Byers wrote:
| I am soliciting your opinions on the different time series objects as to 
| which you prefer, or which you do not prefer.  I have been using 
| dataframes and date columns to handle my timeseries processes for awhile 
| now, and recently began experimenting with different timeseries 
| packages, mainly: zoo, ts, tseries, Rmetric's timeSequences, and 
| rseries.  There are two reasons for my change here
| 1.  Utilize the object oriented components of these packages,
| 2.  Standardization for handling the nuances of my time series data.

For 'financial data' in the large sense I have been using zoo for years,
after having used its. I have also used ts for some ultra-low-frequency
macro-economic series, but ts becomes useless once you leave montly or
quarterly frequencies and their fixed delta-t. I have basically no experience
with the others.

zoo is good as it does all the merging, subsetting, plotting, indexing,
converting to and from (ie I often 'drop' to data.frame(coredata(zooobject))
for analysis) I need.  Working with high-frequency data, I occassionally hit
the constraint of 'has to have unique indices' in which case I just pad a
micro-second each to the occassional identical timestamp. That's a small
price to pay. 

You did not mention xts, which is the newest member of the clan. It extends
zoo and allows 'annotations', ie you can set attributes about the when /
where / how / ... of your data. Xts also overcomes the uniqueness constraint
for the index (and I am not sure how they do it) and offers richer 'logical'
subsetting.  It looks like a winner, and this may just what you need. Have a
look at xts.  Usual caveat: Haven't used it myself yet...

Hope this helps, Dirk

-- 
Three out of two people have difficulties with fractions.


From ecjbosu at aol.com  Sun Jan 20 17:37:08 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Sun, 20 Jan 2008 10:37:08 -0600
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
 object(s) to utilize.
In-Reply-To: <18323.30048.977818.40873@ron.nulle.part>
References: <fmvquf$9hm$1@ger.gmane.org>
	<18323.30048.977818.40873@ron.nulle.part>
Message-ID: <479378B4.8030107@aol.com>

NO

Dirk Eddelbuettel wrote:
> On 20 January 2008 at 09:55, Joe W. Byers wrote:
> | I am soliciting your opinions on the different time series objects as to 
> | which you prefer, or which you do not prefer.  I have been using 
> | dataframes and date columns to handle my timeseries processes for awhile 
> | now, and recently began experimenting with different timeseries 
> | packages, mainly: zoo, ts, tseries, Rmetric's timeSequences, and 
> | rseries.  There are two reasons for my change here
> | 1.  Utilize the object oriented components of these packages,
> | 2.  Standardization for handling the nuances of my time series data.
>
> For 'financial data' in the large sense I have been using zoo for years,
> after having used its. I have also used ts for some ultra-low-frequency
> macro-economic series, but ts becomes useless once you leave montly or
> quarterly frequencies and their fixed delta-t. I have basically no experience
> with the others.
>
> zoo is good as it does all the merging, subsetting, plotting, indexing,
> converting to and from (ie I often 'drop' to data.frame(coredata(zooobject))
> for analysis) I need.  Working with high-frequency data, I occassionally hit
> the constraint of 'has to have unique indices' in which case I just pad a
> micro-second each to the occassional identical timestamp. That's a small
> price to pay. 
>
> You did not mention xts, which is the newest member of the clan. It extends
> zoo and allows 'annotations', ie you can set attributes about the when /
> where / how / ... of your data. Xts also overcomes the uniqueness constraint
> for the index (and I am not sure how they do it) and offers richer 'logical'
> subsetting.  It looks like a winner, and this may just what you need. Have a
> look at xts.  Usual caveat: Haven't used it myself yet...
>   
Did not know about it.  I will take a look myself as well.  As always, 
your input is great.

Take care
Joe

> Hope this helps, Dirk
>
>


From jeff.a.ryan at gmail.com  Sun Jan 20 18:52:08 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sun, 20 Jan 2008 11:52:08 -0600
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
	object(s) to utilize.
In-Reply-To: <479378B4.8030107@aol.com>
References: <fmvquf$9hm$1@ger.gmane.org>
	<18323.30048.977818.40873@ron.nulle.part> <479378B4.8030107@aol.com>
Message-ID: <e8e755250801200952m30058d62u6346e56f4b7fc271@mail.gmail.com>

Hi Joe,

I agree with Dirk's reply. The one thing I'd like to offer is with
respect to xts.

We wrote xts because zoo did essentially everything we needed, yet we
had been wrestling with a way to handle additional attributes cleanly
(we didn't want to 'see' them necessarily), as well as allowing for
specific time-based subsetting via "[".

It is still very early in its development, but it is progressing
rapidly.  The lastest source and builds are available from
http://r-forge.r-project.org/projects/xts and a slightly older version
is on CRAN.

Basically xts is zoo with 2 major(minor) differences.

1. The index must be of a known time-based class. At present this
means either Date of POSIXct, which are native to R - and universal.
There is automatic conversion in as.xts, which handles coercion from
all the classes you mentioned as well as some you haven't.

2. Additional name=value attribute pairs can be passed to the xts
constructors (xts() or as.xts()) which will allow you to attach any
additional attributes you'd like/need. For financial contracts that
could be exhange/data import time/expiration/etc...

xts, because it extends zoo, allows for all 'zoo' methods that usually
just work, to still just work.  An xts object can be used anywhere a
zoo object can. - i.e. inherits(xts.object, "zoo") is TRUE.

The package also offers time based functions (originally/currently)
from the quantmod package - namely the ability to aggregate by common
time ranges - everything from 3 minutes all the way up to yearly, via
'to.period'. The examples page at
http://www.quantmod.com/examples/data gives a rundown of the functions
in some detail.

The new subsetting by the '[' method is sort of cool (IMO):

QQQQ # an object of (you choose: ts, zoo, xts, timeSeries, its, irts,
data.frame, matrix...)

# view all of 2007
as.xts(QQQQ)['2007']

# view all of June of 2007 to the end of the data set:
as.xts(QQQQ)['2007-06::']

# the beginning to 12 June 2007
as.xts(QQQQ)['::2007-06-12']

# the beginning to 12 June 2007 - converted to a _weekly_ series
to.weekly(as.xts(QQQQ)['::2007-06-12'])

The primary motivation behind as.xts was really to make the user's
class choice irrelevant to the function it
it passed to. For example the chartSeries function in quantmod at
present only works well with zoo-like objects.  If you
use timeSeries you were out of luck.  chartSeries(as.xts(QQQQ)) now
just works.  Plus the function 'reclass' allows developers/users the
option of using the time-based subsetting on non-xts objects:

say QQQQ is a timeSeries object, but you'd like to quickly convert to
weekly like the above example, but you want to end
up with your original timeSeries class.

reclass(to.weekly(as.xts(QQQQ)['::2007-06-12']))

yields a timeSeries object converted with xts functionality.  This
works for all convertible objects.

The one correction to Dirk's post is that of micro-seconds/tick-data.
It really doesn't do anything to handle that yet, though
it is planned.  The conversion to POSIXct internally may make it work
on occasion, but it hasn't really been worked out.  SO user beware.

Jeff

As a note: I have been moving the time-based functions from quantmod
to xts, as it seems they have value outside of the more limited
trading scope of the quantmod project.  At the moment it is all in
transition, so things may be not perfectly aligned with respect to
where these funtions reside.

On Jan 20, 2008 10:37 AM, Joe W. Byers <ecjbosu at aol.com> wrote:
> NO
>
>
> Dirk Eddelbuettel wrote:
> > On 20 January 2008 at 09:55, Joe W. Byers wrote:
> > | I am soliciting your opinions on the different time series objects as to
> > | which you prefer, or which you do not prefer.  I have been using
> > | dataframes and date columns to handle my timeseries processes for awhile
> > | now, and recently began experimenting with different timeseries
> > | packages, mainly: zoo, ts, tseries, Rmetric's timeSequences, and
> > | rseries.  There are two reasons for my change here
> > | 1.  Utilize the object oriented components of these packages,
> > | 2.  Standardization for handling the nuances of my time series data.
> >
> > For 'financial data' in the large sense I have been using zoo for years,
> > after having used its. I have also used ts for some ultra-low-frequency
> > macro-economic series, but ts becomes useless once you leave montly or
> > quarterly frequencies and their fixed delta-t. I have basically no experience
> > with the others.
> >
> > zoo is good as it does all the merging, subsetting, plotting, indexing,
> > converting to and from (ie I often 'drop' to data.frame(coredata(zooobject))
> > for analysis) I need.  Working with high-frequency data, I occassionally hit
> > the constraint of 'has to have unique indices' in which case I just pad a
> > micro-second each to the occassional identical timestamp. That's a small
> > price to pay.
> >
> > You did not mention xts, which is the newest member of the clan. It extends
> > zoo and allows 'annotations', ie you can set attributes about the when /
> > where / how / ... of your data. Xts also overcomes the uniqueness constraint
> > for the index (and I am not sure how they do it) and offers richer 'logical'
> > subsetting.  It looks like a winner, and this may just what you need. Have a
> > look at xts.  Usual caveat: Haven't used it myself yet...
> >
> Did not know about it.  I will take a look myself as well.  As always,
> your input is great.
>
> Take care
> Joe
>
> > Hope this helps, Dirk
>
> >
> >
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From brian at braverock.com  Sun Jan 20 18:52:21 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Sun, 20 Jan 2008 11:52:21 -0600
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
 object(s) to utilize.
In-Reply-To: <fmvquf$9hm$1@ger.gmane.org>
References: <fmvquf$9hm$1@ger.gmane.org>
Message-ID: <47938A55.5050900@braverock.com>

Joe W. Byers wrote:
> I am soliciting your opinions on the different time series objects as to 
> which you prefer, or which you do not prefer.  I have been using 
> dataframes and date columns to handle my timeseries processes for awhile 
> now, and recently began experimenting with different timeseries 
> packages, mainly: zoo, ts, tseries, Rmetric's timeSequences, and 
> rseries.  There are two reasons for my change here
> 1.  Utilize the object oriented components of these packages,
> 2.  Standardization for handling the nuances of my time series data.
> 
> 
> Both of these are important, but the second one is specific to my 
> industry: Energy.  I work with assets where the transfer of physical 
> molecules occur over time (ratable) under constraints, not immediately 
> as with fixed income and equities.  I have assets that settle yearly, 
> qrtly, monthly, weekly, balance of the week, balance of the month, 
> daily, hourly, and as low as 5 minute intervals.  I have assets where 
> the underlying is spread over different regions or zones, and the 
> underlying is the asset, an average, an index, a lookback, and many 
> more.  All of these require careful manipulation of the timeseries with 
> great care for calendars to maintain integrity of the system where the 
> energy asset is located.
> 
> I could continue babbling on about my issues but will leave it here with 
> this introduction.  I would appreciate any and all comments.  Also, if 
> this topic should be on the r-users mailing list, I do apologize for my 
> indiscretion. I thought this would be better since the underlying data 
> is financial/business in nature.

I think that this is the right venue for this discussion.

I think that ts and tseries can be basically written off in favor of 
zoo, which is much better and fully backwards compatible because of 
robust as. methods and proper use of reclass()

I would *like* to say that the Rmetrics timeSeries class is the right 
one for your needs, because of its support for multiple financial 
centers.  If you have intraday-data from multiple locations around the 
world and need to line it all up, timeseries is currently the only class 
that will solve your problems.

That said, timeSeries has some serious interoperability issues with the 
rest of R.  These could be easily solved with some as. (most notably an 
as.zoo method that worked well) methods, but those haven't been publicized.

Jeff Ryan recently published an extension to zoo called xts.  I think it 
might address many of the issues you describe, as well as offering 
better compatibility between timeSeries and zoo.  It also has many 
features for manipulating the periods (days,weeks,moths,quarters) that 
should be highly useful for the application you describe.

Regards,

    - Brian


From ggrothendieck at gmail.com  Sun Jan 20 20:19:57 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 20 Jan 2008 14:19:57 -0500
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
	object(s) to utilize.
In-Reply-To: <47938A55.5050900@braverock.com>
References: <fmvquf$9hm$1@ger.gmane.org> <47938A55.5050900@braverock.com>
Message-ID: <971536df0801201119l5992d02dlbe8f768849e3e3d1@mail.gmail.com>

On Jan 20, 2008 12:52 PM, Brian G. Peterson <brian at braverock.com> wrote:
>
> I think that ts and tseries can be basically written off in favor of
> zoo, which is much better and fully backwards compatible because of
> robust as. methods and proper use of reclass()
>
> I would *like* to say that the Rmetrics timeSeries class is the right
> one for your needs, because of its support for multiple financial
> centers.  If you have intraday-data from multiple locations around the
> world and need to line it all up, timeseries is currently the only class
> that will solve your problems.

The decision on what time index to use should be separate from
what time series to use.   The advantage of the zoo framework is
that you can use any time index (for which there exist appropriate
methods or you can write them youself without having to change zoo).

Thus you can have a single framework that can in principle encompass
the xts zoo subclass, yearmon, yearqtr, Date, chron, POSIXct,
financial centers (assuming appropriate methods), numbers, complex
numbers, letters, etc.  Furthermore you could change your mind on
time index yet keep the same time series framework.


From ggrothendieck at gmail.com  Sun Jan 20 20:41:17 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 20 Jan 2008 14:41:17 -0500
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
	object(s) to utilize.
In-Reply-To: <18323.30048.977818.40873@ron.nulle.part>
References: <fmvquf$9hm$1@ger.gmane.org>
	<18323.30048.977818.40873@ron.nulle.part>
Message-ID: <971536df0801201141t4e6cb504ided76f42e3ec0708@mail.gmail.com>

On Jan 20, 2008 11:22 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> On 20 January 2008 at 09:55, Joe W. Byers wrote:
> | I am soliciting your opinions on the different time series objects as to
> | which you prefer, or which you do not prefer.  I have been using
> | dataframes and date columns to handle my timeseries processes for awhile
> | now, and recently began experimenting with different timeseries
> | packages, mainly: zoo, ts, tseries, Rmetric's timeSequences, and
> | rseries.  There are two reasons for my change here
> | 1.  Utilize the object oriented components of these packages,
> | 2.  Standardization for handling the nuances of my time series data.
>
> For 'financial data' in the large sense I have been using zoo for years,
> after having used its. I have also used ts for some ultra-low-frequency
> macro-economic series, but ts becomes useless once you leave montly or
> quarterly frequencies and their fixed delta-t. I have basically no experience
> with the others.

The problem with allowing non-unique time series is that there are
multiple ways to merge them and you don't know what the user wants so
rather than make a choice that is unexpected or undesired its better to
avoid them.  zoo does support non-unique time series with warnings to the
extent that is needed to allow the user to resolve the non-uniqueness.
Furthermore its not just a matter of adding arguments to merge since nearly
every significant operation in zoo such as +, - etc. involves a merge
underneath.

read.zoo(), aggregate.zoo() and zoo() all allow this non-uniqueness
with a warning
so you can still manipluate the series within the zoo framework
sufficiently to get
your series into a proper form for more serious work.

>
> zoo is good as it does all the merging, subsetting, plotting, indexing,
> converting to and from (ie I often 'drop' to data.frame(coredata(zooobject))
> for analysis) I need.  Working with high-frequency data, I occassionally hit
> the constraint of 'has to have unique indices' in which case I just pad a
> micro-second each to the occassional identical timestamp. That's a small
> price to pay.
>
> You did not mention xts, which is the newest member of the clan. It extends
> zoo and allows 'annotations', ie you can set attributes about the when /
> where / how / ... of your data. Xts also overcomes the uniqueness constraint
> for the index (and I am not sure how they do it) and offers richer 'logical'
> subsetting.  It looks like a winner, and this may just what you need. Have a
> look at xts.  Usual caveat: Haven't used it myself yet...
>

xts is a zoo subclass so it seems unlikely it has different uniqueness
constraints or that it works differently with logical variables.  Perhaps you
are referring to its :: notation which seems different than described above.


From alexander.f.moreno at gmail.com  Sun Jan 20 23:47:21 2008
From: alexander.f.moreno at gmail.com (Alexander Moreno)
Date: Sun, 20 Jan 2008 16:47:21 -0600
Subject: [R-SIG-Finance] threshold autoregression
Message-ID: <3303a4570801201447u3abd381dr9b415342c4746168@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080120/87498d46/attachment.pl 

From ezivot at u.washington.edu  Mon Jan 21 00:23:50 2008
From: ezivot at u.washington.edu (Eric Zivot)
Date: Sun, 20 Jan 2008 15:23:50 -0800 (PST)
Subject: [R-SIG-Finance] books?
In-Reply-To: <fmvrlc$bpa$1@ger.gmane.org>
Message-ID: <Pine.LNX.4.43.0801201523500.5885@hymn33.u.washington.edu>

For multivariate time series analysis i recommend the following

1. (New) Introduction to multiple time series by Helmut Lutkepohl (springer verlag). This book is really the multiple time serie bible

2. Analysis of financial time series, second edition, by Ruey Tsay (wiley)

3. Modeling financial time series with Splus, 2nd edition by Eric Zivot and Jiahui wang (springer)


****************************************************************
*  Eric Zivot                  			               *
*  Associate Professor         phone:  206-543-6715            *
*  Department of Economics     fax:    206-685-7477            *
*  Box 353330                  email:  ezivot at u.washington.edu *
*  University of Washington                                    *
*  Seattle, WA 98195-3330                                      *
*                                                              *
*  www:  http://faculty.washington.edu/ezivot                  *
****************************************************************

On Sun, 20 Jan 2008, Joe W. Byers wrote:

> Max Nevill wrote:
>>
>>
>> Hi Everyone!
>>
>> I do some forecasts for my job and recently have started to realize that
>> I may be able to make better forecasts if I look at multivariate
>> analysis rather than fit several univariate models. Unfortunately in
>> undergrad I only took one course in univarate analysis.
>>
>> I was wondering what some good books are on the subject of multivariate
>> time series analysis? Especially any that use R over other languages.
>>
>> Thanks,
>>
>> -Max
>>
>> _________________________________________
>>
>>
>> 	[[alternative HTML version deleted]]
>>
> My base references for time series are
> Statistical Analysis of Financial Data in S-Plus by Carmona
> Time Series Analysis by Hamilton
> and
> Applied Econometric Time Series by Enders
>
> I also use several econometric texts
> Judge, etal's series
> 	Theory and Practice of Econometrics,
> 	Introduction to the ditto, and
> 	Learning and Practicing Econometrics
> Greene's Econometric Analysis.
>
> A side note, my wife and I are building a bibliography of our personal
> libraries of books and papers on www.financialseal.com under analytics
> and research.   It is in the infancy stages, but we are trying to update
> it regularly.  These and other financial and statistical references will
> be there.  The connection is slow, so patience is required.
>
> Joe
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From spencer.graves at pdf.com  Mon Jan 21 02:31:27 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 20 Jan 2008 17:31:27 -0800
Subject: [R-SIG-Finance] books?
In-Reply-To: <Pine.LNX.4.43.0801201523500.5885@hymn33.u.washington.edu>
References: <Pine.LNX.4.43.0801201523500.5885@hymn33.u.washington.edu>
Message-ID: <4793F5EF.9000601@pdf.com>

Hi, Eric: 

      Is there an R package to support your book? 

      You may know that I'm working on a "FinTS" package to support 
Tsay's book.  The version currently available from CRAN includes script 
files to reproduce most of the examples, figures and tables in chapters 
1 and 2 plus portions of chapters 3 and 11. 

      Diethelm Wuertz recommended Lutkepohl to me, but I don't know to 
what extent R functions have been written to facilitate using the 
techniques in that book. 

      Best Wishes,
      Spencer

Eric Zivot wrote:
> For multivariate time series analysis i recommend the following
>
> 1. (New) Introduction to multiple time series by Helmut Lutkepohl (springer verlag). This book is really the multiple time serie bible
>
> 2. Analysis of financial time series, second edition, by Ruey Tsay (wiley)
>
> 3. Modeling financial time series with Splus, 2nd edition by Eric Zivot and Jiahui wang (springer)
>
>
> ****************************************************************
> *  Eric Zivot                  			               *
> *  Associate Professor         phone:  206-543-6715            *
> *  Department of Economics     fax:    206-685-7477            *
> *  Box 353330                  email:  ezivot at u.washington.edu *
> *  University of Washington                                    *
> *  Seattle, WA 98195-3330                                      *
> *                                                              *
> *  www:  http://faculty.washington.edu/ezivot                  *
> ****************************************************************
>
> On Sun, 20 Jan 2008, Joe W. Byers wrote:
>
>   
>> Max Nevill wrote:
>>     
>>> Hi Everyone!
>>>
>>> I do some forecasts for my job and recently have started to realize that
>>> I may be able to make better forecasts if I look at multivariate
>>> analysis rather than fit several univariate models. Unfortunately in
>>> undergrad I only took one course in univarate analysis.
>>>
>>> I was wondering what some good books are on the subject of multivariate
>>> time series analysis? Especially any that use R over other languages.
>>>
>>> Thanks,
>>>
>>> -Max
>>>
>>> _________________________________________
>>>
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>>       
>> My base references for time series are
>> Statistical Analysis of Financial Data in S-Plus by Carmona
>> Time Series Analysis by Hamilton
>> and
>> Applied Econometric Time Series by Enders
>>
>> I also use several econometric texts
>> Judge, etal's series
>> 	Theory and Practice of Econometrics,
>> 	Introduction to the ditto, and
>> 	Learning and Practicing Econometrics
>> Greene's Econometric Analysis.
>>
>> A side note, my wife and I are building a bibliography of our personal
>> libraries of books and papers on www.financialseal.com under analytics
>> and research.   It is in the infancy stages, but we are trying to update
>> it regularly.  These and other financial and statistical references will
>> be there.  The connection is slow, so patience is required.
>>
>> Joe
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>     
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From edd at debian.org  Mon Jan 21 02:40:49 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 20 Jan 2008 19:40:49 -0600
Subject: [R-SIG-Finance] books?
In-Reply-To: <Pine.LNX.4.43.0801201523500.5885@hymn33.u.washington.edu>
References: <fmvrlc$bpa$1@ger.gmane.org>
	<Pine.LNX.4.43.0801201523500.5885@hymn33.u.washington.edu>
Message-ID: <18323.63521.368862.364214@ron.nulle.part>


Hi Eric,

Thanks for your continued contribution to the list which are really
appreciated.

On 20 January 2008 at 15:23, Eric Zivot wrote:
| For multivariate time series analysis i recommend the following
| 3. Modeling financial time series with Splus, 2nd edition by Eric Zivot and Jiahui wang (springer)

I own the first edition, and quite like it as an applied finance book. The
main shortcoming for me as a non-SPlus user is its focus on SPlus as an
implementation language, and the requirement of SPlus and the add-on
packages. 

Does the 2nd edition offer anything more that is useful to the R user?

Regards, Dirk

-- 
Three out of two people have difficulties with fractions.


From atp at piskorski.com  Mon Jan 21 19:35:40 2008
From: atp at piskorski.com (Andrew Piskorski)
Date: Mon, 21 Jan 2008 13:35:40 -0500
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
	object(s) to utilize.
In-Reply-To: <fmvquf$9hm$1@ger.gmane.org>
References: <fmvquf$9hm$1@ger.gmane.org>
Message-ID: <20080121183539.GB12556@piskorski.com>

On Sun, Jan 20, 2008 at 09:55:40AM -0600, Joe W. Byers wrote:
> I am soliciting your opinions on the different time series objects as to 
> which you prefer, or which you do not prefer.  I have been using 

I don't know anything about which R time series packages are better
(or for what), but I do have a list of some of the ones that exist:

    zoo: http://cran.r-project.org/src/contrib/Descriptions/zoo.html
    dse: http://www.bank-banque-canada.ca/pgilbert/
    its: http://cran.r-project.org/src/contrib/Descriptions/its.html
   urca: http://cran.r-project.org/src/contrib/Descriptions/urca.html
    ast: http://sirio.stat.unipd.it/index.php?id=libast
tseries: http://cran.r-project.org/src/contrib/Descriptions/tseries.html
   tsa2: http://www.stat.pitt.edu/stoffer/tsa2/index.html
         http://www.amazon.com/Time-Analysis-Its-Applications-Statistics/dp/0387293175/
RTisean, tsdyn, and tserieschaos:
  http://antonio.fabio.googlepages.com/rprojects
And even more via 'label:timeseries' on Google Code:
  http://code.google.com/hosting/search?q=label:timeseries&filter=0

And here's my super-short summary of what others have said elsewhere
in this thread:

- zoo and/or xts are good.
- zoo and/or xts basically make ts and tseries obsolete.
- Rmetrics' timeSeries is useful but has certain problems.

Useful hints!

-- 
Andrew Piskorski <atp at piskorski.com>
http://www.piskorski.com/


From ggrothendieck at gmail.com  Mon Jan 21 21:13:43 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 21 Jan 2008 15:13:43 -0500
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
	object(s) to utilize.
In-Reply-To: <20080121183539.GB12556@piskorski.com>
References: <fmvquf$9hm$1@ger.gmane.org> <20080121183539.GB12556@piskorski.com>
Message-ID: <971536df0801211213n5d35e87el578dbb6259a9451f@mail.gmail.com>

On Jan 21, 2008 1:35 PM, Andrew Piskorski <atp at piskorski.com> wrote:
> On Sun, Jan 20, 2008 at 09:55:40AM -0600, Joe W. Byers wrote:
> > I am soliciting your opinions on the different time series objects as to
> > which you prefer, or which you do not prefer.  I have been using
>
> I don't know anything about which R time series packages are better
> (or for what), but I do have a list of some of the ones that exist:
>
>    zoo: http://cran.r-project.org/src/contrib/Descriptions/zoo.html
>    dse: http://www.bank-banque-canada.ca/pgilbert/
>    its: http://cran.r-project.org/src/contrib/Descriptions/its.html
>   urca: http://cran.r-project.org/src/contrib/Descriptions/urca.html
>    ast: http://sirio.stat.unipd.it/index.php?id=libast
> tseries: http://cran.r-project.org/src/contrib/Descriptions/tseries.html
>   tsa2: http://www.stat.pitt.edu/stoffer/tsa2/index.html
>         http://www.amazon.com/Time-Analysis-Its-Applications-Statistics/dp/0387293175/
> RTisean, tsdyn, and tserieschaos:
>  http://antonio.fabio.googlepages.com/rprojects
> And even more via 'label:timeseries' on Google Code:
>  http://code.google.com/hosting/search?q=label:timeseries&filter=0

I think many of these are focused on time series functions rather than
time series representations which seems to be more the topic of
discussion.

The CRAN Task Views on empirical finance and econometrics have some
discussion and lists of the time series packages in R.

>
> And here's my super-short summary of what others have said elsewhere
> in this thread:
>
> - zoo and/or xts are good.

Maybe that is going a bit far in cursory treatment.

xts
- what xts is is a subclass of zoo with its own is, as, coredata, print
and [ methods.
- it requires use of Date or POSIXct as the index class.  This has
the advantage that it can make certain assumptions which its
superclass zoo cannot make due to zoo's greater generality.
For example, the :: notation (which is roughly a counterpart of the
window() function in ts and zoo) is made possible by the fact that
it knows that the index is Date or POSIXct whereas zoo's
index could be practically anything.  On the other hand, with
xts you cannot directly model a quarterly series, say, other
than to embed it into a daily series, say.  However, as xts
is a zoo subclass you can directly access all zoo functionality
whenever you want so it would not be a problem to use xts
to model a daily Date class series and then convert it to yearmon
and keep going using just zoo at that point.
- xts has an as.xts.timeSeries method which can convert
timeSeries objects to xts/zoo.
- xts is at version 0.0.x and is not ready for wide use (all
the examples posted here would not work when I tried them)
but over time xts may gain maturity.  Given the interest shown
I would expect it might proceed rapidly.

zoo is a mature package with two vignettes (three in the next
version) that has been around quite a while now and provides
a general framework for modelling irregularly spaces series.
A description of zoo can be found in
library(zoo)
vignette("zoo")
vignette("zoo-quickref")
There are many other packages that use it or add
infrastructure to it such as xts, quantmod, dyn,
strucchange, etc.

> - zoo and/or xts basically make ts and tseries obsolete.

I don't think they make ts obsolete.  zoo (and therefore xts)
models irregularly spaced series and zoo also has a model for
irregularly spaced series which having an underlying regularity
whereas ts is based on strictly regular series.  Of course both
of zoo's irregular ones include regular ones as a subset but there
may be some situations where you prefer to model the regularity
directly.  Also ts is in the core of R and zoo and xts are not.
I use ts sometimes although I use zoo more.

tseries contains a time series model, irts.  The rest of tseries
consists of various time series functions and do not depend on irts
thus tseries as a package is not obsolete.

> - Rmetrics' timeSeries is useful but has certain problems.

Its main advantages are that many or at least some Rmetrics
functions use or can use that representation and it supports
financial centers.

Financial centers is really a property of the index class and not
necessarily of the time series so one could support this in zoo as
well by defining an appropriate index class.  e.g. as.xts.timeSeries


From marcl at svquant.com  Mon Jan 21 23:08:10 2008
From: marcl at svquant.com (Marc E Levitt)
Date: Mon, 21 Jan 2008 14:08:10 -0800 (PST)
Subject: [R-SIG-Finance] TTR
Message-ID: <424415.59814.qm@web32206.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080121/9d55aaa5/attachment.pl 

From ecjbosu at aol.com  Mon Jan 21 23:12:00 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Mon, 21 Jan 2008 16:12:00 -0600
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
 object(s) to utilize.
In-Reply-To: <971536df0801211213n5d35e87el578dbb6259a9451f@mail.gmail.com>
References: <fmvquf$9hm$1@ger.gmane.org> <20080121183539.GB12556@piskorski.com>
	<971536df0801211213n5d35e87el578dbb6259a9451f@mail.gmail.com>
Message-ID: <479518B0.6050507@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080121/5e75a35c/attachment.pl 

From jeff.a.ryan at gmail.com  Mon Jan 21 23:12:16 2008
From: jeff.a.ryan at gmail.com (jeff.a.ryan at gmail.com)
Date: Mon, 21 Jan 2008 22:12:16 +0000
Subject: [R-SIG-Finance] Solicitation of opinions on which
	Timeseriesobject(s) to utilize.
In-Reply-To: <971536df0801211213n5d35e87el578dbb6259a9451f@mail.gmail.com>
References: <fmvquf$9hm$1@ger.gmane.org>
	<20080121183539.GB12556@piskorski.com><971536df0801211213n5d35e87el578dbb6259a9451f@mail.gmail.com>
Message-ID: <2074483611-1200953658-cardhu_decombobulator_blackberry.rim.net-1116687053-@bxe111.bisx.prod.on.blackberry>

I thought I'd clarify some items that Gabor brought up.

xts uses zoo as opposed to rewriting zoo functionality because zoo is essentially perfect.  Its generality is at the core of that.  It is all I ever use and technically what I continue to use - xts just wraps it along the lines of what I need - namely with time-based indicies.

In fact the extension of zoo to xts is because of the reasons that Gabor mentioned - most notably by enforcing the time-based index we can make conversions among classes more robust, and use fancy looking subsetting that is mostly a shortcut to what has always been possible with other commands or series of commands. By making assumptions about the index it can save a lot of as.Date/etc typing...and make things that otherwise would be difficult (error-prone) relatively easy.

The other aspect is the addition of attributes to allow for more detail to be added to an object.  For instance it would be trivial to add the timezone/financial center data to the object.

By relying on zoo it also keeps everything designed for zoo working without modification.

Its version number at 0.0-6 (on r-forge, as it hasn't been pushed to CRAN since 0.0-3) is for a good reason.  Most things work 90+% of the time, but there are bugs that are yet to be worked out.  It is also currently being decided as to what it _should_ do and how to accomplish those tasks. It is probably worthy of a higher number, but I would like it to have some more time... I suspect Gabor had an earlier version which didn't include much of what is there now.

Additionally some of the time-based functionality of quantmod (first/last/breakpoints (now endpoints)/periodicity/to.period) is being transitioned to xts.  The biggest reason xts is on CRAN at all this early is to make this transition as clean as possible.

At present the immediate benefit to xts is in fast and relatively painless coversion to and from the myriad of time-series classes in R.  Basically as.xts takes everything in the object being coverted, saves it to its xtsAttributes list (really just attributes that are hidden during printing) and makes the object behave like a zoo/xts object.  So everything that xts can do, and more importantly, everything that zoo can do is available.  Calling 'reclass' turns it back to the original form - everything intact.  So function development can be from a zoo/xts perspective, yet the input/output user experience will be in whichever class they originally used.  The best example of this is the 'to.period' routine in xts - which takes any of the main class types (timeSeries, zoo, its, irts, ts, ...) and aggregates to some different frequency (to.monthly from daily say) and returns an object *of the ORIGINAL class*.  It works quite well, and to the end user it is seemless.  This function in particular (to.period) even modifies the index class of the returned object if possible to something more logical - such as yearmon for monthly series.  Basically makes writing methods for all possible cases unnecessary, which makes development faster and the user experience seemless.

All that said, xts is still very much a work in progress and any suggestions as to direction/addition/deletion would be greatly appreciated.

We do hope to have at least one vignette available in the coming weeks covering the purpose of xts, as well as its functions/methods.

Jeff
Sent via BlackBerry from T-Mobile

-----Original Message-----
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>

Date: Mon, 21 Jan 2008 15:13:43 
To:r-sig-finance at stat.math.ethz.ch, "Joe W. Byers" <ecjbosu at aol.com>
Subject: Re: [R-SIG-Finance] Solicitation of opinions on which Timeseries
	object(s) to utilize.


On Jan 21, 2008 1:35 PM, Andrew Piskorski <atp at piskorski.com> wrote:
> On Sun, Jan 20, 2008 at 09:55:40AM -0600, Joe W. Byers wrote:
> > I am soliciting your opinions on the different time series objects as to
> > which you prefer, or which you do not prefer.  I have been using
>
> I don't know anything about which R time series packages are better
> (or for what), but I do have a list of some of the ones that exist:
>
>    zoo: http://cran.r-project.org/src/contrib/Descriptions/zoo.html
>    dse: http://www.bank-banque-canada.ca/pgilbert/
>    its: http://cran.r-project.org/src/contrib/Descriptions/its.html
>   urca: http://cran.r-project.org/src/contrib/Descriptions/urca.html
>    ast: http://sirio.stat.unipd.it/index.php?id=libast
> tseries: http://cran.r-project.org/src/contrib/Descriptions/tseries.html
>   tsa2: http://www.stat.pitt.edu/stoffer/tsa2/index.html
>         http://www.amazon.com/Time-Analysis-Its-Applications-Statistics/dp/0387293175/
> RTisean, tsdyn, and tserieschaos:
>  http://antonio.fabio.googlepages.com/rprojects
> And even more via 'label:timeseries' on Google Code:
>  http://code.google.com/hosting/search?q=label:timeseries&filter=0

I think many of these are focused on time series functions rather than
time series representations which seems to be more the topic of
discussion.

The CRAN Task Views on empirical finance and econometrics have some
discussion and lists of the time series packages in R.

>
> And here's my super-short summary of what others have said elsewhere
> in this thread:
>
> - zoo and/or xts are good.

Maybe that is going a bit far in cursory treatment.

xts
- what xts is is a subclass of zoo with its own is, as, coredata, print
and [ methods.
- it requires use of Date or POSIXct as the index class.  This has
the advantage that it can make certain assumptions which its
superclass zoo cannot make due to zoo's greater generality.
For example, the :: notation (which is roughly a counterpart of the
window() function in ts and zoo) is made possible by the fact that
it knows that the index is Date or POSIXct whereas zoo's
index could be practically anything.  On the other hand, with
xts you cannot directly model a quarterly series, say, other
than to embed it into a daily series, say.  However, as xts
is a zoo subclass you can directly access all zoo functionality
whenever you want so it would not be a problem to use xts
to model a daily Date class series and then convert it to yearmon
and keep going using just zoo at that point.
- xts has an as.xts.timeSeries method which can convert
timeSeries objects to xts/zoo.
- xts is at version 0.0.x and is not ready for wide use (all
the examples posted here would not work when I tried them)
but over time xts may gain maturity.  Given the interest shown
I would expect it might proceed rapidly.

zoo is a mature package with two vignettes (three in the next
version) that has been around quite a while now and provides
a general framework for modelling irregularly spaces series.
A description of zoo can be found in
library(zoo)
vignette("zoo")
vignette("zoo-quickref")
There are many other packages that use it or add
infrastructure to it such as xts, quantmod, dyn,
strucchange, etc.

> - zoo and/or xts basically make ts and tseries obsolete.

I don't think they make ts obsolete.  zoo (and therefore xts)
models irregularly spaced series and zoo also has a model for
irregularly spaced series which having an underlying regularity
whereas ts is based on strictly regular series.  Of course both
of zoo's irregular ones include regular ones as a subset but there
may be some situations where you prefer to model the regularity
directly.  Also ts is in the core of R and zoo and xts are not.
I use ts sometimes although I use zoo more.

tseries contains a time series model, irts.  The rest of tseries
consists of various time series functions and do not depend on irts
thus tseries as a package is not obsolete.

> - Rmetrics' timeSeries is useful but has certain problems.

Its main advantages are that many or at least some Rmetrics
functions use or can use that representation and it supports
financial centers.

Financial centers is really a property of the index class and not
necessarily of the time series so one could support this in zoo as
well by defining an appropriate index class.  e.g. as.xts.timeSeries

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

From josh.m.ulrich at gmail.com  Tue Jan 22 00:09:17 2008
From: josh.m.ulrich at gmail.com (Josh Ulrich)
Date: Mon, 21 Jan 2008 17:09:17 -0600
Subject: [R-SIG-Finance] TTR
In-Reply-To: <424415.59814.qm@web32206.mail.mud.yahoo.com>
References: <424415.59814.qm@web32206.mail.mud.yahoo.com>
Message-ID: <8cca69990801211509j2d2eaa24v24544795060a4a40@mail.gmail.com>

Marc,

We have discussed using ta-lib, which is a very fine library.  We
concluded it was too much overhead code for our purposes.  You can see
most of TTR is written in R, with a few 'grunt' functions written in
Fortran for speed.

Also, one main goal of TTR is flexibility.  For example, you can use
two different, non-standard moving averages in the RSI calculation.
We're not proficient in C/C++, so adapting such code for flexibility
is more difficult (for us) than writing in R, and we don't expect the
speeds gains from doing so to be terribly significant.

I'm not opposed to using open-source TA libraries, so long as they fit
with the overall plans for TTR.  Given our absence of C/C++ abilities,
someone else would have to volunteer to make the additions.

Best,
Josh
--
http://quantemplation.blogspot.com


On Jan 21, 2008 4:08 PM, Marc E Levitt <marcl at svquant.com> wrote:
> Josh (& Jeff):
>
> Thanks for making TTR available to the community. Your implementation of some functions in FORTRAN caught my eye and I was just wondering (or perhaps suggesting)  if you checked out wrapping ta-lib vs redoing this work? The site is www.ta-lib.org and it appears to be supported, debugged, and used by a good sized community.
>
> I understand that wrapping a library is less of a learning experience than writing R code - but now that FORTRAN has entered the picture perhaps we can unify some open source TA libraries?
>
> Regards,
> Marc
>
>
> ----- Original Message ----
>
>
> I second John's response.  The EMA function in package TTR uses a
> seeding value of
> ema[n] <- mean(x[1:n])
> and then begins using the exponential smoothing.  You can then compare
> the results of emaTA with EMA and see that the results converge after
> the run-in period.
>
> TTR contains another 30+ TA functions, many of which are chartable via
> the quantmod package.  It is being updated with new functions and
> Fortran implementations of existing functions.  TTR's development
> source code is here:
> https://r-forge.r-project.org/projects/ttr/
>
> HTH,
> Josh
>
> PS Sorry if this is my second reply.  I replied on Friday, but it
> didn't seem to make it to the list.
> --
> http://quantemplation.blogspot.com
>
> ------------------------------
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From MichelBeck at sbcglobal.Net  Tue Jan 22 16:25:19 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Tue, 22 Jan 2008 15:25:19 +0000 (UTC)
Subject: [R-SIG-Finance] Financial Basket Options
References: <loom.20080114T151149-759@post.gmane.org>
	<367338.236.qm@web32206.mail.mud.yahoo.com>
Message-ID: <loom.20080122T151337-838@post.gmane.org>

Moshe Olshansky <m_olshansky <at> yahoo.com> writes:

> 
> Hi Michel,
> 
> I wrote an R code implementing Longstaff-Schwartz
> algorithm for pricing American put Basket option (on
> the portfolio value). This code can be easily changed
> to value call option (I intend to allow for "any"
> payoff function in the future).
> I can send you this code. It is in a very preliminary
> state, so even though I did some basic testing I can
> not guarantee it to be correct.
> 
> Regards,
> 
> Moshe.
> 
> --- MAB <MichelBeck <at> sbcglobal.Net> wrote:
>

> 
> 
Hi Moshe,

Thanks for your reply.

Yes, I would appreciate getting the code, although I'm not sure I am able to 
convert it to valuing call options (European call is what I'm looking at).

Regards,

Michel


From ecjbosu at aol.com  Wed Jan 23 05:08:15 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Tue, 22 Jan 2008 22:08:15 -0600
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
	object(s) to utilize.
In-Reply-To: <20080121183539.GB12556@piskorski.com>
References: <fmvquf$9hm$1@ger.gmane.org> <20080121183539.GB12556@piskorski.com>
Message-ID: <4796BDAF.3050109@aol.com>

Andrew Piskorski wrote:
> On Sun, Jan 20, 2008 at 09:55:40AM -0600, Joe W. Byers wrote:
>> I am soliciting your opinions on the different time series objects as to 
>> which you prefer, or which you do not prefer.  I have been using 
> 
> I don't know anything about which R time series packages are better
> (or for what), but I do have a list of some of the ones that exist:
> 
>     zoo: http://cran.r-project.org/src/contrib/Descriptions/zoo.html
>     dse: http://www.bank-banque-canada.ca/pgilbert/
>     its: http://cran.r-project.org/src/contrib/Descriptions/its.html
>    urca: http://cran.r-project.org/src/contrib/Descriptions/urca.html
>     ast: http://sirio.stat.unipd.it/index.php?id=libast
> tseries: http://cran.r-project.org/src/contrib/Descriptions/tseries.html
>    tsa2: http://www.stat.pitt.edu/stoffer/tsa2/index.html
>          http://www.amazon.com/Time-Analysis-Its-Applications-Statistics/dp/0387293175/
> RTisean, tsdyn, and tserieschaos:
>   http://antonio.fabio.googlepages.com/rprojects
> And even more via 'label:timeseries' on Google Code:
>   http://code.google.com/hosting/search?q=label:timeseries&filter=0
> 
> And here's my super-short summary of what others have said elsewhere
> in this thread:
> 
> - zoo and/or xts are good.
> - zoo and/or xts basically make ts and tseries obsolete.


> - Rmetrics' timeSeries is useful but has certain problems.
> 
The certain problems with timeDate objects of Rmetrics is one of the 
reasons I posted this question.  I have found that even though you can 
give the timeSeries a financial center and a zone, the underlying 
timeDate object still defaults to GMT.  I have to pass or set the 
tz='EST5EDT' or something like that to get the correct numerical time 
integers.  I have not quite figured this one out yet.  This may also be 
an underlying problem with other timeseries packages but I thought 
determining the "best" timeseries package before progressing further was 
a better course .

Thank you all
Joe

> Useful hints!
>


From ggrothendieck at gmail.com  Wed Jan 23 05:18:38 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 22 Jan 2008 23:18:38 -0500
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
	object(s) to utilize.
In-Reply-To: <4796BDAF.3050109@aol.com>
References: <fmvquf$9hm$1@ger.gmane.org> <20080121183539.GB12556@piskorski.com>
	<4796BDAF.3050109@aol.com>
Message-ID: <971536df0801222018x7fe7b366l19ee56eda114129d@mail.gmail.com>

On Jan 22, 2008 11:08 PM, Joe W. Byers <ecjbosu at aol.com> wrote:

> > - Rmetrics' timeSeries is useful but has certain problems.
> >
> The certain problems with timeDate objects of Rmetrics is one of the
> reasons I posted this question.  I have found that even though you can
> give the timeSeries a financial center and a zone, the underlying
> timeDate object still defaults to GMT.  I have to pass or set the
> tz='EST5EDT' or something like that to get the correct numerical time
> integers.  I have not quite figured this one out yet.  This may also be
> an underlying problem with other timeseries packages but I thought
> determining the "best" timeseries package before progressing further was
> a better course .
>

That's to avoid the problems with POSIXct discussed in R News 4/1
(which you should read as it will give perspective on this).
All Rmetrics routines set TZ="GMT" going in and unset it going out
so that such problems are avoided.

This is really a defensive programming feature in Rmetrics that
makes it more robust, not a true problem.


From wojciech.slusarski at gmail.com  Wed Jan 23 11:18:39 2008
From: wojciech.slusarski at gmail.com (Wojciech Slusarski)
Date: Wed, 23 Jan 2008 11:18:39 +0100
Subject: [R-SIG-Finance] Financial Basket Options
In-Reply-To: <367338.236.qm@web32206.mail.mud.yahoo.com>
References: <loom.20080114T151149-759@post.gmane.org>
	<367338.236.qm@web32206.mail.mud.yahoo.com>
Message-ID: <5e64e5be0801230218y3655baabi5405aff1a3a066da@mail.gmail.com>

Hi Mosche,

Once I have implemented L-S algorithm for pricing american-asian
options. I am wondering, how stable it is for basket options. What
sort of polynomials did you use? Are you regressing only paths that
are in-the-money, or all? If yes, what do you do, when the number of
paths is lower, than the number of coefficients in the regression?
Unfourtunately my code is not a generic one, and is not useful for
pricing different option from the one I was pricing, so sharing that
code would not bring any benefit to anybody, though if that is
possible, I would appreciate a lot if you could send me your
implementation if that is a flexible code.

Best regards,
Wojciech

2008/1/16, Moshe Olshansky <m_olshansky at yahoo.com>:
> Hi Michel,
>
> I wrote an R code implementing Longstaff-Schwartz
> algorithm for pricing American put Basket option (on
> the portfolio value). This code can be easily changed
> to value call option (I intend to allow for "any"
> payoff function in the future).
> I can send you this code. It is in a very preliminary
> state, so even though I did some basic testing I can
> not guarantee it to be correct.
>
> Regards,
>
> Moshe.
>
> --- MAB <MichelBeck at sbcglobal.Net> wrote:
>
> > Hi!
> >
> > I am looking for R code to price a multiple asset
> > basket option.
> > So far I only found a 2 asset pricer (in the
> > R-metrics packafe fExoticOption).
> > Where could I find some code for at least 4 assets?
> >
> > Thank you,
> >
> > Michel Beck
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From barth at tac-financial.com  Wed Jan 23 17:32:50 2008
From: barth at tac-financial.com (Sylvain BARTHELEMY)
Date: Wed, 23 Jan 2008 17:32:50 +0100
Subject: [R-SIG-Finance] threshold autoregression
In-Reply-To: <3303a4570801201447u3abd381dr9b415342c4746168@mail.gmail.com>
References: <3303a4570801201447u3abd381dr9b415342c4746168@mail.gmail.com>
Message-ID: <008c01c85ddd$998ed750$ccac85f0$@com>

Hi Alex,

If you want to play with SETAR and more generally with Markov switching
models, my view is that you should use another software than R. 

There is a great pack written by Hanz Martin Krozlig. But as soon as I know,
it only works on Ox and it has not been ported to R. I think that it would
be a great idea to rewrite the pack for R. I initially planned to do one,
but it takes time...

There is also a pack written by Franck Arnaud, that was written for R and is
available here: http://arnaud.ensae.net/Rressources/Rressources.html
But it seems to be a beta version and less powerful than the Krozlig one.

Regards

---
Sylvain Barth?l?my
Research Director, TAC
www.tac-financial.com | www.sylbarth.com


-----Message d'origine-----
De?: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de Alexander
Moreno
Envoy??: dimanche 20 janvier 2008 23:47
??: r-sig-finance at stat.math.ethz.ch
Objet?: [R-SIG-Finance] threshold autoregression

Hi,

Does anyone know of anyone packages to fit tar and setar time series models?

Thanks,
Alex

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

No virus found in this incoming message.



14:15






20:12


From m_olshansky at yahoo.com  Wed Jan 23 23:46:26 2008
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 23 Jan 2008 14:46:26 -0800 (PST)
Subject: [R-SIG-Finance] Financial Basket Options
In-Reply-To: <loom.20080122T151337-838@post.gmane.org>
Message-ID: <992166.74600.qm@web32203.mail.mud.yahoo.com>

Hi Michel,

European option (either put or call) is a much easier
task than the American one since the optimal strategy
is known (and trivial).
Are any dividends expected before maturity? Are they
fixed amount (usually the case when the maturity date
is not too far away) or a known dividend yield?
If you wish I can send you this code as well.

I will try to slightly clean up my code during the
weekend and then I will send it to anyone interested.

Regards,

Moshe.

--- MAB <MichelBeck at sbcglobal.Net> wrote:

> Moshe Olshansky <m_olshansky <at> yahoo.com> writes:
> 
> > 
> > Hi Michel,
> > 
> > I wrote an R code implementing Longstaff-Schwartz
> > algorithm for pricing American put Basket option
> (on
> > the portfolio value). This code can be easily
> changed
> > to value call option (I intend to allow for "any"
> > payoff function in the future).
> > I can send you this code. It is in a very
> preliminary
> > state, so even though I did some basic testing I
> can
> > not guarantee it to be correct.
> > 
> > Regards,
> > 
> > Moshe.
> > 
> > --- MAB <MichelBeck <at> sbcglobal.Net> wrote:
> >
> 
> > 
> > 
> Hi Moshe,
> 
> Thanks for your reply.
> 
> Yes, I would appreciate getting the code, although
> I'm not sure I am able to 
> convert it to valuing call options (European call is
> what I'm looking at).
> 
> Regards,
> 
> Michel
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From m_olshansky at yahoo.com  Thu Jan 24 00:22:01 2008
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 23 Jan 2008 15:22:01 -0800 (PST)
Subject: [R-SIG-Finance] Financial Basket Options
In-Reply-To: <5e64e5be0801230218y3655baabi5405aff1a3a066da@mail.gmail.com>
Message-ID: <353848.90213.qm@web32211.mail.mud.yahoo.com>

Hi Wojciech,

To answer you question, I used all the polynomials
with total degree less than a certain number (poly in
R) and I regressed only for in the money paths. I
always used many paths (at least 10,000) and so I did
not run into the problem of not having enough of them
(even though this can happen in the future). I also
save the state of the random numbers generator so that
I can reproduce  the states of every path at every
time point and so I do not have to store all the paths
(storing them exhausts the memory very quickly).

I have a numerical question connected to this:
If there are N stocks then the number of all the
polynomial of degree <= 3 is C(N+3,3) where C(n,k) is
the binomial coefficient, and this is significantly
more than N. poly() produces that number of columns
and then least squares is applied. If the
corresponding matrix is X then the explicit formula
for least squares could be used ((t(X)*X)^-1...). To
calculate t(X)*X one can produce two columns at a
time, find their dot product and store the result. In
such a case we need memory space for only N+1 columns
(it may take a long time CPU-wise but in many cases
this is not an issue). 
One possible problem is that using the explicit
formula is numerically inferior to the QR algorithm
which is usually used in least squares. Can anybody
say anything about how bad this can be? Let me just
note that we do not need very high precision here.

Regards,

Moshe.

P.S. I started a new job last week and so I am pretty
busy these days. I will try to post my code (without
the use of the explicit formula) over the weekend.
--- Wojciech Slusarski <wojciech.slusarski at gmail.com>
wrote:

> Hi Mosche,
> 
> Once I have implemented L-S algorithm for pricing
> american-asian
> options. I am wondering, how stable it is for basket
> options. What
> sort of polynomials did you use? Are you regressing
> only paths that
> are in-the-money, or all? If yes, what do you do,
> when the number of
> paths is lower, than the number of coefficients in
> the regression?
> Unfourtunately my code is not a generic one, and is
> not useful for
> pricing different option from the one I was pricing,
> so sharing that
> code would not bring any benefit to anybody, though
> if that is
> possible, I would appreciate a lot if you could send
> me your
> implementation if that is a flexible code.
> 
> Best regards,
> Wojciech
> 
> 2008/1/16, Moshe Olshansky <m_olshansky at yahoo.com>:
> > Hi Michel,
> >
> > I wrote an R code implementing Longstaff-Schwartz
> > algorithm for pricing American put Basket option
> (on
> > the portfolio value). This code can be easily
> changed
> > to value call option (I intend to allow for "any"
> > payoff function in the future).
> > I can send you this code. It is in a very
> preliminary
> > state, so even though I did some basic testing I
> can
> > not guarantee it to be correct.
> >
> > Regards,
> >
> > Moshe.
> >
> > --- MAB <MichelBeck at sbcglobal.Net> wrote:
> >
> > > Hi!
> > >
> > > I am looking for R code to price a multiple
> asset
> > > basket option.
> > > So far I only found a 2 asset pricer (in the
> > > R-metrics packafe fExoticOption).
> > > Where could I find some code for at least 4
> assets?
> > >
> > > Thank you,
> > >
> > > Michel Beck
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From stigler3 at etu.unige.ch  Thu Jan 24 11:32:28 2008
From: stigler3 at etu.unige.ch (Matthieu Stigler)
Date: Thu, 24 Jan 2008 11:32:28 +0100
Subject: [R-SIG-Finance] threshold autoregression
In-Reply-To: <008c01c85ddd$998ed750$ccac85f0$@com>
References: <3303a4570801201447u3abd381dr9b415342c4746168@mail.gmail.com>
	<008c01c85ddd$998ed750$ccac85f0$@com>
Message-ID: <4798693C.8020304@etu.unige.ch>

Hello Alex and Sylvain

Alex, do you want to do univariate or multivariate analysis?

For univariate analysis, SETAR models are available in R in package TSA 
(with diagnostics tests, forecasts, simulation... the package is based 
on examples of a book) and in package BAYSTAR for Bayesian estimation.

STAR models are more rare and you can just find some basic features in 
packages tsDyn. You can find also STAR with time varying coefficients in 
package timsac. For others software, I don't know about Ox but have a 
look on Jmulti, free software based on GAUSS code with Terasvitra code 
for STAR. If you are generally interested in TAR models, have also a 
look on the website of Bruce Hansen, who wrote many important articles 
and makes available these papers and the code used, for Matlab (which 
you should able to run from octave, open source software) and GAUSS.

If you are interested in multivariate analysis, there is for now nothing 
in R to my knowledge. I'm working on my part on multivariate series with 
thresholds (TAR and TVECM for threshold cointegration) and I can send to 
you my code if interest.

I' plan to write some functions (which can be applied to univariate 
analysis, like Hansen linearity test for SETAR (1999) or Lundbergh et al 
(2003) test for parameter constancy), so if you or anyone has interest 
in writing some code and extending the actual features tell me!

Regards

Sylvain BARTHELEMY a ?crit :
> Hi A
>
> If you want to play with SETAR and more generally with Markov switching
> models, my view is that you should use another software than R. 
>
> There is a great pack written by Hanz Martin Krozlig. But as soon as I know,
> it only works on Ox and it has not been ported to R. I think that it would
> be a great idea to rewrite the pack for R. I initially planned to do one,
> but it takes time...
>
> There is also a pack written by Franck Arnaud, that was written for R and is
> available here: http://arnaud.ensae.net/Rressources/Rressources.html
> But it seems to be a beta version and less powerful than the Krozlig one.
>
> Regards
>
> ---
> Sylvain Barth?l?my
> Research Director, TAC
> www.tac-financial.com | www.sylbarth.com
>
>
> -----Message d'origine-----
> De : r-sig-finance-bounces at stat.math.ethz.ch
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] De la part de Alexander
> Moreno
> Envoy? : dimanche 20 janvier 2008 23:47
> ? : r-sig-finance at stat.math.ethz.ch
> Objet : [R-SIG-Finance] threshold autoregression
>
> Hi,
>
> Does anyone know of anyone packages to fit tar and setar time series models?
>
> Thanks,
> Alex
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
> No virus found in this incoming message.
>
>
>
> 14:15
>
>
>
>
>
>
> 20:12
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From wojciech.slusarski at gmail.com  Thu Jan 24 13:22:48 2008
From: wojciech.slusarski at gmail.com (Wojciech Slusarski)
Date: Thu, 24 Jan 2008 13:22:48 +0100
Subject: [R-SIG-Finance] Financial Basket Options
In-Reply-To: <353848.90213.qm@web32211.mail.mud.yahoo.com>
References: <5e64e5be0801230218y3655baabi5405aff1a3a066da@mail.gmail.com>
	<353848.90213.qm@web32211.mail.mud.yahoo.com>
Message-ID: <5e64e5be0801240422k64fe6c0ck6659d91dc40bee29@mail.gmail.com>

Hi Mosche,

Well, I was valuing american-asian (hawaian) option with floating
strike (average of last 60 quotes) and even when running 50k paths,
almost every time there was at least on step, where there was too
little degrees of freedom to run the regression. I was using Laguerre
polynomials as specified in original Longstaff-Schwarz paper. It never
happend, to have problems with memory allocation, for regression I
used lm function.

Best regards,
Wojciech

2008/1/24, Moshe Olshansky <m_olshansky at yahoo.com>:
> Hi Wojciech,
>
> To answer you question, I used all the polynomials
> with total degree less than a certain number (poly in
> R) and I regressed only for in the money paths. I
> always used many paths (at least 10,000) and so I did
> not run into the problem of not having enough of them
> (even though this can happen in the future). I also
> save the state of the random numbers generator so that
> I can reproduce  the states of every path at every
> time point and so I do not have to store all the paths
> (storing them exhausts the memory very quickly).
>
> I have a numerical question connected to this:
> If there are N stocks then the number of all the
> polynomial of degree <= 3 is C(N+3,3) where C(n,k) is
> the binomial coefficient, and this is significantly
> more than N. poly() produces that number of columns
> and then least squares is applied. If the
> corresponding matrix is X then the explicit formula
> for least squares could be used ((t(X)*X)^-1...). To
> calculate t(X)*X one can produce two columns at a
> time, find their dot product and store the result. In
> such a case we need memory space for only N+1 columns
> (it may take a long time CPU-wise but in many cases
> this is not an issue).
> One possible problem is that using the explicit
> formula is numerically inferior to the QR algorithm
> which is usually used in least squares. Can anybody
> say anything about how bad this can be? Let me just
> note that we do not need very high precision here.
>
> Regards,
>
> Moshe.
>
> P.S. I started a new job last week and so I am pretty
> busy these days. I will try to post my code (without
> the use of the explicit formula) over the weekend.
> --- Wojciech Slusarski <wojciech.slusarski at gmail.com>
> wrote:
>
> > Hi Mosche,
> >
> > Once I have implemented L-S algorithm for pricing
> > american-asian
> > options. I am wondering, how stable it is for basket
> > options. What
> > sort of polynomials did you use? Are you regressing
> > only paths that
> > are in-the-money, or all? If yes, what do you do,
> > when the number of
> > paths is lower, than the number of coefficients in
> > the regression?
> > Unfourtunately my code is not a generic one, and is
> > not useful for
> > pricing different option from the one I was pricing,
> > so sharing that
> > code would not bring any benefit to anybody, though
> > if that is
> > possible, I would appreciate a lot if you could send
> > me your
> > implementation if that is a flexible code.
> >
> > Best regards,
> > Wojciech
> >
> > 2008/1/16, Moshe Olshansky <m_olshansky at yahoo.com>:
> > > Hi Michel,
> > >
> > > I wrote an R code implementing Longstaff-Schwartz
> > > algorithm for pricing American put Basket option
> > (on
> > > the portfolio value). This code can be easily
> > changed
> > > to value call option (I intend to allow for "any"
> > > payoff function in the future).
> > > I can send you this code. It is in a very
> > preliminary
> > > state, so even though I did some basic testing I
> > can
> > > not guarantee it to be correct.
> > >
> > > Regards,
> > >
> > > Moshe.
> > >
> > > --- MAB <MichelBeck at sbcglobal.Net> wrote:
> > >
> > > > Hi!
> > > >
> > > > I am looking for R code to price a multiple
> > asset
> > > > basket option.
> > > > So far I only found a 2 asset pricer (in the
> > > > R-metrics packafe fExoticOption).
> > > > Where could I find some code for at least 4
> > assets?
> > > >
> > > > Thank you,
> > > >
> > > > Michel Beck
> > > >
> > > > _______________________________________________
> > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > >
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > -- Subscriber-posting only.
> > > > -- If you want to post, subscribe first.
> > > >
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > >
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> >
>
>


From edd at debian.org  Thu Jan 24 14:11:11 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 24 Jan 2008 07:11:11 -0600
Subject: [R-SIG-Finance] Financial Basket Options
In-Reply-To: <5e64e5be0801240422k64fe6c0ck6659d91dc40bee29@mail.gmail.com>
References: <5e64e5be0801230218y3655baabi5405aff1a3a066da@mail.gmail.com>
	<353848.90213.qm@web32211.mail.mud.yahoo.com>
	<5e64e5be0801240422k64fe6c0ck6659d91dc40bee29@mail.gmail.com>
Message-ID: <18328.36463.587364.575227@ron.nulle.part>


Sorry, I meant to chime in earlier on this. Did any of you look at QuantLib?
AFAICT it has a basketoption class allowing for multiple assets, Monte Carlo
pricers for american and european exercise as well as Stulz (1992) method.  

I often look at the available code via what's in the regression tests, so
here is the header test-suite/basketoption.hpp:

class BasketOptionTest {
  public:
    static void testEuroTwoValues();
    static void testBarraquandThreeValues();
    static void testTavellaValues();
    static void testOneDAmericanValues();
    static void testOddSamples();
    static boost::unit_test_framework::test_suite* suite();
};

My RQuantLib package currently does not wrap basket options, but has a few
other exotics one could take as a stanza.  Building the package is possible
on both Windows and Linux, but somewhat more tedious on the former (as you
need to build QL and Boost first).  My main contributor Dominick has a
tarball with that prebuilt if it is of interest.  If there is interest, we
could add this to RQuantLib.  

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From brian at braverock.com  Thu Jan 24 14:39:38 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 24 Jan 2008 07:39:38 -0600
Subject: [R-SIG-Finance] Quantlib in R (was RE:Financial Basket Options)
In-Reply-To: <18328.36463.587364.575227@ron.nulle.part>
References: <5e64e5be0801230218y3655baabi5405aff1a3a066da@mail.gmail.com>	<353848.90213.qm@web32211.mail.mud.yahoo.com>	<5e64e5be0801240422k64fe6c0ck6659d91dc40bee29@mail.gmail.com>
	<18328.36463.587364.575227@ron.nulle.part>
Message-ID: <4798951A.8030801@braverock.com>

Dirk Eddelbuettel wrote:
> Sorry, I meant to chime in earlier on this. Did any of you look at QuantLib?
> AFAICT it has a basketoption class allowing for multiple assets, Monte Carlo
> pricers for american and european exercise as well as Stulz (1992) method.  
> 
> I often look at the available code via what's in the regression tests, so
> here is the header test-suite/basketoption.hpp:
> 
> class BasketOptionTest {
>   public:
>     static void testEuroTwoValues();
>     static void testBarraquandThreeValues();
>     static void testTavellaValues();
>     static void testOneDAmericanValues();
>     static void testOddSamples();
>     static boost::unit_test_framework::test_suite* suite();
> };
> 
> My RQuantLib package currently does not wrap basket options, but has a few
> other exotics one could take as a stanza.  Building the package is possible
> on both Windows and Linux, but somewhat more tedious on the former (as you
> need to build QL and Boost first).  My main contributor Dominick has a
> tarball with that prebuilt if it is of interest.  If there is interest, we
> could add this to RQuantLib.  

Well, I don't really use Windows as an operating system for running R, 
but it seems that adding the required libraries to the Windows version 
of RQuantlib would increase the likelihood of extensions.

On a related note:

I'm really curious whether there's any interest in extending and 
formalizing the SWIG work that was done with Quantlib and R.  It seems 
that the Quantlib team has some interest, as they have added some 
documentation to their site, which is new since the last time I looked:

http://wiki.quantlib.org/twiki/bin/view/Quantlib/RSwigDocumentation

and the SWIG team's "quite incomplete" documentation, which overlaps to 
a degree with the docs on the Quantlib page:

http://www.swig.org/Doc1.3/R.html

Joseph Chen-Yu Wang of the QuantLib team's paper on this may be found here:
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=965317

It seems that it would be good to spend a little time trying to package 
at least some of the SWIG work (possibly into RQuantlib?) so that there 
is a template for adding other functions to R as they are added to Quantlib.

Quantlib is such an extensive and useful collection, it seems that we 
could all benefit from a little more work in that direction.

Regards,

    - Brian


From MichelBeck at sbcglobal.Net  Thu Jan 24 15:47:51 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Thu, 24 Jan 2008 14:47:51 +0000 (UTC)
Subject: [R-SIG-Finance] Financial Basket Options
References: <loom.20080122T151337-838@post.gmane.org>
	<992166.74600.qm@web32203.mail.mud.yahoo.com>
Message-ID: <loom.20080124T144320-451@post.gmane.org>

Moshe Olshansky <m_olshansky <at> yahoo.com> writes:

> 
> Hi Michel,
> 
> European option (either put or call) is a much easier
> task than the American one since the optimal strategy
> is known (and trivial).
> Are any dividends expected before maturity? Are they
> fixed amount (usually the case when the maturity date

Hi Moshe,

I look forward to receiving the code.

The option is on an energy commodity basket so dividends are not an issue. We 
may need eventually to transact and option on the average of the basket (price 
average over life of the basket) or a basket of the averages of each commodity.

Regard,

Michel


From MichelBeck at sbcglobal.Net  Thu Jan 24 15:53:40 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Thu, 24 Jan 2008 14:53:40 +0000 (UTC)
Subject: [R-SIG-Finance] Financial Basket Options
References: <5e64e5be0801230218y3655baabi5405aff1a3a066da@mail.gmail.com>
	<353848.90213.qm@web32211.mail.mud.yahoo.com>
	<5e64e5be0801240422k64fe6c0ck6659d91dc40bee29@mail.gmail.com>
	<18328.36463.587364.575227@ron.nulle.part>
Message-ID: <loom.20080124T145107-310@post.gmane.org>

Dirk Eddelbuettel <edd <at> debian.org> writes:

> 
> 
> Sorry, I meant to chime in earlier on this. Did any of you look at QuantLib?
> AFAICT it has a basketoption class allowing for multiple assets, Monte Carlo
> pricers for american and european exercise as well as Stulz (1992) method.  
> 
> I often look at the available code via what's in the regression tests, so


Hi Dirk!

My appologies if this is a question with an obvious answer:

I'm used to getting R code libraries by looking 
on http://www.cran.r-project.org/
link Packages

Where do I find QuantLib?

Michel


From edd at debian.org  Thu Jan 24 16:00:23 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 24 Jan 2008 09:00:23 -0600
Subject: [R-SIG-Finance] Quantlib in R (was RE:Financial Basket Options)
In-Reply-To: <4798951A.8030801@braverock.com>
References: <5e64e5be0801230218y3655baabi5405aff1a3a066da@mail.gmail.com>
	<353848.90213.qm@web32211.mail.mud.yahoo.com>
	<5e64e5be0801240422k64fe6c0ck6659d91dc40bee29@mail.gmail.com>
	<18328.36463.587364.575227@ron.nulle.part>
	<4798951A.8030801@braverock.com>
Message-ID: <18328.43015.295595.775989@ron.nulle.part>


Hi Brian,

On 24 January 2008 at 07:39, Brian G. Peterson wrote:
| Dirk Eddelbuettel wrote:
| > Sorry, I meant to chime in earlier on this. Did any of you look at QuantLib?
| > AFAICT it has a basketoption class allowing for multiple assets, Monte Carlo
| > pricers for american and european exercise as well as Stulz (1992) method.  
| > 
| > I often look at the available code via what's in the regression tests, so
| > here is the header test-suite/basketoption.hpp:
| > 
| > class BasketOptionTest {
| >   public:
| >     static void testEuroTwoValues();
| >     static void testBarraquandThreeValues();
| >     static void testTavellaValues();
| >     static void testOneDAmericanValues();
| >     static void testOddSamples();
| >     static boost::unit_test_framework::test_suite* suite();
| > };
| > 
| > My RQuantLib package currently does not wrap basket options, but has a few
| > other exotics one could take as a stanza.  Building the package is possible
| > on both Windows and Linux, but somewhat more tedious on the former (as you
| > need to build QL and Boost first).  My main contributor Dominick has a
| > tarball with that prebuilt if it is of interest.  If there is interest, we
| > could add this to RQuantLib.  
| 
| Well, I don't really use Windows as an operating system for running R, 
| but it seems that adding the required libraries to the Windows version 
| of RQuantlib would increase the likelihood of extensions.

I think we are talking about the same issue here, but maybe we are also
talking past each other, so just do be clear:

- Windows users R can just 'click on' RQuantLib and they get a binary ready
  to run; this includes a static library of QuantLib itself.  [ And Debian
  users  get RQuantLib via apt-get as usual, same for Ubuntu etc pp so those
  derivate distros may be a version behind

- Because R on Windows uses MinGW, one needs a MinGW build of QuantLib to
  build R against.  Such builds have in the past been provided by Dominick.
  However, these builds are typically not publically announced but instead
  have been given to Uwe to build CRAN's Windows version of RQuantLib.
  This allows _use_ of RQuantLib but not really development.
 
- It would be great of someone volunteered to provided this library on
  go-forward basis as Dominick indicated that he will no longer do this.
  
| On a related note:
| 
| I'm really curious whether there's any interest in extending and 
| formalizing the SWIG work that was done with Quantlib and R.  It seems 
| that the Quantlib team has some interest, as they have added some 
| documentation to their site, which is new since the last time I looked:
| 
| http://wiki.quantlib.org/twiki/bin/view/Quantlib/RSwigDocumentation
| 
| and the SWIG team's "quite incomplete" documentation, which overlaps to 
| a degree with the docs on the Quantlib page:
| 
| http://www.swig.org/Doc1.3/R.html
| 
| Joseph Chen-Yu Wang of the QuantLib team's paper on this may be found here:
| http://papers.ssrn.com/sol3/papers.cfm?abstract_id=965317
| 
| It seems that it would be good to spend a little time trying to package 
| at least some of the SWIG work (possibly into RQuantlib?) so that there 
| is a template for adding other functions to R as they are added to Quantlib.

I am obviously fully in favour, but I do not have the bandwidth to drive
this along with my current list of things to juggle.

The last time I tried to build this, shortly after Joe Wang finished a major
piece of his work, I found that

- his documentation was inadequate (at least for my Python and Swig skills)
  and wrong (and then offered to proof-read and sadly never got back to him)
  though I ended up getting it build with some help by Joe

- created one monstrously large dynamic library file you need to load;
  basically all of QL becomes one blob which is a tad unwielding if you just
  want one or two pricers

- however, this is really useful piece of code, and QuantLib is about to go
  to the long-awaited 1.0 release so the time may be right.

| Quantlib is such an extensive and useful collection, it seems that we 
| could all benefit from a little more work in that direction.

So despite of what I said above, maybe you and I need to clear a weekend and
make it a Chicago hackathon?  

Anybody else?  Chicago can be reached from just about anywhere, and I sure we
can find a place to stay for any and all volunteers :)  Barring that, we
could always try virtual meetings but real hackathons may be more effective.

Dirk

-- 
Three out of two people have difficulties with fractions.


From brian at braverock.com  Thu Jan 24 16:10:23 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 24 Jan 2008 09:10:23 -0600
Subject: [R-SIG-Finance] Financial Basket Options
In-Reply-To: <loom.20080124T145107-310@post.gmane.org>
References: <5e64e5be0801230218y3655baabi5405aff1a3a066da@mail.gmail.com>	<353848.90213.qm@web32211.mail.mud.yahoo.com>	<5e64e5be0801240422k64fe6c0ck6659d91dc40bee29@mail.gmail.com>	<18328.36463.587364.575227@ron.nulle.part>
	<loom.20080124T145107-310@post.gmane.org>
Message-ID: <4798AA5F.7060604@braverock.com>

MAB wrote:
> My appologies if this is a question with an obvious answer:
> 
> I'm used to getting R code libraries by looking 
> on http://www.cran.r-project.org/
> link Packages
> 
> Where do I find QuantLib?

Same place:

http://cran.r-project.org/src/contrib/Descriptions/RQuantLib.html

Should be available from install.packages() from inside R as well.

   - Brian


From jeff.a.ryan at gmail.com  Thu Jan 24 16:24:25 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 24 Jan 2008 09:24:25 -0600
Subject: [R-SIG-Finance] Quantlib in R (was RE:Financial Basket Options)
In-Reply-To: <18328.43015.295595.775989@ron.nulle.part>
References: <5e64e5be0801230218y3655baabi5405aff1a3a066da@mail.gmail.com>
	<353848.90213.qm@web32211.mail.mud.yahoo.com>
	<5e64e5be0801240422k64fe6c0ck6659d91dc40bee29@mail.gmail.com>
	<18328.36463.587364.575227@ron.nulle.part>
	<4798951A.8030801@braverock.com>
	<18328.43015.295595.775989@ron.nulle.part>
Message-ID: <e8e755250801240724w58789019p67998719498ae83e@mail.gmail.com>

Hi Dirk, Brian, et al,

I'd be game to get this together - though were _my_ skills fit in
(Python maybe???) would have to be sorted out :)

As an aside, I did manage to get RQuantLib on my Mac running via macports.

Tell me when and where.

Jeff

On Jan 24, 2008 9:00 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> Hi Brian,
>
> On 24 January 2008 at 07:39, Brian G. Peterson wrote:
> | Dirk Eddelbuettel wrote:
> | > Sorry, I meant to chime in earlier on this. Did any of you look at QuantLib?
> | > AFAICT it has a basketoption class allowing for multiple assets, Monte Carlo
> | > pricers for american and european exercise as well as Stulz (1992) method.
> | >
> | > I often look at the available code via what's in the regression tests, so
> | > here is the header test-suite/basketoption.hpp:
> | >
> | > class BasketOptionTest {
> | >   public:
> | >     static void testEuroTwoValues();
> | >     static void testBarraquandThreeValues();
> | >     static void testTavellaValues();
> | >     static void testOneDAmericanValues();
> | >     static void testOddSamples();
> | >     static boost::unit_test_framework::test_suite* suite();
> | > };
> | >
> | > My RQuantLib package currently does not wrap basket options, but has a few
> | > other exotics one could take as a stanza.  Building the package is possible
> | > on both Windows and Linux, but somewhat more tedious on the former (as you
> | > need to build QL and Boost first).  My main contributor Dominick has a
> | > tarball with that prebuilt if it is of interest.  If there is interest, we
> | > could add this to RQuantLib.
> |
> | Well, I don't really use Windows as an operating system for running R,
> | but it seems that adding the required libraries to the Windows version
> | of RQuantlib would increase the likelihood of extensions.
>
> I think we are talking about the same issue here, but maybe we are also
> talking past each other, so just do be clear:
>
> - Windows users R can just 'click on' RQuantLib and they get a binary ready
>   to run; this includes a static library of QuantLib itself.  [ And Debian
>   users  get RQuantLib via apt-get as usual, same for Ubuntu etc pp so those
>   derivate distros may be a version behind
>
> - Because R on Windows uses MinGW, one needs a MinGW build of QuantLib to
>   build R against.  Such builds have in the past been provided by Dominick.
>   However, these builds are typically not publically announced but instead
>   have been given to Uwe to build CRAN's Windows version of RQuantLib.
>   This allows _use_ of RQuantLib but not really development.
>
> - It would be great of someone volunteered to provided this library on
>   go-forward basis as Dominick indicated that he will no longer do this.
>
> | On a related note:
> |
> | I'm really curious whether there's any interest in extending and
> | formalizing the SWIG work that was done with Quantlib and R.  It seems
> | that the Quantlib team has some interest, as they have added some
> | documentation to their site, which is new since the last time I looked:
> |
> | http://wiki.quantlib.org/twiki/bin/view/Quantlib/RSwigDocumentation
> |
> | and the SWIG team's "quite incomplete" documentation, which overlaps to
> | a degree with the docs on the Quantlib page:
> |
> | http://www.swig.org/Doc1.3/R.html
> |
> | Joseph Chen-Yu Wang of the QuantLib team's paper on this may be found here:
> | http://papers.ssrn.com/sol3/papers.cfm?abstract_id=965317
> |
> | It seems that it would be good to spend a little time trying to package
> | at least some of the SWIG work (possibly into RQuantlib?) so that there
> | is a template for adding other functions to R as they are added to Quantlib.
>
> I am obviously fully in favour, but I do not have the bandwidth to drive
> this along with my current list of things to juggle.
>
> The last time I tried to build this, shortly after Joe Wang finished a major
> piece of his work, I found that
>
> - his documentation was inadequate (at least for my Python and Swig skills)
>   and wrong (and then offered to proof-read and sadly never got back to him)
>   though I ended up getting it build with some help by Joe
>
> - created one monstrously large dynamic library file you need to load;
>   basically all of QL becomes one blob which is a tad unwielding if you just
>   want one or two pricers
>
> - however, this is really useful piece of code, and QuantLib is about to go
>   to the long-awaited 1.0 release so the time may be right.
>
> | Quantlib is such an extensive and useful collection, it seems that we
> | could all benefit from a little more work in that direction.
>
> So despite of what I said above, maybe you and I need to clear a weekend and
> make it a Chicago hackathon?
>
> Anybody else?  Chicago can be reached from just about anywhere, and I sure we
> can find a place to stay for any and all volunteers :)  Barring that, we
> could always try virtual meetings but real hackathons may be more effective.
>
> Dirk
>
> --
> Three out of two people have difficulties with fractions.
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From edd at debian.org  Thu Jan 24 16:26:01 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 24 Jan 2008 09:26:01 -0600
Subject: [R-SIG-Finance] Financial Basket Options
In-Reply-To: <loom.20080124T145107-310@post.gmane.org>
References: <5e64e5be0801230218y3655baabi5405aff1a3a066da@mail.gmail.com>
	<353848.90213.qm@web32211.mail.mud.yahoo.com>
	<5e64e5be0801240422k64fe6c0ck6659d91dc40bee29@mail.gmail.com>
	<18328.36463.587364.575227@ron.nulle.part>
	<loom.20080124T145107-310@post.gmane.org>
Message-ID: <18328.44553.70933.909658@ron.nulle.part>


On 24 January 2008 at 14:53, MAB wrote:
| Dirk Eddelbuettel <edd <at> debian.org> writes:
| 
| > 
| > 
| > Sorry, I meant to chime in earlier on this. Did any of you look at QuantLib?
| > AFAICT it has a basketoption class allowing for multiple assets, Monte Carlo
| > pricers for american and european exercise as well as Stulz (1992) method.  
| > 
| > I often look at the available code via what's in the regression tests, so
| 
| 
| Hi Dirk!
| 
| My appologies if this is a question with an obvious answer:

It kinda is once you get used to asking Google too ;-)
 
| I'm used to getting R code libraries by looking 
| on http://www.cran.r-project.org/
| link Packages

[ As an aside, given the frentic growth of CRAN, you may enjoy looking at
  http://planetr.stderr.org which covers the CRANberries RSS stream of CRAN
  updates I set up a few month ago. Point your RSS reader to that, you get
  informed whenever CRAN has new packages, and/or see the diffstat summary of
  changes to existing packages. ]
 
| Where do I find QuantLib?

At http://www.quantlib.org -- which is a redirect for sourceforge.

There is quite a community of users and developers but the intersection with
R is fairly small -- so welcome :)

Dirk

-- 
Three out of two people have difficulties with fractions.


From edd at debian.org  Thu Jan 24 16:45:22 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 24 Jan 2008 09:45:22 -0600
Subject: [R-SIG-Finance] Quantlib in R (was RE:Financial Basket Options)
In-Reply-To: <e8e755250801240724w58789019p67998719498ae83e@mail.gmail.com>
References: <5e64e5be0801230218y3655baabi5405aff1a3a066da@mail.gmail.com>
	<353848.90213.qm@web32211.mail.mud.yahoo.com>
	<5e64e5be0801240422k64fe6c0ck6659d91dc40bee29@mail.gmail.com>
	<18328.36463.587364.575227@ron.nulle.part>
	<4798951A.8030801@braverock.com>
	<18328.43015.295595.775989@ron.nulle.part>
	<e8e755250801240724w58789019p67998719498ae83e@mail.gmail.com>
Message-ID: <18328.45714.790278.847038@ron.nulle.part>


On 24 January 2008 at 09:24, Jeff Ryan wrote:
| Hi Dirk, Brian, et al,
| 
| I'd be game to get this together - though were _my_ skills fit in
| (Python maybe???) would have to be sorted out :)

Sounds good. As for up to three, it's a officially a party. And I see that my
previous hunch of this abnormally high concentration of R/Finance hackers in
Chicagoland just had to pay off one day ;-)

| As an aside, I did manage to get RQuantLib on my Mac running via macports.

Wicked -- that's the first I hear about. Could you send me some asci
documentation for inclusion in the package's README, website, my blog, ... so
this doesn't get lost.  Was it tricky?  Is it 'scaleable', ie is this
something CRAN could do too?  And who does the Mac binaries there anyway?
 
| Tell me when and where.

To be sorted out...

Dirk

-- 
Three out of two people have difficulties with fractions.


From m_olshansky at yahoo.com  Sat Jan 26 11:32:28 2008
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Sat, 26 Jan 2008 02:32:28 -0800 (PST)
Subject: [R-SIG-Finance] Financial Basket Options
In-Reply-To: <18328.36463.587364.575227@ron.nulle.part>
Message-ID: <488742.295.qm@web32201.mail.mud.yahoo.com>

Hi Dirk,

I considered using QuantLib, but as you mention it's R
wrapper does not include Basket option.
So the choices I had were either to invoke it using
C++ or to code my own implementation in R. Since
QuantLib contains many classes and I have never used
it before I decided that coding in R will take me less
time and will allow me to add more stuff in the future
(for example getting the upper bound).
My new job is not exactly in Finance (even though it
is also business-oriented) and so I have less strive
to complete this task now, but on the other hand this
is an interesting problem (mathematically) and so I
may continue. 
To be honest, I am not sure about it's importance in
finance, since the theoretical price is not always the
correct one...

Regards,

Moshe.

--- Dirk Eddelbuettel <edd at debian.org> wrote:

> 
> Sorry, I meant to chime in earlier on this. Did any
> of you look at QuantLib?
> AFAICT it has a basketoption class allowing for
> multiple assets, Monte Carlo
> pricers for american and european exercise as well
> as Stulz (1992) method.  
> 
> I often look at the available code via what's in the
> regression tests, so
> here is the header test-suite/basketoption.hpp:
> 
> class BasketOptionTest {
>   public:
>     static void testEuroTwoValues();
>     static void testBarraquandThreeValues();
>     static void testTavellaValues();
>     static void testOneDAmericanValues();
>     static void testOddSamples();
>     static boost::unit_test_framework::test_suite*
> suite();
> };
> 
> My RQuantLib package currently does not wrap basket
> options, but has a few
> other exotics one could take as a stanza.  Building
> the package is possible
> on both Windows and Linux, but somewhat more tedious
> on the former (as you
> need to build QL and Boost first).  My main
> contributor Dominick has a
> tarball with that prebuilt if it is of interest.  If
> there is interest, we
> could add this to RQuantLib.  
> 
> Hth, Dirk
> 
> -- 
> Three out of two people have difficulties with
> fractions.
>


From ravis at ambaresearch.com  Mon Jan 28 09:52:46 2008
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Mon, 28 Jan 2008 14:22:46 +0530
Subject: [R-SIG-Finance] Compute Portfolio Returns
Message-ID: <A36876D3F8A5734FA84A4338135E7CC30305587D@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080128/99da9553/attachment.pl 

From brian at braverock.com  Mon Jan 28 11:33:49 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 28 Jan 2008 04:33:49 -0600
Subject: [R-SIG-Finance] Compute Portfolio Returns
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC30305587D@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC30305587D@BAN-MAILSRV03.Amba.com>
Message-ID: <479DAF8D.8090709@braverock.com>

Ravi S. Shankar wrote:
> I have in my portfolio cash, equity, futures and forwards. I need to
> compute the returns of this portfolio. 
> 
> If say I have cash of $100 and I buy only equity then my weights would
> be the amount invested in each stock upon the total cash and the sum of
> weights would add up to the total cash. The returns computation would be
> a straightforward change in investment.

price * position size (in shares) = position value

You can, as you've noted, back into weights this way.

> However, I need help in understanding how do I compute the returns (and
> the associated weights) from a futures position and a forwards position.
> 
> a)       In the case of futures there would be a daily change in the
> cash position due to Mark to market. So would the change in cash
> position reflect the returns of futures position?

With the futures position you will always calculate the notional value 
to mark to market. For most futures contracts this is the price * a 
multiplier.

Change in notional value is your return (in currency units like US$ or EU).

> b)       In case of forwards how do I compute the weights and the
> corresponding returns? I cannot use the change in investment as there is
> no initial investment and if I use the change in investment approach to
> compute returns I would have infinite returns. 

With a forward contract you would use the difference in price between 
the forward contract face price and the current spot (market) price to 
calculate the current "premium price" on the contract and use the change 
in premium price to calculate your returns.

> Any help would be appreciated (please let me know in case there is a
> paper which I can refer to)

If you do not change your positions from your initial position, then 
your starting weights do not change, but your actual weights will float 
daily (or whatever your observation time is).

If you need the calculations for calculating returns from prices (or 
price*multiplier notionals), look at function Return.calculate in 
PerformanceAnalytics.  Or if you need the method for calculating 
weighted returns from "prices" and starting weights, let me know.

Regards,

    - Brian


From m_olshansky at yahoo.com  Mon Jan 28 12:24:36 2008
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Mon, 28 Jan 2008 03:24:36 -0800 (PST)
Subject: [R-SIG-Finance] Financial Basket Options
In-Reply-To: <992166.74600.qm@web32203.mail.mud.yahoo.com>
Message-ID: <619558.16041.qm@web32205.mail.mud.yahoo.com>

Hi everyone,

As promised I am enclosing my implementation of
Longstaff-Schwartz algorithm for valuation of
(American) Basket option by Monte Carlo.

Let me note that this is a very preliminary attempt
and I am posting this just in case it may be useful to
some people. It is in no way even close to production
quality code.
I do not check for error conditions; polynomial
regression is implemented using all degree 3
polynomials in all the variables etc.

Feel free to ask me any question about the code (but
do not expect me to defend it...).

The code consists of 4 files (3 of which could have
been united into one file): AmericanBasket.R (code for
American Basket option), EuropeanBasket.R (code for
European Basket option), SomePayoffs.R (an example of
two simplest payoffs) and Read.Me (some explanations).

Let me note that current implementation is not
suitable for path-dependent options.

Best regards,

Moshe.

--- Moshe Olshansky <m_olshansky at yahoo.com> wrote:

> Hi Michel,
> 
> European option (either put or call) is a much
> easier
> task than the American one since the optimal
> strategy
> is known (and trivial).
> Are any dividends expected before maturity? Are they
> fixed amount (usually the case when the maturity
> date
> is not too far away) or a known dividend yield?
> If you wish I can send you this code as well.
> 
> I will try to slightly clean up my code during the
> weekend and then I will send it to anyone
> interested.
> 
> Regards,
> 
> Moshe.
> 
> --- MAB <MichelBeck at sbcglobal.Net> wrote:
> 
> > Moshe Olshansky <m_olshansky <at> yahoo.com>
> writes:
> > 
> > > 
> > > Hi Michel,
> > > 
> > > I wrote an R code implementing
> Longstaff-Schwartz
> > > algorithm for pricing American put Basket option
> > (on
> > > the portfolio value). This code can be easily
> > changed
> > > to value call option (I intend to allow for
> "any"
> > > payoff function in the future).
> > > I can send you this code. It is in a very
> > preliminary
> > > state, so even though I did some basic testing I
> > can
> > > not guarantee it to be correct.
> > > 
> > > Regards,
> > > 
> > > Moshe.
> > > 
> > > --- MAB <MichelBeck <at> sbcglobal.Net> wrote:
> > >
> > 
> > > 
> > > 
> > Hi Moshe,
> > 
> > Thanks for your reply.
> > 
> > Yes, I would appreciate getting the code, although
> > I'm not sure I am able to 
> > convert it to valuing call options (European call
> is
> > what I'm looking at).
> > 
> > Regards,
> > 
> > Michel
> > 
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> >
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only. 
> > -- If you want to post, subscribe first.
> >
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: AmericanBasket.R
Type: application/octet-stream
Size: 1704 bytes
Desc: 735043657-AmericanBasket.R
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080128/b719483a/attachment.obj 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: EuropeanBasket.R
Type: application/octet-stream
Size: 1333 bytes
Desc: 1099661355-EuropeanBasket.R
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080128/b719483a/attachment-0001.obj 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: SomePayoffs.R
Type: application/octet-stream
Size: 121 bytes
Desc: 583653873-SomePayoffs.R
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080128/b719483a/attachment-0002.obj 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Read.Me
Type: application/octet-stream
Size: 1524 bytes
Desc: 3319012510-Read.Me
Url : https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080128/b719483a/attachment-0003.obj 

From ravis at ambaresearch.com  Tue Jan 29 13:34:19 2008
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Tue, 29 Jan 2008 18:04:19 +0530
Subject: [R-SIG-Finance] Compute Portfolio Returns
References: <A36876D3F8A5734FA84A4338135E7CC30305587D@BAN-MAILSRV03.Amba.com>
	<479DAF8D.8090709@braverock.com>
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3030A3254@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080129/c9afadf1/attachment.pl 

From brian at braverock.com  Tue Jan 29 14:15:59 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 29 Jan 2008 07:15:59 -0600
Subject: [R-SIG-Finance] Compute Portfolio Returns
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3030A3254@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC30305587D@BAN-MAILSRV03.Amba.com>
	<479DAF8D.8090709@braverock.com>
	<A36876D3F8A5734FA84A4338135E7CC3030A3254@BAN-MAILSRV03.Amba.com>
Message-ID: <479F270F.8010403@braverock.com>

Ravi S. Shankar wrote:
> But if in my portfolio I have cash and because of daily Mark to market 
> my cash position would reflect my earnings from my futures position. 
> Isn?t it sufficient to simply the compute the returns from Cash which 
> includes the changes from futures position?

I'm afraid when you used the word "returns" I made the assumption that 
you were speaking of percentage returns, and wanted the mechanics of 
computing these even on a hypothetical portfolio, not specifically on 
tracking change in value on a real portfolio.  So I'll try to operate in 
the context of a cash portfolio in this email.

Generally futures contracts,like any other derivatives contract will not 
"pay out" till expiration or sale of the contract. So I'm a bit confused 
when you speak of a change in your cash position as a result of the 
futures contract.  You may see a change in the amount of money you are 
able to invest in new purchases because of your margin agreement, but 
the actual cash value in your account should not change, although the 
market value of securities held obviously does change.  So I'll try to 
put this in  terms that I understand, and hopefully we can meet in the 
middle somewhere.

It is certainly possible to track the returns on a "live" portfolio by 
looking at the change in Mark to Market from day to day on all your 
positions, as long as there is no change in investment (cash deposited 
or withdrawn from the trading account).  This is the simplest way of 
tracking returns, to work from a net asset value (NAV) number on the 
portfolio.

If when you speak of your cash position, you are speaking of the Market 
Value of Securities held or the NAV of your account, then I think we are 
speaking of the same thing.

In very compact form:

NAV=Cash deposits+Market Value of Positions-Margin Balance

(adjusted for any inflows or outflows of capital invested from the 
trading account)

> Even for the forward position my cash would reflect it. If I have a 
> entered into a forward contract my cash would go up by the position 
> times the premium amount(?)

Your NAV will reflect it, yes.  While margin or securities may be put up 
as collateral for a forward contract, generally "cash" does not change 
hands until the expiration or sale of the contract.

Contrast this with a swap, where in many cases there are cash payments 
that happen throughout the life of the contract, depending of the terms 
of the contract...

> So I cant individually compute the returns from a future position or a 
> forward position without somehow including my cash position?

I spoke in my earlier email about computing the change in mark to market 
on the derivatives contracts into a returns calculation (as on a 
hypothetical portfolio).    This is, of course, somewhat more work than 
simply tracking the NAV of your account.

Hopefully we're all speaking about the same things now.  If there is 
still some confusion, hopefully we can clear it up in another round or two.

Regards,

   - Brian

> -----Original Message-----
> From: Brian G. Peterson [mailto:brian at braverock.com]
> Sent: Monday, January 28, 2008 4:04 PM
> To: Ravi S. Shankar
> Cc: r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] Compute Portfolio Returns
> 
>  
> 
> Ravi S. Shankar wrote:
> 
>>  I have in my portfolio cash, equity, futures and forwards. I need to
> 
>>  compute the returns of this portfolio.
> 
>>
> 
>>  If say I have cash of $100 and I buy only equity then my weights would
> 
>>  be the amount invested in each stock upon the total cash and the sum of
> 
>>  weights would add up to the total cash. The returns computation would be
> 
>>  a straightforward change in investment.
> 
>  
> 
> price * position size (in shares) = position value
> 
>  
> 
> You can, as you've noted, back into weights this way.
> 
>  
> 
>>  However, I need help in understanding how do I compute the returns (and
> 
>>  the associated weights) from a futures position and a forwards position.
> 
>>
> 
>>  a)       In the case of futures there would be a daily change in the
> 
>>  cash position due to Mark to market. So would the change in cash
> 
>>  position reflect the returns of futures position?
> 
>  
> 
> With the futures position you will always calculate the notional value
> 
> to mark to market. For most futures contracts this is the price * a
> 
> multiplier.
> 
>  
> 
> Change in notional value is your return (in currency units like US$ or EU).
> 
>  
> 
>>  b)       In case of forwards how do I compute the weights and the
> 
>>  corresponding returns? I cannot use the change in investment as there is
> 
>>  no initial investment and if I use the change in investment approach to
> 
>>  compute returns I would have infinite returns.
> 
>  
> 
> With a forward contract you would use the difference in price between
> 
> the forward contract face price and the current spot (market) price to
> 
> calculate the current "premium price" on the contract and use the change
> 
> in premium price to calculate your returns.
> 
>  
> 
>>  Any help would be appreciated (please let me know in case there is a
> 
>>  paper which I can refer to)
> 
>  
> 
> If you do not change your positions from your initial position, then
> 
> your starting weights do not change, but your actual weights will float
> 
> daily (or whatever your observation time is).
> 
>  
> 
> If you need the calculations for calculating returns from prices (or
> 
> price*multiplier notionals), look at function Return.calculate in
> 
> PerformanceAnalytics.  Or if you need the method for calculating
> 
> weighted returns from "prices" and starting weights, let me know.
> 
>  
> 
> Regards,
> 
>  
> 
>     - Brian
> 
> This e-mail may contain confidential and/or privileged...{{dropped:6}}


From babel at centrum.sk  Tue Jan 29 14:27:52 2008
From: babel at centrum.sk (babel at centrum.sk)
Date: Tue, 29 Jan 2008 14:27:52 +0100
Subject: [R-SIG-Finance] fGarch
Message-ID: <200801291427.21847@centrum.cz>

Hello.
I have this problem. Why do I have all fitted values the same??
>  y
   43.097 43.041 43.019 42.769 42.533 42.542 42.466 42.817 42.734 42.770
   42.637 42.710 42.669 42.782 42.993 42.994 42.944 42.902 42.714 42.746
   42.881 42.760 42.489 42.422 42.460 42.641 42.675 42.678 42.827 42.981
   42.930 42.996 42.899 43.037 43.478 43.882 43.886 43.955 43.932 43.998

> library(fGarch)

> fit = garchFit(~garch(1, 1), data = y)

> show.fGARCH(fit)


Title:
 GARCH Modelling 

Call:
 garchFit(formula = ~garch(1, 1), data = y) 

Mean and Variance Equation:
 ~arma(0, 0) + ~garch(1, 1) 

Conditional Distribution:
 dnorm 

Coefficient(s):
         mu        omega       alpha1        beta1  
42.30131209   0.00394317   0.99198425   0.05514354  

Error Analysis:
        Estimate  Std. Error  t value Pr(>|t|)    
mu     42.301312    0.008606 4915.050   <2e-16 ***
omega   0.003943    0.001626    2.426   0.0153 *  
alpha1  0.991984    0.061981   16.005   <2e-16 ***
beta1   0.055144    0.052464    1.051   0.2932    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Log Likelihood:
 3405.273    normalized:  1.706904 



> fit at fitted
 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131
 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131

> predict(fit, n.ahead = 10)
   meanForecast meanError standardDeviation
1      42.30131  2.581365          8.109771
2      42.30131  2.581365          8.298906
3      42.30131  2.581365          8.492442
4      42.30131  2.581365          8.690480
5      42.30131  2.581365          8.893126
6      42.30131  2.581365          9.100487
7      42.30131  2.581365          9.312673
8      42.30131  2.581365          9.529796
9      42.30131  2.581365          9.751972
10     42.30131  2.581365          9.979319

I want to count RMSE and choose which Garch model, is better, but I am not able to make a garch model.Thank you


From babel at centrum.sk  Tue Jan 29 14:41:31 2008
From: babel at centrum.sk (babel at centrum.sk)
Date: Tue, 29 Jan 2008 14:41:31 +0100
Subject: [R-SIG-Finance] Fwd:  fGarch
In-Reply-To: <200801291427.21847@centrum.cz>
References: <200801291427.21847@centrum.cz>
Message-ID: <200801291441.26763@centrum.cz>


______________________ P?vodn? spr?va: ________________________
> Od: babel at centrum.sk
> Komu: <r-sig-finance at stat.math.ethz.ch>
> Datum: 29.01.2008 14:28
> P?edm?t: [R-SIG-Finance] fGarch
>
Hello.
I have this problem. Why do I have all fitted values the same??
>  y
   43.097 43.041 43.019 42.769 42.533 42.542 42.466 42.817 42.734 42.770
   42.637 42.710 42.669 42.782 42.993 42.994 42.944 42.902 42.714 42.746
   42.881 42.760 42.489 42.422 42.460 42.641 42.675 42.678 42.827 42.981
   42.930 42.996 42.899 43.037 43.478 43.882 43.886 43.955 43.932 43.998

> library(fGarch)

> fit = garchFit(~garch(1, 1), data = y)

> show.fGARCH(fit)


Title:
 GARCH Modelling 

Call:
 garchFit(formula = ~garch(1, 1), data = y) 

Mean and Variance Equation:
 ~arma(0, 0) + ~garch(1, 1) 

Conditional Distribution:
 dnorm 

Coefficient(s):
         mu        omega       alpha1        beta1  
42.30131209   0.00394317   0.99198425   0.05514354  

Error Analysis:
        Estimate  Std. Error  t value Pr(>|t|)    
mu     42.301312    0.008606 4915.050   <2e-16 ***
omega   0.003943    0.001626    2.426   0.0153 *  
alpha1  0.991984    0.061981   16.005   <2e-16 ***
beta1   0.055144    0.052464    1.051   0.2932    
---
Signif. codes:  0 `***? 0.001 `**? 0.01 `*? 0.05 `.? 0.1 ` ? 1 

Log Likelihood:
 3405.273    normalized:  1.706904 



> fit at fitted
 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131
 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131

> predict(fit, n.ahead = 10)
   meanForecast meanError standardDeviation
1      42.30131  2.581365          8.109771
2      42.30131  2.581365          8.298906
3      42.30131  2.581365          8.492442
4      42.30131  2.581365          8.690480
5      42.30131  2.581365          8.893126
6      42.30131  2.581365          9.100487
7      42.30131  2.581365          9.312673
8      42.30131  2.581365          9.529796
9      42.30131  2.581365          9.751972
10     42.30131  2.581365          9.979319

I want to count RMSE and choose which Garch model, is better, but I am not
able to make a garch model.Thank you

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From spencer.graves at pdf.com  Wed Jan 30 04:58:10 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 29 Jan 2008 19:58:10 -0800
Subject: [R-SIG-Finance] Fwd:  fGarch
In-Reply-To: <200801291441.26763@centrum.cz>
References: <200801291427.21847@centrum.cz> <200801291441.26763@centrum.cz>
Message-ID: <479FF5D2.707@pdf.com>

Your model "~garch(1,1)" specifies a constant + heteroscedastic noise.  
The slot you printed 'fit at fitted' is the estimated (constant) mean of 
the series.  To see the estimated noise standard deviation, try the 
following: 

 > fit at sigma.t
 [1] 0.4942740 0.3945277 0.3415809 0.3210639 0.1287955 0.2227873 0.2152462
 [8] 0.2816325 0.1532941 0.1202254 0.1291714 0.1460944 0.1200281 0.1303487
[15] 0.1341825 0.2970904 0.2980062 0.2529998 0.2168916 0.1197274 0.1221059
[22] 0.1997035 0.1257199 0.2609890 0.3220154 0.2870766 0.1438314 0.1280727
[29] 0.1270258 0.1597462 0.2861451 0.2407515 0.2998393 0.2143946 0.3378366
[36] 0.7663652 1.1671236 1.1711026 1.2397624 1.2168715
 >
      For a similar example, see section 3.5 of Tsay (2005) Analysis of 
Financial Time Series (Wiley) and "scripts\ch03" in the FinTS package. 

      hope this helps.
      Spencer Graves

babel at centrum.sk wrote:
> ______________________ P?vodn? spr?va: ________________________
>   
>> Od: babel at centrum.sk
>> Komu: <r-sig-finance at stat.math.ethz.ch>
>> Datum: 29.01.2008 14:28
>> P?edm?t: [R-SIG-Finance] fGarch
>>
>>     
> Hello.
> I have this problem. Why do I have all fitted values the same??
>   
>>  y
>>     
>    43.097 43.041 43.019 42.769 42.533 42.542 42.466 42.817 42.734 42.770
>    42.637 42.710 42.669 42.782 42.993 42.994 42.944 42.902 42.714 42.746
>    42.881 42.760 42.489 42.422 42.460 42.641 42.675 42.678 42.827 42.981
>    42.930 42.996 42.899 43.037 43.478 43.882 43.886 43.955 43.932 43.998
>
>   
>> library(fGarch)
>>     
>
>   
>> fit = garchFit(~garch(1, 1), data = y)
>>     
>
>   
>> show.fGARCH(fit)
>>     
>
>
> Title:
>  GARCH Modelling 
>
> Call:
>  garchFit(formula = ~garch(1, 1), data = y) 
>
> Mean and Variance Equation:
>  ~arma(0, 0) + ~garch(1, 1) 
>
> Conditional Distribution:
>  dnorm 
>
> Coefficient(s):
>          mu        omega       alpha1        beta1  
> 42.30131209   0.00394317   0.99198425   0.05514354  
>
> Error Analysis:
>         Estimate  Std. Error  t value Pr(>|t|)    
> mu     42.301312    0.008606 4915.050   <2e-16 ***
> omega   0.003943    0.001626    2.426   0.0153 *  
> alpha1  0.991984    0.061981   16.005   <2e-16 ***
> beta1   0.055144    0.052464    1.051   0.2932    
> ---
> Signif. codes:  0 `***? 0.001 `**? 0.01 `*? 0.05 `.? 0.1 ` ? 1 
>
> Log Likelihood:
>  3405.273    normalized:  1.706904 
>
>
>
>   
>> fit at fitted
>>     
>  42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131
>  42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131 42.30131
>
>   
>> predict(fit, n.ahead = 10)
>>     
>    meanForecast meanError standardDeviation
> 1      42.30131  2.581365          8.109771
> 2      42.30131  2.581365          8.298906
> 3      42.30131  2.581365          8.492442
> 4      42.30131  2.581365          8.690480
> 5      42.30131  2.581365          8.893126
> 6      42.30131  2.581365          9.100487
> 7      42.30131  2.581365          9.312673
> 8      42.30131  2.581365          9.529796
> 9      42.30131  2.581365          9.751972
> 10     42.30131  2.581365          9.979319
>
> I want to count RMSE and choose which Garch model, is better, but I am not
> able to make a garch model.Thank you
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.


From anass.mouhsine at sgcib.com  Wed Jan 30 16:00:07 2008
From: anass.mouhsine at sgcib.com (anass.mouhsine at sgcib.com)
Date: Wed, 30 Jan 2008 16:00:07 +0100
Subject: [R-SIG-Finance] timeDate conversion [C1]
Message-ID: <OF33BEE4EA.17F1467C-ONC12573E0.005006CB-C12573E0.0052689B@fr.world.socgen>



Hello,

I am carrying out some research on tick data and I am faced to a date
problem when trying to transform my dataset into timeSeries.
My date time is written this way
"2005-04-08 08:30:35.916"
where I used this format "%Y-%m-%d %H:%M:%OS" to benfit from the
tick-by-tick information.
I have a problem when I convert this character object to timeDate

> dates.tmp=timeDate("2005-04-08 08:30:35.916",format="%Y-%m-%d %H:%M:%OS")
Error in if (sum(lt$sec + lt$min + lt$hour) == 0) isoFormat = "%Y-%m-%d" :
        missing value where TRUE/FALSE needed
In addition: Warning message:
Unknown Format Specification in: .whichFormat(charvec)

when debugging, I've found the problem coming from the function
..whichFormat(charvec) that don't recognize the format of the input.
Browse[1]> .whichFormat(charvec)
[1] "unknown"
Warning message:
Unknown Format Specification in: .whichFormat(charvec)

Have anyone faced this kind of issue?

Thank you

Anass Mouhsine





*************************************************************************
This message and any attachments (the "message") are con...{{dropped:10}}


From chalabi at phys.ethz.ch  Wed Jan 30 16:58:43 2008
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Wed, 30 Jan 2008 16:58:43 +0100
Subject: [R-SIG-Finance] timeDate conversion [C1]
In-Reply-To: <OF33BEE4EA.17F1467C-ONC12573E0.005006CB-C12573E0.0052689B@fr.world.socgen>
References: <OF33BEE4EA.17F1467C-ONC12573E0.005006CB-C12573E0.0052689B@fr.world.socgen>
Message-ID: <20080130165843.3c979619@yankee-laptop>

>>>> "AMC" == anass.mouhsine at sgcib.com
>>>> on Wed, 30 Jan 2008 16:00:07 +0100

   AMC> > dates.tmp=timeDate("2005-04-08 08:30:35.916",format="%Y-%m-%d
   AMC> %H:%M:%OS")
   AMC> Error in if (sum(lt + lt + lt) == 0) isoFormat = "%Y-%m-%d" :
   AMC> missing value where TRUE/FALSE needed
   AMC> In addition: Warning message:
   AMC> Unknown Format Specification in: .whichFormat(charvec)

Hi Anass,

I couldn't reproduce the error with R-2.6.1 and Rmetrics-260.72.
What version of R and Rmetrics are you using? 


regards,
Yohan


From anass.mouhsine at sgcib.com  Wed Jan 30 18:47:10 2008
From: anass.mouhsine at sgcib.com (anass.mouhsine at sgcib.com)
Date: Wed, 30 Jan 2008 18:47:10 +0100
Subject: [R-SIG-Finance] timeDate conversion [NC]
In-Reply-To: <20080130165843.3c979619@yankee-laptop>
Message-ID: <OF3B8023DC.1E182042-ONC12573E0.00615348-C12573E0.0061B3DC@fr.world.socgen>

Actually I am using R 2.5.1 and Rmetrics221.10065

Thanks for the tip it might be that. I will download the last version and
test back.





|------------------------------                                            
|            chalabi at phys.ethz                                             
|            .ch                                                           
|                                                                          
|            01/30/2008 16:58                                              
|                                                                          
                                                                           
                                                                           
                                                                        To 
                                               Anass                       
                                               MOUHSINE/fr/socgen at socgen   
                                                                        cc 
                                               r-sig-finance at stat.math.eth 
                                               z.ch                        
                                                                   Subject 
                                               Re: [R-SIG-Finance]         
                                               timeDate conversion [C1]    
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




>>>> "AMC" == anass.mouhsine at sgcib.com
>>>> on Wed, 30 Jan 2008 16:00:07 +0100

   AMC> > dates.tmp=timeDate("2005-04-08 08:30:35.916",format="%Y-%m-%d
   AMC> %H:%M:%OS")
   AMC> Error in if (sum(lt + lt + lt) == 0) isoFormat = "%Y-%m-%d" :
   AMC> missing value where TRUE/FALSE needed
   AMC> In addition: Warning message:
   AMC> Unknown Format Specification in: .whichFormat(charvec)

Hi Anass,

I couldn't reproduce the error with R-2.6.1 and Rmetrics-260.72.
What version of R and Rmetrics are you using?


regards,
Yohan

*************************************************************************
This message and any attachments (the "message") are con...{{dropped:10}}


From tom.soyer at gmail.com  Thu Jan 31 18:36:39 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Thu, 31 Jan 2008 11:36:39 -0600
Subject: [R-SIG-Finance] Does R have a formal test for long vs short memory
	process?
Message-ID: <65cc7bdf0801310936s39795251t341f22501a941f83@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080131/40c28b0f/attachment.pl 

From brian at braverock.com  Thu Jan 31 19:21:52 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 31 Jan 2008 12:21:52 -0600
Subject: [R-SIG-Finance] Does R have a formal test for long vs short
 memory process?
In-Reply-To: <65cc7bdf0801310936s39795251t341f22501a941f83@mail.gmail.com>
References: <65cc7bdf0801310936s39795251t341f22501a941f83@mail.gmail.com>
Message-ID: <47A211C0.7010500@braverock.com>

tom soyer wrote:
> Does anyone know if there are formal tests for long vs short memory
> processes? i.e., quantitative tests instead of visual examination of
> corellograms produced by acf.

Perhaps you could be a bit more specific about what you want?

In addition to the ACF chart, the acf calculation calculates confidence 
intervals for significance.  The summary() method on the results of an 
acf will tell you what the values for these confidence intervals are.

There are also several other quantitative methods that have been 
proposed for measuring and dealing with acf and partial acf in financial 
time series.  If you have one of these methods in mind, perhaps we can 
see if they are either already implemented or could be implemented easily.

Regards,

    - Brian


From tom.soyer at gmail.com  Thu Jan 31 19:36:01 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Thu, 31 Jan 2008 12:36:01 -0600
Subject: [R-SIG-Finance] Does R have a formal test for long vs short
	memory process?
In-Reply-To: <47A211C0.7010500@braverock.com>
References: <65cc7bdf0801310936s39795251t341f22501a941f83@mail.gmail.com>
	<47A211C0.7010500@braverock.com>
Message-ID: <65cc7bdf0801311036r4697c42bg733084b039f8996b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080131/e9560e07/attachment.pl 

From spencer.graves at pdf.com  Fri Feb  1 03:20:19 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 31 Jan 2008 18:20:19 -0800
Subject: [R-SIG-Finance] Does R have a formal test for long vs
 short	memory process?
In-Reply-To: <65cc7bdf0801311036r4697c42bg733084b039f8996b@mail.gmail.com>
References: <65cc7bdf0801310936s39795251t341f22501a941f83@mail.gmail.com>	<47A211C0.7010500@braverock.com>
	<65cc7bdf0801311036r4697c42bg733084b039f8996b@mail.gmail.com>
Message-ID: <47A281E3.9040108@pdf.com>

      If it were my problem, I would start by writing probability models 
for long and short memory processes.  I would cast them in a Bayesian 
framework with plausible priors over the unknown parameters;  with 
multiple series, it should be easy enough to get plausible priors.  Then 
I would test one vs. the other using a likelihood ratio of simple 
hypothesis (i.e., the marginal with all the parameters integrated out 
using the posterior distribution) vs. simple alternative.  I could do 
that with Markov Chain Monte Carlo if I didn't feel comfortable with any 
other approximation. 

      The Neyman-Pearson lemma says that the most powerful test of 
simple vs. simple is the likelihood ratio.  I could get p-values by 
Monte Carlo if by nothing else. 

      I'd start with a literature search.  The references I know about 
that are in Tsay (2005) Analysis of Financial Time Series, 2nd ed. 
(Wiley):  Section 2.11 discusses long-memory models, and section 3.13 
describes long-memory stochastic volatility models.  The data sets 
described in that book are all available in the 'FinTS' package, and 
'scripts\ch02.R' includes R code to recreate the figures in chapter 2 
(including Figure 2.22 pertaining to section 2.11). 

      Hope this helps. 
      Spencer Graves

tom soyer wrote:
> Thanks Brian. I wanted to test if a data series has long memory or short
> memory. By short memory process, I meant that their acf declines
> exponentially. For long memory processes, their acf declines very slowly. I
> was thinking that if such a test is available, then one could use it to help
> determine how to model a series, e.g. ARMA vs. GARCH, etc. One could make
> the determination based on a visual examination of the acf correllogram, but
> the problem with this method is that it's not quantitative and therefore not
> automatable. Does that make sense?
>
>
> On 1/31/08, Brian G. Peterson <brian at braverock.com> wrote:
>   
>> tom soyer wrote:
>>     
>>> Does anyone know if there are formal tests for long vs short memory
>>> processes? i.e., quantitative tests instead of visual examination of
>>> corellograms produced by acf.
>>>       
>> Perhaps you could be a bit more specific about what you want?
>>
>> In addition to the ACF chart, the acf calculation calculates confidence
>> intervals for significance.  The summary() method on the results of an
>> acf will tell you what the values for these confidence intervals are.
>>
>> There are also several other quantitative methods that have been
>> proposed for measuring and dealing with acf and partial acf in financial
>> time series.  If you have one of these methods in mind, perhaps we can
>> see if they are either already implemented or could be implemented easily.
>>
>> Regards,
>>
>>    - Brian
>>
>>     
>
>
>
>


From markus at insightfromdata.com  Fri Feb  1 20:03:16 2008
From: markus at insightfromdata.com (Markus Loecher)
Date: Fri, 01 Feb 2008 14:03:16 -0500
Subject: [R-SIG-Finance] plot.zoo axis labels/ticks
Message-ID: <C3C8D724.63E%markus@insightfromdata.com>

Dear zoo experts,
While I follow the custom x-axis example in the help file for plot.zoo(), it
is not clear to me how to easily display ticks at "natural" intervals, such
as months/weeks/days.
For example, when I plot a daily series that extends over ~ 2 years, the
default seems to place labels only at the yearly marks, whereas I would
often want monthly or quarterly ticks/labels.

Any help would be great,
Thanks,
Markus


From jeff.a.ryan at gmail.com  Fri Feb  1 20:20:24 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 1 Feb 2008 19:20:24 +0000
Subject: [R-SIG-Finance] plot.zoo axis labels/ticks
In-Reply-To: <C3C8D724.63E%markus@insightfromdata.com>
References: <C3C8D724.63E%markus@insightfromdata.com>
Message-ID: <e8e755250802011120m123269faw9a9e4bcb09cd4e78@mail.gmail.com>

Markus,

I can't necessarily help with the zoo example, but...

The quantmod package has a charting facility that does what I think you want.

Generally speaking you can pass any zoo object into chartSeries() and
it will draw an OHLC style chart (if it has OHLC qualities/columns),
or a line-chart if it is a univariate object.

The labeling is handled inside the main chartSeries call by a local
function called ticks().  Basically uses some formatting functions
inside of quantmod and xts to find a suitable looking set of tickmarks
- both major (with labels) and minor (lighter color and _no_ labels).
The labels even get formatted based on the underlying periodicity of
the data, as well as the level of detail (intraday will work, as will
yearly...).

chartSeries itself may do all you need.  If not feel free to take the
ticks code and the associated xts/quantmod functions and copy to your
own solution.

If you need more assistance, I can probably elaborate here, or email
me directly.

Jeff
On Feb 1, 2008 7:03 PM, Markus Loecher <markus at insightfromdata.com> wrote:
> Dear zoo experts,
> While I follow the custom x-axis example in the help file for plot.zoo(), it
> is not clear to me how to easily display ticks at "natural" intervals, such
> as months/weeks/days.
> For example, when I plot a daily series that extends over ~ 2 years, the
> default seems to place labels only at the yearly marks, whereas I would
> often want monthly or quarterly ticks/labels.
>
> Any help would be great,
> Thanks,
> Markus
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From ggrothendieck at gmail.com  Fri Feb  1 20:26:06 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 1 Feb 2008 14:26:06 -0500
Subject: [R-SIG-Finance] plot.zoo axis labels/ticks
In-Reply-To: <C3C8D724.63E%markus@insightfromdata.com>
References: <C3C8D724.63E%markus@insightfromdata.com>
Message-ID: <971536df0802011126u25425456vbc2f949bd889a5cf@mail.gmail.com>

plot.zoo uses whatever axis function is available for whatever class you
are using.

For example, if you are using Date as your time class then
axis.Date from R is used as default.

The latest version of zoo, 1.4-2, has completely rewritten axis.yearmon
and axis.yearqtr functions that are invoked if you are using yearmon or
yearqtr time class series.

If you want custom axes then for single panel plot.zoo plots you
call axis in the same way that you do with any classic graphics
plot in R.  For multi-panel plot.zoo plots see the examples provided
in ?plot.zoo

For xyplot.zoo plots you do it the same way as for xyplot in lattice
since ultimately lattice xyplot is called.

On Feb 1, 2008 2:03 PM, Markus Loecher <markus at insightfromdata.com> wrote:
> Dear zoo experts,
> While I follow the custom x-axis example in the help file for plot.zoo(), it
> is not clear to me how to easily display ticks at "natural" intervals, such
> as months/weeks/days.
> For example, when I plot a daily series that extends over ~ 2 years, the
> default seems to place labels only at the yearly marks, whereas I would
> often want monthly or quarterly ticks/labels.
>
> Any help would be great,
> Thanks,
> Markus


From MichelBeck at sbcglobal.Net  Fri Feb  1 21:29:00 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Fri, 1 Feb 2008 20:29:00 +0000 (UTC)
Subject: [R-SIG-Finance] Zoo functions
Message-ID: <loom.20080201T202320-56@post.gmane.org>

Hi!

1) The documentation for the zoo package indicates a function lag.zoo .
I can't find it when I try to use it or do a help(lag.zoo). I can find all the 
other zoo functions I tried so far (allthough diff.zoo is missing also).

Any hints?

2) What does the following message imply?

Warning message:
some methods for "zoo" objects do not work if the index entries in 'order.by' 
are not unique in: zoo(rval, x.index[i]) 


Thanks for your help,

Michel


From ggrothendieck at gmail.com  Fri Feb  1 21:46:55 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 1 Feb 2008 15:46:55 -0500
Subject: [R-SIG-Finance] Zoo functions
In-Reply-To: <loom.20080201T202320-56@post.gmane.org>
References: <loom.20080201T202320-56@post.gmane.org>
Message-ID: <971536df0802011246x3c1b9ec0u810b52c46a33775a@mail.gmail.com>

On Feb 1, 2008 3:29 PM, MAB <MichelBeck at sbcglobal.net> wrote:
> Hi!
>
> 1) The documentation for the zoo package indicates a function lag.zoo .
> I can't find it when I try to use it or do a help(lag.zoo). I can find all the
> other zoo functions I tried so far (allthough diff.zoo is missing also).
>
> Any hints?

Maybe you forgot to issue library(zoo) first or perhaps there
is something wrong with your installation. help(lag.zoo) and
help(diff.zoo) both work for me and point to the same page.
Also, those and all other zoo help files can be found online:

http://cran.r-project.org/src/contrib/Descriptions/zoo.html

>
> 2) What does the following message imply?
>
> Warning message:
> some methods for "zoo" objects do not work if the index entries in 'order.by'
> are not unique in: zoo(rval, x.index[i])

See #1 in the faq:

vignette("zoo-faq")

or get the FAQ at the link mentioned above.


From edd at debian.org  Fri Feb  1 22:03:14 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 1 Feb 2008 15:03:14 -0600
Subject: [R-SIG-Finance] Zoo functions
In-Reply-To: <loom.20080201T202320-56@post.gmane.org>
References: <loom.20080201T202320-56@post.gmane.org>
Message-ID: <18339.35090.314737.735789@ron.nulle.part>


On 1 February 2008 at 20:29, MAB wrote:
| 1) The documentation for the zoo package indicates a function lag.zoo .
| I can't find it when I try to use it or do a help(lag.zoo). I can find all the 
| other zoo functions I tried so far (allthough diff.zoo is missing also).
| 
| Any hints?

Error on your part. The form is function.class, and the .class (here .zoo)
part is ued for internal dispatching of methods to the correct class -- here
zoo.

Do

	> library(zoo)
	> help(lag)

or

	> help(lag, package="zoo")


| 2) What does the following message imply?
| 
| Warning message:
| some methods for "zoo" objects do not work if the index entries in 'order.by' 
| are not unique in: zoo(rval, x.index[i]) 

Two of the letters in zoo stand for 'ordered observations', and zoo expects
these to to (strictly) monotonically increasing. You get by with ties in the
sense that neither the constructor nor the other functions die on you, but
they reserve the right to warn you.  Which happened here.  If you check

	> summary(diff(x.index[i]))

you probably find that the minimum (difference) is not a positive number. You
want it to be one, in most cases.

Dirk

-- 
Three out of two people have difficulties with fractions.


From tom.soyer at gmail.com  Sat Feb  2 02:55:42 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Fri, 1 Feb 2008 19:55:42 -0600
Subject: [R-SIG-Finance] help with garchSim
Message-ID: <65cc7bdf0802011755i80843bepe6defd5c264669f7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080201/c7265067/attachment.pl 

From edd at debian.org  Sat Feb  2 03:29:09 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 1 Feb 2008 20:29:09 -0600
Subject: [R-SIG-Finance] help with garchSim
In-Reply-To: <65cc7bdf0802011755i80843bepe6defd5c264669f7@mail.gmail.com>
References: <65cc7bdf0802011755i80843bepe6defd5c264669f7@mail.gmail.com>
Message-ID: <18339.54645.839916.950757@ron.nulle.part>


On 1 February 2008 at 19:55, tom soyer wrote:
| simulate.garch of S+. I assume that there is no simulate.garch in R. I am

Well you could always test your assumption.  A quick

	> RSiteSearch("simulate garch") 

yields 14 hits. While the first two are off, the rest leads you the right
way.  That said, there are no canned replacement functions that exactly redo
the S+ code.

You could try to write some -- IIRC Insightful created some compatibility
layers, and folks like Paul Gilbert who have been living between S+ and R for
at least a decade even have RNGs that create identical sequences to get the
same quasi-random numbers.



-- 
Three out of two people have difficulties with fractions.


From tom.soyer at gmail.com  Sat Feb  2 03:57:30 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Fri, 1 Feb 2008 20:57:30 -0600
Subject: [R-SIG-Finance] help with garchSim
In-Reply-To: <18339.54645.839916.950757@ron.nulle.part>
References: <65cc7bdf0802011755i80843bepe6defd5c264669f7@mail.gmail.com>
	<18339.54645.839916.950757@ron.nulle.part>
Message-ID: <65cc7bdf0802011857k32e33e45hc1bead3bbd02dfce@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080201/49c36f8b/attachment.pl 

From tom.soyer at gmail.com  Sat Feb  2 17:41:50 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sat, 2 Feb 2008 10:41:50 -0600
Subject: [R-SIG-Finance] Question about garchSim and garch
Message-ID: <65cc7bdf0802020841s6f2005e3n3996194d88d42966@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080202/33f2fdcb/attachment.pl 

From spencer.graves at pdf.com  Sat Feb  2 18:19:17 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Feb 2008 09:19:17 -0800
Subject: [R-SIG-Finance] Question about garchSim and garch
In-Reply-To: <65cc7bdf0802020841s6f2005e3n3996194d88d42966@mail.gmail.com>
References: <65cc7bdf0802020841s6f2005e3n3996194d88d42966@mail.gmail.com>
Message-ID: <47A4A615.8050104@pdf.com>

Hi, Tom: 

      The file 'scripts\ch03.R' in the FinTS package includes a brief 
description of attempts to use garch{tseries} and garchFit{fGarch}.  I 
don't understand either function very well, but I got answers from 
'garchFit' that seemed to match some of the published results in Tsay;  
I gave up on 'garch'. 

      Since 'garchSim' and 'garchFit' are both in 'fGarch', I would 
expect that it should be moderately easy to simulate something, plot the 
result, and see for yourself.  Chapter 3 of Tsay (2005) gives a 
reasonable overview of GARCH and related models with several examples.  
The companion script\ch03.R is far from complete but might help. 

      You may find the following example from 'ch03.R' of interest: 

library(FinTS)
data(sp500)
library(fGarch)
spFit30.11 <- garchFit(sp500~arma(3,0)+garch(1,1), data=sp500)

      This specifies an arma(3,0) mean model with garch(1,1) noise.  
This syntax is buried in the 'garchFit' help page. 

      Hope this helps. 
      Spencer

tom soyer wrote:
> Hi,
>
> I am new to GARCH and I am trying to figure out how to use R's garchSim and
> garch, and I am a bit confused. I am hopeing that R finance experts can help
> me understand them better. If we look at the definition of GARCH(1,1),
> there should be two equations:
> [1]: Y(t) = c + e(t), and
> [2]: sigma^2(t) = a0 + a1*e^2(t-1) + b1*sigma^2(t-1)
>
> So, I would expect any garch simulation function to four parameters: c, a0,
> a1, and b1. But take a look at the garchSim, it has only three parameters:
> model = list(omega = 1.0e-6, alpha = 0.1, beta = 0.8). I assume here that
> omega = a0 in [2], alpha=a1, and beta=b1. If so, then it seems that in
> garchSim, c, the constant (or the mean) in [1], is always assumed to be
> zero. Does anyone know if this is true? I just want to make sure that I
> understand exactly what I should expect from the output of the garchSim
> function.
>
> Also, I have a similar question about garch. It seems that the coefficients
> estimated by garch(x,order=c(1,1)) are a0, a1, and b1. Like garchSim, there
> is no c, the mean. So does this mean garch also assumes zero mean and thus
> actually fits model [2] instead of both [1] and [2]?
>
> Thanks!
>
>


From tom.soyer at gmail.com  Sun Feb  3 00:00:15 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sat, 2 Feb 2008 17:00:15 -0600
Subject: [R-SIG-Finance] Question about garchSim and garch
In-Reply-To: <47A4A615.8050104@pdf.com>
References: <65cc7bdf0802020841s6f2005e3n3996194d88d42966@mail.gmail.com>
	<47A4A615.8050104@pdf.com>
Message-ID: <65cc7bdf0802021500p444aeae0xf4ffc2eb46151d25@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080202/e46f152a/attachment.pl 

From spencer.graves at pdf.com  Sun Feb  3 03:06:36 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Feb 2008 18:06:36 -0800
Subject: [R-SIG-Finance] Question about garchSim and garch
In-Reply-To: <65cc7bdf0802021500p444aeae0xf4ffc2eb46151d25@mail.gmail.com>
References: <65cc7bdf0802020841s6f2005e3n3996194d88d42966@mail.gmail.com>	
	<47A4A615.8050104@pdf.com>
	<65cc7bdf0802021500p444aeae0xf4ffc2eb46151d25@mail.gmail.com>
Message-ID: <47A521AC.8050005@pdf.com>

I got only one hit for

      RSiteSearch("egarch", "fun")

That was for GarchOxInterface {fGarch}

      Spencer

tom soyer wrote:
> Thnaks Spencer. I am glad I am not the only one that find garch 
> strange. I guess I will give up on it too. It seems that garchFit and 
> garchSim are very good. They have been giving me good results so far.
>  
> Thanks for the tip on how to specify arma + garch model. I found this 
> paper also very 
> helpful:http://www.itp.phys.ethz.ch/econophysics/R/pdf/garch.pdf.
>  
> Do you know how to specify arma + egarch model in R? Is it even 
> possible in R without installing Ox?
>
>  
> On 2/2/08, *Spencer Graves* <spencer.graves at pdf.com 
> <mailto:spencer.graves at pdf.com>> wrote:
>
>     Hi, Tom:
>
>          The file 'scripts\ch03.R' in the FinTS package includes a brief
>     description of attempts to use garch{tseries} and garchFit{fGarch}.  I
>     don't understand either function very well, but I got answers from
>     'garchFit' that seemed to match some of the published results in Tsay;
>     I gave up on 'garch'.
>
>          Since 'garchSim' and 'garchFit' are both in 'fGarch', I would
>     expect that it should be moderately easy to simulate something,
>     plot the
>     result, and see for yourself.  Chapter 3 of Tsay (2005) gives a
>     reasonable overview of GARCH and related models with several examples.
>     The companion script\ch03.R is far from complete but might help.
>
>          You may find the following example from 'ch03.R' of interest:
>
>     library(FinTS)
>     data(sp500)
>     library(fGarch)
>     spFit30.11 <- garchFit(sp500~arma(3,0)+garch(1,1), data=sp500)
>
>          This specifies an arma(3,0) mean model with garch(1,1) noise.
>     This syntax is buried in the 'garchFit' help page.
>
>          Hope this helps.
>          Spencer
>
>     tom soyer wrote:
>     > Hi,
>     >
>     > I am new to GARCH and I am trying to figure out how to use R's
>     garchSim and
>     > garch, and I am a bit confused. I am hopeing that R finance
>     experts can help
>     > me understand them better. If we look at the definition of
>     GARCH(1,1),
>     > there should be two equations:
>     > [1]: Y(t) = c + e(t), and
>     > [2]: sigma^2(t) = a0 + a1*e^2(t-1) + b1*sigma^2(t-1)
>     >
>     > So, I would expect any garch simulation function to four
>     parameters: c, a0,
>     > a1, and b1. But take a look at the garchSim, it has only three
>     parameters:
>     > model = list(omega = 1.0e-6, alpha = 0.1, beta = 0.8). I assume
>     here that
>     > omega = a0 in [2], alpha=a1, and beta=b1. If so, then it seems
>     that in
>     > garchSim, c, the constant (or the mean) in [1], is always
>     assumed to be
>     > zero. Does anyone know if this is true? I just want to make sure
>     that I
>     > understand exactly what I should expect from the output of the
>     garchSim
>     > function.
>     >
>     > Also, I have a similar question about garch. It seems that the
>     coefficients
>     > estimated by garch(x,order=c(1,1)) are a0, a1, and b1. Like
>     garchSim, there
>     > is no c, the mean. So does this mean garch also assumes zero
>     mean and thus
>     > actually fits model [2] instead of both [1] and [2]?
>     >
>     > Thanks!
>     >
>     >
>
>
>
>
> -- 
> Tom


From tom.soyer at gmail.com  Sun Feb  3 03:29:34 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sat, 2 Feb 2008 20:29:34 -0600
Subject: [R-SIG-Finance] Zivot vs. Engle vs. Stoffer - help with the meaning
	of different GARCH notations, please!
Message-ID: <65cc7bdf0802021829q62c13915u8a074553b852be88@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080202/dd6e4155/attachment.pl 

From tom.soyer at gmail.com  Sun Feb  3 03:37:09 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sat, 2 Feb 2008 20:37:09 -0600
Subject: [R-SIG-Finance] Question about garchSim and garch
In-Reply-To: <47A521AC.8050005@pdf.com>
References: <65cc7bdf0802020841s6f2005e3n3996194d88d42966@mail.gmail.com>
	<47A4A615.8050104@pdf.com>
	<65cc7bdf0802021500p444aeae0xf4ffc2eb46151d25@mail.gmail.com>
	<47A521AC.8050005@pdf.com>
Message-ID: <65cc7bdf0802021837t5920acdetea202bb9c06642ac@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080202/7dc903f8/attachment.pl 

From brian at braverock.com  Sun Feb  3 04:06:21 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Sat, 02 Feb 2008 21:06:21 -0600
Subject: [R-SIG-Finance] Zivot vs. Engle vs. Stoffer - help with the
 meaning of different GARCH notations, please!
In-Reply-To: <65cc7bdf0802021829q62c13915u8a074553b852be88@mail.gmail.com>
References: <65cc7bdf0802021829q62c13915u8a074553b852be88@mail.gmail.com>
Message-ID: <47A52FAD.3020401@braverock.com>

tom soyer wrote:
> Hi,
> 
> I have a question with regard to different GARCH notations I found in the
> literature, and I am wondering if anyone knows how to reconcile these
> differences. Below are three different notations that supposedly all define
> the GARCH(1,1) process:
> 
> In Zivot's book, MFTSWS, the GARCH(1,1) process is defined as:
> [1]: Y(t) = c + e(t), and
> [2]: sigma^2(t) = a0 + a1*e^2(t-1) + b1*sigma^2(t-1)

I just looked in my current copy of Zivot and Wang MFTSwS+ (2006), p. 
230, Eqs 7.4 and following, and your notation here doesn't match what's 
in the reference (your Eq [2] appears equivalent to Eq. 7.5).  perhaps 
next time you can be more specific in your reference (pages and Eq. 
numbers?)


> In Engle's paper, the GARCH(1,1) process is defined (in financial notation),
> like this:
>  [3]: r(t) = m(t) + sqrt(h(t))*e(t), and
> [4]: h(t+1) = a0 + a1*h(t)*e^2(t) + b1*h(t)

I don't know which Engle paper you're referring to.  With the possible 
exception of m(t) in your Eq[3] and the use of t+1 as the target in 
Eq[4] (thus specifying the prediction), Eq [4] is equivalent to Eq [2] 
and Eq [6]

> In Stoffer's book, the GARCH(1,1) is define as:
> [5]: Y(t) = sigma(t)*e(t), and
> [6]: sigma^2(t) = a0 + a1*Y^2(t-1) + b1*sigma^2(t-1)

Shumway and Stoffer "Time Series Analysis and Its Applications, 2nd 
Ed."(2006), p. 286 Eqs. 5.30 and 5.44 match your Eq [5] and [6] and 
match Zivot&Wang's representation.

Note that Shumway and Stoffer also has several fairly extensive examples 
of working with GARCH models in R.

> Does anyone know if all three above are just different ways of saying the
> same thing, or are they drastically different with respect to the
> specification of the GARCH model to be fitted?

Notation is always a real pain to sort out as you are reading various 
papers and books.  It is not uncommon to find errors in the references, 
which is usually cleared up only via looking further back in time to 
more primary sources.

So, without precise references, I can only give you a qualified "these 
models all appear equivalent".

Regards,

   - Brian


From tom.soyer at gmail.com  Sun Feb  3 04:33:57 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sat, 2 Feb 2008 21:33:57 -0600
Subject: [R-SIG-Finance] Zivot vs. Engle vs. Stoffer - help with the
	meaning of different GARCH notations, please!
In-Reply-To: <47A52FAD.3020401@braverock.com>
References: <65cc7bdf0802021829q62c13915u8a074553b852be88@mail.gmail.com>
	<47A52FAD.3020401@braverock.com>
Message-ID: <65cc7bdf0802021933j68108fbdpaac0b95bcfde1f6f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080202/9524cee4/attachment.pl 

From brian at braverock.com  Sun Feb  3 05:39:21 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Sat, 02 Feb 2008 22:39:21 -0600
Subject: [R-SIG-Finance] Zivot vs. Engle vs. Stoffer - help with the
 meaning of different GARCH notations, please!
In-Reply-To: <65cc7bdf0802021933j68108fbdpaac0b95bcfde1f6f@mail.gmail.com>
References: <65cc7bdf0802021829q62c13915u8a074553b852be88@mail.gmail.com>	
	<47A52FAD.3020401@braverock.com>
	<65cc7bdf0802021933j68108fbdpaac0b95bcfde1f6f@mail.gmail.com>
Message-ID: <47A54579.9020802@braverock.com>

tom soyer wrote:
> Thanks Brian. Sorry about the reference being not clear. Here is my 2nd try:
>  
> Zivot: MFTSWS, 2003 edition. [1] page 212, equation 7.1, and [2] page 
> 216, equation 7.5
> Engle: Garch101, middle of page 160.
> Stoffer: you had it exactly right. page 286, equation 5.30 and 5.44.

I don't have the Engle reference handy, so my comments should be 
construed as mostly from Zivot and Wang, and Shumway and Stoffer.  The 
Engle notations appear to be equivalent, with a possible additional 
term, as I said in my prior email.

> So my follow up question to you is why do "these models all appear 
> equivalent" to you? Take equation [6] for example, it says to me that 
> one could model the variance part of the process using garch and only 
> garch. If these notations were all the same, then garch alone should 
> also be able to model [2], right? But it seems to me that that's not the 
> case, i.e., arma + garch is needed to model [2]. Do you know what I 
> mean? Does that make sense?

Eq's [1],[3],[5] in your list all refer to an AR(1) model for the 
returns, of the variance modified by a white-noise parameter.

Eq's [2],[4],[6] in your list all describe the GARCH(1,1) "generalized" 
extension of the basic ARCH process, which does indeed utilize an ARMA 
process to model y^2(t).  See the discussion around and following 
Shumway and Stoffer's Eq. 5.45 or Zivot and Wang's Eq. 7.6

Regards,

   - Brian


> On 2/2/08, *Brian G. Peterson* <brian at braverock.com 
> <mailto:brian at braverock.com>> wrote:
> 
>     tom soyer wrote:
>      > Hi,
>      >
>      > I have a question with regard to different GARCH notations I
>     found in the
>      > literature, and I am wondering if anyone knows how to reconcile these
>      > differences. Below are three different notations that supposedly
>     all define
>      > the GARCH(1,1) process:
>      >
>      > In Zivot's book, MFTSWS, the GARCH(1,1) process is defined as:
>      > [1]: Y(t) = c + e(t), and
>      > [2]: sigma^2(t) = a0 + a1*e^2(t-1) + b1*sigma^2(t-1)
> 
>     I just looked in my current copy of Zivot and Wang MFTSwS+ (2006), p.
>     230, Eqs 7.4 and following, and your notation here doesn't match what's
>     in the reference (your Eq [2] appears equivalent to Eq. 7.5).  perhaps
>     next time you can be more specific in your reference (pages and Eq.
>     numbers?)
> 
> 
>      > In Engle's paper, the GARCH(1,1) process is defined (in financial
>     notation),
>      > like this:
>      >  [3]: r(t) = m(t) + sqrt(h(t))*e(t), and
>      > [4]: h(t+1) = a0 + a1*h(t)*e^2(t) + b1*h(t)
> 
>     I don't know which Engle paper you're referring to.  With the possible
>     exception of m(t) in your Eq[3] and the use of t+1 as the target in
>     Eq[4] (thus specifying the prediction), Eq [4] is equivalent to Eq [2]
>     and Eq [6]
> 
>      > In Stoffer's book, the GARCH(1,1) is define as:
>      > [5]: Y(t) = sigma(t)*e(t), and
>      > [6]: sigma^2(t) = a0 + a1*Y^2(t-1) + b1*sigma^2(t-1)
> 
>     Shumway and Stoffer "Time Series Analysis and Its Applications, 2nd
>     Ed."(2006), p. 286 Eqs. 5.30 and 5.44 match your Eq [5] and [6] and
>     match Zivot&Wang's representation.
> 
>     Note that Shumway and Stoffer also has several fairly extensive examples
>     of working with GARCH models in R.
> 
>      > Does anyone know if all three above are just different ways of
>     saying the
>      > same thing, or are they drastically different with respect to the
>      > specification of the GARCH model to be fitted?
> 
>     Notation is always a real pain to sort out as you are reading various
>     papers and books.  It is not uncommon to find errors in the references,
>     which is usually cleared up only via looking further back in time to
>     more primary sources.
> 
>     So, without precise references, I can only give you a qualified "these
>     models all appear equivalent".
> 
>     Regards,
> 
>       - Brian


From patrick at burns-stat.com  Sun Feb  3 10:32:55 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Sun, 03 Feb 2008 09:32:55 +0000
Subject: [R-SIG-Finance] Zivot vs. Engle vs. Stoffer - help with the
 meaning of different GARCH notations, please!
In-Reply-To: <47A54579.9020802@braverock.com>
References: <65cc7bdf0802021829q62c13915u8a074553b852be88@mail.gmail.com>		<47A52FAD.3020401@braverock.com>	<65cc7bdf0802021933j68108fbdpaac0b95bcfde1f6f@mail.gmail.com>
	<47A54579.9020802@braverock.com>
Message-ID: <47A58A47.5030307@burns-stat.com>

Brian G. Peterson wrote:

> [... skip (about some references) ...]
>
>>Eq's [1],[3],[5] in your list all refer to an AR(1) model for the 
>>returns, of the variance modified by a white-noise parameter.
>>    
>>

I don't think this is an accurate statement.  In Eq 1 the mean
is modeled by 'c', that is a constant.  In Eq 3 the mean is modeled
by 'm(t)' -- on the surface at least an arbitrary time series model
that could be ARMA or whatever.  Eq 5 assumes a constant mean
of zero.

The other difference in the equations is whether or not 'e(t)' is the
residuals or the standardized residuals.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

>>Eq's [2],[4],[6] in your list all describe the GARCH(1,1) "generalized" 
>>extension of the basic ARCH process, which does indeed utilize an ARMA 
>>process to model y^2(t).  See the discussion around and following 
>>Shumway and Stoffer's Eq. 5.45 or Zivot and Wang's Eq. 7.6
>>
>>Regards,
>>
>>   - Brian
>>
>>
>>    
>>
>>On 2/2/08, *Brian G. Peterson* <brian at braverock.com 
>><mailto:brian at braverock.com>> wrote:
>>
>>    tom soyer wrote:
>>     > Hi,
>>     >
>>     > I have a question with regard to different GARCH notations I
>>    found in the
>>     > literature, and I am wondering if anyone knows how to reconcile these
>>     > differences. Below are three different notations that supposedly
>>    all define
>>     > the GARCH(1,1) process:
>>     >
>>     > In Zivot's book, MFTSWS, the GARCH(1,1) process is defined as:
>>     > [1]: Y(t) = c + e(t), and
>>     > [2]: sigma^2(t) = a0 + a1*e^2(t-1) + b1*sigma^2(t-1)
>>
>>    I just looked in my current copy of Zivot and Wang MFTSwS+ (2006), p.
>>    230, Eqs 7.4 and following, and your notation here doesn't match what's
>>    in the reference (your Eq [2] appears equivalent to Eq. 7.5).  perhaps
>>    next time you can be more specific in your reference (pages and Eq.
>>    numbers?)
>>
>>
>>     > In Engle's paper, the GARCH(1,1) process is defined (in financial
>>    notation),
>>     > like this:
>>     >  [3]: r(t) = m(t) + sqrt(h(t))*e(t), and
>>     > [4]: h(t+1) = a0 + a1*h(t)*e^2(t) + b1*h(t)
>>
>>    I don't know which Engle paper you're referring to.  With the possible
>>    exception of m(t) in your Eq[3] and the use of t+1 as the target in
>>    Eq[4] (thus specifying the prediction), Eq [4] is equivalent to Eq [2]
>>    and Eq [6]
>>
>>     > In Stoffer's book, the GARCH(1,1) is define as:
>>     > [5]: Y(t) = sigma(t)*e(t), and
>>     > [6]: sigma^2(t) = a0 + a1*Y^2(t-1) + b1*sigma^2(t-1)
>>
>>    Shumway and Stoffer "Time Series Analysis and Its Applications, 2nd
>>    Ed."(2006), p. 286 Eqs. 5.30 and 5.44 match your Eq [5] and [6] and
>>    match Zivot&Wang's representation.
>>
>>    Note that Shumway and Stoffer also has several fairly extensive examples
>>    of working with GARCH models in R.
>>
>>     > Does anyone know if all three above are just different ways of
>>    saying the
>>     > same thing, or are they drastically different with respect to the
>>     > specification of the GARCH model to be fitted?
>>
>>    Notation is always a real pain to sort out as you are reading various
>>    papers and books.  It is not uncommon to find errors in the references,
>>    which is usually cleared up only via looking further back in time to
>>    more primary sources.
>>
>>    So, without precise references, I can only give you a qualified "these
>>    models all appear equivalent".
>>
>>    Regards,
>>
>>      - Brian
>>    
>>
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From patrick at burns-stat.com  Sun Feb  3 10:37:23 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Sun, 03 Feb 2008 09:37:23 +0000
Subject: [R-SIG-Finance] Question about garchSim and garch
In-Reply-To: <65cc7bdf0802021500p444aeae0xf4ffc2eb46151d25@mail.gmail.com>
References: <65cc7bdf0802020841s6f2005e3n3996194d88d42966@mail.gmail.com>	<47A4A615.8050104@pdf.com>
	<65cc7bdf0802021500p444aeae0xf4ffc2eb46151d25@mail.gmail.com>
Message-ID: <47A58B53.9050704@burns-stat.com>

tom soyer wrote:

>Thnaks Spencer. I am glad I am not the only one that find garch strange. I
>guess I will give up on it too. It seems that garchFit and garchSim are very
>good. They have been giving me good results so far.
>
>Thanks for the tip on how to specify arma + garch model. I found this paper
>also very helpful:http://www.itp.phys.ethz.ch/econophysics/R/pdf/garch.pdf.
>
>Do you know how to specify arma + egarch model in R? Is it even possible in
>R without installing Ox?
>  
>

In my experience ARMA estimation and garch estimation are
suitably robust to each other.  It is definitely second prize to have
to estimate one and then the other, but your results are unlikely to
be all that different than if you did it "right".  (I'd love to hear of
any counter-examples.)

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

>
>On 2/2/08, Spencer Graves <spencer.graves at pdf.com> wrote:
>  
>
>>Hi, Tom:
>>
>>     The file 'scripts\ch03.R' in the FinTS package includes a brief
>>description of attempts to use garch{tseries} and garchFit{fGarch}.  I
>>don't understand either function very well, but I got answers from
>>'garchFit' that seemed to match some of the published results in Tsay;
>>I gave up on 'garch'.
>>
>>     Since 'garchSim' and 'garchFit' are both in 'fGarch', I would
>>expect that it should be moderately easy to simulate something, plot the
>>result, and see for yourself.  Chapter 3 of Tsay (2005) gives a
>>reasonable overview of GARCH and related models with several examples.
>>The companion script\ch03.R is far from complete but might help.
>>
>>     You may find the following example from 'ch03.R' of interest:
>>
>>library(FinTS)
>>data(sp500)
>>library(fGarch)
>>spFit30.11 <- garchFit(sp500~arma(3,0)+garch(1,1), data=sp500)
>>
>>     This specifies an arma(3,0) mean model with garch(1,1) noise.
>>This syntax is buried in the 'garchFit' help page.
>>
>>     Hope this helps.
>>     Spencer
>>
>>tom soyer wrote:
>>    
>>
>>>Hi,
>>>
>>>I am new to GARCH and I am trying to figure out how to use R's garchSim
>>>      
>>>
>>and
>>    
>>
>>>garch, and I am a bit confused. I am hopeing that R finance experts can
>>>      
>>>
>>help
>>    
>>
>>>me understand them better. If we look at the definition of GARCH(1,1),
>>>there should be two equations:
>>>[1]: Y(t) = c + e(t), and
>>>[2]: sigma^2(t) = a0 + a1*e^2(t-1) + b1*sigma^2(t-1)
>>>
>>>So, I would expect any garch simulation function to four parameters: c,
>>>      
>>>
>>a0,
>>    
>>
>>>a1, and b1. But take a look at the garchSim, it has only three
>>>      
>>>
>>parameters:
>>    
>>
>>>model = list(omega = 1.0e-6, alpha = 0.1, beta = 0.8). I assume here
>>>      
>>>
>>that
>>    
>>
>>>omega = a0 in [2], alpha=a1, and beta=b1. If so, then it seems that in
>>>garchSim, c, the constant (or the mean) in [1], is always assumed to be
>>>zero. Does anyone know if this is true? I just want to make sure that I
>>>understand exactly what I should expect from the output of the garchSim
>>>function.
>>>
>>>Also, I have a similar question about garch. It seems that the
>>>      
>>>
>>coefficients
>>    
>>
>>>estimated by garch(x,order=c(1,1)) are a0, a1, and b1. Like garchSim,
>>>      
>>>
>>there
>>    
>>
>>>is no c, the mean. So does this mean garch also assumes zero mean and
>>>      
>>>
>>thus
>>    
>>
>>>actually fits model [2] instead of both [1] and [2]?
>>>
>>>Thanks!
>>>
>>>
>>>      
>>>
>
>
>
>  
>


From zeno.adams at vwl.uni-freiburg.de  Sun Feb  3 11:48:44 2008
From: zeno.adams at vwl.uni-freiburg.de (Zeno Adams)
Date: Sun, 03 Feb 2008 11:48:44 +0100
Subject: [R-SIG-Finance] Zivot vs. Engle vs. Stoffer - help with the meaning
 of different GARCH notations, please!
Message-ID: <web-96878097@uni-freiburg.de>


I think the equations can be regarded as equivalent. As Patrick pointed
out the main difference is if you write the mean equation in
standardized residuals or not.

<In Zivot's book, MFTSWS, the GARCH(1,1) process is defined as:
<[1]: Y(t) = c + e(t), and
<[2]: sigma^2(t) = a0 + a1*e^2(t-1) + b1*sigma^2(t-1)

e(t) can be written as sigma(t)*v(t) with v(t)~N(0,1) (often also
student's-t or GED)

<In Engle's paper, the GARCH(1,1) process is defined (in financial
<notation),
<like this:
<[3]: r(t) = m(t) + sqrt(h(t))*e(t), and
<[4]: h(t+1) = a0 + a1*h(t)*e^2(t) + b1*h(t)

here you have the more detailed notation with sqrt(h(t)) = sigma(t) and
e(t) ~N(0,1) =  v(t) 
and h(t)*e^2(t) = e^2(t)from Zivot's book
m(t)probably indicates that the mean is calculated as a rolling mean
and so thus changes over time.

<In Stoffer's book, the GARCH(1,1) is define as:
<[5]: Y(t) = sigma(t)*e(t), and
<[6]: sigma^2(t) = a0 + a1*Y^2(t-1) + b1*sigma^2(t-1)

here the constant is neglected in the mean equation as it is very small
for most daily data. Since Y(t) = sigma(t)*e(t) it now shows up as
a1*Y^2(t-1) in the variance equation.


Zeno


From tom.soyer at gmail.com  Sun Feb  3 14:37:18 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sun, 3 Feb 2008 07:37:18 -0600
Subject: [R-SIG-Finance] Zivot vs. Engle vs. Stoffer - help with the
	meaning of different GARCH notations, please!
In-Reply-To: <47A58A47.5030307@burns-stat.com>
References: <65cc7bdf0802021829q62c13915u8a074553b852be88@mail.gmail.com>
	<47A52FAD.3020401@braverock.com>
	<65cc7bdf0802021933j68108fbdpaac0b95bcfde1f6f@mail.gmail.com>
	<47A54579.9020802@braverock.com> <47A58A47.5030307@burns-stat.com>
Message-ID: <65cc7bdf0802030537u32e009dbp96b2807b569a348@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080203/b153131e/attachment.pl 

From tom.soyer at gmail.com  Sun Feb  3 15:11:38 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sun, 3 Feb 2008 08:11:38 -0600
Subject: [R-SIG-Finance] Zivot vs. Engle vs. Stoffer - help with the
	meaning of different GARCH notations, please!
In-Reply-To: <65cc7bdf0802030537u32e009dbp96b2807b569a348@mail.gmail.com>
References: <65cc7bdf0802021829q62c13915u8a074553b852be88@mail.gmail.com>
	<47A52FAD.3020401@braverock.com>
	<65cc7bdf0802021933j68108fbdpaac0b95bcfde1f6f@mail.gmail.com>
	<47A54579.9020802@braverock.com> <47A58A47.5030307@burns-stat.com>
	<65cc7bdf0802030537u32e009dbp96b2807b569a348@mail.gmail.com>
Message-ID: <65cc7bdf0802030611i6b953486y8157fad383709b54@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080203/c49f1577/attachment.pl 

From tom.soyer at gmail.com  Sun Feb  3 16:00:14 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sun, 3 Feb 2008 09:00:14 -0600
Subject: [R-SIG-Finance] Zivot vs. Engle vs. Stoffer - help with the
	meaning of different GARCH notations, please!
In-Reply-To: <65cc7bdf0802030611i6b953486y8157fad383709b54@mail.gmail.com>
References: <65cc7bdf0802021829q62c13915u8a074553b852be88@mail.gmail.com>
	<47A52FAD.3020401@braverock.com>
	<65cc7bdf0802021933j68108fbdpaac0b95bcfde1f6f@mail.gmail.com>
	<47A54579.9020802@braverock.com> <47A58A47.5030307@burns-stat.com>
	<65cc7bdf0802030537u32e009dbp96b2807b569a348@mail.gmail.com>
	<65cc7bdf0802030611i6b953486y8157fad383709b54@mail.gmail.com>
Message-ID: <65cc7bdf0802030700v670e5f57hce11d30e9f859f93@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080203/13cd7883/attachment.pl 

From tom.soyer at gmail.com  Sun Feb  3 16:18:31 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sun, 3 Feb 2008 09:18:31 -0600
Subject: [R-SIG-Finance] Question about garchSim and garch
In-Reply-To: <47A58B53.9050704@burns-stat.com>
References: <65cc7bdf0802020841s6f2005e3n3996194d88d42966@mail.gmail.com>
	<47A4A615.8050104@pdf.com>
	<65cc7bdf0802021500p444aeae0xf4ffc2eb46151d25@mail.gmail.com>
	<47A58B53.9050704@burns-stat.com>
Message-ID: <65cc7bdf0802030718r43fdd22ds28f914f886f0e695@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080203/b5f6f23e/attachment.pl 

From patrick at burns-stat.com  Sun Feb  3 16:55:39 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Sun, 03 Feb 2008 15:55:39 +0000
Subject: [R-SIG-Finance] Question about garchSim and garch
In-Reply-To: <65cc7bdf0802030718r43fdd22ds28f914f886f0e695@mail.gmail.com>
References: <65cc7bdf0802020841s6f2005e3n3996194d88d42966@mail.gmail.com>	
	<47A4A615.8050104@pdf.com>	
	<65cc7bdf0802021500p444aeae0xf4ffc2eb46151d25@mail.gmail.com>	
	<47A58B53.9050704@burns-stat.com>
	<65cc7bdf0802030718r43fdd22ds28f914f886f0e695@mail.gmail.com>
Message-ID: <47A5E3FB.7020500@burns-stat.com>

tom soyer wrote:

> Patrick, I am sorry maybe I didn't explain it well. I was thinking 
> using arma to estimate the mean, and garch for the conditional 
> variance. Does that make sense?


Yes, that makes sense, and that is what I was talking about:
it seems to be the case that estimating the mean model and
the conditional variance model separately tends to give you
a similar answer as estimating them both in a single procedure.

Pat

>  
> With regard to comparing models, do you, or anyone else know how to 
> build news impact curves in R?
>  
> Thanks!
>
>  
> On 2/3/08, *Patrick Burns* <patrick at burns-stat.com 
> <mailto:patrick at burns-stat.com>> wrote:
>
>     tom soyer wrote:
>
>     >Thnaks Spencer. I am glad I am not the only one that find garch
>     strange. I
>     >guess I will give up on it too. It seems that garchFit and
>     garchSim are very
>     >good. They have been giving me good results so far.
>     >
>     >Thanks for the tip on how to specify arma + garch model. I found
>     this paper
>     >also very
>     helpful:http://www.itp.phys.ethz.ch/econophysics/R/pdf/garch.pdf.
>     >
>     >Do you know how to specify arma + egarch model in R? Is it even
>     possible in
>     >R without installing Ox?
>     >
>     >
>
>     In my experience ARMA estimation and garch estimation are
>     suitably robust to each other.  It is definitely second prize to have
>     to estimate one and then the other, but your results are unlikely to
>     be all that different than if you did it "right".  (I'd love to
>     hear of
>     any counter-examples.)
>
>     Patrick Burns
>     patrick at burns-stat.com <mailto:patrick at burns-stat.com>
>     +44 (0)20 8525 0696
>     http://www.burns-stat.com
>     (home of S Poetry and "A Guide for the Unwilling S User")
>
>     >
>     >On 2/2/08, Spencer Graves <spencer.graves at pdf.com
>     <mailto:spencer.graves at pdf.com>> wrote:
>     >
>     >
>     >>Hi, Tom:
>     >>
>     >>     The file 'scripts\ch03.R' in the FinTS package includes a brief
>     >>description of attempts to use garch{tseries} and
>     garchFit{fGarch}.  I
>     >>don't understand either function very well, but I got answers from
>     >>'garchFit' that seemed to match some of the published results in
>     Tsay;
>     >>I gave up on 'garch'.
>     >>
>     >>     Since 'garchSim' and 'garchFit' are both in 'fGarch', I would
>     >>expect that it should be moderately easy to simulate something,
>     plot the
>     >>result, and see for yourself.  Chapter 3 of Tsay (2005) gives a
>     >>reasonable overview of GARCH and related models with several
>     examples.
>     >>The companion script\ch03.R is far from complete but might help.
>     >>
>     >>     You may find the following example from 'ch03.R' of interest:
>     >>
>     >>library(FinTS)
>     >>data(sp500)
>     >>library(fGarch)
>     >>spFit30.11 <- garchFit(sp500~arma(3,0)+garch(1,1), data=sp500)
>     >>
>     >>     This specifies an arma(3,0) mean model with garch(1,1) noise.
>     >>This syntax is buried in the 'garchFit' help page.
>     >>
>     >>     Hope this helps.
>     >>     Spencer
>     >>
>     >>tom soyer wrote:
>     >>
>     >>
>     >>>Hi,
>     >>>
>     >>>I am new to GARCH and I am trying to figure out how to use R's
>     garchSim
>     >>>
>     >>>
>     >>and
>     >>
>     >>
>     >>>garch, and I am a bit confused. I am hopeing that R finance
>     experts can
>     >>>
>     >>>
>     >>help
>     >>
>     >>
>     >>>me understand them better. If we look at the definition of
>     GARCH(1,1),
>     >>>there should be two equations:
>     >>>[1]: Y(t) = c + e(t), and
>     >>>[2]: sigma^2(t) = a0 + a1*e^2(t-1) + b1*sigma^2(t-1)
>     >>>
>     >>>So, I would expect any garch simulation function to four
>     parameters: c,
>     >>>
>     >>>
>     >>a0,
>     >>
>     >>
>     >>>a1, and b1. But take a look at the garchSim, it has only three
>     >>>
>     >>>
>     >>parameters:
>     >>
>     >>
>     >>>model = list(omega = 1.0e-6, alpha = 0.1, beta = 0.8). I assume
>     here
>     >>>
>     >>>
>     >>that
>     >>
>     >>
>     >>>omega = a0 in [2], alpha=a1, and beta=b1. If so, then it seems
>     that in
>     >>>garchSim, c, the constant (or the mean) in [1], is always
>     assumed to be
>     >>>zero. Does anyone know if this is true? I just want to make
>     sure that I
>     >>>understand exactly what I should expect from the output of the
>     garchSim
>     >>>function.
>     >>>
>     >>>Also, I have a similar question about garch. It seems that the
>     >>>
>     >>>
>     >>coefficients
>     >>
>     >>
>     >>>estimated by garch(x,order=c(1,1)) are a0, a1, and b1. Like
>     garchSim,
>     >>>
>     >>>
>     >>there
>     >>
>     >>
>     >>>is no c, the mean. So does this mean garch also assumes zero
>     mean and
>     >>>
>     >>>
>     >>thus
>     >>
>     >>
>     >>>actually fits model [2] instead of both [1] and [2]?
>     >>>
>     >>>Thanks!
>     >>>
>     >>>
>     >>>
>     >>>
>     >
>     >
>     >
>     >
>     >
>
>
>
>
> -- 
> Tom


From tom.soyer at gmail.com  Sun Feb  3 17:18:54 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sun, 3 Feb 2008 10:18:54 -0600
Subject: [R-SIG-Finance] Question about garchSim and garch
In-Reply-To: <47A5E3FB.7020500@burns-stat.com>
References: <65cc7bdf0802020841s6f2005e3n3996194d88d42966@mail.gmail.com>
	<47A4A615.8050104@pdf.com>
	<65cc7bdf0802021500p444aeae0xf4ffc2eb46151d25@mail.gmail.com>
	<47A58B53.9050704@burns-stat.com>
	<65cc7bdf0802030718r43fdd22ds28f914f886f0e695@mail.gmail.com>
	<47A5E3FB.7020500@burns-stat.com>
Message-ID: <65cc7bdf0802030818x573dbc07lb68357701788c105@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080203/1e43ef6a/attachment.pl 

From babel at centrum.sk  Sun Feb  3 17:51:00 2008
From: babel at centrum.sk (babel at centrum.sk)
Date: Sun, 03 Feb 2008 17:51:00 +0100
Subject: [R-SIG-Finance] Garch question
Message-ID: <200802031751.17742@centrum.cz>

1. Can I use garch model on price series or do I need to transform it to return,  for example ret<-diff(log(x))?
2. If yes, then how can I predict the future values, while I am working with return? 
3. library(fArma)   
  fit1 = armaFit(~ arma(1, 0), data = x)
  predict(fit1, 10)

 1.179176 1.179747 1.180312 1.180871 1.181425 1.181974 1.182517 1.183054   1.183586 1.184113


with Arma there is no problem with prediction. But how can I use ARMA to predict a mean and GARCH for variance?

fit = garchFit(~garch(1, 1), data =ret )     #if I use data=x the estimated coeficients are not significant
predict(fit,n.ahead=10)
 meanForecast meanError standardDeviation
1  -0.007308328 0.5299619         0.4586886
2  -0.007308328 0.5299619         0.4588551
....................................................................................
How can this output help me, to improve the result of ARMA forecasting? Should I add Garch standard deviation to ARMA prediction?
Or I can even use this formula>:

fit = garchFit(~arma(1,0,0)+garch(1, 1), data =ret)

 meanForecast meanError standardDeviation
1  -0.025711384 0.5292999         0.4589430
2  -0.006770741 0.5296301         0.4591042

but what to do with this? I expected values like in pure ARMA>>  1.179176 1.179747  .... ...  or can I somehow transform this return back into price time series?

Sorry for my english and poor statistical knowledge, I just dont understand what to do with garch output. I read that GARCH model gives better result in forecasting than ARMA, but I dont know how to get those future values. The values, that tells you something (price values) not the return series. Anyway, many thanks.

John


From spencer.graves at pdf.com  Sun Feb  3 17:53:04 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 03 Feb 2008 08:53:04 -0800
Subject: [R-SIG-Finance] Question about garchSim and garch
In-Reply-To: <65cc7bdf0802030818x573dbc07lb68357701788c105@mail.gmail.com>
References: <65cc7bdf0802020841s6f2005e3n3996194d88d42966@mail.gmail.com>	
	<47A4A615.8050104@pdf.com>	
	<65cc7bdf0802021500p444aeae0xf4ffc2eb46151d25@mail.gmail.com>	
	<47A58B53.9050704@burns-stat.com>	
	<65cc7bdf0802030718r43fdd22ds28f914f886f0e695@mail.gmail.com>	
	<47A5E3FB.7020500@burns-stat.com>
	<65cc7bdf0802030818x573dbc07lb68357701788c105@mail.gmail.com>
Message-ID: <47A5F170.40807@pdf.com>


      * Can 'garchFit' estimate the degrees of freedom for Student's t 
shocks? 

      It has arguments "include.skew, include.shape", but I don't know 
what they do (and haven't taken the time to read the code or ask 
Diethelm to find out). 

      This question relates to the "two-pass estimation" discussed in 
this thread, because Tsay (2005, sec. 3.5) fits arima(0,0,0)+garch(1,1) 
to monthly excess returns of the Standard and Poor's 500 from 1926 
through 1991 using (a) simultaneous estimation, (b) simultaneous 
estimation with Student's t shocks, and (c) two-pass estimation.  He 
gets similar answers. 

      The current 'script\ch03.R' includes estimating models 'a' and 'c' 
but not 'b'.  If someone can help me figure out how to estimate the 
degrees of freedom for Student's t shocks, I'd happily add that to 
'scripts\ch03.R'. 

      Best Wishes,
      Spencer

Spencer

tom soyer wrote:
> OK. Thanks Patrick. I was thinking about the same yesterday! That's 
> one of the reasons that I thought R's garch still has potential. I was 
> going to test your theory, but then I got distracted when I was unable 
> to reconcile the different GARCH notations. But now I am thinking, why 
> do it in multiple steps when you can just do everything at once? Do 
> you have any particular scenarios in mind?
>  
> Thanks!
>
>  
> On 2/3/08, *Patrick Burns* <patrick at burns-stat.com 
> <mailto:patrick at burns-stat.com>> wrote:
>
>     tom soyer wrote:
>
>     > Patrick, I am sorry maybe I didn't explain it well. I was thinking
>     > using arma to estimate the mean, and garch for the conditional
>     > variance. Does that make sense?
>
>
>     Yes, that makes sense, and that is what I was talking about:
>     it seems to be the case that estimating the mean model and
>     the conditional variance model separately tends to give you
>     a similar answer as estimating them both in a single procedure.
>
>     Pat
>
>     >
>     > With regard to comparing models, do you, or anyone else know how to
>     > build news impact curves in R?
>     >
>     > Thanks!
>     >
>     >
>     > On 2/3/08, *Patrick Burns* <patrick at burns-stat.com
>     <mailto:patrick at burns-stat.com>
>     > <mailto:patrick at burns-stat.com <mailto:patrick at burns-stat.com>>>
>     wrote:
>     >
>     >     tom soyer wrote:
>     >
>     >     >Thnaks Spencer. I am glad I am not the only one that find garch
>     >     strange. I
>     >     >guess I will give up on it too. It seems that garchFit and
>     >     garchSim are very
>     >     >good. They have been giving me good results so far.
>     >     >
>     >     >Thanks for the tip on how to specify arma + garch model. I
>     found
>     >     this paper
>     >     >also very
>     >    
>     helpful:http://www.itp.phys.ethz.ch/econophysics/R/pdf/garch.pdf.
>     >     >
>     >     >Do you know how to specify arma + egarch model in R? Is it even
>     >     possible in
>     >     >R without installing Ox?
>     >     >
>     >     >
>     >
>     >     In my experience ARMA estimation and garch estimation are
>     >     suitably robust to each other.  It is definitely second
>     prize to have
>     >     to estimate one and then the other, but your results are
>     unlikely to
>     >     be all that different than if you did it "right".  (I'd love to
>     >     hear of
>     >     any counter-examples.)
>     >
>     >     Patrick Burns
>     >     patrick at burns-stat.com <mailto:patrick at burns-stat.com>
>     <mailto:patrick at burns-stat.com <mailto:patrick at burns-stat.com>>
>     >     +44 (0)20 8525 0696
>     >     http://www.burns-stat.com
>     >     (home of S Poetry and "A Guide for the Unwilling S User")
>     >
>     >     >
>     >     >On 2/2/08, Spencer Graves <spencer.graves at pdf.com
>     <mailto:spencer.graves at pdf.com>
>     >     <mailto:spencer.graves at pdf.com
>     <mailto:spencer.graves at pdf.com>>> wrote:
>     >     >
>     >     >
>     >     >>Hi, Tom:
>     >     >>
>     >     >>     The file 'scripts\ch03.R' in the FinTS package
>     includes a brief
>     >     >>description of attempts to use garch{tseries} and
>     >     garchFit{fGarch}.  I
>     >     >>don't understand either function very well, but I got
>     answers from
>     >     >>'garchFit' that seemed to match some of the published
>     results in
>     >     Tsay;
>     >     >>I gave up on 'garch'.
>     >     >>
>     >     >>     Since 'garchSim' and 'garchFit' are both in 'fGarch',
>     I would
>     >     >>expect that it should be moderately easy to simulate
>     something,
>     >     plot the
>     >     >>result, and see for yourself.  Chapter 3 of Tsay (2005)
>     gives a
>     >     >>reasonable overview of GARCH and related models with several
>     >     examples.
>     >     >>The companion script\ch03.R is far from complete but might
>     help.
>     >     >>
>     >     >>     You may find the following example from 'ch03.R' of
>     interest:
>     >     >>
>     >     >>library(FinTS)
>     >     >>data(sp500)
>     >     >>library(fGarch)
>     >     >>spFit30.11 <- garchFit(sp500~arma(3,0)+garch(1,1), data=sp500)
>     >     >>
>     >     >>     This specifies an arma(3,0) mean model with
>     garch(1,1) noise.
>     >     >>This syntax is buried in the 'garchFit' help page.
>     >     >>
>     >     >>     Hope this helps.
>     >     >>     Spencer
>     >     >>
>     >     >>tom soyer wrote:
>     >     >>
>     >     >>
>     >     >>>Hi,
>     >     >>>
>     >     >>>I am new to GARCH and I am trying to figure out how to
>     use R's
>     >     garchSim
>     >     >>>
>     >     >>>
>     >     >>and
>     >     >>
>     >     >>
>     >     >>>garch, and I am a bit confused. I am hopeing that R finance
>     >     experts can
>     >     >>>
>     >     >>>
>     >     >>help
>     >     >>
>     >     >>
>     >     >>>me understand them better. If we look at the definition of
>     >     GARCH(1,1),
>     >     >>>there should be two equations:
>     >     >>>[1]: Y(t) = c + e(t), and
>     >     >>>[2]: sigma^2(t) = a0 + a1*e^2(t-1) + b1*sigma^2(t-1)
>     >     >>>
>     >     >>>So, I would expect any garch simulation function to four
>     >     parameters: c,
>     >     >>>
>     >     >>>
>     >     >>a0,
>     >     >>
>     >     >>
>     >     >>>a1, and b1. But take a look at the garchSim, it has only
>     three
>     >     >>>
>     >     >>>
>     >     >>parameters:
>     >     >>
>     >     >>
>     >     >>>model = list(omega = 1.0e-6, alpha = 0.1, beta = 0.8). I
>     assume
>     >     here
>     >     >>>
>     >     >>>
>     >     >>that
>     >     >>
>     >     >>
>     >     >>>omega = a0 in [2], alpha=a1, and beta=b1. If so, then it
>     seems
>     >     that in
>     >     >>>garchSim, c, the constant (or the mean) in [1], is always
>     >     assumed to be
>     >     >>>zero. Does anyone know if this is true? I just want to make
>     >     sure that I
>     >     >>>understand exactly what I should expect from the output
>     of the
>     >     garchSim
>     >     >>>function.
>     >     >>>
>     >     >>>Also, I have a similar question about garch. It seems
>     that the
>     >     >>>
>     >     >>>
>     >     >>coefficients
>     >     >>
>     >     >>
>     >     >>>estimated by garch(x,order=c(1,1)) are a0, a1, and b1. Like
>     >     garchSim,
>     >     >>>
>     >     >>>
>     >     >>there
>     >     >>
>     >     >>
>     >     >>>is no c, the mean. So does this mean garch also assumes zero
>     >     mean and
>     >     >>>
>     >     >>>
>     >     >>thus
>     >     >>
>     >     >>
>     >     >>>actually fits model [2] instead of both [1] and [2]?
>     >     >>>
>     >     >>>Thanks!
>     >     >>>
>     >     >>>
>     >     >>>
>     >     >>>
>     >     >
>     >     >
>     >     >
>     >     >
>     >     >
>     >
>     >
>     >
>     >
>     > --
>     > Tom
>
>
>
>
> -- 
> Tom


From spencer.graves at pdf.com  Sun Feb  3 21:19:12 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 03 Feb 2008 12:19:12 -0800
Subject: [R-SIG-Finance] Garch question
In-Reply-To: <200802031751.17742@centrum.cz>
References: <200802031751.17742@centrum.cz>
Message-ID: <47A621C0.4020006@pdf.com>

The whitened residuals are assumed to be serially uncorrelated.  
Therefore, sqrt(cumsum(estimated variance)) should give an estimate of 
the standard deviations of the predictions. 

In particular, consider the following: 

      library(fGarch)
      fit <- garchFit(diff(log(x))

      pred.dlx <- predict(fit) 

      pred.lx <- cumsum(pred.dlx[, 1])

      pred.slx <- sqrt(cumsum(pred.dlx[, 3]^2))

      pred.x <- exp(pred.lx)
      ul <- exp(pred.lx + 1.96*pred.slx)
      ll <- exp(pred.lx - 1.96*pred.slx)

EXAMPLE: 

      library(FinTS)
      library(fGarch)
      data(sp500)
      spFit30.11 <- garchFit(sp500~arma(3,0)+garch(1,1),
                       data=sp500)
      pred.spFit00.11 <- predict(spFit00.11)

      cumPred <- cumsum(pred.spFit00.11[, 1])
      cumPredS <- sqrt(cumsum(pred.spFit00.11[, 3]^2))

... see the discussion of Figure 2.15 in 
"~R\library\FinTS\scripts\ch02.R", where "~R" is your local R 
installation directory. 

      Hope this helps. 
      Spencer

babel at centrum.sk wrote:
> 1. Can I use garch model on price series or do I need to transform it to return,  for example ret<-diff(log(x))?
> 2. If yes, then how can I predict the future values, while I am working with return? 
> 3. library(fArma)   
>   fit1 = armaFit(~ arma(1, 0), data = x)
>   predict(fit1, 10)
>
>  1.179176 1.179747 1.180312 1.180871 1.181425 1.181974 1.182517 1.183054   1.183586 1.184113
>
>
> with Arma there is no problem with prediction. But how can I use ARMA to predict a mean and GARCH for variance?
>
> fit = garchFit(~garch(1, 1), data =ret )     #if I use data=x the estimated coeficients are not significant
> predict(fit,n.ahead=10)
>  meanForecast meanError standardDeviation
> 1  -0.007308328 0.5299619         0.4586886
> 2  -0.007308328 0.5299619         0.4588551
> ....................................................................................
> How can this output help me, to improve the result of ARMA forecasting? Should I add Garch standard deviation to ARMA prediction?
> Or I can even use this formula>:
>
> fit = garchFit(~arma(1,0,0)+garch(1, 1), data =ret)
>
>  meanForecast meanError standardDeviation
> 1  -0.025711384 0.5292999         0.4589430
> 2  -0.006770741 0.5296301         0.4591042
>
> but what to do with this? I expected values like in pure ARMA>>  1.179176 1.179747  .... ...  or can I somehow transform this return back into price time series?
>
> Sorry for my english and poor statistical knowledge, I just dont understand what to do with garch output. I read that GARCH model gives better result in forecasting than ARMA, but I dont know how to get those future values. The values, that tells you something (price values) not the return series. Anyway, many thanks.
>
> John
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From zeno.adams at vwl.uni-freiburg.de  Sun Feb  3 21:47:45 2008
From: zeno.adams at vwl.uni-freiburg.de (Zeno Adams)
Date: Sun, 03 Feb 2008 21:47:45 +0100
Subject: [R-SIG-Finance] Garch question
Message-ID: <web-96889633@uni-freiburg.de>


<1. Can I use garch model on price series or do I need to transform it
to <return, for example ret<-diff(log(x))?

You cannot use GARCH models on price series since they will not be
stationary in most cases. You will get the sum alpha + beta larger than
1 which results in a parameter violation since the long-run
unconditional variance = constant/1-alpha-beta will be negative.

<2. If yes, then how can I predict the future values

You know Pt. You forecast rt+1. You get Pt+1 = 1 + rt+1*Pt if you use
discrete returns or
Pt+1 = exp[rt+1] + Pt if you use continuously compounded returns, i.e.
ret<-diff(log(x)) as you suggested.

Zeno


From joe at gnacademy.org  Sun Feb  3 23:42:10 2008
From: joe at gnacademy.org (Joseph Wang)
Date: Sun, 3 Feb 2008 17:42:10 -0500
Subject: [R-SIG-Finance] R class accessor problem in 2.6
In-Reply-To: <18331.24145.498932.596612@ron.nulle.part>
References: <002101c85fbd$8a8c3040$0ac0a8c0@MightyMini>
	<479B1DCF.4060107@braverock.com>
	<18331.24145.498932.596612@ron.nulle.part>
Message-ID: <200802031742.11535.joe@gnacademy.org>

I was able to track down the problem that causes R-Swig to fail in 2.6.

In 2.5, I am able to use the set method against '$' and '$<-' to cause 
expressions like r$d to get dispatched to get and set functions.  This no 
longer seems to work in 2.6, and I was wondering if anyone can give 
information as why.

The function that are failing are at the end of this e-mail.  In 2.5 a 
function like r$d would get dispatched to A_d_get(r). but in 2.6 it returns a 
NULL.

# Start of accessor method for A
setMethod('$', '_p_A', function(x, name)

{
  print('foo')
  accessorFuns = list('i' = A_i_get, 'ui' = A_ui_get, 'd' = A_d_get, 'str' = 
A_str_get, 'tmp' = A_tmp_get)
  vaccessors = c('i', 'ui', 'd', 'str', 'tmp')
  idx = pmatch(name, names(accessorFuns))
  if(is.na(idx)) 
  return(callNextMethod(x, name))
  f = accessorFuns[[idx]]
  formals(f)[[1]] = x
  if (is.na(match(name, vaccessors))) f else f(x)
}


)
# end of accessor method for A
# Start of accessor method for A
setMethod('$<-', '_p_A', function(x, name, value)

{
  accessorFuns = list('i' = A_i_set, 'ui' = A_ui_set, 'd' = A_d_set, 'str' = 
A_str_set, 'tmp' = A_tmp_set)
  idx = pmatch(name, names(accessorFuns))
  if(is.na(idx)) 
  return(callNextMethod(x, name, value))
  f = accessorFuns[[idx]]
  f(x, value)
  x
}


From tom.soyer at gmail.com  Mon Feb  4 00:11:29 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sun, 3 Feb 2008 17:11:29 -0600
Subject: [R-SIG-Finance] garchFit - where are the fitted values for the
	variance part?
Message-ID: <65cc7bdf0802031511n5e9f2143p5e67ca9a3413c3ac@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080203/bfbcafe8/attachment.pl 

From tom.soyer at gmail.com  Mon Feb  4 01:51:55 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sun, 3 Feb 2008 18:51:55 -0600
Subject: [R-SIG-Finance] garchFit - where are the fitted values for the
	variance part?
In-Reply-To: <65cc7bdf0802031511n5e9f2143p5e67ca9a3413c3ac@mail.gmail.com>
References: <65cc7bdf0802031511n5e9f2143p5e67ca9a3413c3ac@mail.gmail.com>
Message-ID: <65cc7bdf0802031651o78e1aae7s6457096b859c4626@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080203/a9892a79/attachment.pl 

From spencer.graves at pdf.com  Mon Feb  4 02:47:00 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 03 Feb 2008 17:47:00 -0800
Subject: [R-SIG-Finance] garchFit - where are the fitted values for the
 variance part?
In-Reply-To: <65cc7bdf0802031651o78e1aae7s6457096b859c4626@mail.gmail.com>
References: <65cc7bdf0802031511n5e9f2143p5e67ca9a3413c3ac@mail.gmail.com>
	<65cc7bdf0802031651o78e1aae7s6457096b859c4626@mail.gmail.com>
Message-ID: <47A66E94.3020505@pdf.com>

The help page for garchFit says "@h.t :  " a numeric vector with the 
conditional variances."  It says the same thing for "@sigma.t", but 
that's clearly a typo:  sigma.t = the conditional standard deviation, as 
indicated by the following check: 

library(FinTS)
library(fGarch)
data(sp500)
spFit30.11 <- garchFit(sp500~arma(3,0)+garch(1,1),
                       data=sp500)
all.equal(spFit30.11 at h.t, spFit30.11 at sigma.t^2)

TRUE

      hope this helps.
      spencer

tom soyer wrote:
> Nevermind, I found it... sigma.t instead of fitted. What is h.t??
>
> On 2/3/08, tom soyer <tom.soyer at gmail.com> wrote:
>   
>> Hi,
>>
>> I noticed that garchFit provides the fitted mean (i.e., a long repeat of
>> one single value), but it seems there is no fitted values for the variance
>> part. Are they not calculated at all? If so, how could I get the fitted
>> values of the variance? Do I have to manually type the coefficients and
>> write an equation in order to get the fitted values?
>>
>> Thanks,
>>
>> --
>> Tom
>>
>>     
>
>
>
>


From tom.soyer at gmail.com  Mon Feb  4 04:10:51 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sun, 3 Feb 2008 21:10:51 -0600
Subject: [R-SIG-Finance] garchFit - where are the fitted values for the
	variance part?
In-Reply-To: <47A66E94.3020505@pdf.com>
References: <65cc7bdf0802031511n5e9f2143p5e67ca9a3413c3ac@mail.gmail.com>
	<65cc7bdf0802031651o78e1aae7s6457096b859c4626@mail.gmail.com>
	<47A66E94.3020505@pdf.com>
Message-ID: <65cc7bdf0802031910w706eb5ffg1767cbf4559c1dff@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080203/c991bc13/attachment.pl 

From chalabi at phys.ethz.ch  Mon Feb  4 11:51:53 2008
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Mon, 4 Feb 2008 11:51:53 +0100
Subject: [R-SIG-Finance] holidayNYSE missing some
Message-ID: <20080204115153.13a247f4@yankee-laptop>

Hi John,

for whatever reason I missed this thread in September 2007. 
Thanks to Lukas Elmiger how pointed out directly to me this problem, I
made the following changes in the development branch of fCalendar
(https://svn.r-project.org/Rmetrics/trunk/fCalendar/):

- fixed the rule in holidayNYSE : After July 3, 1959, move Saturday
holidays to Friday ... except if at the end of monthly/yearly accounting  
period this is the last business day of a month. 

- fixed the conditions for the year 1970 in holidayNYSE

- fixed USWashingtonsBirthday() before 1971

Regards,
Yohan


From MichelBeck at sbcglobal.Net  Mon Feb  4 18:35:51 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Mon, 4 Feb 2008 17:35:51 +0000 (UTC)
Subject: [R-SIG-Finance] Zoo functions
References: <loom.20080201T202320-56@post.gmane.org>
	<18339.35090.314737.735789@ron.nulle.part>
Message-ID: <loom.20080204T172959-956@post.gmane.org>

Dirk Eddelbuettel <edd <at> debian.org> writes:

> 
> 
> On 1 February 2008 at 20:29, MAB wrote:
> | 1) The documentation for the zoo package indicates a function lag.zoo .
> | I can't find it when I try to use it or do a help(lag.zoo). I can find all 
the 
> | other zoo functions I tried so far (allthough diff.zoo is missing also).
> | 
> | Any hints?

Hi Dirk!

So do I understand correctly that I should not attempt to use a function 
lag.zoo but should use the function lag after loading the zoo library?

This is what I did:

1) I reinstalled zoo
2) I loaded zoo
3) I typed in the commands


library(zoo)
test <- zoo(1:10)
lag.zoo(test)

and got

Error: could not find function "lag.zoo"

SO I should type in lag only to get the proper result?


Thanks,

Michel

PS Indeed I have always been getting the help pages for lag.zoo and diff.zoo


From MichelBeck at sbcglobal.Net  Mon Feb  4 20:00:17 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Mon, 4 Feb 2008 19:00:17 +0000 (UTC)
Subject: [R-SIG-Finance] Importing from Excel
Message-ID: <loom.20080204T185009-74@post.gmane.org>

Hi!

I am trying to diff data imported from Excel.
I use the package xlsReadWrite.

After I load the following XL spreadsheet

C_Dates C_Price C_Prices_edit         C_Return_Raw        C_Return_edit
1   30405   29.40         29.40                 #N/A                 #N/A
2   30406   29.29         NaN                 -0.003               -0.003
3   30407      NA         29.29                    0                    0
(#N/A is a string in one case and the formula =NA() in another)

and try to diff the resulting object, I get:

Error in r[i1, , drop = FALSE] - r[-nrow(r):-(nrow(r) - lag + 1), , drop = 
FALSE] : 
        non-numeric argument to binary operator

I attempt to convert to numeric using as.matrix (or data.matrix),
but this converts the object to all characters.

I can then use as.numeric to get a vector and rebuild the matrix but this gets 
tedious.

Eventually the easiest seems to be to make sure each column in the spreadsheet
starts with a numeric, and replace it once the object is in R.

There is probably a better way.

Michel


From edd at debian.org  Mon Feb  4 20:24:01 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 4 Feb 2008 13:24:01 -0600
Subject: [R-SIG-Finance] Zoo functions
In-Reply-To: <loom.20080204T172959-956@post.gmane.org>
References: <loom.20080201T202320-56@post.gmane.org>
	<18339.35090.314737.735789@ron.nulle.part>
	<loom.20080204T172959-956@post.gmane.org>
Message-ID: <18343.26193.60444.707708@ron.nulle.part>


On 4 February 2008 at 17:35, MAB wrote:
| So do I understand correctly that I should not attempt to use a function 
| lag.zoo but should use the function lag after loading the zoo library?

Correct:

> zoo(1:4, order.by=Sys.Date()+0:3)
2008-02-04 2008-02-05 2008-02-06 2008-02-07 
         1          2          3          4 
> zz <- zoo(1:4, order.by=Sys.Date()+0:3)
> zz
2008-02-04 2008-02-05 2008-02-06 2008-02-07 
         1          2          3          4 
> lag(zz)
2008-02-04 2008-02-05 2008-02-06 
         2          3          4 
> lag(zz, -1)
2008-02-05 2008-02-06 2008-02-07 
         1          2          3 
> 

Dirk

-- 
Three out of two people have difficulties with fractions.


From edd at debian.org  Mon Feb  4 20:25:41 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 4 Feb 2008 13:25:41 -0600
Subject: [R-SIG-Finance] Importing from Excel
In-Reply-To: <loom.20080204T185009-74@post.gmane.org>
References: <loom.20080204T185009-74@post.gmane.org>
Message-ID: <18343.26293.790683.294913@ron.nulle.part>


On 4 February 2008 at 19:00, MAB wrote:
| I am trying to diff data imported from Excel.

Did you read the discussion in the 'R Data Import/Export' manual?
See http://cran.r-project.org/doc/manuals/R-data.html

Dirk

-- 
Three out of two people have difficulties with fractions.


From brian at braverock.com  Mon Feb  4 20:36:38 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 04 Feb 2008 13:36:38 -0600
Subject: [R-SIG-Finance] Importing from Excel
In-Reply-To: <loom.20080204T185009-74@post.gmane.org>
References: <loom.20080204T185009-74@post.gmane.org>
Message-ID: <47A76946.8050503@braverock.com>

MAB wrote:
> Eventually the easiest seems to be to make sure each column in the spreadsheet
> starts with a numeric, and replace it once the object is in R.
> 
> There is probably a better way.

I tend to export to CSV files, and read.table or read.csv those into R. 
  That seems to be the cleanest way, and is well-supported in R.

One note is that if you have both row and column names, you need to 
remove the first column from your header row in the CSV, or R gets confused.

Regards,

   - Brian


From MichelBeck at sbcglobal.Net  Mon Feb  4 20:37:46 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Mon, 4 Feb 2008 19:37:46 +0000 (UTC)
Subject: [R-SIG-Finance] Importing from Excel
References: <loom.20080204T185009-74@post.gmane.org>
	<18343.26293.790683.294913@ron.nulle.part>
Message-ID: <loom.20080204T193639-609@post.gmane.org>

Dirk Eddelbuettel <edd <at> debian.org> writes:

> 
> 
> On 4 February 2008 at 19:00, MAB wrote:
> | I am trying to diff data imported from Excel.
> 
> Did you read the discussion in the 'R Data Import/Export' manual?
> See http://cran.r-project.org/doc/manuals/R-data.html
> 
> Dirk
> 
Hi!

Yes. It recommends not to read from Excel at all which is not a practical 
option in my case.

Michel


From spencer.graves at pdf.com  Mon Feb  4 20:57:09 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 04 Feb 2008 11:57:09 -0800
Subject: [R-SIG-Finance] Importing from Excel
In-Reply-To: <loom.20080204T185009-74@post.gmane.org>
References: <loom.20080204T185009-74@post.gmane.org>
Message-ID: <47A76E15.3070705@pdf.com>

      I've had good luck with 'read.xls{gdata}', but that could have the 
same problem. 

      If this still gives you problems, you could put your favorite 
'read.*(...)' in a function that tests columns you specify to see if 
they are numeric and if not tries to replace them with

           is.numeric(as.character(DF[[i]]))

      for column i of data.frame DF.  You could test that to see what 
percent NAs you have and write a warning, etc., whatever you want. 

      hope this helps. 
      Spencer

MAB wrote:
> Hi!
>
> I am trying to diff data imported from Excel.
> I use the package xlsReadWrite.
>
> After I load the following XL spreadsheet
>
> C_Dates C_Price C_Prices_edit         C_Return_Raw        C_Return_edit
> 1   30405   29.40         29.40                 #N/A                 #N/A
> 2   30406   29.29         NaN                 -0.003               -0.003
> 3   30407      NA         29.29                    0                    0
> (#N/A is a string in one case and the formula =NA() in another)
>
> and try to diff the resulting object, I get:
>
> Error in r[i1, , drop = FALSE] - r[-nrow(r):-(nrow(r) - lag + 1), , drop = 
> FALSE] : 
>         non-numeric argument to binary operator
>
> I attempt to convert to numeric using as.matrix (or data.matrix),
> but this converts the object to all characters.
>
> I can then use as.numeric to get a vector and rebuild the matrix but this gets 
> tedious.
>
> Eventually the easiest seems to be to make sure each column in the spreadsheet
> starts with a numeric, and replace it once the object is in R.
>
> There is probably a better way.
>
> Michel
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Mon Feb  4 21:14:23 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 4 Feb 2008 15:14:23 -0500
Subject: [R-SIG-Finance] Importing from Excel
In-Reply-To: <47A76E15.3070705@pdf.com>
References: <loom.20080204T185009-74@post.gmane.org> <47A76E15.3070705@pdf.com>
Message-ID: <971536df0802041214y222218f7g68290dea4d4818dc@mail.gmail.com>

Regarding read.xls in gdata note that it passes the ... to read.csv so you
could specify na.strings= which should avoid the problem
if its just due to the #NA strings.

On Mon, Feb 4, 2008 at 2:57 PM, Spencer Graves <spencer.graves at pdf.com> wrote:
>      I've had good luck with 'read.xls{gdata}', but that could have the
> same problem.
>
>      If this still gives you problems, you could put your favorite
> 'read.*(...)' in a function that tests columns you specify to see if
> they are numeric and if not tries to replace them with
>
>           is.numeric(as.character(DF[[i]]))
>
>      for column i of data.frame DF.  You could test that to see what
> percent NAs you have and write a warning, etc., whatever you want.
>
>      hope this helps.
>      Spencer
>
>
> MAB wrote:
> > Hi!
> >
> > I am trying to diff data imported from Excel.
> > I use the package xlsReadWrite.
> >
> > After I load the following XL spreadsheet
> >
> > C_Dates C_Price C_Prices_edit         C_Return_Raw        C_Return_edit
> > 1   30405   29.40         29.40                 #N/A                 #N/A
> > 2   30406   29.29         NaN                 -0.003               -0.003
> > 3   30407      NA         29.29                    0                    0
> > (#N/A is a string in one case and the formula =NA() in another)
> >
> > and try to diff the resulting object, I get:
> >
> > Error in r[i1, , drop = FALSE] - r[-nrow(r):-(nrow(r) - lag + 1), , drop =
> > FALSE] :
> >         non-numeric argument to binary operator
> >
> > I attempt to convert to numeric using as.matrix (or data.matrix),
> > but this converts the object to all characters.
> >
> > I can then use as.numeric to get a vector and rebuild the matrix but this gets
> > tedious.
> >
> > Eventually the easiest seems to be to make sure each column in the spreadsheet
> > starts with a numeric, and replace it once the object is in R.
> >
> > There is probably a better way.
> >
> > Michel
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From guillaume.nicoulaud at halbis.com  Tue Feb  5 08:33:06 2008
From: guillaume.nicoulaud at halbis.com (guillaume.nicoulaud at halbis.com)
Date: Tue, 5 Feb 2008 08:33:06 +0100
Subject: [R-SIG-Finance] =?iso-8859-15?q?R=E9f=2E_=3A_Re=3A__Importing_fro?=
	=?iso-8859-15?q?m_Excel?=
Message-ID: <OFC4EB1E3E.E1C54AAE-ONC12573E6.00293B4C-C12573E6.00297BA5@hsbc.fr>

All,

Here my modest contribution (not sure it will actually solve the issue).
Comments are welcome !

Rgds

# ############################################################################
# RXLS
# ############################################################################

# Description
# Functions to manipulate Excel from R.

# Author:        Guillaume Nicoulaud
# Date:          2008-02-05
# Version:       2
# Requirements:  rcom

# Contents
# xlStart --------- Creates a COM object against Excel's API
# xlIsRunning ----- Checks if Excel is running
# xlStop ---------- Removes the COM object (does not always shut Excel down)
# xlVisible ------- Set or get Excel's 'Visible' property
# xlAddWorkbook --- Add a new workbook
# xlIsWorkbook ---- Checks if an R object refers to a workbook
# xlCloseWorkbook - Closes a workbook
# xlOpenWorkbook -- Opens a workbook
# xlPutArray ------ Writes an R array in Excel
# xlRange --------- Points to a range in Excel
# xlGetArray ------ Retrieves an Excel array in R
# xlClearContents - Clears the contents of an Excel range
# xlPutTable ------ Exports a nice table in Excel
# .RXLS ----------- Contents

# ----------------------------------------------------------------------------
# xlStart
# ----------------------------------------------------------------------------

# Description
# Creates a COM object against Excel's API

# Arguments
# None

# Details
# Returns TRUE on success

xlStart = function() {
      if( ! "rcom" %in% .packages() ) library("rcom")
      xl <<- comCreateObject("Excel.Application")
      xlIsRunning() -> ans
      return(ans)
}

# ----------------------------------------------------------------------------
# xlIsRunning
# ----------------------------------------------------------------------------

# Description
# Checks if Excel is running

# Arguments
# None

# Details
# Returns TRUE on success

xlIsRunning = function() {
      ans = FALSE
      if( "xl" %in% ls(.GlobalEnv) & "rcom" %in% .packages() ) {
            if( comIsValidHandle(xl) ) {
                  xl[["Application"]][["Name"]] -> x
                  if( ! is.null(x) ) ans <- x == "Microsoft Excel"
            }
      }
      return(ans)
}

# ----------------------------------------------------------------------------
# xlStop
# ----------------------------------------------------------------------------

# Description
# Removes the COM object (does not always shut Excel down)

# Arguments
# None

# Details
# TRUE means that the COM object has been removed but Excel may still be
# running (even invisibly).

xlStop = function() {
      if( ! xlIsRunning() ) stop("Excel is not running. Use xlStart().")
      if( xlVisible() ) xlVisible(FALSE) -> silent
      xl <<- NULL
      rm( list = "xl", envir = .GlobalEnv )
      gc( verbose = FALSE )
      gc( verbose = FALSE )
      ! xlIsRunning() -> ans
      return(ans)
}

# ----------------------------------------------------------------------------
# xlVisible
# ----------------------------------------------------------------------------

# Description
# Set or get Excel's 'Visible' property

# Arguments
# v --------------- Logical. If NA (the default) returns the current status

# Details
# Returns the status of the 'Visible' property (TRUE or FALSE)

xlVisible = function(v = NA) {
      if( ! xlIsRunning() ) stop("Excel is not running. Use xlStart().")
      if( is.na(v) ) xl[["Visible"]] -> v
      if( ! is.logical(v) ) stop("v must be either TRUE or FALSE")
      xl[["Visible"]] <- v
      xl[["Visible"]] -> ans
      return(ans)
}


# ----------------------------------------------------------------------------
# xlAddWorkbook
# ----------------------------------------------------------------------------

# Description
# Add a new workbook

# Arguments
# path ------------ Full path to the new workbook to be created
# sheets ---------- A vector of sheet names (defaults to Sheet1... Sheet5)

# Details
# Returns an 'xlWorkbook' object - e.g. a list which first element is the
# COM object representing the workbook itself and each of the followings
# beein pointers to the sheets.

xlAddWorkbook = function(path, sheets = paste("Sheet", 1:5, sep ="") ) {
      if( ! xlIsRunning() )
            stop("Excel is not running. Use xlStart().")
      if( ! is.character(sheets) )
            stop("sheets must be a vector of names")

      length(sheets) -> n
      xl[["SheetsInNewWorkbook"]] -> save
      xl[["SheetsInNewWorkbook"]] <- n

      xl[["Workbooks"]]$Add() -> wb
      lapply(1:n, function(i) {
            wb[["Worksheets", i]] -> ws
            ws[["Name"]] <- sheets[i]
            return(ws)
            ws <- NULL
      } ) -> Sheets
      names(Sheets) <- sheets

      xl[["SheetsInNewWorkbook"]] <- save

      wb$SaveAs( normalizePath(path) ) -> ans
      if(!ans) stop("could not create ", sQuote(path))

      c( list(wb = wb), Sheets ) -> res
      attr(res, "class") <- "xlWorkbook"
      wb <- NULL

      return(res)
}

# ----------------------------------------------------------------------------
# xlIsWorkbook
# ----------------------------------------------------------------------------

# Description
# Checks if an R object refers to a workbook

# Arguments
# wb -------------- An R object

# Details
# Returns TRUE on success.

xlIsWorkbook = function(wb) {
      if( ! xlIsRunning() )
            stop("Excel is not running. Use xlStart().")
      class(wb) == "xlWorkbook" -> ans
      wb <- NULL
      return(ans)
}

# ----------------------------------------------------------------------------
# xlCloseWorkbook
# ----------------------------------------------------------------------------

# Description
# Closes a workbook

# Arguments
# wb -------------- An R object
# save ------------ Logical. Defaults to TRUE.

# Details
# Returns TRUE on success. Will also remove the xlWorkbook object.

xlCloseWorkbook = function(wb, save = TRUE) {
      if( ! xlIsRunning() )
            stop("Excel is not running. Use xlStart().")
      if( ! xlIsWorkbook(wb) )
            stop("wb is not a workbook.")
      wb[["wb"]]$Close(save) -> ans
      rm(wb, envir = .GlobalEnv)
      wb <- NULL
      return(ans)
}

# ----------------------------------------------------------------------------
# xlOpenWorkbook
# ----------------------------------------------------------------------------

# Description
# Opens a workbook

# Arguments
# path ------------ Full path to the workbook to be openned

# Details
# Returns an 'xlWorkbook' object - e.g. a list which first element is the
# COM object representing the workbook itself and each of the followings
# beein pointers to the sheets.

xlOpenWorkbook = function(path) {
      if( ! xlIsRunning() )
            stop("Excel is not running. Use xlStart().")

      xl[["Workbooks"]]$Open( normalizePath(path) ) -> wb
      wb[["Sheets"]][["Count"]] -> n
      lapply(1:n, function(i) wb[["Worksheets", i]]) -> Sheets
      sapply(Sheets, function(x) x[["Name"]]) -> names(Sheets)

      c( list(wb = wb), Sheets ) -> res
      attr(res, "class") <- "xlWorkbook"
      wb <- Sheets <- NULL

      return(res)
}

# ----------------------------------------------------------------------------
# xlPutArray
# ----------------------------------------------------------------------------

# Description
# Writes an R array in Excel

# Arguments
# x --------------- An R array or vector
# ws -------------- A pointer to the worksheet where to write the data
# cs -------------- Upper-left cell's reference as a numeric of length 2
# rownames -------- Logical. Should the function export row names?
# colnames -------- Logical. Should the function export column names?
# date.format ----- A string representing user's prefered Excel date format

# Details
# In this function, cs only refers to the upper-left cell of the range where
# to export data.
# The function will perform date conversion automatically provided dates are
# given as standard R Date objects.

xlPutArray = function(x, ws, cs, rownames = FALSE, colnames = FALSE,
      date.format = "aaaa-mm-jj") {
      if( ! xlIsRunning() )
            stop("Excel is not running. Use xlStart().")

      if( ! is.numeric(cs[1]) | ! is.numeric(cs[2]) | length(cs) != 2 )
            stop("cs must be a numeric vector of length 2")

      if( is.null( dim(x) ) ) as.matrix(x) -> x
      nrow(x) -> nrx
      ncol(x) -> ncx

      if(rownames) data.frame(x = rownames(x), x) -> x

      if(colnames) {

            ws[["Range", ws[["Cells", cs[1], cs[2]]],
                  ws[["Cells", cs[1], cs[2] + ncx - 1 ]] ]] -> rr
            temp <- if( is.null(colnames(x)) ) {
                  paste("c", 1:ncx, sep = "")
                  } else { colnames(x) }
            rr[["Value2"]] <- t(temp)
            cs[1] + 1 -> cs[1]
            rr <- NULL

      }

      lapply(1:ncx, function(i) {

            x[, i] -> xi
            ws[["Range", ws[["Cells", cs[1], cs[2] + i - 1 ]],
                  ws[["Cells", cs[1] + nrx - 1, cs[2] + i - 1 ]] ]] -> rr

            class(xi) -> case
            if( case != "numeric" ) {

                  if( case == "Date") {
                        julian(xi, origin = as.Date("1899-12-30")) -> xx
                        rr[["Value2"]] <- as.matrix(xx)
                        rr[["NumberFormat"]] <- date.format
                  }

                  if( case %in% c("character", "factor") ) {
                        rr[["Value2"]] <- as.matrix( as.character(xi) )
                  }

            } else {
                  rr[["Value2"]] <- as.matrix(xi)
            }

            rr <- NULL

      } ) -> silent

      ws <- NULL
      length(silent) == ncx -> ans
      return(ans)
}

# ----------------------------------------------------------------------------
# xlRange
# ----------------------------------------------------------------------------

# Description
# Points to a range in Excel

# Arguments
# ws -------------- A pointer to the worksheet
# cs -------------- Range's references as list of length 2 (see details)
# xlDown ---------- Logical. Should non-empty cells below cs be included ?
# xlToRight ------- Logical. Should non-empty cells next to cs be included ?
# check ----------- Logical. If TRUE returns details on the Excel range

# Details
# The cs argument must be a list of 2 numerical vectors of length 2 giving
# the coordinates of the range to be defined:
# cs = list( upper.left = c(row, col), lower.right = c(row, col) )
# With upper.left being the coordinates (row, col) of the upper-left cell
# of the Excel range and lower.right, the coordinates of the lower-right cell
# of the range. For example cs = list(c(1, 2), c(3, 5)) refers to the A2:C5
# range.
# The function will only xlDown (resp xlToRight) from a horizontal (resp
# vertical) range.

xlRange = function(ws, cs, xlDown = FALSE, xlToRight = FALSE, check = FALSE) {
      if( ! xlIsRunning() )
            stop("Excel is not running. Use xlStart().")

      if( ! is.list(cs) ) {

            if( length(cs) != 2 ) stop("cs must be of length 2")

            ws[["Cells", cs[1], cs[2] ]] -> rr

            if( ! xlDown & ! xlToRight ) rr -> res

            if( xlDown & ! xlToRight ) {
                  ws[["Range", rr, rr[["End", -4121]] ]] -> res
            }

            if( !xlDown & xlToRight ) {
                  ws[["Range", rr, rr[["End", -4161]] ]] -> res
            }

            if( xlDown & xlToRight ) {
                  ws[["Range", rr,
                        rr[["End", -4121]][["End", -4161]] ]] -> res
            }

            rr <- NULL

      } else {

            if( any( sapply(cs, length) != 2 ) )
                  stop("elements of cs must be of length 2.")

            ws[["Range", ws[["Cells", cs[[1]][1], cs[[1]][2] ]],
                  ws[["Cells", cs[[2]][1], cs[[2]][2] ]] ]] -> rr

            rr[["Columns"]][["Count"]] -> nc
            rr[["Rows"]][["Count"]] -> nr

            if( ! xlDown & ! xlToRight ) rr -> res

            if( xlDown & ! xlToRight ) {
                  if( nr != 1 & xlDown )
                    stop("will not xlDown from an horizontal range")
                  ws[["Range", rr, rr[["End", -4121]] ]] -> res
            }

            if( ! xlDown & xlToRight ) {
                  if( nc != 1 )
                    stop("will not xlToRight from a vertical range")
                  ws[["Range", rr, rr[["End", -4161]] ]] -> res
            }

            if( xlDown & xlToRight ) {
                  if( nr != 1 | nc != 1 )
                    stop("will only xlDown & xlToRight from a single cell")
                  ws[["Range", rr,
                        rr[["End", -4121]][["End", -4161]] ]] -> res
            }

            rr <- NULL
      }

      if(check) list(   Pointer = res,
                        Adress = paste(ws[["Name"]], res[["Address"]],
                              sep = "!"),
                        Rows = res[["Rows"]][["Count"]],
                        Columns = res[["Columns"]][["Count"]] ) -> res

      ws <- NULL
      return(res)
}

# ----------------------------------------------------------------------------
# xlGetArray
# ----------------------------------------------------------------------------

# Description
# Retrieves an array from Excel

# Arguments
# ws -------------- A pointer to the worksheet
# cs -------------- A pointer to the range to be exported (see details)
# xlDown ---------- Logical. Should non-empty cells below cs be exported ?
# xlToRight ------- Logical. Should non-empty cells next to cs be exported ?
# rownames -------- Logical. Does the first column contain row names ?
# colnames -------- Logical. Does the first row contain column names ?
# date.format ----- Excel format to be interpreted as dates

# Details
# Arguments work pretty much the same than in xlRange except that cs may be a
# numeric vector of length 2 refering to a unique cell.
# The date.format argument allows the function to identify and convert dates.
# It should be given as a character string representing a date format using
# your local Excel convention (you may wish to re-set the default to
# "yyyy-mm-dd").

# ToDos
# The function fails when cells in the same column have different formats.

xlGetArray = function(ws, cs, xlDown = FALSE, xlToRight = FALSE,
      rownames = FALSE, colnames = FALSE, date.format = "aaaa-mm-jj") {
      if( ! xlIsRunning() )
            stop("Excel is not running. Use xlStart().")

      if( ! is.list(cs) ) list(cs, cs) -> cs
      xlRange(ws, cs, xlDown, xlToRight) -> rr
      rr[["Columns"]][["Count"]] -> ncol
      rr[["Rows"]][["Count"]] -> nrow

      cs[[2]] <- c(cs[[1]][1] + nrow - 1, cs[[1]][2] + ncol - 1)

      if(colnames) {
            if( nrow < 2 )
                  stop("can't extract colnames with less than 2 rows")
            cs -> cs.c
            cs.c[[2]][1] <- cs.c[[1]][1]
            xlRange(ws, cs.c) -> rr
            drop( unlist( rr[["Value2"]] ) ) -> cnames
            if( is.null( rr[["NumberFormat"]] ) ) {
                  rr[["NumberFormat"]] <- "Standard"
            }
            if( rr[["NumberFormat"]] %in% date.format ) {
                  as.character( as.Date("1899-12-30") + cnames ) -> cnames
            }
            if(rownames) cnames[2:length(cnames)] -> cnames
            cs[[1]][1] <- cs[[1]][1] + 1
            rr <- NULL
      }

      if(rownames) {
            if( ncol < 2 )
                  stop("can't extract rownames with less than 2 columns")
            cs -> cs.r
            cs.r[[2]][2] <- cs.r[[1]][2]
            xlRange(ws, cs.r) -> rr
            drop( unlist( rr[["Value2"]] ) ) -> rnames
            if( rr[["NumberFormat"]] %in% date.format ) {
                  as.character( as.Date("1899-12-30") + rnames ) -> rnames
            }
            cs[[1]][2] <- cs[[1]][2] + 1
            rr <- NULL
      }

      lapply(cs[[1]][2]:cs[[2]][2], function(i) {
            cs.d <- cs
            cs.d[[1]][2] <- cs.d[[2]][2] <- i
            xlRange(ws, cs.d) -> rr
            drop( unlist( rr[["Value2"]] ) ) -> out
            if( rr[["NumberFormat"]] %in% date.format ) {
                  as.Date("1899-12-30") + out -> out
            }
            rr <- NULL
            return(out)
      } ) -> out
      data.frame(out) -> res

      colnames(res) <- if(colnames) { cnames
            } else { paste("c", 1:ncol(res), sep = "") }
      rownames(res) <- if(rownames) rnames

      ws <- NULL
      return(res)
}

# ----------------------------------------------------------------------------
# xlClearContents
# ----------------------------------------------------------------------------

# Description
# Clears the content of a range

# Arguments
# ws -------------- A pointer to the worksheet
# cs -------------- Range's references as list of length 2 (see details)
# xlDown ---------- Logical. Should non-empty cells below cs be included?
# xlToRight ------- Logical. Should non-empty cells next to cs be included?
# format ---------- Logical. Should formats be removed too?

# Details
# See xlRange for more information.

xlClearContents = function(ws, cs, xlDown = FALSE, xlToRight = FALSE,
      format = FALSE) {
      if( ! xlIsRunning() )
            stop("Excel is not running. Use xlStart().")

      if( is.null(cs) ) {
            ws[["Cells"]] -> rr
      } else {
            xlRange(ws, cs, xlDown, xlToRight) -> rr
      }

      rr$ClearContents() -> ans
      if(format) rr$ClearFormats() -> ans2; ans & ans2 -> ans

      ws <- rr <- NULL
      return(ans)
}

# ----------------------------------------------------------------------------
# xlPutTable
# ----------------------------------------------------------------------------

# Description
# Exports a formated table to Excel.

# Arguments
# x --------------- An R array or vector
# ws -------------- A pointer to the worksheet where to write the data
# cs -------------- Upper-left cell's reference as a numeric of length 2
# title ----------- Title. If set to NA, no title is exported.
# bline ----------- Logical. Should the bottom line be bold?
# align ----------- A string indicating how columns should be aligned.
# format ----------
# rownames -------- Logical. Should the function export row names?

# Details
# These are the author's preferences ;)
# align must be a sequence of 'l' for left, 'c' for center and 'r' for right.
# align = 'llcr' will cause the 2 columns on the left to be aligned on the
# left, the third column to be centered and the last column to be aligned
# to the right.
# format must be given using Excel conventions (e.g. '@' for string,
# '# ##0.00'for numbers with 2 decimals etc...).

xlPutTable = function(x, ws, cs, title = NA, bline = FALSE, align = NA,
      format = NA, rownames = TRUE) {
      if( ! xlIsRunning() )
            stop("Excel is not running. Use xlStart().")

      ws[["Cells"]] -> cells
      cells[["Interior"]][["Color"]] <- 16777215 # (white)
      cells[["RowHeight"]] <- 14
      cells[["ColumnWidth"]] <- 10
      cells <- NULL

      if( ! is.na(title) ) {
            xlPutArray(title, ws, cs) -> na
            xlRange(ws, cs) -> rr
            rr[["Font"]] -> font
            font[["Color"]] <- 10040115 # (blue)
            font[["Name"]] <- "Tahoma"
            font[["Bold"]] <- TRUE
            font[["Size"]] <- 9
            rr <- NULL
            cs[1] + 2 -> cs[1]
      }

      if(rownames) {
            colnames(x) -> temp
            data.frame(rownames(x), x) -> x
            cnames <- c("'", temp)
            } else {
            cnames <- colnames(x)
      }

      nrow(x) -> nrx
      ncol(x) -> ncx

      # align

      halign = c(l = -4131, c = -4108, r = -4152)

      if( is.na(align) ) {
            do.call(paste, c(list("l"), as.list(rep("r", ncx-1)),
                  list(sep = "") ) ) -> align
            } else {
            if( nchar(align) != ncx )
                  stop("'align' must have ", ncx, " characters")
      }

      sapply(1:nchar(align), function(i) {
            halign[ substr(align, i, i) ]
      } ) -> xlalign

      # format

      c("factor", "character", "logical") -> txt

      if( is.na(format[1]) ) {
            sapply(1:ncx, function(i) class(x[, i]) ) -> cc
            cc[ ! cc %in% c("Date", txt) ] <- "# ##0.00"
            cc[ cc %in% txt ] <- "@"
            cc[ cc == "Date" ] <- "aaaa-mm-jj"
            cc -> xlformat
      } else { format -> xlformat }

      # write by column

      lapply(1:ncx, function(i) {

            xlalign[i] -> xa
            xlformat[i] -> xf

            # column headers

            xlRange(ws, c(cs[1], cs[2] + i - 1)) -> rr
            rr[["Value2"]] <- cnames[i]
            rr[["HorizontalAlignment"]] <- xa

            rr[["Borders", 8]] -> edgetop
            edgetop[["LineStyle"]] <- 1
            edgetop[["Weight"]] <- -4138
            edgetop[["Color"]] <- 10040115 # (blue)

            rr[["Borders", 9]] -> edgebot
            edgebot[["LineStyle"]] <- 1
            edgebot[["Weight"]] <- 2
            edgebot[["Color"]] <- 10040115 # (blue)

            rr[["Font"]] -> font
            font[["Name"]] <- "Tahoma"
            font[["Color"]] <- 10040115 # (blue)
            font[["Bold"]] <- TRUE
            font[["Size"]] <- 9

            # data

            x[, i] -> xi

            if( class(xi) == "Date" ) {
                  julian(xi, origin = as.Date("1899-12-30")) -> xi
            }

            if( class(xi) %in% txt ) as.character(xi) -> xi

            csi <- list(c(cs[1] + 1, cs[2] + i - 1),
                        c(cs[1] + nrx - 1, cs[2] + i - 1 ) )
            xlRange(ws, csi) -> rr

            rr[["Value2"]] <- as.matrix(xi)
            rr[["HorizontalAlignment"]] <- xa
            rr[["NumberFormat"]] <- xf

            rr[["Font"]] -> font
            font[["Name"]] <- "Tahoma"
            font[["Size"]] <- 9

            csy <- c(cs[1] + nrx - 1, cs[2] + i - 1)
            xlRange(ws, csy) -> rr

            rr[["Borders", 9]] -> edgebot
            edgebot[["LineStyle"]] <- 1
            edgebot[["Weight"]] <- -4138
            edgebot[["Color"]] <- 10040115 # (blue)
            edgebot <- NULL

            # bottom row

            if(bline) {

                  rr[["Borders", 8]] -> edgetop
                  edgetop[["LineStyle"]] <- 1
                  edgetop[["Weight"]] <- 2
                  edgetop[["Color"]] <- 10040115 # (blue)
                  edgetop <- NULL

                  rr[["Font"]] -> font
                  font[["Bold"]] <- TRUE
                  font <- NULL
            }

            rr <- NULL

      } ) -> silent

}

# ----------------------------------------------------------------------------
# Contents
# ----------------------------------------------------------------------------

# Description
# List of functions. Use rm(list = .RXLS) to remove all these functions from
# .GlobalEnv.

.RXLS <- c("xlAddWorkbook", "xlClearContents", "xlCloseWorkbook",
      "xlGetArray", "xlIsRunning", "xlIsWorkbook", "xlOpenWorkbook",
      "xlPutArray", "xlRange", "xlStart", "xlStop", "xlVisible", "xlPutTable",
      ".RXLS")








                                                                                                                                                                    
                                                                                                                                                                    
                                                        Pour :   "Spencer Graves" <spencer.graves at pdf.com>                                                          
                                                        cc :     r-sig-finance at stat.math.ethz.ch, MAB <MichelBeck at sbcglobal.net>                                    
                                                        Objet :  Re: [R-SIG-Finance] Importing from Excel                                                           
             "Gabor Grothendieck"                                                                                                                                   
             <ggrothendieck at gmail.com>                                                                                                                              
             Envoy? par :                                                                                                                                           
             r-sig-finance-bounces at stat.math.ethz.                                                                                                                  
             ch                                                                                                                                                     
                                                                                                                                                                    
                                                                                                                                                                    
             04/02/2008 21:14                                                                                                                                       
                                                                                                                                                                    
                                                                                                                                                                    




Regarding read.xls in gdata note that it passes the ... to read.csv so you
could specify na.strings= which should avoid the problem
if its just due to the #NA strings.

On Mon, Feb 4, 2008 at 2:57 PM, Spencer Graves <spencer.graves at pdf.com> wrote:
>      I've had good luck with 'read.xls{gdata}', but that could have the
> same problem.
>
>      If this still gives you problems, you could put your favorite
> 'read.*(...)' in a function that tests columns you specify to see if
> they are numeric and if not tries to replace them with
>
>           is.numeric(as.character(DF[[i]]))
>
>      for column i of data.frame DF.  You could test that to see what
> percent NAs you have and write a warning, etc., whatever you want.
>
>      hope this helps.
>      Spencer
>
>
> MAB wrote:
> > Hi!
> >
> > I am trying to diff data imported from Excel.
> > I use the package xlsReadWrite.
> >
> > After I load the following XL spreadsheet
> >
> > C_Dates C_Price C_Prices_edit         C_Return_Raw        C_Return_edit
> > 1   30405   29.40         29.40                 #N/A                 #N/A
> > 2   30406   29.29         NaN                 -0.003               -0.003
> > 3   30407      NA         29.29                    0                    0
> > (#N/A is a string in one case and the formula =NA() in another)
> >
> > and try to diff the resulting object, I get:
> >
> > Error in r[i1, , drop = FALSE] - r[-nrow(r):-(nrow(r) - lag + 1), , drop =
> > FALSE] :
> >         non-numeric argument to binary operator
> >
> > I attempt to convert to numeric using as.matrix (or data.matrix),
> > but this converts the object to all characters.
> >
> > I can then use as.numeric to get a vector and rebuild the matrix but this gets
> > tedious.
> >
> > Eventually the easiest seems to be to make sure each column in the spreadsheet
> > starts with a numeric, and replace it once the object is in R.
> >
> > There is probably a better way.
> >
> > Michel
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only.
-- If you want to post, subscribe first.




Les informations contenues dans ce message sont confidentielles et peuvent constituer des informations privilegiees. Si vous n etes pas le destinataire de ce message, il vous est interdit de le copier, de le faire suivre, de le divulguer ou d en utiliser tout ou partie. Si vous avez recu ce message par erreur, merci de le supprimer de votre systeme, ainsi que toutes ses copies, et d en avertir immediatement l expediteur par message de retour.
Il est impossible de garantir que les communications par messagerie electronique arrivent en temps utile, sont securisees ou denuees de toute erreur ou virus. En consequence, l expediteur n accepte aucune responsabilite du fait des erreurs ou omissions qui pourraient en resulter.
--- ----------------------------------------------------- ---
The information contained in this e-mail is confidential...{{dropped:9}}


From gchappi at gmail.com  Tue Feb  5 11:23:59 2008
From: gchappi at gmail.com (Hans-Peter)
Date: Tue, 5 Feb 2008 11:23:59 +0100
Subject: [R-SIG-Finance] Importing from Excel
In-Reply-To: <loom.20080204T185009-74@post.gmane.org>
References: <loom.20080204T185009-74@post.gmane.org>
Message-ID: <47fce0650802050223s2e678beeo47d609ec23b9a063@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080205/ffe127f1/attachment.pl 

From comtech.usa at gmail.com  Thu Feb  7 07:57:33 2008
From: comtech.usa at gmail.com (Michael)
Date: Wed, 6 Feb 2008 22:57:33 -0800
Subject: [R-SIG-Finance] how to connect S-Plus to Matlab?
Message-ID: <b1f16d9d0802062257s3eb1a86aw1e2cca86e90c41ae@mail.gmail.com>

Hi all,

I am playing around with the S-Plus ... and I am wondering how to
connect it back and forth with Matlab?

For example, how do I call S-Plus from within Matlab and how do I call
Matlab from within S-Plus?

How do I transfer data back and forth?

Thanks a lot


From comtech.usa at gmail.com  Thu Feb  7 08:04:02 2008
From: comtech.usa at gmail.com (Michael)
Date: Wed, 6 Feb 2008 23:04:02 -0800
Subject: [R-SIG-Finance] where do I find stochastic volatilities models in R
	or Matlab?
Message-ID: <b1f16d9d0802062304h48babcd0g7cb902adcba59087@mail.gmail.com>

Hi all,

Does anybody have the source code of stochastic volatility models in R
or Matlab, for example, the Bayesian based or the simulation based SV
estimations as described by Prof Eric Zivot in the following
discussion?

https://stat.ethz.ch/pipermail/r-sig-finance/2005q4/000501.html

--------------

I am wondering what is the current status of estimating stochastic vol
models and what's the latest development -- which technique is
currently the best method, Bayesian methods or the simualation based
methods, or others?

Thanks a lot!


From brian at braverock.com  Thu Feb  7 12:13:53 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 07 Feb 2008 05:13:53 -0600
Subject: [R-SIG-Finance] where do I find stochastic volatilities models
 in R	or Matlab?
In-Reply-To: <b1f16d9d0802062304h48babcd0g7cb902adcba59087@mail.gmail.com>
References: <b1f16d9d0802062304h48babcd0g7cb902adcba59087@mail.gmail.com>
Message-ID: <47AAE7F1.2070001@braverock.com>

Michael wrote:
> Does anybody have the source code of stochastic volatility models in R
> or Matlab, for example, the Bayesian based or the simulation based SV
> estimations as described by Prof Eric Zivot in the following
> discussion?
> 
> https://stat.ethz.ch/pipermail/r-sig-finance/2005q4/000501.html

See Chapter 7 of the book "Bayesian Core" by Christian Robert and 
Jean-Michel Marin.

Regards,

   - Brian


From brian at braverock.com  Thu Feb  7 12:28:00 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 07 Feb 2008 05:28:00 -0600
Subject: [R-SIG-Finance] how to connect S-Plus to Matlab?
In-Reply-To: <b1f16d9d0802062257s3eb1a86aw1e2cca86e90c41ae@mail.gmail.com>
References: <b1f16d9d0802062257s3eb1a86aw1e2cca86e90c41ae@mail.gmail.com>
Message-ID: <47AAEB40.6010903@braverock.com>

Michael wrote:
> Hi all,
> 
> I am playing around with the S-Plus ... and I am wondering how to
> connect it back and forth with Matlab?
> 
> For example, how do I call S-Plus from within Matlab and how do I call
> Matlab from within S-Plus?
> 
> How do I transfer data back and forth?

Please search CRAN before posting.

http://cran.r-project.org/src/contrib/Descriptions/R.matlab.html


From ahala2000 at yahoo.com  Thu Feb  7 21:10:13 2008
From: ahala2000 at yahoo.com (elton wang)
Date: Thu, 7 Feb 2008 12:10:13 -0800 (PST)
Subject: [R-SIG-Finance] Does R have a formal test for long vs short
	memory process? ---acf confidence intervals
In-Reply-To: <47A211C0.7010500@braverock.com>
Message-ID: <899161.34137.qm@web31405.mail.mud.yahoo.com>

Brian,
For acf chart, how to see the value of confidence
intervals? any reference on the calculation of this?
are they just 1/sqrt(size)? 

Below is my R screen, no information on confidence
intervals.

> summary(acf(rnorm(1000)))
       Length Class  Mode     
acf    31     -none- numeric  
type    1     -none- character
n.used  1     -none- numeric  
lag    31     -none- numeric  
series  1     -none- character
snames  0     -none- NULL     
> 

--- "Brian G. Peterson" <brian at braverock.com> wrote:


> In addition to the ACF chart, the acf calculation
> calculates confidence 
> intervals for significance.  The summary() method on
> the results of an 
> acf will tell you what the values for these
> confidence intervals are.
> 



      ____________________________________________________________________________________
Be a better friend, newshound, and


From spencer.graves at pdf.com  Fri Feb  8 04:14:54 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 07 Feb 2008 19:14:54 -0800
Subject: [R-SIG-Finance] Does R have a formal test for long vs
 short	memory process? ---acf confidence intervals
In-Reply-To: <899161.34137.qm@web31405.mail.mud.yahoo.com>
References: <899161.34137.qm@web31405.mail.mud.yahoo.com>
Message-ID: <47ABC92E.8050004@pdf.com>

      The confidence intervals seem to be computed in plot.acf, not in 
acf.  Do "getAnywhere(plot.acf)", then search for 'clim'.  That just 
identified the following for me: 

    clim0 <- if (with.ci)
        qnorm((1 + ci)/2)/sqrt(x$n.used)
    else c(0, 0)

...

                clim <- clim0 * sqrt(cumsum(c(1, 2 * x$acf[-1,
                  i, i]^2)))

...

            clim <- if (with.ci.ma && i == j)
                clim0 * sqrt(cumsum(c(1, 2 * x$acf[-1, i, j]^2)))
            else clim0


      I'm not certain what all this means, but it looks like it should 
contain the answer to your question. 

      Hope this helps. 
      Spencer

elton wang wrote:
> Brian,
> For acf chart, how to see the value of confidence
> intervals? any reference on the calculation of this?
> are they just 1/sqrt(size)? 
>
> Below is my R screen, no information on confidence
> intervals.
>
>   
>> summary(acf(rnorm(1000)))
>>     
>        Length Class  Mode     
> acf    31     -none- numeric  
> type    1     -none- character
> n.used  1     -none- numeric  
> lag    31     -none- numeric  
> series  1     -none- character
> snames  0     -none- NULL     
>   
>
> --- "Brian G. Peterson" <brian at braverock.com> wrote:
>
>
>   
>> In addition to the ACF chart, the acf calculation
>> calculates confidence 
>> intervals for significance.  The summary() method on
>> the results of an 
>> acf will tell you what the values for these
>> confidence intervals are.
>>
>>     
>
>
>
>       ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From comtech.usa at gmail.com  Fri Feb  8 08:49:07 2008
From: comtech.usa at gmail.com (Michael)
Date: Thu, 7 Feb 2008 23:49:07 -0800
Subject: [R-SIG-Finance] where do I find stochastic volatilities models
	in R or Matlab?
In-Reply-To: <47AAE7F1.2070001@braverock.com>
References: <b1f16d9d0802062304h48babcd0g7cb902adcba59087@mail.gmail.com>
	<47AAE7F1.2070001@braverock.com>
Message-ID: <b1f16d9d0802072349s39db3c78k212cc052e3af3e90@mail.gmail.com>

Okay I am ordering the book...

Does anybody know any recent papers discussing about comparison about
these SV estimation methods? Moreover, where do I find those source
codes in public domain?

Thanks,

-M

On Feb 7, 2008 3:13 AM, Brian G. Peterson <brian at braverock.com> wrote:
> Michael wrote:
> > Does anybody have the source code of stochastic volatility models in R
> > or Matlab, for example, the Bayesian based or the simulation based SV
> > estimations as described by Prof Eric Zivot in the following
> > discussion?
> >
> > https://stat.ethz.ch/pipermail/r-sig-finance/2005q4/000501.html
>
> See Chapter 7 of the book "Bayesian Core" by Christian Robert and
> Jean-Michel Marin.
>
> Regards,
>
>    - Brian
>


From guiseppe.milicia at hsbcib.com  Fri Feb  8 11:49:25 2008
From: guiseppe.milicia at hsbcib.com (guiseppe.milicia at hsbcib.com)
Date: Fri, 8 Feb 2008 10:49:25 +0000
Subject: [R-SIG-Finance] Milliseconds to proper date/time class
Message-ID: <OF1BC56CE8.4B2B0B05-ON802573E9.003AF432-802573E9.003B74EA@hsbcib.com>

Guys,

I'm importing high frequency time series data into R. I'm using the zoo
timeseries class which works quite well.

At the moment the times of my series are specified as milliseconds since
epoch. The issue is that the X axis in plots is not all that useful, the
same goes for things like window, etc.

I was wondering what would be the reccomended way to convert from that
representation to a proper date/time representation that works with zoo,
e.g. Posixct?

I would imagine this to be a rather common  problem, but my searches have
been unfruitful...

Cheers,

-- Giuseppe

************************************************************
HSBC Bank plc may be solicited in the course of its placement efforts for a
new issue, by investment clients of the firm for whom the Bank as a firm
already provides other services. It may equally decide to allocate to its
own proprietary book or with an associate of HSBC Group. This represents a
potential conflict of interest. HSBC Bank plc has internal arrangements
designed to ensure that the firm would give unbiased and full advice to the
corporate finance client about the valuation and pricing of the offering as
well as internal systems, controls and procedures to identify and manage
conflicts of interest.

HSBC Bank plc
Registered Office: 8 Canada Square, London E14 5HQ, United Kingdom
Registered in England - Number 14259
Authorised and regulated by the Financial Services Authority.
************************************************************


-----------------------------------------
SAVE PAPER - THINK BEFORE YOU PRINT!

This transmission has been issued by a member of the HSBC Group
"HSBC" for the information of the addressee only and should not be
reproduced and/or distributed to any other person. Each page
attached hereto must be read in conjunction with any disclaimer
which forms part of it. Unless otherwise stated, this transmission
is neither an offer nor the solicitation of an offer to sell or
purchase any investment. Its contents are based on information
obtained from sources believed to be reliable but HSBC makes no
representation and accepts no responsibility or liability as to its
completeness or accuracy.


From brian at braverock.com  Fri Feb  8 14:41:04 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 08 Feb 2008 07:41:04 -0600
Subject: [R-SIG-Finance] Does R have a formal test for long vs
 short	memory process? ---acf confidence intervals
In-Reply-To: <47ABC92E.8050004@pdf.com>
References: <899161.34137.qm@web31405.mail.mud.yahoo.com>
	<47ABC92E.8050004@pdf.com>
Message-ID: <47AC5BF0.4050306@braverock.com>

I had time to investigate this this morning, and came to the same 
conclusion as Spencer.  Sorry for the earlier confusion.

You can see all the code for acf,pacf, and plot.acf here:

https://svn.r-project.org/R/trunk/src/library/stats/R/acf.R

It would be nice if the summary() or print() method for class acf 
included the confidence interval, as I originally thought it did.

Regards,

   - Brian


Spencer Graves wrote:
>      The confidence intervals seem to be computed in plot.acf, not in 
> acf.  Do "getAnywhere(plot.acf)", then search for 'clim'.  That just 
> identified the following for me:
>    clim0 <- if (with.ci)
>        qnorm((1 + ci)/2)/sqrt(x$n.used)
>    else c(0, 0)
> 
> ...
> 
>                clim <- clim0 * sqrt(cumsum(c(1, 2 * x$acf[-1,
>                  i, i]^2)))
> 
> ...
> 
>            clim <- if (with.ci.ma && i == j)
>                clim0 * sqrt(cumsum(c(1, 2 * x$acf[-1, i, j]^2)))
>            else clim0
> 
> 
>      I'm not certain what all this means, but it looks like it should 
> contain the answer to your question.
>      Hope this helps.      Spencer
> 
> elton wang wrote:
>> Brian,
>> For acf chart, how to see the value of confidence
>> intervals? any reference on the calculation of this?
>> are they just 1/sqrt(size)?
>> Below is my R screen, no information on confidence
>> intervals.
>>
>>  
>>> summary(acf(rnorm(1000)))
>>>     
>>        Length Class  Mode     acf    31     -none- numeric  type    
>> 1     -none- character
>> n.used  1     -none- numeric  lag    31     -none- numeric  series  
>> 1     -none- character
>> snames  0     -none- NULL      
>> --- "Brian G. Peterson" <brian at braverock.com> wrote:
>>
>>
>>  
>>> In addition to the ACF chart, the acf calculation
>>> calculates confidence intervals for significance.  The summary() 
>>> method on
>>> the results of an acf will tell you what the values for these
>>> confidence intervals are.
>>>
>>>     
>>
>>
>>
>>       
>> ____________________________________________________________________________________ 
>>
>> Be a better friend, newshound, and
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only. -- If you want to post, subscribe first.
>>


From brian at braverock.com  Fri Feb  8 14:46:56 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 08 Feb 2008 07:46:56 -0600
Subject: [R-SIG-Finance] where do I find stochastic volatilities models
 in R or Matlab?
In-Reply-To: <b1f16d9d0802072349s39db3c78k212cc052e3af3e90@mail.gmail.com>
References: <b1f16d9d0802062304h48babcd0g7cb902adcba59087@mail.gmail.com>	
	<47AAE7F1.2070001@braverock.com>
	<b1f16d9d0802072349s39db3c78k212cc052e3af3e90@mail.gmail.com>
Message-ID: <47AC5D50.3090204@braverock.com>

Michael wrote:
> Okay I am ordering the book...
> 
> Does anybody know any recent papers discussing about comparison about
> these SV estimation methods? 

I don't have any paper references handy.  I know that Prof. Zivot has 
some working papers on volatility modeling on his website at the 
university of washington.  As I recall, at least one of them includes a 
literature survey.

> Moreover, where do I find those source
> codes in public domain?


The code for the book "Bayesian Core" is here:

http://www.ceremade.dauphine.fr/~xian/BCS/

The code for Albert's "Bayesian Computation with R" is in the LearnBayes 
package on CRAN.  Bayesian Core has a more in-depth discussion of 
Bayesian models for SV.

Both books are well worth owning as references, even though the code is 
available.

Regards,

   - Brian

> On Feb 7, 2008 3:13 AM, Brian G. Peterson <brian at braverock.com> wrote:
>> Michael wrote:
>>> Does anybody have the source code of stochastic volatility models in R
>>> or Matlab, for example, the Bayesian based or the simulation based SV
>>> estimations as described by Prof Eric Zivot in the following
>>> discussion?
>>>
>>> https://stat.ethz.ch/pipermail/r-sig-finance/2005q4/000501.html
 >>
>> See Chapter 7 of the book "Bayesian Core" by Christian Robert and
>> Jean-Michel Marin.


From ahala2000 at yahoo.com  Fri Feb  8 15:05:20 2008
From: ahala2000 at yahoo.com (elton wang)
Date: Fri, 8 Feb 2008 06:05:20 -0800 (PST)
Subject: [R-SIG-Finance] acf confidence intervals in ARMA
In-Reply-To: <47AC5BF0.4050306@braverock.com>
Message-ID: <633613.15300.qm@web31404.mail.mud.yahoo.com>

Thanks for Brian and Spencer's help!
I tried to understand the two formulas below by google
and books I have but have no success so far. Can
anyone give me a hint how to reach these results? 

Sorry for asking if this is too basic...

--- "Brian G. Peterson" <brian at braverock.com> wrote:

> I had time to investigate this this morning, and
> came to the same 
> conclusion as Spencer.  Sorry for the earlier
> confusion.
> 
> You can see all the code for acf,pacf, and plot.acf
> here:
> 
>
https://svn.r-project.org/R/trunk/src/library/stats/R/acf.R
> 
> It would be nice if the summary() or print() method
> for class acf 
> included the confidence interval, as I originally
> thought it did.
> 
> Regards,
> 
>    - Brian
> 
> 
> Spencer Graves wrote:
> >      The confidence intervals seem to be computed
> in plot.acf, not in 
> > acf.  Do "getAnywhere(plot.acf)", then search for
> 'clim'.  That just 
> > identified the following for me:
> >    clim0 <- if (with.ci)
> >        qnorm((1 + ci)/2)/sqrt(x$n.used)
> >    else c(0, 0)
> > 
> > ...
> > 
> >                clim <- clim0 * sqrt(cumsum(c(1, 2
> * x$acf[-1,
> >                  i, i]^2)))
> > 
> > ...
> > 
> >            clim <- if (with.ci.ma && i == j)
> >                clim0 * sqrt(cumsum(c(1, 2 *
> x$acf[-1, i, j]^2)))
> >            else clim0
> > 
> > 
> >      I'm not certain what all this means, but it
> looks like it should 
> > contain the answer to your question.
> >      Hope this helps.      Spencer
> > 
> > elton wang wrote:
> >> Brian,
> >> For acf chart, how to see the value of confidence
> >> intervals? any reference on the calculation of
> this?
> >> are they just 1/sqrt(size)?
> >> Below is my R screen, no information on
> confidence
> >> intervals.
> >>
> >>  
> >>> summary(acf(rnorm(1000)))
> >>>     
> >>        Length Class  Mode     acf    31    
> -none- numeric  type    
> >> 1     -none- character
> >> n.used  1     -none- numeric  lag    31    
> -none- numeric  series  
> >> 1     -none- character
> >> snames  0     -none- NULL      
> >> --- "Brian G. Peterson" <brian at braverock.com>
> wrote:
> >>
> >>
> >>  
> >>> In addition to the ACF chart, the acf
> calculation
> >>> calculates confidence intervals for
> significance.  The summary() 
> >>> method on
> >>> the results of an acf will tell you what the
> values for these
> >>> confidence intervals are.
> >>>
> >>>     
> >>
> >>
> >>
> >>       
> >>
>
____________________________________________________________________________________
> 
> >>
> >> Be a better friend, newshound, and
> >>
> >> _______________________________________________
> >> R-SIG-Finance at stat.math.ethz.ch mailing list
> >>
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >> -- Subscriber-posting only. -- If you want to
> post, subscribe first.
> >>   
> 
> 



      ____________________________________________________________________________________
Looking for last minute shopping deals?


From ggrothendieck at gmail.com  Fri Feb  8 15:24:16 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Feb 2008 09:24:16 -0500
Subject: [R-SIG-Finance] Milliseconds to proper date/time class
In-Reply-To: <OF1BC56CE8.4B2B0B05-ON802573E9.003AF432-802573E9.003B74EA@hsbcib.com>
References: <OF1BC56CE8.4B2B0B05-ON802573E9.003AF432-802573E9.003B74EA@hsbcib.com>
Message-ID: <971536df0802080624x50aad643waf364f4310b51180@mail.gmail.com>

Try this:

library(zoo)
z <- zoo(1:10, 1001:1010)

Now <- Sys.time()
Epoch <- Now - as.numeric(Now)
zct <- aggregate(z, time(z) + Epoch, force)

# If you like you can convert it to chron
# from there; however, its possible that chron
# does not support as high a resolution
# so collapse equal valued times just in case
# Below we take the last one but you could take
# the mean or other value if you prefer or you could
# use force in place of tail, 1 and if that works then
# your times are far enough apart or your machine
# has enough resolution to represent them distinctly
# in chron.

library(chron)
zchron.gmt <- aggregate(zct, as.chron, tail, 1)
zhcron.local <- aggregate(zct, function(x) as.chron(x, tz = ""), tail, 1)

Suggest you read RNews 4/1 and see ?aggregate.zoo
Also read the three zoo vignettes.


On Feb 8, 2008 5:49 AM,  <guiseppe.milicia at hsbcib.com> wrote:
> Guys,
>
> I'm importing high frequency time series data into R. I'm using the zoo
> timeseries class which works quite well.
>
> At the moment the times of my series are specified as milliseconds since
> epoch. The issue is that the X axis in plots is not all that useful, the
> same goes for things like window, etc.
>
> I was wondering what would be the reccomended way to convert from that
> representation to a proper date/time representation that works with zoo,
> e.g. Posixct?
>
> I would imagine this to be a rather common  problem, but my searches have
> been unfruitful...
>
> Cheers,
>
> -- Giuseppe
>
> ************************************************************
> HSBC Bank plc may be solicited in the course of its placement efforts for a
> new issue, by investment clients of the firm for whom the Bank as a firm
> already provides other services. It may equally decide to allocate to its
> own proprietary book or with an associate of HSBC Group. This represents a
> potential conflict of interest. HSBC Bank plc has internal arrangements
> designed to ensure that the firm would give unbiased and full advice to the
> corporate finance client about the valuation and pricing of the offering as
> well as internal systems, controls and procedures to identify and manage
> conflicts of interest.
>
> HSBC Bank plc
> Registered Office: 8 Canada Square, London E14 5HQ, United Kingdom
> Registered in England - Number 14259
> Authorised and regulated by the Financial Services Authority.
> ************************************************************
>
>
> -----------------------------------------
> SAVE PAPER - THINK BEFORE YOU PRINT!
>
> This transmission has been issued by a member of the HSBC Group
> "HSBC" for the information of the addressee only and should not be
> reproduced and/or distributed to any other person. Each page
> attached hereto must be read in conjunction with any disclaimer
> which forms part of it. Unless otherwise stated, this transmission
> is neither an offer nor the solicitation of an offer to sell or
> purchase any investment. Its contents are based on information
> obtained from sources believed to be reliable but HSBC makes no
> representation and accepts no responsibility or liability as to its
> completeness or accuracy.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ahala2000 at yahoo.com  Fri Feb  8 15:24:57 2008
From: ahala2000 at yahoo.com (elton wang)
Date: Fri, 8 Feb 2008 06:24:57 -0800 (PST)
Subject: [R-SIG-Finance] Simulate the stock market for back testing strategy
Message-ID: <335621.23809.qm@web31413.mail.mud.yahoo.com>

Here is a beginner question:
what would be your perferred method if we want to
simulate the stock market for back testing a trading
strategy? 
Using sp500 daily data as example, if given the
knowledge that historical data has time varying
volatility, autocorrelations etc? just fitting a
GARCH(1,1) or doing historical resampling?  (simply
divided the data to in-sample and out sample may not
be sufficient, am I right?)

thanks for any comments!



      ____________________________________________________________________________________
Be a better friend, newshound, and


From edd at debian.org  Fri Feb  8 15:45:04 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 8 Feb 2008 08:45:04 -0600
Subject: [R-SIG-Finance] Milliseconds to proper date/time class
In-Reply-To: <OF1BC56CE8.4B2B0B05-ON802573E9.003AF432-802573E9.003B74EA@hsbcib.com>
References: <OF1BC56CE8.4B2B0B05-ON802573E9.003AF432-802573E9.003B74EA@hsbcib.com>
Message-ID: <18348.27376.242101.901472@ron.nulle.part>


On 8 February 2008 at 10:49, guiseppe.milicia at hsbcib.com wrote:
| Guys,
| 
| I'm importing high frequency time series data into R. I'm using the zoo
| timeseries class which works quite well.
| 
| At the moment the times of my series are specified as milliseconds since
| epoch. The issue is that the X axis in plots is not all that useful, the
| same goes for things like window, etc.
| 
| I was wondering what would be the reccomended way to convert from that
| representation to a proper date/time representation that works with zoo,
| e.g. Posixct?

Not sure about "recommended" but here is one. Note that 'basic C type' is
seconds since epoch, so you need to scale up from millisecs. Then given a
vector of type "numeric" with timestamps in seconds since epoch (incl
fractional seconds), you can obtain POSIXt object by adding the epoch itself
-- and this is key -- as a proper POSIXt type, eg use this

> options('width'=70)
> xvec <- Sys.time() + sort(runif(4)*100)
> xvec
[1] "2008-02-08 08:39:02.412544 CST" "2008-02-08 08:39:33.635201 CST"
[3] "2008-02-08 08:39:44.867410 CST" "2008-02-08 08:39:52.813029 CST"
> dvec <- as.numeric(xvec)
> dvec
[1] 1202481542 1202481574 1202481585 1202481593
> diff(dvec)                              ## ie it really has fractional secs
[1] 31.22266 11.23221  7.94562
> newXvec <- dvec + ISOdatetime(1970,1,1,0,0,0) - 6*60*60  ## Chicago is 6hrs off
> newXvec
[1] "2008-02-08 08:39:02.412544 CST" "2008-02-08 08:39:33.635201 CST"
[3] "2008-02-08 08:39:44.867410 CST" "2008-02-08 08:39:52.813029 CST"
> 

The GMT offset is a little tricky, but there are ways to work that out too.

| SAVE PAPER - THINK BEFORE YOU PRINT!

We should use that for r-help. SAVE ELECTRONS - THINK BEFORE YOU PRESS SEND!

Dirk

-- 
Three out of two people have difficulties with fractions.


From brian at braverock.com  Fri Feb  8 16:01:45 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 08 Feb 2008 09:01:45 -0600
Subject: [R-SIG-Finance] Simulate the stock market for back testing
	strategy
In-Reply-To: <335621.23809.qm@web31413.mail.mud.yahoo.com>
References: <335621.23809.qm@web31413.mail.mud.yahoo.com>
Message-ID: <47AC6ED9.6080603@braverock.com>

elton wang wrote:
> Here is a beginner question:
> what would be your perferred method if we want to
> simulate the stock market for back testing a trading
> strategy? 
> Using sp500 daily data as example, if given the
> knowledge that historical data has time varying
> volatility, autocorrelations etc? just fitting a
> GARCH(1,1) or doing historical resampling?  (simply
> divided the data to in-sample and out sample may not
> be sufficient, am I right?)

You've bitten off one of the most complex and studied problems in finance.

Kalman filtering is often applied to build bands and trends, as are 
straightforward standard deviation based measures such as "Bollinger bands".

Any of the AR methods ARMA, ARIMA, GARCH allow for time-varying changes 
in level and volatility.

Refinement of those models generally involves EMM or Bayesian evolution 
of the moments.

These can all be used as one- or multiple- step-ahead prediction methods.

In general, these predictions would be used as inputs to *create* a 
trading strategy.  You would then backtest your strategy by setting up a 
"learning period" (length depending on the frequency of your data), and 
then letting the model evolve on an out-of-sample basis (by making one 
step ahead or similar predictions).

If you then wanted to further test your models, you could fit various 
distributions to historical data and simulate historical series from 
these distributions.  I'm not really a fan of the pure simulation 
approach unless you are very careful and know what you're doing, because 
there is a huge amount of model risk (risk that you will mis-specify 
starting parameters and therefore get worthless results) involved in 
these pure simulation approaches.

Many Bayesian (and other Monte Carlo) methods use simulation to inform 
their predictions, but this is different than constructing purely 
hypothetical historical series to test a model against.

Regards,

    - Brian


From peter at braverock.com  Fri Feb  8 16:14:49 2008
From: peter at braverock.com (Peter Carl)
Date: Fri, 8 Feb 2008 09:14:49 -0600
Subject: [R-SIG-Finance] acf confidence intervals in ARMA
In-Reply-To: <633613.15300.qm@web31404.mail.mud.yahoo.com>
References: <633613.15300.qm@web31404.mail.mud.yahoo.com>
Message-ID: <200802080914.50436.peter@braverock.com>

On Friday 08 February 2008 8:05:20 am elton wang wrote:
> I tried to understand the two formulas below by google
> and books I have but have no success so far.

?plot.acf
Confidence limits assume a white noise input by default.

? ? "The confidence interval plotted in 'plot.acf' is based on an
? ? ?_uncorrelated_ series and should be treated with appropriate
? ? ?caution. ?Using 'ci.type = "ma"' may be less potentially
? ? ?misleading."

HTH,

pcc
-- 
Peter Carl
145 Scottswood Rd
Riverside, IL 60546
312 307 6346
http://www.braverock.com/~peter


From brian at braverock.com  Fri Feb  8 16:29:19 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 08 Feb 2008 09:29:19 -0600
Subject: [R-SIG-Finance] acf confidence intervals in ARMA
In-Reply-To: <200802080914.50436.peter@braverock.com>
References: <633613.15300.qm@web31404.mail.mud.yahoo.com>
	<200802080914.50436.peter@braverock.com>
Message-ID: <47AC754F.2000903@braverock.com>

Peter Carl wrote:
> On Friday 08 February 2008 8:05:20 am elton wang wrote:
>> I tried to understand the two formulas below by google
>> and books I have but have no success so far.
> 
> ?plot.acf
> Confidence limits assume a white noise input by default.
> 
>     "The confidence interval plotted in 'plot.acf' is based on an
>      _uncorrelated_ series and should be treated with appropriate
>      caution.  Using 'ci.type = "ma"' may be less potentially
>      misleading."

The ci.type of "white" uses a simple volatility model un the acf series 
to calculate the confidence intervals.  The "ma" or correlation model 
uses the volatility of the correlation of the lags of series to 
calculate the confidence intervals.

     with.ci <- ci > 0 && x$type != "covariance"
     with.ci.ma <- with.ci && ci.type == "ma" && x$type == "correlation"
     if(with.ci.ma && x$lag[1,1,1] != 0) {
         warning("can use ci.type=\"ma\" only if first lag is 0")
         with.ci.ma <- FALSE
     }
     clim0 <- if (with.ci) qnorm((1 + ci)/2)/sqrt(x$n.used) else c(0, 0)

Regards,

   - Brian


From jeff.a.ryan at gmail.com  Fri Feb  8 17:38:04 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 8 Feb 2008 10:38:04 -0600
Subject: [R-SIG-Finance] Milliseconds to proper date/time class
In-Reply-To: <18348.27376.242101.901472@ron.nulle.part>
References: <OF1BC56CE8.4B2B0B05-ON802573E9.003AF432-802573E9.003B74EA@hsbcib.com>
	<18348.27376.242101.901472@ron.nulle.part>
Message-ID: <e8e755250802080838w4c9f4b22v2b92d5b13e3aa271@mail.gmail.com>

A quick comment:

I think options('digits.secs'=6) might need to be set as well to be
able to see the fractional part of the second.

Jeff

On Feb 8, 2008 8:45 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> On 8 February 2008 at 10:49, guiseppe.milicia at hsbcib.com wrote:
> | Guys,
> |
> | I'm importing high frequency time series data into R. I'm using the zoo
> | timeseries class which works quite well.
> |
> | At the moment the times of my series are specified as milliseconds since
> | epoch. The issue is that the X axis in plots is not all that useful, the
> | same goes for things like window, etc.
> |
> | I was wondering what would be the reccomended way to convert from that
> | representation to a proper date/time representation that works with zoo,
> | e.g. Posixct?
>
> Not sure about "recommended" but here is one. Note that 'basic C type' is
> seconds since epoch, so you need to scale up from millisecs. Then given a
> vector of type "numeric" with timestamps in seconds since epoch (incl
> fractional seconds), you can obtain POSIXt object by adding the epoch itself
> -- and this is key -- as a proper POSIXt type, eg use this
>
> > options('width'=70)
> > xvec <- Sys.time() + sort(runif(4)*100)
> > xvec
> [1] "2008-02-08 08:39:02.412544 CST" "2008-02-08 08:39:33.635201 CST"
> [3] "2008-02-08 08:39:44.867410 CST" "2008-02-08 08:39:52.813029 CST"
> > dvec <- as.numeric(xvec)
> > dvec
> [1] 1202481542 1202481574 1202481585 1202481593
> > diff(dvec)                              ## ie it really has fractional secs
> [1] 31.22266 11.23221  7.94562
> > newXvec <- dvec + ISOdatetime(1970,1,1,0,0,0) - 6*60*60  ## Chicago is 6hrs off
> > newXvec
> [1] "2008-02-08 08:39:02.412544 CST" "2008-02-08 08:39:33.635201 CST"
> [3] "2008-02-08 08:39:44.867410 CST" "2008-02-08 08:39:52.813029 CST"
> >
>
> The GMT offset is a little tricky, but there are ways to work that out too.
>
> | SAVE PAPER - THINK BEFORE YOU PRINT!
>
> We should use that for r-help. SAVE ELECTRONS - THINK BEFORE YOU PRESS SEND!
>
> Dirk
>
> --
> Three out of two people have difficulties with fractions.
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From edd at debian.org  Fri Feb  8 18:34:12 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 8 Feb 2008 11:34:12 -0600
Subject: [R-SIG-Finance] Milliseconds to proper date/time class
In-Reply-To: <e8e755250802080838w4c9f4b22v2b92d5b13e3aa271@mail.gmail.com>
References: <OF1BC56CE8.4B2B0B05-ON802573E9.003AF432-802573E9.003B74EA@hsbcib.com>
	<18348.27376.242101.901472@ron.nulle.part>
	<e8e755250802080838w4c9f4b22v2b92d5b13e3aa271@mail.gmail.com>
Message-ID: <18348.37524.151633.559966@ron.nulle.part>


On 8 February 2008 at 10:38, Jeff Ryan wrote:
| A quick comment:
| 
| I think options('digits.secs'=6) might need to be set as well to be
| able to see the fractional part of the second.

Quite so, it's actually a default in my ~/.Rprofile.  Thanks for catching
that.

Dirk

-- 
Three out of two people have difficulties with fractions.


From MichelBeck at sbcglobal.Net  Fri Feb  8 21:15:26 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Fri, 8 Feb 2008 20:15:26 +0000 (UTC)
Subject: [R-SIG-Finance] Zoo functions - Plotting
References: <loom.20080201T202320-56@post.gmane.org>
Message-ID: <loom.20080208T195750-818@post.gmane.org>

MAB <MichelBeck <at> sbcglobal.Net> writes:

Hi!

When I plot the time-series in a zoo object using plot.zoo they are all drawn 
successively on the same graphics device. If there are more than 10 series 
this is not legible. With the "standard" plot function I could use the 
graphics parameter mfcol or mfrow to specify the number of plot 
screens/pannels per device.

How can I do this with zoo?

Thanks,

Michel

PS How do I open multiple on-screen graphics device at one time to plot a 
large number of time-series? Within a plotting loop an additional device 
should open each time the maximum number of plot screens/pannels specified per 
device is reached?


From ggrothendieck at gmail.com  Fri Feb  8 21:38:36 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Feb 2008 15:38:36 -0500
Subject: [R-SIG-Finance] Zoo functions - Plotting
In-Reply-To: <loom.20080208T195750-818@post.gmane.org>
References: <loom.20080201T202320-56@post.gmane.org>
	<loom.20080208T195750-818@post.gmane.org>
Message-ID: <971536df0802081238p61762f5bpa18f043e6b2213cb@mail.gmail.com>

If you want some on one page and others on another page
issue two plot.zoo commands.

plot(z[,1:2])
plot(z[,3:4])

Also look at xyplot.zoo which gives you the facilities of lattice xyplot
with zoo objects.


On Feb 8, 2008 3:15 PM, MAB <MichelBeck at sbcglobal.net> wrote:
> MAB <MichelBeck <at> sbcglobal.Net> writes:
>
> Hi!
>
> When I plot the time-series in a zoo object using plot.zoo they are all drawn
> successively on the same graphics device. If there are more than 10 series
> this is not legible. With the "standard" plot function I could use the
> graphics parameter mfcol or mfrow to specify the number of plot
> screens/pannels per device.
>
> How can I do this with zoo?
>
> Thanks,
>
> Michel
>
> PS How do I open multiple on-screen graphics device at one time to plot a
> large number of time-series? Within a plotting loop an additional device
> should open each time the maximum number of plot screens/pannels specified per
> device is reached?
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Fri Feb  8 23:34:29 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 8 Feb 2008 17:34:29 -0500
Subject: [R-SIG-Finance] Zoo functions - Plotting
In-Reply-To: <971536df0802081238p61762f5bpa18f043e6b2213cb@mail.gmail.com>
References: <loom.20080201T202320-56@post.gmane.org>
	<loom.20080208T195750-818@post.gmane.org>
	<971536df0802081238p61762f5bpa18f043e6b2213cb@mail.gmail.com>
Message-ID: <971536df0802081434k51dafc94rd6de37c13857d60@mail.gmail.com>

One other point.

If you only plot a single panel (there can be multiple time series in
each panel)
at a time then you can use mfrow.  For example,  this works:

library(zoo)
opar <- par(mfrow = c(2,2))
z <- zoo(cbind(a = 1:5, b = 2:6, c = 3:7, d = 4:8))

for(cn in colnames(z)) plot(z[, cn], main = cn, ylim = range(z))

par(opar)

On Feb 8, 2008 3:38 PM, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> If you want some on one page and others on another page
> issue two plot.zoo commands.
>
> plot(z[,1:2])
> plot(z[,3:4])
>
> Also look at xyplot.zoo which gives you the facilities of lattice xyplot
> with zoo objects.
>
>
>
> On Feb 8, 2008 3:15 PM, MAB <MichelBeck at sbcglobal.net> wrote:
> > MAB <MichelBeck <at> sbcglobal.Net> writes:
> >
> > Hi!
> >
> > When I plot the time-series in a zoo object using plot.zoo they are all drawn
> > successively on the same graphics device. If there are more than 10 series
> > this is not legible. With the "standard" plot function I could use the
> > graphics parameter mfcol or mfrow to specify the number of plot
> > screens/pannels per device.
> >
> > How can I do this with zoo?
> >
> > Thanks,
> >
> > Michel
> >
> > PS How do I open multiple on-screen graphics device at one time to plot a
> > large number of time-series? Within a plotting loop an additional device
> > should open each time the maximum number of plot screens/pannels specified per
> > device is reached?
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From ahala2000 at yahoo.com  Sat Feb  9 16:05:28 2008
From: ahala2000 at yahoo.com (elton wang)
Date: Sat, 9 Feb 2008 07:05:28 -0800 (PST)
Subject: [R-SIG-Finance] Simulate the stock market for back testing
	strategy ---R bootstrap function
In-Reply-To: <47AC6ED9.6080603@braverock.com>
Message-ID: <921973.3890.qm@web31401.mail.mud.yahoo.com>

Thanks for Brian's reply.
to make this  more relevant to this list, what
functions in R can do bootstrap resampling while
keeping the autocorrelation in the original data? (I
only know function of sample()). Would this resmapled
data do any good on back testing? 

Thanks!




--- "Brian G. Peterson" <brian at braverock.com> wrote:

> elton wang wrote:
> > Here is a beginner question:
> > what would be your perferred method if we want to
> > simulate the stock market for back testing a
> trading
> > strategy? 
> > Using sp500 daily data as example, if given the
> > knowledge that historical data has time varying
> > volatility, autocorrelations etc? just fitting a
> > GARCH(1,1) or doing historical resampling? 
> (simply
> > divided the data to in-sample and out sample may
> not
> > be sufficient, am I right?)
> 
> You've bitten off one of the most complex and
> studied problems in finance.
> 
> Kalman filtering is often applied to build bands and
> trends, as are 
> straightforward standard deviation based measures
> such as "Bollinger bands".
> 
> Any of the AR methods ARMA, ARIMA, GARCH allow for
> time-varying changes 
> in level and volatility.
> 
> Refinement of those models generally involves EMM or
> Bayesian evolution 
> of the moments.
> 
> These can all be used as one- or multiple-
> step-ahead prediction methods.
> 
> In general, these predictions would be used as
> inputs to *create* a 
> trading strategy.  You would then backtest your
> strategy by setting up a 
> "learning period" (length depending on the frequency
> of your data), and 
> then letting the model evolve on an out-of-sample
> basis (by making one 
> step ahead or similar predictions).
> 
> If you then wanted to further test your models, you
> could fit various 
> distributions to historical data and simulate
> historical series from 
> these distributions.  I'm not really a fan of the
> pure simulation 
> approach unless you are very careful and know what
> you're doing, because 
> there is a huge amount of model risk (risk that you
> will mis-specify 
> starting parameters and therefore get worthless
> results) involved in 
> these pure simulation approaches.
> 
> Many Bayesian (and other Monte Carlo) methods use
> simulation to inform 
> their predictions, but this is different than
> constructing purely 
> hypothetical historical series to test a model
> against.
> 
> Regards,
> 
>     - Brian
> 



      ____________________________________________________________________________________
Be a better friend, newshound, and


From edd at debian.org  Sat Feb  9 16:39:08 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 9 Feb 2008 09:39:08 -0600
Subject: [R-SIG-Finance] Simulate the stock market for back
	testing	strategy ---R bootstrap function
In-Reply-To: <921973.3890.qm@web31401.mail.mud.yahoo.com>
References: <47AC6ED9.6080603@braverock.com>
	<921973.3890.qm@web31401.mail.mud.yahoo.com>
Message-ID: <18349.51484.216609.467063@ron.nulle.part>


On 9 February 2008 at 07:05, elton wang wrote:
| Thanks for Brian's reply.
| to make this  more relevant to this list, what
| functions in R can do bootstrap resampling while
| keeping the autocorrelation in the original data? (I
| only know function of sample()). Would this resmapled
| data do any good on back testing? 

No. 

But any decent book on bootstrapping mentions the problem, and many theses
and papers were (are ?) written on the issue. I haven't looked in a while, 
but 'block bootstrap' once was a popular idea for this. And an ad-hoc method
I used five or six years ago for low-frequency (monthly) data was to sample
in two stages 
	first sample an integer (say between 1 and 6) to determine how 'large'
		a chunk I would fetch
	then sample an integer between 1 and N to determine where I pick the
		chunk from
and re-constitute resample series this way.  As I said, 'ad-hoc'.  There are
many other ways.   But don't do just sample() as it is guaranteed to break
any possible structure in the correlation your data.

Hth, Dirk

-- 
Three out of two people have difficulties with fractions.


From brian at braverock.com  Sat Feb  9 16:49:50 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Sat, 09 Feb 2008 09:49:50 -0600
Subject: [R-SIG-Finance] Simulate the stock market for
 back	testing	strategy ---R bootstrap function
In-Reply-To: <18349.51484.216609.467063@ron.nulle.part>
References: <47AC6ED9.6080603@braverock.com>	<921973.3890.qm@web31401.mail.mud.yahoo.com>
	<18349.51484.216609.467063@ron.nulle.part>
Message-ID: <47ADCB9E.2070004@braverock.com>

Dirk Eddelbuettel wrote:
> On 9 February 2008 at 07:05, elton wang wrote:
> | Thanks for Brian's reply.
> | to make this  more relevant to this list, what
> | functions in R can do bootstrap resampling while
> | keeping the autocorrelation in the original data? (I
> | only know function of sample()). Would this resmapled
> | data do any good on back testing? 
> 
> No. 
> 
> But any decent book on bootstrapping mentions the problem, and many theses
> and papers were (are ?) written on the issue. I haven't looked in a while, 
> but 'block bootstrap' once was a popular idea for this. And an ad-hoc method
> I used five or six years ago for low-frequency (monthly) data was to sample
> in two stages 
> 	first sample an integer (say between 1 and 6) to determine how 'large'
> 		a chunk I would fetch
> 	then sample an integer between 1 and N to determine where I pick the
> 		chunk from
> and re-constitute resample series this way.  As I said, 'ad-hoc'.  There are
> many other ways.   But don't do just sample() as it is guaranteed to break
> any possible structure in the correlation your data.

A block bootstrap for time series is implemented in a slightly more 
robust manner than that described by Dirk above in the function 
tsbootstrap(tseries)

There are a number of other bootstrap methods available in package 
"boot" and corresponding function "boot", but I haven't examined these 
in detail for their tuning or applicability in time series.

I think I laid out some basic steps of building a trading model on 
actual historical data in my prior email.  Simulated data (via 
resampling or any other method) after the point where you have a target 
model is only a validator of the model, not the starting point, or 
you're almost certain to get worthless results.

Regards,

   - Brian


From spencer.graves at pdf.com  Sat Feb  9 19:08:59 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 09 Feb 2008 10:08:59 -0800
Subject: [R-SIG-Finance] useR! 2008 Dortmund, August 11-14
In-Reply-To: <47A91324.90309@pdf.com>
References: <47A91324.90309@pdf.com>
Message-ID: <47ADEC3B.5020408@pdf.com>

     This is just to remind you that this year's UseR! conference is
schedule for August 12-14 in Dortmund, with preconference tutorials on
August 11.  For details, see the conference web site
"www.statistik.uni-dortmund.de/useR-2008" (or www.r-project.org ->
News:  UseR! 2008).


PRECONFERENCE TUTORIAL ON "Analysis of Integrated and
Co-integrated Time Series".

       Many of you may be interested to know that Bernhard Pfaff is
scheduled to give a half-day tutorial August 11 on "Analysis of
Integrated and Co-integrated Time Series".  He is working on a second
edition of his 2006 book on that subject.  Whether or not the new 
edition is available in time for this tutorial, many of you will 
appreciate this opportunity to acquaint yourself with some of the most 
recent research results in this area and how they are implemented in R 
and relevant CRAN-packages.


DEADLINE FOR ABSTRACT SUBMISSION AND EARLY REGISTRATION

       The deadline for abstract submission and early (discount)
registration is March 31.  I hope we will get enough presentations on 
finance and time series topics to organize separate sessions on 
computational finance and time series analysis.  However, to do that, we 
must have people volunteering to to give relevant talks.

       I hope to see you in Dortmund.

       Spencer Graves


From adrian_d at eskimo.com  Sun Feb 10 16:21:37 2008
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Sun, 10 Feb 2008 07:21:37 -0800 (PST)
Subject: [R-SIG-Finance] Simulate the stock market for back testing
 strategy ---R bootstrap function
In-Reply-To: <921973.3890.qm@web31401.mail.mud.yahoo.com>
References: <921973.3890.qm@web31401.mail.mud.yahoo.com>
Message-ID: <Pine.SUN.4.58.0802100719020.28232@eskimo.com>


You may want to check the package meboot and the underlying theory.  The
approach does maintain the dependence structure of your time series and
works both in an univariate or multivariate setting.

Adrian


On Sat, 9 Feb 2008, elton wang wrote:

> Thanks for Brian's reply.
> to make this  more relevant to this list, what
> functions in R can do bootstrap resampling while
> keeping the autocorrelation in the original data? (I
> only know function of sample()). Would this resmapled
> data do any good on back testing?
>
> Thanks!
>
>
>
>
> --- "Brian G. Peterson" <brian at braverock.com> wrote:
>
> > elton wang wrote:
> > > Here is a beginner question:
> > > what would be your perferred method if we want to
> > > simulate the stock market for back testing a
> > trading
> > > strategy?
> > > Using sp500 daily data as example, if given the
> > > knowledge that historical data has time varying
> > > volatility, autocorrelations etc? just fitting a
> > > GARCH(1,1) or doing historical resampling?
> > (simply
> > > divided the data to in-sample and out sample may
> > not
> > > be sufficient, am I right?)
> >
> > You've bitten off one of the most complex and
> > studied problems in finance.
> >
> > Kalman filtering is often applied to build bands and
> > trends, as are
> > straightforward standard deviation based measures
> > such as "Bollinger bands".
> >
> > Any of the AR methods ARMA, ARIMA, GARCH allow for
> > time-varying changes
> > in level and volatility.
> >
> > Refinement of those models generally involves EMM or
> > Bayesian evolution
> > of the moments.
> >
> > These can all be used as one- or multiple-
> > step-ahead prediction methods.
> >
> > In general, these predictions would be used as
> > inputs to *create* a
> > trading strategy.  You would then backtest your
> > strategy by setting up a
> > "learning period" (length depending on the frequency
> > of your data), and
> > then letting the model evolve on an out-of-sample
> > basis (by making one
> > step ahead or similar predictions).
> >
> > If you then wanted to further test your models, you
> > could fit various
> > distributions to historical data and simulate
> > historical series from
> > these distributions.  I'm not really a fan of the
> > pure simulation
> > approach unless you are very careful and know what
> > you're doing, because
> > there is a huge amount of model risk (risk that you
> > will mis-specify
> > starting parameters and therefore get worthless
> > results) involved in
> > these pure simulation approaches.
> >
> > Many Bayesian (and other Monte Carlo) methods use
> > simulation to inform
> > their predictions, but this is different than
> > constructing purely
> > hypothetical historical series to test a model
> > against.
> >
> > Regards,
> >
> >     - Brian
> >
>
>
>
>       ____________________________________________________________________________________
> Be a better friend, newshound, and
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From frednovo at pipeline.com  Sun Feb 10 18:42:25 2008
From: frednovo at pipeline.com (Frederick Novomestky)
Date: Sun, 10 Feb 2008 12:42:25 -0500
Subject: [R-SIG-Finance] Simulate the stock market for
	back	testing	strategy ---R bootstrap function
In-Reply-To: <47ADCB9E.2070004@braverock.com>
References: <47AC6ED9.6080603@braverock.com>
	<921973.3890.qm@web31401.mail.mud.yahoo.com>
	<18349.51484.216609.467063@ron.nulle.part>
	<47ADCB9E.2070004@braverock.com>
Message-ID: <6.2.1.2.2.20080210124035.029df7e0@pop.pipeline.com>

To all:

The best reference that I am aware of on boostrap methods, in general, and 
block resampling iw the Davison and Hinkley book, Bootstrap Methods and 
their Applications.  I have used block resampling on vectors of asset class 
returns and it works quite well.

Sincerest regards,

Fred Novomestky
(also Industry Professor Finance and Risk Engineering, Polytechnic 
Unversity, Brooklyn, NY 11201 )

At 10:49 AM 2/9/2008, Brian G. Peterson wrote:
>Dirk Eddelbuettel wrote:
> > On 9 February 2008 at 07:05, elton wang wrote:
> > | Thanks for Brian's reply.
> > | to make this  more relevant to this list, what
> > | functions in R can do bootstrap resampling while
> > | keeping the autocorrelation in the original data? (I
> > | only know function of sample()). Would this resmapled
> > | data do any good on back testing?
> >
> > No.
> >
> > But any decent book on bootstrapping mentions the problem, and many theses
> > and papers were (are ?) written on the issue. I haven't looked in a while,
> > but 'block bootstrap' once was a popular idea for this. And an ad-hoc 
> method
> > I used five or six years ago for low-frequency (monthly) data was to sample
> > in two stages
> >       first sample an integer (say between 1 and 6) to determine how 
> 'large'
> >               a chunk I would fetch
> >       then sample an integer between 1 and N to determine where I pick the
> >               chunk from
> > and re-constitute resample series this way.  As I said, 
> 'ad-hoc'.  There are
> > many other ways.   But don't do just sample() as it is guaranteed to break
> > any possible structure in the correlation your data.
>
>A block bootstrap for time series is implemented in a slightly more
>robust manner than that described by Dirk above in the function
>tsbootstrap(tseries)
>
>There are a number of other bootstrap methods available in package
>"boot" and corresponding function "boot", but I haven't examined these
>in detail for their tuning or applicability in time series.
>
>I think I laid out some basic steps of building a trading model on
>actual historical data in my prior email.  Simulated data (via
>resampling or any other method) after the point where you have a target
>model is only a validator of the model, not the starting point, or
>you're almost certain to get worthless results.
>
>Regards,
>
>    - Brian
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only.
>-- If you want to post, subscribe first.

Frederick Novomestky, Ph.D.
Novomestky Associates LLC
41 Eastover Drive
East Northport, NY 11731-4330
Vox: 1.631.368.0701
Fax: 1.631.368.1696
URL: http://www.novoassoc.com

Confidentiality Notice: This electronic mail transmissio...{{dropped:10}}


From CVorlow at eurobank.gr  Mon Feb 11 16:22:17 2008
From: CVorlow at eurobank.gr (Vorlow Constantinos)
Date: Mon, 11 Feb 2008 17:22:17 +0200
Subject: [R-SIG-Finance] timeSeries
Message-ID: <93843C113DD8914BB1A9A63878E8918C01C0C5CD@EH002EXC.eurobank.efg.gr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080211/ffb91951/attachment.pl 

From CVorlow at eurobank.gr  Mon Feb 11 16:33:34 2008
From: CVorlow at eurobank.gr (Vorlow Constantinos)
Date: Mon, 11 Feb 2008 17:33:34 +0200
Subject: [R-SIG-Finance] timeSeries
In-Reply-To: <93843C113DD8914BB1A9A63878E8918C01C0C5CD@EH002EXC.eurobank.efg.gr>
Message-ID: <93843C113DD8914BB1A9A63878E8918C01C0C5CE@EH002EXC.eurobank.efg.gr>

Dear all,

I am experimenting with Rmetrics and the fImport library...

DJret<-(yahooSeries(symbols = c("^DJI"), from = NULL, to = NULL, 
    nDaysBack = 408, quote = c("Close"), 
    aggregation = c("d"), returnClass = c("timeSeries"), getReturns =
TRUE))

My question is:

Can you run stats, regressions etc. on a "timeSeries" object?   

Also

If you choose to download the data as a "ts" object

DJret<-(yahooSeries(symbols = c("^DJI"), from = NULL, to = NULL, 
    nDaysBack = 408, quote = c("Close"), 
    aggregation = c("d"), returnClass = c("ts"), getReturns = TRUE))

everythings works fine though I can't gate the dates to display on a ts
plot...

Any clues?


I found out that trying to plot some timeSeries objects and calling a
barplot funcion, causes R (eventually) to crash...

Has anybody else experienced this problem?

Thanks in advance for your time,
Costas

platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          6.1                         
year           2007                        
month          11                          
day            26                          
svn rev        43537                       
language       R                           
version.string R version 2.6.1 (2007-11-26)
 



P Think before you print.

Disclaimer:
This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.


From Achim.Zeileis at wu-wien.ac.at  Mon Feb 11 17:19:11 2008
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 11 Feb 2008 17:19:11 +0100 (CET)
Subject: [R-SIG-Finance] timeSeries
Message-ID: <Pine.LNX.4.44.0802111718480.15388-100000@disco.wu-wien.ac.at>

On Mon, 11 Feb 2008, Vorlow Constantinos wrote:

> I am experimenting with Rmetrics and the fImport library...

The usual pointer: these are *packages* not *libraries*.

> Can you run stats, regressions etc. on a "timeSeries" object?

Another pointer as usual: What exactly do you mean by this? Provide a
better example. See also
  http://www.R-project.org/posting-guide.html

> If you choose to download the data as a "ts" object
> everythings works fine though I can't gate the dates to display on a ts
> plot...

...because they are not contained in the "ts" object. (The point about
time series formats such as "timeSeries" or "zoo" is that they can store
such information whereas "ts" cannot.)

> Any clues?

If you want to run a linear regression model (not sure what you mean by
"stats, regression etc."), the package "fRegression" (which has some
functionality for "timeSeries" objects) or the packages "dyn" or "dynlm"
(working with "zoo" objects) might be of help.

An example for an AR(1) OLS regression with dynlm/zoo:

## close prices
library("tseries")
dj <- get.hist.quote("^DJI", start = "2007-01-01", quote = "Close")

## returns
djr <- diff(log(dj))
plot(djr)

## AR(1) regression
library("dynlm")
fm <- dynlm(djr ~ L(djr))
summary(fm)

> I found out that trying to plot some timeSeries objects and calling a
> barplot funcion, causes R (eventually) to crash...

How should we be able to reproduce this?? Again, I can only guess and
  barplot(DJret)
just produces a correct error message.
Z


From timh at insightful.com  Mon Feb 11 19:23:24 2008
From: timh at insightful.com (Tim Hesterberg)
Date: Mon, 11 Feb 2008 10:23:24 -0800
Subject: [R-SIG-Finance] Simulate the stock market for
	back	testing	strategy ---R bootstrap function
In-Reply-To: <921973.3890.qm@web31401.mail.mud.yahoo.com> (message from elton
	wang on Sat, 9 Feb 2008 07:05:28 -0800 (PST))
References: <921973.3890.qm@web31401.mail.mud.yahoo.com>
Message-ID: <uodanl5cz.fsf@insightful.com>

>...what functions in R can do bootstrap resampling while
>keeping the autocorrelation in the original data? (I
>only know function of sample()). Would this resmapled
>data do any good on back testing? 

The two most common bootstrap approaches for time series are:
* block bootstrap
* fitting a model (e.g. ARIMA) and bootstrapping residuals/innovations
  from that model; use the bootstrapped innovations and fitted model
  to construct new series.

These can be combined:
- fit a model,
- block-bootstrap the residuals from that model.
The model could be simpler than would be necessary for a purely
model-based approach, as long as it captures the bulk of the
correlation structure; the block bootstrap would help capture the
rest.

Another alternative is the matched-block bootstrap:
Edward Carlstein, Kim-Anh Do, Peter Hall, Tim Hesterberg, and Hans
R. Kuensch, "Matched-Block Bootstrap for Dependent Data", Bernoulli,
4(3), 1998, 305-328.
A tech report with more details is:
Hesterberg, Tim C. (1997), "Matched-Block Bootstrap for Long Memory
Processes", Technical Report No. 66, Research Department, MathSoft,
Inc. 1700 Westlake Ave. N., Suite 500, Seattle, WA 98109.
http://www.insightful.com/Hesterberg/articles/tech66-matchBlock.pdf

Dirk Eddelbuettel mentioned a block bootstrap with blocks of varying
length, 1 to 6.  In general, a block bootstrap with a fixed block
length is better than one with random lengths.  The main shortcoming
of a block bootstrap is that it loses correlations at block
boundaries.  With a fixed block length, of say length 10, you lose
1/10 of the first-order autocorrelations or other dependencies, 2/10
of the second order, etc. and all of the autocorrelations of order 11
or higher.  With random block lengths, say 1-19, you lose more
low-order autocorrelations, while saving some of the autocorrelations
of order 11-19 - but these are typically less important.

Also, block lengths should be longer.  A quick rule of thumb is
sqrt(n).  There are more rigorous rules, but they are data-dependent.

I would be quite leery of a block bootstrap for back testing stock
trading.  Losing dependence at block boundaries results in a
constructed series that may differ substantially from reality.

You also need to beware of reproducing historical artifacts by doing
simple bootstraps.  If Ford lost value during the historical period,
and you do simple bootstraps without adjustment, then Ford will tend
to lose value in your bootstrap samples.  Other stocks that went up
historically would gain in the bootstrap samples.  This can distort
the evaluation of trading strategies.

I discuss some of this in the bootstrap short course I teach.

Tim Hesterberg

========================================================
| Tim Hesterberg       Senior Research Scientist       |
| timh at insightful.com  Insightful Corp.                |
| (206)802-2319        1700 Westlake Ave. N, Suite 500 |
| (206)283-8691 (fax)  Seattle, WA 98109-3044, U.S.A.  |
|                      www.insightful.com/Hesterberg   |
========================================================
Download S+Resample from www.insightful.com/downloads/libraries

Advanced Programming in S-PLUS: San Antonio TX, March 26-27, 2008.
Bootstrap Methods and Permutation Tests: San Antonio, March 28, 2008.


From tom.soyer at gmail.com  Tue Feb 12 04:05:52 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Mon, 11 Feb 2008 21:05:52 -0600
Subject: [R-SIG-Finance] garch vs garchFit - minimum sample size
Message-ID: <65cc7bdf0802111905u57986f7m1abaa9886cd9dde9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080211/c8c1747a/attachment.pl 

From joe at gnacademy.org  Tue Feb 12 08:01:03 2008
From: joe at gnacademy.org (Joseph Wang)
Date: Tue, 12 Feb 2008 02:01:03 -0500
Subject: [R-SIG-Finance] R class accessor problem in 2.6 - FIXED
In-Reply-To: <200802031742.11535.joe@gnacademy.org>
References: <002101c85fbd$8a8c3040$0ac0a8c0@MightyMini>
	<18331.24145.498932.596612@ron.nulle.part>
	<200802031742.11535.joe@gnacademy.org>
Message-ID: <200802120201.03704.joe@gnacademy.org>

The reason R-swig was not working on 2.6 was that the C code was not setting 
the S4 flag.  On 2.5, R would still treat it as an S4 class, but in 2.6, it 
is necessary to have this flag set for it to be treated as S4.

I've checked in the fix to swig-trunk.  There were no C coding changes, just a 
new r.swg file.  The warning message for 2.6 has also been removed.


From jeff.a.ryan at gmail.com  Tue Feb 12 15:34:34 2008
From: jeff.a.ryan at gmail.com (jeff.a.ryan at gmail.com)
Date: Tue, 12 Feb 2008 14:34:34 +0000
Subject: [R-SIG-Finance] timeSeries
In-Reply-To: <93843C113DD8914BB1A9A63878E8918C01C0C5CD@EH002EXC.eurobank.efg.gr>
References: <93843C113DD8914BB1A9A63878E8918C01C0C5CD@EH002EXC.eurobank.efg.gr>
Message-ID: <179601965-1202827044-cardhu_decombobulator_blackberry.rim.net-115466457-@bxe141.bisx.prod.on.blackberry>

A simple solution to making all common R time series classes into a more 'data.frame/matrix/zoo' style object is available in the new 'xts' package on CRAN with the most recent source and windows builds at http://R-forge.r-project.org/projects/xts

'as.xts' converts the original class into an 'xts' object that contains information about the original objects structure that is usually lost during standard 'as' operations.

A quick example using the quantmod package:

library(quantmod) #load the quantmod package-this loads xts

DJI.timeSeries <- getSymbols('^DJI', src='yahoo', ret='timeSeries', auto.assign=FALSE)

DJI.ts <- getSymbols('^DJI', src='yahoo', ret='ts', auto.assign=FALSE)

class(DJI.timeSeries). # a 'timeSeries' object.
class(DJI.ts) # a 'ts' object

str(DJI.timeSeries)
str(DJI.ts)

# converting both to 'xts' yields
# objects that are nearly identical
# apart from the xtsAttributes - these hold
# non-time-series attributes belonging to the original
# class, or ones added by the user.

str(as.xts(DJI.timeSeries))
str(as.xts(DJI.ts))

At this point 'xts' objects behave much like any standard data.frame, matrix or, most closely, zoo object.  They have some unique user 'xts' methods but all standard 'zoo' methods will work (it just extends 'zoo')

The notable differences:

* The rownames are a character string of the time-index, created automatically.

* The index is a true time based one - meaning it is of class 'Date', 'POSIXct', 'chron', 'timeDate', 'yearmon', or 'yearqtr'.  You can view/change this with 'indexClass' and 'convertIndex'

* the original data to allow for conversion back to the original class is preserved internally.  A call to 'reclass' is all that is needed.  This is handly to allow for temporary use of 'xts'-style subsetting while letting the user continue using his preferred data object.  This is also very useful inside functions to allow for automatic handling of a variety of input objects.  chartSeries in quantmod and to.period in xts use this approach.

str(DJI.timeSeries)
str(as.xts(DJI.timeSeries))
str(reclass(as.xts(DJI.timeSeries)))

An additional benefit to using 'xts' includes explicit time-based subsetting, as well as some additional functionality that utilizes the time-based index.

To get the first 3 months of the series:
as.xts(DJI.timeSeries)['2007-01::2007-03']

The rest of the series:
as.xts(DJI.timeSeries)['2007-04::']

The package is under active development - but the purpose of simplifying the workflow regardless of the user's data object choice is the overall goal.  To that end, the r-forge version could be exactly what you need - soon to be pushed to CRAN.

Jeff

PS. I apologize if some of the above syntax is mis-typed. I am unable to get to a computer to double check.
Sent via BlackBerry from T-Mobile

-----Original Message-----
From: "Vorlow Constantinos" <CVorlow at eurobank.gr>

Date: Mon, 11 Feb 2008 17:22:17 
To:<r-sig-finance at stat.math.ethz.ch>
Subject: [R-SIG-Finance] timeSeries


Dear all,
 
I am experimenting with Rmetrics and the fImport library...
 
DJret<-(yahooSeries(symbols = c("^DJI"), from = NULL, to = NULL, 
    nDaysBack = 408, quote = c("Close"), 
    aggregation = c("d"), returnClass = c("timeSeries"), getReturns =
TRUE))

My question is:
 
Can you run stats, regressions etc. on a "timeSeries" object?   
 
Also
 
If you choose to download the data as a "ts" object
 
DJret<-(yahooSeries(symbols = c("^DJI"), from = NULL, to = NULL, 
    nDaysBack = 408, quote = c("Close"), 
    aggregation = c("d"), returnClass = c("ts"), getReturns = TRUE))
 
everythings works fine though I can't gate the dates to display on a ts
plot...
 
Any clues?
 
 
I found out that trying to plot some timeSeries objects and calling a
barplot funcion, causes R (eventually) to crash...
 
Has anybody else experienced this problem?
 
Thanks in advance for your time,
Costas
 
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          6.1                         
year           2007                        
month          11                          
day            26                          
svn rev        43537                       
language       R                           
version.string R version 2.6.1 (2007-11-26)
 

P Think before you print.

Disclaimer:
This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.


	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

From ecjbosu at aol.com  Tue Feb 12 17:29:55 2008
From: ecjbosu at aol.com (Joe Byers)
Date: Tue, 12 Feb 2008 16:29:55 +0000 (UTC)
Subject: [R-SIG-Finance] holidayNYSE missing some
References: <20080204115153.13a247f4@yankee-laptop>
Message-ID: <loom.20080212T162838-798@post.gmane.org>

Yohan Chalabi <chalabi <at> phys.ethz.ch> writes:

> 
> Hi John,
> 
> for whatever reason I missed this thread in September 2007. 
> Thanks to Lukas Elmiger how pointed out directly to me this problem, I
> made the following changes in the development branch of fCalendar
> (https://svn.r-project.org/Rmetrics/trunk/fCalendar/):
> 
> - fixed the rule in holidayNYSE : After July 3, 1959, move Saturday
> holidays to Friday ... except if at the end of monthly/yearly accounting  
> period this is the last business day of a month. 
> 
> - fixed the conditions for the year 1970 in holidayNYSE
> 
> - fixed USWashingtonsBirthday() before 1971
> 
> Regards,
> Yohan
> 
> 

Yohan,

I am working on adding Haug's Multiasset greeks to the fExotics package.
  Are you interested in adding these?

Joe


From spencer.graves at pdf.com  Tue Feb 12 17:53:59 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 12 Feb 2008 08:53:59 -0800
Subject: [R-SIG-Finance] garch vs garchFit - minimum sample size
In-Reply-To: <65cc7bdf0802111905u57986f7m1abaa9886cd9dde9@mail.gmail.com>
References: <65cc7bdf0802111905u57986f7m1abaa9886cd9dde9@mail.gmail.com>
Message-ID: <47B1CF27.4060604@pdf.com>

Why do you say that 'garch' requires more observations than 'garchFit'?  
Can you outline documentation or tests you've performed? 

Spencer

tom soyer wrote:
> Hi,
>
> It seems that the minimum sample size required by garch is much larger than
> garchFit, does anyone know why? I am guessing between 1,000 and 2,000 for
> garch and ~500 for garchFit. Does anyone know the exact minimum sample size
> for each?
>
> Thanks!
>
>


From tom.soyer at gmail.com  Tue Feb 12 18:10:54 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Tue, 12 Feb 2008 11:10:54 -0600
Subject: [R-SIG-Finance] garch vs garchFit - minimum sample size
In-Reply-To: <47B1CF27.4060604@pdf.com>
References: <65cc7bdf0802111905u57986f7m1abaa9886cd9dde9@mail.gmail.com>
	<47B1CF27.4060604@pdf.com>
Message-ID: <65cc7bdf0802120910y5fc019a5s2c93cdd73367a509@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080212/42ca9c68/attachment.pl 

From tom.soyer at gmail.com  Wed Feb 13 00:03:18 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Tue, 12 Feb 2008 17:03:18 -0600
Subject: [R-SIG-Finance] How can I see a list of the built-in data sets in
	fBasics
Message-ID: <65cc7bdf0802121503s2df7f811l33619016c22922a3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080212/a5839db8/attachment.pl 

From brian at braverock.com  Wed Feb 13 00:13:29 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 12 Feb 2008 17:13:29 -0600
Subject: [R-SIG-Finance] How can I see a list of the built-in data sets
 in	fBasics
In-Reply-To: <65cc7bdf0802121503s2df7f811l33619016c22922a3@mail.gmail.com>
References: <65cc7bdf0802121503s2df7f811l33619016c22922a3@mail.gmail.com>
Message-ID: <47B22819.3010602@braverock.com>

tom soyer wrote:
> I loaded fBasics, and I can access the data sets from MoFiTs. But I am
> wondering if it is possible to see what's in the data set from the R
> console. Right now I just look up the R documentation on ZivotWangData, but
> is there a R command that would give me a list of the various data sets in
> the ZivotWangData?

The documentation is the best place to start, and can be referenced from 
the R console.  Most of the data included with RMetrics has been moved 
to the fEcofin package, so you may wish to look at the documentation for 
fEcofin.

Regards,

   - Brian


From brian at braverock.com  Wed Feb 13 00:29:02 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 12 Feb 2008 17:29:02 -0600
Subject: [R-SIG-Finance] How can I see a list of the built-in data sets
 in	fBasics
In-Reply-To: <47B22819.3010602@braverock.com>
References: <65cc7bdf0802121503s2df7f811l33619016c22922a3@mail.gmail.com>
	<47B22819.3010602@braverock.com>
Message-ID: <47B22BBE.2050703@braverock.com>

Brian G. Peterson wrote:
> tom soyer wrote:
>> I loaded fBasics, and I can access the data sets from MoFiTs. But I am
>> wondering if it is possible to see what's in the data set from the R
>> console. Right now I just look up the R documentation on ZivotWangData, but
>> is there a R command that would give me a list of the various data sets in
>> the ZivotWangData?
> 
> The documentation is the best place to start, and can be referenced from 
> the R console.  Most of the data included with RMetrics has been moved 
> to the fEcofin package, so you may wish to look at the documentation for 
> fEcofin.

one more thing:

?data

Regards,

  - Brian


From tom.soyer at gmail.com  Wed Feb 13 04:40:35 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Tue, 12 Feb 2008 21:40:35 -0600
Subject: [R-SIG-Finance] question about garchPredictor(fGarch)
Message-ID: <65cc7bdf0802121940h431b4aa2j4dd020505a489fa0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080212/b6bacf25/attachment.pl 

From icos.atropa at gmail.com  Wed Feb 13 14:17:16 2008
From: icos.atropa at gmail.com (icosa atropa)
Date: Wed, 13 Feb 2008 06:17:16 -0700
Subject: [R-SIG-Finance] timeseries - xst vs. dataframe?
Message-ID: <681d07c20802130517vecf18d5te922433e3e0d4577@mail.gmail.com>

I've enjoyed your prosthelytizing of xts of late... the syntax you
just highlighted looks appealing.  I've found zoo powerful, and xts
appears to be a useful extension.

One question r.e. xts vs. dataframes: the lack of $a and [["a"]]
notation in zoo has always struck me as a cumbersome difference from
dataframes.  Is "list" syntax planned for inclusion in xst? At
present, column numbering (test.zoo[,1]) seems the best alternative.

Since as.data.frame(test.zoo)$a appears to recover the core data, it
sounds sensible for "test.zoo$a" to extract an object containing the
index and the named column.  Does this break anything?
e.g. :

test.df = data.frame(a=1:5, b=2*(1:5))
test.df$a
#[1] 1 2 3 4 5

index = Sys.time() + 60*1:5
test.zoo = zoo(test.df, order.by=index)
test.zoo$a
#NULL

test.zoo[1:2,1]
# 2008-02-13 05:54:40 2008-02-13 05:55:40
#                  1                   2

coredata(test.zoo)$a
#NULL

as.data.frame(test.zoo)$a
# [1] 1 2 3 4 5

thanks and best,
christian

> At this point 'xts' objects behave much like any standard data.frame, matrix or, most closely, zoo object.  They have some unique user 'xts' methods but all standard 'zoo' methods will work (it just extends 'zoo')


From ggrothendieck at gmail.com  Wed Feb 13 15:16:17 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 13 Feb 2008 09:16:17 -0500
Subject: [R-SIG-Finance] timeseries - xst vs. dataframe?
In-Reply-To: <681d07c20802130517vecf18d5te922433e3e0d4577@mail.gmail.com>
References: <681d07c20802130517vecf18d5te922433e3e0d4577@mail.gmail.com>
Message-ID: <971536df0802130616l1bd2e256x327ca023816f7e71@mail.gmail.com>

On Feb 13, 2008 8:17 AM, icosa atropa <icos.atropa at gmail.com> wrote:
> I've enjoyed your prosthelytizing of xts of late... the syntax you
> just highlighted looks appealing.  I've found zoo powerful, and xts
> appears to be a useful extension.
>
> One question r.e. xts vs. dataframes: the lack of $a and [["a"]]
> notation in zoo has always struck me as a cumbersome difference from

zoo is modelled on the "ts" class, not on the "data.frame" class.
In R, the way it works is that $ is used on list-based objects
and not on array-based objects.  Of course, your are free to define
and redefine operators as you please and since zoo is an S3 class
its possible to add your own S3 methods.   $ indexing is less
than a dozen lines of code to add to your program:

"$.zoo" <- function(object, x) object[, x]

"$<-.zoo" <- function(object, x, value) {
    stopifnot(length(dim(object)) == 2)
    if (x %in% colnames(object)) object[,x] <- value
    else {
        object <- cbind(object, value)
        colnames(object)[ncol(object)] <- x
    }
    object
}

# test
library(zoo)
z <- zoo(cbind(a = 1:3, b = 4:6))
z$c <- z$b + 1
z$a <- z$b - 1
z

> dataframes.  Is "list" syntax planned for inclusion in xst? At
> present, column numbering (test.zoo[,1]) seems the best alternative.
>
> Since as.data.frame(test.zoo)$a appears to recover the core data, it
> sounds sensible for "test.zoo$a" to extract an object containing the
> index and the named column.  Does this break anything?
> e.g. :
>
> test.df = data.frame(a=1:5, b=2*(1:5))
> test.df$a
> #[1] 1 2 3 4 5
>
> index = Sys.time() + 60*1:5
> test.zoo = zoo(test.df, order.by=index)
> test.zoo$a
> #NULL
>
> test.zoo[1:2,1]
> # 2008-02-13 05:54:40 2008-02-13 05:55:40
> #                  1                   2
>
> coredata(test.zoo)$a
> #NULL
>
> as.data.frame(test.zoo)$a
> # [1] 1 2 3 4 5
>
> thanks and best,
> christian
>
> > At this point 'xts' objects behave much like any standard data.frame, matrix or, most closely, zoo object.  They have some unique user 'xts' methods but all standard 'zoo' methods will work (it just extends 'zoo')
>


From Achim.Zeileis at wu-wien.ac.at  Wed Feb 13 15:43:39 2008
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 13 Feb 2008 15:43:39 +0100 (CET)
Subject: [R-SIG-Finance] timeseries - xst vs. dataframe?
In-Reply-To: <971536df0802130616l1bd2e256x327ca023816f7e71@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0802131537490.15388-100000@disco.wu-wien.ac.at>

On Wed, 13 Feb 2008, Gabor Grothendieck wrote:

> zoo is modelled on the "ts" class, not on the "data.frame" class.
> In R, the way it works is that $ is used on list-based objects
> and not on array-based objects.

True, and this is the explanation why we don't have it at the moment. But
the $ operator might be a convenient addition. And we currently have a few
examples where we are consistent with "ts" but provide further features
for convenience. At the moment, I don't think something dangerous would
happen if we add it - or do I overlook something?

Given that you have already written the code, I would vote for including
it in the package.
Z

> Of course, your are free to define
> and redefine operators as you please and since zoo is an S3 class
> its possible to add your own S3 methods.   $ indexing is less
> than a dozen lines of code to add to your program:
>
> "$.zoo" <- function(object, x) object[, x]
>
> "$<-.zoo" <- function(object, x, value) {
>     stopifnot(length(dim(object)) == 2)
>     if (x %in% colnames(object)) object[,x] <- value
>     else {
>         object <- cbind(object, value)
>         colnames(object)[ncol(object)] <- x
>     }
>     object
> }
>
> # test
> library(zoo)
> z <- zoo(cbind(a = 1:3, b = 4:6))
> z$c <- z$b + 1
> z$a <- z$b - 1
> z
>
> > dataframes.  Is "list" syntax planned for inclusion in xst? At
> > present, column numbering (test.zoo[,1]) seems the best alternative.
> >
> > Since as.data.frame(test.zoo)$a appears to recover the core data, it
> > sounds sensible for "test.zoo$a" to extract an object containing the
> > index and the named column.  Does this break anything?
> > e.g. :
> >
> > test.df = data.frame(a=1:5, b=2*(1:5))
> > test.df$a
> > #[1] 1 2 3 4 5
> >
> > index = Sys.time() + 60*1:5
> > test.zoo = zoo(test.df, order.by=index)
> > test.zoo$a
> > #NULL
> >
> > test.zoo[1:2,1]
> > # 2008-02-13 05:54:40 2008-02-13 05:55:40
> > #                  1                   2
> >
> > coredata(test.zoo)$a
> > #NULL
> >
> > as.data.frame(test.zoo)$a
> > # [1] 1 2 3 4 5
> >
> > thanks and best,
> > christian
> >
> > > At this point 'xts' objects behave much like any standard data.frame, matrix or, most closely, zoo object.  They have some unique user 'xts' methods but all standard 'zoo' methods will work (it just extends 'zoo')
> >
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From jeff.a.ryan at gmail.com  Wed Feb 13 15:48:23 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 13 Feb 2008 08:48:23 -0600
Subject: [R-SIG-Finance] timeseries - xst vs. dataframe?
In-Reply-To: <Pine.LNX.4.44.0802131537490.15388-100000@disco.wu-wien.ac.at>
References: <971536df0802130616l1bd2e256x327ca023816f7e71@mail.gmail.com>
	<Pine.LNX.4.44.0802131537490.15388-100000@disco.wu-wien.ac.at>
Message-ID: <e8e755250802130648h714802cbj77fb168550286f@mail.gmail.com>

Hi everyone,

I agree that the '$' operator seems like a nice addition.  The only
problem that I see is:

from help('$')
...
 The default methods work somewhat differently for atomic vectors,
     matrices/arrays and for recursive (list-like, see 'is.recursive')
     objects.  '$' returns 'NULL' (with a warning) except for recursive
     objects, and is only discussed in the section below on recursive
     objects.  Its use on non-recursive objects was deprecated in R
     2.5.0.
...

Since zoo (and thus xts) is really a matrix/array with attributes - it
seems like it has the chance of breaking something - though where I
can't reasonably imagine.

The flip side to the argument against is that returning a NULL object
seems to be of little value to anything.

Is anyone aware of the reason for it being deprecated for non-recursive objects?

Jeff

On Feb 13, 2008 8:43 AM, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> On Wed, 13 Feb 2008, Gabor Grothendieck wrote:
>
> > zoo is modelled on the "ts" class, not on the "data.frame" class.
> > In R, the way it works is that $ is used on list-based objects
> > and not on array-based objects.
>
> True, and this is the explanation why we don't have it at the moment. But
> the $ operator might be a convenient addition. And we currently have a few
> examples where we are consistent with "ts" but provide further features
> for convenience. At the moment, I don't think something dangerous would
> happen if we add it - or do I overlook something?
>
> Given that you have already written the code, I would vote for including
> it in the package.
> Z
>
>
> > Of course, your are free to define
> > and redefine operators as you please and since zoo is an S3 class
> > its possible to add your own S3 methods.   $ indexing is less
> > than a dozen lines of code to add to your program:
> >
> > "$.zoo" <- function(object, x) object[, x]
> >
> > "$<-.zoo" <- function(object, x, value) {
> >     stopifnot(length(dim(object)) == 2)
> >     if (x %in% colnames(object)) object[,x] <- value
> >     else {
> >         object <- cbind(object, value)
> >         colnames(object)[ncol(object)] <- x
> >     }
> >     object
> > }
> >
> > # test
> > library(zoo)
> > z <- zoo(cbind(a = 1:3, b = 4:6))
> > z$c <- z$b + 1
> > z$a <- z$b - 1
> > z
> >
> > > dataframes.  Is "list" syntax planned for inclusion in xst? At
> > > present, column numbering (test.zoo[,1]) seems the best alternative.
> > >
> > > Since as.data.frame(test.zoo)$a appears to recover the core data, it
> > > sounds sensible for "test.zoo$a" to extract an object containing the
> > > index and the named column.  Does this break anything?
> > > e.g. :
> > >
> > > test.df = data.frame(a=1:5, b=2*(1:5))
> > > test.df$a
> > > #[1] 1 2 3 4 5
> > >
> > > index = Sys.time() + 60*1:5
> > > test.zoo = zoo(test.df, order.by=index)
> > > test.zoo$a
> > > #NULL
> > >
> > > test.zoo[1:2,1]
> > > # 2008-02-13 05:54:40 2008-02-13 05:55:40
> > > #                  1                   2
> > >
> > > coredata(test.zoo)$a
> > > #NULL
> > >
> > > as.data.frame(test.zoo)$a
> > > # [1] 1 2 3 4 5
> > >
> > > thanks and best,
> > > christian
> > >
> > > > At this point 'xts' objects behave much like any standard data.frame, matrix or, most closely, zoo object.  They have some unique user 'xts' methods but all standard 'zoo' methods will work (it just extends 'zoo')
> > >
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
> >
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From spencer.graves at pdf.com  Wed Feb 13 16:40:33 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 13 Feb 2008 07:40:33 -0800
Subject: [R-SIG-Finance] garch vs garchFit - minimum sample size
In-Reply-To: <65cc7bdf0802120910y5fc019a5s2c93cdd73367a509@mail.gmail.com>
References: <65cc7bdf0802111905u57986f7m1abaa9886cd9dde9@mail.gmail.com>	
	<47B1CF27.4060604@pdf.com>
	<65cc7bdf0802120910y5fc019a5s2c93cdd73367a509@mail.gmail.com>
Message-ID: <47B30F71.50107@pdf.com>

      In comparing garch{tseries} with garchFit{fGarch}, it seems that 
the latter is more general, as it allows simultaneous estimation of an 
arma model with possibly nonnormal garch or aparch noise, while 'garch' 
fits only a garch model to a series assumed to have mean zero.  I 
simulated 10,000 observations from the garch(1,1) model given as an 
example in the 'garchSim' help page, and got very similar answers from 
'garch' as from 'garchFit' which I restricted the latter to fit the 
model of the former: 

x10k <- garchSim(n=10000)

summary(fit10k <- garch(x10k))
    Estimate  Std. Error  t value Pr(>|t|)   
a0 1.149e-06   1.467e-07    7.832 4.88e-15 ***
a1 1.032e-01   9.433e-03   10.942  < 2e-16 ***
b1 7.852e-01   2.039e-02   38.499  < 2e-16 ***
# simulated a0=1e-6, a1=0.1, b1=0.8

fit10k. <- garchFit(~garch(1,1), data=x10k, include.mean=FALSE)
summary(fit10k.)
        Estimate  Std. Error  t value Pr(>|t|)   
omega  1.148e-06   1.437e-07     7.99 1.33e-15 ***
alpha1 1.032e-01   9.027e-03    11.44  < 2e-16 ***
beta1  7.853e-01   1.963e-02    39.99  < 2e-16 ***

      With only 100 observations, 'garch' complained 'singular 
information' and quite early with different answers from garchFit: 

summary(fit100 <- garch(x10k[1:100]))
    Estimate  Std. Error  t value Pr(>|t|)
a0 9.842e-06          NA       NA       NA
a1 5.000e-02          NA       NA       NA
b1 5.000e-02          NA       NA       NA

fit100. <- garchFit(~garch(1,1), data=x10k[1:100], include.mean=FALSE)
summary(fit100.)
        Estimate  Std. Error  t value Pr(>|t|)  
omega  2.056e-06   1.677e-06    1.226  0.22022  
alpha1 1.387e-01   1.193e-01    1.163  0.24469  
beta1  6.704e-01   2.045e-01    3.278  0.00105 **

      However, with 500 observations, 'garch' thought it converged and 
again gave answers very similar to garchFit: 

summary(fit500 <- garch(x10k[1:500]))
    Estimate  Std. Error  t value Pr(>|t|)   
a0 1.466e-06   6.432e-07    2.279  0.02265 * 
a1 1.340e-01   4.405e-02    3.042  0.00235 **
b1 7.215e-01   9.292e-02    7.765 8.22e-15 ***
fit500. <- garchFit(~garch(1,1), data=x10k[1:500], include.mean=FALSE)
summary(fit500.)
        Estimate  Std. Error  t value Pr(>|t|)   
omega  1.450e-06   6.162e-07    2.352  0.01867 * 
alpha1 1.340e-01   4.425e-02    3.029  0.00245 **
beta1  7.236e-01   8.430e-02    8.583  < 2e-16 ***

      My conclusion from this is to use 'garchFit'. 

      By the way, the more general syntax for 'garchFit' is illustrated 
by the following: 

library(FinTS)
data(sp500)
library(fGarch)
spFit30.11 <- garchFit(sp500~arma(3,0)+garch(1,1), data=sp500)

      Spencer

tom soyer wrote:
> Spencer, take any data series and run garch vs. garchFit with various 
> sample size, and you will see garch needs a lot more data points to 
> get a good fit. Anyway, it probably doesn't matter if one always use a 
> sample size of >2,000. I was just courious.
>
> On 2/12/08, *Spencer Graves* <spencer.graves at pdf.com 
> <mailto:spencer.graves at pdf.com>> wrote:
>
>     Why do you say that 'garch' requires more observations than
>     'garchFit'?
>     Can you outline documentation or tests you've performed?
>
>     Spencer
>
>     tom soyer wrote:
>     > Hi,
>     >
>     > It seems that the minimum sample size required by garch is much
>     larger than
>     > garchFit, does anyone know why? I am guessing between 1,000 and
>     2,000 for
>     > garch and ~500 for garchFit. Does anyone know the exact minimum
>     sample size
>     > for each?
>     >
>     > Thanks!
>     >
>     >
>
>
>
>
> -- 
> Tom


From Achim.Zeileis at wu-wien.ac.at  Wed Feb 13 18:55:14 2008
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 13 Feb 2008 18:55:14 +0100 (CET)
Subject: [R-SIG-Finance] garch vs garchFit - minimum sample size
In-Reply-To: <47B30F71.50107@pdf.com>
Message-ID: <Pine.LNX.4.44.0802131850160.15388-100000@disco.wu-wien.ac.at>

On Wed, 13 Feb 2008, Spencer Graves wrote:

>       In comparing garch{tseries} with garchFit{fGarch}, it seems that
> the latter is more general, as it allows simultaneous estimation of an
> arma model with possibly nonnormal garch or aparch noise, while 'garch'
> fits only a garch model to a series assumed to have mean zero.  I

One additional difference is that garch() estimates the covariance matrix
of the coefficients using an OPG (outer product of gradients) estimator,
using analytical or numerical gradients, respectively.

grachFit(), on the other hand, provides the Hessian as approximated
numerically by the algorithm employed (if I recall correctly).

Conceptually, it should be possible to provide estfun() and bread()
methods for the "sandwich" package, so that various flavors of estimators
could be computed.
Z

> simulated 10,000 observations from the garch(1,1) model given as an
> example in the 'garchSim' help page, and got very similar answers from
> 'garch' as from 'garchFit' which I restricted the latter to fit the
> model of the former:
>
> x10k <- garchSim(n=10000)
>
> summary(fit10k <- garch(x10k))
>     Estimate  Std. Error  t value Pr(>|t|)
> a0 1.149e-06   1.467e-07    7.832 4.88e-15 ***
> a1 1.032e-01   9.433e-03   10.942  < 2e-16 ***
> b1 7.852e-01   2.039e-02   38.499  < 2e-16 ***
> # simulated a0=1e-6, a1=0.1, b1=0.8
>
> fit10k. <- garchFit(~garch(1,1), data=x10k, include.mean=FALSE)
> summary(fit10k.)
>         Estimate  Std. Error  t value Pr(>|t|)
> omega  1.148e-06   1.437e-07     7.99 1.33e-15 ***
> alpha1 1.032e-01   9.027e-03    11.44  < 2e-16 ***
> beta1  7.853e-01   1.963e-02    39.99  < 2e-16 ***
>
>       With only 100 observations, 'garch' complained 'singular
> information' and quite early with different answers from garchFit:
>
> summary(fit100 <- garch(x10k[1:100]))
>     Estimate  Std. Error  t value Pr(>|t|)
> a0 9.842e-06          NA       NA       NA
> a1 5.000e-02          NA       NA       NA
> b1 5.000e-02          NA       NA       NA
>
> fit100. <- garchFit(~garch(1,1), data=x10k[1:100], include.mean=FALSE)
> summary(fit100.)
>         Estimate  Std. Error  t value Pr(>|t|)
> omega  2.056e-06   1.677e-06    1.226  0.22022
> alpha1 1.387e-01   1.193e-01    1.163  0.24469
> beta1  6.704e-01   2.045e-01    3.278  0.00105 **
>
>       However, with 500 observations, 'garch' thought it converged and
> again gave answers very similar to garchFit:
>
> summary(fit500 <- garch(x10k[1:500]))
>     Estimate  Std. Error  t value Pr(>|t|)
> a0 1.466e-06   6.432e-07    2.279  0.02265 *
> a1 1.340e-01   4.405e-02    3.042  0.00235 **
> b1 7.215e-01   9.292e-02    7.765 8.22e-15 ***
> fit500. <- garchFit(~garch(1,1), data=x10k[1:500], include.mean=FALSE)
> summary(fit500.)
>         Estimate  Std. Error  t value Pr(>|t|)
> omega  1.450e-06   6.162e-07    2.352  0.01867 *
> alpha1 1.340e-01   4.425e-02    3.029  0.00245 **
> beta1  7.236e-01   8.430e-02    8.583  < 2e-16 ***
>
>       My conclusion from this is to use 'garchFit'.
>
>       By the way, the more general syntax for 'garchFit' is illustrated
> by the following:
>
> library(FinTS)
> data(sp500)
> library(fGarch)
> spFit30.11 <- garchFit(sp500~arma(3,0)+garch(1,1), data=sp500)
>
>       Spencer
>
> tom soyer wrote:
> > Spencer, take any data series and run garch vs. garchFit with various
> > sample size, and you will see garch needs a lot more data points to
> > get a good fit. Anyway, it probably doesn't matter if one always use a
> > sample size of >2,000. I was just courious.
> >
> > On 2/12/08, *Spencer Graves* <spencer.graves at pdf.com
> > <mailto:spencer.graves at pdf.com>> wrote:
> >
> >     Why do you say that 'garch' requires more observations than
> >     'garchFit'?
> >     Can you outline documentation or tests you've performed?
> >
> >     Spencer
> >
> >     tom soyer wrote:
> >     > Hi,
> >     >
> >     > It seems that the minimum sample size required by garch is much
> >     larger than
> >     > garchFit, does anyone know why? I am guessing between 1,000 and
> >     2,000 for
> >     > garch and ~500 for garchFit. Does anyone know the exact minimum
> >     sample size
> >     > for each?
> >     >
> >     > Thanks!
> >     >
> >     >
> >
> >
> >
> >
> > --
> > Tom
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>
>


From spencer.graves at pdf.com  Wed Feb 13 20:11:53 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 13 Feb 2008 11:11:53 -0800
Subject: [R-SIG-Finance] garch vs garchFit - minimum sample size
In-Reply-To: <Pine.LNX.4.44.0802131850160.15388-100000@disco.wu-wien.ac.at>
References: <Pine.LNX.4.44.0802131850160.15388-100000@disco.wu-wien.ac.at>
Message-ID: <47B340F9.1000707@pdf.com>

Hi, Achim: 

      Thanks for the reply. 

      Is it fair to say that 'garchFit' is newer and uses a more general 
and robust algorithm? 

      Or are there circumstances under which 'garch' would give better 
answers than 'garchFit'? 

      Thanks again,
      Spencer

Achim Zeileis wrote:
> On Wed, 13 Feb 2008, Spencer Graves wrote:
>
>   
>>       In comparing garch{tseries} with garchFit{fGarch}, it seems that
>> the latter is more general, as it allows simultaneous estimation of an
>> arma model with possibly nonnormal garch or aparch noise, while 'garch'
>> fits only a garch model to a series assumed to have mean zero.  I
>>     
>
> One additional difference is that garch() estimates the covariance matrix
> of the coefficients using an OPG (outer product of gradients) estimator,
> using analytical or numerical gradients, respectively.
>
> grachFit(), on the other hand, provides the Hessian as approximated
> numerically by the algorithm employed (if I recall correctly).
>
> Conceptually, it should be possible to provide estfun() and bread()
> methods for the "sandwich" package, so that various flavors of estimators
> could be computed.
> Z
>
>   
>> simulated 10,000 observations from the garch(1,1) model given as an
>> example in the 'garchSim' help page, and got very similar answers from
>> 'garch' as from 'garchFit' which I restricted the latter to fit the
>> model of the former:
>>
>> x10k <- garchSim(n=10000)
>>
>> summary(fit10k <- garch(x10k))
>>     Estimate  Std. Error  t value Pr(>|t|)
>> a0 1.149e-06   1.467e-07    7.832 4.88e-15 ***
>> a1 1.032e-01   9.433e-03   10.942  < 2e-16 ***
>> b1 7.852e-01   2.039e-02   38.499  < 2e-16 ***
>> # simulated a0=1e-6, a1=0.1, b1=0.8
>>
>> fit10k. <- garchFit(~garch(1,1), data=x10k, include.mean=FALSE)
>> summary(fit10k.)
>>         Estimate  Std. Error  t value Pr(>|t|)
>> omega  1.148e-06   1.437e-07     7.99 1.33e-15 ***
>> alpha1 1.032e-01   9.027e-03    11.44  < 2e-16 ***
>> beta1  7.853e-01   1.963e-02    39.99  < 2e-16 ***
>>
>>       With only 100 observations, 'garch' complained 'singular
>> information' and quite early with different answers from garchFit:
>>
>> summary(fit100 <- garch(x10k[1:100]))
>>     Estimate  Std. Error  t value Pr(>|t|)
>> a0 9.842e-06          NA       NA       NA
>> a1 5.000e-02          NA       NA       NA
>> b1 5.000e-02          NA       NA       NA
>>
>> fit100. <- garchFit(~garch(1,1), data=x10k[1:100], include.mean=FALSE)
>> summary(fit100.)
>>         Estimate  Std. Error  t value Pr(>|t|)
>> omega  2.056e-06   1.677e-06    1.226  0.22022
>> alpha1 1.387e-01   1.193e-01    1.163  0.24469
>> beta1  6.704e-01   2.045e-01    3.278  0.00105 **
>>
>>       However, with 500 observations, 'garch' thought it converged and
>> again gave answers very similar to garchFit:
>>
>> summary(fit500 <- garch(x10k[1:500]))
>>     Estimate  Std. Error  t value Pr(>|t|)
>> a0 1.466e-06   6.432e-07    2.279  0.02265 *
>> a1 1.340e-01   4.405e-02    3.042  0.00235 **
>> b1 7.215e-01   9.292e-02    7.765 8.22e-15 ***
>> fit500. <- garchFit(~garch(1,1), data=x10k[1:500], include.mean=FALSE)
>> summary(fit500.)
>>         Estimate  Std. Error  t value Pr(>|t|)
>> omega  1.450e-06   6.162e-07    2.352  0.01867 *
>> alpha1 1.340e-01   4.425e-02    3.029  0.00245 **
>> beta1  7.236e-01   8.430e-02    8.583  < 2e-16 ***
>>
>>       My conclusion from this is to use 'garchFit'.
>>
>>       By the way, the more general syntax for 'garchFit' is illustrated
>> by the following:
>>
>> library(FinTS)
>> data(sp500)
>> library(fGarch)
>> spFit30.11 <- garchFit(sp500~arma(3,0)+garch(1,1), data=sp500)
>>
>>       Spencer
>>
>> tom soyer wrote:
>>     
>>> Spencer, take any data series and run garch vs. garchFit with various
>>> sample size, and you will see garch needs a lot more data points to
>>> get a good fit. Anyway, it probably doesn't matter if one always use a
>>> sample size of >2,000. I was just courious.
>>>
>>> On 2/12/08, *Spencer Graves* <spencer.graves at pdf.com
>>> <mailto:spencer.graves at pdf.com>> wrote:
>>>
>>>     Why do you say that 'garch' requires more observations than
>>>     'garchFit'?
>>>     Can you outline documentation or tests you've performed?
>>>
>>>     Spencer
>>>
>>>     tom soyer wrote:
>>>     > Hi,
>>>     >
>>>     > It seems that the minimum sample size required by garch is much
>>>     larger than
>>>     > garchFit, does anyone know why? I am guessing between 1,000 and
>>>     2,000 for
>>>     > garch and ~500 for garchFit. Does anyone know the exact minimum
>>>     sample size
>>>     > for each?
>>>     >
>>>     > Thanks!
>>>     >
>>>     >
>>>
>>>
>>>
>>>
>>> --
>>> Tom
>>>       
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
>>
>>     
>
>
>


From ksspriggs at gmail.com  Wed Feb 13 21:51:17 2008
From: ksspriggs at gmail.com (Ken Spriggs)
Date: Wed, 13 Feb 2008 12:51:17 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Garman-Klass Volatility in Rmetrics
	package
Message-ID: <15467571.post@talk.nabble.com>


I'm looking at the Rmetrics Reference Card in the section called "Additional
Functions" under the heading "Additional Trading Indicators" and I'm
wondering where the garmanKlassTA function went.  When I do ?garmanKlassTA
nothing comes up, but "garmanKlassTA" is listed in the Contents - however
there's no documentation that appears when you click on it.  

What has happened to the garmanKlaussTA function?   

(I also have fTrading package loaded.)


-- 
View this message in context: http://www.nabble.com/Garman-Klass-Volatility-in-Rmetrics-package-tp15467571p15467571.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Wed Feb 13 22:05:32 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 13 Feb 2008 15:05:32 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Garman-Klass Volatility in
	Rmetrics package
In-Reply-To: <15467571.post@talk.nabble.com>
References: <15467571.post@talk.nabble.com>
Message-ID: <e8e755250802131305x45ed7625u69e2f9133c482fe6@mail.gmail.com>

Hi Ken,

I *think* it is

garmanklassTA

no caps:

> garmanklassTA
function (open, high, low, close)
{
    TS = is.timeSeries(open)
    if (TS) {
        x = open
        open = as.vector(open)
        high = as.vector(high)
        low = as.vector(low)
        close = as.vector(close)
    }
    prices = log(cbind(open, high, low, close))
    n = nrow(prices)
    alpha = 0.12
    f = 0.192
    u = high - open
    d = low - open
    cc = close - open
    oc = (prices[2:n, 1] - prices[1:(n - 1), 4])^2
    garmanklass = 0.511 * (u - d)^2 - 0.019 * (cc * (u + d) -
        2 * u * d) - 0.383 * cc^2
    garmanklass = sqrt(((1 - alpha) * garmanklass[2:n])/(1 -
        f) + (alpha * oc)/f)
    garmanklass = c(NA, garmanklass)
    if (TS) {
        garmanklass = matrix(garmanklass)
        colnames(garmanklass) = "GK"
        rownames(garmanklass) = rownames(x at Data)
        x at Data = garmanklass
    }
    else {
        x = garmanklass
    }
    x
}

Jeff

On Feb 13, 2008 2:51 PM, Ken Spriggs <ksspriggs at gmail.com> wrote:
>
> I'm looking at the Rmetrics Reference Card in the section called "Additional
> Functions" under the heading "Additional Trading Indicators" and I'm
> wondering where the garmanKlassTA function went.  When I do ?garmanKlassTA
> nothing comes up, but "garmanKlassTA" is listed in the Contents - however
> there's no documentation that appears when you click on it.
>
> What has happened to the garmanKlaussTA function?
>
> (I also have fTrading package loaded.)
>
>
> --
> View this message in context: http://www.nabble.com/Garman-Klass-Volatility-in-Rmetrics-package-tp15467571p15467571.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From Achim.Zeileis at wu-wien.ac.at  Wed Feb 13 22:22:58 2008
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 13 Feb 2008 22:22:58 +0100 (CET)
Subject: [R-SIG-Finance] garch vs garchFit - minimum sample size
In-Reply-To: <47B340F9.1000707@pdf.com>
Message-ID: <Pine.LNX.4.44.0802132025450.15388-100000@disco.wu-wien.ac.at>

Spencer,

I'll try to answer your questions but just as a disclaimer: I'm really no
expert in GARCH estimation. Also I have just read some parts of the
garch() and garchFit() code and the authors of the respective function can
surely comment more competently on this.

>       Is it fair to say that 'garchFit' is newer

Yes.

> and uses a more general and robust algorithm?

I'm not sure here. garchFit() certainly fits a much more general model
class but the algorithms (there is not only one) it interfaces are not
as such more general, I think. It is also a bit unclear what that would
mean exactly.

As far as I can see, Diethelm and Yohan have been quite busy improving
the optimizers and also their documentation (which seems to be more
detailed in the devel-version of the package, maybe Yohan or Diethelm
can comment on this). I think (but might be wrong here) that all
optimizers used by garchFit() rely on numerical gradients and numerical
Hessians.

Adrian's code comes with its own optimzer (Quasi-Newton) which is not
available in garchFit() (I think) and provides both analytical and
numerical gradients (Gaussian conditional distribution only).

To the best of my knowledge, analytical Hessians are available in neither
function.

>       Or are there circumstances under which 'garch' would give better
> answers than 'garchFit'?

I encountered situations where the likelihood returned by garch() was
slightly larger than that by garchFit() when both converged to the same
maximum. But it's quite likely that I could have improved that by tweaking
the optimization in garchFit(). And, as you also illustrated, there are
situations where garchFit() converges while garch() does not.

In any case, as I already said in my previous post: "better" just pertains
to the coefficient estimates while the estimates for the standard
deviation are mainly "different". And I think that none of the covariance
estimators is uniformly dominated by all others.

hth,
Z


From ksspriggs at gmail.com  Wed Feb 13 22:25:40 2008
From: ksspriggs at gmail.com (Ken Spriggs)
Date: Wed, 13 Feb 2008 13:25:40 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Garman-Klass Volatility in
	Rmetrics package
In-Reply-To: <e8e755250802131305x45ed7625u69e2f9133c482fe6@mail.gmail.com>
References: <15467571.post@talk.nabble.com>
	<e8e755250802131305x45ed7625u69e2f9133c482fe6@mail.gmail.com>
Message-ID: <15468350.post@talk.nabble.com>


Thanks Jeff, that works.  

The Rmetrics Reference Card has the "K" in Klass capitalized.  Doohhh!



Jeff Ryan wrote:
> 
> Hi Ken,
> 
> I *think* it is
> 
> garmanklassTA
> 
> no caps:
> 
>> garmanklassTA
> function (open, high, low, close)
> {
>     TS = is.timeSeries(open)
>     if (TS) {
>         x = open
>         open = as.vector(open)
>         high = as.vector(high)
>         low = as.vector(low)
>         close = as.vector(close)
>     }
>     prices = log(cbind(open, high, low, close))
>     n = nrow(prices)
>     alpha = 0.12
>     f = 0.192
>     u = high - open
>     d = low - open
>     cc = close - open
>     oc = (prices[2:n, 1] - prices[1:(n - 1), 4])^2
>     garmanklass = 0.511 * (u - d)^2 - 0.019 * (cc * (u + d) -
>         2 * u * d) - 0.383 * cc^2
>     garmanklass = sqrt(((1 - alpha) * garmanklass[2:n])/(1 -
>         f) + (alpha * oc)/f)
>     garmanklass = c(NA, garmanklass)
>     if (TS) {
>         garmanklass = matrix(garmanklass)
>         colnames(garmanklass) = "GK"
>         rownames(garmanklass) = rownames(x at Data)
>         x at Data = garmanklass
>     }
>     else {
>         x = garmanklass
>     }
>     x
> }
> 
> Jeff
> 
> On Feb 13, 2008 2:51 PM, Ken Spriggs <ksspriggs at gmail.com> wrote:
>>
>> I'm looking at the Rmetrics Reference Card in the section called
>> "Additional
>> Functions" under the heading "Additional Trading Indicators" and I'm
>> wondering where the garmanKlassTA function went.  When I do
>> ?garmanKlassTA
>> nothing comes up, but "garmanKlassTA" is listed in the Contents - however
>> there's no documentation that appears when you click on it.
>>
>> What has happened to the garmanKlaussTA function?
>>
>> (I also have fTrading package loaded.)
>>
>>
>> --
>> View this message in context:
>> http://www.nabble.com/Garman-Klass-Volatility-in-Rmetrics-package-tp15467571p15467571.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> 
> 
> -- 
> There's a way to do it better - find it.
> Thomas A. Edison
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Garman-Klass-Volatility-in-Rmetrics-package-tp15467571p15468350.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Wed Feb 13 22:32:56 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 13 Feb 2008 15:32:56 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Garman-Klass Volatility in
	Rmetrics package
In-Reply-To: <15468350.post@talk.nabble.com>
References: <15467571.post@talk.nabble.com>
	<e8e755250802131305x45ed7625u69e2f9133c482fe6@mail.gmail.com>
	<15468350.post@talk.nabble.com>
Message-ID: <e8e755250802131332p3fdc6db5t6fa332de68e6cfa@mail.gmail.com>

I used the backup help tool:

grep 'garman' fTrading/*/*

as I couldn't find any docs either.

Jeff

On Feb 13, 2008 3:25 PM, Ken Spriggs <ksspriggs at gmail.com> wrote:
>
> Thanks Jeff, that works.
>
> The Rmetrics Reference Card has the "K" in Klass capitalized.  Doohhh!
>
>
>
>
> Jeff Ryan wrote:
> >
> > Hi Ken,
> >
> > I *think* it is
> >
> > garmanklassTA
> >
> > no caps:
> >
> >> garmanklassTA
> > function (open, high, low, close)
> > {
> >     TS = is.timeSeries(open)
> >     if (TS) {
> >         x = open
> >         open = as.vector(open)
> >         high = as.vector(high)
> >         low = as.vector(low)
> >         close = as.vector(close)
> >     }
> >     prices = log(cbind(open, high, low, close))
> >     n = nrow(prices)
> >     alpha = 0.12
> >     f = 0.192
> >     u = high - open
> >     d = low - open
> >     cc = close - open
> >     oc = (prices[2:n, 1] - prices[1:(n - 1), 4])^2
> >     garmanklass = 0.511 * (u - d)^2 - 0.019 * (cc * (u + d) -
> >         2 * u * d) - 0.383 * cc^2
> >     garmanklass = sqrt(((1 - alpha) * garmanklass[2:n])/(1 -
> >         f) + (alpha * oc)/f)
> >     garmanklass = c(NA, garmanklass)
> >     if (TS) {
> >         garmanklass = matrix(garmanklass)
> >         colnames(garmanklass) = "GK"
> >         rownames(garmanklass) = rownames(x at Data)
> >         x at Data = garmanklass
> >     }
> >     else {
> >         x = garmanklass
> >     }
> >     x
> > }
> >
> > Jeff
> >
> > On Feb 13, 2008 2:51 PM, Ken Spriggs <ksspriggs at gmail.com> wrote:
> >>
> >> I'm looking at the Rmetrics Reference Card in the section called
> >> "Additional
> >> Functions" under the heading "Additional Trading Indicators" and I'm
> >> wondering where the garmanKlassTA function went.  When I do
> >> ?garmanKlassTA
> >> nothing comes up, but "garmanKlassTA" is listed in the Contents - however
> >> there's no documentation that appears when you click on it.
> >>
> >> What has happened to the garmanKlaussTA function?
> >>
> >> (I also have fTrading package loaded.)
> >>
> >>
> >> --
> >> View this message in context:
> >> http://www.nabble.com/Garman-Klass-Volatility-in-Rmetrics-package-tp15467571p15467571.html
> >> Sent from the Rmetrics mailing list archive at Nabble.com.
> >>
> >> _______________________________________________
> >> R-SIG-Finance at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >> -- Subscriber-posting only.
> >> -- If you want to post, subscribe first.
> >>
> >
> >
> >
> > --
> > There's a way to do it better - find it.
> > Thomas A. Edison
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
> >
>
> --
> View this message in context: http://www.nabble.com/Garman-Klass-Volatility-in-Rmetrics-package-tp15467571p15468350.html
>
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From icos.atropa at gmail.com  Thu Feb 14 08:00:11 2008
From: icos.atropa at gmail.com (icosa atropa)
Date: Thu, 14 Feb 2008 00:00:11 -0700
Subject: [R-SIG-Finance] timeseries - xst vs. dataframe?
In-Reply-To: <e8e755250802130648h714802cbj77fb168550286f@mail.gmail.com>
References: <971536df0802130616l1bd2e256x327ca023816f7e71@mail.gmail.com>
	<Pine.LNX.4.44.0802131537490.15388-100000@disco.wu-wien.ac.at>
	<e8e755250802130648h714802cbj77fb168550286f@mail.gmail.com>
Message-ID: <681d07c20802132300t47a20c70r67e1ac5d6970e1ee@mail.gmail.com>

It looks like help('$') and help('is.atomic') raise some interesting
ts and zoo implementation questions. I understand that zoo is modeled
on ts; I didn't really appreciate until now that ts was atomic. Is an
atomic ts and zoo "correct"? Would a recursive zoo break things? Items
of note:

* Is zoo conceptually atomic if it contains a non-integer index vector
and an integer data vector?

*zoo and ts behave more recursively than atomically under certain conditions:
One example is dynlm. It accepts a number of recursive objects for
"data=", including environments and dataframes. Here zoo and ts as
atomic seem to be temporarily granted recursive, environment-like
status so that the model can be specified using the traditional
environment-element syntax:

# one of these is not like the others
test.matrix = cbind(a=1:5, b=2*1:5)
test.ts = ts(cbind(a=1:5, b=2*1:5))
test.zoo = zoo(data.frame(a=1:5, b=2*1:5), order.by=letters[1:5])
test.df = data.frame(a=1:5, b=2*1:5, group=c('A', 'A', 'A', 'B', 'B'),
  order.by=letters[1:5])

dynlm(a ~ b, data=test.matrix)
dynlm(a ~ b, data=test.ts)
dynlm(a ~ b, data=test.zoo)
dynlm(a ~ b, data=test.df)
# the first one doesn't work, but all the others are identical. same for lm()

*One quirk, in my experience, of zoo being atomic is that different
modes of data for the same observation time must be kept in different
objects due to type coercion. I.e.

# with a dataframe, everything can pack together in one object
subset(test.df, group=='A')
# with zoo, separate objects with the same index are necessary
groups.zoo = zoo(group=c('A', 'A', 'A', 'B', 'B'), order.by=letters[1:5])
subset(test.zoo, groups.zoo=='A')
# this doesn't work, since packing together coerces everything to character
oops.zoo = zoo(data.frame(a=1:5, b=2*1:5, group=c('A', 'A', 'A', 'B', 'B')),
order.by=letters[1:5])

* help('ts') and help('vector') reference Becker et al. (1988), while
help('data.frame') and help('lm') reference Chambers (1992). Is ts
being atomic perhaps an accident of timing and history? Re-reading the
docs for lm, data.frame, and is.atomic gives me a sense that custom
classes tend to model recursive rather than atomic:

help('is.atomic'):
  'is.atomic' is true for the atomic vector types ('"logical"',
  '"integer"', '"numeric"', '"complex"', '"character"' and '"raw"')
  and 'NULL'.

  Most types of language objects are regarded as recursive: those
  which are not are the atomic vector types, 'NULL' and symbols (as
  given by 'as.name').

best,
christian

On Feb 13, 2008 7:48 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Hi everyone,
>
> I agree that the '$' operator seems like a nice addition. The only
> problem that I see is:
>
> from help('$')
> ...
> The default methods work somewhat differently for atomic vectors,
> matrices/arrays and for recursive (list-like, see 'is.recursive')
> objects. '$' returns 'NULL' (with a warning) except for recursive
> objects, and is only discussed in the section below on recursive
> objects. Its use on non-recursive objects was deprecated in R
> 2.5.0.
> ...
>
> Since zoo (and thus xts) is really a matrix/array with attributes - it
> seems like it has the chance of breaking something - though where I
> can't reasonably imagine.
>
> The flip side to the argument against is that returning a NULL object
> seems to be of little value to anything.
>
> Is anyone aware of the reason for it being deprecated for non-recursive objects?
>
> Jeff
>
>
> On Feb 13, 2008 8:43 AM, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> > On Wed, 13 Feb 2008, Gabor Grothendieck wrote:
> >
> > > zoo is modelled on the "ts" class, not on the "data.frame" class.
> > > In R, the way it works is that $ is used on list-based objects
> > > and not on array-based objects.
> >
> > True, and this is the explanation why we don't have it at the moment. But
> > the $ operator might be a convenient addition. And we currently have a few
> > examples where we are consistent with "ts" but provide further features
> > for convenience. At the moment, I don't think something dangerous would
> > happen if we add it - or do I overlook something?
> >
> > Given that you have already written the code, I would vote for including
> > it in the package.
> > Z
> >
> >
> > > Of course, your are free to define
> > > and redefine operators as you please and since zoo is an S3 class
> > > its possible to add your own S3 methods. $ indexing is less
> > > than a dozen lines of code to add to your program:
> > >
> > > "$.zoo" <- function(object, x) object[, x]
> > >
> > > "$<-.zoo" <- function(object, x, value) {
> > > stopifnot(length(dim(object)) == 2)
> > > if (x %in% colnames(object)) object[,x] <- value
> > > else {
> > > object <- cbind(object, value)
> > > colnames(object)[ncol(object)] <- x
> > > }
> > > object
> > > }
> > >
> > > # test
> > > library(zoo)
> > > z <- zoo(cbind(a = 1:3, b = 4:6))
> > > z$c <- z$b + 1
> > > z$a <- z$b - 1
> > > z
> > >
> > > > dataframes. Is "list" syntax planned for inclusion in xst? At
> > > > present, column numbering (test.zoo[,1]) seems the best alternative.
> > > >
> > > > Since as.data.frame(test.zoo)$a appears to recover the core data, it
> > > > sounds sensible for "test.zoo$a" to extract an object containing the
> > > > index and the named column. Does this break anything?
> > > > e.g. :
> > > >
> > > > test.df = data.frame(a=1:5, b=2*(1:5))
> > > > test.df$a
> > > > #[1] 1 2 3 4 5
> > > >
> > > > index = Sys.time() + 60*1:5
> > > > test.zoo = zoo(test.df, order.by=index)
> > > > test.zoo$a
> > > > #NULL
> > > >
> > > > test.zoo[1:2,1]
> > > > # 2008-02-13 05:54:40 2008-02-13 05:55:40
> > > > # 1 2
> > > >
> > > > coredata(test.zoo)$a
> > > > #NULL
> > > >
> > > > as.data.frame(test.zoo)$a
> > > > # [1] 1 2 3 4 5
> > > >
> > > > thanks and best,
> > > > christian
> > > >
> > > > > At this point 'xts' objects behave much like any standard data.frame, matrix or, most closely, zoo object. They have some unique user 'xts' methods but all standard 'zoo' methods will work (it just extends 'zoo')
> > > >
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> > >
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>
>
>
> --
> There's a way to do it better - find it.
> Thomas A. Edison
>
--
Far better an approximate answer to the right question, which is often
vague, than the exact answer to the wrong question, which can always
be made precise -- j.w. tukey


From ggrothendieck at gmail.com  Thu Feb 14 12:39:30 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 14 Feb 2008 06:39:30 -0500
Subject: [R-SIG-Finance] timeseries - xst vs. dataframe?
In-Reply-To: <681d07c20802132300t47a20c70r67e1ac5d6970e1ee@mail.gmail.com>
References: <971536df0802130616l1bd2e256x327ca023816f7e71@mail.gmail.com>
	<Pine.LNX.4.44.0802131537490.15388-100000@disco.wu-wien.ac.at>
	<e8e755250802130648h714802cbj77fb168550286f@mail.gmail.com>
	<681d07c20802132300t47a20c70r67e1ac5d6970e1ee@mail.gmail.com>
Message-ID: <971536df0802140339l7f24f1cag77008df738a8de63@mail.gmail.com>

See interspersed comments.

On Thu, Feb 14, 2008 at 2:00 AM, icosa atropa <icos.atropa at gmail.com> wrote:
> It looks like help('$') and help('is.atomic') raise some interesting
> ts and zoo implementation questions. I understand that zoo is modeled
> on ts; I didn't really appreciate until now that ts was atomic. Is an
> atomic ts and zoo "correct"? Would a recursive zoo break things? Items
> of note:
>
> * Is zoo conceptually atomic if it contains a non-integer index vector
> and an integer data vector?

zoo does not know what the mode or class of its index vector is.  It only deals
with the index vector through the methods of index vector's class that
are required to be there as defined in ?zoo .   The only exceptions
are a few routines which deal with outside world:

- read.zoo knows about certain classes so it can read them in from files
- yearmon and yearqtr are provided classes intended to parallel freq =
3 and 12 in ts
- as.Date is extended slightly to handle numeric arguments without having
  to specify the origin

>
> *zoo and ts behave more recursively than atomically under certain conditions:
> One example is dynlm. It accepts a number of recursive objects for
> "data=", including environments and dataframes. Here zoo and ts as
> atomic seem to be temporarily granted recursive, environment-like
> status so that the model can be specified using the traditional
> environment-element syntax:
>
> # one of these is not like the others
> test.matrix = cbind(a=1:5, b=2*1:5)
> test.ts = ts(cbind(a=1:5, b=2*1:5))
> test.zoo = zoo(data.frame(a=1:5, b=2*1:5), order.by=letters[1:5])
> test.df = data.frame(a=1:5, b=2*1:5, group=c('A', 'A', 'A', 'B', 'B'),
>  order.by=letters[1:5])
>
> dynlm(a ~ b, data=test.matrix)
> dynlm(a ~ b, data=test.ts)
> dynlm(a ~ b, data=test.zoo)
> dynlm(a ~ b, data=test.df)
> # the first one doesn't work, but all the others are identical. same for lm()
>

I just checked and dynlm does this

 if (!is.list(data))
            data <- as.list(data)

and as.list on a matrix does not give anything useful in this context:

Note that as.list.zoo returns a list whose components are the columns
as zoo objects whereas as.list on a matrix returns a list whose components
are the elements of the matrix.

> *One quirk, in my experience, of zoo being atomic is that different
> modes of data for the same observation time must be kept in different
> objects due to type coercion. I.e.

As mentioned above zoo does not know the mode of its index so the
mode of the data is not related to the mode of the index.  What
you are observing is due to whatever coercions exist in R and the methods
being used (which are, in general, not supplied by zoo).  zoo does
not participate.

>
> # with a dataframe, everything can pack together in one object
> subset(test.df, group=='A')
> # with zoo, separate objects with the same index are necessary
> groups.zoo = zoo(group=c('A', 'A', 'A', 'B', 'B'), order.by=letters[1:5])
> subset(test.zoo, groups.zoo=='A')
> # this doesn't work, since packing together coerces everything to character
> oops.zoo = zoo(data.frame(a=1:5, b=2*1:5, group=c('A', 'A', 'A', 'B', 'B')),
> order.by=letters[1:5])
>
> * help('ts') and help('vector') reference Becker et al. (1988), while
> help('data.frame') and help('lm') reference Chambers (1992). Is ts
> being atomic perhaps an accident of timing and history? Re-reading the
> docs for lm, data.frame, and is.atomic gives me a sense that custom
> classes tend to model recursive rather than atomic:
>
> help('is.atomic'):
>  'is.atomic' is true for the atomic vector types ('"logical"',
>  '"integer"', '"numeric"', '"complex"', '"character"' and '"raw"')
>  and 'NULL'.
>

Perhaps its because many computations on arrays are MUCH
faster in R.

>  Most types of language objects are regarded as recursive: those
>  which are not are the atomic vector types, 'NULL' and symbols (as
>  given by 'as.name').
>
> best,
> christian
>
>
> On Feb 13, 2008 7:48 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> > Hi everyone,
> >
> > I agree that the '$' operator seems like a nice addition. The only
> > problem that I see is:
> >
> > from help('$')
> > ...
> > The default methods work somewhat differently for atomic vectors,
> > matrices/arrays and for recursive (list-like, see 'is.recursive')
> > objects. '$' returns 'NULL' (with a warning) except for recursive
> > objects, and is only discussed in the section below on recursive
> > objects. Its use on non-recursive objects was deprecated in R
> > 2.5.0.
> > ...
> >
> > Since zoo (and thus xts) is really a matrix/array with attributes - it
> > seems like it has the chance of breaking something - though where I
> > can't reasonably imagine.
> >
> > The flip side to the argument against is that returning a NULL object
> > seems to be of little value to anything.
> >
> > Is anyone aware of the reason for it being deprecated for non-recursive objects?
> >
> > Jeff
> >
> >
> > On Feb 13, 2008 8:43 AM, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
> > > On Wed, 13 Feb 2008, Gabor Grothendieck wrote:
> > >
> > > > zoo is modelled on the "ts" class, not on the "data.frame" class.
> > > > In R, the way it works is that $ is used on list-based objects
> > > > and not on array-based objects.
> > >
> > > True, and this is the explanation why we don't have it at the moment. But
> > > the $ operator might be a convenient addition. And we currently have a few
> > > examples where we are consistent with "ts" but provide further features
> > > for convenience. At the moment, I don't think something dangerous would
> > > happen if we add it - or do I overlook something?
> > >
> > > Given that you have already written the code, I would vote for including
> > > it in the package.
> > > Z
> > >
> > >
> > > > Of course, your are free to define
> > > > and redefine operators as you please and since zoo is an S3 class
> > > > its possible to add your own S3 methods. $ indexing is less
> > > > than a dozen lines of code to add to your program:
> > > >
> > > > "$.zoo" <- function(object, x) object[, x]
> > > >
> > > > "$<-.zoo" <- function(object, x, value) {
> > > > stopifnot(length(dim(object)) == 2)
> > > > if (x %in% colnames(object)) object[,x] <- value
> > > > else {
> > > > object <- cbind(object, value)
> > > > colnames(object)[ncol(object)] <- x
> > > > }
> > > > object
> > > > }
> > > >
> > > > # test
> > > > library(zoo)
> > > > z <- zoo(cbind(a = 1:3, b = 4:6))
> > > > z$c <- z$b + 1
> > > > z$a <- z$b - 1
> > > > z
> > > >
> > > > > dataframes. Is "list" syntax planned for inclusion in xst? At
> > > > > present, column numbering (test.zoo[,1]) seems the best alternative.
> > > > >
> > > > > Since as.data.frame(test.zoo)$a appears to recover the core data, it
> > > > > sounds sensible for "test.zoo$a" to extract an object containing the
> > > > > index and the named column. Does this break anything?
> > > > > e.g. :
> > > > >
> > > > > test.df = data.frame(a=1:5, b=2*(1:5))
> > > > > test.df$a
> > > > > #[1] 1 2 3 4 5
> > > > >
> > > > > index = Sys.time() + 60*1:5
> > > > > test.zoo = zoo(test.df, order.by=index)
> > > > > test.zoo$a
> > > > > #NULL
> > > > >
> > > > > test.zoo[1:2,1]
> > > > > # 2008-02-13 05:54:40 2008-02-13 05:55:40
> > > > > # 1 2
> > > > >
> > > > > coredata(test.zoo)$a
> > > > > #NULL
> > > > >
> > > > > as.data.frame(test.zoo)$a
> > > > > # [1] 1 2 3 4 5
> > > > >
> > > > > thanks and best,
> > > > > christian
> > > > >
> > > > > > At this point 'xts' objects behave much like any standard data.frame, matrix or, most closely, zoo object. They have some unique user 'xts' methods but all standard 'zoo' methods will work (it just extends 'zoo')
> > > > >
> > > >
> > > > _______________________________________________
> > > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > > -- Subscriber-posting only.
> > > > -- If you want to post, subscribe first.
> > > >
> > > >
> > >
> > > _______________________________________________
> > > R-SIG-Finance at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > > -- Subscriber-posting only.
> > > -- If you want to post, subscribe first.
> > >
> >
> >
> >
> > --
> > There's a way to do it better - find it.
> > Thomas A. Edison
> >
> --
> Far better an approximate answer to the right question, which is often
> vague, than the exact answer to the wrong question, which can always
> be made precise -- j.w. tukey
>


From Achim.Zeileis at wu-wien.ac.at  Thu Feb 14 13:03:40 2008
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 14 Feb 2008 13:03:40 +0100 (CET)
Subject: [R-SIG-Finance] timeseries - xst vs. dataframe?
In-Reply-To: <971536df0802140339l7f24f1cag77008df738a8de63@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0802141250390.15388-100000@disco.wu-wien.ac.at>

Just a few comments in addition to Gabor's:

> > It looks like help('$') and help('is.atomic') raise some interesting
> > ts and zoo implementation questions. I understand that zoo is modeled
> > on ts; I didn't really appreciate until now that ts was atomic. Is an
> > atomic ts and zoo "correct"? Would a recursive zoo break things? Items
> > of note:
> >
> > * Is zoo conceptually atomic if it contains a non-integer index vector
> > and an integer data vector?
>
> zoo does not know what the mode or class of its index vector is.  It only deals
> with the index vector through the methods of index vector's class that
> are required to be there as defined in ?zoo .   The only exceptions
> are a few routines which deal with outside world:

Just to clarify: zoo is the core data plus an index attribute. zoo does
not know anything about the index. But the core data is always atomic
(vector or matrix -- and very limited support for factor).

> > *zoo and ts behave more recursively than atomically under certain conditions:
> > One example is dynlm.

Because it explicitely adds some glue to deal with these situation
(leveraging the tools from "zoo"). It is necessary to write something like
"dynlm" or "dyn" because otherwise the data handling wouldn't work
(sufficiently well) in lm() (and friends).

> > It accepts a number of recursive objects for
> > "data=", including environments and dataframes. Here zoo and ts as
> > atomic seem to be temporarily granted recursive, environment-like
> > status so that the model can be specified using the traditional
> > environment-element syntax:
> >
> > # one of these is not like the others
> > test.matrix = cbind(a=1:5, b=2*1:5)
> > test.ts = ts(cbind(a=1:5, b=2*1:5))
> > test.zoo = zoo(data.frame(a=1:5, b=2*1:5), order.by=letters[1:5])
> > test.df = data.frame(a=1:5, b=2*1:5, group=c('A', 'A', 'A', 'B', 'B'),
> >  order.by=letters[1:5])
> >
> > dynlm(a ~ b, data=test.matrix)
> > dynlm(a ~ b, data=test.ts)
> > dynlm(a ~ b, data=test.zoo)
> > dynlm(a ~ b, data=test.df)
> > # the first one doesn't work, but all the others are identical. same for lm()

Note that the documentation wants you to specify "data" as a
ts or zoo or data.frame (not a matrix).

Best,
Z


From chalabi at phys.ethz.ch  Thu Feb 14 16:14:31 2008
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Thu, 14 Feb 2008 16:14:31 +0100
Subject: [R-SIG-Finance] holidayNYSE missing some
In-Reply-To: <loom.20080212T162838-798@post.gmane.org>
References: <20080204115153.13a247f4@yankee-laptop>
	<loom.20080212T162838-798@post.gmane.org>
Message-ID: <20080214161431.5570342f@yankee-laptop>

>>>> "JB" == Joe Byers <ecjbosu at aol.com>
>>>> on Tue, 12 Feb 2008 16:29:55 +0000 (UTC)

   JB> I am working on adding Haug's Multiasset greeks to the fExotics
   JB> package.
   JB> Are you interested in adding these?

New contribution to any Rmetics package is always very welcome!

Do not hesitate to directly contact me or send your
suggestion/contribution to the Rmetrics core team mailing list. 
(Rmetrics-core at r-project.org)

Thanks in advance for your contribution,
Yohan


From chalabi at phys.ethz.ch  Fri Feb 15 16:40:15 2008
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Fri, 15 Feb 2008 16:40:15 +0100
Subject: [R-SIG-Finance] garch vs garchFit - minimum sample size
In-Reply-To: <Pine.LNX.4.44.0802132025450.15388-100000@disco.wu-wien.ac.at>
References: <47B340F9.1000707@pdf.com>
	<Pine.LNX.4.44.0802132025450.15388-100000@disco.wu-wien.ac.at>
Message-ID: <20080215164015.1c6dfc12@yankee-laptop>

>>>> "AZ" == Achim Zeileis <Achim.Zeileis at wu-wien.ac.at>
>>>> on Wed, 13 Feb 2008 22:22:58 +0100 (CET)


   AZ> As far as I can see, Diethelm and Yohan have been quite busy
   AZ> improving
   AZ> the optimizers and also their documentation (which seems to
   AZ> be more
   AZ> detailed in the devel-version of the package, maybe Yohan
   AZ> or Diethelm
   AZ> can comment on this). I think (but might be wrong here) that all
   AZ> optimizers used by garchFit() rely on numerical gradients
   AZ> and numerical
   AZ> Hessians.

We have implemented a new optimization scheme  "mnfb" in the
devel-version of fGarch
(https://svn.r-project.org/Rmetrics/trunk/fGarch). It is actually the
same fortran library as used in the R function nlminb(). But we have
implemented the whole optimization in fortran. 

As you have noticed it, we are also working on the documentation and we
hope the new manual page is more readable.

   AZ> Adrian's code comes with its own optimzer (Quasi-Newton) which
   AZ> is not
   AZ> available in garchFit() (I think) and provides both analytical
   AZ> and
   AZ> numerical gradients (Gaussian conditional distribution only).

In garchFit you can choose between 5 different optimizations
schemes : "nlminb" , "mnfb" (in devel-version), "sqp", "lbfgsb",
"nlminb+nm", "lbfgsb+nm". Please read the man page for more details. 

Although the analytical gradient and hessian of ARMA-APARCH for
Gaussian conditional distribution can be  calculated without too
much of effort, the analytical solutions for other
distribution are not trivial. Since garchFit can handle different 
conditional distributions ("norm", "snorm", "ged", "sged", "std",
"sstd"), we decieded to use only numerical approximations.

regards,
Yohan


From Achim.Zeileis at wu-wien.ac.at  Fri Feb 15 16:54:22 2008
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 15 Feb 2008 16:54:22 +0100 (CET)
Subject: [R-SIG-Finance] garch vs garchFit - minimum sample size
In-Reply-To: <20080215164015.1c6dfc12@yankee-laptop>
Message-ID: <Pine.LNX.4.44.0802151649530.15388-100000@disco.wu-wien.ac.at>

Yohan,

thanks for info!

> We have implemented a new optimization scheme  "mnfb" in the
> devel-version of fGarch
> (https://svn.r-project.org/Rmetrics/trunk/fGarch). It is actually the
> same fortran library as used in the R function nlminb(). But we have
> implemented the whole optimization in fortran.
>
> As you have noticed it, we are also working on the documentation and we
> hope the new manual page is more readable.
>
>    AZ> Adrian's code comes with its own optimzer (Quasi-Newton) which
>    AZ> is not
>    AZ> available in garchFit() (I think) and provides both analytical
>    AZ> and
>    AZ> numerical gradients (Gaussian conditional distribution only).
>
> In garchFit you can choose between 5 different optimizations
> schemes : "nlminb" , "mnfb" (in devel-version), "sqp", "lbfgsb",
> "nlminb+nm", "lbfgsb+nm". Please read the man page for more details.
>
> Although the analytical gradient and hessian of ARMA-APARCH for
> Gaussian conditional distribution can be  calculated without too
> much of effort, the analytical solutions for other
> distribution are not trivial. Since garchFit can handle different
> conditional distributions ("norm", "snorm", "ged", "sged", "std",
> "sstd"), we decieded to use only numerical approximations.

Ah, yes, I should have written that in my post. In garch() it is possible
to get the analytical gradients because it uses the Gaussian conditional
distribution. And I wasn't suggesting to provide it for all distributions
supported by garchFit(). But for the Gaussian case, it would be a nice
addition for garchFit() to support analytical gradients/Hessian.

Best wishes,
Z


From spencer.graves at pdf.com  Fri Feb 15 16:51:22 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 15 Feb 2008 07:51:22 -0800
Subject: [R-SIG-Finance] garch vs garchFit - minimum sample size
In-Reply-To: <20080215164015.1c6dfc12@yankee-laptop>
References: <47B340F9.1000707@pdf.com>
	<Pine.LNX.4.44.0802132025450.15388-100000@disco.wu-wien.ac.at>
	<20080215164015.1c6dfc12@yankee-laptop>
Message-ID: <47B5B4FA.7070306@pdf.com>

Dear Yohan: 

      Thanks for your work on this. 

      If you haven't already, could you please add descriptions of the 
arguments delta, skew and shape to the help page:  How are they defined, 
e.g., relative to what distribution -- and which parameterization of 
that distribution?  For example, are these passed to documented R 
function(s)?  Also or alternatively, can you site a relevant Wikipedia 
article? 

      In particular, can Student's t be obtained as a special case?  I 
need this for my work with the FinTS package. 

      Thanks again,
      Spencer Graves

Yohan Chalabi wrote:
>>>>> "AZ" == Achim Zeileis <Achim.Zeileis at wu-wien.ac.at>
>>>>> on Wed, 13 Feb 2008 22:22:58 +0100 (CET)
>>>>>           
>
>
>    AZ> As far as I can see, Diethelm and Yohan have been quite busy
>    AZ> improving
>    AZ> the optimizers and also their documentation (which seems to
>    AZ> be more
>    AZ> detailed in the devel-version of the package, maybe Yohan
>    AZ> or Diethelm
>    AZ> can comment on this). I think (but might be wrong here) that all
>    AZ> optimizers used by garchFit() rely on numerical gradients
>    AZ> and numerical
>    AZ> Hessians.
>
> We have implemented a new optimization scheme  "mnfb" in the
> devel-version of fGarch
> (https://svn.r-project.org/Rmetrics/trunk/fGarch). It is actually the
> same fortran library as used in the R function nlminb(). But we have
> implemented the whole optimization in fortran. 
>
> As you have noticed it, we are also working on the documentation and we
> hope the new manual page is more readable.
>
>    AZ> Adrian's code comes with its own optimzer (Quasi-Newton) which
>    AZ> is not
>    AZ> available in garchFit() (I think) and provides both analytical
>    AZ> and
>    AZ> numerical gradients (Gaussian conditional distribution only).
>
> In garchFit you can choose between 5 different optimizations
> schemes : "nlminb" , "mnfb" (in devel-version), "sqp", "lbfgsb",
> "nlminb+nm", "lbfgsb+nm". Please read the man page for more details. 
>
> Although the analytical gradient and hessian of ARMA-APARCH for
> Gaussian conditional distribution can be  calculated without too
> much of effort, the analytical solutions for other
> distribution are not trivial. Since garchFit can handle different 
> conditional distributions ("norm", "snorm", "ged", "sged", "std",
> "sstd"), we decieded to use only numerical approximations.
>
> regards,
> Yohan
>


From babel at centrum.sk  Fri Feb 15 21:58:16 2008
From: babel at centrum.sk (babel at centrum.sk)
Date: Fri, 15 Feb 2008 21:58:16 +0100
Subject: [R-SIG-Finance] S_plus
Message-ID: <200802152158.26300@centrum.cz>

Hello.
Does anybody know where garch function in S_plus store the fitted values, like fit at fitted  in garchFit, package fGarch??
When I type  names(fit), I got this
[1] "residuals" "sigma.t" "df.residual" "coef" "model"
[6] "cond.dist" "likelihood" "opt.index" "cov"
[10] "prediction" "call" "asymp.sd" "series"

But I can find the variable, where is my for example AR1-GARCH 1.1 model hide?
Thanks


From guy.yollin at rotellacapital.com  Sat Feb 16 01:08:39 2008
From: guy.yollin at rotellacapital.com (Guy Yollin)
Date: Fri, 15 Feb 2008 19:08:39 -0500
Subject: [R-SIG-Finance] S_plus
In-Reply-To: <200802152158.26300@centrum.cz>
References: <200802152158.26300@centrum.cz>
Message-ID: <E634AF2410E42246A35865D8C0C784D971A085@MI8NYCMAIL09.Mi8.com>

The fitted values can be obtained using the extractor function fitted as
in fitted(fit) given your example below.

Note, the fitted values can also be calculated by fit$series -
fit$residuals which is what the extractor function returns.

Hope this is what you're looking for.

-- G

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
babel at centrum.sk
Sent: Friday, February 15, 2008 12:58 PM
To: R-SIG-Finance
Subject: [R-SIG-Finance] S_plus

Hello.
Does anybody know where garch function in S_plus store the fitted
values, like fit at fitted  in garchFit, package fGarch??
When I type  names(fit), I got this
[1] "residuals" "sigma.t" "df.residual" "coef" "model"
[6] "cond.dist" "likelihood" "opt.index" "cov"
[10] "prediction" "call" "asymp.sd" "series"

But I can find the variable, where is my for example AR1-GARCH 1.1 model
hide?
Thanks

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From babel at centrum.sk  Sat Feb 16 11:55:13 2008
From: babel at centrum.sk (babel at centrum.sk)
Date: Sat, 16 Feb 2008 11:55:13 +0100
Subject: [R-SIG-Finance] fGarch predict
Message-ID: <200802161155.10583@centrum.cz>

Hello
I want to predict the future values of time series with Garch
When I specified my model like this:
library(fGarch)
ret <- diff(log(x))*100
fit = garchFit(~arma(1,0,0)+garch(1, 1), data =ret)
predict(fit, n.ahead = 10)

 meanForecast  meanError standardDeviation
1    0.01371299 0.03086350        0.03305819
2    0.01211893 0.03094519        0.03350248
....................................................................................

I know that if I use fit = garchFit(~garch(1, 1), data =ret) I  got constant mean, so trherefore I include amra term to move with mean

Iam not sure what values are hiding in this output. 
1. Does menForecast hold my future predicted values?
2.Or I am able to just compute the confidence intervals for my prediction like meanForecast +-2*standardDeviation  ??
3Or I need to compute the future values like yt=meanForecast+meanError*sqrt(standardDeviation)  ???
My return looks like standard return series with plus and minus values, 
[748,]  0.008184311  
[749,]  0.024548914  
[750,] -0.008182302

so I hope I would get similar prediction to this return, not just a postive mean constant.Sorry,  I know that Garch models are for volatility modelling, but I still doesnt find how to use that volatility for forecasting future values. Short example with 5 step ahead prediction will surely help.
Thank you


From spencer.graves at pdf.com  Sat Feb 16 20:00:07 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 16 Feb 2008 11:00:07 -0800
Subject: [R-SIG-Finance] fGarch predict
In-Reply-To: <200802161155.10583@centrum.cz>
References: <200802161155.10583@centrum.cz>
Message-ID: <47B732B7.2010605@pdf.com>

Hi, Yohan: 

      I want to thank you again for working to improve 'garchFit' and 
the documentation. 

      I wonder if you have time to also improve the documentation for 
'predict.garchFit', including the following example: 

           x <- garchSim()
           fit <- garchFit(~arma(1,0)+garch(1,1), data=x)
           predict(fit)

      Secondarily, I get error messages from garch(1, 0) and garch(0, 1): 

fit01 <- garchFit(~garch(0,1), data=x)
Error in .garchInitParameters(formula.mean = formula.mean, formula.var = 
formula.var,  :
  object "alpha" not found

fit10 <- garchFit(~garch(1,0), data=x)
Error in sum(beta) : invalid 'type' (closure) of argument

      Best Wishes,
      Spencer

babel at centrum.sk wrote:
> Hello
> I want to predict the future values of time series with Garch
> When I specified my model like this:
> library(fGarch)
> ret <- diff(log(x))*100
> fit = garchFit(~arma(1,0,0)+garch(1, 1), data =ret)
> predict(fit, n.ahead = 10)
>
>  meanForecast  meanError standardDeviation
> 1    0.01371299 0.03086350        0.03305819
> 2    0.01211893 0.03094519        0.03350248
> ....................................................................................
>
> I know that if I use fit = garchFit(~garch(1, 1), data =ret) I  got constant mean, so trherefore I include amra term to move with mean
>
> Iam not sure what values are hiding in this output. 
> 1. Does menForecast hold my future predicted values?
> 2.Or I am able to just compute the confidence intervals for my prediction like meanForecast +-2*standardDeviation  ??
> 3Or I need to compute the future values like yt=meanForecast+meanError*sqrt(standardDeviation)  ???
> My return looks like standard return series with plus and minus values, 
> [748,]  0.008184311  
> [749,]  0.024548914  
> [750,] -0.008182302
>
> so I hope I would get similar prediction to this return, not just a postive mean constant.Sorry,  I know that Garch models are for volatility modelling, but I still doesnt find how to use that volatility for forecasting future values. Short example with 5 step ahead prediction will surely help.
> Thank you
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From mmiklovic at yahoo.com  Sun Feb 17 13:01:34 2008
From: mmiklovic at yahoo.com (michal miklovic)
Date: Sun, 17 Feb 2008 04:01:34 -0800 (PST)
Subject: [R-SIG-Finance] fGarch predict
Message-ID: <151794.51772.qm@web50106.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080217/e72d9c6d/attachment.pl 

From Mama_Attiglah at ssga.com  Mon Feb 18 13:30:13 2008
From: Mama_Attiglah at ssga.com (Attiglah, Mama)
Date: Mon, 18 Feb 2008 12:30:13 -0000
Subject: [R-SIG-Finance] Quick and simple Simulation of a multivariate
	returns
Message-ID: <D84557769CE9ED47AC31686731990DCB0CD2652B@INCLCW03A.corp.statestr.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080218/86f2b85f/attachment.pl 

From edd at debian.org  Mon Feb 18 13:50:22 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 18 Feb 2008 06:50:22 -0600
Subject: [R-SIG-Finance] Quick and simple Simulation of a
	multivariate	returns
In-Reply-To: <D84557769CE9ED47AC31686731990DCB0CD2652B@INCLCW03A.corp.statestr.com>
References: <D84557769CE9ED47AC31686731990DCB0CD2652B@INCLCW03A.corp.statestr.com>
Message-ID: <18361.32526.382547.706925@ron.nulle.part>


On 18 February 2008 at 12:30, Attiglah, Mama wrote:
| Hi room, 
| Just want to find out if we do have any package in R that simulates a
| multivariate data using historical values.
| What about the simulation but with a long term views on some metrics,
| i.e. Mean, Volatility, correlation, etc...
| Example, I have a 2 years time series of the returns on 5 asset classes
| and I want to project the future returns just on a simulation basis. (It
| needs not preserve necessarily their time dependency distribution).
| Any model or any Monte Carlo methods or bootstrapping should do,
| including a simple block or row re-sampling with repetition. 
| Please reply on the basic of a multivariate historical data. This is not
| a portfolio simulation but a simulation of the portfolio constituents. 
| Any information is highly appreciated.

I've used a simple block bootstrap variant for this in the past. See several
posts in the last few weeks, including a rather substantive one by Tim
Hesterberg, with a particular warning about my use of block bootstrap.

AFAIK no canned function or package does this, but it doesn't take too much
effort to set it up.

Regards, Dirk

-- 
Three out of two people have difficulties with fractions.


From luda at zhaw.ch  Mon Feb 18 16:21:06 2008
From: luda at zhaw.ch (=?iso-8859-1?Q?L=FCthi_David_=28luda=29?=)
Date: Mon, 18 Feb 2008 16:21:06 +0100
Subject: [R-SIG-Finance] Quick and simple Simulation of a
	multivariatereturns
In-Reply-To: <D84557769CE9ED47AC31686731990DCB0CD2652B@INCLCW03A.corp.statestr.com>
Message-ID: <206E81C5DB06934283543543C0E5CDCB5BFE04@langouste.zhaw.ch>

Hi Mama

You can use the package 'ghyp' to fit a multivariate generalized hyperbolic distribution (or a (symmetric) special case) to your data and sample from that distribution afterwards like

library(ghyp)
data(smi.stocks)
fit.mv <- fit.ghypmv(smi.stocks)
sim.data <- rghyp(10000, fit.mv)


Best regards
David

--
David L?thi
idp - Institute of Data Analysis and Process Design
Zurich University of Applied Sciences
Postfach 805
CH-8401 Winterthur

E-mail: david.luethi at zhaw.ch
Phone: 058 934 78 03
http://www.idp.zhaw.ch 
--


-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Attiglah, Mama
Gesendet: Montag, 18. Februar 2008 13:30
An: R-SIG-Finance
Betreff: [R-SIG-Finance] Quick and simple Simulation of a multivariatereturns


Hi room, 
Just want to find out if we do have any package in R that simulates a multivariate data using historical values. What about the simulation but with a long term views on some metrics, i.e. Mean, Volatility, correlation, etc... Example, I have a 2 years time series of the returns on 5 asset classes and I want to project the future returns just on a simulation basis. (It needs not preserve necessarily their time dependency distribution). Any model or any Monte Carlo methods or bootstrapping should do, including a simple block or row re-sampling with repetition. 
Please reply on the basic of a multivariate historical data. This is not a portfolio simulation but a simulation of the portfolio constituents. 
Any information is highly appreciated.
Thanks 
-----
Mama Attiglah, PhD
Quantitative Research Analyst
Advanced Research Center
State Street Bank
+44(0)20 7698 6290 (Direct Line)
+44 (0)207 004 2968 (Direct Fax)
Please visit our Web site at 
www.ssga.com


	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From Mama_Attiglah at ssga.com  Mon Feb 18 18:36:53 2008
From: Mama_Attiglah at ssga.com (Attiglah, Mama)
Date: Mon, 18 Feb 2008 17:36:53 -0000
Subject: [R-SIG-Finance] Quick and simple Simulation of
	amultivariatereturns
In-Reply-To: <206E81C5DB06934283543543C0E5CDCB5BFE04@langouste.zhaw.ch>
Message-ID: <D84557769CE9ED47AC31686731990DCB0D09A837@INCLCW03A.corp.statestr.com>


Thanks David, I will certainly have a thorough look at it and may probably get back to you tomorrow. 

Once again, thanks. 
Mama 
-----
Mama Attiglah, PhD
Quantitative Research Analyst
Advanced Research Center
State Street Bank
+44(0)20 7698 6290 (Direct Line)
+44 (0)207 004 2968 (Direct Fax)
Please visit our Web site at 
www.ssga.com

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of L?thi David (luda)
Sent: 18 February 2008 15:21
To: R-SIG-Finance
Subject: Re: [R-SIG-Finance] Quick and simple Simulation of amultivariatereturns

Hi Mama

You can use the package 'ghyp' to fit a multivariate generalized hyperbolic distribution (or a (symmetric) special case) to your data and sample from that distribution afterwards like

library(ghyp)
data(smi.stocks)
fit.mv <- fit.ghypmv(smi.stocks)
sim.data <- rghyp(10000, fit.mv)


Best regards
David

--
David L?thi
idp - Institute of Data Analysis and Process Design
Zurich University of Applied Sciences
Postfach 805
CH-8401 Winterthur

E-mail: david.luethi at zhaw.ch
Phone: 058 934 78 03
http://www.idp.zhaw.ch 
--


-----Urspr?ngliche Nachricht-----
Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Attiglah, Mama
Gesendet: Montag, 18. Februar 2008 13:30
An: R-SIG-Finance
Betreff: [R-SIG-Finance] Quick and simple Simulation of a multivariatereturns


Hi room, 
Just want to find out if we do have any package in R that simulates a multivariate data using historical values. What about the simulation but with a long term views on some metrics, i.e. Mean, Volatility, correlation, etc... Example, I have a 2 years time series of the returns on 5 asset classes and I want to project the future returns just on a simulation basis. (It needs not preserve necessarily their time dependency distribution). Any model or any Monte Carlo methods or bootstrapping should do, including a simple block or row re-sampling with repetition. 
Please reply on the basic of a multivariate historical data. This is not a portfolio simulation but a simulation of the portfolio constituents. 
Any information is highly appreciated.
Thanks 
-----
Mama Attiglah, PhD
Quantitative Research Analyst
Advanced Research Center
State Street Bank
+44(0)20 7698 6290 (Direct Line)
+44 (0)207 004 2968 (Direct Fax)
Please visit our Web site at 
www.ssga.com


	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From stigler3 at etu.unige.ch  Tue Feb 19 00:13:10 2008
From: stigler3 at etu.unige.ch (Matthieu Stigler)
Date: Tue, 19 Feb 2008 00:13:10 +0100
Subject: [R-SIG-Finance] Quick and simple Simulation
	of	amultivariatereturns
In-Reply-To: <D84557769CE9ED47AC31686731990DCB0D09A837@INCLCW03A.corp.statestr.com>
References: <D84557769CE9ED47AC31686731990DCB0D09A837@INCLCW03A.corp.statestr.com>
Message-ID: <47BA1106.5060800@etu.unige.ch>

Hello Mama

I'm not sure I understood well what you need but if you want to have 
residual bootstrap or simulation of a simple VAR model, I added such 
function today in the repo of the package tsDyn.

The function TVAR_simul allows you to make a raw residual bootstrap with 
given data (or instead of resampling, generating new residuals of same 
variance), or simulate data with given parameter matrix and residual 
variance. Block sized bootstrap could be included, I think it is not too 
difficult, just find out how to resample with block for a usual sequence.

The function is not available on the current version of the package but 
if you are on Linux (sorry for Windows, I don't know) you just need:

svn checkout http://tsdyn.googlecode.com/svn/trunk/ tsdyn-read-only
cd tsdyn
sudo R CMD INSTALL tsDyn


Note that the function may contain errors! I hope not, but...

Best regards

Matthieu




Attiglah, Mama a ?crit :
> Thanks David, I will certainly have a thorough look at it and may probably get back to you tomorrow. 
>
> Once again, thanks. 
> Mama 
> -----
> Mama Attiglah, PhD
> Quantitative Research Analyst
> Advanced Research Center
> State Street Bank
> +44(0)20 7698 6290 (Direct Line)
> +44 (0)207 004 2968 (Direct Fax)
> Please visit our Web site at 
> www.ssga.com
>
> -----Original Message-----
> From: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of L?thi David (luda)
> Sent: 18 February 2008 15:21
> To: R-SIG-Finance
> Subject: Re: [R-SIG-Finance] Quick and simple Simulation of amultivariatereturns
>
> Hi Mama
>
> You can use the package 'ghyp' to fit a multivariate generalized hyperbolic distribution (or a (symmetric) special case) to your data and sample from that distribution afterwards like
>
> library(ghyp)
> data(smi.stocks)
> fit.mv <- fit.ghypmv(smi.stocks)
> sim.data <- rghyp(10000, fit.mv)
>
>
> Best regards
> David
>
> --
> David L?thi
> idp - Institute of Data Analysis and Process Design
> Zurich University of Applied Sciences
> Postfach 805
> CH-8401 Winterthur
>
> E-mail: david.luethi at zhaw.ch
> Phone: 058 934 78 03
> http://www.idp.zhaw.ch 
> --
>
>
> -----Urspr?ngliche Nachricht-----
> Von: r-sig-finance-bounces at stat.math.ethz.ch [mailto:r-sig-finance-bounces at stat.math.ethz.ch] Im Auftrag von Attiglah, Mama
> Gesendet: Montag, 18. Februar 2008 13:30
> An: R-SIG-Finance
> Betreff: [R-SIG-Finance] Quick and simple Simulation of a multivariatereturns
>
>
> Hi room, 
> Just want to find out if we do have any package in R that simulates a multivariate data using historical values. What about the simulation but with a long term views on some metrics, i.e. Mean, Volatility, correlation, etc... Example, I have a 2 years time series of the returns on 5 asset classes and I want to project the future returns just on a simulation basis. (It needs not preserve necessarily their time dependency distribution). Any model or any Monte Carlo methods or bootstrapping should do, including a simple block or row re-sampling with repetition. 
> Please reply on the basic of a multivariate historical data. This is not a portfolio simulation but a simulation of the portfolio constituents. 
> Any information is highly appreciated.
> Thanks 
> -----
> Mama Attiglah, PhD
> Quantitative Research Analyst
> Advanced Research Center
> State Street Bank
> +44(0)20 7698 6290 (Direct Line)
> +44 (0)207 004 2968 (Direct Fax)
> Please visit our Web site at 
> www.ssga.com
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
>


From chalabi at phys.ethz.ch  Tue Feb 19 21:09:00 2008
From: chalabi at phys.ethz.ch (Yohan Chalabi)
Date: Tue, 19 Feb 2008 21:09:00 +0100
Subject: [R-SIG-Finance] garch vs garchFit - minimum sample size
In-Reply-To: <47B5B4FA.7070306@pdf.com>
References: <47B340F9.1000707@pdf.com>
	<Pine.LNX.4.44.0802132025450.15388-100000@disco.wu-wien.ac.at>
	<20080215164015.1c6dfc12@yankee-laptop> <47B5B4FA.7070306@pdf.com>
Message-ID: <20080219210900.4eca5852@yankee-laptop>

>>>> "SG" == Spencer Graves <spencer.graves at pdf.com>
>>>> on Fri, 15 Feb 2008 07:51:22 -0800

   SG> If you haven't already, could you please add descriptions of the
   SG> arguments delta, skew and shape to the help page:  How are
   SG> they defined,
   SG> e.g., relative to what distribution -- and which parameterization
   SG> of
   SG> that distribution?  For example, are these passed to documented R
   SG> function(s)?  Also or alternatively, can you site a relevant
   SG> Wikipedia
   SG> article?
   SG> 
   SG> In particular, can Student's t be obtained as a special case?  I
   SG> need this for my work with the FinTS package.

Hi Spencer,

Please have a look at the help pages of the dev-version of fGarch
(https://svn.r-project.org/Rmetrics/trunk/fGarch). The
paragraph "GARCH Processes with non-normal distributions"
in ?garchFit presents an example with  Student's t conditional 
distribution.

Yohan


From adschai at optonline.net  Wed Feb 20 01:22:34 2008
From: adschai at optonline.net (adschai at optonline.net)
Date: Wed, 20 Feb 2008 00:22:34 +0000 (GMT)
Subject: [R-SIG-Finance] Extracting OHLC from trade price series
Message-ID: <e385f1f9245b9.47bb72ca@optonline.net>

Hi - I have a trade price series whose file is really big. My first question is, is there any existing routine in R to extract OHLC (Open-High-Low-Close) of specified interval from irregular trade price series?

My second question is from my attempt to do this. The logic is not difficult but I am not familiar with fCalendar package especially timeDate object.

First of all, I do:

timeDate('2007-11-01',zone='GMT'); # I expect '2007-11-01 00:00:00 GMT'

The result sometimes show '2007-11-01 GMT' or '2007-10-31 18:00:00 Central standard time'. My question is how do I specify display format, says in 'EST' consistently? Note that my machine time locale is CST.

Third question, I try to generate a sequence of OHLC bar given a user input date. For example, I am interested in extracting OHLC from 6:00:00 EST to 14:30:00 EST on every weekday. How can I do that? 

Thank you,

- adschai


From jeff.a.ryan at gmail.com  Wed Feb 20 01:39:27 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Tue, 19 Feb 2008 18:39:27 -0600
Subject: [R-SIG-Finance] Extracting OHLC from trade price series
In-Reply-To: <e385f1f9245b9.47bb72ca@optonline.net>
References: <e385f1f9245b9.47bb72ca@optonline.net>
Message-ID: <e8e755250802191639p36e42eb3jfc40afcdd2eb1cdf@mail.gmail.com>

Hi,

The package 'xts' (the function in question previously part of
'quantmod') has a nice and fast aggregation function that allows you
to create OHLC from any univariate series, or from an existing OHLC
series - called 'to.period'.

library(quantmod)
getSymbols("QQQQ")

to.monthly(QQQQ)  # yields a monthly series from daily data

The code works equally well for anything from minute bars on up.  It
should work below that, though I can't promise anything as I haven't
really tested that recently.  Other functions in the group are
to.minutes, to.hourly, to to.daily... you get the idea.

to.period is the function you want to look at.  It calls Fortran - so
it is very fast on all but gigantic data sets - and then nothing is :)


The GMT issue has been discussed here many times over - try
Sys.setenv(TZ='GMT') for starters.  You are probably better off _not_
using timeDate - but I wrote xts so I wouldn't have to - to each his
own.


As for the last question... I am not too sure if I understand it
correctly.  As long as you subset the data before hand 'to.period'
shouldn't care about your index dates/times.

'xts' also allows for easier subsetting by date:

as.xts(myts)['2007-01-01::2007-01-23']

The help with regard to the latter is not all there yet - so feel free
to contact me with specific questions.

Jeff


On Feb 19, 2008 6:22 PM,  <adschai at optonline.net> wrote:
> Hi - I have a trade price series whose file is really big. My first question is, is there any existing routine in R to extract OHLC (Open-High-Low-Close) of specified interval from irregular trade price series?
>
> My second question is from my attempt to do this. The logic is not difficult but I am not familiar with fCalendar package especially timeDate object.
>
> First of all, I do:
>
> timeDate('2007-11-01',zone='GMT'); # I expect '2007-11-01 00:00:00 GMT'
>
> The result sometimes show '2007-11-01 GMT' or '2007-10-31 18:00:00 Central standard time'. My question is how do I specify display format, says in 'EST' consistently? Note that my machine time locale is CST.
>
> Third question, I try to generate a sequence of OHLC bar given a user input date. For example, I am interested in extracting OHLC from 6:00:00 EST to 14:30:00 EST on every weekday. How can I do that?
>
> Thank you,
>
> - adschai
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From jeff.a.ryan at gmail.com  Wed Feb 20 17:46:53 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 20 Feb 2008 10:46:53 -0600
Subject: [R-SIG-Finance] Extracting OHLC from trade price series
In-Reply-To: <e8e755250802191639p36e42eb3jfc40afcdd2eb1cdf@mail.gmail.com>
References: <e385f1f9245b9.47bb72ca@optonline.net>
	<e8e755250802191639p36e42eb3jfc40afcdd2eb1cdf@mail.gmail.com>
Message-ID: <e8e755250802200846x77c35cecn97214d3b97518e50@mail.gmail.com>

I wanted to clarify my comment with respect to timeDate:

>
>  The GMT issue has been discussed here many times over - try
>  Sys.setenv(TZ='GMT') for starters.  You are probably better off _not_
>  using timeDate - but I wrote xts so I wouldn't have to - to each his
>  own.
>
>

The 'timeDate' class itself is quite worthwhile.  There are many
methods that seem to be available that can make life easier.  My point
was more to the task at hand.  Simply subsetting an existing series,
or aggregating to a lower periodicity doesn't require much beyond
knowing time-stamps.

If time-stamps are needed - POSIXct, Date, and chron are all up to the
task.  It is entirely possible that 'timeDate' is equally useful, but
my thinking is always to start simple and them progress to the more
complicated only if necessary.

For what it's worth, 'xts' can also handle indexing by 'timeDate'.
See ?indexClass

I'd love to hear a bit from the Rmetrics developers regarding the
merits of 'timeDate'.  A comparison table of the different time/date
classes would be very beneficial to many I suspect.  I'd be more than
willing to try and put one together.

Jeff

On Tue, Feb 19, 2008 at 6:39 PM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> Hi,
>
>  The package 'xts' (the function in question previously part of
>  'quantmod') has a nice and fast aggregation function that allows you
>  to create OHLC from any univariate series, or from an existing OHLC
>  series - called 'to.period'.
>
>  library(quantmod)
>  getSymbols("QQQQ")
>
>  to.monthly(QQQQ)  # yields a monthly series from daily data
>
>  The code works equally well for anything from minute bars on up.  It
>  should work below that, though I can't promise anything as I haven't
>  really tested that recently.  Other functions in the group are
>  to.minutes, to.hourly, to to.daily... you get the idea.
>
>  to.period is the function you want to look at.  It calls Fortran - so
>  it is very fast on all but gigantic data sets - and then nothing is :)
>
>
>  The GMT issue has been discussed here many times over - try
>  Sys.setenv(TZ='GMT') for starters.  You are probably better off _not_
>  using timeDate - but I wrote xts so I wouldn't have to - to each his
>  own.
>
>
>  As for the last question... I am not too sure if I understand it
>  correctly.  As long as you subset the data before hand 'to.period'
>  shouldn't care about your index dates/times.
>
>  'xts' also allows for easier subsetting by date:
>
>  as.xts(myts)['2007-01-01::2007-01-23']
>
>  The help with regard to the latter is not all there yet - so feel free
>  to contact me with specific questions.
>
>  Jeff
>
>
>
>
>  On Feb 19, 2008 6:22 PM,  <adschai at optonline.net> wrote:
>  > Hi - I have a trade price series whose file is really big. My first question is, is there any existing routine in R to extract OHLC (Open-High-Low-Close) of specified interval from irregular trade price series?
>  >
>  > My second question is from my attempt to do this. The logic is not difficult but I am not familiar with fCalendar package especially timeDate object.
>  >
>  > First of all, I do:
>  >
>  > timeDate('2007-11-01',zone='GMT'); # I expect '2007-11-01 00:00:00 GMT'
>  >
>  > The result sometimes show '2007-11-01 GMT' or '2007-10-31 18:00:00 Central standard time'. My question is how do I specify display format, says in 'EST' consistently? Note that my machine time locale is CST.
>  >
>  > Third question, I try to generate a sequence of OHLC bar given a user input date. For example, I am interested in extracting OHLC from 6:00:00 EST to 14:30:00 EST on every weekday. How can I do that?
>  >
>  > Thank you,
>  >
>  > - adschai
>  >
>  > _______________________________________________
>  > R-SIG-Finance at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  > -- Subscriber-posting only.
>  > -- If you want to post, subscribe first.
>  >
>
>
>
>  --
>  There's a way to do it better - find it.
>  Thomas A. Edison
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From ggrothendieck at gmail.com  Wed Feb 20 17:52:15 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 20 Feb 2008 11:52:15 -0500
Subject: [R-SIG-Finance] Extracting OHLC from trade price series
In-Reply-To: <e8e755250802200846x77c35cecn97214d3b97518e50@mail.gmail.com>
References: <e385f1f9245b9.47bb72ca@optonline.net>
	<e8e755250802191639p36e42eb3jfc40afcdd2eb1cdf@mail.gmail.com>
	<e8e755250802200846x77c35cecn97214d3b97518e50@mail.gmail.com>
Message-ID: <971536df0802200852m3fa9b22bq3694f6a64717f1e2@mail.gmail.com>

On Wed, Feb 20, 2008 at 11:46 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> I wanted to clarify my comment with respect to timeDate:
>
> >
> >  The GMT issue has been discussed here many times over - try
> >  Sys.setenv(TZ='GMT') for starters.  You are probably better off _not_
> >  using timeDate - but I wrote xts so I wouldn't have to - to each his
> >  own.
> >
> >
>
> The 'timeDate' class itself is quite worthwhile.  There are many
> methods that seem to be available that can make life easier.  My point
> was more to the task at hand.  Simply subsetting an existing series,
> or aggregating to a lower periodicity doesn't require much beyond
> knowing time-stamps.
>
> If time-stamps are needed - POSIXct, Date, and chron are all up to the
> task.  It is entirely possible that 'timeDate' is equally useful, but
> my thinking is always to start simple and them progress to the more
> complicated only if necessary.
>
> For what it's worth, 'xts' can also handle indexing by 'timeDate'.
> See ?indexClass
>
> I'd love to hear a bit from the Rmetrics developers regarding the
> merits of 'timeDate'.  A comparison table of the different time/date
> classes would be very beneficial to many I suspect.  I'd be more than
> willing to try and put one together.

Date, chron and POSIXct are discussed and compared in the R News 4/1
help desk article.  There is a comparison table at the end of the article.


From jeff.a.ryan at gmail.com  Wed Feb 20 17:56:12 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 20 Feb 2008 10:56:12 -0600
Subject: [R-SIG-Finance] Extracting OHLC from trade price series
In-Reply-To: <971536df0802200852m3fa9b22bq3694f6a64717f1e2@mail.gmail.com>
References: <e385f1f9245b9.47bb72ca@optonline.net>
	<e8e755250802191639p36e42eb3jfc40afcdd2eb1cdf@mail.gmail.com>
	<e8e755250802200846x77c35cecn97214d3b97518e50@mail.gmail.com>
	<971536df0802200852m3fa9b22bq3694f6a64717f1e2@mail.gmail.com>
Message-ID: <e8e755250802200856n1beb55a0i3514f3da7c9a6c56@mail.gmail.com>

Gabor,

Thanks for pointing that out.

I meant to say 'an updated' table.  The one from 4/1 is very useful
indeed - I'd just like to see how 'timeDate' fits into the mix, as
well as any updates since the original was published.

Jeff

On Wed, Feb 20, 2008 at 10:52 AM, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> On Wed, Feb 20, 2008 at 11:46 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
>  > I wanted to clarify my comment with respect to timeDate:
>  >
>  > >
>  > >  The GMT issue has been discussed here many times over - try
>  > >  Sys.setenv(TZ='GMT') for starters.  You are probably better off _not_
>  > >  using timeDate - but I wrote xts so I wouldn't have to - to each his
>  > >  own.
>  > >
>  > >
>  >
>  > The 'timeDate' class itself is quite worthwhile.  There are many
>  > methods that seem to be available that can make life easier.  My point
>  > was more to the task at hand.  Simply subsetting an existing series,
>  > or aggregating to a lower periodicity doesn't require much beyond
>  > knowing time-stamps.
>  >
>  > If time-stamps are needed - POSIXct, Date, and chron are all up to the
>  > task.  It is entirely possible that 'timeDate' is equally useful, but
>  > my thinking is always to start simple and them progress to the more
>  > complicated only if necessary.
>  >
>  > For what it's worth, 'xts' can also handle indexing by 'timeDate'.
>  > See ?indexClass
>  >
>  > I'd love to hear a bit from the Rmetrics developers regarding the
>  > merits of 'timeDate'.  A comparison table of the different time/date
>  > classes would be very beneficial to many I suspect.  I'd be more than
>  > willing to try and put one together.
>
>  Date, chron and POSIXct are discussed and compared in the R News 4/1
>  help desk article.  There is a comparison table at the end of the article.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From jeff.a.ryan at gmail.com  Fri Feb 22 17:05:14 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 22 Feb 2008 10:05:14 -0600
Subject: [R-SIG-Finance] New IBrokers package on CRAN
Message-ID: <e8e755250802220805p4ee30971p62dbe092dae83c93@mail.gmail.com>

R-Sig-Finance:

I'd like to announce the alpha version of a new package IBrokers:

Designed to provide native R access to Interactive Brokers' trading
platform, it is currently in a very alpha stage.  I am releasing at
this early stage it to gauge interest and feedback as to which API
features should be considered priority.

It is working, though very little error checking (read: none) is done
at present.  I will be updating this aggressively.

?IBrokers

R API to the Interactive Brokers Trader Workstation (TWS).

Description:

     A very limited R implementation of the TWS API.  At present it is
     only able to access historic data from the Interactive Brokers
     servers. Future additions will include more API access and
     possible real-time charting via the 'quantmod' package.

     A word of warning.  I wrote this in 3 days.  Much of the current
     event loop is being rewritten/rethought to better handle errors, add
     some sort of callback mechanism, as well as
     speed up large data requests.  This version is mostly a
     proof-of-concept and will be updated regularly.  That said, please
     report any and all bugs/experiences to the maintainer so they can
     be incorporated into development versions.

     **DO NOT USE in an R session that you can't afford to lose!**

Details:


       Package:  IBrokers
       Type:     Package
       Version:  0.0-1
       Date:     2008-02-21
       License:  GPL-3

     The current API methods supported are:

     reqCurrentTime: The TWS server time in seconds since the epoch
     reqHistoricalData: Fetch historical data

Author(s):

     Jeffrey A. Ryan

     Maintainer: Jeffrey A. Ryan <jeff.a.ryan at gmail.com>

References:

     Interactive Brokers: <URL: www.interactivebrokers.com>

-- 
There's a way to do it better - find it.
Thomas A. Edison


From markus at insightfromdata.com  Fri Feb 22 20:14:26 2008
From: markus at insightfromdata.com (Markus Loecher)
Date: Fri, 22 Feb 2008 14:14:26 -0500
Subject: [R-SIG-Finance] abline() for zoo plot ?
Message-ID: <C3E48942.77A%markus@insightfromdata.com>

Can you somehow use the convenient abline() function in zoo plots ?
I am mainly looking for an easy way of adding vertical lines at specific
locations to my zoo plots.

Thanks!

Markus


From ggrothendieck at gmail.com  Fri Feb 22 20:35:36 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 22 Feb 2008 14:35:36 -0500
Subject: [R-SIG-Finance] abline() for zoo plot ?
In-Reply-To: <C3E48942.77A%markus@insightfromdata.com>
References: <C3E48942.77A%markus@insightfromdata.com>
Message-ID: <971536df0802221135j39a4e3bbk96f7951acd6751e8@mail.gmail.com>

Sure. Here are two examples with plot.zoo:

library(zoo)

# single panel
z <- zoo(1:10)
plot(z)
abline(v = 5)

# multipanel
plot(merge(z, lag(z, -1)), panel = function(x, y, ...) {lines(x, y);
abline(v = 5) } )

If you use xyplot.zoo then you can use all the lattice graphics facilities.


On Fri, Feb 22, 2008 at 2:14 PM, Markus Loecher
<markus at insightfromdata.com> wrote:
> Can you somehow use the convenient abline() function in zoo plots ?
> I am mainly looking for an easy way of adding vertical lines at specific
> locations to my zoo plots.
>
> Thanks!
>
> Markus


From edd at debian.org  Fri Feb 22 21:08:54 2008
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 22 Feb 2008 14:08:54 -0600
Subject: [R-SIG-Finance] abline() for zoo plot ?
In-Reply-To: <C3E48942.77A%markus@insightfromdata.com>
References: <C3E48942.77A%markus@insightfromdata.com>
Message-ID: <18367.11222.731322.712168@ron.nulle.part>


On 22 February 2008 at 14:14, Markus Loecher wrote:
| Can you somehow use the convenient abline() function in zoo plots ?
| I am mainly looking for an easy way of adding vertical lines at specific
| locations to my zoo plots.

Take a step back and remember that zoo does all the hard work of plotting
values over ordered observations, presumably dates or times.  So find those
dates and times and use them -- eg either directly via ISOdatetime() or
Date() or from the data.

Example:

> GOOG <- get.hist.quote("GOOG", start=as.Date("2007-01-01"), quote="Close", quiet=TRUE)
> plot(GOOG)
> mi <- which.min(GOOG[,"Close"])
> ma <- which.max(GOOG[,"Close"])
> abline( v=index(GOOG)[mi], col='green')
> abline( v=index(GOOG)[ma], col='red')

As which.min() and which.max() give you indices (ie 'position' in the ordered
obs vector) you need to then use those to index the actual zoo index object.

Hth,  Dirk

-- 
Three out of two people have difficulties with fractions.


From tom.soyer at gmail.com  Sat Feb 23 16:24:41 2008
From: tom.soyer at gmail.com (tom soyer)
Date: Sat, 23 Feb 2008 09:24:41 -0600
Subject: [R-SIG-Finance] functions for quarterly settlement dates
Message-ID: <65cc7bdf0802230724p15ed99dcg391d06e64b565367@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080223/a2df3657/attachment.pl 

From jeff.a.ryan at gmail.com  Sat Feb 23 17:19:07 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sat, 23 Feb 2008 10:19:07 -0600
Subject: [R-SIG-Finance] functions for quarterly settlement dates
In-Reply-To: <65cc7bdf0802230724p15ed99dcg391d06e64b565367@mail.gmail.com>
References: <65cc7bdf0802230724p15ed99dcg391d06e64b565367@mail.gmail.com>
Message-ID: <e8e755250802230819y46c0d425n3099eaf3ef67e28f@mail.gmail.com>

Hi Tom,

This gets the job done in almost all cases - there is an exception in
08, of which I don't have in front of me. At the very least it will
get you started.

> library(quantmod)
Loading required package: xts
Loading required package: zoo
Loading required package: Defaults
quantmod: Quantitative Financial Modelling Framework

Version 0.3-2
http://www.quantmod.com

> getSymbols("QQQQ")
[1] "QQQQ"
> index(QQQQ)[futures.expiry(QQQQ)]
[1] "2007-03-16" "2007-06-15" "2007-09-21" "2007-12-21"
> index(QQQQ)[options.expiry(QQQQ)]
 [1] "2007-01-19" "2007-02-16" "2007-03-16" "2007-04-20" "2007-05-18"
 [6] "2007-06-15" "2007-07-20" "2007-08-17" "2007-09-21" "2007-10-19"
[11] "2007-11-16" "2007-12-21" "2008-01-18" "2008-02-15"
>

Jeff

On Sat, Feb 23, 2008 at 9:24 AM, tom soyer <tom.soyer at gmail.com> wrote:
> Hi,
>
>  Are there built-in R functions that return both the nearest and the next
>  nearest quarterly settlement dates for options and futures?
>
>  Thanks,
>
>  --
>  Tom
>
>         [[alternative HTML version deleted]]
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From jeff.a.ryan at gmail.com  Sat Feb 23 17:22:33 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sat, 23 Feb 2008 10:22:33 -0600
Subject: [R-SIG-Finance] functions for quarterly settlement dates
In-Reply-To: <65cc7bdf0802230724p15ed99dcg391d06e64b565367@mail.gmail.com>
References: <65cc7bdf0802230724p15ed99dcg391d06e64b565367@mail.gmail.com>
Message-ID: <e8e755250802230822i599f9980h3a731935a478777@mail.gmail.com>

There isn't much magic here btw:

> futures.expiry
function (x)
{
    which(format(index(x), "%d") > 14 & format(index(x), "%d") <
        22 & format(index(x), "%w") == 5 & as.numeric(months(x,
        TRUE)) %in% c(3, 6, 9, 12))
}
<environment: namespace:quantmod>

It does need a zoo or a 'xts' object.

On Sat, Feb 23, 2008 at 9:24 AM, tom soyer <tom.soyer at gmail.com> wrote:
> Hi,
>
>  Are there built-in R functions that return both the nearest and the next
>  nearest quarterly settlement dates for options and futures?
>
>  Thanks,
>
>  --
>  Tom
>
>         [[alternative HTML version deleted]]
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From ggrothendieck at gmail.com  Sat Feb 23 17:33:15 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 23 Feb 2008 11:33:15 -0500
Subject: [R-SIG-Finance] functions for quarterly settlement dates
In-Reply-To: <65cc7bdf0802230724p15ed99dcg391d06e64b565367@mail.gmail.com>
References: <65cc7bdf0802230724p15ed99dcg391d06e64b565367@mail.gmail.com>
Message-ID: <971536df0802230833l2e26f3a8u32d746934ff9c430@mail.gmail.com>

On Sat, Feb 23, 2008 at 10:24 AM, tom soyer <tom.soyer at gmail.com> wrote:
> Are there built-in R functions that return both the nearest and the next
> nearest quarterly settlement dates for options and futures?

You can do it yourself using some functions in fCalendar
(see ?timeSpecialDate).

Another possibility is using as.yearqtr, as.yearmon
in zoo together with the nextfri() function defined in
vignette("zoo-quickref").  If d is a Date then
d1 <- as.Date(as.yearmon(as.yearqtr(d)) + 2/12)
would be the first day in the last month of the quarter and
nextfri(d1)+14
would be 3rd Fri that month.

This requires the zoo package for as.yearmon and as.yearqtr
but does not require a zoo object and d can be a vector of dates.


From spencer.graves at pdf.com  Sat Feb 23 20:37:21 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 23 Feb 2008 11:37:21 -0800
Subject: [R-SIG-Finance] IGARCH?
Message-ID: <47C075F1.5050006@pdf.com>

Hi, All: 

      What facilities are available in R for IGARCH estimation -- apart 
from "GarchOxInterface {fGarch}", which seems to require separate 
purchase and installation of Ox. 

      Thanks,
      Spencer Graves


From spencer.graves at pdf.com  Sat Feb 23 22:04:48 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 23 Feb 2008 13:04:48 -0800
Subject: [R-SIG-Finance] GARCH-M?
Message-ID: <47C08A70.40309@pdf.com>

Hi, All:

     What facilities are available in R for GARCH-M estimation, defined 
in Tsay (2005, p. 123) as follows: 


      r[t] = mu + c*s[t]^2 + a[t],

      a[t] = s[t]*e[t],

      s[t]^2 = alpha0 + alpha1 * a[t-1]^2 +  beta1 * s[t-1]^2,


where r[t] = (log) returns of a financial instrument with volatility 
premium "c". 


      Thanks,
      Spencer
Tsay (2005) Analysis of Financial Time Series, 2nd ed. (Wiley)


From jeff.a.ryan at gmail.com  Sun Feb 24 04:00:49 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sat, 23 Feb 2008 21:00:49 -0600
Subject: [R-SIG-Finance] functions for quarterly settlement dates
In-Reply-To: <e8e755250802230822i599f9980h3a731935a478777@mail.gmail.com>
References: <65cc7bdf0802230724p15ed99dcg391d06e64b565367@mail.gmail.com>
	<e8e755250802230822i599f9980h3a731935a478777@mail.gmail.com>
Message-ID: <e8e755250802231900p562d7cej87122220aaf7637e@mail.gmail.com>

Just to be complete:

March 2008 expiry is on the 22nd, but the last trading day for those
contracts is the 20th, as the 21st is an exchange holiday due to the
early Easter.

http://www.optionsclearing.com/publications/xcal/xcal2008.pdf

So futures.expiry and options.expiry will give the wrong day in that
particular case.  In all cases they give the Friday before the
Saturday expiration.

Jeff

On Sat, Feb 23, 2008 at 10:22 AM, Jeff Ryan <jeff.a.ryan at gmail.com> wrote:
> There isn't much magic here btw:
>
>  > futures.expiry
>  function (x)
>  {
>     which(format(index(x), "%d") > 14 & format(index(x), "%d") <
>         22 & format(index(x), "%w") == 5 & as.numeric(months(x,
>         TRUE)) %in% c(3, 6, 9, 12))
>  }
>  <environment: namespace:quantmod>
>
>  It does need a zoo or a 'xts' object.
>
>
>  On Sat, Feb 23, 2008 at 9:24 AM, tom soyer <tom.soyer at gmail.com> wrote:
>
>
> > Hi,
>  >
>  >  Are there built-in R functions that return both the nearest and the next
>  >  nearest quarterly settlement dates for options and futures?
>  >
>  >  Thanks,
>  >
>  >  --
>  >  Tom
>  >
>  >         [[alternative HTML version deleted]]
>  >
>  >  _______________________________________________
>  >  R-SIG-Finance at stat.math.ethz.ch mailing list
>  >  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  >  -- Subscriber-posting only.
>  >  -- If you want to post, subscribe first.
>  >
>
>
>
>
>
> --
>  There's a way to do it better - find it.
>  Thomas A. Edison
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From comtech.usa at gmail.com  Sun Feb 24 06:37:10 2008
From: comtech.usa at gmail.com (Michael)
Date: Sat, 23 Feb 2008 21:37:10 -0800
Subject: [R-SIG-Finance] where can I find source code for particle filters
	applied to stochastic volatilities?
Message-ID: <b1f16d9d0802232137t45a2bce5u6de191a3e7dfef9b@mail.gmail.com>

Hi all,

Could anybody point me to some overview/survey papers about using
particle filters and sequential monte carlo methods to estimate
stochastic volatilities? I couldn't find any such articles giving a
big-picture view of the literature.

How do these estimation methods compare to EMM and other Bayesian
methods for estimating stochastic volatilities?

Also, I am looking for some sample/source code that shows how to
program particle filters and sequential monte carlo methods to
estimate stochastic volatilities... Could anybody give me some
pointers? Thanks a lot


From babel at centrum.sk  Sun Feb 24 18:07:24 2008
From: babel at centrum.sk (babel at centrum.sk)
Date: Sun, 24 Feb 2008 18:07:24 +0100
Subject: [R-SIG-Finance] fitted fGarch model
Message-ID: <200802241807.12262@centrum.cz>

Hello guys
I have this question. How to fit the garch model and print all his fitted values. 
y=c+at  at=vt*sigma   vt=N(0,1)


library(fGarch)
fit = garchFit(~garch(1, 1), data =ret )

c<-fit at fitted
vt<-fit at residuals
sigma<-fit at sigma.t
at<-vt*sigma
model_garch<-c+at



Are these computations correct? Cause if I do model_garch=c+vt  I get the original series. 
Thank you very much for help.


From spencer.graves at pdf.com  Sun Feb 24 18:30:20 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 24 Feb 2008 09:30:20 -0800
Subject: [R-SIG-Finance] fitted fGarch model
In-Reply-To: <200802241807.12262@centrum.cz>
References: <200802241807.12262@centrum.cz>
Message-ID: <47C1A9AC.50603@pdf.com>



babel at centrum.sk wrote:
> Hello guys
> I have this question. How to fit the garch model and print all his fitted values. 
> y=c+at  at=vt*sigma   vt=N(0,1)
>
>
> library(fGarch)
> fit = garchFit(~garch(1, 1), data =ret )
>
> c<-fit at fitted
> vt<-fit at residuals
> sigma<-fit at sigma.t
> at<-vt*sigma
> model_garch<-c+at
>
>
>
> Are these computations correct? Cause if I do model_garch=c+vt  I get the original series. 
> Thank you very much for help.
>
>   
It looks like you've answered your own question.  Spencer
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From spencer.graves at pdf.com  Sun Feb 24 19:02:08 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 24 Feb 2008 10:02:08 -0800
Subject: [R-SIG-Finance] fitted fGarch model
In-Reply-To: <200802241807.12262@centrum.cz>
References: <200802241807.12262@centrum.cz>
Message-ID: <47C1B120.40503@pdf.com>

	  I need to modify my earlier response.

library(FinTS)
data(sp500)
spFit00.11a <- garchFit(sp500~garch(1,1), data=as.numeric(sp500))
at <- spFit00.11a at residuals
quantile(at)
           0%          25%          50%          75%         100%
-0.306849724 -0.028099724  0.001250276  0.029775276  0.414750276

	  From this, it is clear that garchFit()@residuals are NOT standardized.

	
st <- spFit00.11a at sigma.t
qqnorm(at, datax=TRUE)
qqnorm(at/st, datax=TRUE)

	  From comparing these two normal probability plots, it is apparent 
that 'at/st' is more normal than 'at'.

	  However, sd(at/st) = 0.9979, so the standardization is not perfect.

all.equal(as.numeric(sp500), spFit00.11a at fitted+at)	
# TRUE

	  Your pseudocode suggests that 'vt' are the standardized residuals, 
but that's not the case in this example.  If you have a real example, 
that contradicts this case, could you please report it?  Otherwise, I 
will assume that the test you reported was different from what you 
actually did.  It's always best to test an answer before giving it -- 
something I failed to do in my initial response to this question.

	  Best Wishes,
	  Spencer

babel at centrum.sk wrote:
> Hello guys
> I have this question. How to fit the garch model and print all his fitted values. 
> y=c+at  at=vt*sigma   vt=N(0,1)
>
>
> library(fGarch)
> fit = garchFit(~garch(1, 1), data =ret )
>
> c<-fit at fitted
> vt<-fit at residuals
> sigma<-fit at sigma.t
> at<-vt*sigma
> model_garch<-c+at
>
> 
>
> Are these computations correct? Cause if I do model_garch=c+vt  I get the original series. 
> Thank you very much for help.
>
>   
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From adrian_d at eskimo.com  Sun Feb 24 21:22:59 2008
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Sun, 24 Feb 2008 12:22:59 -0800 (PST)
Subject: [R-SIG-Finance] New IBrokers package on CRAN
In-Reply-To: <e8e755250802220805p4ee30971p62dbe092dae83c93@mail.gmail.com>
References: <e8e755250802220805p4ee30971p62dbe092dae83c93@mail.gmail.com>
Message-ID: <Pine.SUN.4.58.0802241207090.21748@eskimo.com>


Jeff,

Many thanks for putting together this package.  It should be very useful.

I am having trouble using it.  I know it's 0.0-1, but I would like to
start using it.  I have R 2.6.2, latest IB Workstation and latest API
9.40.  I use WinXP.

> require(IBrokers)
Loading required package: IBrokers
Loading required package: xts
Loading required package: zoo
IBrokers version 0.0-1: (pre-alpha)
See ?IBrokers for details
> con <- twsConnect()
Error in readChar(s, 1) : negative length vectors are not allowed
>

An IB window pops up and asks me if I want to accept incoming connection.
I press yes (but the error in the R GUI is already there).

If I try to understand where the error comes from:
> s <- socketConnection(port = 7496)
> s
       description              class               mode
text             opened
"->localhost:7496"           "socket"               "a+"
"text"           "opened"
          can read          can write
             "yes"              "yes"
> writeChar(as.character("1"), s)
> (curChar <- readChar(s, 1))

Another IB window pops up informing me that "The Socket client is out of
date and needs to be upgraded." And R GUI hangs up using 99% CPU.

Any suggestions are much appreciated.

In terms of the package wishlist, it would be nice to have access to the
Trades window and the Account window.

Adrian Dragulescu

I tried doing hooking R to IB using their Java API and got something
very rudimentary going on, but my Java wasn't that good to figure out how
to handle the multithreading features.





On Fri, 22 Feb 2008, Jeff Ryan wrote:

> R-Sig-Finance:
>
> I'd like to announce the alpha version of a new package IBrokers:
>
> Designed to provide native R access to Interactive Brokers' trading
> platform, it is currently in a very alpha stage.  I am releasing at
> this early stage it to gauge interest and feedback as to which API
> features should be considered priority.
>
> It is working, though very little error checking (read: none) is done
> at present.  I will be updating this aggressively.
>
> ?IBrokers
>
> R API to the Interactive Brokers Trader Workstation (TWS).
>
> Description:
>
>      A very limited R implementation of the TWS API.  At present it is
>      only able to access historic data from the Interactive Brokers
>      servers. Future additions will include more API access and
>      possible real-time charting via the 'quantmod' package.
>
>      A word of warning.  I wrote this in 3 days.  Much of the current
>      event loop is being rewritten/rethought to better handle errors, add
>      some sort of callback mechanism, as well as
>      speed up large data requests.  This version is mostly a
>      proof-of-concept and will be updated regularly.  That said, please
>      report any and all bugs/experiences to the maintainer so they can
>      be incorporated into development versions.
>
>      **DO NOT USE in an R session that you can't afford to lose!**
>
> Details:
>
>
>        Package:  IBrokers
>        Type:     Package
>        Version:  0.0-1
>        Date:     2008-02-21
>        License:  GPL-3
>
>      The current API methods supported are:
>
>      reqCurrentTime: The TWS server time in seconds since the epoch
>      reqHistoricalData: Fetch historical data
>
> Author(s):
>
>      Jeffrey A. Ryan
>
>      Maintainer: Jeffrey A. Ryan <jeff.a.ryan at gmail.com>
>
> References:
>
>      Interactive Brokers: <URL: www.interactivebrokers.com>
>
> --
> There's a way to do it better - find it.
> Thomas A. Edison
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Sun Feb 24 22:01:36 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Sun, 24 Feb 2008 15:01:36 -0600
Subject: [R-SIG-Finance] New IBrokers package on CRAN
In-Reply-To: <Pine.SUN.4.58.0802241207090.21748@eskimo.com>
References: <e8e755250802220805p4ee30971p62dbe092dae83c93@mail.gmail.com>
	<Pine.SUN.4.58.0802241207090.21748@eskimo.com>
Message-ID: <e8e755250802241301w753732e6ube8ee58dbc133b3a@mail.gmail.com>

Hi Adrian,

I probably should have made it version 0.0-0001 :)

I didn't mention in the docs - but you need to have the TWS
configured as you would for any client:

Configure => Api => All Api settings:

1) Add 127.0.0.1 to 'Trusted IP addresses'
2) Make sure that Enable ActiveX and Socket Clients is checked.

I think since the error checking is currently non-existent, something
must be get messed up
in the delay.

The first thing to the TWS in the connection is '37', which I think is
the client version # as far as the TWS is concerned.  I am very new to
this whole socket programming thing, so your patience is much
appreciated.

see twsConnect

I am running under Mac OSX at the moment, though I also have a Solaris
box.  No windows ones unfortunately to test yet.  One of these days
I'll have to break down and buy one :)

Trades and Account are probably quite easy to add.  I wrote it mostly
to fetch historical data, along the lines of RBloomberg, less the 20k
subscription fee :)

Mutlithreading is probably not even possible, at least not in the
traditional sense, from what I can tell of R's socket mechanisms.

I think the roadmap will be:

get it mostly bug free for data requests - those being
reqHistoricalData, reqMktData, and maybe a way to grab RT bars for a
set period.  Then add basic non-data methods.  From there _maybe_ look
into orders/etc.  But I seriously doubt the current native R code
implementation could handle that well.  I will probably look into a
C/C++ interface to get that done.

Anyway give a try per the above suggestions:  I just ran this myself:

> library(IBrokers)
Loading required package: xts
Loading required package: zoo
IBrokers version 0.0-1: (pre-alpha)
See ?IBrokers for details
> tws <- twsConnect()
> reqCurrentTime(tws)
[1] 1203885994
> dia <- reqHistoricalData(tws,twsEquity("DIA","SMART","ISLAND"))
> head(dia)
                     Open   High    Low  Close Volume    WAP hasGaps Count
2008-01-24 06:00:00 122.90 123.92 122.26 123.45 193868 123.07       0 48447
2008-01-25 06:00:00 124.63 124.71 121.72 122.19 194009 123.15       0 43732
2008-01-28 06:00:00 122.02 123.82 121.00 123.59 162384 122.36       0 37939
2008-01-29 06:00:00 124.21 124.90 123.34 124.56 146030 124.28       0 33402
2008-01-30 06:00:00 124.21 126.68 123.46 123.79 208275 125.05       0 44105
2008-01-31 06:00:00 122.68 126.90 122.41 126.10 212845 124.80       0 47008
>


On Sun, Feb 24, 2008 at 2:22 PM, Adrian Dragulescu <adrian_d at eskimo.com> wrote:
>
>  Jeff,
>
>  Many thanks for putting together this package.  It should be very useful.
>
>  I am having trouble using it.  I know it's 0.0-1, but I would like to
>  start using it.  I have R 2.6.2, latest IB Workstation and latest API
>  9.40.  I use WinXP.
>
>  > require(IBrokers)
>  Loading required package: IBrokers
>  Loading required package: xts
>  Loading required package: zoo
>  IBrokers version 0.0-1: (pre-alpha)
>  See ?IBrokers for details
>  > con <- twsConnect()
>  Error in readChar(s, 1) : negative length vectors are not allowed
>  >
>
>  An IB window pops up and asks me if I want to accept incoming connection.
>  I press yes (but the error in the R GUI is already there).
>
>  If I try to understand where the error comes from:
>  > s <- socketConnection(port = 7496)
>  > s
>        description              class               mode
>  text             opened
>  "->localhost:7496"           "socket"               "a+"
>  "text"           "opened"
>           can read          can write
>              "yes"              "yes"
>  > writeChar(as.character("1"), s)
>  > (curChar <- readChar(s, 1))
>
>  Another IB window pops up informing me that "The Socket client is out of
>  date and needs to be upgraded." And R GUI hangs up using 99% CPU.
>
>  Any suggestions are much appreciated.
>
>  In terms of the package wishlist, it would be nice to have access to the
>  Trades window and the Account window.
>
>  Adrian Dragulescu
>
>  I tried doing hooking R to IB using their Java API and got something
>  very rudimentary going on, but my Java wasn't that good to figure out how
>  to handle the multithreading features.
>
>
>
>
>
>
>
>  On Fri, 22 Feb 2008, Jeff Ryan wrote:
>
>  > R-Sig-Finance:
>  >
>  > I'd like to announce the alpha version of a new package IBrokers:
>  >
>  > Designed to provide native R access to Interactive Brokers' trading
>  > platform, it is currently in a very alpha stage.  I am releasing at
>  > this early stage it to gauge interest and feedback as to which API
>  > features should be considered priority.
>  >
>  > It is working, though very little error checking (read: none) is done
>  > at present.  I will be updating this aggressively.
>  >
>  > ?IBrokers
>  >
>  > R API to the Interactive Brokers Trader Workstation (TWS).
>  >
>  > Description:
>  >
>  >      A very limited R implementation of the TWS API.  At present it is
>  >      only able to access historic data from the Interactive Brokers
>  >      servers. Future additions will include more API access and
>  >      possible real-time charting via the 'quantmod' package.
>  >
>  >      A word of warning.  I wrote this in 3 days.  Much of the current
>  >      event loop is being rewritten/rethought to better handle errors, add
>  >      some sort of callback mechanism, as well as
>  >      speed up large data requests.  This version is mostly a
>  >      proof-of-concept and will be updated regularly.  That said, please
>  >      report any and all bugs/experiences to the maintainer so they can
>  >      be incorporated into development versions.
>  >
>  >      **DO NOT USE in an R session that you can't afford to lose!**
>  >
>  > Details:
>  >
>  >
>  >        Package:  IBrokers
>  >        Type:     Package
>  >        Version:  0.0-1
>  >        Date:     2008-02-21
>  >        License:  GPL-3
>  >
>  >      The current API methods supported are:
>  >
>  >      reqCurrentTime: The TWS server time in seconds since the epoch
>  >      reqHistoricalData: Fetch historical data
>  >
>  > Author(s):
>  >
>  >      Jeffrey A. Ryan
>  >
>  >      Maintainer: Jeffrey A. Ryan <jeff.a.ryan at gmail.com>
>  >
>  > References:
>  >
>  >      Interactive Brokers: <URL: www.interactivebrokers.com>
>  >
>  > --
>  > There's a way to do it better - find it.
>  > Thomas A. Edison
>  >
>
>
> > _______________________________________________
>  > R-SIG-Finance at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  > -- Subscriber-posting only.
>  > -- If you want to post, subscribe first.
>  >
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From LGu at vdm-usa.com  Mon Feb 25 02:32:23 2008
From: LGu at vdm-usa.com (Li Gu)
Date: Sun, 24 Feb 2008 20:32:23 -0500
Subject: [R-SIG-Finance] R-SIG-Finance Digest, Vol 45, Issue 22
Message-ID: <DADF843DEFB08346A7BAA14168699EE1047693BF@VDM01VMAI1.ny.vdms.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080224/3dee8fa2/attachment.pl 

From Mama_Attiglah at ssga.com  Mon Feb 25 11:11:51 2008
From: Mama_Attiglah at ssga.com (Attiglah, Mama)
Date: Mon, 25 Feb 2008 10:11:51 -0000
Subject: [R-SIG-Finance] fPortfolio Bug
Message-ID: <D84557769CE9ED47AC31686731990DCB0D390C53@INCLCW03A.corp.statestr.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080225/eef07dff/attachment.pl 

From peter at braverock.com  Tue Feb 26 01:34:55 2008
From: peter at braverock.com (Peter Carl)
Date: Mon, 25 Feb 2008 18:34:55 -0600
Subject: [R-SIG-Finance] Implementing Sharpe's style analysis with solve.QP
Message-ID: <200802251834.55971.peter@braverock.com>

I've recently become interested in implementing style analysis as described by 
William Sharpe in:
  http://www.stanford.edu/~wfsharpe/art/sa/sa.htm

In particular, I want to implement the 'constrained' method he discusses using 
the quadratic programming approach he advocates.  I thought that this might 
be easy to do in R using the function 'solve.QP' in the package 'quadprog'.  
Easy, of course, is a relative term.  I've found the documentation somewhat 
oblique, and Googling hasn't produced much in the way of help.  

So I thought I would offer what I've been able to figure out and try to get 
some feedback.  What I have here *seems* to work, although I haven't had to 
specify this kind of a problem myself in, well, decades.

According to Sharpe (1994) what we're trying to do is minimize the variance of 
the difference between the observed returns and the modeled returns:
min VAR(R.f - SUM[w_i * R.s_i]) = min VAR(F - w*S)
  s.t. SUM(w_i) = 1; w_i > 0

where:
R.f   Fund returns
R.s   Style returns
w_i   Style weights

Remembering that VAR(aX + bY) = a^2 VAR(X) + b^2 VAR(Y) + 2ab COV(X,Y), 
we can refactor the problem as:

= VAR(R.f) + w'*V*w - 2*w'*COV(R.f,R.s)

where:
V     Variance-covariance matrix of style index matrix
C     Vector of covariances between the style index and the fund

At this point, we can drop VAR[R.f] as it isn't a function of weights and 
multiply both sides by 1/2 to get:

min (1/2) w'*V*w - C'w
  s.t. w'*e = 1, w_i > 0

where:
e     Vector of 1's

Now, map that to the inputs of solve.QP, which is specified as:
min(-d' b + 1/2 b' D b) with the constraints A' b >= b_0

so:
b is the weight vector,
D is the variance-covariance matrix of the styles
d is the covariance vector between the fund and the styles

Here is the function.  The comments embedded within the function discuss how 
each of the components is constructed for solve.QP.

`style.QPfit` <-
function(R.fund, R.style, model=FALSE, ...) 
{
# INPUTS
# R.fund   Vector of a fund return time series
# R.style   Matrix of a style return time series
# 
# 
# OUTPUTS
# weights   Vector of optimal style index weights
# @ todo: TE  Tracking error between the calc'd and actual fund
# @ todo: Fp  Vector of calculated fund time series
# R^2  Coefficient of determination
#
    # Check to see if the required libraries are loaded
    if(!require("quadprog", quietly=TRUE))
        stop("package", sQuote("quadprog"), "is needed.  Stopping")

    R.fund = checkData(R.fund[,1,drop=FALSE], method="matrix")
    R.style = checkData(R.style, method="matrix")

    # @todo: Missing data is not allowed, use = "pairwise.complete.obs" ?
    style.rows = dim(R.style)[1]
    style.cols = dim(R.style)[2]

    # Calculate D and d
    Dmat = cov(R.style)
    dvec = cov(R.fund, R.style)

    # To specify A' b >= b_0, we create an nxn matrix Amat and the constraints 
    # vector b0.  First we tackle Amat.  The first constraint listed above is 
    # SUM[w_i] = 1.  The left side of the constraint is expressed as a vector 
    # of 1's:
    a1 = rep(1, style.cols)

    # In b0, which represents the right side of the equation, this vector is 
    # paired with the value '1'.

    # The second constraint sets the lower bound of the weights to zero. 
    # First, we set up an identity matrix.  
    a2 = matrix(0, style.cols, style.cols)
    diag(a2) = 1

    # It's paired in b0 with a vector of lower bounds set to zeros:
    w.min = rep(0, style.cols)

    # Construct A from the two components a1 and a2
    Amat = t(rbind(a1, a2))

    # Construct b_0
    b0 = c(1, w.min)


    # This is where 'meq' comes in.  The ?solve.QP page says:
    #     meq: the first 'meq' constraints are treated as equality
    #     constraints, all further as inequality constraints (defaults
    #     to 0).
    # I think that the way to interpret this is: if it is set to a value 
    #'q' <= n, the ordered constraints numbered less than or equal to 'q' are
    # treated as an equality constraint.  In this case, we only want to first
    # constraint to be treated as an equality, so that the weights would sum
    # to exactly 1.  So we set meq = 1.

    # With 'meq' set to 1, the second constraint (a2) is treated as an
    # inequality, so each weight is constrainted to be greater than or equal
    # to zero.
    optimal <- solve.QP(Dmat, dvec, Amat, bvec=b0, meq=1)

    weights = as.data.frame(optimal$solution)
    rownames(weights) = colnames(R.style)
    colnames(weights) = colnames(R.fund)[1]

    # Calculate metrics for the quality of the fit
    R.fitted = rowSums(R.style*matrix(rep(t(weights),style.rows), 
byrow=T,ncol=style.cols))
    R.nonfactor = R.fund - R.fitted

    R.squared = as.data.frame(1 - (var(R.nonfactor)/var(R.fund)))
    adj.R.squared = as.data.frame(1 - (1 - R.squared)*(style.rows - 
1)/(style.rows - style.cols - 1))

    rownames(R.squared) = "R-squared"
    rownames(adj.R.squared) = "Adj R-squared"

    if(model) 
        # returns the entire output of the model
        result = optimal
    else 
        result = list(weights = weights, R.squared = R.squared, adj.R.squared 
= adj.R.squared )

    # @todo: retain the labels on the weights
    # @todo: add other values to output, e.g., 
    #    result <- list(weights = optimal$solution, R.squared = , 
tracking.error = )

    return(result)
}

Here's a contrived example.  Using 10 years of monthly index data, I use the 
Russell 1000 Growth and Value indexes to "explain" the returns of the S&P.  
My prior is that it should roughly split the S&P index in half along the 
value and growth lines given how these indexes are constructed, and with a 
very high R^2.

Here's the data: 

> head(R.fund)
         SP500.TR 
Jan 1996   0.0340 
Feb 1996   0.0093 
Mar 1996   0.0096 
Apr 1996   0.0147 
May 1996   0.0258 
Jun 1996   0.0038 
> head(R.style)
         Russell.1000.Growth Russell.1000.Value
Jan 1996              0.0335             0.0312
Feb 1996              0.0183             0.0076
Mar 1996              0.0013             0.0170
Apr 1996              0.0263             0.0038
May 1996              0.0349             0.0125
Jun 1996              0.0014             0.0008

And here's the execution:

> style.QPfit(R.fund, R.style)
$weights
                     SP500.TR
Russell.1000.Growth 0.5047724
Russell.1000.Value  0.4952276

$R.squared
           SP500.TR
R-squared 0.9880977

$adj.R.squared
              SP500.TR
Adj R-squared  0.98793

So, this appears to work (at least for this simple example), and produces a 
high R^2 like we would expect.  To see the whole output of the solve.QP 
model, I can specify 'model' = TRUE:

> style.QPfit(R.fund[,1,drop=FALSE], R.style, model=TRUE)
$solution
[1] 0.5047724 0.4952276

$value
[1] -0.0008888153

$unconstrainted.solution
[1] 0.5040111 0.4815228

$iterations
[1] 2 0

$iact
[1] 1


Make sense?

pcc
-- 
Peter Carl
145 Scottswood Rd
Riverside, IL 60546
312 307 6346
http://www.braverock.com/~peter


From dsmith at viciscapital.com  Tue Feb 26 12:36:10 2008
From: dsmith at viciscapital.com (Dale Smith)
Date: Tue, 26 Feb 2008 06:36:10 -0500
Subject: [R-SIG-Finance] Implementing Sharpe's style analysis with
	solve.QP
In-Reply-To: <200802251834.55971.peter@braverock.com>
References: <200802251834.55971.peter@braverock.com>
Message-ID: <0E4F0C7EEAAB274F8DC6B1543949F05B014B3C09@vicsrv4.viciscapital.com>

Try it with monthly returns of a mutual fund, including bond & stock
indices. Bond mutual funds should have little weight for stocks and
vice-versa.

You may want to do a principal components analysis first as there can be
multicollinearity in market indices.

Dale Smith, Ph.D.
Vicis Capital, LLC
-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Peter Carl
Sent: Monday, February 25, 2008 7:35 PM
To: r-finance
Subject: [R-SIG-Finance] Implementing Sharpe's style analysis with
solve.QP

I've recently become interested in implementing style analysis as
described by 
William Sharpe in:
  http://www.stanford.edu/~wfsharpe/art/sa/sa.htm

In particular, I want to implement the 'constrained' method he discusses
using 
the quadratic programming approach he advocates.  I thought that this
might 
be easy to do in R using the function 'solve.QP' in the package
'quadprog'.  
Easy, of course, is a relative term.  I've found the documentation
somewhat 
oblique, and Googling hasn't produced much in the way of help.  

So I thought I would offer what I've been able to figure out and try to
get 
some feedback.  What I have here *seems* to work, although I haven't had
to 
specify this kind of a problem myself in, well, decades.

According to Sharpe (1994) what we're trying to do is minimize the
variance of 
the difference between the observed returns and the modeled returns:
min VAR(R.f - SUM[w_i * R.s_i]) = min VAR(F - w*S)
  s.t. SUM(w_i) = 1; w_i > 0

where:
R.f   Fund returns
R.s   Style returns
w_i   Style weights

Remembering that VAR(aX + bY) = a^2 VAR(X) + b^2 VAR(Y) + 2ab COV(X,Y), 
we can refactor the problem as:

= VAR(R.f) + w'*V*w - 2*w'*COV(R.f,R.s)

where:
V     Variance-covariance matrix of style index matrix
C     Vector of covariances between the style index and the fund

At this point, we can drop VAR[R.f] as it isn't a function of weights
and 
multiply both sides by 1/2 to get:

min (1/2) w'*V*w - C'w
  s.t. w'*e = 1, w_i > 0

where:
e     Vector of 1's

Now, map that to the inputs of solve.QP, which is specified as:
min(-d' b + 1/2 b' D b) with the constraints A' b >= b_0

so:
b is the weight vector,
D is the variance-covariance matrix of the styles
d is the covariance vector between the fund and the styles

Here is the function.  The comments embedded within the function discuss
how 
each of the components is constructed for solve.QP.

`style.QPfit` <-
function(R.fund, R.style, model=FALSE, ...) 
{
# INPUTS
# R.fund   Vector of a fund return time series
# R.style   Matrix of a style return time series
# 
# 
# OUTPUTS
# weights   Vector of optimal style index weights
# @ todo: TE  Tracking error between the calc'd and actual fund
# @ todo: Fp  Vector of calculated fund time series
# R^2  Coefficient of determination
#
    # Check to see if the required libraries are loaded
    if(!require("quadprog", quietly=TRUE))
        stop("package", sQuote("quadprog"), "is needed.  Stopping")

    R.fund = checkData(R.fund[,1,drop=FALSE], method="matrix")
    R.style = checkData(R.style, method="matrix")

    # @todo: Missing data is not allowed, use = "pairwise.complete.obs"
?
    style.rows = dim(R.style)[1]
    style.cols = dim(R.style)[2]

    # Calculate D and d
    Dmat = cov(R.style)
    dvec = cov(R.fund, R.style)

    # To specify A' b >= b_0, we create an nxn matrix Amat and the
constraints 
    # vector b0.  First we tackle Amat.  The first constraint listed
above is 
    # SUM[w_i] = 1.  The left side of the constraint is expressed as a
vector 
    # of 1's:
    a1 = rep(1, style.cols)

    # In b0, which represents the right side of the equation, this
vector is 
    # paired with the value '1'.

    # The second constraint sets the lower bound of the weights to zero.

    # First, we set up an identity matrix.  
    a2 = matrix(0, style.cols, style.cols)
    diag(a2) = 1

    # It's paired in b0 with a vector of lower bounds set to zeros:
    w.min = rep(0, style.cols)

    # Construct A from the two components a1 and a2
    Amat = t(rbind(a1, a2))

    # Construct b_0
    b0 = c(1, w.min)


    # This is where 'meq' comes in.  The ?solve.QP page says:
    #     meq: the first 'meq' constraints are treated as equality
    #     constraints, all further as inequality constraints (defaults
    #     to 0).
    # I think that the way to interpret this is: if it is set to a value

    #'q' <= n, the ordered constraints numbered less than or equal to
'q' are
    # treated as an equality constraint.  In this case, we only want to
first
    # constraint to be treated as an equality, so that the weights would
sum
    # to exactly 1.  So we set meq = 1.

    # With 'meq' set to 1, the second constraint (a2) is treated as an
    # inequality, so each weight is constrainted to be greater than or
equal
    # to zero.
    optimal <- solve.QP(Dmat, dvec, Amat, bvec=b0, meq=1)

    weights = as.data.frame(optimal$solution)
    rownames(weights) = colnames(R.style)
    colnames(weights) = colnames(R.fund)[1]

    # Calculate metrics for the quality of the fit
    R.fitted = rowSums(R.style*matrix(rep(t(weights),style.rows), 
byrow=T,ncol=style.cols))
    R.nonfactor = R.fund - R.fitted

    R.squared = as.data.frame(1 - (var(R.nonfactor)/var(R.fund)))
    adj.R.squared = as.data.frame(1 - (1 - R.squared)*(style.rows - 
1)/(style.rows - style.cols - 1))

    rownames(R.squared) = "R-squared"
    rownames(adj.R.squared) = "Adj R-squared"

    if(model) 
        # returns the entire output of the model
        result = optimal
    else 
        result = list(weights = weights, R.squared = R.squared,
adj.R.squared 
= adj.R.squared )

    # @todo: retain the labels on the weights
    # @todo: add other values to output, e.g., 
    #    result <- list(weights = optimal$solution, R.squared = , 
tracking.error = )

    return(result)
}

Here's a contrived example.  Using 10 years of monthly index data, I use
the 
Russell 1000 Growth and Value indexes to "explain" the returns of the
S&P.  
My prior is that it should roughly split the S&P index in half along the

value and growth lines given how these indexes are constructed, and with
a 
very high R^2.

Here's the data: 

> head(R.fund)
         SP500.TR 
Jan 1996   0.0340 
Feb 1996   0.0093 
Mar 1996   0.0096 
Apr 1996   0.0147 
May 1996   0.0258 
Jun 1996   0.0038 
> head(R.style)
         Russell.1000.Growth Russell.1000.Value
Jan 1996              0.0335             0.0312
Feb 1996              0.0183             0.0076
Mar 1996              0.0013             0.0170
Apr 1996              0.0263             0.0038
May 1996              0.0349             0.0125
Jun 1996              0.0014             0.0008

And here's the execution:

> style.QPfit(R.fund, R.style)
$weights
                     SP500.TR
Russell.1000.Growth 0.5047724
Russell.1000.Value  0.4952276

$R.squared
           SP500.TR
R-squared 0.9880977

$adj.R.squared
              SP500.TR
Adj R-squared  0.98793

So, this appears to work (at least for this simple example), and
produces a 
high R^2 like we would expect.  To see the whole output of the solve.QP 
model, I can specify 'model' = TRUE:

> style.QPfit(R.fund[,1,drop=FALSE], R.style, model=TRUE)
$solution
[1] 0.5047724 0.4952276

$value
[1] -0.0008888153

$unconstrainted.solution
[1] 0.5040111 0.4815228

$iterations
[1] 2 0

$iact
[1] 1


Make sense?

pcc
-- 
Peter Carl
145 Scottswood Rd
Riverside, IL 60546
312 307 6346
http://www.braverock.com/~peter

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

All e-mail sent to or from this address will be received or otherwise recorded by Vicis Capital, LLC and is subject to archival, monitoring and/or review, by and/or disclosure to, someone other than the recipient.  This message is intended only for the use of the person(s) ("intended recipient") to whom it is addressed.  It may contain information that is privileged and confidential.  If you are not the intended recipient, please contact the sender as soon as possible and delete the message without reading it or making a copy.  Any dissemination, distribution, copying, or other use of this message or any of its content by any person other than the intended recipient is strictly prohibited.  Vicis Capital, LLC only transacts business in states where it is properly registered or notice filed, or excluded or exempted from registration or notice filing requirements.


From babel at centrum.sk  Tue Feb 26 23:29:39 2008
From: babel at centrum.sk (babel at centrum.sk)
Date: Tue, 26 Feb 2008 23:29:39 +0100
Subject: [R-SIG-Finance] garch in R vs Matlab
Message-ID: <200802262329.6847@centrum.cz>

Hello guys
Why residuals in R are different than in matlab. Here is example
fit = garchFit(~garch(1, 1), data =ret)
res1<-fit at residuals
garch11<-ret-res1
garch11 = 0.0001074985  0.0001074985   0.0001074985 ......     which is coefficient  mu  from garchFit

in MATLAB
[coeff,errors,LLF,innovations,sigmas,summary] = garchfit(ret);

garch11<-ret-innovations =    3.066250e-04    1.075012e-04   -1.909928e-04   5.055000e-04 .. 

Innovations (in help there is also use name "residuals") in Matlab should be the same residuals that those in R (fit at residuals) ? Even when coefficients alpha, beta, omega are same. Also simulation results are totally different. Does R count garch different than Matlab, or just additional mathematical operation are needed? 
Regards Jano


From spencer.graves at pdf.com  Wed Feb 27 05:32:47 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 26 Feb 2008 20:32:47 -0800
Subject: [R-SIG-Finance] garch in R vs Matlab
In-Reply-To: <200802262329.6847@centrum.cz>
References: <200802262329.6847@centrum.cz>
Message-ID: <47C4E7EF.1090105@pdf.com>

      Your example is not self contained, and I'm unable to replicate it. 

      Consider the following (from ch03.R in the 'FinTS' package): 

library(FinTS)
library(fGarch)
data(sp500)
spFit00.11a <- garchFit(sp500~garch(1,1), data=as.numeric(sp500))
str(spFit00.11a at residuals)
 num [1:792]  0.015050 -0.051450 -0.066550  0.015250  0.000250 ...

      Clearly, these residuals are not constant. 

      Would you mind running this example through the Matlab 'garchfit' 
function and reporting the answers to us? 

      Also, would you mind providing us with the following: 

str(ret)
sessionInfo()

      Finally, if you'd like to increase the expected utility of replies 
to future posts, I suggest you check the Posting Guide at 
www.r-project.org -> "Mailing Lists" -> "Posting Guide" (in the top line). 
        
      Hope this helps. 
      Spencer

babel at centrum.sk wrote:
> Hello guys
> Why residuals in R are different than in matlab. Here is example
> fit = garchFit(~garch(1, 1), data =ret)
> res1<-fit at residuals
> garch11<-ret-res1
> garch11 = 0.0001074985  0.0001074985   0.0001074985 ......     which is coefficient  mu  from garchFit
>
> in MATLAB
> [coeff,errors,LLF,innovations,sigmas,summary] = garchfit(ret);
>
> garch11<-ret-innovations =    3.066250e-04    1.075012e-04   -1.909928e-04   5.055000e-04 .. 
>
> Innovations (in help there is also use name "residuals") in Matlab should be the same residuals that those in R (fit at residuals) ? Even when coefficients alpha, beta, omega are same. Also simulation results are totally different. Does R count garch different than Matlab, or just additional mathematical operation are needed? 
> Regards Jano
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From mmiklovic at yahoo.com  Wed Feb 27 10:13:48 2008
From: mmiklovic at yahoo.com (michal miklovic)
Date: Wed, 27 Feb 2008 01:13:48 -0800 (PST)
Subject: [R-SIG-Finance] garch in R vs Matlab
Message-ID: <363274.28896.qm@web50109.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080227/2dae8a29/attachment.pl 

From guillaume.nicoulaud at halbis.com  Wed Feb 27 10:18:44 2008
From: guillaume.nicoulaud at halbis.com (guillaume.nicoulaud at halbis.com)
Date: Wed, 27 Feb 2008 10:18:44 +0100
Subject: [R-SIG-Finance] RBloomberg
Message-ID: <OF6B0F32DA.88F0E66B-ONC12573FC.0032FC58-C12573FC.00332779@hsbc.fr>


Dear all,
Using RBloomberg, how do I get historical prices, one per day, but always at a certain time (17:00h for example)?
Thanks


Les informations contenues dans ce message sont confidentielles et peuvent constituer des informations privilegiees. Si vous n etes pas le destinataire de ce message, il vous est interdit de le copier, de le faire suivre, de le divulguer ou d en utiliser tout ou partie. Si vous avez recu ce message par erreur, merci de le supprimer de votre systeme, ainsi que toutes ses copies, et d en avertir immediatement l expediteur par message de retour.
Il est impossible de garantir que les communications par messagerie electronique arrivent en temps utile, sont securisees ou denuees de toute erreur ou virus. En consequence, l expediteur n accepte aucune responsabilite du fait des erreurs ou omissions qui pourraient en resulter.
--- ----------------------------------------------------- ---
The information contained in this e-mail is confidential...{{dropped:3}}


From feanor0 at hotmail.com  Wed Feb 27 15:26:05 2008
From: feanor0 at hotmail.com (Murali Menon)
Date: Wed, 27 Feb 2008 14:26:05 +0000
Subject: [R-SIG-Finance] applying to.period on matrices
Message-ID: <BLU105-W60C608C700AA0380DC80F1EE1A0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080227/de676d3d/attachment.pl 

From davidr at rhotrading.com  Wed Feb 27 15:35:47 2008
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 27 Feb 2008 08:35:47 -0600
Subject: [R-SIG-Finance] RBloomberg
In-Reply-To: <OF6B0F32DA.88F0E66B-ONC12573FC.0032FC58-C12573FC.00332779@hsbc.fr>
References: <OF6B0F32DA.88F0E66B-ONC12573FC.0032FC58-C12573FC.00332779@hsbc.fr>
Message-ID: <F9F2A641C593D7408925574C05A7BE77957C73@rhopost.rhotrading.com>

This has been a perennial problem for me trying to match up different
zones.
If it is just a matter of which close, you might be able to set your
default times through, e.g. PDFG.
It will have to be set for each sector I think.
Otherwise, the only way I have been able to overcome it is to get
intraday data and pick out the matching times.
Of course, that only gets you 50 days or so.
Good luck!
David L. Reiner, PhD
Head Quant
Rho Trading Securities, LLC

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
guillaume.nicoulaud at halbis.com
Sent: Wednesday, February 27, 2008 3:19 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] RBloomberg


Dear all,
Using RBloomberg, how do I get historical prices, one per day, but
always at a certain time (17:00h for example)?
Thanks


Les informations contenues dans ce message sont confidentielles et
peuvent constituer des informations privilegiees. Si vous n etes pas le
destinataire de ce message, il vous est interdit de le copier, de le
faire suivre, de le divulguer ou d en utiliser tout ou partie. Si vous
avez recu ce message par erreur, merci de le supprimer de votre systeme,
ainsi que toutes ses copies, et d en avertir immediatement l expediteur
par message de retour.
Il est impossible de garantir que les communications par messagerie
electronique arrivent en temps utile, sont securisees ou denuees de
toute erreur ou virus. En consequence, l expediteur n accepte aucune
responsabilite du fait des erreurs ou omissions qui pourraient en
resulter.
--- ----------------------------------------------------- ---
The information contained in this e-mail is confidential...{{dropped:8}}


From ggrothendieck at gmail.com  Wed Feb 27 15:51:06 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Feb 2008 09:51:06 -0500
Subject: [R-SIG-Finance] applying to.period on matrices
In-Reply-To: <BLU105-W60C608C700AA0380DC80F1EE1A0@phx.gbl>
References: <BLU105-W60C608C700AA0380DC80F1EE1A0@phx.gbl>
Message-ID: <971536df0802270651u37af4917k4b82eaca5e3417b8@mail.gmail.com>

apply is not a generic function with class-specific methods.  Try this:

do.call(cbind, lapply(as.list(xts.ts), to.weekly))


On Wed, Feb 27, 2008 at 9:26 AM, Murali Menon <feanor0 at hotmail.com> wrote:
>
> Folks,
>
> I understand that the to.weekly (etc.) functions work only on univariate series or OHLC objects, so I thought that the standard apply(x, 2, to.weekly) technique might work. Unfortunately, it doesn't. What am I doing wrong? Please advise.
>
> > xx <- matrix(rnorm(1000), ncol=10)> xts.ts <- xts(xx, as.Date(13514:13613), origin="1970-01-01")> is.xts(xts.ts)[1] TRUE> to.weekly(xts.ts)Error in to.period(x, "weeks", name = name, ...) :   'x' must be a single column or an OHLC type object> apply(xts.ts, 2, to.weekly)Error in UseMethod("as.xts") : no applicable method for "as.xts"
> I'm guessing that the individual columns of the xts.ts object are not themselves xts objects? The index info is lost?
>
> I'm using R2.6.1 on Windows XP, and xts_0.0-10,  zoo_1.4-1
>
> Thanks,
>
> Murali
> _________________________________________________________________
> Climb to the top of the charts! Play the word scramble challenge with star power.
>
>        [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From feanor0 at hotmail.com  Wed Feb 27 16:37:09 2008
From: feanor0 at hotmail.com (Murali Menon)
Date: Wed, 27 Feb 2008 15:37:09 +0000
Subject: [R-SIG-Finance] Using rownames to index into xts objects
Message-ID: <BLU105-W52189E8193CAA739FD9B48EE1A0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080227/c279ffb8/attachment.pl 

From ggrothendieck at gmail.com  Wed Feb 27 16:50:19 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 27 Feb 2008 10:50:19 -0500
Subject: [R-SIG-Finance] Using rownames to index into xts objects
In-Reply-To: <BLU105-W52189E8193CAA739FD9B48EE1A0@phx.gbl>
References: <BLU105-W52189E8193CAA739FD9B48EE1A0@phx.gbl>
Message-ID: <971536df0802270750i4da3a736j87b003adcdc27a36@mail.gmail.com>

Try this:

time(merge(xts.ts, yts.ts, all = FALSE))

See ?merge.zoo


On Wed, Feb 27, 2008 at 10:37 AM, Murali Menon <feanor0 at hotmail.com> wrote:
>
> Hi,
>
> Another index-related query on xts. If I have two xts objects indexed by different dates, and I want to extract the common dates to reindex into these objects (to find contemporaneous values), I thought the following would work:
>
> > xx <- matrix(rnorm(1000), ncol=10); xts.ts <- xts(xx, as.Date(13514:13613), origin="1970-01-01")
> > yy <- matrix(rnorm(1000), ncol=10); yts.ts <- xts(yy, as.Date(13554:13653), origin="1970-01-01")
> > intersect(rownames(xts.ts),rownames(yts.ts)) [1] "2007-02-10" "2007-02-11" "2007-02-12" "2007-02-13" "2007-02-14" "2007-02-15" "2007-02-16" "2007-02-17" "2007-02-18" "2007-02-19" "2007-02-20"[12] "2007-02-21" "2007-02-22" "2007-02-23" "2007-02-24" "2007-02-25" "2007-02-26" "2007-02-27" "2007-02-28" "2007-03-01" "2007-03-02" "2007-03-03"[23] "2007-03-04" "2007-03-05" "2007-03-06" "2007-03-07" "2007-03-08" "2007-03-09" "2007-03-10" "2007-03-11" "2007-03-12" "2007-03-13" "2007-03-14"[34] "2007-03-15" "2007-03-16" "2007-03-17" "2007-03-18" "2007-03-19" "2007-03-20" "2007-03-21" "2007-03-22" "2007-03-23" "2007-03-24" "2007-03-25"[45] "2007-03-26" "2007-03-27" "2007-03-28" "2007-03-29" "2007-03-30" "2007-03-31" "2007-04-01" "2007-04-02" "2007-04-03" "2007-04-04" "2007-04-05"[56] "2007-04-06" "2007-04-07" "2007-04-08" "2007-04-09" "2007-04-10"
>
> so we have 60 common dates between the two series. I now want to fish out just those entries:
>
> > xts.ts[intersect(rownames(xts.ts), rownames(yts.ts)),]2007-02-10 -1.319601 -0.09154885 -1.991032 0.9530218 -0.3577950 -0.02353374 -0.4677263 -1.072972 -1.881379 1.130009
>
> It only returns the first entry. I guess I'm missing some trivial matter here, but can't for the life of me figure out what. Please help!
>
> Cheers,
> Murali


From jeff.a.ryan at gmail.com  Wed Feb 27 16:51:01 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 27 Feb 2008 09:51:01 -0600
Subject: [R-SIG-Finance] Using rownames to index into xts objects
In-Reply-To: <BLU105-W52189E8193CAA739FD9B48EE1A0@phx.gbl>
References: <BLU105-W52189E8193CAA739FD9B48EE1A0@phx.gbl>
Message-ID: <e8e755250802270751w1f9e373ve37bb66b8ec7ea84@mail.gmail.com>

Hi Murali:

Try:

> xts.ts[which(rownames(xts.ts) %in% rownames(yts.ts))]

The problem is that the first '2007-02-10' is getting interpreted as a
'date' index, if you will, and discarding the rest:

The way that gets interpreted is in the following form (with the range
below for illustrative purposes)
.
xts.ts['2007-02-10::2007-02-20']

Jeff

On Wed, Feb 27, 2008 at 9:37 AM, Murali Menon <feanor0 at hotmail.com> wrote:
>
>  Hi,
>
>  Another index-related query on xts. If I have two xts objects indexed by different dates, and I want to extract the common dates to reindex into these objects (to find contemporaneous values), I thought the following would work:
>
>  > xx <- matrix(rnorm(1000), ncol=10); xts.ts <- xts(xx, as.Date(13514:13613), origin="1970-01-01")
>  > yy <- matrix(rnorm(1000), ncol=10); yts.ts <- xts(yy, as.Date(13554:13653), origin="1970-01-01")
>  > intersect(rownames(xts.ts),rownames(yts.ts)) [1] "2007-02-10" "2007-02-11" "2007-02-12" "2007-02-13" "2007-02-14" "2007-02-15" "2007-02-16" "2007-02-17" "2007-02-18" "2007-02-19" "2007-02-20"[12] "2007-02-21" "2007-02-22" "2007-02-23" "2007-02-24" "2007-02-25" "2007-02-26" "2007-02-27" "2007-02-28" "2007-03-01" "2007-03-02" "2007-03-03"[23] "2007-03-04" "2007-03-05" "2007-03-06" "2007-03-07" "2007-03-08" "2007-03-09" "2007-03-10" "2007-03-11" "2007-03-12" "2007-03-13" "2007-03-14"[34] "2007-03-15" "2007-03-16" "2007-03-17" "2007-03-18" "2007-03-19" "2007-03-20" "2007-03-21" "2007-03-22" "2007-03-23" "2007-03-24" "2007-03-25"[45] "2007-03-26" "2007-03-27" "2007-03-28" "2007-03-29" "2007-03-30" "2007-03-31" "2007-04-01" "2007-04-02" "2007-04-03" "2007-04-04" "2007-04-05"[56] "2007-04-06" "2007-04-07" "2007-04-08" "2007-04-09" "2007-04-10"
>
>  so we have 60 common dates between the two series. I now want to fish out just those entries:
>
>  > xts.ts[intersect(rownames(xts.ts), rownames(yts.ts)),]2007-02-10 -1.319601 -0.09154885 -1.991032 0.9530218 -0.3577950 -0.02353374 -0.4677263 -1.072972 -1.881379 1.130009
>
>  It only returns the first entry. I guess I'm missing some trivial matter here, but can't for the life of me figure out what. Please help!
>
>  Cheers,
>  Murali
>  _________________________________________________________________
>  Climb to the top of the charts! Play the word scramble challenge with star power.
>
>         [[alternative HTML version deleted]]
>
>
> _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From spencer.graves at pdf.com  Wed Feb 27 20:21:20 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 27 Feb 2008 11:21:20 -0800
Subject: [R-SIG-Finance] garch in R vs Matlab
In-Reply-To: <363274.28896.qm@web50109.mail.re2.yahoo.com>
References: <363274.28896.qm@web50109.mail.re2.yahoo.com>
Message-ID: <47C5B830.1090007@pdf.com>

      I now see things I didn't see before: 

           (1) With "garch11<-ret-res1", garch11 = mu.  Since the model 
is "~garch(1,1)", I'd be worried if it were otherwise, because the model 
for the 'mean' in this case is only the mean. 

           (2) Per Michal's suggestions, it looks like 'innovations' may 
be standardized residuals, which suggests we might want to compare 
'innovations' with (fit at residuals / fit at sigma.t). 

      In addition, you may wish to consider the following: 

           (3) Using 'garchSim{fGarch}, you could simulate anything you 
want in R, then export the numbers to Matlab and compare the estimates.  
If you made that comparison, I'm sure that other people would be 
interested in the answers.  [If you export from R to a csv file, I 
suggest you first round the numbers off to, say, 4 significant digits, 
then apply 'garchFit' to the rounded numbers, to ensure you are actually 
analyzing the same numbers in R and Matlab.] 

           (4) If you do very much with both R and Matlab, you may wish 
to investigate the R.matlab package if you haven't already.  It's not 
the easiest thing to use, but if you do very much with both R and 
Matlab, you may wish to look at it if you haven't already.   
    
     Spencer    

michal miklovic wrote:
> Hi
>
> I would suggest you to compare standardised residuals, defined as fit at residuals / fit at sigma.t, from garchFit estimation with innovations from matlab.
> Simulation results could be different because random numbers generated by the two programs are different.
>
> Regards
>
> Michal
>
>
>
> ----- Original Message ----
> From: "babel at centrum.sk" <babel at centrum.sk>
> To: R-SIG-Finance <r-sig-finance at stat.math.ethz.ch>
> Sent: Tuesday, February 26, 2008 11:29:39 PM
> Subject: [R-SIG-Finance] garch in R vs Matlab
>
> Hello guys
> Why residuals in R are different than in matlab. Here is example
> fit = garchFit(~garch(1, 1), data =ret)
> res1<-fit at residuals
> garch11<-ret-res1
> garch11 = 0.0001074985  0.0001074985   0.0001074985 ......     which is coefficient  mu  from garchFit
>
> in MATLAB
> [coeff,errors,LLF,innovations,sigmas,summary] = garchfit(ret);
>
> garch11<-ret-innovations =    3.066250e-04    1.075012e-04   -1.909928e-04   5.055000e-04 .. 
>
> Innovations (in help there is also use name "residuals") in Matlab should be the same residuals that those in R (fit at residuals) ? Even when coefficients alpha, beta, omega are same. Also simulation results are totally different. Does R count garch different than Matlab, or just additional mathematical operation are needed? 
> Regards Jano
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
>
>
>
>
>
>       ____________________________________________________________________________________
> Looking for last minute shopping deals?  
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Wed Feb 27 21:28:21 2008
From: jeff.a.ryan at gmail.com (jeff.a.ryan at gmail.com)
Date: Wed, 27 Feb 2008 20:28:21 +0000
Subject: [R-SIG-Finance] applying to.period on matrices
In-Reply-To: <BLU105-W60C608C700AA0380DC80F1EE1A0@phx.gbl>
References: <BLU105-W60C608C700AA0380DC80F1EE1A0@phx.gbl>
Message-ID: <1881555332-1204144126-cardhu_decombobulator_blackberry.rim.net-1265643319-@bxe144.bisx.prod.on.blackberry>

Murali,

The core issue with to.period is that it is aggregating a series by period and tabulating the opening, closing, max (high) and min (low) value over these periods.

The resultant object is by necessity an OHLC object.  Currently Volume is summed if available too, but that isn't required.

Applying to a matrix is sort of ill-defined for this - as I am not too sure what the output would look like.  A list of OHLC objects?

Subsetting an xts does indeed result in another xts object.  Internally xts, like zoo, is just a matrix - with an index attribute.  So technically the columns aren't xts.

If you are just looking to calculate something -along- a period, say a maximum weekly value or a function applied periodically - look at period.apply.

Jeff
Sent via BlackBerry from T-Mobile

-----Original Message-----
From: Murali Menon <feanor0 at hotmail.com>

Date: Wed, 27 Feb 2008 14:26:05 
To:<r-sig-finance at stat.math.ethz.ch>
Subject: [R-SIG-Finance] applying to.period on matrices



Folks,
 
I understand that the to.weekly (etc.) functions work only on univariate series or OHLC objects, so I thought that the standard apply(x, 2, to.weekly) technique might work. Unfortunately, it doesn't. What am I doing wrong? Please advise.
 
> xx <- matrix(rnorm(1000), ncol=10)> xts.ts <- xts(xx, as.Date(13514:13613), origin="1970-01-01")> is.xts(xts.ts)[1] TRUE> to.weekly(xts.ts)Error in to.period(x, "weeks", name = name, ...) :   'x' must be a single column or an OHLC type object> apply(xts.ts, 2, to.weekly)Error in UseMethod("as.xts") : no applicable method for "as.xts"
I'm guessing that the individual columns of the xts.ts object are not themselves xts objects? The index info is lost?
 
I'm using R2.6.1 on Windows XP, and xts_0.0-10,  zoo_1.4-1
 
Thanks,
 
Murali
_________________________________________________________________
Climb to the top of the charts!?Play the word scramble challenge with star power.

	[[alternative HTML version deleted]]


_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

From robert at sanctumfi.com  Thu Feb 28 13:22:24 2008
From: robert at sanctumfi.com (Robert Sams)
Date: Thu, 28 Feb 2008 12:22:24 -0000
Subject: [R-SIG-Finance] RBloomberg
References: <SANCTUMFISERVEROPpB00000602@sanctumfi.com>
Message-ID: <SANCTUMFISERVERRCIl00000970@sanctumfi.com>

Hi,

To my knowledge, the Bloomberg API does not provide the functionality of
timeseries snapped at a user-specified timestamp (if I'm mistaken on
this point, please let me know and I'll incorporate this into
RBloomberg). This is unfortunate, but we have two second-best options.
The first is security specific. Some securities allow you to add a data
source modifier to the ticker that provides some flexibility with
regards to historical timestamps. For example, the 2y USD swap rate:

blpGetData(conn, c("USSW2 CMN3 CURNCY","USSW2 CMPN CURNCY"), "PX_LAST",
start=chron("2/20/8"))
         USSW2 CMN3 CURNCY USSW2 CMPN CURNCY
02/20/08            3.0065            2.9945
02/21/08            2.8560            2.8360
02/22/08            2.8925            2.9225
02/25/08            2.9950            2.9980
02/26/08            2.8910            2.8795
02/27/08            2.8340            2.8245
02/28/08                NA            2.7650

The provider codes CMN3 and CMPN are the Bloomberg composite 3pm and 5pm
New York close rates, respectively. This is not a general solution,
however and the timestamps available via provider codes are limited and
vary from security to security. (Please don't ask me how you can find
5pm GMT prices for your favourite stock because I don't know and you
should be asking Bloomberg such questions.)

The second, more general solution is to make an intraday bar call:

blpGetData(conn, "EDH8 COMDTY", c("BID","ASK"),
start=chron("2/27/08","14:59:00"), end=chron("2/27/08","15:00:00"),
barfields="LAST_PRICE", barsize=1)
                    BID.LAST_PRICE ASK.LAST_PRICE
(02/27/08 14:59:00)          97.05         97.055


This gives you the market at 3pm (london time on my terminal). This
solution is general in the sense that you can do this for any security
for which bloomberg provides intraday historical pricing. The downside
is that it's cumbersome and Bloomberg will only give you data going back
50 days.

If anyone is aware of another solution, please post it here.

Cheers, 
Robert 




-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
guillaume.nicoulaud at halbis.com
Sent: 27 February 2008 09:19
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] RBloomberg


Dear all,
Using RBloomberg, how do I get historical prices, one per day, but
always at a certain time (17:00h for example)?
Thanks


Les informations contenues dans ce message sont confidentielles et
peuvent constituer des informations privilegiees. Si vous n etes pas le
destinataire de ce message, il vous est interdit de le copier, de le
faire suivre, de le divulguer ou d en utiliser tout ou partie. Si vous
avez recu ce message par erreur, merci de le supprimer de votre systeme,
ainsi que toutes ses copies, et d en avertir immediatement l expediteur
par message de retour.
Il est impossible de garantir que les communications par messagerie
electronique arrivent en temps utile, sont securisees ou denuees de
toute erreur ou virus. En consequence, l expediteur n accepte aucune
responsabilite du fait des erreurs ou omissions qui pourraient en
resulter.
--- ----------------------------------------------------- --- The
information contained in this e-mail is confidential...{{dropped:3}}

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From yuri.volchik at gmail.com  Thu Feb 28 14:14:59 2008
From: yuri.volchik at gmail.com (Yuri Volchik)
Date: Thu, 28 Feb 2008 05:14:59 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Extracting OHLC from trade
	price series
Message-ID: <15718653.post@talk.nabble.com>


Hi to all members,

I presume my question is to Jeff mostly concerning to.minutes:
I have a xts object with tick data:

mm<-xts(dd[,2:4],new.timestamps)

                             BO   Price        Quantity
2007-01-02 08:13:52 O  18.50000000   5     
2007-01-02 08:15:02 B  18.29999924   3     
2007-01-02 08:15:02 B  18.29999924  10     
2007-01-02 08:46:31 B  18.00000000  10     
2007-01-02 09:01:43 B  17.85000038   1     
2007-01-02 09:04:48 B  17.85000038   9     
2007-01-02 09:19:58 B  17.85000038   1     
2007-01-02 09:38:19 B  17.85000038   1     
2007-01-02 09:54:08 B  17.70000076   5     
2007-01-02 10:07:25 O  17.79999924   5 
...


and trying to convert to 5 min data using 
xx<-to.minutes(mm[,2],5,'minutes')

                    minutes.Open minutes.High minutes.Low minutes.Close
2007-01-02 08:13:52  18.50000000  18.50000000 18.50000000   18.50000000
2007-01-02 08:15:02  18.29999924  18.29999924 18.29999924   18.29999924
2007-01-02 08:46:31  18.00000000  18.00000000 18.00000000   18.00000000
2007-01-02 09:01:43  17.85000038  17.85000038 17.85000038   17.85000038
2007-01-02 09:04:48  17.85000038  17.85000038 17.85000038   17.85000038
2007-01-02 09:19:58  17.85000038  17.85000038 17.85000038   17.85000038
2007-01-02 09:38:19  17.85000038  17.85000038 17.85000038   17.85000038
2007-01-02 09:54:08  17.70000076  17.70000076 17.70000076   17.70000076
2007-01-02 10:07:28  17.79999924  17.79999924 17.79999924   17.79999924
2007-01-02 10:17:22  17.79999924  17.79999924 17.79999924   17.79999924
....

The question is this output correct, is there a way to convert tick data to
a somewhat 'nice' representation with equally spaced time intervals and
using specific method of interpolation for missing data (or just leaving
them as NA). I created such code in R, but i think it is quite slow. 

Thanks




Jeff Ryan wrote:
> 
> Hi,
> 
> The package 'xts' (the function in question previously part of
> 'quantmod') has a nice and fast aggregation function that allows you
> to create OHLC from any univariate series, or from an existing OHLC
> series - called 'to.period'.
> 
> library(quantmod)
> getSymbols("QQQQ")
> 
> to.monthly(QQQQ)  # yields a monthly series from daily data
> 
> The code works equally well for anything from minute bars on up.  It
> should work below that, though I can't promise anything as I haven't
> really tested that recently.  Other functions in the group are
> to.minutes, to.hourly, to to.daily... you get the idea.
> 
> to.period is the function you want to look at.  It calls Fortran - so
> it is very fast on all but gigantic data sets - and then nothing is :)
> 
> 
> 

-- 
View this message in context: http://www.nabble.com/Extracting-OHLC-from-trade-price-series-tp15579652p15718653.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From jeff.a.ryan at gmail.com  Thu Feb 28 17:26:14 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 28 Feb 2008 10:26:14 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Extracting OHLC from trade
	price series
In-Reply-To: <15718653.post@talk.nabble.com>
References: <15718653.post@talk.nabble.com>
Message-ID: <e8e755250802280826w385adda5n3d7c74b036b534e3@mail.gmail.com>

Hi Yuri:

It is a bug. Sorry and thanks for pointing it out.  I think somewhere
in the process of moving the code to 'xts' the univariate case was
broken. Looking at the code I see that I dropped the 'k' argument for
the periods in the internal calculation of the endpoints - but only
for the univariate case.

I have updated the source on R-forge if you are interested.  Binaries
should follow soon.

# makes the minute data into minute OHLC data - all the same, which is
what you were getting
# the second to.minutes will then work correctly.

With simulated data:
library(xts)
set.seed(100)
mm <- xts(matrix(rnorm(10,50),10,1),as.POSIXct('2007-01-02 08:30:00')
+ sort(ceiling(runif(10)*1000)))

to.minutes(to.minutes(mm),k=5)

-or-

to.minutes5(to.minutes(mm))

# this makes the minute data into minute OHLC data - all the same,
which is what you were getting
# the second to.minutes will then work correctly.

As far as padding NAs I agree that a prettier output would be nice  -
that is 5 minute bars would look better with time stamps at 5 minute
intervals - even if there was no data for that interval.  For the
moment I am not too sure how to implement that well.  I will give it
some thinking though. To that end what do you think it should look
like?

Jeff



On Thu, Feb 28, 2008 at 7:14 AM, Yuri Volchik <yuri.volchik at gmail.com> wrote:
>
>  Hi to all members,
>
>  I presume my question is to Jeff mostly concerning to.minutes:
>  I have a xts object with tick data:
>
>  mm<-xts(dd[,2:4],new.timestamps)
>
>                              BO   Price        Quantity
>  2007-01-02 08:13:52 O  18.50000000   5
>  2007-01-02 08:15:02 B  18.29999924   3
>  2007-01-02 08:15:02 B  18.29999924  10
>  2007-01-02 08:46:31 B  18.00000000  10
>  2007-01-02 09:01:43 B  17.85000038   1
>  2007-01-02 09:04:48 B  17.85000038   9
>  2007-01-02 09:19:58 B  17.85000038   1
>  2007-01-02 09:38:19 B  17.85000038   1
>  2007-01-02 09:54:08 B  17.70000076   5
>  2007-01-02 10:07:25 O  17.79999924   5
>  ...
>
>
>  and trying to convert to 5 min data using
>  xx<-to.minutes(mm[,2],5,'minutes')
>
>                     minutes.Open minutes.High minutes.Low minutes.Close
>  2007-01-02 08:13:52  18.50000000  18.50000000 18.50000000   18.50000000
>  2007-01-02 08:15:02  18.29999924  18.29999924 18.29999924   18.29999924
>  2007-01-02 08:46:31  18.00000000  18.00000000 18.00000000   18.00000000
>  2007-01-02 09:01:43  17.85000038  17.85000038 17.85000038   17.85000038
>  2007-01-02 09:04:48  17.85000038  17.85000038 17.85000038   17.85000038
>  2007-01-02 09:19:58  17.85000038  17.85000038 17.85000038   17.85000038
>  2007-01-02 09:38:19  17.85000038  17.85000038 17.85000038   17.85000038
>  2007-01-02 09:54:08  17.70000076  17.70000076 17.70000076   17.70000076
>  2007-01-02 10:07:28  17.79999924  17.79999924 17.79999924   17.79999924
>  2007-01-02 10:17:22  17.79999924  17.79999924 17.79999924   17.79999924
>  ....
>
>  The question is this output correct, is there a way to convert tick data to
>  a somewhat 'nice' representation with equally spaced time intervals and
>  using specific method of interpolation for missing data (or just leaving
>  them as NA). I created such code in R, but i think it is quite slow.
>
>  Thanks
>
>
>
>
>  Jeff Ryan wrote:
>  >
>  > Hi,
>  >
>  > The package 'xts' (the function in question previously part of
>  > 'quantmod') has a nice and fast aggregation function that allows you
>  > to create OHLC from any univariate series, or from an existing OHLC
>  > series - called 'to.period'.
>  >
>  > library(quantmod)
>  > getSymbols("QQQQ")
>  >
>  > to.monthly(QQQQ)  # yields a monthly series from daily data
>  >
>  > The code works equally well for anything from minute bars on up.  It
>  > should work below that, though I can't promise anything as I haven't
>  > really tested that recently.  Other functions in the group are
>  > to.minutes, to.hourly, to to.daily... you get the idea.
>  >
>  > to.period is the function you want to look at.  It calls Fortran - so
>  > it is very fast on all but gigantic data sets - and then nothing is :)
>  >
>  >
>  >
>
>  --
>  View this message in context: http://www.nabble.com/Extracting-OHLC-from-trade-price-series-tp15579652p15718653.html
>  Sent from the Rmetrics mailing list archive at Nabble.com.
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From markus at insightfromdata.com  Thu Feb 28 21:11:09 2008
From: markus at insightfromdata.com (Markus Loecher)
Date: Thu, 28 Feb 2008 15:11:09 -0500
Subject: [R-SIG-Finance] aggregate over POSIX timestamps not returning
	unique values
Message-ID: <200802282009.m1SK91tA030247@hypatia.math.ethz.ch>


I am puzzled by the following observation. It seems that aggregating 
over POSIX timestamps sometimes returns duplicate entries in the output.
For example:

tmp <- zoo(cumsum(rnorm(35041)), order.by = 
seq(as.POSIXct("2007-01-01 10:00:00"), to = as.POSIXct("2008-01-01 
10:00:00"), by = 15*60))
  plot(tmp)
  tmp.h <- aggregate(coredata(tmp), by = 
list(as.POSIXct(trunc(index(tmp),"hour"))), FUN = mean)
  plot(zoo(tmp.h[,2], as.POSIXct(as.character(tmp.h[,1]))))

 > Warning message:
 > some methods for "zoo" objects do not work if the index entries 
in >'order.by' are not unique in: zoo(tmp.h[, 2], as.POS...

When I use the more elegant aggregate.zoo() version of this, the 
problem is a non issue:

tmp.h <- aggregate(tmp, by = as.POSIXct(trunc(index(tmp),"hour")), FUN = mean)

I would still l like to understand this, any help would be greatly appreciated,
Thanks,
Markus


From ggrothendieck at gmail.com  Thu Feb 28 21:29:59 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 28 Feb 2008 15:29:59 -0500
Subject: [R-SIG-Finance] aggregate over POSIX timestamps not returning
	unique values
In-Reply-To: <200802282009.m1SK91tA030247@hypatia.math.ethz.ch>
References: <200802282009.m1SK91tA030247@hypatia.math.ethz.ch>
Message-ID: <971536df0802281229g1b9a4555odebc2bed8ad6b2b3@mail.gmail.com>

Its a problem with daylight savings time.  If you use chron
or do this first:

Sys.setenv(TZ = "GMT")

to put your session into GMT time then daylight savings time
won't exist and the problem does not arise.

Suggest you read R News 4/1.

Also please make code with random numbers
reproducible by using set.seed, e.g.
set.seed(1)



On Thu, Feb 28, 2008 at 3:11 PM, Markus Loecher
<markus at insightfromdata.com> wrote:
>
> I am puzzled by the following observation. It seems that aggregating
> over POSIX timestamps sometimes returns duplicate entries in the output.
> For example:
>
> tmp <- zoo(cumsum(rnorm(35041)), order.by =
> seq(as.POSIXct("2007-01-01 10:00:00"), to = as.POSIXct("2008-01-01
> 10:00:00"), by = 15*60))
>  plot(tmp)
>  tmp.h <- aggregate(coredata(tmp), by =
> list(as.POSIXct(trunc(index(tmp),"hour"))), FUN = mean)
>  plot(zoo(tmp.h[,2], as.POSIXct(as.character(tmp.h[,1]))))
>
>  > Warning message:
>  > some methods for "zoo" objects do not work if the index entries
> in >'order.by' are not unique in: zoo(tmp.h[, 2], as.POS...
>
> When I use the more elegant aggregate.zoo() version of this, the
> problem is a non issue:
>
> tmp.h <- aggregate(tmp, by = as.POSIXct(trunc(index(tmp),"hour")), FUN = mean)
>
> I would still l like to understand this, any help would be greatly appreciated,
> Thanks,
> Markus
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From comtech.usa at gmail.com  Thu Feb 28 21:55:35 2008
From: comtech.usa at gmail.com (Michael)
Date: Thu, 28 Feb 2008 12:55:35 -0800
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
Message-ID: <b1f16d9d0802281255n5a5fdd22wd8ddd8045d56fbbe@mail.gmail.com>

Hi all,

We followed some books and sample codes and did some EMM estimation,
only to find it won't be able to generate forecast.

This is because in the stochastic volatility models we are estimating,
the volatilities are latent variables, and we want to forecast 1-step
ahead or h-step ahead volatilities.

So it is nice to have the system estimated, but we couldn't get it to
forecast at all.

There is a "Reprojection" Method described in the original EMM paper,
but let's say we reproject to a GARCH(1,1) model, then only the
GARCH(1, 1) parameters are significant, which basically means we
degrade the SV model into a GARCH model. There is no way to do the
forecast...

Could anybody give some pointers?

Thanks!


From ahala2000 at yahoo.com  Thu Feb 28 22:35:31 2008
From: ahala2000 at yahoo.com (elton wang)
Date: Thu, 28 Feb 2008 13:35:31 -0800 (PST)
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
In-Reply-To: <b1f16d9d0802281255n5a5fdd22wd8ddd8045d56fbbe@mail.gmail.com>
Message-ID: <966371.1737.qm@web31403.mail.mud.yahoo.com>

I've heard opinions that GARCH/SV volatility models
are not better on forecasting than simple exponential
moving average volatilities or even rolling window
historical vol.
Any practitioners mind comment?

--- Michael <comtech.usa at gmail.com> wrote:

> Hi all,
> 
> We followed some books and sample codes and did some
> EMM estimation,
> only to find it won't be able to generate forecast.
> 
> This is because in the stochastic volatility models
> we are estimating,
> the volatilities are latent variables, and we want
> to forecast 1-step
> ahead or h-step ahead volatilities.
> 
> So it is nice to have the system estimated, but we
> couldn't get it to
> forecast at all.
> 
> There is a "Reprojection" Method described in the
> original EMM paper,
> but let's say we reproject to a GARCH(1,1) model,
> then only the
> GARCH(1, 1) parameters are significant, which
> basically means we
> degrade the SV model into a GARCH model. There is no
> way to do the
> forecast...
> 
> Could anybody give some pointers?
> 
> Thanks!
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 



      ____________________________________________________________________________________
Looking for last minute shopping deals?


From markleeds at verizon.net  Fri Feb 29 01:02:42 2008
From: markleeds at verizon.net (Mark Leeds)
Date: Thu, 28 Feb 2008 19:02:42 -0500
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
In-Reply-To: <966371.1737.qm@web31403.mail.mud.yahoo.com>
Message-ID: <000c01c87a66$688ccd10$2f01a8c0@coresystem>

I can't say much about Garch/SV being better or worse but I know that's
there an approximate functional equivalence between exponential smoothing
and a regular moving average ( i.e: rolling window ). It's something like
lambda = 1/(2n +1) or something like that but I don't remember. It's in any
decent technical analysis book and it's true empirically because I've played
around with it in the past.



-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of elton wang
Sent: Thursday, February 28, 2008 4:36 PM
To: r-sig-finance at stat.math.ethz.ch; r-help
Subject: Re: [R-SIG-Finance] EMM: how to make forecast using EMM methods?

I've heard opinions that GARCH/SV volatility models
are not better on forecasting than simple exponential
moving average volatilities or even rolling window
historical vol.
Any practitioners mind comment?

--- Michael <comtech.usa at gmail.com> wrote:

> Hi all,
> 
> We followed some books and sample codes and did some
> EMM estimation,
> only to find it won't be able to generate forecast.
> 
> This is because in the stochastic volatility models
> we are estimating,
> the volatilities are latent variables, and we want
> to forecast 1-step
> ahead or h-step ahead volatilities.
> 
> So it is nice to have the system estimated, but we
> couldn't get it to
> forecast at all.
> 
> There is a "Reprojection" Method described in the
> original EMM paper,
> but let's say we reproject to a GARCH(1,1) model,
> then only the
> GARCH(1, 1) parameters are significant, which
> basically means we
> degrade the SV model into a GARCH model. There is no
> way to do the
> forecast...
> 
> Could anybody give some pointers?
> 
> Thanks!
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 



 
____________________________________________________________________________
________
Looking for last minute shopping deals?

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From guy.yollin at rotellacapital.com  Fri Feb 29 01:30:26 2008
From: guy.yollin at rotellacapital.com (Guy Yollin)
Date: Thu, 28 Feb 2008 19:30:26 -0500
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
In-Reply-To: <b1f16d9d0802281255n5a5fdd22wd8ddd8045d56fbbe@mail.gmail.com>
References: <b1f16d9d0802281255n5a5fdd22wd8ddd8045d56fbbe@mail.gmail.com>
Message-ID: <E634AF2410E42246A35865D8C0C784D98345FD@MI8NYCMAIL09.Mi8.com>

Michael,

If I understand correctly, you've used some EMM algorithms to estimate
the parameters of a stochastic volatility model.

If this is the case you should now be able to use Monte Carlo methods to
generate forecasts from your model.

That is, you will generate random variables (according to the
specifications of your model), feed them into your model and hence
simulate your stochastic volatility process.

Note sure what references you have been using but perhaps these would be
helpful:

Gallant, Hsieh and Tauchen (1997). "Estimation of stochastic volatility
models with diagnostics", Journal of Econometrics, 81, 159-192. 

Andersen, T.G. H.-J. Chung, and B.E. Sorensen (1999). "Efficient Method
of Moments Estimation of a Stochastic Volatility Model: A Monte Carlo
Study," Journal of Econometrics, 91, 61-87.

Best,

-- G



-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Michael
Sent: Thursday, February 28, 2008 12:56 PM
To: r-sig-finance at stat.math.ethz.ch; r-help
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?

Hi all,

We followed some books and sample codes and did some EMM estimation,
only to find it won't be able to generate forecast.

This is because in the stochastic volatility models we are estimating,
the volatilities are latent variables, and we want to forecast 1-step
ahead or h-step ahead volatilities.

So it is nice to have the system estimated, but we couldn't get it to
forecast at all.

There is a "Reprojection" Method described in the original EMM paper,
but let's say we reproject to a GARCH(1,1) model, then only the
GARCH(1, 1) parameters are significant, which basically means we
degrade the SV model into a GARCH model. There is no way to do the
forecast...

Could anybody give some pointers?

Thanks!

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From comtech.usa at gmail.com  Fri Feb 29 02:46:23 2008
From: comtech.usa at gmail.com (Michael)
Date: Thu, 28 Feb 2008 17:46:23 -0800
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
In-Reply-To: <E634AF2410E42246A35865D8C0C784D98345FD@MI8NYCMAIL09.Mi8.com>
References: <b1f16d9d0802281255n5a5fdd22wd8ddd8045d56fbbe@mail.gmail.com>
	<E634AF2410E42246A35865D8C0C784D98345FD@MI8NYCMAIL09.Mi8.com>
Message-ID: <b1f16d9d0802281746r30039324n18fe7704440a3c01@mail.gmail.com>

Hi Guy,

Thanks for your help! Yes, we have the coefficient estimated using
EMM. And we followed those papers.

Just want to check my understanding about your suggestion:

Do you mean that after we obtain the estimated coefficients,

we run one simulation to obtain the whole sequence of latent variable
(the volatility time series, from time 0 to time t+1),

where time t is today, and t+1 is tomorrow(one step forecast);

And that's one simulation.

And we run such simulation for N times, let's say N=10000,

and obtain 10000 such volatility time series, each ending at time t+1,

and then we take average of the 10000 data points at t+1,

the average will be the mean-forecast of the volatility tomorrow(i.e.
that's the one step forecast that we want)...

Am I right in doing these procedures?

Thanks



On Thu, Feb 28, 2008 at 4:30 PM, Guy Yollin
<guy.yollin at rotellacapital.com> wrote:
> Michael,
>
>  If I understand correctly, you've used some EMM algorithms to estimate
>  the parameters of a stochastic volatility model.
>
>  If this is the case you should now be able to use Monte Carlo methods to
>  generate forecasts from your model.
>
>  That is, you will generate random variables (according to the
>  specifications of your model), feed them into your model and hence
>  simulate your stochastic volatility process.
>
>  Note sure what references you have been using but perhaps these would be
>  helpful:
>
>  Gallant, Hsieh and Tauchen (1997). "Estimation of stochastic volatility
>  models with diagnostics", Journal of Econometrics, 81, 159-192.
>
>  Andersen, T.G. H.-J. Chung, and B.E. Sorensen (1999). "Efficient Method
>  of Moments Estimation of a Stochastic Volatility Model: A Monte Carlo
>  Study," Journal of Econometrics, 91, 61-87.
>
>  Best,
>
>  -- G
>
>
>
>
>  -----Original Message-----
>  From: r-sig-finance-bounces at stat.math.ethz.ch
>
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Michael
>  Sent: Thursday, February 28, 2008 12:56 PM
>  To: r-sig-finance at stat.math.ethz.ch; r-help
>
>
> Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
>
>  Hi all,
>
>  We followed some books and sample codes and did some EMM estimation,
>  only to find it won't be able to generate forecast.
>
>  This is because in the stochastic volatility models we are estimating,
>  the volatilities are latent variables, and we want to forecast 1-step
>  ahead or h-step ahead volatilities.
>
>  So it is nice to have the system estimated, but we couldn't get it to
>  forecast at all.
>
>  There is a "Reprojection" Method described in the original EMM paper,
>  but let's say we reproject to a GARCH(1,1) model, then only the
>  GARCH(1, 1) parameters are significant, which basically means we
>  degrade the SV model into a GARCH model. There is no way to do the
>  forecast...
>
>  Could anybody give some pointers?
>
>  Thanks!
>
>
>
> _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>


From chalabi at phys.ethz.ch  Fri Feb 29 10:30:22 2008
From: chalabi at phys.ethz.ch (chalabi at phys.ethz.ch)
Date: Fri, 29 Feb 2008 10:30:22 +0100
Subject: [R-SIG-Finance] fGarch predict
In-Reply-To: <47B732B7.2010605@pdf.com> (Spencer Graves's message of "Sat, 16
	Feb 2008 11:00:07 -0800")
References: <200802161155.10583@centrum.cz> <47B732B7.2010605@pdf.com>
Message-ID: <87tzjsaz4h.fsf@phys.ethz.ch>

Hi Spencer,

I checked this morning your issue with garch(1,0). 

In fGarch package the conditional variance of the GARCH(p,q) is defined as

  \sigma_t^2  =  \omega 
                  + \sum_{i=1}^p \alpha_i \epsilon_{t-i}^2
                  + \sum_{j=1}^q \beta_j \sigma_{t-j}^2

When the order q of this GARCH model is zero, the model is reduced to the
ARCH(p) model. But when the order p is zero, the model is
ill-specified (i.e p > 1).

It is normal that garch(0,1) fails. But there is indeed a problem in the case
garch(1,0). I made the appropriate changes to the dev-version of Rmetrics
to make it work and I added an error message when p is set to zero.

thanks for your feedback,
Yohan




and I made the
appropriate change to the dev-version of Rmetrics. 
But in the case garch(0,1) is normat that it fails.

Spencer Graves <spencer.graves at pdf.com> writes:

> Hi, Yohan: 
>
>      I want to thank you again for working to improve 'garchFit' and
> the documentation. 
>
>      I wonder if you have time to also improve the documentation for
> predict.garchFit', including the following example: 
>
>           x <- garchSim()
>           fit <- garchFit(~arma(1,0)+garch(1,1), data=x)
>           predict(fit)
>
>      Secondarily, I get error messages from garch(1, 0) and garch(0,
> 1): 
>
> fit01 <- garchFit(~garch(0,1), data=x)
> Error in .garchInitParameters(formula.mean = formula.mean, formula.var
> = formula.var,  :
>  object "alpha" not found
>
> fit10 <- garchFit(~garch(1,0), data=x)
> Error in sum(beta) : invalid 'type' (closure) of argument
>
>      Best Wishes,
>      Spencer
>
> babel at centrum.sk wrote:
>> Hello
>> I want to predict the future values of time series with Garch
>> When I specified my model like this:
>> library(fGarch)
>> ret <- diff(log(x))*100
>> fit = garchFit(~arma(1,0,0)+garch(1, 1), data =ret)
>> predict(fit, n.ahead = 10)
>>
>>  meanForecast  meanError standardDeviation
>> 1    0.01371299 0.03086350        0.03305819
>> 2    0.01211893 0.03094519        0.03350248
>> ....................................................................................
>>
>> I know that if I use fit = garchFit(~garch(1, 1), data =ret) I  got constant mean, so trherefore I include amra term to move with mean
>>
>> Iam not sure what values are hiding in this output. 1. Does
>> menForecast hold my future predicted values?
>> 2.Or I am able to just compute the confidence intervals for my prediction like meanForecast +-2*standardDeviation  ??
>> 3Or I need to compute the future values like yt=meanForecast+meanError*sqrt(standardDeviation)  ???
>> My return looks like standard return series with plus and minus
>> values, [748,]  0.008184311  [749,]  0.024548914  [750,]
>> -0.008182302
>>
>> so I hope I would get similar prediction to this return, not just a postive mean constant.Sorry,  I know that Garch models are for volatility modelling, but I still doesnt find how to use that volatility for forecasting future values. Short example with 5 step ahead prediction will surely help.
>> Thank you
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only. -- If you want to post, subscribe first.
>>   

-- 
Yohan Chalabi


From patrick at burns-stat.com  Fri Feb 29 10:56:46 2008
From: patrick at burns-stat.com (Patrick Burns)
Date: Fri, 29 Feb 2008 09:56:46 +0000
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
In-Reply-To: <966371.1737.qm@web31403.mail.mud.yahoo.com>
References: <966371.1737.qm@web31403.mail.mud.yahoo.com>
Message-ID: <47C7D6DE.5080604@burns-stat.com>

(This is not being sent to R-help.  It is considered
impolite to cross-post messages, especially on topics
that are purely financial.)

As always, the answer is, "That depends."

The key question is the time frame of the
prediction.  If the prediction is for a month or
more, then there's unlikely to be much advantage
in a fancy model.  If the time frame is a few
days, then something like a garch model will
vastly outperform a rolling window.  How much
a garch model would outperform an exponential
smooth depends on the smoothing parameter (an
exponential smooth is a degenerate form of a
garch model).  As far as I know, there is not a
clear winner between garch models and stochastic
volatility models, but with some evidence that garch
might be better.  Corrections to this impression are
certainly welcome.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

elton wang wrote:

>I've heard opinions that GARCH/SV volatility models
>are not better on forecasting than simple exponential
>moving average volatilities or even rolling window
>historical vol.
>Any practitioners mind comment?
>
>--- Michael <comtech.usa at gmail.com> wrote:
>
>  
>
>>Hi all,
>>
>>We followed some books and sample codes and did some
>>EMM estimation,
>>only to find it won't be able to generate forecast.
>>
>>This is because in the stochastic volatility models
>>we are estimating,
>>the volatilities are latent variables, and we want
>>to forecast 1-step
>>ahead or h-step ahead volatilities.
>>
>>So it is nice to have the system estimated, but we
>>couldn't get it to
>>forecast at all.
>>
>>There is a "Reprojection" Method described in the
>>original EMM paper,
>>but let's say we reproject to a GARCH(1,1) model,
>>then only the
>>GARCH(1, 1) parameters are significant, which
>>basically means we
>>degrade the SV model into a GARCH model. There is no
>>way to do the
>>forecast...
>>
>>Could anybody give some pointers?
>>
>>Thanks!
>>
>>_______________________________________________
>>R-SIG-Finance at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>>-- Subscriber-posting only. 
>>-- If you want to post, subscribe first.
>>
>>    
>>
>
>
>
>      ____________________________________________________________________________________
>Looking for last minute shopping deals?
>
>_______________________________________________
>R-SIG-Finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>-- Subscriber-posting only. 
>-- If you want to post, subscribe first.
>
>
>  
>


From nicolas.mougeot at db.com  Fri Feb 29 11:12:27 2008
From: nicolas.mougeot at db.com (Nicolas Mougeot)
Date: Fri, 29 Feb 2008 10:12:27 +0000
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
In-Reply-To: <47C7D6DE.5080604@burns-stat.com>
Message-ID: <OFB17582E4.4352994A-ON802573FE.0037C934-802573FE.0038124B@db.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080229/a301707d/attachment.pl 

From yuri.volchik at gmail.com  Fri Feb 29 11:27:26 2008
From: yuri.volchik at gmail.com (Yuri Volchik)
Date: Fri, 29 Feb 2008 02:27:26 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] Extracting OHLC from trade
	price series
In-Reply-To: <e8e755250802280826w385adda5n3d7c74b036b534e3@mail.gmail.com>
References: <e385f1f9245b9.47bb72ca@optonline.net>
	<e8e755250802191639p36e42eb3jfc40afcdd2eb1cdf@mail.gmail.com>
	<15718653.post@talk.nabble.com>
	<e8e755250802280826w385adda5n3d7c74b036b534e3@mail.gmail.com>
Message-ID: <15755859.post@talk.nabble.com>


Hi Jeff,

suppose tick data looks like this:
01/06/2007,00:35:33,2795.,9,
01/06/2007,01:13:14,2799.,1,
01/06/2007,01:13:49,2799.75,5,
01/06/2007,01:19:24,2799.75,4,
01/06/2007,01:20:00,2800.,6,
01/06/2007,01:20:00,2800.,27,
01/06/2007,01:20:20,2800.,1,
1/06/2007,01:20:33,2800.,1,

so formatted data looks:
2007/06/01 00:55:00,#NA,#NA,#NA,#NA,#NA,#NA
2007/06/01 01:00:00,#NA,#NA,#NA,#NA,#NA,#NA
2007/06/01 01:05:00,#NA,#NA,#NA,#NA,#NA,#NA
2007/06/01 01:10:00,#NA,#NA,#NA,#NA,#NA,#NA
2007/06/01 01:15:00,2799,2799.75,6,2,194,71
2007/06/01 01:20:00,2799.75,2799.75,4,1,264,36
2007/06/01 01:25:00,2800,2800,10,4,0,239
2007/06/01 01:30:00,#NA,#NA,#NA,#NA,#NA,#NA
2007/06/01 01:35:00,#NA,#NA,#NA,#NA,#NA,#NA
2007/06/01 01:40:00,#NA,#NA,#NA,#NA,#NA,#NA
2007/06/01 01:45:00,#NA,#NA,#NA,#NA,#NA,#NA
2007/06/01 01:50:00,2794,2794,1,1,10,290
2007/06/01 01:55:00,2799,2800,2,2,116,174

I calculate some other stuff on ticks, not just OHLC.

This is my R routine, very slow and inefficient i'm sure:

number.of.intervals<-24*60/TimeFrame   #Number of time intervals in one day
seconds.in.time.interval=TimeFrame*60
times<-seq(ISOdatetime(2000,1,1,0,TimeFrame,0),by=seconds.in.time.interval,length=number.of.intervals) 
#names of time intervals
times<-format(times,"%H:%M")
d.start<-as.POSIXlt(as.Date(time(dat.)[1]))+TimeFrame*60
d.end<-as.POSIXlt(as.Date(time(dat.)[dim(dat.)[1]]))+3600*24
Index<-seq(from=d.start,to=d.end,by=TimeFrame*60)
dd<-matrix(NA,nrow=length(Index),ncol=1)
dd[Index%in%time(dat.),1]<-dat.[time(dat.)%in%Index,Col]
if (na.max>0)
dd[,1]<-remove.na(dd,max.na=na.max,interp.type=interp.type,column=1)
n.days<-length(Index)/number.of.intervals
dd<-t(dd)
dim(dd)<-c(number.of.intervals,n.days)
ddd<-zoo(t(dd),order.by=seq(from=as.Date(d.start),to=as.Date(d.end-3600*24),by='day'))
colnames(ddd)<-times
days<-unique(as.Date(time(dat.)))
ddd<-ddd[time(ddd)%in%days,]


I'm sure it can be improved. I think the design should incorporate ability
to specify custom column with price data and different aggregation
procedures to be performed on data. Would be willing to help you to develop
such functions.

Best regards,
Yuri



-- 
View this message in context: http://www.nabble.com/Extracting-OHLC-from-trade-price-series-tp15579652p15755859.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From guy.yollin at rotellacapital.com  Fri Feb 29 16:34:46 2008
From: guy.yollin at rotellacapital.com (Guy Yollin)
Date: Fri, 29 Feb 2008 10:34:46 -0500
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
In-Reply-To: <b1f16d9d0802281746r30039324n18fe7704440a3c01@mail.gmail.com>
References: <b1f16d9d0802281255n5a5fdd22wd8ddd8045d56fbbe@mail.gmail.com>
	<E634AF2410E42246A35865D8C0C784D98345FD@MI8NYCMAIL09.Mi8.com>
	<b1f16d9d0802281746r30039324n18fe7704440a3c01@mail.gmail.com>
Message-ID: <E634AF2410E42246A35865D8C0C784D98CDD55@MI8NYCMAIL09.Mi8.com>

Hi Michael,

Yes, this is what I'm suggesting.  Bear in mind, your model estimation
process should have also resulted in volatility estimates for t-1, t-2,
etc.

Your simulation will require one or more of these terms as input (in
addition to the random innovations) since your stochastic volatility
model will have lagged volatility terms.

Good luck.

-- G


-----Original Message-----
From: Michael [mailto:comtech.usa at gmail.com] 
Sent: Thursday, February 28, 2008 5:46 PM
To: Guy Yollin; r-help; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] EMM: how to make forecast using EMM
methods?

Hi Guy,

Thanks for your help! Yes, we have the coefficient estimated using
EMM. And we followed those papers.

Just want to check my understanding about your suggestion:

Do you mean that after we obtain the estimated coefficients,

we run one simulation to obtain the whole sequence of latent variable
(the volatility time series, from time 0 to time t+1),

where time t is today, and t+1 is tomorrow(one step forecast);

And that's one simulation.

And we run such simulation for N times, let's say N=10000,

and obtain 10000 such volatility time series, each ending at time t+1,

and then we take average of the 10000 data points at t+1,

the average will be the mean-forecast of the volatility tomorrow(i.e.
that's the one step forecast that we want)...

Am I right in doing these procedures?

Thanks



On Thu, Feb 28, 2008 at 4:30 PM, Guy Yollin
<guy.yollin at rotellacapital.com> wrote:
> Michael,
>
>  If I understand correctly, you've used some EMM algorithms to
estimate
>  the parameters of a stochastic volatility model.
>
>  If this is the case you should now be able to use Monte Carlo methods
to
>  generate forecasts from your model.
>
>  That is, you will generate random variables (according to the
>  specifications of your model), feed them into your model and hence
>  simulate your stochastic volatility process.
>
>  Note sure what references you have been using but perhaps these would
be
>  helpful:
>
>  Gallant, Hsieh and Tauchen (1997). "Estimation of stochastic
volatility
>  models with diagnostics", Journal of Econometrics, 81, 159-192.
>
>  Andersen, T.G. H.-J. Chung, and B.E. Sorensen (1999). "Efficient
Method
>  of Moments Estimation of a Stochastic Volatility Model: A Monte Carlo
>  Study," Journal of Econometrics, 91, 61-87.
>
>  Best,
>
>  -- G
>
>
>
>
>  -----Original Message-----
>  From: r-sig-finance-bounces at stat.math.ethz.ch
>
> [mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Michael
>  Sent: Thursday, February 28, 2008 12:56 PM
>  To: r-sig-finance at stat.math.ethz.ch; r-help
>
>
> Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
>
>  Hi all,
>
>  We followed some books and sample codes and did some EMM estimation,
>  only to find it won't be able to generate forecast.
>
>  This is because in the stochastic volatility models we are
estimating,
>  the volatilities are latent variables, and we want to forecast 1-step
>  ahead or h-step ahead volatilities.
>
>  So it is nice to have the system estimated, but we couldn't get it to
>  forecast at all.
>
>  There is a "Reprojection" Method described in the original EMM paper,
>  but let's say we reproject to a GARCH(1,1) model, then only the
>  GARCH(1, 1) parameters are significant, which basically means we
>  degrade the SV model into a GARCH model. There is no way to do the
>  forecast...
>
>  Could anybody give some pointers?
>
>  Thanks!
>
>
>
> _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>


From ahala2000 at yahoo.com  Fri Feb 29 16:37:27 2008
From: ahala2000 at yahoo.com (elton wang)
Date: Fri, 29 Feb 2008 07:37:27 -0800 (PST)
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
In-Reply-To: <E634AF2410E42246A35865D8C0C784D98CDD55@MI8NYCMAIL09.Mi8.com>
Message-ID: <189598.31486.qm@web31407.mail.mud.yahoo.com>

But I doubt this is not a one-step forecast.
For one-step cast, you only need start from today's
value and simulate one step ahead. you need to use the
orignal innovations as of today instead of simulating
from day 1.

--- Guy Yollin <guy.yollin at rotellacapital.com> wrote:

> Hi Michael,
> 
> Yes, this is what I'm suggesting.  Bear in mind,
> your model estimation
> process should have also resulted in volatility
> estimates for t-1, t-2,
> etc.
> 
> Your simulation will require one or more of these
> terms as input (in
> addition to the random innovations) since your
> stochastic volatility
> model will have lagged volatility terms.
> 
> Good luck.
> 
> -- G
> 
> 
> -----Original Message-----
> From: Michael [mailto:comtech.usa at gmail.com] 
> Sent: Thursday, February 28, 2008 5:46 PM
> To: Guy Yollin; r-help;
> r-sig-finance at stat.math.ethz.ch
> Subject: Re: [R-SIG-Finance] EMM: how to make
> forecast using EMM
> methods?
> 
> Hi Guy,
> 
> Thanks for your help! Yes, we have the coefficient
> estimated using
> EMM. And we followed those papers.
> 
> Just want to check my understanding about your
> suggestion:
> 
> Do you mean that after we obtain the estimated
> coefficients,
> 
> we run one simulation to obtain the whole sequence
> of latent variable
> (the volatility time series, from time 0 to time
> t+1),
> 
> where time t is today, and t+1 is tomorrow(one step
> forecast);
> 
> And that's one simulation.
> 
> And we run such simulation for N times, let's say
> N=10000,
> 
> and obtain 10000 such volatility time series, each
> ending at time t+1,
> 
> and then we take average of the 10000 data points at
> t+1,
> 
> the average will be the mean-forecast of the
> volatility tomorrow(i.e.
> that's the one step forecast that we want)...
> 
> Am I right in doing these procedures?
> 
> Thanks
> 
> 
> 
> On Thu, Feb 28, 2008 at 4:30 PM, Guy Yollin
> <guy.yollin at rotellacapital.com> wrote:
> > Michael,
> >
> >  If I understand correctly, you've used some EMM
> algorithms to
> estimate
> >  the parameters of a stochastic volatility model.
> >
> >  If this is the case you should now be able to use
> Monte Carlo methods
> to
> >  generate forecasts from your model.
> >
> >  That is, you will generate random variables
> (according to the
> >  specifications of your model), feed them into
> your model and hence
> >  simulate your stochastic volatility process.
> >
> >  Note sure what references you have been using but
> perhaps these would
> be
> >  helpful:
> >
> >  Gallant, Hsieh and Tauchen (1997). "Estimation of
> stochastic
> volatility
> >  models with diagnostics", Journal of
> Econometrics, 81, 159-192.
> >
> >  Andersen, T.G. H.-J. Chung, and B.E. Sorensen
> (1999). "Efficient
> Method
> >  of Moments Estimation of a Stochastic Volatility
> Model: A Monte Carlo
> >  Study," Journal of Econometrics, 91, 61-87.
> >
> >  Best,
> >
> >  -- G
> >
> >
> >
> >
> >  -----Original Message-----
> >  From: r-sig-finance-bounces at stat.math.ethz.ch
> >
> > [mailto:r-sig-finance-bounces at stat.math.ethz.ch]
> On Behalf Of Michael
> >  Sent: Thursday, February 28, 2008 12:56 PM
> >  To: r-sig-finance at stat.math.ethz.ch; r-help
> >
> >
> > Subject: [R-SIG-Finance] EMM: how to make forecast
> using EMM methods?
> >
> >  Hi all,
> >
> >  We followed some books and sample codes and did
> some EMM estimation,
> >  only to find it won't be able to generate
> forecast.
> >
> >  This is because in the stochastic volatility
> models we are
> estimating,
> >  the volatilities are latent variables, and we
> want to forecast 1-step
> >  ahead or h-step ahead volatilities.
> >
> >  So it is nice to have the system estimated, but
> we couldn't get it to
> >  forecast at all.
> >
> >  There is a "Reprojection" Method described in the
> original EMM paper,
> >  but let's say we reproject to a GARCH(1,1) model,
> then only the
> >  GARCH(1, 1) parameters are significant, which
> basically means we
> >  degrade the SV model into a GARCH model. There is
> no way to do the
> >  forecast...
> >
> >  Could anybody give some pointers?
> >
> >  Thanks!
> >
> >
> >
> > _______________________________________________
> >  R-SIG-Finance at stat.math.ethz.ch mailing list
> > 
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >  -- Subscriber-posting only.
> >  -- If you want to post, subscribe first.
> >
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 



      ____________________________________________________________________________________
Be a better friend, newshound, and


From comtech.usa at gmail.com  Fri Feb 29 19:54:51 2008
From: comtech.usa at gmail.com (Michael)
Date: Fri, 29 Feb 2008 10:54:51 -0800
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
In-Reply-To: <189598.31486.qm@web31407.mail.mud.yahoo.com>
References: <E634AF2410E42246A35865D8C0C784D98CDD55@MI8NYCMAIL09.Mi8.com>
	<189598.31486.qm@web31407.mail.mud.yahoo.com>
Message-ID: <b1f16d9d0802291054s4cc333e7sfe09189cc87e4359@mail.gmail.com>

Hi Guy and Elton,

Thanks for the replies.

However this is exactly the weird thing about EMM. It simulates the
latent variables when doing the estimating itself. So there is no
clear estimates of the latent variables itself. That's to say, I don't
have a "today's value" to work out the 1-step ahead forecast...

That's kind of strange...

Any thoughts?

On Fri, Feb 29, 2008 at 7:37 AM, elton wang <ahala2000 at yahoo.com> wrote:
> But I doubt this is not a one-step forecast.
>  For one-step cast, you only need start from today's
>  value and simulate one step ahead. you need to use the
>  orignal innovations as of today instead of simulating
>  from day 1.
>
>
>
>  --- Guy Yollin <guy.yollin at rotellacapital.com> wrote:
>
>  > Hi Michael,
>  >
>  > Yes, this is what I'm suggesting.  Bear in mind,
>  > your model estimation
>  > process should have also resulted in volatility
>  > estimates for t-1, t-2,
>  > etc.
>  >
>  > Your simulation will require one or more of these
>  > terms as input (in
>  > addition to the random innovations) since your
>  > stochastic volatility
>  > model will have lagged volatility terms.
>  >
>  > Good luck.
>  >
>  > -- G
>  >
>  >
>  > -----Original Message-----
>  > From: Michael [mailto:comtech.usa at gmail.com]
>  > Sent: Thursday, February 28, 2008 5:46 PM
>  > To: Guy Yollin; r-help;
>  > r-sig-finance at stat.math.ethz.ch
>  > Subject: Re: [R-SIG-Finance] EMM: how to make
>  > forecast using EMM
>  > methods?
>  >
>  > Hi Guy,
>  >
>  > Thanks for your help! Yes, we have the coefficient
>  > estimated using
>  > EMM. And we followed those papers.
>  >
>  > Just want to check my understanding about your
>  > suggestion:
>  >
>  > Do you mean that after we obtain the estimated
>  > coefficients,
>  >
>  > we run one simulation to obtain the whole sequence
>  > of latent variable
>  > (the volatility time series, from time 0 to time
>  > t+1),
>  >
>  > where time t is today, and t+1 is tomorrow(one step
>  > forecast);
>  >
>  > And that's one simulation.
>  >
>  > And we run such simulation for N times, let's say
>  > N=10000,
>  >
>  > and obtain 10000 such volatility time series, each
>  > ending at time t+1,
>  >
>  > and then we take average of the 10000 data points at
>  > t+1,
>  >
>  > the average will be the mean-forecast of the
>  > volatility tomorrow(i.e.
>  > that's the one step forecast that we want)...
>  >
>  > Am I right in doing these procedures?
>  >
>  > Thanks
>  >
>  >
>  >
>  > On Thu, Feb 28, 2008 at 4:30 PM, Guy Yollin
>  > <guy.yollin at rotellacapital.com> wrote:
>  > > Michael,
>  > >
>  > >  If I understand correctly, you've used some EMM
>  > algorithms to
>  > estimate
>  > >  the parameters of a stochastic volatility model.
>  > >
>  > >  If this is the case you should now be able to use
>  > Monte Carlo methods
>  > to
>  > >  generate forecasts from your model.
>  > >
>  > >  That is, you will generate random variables
>  > (according to the
>  > >  specifications of your model), feed them into
>  > your model and hence
>  > >  simulate your stochastic volatility process.
>  > >
>  > >  Note sure what references you have been using but
>  > perhaps these would
>  > be
>  > >  helpful:
>  > >
>  > >  Gallant, Hsieh and Tauchen (1997). "Estimation of
>  > stochastic
>  > volatility
>  > >  models with diagnostics", Journal of
>  > Econometrics, 81, 159-192.
>  > >
>  > >  Andersen, T.G. H.-J. Chung, and B.E. Sorensen
>  > (1999). "Efficient
>  > Method
>  > >  of Moments Estimation of a Stochastic Volatility
>  > Model: A Monte Carlo
>  > >  Study," Journal of Econometrics, 91, 61-87.
>  > >
>  > >  Best,
>  > >
>  > >  -- G
>  > >
>  > >
>  > >
>  > >
>  > >  -----Original Message-----
>  > >  From: r-sig-finance-bounces at stat.math.ethz.ch
>  > >
>  > > [mailto:r-sig-finance-bounces at stat.math.ethz.ch]
>  > On Behalf Of Michael
>  > >  Sent: Thursday, February 28, 2008 12:56 PM
>  > >  To: r-sig-finance at stat.math.ethz.ch; r-help
>  > >
>  > >
>  > > Subject: [R-SIG-Finance] EMM: how to make forecast
>  > using EMM methods?
>  > >
>  > >  Hi all,
>  > >
>  > >  We followed some books and sample codes and did
>  > some EMM estimation,
>  > >  only to find it won't be able to generate
>  > forecast.
>  > >
>  > >  This is because in the stochastic volatility
>  > models we are
>  > estimating,
>  > >  the volatilities are latent variables, and we
>  > want to forecast 1-step
>  > >  ahead or h-step ahead volatilities.
>  > >
>  > >  So it is nice to have the system estimated, but
>  > we couldn't get it to
>  > >  forecast at all.
>  > >
>  > >  There is a "Reprojection" Method described in the
>  > original EMM paper,
>  > >  but let's say we reproject to a GARCH(1,1) model,
>  > then only the
>  > >  GARCH(1, 1) parameters are significant, which
>  > basically means we
>  > >  degrade the SV model into a GARCH model. There is
>  > no way to do the
>  > >  forecast...
>  > >
>  > >  Could anybody give some pointers?
>  > >
>  > >  Thanks!
>  > >
>  > >
>  > >
>  > > _______________________________________________
>  > >  R-SIG-Finance at stat.math.ethz.ch mailing list
>  > >
>  > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  > >  -- Subscriber-posting only.
>  > >  -- If you want to post, subscribe first.
>  > >
>  >
>  > _______________________________________________
>  > R-SIG-Finance at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  > -- Subscriber-posting only.
>  > -- If you want to post, subscribe first.
>  >
>
>
>
>       ____________________________________________________________________________________
>  Be a better friend, newshound, and
>
>
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>


From Robert.McGehee at geodecapital.com  Fri Feb 29 22:19:42 2008
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Fri, 29 Feb 2008 16:19:42 -0500
Subject: [R-SIG-Finance] Risk Neutral Probability Distribution
Message-ID: <EEBC169715EB8C438D3C9283AF0F201C17C995@MSGBOSCLM2WIN.DMN1.FMR.COM>

R-Financiers,
Are there R functions available for extracting the risk-neutral
probability distribution from option prices?

A paper reviewing the many techniques available is available below:
http://www.uni-konstanz.de/FuF/wiwi/jackwerth/jackwerth/paper14.pdf

Cheers, Robert

Robert McGehee, CFA
Geode Capital Management, LLC
One Post Office Square, 28th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended fo...{{dropped:11}}


From ezivot at u.washington.edu  Fri Feb 29 23:54:21 2008
From: ezivot at u.washington.edu (Eric Zivot)
Date: Fri, 29 Feb 2008 14:54:21 -0800
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?
In-Reply-To: <E634AF2410E42246A35865D8C0C784D98345FD@MI8NYCMAIL09.Mi8.com>
Message-ID: <200802292254.m1TMsLSx011024@smtp.washington.edu>

For simple SV models (e.g. log normal ar(1)), the model can be written in
state space form and the the Kalman filter may be used to forecast the
latent volatility. See Harvey, Ruiz and Shephard's paper in ReStud for
details. However, the Kalman filter is only the best linear forecast. In
general, the SV models are non-linear and non-gaussian state space models
and the optimal forecasting algorithms are given by the particle filter. I
have a short paper that describes how to do this on my webpage

 http://faculty.washington.edu/ezivot/research/Creal_Gu_Zivot_2007.pdf

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Guy Yollin
Sent: Thursday, February 28, 2008 4:30 PM
To: Michael; r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] EMM: how to make forecast using EMM methods?

Michael,

If I understand correctly, you've used some EMM algorithms to estimate the
parameters of a stochastic volatility model.

If this is the case you should now be able to use Monte Carlo methods to
generate forecasts from your model.

That is, you will generate random variables (according to the specifications
of your model), feed them into your model and hence simulate your stochastic
volatility process.

Note sure what references you have been using but perhaps these would be
helpful:

Gallant, Hsieh and Tauchen (1997). "Estimation of stochastic volatility
models with diagnostics", Journal of Econometrics, 81, 159-192. 

Andersen, T.G. H.-J. Chung, and B.E. Sorensen (1999). "Efficient Method of
Moments Estimation of a Stochastic Volatility Model: A Monte Carlo Study,"
Journal of Econometrics, 91, 61-87.

Best,

-- G



-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of Michael
Sent: Thursday, February 28, 2008 12:56 PM
To: r-sig-finance at stat.math.ethz.ch; r-help
Subject: [R-SIG-Finance] EMM: how to make forecast using EMM methods?

Hi all,

We followed some books and sample codes and did some EMM estimation, only to
find it won't be able to generate forecast.

This is because in the stochastic volatility models we are estimating, the
volatilities are latent variables, and we want to forecast 1-step ahead or
h-step ahead volatilities.

So it is nice to have the system estimated, but we couldn't get it to
forecast at all.

There is a "Reprojection" Method described in the original EMM paper, but
let's say we reproject to a GARCH(1,1) model, then only the GARCH(1, 1)
parameters are significant, which basically means we degrade the SV model
into a GARCH model. There is no way to do the forecast...

Could anybody give some pointers?

Thanks!

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From yuri.volchik at gmail.com  Sun Mar  2 12:37:32 2008
From: yuri.volchik at gmail.com (Yuri Volchik)
Date: Sun, 2 Mar 2008 03:37:32 -0800 (PST)
Subject: [R-SIG-Finance] [R-sig-finance] http://www.market-topology.com/
Message-ID: <15786834.post@talk.nabble.com>


Hi,

was wondering what algorithm they are using and if it is available in R.
Using my own judgment it looks like minimum spanning tree algorithm and in R
it's available in the packages igraph and vegan, is it correct?

Thanks 
-- 
View this message in context: http://www.nabble.com/http%3A--www.market-topology.com--tp15786834p15786834.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From stigler3 at etu.unige.ch  Sun Mar  2 17:22:52 2008
From: stigler3 at etu.unige.ch (Matthieu Stigler)
Date: Sun, 02 Mar 2008 17:22:52 +0100
Subject: [R-SIG-Finance] Frage uber package strucchange
Message-ID: <47CAD45C.9060306@etu.unige.ch>

Hello

I'm using the nice package strucchange to see whether there is a 
structural change in a regression, and especially, if the change is in 
the slope or intercept coefficient.

The "Vignette" with the package (see fig 4 page 8) gives an example to 
obtain a plot of the empirical process for each coefficient. However, I 
don't obtain it, even with the code given in example...

Do you have also only one plot? Do you see what I do wrong, and how I 
could obtain the same plot as in example?

Thanks  a lot!!

The code in example is:

library("strucchange")
data("USIncExp")
USIncExp2 <- window(USIncExp, start = c(1985, 12))

coint.res <- residuals(lm(expenditure ~ income, data = USIncExp2))
coint.res <- lag(ts(coint.res, start = c(1985, 12), freq = 12),
    k = -1)
USIncExp2 <- cbind(USIncExp2, diff(USIncExp2), coint.res)
USIncExp2 <- window(USIncExp2, start = c(1986, 1), end = c(2001,
    2))
colnames(USIncExp2) <- c("income", "expenditure", "diff.income",
    "diff.expenditure", "coint.res")
ecm.model <- diff.expenditure ~ coint.res + diff.income


me <- efp(ecm.model, type = "ME", data = USIncExp2, h = 0.2)

plot(me, functional = NULL)


From spencer.graves at pdf.com  Sun Mar  2 18:49:55 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 02 Mar 2008 09:49:55 -0800
Subject: [R-SIG-Finance] [R-sig-finance] http://www.market-topology.com/
In-Reply-To: <15786834.post@talk.nabble.com>
References: <15786834.post@talk.nabble.com>
Message-ID: <47CAE8C3.90306@pdf.com>


      The text on 'www.market-topology.com' includes the following:  
"The engine developed by Market Topology SPRL emphasizes the relevant 
correlations which connect the equities. ... In addition, a procedure 
selects the highest coefficients for placing them in a connected graph 
which is a tree." 

      This suggests they are doing some sort of cluster analysis.  
RSiteSearch('cluster analysis', 'fun') just produced 314 hits for me.  I 
rarely use cluster analysis, but I would guess that the most popular 
methods would include hclust{stats}, agnes{cluster}, and Mclust{mclust}. 

      Note, however, that many cluster analysis methods are similar to 
reading tea leaves (http://en.wikipedia.org/wiki/Tasseography) in the 
sense that they will find relationships, independent of whether there 
are relationships to find.  Mclust is model based, which suggests to me 
that it may be less subject to "false positives" than other methods.  
However, you should not take my word for this;  perhaps someone with 
more experience with these methods will enlighten us. 

      A major problem is that you are looking for relationships among 
5,000 or so financial time series, possibly with fewer than 5,000 
observations.  To obtain a full rank estimate of a correlation matrix of 
that size, you need at least that many observations -- and much of the 
subtle structure on which this analysis depends would typically be 
poorly estimated.  Moreover, it would take roughly 20 years of daily log 
returns, for example, just to get a full rank estimate.  Just computing 
that matrix assumes that the relationships are all constant over that 
period of time. 

      It may still be useful, but one would need to understand its 
limitations. 

      Hope this helps. 
      Spencer

Yuri Volchik wrote:
> Hi,
>
> was wondering what algorithm they are using and if it is available in R.
> Using my own judgment it looks like minimum spanning tree algorithm and in R
> it's available in the packages igraph and vegan, is it correct?
>
> Thanks 
>


From babel at centrum.sk  Sun Mar  2 21:24:41 2008
From: babel at centrum.sk (babel at centrum.sk)
Date: Sun, 02 Mar 2008 21:24:41 +0100
Subject: [R-SIG-Finance] another garch question
Message-ID: <200803022124.9615@centrum.cz>

Hello scientists
How can I quantify the garch model? The equations are for example from book Analysis of financial time series by Tsay
rt = 0.0067 + at,     at = ?t * et 
 ?2t = 0.000119 + 0.8059? 2t?1 + 0.1941a2t?1

fit = garchFit(~garch(1, 1), data =ret)

What slot holds the et variable in R? (fit at residuals ??? I doubt) In every book I read there was only coefficients estimated, but no graphical output of modelled serie produced. So how can I said that for example model garch(1,1) is better than arch(2)? ArchTest,LM test and BDS test of the standardized residuals are the only options? or AIC, SIC?

I tried 2 models
fit1 = garchFit(~arma(1, 0)+garch(1,1), data =ret)
fit2 = garchFit(~arma(1, 0)+garch(2,1), data =ret)
and the fit1, fit2 at fitted were slightly different, so the volatility (garch term) must influence somehow to mean estimation. 
Why so many books ended with coefficients estimation?And what tells me the predicted standard deviation about the future behaviour of return series? Are there good only for making confidence intarvals +-2*sd or to print simga.t and said so, this is my volatility?
So how correctly use garch in modelling time series and comparing it with different methods if garch is for volatility and not for mean?

Sorry for slow thinking :)))


From spencer.graves at pdf.com  Mon Mar  3 01:09:56 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 02 Mar 2008 16:09:56 -0800
Subject: [R-SIG-Finance] another garch question
In-Reply-To: <200803022124.9615@centrum.cz>
References: <200803022124.9615@centrum.cz>
Message-ID: <47CB41D4.4070609@pdf.com>

      1.  Have you studied the 'garchFit' help page, especially the 
"Value" section? 

      2.  Are you familiar with the 'str' function? 

      3.  Between "1" and "2" and playing with simple examples, I'm 
usually able to get answers to most of my questions for things like 
this.  When that fails, I sometimes try simulated examples, e.g., 
generated via 'garchSim'.  Sometimes I work examples -- or parts of 
examples in something like MS Excel. 

      4.  The ultimately documentation is the source code.  You can get 
source code with comments from 'www.r-project.org' -> CRAN -> (select a 
mirror) -> Packages -> fGarch -> "Downloads:  Package source:  
fGarch_260.72.tar.gz".  Saying 'debug(garchFit)', then executing a call 
to garchFit will allow you to walk through the code line by line. 

      You might like to have documentation that is more user friendly, 
but you will NOWHERE find documentation that is as complete as the 
source code -- nor as easy to modify for slightly different purposes. 

      This may not answer your question, but I've given you here at 
least 4 different fish hooks.  You supply the bait, and you should be 
able to catch your fish. 

      Hope this helps. 
      Spencer

babel at centrum.sk wrote:
> Hello scientists
> How can I quantify the garch model? The equations are for example from book Analysis of financial time series by Tsay
> rt = 0.0067 + at,     at = ?t * et 
>  ?2t = 0.000119 + 0.8059? 2t?1 + 0.1941a2t?1
>
> fit = garchFit(~garch(1, 1), data =ret)
>
> What slot holds the et variable in R? (fit at residuals ??? I doubt) In every book I read there was only coefficients estimated, but no graphical output of modelled serie produced. So how can I said that for example model garch(1,1) is better than arch(2)? ArchTest,LM test and BDS test of the standardized residuals are the only options? or AIC, SIC?
>
> I tried 2 models
> fit1 = garchFit(~arma(1, 0)+garch(1,1), data =ret)
> fit2 = garchFit(~arma(1, 0)+garch(2,1), data =ret)
> and the fit1, fit2 at fitted were slightly different, so the volatility (garch term) must influence somehow to mean estimation. 
> Why so many books ended with coefficients estimation?And what tells me the predicted standard deviation about the future behaviour of return series? Are there good only for making confidence intarvals +-2*sd or to print simga.t and said so, this is my volatility?
> So how correctly use garch in modelling time series and comparing it with different methods if garch is for volatility and not for mean?
>
> Sorry for slow thinking :)))
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.


From ecjbosu at aol.com  Mon Mar  3 05:12:58 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Sun, 02 Mar 2008 22:12:58 -0600
Subject: [R-SIG-Finance] holidayNYSE missing some
In-Reply-To: <20080214161431.5570342f@yankee-laptop>
References: <20080204115153.13a247f4@yankee-laptop>
	<loom.20080212T162838-798@post.gmane.org>
	<20080214161431.5570342f@yankee-laptop>
Message-ID: <47CB7ACA.4040209@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080302/954f0d87/attachment.pl 

From iacopetti at fastpiu.it  Mon Mar  3 15:34:26 2008
From: iacopetti at fastpiu.it (Iacopetti dott. Roberto)
Date: Mon, 3 Mar 2008 15:34:26 +0100
Subject: [R-SIG-Finance] ARIMA XREG ERROR (package stats, mass)
Message-ID: <001f01c87d3b$b4dd65d0$0200a8c0@xserver>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080303/8deb703a/attachment.pl 

From Wayne.W.Jones at shell.com  Mon Mar  3 15:49:37 2008
From: Wayne.W.Jones at shell.com (Wayne.W.Jones at shell.com)
Date: Mon, 3 Mar 2008 14:49:37 -0000
Subject: [R-SIG-Finance] ARIMA XREG ERROR (package stats, mass)
In-Reply-To: <001f01c87d3b$b4dd65d0$0200a8c0@xserver>
Message-ID: <77693D6263D9B94AA3C6384F1474E26A02CE576C@wyt-s-019.europe.shell.com>



Hi Roberto, 

Your examples work fine on my 2.6.1 version of R running on windows, see below. 

The main difference in your output is that R says it is trying to fit a model with a zero mean.

try this: 

arima(LakeHuron, order = c ( 2 , 0 , 0 )  , include.mean=T, xreg =  time(LakeHuron) - 1920 )


What version of R are you using on what platform?

Regards

Wayne





> arima(LakeHuron, order = c ( 2 , 0 , 0 )  ,  xreg =  time(LakeHuron) - 1920 )

Call:
arima(x = LakeHuron, order = c(2, 0, 0), xreg = time(LakeHuron) - 1920)

Coefficients:
         ar1      ar2  intercept  time(LakeHuron) - 1920
      1.0048  -0.2913   579.0993                 -0.0216
s.e.  0.0976   0.1004     0.2370                  0.0081

sigma^2 estimated as 0.4566:  log likelihood = -101.2,  aic = 212.4


> arima( beav2$temp , c( 1 , 0 , 0 ), xreg = beav2$activ)

Call:
arima(x = beav2$temp, order = c(1, 0, 0), xreg = beav2$activ)

Coefficients:
         ar1  intercept  beav2$activ
      0.8733    37.1920       0.6139
s.e.  0.0684     0.1187       0.1381

sigma^2 estimated as 0.01518:  log likelihood = 66.78,  aic = -125.55




-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch]On Behalf Of Iacopetti
dott. Roberto
Sent: 03 March 2008 14:34
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] ARIMA XREG ERROR (package stats, mass)


Dear list,

i'm not able to fit an arima model with an external regressor:
an error message appears :
 "regression variables fitted: error in dim(data) <- dim:dim<- :  first argument not valid "

this appens also in a simple examples point out in the function documentation

see   stats 
 
> arima(LakeHuron, order = c ( 2 , 0 , 0 )  ,  xreg =  time(LakeHuron) - 1920 )
Series: LakeHuron 
ARIMA(2,0,0) with zero mean     
Regression variables fitted:
Errore in dim(data) <- dim : dim<- : primo argomento non valido

and more MASS


>arima( beav2$temp , c( 1 , 0 , 0 ), xreg = beav2$activ)
Series: beav2$temp 
ARIMA(1,0,0) with zero mean     

Regression variables fitted:
Errore in dim(data) <- dim : dim<- : primo argomento non valido

Anyone know this problem ??

Any help is very appreciated...........



Roberto Iacopetti


**** Riservatezza / Confidentiality **** 
In ottemperanza al D.L. n. 196 del 30/6/2003 in materia di protezione dei dati personali, le informazioni contenute in questo messaggio sono strettamente riservate ed esclusivamente indirizzate al destinatario indicato (oppure alla persona responsabile di rimetterlo al destinatario). Vogliate tener presente che qualsiasi uso, riproduzione o divulgazione di questo messaggio e vietato. Nel caso in cui aveste ricevuto questo messaggio per errore, vogliate cortesemente avvertire il mittente e distruggere il presente messaggio.

According to Italian law D.L. 196/2003 concerning privacy, if you are not the addressee (or responsible for delivery of the message to such person) you are hereby notified that any disclosure, reproduction, distribution or other dissemination or use of this communication is strictly prohibited. If you have received this message in error, please destroy it and notify us by email

	[[alternative HTML version deleted]]

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.


From Achim.Zeileis at wu-wien.ac.at  Mon Mar  3 17:38:51 2008
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 3 Mar 2008 17:38:51 +0100 (CET)
Subject: [R-SIG-Finance] Frage uber package strucchange
In-Reply-To: <47CAD45C.9060306@etu.unige.ch>
Message-ID: <Pine.LNX.4.44.0803031713220.10626-100000@disco.wu-wien.ac.at>

On Sun, 2 Mar 2008, Matthieu Stigler wrote:

> Hello
>
> I'm using the nice package strucchange to see whether there is a
> structural change in a regression, and especially, if the change is in
> the slope or intercept coefficient.
>
> The "Vignette" with the package (see fig 4 page 8) gives an example to
> obtain a plot of the empirical process for each coefficient. However, I
> don't obtain it, even with the code given in example...
>
> Do you have also only one plot? Do you see what I do wrong, and how I
> could obtain the same plot as in example?

Hmmm, I don't know what could go wrong for you. For me, the code you
attached below produces a plot with three panels (one for each parameter).
Unsurprisingly, this looks like the vignette because it was generated with
the same code.

I've got no idea what might go wrong on your machine...

One additional remark: The function gefp() (instead of efp) has some more
flexible plotting functionality. You can look at

  Achim Zeileis (2006). Implementing a Class of Structural Change
  Tests: An Econometric Computing Approach. Computational Statistics
  and Data Analysis, 50, 2987-3008.

for details. A preprint version of that paper is available from my Web
page.

Best,
Z

> Thanks  a lot!!
>
> The code in example is:
>
> library("strucchange")
> data("USIncExp")
> USIncExp2 <- window(USIncExp, start = c(1985, 12))
>
> coint.res <- residuals(lm(expenditure ~ income, data = USIncExp2))
> coint.res <- lag(ts(coint.res, start = c(1985, 12), freq = 12),
>     k = -1)
> USIncExp2 <- cbind(USIncExp2, diff(USIncExp2), coint.res)
> USIncExp2 <- window(USIncExp2, start = c(1986, 1), end = c(2001,
>     2))
> colnames(USIncExp2) <- c("income", "expenditure", "diff.income",
>     "diff.expenditure", "coint.res")
> ecm.model <- diff.expenditure ~ coint.res + diff.income
>
>
> me <- efp(ecm.model, type = "ME", data = USIncExp2, h = 0.2)
>
> plot(me, functional = NULL)
>
>
>
>


From thomas.dionysopoulos at sgcib.com  Wed Mar  5 09:31:34 2008
From: thomas.dionysopoulos at sgcib.com (thomas.dionysopoulos at sgcib.com)
Date: Wed, 5 Mar 2008 09:31:34 +0100
Subject: [R-SIG-Finance] Rbloomberg Static Data like CRNCY [C1]
Message-ID: <OF655EF606.FA01A16C-ONC1257403.002E82F4-C1257403.002ED5EA@fr.world.socgen>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080305/b92c0c08/attachment.pl 

From anass.mouhsine at sgcib.com  Wed Mar  5 15:21:33 2008
From: anass.mouhsine at sgcib.com (anass.mouhsine at sgcib.com)
Date: Wed, 5 Mar 2008 15:21:33 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by tick timeSeries
	[C1]
Message-ID: <OF707E9C32.7439CF3D-ONC1257403.004E3687-C1257403.004EE0A9@fr.world.socgen>


Hi guys,


I have a timeSeries object like this one
                        V10
2005-04-08 17:31:41 0.01
2005-04-08 17:31:57 0.02
2005-04-08 17:32:00 0.02
2005-04-08 17:32:57 0.02
2005-04-08 17:38:34 0.02
2005-04-08 17:38:49 0.01

and I would like to aggregate the timeSeries in hours or minutes in order
to apply whatever function on the aggregated data (e.g mean, standard dev,
etc...)
I have seen some aggreagation functions like aggregate in the fSeries
package but it aggregates only monthly or quaterly.
I am sure some of you guys were faced to this kind of issue.

Could anyone give me some hints on how to solve my problem?

thanks in advance

Anass
*************************************************************************
This message and any attachments (the "message") are con...{{dropped:10}}


From jeff.a.ryan at gmail.com  Wed Mar  5 16:21:50 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 5 Mar 2008 09:21:50 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by tick
	timeSeries [C1]
In-Reply-To: <OF707E9C32.7439CF3D-ONC1257403.004E3687-C1257403.004EE0A9@fr.world.socgen>
References: <OF707E9C32.7439CF3D-ONC1257403.004E3687-C1257403.004EE0A9@fr.world.socgen>
Message-ID: <e8e755250803050721q7ff85c83s2b6fc69926684bbf@mail.gmail.com>

Anass,

If you want the mean/sd/etc for each hour ?aggregate from zoo will do
the job very well. To get there from timeSeries you can use the new
'xts' package:

myTS [a timeSeries object]

as.xts(myTS) [ now an 'xts' object - which inherits from zoo]

--or--

as.zoo(myTS)

you can even put it back to a timeSeries with little information loss
if you use xts via 'reclass' (changing the series outside of xts
functions currently isn't perfect - but it is good)

If you want to just aggregate the data into OHLC per time period -
to.period in xts is fast and flexible:

to.period(myTS,'minutes',15)
to.period(myTS,'hours')
to.monthly(myTS)

All these work by converting and reconverting to an 'xts' class
internally - so most any (including timeSeries) class will just work
with the function.  In addition it is all compiled code - so it works
well.

One caveat - you'll need to get the most recent xts from
http://r-forge.r-project.org/projects/xts - as there was a minor bug
in the transition of 'to.period' code from quantmod to xts.

Jeff

On Wed, Mar 5, 2008 at 8:21 AM,  <anass.mouhsine at sgcib.com> wrote:
>
>  Hi guys,
>
>
>  I have a timeSeries object like this one
>                         V10
>  2005-04-08 17:31:41 0.01
>  2005-04-08 17:31:57 0.02
>  2005-04-08 17:32:00 0.02
>  2005-04-08 17:32:57 0.02
>  2005-04-08 17:38:34 0.02
>  2005-04-08 17:38:49 0.01
>
>  and I would like to aggregate the timeSeries in hours or minutes in order
>  to apply whatever function on the aggregated data (e.g mean, standard dev,
>  etc...)
>  I have seen some aggreagation functions like aggregate in the fSeries
>  package but it aggregates only monthly or quaterly.
>  I am sure some of you guys were faced to this kind of issue.
>
>  Could anyone give me some hints on how to solve my problem?
>
>  thanks in advance
>
>  Anass
>  *************************************************************************
>  This message and any attachments (the "message") are con...{{dropped:10}}
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From anass.mouhsine at sgcib.com  Wed Mar  5 16:29:58 2008
From: anass.mouhsine at sgcib.com (anass.mouhsine at sgcib.com)
Date: Wed, 5 Mar 2008 16:29:58 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by tick
	timeSeries [NC]
In-Reply-To: <e8e755250803050721q7ff85c83s2b6fc69926684bbf@mail.gmail.com>
Message-ID: <OF1F412160.7D7975B8-ONC1257403.0054DA2F-C1257403.0055240A@fr.world.socgen>

Thank you Jeff,


my aim was to calculate standard statistics + volume weighted prices and
spreads.


I will try out the xts package, but I am wondering if it can aggregate
Multivariate timeSeries (e.g, (spread, volume) or (price,volume)).


thanks again


|------------------------------                                            
|            jeff.a.ryan at gmail                                             
|            .com                                                          
|                                                                          
|            03/05/2008 16:21                                              
|                                                                          
                                                                           
                                                                           
                                                                        To 
                                               Anass                       
                                               MOUHSINE/fr/socgen at socgen   
                                                                        cc 
                                               r-sig-finance at stat.math.eth 
                                               z.ch                        
                                                                   Subject 
                                               Re: [R-SIG-Finance]         
                                               [R-sig-finance] Aggregating 
                                               tick by tick timeSeries     
                                               [C1]                        
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Anass,

If you want the mean/sd/etc for each hour ?aggregate from zoo will do
the job very well. To get there from timeSeries you can use the new
'xts' package:

myTS [a timeSeries object]

as.xts(myTS) [ now an 'xts' object - which inherits from zoo]

--or--

as.zoo(myTS)

you can even put it back to a timeSeries with little information loss
if you use xts via 'reclass' (changing the series outside of xts
functions currently isn't perfect - but it is good)

If you want to just aggregate the data into OHLC per time period -
to.period in xts is fast and flexible:

to.period(myTS,'minutes',15)
to.period(myTS,'hours')
to.monthly(myTS)

All these work by converting and reconverting to an 'xts' class
internally - so most any (including timeSeries) class will just work
with the function.  In addition it is all compiled code - so it works
well.

One caveat - you'll need to get the most recent xts from
http://r-forge.r-project.org/projects/xts - as there was a minor bug
in the transition of 'to.period' code from quantmod to xts.

Jeff

On Wed, Mar 5, 2008 at 8:21 AM,  <anass.mouhsine at sgcib.com> wrote:
>
>  Hi guys,
>
>
>  I have a timeSeries object like this one
>                         V10
>  2005-04-08 17:31:41 0.01
>  2005-04-08 17:31:57 0.02
>  2005-04-08 17:32:00 0.02
>  2005-04-08 17:32:57 0.02
>  2005-04-08 17:38:34 0.02
>  2005-04-08 17:38:49 0.01
>
>  and I would like to aggregate the timeSeries in hours or minutes in
order
>  to apply whatever function on the aggregated data (e.g mean, standard
dev,
>  etc...)
>  I have seen some aggreagation functions like aggregate in the fSeries
>  package but it aggregates only monthly or quaterly.
>  I am sure some of you guys were faced to this kind of issue.
>
>  Could anyone give me some hints on how to solve my problem?
>
>  thanks in advance
>
>  Anass
>
*************************************************************************
>  This message and any attachments (the "message") are
con...{{dropped:10}}
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



--
There's a way to do it better - find it.
Thomas A. Edison

*************************************************************************
This message and any attachments (the "message") are con...{{dropped:10}}


From jeff.a.ryan at gmail.com  Wed Mar  5 16:55:42 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 5 Mar 2008 09:55:42 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by tick
	timeSeries [NC]
In-Reply-To: <OF1F412160.7D7975B8-ONC1257403.0054DA2F-C1257403.0055240A@fr.world.socgen>
References: <e8e755250803050721q7ff85c83s2b6fc69926684bbf@mail.gmail.com>
	<OF1F412160.7D7975B8-ONC1257403.0054DA2F-C1257403.0055240A@fr.world.socgen>
Message-ID: <e8e755250803050755j1d7b88e9ie2fcf541a6068d38@mail.gmail.com>

Anass,

I suspect you will have to write it yourself - as I haven't seen a
general implementation of that myself.  If you go that route it would
be great if you could share your efforts with the list.

Jeff

On Wed, Mar 5, 2008 at 9:29 AM,  <anass.mouhsine at sgcib.com> wrote:
> Thank you Jeff,
>
>
>  my aim was to calculate standard statistics + volume weighted prices and
>  spreads.
>
>
>  I will try out the xts package, but I am wondering if it can aggregate
>  Multivariate timeSeries (e.g, (spread, volume) or (price,volume)).
>
>
>  thanks again
>
>
>  |------------------------------
>  |            jeff.a.ryan at gmail
>  |            .com
>  |
>  |            03/05/2008 16:21
>  |
>
>
>                                                                         To
>                                                Anass
>                                                MOUHSINE/fr/socgen at socgen
>                                                                         cc
>                                                r-sig-finance at stat.math.eth
>                                                z.ch
>                                                                    Subject
>                                                Re: [R-SIG-Finance]
>                                                [R-sig-finance] Aggregating
>                                                tick by tick timeSeries
>                                                [C1]
>
>
>
>
>
>
>
>
>
>
>  Anass,
>
>  If you want the mean/sd/etc for each hour ?aggregate from zoo will do
>  the job very well. To get there from timeSeries you can use the new
>  'xts' package:
>
>  myTS [a timeSeries object]
>
>  as.xts(myTS) [ now an 'xts' object - which inherits from zoo]
>
>  --or--
>
>  as.zoo(myTS)
>
>  you can even put it back to a timeSeries with little information loss
>  if you use xts via 'reclass' (changing the series outside of xts
>  functions currently isn't perfect - but it is good)
>
>  If you want to just aggregate the data into OHLC per time period -
>  to.period in xts is fast and flexible:
>
>  to.period(myTS,'minutes',15)
>  to.period(myTS,'hours')
>  to.monthly(myTS)
>
>  All these work by converting and reconverting to an 'xts' class
>  internally - so most any (including timeSeries) class will just work
>  with the function.  In addition it is all compiled code - so it works
>  well.
>
>  One caveat - you'll need to get the most recent xts from
>  http://r-forge.r-project.org/projects/xts - as there was a minor bug
>  in the transition of 'to.period' code from quantmod to xts.
>
>  Jeff
>
>  On Wed, Mar 5, 2008 at 8:21 AM,  <anass.mouhsine at sgcib.com> wrote:
>  >
>  >  Hi guys,
>  >
>  >
>  >  I have a timeSeries object like this one
>  >                         V10
>  >  2005-04-08 17:31:41 0.01
>  >  2005-04-08 17:31:57 0.02
>  >  2005-04-08 17:32:00 0.02
>  >  2005-04-08 17:32:57 0.02
>  >  2005-04-08 17:38:34 0.02
>  >  2005-04-08 17:38:49 0.01
>  >
>  >  and I would like to aggregate the timeSeries in hours or minutes in
>  order
>  >  to apply whatever function on the aggregated data (e.g mean, standard
>  dev,
>  >  etc...)
>  >  I have seen some aggreagation functions like aggregate in the fSeries
>  >  package but it aggregates only monthly or quaterly.
>  >  I am sure some of you guys were faced to this kind of issue.
>  >
>  >  Could anyone give me some hints on how to solve my problem?
>  >
>  >  thanks in advance
>  >
>  >  Anass
>  >
>  *************************************************************************
>  >  This message and any attachments (the "message") are
>  con...{{dropped:10}}
>  >
>  >  _______________________________________________
>  >  R-SIG-Finance at stat.math.ethz.ch mailing list
>  >  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  >  -- Subscriber-posting only.
>  >  -- If you want to post, subscribe first.
>  >
>
>
>
>  --
>  There's a way to do it better - find it.
>  Thomas A. Edison
>
>  *************************************************************************
>  This message and any attachments (the "message") are confidential and intended solely for the addressee(s).
>  Any unauthorised use or dissemination is prohibited. E-mails are susceptible to alteration.
>  Neither SOCIETE GENERALE nor any of its subsidiaries or affiliates shall be liable for the message if altered, changed or
>  falsified.
>                               ************
>  Ce message et toutes les pieces jointes (ci-apres le "message") sont confidentiels et etablis a l'intention exclusive de ses
>  destinataires. Toute utilisation ou diffusion non autorisee est interdite. Tout message electronique est susceptible d'alteration.
>  La SOCIETE GENERALE et ses filiales declinent toute responsabilite au titre de ce message s'il a ete altere, deforme ou falsifie.
>  *************************************************************************
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From ggrothendieck at gmail.com  Wed Mar  5 17:03:37 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Mar 2008 11:03:37 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by tick
	timeSeries [NC]
In-Reply-To: <OF1F412160.7D7975B8-ONC1257403.0054DA2F-C1257403.0055240A@fr.world.socgen>
References: <e8e755250803050721q7ff85c83s2b6fc69926684bbf@mail.gmail.com>
	<OF1F412160.7D7975B8-ONC1257403.0054DA2F-C1257403.0055240A@fr.world.socgen>
Message-ID: <971536df0803050803i31267b67m58f17d3d2e06874a@mail.gmail.com>

If you are referring to rolling statistics then see ?rollapply in the
zoo package
and note the by.column argument.

On Wed, Mar 5, 2008 at 10:29 AM,  <anass.mouhsine at sgcib.com> wrote:
> Thank you Jeff,
>
>
> my aim was to calculate standard statistics + volume weighted prices and
> spreads.
>
>
> I will try out the xts package, but I am wondering if it can aggregate
> Multivariate timeSeries (e.g, (spread, volume) or (price,volume)).
>
>
> thanks again
>
>
> |------------------------------
> |            jeff.a.ryan at gmail
> |            .com
> |
> |            03/05/2008 16:21
> |
>
>
>                                                                        To
>                                               Anass
>                                               MOUHSINE/fr/socgen at socgen
>                                                                        cc
>                                               r-sig-finance at stat.math.eth
>                                               z.ch
>                                                                   Subject
>                                               Re: [R-SIG-Finance]
>                                               [R-sig-finance] Aggregating
>                                               tick by tick timeSeries
>                                               [C1]
>
>
>
>
>
>
>
>
>
>
> Anass,
>
> If you want the mean/sd/etc for each hour ?aggregate from zoo will do
> the job very well. To get there from timeSeries you can use the new
> 'xts' package:
>
> myTS [a timeSeries object]
>
> as.xts(myTS) [ now an 'xts' object - which inherits from zoo]
>
> --or--
>
> as.zoo(myTS)
>
> you can even put it back to a timeSeries with little information loss
> if you use xts via 'reclass' (changing the series outside of xts
> functions currently isn't perfect - but it is good)
>
> If you want to just aggregate the data into OHLC per time period -
> to.period in xts is fast and flexible:
>
> to.period(myTS,'minutes',15)
> to.period(myTS,'hours')
> to.monthly(myTS)
>
> All these work by converting and reconverting to an 'xts' class
> internally - so most any (including timeSeries) class will just work
> with the function.  In addition it is all compiled code - so it works
> well.
>
> One caveat - you'll need to get the most recent xts from
> http://r-forge.r-project.org/projects/xts - as there was a minor bug
> in the transition of 'to.period' code from quantmod to xts.
>
> Jeff
>
> On Wed, Mar 5, 2008 at 8:21 AM,  <anass.mouhsine at sgcib.com> wrote:
> >
> >  Hi guys,
> >
> >
> >  I have a timeSeries object like this one
> >                         V10
> >  2005-04-08 17:31:41 0.01
> >  2005-04-08 17:31:57 0.02
> >  2005-04-08 17:32:00 0.02
> >  2005-04-08 17:32:57 0.02
> >  2005-04-08 17:38:34 0.02
> >  2005-04-08 17:38:49 0.01
> >
> >  and I would like to aggregate the timeSeries in hours or minutes in
> order
> >  to apply whatever function on the aggregated data (e.g mean, standard
> dev,
> >  etc...)
> >  I have seen some aggreagation functions like aggregate in the fSeries
> >  package but it aggregates only monthly or quaterly.
> >  I am sure some of you guys were faced to this kind of issue.
> >
> >  Could anyone give me some hints on how to solve my problem?
> >
> >  thanks in advance
> >
> >  Anass
> >
> *************************************************************************
> >  This message and any attachments (the "message") are
> con...{{dropped:10}}
> >
> >  _______________________________________________
> >  R-SIG-Finance at stat.math.ethz.ch mailing list
> >  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> >  -- Subscriber-posting only.
> >  -- If you want to post, subscribe first.
> >
>
>
>
> --
> There's a way to do it better - find it.
> Thomas A. Edison
>
> *************************************************************************
> This message and any attachments (the "message") are con...{{dropped:10}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From davidr at rhotrading.com  Wed Mar  5 17:09:33 2008
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 5 Mar 2008 10:09:33 -0600
Subject: [R-SIG-Finance] Rbloomberg Static Data like CRNCY [C1]
In-Reply-To: <OF655EF606.FA01A16C-ONC1257403.002E82F4-C1257403.002ED5EA@fr.world.socgen>
References: <OF655EF606.FA01A16C-ONC1257403.002E82F4-C1257403.002ED5EA@fr.world.socgen>
Message-ID: <F9F2A641C593D7408925574C05A7BE77958271@rhopost.rhotrading.com>

I think you just need to add retval="raw" to your blpGetData call, and
then you'll need to parse out what you need:
> res <- blpGetData(conn, "ES1 Index", "CRNCY", retval="raw")
> res[[1]][[1]]
[1] "USD"

If I recall correctly, blpGetData does some conversions otherwise.

David L. Reiner, PhD
Head Quant
Rho Trading Securities, LLC

-----Original Message-----
From: r-sig-finance-bounces at stat.math.ethz.ch
[mailto:r-sig-finance-bounces at stat.math.ethz.ch] On Behalf Of
thomas.dionysopoulos at sgcib.com
Sent: Wednesday, March 05, 2008 2:32 AM
To: r-sig-finance at stat.math.ethz.ch
Subject: [R-SIG-Finance] Rbloomberg Static Data like CRNCY [C1]

Hi guys

Using  blpConnect and  blpGetData 
I never managed to download static data for example the name of a
security 
or its Currency.

 conn <- blpConnect(iface = "COM", na.action = "bloomberg.handles", 
periodicity = "daily")

res  <-  blpGetData(conn, "ES1 Index",  "CRNCY" , bla, bla ......)



Any ideas? Anyone has done this before?

Thanks
************************************************************************
*
This message and any attachments (the "message") are\ co...{{dropped:9}}


From brian at braverock.com  Wed Mar  5 18:15:06 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 05 Mar 2008 11:15:06 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by
 tick	timeSeries [NC]
In-Reply-To: <OF1F412160.7D7975B8-ONC1257403.0054DA2F-C1257403.0055240A@fr.world.socgen>
References: <OF1F412160.7D7975B8-ONC1257403.0054DA2F-C1257403.0055240A@fr.world.socgen>
Message-ID: <47CED51A.8000707@braverock.com>

aggregate() from zoo should be able to do what you need once your 
timeSeries is converted to xts/zoo.

anass.mouhsine at sgcib.com wrote:
> Thank you Jeff,
> 
> 
> my aim was to calculate standard statistics + volume weighted prices and
> spreads.
> 
> 
> I will try out the xts package, but I am wondering if it can aggregate
> Multivariate timeSeries (e.g, (spread, volume) or (price,volume)).
> 
> 
> thanks again
> 
> 
> |------------------------------                                            
> |            jeff.a.ryan at gmail                                             
> |            .com                                                          
> |                                                                          
> |            03/05/2008 16:21                                              
> |                                                                          
>                                                                            
>                                                                            
>                                                                         To 
>                                                Anass                       
>                                                MOUHSINE/fr/socgen at socgen   
>                                                                         cc 
>                                                r-sig-finance at stat.math.eth 
>                                                z.ch                        
>                                                                    Subject 
>                                                Re: [R-SIG-Finance]         
>                                                [R-sig-finance] Aggregating 
>                                                tick by tick timeSeries     
>                                                [C1]                        
>                                                                            
>                                                                            
>                                                                            
>                                                                            
>                                                                            
>                                                                            
> 
> 
> 
> 
> Anass,
> 
> If you want the mean/sd/etc for each hour ?aggregate from zoo will do
> the job very well. To get there from timeSeries you can use the new
> 'xts' package:
> 
> myTS [a timeSeries object]
> 
> as.xts(myTS) [ now an 'xts' object - which inherits from zoo]
> 
> --or--
> 
> as.zoo(myTS)
> 
> you can even put it back to a timeSeries with little information loss
> if you use xts via 'reclass' (changing the series outside of xts
> functions currently isn't perfect - but it is good)
> 
> If you want to just aggregate the data into OHLC per time period -
> to.period in xts is fast and flexible:
> 
> to.period(myTS,'minutes',15)
> to.period(myTS,'hours')
> to.monthly(myTS)
> 
> All these work by converting and reconverting to an 'xts' class
> internally - so most any (including timeSeries) class will just work
> with the function.  In addition it is all compiled code - so it works
> well.
> 
> One caveat - you'll need to get the most recent xts from
> http://r-forge.r-project.org/projects/xts - as there was a minor bug
> in the transition of 'to.period' code from quantmod to xts.
> 
> Jeff
> 
> On Wed, Mar 5, 2008 at 8:21 AM,  <anass.mouhsine at sgcib.com> wrote:
>>  Hi guys,
>>
>>
>>  I have a timeSeries object like this one
>>                         V10
>>  2005-04-08 17:31:41 0.01
>>  2005-04-08 17:31:57 0.02
>>  2005-04-08 17:32:00 0.02
>>  2005-04-08 17:32:57 0.02
>>  2005-04-08 17:38:34 0.02
>>  2005-04-08 17:38:49 0.01
>>
>>  and I would like to aggregate the timeSeries in hours or minutes in
> order
>>  to apply whatever function on the aggregated data (e.g mean, standard
> dev,
>>  etc...)
>>  I have seen some aggreagation functions like aggregate in the fSeries
>>  package but it aggregates only monthly or quaterly.
>>  I am sure some of you guys were faced to this kind of issue.
>>
>>  Could anyone give me some hints on how to solve my problem?
>>
>>  thanks in advance
>>
>>  Anass


From brian at braverock.com  Wed Mar  5 18:21:14 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 05 Mar 2008 11:21:14 -0600
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by
 tick	timeSeries [NC]
In-Reply-To: <971536df0803050803i31267b67m58f17d3d2e06874a@mail.gmail.com>
References: <e8e755250803050721q7ff85c83s2b6fc69926684bbf@mail.gmail.com>	<OF1F412160.7D7975B8-ONC1257403.0054DA2F-C1257403.0055240A@fr.world.socgen>
	<971536df0803050803i31267b67m58f17d3d2e06874a@mail.gmail.com>
Message-ID: <47CED68A.2010608@braverock.com>

So, to Gabor's point,  A combination of apply() or rollapply() and 
aggregate() should be able to apply or run specific statistics on your 
(spread, volume) or (price,volume) series.

To Jeff's point, you will need to write some code to wrap the various 
functions you want to call, but it doesn't sound like anything you've 
described so far is particularly difficult.


If you could be more specific about what you want to do, the list may be 
able to help a little more.

Cheers,

   - Brian

Gabor Grothendieck wrote:
> If you are referring to rolling statistics then see ?rollapply in the
> zoo package
> and note the by.column argument.
> 
> On Wed, Mar 5, 2008 at 10:29 AM,  <anass.mouhsine at sgcib.com> wrote:
>> Thank you Jeff,
>>
>>
>> my aim was to calculate standard statistics + volume weighted prices and
>> spreads.
>>
>>
>> I will try out the xts package, but I am wondering if it can aggregate
>> Multivariate timeSeries (e.g, (spread, volume) or (price,volume)).
>>
>>
>> thanks again
>>
>>
>> |------------------------------
>> |            jeff.a.ryan at gmail
>> |            .com
>> |
>> |            03/05/2008 16:21
>> |
>>
>>
>>                                                                        To
>>                                               Anass
>>                                               MOUHSINE/fr/socgen at socgen
>>                                                                        cc
>>                                               r-sig-finance at stat.math.eth
>>                                               z.ch
>>                                                                   Subject
>>                                               Re: [R-SIG-Finance]
>>                                               [R-sig-finance] Aggregating
>>                                               tick by tick timeSeries
>>                                               [C1]
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> Anass,
>>
>> If you want the mean/sd/etc for each hour ?aggregate from zoo will do
>> the job very well. To get there from timeSeries you can use the new
>> 'xts' package:
>>
>> myTS [a timeSeries object]
>>
>> as.xts(myTS) [ now an 'xts' object - which inherits from zoo]
>>
>> --or--
>>
>> as.zoo(myTS)
>>
>> you can even put it back to a timeSeries with little information loss
>> if you use xts via 'reclass' (changing the series outside of xts
>> functions currently isn't perfect - but it is good)
>>
>> If you want to just aggregate the data into OHLC per time period -
>> to.period in xts is fast and flexible:
>>
>> to.period(myTS,'minutes',15)
>> to.period(myTS,'hours')
>> to.monthly(myTS)
>>
>> All these work by converting and reconverting to an 'xts' class
>> internally - so most any (including timeSeries) class will just work
>> with the function.  In addition it is all compiled code - so it works
>> well.
>>
>> One caveat - you'll need to get the most recent xts from
>> http://r-forge.r-project.org/projects/xts - as there was a minor bug
>> in the transition of 'to.period' code from quantmod to xts.
>>
>> Jeff
>>
>> On Wed, Mar 5, 2008 at 8:21 AM,  <anass.mouhsine at sgcib.com> wrote:
>>>  Hi guys,
>>>
>>>
>>>  I have a timeSeries object like this one
>>>                         V10
>>>  2005-04-08 17:31:41 0.01
>>>  2005-04-08 17:31:57 0.02
>>>  2005-04-08 17:32:00 0.02
>>>  2005-04-08 17:32:57 0.02
>>>  2005-04-08 17:38:34 0.02
>>>  2005-04-08 17:38:49 0.01
>>>
>>>  and I would like to aggregate the timeSeries in hours or minutes in
>> order
>>>  to apply whatever function on the aggregated data (e.g mean, standard
>> dev,
>>>  etc...)
>>>  I have seen some aggreagation functions like aggregate in the fSeries
>>>  package but it aggregates only monthly or quaterly.
>>>  I am sure some of you guys were faced to this kind of issue.
>>>
>>>  Could anyone give me some hints on how to solve my problem?
>>>
>>>  thanks in advance
>>>
>>>  Anass


From anass.mouhsine at sgcib.com  Thu Mar  6 11:38:31 2008
From: anass.mouhsine at sgcib.com (anass.mouhsine at sgcib.com)
Date: Thu, 6 Mar 2008 11:38:31 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by
	tick	timeSeries [NC]
In-Reply-To: <47CED68A.2010608@braverock.com>
Message-ID: <OFC6DD2DFA.A7013EBB-ONC1257404.0039D391-C1257404.003A752F@fr.world.socgen>

thank you gus for your thoughts!!

I am starting to replicate some C++ code I developed years ago on LOB
(Limit Order Book) analysis into R.
So I start with the simple VWAP, VWAS, TWAS etc...but the objective is to
analyse the Order book resiliency and depthness.
For that purpose, I need to apply a set of  functions to a multivariate
tick by tick timeSeries and aggregate by time bar or even volume bars.

thanks again for your help

cheers,

Anass


|------------------------------                                            
|            brian at braverock.c                                             
|            om                                                            
|                                                                          
|            03/05/2008 18:21                                              
|                                                                          
                                                                           
                                                                           
                                                                        To 
                                                                           
                                                                        cc 
                                               Anass                       
                                               MOUHSINE/fr/socgen at socgen,  
                                               r-sig-finance at stat.math.eth 
                                               z.ch                        
                                                                   Subject 
                                               Re: [R-SIG-Finance]         
                                               [R-sig-finance] Aggregating 
                                               tick by tick    timeSeries  
                                               [NC]                        
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




So, to Gabor's point,  A combination of apply() or rollapply() and
aggregate() should be able to apply or run specific statistics on your
(spread, volume) or (price,volume) series.

To Jeff's point, you will need to write some code to wrap the various
functions you want to call, but it doesn't sound like anything you've
described so far is particularly difficult.


If you could be more specific about what you want to do, the list may be
able to help a little more.

Cheers,

   - Brian

Gabor Grothendieck wrote:
> If you are referring to rolling statistics then see ?rollapply in the
> zoo package
> and note the by.column argument.
>
> On Wed, Mar 5, 2008 at 10:29 AM,  <anass.mouhsine at sgcib.com> wrote:
>> Thank you Jeff,
>>
>>
>> my aim was to calculate standard statistics + volume weighted prices and
>> spreads.
>>
>>
>> I will try out the xts package, but I am wondering if it can aggregate
>> Multivariate timeSeries (e.g, (spread, volume) or (price,volume)).
>>
>>
>> thanks again
>>
>>
>> |------------------------------
>> |            jeff.a.ryan at gmail
>> |            .com
>> |
>> |            03/05/2008 16:21
>> |
>>
>>
>>
To
>>                                               Anass
>>                                               MOUHSINE/fr/socgen at socgen
>>
cc
>>
r-sig-finance at stat.math.eth
>>                                               z.ch
>>
Subject
>>                                               Re: [R-SIG-Finance]
>>                                               [R-sig-finance]
Aggregating
>>                                               tick by tick timeSeries
>>                                               [C1]
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> Anass,
>>
>> If you want the mean/sd/etc for each hour ?aggregate from zoo will do
>> the job very well. To get there from timeSeries you can use the new
>> 'xts' package:
>>
>> myTS [a timeSeries object]
>>
>> as.xts(myTS) [ now an 'xts' object - which inherits from zoo]
>>
>> --or--
>>
>> as.zoo(myTS)
>>
>> you can even put it back to a timeSeries with little information loss
>> if you use xts via 'reclass' (changing the series outside of xts
>> functions currently isn't perfect - but it is good)
>>
>> If you want to just aggregate the data into OHLC per time period -
>> to.period in xts is fast and flexible:
>>
>> to.period(myTS,'minutes',15)
>> to.period(myTS,'hours')
>> to.monthly(myTS)
>>
>> All these work by converting and reconverting to an 'xts' class
>> internally - so most any (including timeSeries) class will just work
>> with the function.  In addition it is all compiled code - so it works
>> well.
>>
>> One caveat - you'll need to get the most recent xts from
>> http://r-forge.r-project.org/projects/xts - as there was a minor bug
>> in the transition of 'to.period' code from quantmod to xts.
>>
>> Jeff
>>
>> On Wed, Mar 5, 2008 at 8:21 AM,  <anass.mouhsine at sgcib.com> wrote:
>>>  Hi guys,
>>>
>>>
>>>  I have a timeSeries object like this one
>>>                         V10
>>>  2005-04-08 17:31:41 0.01
>>>  2005-04-08 17:31:57 0.02
>>>  2005-04-08 17:32:00 0.02
>>>  2005-04-08 17:32:57 0.02
>>>  2005-04-08 17:38:34 0.02
>>>  2005-04-08 17:38:49 0.01
>>>
>>>  and I would like to aggregate the timeSeries in hours or minutes in
>> order
>>>  to apply whatever function on the aggregated data (e.g mean, standard
>> dev,
>>>  etc...)
>>>  I have seen some aggreagation functions like aggregate in the fSeries
>>>  package but it aggregates only monthly or quaterly.
>>>  I am sure some of you guys were faced to this kind of issue.
>>>
>>>  Could anyone give me some hints on how to solve my problem?
>>>
>>>  thanks in advance
>>>
>>>  Anass

*************************************************************************
This message and any attachments (the "message") are con...{{dropped:10}}


From john_d_mchenry at yahoo.com  Sat Mar  8 18:40:16 2008
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Sat, 8 Mar 2008 09:40:16 -0800 (PST)
Subject: [R-SIG-Finance] plotting NAs
Message-ID: <236280.35484.qm@web35408.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080308/0eef8241/attachment.pl 

From elise at predictionimpact.com  Wed Mar 12 05:56:00 2008
From: elise at predictionimpact.com (Elise Johnson)
Date: Tue, 11 Mar 2008 21:56:00 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Predictive Analytics for Business,
 Marketing and Web (April 3-4, May 8-9, June 5-6)
Message-ID: <15997933.post@talk.nabble.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080311/7fb74152/attachment.pl 

From adschai at optonline.net  Fri Mar 14 04:32:59 2008
From: adschai at optonline.net (adschai at optonline.net)
Date: Fri, 14 Mar 2008 03:32:59 +0000 (GMT)
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
Message-ID: <e7d6d594152a1.47d9f1eb@optonline.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080314/f7aedea8/attachment.pl 

From brian at braverock.com  Fri Mar 14 12:52:15 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 14 Mar 2008 06:52:15 -0500
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
In-Reply-To: <e7d6d594152a1.47d9f1eb@optonline.net>
References: <e7d6d594152a1.47d9f1eb@optonline.net>
Message-ID: <200803140652.15423.brian@braverock.com>

On Thursday 13 March 2008 22:32:59 adschai at optonline.net wrote:
> Hi,I'm looking for VAR allocation framework among traders. I saw some
> papers but none of which (at least that I saw) look practical. I am
> wondering if anyone can hint me some idea or some reference? The situation
> is if at the desk level you were given a certain amount of VAR limit, how
> should one allocate the number among traders? THank you.adschai

Calculate Component VaR.

The first definition (as far as I know) is in Garman in Risk Magazine.  The 
article may be found here:

Garman, Mark, "Taking VaR to Pieces (Component VaR)," RISK 10, 10, October 
1997.
http://www.fea.com/pdf/componentvar.pdf

He also has a longer working paper on the topic here:

http://www.gloriamundi.org/detailpopup.asp?ID=453055537

We implemented Component VaR for assets with non-normal distribution in our 
recent paper here:

Boudt, Kris, Peterson, Brian G. and Croux, Christophe, "Estimation and 
Decomposition of Downside Risk for Portfolios With Non-Normal Returns" 
(October 31, 2007).
http://ssrn.com/abstract=1024151

All code for our paper was implemented in R, and is available.  We will also 
be cleaning up and documenting the functions in the next version of 
PerformanceAnalytics.

Regards,
  
    - Brian


From sergi.mg at gmail.com  Fri Mar 14 15:12:52 2008
From: sergi.mg at gmail.com (=?ISO-8859-1?Q?Sergi_Mart=EDnez?=)
Date: Fri, 14 Mar 2008 15:12:52 +0100
Subject: [R-SIG-Finance] CreditRisk+
Message-ID: <222a98370803140712m68b019fwc0549efb71b11a37@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080314/932330de/attachment.pl 

From brian at braverock.com  Fri Mar 14 17:59:10 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 14 Mar 2008 11:59:10 -0500
Subject: [R-SIG-Finance] CreditRisk+
In-Reply-To: <222a98370803140712m68b019fwc0549efb71b11a37@mail.gmail.com>
References: <222a98370803140712m68b019fwc0549efb71b11a37@mail.gmail.com>
Message-ID: <47DAAEDE.7060803@braverock.com>

Sergi Mart?nez wrote:
>  <r-sig-finance at stat.math.ethz.ch>
> I know that your time is valuable, therefore I will be brief.
> 
> I am looking for some packages that it has implemented the CreditRisk+ model
> in R by any of its variants (FFT, the Saddle Point approximation and the
> Panjer recursion), for one or several sectors.

Have you looked at the 'creditmetrics' package?  creditrisk+ and 
creditmetrics have some very similar model capabilities.

http://cran.r-project.org/web/packages/CreditMetrics/index.html

Also, I assume you're referring to the CSFB CreditRisk+ product, and the 
models defined here:

http://www.csfb.com/institutional/research/assets/creditrisk.pdf

R has the function 'fft' for Fast Fourier Transform

The 'boot' package provides several saddle point distribution functions.

Several methods proposed by Panjer are available in the R package 'actuar'

Please share your results with this list as you experiment with these 
various functions.

Regards,

   - Brian


From ecjbosu at aol.com  Sat Mar 15 13:19:20 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Sat, 15 Mar 2008 07:19:20 -0500
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
In-Reply-To: <200803140652.15423.brian@braverock.com>
References: <e7d6d594152a1.47d9f1eb@optonline.net>
	<200803140652.15423.brian@braverock.com>
Message-ID: <47DBBEC8.5090904@aol.com>

Brian G. Peterson wrote:
> On Thursday 13 March 2008 22:32:59 adschai at optonline.net wrote:
>> Hi,I'm looking for VAR allocation framework among traders. I saw some
>> papers but none of which (at least that I saw) look practical. I am
>> wondering if anyone can hint me some idea or some reference? The situation
>> is if at the desk level you were given a certain amount of VAR limit, how
>> should one allocate the number among traders? THank you.adschai
> 
> Calculate Component VaR.
> 
> The first definition (as far as I know) is in Garman in Risk Magazine.  The 
> article may be found here:
Riskmetrics is also a good resource for this
http://www.issproxy.com/publications/techdoc.html
The first two publications with the second having component var.  Note 
riskmetrics and jorion's definitions of component var differ.

> 
> Garman, Mark, "Taking VaR to Pieces (Component VaR)," RISK 10, 10, October 
> 1997.
> http://www.fea.com/pdf/componentvar.pdf
> 
> He also has a longer working paper on the topic here:
> 
> http://www.gloriamundi.org/detailpopup.asp?ID=453055537
> 
> We implemented Component VaR for assets with non-normal distribution in our 
> recent paper here:
> 
> Boudt, Kris, Peterson, Brian G. and Croux, Christophe, "Estimation and 
> Decomposition of Downside Risk for Portfolios With Non-Normal Returns" 
> (October 31, 2007).
> http://ssrn.com/abstract=1024151
> 
> All code for our paper was implemented in R, and is available.  We will also 
> be cleaning up and documenting the functions in the next version of 
> PerformanceAnalytics.
> 
> Regards,
>   
>     - Brian
>


From ecjbosu at aol.com  Sat Mar 15 13:19:20 2008
From: ecjbosu at aol.com (Joe W. Byers)
Date: Sat, 15 Mar 2008 07:19:20 -0500
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
In-Reply-To: <200803140652.15423.brian@braverock.com>
References: <e7d6d594152a1.47d9f1eb@optonline.net>
	<200803140652.15423.brian@braverock.com>
Message-ID: <47DBBEC8.5090904@aol.com>

Brian G. Peterson wrote:
> On Thursday 13 March 2008 22:32:59 adschai at optonline.net wrote:
>> Hi,I'm looking for VAR allocation framework among traders. I saw some
>> papers but none of which (at least that I saw) look practical. I am
>> wondering if anyone can hint me some idea or some reference? The situation
>> is if at the desk level you were given a certain amount of VAR limit, how
>> should one allocate the number among traders? THank you.adschai
> 
> Calculate Component VaR.
> 
> The first definition (as far as I know) is in Garman in Risk Magazine.  The 
> article may be found here:
Riskmetrics is also a good resource for this
http://www.issproxy.com/publications/techdoc.html
The first two publications with the second having component var.  Note 
riskmetrics and jorion's definitions of component var differ.

> 
> Garman, Mark, "Taking VaR to Pieces (Component VaR)," RISK 10, 10, October 
> 1997.
> http://www.fea.com/pdf/componentvar.pdf
> 
> He also has a longer working paper on the topic here:
> 
> http://www.gloriamundi.org/detailpopup.asp?ID=453055537
> 
> We implemented Component VaR for assets with non-normal distribution in our 
> recent paper here:
> 
> Boudt, Kris, Peterson, Brian G. and Croux, Christophe, "Estimation and 
> Decomposition of Downside Risk for Portfolios With Non-Normal Returns" 
> (October 31, 2007).
> http://ssrn.com/abstract=1024151
> 
> All code for our paper was implemented in R, and is available.  We will also 
> be cleaning up and documenting the functions in the next version of 
> PerformanceAnalytics.
> 
> Regards,
>   
>     - Brian
>


From kriskumar at earthlink.net  Sat Mar 15 14:37:46 2008
From: kriskumar at earthlink.net (kriskumar at earthlink.net)
Date: Sat, 15 Mar 2008 13:37:46 +0000
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
In-Reply-To: <47DBBEC8.5090904@aol.com>
References: <e7d6d594152a1.47d9f1eb@optonline.net><200803140652.15423.brian@braverock.com><47DBBEC8.5090904@aol.com>
Message-ID: <2078636675-1205584756-cardhu_decombobulator_blackberry.rim.net-653392480-@bxe108.bisx.prod.on.blackberry>

My two cents on this... to paraphrase taleb using var to manage risk or allocate capital is like driving an airplane with a faulty altimeter. Probably ok in quieter markets but quite risky in these turbulent markets. I might be mistaken but I think  comp VaR is not additive?

Finally if you are going to use this because of Basel-2 etc I think it is key to make sure that it is not gamed. 

Best
Krishna


----
"When I get a little money, I buy books and if any 
      is left, I buy food and clothes."  -- Erasmus

-----Original Message-----
From: "Joe W. Byers" <ecjbosu at aol.com>

Date: Sat, 15 Mar 2008 07:19:20 
To:r-sig-finance at stat.math.ethz.ch
Cc:r-sig-finance at stat.math.ethz.ch
Subject: Re: [R-SIG-Finance] Framework for VAR allocation among traders


Brian G. Peterson wrote:
> On Thursday 13 March 2008 22:32:59 adschai at optonline.net wrote:
>> Hi,I'm looking for VAR allocation framework among traders. I saw some
>> papers but none of which (at least that I saw) look practical. I am
>> wondering if anyone can hint me some idea or some reference? The situation
>> is if at the desk level you were given a certain amount of VAR limit, how
>> should one allocate the number among traders? THank you.adschai
> 
> Calculate Component VaR.
> 
> The first definition (as far as I know) is in Garman in Risk Magazine.  The 
> article may be found here:
Riskmetrics is also a good resource for this
http://www.issproxy.com/publications/techdoc.html
The first two publications with the second having component var.  Note 
riskmetrics and jorion's definitions of component var differ.

> 
> Garman, Mark, "Taking VaR to Pieces (Component VaR)," RISK 10, 10, October 
> 1997.
> http://www.fea.com/pdf/componentvar.pdf
> 
> He also has a longer working paper on the topic here:
> 
> http://www.gloriamundi.org/detailpopup.asp?ID=453055537
> 
> We implemented Component VaR for assets with non-normal distribution in our 
> recent paper here:
> 
> Boudt, Kris, Peterson, Brian G. and Croux, Christophe, "Estimation and 
> Decomposition of Downside Risk for Portfolios With Non-Normal Returns" 
> (October 31, 2007).
> http://ssrn.com/abstract=1024151
> 
> All code for our paper was implemented in R, and is available.  We will also 
> be cleaning up and documenting the functions in the next version of 
> PerformanceAnalytics.
> 
> Regards,
>   
>     - Brian
>

_______________________________________________
R-SIG-Finance at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-finance
-- Subscriber-posting only. 
-- If you want to post, subscribe first.

From brian at braverock.com  Sat Mar 15 13:51:36 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Sat, 15 Mar 2008 07:51:36 -0500
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
In-Reply-To: <2078636675-1205584756-cardhu_decombobulator_blackberry.rim.net-653392480-@bxe108.bisx.prod.on.blackberry>
References: <e7d6d594152a1.47d9f1eb@optonline.net> <47DBBEC8.5090904@aol.com>
	<2078636675-1205584756-cardhu_decombobulator_blackberry.rim.net-653392480-@bxe108.bisx.prod.on.blackberry>
Message-ID: <200803150751.36337.brian@braverock.com>

On Saturday 15 March 2008 08:37:46 kriskumar at earthlink.net wrote:
> My two cents on this... to paraphrase taleb using var to manage risk or
> allocate capital is like driving an airplane with a faulty altimeter.
> Probably ok in quieter markets but quite risky in these turbulent markets.

I recommend against using a normal distribution assumption at any time.  One 
of the reasons I like the Cornish Fisher expansion for incorporating 
non-normality is that it directly ustilizes skewness and kiurtosis, which 
have meaning independent of the VaR calculation.  Extending this to Component 
VaR (or ES) incorporates the higher co-moments as well.

I also think that measuring VaR (preferably without assuming the normal) and 
stress or scenario testing the portfolio are two separate and complimentary 
activities.

Kris has mentioned Taleb ("Black Swans"), and I can also recommend 
Rebonato's "Plight of the Fortune Tellers" as an excellent deconstruction of 
many current risk practices (without resorting to the math).

> I might be mistaken but I think  comp VaR is not additive?

Component VaR and Component ES are coherent risk measures (per Artzner), and 
subadditive.

What most authors call Marginal VaR is *not* subadditive.

> Finally if you are going to use this because of Basel-2 etc I think it is
> key to make sure that it is not gamed.

Wholeheartedly agreed.  Note that the regulators require both VaR calculation 
and stress tests, as described above.

Regards,

   - Brian


From ahala2000 at yahoo.com  Mon Mar 17 14:00:04 2008
From: ahala2000 at yahoo.com (elton wang)
Date: Mon, 17 Mar 2008 06:00:04 -0700 (PDT)
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
In-Reply-To: <200803140652.15423.brian@braverock.com>
Message-ID: <996045.65801.qm@web31402.mail.mud.yahoo.com>

Brian,
I have a question on your paper:
If you use skewness and kurtosis in the VaR
calculation, you want to make sure:
1. these are exist if the underlying distribution is
non-normal.
2. your sample skewness and kurtosis is good estimates
of true skewness and hurtosis.

In part 5 you discussed the Robust estimation but it
could be stronger argument IMHO. For example, do you
have convergence/sensitivity analysis on estimated
skewness/kurtosis results for your cleaning method? 


Thanks,

E wang
 

--- "Brian G. Peterson" <brian at braverock.com> wrote:

> On Thursday 13 March 2008 22:32:59
> adschai at optonline.net wrote:
> > Hi,I'm looking for VAR allocation framework among
> traders. I saw some
> > papers but none of which (at least that I saw)
> look practical. I am
> > wondering if anyone can hint me some idea or some
> reference? The situation
> > is if at the desk level you were given a certain
> amount of VAR limit, how
> > should one allocate the number among traders?
> THank you.adschai
> 
> Calculate Component VaR.
> 
> The first definition (as far as I know) is in Garman
> in Risk Magazine.  The 
> article may be found here:
> 
> Garman, Mark, "Taking VaR to Pieces (Component
> VaR)," RISK 10, 10, October 
> 1997.
> http://www.fea.com/pdf/componentvar.pdf
> 
> He also has a longer working paper on the topic
> here:
> 
>
http://www.gloriamundi.org/detailpopup.asp?ID=453055537
> 
> We implemented Component VaR for assets with
> non-normal distribution in our 
> recent paper here:
> 
> Boudt, Kris, Peterson, Brian G. and Croux,
> Christophe, "Estimation and 
> Decomposition of Downside Risk for Portfolios With
> Non-Normal Returns" 
> (October 31, 2007).
> http://ssrn.com/abstract=1024151
> 
> All code for our paper was implemented in R, and is
> available.  We will also 
> be cleaning up and documenting the functions in the
> next version of 
> PerformanceAnalytics.
> 
> Regards,
>   
>     - Brian
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From brian at braverock.com  Mon Mar 17 15:20:56 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 17 Mar 2008 09:20:56 -0500
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
In-Reply-To: <996045.65801.qm@web31402.mail.mud.yahoo.com>
References: <996045.65801.qm@web31402.mail.mud.yahoo.com>
Message-ID: <47DE7E48.4010400@braverock.com>

elton wang wrote:
> Brian,
> I have a question on your paper:
> If you use skewness and kurtosis in the VaR
> calculation, you want to make sure:
 >
> 1. these are exist if the underlying distribution is
> non-normal.

At least one of skewness!=0 or kurtosis!=3 exist if the underlying 
distribution is non-normal.  Perhaps I don't understand your first point?

If skewness=0 and kurtosis=3, the Cornish-Fisher expansion does not 
change the Gaussian normal distribution.  So it should have no adverse 
consequences if utilized even if all portfolio assets were normal (which 
seems a highly unlikely circumstance).

> 2. your sample skewness and kurtosis is good estimates
> of true skewness and hurtosis.

While it is possible to fit many different fat-tailed distributions to 
the sample, and derive skewness and kurtosis from these, I don't see how 
this is a better approach than utilizing the sample skewness and 
kurtosis.  We did show in the paper how to test the Cornish Fisher and 
Edgeworth expansion against a very skewed and fat-tailed Skew Student-t 
distribution.

Another problem with utilizing a fitted distribution is that many fitted 
distributions would  not carry the same properties of being 
differentiable by the weight (properties of the Gaussian normal and 
Cornish Fisher distributions) in a portfolio to obtain a good estimator 
of Component Risk in a portfolio.

In the main, the data cleaning method is most valuable for adding 
stability to the effects of the co-moments in decomposing the risk to 
avoid undue influence by a small number of extreme events.  The method 
was developed to specifically not change observations that were not "in 
the tail", and to keep the direction (but not the absolute magnitude) of 
the extreme events.  As I discussed in the text of the paper, I do not 
believe that you would ever use the cleaning method for measuring VaR or 
ES ex port, but only to stabilize the predictions of contribution on a 
forward-looking ex ante basis.

> In part 5 you discussed the Robust estimation but it
> could be stronger argument IMHO. For example, do you
> have convergence/sensitivity analysis on estimated
> skewness/kurtosis results for your cleaning method? 

I agree that a sensitivity analysis would be a good addition.  I will 
start thinking about how to add that.

Regards,

   - Brian


 > --- "Brian G. Peterson" <brian at braverock.com> wrote:
 >
 >> On Thursday 13 March 2008 22:32:59
 >> adschai at optonline.net wrote:
 >>> Hi,I'm looking for VAR allocation framework among
 >> traders. I saw some
 >>> papers but none of which (at least that I saw)
 >> look practical. I am
 >>> wondering if anyone can hint me some idea or some
 >> reference? The situation
 >>> is if at the desk level you were given a certain
 >> amount of VAR limit, how
 >>> should one allocate the number among traders?
 >> Thank you.adschai
 >>
 >> Calculate Component VaR.
 >>
 >> The first definition (as far as I know) is in Garman
 >> in Risk Magazine.  The
 >> article may be found here:
 >>
 >> Garman, Mark, "Taking VaR to Pieces (Component
 >> VaR)," RISK 10, 10, October
 >> 1997.
 >> http://www.fea.com/pdf/componentvar.pdf
 >>
 >> He also has a longer working paper on the topic
 >> here:
 >>
 >>
 > http://www.gloriamundi.org/detailpopup.asp?ID=453055537
 >> We implemented Component VaR for assets with
 >> non-normal distribution in our
 >> recent paper here:
 >>
 >> Boudt, Kris, Peterson, Brian G. and Croux,
 >> Christophe, "Estimation and
 >> Decomposition of Downside Risk for Portfolios With
 >> Non-Normal Returns"
 >> (October 31, 2007).
 >> http://ssrn.com/abstract=1024151
 >>
 >> All code for our paper was implemented in R, and is
 >> available.  We will also
 >> be cleaning up and documenting the functions in the
 >> next version of
 >> PerformanceAnalytics.
 >>
 >> Regards,
 >>
 >>     - Brian
 >>
 >> _______


From ahala2000 at yahoo.com  Mon Mar 17 15:37:14 2008
From: ahala2000 at yahoo.com (elton wang)
Date: Mon, 17 Mar 2008 07:37:14 -0700 (PDT)
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
In-Reply-To: <47DE7E48.4010400@braverock.com>
Message-ID: <77424.77986.qm@web31413.mail.mud.yahoo.com>

For example, if underlying is a t distribution with
DOF=4, then kurtosis does not exsit. Any sample
kurtosis (with any cleaning tech or not) would be a
false stat of underlying didstribution. 
How can you rule out this possibility of underlying
distribution?

--- "Brian G. Peterson" <brian at braverock.com> wrote:

> elton wang wrote:
> > Brian,
> > I have a question on your paper:
> > If you use skewness and kurtosis in the VaR
> > calculation, you want to make sure:
>  >
> > 1. these are exist if the underlying distribution
> is
> > non-normal.
> 
> At least one of skewness!=0 or kurtosis!=3 exist if
> the underlying 
> distribution is non-normal.  Perhaps I don't
> understand your first point?
> 
> If skewness=0 and kurtosis=3, the Cornish-Fisher
> expansion does not 
> change the Gaussian normal distribution.  So it
> should have no adverse 
> consequences if utilized even if all portfolio
> assets were normal (which 
> seems a highly unlikely circumstance).
> 
> > 2. your sample skewness and kurtosis is good
> estimates
> > of true skewness and hurtosis.
> 
> While it is possible to fit many different
> fat-tailed distributions to 
> the sample, and derive skewness and kurtosis from
> these, I don't see how 
> this is a better approach than utilizing the sample
> skewness and 
> kurtosis.  We did show in the paper how to test the
> Cornish Fisher and 
> Edgeworth expansion against a very skewed and
> fat-tailed Skew Student-t 
> distribution.
> 
> Another problem with utilizing a fitted distribution
> is that many fitted 
> distributions would  not carry the same properties
> of being 
> differentiable by the weight (properties of the
> Gaussian normal and 
> Cornish Fisher distributions) in a portfolio to
> obtain a good estimator 
> of Component Risk in a portfolio.
> 
> In the main, the data cleaning method is most
> valuable for adding 
> stability to the effects of the co-moments in
> decomposing the risk to 
> avoid undue influence by a small number of extreme
> events.  The method 
> was developed to specifically not change
> observations that were not "in 
> the tail", and to keep the direction (but not the
> absolute magnitude) of 
> the extreme events.  As I discussed in the text of
> the paper, I do not 
> believe that you would ever use the cleaning method
> for measuring VaR or 
> ES ex port, but only to stabilize the predictions of
> contribution on a 
> forward-looking ex ante basis.
> 
> > In part 5 you discussed the Robust estimation but
> it
> > could be stronger argument IMHO. For example, do
> you
> > have convergence/sensitivity analysis on estimated
> > skewness/kurtosis results for your cleaning
> method? 
> 
> I agree that a sensitivity analysis would be a good
> addition.  I will 
> start thinking about how to add that.
> 
> Regards,
> 
>    - Brian
> 
> 
>  > --- "Brian G. Peterson" <brian at braverock.com>
> wrote:
>  >
>  >> On Thursday 13 March 2008 22:32:59
>  >> adschai at optonline.net wrote:
>  >>> Hi,I'm looking for VAR allocation framework
> among
>  >> traders. I saw some
>  >>> papers but none of which (at least that I saw)
>  >> look practical. I am
>  >>> wondering if anyone can hint me some idea or
> some
>  >> reference? The situation
>  >>> is if at the desk level you were given a
> certain
>  >> amount of VAR limit, how
>  >>> should one allocate the number among traders?
>  >> Thank you.adschai
>  >>
>  >> Calculate Component VaR.
>  >>
>  >> The first definition (as far as I know) is in
> Garman
>  >> in Risk Magazine.  The
>  >> article may be found here:
>  >>
>  >> Garman, Mark, "Taking VaR to Pieces (Component
>  >> VaR)," RISK 10, 10, October
>  >> 1997.
>  >> http://www.fea.com/pdf/componentvar.pdf
>  >>
>  >> He also has a longer working paper on the topic
>  >> here:
>  >>
>  >>
>  >
>
http://www.gloriamundi.org/detailpopup.asp?ID=453055537
>  >> We implemented Component VaR for assets with
>  >> non-normal distribution in our
>  >> recent paper here:
>  >>
>  >> Boudt, Kris, Peterson, Brian G. and Croux,
>  >> Christophe, "Estimation and
>  >> Decomposition of Downside Risk for Portfolios
> With
>  >> Non-Normal Returns"
>  >> (October 31, 2007).
>  >> http://ssrn.com/abstract=1024151
>  >>
>  >> All code for our paper was implemented in R, and
> is
>  >> available.  We will also
>  >> be cleaning up and documenting the functions in
> the
>  >> next version of
>  >> PerformanceAnalytics.
>  >>
>  >> Regards,
>  >>
>  >>     - Brian
>  >>
>  >> _______
>


From ahala2000 at yahoo.com  Mon Mar 17 15:45:10 2008
From: ahala2000 at yahoo.com (elton wang)
Date: Mon, 17 Mar 2008 07:45:10 -0700 (PDT)
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
Message-ID: <136011.66476.qm@web31407.mail.mud.yahoo.com>

My point is,
when underlying is non normal, any sample higher
moments may highly sensitive to outliers; without a
study of sample moments sensitity and converegence to
outliers, you can not justify the quality of VaR
modification.


you tested/simulated one skewed t distribution, but
you can not rule out all other underlying distribution
possibilities even within t-distribution with
different DOF.

These higher momonents mod on VaR are overdone IMHO.


--- elton wang <ahala2000 at yahoo.com> wrote:

> For example, if underlying is a t distribution with
> DOF=4, then kurtosis does not exsit. Any sample
> kurtosis (with any cleaning tech or not) would be a
> false stat of underlying didstribution. 
> How can you rule out this possibility of underlying
> distribution?
> 
> --- "Brian G. Peterson" <brian at braverock.com> wrote:
> 
> > elton wang wrote:
> > > Brian,
> > > I have a question on your paper:
> > > If you use skewness and kurtosis in the VaR
> > > calculation, you want to make sure:
> >  >
> > > 1. these are exist if the underlying
> distribution
> > is
> > > non-normal.
> > 
> > At least one of skewness!=0 or kurtosis!=3 exist
> if
> > the underlying 
> > distribution is non-normal.  Perhaps I don't
> > understand your first point?
> > 
> > If skewness=0 and kurtosis=3, the Cornish-Fisher
> > expansion does not 
> > change the Gaussian normal distribution.  So it
> > should have no adverse 
> > consequences if utilized even if all portfolio
> > assets were normal (which 
> > seems a highly unlikely circumstance).
> > 
> > > 2. your sample skewness and kurtosis is good
> > estimates
> > > of true skewness and hurtosis.
> > 
> > While it is possible to fit many different
> > fat-tailed distributions to 
> > the sample, and derive skewness and kurtosis from
> > these, I don't see how 
> > this is a better approach than utilizing the
> sample
> > skewness and 
> > kurtosis.  We did show in the paper how to test
> the
> > Cornish Fisher and 
> > Edgeworth expansion against a very skewed and
> > fat-tailed Skew Student-t 
> > distribution.
> > 
> > Another problem with utilizing a fitted
> distribution
> > is that many fitted 
> > distributions would  not carry the same properties
> > of being 
> > differentiable by the weight (properties of the
> > Gaussian normal and 
> > Cornish Fisher distributions) in a portfolio to
> > obtain a good estimator 
> > of Component Risk in a portfolio.
> > 
> > In the main, the data cleaning method is most
> > valuable for adding 
> > stability to the effects of the co-moments in
> > decomposing the risk to 
> > avoid undue influence by a small number of extreme
> > events.  The method 
> > was developed to specifically not change
> > observations that were not "in 
> > the tail", and to keep the direction (but not the
> > absolute magnitude) of 
> > the extreme events.  As I discussed in the text of
> > the paper, I do not 
> > believe that you would ever use the cleaning
> method
> > for measuring VaR or 
> > ES ex port, but only to stabilize the predictions
> of
> > contribution on a 
> > forward-looking ex ante basis.
> > 
> > > In part 5 you discussed the Robust estimation
> but
> > it
> > > could be stronger argument IMHO. For example, do
> > you
> > > have convergence/sensitivity analysis on
> estimated
> > > skewness/kurtosis results for your cleaning
> > method? 
> > 
> > I agree that a sensitivity analysis would be a
> good
> > addition.  I will 
> > start thinking about how to add that.
> > 
> > Regards,
> > 
> >    - Brian
> > 
> > 
> >  > --- "Brian G. Peterson" <brian at braverock.com>
> > wrote:
> >  >
> >  >> On Thursday 13 March 2008 22:32:59
> >  >> adschai at optonline.net wrote:
> >  >>> Hi,I'm looking for VAR allocation framework
> > among
> >  >> traders. I saw some
> >  >>> papers but none of which (at least that I
> saw)
> >  >> look practical. I am
> >  >>> wondering if anyone can hint me some idea or
> > some
> >  >> reference? The situation
> >  >>> is if at the desk level you were given a
> > certain
> >  >> amount of VAR limit, how
> >  >>> should one allocate the number among traders?
> >  >> Thank you.adschai
> >  >>
> >  >> Calculate Component VaR.
> >  >>
> >  >> The first definition (as far as I know) is in
> > Garman
> >  >> in Risk Magazine.  The
> >  >> article may be found here:
> >  >>
> >  >> Garman, Mark, "Taking VaR to Pieces (Component
> >  >> VaR)," RISK 10, 10, October
> >  >> 1997.
> >  >> http://www.fea.com/pdf/componentvar.pdf
> >  >>
> >  >> He also has a longer working paper on the
> topic
> >  >> here:
> >  >>
> >  >>
> >  >
> >
>
http://www.gloriamundi.org/detailpopup.asp?ID=453055537
> >  >> We implemented Component VaR for assets with
> >  >> non-normal distribution in our
> >  >> recent paper here:
> >  >>
> >  >> Boudt, Kris, Peterson, Brian G. and Croux,
> >  >> Christophe, "Estimation and
> >  >> Decomposition of Downside Risk for Portfolios
> > With
> >  >> Non-Normal Returns"
> >  >> (October 31, 2007).
> >  >> http://ssrn.com/abstract=1024151
> >  >>
> >  >> All code for our paper was implemented in R,
> and
> > is
> >  >> available.  We will also
> >  >> be cleaning up and documenting the functions
> in
> > the
> >  >> next version of
> >  >> PerformanceAnalytics.
> >  >>
> >  >> Regards,
> >  >>
> >  >>     - Brian
> >  >>
> >  >> _______
> > 
> 
> 
> 
>      
>
____________________________________________________________________________________


> 



      ____________________________________________________________________________________
Be a better friend, newshound, and


From brian at braverock.com  Mon Mar 17 15:48:51 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 17 Mar 2008 09:48:51 -0500
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
In-Reply-To: <77424.77986.qm@web31413.mail.mud.yahoo.com>
References: <77424.77986.qm@web31413.mail.mud.yahoo.com>
Message-ID: <47DE84D3.40908@braverock.com>

elton wang wrote:
> For example, if underlying is a t distribution with
> DOF=4, then kurtosis does not exsit. Any sample
> kurtosis (with any cleaning tech or not) would be a
> false stat of underlying distribution. 
> How can you rule out this possibility of underlying
> distribution?

I am not in general a believer that real returns are generated by 
mathematically ideal distributions.  Ideal distributions may make good 
estimators (thus the rationale for any parametric estimation method), 
but are rarely if ever the actual generator of returns.

You can't rule out the possibility that the "generating" distribution 
has different moments from the observed distribution.  You might be able 
to cast doubt on the observed moments by fitting various distributions, 
but I think that's all you could do with any confidence.

One of the reasons I prefer the Cornish-Fisher expansion over most other 
fitted distributions is precisely because it directly utilizes the 
observable moments of the distribution.  These moments have intuitive 
financial as well as mathematical meaning.  As I discussed below, if the 
magnitude of the observed higher moments are small, the effect on the 
estimate will also be small.

If you believe that you have a better estimator of the moments for your 
data than the observed moments (possibly via fitting to an ideal 
distribution) you can plug those into the Cornish Fisher expansion and 
still take advantage of the component decomposition offered via the CF 
process.

Again, thank you for your thoughtful commentary.

Regards,

   - Brian

> --- "Brian G. Peterson" <brian at braverock.com> wrote:
> 
>> elton wang wrote:
>>> Brian,
>>> I have a question on your paper:
>>> If you use skewness and kurtosis in the VaR
>>> calculation, you want to make sure:
>>  >
>>> 1. these are exist if the underlying distribution
>> is
>>> non-normal.
>> At least one of skewness!=0 or kurtosis!=3 exist if
>> the underlying 
>> distribution is non-normal.  Perhaps I don't
>> understand your first point?
>>
>> If skewness=0 and kurtosis=3, the Cornish-Fisher
>> expansion does not 
>> change the Gaussian normal distribution.  So it
>> should have no adverse 
>> consequences if utilized even if all portfolio
>> assets were normal (which 
>> seems a highly unlikely circumstance).
>>
>>> 2. your sample skewness and kurtosis is good
>> estimates
>>> of true skewness and hurtosis.
>> While it is possible to fit many different
>> fat-tailed distributions to 
>> the sample, and derive skewness and kurtosis from
>> these, I don't see how 
>> this is a better approach than utilizing the sample
>> skewness and 
>> kurtosis.  We did show in the paper how to test the
>> Cornish Fisher and 
>> Edgeworth expansion against a very skewed and
>> fat-tailed Skew Student-t 
>> distribution.
>>
>> Another problem with utilizing a fitted distribution
>> is that many fitted 
>> distributions would  not carry the same properties
>> of being 
>> differentiable by the weight (properties of the
>> Gaussian normal and 
>> Cornish Fisher distributions) in a portfolio to
>> obtain a good estimator 
>> of Component Risk in a portfolio.
>>
>> In the main, the data cleaning method is most
>> valuable for adding 
>> stability to the effects of the co-moments in
>> decomposing the risk to 
>> avoid undue influence by a small number of extreme
>> events.  The method 
>> was developed to specifically not change
>> observations that were not "in 
>> the tail", and to keep the direction (but not the
>> absolute magnitude) of 
>> the extreme events.  As I discussed in the text of
>> the paper, I do not 
>> believe that you would ever use the cleaning method
>> for measuring VaR or 
>> ES ex post, but only to stabilize the predictions of
>> contribution on a 
>> forward-looking ex ante basis.
>>
>>> In part 5 you discussed the Robust estimation but
>> it
>>> could be stronger argument IMHO. For example, do
>> you
>>> have convergence/sensitivity analysis on estimated
>>> skewness/kurtosis results for your cleaning
>> method? 
>>
>> I agree that a sensitivity analysis would be a good
>> addition.  I will 
>> start thinking about how to add that.
>>
>> Regards,
>>
>>    - Brian
>>
>>
>>  > --- "Brian G. Peterson" <brian at braverock.com>
>> wrote:
>>  >
>>  >> On Thursday 13 March 2008 22:32:59
>>  >> adschai at optonline.net wrote:
>>  >>> Hi,I'm looking for VAR allocation framework
>> among
>>  >> traders. I saw some
>>  >>> papers but none of which (at least that I saw)
>>  >> look practical. I am
>>  >>> wondering if anyone can hint me some idea or
>> some
>>  >> reference? The situation
>>  >>> is if at the desk level you were given a
>> certain
>>  >> amount of VAR limit, how
>>  >>> should one allocate the number among traders?
>>  >> Thank you.adschai
>>  >>
>>  >> Calculate Component VaR.
>>  >>
>>  >> The first definition (as far as I know) is in
>> Garman
>>  >> in Risk Magazine.  The
>>  >> article may be found here:
>>  >>
>>  >> Garman, Mark, "Taking VaR to Pieces (Component
>>  >> VaR)," RISK 10, 10, October
>>  >> 1997.
>>  >> http://www.fea.com/pdf/componentvar.pdf
>>  >>
>>  >> He also has a longer working paper on the topic
>>  >> here:
>>  >>
>>  >>
>>  >
>>
> http://www.gloriamundi.org/detailpopup.asp?ID=453055537
>>  >> We implemented Component VaR for assets with
>>  >> non-normal distribution in our
>>  >> recent paper here:
>>  >>
>>  >> Boudt, Kris, Peterson, Brian G. and Croux,
>>  >> Christophe, "Estimation and
>>  >> Decomposition of Downside Risk for Portfolios
>> With
>>  >> Non-Normal Returns"
>>  >> (October 31, 2007).
>>  >> http://ssrn.com/abstract=1024151
>>  >>
>>  >> All code for our paper was implemented in R, and
>> is
>>  >> available.  We will also
>>  >> be cleaning up and documenting the functions in
>> the
>>  >> next version of
>>  >> PerformanceAnalytics.
>>  >>
>>  >> Regards,
>>  >>
>>  >>     - Brian
>>  >>
>>  >> _______


From brian at braverock.com  Mon Mar 17 15:55:32 2008
From: brian at braverock.com (Brian G. Peterson)
Date: Mon, 17 Mar 2008 09:55:32 -0500
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
In-Reply-To: <136011.66476.qm@web31407.mail.mud.yahoo.com>
References: <136011.66476.qm@web31407.mail.mud.yahoo.com>
Message-ID: <47DE8664.6040605@braverock.com>

elton wang wrote:
> My point is,
> when underlying is non normal, any sample higher
> moments may highly sensitive to outliers; without a
> study of sample moments sensitity and converegence to
> outliers, you can not justify the quality of VaR
> modification.

I agree that the sample moments (and especially the co-moments) are 
highly sensitive to outliers.  This is why we developed a cleaning 
method to decrease the sensitivity.

I also agree that it could make sense to test for the sensistivity of 
the estimates of the moments.  I can envision how to test this using 
Baysian methods.  I would appreciate any insight you might have on other 
more computationally tractable sensitivity tests for skewness and kurtosis.

> you tested/simulated one skewed t distribution, but
> you can not rule out all other underlying distribution
> possibilities even within t-distribution with
> different DOF.

Many other papers have shown that the Cornish fisher expansion is a good 
estimator even when compared to multiple other ideal fitted 
distributions.  We did not feel the need to redo that work.

> These higher momonents mod on VaR are overdone IMHO.

That might be true if the data you are working with approximate the 
normal distribution, or are otherwise well-behaved.  When basically all 
the series that you look at are significantly non-normal, as is the case 
with hedge fund returns, then some method of accounting for that 
non-normality is also required.

Regards,

   - Brian

> --- elton wang <ahala2000 at yahoo.com> wrote:
> 
>> For example, if underlying is a t distribution with
>> DOF=4, then kurtosis does not exsit. Any sample
>> kurtosis (with any cleaning tech or not) would be a
>> false stat of underlying didstribution. 
>> How can you rule out this possibility of underlying
>> distribution?
>>
>> --- "Brian G. Peterson" <brian at braverock.com> wrote:
>>
>>> elton wang wrote:
>>>> Brian,
>>>> I have a question on your paper:
>>>> If you use skewness and kurtosis in the VaR
>>>> calculation, you want to make sure:
>>>  >
>>>> 1. these are exist if the underlying
>> distribution
>>> is
>>>> non-normal.
>>> At least one of skewness!=0 or kurtosis!=3 exist
>> if
>>> the underlying 
>>> distribution is non-normal.  Perhaps I don't
>>> understand your first point?
>>>
>>> If skewness=0 and kurtosis=3, the Cornish-Fisher
>>> expansion does not 
>>> change the Gaussian normal distribution.  So it
>>> should have no adverse 
>>> consequences if utilized even if all portfolio
>>> assets were normal (which 
>>> seems a highly unlikely circumstance).
>>>
>>>> 2. your sample skewness and kurtosis is good
>>> estimates
>>>> of true skewness and hurtosis.
>>> While it is possible to fit many different
>>> fat-tailed distributions to 
>>> the sample, and derive skewness and kurtosis from
>>> these, I don't see how 
>>> this is a better approach than utilizing the
>> sample
>>> skewness and 
>>> kurtosis.  We did show in the paper how to test
>> the
>>> Cornish Fisher and 
>>> Edgeworth expansion against a very skewed and
>>> fat-tailed Skew Student-t 
>>> distribution.
>>>
>>> Another problem with utilizing a fitted
>> distribution
>>> is that many fitted 
>>> distributions would  not carry the same properties
>>> of being 
>>> differentiable by the weight (properties of the
>>> Gaussian normal and 
>>> Cornish Fisher distributions) in a portfolio to
>>> obtain a good estimator 
>>> of Component Risk in a portfolio.
>>>
>>> In the main, the data cleaning method is most
>>> valuable for adding 
>>> stability to the effects of the co-moments in
>>> decomposing the risk to 
>>> avoid undue influence by a small number of extreme
>>> events.  The method 
>>> was developed to specifically not change
>>> observations that were not "in 
>>> the tail", and to keep the direction (but not the
>>> absolute magnitude) of 
>>> the extreme events.  As I discussed in the text of
>>> the paper, I do not 
>>> believe that you would ever use the cleaning
>> method
>>> for measuring VaR or 
>>> ES ex port, but only to stabilize the predictions
>> of
>>> contribution on a 
>>> forward-looking ex ante basis.
>>>
>>>> In part 5 you discussed the Robust estimation
>> but
>>> it
>>>> could be stronger argument IMHO. For example, do
>>> you
>>>> have convergence/sensitivity analysis on
>> estimated
>>>> skewness/kurtosis results for your cleaning
>>> method? 
>>>
>>> I agree that a sensitivity analysis would be a
>> good
>>> addition.  I will 
>>> start thinking about how to add that.
>>>
>>> Regards,
>>>
>>>    - Brian
>>>
>>>
>>>  > --- "Brian G. Peterson" <brian at braverock.com>
>>> wrote:
>>>  >
>>>  >> On Thursday 13 March 2008 22:32:59
>>>  >> adschai at optonline.net wrote:
>>>  >>> Hi,I'm looking for VAR allocation framework
>>> among
>>>  >> traders. I saw some
>>>  >>> papers but none of which (at least that I
>> saw)
>>>  >> look practical. I am
>>>  >>> wondering if anyone can hint me some idea or
>>> some
>>>  >> reference? The situation
>>>  >>> is if at the desk level you were given a
>>> certain
>>>  >> amount of VAR limit, how
>>>  >>> should one allocate the number among traders?
>>>  >> Thank you.adschai
>>>  >>
>>>  >> Calculate Component VaR.
>>>  >>
>>>  >> The first definition (as far as I know) is in
>>> Garman
>>>  >> in Risk Magazine.  The
>>>  >> article may be found here:
>>>  >>
>>>  >> Garman, Mark, "Taking VaR to Pieces (Component
>>>  >> VaR)," RISK 10, 10, October
>>>  >> 1997.
>>>  >> http://www.fea.com/pdf/componentvar.pdf
>>>  >>
>>>  >> He also has a longer working paper on the
>> topic
>>>  >> here:
>>>  >>
>>>  >>
>>>  >
>>>
> http://www.gloriamundi.org/detailpopup.asp?ID=453055537
>>>  >> We implemented Component VaR for assets with
>>>  >> non-normal distribution in our
>>>  >> recent paper here:
>>>  >>
>>>  >> Boudt, Kris, Peterson, Brian G. and Croux,
>>>  >> Christophe, "Estimation and
>>>  >> Decomposition of Downside Risk for Portfolios
>>> With
>>>  >> Non-Normal Returns"
>>>  >> (October 31, 2007).
>>>  >> http://ssrn.com/abstract=1024151
>>>  >>
>>>  >> All code for our paper was implemented in R,
>> and
>>> is
>>>  >> available.  We will also
>>>  >> be cleaning up and documenting the functions
>> in
>>> the
>>>  >> next version of
>>>  >> PerformanceAnalytics.
>>>  >>
>>>  >> Regards,
>>>  >>
>>>  >>     - Brian
>>>  >>
>>>  >> _______
>>>
>>
>>
>>      
>>
> ____________________________________________________________________________________
>> Never miss a thing.  Make Yahoo your home page. 
>> http://www.yahoo.com/r/hs
>>
> 
> 
> 
>       ____________________________________________________________________________________
> Be a better friend, newshound, and 
> know-it-all with Yahoo! Mobile.  Try it now.  http://mobile.yahoo.com/;_ylt=Ahu06i62sR8HDtDypao8Wcj9tAcJ


From anass.mouhsine at sgcib.com  Mon Mar 17 16:13:10 2008
From: anass.mouhsine at sgcib.com (anass.mouhsine at sgcib.com)
Date: Mon, 17 Mar 2008 16:13:10 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by tick
	timeSeries [NC]
In-Reply-To: <e8e755250803050721q7ff85c83s2b6fc69926684bbf@mail.gmail.com>
Message-ID: <OFC4DDBF44.DB9E475E-ONC125740F.00521F82-C125740F.00539A31@fr.world.socgen>

Hi Jeff,

I used the xts package as you advised.
I have a timeSeries object called test

>test[1000:1005,]
                                     V10  V7
2005-04-08 10:55:16 0.3202287 448
2005-04-08 10:55:16 0.3945675 552
2005-04-08 10:55:16 0.0000000   0
2005-04-08 10:55:18 0.0000000   0
2005-04-08 10:55:30 0.0000000   0
2005-04-08 10:55:35 0.0000000   0

the first column stand for the spread*volume and the second one for the
traded volume.
I want to calculate the Volume Weighted Spread hourly.

>test.xts=as.xts(test)

>ep<-endpoints(test.xts,'hours')

>VWAS<-period.apply(test.xts[,1],ep,sum)/period.apply(test.xts[,2],ep,sum)

>VWAS
[1] 0.0006517149 0.0011587963 0.0008660431 0.0007342648 0.0010022736
[6] 0.0009949343 0.0009114276 0.0009170628 0.0008334971

Here I have what I want but need to improve the result:
1) I do not know how to get back to a timeSeries object using reclass() and
have a timeDate object as an index.
2) Is it possible to do the same calculation from a timeSeries object of
two columns (spread,volume)  whithout calculating a column spread* volume
and doing two univariate sums.

Could you give some hints on the subject?

regards,
Anass


|------------------------------                                            
|            jeff.a.ryan at gmail                                             
|            .com                                                          
|                                                                          
|            03/05/2008 16:21                                              
|                                                                          
                                                                           
                                                                           
                                                                        To 
                                               Anass                       
                                               MOUHSINE/fr/socgen at socgen   
                                                                        cc 
                                               r-sig-finance at stat.math.eth 
                                               z.ch                        
                                                                   Subject 
                                               Re: [R-SIG-Finance]         
                                               [R-sig-finance] Aggregating 
                                               tick by tick timeSeries     
                                               [C1]                        
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Anass,

If you want the mean/sd/etc for each hour ?aggregate from zoo will do
the job very well. To get there from timeSeries you can use the new
'xts' package:

myTS [a timeSeries object]

as.xts(myTS) [ now an 'xts' object - which inherits from zoo]

--or--

as.zoo(myTS)

you can even put it back to a timeSeries with little information loss
if you use xts via 'reclass' (changing the series outside of xts
functions currently isn't perfect - but it is good)

If you want to just aggregate the data into OHLC per time period -
to.period in xts is fast and flexible:

to.period(myTS,'minutes',15)
to.period(myTS,'hours')
to.monthly(myTS)

All these work by converting and reconverting to an 'xts' class
internally - so most any (including timeSeries) class will just work
with the function.  In addition it is all compiled code - so it works
well.

One caveat - you'll need to get the most recent xts from
http://r-forge.r-project.org/projects/xts - as there was a minor bug
in the transition of 'to.period' code from quantmod to xts.

Jeff

On Wed, Mar 5, 2008 at 8:21 AM,  <anass.mouhsine at sgcib.com> wrote:
>
>  Hi guys,
>
>
>  I have a timeSeries object like this one
>                         V10
>  2005-04-08 17:31:41 0.01
>  2005-04-08 17:31:57 0.02
>  2005-04-08 17:32:00 0.02
>  2005-04-08 17:32:57 0.02
>  2005-04-08 17:38:34 0.02
>  2005-04-08 17:38:49 0.01
>
>  and I would like to aggregate the timeSeries in hours or minutes in
order
>  to apply whatever function on the aggregated data (e.g mean, standard
dev,
>  etc...)
>  I have seen some aggreagation functions like aggregate in the fSeries
>  package but it aggregates only monthly or quaterly.
>  I am sure some of you guys were faced to this kind of issue.
>
>  Could anyone give me some hints on how to solve my problem?
>
>  thanks in advance
>
>  Anass
>
*************************************************************************
>  This message and any attachments (the "message") are
con...{{dropped:10}}
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



--
There's a way to do it better - find it.
Thomas A. Edison

*************************************************************************
This message and any attachments (the "message") are con...{{dropped:10}}


From ahala2000 at yahoo.com  Mon Mar 17 22:25:20 2008
From: ahala2000 at yahoo.com (elton wang)
Date: Mon, 17 Mar 2008 14:25:20 -0700 (PDT)
Subject: [R-SIG-Finance] Framework for VAR allocation among traders
In-Reply-To: <47DE84D3.40908@braverock.com>
Message-ID: <287405.64287.qm@web31401.mail.mud.yahoo.com>

Brian,
In case I have doubt with observed moments, I will not
using these numbers on VaR modification; a simple non
paremetric distriubtion will catch the tail risk in a
more reliable way.

it is not the  case "> but I think that's all you
could do with any confidence." No one forces us to use
these higher moments even admitting non normal tail
risks, we have other ways.

The example of t distribution with DOF=4 is not
arguing one or another particular ideal distrubtion,
it is to show that it is totally possible that
underlying kurtosis does not converge/exist at all.
when you cleaning the data, the number change; when
you get more data, the number change.





--- "Brian G. Peterson" <brian at braverock.com> wrote:

> elton wang wrote:
> > For example, if underlying is a t distribution
> with
> > DOF=4, then kurtosis does not exsit. Any sample
> > kurtosis (with any cleaning tech or not) would be
> a
> > false stat of underlying distribution. 
> > How can you rule out this possibility of
> underlying
> > distribution?
> 
> I am not in general a believer that real returns are
> generated by 
> mathematically ideal distributions.  Ideal
> distributions may make good 
> estimators (thus the rationale for any parametric
> estimation method), 
> but are rarely if ever the actual generator of
> returns.
> 
> You can't rule out the possibility that the
> "generating" distribution 
> has different moments from the observed
> distribution.  You might be able 
> to cast doubt on the observed moments by fitting
> various distributions, 
> but I think that's all you could do with any
> confidence.
> 
> One of the reasons I prefer the Cornish-Fisher
> expansion over most other 
> fitted distributions is precisely because it
> directly utilizes the 
> observable moments of the distribution.  These
> moments have intuitive 
> financial as well as mathematical meaning.  As I
> discussed below, if the 
> magnitude of the observed higher moments are small,
> the effect on the 
> estimate will also be small.
> 
> If you believe that you have a better estimator of
> the moments for your 
> data than the observed moments (possibly via fitting
> to an ideal 
> distribution) you can plug those into the Cornish
> Fisher expansion and 
> still take advantage of the component decomposition
> offered via the CF 
> process.
> 
> Again, thank you for your thoughtful commentary.
> 
> Regards,
> 
>    - Brian
> 
> > --- "Brian G. Peterson" <brian at braverock.com>
> wrote:
> > 
> >> elton wang wrote:
> >>> Brian,
> >>> I have a question on your paper:
> >>> If you use skewness and kurtosis in the VaR
> >>> calculation, you want to make sure:
> >>  >
> >>> 1. these are exist if the underlying
> distribution
> >> is
> >>> non-normal.
> >> At least one of skewness!=0 or kurtosis!=3 exist
> if
> >> the underlying 
> >> distribution is non-normal.  Perhaps I don't
> >> understand your first point?
> >>
> >> If skewness=0 and kurtosis=3, the Cornish-Fisher
> >> expansion does not 
> >> change the Gaussian normal distribution.  So it
> >> should have no adverse 
> >> consequences if utilized even if all portfolio
> >> assets were normal (which 
> >> seems a highly unlikely circumstance).
> >>
> >>> 2. your sample skewness and kurtosis is good
> >> estimates
> >>> of true skewness and hurtosis.
> >> While it is possible to fit many different
> >> fat-tailed distributions to 
> >> the sample, and derive skewness and kurtosis from
> >> these, I don't see how 
> >> this is a better approach than utilizing the
> sample
> >> skewness and 
> >> kurtosis.  We did show in the paper how to test
> the
> >> Cornish Fisher and 
> >> Edgeworth expansion against a very skewed and
> >> fat-tailed Skew Student-t 
> >> distribution.
> >>
> >> Another problem with utilizing a fitted
> distribution
> >> is that many fitted 
> >> distributions would  not carry the same
> properties
> >> of being 
> >> differentiable by the weight (properties of the
> >> Gaussian normal and 
> >> Cornish Fisher distributions) in a portfolio to
> >> obtain a good estimator 
> >> of Component Risk in a portfolio.
> >>
> >> In the main, the data cleaning method is most
> >> valuable for adding 
> >> stability to the effects of the co-moments in
> >> decomposing the risk to 
> >> avoid undue influence by a small number of
> extreme
> >> events.  The method 
> >> was developed to specifically not change
> >> observations that were not "in 
> >> the tail", and to keep the direction (but not the
> >> absolute magnitude) of 
> >> the extreme events.  As I discussed in the text
> of
> >> the paper, I do not 
> >> believe that you would ever use the cleaning
> method
> >> for measuring VaR or 
> >> ES ex post, but only to stabilize the predictions
> of
> >> contribution on a 
> >> forward-looking ex ante basis.
> >>
> >>> In part 5 you discussed the Robust estimation
> but
> >> it
> >>> could be stronger argument IMHO. For example, do
> >> you
> >>> have convergence/sensitivity analysis on
> estimated
> >>> skewness/kurtosis results for your cleaning
> >> method? 
> >>
> >> I agree that a sensitivity analysis would be a
> good
> >> addition.  I will 
> >> start thinking about how to add that.
> >>
> >> Regards,
> >>
> >>    - Brian
> >>
> >>
> >>  > --- "Brian G. Peterson" <brian at braverock.com>
> >> wrote:
> >>  >
> >>  >> On Thursday 13 March 2008 22:32:59
> >>  >> adschai at optonline.net wrote:
> >>  >>> Hi,I'm looking for VAR allocation framework
> >> among
> >>  >> traders. I saw some
> >>  >>> papers but none of which (at least that I
> saw)
> >>  >> look practical. I am
> >>  >>> wondering if anyone can hint me some idea or
> >> some
> >>  >> reference? The situation
> >>  >>> is if at the desk level you were given a
> >> certain
> >>  >> amount of VAR limit, how
> >>  >>> should one allocate the number among
> traders?
> >>  >> Thank you.adschai
> >>  >>
> >>  >> Calculate Component VaR.
> >>  >>
> >>  >> The first definition (as far as I know) is in
> >> Garman
> >>  >> in Risk Magazine.  The
> >>  >> article may be found here:
> >>  >>
> >>  >> Garman, Mark, "Taking VaR to Pieces
> (Component
> >>  >> VaR)," RISK 10, 10, October
> >>  >> 1997.
> >>  >> http://www.fea.com/pdf/componentvar.pdf
> >>  >>
> >>  >> He also has a longer working paper on the
> topic
> >>  >> here:
> >>  >>
> >>  >>
> >>  >
> >>
> >
>
http://www.gloriamundi.org/detailpopup.asp?ID=453055537
> >>  >> We implemented Component VaR for assets with
> >>  >> non-normal distribution in our
> >>  >> recent paper here:
> 
=== message truncated ===



      ____________________________________________________________________________________
Be a better friend, newshound, and


From lim at umail.ucsb.edu  Mon Mar 17 22:37:04 2008
From: lim at umail.ucsb.edu (Bryan Lim)
Date: Mon, 17 Mar 2008 14:37:04 -0700
Subject: [R-SIG-Finance] Rolling Windows / Regressions / Predictions
Message-ID: <47DEE480.8050704@umail.ucsb.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080317/c14014d8/attachment.pl 

From peter at braverock.com  Mon Mar 17 23:02:56 2008
From: peter at braverock.com (Peter Carl)
Date: Mon, 17 Mar 2008 17:02:56 -0500
Subject: [R-SIG-Finance] Rolling Windows / Regressions / Predictions
In-Reply-To: <47DEE480.8050704@umail.ucsb.edu>
References: <47DEE480.8050704@umail.ucsb.edu>
Message-ID: <1205791376.27040.46.camel@localhost>


On Mon, 2008-03-17 at 14:37 -0700, Bryan Lim wrote:
> i'm basically trying to solve for excess returns 
> (ie, ALPHA) based on rolling betas.

Take a look at the PerformanceAnalytics package to see if it does what
you're looking for:

http://braverock.com/brian/R/PerformanceAnalytics/html/PerformanceAnalytics-package.html

It's available on CRAN and can be installed using install.packages().

The function chart.RollingRegression displays the coefficients of a
linear model fitted over rolling periods.  I'd be happy to confer with
you about adding n-step ahead predictions.

pcc
-- 
Peter Carl <peter at braverock.com>
http://www.braverock.com/~peter


From chalabi at phys.ethz.ch  Tue Mar 18 08:14:27 2008
From: chalabi at phys.ethz.ch (chalabi at phys.ethz.ch)
Date: Tue, 18 Mar 2008 08:14:27 +0100
Subject: [R-SIG-Finance] Rmetrics - R-Forge - Workshop
Message-ID: <87hcf4a4h8.fsf@phys.ethz.ch>


    Dear Members of the R-Core Team,
    Rmetrics Developers, and Rmetrics
    Users ...
    
    The repository of the development version 
    of the Rmetrics software environment has 
    been moved to R-forge.
    
    The new R-Forge framework for R-project 
    developers based on GForge offers us easy 
    access to SVN, daily built and checked 
    packages, mailing lists, bug tracking, 
    message boards/forums, site hosting, a
    permanent file archival, and full backups.
    Thanks to the people who brought it up.
    
    Up to now the Rmetrics repository was
    hosted by the statistics institute at ETH
    Zurich with the generous help of Martin 
    Maechler. He also helped us to move from
    Zurich to Vienna. Thanks to him for all
    his support and help during the last years.
    
    The responsibility for the new repository 
    system administration will be taken by 
    Yohan Chalabi. If you have any questions,
    please do not hesitate to get in contact
    with him.
    
    Furthermore, Rmetrics will move this week
    to his new own home, the old link via 
    "www.itp.phys.ethz.ch" will no longer be 
    active due to the reorganisation of the
    ITP web server at ETH Zurich. Please use 
    directly "www.rmetrics.org".
    
    Please note, the next workshop for R/Rmetrics 
    developers and users
    
    Computational Finance and Financial Engineering
    R/Rmetrics User and Developer Workshop 2008
    
    will be held at Meielisalp, Lake Thune, 
    Switzerland, June 29th to July 3rd. For 
    further information have a look on the
    announcements at "www.rmetrics.org". We 
    would be very appreciated to welcome you
    at the Meielisalp.
    
    
    Best regards
    Diethelm and Yohan

-- 


The 2nd International R/Rmetrics User and Developer Workshop ...
[http://www.rmetrics.org]


From reinhold.hafner at risklab.de  Tue Mar 18 17:34:44 2008
From: reinhold.hafner at risklab.de (reini)
Date: Tue, 18 Mar 2008 09:34:44 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Specification of Mu and Sigma in
	fportfolio
Message-ID: <16125122.post@talk.nabble.com>


Dear all,
In using fportfolio I would like to directly specify a mu and sigma rather
than to hand over a multivariate time series from which mu and sigma are
estimated by the relevant functions. How does this work? From the
documentation it seemed possible to me, however, when I investigated the
relevant code it is always asking for a time series. 
Many thanks,
Reinhold 
-- 
View this message in context: http://www.nabble.com/Specification-of-Mu-and-Sigma-in-fportfolio-tp16125122p16125122.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From spencer.graves at pdf.com  Tue Mar 18 21:55:06 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 18 Mar 2008 13:55:06 -0700
Subject: [R-SIG-Finance] useR! 2008 submission deadline
Message-ID: <47E02C2A.2050102@pdf.com>

Dear All:  

	  I'd like to relay a reminder from the useR!2008 Program Chair, Achim Zeileis, that the deadline for abstract submission is approaching:

  Submission deadline: 2008-03-31

	  This is also the early registration deadline. Both online
submission and registration are available on the conference Web page at

  http://www.R-project.org/useR-2008/

	  Please consider submitting an abstract and present your work with R at the
conference. We hope to see many of you in Dortmund this August.

For the program committee, 
Spencer Graves


From MichelBeck at sbcglobal.Net  Tue Mar 18 23:06:46 2008
From: MichelBeck at sbcglobal.Net (MAB)
Date: Tue, 18 Mar 2008 22:06:46 +0000 (UTC)
Subject: [R-SIG-Finance] Calling R from Mathlab
Message-ID: <loom.20080318T220230-848@post.gmane.org>

Can R be called from Mathlab?

I did come across mention of a tool at

http://www.omegahat.org/RMatlab/

but I do not know how developed it is and it is described as only working 
under Unix currently.


From spencer.graves at pdf.com  Wed Mar 19 01:26:48 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 18 Mar 2008 17:26:48 -0700
Subject: [R-SIG-Finance] Calling R from Mathlab
In-Reply-To: <loom.20080318T220230-848@post.gmane.org>
References: <loom.20080318T220230-848@post.gmane.org>
Message-ID: <47E05DC8.8050609@pdf.com>

      I know nothing about the "omegahat.org/RMatlab" package, except 
that (a) I just downloaded the *.tar.gz file and found that the most 
recent date seems to be Feb. 4, 2007, and (b) Duncan Temple Lang, whose 
name appears at the bottom of the web site you mentioned, is a regular 
contributor to this list. 

      Have you tried the "R.matlab" package available from CRAN?  If no, 
you might try that.  I've used it to run Matlab from R.  If you try it 
and have problems, try "setverbose(..., -100)".  If you still have 
problems, try this list and / or the maintainer and / or 
"omegahat.org/RMatlab". 

      Hope this helps. 
      Spencer     

MAB wrote:
> Can R be called from Mathlab?
>
> I did come across mention of a tool at
>
> http://www.omegahat.org/RMatlab/
>
> but I do not know how developed it is and it is described as only working 
> under Unix currently.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>


From landronimirc at gmail.com  Thu Mar 20 16:59:26 2008
From: landronimirc at gmail.com (Liviu Andronic)
Date: Thu, 20 Mar 2008 16:59:26 +0100
Subject: [R-SIG-Finance] fBrowser in Rmetrics - does it exist?
	[r-sig-finance]
Message-ID: <68b1e2610803200859m1402387fw44ae9955a78ae25f@mail.gmail.com>

Dear useRs,

I have tried my luck on R-help, with no luck. So, here I go:

I have read in several sources [1] [2] [3] that Rmetrics includes an
Rcmdr-based graphical user interface named fBrowser GUI. However, I
have a hard time finding it anywhere (on the net, etc.), nor do I
manage to load it (having had Rmetrics installed).

Could anyone shed some light on this? Does fBrowser exist, and if it
does, how should one install/use it?

Thank you in advance,
Liviu

[1] http://zoonek.free.fr/blosxom/R/2006-06-22_useR2006.html
(2.Tutorial: Rmetrics)
[2] http://www.r-project.org/useR-2006/Tutorials/Wuertz.html
[3] http://zoonek2.free.fr/UNIX/48_R/01.html#4


From anass.mouhsine at sgcib.com  Fri Mar 21 13:53:39 2008
From: anass.mouhsine at sgcib.com (anass.mouhsine at sgcib.com)
Date: Fri, 21 Mar 2008 13:53:39 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by tick
	timeSeries [C1]
Message-ID: <OF08E38020.7FB30155-ONC1257413.0046C7E2-C1257413.0046D481@fr.world.socgen>



Hi Jeff,

I used the xts package as you advised.
I have a timeSeries object called test

>test[1000:1005,]
                                     V10  V7
2005-04-08 10:55:16 0.3202287 448
2005-04-08 10:55:16 0.3945675 552
2005-04-08 10:55:16 0.0000000   0
2005-04-08 10:55:18 0.0000000   0
2005-04-08 10:55:30 0.0000000   0
2005-04-08 10:55:35 0.0000000   0

the first column stand for the spread*volume and the second one for the
traded volume.
I want to calculate the Volume Weighted Spread hourly.

>test.xts=as.xts(test)

>ep<-endpoints(test.xts,'hours')

>VWAS<-period.apply(test.xts[,1],ep,sum)/period.apply(test.xts[,2],ep,sum)

>VWAS
[1] 0.0006517149 0.0011587963 0.0008660431 0.0007342648 0.0010022736
[6] 0.0009949343 0.0009114276 0.0009170628 0.0008334971

Here I have what I want but need to improve the result:
1) I do not know how to get back to a timeSeries object using reclass() and
have a timeDate object as an index.
2) Is it possible to do the same calculation from a timeSeries object of
two columns (spread,volume)  whithout calculating a column spread* volume
and doing two univariate sums.

Could you give some hints on the subject?

regards,
Anass


|------------------------------                                            
|            jeff.a.ryan at gmail                                             
|            .com                                                          
|                                                                          
|            03/05/2008 16:21                                              
|                                                                          
                                                                           
                                                                           
                                                                        To 
                                               Anass                       
                                               MOUHSINE/fr/socgen at socgen   
                                                                        cc 
                                               r-sig-finance at stat.math.eth 
                                               z.ch                        
                                                                   Subject 
                                               Re: [R-SIG-Finance]         
                                               [R-sig-finance] Aggregating 
                                               tick by tick timeSeries     
                                               [C1]                        
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Anass,

If you want the mean/sd/etc for each hour ?aggregate from zoo will do
the job very well. To get there from timeSeries you can use the new
'xts' package:

myTS [a timeSeries object]

as.xts(myTS) [ now an 'xts' object - which inherits from zoo]

--or--

as.zoo(myTS)

you can even put it back to a timeSeries with little information loss
if you use xts via 'reclass' (changing the series outside of xts
functions currently isn't perfect - but it is good)

If you want to just aggregate the data into OHLC per time period -
to.period in xts is fast and flexible:

to.period(myTS,'minutes',15)
to.period(myTS,'hours')
to.monthly(myTS)

All these work by converting and reconverting to an 'xts' class
internally - so most any (including timeSeries) class will just work
with the function.  In addition it is all compiled code - so it works
well.

One caveat - you'll need to get the most recent xts from
http://r-forge.r-project.org/projects/xts - as there was a minor bug
in the transition of 'to.period' code from quantmod to xts.

Jeff

On Wed, Mar 5, 2008 at 8:21 AM,  <anass.mouhsine at sgcib.com> wrote:
>
>  Hi guys,
>
>
>  I have a timeSeries object like this one
>                         V10
>  2005-04-08 17:31:41 0.01
>  2005-04-08 17:31:57 0.02
>  2005-04-08 17:32:00 0.02
>  2005-04-08 17:32:57 0.02
>  2005-04-08 17:38:34 0.02
>  2005-04-08 17:38:49 0.01
>
>  and I would like to aggregate the timeSeries in hours or minutes in
order
>  to apply whatever function on the aggregated data (e.g mean, standard
dev,
>  etc...)
>  I have seen some aggreagation functions like aggregate in the fSeries
>  package but it aggregates only monthly or quaterly.
>  I am sure some of you guys were faced to this kind of issue.
>
>  Could anyone give me some hints on how to solve my problem?
>
>  thanks in advance
>
>  Anass
>
*************************************************************************
>  This message and any attachments (the "message") are
con...{{dropped:10}}
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



--
There's a way to do it better - find it.
Thomas A. Edison



*************************************************************************
This message and any attachments (the "message") are con...{{dropped:10}}


From jeff.a.ryan at gmail.com  Fri Mar 21 16:53:13 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Fri, 21 Mar 2008 10:53:13 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by tick
	timeSeries [C1]
In-Reply-To: <OF08E38020.7FB30155-ONC1257413.0046C7E2-C1257413.0046D481@fr.world.socgen>
References: <OF08E38020.7FB30155-ONC1257413.0046C7E2-C1257413.0046D481@fr.world.socgen>
Message-ID: <e8e755250803210853m27f3fa0x9dc30c017170479e@mail.gmail.com>

Hi Anass,

My apologies for the delayed response, have been extra short on time
the last few weeks.

You can't get back to a timeSeries object using reclass() because
you've created an entirely new object - which is no longer of class
`xts`.  reclass() provides
class-conversion of an `xts` object only.  This should be possible in
the future though, once the period.apply family of functions are fully
incorporated into xts -- but it isn't now.

You didn't supply much data, but I think these randomly generated
numbers should provide a sufficient example of how to create a *new*
timeSeries object.

> test.xts <- xts(matrix(c(runif(3400),floor(runif(3400,200,500))),nc=2),(as.POSIXct('2005-04-08 10:55:16') + 1:34000))

> ep <- endpoints(test.xts,'hours')

> VWAS <- period.apply(test.xts[,1],ep,sum)/period.apply(test.xts[,2],ep,sum)

> timeSeries(VWAS,index(test.xts)[ep[-1]])
                              TS.1
2005-04-08 10:59:59 0.001369137492
2005-04-08 11:59:59 0.001432987385
2005-04-08 12:59:59 0.001427308452
2005-04-08 13:59:59 0.001428589601
2005-04-08 14:59:59 0.001430663890
2005-04-08 15:59:59 0.001425880024
2005-04-08 16:59:59 0.001423586660
2005-04-08 17:59:59 0.001427435076
2005-04-08 18:59:59 0.001434531036
2005-04-08 19:59:59 0.001433269336
2005-04-08 20:21:56 0.001420547518
>

an alternate approach which maintains the underlying zoo structure
(this moved from quantmod, and hasn't been updated to make use of xts)
AND is 5x faster than the pure R period.apply...

> VWAS2 <- xts:::period.sum(test.xts[,1],ep)/xts:::period.sum(test.xts[,2],ep)
> as.timeSeries(VWAS2)
                                 x
2005-04-08 10:59:59 0.001369137492
2005-04-08 11:59:59 0.001432987385
2005-04-08 12:59:59 0.001427308452
2005-04-08 13:59:59 0.001428589601
2005-04-08 14:59:59 0.001430663890
2005-04-08 15:59:59 0.001425880024
2005-04-08 16:59:59 0.001423586660
2005-04-08 17:59:59 0.001427435076
2005-04-08 18:59:59 0.001434531036
2005-04-08 19:59:59 0.001433269336
2005-04-08 20:21:56 0.001420547518

timeSeries by its nature stores all 'index' values for the date/time
as a character string, so the timeDate properties will persist if you
need those - as that is a separate conversion. (thanks to Yohan from
Rmetrics for pointing that out to me.)

As far as a more direct way to do the calculation - I don't think
there is.  I am sure there are many other ways to handle this though.

Jeff (with input from Josh)


From wuertz at itp.phys.ethz.ch  Fri Mar 21 19:51:41 2008
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Fri, 21 Mar 2008 19:51:41 +0100
Subject: [R-SIG-Finance] R/Rmetrics Workshop, Meielisalp 2008,
	June 29 - July 3
Message-ID: <47E403BD.3040807@itp.phys.ethz.ch>


** R/RMETRICS MEIELISALP WORKSHOP 2008 **

Computational Finance and Financial Engineering
R/Rmetrics User and Developer Workshop 2008
Lake Thune, Switzerland, June 29th to July 3rd


        **** www.rmetrics.org ****


The great interest in the R/Rmetrics software
environment and the success of the first
Rmetrics user and developer workshop on
"Computational Finance and Financial
Engineering" have encouraged us to organize
the Second Rmetrics Workshop in 2008,
June 29th - July 3rd, at Meielisalp, Lake Thune,
Switzerland.

We cordially invite you to participate and
contribute in this summer school-type workshop.

The Workshop focuses on using R/Rmetrics as
the premier open source solution for financial
market analysis, valuation of financial
instruments, and insurance tasks. We provide
a platform for R/Rmetrics users to discuss
and exchange ideas how R and Rmetrics can be
used for computations, data analysis, and
visualization in finance and insurance. We
give an overview of the new features of the
rapidly evolving R/Rmetrics project and discuss
future developments.

The Workshop is created by R/Rmetrics users and
developers for R/Rmetrics users. The program
consists of presentations of new R/Rmetrics
directions and developments through keynote
lectures and user-contributed presentations
reflecting the wide range of fields in which
R and Rmetrics are used in finance and insurance
to analyze and model data. The aim is to bring
together developers, practitioners, and users
from finance and insurance providing a platform
for common discussions and exchange of ideas.

On behalf of the organizing committee of the
Workshop and the Rmetrics Core Team,

Kurt Hornik
David Scott
Diethelm Wuertz


From dlincke at lincke.net  Sat Mar 22 23:37:57 2008
From: dlincke at lincke.net (David-Michael Lincke)
Date: Sat, 22 Mar 2008 18:37:57 -0400
Subject: [R-SIG-Finance] R package for continuous futures contract
	construction
Message-ID: <199201c88c6d$62222d30$2901a8c0@lincke.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080322/68d70d61/attachment.pl 

From armstrong.whit at gmail.com  Sun Mar 23 18:33:39 2008
From: armstrong.whit at gmail.com (Whit Armstrong)
Date: Sun, 23 Mar 2008 13:33:39 -0400
Subject: [R-SIG-Finance] R package for continuous futures contract
	construction
In-Reply-To: <199201c88c6d$62222d30$2901a8c0@lincke.net>
References: <199201c88c6d$62222d30$2901a8c0@lincke.net>
Message-ID: <8ec76080803231033g404905d0u9efe287fbd915ccd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080323/2a0b2ba0/attachment.pl 

From ravis at ambaresearch.com  Mon Mar 24 12:26:01 2008
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Mon, 24 Mar 2008 16:56:01 +0530
Subject: [R-SIG-Finance] Monthly returns from Daily prices
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3033BECBD@BAN-MAILSRV03.Amba.com>

Hi,

I have the daily data in the following format

	RIC 	Trade.Date 	Close.Price Currency.Code
 ABCd.xx	2008/02/29        15.3           CNY
 ABCd.xx	2008/02/28        15.1           CNY
 ABCd.xx	2008/02/27       15.28           CNY
 ABCd.xx	2008/02/26       15.26           CNY
 ABCd.xx	2008/02/25       14.88           CNY
 ABCd.xx	2008/02/22       15.64           CNY


I have about 1000 rics and one year daily price history for each of the
rics. I need to compute the monthly returns for this data.

I tried the following code to get the monthly data from the daily prices
library(zoo)
ss=read.csv("C:\\Documents and
Settings\\ravis\\Desktop\\ravi.csv",header=T)
ss$Trade.Date=as.Date(ss$Trade.Date,"%Y/%m/%d")
ss$ym=as.yearmon(ss$Trade.Date)
ss.mon=aggregate(ss$Close.Price,list(RIC=ss$RIC,Month_Year=ss$ym),functi
on(x) tail(x,1))

I am interested in an alternate way of doing this.
Any help would be appreciated.

Thank you,
Regards,

Ravi Shankar S 
 

This e-mail may contain confidential and/or privileged i...{{dropped:10}}


From dlincke at lincke.net  Mon Mar 24 12:48:43 2008
From: dlincke at lincke.net (David-Michael Lincke)
Date: Mon, 24 Mar 2008 07:48:43 -0400
Subject: [R-SIG-Finance] R package for continuous futures contract
	construction
In-Reply-To: <8ec76080803231529i6755c97coeb69a37e4230832f@mail.gmail.com>
References: <16456795.2816491206301438737.JavaMail.servlet@perfora>
	<8ec76080803231529i6755c97coeb69a37e4230832f@mail.gmail.com>
Message-ID: <1abb01c88da5$06017c20$2901a8c0@lincke.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080324/ad232f1f/attachment.pl 

From ryan.sheftel at malbecpartners.com  Mon Mar 24 13:02:10 2008
From: ryan.sheftel at malbecpartners.com (Ryan Sheftel)
Date: Mon, 24 Mar 2008 08:02:10 -0400
Subject: [R-SIG-Finance] R package for continuous futures
	contract	construction
In-Reply-To: <1abb01c88da5$06017c20$2901a8c0@lincke.net>
Message-ID: <OFCBB3E072.3264FEAC-ON85257416.0041E11B-85257416.00422300@fftw.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080324/6c8074a2/attachment.pl 

From jeff.a.ryan at gmail.com  Mon Mar 24 13:02:20 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Mon, 24 Mar 2008 07:02:20 -0500
Subject: [R-SIG-Finance] Monthly returns from Daily prices
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3033BECBD@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3033BECBD@BAN-MAILSRV03.Amba.com>
Message-ID: <e8e755250803240502l3994354at15e15ab448a78a10@mail.gmail.com>

Hi Ravi,

I can't really reproduce your data, but once you get the date
converted the following should work.  I've used Starbucks daily data
as an example (and because I need more coffee...)

library(quantmod)
getSymbols("SBUX", src='yahoo')

Delt(Cl(to.monthly(SBUX)),type='arithmetic')
Delt(Cl(to.monthly(SBUX)),type='log')

# this will compute the discrete (arithmetic) return as well (though
it is slower):

monthlyReturn(Cl(SBUX))

Jeff

On Mon, Mar 24, 2008 at 6:26 AM, Ravi S. Shankar <ravis at ambaresearch.com> wrote:
> Hi,
>
>  I have the daily data in the following format
>
>         RIC     Trade.Date      Close.Price Currency.Code
>   ABCd.xx        2008/02/29        15.3           CNY
>   ABCd.xx        2008/02/28        15.1           CNY
>   ABCd.xx        2008/02/27       15.28           CNY
>   ABCd.xx        2008/02/26       15.26           CNY
>   ABCd.xx        2008/02/25       14.88           CNY
>   ABCd.xx        2008/02/22       15.64           CNY
>
>
>  I have about 1000 rics and one year daily price history for each of the
>  rics. I need to compute the monthly returns for this data.
>
>  I tried the following code to get the monthly data from the daily prices
>  library(zoo)
>  ss=read.csv("C:\\Documents and
>  Settings\\ravis\\Desktop\\ravi.csv",header=T)
>  ss$Trade.Date=as.Date(ss$Trade.Date,"%Y/%m/%d")
>  ss$ym=as.yearmon(ss$Trade.Date)
>  ss.mon=aggregate(ss$Close.Price,list(RIC=ss$RIC,Month_Year=ss$ym),functi
>  on(x) tail(x,1))
>
>  I am interested in an alternate way of doing this.
>  Any help would be appreciated.
>
>  Thank you,
>  Regards,
>
>  Ravi Shankar S
>
>
>  This e-mail may contain confidential and/or privileged i...{{dropped:10}}
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From ggrothendieck at gmail.com  Mon Mar 24 13:04:51 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Mar 2008 08:04:51 -0400
Subject: [R-SIG-Finance] Monthly returns from Daily prices
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3033BECBD@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3033BECBD@BAN-MAILSRV03.Amba.com>
Message-ID: <971536df0803240504t6f828d35v7ff45279982a91f0@mail.gmail.com>

Your file is not a csv file so I would not name it *.csv.
Try this.

library(zoo)
Lines <- "RIC     Trade.Date      Close.Price Currency.Code
 ABCd.xx        2008/02/29        15.3           CNY
 ABCd.xx        2008/02/28        15.1           CNY
 ABCd.xx        2008/02/27       15.28           CNY
 ABCd.xx        2008/02/26       15.26           CNY
 ABCd.xx        2008/02/25       14.88           CNY
 ABCd.xx        2008/02/22       15.64           CNY
"
zd <- read.zoo(textConnection(Lines), # change to "myfile.txt"
    colClasses = c("NULL", "character", "numeric", "NULL"),
    header = TRUE, format = "%Y/%m/%d")
z <- aggregate(z, as.yearmon, tail, 1)



On Mon, Mar 24, 2008 at 7:26 AM, Ravi S. Shankar <ravis at ambaresearch.com> wrote:
> Hi,
>
> I have the daily data in the following format
>
>        RIC     Trade.Date      Close.Price Currency.Code
>  ABCd.xx        2008/02/29        15.3           CNY
>  ABCd.xx        2008/02/28        15.1           CNY
>  ABCd.xx        2008/02/27       15.28           CNY
>  ABCd.xx        2008/02/26       15.26           CNY
>  ABCd.xx        2008/02/25       14.88           CNY
>  ABCd.xx        2008/02/22       15.64           CNY
>
>
> I have about 1000 rics and one year daily price history for each of the
> rics. I need to compute the monthly returns for this data.
>
> I tried the following code to get the monthly data from the daily prices
> library(zoo)
> ss=read.csv("C:\\Documents and
> Settings\\ravis\\Desktop\\ravi.csv",header=T)
> ss$Trade.Date=as.Date(ss$Trade.Date,"%Y/%m/%d")
> ss$ym=as.yearmon(ss$Trade.Date)
> ss.mon=aggregate(ss$Close.Price,list(RIC=ss$RIC,Month_Year=ss$ym),functi
> on(x) tail(x,1))
>
> I am interested in an alternate way of doing this.
> Any help would be appreciated.
>
> Thank you,
> Regards,
>
> Ravi Shankar S
>
>
> This e-mail may contain confidential and/or privileged i...{{dropped:10}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From ggrothendieck at gmail.com  Mon Mar 24 13:05:31 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Mar 2008 08:05:31 -0400
Subject: [R-SIG-Finance] Monthly returns from Daily prices
In-Reply-To: <971536df0803240504t6f828d35v7ff45279982a91f0@mail.gmail.com>
References: <A36876D3F8A5734FA84A4338135E7CC3033BECBD@BAN-MAILSRV03.Amba.com>
	<971536df0803240504t6f828d35v7ff45279982a91f0@mail.gmail.com>
Message-ID: <971536df0803240505s963c3d6t27dd95884512ef34@mail.gmail.com>

The last line should have been

z <- aggregate(zd, as.yearmon, tail, 1)

On Mon, Mar 24, 2008 at 8:04 AM, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> Your file is not a csv file so I would not name it *.csv.
> Try this.
>
> library(zoo)
> Lines <- "RIC     Trade.Date      Close.Price Currency.Code
>  ABCd.xx        2008/02/29        15.3           CNY
>  ABCd.xx        2008/02/28        15.1           CNY
>  ABCd.xx        2008/02/27       15.28           CNY
>  ABCd.xx        2008/02/26       15.26           CNY
>  ABCd.xx        2008/02/25       14.88           CNY
>  ABCd.xx        2008/02/22       15.64           CNY
> "
> zd <- read.zoo(textConnection(Lines), # change to "myfile.txt"
>    colClasses = c("NULL", "character", "numeric", "NULL"),
>    header = TRUE, format = "%Y/%m/%d")
> z <- aggregate(z, as.yearmon, tail, 1)
>
>
>
> On Mon, Mar 24, 2008 at 7:26 AM, Ravi S. Shankar <ravis at ambaresearch.com> wrote:
>
> > Hi,
> >
> > I have the daily data in the following format
> >
> >        RIC     Trade.Date      Close.Price Currency.Code
> >  ABCd.xx        2008/02/29        15.3           CNY
> >  ABCd.xx        2008/02/28        15.1           CNY
> >  ABCd.xx        2008/02/27       15.28           CNY
> >  ABCd.xx        2008/02/26       15.26           CNY
> >  ABCd.xx        2008/02/25       14.88           CNY
> >  ABCd.xx        2008/02/22       15.64           CNY
> >
> >
> > I have about 1000 rics and one year daily price history for each of the
> > rics. I need to compute the monthly returns for this data.
> >
> > I tried the following code to get the monthly data from the daily prices
> > library(zoo)
> > ss=read.csv("C:\\Documents and
> > Settings\\ravis\\Desktop\\ravi.csv",header=T)
> > ss$Trade.Date=as.Date(ss$Trade.Date,"%Y/%m/%d")
> > ss$ym=as.yearmon(ss$Trade.Date)
> > ss.mon=aggregate(ss$Close.Price,list(RIC=ss$RIC,Month_Year=ss$ym),functi
> > on(x) tail(x,1))
> >
> > I am interested in an alternate way of doing this.
> > Any help would be appreciated.
> >
> > Thank you,
> > Regards,
> >
> > Ravi Shankar S
> >
> >
> > This e-mail may contain confidential and/or privileged i...{{dropped:10}}
> >
> > _______________________________________________
> > R-SIG-Finance at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> > -- Subscriber-posting only.
> > -- If you want to post, subscribe first.
> >
>


From dlincke at lincke.net  Tue Mar 25 01:15:12 2008
From: dlincke at lincke.net (David-Michael Lincke)
Date: Mon, 24 Mar 2008 20:15:12 -0400
Subject: [R-SIG-Finance] R package for continuous futures contract
	construction
In-Reply-To: <OFCBB3E072.3264FEAC-ON85257416.0041E11B-85257416.00422300@fftw.com>
References: <1abb01c88da5$06017c20$2901a8c0@lincke.net>
	<OFCBB3E072.3264FEAC-ON85257416.0041E11B-85257416.00422300@fftw.com>
Message-ID: <1b9101c88e0d$4ec2afc0$2901a8c0@lincke.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080324/c80bbd36/attachment.pl 

From armstrong.whit at gmail.com  Tue Mar 25 01:39:02 2008
From: armstrong.whit at gmail.com (Whit Armstrong)
Date: Mon, 24 Mar 2008 20:39:02 -0400
Subject: [R-SIG-Finance] R package for continuous futures contract
	construction
In-Reply-To: <1b9101c88e0d$4ec2afc0$2901a8c0@lincke.net>
References: <1abb01c88da5$06017c20$2901a8c0@lincke.net>
	<OFCBB3E072.3264FEAC-ON85257416.0041E11B-85257416.00422300@fftw.com>
	<1b9101c88e0d$4ec2afc0$2901a8c0@lincke.net>
Message-ID: <8ec76080803241739j13f4488drfd47cd5a99300d4c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080324/dd2de56a/attachment.pl 

From anass.mouhsine at sgcib.com  Tue Mar 25 09:47:19 2008
From: anass.mouhsine at sgcib.com (anass.mouhsine at sgcib.com)
Date: Tue, 25 Mar 2008 09:47:19 +0100
Subject: [R-SIG-Finance] [R-sig-finance] Aggregating tick by tick
	timeSeries [NC]
In-Reply-To: <e8e755250803210853m27f3fa0x9dc30c017170479e@mail.gmail.com>
Message-ID: <OFEC7F6404.C136FCE3-ONC1257417.003031E3-C1257417.00304700@fr.world.socgen>


thanks Jeff for your detailed answer.
It worked just fine.

regards,
Anass





|------------------------------                                            
|            jeff.a.ryan at gmail                                             
|            .com                                                          
|                                                                          
|            03/21/2008 16:53                                              
|                                                                          
                                                                           
                                                                           
                                                                        To 
                                               Anass                       
                                               MOUHSINE/fr/socgen at socgen   
                                                                        cc 
                                               r-sig-finance at stat.math.eth 
                                               z.ch                        
                                                                   Subject 
                                               Re: [R-SIG-Finance]         
                                               [R-sig-finance] Aggregating 
                                               tick by tick timeSeries     
                                               [C1]                        
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Hi Anass,

My apologies for the delayed response, have been extra short on time
the last few weeks.

You can't get back to a timeSeries object using reclass() because
you've created an entirely new object - which is no longer of class
`xts`.  reclass() provides
class-conversion of an `xts` object only.  This should be possible in
the future though, once the period.apply family of functions are fully
incorporated into xts -- but it isn't now.

You didn't supply much data, but I think these randomly generated
numbers should provide a sufficient example of how to create a *new*
timeSeries object.

> test.xts <-
xts(matrix(c(runif(3400),floor(runif(3400,200,500))),nc=2),(as.POSIXct('2005-04-08
 10:55:16') + 1:34000))

> ep <- endpoints(test.xts,'hours')

> VWAS <-
period.apply(test.xts[,1],ep,sum)/period.apply(test.xts[,2],ep,sum)

> timeSeries(VWAS,index(test.xts)[ep[-1]])
                              TS.1
2005-04-08 10:59:59 0.001369137492
2005-04-08 11:59:59 0.001432987385
2005-04-08 12:59:59 0.001427308452
2005-04-08 13:59:59 0.001428589601
2005-04-08 14:59:59 0.001430663890
2005-04-08 15:59:59 0.001425880024
2005-04-08 16:59:59 0.001423586660
2005-04-08 17:59:59 0.001427435076
2005-04-08 18:59:59 0.001434531036
2005-04-08 19:59:59 0.001433269336
2005-04-08 20:21:56 0.001420547518
>

an alternate approach which maintains the underlying zoo structure
(this moved from quantmod, and hasn't been updated to make use of xts)
AND is 5x faster than the pure R period.apply...

> VWAS2 <-
xts:::period.sum(test.xts[,1],ep)/xts:::period.sum(test.xts[,2],ep)
> as.timeSeries(VWAS2)
                                 x
2005-04-08 10:59:59 0.001369137492
2005-04-08 11:59:59 0.001432987385
2005-04-08 12:59:59 0.001427308452
2005-04-08 13:59:59 0.001428589601
2005-04-08 14:59:59 0.001430663890
2005-04-08 15:59:59 0.001425880024
2005-04-08 16:59:59 0.001423586660
2005-04-08 17:59:59 0.001427435076
2005-04-08 18:59:59 0.001434531036
2005-04-08 19:59:59 0.001433269336
2005-04-08 20:21:56 0.001420547518

timeSeries by its nature stores all 'index' values for the date/time
as a character string, so the timeDate properties will persist if you
need those - as that is a separate conversion. (thanks to Yohan from
Rmetrics for pointing that out to me.)

As far as a more direct way to do the calculation - I don't think
there is.  I am sure there are many other ways to handle this though.

Jeff (with input from Josh)

*************************************************************************
This message and any attachments (the "message") are con...{{dropped:10}}


From chalabi at phys.ethz.ch  Tue Mar 25 17:19:15 2008
From: chalabi at phys.ethz.ch (chalabi at phys.ethz.ch)
Date: Tue, 25 Mar 2008 17:19:15 +0100
Subject: [R-SIG-Finance] fBrowser in Rmetrics - does it exist?
	[r-sig-finance]
In-Reply-To: <68b1e2610803200859m1402387fw44ae9955a78ae25f@mail.gmail.com>
	(Liviu Andronic's message of "Thu, 20 Mar 2008 16:59:26 +0100")
References: <68b1e2610803200859m1402387fw44ae9955a78ae25f@mail.gmail.com>
Message-ID: <87d4pi93p8.fsf@phys.ethz.ch>

"Liviu Andronic" <landronimirc at gmail.com> writes:

> Dear useRs,
>
> I have tried my luck on R-help, with no luck. So, here I go:
>
> I have read in several sources [1] [2] [3] that Rmetrics includes an
> Rcmdr-based graphical user interface named fBrowser GUI. However, I
> have a hard time finding it anywhere (on the net, etc.), nor do I
> manage to load it (having had Rmetrics installed).
>
> Could anyone shed some light on this? Does fBrowser exist, and if it
> does, how should one install/use it?
>
> Thank you in advance,
> Liviu

Hi Liviu,

fBrowser is rather old and its development was stopped because 
Rcmdr was a good alternative. 

regards,
Yohan

-- 


The 2nd International R/Rmetrics User and Developer Workshop ...
[http://www.rmetrics.org]


From landronimirc at gmail.com  Tue Mar 25 22:06:26 2008
From: landronimirc at gmail.com (Liviu Andronic)
Date: Tue, 25 Mar 2008 22:06:26 +0100
Subject: [R-SIG-Finance] fBrowser in Rmetrics - does it exist?
	[r-sig-finance]
In-Reply-To: <87d4pi93p8.fsf@phys.ethz.ch>
References: <68b1e2610803200859m1402387fw44ae9955a78ae25f@mail.gmail.com>
	<87d4pi93p8.fsf@phys.ethz.ch>
Message-ID: <68b1e2610803251406v7ab8dec3kdd6c6cae6ff40a5@mail.gmail.com>

Thank you for answering, Yohan.

On Tue, Mar 25, 2008 at 5:19 PM,  <chalabi at phys.ethz.ch> wrote:
>  fBrowser is rather old and its development was stopped because
>  Rcmdr was a good alternative.

No doubt Rcmdr is good -- lacking advanced graphical functionality
that, say, SPSS would provide, but good and looking better with the
recent introduction of the plugins infrastructure. However, I
understood that fBrowser had a more "finance approach" and at the
moment  Rcmdr provides little functionality specific to Finance. I am
curious whether there are any plans for an Rcmdr plugin that would
make use of Rmetrics functions.

Thanks,
Liviu


From yuri.volchik at gmail.com  Wed Mar 26 16:43:19 2008
From: yuri.volchik at gmail.com (Yuri Volchik)
Date: Wed, 26 Mar 2008 08:43:19 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Time index conversion in zoo/xts
Message-ID: <16304015.post@talk.nabble.com>


Hi to all,

presume question mostly is to Gabor or Jeff:
when i add milliseconds to time index and create zoo (xts) object using some
simple code everything works fine (index (zoo,xts) has unique elements), but
when i added one line to correct for DST changes :

timestamps[format(timestamps,"%H:%M")<'07:00']<-timestamps[format(timestamps,"%H:%M")<'07:00']+3600

suddenly i get a warning about non-unique indices and later i get an error
when trying to merger xts (zoo) object.

I checked classes before and after this line and those seems to be the same
"POSIXt"  "POSIXlt".

Thanks
 

-- 
View this message in context: http://www.nabble.com/Time-index-conversion-in-zoo-xts-tp16304015p16304015.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Wed Mar 26 17:33:43 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Mar 2008 12:33:43 -0400
Subject: [R-SIG-Finance] [R-sig-finance] Time index conversion in zoo/xts
In-Reply-To: <16304015.post@talk.nabble.com>
References: <16304015.post@talk.nabble.com>
Message-ID: <971536df0803260933u325ed1ecg6803dd0adf9e5ad0@mail.gmail.com>

Without something reproducible I can't say definitively but the usual
problem one sees with DST is that there before and after the switchover
there are times that map to the same time thereby giving you an illegal
object.

If you can use tz = "GMT" then there will be no DST and you should not
run into a problem (if the above is the source).   Another approach
is to aggregate over the times to eliminate one of them.

On Wed, Mar 26, 2008 at 11:43 AM, Yuri Volchik <yuri.volchik at gmail.com> wrote:
>
> Hi to all,
>
> presume question mostly is to Gabor or Jeff:
> when i add milliseconds to time index and create zoo (xts) object using some
> simple code everything works fine (index (zoo,xts) has unique elements), but
> when i added one line to correct for DST changes :
>
> timestamps[format(timestamps,"%H:%M")<'07:00']<-timestamps[format(timestamps,"%H:%M")<'07:00']+3600
>
> suddenly i get a warning about non-unique indices and later i get an error
> when trying to merger xts (zoo) object.
>
> I checked classes before and after this line and those seems to be the same
> "POSIXt"  "POSIXlt".
>
> Thanks
>
>
> --
> View this message in context: http://www.nabble.com/Time-index-conversion-in-zoo-xts-tp16304015p16304015.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Wed Mar 26 18:35:08 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 26 Mar 2008 12:35:08 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Time index conversion in zoo/xts
In-Reply-To: <16304015.post@talk.nabble.com>
References: <16304015.post@talk.nabble.com>
Message-ID: <e8e755250803261035s63d894d0q6604b036f0f11172@mail.gmail.com>

Hi Yuri,

I think the issue is that you are sub-setting and then changing only
_part_ of the index, which leaves part of it unadjusted and
potentially overlapping.  An reproducible example would be definitive,
but this is my attempt to explain what I _suspect_ is happening.

Create an xts/zoo object from 6:55 to 7:04 - 10 minutes (so as to fit
in this email),
and assign the index to 'timestamps'
> myxts <- xts(runif(10,40,50),as.POSIXct('2007-01-01 06:55:00')+seq(0,600-1,by=60))
> timestamps <- index(myxts)
> timestamps
 [1] "2007-01-01 06:55:00 UTC" "2007-01-01 06:56:00 UTC"
 [3] "2007-01-01 06:57:00 UTC" "2007-01-01 06:58:00 UTC"
 [5] "2007-01-01 06:59:00 UTC" "2007-01-01 07:00:00 UTC"
 [7] "2007-01-01 07:01:00 UTC" "2007-01-01 07:02:00 UTC"
 [9] "2007-01-01 07:03:00 UTC" "2007-01-01 07:04:00 UTC"

This is the subset you asked for:
> timestamps[format(timestamps,'%H:%M') < '07:00']
[1] "2007-01-01 06:55:00 UTC" "2007-01-01 06:56:00 UTC"
[3] "2007-01-01 06:57:00 UTC" "2007-01-01 06:58:00 UTC"
[5] "2007-01-01 06:59:00 UTC"

Which is only 50% of _my_ data

This is it advanced by 10 minutes:
> timestamps[format(timestamps,'%H:%M') < '07:00']+360
[1] "2007-01-01 07:01:00 UTC" "2007-01-01 07:02:00 UTC"
[3] "2007-01-01 07:03:00 UTC" "2007-01-01 07:04:00 UTC"
[5] "2007-01-01 07:05:00 UTC"

As you may be quick to see, you have duplicated part of the original
timestamps index.

So replacing via:
> timestamps[format(timestamps,'%H:%M') < '07:00'] <- timestamps[format(timestamps,'%H:%M') < '07:00']+360
> timestamps
 [1] "2007-01-01 07:01:00 UTC" "2007-01-01 07:02:00 UTC"
 [3] "2007-01-01 07:03:00 UTC" "2007-01-01 07:04:00 UTC"
 [5] "2007-01-01 07:05:00 UTC" "2007-01-01 07:00:00 UTC"
 [7] "2007-01-01 07:01:00 UTC" "2007-01-01 07:02:00 UTC"
 [9] "2007-01-01 07:03:00 UTC" "2007-01-01 07:04:00 UTC"
>

causes duplicate entries.

I suspect this is what is happening - and why you are getting the
warnings and errors. Without the actual data to work with though it is
only a best guess.

Jeff

>




On Wed, Mar 26, 2008 at 10:43 AM, Yuri Volchik <yuri.volchik at gmail.com> wrote:
>
>  Hi to all,
>
>  presume question mostly is to Gabor or Jeff:
>  when i add milliseconds to time index and create zoo (xts) object using some
>  simple code everything works fine (index (zoo,xts) has unique elements), but
>  when i added one line to correct for DST changes :
>
>  timestamps[format(timestamps,"%H:%M")<'07:00']<-timestamps[format(timestamps,"%H:%M")<'07:00']+3600
>
>  suddenly i get a warning about non-unique indices and later i get an error
>  when trying to merger xts (zoo) object.
>
>  I checked classes before and after this line and those seems to be the same
>  "POSIXt"  "POSIXlt".
>
>  Thanks
>
>
>  --
>  View this message in context: http://www.nabble.com/Time-index-conversion-in-zoo-xts-tp16304015p16304015.html
>  Sent from the Rmetrics mailing list archive at Nabble.com.
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From jeff.a.ryan at gmail.com  Wed Mar 26 18:35:23 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Wed, 26 Mar 2008 12:35:23 -0500
Subject: [R-SIG-Finance] [R-sig-finance] Time index conversion in zoo/xts
In-Reply-To: <16304015.post@talk.nabble.com>
References: <16304015.post@talk.nabble.com>
Message-ID: <e8e755250803261035o77f9112o9e1884271e1e9c84@mail.gmail.com>

Hi Yuri,

I think the issue is that you are sub-setting and then changing only
_part_ of the index, which leaves part of it unadjusted and
potentially overlapping.  An reproducible example would be definitive,
but this is my attempt to explain what I _suspect_ is happening.

Create an xts/zoo object from 6:55 to 7:04 - 10 minutes (so as to fit
in this email),
and assign the index to 'timestamps'
> myxts <- xts(runif(10,40,50),as.POSIXct('2007-01-01 06:55:00')+seq(0,600-1,by=60))
> timestamps <- index(myxts)
> timestamps
 [1] "2007-01-01 06:55:00 UTC" "2007-01-01 06:56:00 UTC"
 [3] "2007-01-01 06:57:00 UTC" "2007-01-01 06:58:00 UTC"
 [5] "2007-01-01 06:59:00 UTC" "2007-01-01 07:00:00 UTC"
 [7] "2007-01-01 07:01:00 UTC" "2007-01-01 07:02:00 UTC"
 [9] "2007-01-01 07:03:00 UTC" "2007-01-01 07:04:00 UTC"

This is the subset you asked for:
> timestamps[format(timestamps,'%H:%M') < '07:00']
[1] "2007-01-01 06:55:00 UTC" "2007-01-01 06:56:00 UTC"
[3] "2007-01-01 06:57:00 UTC" "2007-01-01 06:58:00 UTC"
[5] "2007-01-01 06:59:00 UTC"

Which is only 50% of _my_ data

This is it advanced by 10 minutes:
> timestamps[format(timestamps,'%H:%M') < '07:00']+360
[1] "2007-01-01 07:01:00 UTC" "2007-01-01 07:02:00 UTC"
[3] "2007-01-01 07:03:00 UTC" "2007-01-01 07:04:00 UTC"
[5] "2007-01-01 07:05:00 UTC"

As you may be quick to see, you have duplicated part of the original
timestamps index.

So replacing via:
> timestamps[format(timestamps,'%H:%M') < '07:00'] <- timestamps[format(timestamps,'%H:%M') < '07:00']+360
> timestamps
 [1] "2007-01-01 07:01:00 UTC" "2007-01-01 07:02:00 UTC"
 [3] "2007-01-01 07:03:00 UTC" "2007-01-01 07:04:00 UTC"
 [5] "2007-01-01 07:05:00 UTC" "2007-01-01 07:00:00 UTC"
 [7] "2007-01-01 07:01:00 UTC" "2007-01-01 07:02:00 UTC"
 [9] "2007-01-01 07:03:00 UTC" "2007-01-01 07:04:00 UTC"
>

causes duplicate entries.

I suspect this is what is happening - and why you are getting the
warnings and errors. Without the actual data to work with though it is
only a best guess.

Jeff

>




On Wed, Mar 26, 2008 at 10:43 AM, Yuri Volchik <yuri.volchik at gmail.com> wrote:
>
>  Hi to all,
>
>  presume question mostly is to Gabor or Jeff:
>  when i add milliseconds to time index and create zoo (xts) object using some
>  simple code everything works fine (index (zoo,xts) has unique elements), but
>  when i added one line to correct for DST changes :
>
>  timestamps[format(timestamps,"%H:%M")<'07:00']<-timestamps[format(timestamps,"%H:%M")<'07:00']+3600
>
>  suddenly i get a warning about non-unique indices and later i get an error
>  when trying to merger xts (zoo) object.
>
>  I checked classes before and after this line and those seems to be the same
>  "POSIXt"  "POSIXlt".
>
>  Thanks
>
>
>  --
>  View this message in context: http://www.nabble.com/Time-index-conversion-in-zoo-xts-tp16304015p16304015.html
>  Sent from the Rmetrics mailing list archive at Nabble.com.
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From CVorlow at eurobank.gr  Thu Mar 27 11:17:45 2008
From: CVorlow at eurobank.gr (Vorlow Constantinos)
Date: Thu, 27 Mar 2008 12:17:45 +0200
Subject: [R-SIG-Finance] filter() on zoo objects
Message-ID: <93843C113DD8914BB1A9A63878E8918C01C0C9F9@EH002EXC.eurobank.efg.gr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080327/d4de1d04/attachment.pl 

From jeff.a.ryan at gmail.com  Thu Mar 27 17:18:42 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 27 Mar 2008 11:18:42 -0500
Subject: [R-SIG-Finance] filter() on zoo objects
In-Reply-To: <93843C113DD8914BB1A9A63878E8918C01C0C9F9@EH002EXC.eurobank.efg.gr>
References: <93843C113DD8914BB1A9A63878E8918C01C0C9F9@EH002EXC.eurobank.efg.gr>
Message-ID: <e8e755250803270918p4feabd95me4e34e7b952901ea@mail.gmail.com>

Hi Costas:

I don't think you are using loess correctly - it requires a formula, see ?loess.

with quantmod and xts/zoo:

> getSymbols("^DJI",from='1990-01-01')
[1] "DJI"

> loess(dailyReturn(DJI,type="log") ~ as.numeric(time(DJI)))
Call:
loess(formula = dailyReturn(DJI,type="log") ~ as.numeric(time(DJI)))

Number of Observations: 4595
Equivalent Number of Parameters: 4.34
Residual Standard Error: 0.009854
>

You may need to call Delt(Cl(DJI)) in place of dailyReturn if you
don't have the most recent R-forge version of quantmod.

Additionally filter works too:

> filter(DJI,2)
         [,1]    [,2]    [,3]    [,4]      [,5]    [,6]
 [1,] 5506.40 5623.30 5465.02 5620.30 324140000 5620.30
 [2,] 5620.30 5668.08 5572.52 5619.46 384660000 5619.46
 [3,] 5619.46 5642.92 5532.84 5592.16 354000000 5592.16
 [4,] 5592.16 5620.30 5516.22 5546.50 317060000 5546.50
 [5,]      NA      NA      NA      NA        NA      NA
 [6,]      NA      NA      NA      NA        NA      NA
 [7,] 5546.50 5607.94 5506.82 5588.74 280220000 5588.74
 [8,] 5588.74 5621.58 5520.06 5532.00 310420000 5532.00
 [9,] 5532.00 5545.64 5450.94 5501.28 351980000 5501.28
[10,] 5501.28 5566.54 5496.58 5521.34 308780000 5521.34
...
> as.xts(filter(DJI,2))

1990-01-02 5506.40 5623.30 5465.02 5620.30 324140000 5620.30
1990-01-03 5620.30 5668.08 5572.52 5619.46 384660000 5619.46
1990-01-04 5619.46 5642.92 5532.84 5592.16 354000000 5592.16
1990-01-05 5592.16 5620.30 5516.22 5546.50 317060000 5546.50
1990-01-06      NA      NA      NA      NA        NA      NA
1990-01-07      NA      NA      NA      NA        NA      NA
1990-01-08 5546.50 5607.94 5506.82 5588.74 280220000 5588.74
1990-01-09 5588.74 5621.58 5520.06 5532.00 310420000 5532.00
1990-01-10 5532.00 5545.64 5450.94 5501.28 351980000 5501.28
1990-01-11 5501.28 5566.54 5496.58 5521.34 308780000 5521.34
...

My opinion on the conversion is that it is painful and a waste of
development resources.  All of the time-series objects in R exist for
a reason, and many people are accustomed to one class or another.  As
a developer you had to choose which to support and which to add to
your WISHLIST.  Forcing the user to adopt a particular class such as
'ts' or 'timeSeries' is at best unappealing, and most likely will
simply force the user to look elsewhere for a solution.

The general problem with conversion/reconversion is what we have been
addressing with the `xts` package.  It has yet to be fully sorted out,
and certainly has yet to see wide adoption by other package
developers, but the core concept is simple, and it _is_ implemented so
far in a few functions:

Basically, functions can call `as.xts` on any incoming
time-series-like object, handle it with all the tools available to
zoo/xts objects, and then simply call `reclass` to convert the object
back to its original class to return to the user the class of object
he used in the original call.  There are some minor things to keep
track of if very different objects are returned, but for the most part
it is that simple.

If the object need not be returned then the process truly only
requires a call to 'as.xts' at the beginning.  The rest of the
function can now handle the data as a zoo/xts object.  The majority of
functions seem to only need this approach.

At present you can see 'reclass' in the `to.period` function in `xts`,
as well as the `periodReturn` function that is in the quantmod version
on R-forge.  'as.xts' is used in many functions in both packages to
allow for maximum time-series-like class support - including
chartSeries from quantmod.

The issue that this approach addresses is the simplified and uniform
handling of all of R's disparate time-series-like objects.  `xts`
attempts to handle the conversion and reconversion without having the
package/function author write methods for each class of object.  It
saves time, and makes for a far less error-filled final product.  In
reality it also means that new functions will *just work* with almost
any object that is passed in.

I am working on putting together a vignette to address developing
functions, as well as modifying current ones, with xts.  I will make
it known to the list when it is complete.

Jeff

On Thu, Mar 27, 2008 at 5:17 AM, Vorlow Constantinos
<CVorlow at eurobank.gr> wrote:
> Dear all,
>
>  Am I right in understanding you cannot directly apply functions such as
>  loess() (filter() as well ?) to zoo objects and you need to use the
>  rollapply/rollmean functions instead?
>
>  For example:
>
>  library("tseries")
>  DJ<- get.hist.quote("^DJI", start = "1990-01-01", quote = "Close")
>  DJret<-diff(log(DJ))
>
>  # the
>
>  loess(DJret, time(DJret))
>
>  does not work(?). What am I understanding  wrong? I found a reply by
>  Achim on a similar issue.
>
>  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/88599.html
>
>  Is Achim's answer all there is to it?
>
>  I.e., my problem is a little bit  more general:
>
>  Say I want to produce fits and then forecasts on a time series (zoo
>  mostly) using a non-zoo routine which "understands" ts or timeSeries
>  objects (or simple vectors).  Do I always have to "translate" zoo
>  objects to vectors or to ts/simeSeries ones, run the routines and then
>  put back the zoo attribute with the appropriate dates so as to produce
>  correct time-series plots (with dates etc, especially correct dates
>  alligned to the forecast period - which could be postsample)?
>
>  Is there an easier way to shift between zoo-ts/timeSeries objects and
>  produce plots statistical analysis that will have the dates correctly
>  aligned on (to account for example for irregularly sampled sequences
>  such as stock prices with non-trading days etc. etc.)?
>
>  Thanks in advance,
>  Costas
>
>
>
>  P Think before you print.
>
>  Disclaimer:
>  This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
>  EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
>  EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.
>
>
>         [[alternative HTML version deleted]]
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From ggrothendieck at gmail.com  Thu Mar 27 18:06:59 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Mar 2008 13:06:59 -0400
Subject: [R-SIG-Finance] filter() on zoo objects
In-Reply-To: <93843C113DD8914BB1A9A63878E8918C01C0C9F9@EH002EXC.eurobank.efg.gr>
References: <93843C113DD8914BB1A9A63878E8918C01C0C9F9@EH002EXC.eurobank.efg.gr>
Message-ID: <971536df0803271006i241fa745n6aee33fbd3bf8673@mail.gmail.com>

Two points:

1. If you have a routine that accepts ts but not zoo then you have a problem
even beyond representations since the routine is assuming equally
spaced points.  Assuming your data is not equally spaced, e.g.
no data on weekends, then its up to you to figure out how you
want to map it to equally spaced points.  Two possibilities for
zoo object z are:

- as.ts(z) which will make it equally spaced by inserting NAs (e.g.
where weekends are)

- ts(coredata(z)) which uses the time base 1, 2, 3, ... or you can
use other args of ts to use a different time base.

If the routine you are calling returns a ts object, out,
then it may or may not be that as.zoo(out) or
zoo(coredata(out), time(z)) make sense.  It will all
depend on the situation.  You may have to write
a custom mapping to the time base or not use that
ts-only routine in the first place -- see first paragraph above.

2. Aside from rollapply and friends,  The last question in the zoo
faq gives a list of the packages that work with zoo objects --
there are now about 20 of them.  For example, dyn and dynlm
packages can perform regression on zoo series with lags
and diffs and keep track of the time base.  You want to stick
with zoo-capable routines if possible.

On Thu, Mar 27, 2008 at 6:17 AM, Vorlow Constantinos
<CVorlow at eurobank.gr> wrote:
> Dear all,
>
> Am I right in understanding you cannot directly apply functions such as
> loess() (filter() as well ?) to zoo objects and you need to use the
> rollapply/rollmean functions instead?
>
> For example:
>
> library("tseries")
> DJ<- get.hist.quote("^DJI", start = "1990-01-01", quote = "Close")
> DJret<-diff(log(DJ))
>
> # the
>
> loess(DJret, time(DJret))
>
> does not work(?). What am I understanding  wrong? I found a reply by
> Achim on a similar issue.
>
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/88599.html
>
> Is Achim's answer all there is to it?
>
> I.e., my problem is a little bit  more general:
>
> Say I want to produce fits and then forecasts on a time series (zoo
> mostly) using a non-zoo routine which "understands" ts or timeSeries
> objects (or simple vectors).  Do I always have to "translate" zoo
> objects to vectors or to ts/simeSeries ones, run the routines and then
> put back the zoo attribute with the appropriate dates so as to produce
> correct time-series plots (with dates etc, especially correct dates
> alligned to the forecast period - which could be postsample)?
>
> Is there an easier way to shift between zoo-ts/timeSeries objects and
> produce plots statistical analysis that will have the dates correctly
> aligned on (to account for example for irregularly sampled sequences
> such as stock prices with non-trading days etc. etc.)?
>
> Thanks in advance,
> Costas
>
>
>
> P Think before you print.
>
> Disclaimer:
> This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
> EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
> EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From jeff.a.ryan at gmail.com  Thu Mar 27 18:25:15 2008
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 27 Mar 2008 12:25:15 -0500
Subject: [R-SIG-Finance] filter() on zoo objects
In-Reply-To: <971536df0803271006i241fa745n6aee33fbd3bf8673@mail.gmail.com>
References: <93843C113DD8914BB1A9A63878E8918C01C0C9F9@EH002EXC.eurobank.efg.gr>
	<971536df0803271006i241fa745n6aee33fbd3bf8673@mail.gmail.com>
Message-ID: <e8e755250803271025id96c6c6v6e64ae7172c3c30e@mail.gmail.com>

One comment (caution?) to add to Gabor's excellent points:

If you use the *timeSeries* class - be careful as to the conversion of
irregular series to regular.

as.ts.timeSeries converts what _may_ be an irregular series into a
_regular_ one by dropping the time altogether - much like the
ts(coredata(z)) approach outlined by Gabor.

While this may be what you want - it is not necessarily correct IMO.
In fact just the choice of class can lead to unintended consequences:

Please ignore the simplicity of the example:

> getSymbols("MSFT",ret='zoo')
> arima(Cl(MSFT),order=c(2,0,0))

Call:
arima(x = Cl(MSFT), order = c(2, 0, 0))

Coefficients:
         ar1     ar2  intercept
      0.8375  0.1437    30.1932
s.e.  0.0631  0.0633     1.0612

sigma^2 estimated as 0.2238:  log likelihood = -239.51,  aic = 487.01

> arima(as.ts(coredata(Cl(MSFT))),order=c(2,0,0))

Call:
arima(x = as.ts(coredata(Cl(MSFT))), order = c(2, 0, 0))

Coefficients:
         ar1     ar2  intercept
      0.9582  0.0190    30.1612
s.e.  0.0569  0.0569     1.0718

sigma^2 estimated as 0.2359:  log likelihood = -216.81,  aic = 441.62


> getSymbols("MSFT",ret='timeSeries')
[1] "MSFT"
> arima(Cl(MSFT),order=c(2,0,0))

Call:
arima(x = Cl(MSFT), order = c(2, 0, 0))

Coefficients:
         ar1     ar2  intercept
      0.9582  0.0190    30.1612
s.e.  0.0569  0.0569     1.0718

sigma^2 estimated as 0.2359:  log likelihood = -216.81,  aic = 441.62
>

To me, the timeSeries class is making a decision that is not
necessarily the intended one (in general) - and the results are
therefore possibly different than what you might expect.

Jeff

On Thu, Mar 27, 2008 at 12:06 PM, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> Two points:
>
>  1. If you have a routine that accepts ts but not zoo then you have a problem
>  even beyond representations since the routine is assuming equally
>  spaced points.  Assuming your data is not equally spaced, e.g.
>  no data on weekends, then its up to you to figure out how you
>  want to map it to equally spaced points.  Two possibilities for
>  zoo object z are:
>
>  - as.ts(z) which will make it equally spaced by inserting NAs (e.g.
>  where weekends are)
>
>  - ts(coredata(z)) which uses the time base 1, 2, 3, ... or you can
>  use other args of ts to use a different time base.
>
>  If the routine you are calling returns a ts object, out,
>  then it may or may not be that as.zoo(out) or
>  zoo(coredata(out), time(z)) make sense.  It will all
>  depend on the situation.  You may have to write
>  a custom mapping to the time base or not use that
>  ts-only routine in the first place -- see first paragraph above.
>
>  2. Aside from rollapply and friends,  The last question in the zoo
>  faq gives a list of the packages that work with zoo objects --
>  there are now about 20 of them.  For example, dyn and dynlm
>  packages can perform regression on zoo series with lags
>  and diffs and keep track of the time base.  You want to stick
>  with zoo-capable routines if possible.
>
>
>  On Thu, Mar 27, 2008 at 6:17 AM, Vorlow Constantinos
>  <CVorlow at eurobank.gr> wrote:
>
>
> > Dear all,
>  >
>  > Am I right in understanding you cannot directly apply functions such as
>  > loess() (filter() as well ?) to zoo objects and you need to use the
>  > rollapply/rollmean functions instead?
>  >
>  > For example:
>  >
>  > library("tseries")
>  > DJ<- get.hist.quote("^DJI", start = "1990-01-01", quote = "Close")
>  > DJret<-diff(log(DJ))
>  >
>  > # the
>  >
>  > loess(DJret, time(DJret))
>  >
>  > does not work(?). What am I understanding  wrong? I found a reply by
>  > Achim on a similar issue.
>  >
>  > http://finzi.psych.upenn.edu/R/Rhelp02a/archive/88599.html
>  >
>  > Is Achim's answer all there is to it?
>  >
>  > I.e., my problem is a little bit  more general:
>  >
>  > Say I want to produce fits and then forecasts on a time series (zoo
>  > mostly) using a non-zoo routine which "understands" ts or timeSeries
>  > objects (or simple vectors).  Do I always have to "translate" zoo
>  > objects to vectors or to ts/simeSeries ones, run the routines and then
>  > put back the zoo attribute with the appropriate dates so as to produce
>  > correct time-series plots (with dates etc, especially correct dates
>  > alligned to the forecast period - which could be postsample)?
>  >
>  > Is there an easier way to shift between zoo-ts/timeSeries objects and
>  > produce plots statistical analysis that will have the dates correctly
>  > aligned on (to account for example for irregularly sampled sequences
>  > such as stock prices with non-trading days etc. etc.)?
>  >
>  > Thanks in advance,
>  > Costas
>  >
>  >
>  >
>  > P Think before you print.
>  >
>  > Disclaimer:
>  > This e-mail is confidential. If you are not the intended recipient, you should not copy it, re-transmit it, use it or disclose its contents, but should return it to the sender immediately and delete the copy from your system.
>  > EFG Eurobank Ergasias S.A. is not responsible for, nor endorses, any opinion, recommendation, conclusion, solicitation, offer or agreement or any information contained in this communication.
>  > EFG Eurobank Ergasias S.A. cannot accept any responsibility for the accuracy or completeness of this message as it has been transmitted over a public network. If you suspect that the message may have been intercepted or amended, please call the sender.
>  >
>  >
>  >        [[alternative HTML version deleted]]
>  >
>  > _______________________________________________
>  > R-SIG-Finance at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  > -- Subscriber-posting only.
>  > -- If you want to post, subscribe first.
>  >
>
>  _______________________________________________
>  R-SIG-Finance at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>  -- Subscriber-posting only.
>  -- If you want to post, subscribe first.
>



-- 
There's a way to do it better - find it.
Thomas A. Edison


From yuri.volchik at gmail.com  Thu Mar 27 20:23:51 2008
From: yuri.volchik at gmail.com (Yuri Volchik)
Date: Thu, 27 Mar 2008 12:23:51 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Time index conversion in zoo/xts
In-Reply-To: <e8e755250803261035o77f9112o9e1884271e1e9c84@mail.gmail.com>
References: <16304015.post@talk.nabble.com>
	<e8e755250803261035o77f9112o9e1884271e1e9c84@mail.gmail.com>
Message-ID: <16332124.post@talk.nabble.com>


Thanks very much for answers,
problem was indeed with DST settings. Another hidden problem with DST can be
when extracting date of, say, 2005-05-01 00:15 we can get 2005-04-31.

I was wondering if any members of the list managed to align timeseries from
different timezones to a common denominator using timeDate class? 

Best regards,
Yuri
-- 
View this message in context: http://www.nabble.com/-R-sig-finance--Time-index-conversion-in-zoo-xts-tp16307968p16332124.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Thu Mar 27 20:30:27 2008
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Mar 2008 15:30:27 -0400
Subject: [R-SIG-Finance] [R-sig-finance] Time index conversion in zoo/xts
In-Reply-To: <16332124.post@talk.nabble.com>
References: <16304015.post@talk.nabble.com>
	<e8e755250803261035o77f9112o9e1884271e1e9c84@mail.gmail.com>
	<16332124.post@talk.nabble.com>
Message-ID: <971536df0803271230g4d16120aw45f607b50da29d55@mail.gmail.com>

Use "GMT" time zone -- then there is no DST.  Note that internally
the rmetrics routines do use GMT for that reason.

On Thu, Mar 27, 2008 at 3:23 PM, Yuri Volchik <yuri.volchik at gmail.com> wrote:
>
> Thanks very much for answers,
> problem was indeed with DST settings. Another hidden problem with DST can be
> when extracting date of, say, 2005-05-01 00:15 we can get 2005-04-31.
>
> I was wondering if any members of the list managed to align timeseries from
> different timezones to a common denominator using timeDate class?
>
> Best regards,
> Yuri
> --
> View this message in context: http://www.nabble.com/-R-sig-finance--Time-index-conversion-in-zoo-xts-tp16307968p16332124.html
>
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From yuri.volchik at gmail.com  Thu Mar 27 20:53:14 2008
From: yuri.volchik at gmail.com (Yuri Volchik)
Date: Thu, 27 Mar 2008 12:53:14 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] http://www.market-topology.com/
In-Reply-To: <47CAE8C3.90306@pdf.com>
References: <15786834.post@talk.nabble.com> <47CAE8C3.90306@pdf.com>
Message-ID: <16332263.post@talk.nabble.com>


Hi Spencer,
thanks for your answer, i did look at those packages.
given your very informative post (i understand your point about possible
changes in correlation structure etc) how they can do what they do :) ,their
classification makes sense to me.

I presume one can use intraday prices, say with 5 min intervals, thus in one
year would have around 20000 observations so it should be enough from a
computational viewpoint.

If there is any good link (book) on the subj, would appreciate it.

Thanks  



-- 
View this message in context: http://www.nabble.com/http%3A--www.market-topology.com--tp15786834p16332263.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From yuri.volchik at gmail.com  Thu Mar 27 20:55:50 2008
From: yuri.volchik at gmail.com (Yuri Volchik)
Date: Thu, 27 Mar 2008 12:55:50 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] http://www.market-topology.com/
Message-ID: <16332263.post@talk.nabble.com>


Hi Spencer,
thanks for your answer, i did look at those packages.
given your very informative post (i understand your point about possible
changes in correlation structure etc) how they can do what they do :) ,their
classification makes sense to me.

Another link with MST in finance
http://www.etf-central.com/select-sector-spdr-minimum-spanning-trees-november-27th%2C-2007-203

I presume one can use intraday prices, say with 5 min intervals, thus in one
year would have around 20000 observations so it should be enough from a
computational viewpoint.

If there is any good link (book) on the subj, would appreciate it.

Thanks  



-- 
View this message in context: http://www.nabble.com/http%3A--www.market-topology.com--tp15786834p16332263.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From kriskumar at earthlink.net  Thu Mar 27 21:34:52 2008
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Thu, 27 Mar 2008 16:34:52 -0400
Subject: [R-SIG-Finance] [R-sig-finance] http://www.market-topology.com/
In-Reply-To: <16332263.post@talk.nabble.com>
References: <15786834.post@talk.nabble.com> <47CAE8C3.90306@pdf.com>
	<16332263.post@talk.nabble.com>
Message-ID: <47EC04EC.8060505@earthlink.net>

Yuri Volchik wrote:
> Hi Spencer,
> thanks for your answer, i did look at those packages.
> given your very informative post (i understand your point about possible
> changes in correlation structure etc) how they can do what they do :) ,their
> classification makes sense to me.
>
> I presume one can use intraday prices, say with 5 min intervals, thus in one
> year would have around 20000 observations so it should be enough from a
> computational viewpoint.
>
> If there is any good link (book) on the subj, would appreciate it.
>
> Thanks  
>
>
>
>   
There is a reference to MST in the book by Stanley and Mantega,
http://www.amazon.com/exec/obidos/ASIN/0521620082/kriskumar-20

There is also a couple of papers on the web co-authored  by some folks 
with Mantega. Also you will find this fx reference useful
http://arxiv.org/abs/physics/0503014

Cheers
Krishna


From dlincke at lincke.net  Sat Mar 29 22:58:18 2008
From: dlincke at lincke.net (David-Michael Lincke)
Date: Sat, 29 Mar 2008 17:58:18 -0400
Subject: [R-SIG-Finance] R package for continuous futures contract
	construction
In-Reply-To: <8ec76080803241739j13f4488drfd47cd5a99300d4c@mail.gmail.com>
References: <1abb01c88da5$06017c20$2901a8c0@lincke.net>
	<OFCBB3E072.3264FEAC-ON85257416.0041E11B-85257416.00422300@fftw.com>
	<1b9101c88e0d$4ec2afc0$2901a8c0@lincke.net>
	<8ec76080803241739j13f4488drfd47cd5a99300d4c@mail.gmail.com>
Message-ID: <1fc401c891e8$018b0f70$2901a8c0@lincke.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080329/a092fc4a/attachment.pl 

From matthieudm.boyer at gmail.com  Sun Mar 30 12:40:40 2008
From: matthieudm.boyer at gmail.com (Matthieu Boyer)
Date: Sun, 30 Mar 2008 12:40:40 +0200
Subject: [R-SIG-Finance] Garch and multivariate garch
Message-ID: <ac8dee0a0803300340w6a5ecc35t2809f011488c8558@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080330/d25f630e/attachment.pl 

From ravis at ambaresearch.com  Mon Mar 31 08:33:12 2008
From: ravis at ambaresearch.com (Ravi S. Shankar)
Date: Mon, 31 Mar 2008 12:03:12 +0530
Subject: [R-SIG-Finance] Grouping of stocks
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3034001EF@BAN-MAILSRV03.Amba.com>

Hi R,

I am trying to find the number of stocks rising and number of stocks
falling each month. What I had in mind was I would compare the price of
each stock with the previous month's price and then group the stocks. Is
there a better way of doing this? An apology if this seems a very basic
question!
I have about 1500 stocks in my universe. 

Ravi

This e-mail may contain confidential and/or privileged i...{{dropped:10}}


From reinhold.hafner at risklab.de  Mon Mar 31 08:47:11 2008
From: reinhold.hafner at risklab.de (reini)
Date: Sun, 30 Mar 2008 23:47:11 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Direct Specification of Mu and
	Sigma in fportfolio
Message-ID: <16392702.post@talk.nabble.com>


Dear all, 
I was wondering whether someone can help me to solve a problem that I posted
some weeks ago: 

In using fportfolio I would like to directly specify a mu and sigma rather
than to hand over a multivariate time series from which mu and sigma are
estimated by the relevant functions. How does this work? From the
documentation it seemed possible to me, however, when I investigated the
relevant code it is asking for a time series. 

Many thanks, 
Reinhold 
-- 
View this message in context: http://www.nabble.com/Direct-Specification-of-Mu-and-Sigma-in-fportfolio-tp16392702p16392702.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From agehr at mozart.depaul.edu  Mon Mar 31 09:50:38 2008
From: agehr at mozart.depaul.edu (Adam Gehr)
Date: Mon, 31 Mar 2008 02:50:38 -0500 (CDT)
Subject: [R-SIG-Finance] [R-sig-finance] Direct Specification of Mu and
 Sigma in fportfolio
In-Reply-To: <16392702.post@talk.nabble.com>
References: <16392702.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.63.0803310248200.12514@mozart.depaul.edu>

If I understand what you want to do correctly, the way I've solved similar 
problems in the past is just to use one of the quadratic programming 
routines. solve.QP, etc.

Adam Gehr


On Sun, 30 Mar 2008, reini wrote:

>
> Dear all,
> I was wondering whether someone can help me to solve a problem that I posted
> some weeks ago:
>
> In using fportfolio I would like to directly specify a mu and sigma rather
> than to hand over a multivariate time series from which mu and sigma are
> estimated by the relevant functions. How does this work? From the
> documentation it seemed possible to me, however, when I investigated the
> relevant code it is asking for a time series.
>
> Many thanks,
> Reinhold
> -- 
> View this message in context: http://www.nabble.com/Direct-Specification-of-Mu-and-Sigma-in-fportfolio-tp16392702p16392702.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only.
> -- If you want to post, subscribe first.
>


From reinhold.hafner at risklab.de  Mon Mar 31 10:44:30 2008
From: reinhold.hafner at risklab.de (reini)
Date: Mon, 31 Mar 2008 01:44:30 -0700 (PDT)
Subject: [R-SIG-Finance] [R-sig-finance] Direct Specification of Mu and
 Sigma in fportfolio
In-Reply-To: <Pine.LNX.4.63.0803310248200.12514@mozart.depaul.edu>
References: <16392702.post@talk.nabble.com>
	<Pine.LNX.4.63.0803310248200.12514@mozart.depaul.edu>
Message-ID: <16393833.post@talk.nabble.com>


In principle, you are right. I was just wondering whether this functionality
is directly provided by fportfolio.


Adam Gehr wrote:
> 
> If I understand what you want to do correctly, the way I've solved similar 
> problems in the past is just to use one of the quadratic programming 
> routines. solve.QP, etc.
> 
> Adam Gehr
> 
> 
> On Sun, 30 Mar 2008, reini wrote:
> 
>>
>> Dear all,
>> I was wondering whether someone can help me to solve a problem that I
>> posted
>> some weeks ago:
>>
>> In using fportfolio I would like to directly specify a mu and sigma
>> rather
>> than to hand over a multivariate time series from which mu and sigma are
>> estimated by the relevant functions. How does this work? From the
>> documentation it seemed possible to me, however, when I investigated the
>> relevant code it is asking for a time series.
>>
>> Many thanks,
>> Reinhold
>> -- 
>> View this message in context:
>> http://www.nabble.com/Direct-Specification-of-Mu-and-Sigma-in-fportfolio-tp16392702p16392702.html
>> Sent from the Rmetrics mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only.
>> -- If you want to post, subscribe first.
>>
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/Direct-Specification-of-Mu-and-Sigma-in-fportfolio-tp16392702p16393833.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From kriskumar at earthlink.net  Mon Mar 31 14:30:26 2008
From: kriskumar at earthlink.net (Krishna Kumar)
Date: Mon, 31 Mar 2008 08:30:26 -0400
Subject: [R-SIG-Finance] Grouping of stocks
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3034001EF@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3034001EF@BAN-MAILSRV03.Amba.com>
Message-ID: <47F0D962.1000009@earthlink.net>

"rising" is a broad word do you mean something that went up or something 
that has momentum??
There are many momentum indicators that can show you if something has 
been going up and you
could use that to rank the stocks in your universe.


Ravi S. Shankar wrote:
> Hi R,
>
> I am trying to find the number of stocks rising and number of stocks
> falling each month. What I had in mind was I would compare the price of
> each stock with the previous month's price and then group the stocks. Is
> there a better way of doing this? An apology if this seems a very basic
> question!
> I have about 1500 stocks in my universe. 
>
> Ravi
>
> This e-mail may contain confidential and/or privileged i...{{dropped:10}}
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
>
>


From spencer.graves at pdf.com  Mon Mar 31 19:17:03 2008
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 31 Mar 2008 10:17:03 -0700
Subject: [R-SIG-Finance] bilinear and non linear time series?
Message-ID: <47F11C8F.6020208@pdf.com>

Hello: 

      Are there any facilities in R for estimating bilinear time series, 
i.e., ARMA models with cross products between AR and MA terms?  I ask, 
because Tsay (2005, sec. 4.1.1) discusses this.  RSiteSearch("bilinear 
time series") produced a post from May 2004 on this issue.  Is the 
answer today the same as in 2004:  Use 'nls' or write a likelihood 
function and use 'optim'? 

      Thanks,
      Spencer


From josh at gghc.com  Mon Mar 31 20:49:41 2008
From: josh at gghc.com (Joshua Reich)
Date: Mon, 31 Mar 2008 14:49:41 -0400
Subject: [R-SIG-Finance] R + NVIDIA CUDA
In-Reply-To: <47F11C8F.6020208@pdf.com>
References: <47F11C8F.6020208@pdf.com>
Message-ID: <C20EA84D76C94F4E999DC041E81C0D11058BD5DA@exchange2k3.ny.gghc.com>

>From browsing the NVIDIA CUDA forum
(http://www.nvidia.com/object/cuda_home.html) it seems that quite a few
people are working on Monte Carlo option pricing libraries using GPU
technology for impressive speed-ups. Does anyone here know of any
attempts to use NVIDIA's BLAS library with R?

Thanks,

Josh Reich


From chalabi at phys.ethz.ch  Mon Mar 31 21:12:24 2008
From: chalabi at phys.ethz.ch (chalabi at phys.ethz.ch)
Date: Mon, 31 Mar 2008 21:12:24 +0200
Subject: [R-SIG-Finance] [R-sig-finance] Direct Specification of Mu and
	Sigma in fportfolio
In-Reply-To: <16392702.post@talk.nabble.com> (reini's message of "Sun, 30 Mar
	2008 23:47:11 -0700 (PDT)")
References: <16392702.post@talk.nabble.com>
Message-ID: <87iqz2aesn.fsf@phys.ethz.ch>

reini <reinhold.hafner at risklab.de> writes:

> Dear all, 
> I was wondering whether someone can help me to solve a problem that I posted
> some weeks ago: 
>
> In using fportfolio I would like to directly specify a mu and sigma rather
> than to hand over a multivariate time series from which mu and sigma are
> estimated by the relevant functions. How does this work? From the
> documentation it seemed possible to me, however, when I investigated the
> relevant code it is asking for a time series. 
>
> Many thanks, 
> Reinhold 
> -- 
> View this message in context: http://www.nabble.com/Direct-Specification-of-Mu-and-Sigma-in-fportfolio-tp16392702p16392702.html
> Sent from the Rmetrics mailing list archive at Nabble.com.
>
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.

Hi Reini,

we are currently working on an update of the fPortfolio package. Many
new features will become available, like LPM optimization, and second
order cone programming in a separate package.

We are trying to make fPortfolio as modular as possible to help users to
defined their own functions.

The "unfinished" manual page of portfolioSpec describes how to do it.

a quick example :

--------------

library(fPortfolio)
# only with development version of fPortfolio available on R-Forge

# now you can define your own estimator which must returns a list with a
# named list, with at least the following two entries '\$mu' and
# '\$Sigma', which represent estimators for the mean and covariance,
# respectively.
myEstimator <- 
    function(x, spec = NULL, ...) list(mu = colMeans(x), Sigma = cov(x))

Spec <- portfolioSpec() # default portfolio specification  
setEstimator(Spec) <- "myEstimator" # new estimator
Spec

# Load Data and Convert to timeSeries Object:
Data = as.timeSeries(data(smallcap.ts))
Data = Data[, c("BKE", "GG", "GYMB", "KRON")]
Data

## Compute properties of Efficient Portfolio
frontier <- portfolioFrontier(Data, Spec, "LongOnly")
plot(frontier)

--------------

you can download the latest development version from our R-Forge project
page at http://r-forge.r-project.org/projects/rmetrics/. But bear in
mind that this is a development version... if you have any suggestion to
improve the package, do not hesitate to send us your feedback!

all my apologises for the late response,
Yohan


-- 


The 2nd International R/Rmetrics User and Developer Workshop ...
[http://www.rmetrics.org]


From dave at lincke.net  Sun Mar 23 20:42:24 2008
From: dave at lincke.net (dave at lincke.net)
Date: Sun, 23 Mar 2008 19:42:24 -0000
Subject: [R-SIG-Finance] R package for continuous futures contract
 construction
Message-ID: <7767.2816421206301329305.JavaMail.servlet@perfora>

Thanks for the suggestions. However, investing into a whole new data server and mangement platform is overkill and not really an option for us in addressing our requirements wrt continuous contracts. Also, a quick perusal of the LIM Rollover language manual suggests that its functionality does not fully cover our requirements anyway.


>If you have a little cash to spare you might take a look at the LIM  
>databasae products that do all of this for you...
>
>On 22 Mar 2008, at 22:37, David-Michael Lincke wrote:
>
>> Before I reinvent the wheel, I was wondering if anybody is aware of a
>> preexisting R or Matlab package for the assembly of of continuous  
>> commodity
>> futures contracts? For my purposes this would have to support  
>> construction
>> of ratio-adjusted time series based on a configurable roll calendar.
>>
>> David
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-SIG-Finance at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>> -- Subscriber-posting only. 
>> -- If you want to post, subscribe first.
>


From ecjbosu at aim.com  Wed Jan 23 05:11:44 2008
From: ecjbosu at aim.com (Joe W. Byers)
Date: Wed, 23 Jan 2008 04:11:44 -0000
Subject: [R-SIG-Finance] Solicitation of opinions on which Timeseries
	object(s) to utilize.
In-Reply-To: <20080121183539.GB12556@piskorski.com>
References: <fmvquf$9hm$1@ger.gmane.org> <20080121183539.GB12556@piskorski.com>
Message-ID: <4796BE50.7040707@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080123/e15cbf12/attachment.pl>

From marcin.kopaczynski at soundinvest.net  Fri Feb  1 07:18:47 2008
From: marcin.kopaczynski at soundinvest.net (Marcin Kopaczynski)
Date: Fri, 01 Feb 2008 06:18:47 -0000
Subject: [R-SIG-Finance] [R-sig-finance] [R] Bloomberg Data Import to R
Message-ID: <47A2B9DE.4010101@soundinvest.net>

hi robert,

i?ve got a problem with the bloomberg data import. usually all works 
well, but sometimes it seems that nothing is downloaded at all from 
bloomberg.

example: i wanted to download px_last for 900 us-stocks beginning with 
19900101 (giving him the appropriate chron-date format). the download 
starts, the downloaded data is stored and written out to a file. new 
data is being concatenated with the old data (column by column) until 
all 900 stocks are downloaded. during this process the download breaks 
down, and the strange thing is, that it does happen SOMETIMES and not at 
the same place (i.e. not at stock number 390, but at number 401 at one 
time and 789 at other time). sometimes it does not happen at all. also, 
when i resume the download, i.e. tell him to download the data from 
where he stopped, it then works.

so i checked the functions in your package and the problem really seems 
to be with the download itself. what happens is: in the function 
<blpGetHistoricalData> the object <lst> is NULL. this makes the command 
"attr(lst, "num.of.date.cols")  <- 1" to throw an error, because he is 
not able to assign an attribute to a NULL object:
"attr(lst, "num.of.date.cols") <- 1 : attempt to set an attribute on 
NULL". the object <lst> becomes then a data.frame of the dimension 
[0:number.of.tickers], which obviously is not what one would expect to get.

my question is: have you ever experienced such a problem? the problem 
seems to become more probable, the more data one downloads from bloomberg.

here are some infos on my environment:

R 2.5.1
windows xp on the bloomberg terminal
chron 2.3.16
RBloomberg 0.1-10
RDCOMClient 0.91-0
zoo 1.4-2

i?ve been talking to bloomberg about this problem, as well. they assured 
me that it is not about the amount of data i was downloading. so there 
must be something else.

thx in advance for your help,

marcin


From jvdeblasio at optonline.net  Fri Feb  1 17:36:24 2008
From: jvdeblasio at optonline.net (John DeBlasio)
Date: Fri, 01 Feb 2008 16:36:24 -0000
Subject: [R-SIG-Finance]  R / Risk measurement and the problem of data
Message-ID: <002101c864f0$34f24380$6a01a8c0@internal.accunet.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080201/ebf510a0/attachment.pl>

From jwibb at verizon.net  Mon Feb  4 22:50:10 2008
From: jwibb at verizon.net (Wibbs)
Date: Mon, 04 Feb 2008 21:50:10 -0000
Subject: [R-SIG-Finance] R question
Message-ID: <002201c86777$d6222430$02017fa0@IBM686835U>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080204/e9d1b643/attachment.pl>

From mats_pistol at yahoo.com  Tue Feb  5 01:18:11 2008
From: mats_pistol at yahoo.com (mats pistol)
Date: Tue, 05 Feb 2008 00:18:11 -0000
Subject: [R-SIG-Finance] finance heat maps
Message-ID: <201129.54303.qm@web61118.mail.yahoo.com>

Hi

I am looking at using heat maps to display the
variation in a portfolio weights as the efficient
frontier is traversed.

Has anyone tried using heat maps for this purpose, and
are they the right type of graph to use to display
this information?

Thanks

Mats


From marc.dassler at bearingpoint.com  Thu Feb  7 18:52:45 2008
From: marc.dassler at bearingpoint.com (Dassler, Marc)
Date: Thu, 07 Feb 2008 17:52:45 -0000
Subject: [R-SIG-Finance] Experience of large scale use of R in financial
	services
Message-ID: <59C6D502E2521141B9F6C3D857FB24BA0DBB19@becxoex03.corp.kpmgconsulting.com>

Hi!

We, a management and technology consultants company in financial
services, are planning a large scale use of R in a bank. We would like
to use R as calculation kernel for creating ratings of special
transactions based on cash flow simulations. 

Here comes my question: does anyone have experience of large scale usage
of R in an operational system?

Best regards,
Marc
***************************************************************************************************
The information in this email is confidential and may be...{{dropped:18}}


From boldinm at wharton.upenn.edu  Mon Feb 18 23:10:07 2008
From: boldinm at wharton.upenn.edu (michaelb)
Date: Mon, 18 Feb 2008 22:10:07 -0000
Subject: [R-SIG-Finance] [R-sig-finance] garchFit failing to give a solution
Message-ID: <15549288.post@talk.nabble.com>


I saw the messages on garch vs garchFit - minimum sample size  and my
experinces are the same. For < 500 obs garchFit() often fails and < 1000
garch() fails.

In fact, using garchSim to create test data with obs < 2000 I found garchFit
failed more often then I would expect.  I conjecture 2 reasons for this (1)
the alpha1=.10 and beta1=.80 starting values are not good choices for many
cases. (2) the lower bound restriction of 10E-6 on each alpha and beta
should be replaced with  0 < sum(alpha,beta) < 1.

I might have missed how to change these in the garchFit documentation.  Or
maybe there are too embedded in the procedures to change, and if so I
suggest adding more flexible upper and lower bounds settings in the next
version of garchFit, and also better diagnostics on non-converged or early
breaking in the optimization step.  

I see the Rmetrics developers contribute here , so IMHO :
For multiple parmater restrictions such 0 < sum(alpha,beta) < 1 a llh
penalty is usually easy to add and works too, and checking h > 0 for all t
might also be necesssary.  Also an initial step that selects the best
starting values from a set of 4 or 5 would add robustness.   Finally, is
there a way to check the garchFit version I am running?  adding the version
number would be useful to add to the output description, as well as the
method used to compute the parameter standard errors and whether a
convergence criteria was met.
-- 
View this message in context: http://www.nabble.com/garchFit-failing-to-give-a-solution-tp15549288p15549288.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From boldinm at wharton.upenn.edu  Tue Feb 19 17:42:51 2008
From: boldinm at wharton.upenn.edu (michaelb)
Date: Tue, 19 Feb 2008 16:42:51 -0000
Subject: [R-SIG-Finance] [R-sig-finance] garchFit failing to give a solution
Message-ID: <15549288.post@talk.nabble.com>


I saw the messages on garch vs garchFit - minimum sample size  and my
experinces are the same. For < 500 obs garchFit() often fails and < 1000
garch() fails.

In fact, using garchSim to create test data with obs < 2000 I found garchFit
failed more often then I would expect.  I conjecture 2 reasons for this (1)
the alpha1=.10 and beta1=.80 starting values are not good choices for many
cases. (2) the lower bound restriction of 10E-6 on each alpha and beta
should be replaced with  0 < sum(alpha,beta) < 1.

I might have missed how to change these in the garchFit documentation.  Or
maybe there are too embedded in the procedures to change, and if so I
suggest adding more flexible upper and lower bounds settings in the next
version of garchFit, and also better diagnostics on non-converged or early
breaking in the optimization step.  

I see the Rmetrics developers contribute here , so IMHO :
For multiple parmater restrictions such 0 < sum(alpha,beta) < 1 a llh
penalty is usually easy to add and works too, and checking h > 0 for all t
might also be necesssary.  Also an initial step that selects the best
starting values from a set of 4 or 5 would add robustness.   Finally, is
there a way to check the garchFit version I am running?  adding the version
number would be useful to add to the output description, as well as the
method used to compute the parameter standard errors and whether a
convergence criteria was met.
-- 
View this message in context: http://www.nabble.com/garchFit-failing-to-give-a-solution-tp15549288p15549288.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From ed_7bit at yahoo.com  Sat Feb 23 20:10:23 2008
From: ed_7bit at yahoo.com (EdNel)
Date: Sat, 23 Feb 2008 19:10:23 -0000
Subject: [R-SIG-Finance] [R-sig-finance] chart.Histogram differences between
 PerformanceAnalytics 9.5 and 9.6
Message-ID: <15655891.post@talk.nabble.com>


I've encountered two differences between versions 9.5 and 9.6 in the results
I get for of the chart.Histogram function :

i) The y axis (density) limits are fixed at 0,10 but they used to follow the
default behaviour of the plot function and adjust to accomodate the maximum
value

ii) I get lots of warnings like this :
"In VaR.CornishFisher(x, p = p) :
  Cornish-Fisher Expansion produces unreliable result (risk over 100%) for
column: 1 : 6.21173467258483"
I assume the function is calculating VaRs though (methods =
c("add.density","add.centered")), I'm not actually using them.

Are these bugs or are there parameters I can change to get the 9.5 behaviour
back?

Thanks.

-- 
View this message in context: http://www.nabble.com/chart.Histogram-differences-between-PerformanceAnalytics-9.5-and-9.6-tp15655891p15655891.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From info at genetrix.se  Sun Feb 24 19:34:09 2008
From: info at genetrix.se (Gex)
Date: Sun, 24 Feb 2008 18:34:09 -0000
Subject: [R-SIG-Finance] [R-sig-finance] ohlcPlot
In-Reply-To: <14011629.post@talk.nabble.com>
References: <14011629.post@talk.nabble.com>
Message-ID: <15667519.post@talk.nabble.com>


Hello again, if someone can help with the ohlcPlot problem i would be very
grateful indeed !
At least i think its a bit weird, both commands used are within the Rmetrics
suite, the data
downloaded with yahooSeries should be compatible with ohlcPlot. Should i
omit 'volume' 
perhaps ?


Gex wrote:
> 
> Hello (again), i have a small problem with the ohlcPlot fSeries package
> using yahooImport. 
> I've tried with returnClass timeSeries, ts, data.frame and matrix. 
> I also get the same error using get.hist.quote returnClass zoo.
> 
> sp500 <-
> yahooSeries("^GSPC",nDaysBack=252*2,quote=c("Open","High","Low","Close","Volume"),
> aggregation="d", returnClass = "timeSeries")
> 
> 
> str(sp500)
> Formal class 'timeSeries' [package "fSeries"] with 8 slots
>   ..@ Data         : num [1:360, 1:5] 1272 1259 1242 1236 1234 ...
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : atomic [1:360] 2006-07-12 2006-07-13 2006-07-14 2006-07-17
> ...
>   .. .. .. ..- attr(*, "control")= Named chr "GMT"
>   .. .. .. .. ..- attr(*, "names")= chr "FinCenter"
>   .. .. ..$ : chr [1:5] "^GSPC.Open" "^GSPC.High" "^GSPC.Low"
> "^GSPC.Close" ...
>   ..@ positions    : atomic [1:360] 2006-07-12 2006-07-13 2006-07-14
> 2006-07-17 ...
>   .. ..- attr(*, "control")= Named chr "GMT"
>   .. .. ..- attr(*, "names")= chr "FinCenter"
>   ..@ format       : chr "%Y-%m-%d"
>   ..@ FinCenter    : chr "GMT"
>   ..@ units        : chr [1:5] "^GSPC.Open" "^GSPC.High" "^GSPC.Low"
> "^GSPC.Close" ...
>   ..@ recordIDs    :'data.frame':       0 obs. of  0 variables
>   ..@ title        : chr "Time Series Object"
>   ..@ documentation: chr "Wed Nov 28 22:53:50 2007"
> 
> 
> then the error:
> 
>> ohlcPlot(sp500)
> Error in ohlcPlot(sp500) : x is not a open/high/low/close time series
> 
> 
> Thank you.
> 

-- 
View this message in context: http://www.nabble.com/ohlcPlot-tp14011629p15667519.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From endres.karl at web.de  Tue Feb 26 19:28:31 2008
From: endres.karl at web.de (karl endres)
Date: Tue, 26 Feb 2008 18:28:31 -0000
Subject: [R-SIG-Finance] =?iso-8859-15?q?modifiedVaR_at__coskewness=2C_cok?=
 =?iso-8859-15?q?urtosis=2C_and_higher_beta_co-moments_of_the_return?=
Message-ID: <487840648@web.de>

Hello, i think in the formula at the line with the quantil's its something wrong with the modified VaR

if ( p == 0.95 ) {
        zc = -1.96


zc is for alpha = 0.025 and not for 0.05 
or not ?

have a nice day

_____________________________________________________________________
Unbegrenzter Speicherplatz f?r Ihr E-Mail Postfach? Jetzt aktivieren!
http://www.digitaledienste.web.de/freemail/club/lp/?lp=7


From yuri.volchik at gmail.com  Wed Feb 27 18:44:48 2008
From: yuri.volchik at gmail.com (Yuri Volchik)
Date: Wed, 27 Feb 2008 17:44:48 -0000
Subject: [R-SIG-Finance] [R-sig-finance] Extracting OHLC from trade
	price series
In-Reply-To: <e8e755250802191639p36e42eb3jfc40afcdd2eb1cdf@mail.gmail.com>
References: <e385f1f9245b9.47bb72ca@optonline.net>
	<e8e755250802191639p36e42eb3jfc40afcdd2eb1cdf@mail.gmail.com>
Message-ID: <15718653.post@talk.nabble.com>


Hi to all members,

I presume my question is to Jeff mostly concerning to.minutes:
I have a xts object with tick data:

mm<-xts(dd[,2:4],new.timestamps)

                             BO   Price        Quantity
2007-01-02 08:13:52 O  18.50000000   5     
2007-01-02 08:15:02 B  18.29999924   3     
2007-01-02 08:15:02 B  18.29999924  10     
2007-01-02 08:46:31 B  18.00000000  10     
2007-01-02 09:01:43 B  17.85000038   1     
2007-01-02 09:04:48 B  17.85000038   9     
2007-01-02 09:19:58 B  17.85000038   1     
2007-01-02 09:38:19 B  17.85000038   1     
2007-01-02 09:54:08 B  17.70000076   5     
2007-01-02 10:07:25 O  17.79999924   5 
...


and trying to convert to 5 min data using 
xx<-to.minutes(mm[,2],5,'minutes')

                    minutes.Open minutes.High minutes.Low minutes.Close
2007-01-02 08:13:52  18.50000000  18.50000000 18.50000000   18.50000000
2007-01-02 08:15:02  18.29999924  18.29999924 18.29999924   18.29999924
2007-01-02 08:46:31  18.00000000  18.00000000 18.00000000   18.00000000
2007-01-02 09:01:43  17.85000038  17.85000038 17.85000038   17.85000038
2007-01-02 09:04:48  17.85000038  17.85000038 17.85000038   17.85000038
2007-01-02 09:19:58  17.85000038  17.85000038 17.85000038   17.85000038
2007-01-02 09:38:19  17.85000038  17.85000038 17.85000038   17.85000038
2007-01-02 09:54:08  17.70000076  17.70000076 17.70000076   17.70000076
2007-01-02 10:07:28  17.79999924  17.79999924 17.79999924   17.79999924
2007-01-02 10:17:22  17.79999924  17.79999924 17.79999924   17.79999924
....

The question is this output correct, is there a way to convert tick data to
a somewhat 'nice' representation with equally spaced time intervals and
using specific method of interpolation for missing data (or just leaving
them as NA). I created such code in R, but i think it is quite slow. 

Thanks




Jeff Ryan wrote:
> 
> Hi,
> 
> The package 'xts' (the function in question previously part of
> 'quantmod') has a nice and fast aggregation function that allows you
> to create OHLC from any univariate series, or from an existing OHLC
> series - called 'to.period'.
> 
> library(quantmod)
> getSymbols("QQQQ")
> 
> to.monthly(QQQQ)  # yields a monthly series from daily data
> 
> The code works equally well for anything from minute bars on up.  It
> should work below that, though I can't promise anything as I haven't
> really tested that recently.  Other functions in the group are
> to.minutes, to.hourly, to to.daily... you get the idea.
> 
> to.period is the function you want to look at.  It calls Fortran - so
> it is very fast on all but gigantic data sets - and then nothing is :)
> 
> 
> 

-- 
View this message in context: http://www.nabble.com/Extracting-OHLC-from-trade-price-series-tp15579652p15718653.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From molyboga at gmail.com  Wed Feb 27 21:49:33 2008
From: molyboga at gmail.com (maratikus)
Date: Wed, 27 Feb 2008 20:49:33 -0000
Subject: [R-SIG-Finance] [R-sig-finance] robust portfolio optimization
Message-ID: <15722777.post@talk.nabble.com>


I am exploring robust portfolio optimization.  I have historical daily data
for 20 stocks over 2 year period.  i'd like to simulate 1,000 datasets of 1
year each that have autocorrelation and cross-correlation properties similar
to those of the historical data.  Then I'd like to find allocation that
maximizes minimum risk-adjusted return over 1,000 datasets.  All suggestions
are appreciated!
-- 
View this message in context: http://www.nabble.com/robust-portfolio-optimization-tp15722777p15722777.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From reinhold.hafner at gmx.de  Mon Mar  3 12:44:22 2008
From: reinhold.hafner at gmx.de (Reinhold Hafner)
Date: Mon, 03 Mar 2008 11:44:22 -0000
Subject: [R-SIG-Finance] fPortfolio
Message-ID: <000001c87d23$e61e1e40$16b2a8c0@Reinhold>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080303/fba90734/attachment.pl>

From molyboga at gmail.com  Wed Mar  5 23:14:37 2008
From: molyboga at gmail.com (maratikus)
Date: Wed, 05 Mar 2008 22:14:37 -0000
Subject: [R-SIG-Finance] [R-sig-finance] question about optim
Message-ID: <15861138.post@talk.nabble.com>


I'm trying to run optimization with box constraints and sum of all weights
equal to 1.  I don't see how I can add second constraint.  Your help is
appreciated.  
-- 
View this message in context: http://www.nabble.com/question-about-optim-tp15861138p15861138.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From reinhold.hafner at gmx.de  Sun Mar  9 15:30:58 2008
From: reinhold.hafner at gmx.de (Reinhold Hafner)
Date: Sun, 09 Mar 2008 14:30:58 -0000
Subject: [R-SIG-Finance] fporfolio
Message-ID: <000001c881f2$2addd930$16b2a8c0@Reinhold>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080309/60ab4ea4/attachment.pl>

From Daniel.Gerlanc at geodecapital.com  Tue Mar 11 19:10:20 2008
From: Daniel.Gerlanc at geodecapital.com (Gerlanc, Daniel)
Date: Tue, 11 Mar 2008 18:10:20 -0000
Subject: [R-SIG-Finance] Risk Model Mapping
Message-ID: <8054312E4FF7D548A21B3370F212DD481B7AB0@MSGMROCLB2WIN.DMN1.FMR.COM>

Hello,

My problem involves mapping between different proprietary risk models,
specifically the Barra USE3L model and the Barra GEM model.  Allow me to
provide some background.

We subscribe to proprietary risk models developed by Barra.  These risk
models provide are grouped largely by region so we have a US model
(USE3L) and a global model (GEMM).  These models provide the covariance
of equities with different factors such as size, value, growth, momentum
and also map each security to one or more industries.

Each model contains different sets of factors.  For example, the USE3L
has 13 risk factors and GEM has 4 risk factors.  Some of these factors
refer to similar concepts. The US model has a factor called success and
the global model has a factor called momentum, and the relationship
between them is fairly linear.  The US model also maps each security to
up to 5 different industries with each industry having its own weight.
The global model maps each security to a single industry in a binary
manner.  A security either belongs to that industry or it does not.

The USE3L model covers some Canadian securities, but not all the
Canadian securities I would like it to.  Fortunately, these securities
are included in the GEMM model.  What I would like to do is continue
using the USE3L model, but find a way to shoehorn the GEM Canadian
stocks into the USE3L model.

Here are the approaches I've considered for doing this:

1. Estimate a linear model between style factors such as success and
momentum using the US and global data, and use the estimated regression
equation to map the GEM risk values to USE3L values.

2. Estimate the correlations between different industries in GEM and
USE3L.  Map GEM industries to the USE3L industries with the largest,
positive significant correlation.  Alternatively, keep the top 3
positively, significantly correlated industries; however, this will
likely assign related industries that a company may not actually
participate in.

3. Look up the data for each company and calculate its exposure to each
industry by the average amount of revenue it has generated in the last 3
years by industry.

In addition, I need some type of metric for determining how well the
mapping has worked.  Ultimately, what I want to produce is a correlation
matrix between stocks.  I would hope that the correlations produced by
the modified USE3L model would be similar to the correlations produced
by the GEM model.  To test this, I planned to subtract the GEM
correlation matrix from the USE3L correlation matrix after subsetting
out only the Canadian stocks.  Taking the absolute value of the matrix
and summing the values would give me an idea of the cumulative absolute
difference between the new and old model.

Any ideas on better ways to map between the models or a better metric
for testing how successful the mapping is?

Thanks!


Daniel Gerlanc
Associate Analyst
Geode Capital Management
1 Post Office Sq, Floor 28
Boston, MA 02109
Daniel.Gerlanc at geodecapital.com


From david.merritt at bris.ac.uk  Thu Mar 20 12:27:28 2008
From: david.merritt at bris.ac.uk (DavidM.UK)
Date: Thu, 20 Mar 2008 11:27:28 -0000
Subject: [R-SIG-Finance] [R-sig-finance] timeDate conversion [C1]
In-Reply-To: <OF33BEE4EA.17F1467C-ONC12573E0.005006CB-C12573E0.0052689B@fr.world.socgen>
References: <OF33BEE4EA.17F1467C-ONC12573E0.005006CB-C12573E0.0052689B@fr.world.socgen>
Message-ID: <16176711.post@talk.nabble.com>


If you're dealing with tick data (which is usually extensive) I'd advise you
do any time/date processing outside of R.  My suggestion would be to use
Perl to format the dates and then read the data into R, this is how I deal
with tick data at least.

Best,

David

anass.mouhsine wrote:
> 
> 
> 
> Hello,
> 
> I am carrying out some research on tick data and I am faced to a date
> problem when trying to transform my dataset into timeSeries.
> My date time is written this way
> "2005-04-08 08:30:35.916"
> where I used this format "%Y-%m-%d %H:%M:%OS" to benfit from the
> tick-by-tick information.
> I have a problem when I convert this character object to timeDate
> 
>> dates.tmp=timeDate("2005-04-08 08:30:35.916",format="%Y-%m-%d %H:%M:%OS")
> Error in if (sum(lt$sec + lt$min + lt$hour) == 0) isoFormat = "%Y-%m-%d" :
>         missing value where TRUE/FALSE needed
> In addition: Warning message:
> Unknown Format Specification in: .whichFormat(charvec)
> 
> when debugging, I've found the problem coming from the function
> ..whichFormat(charvec) that don't recognize the format of the input.
> Browse[1]> .whichFormat(charvec)
> [1] "unknown"
> Warning message:
> Unknown Format Specification in: .whichFormat(charvec)
> 
> Have anyone faced this kind of issue?
> 
> Thank you
> 
> Anass Mouhsine
> 
> 
> 
> 
> 
> *************************************************************************
> This message and any attachments (the "message") are con...{{dropped:10}}
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.
> 
> 

-- 
View this message in context: http://www.nabble.com/timeDate-conversion--C1--tp15184361p16176711.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From rsumithran at yahoo.com  Tue Mar 25 00:55:32 2008
From: rsumithran at yahoo.com (Lavan)
Date: Mon, 24 Mar 2008 23:55:32 -0000
Subject: [R-SIG-Finance] [R-sig-finance] matrix
Message-ID: <16265306.post@talk.nabble.com>


Hi, I'm new to R and need some help.

I have a function in x and y and let's call it f(x,y). I need to get the
Hessian matrix. i.e I need (d^2f/dx^2), (d^2f/dxdy), (d^2f/dydx),
(d^2f/dy^2).I can get these using the D function. now I need to evaluste the
hessian matrix for -5<x<5 and -2<y<2 and I also need to print the output.

I tried  using a for loop,but it is giving me an error.

Thanks
-- 
View this message in context: http://www.nabble.com/matrix-tp16265306p16265306.html
Sent from the Rmetrics mailing list archive at Nabble.com.


From Faan.Louw at nwu.ac.za  Wed Mar 26 08:40:52 2008
From: Faan.Louw at nwu.ac.za (Faan Louw)
Date: Wed, 26 Mar 2008 07:40:52 -0000
Subject: [R-SIG-Finance]
	threshold	autoregression&In-Reply-To=008c01c85ddd$998ed750$ccac85f0$@com
Message-ID: <20080326T094050Z_6AF100000000@nwu.ac.za>

Matthiew,

I am a lecturer in economics at the North est University, South Africa. I read with great interest about your work on threshold cointehration and the indication that you may have a piece of software in R that will allow one to do a threshold cointegration. Was my understanding correct? If so, could you please share your R threshold cointegration stuff mith me. 

Regards

Faan Louw
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-finance/attachments/20080326/c5b66cc3/attachment.html>

